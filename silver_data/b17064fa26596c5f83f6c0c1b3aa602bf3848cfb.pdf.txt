Multi - Objective Deep Inverse Reinforcement Learning through Direct Weights and Rewards Estimation Daiko Kishikawa 1 † and Sachiyo Arai 1 1 Department of Urban Environmental Systems , Chiba University , Chiba , Japan ( E - mail : d . kishikawa @ chiba - u . jp ) Abstract : Inverse reinforcement learning ( IRL ) methods effectively imitate complex behaviors and analyze intentions by estimating rewards from expert demonstrations . However , while most real - world problems are multi - objective problems ( MOPs ) , most IRL methods target the estimation of a single - objective reward , thus creating a need for multi - objective IRL ( MOIRL ) methods to estimate rewards for MOPs . Previous MOIRL methods used matrix factorization , an indirect estimation method with a two - step learning process . We propose a novel MODIRL framework that directly estimates weights and rewards in MOIRL . Further , we experimentally verify that the proposed method can simultaneously estimate both : appropriate weights and rewards . Keywords : Multi - objective inverse reinforcement learning , linear scalarization 1 . INTRODUCTION In reinforcement learning ( RL ) , agents learn optimal actions through trial and error ; RL methods learn poli - cies that maximize expected cumulative returns under a Markov decision process ( MDP ) formulation . Tradition - ally , RL methods have faced two challenges : the feature design problem and the reward design problem . The former refers to the problem of designing a func - tion that extracts appropriate features from observations for appropriate learning . For this problem , deep RL methods that apply neural networks have been proposed in recent years . The performance of RL methods has been dramatically improved because neural networks automate feature extraction and function approximation . On the other hand , the latter refers to the problem that an appro - priate policy cannot be obtained if the reward function cannot properly evaluate the goodness of the state and action . Traditionally , a human has been required to de - sign a complex reward function , but the Learning - from - Demonstration ( LfD ) method has been proposed to ad - dress this problem . LfD methods imitate data comprising recorded expert policies or transitions ( demonstrations ) . In recent years , they have been used in many studies because it is of - ten easier to collect demonstrations than to design reward functions . LfD methods can be divided into two main ap - proaches : imitation learning ( IL ) and inverse reinforce - ment learning ( IRL ) [ 1 ] . IL is an approach that directly imitates the expert pol - icy , whereas IRL is an approach that indirectly imitates the expert policy by ﬁrst estimating the reward from the demonstration and then learning the optimal policy for that reward . By re - training on the rewards obtained by IRL , the obtained agent is robust to the problem of covari - ate shift . In other words , it is possible to obtain a more robust policy for unknown states that are not included in the demonstration . The reward estimation also makes it possible to analyze the expert’s intentions . † Daiko Kishikawa is the presenter of this paper . Traditional IRL methods assume that the expert is op - timizing a single - objective reward . However , many real - world problems are multi - objective problems ( MOPs ) in which multiple objectives are optimized . For example , it is possible to set two objectives for autonomous driv - ing : speed and safety . Increasing speed leads to a de - crease in safety , and vice versa . In this way , objectives in MOP often have a trade - off relationship . Decision - making problems considering multiple objectives are for - mulated under a multi - objective MDP ( MOMDP ) . We deﬁne a multi - objective IRL ( MOIRL ) method as an ex - tension of IRL to apply to MOMDPs . As a MOIRL method , a method based on non - negative matrix factorization ( NMF ) ( NMF - MOIRL ) [ 2 ] has been proposed . NMF - MOIRL assumes that an agent learns a weighted sum of multi - objective preferences ( weights ) and multi - objective rewards and that multiple agent demonstrations with different weights are obtained . The decomposition procedure for NMF - MOIRL is as follows : First , the single - objective IRL method is ap - plied separately to each agent’s demonstration ( IRL step ) . Then , NMF decomposes the resulting agent reward ma - trix into the weight vector of each agent and the reward vector of each objective ( NMF step ) . As an extension of NMF - MOIRL , reward matrix decomposition ( RMD ) [ 3 ] was also proposed . It involves removing the non - negative constraint and adding a weights constraint . NMF - MOIRL and RMD are indirect estimation meth - ods through two - step learning , although the formulation is reasonable . Two - step learning in these methods has the following three problems : First , it is necessary to run as many IRL methods as the number of agents in the IRL step . Second , the weight and reward vectors in the NMF step cannot be used to update the estimated reward val - ues in the IRL step . Third , the matrix decomposition method requires a dimensionality reduction method when the number of state dimensions is greater than two . In this study , we propose an MO deep IRL ( MODIRL ) framework that directly estimates multi - objective weights and multi - objective rewards . The MODIRL framework 2022 61st Annual Conference of the Society of Instrument and Control Engineers of Japan ( SICE ) September 6 - 9 , 2022 , Kumamoto , Japan 978 - 4 - 9077 - 6478 - 4 PR0001 / 22 ¥400 © 2022 SICE 122 inherits the formulations in NMF - MOIRL and RMD and the constraints on weights proposed in RMD . The MODIRL framework utilizes a weight - indexing network and a multi - objective reward network . The MODIRL framework can be applied by extending the existing deep IRL methods . In particular , it can be applied to ofﬂine IRL methods for fast weight and reward estimation . We apply the MODIRL framework to an IRL method based on the reward - seeker principle and experimentally verify that the proposed framework can estimate weights and rewards in a multi - objective Mountain Car Continu - ous ( MOMCC ) environment . 2 . RELATED WORK A comparison of the MOIRL method and related methods is shown in Table 1 . First , the multi - objective weight learning ( MOWL ) method is an approach similar to the MOIRL method ; it estimates the unknown weights of the expert from the demonstration . Focusing on the fact that early IRL methods learned a linear reward func - tion consisting of weights and features , we propose a method in which features are replaced by a reward vector [ 4 ] . However , the MOWL method is not an IRL method that estimates rewards because it takes rewards as given and thus cannot be used when the rewards are unknown . A method has also been proposed to estimate the multi - objective reward values by interpolating them from the data [ 5 ] . We call this method multi - objective reward interpolation ( MORI ) . MORI cannot be applied to the case where the reward is completely unknown because it requires ( though partially required ) the true reward value for interpolation . It also does not consider the estimation of weights . MOIRL methods need to estimate both the unknown weights of the agents and the unknown multi - objective rewards . To the best of our knowledge , no MOIRL method has been proposed except for the two methods described later . Table 1 Comparison of the characteristics of each method . “Known” indicates that the information is assumed to be known , and “Unknown” indicates that the method can deal with unknown information . Methods Weight Reward MOWL Unknown Known MORI - Known ( Partially ) MOIRL Unknown Unknown 3 . BACKGROUND 3 . 1 . Reinforcement learning and Markov decision process In reinforcement learning ( RL ) , an agent learns opti - mal actions under the Markov decision process ( MDP ) . The MDP is represented by a state S , an action A , a re - ward R , a state transition probability P , and a discount rate γ ( 0 ≤ γ ≤ 1 ) . The agent maximizes the ex - pected discounted cumulative reward obtained in one trial ( episode ) . 3 . 2 . Multi - objective RL and multi - objective MDP Real - world decision - making problems require the consideration of multiple objectives . RL methods that consider multiple objectives are called multi - objective RL ( MORL ) methods . MORL learns an optimal pol - icy according to a multi - objective MDP ( MOMDP ) ⟨ S , A , P , R ob , γ ⟩ . MOMDP is a multi - objective exten - sion of MDP , where a multi - objective reward vector for each objective R ob replaces the reward R of MDP . There are two main approaches to MORL methods . One is the multiple - policy approach , in which the agent maintains policies for each objective in the reward vec - tor . The other is the single - policy approach , in which the reward vector is scalarized in some way , and the agent learns a single policy based on the scalar reward values . The most common method used in the single - policy ap - proach is linear scalarization . Linear scalarization , also known as the weighted sum method [ 6 ] , multiplies the re - ward of each objective by its weight , which is the impor - tance of each objective , and calculates the sum of them to obtain a scalar . 3 . 3 . Inverse reinforcement learning and multi - objective inverse reinforcement learning Inverse reinforcement learning ( IRL ) [ 1 ] methods are the inverse of RL methods . While RL methods learn an optimal policy from rewards , IRL methods estimate rewards from data ( demonstrations ) that record optimal policies or optimal state - transition sequences . The meth - ods that approximate the reward by a neural network and estimate a nonlinear reward are called deep IRL ( DIRL ) methods . Traditional IRL methods are based on MDPs and as - sume that the expert considers a single - objective reward . In contrast , multi - objective IRL ( MOIRL ) methods as - sume that the expert considers a multi - objective reward vector , i . e . , learns according to the MOMDP . To solve the MOIRL method , we focus on linear scalarization and make the following two assumptions : Assumption 1 . The multi - objective reward vector R ob is common to all agents , and what differs among agents is the weight for each objective ( agent weight vec - tor ) w ag . The agent then learns a policy according to the reward value r obtained by linearly scalarizing the n ob - jective weights and the multi - objective reward , as shown in Eq . ( 1 ) . r ( s ) = w 1 R 1 ( s ) + w 2 R 2 ( s ) + · · · + w n R n ( s ) ( 1 ) 123 Rewriting Eq . ( 1 ) in vector form , we obtain Eq . ( 2 ) . r = w ag · R ob ( 2 ) where · denotes the dot product . Assumption 2 . We have access to several agent demonstrations with different weights grouped for each agent . 3 . 4 . Previous MOIRL methods and their problems Two MOIRL methods have been proposed for the above - mentioned assumptions : the MOIRL method with non - negative matrix factorization ( NMF - MOIRL ) [ 2 ] and the reward matrix decomposition ( RMD ) [ 3 ] . These MOIRL methods separate the agent weight vector and multi - objective reward vector by ﬁrst estimating the re - wards for each agent’s demonstration data group using the single - objective IRL method , then matrixing the ob - tained rewards , and ﬁnally applying matrix factorization . There are two problems with these MOIRL methods . First , the ﬂow from the single - objective IRL step to the matrix factorization step is unidirectional . In other words , updating the weight and reward vectors does not affect the estimated rewards in IRL . Therefore , if the estimated reward in IRL is inaccurate , the weight and reward vec - tors approximated by the matrix factorization will also be inaccurate . Next , to use matrix factorization , it is necessary to con - vert the estimated rewards into a vector of a ﬁnite num - ber of elements with the reward values of all states and combine them into a matrix for each agent . In addition , matrix factorization can only handle a matrix of up to two dimensions . Thus , some dimensionality reduction method is required when the number of state dimensions is greater than two . 3 . 5 . A solution to the traditional problem Consequently , we consider directly learning agent weight vectors and multi - objective reward vectors using an IRL method without matrix factorization . In the next section , we describe the details of the proposed frame - work . 4 . MODIRL FRAMEWORK THROUGH DIRECT WEIGHTS AND REWARDS ESTIMATION We propose a multi - objective deep IRL ( MODIRL ) framework that directly estimates agent weight vectors and multi - objective reward vectors . Fig . 1 shows the structural differences between the traditional deep IRL method and the MODIRL framework . First , the weight - indexing network WINet takes an agent number i as input and outputs the corresponding agent weight vector w ag ( i ) . w winet ( i ) is shown in Eq . ( 3 ) . w ag ( i ) = WINet ( i ) = softmax o ( f ( i , θ w ) ) ( 3 ) where f ( x ; θ ) denotes the neural network with parame - ters θ , and softmax o denotes the softmax normalization function in the axial direction for the objective o , respec - tively . Next , the multi - objective reward network MORNet takes a state s as input and outputs the correspond - ing multi - objective reward vector R ob ( s ) . MORNet is shown in Eq . ( 4 ) . R ob ( s ) = MORNet ( s ) = f ( s ; θ r ) ( 4 ) The resulting agent weight vector and multi - objective re - ward vector are then linearly scalarized to obtain a scalar estimated reward value ˜ r i ( s ) . This operation is shown in Eq . ( 5 ) . ˜ r i ( s ) = w ag ( i ) · R ob ( s ) ( 5 ) In Eq . ( 5 ) , · denotes the dot product . Finally , by optimizing the scalar estimated reward val - ues with the usual DIRL method , the agent weight vec - tor and the multi - objective reward network are learned simultaneously . The MOIDRL framework can be applied to any of the DIRL methods . In particular , the MODIRL framework can be learned quickly by applying it to DIRL meth - ods that learn by classiﬁcation ( e . g . , Logistic Regression - Based IRL ( LogReg - IRL ) [ 7 ] or Trajectory - ranked Re - ward Extrapolation ( T - REX ) [ 8 ] ) . 4 . 1 . Beneﬁts of the MODIRL framework The beneﬁts of our proposed MODIRL framework are as follows : Feedback to the estimated rewards is pos - sible . In the MODIRL framework , the estimated reward is composed of WINet and MORNet . Thus , updating these networks provides feedback to the current estimate of reward values in IRL training . The structure of the framework reproduces the matrix decomposition structure . In the MODIRL framework , MORNet is shared among the agents , and only the weight vectors are different . Therefore , differ - ences in the estimated rewards of each agent are absorbed by the weight vector output by WINet , and MORNet absorbs the common part behind the estimated rewards . The structure of the MODIRL framework is expected to lead to similar constraints as the matrix factorization ap - proach . Softmax normalization satisﬁes the constraints on the weights . Constraints on the weights are achieved by normalizing the output of WINet by a softmax function . No dimensionality reduction method is required . The number of inputs to MORNet can be increased as much as the dimensionality of the state , and so there is no need to use dimensionality reduction methods . Based on the beneﬁts listed above , we consider that the MODIRL framework can solve the problems of tradi - tional MOIRL methods . 4 . 2 . Two techniques for stable training We found two techniques for stable learning in the MODIRL framework . They have been described below . 124 𝑠 𝑖 𝑹 ob ( 𝑠 ) 𝒘 ag ǁ𝑟 ( 𝑠 ) ǁ𝑟 ( 𝑠 ) 𝑠 𝑖 Fig . 1 The reward network in traditional deep IRL methods ( left ) and the MODIRL framework ( right ) 4 . 2 . 1 . Pre - training for WINet We have sometimes encountered a situation in which the weights of all agents converge to 0 or 1 , depending on the initial values of the network parameters , when WINet is initialized , and the MODIRL training begins immedi - ately . Therefore , we pre - trained WINet to output 1 / N ob as the initial value of the weights . Speciﬁcally , learning is performed by minimizing Eq . ( 6 ) . ( WINet ( i ) − 1 N ob ) 2 ( 6 ) 4 . 2 . 2 . Delayed learning start for the weight - indexing network Suppose WINet and MORNet start learning simulta - neously . In that case , it may not be possible to estimate the appropriate weights and rewards because the learning of the weights is comparatively faster than the learning of the rewards . Therefore , WINet is frozen until the middle of the learning process . We start training WINet when MORNet has been well - trained . 5 . RESULTS AND DISCUSSION We conducted experiments to validate our proposed MOIDRL framework . Below is a description of the DIRL method , the RL method , and the experimental environ - ment used , respectively . 5 . 1 . Reward - seeker training We employed a DIRL method based on reward - seeker training ( RESET ) , a learning method based on the fol - lowing reward - seeker principle : Reward - seeker principle . Expert agents always tran - sition toward states of equal or greater preference to seek higher rewards . In other words , the following preference relation holds for any pair of state transitions sampled from the expert demonstration . s ∗ t + 1 ⪰ s ∗ t ( 7 ) From Eq . ( 7 ) , the expert’s reward always satisﬁes the relation Eq . ( 8 ) . r ∗ ( s t + 1 ) ≥ r ∗ ( s t ) ( 8 ) Based on this principle , we learn a Bradley - Terry model [ 9 ] composed of estimated reward ˜ r ( x ) . Eq . ( 7 ) is ex - pressed by the Bradley - Terry model to obtain Eq . ( 9 ) . p ( s t + 1 ⪰ s t ) = exp ( ˜ r ( s t + 1 ) ) exp ( ˜ r ( s t ) ) + exp ( ˜ r ( s t + 1 ) ) ( 9 ) Eq . ( 9 ) can be transformed to Eq . ( 10 ) with the sigmoid function σ ( x ) . Because Eq . ( 10 ) is numerically more sta - ble than Eq . ( 9 ) , we employ Eq . ( 10 ) for reward learning . p ( s t + 1 ⪰ s t ) = σ { ˜ r ( s t + 1 ) − ˜ r ( s t ) } ( 10 ) Reward learning is performed as follows : First , Eq . ( 10 ) is computed for the expert transition pair ( s ∗ t , s ∗ t + 1 ) to obtain the probability p ∗ . Next , we deﬁne a transition pair ( s ∗ t , ¯ s t + 1 ) , where only the next state is replaced by a sample ¯ s t + 1 generated from a uniform distribution in the range [ S min , S max ] , as a transition candidate . Then , the same Eq . ( 10 ) for the candidate transitions is computed to obtain the probability ¯ p . Finally , the reward is updated by minimizing the squared loss in Eq . ( 11 ) so as to maximize p ∗ and minimize ¯ p . L ( ˜ r ) = ( p ∗ − 1 ) 2 + ( ¯ p − 0 ) 2 ( 11 ) T - REX also employs the Bradley - Terry model , and RE - SET can be regarded as a state - centric extension of T - REX’s trajectory - centric model . T - REX requires a trajec - tory ( or state sequence ) , whereas RESET requires only a pair of state transitions ; it can even learn a fragmentary trajectory . RESET does not need ranking data , which is a necessity for T - REX , and can be learned by demon - stration only . T - REX is an ofﬂine IRL method that does not require access to the environment when estimating re - wards . RESET , which uses the same formulation , is also an ofﬂine IRL method and can be trained quickly . We deﬁne RESET - MODIRL as a method that applies RESET to the MODIRL framework , shown in Algorithm 1 . 5 . 2 . RL method As an RL method , we use Soft Actor - Critic ( SAC ) [ 10 ] , used to train expert agents and re - training . The SAC agent is given a linearly scalarized reward vector consist - ing of an agent weight vector and a multi - objective re - ward vector . The SAC agent learns an optimal policy for the scalarized reward . 5 . 3 . Settings in IRL method WINet and MORNet are multi - layer perceptrons consisting of an input layer ( number of neurons N n = 125 Fig . 2 The estimated reward for shortest path ( left ) and action ( right ) ; X - axis indicates position ; Y - axis indicates velocity ; dark red indicates a large positive reward , and dark blue indicates a large negative reward . Algorithm 1 RESET - MODIRL Require : State space S , demonstration groups recorded from m agents D = { τ 1 , τ 2 , · · · , τ m } , objective number n , epoch number N w to resume updating the WINet Ensure : Weight - indexing network WINet ( x ) , multi - objective reward network MORNet ( x ) 1 : Initialize WINet ( x ) and MORNet ( x ) 2 : for each epoch do 3 : Train WINet ( x ) by minimizing Eq . ( 6 ) 4 : end for 5 : Freeze WINet 6 : for each epoch do 7 : if epoch is greater than N w then 8 : Resume WINet updates 9 : end if 10 : Sample expert state transition pair ( s ∗ t , s ∗ t + 1 ) and agent number i from D 11 : Compute ˜ r ( s ∗ t ) and ˜ r ( s ∗ t + 1 ) according to Eq . ( 5 ) 12 : Compute p ∗ from ˜ r ( s ∗ t ) and ˜ r ( s ∗ t + 1 ) according to Eq . ( 10 ) 13 : Generate ¯ s t + 1 from a uniform distribution in the range [ S min , S max ] . 14 : Compute ˜ r ( ¯ s t + 1 ) according to Eq . ( 5 ) 15 : Compute ¯ p from ˜ r ( s ∗ t ) and ˜ r ( ¯ s t + 1 ) according to Eq . ( 10 ) 16 : Compute the loss using Eq . ( 11 ) 17 : Update each network according to the loss 18 : end for n i ) , an output layer ( N n = n o ) , and two hidden layers ( N n = n h ) . For the later experiment , the [ n i , n h , n o ] of WINet and MORNet were set to [ 1 , 256 , 2 ] and [ 2 , 256 , 2 ] , respectively . Adam [ 11 ] ( learning rate : 10 − 4 ) was used for optimization . Dropout [ 12 ] was applied to the input layer with a probability of 0 . 2 and the hidden layer with a probability of 0 . 5 . The batch size was 1024 . [ S min , S max ] was set to [ − 1 . 5 , 1 . 5 ] . Leaky ReLU was used as the activation function except for the output layer . The output of MORNet was clipped at [ − 10 . 0 , 10 . 0 ] . The total number of training epochs was set to 100000 , and N w was set to 50000 . 5 . 4 . Multi - Objective Mountain Car Continuous ( MOMCC ) environment The experimental environment was the multi - objective Mountain Car Continuous ( MOMCC ) environment , which extends the OpenAI Gym’s MountainCarContinuous - v0 environment [ 13 ] with a multi - objective reward vector . The agent considers two objectives : O 1 reaching the goal in the shortest path and O 2 reducing the output of the action as much as possible . As reaching the goal in the shortest path requires a large action output , O 1 and O 2 have a trade - off relationship . Each reward is given by Eqs . ( 12 ) and ( 13 ) . R ( O 1 ) = { 10 ( goal ) − 0 . 01 ( otherwise ) ( 12 ) R ( O 2 ) = 0 . 01 ( 1 − | a t | ) ( 13 ) Eq . ( 12 ) is the reward function for learning the shortest path and rewards 10 when the agent reaches the goal and terminates the episode , and - 0 . 01 otherwise . Eq . ( 13 ) is a reward function for the magnitude of the action , 0 when the action is the largest , and 0 . 01 when the action is the smallest . We prepared 11 weights w 1 for R ( O 1 ) , [ 0 . 0 , 0 . 1 , 0 . 2 , · · · , 1 . 0 ] ( increments of 0 . 1 , w 2 = 1 − w 1 for R ( O 2 ) ) and trained 11 SAC agents . 5 . 5 . Estimated rewards After recording 100 demonstrations each from 11 SAC agents , we estimated weights and rewards using RESET - MODIRL with n = 2 as the number of objectives . The results of visualizing the estimated rewards are exhibited in Fig . 2 . The rewards on the left side of Fig . 2 are higher for the states near the goal and lower for the other states , suggesting that the reward for the shortest path was ob - tained . The rewards on the right side of Fig . 2 are higher around the initial state . When the agent takes only small actions , it stays around the initial state , so the rewards on the right side of Fig . 2 are the rewards for the actions . 5 . 6 . Results of re - training Eleven SAC agents were re - trained using the estimated 11 different weights and multi - objective reward vectors . The distribution of solutions ( Pareto front ) for each agent is shown on the left and right of Fig . ( 3 ) , respectively , as they were when the demonstration started and as the re - sult of re - training . The number in each solution in Fig . ( 3 ) indicates the weight w 1 for the ﬁrst objective dur - ing expert learning . The solutions are divided into three groups : Group 1 , which emphasizes the reward for the action ; Group 2 , which considers both the action and the 126 Fig . 3 The distribution of solutions in the demonstration ( left ) and in re - trained agents ( right ) . Each point represents the expected cumulative reward ( using the true reward function ) for each solution , for the ﬁrst objective on the x - axis and the second objective on the y - axis . reward for the shortest path ; and Group 3 , which empha - sizes the reward for the shortest path . The composition of the agents in each distribution is consistent between the demonstration generation and the re - training , indicating that the Pareto front is partially re - produced . However , the position of Group 2 is different between the two distributions . This difference may be due to the reward network approximating a function re - lated only to the state , thus not accurately reproducing the rewards related to the action . 6 . CONCLUSION We proposed a novel MOIRL framework , called MODIRL , which directly and simultaneously estimates multi - objective weight and reward vectors . The frame - work applies to any DIRL method , and the method imple - mented using RESET and RESET - MODIRL can estimate weight and reward vectors from just the demonstrations . Experimental results show that the proposed framework successfully estimates the weights and rewards and can approximately reproduce the distribution of expert solu - tions by re - training on the estimated weights and rewards . The current MODIRL framework requires that the ob - jective number be given in advance . However , in real - world problems , the objective number is often unknown . We will consider extending the framework to a method that automatically estimates the objective number in fu - ture work . In addition , we will consider combining this method with other DIRL methods and experimenting with other tasks . REFERENCES [ 1 ] S . Russell , “Learning agents for uncertain envi - ronments” , Proceedings of the eleventh annual conference on Computational learning theory , pp . 101 – 103 , 1998 . [ 2 ] D . Kishikawa and S . Arai , “Multi - objective in - verse reinforcement learning via non - negative ma - trix factorization” , 9th International Conference on Smart Computing and Artiﬁcial Intelligence , 2021 . to appear , 6 pages . [ 3 ] D . Kishikawa and S . Arai , “Reward matrix de - composition for multi - objective inverse reinforce - ment learning” , The 36th Annual Conference of the Japanese Society for Artiﬁcial Intelligence , 2022 , 2022 . to appear , 4 pages . [ 4 ] A . Ikenaga and S . Arai , “Inverse reinforcement learning approach for elicitation of preferences in multi - objective sequential optimization” , 2018 IEEE International Conference on Agents ( ICA ) , pp . 117 – 118 . IEEE , 2018 . [ 5 ] B . A . Mitchell , Modeling and Control of Large Scale Neural Systems . eScholarship , University of California , 2019 . [ 6 ] L . Zadeh , “Optimality and non - scalar - valued per - formance criteria” , IEEE transactions on Auto - matic Control , 8 , ( 1 ) , 59 – 60 , 1963 . [ 7 ] E . Uchibe , “Model - free deep inverse reinforce - ment learning by logistic regression” , Neural Pro - cessing Letters , 47 , ( 3 ) , 891 – 905 , 2018 . [ 8 ] D . Brown , W . Goo , P . Nagarajan , and S . Niekum , “Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observa - tions” , International conference on machine learn - ing , pp . 783 – 792 . PMLR , 2019 . [ 9 ] R . A . Bradley and M . E . Terry , “Rank analy - sis of incomplete block designs : i . the method of paired comparisons” , Biometrika , 39 , ( 3 / 4 ) , 324 – 345 , 1952 . [ 10 ] T . Haarnoja , A . Zhou , K . Hartikainen , G . Tucker , S . Ha , J . Tan , V . Kumar , H . Zhu , A . Gupta , P . Abbeel , et al . , “Soft actor - critic algorithms and applications” , arXiv preprint arXiv : 1812 . 05905 , 2018 . [ 11 ] D . P . Kingma and J . Ba , “Adam : a method for stochastic optimization” , arXiv preprint arXiv : 1412 . 6980 , 2014 . [ 12 ] N . Srivastava , G . Hinton , A . Krizhevsky , I . Sutskever , and R . Salakhutdinov , “Dropout : a sim - ple way to prevent neural networks from overﬁt - ting” , The journal of machine learning research , 15 , ( 1 ) , 1929 – 1958 , 2014 . [ 13 ] MountainCarContinuous - v0 , https : / / gym . openai . com / envs / MountainCarContinuous - v0 / . 127