POLESTAR – Collaborative Knowledge Management and Sensemaking Tools for Intelligence Analysts Nicholas J . Pioch , John O . Everett BAE Systems Advanced Information Technologies 6 New England Executive Park Burlington , MA 01803 781 - 273 - 3388 x379 , x275 { nicholas . pioch , john . everett } @ baesystems . com ABSTRACT In this paper , we describe POLESTAR ( POLicy Explanation using STories and ARguments ) , an integrated suite of knowledge management and collaboration tools for intelligence analysts . POLESTAR provides built - in support for analyst workflow , including collection of textual facts from source documents , structured argumentation , and automatic citation in analytic product documents . Underlying POLESTAR is a scalable dependency repository , which provides traceability from product documents to source snippets . The repository’s notification engine allows POLESTAR to alert analysts when dependent sources are discredited and aid them in repairing affected arguments . The paper then discusses recent extensions to POLESTAR to support collaborative analysis through community - of - interest finding , portfolio sharing , and peer review of arguments . We conclude with a preview of future research and summary of POLESTAR’s primary benefits from the point of view of its deployed users . Categories and Subject Descriptors H . 4 . 1 [ Office Automation ] : Groupware , Workflow Management , H . 5 . 2 [ User Interfaces ] : Graphical User Interfaces , I . 2 . 1 [ AI Applications and Expert Systems ] : Office automation General Terms : Algorithms , Management , Human Factors Keywords Knowledge Management , Intelligence Analysis , Collaboration , Argument Structuring , Sensemaking , Workflow Management , Knowledge Representation 1 . INTRODUCTION Accurate and timely intelligence analysis is essential to countering the asymmetric threat of terrorism , both abroad and in the homeland . Threats today fluidly move across geographic boundaries , exploit the openness of our society , and leave minimal traces of their presence scattered across vast amounts of structured and unstructured data . As recent intelligence failures have demonstrated , our current information systems inhibit collaboration and stifle insight with antiquated processes that encode Cold War compartmentalization and a posture better suited to dealing with nation - state adversaries . A new kind of analytic environment is needed—one that promotes the formation of ad hoc collaborative teams that span multiple agencies and jurisdictions to rapidly find the fragments of a particular needle buried in haystacks of needles . The POLESTAR ( POLicy Explanation using STories and Arguments ) system makes significant strides toward this new analytic environment by integrating a sophisticated knowledge management infrastructure with collaborative sensemaking tools . POLESTAR provides tools for intelligence analysts to help them individually and collaboratively construct more deeply reasoned intelligence reports in less time . POLESTAR does this by making the structure of the argument visible , in terms of claims that are supported by evidence and tempered by caveats . Make the structure explicit , and it’s far easier to zero in on its weak spots and evaluate its strengths . POLESTAR also tracks every piece of information that flows through the system , in a repository of links that enables the system to discover emergent collaborations , put analysts working on similar problems in touch with each other , and find patterns in the use of intelligence information that would otherwise remain invisible . Taken to its logical extent , POLESTAR could be the catalyst for converting a highly stovepiped and rigid intelligence community into a fluid , adaptive system that seamlessly shifts and connects resources around emergent threats , facilitating early identification and disruption . Dependency Tracking Repository Dependency Tracking Repository • Fact collection via highlighting • Fact collection via highlighting Structure Read Collect Write Review Search Revise Structure Read Collect Write Review Search Revise Structure Read Collect Write Review Search Revise Structure Read Collect Write Review Search Revise Structure Read CollectCollect Write Review Search Revise • Document declassification • Locating documents that depend on facts no longer believed • Document declassification • Locating documents that depend on facts no longer believed • Argument structure creation • Visualization • Graphical manipulation • Argument structure creation • Visualization • Graphical manipulation • Qualitative analysis • Probabilistic analysis • Qualitative analysis • Probabilistic analysis • Search and browse analysis products , with links to authors , source documents • Search shoeboxes • Search and browse analysis products , with links to authors , source documents • Search shoeboxes Boeing engineers assessed six possible worst - case situations and concluded that four posed no risk If ice strikes the wing ' s leading edge at > 15 degrees it could penetrate the carbon coating The wing was struck by the largest piece of debris ever to hit a space shuttle . The Boeing team never calculated how much worse the damage would have been if the debris was ice The predicted angles were 22 degrees In 1992 , the debris struck at an angle of 3 . 2 degrees . Data from a 1992 Columbia mission that had absorbed a similar debris impact ; the debris left a gouge only a half - inch deep . The program was designed to be conservative Crater predicted significant tile damage , with gouge depths that exceeded the thickness of tiles in several spots . Crater is based on laboratory data of foam hitting tiles . The predicted impact area included several vulnerable points on the wing Prediction process is not reliable Only 11 times have shuttles returned with significant tile damage . Never had a debris strike on a tile been judged a flight risk . The chunk broke into " a spray of white - colored particles " on the wing ' s underside . CLAIM : The debris was foam insulation shed by the external tank . The insulation has a long history of breaking free during liftoff CLAIM : The debris was ice from rain or condensation on the external tank . Ice could do far more damage than foam . Inspectors had not seen any dangerous ice buildup on the external tank before liftoff . CLAIM : The shuttle will disintegrate on reentry CLAIM : The damage is severe CLAIM : The damage is in a vulnerable spot Boeing engineers assessed six possible worst - case situations and concluded that four posed no risk If ice strikes the wing ' s leading edge at > 15 degrees it could penetrate the carbon coating The wing was struck by the largest piece of debris ever to hit a space shuttle . The Boeing team never calculated how much worse the damage would have been if the debris was ice The predicted angles were 22 degrees In 1992 , the debris struck at an angle of 3 . 2 degrees . Data from a 1992 Columbia mission that had absorbed a similar debris impact ; the debris left a gouge only a half - inch deep . The program was designed to be conservative Crater predicted significant tile damage , with gouge depths that exceeded the thickness of tiles in several spots . Crater is based on laboratory data of foam hitting tiles . The predicted impact area included several vulnerable points on the wing Prediction process is not reliable Only 11 times have shuttles returned with significant tile damage . Never had a debris strike on a tile been judged a flight risk . The chunk broke into " a spray of white - colored particles " on the wing ' s underside . CLAIM : The debris was foam insulation shed by the external tank . The insulation has a long history of breaking free during liftoff CLAIM : The debris was ice from rain or condensation on the external tank . Ice could do far more damage than foam . Inspectors had not seen any dangerous ice buildup on the external tank before liftoff . CLAIM : The shuttle will disintegrate on reentry CLAIM : The damage is severe CLAIM : The damage is in a vulnerable spot Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . CIKM’06 , November 5 - 11 , 2006 , Arlington , VA , USA . Copyright 2006 ACM 1 - 59593 - 433 - 2 / 06 / 0011… $ 5 . 00 . Figure 1 . POLESTAR supports the end - to - end intelligence analysis process . 513 The POLESTAR system , described in the subsequent sections , comprises the analytic tools and supporting software components enabling the end - to - end analytic process illustrated in Figure 1 . The next section provides an overview of the evidence marshalling and knowledge structuring capabilities . Section 3 describes the dependency repository that serves as POLESTAR’s knowledge management infrastructure . It also illustrates how it is used in several end - user applications , including alerting analysts of discredited source documents . Section 4 discusses recent enhancements to POLESTAR to support collaborative analysis . The paper concludes with a discussion of POLESTAR’s deployed capabilities and future research directions . 2 . ANALYST WORKFLOW This section summarizes POLESTAR’s workflow support for individual analysts . 2 . 1 Snippet Collection The analytic process in POLESTAR begins with fact collection . POLESTAR minimizes the additional workload of computer - assisted collection through seamless integration with the same tools analysts already use to view source documents . BAE Systems has developed a set of Fact Collector plug - ins embedded in Microsoft Word and Internet Explorer . These plug - ins allow users to highlight text snippets of interest and drag - and - drop them into the Portfolio Browser for subsequent analysis . Optionally , users may enter meta - data about the collected intelligence and its source as well as their own interpretation of the fact within a Fact Card dialog . Figure 2 shows this Fact Card with the user’s interpretation of a fact collected from Internet Explorer . Figure 2 . POLESTAR’s Fact Collector plug - in allows analysts to gather evidence of interest from common desktop web browsers and document editors . 2 . 2 Portfolio Browser The Portfolio Browser ( also called the Portfolio Browser in our deployed system ) is the immediate point of storage for all collected facts . It is designed much like the familiar Windows Explorer , with named folders on the left side and facts arrayed in a list view on the right side . These named folders allow users to organize evidence according to categories of their own choosing . For example , users may place snippets relating to a particular person or country of interest in a folder named for them . Alternatively , a user may name folders after a particular analytic deliverable that is in progress , ranging from a short Special Intelligence Report ( SIR ) to a longer National Intelligence Estimate ( NIE ) . Figure 3 . The Portfolio Browser ( aka Portfolio Browser ) supports organization of facts into named folders . The Portfolio Browser is the user’s interface to the dependency - tracking repository backend of the system . When a user captures a snippet of information , the Argument Engine Core , which is the in - memory object cache and middleware infrastructure for POLESTAR , automatically acquires the snippet and stores it in the POLESTAR repository . POLESTAR also stores metadata associated with the snippet , including its source document identifier , source organization , and classification . Dragging a snippet from the Portfolio Browser into a working draft analysis document in Microsoft Word causes the Argument Engine Core to automatically insert a citation to the source document of that snippet . 2 . 3 Knowledge Structuring The Portfolio Browser is also the launch point for several knowledge structuring tools that help the user spatially , chronologically , or logically arrange evidence collected from the Portfolio Browser . 2 . 3 . 1 Wall of Facts The POLESTAR Wall of Facts , shown in Figure 4 , is a blank workspace onto which the analyst can drag and drop snippets of information that they have collected . Dragging snippets to the edges of the space causes them to shrink , enabling one to work with up to 200 snippets on a single screen . Snippets in the center of the screen are full - size , so analysts typically work in this region with their most current snippets of interest . Analysts may optionally add from scratch a claim textbox , which is the building block of an argument ( see Section 2 . 3 . 3 ) . Evidence relating to a particular claim may be clustered near it spatially . Furthermore , snippets can be grouped into hierarchically nested sub - workspaces . The intent is to enable analysts to rapidly sort through many snippets by sorting them spatially and / or hierarchically into groups , without having to commit to a formal categorization system . Boeing engineers assessed six possible worst - case situations and concluded that four posed no risk The wing was struck by the largest piece of debris ever to hit a space shuttle . The Boeing team never calculated how much worse the damage would have been if the debris was ice The predicted angles were 22 degrees In 1992 , the debris struck at an angle of 3 . 2 degrees . CRATER was designed to be conservative CRATER predicted significant tile damage , with gouge depths that exceeded the thickness of tiles in several spots . CRATER is based on laboratory data of foam hitting tiles . The predicted impact area included several vulnerable points on the wing Prediction process is not reliable Only 11 times have shuttles returned with significant tile damage . Never had a debris strike on a tile been judged a flight risk . The chunk broke into " a spray of white - colored particles " on the wing ' s underside . The insulation has a long history of breaking free during liftoff Ice could do far more damage than foam . Inspectors had not seen any dangerous ice buildup on the external tank before liftoff . If ice strikes the wing ' s leading edge at > 15 degrees it could penetrate the carbon coating Data from a 1992 Columbia mission that had absorbed a similar debris impact ; the debris left a gouge only a half - inch deep . Figure 4 . The Wall of Facts allows analysts to spatially organize and cluster information . 2 . 3 . 2 Timelines The wall of facts includes a timeline view ( see Figure 5 ) , which arrays snippets linearly in order of dates that users add to the metadata of each snippet . Seeing this arrangement can clarify relationships that are hard to detect when looking at a series of textual dates . For example , a timeline of terrorist attacks on US interests over the ten years leading up to 9 / 11 very clearly shows an increase in the frequency of attacks . Much like the Wall of Facts , analysts may create sub - timelines to cluster events into separate stages or categories . Figure 5 . Chronological ordering and grouping of facts is possible in the Timeline View . 2 . 3 . 3 Argument Tree Editor After spatially or chronologically organizing evidence , analysts formulate hypotheses as arguments . POLESTAR uses a graphical approach to argument structuring , based on methods for analyzing legal documents formulated by Wigmore [ 10 ] and Toulmin [ 7 ] . POLESTAR represents arguments as a tree structure of claims , each supported by at least one piece of evidence ( i . e . , information drawn from a primary source ) . Claims may also have caveats that weaken or restrict their force , and subclaims that act as supports or caveats . The general representation for POLESTAR arguments is shown hierarchically in Figure 6 . Fact Interpretation Claim Hypothesis Document Claim Claim Reliability Source Support / Rebut OriginSupport / Rebut Origin InfoType Assumption Classification Figure 6 . POLESTAR’s recursive argument structure includes supporting and rebutting claims with supporting evidence from primary document sources . A specific example of a POLESTAR argument is shown in the screenshot in Figure 7 . The Graphical Argument Editor at the lower - left enables analysts to use the Wall of Facts to construct a boxes - and - lines picture of such an argument structure . Argument structures can get quite large , so this interface is of most use when drilling down to look at a particular portion of an argument . To get a better sense of the overall structure of an argument , the Outline - based Argument Editor ( right ) indicates substructures of the argument via indentation , in effect presenting it as an outline . Supporting claims and facts are preceded by green bullets with upward pointing arrows superimposed on them , and caveats are preceded by red bullets with downward arrows . Because each claim is hyper - linked to a corresponding sentence in the product document , users are able to switch fluidly between these Argument Tree Editors and the document view provided by Microsoft Word . Hypothesis Supporting Claim Supporting Facts Rebutting Claim Figure 7 . Analysts elaborate arguments via a tree structure of claims , subclaims , and facts linked to the product text . 3 . DEPENDENCY REPOSITORY POLESTAR has been developed with a multi - tier architecture , as shown in Figure 8 . The tools described in Section 2 constitute the application tier , comprised of stand - alone user interface applications such as the Portfolio Browser , and plug - ins to third party applications , such as the Fact Collector . The Argument Engine Core is the middle tier , which also runs on the client . It enables message - based communication among client applications and provides rapid access to in - memory storage of POLESTAR objects and their dependencies . It also interfaces with the third tier , the Dependency Repository . The Dependency Repository supports the POLESTAR clients via a web application server that makes POLESTAR compatible with Service Oriented Architectures ( SOAs ) . It also provides persistent storage of POLESTAR objects and dependencies via a lightweight knowledge representation layer synchronized with a relational database . The repository has built - in support for atomic transactions with optional rollback . It has been successfully tested for scalability up to 1 , 000 users and 5 , 000 , 000 objects . Finally , it has been integrated with both lightweight open source and leading COTS industrial strength relational databases , and exploits emerging RDBMS capabilities such as fast text search . Dependency tracking repository Dependency tracking repository Argument Engine Core Stand - alone Applications Plug - in Components Shoebox Explorer , Argument Tree Editor , Wall of Facts , Qualitative Evaluator , Argument Templates Shoebox Explorer , Argument Tree Editor , Wall of Facts , Qualitative Evaluator , Argument Templates Internet Explorer Fact Collector , MS Word Fact Collector , Citation Handler , Discredited Source Manager , Peer Review Plug - in Internet Explorer Fact Collector , MS Word Fact Collector , Citation Handler , Discredited Source Manager , Peer Review Plug - in AppApp TierTier Middle Middle Tier Tier DataData Tier Tier Figure 8 . The Argument Engine Core mediates communication among POLESTAR’s client tools and the Dependency Repository . 3 . 1 Dependency View One of the guiding principles of POLESTAR is traceability back to source documents from any stage of analysis or internal object in the dependency network . This allows POLESTAR to provide affordances for calling up source documents at key points in the analytic workflow , for example , at a citation in a product document or a snippet entry in the Portfolio Browser . Another tenet of POLESTAR is transparency . By revealing to analysts the dependency network underlying their workflow , POLESTAR can provide insights into their effectiveness . For example , users can discover how much of the evidence they collect is actually used in product documents and accordingly narrow or widen their net . Users can directly examine their own dependency network of source documents , snippets , portfolios , and product documents using the Dependency View . This view is a graphical iconic representation of the network built in i2 Inc . ’s Analyst’s Notebook ® , which is widely used in the intelligence community for link analysis [ 3 ] . Here , instead of analyzing suspicious links among crime suspects , POLESTAR applies it to self - reflection of links in the analytic workflow by the analysts themselves . Figure 9 shows a simple example in which user mrogers has collected two facts from a single source document , placed them in the portfolio named korea , and cited one of them in a product document named “SIR - Al - Qaeda . doc . ” As we will see later , broadening the scope of the Dependency View to include other users can be used to help discover effective teams for collaborative analysis . Figure 9 . The Dependency View provides a global picture of a user’s analytic activity traced to primary sources . 3 . 2 Aggregate Metrics In addition to individual analyst activities , the Dependency Repository allows for reporting of aggregate metrics of analysts’ activities . Having access to the metadata for each piece of information used in the argument structure of a document enables the Qualitative Argument Evaluator to provide reports on the diversity and adequacy of sourcing . This Evaluator computes statistics , such as number of evidence snippets per claim , the temporal distribution of evidence snippets , and diversity in both sourcing and classification of source information . The resulting report highlights any of these statistics that deviates from qualitative norms for sound analysis , providing a means for analysts and managers to rapidly assess the overall quality of reports . The Evaluator is implemented as a web browser plug - in that displays analytic metrics within basic business chart graphics ( bar , pie , line , etc . ) . These graphics themselves may be easily exported into product documents to convey to policymakers the depth of arguments and variety of sources used for a major activity . The screenshot in Figure 10 shows a notional organization that is currently reliant on Saudi Intelligence for 70 % of its sources . Figure 10 . The Qualitative Argument Evaluator provides aggregate metrics on source usage and argumentation . 3 . 3 Recalled Document Notification SnippetSnippet SnippetSnippet SnippetSnippet SnippetSnippet SnippetSnippet SnippetSnippet SnippetSnippet SnippetSnippet Intel sources Finished Intel Figure 11 . POLESTAR alerts users of products and citations impacted by recalled sources . As shown in Figure 11 , the POLESTAR dependency - tracking repository records links among users , the information they manipulate , the source documents they draw from , and the finished reports they produce . POLESTAR can exploit this link structure to notify analysts of source document recalls ( e . g . from sources now considered unreliable ) and automatically call up dependent analyses to assess the impact of the recall . POLESTAR provides two levels of recall notification : affected product documents and affected claims ( citations ) within each product . For each affected item , the author may either mark the recall as leading to rejection or acceptance ( no impact ) of the item , optionally typing in an explanation of the degree of impact . These annotations are available in the permanent record of the product and eventually may be automatically transmitted to known consumers of the products to ensure policy - makers have an up - to - date understanding of the quality of the analysis . Figure 11 conceptually illustrates this process . A particular source document has been labeled as discredited . This triggers the antecedent of a rule registered in the repository’s notification engine . The consequent portion of the rule is a sequence of actions . First , the repository follows the dependency network forward to find in this case two product documents citing snippets collected from the recalled source . Then , the repository looks up the author of each product . The third and final repository action is to send a notification message to each author , which includes information on the affected product documents and citations . The POLESTAR Argument Engine Core contains a notification queue , which receives the incoming messages . At this point , POLESTAR displays a hyperlinked alert to the user , appearing as a popup window in the lower - right corner of their desktop ( see Figure 12 ) . Figure 12 . Alerting a user of a recalled source document . Clicking on the text in the popup window invokes the “Affected Products” dialog , shown in Figure 13 , which lists all products containing snippet citations stemming from the discredited source . POLESTAR can find this information because it continually logs the user’s insertion of snippets into Microsoft Word documents via drag and drop from the Portfolio Browser ( see Section 2 . 2 ) . Figure 13 . POLESTAR reveals product documents dependent on the recalled source . When the user opens an affected product document from this dialog , POLESTAR drills down one level further , listing the affected claims ( citations ) in a secondary dialog . The affected citations are also highlighted directly in the text via POLESTAR’s citation - handler plug - in to Microsoft Word ( see Figure 14 ) . The density of highlighted citations gives the analyst an immediate sense of how deeply the analysis is affected by the recalled source . The analyst may then use the Affected Claims dialog to enter a comment for each affected citation , typically to summarize the local impact of the discredited citation on the argument . These comments are automatically inserted into the document margin as Microsoft Word comment bubbles , and hence are visible to any future readers of the analysis . Finally , the user may mark both individual citations and entire analysis documents as retracted or repaired via the Retract / Repair buttons in the corresponding dialog . Figure 14 . POLESTAR highlights affected citations in previously written analysis documents . 4 . COLLABORATIVE SENSEMAKING The capabilities in the previous sections were mainly aimed at helping individual analysts make sense of evidence and write more deeply reasoned arguments . In this section we turn our discussion to collaborative argument evolution , via three new capabilities : community - of - interest finding , portfolio sharing , and peer review . 4 . 1 Community - of - Interest Formation Traditionally , POLESTAR has restricted views of dependency data to show only the current user’s activities , or to anonymously summarize aggregate metrics of analysts’ activities . This is consisted with the mainly compartmentalized approach to intelligence sharing used throughout the Cold War . Given the 9 / 11 commission’s recent mandate to the intelligence community to increase its knowledge sharing to more rapidly detect asymmetric threats from terrorist organizations or rogue states [ 5 ] , the POLESTAR dependency linkage structure may be exploited to discover useful teams for community - of - interest formation . By analyzing the common source dependencies and textual content of snippets being collected by all users , it is possible to identify users who are working on similar problems , and put them in contact with each other . For example , Drug Enforcement Agency staff working on a money laundering ring might discover that CIA analysts are interested in the same individuals because they are funding terrorist activities . As an initial proof of concept , we expanded the Dependency View to include artifacts and dependencies created not only by the current user , but also by other users to whom a direct path exists in the dependency network . Typically , this direct path flows through one or more common source documents from which both users have collected snippets . Figure 15 extends Figure 9 to include a second notional user collecting snippets from the same source document . Through this view , the user mrogers may surmise that user asmith has overlapping interests and may decide to make contact and potentially collaborate by pooling their knowledge . Figure 15 . The Dependency View reveals users that have collected snippets from common source documents . For a more advanced demonstration of this capability , we have teamed with 21 st Century Technologies to integrate POLESTAR with their Terrorist Modus Operandi Discovery System ( TMODS ) social network analyzer . POLESTAR passes its meta - data and dependency information on all users , facts , source documents , and portfolios to TMODS . Using group detection algorithms that trace the current user to other users through connections to common source documents , TMODS provides a recommendation of related users in a graphical display that also highlights non - overlapping source documents of interest from which the other users have collected facts . This interface is similar to recommendation agents popularized by amazon . com for recommending books or movies purchased by other customers with overlapping interests [ 1 ] [ 4 ] . For details on the underlying group detection algorithms used here , see [ 2 ] . Recommended source documents Recommended users and their shoeboxes , sources Figure 16 . POLESTAR uses 21 st Century Technologies’ TMODS to analyze and display related users and source documents for large - scale cases . To test the advanced community - finding capability , we developed a more in - depth scenario involving ten fictional users , each with one or more named portfolios with varying degrees of overlap . Each user has also collected between three and ten snippets from a mid - 2003 Foreign Broadcast Information Services ( FBIS ) 1 corpus of hundreds of short text documents . The “current user” in the demonstration is hmifflin , who is collecting information for a global terrorism survey . As depicted in the ground truth diagram in Figure 17 , several other users are also investigating more specific terrorism topics , including jsmith , investigating Al Qaida ; zdavis , focusing on Iraq terrorism ; rcrusoe , focusing on terrorism in Israel ; and jbond , looking at terrorism in Europe . Only the users in Group 1 form a cohesive group that includes hmifflin , where a group is defined as a set of users in which each user’s source documents overlap with at least one other user in the group . The scenario also includes a smaller group of two users investigating reaction to the Iraq War , and three “stand - alone” users that do not overlap with anyone else . jsmith aames ehunt erosenberg zdavis hmifflin rcrusoe jbond jdoe rroe Al Qaida Iraq media world reaction Al Jazirah Iraq terrorism terrorism Israel Europe Germany Greece Bosnia Africa Zimbabwe North Korea Japan Russia Iraq reaction Chechnya 1269 1227 1017 1099 1149 1201 1190 1009 Outliers : POLESTAR Community of Interest Finding Scenario6 / 22 / 2005 Group 2 Group 1 ( demo user ) ( only shared source documents shown ) Ground Truth Figure 17 . Ground truth for the advanced Community - of - Interest Finding scenario . Comparing the TMODS results in Figure 16 to the ground truth in Figure 17 , we see that the system has correctly discovered the four related users jsmith , zdavis , rcrusoe , jbond , their portfolios , and their overlapping source documents , shown in the bottom pane . This ability of the TMODS group detection to find indirectly related users like jsmith transcends the capability of the Dependency View , which was limited to finding only directly related users . In the top pane , TMODS lists the non - overlapping source documents from which these users collected snippets and highlights the path of the currently selected document to hmifflin . This path includes the other user’s containing portfolio , which can be used by hmifflin to filter for relevance . For example , doc2112 . txt is a non - overlapping document under rcrusoe’s “Israel” portfolio , so hmifflin can judge whether or not Israel is a region of interest for the global survey . 4 . 2 Portfolio Sharing Once a collaborative team has been identified , the team needs a mechanism to collaboratively marshal evidence . Previously , POLESTAR’s Portfolio Browser was designed such that facts 1 Now the Open Source Center at https : / / www . opensource . gov / were kept private to the original user who collected them . To encourage collaboration and sharing of intelligence among users , the Portfolio Browser has been extended with a new portfolio sharing capability to define named groups of users and create shared portfolios that may be viewed and modified by anyone in the group . When one adds a fact or subordinate folder to the group’s shared portfolio , the Portfolio Browser automatically updates in real - time for all other users in the group . POLESTAR also allows for embedding hyper - links to shared snippets inside text messages or peer review comments , in order to provide specific guidance on their usage . To ensure integrity of shared objects in the repository , we have extended the Argument Engine Core to allow for centralized locking , updating , and notification of changes on portfolio objects . These changes also lay the foundation for enabling collaboration in other POLESTAR tools , such as the Wall of Facts . The portfolio - sharing process begins with the creation of a new shared portfolio within the Portfolio Browser . The user is prompted to invite either a subset of registered users or a pre - defined named group of users to contribute to the new portfolio . Internally , POLESTAR then adds an association in the Dependency Repository between the shared portfolio and the chosen group of users . Finally , as with regular portfolios , the inviting user defines a name for the portfolio that is displayed next to the folder icon in the Portfolio Browser . The next time an invited user logs into POLESTAR , he or she will receive a popup notification , similar to that in Figure 12 , but with a message inviting the user to contribute to the shared portfolio . Clicking on the hyperlinked text in the popup causes POLESTAR to automatically add the shared portfolio to the Portfolio Browser under a special shared root folder . Any user in the group that accepted the invitation may drag and drop copies of their existing snippets from their private portfolios into the shared portfolio . Whenever a new snippet is added , a special rule fires in the repository . The rule consequent is to send another notification message to each user announcing the change . Figure 18 illustrates this step for user jbond , who clicks on the popup message announcing a new snippet has been added to the Global Terrorism shared portfolio . This causes POLESTAR to automatically update the contents of that portfolio and browse to it in the Portfolio Browser . Germany : Arrest Warrant for Suspected Terrorist Issued Unclassified Figure 18 . A user receives a notification of an update to a shared portfolio . It is important to note that if multiple users in a group are logged into POLESTAR simultaneously , these notifications occur in real time , much like familiar collaborative chat or whiteboard tools such as Microsoft’s Groove or AOL’s Instant Messenger . Also , any updates made to a snippet’s metadata within the Fact Card will be broadcast to the other users and displayable in the Portfolio Browser or Fact Card . 4 . 3 Peer Review Current practice for coordination of review of analytic products entails sending out a draft via email and merging the resulting comments , which come back in multiple formats . Some reviewers send comments in email , whereas others send back the document marked up , either directly in the text or via Microsoft Word’s Track Changes feature . In POLESTAR , as the final stage in the collaborative argument evolution process , we have developed a unified interface for reviewers to attach comments and rebuttals to the document text itself . The author has a separate comment summary interface for stepping through and addressing each of these comments in turn . Managers can readily check that all rebuttals have been resolved prior to report distribution . POLESTAR maintains a record of all comments and changes to the document over time , enabling analysts to reconstruct their thinking at any point in time . POLESTAR’s peer review plug - in is integrated with Microsoft Word , allowing reviewers to rapidly make in - line comments close to the actual text . In addition , POLESTAR allows multiple reviewers to work in parallel , with comments automatically merged into the author’s view as they are submitted . Peer review is implemented as a Microsoft Word plug - in whose actions are available through a new Collaboration Toolbar that appears in Word . Like portfolio sharing , the initial step of the peer review process is to invite a group of users , with an extra optional step of providing guidance to the reviewers in a textfield . Figure 19 . A user invites peers to review a product document . Later , when one of the invited reviewers logs in , POLESTAR presents another popup invitation message similar to that of a shared portfolio . However , in this case , accepting the invitation causes POLESTAR to open a reviewer - private copy of the product document in Microsoft Word . The reviewer may select any subset of text in Word and invoke a new POLESTAR right - click menu action , “Enter POLESTAR Comment . ” This in turn invokes a special POLESTAR comment dialog pointing to the selected text . The dialog provides a text field to enter specific feedback as well as a choice box specifying the type of feedback . This type can be one of the following : • Agreement • Request for Clarification • Supporting Evidence Suggestion • Rebuttal • Other After entering each comment , it is added to the document margin as a Microsoft Word comment bubble . Figure 20 . A reviewer enters categorized feedback that later appears in margin comment bubbles . When the reviewer is finished entering comments , he or she clicks the Submit Feedback button in the Collaboration Toolbar . Because each reviewer works with a separate copy of the document , an arbitrary number of reviewers may enter feedback in parallel . This is a distinct advantage over certain COTS groupware and document management systems that enforce a sequential chain of review , leading to a lengthy review process prone to bottlenecks . In addition , many of these knowledge management portals are limited to external annotation methods in which comments are entered and displayed in a web - based form or table far removed from the corresponding document text . By wrapping around Word’s embedded comment feature , reviewers can enter and view their comments in - line at the point of the relevant text , with the entire document itself available for further context . Next we turn our attention back to the author . For each reviewer who submitted feedback , the author receives a popup notification . Clicking on the latest popup will cause POLESTAR to automatically merge the comments submitted by all reviewers so far and open the original document with these merged comments shown in the margin . Furthermore , POLESTAR provides a Feedback Summary window showing a consolidated tabular summary of comments received to date ( see Figure 21 ) . For each comment , the table shows the originating reviewer , timestamp , comment type , and comment text . The window becomes semi - transparent when other windows are moved in front of it , so that the author may easily read both the document in Microsoft Word and the consolidated summary without sacrificing screen real estate . Figure 21 . The author views consolidated feedback from all reviewers to date . 5 . FUTURE WORK The POLESTAR development team is considering several short - term enhancements and new capabilities for Collaborative Argument Evolution : • Extend the Community - of - Interest Formation capability to account for similar topics among document sources . Wang et al’s Group - Topic model [ 9 ] is an attractive algorithm in that it simultaneously discovers groups and topics , accounting for mutual information in both dimensions . • Extend the Peer Review process to allow annotations directly on the Argument Tree Editor . • Enable real - time collaborative information sharing and editing within other views besides the Portfolio Browser , such as the Argument Tree Editor , Wall of Facts , or Timeline . Next year and beyond , POLESTAR is planning to research advanced search , analysis , and reporting capabilities that operate on the aggregate recorded collaborative activity within an agency , including : • A restricted natural language dependency query interface with hyperlinked results shown in a web browser , with drilldown to show metadata and follow dependency links . • Automated agents that find relevant information in source documents or portfolios and dynamically update their recommendations for collaboration . • New management views similar to the Qualitative Argument Evaluator that summarize information sharing activities , including shared portfolios and peer reviews . • Visualizations that summarize how collaborative groups and salient topics in analytic products evolve over time [ 8 ] . In addition , we plan to integrate POLESTAR with an existing BAE Systems testbed of intelligence link discovery and visualization tools , described in [ 6 ] . 6 . CONCLUSIONS We have presented POLESTAR , a suite of tools for collaborative sensemaking and knowledge management to support the intelligence analysis workflow . POLESTAR’s principles of transparent source traceability and seamless integration with existing desktop editors and have led to early transition of selected capabilities to a government agency , where the currently operational system now supports well over 200 users . The key benefit of the system is the automation of metadata handling in the workflow , allowing users to spend more of their time on task and less on administrative overhead . Users are vocally enthusiastic about the system , saying that it saves them hours of time in the course of their work week . POLESTAR’s recent collaborative analysis capabilities show strong potential to foster ad - hoc team formation and information sharing within and across intelligence agencies . We plan to deliver an information - sharing version of our deployed system later this year , even as we continue to evolve our research agenda toward more advanced collaborative search , analysis , and reporting capabilities . 7 . ACKNOWLEDGMENTS This material is based upon work supported by the United States Air Force Research Laboratory under Contract No . F30602 - 03 - C - 0005 . Any opinions , findings and conclusions or recommendations expressed in this material are those of the author ( s ) and do not necessarily reflect the views of the United States Air Force . 8 . REFERENCES [ 1 ] Chen , A . , and McLeod , D . , Collaborative Filtering for Information Recommendation Systems . Encyclopedia of Data Warehousing and Mining , Idea Group , 2005 . [ 2 ] Coffman , T . , Greenblatt , S . , and Marcus , S . Graph - based technologies for intelligence analysis . Communications of the ACM , 47 , 3 ( Mar . 2004 ) , 45 - 47 . [ 3 ] http : / / www . i2inc . com / Products / Analysts _ Notebook / default . a sp , http : / / www . cnn . com / 2003 / TECH / ptech / 12 / 23 / tracking . rebels . ap / index . html , http : / / www . i2inc . com / Company / Press / 040306 . asp . [ 4 ] Linden , G . , Smith , B . , & York , J . ( 2003 ) . Amazon . com recommendations . IEEE Internet Computing , 7 , 1 , ( Jan . - Feb . 2003 ) , 76 - 80 . [ 5 ] National Commission on Terrorist Attacks . The 9 / 11 Commission Report : Final Report of the National Commission on Terrorist Attacks Upon the United States , W . W . Norton & Company , New York , NY , 2004 . [ 6 ] Pioch , N . , Barlos , F . , Fournelle , C . and Stephenson , T . A Link and Group Analysis Toolkit ( LGAT ) for Intelligence Analysis . Int’l Conference on Intelligence Analysis , 2005 . [ 7 ] Toulmin , S . The Uses of Argument . Cambridge University Press , 1958 . [ 8 ] Wang , X . and McCallum , A . Topics over Time : A Non - Markov Continuous - Time Model of Topical Trends . Draft submitted to Conference on Knowledge Discovery and Data Mining ( KDD ) , 2006 . [ 9 ] Wang , X . , Mohanty N . , and McCallum , A . Group and Topic Discovery from Relations and Text . KDD Workshop on Link Discovery : Issues , Approaches and Applications ( LinkKDD ) , 2005 . [ 10 ] Wigmore , J . H . The Science of Judicial Proof : as Given by Logic , Psychology , and General Experience and Illustrated in Judicial Trials . Little , Brown , 1937 .