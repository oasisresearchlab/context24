Computational reproducibility of Jupyter notebooks from biomedical publications Computational reproducibility of Jupyter notebooks from biomedical publications Sheeba Samuel 1 , 2 * † and Daniel Mietchen 3 , 4 , 5 * † * For correspondence : sheeba . samuel @ uni - jena . de ( SS ) ; daniel . mietchen @ ronininstitute . org ( DM ) † These authors contributed equally to this work 1 Heinz - Nixdorf Chair for Distributed Information Systems , Friedrich Schiller University Jena , Germany ; 2 Michael Stifel Center Jena , Germany ; 3 Ronin Institute , Montclair , New Jersey , United States ; 4 Institute for Globally Distributed Open Research and Education ( IGDORE ) ; 5 FIZ Karlsruhe — Leibniz Institute for Information Infrastructure , Berlin , Germany Abstract Jupyter notebooks allow to bundle executable code with its documentation and output in one interactive environment , and they represent a popular mechanism to document and share computational workﬂows , including for research publications . Here , we analyze the computational reproducibility of 9625 Jupyter notebooks from 1117 GitHub repositories associated with 1419 publications indexed in the biomedical literature repository PubMed Central . 8160 of these were written in Python , including 4169 that had their dependencies declared in standard requirement ﬁles and that we attempted to re - run automatically . For 2684 of these , all declared dependencies could be installed successfully , and we re - ran them to assess reproducibility . Of these , 396 notebooks ran through without any errors , including 245 that produced results identical to those reported in the original . Running the other notebooks resulted in exceptions . We zoom in on common problems and practices , highlight trends and discuss potential improvements to Jupyter - related workﬂows associated with biomedical publications . 1 of 31 a r X i v : 2209 . 04308v1 [ c s . C E ] 9 S e p 2022 Computational reproducibility of Jupyter notebooks from biomedical publications Contents Introduction 4 Reproducibility issues in contemporary research . . . . . . . . . . . . . . . . . . . . . . . . 4 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Computational reproducibility in biomedical research . . . . . . . . . . . . . . . . . . . . . 4 PubMed Central . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 GitHub . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Jupyter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Jupyter and reproducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Wikidata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Environmental footprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Methods 6 Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Reproduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Results 9 General statistics of our study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Programming languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Versions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Notebook structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Notebook naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Notebook modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Notebook dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Notebook Reproducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Successful replications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Notebook Styling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Discussion 23 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Conclusions 26 Data availability 26 Conﬂict of interest 26 Ethics 26 Acknowledgements 26 List of Figures 1 Workﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2 Journals with the highest number of articles that had a valid GitHub repository and at least one Jupyter notebook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3 Journals by the number of GitHub repositories and by the number of GitHub reposi - tories with at least one Jupyter notebook . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4 Journals by number of GitHub repositories with Jupyter notebooks . . . . . . . . . . . 11 5 Articles by number of GitHub repositories with at least one Jupyter notebook by year . 11 6 ORCID usage in our collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 7 Programming languages of the notebooks . . . . . . . . . . . . . . . . . . . . . . . . . . 12 8 Relative proportion of the most frequent programming languages used in the note - books per year . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 9 Python notebooks by minor Python version by year of last commit to the GitHub repository . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 10 Python notebooks by major Python version by year of ﬁrst commit . . . . . . . . . . . 14 11 Analysis of the notebook structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 12 Most frequent notebook titles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 13 Notebooks with the string test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 14 Notebook title length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 15 Top Python modules declared in Jupyter Notebooks . . . . . . . . . . . . . . . . . . . 17 16 Load extension modules in Jupyter Notebooks . . . . . . . . . . . . . . . . . . . . . . 17 17 Dependencies of Juypter Notebooks and GitHub repositories . . . . . . . . . . . . . . 18 18 Exceptions occurring in Jupyter Notebooks . . . . . . . . . . . . . . . . . . . . . . . . . 19 19 ModuleNotFoundError , ImportError and FileNotFoundError exceptions by year of publication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 20 Exceptions by year of publication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 21 Exceptions by article type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 22 Exceptions by journal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 23 Frequent notebook code style errors as per the Python code style guide . . . . . . . 23 List of Tables 1 Comparison of notebooks that were successfully executed without errors . . . . . . 21 2 Comparison of most frequent Python versions declared for notebooks that were suc - cessfully executed without errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3 Common Python Notebook Code Warning / Style Error found in our Study . . . . . . 24 3 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Introduction Many factors contribute to the progress of scientiﬁc research , including the precision , scale , and speed at which research can be performed and shared and the degree to which research processes and their outcomes can be trusted ( Siebert et al . , 2015 ; Contera , 2021 ) . This trust , in turn , and the credibility that comes with it , are a social construct that depends on past experience or proxies to it ( Gray et al . , 2012 ; Kroeger et al . , 2018 ; Jamieson et al . , 2019 ) . A good proxy here is reproducibility , at least in principle ( Hsieh et al . , 2018 ) : if a study addressing a particular research question can be re - analyzed and that analysis leads to the same conclusions as the original study , then these conclusions can generally be more trusted than if the conclusions diﬀer between the original and the replication study . Reproducibility issues in contemporary research Over recent years , the practical replicability of published research has come into focus and turned into a research area in and of itself ( Peng , 2015 ; Samuel and König - Ries , 2021 ) . As a result , sys - tematic issues with reproducibility have been the subject of many publications in various research ﬁelds as well as prominent mentions in the mass media ( The Economist , 2013 ) . These research ﬁelds range from psychology ( Simmons et al . , 2011 ) to cell culture ( Hussain et al . , 2013 ; Bairoch , 2018 ) to ecology ( Kelly , 2019 ) , geosciences ( Ledermann and Gartner , 2021 ) and beyond and include software - aﬃne domains such as health informatics ( Coiera et al . , 2018 ) , human - computer inter - actions ( Hinsen , 2018 ) , artiﬁcial intelligence ( Hutson , 2018 ) , software engineering ( Shepperd et al . , 2018 ) and research software ( Crick et al . , 2017 ) . This is often framed in terms of a “reproducibility crisis” ( Baker , 2016 ) , though that may not necessarily be the most productive approach to address - ing the underlying issues ( Hunter , 2017 ; Fanelli , 2018 ; Guttinger , 2020 ) . In more practical terms , Näpﬂin et al . ( 2019 ) observe that “appropriate workﬂow documentation is essential” . Terminology Within this broader context , distinctions between replicability , reproducibility , and repeatability are often important or even necessary ( Meng , 2020 ) but not consistently made in the literature ( Plesser , 2017 ) . A potential solution to this confusion is the proposed distinction ( Goodman et al . , 2016 ) be - tween Methods reproducibility ( providing enough detail about the original study that the procedures and data can be repeated exactly ) , Results reproducibility ( obtaining the same results when match - ing the original procedures and data as closely as possible ) and Inferential reproducibility ( leading to the same scientiﬁc conclusions as the original study , either by reanalysis or by independent replication ) . In the following , we will concentrate on “Methods reproducibility in computational research” , i . e . using the same code on the same data source . For this , we will use the shorthand “Computational reproducibility” . In doing so , we are conscious that the “same code” can yield diﬀerent results depending on the execution environment and that the “same data source” might actually mean diﬀerent data if the data source is dynamic or if the code involves manipulating the data in a way that changes over time . We are also aware that the shorthand “Computational reproducibility” can also be applied , e . g . , to “Results reproducibility in computational research” in cases where the algorithm described for the original study was re - implemented in a follow - up study . For instance , Burlingame et al . ( 2021 ) were striving for Results reproducibility when they re - implemented the PhenoGraph algorithm – which originally only ran on CPUs – such that it could be run on GPUs and thus at higher speed . However , Results reproducibility is not the focus of our study . Computational reproducibility in biomedical research In light of the reproducibility issues outlined above , there have been calls for better standardiza - tion of biomedical research software – see Russell et al . ( 2018 ) for an example . In line with such standardization calls , a number of guidelines or principles to achieve methods reproducibility in several computational research contexts have been proposed . For instance , Sandve et al . ( 2013 ) , 4 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Gil et al . ( 2016 ) and Willcox ( 2021 ) laid out principles for reproducible computational research in general . In a similar vein , Grüning et al . ( 2018 ) and Brito et al . ( 2020 ) looked at speciﬁcs of com - putational reproducibility in the life sciences , Nüst et al . ( 2020 ) explored the use of Docker – a containerization tool – in reproducibility contexts , and Trisovic et al . ( 2022 ) looked at the repro - ducibility of R scripts archived in an institutional repository , while Rule et al . ( 2019 ) , Pimentel et al . ( 2019 ) as well as Wang et al . ( 2020a ) ; Willis et al . ( 2020 ) and Wang et al . ( 2020b ) zoomed in on Jupyter notebooks , a popular ﬁle format for documenting and sharing code . While most of these are language agnostic , language - speciﬁc approaches to computational reproducibility have also been outlined , e . g . for Python ( Halchenko et al . , 2021 ) . However , compliance with such standards and guidelines is not a given ( Russell et al . , 2018 ; Rule et al . , 2018 ; Pimentel et al . , 2021 ) , so we set out to measure it speciﬁcally for Jupyter notebooks in the life sciences and to explore options to bridge the gap between recommended and actual practice . In order to do so , we mined a popular repository of biomedical fulltexts ( PubMed Central ) for mentions of Jupyter notebooks alongside mentions of a popular repository for open - source software ( GitHub ) . PubMed Central PubMed Central ( PMC ) 1 is a literature repository containing full texts of biomedical articles . At the time of writing , it contained about 7 . 5 million articles . Founded in the context of the Open Access mandate issued by the National Institutes of Health ( NIH ) in the United States ( Roberts , 2001 ) , PMC is operated by the National Center for Biotechnology Information ( NCBI ) , a branch of the National Library of Medicine ( NLM ) , which is part of the NIH . PMC hosts the articles using the Journal Article Tagging Suite ( JATS ) , an XML standard , and makes them available for manual and programmatic access in various ways , of which we used the Entrez API ( Sayers , 2010 ) . GitHub GitHub 2 is a website that combines git - based version control with support for collaboration and automation . It is a popular place for sharing software and developing it collaboratively , including for Jupyter notebooks ( Rule et al . , 2018 ) and for code associated with research articles available through PubMed Central ( Russell et al . , 2018 ) . Jupyter Jupyter notebooks 3 ( Kluyver et al . , 2016 ; Granger and Perez , 2021 ) are a computing environment in which code , code documentation , and output of the code can be explored interactively . They have become a popular mechanism to share computational workﬂows in a variety of ﬁelds ( Kluyver et al . , 2016 ) , including astronomy ( Randles et al . , 2017 ; Woﬀord et al . , 2019 ) and biosciences ( Schröder et al . , 2019 ) . Here , we build on past studies of the reproducibility of Jupyter notebooks ( Rule et al . , 2018 ; Pimentel et al . , 2019 ) and analyze Jupyter notebooks available through GitHub repositories associated with publications available through the biomedical literature repository PubMed Central . Jupyter and reproducibility Jupyter notebooks can , in principle , be used to enhance reproducibility , and they are often pre - sented as such , yet using them does not automatically confer reproducibility to the code they con - tain . Several studies have been conducted in recent years to explore the reproducibility of Jupyter Notebooks . A recent one has investigated the reproducibility of Jupyter notebooks associated with ﬁve publications from the PubMed Central database ( Schröder et al . , 2019 ) . In their reproducibil - ity analysis , they looked for the presence of notebooks , source code artifacts , documentation of 1 https : / / www . ncbi . nlm . nih . gov / pmc / 2 https : / / github . com / 3 https : / / jupyter . org / 5 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications the software requirements , and whether the notebooks can be re - executed with the same results . According to their results , the authors successfully reproduced only three of 22 notebooks from ﬁve publications . Rule et al . ( Rule et al . , 2018 ) explored 1 million notebooks available on GitHub . In their study , they explored repositories , language , packages , notebook length , and execution or - der , focusing on on the structure and formatting of computational notebooks . As a result , they provided ten best practices to follow when writing and sharing computational analyses in Jupyter Notebooks ( Rule et al . , 2019 ) . Another study ( Pimentel et al . , 2021 ) focused on the reproducibility of 1 . 4 million notebooks collected from GitHub . It provides an extensive analysis of the factors that impact reproducibility based on Jupyter notebooks . Chattopadhyay et al . ( Chattopadhyay et al . , 2020 ) reported on the results of a survey conducted among 156 data scientists on the diﬃ - culties when working with notebooks . Other studies focus on best practices on writing and sharing Jupyter notebooks ( Rule et al . , 2019 ; Pimentel et al . , 2021 ; Willis et al . , 2020 ; Wang et al . , 2020b ) . As a result , tools have been developed to support provenance and reproducibility in Jupyter Note - books ( Chirigati et al . , 2013 ; Boettiger , 2015 ; Samuel and König - Ries , 2018 ; Project Jupyter et al . , 2018 ) . Cases where Jupyter notebooks have played a key role in some actual replication attempts have also begun to appear in the literature . For instance , Baker et al . ( 2019 ) assembled a Jupyter notebook as part of a published correction . Shortly after we had created our corpus , a paper was published with a Jupyter notebook that enabled others to reproduce the computational workﬂows , ultimately leading to the retraction of the original work , as detailed in Meyerowitz - Katz et al . ( 2021 ) . Wikidata Wikidata is a cross - disciplinary and multilingual database through which a global community cu - rates FAIR and open data to serve as general reference information ( Waagmeester et al . , 2020 ; Rutz et al . , 2022 ) . This includes information about key elements of the research ecosystem , from researchers to research ﬁelds and research organizations , from methods to datasets , software and publications . This information can then be explored in various ways , e . g . through the visu - alization tool Scholia ( Nielsen et al . , 2017 ) , which provides proﬁles for diﬀerent types of entities or relationships . For entities of the type Jupyter notebook ( known to Wikidata as Q70357595 ) , the most relevant proﬁle types are those for a topic 4 , a software 5 or a resource used 6 . Environmental footprint Computations ultimately require physical resources , and both the production and the use of these resources can have a considerable environmental footprint ( Lannelongue et al . , 2021b ) . The more reproducible some workﬂows become , the more accurate their environmental footprint can be as - sessed ( Taddeo et al . , 2021 ) . This can then lead to an optimization of the environmental footprint , especially since it often correlates with the ﬁnancial footprint of using computational resources ( Schwartz et al . , 2020 ) . One of our aims in this study is thus to get an overview of the contribu - tion of Jupyter - based workﬂows to the environmental footprint of biomedical research involving computation . This is in line with the recommendation in Lannelongue et al . ( 2021a ) to integrate routine environmental footprint assessment into research practice . Methods Pipeline In this section , we describe the key steps of the pipeline we used for assessing the reproducibility of Jupyter notebooks associated with publications extracted from PubMed Central . The driver ﬁle for running the workﬂow is documented in r0 _ main . py and the driver notebook for the analysis of the collected data is documented in the notebook named “Index . ipynb” . Figure 1 provides an overview of the workﬂow used in this study . 4 https : / / scholia . toolforge . org / topic / Q70357595 5 https : / / scholia . toolforge . org / software / Q70357595 6 https : / / scholia . toolforge . org / use / Q70357595 6 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Search in PMC ' ( ipynb OR jupyter OR ipython ) AND github ' Collect Publication Metadata using NCBI Entrez via Biopython in XML format Extract Publication Metadata Extract GitHub Links Store Metadata in SQLite Database Check code availability on GitHub Clone repository if available Collect execution environment information of the repository Prepare conda environment Install dependencies inside Conda environment based on the requirements Collect Jupyter Notebooks of the repositories Run and reproduce notebooks Compute diff of notebooks using nbdime library Store the reproducibility results of notebooks Check Python Code Styling using flakenb library Analyze the reproducibility results of notebooks Figure 1 . Fully automated workﬂow used for assessing the reproducibility of Jupyter notebooks from publications indexed in PubMed Central : the PMC search query resulted in a list of article identiﬁers that were then used to retrieve the full - text XML , from which publication metadata and GitHub links were extracted and entered into an SQLite database . If the links pointed to valid GitHub repositories containing valid Jupyter notebooks , then metadata about these were gathered , and the Python - based notebooks were run with all identiﬁable dependencies , and their results analyzed with respect to the originally reported ones . We used the esearch function to search PMC for Jupyter notebooks on 24 th February , 2021 . We looked for publications that mentioned GitHub together with either the string “Jupyter” or some closely associated ones , namely “ipynb” ( the ﬁle ending / extension of Jupyter notebooks ) or “iPython” ( the name of a precursor to Jupyter ) . The search query used was “ ( ipynb OR jupyter OR ipython ) AND github” . Based on the primary PMC IDs received from the esearch utility , we retrieved records in the XML format using the efetch function and collected the publication metadata from PMC ( Roberts , 2001 ) using NCBI Entrez utilities via Biopython ( Cock et al . , 2009 ) . In the next step , we processed the XML fetched from PMC . We used an SQLite database 7 for storing all the data related to our pipeline . We collected information on journals and articles . We ﬁrst extracted information about the journal . For this , we created a database table for the journal and extracted the ISSN 8 ( International Identiﬁer for serials ) , the journal title , the NLM’s ( National Library of Medicine ) abbreviated journal title , and the ISO 9 ( International Organization for Stan - dardization ) abbreviation . We then created a database table for the articles and populated it with article metadata . The metadata includes the article name , Pubmed ID , PMC ID , Publisher id and name , DOI , subject , the dates when the article was received , accepted , and published , the license , the copyright statement , keywords , and the GitHub repositories mentioned in the publication . For each article , we also extracted the Medical Subject Headings ( MeSH terms ) 10 to get the subject area of the article . To extract the GitHub repositories mentioned in each article , we looked for mentions of GitHub links anywhere in the article , including the abstract , the article body , data availability statement , and supplementary information . GitHub links were available in diﬀerent formats . We normalized them to the standard format ’https : / / github . com / { username } / { repositoryname } ’ . For example , we extracted the GitHub repository from nbviewer 11 links and transformed its representation to the 7 https : / / www . sqlite . org 8 https : / / www . issn . org / 9 https : / / www . iso . org 10 https : / / www . ncbi . nlm . nih . gov / mesh 11 https : / / nbviewer . org / 7 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications standard format . We excluded 692 GitHub links that mentioned only the username or organization name or github pages and not a speciﬁc repository name . After preprocessing and extracting GitHub links from each article , we added the GitHub repositories to the database table for the corresponding articles . Likewise , we linked the article’s entry in the table to the journal where it was published . We also collected information on the authors of the article in a separate database table : we created an author database table , extracted the ﬁrst and last name , ORCID , email , and connected these data to the corresponding entries in the article table . Based on the GitHub repository name collected from the article , we checked whether these repositories were available at the original link or not . If the repository existed , we cloned it ( ignor - ing branches , i . e . just taking the base one , which is usually called “main” ) and collected informa - tion about the repositories using the GitHub REST API 12 . On that basis , we created a repository database table . For each GitHub repository , an entry is created in the table and connected to the article where it is mentioned . We collected the execution environment information by looking into the dependency information declared in the repositories in terms of ﬁles like requirements . txt , setup . py and pipﬁle . Additional information for each repository is also collected from the GitHub API . This includes the dates of the creation , updates , or pushes to the repository , and the program - ming languages used in each repository . Further information includes the number of subscribers , forks , issues , downloads , license name and type , total releases , and total commits after the respec - tive dates for when the article was published , accepted , and received . After collecting and creating these data tables , we ran a pipeline to collect the Jupyter notebooks contained in the GitHub repos - itories . The code for the pipeline is adapted from ( Pimentel et al . , 2019 ; Samuel and König - Ries , 2021 ) . Hence , the method to reproduce the notebooks in this study is similar to ( Pimentel et al . , 2019 ) . For each notebook , we collected information on the name , nbformat , kernel , language , num - ber of diﬀerent types of cells , and the maximum execution count number . We extracted the source and output of each cell for further analysis . Using Python Abstract Syntax Tree ( AST ) 13 the pipeline extracted information on the use of modules , functions , classes , and imports . After collecting all the required information for the execution of Python notebooks from the repositories , we prepared a Conda 14 environment based on the python version declared in the notebook . Conda is an open source package and environment management system which helps users to easily ﬁnd and install packages and create , save , load and switch between environments . The pipeline then installed all the dependencies collected from the corresponding ﬁles like require - ments . txt , setup . py and pipﬁle inside the Conda environment . For the repositories that did not pro - vide any dependencies using the above mentioned ﬁles , the pipeline executed the notebooks by installing all the anaconda dependencies 15 . Anaconda is a Python and R distribution which pro - vides data science packages including scikit - learn , numpy , matplotlib , and pandas . We used the nbdime 16 library from Project Jupyter to compute diﬀs of the notebooks . We used the tools adapted from ( Pimentel et al . , 2019 ; Samuel and König - Ries , 2021 ) . The code from ( Pi - mentel et al . , 2019 ) provides a basis for reproducing Jupyter notebooks from GitHub repositories . The ReproduceMeGit ( Samuel and König - Ries , 2021 ) extended from ( Pimentel et al . , 2019 ) , is a visualization tool for analyzing the reproducibility of Jupyter Notebooks , along with provenance information of the execution . ReproduceMeGit provides the diﬀerence between the results of the executions of notebooks using the nbdime library . These two tools provide the basis for our code for the reproducibility study . After collecting the notebooks , we also ran a Python code styling check using the ﬂakenb 17 library on the notebooks , since code styling consistency is a potential indicator for the extent of care that went into a given piece of software . The ﬂakenb library is a tool for code style guide 12 https : / / docs . github . com / en / rest / guides / getting - started - with - the - rest - api 13 https : / / docs . python . org / 3 / library / ast . html 14 https : / / docs . conda . io / en / latest / 15 https : / / docs . anaconda . com / anaconda / packages / pkg - docs / 16 https : / / github . com / jupyter / nbdime 17 https : / / github . com / s - weigand / ﬂake8 - nb 8 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications enforcement for notebooks . It helps to check code against some of the style conventions in PEP 8 18 , a style guide for Python code . The ﬂakenb library provides an ignore ﬂag to ignore some speciﬁed errors . In this study , we did not use this ﬂag and collected all errors detected by the library . For the styling of notebooks , we collected information on the pycode styling error code and description 19 . Reproduction The complete pipeline was run on the Friedrich Schiller University Ara Cluster 20 . The computational experiments were performed on a Skylake Standard Node ( 2x Intel Xeon Gold 6140 18 Core 2 , 3 GHz , 192 GB RAM ) . This node has two CPUs , each with 18 cores , and 192 GB RAM in total . The complete pipeline ran in 117 hours and 52 minutes from 24 th - 28 th February 2021 . We then used the website https : / / green - algorithms . org v2 . 2 Lannelongue et al . ( 2021b ) to estimate that the pipeline run drew 47 . 38 kWh . Based in Germany , this has a carbon footprint of 16 . 05 kg CO2e , which is equivalent to 17 . 51 tree - months . Results In this section , we present the results of our study on analyzing the computational reproducibility of Jupyter notebooks from biomedical publications . We extracted metadata from 1419 publications from PubMed Central . These articles had been published in 373 journals and had 2398 mentions of GitHub repository links . At the time of data collection , 49 GitHub repositories mentioned in the articles were not accessible , returning a “page not found” error instead . Out of 2177 unique and valid GitHub repositories cloned , only 1117 had one or more Jupyter notebooks . From these repos - itories , a total of 9625 Jupyter notebooks were downloaded for further reproducibility analysis . General statistics of our study 81 68 64 59 52 49 38 37 34 34 e L i f e P L o S O n e P L o S C o m p u t B i o l N a t C o m m u n B i o i n f o r m a t i c s S c i R e p F r o n t N e u r o i n f o r m S c i D a t a G i g a s c i e n c e F 1 0 0 0 R e s 0 10 20 30 40 50 60 70 80 Journal N u m b e r o f a r t i c l e s Figure 2 . Journals with the highest number of articles that had a valid GitHub repository and at least one Jupyter notebook . In the ﬁgures , journal names are styled as in the XML ﬁles we parsed , e . g . ( “PLoS Comput Biol” ) . In the text , we use the full name in its current styling , e . g . “PLOS Computational Biology” . 18 https : / / www . python . org / dev / peps / pep - 0008 / 19 https : / / pycodestyle . pycqa . org / en / latest / intro . html 20 https : / / wiki . uni - jena . de / pages / viewpage . action ? pageId = 22453005 9 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 202 130 127 120 102 94 87 84 68 56 137 68 68 58 30 33 48 31 36 37 e L i f e P L o S C o m p u t B i o l P L o S O n e N a t C o m m u n G i g a s c i e n c e B M C B i o i n f o r m a t i c s S c i D a t a F r o n t N e u r o i n f o r m B i o i n f o r m a t i c s S c i R e p 40 60 80 100 120 140 160 180200220 Repositories Repositories with notebooks Journal N u m b e r o f r e p o s i t o r i e s Figure 3 . Journals by the number of GitHub repositories and by the number of GitHub repositories with at least one Jupyter notebook . Figure 2 shows the top ten journals with the highest number of articles that had a valid GitHub repository with at least one Jupyter notebook . Figure 3 shows the journals by the number of GitHub repositories and repositories with Jupyter notebooks . The journal eLife topped the list in both the rankings , which is why we chose to submit our manuscript there . It was followed by PLOS ONE and PLOS Computational Biology . The ratio of notebooks per GitHub repository varies across jour - nals , with the range being between 3 . 4 : 1 in GigaScience , 2 : 1 in Nature Communications , and 1 . 5 : 1 in Scientiﬁc Reports . From the 1117 repositories with Jupyter notebooks , 290 ( 25 . 9 % ) of reposito - ries had one Jupyter notebook , 462 ( 41 . 4 % ) had two notebooks , and 249 ( 22 . 3 % ) had ten or more notebooks . 6 , 782 ( 70 . 4 % ) of the notebooks belonged to repositories with ten or more notebooks . Figure 5 shows the maximum number of notebooks for articles published in the respective journal . Among the top ten journals with notebooks , Nature Communications had the maximum number of notebooks ; however , it ranked fourth in terms of journals with repositories with note - books . Figure 5 shows the timeline of the articles by the number of Github repositories with at least one Jupyter notebook . This indicates a growing trend of articles with notebooks . In parallel to exploring trends related to Jupyter notebooks , we analyzed the uptake of ORCID iden - tiﬁers 21 over time in the collected journal articles with notebooks ( Figure 6 ) . ORCID provides a per - sistent digital identiﬁer to uniquely identify authors and contributors of scholarly articles . While iPython notebooks go back to 2001 , the Jupyter notebooks with kernels for multiple languages be - came available in 2014 , whereas ORCID was launched in 2012 . Hence , both are relatively recent innovations in the scholarly communications ecosystem , and their respective uptake processes occur in parallel . There are in total 11594 authors in the 1419 publications . We have not performed any author disambiguation to distinguish unique authors in our corpus . However , such disambiguation is taking place at scale in Wikidata ( see Discussion ) . There are 2 , 720 ( 23 . 46 % ) authors with ORCID 21 https : / / orcid . org / 10 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 137 68 68 58 30 33 48 31 36 37 e L i f e P L o S C o m p u t B i o l P L o S O n e N a t C o m m u n G i g a s c i e n c e B M C B i o i n f o r m a t i c s S c i D a t a F r o n t N e u r o i n f o r m B i o i n f o r m a t i c s S c i R e p 30 40 50 60 70 80 90 100 110 120130140 50 100 150 200 250 Notebook count Journal R e p o s i t o r i e s w i t h n o t e b oo k s Figure 4 . Journals by number of GitHub repositories with Jupyter notebooks . For each journal , the notebook count gives the maximum number of notebooks within a repository associated with an article published in the journal . 770 585 325 184 94 84 50 34 13 419 287 156 100 48 46 28 10 5 2 0 1 4 2 0 1 6 2 0 1 8 2 0 2 0 0 100 200 300 400 500 600 700 800 Articles Articles with notebooks Published year N u m b e r o f a r t i c l e s Figure 5 . Articles by number of GitHub repositories with at least one Jupyter notebook by year . and 8 , 874 ( 76 . 54 % ) without . In 2020 , there were 3545 mentions of author ORCIDs and the highest number of articles ( 419 ) with notebooks . The ﬁgure shows an increase in the usage of ORCID as well as articles with notebooks from 2016 . Programming languages Figure 7 and Figure 8 show analyses of the programming languages used in the Jupyter notebooks present in the collected publications . Figure 7 presents ( using a log scale ) the most common pro - gramming languages used in the notebooks . Python ( 84 . 8 % ) is the most common programming 11 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 3545 2776 1701 1706 448 710 361 175 38 2 0 1 4 2 0 1 6 2 0 1 8 2 0 2 0 0 500 1000 1500 2000 2500 3000 3500 50 100 150 200 250 300 350 400 Articles with notebooks Published year O R C I D Figure 6 . ORCID usage in our collection . Bars indicate the total number of ORCIDs found each year for authors of articles in our collection . Colors indicate the number of articles that year with Jupyter notebooks . Note that data for 2021 is incomplete , as only articles published by mid - February have been included . 8160 720 461 95 59 29 24 24 9 8 P y t h o n U n k n o w n R G r o o v y J u li a S c a l a B a s h S o s M a t l a b J a v a 10 2 5 100 2 5 1000 2 5 10k Programming language N u m b e r o f n o t e b oo k s Figure 7 . Programming languages of the notebooks . “Unknown” means the language kernel used was not indicated in a standardized fashion . language , followed by unknown ( 7 . 5 % ) and R ( 4 . 8 % ) . Unknown notebooks are those which do not declare the programming language or its version in the notebook . A total of 720 notebooks do not declare a programming language . From the ﬁgure , we can see that the Jupyter ecosystem is not just Python anymore , but Python is most prominent , and none of the other languages have overtaken the “Unknown” group , which is primarily due to early notebooks in which Python was hardcoded , or the language stated in some other non - standard fashion . Jupyter Notebooks are also used for other languages like Bash , Matlab , and Java . Figure 8 shows the top programming 12 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 2010 2012 2014 2016 2018 2020 0 500 1000 1500 2000 Language Python R Unknown Groovy Julia Scala Sos Bash Matlab Year of repository creation N u m b e r o f n o t e b oo k s Figure 8 . Relative proportion of the most frequent programming languages used in the notebooks per year . languages used in notebooks based on the published year of the article . There is a steady use of Python in Jupyter Notebooks . However , fewer notebooks have undeclared programming lan - guage in 2018 and 2020 . There is also an increase in the use of R in notebooks . Authors can see the timeline of other programming languages in the analysis notebook provided in the repository . Versions 2 0 1 6 2 0 1 7 2 0 1 8 2 0 1 9 2 0 2 0 2 0 2 1 0 500 1000 1500 2000 2500 3000 3500 4000 Minor version 2 . 7 3 . 4 3 . 5 3 . 6 3 . 7 3 . 8 3 . 9 unk Year of last update N u m b e r o f n o t e b oo k s Figure 9 . Python notebooks by minor Python version by year of last commit to the GitHub repository . Figure 9 shows the Python version of notebooks based on the year in which the repository was last updated . 2471 notebooks have Python version 3 . 6 , followed by 2031 notebooks with Python version 3 . 7 . Python version 3 . 6 and 3 . 7 are commonly used in recent years , followed by version 13 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 2 0 1 0 2 0 1 2 2 0 1 4 2 0 1 6 2 0 1 8 2 0 2 0 0 200 400 600 800 1000 1200 1400 1600 1800 Major version 3 2 u Year of creation N u m b e r o f n o t e b oo k s Figure 10 . Python notebooks by major Python version by year of ﬁrst commit . 2 . 7 . There are also some python notebooks without any version declared . 6028 notebooks have Python major version 3 , 1802 notebooks have Python major version 2 , and 329 notebooks have an unknown Python version . Notebook structure 0 100 200 300 400 500 N o t e b oo k s 5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100105 Code cells 0 7 13 24 49 max : 431 ( a ) Distribution of the number of code cells across notebooks in our corpus . 0 500 1 , 000 1 , 500 2 , 000 2 , 500 N o t e b oo k s 2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 Code cells with output 0 3 8 20 max : 163 ( b ) Distribution of the number of code cells with outputs across notebooks in our corpus . 0 250 500 750 1 , 000 1 , 250 1 , 500 1 , 750 N o t e b oo k s 3 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66 69 72 Markdown cells 0 1 6 13 31 max : 383 ( c ) Distribution of the number of Markdown cells across notebooks in our corpus . 0 50 100 150 200 250 N o t e b oo k s 0 25 50 75 100 125 150 175 200 225 250 Maximum value in execution counters 1 10 20 47 102 max : 2076 ( d ) Distribution of the maximum execution count across notebooks in our corpus . Figure 11 . Analysis of the notebook structure 14 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Figure 11 shows the statistics on the structure of notebooks . Notebooks have a median of 20 cells and 13 code cells . The average number of cells with outputs in notebooks found in our study is three , with zero being the least ( Figure 11b ) . The maximum number of cells , code cells , and cells with output seen in a notebook are 95 , 431 , and 163 , respectively . The maximum number of raw and empty cells seen in a notebook is 49 and 31 , respectively . Raw cells let the users write output directly and the kernel does not evaluate them . The average number of markdown cells in note - books is six , with the maximum being 383 . 6311 ( 65 . 77 % ) of the notebooks have markdown cells , while 3284 ( 34 . 23 % ) notebooks do not . 96 . 58 % of notebooks use English in the markdown cells ; While 46 . 27 % notebooks use only English in the markdown cells . In addition to English , French ( 11 . 76 % ) and Danish ( 3 . 96 % ) are the other popular natural languages used in the markdown . In 1909 ( 30 . 25 % ) notebooks , we could not detect the language in the markdown cells . Further analy - sis of markdown cells shows that the average number of lines and words seen in markdown cells are 20 and 145 , respectively . Paragraphs and headers , the most commonly seen markdown ele - ments , appear in 92 . 65 % and 81 . 81 % notebooks , respectively . 1 , 449 ( 17 . 76 % ) notebooks do not have execution numbers and 6 , 710 ( 82 . 24 % ) notebooks have execution numbers . The maximum execution count seen in a notebook is 2076 ( Figure 11d ) . Notebook naming 34 22 18 15 14 12 12 12 9 . 0 8 . 0 U n t i t l e d . i p y n b p r o g r a m m i n g . i p y n b i n d e x . i p y n b t a l k t o r i a l . i p y n b a n a l y s i s _ n o t e b oo k . i p y n b I n d e x . i p y n b d e m o . i p y n b U n t i t l e d 1 . i p y n b R E A D M E . i p y n b t e s t . i p y n b 0 5 10 15 20 25 30 35 Notebook Name C o un t Figure 12 . Most frequent notebook titles . Figure 12 shows the most frequently used titles in notebooks from our collected data . “Untitled” , “programming” and “index” are the three most common notebook names . There are 63 ( 0 . 65 % ) whose title is or starts with “Untitled” . There are 21 ( 0 . 22 % ) notebooks that contain the name ’Copy’ . We also see many notebooks with the string ’test’ in their names ( Figure 13 ) . 1 , 070 ( 11 . 12 % ) notebooks have names that are not recommended by the POSIX fully portable ﬁlenames guide ( Pimentel et al . , 2019 ) . Only four notebooks have names that are disallowed in Windows . There are no notebooks without a title ( i . e . , notebooks with just a ’ . ipynb’ extension ) . Figure 14 shows the distribution of length of notebook title . The average length of the notebook title is 18 characters , with a maximum of 123 characters and a minimum of 2 . 15 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications s t a t s m o d e l s . s t a t s . m u l t i t e s t c o b r a . t e s t q i s k i t . t e s t . m o c k h y p o t e s t . g r a p h _ g e n e r a t i o n t i g r a m i t e . i n d e p e n d e n c e _ t e s t s un i tt e s t m e d u s a . t e s t m o c k h y p o t e s t . g r a p h _ m u t a t i o n h y p o t e s t . i o h y p o t e s t . o n t o l o g i e s h y p o t e s t . s t a t s h y p o t e s t . s e t u p _ h y p o t h g r a p h m a b o ss _ t e s t h y p o t e s t . c o n f i d e n c e 0 5 10 15 20 20 12 8 8 8 7 7 6 6 5 5 4 3 3 3 Figure 13 . Notebooks with the string test 0 100 200 300 400 N o t e b oo k s 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Name Length 2 12 18 27 49 max : 123 Figure 14 . Notebook title length Notebook modules Figure 15 shows the analysis of modules declared in notebooks . Using AST 22 , we analyzed the valid Python notebooks . 5 , 248 ( 69 . 06 % ) notebooks had imports , of which 714 ( 9 . 40 % ) had local imports , while 5 , 216 ( 68 . 64 % ) had external modules . Local imports denote the import of modules deﬁned in the notebook repository’s directory . There are 1035 local and 38229 external modules declared in the collected Python notebooks . Figure 15 shows the top ten commonly used Python modules declared in the notebooks . The most used modules are numpy ( 3255 ) , pandas ( 2428 ) , and matplotlib . pyplot ( 2411 ) . These are widely used modules for data manipulation , analytics , and visualizations . Notebook dependencies Figure 17 shows the analysis of the declared dependencies of GitHub repositories and notebooks . 4650 ( 48 . 31 % ) of notebooks belong to repositories which have declared dependencies using setup . py , requirements . txt , or pipﬁle . There are 492 repositories with declared dependencies ( Figure 17b ) . 22 https : / / docs . python . org / 3 / library / ast . html 16 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications nu m p y p a n d a s m a t p l o t li b . p y p l o t o s s e a b o r n s y s m a t p l o t li b s c i p y I P y t h o n . d i s p l a y g l o b 0 500 1 , 000 1 , 500 2 , 000 2 , 500 3 , 000 3 , 255 2 , 428 2 , 411 1 , 998 1 , 056 1 , 055 914 573 482 465 Figure 15 . Top Python modules declared in Jupyter Notebooks . a u t o r e l o a d r p y 2 . i p y t h o n w a t e r m a r k f e rr e t m a g i c C y t h o n s a g e i v e r s i o n s v e r s i o n _ i n f o r m a t i o n m e m o r y _ p r o f il e r 0 50 100 150 200 250 300 290 118 35 9 5 1 1 1 1 Figure 16 . Load extension modules in Jupyter Notebooks There are 194 repositories with setup . py ﬁle , 117 repositories with requirements . txt ﬁle . 180 repos - itories have both setup . py and requirements . txt ﬁle . Only 10 repositories are with pipﬁle ( 0 . 90 % ) . In our study , 3845 ( 39 . 95 % ) of notebooks use setup . py ﬁle , 2765 ( 28 . 73 % ) notebooks use require - ments . txt and only 186 ( 1 . 93 % ) notebooks use pipﬁle . Notebook Reproducibility In our reproducibility study , we executed 4169 ( 43 . 45 % ) Python notebooks . The dependencies of the notebooks , as mentioned in their respective repositories , were installed in conda environments . But , dependencies of 1 , 485 ( 35 . 62 % ) notebooks failed to install . None of the ﬁles were malformed with wrong syntax or conﬂicting dependencies . We did not ﬁnd any missing ﬁles that required other requirement ﬁles which were unavailable or ﬁles that needed external tools . Hence , the reason for the failed installed error is unknown . We attempted to execute 2 , 684 ( 64 . 38 % ) notebooks for the reproducibility study after successfully installing all the requirements . However , many notebooks failed to execute even after installing all the requirements successfully . 17 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 32 4 , 534 682 local external ( a ) External versus local modules declared in Jupyter notebooks . 191 117 174 1 3 0 6 setup . py requirements . txt Pipfile 625 ( b ) Repositories with dependencies . 1 , 828 802 1 , 834 3 54 129 setup . py requirements . txt Pipfile 4 , 975 ( c ) Notebooks with dependencies . Figure 17 . Dependencies of Juypter Notebooks and GitHub repositories . In ( a ) , the notebooks depending on external modules ( green ) are plotted against notebooks depending on local modules ( red ) and notebooks that had both ( brown ) . In ( b ) and ( c ) , GitHub repositories and Jupyter notebooks are shown as to whether they declared their dependencies via any combination of setup . py ( red ) , requirements . txt ( green ) or a pipfile ( pink ) . Exceptions 2 , 265 ( 84 . 39 % ) notebooks resulted in exceptions due to several reasons . Figure 18 shows the top ten exceptions that occurred while executing the notebooks . ModuleNotFoundError , ImportError , and FileNotFoundError are the most common reason that resulted in failure of execution in note - books . 1 , 362 ( 32 . 67 % ) of the executions failed because of ModuleNotFoundError and ImportError exceptions . ModuleNotFoundError exception occurs when a Python module used by the notebook could not be found . ImportError exception occurs when a Python module used by the notebook could not be imported . These two errors occur mainly due to missing dependencies . 132 ( 3 . 17 % ) notebooks have NameError , which occurs when a declared variable in the notebook is not de - ﬁned . 374 ( 8 . 97 % ) notebooks have FileNotFoundError or IOError . These exceptions occur when absolute paths are used to access data or when the data ﬁles are not included in the repository . Figure 19 shows how the top three common exceptions ModuleNotFoundError , ImportError , and FileNotFoundError change with the year the article was published . We see an increase in the Mod - uleNotFoundError through the years . In the years before 2019 , the ImportError outnumbered Mod - uleNotFoundError . Figure 20 shows the trend of exceptions by the year of publication normalized by the number 18 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 832 530 324 132 76 54 50 40 24 24 M o d u l e N o t F o u n d E rr o r I m p o r t E rr o r F il e N o t F o u n d E rr o r N a m e E rr o r M a l f o r m e d N o t e b o o k O S E rr o r I O E rr o r T y p e E rr o r C a ll e d P r o c e s s E rr o r A tt r i b u t e E rr o r 2 3 4 5 6789 100 2 3 4 5 6789 1000 Exception N o t e b oo k e x e c u t i o n s Figure 18 . Exceptions occurring in Jupyter Notebooks . 432 223 69 23 9 7 164 84 44 20 9 1 125 112 96 88 67 20 19 2 0 1 6 2 0 1 8 2 0 2 0 1 2 5 10 2 5 100 2 5 ModuleNotFoundError FileNotFoundError ImportError Publication year N u m b e r o f e x c e p t i o n s Figure 19 . ModuleNotFoundError , ImportError and FileNotFoundError exceptions by year of publication . of notebooks . In 2020 , we observed the highest number of exceptions and notebooks . Figure 21 shows the exceptions by the type of the article . The research articles have the most number of exceptions . Figure 22 shows the exceptions by journal , normalized by the number of notebooks . The journal eLife has the most number of notebooks with the most number of exceptions , followed by PLoS One . Successful replications 396 ( 9 . 50 % ) of the notebooks in our corpus ﬁnished their execution successfully without any errors . However , for 151 notebooks ( 3 . 62 % ) , our execution generated results that diﬀered from the origi - nal ones , while 245 notebooks ( 5 . 88 % ) produced the same results in our execution as documented 19 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 111 46 144 492 566 1 . 121k 99 2 0 1 5 2 0 1 6 2 0 1 7 2 0 1 8 2 0 1 9 2 0 2 0 2 0 2 1 4 5 6 7 89 100 2 3 4 5 6 7 89 1000 50 100 150 200 250 300 350 400 Number of notebooks Published year N u m b e r o f e x c e p t i o n s Figure 20 . Exceptions by year of publication . 870 625 32 43 88 42 49 22 41 42 R e s e a r c h A r t i c l e A r t i c l e N e u r o s c i e n c e S o f t w a r e D a t a D e s c r i p t o r T o o l s a n d R e s o u r c e s R e s e a r c h S o f t w a r e T o o l A r t i c l e O r i g i n a l A r t i c l e O r i g i n a l P a p e r s 2 3 4 5 6789 100 2 3 4 5 6789 1000 50 100 150 200 250 Number of articles Article type N u m b e r o f e x c e p t i o n s Figure 21 . Exceptions by article type . for the original notebooks . Table 1 zooms in on the successfully executed notebooks and compares those that did not yield results the same results as the original ones ( different group ) with those that did ( identical group ) . A clear diﬀerence between both groups is that many of the notebooks in the identical group had their dependencies speciﬁed via either setup . py or requirements . txt or both , in contrast to none of the notebooks in the different group . Since notebooks with no dependency declara - tions were run using the default conda dependencies , the fact that they successfully ﬁnished means that all dependencies were covered . However , as the version of the dependencies used in the orig - inal notebook was not documented , it may have diﬀered from the version that was provided in our respective Conda environment . 20 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications 412 203 277 99 121 74 53 46 29 29 e L i f e P L o S O n e P L o S C o m p u t B i o l N a t C o m m u n S c i D a t a S c i R e p B i o i n f o r m a t i c s B M C B i o i n f o r m a t i c s F r o n t N e u r o i n f o r m G i g a s c i e n c e 3 4 5 6 7 8 9 100 2 3 4 40 60 80 100 120 Number of notebooks Journal N u m b e r o f e x c e p t i o n s Figure 22 . Exceptions by journal . Table 1 . Comparison of notebooks that were successfully executed without errors , grouped by whether their results were different from or identical to the results documented for the original notebook . For features listed in italics , the mean values per notebook are indicated , otherwise totals across all notebooks per group . Features Notebooks with different results Notebooks with identical results Number of notebooks 151 245 setup . py 0 98 requirement . txt 0 107 pipﬁle 0 0 Total cells 17 . 9 17 . 1 Code cells 12 . 3 9 . 8 Markdown cells 5 . 6 6 . 7 Ratio of Markdown vs . code cells 0 . 46 0 . 68 Empty cells 0 . 7 0 . 7 Diﬀerences 5 . 3 0 Execution time ( s ) 22 . 1 16 . 4 Execution time per code cell ( s ) 1 . 80 1 . 67 Besides versioning of dependencies , there could be a number of other reasons as to why an error - free execution might yield diﬀerent results . For instance , random functions may be invoked , or code cells in the original might have been executed multiple times or in a diﬀerent order than in our execution , which ran every code cell just once , from top to bottom . However , we would not expect the invocation of random functions or an inconsistent execution order to correlate so strongly with whether the dependencies had been explicitly declared or not . In contrast to the dependency declarations , other features in Table 1 show more gradual dif - ferences between the two groups , and they largely ﬁt with intuition . For instance , it is understand - able that notebooks with more code cells take longer to execute and that code whose execution per code cell takes longer is somewhat more complex , thus raising the probability of diﬀerent out - comes . It is also not surprising that , while the total number of cells per notebook is nearly the same 21 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications in both groups , notebooks in the identical group show a higher ratio of Markdown versus code cells , since that ratio is indicative of documentation eﬀorts , and better documentation would be expected to go with better reproducibility . The average number of diﬀerences observed per notebook ( or even per code cell ) is not easy to interpret on its own , as it includes diﬀerences in output cells , cell counter values or in output ﬁles , and a diﬀerence early in a notebook can lead to further diﬀerences later . Table 2 illustrates how diﬀerent Python versions performed in terms of successful executions : amongst the top 5 versions for notebooks yielding diﬀerent results , there were three 2 . 7 versions , whereas there were three 3 . 7 versions in the group that yielded identical results , and 3 . 6 . 9 and 3 . 6 . 5 were represented roughly equally in both groups . Table 2 . Comparison of most frequent Python versions declared for notebooks that were successfully executed without errors , grouped by whether their results were different from or identical to the results documented for the original notebook . Versions listed in italics occur in both top - 5 groups , versions listed in bold in only one . The absolute columns give total number of notebooks per version and group , while the relative columns normalize the absolute values as a percentage of the total number of notebooks per group , i . e . 151 for different and 245 for identical , as per Table 1 . In both groups , the top - 5 versions account for slightly over half of the notebooks . different identical rank version absolute relative version absolute relative 1 3 . 6 . 9 28 18 . 5 3 . 7 . 3 46 18 . 8 2 2 . 7 . 6 17 11 . 3 3 . 6 . 9 30 12 . 2 3 2 . 7 . 10 13 8 . 6 3 . 7 . 1 26 10 . 6 4 3 . 6 . 5 12 7 . 9 3 . 7 . 4 22 9 . 0 5 2 . 7 . 9 11 7 . 3 3 . 6 . 5 15 6 . 1 Other parameters that we considered but did not include in the analysis of the ﬁnished note - books were the number of dependencies ( the more there are , the more likely replicability is re - duced ; see also section Notebook dependencies and in particular Fig . 17 ) , the type of dependen - cies ( e . g . local code or environment , Python package , local or remote ﬁle or service , each of which could complicate replication ; see also Fig . 15 and 16 ) , the recency ( cf . Fig . and 19 and 20 ) of the notebooks ( more recent ones would be expected to be more replicable ) or notebook titles ( cf . Fig . 12 , 13 and 14 ) containing strings like “tutorial” or “demo” ( which might be indicative of expected reuse , thus perhaps triggering more careful documentation ) or “untitled” ( which is the default title and may thus indicate a lack of attention to documentation and , consequently , a higher likelihood for replication attempts to fail ) . Notebook Styling In addition to the common exceptions in the notebooks , we also checked the notebook code styling errors . Figure 23 shows the most common Python code warning / style errors found in our study . Table 3 presents the code for the Python code warnings and style errors found in our study . E231 is the most common coding style error , followed by E225 and E265 , respectively . There are also some common content errors other than styling errors like F403 and F405 – these are related to variable and module deﬁnition errors . The W601 and W606 warnings relate to the use of depre - cated and reserved keys . 22 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN E 231 E 225 E 265 E 262 F 405 E 203 E 703 E 302 E 127 E 211 W 191 E 999 E 741 E 124 F 403 F 841 E 271 E 713 E 272 E 306 E 101 E 129 E 125 F 633 F 823 W 606 2 5 10 2 5 100 2 5 1000 2 5 10k 2 5 100k Error Code E rr o r C o d e C o un t Figure 23 . Frequent notebook code style errors as per the Python code style guide . Discussion In this study , we have analyzed the Method reproducibility – in the sense of Goodman et al . ( 2016 ) – of Jupyter notebooks written in Python and publicly hosted on GitHub that are mentioned in pub - lications whose full text was available via PubMed Central by the reference period , i . e . the time when our reproducibility pipeline was run on 24 - 28 February 2021 . We will now discuss the limi - tations of the study and then its implications , again primarily for Method reproducibility of Jupyter notebooks associated with biomedical publications . Limitations The present study does not address Inferential reproducibility and only brieﬂy touches upon Results reproducibility . Furthermore , we made no attempt to re - run computational notebooks that met any of the following exclusion criteria during the reference period : ( a ) they did not use Jupyter ( or its precursor , IPython ) , ( b ) they were not written in Python , ( c ) they were not publicly available on GitHub , ( d ) they were not mentioned in publications available from PubMed Central , ( e ) they were not on the base branch of their GitHub repository ( which is the only branch we looked at ) . Our reproducibility workﬂow is based on that by Pimentel et al . ( 2019 ) , with some changes to include GitHub repositories from publications and using the nbdime library ( Project Jupyter , 2021 ) from Jupyter instead of string matching for ﬁnding diﬀerences in the notebook outputs . The ap - proach is using conda ( Conda community , 2017 ) environments . We did not use any Docker images ( Docker , 2013 ) for the execution environment , even in cases when they were available . This work - ﬂow being fully automated , we did not spend any manual eﬀort on ﬁxing any of the errors that came up for an individual notebook – see Woodbridge ( 2017 ) for a report of an attempt to do so , which also provided the foundation for a prototypical validation tool that makes use of GitLab Ac - tions For a good number of the reported problems ( especially the missing software or data depen - dencies , as per Fig . 18 ) , it is often straightforward to ﬁx them manually for individual notebooks , yet undertaking manual ﬁxes systematically was not practical at the scale of the thousands of note - books rerun here . If the original code had speciﬁed dependencies without referring to a speciﬁc version , our rerun would use the most recent conda - installable version of that library . Finally , in estimating the environmental footprint of this study , we only included the footprint due to running 23 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Table 3 . Common Python Notebook Code Warning / Style Error found in our Study Error code Description Count E231 missing whitespace after commas , semicolons or colons 102218 E225 missing whitespace around operator 25979 E265 block comment should start with ‘ # ‘ 10769 E402 module level import not at top of ﬁle 10478 E262 inline comment should start with ‘ # ‘ 8369 E703 statement ends with a semicolon 2023 E127 continuation line over - indented for visual indent 1290 E701 multiple statements on one line 500 E741 do not use variables named ‘l’ , ‘O’ , or ‘I’ 432 E401 multiple imports on one line 95 E101 indentation contains mixed spaces and tabs 32 F405 name may be undeﬁned , or deﬁned from star im - ports : module 4840 F401 module imported but unused 3938 F821 undeﬁned name ’X’ 2071 F403 ’from module import * ’ used ; unable to detect un - deﬁned names 263 F841 local variable ’X’ is assigned to but never used 225 F404 future import ( s ) name after other statements 44 F402 import ’X’ from line Y shadowed by loop variable 10 F633 use of » is invalid with print function 6 F823 local variable ’X’ deﬁned in enclosing scope on line Y referenced before assignment 4 W601 . has _ key ( ) is deprecated , use ‘in’ 7 W606 ’async’ and ’await’ are reserved keywords starting with Python 3 . 7 3 the full pipeline once – we did not include the eﬀorts involved in preparing the pipeline , analyzing the data or writing the manuscript . Implications There are several implications of this study . First , on a general level , the low degree of reproducibil - ity that we documented here for Jupyter notebooks associated with biomedical publications goes conform with similarly low levels of reproducibility that were found in earlier domain - generic stud - ies , both for Python ( Rule et al . , 2018 ; Pimentel et al . , 2021 ) and R ( Trisovic et al . , 2022 ) . Second , considering that the notebooks we explored here were associated with peer - reviewed publications , it is clear that the review processes currently in place at journals within our corpus does not generally pay much attention to the reproducibility of the notebooks . This clearly needs to change , and we need systemic approaches to that rather than just adding this to the list of things the reviewers are expected to attend to . As our study demonstrates , a basic level of reproducibility assessment can well be achieved in a fully automated fashion , so it would probably be beneﬁcial in terms of research quality to include such automated basic checks – for notebooks and other software – into standard review procedures . Ideally , this would be done in a way that works across publishers as well as for a variety of technology stacks and programming languages . Third , while there is a large variety in the types of errors aﬀecting reproducibility , some of the 24 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications most common errors concentrate around dependencies ( cf . Fig . 15 , 16 , 17 and 18 ) , so eﬀorts aimed at systemic improvements of dependency handling – e . g . as per Zhu et al . ( 2021 ) – have the poten - tial to increase reproducibility considerably . Here , programming language - speciﬁc eﬀorts regard - ing code dependencies can be combined with eﬀorts targeted at improving the automated han - dling of data dependencies , which would be beneﬁcial irrespective of the speciﬁc programming language . Fourth , zooming in on Python speciﬁcally , wider adoption of existing workﬂows for code depen - dency management ( such as requirements . txt ) as well as associated checks during the publishing process would help . Researchers attempting to publish research with associated notebooks should not have to do this all by themselves – research infrastructures as well as publishers and funders can all help facilitate establishing best practice here and engaging communities around them . Fifth , the few notebooks that actually did reproduce ( cf . Successful replications ) are not equally distributed . This means that reproducibility could probably be strengthened by enhancing or high - lighting the features that correlate with it . For instance , Jupyter notebooks with more emphasis on documentation scored better than others , and there is merit in the idea of making Jupyter note - books or similar computational notebooks a publication type of their own . This is already the case in some places , as exampliﬁed by Constantine et al . ( 2016 ) or Garg et al . ( 2022 ) in the Journal of Open Source Software . Sixth , the ongoing diversiﬁcation of the Jupyter ecosystem – e . g . in terms of programming lan - guages , deployment frameworks or cloud infrastructure – is increasingly reﬂected , albeit with de - lay , in the biomedical literature . In parallel , while GitHub remains hugely popular , alternatives like GitLab , Gitee or Codeberg are growing too . Future assessments of Jupyter reproducibility will thus need to take this increasing complexity into account , and ideally present some systematic approach to it . Seventh , the delays that come with current publishing practices also mean that Jupyter note - books associated with freshly published papers are using software versions near or even beyond their respective support window ( which is 42 months in much of the Python ecosystem 23 ) . For instance , the oldest Python version still oﬃcially supported in 2021 was 3 . 6 ( which was itself retired by the end of 2021 , when 3 . 10 was released 24 ) , yet as shown in Figure 9 , over a thousand Python notebooks in our corpus whose last commit was in 2021 still featured earlier Python ver - sions , mainly 2 . 7 ( outphased in 2020 ) but also 3 . 4 ( 2019 ) , 3 . 5 ( 2020 ) and some for which the version could not be determined . This contributes to reproducibility issues . A similar issue exists with the versions of the libraries called from any given notebook , though the eﬀects might diﬀer as a func - tion of whether they have been invoked with or without the version being speciﬁed . If the version had been speciﬁed , its oﬃcial end of life might go back even further . If the version was not spec - iﬁed , the newest available version would be invoked , which may not be compatible with the way the library had been used in the original notebook . Similar issues can arise with the versioning of APIs , datasets , ontologies or other standards used in the notebook , all of which can contribute to reduced reproducibility . To some extent , these version delay issues can be shortened by preprints : since they are ( essentially by deﬁnition , but not always in practice ) published before the ﬁnal ver - sion of the associated manuscript , and hence their delays should be shorter , with lower reductions in reproducibility , though we did not investigate that in detail . Eight , the variety and scale of issues encountered in the notebooks analyzed here provides am - ple opportunities for use in educational contexts – including instructed , self - guided or group learn - ing – since ﬁxing real - life bugs can be more motivating than working primarily with textbook exam - ples . To do this eﬀectively would require some mapping of the strengths and weaknesses of the notebooks to learning objectives , which may range from understanding programming paradigms , software engineering principles or data integration workﬂows to developing an appreciation for documentation and other aspects of good scientiﬁc practice . Given the continuously expanding 23 Cf . https : / / numpy . org / neps / nep - 0029 - deprecation _ policy . html 24 See https : / / endoﬂife . date / python for release schedule 25 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications breadth of publications that use Jupyter notebooks , it is also steadily becoming easier to ﬁnd publi - cations where they have been used in research meeting speciﬁc criteria . These could be a particular topic – e . g . natural products research ( Mayr et al . , 2020 ) or invasion biology ( Bors et al . , 2019 ) – or workﬂows involving a particular experimental methodology like single - cell RNA sequencing ( Vargo and Gilbert , 2020 ) or other software tools like ImageJ ( Bryson et al . , 2020 ) . Conclusions On the basis of re - running 4169 Jupyter notebooks associated with 1419 publications whose full text is available via Pubmed Central , we conclude that such notebooks are becoming more and more popular for sharing code associated with biomedical publications , that the range of program - ming languages or journals they cover is continuously expanding and that their reproducibility is low but improving , consistent with earlier studies on Jupyter notebooks shared in other contexts . The main issues are related to dependencies – both code and data – which means that reproducibil - ity could likely be improved considerably if the code – and dependencies in particular – were better documented . Further improvements could be expected if some basic and automated reproducibil - ity checks of the kind performed here were to be systematically included in the peer review process or if computational notebooks – Jupyter or otherwise – were combined with additional approaches that address reproducibility from other angles , e . g . registered reports . Data availability All the data generated during this study is available at https : / / doi . org / 10 . 5281 / zenodo . 6802158 . The code used is available at https : / / github . com / fusion - jena / computational - reproducibility - pmc . The code contains notebooks used for analysis of the results . Conﬂict of interest The authors declare there are no competing interests . Ethics No facet of the research reported here triggered a requirement for ethical review . While our data contains personally identiﬁable information , it was taken directly from PMC . We did , however , consider the ethical implications of automated reproducibility studies of the kind presented here , which led us to ( a ) highlight systemic aspects , ( b ) not zoom in on individual stakeholders other than journals and ( c ) include environmental footprint information . Acknowledgements We would like to thank the providers of infrastructure , data and code that we used in this study . These include the PubMed Central repository hosted by the National Center for Biotechnology Information in the United States and the Ara Cluster at the University of Jena as well as the Python , Jupyter and Conda communities and their respective dependencies . We acknowledge the Open Research Doathon on the occasion of Open Data Day 2017 , where the ﬁrst attempts at systematic reproduction of PMC - indexed Jupyter notebooks were made Woodbridge ( 2017 ) . Special thanks go to JupyterCon , which made the two of us aware of each other’s work and provided the nucleus for our collaboration . Work by S . S . was supported by the Carl Zeiss Foundation for the project “A Virtual Werkstatt for Digitization in the Sciences ( K3 ) ” ( Samuel et al . , 2020 ) within the scope of the program line “Breakthroughs : Exploring Intelligent Systems for Digitization - explore the basics , use applications” . Work by D . M . was supported by the Alfred P . Sloan Foundation under grant number G - 2021 - 17106 . The computational experiments were performed on resources of Friedrich Schiller University Jena supported in part by DFG grants INST 275 / 334 - 1 FUGG and INST 275 / 363 - 1 FUGG . 26 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications References Bairoch AM . The Cellosaurus , a Cell - Line Knowledge Resource . Journal of Biomolecular Techniques . 2018 ; 29 ( 2 ) : 25 – 38 . Baker AM , Cereser B , Melton S , Fletcher AG , Rodriguez - Justo M , Tadrous PJ , Humphries A , Elia G , McDonald SAC , Wright NA , Simons BD , Jansen M , Graham TA . Quantiﬁcation of Crypt and Stem Cell Evolution in the Normal and Neoplastic Human Colon . Cell Reports . 2019 may 1 ; 27 ( 8 ) : 2524 . Baker M . 1 , 500 scientists lift the lid on reproducibility . Nature . 2016 may 25 ; 533 ( 7604 ) : 452 – 454 . Boettiger C . An Introduction to Docker for Reproducible Research . SIGOPS Oper Syst Rev . 2015 Jan ; 49 ( 1 ) : 71 – 79 . http : / / doi . acm . org / 10 . 1145 / 2723872 . 2723882 , doi : 10 . 1145 / 2723872 . 2723882 . Bors EK , Herrera S , Morris JA , Shank TM . Population genomics of rapidly invading lionﬁsh in the Caribbean reveals signals of range expansion in the absence of spatial population structure . Ecology and evolution . 2019 feb 10 ; 9 ( 6 ) : 3306 – 3320 . Brito JJ , Li J , Moore JH , Greene CS , Nogoy NA , Garmire LX , Mangul S . Recommendations to enhance rigor and reproducibility in biomedical research . GigaScience . 2020 06 ; 9 ( 6 ) . https : / / doi . org / 10 . 1093 / gigascience / giaa056 , doi : 10 . 1093 / gigascience / giaa056 , giaa056 . Bryson AE , Brown MW , Mullins J , Dong W , Bahmani K , Bornowski N , Chiu C , Engelgau P , Gettings B , Gomezcano F , Gregory LM , Haber AC , Hoh D , Jennings EE , Ji Z , Kaur P , Raju SKK , Long Y , Lotreck SG , Mathieu DT , et al . Composite modeling of leaf shape along shoots discriminates Vitis species better than individual leaves . Applications in plant sciences . 2020 dec 3 ; 8 ( 12 ) : e11404 . Burlingame EA , Eng J , Thibault G , Chin K , Gray JW , Chang YH . Toward reproducible , scalable , and robust data analysis across multiplex tissue imaging platforms . Cell Reports Methods . 2021 8 ; 1 ( 4 ) : 100053 . Chattopadhyay S , Prasad I , Henley AZ , Sarma A , Barik T . What’s Wrong with Computational Notebooks ? Pain Points , Needs , and Design Opportunities . In : Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ; 2020 . p . 1 – 12 . Chirigati F , Shasha D , Freire J . ReproZip : Using Provenance to Support Computational Re - producibility . In : Presented as part of the 5th USENIX Workshop on the Theory and Prac - tice of Provenance Lombard , IL : USENIX ; 2013 . https : / / www . usenix . org / conference / tapp13 / reprozip - using - provenance - support - computational - reproducibilitythe . Cock PJA , Antao T , Chang JT , Chapman BA , Cox CJ , Dalke A , Friedberg I , Hamelryck T , Kauﬀ F , Wilczynski B , de Hoon MJL . Biopython : freely available Python tools for computational molecular biology and bioin - formatics . Bioinformatics . 2009 03 ; 25 ( 11 ) : 1422 – 1423 . https : / / doi . org / 10 . 1093 / bioinformatics / btp163 , doi : 10 . 1093 / bioinformatics / btp163 . Coiera EW , Ammenwerth E , Georgiou A , Magrabi F . Does health informatics have a replication crisis ? Journal of the American Medical Informatics Association . 2018 aug 1 ; 25 ( 8 ) : 963 – 968 . doi : 10 . 1093 / JAMIA / OCY028 . Conda community , Conda ; 2017 . https : / / conda . io / . Constantine P , Howard R , Glaws A , Grey Z , Diaz P , Fletcher L . Python Active - subspaces Utility Library . Journal of Open Source Software . 2016 sep 29 ; 1 ( 5 ) : 79 . Contera S . Communication is central to the mission of science . Nature Reviews Materials . 2021 ; 6 ( 5 ) : 377 – 378 . Crick T , Hall BA , Ishtiaq S . Reproducibility in Research : Systems , Infrastructure , Culture . Journal of open re - search software . 2017 nov 9 ; 5 ( 1 ) : 32 . Docker , Docker ; 2013 . https : / / www . docker . com . Fanelli D . Opinion : Is Science Really Facing a Reproducibility Crisis , and Do We Need It To ? Proceedings of the National Academy of Sciences of the United States of America . 2018 ; 115 ( 11 ) : 2628 – 2631 . Garg A , Smith - Unna RD , Murray - Rust P . pygetpapers : a Python library for automated retrieval of scientiﬁc literature . Journal of Open Source Software . 2022 jul 7 ; 7 ( 75 ) : 4451 . Gil Y , David CH , Demir I , Essawy BT , Fulweiler RW , Goodall JL , Karlstrom L , Lee H , Mills HJ , Oh JH , Pierce SA , Pope A , Tzeng MW , Villamizar SR , Yu X . Toward the Geoscience Paper of the Future : Best practices for documenting and sharing research from data to software to provenance . Earth and Space Science . 2016 ; 3 ( 10 ) : 388 – 415 . https : / / agupubs . onlinelibrary . wiley . com / doi / abs / 10 . 1002 / 2015EA000136 , doi : 10 . 1002 / 2015EA000136 . 27 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Goodman SN , Fanelli D , Ioannidis JPA . What does research reproducibility mean ? Science Translational Medicine . 2016 jun 1 ; 8 ( 341 ) : 341ps12 . doi : 10 . 1126 / scitranslmed . aaf5027 . Granger BE , Perez F . Jupyter : Thinking and Storytelling With Code and Data . Computing in Science and Engi - neering . 2021 mar 26 ; 23 ( 2 ) : 7 – 14 . Gray S , Shwom R , Jordan R . Understanding factors that inﬂuence stakeholder trust of natural resource science and institutions . Environmental management . 2012 ; 49 ( 3 ) : 663 – 674 . Grüning B , Chilton J , Köster J , Dale R , Soranzo N , van den Beek M , Goecks J , Backofen R , Nekrutenko A , Taylor J . Practical Computational Reproducibility in the Life Sciences . Cell systems . 2018 jun 1 ; 6 ( 6 ) : 631 – 635 . doi : 10 . 1016 / j . cels . 2018 . 03 . 014 . Guttinger S . The limits of replicability . European journal for philosophy of science . 2020 jan 15 ; 10 ( 2 ) . doi : 10 . 1007 / s13194 - 019 - 0269 - 1 . Halchenko YO , Meyer K , Poldrack B , Solanky DS , Wagner AS , Gors J , MacFarlane D , Pustina D , Sochat V , Ghosh SS , Mönch C , Markiewicz CJ , Waite L , Shlyakhter I , de la Vega A , Hayashi S , Häusler CO , Poline JB , Kadelka T , Skytén K , et al . DataLad : distributed system for joint management of code , data , and their re - lationship . Journal of Open Source Software . 2021 ; 6 ( 63 ) : 3262 . https : / / doi . org / 10 . 21105 / joss . 03262 , doi : 10 . 21105 / joss . 03262 . Hinsen K . Veriﬁability in computer - aided research : the role of digital scientiﬁc notations at the human - computer interface . PeerJ Computer Science . 2018 ; 4 : e158 . Hsieh T , Vaickus MH , Remick DG . Enhancing scientiﬁc foundations to ensure reproducibility : a new paradigm . The American journal of pathology . 2018 ; 188 ( 1 ) : 6 – 10 . Hunter P . The reproducibility “crisis” . EMBO reports . 2017 ; 18 ( 9 ) : 1493 – 1496 . https : / / www . embopress . org / doi / abs / 10 . 15252 / embr . 201744876 , doi : 10 . 15252 / embr . 201744876 . Hussain W , Moens N , Veraitch FS , Hernandez D , Mason C , Lye GJ . Reproducible Culture and Diﬀerentiation of Mouse Embryonic Stem Cells Using an Automated Microwell Platform . Biochemical engineering journal . 2013 ; 77 ( 100 ) : 246 – 257 . Hutson M . Artiﬁcial intelligence faces reproducibility crisis . Science . 2018 ; 359 ( 6377 ) : 725 – 726 . https : / / science . sciencemag . org / content / 359 / 6377 / 725 , doi : 10 . 1126 / science . 359 . 6377 . 725 . Jamieson KH , McNutt M , Kiermer V , Sever R . Signaling the trustworthiness of science . Proceedings of the National Academy of Sciences . 2019 ; 116 ( 39 ) : 19231 – 19236 . https : / / www . pnas . org / content / 116 / 39 / 19231 , doi : 10 . 1073 / pnas . 1913039116 . Project Jupyter , Bussonnier M , Forde J , Freeman J , Granger BE , Head T , Holdgraf C , Kelley K , Nalvarte G , Os - heroﬀ A , Pacer M , Panda Y , Pérez F , Ragan - Kelley B , Willing C . Binder 2 . 0 - Reproducible , interactive , sharable environments for science at scale . In : Proceedings of the 17th Python in Science Conference ; 2018 . p . 113 – 120 . doi : 10 . 25080 / Majora - 4af1f417 - 011 . Kelly CD . Rate and success of study replication in ecology and evolution . PeerJ . 2019 sep 10 ; 7 : e7654 . doi : 10 . 7717 / peerj . 7654 . Kluyver T , Ragan - Kelley B , Pérez F , Granger B , Bussonnier M , Frederic J , Kelley K , Hamrick J , Grout J , Corlay S , Ivanov P , Avila D , Abdalla S , Willing C , the Jupyter development team . Jupyter Notebooks - a publishing format for reproducible computational workﬂows . In : ELPUB ; 2016 . p . 87 – 90 . Kroeger CM , Garza C , Lynch CJ , Myers E , Rowe S , Schneeman BO , Sharma AM , Allison DB . Scientiﬁc rigor and credibility in the nutrition research landscape . The American journal of clinical nutrition . 2018 March ; 107 ( 3 ) : 484—494 . https : / / europepmc . org / articles / PMC6248649 , doi : 10 . 1093 / ajcn / nqx067 . Lannelongue L , Grealey J , Bateman A , Inouye M . Ten simple rules to make your computing more environmen - tally sustainable . PLOS Computational Biology . 2021 sep 20 ; 17 ( 9 ) : e1009324 . Lannelongue L , Grealey J , Inouye M . Green Algorithms : Quantifying the Carbon Footprint of Computation . Advanced Science . 2021 ; n / a ( n / a ) : 2100707 . https : / / onlinelibrary . wiley . com / doi / abs / 10 . 1002 / advs . 202100707 , doi : https : / / doi . org / 10 . 1002 / advs . 202100707 . Ledermann F , Gartner G . Towards Conducting Reproducible Distributed Experiments in the Geosciences . AG - ILE : GIScience Series . 2021 jun 4 ; 2 : 1 – 7 . doi : 10 . 5194 / agile - giss - 2 - 33 - 2021 . 28 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Mayr F , Möller G , Garscha U , Fischer J , Castaño PR , Inderbinen SG , Temml V , Waltenberger B , Schwaiger S , HartmannRW , GegeC , MartensS , OdermattA , PandeyAV , WerzO , AdamskiJ , StuppnerH , SchusterD . Finding New Molecular Targets of Familiar Natural Products Using In Silico Target Prediction . International Journal of Molecular Sciences . 2020 sep 26 ; 21 ( 19 ) . Meng XL . Reproducibility , Replicability , and Reliability . Harvard Data Science Review . 2020 oct 29 ; 2 ( 4 ) . Https : / / hdsr . mitpress . mit . edu / pub / hn51kn68 . Meyerowitz - Katz G , Besançon L , Flahault A , Wimmer R . Impact of mobility reduction on COVID - 19 mortality : absence of evidence might be due to methodological issues . Scientiﬁc Reports . 2021 dec 7 ; 11 ( 1 ) . Näpﬂin K , O’Connor EA , Becks L , Bensch S , Ellis VA , Hafer - Hahmann N , Harding KC , Lindén SK , Olsen MT , Roved J , Sackton TB , Shultz AJ , Venkatakrishnan V , Videvall E , Westerdahl H , Winternitz JC , Edwards SV , Genomics of host - pathogen interactions : challenges and opportunities across ecological and spatiotemporal scales ; 2019 . Nielsen FÅ , Mietchen D , Willighagen E . Scholia , Scientometrics and Wikidata . In : The Semantic Web : ESWC 2017 Satellite Events ; 2017 . p . 237 – 259 . doi : 10 . 1007 / 978 - 3 - 319 - 70407 - 4 _ 36 . Nüst D , Sochat VV , Marwick B , Eglen SJ , Head T , Hirst T , Evans BD . Ten simple rules for writing Dockerﬁles for reproducible data science . PLOS Computational Biology . 2020 nov 10 ; 16 ( 11 ) : e1008316 . doi : 10 . 1371 / JOUR - NAL . PCBI . 1008316 . Peng R . The reproducibility crisis in science : A statistical counterattack . Signiﬁcance . 2015 ; 12 ( 3 ) : 30 – 32 . Pimentel JaF , Murta L , Braganholo V , Freire J . A Large - scale Study About Quality and Reproducibility of Jupyter Notebooks . In : Proceedings of the 16th International Conference on Mining Software Repositories MSR ’19 , Pis - cataway , NJ , USA : IEEE Press ; 2019 . p . 507 – 517 . doi : 10 . 1109 / MSR . 2019 . 00077 . Pimentel JF , Murta L , Braganholo V , Freire J . Understanding and improving the quality and reproducibility of Jupyter notebooks . Empir Softw Eng . 2021 ; 26 ( 4 ) : 65 . https : / / doi . org / 10 . 1007 / s10664 - 021 - 09961 - 9 , doi : 10 . 1007 / s10664 - 021 - 09961 - 9 . Plesser HE . Reproducibility vs . Replicability : A Brief History of a Confused Terminology . Frontiers in Neuroin - formatics . 2017 jan 1 ; 11 : 76 . Project Jupyter , nbdime : Jupyter Notebook Diﬀ and Merge tools ; 2021 . Accessed 18 May 2021 . https : / / github . com / jupyter / nbdime . Randles BM , Pasquetto IV , Golshan MS , Borgman CL . Using the Jupyter notebook as a tool for open science : An empirical study . In : 2017 ACM / IEEE Joint Conference on Digital Libraries ( JCDL ) IEEE ; 2017 . p . 1 – 2 . Roberts RJ . PubMed Central : The GenBank of the published literature . Proceedings of the National Academy of Sciences . 2001 ; 98 ( 2 ) : 381 – 382 . https : / / www . pnas . org / content / 98 / 2 / 381 , doi : 10 . 1073 / pnas . 98 . 2 . 381 . Rule A , Birmingham A , Zuniga C , Altintas I , Huang S , Knight R , Moshiri N , Nguyen M , Rosenthal S , Pérez F , et al . Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks . Plos Computational Biology . 2019 ; 15 ( 7 ) : e1007007 – e1007007 . Rule A , Tabard A , Hollan JD . Exploration and Explanation in Computational Notebooks . In : Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems CHI ’18 , New York , NY , USA : ACM ; 2018 . p . 32 : 1 – 32 : 12 . doi : 10 . 1145 / 3173574 . 3173606 . Russell PH , Johnson RL , Ananthan S , Harnke B , Carlson NE . A large - scale analysis of bioinformatics code on GitHub . PLOS ONE . 2018 ; 13 ( 10 ) : e0205898 . Rutz A , Sorokina M , Galgonek J , Mietchen D , Willighagen E , Gaudry A , Graham JG , Stephan R , Page R , Vondrášek J , Steinbeck C , Pauli GF , Wolfender JL , Bisson J , Allard PM . The LOTUS initiative for open knowledge manage - ment in natural products research . eLife . 2022 may 26 ; 11 . Samuel S , König - Ries B . ProvBook : Provenance - based Semantic Enrichment of Interactive Notebooks for Re - producibility . In : van Erp M , Atre M , López V , Srinivas K , Fortuna C , editors . Proceedings of the ISWC 2018 Posters & Demonstrations , Industry and Blue Sky Ideas Tracks co - located with 17th International Semantic Web Conference ( ISWC 2018 ) , Monterey , USA , October 8th - to - 12th , 2018 , vol . 2180 of CEUR Workshop Proceedings CEUR - WS . org ; 2018 . http : / / ceur - ws . org / Vol - 2180 / paper - 57 . pdf . 29 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Samuel S , König - Ries B . ReproduceMeGit : A Visualization Tool for Analyzing Reproducibility of Jupyter Note - books . In : Glavic B , Braganholo V , Koop D , editors . Provenance and Annotation of Data and Processes Cham : Springer International Publishing ; 2021 . p . 201 – 206 . Samuel S , König - Ries B . Understanding experiments and research practices for reproducibility : an exploratory study . PeerJ . 2021 Apr ; 9 : e11140 . https : / / doi . org / 10 . 7717 / peerj . 11140 , doi : 10 . 7717 / peerj . 11140 . Samuel S , Shadaydeh M , Böcker S , Brügmann B , Bucher SF , Deckert V , Denzler J , Dittrich P , von Eggeling F , Güllmar D , Guntinas - Lichius O , König - Ries B , Löﬄer F , Maicher L , Marz M , Migliavacca M , Reichenbach JR , Reichstein M , Römermann C , Wittig A . A virtual “Werkstatt” for digitization in the sciences . Research Ideas and Outcomes . 2020 may 11 ; 6 . Sandve GK , Nekrutenko A , Taylor J , Hovig E . Ten Simple Rules for Reproducible Computational Research . PLOS Computational Biology . 2013 10 ; 9 ( 10 ) : 1 – 4 . https : / / doi . org / 10 . 1371 / journal . pcbi . 1003285 , doi : 10 . 1371 / jour - nal . pcbi . 1003285 . Sayers E . A General Introduction to the E - utilities . Entrez Programming Utilities Help [ Internet ] Bethesda ( MD ) : National Center for Biotechnology Information ( US ) . 2010 ; . SchröderM , KrügerF , SporsS . ReproducibleResearchismorethanPublishingResearchArtefacts : ASystematic Analysis of Jupyter Notebooks from Research Articles . CoRR . 2019 ; abs / 1905 . 00092 . http : / / arxiv . org / abs / 1905 . 00092 . Schwartz R , Dodge J , Smith NA , Etzioni O . Green AI . Communications of the ACM . 2020 nov 17 ; 63 ( 12 ) : 54 – 63 . Shepperd M , Ajienka N , Counsell S . The role and value of replication in empirical software engineering results . Information and Software Technology . 2018 7 ; 99 : 120 – 132 . Siebert S , Machesky LM , Insall RH . Point of view : Overﬂow in science and its implications for trust . Elife . 2015 ; 4 : e10825 . Simmons JP , Nelson LD , Simonsohn U . False - Positive Psychology : Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Signiﬁcant . Psychological Science . 2011 oct 17 ; 22 ( 11 ) : 1359 – 1366 . doi : 10 . 1177 / 0956797611417632 . Taddeo M , Tsamados A , Cowls J , Floridi L . Artiﬁcial intelligence and the climate emergency : Opportunities , challenges , and recommendations . One Earth . 2021 ; 4 ( 6 ) : 776 – 779 . The Economist , Trouble at the lab ; 2013 . https : / / www . economist . com / brieﬁng / 2013 / 10 / 18 / trouble - at - the - lab . Trisovic A , Lau MK , Pasquier T , Crosas M . A large - scale study on research code quality and execution . Scientiﬁc Data . 2022 feb 21 ; 9 ( 1 ) : 60 . Vargo AHS , Gilbert AC . A rank - based marker selection method for high throughput scRNA - seq data . BMC Bioinformatics . 2020 oct 23 ; 21 ( 1 ) : 477 . Waagmeester A , Stupp G , Burgstaller - Muehlbacher S , Good BM , Griﬃth M , Griﬃth O , Hanspers K , Hermjakob H , Hudson T , Hybiske K , Keating SM , Manske M , Mayers M , Mietchen D , Mitraka E , Pico AR , Putman TE , Riutta A , Rosinach NQ , Schriml L , et al . Wikidata as a knowledge graph for the life sciences . eLife . 2020 mar 17 ; 9 . Wang J , Kuo Ty , Li L , Zeller A . Restoring Reproducibility of Jupyter Notebooks . In : 2020 IEEE / ACM 42nd Interna - tional Conference on Software Engineering : Companion Proceedings ( ICSE - Companion ) ; 2020 . p . 288 – 289 . Wang J , Li L , Zeller A . Better code , better sharing : on the need of analyzing jupyter notebooks . In : Proceedings of the ACM / IEEE 42nd International Conference on Software Engineering : New Ideas and Emerging Results ; 2020 . p . 53 – 56 . Willcox A , ReSearchOps : a principled framework and guide to computational reproducibility . Open Science Framework ; 2021 . Willis A , Charlton P , Hirst T . Developing students’ written communication skills with Jupyter notebooks . In : Proceedings of the 51st ACM Technical Symposium on Computer Science Education ; 2020 . p . 1089 – 1095 . Woﬀord MF , Boscoe BM , Borgman CL , Pasquetto IV , Golshan MS . Jupyter notebooks as discovery mechanisms for open science : Citation practices in the astronomy community . Computing in Science & Engineering . 2019 ; 22 ( 1 ) : 5 – 15 . 30 of 31 Computational reproducibility of Jupyter notebooks from biomedical publications Woodbridge M , Jupyter Notebooks and reproducible data science ; 2017 . https : / / markwoodbridge . com / 2017 / 03 / 05 / jupyter - reproducible - science . html . Zhu C , Saha RK , Prasad MR , Khurshid S . Restoring the Executability of Jupyter Notebooks by Automatic Upgrade of Deprecated APIs . In : 2021 36th IEEE / ACM International Conference on Automated Software Engineering ( ASE ) ; 2021 . p . 240 – 252 . doi : 10 . 1109 / ASE51524 . 2021 . 9678889 . 31 of 31