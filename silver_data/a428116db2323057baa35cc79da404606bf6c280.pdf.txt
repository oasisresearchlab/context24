Towards Uniﬁed Data and Lifecycle Management for Deep Learning Hui Miao , Ang Li , Larry S . Davis , Amol Deshpande Department of Computer Science University of Maryland , College Park , MD 20742 { hui , angli , lsd , amol } @ cs . umd . edu Abstract —Deep learning has improved state - of - the - art results in many important ﬁelds , and has been the subject of much research in recent years , leading to the development of several systems for facilitating deep learning . Current systems , however , mainly focus on model building and training phases , while the issues of data management , model sharing , and lifecycle management are largely ignored . Deep learning modeling lifecycle generates a rich set of data artifacts , such as learned parameters and training logs , and comprises of several frequently conducted tasks , e . g . , to understand the model behaviors and to try out new models . Dealing with such artifacts and tasks is cumbersome and largely left to the users . This paper describes our vision and implementation of a data and lifecycle management system for deep learning . First , we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers , and propose a high - level domain speciﬁc language ( DSL ) , inspired by SQL , to raise the abstraction level and accelerate the modeling process . To manage the data artifacts , especially the large amount of checkpointed ﬂoat parameters , we design a novel model versioning system ( dlv ) , and a read - optimized parameter archival storage system ( PAS ) that minimizes storage footprint and accelerates query workloads without losing accuracy . PAS archives versioned models using deltas in a multi - resolution fashion by separately storing the less signiﬁcant bits , and features a novel progressive query ( inference ) evaluation algorithm . Third , we show that archiving versioned models using deltas poses a new dataset versioning problem and we develop e ﬃ cient algorithms for solving it . We conduct extensive experiments over several real datasets from computer vision domain to show the e ﬃ ciency of the proposed techniques . I . I ntroduction Deep learning models , also called deep neural networks ( DNN ) , have dramatically improved the state - of - the - art results for many important reasoning and learning tasks including speech recognition , object recognition , and natural language processing in recent years [ 1 ] . Learned using massive amounts of training data , DNN models have superior generalization capabilities , and the intermediate layers in many deep learning models have been proven useful in providing e ﬀ ective semantic features that can be used with other learning techniques and are applicable to other problems . However , there are many critical large - scale data management issues in learning , storing , sharing , and using deep learning models , which are largely ignored by researchers today , but are coming to the forefront with the increased use of deep learning in a variety of domains . In this paper , we discuss some of those challenges in the con - text of the modeling lifecycle , and propose a comprehensive system to address them . Given the large scale of data involved ( both training data and the learned models themselves ) and Create / Update Model ReferenceModels Train / Test Model Evaluate Model Data & Labels if accuracy is unsatisfactory , repeat ServeModel Fig . 1 . Deep Learning Modeling Lifecycle the increasing need for high - level declarative abstractions , we argue that database researchers should play a much larger role in this area . Although this paper primarily focuses on deep neural networks , similar data management challenges are seen in lifecycle management of others types of ML models like logistic regression , matrix factorization , etc . DNN Modeling Lifecycle and Challenges : Compared with the traditional approach of feature engineering followed by model training [ 2 ] , deep learning is an end - to - end learning approach , i . e . , the features are not given by a human but are learned in an automatic manner from the input data . Moreover , the features are complex and have a hierarchy along with the network representation . This requires less domain expertise and experience from the modeler , but understanding and explaining the learned models is di ﬃ cult ; why even well - studied models work so well is still a mystery and under active research . Thus , when developing new models , changing the learned model ( especially its network structure and hyper - parameters ) becomes an empirical search task . In Fig . 1 , we show a typical deep learning modeling lifecycle ( we present an overview of deep neural networks in the next section ) . Given a prediction task , a modeler often starts from well - known models that have been successful in similar task domains ; she then speciﬁes input training data and output loss functions , and repeatedly adjusts the DNN on operators and connections like Lego bricks , tunes model hyper - parameters , trains and evaluates the model , and repeats this loop until prediction accuracy does not improve . Due to a lack of understanding about why models work , the adjustments and tuning inside the loop are driven by heuristics , e . g . , adjusting hyper - parameters that appear to have a signiﬁcant impact on the learned weights , applying novel layers or tricks seen in recent empirical studies , and so on . Thus , many similar models are trained and compared , and a series of model variants needs to be explored and developed . Due to the expensive learning / training phase , each iteration of the modeling loop takes a long period of time and produces many ( checkpointed ) snapshots of the model . As we noted above , this is a common a r X i v : 1611 . 06224v1 [ c s . D B ] 18 N ov 2016 workﬂow across many other ML models as well . Current systems ( Ca ﬀ e [ 3 ] , Theano , Torch , TensorFlow [ 4 ] , etc . ) mainly focus on model building and training phases , while the issues of data management , model sharing , and lifecycle management are largely ignored . Modelers are required to write external imperative scripts , edit conﬁgurations by hand and manually maintain a manifest of model variations that have been tried out ; not only are these tasks irrelevant to the modeling objective , but they are also challenging and nontrivial due to the complexity of the model as well as large footprints of the learned models . More speciﬁcally , the tasks and data artifacts in the modeling lifecycle expose several systems and data management challenges , which include : • Understanding & Comparing Models : It is di ﬃ cult to keep track of the many models developed and / or understand the di ﬀ erences amongst them . Di ﬀ erences among both the metadata about the model ( training sample , hyperparameters , network structure , etc . ) , as well as the actual learned param - eters , are of interest . It is common to see a modeler write all conﬁgurations in a spreadsheet to keep track of temporary folders of input , setup scripts , snapshots and logs , which is not only a cumbersome but also an error - prone process . • Repetitive Adjusting of Models : The development life - cycle itself has time - consuming repetitive sub - steps , such as adding a layer at di ﬀ erent places to adjust a model , searching through a set of hyper - parameters for the di ﬀ erent variations , reusing learned weights to train models , etc . , which currently have to be performed manually . • Model Versioning : Similar models are possibly trained and run multiple times , reusing others’ weights as initialization , either because of a changed input or discovery of an error . There is thus a need to keep track of multiple model versions and their relationships over time , although the utilities of di ﬀ erent models are very di ﬀ erent . • Parameter Archiving : The storage footprint of deep learn - ing models tends to be very large . Recent top - ranked models in the ImageNet task have billions of ﬂoating - point param - eters and require hundreds of MBs to store one snapshot during training . Due to resource constraints , the modeler has to limit the number of snapshots , even drop all snapshots of a model at the cost of retraining when needed . • Reasoning about Model Results : Another key data artifact that often needs to be reasoned about is the results of running a learned model on the training or testing dataset . By comparing the results across di ﬀ erent models , a modeler can get insights into di ﬃ cult training examples or understand cor - relations between speciﬁc adjustments and the performance . In addition , although not a focus of this paper , sharing and reusing models is not easy , especially because of the large model sizes and specialized tools used for learning and mod - eler generated scripts in the lifecycle . ModelHub : In this paper , we propose the M odel H ub system to address these challenges . The M odel H ub system is not meant to replace popular training - focused DNN systems , but rather designed to be used with them to accelerate modeling tasks and manage the rich set of lifecycle artifacts . It consists of three key components : ( a ) a model versioning system ( DLV ) to store , query and aid in understanding the models and their versions , ( b ) a model network adjustment and hyper - parameter tuning domain speciﬁc language ( DQL ) to serve as an abstraction layer to help modelers focus on the creation of the models , ( c ) a hosted deep learning model sharing system ( M odel H ub ) to exchange DLV repositories and enable publishing , discovering and reusing models from others . The key features and innovative design highlights of M odel H ub are : ( a ) We use a git - like VCS interface as a familiar user interface to let the modeler manage and explore the created models in a repository , and an SQL - like model enumeration DSL to aid modelers in making and examining multiple model adjustments easily . ( b ) Behind the declarative constructs , M odel H ub manages di ﬀ erent artifacts in a split back - end storage : structured data , such as network structure , training logs of a model , lineages of di ﬀ erent model versions , output results , are stored in a relational database , while learned ﬂoat - point parameters of a model are viewed as a set of ﬂoat matrices and managed in a read - optimized archival storage ( PAS ) . ( c ) Parameters dominate the storage footprint and ﬂoats are well - known at being di ﬃ cult to compress . We study PAS implementation thoroughly under the context of DNN query workload and advocate a segmented approach to store the learned parameters , where the low - order bytes are stored independently of the high - order bytes . We also develop novel model evaluation schemes to use high order bytes solely and progressively uncompress less - signiﬁcant chunks if needed to ensure the correctness of an inference query . ( d ) Due to the di ﬀ erent utility of developed models , archiving versioned mod - els using parameter matrix deltas exhibits a new type of dataset versioning problem which not only optimizes between storage and access tradeo ﬀ but also has model - level constraints . ( e ) Finally , the VCS model repository design extends naturally to a collaborative format and online system which contain rich model lineages and enables sharing , reusing , reproducing DNN models which are compatible across training systems . Contributions : Our key research contributions are : • We propose the ﬁrst comprehensive DNN lifecycle man - agement system , study its design requirements , and propose declarative constructs ( DLV and DQL ) to provide high - level abstractions . • We develop PAS , a read - optimized archival storage system for dealing with a large collection of versioned ﬂoat matrices . • We formulate a new dataset versioning problem with co - usage constraints , analyze its complexity , and design e ﬃ cient algorithms for solving it . • We develop a progressive , approximate query evaluation scheme that avoids reading low - order bytes of the parameter matrices unless necessary . • We present a comprehensive evaluation of M odel H ub that shows the proposed techniques are useful in real life models , and scale well on synthetic models . Outline : In Section II , we provide background on related topics in DNN modeling lifecycle . In Section III , we present an overview of M odel H ub , and discuss the declarative interfaces . We describe the parameter archival store ( PAS ) in Section IV , present an experimental evaluation in Section V , and closely related work in Section VI . II . B ackground To support our design decisions , we overview the artifacts and common task practices in DNN modeling lifecycle . 12 (cid:1) 12 (cid:1) 20 (cid:1) 5 (cid:1) 5 (cid:1) 24 (cid:1) 24 (cid:1) 20 (cid:1) 2 (cid:1) 2 (cid:1) 1 (cid:1) 4 (cid:1) 4 (cid:1) 50 (cid:1) 500 (cid:1) 10 (cid:1) 50 (cid:1) 8 (cid:1) 8 (cid:1) 2 (cid:1) 2 (cid:1) 28 (cid:1) 28 (cid:1) 5 (cid:1) 5 (cid:1) conv1 pool1 ( non - parametric ) conv2 pool2 ( non - parametric ) ip1 ip2 W conv2 2 R 501 ⇥ 50 Fig . 2 . Anatomy of A DNN Model ( LeNet ) Deep Neural Networks : A deep learning model is a deep neural network ( DNN ) consisting of many layers having nonlinear activation functions that are capable of represent - ing complex transformations between input data and desired output . Let D denote a data domain and O denote a prediction label domain ( e . g . , D may be a set of images ; O may be the names of the set of objects we wish to recognize , i . e , labels ) . As with any prediction model , a DNN is a mapping function f : D → O that minimizes a certain loss function L , and is of the following form : f 0 = σ 0 ( W 0 d + b 0 ) d ∈ D f i = σ i ( W i f i − 1 + b i ) 0 < i ≤ n L ( f n , l d ) l d ∈ O d f 0 f 1 f 2 ˆ l d Here i denotes the layer number , ( W i , b i ) are learnable weights and bias parameters in layer i , and σ i is an activation function that non - linearly transforms the result of the previous layer ( common activation functions include sigmoid , ReLU , etc . ) . Given a learned model and an input d , applying f 0 , f 1 , . . . , f n in order gives us the prediction label for that input data . In the training phase , the model parameters are learned by minimizing L ( f n , l d ) , typically done through iterative methods , such as stochastic gradient descent . Fig . 2 shows a classic convolutional DNN , LeNet . LeNet is proposed to solve a prediction task from handwritten images to digit labels { 0 · · · 9 } . In the ﬁgure , a cube represents an inter - mediate tensor , while the dotted lines are unit transformations between tensors . More formally , a layer , L i : ( W , H , X ) (cid:55)→ Y , is a function which deﬁnes data transformations from tensor X to tensor Y . W are the parameters which are learned from the data , and H are the hyperparameters which are given beforehand . A layer is non - parametric if W = ∅ . In the computer vision community , the layers deﬁning transformations are considered building blocks of a DNN model , and referred to using a conventional name , such as full layer , convolution layer , pool layer , normalization layer , etc . The chain is often called the network architecture . The LeNet architecture has two convolution layers , each followed by a pool layer , and two full layers , shown with layer shapes and hyperparameters in Fig . 2 . Moreover , winning models in recent ILSVRC ( ImageNet Large Scale Vision Recognition Competitions ) are shown in Table I , with their architectures described by a composition of common layers in regular expressions syntax for illustrating the similarities ( Note the activation functions and detailed connections are omitted ) . Network Architecture ( in regular expression ) | W | ( ﬂops ) LeNet [ 6 ] ( L conv L pool ) { 2 } L ip { 2 } 4 . 31 × 10 5 AlexNet [ 7 ] ( L conv L pool ) { 2 } ( L conv { 2 } L pool ) { 2 } L ip { 3 } 6 × 10 7 VGG [ 8 ] ( L conv { 2 } L pool ) { 2 } ( L conv { 4 } L pool ) { 3 } L ip { 3 } 1 . 96 × 10 10 ResNet [ 9 ] ( L conv L pool ) ( L conv ) { 150 } L pool L ip 1 . 13 × 10 10 TABLE I . P opular CNN M odels for O bject R ecognition DNN models are learned from massive data based on some architecture , and modern successful computer vision DNN ar - chitectures consist of a large number of ﬂoat weight parameters ( ﬂops ) shown in Table I , resulting in large storage footprints ( GBs ) and long training times ( often weeks ) . Furthermore , the training process is often checkpointed and variations of models need to be explored , leading to many model copies . Modeling Data Artifacts : Unlike many other prediction methods , DNN modeling results in a very large number of weight parameters , a rich set of hyperparameters , and learning measurements , which are used in unique ways in practice , resulting in a mixture of structured data , ﬁles and binary ﬂoating number artifacts : • Non - convexity & Hyperparameters : A DNN model is typically non - convex , and { W } is a local optimum of the un - derlying loss - minimization problem . Optimization procedure employs many tricks to reach a solution quickly [ 5 ] . The set of hyperparameters ( e . g . , learning rate , momentum ) w . r . t . to the optimization algorithm need to be maintained . • Iterations & Measurements : Models are trained iteratively and checkpointed periodically due to the long running times . A set of learning measurements are collected in various logs , including objective loss values and accuracy scores . • Fine - tuning & Snapshots : Well - known models are often learned from massive real - world data ( ImageNet ) , and re - quire large amounts of resources to train ; when prediction tasks do not vary much ( e . g . , animal recognition vs dog recognition ) , the model parameters are reused as initializa - tions and adjusted using new data ; this is often referred to as ﬁne - tuning . On the other hand , not all snapshots can be simply deleted , as the convergence is not monotonic . • Provenance & Arbitrary Files : Alternate ways to con - struct architectures or to set hyperparameters lead to human - in - the - loop model adjustments . Initialization , preprocessing schemes , and hand - crafted scripts are crucial provenance information to explore models and reproduce results . Model Adjustment : In a modeling lifecycle for a prediction task , the update - train - evaluate loop is repeated in daily work , and many model variations are adjusted and trained . In general , once data and loss are determined , model adjustment can be done in two orthogonal steps : a ) network architecture adjust - ments where layers are dropped or added and layer function templates are varied , and b ) hyperparameter selections , which a ﬀ ect the behavior of the optimization algorithms . There is much work on search strategies to enumerate and explore both . Model Sharing : Due to the good generalizability , long train - ing times , and verbose hyperparameters required for large DNN models , there is a need to share the trained models . Jia et al . [ 3 ] built an online venue ( Ca ﬀ e Model Zoo ) to share ModelHub Client Hosted DLV Repositories Server ModelSearch Model Publish Model Learning Module DQL Module DQL Parser & Optimizer DLV Module Command - line & UI toolkit caffe Wrapper publish reuse manage versions explore models enumerate models Modeler Local DLV Repository PAS : Parameter Archival Store git repo facade metadata catalog Fig . 3 . M odel H ub System Architecture models . Brieﬂy , Model Zoo is part of a github repository 1 with a markdown ﬁle edited collaboratively . To publish models , modelers add an entry with links to download trained parame - ters in caffe format . Apart from the ca ﬀ e community , similar initiatives are in place for other training systems . III . M odel H ub S ystem O verview We show the M odel H ub architecture including the key components and their interactions in Fig . 3 . At a high level , the M odel H ub functionality is divided among a local component and a remote component . The local functionality includes the integration with popular DNN systems such as caffe , torch , tensorflow , etc . , on a local machine or a cluster . The remote functionality includes sharing of models , and their versions , among di ﬀ erent groups of users . We primarily focus on the local functionality in this paper . On the local system side , DLV is a version control system ( VCS ) implemented as a command - line tool ( dlv ) , that serves as an interface to interact with the rest of the local and remote components . Use of a specialized VCS instead of a general - purpose VCS such as git or svn allows us to better portray and query the internal structure of the artifacts generated in a modeling lifecycle , such as network deﬁnitions , training logs , binary weights , and relationships between models . The key utilities of dlv are listed in Table III - A , grouped by their purpose ; we explain these in further detail in Sec . III - B . DQL is a DSL we propose to assist modelers in deriving new models ; the DQL query parser and optimizer components in the ﬁgure are used to support this language . The model learning module interacts with external deep learning tools that the modeler uses for training and testing . They are essentially wrappers on speciﬁc DNN systems that extract and reproduce modeling artifacts . Finally , the M odel H ub service is a hosted toolkit to support publishing , discovering and reusing models , and serves similar role for DNN models as github for software development or DataHub for data science [ 10 ] . A . Data Model M odel H ub works with two data models : a conceptual DNN model , and a data model for the versions in a DLV repository . DNN Model : A DNN model can be understood in di ﬀ erent ways , as one can tell from the di ﬀ erent model creation APIs in popular deep learning systems . In the formulation mentioned in Sec . I , if we view a function f i as a node and dependency 1 Ca ﬀ e Model Zoo : https : / / github . com / BVLC / ca ﬀ e / wiki / Model - Zoo Type Command Description model version management init Initialize a dlv repository . add Add model ﬁles to be committed . commit Commit the added ﬁles . copy Sca ﬀ old model from an old one . archive Archive models in the repository . model exploration list List models and related lineages . desc Describe a particular model . diff Compare multiple models . eval Evaluate a model with given data . model enumeration query Run DQL clause . remote interaction publish Publish a model to ModelHub . search Search models in ModelHub . pull Download from ModelHub . TABLE II . A list of key dlv utilities . relationship ( f i , f i − 1 ) as an edge , it becomes a directed acyclic graph ( DAG ) . Depending on the granularity of the function in the DAG , either at the tensor arithmetic operator level ( add , multiply ) , or at a logical composition of those operators ( convolution layer , full layer ) , it forms di ﬀ erent types of DAGs . In M odel H ub , we consider a DNN model node as a composition of unit operators ( layers ) , often adopted by computer vision models . The main reason for this decision is that we focus on productivity improvement in the lifecycle , rather than implementation e ﬃ ciencies of training and testing . VCS Data Model : When managing DNN models in the VCS repository , a model version represents the contents in a single version . It consists of a network deﬁnition , a collection of weights ( each of which is a value assignment for the weight parameters ) , a set of extracted metadata ( such as hyper - parameter , accuracy and loss generated in the training phase ) , and a collection of ﬁles used together with the model instance ( e . g . , scripts , datasets ) . In addition , we enforce that a model version must be associated with a human readable name for better utility , which reﬂects the logical groups of a series of improvement e ﬀ orts over a DNN model in practice . In the implementation , model versions can be viewed as a relation model version ( name , id , N , W , M , F ) , where id is part of the primary key of model versions and is auto - generated to distinguish model versions with the same name . In brief , N , W , M , F are the network deﬁnition , weight values , extracted metadata and associated ﬁles respectively . The DAG , N , is stored as two tables : Node ( id , node , A ) , where A is a list of attributes such as layer name , and Edge ( from , to ) . W is managed in our learned parameter storage ( PAS , Sec . IV ) . M , the metadata , captures the provenance information of training and testing a particular model ; it is extracted from training logs by the wrapper module , and includes the hyperparameters when training a model , the loss and accuracy measures at some iterations , as well as dynamic parameters in the optimization process , such as learning rate at some iterations . Finally , F is ﬁle list marked to be associated with a model version , including data ﬁles , scripts , initial conﬁgurations , and etc . Besides a set of model version s , the lineage of the model versions are captured using a separate parent ( base , derived , commit ) relation . All of these relations are maintained / updated in a relational backend when the modeler runs the di ﬀ erent dlv commands that update the repository . B . Query Facilities Once the DNN models and their relationships are managed in DLV , the modeler can interact with them easily . The query facilities we provide can be categorized into two types : a ) model exploration queries and b ) model enumeration queries . 1 ) Model Exploration Queries : Model exploration queries interact with the models in a repository , and are used to understand a particular model , to query lineages of the models , and to compare several models . For usability , we design it as query templates via dlv sub - command , similar to other VCS . List Models & Related Lineages : By default , the query lists all versions of all models including their commit descriptions and parent versions ; it also takes options , such as showing results for a particular model , or limiting the number of versions to be listed . dlv list [ - - model _ name ] [ - - commit _ msg ] [ - - last ] Describe Model : dlv desc shows the extracted metadata from a model version , such as the network deﬁnition , learnable parameters , execution footprint ( memory and runtime ) , activa - tions of convolutional DNNs , weight matrices , and evaluation results across iterations . Note the activation is the intermediate output of a DNN model in computer vision and often used as an important tool to understand the model . The current output formats are a result of discussions with computer vision modelers to deliver tools that ﬁt their needs . In addition to printing to console , the query supports HTML output for displaying the images and visualizing the weight distribution . dlv desc [ - - model _ name | - - version ] [ - - output ] Compare Models : dlv diff takes a list of model names or version ids and allows the modeler to compare the DNN models . Most of desc components are aligned and returned in the query result side by side . dlv diff [ - - model _ names | - - versions ] [ - - output ] Evaluate Model : dlv eval runs test phase of the managed models with an optional conﬁg specifying di ﬀ erent data or changes in the current hyper - parameters . The main usages of exploration query are two - fold : 1 ) for the users to get familiar with a new model , 2 ) for the user to test known models on di ﬀ erent data or settings . The query returns the accuracy and optionally the activations . It is worth pointing out that complex evaluations can be done via model enumeration queries in DQL . dlv eval [ - - model _ name | - - versions ] [ - - config ] 2 ) Model Enumeration Queries : Model enumeration queries are used to explore variations of currently available models in a repository by changing network structures or tuning hyper - parameters . There are several operations that need to be done in order to derive new models : 1 ) Select models from the repository to improve ; 2 ) Slice particular models to get reusable components ; 3 ) Construct new models by mutating the existing ones ; 4 ) Try the new models on di ﬀ erent hyper - parameters and pick good ones to save and work with . When enumerating models , we also want to stop exploration of bad models early . To support this rich set of requirements , we propose the DQL domain speciﬁc language , that can be executed using “ dlv query ” . Challenges of designing the language are : a ) the data model is a mix of relational and the graph data models select m1 where m1 . name like " alexnet _ % " and m1 . creation _ time > " 2015 - 11 - 22 " and m1 [ " conv [ 1 , 3 , 5 ] " ] . next has POOL ( " MAX " ) Query 1 . DQL select query to pick the models . slice m2 from m1 where m1 . name like " alexnet - origin % " mutate m2 . input = m1 [ " conv1 " ] and m2 . output = m1 [ " fc7 " ] Query 2 . DQL slice query to get a sub - network . construct m2 from m1 where m1 . name like " alexnet - avgv1 % " and m1 [ " conv * ( $ 1 ) " ] . next has POOL ( " AVG " ) mutate m1 [ " conv * ( $ 1 ) " ] . insert = RELU ( " relu $ 1 " ) Query 3 . DQL construct query to derive more models on existing ones . evaluate m from " query3 " with config = " path to config " vary config . base _ lr in [ 0 . 1 , 0 . 01 , 0 . 001 ] and config . net [ " conv * " ] . lr auto and config . input _ data in [ " path1 " , " path2 " ] keep top ( 5 , m [ " loss " ] , 100 ) Query 4 . DQL evaluate query to enumerate models with di ﬀ erent network architectures , search hyper - parameters , and eliminate models . and b ) the enumeration includes hyper - parameter tuning as well as network structure mutations , which are very di ﬀ erent operations . We omit a thorough explanation of the language due to space constraints , and instead show the key operators and constructs along with a set of examples ( Query 1 ∼ 4 ) to show how requirements are met . Key Operators : We adopt the standard SQL syntax to interact with the repository . DQL views the repository as a single model version table . A model version instance is a DAG , which can be viewed as object types in modern SQL conventions . In DQL , attributes can be referenced using attribute names ( e . g . m1 . name , m1 . creation _ time , m2 . input , m2 . output ) , while navigating the internal structures of the DAG , i . e . the Node and Edge EDB , we provide a regexp style selector operator on a model version to access individual DNN nodes , e . g . m1 [ " conv [ 1 , 3 , 5 ] " ] in Query 1 ﬁlters the nodes in m1 . Once the selector operator returns a set of nodes , prev and next attributes of the node allow 1 - hop traversal in the DAG . Note that POOL ( " MAX " ) is one of the standard built - in node templates for condition clauses . Using SPJ operators with object type attribute access and the selector operator , we allow relational queries to be mixed with graph traversal conditions . To retrieve reusable components in a DAG , and mutate it to get new models , we provide slice , construct and mutate operators . Slice originates in programming analysis research ; given a start and an end node , it returns a subgraph including all paths from the start to the end and the connections which are needed to produce the output . Construct can be found in graph query languages such as SPARQL to create new graphs . We allow construct to derive new DAGs by using selected nodes to insert nodes by splitting an outgoing edge or to delete an outgoing edge connecting to another node . . . . . . . . . . . . . . . . . . . . . . s 1 s 2 latest snapshots model version one snapshot Fig . 4 . Relationships of Model Versions and Snapshots Mutate limits the places where insert and delete can occur . For example , Query 2 and 3 generate reusable subgraphs and new graphs . Query 2 slices a sub - network from matching models between convolution layer ‘conv1’ and full layer ‘fc7’ , while Query 3 derives new models by appending a ReLU layer after all convolution layers followed by an average pool . All queries can be nested . Finally , evaluate can be used to try out new models , with potential for early out if expectations are not reached . We separate the network enumeration component from the hyper - parameter turning component ; while network enumeration can be nested in the from clause , we introduce a with opera - tor to take an instance of a tuning conﬁg template , and a vary operator to express the combination of activated multi - dimensional hyper - parameters and search strategies . auto is keyword implemented using default search strategies ( currently grid search ) . To stop early and let the user control the stopping logic , we introduce a keep operator to take a rule consisting of stopping condition templates , such as top - k of the evaluated models , or accuracy threshold . Query 4 evaluates the models constructed and tries combinations of at least three di ﬀ erent hyper - parameters , and keeps the top 5 models w . r . t . the loss after 100 iterations . C . ModelHub Implementation On the local side , the current implementation of M od - el H ub maintains the data model in multiple back - ends and utilizes git to manage the arbitrary ﬁle di ﬀ s . Various queries are decomposed and sent to di ﬀ erent backends and chained accordingly . On the other hand , as the model repository is standalone , we host the repositories as a whole in a M odel H ub service . The modeler can use the dlv publish to push the repository for archiving , collaborating or sharing , and use dlv search and dlv pull to discover and reuse remote models . We envision such a form of collaboration can facilitate a learning environment , as all versions in the lifecycle are accessible and understandable with ease . IV . P arameter archival storage ( PAS ) Modeling lifecycle for DNNs , and machine learning mod - els in general , is centered around the learned parameters , whose storage footprint can be very large . The goal of PAS is to maintain a large number of learned models as compactly as possible , without compromising on the query performance . Before introducing our design , we ﬁrst discuss the queries of interest , and some key properties of the model artifacts . We then describe di ﬀ erent options to store a single ﬂoat matrix , and to construct deltas ( di ﬀ erences ) between two matrices . We then formulate the optimal version graph storage problem , discuss how it di ﬀ ers from the prior work , and present algo - rithms for solving it . Finally , we develop a novel approximate model evaluation technique , suitable for the segmented storage technique that PAS uses . A . Weight Parameters & Query Type of Interests We illustrate the key weight parameter artifacts and the relationships among them in Fig . 4 , and also explain some of the notations used in this section . At a high level , the predecessor - successor relationships between all the developed models is captured as a version graph . These relationships are user - speciﬁed and conceptual in nature , and the interpretation is left to the user ( i . e . , an edge v i → v j indicates that v j was an updated version of the model that the user checked in after v i , but the nature of this update is irrelevant for storage purposes ) . A model version v i itself consists of a series of snapshots , s 1 , . . . , s n , which represent checkpoints during the training process ( most systems will take such snapshots due to the long running times of the iterations ) . We refer the last or the best checkpointed snapshot s n as the latest snapshot of v i , and denote it by s v i . One snapshot , in turn , consists of intermediate data X and trained parameters W ( e . g . , in Fig . 2 , the model has 431080 parameters for W , and 19694 · b dimensions for X , where b is the minibatch size ) . Since X is useful only if training needs to be resumed , only W is stored in PAS . Outside of a few rare exceptions , W can always be viewed as a collection of ﬂoat matrices , R m × n , m ≥ 1 , n ≥ 1 , which encode the weights on the edges from outputs of the neurons in one layer to the inputs of the neurons in the next layer . Thus , we treat a ﬂoat matrix as a ﬁrst class data type in PAS 2 . The retrieval queries of interest are dictated by the oper - ations that are done on these stored models , which include : ( a ) testing a model , ( b ) reusing weights to ﬁne - tune other models , ( c ) comparing parameters of di ﬀ erent models , ( d ) comparing the results of di ﬀ erent models on a dataset , and ( e ) model exploration queries ( Sec . III - B ) . Most of these operations require execution of group retrieval queries , where all the weight matrices in a speciﬁc snapshot need to be retrieved . This is di ﬀ erent from range queries seen in array databases ( e . g . , SciDB ) , and also have unique characteristics that inﬂuence the storage and retrieval algorithms . • Similarity among Fine - tuned Models : Although non - convexity of the training algorithm and di ﬀ erences in net - work architectures across models lead to non - correlated parameters , the widely - used ﬁne - tuning practices ( Sec . II ) generate model versions with similar parameters , resulting in e ﬃ cient delta encoding schemes . • Co - usage constraints : Prior work on versioning and re - trieval [ 11 ] has focused on retrieving a single artifact stored in its entirety . However , we would like to store the di ﬀ erent matrices in a snapshot independently of each other , but we must retrieve them together . These co - usage constraints make the prior algorithms inapplicable as we discuss later . 2 We do not make a distinction about the bias weight ; the typical linear transformation W (cid:48) x + b is treated as W · ( x , 1 ) = ( W (cid:48) , b ) T · ( x , 1 ) . • Low Precision Tolerance : DNNs are well - known for their tolerance to using low - precision ﬂoating point numbers ( Sec . VI ) , both during training and evaluation . Further , many types of queries ( e . g . , visualization and comparisons ) do not require retrieving the full - precision weights . • Unbalanced Access Frequencies : Not all snapshots are used frequently . The latest snapshots with the best testing accuracy are used in most of the cases . The checkpointed snapshots have limited usages , including debugging and comparisons . B . Parameters As Segmented Float Matrices Float Data Type Schemes : Although binary ( 1 / - 1 ) or ternary ( 1 / 0 / - 1 ) matrices are sometimes used in DNNs , in general PAS handles real number weights . Due to di ﬀ erent usages of snapshots , PAS o ﬀ ers a handful of ﬂoat representations to let the user trade - o ﬀ storage e ﬃ ciency with lossyness using dlv . • Float Point : DNNs are typically trained with single pre - cision ( 32 bit ) ﬂoats . This scheme uses the standard IEEE 754 ﬂoating point encoding to store the weights with sign , exponent , and mantissa bits . IEEE half - precision proposal ( 16 bits ) and tensorﬂow truncated 16bits [ 4 ] are supported as well and can be used if desired . • Fixed Point : Fixed point encoding has a global exponent per matrix , and each ﬂoat number only has sign and mantissa using all k bits . This scheme is a lossy scheme as tail positions are dropped , and a maximum of 2 k di ﬀ erent values can be expressed . The entropy of the matrix also drops considerably , aiding in compression . • Quantization : Similarly , PAS supports quantization using k bits , k ≤ 8 , where 2 k possible values are allowed . The quantization can be done in random manner or uniform manner by analyzing the distribution , and a coding table is used to maintain the integer codes stored in the matrices in PAS . This is most useful for snapshots whose weights are primarily used for ﬁne - tuning or initialization . The ﬂoat point schemes present here are not new , and are used in DNN systems in practice [ 12 ] , [ 13 ] , [ 14 ] . As a lifecycle management tool , PAS lets experienced users select schemes rather than deleting snapshots due to resource constraints . Our evaluation shows storage / accuracy tradeo ﬀ s of these schemes . Bytewise Segmentation for Float Matrices : One challenge for PAS is the high entropy of ﬂoat numbers in the ﬂoat arithmetic representations , which leads to them being very hard to compress . Compression ratio shown in related work for scientiﬁc ﬂoat point datasets , e . g . , simulations , is very low . The state of art compression schemes do not work well for DNN parameters either ( Sec . VI ) . By exploiting DNN low - precision tolerance , we adopt bytewise decomposition from prior work [ 15 ] , [ 16 ] and extend it to our context to store the ﬂoat matrices . The basic idea is to separate the high - order and low - order mantissa bits , and so a ﬂoat matrix is stored in multiple chunks ; the ﬁrst chunk consists of 8 high - order bits , and the rest are segmented one byte per chunk . One major advantage is the high - order bits have low entropy , and standard compression schemes ( e . g . , zlib ) are e ﬀ ective for them . Apart from the simplicity of the approach , the key beneﬁts of segmented approach are two - fold : ( a ) it allows o ﬄ oading low - order bytes to remote storage , ( b ) PAS queries can read high - order bytes only , in exchange for tolerating small errors . s1 s2 m1 m2 m3 v0 m4 ( 2 , 1 ) ( 8 , 2 ) ( 1 , 0 . 5 ) ( 4 , 1 ) ( 2 , 1 ) ( 8 , 2 ) m5 ( 4 , 1 ) ( 4 , 1 ) ( 8 , 2 ) ( a ) Matrix Storage Graph s1 s2 m1 m2 m3 v0 m4 ( 2 , 1 ) ( 8 , 2 ) ( 1 , 0 . 5 ) ( 4 , 1 ) m5 ( 4 , 1 ) ( b ) Optimal Plan without Constraints s1 s2 m1 m2 m3 v0 m4 ( 2 , 1 ) ( 8 , 2 ) ( 2 , 1 ) ( 8 , 2 ) m5 ( 4 , 1 ) ( c ) Optimal Plan with C r ψ i ( s 1 ) ≤ 3 ∧ C r ψ i ( s 2 ) ≤ 6 Fig . 5 . Illustrating Matrix Storage Graph & Plan using a Toy Example Comparison and exploration queries ( dlv desc , dlv diff ) can easily tolerate such errors and , as we show in this paper , dlv eval queries can also be made tolerant to these errors . Delta Encoding Across Snapshots : We observed that , due to the non - convexity in training , even re - training the same model with slightly di ﬀ erent initializations results in very di ﬀ erent pa - rameters . However , the parameters from checkpoint snapshots for the same or similar models tend to be close to each other . Furthermore , across model versions , ﬁne - tuned models gener - ated using ﬁxed initializations from another model often have similar parameters . The observations naturally suggest use of delta encoding between checkpointed snapshots in one model version and latest snapshots across multiple model versions ; i . e . , instead of storing all matrices in entirety , we can store some in their entirety and others as di ﬀ erences from those . Two possible delta functions ( denoted (cid:9) ) are arithmetic subtraction and bitwise XOR 3 . We ﬁnd the compression footprints when applying the di ﬀ (cid:9) in di ﬀ erent directions are similar . We study the delta operators on real models in Sec . V . C . Optimal Parameter Archival Storage Given the above background , we next address the question of how to best store a collection of model versions , so that the total storage footprint occupied by the large segmented ﬂoat matrices is minimized while the retrieval performance is not compromised . This recreation / storage tradeo ﬀ sits at the core of any version control system . In recent work [ 11 ] , the authors study six variants of this problem , and show the NP - hardness of most of those variations . However , their techniques cannot be directly applied in PAS , primarily because their approach is not able to handle the group retrieval ( co - usage ) constraints . We ﬁrst introduce the necessary notation , discuss the di ﬀ erences from prior work , and present the new techniques we developed for PAS . In Fig . 4 , a model version v ∈ V consists of time - ordered checkpointed snapshots , S v = s 1 , . . . , s n . Each snapshot , s i consists of a named list of ﬂoat matrices M v , i = { m k } representing the learned parameters . All matrices in a repository , M = (cid:83) v ∈ V (cid:83) s i ∈ S v M v , i , are the parameter artifacts to archive . Each matrix m ∈ M is either stored directly , or is recovered through another matrix m (cid:48) ∈ M via a delta operator (cid:9) , i . e . m = m (cid:48) (cid:9) d , where d is the delta computed using one of the techniques discussed above . In the latter case , the matrix d is stored instead of m . To unify the two cases , we introduce a empty matrix ν 0 , and deﬁne ∀(cid:9)∀ m ∈ M , m (cid:9) ν 0 = m . 3 Delta functions for matrices with di ﬀ erent dimensions are discussed in the long version of the paper ; techniques in Sec IV work with minor modiﬁcation . Deﬁnition 1 ( Matrix Storage Graph ) : Given a repository of model versions V , let ν 0 be an empty matrix , and V = M ∪ { ν 0 } be the set of all parameter matrices . We denote by E = { m i (cid:9) m j } ∪ { m i (cid:9) ν 0 } the available deltas between all pairs of matrices . Abusing notation somewhat , we also treat E as the set of all edges in a graph where V are the vertices . Finally , let G V ( V , E , c s , c r ) denote the matrix storage graph of V , where edge weights c s , c r : E (cid:55)→ R + are storage cost and recreation cost of an edge respectively . Deﬁnition 2 ( Matrix Storage Plan ) : Any connected sub - graph of G V ( V , E ) is called a matrix storage plan for V , and denoted by P V ( V P , E P ) , where V P = V and E P ⊆ E . Example 1 : In Fig . 5 ( a ) , we show a matrix storage graph for a repository with two snapshots , s 1 = { m 1 , m 2 } and s 2 = { m 3 , m 4 , m 5 } . The weights associated with an edge e = ( ν 0 , m i ) reﬂect the cost of materializing the matrix m i and retrieving it directly . On the other hand , for an edge between two matrices , e . g . , e = ( m 2 , m 5 ) , the weights denote the storage cost of the corresponding delta and the recreation cost of applying that delta . In Fig . 5 ( b ) and 5 ( c ) , two matrix storage plans are shown . For a matrix storage plan P V ( V P , E P ) , PAS stores all its edges and is able to recreate any matrix m i follow - ing a path starting from ν 0 . The total storage cost of P V , denoted as C s ( P V ) , is simply the sum of edge stor - age costs , i . e . C s ( P V ) = (cid:80) e ∈E P c s ( e ) . Computation of the average snapshot recreation cost is more involved and de - pends on the retreival scheme used : • Independent scheme recreates each matrix m i one by one by following the shortest path ( Υ ν 0 , m i ) to m i from ν 0 . In that case , the recreation cost is simply computed by summing the recreation costs for all the edges along the shortest path . • Parallel scheme accesses all matrices of a snapshot in parallel ( using multiple threads ) ; the longest shortest path from ν 0 deﬁnes the recreation cost for the snapshot . • Reusable scheme considers caching deltas on the way , i . e . , if paths from ν 0 to two di ﬀ erent matrices overlap , then the shared computation is only done once . In that case , we need to construct the lowest - cost Steiner tree ( T P V , s i ) involving ν 0 and the matrices in the snapshot . However , because multiple large matrices need to be kept in memory simultaneously , the memory consumption of this scheme can be large . Retrieval Scheme Recreation C r ψ ( P V , s i ) Solution of Prob . 1 Independent ( ψ i ) (cid:80) mj ∈ si (cid:80) ek ∈ Υ ν 0 , mj c r ( e k ) Spanning tree Parallel ( ψ p ) max mj ∈ si { (cid:80) ek ∈ Υ ν 0 , mj c r ( e k ) } Spanning tree Reusable ( ψ r ) (cid:80) ek ∈TP V , si c r ( e k ) Subgraph TABLE III . R ecreation C ost of a S napshot s i C r ( P V , s i ) in a plan P V PAS can be conﬁgured to use any of these options during the actual query execution . However , solving the storage opti - mization problem with Reusable scheme is nearly impossible ; since the Steiner tree problem is NP - Hard , just computing the cost of a solution becomes intractable making it hard to even compare two di ﬀ erent storage solutions . Hence , during the storage optimization process , PAS can only support Inde - pendent or Parallel schemes . In the example above , the edges are shown as being undi - rected indicating that the deltas are symmetric . In general , we allow for directed deltas to handle asymmetric delta functions , and also for multiple directed edges between the same two matrices . The latter can be used to capture di ﬀ erent options for storing the delta ; e . g . , we may have one edge corresponding to a remote storage option , where the storage cost is lower and the recreation cost is higher ; whereas another edge ( between the same two matrices ) may correspond to a local SSD storage option , where the storage cost is the highest and the recreation cost is the lowest . Our algorithms can thus automatically choose the appropriate storage option for di ﬀ erent deltas . Similarly , PAS is able to make decisions at the level of byte segments of ﬂoat matrices , by treating them as separate matrices that need to be retrieved together in some cases , and not in other cases . This , combined with the ability to incorpo - rate di ﬀ erent storage options , is a powerful generalization that allows PAS to make decisions at a very ﬁne granularity . Given this notation , we can now state the problem formally . Since there are multiple optimization metrics , we assume that constraints on the retrieval costs are provided and ask to minimize the storage . Problem 1 ( Optimal Parameter Archival Storage ) : Given a matrix storage graph G V ( V , E , c s , c r ) , let θ i be the snapshot recreation cost budget for each s i ∈ S . Under a retrieval scheme ψ , ﬁnd a matrix storage plan P V ∗ that minimizes the total storage cost , while satisfying recreation constraints , i . e . : minimize P V C s ( P V ) ; s . t . ∀ s i ∈ S , C r ψ ( P V , s i ) ≤ θ i Example 2 : In Fig . 5 ( b ) , without any recreation con - straints , we show the best storage plan , which is the mini - mum spanning tree based on c s of the matrix storage graph , C s ( P V ) = 19 . Under independent scheme ψ i , C r ψ i ( P V , s 1 ) = 3 and C r ψ i ( P V , s 2 ) = 7 . 5 . In Fig . 5 ( c ) , after adding two con - straints θ 1 = 3 and θ 2 = 6 , we shows an optimal storage plan P V ∗ satisfying all constraints . The storage cost increases , C s ( P V ∗ ) = 24 , while C r ψ i ( P V ∗ , s 1 ) = 3 and C r ψ i ( P V ∗ , s 2 ) = 6 . Although this problem variation might look similar to the ones considered in recent work [ 11 ] , none of the variations studied there can handle the co - usage constraints ( i . e . , the constraints on simultaneously retrieving a group of versioned data artifacts ) . One way to enforce such constraints is to treat the entire snapshot as a single data artifact that is stored together ; however , that may force us to use an overall suboptimal solution because we would not be able to choose the most appropriate delta at the level of individual matrices . Another option would be to sub - divide the retrieval budget for a snapshot into constraints on individual matrices in the snapshot . As our experiments show , that can lead to signiﬁ - cantly higher storage utilization . Thus the formulation above is a strict generalization of the formulations considered in that prior work . Theorem 1 : Optimal Parameter Archival Storage Problem is NP - hard for all retrieval schemes in Table III . Proof : We reduce Prob . 5 in [ 11 ] to the independent scheme ψ i , and Prob . 6 in [ 11 ] to the parallel scheme ψ p , by mapping each datasets as vertices in storage graph , and introducing a snapshot holding all matrices with recreation bound Θ g . For reuse scheme ψ r , it is at least as hard as weighted set cover problem if reducing a set to an edge e with storage cost c s ( e ) as weight , an item to an vertex in G V ( V , E ) , and set recreation budget Θ g = ∞ . Lemma 2 : The optimal solution for Problem 1 is a span - ning tree when retrieval scheme ψ is independent or parallel . Proof : Suppose we have a non - tree solution P V satisfying the constraints , and also minimize the objective . Note that parallel and independent schemes are based on shortest path Υ ν 0 , m in P V from ν 0 to each matrix m , so the union of each shortest path forms a shortest path tree . If we remove edges which are not in the shortest path tree from the plan to P V (cid:48) , it results in a lower objective C s ( P V (cid:48) ) , but still satisfying all recreation constraints , which leads to a contradiction . Lemma 2 shows P V ∗ is a spanning tree and connects our prob - lem to a class of constrained minimum spanning tree problems . The above lemma is not true for the reusable scheme ( ψ r ) ; snapshot Steiner trees satisfying di ﬀ erent recreation constraints may share intermediate nodes resulting in a subgraph solution . Constrained Spanning Tree Problem : In Problem 1 , storage cost minimization while ignoring the recreation constraints leads to a minimum spanning tree ( MST ) of the storage matrix ; whereas the snapshot recreation constraints are best satisﬁed by using a shortest path tree ( SPT ) . These problems are often referred to as constrained spanning tree problems [ 17 ] or shallow - light tree constructions [ 18 ] , which have been studied in areas other than dataset versioning , such as VLSI designs . Khuller et al . [ 19 ] propose an algorithm called LAST to construct such a “balanced” spanning tree in an undirected graph G . LAST starts with a minimum spanning tree of the provided graph , traverses it in a DFS manner , and adjusts the tree by changing parents to ensure the path length in constructed solution is within ( 1 + (cid:15) ) times of shortest path in G , i . e . C r ( T , v i ) ≤ ( 1 + (cid:15) ) C r ( Υ ν 0 , v i , v i ) , while total storage cost is within ( 1 + 2 (cid:15) ) times of MST . In our problem , the co - usage constraints of matrices in each snapshot form hyperedges over the graph making the problem more di ﬃ cult . In the rest of the discussion , we adapt meta - heuristics for constrained MST problems to develop two algorithms : the ﬁrst one ( PAS - MT ) is based on an iterative reﬁnement scheme , where we start from an MST and then adjust it to satisfy constraints ; the second one is a priority - based tree construction algorithm ( PAS - PT ) , which adds nodes one by one and encodes heuristic in the priority function . Both algorithms aim to solve the parallel and independent recreation schemes , and thus can also ﬁnd feasible solution for reusable scheme . Due to large memory footprints of intermediate matrices , we leave improving reusable scheme solutions for future work . PAS - MT : The algorithm starts with T as the MST of G V ( V , E ) , and iteratively adjusts T to satisfy the broken snapshot recre - ation constraints , U = { s i | C r ( T , s i ) > θ i } , by swapping one edge at a time . We denote p i as the parent of v i , ( p i , v i ) ∈ T and p 0 = φ , and successors of v i in T as D i . A swap operation on ( p i , v i ) to edge ( v s , v i ) ∈ E − T changes parent of v i to v s in T . Lemma 3 : A swap operation on v i changes storage cost of C s ( T ) by c s ( p i , v i ) − c s ( v s , v i ) , and changes recreation costs of v i and its successors D i by : C r ( T , v i ) − C r ( T , v s ) − c r ( v s , v i ) . The proof can be derived from deﬁnition of C s and C r by Algorithm 1 PAS - MT Input : G V ( V , E , c s , c r ) , snapshots S , recreation cost { θ i ≥ 0 | s i ∈ S } . Output : A spanning tree T satisfying constraints { C r ( T , s i ) ≤ θ i } 1 : let T = MST of G V ( V , E ) ; 2 : while unsatisﬁed constraints U = { s i | C r ( T , s i ) > θ i } (cid:44) ∅ do 3 : for each edge e si = ( v s , v i ) ∈ E − T do 4 : calculate gain ( e si ) with Eq . 1 ( Eq . 2 for scheme ψ p ) 5 : end for 6 : ﬁnd e (cid:48) si = max { e si | gain ( e si ) } 7 : break if gain ( e (cid:48) si ) ≤ 0 8 : swap ( p i , v i ) with e (cid:48) si : T = ( T − { ( p i , v i ) } ) ∪ { e (cid:48) si } 9 : end while 10 : return T unless U (cid:44) ∅ Algorithm 2 PAS - PT Input : G V ( V , E , c s , c r ) , snapshots S , recreation cost { θ i ≥ 0 | s i ∈ S } . Output : A spanning tree T satisfying constraints { C r ( T , s i ) ≤ θ i } 1 : let T = ∅ and Q be a priority queue of edges based on c s 2 : push { ( ν 0 , v i ) | v i ∈ V } in Q 3 : while Q (cid:44) ∅ do 4 : pop e ij = ( v i , v j ) from Q ; let T = T ∪ { e ij } 5 : let constraints satisfaction ﬂag be Θ e ij satisfy = true 6 : for each snapshot constraint s a ∈ { s | s ∈ S ∧ v j ∈ s } do 7 : estimate recreation cost ˆ C r ( T , s a ) 8 : Θ e ij satisfy = false and break if ˆ C r ( T , s a ) > θ a 9 : end for 10 : if Θ e ij satisfy is false , then T = T − { e ij } and goto line 3 11 : pop inner edges of v j I jT = { ( v k , v j ) | v k ∈ T } from Q 12 : push outer edges O j E− T = { ( v j , v k ) | v k ∈ E − T } to Q 13 : for ( v k , v j ) ∈ T , change p k improves C s , and no worse C r do 14 : swap ( p k , v k ) ∈ T with ( v j , v k ) 15 : end for 16 : end while 17 : if T is not a spanning tree then 18 : for each v u ∈ V − V T , do T = T ∪ { e 0 u = ( ν 0 , v u ) } 19 : adjust T using PAS - MT heuristic . 20 : end if 21 : return T if T is a matrix storage plan inspection . When selecting edges in E − T , we choose the one which has the largest marginal gain for unsatisﬁed constraints : ψ i : max ( v s , v i ) ∈E− T { (cid:80) s k ∈ U (cid:80) v j ∈ s k ∩D i ( C r ( T , v i ) − C r ( T , v s ) − c r ( v s , v i ) ) c s ( v s , v i ) − c s ( p i , v i ) } ( 1 ) ψ p : max ( v s , v i ) ∈E− T { (cid:80) s k ∈ U ( C r ( T , v i ) − C r ( T , v s ) − c r ( v s , v i ) ) c s ( v s , v i ) − c s ( p i , v i ) } ( 2 ) The actual formula used is somewhat more complex , and handles negative denominators . Eq . 1 sums the gain of recre - ation cost changes among all matrices in the same snapshot s i ( for the independent scheme ) , while Eq . 2 uses the max change instead ( for the parallel scheme ) . The algorithm iteratively swaps edges and stops if all recreation constraints are satisﬁed or no edge returns a positive gain . A single step examines | E − T | edges and | U | unsatisﬁed constraints , and there are at most | E | steps . Thus the complexity is bounded by O ( | E | 2 | S | ) . PAS - PT : This algorithm constructs a solution by “growing” a tree starting with an empty tree . The algorithm examines the edges in G V ( V , E ) in the increasing order by the storage cost c s ; a priority queue is used to maintain all the candidate edges and is populated with all the edges from v 0 in the beginning . At any point , the edges in Q are the ones that connect a vertex T , to a vertex outside T . Using an edge e ij = ( v i , v j ) ( s . t . , v i ∈ V T ∧ v j ∈ V− V T ) popped from Q , the algorithm tries to add v j to T with minimum storage increment c s ( e ij ) . Before adding v j , it examines whether the constraints of a ﬀ ected groups s a ( s . t . , v j ∈ s a ) are satisﬁed using actual and estimated recreation costs for vertices { v k ∈ s a } in T and V − T respectively ; if v k ∈ T , actual recreation cost C r ( T , v k ) is used , otherwise the lower bound of it , i . e . c r ( ν 0 , v k ) is used as an estimation . We refer the estimation for s a as ˆ C r ( T , s a ) . Once an edge e ij is added to T , the inner edges I jT = { ( v k , v j ) | v k ∈ T } of newly added v j are dequeued from Q , while the outer edges O j E− T = { ( v j , v k ) | v k ∈ E − T } are enqueued . If the storage cost of existing vertices in T can be improved ( i . e . , C s ( T , v k ) > c s ( v k , v j ) ) , and recreation cost is not more ( i . e . C r ( T , v k ) ≥ C r ( T , v j ) + c r ( v k , v j ) ) , then the parent p k of v k in T is replaced to v j via the swap operation , which obviously decreases the storage cost and a ﬀ ected group recreation cost . The algorithm stops if Q is empty and T is a span - ning tree . In the case when Q is empty but V T ⊂ V , an adjustment operation on T to increase storage cost and satisfy the group recreation constraints is performed . For each v u ∈ V − V T , we append it to ν 0 , then in each unsatisﬁed group s i that v u belongs to , optimally , we want to choose a set of { v g } ⊆ s i ∩ T to change their parents in T , such that the decrement of storage cost is minimized while recreation cost is satisﬁed . The optimal adjustment itself can be viewed as a knapsack problem with extra non - cyclic constraint of T , which is NP - hard . Instead , we use the same heuristic in Eq . 1 to adjust v g ∈ s i ∩ T one by one by replacing its parent p g to v s until the group constraint in s i is satisﬁed . As before , the parallel scheme ψ p di ﬀ ers from independent case ψ i in the adjustment operator using Eq . 2 . The complexity of this algorithm is O ( | E | 2 | S | ) . D . Model Evaluation Scheme in PAS Model evaluation , i . e . , applying a DNN forward on a data point to get the prediction result , is a common task to explore , debug and understand models . Given a PAS storage plan , an dlv eval query requires uncompressing and applying deltas along the path to the model . We develop a novel model evalua - tion scheme utilizing the segmented design , that progressively accesses the low - order segments only when necessary , and guarantees no errors for arbitrary data points . The basic intuition is that : when retrieving segmented parameters , we know the minimum and maximum values of the parameters ( since higher order bytes are retrieved ﬁrst ) . If the prediction result is the same for the entire range of those values , then we do not need to access the lower order bytes . However , considering the high dimensions of parameters , non - linearity of the DNN model , unknown full precision value when issuing the query , it is not clear if this is feasible . We deﬁne the problem formally , and illustrate the de - terminism condition that we use to develop our algorithm . Our technique is inspired from theoretical stability analysis in numerical analysis . We make the formulation general to be applicable to other prediction functions . The basic assumption is that the prediction function returns a vector showing relative strengths of the classiﬁcation labels , then the dimension index with the maximum value is used as the predicted label . Problem 2 ( Parameter Perturbation Error Determination ) : Given a prediction function F ( d , W ) : R m × R n (cid:55)→ R c , where d is the data and W are the learned weights , the prediction result c d is the dimension index with the highest value in the output o ∈ R c . When W value is uncertain , i . e . , each w i ∈ W in known to be in the range [ w i , min , w i , max ] , determine whether c d can be ascertained without error . When W is uncertain , the output o is uncertain as well . However , if we can bound the individual entries in o , then the following condition is an applicable necessary condition for determining error : Lemma 4 : Let o i ∈ o vary in range [ o i , min , o i , max ] . If ∃ k such that ∀ i , o k , min > o i , max , then prediction result c d is k . Next we illustrate a query procedure , that given data d , evaluates a DNN with weight perturbations and determines the output perturbation on the ﬂy . Recall that DNN is a nested function ( Sec . II ) , we derive the output perturbations when evaluating a model while preserving perturbations step by step : x 0 , k = (cid:88) j W 0 , k , j d j + b 0 , k x 0 , k , min = (cid:88) j min { W 0 , k , j d j } + min { b 0 , k } x 0 , k , max = (cid:88) j max { W 0 , k , j d j } + max { b 0 , k } Next , activation function σ 0 is applied . Most of the common activation functions are monotonic function R (cid:55)→ R , ( e . g . sigmoid , ReLu ) , while pool layer functions are min , max , avg functions over several dimensions . It is easy to derive the per - turbation of output of the activation function , [ f 0 , k , min , f 0 , k , max ] . During the evaluation query , instead of 1 - D actual output , we carry 2 - D perturbations , as the actual parameter value is not available . Nonlinearity decreases or increases the perturbation range . Now the output perturbation at f i can be calculated similarly , except now both W and f i − 1 are uncertain : x i , k = (cid:88) j W i , k , j f i − 1 , j + b i , k x i , k , min = (cid:88) j min { W i , k , j f i − 1 , j } + min { b i , k } x i , k , max = (cid:88) j max { W i , k , j f i − 1 , j } + max { b i , k } Applying these steps iteratively until last layer , we can then apply Lemma 4 , the condition of error determinism , to check if the result is correct . If not , then lower order segments of the ﬂoat matrices are retrieved , and the evaluation is re - performed . This progressive evaluation query techniques dramatically improve the utility of PAS , as we further illustrate in our experimental evaluation . Note that , other types of queries , e . g . , matrix plots , activation plots , visualizations , etc . , can often be executed without retrieving the lower - order bytes either . ( a ) Compression - Accuracy Tradeo ﬀ for Float Representation Schemes ( b ) Compression Performance for Di ﬀ erent Delta Schemes & Models ( c ) PAS Optimal Archival Storage Algorithms Results for SD ( d ) Progressive Evaluation Query Processing Using High - Order Bytes Fig . 6 . Evaluation Results for PAS V . E valuation S tudy M odel H ub is designed to work with a variety of deep learn - ing backends ; our current prototype interfaces with caffe [ 3 ] through a wrapper that can extract ca ﬀ e training logs , and read and write parameters for training . We have also built a custom layer in caffe to support progressive queries . The dlv command - line suite is implemented as a Ruby gem , utilizing git as internal VCS and sqlite3 and PAS as backends to manage the set of heterogeneous artifacts in the local client . PAS is built in C + + with gcc 5 . 4 . 0 . All experiments are conducted on a Ubuntu Linux 16 . 04 machine with an 8 - core 3 . 0GHz AMD FX - 380 processor , 16GB memory , and NVIDIA GTX 370 GPU . We use zlib for compression ; unless speciﬁcally mentioned , the compression level is set to 6 . When wrapping and modifying ca ﬀ e , the code base version is rc3 . In this section , we present a comprehensive evaluation with real - world and synthetic datasets aimed at examining our design decisions , di ﬀ erences of conﬁgurations in PAS , and performance of archiving and progressive query evaluation techniques proposed in earlier sections . A . Dataset Description Real World Dataset : To study the performance of PAS design decisions , ﬂoat representations , segmented ﬂoat storage , delta encoding and overall compression performance , we use a col - lection of shared ca ﬀ e models published in ca ﬀ e repository or Model Zoo . In brief , LeNet - 5 [ 6 ] is a convolutional DNN with 431k parameters . The reference model has 0 . 88 % error rate on MNIST . AlexNet [ 7 ] is a medium - sized model with 61 million parameters , while VGG - 16 [ 8 ] has 1 . 9 billion parameters . Both AlexNet and VGG - 16 are tested on ILSVRC - 2012 dataset . The downloaded models have 43 . 1 % , and 31 . 6 % top - 1 error rate respectively . Besides , to study the delta performance on model repositories under di ﬀ erent workloads ( i . e . , retraining , ﬁne - tuning ) : we use CNN - S / M / F [ 20 ] , a set of similar models developed by VGG authors to study model variations . These models are similar to VGG in architecture , and retrained from scratch ; for ﬁne - tuning , we use VGG - Salient [ 21 ] a ﬁne - tuning VGG model which only changes last full layer . Synthetic Datasets : Lacking su ﬃ ciently ﬁne - grained real - world repositories of models , to evaluate performance of parameter archiving algorithms , we developed an automatic modeler to enumerate models and hyperparameters to produce a dlv repository . We generated a synthetic dataset ( SD ) : simulating a modeler who is enumerating models to solve a face recognition task , and ﬁne - tuning a trained VGG . SD results in similar DNNs and relatively similar parameters across the models . The datasets are shared online 4 . To elaborate , the automation is driven by a state machine that applies modeling practices from the real world . The modeler updates the VGG network architecture slightly and changes VGG object recognition goal to a face prediction task ( prediction labels changed from 1000 to 100 , so the last layer is changed ) ; various ﬁne - tuning hyperparameter alternations are applied by mimicking practice [ 22 ] . SD in total has 54 model versions , each of which have 10 snapshots . A snapshot has 16 parametric layers and a total of 1 . 96 × 10 10 ﬂoats . B . Evaluation Results Float Representation & Accuracy : We show the e ﬀ ect of di ﬀ erent ﬂoat encoding schemes on the compression and accuracy in Fig . 6 ( a ) ; this is a tradeo ﬀ that the user often needs to consider when conﬁguring M odel H ub to save a model . In Fig . 6 ( a ) , for each scheme , we plot the average compression ratio versus the average accuracy drop when applying PAS ﬂoat schemes on the three real world models . Here , random and uniform denote two standard quantization schemes . As we can see , we can get very high compression ratios ( a factor of 20 or so ) without a signiﬁcant loss in accuracy , which may be acceptable in many scenarios . Delta Encoding & Compression Ratio Gain : Next we study the usefulness of delta encoding in real world models in the following scenarios : a ) Similar : latest snapshots across similar models ( CNN - S / M / F , VGG - 16 ) ; b ) Fine - tuning : ﬁne - tuning models ( VGG - 16 , VGG - Salient ) ; and c ) Snapshots : snapshots for the same VGG models in SD between iterations . In Fig . 6 ( b ) , for di ﬀ erent delta schemes , namely , storing original matrices ( Materialize ) , arithmetic subtraction ( Delta - SUB ) , and bitwise XOR di ﬀ ( Delta - XOR ) , the comparison is shown ( i . e . , we show the results of compressing the resulting matrices using zlib ) . The ﬁgure shows the numbers under lossless compression scheme ( ﬂoat 32 ) , which has the largest storage footprint . As we can see , delta scheme is not always good , due to the non - convexity and high entropy of parameters . For models under similar architectures , storing materialized original pa - rameters is often better than applying delta encoding . With ﬁne - tuning and nearby snapshots , the delta is always better , and arithmetic subtraction is consistently better than bitwise 4 Dataset Details : http : / / www . cs . umd . edu / ∼ hui / code / modelhub XOR . We saw similar results for many other models . These ﬁndings are useful for PAS implementation decisions , where we only perform delta between nearby snapshots in a single model , or for the ﬁne - tuning setting among di ﬀ erent models . Optimal Parameter Archival Storage : We study the opti - mality and scalability of PAS - PT and PAS - MT with the base - line LAST [ 19 ] on the optimal parameter archival problem . We use SD here , from which we derive nearby snapshot deltas as well as model - wise deltas among the latest snapshots . For the access frequencies , in practice , the modeler tends to access better models more frequently than bad ones . We use multinomial distributions based on model accuracies to mimic the behavior . As LAST cannot handle the co - usage constraints , to have a reasonable baseline , we decompose the co - usage constraint to be proportional to each layer matrix size . The performance of the algorithms is shown in Fig . 6 ( c ) . Two dotted lines are the minimum possible storage cost ( MST ) and the recreation cost ( SPT ) for the independent scheme in version graph . Each data point is a storage plan found by the algorithms given a recreation cost constraint . As we can see , PAS algorithms ( PT and MT ) consistently perform better than LAST . The reason is both of them consider the constraints as a group and thus choose better storage plans . Progressive Query Evaluation : We study the e ﬃ ciency of the progressive evaluation technique using perturbation error determination scheme on real world models ( LeNet , AlexNet , VGG16 ) and their corresponding datasets . The original pa - rameters are 4 - byte ﬂoats , which are archived in segments in PAS . We modify caffe implementation of involved layers and pass two additional blobs ( min / max errors ) between layers . The perturbation error determination algorithm uses high order segments , and answers eval query on the whole test dataset . The algorithm determines whether top - k ( 1 or 5 ) result needs lower order bytes ( i . e . , matched index value range overlaps with k + 1 index value range ) . The result is summarized in Fig . 6 ( d ) . The y - axis shows the error rate , i . e . , the percentage of test dataset that may have perturbation errors due to low precision . The x - axis shows the percentage of data that needs to be retrieved ( i . e . , 2 bytes or 1 byte per ﬂoat ) . As one can see , the prediction errors requiring full precision lower - order bytes are very small . The less high - order bytes used , higher the chance of potential errors . The consistent result of progressive query evaluation on real models supports our design decision of segmented ﬂoat storage . VI . R elated W ork Machine Learning Systems : There have been several high - proﬁle deep learning systems in recent years , but those typi - cally focus on the training aspects ( e . g . , on distributed training , how to utilize GPUs or allow symbolic formulas , etc . ) [ 3 ] , [ 4 ] , [ 23 ] , [ 24 ] , [ 25 ] . The data and lifecycle management challenges discussed above have been largely ignored so far , but are be - coming critical as the use of deep learning permeates through a variety of application domains , since those pose a high barrier to entry for many potential users . In the database community , there has been increasing work on developing general - purpose systems for supporting machine learning , including pushing predictive models into databases [ 26 ] , [ 27 ] , accelerating tasks using database optimizing methods [ 2 ] , [ 28 ] , and managing modeling lifecycles and serving predictive models in advanced ways [ 29 ] , [ 30 ] . M odel H ub is motivated by similar principles ; aside from a focus on DNNs , it also supports versioning as a ﬁrst - class construct [ 10 ] which di ﬀ erentiates it from that work . DNN Compression : There has been increasing interest on compressing DNN models , motivated in part by the need to deploy them on devices with simple instruction sets , low memory , and / or energy constraints [ 13 ] , [ 31 ] , [ 32 ] . However , the goal of those works is simplify the model in a lossy manner with as little loss of accuracy as possible , which makes this work orthogonal to the archival approach we take in M odel H ub ; in fact , simpliﬁed models are likely to compress much better , magnifying the gains of our approach as our experimental results show . Further , these methods often require heavy retraining or expensive computations ( k - means , SVD , etc . ) to derive simpler models , which makes them too heavy - weight in an interactive setting for which DLV is designed . DNNs with Low Precision Floats : Low precision ﬂoats are exploited in accelerating training and testing systems [ 12 ] , [ 32 ] , [ 33 ] , showing techniques and empirical results when training , testing DNN with limited precisions . M odel H ub di ﬀ ers from their work by exploiting parameters archiving , and use segmented ﬂoats to answer lifecycle modeling queries . Stability Analysis Results Stability analysis of DNN is stud - ied in the literature [ 34 ] , [ 35 ] , [ 36 ] , where the problem setting is perturbation analysis regardless speciﬁc data , rather focus on statistical measures of stability . M odel H ub uses basic per - turbation analysis techniques and focus on novel progressive query answering in a segmented ﬂoat storage . VII . C onclusion and F uture W ork In this paper , we described some of the key data man - agement challenges in learning , managing , and adjusting deep learning models , and presented our M odel H ub system that attempts to address those challenges in a systematic fashion . The goals of M odel H ub are multi - fold : ( a ) to make it easy for a user to explore the space of potential models by tweaking the network architecture and / or the hyperparameter values , ( b ) to minimize the burden in keeping track of the metadata including the accuracy scores and the ﬁne - grained results , and ( c ) to compactly store a large number of models and constituent snapshots without compromising on query or retrieval perfor - mance . We presented several high - level abstractions , including a command - line version management tool and a domain - speciﬁc language , for addressing the ﬁrst two goals . Anecdotal experience with our early users suggests that both of those are e ﬀ ective at simplifying the model exploration tasks . We also developed a read - optimized parameter archival storage for storing the learned weight parameters , and designed novel algorithms for storage optimization and for progressive query evaluation . Extensive experiments on real world and synthetic models verify the design decisions we made and demonstrate the advantages of proposed techniques . R eferences [ 1 ] Y . LeCun , Y . Bengio , and G . Hinton , “Deep learning , ” Nature , 2015 . [ 2 ] C . Zhang , A . Kumar , and C . R´e , “Materialization optimizations for feature selection workloads , ” in SIGMOD , 2014 . [ 3 ] Y . Jia et al . , “Ca ﬀ e : Convolutional architecture for fast feature embed - ding , ” in ACM MM , 2014 . [ 4 ] M . Abadi et al . , “TensorFlow : A system for large - scale machine learning , ” in OSDI , 2016 . [ 5 ] L . Bottou , “Stochastic gradient descent tricks , ” in Neural Networks : Tricks of the Trade . Springer , 2012 , pp . 421 – 436 . [ 6 ] Y . LeCun et al . , “Handwritten digit recognition with a back - propagation network , ” in NIPS , 1990 . [ 7 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton , “Imagenet classiﬁcation with deep convolutional neural networks , ” in NIPS , 2012 . [ 8 ] K . Simonyan and A . Zisserman , “Very deep convolutional networks for large - scale image recognition , ” CoRR , vol . abs / 1409 . 1556 , 2014 . [ 9 ] K . He , X . Zhang , S . Ren , and J . Sun , “Deep residual learning for image recognition , ” in CVPR , 2016 . [ 10 ] A . Bhardwaj et al . , “DataHub : Collaborative data science and dataset version management at scale , ” in CIDR , 2015 . [ 11 ] S . Bhattacherjee , A . Chavan , S . Huang , A . Deshpande , and A . Parameswaran , “Principles of dataset versioning : Exploring the recreation / storage tradeo ﬀ , ” PVLDB , 2015 . [ 12 ] V . Vanhoucke , A . Senior , and M . Z . Mao , “Improving the speed of neural networks on CPUs , ” in Proc . Deep Learning and Unsupervised Feature Learning NIPS Workshop , 2011 . [ 13 ] S . Han et al . , “Deep compression : Compressing deep neural networks with pruning , trained quantization and hu ﬀ man coding , ” in ICLR , 2016 . [ 14 ] M . Courbariaux et al . , “Training deep neural networks with low precision multiplications , ” arXiv preprint arXiv : 1412 . 7024 , 2014 . [ 15 ] E . R . Schendel et al . , “Isobar preconditioner for e ﬀ ective and high - throughput lossless data compression , ” in ICDE , 2012 . [ 16 ] S . Bhattacherjee , A . Deshpande , and A . Sussman , “Pstore : an e ﬃ cient storage framework for managing scientiﬁc data , ” in SSDBM , 2014 . [ 17 ] N . Deo and N . Kumar , “Computation of constrained spanning trees : A uniﬁed approach , ” in Network Optimization , 1997 . [ 18 ] A . B . Kahng and G . Robins , On optimal interconnections for VLSI . Springer Science & Business Media , 1994 , vol . 301 . [ 19 ] S . Khuller , B . Raghavachari , and N . Young , “Balancing minimum spanning trees and shortest - path trees , ” Algorithmica , 1995 . [ 20 ] K . Chatﬁeld et al . , “Return of the devil in the details : Delving deep into convolutional nets , ” in BMVC , 2014 . [ 21 ] J . Zhang , S . Ma , M . Sameki , S . Sclaro ﬀ , M . Betke , Z . Lin , X . Shen , B . Price , and R . Mech , “Salient object subitizing , ” in CVPR , 2015 . [ 22 ] R . Girshick , “Fast R - CNN , ” in ICCV , 2015 . [ 23 ] J . Dean et al . , “Large scale distributed deep networks , ” in NIPS , 2012 . [ 24 ] T . Chilimbi et al . , “Project ADAM : Building an e ﬃ cient and scalable deep learning training system , ” in OSDI , 2014 . [ 25 ] W . Wang et al . , “Singa : Putting deep learning in the hands of multimedia users , ” in ACM MM , 2015 . [ 26 ] M . Akdere , Cetintemel et al . , “The case for predictive database systems : Opportunities and challenges . ” in CIDR , 2011 . [ 27 ] X . Feng , A . Kumar , B . Recht , and C . R´e , “Towards a uniﬁed architecture for in - rdbms analytics , ” in SIGMOD , 2012 . [ 28 ] A . Kumar , J . Naughton , and J . M . Patel , “Learning generalized linear models over normalized data , ” in SIGMOD , 2015 . [ 29 ] M . Vartak et al . , “Supporting fast iteration in model building , ” in LearningSys , 2015 . [ 30 ] D . Crankshaw , X . Wang , J . E . Gonzalez , and M . J . Franklin , “Scalable training and serving of personalized models , ” in LearningSys , 2015 . [ 31 ] E . L . Denton et al . , “Exploiting linear structure within convolutional networks for e ﬃ cient evaluation , ” in NIPS , 2014 . [ 32 ] W . Sung , S . Shin , and K . Hwang , “Resiliency of deep neural networks under quantization , ” arXiv preprint arXiv : 1511 . 06488 , 2015 . [ 33 ] S . Gupta , A . Agrawal , K . Gopalakrishnan , and P . Narayanan , “Deep learning with limited numerical precision , ” in ICML , 2015 . [ 34 ] M . Stevenson et al . , “Sensitivity of feedforward neural networks to weight errors , ” IEEE Trans . Neural Networks , vol . 1 , 1990 . [ 35 ] X . Zeng et al . , “Sensitivity analysis of multilayer perceptron to input and weight perturbations , ” IEEE Trans . Neural Networks , vol . 12 , 2001 . [ 36 ] J . Yang , X . Zeng , and S . Zhong , “Computation of multilayer perceptron sensitivity to input perturbation , ” Neurocomputing , 2013 .