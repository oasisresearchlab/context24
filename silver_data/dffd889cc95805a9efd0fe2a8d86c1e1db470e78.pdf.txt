11 IDE - Based Learning Analytics for Computing Education : A Process Model , Critical Review , and Research Agenda C . D . HUNDHAUSEN and D . M . OLIVARES , Washington State University A . S . CARTER , Humboldt State University In recent years , learning process data have become increasingly easy to collect through computer - based learning environments . This has led to increased interest in the field of learning analytics , which is concerned with leveraging learning process data in order to better understand , and ultimately to improve , teaching and learning . In computing education , the logical place to collect learning process data is through integrated development environments ( IDEs ) , where computing students typically spend large amounts of time working on programming assignments . While the primary purpose of IDEs is to support computer programming , they might also be used as a mechanism for delivering learning interventions designed to enhance student learning . The possibility of using IDEs both to collect learning process data , and to strategically intervene in the learning process , suggests an exciting design space for computing education research : that of IDE - based learning analytics . In order to facilitate the systematic exploration of this design space , we present an IDE - based data analytics process model with four primary activities : ( 1 ) Collect data , ( 2 ) Analyze data , ( 3 ) Design intervention , and ( 4 ) Deliver intervention . For each activity , we identify key design dimensions and review relevant computing education literature . To provide guidance on designing effective interventions , we describe four relevant learning theories , and consider their implications for design . Based on our review , we present a call - to - action for future research into IDE - based learning analytics . CCS Concepts : • Information systems → Data analytics ; • Social and professional topics → Comput - ing education ; • Applied computing → Interactive learning environments ; E - learning ; Additional Key Words and Phrases : Learning analytics , learning process data , learning interventions ACM Reference format : C . D . Hundhausen , D . M . Olivares , and A . S . Carter . 2017 . IDE - Based Learning Analytics for Computing Ed - ucation : A Process Model , Critical Review , and Research Agenda . ACM Trans . Comput . Educ . 17 , 3 , Article 11 ( August 2017 ) , 26 pages . https : / / doi . org / 10 . 1145 / 3105759 1 INTRODUCTION In recent years , learning process data have become increasingly easy to collect through computer - based learning environments . Moreover , the availability of low - cost , high - power machines has This work is supported by the National Science Foundation , under grant no . IIS - 1321045 . Authors’ addresses : C . D . Hundhausen and D . M . Olivares , Human - centered Environments for Learning and Programming ( HELP ) Lab , School of Electrical Engineering and Computer Science , Washington State University , Pullman , WA 99164 - 2752 ; A . S . Carter , Department of Computer Science , Humboldt State University , 1 Harpst Street , Arcata , CA 95521 . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , to republish , to post on servers , to redistribute to lists , or to use any component of this work in other works requires prior specific permission and / or a fee . Permissions may be requested from Publications Dept . , ACM , Inc . , 2 Penn Plaza , Suite 701 , New York , NY 10121 - 0701 USA , fax + 1 ( 212 ) 869 - 0481 , or permissions @ acm . org . © 2017 ACM 1946 - 6226 / 2017 / 08 - ART11 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3105759 ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 2 C . D . Hundhausen et al . made it increasingly easy to store and process such data . These developments have led to an explosion in the field of learning analytics ( see , e . g . , [ 6 ] ) , which is concerned with leveraging continuously - updated streams of learning process data in order to make sense of learners’ be - haviors , and ultimately to better tailor instruction to learners’ needs . In computing education , the logical place to collect continuous data on students’ learning ac - tivities is through integrated development environments ( IDEs ) , where computing students typi - cally spend large amounts of time working on programming assignments . While their primary purpose is to support the development of computer programs , several IDEs have been instru - mented to collect data on students’ programming processes , including their edits , compilation er - rors , and runtime exceptions ( see , e . g . , [ 16 ] ) . IDEs can also be augmented with additional features that collect assessment data while potentially enhancing student learning . For example , in our own research , we have augmented an IDE with a social networking - style activity stream , which automatically collects data on students’ online conversations about their programming activities— conversations that take place in the same context as those programming activities [ 23 ] . Moreover , one can imagine augmenting an IDE with additional data collection mechanisms that provide a basis for delivering enhanced learning experiences . For example , survey or quiz questions could be administered through the IDE during the programming process , in order to collect data on student understanding of concepts , or on students’ attitudes during the programming process ( e . g . , [ 14 , 64 , 65 ] ) . Likewise , one could collect eye tracking data ( e . g . , [ 18 ] ) or physiological data ( e . g . , galvanic skin response ; see [ 69 ] ) as students work within an IDE . These data could be used to interpret or augment traditional data on students’ programming processes . Just as an IDE can be used to collect learning process data , so too might it be used as a mech - anism for delivering learning interventions designed to enhance students’ learning processes and outcomes . For example , suppose that , based on programming process data , an IDE detects that a student is engaged in programming behaviors that , according to a predictive model ( e . g . , [ 22 , 44 , 80 ] ) , are negatively correlated with course success . The IDE could then present the learner with an intervention—for example , a pop - up message that nudged the learner toward a more productive behavior . Likewise , imagine an IDE that presents computing students with a continuously - updated visual analytics dashboard ( e . g , [ 77 ] ) of their progress toward established learning goals . Students could use such a dashboard to guide them toward virtuous learning activities . The possibility of using the IDE both to collect learning process data , and to intervene in the learning process , suggests an exciting design space for computing education researchers to ex - plore . As researchers explore this design space of IDE - based learning analytics within the context of computing education , at least three key research questions must be addressed : —RQ1 : What learning data should be collected within an IDE in order to provide a foundation for improving student learning ? —RQ2 : How should the learning data be analyzed in order to provide useful information on student learning ? —RQ3 : Based on the learning data , what interventions should be delivered through an IDE in order to benefit student learning ? In this article , we propose a process model for IDE - based learning analytics that can help re - searchers to explore these research questions systematically . Within the context of computer pro - gramming activities , our process model decomposes the cyclical learning analytics process into four key activities : ( 1 ) Collect data , ( 2 ) Analyze data , ( 3 ) Design intervention , and ( 4 ) Deliver inter - vention . For each activity in this model , we identify the key design dimensions and review relevant computing education literature . In order to provide a foundation for designing effective interven - tions , we introduce four relevant learning theories and identify their implications for intervention ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 3 design . Finally , based on our review of existing computing education and relevant theory , we pro - pose an agenda for future research that can advance the field . In so doing , our article contributes to the computing education literature by furnishing a principled model for organizing and situating learning analytics research within computing education , as well as a theoretical foundation for de - signing effective interventions . This , in turn , allows us both to critically review existing research , and to identify viable areas for future research . 2 BACKGROUND AND SCOPE This article uses a process model for IDE - based learning analytics to organize a review of existing work . We begin by delineating the scope of the review : IDE - based learning analytics . Next , to provide further background , we review computing education research that has focused on learning process data automatically collected within an IDE . Finally , we distinguish IDE - based learning analytics from closely - related research within the well - known area of intelligent tutoring systems . 2 . 1 IDE - Based Learning Analytics and Intervention Learning analytics ( see , e . g . , [ 6 , 78 ] ) encompasses the analysis of learning process data in order to better understand , and ultimately improve , student learning . A foundational idea is to build learner models that infer learners’ background knowledge , learning strategies , and motivations based on learning process data [ 53 ] . In turn , such models can be used to derive learning interventions that adapt instruction to learner needs . In this context , we use the term learning intervention to de - note an event in which some combination of information , guidance , and feedback is shared with learners for the purpose of positively influencing the learner’s behavior , attitudes , or physiological state . Within computing education , integrated development environments ( IDEs ) provide an ideal foun - dation for performing learning analytics and delivering learning interventions for at least three reasons : ( a ) computing students spend much of their time working on programming assignments within IDEs ; ( b ) IDEs can be easily instrumented with data collection facilities that make it possi - ble to automatically collect learning process data as students work on programming assignments ; and ( c ) IDEs are the logical place to deliver learning interventions to students dynamically —as the learning process unfolds . For these reasons , we focus in this article on ( a ) the collection and analysis of data on student learning and programming that takes place within an IDE , and ( b ) the delivery of interventions directly within the IDE . 2 . 2 Approaches to Learning Analytics Research Duval and Verbert [ 78 ] define two approaches to learning analytics research : ( a ) identify patterns of behavior based on the learning process data collected , and ( b ) derive interventions aimed at improving the learning process . In the following , we briefly review computing education research that has taken each of these approaches . 2 . 2 . 1 Identify Patterns of Behavior . Computing educators’ interest in studying programming behavior dates back at least to the mid - 1970s [ 70 ] . Many of the studies of novice programming that occurred in the 1980s relied on think - aloud protocols [ 32 ] . The first studies to collect and analyze log data on learners’ programming processes can be traced to the mid - 1980s , when Bonar and Soloway [ 13 ] studied novice Pascal programmers by collecting code snapshots submitted to the compiler . Another early effort to collect and analyze programming log data was that of Guzdial [ 35 ] . Building on the software - logging techniques used by Card , Moran , and Newell [ 19 ] in their seminal studies of human - computer interaction , Guzdial [ 35 ] investigated novice programming practices at a finer level of granularity by collecting keystroke - level data . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 4 C . D . Hundhausen et al . Since the early 2000s , computing educators’ interest in leveraging IDE log data to study and understand programming behavior has gradually increased . In an influential line of work , Jadud [ 44 ] studied novice programming behavior based on log data automatically collected through the BlueJ novice programming environment [ 50 ] . This research led to one of the first predictive mod - els of novice programming performance . A number of efforts to understand and predict novice performance and outcomes based on automatically - collected IDE log data have followed ( see , e . g . , [ 1 , 22 , 80 ] ) . Ihantola et al . [ 43 ] present the most comprehensive review of this growing body of research to date . Drawing on 76 research studies published in 10 different computing education venues be - tween 2005 and 2015 , their review classifies the studies along eight different dimensions , ranging from research goals , to data collection methods , to research quality . In addition , they present case studies that highlight the difficulties of replicating the methods and results of previously - published research , and identify grand challenges for future learning analytics research in computing edu - cation . The review presented in this article differs from the Ihantola et al . [ 43 ] review in three key ways : ( a ) by presenting a principled process model for organizing the learning analytics literature , ( b ) by expanding the focus of learning analytics to include in - IDE intervention design , and ( c ) by identifying theoretical orientations that can guide the design of effective interventions . 2 . 2 . 2 Derive Interventions . As illustrated in the previous section , a large body of computing education research has used programming process log data to study computer programming . A markedly smaller body of research has actually used such data as a basis for designing IDE inter - ventions . Examples of interventions developed in this body of research include —dynamically tailoring feedback [ 17 ] , and providing incentive mechanisms [ 17 , 73 ] , in order to improve students’ testing behaviors . —enhancing syntax error messages to make them more understandable [ 28 ] . —dynamically generating hints based on patterns mined from the programming data of pre - vious and current users of an IDE [ 29 , 38 , 62 ] . —awarding badges to students who meet certain time management and learning goals in computer programming exercises [ 37 ] . —scaffolding the programming process by providing intermediate goals [ 35 , 79 ] . 2 . 3 Learning Analytics vs . Intelligent Tutoring Systems Within computing education , there has been a long tradition of research into intelligent tutoring systems ( ITSs ) , which aim to provide individually - tailored feedback and assistance to learners as they perform programming tasks ( see , e . g . , [ 3 , 4 , 10 , 26 ] ) . ITSs are based on expert models of how a given programming task should be performed . As a learner programs , the learner’s trajectory is compared against the expert’s trajectory , and individualized hints and assistance are dynamically generated in order to keep the learner on track . While ITS research has much in common with learning analytics research , the two research areas are different in at least two important respects : —A key goal of ITSs is to automate the learning and teaching process by replacing home - work and practice drills . To that end , ITSs furnish a curriculum ( a series of programming problems ) and instruction ( individually - tailored hints as learners work on those problems ) . In contrast , learning analytics aim to augment traditional classroom instruction , not to re - place it . As a result , a learning analytics infrastructure may ( a ) provide teachers with the information and resources to teach better , and / or ( b ) help learners to connect with others in their class who can provide support and assistance . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 5 —Because they are based on expert models of problem - solving , ITSs rely on knowledge of the specific programming problems being solved by learners . Individualized feedback is gener - ated by comparing the learner’s solution path against the expert’s solution path for a given problem . More recently , research in the educational data mining community has explored ITSs that automatically generate hints and feedback based on learning process data [ 45 , 63 , 75 ] . In contrast to much of the ITS research , IDE - based learning analytics environments assume no knowledge of the specific problems being solved . In addition , in the learning an - alytics design space assumed in this article , individual programming problems are assigned by course instructors , not by an ITS . Thus , the kinds of expert and learner models typically employed by ITSs cannot be leveraged by a learning analytics environment , which must instead provide feedback based on more general information on learners’ programming processes , outcomes , and attitudes . 2 . 4 Summary While computing education researchers have taken great interest in collecting and analyzing learn - ing data , relatively little of this work has leveraged those analyses to dynamically generate inter - ventions . Moreover , the research that has explored interventions tends to be ad hoc , designing interventions to meet specific goals without regard to a more general learning theory . In the next two sections , we present a principled process model for organizing in - IDE learning analytics and intervention design research , and we explore the implications of a range of theoretical orientations for the design of effective interventions . These sections lay the groundwork for Section 5 , which presents a call - to - action for future research into IDE - based learning analytics . 3 A PROCESS MODEL - GUIDED REVIEW In software engineering , process models have traditionally been used as abstractions for a given software development process [ 68 ] . Such models identify and parameterize the key activities of the given process , providing a basis for discussing , reasoning about , and simulating those activities without actually performing them . In this spirit , we propose a cyclical process model for IDE - based learning analytics that distills the learning analytics process into four essential activities ( see Figure 1 ) : ( 1 ) Collect data , ( 2 ) Analyze data , ( 3 ) Design intervention , and ( 4 ) Deliver intervention . Later in the text , we use this process model to explore the design space of IDE - based learning analytics . For each activity , we identify key design dimensions and review relevant research . 3 . 1 Collect Data As described in the review of Ihantola et al . [ 43 ] , a variety of mechanisms have been used to collect students’ programming data , including automated grading systems ( e . g . , [ 31 ] ) and version control systems . However , the learning analytics process considered in this article focuses on data collected directly through the IDE—what Ihantola et al . [ 43 ] call IDE instrumentation . 3 . 1 . 1 Standard Data . The most obvious learning data to collect through an IDE are program - ming data generated by students as they engage in programming tasks within an IDE . Figure 2 presents a taxonomy of standard programming data that can be automatically collected by instru - menting a traditional IDE . Commercial IDEs such as Eclipse and Netbeans typically support the automatic collection of programming data ( see , e . g . , [ 30 , 56 ] ) . However , in order to support product improvements , such data collection is directed more toward understanding feature usage than toward understanding programming processes . In contrast , within computing education , there is a clear focus on bet - ter understanding how students learn to program . In the 1990s , some research prototype IDEs , ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 6 C . D . Hundhausen et al . Fig . 1 . Process model for IDE - based learning analytics in computing education . including Emile [ 35 ] and GPCEdit [ 36 ] , supported automatic data collection in order to explore related research questions . More recently , automatic data collection facilities have been increas - ingly integrated into IDEs that are more widely distributed , including stand - alone IDEs such as BlueJ [ 16 , 58 ] , Eclipse [ 71 ] , and NetBeans [ 79 ] , and web - based IDEs such as CloudCoder [ 60 ] . Hackystat [ 47 ] is unique in that it is IDE - agnostic ; it can be attached to any IDE through a web services API . 3 . 1 . 2 Augmented Data . The interface and functionality of many IDEs can be extended through a plug - in architecture . This opens up the possibility of augmenting an IDE to facilitate the collection of a broader range of data that might provide additional insight into the learning process . Figure 3 presents a taxonomy of additional data that can be collected through an IDE augmented with additional features and functionality . By augmenting an IDE with a social - networking activity feed , as we have done in our own research [ 23 ] , one can collect Social Data : asynchronous class discussions ( Posts and Replies ) that take place within the IDE as students work on programming assignments , as well as the Helpful Marks ( or “likes” ) that participants give to posts and replies . It is also possible to support one - on - one and group conversations through a Personal Messaging System , and to support Badges or reputation points , as are used in some question - answering systems such as Stack Overflow [ 74 ] . Automated testing tools enable a computer program to be automatically tested against a set of test cases ( e . g . , [ 31 ] ) . When integrated into an IDE , they can be used as a basis for collecting Testing Data on student testing practices and performance . For example , IDE plugins have been developed to interface Eclipse with automated testing tools such as Web - CAT [ 52 ] . Empirical studies within computing education have been interested in identifying relationships between student attitudes and outcomes ( see , e . g . , [ 41 , 66 ] ) . In a similar vein , computing educa - tors often administer quizzes or tests intermittently throughout an academic term—most often for the purpose of evaluating student knowledge and performance . While computing educators typically administer both surveys and quizzes / tests relatively infrequently , the ability to collect Survey / Quiz Data more frequently in the context of programming tasks could provide additional insight into students’ programming knowledge , including their misconceptions . Survey and quiz ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 7 Fig . 2 . Programming process data that can be automatically collected through a standard IDE . questions could , for example , be dynamically tailored to the situation in order to better understand students’ attitudes about , and conceptual understanding of , the tasks and concepts immediately before them . Perhaps the only example of an IDE with such a facility is TestMyCode [ 42 ] , a Net - beans plugin that supports a dynamic quiz facility . Finally , a relatively new research area within computing education is the collection and analysis of Physiological Data on programmers as they perform programming tasks . The best - known examples are studies that collect Eye - Tracking Data on programmers ( see , e . g . , [ 11 , 18 , 49 ] ) . In addition , within the context of computer programming , there has been some interest in col - lecting and analyzing Mouse and Keyboard Pressure [ 12 ] , Electro - Dermal Activity [ 2 , 54 ] , and Heart Rate [ 12 , 54 , 55 ] . Even though , to our knowledge , none of the hardware devices re - quired to collect these kinds of data has been directly integrated with an IDE , such integration would enable a broader range of data to be brought to bear on the IDE - based learning analytics process . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 8 C . D . Hundhausen et al . A UGMENTED IDE D ATA S OCIAL D ATA A CTIVITY F EED A UDIENCE P OSTS R EPLIES H ELPFUL M ARKS P ERSONAL M ESSAGING P ARTICIPANTS M ESSAGES B ADGES AND R EPUTATION P OINTS T ESTING D ATA T ESTS EXECUTED T EST RESULTS S UVEY / Q UIZ D ATA Q UESTIONS ASKED R ESPONSES P HYSIOLOGICAL D ATA E YE TRACKING F IXATIONS S ACCADES M OUSE / KEYBO ARD PRESSURE E LECT - DERMAL ACTIVITY H EART RATE Fig . 3 . Data that can be automatically collected through an IDE augmented with additional features and functionality . Table 1 presents a comparison of seven publicly - available IDE tools used in computing education with respect to their support for automatic data collection , using the data taxonomies presented in Figures 2 and 3 as a basis for the comparison . Two of these work in conjunction with the BlueJ novice IDE : the built - in Blackbox data collection facility [ 16 ] and the Clock - It plug - in for BlueJ [ 58 ] . The next three of these work with the Eclipse IDE : the DevEventTracker plug - in that interfaces Eclipse with WebCat [ 52 ] ; the Marmoset plug - in to Eclipse [ 72 ] ; and the open - source HackyStat data collection and analysis framework , which interfaces with Eclipse through a Web API [ 47 ] . The final two of these are plug - ins to Visual Studio [ 21 ] and Netbeans [ 79 ] . While by no means exhaustive , the set of IDE tools compared in Table 1 represents a majority of the publicly - available tools . As such , it provides a reasonable foundation for making at least two ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 9 Table 1 . Comparison of Five IDEs Used in Computing Education Based on Data They Collect ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 10 C . D . Hundhausen et al . general observations regarding the state - of - the - art with respect to IDE data collection tools . First , notice that relative to the basic programming data they collect , the seven tools vary widely . In fact , although not shown in Table 1 , the seven tools even vary with respect to the manner in which they collect individual data points . For example , the tools employ a variety of techniques for collecting editing operations : some collect all keystrokes , while others collect the addition and deletion of lines or known programming constructs . Second , notice that , while four of the seven tools collect augmented data beyond the basic programming data , only a limited range of augmented data are collected . Three of the seven tools collect testing data , with one tool collecting some social data and another tool collecting some Survey / Quiz data . This suggests that existing publicly - available IDE tools collect only a limited subset of the range of data that could be collected . 3 . 2 Analyze Data In the second step of the process model , data automatically collected through the IDE are further processed and analyzed . In this step , a fundamental task is to transform the data into useful in - formation that sheds further light on students’ learning processes , attitudes , and outcomes , and therefore can serve as a suitable foundation for educationally - effective interventions . Table 2 pro - poses a taxonomy of useful information . In order to generate this taxonomy , we began by creating a list of information that would be potentially derivable from the raw data presented in Figures 2 and 3 . Then , in collaboration with a team of computing education researchers and practitioners who attended a learning analytics workshop held in conjunction with the 2015 ACM International Computing Education Research ( ICER ) Conference , we narrowed down the list to those items that we believed would be of potential value to computing education researchers and practitioners interested in intervening in the learning process . While we make no claim that the taxonomy in - cludes all information that ( a ) is derivable from IDE data and ( b ) is potentially valuable to the design of interventions , we believe it serves as a reasonable starting point for researchers in the field to build on . The taxonomy includes seven top - level information categories that correspond with the range of learning behaviors , attitudes , and physiological responses that might be captured through automatically - collected data . In order to provide further organization , the taxonomy partitions some of the top - level categories into logical subcategories . The leaf nodes are the actual informa - tional measures ( i . e . , quantifications of data ) or metric s ( i . e . , derivatives of one or more measures ) that can be extracted through further processing or analysis of the data . Some of these were in - spired by Cardell - Oliver’s [ 20 ] description of software metrics for novice programmers . The right - hand column of the taxonomy indicates the analysis technique used to derive each informational measure or metric . As can be seen , at least six different techniques can be used : — : Useful information can be obtained by simply counting raw data points . In order to do so , one needs to select the data points to be counted—for example , based on a timespan within which they occurred , based on the type of data , or based on the person to whom the data corresponds . — : A slightly more sophisticated analysis technique is a mathematical formula that com - putes information from data . A common example of this is to take the average of a set of data over time , or to compute the percentage of data points that meet a certain criterion . — : One often needs to apply some sort of algorithm to the data in order to transform the data into useful information . For example , judgments of quality or content may re - quire the algorithmic application of a set of heuristics . Likewise , predictive measures are ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 11 Table 2 . A Taxonomy of Useful Information Derivable from IDE Data ( Continued ) ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 12 C . D . Hundhausen et al . Table 2 . ( Continued ) typically computed through some sort of algorithm that processes relevant data and out - puts a number corresponding to the prediction . — : Visualizations of data can often furnish valuable insights that could not be gained by simply analyzing raw data points . — : Machine learning represents a special class of algorithms that aim to learn from , identify patterns in , and make predictions about data . It is important to underscore that the information items identified in the taxonomy serve as ( potentially crude ) proxies for the kinds of information described by the top - level categories . For example , while the time between a student’s first edit and the assignment deadline can be sug - gestive of the extent to which the student has procrastinated , it is by no means always suggestive of procrastination . Indeed , it may well generate a false positive due to the limits of in - IDE data collection : A student may have worked on the assignment outside of the IDE well before that first edit . Likewise , the number of executions between compilation attempts may provide useful infor - mation regarding a student’s reliance on execution to test code , but is not sensitive to any mental simulation of the code that the student may be doing—an activity that could also be potentially valuable to the programming process . Clearly , any effort to pursue learning analytics based on automatically - collected IDE data needs to be sensitive to this limitation . 3 . 3 Design Interventions In this step of the process , one leverages the data analysis performed in the previous step to de - sign in - IDE interventions . Here , we use the term intervention to denote an event in which some combination of information , guidance , and feedback is shared with learners for the purpose of positively influencing the learner’s behavior , attitudes , or physiological state . Figure 4 presents a taxonomy of design dimensions for in - IDE interventions . The taxonomy is organized around three fundamental high - level questions that any intervention designer must address : ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 13 D ESIGN D IMENSIONS C ONTENT D ATA I NFORMATION C RITIQUE S UGGESTION E NCOURAGEMENT P RESENTATION V ISUALIZATIONS N OTIFICATIONS C ONSTRAINTS T IMING P ERSISTENT E VENT - B ASED S TATE - B ASED ON R EQUEST Fig . 4 . Taxonomy of design dimensions for IDE - based interventions . — Content : What information will the intervention contain ? — Presentation : How will the intervention be presented to the learner ? — Timing : When will the intervention be delivered ? Later in the text , we describe these dimensions in further detail . 3 . 3 . 1 Content . Perhaps the most basic choice an intervention designer must make is what to present to the learner . One option is simply to update learners on their processes and progress by presenting the types of data and information described in Figure 3 and Table 2 . Such data and information can provide a basis for providing a critiqe of learners’ processes or progress . In order to encourage learners to improve , one could also present a concrete suggestion , or even provide encouragement not to give up , or to keep up the good work . 3 . 3 . 2 Presentation . A second fundamental design choice concerns the manner in which the intervention is presented to the learner . A computer - based learning environment such as an IDE offers at least three distinct mechanisms for presenting interventions , as discussed later in the text . Visualizations . In the spirit of learning dashboards , a popular means of presenting visual an - alytics in a learning environment ( e . g . , [ 77 ] ) , an IDE could present interventions in the form of visualizations ( e . g . , graphs and charts ) that enable learners to visually explore their learning ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 14 C . D . Hundhausen et al . process and progress . An important design consideration for visualizations is that they are most effective in promoting learning if they actively engage the learner [ 40 ] . This implies the need to provide some level of interactivity in the visualizations that are used as interventions—by , for ex - ample , allowing learners to perform what - if analyses . Basic principles of human perception and human - computer interaction can also improve the effectiveness of visualizations . For example , vi - sualizations should be designed so as present information relevant to learners’ immediate tasks , with the relevant information easily recognizable and within the learner’s foveal view [ 46 , 57 ] . Notifications . The Notification presentation mechanism differs from the Visualization deliv - ery mechanism in two key ways . First , it is a textual message , as opposed to a visual representation . Second , whereas a visualization intervention is presented to the learner in a non - obtrusive way ( the learner must potentially seek it out by navigating to it and clicking on it ) , a notification may be , but is not necessarily , delivered obtrusively , in the form of a modal pop - up message that inter - rupts the learner within the IDE . For example , a modal dialog box could pop - up within an IDE in order to help the learner get back on track at points at which the learner appears to be struggling . Constraints . In user interface design , constraints prevent illegal or undesirable user interface actions , thereby guiding the user toward legal or desirable user interface actions [ 57 ] . Constraints typically manifest themselves as user interface controls that limit the actions that can be taken— for example , menu items that contain only allowable actions , or selection sliders that are capable of moving over only a legal range . In the context of IDE interventions , constraints could be used to prevent the learner from taking actions that are seen as incompatible with desirable trajectories , thus guiding the learner toward desirable trajectories . For instance , if the learner has not compiled her program in a long time , the IDE could disable editing ( a constraint ) and present a pop - up a message ( a notification ) informing the user that compilation might be a good idea . Note that constraints are an extreme form of intervention ; they prevent the user from performing further actions . Therefore , they should be used with care—only in situations that warrant them , such as when other , less extreme forms of interventions have been tried repeatedly and failed [ 46 ] . 3 . 3 . 3 Timing . The Timing dimension relates to when a given intervention should be presented to the learner . One option is for an intervention to be Persistent—that is , continuously - available in the IDE . A persistent intervention could be part of a tab or area within the IDE to which users navigate when they want to access the intervention . An example of this would be a learner dash - board ( e . g . , [ 77 ] ) that shows the learner’s current goals and progress . A second option is a Trig - gered intervention : an intervention that is dynamically delivered to the learner in response to the immediate actions the learner is taking . For example , if the learner performs an action that is rec - ognized as generally unproductive with respect to the learner’s goals , that action could trigger a notification that suggests an alternate course of action . Conversely , if a learner performs an action that is recognized as educationally beneficial , that action could trigger a notification that provides encouragement to keep up the good work . In a similar vein , another option is a State - Based intervention that is triggered when the user is recognized to transition into , or to remain within , a state that is believed to be educationally - unproductive . For example , the Normalized Programming State Model ( NPSM ) described earlier [ 22 ] characterizes programming behavior based on the current semantic and syntactic correctness of the program being edited . If a learner remains for long periods of time in a syntactically un - known state ( i . e . , the learner has not compiled the program ) , such behavior negatively correlates with programming success . Hence , a state - based intervention could remind the learner to compile her program . A final Timing option is to deliver an intervention On Demand , i . e . , only when the user explicitly requests it . For example , an IDE could provide a “Get Hint” button that , when clicked , would deliver a suggestion or critique to the learner ( e . g . , [ 29 , 62 ] ) . This type of timing differs somewhat from ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 15 the Persistent timing option in that , although it is always available , it is not actually visible to the learner unless the learner explicitly requests it . 3 . 4 Deliver Interventions In the final step of the process , an intervention designed in the previous step is delivered to learners through the IDE . This is the culmination of the learning analytics process . If designed effectively , an intervention should effect some sort of change in the learner . For example , the learner may be prompted to act differently , or the learner may experience some sort of attitudinal or physiological shift . Ideally , such changes will lead to improved learning processes and outcomes—improvements that manifest themselves in the “Collect Data” step and that can be recognized in the “Analyze Data” step . It is important to recognize that changes in response to interventions often happen gradually , not immediately . This is because the learning analytics process is inherently cyclical : learning data leads to interventions , which lead to new learning data and additional interventions . Hence , even if initial interventions lead to no detectable changes in learner behavior or attitudes , detectable changes may well occur over a longer period of time through an iterative process of progressive refinement . 4 APPLYING LEARNING THEORY TO DESIGN EFFECTIVE INTERVENTIONS In Section 3 . 3 , we described the design space of in - IDE interventions in terms of three high - level dimensions : Content , Presentation , and Timing . A key question arises— one that lies at the heart of the learning analytics process model we have described : Key : How can an intervention designer make choices along these dimensions so as to design Question : effective interventions—that is , interventions that effect positive changes in student learning processes , outcomes , and attitudes ? We argue that , whether explicated or not , some underlying learning theory must be brought to bear on the design of any intervention . Learning theories tend to be quite broad : they describe how people learn and what conditions best promote learning . However , in the discussion that follows , we necessarily assume a narrow learning context : students’ use of an IDE to complete individual programming assignments within some sort of computing course consisting of students and instructional personnel . Within that context , we briefly introduce four theories—two that focus on individual learning , and two that focus on social learning—and consider their implications for the design of interventions . Our aim is not to present a comprehensive review of learning theories , but rather to illustrate the ways in which a range of influential learning theories might be applied to IDE - based learning analytics . 4 . 1 Cognitive Load Theory Cognitive load theory ( CLT , see , e . g . , [ 59 , 76 ] ) is based on a view of how human cognition works . On this view , there are two types of memory : long - term memory and working memory . Whereas long - term memory can hold on to information for a long time , working memory is highly volatile . Moreover , long - term memory holds on to schemas ( i . e . , meaningful clusters of information ) that help synthesize information in working memory to facilitate the ability to perform tasks . However , learners tend not to have schemas relevant to the tasks that are being learned . In the absence of such schemas , learners need extra support and guidance from an instructional environment . How can an instructional environment such as an IDE provide such support and guidance ? According to CLT , instructional designers need to pay attention to the cognitive load imposed upon learners’ working memory as they engage in learning tasks . CLT defines three distinct types : ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 16 C . D . Hundhausen et al . — Intrinsic cognitive load is associated with the task’s inherent difficulty . For example , in the task of implementing a bubble sort algorithm , there is an inherent difficulty in defining the procedural logic required to iterate through the elements to be sorted . — Extraneous cognitive load is generated through the instructional design itself ; it is the over - head of instruction . For example , poorly - designed instruction may introduce unnecessary concepts ( e . g . , loop invariants when introducing the bubble sort algorithm ) that impose an extra load on learners . Conversely , well - designed instruction presents only what is nec - essary , and it does so using representations and examples that are well - matched to the instructional content ( e . g . , examples that animate the bubble sort in action ) . — Germane cognitive load is the load associated with constructing and automating new schemas—that is , it is the overhead of the learning process itself . Because it is a necessary part of learning , it should be promoted by instructional design . For instance , in learning the bubble sort algorithm , a learner needs to develop a schema that associates the iterative nature of the algorithm with a looping construct in a programming language . While these three forms of cognitive load are additive , working memory capacity is limited . Hence , a key objective for the instructional designer is to reduce extraneous load , in order to make more room for the germane load necessary for learning . With respect to the design of in - IDE interventions to support programming tasks , three key design principles follow from the need to reduce extraneous cognitive load : CLT1 : Place information that is necessary for the programming task physically close to where the task is being completed so that the learner does not have to use working memory to integrate it from multiple places . CLT2 : Reduce redundancy in the information presented during programming tasks so that the learner does not unnecessarily have to process the same information multiple times . CLT3 : Use both the visual and auditory channels to present complementary streams of informa - tion necessary for programming tasks in order to maximize working memory . We now illustrate how these principles could be applied to intervention design . CLT1 suggests that information relevant to task completion needs to be placed in the immediate context of the task . For the common task of resolving a syntax error , this suggests , for example , that —the definition of the syntax error must be clear to the learner without having to search elsewhere for the meaning of the error . If additional information is needed to interpret the error , it should be delivered as an intervention directly alongside the syntax error . —the actual line of code that has the syntax error should be located in close physical proximity to the syntax error message and any supplementary information . CLT2 requires that the redundancy of information presented during programming tasks in the IDE be reduced . For the example task of resolving a syntax error , this suggests that each error type should be presented just once , rather than multiple times , as is standard in IDEs . An intervention that follows from CLT2 would be to consolidate similar syntax errors , so that the learner has to read the error only once . Then , based on CLT1 , the lines on which each similar syntax error occurred could be presented side - by - side with the explanation of the syntax error . Lastly , CLT3 suggests the use of multiple channels ( auditory and visual ) to present comple - mentary information . In our syntax error example , this could be as simple as announcing the description of the error aurally , while presenting the locations of that error visually on the screen . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 17 4 . 2 Locus of Control Locus of control is the best - known concept of a broader learning theory put forward by Julian Rotter [ 67 ] . Rotter’s theory is social insofar as it conceives of learning as an interaction between the individual and the environment ; however , the locus of control concept focuses on individual beliefs . A learner with an internal locus of control believes she has control over her behavior and can take action to affect an outcome , whereas a learner with an external locus of control sees herself as powerless to take action ; the outcome is seen as being governed by external factors . In many situations , including situations in which one is learning a new skill such as program - ming , there are key benefits to having an internal locus of control [ 33 ] : those with an internal locus of control tend to work harder and achieve better learning outcomes . This is because they believe that working hard will pay off in the end . In contrast , those with an external locus of con - trol tend to lack motivation to stick with a task . They may see the outcome as out of their hands , and therefore not worth their effort . Rather than being a fixed perception , one’s locus of control beliefs can change over time , and can even be influenced by instruction . One proven instructional technique for shifting a learner’s locus of control belief from external to internal is known as attributional feedback [ 27 ] , which focuses on emphasizing that learners’ efforts and time are paying off , as well as emphasizing the positive strides that learners are making toward learning goals . This suggests the following high - level guidelines for the design of in - IDE interventions : LC1 . Help learners identify virtuous learning goals . LC2 . When learners appear not to be making progress toward learning goals , encourage them to stick with the programming process , emphasizing that their time and effort will pay off . LC3 . When learners appear to be making progress toward learning goals , acknowledge that progress and congratulate learners for their efforts . Applying LC1 to the design of IDE interventions suggests that some sort of learner dashboard of virtuous learning goals could be made available in the IDE . The goals could be aligned with best programming practices regarding , e . g . , time - on - task ( “Spend 5 hours on this assignment” ) or compilation behavior ( “Compile your program every 10 minutes” ) . This type of learner dashboard could be persistent ( e . g . , through a tab in the IDE ) , and could visually indicate the learner’s current progress toward each goal . Applying LC2 suggests that we first recognize points at which the learner is struggling , and then intervene with positive words of encouragement that emphasize that the learner will succeed if she sticks with it . For example , if we recognized that the learner’s compilation attempts consistently yielded errors , we could pop up an intervention to the effect of “Keep at it ! You can do it ! ” Conversely , LC3 tells us first to recognize successes in students’ learning processes—even small ones—and then to acknowledge those successes . For example , if the learner successfully executes her program for the first time without a runtime exception , we could present the learner with a congratulatory message such as “Great job : Your program runs without errors ! Keep up the good work ! ” 4 . 3 Situated Learning Theory and Cognitive Apprenticeship In contrast to the previous two theories , Situated Learning Theory ( SLT , [ 51 ] ) holds that knowledge is a social construction relative to a particular community of practice . On this view , learning is a fundamentally social endeavor that involves participating , in increasingly central ways , in the practices of a community . Learners start out as legitimate peripheral participants : learners engage in necessary community practices that , because they require less community expertise , are often ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 18 C . D . Hundhausen et al . undertaken by community newcomers . Over time , learners take on new forms of participation that are associated with more advanced community membership . Ultimately , they take on central community practices—a hallmark of being a “master” or “old - timer” in the community . Through this process of perpetuating the community , learning is seen as a progression through different forms of increasingly - central participatory practices . Lave and Wenger’s [ 51 ] situated learning model is inspired by ethnographic studies of infor - mal apprenticeship embedded in a range of communities of practice . In these informal educational settings , Lave and Wenger [ 51 ] intentionally , “ [ focus ] attention on the structure of social prac - tice rather than . . . pedagogy” ( p . 113 ) . But what implications does their account of social learning have for formal educational settings such as a computing course with individual programming assignments ? One serious attempt to apply the situated learning model to formal learning settings where cognitive skills are paramount is cognitive apprenticeship ( CA , [ 15 , 25 ] ) . CA aims to “make the thinking processes of a learning activity visible to both the students and the teacher” [ 34 ] , so that the methods of informal apprenticeship can be applied . Ghefaili [ 34 ] identifies key implications of the CA model for the design of learning technologies . The following list distills those implications into a set of five guidelines relevant to the design of IDE interventions : CA1 . Modeling : Provide access to the thinking and problem - solving processes of expert practi - tioners . CA2 . Coaching : Provide expert coaching at points in the learning process were learners are having difficulty . To the extent possible , the coaching should be tailored to the needs of individual learners . CA3 . Scaffolding : Provide scaffolding to enable learners to participate in tasks and activities that would be otherwise out of reach . Fade ( i . e . , gradually remove ) the scaffolding as learners become more proficient . CA4 . Articulation : Provide opportunities for learners to articulate what they are doing , including their knowledge , reasoning , and problem - solving strategies . CA5 . Reflection : Provide opportunities for learners to reflect on their work , and to compare their work to that of experts and other learners . With respect to CA1 , IDEs have an opportunity to connect learners with the performances of professional software engineers through videotaped or simulcast sessions in which software engineers think aloud as they code the solution to a programming problem similar to the one to be solved by learners . Rather than such sessions being in response to learner actions , they could coincide with the release of new course assignments . CA2 suggests the need to be able to identify when a learner is struggling and to provide coach - ing tailored to the learner’s needs . While the CA model does not provide advice on the specific form that such coaching should take , it might come in the form of “hints , feedback , . . . , additional modeling , or explanation” [ 34 ] . Given that , in IDE - based learning analytics , we assume no knowl - edge of the specific programming problem being solved , coaching interventions will necessarily be more general than could be achieved in an intelligent tutoring system , which can build learner and expert problem - solving models based on knowledge of the specific problem being solved . Thus , coaching interventions will necessarily dispense tips , feedback , and explanations based on general - purpose programming practices and knowledge—e . g . , “It looks like you may be having trouble defining an array . In C , you can use a statement like the following to define an array . . . ” With respect to providing scaffolding that ultimately fades away ( CA3 ) , Guzdial [ 35 ] focuses on the design of “software - realized scaffolding” for programming environments . A key form of ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 19 scaffolding identified by Guzdial is that of the checklist , which an IDE can provide to the learner in order to communicate and track the structure of the programming process that learners should fol - low . As learners complete items , those items could be automatically checked off , or learners could check them off themselves . For example , a general - purpose programming process involves edit - ing , compiling , and testing ( i . e . , running or debugging ) a program . A persistent IDE intervention could present this process ( perhaps graphically ) , show where the learner is in the process , remind the learner if she misses a step , and potentially require the learner to engage in a best practice in a given step . For example , running a program within debug mode makes it easier to diagnose runtime exceptions . An IDE constraint intervention could require learners to always execute their programs in debug mode . This form of scaffolding could fade over time , as learners demonstrate increased proficiency . Finally , an IDE might provide opportunities for learners to articulate ( CA4 ) and reflect on ( CA5 ) their process and reasoning in at least two complementary ways . First , an intervention could pe - riodically prompt the learner—perhaps at strategic points in the programming process—to write brief articulation and reflection statements . Second , an IDE could provide learners with an oppor - tunity to participate in a social network focused on programming , as is supported by the OSBIDE social programming environment [ 23 ] . In fact , there is no reason such an IDE - based social network could not include software professionals in addition to learners and instructors , thus supporting a broader social learning community in which to articulate and reflect on programming - related issues . This kind of broader online social network would seem to be very much in the spirit of the situated learning model described by Lave and Wenger [ 51 ] . 4 . 4 Social Cognitive Theory Like SLT , Social Cognitive Theory ( SCT , [ 9 ] ) is a social learning theory . It holds that people learn from acting , doing , and communicating with others . Central to SLT is the notion of self - efficacy [ 8 ] , which has been positively correlated with the concept of locus of control [ 48 ] described previously . Self - efficacy is a measure of a person’s perception of her capability to perform a task such as computer programming . In fact , a specific scale has been developed to measure programming self - efficacy [ 5 ] . In addition , it is known to play a role in overall career selection [ 7 ] , including the decision to major in computing [ 61 ] . According to SCT , an individual’s self - efficacy comes from four different sources : — Enactive experiences in which learners actually perform target skills . As these events are directly experienced , they tend to be the most influential in determining self - efficacy . Early enactive experiences are of critical importance and poor results can lead to significant drops in self - efficacy . — Vicarious experiences in which learners observe others’ performances . Generally , these ex - periences have a weaker impact on self - efficacy , but vicarious experiences have the power to override poor enactive experiences . — Verbal persuasion refers to after - the - fact affirmations of faith and encouragement . According to Bandura , verbal persuasion is likely to result in a boost to self - efficacy as long as the encouragement is within a reasonable distance to a person’s ability . Conversely , providing unrealistic encouragement is likely to lower self - efficacy . — Physiological state refers to the physical and mental well - being of the person and can affect self - efficacy . Bandura [ 8 ] cites research that indicates that being in a poor mood makes it more likely for a person to recall past failures , whereas a good mood makes it more likely to recall past triumphs . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 20 C . D . Hundhausen et al . How might IDE interventions positively influence self - efficacy beliefs ? The process of program - ming within an IDE is a type of enactive experience in and of itself ; it provides learners with direct experience with performing target programming skills without any form of in - IDE intervention . Hence , the concept of an enactive experience has no specific implications for the design of IDE interventions , other than to suggest that if students’ enactive programming experiences within an IDE are positive , they will tend to have higher programming self - efficacy . Likewise , physiological state would appear to be outside the control of an IDE ( although it could certainly be measured or approximated through an IDE , as suggested in Table 2 ) . However , IDE interventions could be used to facilitate both vicarious experiences and verbal persuasion . For example , by making a learner’s programming process more visible to peers and in - structors , an IDE that includes a built - in social networking - style activity feed such as OSBIDE [ 23 ] can facilitate both vicarious experiences and verbal persuasion . In this kind of social environment , interventions could be used to draw a learner’s attention to others who are experiencing similar struggles ( vicarious experiences ) , and to provide opportunities for learners to encourage each other during the programming process through activity feed messages ( verbal persuasion ) . Table 3 summarizes the design implications of each theory vis - à - vis the three design dimen - sions of interventions described in Section 3 . 3 . Unsurprisingly , the four theories differ widely with respect to their design guidance . CLT is concerned with presenting only the information salient to the task precisely when that information is needed . LOC focuses on delivering encouragement and praise based on how students are performing relative to learning goals . SLT / CA advocates expert modeling at the beginning of the programming process , expert feedback as needed , soft - ware scaffolding to push learners to go beyond their current capabilities , and learner articulation and reflection throughout the learning process . Finally , SCT stresses the need for learners to eval - uate themselves relative to their peers throughout the learning process , and to give and receive messages of affirmation and praise . 5 CALL - TO - ACTION In this article , we have presented a review of IDE - based learning analytics structured around a process model with four activities : ( 1 ) Collect data , ( 2 ) Analyze data , ( 3 ) Design interventions , and ( 4 ) Deliver interventions . Focusing on activity ( 3 ) , we then reviewed four relevant learning theories and identified their implications for design . Based on the foregoing review , we conclude by presenting a call - to - action for IDE - based learning analytics research : A set of agenda items that we believe will help advance the field . 5 . 1 Study Privacy Concerns The automatic collection of the kinds of learning data described in this article raises obvious eth - ical concerns with respect to data privacy and security . Indeed , just as consumers may not be comfortable with the automatic collection of their online activity by search engines and websites , so too may computing students be uncomfortable with the automatic collection of their in - IDE activities . What data should be collected ? How should the data be stored ? Who will have access to the data ? For what purposes will the data be used ? Can data collection be terminated at any point ? Per standard ethical practices , IDEs that automatically collect data should provide , in a clear policy statement , answers to questions like these . Moreover , in any research endeavor involving the automatic collection of IDE data , an IDE should automatically collect data on a given student only if that student explicitly grants informed consent . As studies that use automatically - collected IDE data become more common in computing ed - ucation , researchers will need to gain a better understanding of increasingly more stringent laws governing educational data collection ( see , e . g . , [ 39 ] ) , as well as the specific privacy concerns and ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 21 Table 3 . Comparison of Four Learning Theories vis - à - vis Their Implications for Making Design Choices along Three Dimensions of Intervention Design limits of those they are studying . Addressing questions like the following will thus become in - creasingly important : —What level of privacy do students expect to maintain as they work on programming assign - ments , potentially in collaboration with others ? —To what degree should students be able to customize the amount and type of their own learning data that they share with their classmates , instructors , and researchers ? What cus - tomizations are most important ? —Should students be able to share their data anonymously with others ? Will such sharing have the same educational value as it would if students’ identities were associated with their data ? —Might students and instructors be willing to participate in learning communities that go beyond local classes , in which the sharing and discussion of learning data across courses are central community activities ? ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 22 C . D . Hundhausen et al . 5 . 2 Develop a More Unified Research Infrastructure 5 . 2 . 1 Standardized Data Format . Our review indicated that while numerous IDEs now support automatic data collection , IDEs vary widely with respect to both the data they collect , and the manner in which they collect the same types of data ( see Table 1 ) . For instance , we saw that while many IDEs collect editing data , some do so at the keystroke level , while others log snapshots of the code when the code is compiled or saved . Because the data collection approaches of IDEs differ widely , it is difficult to perform empirical comparisons across IDEs . We believe this is unfortunate : As the field of IDE - based learning analyt - ics matures , it will be increasingly important to study learning differences promoted by different IDEs . Therefore , we think it is in the best interest of researchers who collect and analyze IDE log data to work collaboratively toward a standardized format for IDE log data . Learning analytics researchers working in this space could go even further by building a uni - versal repository of anonymized IDE log data . In the spirit of BlueJ’s Black Box project [ 16 ] , we envision a publicly - accessible , web - based repository of large anonymized corpora of data collected through numerous IDEs , in many different computing courses , and at many different institutions . Such a repository would make it possible for more researchers , even those who do not collect their own data , to conduct the kinds of cross - IDE , cross - language studies that are conspicuously lacking in computing education research . 5 . 2 . 2 Standardized Intervention Architecture . We believe that the non - standard nature of the plug - in architectures supported by IDEs also poses a barrier to conducting cross - IDE , cross - language research . While plug - in architectures make it increasingly easy to augment IDEs with facilities both to automatically collect data and to deliver dynamic interventions , building such facilities remains a specialized craft . In order to do so , one needs to become intimately familiar with the plug - in architecture of a specific IDE . Unfortunately , implementation skills do not readily transfer between different IDEs , as each plug - in architecture has its own peculiar standards and libraries . We believe that this barrier could be reduced through the development of a standard API for implementing IDE plug - ins . Of course , teams of programmers would need to write , for each IDE , the specialized code to support the API . However , if such an API could be ultimately supported by a broad range of IDEs , data collection and intervention design could be more easily replicated across IDEs . This , in turn , would increase the likelihood that ( a ) studies of student programming processes , and the impact of in - IDE interventions , could be replicated across a broad range of IDEs , and ultimately that ( b ) there would be an increase in the kinds of cross - IDE , cross - language empirical studies that we believe are important to advancing the field . 5 . 3 Develop IDE Facilities to Collect and Analyze Broader Range of Data Our review revealed that IDE - based learning analytics research presently collects and analyzes only a small fraction of the possible data that could be automatically collected ( see Table 2 ) . The present focus is squarely on programming process data related to editing , compiling , running , and testing programs . This focus neglects many of the augmented data we identified , including social data , survey / quiz data , and physiological data . Our own research suggests that we can develop better predictive models of performance and gain greater insight into the learning process by studying the interplay of multiple types of IDE data [ 24 ] . We also suspect that , with a broader array of IDE data , interventions can be designed so as to be not only more helpful to learners , but also more responsive to their needs . Therefore , we think it will be important , in future research , to develop IDE facilities that support the collection analysis of more diverse sets of data , including the augmented data described in Figure 3 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 23 5 . 4 Design and Evaluate Interventions Based on Theory In Section 4 , we reviewed a range of learning theories that might be brought to bear on the design of IDE - based interventions . In our review , we identified only a couple of lines of past research into intervention design that were actually driven by learning theory : Guzdial [ 35 ] and Carter and Hundhausen [ 23 ] . In computing education research in general , and IDE - based learning ana - lytics research in particular , our observation has been that research frequently proceeds in an ad hoc fashion . Key research decisions are often dictated by the particular courses and educational technologies to which a researcher has access , rather than by a theoretical account of what might actually be effective . We certainly sympathize with this state of affairs : research must inevitably proceed within the messy confines of real courses and institutions . At the same time , our review suggests that learning theory has a lot to say about how to design educationally - effective inter - ventions , including what they should include ( content ) , what they should look like ( presentation ) , and when they should be delivered ( timing ) . We argue that , in order to have the best chance to systematically advance the field , researchers need to proceed from explicit theoretical orientations like those presented in Section 4 . We hope that our review of applicable learning theories can help motivate a shift toward more theoretically - driven research into IDE - based learning analytics . REFERENCES [ 1 ] A . Ahadi , R . Lister , H . Haapala , and A . Vihavainen . 2015 . Exploring machine learning methods to automatically identify students in need of assistance . In Proceedings of the 11th Annual International Conference on International Computing Education Research . [ 2 ] L . Ahonen , B . Cowley , J . Torniainen , A . Ukkonen , A . Vihavainen , and K . Puolamäki . 2016 . Cognitive collaboration found in cardiac physiology : Study in classroom environment . PLoS ONE 11 , 7 ( 2016 ) , e0159178 . https : / / doi . org / 10 . 1371 / journal . pone . 0159178 . [ 3 ] J . R . Anderson , F . G . Conrad , and A . T . Corbett . 1989 . Skill acquisition and the LISP tutor . Cognitive Science 13 , ( 1989 ) , 467 – 506 . [ 4 ] J . R . Anderson and E . Skwarecki . 1986 . The automated tutoring of introductory computer programming . Commun . ACM . 29 , 9 ( Sep . 1986 ) , 842 – 849 . [ 5 ] P . Askar and D . Davenport . 2009 . An investigation of factors related to self efficacy for java programming among engineering students . Turkish Journal of Educational Technology 8 , 1 ( 2009 ) , 26 – 32 . [ 6 ] R . S . J . Baker and G . Siemens . 2014 . Educational data mining and learning analytics . The Cambridge Handbook of the Learning Sciences . Cambridge University Press . 253 – 274 . [ 7 ] A . Bandura , C . Barbaranelli , G . V . Caprara , and C . Pastorelli . 2001 . Self - efficacy beliefs as shapers of children’s aspi - rations and career trajectories . Child Development . 72 , 1 ( Feb . 2001 ) , 187 – 206 . [ 8 ] A . Bandura . 1997 . Self - efficacy : the Exercise of Control . Worth Publishers . [ 9 ] A . Bandura . 1986 . Social Foundations of Thought and Action : A Social Cognitive Theory . Prentice Hall . [ 10 ] A . Barr , M . Beard , and R . C . Atkinson . 1976 . The computer as a tutorial laboratory : The Stanford BIP Project . Inter - national Journal of Man - Machine Studies 8 , ( 1976 ) , 567 – 596 . [ 11 ] R . Bednarik and M . Tukiainen . 2004 . Visual attention tracking during program debugging . In Proceedings of the 3rd Nordic Conference on Human - computer Interaction . 331 – 334 . [ 12 ] A . Begel . 2016 . Fun with software developers and biometrics : Invited talk . In Proceedings of the 1st International Workshop on Emotion Awareness in Software Engineering . 1 – 2 . [ 13 ] J . Bonar and E . Soloway . 1983 . Uncovering principles of novice programming . In Proceedings of the 10th ACM SIGACT - SIGPLAN Symposium on Principles of Programming Languages ( POPL’83 ) . 10 – 13 . [ 14 ] N . Bosch and S . K . D’Mello . 2013 . Sequential patterns of affective states of novice programmers . In Proceedings of the Workshop on AI - supported Education for Computer Science at the 16th International Conference on Artificial Intelligence in Education ( AIEDws’13 ) , E . Walker and C . - K . Looi ( Eds . ) . [ 15 ] J . S . Brown . 1989 . Situated cognition and the culture of learning . Educational Researcher 18 , 1 ( 1989 ) , 32 – 42 . [ 16 ] N . C . C . Brown , M . Kölling , D . McCall , and I . Utting . 2014 . Blackbox : A large scale repository of novice programmers’ activity . In Proceedings of the 45th ACM Technical Symposium on Computer Science Education . ACM , 223 – 228 . [ 17 ] K . BuffardiandS . H . Edwards . 2013 . Impactsofadaptivefeedbackonteachingtest - drivendevelopment . In Proceedings of the 44th ACM Technical Symposium on Computer Science Education . 293 – 298 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 24 C . D . Hundhausen et al . [ 18 ] T . Busjahn et al . 2014 . Eye tracking in computing education . In Proceedings of the 10th Annual Conference on Interna - tional Computing Education Research . [ 19 ] S . K . Card , T . P . Moran , and A . Newell . 1983 . The Psychology of Human - Computer Interaction . Lawrence Erlbaum Associates . [ 20 ] R . Cardell - Oliver . 2011 . How can software metrics help novice programmers ? In Proceedings of the Thirteenth Aus - tralasian Computing Education Conference - Volume 114 . 55 – 62 . [ 21 ] A . C . Carter . 2013 . OSBIDE . http : / / osbide . codeplex . com . Accessed date 26 July 2016 . [ 22 ] A . S . Carter , C . D . Hundhausen , and O . Adesope . 2015 . The normalized programming state model : Predicting student performance in computing courses based on programming behavior . In Proceedings of the 11th Annual International Conference on International Computing Education Research . ACM , 141 – 150 . [ 23 ] A . S . Carter and C . D . Hundhausen . 2015 . The design of a programming environment to support greater social aware - ness and participation in early computing courses . Journal of Computing Sciences in Colleges 31 , 1 ( 2015 ) , 143 – 153 . [ 24 ] A . S . Carter and C . D . Hundhausen . 2016 . With a little help from my friends : An empirical study of the interplay of students’ social activities , programming activities , and course success . In Proceedings of the 2016 ACM Conference on International Computing Education Research . ACM , 201 – 209 . [ 25 ] A . Collins , J . S . Brown , and S . E . Newman . 1989 . Cognitive apprenticeship : Teaching the craft of reading , writing , and mathematics . Knowing , Learning , and Instruction : Essays in honor of Robert Glaser . L . B . Resnick , ed . Lawrence Erlbaum Associates . 453 – 494 . [ 26 ] A . T . Corbett and J . R . Anderson . 1992 . The LISP intelligent tutoring system : Research in skill acquisition . In Computer Assisted Instruction and Intelligent Tutoring Systems : Establishing Communication and Collaboration . Erlbaum . 73 – 110 . [ 27 ] R . G . Craven , H . W . Marsh , and R . L . Debus . 1991 . Effects of internally focused feedback and attributional feedback on enhancement of academic self - concept . Journal of Educational Psychology 83 , 1 ( 1991 ) , 17 – 27 . [ 28 ] P . Denny , A . Luxton - Reilly , and D . Carpenter . 2014 . Enhancing syntax error messages appears ineffectual . In Proceed - ings of the 2014 Conference on Innovation & Technology in Computer Science Education . 273 – 278 . [ 29 ] A . K . Dominguez , K . Yacef , and J . R . Curran . 2010 . Data mining for individualized hints in e - Learning . In Proceedings of the International Conference on Educational Data Mining . 91 – 100 . [ 30 ] Eclipse . org . 2016 . Usage Data Collector User Guide . https : / / eclipse . org / org / usagedata / userguide . php . Accessed date 26 July 2016 . [ 31 ] S . H . Edwards and M . A . Perez - Quinones . 2008 . Web - CAT : Automatically grading programming assignments . In Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education . 328 – 328 . [ 32 ] K . A . Ericsson and H . A . Simon . 1984 . Protocol Analysis : Verbal Reports as Data . MIT Press . [ 33 ] M . J . Findley and H . M . Cooper . 1983 . Locus of control and academic achievement : A literature review . Journal of Personality and Social Psychology 44 , 2 ( 1983 ) , 419 – 427 . [ 34 ] A . Ghefaili . 2003 . Cognitive apprenticeship , technology , and the contextualization of learning environments . Journal of Educational Computing , Design & Online Learning 4 ( 2003 ) , 1 – 27 . [ 35 ] M . Guzdial . 1994 . Software - realized scaffolding to facilitate programming for science learning . Interactive learning Environments 4 , 1 ( 1994 ) , 1 – 44 . [ 36 ] M . Guzdial , L . Hohmann , M . Konneman , C . Walton , and E . Soloway . 1998 . Supporting programming and learning - to - program with an integrated CAD and scaffolding workbench . Interactive Learning Environments 6 , 1 – 2 ( 1998 ) , 143 – 179 . [ 37 ] L . Haaranen , P . Ihantola , L . Hakulinen , and A . Korhonen . 2014 . How ( not ) to introduce badges to online exercises . In Proceedings of the 45th ACM Technical Symposium on Computer Science Education . 33 – 38 . [ 38 ] B . Hartmann , D . MacDougall , J . Brandt , and S . R . Klemmer . 2010 . What would other programmers do : Suggesting solutions to error messages . In Proceedings of the 28th Conference on Human Factors in Computing Systems . ACM , 1019 – 1028 . [ 39 ] B . Herold . 2014 . “Landmark” student - data - privacy law enacted in California . Education Week . [ 40 ] C . D . Hundhausen , S . A . Douglas , and J . T . Stasko . 2002 . A meta - study of algorithm visualization effectiveness . Journal of Visual Languages and Computing 13 , 3 ( 2002 ) , 259 – 290 . [ 41 ] C . D . Hundhausen , A . Agrawal , D . Fairbrother , and M . Trevisan . 2010 . Does studio - based instruction work in CS 1 ? : An empirical comparison with a traditional approach . In Proceedings of the 41st ACM Technical Symposium on Computer Science Education . ACM , 500 – 504 . [ 42 ] P . Ihantola , J . Sorva , and A . Vihavainen . 2014 . Automatically detectable indicators of programming assignment diffi - culty . In Proceedings of the 15th Annual Conference on Information Technology Education . 33 – 38 . [ 43 ] P . Ihantola et al . 2015 . Educational data mining and learning analytics in programming : Literature review and case studies . In Proceedings of the 2015 ITiCSE on Working Group Reports ( 2015 ) . New York , NY , 41 – 63 . [ 44 ] M . C . Jadud . 2006 . Methods and tools for exploring novice compilation behaviour . In Proceedings of the Second Inter - national Workshop on Computing Education Research . ACM , 73 – 84 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . IDE - Based Learning Analytics for Computing Education 11 : 25 [ 45 ] W . Jin , T . Barnes , M . Eagle , M . W . Johnson , and L . Lehmann . 2012 . Program representation for automatic hint genera - tion for a data - driven novice programming tutor . In Proceedings of the International Conference on Intelligent Tutorins Systems . Springer Verlag . 304 – 309 . [ 46 ] J . Johnson . 2010 . Designing with the Mind in Mind : Simple Guide to Understanding User Interface Design Rules . Morgan Kaufmann Publishers Inc . [ 47 ] P . Johnson . 2010 . Hackystat - A framework for collection , analysis , visualization , interpretation , annotation , and dissemination of software development process and product data . https : / / code . google . com / p / hackystat / . Accessed date 11 August 2016 . [ 48 ] T . A . Judge , A . Erez , J . E . Bono , and C . J . Thoresen . 2002 . Are measures of self - esteem , neuroticism , locus of control , and generalized self - efficacy indicators of a common core construct ? Journal of Personality and Social Psychology 83 , 3 ( Sep . 2002 ) , 693 – 710 . [ 49 ] K . Kevic , B . M . Walters , T . R . Shaffer , B . Sharif , D . C . Shepherd , and T . Fritz . 2015 . Tracing software developers’ eyes and interactions for change tasks . In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering ( 2015 ) . New York , NY , 202 – 213 . [ 50 ] M . Kölling , B . Quig , A . Patterson , and J . Rosenberg . 2003 . The BlueJ system and its pedagogy . Journal of Compuer Science Education 13 , 4 ( 2003 ) , 249 – 268 . [ 51 ] J . Lave and E . Wenger . 1991 . Situated Learning : Legitimate Peripheral Participation . Cambridge University Press . [ 52 ] J . A . Luke . 2015 . Continuously Collecting Software Development Event Data As Students Program . M . S . Thesis , Virginia Tech . [ 53 ] W . Ma , O . O . Adesope , J . C . Nesbit , and Q . Liu . 2014 . Intelligent tutoring systems and learning outcomes : A meta - analytic survey . Journal of Educational Psychology 106 , 2007 ( 2014 ) , 901 – 918 . [ 54 ] S . C . Müller . 2015 . Measuring software developers’ perceived difficulty with biometric sensors . In Proceedings of the 37th International Conference on Software Engineering - Volume 2 ( 2015 ) . Piscataway , NJ , 887 – 890 . [ 55 ] S . C . MüllerandT . Fritz . 2016 . Using ( Bio ) metricstopredictcodequalityonline . In Proceedingsofthe38thInternational Conference on Software Engineering . 452 – 463 . [ 56 ] Netbeans . org . 2016 . Netbeans Usage Data Tracking . http : / / netbeans . org / about / usage - tracking . html . Accessed date 26 July 2016 . [ 57 ] D . A . Norman . 2013 . The Design of Everyday Things : Revised and Expanded Edition . Basic Books . [ 58 ] C . Norris , F . Barry , J . B . Fenwick Jr , K . Reid , and J . Rountree . 2008 . ClockIt : Collecting quantitative data on how beginning software developers really work . SIGCSE Bull . 40 , 3 ( Jun . 2008 ) , 37 – 41 . [ 59 ] F . Paas , A . Renkl , and J . Sweller . 2003 . Cognitive load theory and instructional design : Recent developments . Educa - tional Psychologist . 38 , 1 ( 2003 ) , 1 – 4 . [ 60 ] A . Papancea , J . Spacco , and D . Hovemeyer . 2013 . An open platform for managing short programming exercises . In Proceedings of the 9th Annual International ACM Conference on International Computing Education Research . 47 – 52 . [ 61 ] M . Papastergiou . 2008 . Arecomputerscienceandinformationtechnologystillmasculinefields ? Highschoolstudents’ perceptions . Computers & Education 51 , ( 2008 ) , 594 – 608 . [ 62 ] C . Piech , M . Sahami , J . Huang , and L . Guibas . 2015 . Autonomously generating hints by inferring problem solving policies . In Proceedings of the 2nd ACM Conference on Learning @ Scale . 195 – 204 . [ 63 ] K . Rivers and K . R . Koedinger . 2013 . Automatic generation of programming feedback : A data - driven approach . In Proceedings of the 1st Workshop on AI - supported Education for Computer Science . [ 64 ] M . M . T . Rodrigo et al . 2009 . Affective and behavioral predictors of novice programmer achievement . SIGCSE Bull . 41 , 3 ( Jul . 2009 ) , 156 – 160 . [ 65 ] M . M . T . Rodrigo and R . S . J . d . Baker . 2009 . Coarse - grained detection of student frustration in an introductory pro - grammingcourse . In Proceedingsofthe5thInternationalWorkshoponComputingEducationResearch Workshop . 75 – 80 . [ 66 ] M . B . Rosson , J . M . Carroll , and H . Sinha . 2011 . Orientation of undergraduates toward careers in the computer and information sciences : Gender , self - efficacy and social support . ACM Transactions on Computing Education 11 , 3 ( Oct . 2011 ) , 1 – 23 . [ 67 ] J . B . Rotter . 1966 . Generalized expectancies for internal versus external control of reinforcement . Psychological Mono - graphs : General and Applied 80 , 1 ( 1966 ) , 1 – 28 . [ 68 ] W . Scacchi . 2002 . Process models in software engineering . In Encyclopedia of Software Engineering . John Wiley & Sons , Inc . [ 69 ] Y . Shi , N . Ruiz , R . Taib , E . Choi , and F . Chen . 2007 . Galvanic skin response ( GSR ) as an index of cognitive load . In CHI’07 Extended Abstracts on Human Factors in Computing Systems . 2651 – 2656 . [ 70 ] B . Shneiderman . 1976 . Exploratory experiments in programmer behavior . International Journal of Man - Machine Stud - ies 5 , 2 ( 1976 ) , 123 – 143 . [ 71 ] J . Spacco , D . Hovemeyer , and W . Pugh . 2004 . An eclipse - based course project snapshot and submission system . In Proceedings of the 2004 OOPSLA Workshop on Eclipse Technology Exchange . 52 – 56 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 . 11 : 26 C . D . Hundhausen et al . [ 72 ] J . Spacco , D . Hovemeyer , W . Pugh , F . Emad , J . K . Hollingsworth , and N . Padua - Perez . 2006 . Experiences with mar - moset : designing and using an advanced submission and testing system for programming courses . SIGCSE Bull . 38 , 3 ( Jun . 2006 ) , 13 – 17 . [ 73 ] J . Spacco , D . Fossati , J . Stamper , andK . Rivers . 2013 . Towardsimprovingprogramminghabitstocreatebettercomputer science course outcomes . In Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education . 243 – 248 . [ 74 ] Stack Overflow . 2012 . Stack Overflow . http : / / stackoverflow . com / . Accessed date 21 November 2016 . [ 75 ] J . Stamper , M . Eagle , T . Barnes , and M . Croy . 2013 . Experimental evaluation of automatic hint generation for a logic tutor . International Journal of Artificial Intelligence in Education 22 , 1 – 2 ( 2013 ) , 3 – 17 . [ 76 ] J . Sweller , J . J . Van Merrienboer , and F . G . W . Paas . 1998 . Cognitive architecture and instructional design . Educational Psychology Review 10 , 3 ( 1998 ) , 251 – 296 . [ 77 ] K . Verbert et al . 2014 . Learning dashboards : An overview and future research opportunities . Personal Ubiquitous Comput . 18 , 6 ( Aug . 2014 ) , 1499 – 1514 . [ 78 ] K . Verbert and E . Duval . 2012 . Learning analytics . ELEED : E - Learning and Education 8 , 1 ( 2012 ) . Retrieved from https : / / eleed . campussource . de / archive / 8 / 3336 Google Scholar . [ 79 ] A . Vihavainen , T . Vikberg , M . Luukkainen , and M . Pärtel . 2013 . Scaffolding students’ learning using Test My Code . In Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education . 117 – 122 . [ 80 ] C . Watson , F . W . B . Li , and J . L . Godwin . 2013 . Predicting performance in an introductory programming course by logging and analyzing student programming behavior . In Proceedings of the 2013 IEEE 13th International Conference on Advanced Learning Technologies . 319 – 323 . Received August 2016 ; revised May 2017 ; accepted June 2017 ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 11 . Publication date : August 2017 .