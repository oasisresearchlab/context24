Bans vs . Warning Labels : Examining Support for Community - wide Moderation Interventions Shagun Jhaver Social media platforms like Facebook and Reddit host thousands of independently governed online communities . These platforms sanction communities that frequently violate platform policies ; however , user perceptions of such sanctions remain unclear . In a pre - registered survey conducted in the US , I explore user perceptions of content moderation for communities that frequently feature hate speech , violent content , and sexually explicit content . Two community - wide moderation interventions are tested : ( 1 ) community bans , where all community posts and access to them are removed , and ( 2 ) community warning labels , where an interstitial warning label precedes access . I examine how third - person effects and support for free speech mediate user approval of these interventions . My findings show that presumed effects on others ( PME3 ) is a significant predictor of backing for both interventions , while free speech beliefs significantly influence participants’ inclination for using warning labels . I discuss the implications of these results for platform governance and free speech scholarship . Introduction Today , social media companies like Facebook , Reddit , and YouTube are subject to increasing pressure from law enforcement agencies and private actors worldwide to act more forcefully to address online harm . In response , platforms are investing in better monitoring of content posted on their sites and experimenting with a new range of moderation strategies . One such strategy is to sanction an entire online community that hosts inappropriate content to quickly tamp down its spread . Such actions are prevalent since they let platforms demonstrate their commitment to reducing harm . However , these sanctions can be deemed excessively broad . By removing non - offending speech alongside offending content , they raise questions about violating some users’ freedom of speech . I examine the public response to community - wide moderation interventions in the context of three categories of norm - violating online communities : those frequently featuring hate speech , violent content , and sexually explicit content . Informed by the third - person effects literature ( Davison , 1983 ; Guo and Johnson , 2020 ) , I evaluate the role that perceived influences of communities hosting such content have on bystanders’ willingness to support two types of community - wide sanctions prevalent across social media : 1 ) Community bans , where all community content and access to it are permanently removed ( Perez , 2020 ; Chandrasekharan et al . , 2017 ) . Sites like Facebook ( on Facebook Groups ) ( Zadrozny , 2021 ) , Reddit ( on its subreddits ) ( Chandrasekharan et al . , 2017 ) , and YouTube ( on its channels ) ( Rauchfleisch and Kaiser , 2021 ) frequently deploy such bans . 2 ) Community warning labels , where a warning label precedes access to community content . Quarantines on Reddit ( Shen and Rose ; Carlson et al . , 2020 ) exemplify this intervention . Similarly , Facebook shows a warning message to users about to join a group that allows posts violating the site’s community standards ( Musil , 2021 ) . Other examples of warnings about content access have been observed on Instagram , search engines , and Pinterest ( Chandrasekharan et al . , 2022 ) . This article also explores how free speech support influences user perceptions of community - wide interventions . In the United States , constitutional rights such as freedom of speech and association apply vis - a - vis the government , not private companies like Facebook and Reddit ( Klonick , 2017 ) . Therefore , technically , the legal protections of freedom of speech do not apply to social media platforms ( Grimmelmann , 2009 ) . However , since a few large platforms have become the central avenues for our public discourse – “our global town square , ” as Twitter’s former CEO observed ( Costolo , 2013 ) – they essentially serve as public spaces . Platforms’ terms of service and moderation decisions are integral to how we participate in those spaces ( Suzor , 2019 ) . Therefore , when companies make major moderation decisions , such as community - wide bans or sanctions of public figures ( Romm and Dwoskin , 2019 ; Jhaver et al . , 2021 ) , they raise issues akin to free speech and censorship that warrant careful consideration . A single community - wide sanction decision can influence or prohibit all subsequent community activity , thereby regulating thousands of bad actors at once . Therefore , such sanctions present an enticing strategy for platforms to address the ever - growing challenges of scale . However , their deployment also raises , perhaps more strongly than ever , longstanding , familiar tensions between libertarian tendencies ( e . g . , freedom of speech ) and authoritarian practices ( e . g . , censorship ) ( Ullmann and Tomalin , 2020 ) . Though prior work theoretically examined the tensions involved in community - wide sanctions ( Carlson et al . , 2020 ) , this article evaluates broader public opinions on these issues . Understanding the levels of public support for different interventions can help guide future community - wide moderation practices on social media platforms . Literature Review and Hypotheses Content Moderation and Community - wide Moderation Interventions As platforms grow , the challenge of efficiently and economically moderating high volumes of user - generated content becomes increasingly salient ( Glaser , 2018 ; Jhaver et al . , 2019b ; Madrigal , 2018 ) . To address this challenge , each platform has created its own set of complex , ad hoc systems ( Jhaver et al . , 2019b ) that apply moderation actions at different levels of granularity – from removing a post or de - platforming a user ( Jhaver et al . , 2021 ) to sanctioning an entire community ( Carlson et al . , 2020 ; Chandrasekharan et al . , 2022 ) . My focus is on community - wide moderation sanctions . I distinguish these from user - level sanctions that apply to only a single post or poster . Prior research examined user perceptions of the censorship of hateful messages ( Riedl et al . , 2021 ; Guo and Johnson , 2020 ; Jang and Kim , 2018 ) and user reactions to experiencing sanctions themselves ( Jhaver et al . , 2019a ; West , 2018 ) . However , as far as I know , third - party user perceptions of community - wide moderation interventions have yet to be explored . This article seeks to take the first steps toward filling this gap because user support and attitudes toward content moderation should be critical considerations for platforms when designing their moderation policies ( Riedl et al . , 2021 ) , especially regarding sweeping actions that impact entire communities . I specifically address communities like Facebook Groups and Reddit’s subreddits , which are governed by volunteer moderators usually selected from users active within the community ( Matias , 2019 ; Jhaver et al . , 2019b ; Malinen ) . These moderators work autonomously , creating their community submission guidelines , reviewing all posted content , and meting out sanctions when warranted ( Seering et al . , 2019 ) . However , in some cases , when a community violates the platform’s terms of service , platform staff , not moderators , exercise their authority to regulate it . Prior research computationally evaluated the effects of these interventions on the behavior of affected community members , finding their utility in decreasing hate speech usage ( Chandrasekharan et al . , 2022 ; Chandrasekharan et al . , 2017 ; Horta Ribeiro et al . , 2021 ; Newell et al . ; Saleem and Ruths , 2018 ; Trujillo and Cresci , 2022 ) . I add to this literature by surfacing the perspectives of bystanders , a large portion of users who are not directly impacted by the community - wide moderation but are , nonetheless , essential stakeholders for platforms . When bystanders perceive moderation of other users’ content as fair , they are more likely to reduce inappropriate behavior through social learning ( Bandura , 2005 ) and increase their efforts to enforce norms , e . g . , through reporting infractions ( Naab et al . , 2018 ) . I examine the factors that play a role in bystander support for bans and warning labels on inappropriate online communities . Banning Versus Warning Decisions about whether a platform should ban an offensive community or merely put a warning label on it can be complex and subjective . Platform policies usually address the moderation of individual pieces of content . However , it can be difficult for platforms to determine and specify which behavioral features of an entire community warrant banning it instead of just informing users about offensive content through labeling . While research on both community bans ( Chandrasekharan et al . , 2017 ) and community warning labels ( Chandrasekharan et al . , 2022 ; Carlson et al . , 2020 ; Shen and Rose ; Ullmann and Tomalin , 2020 ) continues to expand , I have found no prior study that directly compares user preferences for the two interventions . However , in the related context of user preferences for moderating individual news articles , user survey data from Atreja et al . ( 2022 ) show that attaching warning labels is a more popular option than removing the news articles containing misinformation . Similarly , Wihbey et al . ( 2022 ) found that across four diverse democracies , people show more significant support for moderation using content labeling than for shutting down accounts . These insights suggest that a warning label before a community would be more popular than banning it . Therefore , I raise the following hypothesis : H1 : For each type of norm - violating community , the average support for adding warning labels before it will exceed the average support for banning it . Third - Person Effects ( TPE ) Davison ( 1983 ) originally proposed the third - person effect ( TPE ) hypothesis , which posits that individuals exposed to a mass media message will expect that the communication’s greatest impact “will not be on ‘me’ or ‘you , ’ but on ‘them’ – the third persons . ” According to Davison , TPE has two distinctive components : ( 1 ) the perceptual component , which predicts that people perceive themselves as invulnerable to the effects of media and perceive others as more greatly affected , and ( 2 ) the behavioral component , which predicts that the perceptions of negative effects on others would lead to certain cognitive , attitudinal , and behavioral consequences ( Chung et al . , 2015 ; Peiser and Peter , 2000 ) . The perceptual component of TPE is widely established across a wide range of message types , effect domains , and populations . Historically , TPE was first observed in perceptions of the effects of traditional mass media , such as television violence ( Scharrer , 2002 ) , defamatory news articles ( Gunther , 1991 ) , product advertising ( Gunther and Mundy , 1993 ) , and rap music ( McLeod et al . , 1997 ) . More recently , the study of TPE has been extended to digital media , documenting its appearance on blogs , websites , and social media ( Lev - On , 2017 ; Li , 2008 ) . For example , Paradise and Sullivan ( 2012 ) surveyed 357 undergraduates in the Northeastern US ; they found that young people perceived Facebook usage to have a more significant negative impact on others’ personal relationships , future employment , and privacy than on their own . The self - other disparity in TPE is particularly notable when the content of persuasive communication is socially undesirable or harmful ( Lim , 2017 ) . Applying TPE to the context of inappropriate online communities on social media , I expect that users would perceive such communities to exert greater influence on others than on themselves . The second hypothesis can thus be raised : H2 : For each type of norm - violating community , participants will perceive a greater effect of that community on others than on themselves . The behavioral component of TPE predicts that perceiving others as more influenced than oneself will lead individuals to take remedial actions , such as supporting censorship of media content ( Davison , 1983 ; Lev - On , 2017 ) . Scholars have argued that while the perceptual component of TPE is intriguing , the behavioral component is more relevant to social researchers and media practitioners ( Paradise and Sullivan , 2012 ; McLeod et al . , 1997 ) . For example , Gunther ( 2006 ) contends that TPE’s significance lies in predicting individuals’ readiness to advocate action to shield others from perceived negative media impact . Most prior literature on TPE consequences has found that it leads to support for censorship or governmental regulation of the media ( Golan and Banning , 2008 ; Gunther , 2006 ) . In particular , researchers have studied how socially undesirable media messages , such as political advertisements against liked candidates , result in support for regulation ( Cohen and Davis , 1991 ) . Analyzing data from past research on TPE consequences , Chung and Moon ( 2016 ) found that the media’s presumed effect on others ( PME3 ) is a stronger predictor of censorship attitudes than the other - self disparity in perceived media effects . In line with this , Riedl et al . found that the perceived effects of social media content on others are a significant predictor of support for content moderation ( Riedl et al . , 2021 ) . Accordingly , I expect that in encountering inappropriate online communities , the perceived effect on others will likely result in support for community - wide bans . Therefore , I raise the third hypothesis regarding the TPE behavioral component : H3 : For each type of norm - violating community , its perceived effects on others will be positively related to support for the platform’s banning of that community . While support for censorship as a behavioral component of TPE continues to be the mainstay of third - person effects researchers , a growing body of studies has also identified corrective actions as another behavioral outcome of the TPE ( Lim and Golan , 2011 ; Sun et al . , 2008 ) . In contrast to restrictive actions , such as censorship or government regulation , corrective actions “refer to individuals’ engagement in reactive action against potentially harmful influence” ( Lim , 2017 ) . In the context of online content moderation , researchers have proposed calls for media literacy interventions as one of the most prominent corrective efforts ( Lazer et al . , 2017 ; Jang and Kim , 2018 ) . Jang and Kim ( 2018 ) found that people with a greater third - person perception were more likely to support the media literacy approaches to address fake news . In line with this , perceived harms of an offensive community on others are likely to result in support for improving media literacy about that community . Putting warning labels on inappropriate communities is a potent media literacy intervention since it aims to educate users on critically evaluating the community’s content . Therefore , I examine the following hypothesis : H4 : For each type of norm - violating community , its perceived effects on others will be positively related to support for the platform’s adding a warning label on that community . Support for Freedom of Speech Freedom of speech is inextricably linked to any discussion on community - wide moderation interventions . In most democracies , freedom of speech is recognized as a core value and is constitutionally protected as a human right ( Parekh , 2012 ) . However , even in the United States , where free speech is most strongly protected , scholars have found that Americans’ tolerance for hate speech is low despite their high overall support for freedom of expression ( Sullivan et al . , 1993 ) . This insight suggests that individuals’ appreciation of the expression rights in the abstract may give only an incomplete picture of their tolerance for opposing expressions ( Naab , 2012 ) . To inquire into this incongruence in the context of community - wide moderation interventions , it is crucial to examine how individuals’ support for freedom of speech impacts their censorship attitudes . Prior research has explored connections between attitudes toward free speech and attitudes about content moderation , but findings are largely mixed . For example , Riedl et al . ( 2021 ) found that support for free speech did not significantly impact attitudes about moderation . However , a related measure , opposition to censorship , significantly negatively affected support for social media content review . Guo and Johnson ( 2020 ) showed that a lack of support for freedom of expression predicts support for Facebook’s content moderation of anti - LGBT speech but not for the moderation of racist or sexist speech . Jang and Kim ( 2018 ) argued that support for freedom of speech reduces support for regulating fake news despite the existence of TPE . Since it is unclear how support for free speech might impact participants’ support for community - wide moderation , I ask the following research question : RQ1 : For each type of norm - violating community , how does support for freedom of expression relate to support for the platform’s banning of that community ? In contrast to community bans , adding a warning label to an online community does not remove its content and lets users decide whether to engage . I expect this intervention to be perceived as free speech preserving , especially because it contrasts with community bans that let platforms unilaterally remove all prior community posts and stall any further activity . Prior research showed how free speech supporters prefer an approach to content curation that emphasizes individual choice instead of top - down content removal by social media platforms ( Jhaver and Zhang , 2023 ) . Therefore , I raise the following hypothesis : H5 : For each type of norm - violating community , participants’ support for freedom of expression will be positively related to support for the platform’s adding a warning label on that community . Online Harms The study I conduct here focuses on the harms caused by viewing inappropriate content on social media platforms ( Jhaver et al . , 2022 ) . Specifically , it explores three types of content - based harm examined in prior literature ( Jhaver and Zhang , 2023 ) : hate speech , violent content , and sexually explicit content . However , what is considered harmful can vary based on culture and personal beliefs ( Jhaver et al . , 2018 ) . Previous studies have found that repeated viewing of disturbing posts can negatively affect mental health and result in panic attacks and secondary trauma ( Roberts , 2019 ) . In addition , online platforms often develop ad hoc content moderation approaches to deal with such harmful content ( Pater et al . , 2016 ) . Community - wide bans and warning labels are examples of how platforms attempt to address harm . Given the especially broad - brush impact of such actions , it is vital to examine bystanders’ perceptions of them as a measure to address content - based harm . Therefore , this inquiry forms the focus of my research . Methods My University’s IRB i examined this study and determined it to be exempt from a full review on Dec 30 , 2022 . I recruited participants through Lucid , ii a survey company that provides researchers access to nationally representative samples . My inclusion criterion for the survey participants was all adult internet users in the US . I paid participants through the Lucid system . On average , the participants took 4 minutes to complete the survey . Each participant received compensation of $ 1 . 5 for their participation . This study was preregistered at OSF . iii I designed survey questions to test the hypotheses proposed in the previous section and adapted survey instruments from relevant prior literature to test specific measures ; I describe these measures in more detail below . To increase the survey’s validity , I sought feedback on an early draft of the questionnaire from students and colleagues at my institution . Nine individuals who were not involved with the project responded to my request and provided feedback on the wording of the questions and survey flow , which I incorporated into the final survey design . I also piloted the survey with a sample of 30 Lucid participants , which resulted in another round of iteration before the questionnaire reached the desired quality . The survey questionnaire consisted of three blocks containing similar questions about communities featuring hate speech , sexually explicit content , and violent content . To counter the effects of the order of presentation on survey results , I counterbalanced the order in which the three blocks were shown to participants . At the beginning of each block , I specified the norm - violating category that the following questions related to and defined that category . I used the following definitions : • Hate speech : “Hate speech includes speech that is dehumanizing , stereotyping , or insulting based on identity markers such as race / ethnicity , gender , sexual orientation , religion , etc . ” • Violent content : “Violent content includes threats to commit violence , glorifying violence or celebrating suffering , depictions of violence that are gratuitous or gory , and animal abuse . ” • Sexually explicit content : “Sexually explicit content includes content showing sexual activity , offering or requesting sexual activity , female nipples ( except breastfeeding , health , and acts of protest ) , nudity showing genitals , and sexually explicit language . ” These definitions were inspired by the language provided by the Facebook site when reporting any post under the category of hate speech , violence , and nudity , respectively . I administered the survey online using the survey software package Qualtrics . I launched the survey on May 13 , 2023 . Table 1 presents the demographic details of my final sample of 1 , 023 participants after data cleaning and compares it to the demographics of the adult US internet population . Table 1 : Demographic details of survey participants This study , US survey May 2023 ( % ) American Community Survey , US sample 2021 ( % ) Age 18 - 29 21 . 2 17 . 4 30 - 49 38 . 0 29 . 5 50 - 64 23 . 3 25 . 6 65 + 17 . 5 27 . 3 Gender Male 48 . 0 48 . 6 Female 52 . 0 51 . 4 Race / Ethnicity White 73 . 4 68 . 3 Black 12 . 9 9 . 3 Other 13 . 7 22 . 4 Hispanic Yes 12 . 9 13 . 7 Education High school or less 32 . 8 33 . 5 Some college 25 . 7 33 . 3 College + 43 . 9 33 . 1 Measures Perceived Effects of an Inappropriate Community on the Self and Others For each norm - violating speech category – hate speech , violent content , and sexually explicit content – I asked survey participants to estimate the influence of engaging with communities containing it on the self and others . I adapted questions for each category based on survey instruments measuring third - person effects for that category in prior literature . For hate speech , participants rated the following two statements ( Guo and Johnson , 2020 ) : 1 . Engaging with online communities that frequently contain hate speech would negatively influence my attitudes toward the targeted groups . 2 . Engaging with online communities that frequently contain hate speech would negatively influence my attitudes toward anti - discrimination policies . For violent content , participants rated the following statements ( Hoffner et al . , 2006 ) : 1 . Engaging with online communities that frequently contain violent posts would lead me to view the world as a more dangerous place . 2 . Engaging with online communities that frequently contain violent posts would lead me to think that aggression is acceptable . For sexually explicit content , participants rated the following two statements ( Lee and Tamborini , 2006 ; Zhou and Zhang , 2023 ) : 1 . Engaging with online communities that frequently contain sexually explicit posts would negatively influence my moral values about sex . 2 . Engaging with online communities that frequently contain sexually explicit posts would negatively influence my attitudes toward the opposite sex . For each question , the response levels ranged on a 7 - point Likert - type scale from 1 ( strongly disagree ) to 7 ( strongly agree ) . The two corresponding items for each speech category were averaged to create a measure of the perceived influence of communities featuring that category on the self ( Hate speech : M = 4 . 26 , SD = 1 . 89 , a = . 85 ; Violent speech : M = 4 . 45 , SD = 1 . 61 , a = . 71 ; Sexually explicit speech : M = 3 . 89 , SD = 1 . 83 , a = . 85 ) . For each category , I asked another two questions replacing the words “me” with “other people” and “my” with “other people’s . ” I averaged the responses to these questions to create an index of the perceived influence of communities featuring that category on others ( Hate speech : M = 5 . 00 , SD = 1 . 49 , a = . 90 ; Violent speech : M = 5 . 09 , SD = 1 . 39 , a = . 75 ; Sexually explicit speech : M = 4 . 52 , SD = 1 . 58 , a = . 90 ) . Figure 1 : Survey question asking participants to rate their support for platforms banning communities that frequently contain hate speech . Support for Freedom of Speech This variable was assessed based on free speech measures examined in previous work ( Guo and Johnson , 2020 ) . It included the following items : ( a ) In general , I support the First Amendment , ( b ) Freedom of expression is essential to democracy , ( c ) Democracy works best when citizens communicate in an unregulated marketplace of ideas , and ( d ) Even extreme viewpoints deserve to be voiced in society . I included the First Amendment statement iv in the first question to clarify its meaning . Participants rated these four items on a Likert - type scale ranging from 1 ( strongly disagree ) to 7 ( strongly agree ) . These four items were averaged to create an index for “support for freedom of speech” ( M = 5 . 39 , SD = 1 . 13 , α = . 78 ) . Dependent Variables Related to Community - wide Moderation Support for the platform - enacted ban of communities featuring each norm - violating speech category was assessed by asking participants to rate the following statement : “I support social media platforms banning any online community that frequently contains < speech category > . ” The responses for this statement ranged on a 7 - point Likert - type scale , where 1 = “strongly disagree” and 7 = “strongly agree” ( see Figure 1 ) . Figure 2 : Survey question asking participants to rate their support for platforms adding a warning label before a community featuring hate speech . To assess support for adding a warning label against communities for each category , I showed participants an example of a warning message preceding a community that lets users decide whether to continue accessing that community ( see Figure 2 ) . I asked participants to rate their support for inserting this community label on a Likert - type scale ranging from 1 ( strongly disagree ) to 7 ( strongly agree ) . In addition to these two measures , I also assessed choosing community ban vs a community warning label vs neither for each speech category . I asked respondents a binary question : “Given a choice between a ban and a warning label to handle a community that frequently features < speech category > , which would you prefer to have ? ” The response categories included : ( 1 ) Community ban : Platforms should ban this community so that no one can participate in it , ( 2 ) Community warning label : Platforms should place a warning label before this community but let users who want to participate access it , and ( 3 ) Neither : Platforms should neither ban this community nor put a warning label on it . Control Variables Previous studies have shown that socio - demographic variables relate to third - person effects , free speech values , and attitudes toward media regulation ( Lambe , 2002 ; Gunther , 2006 ; Lee , 2009 ; Lo and Chang , 2006 ) . In addition , social media use also relates to individuals’ perceptions of harmful content and how to address it ( Naab et al . , 2018 ; Kalch and Naab , 2017 ; Kenski et al . , 2020 ; Ziegele et al . , 2020 ) . Therefore , I controlled for age , education , gender , race , political affiliation ( 1 = “very liberal” , 7 = “very conservative” ) , and social media use of each respondent . To assess the frequency of social media use , I followed Ernala et al . ( 2020 ) recommendations and asked participants to respond to the question , “In the past week , on average , approximately how much time PER DAY have you spent actively using any social media sites like Facebook and Reddit ? ” Figure 3 : Frequency of participants’ responses to survey questions about support for bans and putting warning labels before communities featuring hate speech , violent content , and sexually explicit content , measured in percentage . Results My analysis shows that 70 . 8 % , 69 . 8 % , and 56 . 6 % of participants at least somewhat agreed that platforms should ban communities frequently featuring hate speech , violent content , and sexually explicit content , respectively . Further , 75 . 2 % , 77 . 3 % , and 79 . 7 % of participants at least somewhat agreed that platforms should offer warning labels before communities frequently featuring hate speech , violent content , and sexually explicit content , respectively ( see Figure 3 ) . When making a choice between bans , warning labels , or neither to regulate online communities , my analysis shows different trends for different content types ( see Figure 4 ) . More participants preferred bans ( 45 . 1 % ) over warning labels ( 44 . 1 % ) for handling communities frequently containing hate speech . In contrast , more participants preferred warning labels over bans for regulating communities often featuring violent content ( 51 . 1 % v / s 39 . 8 % , respectively ) and sexually explicit content ( 61 . 8 % v / s 28 . 8 % , respectively ) . Thus , H1 , which predicted that for each type of norm - violating community , the average support for adding warning labels would exceed the average support for banning it , was only partially supported . Figure 4 : Participants ' responses to a choice between bans , warning labels , or neither to regulate online communities featuring inappropriate content . In line with research on the third - person effects , H2 predicted that for each norm - violating speech category , participants would perceive the effects of communities frequently featuring that content to be stronger on others than on themselves . I ran paired t - tests and found the perceived effects on others to be significantly stronger than on oneself for each category ( see Table 2 ) . Thus , my results support H2 . Table 2 : Mean , standard deviations , and standard errors of participants’ perceived effects of communities featuring hate speech , violent content , and sexually explicit content on others and self , and t - test results comparing perceived effects on others and self for each speech category ( N = 1 , 023 ) . * * * denotes p < . 001 M SD SE t Cohen’s d Hate speech Effects on others 5 . 00 1 . 49 . 05 15 . 03 * * * . 470 Effects on self 4 . 26 1 . 89 . 06 Violent content Effects on others 5 . 09 1 . 39 . 04 15 . 78 * * * . 493 Effects on self 4 . 45 1 . 61 . 05 Sexually explicit content Effects on others 4 . 52 1 . 58 . 05 14 . 66 * * * . 458 Effects on self 3 . 89 1 . 83 . 06 Support for Community Bans I computed hierarchical linear regression to test my hypothesis H3 and answer RQ1 . For each norm - violating speech category , I created a model in which the dependent variable was participant’s support for platforms banning communities featuring that category . In Step 1 , I included the control variables age , gender , education , race , political affiliation , and social media use . In Step 2 , I introduced the independent variables PME3 ( perceived effects on others ) for that category and support for free speech ( Table 3 ) . Table 3 : Hierarchical multiple regression analyses predicting support for platforms ' banning of communities featuring hate speech , violent content , and sexually explicit content ( N = 1 , 018 ) . Independent Variable Support for platform ban of hate speech ( β ) Support for platform ban of violent content ( β ) Support for platform ban of sexually explicit content ( β ) Model # Model 1 Model 2 Model 3 Step 1 Age . 078 * * . 069 * . 122 * * * Gender ( Female ) . 076 * * * . 138 * * * . 139 * * * Race ( White ) . 030 . 041 . 000 Education a . 013 . 028 - . 20 Political affiliation b - . 148 * * * - . 118 * * * . 010 Social media use c . 047 . 044 . 052 R 2 . 062 * * * . 079 * * * . 068 * * * Step 2 Support for free speech - . 054 - . 049 - . 025 Perceived effects of hate speech on others . 404 * * * - - Perceived effects of violent content on others - . 446 * * * - Perceived effects of sexually explicit content on others - - . 473 * * * R 2 change . 156 * * * . 187 * * * . 215 * * * Total R 2 . 218 . 266 . 283 * p < . 05 , * * p < . 01 , * * * p < . 001 ( t test for β , two - tailed ; F test for R 2 , two - tailed ) . a 0 = Less than secondary education ; 1 = Secondary education or more . b 1 = Strong Democrat , 7 = Strong Republican . c 1 = Less than 10 minutes per day , 6 = More than 3 hours per day . β = Standardized beta from the full model ( final beta controlling for all variables in the model ) . For each norm - violating speech category , the regression models show significant influences of the participants’ perceived effects of communities featuring that category on others ( PME3 ) on their support for a platform ban of such communities ( Model 1 : hate speech – β = . 404 , p < . 001 ; Model 2 : violent content – β = . 446 , p < . 001 ; Model 3 : sexually explicit content – β = . 473 , p < . 001 ) , supporting H3 . On the other hand , greater support for free speech does not influence support for platform bans of communities in any category : hate speech ( β = - . 054 , p > . 05 ) , violent content ( β = - . 049 , p > . 05 ) or sexually explicit content ( β = - . 025 , p > . 05 ) . This answers RQ1 . Support for Community Warning Labels I computed hierarchical linear regression to test hypotheses H4 and H5 . For each norm - violating speech category , I created a model where the dependent variable was participant’s support for putting warning labels before communities frequently containing that content category . Similar to models for community bans , in Step 1 of the three regression models , I included the control variables age , gender , education , race , political affiliation , and social media use . In Step 2 , I introduced the independent variables PME3 ( perceived effects on others ) for that category and support for free speech ( Table 4 ) . Table 4 : Hierarchical multiple regression analyses predicting support for platforms putting warning labels before communities frequently featuring hate speech , violent content , and sexually explicit content ( N = 1 , 018 ) Independent Variable Support for warning labels before hate speech ( β ) Support for warning labels before violent content ( β ) Support for warning labels before sexually explicit content ( β ) Model # Model 4 Model 5 Model 6 Step 1 Age . 052 . 007 . 045 Gender ( Female ) . 072 * . 036 . 072 * Race ( White ) . 016 . 020 - . 026 Education a - . 024 . 038 - . 038 Political affiliation b - . 025 - . 010 - . 093 * * Social media use c . 045 . 083 * * . 075 * R 2 . 020 * * . 018 * * . 023 * * * Step 2 Support for free speech . 212 * * * . 242 * * * . 237 * * * Perceived effects of hate speech on others . 242 * * * - - Perceived effects of violent content on others - . 220 * * * - Perceived effects of sexually explicit content on others - - . 220 * * * R 2 change . 112 * * * . 119 * * * . 108 * * * Total R 2 . 132 . 137 . 131 * * p < . 01 , * * * p < . 001 ( t test for β , two - tailed ; F test for R 2 , two - tailed ) . a 0 = Less than secondary education ; 1 = Secondary education or more . b 1 = Strong Democrat , 7 = Strong Republican . c 1 = Less than 10 minutes per day , 6 = More than 3 hours per day . β = Standardized beta from the full model ( final beta controlling for all variables in the model ) . For each norm - violating speech category , the regression models show significant influences of the participants’ perceived effects of communities featuring that category on others ( PME3 ) on their support for using warning labels before those communities ( Model 4 : hate speech – β = . 242 , p < . 001 ; Model 5 : violent content – β = . 220 , p < . 001 ; Model 6 : sexually explicit content – β = . 220 , p < . 001 ) , supporting H4 . Greater support for free speech has a significant positive influence on participants’ support for using warning labels before communities featuring each norm - violating category ( Model 4 : hate speech – β = . 212 , p < . 001 ; Model 5 : violent content – β = . 242 , p < . 001 ; Model 6 : sexually explicit content – β = . 237 , p < . 001 ) , supporting H5 . Discussion It is not immediately apparent how people would perceive community - wide sanctions . On the one hand , they do not specifically target an individual , and this absence of calling someone out as a violator may reduce personal stake in the moderation decision and its effects . On the other hand , sanctions that simultaneously impact many users can be perceived as overly general , affecting those whose contributions to said community did not violate platform rules . This study shows that a majority of US adults at least somewhat agree to support both moderation interventions ( i . e . , bans and warning labels ) for communities featuring each norm - violating content category I tested . Given a choice between a ban , a warning label , or neither intervention , about only 10 % of participants opted to select neither intervention in each case . This widespread acceptance of community - wide moderation interventions should empower platforms to take such actions when community content shows clear , persistent patterns of norm violations . The difference in selecting bans versus warning labels varies across the three content categories , with more participants preferring to ban communities frequently featuring hate speech rather than putting warning labels on them . However , participants preferred warning labels over bans for communities with violent and sexually explicit content . This finding adds more nuance to prior literature , which observed a preference for warning labels over bans for handling misinformation ( Atreja et al . , 2022 ; Wihbey et al . , 2022 ) . It shows that users prefer moderation approaches of varying severity for different categories of norm - violating content . Therefore , platforms should characterize community - wide norm violations of speech content more granularly and deploy sanctions proportionate to the offense . Study findings support the perceptual component of the TPE hypothesis , revealing that participants believed communities featuring hate speech , violent content , and sexually explicit material had a greater influence on others than on themselves . Additionally , for each content category , participants’ perception of the effects on others ( PME3 ) was a significant predictor of their support for both community bans and warning labels . This finding contributes to the advancement of TPE research by highlighting the crucial role of perceived effects on others in triggering censorial behavior . It also suggests that when users perceive online communities as having detrimental effects on the broader user population , they desire social media platforms to take comprehensive actions against those communities . Further , it shows that an overestimation of the impact of norm - violating content on other users partly drives support for community - wide sanctions . Therefore , when making decisions about moderation policies , it is crucial for companies to acknowledge and consider the influence of third - person effects . I found no evidence of any relationship between free speech and community - wide ban support for each of the three norm - violating categories . This result is consistent with the mixed findings for the relationship between free speech support and supportive attitudes toward platform censorship observed in prior literature ( Guo and Johnson , 2020 ; Jhaver and Zhang , 2023 ) . On the other hand , I found that support for free speech predicted support for warning labels before communities for each inappropriate speech category . This suggests that people may perceive warning labels not as a violation of others’ freedom of speech but rather as a means to empower themselves and others in shaping the content they encounter . It also emphasizes that free speech supporters would prefer a fresh approach to content curation that prioritizes individual choice instead of endorsing centralized censorship by platforms . Free speech scholars can expand on this work by examining how individuals’ support for freedom of expression influences their perceptions of their social media accounts being labeled under a norm - violating category . Additional effects that were not the primary focus of the hypotheses being tested were also found . Age was positively related to support for community - wide bans for each speech category . Females showed greater support for community - wide bans in all speech categories compared to males . Democrats were more likely than Republicans to support community - wide bans of hate speech and violent content , but not sexually explicit content ; however , Democrats were more likely than Republicans to support warning labels before communities featuring sexually explicit content . The frequency of social media use was positively related to support for warning labels before communities featuring violent and sexually explicit content , but not hate speech . To gain a deeper understanding of these findings , future research could investigate how users’ political ideology , media consumption habits , parental concerns , and previous experiences on social media influence their preferences for content moderation . Limitations and Future Work It is important to acknowledge several limitations of this study . First , due to the survey design , I was unable to deeply explore the underlying motivations behind specific perceptions of community - wide moderation . Second , I cannot draw definitive conclusions about causal relationships in this cross - sectional study . My investigation serves as an initial exploration of third - person effects related to support for community - wide moderation . Future research can expand on this work by examining third - person effects in a broader range of community settings , e . g . , measuring the effects of community size or using a wider variety of inappropriate speech categories . I presented participants with questions about speech categories that allowed for broad interpretation since I aimed to enhance the applicability of my findings . However , users may have varying interpretations of what constitutes hate speech , violent content , or sexually explicit content . Previous research on moderation recognized this complexity as a significant challenge in social media regulation ( Jhaver et al . , 2018 ) . To clarify the scope of each category , I provided definitions in the survey . Nonetheless , further research could provide valuable insights by utilizing stimulus - based designs that present specific instances of communities containing examples of norm - violating posts to participants . Finally , my survey questions were not grounded in a specific platform to ensure broader applicability . Future studies that focus on particular social media sites could explore whether users’ attitudes toward specific platforms influence their perceptions of community - wide moderation actions . i I will specify the Institute name after the peer review process is completed . ii https : / / lucidtheorem . com iii https : / / osf . io / pnv29 / ? view _ only = 083d7f5dad1a46a1a32321bf2313a7e4 iv The First Amendment to the United States Constitution states : “Congress shall make no law respecting an establishment of religion , or prohibiting the free exercise thereof ; or abridging the freedom of speech , or of the press ; or the right of the people peaceably to assemble , and to petition the Government for a redress of grievances . ” References Atreja S , Hemphill L and Resnick P ( 2022 ) What is the Will of the People ? Moderation Preferences for Misinformation . arXiv preprint arXiv : 2202 . 00799 . Bandura A ( 2005 ) The evolution of social cognitive theory . Great minds in management . 9 - 35 . Carlson CR , Cousineau L and Carlson CR ( 2020 ) Are You Sure You Want to View This Community ? Exploring the Ethics of Reddit ' s Quarantine Practice . Journal of Media Ethics 00 ( 00 ) : 1 – 12 - 11 – 12 . Chandrasekharan E , Jhaver S , Bruckman A , et al . ( 2022 ) Quarantined ! Examining the Effects of a Community - Wide Moderation Intervention on Reddit . ACM Trans . Comput . - Hum . Interact . Chandrasekharan E , Pavalanathan U , Srinivasan A , et al . ( 2017 ) You Can ' t stay here : The efficacy of Reddit ' s 2015 ban examined through hate speech . Proc . ACM Hum . - Comput . Interact . 1 ( CSCW ) : 31 : 31 – 31 : 22 - 31 : 31 – 31 : 22 . Chung M , Munno GJ and Moritz B ( 2015 ) Triggering participation : Exploring the effects of third - person and hostile media perceptions on online participation . Computers in Human Behavior 53 : 452 - 461 . Chung S and Moon S - I ( 2016 ) Is the Third - Person Effect Real ? a Critical Examination of Rationales , Testing Methods , and Previous Findings of the Third - Person Effect on Censorship Attitudes . Human Communication Research 42 ( 2 ) : 312 - 337 . Cohen J and Davis RG ( 1991 ) Third - Person Effects and the Differential Impact in Negative Political Advertising . Journalism Quarterly 68 ( 4 ) : 680 - 688 . Costolo D ( 2013 ) The “Town Square” in the Social Media Era : A Conversation with Twitter CEO Dick Costolo . Available at : https : / / www . brookings . edu / events / the - town - square - in - the - social - media - era - a - conversation - with - twitter - ceo - dick - costolo / ( accessed 2022 - 07 - 22 ) . Davison WP ( 1983 ) The Third - Person Effect in Communication . Public Opinion Quarterly 47 ( 1 ) : 1 - 15 . Ernala SK , Burke M , Leavitt A , et al . ( 2020 ) How Well Do People Report Time Spent on Facebook ? An Evaluation of Established Survey Questions with Recommendations . In : Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , Honolulu , HI , USA , pp . 1 – 14 . Association for Computing Machinery . Glaser A ( 2018 ) Want a Terrible Job ? Facebook and Google May Be Hiring . Golan GJ and Banning SA ( 2008 ) Exploring a Link Between the Third - Person Effect and the Theory of Reasoned Action : Beneficial Ads and Social Expectations . American Behavioral Scientist 52 ( 2 ) : 208 - 224 . Grimmelmann J ( 2009 ) Virtual world feudalism . Yale Law Journal Pocket Part 118 : 126 . Gunther A ( 1991 ) What We Think Others Think : Cause and Consequence in the Third - Person Effect . Communication Research 18 ( 3 ) : 355 - 372 . Gunther AC ( 2006 ) Overrating the X - Rating : The Third - Person Perception and Support for Censorship of Pornography . Journal of Communication 45 ( 1 ) : 27 - 38 . Gunther AC and Mundy P ( 1993 ) Biased Optimism and the Third - Person Effect . Journalism Quarterly 70 ( 1 ) : 58 - 67 . Guo L and Johnson BG ( 2020 ) Third - Person Effect and Hate Speech Censorship on Facebook . Social Media + Society 6 ( 2 ) : 2056305120923003 . Hoffner C , Plotkin RS , Buchanan M , et al . ( 2006 ) The Third - Person Effect in Perceptions of the Influence of Television Violence . Journal of Communication 51 ( 2 ) : 283 - 299 . Horta Ribeiro M , Jhaver S , Zannettou S , et al . ( 2021 ) Do Platform Migrations Compromise Content Moderation ? Evidence from r / The \ _ { D } { o } { n } ald and r / Incels . Proc . ACM Hum . - Comput . Interact . 5 ( CSCW2 ) . Jang SM and Kim JK ( 2018 ) Third person effects of fake news : Fake news regulation and media literacy interventions . Computers in Human Behavior 80 : 295 - 302 . Jhaver S , Appling DS , Gilbert E , et al . ( 2019a ) “Did You Suspect the Post Would Be Removed ? ” : Understanding User Reactions to Content Removals on Reddit . Proc . ACM Hum . - Comput . Interact . 3 ( CSCW ) . Jhaver S , Birman I , Gilbert E , et al . ( 2019b ) Human - Machine Collaboration for Content Regulation : The Case of Reddit Automoderator . ACM Trans . Comput . - Hum . Interact . 26 ( 5 ) . Jhaver S , Boylston C , Yang D , et al . ( 2021 ) Evaluating the Effectiveness of Deplatforming as a Moderation Strategy on Twitter . Proc . ACM Hum . - Comput . Interact . 5 ( CSCW2 ) . Jhaver S , Chan L and Bruckman A ( 2018 ) The View from the Other Side : The Border Between Controversial Speech and Harassment on Kotaku in Action . First Monday 23 ( 2 ) . Jhaver S , Chen QZ , Knauss D , et al . ( 2022 ) Designing Word Filter Tools for Creator - Led Comment Moderation . Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . Association for Computing Machinery . Jhaver S and Zhang A ( 2023 ) Do Users Want Platform Moderation or Individual Control ? Examining the Role of Third - Person Effects and Free Speech Support in Shaping Moderation Preferences . Under Review at New Media & Society . Kalch A and Naab TK ( 2017 ) Replying , disliking , flagging : How users engage with uncivil and impolite comments on news sites . Kenski K , Coe K and Rains SA ( 2020 ) Perceptions of Uncivil Discourse Online : An Examination of Types and Predictors . Communication Research 47 ( 6 ) : 795 - 814 . Klonick K ( 2017 ) The new governors : The people , rules , and processes governing online speech . Harv . L . Rev . 131 : 1598 - 1598 . Lambe JL ( 2002 ) Dimensions of censorship : Reconceptualizing public willingness to censor . Communication Law and Policy 7 : 187 - 235 . Lazer D , Baum M , Grinberg N , et al . ( 2017 ) Combating fake news : An agenda for research and action . Lee B and Tamborini R ( 2006 ) Third - Person Effect and Internet Pornography : The Influence of Collectivism and Internet Self - Efficacy . Journal of Communication 55 ( 2 ) : 292 - 310 . Lee FLF ( 2009 ) The Prevention Effect of Third - Person Perception : A Study on the Perceived and Actual Influence of Polls . Mass Communication and Society 13 ( 1 ) : 87 - 110 . Lev - On A ( 2017 ) The third - person effect on Facebook : The significance of perceived proficiency . Telematics and Informatics 34 ( 4 ) : 252 - 260 . Li X ( 2008 ) Third - Person Effect , Optimistic Bias , and Sufficiency Resource in Internet Use . Journal of Communication 58 ( 3 ) : 568 - 587 . Lim JS ( 2017 ) The Third - Person Effect of Online Advertising of Cosmetic Surgery : A Path Model for Predicting Restrictive Versus Corrective Actions . Journalism & Mass Communication Quarterly 94 ( 4 ) : 972 - 993 . Lim JS and Golan GJ ( 2011 ) Social Media Activism in Response to the Influence of Political Parody Videos on YouTube . Communication Research 38 ( 5 ) : 710 - 727 . Lo V - h and Chang C ( 2006 ) Knowledge about the Gulf Wars : A Theoretical Model of Learning from the News . Harvard International Journal of Press / Politics 11 ( 3 ) : 135 - 155 . Madrigal A ( 2018 ) Inside Facebook ' s Fast - Growing Content - Moderation Effort . Malinen S The owners of information : Content curation practices of middle - level gatekeepers in political Facebook groups . New Media & Society 0 ( 0 ) : 14614448211062123 . Matias JN ( 2019 ) The Civic Labor of Volunteer Moderators Online . Social Media + Society 5 ( 2 ) : 2056305119836778 . McLeod DM , Eveland WP and Nathanson AI ( 1997 ) Support for Censorship of Violent and Misogynic Rap Lyrics : An Analysis of the Third - Person Effect . Communication Research 24 ( 2 ) : 153 - 174 . Musil S ( 2021 ) Facebook will warn you when you ' re about to join a group that broke its rules . Available at : https : / / www . cnet . com / tech / services - and - software / facebook - will - warn - you - when - youre - about - to - join - a - group - that - broke - its - rules / . Naab T ( 2012 ) The relevance of people ' s attitudes towards freedom of expression in a changing media environment . ESSACHESS Journal for Communication Studies 5 ( 1 ) . Naab TK , Kalch A and Meitz TG ( 2018 ) Flagging uncivil user comments : Effects of intervention information , type of victim , and response comments on bystander behavior . New Media & Society 20 ( 2 ) : 777 - 795 . Newell E , Jurgens D , Saleem HM , et al . User Migration in Online Social Networks : A Case Study on Reddit During a Period of Community Unrest . Tenth International AAAI Conference on Web and Social Media . ICWSM ed . , 279 – 288 - 279 – 288 . Paradise A and Sullivan M ( 2012 ) ( In ) visible threats ? The third - person effect in perceptions of the influence of Facebook . Cyberpsychology , Behavior , and Social Networking 15 ( 1 ) : 55 - 60 . Parekh B ( 2012 ) Is there a case for banning hate speech ? The content and context of hate speech : Rethinking regulation and responses 40 : 22 - 23 . Pater JA , Kim MK , Mynatt ED , et al . ( 2016 ) Characterizations of Online Harassment : Comparing Policies Across Social Media Platforms . Proceedings of the 19th International Conference on Supporting Group Work . ACM , 369 – 374 - 369 – 374 . Peiser W and Peter J ( 2000 ) Third - person perception of television - viewing behavior . Journal of Communication 50 ( 1 ) : 25 - 45 . Perez S ( 2020 ) Facebook tries to clean up Groups with new policies . Available at : https : / / techcrunch . com / 2020 / 09 / 17 / facebook - tries - to - clean - up - groups - with - new - policies / ( accessed 07 / 22 / 2022 ) . Rauchfleisch A and Kaiser J ( 2021 ) Deplatforming the far - right : An analysis of YouTube and BitChute . SSRN . Riedl MJ , Whipple KN and Wallace R ( 2021 ) Antecedents of support for social media content moderation and platform regulation : the role of presumed effects on self and others . Information , Communication & Society . DOI : 10 . 1080 / 1369118X . 2021 . 1874040 . 1 - 18 . Roberts ST ( 2019 ) Behind the screen : Content Moderation in the Shadows of Social Media . Yale University Press . Romm T and Dwoskin E ( 2019 ) Twitter adds labels for tweets that break its rules - a move with potentially stark implications for Trump ' s account . WP Company . Saleem HM and Ruths D ( 2018 ) The aftermath of disbanding an online hateful community . arXiv preprint arXiv : 1804 . 07354 . Scharrer E ( 2002 ) Third - Person Perception and Television Violence : The Role of Out - Group Stereotyping in Perceptions of Susceptibility to Effects . Communication Research 29 ( 6 ) : 681 - 704 . Seering J , Wang T , Yoon J , et al . ( 2019 ) Moderator engagement and community development in the age of algorithms . New Media & Society . 1461444818821316 - 1461444818821316 . Shen Q and Rose C The Discourse of Online Content Moderation : Investigating Polarized User Responses to Changes in Reddit’s Quarantine Policy . Proceedings of the Third Workshop on Abusive Language Online . 58 – 69 - 58 – 69 . Sullivan JL , Piereson J and Marcus GE ( 1993 ) Political tolerance and American democracy . University of Chicago Press . Sun Y , Pan Z and Shen L ( 2008 ) Understanding the Third - Person Perception : Evidence From a Meta - Analysis . Journal of Communication 58 ( 2 ) : 280 - 300 . Suzor NP ( 2019 ) Lawless : the secret rules that govern our digital lives . Cambridge University Press . Trujillo A and Cresci S ( 2022 ) Make reddit great again : assessing community effects of moderation interventions on r / the _ donald . arXiv preprint arXiv : 2201 . 06455 . Ullmann S and Tomalin M ( 2020 ) Quarantining online hate speech : technical and ethical perspectives . Ethics and information technology 22 ( 1 ) : 69 - 80 . West SM ( 2018 ) Censored , suspended , shadowbanned : User interpretations of content moderation on social media platforms . New Media & Society . Wihbey J , Chung M , Peacey M , et al . ( 2022 ) Divergent Global Views on Social Media , Free Speech , and Platform Regulation : Findings from the United Kingdom , South Korea , Mexico , and the United States . Free Speech , and Platform Regulation : Findings from the United Kingdom , South Korea , Mexico , and the United States ( January 3 , 2022 ) . Zadrozny B ( 2021 ) Facebook to crack down on groups that break its rules . NBC NEws . Zhou S and Zhang Z ( 2023 ) Impact of Internet Pornography on Chinese Teens : The Third - Person Effect and Attitudes Toward Censorship . Youth & Society 55 ( 1 ) : 83 - 102 . Ziegele M , Naab TK and Jost P ( 2020 ) Lonely together ? Identifying the determinants of collective corrective action against uncivil comments . New Media & Society 22 ( 5 ) : 731 - 751 .