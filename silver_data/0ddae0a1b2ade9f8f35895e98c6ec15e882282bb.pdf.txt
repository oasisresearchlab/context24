Maximizing non - monotone submodular functions Uriel Feige ∗ Dept . of Computer Science and Applied Mathematics The Weizmann Institute Rehovot , Israel uriel . feige @ weizmann . ac . il Vahab S . Mirrokni Microsoft Research Redmond , WA mirrokni @ microsoft . com Jan Vondr´ak Dept . of Mathematics Princeton University Princeton , NJ jvondrak @ math . princeton . edu Abstract Submodular maximization generalizes many important problems including Max Cut in directed / undirected graphs and hypergraphs , certain constraint satisfaction problems and maximum facility location problems . Unlike the prob - lem of minimizing submodular functions , the problem of maximizing submodular functions is NP - hard . In this paper , we design the ﬁrst constant - factor approxi - mation algorithms for maximizing nonnegative submodular functions . In particular , we give a deterministic local search 13 - approximation and a randomized 25 - approximation algo - rithm for maximizing nonnegative submodular functions . We also show that a uniformly random set gives a 14 - approximation . For symmetric submodular functions , we show that a random set gives a 12 - approximation , which can be also achieved by deterministic local search . These algorithms work in the value oracle model where the submodular function is accessible through a black box returning f ( S ) for a given set S . We show that in this model , 12 - approximation for symmetric submodular func - tions is the best one can achieve with a subexponential num - ber of queries . For the case where the function is given explicitly ( as a sum of nonnegative submodular functions , each depending only on a constant number of elements ) , we prove that it is NP - hard to achieve a ( 34 + (cid:15) ) - approximation in the general case ( or a ( 56 + (cid:15) ) - approximation in the sym - metric case ) . ∗ This work was done while the author was at Microsoft Research , Red - mond , WA . 1 Introduction We consider the problem of maximizing a nonnegative submodular function . This means , given a submodular function f : 2 X → R + , we want to ﬁnd a subset S ⊆ X maximizing f ( S ) . Deﬁnition 1 . 1 . A function f : 2 X → R is submodular if for any S , T ⊆ X , f ( S ∪ T ) + f ( S ∩ T ) ≤ f ( S ) + f ( T ) . An alternative deﬁnition of submodularity is the property of decreasing marginal values : For any A ⊆ B ⊆ X and x ∈ X \ B , f ( B ∪ { x } ) − f ( B ) ≤ f ( A ∪ { x } ) − f ( A ) . This can be deduced from the ﬁrst deﬁnition by substituting S = A ∪ { x } and T = B ; the reverse implication also holds . We assume a value oracle access to the submodular func - tion ; i . e . , for a given set S , an algorithm can query an oracle to ﬁnd its value f ( S ) . Background . Submodularity , a discrete analog of con - vexity , has played an essential role in combinatorial opti - mization [ 27 ] . It appears in many important settings in - cluding cuts in graphs [ 16 , 33 , 14 ] , rank function of ma - troids [ 8 , 15 ] , set covering problems [ 10 ] , and plant location problems [ 5 , 6 ] . In many settings such as set covering or matroid optimization , the relevant submodular functions are monotone , meaning that f ( S ) ≤ f ( T ) whenever S ⊆ T . Here , we are more interested in the general case where f ( S ) is not necessarily monotone . A canonical example of such a submodular function is f ( S ) = P e ∈ δ ( S ) w ( e ) , where δ ( S ) 1 is a cut in a graph ( or hypergraph ) induced by a set of ver - tices S and w ( e ) is the weight of edge e . Cuts in undi - rected graphs and hypergraphs yield symmetric submodular functions , satisfying f ( S ) = f ( ¯ S ) for all sets S . Symmet - ric submodular functions have been considered widely in the litrature [ 13 , 33 ] . It appears that symmetry allows bet - ter / simpler approximation results , and thus deserves sepa - rate attention . The problem of maximizing a submodular function is of central importance , with special cases including Max Cut [ 16 ] , Max Directed Cut [ 21 ] , hypergraph cut problems , maximum facility location [ 1 , 5 , 6 ] , and certain restricted satisﬁability problems [ 22 , 9 ] . While the Min Cut problem in graphs is a classical polynomial - time solvable problem , and more generally it has been shown that any submodu - lar function can be minimized in polynomial time [ 35 , 14 ] , maximization turns out to be more difﬁcult and indeed all the aforementioned special cases are NP - hard . A related problem is Max - k - Cover , where the goal is to choose k sets whose union is as large as possible . It is known that a greedy algorithm provides a ( 1 − 1 / e ) - approximation for Max - k - Cover and this is optimal unless P = NP [ 10 ] . More generally , this problem can be viewed as maximization of a monotone submodular function un - der a cardinality constraint , i . e . max { f ( S ) : | S | ≤ k } , assuming 0 ≤ f ( S ) ≤ f ( T ) whenever S ⊆ T . Again , the greedy algorithm provides a ( 1 − 1 / e ) - approximation for this problem [ 30 ] . A 12 - approximation has been devel - oped for maximizing monotone submodular functions un - der a matroid constraint [ 31 ] . A ( 1 − 1 / e ) - approximation has been also obtained for a knapsack constraint [ 36 ] , and for a special class of submodular functions under a matroid constraint [ 3 ] . In contrast , here we consider the unconstrained maxi - mization of a submodular function which is not necessarily monotone . We only assume that the function is nonnega - tive . 1 Typical examples of such a problem are Max Cut and Max Directed Cut . Here , the best approximation factors have been achieved using semideﬁnite programming : 0 . 878 for Max Cut [ 16 ] and 0 . 874 for Max Di - Cut [ 9 , 25 ] . The approximation factor for Max Cut has been proved optimal , assuming the Unique Games Conjecture [ 24 , 29 ] . It should be noted that the best known combinatorial algorithms for Max Cut and Max Di - Cut achieve only a 12 - approximation , which is trivial for Max Cut but not for Max Di - Cut [ 21 ] . More generally , submodular maximization encompasses such problems as Max Cut in hypergraphs and Max SAT with no mixed clauses ( every clause contains only positive 1 For submodular functions without any restrictions , verifying whether the maximum of the function is greater than zero or not is NP - hard . Thus , no approximation algorithm can be found for this problem unless P = NP . For a general submodular function f with minimum value f ∗ , we can design an approximation algorithm to maximize a normalized submodular function g where g ( S ) = f ( S ) − f ∗ . or only negative literals ) . Tight results are known for Max Cut in k - uniform hypergraphs for any ﬁxed k ≥ 4 [ 22 , 19 ] where the optimal approximation factor ( 1 − 2 − k + 1 ) is achieved by a random solution ( and the same result holds for Max ( k − 1 ) - SAT with no mixed clauses [ 19 , 20 ] ) . The lowest approximation factor ( 78 ) is achieved for k = 4 ; for k < 4 , better than random solutions can be found by semideﬁnite programming . Submodular maximization also appears in maximizing the difference of a monotone submodular function and a lin - ear function . An illustrative example of this type is the max - imum facility location problem in which we want to open a subset of facilities and maximize the total proﬁt from clients minus the opening cost of facilities . In a series of papers , approximation algorithms have been developed for a vari - ant of this problem which is a special case of maximizing nonnegative submodular functions [ 5 , 6 , 1 ] . The best ap - proximation factor known for this problem is 0 . 828 [ 1 ] . In the general case of non - monotone submodular func - tions , the maximization problem has been studied in the operations research community . Many efforts have been focused on designing heuristics for this problem , includ - ing data - correcting search methods [ 17 , 18 , 23 ] , accelatered greedy algorithms [ 32 ] , and polyhedral algorithms [ 26 ] . Prior to our work , to the best of our knowledge , no guaran - teed approximation factor was known for maximizing non - monotone submodular functions . Our results . We design several constant - factor approx - imation algorithms for maximization of nonnegative sub - modular functions . We also prove negative results , in par - ticular a query complexity result matching our algorithmic result in the symmetric case . Model RS NA DET RND VQ NP Symm . 1 / 2 1 / 2 1 / 2 1 / 2 1 / 2 5 / 6 General 1 / 4 1 / 3 1 / 3 2 / 5 1 / 2 3 / 4 Figure 1 . Summary of results . Notation : RS = random set , NA = nonadaptive , DET = deterministic adaptive , RND = randomized adaptive , VQ = value query hardness , NP = NP - hardness . Non - adaptive algorithms . A non - adaptive algorithm is al - lowed to generate a ( possibly random ) sequence of poly - nomially many sets , query their values and then pro - duce a solution . In this model , we show that a 14 - approximation is achieved in expectation by a uniformly random set . For symmetric submodular functions , this gives a 12 - approximation . This coincides with the approximation factors obtained by random sets for Max Di - Cut and Max Cut . We prove that these factors cannot be improved , as - suming that the algorithm returns one of the queried sets . However , we also design a non - adaptive algorithm which performs a polynomial - time computation on the obtained values and achieves a 13 - approximation . In the symmetric case , we prove that the 12 - approximation is optimal even among adaptive algorithms ( see below ) . Adaptive algorithms . An adaptive algorithm is allowed to perform a polynomial time computation including a poly - nomial number of queries to a value oracle . In this ( most natural ) model , we develop a local search 12 - approximation in the symmetric case and a 13 - approximation in the general case . Then we improve this to a 25 - approximation using a randomized “smooth local search” . This is perhaps the most noteworthy of our algorithms ; it proceeds by locally opti - mizing a smoothed variant of f ( S ) , obtained by biased sam - pling depending on S . The approach of locally optimizing a modiﬁed function has been referred to as “non - oblivious lo - cal search” in the literature ; e . g . , see [ 2 ] for a non - oblivious local search 25 - approximation for the Max Di - Cut problem . Another ( simpler ) 25 - approximation algorithm for Max Di - Cut appears in [ 21 ] . However , these algorithms do not gen - eralize naturally to ours and the re - appearance of the same approximation factor seems coincidental . Hardness results . We show that it is impossible to im - prove the 12 - approximation algorithm for maximizing sym - metric nonnegative submodular functions in the value ora - cle model . We prove that for any ﬁxed (cid:15) > 0 , a ( 12 + (cid:15) ) - approximation algorithm would require exponentially many queries . This settles the status of symmetric submodular maximization in the value oracle model . Note that this query complexity lower bound does not assume any com - putational restrictions . In particular , in the special case of Max Cut , polynomially many value queries sufﬁce to infer all edge weights in the graph , and thereafter an exponen - tial time computation ( involving no further queries ) would actually produce the optimal cut . For explicitly represented submodular functions , known inapproximability results for Max Cut in graphs and hyper - graphs provide an obvious limitation to the best possible ap - proximation ratio . We prove stronger limitations . For any ﬁxed (cid:15) > 0 , it is NP - hard to achieve an approximation fac - tor of ( 34 + (cid:15) ) ( or 56 + (cid:15) ) in the general ( or symmetric ) case , respectively . These results are valid even when the submod - ular function is given as a sum of polynomially many non - negative submodular functions , each depending only on a constant number of elements , which is the case for all the aforementioned problems . 2 Non - adaptive algorithms It is known that simply choosing a random cut is a good choice for Max Cut and Max Di - Cut , achieving an approx - imation factor of 1 / 2 and 1 / 4 respectively . We show the natural role of submodularity here by presenting the same approximation factors in the general case of submodular functions . The Random Set Algorithm : RS . • Return R = X ( 1 / 2 ) , a uniformly random subset of X . Theorem 2 . 1 . Let f : 2 X → R + be a submodular func - tion , OPT = max S ⊆ X f ( S ) and let R denote a uniformly random subset R = X ( 1 / 2 ) . Then E [ f ( R ) ] ≥ 14 OPT . In addition , if f is symmetric ( f ( S ) = f ( X \ S ) for every S ⊆ X ) , then E [ f ( R ) ] ≥ 12 OPT . Before proving this result , we show a useful probabilistic property of submodular functions ( extending the consider - ations of [ 11 , 12 ] ) . This property will be essential in the analysis of our improved randomized algorithm as well . Lemma 2 . 2 . Let g : 2 X → R be submodular . Denote by A ( p ) a random subset of A where each element appears with probability p . Then E [ g ( A ( p ) ) ] ≥ ( 1 − p ) g ( ∅ ) + p g ( A ) . Proof . By induction on the size of A : For A = ∅ , the lemma is trivial . So assume A = A 0 ∪ { x } , x / ∈ A 0 . Then A ( p ) ∩ A 0 is a subset of A 0 where each element appears with probability p ; hence we denote it A 0 ( p ) . By submodularity , g ( A ( p ) ) − g ( A 0 ( p ) ) ≥ g ( A ( p ) ∪ A 0 ) − g ( A 0 ) , and therefore E [ g ( A ( p ) ) ] ≥ E [ g ( A 0 ( p ) ) + g ( A ( p ) ∪ A 0 ) − g ( A 0 ) ] = E [ g ( A 0 ( p ) ) ] + p ( g ( A ) − g ( A 0 ) ) . Applying the inductive hypothesis , E [ g ( A 0 ( p ) ) ] ≥ ( 1 − p ) g ( ∅ ) + p g ( A 0 ) , we get the statement of the lemma . By a double application of Lemma 2 . 2 , we obtain the fol - lowing . Lemma 2 . 3 . Let f : 2 X → R be submodular , A , B ⊆ X two ( not necessarily disjoint ) sets and A ( p ) , B ( q ) their independently sampled subsets , where each element of A appears in A ( p ) with probability p and each element of B appears in B ( q ) with probability q . Then E [ f ( A ( p ) ∪ B ( q ) ) ] ≥ ( 1 − p ) ( 1 − q ) f ( ∅ ) + p ( 1 − q ) f ( A ) + ( 1 − p ) q f ( B ) + pq f ( A ∪ B ) . Proof . Condition on A ( p ) = R and deﬁne g ( T ) = f ( R ∪ T ) . This is a submodular function as well and Lemma 2 . 2 implies E [ g ( B ( q ) ) ] ≥ ( 1 − q ) f ( R ) + q f ( R ∪ B ) . Also , E [ g ( B ( q ) ) ] = E [ f ( A ( p ) ∪ B ( q ) ) | A ( p ) = R ] , and by unconditioning : E [ f ( A ( p ) ∪ B ( q ) ) ] ≥ E [ ( 1 − q ) f ( A ( p ) ) + q f ( A ( p ) ∪ B ) ] . Finally , we apply Lemma 2 . 2 once again : E [ f ( A ( p ) ) ] ≥ ( 1 − p ) f ( ∅ ) + p f ( A ) , and by applying the same to the submodular function h ( S ) = f ( S ∪ B ) , E [ f ( A ( p ) ∪ B ) ] ≥ ( 1 − p ) f ( B ) + p f ( A ∪ B ) . This implies the claim . This lemma gives immediately the performance of Algo - rithm RS . Proof . Denote the optimal set by S and its complement by ¯ S . We can write R = S ( 1 / 2 ) ∪ ¯ S ( 1 / 2 ) . Using Lemma 2 . 3 , we get E [ f ( R ) ] ≥ 1 4 f ( ∅ ) + 1 4 f ( S ) + 1 4 f ( ¯ S ) + 1 4 f ( X ) . Every term is nonnegative and f ( S ) = OPT , so we get E [ f ( R ) ] ≥ 14 OPT . In addition , if f is symmetric , we also have f ( ¯ S ) = OPT and then E [ f ( R ) ] ≥ 12 OPT . As we show in Section 4 . 2 , 14 - approximation is optimal for non - adaptive algorithms that are required to return one of the queried sets . On the other hand , it is possible to de - sign a 1 3 - approximation algorithm which queries a polyno - mial number of sets non - adaptively and then returns a pos - sibly different set after a polynomial - time computation . We omit the details from this extended abstract . What is more surprising , the 12 - approximation for symmetric functions turns out to be optimal even among adaptive algorithms in the value oracle model ( see Section 4 . 2 ) . 3 Adaptive algorithms We turn to adaptive algorithms for the problem of max - imizing a general nonnegative submodular function . We propose several algorithms improving the 14 - approximation achieved by a random set . 3 . 1 Deterministic local search Our deterministic algorithm is based on a simple local search technique . We try to increase the value of our solu - tion S by either including a new element in S or discarding one of the elements of S . We call S a local optimum if no such operation increases the value of S . Local optima have the following property which was ﬁrst observed in [ 4 , 18 ] . Lemma 3 . 1 . Given a submodular function f , if S is a local optimum of f , and I ⊆ S or I ⊇ S , then f ( I ) ≤ f ( S ) . This property turns out to be very useful in comparing a local optimum to the global optimum . However , it is known that ﬁnding a local optimum for the Max Cut problem is PLS - complete [ 34 ] . Therefore , we relax our local search and ﬁnd an approximate local optimal solution . Local Search Algorithm : LS . 1 . Let S : = { v } where f ( { v } ) is the maximum over all singletons v ∈ X . 2 . If there exists an element a ∈ X \ S such that f ( S ∪ { a } ) > ( 1 + (cid:15)n 2 ) f ( S ) , then let S : = S ∪ { a } , and go back to Step 2 . 3 . If there exists an element a ∈ S such that f ( S \ { a } ) > ( 1 + (cid:15)n 2 ) f ( S ) , then let S : = S \ { a } , and go back to Step 2 . 4 . Return the maximum of f ( S ) and f ( X \ S ) . It is easy to see that if the algorithm terminates , the set S is a ( 1 + (cid:15)n 2 ) - approximate local optimum , in the following sense . Deﬁnition 3 . 2 . Given f : 2 X → R , a set S is called a ( 1 + α ) - approximate local optimum , if ( 1 + α ) f ( S ) ≥ f ( S \ { v } ) for any v ∈ S , and ( 1 + α ) f ( S ) ≥ f ( S ∪ { v } ) for any v / ∈ S . We prove the following analogue of Lemma 3 . 1 . Lemma 3 . 3 . If S is an ( 1 + α ) - approximate local optimum for a submodular function f then for any subset I such that I ⊆ S or I ⊇ S , we have f ( I ) ≤ ( 1 + nα ) f ( S ) . Proof . Let I = T 1 ⊆ T 2 ⊆ . . . ⊆ T k = S be a chain of sets where T i \ T i − 1 = { a i } . For each 2 ≤ i ≤ k , we know that f ( T i ) − f ( T i − 1 ) ≥ f ( S ) − f ( S \ { a i } ) ≥ − αf ( S ) using the submodularity and approximate local optimality of S . Summing up these inequalities , we get f ( S ) − f ( I ) ≥ − kαf ( S ) . Thus f ( I ) ≤ ( 1 + kα ) f ( S ) ≤ ( 1 + nα ) f ( S ) . This completes the proof for set I ⊆ S . The proof for I ⊇ S is very similar . Theorem 3 . 4 . Algorithm LS is a (cid:0) 13 − (cid:15)n (cid:1) - approximation algorithm for maximizing nonnegative submodular func - tions , and a (cid:0) 12 − (cid:15)n (cid:1) - approximation algorithm for maxi - mizing nonnegative symmetric submodular functions . The algorithm uses at most O ( 1 (cid:15) n 3 log n ) oracle calls . Proof . Consider an optimal solution C and let α = (cid:15)n 2 . If the algorithm terminates , the set S obtained at the end is a ( 1 + α ) - approximate local optimum . By Lemma 3 . 3 , f ( S ∩ C ) ≤ ( 1 + nα ) f ( S ) and f ( S ∪ C ) ≤ ( 1 + nα ) f ( S ) . Using submodularity , f ( S ∪ C ) + f ( X \ S ) ≥ f ( C \ S ) + f ( X ) ≥ f ( C \ S ) , and f ( S ∩ C ) + f ( C \ S ) ≥ f ( C ) + f ( ∅ ) ≥ f ( C ) . Putting these inequalities together , we get 2 ( 1 + nα ) f ( S ) + f ( X \ S ) ≥ f ( S ∩ C ) + f ( S ∪ C ) + f ( X \ S ) ≥ f ( S ∩ C ) + f ( C \ S ) ≥ f ( C ) . For α = (cid:15)n 2 , this implies that either f ( S ) ≥ ( 13 − (cid:15)n ) OPT or f ( X \ S ) ≥ ( 1 3 − (cid:15) n ) OPT . For symmetric submodular functions , we get 2 ( 1 + nα ) f ( S ) ≥ f ( S ∩ C ) + f ( S ∪ ¯ C ) = f ( S ∩ C ) + f ( ¯ S ∩ C ) ≥ f ( C ) and hence f ( S ) is a ( 12 − (cid:15)n ) - approximation . To bound the running time of the algorithm , let v be a singleton of maximum value f ( { v } ) . It is simple to see that OPT ≤ nf ( { v } ) . After each iteration , the value of the function increases by a factor of at least ( 1 + (cid:15)n 2 ) . Therefore , if we iterate k times , then f ( S ) ≥ ( 1 + (cid:15)n 2 ) k f ( { v } ) and yet f ( S ) ≤ nf ( { v } ) , hence k = O ( 1 (cid:15) n 2 log n ) . The number of queries is O ( 1 (cid:15) n 3 log n ) . 3 . 2 Randomized local search Next , we present a randomized algorithm which im - proves the approximation ratio of 1 / 3 . The main idea be - hind this algorithm is to ﬁnd a “smoothed” local optimum , where elements are sampled randomly but with different probabilities , based on some underlying set A . Deﬁnition 3 . 5 . We say that a set is sampled with bias δ based on A , if elements in A are sampled independently with probability p = 1 + δ 2 and elements outside of A are sampled independently with probability q = 1 − δ 2 . We de - note this random set by R ( A , δ ) . The Smooth Local Search algorithm : SLS . 1 . Fix parameters δ , δ 0 ∈ [ − 1 , 1 ] . Start with A = ∅ . Let n = | X | denote the total number of elements . In the following , use an estimate for OPT , for example from Algorithm LS . 2 . For each element x , deﬁne ω A , δ ( x ) = E [ f ( R ( A , δ ) ∪ { x } ) ] − E [ f ( R ( A , δ ) \ { x } ) ] . By repeated sampling , we compute ˜ ω A , δ ( x ) , an esti - mate of ω A , δ ( x ) within ± 1 n 2 OPT w . h . p . 3 . If there is x ∈ X \ A such that ˜ ω A , δ ( x ) > 2 n 2 OPT , include x in A and go to Step 2 . 4 . If there is x ∈ A s . t . ˜ ω A , δ ( x ) < − 2 n 2 OPT , remove x from A and go to Step 2 . 5 . Return a random set R ( A , δ 0 ) . In effect , we ﬁnd an approximate local optimum of a de - rived function Φ ( A ) = E [ f ( R ( A , δ ) ) ] . Then we return a set sampled according to R ( A , δ 0 ) ; possibly for δ 0 6 = δ . One can run Algorithm SLS with δ = δ 0 and prove that the best approximation for such parameters is achieved by set - ting δ = δ 0 = √ 5 − 1 2 , the golden ratio . Then , we get an approximation factor of 3 −√ 5 2 − o ( 1 ) ’ 0 . 38 . However , our best result is achieved by taking a combination of two ( δ , δ 0 ) pairs as follows . Theorem 3 . 6 . Algorithm SLS runs in polynomial time . If we run SLS for two choices of parameters , ( δ = 13 , δ 0 = 13 ) and ( δ = 13 , δ 0 = − 1 ) , the better of the two solutions has expected value at least ( 25 − o ( 1 ) ) OPT . Proof . Let Φ ( A ) = E [ f ( R ( A , δ ) ) ] . We set B = X \ A . Recall that in R ( A , δ ) , elements from A are sampled with probability p = 1 + δ 2 , while elements from B are sampled with probability q = 1 − δ 2 . Consider Step 3 where an ele - ment x is added to A . Let A 0 = A ∪ { x } and B 0 = B \ { x } . The reason why x is added to A is that ˜ ω A , δ ( x ) > 2 n 2 OPT ; i . e . ω A , δ ( x ) > 1 n 2 OPT . During this step , Φ ( A ) increases by Φ ( A 0 ) − Φ ( A ) = E [ f ( A 0 ( p ) ∪ B 0 ( q ) ) − f ( A ( p ) ∪ B ( q ) ) ] = ( p − q ) E [ f ( A ( p ) ∪ B 0 ( q ) ∪ { x } ) − f ( A ( p ) ∪ B 0 ( q ) ) ] = δ E [ f ( R ( A , δ ) ∪ { x } ) − f ( R ( A , δ ) \ { x } ) ] = δ ω A , δ ( x ) > δn 2 OPT . Similarly , executing Step 4 increases Φ ( A ) by at least δn 2 OPT . Since the value of Φ ( A ) is always between 0 and OPT , the algorithm cannot iterate more than n 2 / δ times and thus it runs in polynomial time . From now on , let A be the set at the end of the algorithm and B = X \ A . We also use R = A ( p ) ∪ B ( q ) to denote a random set from the distribution R ( A , δ ) . We denote by C the optimal solution , while our algorithm returns either R ( for δ 0 = δ ) or B ( for δ 0 = − 1 ) . When the algorithm terminates , we have ω A , δ ( x ) ≥ − 3 n 2 OPT for any x ∈ A , and ω A , δ ( x ) ≤ 3 n 2 OPT for any x ∈ B . Consequently , for any x ∈ B we have E [ f ( R ∪ { x } ) − f ( R ) ] = Pr [ x / ∈ R ] E [ f ( R ∪ { x } ) − f ( R \ { x } ) ] = 23 ω A , δ ( x ) ≤ 2 n 2 OPT , using Pr [ x / ∈ R ] = p = 2 / 3 . By submodularity , we get E [ f ( R ∪ ( B ∩ C ) ) ] ≤ E [ f ( R ) ] + P x ∈ B ∩ C E [ f ( R ∪ { x } ) − f ( R ) ) ] ≤ E [ f ( R ) ] + | B ∩ C | 2 n 2 OPT ≤ E [ f ( R ) ] + 2 n OPT . Similarly , we can obtain E [ f ( R ∩ ( B ∪ C ) ) ] ≤ E [ f ( R ) ] + 2 n OPT . This means that instead of R , we can analyze R ∪ ( B ∩ C ) and R ∩ ( B ∪ C ) . In order to estimate E [ f ( R ∪ ( B ∩ C ) ) ] and E [ f ( R ∩ ( B ∪ C ) ) ] , we use a further extension of Lemma 2 . 3 which can be proved by another iteration of the same proof : ( ∗ ) E [ f ( A 1 ( p 1 ) ∪ A 2 ( p 2 ) ∪ A 3 ( p 3 ) ) ] ≥ P I ⊆ { 1 , 2 , 3 } Q i ∈ I p i Q i / ∈ I ( 1 − p i ) f (cid:0)S i ∈ I A i (cid:1) . First , we deal with R ∩ ( B ∪ C ) = ( A ∩ C ) ( p ) ∪ ( B ∩ C ) ( q ) ∪ ( B \ C ) ( q ) . We plug in δ = 1 / 3 , i . e . p = 2 / 3 and q = 1 / 3 . Then ( * ) yields E [ f ( R ∩ ( B ∪ C ) ) ] ≥ 827 f ( A ∩ C ) + 227 f ( B ∪ C ) + 227 f ( B ∩ C ) + 427 f ( C ) + 427 f ( F ) + 127 f ( B ) where we denote F = ( A ∩ C ) ∪ ( B \ C ) and we discarded the terms f ( ∅ ) ≥ 0 and f ( B \ C ) ≥ 0 . Similarly , we estimate E [ f ( R ∪ ( B ∩ C ) ) ] , applying ( * ) to a submodular function h ( R ) = f ( R ∪ ( B ∩ C ) ) and writing E [ f ( R ∪ ( B ∩ C ) ) ] = E [ h ( R ) ] = E [ h ( ( A ∩ C ) ( p ) ∪ ( A \ C ) ( p ) ∪ B ( q ) ) ] : E [ f ( R ∪ ( B ∩ C ) ) ] ≥ 827 f ( A ∪ C ) + 227 f ( B ∪ C ) + 227 f ( B ∩ C ) + 427 f ( C ) + 427 f ( ¯ F ) + 127 f ( B ) . Here , ¯ F = ( A \ C ) ∪ ( B ∩ C ) . We use E [ f ( R ) ] + 2 n OPT ≥ 12 ( E [ f ( R ∩ ( B ∪ C ) ) ] + E [ f ( R ∪ ( B ∩ C ) ) ] ) and combine the two estimates . E [ f ( R ) ] + 2 n OPT ≥ 427 f ( A ∩ C ) + 427 f ( A ∪ C ) + 227 f ( B ∩ C ) + 227 f ( B ∪ C ) + 427 f ( C ) + 227 f ( F ) + 227 f ( ¯ F ) + 127 f ( B ) . Now we add 3 27 f ( B ) on both sides and apply submodular - ity : f ( B ) + f ( F ) ≥ f ( B ∪ C ) + f ( B \ C ) ≥ f ( B ∪ C ) and f ( B ) + f ( ¯ F ) ≥ f ( B ∪ ( A \ C ) ) + f ( B ∩ C ) ≥ f ( B ∩ C ) . This leads to E [ f ( R ) ] + 19 f ( B ) + 2 n OPT ≥ 427 f ( A ∩ C ) + 427 f ( A ∪ C ) + 427 f ( B ∩ C ) + 427 f ( B ∪ C ) + 427 f ( C ) and since f ( A ∩ C ) + f ( B ∩ C ) ≥ f ( C ) and f ( A ∪ C ) + f ( B ∪ C ) ≥ f ( C ) , we get E [ f ( R ) ] + 1 9 f ( B ) + 2 nOPT ≥ 12 27 f ( C ) = 4 9 OPT . To conclude , either E [ f ( R ) ] or f ( B ) must be at least ( 25 − 2 n ) OPT . 4 Inapproximability Results In this section , we give hardness results for submodular maximization . Our results are of two ﬂavors . First , we con - sider submodular functions in the form of a sum of “build - ing blocks” of constant size , more precisely nonnegative submodular functions depending only on a constant num - ber of elements . We refer to this as succint representation . Note that all the special cases such as Max Cut are of this type . For algorithms in this model , we prove complexity - theoretic inapproximability results , the strongest one stat - ing that in the general case , a ( 34 + (cid:15) ) - approximation for any ﬁxed (cid:15) > 0 would imply P = NP . In the value oracle model , we show a much tighter result . Namely , any algorithm achieving a ( 1 2 + (cid:15) ) - approximation for a ﬁxed (cid:15) > 0 would require an exponential number of queries to the value oracle . This holds even in the case of symmetric submodular functions , i . e . our 12 - approximation algorithm is optimal in this case . 4 . 1 NP - hardness results Our reductions are based on H˚astad’s 3 - bit and 4 - bit PCP veriﬁers [ 22 ] . Some inapproximability results can be ob - tained immediately from [ 22 ] , by considering the known special cases of submodular maximization , e . g . Max Cut in 4 - uniform hypergraphs which is NP - hard to approximate within a factor better than 78 . We obtain stronger hardness results by reductions from systems of parity equations . The parity function is not sub - modular , but we can obtain hardness results by a careful construction of a “submodular gadget” for each equation . Theorem 4 . 1 . There is no polynomial - time ( 56 + (cid:15) ) - approximation algorithm to maximize a nonnegative sym - metric submodular function in succint representation , un - less P = NP . Proof . Consider an instance of Max E4 - Lin - 2 , a system E of m parity equations on 4 boolean variables each . Let’s deﬁne two elements for each variable , T i and F i , corre - sponding to variable x i being either true or false . For each equation e on variables ( x i , x j , x k , x ‘ ) , we deﬁne a function g e ( S ) . ( This is our “submodular gadget” . ) Let S 0 = S ∩ { T i , F i , T j , F j , T k , F k , T ‘ , F ‘ } . We say that S 0 is a valid quadruple , if it deﬁnes a boolean assignment to x i , x j , x k , x ‘ , i . e . contains exactly one element from each pair { T i , F i } . The function value is determined by S 0 : • If | S 0 | < 4 , let g e ( S ) = | S 0 | . If | S 0 | > 4 , let g e ( S ) = 8 − | S 0 | . • If S 0 is a valid quadruple satisfying e , let g e ( S ) = 4 ( a true quadruple ) . • If S 0 is a valid quadruple not satisfying e , let g e ( S ) = 8 / 3 ( a false quadruple ) . • If | S 0 | = 4 but S 0 is not a valid quadruple , let g e ( S ) = 10 / 3 ( an invalid quadruple ) . It can be veriﬁed that this is a submodular function , us - ing the structure of the parity constraint . We deﬁne f ( S ) = P e ∈E g e ( S ) . This is again a nonnegative submodular func - tion . Observe that for each equation , it is more proﬁtable to choose an invalid assignment than a valid assignment which does not satisfy the equation . Nevertheless , we claim that WLOG the maximum is obtained by selecting exactly one of T i , F i for each variable : Consider a set S and call a vari - able undecided , if S contains both or neither of T i , F i . For each equation with an undecided variable , we get value at most 10 / 3 . Now , modify S into ˜ S by randomly selecting exactly one of T i , F i for each undecided variable . The new set ˜ S induces a valid assignment to all variables . For equa - tions which had a valid assignment already in S , the value does not change . Each equation which had an undecided varible is satisﬁed by ˜ S with probability 1 / 2 . Therefore , the expected value for each such equation is 12 ( 83 + 4 ) = 103 , at least as before , and E [ f ( ˜ S ) ] ≥ f ( S ) . Hence there must exist a set ˜ S such that f ( ˜ S ) ≥ f ( S ) and ˜ S induces a valid assignment . Consequently , we have OPT = max f ( S ) = 83 m + 43 # SAT where # SAT is the maximum number of satisﬁ - able equations . Since it is NP - hard to distinguish whether # SAT ≥ ( 1 − (cid:15) ) m or # SAT ≤ ( 12 + (cid:15) ) m , it is also NP - hard to distinguish between OPT ≥ ( 4 − (cid:15) ) m and OPT ≤ ( 103 + (cid:15) ) m . In the case of general nonnegative submodular functions , we improve the hardness threshold to 3 / 4 . This hardness re - sult is slightly more involved . It requires certain properties of H˚astad’s 3 - bit PCP veriﬁer , implying that Max E3 - Lin - 2 is NP - hard to approximate even for linear systems of a special structure . Lemma 4 . 2 . Fix any (cid:15) > 0 and consider systems of weighted linear equations ( of total weight 1 ) over boolean variables , partitioned into X and Y , so that each equation contains 1 variable x i ∈ X and 2 variables y j , y k ∈ Y . Deﬁne a matrix P ∈ [ 0 , 1 ] Y×Y where P jk is the weight of all equations where the ﬁrst variable from Y is y j and the second variable is y k . Then it’s NP - hard to decide whether there is a solution satisfying equations of weight at least 1 − (cid:15) or whether any solution satisﬁes equations of weight at most 1 / 2 + (cid:15) , even in the special case where P is positive semideﬁnite . Proof . We show that the system of equations arising from H˚astad’s 3 - bit PCP ( see [ 22 ] , pages 24 - 25 ) satisﬁes the properties that we need . In his notation , the equations are generated by choosing f ∈ F U and g 1 , g 2 ∈ F W where U and W , U ⊂ W , are randomly chosen and F U , F W are the spaces of all ± 1 functions on { − 1 , + 1 } U and { − 1 , + 1 } W , respectively . An equation corresponds to a 3 - bit test on f , g 1 , g 2 and its weight is the probability that the veriﬁer performs this particular test . One variable is associated with f ∈ F U , indexing a bit in the Long Code of the ﬁrst prover , and two variables are associated with g 1 , g 2 ∈ F W , index - ing bits in the Long Code of the second prover . This deﬁnes a natural partition of variables into X and Y . The actual variables appearing in the equations are de - termined by the folding convention ; for the second prover , let’s denote them by y j , y k where j = φ ( g 1 ) , k = φ ( g 2 ) . The particular function φ will not matter to us , as long as it is the same for both g 1 and g 2 ( which is the case in [ 22 ] ) . P jk is the probability that the selected variables corresponding to the second prover are y j and y k . Let P U , W jk be the same probability , conditioned on a particu - lar choice of U , W . Since P is a positive linear combina - tion of P U , W , it sufﬁces to prove that each P U , W is pos - itive semideﬁnite . The way that g 1 , g 2 are generated ( for given U , W ) is that g 1 : { − 1 , + 1 } W → { − 1 , + 1 } is uniformly random and g 2 ( y ) = g 1 ( y ) f ( y | U ) µ ( y ) , where f : { − 1 , + 1 } U → { − 1 , + 1 } uniformly random and µ : { − 1 , + 1 } W → { − 1 , + 1 } is a “random noise” , where µ ( x ) = 1 with probability 1 − (cid:15) and − 1 with probability (cid:15) . The value of (cid:15) will be very small , certainly (cid:15) < 1 / 2 . Both g 1 and g 2 are distributed uniformly ( but not inde - pendently ) in F W . The probability of sampling ( g 1 , g 2 ) is the same as the probability of sampling ( g 2 , g 1 ) , hence P U , W is a symmetric matrix . It remains to prove posi - tive semideﬁniteness . Let’s choose an arbitrary function A : Y → R and analyze X j , k P U , W jk A ( j ) A ( k ) = E g 1 , g 2 [ A ( φ ( g 1 ) ) A ( φ ( g 2 ) ) ] = E g 1 , f , µ [ A ( φ ( g 1 ) ) A ( φ ( g 1 fµ ) ) ] where g 1 , f , µ are sampled as described above . If we prove that this quantity is always nonnegative , then P U , W is pos - itive semideﬁnite . Let B : F W → R , B = A ◦ φ ; i . e . , we want to prove E [ B ( g 1 ) B ( g 1 fµ ) ] ≥ 0 . We can expand B using its Fourier transform , B ( g ) = X α ⊆ { − 1 , + 1 } W ˆ B ( α ) χ α ( g ) . Here , χ α ( g ) = Q x ∈ α g ( x ) are the Fourier basis functions . We obtain E [ B ( g 1 ) B ( g 1 fµ ) ] = X α , β ⊆ { − 1 , + 1 } W E [ ˆ B ( α ) χ α ( g 1 ) ˆ B ( β ) χ β ( g 1 fµ ) ] = X α , β ⊆ { − 1 , + 1 } W ˆ B ( α ) ˆ B ( β ) Y x ∈ α ∆ β E g 1 [ g 1 ( x ) ] · E f [ Y y ∈ β f ( y | U ) ] Y z ∈ β E µ [ µ ( z ) ] . The terms for α 6 = β are zero , since then E g 1 [ g 1 ( x ) ] = 0 for each x ∈ α ∆ β . Therefore , E [ B ( g 1 ) B ( g 1 fµ ) ] = X β ⊆ { − 1 , + 1 } W ˆ B 2 ( β ) E f [ Y y ∈ β f ( y | U ) ] Y z ∈ β E µ [ µ ( z ) ] . Now all the terms are nonnegative , since E µ [ µ ( z ) ] = 1 − 2 (cid:15) > 0 for every z and E f [ Q y ∈ β f ( y | U ) ] = 1 or 0 , depend - ing on whether every string in { − 1 , + 1 } U is the projection of an even number of strings in β ( in which case the prod - uct is 1 ) or not ( in which case the expectation gives 0 by symmetry ) . To conclude , X j , k P U , W jk A ( j ) A ( k ) = E [ B ( g 1 ) B ( g 1 fµ ) ] ≥ 0 for any A : Y → R , which means that each P U , W and consequently also P is positive semideﬁnite . Now we are ready to show the following . Theorem 4 . 3 . There is no polynomial - time ( 34 + (cid:15) ) - approximation algorithm to maximize a nonnegative sub - modular function in succint representation , unless P = NP . Proof . We deﬁne a reduction from the system of linear equations provided by Lemma 4 . 2 . For each variable x i ∈ X , we have two elements T i , F i and for each variable y j ∈ Y , we have two elements ˜ T j , ˜ F j . Denote the set of equations by E . Each equation e contains one variable from X and two variables from Y . For each e ∈ E , we deﬁne a submodular function g e ( S ) tailored to this structure . As - sume that S ⊆ { T i , F i , ˜ T j , ˜ F j , ˜ T k , ˜ F k } , the elements cor - responding to this equation ; g e does not depend on other than these 6 elements . We say that S is a valid triple , if it contains exactly one of each { T i , F i } . • The value of each singleton T i , F i corresponding to a variable in X is 1 . • The value of each singleton ˜ T j , ˜ F j corresponding to a variable in Y is 1 / 2 . • For | S | < 3 , g e ( S ) is the sum of its singletons , except g e ( { T i , F i } ) = 1 ( a weak pair ) . • For | S | > 3 , g e ( S ) = g e ( ¯ S ) . • If S is a valid triple satisfying e , let g e ( S ) = 2 ( true triple ) . • If S is a valid triple not satisfying e , let g e ( S ) = 1 ( false triple ) . • If S is an invalid triple containing exactly one of { T i , F i } then g e ( S ) = 2 ( invalid triple of type I ) . • If S is an invalid triple containing both / neither of { T i , F i } then g e ( S ) = 3 / 2 ( invalid triple of type II ) . Verifying that g e is submodular is more tedious here ; we omit the details . Let us move on to the important properties of g e . A true triple gives value 2 , while a false triple gives value 1 . For invalid assignments of value 3 / 2 , we can argue as before that a random valid assignment achieves expected value 3 / 2 as well , so we might as well choose a valid assign - ment . However , in this gadget we also have invalid triples of value 2 ( type I - we cannot avoid this due to submodular - ity . ) Still , we prove that the optimum is attained for a valid boolean assignment . The main argument is , roughly , that if there are many invalid triples of type I , there must be also many equations where we get value only 1 ( a weak pair or its complement ) . For this , we use the positive semideﬁnite property from Lemma 4 . 2 . We deﬁne f ( S ) = P e ∈E w ( e ) g e ( S ) where w ( e ) is the weight of equation e . We claim that max f ( S ) = 1 + max w SAT , where w SAT is the weight of satisﬁed equa - tions . First , for a given boolean assignment , the correspond - ing set S selecting T i or F i for each variable achieves value f ( S ) = w SAT · 2 + ( 1 − w SAT ) · 1 = 1 + w SAT . The non - trivial part is proving that the optimum f ( S ) is attained for a set inducing a valid boolean assignment . Consider any set S and deﬁne V : E → { − 1 , 0 , + 1 } where V ( e ) = + 1 if S induces a satisfying assignment to equation e , V ( e ) = − 1 if S induces a non - satisfying as - signment to e and V ( e ) = 0 if S induces an invalid as - signment to e . Also , deﬁne A : Y → { − 1 , 0 , + 1 } , where A ( j ) = | S ∩ { ˜ T j , ˜ F j } | − 1 , i . e . A ( j ) = 0 if S induces a valid assignment to y j , and A ( j ) = ± 1 if S contains both / neither of ˜ T j , ˜ F j . Observe that for an equation e whose Y - variables are y j , y k , only one of V ( e ) and A ( j ) A ( k ) can be nonzero . The gadget g e ( S ) is designed in such a way that g e ( S ) ≤ 1 2 ( 3 − A ( j ) A ( k ) + V ( e ) ) . This can be checked case by case : for valid assignments , A ( j ) A ( k ) = 0 and we get value 2 or 1 depending on V ( e ) = ± 1 . For invalid assignments , V ( e ) = 0 ; if at least one of the variables y j , y k has a valid assignment , then A ( j ) A ( k ) = 0 and we can get at most 3 / 2 ( an invalid triple of type II ) . If both y j , y k are invalid and A ( j ) A ( k ) = 1 , then we can get only 1 ( a weak pair or its complement ) and if A ( j ) A ( k ) = − 1 , we can get 2 ( an invalid triple of type I ) . The total value is f ( S ) = P e ∈E w ( e ) g e ( S ) ≤ P e = ( x i , y j , y k ) w ( e ) · 12 ( 3 − A ( j ) A ( k ) + V ( e ) ) . Now we use the positive semideﬁnite property of our linear system , which means that P e = ( x , y j , y k ) w ( e ) A ( j ) A ( k ) = P j , k P jk A ( j ) A ( k ) ≥ 0 for any function A . Hence , f ( S ) ≤ 12 P e ∈E w ( e ) ( 3 + V ( e ) ) . Let’s modify S into a valid boolean assignment by choosing randomly one of T i , F i for all variables such that S contains both / neither of T i , F i . Denote the new set by ˜ S and the equations containing any randomly chosen vari - able by R . We satisfy each equation in R with probability 1 / 2 , which gives expected value 3 / 2 for each such equation , while the value for other equations remains unchanged . E [ f ( ˜ S ) ] = 3 2 X e ∈R w ( e ) + 1 2 X e ∈E \ R w ( e ) ( 3 + V ( e ) ) = 1 2 X e ∈E w ( e ) ( 3 + V ( e ) ) ≥ f ( S ) . This means that there is a set ˜ S of optimal value , inducing a valid boolean assignment . 4 . 2 Value query complexity results Finally , we prove that our 12 - approximation for sym - metric submodular functions is optimal in the value oracle model . First , we present a similar result for the “random set” model , which illustrates some of the ideas needed for the more general result . Proposition 4 . 4 . For any δ > 0 , there is (cid:15) > 0 such that for any ( random ) sequence of queries Q ⊆ 2 X , | Q | ≤ 2 (cid:15)n , there is a nonnegative submodular function f such that ( with high probability ) for all queries Q ∈ Q , f ( Q ) ≤ (cid:18) 1 4 + δ (cid:19) OPT . Proof . Let (cid:15) = 132 δ 2 and ﬁx a sequence Q ⊆ 2 X of 2 (cid:15)n queries . We prove the existence of f by the probabilis - tic method . Consider functions corresponding to cuts in a complete bipartite directed graph on ( C , D ) , f C ( S ) = | S ∩ C | · | ¯ S ∩ D | . We choose a uniformly random C ⊆ X and D = X \ C . The idea is that for any query , a typi - cal C bisects both Q and its complement , which means that f C ( Q ) is roughly 14 OPT . We call a query Q ∈ Q “suc - cessful” , if f C ( Q ) > ( 14 + δ ) OPT . Our goal is to prove that with high probability , C avoids any successful query . We use Chernoff’s bound : For any set A ⊆ X of size a , Pr [ | A ∩ C | > 1 2 ( 1 + δ ) | A | ] = Pr [ | A ∩ C | < 1 2 ( 1 − δ ) | A | ] < e − δ 2 a / 2 . With probability at least 1 − 2 e − 2 δ 2 n , the size of C is in [ ( 12 − δ ) n , ( 12 + δ ) n ] , so we can assume this is the case . We have OPT ≥ ( 14 − δ 2 ) n 2 ≥ 14 n 2 / ( 1 + δ ) ( for small δ > 0 ) . No query can achieve f C ( Q ) > ( 14 + δ ) OPT ≥ 116 n 2 unless | Q | ∈ [ 116 n , 1516 n ] , so we can assume this is the case for all queries . By Chernoff’s bound , Pr [ | Q ∩ C | > 1 2 ( 1 + δ ) | Q | ] < e − δ 2 n / 32 and Pr [ | ¯ Q ∩ D | > 1 2 ( 1 + δ ) | ¯ Q | ] < e − δ 2 n / 32 . If neither of these events occurs , the query is not successful , since f C ( Q ) = | Q ∩ C | · | ¯ Q ∩ D | < 14 ( 1 + δ ) 2 | Q | · | ¯ Q | ≤ 116 ( 1 + δ ) 2 n 2 ≤ 14 ( 1 + δ ) 3 OPT ≤ ( 14 + δ ) OPT . For now , ﬁx a sequence of queries . By the union bound , we get that the probability that any query is successful is at most 2 (cid:15)n 2 e − δ 2 n / 32 = 2 ( 2 e ) (cid:15)n . Thus with high probabil - ity , there is no successful query for C . Even for a random sequence , the probabilistic bound still holds by averaging over all possible sequences of queries . We can ﬁx any C for which the bound is valid , and then the claim of the lemma holds for the submodular function f C . This means that in the model where an algorithm only samples a sequence of polynomially many sets and re - turns the one of maximal value , we cannot improve our 14 - approximation ( Section 2 ) . As we show next , this ex - ample can be modiﬁed for the model of adaptive algorithms with value queries , to show that our 12 - approximation for symmetric submodular functions is optimal , even among all adaptive algorithms ! Theorem 4 . 5 . For any (cid:15) > 0 , there are instances of nonneg - ative symmetric submodular maximization , such that there is no ( adaptive , possibly randomized ) algorithm using less than e (cid:15) 2 n / 16 queries that always ﬁnds a solution of expected value at least ( 12 + (cid:15) ) OPT . Proof . We construct a nonnegative symmetric submodular function on [ n ] = C ∪ D , | C | = | D | = n / 2 , which has the following properties : • f ( S ) depends only on k = | S ∩ C | and ‘ = | S ∩ D | . Henceforth , we write f ( k , ‘ ) to denote the value of any such set . • When | k − ‘ | ≤ (cid:15)n , the function has the form f ( k , ‘ ) = ( k + ‘ ) ( n − k − ‘ ) = | S | ( n − | S | ) , i . e . , the cut function of a complete graph . The value depends only on the size of S , and the maximum at - tained by such sets is 14 n 2 . • When | k − ‘ | > (cid:15)n , the function has the form f ( k , ‘ ) = k ( n − 2 ‘ ) + ( n − 2 k ) ‘ − O ( (cid:15)n 2 ) , close to the cut function of a complete bipartite graph on ( C , D ) with edge weights 2 . The maximum in this range is OPT = 12 n 2 ( 1 − O ( (cid:15) ) ) , attained for k = 12 n and ‘ = 0 ( or vice versa ) . If we construct such a function , we can argue as fol - lows . Consider any algorithm , for now deterministic . ( For a randomized algorithm , let’s condition on its random bits . ) Let the partition ( C , D ) be random and unknown to the algorithm . The algorithm issues some queries Q to the value oracle . Call Q “unbalanced” , if | Q ∩ C | differs from | Q ∩ D | by more than (cid:15)n . For any query Q , the probability that Q is unbalanced is at most e − (cid:15) 2 n / 8 , by standard Cher - noff bounds . Therefore , for any ﬁxed sequence of e (cid:15) 2 n / 16 queries , the probability that any query is unbalanced is still at most e (cid:15) 2 n / 16 · e − (cid:15) 2 n / 8 = e − (cid:15) 2 n / 16 . As long as queries are balanced , the algorithm gets the same answer regardless of ( C , D ) . Hence , it follows the same path of computa - tion and issues the same queries . With probability at least 1 − e − (cid:15) 2 n / 16 , all its queries will be balanced and it will never ﬁnd out any information about the partition ( C , D ) . For a randomized algorithm , we can now average over its random choices ; still , with probability at least 1 − e − (cid:15) 2 n / 16 the algorithm will never query any unbalanced set . Alternatively , consider a function g ( S ) which is deﬁned by g ( S ) = | S | ( n − | S | ) for all sets S . We proved that with high probability , the algorithm will never query a set where f ( S ) 6 = g ( S ) and hence cannot distinguish between the two instances . However , max S f ( S ) = 12 n 2 ( 1 − O ( (cid:15) ) ) , while max S g ( S ) = 14 n 2 . This means that there is no ( 12 + (cid:15) ) - approximation algorithm with a subexponential number of queries , for any (cid:15) > 0 . It remains to construct the function f ( k , ‘ ) and prove its submodularity . For convenience , assume that (cid:15)n is an inte - ger . In the range where | k − ‘ | ≤ (cid:15)n , we already deﬁned f ( k , ‘ ) = ( k + ‘ ) ( n − k − ‘ ) . In the range where | k − ‘ | ≥ (cid:15)n , let us deﬁne f ( k , ‘ ) = k ( n − 2 ‘ ) + ( n − 2 k ) ‘ + (cid:15) 2 n 2 − 2 (cid:15)n | k − ‘ | . The (cid:15) - terms are chosen so that f ( k , ‘ ) is a smooth function on the boundary of the two regions . E . g . , for k − ‘ = (cid:15)n , we get f ( k , ‘ ) = ( 2 k − (cid:15)n ) ( n − 2 k + (cid:15)n ) for both expres - sions . Moreover , the marginal values also extend smoothly . Consider an element i ∈ C ( for i ∈ D the situation is symmetric ) . The marginal value of i added to a set S is f ( S ∪ { i } ) − f ( S ) = f ( k + 1 , ‘ ) − f ( k , ‘ ) . We split into three cases : • If k − ‘ < − (cid:15)n , we have f ( k + 1 , ‘ ) − f ( k , ‘ ) = ( n − 2 ‘ ) + ( − 2 ‘ ) + 2 (cid:15)n = ( 1 + 2 (cid:15) ) n − 4 ‘ . • If − (cid:15)n ≤ k − ‘ < (cid:15)n , we have f ( k + 1 , ‘ ) − f ( k , ‘ ) = ( k + 1 + ‘ ) ( n − k − 1 − ‘ ) − ( k + ‘ ) ( n − k − ‘ ) = ( n − k − 1 − ‘ ) − ( k + 1 + ‘ ) = n − 2 k − 2 ‘ − 2 . In this range , this is between ( 1 ± 2 (cid:15) ) − 4 ‘ . • If k − ‘ ≥ (cid:15)n , we have f ( k + 1 , ‘ ) − f ( k , ‘ ) = ( n − 2 ‘ ) + ( − 2 ‘ ) − 2 (cid:15)n = ( 1 − 2 (cid:15) ) n − 4 ‘ . Now it’s easy to see that the marginal value is decreasing in both k and ‘ , in each range and also across ranges . Acknowledgements . The second author thanks Maxim Sviridenko for pointing out some related work . References [ 1 ] A . Ageev and M . Sviridenko . An 0 . 828 approximation algorithm for uncapacitated facility location problem , Discrete Applied Mathematics 93 : 2 – 3 , ( 1999 ) 149 – 156 . [ 2 ] P . Alimonti . Non - oblivious Local Search for MAX 2 - CCSP with application to MAX DICUT , Proc . of the 23rd International Workshop on Graph - theoretic Con - cepts in Computer Science ( 1997 ) . [ 3 ] G . Calinescu , C . Chekuri , M . P´al and J . Vondr´ak . Max - imizing a submodular set function subject to a matroid constraint , Proc . of 12th IPCO ( 2007 ) , 182 – 196 . [ 4 ] V . Cherenin . Solving some combinatorial problems of optimal planning by the method of successive calcula - tions , Proc . of the Conference of Experiences and Per - spectives of the Applications of Mathematical Meth - ods and Electronic Computers in Planning ( in Russian ) , Mimeograph , Novosibirsk ( 1962 ) . [ 5 ] G . Cornuejols , M . Fischer and G . Nemhauser . Location of bank accounts to optimize ﬂoat : an analytic study of exact and approximation algorithms , Management Sci - ence , 23 ( 1977 ) , 789 – 810 . [ 6 ] G . Cornuejols , M . Fischer and G . Nemhauser . On the uncapacitated location problem , Annals of Discrete Math 1 ( 1977 ) , 163 – 178 . [ 7 ] G . P . Cornuejols , G . L . Nemhauser and L . A . Wolsey . The uncapacitated facility location problem , Discrete Location Theory ( 1990 ) , 119 – 171 . [ 8 ] J . Edmonds . Matroids , submodular functions and cer - tain polyhedra , Combinatorial Structures and Their Ap - plications ( 1970 ) , 69 – 87 . [ 9 ] U . Feige and M . X . Goemans . Approximating the value of two - prover systems , with applications to MAX - 2SAT and MAX - DICUT , Proc . of the 3rd Israel Sym - posium on Theory and Computing Systems , Tel Aviv ( 1995 ) , 182 – 189 . [ 10 ] U . Feige . A threshold of ln n for approximating Set Cover , Journal of the ACM 45 ( 1998 ) , 634 – 652 . [ 11 ] U . Feige . Maximizing social welfare when utility functions are subadditive , Proc . of 38th STOC ( 2006 ) , 41 – 50 . [ 12 ] U . Feige and J . Vondr´ak . Approximation algorithms for combinatorial allocation problems : Improving the factor of 1 − 1 / e , Proc . of 47th FOCS ( 2006 ) , 667 – 676 . [ 13 ] S . Fujishige . Canonical decompositions of symmetric submodular systems , Discrete Applied Mathematics 5 ( 1983 ) , 175 – 190 . [ 14 ] L . Fleischer , S . Fujishige and S . Iwata . A combinato - rial , strongly polynomial - time algorithm for minimiz - ing submodular functions , Journal of the ACM 48 : 4 ( 2001 ) , 761 – 777 . [ 15 ] A . Frank . Matroids and submodular functions , An - notated Biblographies in Combinatorial Optimization ( 1997 ) , 65 – 80 . [ 16 ] M . X . Goemans and D . P . Williamson . Improved ap - proximation algorithms for maximum cut and satisﬁa - bility problems using semideﬁnite programming , Jour - nal of the ACM 42 ( 1995 ) , 1115 – 1145 . [ 17 ] B . Goldengorin , G . Sierksma , G . Tijsssen and M . Tso . The data correcting algorithm for the minimization of supermodular functions , Management Science , 45 : 11 ( 1999 ) , 1539 – 1551 . [ 18 ] B . Goldengorin , G . Tijsssen and M . Tso . The maxi - mization of submodular functions : Old and new proofs for the correctness of the dichotomy algorithm , SOM Report , University of Groningen ( 1999 ) . [ 19 ] V . Guruswami . Inapproximability results for set split - ting and satisﬁability problems with no mixed clauses , Algorithmica 38 ( 2004 ) , 451 – 469 . [ 20 ] V . Guruswami and S . Khot . Hardness of Max 3 - SAT with no mixed clauses , Proc . of 20th IEEE Conference on Computational Complexity ( 2005 ) , 154 – 162 . [ 21 ] E . Halperin and U . Zwick . Combinatorial approxima - tion algorithms for the maximum directed cut problem , Proc . of 12th SODA ( 2001 ) , 1 – 7 . [ 22 ] J . H˚astad . Some optimal inapproximability results , Journal of the ACM 48 ( 2001 ) , 798 – 869 . [ 23 ] V . R . Khachaturov . Mathematical methods of regional programming ( in Russian ) , Nauka , Moscow ( 1989 ) . [ 24 ] S . Khot , G . Kindler , E . Mossel and R . O’Donnell . Optimal inapproximability results for MAX - CUT and other two - variable CSPs ? Proc . of 45th FOCS ( 2004 ) , 146 – 154 . [ 25 ] D . Livnat , M . Lewin and U . Zwick . Improved round - ing techniques for the MAX 2 - SAT and MAX DI - CUT problems . Proc . of 9th IPCO ( 2002 ) , 67 – 82 . [ 26 ] H . Lee , G . Nemhauser and Y . Wang . Maximizing a submodular function by integer programming : Polyhe - dral results for the quadratic case , European Journal of Operational Research 94 , 154 – 166 . [ 27 ] L . Lov´asz . Submodular functions and convexity . A . Bachem et al . , editors , Mathematical Programmming : The State of the Art , 235 – 257 . [ 28 ] M . Minoux . Accelerated greedy algorithms for max - imizing submodular functions , J . Stoer , ed . , Actes Congress IFIP , Springer Verlag , Berlin ( 1977 ) , 234 – 243 . [ 29 ] E . Mossel , R . O’Donnell and K . Oleszkiewicz . Noise stability of functions with low inﬂuences : invariance and optimality , Proc . of 46th FOCS ( 2005 ) , 21 – 30 . [ 30 ] G . L . Nemhauser , L . A . Wolsey and M . L . Fisher . An analysis of approximations for maximizing submodular set functions I , Mathematical Programming 14 ( 1978 ) , 265 – 294 . [ 31 ] G . L . Nemhauser , L . A . Wolsey and M . L . Fisher . An analysis of approximations for maximizing submodu - lar set functions II , Mathematical Programming Study 8 ( 1978 ) , 73 – 87 . [ 32 ] T . Robertazzi and S . Schwartz . An accelated sequen - tial algorithm for producing D - optimal designs , SIAM Journal on Scientiﬁc and Statistical Computing 10 , 341 – 359 . [ 33 ] M . Queyranne . A combinatorial algorithm for mini - mizing symmetric submodular functions , Proc . of 6th SODA ( 1995 ) , 98 – 101 . [ 34 ] A . Sch¨afer and M . Yannakakis . Simple local search problems that are hard to solve , SIAM J . Comput . 20 : 1 ( 1991 ) , 56 – 87 . [ 35 ] A . Schrijver . A combinatorial algorithm minimiz - ing submodular functions in strongly polynomial time , Journal of Combinatorial Theory , Series B 80 ( 2000 ) , 346 – 355 . [ 36 ] M . Sviridenko . A note on maximizing a submodular set function subject to a knapsack constraint , Opera - tions Research Letters 32 ( 2004 ) , 41 – 43 .