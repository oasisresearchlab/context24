129 https : / / doi . org / 10 . 3102 / 0091732X21990620 Review of Research in Education March 2021 , Vol . 45 , pp . 129 – 169 DOI : 10 . 3102 / 0091732X21990620 Chapter reuse guidelines : sagepub . com / journals - permissions © 2021 AERA . journals . sagepub . com / home / rre Chapter 5 Research Worth Using : ( Re ) Framing Research Evidence Quality for Educational Policymaking and Practice N orma C . m iNg San Francisco Unified School District L aureN B . g oLdeNBerg New York City Department of Education This chapter calls for researchers to reconceptualize research quality from the perspective of its expected use , attending to power dynamics that influence how knowledge is defined , constructed , and validated through the research enterprise . Addressing these concerns when designing and conducting education research can yield more useful research evidence for building more equitable education systems . Anchored in scholarship on research utilization and methodological critiques , the chapter introduces a research quality framework that integrates relevance and rigor through five key dimensions of Research Worth Using : ( 1 ) relevance of question : alignment of research topics to practical priorities ; ( 2 ) theoretical credibility : explanatory strength and coherence of principles investigated ; ( 3 ) methodological credibility : internal and external credibility of study design and execution ; ( 4 ) evidentiary credibility : robustness and consistency of cumulative evidence ; and ( 5 ) relevance of answers : justification for practical application . This framework simultaneously uplifts the voices and needs of policymakers , practitioners , and community members , while elevating standards for excellence in education research . We call attention to the myriad ways in which the quality of evidence generated can be strengthened , before describing implications for curating and using research . We conclude by offering suggestions for applying and further developing the framework . O ne expectation for education research is that policymakers and practitioners will use it to improve educational structures , systems , and practices toward the ultimate goal of creating equitable , high - quality learning environments for all stu - dents . Fulfilling this expectation requires researchers to reconceptualize research 990620 RREXXX10 . 3102 / 0091732X21990620Review of Research in Education Ming , Goldenberg : Research Worth Using research - article 2021 130 Review of Research in Education , 45 quality in ways that reflect what policymakers , practitioners , and community mem - bers need and value in research . This chapter calls for education scholars to reframe their criteria of research quality by incorporating stakeholder perspectives to yield more useful research evidence . Efforts to improve research production , curation , and utilization have been increasing . These include structures that involve practitioners in the production of usable knowledge , such as research - practice partnerships ( Arce - Trigatti et al . , 2018 ; Coburn & Penuel , 2016 ) , design - based implementation research ( Penuel et al . , 2011 ) , participatory research methods ( Adelman , 1993 ) , and continuous improve - ment ( Bryk et al . , 2015 ) . Resources devoted to curating research evidence have grown , including evidence registries and practice guides such as the What Works Clearinghouse in the United States and the Education Endowment Fund in the United Kingdom . Finally , several bodies of literature address the dissemination and application of scientific knowledge , including research use ( Davies & Nutley , 2008 ; Farley - Ripple et al . , 2018 ; Tseng , 2012 ) , evidence - based practice and policy ( Head , 2008 ) , knowledge mobilization / utilization ( B . Levin , 2004 , 2008 ; Weiss & Bucuvalas , 1980 ) , and implementation science ( Grimshaw et al . , 2012 ) . This scholarship illumi - nates the conditions , activities , and mechanisms that support the production , cura - tion , and utilization of research . As promising as these approaches are , they are insufficient for addressing the nec - essary scope and scale of producing high - quality education research evidence for practical application , especially to advance equity in educational opportunities and outcomes . Coproduction approaches typically focus on the usefulness of a specific set of findings in a particular context rather than cultivating a broader evidence base . Evidence repositories tend to prioritize the questions and methods valued by research - ers over those of practitioners . Finally , research utilization focuses on using existing research , which runs the risk that “weak research may have unwarranted impact , and researchers may be tempted to push for unwarranted impact” ( Gorard et al . , 2020 , p . 4 ) . High - quality evidence is necessary , although not sufficient , for high - quality use . We argue that the quality of education research must be defined based on its potential for use , incorporating policymaker , practitioner , and community member perspectives . The absence of their contributions leads to blind spots in research top - ics , diminished utility of research evidence , and decreased use . Responding to a key question posed by this volume—“How can we conceptualize quality in ways that engage practitioners and policymakers , to make our highest quality work accessible and relevant ? ”—this chapter introduces a research quality framework that integrates both relevance and rigor through five key dimensions of research worth using : 1 . Relevance of question : alignment of research topics to practical priorities 2 . Theoretical credibility : explanatory strength and coherence of principles investigated 3 . Methodological credibility : internal and external credibility of study design and execution Ming , Goldenberg : Research Worth Using 131 4 . Evidentiary credibility : robustness and consistency of cumulative evidence 5 . Relevance of answers : justification for practical application Useful research must investigate relevant questions given its intended use , timing , audience , and content . Rigor encompasses both the questions posed and the answers accumulated , with investigations anchored in a broader body of coherent theory and robust evidence . Theoretical credibility enables greater conceptual understanding on the part of users . Methodological credibility requires transparency across methods , con - sidering variation regarding “for whom” and “under what conditions” the results dem - onstrate practical significance . Evidentiary credibility derives from convergence , upon synthesizing findings across a broad range of studies and disciplines . Finally , relevant answers should include information to guide decisions about whether and how to apply the research in practice , addressing costs as well as sustainability , spread , and scale . Our perspectives in developing this framework are informed by our experiences as school district research staff as well as our academic training as learning scientists . Both authors engage in all stages of the research process as producers , consumers , and facilitators of the collaborative production and use of evidence . We work with district and school - based leaders and educators to improve how knowledge is developed , shared , and applied to yield better learning outcomes . Our positions situate us on the supposed “bridge” between research and practice , where we shape study designs and convey practical priorities on one side , and field questions about relevant research and guide the interpretation of evidence on the other . In these roles , we manage external research projects and partnerships as well as internal evaluations , applying a lens of continuous improvement to multiple evidence sources to help stakeholders iterate , innovate , and achieve system - wide goals . Given our positions in urban school district offices , as well as our identities as a Taiwanese American woman and as a White Jewish woman , we confront ongoing equity issues where we live and work , including how our own biases and actions may contribute toward perpetuating inequitable outcomes for students . Our colleagues who work more directly with school staff , families , and students routinely remind us of the inequities they see daily within and between schools . We credit them with rais - ing critical issues around how histories of racism , dispossession , and accumulation of power influence our education systems . They have elevated the voices of critical race activists , advocates , and scholars throughout our respective organizations , engaging us in courageous conversations about racial equity ( Jones & Okun , 2001 ; Singleton , 2014 ) , cultural responsiveness ( Bishop , 1990 ; Hammond , 2014 ; Ladson - Billings , 1995 , 2014 ) , and disrupting systemic oppression ( National Equity Project , 2020 ) . Amid persistent opportunity gaps in our educational systems , it becomes all the more critical to scrutinize the power dynamics by which knowledge is defined , constructed , and shared through the research enterprise . While we do not expect the production or use of research alone to resolve these inequities , we believe that incorporating practitioner and community perspectives into research quality standards can heighten attention to these systemic issues and yield more equitable impacts . 132 Review of Research in Education , 45 Inequities in research production systems have cascading effects on inequities in the processes and outcomes of research use , as power imbalances within the research community extend beyond it . The education research community must recognize that research evidence is not “neutral . ” Rather , it is shaped by who conducts the research , who participates in research , and who uses it , for whom , often reifying exist - ing social structures and power dynamics ( Chicago Beyond , 2019 ; Kirkland , 2019 ; Milner , 2020 ; Philip et al . , 2018 ) . McKenzie and Phillips ( 2016 ) warn of “equity traps” such as deficit thinking , racial erasure , and naïve acceptance of meritocracy , all of which can allow people—including researchers—to remain unconscious of the social structures and systems that perpetuate educational inequities and their own role in preserving them . This plays out in research that extracts pain narratives with - out exposing the oppression that caused them ( Tuck & Yang , 2014 ) , research that reproduces racialized power dynamics underneath pretenses of color - blindness ( Vakil et al . , 2016 ) , and research being used to justify inequitable power hierarchies ( Doucet , 2019 ) . These dynamics impede the quality of the research , the quality of its use , and the equity of its impact . We developed the framework through a descriptive synthesis of the scholarship on quality of research evidence and use , including the use of research evidence , evidence - informed decision making , evidence - based practice and policy , knowledge mobiliza - tion / utilization , quality of research evidence , and evidence synthesis , drawing from the social sciences and health care as well as education . To elaborate further on the specific dimensions of the framework , we drew on debates about research method - ologies , including critical works on causal modeling , statistical and practical signifi - cance , and external validity . We also consulted research from continuous improvement , implementation science , design - based research , research - practice partnerships , par - ticipatory action research , and cost analyses . We expanded the bodies of research we reviewed by using the reference lists of key articles and searching the grey literature ( research , evaluation , and technical reports from research , policy , and governmental organizations ) as it provides authors greater flexibility for writing and disseminating works in progress in emerging fields prior to journal publication . DEFininG “REsEaRCh EviDEnCE” anD “UsE” “Research evidence” and “use” have many meanings . We acknowledge that educa - tion professionals consult research as part of a wide range of evidence sources in their practice ( Tseng , 2012 ) , which include local or state assessment data , internal reports , improvement data , government reports , research and evaluation reports , published academic research , and more . Here , we focus on research evidence , following the W . T . Grant Foundation’s ( n . d . ) definition : “a type of evidence derived from applying systematic methods and analyses to address a predefined question or hypothesis . ” The word use masks the many complex ways educators in different roles apply research evidence to their settings . A common image of use is when social science research or evaluation results directly influence a decision ( Kennedy , 1982 ; Tseng Ming , Goldenberg : Research Worth Using 133 et al . , 2017 ; Weiss et al . , 2005 ) . Often referred to as instrumental use , metaphors for this somewhat idealized approach invoke a linear progression or “one - way street” from evidence to decision ( Tseng et al . , 2017 ) via a producer - push model ( Lavis et al . , 2003 ) . However , many empirical studies depict an alternate view of the “diffuse and circuitous” process where policymakers use research to orient themselves to prob - lems and potential solutions ( Weiss , 1977 , pp . 533 – 534 ) . This model of conceptual research use incorporates thinking and considering evidence as part of a process ( Kennedy , 1982 ; Weiss , 1980 ) . Two other common uses of research evidence are symbolic / political ( Weiss & Bucuvalas , 1980 ) and imposed ( Weiss et al . , 2005 ) . The former is often portrayed negatively by researchers , yet it is essential for mobilizing stakeholder support for evidence - based practices ( Nutley et al . , 2003 , 2007 ) . The latter describes the “carrot - and - stick” approach to research evidence when its use is mandated by law or policy . In this chapter , we recognize the potential legitimacy of all these forms of research use . Furthermore , the word use glosses over the “users . ” Though many studies treat educators as a monolithic group , research on using evidence has differentiated between users who serve in various roles such as teachers , school leaders , district poli - cymakers , and central office staff ( Honig , 2013 ; Spillane , 1998 ) . Other stakeholders , such as community members and elected officials , also use research evidence . While users in all roles value high - quality evidence , their needs for research curation and synthesis may vary , along with their prior knowledge , beliefs , and contextual infor - mation ( Coburn , Honig , et al . , 2009 ) . Using research evidence presupposes quality . The education research community has long wrestled with how to define quality of research evidence , with recurring calls for increased rigor ( Lagemann , 2000 ) . One approach is establishing shared standards by professional associations of researchers ( e . g . , American Educational Research Association , 2006 , 2009 ) . Another is funding studies that use particular methodolo - gies . One well - known example is how the Institute for Education Sciences , estab - lished by Congress in 2002 , accelerated the use of causal research methods such as randomized controlled field trials in studies of education - related topics through its research programs . More recently , the Every Student Succeeds Act passed by Congress in 2015 established “evidence of effectiveness” tiers for receiving federal funding that include quasi - experimental and correlational methods and stipulate that programs must have no evidence of negative effect . The remainder of this chapter is organized into two sections . First , building on these definitions of research evidence and use , we offer a framework for defining research evidence quality through the lens of practical use that is broader than specific methodologies and stringent in the comprehensiveness of evidence . We describe the Research Worth Using framework’s five dimensions , along with the warrants for each . Second , we illustrate their application to issues at various levels of the education system and outline their implications for research production , curation , and utiliza - tion , noting opportunities to further develop the framework . 134 Review of Research in Education , 45 FRaMEWoRk FoR REsEaRCh WoRth UsinG This section describes the Research Worth Using framework’s five dimensions in detail . We elaborate on possible subdimensions by which to evaluate research . The framework goes beyond what may be expected of any single study , mirroring the need for policymaking to be based on multiple sources and forms of evidence . Relevance of Question : alignment of Research topic to Practical Priorities The first dimension addresses the challenge of successfully aligning research to practical needs . This requires attending closely to its purpose and intended use ( why ? ) , timing ( when ? ) , audience ( for whom ? ) , and content ( what ? ) . Purpose Practitioners , policymakers , and researchers across multiple fields have empha - sized the importance for research to address issues that are timely and relevant to policy and practice ( Coburn , Honig , et al . , 2009 ; Lugo - Gil et al . , 2019 ; Oliver & Cairney , 2019 ) . Possible purposes for research use may range from building political support for particular improvement initiatives to selecting programs and allocating staff ( Honig & Coburn , 2008 ) . Other research use activities include adopting cur - riculum , designing professional development , and implementing policy ( Penuel , Farrell , et al . , 2016 ) , as well as teacher - level decisions about lesson planning , instruc - tional strategies , and formative assessment ( Coldwell et al . , 2017 ) . Davies and Powell ( 2010 ) propose a typology that goes beyond “what works” to encompass multiple additional categories of knowledge : the nature of a problem , why an action is required , how to put the action in practice , and whom to involve . Timing Numerous authors have emphasized the timeliness of research as a critical factor for influencing policy and practice ( e . g . , Lugo - Gil et al . , 2019 ; Oliver & Cairney , 2019 ; Penuel , Briggs , et al . , 2016 ) . Policymaking process models often outline four distinct phases of formulation , adoption , implementation , and revision ( Day et al . , 2019 ) , across which information needs differ . Policymakers and practitioners need evidence during policy development and program implementation to inform future decisions , not just evaluate past actions ( O’Brien & Martinez - Vidal , 2016 ) . Audience The relevance of research evidence depends on the specific user’s role , context , and sphere of influence ( Claes et al . , 2015 ; Honig et al . , 2017 ; Lavis et al . , 2003 ; Ward , 2017 ) . Policymakers and practitioners face “differing incentives , goals , language , demands and time frames” ( Tseng & Nutley , 2014 , p . 171 ) . Policymakers attend more closely to financial costs and harmful effects ( Brick & Freeman , 2019 ) and “mobilize information to support an agenda” ( Shonkoff , 2000 , p . 181 ) . School staff Ming , Goldenberg : Research Worth Using 135 value actionability of concrete procedures and compatibility with local context ( Hemsley - Brown & Sharp , 2003 ; Neal et al . , 2018 ) . While systemic improvement demands coherence across multiple levels ( Bryk et al . , 2010 ; Forman et al . , 2017 ; Tucker & Slavin , 2018 ) , perspectives and relation - ships may vary considerably across staff at different levels within the same institution , along with their influence on evidence use ( Honig & Venkateswaran , 2012 ) . The “complex and departmentalized organizational structures” in educational agencies yield different content knowledge and perspectives ( Coburn , Toure , et al . , 2009 , p . 1139 ) , whether due to disciplinary backgrounds ( Spillane , 1998 ) or nature of the work ( Coburn & Talbert , 2006 ) . District leaders may be further subdivided into principals , principal supervisors , and personnel across different central offices , such as curriculum and instruction , assessment , special education , and federal programs , who vary in the research topics they value ( Penuel , Briggs , et al . , 2016 ) . Potential research users may extend beyond the institutions formally tasked with providing and improving educational services . As advocated by Gutiérrez and Penuel ( 2014 ) , negotiating the problem of focus should “include district and school leaders , teachers , parents , community stakeholders , and wherever possible , children and youth” ( p . 20 ) . Determining relevance requires “working inside a more complex and dynamic understanding” of the wisdom and desires of marginalized communities ( Tuck & Yang , 2014 , p . 231 ) . Values , knowledge , needs , and opportunities for research engagement may shift depending on which stakeholders’ interests are repre - sented , yet policymakers have more power in the conversations that influence how research is produced and consulted . As Kirkland ( 2019 ) notes , who determines what research is relevant influences who uses which research , why , for whom , and how . He observes that under the guise of objectivity , research evidence has historically been wielded as a system of power to preserve White supremacy and anti - Black racism . Such power dynamics play out in both the production and use of research . Content Identifying the audience for the research leads to questions about how to align the practices and outcomes under investigation with the interests of those responsible for implementing those changes . Are the practices under investigation appropriate to the work of the intended users ? Are the outcomes of interest those which the users are able to directly affect ? For example , a study on instructional strategies may inspire teachers to adapt their classroom practice , while motivating school and district lead - ers to modify their professional supports . Rather than indulging in the “metrico - philia” ( R . Smith , 2010 ) of fixating only on readily available yet distal student outcome data , useful research must also examine formative outcomes that are closer to the practices being studied to better understand when and how practices are influ - encing those outcomes . The importance of more direct connections between the topics of the research and the people most directly involved in the research highlights issues of agency and 136 Review of Research in Education , 45 power . When those who decide which questions to ask in the research are too distant from those who may act on or be acted on by the findings of the research , they risk concentrating unhelpfully on detailing the problem without considering solutions , otherizing those who are being studied , and presuming individual rather than under - lying systemic factors ( Chicago Beyond , 2019 ; Doucet , 2019 ; Kirkland , 2019 ; Tuck & Yang , 2014 ) . theoretical Credibility The framework’s second dimension focuses on theoretical credibility . High - quality research should be anchored in a coherent theory that explains how a given set of conditions and actions influence the outcome of interest . By providing summary principles and understandable explanations , theoretical credibility enables greater use of research through conceptual understanding , which reinforces the associated actions . Such explanation is not merely valuable for the usability of the research but is essential for scientific rigor . Observations require explanations to distinguish the incidental from the influential , to connect disparate findings , to guide predictions , and to permit falsification . Usability of Research With Explanatory Principles One perspective on the benefits of the conceptual use of research comes from the research users themselves . Practitioners and administrators report valuing evidence that offers insight into student reasoning ( Coburn & Talbert , 2006 ) , practices that change thinking ( Palinkas et al . , 2011 ) , and books and articles that provide concep - tual frameworks ( Penuel , Farrell , et al . , 2016 ) . The literature on research utilization further underscores the importance of the conceptual use of research , suggesting that it may be more durable and perhaps more essential than instrumental use alone . As “replication of research findings more often proceeds in terms of applying generic principles rather than prescribed practices” ( Nutley et al . , 2003 , p . 131 ) , providing these principles may facilitate greater research use . This combination mirrors cogni - tive science research demonstrating the benefits of learning principles alongside examples ( e . g . , VanLehn , 1996 ) and balancing conceptual with procedural knowl - edge ( e . g . , Rittle - Johnson et al . , 2001 ) . Research use may also be viewed in terms of the cognitive processes of assimilating and accommodating new knowledge with existing knowledge ( DesForges , 2001 ; Piaget , 1952 ; Vygotsky , 1978 ) , sense - making ( Spillane et al . , 2002 ) , or conceptual change ( e . g . , Posner et al . , 1982 ) . Particularly as practitioners may need to “tinker” to adapt research in practice ( Hargreaves , 1998 ) , trade - offs ( Green et al . , 2009 ) and risks when adapting “ambi - tious and demanding innovations” ( Berman & McLaughlin , 1978 , p . 25 ) further highlight the need for conceptual understanding of research findings . Continuous improvement approaches demand testing and revising a well - specified theory of improvement that articulates concrete predictions about how changes in practices will affect outcomes ( Bennett & Provost , 2015 ; Deming , 1994 ) . Emphasizing Ming , Goldenberg : Research Worth Using 137 theoretical explanations facilitates greater conceptual and instrumental use of research , decreasing the risk that practitioners and policymakers may inadvertently permit or promote modifications that undermine effectiveness . One critical challenge when foregrounding conceptual use of research is consider - ing whose conceptual understanding to prioritize . Given the primacy of key decision makers in setting policies and allocating resources , capturing their attention and influencing their thinking is understandably important . Yet privileging policymakers’ interests over the concerns of practitioners tasked with implementing those policies risks neglecting practitioners’ experiences , knowledge , and local conditions that shape actual practices . Even if they are nominally supportive of a new policy or reform initiative , how practitioners make sense of it determines the alignment between its design and its execution ( e . g . , D . K . Cohen , 1990 ) . Considering the perspectives of a diversity of practitioners and other community members can further enrich the theory by accounting for contextual factors that may interact with an intervention’s effect . Scientific Rigor of Causal Modeling Science advances through theory development , not the mere accumulation of data . Evidence is valuable only insofar as it elucidates theory , whether through build - ing or testing theories . Causal models bridge the gap between the observed world and the counterfactual world by constructing testable theories that can predict the out - comes of as - yet - unobserved situations ( Pearl & MacKenzie , 2018 ) . Such models leverage existing knowledge to constrain the search for evidence , increasing the power of discovery using fewer data . As Cowen ( 2019 ) argues , “Without theoretical guidance , the possible combination of contributing factors is infinite” ( p . 83 ) . The value of this guidance becomes especially apparent when seek - ing to apply a lesson to a new setting . Much like knowledge transfer , policy transfer requires identifying the fundamental conditions and components , or “causal roles and support factors , ” under which the claim holds true ( Cartwright & Hardie , 2012 , p . 49 ) . Deaton ( 2019 ) highlights the value of specifying causal models “with careful attention to mechanisms” that delineate “which parts of the structure could or could not be estimated from data” before testing them empirically ( p . 39 ) . Theory restricts the problem space to explore . Without causal explanations , mere descriptions of correlations risk attributing causes to the wrong factors , misdirecting attention away from the actual levers for change that address underlying causes . This becomes especially problematic if the careless observer fails to recognize systemic barriers , interpersonal microaggressions ( Sue et al . , 2007 ) , or other mechanisms that are left invisible in the exploration . Kirkland ( 2019 ) objects to the overly simplified data narrative which “tends to locate a problem in the individual—usually Black or Brown—who lacks something , ” instead of investigating deeper structural causes ( p . 5 ) . Similarly , Irons ( 2019 ) urges researchers to undertake theoretically and empirically grounded investigations of the root causes of inequality before formulating potential strategies to address them . 138 Review of Research in Education , 45 These critiques raise important questions about whose perspectives are reflected in the theorizing about potential causal explanations . Just as practitioners and leaders in different contexts and roles will highlight different factors , researchers from different backgrounds—disciplinary and otherwise—will attend to different phenomena in the causal explanations they propose . These insights reveal not only how essential theory development is for science but also how it accelerates its progress by guiding evidence collection , reducing data needs , and exposing assumptions and beliefs for analysis and intervention . Causal explanations enable wider application of knowledge to different contexts , as well as a more complete understanding of the world with greater potential for resolving ineq - uities , depending on how deeply those causes are interrogated . Whether descriptive or inferential , research should clearly articulate the causal model under exploration by delineating which factors matter , which factors may vary , and the range of condi - tions under which the theory applies , including explicit comparisons of how well competing theories explain the evidence . How to engage in valid theory development given the evidence is the subject of the next section . Methodological Credibility The third dimension examines methodological credibility , where debates have his - torically privileged academic over practical concerns , internal over external validity , experimental over descriptive methods , and quantitative over qualitative methods . By constraining what is studied , learned , trusted , and used , narrow definitions of what counts as “rigorous” lead education research to overlook important phenomena , set - tings , and populations , and to exclude the experiences of the multiply marginalized . We argue that methodological credibility must include a broader range of methods , refine standards for statistical inference and practical significance , and raise expecta - tions for evaluating the applicability of research in practice . Multiple Perspectives , Methodologies , and Disciplines Complementary methods enrich our understanding of both problem and solu - tion . In categorizing research questions as descriptive , predictive , or causal , Singer ( 2019 ) highlights the distinct contributions of research exploring each type of ques - tion . Qualitative research “is essential if causal analysis is to succeed . A logically and empirically prior question to ‘Did it work ? ’ is ‘What was the “it” ? ’—‘What was the “treatment” as actually delivered ? ’” ( Erickson & Gutierrez , 2002 , p . 21 ) . Integrating qualitative and quantitative methods also supports formulating hypotheses before testing them , developing a fuller picture of the phenomenon , explaining outliers , guiding participant selection , and validating findings ( e . g . , Creswell & Clark , 2017 ; Kelle , 2006 ; Onwuegbuzie & Leech , 2005 ; Wiggins , 2009 ) . Transparency in methodological decisions , often described as reflexivity , reveals their impact on what investigators choose to study and how ( G . M . Russell & Kelly , 2002 ; Watt , 2007 ; Wiggins , 2009 ) . Such critical inquiry may be especially valuable Ming , Goldenberg : Research Worth Using 139 for quantitative researchers in unearthing nuance in the data and their implications for action ( Doucet , 2019 ; Gillborn et al . , 2018 ) . As outsiders , researchers risk impos - ing their values on the systems and people they study through their decisions about how to collect , analyze , and interpret data on others’ experiences and perspectives . What researchers do not say may be just as consequential as what they do say , insofar as omissions may function as a form of erasure . Internal Validity , Credibility , and Confirmability Criteria for internal validity or credibility are essential for addressing method - ological questions about our confidence in the findings of a particular study , as implementing inaccurate knowledge in practice may carry great costs . Improving internal credibility is especially critical to build research on minoritized populations , explicate causal mechanisms , and evaluate practical significance . Practitioners’ perspectives . Interviews of district leaders indicate that they value many of the same dimensions of internal validity espoused by researchers , such as experimental design , replicability , journal reputation , and statistical significance ( Coburn & Talbert , 2006 ) . However , they also revealed less understanding of selec - tion bias and interpreting case studies , as well as ambivalence about the validity of education research ( Penuel , Briggs , et al . , 2016 ) . Whether due to limited research literacy or a sophisticated understanding of how bias may influence findings , this highlights the importance for research to uphold scrupulous methodological stan - dards to be worthy of practitioners’ trust . Qualitative methods . The above findings hint at the dominance of quantita - tive methods and underscore the need for clearer standards of qualitative method - ological credibility for educators to find it useful . Among the multiple frameworks available , dimensions overlap while constructs and terms proliferate ( e . g . , Cre - swell , 1998 ; Hammersley , 1990 ; Lincoln & Guba , 1985 ; Maxwell , 1992 ; Miles & Huberman , 1994 ; Winter , 2000 ) . Themes include purposeful sampling to guide data gathering , transparency in analysis , and techniques for checking bias in inter - pretation . Distilling key principles may help users of qualitative research take bet - ter advantage of it , especially important given its role in addressing questions left unanswered by other methods . Quantitative methods . Beyond mere academic debates , the underappreciated limitations of quantitative methods distort their influence , overemphasizing ques - tions of “whether” at the expense of “for whom , ” “how , ” and “how much . ” The quantitative methods that are often championed are not well suited to studying smaller populations or complex phenomena in natural settings . Even more substan - tive concerns encompass study design 1 as well as analytical techniques 2 that may yield misleading inferences . Criticisms over the misuse of thresholds for p values 3 140 Review of Research in Education , 45 challenge their function as gatekeepers of knowledge . If they are set without regard to considerations for practical questions and concerns about equity , they risk reproduc - ing the existing academic hierarchies that privilege certain discoveries at the expense of others . Related criticisms of effect sizes 4 reveal their limitations in measuring practical significance , as well as their consequences in distorting research design . 5 Despite ongoing debates over appropriate thresholds and metrics for quantifying effect , interpreting magnitudes remains unresolved ( e . g . , Kraft , 2020 ; Pogrow , 2019 ; Valentine et al . , 2019 ) . Without reliable statistical guidance , researchers and practitioners cannot trust these metrics for prioritizing which phenomena are most impactful to study and to apply . External Validity and Transferability While internal validity captures confidence in the study’s conclusion about what happened within a given setting , external validity explores our expectation of it hap - pening beyond that setting . When consulting past research to guide their work , prac - titioners often consider population similarity ( Coburn & Talbert , 2006 ; Tseng , 2012 ) along with detailed descriptions and guidelines about implementation ( Penuel , Farrell , et al . , 2016 ) . Insufficient attention to these factors means that research lacks critical information about contexts , phenomena , and populations , whether through failure to study them or to report relevant detail . Conceptualizing external validity . As Wiliam ( 2019 ) succinctly stated , “Research tells us only what was , not what might be” ( p . 132 ) , highlighting the tension between “what worked” and “what will work . ” 6 Yet the premise of evidence - based practice demands extrapolating from past inferences to make predictions about future con - texts , 7 whether framed as universal generalizability ( D . T . Campbell & Stanley , 1963 ) or as contextualized transferability or applicability ( Cartwright & Hardie , 2017 ; Joyce & Cartwright , 2019 ; Lincoln & Guba , 1985 ; Miles & Huberman , 1994 ) . Since people more readily transfer knowledge across similar contexts ( e . g . , Barnett & Ceci , 2002 ; Detterman , 1993 ) , what deserves further study is determining the extent of similarity necessary for a given finding to successfully transfer in both conceptual use—where educators recognize the study setting as resembling their context—and instrumental use—where core features are sufficiently similar to justify legitimate applicability . Findings that cannot be applied to new populations and settings risk further magnifying the existing biases of convenience sampling and centering domi - nant experiences . Variability . Rather than averaging results across a larger sample to yield a gen - eral claim , research needs to detect interactions between factors that vary within the sample ( Bryk , 2017 ) . To increase the range and representativeness of variation that emerges , study designs should go beyond controlled , “black - box” experiments to include observations and interventions in naturalistic settings . For example , rapid cycle improvement ( Alemi et al . , 1998 ; Brown & Hare , 2002 ) can introduce sequential Ming , Goldenberg : Research Worth Using 141 changes to test and update hypotheses more nimbly , across a greater range of inter - acting factors . Naturalistic observations allow for exploring processes and discovering potential mechanisms with more nuance . By anticipating that implementation will vary as practitioners adapt processes , approaches such as design research ( Anderson & Shattuck , 2012 ; Penuel et al . , 2011 ) , implementation science ( Barrett , 2004 ; Cen - tury & Cassata , 2016 ; Fixsen et al . , 2005 ) , and improvement methods ( Bryk et al . , 2015 ) can detail those changes and their effects . Researchers should also incorporate analytical methods to explore the variation in these factors systematically , both individually and in combination . Possible approaches include microgenetic methods that trace developing changes over time ( Siegler & Crowley , 1991 ) , time - series modeling to explain subtle yet significant shifts ( Alwan & Roberts , 1988 ; Box & Jenkins , 1970 ) , and linear and nonlinear factor analysis to discover interactions among different mixtures of causes ( Roweis & Ghahramani , 1999 ) . From the quality improvement literature , statistical process control highlights the importance of closely monitoring multiple types of variation across multiple dimensions , particularly over time , across subgroups , and across contexts ( Deming , 1982 ; Juran , 1951 ; Provost & Murray , 2011 ; Shewhart , 1931 ) . Advantages include increased statistical power ( Diaz & Neuhauser , 2005 ) and rapid monitoring of mul - tiple sequential interventions . Although statistical process control is not well - known among researchers ( Fretheim & Tomic , 2015 ) , it is prevalent in health care practice ( e . g . , Benneyan et al . , 2003 ; Thor et al . , 2007 ) and can also be applied in education ( e . g . , Ming & Kennedy , 2020 ) . These methods are more sensitive for discovering nuance in effects across changing conditions or for small groups . Dimensions of applicability . Understanding the range of applicability demands understanding “what works , under what conditions , and for whom” ( Gutiérrez & Penuel , 2014 , p . 22 ) , as well as “why” ( Cowen , 2019 ) . The question of “what” is being studied includes not just the specific intervention but the underlying phenom - enon . “Under what conditions” describes the context or setting in which the prac - tice or phenomenon exhibits the impact observed . “For whom” comprises all those affected by the new practice , whether as providers or recipients , whether as teachers or leaders . “Why” circumscribes the expected bounds on all of these factors com - bined . Neglecting these dimensions hinders research from being usefully applied . Phenomena . Interventions depend on setting , staff , and time , which may change from unintended modifications ( D . K . Cohen , 1990 ; Dobson & Cook , 1980 ; O’Donnell , 2008 ; Penuel & Means , 2004 ) , through local adaptations ( Coburn , 2003 ; LeMahieu , 2011 ; J . L . Russell et al . , 2017 ) , or on institutionalization and scal - ing up ( D . A . Chambers et al . , 2013 ; D . K . Cohen & Moffitt , 2009 ; Green et al . , 2009 ) . Implementation science 8 and process evaluations 9 monitor multiple relevant dimensions for guiding practitioners in adapting successful practices . However , not all phenomena are comparably well studied across different methods . 10 For example , out - of - school factors , systems - level changes , and policy decisions do not lend themselves to 142 Review of Research in Education , 45 experimental designs and are insufficiently studied ( Michener , 2020 ; Scott , 2020 ) . By failing to explore alternate causes and explanations for both observed and possible outcomes , researchers neglect important phenomena that may exert greater impact . Conditions . The conditions influencing the application of research may be cat - egorized as structures , supports , and derailers ( Munro et al . , 2016 ) , or as drivers and restrainers ( Lewin , 1951 ; Thomas , 1985 ) . 11 Failing to account for these factors may yield unrealistic prescriptions when real - world conditions cannot match intensively resourced , carefully controlled study conditions . Furthermore , focusing narrowly on interventions while overlooking system - level supports may ignore potentially more powerful policy levers . Populations . Potential study populations receive unequal attention , whether due to convenience of access to “WEIRD” ( Western , Educated , Industrialized , Rich , Democratic ) participants ( Henrich et al . , 2010 ) or neglect of nondominant groups ( e . g . , Kim et al . , 2010 ) . The actual study sample may systematically differ from the intended study sample , depending on recruitment and selection procedures , partici - pation rates , and attrition patterns ( Gorard , 2006 ; Green et al . , 2009 ) . In parallel , intervention staff and other supporting adults receive less attention , either due to incomplete description of roles or due to a tendency to focus on teachers and princi - pals rather than on paraprofessionals or central office staff ( Green et al . , 2009 ; Honig & Venkateswaran , 2012 ) . These disproportionalities in study populations result in research that does not represent all . Explanations . Theoretical guidance is invaluable for identifying which factors are causally related to the phenomenon and outcome , including population character - istics , environmental factors , and alternate pathways . Researchers must then collect data to explore boundary conditions and counterfactuals to adequately test their theories and discover their limitations . From a continuous improvement perspective , some of this may be handled through monitoring balancing measures ( Institute for Healthcare Improvement , n . d . ; Solberg et al . , 1997 ) . Beyond what is already antici - pated , capturing data on other unexpected conditions and outcomes may also yield serendipitous discoveries . Summary These multiple dimensions of methodological credibility underscore their critical consequences on determining what is discovered and what remains hidden through the research process . Prevailing academic norms favor designs and techniques that translate poorly to messy , real - world contexts , neglecting nuance in both mechanisms and systems . These oversimplify what research is able to recommend to address the complexities of inequities in schools . Ming , Goldenberg : Research Worth Using 143 Evidentiary Credibility The framework’s fourth dimension addresses the importance of what we are call - ing evidentiary credibility . While research institutions incentivize novelty , policy - makers need consistency . Policymaker , practitioner , and researcher recommendations across multiple fields highlight the importance of robust research , particularly sys - tematic reviews and evidence syntheses ( Oliver & Cairney , 2019 ) . Interviews of dis - trict leaders reflect similar values for research “having a robust theoretical base , carried out by multiple investigators over a long period of time , and contributing to conver - gence around key findings or trends” ( Coburn & Talbert , 2006 , p . 480 ) . Leaders also expressed appreciation for replication and multiple measures . Research reviews offer promise for efficiently and reliably condensing a body of research into actionable insights , especially valuable when the supply of information exceeds decision - makers’ “bandwidth” ( Topp et al . , 2018 ) . While evidence registries compile lists of studies meeting various criteria for quality , systematic reviews provide more powerful evidence , and sometimes different conclusions , compared with indi - vidual studies ( Lavis et al . , 2003 ; B . Levin , 2008 ) . Syntheses help to guard against the outsized influence of any individual study , researcher , or institution , correcting for outlier findings and enabling more equitable research use processes . Purposes and Types of Synthesis Evidence syntheses range from systematic reviews , evidence portals , and policy briefs , to other review - derived products , where the degree and scope of synthesis that is most useful depends on both the context and the purpose of its use ( Caird et al . , 2015 ; White , 2019 ) . Different purposes may be to highlight information relevant to decisions , to identify gaps in the policy questions that the research literature has addressed , or to illuminate problems and policy options ( Lavis , 2009 ) . Pragmatic concerns include the timescale needed , the user’s capacity to interpret the results , and the availability of support to guide interpretation . Breadth of Search Which sources to consider reviewing for the synthesis depends on the question posed . While large - scale evidence registries often focus on formal program evalua - tions ( Westbrook et al . , 2017 ) , others highlight the benefits of evaluating interven - tion practices as well as nonexperimental studies ( Means et al . , 2015 ) . Table 1 summarizes some examples of the policymaking questions addressed by a range of study types ( Lavis , 2009 ) . The relative prominence of program evaluation registries suggests a need for more accessible synthesis of a broader range of evidence types to inform other policymaking questions beyond just effectiveness . In addition to study type and publication source , parameters such as the data - bases , time frames , and search terms influence the evidence considered for review ( Alexander , 2020 ) . Prespecification may help balance dominant voices with a wider range of sources . Tools such as AMSTAR 2 ( A MeaSurement Tool to Assess 144 Review of Research in Education , 45 systematic Reviews ) delineate multiple criteria for evaluating systematic reviews of randomized controlled trials ( RCTs ) and observational studies ( Shea et al . , 2017 ) . Search tools for qualitative evidence synthesis highlight parameters such as the Popu - lation , Intervention , Comparison , Outcomes , and Study design ( PICOS ) , or Sample , Phenomenon of Interest , Design , Evaluation , and Research type ( SPIDER ; Cooke et al . , 2012 ; Methley et al . , 2014 ) . Some approaches to synthesis ask , “Which stake - holder perspectives [ are ] relevant ? ” ( Caird et al . , 2015 , p . 83 ) or explicitly endorse principles of inclusivity : “Considers many types and sources of evidence ; Uses a range of skills and people” ( Donnelly et al . , 2018 , p . 362 ) . Syntheses with narrower restric - tions on the range of publications , disciplines , or methodologies that they consider may be more limited in the inferences they draw from the evidence . By omitting nontraditional publication sources or studies from different disciplinary or method - ological perspectives , such syntheses may be overlooking valuable information of interest to practitioners and policymakers . Criteria for Selection Inclusion and exclusion criteria differ in both comprehensiveness and stringency . Definitions of robustness vary , both quantitatively and qualitatively . Some set a mini - mum number of replications required or restrict studies based on peer review or evaluator independence ( Zack et al . , 2019 ) , while others advocate seeking expert consensus rather than quantity ( Sutherland , 2013 ) . Additional methodological con - cerns address study designs , sample size requirements , study execution , and outcomes tablE 1 summary of Examples of types of systematic Reviews and the Policymaking Questions they address Study Types Examples of Policymaking Questions Addressed Observational studies • Establish the magnitude of the problem ( or its contributing factors ) • Characterize the harms and key elements of policy and program options for addressing the problem • Identify potential barriers to implementing a preferred option Qualitative studies • Identify alternative framings of the problem • Understand how or why a policy or program option works ( particularly if local adaptation is being considered ) • Appreciate stakeholders’ views about and experiences with particular options • Identify potential barriers to implementing a preferred option Effectiveness studies • Characterize the benefits and sometimes the harms of each option being considered Economic evaluations • Characterize the cost - effectiveness or cost - benefits of options Source . Adapted from Lavis ( 2009 , Table 1 ) . Ming , Goldenberg : Research Worth Using 145 of interest , among many other features ( Walker et al . , 2017 ; Westbrook et al . , 2017 ) . Critiques of studies relying on measures that are overaligned to the intervention ( Wiliam , 2019 ) echo cognitive scientists’ recommendations to assess learning through long - term retention and far transfer ( e . g . , Schmidt & Bjork , 1992 ) . Some scholars argue for greater flexibility or different criteria beyond method - ological constraints . These include selecting source studies “on the basis of theory and relevance and not just technical conformity” ( Wrigley & McCusker , 2019 , p . 123 ) , weighing the influence of context ( Donaldson et al . , 2009 ) , and considering ethics and impartiality ( Gambrill , 2006 ) . Syntheses vary in the thresholds and weights they use for criteria for internal and external validity . As Means et al . ( 2015 ) note , “Different values are at play in the world of evidence - based programs , so different users will value different types , qualities , and standards of evidence” ( p . 107 ) . Kirkland ( 2019 ) cautions against presuming a false dichotomy between method - ological rigor and equity , urging for “an integration of strengths” to better interrupt inequities ( p . 7 ) . We believe that evidence synthesis can maintain high method - ological standards while broadening inclusion criteria , by strengthening the focus on theory and relevance to guide the synthesis . Methods and Dimensions for Synthesis With the essence of synthesis depending on how to discover and illuminate pat - terns across multiple studies , a critical starting point is identifying the study charac - teristics and outcomes that are meaningful to the guiding question ( Alexander , 2020 ) . Again , we emphasize the importance of describing the theoretical frameworks , causal mechanisms , or logic models that informed the design of the studies being synthe - sized , as those shape the inferences being drawn from the evidence . Determining what signals a reliable trend or theme when integrating the results across multiple studies constitutes another key decision in the synthesis . Meta - analyses offer a quantitative approach for aggregating effect sizes across studies ( Glass , 1976 ; Hedges & Olkin , 2014 ; Pigott & Polanin , 2020 ) . Methodological concerns include the improper handling of heterogeneity in some meta - analytic approaches , which combine effects across studies of substantively different designs , populations , interventions , and measures ( Bergeron & Rivard , 2017 ; Kvarven et al . , 2019 ; Wiliam , 2019 ; Wrigley & McCusker , 2019 ) . Other guidelines are available for comparing quantitative effects when meta - analysis is not possible ( M . Campbell et al . , 2020 ) , and for narrative synthesis of effectiveness studies and implementation studies ( Popay et al . , 2006 ) . Critical considerations here inherit many of the same concerns raised for theo - retical and methodological credibility . Without reliable methods for synthesizing theory , evidence syntheses instead elevate the technical details of individual studies over the features of the theories being tested . Inclusion and exclusion criteria tend to reproduce the academic hierarchy that privileges academic journals over practitio - ner - oriented publications and quantitative methods over qualitative methods . Synthesizing findings that apply to minoritized populations requires explicitly 146 Review of Research in Education , 45 coding and extracting that information from the research , but aggregating small groups across disparate studies remains challenging . Relevance of answers : Justification for Practical application While the first dimension of the framework addresses the relevance of the ques - tion posed for the research , the fifth dimension examines the relevance of the answers . In contrast to external validity , which explores whether a research finding can be legitimately applied to a particular context , this dimension focuses on whether the research should be applied , given implementation constraints , the values of those affected , long - term goals , and available resources . Taking action demands sufficient justification with not only actionable but also action - worthy implications . While these are ultimately value judgments , high - quality evidence should inform these judgments . The key questions here address for whom the answer is relevant , how to apply it , how much its adoption would affect practices and outcomes , and how to support its continuation . For Whom Whether it is justifiable to take action based on the research findings depends not just on the action but also on who would take action and who would benefit . This carries implications for the timing and detail needed for the former to act on the research , and for questions regarding how to balance the benefits and burdens borne by different people . Research evidence may influence decision makers as they weigh the concerns of implementers and multiple potential beneficiaries . How to Apply For any research - informed policy , understanding how it may play out in practice is necessary before expecting leaders and practitioners to implement it . As discussed in the external credibility section , rich detail about past implementation in varied contexts can guide discussions of potential applicability of the research to the present context . Yet extrapolating to future contexts with enough confidence to justify action may still require additional evidence about what implementation will entail across multiple levels and contexts and how key stakeholders are likely to adapt . This may include more locally grounded projections , small - scale pilots , participatory engage - ment , or iterative cycles of design and improvement ( e . g . , Bryk et al . , 2015 ; Kemmis et al . , 2014 ; Penuel et al . , 2011 ; Shalowitz et al . , 2009 ; Sharples et al . , 2019 ) . How Much To evaluate the inevitable trade - offs in any decision , policymakers must weigh the benefits compared with the costs of its implementation ( Duncan & Magnuson , 2007 ; Harris , 2009 ) . This section elaborates on what information should be included in such calculations . Ming , Goldenberg : Research Worth Using 147 Impacts . To justify its consideration for informing policy or practice , relevant research evidence must define and measure its impact on outcomes important to the range of stakeholders affected ( Green et al . , 2009 ; Topp et al . , 2018 ) . Impact may be measured as monetary benefit ( Mishan , 1976 ) , effectiveness ( H . M . Levin , 1995 , 2001 ; H . M . Levin & Belfield , 2015 ) , or utility , as perceived by various decision makers and stakeholders ( Hollands et al . , 2019 ) . Impact may alternately be mea - sured with respect to a needs assessment ( Watkins et al . , 2012 ) , with improvements addressing more urgent needs receiving higher utility values ( Ross , 2008 ) . Quantify - ing impact should consider its extent , durability , and distribution ( H . M . Levin & McEwan , 2002 , 2003 ) . Extent of impact encompasses benefits accumulated across the population and over time . Durability may vary by participant age ( Duncan & Magnuson , 2007 ) and change over time due to decay , compounding , and future discounting ( Harris , 2009 ) . Variation in who benefits the most raises questions about how to value equity , especially when calculating this variation may itself be costly ( Duncan & Magnuson , 2007 ; Gamoran , 2015 ; Iatarola & Stiefel , 2003 ; Topp et al . , 2018 ) . Finally , as with effect size , calculations of impact are measured relative to a counterfactual , so researchers and decision makers must ensure consistency in defin - ing the alternative . Costs . Relevant evidence must evaluate the policy’s costs . As noted by one decision maker , “If I can’t afford it , then there is no point of going further” ( Palinkas et al . , 2017 , p . 245 ) . Costs may include resources such as personnel , facilities , equipment , materials , donations , volunteer time , and other resources , whether identified by the ingredients method ( H . M . Levin , 1975 , 1983 ) or the resource cost model approach ( J . G . Chambers , 1999 ) . Just as impacts must be measured relative to alternatives , costs must also be measured relative to the relevant policy alternatives ( Conaway & Goldhaber , 2018 ; Rice , 1997 ) . Equity concerns again demand considering the distri - bution of costs borne by recipients and intervenors . A different set of hidden costs may comprise side effects , induced costs , adverse consequences , or other possible harms ( Brick & Freeman , 2019 ; Green et al . , 2009 ; Zhao , 2017 ) . Some possibilities include psychological constructs such as motivation , creativity , learning orientation , self - efficacy , or other socioemotional learning compo - nents ( Zhao , 2017 ) . Additional harms might include stigma associated with an inter - vention , reinforcing deficit mindsets , errors in identifying students , lost opportunities due to misaligned or delayed services , or other disadvantages and inequities ( Chicago Beyond , 2019 ; Hammond , 2014 ; Hughes & Dexter , 2011 ; McKenzie & Phillips , 2016 ; Murawski & Hughes , 2009 ) . Reconciling impacts and costs . Determining whether a policy is worth undertaking demands careful modeling of the probabilities and trade - offs between impacts and costs across different policy options ( Basbøll , 2018 ; Sutherland & Burgman , 2015 ; Topp et al . , 2018 ) . Such analyses of benefits and harms provide essential evidence for evaluating policies . Without this feedback , decision makers who are disconnected 148 Review of Research in Education , 45 from the populations they serve may risk causing further harm . This raises critical questions about the gaps in knowledge and power between who evaluates the trade - offs , who sets the thresholds for risks and benefits , and who subsequently bears the burden . How to Support Finally , decision makers may also require evidence about the organizational and system - level supports necessary for the intervention’s continued success , which may be framed as sustainability , spread , and scale . One conceptualization of scale from Coburn ( 2003 ) outlines four dimensions : depth , encompassing the nature and qual - ity of implementation , as well as beliefs , norms , and principles guiding implementa - tion ; sustainability , addressing reform coherence and postreform durability ; spread of those beliefs , norms , and principles through policies and practices at multiple levels within the organization ; and shift in reform ownership , where essential knowledge and leadership for reform are internalized within the system . The health care improve - ment literature delineates in more detail the drivers of sustainability ( Scoville et al . , 2016 ) , a framework for spread ( Massoud et al . , 2006 ) , and supports for scale ( McCannon et al . , 2008 ) , some of which has begun to be adopted in education ( e . g . , Sherer et al . , 2019 ) . Perhaps most fundamentally , introducing any change into a complex system will trigger a web of other effects ( Meadows , 2008 ) . Neglecting to specify these supports , barriers , and interactions hinders decision makers from fairly judging whether and how to apply a research finding in their contexts . Lack of atten - tion and planning around these supports may create friction , churn , or reform fatigue among underserved populations for whom various initiatives come and go , but sel - dom endure . lookinG toWaRD thE FUtURE This chapter advances a perspective on improving research quality for useful action . In this section , two use cases illustrate practical applications of the five dimen - sions . Then , we suggest how the framework might be helpful in reimagining research production , curation , and utilization and lay out some next steps for developing and applying the framework . application to Practice We share a brief but powerful use case—teaching children to read—to illustrate the potential power of the Research Worth Using framework . The relevance of the question is undeniable ; reading ability is fundamental to schooling and citizenship in the 21st century . Decades of research in psychology , linguistics , neuroscience , neuro - biology , and other fields have established robust theoretical and methodological credi - bility on how people learn to read . Comprehensive reviews by well - respected groups of literacy experts and academics have examined the evidence on reading , providing evidentiary credibility ( e . g . , Castles et al . , 2018 ; National Institute of Child Health Ming , Goldenberg : Research Worth Using 149 and Human Development , 2000 ; National Research Council , 1998 ; Rayner et al . , 2001 ) . In contrast , research on the details of implementation , such as materials , instructional sequencing , and teacher development , is scarcer , falling short in its jus - tification for practical application . More research is also needed on making reading instruction culturally responsive and addressing the needs of all students—for exam - ple , nontypically developing students , multilingual learners , and those without for - mal prekindergarten education . Most important , we need evidence on how to systematically integrate the strands of teaching and learning research . Another timely use case , as we write this chapter , is how education agencies should respond to the myriad challenges wrought by the COVID - 19 pandemic . Education leaders sought evidence to inform difficult decisions about instruction , health , safety , and well - being , with urgent questions about the rapid transition to emergency educa - tion , implementation of K – 12 remote learning , and balancing synchronous and asynchronous instruction . The limited amount of trustworthy research available in the early months of the pandemic lacked sufficient theoretical and practical specifica - tion about age levels , the interventions , and the context for reliable application . New policy briefs drawing from prior literature have since been published , which would have offered greater utility if available earlier when key decisions were being made . implications Based on the Research Worth Using framework , we offer implications for how to improve the quality of research production , curation , and utilization to benefit edu - cational practice and support more equitable outcomes , borrowing from B . Levin’s ( 2011 , 2013 ) model of these overlapping functions . Table 2 summarizes the implica - tions of the five framework dimensions for each function . Research Production The research production section addresses not only researchers but also funders and research institutions , whose policies incentivize or constrain the research that gets conducted . Since coproduction does not guarantee research quality and may even burden practitioners with unhelpful work , we highlight specific actions that the research community may undertake to address each dimension of the framework . To increase the relevance of the questions investigated , research should explore a wider range of phenomena with more immediate or impactful practical value , par - ticularly structural factors and mechanisms that maintain or dismantle inequity . Researchers should invest time in learning about the policies and priorities affect - ing the audiences they seek to help . This may entail studying applicable federal , state , and local policies , as well as reading practitioner publications on topics of interest . Building on this information gathering can then allow researchers to focus their interactions with stakeholders on soliciting input about their values , needs , problems of practice , policy opportunities , and decision - making timelines to guide the development of research projects . Connecting with practitioners across 150 Review of Research in Education , 45 multiple levels and roles in the education organization can further refine questions to yield more actionable and useful information . Deepening partnerships with stu - dents , families , and community members may further improve how well the research represents their perspectives , provided that such partnerships address power dynamics thoughtfully . To interrogate the theoretical credibility of their work , we encourage researchers to attend more closely to the explanatory coherence of the theories they are investigat - ing , to compare those to existing frameworks already influential in the local context , and to consider whose understandings are privileged . Greater disciplinary breadth may contribute to strengthening the research and its application toward achieving more equitable outcomes . For methodological credibility , we invite the research community to increase the visibility of standards for qualitative research validity and to develop clearer , more useful standards for practical significance , in collaboration with policymakers , prac - titioners , and research brokers . Doing so would enrich the range of knowledge gained , boost the likelihood of obtaining applicable results , and improve theory building by testing claims under a broader range of conditions . Some technical exam - ples include setting consistent expectations for balancing different error tolerances , learning from null results , and distinguishing clarity of study design from magnitude tablE 2 Research Worth Using Framework’s implications for Research Production , Curation , and Utilization Framework Dimension Production Curation Utilization Relevance of question Phenomena and populations linked to audience priorities Select for impact and organize around practical questions Develop learning agendas to drive evidence search Theoretical credibility Explanatory coherence with respect to influential frameworks Synthesize across theories using content expertise Seek explanatory principles and limits on theories Methodological credibility Practical significance External credibility Broader methods ( esp . qualitative ) ; external credibility Highlight risks of false negatives and inaction Evidentiary credibility Clarify consensus , not just novelty and debate Comprehensive search Clarify synthesis standards Multiple evidence sources , syntheses , confidence ratings Relevance of answer Implementation studiesCost - utility analyses More detail on context , implementation , cost Risks , opportunity costs , hidden costs , counterfactuals Ming , Goldenberg : Research Worth Using 151 of impact . More critical changes involve incorporating greater methodological breadth , encouraging deeper exploration of sources of variation and interactions , and magnifying attention to external validity in both the design and reporting of research to examine who is represented in the research and from whose perspectives . Strengthening evidentiary credibility requires researchers to examine the cumula - tive body of research , ideally discussing it with potential audiences before embarking on new studies , to determine where the field needs additional evidence to resolve important questions . We encourage researchers to clearly convey where there is gen - eral consensus while acknowledging the nuances of ongoing debate , where there is foundational knowledge but not application or implementation evidence , and what is contested . Adjusting academic and funding incentives to reward coherence rather than novelty may function as another key lever to build a comprehensive and useful evidence base for practical decision making . Justifying the application of research findings demands considerably greater evi - dence from cost - benefit / - effectiveness / - utility analyses , participatory stakeholder engagement , iterative design and improvement cycles , and implementation studies to support spread and scale across a range of audiences and contexts . Academia and funding agencies can support the field to move in this direction by illuminating both the intellectual and practical value of such investigations . Research Curation This section concerns journal reviewers and editors , evidence synthesis scholars , evidence brokers , funders , and providers of technical assistance such as the Regional Educational Laboratories . To increase the salience of the relevance of the question in guiding research and practice , we urge for more research curation to be organized around the questions that policymakers and practitioners pose , rather than the phenomena that researchers publish on or the programs that funders seek to evalu - ate . Strengthening the impact criteria for inclusion in evidence syntheses can also raise the bar for research that improves practice , so that studies must demonstrate meaningful impacts , not merely meet specifications for design quality ( see Taylor et al . , this volume ) . We advocate for boosting the importance of theoretical credibility in evidence syn - thesis by developing more explicit expectations and methods for synthesizing across theories and logic models . Consistently incorporating content expertise into this pro - cess would serve as an invaluable investment . The forms of methodological credibility adopted by evidence registries have tended to privilege quantitative over qualitative methodologies , both in the synthesis and in the studies synthesized , although qualitative synthesis methods have been growing ( e . g . , Claes et al . , 2015 ; Methley et al . , 2014 ) . We urge greater inclusion of research representing a broad range of methods , as well as greater attention to addressing issues of external validity and transferability of research in both publication and syn - thesis ( e . g . , Munthe - Kaas et al . , 2020 ) . This can raise the profile of available evidence on less - studied populations and phenomena , while also highlighting questions 152 Review of Research in Education , 45 needing further research . Establishing consistent criteria for study execution as well as design would further strengthen the evidence base . For consistent standards of evidentiary credibility , we seek a resolution of different perspectives among scholars in evidence synthesis and meta - analysis about reporting and interpreting effect sizes , as well as greater clarity about standards for synthesis ( e . g . , Slavin , 2018 ) . We also note that syntheses will be most valuable if they under - take a comprehensive search of the available evidence for potential inclusion , such as by including unpublished literature or soliciting nominations of studies submitted by external contributors . To meet the demand for evidence synthesis , we encourage the field to continue to pursue multiple forms of synthesis , including rapid evidence assessments , systematic reviews , meta - analyses , and reviews of reviews ( Lavis , 2009 ; White , 2019 ) . To better inform justification for practical application , funders and journal editors could require study authors to include more information on context , implementa - tion , and cost in their publications . We also suggest that evidence registries curate research along multiple dimensions with user - adjustable criteria to accommodate the parameters relevant to their decision - making priorities . Research Utilization Here , we address policymakers , practitioners , and other members of the public interested in using research to inform decisions , or brokers acting on their behalf , as well as those who influence legislation on mechanisms to encourage use of research evidence , such as the most recent reauthorization of U . S . federal education legisla - tion , the Every Student Succeeds Act ( 2015 ) . For demand to meaningfully shape the supply of high - quality research that addresses relevant questions , the marketplace of ideas needs a stronger signal from would - be research users about what they value . We encourage decision makers who consult research or engage in formal research col - laborations to develop learning agendas to guide their search and to share them with the research community where possible . Starting with high - priority goals and intended actions , these agendas would identify questions whose answers may help inform those decisions . If new research is needed to address gaps in the existing litera - ture , highlighting those limitations would be valuable to inform the broader field . To maximize their use of and expectations for theoretical credibility in research , policymakers and practitioners should seek principles that explain recommended practices , considering the perspectives of implementers and service users , not just decision makers . In examining the theories underlying these principles , they should expect researchers to articulate the limits of the theory , determining when the theory applies and when it would not apply , as well as acknowledging the conditions under which it has not yet been tested . Research users can play a valuable role in strengthening the methodological credi - bility of research . While this is most obvious for external validity , it also applies to internal validity , in highlighting the need to more heavily weight the risks of false negatives , not just false positives . Communicating this to researchers can help them Ming , Goldenberg : Research Worth Using 153 better understand practitioners’ urgent need to take action under conditions of uncertainty , thereby motivating research to reduce the uncertainty , while maintain - ing rigorous standards in how they report research . To yield more reliable decision - making guidance , consumers and brokers of research can take fuller advantage of evidentiary credibility by consulting literature reviews , research syntheses , and confidence ratings rather than relying on individual studies or favored researchers with a prominent platform . Similarly , research users can search evidence registries for reviews of specific programs , interventions , or curricula ( see examples in the Online Appendix in the online version of the journal ) . Legislation such as Every Student Succeeds Act ( 2015 ) could promote consulting syntheses , not just a handful of studies , to guide practice . Finally , representatives of practice should continually demand better evidence to justify the application of research to a given context . By requesting systematic analyses of risks , hidden costs , opportunity costs , and other balancing measures across multi - ple alternatives , both to implement and to sustain successful practices at scale , poli - cymakers can improve the quality of evidence for their own decision making as well as the standards for the field . new Directions Doucet ( 2019 ) argued that for research to be useful , its production needs to be reimagined . This chapter introduces a novel way to reimagine research quality , knit - ting together ideas from separate but related fields into a coherent framework that encompasses both rigor and relevance . We have attempted to reimagine how the field might define high - quality education research evidence for useful action . We consider this framework a draft and offer several suggestions to strengthen it . First , considerations regarding equity , power , and agency are loosely incorporated into the framework and deserve further development ( Table 3 ) . They provoke impor - tant questions , which we hope will catalyze conversations about the roles of research - ers , policymakers , and practitioners in the processes of producing and using research evidence ( see Wong , this volume , for a deeper dive into these issues ) . A second area of activity is to elaborate more concretely on the framework dimen - sions . The theoretical and methodological credibility dimensions are built on cri - tiques of research methodologies and definitions of validity , which here have been condensed to a few paragraphs . The evidentiary credibility section similarly con - denses approaches to reviewing , synthesizing , and curating research evidence . A related goal is to test the framework by applying it to a set of use cases across a range of domains and levels . Operationalizing the framework through a simplified rubric or set of guiding questions may facilitate testing and applying it in practice . Finally , and most important , we invite the researcher , policymaker , and practi - tioner communities to apply and refine the framework . All dimensions of the framework will benefit from further scrutiny and input , as it will only be helpful if it is used . 154 Review of Research in Education , 45 aCknoWlEDGMEnts We are deeply grateful to this volume’s editors , the anonymous reviewers , and Paula Arce - Trigatti , Kathleen Bradley , Cynthia Coburn , Fiona Hollands , Jonathan Kay , Jim Kemple , Alec Kennedy , Joy Lesnick , and Matthew Soldner for their gracious feedback and the invalu - able insights they shared on reading previous drafts of this chapter . oRCiD iD Norma C . Ming https : / / orcid . org / 0000 - 0002 - 7991 - 5929 notEs 1 Causal and correlational methods may suffer from flawed design and execution . Criticisms of RCTs include inability to blind educational interventions involving changes in teacher practice ( Wrigley & McCusker , 2019 ) ; failure to randomize on critical factors such as teacher quality ( Wiliam , 2019 ) or instructional time ( Ginsburg & Smith , 2016 ) ; confounds that emerge after randomization ( Cartwright , 2019 ; Schanzenbach , 2012 ) ; and artificial con - trols inducing relationships that do not exist ( Cowen , 2019 ) . Large - scale studies may overlook effects for small populations and neglect key information about mechanism and variability in implementation . These limitations expose the overinflated status of these designs in influenc - ing what is learned from research . 2 Criticisms of analytical techniques cite misconceptions around hypothesis testing and p values ( Wasserstein & Lazar , 2016 ) . One misconception is the classic error of confusing likeli - hoods with posterior probabilities ( Bayes , 1763 ; Box & Tiao , 1973 ; Gorard , 2010 ) . Rather than accepting the proposed model merely on rejecting the null hypothesis , researchers should engage in an explicit comparison between models to determine which provides a better expla - nation of the data . Correcting these errors would require researchers to estimate their prior beliefs about the probability of each model , or at least to explicitly discuss these assumptions and expectations ( McShane et al . , 2019 ) . Such advice to articulate one’s assumptions echoes tablE 3 Research Worth Using Framework and Critical Consideration Regarding Equity , Power , and agency Framework Dimension Critical Questions 1 . Relevance of question : Alignment of research topic to practical priorities Who knows ? Who decides ? Who acts ? 2 . Theoretical credibility : Explanatory strength and coherence of principles investigated Who needs to understand ? Whose understanding takes priority ? 3 . Methodological credibility : Internal and external credibility of study design and execution Who decides what’s credible ? As a result , what’s studied , learned , trusted , and used ? 4 . Evidentiary credibility : Robustness and consistency of cumulative evidence Who does the gatekeeping ? What’s lost in synthesis ? 5 . Relevance of answer : Justification for practical application Who benefits ? Who bears burdens and risks ? Who decides the risk - benefit calculus ? Ming , Goldenberg : Research Worth Using 155 qualitative researchers’ recommendations to examine reflexivity to reveal how investigator bias may shape conclusions . 3 Related criticisms over the misuse of p values pertain to their thresholds for determining statistical significance ( e . g . , Amrhein et al . , 2019 ; Wasserstein & Lazar , 2016 ) . Some propose different thresholds ( e . g . , Gelman & Robert , 2014 ; Ioannidis , 2019 ; Johnson , 2013 ; Mayo , 2019 ) , while others object to creating arbitrary thresholds ( Ziliak & McCloskey , 2008 ) or advocate learning from null results ( e . g . , Jacob et al . , 2019 ) . These debates are about not just wrongly excluding findings but also wrongly including findings , threatening confidence in research . 4 Although it is commonly interpreted as quantifying the magnitude of the phenom - enon when the null hypothesis was rejected , J . Cohen ( 1962 ) introduced effect size to address the reverse question : “Given an experimental effect in a population , how likely is the null hypothesis to be rejected ? ” ( 2018 , p . 145 ) . As Simpson explains ( 2017 , 2018 , 2019 , 2020 ) , effect size measures the clarity of the study design ( i . e . , how clear the difference between groups appeared on the measure used in the study ) , not the impact of the phenomenon being studied . Inverting the calculation to estimate the phenomenon’s magnitude requires knowledge of the study design and analysis , including sample size and heterogeneity , outcome measure , and differences between groups or conditions being compared ( Simpson , 2018 ; Kraft , 2020 ) . 5 Favoring designs that find a large average effect or contrast between conditions restricts what gets studied . These preferences incentivize researchers to study homogeneous rather than diverse populations and to compare interventions against an uninteresting con - trol condition . Such consequential decisions must not be determined by questionable ana - lytical standards . 6 Debates over the relationship between internal and external validity echo long - stand - ing discussions about deductive versus inductive reasoning ( Hume , 1748 / 1975 ; Popper , 1934 / 2005 ) , pertaining to the disconnect between describing the observed past and predict - ing the unobserved future . As explained by Cartwright ( 2019 ) , “Hume argued that there is no rational argument to infer knowledge of the unobserved from knowledge of the observed” ( p . 67 ) . 7 Quality improvement distinguishes between enumerative studies , which describe the current state , and analytical studies , which predict a future state . Per Provost’s ( 2011 ) analogy , enumerative studies examine water from an essentially static pond , while analytical studies examine water from a constantly moving river . Within an enumerative study , extrapolation is valid from a sample to the pond ( or to the frame sampled ) but not beyond . In an analytical study , prediction requires not just an assessment of how much the conditions vary from the study but also a theoretical explanation of how much that variation matters ( Deming , 1942 , 1953 , 1975 ; Provost , 2011 ) . 8 A synthesis of the implementation science literature highlights the importance of these factors : implementation stage ; implementation fidelity ; staff selection , training , coaching , and evaluation ; and organizational context ( Fixsen et al . , 2005 ) . Measuring fidelity may detect when an intervention shows no effect due to a failure to implement it as designed ( Dobson & Cook , 1980 ; O’Donnell , 2008 ; Yeaton & Sechrest , 1981 ) . Dimensions include structural fidelity ( e . g . , use of specific program materials and practices ) , process fidelity ( e . g . , more com - plex classroom behaviors and outcomes ) , and dosage fidelity ( e . g . , extent of training on the treatment ; Hill & Erickson , 2019 ) . 9 Process evaluations examine practices in the treatment and comparison conditions , resources and training , barriers , and unintended consequences ( Gorard et al . , 2017 ; Siddiqui et al . , 2018 ) . Other factors include quality , reach , responsiveness , program differentiation , monitoring of comparison groups , and adaptation ( Humphrey et al . , 2016 ) , depending on whether the intervention is being tested at the pilot , efficacy , or effectiveness stage . Additional 156 Review of Research in Education , 45 relevant factors include preplanning , support system , environment , implementer factors , and intervention characteristics . 10 For example , one critique of RCTs is their inability to study phenomena that cannot be randomized or ethically manipulated ( Joyce , 2019 ; Ravallion , 2018 ; G . C . S . Smith & Pell , 2003 ) . Even when included in an experiment , the phenomena investigated may not be representative . The expectation for carefully controlled RCTs that demand fidelity of imple - mentation inhibits studying real - world variation and discovering other influences or alternate explanations for the outcomes . 11 Structures may be social , economic , cultural , or physical , such as sociocultural contexts , family dynamics , community norms , school culture , and organizational factors ( Joyce & Cartwright , 2019 ; Phillips , 2019 ) . Supports or drivers may include factors such as human capital , technology resources , or computer literacy ( Joyce & Cartwright , 2019 ) . Derailers or restrainers may include interruptions in a necessary resource , offsetting due to other factors undermining the effects , and self - defeat by causing other harms ( Cartwright & Hardie , 2017 ) . REFEREnCEs Adelman , C . ( 1993 ) . Kurt Lewin and the origins of action research . Educational Action Research , 1 ( 1 ) , 7 – 24 . https : / / doi . org / 10 . 1080 / 0965079930010102 Alemi , F . , Moore , S . , Headrick , L . , Neuhauser , D . , Hekelman , F . , & Kizys , N . ( 1998 ) . Rapid improvement teams . Joint Commission Journal on Quality Improvement , 24 ( 3 ) , 119 – 129 . https : / / doi . org / 10 . 1016 / s1070 - 3241 ( 16 ) 30366 - 2 Alexander , P . A . ( 2020 ) . Methodological guidance paper : The art and science of qual - ity systematic reviews . Review of Educational Research , 90 ( 1 ) , 6 – 23 . https : / / doi . org / 10 . 3102 / 0034654319854352 Alwan , L . C . , & Roberts , H . V . ( 1988 ) . Time - series modeling for statistical process control . Journal of Business & Economic Statistics , 6 ( 1 ) , 87 – 95 . https : / / doi . org / 10 . 1080 / 0735001 5 . 1988 . 10509640 American Educational Research Association . ( 2006 ) . Standards for reporting on empirical social science research in AERA publications . Education Researcher , 35 ( 6 ) , 33 – 40 . https : / / doi . org / 10 . 3102 / 0013189X035006033 American Educational Research Association . ( 2009 ) . Standards for reporting on humanities - oriented research in AERA publications . Educational Researcher , 38 ( 6 ) , 481 – 486 . https : / / doi . org / 10 . 3102 / 0013189X09341833 Amrhein , V . , Greenland , S . , & McShane , B . ( 2019 ) . Scientists rise up against statistical sig - nificance . Nature , 567 , 305 – 307 . https : / / doi . org / 10 . 1038 / d41586 - 019 - 00857 - 9 Anderson , T . , & Shattuck , J . ( 2012 ) . Design - based research : A decade of prog - ress in education research ? Educational Researcher , 41 ( 1 ) , 16 – 25 . https : / / doi . org / 10 . 3102 / 0013189X11428813 Arce - Trigatti , P . , Chukhray , I . , & Turley , R . N . L . ( 2018 ) . Research – practice partnerships in education . In B . Schneider ( Ed . ) , Handbook of the sociology of education in the 21st century ( pp . 561 – 579 ) . Springer . Barnett , S . M . , & Ceci , S . J . ( 2002 ) . When and where do we apply what we learn ? A taxonomy for far transfer . Psychological Bulletin , 128 ( 4 ) , 612 – 637 . https : / / doi . org / 10 . 1037 / 0033 - 2909 . 128 . 4 . 612 Barrett , S . M . ( 2004 ) . Implementation studies : Time for a revival ? Personal reflections on 20 years of implementation studies . Public Administration , 82 ( 2 ) , 249 – 262 . https : / / doi . org / 10 . 1111 / j . 0033 - 3298 . 2004 . 00393 . x Basbøll , T . ( July 30 , 2018 ) . We need our scientists to build models that frame our policies , not to tell stories that shape them . LSE ( London School of Economics ) Impact Blog . https : / / Ming , Goldenberg : Research Worth Using 157 blogs . lse . ac . uk / impactofsocialsciences / 2018 / 07 / 30 / we - need - our - scientists - to - build - mod - els - that - frame - our - policies - not - to - tell - stories - that - shape - them / Bayes , M . ( 1763 ) . An essay towards solving a problem in the doctrine of chances . By the Late Rev . Mr . Bayes , F . R . S . Communicated by Mr . Price , in a Letter to John Canton , A . M . F . R . S . Philosophical Transactions of the Royal Society of London , 53 , 370 – 418 . https : / / doi . org / 10 . 1098 / rstl . 1763 . 0053 Bennett , B . , & Provost , L . ( 2015 ) . What’s your theory ? Quality Progress , 48 ( 7 ) , 36 – 43 . Benneyan , J . C . , Lloyd , R . C . , & Plsek , P . E . ( 2003 ) . Statistical process control as a tool for research and healthcare improvement . BMJ Quality & Safety , 12 ( 6 ) , 458 – 464 . https : / / doi . org / 10 . 1136 / qhc . 12 . 6 . 458 Bergeron , P . - J . , & Rivard , L . ( 2017 ) . How to engage in pseudoscience with real data : A criti - cism of John Hattie’s arguments in visible learning from the perspective of a statistician . McGill Journal of Education / Revue des sciences de l’éducation de McGill , 52 ( 1 ) , 237 – 246 . https : / / doi . org / 10 . 7202 / 1040816ar Berman , P . , & McLaughlin , M . W . ( 1978 ) . Federal programs supporting educational change : Vol . 8 . Implementing and sustaining innovations . RAND Corporation . https : / / www . rand . org / pubs / reports / R1589z8 . html Bishop , R . S . ( 1990 ) . Mirrors , windows , and sliding glass doors . Perspectives , 6 ( 3 ) , ix – xi . Box , G . E . P . , & Jenkins , G . ( 1970 ) . Time series analysis , forecasting and control . Holden - Day . Box , G . E . P . , & Tiao , G . C . ( 1973 ) . Bayesian inference in statistical analysis . Addison - Wesley . Brick , C . , & Freeman , A . ( 2019 ) . Communicating evidence for policy makers in icons and tables : What works ? Preprint . University of Cambridge . https : / / doi . org / 10 . 31234 / osf . io / ujsxn Brown , P . , & Hare , D . ( 2002 ) . Rapid cycle improvement : Controlling change . Journal of the Arkansas Medical Society , 98 ( 10 ) , 2 – 3 . https : / / www . ncbi . nlm . nih . gov / pubmed / 12868134 Bryk , A . S . ( 2017 , March 27 ) . Redressing inequities : An aspiration in search of a method . Speech presented at Fourth Annual Carnegie Foundation Summit on Improvement in Education in California ( CA ) , San Francisco . https : / / www . carnegiefoundation . org / wp - content / uploads / 2017 / 04 / Carnegie _ Bryk _ Summit _ 2017 _ Keynote . pdf Bryk , A . S . , Gomez , L . M . , Grunow , A . , & LeMahieu , P . G . ( 2015 ) . Learning to improve : How America’s schools can get better at getting better . Harvard Education Press . Bryk , A . , Sebring , P . B . , Allensworth , E . L . , Luppescu , S . S . , & Easton , J . ( 2010 ) . Organizing schools for improvement : Lessons from Chicago . University of Chicago Consortium on Chicago School Research . Caird , J . , Sutcliffe , K . , Kwan , I . , Dickson , K . , & Thomas , J . ( 2015 ) . Mediating policy - relevant evidence at speed : Are systematic reviews of systematic reviews a useful approach ? Evidence & Policy , 11 ( 1 ) , 81 – 97 . https : / / doi . org / 10 . 1332 / 174426514X13988609036850 Campbell , D . T . , & Stanley , J . C . ( 1963 ) . Experimental and quasi experimental designs for research . Rand - McNally . Campbell , M . , McKenzie , J . E . , Sowden , A . , Katikireddi , S . V . , Brennan , S . E . , Ellis , S . , Hartmann - Boyce , J . , Ryan , R . , Shepperd , S . , Thomas , J . , Welch , V . , & Thomson , H . ( 2020 ) . Synthesis without meta - analysis ( SWiM ) in systematic reviews : Reporting guide - line . BMJ , 368 , 1 – 6 . http : / / doi . org / 10 . 1136 / bmj . l6890 Cartwright , N . ( 2019 ) . What is meant by “rigour” in evidence - based educational policy and what’s so good about it ? Educational Research and Evaluation , 25 ( 1 – 2 ) , 63 – 80 . https : / / doi . org / 10 . 1080 / 13803611 . 2019 . 1617990 Cartwright , N . , & Hardie , J . ( 2012 ) . Evidence - based policy : A practical guide to doing it better . Oxford University Press . Cartwright , N . , & Hardie , J . ( 2017 ) . Predicting what will happen when you intervene . Clinical Social Work Journal , 45 ( 3 ) , 270 – 279 . https : / / doi . org / 10 . 1007 / s10615 - 016 - 0615 - 0 158 Review of Research in Education , 45 Castles , A . , Rastle , K . , & Nation , K . ( 2018 ) . Ending the reading wars : Reading acquisition from novice to expert . Psychological Science in the Public Interest , 19 ( 1 ) , 5 – 51 . https : / / doi . org / 10 . 1177 / 1529100618772271 Century , J . , & Cassata , A . ( 2016 ) . Implementation research : Finding common ground on what , how , why , where , and who . Review of Research in Education , 40 ( 1 ) , 169 – 215 . https : / / doi . org / 10 . 3102 / 0091732X16665332 Chambers , D . A . , Glasgow , R . E . , & Stange , K . C . ( 2013 ) . The dynamic sustainability frame - work : Addressing the paradox of sustainment amid ongoing change . Implementation Science , 8 ( 1 ) , 117 . https : / / doi . org / 10 . 1186 / 1748 - 5908 - 8 - 117 Chambers , J . G . ( 1999 ) . Measuring resources in education : From accounting to the resource cost model approach ( Working Paper Series ) . Office of Educational Research and Improvement , National Center of Education Statistics . https : / / eric . ed . gov / ? id = ED433613 Chicago Beyond . ( 2019 ) . Why am I always being researched ? Vol . 1 . Chicago beyond equity series . https : / / chicagobeyond . org / researchequity / Claes , C . , van Loon , J . , Vandevelde , S . , & Schalock , R . ( 2015 ) . An integrative approach to evidence based practices . Evaluation and Program Planning , 48 , 132 – 136 . https : / / doi . org / 10 . 1016 / j . evalprogplan . 2014 . 08 . 002 Coburn , C . E . ( 2003 ) . Rethinking scale : Moving beyond numbers to deep and lasting change . Educational Researcher , 32 ( 6 ) , 3 – 12 . https : / / doi . org / 10 . 3102 / 0013189X032006003 Coburn , C . E . , Honig , M . I . , & Stein , M . K . ( 2009 ) . What’s the evidence on districts’ use of evidence . In J . D . Bransford , D . J . Stipek , N . J . Vye , L . M . Gomez , & D . Lam ( Eds . ) , The role of research in educational improvement ( pp . 67 – 87 ) . Harvard Education Press . Coburn , C . E . , & Penuel , W . R . ( 2016 ) . Research – practice partnerships in education : Outcomes , dynamics , and open questions . Educational Researcher , 45 ( 1 ) , 48 – 54 . https : / / doi . org / 10 . 3102 / 0013189X16631750 Coburn , C . E . , & Talbert , J . E . ( 2006 ) . Conceptions of evidence use in school districts : Mapping the terrain . American Journal of Education , 112 ( 4 ) , 469 – 495 . https : / / doi . org / 10 . 1086 / 505056 Coburn , C . E . , Toure , J . , & Yamashita , M . ( 2009 ) . Evidence , interpretation , and persua - sion : Instructional decision making at the district central office . Teachers College Record , 111 ( 4 ) , 1115 – 1161 . Cohen , D . K . ( 1990 ) . A revolution in one classroom : The case of Mrs . Oublier . Educational Evaluation and Policy Analysis , 12 ( 3 ) , 311 – 329 . https : / / doi . org / 10 . 3102 / 016237370120 03311 Cohen , D . K . , & Moffitt , S . L . ( 2009 ) . The ordeal of equality : Did federal regulation fix the schools ? Harvard University Press . Cohen , J . ( 1962 ) . The statistical power of abnormal - social psychological research : A review . Journal of Abnormal and Social Psychology , 65 ( 3 ) , 145 – 153 . https : / / doi . org / 10 . 1037 / h0045186 Coldwell , M . , Greaney , T . , Higgins , S . , Brown , C . , Maxwell , B . , Stiell , B . , Stoll , L . , Willis , B . , & Burns , H . ( 2017 , July ) . Evidence - informed teaching : An evaluation of progress in England . ( Research Report ) . Department for Education . https : / / www . gov . uk / govern - ment / publications / evidence - informed - teaching - evaluation - of - progress - in - england Conaway , C . , & Goldhaber , D . ( 2018 ) . Appropriate standards of evidence for education policy decision - making ( CEDR Policy Brief No . 04032018 - 1 - 3 ) . University of Washington . Cooke , A . , Smith , D . , & Booth , A . ( 2012 ) . Beyond PICO : The SPIDER tool for quali - tative evidence synthesis . Qualitative Health Research , 22 ( 10 ) , 1435 – 1443 . https : / / doi . org / 10 . 1177 / 1049732312452938 Cowen , N . ( 2019 ) . For whom does “what works” work ? The political economy of evidence - based education . Educational Research and Evaluation , 25 ( 1 – 2 ) , 81 – 98 . https : / / doi . org / 1 0 . 1080 / 13803611 . 2019 . 1617991 Ming , Goldenberg : Research Worth Using 159 Creswell , J . W . ( 1998 ) . Qualitative inquiry and research design : Choosing among five traditions . Sage . Creswell , J . W . , & Clark , V . L . P . ( 2017 ) . Designing and conducting mixed methods research . Sage . Davies , H . , & Powell , A . ( 2010 ) . Helping social research make a difference . In Health Foundation Seminar , November ( Unpublished Discussion Paper ) . https : / / www . tepou . co . nz / assets / images / content / training _ funding / tools - for - learning / files / Davies % 20 % 20 Powell % 20 - % 20Helping % 20social % 20research % 20make % 20a % 20difference % 20 final . pdf Davies , H . T . O . , & Nutley , S . ( 2008 ) . Learning more about how research - based knowledge gets used ( Unpublished Working Paper ) . William T . Grant Foundation . http : / / wtgrantfoun - dation . org / library / uploads / 2015 / 10 / Guidance - in - the - Development - of - New - Empirical - Research . pdf Day , E . , Wadsworth , S . M . , Bogenschneider , K . , & Thomas - Miller , J . ( 2019 ) . When univer - sity researchers connect with policy : A framework for whether , when , and how to engage . Journal of Family Theory & Review , 11 ( 1 ) , 165 – 180 . https : / / doi . org / 10 . 1111 / jftr . 12306 Deaton , A . ( 2019 ) . Randomization in the tropics revisited : A theme and eleven variations . In F . Bédécarrats , I . Guérin , & F . Roubaud ( Eds . ) , Randomized controlled trials in the field of development : A critical perspective ( pp . 29 – 46 ) . Oxford University Press . Deming , W . E . ( 1942 ) . On a classification of the problems of statistical inference . Journal of the American Statistical Association , 37 ( 218 ) , 173 – 185 . https : / / doi . org / 10 . 2307 / 2279212 Deming , W . E . ( 1953 ) . On the distinction between enumerative and analytic surveys . Journal of the American Statistical Association , 48 ( 262 ) , 244 – 255 . https : / / doi . org / 10 . 1080 / 0162 1459 . 1953 . 10483470 Deming , W . E . ( 1975 ) . On probability as a basis for action . The American Statistician , 29 ( 4 ) , 146 – 152 . https : / / doi . org / 10 . 1080 / 00031305 . 1975 . 10477402 Deming , W . E . ( 1982 ) . Quality , productivity and competitive position . MIT Center for Advanced Engineering Study . Deming , W . E . ( 1994 ) . The new economics for industry , government , education ( 2nd ed . ) . MIT Press . DesForges , C . ( 2001 ) . Putting educational research to use through knowledge transformation : The agency comments . Learning and Skills Development Agency . https : / / files . eric . ed . gov / fulltext / ED451373 . pdf Detterman , D . K . ( 1993 ) . The case for the prosecution : Transfer as an epiphenomenon . In D . K . Detterman & R . J . Sternberg ( Eds . ) , Transfer on trial : Intelligence , cognition , and instruction ( pp . 1 – 24 ) . Ablex . Diaz , M . , & Neuhauser , D . ( 2005 ) . Pasteur and parachutes : When statistical process control is better than a randomized controlled trial . BMJ Quality & Safety , 14 ( 2 ) , 140 – 143 . http : / / doi . org / 10 . 1136 / qshc . 2005 . 013763 Dobson , D . , & Cook , T . J . ( 1980 ) . Avoiding Type III error in program evaluation : Results from a field experiment . Evaluation and Program Planning , 3 ( 4 ) , 269 – 276 . https : / / doi . org / 10 . 1016 / 0149 - 7189 ( 80 ) 90042 - 7 Donaldson , S . I . , Christie , C . A . , & Mark , M . M . ( 2009 ) . What counts as credible evidence in applied research and evaluation practice ? Sage . Donnelly , C . A . , Boyd , I . , Campbell , P . , Craig , C . , Vallance , P . , Walport , M . , Whitty , C . J . M . , Woods , E . , & Wormald , C . ( 2018 ) . Four principles to make evidence synthesis more useful for policy . Nature , 558 , 361 – 364 . https : / / www . nature . com / articles / d41586 - 018 - 05414 - 4 Doucet , F . ( 2019 ) . Centering the margins : ( Re ) defining useful research evidence through criti - cal perspectives . William T . Grant Foundation . http : / / wtgrantfoundation . org / library / uploads / 2019 / 12 / Fabienne - Doucet - 2019 - WTG - Digest . pdf 160 Review of Research in Education , 45 Duncan , G . J . , & Magnuson , K . ( 2007 ) . Penny wise and effect size foolish . Child Development Perspectives , 1 ( 1 ) , 46 – 51 . https : / / doi . org / 10 . 1111 / j . 1750 - 8606 . 2007 . 00009 . x Erickson , F . , & Gutierrez , K . ( 2002 ) . Comment : Culture , rigor , and science in educa - tional research . Educational Researcher , 31 ( 8 ) , 21 – 24 . https : / / doi . org / 10 . 3102 / 00131 89X031008021 Every Student Succeeds Act . ( 2015 ) . Every Student Succeeds Act of 2015 , Pub . L . No . 114 - 95 § 114 Stat . 1177 ( 2015 – 2016 ) . Farley - Ripple , E . , May , H . , Karpyn , A . , Tilley , K . , & McDonough , K . ( 2018 ) . Rethinking connections between research and practice in education : A conceptual framework . Educational Researcher , 47 ( 4 ) , 235 – 245 . https : / / doi . org / 10 . 3102 / 0013189X18761042 Fixsen , D . L . , Naoom , S . F . , Blase , K . A . , Friedman , R . M . , & Wallace , F . ( 2005 ) . Implementation research : A synthesis of the literature . University of South Florida , Louis de la Parte Florida Mental Health Institute , The National Implementation Research Network ( FMHI Publication # 231 ) . Forman , M . L . , Stosich , E . L . , & Bocala , C . ( 2017 ) . The internal coherence framework : Creating the conditions for continuous improvement in schools . Harvard Education Press . Fretheim , A . , & Tomic , O . ( 2015 ) . Statistical process control and interrupted time series : A golden opportunity for impact evaluation in quality improvement . BMJ Quality & Safety , 24 , 748 – 752 . https : / / doi . org / 10 . 1136 / bmjqs - 2014 - 003756 Gambrill , E . ( 2006 ) . Critical thinking in clinical practice : Improving the quality of judgments and decisions . Wiley . Gamoran , A . ( 2015 ) . The future of educational inequality in the United States : What went wrong , and how can we fix it ? William T . Grant Foundation . http : / / wtgrantfoundation . org / resource / the - future - of - educational - inequality - what - went - wrong - and - how - can - we - fix - it Gelman , A . , & Robert , C . P . ( 2014 ) . Revised evidence for statistical standards . Proceedings of the National Academy of Sciences of the United States of America , 111 ( 19 ) , E1933 . https : / / doi . org / 10 . 1073 / pnas . 1322995111 Gillborn , D . , Warmington , P . , & Demack , S . ( 2018 ) . QuantCrit : Education , policy , “big data” and principles for a critical race theory of statistics . Race Ethnicity and Education , 21 ( 2 ) , 158 – 179 . https : / / doi . org / 10 . 1080 / 13613324 . 2017 . 1377417 Ginsburg , A . , & Smith , M . S . ( 2016 ) . Do randomized controlled trials meet the “gold stan - dard” ? American Enterprise Institute . https : / / www . carnegiefoundation . org / wp - content / uploads / 2016 / 03 / Do - randomized - controlled - trials - meet - the - gold - standard . pdf Glass , G . V . ( 1976 ) . Primary , secondary , and meta - analysis of research . Educational Researcher , 5 ( 10 ) , 3 – 8 . https : / / doi . org / 10 . 3102 / 0013189X005010003 Gorard , S . ( 2006 ) . Towards a judgement - based statistical analysis . British Journal of Sociology of Education , 27 ( 1 ) , 67 – 80 . https : / / doi . org / 10 . 1080 / 01425690500376663 Gorard , S . ( 2010 ) . All evidence is equal : The flaw in statistical reasoning . Oxford Review of Education , 36 ( 1 ) , 63 – 77 . https : / / doi . org / 10 . 1080 / 03054980903518928 Gorard , S . , See , B . H . , & Siddiqui , N . ( 2017 ) . The trials of evidence - based education : The prom - ises , opportunities and problems of trials in education . Routledge . Gorard , S . , See , B . H . , & Siddiqui , N . ( 2020 ) . What is the evidence on the best way to get evidence into use in education ? Review of Education . https : / / doi . org / 10 . 1002 / rev3 . 3200 Green , L . W . , Glasgow , R . E . , Atkins , D . , & Stange , K . ( 2009 ) . Making evidence from research more relevant , useful , and actionable in policy , program planning , and practice : Slips “twixt cup and lip . ” American Journal of Preventive Medicine , 37 ( 6 ) , S187 - S191 . https : / / doi . org / 10 . 1016 / j . amepre . 2009 . 08 . 017 Grimshaw , J . M . , Eccles , M . P . , Lavis , J . N . , Hill , S . J . , & Squires , J . E . ( 2012 ) . Knowledge translation of research findings . Implementation Science , 7 ( 1 ) , Article 50 . https : / / doi . org / 10 . 1186 / 1748 - 5908 - 7 - 50 Ming , Goldenberg : Research Worth Using 161 Gutiérrez , K . D . , & Penuel , W . R . ( 2014 ) . Relevance to practice as a criterion for rigor . Educational Researcher , 43 ( 1 ) , 19 – 23 . https : / / doi . org / 10 . 3102 / 0013189X13520289 Hammersley , M . ( 1990 ) . Reading ethnographic research : A critical guide . Longman . Hammond , Z . ( 2014 ) . Culturally responsive teaching and the brain : Promoting authentic engage - ment and rigor among culturally and linguistically diverse students . Corwin Press . Hargreaves , D . ( 1998 ) . Creative professionalism : The role of teachers in the knowledge society . Demos . Harris , D . N . ( 2009 ) . Toward policy - relevant benchmarks for interpreting effect sizes : Combining effects with costs . Educational Evaluation and Policy Analysis , 31 ( 1 ) , 3 – 29 . https : / / doi . org / 10 . 3102 / 0162373708327524 Head , B . W . ( 2008 ) . Three lenses of evidence - based policy . Australian Journal of Public Administration , 67 ( 1 ) , 1 – 11 . https : / / doi . org / 10 . 1111 / j . 1467 - 8500 . 2007 . 00564 . x Hedges , L . V . , & Olkin , I . ( 2014 ) . Statistical methods for meta - analysis . Academic Press . Hemsley - Brown , J . , & Sharp , C . ( 2003 ) . The use of research to improve professional practice : A systematic review of the literature . Oxford Review of Education , 29 ( 4 ) , 449 – 471 . https : / / doi . org / 10 . 1080 / 0305498032000153025 Henrich , J . , Heine , S . J . , & Norenzayan , A . ( 2010 ) . Beyond WEIRD : Towards a broad - based behavioral science . Behavioral and Brain Sciences , 33 ( 2 – 3 ) , 111 – 135 . https : / / doi . org / 10 . 1017 / s0140525x10000725 Hill , H . C . , & Erickson , A . ( 2019 ) . Using implementation fidelity to aid in interpreting program impacts : A brief review . Educational Researcher , 48 ( 9 ) , 590 – 598 . https : / / doi . org / 10 . 3102 / 0013189X19891436 Hollands , F . , Pan , Y . , & Escueta , M . ( 2019 ) . What is the potential for applying cost - utility analysis to facilitate evidence - based decision making in schools ? Educational Researcher , 48 ( 5 ) , 287 – 295 . https : / / doi . org / 0013189X19852101 Honig , M . I . ( 2013 ) . Beyond the policy memo : Designing to strengthen the practice of district central office leadership for instructional improvement at scale . National Society for the Study of Education Yearbook , 112 ( 2 ) , 256 – 273 . Honig , M . I . , & Coburn , C . ( 2008 ) . Evidence - based decision making in school district central offices : Toward a policy and research agenda . Educational Policy , 22 ( 4 ) , 578 – 608 . https : / / doi . org / 10 . 1177 / 0895904807307067 Honig , M . I . , & Venkateswaran , N . ( 2012 ) . School - central office relationships in evidence use : Understanding evidence use as a systems problem . American Journal of Education , 118 ( 2 ) , 199 – 222 . https : / / doi . org / 10 . 1086 / 663282 Honig , M . I . , Venkateswaran , N . , & McNeil , P . ( 2017 ) . Research use as learning : The case of fundamental change in school district central offices . American Educational Research Journal , 54 ( 5 ) , 938 – 971 . https : / / doi . org / 10 . 3102 / 0002831217712466 Hughes , C . A . , & Dexter , D . D . ( 2011 ) . Response to intervention : A research - based summary . Theory Into Practice , 50 ( 1 ) , 4 – 11 . http : / / doi . org / 10 . 1080 / 00405841 . 2011 . 534909 Hume , D . ( 1975 ) . An enquiry concerning human understanding . In P . Nidditch ( Ed . ) , Enquiries concerning human understanding and concerning the principles of morals ( 3rd ed . ) . Clarendon Press . ( Original work published 1748 ) Humphrey , N . , Lendrum , A . , Ashworth , E . , Frearson , K . , Buck , R . , & Kerr , K . ( 2016 ) . Implementation and process evaluation ( IPE ) for interventions in education settings : An intro - ductory handbook . Education Endowment Foundation . Iatarola , P . , & Stiefel , L . ( 2003 ) . Intradistrict equity of public education resources and per - formance . Economics of Education Review , 22 ( 1 ) , 69 – 78 . https : / / doi . org / 10 . 1016 / S0272 - 7757 ( 01 ) 00065 - 6 Institute for Healthcare Improvement . ( n . d . ) . Science of improvement : Establishing mea - sures . http : / / www . ihi . org / knowledge / Pages / HowtoImprove / ScienceofImprovement EstablishingMeasures . aspx 162 Review of Research in Education , 45 Ioannidis , J . P . ( 2019 ) . The importance of predefined rules and prespecified statistical analy - ses : Do not abandon significance . Journal of American Medical Association , 321 ( 21 ) , 2067 – 2068 . https : / / doi . org / 10 . 1001 / jama . 2019 . 4582 Irons , J . ( 2019 ) . Shifting the lens : Why conceptualization matters in research on reducing inequality . William T . Grant Foundation . http : / / wtgrantfoundation . org / digest / shifting - the - lens - why - conceptualization - matters - in - research - on - reducing - inequality / jenny - irons - 2019 - wtg - digest Jacob , R . T . , Doolittle , F . , Kemple , J . , & Somers , M . A . ( 2019 ) . A framework for learning from null results . Educational Researcher , 48 ( 9 ) , 580 – 589 . https : / / doi . org / 10 . 3102 / 0013189X19891955 Johnson , V . E . ( 2013 ) . Revised standards for statistical evidence . Proceedings of the National Academy of Sciences of the United States of America , 110 ( 48 ) , 19313 – 19317 . https : / / doi . org / 10 . 1073 / pnas . 1313476110 Jones , K . , & Okun , T . ( 2001 ) . White supremacy culture : From dismantling racism : A work - book for social change groups . http : / / www . cwsworkshop . org / PARC _ site _ B / dr - culture . html Joyce , K . E . ( 2019 ) . The key role of representativeness in evidence - based education . Educational Research and Evaluation , 25 ( 1 – 2 ) , 43 – 62 . https : / / doi . org / 10 . 1080 / 138036 11 . 2019 . 1617989 Joyce , K . E . , & Cartwright , N . ( 2019 ) . Bridging the gap between research and practice : Predicting what will work locally . American Educational Research Journal . https : / / doi . org / 10 . 3102 / 0002831219866687 Juran , J . M . ( 1951 ) . Quality control handbook . McGraw - Hill . Kelle , U . ( 2006 ) . Combining qualitative and quantitative methods in research practice : Purposes and advantages . Qualitative Research in Psychology , 3 ( 4 ) , 293 – 311 . https : / / doi . org / 10 . 1177 / 1478088706070839 Kemmis , S . , McTaggart , R . , & Nixon , R . ( 2014 ) . Introducing critical participatory action research . In The action research planner ( pp . 1 – 31 ) . Springer . https : / / doi . org / 10 . 1007 / 978 - 981 - 4560 - 67 - 2 _ 1 Kennedy , M . M . ( 1982 ) . Working knowledge and other essays . Huron Institute . Kim , A . M . , Tingen , C . M . , & Woodruff , T . K . ( 2010 ) . Sex bias in trials and treatment must end . Nature , 465 ( 7299 ) , 688 – 689 . https : / / doi . org / 10 . 1038 / 465688a Kirkland , D . E . ( 2019 ) . No small matters : Reimagining the use of research evidence from a racial justice perspective . William T . Grant Foundation . http : / / wtgrantfoundation . org / digest / no - small - matters - reimagining - the - use - of - research - evidence - from - a - racial - justice - per - spective / david - e - kirkland - 2019 - wtg - digest Kraft , M . A . ( 2020 ) . Interpreting effect sizes of education interventions . Educational Researcher , 49 ( 4 ) , 241 – 253 . https : / / doi . org / 10 . 3102 / 0013189X20912798 Kvarven , A . , Strømland , E . , & Johannesson , M . ( 2019 ) . Comparing meta - analyses and pre - registered multiple - laboratory replication projects . Nature Human Behaviour , 4 , 423 – 434 . https : / / doi . org / 10 . 1038 / s41562 - 019 - 0787 - z Ladson - Billings , G . ( 1995 ) . Toward a theory of culturally relevant pedagogy . American Educa - tional Research Journal , 32 ( 3 ) , 465 – 491 . https : / / doi . org / 10 . 3102 / 00028312032003465 Ladson - Billings , G . ( 2014 ) . Culturally relevant pedagogy 2 . 0 : Aka the remix . Harvard Educational Review , 84 ( 1 ) , 74 – 84 . https : / / doi . org / 10 . 17763 / haer . 84 . 1 . p2rj131485484751 Lagemann , E . C . ( 2000 ) . An elusive science : The troubling history of education research . University of Chicago Press . Lavis , J . N . ( 2009 ) . How can we support the use of systematic reviews in policymaking ? PLoS Medicine , 6 ( 11 ) , e1000141 . https : / / doi . org / 10 . 1371 / journal . pmed . 1000141 Lavis , J . N . , Robertson , D . , Woodside , J . M . , McLeod , C . B . , Abelson , J . , & the Knowledge Transfer Study Group . ( 2003 ) . How can research organizations more effectively transfer Ming , Goldenberg : Research Worth Using 163 research knowledge to decision makers ? Milbank Quarterly , 81 ( 2 ) , 221 – 248 . https : / / doi . org / 10 . 1111 / 1468 - 0009 . t01 - 1 - 00052 LeMahieu , P . ( 2011 , October 11 ) . What we need in education is more integrity ( and less fidelity ) of implementation [ blog post ] . https : / / www . carnegiefoundation . org / blog / what - we - need - in - education - is - more - integrity - and - less - fidelity - of - implementation / Levin , B . ( 2004 ) . Making research matter more . Education Policy Analysis Archives , 12 ( 56 ) , 1 – 20 . https : / / doi . org / 10 . 14507 / epaa . v12n56 . 2004 Levin , B . ( 2008 ) . Thinking about knowledge mobilization . Institute for Studies in Education . http : / / en . copian . ca / library / research / ccl / knowledge _ mobilization / knowledge _ mobiliza - tion . pdf Levin , B . ( 2011 ) . Mobilising research knowledge in education . London Review of Education , 9 ( 1 ) , 15 – 26 . https : / / doi . org / 10 . 1080 / 14748460 . 2011 . 550431 Levin , B . ( 2013 ) . To know is not enough : Research knowledge and its use . Review of Education , 1 ( 1 ) , 2 – 31 . https : / / doi . org / 10 . 1002 / rev3 . 3001 Levin , H . M . ( 1975 ) . Cost - effectiveness analysis in evaluation research . In M . Guttentag & E . L . Struening ( Eds . ) , Handbook of evaluation research ( Vol . 2 ) . Sage . Levin , H . M . ( 1983 ) . Cost - effectiveness analysis : A primer . Sage . Levin , H . M . ( 1995 ) . Cost - effectiveness analysis . In M . Carnoy ( Ed . ) , International encyclope - dia of economics of education ( Vol . 2 , pp . 381 – 386 ) . Pergamon . Levin , H . M . ( 2001 ) . Waiting for Godot : Cost - effectiveness analysis in education . New Directions for Evaluation , 2001 ( 90 ) , 55 – 68 . https : / / doi . org / 10 . 1002 / ev . 12 Levin , H . M . , & Belfield , C . ( 2015 ) . Guiding the development and use of cost - effectiveness analysis in education . Journal of Research on Educational Effectiveness , 8 ( 3 ) , 400 – 418 . https : / / doi . org / 10 . 1080 / 19345747 . 2014 . 915604 Levin , H . M . , & McEwan , P . J . ( 2002 ) . Are cost - effectiveness methods used correctly ? In L . N . Masse , W . S . Barnett , H . M . Levin , & P . J . McEwan ( Eds . ) , Cost - effectiveness and educational policy ( pp . 37 – 53 ) . Eye on Education . Levin , H . M . , & McEwan , P . J . ( 2003 ) . Cost - effectiveness analysis as an evaluation tool . In T . Kellaghan & D . L . Stufflebeam ( Eds . ) , International handbook of educational evalua - tion ( pp . 125 – 152 ) . Kluwer Academic . https : / / doi . org / 10 . 1007 / 978 - 94 - 010 - 0309 - 4 _ 10 Lewin , K . ( 1951 ) . Field theory in social science . Harper & Row . Lincoln , Y . S . , & Guba , E . G . ( 1985 ) . Naturalistic inquiry . Sage . Lugo - Gil , J . , Jean - Baptiste , D . , & Jaramillo , L . F . ( 2019 ) . Use of evidence to drive decision - making in government . Mathematica Policy Research . Massoud , M . R . , Nielsen , G . A . , Nolan , K . , Nolan , T . , Schall , M . W . , & Sevin , C . ( 2006 ) . A framework for spread : From local improvements to system - wide change . IHI Innovation Series White Paper . Institute for Healthcare Improvement . http : / / www . ihi . org / resources / Pages / IHIWhitePapers / AFrameworkforSpreadWhitePaper . aspx Maxwell , J . A . ( 1992 ) . Understanding and validity in qualitative research . Harvard Educational Review , 62 ( 3 ) , 279 – 300 . https : / / doi . org / 10 . 17763 / haer . 62 . 3 . 8323320856251826 Mayo , D . G . ( 2019 ) . P - value thresholds : Forfeit at your peril . European Journal of Clinical Investigation , 49 ( 10 ) , e13170 . https : / / doi . org / 10 . 1111 / eci . 13170 McCannon , C . J . , Schall , M . W . , & Perla , R . J . ( 2008 ) . Planning for scale : A guide for design - ing large - scale improvement initiatives . IHI Innovation Series White Paper . Institute for Healthcare Improvement . http : / / www . ihi . org / resources / Pages / IHIWhitePapers / PlanningforScaleWhitePaper . aspx McKenzie , K . B . , & Phillips , G . A . ( 2016 ) . Equity traps then and now : Deficit thinking , racial erasure and naïve acceptance of meritocracy . Whiteness and Education , 1 ( 1 ) , 26 – 38 . https : / / doi . org / 10 . 1080 / 23793406 . 2016 . 1159600 McShane , B . B . , Gal , D . , Gelman , A . , Robert , C . , & Tackett , J . L . ( 2019 ) . Abandon statistical significance . The American Statistician , 73 ( Suppl . 1 ) , 235 – 245 . https : / / doi . org / 10 . 1080 / 00031305 . 2018 . 1527253 164 Review of Research in Education , 45 Meadows , D . H . ( 2008 ) . Thinking in systems : A primer . Chelsea Green . Means , S . N . , Magura , S . , Burkhardt , J . T . , Schröter , D . C . , & Coryn , C . L . ( 2015 ) . Comparing rating paradigms for evidence - based program registers in behavioral health : Evidentiary criteria and implications for assessing programs . Evaluation and Program Planning , 48 , 100 – 116 . https : / / doi . org / 10 . 1016 / j . evalprogplan . 2014 . 09 . 007 Methley , A . M . , Campbell , S . , Chew - Graham , C . , McNally , R . , & Cheraghi - Sohi , S . ( 2014 ) . PICO , PICOS and SPIDER : A comparison study of specificity and sensitivity in three search tools for qualitative systematic reviews . BMC Health Services Research , 14 ( 1 ) , Article 579 . https : / / doi . org / 10 . 1186 / s12913 - 014 - 0579 - 0 Michener , J . ( 2020 , October 14 ) . Research evidence for an equitable democracy [ Conference pre - sentation ] . Panel Discussion : Critical Race Perspectives on the Use of Research Evidence . W . T . Grant Foundation . Miles , M . B . , & Huberman , A . M . ( 1994 ) . Qualitative data analysis : An expanded sourcebook ( 2nd ed . ) . Sage . Milner , H . R . , IV . ( 2020 ) . Disrupting racism and whiteness in researching a science of read - ing . Reading Research Quarterly , 55 , S249 – S253 . https : / / doi . org / 10 . 1002 / rrq . 347 Ming , N . C . , & Kennedy , A . I . ( 2020 ) . Developing and using indicators for continuous improvement . Teachers College Record ( Yearbook ) , 122 ( 14 ) . https : / / www . tcrecord . org / Content . asp ? ContentId = 23462 Mishan , E . J . ( 1976 ) . Cost - benefit analysis ( Vol . 454 ) . Praeger . Munro , E . , Cartwright , N . , Hardie , J . , & Montuschi , E . ( 2016 ) . Improving child safety : Deliberation , judgement and empirical research . Centre for Humanities , Engaging Science and Society ( CHESS ) . http : / / eprints . lse . ac . uk / id / eprint / 86549 Munthe - Kaas , H . , Nøkleby , H . , Lewin , S . , & Glenton , C . ( 2020 ) . The TRANSFER approach for assessing the transferability of systematic review findings . BMC Medical Research Methodology , 20 ( 1 ) , Article 11 . https : / / doi . org / 10 . 1186 / s12874 - 019 - 0834 - 5 Murawski , W . W . , & Hughes , C . E . ( 2009 ) . Response to intervention , collaboration , and co - teaching : A logical combination for successful systemic change . Preventing School Failure : Alternative Education for Children and Youth , 53 ( 4 ) , 267 – 277 . https : / / doi . org / 10 . 3200 / PSFL . 53 . 4 . 267 - 277 National Equity Project . ( 2020 ) . The lens of systemic oppression [ web page ] . https : / / www . natio - nalequityproject . org / frameworks / lens - of - systemic - oppression National Institute of Child Health and Human Development . ( 2000 ) . Report of the National Reading Panel : Teaching children to read : Reports of the subgroups ( 00 - 4754 ) . Government Printing Office . National Research Council . ( 1998 ) . Preventing reading difficulties in young children . National Academies Press . Neal , J . W . , Neal , Z . P . , Lawlor , J . A . , Mills , K . J . , & McAlindon , K . ( 2018 ) . What makes research useful for public school educators ? Administration and Policy in Mental Health and Mental Health Services Research , 45 ( 3 ) , 432 – 446 . https : / / doi . org / 10 . 1007 / s10488 - 017 - 0834 - x Nutley , S . , Walter , I . , & Davies , H . T . O . ( 2003 ) . From knowing to doing : A framework for understanding the evidence - into - practice agenda . Evaluation , 9 ( 2 ) , 125 – 148 . https : / / doi . org / 10 . 1177 / 1356389003009002002 Nutley , S . , Walter , I . , & Davies , H . T . O . ( 2007 ) . Using evidence : How research can inform public services . Policy Press . O’Brien , E . , & Martinez - Vidal , E . ( 2016 ) . Evidence - based state health policymaking . https : / / www . academyhealth . org / sites / default / files / AH % 20Evidence % 20Based % 20State % 20 Health % 20report . pdf O’Donnell , C . L . ( 2008 ) . Defining , conceptualizing , and measuring fidelity of implementa - tion and its relationship to outcomes in K – 12 curriculum intervention research . Review of Educational Research , 78 ( 1 ) , 33 – 84 . https : / / doi . org / 10 . 3102 / 0034654307313793 Ming , Goldenberg : Research Worth Using 165 Oliver , K . , & Cairney , P . ( 2019 ) . The dos and don’ts of influencing policy : A systematic review of advice to academics . Palgrave Communications , 5 ( 1 ) , Article 21 . https : / / doi . org / 10 . 1057 / s41599 - 019 - 0232 - y Onwuegbuzie , A . J . , & Leech , N . L . ( 2005 ) . On becoming a pragmatic researcher : The importance of combining quantitative and qualitative research methodolo - gies . International Journal of Social Research Methodology , 8 ( 5 ) , 375 – 387 . https : / / doi . org / 10 . 1080 / 13645570500402447 Palinkas , L . A . , Finno , M . , Fuentes , D . , Garcia , A . , & Holloway , I . W . ( 2011 , August ) . Evaluating dissemination of research evidence in public youth - serving systems . Paper pre - sented at the National Child Welfare Evaluation Summit , Washington , DC . Palinkas , L . A . , Saldana , L . , Chou , C . P . , & Chamberlain , P . ( 2017 ) . Use of research evidence and implementation of evidence - based practices in youth - serving systems . Children and Youth Services Review , 83 , 242 – 247 . https : / / doi . org / 10 . 1016 / j . childyouth . 2017 . 11 . 005 Pearl , J . , & MacKenzie , D . ( 2018 ) . The book of why : The new science of cause and effect . Basic Books . Penuel , W . R . , Briggs , D . C . , Davidson , K . L . , Herlihy , C . , Sherer , D . , Hill , H . C . , Farrell , C . C . , & Allen , A . R . ( 2016 ) . Findings from a national survey on research use among school and district leaders ( Technical Report No . 1 ) . National Center for Research in Policy and Practice . http : / / ncrpp . org / assets / documents / NCRPP _ Technical - Report _ 180302 . pdf Penuel , W . R . , Farrell , C . C . , Allen , A . R . , Toyama , Y . , & Coburn , C . E . ( 2016 ) . What research district leaders find useful . Educational Policy , 32 ( 4 ) , 540 – 568 . https : / / doi . org / 10 . 1177 / 0895904816673580 Penuel , W . R . , Fishman , B . J . , Haugan Cheng , B . , & Sabelli , N . ( 2011 ) . Organizing research and development at the intersection of learning , implementation , and design . Educational Researcher , 40 ( 7 ) , 331 – 337 . https : / / doi . org / 10 . 3102 / 0013189X11421826 Penuel , W . R . , & Means , B . ( 2004 ) . Implementation variation and fidelity in an inquiry sci - ence program : Analysis of GLOBE data reporting patterns . Journal of Research in Science Teaching , 41 ( 3 ) , 294 – 315 . https : / / doi . org / 10 . 1002 / tea . 20002 Philip , T . M . , Bang , M . , & Jackson , K . ( 2018 ) . Articulating the “how , ” the “for what , ” the “for whom , ” and the “with whom” in concert : A call to broaden the benchmarks of our scholarship . Cognition and Instruction , 36 ( 2 ) , 83 – 88 . https : / / doi . org / 10 . 1080 / 07370008 . 2018 . 1413530 Phillips , D . C . ( 2019 ) . Evidence of confusion about evidence of causes : Comments on the debate about EBP in education . Educational Research and Evaluation , 25 ( 1 – 2 ) , 7 – 24 . http : / / doi . org / 10 . 1080 / 13803611 . 2019 . 1617980 Piaget , J . ( 1952 ) . The origins of intelligence in children . International Universities Press . Pigott , T . , & Polanin , J . ( 2020 ) . Methodological guidance papers : High - quality meta - anal - ysis in a systematic review . Review of Educational Research , 90 ( 1 ) , 1 – 23 . https : / / doi . org / 10 . 3102 / 0034654319877153 Pogrow , S . ( 2019 ) . How effect size ( practical significance ) misleads clinical practice : The case for switching to practical benefit to assess applied research findings . The American Statistician , 73 ( Suppl . 1 ) , 223 – 234 . https : / / doi . org / 10 . 1080 / 00031305 . 2018 . 1549101 Popay , J . , Roberts , H . , Sowden , A . , Petticrew , M . , Arai , L . , Rodgers , M . , Britten , N . , Roen , K . , & Duffy , S . ( 2006 ) . Guidance on the conduct of narrative synthesis in systematic reviews : A product from the ESRC Methods Programme . Lancaster University . https : / / www . lancaster . ac . uk / media / lancaster - university / content - assets / documents / fhm / dhr / chir / NSsynthesisguidanceVersion1 - April2006 . pdf Popper , K . ( 2005 ) . The logic of scientific discovery . Routledge . ( Original work published 1934 ) Posner , G . J . , Strike , K . A . , Hewson , P . W . , & Gertzog , W . A . ( 1982 ) . Accommodation of a scientific conception : Toward a theory of conceptual change . Science Education , 66 ( 2 ) , 211 – 227 . 166 Review of Research in Education , 45 Provost , L . P . ( 2011 ) . Analytical studies : A framework for quality improvement design and analysis . BMJ Quality & Safety , 20 ( Suppl . 1 ) , i92 – i96 . Provost , L . P . , & Murray , S . ( 2011 ) . The health care data guide : Learning from data for improve - ment . Wiley . Ravallion , M . ( 2018 ) . Should the randomistas ( continue to ) rule ? ( Working Paper No . 492 ) . Center for Global Development . https : / / www . cgdev . org / sites / default / files / should - ran - domistas - continue - rule - revised - jan - 2019 . pdf Rayner , K . , Foorman , B . R . , Perfetti , C . A . , Pesetsky , D . , & Seidenberg , M . S . ( 2001 ) . How psychological science informs the teaching of reading . Psychological Science in the Public Interest , 2 ( 2 ) , 31 – 74 . https : / / doi . org / 10 . 1111 / 1529 - 1006 . 00004 Rice , J . K . ( 1997 ) . Cost analysis in education : Paradox and possibility . Educational Evaluation and Policy Analysis , 19 ( 4 ) , 309 – 317 . https : / / doi . org / 10 . 3102 / 01623737019004309 Rittle - Johnson , B . , Siegler , R . S . , & Alibali , M . W . ( 2001 ) . Developing conceptual under - standing and procedural skill in mathematics : An iterative process . Journal of Educational Psychology , 93 ( 2 ) , 346 – 362 . http : / / doi . org / 10 . 1037 / 0022 - 0663 . 93 . 2 . 346 Ross , J . A . ( 2008 ) . Cost – utility analysis in educational needs assessment . Evaluation and Program Planning , 31 ( 4 ) , 356 – 367 . https : / / doi . org / 10 . 1016 / j . evalprogplan . 2008 . 06 . 003 Roweis , S . , & Ghahramani , Z . ( 1999 ) . A unifying review of linear Gaussian models . Neural Computation , 11 ( 2 ) , 305 – 345 . http : / / doi . org / 10 . 1162 / 089976699300016674 Russell , G . M . , & Kelly , N . H . ( 2002 , September ) . Research as interacting dialogic pro - cesses : Implications for reflexivity . Forum : Qualitative Social Research , 3 ( 3 ) . http : / / doi . org / 10 . 17169 / fqs - 3 . 3 . 831 Russell , J . L . , Bryk , A . S . , Dolle , J . , Gomez , L . M . , LeMahieu , P . , & Grunow , A . ( 2017 ) . A framework for the initiation of networked improvement communities . Teachers College Record , 119 ( 7 ) , 1 – 36 . Schanzenbach , D . W . ( 2012 ) . Limitations of experiments in education research . Education Finance and Policy , 7 ( 2 ) , 219 – 232 . https : / / doi . org / 10 . 1162 / EDFP _ a _ 00063 Schmidt , R . A . , & Bjork , R . A . ( 1992 ) . New conceptualizations of practice : Common prin - ciples in three paradigms suggest new concepts for training . Psychological Science , 3 ( 4 ) , 207 – 218 . https : / / doi . org / 10 . 1111 / j . 1467 - 9280 . 1992 . tb00029 . x Scott , J . ( 2020 , July 22 ) . Challenges to transforming education practice : The role of politics in centering equity [ Conference presentation ] . National Network of Education Research - Practice Partnerships ( NNERPP ) Virtual Annual Forum . https : / / nnerppannualforum . rice . edu / Scoville , R . , Little , K . , Rakover , J . , Luther , K . , & Mate , K . ( 2016 ) . Sustaining improvement ( IHI White Paper ) . Institute for Healthcare Improvement . http : / / www . ihi . org / resources / Pages / IHIWhitePapers / Sustaining - Improvement . aspx Shalowitz , M . U . , Isacco , A . , Barquin , N . , Clark - Kauffman , E . , Delger , P . , Nelson , D . , Quinn , A . , & Wagenaar , K . A . ( 2009 ) . Community - based participatory research : A review of the literature with strategies for community engagement . Journal of Developmental & Behavioral Pediatrics , 30 ( 4 ) , 350 – 361 . https : / / doi . org / 10 . 1097 / DBP . 0b013e3181b0ef14 Sharples , J . , Albers , B . , Fraser , S . , & Kime , S . ( 2019 ) . Putting evidence to work : A school’s guide to implementation ( 2nd ed . ) [ Online Guidance Report ] . Education Endowment Foundation . https : / / educationendowmentfoundation . org . uk / public / files / Publications / Implementation / EEF _ Implementation _ Guidance _ Report _ 2019 . pdf Shea , B . J . , Reeves , B . C . , Wells , G . , Thuku , M . , Hamel , C . , Moran , J . , Moher , D . , Tugwell , P . , Welch , V . , Kristjansson , E . , & Henry , D . A . ( 2017 ) . AMSTAR 2 : A critical appraisal tool for systematic reviews that include randomised or non - randomised studies of health - care interventions , or both . British Medical Journal ( Online ) , 358 , j4008 . https : / / doi . org / 10 . 1136 / bmj . j4008 Ming , Goldenberg : Research Worth Using 167 Sherer , D . , Norman , J . , Bryk , A . S . , Peurach , D . J . , Vasudeva , A . , & McMahon , K . ( 2019 ) . Evidence for improvement : An integrated analytic approach for supporting networks in educa - tion . Carnegie Foundation for the Advancement of Teaching . Shewhart , W . ( 1931 ) . Economic control of quality of manufactured product . Macmillan . Shonkoff , J . P . ( 2000 ) . Science , policy , and practice : Three cultures in search of a shared mission . Child Development , 71 ( 1 ) , 181 – 187 . https : / / doi . org / 10 . 1111 / 1467 - 8624 . 00132 Siddiqui , N . , Gorard , S . , & See , B . H . ( 2018 ) . The importance of process evaluation for ran - domised control trials in education . Educational Research , 60 ( 3 ) , 357 – 370 . https : / / doi . org / 10 . 1080 / 00131881 . 2018 . 1493349 Siegler , R . S . , & Crowley , K . ( 1991 ) . The microgenetic method : A direct means for studying cognitive development . American Psychologist , 46 ( 6 ) , 606 – 620 . Simpson , A . ( 2017 ) . The misdirection of public policy : Comparing and combining stan - dardised effect sizes . Journal of Education Policy , 32 ( 4 ) , 450 – 466 . https : / / doi . org / 10 . 108 0 / 02680939 . 2017 . 1280183 Simpson , A . ( 2018 ) . Princesses are bigger than elephants : Effect size as a category error in evidence - based education . British Educational Research Journal , 44 ( 5 ) , 897 – 913 . https : / / doi . org / 10 . 1002 / berj . 3474 Simpson , A . ( 2019 ) . Separating arguments from conclusions : The mistaken role of effect size in educational policy research . Educational Research and Evaluation , 25 ( 1 – 2 ) , 99 – 109 . https : / / doi . org / 10 . 1080 / 13803611 . 2019 . 1617170 Simpson , A . ( 2020 ) . On the misinterpretation of effect size . Educational Studies in Mathematics , 103 ( 1 ) , 125 – 133 . https : / / doi . org / 10 . 1007 / s10649 - 019 - 09924 - 4 Singer , J . D . ( 2019 ) . Reshaping the arc of quantitative educational research : It’s time to broaden our paradigm . Journal of Research on Educational Effectiveness , 12 ( 4 ) , 570 – 593 . https : / / doi . org / 10 . 1080 / 19345747 . 2019 . 1658835 Singleton , G . E . ( 2014 ) . Courageous conversations about race : A field guide for achieving equity in schools . Corwin Press . Slavin , R . ( 2018 , June 21 ) . John Hattie is wrong [ Robert Slavin’s blog ] . https : / / robertslavins - blog . wordpress . com / 2018 / 06 / 21 / john - hattie - is - wrong Smith , G . C . S . , & Pell , J . P . ( 2003 ) . Parachute use to prevent death and major trauma related to gravitational challenge : Systematic review of randomized controlled tri - als . British Medical Journal , 327 ( 20 – 7 ) , 1459 – 1461 . https : / / doi . org / 10 . 1136 / bmj . 327 . 7429 . 1459 Smith , R . ( 2010 ) . A bubble for the spirit level : Metricophilia , rhetoric and philosophy . In P . Smeyers , & M . Depaepe ( Eds . ) , Educational research : The ethics and aesthetics of statistics ( Vol . 5 , pp . 189 – 204 ) . Springer . https : / / doi . org / 10 . 1007 / 978 - 90 - 481 - 9873 - 3 _ 13 Solberg , L . I . , Mosser , G . , & McDonald , S . ( 1997 ) . The three faces of performance measure - ment : Improvement , accountability , and research . Joint Commission Journal on Quality Improvement , 23 ( 3 ) , 135 – 147 . https : / / doi . org / 10 . 1016 / S1070 - 3241 ( 16 ) 30305 - 4 Spillane , J . P . ( 1998 ) . State policy and the non - monolithic nature of the local school district : Organizational and professional considerations . American Educational Research Journal , 35 ( 1 ) , 33 – 63 . https : / / doi . org / 10 . 3102 / 00028312035001033 Spillane , J . P . , Reiser , B . P . , & Reimer , T . ( 2002 ) . Policy implementation and cognition : Reframing and refocusing implementation research . Review of Educational Research , 72 ( 3 ) , 387 – 431 . https : / / doi . org / 10 . 3102 / 00346543072003387 Sue , D . W . , Capodilupo , C . M . , Torino , G . C . , Bucceri , J . M . , Holder , A . , Nadal , K . L . , & Esquilin , M . ( 2007 ) . Racial microaggressions in everyday life : Implications for clini - cal practice . American Psychologist , 62 ( 4 ) , 271 – 286 . https : / / doi . org / 10 . 1037 / 0003 - 066X . 62 . 4 . 271 Sutherland , W . J . ( 2013 ) . Review by quality not quantity for better policy . Nature , 503 ( 7475 ) , 167 . https : / / doi . org / 10 . 1038 / 503167a 168 Review of Research in Education , 45 Sutherland , W . J . , & Burgman , M . ( 2015 ) . Policy advice : Use experts wisely . Nature , 526 ( 7573 ) , 317 – 318 . https : / / doi . org / 10 . 1038 / 526317a Taylor , J . A . , Davis , E . , & Michaelson , L . E . ( 2021 ) . Considerations for evidence frame - works in education research . Review of Research in Education , 45 ( 1 ) , 101 – 128 . https : / / doi . org / 10 . 3102 / 0091732X20985077 Thomas , J . ( 1985 ) . Force field analysis : A new way to evaluate your strategy . Long Range Planning , 18 ( 6 ) , 54 – 59 . https : / / doi . org / 10 . 1016 / 0024 - 6301 ( 85 ) 90064 - 0 Thor , J . , Lundberg , J . , Ask , J . , Olsson , J . , Carli , C . , Härenstam , K . P . , & Brommels , M . ( 2007 ) . Application of statistical process control in healthcare improvement : Systematic review . BMJ Quality & Safety in Healthcare , 16 ( 5 ) , 387 – 399 . http : / / dx . doi . org / 10 . 1136 / qshc . 2006 . 022194 Topp , L . , Mair , D . , Smillie , L . , & Cairney , P . ( 2018 ) . Knowledge management for policy impact : The case of the European Commission’s Joint Research Centre Introduction : Why we need knowledge management for policy . Palgrave Communications , 4 ( 1 ) , Article 87 . https : / / doi . org / 10 . 1057 / s41599 - 018 - 0143 - 3 Tseng , V . ( 2012 ) . The uses of research in policy and practice . Society for Research in Child Development . https : / / srcd . onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / j . 2379 - 3988 . 2012 . tb00071 . x Tseng , V . , Easton , J . Q . , & Supplee , L . H . ( 2017 ) . Research - practice partnerships : Building two - way streets of engagement . Social Policy Report , 30 ( 4 ) , 1 – 17 . https : / / doi . org / 10 . 1002 / j . 2379 - 3988 . 2017 . tb00089 . x Tseng , V . , & Nutley , S . ( 2014 ) . Building the infrastructure to improve the use and usefulness of research in education . In K . Finnigan & A . Daly ( Eds . ) , Using research evidence in edu - cation : Policy implications of research in education ( Vol . 2 , pp . 163 – 175 ) . Springer . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 04690 - 7 _ 11 Tuck , E . , & Yang , K . W . ( 2014 ) . R - words : Refusing research . In D . Paris & M . T . Winn ( Eds . ) , Humanizing research : Decolonizing qualitative inquiry with youth and communities ( pp . 223 – 247 ) . https : / / doi . org / 10 . 4135 / 9781544329611 . n12 Tucker , M . , & Slavin , R . ( 2018 , April 23 ) . What should researchers research ? Point and counter - point from Marc Tucker and Robert E . Slavin . Johns Hopkins Institute for Education Policy . https : / / edpolicy . education . jhu . edu / researchers - research - point - counter - point - marc - tucker - robert - e - slavin / Vakil , S . , McKinney de Royston , M . , Suad Nasir , N . I . , & Kirshner , B . ( 2016 ) . Rethinking race and power in design - based research : Reflections from the field . Cognition and Instruction , 34 ( 3 ) , 194 – 209 . https : / / doi . org / 10 . 1080 / 07370008 . 2016 . 1169817 Valentine , J . C . , Aloe , A . M . , & Wilson , S . J . ( 2019 ) . Interpreting effect sizes . In H . Cooper , L . V . Hedges , & J . C . Valentine ( Eds . ) , The handbook of research synthesis and meta - analysis ( pp . 433 – 452 ) . Russell Sage Foundation . VanLehn , K . ( 1996 ) . Cognitive skill acquisition . Annual Review of Psychology , 47 ( 1 ) , 513 – 539 . https : / / doi . org / 10 . 1146 / annurev . psych . 47 . 1 . 513 Vygotsky , L . S . ( 1978 ) . Mind in society : The development of higher psychological processes . Harvard University Press . Walker , S . C . , Lyon , A . R . , Aos , S . , & Trupin , E . W . ( 2017 ) . The consistencies and vagaries of the Washington state inventory of evidence - based practice : The definition of “evidence - based” in a policy context . Administration and Policy in Mental Health and Mental Health Services Research , 44 ( 1 ) , 42 – 54 . https : / / doi . org / 10 . 1007 / s10488 - 015 - 0652 - y Ward , V . ( 2017 ) . Why , whose , what and how ? A framework for knowledge mobilisers . Evidence & Policy , 13 ( 3 ) , 477 – 497 . https : / / doi . org / 10 . 1332 / 174426416X14634763278725 Wasserstein , R . L . , & Lazar , N . A . ( 2016 ) . The ASA statement on p - values : Context , process , and purpose . The American Statistician , 70 ( 2 ) , 129 – 133 , https : / / doi . org / 10 . 1080 / 00031 305 . 2016 . 1154108 Ming , Goldenberg : Research Worth Using 169 Watkins , R . , West Meiers , M . , & Visser , Y . ( 2012 ) . A guide to assessing needs : Essential tools for collecting information , making decisions , and achieving development results . World Bank . https : / / doi . org / 10 . 1596 / 978 - 0 - 8213 - 8868 - 6 Watt , D . ( 2007 ) . On becoming a qualitative researcher : The value of reflexivity . Qualitative Report , 12 ( 1 ) , 82 – 101 . https : / / eric . ed . gov / ? id = EJ800164 Weiss , C . H . ( 1977 ) . Research for policy’s sake : The enlightenment function of social research . Policy Analysis , 3 ( 4 ) , 531 – 545 . www . jstor . org / stable / 42783234 Weiss , C . H . ( 1980 ) . Knowledge creep and decision accretion . Knowledge , 1 ( 3 ) , 381 – 404 . https : / / doi . org / 10 . 1177 / 107554708000100303 Weiss , C . H . , & Bucuvalas , M . J . ( 1980 ) . Truth tests and utility tests : Decision - makers’ frames of reference for social science research . American Sociological Review , 45 ( 2 ) , 302 – 313 . https : / / doi . org / 10 . 2307 / 2095127 Weiss , C . H . , Murphy - Graham , E . , & Birkeland , S . ( 2005 ) . An alternate route to policy influence : How evaluations affect DARE . American Journal of Evaluation , 26 ( 1 ) , 12 – 30 . https : / / doi . org / 10 . 1177 / 1098214004273337 Westbrook , T . P . R . , Avellar , S . A . , & Seftor , N . ( 2017 ) . Reviewing the reviews : Examining similarities and differences between federally funded evidence reviews . Evaluation Review , 41 ( 3 ) , 183 – 211 . https : / / doi . org / 10 . 1177 / 0193841X16666463 White , H . ( 2019 ) . The twenty - first century experimenting society : The four waves of the evi - dence revolution . Palgrave Communications , 5 ( 47 ) , 1 – 7 . https : / / doi . org / 10 . 1057 / s41599 - 019 - 0253 - 6 Wiggins , B . ( 2009 ) . William James and methodological pluralism : Bridging the qualitative and quantitative divide . Journal of Mind and Behavior , 30 ( 3 ) , 165 – 183 . www . jstor . org / stable / 43854247 Wiliam , D . ( 2019 ) . Some reflections on the role of evidence in improving education . Educational Research and Evaluation , 25 ( 1 – 2 ) , 127 – 139 . https : / / doi . org / 10 . 1080 / 1380 3611 . 2019 . 1617993 Winter , G . ( 2000 ) . A comparative discussion of the notion of validity in qualitative and quan - titative research . Qualitative Report , 4 ( 3 ) , 1 – 14 . https : / / nsuworks . nova . edu / tqr / vol4 / iss3 / 4 Wong , C . P . ( 2021 ) . The wretched of the research : Disenchanting Man2 - as - educational researcher and entering the 36th chamber of education research . Review of Research in Education , 45 ( 1 ) , 27 – 66 . https : / / doi . org / 10 . 3102 / 0091732X21990609 Wrigley , T . , & McCusker , S . ( 2019 ) . Evidence - based teaching : A simple view of “science . ” Educational Research and Evaluation , 25 ( 1 – 2 ) , 110 – 126 . https : / / doi . org / 10 . 1080 / 13803 611 . 2019 . 1617992 W . T . Grant Foundation . ( n . d . ) . Research grants on improving the use of research evidence . http : / / wtgrantfoundation . org / grants / research - grants - improving - use - research - evidence Yeaton , W . , & Sechrest , L . ( 1981 ) . Meaningful measures of effect . Journal of Consulting and Clinical Psychology , 49 ( 5 ) , 766 – 767 . https : / / doi . org / 10 . 1037 / 0022 - 006X . 49 . 5 . 766 Zack , M . K . , Karre , J . K . , Olson , J . , & Perkins , D . F . ( 2019 ) . Similarities and differences in program registers : A case study . Evaluation and Program Planning , 76 , 101676 . https : / / doi . org / 10 . 1016 / j . evalprogplan . 2019 . 101676 Zhao , Y . ( 2017 ) . What works may hurt : Side effects on education . Journal of Educational Change , 18 ( 1 ) , 1 – 19 . https : / / doi . org / 10 . 1007 / s10833 - 016 - 9294 - 4 Ziliak , S . , & McCloskey , D . N . ( 2008 ) . The cult of statistical significance : How the standard error costs us jobs , justice , and lives . University of Michigan Press .