A Contextual - Bandit Approach to Personalized News Article Recommendation Lihong Li † , Wei Chu † , † Yahoo ! Labs lihong , chuwei @ yahoo - inc . com John Langford ‡ ‡ Yahoo ! Labs jl @ yahoo - inc . com Robert E . Schapire + ∗ + Dept of Computer Science Princeton University schapire @ cs . princeton . edu ABSTRACT Personalized web services strive to adapt their services ( advertise - ments , news articles , etc . ) to individual users by making use of both content and user information . Despite a few recent advances , this problem remains challenging for at least two reasons . First , web service is featured with dynamically changing pools of con - tent , rendering traditional collaborative ﬁltering methods inappli - cable . Second , the scale of most web services of practical interest calls for solutions that are both fast in learning and computation . In this work , we model personalized recommendation of news articles as a contextual bandit problem , a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles , while simultaneously adapting its article - selection strategy based on user - click feedback to maximize total user clicks . The contributions of this work are three - fold . First , we propose a new , general contextual bandit algorithm that is computationally efﬁcient and well motivated from learning theory . Second , we ar - gue that any bandit algorithm can be reliably evaluated ofﬂine us - ing previously recorded random trafﬁc . Finally , using this ofﬂine evaluation method , we successfully applied our new algorithm to a Yahoo ! Front Page Today Module dataset containing over 33 million events . Results showed a 12 . 5 % click lift compared to a standard context - free bandit algorithm , and the advantage becomes even greater when data gets more scarce . Categories and Subject Descriptors H . 3 . 5 [ Information Systems ] : On - line Information Services ; I . 2 . 6 [ Computing Methodologies ] : Learning General Terms Algorithms , Experimentation Keywords Contextual bandit , web service , personalization , recommender sys - tems , exploration / exploitation dilemma 1 . INTRODUCTION This paper addresses the challenge of identifying the most appro - priate web - based content at the best time for individual users . Most ∗ This work was done while R . Schapire visited Yahoo ! Labs . Copyright is held by the International World Wide Web Conference Com - mittee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26 – 30 , 2010 , Raleigh , North Carolina , USA . ACM 978 - 1 - 60558 - 799 - 8 / 10 / 04 . service vendors acquire and maintain a large amount of content in their repository , for instance , for ﬁltering news articles [ 14 ] or for the display of advertisements [ 5 ] . Moreover , the content of such a web - service repository changes dynamically , undergoing frequent insertions and deletions . In such a setting , it is crucial to quickly identify interesting content for users . For instance , a news ﬁlter must promptly identify the popularity of breaking news , while also adapting to the fading value of existing , aging news stories . It is generally difﬁcult to model popularity and temporal changes based solely on content information . In practice , we usually ex - plore the unknown by collecting consumers’ feedback in real time to evaluate the popularity of new content while monitoring changes in its value [ 3 ] . For instance , a small amount of trafﬁc can be des - ignated for such exploration . Based on the users’ response ( such as clicks ) to randomly selected content on this small slice of traf - ﬁc , the most popular content can be identiﬁed and exploited on the remaining trafﬁc . This strategy , with random exploration on an ! fraction of the trafﬁc and greedy exploitation on the rest , is known as ! - greedy . Advanced exploration approaches such as EXP3 [ 8 ] or UCB1 [ 7 ] could be applied as well . Intuitively , we need to dis - tribute more trafﬁc to new content to learn its value more quickly , and fewer users to track temporal changes of existing content . Recently , personalized recommendation has become a desirable feature for websites to improve user satisfaction by tailoring con - tent presentation to suit individual users’ needs [ 10 ] . Personal - ization involves a process of gathering and storing user attributes , managing content assets , and , based on an analysis of current and past users’ behavior , delivering the individually best content to the present user being served . Often , both users and content are represented by sets of fea - tures . User features may include historical activities at an aggre - gated level as well as declared demographic information . Content features may contain descriptive information and categories . In this scenario , exploration and exploitation have to be deployed at an in - dividual level since the views of different users on the same con - tent can vary signiﬁcantly . Since there may be a very large number of possible choices or actions available , it becomes critical to rec - ognize commonalities between content items and to transfer that knowledge across the content pool . Traditional recommender systems , including collaborative ﬁl - tering , content - based ﬁltering and hybrid approaches , can provide meaningful recommendations at an individual level by leveraging users’ interests as demonstrated by their past activity . Collaborative ﬁltering [ 25 ] , by recognizing similarities across users based on their consumption history , provides a good recommendation solution to the scenarios where overlap in historical consumption across users is relatively high and the content universe is almost static . Content - based ﬁltering helps to identify new items which well match an WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 661 existing user’s consumption proﬁle , but the recommended items are always similar to the items previously taken by the user [ 20 ] . Hybrid approaches [ 11 ] have been developed by combining two or more recommendation techniques ; for example , the inability of collaborative ﬁltering to recommend new items is commonly alle - viated by combining it with content - based ﬁltering . However , as noted above , in many web - based scenarios , the con - tent universe undergoes frequent changes , with content popular - ity changing over time as well . Furthermore , a signiﬁcant num - ber of visitors are likely to be entirely new with no historical con - sumption record whatsoever ; this is known as a cold - start situa - tion [ 21 ] . These issues make traditional recommender - system ap - proaches difﬁcult to apply , as shown by prior empirical studies [ 12 ] . It thus becomes indispensable to learn the goodness of match be - tween user interests and content when one or both of them are new . However , acquiring such information can be expensive and may reduce user satisfaction in the short term , raising the question of optimally balancing the two competing goals : maximizing user sat - isfaction in the long run , and gathering information about goodness of match between user interests and content . The above problem is indeed known as a feature - based explo - ration / exploitation problem . In this paper , we formulate it as a con - textual bandit problem , a principled approach in which a learning algorithm sequentially selects articles to serve users based on con - textual information of the user and articles , while simultaneously adapting its article - selection strategy based on user - click feedback to maximize total user clicks in the long run . We deﬁne a bandit problem and then review some existing approaches in Section 2 . Then , we propose a new algorithm , LinUCB , in Section 3 which has a similar regret analysis to the best known algorithms for com - peting with the best linear predictor , with a lower computational overhead . We also address the problem of ofﬂine evaluation in Section 4 , showing this is possible for any explore / exploit strat - egy when interactions are independent and identically distributed ( i . i . d . ) , as might be a reasonable assumption for different users . We then test our new algorithm and several existing algorithms using this ofﬂine evaluation strategy in Section 5 . 2 . FORMULATION & RELATED WORK In this section , we deﬁne the K - armed contextual bandit prob - lem formally , and as an example , show how it can model the per - sonalized news article recommendation problem . We then discuss existing methods and their limitations . 2 . 1 A Multi - armed Bandit Formulation The problem of personalized news article recommendation can be naturally modeled as a multi - armed bandit problem with context information . Following previous work [ 18 ] , we call it a contextual bandit . 1 Formally , a contextual - bandit algorithm A proceeds in dis - crete trials t = 1 , 2 , 3 , . . . In trial t : 1 . The algorithm observes the current user u t and a set A t of arms or actions together with their feature vectors x t , a for a ∈A t . The vector x t , a summarizes information of both the user u t and arm a , and will be referred to as the context . 2 . Based on observed payoffs in previous trials , A chooses an arm a t ∈A t , and receives payoff r t , a t whose expectation depends on both the user u t and the arm a t . 3 . The algorithm then improves its arm - selection strategy with the new observation , ( x t , a t , a t , r t , a t ) . It is important to em - 1 In the literature , contextual bandits are sometimes called bandits with covariate , bandits with side information , associative bandits , and associative reinforcement learning . phasize here that no feedback ( namely , the payoff r t , a ) is observed for unchosen arms a # = a t . The consequence of this fact is discussed in more details in the next subsection . In the process above , the total T - trial payoff of A is deﬁned as P Tt = 1 r t , a t . Similarly , we deﬁne the optimal expected T - trial pay - off as E hP Tt = 1 r t , a ∗ t i , where a ∗ t is the arm with maximum ex - pected payoff at trial t . Our goal is to design A so that the expected total payoff above is maximized . Equivalently , we may ﬁnd an al - gorithm so that its regret with respect to the optimal arm - selection strategy is minimized . Here , the T - trial regret R A ( T ) of algorithm A is deﬁned formally by R A ( T ) def = E " T X t = 1 r t , a ∗ t # − E " T X t = 1 r t , a t # . ( 1 ) An important special case of the general contextual bandit prob - lem is the well - known K - armed bandit in which ( i ) the arm set A t remains unchanged and contains K arms for all t , and ( ii ) the user u t ( or equivalently , the context ( x t , 1 , · · · , x t , K ) ) is the same for all t . Since both the arm set and contexts are constant at every trial , they make no difference to a bandit algorithm , and so we will also refer to this type of bandit as a context - free bandit . In the context of article recommendation , we may view articles in the pool as arms . When a presented article is clicked , a payoff of 1 is incurred ; otherwise , the payoff is 0 . With this deﬁnition of payoff , the expected payoff of an article is precisely its click - through rate ( CTR ) , and choosing an article with maximum CTR is equivalent to maximizing the expected number of clicks from users , which in turn is the same as maximizing the total expected payoff in our bandit formulation . Furthermore , in web services we often have access to user infor - mation which can be used to infer a user’s interest and to choose news articles that are probably most interesting to her . For example , it is much more likely for a male teenager to be interested in an arti - cle about iPod products rather than retirement plans . Therefore , we may “summarize” users and articles by a set of informative features that describe them compactly . By doing so , a bandit algorithm can generalize CTR information from one article / user to another , and learn to choose good articles more quickly , especially for new users and articles . 2 . 2 Existing Bandit Algorithms The fundamental challenge in bandit problems is the need for balancing exploration and exploitation . To minimize the regret in Eq . ( 1 ) , an algorithm A exploits its past experience to select the arm that appears best . On the other hand , this seemingly optimal arm may in fact be suboptimal , due to imprecision in A ’s knowledge . In order to avoid this undesired situation , A has to explore by actually choosing seemingly suboptimal arms so as to gather more informa - tion about them ( c . f . , step 3 in the bandit process deﬁned in the pre - vious subsection ) . Exploration can increase short - term regret since some suboptimal arms may be chosen . However , obtaining infor - mation about the arms’ average payoffs ( i . e . , exploration ) can re - ﬁne A ’s estimate of the arms’ payoffs and in turn reduce long - term regret . Clearly , neither a purely exploring nor a purely exploiting algorithm works best in general , and a good tradeoff is needed . The context - free K - armed bandit problem has been studied by statisticians for a long time [ 9 , 24 , 26 ] . One of the simplest and most straightforward algorithms is ! - greedy . In each trial t , this algorithm ﬁrst estimates the average payoff ˆ µ t , a of each arm a . Then , with probability 1 − ! , it chooses the greedy arm ( i . e . , the arm with highest payoff estimate ) ; with probability ! , it chooses a random arm . In the limit , each arm will be tried inﬁnitely often , WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 662 and so the payoff estimate ˆ µ t , a converges to the true value µ a with probability 1 . Furthermore , by decaying ! appropriately ( e . g . , [ 24 ] ) , the per - step regret , R A ( T ) / T , converges to 0 with probability 1 . In contrast to the unguided exploration strategy adopted by ! - greedy , another class of algorithms generally known as upper con - ﬁdence bound algorithms [ 4 , 7 , 17 ] use a smarter way to balance exploration and exploitation . Speciﬁcally , in trial t , these algo - rithms estimate both the mean payoff ˆ µ t , a of each arm a as well as a corresponding conﬁdence interval c t , a , so that | ˆ µ t , a − µ a | < c t , a holds with high probability . They then select the arm that achieves a highest upper conﬁdence bound ( UCB for short ) : a t = arg max a ( ˆ µ t , a + c t , a ) . With appropriately deﬁned conﬁdence in - tervals , it can be shown that such algorithms have a small total T - trial regret that is only logarithmic in the total number of trials T , which turns out to be optimal [ 17 ] . While context - free K - armed bandits are extensively studied and well understood , the more general contextual bandit problem has remained challenging . The EXP4 algorithm [ 8 ] uses the exponen - tial weighting technique to achieve an ˜ O ( √ T ) regret , 2 but the com - putational complexity may be exponential in the number of fea - tures . Another general contextual bandit algorithm is the epoch - greedy algorithm [ 18 ] that is similar to ! - greedy with shrinking ! . This algorithm is computationally efﬁcient given an oracle opti - mizer but has the weaker regret guarantee of ˜ O ( T 2 / 3 ) . Algorithms with stronger regret guarantees may be designed un - der various modeling assumptions about the bandit . Assuming the expected payoff of an arm is linear in its features , Auer [ 6 ] de - scribes the LinRel algorithm that is essentially a UCB - type ap - proach and shows that one of its variants has a regret of ˜ O ( √ T ) , a signiﬁcant improvement over earlier algorithms [ 1 ] . Finally , we note that there exist another class of bandit al - gorithms based on Bayes rule , such as Gittins index meth - ods [ 15 ] . With appropriately deﬁned prior distributions , Bayesian approaches may have good performance . These methods require extensive ofﬂine engineering to obtain good prior models , and are often computationally prohibitive without coupling with approxi - mation techniques [ 2 ] . 3 . ALGORITHM Given asymptotic optimality and the strong regret bound of UCB methods for context - free bandit algorithms , it is tempting to de - vise similar algorithms for contextual bandit problems . Given some parametric form of payoff function , a number of methods exist to estimate from data the conﬁdence interval of the parameters with which we can compute a UCB of the estimated arm payoff . Such an approach , however , is expensive in general . In this work , we show that a conﬁdence interval can be com - puted efﬁciently in closed form when the payoff model is linear , and call this algorithm LinUCB . For convenience of exposition , we ﬁrst describe the simpler form for disjoint linear models , and then consider the general case of hybrid models in Section 3 . 2 . We note LinUCB is a generic contextual bandit algorithms which applies to applications other than personalized news article recommendation . 3 . 1 LinUCB with Disjoint Linear Models Using the notation of Section 2 . 1 , we assume the expected payoff of an arm a is linear in its d - dimensional feature x t , a with some unknown coefﬁcient vector θθθ ∗ a ; namely , for all t , E [ r t , a | x t , a ] = x " t , a θθθ ∗ a . ( 2 ) This model is called disjoint since the parameters are not shared 2 Note ˜ O ( · ) is the same as O ( · ) but suppresses logarithmic factors . among different arms . Let D a be a design matrix of dimension m × d at trial t , whose rows correspond to m training inputs ( e . g . , m contexts that are observed previously for article a ) , and b a ∈ R m be the corresponding response vector ( e . g . , the corresponding m click / no - click user feedback ) . Applying ridge regression to the training data ( D a , c a ) gives an estimate of the coefﬁcients : ˆ θθθ a = ( D " a D a + I d ) − 1 D " a c a , ( 3 ) where I d is the d × d identity matrix . When components in c a are independent conditioned on corresponding rows in D a , it can be shown [ 27 ] that , with probability at least 1 − δ , ˛˛˛ x " t , a ˆ θθθ a − E [ r t , a | x t , a ] ˛˛˛ ≤ α q x " t , a ( D " a D a + I d ) − 1 x t , a ( 4 ) for any δ > 0 and x t , a ∈ R d , where α = 1 + p ln ( 2 / δ ) / 2 is a constant . In other words , the inequality above gives a reasonably tight UCB for the expected payoff of arm a , from which a UCB - type arm - selection strategy can be derived : at each trial t , choose a t def = arg max a ∈A t „ x " t , a ˆ θθθ a + α q x " t , a A − 1 a x t , a « , ( 5 ) where A a def = D " a D a + I d . The conﬁdence interval in Eq . ( 4 ) may be motivated and derived from other principles . For instance , ridge regression can also be interpreted as a Bayesian point estimate , where the posterior dis - tribution of the coefﬁcient vector , denoted as p ( θθθ a ) , is Gaussian with mean ˆ θθθ a and covariance A − 1 a . Given the current model , the predictive variance of the expected payoff x " t , a θθθ ∗ a is evaluated as x " t , a A − 1 a x t , a , and then q x " t , a A − 1 a x t , a becomes the standard de - viation . Furthermore , in information theory [ 19 ] , the differential entropy of p ( θθθ a ) is deﬁned as − 12 ln ( ( 2 π ) d det A a ) . The entropy of p ( θθθ a ) when updated by the inclusion of the new point x t , a then becomes − 12 ln ( ( 2 π ) d det ( A a + x t , a x " t , a ) ) . The entropy reduc - tion in the model posterior is 12 ln ( 1 + x " t , a A − 1 a x t , a ) . This quan - tity is often used to evaluate model improvement contributed from x t , a . Therefore , the criterion for arm selection in Eq . ( 5 ) can also be regarded as an additive trade - off between the payoff estimate and model uncertainty reduction . Algorithm 1 gives a detailed description of the entire LinUCB algorithm , whose only input parameter is α . Note the value of α given in Eq . ( 4 ) may be conservatively large in some applications , and so optimizing this parameter may result in higher total payoffs in practice . Like all UCB methods , LinUCB always chooses the arm with highest UCB ( as in Eq . ( 5 ) ) . This algorithm has a few nice properties . First , its computational complexity is linear in the number of arms and at most cubic in the number of features . To decrease computation further , we may update A a t in every step ( which takes O ( d 2 ) time ) , but compute and cache Q a def = A − 1 a ( for all a ) periodically instead of in real - time . Second , the algorithm works well for a dynamic arm set , and remains efﬁcient as long as the size of A t is not too large . This case is true in many applications . In news article recommendation , for instance , editors add / remove articles to / from a pool and the pool size remains essentially constant . Third , although it is not the focus of the present paper , we can adapt the analysis from [ 6 ] to show the following : if the arm set A t is ﬁxed and contains K arms , then the conﬁdence interval ( i . e . , the right - hand side of Eq . ( 4 ) ) decreases fast enough with more and more data , and then prove the strong regret bound of ˜ O ( √ KdT ) , matching the state - of - the - art result [ 6 ] for bandits satisfying Eq . ( 2 ) . These theoretical results indicate fundamental soundness and efﬁciency of the algorithm . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 663 Algorithm 1 LinUCB with disjoint linear models . 0 : Inputs : α ∈ R + 1 : for t = 1 , 2 , 3 , . . . , T do 2 : Observe features of all arms a ∈A t : x t , a ∈ R d 3 : for all a ∈A t do 4 : if a is new then 5 : A a ← I d ( d - dimensional identity matrix ) 6 : b a ← 0 d × 1 ( d - dimensional zero vector ) 7 : end if 8 : ˆ θθθ a ← A − 1 a b a 9 : p t , a ← ˆ θθθ " a x t , a + α q x " t , a A − 1 a x t , a 10 : end for 11 : Choose arm a t = arg max a ∈A t p t , a with ties broken arbi - trarily , and observe a real - valued payoff r t 12 : A a t ← A a t + x t , a t x " t , a t 13 : b a t ← b a t + r t x t , a t 14 : end for Finally , we note that , under the assumption that input features x t , a were drawn i . i . d . from a normal distribution ( in addition to the modeling assumption in Eq . ( 2 ) ) , Pavlidis et al . [ 22 ] came up with a similar algorithm that uses a least - squares solution ˜ θθθ a instead of our ridge - regression solution ( ˆ θθθ a in Eq . ( 3 ) ) to compute the UCB . However , our approach ( and theoretical analysis ) is more general and remains valid even when input features are nonstationary . More importantly , we will discuss in the next section how to extend the basic Algorithm 1 to a much more interesting case not covered by Pavlidis et al . 3 . 2 LinUCB with Hybrid Linear Models Algorithm 1 ( or the similar algorithm in [ 22 ] ) computes the in - verse of the matrix , D " a D a + I d ( or D " a D a ) , where D a is again the design matrix with rows corresponding to features in the train - ing data . These matrices of all arms have ﬁxed dimension d × d , and can be updated efﬁciently and incrementally . Moreover , their inverses can be computed easily as the parameters in Algorithm 1 are disjoint : the solution ˆ θθθ a in Eq . ( 3 ) is not affected by training data of other arms , and so can be computed separately . We now consider the more interesting case with hybrid models . In many applications including ours , it is helpful to use features that are shared by all arms , in addition to the arm - speciﬁc ones . For example , in news article recommendation , a user may prefer only articles about politics for which this provides a mechanism . Hence , it is helpful to have features that have both shared and non - shared components . Formally , we adopt the following hybrid model by adding another linear term to the right - hand side of Eq . ( 2 ) : E [ r t , a | x t , a ] = z " t , a βββ ∗ + x " t , a θθθ ∗ a , ( 6 ) where z t , a ∈ R k is the feature of the current user / article combina - tion , and βββ ∗ is an unknown coefﬁcient vector common to all arms . This model is hybrid in the sense that some of the coefﬁcients βββ ∗ are shared by all arms , while others θθθ ∗ a are not . For hybrid models , we can no longer use Algorithm 1 as the conﬁdence intervals of various arms are not independent due to the shared features . Fortunately , there is an efﬁcient way to compute an UCB along the same line of reasoning as in the previous sec - tion . The derivation relies heavily on block matrix inversion tech - niques . Due to space limitation , we only give the pseudocode in Algorithm 2 ( where lines 5 and 12 compute the ridge - regression solution of the coefﬁcients , and line 13 computes the conﬁdence interval ) , and leave detailed derivations to a full paper . Here , we Algorithm 2 LinUCB with hybrid linear models . 0 : Inputs : α ∈ R + 1 : A 0 ← I k ( k - dimensional identity matrix ) 2 : b 0 ← 0 k ( k - dimensional zero vector ) 3 : for t = 1 , 2 , 3 , . . . , T do 4 : Observe features of all arms a ∈A t : ( z t , a , x t , a ) ∈ R k + d 5 : ˆ βββ ← A − 1 0 b 0 6 : for all a ∈A t do 7 : if a is new then 8 : A a ← I d ( d - dimensional identity matrix ) 9 : B a ← 0 d × k ( d - by - k zero matrix ) 10 : b a ← 0 d × 1 ( d - dimensional zero vector ) 11 : end if 12 : ˆ θθθ a ← A − 1 a “ b a − B a ˆ βββ ” 13 : s t , a ← z " t , a A − 1 0 z t , a − 2 z " t , a A − 1 0 B " a A − 1 a x t , a + x " t , a A − 1 a x t , a + x " t , a A − 1 a B a A − 1 0 B " a A − 1 a x t , a 14 : p t , a ← z " t , a ˆ βββ + x " t , a ˆ θθθ a + α √ s t , a 15 : end for 16 : Choose arm a t = arg max a ∈A t p t , a with ties broken arbi - trarily , and observe a real - valued payoff r t 17 : A 0 ← A 0 + B " a t A − 1 a t B a t 18 : b 0 ← b 0 + B " a t A − 1 a t b a t 19 : A a t ← A a t + x t , a t x " t , a t 20 : B a t ← B a t + x t , a t z " t , a t 21 : b a t ← b a t + r t x t , a t 22 : A 0 ← A 0 + z t , a t z " t , a t − B " a t A − 1 a t B a t 23 : b 0 ← b 0 + r t z t , a t − B " a t A − 1 a t b a t 24 : end for only point out the important fact that the algorithm is computation - ally efﬁcient since the building blocks in the algorithm ( A 0 , b 0 , A a , B a , and b a ) all have ﬁxed dimensions and can be updated incrementally . Furthermore , quantities associated with arms not existing in A t no longer get involved in the computation . Finally , we can also compute and cache the inverses ( A − 1 0 and A − 1 a ) pe - riodically instead of at the end of each trial to reduce the per - trial computational complexity to O ( d 2 + k 2 ) . 4 . EVALUATION METHODOLOGY Compared to machine learning in the more standard supervised setting , evaluation of methods in a contextual bandit setting is frus - tratingly difﬁcult . Our goal here is to measure the performance of a bandit algorithm π , that is , a rule for selecting an arm at each time step based on the preceding interactions ( such as the algorithms de - scribed above ) . Because of the interactive nature of the problem , it would seem that the only way to do this is to actually run the algo - rithm on “live” data . However , in practice , this approach is likely to be infeasible due to the serious logistical challenges that it presents . Rather , we may only have ofﬂine data available that was collected at a previous time using an entirely different logging policy . Be - cause payoffs are only observed for the arms chosen by the logging policy , which are likely to often differ from those chosen by the algorithm π being evaluated , it is not at all clear how to evaluate π based only on such logged data . This evaluation problem may be viewed as a special case of the so - called “off - policy evaluation problem” in reinforcement learning ( see , c . f . , [ 23 ] ) . One solution is to build a simulator to model the bandit process from the logged data , and then evaluate π with the simulator . How - ever , the modeling step will introduce bias in the simulator and so make it hard to justify the reliability of this simulator - based evalu - WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 664 ation approach . In contrast , we propose an approach that is simple to implement , grounded on logged data , and unbiased . In this section , we describe a provably reliable technique for car - rying out such an evaluation , assuming that the individual events are i . i . d . , and that the logging policy that was used to gather the logged data chose each arm at each time step uniformly at random . Although we omit the details , this latter assumption can be weak - ened considerably so that any randomized logging policy is allowed and our solution can be modiﬁed accordingly using rejection sam - pling , but at the cost of decreased efﬁciency in using data . More precisely , we suppose that there is some unknown dis - tribution D from which tuples are drawn i . i . d . of the form ( x 1 , . . . , x K , r 1 , . . . , r K ) , each consisting of observed feature vec - tors and hidden payoffs for all arms . We also posit access to a large sequence of logged events resulting from the interaction of the log - ging policy with the world . Each such event consists of the context vectors x 1 , . . . , x K , a selected arm a and the resulting observed pay - off r a . Crucially , only the payoff r a is observed for the single arm a that was chosen uniformly at random . For simplicity of presenta - tion , we take this sequence of logged events to be an inﬁnitely long stream ; however , we also give explicit bounds on the actual ﬁnite number of events required by our evaluation method . Our goal is to use this data to evaluate a bandit algorithm π . Formally , π is a ( possibly randomized ) mapping for selecting the arm a t at time t based on the history h t − 1 of t − 1 preceding events , together with the current context vectors x t 1 , . . . , x tK . Our proposed policy evaluator is shown in Algorithm 3 . The method takes as input a policy π and a desired number of “good” events T on which to base the evaluation . We then step through the stream of logged events one by one . If , given the current his - tory h t − 1 , it happens that the policy π chooses the same arm a as the one that was selected by the logging policy , then the event is retained , that is , added to the history , and the total payoff R t up - dated . Otherwise , if the policy π selects a different arm from the one that was taken by the logging policy , then the event is entirely ignored , and the algorithm proceeds to the next event without any other change in its state . Note that , because the logging policy chooses each arm uni - formly at random , each event is retained by this algorithm with probability exactly 1 / K , independent of everything else . This means that the events which are retained have the same distribution as if they were selected by D . As a result , we can prove that two processes are equivalent : the ﬁrst is evaluating the policy against T real - world events from D , and the second is evaluating the policy using the policy evaluator on a stream of logged events . T HEOREM 1 . For all distributions D of contexts , all policies π , all T , and all sequences of events h T , Pr Policy _ Evaluator ( π , S ) ( h T ) = Pr π , D ( h T ) where S is a stream of events drawn i . i . d . from a uniform random logging policy and D . Furthermore , the expected number of events obtained from the stream to gather a history h T of length T is KT . This theorem says that every history h T has the identical prob - ability in the real world as in the policy evaluator . Many statistics of these histories , such as the average payoff R T / T returned by Algorithm 3 , are therefore unbiased estimates of the value of the algorithm π . Further , the theorem states that KT logged events are required , in expectation , to retain a sample of size T . P ROOF . The proof is by induction on t = 1 , . . . , T starting with a base case of the empty history which has probability 1 when t = 0 Algorithm 3 Policy _ Evaluator . 0 : Inputs : T > 0 ; policy π ; stream of events 1 : h 0 ← ∅ { An initially empty history } 2 : R 0 ← 0 { An initially zero total payoff } 3 : for t = 1 , 2 , 3 , . . . , T do 4 : repeat 5 : Get next event ( x 1 , . . . , x K , a , r a ) 6 : until π ( h t − 1 , ( x 1 , . . . , x K ) ) = a 7 : h t ← CONCATENATE ( h t − 1 , ( x 1 , . . . , x K , a , r a ) ) 8 : R t ← R t − 1 + r a 9 : end for 10 : Output : R T / T under both methods of evaluation . In the inductive case , assume that we have for all t − 1 : Pr Policy _ Evaluator ( π , S ) ( h t − 1 ) = Pr π , D ( h t − 1 ) and want to prove the same statement for any history h t . Since the data is i . i . d . and any randomization in the policy is independent of randomization in the world , we need only prove that conditioned on the history h t − 1 the distribution over the t - th event is the same for each process . In other words , we must show : Pr Policy _ Evaluator ( π , S ) ( ( x t , 1 , . . . , x t , K , a , r t , a ) | h t − 1 ) = Pr D ( x t , 1 , . . . , x t , K , r t , a ) Pr π ( h t − 1 ) ( a | x t , 1 , . . . , x t , K ) . Since the arm a is chosen uniformly at random in the logging pol - icy , the probability that the policy evaluator exits the inner loop is identical for any policy , any history , any features , and any arm , im - plying this happens for the last event with the probability of the last event , Pr D ( x t , 1 , . . . , x t , K , r t , a ) . Similarly , since the policy π ’s distribution over arms is independent conditioned on the history h t − 1 and features ( x t , 1 , . . . , x t , K ) , the probability of arm a is just Pr π ( h t − 1 ) ( a | x t , 1 , . . . , x t , K ) . Finally , since each event from the stream is retained with proba - bility exactly 1 / K , the expected number required to retain T events is exactly KT . 5 . EXPERIMENTS In this section , we verify the capacity of the proposed LinUCB algorithm on a real - world application using the ofﬂine evaluation method of Section 4 . We start with an introduction of the problem setting in Yahoo ! Today - Module , and then describe the user / item attributes we used in experiments . Finally , we deﬁne performance metrics and report experimental results with comparison to a few standard ( contextual ) bandit algorithms . 5 . 1 Yahoo ! Today Module The Today Module is the most prominent panel on the Yahoo ! Front Page , which is also one of the most visited pages on the In - ternet ; see a snapshot in Figure 1 . The default “Featured” tab in the Today Module highlights one of four high - quality articles , mainly news , while the four articles are selected from an hourly - refreshed article pool curated by human editors . As illustrated in Figure 1 , there are four articles at footer positions , indexed by F1 – F4 . Each article is represented by a small picture and a title . One of the four articles is highlighted at the story position , which is featured by a large picture , a title and a short summary along with related links . By default , the article at F1 is highlighted at the story position . A WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 665 Figure 1 : A snapshot of the “Featured” tab in the Today Mod - ule on Yahoo ! Front Page . By default , the article at F1 position is highlighted at the story position . user can click on the highlighted article at the story position to read more details if she is interested in the article . The event is recorded as a story click . To draw visitors’ attention , we would like to rank available articles according to individual interests , and highlight the most attractive article for each visitor at the story position . 5 . 2 Experiment Setup This subsection gives a detailed description of our experimental setup , including data collection , feature construction , performance evaluation , and competing algorithms . 5 . 2 . 1 Data Collection We collected events from a random bucket in May 2009 . Users were randomly selected to the bucket with a certain probability per visiting view . 3 In this bucket , articles were randomly selected from the article pool to serve users . To avoid exposure bias at footer positions , we only focused on users’ interactions with F1 articles at the story position . Each user interaction event consists of three components : ( i ) the random article chosen to serve the user , ( ii ) user / article information , and ( iii ) whether the user clicks on the ar - ticle at the story position . Section 4 shows these random events can be used to reliably evaluate a bandit algorithm’s expected payoff . There were about 4 . 7 million events in the random bucket on May 01 . We used this day’s events ( called “tuning data” ) for model validation to decide the optimal parameter for each competing ban - dit algorithm . Then we ran these algorithms with tuned parameters on a one - week event set ( called “evaluation data” ) in the random bucket from May 03 – 09 , which contained about 36 million events . 5 . 2 . 2 Feature Construction We now describe the user / article features constructed for our ex - periments . Two sets of features for the disjoint and hybrid models , respectively , were used to test the two forms of LinUCB in Sec - tion 3 and to verify our conjecture that hybrid models can improve learning speed . We start with raw user features that were selected by “support” . The support of a feature is the fraction of users having that feature . To reduce noise in the data , we only selected features with high support . Speciﬁcally , we used a feature when its support is at least 0 . 1 . Then , each user was originally represented by a raw feature vector of over 1000 categorical components , which include : ( i ) de - mographic information : gender ( 2 classes ) and age discretized into 10 segments ; ( ii ) geographic features : about 200 metropolitan lo - cations worldwide and U . S . states ; and ( iii ) behavioral categories : 3 We call it view - based randomization . After refreshing her browser , the user may not fall into the random bucket again . about 1000 binary categories that summarize the user’s consump - tion history within Yahoo ! properties . Other than these features , no other information was used to identify a user . Similarly , each article was represented by a raw feature vector of about 100 categorical features constructed in the same way . These features include : ( i ) URL categories : tens of classes inferred from the URL of the article resource ; and ( ii ) editor categories : tens of topics tagged by human editors to summarize the article content . We followed a previous procedure [ 12 ] to encode categorical user / article features as binary vectors and then normalize each fea - ture vector to unit length . We also augmented each feature vector with a constant feature of value 1 . Now each article and user was represented by a feature vector of 83 and 1193 entries , respectively . To further reduce dimensionality and capture nonlinearity in these raw features , we carried out conjoint analysis based on ran - dom exploration data collected in September 2008 . Following a previous approach to dimensionality reduction [ 13 ] , we projected user features onto article categories and then clustered users with similar preferences into groups . More speciﬁcally : • We ﬁrst used logistic regression ( LR ) to ﬁt a bilinear model for click probability given raw user / article features so that φ φ φ " u W φ φ φ a approximated the probability that the user u clicks on article a , where φφφ u and φφφ a were the corresponding feature vectors , and W was a weight matrix optimized by LR . • Raw user features were then projected onto an induced space by computing ψψψ u def = φφφ " u W . Here , the i th component in ψψψ u for user u may be interpreted as the degree to which the user likes the i th category of articles . K - means was applied to group users in the induced ψψψ u space into 5 clusters . • The ﬁnal user feature was a six - vector : ﬁve entries corre - sponded to membership of that user in these 5 clusters ( com - puted with a Gaussian kernel and then normalized so that they sum up to unity ) , and the sixth was a constant feature 1 . At trial t , each article a has a separate six - dimensional feature x t , a that is exactly the six - dimensional feature constructed as above for user u t . Since these article features do not overlap , they are for disjoint linear models deﬁned in Section 3 . For each article a , we performed the same dimensionality reduc - tion to obtain a six - dimensional article feature ( including a constant 1 feature ) . Its outer product with a user feature gave 6 × 6 = 36 features , denoted z t , a ∈ R 36 , that corresponded to the shared fea - tures in Eq . ( 6 ) , and thus ( z t , a , x t , a ) could be used in the hybrid linear model . Note the features z t , a contains user - article interac - tion information , while x t , a contains user information only . Here , we intentionally used ﬁve users ( and articles ) groups , which has been shown to be representative in segmentation anal - ysis [ 13 ] . Another reason for using a relatively small feature space is that , in online services , storing and retrieving large amounts of user / article information will be too expensive to be practical . 5 . 3 Compared Algorithms The algorithms empirically evaluated in our experiments can be categorized into three groups : I . Algorithms that make no use of features . These correspond to the context - free K - armed bandit algorithms that ignore all contexts ( i . e . , user / article information ) . • random : A random policy always chooses one of the candi - date articles from the pool with equal probability . This algo - rithm requires no parameters and does not “learn” over time . • ! - greedy : As described in Section 2 . 2 , it estimates each arti - cle’s CTR ; then it chooses a random article with probability ! , and chooses the article of the highest CTR estimate with probability 1 − ! . The only parameter of this policy is ! . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 666 1 1 . 2 1 . 4 1 . 6 1 . 8 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 c t r ε ε - greedy ε - greedy ( warm ) ε - greedy ( seg ) ε - greedy ( disjoint ) ε - greedy ( hybrid ) omniscient ( a ) Deployment bucket . 1 1 . 2 1 . 4 1 . 6 1 . 8 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 1 . 2 1 . 4 c t r α ucb ucb ( warm ) ucb ( seg ) linucb ( disjoint ) linucb ( hybrid ) omniscient ( b ) Deployment bucket . 1 1 . 2 1 . 4 1 . 6 1 . 8 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 c t r ε ε - greedy ε - greedy ( warm ) ε - greedy ( seg ) ε - greedy ( disjoint ) ε - greedy ( hybrid ) omniscient ( c ) Learning bucket . 1 1 . 2 1 . 4 1 . 6 1 . 8 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 1 . 2 1 . 4 c t r α ucb ucb ( warm ) ucb ( seg ) linucb ( simple ) linucb ( hybrid ) omniscient ( d ) Learning bucket . Figure 2 : Parameter tuning : CTRs of various algorithms on the one - day tuning dataset . • ucb : As described in Section 2 . 2 , this policy estimates each article’s CTR as well as a conﬁdence interval of the estimate , and always chooses the article with the highest UCB . Speciﬁ - cally , following UCB1 [ 7 ] , we computed an article a ’s conﬁ - dence interval by c t , a = α √ n t , a , where n t , a is the number of times a was chosen prior to trial t , and α > 0 is a parameter . • omniscient : Such a policy achieves the best empirical context - free CTR from hindsight . It ﬁrst computes each ar - ticle’s empirical CTR from logged events , and then always chooses the article with highest empircal CTR when it is evaluated using the same logged events . This algorithm re - quires no parameters and does not “learn” over time . II . Algorithms with “warm start” —an intermediate step towards personalized services . The idea is to provide an ofﬂine - estimated user - speciﬁc adjustment on articles’ context - free CTRs over the whole trafﬁc . The offset serves as an initialization on CTR estimate for new content , a . k . a . “warm start” . We re - trained the bilinear lo - gistic regression model studied in [ 12 ] on Sept 2008 random trafﬁc data , using features z t , a constructed above . The selection criterion then becomes the sum of the context - free CTR estimate and a bi - linear term for a user - speciﬁc CTR adjustment . In training , CTR was estimated using the context - free ! - greedy with ! = 1 . • ! - greedy ( warm ) : This algorithm is the same as ! - greedy except it adds the user - speciﬁc CTR correction to the article’s context - free CTR estimate . • ucb ( warm ) : This algorithm is the same as the previous one but replaces ! - greedy with ucb . III . Algorithms that learn user - speciﬁc CTRs online . • ! - greedy ( seg ) : Each user is assigned to the closest user cluster among the ﬁve constructed in Section 5 . 2 . 2 , and so all users are partitioned into ﬁve groups ( a . k . a . user segments ) , in each of which a separate copy of ! - greedy was run . • ucb ( seg ) : This algorithm is similar to ! - greedy ( seg ) ex - cept it ran a copy of ucb in each of the ﬁve user segments . • ! - greedy ( disjoint ) : This is ! - greedy with disjoint models , and may be viewed as a close variant of epoch - greedy [ 18 ] . • linucb ( disjoint ) : This is Algorithm 1 with disjoint models . • ! - greedy ( hybrid ) : This is ! - greedy with hybrid models , and may be viewed as a close variant of epoch - greedy . • linucb ( hybrid ) : This is Algorithm 2 with hybrid models . 5 . 4 Performance Metric An algorithm’s CTR is deﬁned as the ratio of the number of clicks it receives and the number of steps it is run . We used all algorithms’ CTRs on the random logged events for performance comparison . To protect business - sensitive information , we report an algorithm’s relative CTR , which is the algorithm’s CTR divided by the random policy’s . Therefore , we will not report a random pol - icy’s relative CTR as it is always 1 by deﬁnition . For convenience , we will use the term “CTR” from now on instead of “relative CTR” . For each algorithm , we are interested in two CTRs motivated by our application , which may be useful for other similar applica - tions . When deploying the methods to Yahoo ! ’s front page , one reasonable way is to randomly split all trafﬁc to this page into two buckets [ 3 ] . The ﬁrst , called “learning bucket” , usually consists of a small fraction of trafﬁc on which various bandit algorithms are run to learn / estimate article CTRs . The other , called “deployment bucket” , is where Yahoo ! Front Page greedily serves users using CTR estimates obained from the learning bucket . Note that “learn - ing” and “deployment” are interleaved in this problem , and so in every view falling into the deployment bucket , the article with the highest current ( user - speciﬁc ) CTR estimate is chosen ; this esti - mate may change later if the learning bucket gets more data . CTRs in both buckets were estimated with Algorithm 3 . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 667 algorithm size = 100 % size = 30 % size = 20 % size = 10 % size = 5 % size = 1 % deploy learn deploy learn deploy learn deploy learn deploy learn deploy learn ! - greedy 1 . 596 1 . 326 1 . 541 1 . 326 1 . 549 1 . 273 1 . 465 1 . 326 1 . 409 1 . 292 1 . 234 1 . 139 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % ucb 1 . 594 1 . 569 1 . 582 1 . 535 1 . 569 1 . 488 1 . 541 1 . 446 1 . 541 1 . 465 1 . 354 1 . 22 0 % 18 . 3 % 2 . 7 % 15 . 8 % 1 . 3 % 16 . 9 % 5 . 2 % 9 % 9 . 4 % 13 . 4 % 9 . 7 % 7 . 1 % ! - greedy ( seg ) 1 . 742 1 . 446 1 . 652 1 . 46 1 . 585 1 . 119 1 . 474 1 . 284 1 . 407 1 . 281 1 . 245 1 . 072 9 . 1 % 9 % 7 . 2 % 10 . 1 % 2 . 3 % − 12 % 0 . 6 % − 3 . 1 % 0 % − 0 . 8 % 0 . 9 % − 5 . 8 % ucb ( seg ) 1 . 781 1 . 677 1 . 742 1 . 555 1 . 689 1 . 446 1 . 636 1 . 529 1 . 532 1 . 32 1 . 398 1 . 25 11 . 6 % 26 . 5 % 13 % 17 . 3 % 9 % 13 . 6 % 11 . 7 % 15 . 3 % 8 . 7 % 2 . 2 % 13 . 3 % 9 . 7 % ! - greedy ( disjoint ) 1 . 769 1 . 309 1 . 686 1 . 337 1 . 624 1 . 529 1 . 529 1 . 451 1 . 432 1 . 345 1 . 262 1 . 183 10 . 8 % − 1 . 2 % 9 . 4 % 0 . 8 % 4 . 8 % 20 . 1 % 4 . 4 % 9 . 4 % 1 . 6 % 4 . 1 % 2 . 3 % 3 . 9 % linucb ( disjoint ) 1 . 795 1 . 647 1 . 719 1 . 507 1 . 714 1 . 384 1 . 655 1 . 387 1 . 574 1 . 245 1 . 382 1 . 197 12 . 5 % 24 . 2 % 11 . 6 % 13 . 7 % 10 . 7 % 8 . 7 % 13 % 4 . 6 % 11 . 7 % − 3 . 5 % 12 % 5 . 1 % ! - greedy ( hybrid ) 1 . 739 1 . 521 1 . 68 1 . 345 1 . 636 1 . 449 1 . 58 1 . 348 1 . 465 1 . 415 1 . 342 1 . 2 9 % 14 . 7 % 9 % 1 . 4 % 5 . 6 % 13 . 8 % 7 . 8 % 1 . 7 % 4 % 9 . 5 % 8 . 8 % 5 . 4 % linucb ( hybrid ) 1 . 73 1 . 663 1 . 691 1 . 591 1 . 708 1 . 619 1 . 675 1 . 535 1 . 588 1 . 507 1 . 482 1 . 446 8 . 4 % 25 . 4 % 9 . 7 % 20 % 10 . 3 % 27 . 2 % 14 . 3 % 15 . 8 % 12 . 7 % 16 . 6 % 20 . 1 % 27 % Table 1 : Performance evaluation : CTRs of all algorithms on the one - week evaluation dataset in the deployment and learning buckets ( denoted by “deploy” and “learn” in the table , respectively ) . The numbers with a percentage is the CTR lift compared to ! - greedy . Since the deployment bucket is often larger than the learning bucket , CTR in the deployment bucket is more important . How - ever , a higher CTR in the learning bucket suggests a faster learning rate ( or equivalently , smaller regret ) for a bandit algorithm . There - fore , we chose to report algorithm CTRs in both buckets . 5 . 5 Experimental Results 5 . 5 . 1 Results for Tuning Data Each of the competing algorithms ( except random and omni - scient ) in Section 5 . 3 requires a single parameter : ! for ! - greedy algorithms and α for UCB ones . We used tuning data to optimize these parameters . Figure 2 shows how the CTR of each algorithm changes with respective parameters . All results were obtained by a single run , but given the size of our dataset and the unbiasedness result in Theorem 1 , the reported numbers are statistically reliable . First , as seen from Figure 2 , the CTR curves in the learning buck - ets often possess the inverted U - shape . When the parameter ( ! or α ) is too small , there was insufﬁcient exploration , the algorithms failed to identify good articles , and had a smaller number of clicks . On the other hand , when the parameter is too large , the algorithms appeared to over - explore and thus wasted some of the opportunities to increase the number of clicks . Based on these plots on tuning data , we chose appropriate parameters for each algorithm and ran it once on the evaluation data in the next subsection . Second , it can be concluded from the plots that warm - start in - formation is indeed helpful for ﬁnding a better match between user interest and article content , compared to the no - feature versions of ! - greedy and UCB . Speciﬁcally , both ! - greedy ( warm ) and ucb ( warm ) were able to beat omniscient , the highest CTRs achiev - able by context - free policies in hindsight . However , performance of the two algorithms using warm - start information is not as stable as algorithms that learn the weights online . Since the ofﬂine model for “warm start” was trained with article CTRs estimated on all ran - dom trafﬁc [ 12 ] , ! - greedy ( warm ) gets more stable performance in the deployment bucket when ! is close to 1 . The warm start part also helps ucb ( warm ) in the learning bucket by selecting more at - tractive articles to users from scratch , but did not help ucb ( warm ) in determining the best online for deployment . Since ucb relies on the a conﬁdence interval for exploration , it is hard to correct the initialization bias introduced by “warm start” . In contrast , all online - learning algorithms were able to consistently beat the omni - scient policy . Therefore , we did not try the warm - start algorithms on the evaluation data . Third , ! - greedy algorithms ( on the left of Figure 2 ) achieved sim - ilar CTR as upper conﬁdence bound ones ( on the right of Figure 2 ) in the deployment bucket when appropriate parameters were used . Thus , both types of algorithms appeared to learn comparable poli - cies . However , they seemed to have lower CTR in the learning bucket , which is consistent with the empirical ﬁndings of context - free algorithms [ 2 ] in real bucket tests . Finally , to compare algorithms when data are sparse , we repeated the same parameter tuning process for each algorithm with fewer data , at the level of 30 % , 20 % , 10 % , 5 % , and 1 % . Note that we still used all data to evaluate an algorithm’s CTR as done in Algo - rithm 3 , but then only a fraction of available data were randomly chosen to be used by the algorithm to improve its policy . 5 . 5 . 2 Results for Evaluation Data With parameters optimized on the tuning data ( c . f . , Figure 2 ) , we ran the algorithms on the evaluation data and summarized the CTRs in Table 1 . The table also reports the CTR lift compared to the baseline of ! - greedy . The CTR of omniscient was 1 . 615 , and so a signiﬁcantly larger CTR of an algorithm indicates its effective use of user / article features for personalization . Recall that the reported CTRs were normalized by the random policy’s CTR . We examine the results more closely in the following subsections . On the Use of Features . We ﬁrst investigate whether it helps to use features in article rec - ommendation . It is clear from Table 1 that , by considering user features , both ! - greedy ( seg / disjoint / hybrid ) and UCB methods ( ucb ( seg ) and linucb ( disjoint / hybrid ) ) were able to achieve a CTR lift of around 10 % , compared to the baseline ! - greedy . To better visualize the effect of features , Figure 3 shows how an article’s CTR ( when chosen by an algorithm ) was lifted compared to its base CTR ( namely , the context - free CTR ) . 4 Here , an article’s base CTR measures how interesting it is to a random user , and was estimated from logged events . Therefore , a high ratio of the lifted and base CTRs of an article is a strong indicator that an algorithm does recommend this article to potentially interested users . Fig - ure 3 ( a ) shows neither ! - greedy nor ucb was able to lift article CTRs , since they made no use of user information . In contrast , all 4 To avoid inaccurate CTR estimates , only 50 articles that were chosen most often by an algorithm were included in its own plots . Hence , the plots for different algorithms are not comparable . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 668 0 1 2 3 0 1 2 3 li ft ed c t r base ctr ( a ) ! - greedy and ucb 0 1 2 3 0 1 2 3 li ft ed c t r base ctr ( b ) seg : ! - greedy and ucb 0 1 2 3 0 1 2 3 li ft ed c t r base ctr ( c ) disjoint : ! - greedy and linucb 0 1 2 3 0 1 2 3 li ft ed c t r base ctr ( d ) hybrid : ! - greedy and linucb Figure 3 : Scatterplots of the base CTR vs . lifted CTR ( in the learning bucket ) of the 50 most frequently selected articles when 100 % evaluation data were used . Red crosses are for ! - greedy algorithms , and blue circles are for UCB algorithms . Note that the sets of most frequently chosen articles varied with algorithms ; see the text for details . the other three plots show clear beneﬁts by considering personal - ized recommendation . In an extreme case ( Figure 3 ( c ) ) , one of the article’s CTR was lifted from 1 . 31 to 3 . 03 —a 132 % improvement . Furthermore , it is consistent with our previous results on tuning data that , compared to ! - greedy algorithms , UCB methods achieved higher CTRs in the deployment bucket , and the advantage was even greater in the learning bucket . As mentioned in Section 2 . 2 , ! - greedy approaches are unguided because they choose articles uni - formly at random for exploration . In contrast , exploration in upper conﬁdence bound methods are effectively guided by conﬁdence intervals—a measure of uncertainty in an algorithm’s CTR esti - mate . Our experimental results imply the effectiveness of upper conﬁdence bound methods and we believe they have similar bene - ﬁts in many other applications as well . On the Size of Data . One of the challenges in personalized web services is the scale of the applications . In our problem , for example , a small pool of news articles were hand - picked by human editors . But if we wish to allow more choices or use automated article selection methods to determine the article pool , the number of articles can be too large even for the high volume of Yahoo ! trafﬁc . Therefore , it becomes critical for an algorithm to quickly identify a good match between user interests and article contents when data are sparse . In our ex - periments , we artiﬁcially reduced data size ( to the levels of 30 % , 20 % , 10 % , 5 % , and 1 % , respectively ) to mimic the situation where we have a large article pool but a ﬁxed volume of trafﬁc . To better visualize the comparison results , we use bar graphs in Figure 4 to plot all algorithms’ CTRs with various data sparsity levels . A few observations are in order . First , at all data sparsity levels , features were still useful . At the level of 1 % , for instance , we observed a 10 . 3 % improvement of linucb ( hybrid ) ’s CTR in the deployment bucket ( 1 . 493 ) over ucb ’s ( 1 . 354 ) . Second , UCB methods consistently outperformed ! - greedy ones in the deployment bucket . 5 The advantage over ! - greedy was even more apparent when data size was smaller . Third , compared to ucb ( seg ) and linucb ( disjoint ) , linucb ( hy - brid ) showed signiﬁcant beneﬁts when data size was small . Re - call that in hybrid models , some features are shared by all articles , making it possible for CTR information of one article to be “trans - ferred” to others . This advantage is particularly useful when the article pool is large . In contrast , in disjoint models , feedback of 5 In the less important learning bucket , there were two exceptions for linucb ( disjoint ) . one article may not be utilized by other articles ; the same is true for ucb ( seg ) . Figure 4 ( a ) shows transfer learning is indeed helpful when data are sparse . Comparing ucb ( seg ) and linucb ( disjoint ) . From Figure 4 ( a ) , it can be seen that ucb ( seg ) and linucb ( dis - joint ) had similar performance . We believe it was no coincidence . Recall that features in our disjoint model are actually normalized membership measures of a user in the ﬁve clusters described in Section 5 . 2 . 2 . Hence , these features may be viewed as a “soft” version of the user assignment process adopted by ucb ( seg ) . Figure 5 plots the histogram of a user’s relative membership measure to the closest cluster , namely , the largest component of the user’s ﬁve , non - constant features . It is clear that most users were quite close to one of the ﬁve cluster centers : the maximum mem - bership of about 85 % users were higher than 0 . 5 , and about 40 % of them were higher than 0 . 8 . Therefore , many of these features have a highly dominating component , making the feature vector similar to the “hard” version of user group assignment . We believe that adding more features with diverse components , such as those found by principal component analysis , would be nec - essary to further distinguish linucb ( disjoint ) from ucb ( seg ) . 6 . CONCLUSIONS This paper takes a contextual - bandit approach to personalized web - based services such as news article recommendation . We pro - posed a simple and reliable method for evaluating bandit algo - rithms directly from logged events , so that the often problematic simulator - building step could be avoided . Based on real Yahoo ! Front Page trafﬁc , we found that upper conﬁdence bound methods generally outperform the simpler yet unguided ! - greedy methods . Furthermore , our new algorithm LinUCB shows advantages when data are sparse , suggesting its effectiveness to personalized web services when the number of contents in the pool is large . In the future , we plan to investigate bandit approaches to other similar web - based serviced such as online advertising , and com - pare our algorithms to related methods such as Banditron [ 16 ] . A second direction is to extend the bandit formulation and algorithms in which an “arm” may refer to a complex object rather than an item ( like an article ) . An example is ranking , where an arm corre - sponds to a permutation of retrieved webpages . Finally , user inter - ests change over time , and so it is interesting to consider temporal information in bandit algorithms . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 669 1 1 . 2 1 . 4 1 . 6 1 . 8 100 % 30 % 20 % 10 % 5 % 1 % c t r data size ε - greedyucb ε - greedy ( seg ) ucb ( seg ) ε - greedy ( disjoint ) linucb ( disjoint ) ε - greedy ( hybrid ) linucb ( hybrid ) omniscient ( a ) CTRs in the deployment bucket . 1 1 . 2 1 . 4 1 . 6 1 . 8 100 % 30 % 20 % 10 % 5 % 1 % c t r data size ( b ) CTRs in the learning bucket . Figure 4 : CTRs in evaluation data with varying data sizes . 0 0 . 05 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 maximum user membership feature Figure 5 : User maximum membership histogram . 7 . ACKNOWLEDGMENTS We thank Deepak Agarwal , Bee - Chung Chen , Daniel Hsu , and Kishore Papineni for many helpful discussions , István Szita and Tom Walsh for clarifying their algorithm , and Taylor Xi and the anonymous reviewers for suggestions that improved the presenta - tion of the paper . 8 . REFERENCES [ 1 ] N . Abe , A . W . Biermann , and P . M . Long . Reinforcement learning with immediate rewards and linear hypotheses . Algorithmica , 37 ( 4 ) : 263 – 293 , 2003 . [ 2 ] D . Agarwal , B . - C . Chen , and P . Elango . Explore / exploit schemes for web content optimization . In Proc . of the 9th International Conf . on Data Mining , 2009 . [ 3 ] D . Agarwal , B . - C . Chen , P . Elango , N . Motgi , S . - T . Park , R . Ramakrishnan , S . Roy , and J . Zachariah . Online models for content optimization . In Advances in Neural Information Processing Systems 21 , pages 17 – 24 , 2009 . [ 4 ] R . Agrawal . Sample mean based index policies with o ( log n ) regret for the multi - armed bandit problem . Advances in Applied Probability , 27 ( 4 ) : 1054 – 1078 , 1995 . [ 5 ] A . Anagnostopoulos , A . Z . Broder , E . Gabrilovich , V . Josifovski , and L . Riedel . Just - in - time contextual advertising . In Proc . of the 16th ACM Conf . on Information and Knowledge Management , pages 331 – 340 , 2007 . [ 6 ] P . Auer . Using conﬁdence bounds for exploitation - exploration trade - offs . Journal of Machine Learning Research , 3 : 397 – 422 , 2002 . [ 7 ] P . Auer , N . Cesa - Bianchi , and P . Fischer . Finite - time analysis of the multiarmed bandit problem . Machine Learning , 47 ( 2 – 3 ) : 235 – 256 , 2002 . [ 8 ] P . Auer , N . Cesa - Bianchi , Y . Freund , and R . E . Schapire . The nonstochastic multiarmed bandit problem . SIAM Journal on Computing , 32 ( 1 ) : 48 – 77 , 2002 . [ 9 ] D . A . Berry and B . Fristedt . Bandit Problems : Sequential Allocation of Experiments . Monographs on Statistics and Applied Probability . Chapman and Hall , 1985 . [ 10 ] P . Brusilovsky , A . Kobsa , and W . Nejdl , editors . The Adaptive Web — Methods and Strategies of Web Personalization , volume 4321 of Lecture Notes in Computer Science . Springer Berlin / Heidelberg , 2007 . [ 11 ] R . Burke . Hybrid systems for personalized recommendations . In B . Mobasher and S . S . Anand , editors , Intelligent Techniques for Web Personalization . Springer - Verlag , 2005 . [ 12 ] W . Chu and S . - T . Park . Personalized recommendation on dynamic content using predictive bilinear models . In Proc . of the 18th International Conf . on World Wide Web , pages 691 – 700 , 2009 . [ 13 ] W . Chu , S . - T . Park , T . Beaupre , N . Motgi , A . Phadke , S . Chakraborty , and J . Zachariah . A case study of behavior - driven conjoint analysis on Yahoo ! : Front Page Today Module . In Proc . of the 15th ACM SIGKDD International Conf . on Knowledge Discovery and Data Mining , pages 1097 – 1104 , 2009 . [ 14 ] A . Das , M . Datar , A . Garg , and S . Rajaram . Google news personalization : scalable online collaborative ﬁltering . In Proc . of the 16th International World Wide Web Conf . , 2007 . [ 15 ] J . Gittins . Bandit processes and dynamic allocation indices . Journal of the Royal Statistical Society . Series B ( Methodological ) , 41 : 148 – 177 , 1979 . [ 16 ] S . M . Kakade , S . Shalev - Shwartz , and A . Tewari . Efﬁcient bandit algorithms for online multiclass prediction . In Proc . of the 25th International Conf . on Machine Learning , pages 440 – 447 , 2008 . [ 17 ] T . L . Lai and H . Robbins . Asymptotically efﬁcient adaptive allocation rules . Advances in Applied Mathematics , 6 ( 1 ) : 4 – 22 , 1985 . [ 18 ] J . Langford and T . Zhang . The epoch - greedy algorithm for contextual multi - armed bandits . In Advances in Neural Information Processing Systems 20 , 2008 . [ 19 ] D . J . C . MacKay . Information Theory , Inference , and Learning Algorithms . Cambridge University Press , 2003 . [ 20 ] D . Mladenic . Text - learning and related intelligent agents : A survey . IEEE Intelligent Agents , pages 44 – 54 , 1999 . [ 21 ] S . - T . Park , D . Pennock , O . Madani , N . Good , and D . DeCoste . Naïve ﬁlterbots for robust cold - start recommendations . In Proc . of the 12th ACM SIGKDD International Conf . on Knowledge Discovery and Data Mining , pages 699 – 705 , 2006 . [ 22 ] N . G . Pavlidis , D . K . Tasoulis , and D . J . Hand . Simulation studies of multi - armed bandits with covariates . In Proceedings on the 10th International Conf . on Computer Modeling and Simulation , pages 493 – 498 , 2008 . [ 23 ] D . Precup , R . S . Sutton , and S . P . Singh . Eligibility traces for off - policy policy evaluation . In Proc . of the 17th Interational Conf . on Machine Learning , pages 759 – 766 , 2000 . [ 24 ] H . Robbins . Some aspects of the sequential design of experiments . Bulletin of the American Mathematical Society , 58 ( 5 ) : 527 – 535 , 1952 . [ 25 ] J . B . Schafer , J . Konstan , and J . Riedi . Recommender systems in e - commerce . In Proc . of the 1st ACM Conf . on Electronic Commerce , 1999 . [ 26 ] W . R . Thompson . On the likelihood that one unknown probability exceeds another in view of the evidence of two samples . Biometrika , 25 ( 3 – 4 ) : 285 – 294 , 1933 . [ 27 ] T . J . Walsh , I . Szita , C . Diuk , and M . L . Littman . Exploring compact reinforcement - learning representations with linear regression . In Proc . of the 25th Conf . on Uncertainty in Artiﬁcial Intelligence , 2009 . WWW 2010 • Full Paper April 26 - 30 • Raleigh • NC • USA 670