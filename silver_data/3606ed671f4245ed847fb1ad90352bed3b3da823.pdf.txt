Procid : Bridging Consensus Building Theory with the Practice of Distributed Design Discussions Roshanak Zilouchian Moghaddam , Zane Nicholson , and Brian P . Bailey Department of Computer Science University of Illinois Urbana , IL 61801 { rzilouc2 , znichol2 , bpbailey } @ illinois . edu ABSTRACT Consensus is a desired but elusive goal in many distributed discussions . A critical problem is that discussion platforms lack mechanisms for realizing consensus strategies and realizing these strategies without tool support can be hard . This paper introduces Procid , a novel browser plugin that provides interaction and visualization features for bringing consensus strategies to distributed design discussions . Key features include the ability to organize discussions around ideas , to register and visualize support for or against ideas , and to define criteria for evaluating ideas . It also applies interaction constraints fostering best practices of consensus building . Procid extends the discussion platform of one open source software community . Two evaluations were conducted . The first collected perceptions of the tool from members of the community for their own discussions . The second compared how Procid affects a distributed design discussion relative to the current discussion platform in the community . Results of both studies showed that users found the features of our tool beneficial and perceived it as more effective for consensus building than the existing platform . Author Keywords Consensus building ; design ; open source ACM Classification Keywords H . 5 . 2 [ Information Interfaces and Presentation ] : User Interfaces - Interaction styles . INTRODUCTION User interface ( UI ) design is a complex , communication - driven process where stakeholders with different interests generate and debate design proposals to solve usability problems and integrate new features . Due to the rise in distributed teams ( e . g . open source software ) , many design discussions now unfold through interactive platforms such as Web forums . Building consensus is desirable in these discussions because it improves participants’ experience and increases commitment to the outcome [ 31 ] . However , existing discussion platforms only offer basic commenting and sometimes voting whereas consensus theory argues for a broader set of strategies [ 5 , 31 ] , e . g . , to track alternatives , define evaluation criteria , and promote a positive tone . Such strategies are not directly supported in existing platforms . Research has shown the need for consensus building in distributed discussions . For example , a study of one large open source software community reported that 42 % of the UI design discussions do not reach consensus [ 35 ] . This can cause loss of community effort , missed opportunities for enhancing the product , and attrition of community members [ 35 ] . In fact , an outflow of Wikipedia volunteers has been attributed to the lack of consensus building mechanisms [ 3 ] . In this paper we describe Procid , a novel browser extension that enables consensus building strategies to be realized in un - moderated distributed design discussions . Key features of the tool include ( i ) the ability to visually organize the discussion around idea proposals ; ( ii ) the ability to define persistent criteria and rate the ideas against the criteria ; ( iii ) the ability to register and visualize strong support for or against ideas , empowering individuals with a stronger voice in the discussion ; ( iv ) interaction constraints that promote best practices of consensus efforts , e . g . , encouraging the first comment on an idea to be supportive of it ; and ( v ) the ability to identify candidates to invite to active discussions using attributes important for consensus in the domain . Consensus building involves a broad set of procedural , rhetorical , and social strategies [ 5 , 20 , 21 , 30 , 31 , 35 ] . Procid prioritizes a subset of these strategies that is feasible to implement and would be difficult to realize without tool support or a moderator ( e . g . organizing a lengthy discussion around the ideas ) . Additional strategies can be implemented as experience is gained or integrated into an onboarding process for new members [ 11 ] . Procid represents a system design pattern that can be mimicked to bring the benefits of consensus building to online deliberation platforms . Procid extends the Web - based discussion platform used for distributed design discussions in one mature open source software community ( Drupal . org ) . This approach allows the features of our tool to be experienced and tested in context Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . CSCW ' 15 , March 14 - 18 2015 , Vancouver , BC , Canada Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 2922 - 4 / 15 / 03… $ 15 . 00 http : / / dx . doi . org / 10 . 1145 / 2675133 . 2675272 Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 686 of authentic discussions in the community . It also enables novel features ( e . g . participant invitations ) by utilizing the social structure and interaction history of the community . Two evaluations of Procid were conducted . For the first , we rendered five active discussions in the community in Procid and invited participants from those discussions to use and provide feedback on the features of the tool and the overall approach . The second evaluation compared how the use of Procid affects a distributed design discussion relative to the existing platform used in the community . Results of both studies showed that users found the features of our tool beneficial for consensus and perceived the tool as more effective for consensus building than the existing platform . RELATED WORK We describe the benefits and prior studies of consensus building and how our work differs from prior tools and visualizations used for collaborative problem solving . Consensus Building In group decision making , consensus is reached when all participants commit to a proposal despite the fact that objections may remain [ 10 ] . The process of building consensus is a distinct form of face - to - face decision making because decisions are reached through mutual consent , each participant is given the power of veto , all stakeholders are invited to participate , and the process is based on openness and trust [ 30 ] . Following a consensus approach can increase understanding of the issues , ensure all solution proposals are considered , and encourage commitment to the outcome [ 30 ] . Above all , participants should feel good about the social process through which the outcome was achieved . Due to its benefits , consensus building is being increasingly utilized for face - to - face decision making , especially in complex domains such as design where no one person has all the required expertise to solve a given problem [ 31 ] . Researchers have recognized the need for and studied consensus building in many distributed communities . For example , Kriplean et al . examined how Wikipedia policies are employed through the consensus building process in talk pages [ 21 ] . The authors recommended building tools that summarize behavior to help identify different aspects of a conflict over time and annotating prior instances of consensus and the participants involved . In open source communities , prior work showed that who is involved in the discussion affects consensus and recommended building interface features that allow users to identify other members who could best contribute to the discussion [ 35 ] . Research in open source has also shown that the types of arguments and rationale applied in distributed discussions need to be better oriented toward consensus [ 20 ] . However , none of these recommendations have been implemented or tested . Without an implementation it is difficult to acquire knowledge about the effectiveness of the recommendations , how users would perceive such a system , or how it would affect consensus . Our research extends prior work on consensus building by developing a system that implements many of the prior recommendations and by evaluating the utility of the system for distributed design discussions . Tools for Group Decision Making The CSCW community has a rich history of building tools that support group decision making . The design of these tools typically varies based on the proximity of group members , group size , and the task [ 13 ] . For instance , in co - located settings , prior work has studied how collaborative use of large or multiple displays affect negotiation [ 9 ] , software development [ 8 ] , and design [ 4 , 27 ] . In distributed settings , researchers have developed Web - based systems to support tasks such as information security planning [ 14 ] . Other research supports decision making by enabling the decision process to be tracked , archived , and reviewed . For example , gIBIS and Compendium support design decision making by tracking issues , positions , and arguments [ 12 , 29 ] . Pathfinder provides an explicit argumentation model and visualization of that model for engaging citizens in distributed discussions of research questions [ 25 ] . Polestar aids intelligence analysts by supporting structured argumentation and fact collection from documents [ 28 ] while CommentSpace integrates a small set of tags and link structure with comments to aid evidence gathering and data exploration during collaborative sense - making [ 33 ] . Our system is original relative to this prior work because it promotes the social process of decision making in addition to visualizing and tracking the discussion . This is achieved by leveraging consensus theory in its design . Our work also contributes results from a study empirically comparing how the use of our tool affects distributed discussions relative to the use of an existing Web platform . Visualization for Collaborative Problem Solving Research has shown that visualization and interaction can foster collaborative problem solving [ 2 , 7 , 23 , 32 ] . For instance , Balakrishnan et al . showed that providing a graph visualization of relevant information and shared interaction promotes topical focus and higher solution rates [ 7 ] . Alonso et al . proposed a similar graph visualization that represents the closeness of opinions to support consensus building in small groups [ 2 ] . Our work is inspired by the success of these types of visualizations for consensus building in lab settings , but our tool provides broader support for the consensus method ( e . g . user - defined criteria for rating ideas ) and addresses the challenge of aiding real design discussions in an existing open source software community . Another thread of research has created visualizations to support large - scale deliberation . For instance , ConsiderIt is a visualization tool that promotes public deliberation by guiding people to reflect on the perspectives of others on topics of civic interest ( e . g . ballot initiatives ) [ 22 ] . Opinion Space is a Web interface for collecting and visualizing users’ opinions on topics such as politics , parenting , and art Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 687 [ 15 ] . While these tools focus on visualizing the deliberation process , our work focuses on implementing and evaluating strategies for consensus building in distributed discussions . STRATEGIES FOR CONSENSUS BUILDING Developing a system that embodies consensus building in distributed design discussions is a big challenge . To address it , we enumerated many consensus building strategies recommended for moderated face - to - face discussions [ 5 , 20 , 21 , 30 , 31 , 35 ] . See Table 1 . The strategies were categorized as social , rhetorical , or procedural . Social strategies relate to who participates in a discussion and their relationships and perceptions . Rhetorical strategies relate to the content of a discussion while procedural strategies relate to how a discussion is organized . As shown in Table 1 , we prioritized strategies that would be feasible to implement , be hard to realize without tool support , and form a coherent user experience . This led us to focus on six strategies : S1 : Promote perception of valued contribution and sense of empowerment . All participants should feel welcome to contribute to a discussion , that their comments are valued , and that they have the power to affect the decision outcome . Enhancing these perceptions promotes understanding of the decision and increased commitment to its implementation [ 5 ] . Existing discussion interfaces cannot directly support this strategy because they process and represent all comments the same , regardless of who wrote them or if the content conveys strong support for or objection to an idea . S2 : Express concerns in a constructive manner . A discussion needs to be constructive , even when there are strong disagreements [ 31 ] . Similarly , when ideas are proposed , the strengths of the ideas should be discussed first and then the weaknesses . In absence of a facilitator , however , distributed discussions can be plagued by destructive comments and flame wars , causing participants to leave the discussion , or discouraging participation . S3 : Build on prior relationships and seek expert advice . Some participants can promote consensus more than others . For instance , experienced and / or trusted participants can promote consensus by helping opposing parties develop options or by identifying barriers to effective negotiation [ 31 , 35 ] . However , existing discussion platforms do not provide mechanisms for including potential participants based on their ability to promote consensus in a discussion . S4 : Evaluate and decide according to established criteria . Solution alternatives need to be evaluated to find one that best satisfies all stakeholders’ interests [ 31 ] . The criteria help participants weigh trade - offs and identify the solution that best matches the majority’s interest . But in existing discussion forums , criteria must be proposed as part of the text in a comment , thereby becoming fragmented across the discussion . Also , there is no explicit means for evaluating the idea proposals against the set of criteria defined . S5 : Maintain a visual summary of key points of agreement and disagreement . In moderated face - to - face discussions , a recorder tracks the proposed options and organizes the key points of agreement and disagreement [ 31 ] , which can be shared during the meeting . This helps the group see the direction of the discussion and maintain task focus [ 5 ] . In distributed discussions , the participants carry the burden of tracking ideas and related arguments , making it easy to lose them in the discussion . S6 : Mark and revisit influential points of the discussion . Distributed discussions often contain many comments posted over a wide window of time . It is therefore easy for important discussion points to become lost or forgotten . This is especially problematic for participants who join late , or who enter and leave throughout and only read subsets of comments as time allows . In current discussion platforms , there is no easy way to mark and revisit important discussion points and share these with others . As shown in Table 1 , we selected a subset of the known strategies for our design . Three of the six are procedural ( S4 - 6 ) , which are generally amenable to tool support . Our tool supports these strategies through visualization design . From the remaining strategies , one is rhetorical ( S2 ) while Social Strategies Promote perception of valued contribution ( S1 ) . Build on prior relationships and seek expert opinions ( S3 ) . Promote community ownership of ideas . Help new members become part of the group . Rhetorical Strategies Express concerns in a constructive manner ( S2 ) . Value feelings and show empathy towards strong emotions . Avoid answering to all the objections to your viewpoint . Provide descriptive , specific , and tentative feedback . Disagree with ideas , not people . Keep the discussion on topic . Listen for the aspects of an idea that you find attractive and acknowledge the positive when responding . Procedural Strategies Evaluate and decide according to established criteria ( S4 ) . Maintain a visual summary of key points of agreement and disagreement ( S5 ) . Mark and revisit influential points of the discussion ( S6 ) . Respond to disruptive behavior . Generate a wide variety of proposals . Try to equalize power and balance participation . Blend ideas together and try to maximize joint gains . Don’t quit after the first good idea . When all viewpoints have been expressed , state the conclusion toward which the group appears to be moving . Test for the agreement . Before proposing solutions , understand the problem . Table 1 . Consensus building strategies derived from prior studies of moderated face - to - face discussions . The strategies were grouped into three categories , but are not intended to be exclusive . The grouping did not affect our design and was done for the purpose of presentation . Highlighted strategies are the focus of our design . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 688 Figure 1 . Procid is a browser script that extends the existing discussion interface in Drupal with a navigation panel on the top and a lens panel on the left . ( a ) The three icons on the top panel enable navigation between the main discussion page , the idea - centric view , and the invite page . ( b ) Five filters are available at the top of the lens panel that toggle the highlighting of comments with the corresponding property , e . g . , the must read ( leftmost ) lens highlights the comments endorsed as must read by participants . the other two ( S1 , S3 ) are social . These are typically more difficult to implement , but are approached in our tool by leveraging content analysis methods , interaction constraints , and the interaction history of the community . These strategies provide a minimal but reasonable starting point for exploring how to design a system that brings consensus theory to the practice of un - moderated online design discussions . As experience is gained , additional strategies could be implemented or made accessible to community members via onboarding or other newcomer socialization techniques [ 11 ] . PROCID Procid is a browser extension that supports consensus building strategies for distributed design discussions . See Figure 1 . We built Procid as an add - on to an existing discussion platform , rather than as a stand - alone system . The benefits are that we can deploy and test our interface in an existing community , recruit participants who care about the discussion topic , and integrate an existing workflow . It also allows us to leverage the community’s social structure to build novel features ( e . g . participant invitations ) . To provide a testbed , we built Procid to augment design discussions in Drupal , an open source content management system initiated in 2001 . Drupal is a mature open source community with an established workflow and organization . Drupal’s issue management system includes thousands of discussions requesting usability fixes , improved designs or new features for the Drupal product . Members participate in the discussions by proposing design alternatives , critiquing the alternatives , implementing an alternative ( writing a patch ) , reviewing a patch , or offering other insights . As the Drupal product expands and grows a large user base , adding features or fixing usability issues is getting harder . For example , proposed changes must now often consider customer preferences , product roadmaps , and consistency issues ; along with the ( strong ) opinions of current members . By interacting with the Drupal community for the past four years , the first author has become aware of the need for employing consensus building strategies in the community . This interaction has included participating in and analyzing online discussions in the community , discussing consensus building with members through interviews and existing threads , and presenting at a regional Drupal conference . Procid organizes its features into three groups : an idea - centric view that organizes the proposed ideas , criteria , and comments ; an invite page that lists potential members to invite to a discussion , and a lens panel that wraps the existing discussion and enables filtering of the comments . Idea - Centric View : Organize Discussions around Ideas Proposed ideas are the focal point of any design discussion . Keeping track of the ideas enables participants to evaluate or build upon each other’s ideas and revisit ideas not yet discussed . For tracking ideas and visualizing agreement and tension , Procid offers an idea - centric view of a discussion . This type of view prevents ideas from becoming lost in a discussion [ 36 ] . The view summarizes the proposed ideas , their status ( Dropped , Ongoing , or Implemented ) , and the criteria for evaluating the ideas . It also spatially organizes the comments for each idea into supportive , neutral , or constructive categories ( Figure 2 ) . Before detailing the features of this view , we will explain how the system knows which comments contain ideas or refer to ideas . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 689 Figure 2 . Selecting the light bulb icon from the navigation panel opens the idea - centric view ( shown ) which contains four columns of information . ( a ) In this view ideas are represented with the image that was attached to the corresponding comment or the initial text if no image was attached . ( b ) A menu for setting the status of the idea and ( c ) a list of user - defined criteria . Each criterion has a shared slider for rating the idea which any user in the discussion can manipulate . ( d ) The comments referring to an idea are spatially organized into three rows : supportive ( top row ) , neutral ( middle ) or constructive ( bottom ) . Comments registered as strong support or objections are rendered with larger circles , which are intended to attract more attention than the other comments . Figure 3 . When authoring a comment , Procid allows the user to mark if the comment ( a ) proposes an idea or ( b ) refers to an existing idea and , if so , to indicate their disposition toward it . Identifying Ideas and participants’ opinions about ideas Procid inserts an interaction panel within the existing dialog for authoring a comment . In this panel , a participant can toggle if the comment proposes an idea or if it refers to an existing idea . For the latter , the participant can also indicate her disposition toward the idea ( Figure 3 ) . Whether a comment contains an ‘idea’ can be later changed by selecting the icon inserted by our tool in the bottom right of the rendered view of the comment . Asking for user input to render alternative views of a discussion follows a long thread of CSCW research ( e . g . [ 12 , 25 , 28 ] ) . A unique aspect of our approach , however , is that we are injecting the request into an existing community interface . This imposes a constraint on how much input we can realistically request for fear of community backlash . It also prevents us from expanding the input to the point of making the system undesirable to use ( an important lesson learned from the class of design rationale systems ) . Our design therefore only requires this minimal input , but in return can render the view shown in Figure 2 . From this view , users can quickly determine which ideas are favored or opposed , which need more attention , patterns in the comments , and how the ideas compare based on the criteria . Establishing concrete criteria to evaluate ideas In design discussions , the solution alternatives need to be evaluated to find the one that best satisfies all participants’ interests . In the idea - centric view , participants can define the criteria for evaluating ideas . For example , if a solution must have low implementation cost , a participant can add cost effectiveness as a criterion . This is achieved using the criteria editor , accessed via the pencil icon ( Figure 4 ) . For each new criterion , a slider is generated and replicated in the row of each idea ( Figure 2c ) . Using the shared slider , participants can specify how much each idea satisfies the criterion on a 7 - point scale ( unsatisfactory to satisfactory ) . Upon changing the rating , the participant is prompted to justify the change which will be posted in the discussion ( Figure 5 ) . The prompt for rationale is meant to reduce superficial back - and - forth with the ratings . The scales allow Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 690 Figure 4 . Users can establish new criteria for evaluating the ideas ( Add Criteria + ) and edit the existing criteria . Figure 5 . Users can use the generated scale to specify how much each idea satisfies each user - defined criterion . participants to visualize and discuss trade - offs of ideas and eventually determine the one that best satisfies the criteria . The criteria scale shows only the most recent rating , but the tool maintains the prior ratings and associated comments . This history can be accessed by selecting the button next to each scale and is intended to prevent prior users from feeling their opinions have been supplanted . Spatially organizing comments around ideas The idea - centric view organizes the comments referring to an idea into supportive , neutral , or constructive categories . The category of a comment is determined by the disposition rating provided when it was authored ( Figure 3b ) . A participant can also add a comment from this view by selecting the “ + ” button corresponding to the desired disposition and idea . The advantage is that this opens the comment authoring dialog with the input parameters already set . When an idea is first posted , the “ + ” button for constructive ( critical ) comments is disabled until a neutral or supportive comment is added . The purpose of this constraint is to encourage discussing the strengths of an idea before discussing its weaknesses [ 19 ] . When a user is adding a supportive comment for an idea , s / he can register it as strong support for the idea ( Figure 3b ) . Alternatively , when users are adding a constructive comment , they can register it as a strong objection to the idea . Comments registered as objections or strong support are rendered with larger circles to indicate unusually strong opinions about an idea ( Figure 2d ) . The purpose is to provide participants with a stronger sense of empowerment over the proposed ideas and decision process and make these opinions easily noticeable in the visualization . Figure 6 . The invite page generates a list of users to potentially invite to the discussion . It considers criteria important for consensus such as the number of patches contributed and prior interaction history with participants in the discussion . If an idea receives several objections , the person who proposed the idea can set its status to “Dropped” . This status informs others that the idea need no longer be considered and the ability to add comments is disabled . Invite Page : Seeking potential contributors A discussion can become stalled due to controversy over implementation or strong disagreements between parties . Seeking additional expert advice or inviting trusted colleagues can help move the discussion forward . However , there is no useful mechanism for finding people to invite to the discussion who could contribute to the consensus effort . To explore such a mechanism , the Invite page in our tool dynamically creates a list of community members sortable by attributes important for consensus in the domain ( Figure 6 ) . The attributes include the duration of membership , number of patches submitted , recent participation , prior interaction with participants in the current discussion , and number of prior discussions involved in that reached consensus ( i . e . the discussions were closed ) . For example , experienced members may bring historical perspectives on the product and past decisions that can influence the direction of the discussion and those with connections to participants in the current discussion may be more likely to accept an invitation to participate . The set of attributes could be expanded in future work , such as including a topic - based relevance score and the number of active discussions to gauge workloads prior to invitation . Discussion participants can sort the list to seek potential members to invite , and not have to guess , rely on memory , or blindly search for possible contributors . The list can be especially helpful in situations where the user is less familiar with the topic or current participants . To compute these attributes , our tool analyzes the social graph [ 18 ] , interaction history , and profiles of community members as an offline process and repeats it periodically . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 691 Lens Panel : Gaining a Rapid Overview of Discussion Procid extracts significant discussion bits from multiple perspectives including community endorsement , the content , and the conversation behavior of the posters . These perspectives are captured in five lenses available in the lens panel wrapped around the main discussion page : must read , idea , conversation , experienced , and patch ( Figure 1 ) . Through these lenses , Procid allows newcomers to quickly gain awareness and existing participants to mark and revisit key points of the discussion ( Figure 7 ) . For example the must read lens highlights comments that are critical for understanding the discussion’s trajectory . Examples include comments that summarize the discussion to date , plot the direction of the discussion , or report conclusions from synchronized discussions . Any participant can endorse a comment as must read by toggling an icon that our tool inserts with the comment . It can be toggled off if a participant later feels the comment was incorrectly marked or is no longer deserving of this property . Figure 7 . The idea lens ( in blue ) highlights all of the comments marked as containing an idea . Figure 8 . Procid analyzes the sentiment of the comment just before it is posted and , if an overly negative tone is detected , highlights the negative words and encourages revision . Promoting a Positive Tone Strong disagreements may result in destructive comments , hindering consensus . Our system promotes a positive tone by detecting when a comment has an overly negative tone relative to prior comments in the community and alerts the author just prior to posting it ( Figure 8 ) . The goal is not to censor the user , but to prompt a revision that is more constructive . However , once alerted , the user chooses whether to revise or post it as is . To determine if a comment is overly negative , we computed the sentiment of 9000 randomly chosen comments from Drupal using AlchemyAPI [ 1 ] . We sorted the comments based on their negative sentiment and defined the threshold as the average of the top 1 % of the comments . Just before a new comment is posted , Procid compares the sentiment of the comment to the threshold . If exceeded , the negative words in the comment are highlighted and the user is encouraged ( but not forced ) to revise it [ 16 ] . The threshold can be easily tuned or more advanced sentiment analysis techniques can be applied in future iterations . Alternatively , the user can choose to have the system check the tone of the comment and highlight the negative words , regardless of how it compares to prior comments . Rendering Ongoing Discussions It is very likely that community members would use Procid to participate in ongoing discussions with existing content . For its features to be effective , the tool needs to know which comments propose ideas , which comments refer to ideas , the comment dispositions , etc . One approach is to prompt users to provide this information at first use , but this upfront cost could be large . Instead , we apply heuristics to determine the ideas , comments referring to the ideas , and dispositions of comments toward the ideas and offer interactions for revising the assigned values . For example , Procid detects if a comment contains an idea by calculating a score based on whether the comment contains an image or patch , the number of references to the comment , and the sentiment of the references . If the score exceeds a certain threshold , it marks the comment as an idea . Figure 9 . Users can change the classification of comments by selecting the relevant icon at the bottom of the comment box . For each marked idea , Procid parses the discussion to find the comments referring to it . The tool then uses an internal sentiment analysis tool to categorize the comments . Similar to other tools ( e . g . SentiWordnet [ 6 ] and LIWC [ 24 ] ) , Procid uses a lexicon that consists of two categories of negative ( e . g . disappointment , criticism ) and positive ( e . g . gratitude , pride ) words derived from a commonly used online dictionary . A comment is categorized as supportive if it has more positive words , constructive if it has more negative words , and neutral if it has none or nearly equal negative and positive words . This classification provides a Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 692 default value , but users can override these values using toggles at the bottom of the comment box ( Figure 9 ) . REVISITING STRATEGIES We summarize how our tool realizes the six strategies . To promote perception of valued contribution ( S1 ) , Procid prominently displays the proposed ideas to ensure that discussion participants do not ignore those contributions and organizes the comments referring to those ideas . This helps participants track the ideas and their status and notice patterns of comments for and against the ideas . Also , to help users consider opposing views , Procid refrains from using colors commonly associated with negative affect ( e . g . red ) [ 34 ] . To cultivate a sense of empowerment ( S1 ) , the tool enables registering strong support or objection for ideas and visually emphasizes these dispositions in the idea - centric view ( Figure 2d ) . To promote constructive tone ( S2 ) , Procid analyzes each comment just before it is posted and , if an overly negative tone is detected , cautions the user about posting it and reminds him or her why this is important . For building on prior relationships ( S3 ) , Procid generates a list of potential participants for including in a discussion by analyzing their prior contributions , social graphs , and public profiles . Procid’s idea - centric view visually summarizes key points of agreement and disagreement ( S5 ) . It lists the ideas along with their supportive and constructive comments ( Figure 2 ) . It also allows participants to create concrete criteria and rate the ideas against the criteria ( S4 ) . Procid offers five lenses on the main page for marking and revisiting influential points of a discussion based on community endorsement , reply behavior , and content of the comments ( S6 ) . IMPLEMENTATION Procid was built using a client / server architecture . The client is a user script ( JavaScript and jQuery code ) that executes on a browser and customizes the appearance and functionality of the existing discussion platform . To track user data and perform data analysis the script connects to the server we built with Ruby on rails and MySQL . Both client side and server side are open source software under Apache license 2 . 0 and freely available on Github . As a workaround to Javascript same - origin - policy , the client uses CORS ( Cross - Origin Resource Sharing ) to communicate with the server and exchange data using JSON . When the user installs the client and opens a discussion , it sends the discussion identifier along with the comments to the server . The server checks whether any records for the discussion exist in the database . It then processes the comments . For each comment , if there is a record in the database , the stored tags ( e . g . idea , patch , conversation , etc . ) are assigned to the comment . Otherwise the server computes values for as many tags as possible for the comment and stores them in the database . The server then sends the comments , tags , criteria , etc . to the client . The implementation of the client is customized to work with the Drupal issue management system . It parses Drupal’s HTML code and attaches its components to the HTML elements . However , the client uses a modular architecture . Therefore , by updating the parser module ( around 15 lines of code ) and the HTML elements that the client will attach to ( around 10 lines of code ) , it can work with other issue management systems . The server has its own independent API which is accessible by any client . EVALUATION We conducted two studies of Procid . The first study gauged initial reactions to the tool and collected feedback from the community . The second study was a controlled experiment aimed at empirically comparing the use of our tool to the existing community interface for design discussions . Study 1 : Initial Reactions from the Community We randomly chose five UI design discussions from Drupal . The discussions were active and had recent activity and many comments ( µ = 124 comments ) . We then tailored an interface walkthrough for each of these discussions and invited the participants in the discussions to complete the walkthrough . The key point is that a participant could see the discussion s / he was actively participating in rendered in our tool . Participants were informed that their interactions with Procid would not affect the actual discussion because the content had been mirrored on our server . Nine community members involved in the five selected discussions responded to our invitation to participate . The participants were active members of Drupal . Three ( male ) were designers with an average of 5 . 6 years of experience , one ( male ) was a site builder with four years of experience building websites with Drupal , and one was both a designer and developer ( female ) with two years of experience in the community . The four remaining participants ( male ) were developers with an average of 5 . 25 years of experience . The walkthrough began by having a participant install Procid in her Web browser . The participant then navigated to a page that mirrored his or her own Drupal discussion along with Procid . The participant then performed specific tasks relating to each main feature of the tool and freely explored that feature of the tool . After each task and exploration , participants rated their perception of the feature and commented on its strengths and weaknesses . Finally , participants rated and commented on the overall utility of the tool and the discussion interface currently used in the community . Participants received $ 15 for remuneration . We collected survey responses and logged interactions . In the survey , participants rated the perceived usefulness of different features of the tool , the tool overall , and the discussion interface currently used . All ratings were made on a 7 - point Likert scale ranging from strongly disagree ( 1 ) to strongly agree ( 7 ) . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 693 Results Participants rated Procid to be more useful for consensus building ( µ = 4 . 5 , σ = 1 . 2 ) than the discussion interface currently used ( µ = 3 . 4 , σ = 1 . 6 ; ( t ( 8 ) = 4 . 2 , p = 0 . 003 using paired samples t - test ) . Two representative statements are : “ I think this could help people see the current status of an issue in a way that our current ' tool ' for this ( manually - written issue summaries ) can ' t , and in a more in - depth and balanced way than that tool allows . ” [ P9 ] “This tool could help focusing on ideas and not so much on people . Furthermore this tool could help to invite feedback even for long running issues with a lot of comments . Such issues normally are very hard to get into but would profit most from additional input . ” [ P5 ] The feature that participants found most useful in Procid was the ability to establish criteria ( µ = 5 . 9 , σ = 0 . 6 ) . As illustrated by P7 : “ [ Criteria related features are ] useful for reviewing : you know what to pay attention to . Useful for establishing what should be in scope of this current issue and what could be a follow - up issue . ” Participants also agreed that the ability to read the rationale behind idea ratings is useful ( µ = 5 . 7 , σ = 0 . 7 ) . For example , P9 said : “I don ' t think the ranking itself is as important as being able to actually read the rationale behind it ( e . g . , who gave it the ranking and whether they had something useful and constructive to say about the particular criterion when they did so ) . ” Participants found the invitation list and summary of attributes useful for identifying who to invite to the discussion ( µ = 5 . 6 , σ = 0 . 7 ) . P1 stated : “ This is a good idea , if one wants to get someone involved . Else there is hardly a way to do this if you do not want to use their contact form on d . o . [ drupal . org ] ” . The majority of participants stated that in the proposed list they actually found a member they would like to invite to the discussion ( 78 % ) . P4 said : “ Yes , I immediately saw people that I should invite to the discussion . The algorithm used for this is interesting . ” Participants also suggested improvements to the algorithm . For example , one participant stated that selecting people based on experience could miss people who are currently active in the usability discussions . Another participant recommended that the algorithm include topic similarity . P8 : “ Facts that person has X patches , or participated in an issue which is now successfully closed , or was recently on the site , can ' t guarantee that person will be interested in this specific issue . [ … ] What is useful is relevance , e . g . if person participated in similar issues on the same topic . ” A third participant recommended that we add the invite feature on the comment level as well as the issue level . This would enable the participants to invite members to respond to a particular comment , rather than the whole issue . Participants found the idea page useful for tracking the ideas ( µ = 5 . 1 , σ = 1 . 3 ) . P9 said : “It reminded me of some of the earlier ideas proposed in the drupal . org issue ( I ' m already a follower of this particular drupal . org issue so I ' m somewhat familiar with it , but I ' d forgotten about some of the earlier discussion and this helped me remember it ) . ” Participants found the five lenses useful for navigating to different parts of the discussion ( µ = 5 . 3 , σ = 0 . 9 ) and for identifying important comments ( µ = 5 , σ = 0 . 9 ) . For example , P5 said “ The panel absolutely helped to dive into a bigger , already evolved issue . Usually those issues are very hard to get into and it gets cumbersome to read similar comments over and over again especially if the issue goes on for a longer time period . I especially like the comment highlighting . This feature helps massively to find the comments that bring the meat to the issue so to speak . ” To make Procid more useful for consensus building , participants suggested ranking the ideas in descending order or adding a status field to communicate the status of the whole discussion . P7 : “ Maybe the ideas page can be sorted in reverse , with the latest most current idea on top . Might help focus the discussion on the latest and greatest idea . [ . . ] . Another thing that could be useful is to have some kind of status indication for the issue as a whole : are we still exploring options or are we looking for detailed reviews on an almost done solution . ” Study 2 : Comparative Evaluation The purpose of the second study was to compare how Procid affects the content , interactions , and user perceptions for distributed design discussions relative to the existing discussion platform used in Drupal . The latter platform is representative of most interactive Web forums used today . Given the limited interaction participants would have with either interface , our focus was on user perceptions of and experiences with the platforms rather than decision quality . Design Tasks and Participants The design task was to propose and debate names for a startup and was selected for three reasons . First , the task of naming a startup is a hard design problem . The task is ill - structured , many creative solutions are possible , there are no defined evaluation criteria , and the outcomes matter . Second , many design discussions in the community target selecting effective labels for various interface elements . Finally , proposing and naming a startup does not require user interface design and development expertise , therefore making the task accessible to a wider range of participants . To make the discussion authentic , we interacted with two technology startups that were seeking effective names . One company is commercializing technology to verify network data flow security and correctness in real time ( startup A ) . The other one is developing a content creation and camera hardware solution that enables users to create immersive experiences for advertising and entertainment ( startup B ) . We recruited 37 ( 15 female ) participants with diverse disciplinary backgrounds from a large university . None had Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 694 knowledge of this project . Ages ranged from 18 - 34 . Based on self - reports , about half ( 18 ) of the participants frequently participated in Web forums , while the other half did not . Participants received $ 15 for participating in the study . Discussion Interfaces and Experimental Design We created eight issues in the Drupal issue management system for the design tasks and participants created Drupal accounts . Permission to do this was requested and granted from the administrators of Drupal . Our approach allowed participants to use the discussion interface in Drupal and to use Procid as described earlier . To disguise which interface was ours , we always referred to the existing discussion interface as Interface A and our tool as Interface B . Also , participants installed a script for each interface , though the script for Interface A performed no function . The study was a within - subject design with Interface ( our tool vs . existing discussion interface ) as the one factor . Status quo Procid Week 1 Week 2 Week 1 Week 2 Startup A G1 G4 G3 G2 Startup B G2 G3 G4 G1 Table 2 . The naming tasks and interfaces were assigned to the groups using a Latin - square design . Procedure Participants went through an informed consent process and were given an overview of the study . Participants were then randomly assigned to one of four groups ( G1 , G2 , G3 , G4 ) , each with 10 - 12 participants . The order of the naming tasks and interfaces used were balanced using a Latin - square design ( see Table 2 ) . The study was conducted over two weeks . During the first week , two groups ( G1 and G2 ) used the status quo interface to name startups A and B , respectively . The other groups ( G3 and G4 ) used Procid to name startups A and B respectively . At the beginning of the week , participants received the installation instructions for the assigned Interface , a video demonstrating its main features , and details of the task in an email . Participants were requested to generate and debate as many names as possible , and to identify their favorite name by the end of the week . Also , participants completed a survey on their experience using the assigned Interface . During the second week , each group switched to use the other Interface for the other design task . The procedure was then the same as the first week . Each participant was asked to post at least ten helpful comments to a discussion . To motivate participation , we framed the naming tasks as a competition for each week . Groups earned points using a simple point system ( Table 3 ) intended to reward effort and quality . The group with the most points at the end of the week won the competition for that week and each participant received an additional $ 15 . To make the discussions engaging and realistic , the startup founders gave feedback on the direction of the discussion twice during each week . After the study , we conducted interviews with eleven of the participants asking about their perceptions of and experiences with each interface . Measures We collected data from the discussion content , interaction logs , surveys , and interviews . For the content , we measured the number of comments , names proposed , criteria defined , and evaluative statements . The interaction logs recorded each participant’s interactions with the features of Procid . For the surveys , participants rated the perceived usefulness of the assigned interface for the consensus - related tasks and its overall usefulness . Ratings were made on a 7 - point Likert scale ranging from strongly disagree ( 1 ) to strongly agree ( 7 ) . At the end of the second survey , participants identified what they felt were the important differences between the two interfaces , selected which interface they would use in the future and why , and explained which interface helped the most with consensus building and how . We will refer to survey participants as S # . At the end of the study , we conducted semi - structured interviews with 11 participants ( referred to as I # ) . We asked about their overall perceptions of Procid and the usefulness of its main features . Each interview lasted 15 - 20 minutes . G1 G2 G3 G4 S t a t u s qu o Comments 36 62 66 40 Criteria 3 1 2 3 Ideas 41 36 41 21 Evaluations 17 34 30 15 P r o c i d Comments 55 59 83 79 Criteria 3 5 5 4 Ideas 46 19 18 32 Evaluations 31 27 53 39 Table 4 . Number of comments , proposed ideas , user - defined criteria , and evaluation statements in the discussions . Results Participants posted a total of 480 comments during the study . Of those , 254 contained proposed names ( ideas ) and Contribution Points Propose a meaningful startup name . 2 Evaluate a proposed name . 3 Determine criteria to evaluate the names . 3 Post other meaningful comment . 1 Build consensus on a name that company founders chose as their favorite . 15 Table 3 . A simple point system was used to motivate participation and determine the winning group . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 695 26 had criteria for evaluating the names . This shows that the participants were engaged in the study . Examples of some of the proposed ideas were VeriPut , DataRail , Raptor , GreenWall , and SecuraNet for startup A and Phantomizer , Mirage Maker , ProMagix , Luminocity , and Morphology for startup B . The winning name for startup A was NetGator and the winning name for startup B was Astral , both of which the startup founders thought were creative and had not been considered beforehand . The sets of proposed names were shaped and filtered based on user - defined criteria that emerged during the discussions . Examples of criteria were “ Reflective : The name should reflect the company’s idea ” , “ Trademark available : Whether trademark is available ” , “ Catchiness : How catchy the name is ” , and “ Uniqueness : The name should be different from the existing companies . ” For example , the name GreenWall was decided against because it wasn’t reflective as S25 pointed out in the discussion : “ The name GreenWall suggests something related to the environment . But the company is focused on security in a network ” . Table 4 summarizes the comments , ideas , criteria , and evaluative statements for both interfaces . Though the trends favored Procid , there were no statistical differences due to the relatively small number of groups . Overall Preference From the survey , a large majority of participants ( 31 out of the 37 ) experienced Procid to be more helpful for consensus building than the status quo interface and preferred to use Procid for future design discussions . From the open - ended responses , the strong preference for Procid was due to its organization of comments ( n = 5 ) , the ability to create and evaluate names based on the criteria ( n = 7 ) , the ability to view the summary of agreements and disagreements with each name ( n = 7 ) , and the ability to track the proposed names ( n = 5 ) . Three representative statements were : “ Interface B ' s second screen ; a lot more information is presented in a shorter amount of time . Even though I like to read the paragraphs for a poster ' s entire meaning / explanation behind his or her proposal , I think most forum users / readers do not have the patience to go through an entire thread . Interface B seems to solve that problem by sifting out the important bits and showing the highlights . Though scoring each post at the time may take longer , in the long run and for others on the forum it makes things a lot easier . ” [ S4 ] “ It was easier to reach a group consensus on Interface B , because you could make a Positive , Neutral , or Critical vote / comment on someone ' s proposed name . This made it way easy to see the front - runners in public opinion of the board , rather than trying to keep track of which people agreed , which results in the last discussed names to be the main ones being considered . ” [ S19 ] “ B because you can easily view suggested names , provide feedback and rate them in a separate view . In A you have to scan through posts and keep track of the thread . ” [ S5 ] Questions Status Quo Procid p µ σ µ σ Q1 . I felt my comments were considered by others in the discussion . 5 . 73 1 . 18 5 . 62 1 . 07 0 . 34 Q2 . I felt that I could express strong support for or against a name . 5 . 24 1 . 44 5 . 95 1 . 11 0 . 00 * * Q3 . The proposed names received critical comments after posting . 5 . 11 1 . 16 4 . 95 1 . 01 0 . 24 Q4 . It is important to have objective criteria for evaluating the names . 5 . 70 1 . 21 6 . 14 0 . 96 0 . 00 * * Q5 . It was easy to establish objective criteria to evaluate the names . 5 . 08 1 . 17 5 . 89 0 . 95 0 . 00 * * Q6 . Participants evaluated the names based on the criteria . 5 . 00 1 . 19 5 . 81 1 . 01 0 . 00 * * Q7 . It was easy to track the discussion and the status of the names . 3 . 78 1 . 71 4 . 97 1 . 65 0 . 00 * * Q8 . It was easy to find other participants’ opinions about a name . 4 . 03 1 . 75 4 . 81 1 . 61 0 . 03 * * Q9 . It was easy to find the important comments in the discussion . 3 . 65 1 . 76 4 . 81 1 . 56 0 . 00 * * Q10 . I felt that I was encouraged to add positive comments . 5 . 14 1 . 32 5 . 41 1 . 17 0 . 06 * Q11 . In our group , we had a collaborative discussion . 5 . 46 1 . 20 5 . 65 0 . 85 0 . 23 Q12 . This interface supports the consensus building process . 4 . 76 1 . 38 5 . 24 1 . 12 0 . 06 * Table 5 . Summary of the survey results . From left to right , the table contains the questions , mean and standard deviation for the interfaces tested , and results of a paired - sample t - test . Responses used a 7 - point Likert scale ranging from strongly disagree ( 1 ) to strongly agree ( 7 ) . * * = p < 0 . 05 ; * = p < 0 . 10 . Conversely , the majority of participants noted the lack of organization of comments and difficulty in tracking names as weaknesses of the status quo interface ( Interface A ) . For instance , S6 wrote : “ It was really hard to keep track of all the posts . Some of them had proposed new criteria , some new names , and some where just supporting , or disagreeing with the suggested names . The main con was that all these posts were in one thread and there was no organization . For example , if you just wanted to know a list of all suggested names you had to read all the posts . ” Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 696 Experienced Differences From the survey , participants reported experiencing three key advantages of Procid relative to the status quo interface . One advantage was the ability to evaluate ideas based on user - defined criteria ( n = 13 ) . Three representative responses from the surveys and interviews were : “ Interface B allowed you to rate names based on criteria and it made it easy to move forward from there . Whereas Interface A was simply a forum discussion . ” [ S3 ] “ Interface A was quite plain and not very interactive but got the job done . Interface B on the other hand was very useful and allowed posters to really evaluate other posts and facilitated a meaningful discussion ” [ S5 ] . “ I feel it’s pretty much one of the only ways you can do that sort of thing . It ' s like you cannot really have an abstract goodness or badness you have to say why it ' s good why it ' s bad . I feel like the criteria , really do help you determine why it ' s good or why it ' s bad . ” [ I5 ] As shown in Table 5 , the survey responses also showed significantly higher ratings for Procid than the status quo interface on the three questions relating to criteria use ( Q4 , Q5 , Q6 ) . From the content analysis , groups created 14 criteria when using Procid , but created only 9 criteria when using the status quo interface . Participants also wrote 150 evaluative statements based on criteria when using Procid but only 96 statements when using the status quo interface . A second key advantage was the ability to track the proposed names when using Procid ( n = 10 ) : “ The option for viewing the already proposed names is super convenient because then you don’t have to read the whole discussion to see them . ” [ S26 ] “ Interface B was easy to follow different names and you can just focus on the specific parts . However , Interface A was simple and much easier to work with . You had to read most of the comments to be aware of discussion . ” [ S35 ] Procid was also rated as being significantly better for tracking ideas ( Q7 , Q8 ) than the status quo interface . The third major difference was the better organization of comments provided by Procid ( n = 8 ) : “… [ Interface B ] seems much more organized . Interface A is simpler , but it can be hard to determine what the key points in the conversation are . This would be particularly true if the discussion is very long . ” [ S25 ] “ Interface A was just a discussion that kind of just blended down so like people would just keep going and going which is good but then they forget the stuff that happened before and then we would have to bring that up again and so Interface A was just kind of like a flow of stuff where you could forget the past discussion whereas Interface B you could advance your discussion a lot more better because things in the past are more apparent ” . [ I9 ] In contrast , the main relative strengths of the status quo interface were openness ( n = 8 ) and ease of use ( n = 9 ) . Feature Use From the interaction data , we found that the groups used most of the key features of Procid ( see Table 6 ) with Groups 3 and 4 making the most extensive use of them . The most common interactions were with defining criteria , evaluating ideas based on the criteria , checking the tone of the comment , and applying lenses to filter comments . The interaction data also shows that all but one group spent at least 36 percent of their time using the idea - centric view . This usage was balanced toward the latter stages of the discussion when there was more content . G1 G2 G3 G4 Rate ideas based on criteria 24 1 18 26 Add a new criteria 3 3 4 4 Check a comment’s tone 5 5 26 18 Register strong opinion 0 0 1 4 Drop an idea 0 0 2 4 Tag comment as mustread 3 0 3 7 Click on the mustread lens 10 5 25 24 Tag comment as idea 16 16 11 10 Click on the idea lens 11 10 41 32 Percentage of the time spent on the idea - centric view 36 27 36 36 Table 6 . Frequency of actions performed using Procid . One interesting pattern from Table 6 is that G1 and G2 used the features with the same relative proportions as G3 and G4 , but less overall . One explanation is because G1 and G2 used the status quo interface first and , once assigned to use Procid the next week , did not discover its features as quickly as those who used it first . Opportunities for Improvement To improve Procid , participants identified wanting better support for comments that contain multiple names or criteria ( n = 5 ) , better awareness and centrality of the provided features ( n = 3 ) , and the ability to define an aggregate score for the proposed names . Future work should also address unexpected behaviors observed in the study . For example , one behavior identified by the interviewees was participants positively rating their own ideas , which obviously did not carry much weight . For example , I10 said : “… I saw there was like three pluses for somebody and I was like oh somebody really likes this idea , and I looked and it was their idea . So , if you are doing such a thing , like three pluses is really good when we were doing Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 697 it , that might confuse people … that plus feature kinds of need some sifting , you need something to be able to turn that off so you can see what other people think of other people ' s ideas . ” DISCUSSION From both studies , we found support for the six consensus strategies implemented in Procid . There was strong support for S4 ( evaluate and decide according to criteria ) and S5 ( maintain visual summary of agreement and disagreement ) . The relevant features in Procid were frequently used ( e . g . the idea - centric view and criteria interactions ) and reported as a distinct advantage of the tool , and the ratings of Procid for the associated tasks were higher than the status quo interface . There was support for S1 ( promote perception of valued contribution ) as participants rated Procid higher for tracking each other’s contributions ( ideas , criteria , comments ) . From Study 1 , we found support for S3 ( build on relationships and seek expert advice ) , as community members were able to use the invitation feature of our tool to locate at least one member to include in a discussion who may not have been otherwise considered . S6 ( mark and revisit influential points ) was supported as the lenses were frequently applied and Procid was rated higher for filtering key comments . Finally , little support was found for S2 ( express concerns in a constructive manner ) . The ‘tone’ dialog in the tool was accessed , but the comments did not contain an overly negative tone to begin with and therefore the comments were not revised . Despite promising results , our studies focused on perceptions of and experiences with the consensus - based features in Procid . Additional studies are needed to assess how these features are used to shape discussions and how their patterns of use may change over time , with larger groups or with different design problems . The two studies conducted reported on measures that could be immediately collected ( e . g . survey and interaction data ) . Future field studies should also measure the longer - term impact of a tool such as Procid on a community . For example , measures could include changes in membership duration and turnover , changes in patterns of community discourse , and other health metrics [ 26 ] . One challenge of conducting such a study is gaining a critical mass of users [ 17 ] . One possible way to approach this challenge is to plant seeds of use that grow over time . For example , the creator of an issue could be given the option of requiring that Procid ( or a similar tool ) be used for the discussion . Alternatively , the community could identify a subset of threads ( e . g . on a particular topic ) that would use the tool by default for a period of time . In either case , involving the community in the design of the tool and considering their feedback would help create ‘buy - in’ and reduce resistance to its adoption . For example , participants in our first evaluation suggested adding a discussion about Procid on drupal . org . This would allow the community members an opportunity to suggest , comment on , and understand the features of our tool . The implementation of Procid requires knowledge of the structure of the HTML code in a discussion in order to track and manipulate it . We extracted the information necessary to support the discussions in one community , Drupal . This step would have to be repeated to support discussions in other communities . It is also possible with minor extensions to support stand - alone discussions ( those not linked to any existing community ) in the tool . Procid was inspired by and targets discussions about design issues , particularly in the UI domain . However , the design of the system should generalize to any situation in which distributed participants need to propose and debate concrete proposals for solving problems , e . g . to name a startup . The system is not tied to the type or content of the discussion . Procid encourages recommended practices of consensus building , e . g . , the first comment on a proposed idea should be positive and even critical points should be written in a constructive tone . We believe , in time , some behaviors encouraged by the constraints introduced in our tool would become integrated into the practice of the community . In addition , for practices that cannot be easily integrated into the tool , communities could include training for consensus building as part of the onboarding process [ 11 ] . Our system provides features useful for consensus building , but it is up to the participants to decide how to use these features during the discussion . Even if fully utilized , there is no guarantee that consensus will be reached or that users are satisfied with the experience of the discussion . Finally , there may be situations in which the consensus building approach is not desirable , e . g . , participants may be reluctant to concede individual influence to the collective good . In such cases , the use of a tool such as Procid may still aid participants by better organizing the discussion . CONCLUSION Reaching consensus is important for distributed design discussions , yet the discussion platforms commonly used do not provide mechanisms for building consensus . Our work has made three contributions to close this gap . First , we built a novel system ( Procid ) that demonstrates interaction and visualization features for bringing consensus building strategies into distributed discussions . It provides a system design pattern that existing discussion platforms can mimic to better promote consensus building . Second , we extracted and shared consensus building strategies from the literature that can shape the design of systems like Procid . Finally , we reported results from two evaluations showing that users found the main features of the tool useful for consensus building and perceived the tool to be more effective for consensus relative to a status quo discussion platform . One immediate direction for future work is to conduct a longer study probing how the use of our system affects consensus building in authentic design discussions and how different user categories such as designers and developers utilize the system to meet their needs . Another direction is Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 698 to integrate more consensus building strategies into the tool and compare how the addition of those strategies affects the discussion experience and outcomes . A third direction is to refine and extend Procid’s interface to better support consensus building based on the user feedback collected . REFERENCES 1 . AlchemyAPI Sentiment Analysis http : / / www . alchemyapi . com . 2 . Alonso , S . , et al . Using Visualization Tools to Guide Consensus in Group Decision Making . WILF , 2007 , 77 - 85 . 3 . Angwin , J . and Fowler , G . A . Volunteers Log Off As Wikipedia Ages . The Wall Street Journal ( Nov . 23 , 2009 ) , pages A . 1 . 4 . Applegate , L . M . , et al . A group decision support system for idea generation and issue analysis in organization planning . CSCW , 1986 , 16 - 34 . 5 . Avery , M . , et al . Building united judgment : a handbook for consensus decision making . The Center for Conflict Resolution , Madision , Wis . , 1981 . 6 . Baccianella , S . , et al . SentiWordNet 3 . 0 : An enhanced lexical resource for sentiment analysis and opinion mining . LREC , 2010 , 2200 - 2204 . 7 . Balakrishnan , A . D . , et al . Do Visualizations Improve Synchronous Remote Collaboration ? CHI , 2008 , 1227 - 1236 . 8 . Biehl , J . T . , et al . Impromptu : a new interaction framework for supporting collaboration in multiple display environments and its field evaluation for co - located software development . CHI , 2008 , 939 - 948 . 9 . Birnholtz , J . P . , et al . An exploratory study of input configuration and group process in a negotiation task using a large display . CHI , 2007 , 91 - 100 . 10 . Briggs , R . O . , et al . Toward a Theoretical Model of Consensus Building . Americas Conference on Information Systems , 2005 . 11 . Choi , B . , et al . Socialization tactics in wikipedia and their effects . CSCW , 2010 . 12 . Conklin , J . and Begeman , M . L . glBIS : A Hypertext Tool for Exploratory Policy Discussion . ACM Transactions of Office Information Systems , 6 , 4 ( 1988 ) , 303 - 331 . 13 . DeSanctis , G . and Gallupe , R . B . A Foundation for the Study of Group Decision Support Systems . Management Science , 33 , 5 ( 1987 ) , 589 - 609 . 14 . El - Gayar , O . F . and Fritz . , B . D . A web - based multi - perspective decision support system for information security planning . Decis . Support Syst . , 50 , 1 ( 2010 ) , 43 - 54 . 15 . Faridani , S . , et al . Opinion space : a scalable tool for browsing online comments . CHI , 2010 , 1175 - 1184 16 . Friedman , R . , et al . The Positive and Negative Effects of Anger on Dispute Resolution : Evidence From Electronically Mediated Disputes . Applied Psychology , 89 , 2 ( 2004 ) , 369 - 376 . 17 . Grudin , J . Groupware and social dynamics : Eight challenges for developers . Communications of the ACM , 37 , 1 ( 1994 ) , 92 - 105 . 18 . Horowitz , D . and Kamvar , S . D . The anatomy of a large - scale social search engine . WWW , 2010 , 431 - 440 . 19 . Kelley , T . and Littman , J . The Art of Innovation : Lessons in creativity From IDEO , America’s Leading Design Firm . Doubleday , New York , NY , 2001 . 20 . Ko , A . J . and Chilana , P . K . Design , Discussion , and Dissent in Open Bug Reports . iConference , 2011 , 106 - 113 . 21 . Kriplean , T . , et al . Community , consensus , coercion , control : cs * w or how policy mediates mass participation . GROUP , 2007 , 167 - 176 . 22 . Kriplean , T . , et al . Supporting reflective public thought with considerit . CSCW , 2012 , 265 - 274 . 23 . Kriplean , T . , et al . Is this what you meant ? : promoting listening on the web with reflect . . CHI , 2012 , 1559 - 1568 . 24 . LLWC Linguistic Inquiry and Word Count . http : / / www . liwc . net . 25 . Luther , K . , et al . Pathfinder : an online collaboration environment for citizen scientists . CHI , 2009 , 239 - 248 . 26 . Matthews , T . , et al . Community insights : helping community leaders enhance the value of enterprise online communities . 2013 . 27 . Oehlberg , L . , et al . Showing is sharing : building shared understanding in human - centered design teams with Dazzle . DIS , 2012 , 669 - 678 . 28 . Pioch , N . J . and Everett . , J . O . Polestar : collaborative knowledge management and sensemaking tools for intelligence analysts . CIKM , 2006 , 513 - 521 . 29 . Shum , S . J . B . , et al . Hypermedia Support for Argumentation - Based Rationale . 2006 . 30 . Sidaway , R . Consensus Building . Scottish National Rural Partnership , Edinburgh , 1998 . 31 . Susskind , L . , et al . The Consensus Building Handbook . Sage Publications , Thousand Oaks , CA , 1999 . 32 . Tegarden , D . P . Business information visualization . Communications of the AIS , 1 , 4 ( 1999 ) . 33 . Willett , W . , et al . CommentSpace : Structured Support for Collaborative Visual Analysis . CHI , 2011 , 3131 - 3140 . 34 . Yasuda , M . , et al . Color and facial expressions . Journal of Vision , 7 , 9 ( 2007 ) , 946 . 35 . Zilouchian Moghaddam , R . , et al . Consensus building in open source user interface design discussions . CHI , 2012 , 1491 - 1500 . 36 . Zilouchian Moghaddam , R . , et al . Ideatracker : an interactive visualization supporting collaboration and consensus building in online interface design discussions . INTERACT , 2011 , 259 - 276 . Teamwork Challenges CSCW 2015 , March 14 - 18 , 2015 , Vancouver , BC , Canada 699