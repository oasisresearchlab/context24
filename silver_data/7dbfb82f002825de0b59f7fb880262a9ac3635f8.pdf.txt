MicroTalk : Using Argumentation to Improve Crowdsourcing Accuracy Ryan Drapeau , Lydia B . Chilton , Jonathan Bragg , Daniel S . Weld Department of Computer Science and Engineering University of Washington Seattle , WA 98195 { drapeau , hmslydia , jbragg , weld } @ cs . washington . edu Abstract Crowd workers are human and thus sometimes make mis - takes . In order to ensure the highest quality output , requesters often issue redundant jobs with gold test questions and so - phisticated aggregation mechanisms based on expectation maximization ( EM ) . While these methods yield accurate re - sults in many cases , they fail on extremely difﬁcult problems with local minima , such as situations where the majority of workers get the answer wrong . Indeed , this has caused some researchers to conclude that on some tasks crowdsourcing can never achieve high accuracies , no matter how many workers are involved . This paper presents a new quality - control workﬂow , called MicroTalk , that requires some workers to Justify their reason - ing and asks others to Reconsider their decisions after reading counter - arguments from workers with opposing views . Ex - periments on a challenging NLP annotation task with work - ers from Amazon Mechanical Turk show that ( 1 ) argumen - tation improves the accuracy of individual workers by 20 % , ( 2 ) restricting consideration to workers with complex expla - nations improves accuracy even more , and ( 3 ) our complete MicroTalk aggregation workﬂow produces much higher ac - curacy than simpler voting approaches for a range of budgets . Introduction Crowdsourcing , the outsourcing of tasks to a crowd of un - known people ( “workers” ) in an open call , is rapidly ris - ing in popularity . It is already being heavily used by nu - merous employers ( “requesters” ) for tasks ranging from audio transcription to NLP data annotation . However , en - suring output quality remains a key challenge , because of the high variability in worker abilities . The use of “gold - standard” screening questions can help , but even the best workers make mistakes . To achieve the highest quality re - sults , therefore , most requesters issue their tasks to multiple , independent workers and aggregate the results . A variety of techniques have been tried , but most employ some sort of weighted majority vote or expectation maximization ( Dawid and Skene 1979 ) . While these methods yield accurate re - sults in many cases , they often converge to local maxima and hence fail on extremely difﬁcult problems , where a sig - niﬁcant majority of workers get the answer wrong . This Copyright c (cid:13) 2016 , Association for the Advancement of Artiﬁcial Intelligence ( www . aaai . org ) . All rights reserved . has caused some researchers to conclude that crowdsourc - ing can never achieve near - perfect accuracy , no matter how many workers are involved ( Demartini , Difallah , and Cudr´e - Mauroux 2012 ) . We argue that part of the quality - control problem is an in - sistence on worker independence . Instead of isolating work - ers and blindly combining their votes , why not engage them to debate the question , seeking a consensus solution ? Since Wikipedia’s talk pages and dispute - resolution mechanisms have led to extremely high - quality content ( Giles 2005 ) , we adapt these mechanisms to the context of microtask crowd - sourcing , creating the MicroTalk workﬂow . MicroTalk starts by training workers . For the use case of producing training data for use by a machine learning al - gorithm , this involves presenting annotation guidelines and testing workers’ responses on “gold standard” questions with known answers . Then , to get actual work done , Mi - croTalk composes three primitive microtasks — Assess , Jus - tify , and Reconsider — to support asynchronous argumen - tation ( Figure 1 ) . In an Assess task , workers are asked to provide an answer to a given question . In a Justify task , workers provide reasoning with their answer in terms of the task guidelines taught during training . In a Reconsider task , workers are shown an argument for the opposing an - swer and then asked to reconﬁrm their original decision or change their answer . MicroTalk adaptively combines these tasks to collect a set of good justiﬁcations and arrive at a well - considered consensus answer . This paper makes the following contributions : • We introduce micro - argumentation as a way to increase the accuracy of crowdsourcing on objective , consensus questions by having some workers justify their reason - ing rather than just vote for an answer . When workers disagree , they are confronted with opposing arguments and given a chance to reconsider their assessment . • We show that , by itself , argumentation is moderately ef - fective at improving individual worker accuracy ; but if one ﬁrst ﬁlters the crowd to select workers producing better arguments ( “discerning workers” ) , the approach works even better . • We describe an adaptive , asynchronous workﬂow , Mi - croTalk , that combines Assess , Justify , and Reconsider tasks in a cost - effective manner . MicroTalk uses sim - Proceedings , The Fourth AAAI Conference on Human Computation and Crowdsourcing 32 Figure 1 : For each question , MicroTalk issues workers an Assess task , presenting them a ( claim , sentence ) pair and prompting them to enter their answer . Some workers are then asked to provide an argument supporting their answer during a Justify phase . If there is a counterargument available , then a Reconsider task displays it , asking the worker if they wish to change their answer . ple consensus on easy problems , requests justiﬁcations when disagreement occurs , identiﬁes “discerning work - ers” who are likely to intelligently respond to arguments , and prompts them to consider alternative answers . • We present experiments on an NLP relation - extraction task with workers from Mechanical Turk , showing ( 1 ) presenting arguments from automatically - identiﬁed “discerning workers” increases individual worker accu - racy from 58 % to 78 % , and ( 2 ) controlling for cost , MicroTalk achieves 84 % accuracy overall , compared to 64 % for soft EM , and 58 % for simple majority vote . Previous Work Virtually all previous approaches for addressing variability in worker skill ask multiple workers to perform the same ( or related ) tasks and then aggregate responses to infer the cor - rect answers . Since the effectiveness of the method is highly dependent on the method for aggregating responses , numer - ous strategies have been investigated . Most approaches assume that the question posed to crowdsourced workers is multiple - choice and has an ob - jective answer , but workers may not answer correctly . The approaches further assume that the majority of workers are more likely to be correct than to make a mistake . Under these assumptions , Snow et al . ( Snow et al . 2008 ) showed that one could often achieve high accuracy by exploiting redundancy via majority voting ; indeed , this simple method can enable a crowd of ten novices to reach accuracies of 75 % on NLP tasks such as sentiment analysis . More robust methods learn worker skills . Rather than a simple majority vote , these approaches weight responses by using models of workers’ abilities . The most common skill assessment method is very simple : into a mix of questions given to workers , include a random sample of gold ques - tions , so named because the requester already has “gold standard” answers for these questions . Workers who fail to correctly answer these questions are dismissed or have their weights lowered . To avoid gaming behavior it is common to intermix questions with known and unknown answers . However , even this strategy is foiled by scammers building bots that utilize databases of known questions , necessitating elaborate strategies for programmatically generating an un - bounded number of gold questions ( Oleson et al . 2011 ) . More sophisticated approaches eschew gold questions , in - stead using unsupervised learning to jointly estimate worker accuracies and consensus answers . This method stems from Dawid and Skene ( Dawid and Skene 1979 ) , who consider a single question with an unknown correct answer and also parameters , P w ( r | a ) , for each worker and each possible re - sponse , describing the probability that worker w will re - spond r when the true answer is a ( a simple model of worker abilities ) . Subject to an important assumption , that work - ers’ responses are conditionally independent of each other given the true answer , Dawid and Skene use expectation - maximization ( EM ) to estimate the latent question answers and worker accuracies . Whitehill et al . ( Whitehill et al . 2009 ) note that worker responses are not really indepen - dent unless conditioned on both the correct answer and the question difﬁculty ( another latent variable to be estimated ) . Welinder et al . ( Welinder et al . 2010 ) take Whitehill’s ap - proach a step further , designing a model with general multi - dimensional parameters . Questions have many features , one of which could be difﬁculty , and workers are modeled as lin - ear classiﬁers who make their responses by weighting those features . Kamar et al . ( Kamar , Hacker , and Horvitz 2012 ) extract features from the task at hand and use Bayesian Structure Learning to learn the worker response model . Unfortunately , all EM - based methods share several sig - niﬁcant limitations . First , their greedy optimization is prone to local maxima ; convergence is guaranteed , but not opti - mality . Speciﬁcally , if the data set includes difﬁcult prob - lems where a signiﬁcant majority of workers gets the answer wrong , EM will likely converge to the incorrect answer . In - deed , these problems appear to surface often in practice . For example , Demartini et al . describe ZenCrowd , an elaborate method for named entity linking that combines crowdsourc - ing with probabilistic reasoning yet can’t achieve precision higher than 85 % no matter the number of crowd workers as - signed ( Demartini , Difallah , and Cudr´e - Mauroux 2012 ) . In 33 fact , the authors note “augmenting the number of workers performing a given task is not always beneﬁcial . ” The methods described above are post - hoc — they are ap - plied to a set of worker responses after they are gathered . A complementary set of approaches actively chooses which questions to ask which workers and how many times to ask each question . Karger et al . ( Karger , Oh , and Shah 2011 ) al - gorithmically assign tasks to workers using a low - rank ma - trix approximation , but their method assumes that all tasks are equally difﬁcult and workers are either perfectly cor - rect or randomly guess . Dai et al . pose the worker control problem as a partially - observable Markov decision process ( POMDP ) showing substantial accuracy improvements for a given budget ( Dai et al . 2013 ) . Kamar et al . apply similar techniques to citizen - science applications ( Kamar , Hacker , and Horvitz 2012 ) . These methods are powerful , but they still use techniques like EM to combine worker assessments and hence are susceptible to local maxima . Furthermore , these methods keep workers isolated in order to maximize the independence of errors . As a result there is no way for an expert worker , who has correctly solved a problem , to convince other workers that her answer is correct . Other forms of worker coordination can also be used to improve answer quality . Some complex problems can be solved by dividing work between several workers , through either sequential or simultaneous work ( Andr´e , Kraut , and Kittur 2014 ) . On certain problems , global constraints may be used to coordinate the activities of multiple workers ( Zhang et al . 2012b ) . Unfortunately , some problems , like those we consider , are difﬁcult to partition and may require careful reasoning in order to arrive at a correct answer . Kriplean et al . ( Kriplean et al . 2014 ) built a system for public deliber - ation on election ballot measures , which uses pro / con points to help individual voters make informed decisions ; by con - trast , we seek to use deliberation to arrive a single objective correct answer . Question - and - answer sites like StackOver - ﬂow also produce answers to difﬁcult problems , but can be tricked when the majority is wrong . A complementary form of worker coordination involves interactions between work - ers with various degrees of expertise , either directly ( e . g . , through mentoring ( Suzuki et al . 2016 ) ) or indirectly ( e . g . , through worked examples ( Doroudi et al . 2016 ) ) , in order to improve the expertise of the worker pool . One could also seek to improve expertise by developing an effective proce - dure for training and testing workers ( Liu et al . 2016 ) . Other researchers have developed methods for dealing with cases where the majority may be wrong , including tournament voting ( Sun et al . 2011 ) and Bayesian truth serum ( Prelec and Seung 2006 ) , but it is not clear these methods work well for binary questions or that they will work on problems that require careful reasoning . Another line of work has shown that having annotators provide “rationales” for their answers by highlighting portions of text ( Zaidan , Eisner , and Piatko 2007 ) or an image ( Donahue and Grauman 2011 ) can improve machine learning classi - ﬁers . In contrast , we consider more complex arguments that can include information not found in the example to be clas - siﬁed , and show arguments to other workers rather than a machine . Finally , we note that incentives may also help to LivedIn means that a person spent time in a place for more than a visit . Working in a location does not imply that a person has a LivedIn relation . However , you may assume that someone who has held a national ofﬁce or played for a national sports team has lived in the country they serve or represent . Ambassadors should be counted as national - level ofﬁcials . For example , Claim : Ian Khama ”lived in” Botswana Sentence : Botswana ’s President Ian Khama is one of the few African leaders to openly criticize Mugabe . Ian Khama holds national ofﬁce for Botswana , therefore it can be concluded that he also lives in Botswana . The answer to this question is True . Other Rules : You should only select relations that can be inferred by reading the sentence , even if you know others are true . You also should not select facts that are likely to be true , but you are not sure after reading the sentence . Figure 2 : Annotation guidelines for the LivedIn relation . improve answers or arguments ; Lasecki et al . perform qual - ity control in a crowd - powered conversational agent by pay - ing more to workers who propose or vote on winning an - swers ( Lasecki et al . 2013 ) . Relation Extraction Domain While our argumentation method is general , our evaluation of the method must be in a speciﬁc domain . We introduce that domain now , since it is helpful to use concrete examples as illustrations in the next section . As our test domain we chose the high - level task of an - notating training data for relation extraction , the task of generating structured information ( relational tuples , such as would be found in a SQL database ) from natural language text . Many researchers and practitioners are interested in relation extraction , and several recent efforts use machine learning approaches based on crowdsourced data acquisi - tion ( Zhang et al . 2012a ; Angeli et al . 2014 ; Liu et al . 2016 ) . Speciﬁcally , we consider the problem of annotating a sen - tence to indicate whether it encodes the TAC KBP rela - tion LivedIn . While determining whether a sentence sup - ports the conclusion that a person lived ( or lives ) in a lo - cation might seem simple , the Linguistic Data Consortium annotation guidelines are surprisingly complex ( Surdeanu 2013 ) and some sentences are tricky . For example , just be - cause someone was born in a city or works there does not imply that they lived there . Figure 2 shows the instructions given to our workers . Figure 3 shows a True example . Crowdsourcing Argumentation In order to encourage discussion and reﬂection about the job being performed , MicroTalk combines several microtasks in an adaptive manner . We start by discussing high - level design decisions about the style of argumentation . We next explain MicroTalk’s three microtasks . We argue that some workers are better at argumentation than others and present a method for selecting these “discerning workers . ” The section ends with a description of the complete MicroTalk workﬂow . 34 Figure 3 : The Assess microtask . Figure 4 : The Justify microtask . Synchronous vs . Asynchronous Argumentation The most critical decision when trying to support automated argumentation is whether to implement a synchronous pro - cess , with multiple workers interacting in real time , or an asynchronous method , in which workers can only see the output of previous workers with no back and forth commu - nication . Clearly , the synchronous context is closer to the forms of debate we ﬁnd natural and the ability for partici - pants to discuss each others points interactively would offer many advantages . Unfortunately , implementing such a real - time approach is expensive , requiring a retainer model or other incentives for rapid response ( Bernstein et al . 2011 ; Lasecki and Bigham 2013 ) . As a result , we defer syn - chronous argumentation to future work . In this paper we focus on an asynchronous approach , which simpliﬁes the worker recruitment model and also the design of the primitive microtasks , but poses its own chal - lenges . Speciﬁcally , how can one best simulate a discus - sion when one participant leaves before the other arrives ? Clearly , arguments must be cached , but the order in which they are collected may have a strong effect on workﬂow ef - ﬁciency . We return to these questions after discussing the base units of our workﬂow . Primitive Tasks In order to collect high - quality data from crowd workers , MicroTalk combines worker training and qualiﬁcation with three different microtasks : Assess , Justify , and Reconsider . Training is fairly conventional : MicroTalk displays a set of instructions and rules to the worker in order to explain the task of relation extraction . We determine if a worker is qual - iﬁed to complete our tasks by asking them a set of gold ques - tions with known answers . Workers who answer incorrectly are removed from the rest of the experiment but are paid for the short time they spent . Workers begin to see the other mi - crotasks after completing the training step . Each Assess task asks a worker a question . In our case , we ask them to assess whether a sentence states the LivedIn relation between two speciﬁc entities ( e . g . , see Figure 3 ) . A Justify task may be issued after a worker has com - pleted an assessment . In it , the worker is simply asked to explain their reasoning using a text box with no enforced limit on length . Figure 4 shows an example . In preliminary studies on another NLP domain ( named entity linking ) we considered a variation of the Justify task that provided a more structured interface , requiring workers’ arguments to reference numbered rules in the annotation guidelines . Our reasoning was that this approach might make it faster for workers to both construct and validate arguments , but the process proved cumbersome . When we redesigned the task , we aimed for maximum ﬂexibility , inspired by lessons from Groupware ( Grudin 1994 ) , but as the underlying job ( and hence , justiﬁcations ) becomes increasingly complex , struc - tured arguments may be worth revisiting in future work . MicroTalk’s ﬁnal microtask is Reconsider , which may be issued when a worker selects an answer for which an oppos - ing argument has been acquired in a previous justify task . In such a case , MicroTalk notiﬁes the worker that someone disagrees with his or her decision and presents the counter - argument . The worker is then given a chance to alter their previous decision for full “credit” on the task ( Figure 5 ) . Of course , there is no guarantee that the counterargument is cor - rect , so the success of MicroTalk is ( in part ) dependent on its ability to identify either good justiﬁcations or discerning workers who will recognize their mistakes , but not be fooled by poor arguments . ( More on this in the next section ) . In early prototypes , we also considered additional task designs . For example , we thought workers might ﬁnd it useful to be presented with dueling “pro” and “con” ar - guments during their initial assessment of a question ; this led to the construction of an Assess - w - arguments microtask . Unfortunately , initial experiments worked poorly — work - ers were no more likely to select the correct answer when presented with arguments for both sides . We also consid - ered microtasks that implemented an iterative improvement workﬂow ( Little et al . 2009 ) for enhancing the quality of both “pro” and “con” justiﬁcations . Better justiﬁcations may cause workers to think more deeply about switching their answer ; we further address this in the experiments section . MicroTalk uses the Assess , Justify , and Reconsider tasks in different combinations depending on the current worker’s assessment and prior workers’ responses . Figure 1 summa - rizes the constraints governing the order in which the tasks may be invoked on any given question , and we discuss the ordering in more detail below . Selecting Discerning Workers When experimenting with an early prototype of MicroTalk , we were surprised that almost as many workers were con - vinced to switch away from a correct answer by a bad argu - ment as were moved to ﬁx their answer by sound reasoning . Since crowd workers have been shown to have highly het - erogeneous skills at many domain - speciﬁc tasks , we won - dered if some workers might be better at explicitly reason - ing about the logic underlying their answers than others . We 35 Figure 5 : The Reconsider microtask . deemed these hypothetical savants “discerning workers” and explored methods for identifying them . We conjectured that Law School Admission Test ( LSAT ) questions from the “Critical Reading” section might provide a good method for ﬁltering workers , but initial experiments gave inconsistent and unpromising results . Another method we tried was selecting workers who pro - vide lengthy justiﬁcations for their decisions . Of course , there is no guarantee that verbose writers are actually better workers than the terse , and a percentage of lengthy justiﬁ - cations are indeed muddled . However , word count has been shown to be a surprisingly accurate measure of Wikipedia article’s quality ( Blumenstock 2008 ) . Although this method worked well for selecting “discerning workers” in our ex - periments , it may not generalize to other task domains . Instead , we settled on using Flesch - Kincaid readability tests to determine the complexity of a worker’s argument ( Kincaid et al . 1975 ) . By asking workers to provide justiﬁ - cations on gold questions during the training phase , we can compute the median complexity of submitted justiﬁcations and retain only workers who provide explanations of at least a certain grade level . As we show in our Experiments sec - tion , speciﬁcally Figure 6 , this approach is surprisingly ef - fective . Proactive vs . Lazy Justiﬁcation Given our Assess , Justify , and Reconsider microtasks , we now consider the best way to compose them . As we show in the Experiments section , Reconsider tasks produce higher worker accuracy than Assess tasks , but they are also more expensive . 1 If most questions are easy with no disagreement , then there will be no need to ever issue Reconsider tasks and hence no need to ask any worker to justify their decision . Thus , if most questions are easy , the workﬂow should be lazy and delay issuing Justify tasks until the ﬁrst worker dis - agrees in their initial assessment . On the other hand , if one expects disagreement on most questions , then the workﬂow should proactively ask workers to justify their decisions so it is prepared to issue Reconsider tasks as soon as possible . To visualize this tradeoff , consider two possible se - quences , S and S (cid:48) , of ﬁve worker assessments , reﬂecting 1 In order to Reconsider , the worker must ﬁrst perform Assess and another worker must have completed Assess and Justify . easy and hard questions respectively : S = (cid:104) T , T , T , T , T (cid:105) S (cid:48) = (cid:104) T , T , F , F , T (cid:105) Assume that the system only wishes to collect one justiﬁ - cation for each disputed answer . If the workﬂow issues Jus - tify tasks proactively , then S would generate the following sequence of tasks : (cid:104) A 1 , J 1 , A 2 , A 3 , A 4 , A 5 (cid:105) where A indi - cates an Assess task , J signiﬁes a Justify task , and the su - perscript indicates the worker performing the work . Proac - tive assignment of Justify tasks results in one unnecessary task for sequence S —the lack of disagreement renders the justiﬁcation unnecessary , since no Reconsider is ever issued . In contrast , proactively initiating Justify on S (cid:48) would generate the following sequence of 10 microtasks : (cid:104) A 1 , J 1 , A 2 , A 3 , J 3 , R 3 , A 4 , R 4 , A 5 , R 5 (cid:105) with three recon - siders . If the workﬂow had instead waited for disagreement before requesting justiﬁcation , it would have produced this sequence : (cid:104) A 1 , A 2 , A 3 , J 3 , A 4 , A 5 , J 5 , R 5 (cid:105) . Note that this workﬂow is only able to issue one Reconsider task ( vs . three ) because it doesn’t have the necessary justiﬁcations . If it con - tinued to get workers whose initial assessments followed S (cid:48) then it would continue with (cid:104) A 6 , R 6 , A 7 , R 7 (cid:105) , which re - quires 12 microtasks before getting three Reconsiders . Given the prior probability that independent workers will disagree , one can calculate the expected cost of proactive vs . lazy justiﬁcation . But qualitatively , it is clear that the lazy approach only makes sense when most problems are easy and workers agree . Furthermore , in the worst case proactive justiﬁcation will only require one additional microtask per question , so that is the approach we adopt in MicroTalk . The ( Final ) MicroTalk Workﬂow Algorithm 1 summarizes the complete MicroTalk workﬂow operating on a single Boolean question . In order to qual - ify “discerning workers” a preprocessing phase ( not shown ) trains a sample of k workers and estimates their median jus - tiﬁcation complexity on gold standard problems . This me - dian complexity is then used in the qualiﬁcation step of Al - gorithm 1 . As input the algorithm is given a budget ; each call to Assess , Justify , and Reconsider decreases the remain - ing budget by C a , C j , and C r , respectively . Operation is straightforward . MicroTalk asks the ﬁrst worker to answer the question and also to justify it , since no justiﬁcation exists for her answer ( J a 1 = ‘ ’ ) . In subse - quent iterations MicroTalk tests to see if a counterargument exists for each answer ; a i denotes the opposite answer from a i so if J a i (cid:54) = ‘ ’ , then a counterargument exists . The ﬁrst worker who disagrees with previous answers not only provides a justiﬁcation , but also is given the option to reconsider their answer . If this worker decides to switch their assessment , then their original argument is not kept as a jus - tiﬁcation for their original answer , because the worker was not conﬁdent enough in his or her own reasoning to value it higher than the presented counterargument . After spending the budget on a mixture of Assess , Jus - tify , and Reconsider microtasks , MicroTalk aggregates the 36 Input : A question q , budget B , and task costs C a , C j , C r Output : An answer A ( q ) j T : = j F : = ‘ ’ ; b : = 0 ; for i : = 1 , increase by 1 , while b < B do Justifying = F ; Train and qualify worker w i ; a i : = Assess ( q , w i ) ; b : = b + C a ; if j a i = ‘ ’ then j a 1 : = Justify ( q , a i , w i ) ; b : = b + C j ; Justifying : = T endif j a i (cid:54) = ‘ ’ then a : = Reconsider ( q , j a i , w i ) ; b : = b + C r ; if a (cid:54) = a i ∧ Justifying then j a i : = ‘ ’ end a i : = a end endreturn Aggregate ( a 1 , . . . , a i ) ; Algorithm 1 : The MicroTalk argumentation workﬂow . workers’ disparate opinions and returns the result . Major - ity vote is one way to aggregate assessments , but expecta - tion maximization ( EM ) works better ( as we show in the next section ) . Explicitly incorporating EM into our pseu - docode would complicate the logic , since it requires reason - ing across multiple questions . Experiments The experiments in this section address the following ques - tions : 1 ) Are workers able to formulate a convincing argu - ment with their assessment ? 2 ) Does argumentation have an effect on individual workers’ accuracy ? 3 ) Do workers per - form better when higher quality justiﬁcations are shown ? 4 ) How do we ﬁnd high - quality workers and how much bet - ter at argumentation are they ? 5 ) How does the MicroTalk workﬂow compare with other approaches and is it cost ef - fective ? Experimental Setup To ﬁnd a set of challenging TAC KBP questions , we consid - ered worker annotations for a set of ﬁve TAC KBP person - place relations collected by previous researchers ( Liu et al . 2016 ) . We determined the most difﬁcult sentences by rank - ing them according to the average L 1 distance between the boolean vector representing a worker’s labeling of the ﬁve relations for that sentence and the boolean vector for the ground - truth labeling . Since 20 of the 25 highest ranked ( most difﬁcult ) sentences were positive instances of the LivedIn relation , we used those for our experiments . We ran experiments on Amazon Mechanical Turk , using workers who had completed at least 1 , 000 tasks with a 97 % acceptance rate . As described earlier , the TAC KBP annota - tion guidelines for the LivedIn relation specify a number of counter - intuitive rules , which we make available to workers throughout each experiment as shown in Figure 2 . In each experiment , we ﬁrst trained workers on the guide - lines for annotating the LivedIn relation . Workers were given ﬁve gold standard questions and had to get three ( 60 % ) or more correct to complete the rest of the experiment . For this qualiﬁer , we chose questions that had previously been shown to be the strongest indicators of whether a worker was likely to have a high or low overall accuracy . In the training phase , workers were asked to Assess and then Justify each gold standard question , in order to provide an indication of the quality of that worker’s answers and the arguments they would be likely to supply . Our workﬂows set the payment for each microtask ( As - sess , Justify , Reconsider ) at $ 0 . 05 each . These rates were chosen such that the hourly payout was roughly equivalent across workers in each experiment . Workers were incen - tivized with bonuses for the Justify and Reconsider tasks — they were told they would receive a bonus of $ 0 . 05 for every high - quality argument they provided as well as a bonus of $ 0 . 05 if they chose the correct response for Reconsider . Can Workers Generate Quality Justiﬁcations ? Since the success of the MicroTalk workﬂow depends on the ability of workers to write convincing arguments , our ﬁrst experiment analyzed a sample of the arguments gener - ated by workers to determine their quality . We found that workers provide arguments of highly varying quality , and that workers with higher overall accuracy tended to produce more convincing arguments . As an example , consider the following sentence ( with bolded entity mentions ) taken from our pool of experimental questions : “The United States needs to be ready to press compro - mise proposals , something Bush and his secretary of state , Condoleezza Rice , show little interest in doing . ” We looked at top performing workers’ justiﬁcations and compared these by hand to those of the least accurate work - ers . Justiﬁcations from accurate workers tend towards ex - plicit reasoning , making reference to the annotation guide - lines . Two example justiﬁcations are : “Bush may have been the leader of a different country other than the US that was the other partner in the attempted negotiation . ” and “It is safe to assume that they are talking about President Bush in men - tioning ‘his secretary of state’ , the past president of the US , therefore he would have to live in the US . ” The second jus - tiﬁcation explains that Bush must be a national ofﬁcial be - cause he has a secretary of state , which justiﬁes using the rule that national ofﬁcials lived in a country . However , the ﬁrst claims that this sentence could be referring to another pair of national ofﬁcials that are not necessarily from the United States . On the other hand , low - performing workers tended to fa - vor less complex sentences and used logic that depends on 37 information not found in the sentence . Two example justi - ﬁcations are : “He was president” and “I lived through that disaster . I can honestly say that , yes , Bush lived in this coun - try and he was somehow president . ” The brevity of the ﬁrst justiﬁcation and the reasoning based on personal life expe - rience in the second answer do not make convincing argu - ments for the correct answer . In the following sections , we evaluate techniques for se - lecting workers who can improve their answer accuracy by reconsidering questions in response to high - quality argu - ments , and who are infrequently tricked by incorrect argu - ments . Effect of Argumentation on Individual Workers Our second experiment considers whether argumentation can improve the answer quality of individual workers . Work - ers were assigned to one of two conditions . In the baseline condition ( N = 51 workers ) , workers were only presented with the Assess microtask for each question . In the other condition ( N = 116 workers ) , workers participated in a non - adaptive workﬂow , which consisted of an Assess task , a Justify task , and a Reconsider task . The arguments presented in the Reconsider microtasks were chosen from a previous run of the same experiment , selecting those from the most accurate workers ( the most accurate worker for each ques - tion was determined by that worker’s accuracy on the re - maining questions ) . This experiment used a subset of 10 out of the 20 candidate sentences . 2 As in all our experiments , workers were asked to ﬁrst answer the 5 gold questions and they were paid $ 0 . 05 per task completed . Workers given all three microtasks were signiﬁcantly more accurate than those that only completed Assess tasks . Accuracy in the baseline condition ( Assess only ) was 59 % . Accuracy in the experimental condition ( Assess , Justify , and Reconsider tasks ) was 71 % . This 12 % accuracy improve - ment is statistically signiﬁcant ( p = 0 . 0003 , t = 3 . 695 ) , indicating that Justifying and Reconsidering answers im - proves worker accuracy . We also conﬁrm that workers’ ac - curacies are normally distributed by using a Shapiro - Wilk test for normality ( p > 0 . 05 ) ( Shapiro and Wilk 1965 ) . Some of the increase in accuracy appears to come sim - ply from the act of justifying one’s answers . Even ignoring the revised decisions from Reconsider tasks , workers had a 7 % increase in accuracy , from 59 % in the baseline condi - tion ( Assess task only ) to 66 % in the experimental condi - tion . This difference is signiﬁcant under a two - sided t - test ( p = 0 . 022 , t = 2 . 318 ) . We note that further experiments are required to com - pletely determine whether the improvement in accuracy is due to the Justify or Reconsider tasks , or the combination . Previous research has shown that both self assessment ( Dow et al . 2012 ) and comparing solutions to those provided by other workers ( Mamykina et al . 2016 ) can improve answer quality . Dropout rates were not signiﬁcantly different be - tween the two conditions , but we are unable to rule out a 2 Since workers would be providing arguments and reconsid - ering for every question , we wanted to keep the total time of the experiment comparable to the other experiments . worker selection effect . Either way , the inclusion of Justify and Reconsider jobs substantially improves answer quality compared to solitary Assess microtasks . Effect of High - Quality Justiﬁcations on Workers We also wanted to explore whether more convincing argu - ments written by experts would have an effect on how of - ten workers changed their answer and the accuracy of their answers . To test this , we chose 5 sentences and launched a third experiment where each worker would be assigned a se - quence of Assess and Reconsider microtasks . The counter - argument seen by workers during the Reconsider phase was randomly chosen : either an “expert” argument written col - lectively by the authors or a worker - generated justiﬁcation from an earlier experiment . Workers that saw arguments written by other workers changed their answer 20 % of the time . However , when workers were shown arguments written by the authors , this rate increases to 46 % . Simply put , arguments written by ex - perts are signiﬁcantly more convincing than arguments from workers , χ 2 ( 1 , N = 335 ) = 26 . 29 , p < 0 . 00001 . There was no statistically signiﬁcant difference in the number of responses that were changed to the correct answer in either condition , χ 2 ( 1 , N = 107 ) = 0 . 009 , p = 0 . 924 . Although expert justiﬁcations made no difference in the percentage of responses changing to the correct answer , they were able to convince a signiﬁcantly higher number of workers to change their answer . Reconsidered responses from workers that were shown worker - generated justiﬁcations had a mean accuracy of 71 % , which is similar to the accuracy observed in the previous experiment . Reconsidered responses from workers that were shown arguments written by experts im - proved this accuracy by 9 % from 71 % to 80 % ; however , the difference is only marginally signiﬁcant , χ 2 ( 1 , N = 335 ) = 3 . 023 , p = 0 . 08 , perhaps due to the small sample size . We deem the results promising enough that we wish to consider a future version of MicroTalk which includes iterative im - provement steps to hone the best worker justiﬁcations . Beneﬁts of ‘Discerning Workers’ When experimenting with an early prototype , we found that many workers would switch away from the correct answer when shown a bogus justiﬁcation . To combat this effect we ﬁltered our crowd for “discerning workers” using justiﬁca - tion complexity ( speciﬁcally , the Flesch - Kincaid readability test ( Kincaid et al . 1975 ) ) as our selection metric . To se - lect promising workers , we ﬁrst computed the median read - ability score for workers’ justiﬁcations on the gold - standard ﬁltering questions . We then computed the number of times that a speciﬁc worker has a justiﬁcation with a higher read - ing grade level than the median complexity , averaged over the ﬁve gold questions . Our analysis , shown in Figure 6 , shows the strong effects of this ﬁltering technique applied to workers from our second experiment ( non - adaptive Assess , Justify , and Reconsider microtasks ) . Filtering has a strong positive effect on the reconsidered accuracy of workers ( the magnitude of the effect is stronger for more stringent ﬁlter - ing ) but almost no effect on the accuracy when the workers’ changes are not taken into account . 38 Figure 6 : On average , workers were more accurate after reconsidering their answer in the context of opposing arguments ( red points above blue ) . The X - axis shows the minimum number of times a worker provided an argument more complex than the median complexity on gold standard questions . Filtering workers with simple arguments does not affect the accuracy of their initial assessment ( blue points trend ﬂat as X increases ) , but it strongly improves the accuracy of reconsidered decisions ( red points climb ) . X = 0 shows the crowd unﬁltered , while X = 5 is only workers who had complex arguments on all gold standard questions . The number of workers , N , in each group is displayed as an annotation above the ( red ) reconsidered points . We deﬁne discerning workers as workers with at least 4 ( ≥ 80 % ) justiﬁcations with a higher Flesch - Kincaid score than the median reading grade level . The average accuracy of workers in this group is 63 % without reconsideration and 78 % with reconsideration . ( Recall from the previous section that average accuracy with reconsideration for the general population is 71 % , also shown at X = 0 in Figure 6 . ) In other words , ﬁltering for discerning workers yields a 41 % reduction in error . The signiﬁcance of the improvement can be computed using a non - parametric test by bootstrapping a distribution of the overall mean accuracy for the discern - ing workers and the unﬁltered crowd . Taking the paired dif - ference ( discerning workers — unﬁltered crowd ) between these accuracy distributions results in a distribution of the improvement in accuracy . Bootstrapping a distribution of 10 , 000 samples results in a mean improvement in accuracy of 6 % with a signiﬁcance test for difference : α = 0 . 026 . It is also important to note that our experimental discern - ing worker ﬁltering was done post - hoc , which means that discerning workers were responding to counterarguments that were written by non - discerning workers in most cases . Our result shows that even though discerning workers may be seeing lesser quality arguments , they are still able to make correct decisions about the arguments’ correctness . An inter - esting direction for future work would be to see if the mean improvement in accuracy would increase further if discern - ing workers were constrained to only seeing justiﬁcations provided by other discerning workers . Beneﬁts of Complete Workﬂow While we have shown that our argumentation workﬂow sig - niﬁcantly improves individual worker accuracy , the addi - tional cost of the workﬂow ( i . e . , the Justify and Reconsider steps ) may not be worth the improvement in answer quality . For instance , an alternative use of budget would be to show additional Assess tasks for each question and aggregate an - swers from a larger number of workers . In this section , we demonstrate that controlling for cost , the MicroTalk work - ﬂow out - performs this simpler approach . We launched two different workﬂows for experimenta - tion : Simply Ask and MicroTalk . In Simply Ask , workers did not provide or see justiﬁcations nor were they given Recon - sider tasks . Workers were instead given a sequence of As - sess microtasks . MicroTalk was run as previously described — all workers performed Assess and some were given Jus - tify and Reconsider tasks according to the logic shown in Algorithm 1 . Workers took a median time of 15 minutes to complete twenty sentences using Simply Ask , and a median time of 25 minutes using MicroTalk . The differences in time can be attributed to the extra tasks workers must complete in MicroTalk , which are not present in Simply Ask . MicroTalk requires requesters to implement two addi - tional microtasks over the Simply Ask workﬂow , which has workers only complete the Assess step . In total , our ﬁnal experiment collected responses from 37 workers in our Mi - croTalk workﬂow and 68 workers for the Simply Ask base - line . In order to properly compare the two workﬂows , we ﬁxed a budget for each question and simulated hiring work - ers using data from each . We gathered workers and varied 39 Figure 7 : The best variant of our proposed MicroTalk workﬂow ( MicroTalk - EM ) signiﬁcantly outperforms the best baseline ( Simply Ask - EM ) , reducing mean question error by over 54 % . The mean accuracies shown ( with 95 % conﬁdence intervals ) are computed over 1 , 000 runs of the workﬂow ( randomizing worker order ) on 20 questions . the budget from $ 0 . 00 until a maximum budget of $ 1 . 20 was reached , since that is when the entire population of dis - cerning workers was exhausted in the MicroTalk workﬂow . This was repeated 1 , 000 times per budget in different itera - tions , where the order of workers was randomized each time ( as was the sample of workers since there is a ﬁxed bud - get ) . The two workﬂows would run until the ﬁxed budget was reached , at which point the iteration would terminate and question accuracy was calculated . For MicroTalk’s iter - ations , only workers after the ﬁrst disagreement had a ran - domized order because all workers before the disagreement had the same answer . The point of disagreement is different for every question in the experiment ( and for both condi - tions ) and is determined by the ﬁrst worker to answer dif - ferently than all prior workers . All workers in Simply Ask had a randomized order since they were only completing an Assess task . We then plot the mean accuracy from each con - strained budget that was tested to determine the accuracy of each workﬂow on our set of 20 questions . We consider two versions of Simply Ask : one where deci - sions are aggregated with majority vote and one with expec - tation maximization ( EM ) . With MicroTalk , we also show performance with and without discerning worker ﬁltering . Figure 7 plots the results . Simply Ask performs similarly to previous baseline experiments with an average worker accu - racy of 58 % . Every workﬂow has an expected accuracy of 50 % at X = 0 , since a budget of zero means there are no workers in the worker pool , and aggregation reduces to ran - dom guess . As the budget of each problem is increased , the effects of argumentation become more apparent . MicroTalk ( without discerning worker ﬁltering ) performs as well as Simply Ask at X = 0 . 3 , which translates to 6 workers in Simply Ask . The effects are stronger in MicroTalk where workers are ﬁltered to be discerning workers . At X = 0 . 4 ( 8 workers in Simply Ask ) , MicroTalk is able to achieve an average accuracy of 77 % compared with Simply Ask ( EM ) at 64 % . We note that workers in Simply Ask converge to an an - swer almost immediately with only 3 – 4 workers . By increas - ing the budget and hiring more workers , no new information is gained , and the majority is still incorrect ; thus , Simply Ask never achieves accuracy greater than 65 % in our chal - lenging domain . ( A similar result was reported previously by ( Demartini , Difallah , and Cudr´e - Mauroux 2012 ) . ) In con - trast , MicroTalk produces an 84 % accuracy , achieving an er - ror reduction of over 54 % . Conclusion Instead of isolating workers , we argue that they should de - bate their decisions . As a ﬁrst step towards this vision , we in - troduce a novel , asynchronous workﬂow , MicroTalk , which prompts workers to justify their assessments and confronts them with counterarguments , allowing them to reconsider their decisions . While argumentation improves the accuracy of most people , it is especially successful for the subset of workers who write longer and grammatically sophisticated justiﬁcations for their own reasoning , a class we term “dis - cerning workers . ” We experiment on the NLP task of re - lation extraction ; our most signiﬁcant results show ( 1 ) pre - senting arguments from automatically - identiﬁed “discerning workers” increases individual worker accuracy from 58 % to 78 % , and ( 2 ) controlling for cost , the MicroTalk workﬂow achieves 84 % accuracy overall , compared to 58 % from sim - ple majority vote and 64 % from soft EM . Our future work includes applying MicroTalk to addi - tional tasks , hopefully demonstrating its generality . To fur - ther improve the quality of results , we would like to try offer - ing ﬁnancial incentives for convincing arguments . We also hope to ask multiple workers to iteratively improve prior justiﬁcations , honing the best arguments . Alternatively , one 40 could implement a micro - version of an explicit coordina - tion model like that seen on Wikipedia ( Kittur and Kraut 2008 ) . We suspect that decision - theoretic methods , like those of ( Dai et al . 2013 ) , would improve efﬁciency . Perhaps most interesting would be to explore synchronous workﬂows for argumentation . Challenges would include mechanisms for maintaining a critical mass of real - time workers ( Bern - stein et al . 2011 ) , and some way to ensure that discussion stays focused on the task at hand rather than diverging to - wards entertainment . Acknowledgements Shih - Wen Huang , Mausam , Chris Lin , and the anonymous reviewers gave extremely helpful feedback ; Chris Lin and Angli Liu kindly provided relation extraction data . We thank the crowd workers who made this work possible and note NSF grant IIS - 1420667 , ONR grant N00014 - 15 - 1 - 2774 , the WRF / Cable Professorship , and a gift from Google . References Andr ´ e , P . ; Kraut , R . E . ; and Kittur , A . 2014 . Effects of Simultane - ous and Sequential Work Structures on Distributed Collaborative Interdependent Tasks . In CHI , 139 – 148 . Angeli , G . ; Tibshirani , J . ; Wu , J . ; and Manning , C . D . 2014 . Com - bining distant and partial supervision for relation extraction . In EMNLP 2014 , 1556 – 1567 . Bernstein , M . S . ; Brandt , J . ; Miller , R . C . ; and Karger , D . R . 2011 . Crowds in two seconds : Enabling realtime crowd - powered inter - faces . In UIST . Blumenstock , J . E . 2008 . Size matters : word count as a measure of quality on wikipedia . In WWW 2008 , 1095 – 1096 . Dai , P . ; Lin , C . H . ; Mausam ; and Weld , D . S . 2013 . Pomdp - based control of workﬂows for crowdsourcing . Artiﬁcial Intelli - gence 202 : 52 – 85 . Dawid , A . , and Skene , A . M . 1979 . Maximum likelihood esti - mation of observer error - rates using the em algorithm . Applied Statistics 28 ( 1 ) : 20 – 28 . Dekel , O . , and Shamir , O . 2009a . Good learners for evil tecahers . In ICML . Dekel , O . , and Shamir , O . 2009b . Vox populi : Collecting high - quality labels from a crowd . In COLT . Demartini , G . ; Difallah , D . E . ; and Cudr´e - Mauroux , P . 2012 . Zencrowd : Leveraging probabilistic reasoning and crowdsourcing techniques for large - scale entity linking . In WWW 2012 . Donahue , J . , and Grauman , K . 2011 . Annotator rationales for vi - sual recognition . In ICCV 2011 . Doroudi , S . ; Kamar , E . ; Brunskill , E . ; and Horvitz , E . 2016 . To - ward a Learning Science for Complex Crowdsourcing Tasks . In CHI . Dow , S . ; Kulkarni , A . ; Klemmer , S . ; and Hartmann , B . 2012 . Shep - herding the crowd yields better work . In CSCW 2012 . Giles , J . 2005 . Internet encyclopaedias go head to head . Nature . Grudin , J . 1994 . Groupware and social dynamics : Eight challenges for developers . Commun . ACM 37 ( 1 ) : 92 – 105 . Kamar , E . ; Hacker , S . ; and Horvitz , E . 2012 . Combining human and machine intelligence in large - scale crowdsourcing . In AAMAS . Karger , D . R . ; Oh , S . ; and Shah , D . 2011 . Budget - optimal crowd - sourcing using low - rank matrix approximations . In Conference on Communication , Control , and Computing . Kincaid , J . P . ; Fishburne Jr , R . P . ; Rogers , R . L . ; and Chissom , B . S . 1975 . Derivation of new readability formulas ( automated readabil - ity index , fog count and ﬂesch reading ease formula ) for navy en - listed personnel . Technical report , DTIC Document . Kittur , A . , and Kraut , R . E . 2008 . Harnessing the wisdom of crowds in wikipedia : quality through coordination . In CSCW 2008 . Kriplean , T . ; Bonnar , C . ; Borning , A . ; Kinney , B . ; and Gill , B . 2014 . Integrating on - demand fact - checking with public dialogue . In CSCW , 1188 – 1199 . Lasecki , W . S . , and Bigham , J . P . 2013 . Interactive crowds : Real - time crowdsourcing and crowd agents . In Handbook of Human Computation . 509 – 521 . Lasecki , W . S . ; Wesley , R . ; Nichols , J . ; Kulkarni , A . ; Allen , J . F . ; and Bigham , J . P . 2013 . Chorus : A Crowd - Powered Conversational Assistant . In UIST , 151 – 162 . Little , G . ; Chilton , L . B . ; Goldman , M . ; and Miller , R . C . 2009 . TurKit : Tools for Iterative Tasks on Mechanical Turk . In HCOMP . Liu , A . ; Soderland , S . ; Bragg , J . ; Lin , C . H . ; Ling , X . ; and Weld , D . S . 2016 . Effective crowd annotation for relation extraction . In Proceedings of NAACL and HLT 2016 . Mamykina , L . ; Dimond , J . ; Smyth , T . ; and Gajos , K . Z . 2016 . Learning From the Crowd : Observational Learning in Crowd - sourcing Communities . In CHI . Oleson , D . ; Sorokin , A . ; Laughlin , G . P . ; Hester , V . ; Le , J . ; and Biewald , L . 2011 . Programmatic gold : Targeted and scalable qual - ity assurance in crowdsourcing . In Human Computation Workshop . Prelec , D . , and Seung , S . 2006 . An algorithm that ﬁnds truth even if most people are wrong . Working paper . Shapiro , S . S . , and Wilk , M . B . 1965 . An analysis of variance test for normality ( complete samples ) . Biometrika 52 ( 3 / 4 ) : 591 – 611 . Snow , R . ; O’Connor , B . ; Jurafsky , D . ; and Ng , A . Y . 2008 . Cheap and fast - but is it good ? evaluating non - expert annotations for nat - ural language tasks . In EMNLP , 254 – 263 . Sun , Y . - A . ; Dance , C . R . ; Roy , S . ; and Little , G . 2011 . How to assure the quality of human computation tasks when majority vot - ing fails . In Workshop on Computational Social Science and the Wisdom of Crowds , NIPS . Surdeanu , M . 2013 . Overview of the TAC2013 knowledge base population evaluation : English slot ﬁlling and temporal slot ﬁlling . In TAC 2013 . Suzuki , R . ; Salehi , N . ; Lam , M . S . ; Marroquin , J . C . ; and Bernstein , M . S . 2016 . Atelier : Repurposing Expert Crowdsourcing Tasks as Micro - internships . In CHI 2016 . Wauthier , F . L . , and Jordan , M . I . 2011 . Bayesian bias mitigation for crowdsourcing . In NIPS . Welinder , P . ; Branson , S . ; Belongie , S . ; and Perona , P . 2010 . The multidimensional wisdom of crowds . In NIPS . Whitehill , J . ; Ruvolo , P . ; Bergsma , J . ; Wu , T . ; and Movellan , J . 2009 . Whose vote should count more : Optimal integration of labels from labelers of unknown expertise . In NIPS . Zaidan , O . F . ; Eisner , J . ; and Piatko , C . D . 2007 . Using “annotator rationales” to improve machine learning for text categorization . In Proceedings of NAACL and HLT 2007 . Zhang , C . ; Niu , F . ; R´e , C . ; and Shavlik , J . 2012a . Big data versus the crowd : Looking for relationships in all the right places . In ACL . Zhang , H . ; Law , E . ; Miller , R . ; Gajos , K . ; Parkes , D . C . ; and Horvitz , E . 2012b . Human computation tasks with global con - straints . In CHI 2012 . 41