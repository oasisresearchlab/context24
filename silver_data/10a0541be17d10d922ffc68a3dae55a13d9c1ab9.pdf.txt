LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation Shuyin Ouyang University of Bristol Bristol , United Kingdom Jie M . Zhang King’s College London London , United Kingdom Mark Harman University College London London , United Kingdom Meng Wang University of Bristol Bristol , United Kingdom ABSTRACT There has been a recent explosion of research on Large Language Models ( LLMs ) for software engineering tasks , in particular code generation . However , results from LLMs can be highly unstable ; nondeterministically returning very different codes for the same prompt . Non - determinism is a potential menace to scientific conclu - sion validity . When non - determinism is high , scientific conclusions simply cannot be relied upon unless researchers change their be - haviour to control for it in their empirical analyses . This paper conducts an empirical study to demonstrate that non - determinism is , indeed , high , thereby underlining the need for this behavioural change . We choose to study ChatGPT because it is already highly prevalent in the code generation research literature . We report results from a study of 829 code generation problems from three code generation benchmarks ( i . e . , CodeContests , APPS , and Hu - manEval ) . Our results reveal high degrees of non - determinism : the ratio of coding tasks with zero equal test output across different requests is 72 . 73 % , 60 . 40 % , and 65 . 85 % for CodeContests , APPS , and HumanEval , respectively . In addition , we find that setting the tem - perature to 0 does not guarantee determinism in code generation , although it indeed brings less non - determinism than the default configuration ( temperature = 1 ) . These results confirm that there is , currently , a significant threat to scientific conclusion validity . In order to put LLM - based research on firmer scientific foundations , researchers need to take into account non - determinism in drawing their conclusions . 1 INTRODUCTION LLMs are nondeterministic by nature [ 28 ] . This is because LLMs predict the probability of a word or token given the context , rep - resented by a sample of words . The randomness in large language models typically comes from the sampling methods used during text generation , such as top - k sampling or nucleus sampling [ 26 , 39 ] . As a result , identical instructions or prompts can yield completely different responses in different requests . This non - determinism ( i . e . , the inconsistency in the code candi - dates generated in different requests with identical prompts ) 1 is an essential consideration when using LLM in practice [ 48 ] . Unreli - able and inconsistent code snippets can have significant negative effects on the process of software development , particularly in safety - critical applications where consistency and reliability are paramount [ 10 , 25 ] . It may also undermine developers’ trust in 1 There are other terms in the literature that also refer to non - determinism , such as inconsistency , variance , randomness , and instability . LLMs when completely different suggestions are given at different times [ 53 ] . Moreover , non - determinism affects the reliability and repro - ducibility of empirical software engineering [ 43 ] . Indeed , compared to other tasks of ChatGPT , such as question answering and text summary , the non - determinism threat in code - related tasks is much more serious , because the inconsistency ( especially semantic in - consistency ) often suggests errors in the generated code [ 23 ] . It is therefore of vital importance to understand how serious the non - determinism is for LLM - based software engineering tasks . This paper presents the first systematic empirical study on the threat of non - determinism of ChatGPT in code generation tasks . We choose the code generation task because code generation with Large Language Models ( LLMs ) , such as ChatGPT , has recently attracted significant attention due to its impressive and cutting - edge performance [ 9 , 29 ] . Many publications have emerged from both the software engineering community and the machine learn - ing community on evaluating the capability of ChatGPT in code generation [ 5 , 9 , 14 , 33 , 57 ] . We conduct a series of experiments using the ChatGPT models on three widely - studied code generation benchmarks ( i . e . CodeCon - tests , APPS , and HumanEval ) with 829 coding problems . For each code generation task , we let ChatGPT make five predictions . We then compare the five code candidates from three aspects , namely semantic similarity , syntactic similarity , and structural similarity . We also explore the influence of temperature ( i . e . , a parameter that controls the randomness of the text generated by ChatGPT ) on non - determinism , as well as the correlation between non - determinism and coding task features such as the length of coding instruction and the difficulty of the task . Finally , we compare the non - determinism between GPT - 3 . 5 and GPT - 4 . Our results reveal that the threat of non - determinism in Chat - GPT for code generation is serious . In particular , 1 ) the ratio of problems with not a single equal test output among the top - five code candidates is above 60 % for all the benchmarks we study ; 2 ) the maximum difference of the test pass rate reaches 1 . 00 for all three datasets , and accounts for 48 . 17 % of the problems in Hu - manEval , the most widely used code generation benchmark ; In addition , contrary to the widely held belief ( and practice followed to minimize nondeterminism ) [ 6 , 12 , 31 ] , setting the temperature to zero does not guarantee determinism in code generation . Also interestingly , our correlation analysis suggests that the length of coding instructions has a negative correlation with syntactic , and a r X i v : 2308 . 02828v1 [ c s . S E ] 5 A u g 2023 , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang structural similarity , and test pass rate , meaning that longer de - scription length tends to yield code candidates with less syntactic and structural similarity and more buggy code . To understand how the literature handles the non - determinism threat , we collect 76 LLM - based code generation papers that ap - peared in the past 2 years . Our manual analysis results highlight that only 22 . 4 % of these papers consider the non - determinism threat in their experiments . These results highlight that there is currently a significant threat to scientific conclusion validity . We call for researchers to take into account the non - determinism threat in drawing their conclusions . To summarize , this paper makes the following contributions : • We present the first study of the non - determinism threat in code generation tasks on ChatGPT , with three widely - studied datasets ( CodeContest , APPS , HumanEval ) and three types of similar - ity measurements . Our results reveal that the non - determinism threat is serious and deserves attention from both academia and industry . • We study the influence of temperature on the non - determinism of ChatGPT and find that setting temperature to zero does not guarantee determinism in code generation , which is contrary to many people’s beliefs . • We study the correlation between coding task features and the degree of non - determinism . The results reveal that the length of coding instruction has a negative correlation with syntactic and structural similarity , as well as the average correctness of the generated code . • We collect and analyze the LLM - based code generation papers that appeared in the past 2 years and find that only 21 . 1 % of these papers consider the non - determinism threat in their experiments . We call for researchers to take into account non - determinism when conducting experiments and drawing conclusions , consid - ering the nature of the non - determinism of LLMs . We release our data , code , and results at our homepage [ 1 ] . The rest of the paper is organized as follows . Section 2 introduces the main procedure of our study . Section 3 describes the design of the experiments , including research questions , benchmarks , selected models , and measurement tools . Section 4 presents the results and discusses some interesting findings based on the experimental re - sults we obtained . Section 5 discusses the threats to validity in two aspects , namely , dataset construction and test assessment met - rics and tools . Section 6 introduces the related work of our study . Section 7 discusses the implications for software developers and researchers . Section 8 concludes . 2 METHOD Fig 1 shows an overview of our experimental procedure . For each code generation task , our study first produces a prompt with a coding instruction , then feeds this prompt to ChatGPT API 2 to generate code ( zero - shot ) . We call the API five times to let ChatGPT make five predictions with the same prompt . We then extract code from each of the five responses , to get five code candidates . Our non - determinism analysis compares the five code candidates in terms of their semantic similarity , syntactic similarity , and structural similarity . 2 https : / / platform . openai . com / docs / api - reference / chat datasets Optimised code Bugs existence ? Generated code S context - similar mutation structural filtering similarity analysis probability or cross reference machine translator automatic test input generation automatic test oracle generation automatic inconsistency repair mutant candidates filtered mutants original translation mutant translations final translation Inconsistency ? Yes best translation original sentence machine translator t r an s l a t i on m app i ng original translation input t ( S ) translation output for S S ? transformed translation input t ( S ? ) translation output for S ? machine translator machine translator sim ( t ( S ) , t ( S ' ) ) < r ? similarity analysis between t ( S ) and t ( S ' ) repair t ( S ) repair t ( S ' ) Yes repair with the best mutant translation R ( t ( S ) ) R ( t ( S ' ) ) similarity analysis between R ( t ( S ) ) and R ( t ( S ' ) ) sim ( R ( t ( S ) ) , R ( t ( S ' ) ) ) < r ? Is the similarity between the two translations smaller than threshold r ? Is the similarity between the two repaired translations smaller than threshold r ? repair t ( S ' ) with another mutant translation Yes end No au t o m a t i c i n c on s i s t en cy r epa i r au t o m a t i c t e s t i ng t ( S ' ) has other mutant translations ? Does t ( S ' ) have mutant translations that have not been used ? Yes No repair results repair with the next - best mutant translation Learning program Conversational AI model building Offline validation Online deployment chatbot user User input Original bot response Online fairness testing Online fairness enhancement Fair bot response Explainability improvement via per - mutation and causal analysis Training data data generation algorithm improvement data augementation mutation filtering fairness analysis ensemble chatbot WP1 : test input and oracle design mutant candidates filtered mutants resonse for mutants final bot response unfairness ? best and fair response user input r e s pon s e ed i t i ng chatbot augmented data fine - tuning WP2 : automatic fairness testing mutation filtering fairness analysis ensemble chatbot mutant candidates filtered mutants original bot reponse resonse for mutants final bot response unfairness ? best and fair response user input r e s pon s e ed i t i ng chatbot augmented data fine - tuning Yes WP3 : automatic fairness enhancement Yes original bot reponse WP4 : Data augmentation and fine - tuning AI code generator WP1 : Bug prediction WP2 : Automatic testing WP3 : Automatic bug localisation WP4 : Automatic bug repair Developers Yes Bug corpus Black - box optimisation Return original code to developers No WP5 : Tool and dataset developoment Automatic bug detection Automatic bug repair Understanding W1 : Capabilitites and risks W2 : Influence on the society Correctness and Performance Security , Fairness , and Privacy Robustness and Stability Understandability and Evolvability Industry Education Research Before code generation W4 : Code assessment W5 : Code optimisation W3 : Prompt engineering Assuring After code generation Assessing Before code generation Code testing Code optimisation Prompt engineering Assuring After code generation Piloting Influence on the society Industry Education Research Government Trustworthiness of code Correctness and Performance Security , Fairness , and Privacy Robustness and Stability Understandability and Evolvability prompt ChatGPT problem description response 1 response 2 response n . . . program 1 program 2 program n . . . test execution results text analysis AST Semantic similarity Syntactic similarity Structural similarity test suite Figure 1 : Overview of the experimental method . Prompt Preparation : The first step in our study is prompt prepara - tion . Following the common practice in LLM - based code generation assessment [ 4 ] , we ask ChatGPT to generate Python code for each code generation task with zero - shot prompting . To guarantee that ChatGPT produces code rather than pure natural languages in its response , we augment the original code generation description with a request for Python code . One challenge in extracting the code from the API response is that there is no clear signal to distin - guish code with plain text in the response , which is different from ChatGPT’s web chat window ( i . e . in the chat window , codes are returned with Markdown code blocks ) . To address this problem , we specify the format of the generated code into ‘Markdown’ . Thus , for each code generation task , our prompt becomes ‘Generate Python3 code ( Markdown ) : ’ plus the original task description . Code Extraction and Script Writing : After receiving the re - sponse from ChatGPT , we apply code extraction to retrieve the code from the generated text . We compile the code directly with - out making any modifications . Our experiments are mainly run on Google Deep Learning VM instances , with the Linux environ - ment pre - installed from open images 3 . All of the necessary libraries are pre - installed . In this way , it can ensure to the greatest extent that the generated code will not cause import errors caused by the library not being installed during running . Test Case Execution : To evaluate the semantics of ChatGPT’s generated code , we use the test suite that is suited to each bench - mark . We not only record whether each test passes or not but also record every specific test output , which enables us to compare the similarity of test outputs even if they both fail . For CodeContests and HumanEval datasets , every problem has a certain timeout value of 3 seconds . The APPS dataset does not provide a default timeout value , and we set the value to be 3 seconds as well . We use single - threaded scripts to run the tests to ensure that the test cases are executed sequentially to avoid race conditions that may arise from concurrent executions . Similarity Checking : To measure the similarity between code candidates , we introduce similarity measurement tools that eval - uated the semantic , syntactic , and structural similarity between the generated code solutions . The semantic similarity is measured by comparing test execution outputs . The syntactic similarity is measured by comparing the text similarity between codes . The structural similarity is evaluated by comparing the code candidates’ abstract syntax trees ( ASTs ) . More details about our similarity mea - surement methods are mentioned in Section 3 . 4 . 3 https : / / cloud . google . com / compute / docs / images LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation , , 3 EXPERIMENTAL DESIGN 3 . 1 Research Questions This study answers the following questions : RQ1 : TowhatextentisChatGPTsusceptibleto non - determinism in code generation ? This RQ investigates the non - determinism of ChatGPT in terms of the semantic , syntactic , and structural similar - ity among the code candidates generated with identical instructions . There are three sub - RQs : • Sub - RQ1 . 1 : To what extent is ChatGPT susceptible to non - determinism in terms of semantic similarity ? • Sub - RQ1 . 2 : To what extent is ChatGPT susceptible to non - determinism in terms of syntactic similarity ? • Sub - RQ1 . 3 : To what extent is ChatGPT susceptible to non - determinism in terms of structural similarity ? RQ2 : Howdoes temperatureaffectthedegreeofnon - determinism ? Temperature is a hyperparameter of LLMs for controlling the ran - domness of the predictions . This RQ checks and compares the non - determinism of ChatGPT in code generation with different choices of temperature . RQ3 : How does the non - determinism compare to the similarity of the top code candidates generated within the same predic - tion ? ChatGPT can be configured to generate multiple candidates for one prediction , which are ranked by their predictive probability . This RQ compares the similarity of the code candidates obtained in different predictions with those obtained within the same predic - tion . RQ4 : Is there any correlation between different code generation tasks and the non - determinism degree ? To understand what affects non - determinism , this RQ studies the correlation between code generation features ( e . g . , the length of code generation instruc - tions and the code generation difficulty ) and the similarity metric uesed in our study . RQ5 : How is GPT - 4’s non - determinism compared with GPT - 3 . 5 ? This RQ compares GPT - 3 . 5 and GPT - 4 in their degree of non - determinism in generating code . 3 . 2 Code Generation Benchmarks Our experiments use the three most widely studied code generation benchmarks : CodeContest [ 29 ] , APPS [ 22 ] , and HumanEval [ 11 ] . Table 1 shows the details of each benchmark . We also introduce more details below : Table 1 : Code generation benchmarks Name Mean Length No . of Mean No . of Mean No . of of description Problems Test Cases Correct Codes CodeContests 1989 . 19 165 203 . 84 49 . 99 APPS 1663 . 94 500 80 . 43 20 . 92 HumanEval 450 . 60 164 9 . 24 1 . 00 CodeContests : CodeContests is used when training AlphaCode , which comprises programming problems from various sources such as Aizu 4 , AtCoder 5 , CodeChef 6 , CodeforcesCodeChef 7 , and Hack - erEarthCodeChef 8 . In our experiment , following the assessment practice of AlphaCode , we use the test set of CodeContests to bench - mark the code generation tasks of ChatGPT . APPS : APPS includes 10 , 000 problems ( both the training set and testing set ) , with different difficulty levels ( i . e . introductory level , interview level , and competition level ) . This dataset is exclusively designed for Python program synthesis evaluation . The original test set contains 5 , 000 code - generation problems , and we randomly sample 500 problems . HumanEval : The HumanEval dataset is an evaluation set first proposed in [ 11 ] , which contains 164 hand - written code problems . Each problem includes a function signature , docstring , body , and several unit tests , with an average of 9 . 24 test cases per problem . We use the whole dataset to benchmark our experiments . As mentioned in Section 2 , we especially focus on the code generated with Python3 language , since it is one of the most widely studied programming languages in code generation [ 4 , 11 , 15 , 29 , 50 , 52 , 54 ] . 3 . 3 Configuration of ChatGPT ChatGPT has gained widespread popularity and recognition in multiple tasks including question - answering , language translation , sentiment analysis , and text summarising , among which code gen - eration is one of the most impressive tasks [ 9 , 29 ] . There are several reasons why we have chosen ChatGPT as our research target among all large language models . Firstly , ChatGPT has the ability to gen - erate highly coherent and contextually appropriate responses to a wide variety of textual prompts [ 21 ] . This makes it an ideal tool for conducting research in areas of code generation by designing spe - cific prompts . Secondly , the GPT - 3 . 5 series is a particularly attractive option due to its impressive performance and large - scale training data , which allows for more accurate and nuanced language pro - cessing capabilities [ 36 ] . Thirdly , the model API ‘gpt - 3 . 5 - turbo’ and ‘gpt - 4’ released with ChatGPT have not been extensively studied in academia , and their capabilities in terms of code generation are thus still unknown . Therefore , we choose them as our experiment target models . Written in ChatGPT’s official website 9 , using ChatGPT’s model API requires various parameters . We use the default values for most of the parameters in addition to the following ones : • model : ID of the model to use , such as ‘gpt - 3 . 5 - turbo’ and ‘text - davinci - 003’ . This parameter is strictly required , and in our case , we set this parameter to ‘gpt - 3 . 5 - turbo’ or ‘gpt - 4’ . • message : A list of messages describing the conversation so far , where two key values ‘role’ and ‘content’ should be filled . This parameter is also strictly required . In our experiments , the mes - sage’s ‘role’ is ‘user’ and the ‘content’ contains the prompt we used for requesting . • temperature : What sampling temperature to use , between 0 and 2 ( Default value is 1 ) . Higher values will make the output more 4 https : / / judge . u - aizu . ac . jp 5 https : / / atcoder . jp 6 https : / / www . codechef . com 7 https : / / codeforces . com 8 https : / / www . hackerearth . com 9 https : / / platform . openai . com / docs / api - reference / chat / create , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang random , while lower values will make it more focused and deter - ministic . In our study , we study the influence of temperature in RQ3 with three temperature values : 0 , 1 , and 2 . For the remaining RQs , we use the default temperature value ( i . e . , temperature = 1 ) . • top _ p : An alternative to sampling with temperature , called nu - cleus sampling , where the model considers the results of the tokens with top _ p probability mass . In our experiment , we don’t take it into consideration and set this value to remain at its default setting ( i . e . , top _ p = 1 ) . • n : How many chat completion choices to generate for each input message . ( Default value is 1 ) This parameter is specifically used for RQ3 which compares the difference in different requesting ways . We choose n = 5 , since five is a commonly used figure in variance study papers [ 40 ] . We generate five responses separately by requesting the ChatGPT model five times but only take the first response from each time request . 3 . 4 Non - determinism Measurement In order to answer our research questions , we introduce the follow - ing tools for measuring the degree of non - determinism . 3 . 4 . 1 Semantic similarity . We measure the semantic similarity of different code candidates by checking their similarity in test exe - cution results , including test pass rate and output equivalence rate . The test pass rate calculates the ratio of the passed test case number against the total test case number for code candidates . It is one of the most widely used measurement metrics for assessing code generation capabilities 10 [ 4 , 11 , 22 , 29 , 60 ] . Each code genera - tion problem has five test pass rates , one for each code candidate . We use the variance and maximum difference of the five values to indicate semantic similarity . We also calculate the mean of the five values for the purpose of understanding correctness as well as the correlation between correctness and non - determinism ( RQ4 ) . The output equivalence rate records the ratio of equivalent test outputs ( across different code candidates for the same code genera - tion instruction ) against the total test outputs . Each instruction has one output equivalence rate . For tests that produce specific outputs ( without exceptions or errors ) , we check whether the output values of different code candidates are equal to each other . In the following parts of this paper , we use OER to represent output equivalence rate and use OER ( no ex . ) to represent output equivalence rate ( without exceptions or errors ) for short . Each code generation problem has only one OER and OER ( no ex . ) . For tests that produce specific out - puts ( without exceptions or errors ) , we check whether the output values of different code candidates are equal to each other . For tests that yield exceptions or timeout errors , we consider the test outputs to be the same if the exception or error messages are the same . Some papers use the pass @ k metric [ 11 , 27 ] ( i . e . , the ratio of coding tasks with 100 % test pass rate ) to indicate the high - level code generation correctness of a code generation approach . We do not use this metric in our experiments because we focus on the non - determinism threat , while pass @ k ignores the correctness of each single coding task and concentrates only on the ratio of correct code 10 Although the benchmarks are very widely studied , their test suites can be inade - quate [ 33 , 34 , 45 ] . This paper is less affected by the inadequate test suite issue as we focus on the similarity of test pass rate , rather than the absolute value of test pass rate . candidates in all the tasks , which can cover the non - determinism across different requests . 3 . 4 . 2 Syntactic similarity . The syntactic similarity in this study treats different code candidates as texts and checks their textual similarity . We choose the Longest Common Subsequence and Levenshtein Edit Distance as evaluation tools . In the following content , we use LCS and LED to represent the Longest Common Subsequence and Levenshtein Edit Distance for short respectively . LCS measures the similarity via the normalized length of the longest common subsequence between two sequences . LED measures the minimum number of single - token edits ( insertions , deletions , or substitutions ) required to change one code into the other . LCS and LED both regard the token as the smallest unit , and the token is divided by the . split ( ) method , that is , any whitespace is used as the separator to divide the code into tokens . LCS and LED are used to measure the other four code candidates’ syntactic similarity with the first one . Thus , each code - generation problem has four values of each metric . We use the mean and worst value to indicate the syntactic similarity measured by each metric . Below are the formulas for the LCS and LED : 𝐿𝐶𝑆 = 𝑙𝑒𝑛 ( 𝑙𝑐𝑠 ( 𝑠 , 𝑡 ) ) 𝑙𝑒𝑛 ( 𝑠 ) where 𝑠 is reference string , 𝑡 is the string to be compared , 𝑙𝑐𝑠 ( 𝑠 , 𝑡 ) is the longest common subsequence between 𝑠 and 𝑡 . LED 𝑠 , 𝑡 ( 𝑖 , 𝑗 ) =   max ( 𝑖 , 𝑗 ) if min ( 𝑖 , 𝑗 ) = 0 min   led 𝑠 , 𝑡 ( 𝑖 − 1 , 𝑗 ) + 1 led 𝑠 , 𝑡 ( 𝑖 , 𝑗 − 1 ) + 1 led 𝑠 , 𝑡 ( 𝑖 − 1 , 𝑗 − 1 ) + 1 ( 𝑠𝑖 ≠ 𝑡𝑗 ) otherwise where LED 𝑠 , 𝑡 ( 𝑖 , 𝑗 ) is the LED between the first 𝑖 characters of 𝑠 and the first 𝑗 characters of 𝑡 , and diff ( 𝑠 𝑖 , 𝑡 𝑗 ) is 0 if the 𝑖 - th character of 𝑠 is the same as the 𝑗 - th character of 𝑡 , and 1 otherwise . 3 . 4 . 3 Structural similarity . We design structural similarity to mea - sure the code similarity in terms of the Abstract Syntax Tree ( AST ) . AST is a tree - like representation of the source code in which each node in the tree represents a construct in the code , such as variable , function , or control structure , and the edges between nodes repre - sent the relationships between these constructs . We use a Python library called pycode _ similar 11 [ 30 , 55 ] to calculate the similarity . The pycode _ similar normalizes Python code into AST represen - tation and uses Python library difflib to get the modification from referenced code to target code . There are two difference mea - surement settings , i . e . Unified _ Diff and Tree _ Diff . Unified _ Diff measures the difference of normalized function AST string lines , while Tree _ Diff measures the difference in tree edit distance be - tween two given ASTs . Similar to syntactic similarity , for each code generation problem , we compare the first code candidate with the remaining four and report the mean and smallest similarity values . 4 RESULTS AND FINDINGS This section introduces the experimental results as well as the analysis and discussion for each RQ . 11 https : / / github . com / fyrestone / pycode _ similar LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation , , 4 . 1 RQ1 : Non - determinism of ChatGPT with Three Types of Similarities 4 . 1 . 1 RQ1 . 1 : Semantic Similarity . Semantic similarity is measured by the following metrics : test pass rate and OER ( output equivalence rate ) , and OER excluding exceptions . As mentioned in Section 3 . 4 , each coding problem has five test pass rates , we use the variance and maximum difference of these five values to indicate ChatGPT’s non - determinism in generating code for the task . We also report the mean value , which represents the average correctness of the generated code . For OER or OER ( no ex . ) , each coding problem has only one value , indicating the test output similarity across all five code candidates . For each dataset , we report the distribution of different measurements in Figure 2 and Figure 3 . The mean measurement values for all the coding problems ( the mean value inside each bar in each bar chart ) in a dataset are shown in Table 2 . The max diff refers to the maximum value of the max diff among all the code problems . In addition , Table 2 also shows the “Ratio of worst cases” , which is the ratio of problems with maximum diff of test pass rate being 1 or OER being 0 . C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 0 0 . 1 0 . 2 ( a ) Variance C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( b ) Mean C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( c ) Max Diff Figure 2 : RQ1 . 1 : Distribution of semantic similarity in terms of test pass rate . C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( a ) OER C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( b ) OER ( no ex . ) Figure 3 : RQ1 . 1 : Distribution of semantic similarity in terms of test output equivalence rate ( OER and OER ( no ex . ) ) . From Figure 2 , Figure 3 , and Table 2 , we observe that ChatGPT is very unstable in generating semantically consistent code candidates . In particular , the ratios of tasks with zero equal test output ( i . e . , OER = 0 ) among the five code candidates are 72 . 73 % , 60 . 40 % , and 65 . 85 % for the three datasets , respectively . This indicates that for the majority of the cases , ChatGPT generates code candidates with completely different semantics from identical instructions . The mean variance of the test pass rate is not big from Table 2 , ranging between 0 . 03 and 0 . 10 , this is because the test pass rate of different code candidates is often equally worse , as can be observed Table 2 : RQ1 . 1 : Results of semantic similarity . OER and OER ( no ex . ) are the output equivalence rate and the equivalence rate excluding exceptions . Semantic similarity Metric CodeContests APPS HumanEval Test pass rate Mean value 0 . 13 0 . 35 0 . 55 Mean variance 0 . 03 0 . 04 0 . 10 Mean max diff 0 . 27 0 . 59 0 . 36 Max diff 1 . 00 1 . 00 1 . 00 Ratio of worst cases 7 . 88 % 10 . 80 % 48 . 17 % OER Mean value 0 . 13 0 . 22 0 . 30 Worst value 0 . 00 0 . 00 0 . 00 Ratio of worst cases 72 . 73 % 60 . 40 % 65 . 85 % OER ( no ex . ) Mean value 0 . 05 0 . 17 0 . 25 Worst value 0 . 00 0 . 00 0 . 00 Ratio of worst cases 81 . 82 % 66 . 00 % 71 . 34 % from Figure 2 . ( a ) . However , the max diff of the test pass rate reaches 1 . 00 for all three datasets and accounts for 48 . 17 % of the problems in HumanEval , the most widely used code generation benchmark . This indicates the correctness of code candidates generated from the same instruction can vary significantly . Among the three datasets , we observe that the HumanEval dataset demonstrates the largest variance in the test pass rate , but also the best correctness in terms of test pass rate . We suspect that this is because the coding tasks in HumanEval are relatively easy for ChatGPT . The large difference in different datasets also sheds light on the importance of using multiple datasets when assessing the code generation performance for large language models . Answer to RQ1 . 1 : The semantic difference among the code generated by ChatGPT in different requests is sig - nificant . In particular , the ratio of coding tasks with not a single equal test output among the five different requests is 72 . 73 % , 60 . 40 % , and 65 . 85 % for CodeContests , APPS , and HumanEval , respectively . In addition , the maximum differ - ence of the test pass rate reaches 1 . 00 for all three datasets and accounts for 48 . 17 % of the problems in HumanEval , the most widely used code generation benchmark . 4 . 1 . 2 RQ1 . 2 : Syntactic Similarity . Syntactic similarity measures the text similarity among code candidates . In our experiment , the syntactic similarity is evaluated by the following metrics : LCS and LED ( more details in Section 3 . 4 ) . For the five code candidates for each coding problem , we use the first code candidate as reference and calculate the LCS and LED between the reference and the remaining four candidates . Thus , each problem has four LCS values and LED values , each value indicating a syntactic similarity . We use the mean of these four values as well as the worst of them ( i . e . , the smallest value for LCS and the largest value for LED ) to represent each problem’s syntactic similarity . Figure 4 shows the distribution of LCS and LED for all the problems in each dataset . Table 3 shows the mean and worst LCS and LED values for all the coding problems ( the mean value inside each bar in the figures ) in a dataset . We observe that the code candidates generated from the same instruction also differ largely in the syntactic measure . Specifically , the mean LCS is 0 . 12 , 0 . 14 , and 0 . 45 for CodeContests , APPS , and , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( a ) LCS mean C o d e C o n t e s t s A P P S H u m a n E v a l 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 ( b ) LCS worst C o d e C o n t e s t s A P P S H u m a n E v a l 0 200 400 600 ( c ) LED Mean C o d e C o n t e s t s A P P S H u m a n E v a l 0 200 400 600 ( d ) LED Worst Figure 4 : RQ1 . 2 : Distribution of syntactic similarity ( LCS & LED ) . Lower LCS and higher LED indicate less syntactic similarity . Table 3 : RQ1 . 2 : Syntactic similarity ( LCS and LED ) . Lower LCS and higher LED indicate lower syntactic similarity . Syntactic Similarity Metric CodeContests APPS HumanEval LCS Mean value 0 . 12 0 . 14 0 . 45 Worst value 0 . 07 0 . 09 0 . 33 LED Mean value 119 . 75 85 . 61 38 . 86 Worst value 165 . 28 120 . 27 60 . 52 HumanEval , respectively , indicating the mean ratio of the longest common subsequences among the code candidates . For the three datasets , we could see from Table 3 that the lowest LCS and largest LED values both happen for the CodeContests dataset . By contrast , the largest LCS and smallest LED values both happen for HumanEval . This indicates that ChatGPT is most un - stable syntactically for the code generation tasks in CodeContests , and most stable for HumanEval . We further explore the correlation between different similarities and code task features in RQ4 . Answer to RQ1 . 2 : Code candidates generated by Chat - GPT in different requests also differ significantly in syntax . The mean syntax similarity ( LCS ) is only 0 . 12 , 0 . 14 , and 0 . 45 for CodeContests , APPS , and HumanEval , respectively . 4 . 1 . 3 RQ1 . 3 : Structural Similarity . Structural similarity measures the codes’ similarity based on their AST . In our experiment , the structuralsimilarityismainlymeasuredby thetool pycode _ similar with two difference settings , namely United _ Diff and Tree _ Diff ( more details in Section 3 . 4 ) . For the five code candidates for each coding problem , we use the first code candidate as reference and cal - culate the structural similarity between the first candidate with the remaining four candidates under United _ Diff and Tree _ Diff settings . Thus , each problem has four values for United _ Diff and Tree _ Diff respectively , with each value indicating a structural similarity mea - sure . We use the mean of these four values as well as the worst of them ( i . e . , the smallest value for United _ Diff and Tree _ Diff ) to represent each problem’s structural similarity . Fig 5 shows the dis - tribution of United _ Diff and Tree _ Diff for all the problems in each dataset . Table 4 shows the mean and worst values under United _ Diff and Tree _ Diff settings for all the coding problems ( the mean value inside each bar in the figures ) in a dataset . Table 4 : RQ1 . 3 : Structural similarity . Structural Similarity Metric CodeContests APPS HumanEval United _ Diff Mean value 0 . 46 0 . 52 0 . 69 Worst value 0 . 25 0 . 33 0 . 43 Tree _ Diff Mean value 0 . 60 0 . 68 0 . 72 Worst value 0 . 36 0 . 46 0 . 44 CodeContests APPS HumanEval 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 UnifiedDiff TreeDiff ( a ) Mean CodeContests APPS HumanEval 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 UnifiedDiff TreeDiff ( b ) Worst Figure 5 : RQ3 : Structural Similarity ( UnitedDiff & TreeDiff ) . We observe that the code candidates generated from the same instruction show great similarity in structure . Specifically , the mean values are 0 . 46 , 0 . 52 , and 0 . 69 under the United _ Diff setting , and 0 . 60 , 0 . 68 , and 0 . 72 under Tree _ Diff setting for CodeContests , APPS , and HumanEval , respectively , which indicates the mean ratio of AST similarity among the code candidates . For the three datasets , we could see from Table 4 that the lowest values under United _ Diff and Tree _ Diff happen for the CodeCon - tests dataset . By contrast , the largest values under the two settings both happen for HumanEval . This indicates that ChatGPT is most unstable in structure for the code generation tasks in CodeContests , and most stable for HumanEval . We further explore the correlation between different similarities and task features in RQ4 . Answer to RQ1 . 3 : Code candidates show high structural similarity under UnitedDiff and TreeDiff settings . We ob - serve that the code candidates generated from the same in - struction have high similarity in structure . Specifically , the mean values are 0 . 46 , 0 . 52 , and 0 . 69 under the United _ Diff setting , and 0 . 60 , 0 . 68 , and 0 . 72 under Tree _ Diff setting for CodeContests , APPS , and HumanEval , respectively . LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation , , 4 . 2 RQ2 : Influence of Temperature The default temperature of ChatGPT is 1 . This RQ explores whether the code generation non - determinism of ChatGPT changes with the changes in temperature . We use identical measurements as in RQ1 . Due to space limitation , we only show the results for the CodeContests dataset as ChatGPT performs worst on it in different types of similarity in code generation , allowing us more space to observe the influence of temperature . The results for the other two datasets are on our homepage [ 1 ] . Table 5 : RQ2 : Influence of temperature . Temperature Test Pass Rate Mean Mean Mean Max Ratio of value variance max diff diff worst cases 0 0 . 15 0 . 02 0 . 21 1 . 00 4 . 85 % 1 0 . 13 0 . 03 0 . 27 1 . 00 7 . 88 % 2 0 . 01 0 . 01 0 . 02 1 . 00 0 . 61 % Temperature OER Mean value Worst value Ratio of worst cases 0 0 . 72 0 . 00 16 . 36 % 1 0 . 13 0 . 00 72 . 73 % 2 0 . 25 0 . 00 75 . 15 % Temperature OER ( no ex . ) Mean value Worst value Ratio of worst cases 0 0 . 52 0 . 00 33 . 94 % 1 0 . 05 0 . 00 81 . 82 % 2 0 . 01 0 . 00 99 . 39 % Temperature LCS LED Mean value Worst value Mean value Worst value 0 0 . 77 0 . 66 15 . 19 26 . 91 1 0 . 12 0 . 07 119 . 75 165 . 28 2 0 . 00 0 . 00 52 . 18 96 . 10 Temperature United _ Diff Tree _ Diff Mean value Worst value Mean value Worst value 0 0 . 84 0 . 79 0 . 86 0 . 83 1 0 . 34 0 . 25 0 . 45 0 . 34 2 0 . 00 0 . 00 0 . 01 0 . 00 Table 5 shows the results . Overall , we observe that when tem - perature = 0 , ChatGPT has better determinism than the default con - figuration ( temperature = 1 ) for all three types of similarities . How - ever , setting the temperature to 0 does not completely avoid non - determinism . Take OER as an example , there are still 16 . 36 % of problems with no equal test output among the five code candidates . This is contrary to many people’s belief ( and common practice ) that setting the temperature to 0 can make ChatGPT determinis - tic [ 6 , 12 , 31 ] . When temperature = 2 , interestingly , we observe that the mean variance of the test pass rate is only 0 . 01 , lower than both temper - ature = 0 and temperature = 1 , and the ratio of worst cases is only 0 . 61 % . This is because the temperate brings too much creativity to ChatGPT and affects its ability in generating correct code ( as can be observed from the mean test pass rate ) . As a result , the generated code candidates tend to be uniformly worse . Answer to RQ2 : Contrary to the widely held belief ( and common practices ) , setting temperature to 0 does not guar - antee determinism in code generation , although it indeed brings better determinism than the default configuration ( temperature = 1 ) for all three types of similarities . Setting the temperature to 2 tends to yield uniformly worse code candidates , which may perversely improve the similarity among code candidates . 4 . 3 RQ3 : Non - determinism Comparison with Top Candidates in the Same Prediction RQ1 and RQ2 compare the similarity of 5 code candidates gener - ated in multiple requests . Each candidate is the top - 1 candidate in each request . However , ChatGPT can also generate 5 code candi - dates within the same request ( the top - 5 candidates ranked by their predictive probabilities ) . This RQ compares the non - determinism degree of code candidates for the two request configurations men - tioned above . Table 6 shows the results for dataset CodeContests ( full results on our homepage [ 1 ] . For ease of presentation , we use R1 to refer to one - time requests , and R2 to refer to multiple requests . Intuitively , the 5 top - 1 candidates from 5 different requests ( R2 ) should have more similarity than the top - 5 code candidates from one single request ( R1 ) , because the former is a collection of the best responses for each request and the latter is only the top - 5 best . Nevertheless , our observation indicates that the top - 5 code candi - dates from one single request ( R1 ) have very similar similarities to the 5 top - 1 candidates from multiple requests ( R2 ) . Table 6 : RQ3 : Similarity for different request ways . Request Test Pass Rate Way Mean Mean Mean Max Ratio of value variance max diff diff worst cases R1 0 . 13 0 . 03 0 . 22 1 . 00 4 . 24 % R2 0 . 13 0 . 03 0 . 27 1 . 00 7 . 88 % Request OER Way Mean value Worst value Ratio of worst cases R1 0 . 12 0 . 00 75 . 15 % R2 0 . 13 0 . 00 72 . 73 % Request OER ( no ex . ) Way Mean value Worst value Ratio of worst cases R1 0 . 03 0 . 00 86 . 06 % R2 0 . 05 0 . 00 81 . 82 % Request LCS LED Way Mean value Worst value Mean value Worst value R1 0 . 12 0 . 08 117 . 49 542 . 25 R2 0 . 12 0 . 07 119 . 75 165 . 28 Request United _ Diff Tree _ Diff Way Mean value Worst value Mean value Worst value R1 0 . 36 0 . 27 0 . 47 0 . 36 R2 0 . 34 0 . 25 0 . 45 0 . 34 , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang Answer to RQ3 : The top - 5 code candidates from one sin - gle request have very similar non - determinism with the 5 top - 1 candidates from different requests for ChatGPT . 4 . 4 RQ4 : Correlations between Coding Tasks and Non - determinism Degree Our previous experiments demonstrate that there are many non - determinisms in ChatGPT in code generation . This RQ investigates what affects such non - determinism by checking the correlation between characteristics of coding tasks and similarity metric values . We use the CodeContests dataset for this RQ because it is the only dataset with various features for each coding task , including difficulty , time limit , and CF rating . The difficulty of a problem is a qualitative measure that indicates the problem’s level of complexity and the programming knowledge and skills required to solve it . The time out indicates the program’s maximum running time limitation . The CF rating of a problem is a quantitative measure that represents the problem’s relative difficulty level compared to other problems on the Codeforces platform . In addition , we also consider description length ( i . e . , number of characters ) for each coding task . Note that in this section , we only focus on correlation analysis , and we do not aim to obtain any causal conclusions . Figure 6 shows the results . For the four coding task features ( description length , difficulty , time limit , and CF rating ) , We observe that description length has a negative correlation with the mean test pass rate . This means that problems with longer descriptions tend to have worse test pass rates . We suspect that this is because a longer description may reduce ChatGPT’s understanding of the coding requirements [ 35 ] . Description length also has a negative correlation with the variance of test pass rate , and max diff of test pass rate . This is because with longer descriptions different code candidates tend to be uniformally worse in their pass rates . Nevertheless , the description length has a negative correlation with LCS and United _ Diff , which means that problems with longer descriptions tend to yield more inconsistent code candidates in syntax and structure . The difficulty level has a positive correlation with the test pass rate variance , which means that the problem with a higher difficulty level has a high variance in the test pass rate . CF rating has the only significant negative correlation with the test pass rate mean , which implies that code problems with a higher CF rating may have a lower test pass rate on average . We also observe that the features clustered in the same type of similarity measurements demonstrate high correlations with p - value < 0 . 05 , such as test pass rate mean , test pass rate mean variance , and test pass rate mean max diff . This also indicates the reliability of the different measurements we have chosen for each type of similarity . Interestingly , we do not observe any correlation between semantic similarity and syntactic similarity , although the correlation between syntactic similarity and structural similarity is strong . T P R m e a n v a l u e T P R m e a n v a r i a n c e T P R m e a n m a x d i ff O E R m e a n O E R ( n o e x . ) m e a n L C S m e a n L C S w o r s t L E D m e a n L E D w o r s t U n i t e d _ D i ff m e a n U n i t e d _ D i ff w o r s t T r ee _ D i ff m e a n T r ee _ D i ff w o r s t d e s c r i p t i o n l e n g t h d i ff i c u l t y t i m e _ li m i t c f _ r a t i n g TPR mean value TPR mean variance TPR mean max diff OER mean OER ( no ex . ) mean LCS mean LCS worst LED mean LED worst United _ Diff mean United _ Diff worst Tree _ Diff mean Tree _ Diff worst description length difficulty time _ limit cf _ rating 1 . 0 0 . 67 0 . 75 - 0 . 18 - - - - 0 . 160 . 23 - 0 . 25 0 . 16 - 0 . 29 - - - 0 . 19 0 . 67 1 . 0 0 . 94 - - - - - - 0 . 19 - 0 . 19 - - 0 . 210 . 18 - - 0 . 75 0 . 94 1 . 0 - - - - - - 0 . 22 - 0 . 23 0 . 16 - 0 . 21 - - - - - - 1 . 0 0 . 55 - - - 0 . 21 - 0 . 23 - - - - - - - - 0 . 18 - - 0 . 55 1 . 0 - 0 . 17 - 0 . 22 - 0 . 22 - - - - - - - - - - - - - 1 . 0 0 . 84 - 0 . 42 - 0 . 310 . 64 0 . 68 0 . 56 0 . 6 - 0 . 22 - - - - - - - 0 . 17 0 . 84 1 . 0 - 0 . 36 - 0 . 3 0 . 69 0 . 73 0 . 64 0 . 67 - 0 . 22 - - - - - - - 0 . 21 - 0 . 22 - 0 . 42 - 0 . 36 1 . 0 0 . 9 - 0 . 2 - 0 . 26 - - 0 . 21 - - - - - 0 . 16 - - - 0 . 23 - 0 . 22 - 0 . 31 - 0 . 3 0 . 9 1 . 0 - - 0 . 16 - - - - - - 0 . 23 0 . 19 0 . 22 - - 0 . 64 0 . 69 - 0 . 2 - 1 . 0 0 . 92 0 . 94 0 . 87 - 0 . 17 - - - - - - - - 0 . 68 0 . 73 - 0 . 26 - 0 . 160 . 92 1 . 0 0 . 87 0 . 94 - - - - 0 . 25 0 . 19 0 . 23 - - 0 . 56 0 . 64 - - 0 . 94 0 . 87 1 . 0 0 . 91 - - - - 0 . 16 - 0 . 16 - - 0 . 6 0 . 67 - 0 . 21 - 0 . 87 0 . 94 0 . 91 1 . 0 - - - - - 0 . 29 - 0 . 21 - 0 . 21 - - - 0 . 22 - 0 . 22 - - - 0 . 17 - - - 1 . 0 - - - - 0 . 18 - - - - - - - - - - - - 1 . 0 0 . 28 0 . 64 - - - - - - - - - - - - - - 0 . 28 1 . 0 0 . 44 - 0 . 19 - - - - - - - - - - - - - 0 . 64 0 . 44 1 . 0 0 . 4 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Figure 6 : RQ4 : Correlations between coding tasks and non - determinism . Only significant correlations will be displayed on the heatmap , while the insignificant correlations ( i . e . p - value > 0 . 05 ) are masked by ’ - ’ . Answer to RQ4 : Description length of a coding task has a negative correlation with LCS , United _ Diff , and test pass rate . This means that a longer description of a coding task tends to suffer from more non - determinism in the gen - erated code in terms of code syntax and structure . The generated code also tends to be more buggy . 4 . 5 RQ5 : GPT - 4 vs . GPT - 3 . 5 GPT - 4 is believed to be “more reliable , creative , and able to han - dle much more nuanced instructions than GPT - 3 . 5” according to OpenAI [ 41 ] . This research question compares GPT - 3 . 5 and GPT - 4 in the non - determinism degree of code generation . To answer this research question , we keep the default setting and use all the mea - surements listed in RQ1 . We report the results only on the most widely used HumanEval dataset ( with temperature = 1 ) due to the rate limit issue [ 2 ] and the high price [ 3 ] of GPT - 4 . From Table 7 , we observe that the non - determinism issue of GPT - 4 is also severe , and is overall slightly worse than GPT - 3 . 5 in addition to the OER similarity . This is because GPT - 4 is known to be more creative than GPT - 3 . 5 , while more creativity often means more non - determinism [ 9 ] . For OER , GPT - 4 has a larger similarity than GPT - 3 . 5 . We suspect that this is because GPT - 4 produces more LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation , , cases with a test pass rate of 1 . 0 , thereby introducing more identical test outputs . This observation does not apply to the similarity of test pass rate , because the code candidates from GPT - 3 . 5 can be equally incorrect . For example , for a certain code task in our experiments , GPT - 3 . 5’s test pass rate is [ 0 , 0 , 0 , 0 . 8 , 0 . 8 ] , while GPT - 4’s test pass rate is [ 1 , 1 , 1 , 1 , 0 ] . GPT - 4’s OER , and OER ( no ex . ) are obviously higher than GPT - 3 . 5 , but the test pass rate variance and max diff are slightly larger . Table 7 : RQ5 : Non - determinism of GPT - 4 v . s . GPT - 3 . 5 . Model Test Pass Rate Mean Mean Mean Max Ratio of value variance max diff diff worst cases GPT - 4 0 . 73 0 . 11 0 . 60 1 . 00 53 . 66 % GPT - 3 . 5 0 . 55 0 . 10 0 . 36 1 . 00 48 . 17 % Model OER Mean value Worst value Ratio of worst cases GPT - 4 0 . 37 0 . 00 59 . 76 % GPT - 3 . 5 0 . 30 0 . 00 65 . 85 % Model OER ( no ex . ) Mean value Worst value Ratio of worst cases GPT - 4 0 . 35 0 . 00 60 . 98 % GPT - 3 . 5 0 . 25 0 . 00 71 . 34 % Model LCS LED Mean value Worst value Mean value Worst value GPT - 4 0 . 22 0 . 11 39 . 30 57 . 00 GPT - 3 . 5 0 . 45 0 . 33 38 . 86 60 . 52 Model United _ Diff Tree _ Diff Mean value Worst value Mean value Worst value GPT - 4 0 . 52 0 . 31 0 . 65 0 . 45 GPT - 3 . 5 0 . 69 0 . 43 0 . 72 0 . 44 Answer to RQ5 : The non - determinism issue of GPT - 4 is also severe , and is overall slightly worse than GPT - 3 . 5 . 5 THREATS TO VALIDITY The threats to internal validity mainly lie in the implementation of our experiment and manual analysis . To reduce the first threat , we checked our code twice , once during the experiment stage , and once during the record analysis stage . To reduce the second threat , the two authors independently analyzed the experiment results and drew experimental conclusions separately . Once their analysis results are different , the third author discussed with them to determine the final result . The threats to external validity mainly lie in the datasets and measurement tools used in our study . To reduce the first threat , we use three diverse datasets that are widely used in code generation tasks . Additionally , the problems in our dataset are from different contests with different difficulties . For example , CodeContests is the most challenging dataset , while HumanEval is the easiest , in terms of the average difficulty of code problems . To reduce the second threat , we consider three types of similarities and choose at least two measurements for each type of similarity . 6 RELATED WORK 6 . 1 Code Generation Code generation generates programs that need to satisfy all the constraints defined by the underlying task . Usually , the constraints are represented in various forms , e . g . input / output pairs , examples , problem descriptions , partial programs , and assertions . Relatively early work includes deductive synthesis approaches [ 17 , 38 ] and in - ductive synthesis approaches [ 7 , 46 , 47 , 49 ] . The deductive synthesis approach operated under the assumption that a comprehensive and precise formal specification of the user’s desired intention would be provided . However , in many instances , this turned out to be just as intricate and challenging as creating the actual program . While the inductive synthesis approach was based on inductive specifications such as input / output pairs and examples etc , such as works on Lisp programs [ 7 , 46 , 49 ] , Pygmalion [ 47 ] and more recently FlashFill [ 18 ] . More information could be found in a survey [ 19 ] , which covers notable work on the development of program synthesis approaches . In recent years , more and more researchers apply neural net - works in code generation . Yin and Neubig [ 58 ] combined the gram - mar rules with the decoder and propose a syntax - driven neural architecture to improve code generation performance . Instead of RNN , Sun et al . [ 50 ] a grammar - based structural CNN to capture the long dependency in code . Bolin et al . [ 54 ] propose a dual learning framework that jointly trains the code generation model and code summarization model together to achieve better performance in both tasks . Xu et al . [ 56 ] present a user study in - IDE code genera - tion , demonstrating challenges such as time efficiency , correctness , and code quality , as well as the willingness to use code generation tools from developers . 6 . 2 Language Model for Code generation The triumph of transformers in natural language modeling [ 8 ] has stimulated considerable interest among researchers in applying transformer models for code generation . Existing research on code generation models can be classified into three categories : sequence - based techniques , tree - based methods , and pre - trained models . Sequence - based techniques take code as a sequence of tokens and employ language models to produce source code one token at a time based on input descriptions . Ling et al . [ 32 ] propose a generative model for code generation along with a character level softmax and multi - pointer network to address the problem of gen - erating code from a mixed language and structured specification , and receiving success in trading card games ( Magic the Gathering and Hearthstone ) . Hashimoto et al . [ 20 ] train a retrieval model with a noisy encoder - decoder to enable similar code retrieving , and then use the similar code as an additional input to improve the performance of the generator . Tree - based methods generate a parse tree of the code , e . g . Abstract Syntax Tree ( AST ) , based on the input description , and then convert the parse tree into the corresponding code . Dong et al . [ 13 ] encode natural language utterances into vectors and generate their corresponding logical forms as trees using the LSTM model . Yin et al . [ 59 ] propose a semantic parser ‘Tranx’ , which generates the tree - construction action sequence with a transition - based neural model , and constructs the AST from the action sequence . , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang Pre - trained models are obtained from training on massive data of source code , which could be later fine - tuned on certain datasets for code generation purposes . Encoder pre - trained models , such as CodeBERT [ 15 ] , usually are trained with two objectives , i . e . , Masked Language Modeling and Replaced Token Detection . During the fine - tuning phase , the input should be fed in the same way as the pre - training phrase , so that semantic relevance could be measured . Decoder pre - trained models are designed to predict the next token based on a given input context . GPT - series [ 44 ] are typical Decoder pre - trained models , and based on GPT , there are many efforts on code generation . Based on GPT - 2 , Lu et al . [ 37 ] provide CodeGPT for code completion and text - to - code generation . After GPT - 3 was de - veloped , CodeX 12 and GitHub Compilot 13 was created and released their beta version for trial by academia and industry . Due to neither Codex nor GitHub Copilot being open - sourced , there are several attempts to reproduce their performance , like PYCODEGPT - CERT [ 60 ] , CodeParrot 14 , and GPT - CC 15 . Encoder - decoder pre - trained models are composed of an encoder and a decoder . AlphaCode [ 29 ] , which is pre - trained through GitHub repositories with 715 . 1 GB of code , uses an encoder - decoder transformer architecture . It achieves on average a ranking in the top 54 % in competitions with more than 5 , 000 participants in simulated evaluations . ChatGPT , a language model developed by the team of OpenAI , has the potential to play a role in code generation . As it is widely known , ChatGPT offers a chat window to enable interaction in a conversational way . In addition to its powerful capabilities for natural language processing tasks , ChatGPT inherits the code gen - eration capabilities from Codex and can perform even better , so the OpenAI team has announced the deprecation of Codex series mod - els in its official documents . There are several research works that mentioned its ability in code - related areas , including mathematical capability [ 16 ] , bug - solving capability [ 51 ] , and software testing [ 24 ] . ChatGPT’s ‘Regenerate response’ function demonstrates the diversity of its output , but at the same time , it also raises concerns about the consistency of its output given the same input . Currently , people are amazed by its superficial performance in terms of code generation , however , there is still no research work focused on the threat of non - determinism . Therefore , we think it is necessary to make a comprehensive evaluation of ChatGPT’s ability in code generation . More detailed information could be found on its official website’s blog 16 . 6 . 3 Non - determinism Handling in the Literature The non - determinism issue has been studied well in traditional Deep Learing - related research . Pham et al . [ 42 ] measure the influ - ence of nondeterminism - introducing ( NI ) - factors in Deep Learning , and study the awareness of this variance among researchers and practitioners . However , the common practice of handling the threat of non - determinism in LLM - based code generation remains unclear . To understand how well LLM - based code generation papers han - dle the threat of non - determinism , we collect research articles from 12 https : / / openai . com / blog / openai - codex 13 https : / / github . com / features / copilot 14 https : / / huggingface . co / codeparrot / codeparrot 15 https : / / github . com / CodedotAl / gpt - code - clippy 16 https : / / chat . openai . com / chat Google Scholar with the query ‘code generation’ AND ‘Large Lan - guage Model’ in the past 2 years ( from January 2022 to July 2023 ) . There are 107 papers obtained from Google Scholar according to their relevance rankings . In this survey , we mainly focus on articles with experiments and exclude those with posters and visions only , which yields a set of 76 papers . After an in - depth reading of the experimental design and discussion in each paper , we find that only 35 . 5 % ( 27 / 76 ) out of the 76 papers mention non - determinism or related terms ( e . g . , stability , randomness , and variance ) in their pa - pers . Among them , 21 . 1 % ( 16 / 76 ) papers consider non - determinism in their experimental evaluation , including fixed random seeds , multiple runs of experiments with different fixed random seeds , and report results with error bars or standard deviation . In addition , 14 . 5 % ( 11 / 76 ) of the papers do not consider non - determinism in their experiments , but discuss the threat of non - determinism in their paper . 7 IMPLICATIONS FOR SOFTWARE DEVELOPERS AND RESEARCHERS For developers , it is essential to recognize the limitations of Chat - GPT and the potential risks of using generated code in production . If developers prefer a more stable code , they can use a smaller temperature but should keep in mind that even the smallest tem - perature ( i . e . , temperature = 0 ) could not guarantee the determinism . Moreover , our observation on the correlation between the length of prompts and code correctness / non - determinism suggests the importance of prompt engineering . Developers should thoroughly test the generated code before deploying it , and even consider in - corporating more robust testing and validation processes to ensure the determinism and reliability of the generated code . For researchers , the variance of the generated code raises ques - tions about the quality and validity of the results obtained from assessing LLMs in code generation . If the code generated from ChatGPT is unstable , it can lead to non - reproducible results and unreliable conclusions . Therefore , researchers should carefully con - sider the limitations of ChatGPT when designing experiments and interpreting results . To reduce the randomness caused by the non - determinism issue , researchers can report the average results , vari - ance , or distribution from multiple requests . Also , it is important to use different datasets , since our study finds that both the correct - ness and non - determinism of the generated code vary significantly from dataset to dataset . Our paper points out the issue of non - determinism in ChatGPT in code generation . However , it is worth noting that non - determinism also has benefits for the creativity of LLMs . Achieving determinism in LLMs may sometimes come at the cost of reduced creativity or diversity in the generated text . Striking the right balance be - tween determinism and creativity is essential for optimizing the performance of LLMs for various applications . 8 CONCLUSION This work studies the non - determinism threat of code generation with ChatGPT . We perform experiments on three widely - studied code generation benchmarks and find that the correctness , test outputs , as well as syntax and structure of code candidates gen - erated from the same instruction , vary significantly in different LLM is Like a Box of Chocolates : the Non - determinism of ChatGPT in Code Generation , , requests . The ratio of code generation tasks with zero identical test output among code candidates is 72 . 73 % , 60 . 40 % , and 65 . 85 % for CodeContests , APPS , and HumanEval , respectively . We also find that GPT - 4 has slightly worse determinism than GPT - 3 . 5 with the default configuration . We hope that this paper could raise aware - ness of the threat of non - determinism in future code generation tasks when using large language models . REFERENCES [ 1 ] [ n . d . ] . . https : / / github . com / CodeHero0 / Nondeterminism - of - ChatGPT - in - Code - Generation [ 2 ] [ n . d . ] . . https : / / platform . openai . com / docs / guides / rate - limits / gpt - 4 - rate - limits [ 3 ] [ n . d . ] . . https : / / openai . com / pricing # language - models [ 4 ] Jacob Austin , Augustus Odena , Maxwell Nye , Maarten Bosma , Henryk Michalewski , David Dohan , Ellen Jiang , Carrie Cai , Michael Terry , Quoc Le , et al . 2021 . Program synthesis with large language models . arXiv preprint arXiv : 2108 . 07732 ( 2021 ) . [ 5 ] Y Bang , S Cahyawijaya , N Lee , W Dai , D Su , B Wilie , H Lovenia , Z Ji , T Yu , W Chung , et al . 2023 . A multitask , multilingual , multimodal evaluation of ChatGPT on reasoning , hallucination , and interactivity . arXiv . [ 6 ] Bhavya Bhavya , Jinjun Xiong , and Chengxiang Zhai . 2022 . Analogy generation by prompting large language models : A case study of instructgpt . arXiv preprint arXiv : 2210 . 04186 ( 2022 ) . [ 7 ] Alan W Biermann . 1978 . The inference of regular LISP programs from examples . IEEE transactions on Systems , Man , and Cybernetics 8 , 8 ( 1978 ) , 585 – 600 . [ 8 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . Language models are few - shot learners . Advances in neural information processing systems 33 ( 2020 ) , 1877 – 1901 . [ 9 ] Sébastien Bubeck , Varun Chandrasekaran , Ronen Eldan , Johannes Gehrke , Eric Horvitz , EceKamar , PeterLee , YinTatLee , YuanzhiLi , ScottLundberg , etal . 2023 . Sparks of artificial general intelligence : Early experiments with gpt - 4 . arXiv preprint arXiv : 2303 . 12712 ( 2023 ) . [ 10 ] Subhashis Chatterjee , Deepjyoti Saha , Akhilesh Sharma , and Yogesh Verma . 2022 . Reliability and optimal release time analysis for multi up - gradation software with imperfect debugging and varied testing coverage under the effect of random field environments . Annals of Operations Research ( 2022 ) , 1 – 21 . [ 11 ] Mark Chen , Jerry Tworek , Heewoo Jun , Qiming Yuan , Henrique Ponde de Oliveira Pinto , Jared Kaplan , Harri Edwards , Yuri Burda , Nicholas Joseph , Greg Brockman , et al . 2021 . Evaluating large language models trained on code . ( 2021 ) . arXiv preprint arXiv : 2107 . 03374 ( 2021 ) . [ 12 ] YinlinDeng , ChunqiuStevenXia , ChenyuanYang , ShizhuoDylanZhang , Shujing Yang , and Lingming Zhang . 2023 . Large language models are edge - case fuzzers : Testingdeeplearninglibrariesviafuzzgpt . arXivpreprintarXiv : 2304 . 02014 ( 2023 ) . [ 13 ] LiDongandMirellaLapata . 2016 . Languagetologicalformwithneuralattention . arXiv preprint arXiv : 1601 . 01280 ( 2016 ) . [ 14 ] Yunhe Feng , Sreecharan Vanam , Manasa Cherukupally , Weijian Zheng , Meikang Qiu , andHaihuaChen . 2023 . InvestigatingCodeGenerationPerformanceofChat - GPT with Crowdsourcing Social Data . In Proceedings of the 47th IEEE Computer Software and Applications Conference . 1 – 10 . [ 15 ] Zhangyin Feng , Daya Guo , Duyu Tang , Nan Duan , Xiaocheng Feng , Ming Gong , Linjun Shou , Bing Qin , Ting Liu , Daxin Jiang , et al . 2020 . Codebert : A pre - trained model for programming and natural languages . arXiv preprint arXiv : 2002 . 08155 ( 2020 ) . [ 16 ] Simon Frieder , Luca Pinchetti , Ryan - Rhys Griffiths , Tommaso Salvatori , Thomas Lukasiewicz , PhilippChristianPetersen , AlexisChevalier , andJuliusBerner . 2023 . Mathematical capabilities of chatgpt . arXiv preprint arXiv : 2301 . 13867 ( 2023 ) . [ 17 ] Cordell Green . 1981 . Application of theorem proving to problem solving . In Readings in Artificial Intelligence . Elsevier , 202 – 222 . [ 18 ] Sumit Gulwani . 2011 . Automating string processing in spreadsheets using input - output examples . ACM Sigplan Notices 46 , 1 ( 2011 ) , 317 – 330 . [ 19 ] SumitGulwani , OleksandrPolozov , RishabhSingh , etal . 2017 . Programsynthesis . Foundations and Trends® in Programming Languages 4 , 1 - 2 ( 2017 ) , 1 – 119 . [ 20 ] Tatsunori B Hashimoto , Kelvin Guu , Yonatan Oren , and Percy S Liang . 2018 . A retrieve - and - edit framework for predicting structured outputs . Advances in Neural Information Processing Systems 31 ( 2018 ) . [ 21 ] Hossein Hassani and Emmanuel Sirmal Silva . 2023 . The role of ChatGPT in data science : how ai - assisted conversational interfaces are revolutionizing the field . Big data and cognitive computing 7 , 2 ( 2023 ) , 62 . [ 22 ] Dan Hendrycks , Steven Basart , Saurav Kadavath , Mantas Mazeika , Akul Arora , EthanGuo , CollinBurns , SamirPuranik , HoraceHe , DawnSong , etal . 2021 . Mea - suring coding challenge competence with apps . arXiv preprint arXiv : 2105 . 09938 ( 2021 ) . [ 23 ] Jeevana Priya Inala , Chenglong Wang , Mei Yang , Andres Codas , Mark Encar - nación , Shuvendu Lahiri , Madanlal Musuvathi , and Jianfeng Gao . 2022 . Fault - aware neural code rankers . Advances in Neural Information Processing Systems 35 ( 2022 ) , 13419 – 13432 . [ 24 ] Sajed Jalil , Suzzana Rafi , Thomas D LaToza , Kevin Moran , and Wing Lam . 2023 . Chatgpt and software testing education : Promises & perils . In 2023 IEEE Inter - national Conference on Software Testing , Verification and Validation Workshops ( ICSTW ) . IEEE , 4130 – 4137 . [ 25 ] Andrej Kiviriga . 2023 . Efficient Model Checking : The Power of Randomness . ( 2023 ) . [ 26 ] Kalpesh Krishna , Yapei Chang , John Wieting , and Mohit Iyyer . 2022 . Rankgen : Improving text generation with large ranking models . arXiv preprint arXiv : 2205 . 09726 ( 2022 ) . [ 27 ] Sumith Kulal , Panupong Pasupat , Kartik Chandra , Mina Lee , Oded Padon , Alex Aiken , andPercySLiang . 2019 . Spoc : Search - basedpseudocodetocode . Advances in Neural Information Processing Systems 32 ( 2019 ) . [ 28 ] Mina Lee , Percy Liang , and Qian Yang . 2022 . Coauthor : Designing a human - ai collaborative writing dataset for exploring language model capabilities . In Proceedings of the 2022 CHI conference on human factors in computing systems . 1 – 19 . [ 29 ] YujiaLi , DavidChoi , JunyoungChung , NateKushman , JulianSchrittwieser , Rémi Leblond , Tom Eccles , James Keeling , Felix Gimeno , Agustin Dal Lago , et al . 2022 . Competition - level code generation with alphacode . Science 378 , 6624 ( 2022 ) , 1092 – 1097 . [ 30 ] Zongjie Li , Chaozheng Wang , Zhibo Liu , Haoxuan Wang , Dong Chen , Shuai Wang , and Cuiyun Gao . 2023 . Cctest : Testing and repairing code completion systems . In 2023 IEEE / ACM 45th International Conference on Software Engineering ( ICSE ) . IEEE , 1238 – 1250 . [ 31 ] Jacky Liang , Wenlong Huang , Fei Xia , Peng Xu , Karol Hausman , Brian Ichter , Pete Florence , and Andy Zeng . 2023 . Code as policies : Language model programs for embodied control . In 2023 IEEE International Conference on Robotics and Automation ( ICRA ) . IEEE , 9493 – 9500 . [ 32 ] Wang Ling , Edward Grefenstette , Karl Moritz Hermann , Tomáš Kočisk ` y , Andrew Senior , Fumin Wang , and Phil Blunsom . 2016 . Latent predictor networks for code generation . arXiv preprint arXiv : 1603 . 06744 ( 2016 ) . [ 33 ] Jiawei Liu , Chunqiu Steven Xia , Yuyao Wang , and Lingming Zhang . 2023 . Is your code generated by chatgpt really correct ? rigorous evaluation of large language models for code generation . arXiv preprint arXiv : 2305 . 01210 ( 2023 ) . [ 34 ] Kaibo Liu , Yudong Han , Jie M Zhang , Zhenpeng Chen , Federica Sarro , Mark Harman , Gang Huang , and Yun Ma . 2023 . Who Judges the Judge : An Empirical StudyonOnlineJudgeTests . In Proceedingsofthe32ndACMSIGSOFTInternational Symposium on Software Testing and Analysis . [ 35 ] Nelson F Liu , Kevin Lin , John Hewitt , Ashwin Paranjape , Michele Bevilacqua , Fabio Petroni , and Percy Liang . 2023 . Lost in the middle : How language models use long contexts . arXiv preprint arXiv : 2307 . 03172 ( 2023 ) . [ 36 ] Yiheng Liu , Tianle Han , Siyuan Ma , Jiayue Zhang , Yuanyuan Yang , Jiaming Tian , Hao He , Antong Li , Mengshen He , Zhengliang Liu , et al . 2023 . Summary of chatgpt / gpt - 4 research and perspective towards the future of large language models . arXiv preprint arXiv : 2304 . 01852 ( 2023 ) . [ 37 ] Shuai Lu , Daya Guo , Shuo Ren , Junjie Huang , Alexey Svyatkovskiy , Ambro - sio Blanco , Colin Clement , Dawn Drain , Daxin Jiang , Duyu Tang , et al . 2021 . Codexglue : A machine learning benchmark dataset for code understanding and generation . arXiv preprint arXiv : 2102 . 04664 ( 2021 ) . [ 38 ] Zohar Manna and Richard J Waldinger . 1971 . Toward automatic program synthe - sis . Commun . ACM 14 , 3 ( 1971 ) , 151 – 165 . [ 39 ] Eric Mitchell , Yoonho Lee , Alexander Khazatsky , Christopher D Manning , and Chelsea Finn . 2023 . Detectgpt : Zero - shot machine - generated text detection using probability curvature . arXiv preprint arXiv : 2301 . 11305 ( 2023 ) . [ 40 ] Prabhat Nagarajan , Garrett Warnell , and Peter Stone . 2018 . Deterministic imple - mentations for reproducibility in deep reinforcement learning . arXiv preprint arXiv : 1809 . 05676 ( 2018 ) . [ 41 ] OpenAI . 2023 . GPT - 4 Technical Report . arXiv : 2303 . 08774 [ cs . CL ] [ 42 ] Hung Viet Pham , Shangshu Qian , Jiannan Wang , Thibaud Lutellier , Jonathan Rosenthal , Lin Tan , Yaoliang Yu , and Nachiappan Nagappan . 2020 . Problems and opportunities in training deep learning software systems : An analysis of vari - ance . In Proceedings of the 35th IEEE / ACM international conference on automated software engineering . 771 – 783 . [ 43 ] GabrielPoesia , OleksandrPolozov , VuLe , AshishTiwari , GustavoSoares , Christo - pher Meek , and Sumit Gulwani . 2022 . Synchromesh : Reliable code generation from pre - trained language models . arXiv preprint arXiv : 2201 . 11227 ( 2022 ) . [ 44 ] Alec Radford , Karthik Narasimhan , Tim Salimans , Ilya Sutskever , et al . 2018 . Improving language understanding by generative pre - training . ( 2018 ) . [ 45 ] Baptiste Roziere , Jie Zhang , Francois Charton , Mark Harman , Gabriel Synnaeve , and Guillaume Lample . 2021 . Leveraging Automated Unit Tests for Unsupervised Code Translation . In International Conference on Learning Representations . [ 46 ] David E Shaw , William R Swartout , and C Cordell Green . 1975 . Inferring LISP Programs From Examples . . In IJCAI , Vol . 75 . 260 – 267 . , , Shuyin Ouyang , Jie M . Zhang , Mark Harman , and Meng Wang [ 47 ] David Canfield Smith . 1975 . Pygmalion : a creative programming environment . Stanford University . [ 48 ] Ioana Baldini Soares , Dennis Wei , Karthikeyan Natesan Ramamurthy , Moninder Singh , andMikhailYurochkin . 2022 . Yourfairnessmayvary : pretrainedlanguage model fairness in toxic text classification . In Annual Meeting of the Association for Computational Linguistics . [ 49 ] Phillip D Summers . 1977 . A methodology for LISP program construction from examples . Journal of the ACM ( JACM ) 24 , 1 ( 1977 ) , 161 – 175 . [ 50 ] Zeyu Sun , Qihao Zhu , Lili Mou , Yingfei Xiong , Ge Li , and Lu Zhang . 2019 . A grammar - based structural cnn decoder for code generation . In Proceedings of the AAAI conference on artificial intelligence , Vol . 33 . 7055 – 7062 . [ 51 ] Nigar M Shafiq Surameery and Mohammed Y Shakor . 2023 . Use chat gpt to solve programming bugs . International Journal of Information Technology & Computer Engineering ( IJITC ) ISSN : 2455 - 5290 3 , 01 ( 2023 ) , 17 – 22 . [ 52 ] Alexey Svyatkovskiy , Shao Kun Deng , Shengyu Fu , and Neel Sundaresan . 2020 . Intellicodecompose : Codegenerationusingtransformer . In Proceedingsofthe28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 1433 – 1443 . [ 53 ] Priyan Vaithilingam , Tianyi Zhang , and Elena L Glassman . 2022 . Expectation vs . experience : Evaluating the usability of code generation tools powered by large language models . In Chi conference on human factors in computing systems extended abstracts . 1 – 7 . [ 54 ] Bolin Wei , Ge Li , Xin Xia , Zhiyi Fu , and Zhi Jin . 2019 . Code generation as a dual task of code summarization . Advances in neural information processing systems 32 ( 2019 ) . [ 55 ] Xiongfei Wu , Liangyu Qin , Bing Yu , Xiaofei Xie , Lei Ma , Yinxing Xue , Yang Liu , and Jianjun Zhao . 2020 . How are deep learning models similar ? an empirical study on clone analysis of deep learning software . In Proceedings of the 28th International Conference on Program Comprehension . 172 – 183 . [ 56 ] Frank F Xu , Bogdan Vasilescu , and Graham Neubig . 2022 . In - ide code generation from natural language : Promise and challenges . ACM Transactions on Software Engineering and Methodology ( TOSEM ) 31 , 2 ( 2022 ) , 1 – 47 . [ 57 ] Burak Yetiştiren , Işık Özsoy , Miray Ayerdem , and Eray Tüzün . 2023 . Evalu - ating the Code Quality of AI - Assisted Code Generation Tools : An Empirical Study on GitHub Copilot , Amazon CodeWhisperer , and ChatGPT . arXiv preprint arXiv : 2304 . 10778 ( 2023 ) . [ 58 ] Pengcheng Yin and Graham Neubig . 2017 . A syntactic neural model for general - purpose code generation . arXiv preprint arXiv : 1704 . 01696 ( 2017 ) . [ 59 ] Pengcheng Yin and Graham Neubig . 2018 . Tranx : A transition - based neural abstract syntax parser for semantic parsing and code generation . arXiv preprint arXiv : 1810 . 02720 ( 2018 ) . [ 60 ] Daoguang Zan , Bei Chen , Dejian Yang , Zeqi Lin , Minsu Kim , Bei Guan , Yongji Wang , WeizhuChen , andJian - GuangLou . 2022 . CERT : ContinualPre - trainingon Sketches for Library - oriented Code Generation . arXiv preprint arXiv : 2206 . 06888 ( 2022 ) .