Journal of Experimental Psychology : Learning , Memory , and Cognition 1984 , Vol . 10 , No . 2 , 234 - 257 Copyright 1984 by the American Psychological Association , Inc . Induction of Category Distributions : A Framework for Classification Learning Lisbeth S . Fried and Keith J . Holyoak University of Michigan ( Ann Arbor ) We present a framework for classification learning that assumes that learners use presented instances ( whether labeled or unlabeled ) to infer the density functions of category exemplars over a feature space and that subsequent classification decisions employ a relative likelihood decision rule based on these inferred density functions . A specific model based on this general framework , the category density model } was proposed to account for the induction of normally distributed categories either with or without error correction or provision of labeled instances ' . The model was implemented as a computer simulation . Results of five experiments indicated that people could learn category distributions not only without error correction , but without knowledge of the number of categories or even that there were categories to be learned . These and other findings dictated a more general learning model that integrated distributional representations based on both parametric descriptions and stored instances . In this article we present a new model of category learning and classification based on the acquisition and use of distributional knowledge . This category density model , de - rived from work by Fried ( 1979 ) , makes the central assumption that the goal of the category learner is to develop a schematic description Experiment IA was reported at the meeting of the Psy - chonomic Society in San Antonio , Texas , November 1978 , and Experiment 3 was reported at the meeting of the Mathematical Psychology Society in Santa Barbara , Cal - ifornia , August 1981 . An earlier version of the present article appeared as Cognitive Science Technical Rep . No . 38 , University of Michigan , 1982 . This research was supported by National Science Foun - dation Grant BNS - 7904730 to both authors , and a Rack - ham Faculty Grant from the University of Michigan and National Institute of Mental Health Grant 1 - K02 - MH00342 - 03 to K . Holyoak . We thank the numerous colleagues and students who helped us clarify our ideas in seminars and discussions . Dorrit Billman , Mary Gick , David H . Krantz , Tracy Sher - man , Wilson P . Tanner , Jr . , and J . E . Keith Smith provided valuable comments on the earliest draft of this paper . Wil - liam Estes , Don Homa , Kyunghee Koh , Doug Medin , Edward Smith , and Tom Wallsten commented on sub - sequent drafts . Jack Abraham , Bill Barr , Holly Brewer , Ellen Junn , Roberta Mehoke , Shannon McDonnell , Allan Salmi , and Jan Stern assisted in testing subjects , and John Patterson provided programming assistance . Requests for reprints should be sent to Lisbeth S . Fried or Keith J . Holyoafc , University of Michigan , Human Per - formance Center , 330 Packard Road , Ann Arbor , Michigan 48104 . of the distributions of category exemplars over a feature space . Highly salient features will tend to be encoded initially , although the learner may actively search for less salient fea - tures that are more diagnostic of category membership . We assume that the schematic representation is a parametric encoding of the category distribution over the feature dimen - sions to which the learner is currently attend - ing . Suppose , for example , that a learner is shown a set of exemplars randomly sampled from a category population that is normally distributed over n independent feature di - mensions . The density function for such a cat - egory can be sufficiently described by a vector of 2n parameters—the mean and variance of the population along each feature dimension . The density model assumes that in this ex - ample the effective representation of the cat - egory distribution will correspond to this pa - rameter vector . This assumption implies that the presented instances will be treated as a sample that can be used to estimate the dis - tributional properties of an indefinitely large population of potential category exemplars . A parametric representation also implies that there exists a set of statistics for each fea - ture dimension that is sufficient to describe the learner ' s conception of the category dis - tribution . The types of category distributions that people can encode parametrically may be 234 CATEGORY INDUCTION 235 small , although this is an open empirical issue . In the present study we will focus on a specific version of the category density model that ac - counts for learning of multidimensional nor - mally distributed categories . Normal distri - butions may have particular ecological im - portance . Basic - level natural categories seem to consist of a dense central region of typical instances , surrounded by sparser regions of atypical instances ( Rosch , 1973 , 1978 ; Rosch & Mervis , 1975 ; Rosch , Mervis , Gray , John - son , & Boyes - Braem , 1976 ) . People may therefore expect new categories to be unimodal and to have roughly symmetrical density functions , which may be well approximated by multidimensional normal distributions . A second major assumption of the category density model is that classification decisions are based on relative likelihood . This decision rule is related to that of signal detectability theory ( Swets , Tanner , & Birdsall , 1961 ) , ex - cept that it is based on distributional infor - mation acquired during category learning rather than on information assumed known a priori . The subjective probability > £ , , , that the decision maker considers item X to be a member of category C , on trial t is assumed to be given by Bayes ' theorem ; that is , = P , ( C , \ X ) = P , ( x \ c m ) Pt ( c m ) ( 1 ) where p , ( X \ C , ) is the subjective conditional probability on trial t of item X given category ChPt ( Ct ) is the subjective prior probability of C ( as of trial t , and k is the number of alter - native categories . The model further assumes that the decision maker ' s probability of making response C , on trial t given item X is ( 2 ) k 2 m - 1 where ft is a constant for each category re - flecting factors such as an asymmetrical payoff matrix for different classification responses . Note that when the values of p , ( Q and ft are equal for all categories ( as will be assumed in all applications of Equations 1 and 2 in the present article because prior probabilities and payoffs were made equal and symmetric in all experiments ) , it then follows from Equation 2 that the relative frequencies of the alternative categories as responses to item X will be equal to the subjective relative likelihoods of the item X given the alternative categories . We will therefore refer to Equations 1 and 2 jointly as the relative likelihood decision rule . This rule will be used as a heuristic device for measuring distributional learning , A third assumption of the model , related to Bayesian learning theory ( Edwards , Lind - man , & Savage , 1963 ) , is that category learning is based on a cyclic process of parameter re - vision . We assume that people expect feature dimensions of perceptual categories to be nor - mally distributed , and that they enter a cat - egory learning task with ( perhaps very vague ) initial opinions about the central tendency and degree of variability of each category on its salient feature dimensions . People then use presented instances to revise these prior ex - pectations . The revised opinions generate ex - pectancies in terms of which the next obser - vation is evaluated . , The density model includes a mechanism by which normal distributions can be learned by revising parameter vectors in response to each successive instance in a set of training exemplars , with minimal reliance on memory for prior instances ( discussion follows ) . In a typical experiment on classification learning , subjects are told the category to which each training instance belongs . Under such condi - tions the parameter - revision process for learn - ing normal distributions is straightforward . On each trial the feature values of the current instance are used to update the dimension means and variances for the appropriate cat - egory , while the occurrence of the category label is used to update the index of the cate - gory ' s frequency . The new parameter values are then saved ; the current instance may be incidentally stored in memory , but it plays no further necessary role in learning or classifi - cation . In naturalistic learning situations , unlike standard experimental paradigms , external error feedback may be delayed , unreliable , or completely absent ( Bruner , Goodnow , & Aus - tin , 1956 , p . 68 ) . Furthermore , there : is ex - perimental evidence that people can sometimes learn to classify instances of probabilistic cat - egories without any error correction or pro - 236 LISBETH S . FRIED AND KEITH J . HOLYOAK vision of category labels for instances ( Ed - monds & Evans , 1966 ; Fried , 1979 ) , although learning is not always entirely successful under such conditions ( Evans & , Arnoult , 1967 ; see also Bersted , Brown , & Evans , 1969 ; Tracy & Evans , 1967 ) . The - possibility of learning with - out external feedback is also suggested by E . Gibson ' s theory of perceptual learning ( 1953 , 1969 ; Gibson & Gibson , 1955 ) . Under certain conditions , parameter revision can be used to learn category distributions even in the absence of error feedback , The task of learning dis - tributions without feedback can be modeled as a problem of decomposing the overall mix - ture density of the presented instances into its component densities . This decomposition can be accomplished by parameter - revision pro - cedures for mixtures of normal densities if the learner knows the number of categories present in the mixture ( Duda & Hart , 1973 ; Fried , 1979 ) . Category Density Model and Its Simulation A version of the category density model for normal distributions was implemented as a FORTRAN program . 1 The program estimates the distributional parameters for categories denned by independent , multidimensional normal distributions ( i . e . , a mean and variance for each dimension of each category , and a frequency parameter for each category ) . The program can learn these parameters with or without information about the category mem - bership of training exemplars , and was used to validate the qualitative predictions outlined below . The learning sequence for the FORTRAN program consists of randomly intermixed « - dimensional stimuli ( « ^ 5 ) drawn from k categories ( k < , 5 ) . Each category is denned by normal distributions of values over each of the dimensions . The dimensions are statis - tically independent . The category distributions are specified by providing the program with a mean and variance for each dimension of each category , and a relative frequency for each category . Each stimulus is thus represented by a vector of numbers , with each number rep - resenting a value on a dimension . The number of categories and dimensions is specified . Runs used to validate the qualitative predictions tested in the present paper ( described later ) used two 2 - dimensional categories . The learning process operates either with knowledge of the category membership of the training instances ( feedback condition ) or without such knowledge ( no - feedback condi - tion ) . In either case learning involves two stages : formation of initial parameter esti - mates , followed by iterative revision of them . In the feedback condition , the dimension value of the first instance of each category Q is used as the initial mean A / , , , for the rth category ' s Jih dimension . The initial value of the fre - quency N ] of this category is then 1 . The di - mension variances V u for category C , are ini - tialized when the second observation for that category is obtained . The initial variance es - timate is a weighted average of the sample variance of the two observations and an ar - bitrary large value that represents the vague - ness of the learner ' s prior opinion about the distribution of category / oirdimension j . In the no - feedback condition , initial pa - rameter estimates are formed after accumu - lating the first s instances , where s ( atfc ) is the size of a short - term memory buffer ( set at 6 in the runs reported later ) . The s observations are represented as points in an « - dimensional space , and a clustering algorithm uses the Eu - clidean distances between the points to divide the observations into k groups . The algorithm used is based on the centroid method ( Everitt , 1974 , pp ' . 12 - 14 ) . The distance between two groups is defined as the distanceibetween their centroids , where the centroid is the mean of the coordinate values for the items in a group . Initially each of the s instances is defined as a group with one member . The two closest groups are then merged and replaced by the coordinates of their centroid . This procedure is iterated until k groups remain . The initial frequency JV / of each category is then set equal to the number of instances in a corresponding cluster , and the initial dimensional mean M u for each category is set equal to the mean of the clustered instances on each dimension . The initial variance Vy of each category on each ' The simulation model was programmed by Kyunghee Koh , who also helped formulate the implemented pro - cedure for learning without feedback . CATEGORY INDUCTION 237 dimension is obtained by pooling an arbitrary large value ( as in the feedback condition ) with the variance of the cluster . ( This pooling pro - cedure has the incidental effect of ensuring that even a category with just one initial mem - ber will have a nonzero initial variance . ) After initial parameter estimates are formed , each successive observation is then used to revise the estimates . On trial t , the first step is to determine the probability * / > t that cat - egory Ci generated the observed instance X . For the feedback condition these probabilities are as follows : 1 , if X is labeled as a member of C , 0 , otherwise . ( 3 ) For the no - feedback condition ^ ( > , is calculated by inserting the parameter estimates as of trial t - 1 in a version of Equation 1 of the relative likelihood rule . If an item X is represented by a vector of values on n independent feature dimensions , x \ , x 2 , . . . x n , then Equation 1 can be restated as follows : , ( C . ) [ A , ( 4 ) 2 p , ( c m ) n andp , ( Xj \ C , ) can be determined by substituting the current estimates of MIJ and V iti in the equation for the normal distribution ^ that is , tjj - \ . ( 5 ) In calculating p t ( Ci ) , the program allows a bias toward an assumption that the k categories are of equal frequency . The bias is given as a weight Wf ( 0 £ . Wf < . 1 ) , which can be inter - preted as a measure of the learner ' s confidence after one observation that the categories are equally likely . The result is given by the fol - lowing : where T is the total number of observations , ( 7 ) fc T = 2 i = i In Equation 6 the observed relative frequency , Ntj - i / Tt - i , is . weightedby 7 ^ _ , ( 1 - w f ) . Hence if Wf + 0 or 1 , the impact of the observed relative frequency will increase with the num - ber of observations . After determining the values of * , j ( , the next step is to revise the parameters for all cate - gories , with the degree of revision for each category weighted in accord with ty itt . For the no - feedback condition ty , , may be fractional . For example , suppose X is twice as likely to have been generated by Q than by C 2 , given the current parameter estimates . Then the pa - rameters will be revised as if two thirds of an observation ( with the dimension values of the new item ) had accrued to Ci and one third of an observation had accrued to €2 . The re - vision procedures are based on standard equa - tions for revising running frequencies , means , and variances ( Raiffa & Schlaifer , 1961 ) , gen - eralized to accommodate fractional observa - tions . The revised N t is given by the following : N t > t = NU - ! + # , , „ and the revised M itJ is as follows : ( 8 ) ( 9 ) In updating K w , the program allows a bias toward an assumption that the variances for any given dimension are equal across all cat - egories . V ijtt is a weighted average of the es - timated variance of the individual category C , on dimension j , IV ^ , and of the variance pooled over all categories , PV j > t , with relative weights determined by a parameter w v , defined analogously to vty . The revision of V itj proceeds by first calculating the individual variance : j - i - M ut ) 2 - 1 ) , ( 10 ) 238 LISBETH S . FRIED AND KEITH J . HOLYOAK and the pooled variance : 2 ( N iit - PVj . t = T , - k ( 11 ) The revised variance is then given by the weighted average of 7F , , , , , and PV jtl , 7X1 - w v ( 12 ) The program can use its current parameter estimates to classify items in accord with the relative likelihood rule . The learning phase proceeds either until some criterion is reached ( e . g . , 10 correct classifications in a row ) , or a fixed number of observations have been pre - sented . A transfer phase is then simulated , in which the program - uses its final parameter estimates to classify transfer items drawn from very broad distributions around the multidi - mensional means of the categories presented during the learning phase . An other response can optionally be allowed , in which case the program assumes that all instances have a fixed likelihood ( a parameter that is specified ) of being generated by an other category . The pro - gram thus treats other as a category with a specified uniform distribution , so that the rel - ative likelihood that an item is drawn from the other category can be calculated using the relative likelihood rule . / Predictions of the Density Model The category density model as described in the simulation generates a variety of qualitative predictions . The major predictions tested in the present experiments can be divided into two groups : those that concern learning and those that concern transfer performance . Predictions concerning learning . The fol - lowing predictions about learning normally distributed categories can be derived from the density model . LI . When subjects know the number of cat - egories to be learned , and correctly assume that the distributions are normal , learning is possible even without error feedback or in - stances labeled with respect to category mem - bership . L2 . Labeled instances will facilitate con - vergence on accurate estimates of distribu - tional parameters , relative to a no - feedback condition . Feedback enables thelearner to use each observation to revise only the correct cat - egory distribution , rather than apportioning its value across all categories . L3 . Low - variability categories can be learned with fewer observations than required to learn high - variability categories with the same means . L4 . Learning will be facilitated if the learner knows the number of categories to be learned . Indeed , knowledge of number of categories is an essential prerequisite for the parameter - re - vision procedure described earlier . Predictions concerning transfer . The fol - lowing predictions involve transfer perfor - mance after learning has taken place . Tl . Classification performance will be in accord with the relative likelihood rule ( e . g . , the probability of classifying a novel instance as a member of a category will be directly proportional to its subjective likelihood of being generated by the category distribution ) . T2 . When a random or other alternative is available at transfer , exemplars far from the mean ( prototype ) of a learned category will more likely be classified as members of that category if the variability of the learned cat - egories is high . In contrast , such exemplars will more often be classified as other when the variability of the learned categories is low . This prediction follows from the fact that the like - lihood that a category will generate atypical exemplars is greater if the variarice of the cat - egory is relatively high . In situations in which other responses are classified as errors , the per - centage correct will be higher for groups trained on high - variability instances . T3 . The above advantage of learning high - variability rather than low - variability cate - gories in classifying exemplars far from the prototype will not be obtained in the absence of an other alternative at transfer . T4 . If subjects learn two equally probable categories of unequal variability , they will tend to classify more items into the high - variability category at transfer , including some items that are closer to the mean of the low - variability category but more likely to have been gener - ated by the high - variability one * CATEGORY INDUCTION 239 Sample of Performance by the Simulation Program Because the stimuli used in the experiments reported later were complex forms for which the psychological features encoded by subjects were not known , the simulation program can - not generate precise quantitative predictions for our experiments . However , the general model can be applied even if the psychological features are unknown and different subjects encode stimuli in terms of different features , as long as subjects ' feature sets can be ap - proximated by normal densities . The quali - tative predictions outlined earlier hold re - gardless of the specific nature or number of features used by subjects . As an illustration of some of our qualitative predictions , we will report the results of some sample runs of the program . These test runs used two 2 - dimen - sional categories , denned by bivariate normal distributions with means ( 3 , 6 ) and ( 6 , 3 ) , re - spectively , in arbitrary units . In the simulated low - variability condition the variances of both categories on each dimension were set equal to 1 ( so that d ' = 3 on each dimension ) , and in the high - variability condition the variances were set equal to 4 ( d 1 - 1 . 5 ) . The arbitrary value used in initiating the variance estimates was 20 . In the no - feedback condition param - eters were initialized after clustering the first six items . The values of viy and w a were set equal to 0 . 9 and 0 . 1 , respectively . Ten simu - lated subjects were used in each run . As a measure of rate of learning , the mean number of trials required to reach a criterion of 10 correct responses in a row was measured . A maximum of 300 learning trials were al - lowed . The mean number of trials to criterion was 32 for the low - variability , feedback con - dition ; 62 for the low - variability , no - feedback condition ; 74 for the high - variability , feedback condition ; and 146 for the high - variability , no - feedback condition . These results illustrate Predictions L2 ( advantage of labeled instances ) and L3 ( advantage of low - variability training ) . Other runs were used to simulate transfer performance after a fixed number of learning trials ( 100 ) . The first set of runs , presented in Table 1 ( top half ) , included an other alternative at transfer . The subjective likelihood that any item was an other was specified to be . 002 . In fact , all transfer items were drawn from broad distributions around the prototypes of the two categories presented during the learning phase . Table 1 ( top half ) features both the obtained mean percentage correct and mean percentage other responses as a function of the Euclidean distance of transfer items from their generating prototype ( as measured in the same arbitrary units ) . For all learning conditions the per - centage correct decreased and the percentage other increased with distance from the pro - totype , exemplifying Prediction Tl . The pre - dicted greater percentage correct and lesser percentage other responses at high distances for groups trained on high - rather than low - variability exemplars ( Prediction T2 ) was also apparent . Presented in the bottom half of Table 1 are runs in which no other alternative was allowed . Here the advantage of high - variability Table 1 Transfer Performance With and Without Availability of an Other Category Distance from prototype Training condition 1 , 2 3 45 With other category Low variability Feedback % correct % other No feedback % correct % other High variability Feedback % correct % other No feedback % correct % other . 99 . 93 . 72 . 31 . 03 . 01 . 04 . 15 . 45 . 72 . 97 . 84 . 70 . 46 . 19 . 01 . 04 . 12 . 29 . 54 . 88 . 82 . 71 . 61 , 49 . 01 . 06 . 08 . 13 . 25 . 82 . 76 . 68 . 58 . 49 . 02 . 06 . 09 . 14 . 26 Without other category Low variability Feedback ( % correct ) . 99 , 96 . 85 . 73 . 67 No feedback ( % correct ) . 98 . 88 . 82 . 71 . 67 High variability Feedback ( % correct ) . 89 . 86 . 77 . 72 . 69 No feedback ( % correct ) . 85 . 81 . 74 . 68 . 71 Note . Distance from the prototype is measured in arbitrary units . 240 LISBETH S . FRIED AND KEITH J . HOLYOAK training in percentage correct for items far from the prototype was eliminated ( Prediction T3 ) . As in the runs that included the other alternative , low - variability training yielded higher percentage correct for items close to the prototype , because the subjective likelihood of such items is relatively high when the es - timated category variance is relatively low . In the runs in Table 1 the feedback conditions tended to yield higher percentage correct than the no - feedback conditions , indicating that the latter had not achieved asymptotic learning after 100 training trials . In general , no - feedback learning by the simulation program is more variable than learning with feedback , largely because the former is more sensitive to the accuracy of early parameter estimates . We have run the program with s set at 2 , thus simply using the first two items as the initial estimates of the means of the two categories . We have also run the program setting w / and w v to 0 , thus re - moving any biases toward the assumptions of equal category frequencies and equal category variances . In both cases learning still takes place , although somewhat more slowly than with the parameter values used in the runs presented earlier . However , if both changes are made ( i . e . , s — 2 and w / = w v = 0 ) , virtually no learning takes place in the high - variability condition , although some learning is still pos - sible in the low - variability condition . Comparison With Previous Models With respect to its representational as - sumptions , the category density model is most similar to prototype models ( Posner & Keele , 1968 ; Reed , 1972 ) . Like prototype models , the density model assumes that a true induc - tion process takes place : The learner goes be - yond the sampled instances to infer category - level information . Furthermore , both types of models assume that this category - level infor - mation is represented parametrically . But whereas a simple prototype represents the central tendencies of the category instances on their feature dimensions , parameters can also be used to represent the variability of a dis - tribution , as discussed earlier , and perhaps other distributional properties as well ( e . g . , skewness ) . Like a prototype , however , repre - sentations postulated by the density model can be characterized as schemata ( Attneave , 1957 ; Oldfield , 1954 ) . Indeed , for the special case of multidimensional normal distributions ( the focus of the present article ) , the density model is equivalent to a model that assumes the learner abstracts the prototype plus variance for each category . However , in terms of its decision rule for classification , the density model is more similar to feature frequency ( e . g . , Hayes - Roth & Hayes - Roth , 1977 ) and instance models ( Medin & Schaffer , 1978 ) than to simple pro - totype models . 2 Unlike the closest prototype decision rule , the relative likelihood rule is sensitive to category variability and other fac - tors that influence the degree of overlap among exemplars of alternative categories . Unlike other classification models , the cat - egory density model provides an explicit mechanism by which categories can , under some conditions , be learned without any ex - ternal , instance - specific feedback . Regardless of whether instances are being averaged to form prototypes , used to tabulate feature fre - quencies , or simply stored in memory , other models have tacitly assumed that error feed - back is critical in category learning , since the learner must know the category to which an instance belongs in order to use it to modify the appropriate category representation . The 2 Medin and Schaffer ( 1978 ) pointed out that the distance and cue validity decision rules proposed in the classification literature ( Hayes - Roth & Hayes - Roth , 1977 ; Reed , 1972 ) are independent cue models , that is , rules that assume the information entering into category judgments is based on an additive combination of the information derived from the component feature dimensions . It follows from the nature of probability that the relative likelihood rule is not an independent cue model ; rather , as Equation 4 makes clear , it implies a multiplicative combination of dimensional information . In this respect the relative likelihood rule resembles the context model proposed by Medin and Schaffer . But whereas the latter model assumes that di - mensional information is combined to calculate a measure of instance - to - instance similarity , the relative likelihood rule assumes that such information is combined to cal - culate the conditional probability of an instance given a particular category . Wallsten ( 1976 ) presented evidence indicating that the impact of a dimension value on subjects ' decisions depends on the dimension ' s salience as well as on its diagnosticity . Salience could be represented by different weights asso - ciated with each dimension . CATEGORY INDUCTION 241 generalization procedures that have been pro - posed to reduce the storage requirements of feature frequency models ( Anderson , Kline , & Beasley , 1979 ; Patterson , 1979 ) depend upon provision of error correction . Experiments IA and IB Experiment IA focused on tests of transfer predictions T1 - T3 . Prediction T2 is in fact supported by Posner and Keele ' s ( 1968 ) find - ing that greater variability of training exem - plars produced slower initial learning , but more accurate transfer performance in a clas - sification task . The bulk of the transfer errors in the Posner and Keele study ( Experiment 2 ) were made by the low - variability group , and involved the erroneous classification of the highly distorted exemplars of meaningful pro - totypes into a category based on a random dot configuration . The random - prototype cat - egory might have been viewed by subjects as a flat , rectangular distribution with a very wide range of acceptability on the feature dimen - sions . Recall that in terms of the category density model , if category prototypes are kept con - stant , an increase in category variability will result in reduced discriminability between categories ( measured in d ' ) , making learning more difficult ( Prediction L3 ) . In a subsequent transfer task , however , subjects trained on two high - variability categories will view highly distorted exemplars as relatively likely to have been generated by the category , and so will classify them correctly . In contrast , those trained on two low - variability categories will not view highly distorted exemplars as likely to have been drawn from the category . Con - sequently , if a random alternative category is available , they will tend to classify those items as random . Thus when this alternative category is available , some highly distorted items are predicted to be classified differently depending only on the variability of the training items . This prediction of the category density model is not accounted for by a distance to prototype decision rule , because the items are the same distance from the prototypes in both groups . The above prediction was supported in pre - vious research when feedback was provided during training ( Fried , 1979 ) , but has not been previously tested when learning takes place without trial - by - trial error correction . The predicted difference between the two vari - ability groups depends on the presence of an alternative random category , because without such an alternative category the relative like - lihood rule predicts no advantage for a group that learned relatively high - variability cate - gories ( Prediction T3 ) . This latter prediction was investigated in Experiment IB , in which the random alternative or other category was removed . Method Stimuli . The choice of stimuli was guided by several criteria . We wanted stimuli : ( a ) that would allow an es - sentially infinite population of category exemplars ; ( b ) for which objective measures of both distance between any two items and of the likelihood of any item given any category could be calculated ; ( c ) for which category vari - ability could be systematically manipulated ; ( d ) with a relatively realistic degree of perceptual complexity ; and ( e ) that could be generated and displayed under computer control . These criteria were met by visual grid patterns of the sort depicted in Figure 1 . The categories to be learned in Experiments IA and IB consisted of two sets of such visual patterns , each composed of instances derived from a standard pattern by means of a probabilistic dis - tortion rule . All patterns consisted of light and dark cells in a 10 X 10 grid displayed on a computer - controlled TV screen . The two standard patterns , shown in Figure 1 , were created using a modification of the method for gen - erating figures specified by Attneave . and Arnoult ( 1956 ; see Fried , 1979 ) . The standards were adjusted so that 50 cells in each were dark and 50 were light . In addition , 50 cells overlapped between the two standards . Distortions were generated on - line by changing each cell of the standard from light to dark or vice versa with some specified dis - tortion probability , p . Increasing the distortion probability in the range . 00 to . 50 increases the variability of the distribution of instances , defined in terms of number of cells changed from the standard . These distributions were binomial approximations to the normal , with the mean number of cells changed equal to lOOp and variance equal to lOOp ( 1 - p ) , where pis the distortion probability and 100 is the number of cells in each pattern . A total of 2 100 patterns were possible . The likelihood of any particular pattern wasp " ( l - p ) > 00 ~ N , where . / Vis the number of cells distinguishing the pattern from the generating standard . The categories were thus distinguishable only in their like - lihood of generating each of the 2 100 possible patterns . The standard itself was the most likely individual pattern of each category , but since its probability was nonetheless vanishingly small , ( 1 - p ) 100 , it never actually was presented during learning trials in our experiments . As in Fried ' ( 1979 ) , distortion probabilities of . 07 and . 15 were used for the low - and high - variability learning conditions , re - spectively , in all experiments to be reported . Figure 1 il - lustrates . 07 and . 15 distortions of each of the standard 242 LISBETH S . FRIED AND KEITH J . HOLYOAK STANDARDS EXAMPLES OF RANDOM DISTORTIONS Figure 1 . The two standards used in Experiment 1 , and examples of , 07 and . 15 distortions of each . patterns . In these illustrative examples the number of changed cells is set equal to exactly lOOp . 3 Design and procedure ( Experiment I A ) . Subjects were randomly assigned to one of four conditions , denned by the 2 X 2 ( Variability X Feedback ) factorial combination of low versus high variability of training exemplars ( . 07 and . 15 distortion probabilities , respectively ) and presence versus absence of item - specific error feedback . All subjects were told that they would see a mixture of geometric pat - terns designed by two artists , named Smith and Wilson , and that they would have to distinguish the work of Smith from that of Wilson . The two standards shown in Figure 1 were used for all subjects , but eaph subject saw a different random sample of distortions , and no subject saw the standard . The patterns were displayed on a TV screen controlled by an IBM 1800 computer . During the learning phase all subjects classified a series of patterns into two categories by pressing one of two response keys . A maximum of 7 s was allowed to make each response . Subjects receiving instance - specific error feedback were told whether or not they were correct immediately after each response , thus effectively labeling the instances with respect to category membership . Subjects not receiving instance - specific error feedback did not receive such information . However , all the subjects were told the number of correct and incorrect responses they had made for each block of 10 trials . All subjects in Experiment IA thus received general infor - mation about whether their classification accuracy was improving . However , the nonspecific feedback subjects were never told the category to which any particular instance belonged . Subjects received a bonus of 1 cent for each correct answer and were fined 1 cent for each error . In addition , their pay decreased 1 cent for every 10 trials they required to learn the categories . This learning phase continued until subjects responded correctly . 10 times in a row , or reached a maximum of 200 trials . The response key assigned to a particular category by subjects in the nonspecific feedback condition was necessarily arbitrary . Their responses were scored as correct in the manner that , maximized their score over all learning trials . After completion of the learning phase , all subjects re - ceived an additional 100 transfer trials , without error cor - rection . Subjects were told that the patterns would include new Wilsons and new Smiths , but also an unspecified number of patterns designed by other people . In fact , there were no true others ; all the patterns presented during the transfer phase were actually derived with equal frequencies from the two original standards . Equal proportions of the transfer items were created at each of four distortion prob - abilities : . 10 , . 20 , . 30 , and . 40 . The transfer patterns there - fore included instances at higher levels of distortion than those that were presented during learning , even in the high - variability learning conditions . On each trial subjects pressed one of three response keys to classify the pattern as a Wilson , a Smith , or an other . Subjects in the nonspecific feedback condition had to maintain the same response - key assignments as they had established during the learning phase . A maximum of 5 s was allowed to make a response . Subjects received a 1 cent bonus for each correct classi - fication , lost 1 cent for classifying a Wilson as a Smith or vice versa , and neither won nor lost money for other re - sponses . Forty - five University of Michigan undergraduates served as paid subjects . Design and procedure ( Experiment IB ) . The design and procedure used in Experiment IB were identical to 3 We assume that on average there is a monotonic re - lationship between number of cells changed ( an objective city - block measure of distance from the standard ) and psychological distance , for the range from 0 to 50 changed cells . A distortion probability of . 50 ( expected number of changed cells equal to 50 ) yields patterns statistically un - related to the generating standard . CATEGORY INDUCTION 243 those used in Experiment IA , except for two changes . First , the summary information provided to subjects in Experiment 1 A after every 10 learning trials was eliminated . Subjects in the resulting no - feedback condition , unlike those in the nonspecific feedback condition of Experiment I A , therefore received no information about the degree to which their classification accuracy was improving . The feedback condition in Experiment IB received the same item - specific feedback as did the comparable condition of Experiment 1 A . Second , subjects were told that all transfer patterns were either Smiths or Wilsons , and were required to classify each pattern into one of those two categories ; that is , no third other alternative was available at transfer . Forty - five University of Michigan undergraduates served as paid subjects . Results and Discussion ( Experiment IA ) Learning phase . Of the 45 subjects tested , 8 had not reached criterion within the max - imum 200 trials . As expected ( Prediction L3 ) , all of these subjects were in the high - variability condition : 2 subjects who received specific er - ror feedback and 6 who received nonspecific feedback . The mean number of learning trials for all subjects was 39 for the low - variability , specific feedback condition ; 51 for the low - variability , nonspecific feedback condition ; 108 for the high - variability , specific feedback con - dition ; and 141 for the high - variability , non - specific feedback condition . The learning - trials measure proved to be highly variable ( MS E = 2 , 692 ) , reducing statistical power . Nevertheless , as in previous research ( Fried , 1979 ; Posner & Keele , 1968 ) , subjects in the high - variability conditions required significantly more learning trials to reach criterion , F ( l , 41 ) = 25 . 9 , p < . 001 , in accord with Prediction L3 . The non - specific feedback conditions tended to require more learning trials than the specific feedback conditions ; however , this trend was not sig - nificant , F ( \ , 41 ) = 2 . 09 , p < . 20 . The fact that learning was possible without specific feedback provides support for Prediction LI . Transfer phase . Of the 8 subjects who had not reached criterion within 200 trials , 3 ( all in the nonspecific feedback condition ) re - sponded with accuracy levels significantly above chance during the transfer task . Since we were interested in transfer performance af - ter at least some learning had taken place , data from the other 5 subjects were excluded from transfer analyses . The remaining 40 subjects included 10 in each of the four conditions . The relative likelihood rule predicts that if subjects had learned the mean ( or generating standard ) of each category , the percentage of patterns called other would increase as a func - tion of distance from the standards . The rule also - predicts that if subjects had learned cat - egory variability , those in the high - variability conditions would classify fewer patterns far from the standard as other ( Prediction T2 ) . Furthermore , this pattern should obtain re - gardless of whether specific error feedback is given ( Prediction Tl ) . Presented in Figure 2 is the percentage of patterns called other as a function of the number of cells by which the distorted pattern differed from the standard used to generate it ( averaging over blocks of 10 cells ) . 4 Percentage of patterns called other increased with increasing distance from the standard for all groups , F ( 4 , 144 ) = 23 . 0 , p < . 001 , s Subjects in the low - variability conditions tended to make more other responses overall than did those in the high - variability condi - tions , F ( l , 36 ) = 3 . 76 , p < . 10 . More impor - tantly , this difference became greater as the distance from the transfer pattern to the stan - dard increased , f ( 144 ) = 2 . 60 , p < . 02 , by a bilinear trend test . Furthermore , lack of item - specific error feedback did not affect the overall pattern of results , F ( l , 36 ) = 1 . 78 , p > . 25 , and produced no significant interactions . The relative likelihood rule predicts that the percentage called other should be a decreasing function of relative likelihood ; that is , the greater the relative likelihood the greater the probability that the item will be classified into the appropriate category , and the lower the probability that the pattern will be put into an erroneous category , such as other . Presented in Figure 3 is the percentage of patterns called other as a function of the natural logarithm of the likelihood ratio in favor of the correct category , p ( X \ S c ) / p ( X \ S A ) , where S c and S * are the correct , and alternative standards , re - spectively . The data points plotted in Figure 3 were obtained by averaging over blocks of approximately 20 log units of likelihood ratio . Likelihood ratio reaches higher levels for the 4 About 3 % of the transfer patterns were actually closer to the alternative standard than to the standard used to generate them . However , the pattern of results was un - changed when the closer standard was scored as correct . 5 Throughout this article , all analyses of variance on proportions were performed after applying an arc sine * transformation . 244 LISBETH S . FRIED AND KEITH J . HOLYOAK Non - Specific Feedback , Low Variability Non - Specific Feedback , High Variability Specific Feedback , Low Variability Specific Feedback , High Variability 10 20 30 40 50 Distance from Correct Standard Figure 2 . Percentage of transfer patterns called other as a function of distance from the correct standard ( Experiment IA ) . low - variability conditions , since low - variability also make it less likely that such instances distributions make it more likely that patterns could have been derived from the alternative close to the standard will be generated , and standard . This analysis shows - that the per - . 70 . . 60 . . 50 . . 30j . id »• - - * Non - Specific Feedback , Low Variability •—• Non - Specific Feedback , High Variability O - - - O Specific Feedback , Low Variability Specific Feedback , High Variability 10 20 30 40 50 60 70 80 90 100 Log Likelihood Ratio in Favor of Correct Category Figure 3 . Percentage of transfer patterns called other as function of log likelihood ratio ( Experiment IA ) . no CATEGORY INDUCTION 245 centage called other decreased monotonically across the four levels of relative likelihood at which all conditions can be compared , F ( 3 , 144 ) = 13 . 5 , p < . 001 . The relative likelihood rule predicts that among patterns equated on likelihood ratio with respect to the two learned categories , low - variability training will pro - duce a greater proportion of other responses ( since in this case the other category will often seem more likely to have generated the item than either of the two low - variability catego - ries ) . This prediction was supported , F ( l , 36 ) = 16 . 1 , p < . 001 . The effect of specificity of error feedback did not approach signifi - cance . ( Since for equal - variance categories log likelihood is highly correlated with distance from the prototype , in subsequent cases we will report only one measure . ) Because the high - variability conditions produced fewer other responses , did they then produce more correct responses ? Presented in Figure 4 is the percentage correct for all four groups as a function of likelihood ratio . Ac - curacy increased with increasing likelihood ratio , F ( 3 , 108 ) = 25 . 8 , p < . 001 . Provision of specific error feedback during training did not produce an advantage in percentage cor - rect ( F < I ) . However , variability of the train - ing instances significantly affected transfer ac - curacy . The high - variability group was signif - icantly more accurate for patterns equated on likelihood ratio , F ( l , 36 ) = 16 . 5 , p < . 001 . It is apparent from inspection of Figure 4 that for any given level of likelihood ratio the low - variability group was more likely to call a pat - tern other ( and thus have it counted as an error ) , whereas the high - variability group was more likely to classify it correctly . This pattern supports Prediction T2 . Results and Discussion ( Experiment IB ) Learning phase . Five of the 45 subjects failed to reach the criterion of 10 correct trials in a row within the maximum allotment of 200 trials . All of these were in the high - vari - ability conditions , with 3 in the no - feedback condition , and 2 in the feedback condition . •••« Non - Specific Feedback , Low Variability •—• Non - Specific Feedback , High Variability O - O Specific Feedback , Low Variability i Specific Feedback , High Variability 10 20 30 40 50 60 70 80 90 100 110 Log Likelihood Ratio in Favor of the Correct Category Figure 4 , Percentage of transfer patterns classified correctly as a function of log likelihood ratio ( Experiment U ) . 246 LISBETH S , FRIED AND KEITH J . HOLYOAK As in Experiment IA , the learning trials mea - sure was highly variable ( MS E = 3 , 532 ) . Once again subjects in the high - variability conditions required more learning trials than did those in the low - variability conditions ( M = 91 trials and M = 43 trials , respectively ) , P ( l , 41 ) = 7 . 17 , p < . 025 . Although the trend favored the subjects who received error feedback over those who did not ( 65 trials vs . 75 trials ) , this difference did not approach significance ( F < 1 ) . Weak statistical power suggests caution in accepting the null hypothesis ; however , it is clear that error feedback was not a necessary condition for category learning in the present task . It can also be concluded that the non - specific feedback in Experiment IA was not instrumental in producing learning in that condition . Transfer phase . The 5 subjects who failed to reach the learning criterion were excluded from analyses of transfer performance . Pre - sented in Figure 5 is the percentage correct classification at transfer as a function of log likelihood ratio in favor of the correct category . As in Experiment IA , percentage correct in - creased as a function of likelihood ratio , F ( 3 , 108 ) = ll , l , p < . 001 . Also as in Experiment 1 A , overall percentage correct was not influ - enced by error feedback ( F < 1 ) . However , level of feedback did interact significantly with variability of the training set , F ( \ , 36 ) = 8 . 20 , p < . 01 . As is apparent in Figure 5 , this in - teraction was mainly due to the especially ac - curate performance of the high - variability feedback condition at its two highest levels of likelihood ratio . Error feedback did not have a significant effect for the low - variability con - dition , F ( \ , 18 ) = \ A \ , p > . 25 . As predicted by the relative likelihood rule ( Prediction T3 ) , and in sharp contrast to Ex - periment IA , high - variability training in - stances did not improve transfer accuracy when an other category was : not available . When only patterns equated on likelihood ratio were considered ( thus excluding patterns at the two highest levels of likelihood ratio , ex - perienced only by the low - variability groups ) , variability had no significant effect , F ( 1 , 36 ) = High Variance , Feedback High Variance , No Feedback Low Variance , Feedback Low Variance , No Feedback 0 10 20 30 40 50 60 70 80 90 100 110 Log Likelihood Ratio in Favor of the Correct Category Figure 5 . Percentage of transfer patterns classified correctly as a function of log likelihood ratio ( Experiment IB ) . CATEGORY INDUCTION 247 1 . 68 , p > . 20 , as the relative likelihood rule predicts . The results of Experiment IB indicate that high - variability training does not always produce more accurate transfer performance for exemplars far from the standard , nor does exposure to a larger set of training items ( be - cause in both Experiments 1 A and IB , subjects in the high - variability condition received more instances before reaching the learning crite - rion ) . Rather , the existence of a random or 0 / Aeralternative at transfer ( as in Experiment IA ) is critical to producing an advantage of high - variability training . Experiment 2 In Experiments IA and IB variability was manipulated across different groups of sub - jects . The relative likelihood rule can also be tested by training a single group of subjects with two equally probable categories of un - equal variability . If subjects learn the category distributions , the rule predicts that they will tend to classify more patterns into the high - variability category , even though exemplars of the two categories are equally likely a priori ( Prediction T4 ) . In particular , some patterns that are physically more similar to the standard of the low - variability category will be more likely to be generated by the high - variability category . If subjects learn the distributions and follow the relative likelihood decision rule , they should tend to classify such patterns into the high - variability category . In contrast , if sub - jects employ a closest prototype rule , based on some monotonic function of physical dis - tance , they will tend to classify such patterns into the low - variability category . Method Two new standard patterns were used in Experiment 2 . These were constructed in the same manner as the stan - dards used in Experiments 1 A and IB except that the new standards were the same in only 40 ( rather than SO ) of the 100 cells . During the learning phase the instances of one standard were derived by a . 07 distortion probability ( the low - variability category ) , and the instances of the other standard were derived by a . 15 distortion probability ( the high - variability category ) . Assignment of the two standards to distortion level was counterbalanced across subjects . One other major change was introduced in the learning phase of Experiment 2 . The results of Experiment 1 and earlier studies indicate that high - variability categories are harder to learn than low - variability categories . If subjects simply had to discriminate instances of a low - versus a high - variability category , they could do so by learning only the low - variability category , and then assigning all re - maining instances to the high - variability category . Subjects would thus never need to acquire a clear conception of the high - variability category . Under these conditions the high - variability category would presumably be treated as an other alternative during transfer . As a result , high - level distortions of the low - variability category would tend to be classified as members of the high - variability category , but not for the theoretically relevant reason . It was therefore important to ensure that subjects would actually learn the distribution of the high - variability cat - egory during the learning phase , rather than treat it as a vague other category . Accordingly , the training set consisted of equal numbers of . 07 distortions of the standard for the low - variability category , . 15 distortions of the standard for the high - variability category , and . 50 distortions of both standards . Instances created by a . 50 distortion prob - ability are truly random ( i . e . , they are statistically inde - pendent of the generating standard ) , and thus constituted a true other category . To reach the learning criterion of 10 successive correct trials , subjects therefore had to learn not only to discriminate instances of the low - versus high - variability categories but also to discriminate instances of the high - variability category from others . Subjects were required to make a decision for each pat - tern within 7 s . Half the subjects received error correction on each trial and half never received error correction . For those trained without error feedback the assignment of category label ( Smith or Wilson ) to the two standard cat - egories was arbitrary ; however , the category to be labeled other •was nonarbitrary . At the beginning of the transfer phase , subjects were told that they would see new works by Wilson and Smith , not necessarily in equal numbers , and that no works by other people would be included . They had to classify each pattern as either a Wilson or a Smith , and thus were forced to discriminate solely between the high - variability and the low - variability categories . Chance accuracy was therefore 50 % . The transfer set consisted of a total of 100 patterns , half derived from each standard , with equal numbers gen - erated at distortion probabilities of . 15 , . 25 , . 30 , and . 35 . As in Experiments 1 A and IB , the assignments of category labels to instances in the transfer phase had to be the same as those established during learning ( i . e . , for subjects in the no - feedback condition , assignments were not arbitrary at transfer ) . Twenty - five University of Michigan students served as paid subjects . Results and Discussion Learning phase . The mean number of learning trials was virtually identical for sub - jects who received feedback and those who did not ( although medians favored the feedback subjects , 119 vs . 149 ) . However , consistent with Prediction L2 , 5 subjects in the no - feedback condition ( vs . none in the feedback condition ) failed to reach the learning criterion within 200 trials . In all cases the nonlearners had 248 LISBETH S . FRIED AND KEITH J . HOLYOAK difficulty discriminating the high - variability and other categories . Transfer phase . Only data for the 20 sub - jects who reached the learning criterion were analyzed . An overall picture of transfer per - formance is provided by Figure 6 , in which appears the percentage of items classified into the highwariability category as a function of the log likelihood ratio favoring that category . Percentage classified into the high - variability category was an increasing function of like - lihood ratio in favor of that category , F ( 5 , 90 ) = 23 . 0 , p < . 001 . Error feedback did not significantly influence the pattern of results ; however , the large percentage of nonfeedback subjects who failed to learn cautions against accepting the null hypothesis . The results also did not differ as a function of which standard was assigned to the high - variability category . The main concern in Experiment 2 was to determine whether subjects base their classi - fication decisions on likelihood or distance . The relative likelihood rule predicts that more patterns will be classified into the high - vari - ability than the low - variability category , since subjects will have learned a broader density function for the former category . In contrast , a strict distance - to - prototype rule predicts that an equal proportion of instances will be clas - sified into each category , because the distri - butions of instances around their standards were actually identical for the two categories during the transfer phase . The prediction of the relative likelihood rule was supported , as 61 % of the transfer items were placed in the high - variability category . This figure was sig - nificantly higher than the 50 % predicted by the distance rule , f ( 19 ) = 3 . 28 , p < . 01 . A separate analysis was performed for just those items that were closer to the standard of the low - variability - category ( in terms of changed cells ) , but more likely to be generated by the high - variability category . These items provide a particularly strong test of whether subjects used a decision rule based on distance or likelihood . If classifications were based on any monotonic function of physical distance that is constant over category variability , these patterns would tend to be placed in the low - variability category ; but if classifications were based on any mohptonic function of likeli - hood , these items would tend to be placed in the high - variability category . The prediction of the relative likelihood rule was confirmed ; 64 % of these critical items , which were phys - ically closer to the low - variance standard , were actually classified into the high - variability cat - egory . This percentage was significantly higher than 50 % , t ( \ 9 ) = 2 . 67 , p < . 02 . Experiment 3 The purpose of Experiment 3 was to provide a more extensive investigation - of the role of . 80 . . 60 . | 40 . •S . 20 . No Feedback Feedback j - 80 - 60 - 40 - 20 0 20 40 60 80 100 120 Log likelihood ratio in favor of the high variability category Figure 6 . Percentage of transfer patterns classified into the high - variability category ( Experiment 2 ) . CATEGORY INDUCTION 249 labeled instances and other types of supple - mentary information ( Tracy & Evans , 1967 ) in facilitating category learning , and in par - ticular to determine whether knowledge of the number of categories facilitates learning ( Pre - diction L4 ) . Different groups of subjects re - ceived one of four levels of supplementary in - formation . Subjects in the labeled instances condition received a category label ( Wilson or Smith ) with each exemplar . Subjects in the number known condition received neither category labels nor error correction , but were told ( correctly ) that there were two categories to be learned . The first of these two conditions represented complete information , whereas the second was similar to the no - feedback con - ditions of Experiments IB and 2 in the amount of information available . Subjects in two ad - ditional conditions received still less infor - mation about the learning task . Those in the number unknown condition were told they were to try to learn the categories represented in the training set , but were not told how many categories would be present . Finally , those in the observation only condition were not told that their task was to learn categories , nor that categories were present ; they were simply told to " pay the utmost attention " to each pattern in the set ( cf . Reber & Allen , 1978 ) . Prior to a subsequent transfer task , all subjects were informed that they had seen exemplars drawn from exactly two categories . The category density model predicts that the labeled in - stances condition will yield superior learning to the number known condition ( Prediction L2 ) , and that the latter condition will yield superior learning to the remaining two con - ditions ( Prediction L4 ) . In fact , because the parameter - revision procedure can only operate if the number of categories is known , themodel predicts that no learning will take place in the number unknown and observation only con - ditions ( unless subjects in these conditions happen to guess the correct number of cate - gories ) . Method Apparatus and patterns . The patterns were presented on a Hazeltine text terminal controlled by a PDF 11 / 34 computer . Each pattern was composed of a 10 X 10 grid , in which each cell consisted of two horizontally aligned character spaces . If the cell was defined as black , both character spaces were blank ; if it was defined as white , each space was occupied by the rectangular ASCII character 127 rubout . Two standard patterns were generated for each subject . The first standard was created by randomly making each cell black or white with an equal probability . The second standard was derived from the first by switching 50 randomly selected cells from black to white or vice versa . Different standards were randomly generated for each subject within a given condition , whereas across learning conditions subjects were yoked with respect to the standards , training exemplars , and transfer set . As in previous experiments , exemplars were generated by prob - abilistic distortions of the standards . Design and procedure . ' Subjects were randomly assigned to one of eight experimental conditions , defined by the 2 X4 ( Variability X Condition ) factorial combination of low - versus high - variability of training exemplars ( . 07 vs . . 15 distortion probabilities ) , and the four instructional conditions outlined earlier . Following initial instructions , all the experimental subjects participated in a training phase in which they viewed 200 exemplars . These consisted of a random mixture of 100 instances derived from each standard . A major methodological change introduced in Experiment 3 was thus to present subjects with a fixed number of training exemplars , rather than to allow subjects to reach a learning criterion . Using a fixed number of learning trials avoids several methodological problems in - herent in a criterion procedure . A criterion measure creates a confounding between learning difficulty and number of training trials . In addition , the exclusion of those subjects who failed to reach the criterion can bias analyses of transfer performance in favor of conditions that are more difficult to learn . Although using a fixed number of learning trials . avoids the above problems , it does have a disadvantage of its own . Differences in learning rate among instructional conditions may not be observed if all conditions achieve asymptote within the allotted number of learning trials . This is most likely to occur for groups who receive low - variability training exemplars . Each pattern was presented for just 2 s . The subject then pressed a key to initiate presentation of the next pattern . Subjects did not make overt classification responses during learning , but simply observed the exemplars . Except for the labeled instances condition , for which a category label , Wilson or Smith , was written beneath each pattern , all subjects had the same type of observation experience . After completing the training phase , subjects in the number unknown and observation only conditions were debriefed to find out whether they had noticed that the patterns were drawn from two categories . Subjects in the observation only condition were asked a series of increas - ingly directive questions : ( a ) Did you notice anything in - teresting about the patterns ? ( b ) Did you notice any sim - ilarities among them ? ( c ) Did you notice that the patterns fell into different groups of categories ? and finally , ( d ) How many categories do you think the patterns were di - vided into ? The latter two questions were also asked of subjects in the number unknown condition . All sub - jects were then informed that the number of categories was two . The subjects then received 100 transfer trials . In the manner of Experiment IA , subjects were told that the patterns would include new works by Wilson and Smith , as well as an unspecified number of works by other people . The transfer patterns actually consisted of 50 exemplars 250 LISBETH S . FRIED AND KEITH J . HOLYOAK from each category , 10 at each of five exact distances from the standard ( in terms of number of changed cells ) : 5 , 15 , 25 , 35 , and 45 . Each pattern was presented for a maximum of 7 s , and subjects pressed one of three keys to classify the pattern as a Wilson , a Smith , or an other . No error correction was given . As in previous experiments , subjects in all except the labeled instances condition were free to interchange the keys for Wilson and Smith ; their key as - signments were determined afterward by the usual con - sistency test . In addition to the eight experimental conditions de - scribed so far , an additional transfer control condition was included . 6 Subjects in this condition did not receive any learning trials . The transfer control condition was included to provide a base - rate estimate of the amount of learning that could occur without feedback solely during the transfer phase of the experiment . One hundred undergraduates served as paid subjects . Ten were assigned to each of the eight experimental con - ditions , and 20 to the transfer control condition . Results Knowledge of category number . A prelim - inary assessment of the difficulty of the learn - ing task in Experiment 3 is provided by the responses to the questions asked subjects in the number unknown and observation only conditions . The first two questions directed to the observation only subjects ( Did you notice anything interesting about the patterns ? Any similarities among them ? ) failed to elicit any clear statements regarding the presence of cat - egories . When asked whether they had noticed that the patterns were divided into categories , 8 subjects in the observation only condition said yes , 11 said no , and 1 did not respond ( without notable differences between those who had viewed low - vs . high - variability dis - tributions ) . Finally , subjects in both the ob - servation only and number unknown condi - tions were directly asked to estimate the num - ber of categories . ( Subjects in the observation only condition were first told that the patterns were indeed drawn from different categories . ) The distributions of estimates did not differ across either instructional conditions or levels of training variability , so we will report the aggregate results for all 40 subjects : The me - dian estimate was 4 , with a range of 2 to IS . Previous studies have also reported overesti - mation of the number of categories ( Bersted et al . , 1969 ; Hartley & Homa , 1981 ) . Only 4 subjects said there had been two categories ; all of these 4 had received observation only instructions , and 3 had seen high - variability distributions—the learning situation one might well suppose had the least a priori like - lihood of enabling the number of categories to be learned . Given the diversity of the es - timates , it seems quite likely that the few sub - jects who gave the correct answer did so by fortuitous guessing . It is clear that virtually ( and perhaps literally ) none of the subjects learned the number of categories used to ' gen - erate the patterns , even with exposure to 200 examples . Transfer performance . Presented in Figure 7 is the percentage of patterns classified cor - rectly as a function of distance from the stan - dard , plotted separately for each instructional condition . The results for the conditions that received low - variability distributions during the learning phase appear in panel A , whereas the results for the conditions thatreceived high - variability distributions appear in panel B . For purposes of comparison , the results for the transfer control condition are plotted in both panels . An analysis of variance was performed on these data , with the transfer control subjects divided into two arbitrary groups to create a balanced design . Percentage correct declined significantly with increasing distance from the standard for each of the eight experimental groups ( p < . 01 ) but not for the transfer control condition . Since subjects in the latter condition did not receive a training phase , any category learning would have had to take place over the course of the transfer trials . In fact , even this control condition produced a significant effect of distance from the standard when only data for the second half of the transfer trials were considered , F ( l , 76 ) = 11 . 4 , p < . 001 , indicating that learning did take place during the transfer phase . Percentage correct declined from . 47 for the items closest to the standard to . 36 for those farthest from it . When the response other is available , per - centage correct should show a greater decline with increasing distance for the low - than for the high - variability groups ( as in Experiment IA ) . As can be seen by comparing panels A and B in Figure 7 , this prediction was con - firmed , F ( 4 , 360 ) = 8 . 28 , p < . 001 . Collapsing over the four experimental conditions within each variability level , this interaction took the 6 This control condition was suggested by Michael Flan - nagan . CATEGORY INDUCTION 251 form of a crossover : For those patterns closest to the standard , the percentage correct was higher for the low - variability condition ( 77 % vs . 63 % ) , whereas for those patterns furthest from the standard this difference reversed ( 20 % vs . 32 % ) . This pattern confirms the compa - rable result ( Prediction T2 ) obtained in Ex - periment IA . The results of primary interest concern the effects of the different types of supplementary information on learning . Because subjects in the various instructional conditions within each variability level received exactly the same exemplars during learning , differences among the groups within each level of variability must reflect differential learning of the distributions . Superior learning should be evidenced by higher percentage correct for those patterns most likely to belong to the training distri - bution ( i . e . , those relatively close to the stan - dard ) . ( From the learner ' s point of view , pat - terns relatively far from the standard ought to be classified into the other category . ) For the low - variability groups , an analysis was per - formed on the percentage correct data for pat - terns 5 cells from the prototype . The labeled instances and number known conditions did not differ ( t < 1 ) but were superior to the number unknown and observation only con - ditions ( p < . 01 ) . The latter two groups did not differ from each other ( t < 1 ) but were superior to the transfer control condition ( p < . 01 ) . A comparable analysis was performed for the high - variability groups , examining per - centage correct for patterns 5 and 15 cells from the standard ( the patterns most consistent with the high - variability training distributions ) . In this analysis the labeled instances group ex - Labelled Instances Number Known Number Unknown Observation Only Transfer Control A . Low Variability Training B . High Variability Training 15 25 35 45 15 25 35 45 Distance from Correct Standard Figure 7 . Percentage of transfer patterns classified correctly as a function of distance from the correct standard for the various instructional conditions ( Experiment 3 ) . ( Data for low - variability conditions appear in panel A , and data for high - variability conditions appear in panel B ; data for the transfer control condition are plotted in both panels . ) 252 LISBETH S . FRIED AND KEITH J . HOLYOAK ceeded the other three experimental conditions ( pk . . 0l ) . The latter did not differ among themselves , but were collectively superior to the transfer control condition ( p < . 05 ) . A possibility to be considered is that the above differences in percentage correct only reflect differences among the propensities pf different groups to use the other category , since other responses were always scored as incor - rect . However , if only such response biases were operating , groups ranking relatively high in overall percentage correct would tend to rank relatively low in percentage correct of those patterns classified into a nonother category . No such trade - off was apparent ; rather , groups ranked relatively high in overall percentage correct tended to also be ranked relatively high in percentage correct of those patterns not classified other . For the low - variability groups conditional percentage correct was . 79 , . 79 , . 73 , . 71 , and . 60 for the labeled instances , number known , number unknown , observa - tion only , and control conditions , respectively . Comparable figures for the high - variability groups were . 75 , . 63 , . 66 , . 61 , and . 60 . Discussion The results just presented provide a mixture of support and difficulty for the category den - sity model . First , consider the conditions that received low - variability training exemplars . Subjects in the labeled instances and number known conditions were most accurate in clas - sifying the patterns closest to the standard . These are the only two learning conditions that had the prerequisite information for use of a strategy of parameter revision . The lack of difference between the labeled instances and number known conditions at the lower level of variability suggests that learning had ap - proached asymptote in these two conditions after the 200 training trials . This result is thus consistent with the category density model . The advantage displayed by the number known condition relative to the number unknown and observation only conditions also supports the model ( Prediction L4 ) , because knowledge of category number is critical to the postulated parameter - revision process . However , the sub - stantial , albeit lesser , degree of learning by subjects in the latter two conditions cannot be explained by a parameter - revision process . These subjects were not told the number of categories in advance , nor did they determine the number of categories during the learning trials ; accordingly , they lacked the prerequisite information for parameter revision . The results thus implicate a second type of learning mechanism that requires even less supple - mentary information than does parameter re - vision . The data for the high - variability condition also present a mixed picture for the category density model . The predicted advantage of re - ceiving labeled instances ( Prediction L2 ) was confirmed for categories with a high degree of overlap . However , as in the low - variability condition , substantial learning also took place in the number unknown and observation only conditions . Unlike the result obtained for the comparable low - variability groups , the number known condition was not superior to the two conditions that lacked knowledge of the num - ber of categories ( a failure of Prediction L4 ) . The results thus suggest that subjects had available some other learning mechanism that operates as effectively for high - variability cat - egories as does parameter revision without feedback . Experiment 4 The relative efficacy of the various instruc - tional conditions should be independent of whether or not an other alternative is provided at transfer . In the absence of an other category , superior learning should again be evidenced by higher percentage correct for patterns at the distortion levels most likely to have been observed during training . Without an other category , accuracy will necessarily approach an asymptote at chance level as distortion level is increased . Differences in percentage correct across instructional conditions should there - fore be progressively attenuated as transfer patterns become more remote from the stan - dards . As a result , the function relating per - centage correct to distance from the correct standard should have a steeper slope for an instruction condition that produces superior learning ( when variability of the training ex - emplars is held constant ) . Experiment 4 was performed to examine the effects of alternative levels of supplementary information on trans - fer performance in the absence of an other CATEGORY INDUCTION 253 alternative . Rather than repeating the full de - sign of Experiment 3 , only two comparisons of particular theoretical import were made . First , the number known and number un - known conditions with low - variability training were compared . We wished to replicate the advantage of the number known group ( Pre - diction L4 ) , observed for the comparable con - ditions in Experiment 3 . Second , the labeled instances and number known conditions with high - variability training ( for which subjects are not likely to reach asymptote after 200 learning trials ) were also compared . Obtaining the pre - dicted advantage of providing labeled instances ( Prediction L2 ) would extend the comparable result observed in Experiment 3 . Method The method of Experiment 4 was identical to that of Experiment 3 , except that only the four groups mentioned above were included in the design , and no other alternative was provided during the transfer phase ( as in Experiment IB ) . Thirteen subjects served in each of the two low - vari - ability conditions , and 8 served in each of the high - vari - ability conditions . Results and Discussion Presented in Figure 8 is the mean percentage correct for the four conditions as a function of distance from the standard . All conditions yielded response functions with significant negative slopes ( p < . 01 ) approaching asymp - tote at the chance expectation of 50 % for pat - terns maximally dissimilar to the standard . Both of the critical comparisons between con - ditions were in accord with the predictions of the density model . For the two low - variability groups ( panel A in Figure 8 ) , the slope of the distance function was significantly steeper for the number known than for the number un - known condition , t ( 96 ) = 2 . 72 , p < . 01 ( Pre - diction L4 ) . Tests of the simple main effects 100 90 £ 80 o o c 0 ) o t - cu0 - c . o 0 ) - \ 70 60 50 40 - Labelled Instances • - Number Known o - Number Unknown x - . \ ' ' ~ ow Variability Training B . High Variability ' Training _ L 15 25 35 45 15 25 45 Distance from Correct Standard Figure 8 . Percentage of transfer patterns classified correctly as a function of distance from the correct standard for the various instructional conditions { Experiment 4 ) . ( Data for low - variability , conditions appear in panel A , and data for high - variability conditions appear in panel B . ) 254 LISBETH S . FRIED AND KEITH J . HOLYOAK of instructional condition revealed that the number known condition produced signifi - cantly higher percentage correct for patterns five cells from the standard ( p < . 01 ) whereas the two groups did not differ significantly for patterns at higher distortion levels . As in Ex - periment 3 , subjects in the number unknown group could not report the true number of categories ( 2 ) at the end of the learning phase . The median estimate was 4 , with a range of 1 to 15 . The superior performance exhibited by the number known condition for the pat - terns most likely to be generated by the training distribution can be attributed to the use of a parameter - revision strategy , which knowledge of category number makes possible . These re - sults replicate and extend the parallel findings of Experiment 3 . Presented in panel B of Figure 8 are the results for the two high - variability conditions that were tested . The slope of the distance function was higher for the labeled instances than for the number known condition , / ( 56 ) = 4 . 96 , p < . 001 , and percentage correct was significantly higher for the former condition at the two lowest distortion levels ( p < . 01 , Prediction L2 ) . These results extend the com - parable findings obtained in Experiment 3 . General Discussion Summary and Implications The present results provide a broad initial basis of support for the category density model and its assumptions about distribution learn - ing , as well as suggesting ways in which the model requires revision . The first two exper - iments yielded a number of results predicted by the proposed relative likelihood decision rule . These include the following : ( a ) the su - perior transfer performance that resulted from training on high - variability instances when an other category was available ( Experiment 1 A ) ; ( b ) the absence of any clear advantage for the high - variability condition when the other cat - egory was removed ( Experiment B ) ; and ( c ) the tendency to classify items into the more likely high - variance category , even for patterns physically closer to the low - variance standard ( Experiment 2 ) . These results indicate that learners use exemplars to induce the distri - bution functions of categories , and then clas - sify novel instances according to a relative likelihood rule based on these induced density functions . Other results obtained in Experiments 1 and 2 , plus the more detailed exploration of the learning process in Experiments 3 and 4 , have implications for possible mechanisms of dis - tribution learning . Major findings included the following : ( a ) Category distributions can gen - erally be learned regardless of whether or not the learner receives labeled instances or error correction ( Experiments lA - 3 ) ; ( b ) labeled in - stances , which obviate the need for a decom - position process , facilitate category learning under conditions of high distributional overlap ( Experiments 3 - 4 ) ; , ( c ) without labeled in - stances , prior knowledge of the number of cat - egories ( a prerequisite for paramenter revision ) facilitates , acquisition of category knowledge when the distributions do not overlap exces - sively ( Experiments 3 - 4 ) ; and ( d ) category structure can still be learned when the learner does not receive error correction , information about category number , or even instructions to learn categories ( Experiment 3 ) . Toward a General Model of Distribution Learning This article has focused on a specific version of the category density model that can account for the induction of normally distributed cat - egories when subjects know ( or assume ) the form of the distributions and the number of categories . It is clear that this specific model is loo restrictive to account for all aspects of human capacity to learn category distribu - tions . As we argued at the outset , normal dis - tributions may well be an ecologically impor - tant special case ; nonetheless , there is exper - imental evidence that people can sometimes learn markedly nonnormal distributions ( Neumann , 1977 ) . Furthermore , the results of Experiment 3 demonstrated that people can learn distributions to some degree not only without knowledge of the number of catego - ries , but without knowledge that they are in a category - learning task at all . Perhaps a learning model entirely different from the cat - egory density model is required ^ However , we will instead suggest how the model can be re - vised and extended . The present findings , as well as other considerations discussed later , lead us to sketch a more general learning model which , while speculative , yields testable pre - dictions . CATEGORY INDUCTION 255 The parameter - revision process described earlier allows a schematic representation of distributional knowledge to be updated with - out exemplars necessarily being stored in memory . The model is thus quite contrary in spirit to instance - storage models such as that proposed by Medin and Schaffer ( 1978 ) . How - ever , a more general model can be devised by integrating these two approaches . In this dual - representation , dual - process model , category distributions can be represented either by sta - tistical parameters or by memory traces of in - stances . The latter type of representation per - mits a mechanism for learning category dis - tributions without knowledge of the number of categories or the form of their density func - tions . This strategy , which corresponds closely to a proposal made by Evans ( 1967 ) , involves storing traces of presented instances until sep - arable clusters emerge . These memory traces need not be highly veridical , as long as they collectively approximate the mixture density of the sample . , Each observed instance could be encoded as a point in a feature space , with an associated gradient of generalization . As further instances are encoded , a multidimen - sional frequency histogram will gradually be built up , providing a nonparametric repre - sentation of the mixture density . An instance - clustering process might enable a person to learn categories without error cor - rection , even if the learner does not initially know either the number of categories or the form of their distributions . If the underlying category distributions are sufficiently discrim - inable , separable clusters will eventually emerge , corresponding to peaks and valleys in the histogram for the mixture . Various clus - tering algorithms could be used to model the decomposition process ( Duda & Hart , 1973 ; Everitt , 1974 ) , As is the case for parameter revision , feedback would presumably facilitate decomposition . The two learning mechanisms we have out - lined—parameter revision and instance clus - tering—could be integrated by elaborating . the learning procedure embodied in the computer simulation described in the beginning of this article . In the simulation , a clustering algo - rithm is used on the first few instances to derive initial parameter estimates , which are then updated using parameter revision . More gen - erally , people may initially store and cluster instances in order to get a general idea of what the categories being presented are like . They may then summarize the information ex - tracted from the stored instances as a para - metric description , which subsequently can be fine - tuned using parameter revision . Initial instance clustering can be used to form initial conceptions of the category distribution , or to check a priori assumptions ; parameter esti - mation and revision can be implemented at any point during the learning process once the learner is sufficiently confident about the number and form of the distributions . Even after a parameter - revision strategy is invoked , continued incidental instance storage may play a role in detecting violations of basic assumptions about the form of the category distributions . For example , suppose the learner assumes the categories to be learned are nor - mally distributed over a feature space , when in fact the distributions are markedly non - normal ( e . g . , V shaped ) . A strict parameter - revision process would never be able to correct this misconception . Presumably , however , current parameter estimates would yield ex - pectations about the frequencies of possible , instances of the categories . If the learner found that supposedly rare instances were appearing too frequently , this would cast doubt on the assumed form of the density functions . A ra - tional learner would then temporarily abandon parameter revision and shift to instance clus - tering . In addition , there are very likely circum - stances in which stored instances provide the only possible memory representation for a category distribution . Possible examples in - clude the following situations : 1 . The distributions of the underlying cat - egories may be so irregular that no simple ^ parametric description of them exists . For ex - ample , the category of things stored in my attic may have no simpler description than a list of all the items , 2 . The learning set presented to subjects may be so small and / or variable that the cat - egories appear to be collections of unrelated objects , as in Situation 1 just described . Nu - merous studies in the classification literature may exemplify this type of situation ( e . g . , Pe - terson , Meagher , Chait , & Gillie , 1973 ) . 3 . The learning task may emphasize rote memorization of instances and disguise the underlying category structure present in the learning set ( Brooks , 1978 ) . 256 LISBETH S . FRIED AND KEITH J . HOLYOAK , These three situations are cases in which no simple schematic description of the cate - gory is available . This raises a question that is basic to the distributional framework : What is the range of category distributions that peo - ple can encode as parameter vectors ? The present paper has emphasized normal distri - butions , which we suggested may be psycho - logically natural for continuous dimensions ; however , other types of parametric represen - tations may also prove important . For ex - ample , a binary feature dimension can be de - scribed by a Bernoulli process with its single parameter , p . Presumably there are limits on the types of distributions that people can ve - ridically represent parametrically , and on those that may be learned by instance storage . Some research on the acquisition of nonnormal dis - tributions has been done in studies of decision making ( Pitz , Leung , Hamilos , & Terpening , 1976 ) and of category learning ( Flannagan , Fried , & Holyoak , 1981 ; Neumann , 1977 ) , but more work on this issue is clearly called for . A second question that needs to be explored concerns the learning and representation of correlations between features within a category ( e . g . , members of small - bird species are more likely to sing than members of large - bird spe - cies ) . Medin and Schaffer ( 1978 ) have shown that people are sensitive to within - category feature correlations for artificial categories . Such correlational information could be rep - resented parametrically by the equivalent of a variance - covariance matrix . Alternatively , a superordinate category composed of several distinct subcategories ( i . e . , separate or over - lapping instance clusters ) could be represented as the disjunction of the distributions of the subcategories ; the features within each sub - category might be independent . Individual stored instances can be viewed as the limiting case of a category representation based on the disjunction of multiple distributions . 7 Further Directions We have already touched on a number of directions in which the category density model may guide research . There is some evidence that prior expectations can influence the in - duction of category distributions ( Neumann , 1977 ) , but the issue has just begun to be in - vestigated systematically by manipulating the form of the distributions of category exemplars over known feature dimensions ( Flannagan et al . , 1981 ) . In addition , the possibility that the learning process may undergo qualitative changes ( e . g . , a shift from instance clustering to parameter revision ) calls for further studies that vary degree of category acquisition ( Homa , Sterling , & Trepel , 1981 ) . The types of categories studied also need to be extended . The category density model is not inherently restricted to categories denned solely by per - ceptual features as in the present study and most similar research . In principle people could learn category distributions over se - mantic or functional dimensions as well as perceptual ones . Finally , it should be emphasized that the relationship between instance and category knowledge has broad import for cognitive the - ory . For example , Gick and Holyoak ( 1983 ) have investigated the induction of a " problem schema " from experience with multiple anal - ogous problems . The category density model assumes that the ideal outcome of category learning is a representation of the dimensions of variation among category exemplars , to - gether with a parametric description of cate - gory distributions over these dimensions . Since in its broader sense category learning is clearly 7 The relative likelihood rule can be applied even if categories are represented solely by stored instances . Each stored instance will establish a microdistribution , equivalent to a generalization gradient around the point in a feature space denned by the instance . The relative likelihood rule then predicts that the subjective probability of a particular novel item given a particular category will be proportional to the sum of the subjective probabilities 1 of the item given the microdistributions associated with each , of the stored instances of that category ( assuming the stored instances to be equally likely ) . It should be noted that in this special case , in which distributions are represented solely by stored instances , the relative likelihood - rule is isomorphic to the decision rule specified by Medin and Schaffer ( 1978 , p . 211 , As - sumptions 2 , 4 , and 5 ) . Their rule operates on a similarity parameter for each feature , which is denned to range in value between 0 and 1 . The corresponding parameter of the relative likelihood rule , expressed in terms of features ( Equation 4 ) , is interpreted as the subjective likelihood of observing the feature value of the item to be classified given the feature value of the stored instance to which it is being compared . Because this likelihood ranges between 0 and 1 and is assumed to decrease monotonically with increasing distance between the two feature values , it has the same properties as the corresponding similarity pa - rameter specified by Medin and Schaffef ' s rule . The two rules operate on these basic corresponding parameters in an entirely parallel fashion . CATEGORY INDUCTION 257 involved in various complex domains , such as problem solving and story understanding , a general theory of the learning process could serve to highlight commonalities among a wide range of cognitive activities . References Anderson , J . R . , Kline , P . J . , & Beasley , C . M . ( 1979 ) . A general learning theory and its application to schema abstraction . In G . H . Bower ( Ed . ) , The psychology of learning and motivation ( Vol . 13 , pp . 277 - 318 ) , New • York : Academic Press . Attneave , F . ( 1957 ) . Transfer of experience with a class - schema to identification learning of patterns and shapes . Journal of Experimental Psychology , 54 , 81 - 88 , Attneave , E , & Arnoult , M . D . ( 1956 ) . The quantitative study of shape and pattern perception . Psychological Bulletin , 53 , 452 - 471 . Bersted , C . T . , Brown , B . R . , & Evans , S . H . ( 1969 ) . Free sorting with stimuli clustered in a multidimensional space . Perception & Psychophysics , 6 , 409 - 414 . Brooks , L . ( 1978 ) . Nonanalytic concept formation and memory for instances . In E . Rosch & B . B . Lloyd ( Eds . ) , Cognition and categorization ( pp . 170 - 207 ) . New York : Wiley . Bruner , J . S . , Goodnow , J . J . , & Austin , G . A . ( 1956 ) . A study of thinking . New York : Wiley . Duda , R , O . , & Hart , P . E . ( 1973 ) . Pattern classification and scene analysis . New York : Wiley . Edmonds , E . M , , & Evans , S . H . ( 1966 ) . Schema learning without a prototype . Psychonomic Science , 5 , 247 - 248 . Edwards , W . , Lindman , H . , & Savage , L . J . ( 1963 ) . Bayesian statistical inference for psychological research . Psycho - logical Review , 70 , 193 - 242 . Evans , S . H . ( 1967 ) . A brief statement of schema theory . Psychonomic Science , 8 , 87 - 88 . Evans , S . H . , & Arnoult , M . D . ( 1967 ) . Schematic concept formation : Demonstration in a free sorting task . Psy - chonomic Science , 9 , 221 - 222 . Everitt , B , ( 1974 ) . Cluster analysis . London : Heinemann Educational Books . Flannagan , M . , Fried , L . S . , & Holyoak , K . J . ( 1981 , No - vember ) . Perceptual category learning and distributional structure . Paper presented at the 22nd meeting of the Psychonomic Society , Philadelphia , PA . Fried , L . S . ( 1979 ) . Perceptual learning and classification with ill - defined categories ( Tech . Rep . No . MMPP 79 - 6 ) . Ann Arbor : University of Michigan , Michigan Mathematical Psychology Program . Gibson , E . J . ( 1953 ) . Improvement in perceptual judgments as a function of controlled practice or training . Psy - chological Review , 50 , 401 - 431 . Gibson , E . J . ( 1969 ) . Principles of perceptual learning and development . New York : Appleton . Gibson , J . J . , & Gibson , E . J . ( 1955 ) . Perceptual learning : Differentiation or enrichment ? Psychological Review , 62 , 32 - 41 . Gick , M . L . , & Holyoak , K . J . ( 1983 ) . Schema induction and analogical transfer . Cognitive Psychology , 15 , 1 - 38 . Hartley , J . , & Homa , D . ( 1981 ) . Abstraction of stylistic concepts . Journal of Experimental Psychology : Human Learning and Memory , 7 , 33 - 46 . Hayes - Roth , B . , & Hayes - Roth , F . ( 1977 ) . Concept learn - ing and the recognition and classification of exemplars . Journal of Verbal Learning and Verbal Behavior , 16 , 321 - 338 . Homa , D . , Sterling , S . , & Trepel , L . ( 1981 ) . Limitations of exemplar - based generalization and the abstraction of categorical information . Journal of Experimental Psy - chology : Human Learning and Memory , 7 , 418 - 439 . Medin , D . C . , & Schaffer , M . M . ( 1978 ) . Context theory of classification learning . Psychological Review , 85 , 207 - 238 . Neumann , P . G . ( 1977 ) . Visual prototype information with discontinuous representation of dimensions of variability . Memory & Cognition , 5 , 187 - 197 . Oldfield , R . C . ( 1954 ) . Memory mechanisms and the theory of schemata . British Journal of Psychology , 45 , 14 - 23 . Patterson , J . F . ( 1979 ) . Hypothesis testing in a prototype abstraction task . Unpublished doctoral dissertation , University of Michigan . Peterson , M . J . , Meagher , R . B . , Chait , H . , & Gillie , S . ( 1973 ) . The abstraction and generalization of dot pat - terns . Cognitive Psychology , 4 , 378 - 398 . Pitz , G . F . , Leung , L . S . , Hamilos , C . , & Terpening , W . ( 1976 ) . The use of probabilistic information in making predictions . Organizational Behavior and Human Per - formance , 17 , 1 - 18 . Posner , M . I . , & Keele , S . W . ( 1968 ) . On the genesis of abstract ideas . Journal of Experimental Psychology , 77 , 353 - 363 . Raiffa , H . , & Schlaifer , R . ( 1961 ) . Applied statistical de - cision theory . Cambridge , MA : Graduate School of Business Administration of Harvard University . Reber , A . S . , & Allen , R . ( 1978 ) . Analogic and abstraction strategies in synthetic grammar learning : A functionalist interpretation . Cognition , 6 , 189 - 221 . Reed , S . K . ( 1972 ) . Pattern recognition and categorization . Cognitive Psychology , 3 , 383 - 407 . Rosch , E . ( 1973 ) . On the internal structure of perceptual and semantic categories . In T . E . Moore ( Ed . ) , Cognitive development and the acquisition of language ( pp . 111 - 144 ) . New York : Academic Press . Rosch , E . ( 1978 ) . Principles of categorization . In E . Rosch & B . B . Lloyd ( Eds . ) , Cognition and categorization ( pp . 28 - 46 ) . New York : Wiley . Rosch , E . , & Mervis , C . B , ( 1975 ) . Family resemblances : Studies on the internal structure of categories . Cognitive Psychology , 7 , 573 - 605 . Rosch , E . , Mervis , C . B . , Gray , W . D . , Johnson , D . M . , & Boyes - Braem , P . ( 1976 ) . Basic objects in natural cat - egories . Cognitive Psychology , 8 , 382 - 439 . Swets , J . A . , Tanner , W . P . , Jr . , & Birdsall , T . G . ( 1961 ) . Decision processes in perception . Psychological Review , 68 , 301 - 340 . Tracy , J . F . , & Evans , S . H . ( 1967 ) . Supplementary in - formation in schematic concept formation . Psychonomic Science , 9 , 313 - 314 . Wallsten , T , S . ( 1976 ) . Using conjoint - measurement models to investigate a theory about probabilistic information processing . Journal of Mathematical Psychology , 14 , 144 - 185 . Received April 30 , 1982 Revision received May 16 , 1983