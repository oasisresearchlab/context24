Scatter / Gather : A Cluster - based Approach to Browsing Large Document Collections Douglass R . Cutting 1 David R . Karger 1 ; 2 Jan O . Pedersen 1 John W . Tukey 1 ; 3 Abstract Document clustering has not been well received as an in - formation retrieval tool . Objections to its use fall into two main categories : (cid:12)rst , that clustering is too slow for large corpora ( with running time often quadratic in the number of documents ) ; and second , that clustering does not appreciably improve retrieval . We argue that these problems arise only when cluster - ing is used in an attempt to improve conventional search techniques . However , looking at clustering as an informa - tion access tool in its own right obviates these objections , and provides a powerful new access paradigm . We present a document browsing technique that employs document clustering as its primary operation . We also present fast ( linear time ) clustering algorithms which support this in - teractive browsing paradigm . 1 Introduction Document clustering has been extensively investigated as a methodology for improving document search and re - trieval ( see [ 15 ] for an excellent review ) . The general as - sumption is that mutually similar documents will tend to be relevant to the same queries , and , hence , that au - tomatic determination of groups of such documents can improve recall by e(cid:11)ectively broadening a search request ( see [ 11 ] for a discussion of the cluster hypothesis ) . Typ - ically a (cid:12)xed corpus of documents is clustered either into an exhaustive partition , disjoint or otherwise , or into a hierarchical tree structure ( see , for example , [ 8 , 13 , 2 ] ) . In the case of a partition , queries are matched against clusters and the contents of the best scoring clusters are returned as a result , possibly sorted by score . In the case 1 Xerox Palo Alto Research Center 3333 Coyote Hill Road , Palo Alto , CA 94304 2 Stanford University 3 Princeton University Permission to copy without fee all or part of this material is granted provided that the copies are not made or dis - tributed for direct commercial advantage , the ACM copy - right notice and the title of the publication and its date appear , and notice is given that copying is by permission of the Association for Computing Machinery . 15th Ann Int ' l SIGIR ' 92 / Denmark - 6 / 92 c(cid:13)1992 ACM 0 - 89791 - 542 - 0 / 92 / 0006 / 0318 of a hierarchy , queries are processed downward , always taking the highest scoring branch , until some stopping condition is achieved . The subtree at that point is then returned as a result . Hybrid strategies are also available . These strategies are essentially variations of near - neighbor search 1 where nearness is de(cid:12)ned in terms of the pairwise document similarity measure used to gen - erate the clustering . Indeed , cluster search techniques are typically compared to direct near - neighbor search [ 9 ] , and are evaluated in terms of precision and recall . Vari - ous studies indicate that cluster search strategies are not markedly superior to near - neighbor search , and , in some situations , can be inferior ( see , for example , [ 6 , 12 , 4 ] ) . Furthermore , document clustering algorithms are often slow , with quadratic running times . It is therefore un - surprising that cluster search , with its indi(cid:11)erent perfor - mance , has not gained wide popularity . Document clustering has also been studied as a method for accelerating near - neighbor search , but the develop - ment of fast algorithms for near - neighbor search has de - creased interest in that possibility [ 1 ] . In this paper , we take a new approach to document clustering . Rather than dismissing document clustering as a poor tool for enhancing near - neighbor search , we ask how clustering can be e(cid:11)ective as an access method in its own right . We describe a document browsing method , called Scatter / Gather , which uses document clustering as its primitive operation . This technique is directed towards information access with non - speci(cid:12)c goals and serves as a complement to more focused techniques . To implement Scatter / Gather , fast document cluster - ing is a necessity . We introduce two new near linear time clustering algorithms which experimentation has shown to be e(cid:11)ective , and also discuss reasons for their e(cid:11)ec - tiveness . 1 . 1 Browsing vs Search The standard formulationof the information access prob - lem presumes a query , the user ' s expression of an infor - mation need . The task is then to search a corpus for doc - uments that match this need . However , it is not di(cid:14)cult to imagine a situation in which it is hard , if not impossi - ble , to formulate such a query precisely . For example , the 1 Also known as \ vector space " or \ similarity " search . user may not be familiar with the vocabulary appropri - ate for describing a topic of interest , or may not wish to commit himself to a particular choice of words . Indeed , the user may not be looking for anything speci(cid:12)c at all , but rather may wish to discover the general information content of the corpus . Access to a document collection in fact covers an entire spectrum : at one end is a narrowly speci(cid:12)ed search for a particular document , given some - thing as speci(cid:12)c as its title ; at the other end is a browsing session with no well de(cid:12)ned goal , satisfying a need to learn more about the document collection . It is common for a session to move across the spectrum , from browsing to search : the user starts with a partially de(cid:12)ned goal which is re(cid:12)ned as he (cid:12)nds out more about the document collection . Standard information access techniques tend to emphasize the search end of the spectrum . A glaring example of this emphasis is cluster search , where clus - tering , a technology capable of topic extraction , is sub - merged from view and used only to assist near - neighbor search . We propose an alternative application for clustering in informationaccess , taking our inspiration from the access methods typically provided with a conventional textbook . If one has a speci(cid:12)c question in mind , and speci(cid:12)c terms which de(cid:12)ne that question , one consults the index , which directs one to passages of interest . However , if one is simply interested in gaining an overview , or has a gen - eral question , one peruses the table of contents , which lays out the logical structure of the text . The table of contents gives a sense of what sort of questions might be answered by a more intensive examinationof the text , and may also lead to speci(cid:12)c sections of interest . One can eas - ily alternate between browsing the table of contents and searching the index . By direct analogy , we propose an information access system with two components : our browsing method , Scatter / Gather , which uses a cluster - based , dynamic table - of - contents metaphor for navigating a collection of documents ; and one or more word - based , directed , text search methods , such as near - neighbor search or snippet search [ 7 ] . The browsing component describes groups of similar documents , one or more of which can be selected for further examination . This can be iterated until the user is directly viewing individual documents . Based on documents found in this process , or on terms used to de - scribe document groups , the user may , at any time , switch to a more focused search method . In particular , we antic - ipate that the browsing tool will not necessarily be used to (cid:12)nd particular documents , but may instead help the user formulate a search request , which will then be ser - viced by some other means . Scatter / Gather may also be used to organize the results of word - based queries that retrieve too many documents . 2 Scatter / Gather Browsing In the basic iteration of the proposed browsing method , the user is presented with short summaries of a small number of document groups . Initially the system scatters the collection into a small number of document groups , or clusters , and presents short summaries of them to the user . Based on these summaries , the user selects one or more of the groups for further study . The selected groups are gathered together to form a subcollection . The system then applies clus - tering again to scatter the new subcollection into a small number of document groups , which are again presented to the user . With each successive iteration the groups become smaller , and therefore more detailed . Ultimately , when the groups become small enough , this process bot - toms out by enumerating individual documents . 2 . 1 An Illustration We now describe a Scatter / Gather session , where the text collection consists of about 5000 articles posted to the New York Times News Service during the month of Au - gust 1990 . This session is summarized in (cid:12)gure 1 . Here , to simplify the (cid:12)gure , we manually assigned single - word labels based on the full cluster descriptions . The full ses - sion is provided as Appendix A . Suppose the user wants to (cid:12)nd out what happened that month . Several issues prevent the application of conven - tional search techniques : (cid:15) The information need is too vague to be described as a single topic . (cid:15) Even if a topic were available , the words used to de - scribe it may not be known to the user . (cid:15) The words used to describe a topic may not be those used to discuss the topic and may thus fail to ap - pear in articles of interest . For example , articles concerning international events need never use the words \ international event " . (cid:15) Even if some words used in discussion of the topic were available , documents may fail to use precisely those words , e . g . , synonyms may be used instead . With Scatter / Gather , rather than being forced to pro - vide terms , the user is presented with a set of clusters , an outline of the corpus . She need only select those clusters which seem potentially relevant to the topic of interest . In the example , the big stories of the month are imme - diately obvious from the initial scattering : Iraq invades Kuwait , and Germany considers reuni(cid:12)cation . This leads the user to focus on international issues : she selects the ` Kuwait ' and ` Germany ' and ` Oil ' clusters . These three clusters are gathered together . 2 New York Times News Service , August 1990 Education Domestic Iraq Arts Sports Oil Germany Legal Deployment Politics Germany Pakistan Africa Markets Oil Hostages Trinidad W . Africa S . Africa International Lebanon Pakistan Scatter International Stories Gather Scatter Smaller International Stories Gather Scatter Security Japan Figure 1 : Illustration of Scatter / Gather This reduced corpus is then reclustered on the (cid:13)y to produce eight new clusters covering the reduced corpus . Since the reduced corpus contains a subset of the articles , these new clusters reveal a (cid:12)ner level of detail than the original eight . The articles on the Iraqi invasion and some of the ` Oil ' articles have now been separated into clusters discussing the U . S . military deployment , the e(cid:11)ects of the invasion upon the oil market , and one which the user deduces is about hostages in Kuwait . The user feels her understanding of these large stories is adequate , but wishes to (cid:12)nd out what happened in other corners of the world . She selects the ` Pakistan ' cluster , which also contains other foreign political stories , and a cluster containing articles about Africa . This reveals a number of speci(cid:12)c international situations as well as a small collection of miscellaneous international articles . The user thus learns of a coup in Pakistan , and about hostages being taken in Trinidad , stories otherwise lost among the major stories of that month . 2 . 2 Requirements Scatter / Gather depends on the existence of two facili - ties . First , since clustering and reclustering is an essential part of the basic iteration , we need an algorithm which can appropriately cluster a large number of documents within a time tolerable for user interaction ( e . g . , less than a minute ) . Second , given a group of documents , some method for automatically summarizing that group must be speci(cid:12)ed . This cluster description must be su(cid:14)ciently revealing for to give the user a sense of the topic de(cid:12)ned by the group , yet short enough for many descriptions to be appreciated simultaneously . We will present two algorithms that meet the (cid:12)rst requirement . Buckshot is a fast clustering algorithm suitable for the online reclustering essential for Scat - ter / Gather . Fractionation is another , more careful , clus - tering algorithm whose greater accuracy makes it suit - able for the static o(cid:15)ine partitioning of the entire corpus which is presented (cid:12)rst to the user . We also de(cid:12)ne the cluster digest , an easy to generate , concise description of a cluster suitable for Scatter / Gather . 3 Document Clustering Before presenting our document clustering algorithms , we review the terminologyestablished in prior work , and dis - cuss why existing clustering algorithms fail to meet our needs . Throughout this paper , n denotes the number of documents in the collection , and k denotes the desired number of clusters . In order to cluster documents one must (cid:12)rst establish a pairwise measure of document similarity and then de(cid:12)ne a method to partition the collection into clusters of simi - lar documents . Numerous document similarity measures have been proposed , all of which treat each document as a 3 set of words , often with frequency information , and mea - sure the degree of word overlap between documents [ 11 ] . The documents are typically represented by sparse vec - tors of length equal to the number of unique words ( or types ) in the corpus . Each component of the vector has a value re(cid:13)ecting the occurrence of the corresponding word in the document . We may use a binary scale , in which the value is one or zero , to represent the presence or the absence of the word , or the value might be some function of the word ' s frequency within that document . If a word does not occur in a document , its value is zero . A popu - lar similarity measure , the cosine measure , computes the cosine of the angle between these two sparse vectors . If both document vectors are normalized to unit length , the cosine is , of course , simply the inner product of the two vectors . Other measures include the Dice and Jaccard coe(cid:14)cients , which are normalized word overlap counts . Willett [ 15 ] has suggested that that the choice of simi - larity measure has less qualitative impact on clustering results than the choice of clustering algorithm . Two di(cid:11)erent types of document clusters can be con - structed . One is a (cid:13)at partition of the documents into a collection of subsets . The other is a hierarchical clus - ter , which can be de(cid:12)ned recursively as either an indi - vidual document or a partition of the corpus into sets , each of which is then hierarchically clustered . A hierar - chical clustering de(cid:12)nes a tree , called a dendrogram , on the documents . Numerous clustering algorithms have been applied to build hierarchical document clusters , including , most prominently , single - linkage hierarchical clustering [ 5 , 2 ] . These algorithms generally proceed by iteratively consid - ering all pairs of clusters built so far , and fusing the pair which exhibits the greatest similarity into a single docu - ment group ( which then becomes a node of the dendro - gram ) . They di(cid:11)er in the procedure used to compute sim - ilarity when one of the pair is the product of a previous fusion . Single - linkage clustering de(cid:12)nes the similarity as the maximumsimilaritybetween any two individuals , one from each of the two groups . Alternative methods con - sider the minimum similarity ( complete - linkage ) , the av - erage similarity ( group - average linkage ) , as well as other aggregate measures . Although single - linkage clustering is known to have an undesirable chaining behavior , typically forming elongated straggly clusters , it remains popular due to its simplicity and the availability of an optimal space and time algorithm for its computation [ 10 ] . These algorithmsshare certain commoncharacteristics . They are agglomerative , in that they proceed by itera - tively choosing two document groups to agglomerate into a single document group . They agglomerate in a greedy manner , in that the pair of document groups chosen for agglomerationis the pair which is considered best or most similar under some criterion . Lastly , they are global in that all pairs of inter - group similarities are considered in the course of selecting an agglomeration . Global algo - rithms have running times which are intrinsically (cid:10) ( n 2 ) , 2 because all pairs of similarities must be considered . This sharply limits their usefulness , even given algorithmsthat attain the theoretical quadratic lower bound on perfor - mance . Partitional strategies , those that strive for a (cid:13)at decom - position of the collection into sets of documents rather than a hierarchy of nested partitions , have also been studied [ 8 , 13 ] . Some of these algorithms are global in nature and thus have the same slow performance as the above mentioned greedy , global , agglomerative algo - rithms . Other partitional algorithms , by contrast , typi - cally have rectangular running times , i . e . , O ( kn ) . Gener - ally , these algorithms proceed by choosing , in some man - ner , a number of seeds equal to the desired size ( number of sets ) of the (cid:12)nal partition . Each document in the collec - tion is then assigned to the closest seed . As a re(cid:12)nement , the procedure can be iterated , with , at each stage , an im - proved selection of cluster seeds . It is noteworthy that any partitional clustering algorithm can be transformed into a hierarchical clustering algorithmby recursively par - titioning each of the clusters found in an application of the partitioning algorithm . One application of a partitional clustering has been to improve the performance of near - neighbor search by in - cluding , with each document , some closely related doc - uments that might otherwise be missed . However , to be useful for near - neighbor search , the partition must be fairly (cid:12)ne , since it is desirable for each set to only contain a few documents . For example , Willett generates a parti - tion who size is related to the number of unique words in the document collection [ 13 ] . From this perspective , the potential computational bene(cid:12)ts of a seed - based strat - egy are largely obviated by the large size ( relative to the number of documents ) of the required partition . For this reason partitional strategies have not been aggressively pursued by the information retrieval community . We present two partitioning algorithmswhich use tech - niques drawn from the hierarchical algorithms , but which acheive rectangular time bounds . For our application , the number of clusters desired is small and thus the speedup over quadratic time algorithms is substantial . 2 Willett [ 14 ] discusses an inverted (cid:12)le approach which can ame - lioratethisquadraticbehaviorwhenalargenumberofsmallclusters are desired . Unfortunately , when clusters are large enough to con - tain a large proportion of the terms in the corpus , this approach yields less improvement . 4 4 De(cid:12)nitions For each document (cid:11) in a collection ( or corpus ) C , let the count(cid:12)le c ( (cid:11) ) be the set of words , with their frequencies , that occur in that document . 3 Let V be the set of unique words occurring in C . Then c ( (cid:11) ) can be represented as a vector of length jV j ; c ( (cid:11) ) = ff ( wi ; (cid:11) ) g jVj i = 1 where wi is the ith word in V and f ( wi ; (cid:11) ) is the frequency of wi in (cid:11) . To measure the similarity between pairs of documents , (cid:11) and (cid:12) , let us employ the cosine between monotone element - wise functions of c ( (cid:11) ) and c ( (cid:12) ) . In particular , let s ( (cid:11) ; (cid:12) ) = hg ( c ( (cid:11) ) ) ; g ( c ( (cid:12) ) ) i kg ( c ( (cid:11) ) ) kkg ( c ( (cid:12) ) ) k where g is a monotone damping function , \ h(cid:1) ; (cid:1)i " denotes inner product , and k (cid:1) k denotes vector norm . It has been our experience that taking g to be component - wise square - root produces better results than the traditional component - wise logarithm . It is useful to consider similarity to be a function of document pro(cid:12)les p ( (cid:11) ) , where p ( (cid:11) ) = g ( c ( (cid:11) ) ) kg ( c ( (cid:11) ) ) k ; in which case s ( (cid:11) ; (cid:12) ) = hp ( (cid:11) ) ; p ( (cid:12) ) i = jVj X i = 1 p ( (cid:11) ) ip ( (cid:12) ) i : Suppose (cid:0) is a set of documents , or a document group . A simple pro(cid:12)le can be associated with (cid:0) by de(cid:12)ning it to be the normalized sum of pro(cid:12)les of the contained in - dividuals . Let ^ p ( (cid:0) ) = X (cid:11)2(cid:0) p ( (cid:11) ) be the unnormalized sum pro(cid:12)le , and then p ( (cid:0) ) = ^ p ( (cid:0) ) k ^ p ( (cid:0) ) k : Similarly , the cosine measure can be extended to (cid:0) by employing this pro(cid:12)le de(cid:12)nition : s ( (cid:0) ; x ) = hp ( (cid:0) ) ; p ( x ) i : Sometimesfor our purposes , the normalizedsum pro(cid:12)le is not a good measure of a document group ' s \ contents " because it takes into account documents which lie on the 3 Throughoutthis paper , lower case Greek letters will be used to denote individual documents . Upper case Greek letters will denote setsof documents ( documentgroups ) anduppercase Romanletters will denote sets of documentgroups . outskirts of the group . To solve this problem , we de - (cid:12)ne the trimmed sum pro(cid:12)le pm ( (cid:0) ) for any cluster (cid:0) by considering only the m \ most central " documents of the cluster . For every (cid:11) in (cid:0) let rm ( (cid:0) ) be the m documents (cid:11) whose similarity to (cid:0) , namely s ( (cid:11) ; (cid:0) ) , is largest . Then de(cid:12)ne ^ pm ( (cid:0) ) = X (cid:11)2rm ( (cid:0) ) p ( (cid:11) ) : and pm ( (cid:0) ) = ^ pm ( (cid:0) ) = k ^ pm ( (cid:0) ) k : This computation can be completed in time proportional to j(cid:0)j . 4 The trimmingparameter m maybe de(cid:12)ned adap - tively as some percentage of j(cid:0)j , or may be (cid:12)xed . 4 . 1 Cluster Digest Another description of a document group is in some sense dual to the trimmed sum pro(cid:12)le . Rather than considering the central documents of a cluster , we can consider the central words , namelythose which appear most frequently in the group as a whole . We thus de(cid:12)ne tw ( (cid:0) ) , the topical words of (cid:0) , to be the w highest weighted terms in p ( (cid:0) ) ( or perhaps in pm ( (cid:0) ) ) . Taken together , the two sets ( rm ( (cid:0) ) ; tw ( (cid:0) ) ) form the ( m ; w ) cluster digest of (cid:0) , a short description of the con - tents of the cluster . The cluster digest can easily be com - puted in time O ( j(cid:0)j + jV j ) , and is in fact the summary used to describe a cluster to a user of Scatter / Gather . 5 Partitional Clustering Seed - based partitional clustering algorithms have three phases : 1 Find k centers . 2 Assign each document in the collection to a center . 3 Re(cid:12)ne the partition so constructed . The result is a set P of k disjoint document groups such that S (cid:5)2P (cid:5) = C . The Buckshot and Fractionation algorithms are both designed to (cid:12)nd the initial centers . They can be thought of as rough clustering algorithms , however their output is only used to de(cid:12)ne centers . Both algorithms assume the existence of some algorithm which clusters well , but which may run slowly . Let us call this procedure the cluster subroutine . We use group average agglomerative clustering for this subroutine ( see appendix B ) . Each of our algorithms uses this cluster subroutine locally over small sets , and builds on its results to (cid:12)nd the k centers . 4 A full sort of the similarities is not required . 5 Buckshot applies the cluster subroutine to a random sample to (cid:12)nd centers . Fractionation uses successive ap - plication of the cluster subroutine over (cid:12)xed sized groups to (cid:12)nd centers . We believe that Fractionation is the more accurate center (cid:12)nding procedure . However , Buckshot is signi(cid:12)cantly faster , and , hence , is more appropriate for the on - the - (cid:13)y online reclustering required by iterations of Scatter / Gather . Fractionation can be used to establish the primary partitioning of the entire corpus , which is displayed in the (cid:12)rst iteration of Scatter / Gather . We implement Step 2 by assigning each document to the \ nearest " center ( in a sense to be de(cid:12)ned later ) . Our re(cid:12)nement algorithms also re(cid:13)ect a time - accuracy tradeo(cid:11) . The simplest re(cid:12)nement procedure , iterated move - to - nearest , is fast but limited . A more comprehen - sive re(cid:12)nement is achieved through repeated application of procedures that attempt to Split , Join , and clarify el - ements of the partition P . 5 . 1 Finding Initial Centers Buckshot The idea of the buckshot algorithm is quite simple . To achieve a rectangular time clustering algorithm , merely choose a small random sample of the documents ( of size p kn ) , and apply the cluster subroutine . Return the cen - ters of the clusters found . This algorithm clearly runs in time O ( kn ) . Since random sampling is employed , the Buckshot al - gorithm is not deterministic . That is , repeated calls to this algorithm on the same corpus may produce di(cid:11)er - ent partitions , although in our experience repeated trials generally produce qualitatively similar partitions . Fractionation The Fractionation algorithm (cid:12)nds k centers by initially breaking C into N = m buckets of a (cid:12)xed size m > k . The cluster subroutine is then applied to each of these buck - ets separately to agglomerate individuals into document groups such that the reduction in number ( from individ - uals to groups in each bucket ) is roughly a factor of (cid:26) . These groups are now treated as if they were individuals , and the entire process repeated . The iteration terminates when only k groups remain . Fractionation can be viewed as building a 1 = (cid:26) branching tree bottom up , where the leaves are individual documents , terminating when only k roots remain . Suppose the individuals in C are enumerated , so that C = (cid:11)1 ; (cid:11)2 ; : : : ; (cid:11)n . This ordering could re(cid:13)ect an extrin - sic ordering on C , but a better procedure sorts C based on a key which is the word index of the j th most com - mon word in each individual . Typically j is a small num - ber , such as three , which favors medium frequency terms . This procedure thus encourages nearby individuals in the corpus ordering to have at least one word in common . The initial bucketing creates a partition B = f(cid:2)1 ; (cid:2)2 ; : : : ; (cid:2)n = mg such that (cid:2)i = f(cid:11)m ( i(cid:0)1 ) + 1 ; (cid:11)m ( i(cid:0)1 ) + 2 ; : : : ; (cid:11)mig : Each (cid:2)i is then separately clustered ( using the cluster subroutine ) into (cid:26)m groups , where (cid:26) is the desired reduc - tion factor . Note that each of these computations occurs in m 2 time , and , hence , all n = m occur in nm time . Each application of agglomerative clustering produces an asso - ciated partition Ri = f(cid:8)i ; 1 ; (cid:8)i ; 2 ; : : : ; (cid:8)i ; (cid:26)mg . The union of the documents groups contained in these partitions are then treated as individuals for the next iteration . That is , de(cid:12)ne C 0 = f(cid:8)i ; j : 1 (cid:20) i (cid:20) n = m ; 1 (cid:20) j (cid:20) (cid:26)mg C 0 inherits an enumeration order by taking the (cid:8)i ; j in lexicographic order on i and j . The process is then re - peated with C 0 replacing C . That is , the (cid:26)n components of C 0 are broken into (cid:26)n = m buckets , which are further reduced to (cid:26) 2 n groups through separate agglomeration . The process terminates at iteration j if (cid:26) j n < k . At this point one (cid:12)nal application of agglomerativeclustering can reduce the remaining groups to a partition P of size k . To determine the running time , observe that the j th iteration , which operates on (cid:26) j n items , takes time (cid:26) j nm . The overall running time is thus O ( nm ( 1 + (cid:26) + (cid:26) 2 + : : : ) ) = O ( mn ) . Thus if m = O ( k ) this algorithm has rectangular running time . 5 . 2 Assigning Documents to Centers Once k centers have been found , and suitable pro(cid:12)les de - (cid:12)ned for those centers , each document in C must be as - signed to one of those centers based on some criterion . The simplest algorithm , Assign - to - Nearest , assigns each document to the nearest center . Let G be a partition of the collection into k groups , and let (cid:0)i be the ith group in G . Let (cid:11) 2 (cid:5)i if i maximizes s ( (cid:11)(cid:0)i ) . Ties can be broken by assigning (cid:11) to the group with lowest index . The set P = f(cid:5)ig ; 0 (cid:20) i (cid:20) k is then the desired partition . P can be e(cid:14)ciently computed by constructing an in - verted map for the k centers pm ( (cid:0)i ) , and for each (cid:11) 2 C simultaneouslycomputingthe similarityto allthe centers . In any case , the cost of this procedure is proportional to kn . 5 . 3 Re(cid:12)nement Given an initial clustering , it is now desirable to re(cid:12)ne it into a better one . As with our initial clustering algo - rithms , there is a tradeo(cid:11) between speed and accuracy . 6 The simplest process is simply to iterate the Assign - to - Nearest process just discussed . The Split algorithm sepa - rates poorly de(cid:12)ned clusters into two well separated parts and Join merges clusters which are too similar . Iterated Assign - to - Nearest The Assign - to - Nearest procedure mentioned above can also be seen as the (cid:12)rst of our re(cid:12)nement algorithms . From a given set of clusters , we generate cluster centers using the trimmed sum pro(cid:12)les above , and we then assign each document to the nearest center so as to form new clusters . This process can be iterated inde(cid:12)nitely , though it makes its greatest gains in the (cid:12)rst few steps , and hence is typically iterated only a small (cid:12)xed number of times . 5 Split Split divides each document group (cid:0) in a partition P into two new groups . This can be accomplished by applying Buckshot clustering ( without re(cid:12)nement ) with C = (cid:0) and k = 2 . The resulting Buckshot partition G provides the two new groups . Let P = f(cid:0)1 ; (cid:0)2 ; : : : ; (cid:0)kg and let Gi = f(cid:0)i ; 1 ; (cid:0)i ; 2g be a two element Buckshot partition of (cid:0)i . The new partition P 0 is simply the union of the Gi ' s : P 0 = k [ i = 1 Gi : Each application of Buckshot requires time proportional to j(cid:0)ij . Hence , the overall computation can be performed in time proportional to N . A modi(cid:12)cation of this procedure would only split groups that score poorly on some coherency criterion . One simple criterion is the cluster self similarity s ( (cid:0) ; (cid:0) ) . This quantity is in fact proportional to the average simi - larity between documents in the cluster , as well as to the average similarity of a document to the cluster centroid . We thus de(cid:12)ne : A ( (cid:0) ) = s ( (cid:0) ; (cid:0) ) : Let r ( (cid:0)i ; P ) be the rank of A ( (cid:0)i ) in the set fA ( (cid:0)1 ) ; A ( (cid:0)2 ) ; : : : ; A ( (cid:0)k ) g : The procedure would then only split groups such that r ( (cid:0) ; P ) < (cid:26)k for some (cid:26) ; 0 < (cid:26) (cid:20) 1 . This modi(cid:12)cation does not change the order of the algorithm since the co - herence criterion can be computed in time proportional to N . 5 Excessiveiterationmayin factworsenthe partitionratherthan improving it , since \ fuzzy " elongated clusters can pull documents away from other clusters and become even fuzzier . Join The purpose of the Join re(cid:12)nement operator is to merge document groups in a partition P that are not usefully distinguished by their cluster digests . Since , by de(cid:12)nition , any two elements of P are disjoint , they will never have \ typical " documents in common . However , their lists of \ topical " words may well overlap . Therefore the criterion of distinguishability between two groups (cid:0) and (cid:1) will be T ( (cid:0) ; (cid:1) ) = jtw ( (cid:0) ) \ tw ( (cid:1) ) j where tw ( (cid:0) ) is the list of w most topical words for (cid:0) . We merge (cid:0) and (cid:1) if T ( (cid:0) ; (cid:1) ) > p , for some p ; 0 < p (cid:20) w . Determining the topical words for each cluster takes time proportional to the number of words in the cor - pus , and we must then compute k 2 intersections to decide which clusters to merge . In large corpora , the number of words is typically less than the number of documents , and the running time of Join is thus O ( kn ) . 6 Application to Scatter / Gather Combinations of the various initial clustering and re(cid:12)ne - ment procedures give several possible complete clustering algorithms . We have used two of these combinations in the course of implementing the Scatter / Gather method . The initial partition used in Scatter / Gather is com - pletely determined by the corpus under consideration . Hence , when the corpus is available in advance , one can compute the initial partition o(cid:15)ine . We can therefore use a slower clustering algorithm to improve the accuracy of the initial partition . However , for corpora consisting of tens of thousands of documents , a quadratic time algo - rithm is likely to be too slow even for o(cid:15)ine computation . We thus use the Fractionation algorithm to (cid:12)nd centers , and then perform a great deal of re(cid:12)nement using the Split , Join , and Assign - to - Nearest operators . Note that the running time for each of the re(cid:12)nement procedures is O ( kN ) and thus does not a(cid:11)ect the overall running time . In an interactive session , however , it is vital for the clus - tering algorithm to run as quickly as possible , even at the expense of some accuracy . We therefore use the Buck - shot center (cid:12)nding procedure , and then follow it with a bare minimum of re(cid:12)nement . We have found that two iterations of the Assign - to - Nearest procedure yield a rea - sonably accurate clustering , and that further re(cid:12)nement produces additional improvement , but with quickly di - minishing returns . By virtue of the Buckshot center (cid:12)nding procedure this algorithm is not deterministic . However , in the contem - plated application , Scatter / Gather , it is more important that the partition be computed at high speed than that the algorithm be deterministic . Indeed , the lack of deter - minism might be interpreted as a feature , since the user then has the option of discarding an unrevealing partition in favor of a fresh reclustering . 7 The overall complexity of both clustering procedures described in this section is clearly O ( kN ) . The constant factor for the Buckshot - based procedure is small enough to permit interactive use with large document collections . The Fractionation - basedprocedure has a somewhatlarger constant factor , but one which is still acceptable for o(cid:15)ine applications . 6 . 1 Naturally Clustered Data It is worth examining the performance of our algorithms when the data set consists of well separated clusters of points . If the input data has k natural clusters , i . e . , the smallest intra - cluster document similarity is larger than the largest inter - cluster document similarity , then both of our algorithms will (cid:12)nd this partition . For Buckshot , if we have a corpus containing k widely separated and equal size centers , then a random sample of size p kn will select some documents from each of the centers with high probability so long as n (cid:29) klnk . This will certainly be true for our case in which k = 20 or so . To see this , compute the probability that , if we choose a sample of size s , we fail to get any individual from some cluster . This is at most k times the probability that none of our s individuals is a member of cluster i , namely ( 1 (cid:0) 1 = k ) s . So , the total probability of failure is at most k ( 1 (cid:0) 1 = k ) s . If we now take s = aklnk for some a , then the failure probability is at most k ( 1 (cid:0) 1 = k ) aklnk (cid:20) k 1(cid:0)a : Thus in our case , with k = 20 , taking a = 5 means that 400 samples (cid:12)nd all the clusters 999 times in 1000 . Given that we start with at least one element from each cluster , our resulting clusters will each be a subset of one of the clusters . Thus the set of centers found will include a center within each actual cluster . For Fractionation , we need merely note that if we have more than k documents in a single bucket , some pair of them is necessarily in the same actual cluster . Then clearly , this pair will be merged in preference to any other pair . Therefore , no pair of documents not in the same cluster will ever be merged . Thus , when we (cid:12)nish , each cluster we have found will be a subset of some one of the actual clusters . 7 Conclusion Scatter / Gather demonstrates that document clustering can be an e(cid:11)ective informationaccess tool in its own right . The table - of - contents metaphor gives the method an in - tuitive basis , and experience has shown that it is indeed easy to use . Scatter / Gather is particularly helpful in sit - uations in which it is di(cid:14)cult or undesirable to specify a query formally . Claims of improved performance must await evaluation metrics appropriate to the vaguely de - (cid:12)ned information access goals in which Scatter / Gather excels . To support Scatter / Gather , fast clustering algorithms are essential . Clustering can be done quickly by working in a local manner on small groups of documents rather than trying to deal with the entire corpus globally . For extremely large corpora , even the linear time clus - tering achieved by the Buckshot or Fractionation algo - rithms may be too slow . We are working to develop vari - ations on Scatter / Gather which will scale to arbitrarily large corpora , under the assumption that linear time pre - processing will always be feasible . Clearly , the accuracy of the Buckshot and Fractiona - tion algorithms is a(cid:11)ected by the quality of the clustering provided by the slow cluster subroutine . This provides further motivation to (cid:12)nd highly accurate clustering al - gorithms , whatever their running time may be . A A Scatter / Gather Session In (cid:12)gures 2 through 5 , we present the full output of the Scatter / Gather session described in section 2 . 1 . The cor - pus is the set of articles distributed by the New York Times News Service during the month of August 1990 . This consists of roughly 30 megabytes of ASCII text in about 5000 articles . Some articles are repeated due to updates of news stories . Here our goal is to learn about international political events during this month . To create the initial parti - tion we ' ve applied the Buckshot clustering algorithm ( (cid:12)g - ure 2 ) . Fractionation is recommended for this task , time permitting . Each cluster is described with the two line display of its cluster digest . The (cid:12)rst line contains the number of the cluster , the number of documents in the cluster , and titles of documents near the centroid . The second line contains words frequent in the cluster . We select clusters 2 ( Iraq ' s invasion of Kuwait ) , 5 ( Mar - kets , including oil ) and 6 ( Germany , and probably other international issues ) as those which seem likely to contain articles of interest , recluster , and display a new cluster di - gest ( (cid:12)gure 3 ) . Next , in (cid:12)gure 4 , we iterate , this time selecting clusters 3 ( Pakistan , and probably other international issues ) and 4 ( African issues ) . Speci(cid:12)c incidents have been separated out . We (cid:12)nd hostages in Trinidad , war in Liberia , police action in South Africa , and so on . We obtain more detail about the situation in Liberia by viewing the titles of the articles contained in that cluster ( (cid:12)gure 5 ) . 8 > ( time ( setq first ( outline ( all - docs tdb ) ) ) ) cluster 4970 items global cluster 199 items . . . sizes : 18 24 53 5 25 47 13 14 move to nearest . . . sizes : 517 1293 835 86 677 1020 273 269 move to nearest . . . sizes : 287 1731 749 275 481 844 310 293 0 ( 287 ) CRITICS URGE NEW METHODS ; PROGRAMS FOR PARENTS THE ; TEACHING SUBJECTS T school , year , student , child , university , state , program , percent , study , educ 1 ( 1731 ) FEDERAL WORK PROGRAMS HE ; RESORT TAKES STEPS TO PR ; AMERICANS CUT BACK year , state , york , city , million , day , service , company , week , official , house 2 ( 749 ) PENTAGON SAYS 60 , 000 IRA ; BUSH ` ` DRAWS A LINE ' ' IN ; BUSH SAYS FOREIGNER iraq , iraqi , kuwait , american , state , unite , saudi , official , military , presid 3 ( 275 ) Trillin ' s Many Hats ; New Musical from the cre ; After Nasty Teen - Agers i film , year , music , play , company , movie , art , angeles , york , american , directo 4 ( 481 ) TWISTS AND TURNS MAY MEA ; SAX LOOKING FOR RELIEF I ; PAINTING THE DODGER game , year , play , team , season , win , player , day , league , hit , right , coach , l 5 ( 844 ) CRISIS PUSHES OIL PRICES ; WHY MAJOR PANIC OVER A M ; OIL PRICES RISE AS price , oil , percent , market , company , year , million , stock , day , rate , week , s 6 ( 310 ) LEADERS OF TWO GERMANYS ; REPRESENTATIVES OF TWO G ; SECURITY COUNCIL RE government , year , state , party , political , country , official , leader , presiden 7 ( 293 ) U . S . APPEALS ORDER FREEI ; DID JUDGE MOVE TOO HASTI ; MAYOR BARRY CONVICT case , court , charge , year , judge , lawyer , attorney , trial , jury , federal , dist real time 131258 msec Figure 2 : Initial Scattering > ( time ( setq second ( outline first 2 5 6 ) ) ) cluster 1903 items global cluster 123 items . . . sizes : 51 8 5 5 4 7 28 15 move to nearest . . . sizes : 730 67 65 62 56 99 714 110 move to nearest . . . sizes : 650 66 57 117 59 242 586 126 0 ( 650 ) PENTAGON SAYS 60 , 000 IRA ; BUSH SAYS FOREIGNERS DET ; BUSH ` ` DRAWS A LINE iraq , iraqi , american , kuwait , state , unite , military , official , president , sa 1 ( 66 ) LEGISLATIVE LEADERS BACK ; THE PROBLEM WITH AN EARL ; ROAD STILL TOUGH FOR party , state , election , year , political , candidate , vote , campaign , democratic 2 ( 57 ) IN PUSH FOR UNIFICATION , ; IN PUSH FOR UNIFICATION , ; LEADERS OF TWO GERMA german , east , germany , west , year , government , soviet , union , state , unificati 3 ( 117 ) BHUTTO GOVERNMENT DISMIS ; IN FRACTIOUS PAKISTAN , G ; PAKISTANIS FEEL LET government , minister , year , party , political , military , country , official , sta 4 ( 59 ) DEATH TOLL EXCEEDS 500 I ; DE KLERK , MANDELA HOLD U ; NEGOTIATIONS TO SETT african , government , south , leader , police , national , fight , group , official , 5 ( 242 ) WEST GERMANS TO BUY FIRE ; FIRST EXECUTIVE CORP . EA ; FARM BANK , MERRILL company , million , percent , share , year , corp , stock , market , sell , price , pres 6 ( 586 ) OIL PRICES RISE AS STOCK ; MIDEAST CRISIS PUSHES OI ; WHY MAJOR PANIC OVE oil , price , percent , market , year , company , day , stock , million , rate , week , f 7 ( 126 ) IRAQ GRANTS 237 FOREIGN ; WOMAN TELLS OF 12 DAYS I ; CONCERN HEIGHTENS F kuwait , iraqi , american , iraq , saudi , day , year , invasion , country , state , ara real time 54184 msec Figure 3 : Second Scatter 9 > ( time ( setq third ( outline second 3 4 ) ) ) cluster 176 items global cluster 37 items . . . sizes : 1 4 12 1 5 3 8 3 move to nearest . . . sizes : 4 16 44 1 23 7 71 10 move to nearest . . . sizes : 5 16 28 1 51 7 55 13 0 ( 5 ) MUSLIM MILITANTS LAY DOW ; MUSLIM MILITANTS LAY DOW ; DRAMA IS OVER BUT BOO government , trinidad , minister , parliament , wednesday , bakr , hostage , robinson 1 ( 16 ) NEGOTIATIONS TO SETTLE L ; NEGOTIATIONS TO SETTLE L ; WEST AFRICAN FORCE S rebel , african , taylor , west , liberia , troop , group , liberian , leader , officia 2 ( 28 ) DEATH TOLL EXCEEDS 500 I ; DE KLERK , MANDELA HOLD U ; COMPETING FACTIONS T south , police , african , black , mandela , africa , congress , anc , political , gove 3 ( 1 ) SHIFT IN U . S . COMPUTER S ; security , agency , computer , technology , national , center , communication , milit 4 ( 51 ) SECURITY COUNCIL REACHES ; @ SECURITY COUNCIL REACHE ; NEW U . S . POLICY IS W government , year , state , official , army , country , group , guerrilla , war , natio 5 ( 7 ) CLASHES BETWEEN RIVAL SH ; MUSLIM FACTIONS BATTLE I ; BOMBINGS IN SOUTHERN lebanon , muslim , christian , al , party , kill , god , lebanese , aoun , beirut , amal 6 ( 55 ) BHUTTO GOVERNMENT DISMIS ; MS . BHUTTO CALLS HER OUS ; MS . BHUTTO CALLS HER government , minister , party , political , military , prime , pakistan , president , 7 ( 13 ) SHEVARDNADZE TO VISIT TO ; 45 YEARS AFTER WAR ' S END ; JAPAN ' S ROLE IN WORL japan , soviet , war , korean , japanese , year , tokyo , government , south , korea , w real time 11140 msec Figure 4 : Third Scatter > ( print - titles ( nth 1 third ) ) 3720 REBEL LEADER SEIZES ABOUT A DOZEN FOREIGNERS 4804 WEST AFRICAN FORCE SENT TO LIBERIA AS TALKS REMAIN DEADLOCKED 4778 WAR THREATENS TO WIDEN AS NEIGHBORING COUNTRIES TAKE SIDES 3719 REBEL LEADER AGREES TO HOLD CEASE - FIRE TALKS 3409 OUSTER OF LIBERIAN PRESIDENT NOW SEEMS INEVITABLE 3114 NEGOTIATIONS TO SETTLE LIBERIAN WAR END IN FAILURE 3113 NEGOTIATIONS TO SETTLE LIBERIAN WAR END IN FAILURE 2785 LIBERIANS IN U . S . CRITICAL OF ADMINISTRATION POLICY 2784 LIBERIANS IN U . S . CRITICAL OF ADMINISTRATION POLICY 2783 LIBERIAN REBEL LEADER CHARLES TAYLOR HURT EN ROUTE TO CEASE - FIRE 2782 LIBERIA LEADER , REJECTING TRUCE OFFER , WON ' T QUIT 1801 FIVE WEST AFRICAN NATIONS MOVING TROOPS TOWARD LIBERIA 1685 FACES OF DEATH IN LIBERIA 1684 FACES OF DEATH IN LIBERIA 248 OUSTER OF LIBERIAN PRESIDENT NOW SEEMS INEVITABLE Figure 5 : Titles of articles in topic 1 from Figure 4 10 B Group Average Clustering Here we present a quadratic time greedy global agglom - erative clustering algorithm which has given good results in our implementation . This is similar to the algorithm presented in [ 3 ] . Let (cid:0) be a document group . The average similarity between any two documents in (cid:0) is de(cid:12)ned to be S ( (cid:0) ) = 1 j(cid:0)j ( j(cid:0)j(cid:0) 1 ) X (cid:11)2(cid:0) X (cid:12)6 = (cid:11) s ( (cid:11) ; (cid:12) ) : Let G be a set of disjoint document groups . The basic iteration of group average agglomerative clustering (cid:12)nds the two di(cid:11)erent clusters (cid:0) 0 and (cid:1) 0 which maximizeS ( (cid:0) [ (cid:1) ) over all choices from G . A new , smaller , partition G 0 is then constructed by merging (cid:0) 0 with (cid:1) 0 . G 0 = ( G (cid:0) f(cid:0) 0 ; (cid:1) 0 g ) [ f(cid:0) 0 [ (cid:1) 0 g : Initially , G is simplya set of singleton groups , one for each individual to be clustered . The iteration terminates when jG 0 j = k . Note that the output from this procedure is the (cid:12)nal (cid:13)at partition G 0 , rather than a nested hierarchy of partitions , although the latter could be computed by recording each pairwise join as one level in a dendrogram . If we employ the cosine similarity measure , the in - ner maximization can be signi(cid:12)cantly accelerated . Recall that ^ p ( (cid:0) ) is the unnormalized sum pro(cid:12)le associated with (cid:0) . Then the average pairwise similarity , S ( (cid:0) ) , is simply related to the inner product , h ^ p ( (cid:0) ) ; ^ p ( (cid:0) ) i . That is , since h ^ p ( (cid:0) ) ; ^ p ( (cid:0) ) i = X (cid:11)2(cid:0) X (cid:12)2(cid:0) hp ( (cid:11) ) ; p ( (cid:12) ) i = j(cid:0)j ( j(cid:0)j(cid:0) 1 ) S ( (cid:0) ) + X (cid:11)2(cid:0) hp ( (cid:11) ) ; p ( (cid:11) ) i = j(cid:0)j ( j(cid:0)j(cid:0) 1 ) S ( (cid:0) ) + j(cid:0)j ; S ( (cid:0) ) = h ^ p ( (cid:0) ) ; ^ p ( (cid:0) ) i (cid:0) j(cid:0)j j(cid:0)j ( j(cid:0)j(cid:0) 1 ) : Similarly , for the union of two disjoint groups , (cid:3) = (cid:0) [ (cid:1) S ( (cid:3) ) = h ^ p ( (cid:3) ) ; ^ p ( (cid:3) ) i (cid:0) ( j(cid:0)j + j(cid:1)j ) ( j(cid:0)j + j(cid:1)j ) ( ( j(cid:0)j + j(cid:1)j ) (cid:0) 1 ) where h ^ p ( (cid:3) ) ; ^ p ( (cid:3) ) i = h ^ p ( (cid:0) ) ; ^ p ( (cid:0) ) i + 2h ^ p ( (cid:0) ) ; ^ p ( (cid:1) ) i + h ^ p ( (cid:1) ) ; ^ p ( (cid:1) ) i Therefore , if for every (cid:0) 2 G , S ( (cid:0) ) and ^ p ( (cid:0) ) are known , the pairwise merge that will produce the least decrease in average similarity can be cheaply updated each time a merge is performed . Further , suppose for every (cid:0) 2 G the (cid:1) were known such that S ( (cid:0) \ (cid:1) ) = max (cid:1)6 = (cid:0) S ( (cid:0) \ (cid:1) ) ; then (cid:12)nding the best pair would simply involve scanning the jGj candidates . Updating these quantities with each iteration is straightforward , since only those involving (cid:0) 0 and (cid:1) 0 need be recomputed . Using techniques such as these , it can be seen that the average time complexity for truncated group average ag - glomerative clustering is O ( n 2 ) where n is equal to the number of individuals to be clustered . 11 References [ 1 ] Chris Buckley and Alan F . Lewit . Optimizations of inverted vector searches . In Proceedings of the Eighth Annual International ACM SIGIR Confer - ence on Research and Development in Information Retrieval , pages 97 { 110 , 1985 . [ 2 ] W . B . Croft . Clustering large (cid:12)les of documents us - ing the single - link method . Journal of the American Society for Information Science , 28 : 341 { 344 , 1977 . [ 3 ] A . El - Hamdouchi and P . Willett . Hierarchical doc - ument clustering using Ward ' s method . In Proceed - ings of the Ninth International Conference on Re - search and Development in Information Retrieval , pages 149 { 156 , 1986 . [ 4 ] A . Gri(cid:14)ths , H . C . Luckhurst , and P . Willett . Using inter - document similarity information in document retrieval systems . Journal of the American Society for Information Science , 37 : 3 { 11 , 1986 . [ 5 ] Anil K . Jain and Richard C . Dubes . Algorithms for Clustering Data . Pretice Hall , Engelwood Cli(cid:11)s , N . J . 07632 , 1988 . [ 6 ] N . Jardine and C . J . van Rijsbergen . The use of hi - erarchical clustering in information retrieval . Infor - mation Storage and Retrieval , 7 : 217 { 240 , 1971 . [ 7 ] J . O . Pedersen , D . R . Cutting , and J . W . Tukey . Snippet search : a single phrase approach to text access . In Proceedings of the 1991 Joint Statisti - cal Meetings . American Statistical Association , 1991 . Also available as Xerox PARC technical report SSL - 91 - 08 . [ 8 ] G . Salton . The SMART Retrieval System . Prentice - Hall , Englewood Cli(cid:11)s , N . J . , 1971 . [ 9 ] G . Salton and M . J . McGill . Introduction to Modern Information Retrieval . McGraw - Hill , 1983 . [ 10 ] R . Sibson . SLINK : an optimally e(cid:14)cient algorithm for the single linkcluster method . Computer Journal , 16 : 30 { 34 , 1973 . [ 11 ] C . J . van Rijsbergen . Information Retrieval . Butter - worths , London , second edition , 1979 . [ 12 ] C . J . van Rijsbergen and W . B . Croft . Document clus - tering : An evaluation of some experiments with the Cran(cid:12)eld 1400 collection . Information Processing & Management , 11 : 171 { 182 , 1975 . [ 13 ] P . Willett . Document clustering using an inverted (cid:12)le approach . Journal of Information Science , 2 : 223 { 231 , 1980 . [ 14 ] P . Willett . A fast procedure for the calculation of similarity coe(cid:14)cients in automatic classi(cid:12)cation . Information Processing & Management , 17 : 53 { 60 , 1981 . [ 15 ] P . Willett . Recent trends in hierarchical document clustering : A critical review . Information Processing & Management , 24 ( 5 ) : 577 { 597 , 1988 . 12