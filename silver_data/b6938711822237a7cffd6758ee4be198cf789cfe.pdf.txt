VERIFICATION OF ALGEBRA STEP PROBLEMS : A CHRONOMETRIC STUDY OF HUMAN PROBLEM SOLVING by Paul G . Matthews and Richard C . Atkinson TECHNICAL REPORT NO . 253 May 15 , 1975 PSYCHOLOGY AND EDUCATION SERIES Reproduction in Whole or in Part is Permitted for Any Purpose of the United States Government This research was supported by National Institute of Mental Health Grant MH21747 . INSTITUTE FOR MATHEMATICAL STUDIES IN THE SOCIAL SCIENCES STANFORD UNIVERSITY STANFORD , CALIFORNIA 94305 Abstract A class of simple problem solving tasks requiring fast accurate solutions is introduced . In an experiment subjects memorised a mapping rule represented by lists of words labelled by cue words and made true / false decisions about conjunctions of propositions of the form , . ~ is in the list labelled by X , " written " X ~ Y " . Response times are analysed using a " stage modeling " technique where problem solving algorithms are composed using a small set of psychological operations that have real time characteristics specified parametrically . The theoretical analysis shows that response time performance is adequately described in terms of the sequential application of elementary psychological operations . Unexpectedly it was found that the proposition " X ~ Y and X ~ Z " was verified as quickly as the apparently simpler " X ~ " . A case is presented for the modeling technique as applied to memory and problem solving tasks in terms of theoretical parsimony , statistical simplicity , and flexibility in investigative empirical research . Suggestions are made as to possible theoretical relations among , fas , t problem solving , more complex and slower problem solving , and research in fundamental memory processes . Introduction The dominant theoretical approach to the analysis of problem solving has been to construct a formal model , often in the form of a computer program , that simulates some qualitative aspects of human problem solving performance such as the protocol sequences 09served in deriving logic theorems ( Newell & Simon , 1972 ) . In these analyses emphasis is placed on the integration of elementary information operations into a problem solving algorithm while less attention is given to the elementary operations themselves . An approach that has been relatively less well explored is to specify the processing time implications of proposed algorithms and to determine whether observed human response times ( RT ' s ) are consistent with the predicted pattern . From a statistical point of view , problems that require several minutes to solve or involve extensive searching for a solution ( e . g . , looking for the best move in a chess position , de Groot , 1965 ) might be expected to have large RT variances even for an individual subject such that it becomes impractical to model the fine details of RT . However , for simple problems where human subjects are easily able to respond 1 . correctly in a matter of a few seconds , it should be possible to verify the . processing time predictions of specific problem solving algorithms . One method for deriving RT predictions is to describe problem solving algorithms in terms of the sequential application of a set of basic psychological operations ( procedures , subroutines , or " stages " ) each of which requires real processing time and has some probability of producing an error . Leaving the details for later discussion , the theoretical RT for an algorithm applied to a particular problem can be described as the sum of the . processing times . of the operations applied and the error rate is roughly I minus the product of the correct probabilities of these operations . An alternative technique for making RT predictions is to assign computational complexity measures to the basic operations and to derive the complexity of an algorithm as the sum of the complexities of its component operations ; linearly related to computational complexity is then directly interpreted as / theoretical mean RT . This complexity assignment method yields the same description of mean RT ' s as does the corresponding stage model although it does not describe higher RT moments . Note that both methods are . easily generalized to take account of the possibility of mixed ( randomized ) strategies for applying availab Ie algorlthms • On a general theoretical level , the RT analysis of fast accurate problem solving can be a valuable s9urce of evidence in deciding on a set of basic psychological operations used in human problem solving . The case is similar to that for chronometric studies of linguistic comprehension ( Chase & Clark , 1972 ) where alternative representations of propositions can sometimes be discriminated by c9nstructing RT models 2 for processing propositions to make true / false decisions . For problem solving , theories it is desirable to build algorithms working with a set of elementary operations which have some preferred characteristics , such as corresponding to procedures or subroutines that can be conveniently written as logical units when programming in a particular language , or being general in the sense that the same set of operations can be used in solving several types of problems . Another preferred characteristic is that the set of operations has " psychological validity " insofar as real time ' pr9cessing aspects of the operations can be defined and verified in observed RT performance . 3 Algebra step problems To pursue these ~ deas in experimental task was sought where subjects would learn a set of rules ( e . g . , the moves of , piec ~ s in a board game ; or , a mapping of one set of objects into another ) and be required to solve tr ~ e / false problems by repeated application of these rules . It was thought thatca model for the single application of a rule could then be extended to a model for the entire problem solving task by specifying the way rules were applied to solve a problem . Consider a small finite set X and a rule that assigns toC each element of X a subset of X . Such a rule can he written in the form of a transition table such as that in Figure 1 which was used in an experiment to be described later . Figure 1 about here A memorised transition table , say where X is a set of consonant - vowel - consonant ( eve ) words , might be represented as " lists " in some memory store with " addresses " corresponding to the elements of X . One of the most basic propositions that can he made about a particular transition table is that xi is mapped into a list that contains xl ' written diagram , where x \ and Xj are variables standing for elements of X ; this proposition is either true or false . A subject who has memorised a transition table can be presented with the proposition X \ . - - + - X j and be required to make a true / false decision using his knowledge of theC rule as defined by the table . In the experiment to be described , subjectswere 4 Figure 1 ASP Transition Table x X T I xi . Xll X ~ X ~ x . . x , X s X s X & , X2 . X . X3 Xq . X T xi . X , + X s X \ l X ~ X3 Xs X : l X . XT XI 4a presented with logical " and " conjunctions of these simple propositions and , RT ' a for a true / false decision were measured . The propositional forms or problem types used are listed in Figure 2 in three groups ( A , B and C ) according to the geometric shapes of the mapping diagrams . Figure 2 about here A problem is true if and only if all the propositions represented by the arrows or links ' are true ; if just one link is false then the problem is false . For example , P ( - - - ) in Figure 2 is true only if Xl . ~ X . : i and Xj ~ X - . and x ~ ~ x ~ ; it is false if anyone of these propositions is false . Similarly P ( - < ) is true only if and xr ~ x ~ ; and P ( > - ) only if x , < + - x ~ and Xj - ' r - x \ < . and x . . : 7 - xJl . In the experiment subjects memorised transition tables of the form represented in Figure 1 where the elements of X were eve words , and were tested with problems of the sort illustrated in Figure 2 . Representing a transition table in memory as stored lists , an individual link , x ~ - + - Xj ' could he verified true or false by using the cue xi . . to " access " the appropriate list in memory and then " scanning " the probe xi against this list for a " match " ; if a match is obtained then the link . is true and otherwise false . A model for the verification of the conjunctive propositions could then be obtained on the assumption that verification proceeds one link at a time in some specified order . These notions are developed in the discussion section below . Since it is possible to verify mapping diagrams by checking each link in a step by step manner , the test items used in this task are referred to as algebra step problems ( ASP ) . for each problem type ASP items / were selected from a computer generated listing of all 5 Figure 2 - . ASP Problem Types problem type ( A ) PH P ( - - ) P ( - - - ) ( B ) P« ) P ( - < ) mapping diagram Xi . ~ j < " ' j x· L xl < < It X· . - + - X . L J l - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ( C ) P ( » P ( > - ) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 5a possible items given the transition table ( Figure 1 ) such that within each· problem type there were an equal number of true and false items . For false problems exactly one link was false , and for each type the false link occurred with equal frequency at each link position . In addition an effort was made to match the frequencies of occurrence of " CVC words between true and false items within each problem type so as to avoid a possible source of response bias . The total pool of about 400 distinct items was divided into four blocks ; the same P ( - ) items occuring in each block but otherwise there was no overlap . Denoting blocks by B " B ~ , B 3 , ~ ~ , subjects were tested over six sessions with one block per day in the order BtB , B3B ~ B \ B ~ , where blocks and trials within blocks were randomised for each subject individually . A set of nine CVC words was randomly assigned to the abstract transition table scheme for each subject : no two subjects had identical transition tables although all tables had the same formal structure . The experiment was run using an Imlac Corporation PDS - 1 cathode ray tube ( CRT ) display and keyboard , interfaced with a PDP - 10 computer . Six female subjects ran for seven sessions ; the first session was devoted to a transition table learning drill ( subjects did not memorise their transition tables prior to the first session ) , and the remaining six sessions were used for ASP test items . On a single trial of the drill a cue word was presented on the CRT and the subject was required to type the appropriate list in serial order ( since the eve words used had unique initial consonants , the subject typed only the first letter of each word and the computer completed the words with suitable horizontal spacing ) . On completing her response to a cue the subject pressed the keyboar ~ spacebar and the correct list was printed horizontally directly beneath . the typed response , providing feedback and an opportunity for study . Permutations of the nine cue words were run 6 and following each permutation , the subject was told her percentage of correct responses and the time taken to respond to all the cues . Subjects were required to participate in the drill until they could consistentiy achieve perfect accuracy with a response time Ullder 25 seconds ; all subjects met this criterion within 30 to 50 minutes of the drill . In the problem sessions ASP items were displayed at the center of the CRT and subjects responded true / false using two keys on the lower row of the keyboard . The subject initiated trials by pressing the spacebar following a ready signal . Items were preceded by a second duration fixation cross and appeared just to the right of the cross , remaining on the screen until the subject responded . Immediately after responding subjects received a feedback message indicating correct / error and response time . Before the first problem session subjects were shown examples of the seven problem types and told to respond " true " if and only if all the links in an item were true and to respond " falsel ! as soon as they knew that one link was false . Subjects were informed that there were an equal number of true and false items within each problem type on each day and that false items had exactly one link which was equally likely to occur in any position . On the first day of problems subjects were instructed to be completely accurate for the initial thirty or forty trials and then to increase their speed as they got a feeling for the task . For subsequent testing sessions subjects were instructed to respond as quickly they could without making more than about one error in twenty trials on average . Subjects were explicitly instructed never to guess and never to " think twice " about their response once they had made a decision . 7 To eliminate Experimental Results early practice effects and to facilitate the observation of stable task strategies the data for each subject from the first of the six testing sessions was discarded together with the first ten trials of the remaining five sessions , yielding on the order of 550 trials per subject . Only correct RT ' s excluding outliers were analysed . Correct RT histograms were plotted separately for each problem type , for both true and false responses , and for each subject to identify possible outliers . Response times falling more than 1 second above the main distribution as determined by the mode and the contiguous tails were eliminated ; such outliers constituted about 2 % of the correct RT data . Table 1 about here Due to the complex description of the ASP items it is not possible to represent all aspects of the data simultaneously in a single graph or table . However , by collapsing across various subsets of the data we can obtain a reasonable picture of major effects which can then direct more detailed modeling and statistical evaluation . Since plots of the data for individual subjects showed subjects to , be qualitatively comparable , the RT data for all six subjects was pooled to simplify the presentation of results . Table 1 presents RT and error rate data classified by problem type The notation and position of the false link ( if any ) . indicates that the third link from the left was false ; P ( > - ) FTT PC - - - ) TTF that the upper link of the branch ( » was false ; PC - < ) TTF that the lower link of 8 Table 1 - Group RT Means and Errors ( by problem type and position of false link ) type false obs th obs error total link mean mean s . d . % N ( msec ) ( msec ) ( msec ) P ( - ) T 1576 1529 590 5 . 6 245 P ( - ) F 2041 1993 741 5 . 3 243 P ( - - ) TT 2468 2541 905 2 . 3 251 P ( - - ) FT 2101 1842 862 7 . 8 117 P ( - - ) TF 3161 3187 1035 3 . 8 121 P ( - - - ) TTT 3631 3584 1103 4 . 3 388 P ( - - - ) FTT 2137 1871 949 7 . 5 121 P ( - - - ) TFT 3374 3086 1252 6 . 7 125 P ( - - - ) TTF 4184 4046 1106 10 . 5 121 P« ) TT 1580 1567 532 4 . 5 161 P« ) FT 2287 2068 815 8 . 3 75 P« ) TF 2152 2014 964 3 . 0 88 P ( - < ) TTT 2592 2534 933 4 . 3 328 P ( - < ) FTT 1950 1825 994 10 . 2 112 P ( - < ) TFT 2972 3001 895 5 . 9 108 P ( - < ) TTF 2864 3000 858 6 . 8 112 P ( » TT 2575 2610 752 5 . 1 121 P ( » FT 2501 2211 835 5 . 0 63 P ( » TF 3002 2876 735 3 . 1 61 P ( > - ) TTT 3657 3539 1045 3 . 7 237 P ( > - ) FTT 2622 2406 1146 4 . 2 78 P ( > - ) TFT 3342 2990 1169 6 . 7 79 P ( > - ) TTF 4002 4153 1060 1 . 7 73 8a the branch « ) was false . Observed means and variances and theoretical means ( derived from a statistical model introduced below ) are averages across subjects weighted by the numbers of correct RT ' s observed . Figure 3 about here Figures 3 and 4 are based on the data of Table 1 ; curveS represent theoretical mean , RT . Figure 3 plots true and false mean RT by problem types . A striking feature about these data are the following approximate equalities of mean RT ' s obtaining among the problem types ; P« ) P ( - ) and P ( - < ) = P ( - - ) P ( » = P ( - - ) and P ( > - ) P ( - - - ) Of course these equalities hold among averages including quite distinct items within each problem type , but they do suggest that the time to verify a left branch configuration « ) is not substantially different from the time for a simple link ( - ) . In contrast , verifying two links in the ( » configuration appears to take the same time as two links in the ( - - ) configuration . In what follows the « ) configuration will be referred to as a double probe link and ( - ) as a single probe link . Within each of the problem groups RT increases with the number of links . If a sequential processing of links is assumed then the slopes of the true curves directly reflect the average time taken to verify that a link is true . Note that the three true curves plotted in Figure 3 have approximately the same slopes , which together with the equalities remarked above is consistent with a sequential processing account . A way to investigate order in sequential processing iato examine false RT ' s for each 9 4 I e - 0 v T cu " II > . . . . . 3 " cu / F E F cu III \ 0 g 2 0 . . ' " II > cu tt : c 0 cu • True : E •• o False P ( > - ) P ( » P ( - - ) . · P ( - - - ) P« ) P ( - < ) Problem Type I I I I I I I I I o P ( _ ) Figure 3 - Mean correct RT ' s plotted by : problem type and true / false . ( Points are data and curvef / are theoretical . ) problem type in a group as a function of the position of the false link assuming that subjects responded " false " as soon as they discovered afalse link . Figure 4 illustrates graphically this order of processing analysis . Figure 4 about here Figure 4a shows that for group A problems RT increases as the false link is moved from the first to the third position with a slope about the same as the true slopes in Figure 3 : this indicates a strict left / right processing order . Figure 4b shows that for P ( - < ) the tail link ( - ) is almost always verified before the left branch « ) , while within the branch there is no strong up / doWn processing order . This is interpreted as consistent with the proposal that the double probe link is verified in one step ( i . e . , not as separate simple links ) which implies that there should be no upidown processing order as such . Figure 4c presents a more complicated story for group C . While link processing for this group tends to be up / down on the right branch ( » and branch ( » before tail ( - ) ( i . e . , left / right ) in P ( > - ) this order cannot be strict since the RT slopes as the false link position moves are noticeably less than the true slopes in Figure 3 . A probabilistic order of processing is appropriate for group C problems . Figure 5 about here The verification of a - link is in some respects similar to memory scanning tasks ( Sternberg , 1969a ) that require subjects to decide whether a probe symbol is contained in a memorised set of symbols . For an ASP transition table the number of elements in a list labelled by a cue word is lO 5 . . I ( A ) I ( En I ( e ) U4 ~ A + • P ( - < ) III / o P« ) - GJ E F III 3 r g I . / I • f - ' c - o ell m GJ a : : t . / 0 j2 ~ ¥ 0 A P ( - - - ) T • P { > - ) · • P ( - - ) o P ( » o P ( - ) FTI TFT TTF FTT FT« ) TF«l Position of Folse Unk FTT TFT TTF Figure 4 - Mean correctRT ' s for false items plotted by problem type and position of false link . 3 . . - - - - , - - - r - - - , - - - - , u eu en • True o Folse O ' - - _ . L - _ - - I - - - - L - - - - - J 2 Cue Set Size Figure 5 - Mean correct RT I S for problem type p ( _ ) plotted Qy cue . set size and true / false . lO b referred to as the cue set size ; Figure 5 plots true and false RT ' s for P ( - ) by cue set ·size to illustrate set size effects analogous to those found in memory scanning tasks . The true and false curves are separated by a constant , suggesting a simple additive effect on RT of the process differences between true and false link verifications . Errors were infrequent under the speed / accuracy instructions given the subjects ; the error rate over all conditions and subjects was 5 . 2 percent . Group error rates broken down by problem type and position of the false link are presented in Table 1 . While the ' authors recognise the possibility of important theoretical relations between response times and error rates as for example suggested by Pache11a ( 1974 ) among others , a rigorous analysis relating the two was not performed for the data presented here . This omission is partly justified by the empirical observation that while - mean RT ' a showed a consistent pattern across subjects , error rates did not . Also , from purely statistical considerations when data is so finely classified that some classifications have twenty or fewer observations , error rates may not be sufficiently reliable for the analysis of data from an individual subject whereas RT ' s may still be meaningful in providing insight into psychological processes . 11 Theoretical Analysis Suppose that for a particular ASP item we have been given a description of the sequence of psychological operations used to solve it . The stage modeling technique to be used here assigns to each operation or stage , S , of the processing a tuple of parameters , < , . . . ( 8 ) , / ( 8 ) > corresponding to the theoretical mean and variance of processing time associated with that stage . In cases more general than that considered here this tuple may become a family of tuples corresponding to various states of the cognitive system that could exist when the stage operates ( i . e . , stages are specified conditionally ) or tuples may contain additional parameters such as higher RT moments or the probability of a processing failure in that stage . If stages 8 1 , 3 70 " " , S ~ are applied in sequence to process the item then the RT mean and variance for the item are 5 imply , ~ . . . . , . . ( RT ) = . ' i : f " ' ( 8 j ) and J = l l ( RT ) The additivity of variances follows from the assumption that stage processing times are stochastically independent . Now suppose that there are two sequences of stages that could be applied to the item , Sil ' S \ z . , ••• , SIfM . \ and S ' t . \ ' S ? l . , ••• , ~ ~ , and that these two sequences are observed with • probability p and ( 1 - p ) respectively . Let , ~ . ) A ~ = t . , u ( 8 ' J ) and . . : = . 1 : : . \ i 1 , 2 Then the mean and variance of the overall RT are , 12 . . . . . . ( RT ) = Pl " ' , + ( l - p» ) A ~ t ( 1 - p ) 6 . . + . . p ( l - p ) ( ) A , - ) A , ) Without going into further detail , similar expressions can be derived whenever RT is assumed to arise from the probabilistic mixture of sequences of stages . Proceding on the basis of the observations made in the results section above , a stage model was constructed using , 8 small number of inclusive stages that are identifiable ( i . e . , · in the sense of unique parameter estimates ) and that have direct theoretical interpretation . These stages are ; stage K D stage description verification of a single probe link with cue set size of n verification of a double probe link with cue set size of n orientation , attention , perception and miscellaneous set - up and bookeeping processes decision and response processes that differ between " true I ! and " false " responses Processes involved in the verification of single and double probe links have been summed together in the V - " . , and Wn . parameters respectively . Due to the problem of identifying parameters it is not possible to make definitive interpretations of the stages K and D . The K stage includes all those operations which are in common across problem ' items , such as attending to the - CRT display or executing the - motor components of a keypress response ; in 13 addition K may be regarded as incorporating incidental processes required for the logical completeness of the model such as recording the input and output of stage operations . Any processing differences between true and false responses , including handedness , are incorporated in the D stage . For the experimental data false responses are slower than comparable true responses ; the D parameters reflect this aspect of the data . Derivation of , theoretical expressions The derivation of expressions for theoretical RT means and . variances will be illustrated by examples since there is insufficient space for an exhaustive treatment . In the following let n· , be the cue set size associated with the symbol xL - Example 1 : ( T ) To solve this simplest problem the subject need only verify one link ; hence exactly the stages V ~ ~ and K occur . Then , jA ( RT ) l ( RT ) = t ( V ~ . ) + l5 ' \ K ) , Example 2 : ( TFT ) Assuming that P ( ~ ~ ~ ) has a strict left / right processing order the subject l4 first verifies that xi . - 7 - - x J is true and then finds that Xj ~ X ~ is false ; the subject responds ! lfalse " as soon as she finds this link so that only stages Vv \ . ' , Vr - . . _ , K and D occur . . , , " . . . ( RT ) / " - ( V . . . . ) + r ( V " . ) + r ( K ) + r ( D ) , , cl ( RT ) tS ' L ( V . . . , ) + 6 . . . ( V . . . . ) + l ( K ) + 6 ~ ( D ) , Example 3 : < x , , - x· - + - x . , , x . . ( TFT ) It is assumed that the double probe link « ) is verified in a single operation , W " . , and that the tail ( - ) is checked before the branch « ) , so , that the stages are V n . , Wv . . . . , K and D . , , j . A ( RT ) Example 4 : + jA ( W ) " , ( FT ) . . . . . + IS ( K ) + d ( D ) A probabilistic order of processing was suggested for types P ( » and p ( > - ) . This order will be defined by two probability parameters . Let q be the probability that within a right branch ( » the upper link is checked before the lower link , and let r be the probability that for P ( > - ) the branch ( » is checked before the tail ( - ) . In Example 4 the parameter r is not involved . With probability q the stages are Vn . - ' , K and , D , and with probability ( 1 - q ) the stages are V " ~ , V hj ' K and D , with the result that , 15 ) . A ( RT ) = ) J«V " . ) + ( 1 - q ) Jv - ( V " . ) + J - A ( K ) + p ( D ) , J 6 ~ ( RT ) l ( V " . ) + ( 1 - q ) " ' - ( V , , . ) + , , ' " ( K ) + , s " " ( D ) , J + q ( 1 - q ) [ P ( V " . ) ] 2 J The expression for , , ' - ( RT ) sequences of stages . is that for the probability mixture of two Example 5 : ( TFT ) In Example 5 , with probability rq the stages are Vh . ' , V . , . , K and D ; with , J probability r ( 1 - q ) , V " . , K and D ; with probability J ( 1 - r ) q , V " ' Io . ' V . . . , ' Vt \ . o , J K and D ; and with probability ( 1 - r ) ( 1 - q ) , V . . . ~ , V " j ' K and D . Hence , , . . . . ( RT ) o ' - ( RT ) = q y ( V " . ) + jJ - ( V . . . . . ) + ( 1 - r ) j . A . ( V " , ) ' J 10 . + j - A ( K ) + j - A ( D ) q o " ' - ( V " , ) + l ( v , , ) + ( 1 - r ) 6 " ' ( V " , ) + o ' - ( K ) + , , " ' - ( D ) + q ( 1 - q ) [ fA ( V " , » ) ' " + r ( 1 - r ) [ J ' . A ( V " , ) t The expression for , , ' - ( RT ) is an algebraic simplification of a general expression . Example 6 : ( TTT ) Since all links are true the same stages must occur whatever the order of processing . Consequently , l6 r ( RT ) = c . . . ( V . . . . ) + C ~ ( V " . ) . , + r ( K ) These examples should convey the gist of the statistical model . Note that for ev ~ ry ASP item the theoretical RT mean and a variance can be expressed in the following canonical form , . M ( RT ) a , r ( V , ) + a . . r ( V . , ) + as j - ' ( V 3 ) + a . , f " ( W ~ ) + as fA ( W 3 ) + a ~ r ( K ) + " Tr ( D ) 6 " " ( RT ) a , 6 . . . . ( V , ) + a . . d " ' ( V . , ) + ~ d " - ( V 3 ) + a ~ 6 . . . . ( W• . l + as oS " ' ( W3 ) + a ~ 6 . . . . ( K ) + a . " s " ' ( D ) + b ' Z . where the a ~ ' s ( i = l , . . . , 7 ) can be interpreted as the average number of times the corresponding stage occurs , and b 2 is the " mixture variance " ( i . e , the variance added by mixing processing strategies where strategies may require differing amounts of time ) . Writing the row vectors , e = ' " v = ' " a = ' " the canonical forms become , ; - «RT ) l7 , jL ( RT ) = where . , ; ; is the transpose of e and v T ~ : : : ' Co the transpose of v . " ' - For all true items and for false items in groups A and B , each ai . is an integer and bL = O ; for false items in group C the a , ' s may be functions ' of q and r , and . . b > 0 is a function of , q and r . Note that ASP items can be classified according to their coefficient vectors , ~ , and mixing variances ; b L ; under the model this classification is a full specification of the items . For the items used in the experiment forty - six such classification categories occurred . Statistical Evaluation A discussion of parameter estimation and statistical techniques is presented in the appendix . Best estimates of parameters were obtained for each subject by numerical methods using a quadratic loss function , and the fit of the model to the RT data was primarily evaluated by constructing minimum simultaneous confidence regions containing all the RT means and variances predicted by the model . Parameter estimates are given in Tables 2 and 3 ; statistics are listed in Tables 4 and 5 . Tables 2 and 3 about here For the mean RT data the statistics in Table 4a show that while the model does account for a substantial percentage of the between and total variances ( PBV and PTV columns ) , the maximum modulus t test applied to the group suggests that the model is probably not a complete account of the data for every subject in the experiment ( g * for the group is . 004 which is the probability of observing a t * value of 4 . 44 or greater ) . 18 Table 2 Parameter Values , Averaged Estimates , M < S ( msec ) ( msec ) single probe ( - ) verification : cue set size 1 V1 676 241 cue set size 2 V2 1169 446 cue set size 3 V3 1146 563 double probe « ) verification : cue set size 2 W2 924 365 cue set size 3 W3 1206 412 set - up processes true / false difference K D 446 466 210 200 probability of up before down on ( » , q = 0 . 85 probability of ( » before ( - ) on ( > - ) , r = 0 . 89 l8a Table 3 - Parameter Values , Individual Subjects Subject 1 Subject 2 Subject 3 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - IV ' d IV ' d IV ' < S Vl 878 566 449 3 571 0 V2 1545 676 867 293 1001 450 v3 1587 720 913 593 775 446 W2 968 575 789 399 748 398 W3 1144 442 986 421 768 284 K 475 1 545 1 394 0 D 729 1 346 489 452 0 q 0 . 79 1 . 00 0 . 78 r 0 . 88 0 . 48 1 , 00 Subject 4 Subject 5 Subject 6 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - J - ' - < ! / - A ' < 5 / ' - ' - < 5 Vl 956 0 594 259 607 532 V2 1375 671 1430 635 798 230 V3 1348 729 1271 646 980 636 W2 1156 689 1045 31 838 78 W3 1582 720 1455 485 1301 88 K 292 1 462 307 506 414 D 459 302 2 506 1 q 0 . 61 0 . 92 1 . 00 r 1 , 00 0 . 99 1 , 00 18b Table 4 about here In Table 4a two of six subjects have g * > . 10 indicating a good fit of the model for these individual data , and in Table 4b g * > . 10 for three subjects . The third column of Table 4 gives the number of points lying outside a . 90 simultaneous confidence interval ; any such point implies that g * < . 10 • Table 5 about here Table 5 presents statistics for RT variances . Due to large sampling variability of variance estimates , the g * statistics are not very informative since many models would he acceptable within wide limits of variability . The third column of Table 5 compares the model to the hypothesis that all RT variances are the same , in terms of the proportion of points for which the model makes a more accurate prediction . Evaluated ~ sing ' this statistic the model does no better than the " same " hypothesis although both are acceptable given the variance of estimators . Since the averaged RT variance parameters presented in Table 2 appear to be orderly they will be discussed although no strong conclusions should be drawn . 19 Table 4 - Statistical Analysis for RT Means points subject t * g * outside PBV PTV 90 % region A . model classification ( 46 points ) 1 4 . 44 0 . 000 2 79 . 7 42 . 7 2 2 . 78 0 . 182 0 * 76 . 7 35 . 6 3 3 . 32 0 . 029 2 78 . 6 40 . 7 4 4 . 44 0 . 000 1 79 . 3 36 . 5 5 3 . 86 0 . 007 2 57 . 7 28 . 1 6 2 . 84 0 . 157 0 * 68 . 7 27 . 8 group 4 . 44 0 . 004 3 73 . 5 35 . 2 B . type X false link classification ( 23 points ) 1 3 . 28 0 . 014 1 85 . 6 41 , 5 2 1 , 74 0 • . 697 0 * 84 . 3 32 . 1 3 2 . 42 0 . 198 0 * 89 . 5 37 . 3 4 3 . 01 0 . 036 1 83 . 7 35 . 8 5 3 . 44 0 . 008 1 64 . 5 26 . 6 6 2 . 67 0 . 101 0 * 71 , 0 24 . 5 group 3 . 44 0 . 049 2 79 . 8 33 . 0 Table 5 Statistical Analysis for RT Variances ( model classification , 46 points ) proportion subject t * g * modelvs I ' same tl 1 0 . 83 0 . 999 0 . 478 2 0 . 63 0 . 999 0 . 500 3 1 . 00 0 . 999 0 . 348 4 1 . 07 0 . 999 0 . 522 5 0 . 85 0 . 999 0 . 478 6 0 . 56 0 . 999 0 . 391 19b Discussion of Stage modeling has been conceived / in terms of a formal processing language description of memory operations : stages are analogous to procedures or subroutines , perhaps probabilistic in their execution , organised by call sequences into memory processes . Within such a stage modeling framework various levels of detailed description are possible . For example , one might consider macro stages such as " perception " , " memory " and " response " , or comparatively micro stages such as " input the symbol in position p of the stimulus array " or " compare the code for symbol X with the code for symbol Y " • No particular level of detail can be regarded as preferred : theoretical descriptions in stage terms must be evaluated with respect to the relevant data . However , the stage modeling framework does in principle relate all levels of description in terms of the nesting of procedures in call sequences , thus providing the possibility of consistently treating the results of simple and relatively more complex laboratory tasks with the same overall processing model ( Atkinson & Wescourt , in press ) . A stage model can be most productively regarded as a rational basis tor the construction of statistical models . Each statistical model stemming trom a stage model can be evaluated with respect to the data , successes and tailures yielding new information about the data possibly not apparent on inspection or available from other analyses . In general it is not necessary that every statistical model derived from a particular processing language description be " successful " , but only that some ate and that these provide a usetul characterisation of the data . Of course , if a stage model were taken as a literal model of a specific real time process , 20 say specific interactions among brain centers and layers o ~ brain tissue , it would be important to verify all the statistical models derived from the stages theorye However , for the analysis of cognitive performance the authors regard stage models as non - literal information processing descriptions from which statistical analyses are derived that provoke a deeper and more adequate characterization of patterns present in the data . The statistical models for RT means and variances developed above msy be regarded as an intermediate level of stages analysis appropriate to the level of observable data : it does not explicitly describe either the component processes of individual link verifications or the overall control structure in which the problem solving algorithms are embedded . Since these additional levels of analysis are of theoretical interest , the discussion will turn to bridging these conceptual gaps . The following stages analysis of single and double probe link verification is given ; 21 determine whether single or . double probe ( s or d ) input cue L1 . L ~ L ~ acc ~ ssmemory list associated with cue input probe 1 ; if d then input probe 2 reset match register 1 ; if d then reset register 2 unpack an element from the memory list match the element against probe 1 and increment match register 1 by the value of the ·· " goodness - of - match lt ; if d then match against probe 2 and increment register 2 if the entire list has been unpacked then continue else return to L s if d then add match register 2 to register 1 if s then if the value of match register 1 exceeds a criterion C s then return true else return false ; if d then if the value exceeds cd . return true else return false Note that the analysis is essentially an " exhaustive scan " model , where matching is . not necessarily all - or - none , and where the representation of lists in memory and the co - ordinate retrieval or unpacking process may be more involved than reading from a list of symbols at a uniform rate . Representing a list as a cluster of symbols bound to a memory node by associative linkages and defining retrieval processes in terms of thi ~ representation would he one way of conceiving of an unpacking operation with more complex characteristics , although such " built in " characteristics may have limited conceptual and theoretical interest . The claim is that this model of link verification is consistent with 22 the stage parameter estimates in Table 2 ; for the sake of simplicity only the average values of parameter estimates are discussed . values in Table 2 may be qualitatively summarised as follows ; " , , ( V , ) < . . . . . ( V• . > = J - ' ( V ~ ) " , ( W . ) < ~ 3 ) The parameter ( j ' ( V , ) < O " - ( V• . > < O ' ( V 3 ) O " - ( W . . ) < " l ( W , ) ( $ " - ( W ) < O " ' ( V• . > . . J " ( W 3 ) < 6 " ' ( V 3 ) This summary can be regarded as an hypothesis that , within the sampling variability of the parameter estimates , is not disconfirmed . A problematic aspect of this summary is that jA ( W . , ) < , M ( V . ) by 245 milliseconds , yet ~ ( W3 ) = fA ( V 3 ) . This result may be attributable in some way to the fact that for W ~ the number of probes is the same as the cue set size , but in the absence of additional controls no ad hoc explanations are offered . If it is assumed that stages L ; p Ls and L " l account for the major part of link verification time , then a gross simililarity would be predicted . between single and double probe links . With suitably complex representations of lists the mean unpacking time for lists of lengths 2 and 3 may be comparable , yielding fA ( V . ' > = , M ( V3 ) ; the speed of V I could be explained by the simplicity of the representation for a list with a single symbol requiring fewer unpacking manipulations . 23 Single and double probe link verifications differ in stages L ' i ! and L ~ . Lf the matching process is probabilistic ( e . g . , due to variable imperfect coding of symbols ) then the final match value in register 1 will be distributed differently for singLe and double probes ( e . g . , double probes will have greater mean and variance for both true and false links ) . This together with the two criteria , C s and cd ' might account for decision component differences in ways similar to signal detection models that relate RT to criteria placements in relation to signal and noise distributions ( Th 971 ) " - ( ) ' L ( ) ' L ( ) " - omas , 1 • The observed cs W . . < 6 V . . . and < $ W . < < $ ( V3 ) are interpreted as due to such differential effects ill stage ~ . The value fA ( D ) = 466 is greater than would be expected on the basis of handedness alone , suggesting genuine decision component differences ; again this is interpreted as a stage L ~ effect . Since successive link verifications are required by some ASP items , in order to achieve an acceptable error rate ( subjects were instructed to be accurate ) it is necessary to make a more accurate decision for each intermediate verification than would be needed if only a single link were verified On each trial . Also since over all items there are more true than false links , stage L might be t1 tuned ll for a true verification . The demand for increased accuracy together with a true verfication expectancy could account for the observed value of tA ( D ) . The apparent constancy of fA ( D ) over problem types , even those where only one link is verified , is consistent with the theoretical conception that the same link verification mechanisms are used for all problems without modification according to problem type . From these considerations it would be predicted that encouraging speed over accuracy , using only single link problems , reducing the variety of ASP items used within a single experiment , or using multi - link items with more than one false link would all have an effect in reducing the value of fA ( D ) . 24 As an aside , it may be possible to use an empirical speed / accuracy tradeoff to further investigate the verification mechanisms found in the ASP task . A direct implication of the theory discussed above is that under speed instructions each link verification will be less accurate as processing is modified for speed or cut short , with the results that errors will tend to increase relatively more for items with many links compared to those with few , and that error RTfs for multi - link true items will decrease relative to correct RTfs while error RTfs for multi - link false items will increase . Other quite different effects of speed instructions might be to induce subjects to implement faster problem solving algorithms , say with some sort of simultaneous verification of links , to nprime " access to certain algorithms and retrieval mechanisms in anticipation of the next problem , or to adopt sophisticated guessing strategies . The issueswith regard to speed / accuracy effects in ASP problem solving are manifold and may perhaps be most productively approached by comparing experiments to determine what effects might be present . results across In stage terms a stable strategy is a problem solving algorithm that is not modified with use . Empirically , stable strategies would be expected for practiced subjects who have in some sense developed optimal task techniques , with the required amount of practice depending on the particular task . The present experiment was designed to observe only asymptotic performance , making it in principle possible to specify a single set of algorithms or strategies governing the processing of ASP items . A theory as to how these strategies are s ~ t up with practice is not developed here ; however , the authors do conceptuali . se an interactive feedback syst ~ m whe . re· the state space of the system consists of algorithms and the effects of control inputs are to rebuild algorithms . It is proposed that for tasks where alternative processing strategies are a genuine theoretical possibility , it may be more appropriate to analyse data frc ~ trials early in the experiment in terms of a mixture of 8tr ~ tegies rather than a ' single stable strategy . For the sake of completeness of theoretical conception it is assumed that the strategies for the various problem types are called by a controlling stage that on each trial identifies the problem type on the basis of its mapping diagram configuration and calls the corresponding problem solving algorithm . Additional empirical work is required to evaAuate these conceptual analyses of control and component processes . For example , one line of that such verification ASP and problem types . more . complex than reveal more about the ASP problems tasks could thoroughly examine transition table more examine of the experimentation would be to problems , with manipulations Another line would be to verification , with the idea construction of strategies , that is about how component processes are used to build problem solving algorithms . Alternatively the verification of isolated single and double probe links could be· examined in greater experimental detail . All these levels of experimental investigation are well integrated within the stage modeling framework , which is , again to emphasise , one of the main theoretical motivations for using such a framework as a basis for data analysis . From a theoretical standpoint a close relationship exists between link verification and some memory - scanning tasks . In both cases a probe item must in some sense be co ~ ared against a list of symbols in memory to determine if the probe is a member of the list . A point of interest is whether memory - scanning mechanisms that have been investigated in the laboratory can be identified as components of relatively more complex tasks 26 such as solving ASP verification problems . The model constructed for the ASP problems investigated in this paper can be regarded as an attempt to tackle this issue . About the simplest relation that could obtain between 11 ! emory scanning and ASP problem solving would be that the scanning mechanisms engaged by stategies to yield intermediate results have the same characteristics as those , observed with simple memory scanning tasks . Yet this need not be so . It is· conceivable that as strategies for the more complex storage , retrieval and decision making required by ASP problems are constructed in the memory system ( Atkinson & Wescourt ' , 1975 ) new demands for rapid access to a larger volume of stored information , for the recording of intermediate results which direct further processing , and for controlling error rates when intermediate results are combined or cascade in a final decision , demand scanning mechanisms having different characteristics . The data from the present experiment are not in themselves conclusive , but the parameter values of Table 2 as discussed above suggest that the inferred scanning ( link verification ) mechanisms and decision processes yield values of RT parameters that differ from those typically found in the memory scanning literature . There is the unexpected result that verifying a double probe link is as fast as verifying a single probe link ; the fact that for single probe links verification times for cue set sizes two and three do not differ from each other but are dramatically different from the verification time for cue set size one ; and the unusually large constant difference between true and false RT ' a . Each of these effects is of course subject to further investigation and taken - one at a time are not without some parallel in the memory literature , but the authors believe that they provoke an examination of the issue of how memory scanning mechanisms relate to the larger human memory system . It is fair to say that proportionally mOre 2 ' 7 described here effort has been devoted to unravelling· the effects of experimental manipulations on basic memory scanning tasks and constructing sophisticated and interesting models for these data ( e . g . Theios , 1972 ; Anderson , 1973 ; Shevell & Atkinson , 1974 ) , than has been devoted to examining the possible roles of memory scanning mechanisms in human memory systems that are sufficient to support more involved cognitive processing . The stage model developed for the experiment characterised each stage by two parameters , the mean and variance of processing time ; as remarked above this type of model can be generalised to include more parameters such as the probability of an error in that stage or higher moments nature of the of the processing time distribution . Without modeling technique , stage parameters could changing the be expressed conditionally on the state of processing , as for example on the input to the stage from previously operating stages . Even with these generalisations parameter estimation and statistical procedures can be derived in a mathematically simple way . Granted that it is one opinion , the authors feel that statistical methods such as those described in this paper that are based on a formal but flexible model of psychological processing should in many cases be both practical and more incisive than the standard linear statistical analyses often found in the memory and problem solving literature . 28 Comparison with Hayes ' spy problems Hayes ( 1965 , 1966 ) has reported studies using a problem solving task similar to that of the ASP problems defined here . Subjects in Hayes ' experiments learned a list of " spy " names together with rules about which spies could talk to each other ; the list of these " talking connexionsll may be regarded as a transition table . In the basic experiment , subjects were given two spy names and required to find a chain of spy - to - spy communications conveying a message from the one spy to the other . Subjects were instructed - to " thinkaloud ll and their protocols were analysed with respect to the overall time taken to solve a problem , the rate at which links in the communication chain were generated , and diversions into " blind alleyll side chains ( i . e . , passing the message to a spy who did not have the connexions to get it to the goal spy ) . Subjects were able to solve spy problems in a matter ofa few minutes , occasionally entering side chains and usually achieving a solution chain longer than the minimal required chain ; thereader is referred to the original papers for Hayes ' analysis of his results . In terms of the type of theory proposed here for ASP problems , the solution of spy problems would be described by algorithms constructed using a small set of basic psychological operations and following specific search - and - test methods of chain construction . Insofar as the model stated definite algorithms it would have the potential to account for protocols ; as stage models the algorithms would also make quantitative predictions about the pattern of observed RT ' s and error rateS . Of course the particular theory of ASP problem solving outlined in this paper is not sufficient in itself to account for Hayes ' s results such as the end - acceleration phenomenon : in addition explicit algorithms would have to be constructed and demonstrated by computer simulation or by inferential data analysis to produce the observed pattern of results . The stage modeling technique It is worthwhile to emphasise the positive aspects of stage modeling as a technique for the analysis of RT tasks . Interesting arguments related to those presented here have been given by Sternberg ( Sternberg , 1969b ) with respect to the so - called additive factors method . First as has been noted t considering psychological processes as procedures or subroutines in the sense of a formal computer language provides an easily conceived unifying framework for theoretical analysis and a rationale for investigating memory mechanisms as they occur both in simple and complex laboratory tasks . Second , from a statisitical standpoint regression models for RT moments can be derived from a stages theory ina relatively simple manner , basically by counting the occurrences of stages . The parameters in the regression model have direct psychological interpretation in terms of real processing time , and the parameters can be estimated by common analytic or numerical methods irrespective of the number of classification categories or the number of observations in each category . With regard to predictive power , stage models can provide accounts for RT moments of all orders and together with notions of processing variabili ~ ydefined at specific stages can at the same time provide an account of errors . Even though the techn ~ que is mathematically simple , the underlying process representation is that of a quite general sequence of random variables ( or random vectors ) corresponding 30 to the definition of a discrete stochastic process ( viz . a " family of random variables with a countable index set ) with very few restrictions ( e . g . most of the random variables can be assumed to be finite valued ) . This suggests that many models of memory processes will be at least formally " nearly " equivalent to some stage model as defined here . The nature of this equivalence can be formalised in terms of the partitioning of the event space of the experiment ( i . e . the set of all possible data points ) induced by the inverse mapping of the goodness of fit measure regarded as a random variable . Simple and complex tasks The algebra step problems introduced in this paper are , like other artificial memory and problem solving tasks , not advocated for their intrinsic interest but rather as one experimental paradigm for testing our understanding of human memory systems . Fast accurate problem solving has on the one hand clear theoretical relations to conceptions of basic memory mechanisms and the manner in which these mechanisms come to play in a larger memory system , and on the other hand it is a bridge to the chronometic analysis of more traditional problem solving tasks . While the investigation of simple tasks is indispensible it is surely necessary to develop theoretical constructions for more complex tasks with equal vigor : the chronometric analysis of tasks at the level of ASP problems is intended as one step in this direction . In philosophical perspective there is no assurance that even a detailed understanding of the models required to account for isolated simple memory tasks will automatically lead to an 3 . 1 adequate conception of human memory systems that are capable of supporting such routine cognitive functions as the retrieval of propositional information ( Anderson & Bower , 1973 ) or grade school arithmetic problems ( Suppes , Loftus & Jerman , 1969 ) . The data and analysis presented in this paper suggest that analysis of RT ' s on the order of five seconds is feasible without undue loss of precision either in the conceptual model or the statistical treatment . Across experiments it should be possible to identify the characteristies of memory mechanisms as they occur in memory systems where processes involving alternative strategies , ' intermediate processi ~ g _ results and decisions about subsequent processing , and rapid access to large amounts of stored information are operating . Such a program of research has the potential to develop the basis for more exacting analyses of problem solving tasks in terms of an explicit theory of human memory , to elucidate the role of control and decision processes , and to qualify our understanding of memory mechanisms discovered through research on simple tasks . 32 Statistical Appendix The coefficient vectors ~ = < a \ , . . . , a ? > define a classification of observations into distinct categories under the model ; forty - six such classification categories were observed in the experiment ( i ~ e . , there were 46 distinct ~ vectors ) . The notation below will be used in what follows . The index " i " refers to the i ' " ' " subject and " jll to the j1 ' h . classification category . n number of classification categories under the model s number of subjects N· . number of observations ' J Mi . , i RT sample mean Mi + RT grand sample mean . . Stj RT sample variance . . . . " Ti . . , i sample variance of S , - j ( see methods in , Kendall & Stuart , 1969 ) Parameter estimation The approach taken to parameter estimation was to choose a loss function conceived of as a function of the parameters given the data , and to find parameter values that minimised this function . Since function minima were found using a numerical grid search method , computationally efficient quadratic ( least squares ) loss functions were chosen . estimated for each subject individually . 33 Parameters were The actual estimation proceded in two steps . First values of qt and r ~ were determined using the loss function , T . . . . ( M . . - a·· e· ) I . J : : : : : : : $ I . ~ = ~ N ~ - - - - - - i - - - - - - - - Si . j " " " " ~ Second the parameter values , ~ i ' q ~ , r ~ , were treated as constant ' and ~ L estimated with the loss function , LS . . ( if ) e . - q . " r . ) z . . , . . . ~ n . ~ j . : : I 1 . " { ' " 1 . ' 2 . l S " ( g ' i ¥ , + btl ) ] - - - - - - - - - - - - - - - - - - - ~ - - - - - - . . . . TlJ An alternative procedure would have been to simultaneously estimate all parameters using a combined loss function of the form , LS wLS , + ( 1 - w ) LS . . o < w < 1 However , it was observed that the RT means showed a clearer pattern than the RT variances ; so that estimates of the mean RT parameters " uncontaminated II by possible failures of the model for RT variances were considered appropriate . Parameter estimates for individual subjects are listed in Table 3 . The numerical method used to estimate variance parameters excluded negative variances with one result that some parameters were estimated to be near zero ( the loss function , LS ' L , would have been reduced had negative values been accepted for these parameters ) . An inherent problem in the analysis of RT variances is that for classification categories with small sample sizes the variability of the sample 1 - variance , Sij , is large relative to that·for the sample mean , M ij : consequently parameter estimates will also have large variability . Note that variance parameter estimates averaged across subjects are more readily interpretable as variability is reduced through averaging ~ Goodness of fit measures Consider the statistic defined for the i ' " subject and j ' " category by , ( M·· - Ll which for suitable models maybe assumed to be approximately distributed as Student ' s t under the hypothesis that the theoretical mean , ~ ~ ~ : , iathe true mean of the ij * " RT distribution . One method of evaluating the fit of the model to mean RT ' s is to construct the smallest possible uniform simultaneous confidence region containing all the tij ' s and ~ to note the probability of the complement of the region . This probability is the minimum value of d . . ( the probability of a type I error ) for which the hypothesis that the model is true can be rejected ; small values indicate that the model is probably not a full account of the mean RT data . If the ' distribution of t , j is approximated by N ( O , l ) instead of by Student ' s t , a conservative bias is introduced in the sense that the value of ~ is necessarily reduced ~ Since the normal approximation simplifies the calculation of a simultaneous confidence region this assumption is adopted ~ For the i ~ subject define , t . * , 35 prt - c < t ~ ~ < cl If the t tj ' s were independent then for any positive number , c , Pr { tt > c 1 = 1 - Pr { tt < c 1 ~ 1 - TT , ' 1 1 - [ pr ~ - c < z < cJJn where z ~ N ( O . l ) . This is the probability that for a fixed i all the t ' i ' s are contained within a uniform , symmetric confidence band of width 2c . But for each i the t ~ i ' s are correlated through the estimation procedure , and with enough parameters it may be possible to obtain all the t , j = 0 , rendering the preceding probability statements meaningless . Accordingly some conservative adjustment should be made taking into account at least the number of free parameters , p . The choice for the present analysis was to take , ] , , , - p ) g ; ' < = 1 - [ pr { - c < z < c } " in place of Pr { tl . ' > c 1 above . If t ~ = c is observed then g ~ " , is a statistical measure of the fit of the model for the i ~ . subject . Similarly for a sample of s subjects define , t * = max I t ' i ' I max t· . I ! = . ~ ~ s • ' i $ ; i . . 6S " \ ~ js ; V \ . then , g * = 1 - [ Pr { - c < Z < c } ] s ( n - p ) is a goodness of fit measure for the sample as a whole . This procedure is a type of multiple modulus test ( Miller , 1966 ) referred to here as a " maximum modulus t test " with ( n - p ) or s ( n - p ) " degrees of freedom " , taking some licence with terminology . A related procedure can be followed in evaluating theoretical versus observed RTvariances . The statistics defined by , I t· . ' J 2 . _ T 1 - S . . ( a·· v· + b , ' J ' ) t . ~ ~ LJ % L . . T· . CJ I can be treated in the s . arne manner as the t ~ ' s above although t ~ ~ cannot be ~ egarded as having Student ' s t distribution and g * in this case ought to be taken as a transformation of , the t ~ j ' sreflecting goodness of fit rather than as an approximation to a true probability . To obtain a firmer statement about goodness of fit a second measure was sought . Although the model under consideration is not linear , the total sum of squares can be partitioned in such a way as to yield statistics reflecting the goodness of fit of the model to RT means in a way similar to the percentage of between variance accounted for and the sample correlation coefficient in linear regression . Define for any set of theoretical means , { - fi . jI ' for the it ' - . . subject , SS ( between ) - t N ll ( fll . i = l 2 M· . ) CJ = " - 2 I L Nlj ( f tj j = - I = 100 SS ( between ) 37 = 100 SS ( total ) If the fi . , i ' s were determined under a linear regression model , then , , MV . , SS ( linear regression ) The results of the maximum modulus t , PBV and PTV analyses for RT means are presented in Table 4 . Table 4a gives these statistics for the classification categories determined by the a vectors of the model ; " " Table 4b represents the same analysis applied to the classification of Table 1 ( problem type X position of false link ) . From Table 4a it is clear that the model accounts for a fair proportion of the variance ( average PBV is 73 . 5 and average PTV is 35 . 2 ) , yet only two subjects have g * > . 10 which is a " reasonable " criterion for a good fit . Additional information about the maximum modulus t test is given by the number of points falling outside the . 90 confidence region ; g * > . 10 if and only if this number is zero . It . should be noted that points which lie outside the confidence region are not necessarily those which the model fails to account for since when parameters are estimated simultaneously for all points an " exceptional " point can adversely influence the prediction for other " normal n points . For the group of six sUbjects the maximum modulus t test indicates that the model is true can be rejected for d . = . 004 . It should be noted that one bad data point for a single subject can be sufficient to reject the model for the group using the maximum modulus t test ; the proportion of subjects for which the model is not rejected is perhaps a more appropriate group statistic . In view of the all too common practice in the literature of presenting statistics for averaged group data it is difficult to make a firm statement on this point based on the results of other comparable analyses . The analysis presented in Table 4b indicates a slightly better fit although it is derived from a less strict interpretation of the model . Some improvement is expected since more extensive averaging may cancel out effects not accounted for by the model and estimated error variance is increased slightly as observations with different means are pooled . However , this second classification does correspond to an intuitively natural division of the data . Table 5 presents an evaluation of the model ' s success in accounting for small As remarked above for RT variances . 1 the variance of S ~ is large sample sizes : for the experimental data this renders the maximum modulus t test uninteresting because for 1 - • individual subjects the T ~ j s are too large to reject any set of ballpark estimates for the variances . Variance predictions under the model were compared to the the hypothesis that all the ' I . • S ~ s are the same , using the proportion of points better accounted for by the model ( absolute differences between predicted and observed were compared ) . Referring to Table 5 , the model succeeds about as well as the " same " hypothesis for four subjects and does worse for the remaining two subjects ' data . This is not strong support for the model applied to RT variances hut may be interpreted to mean that , compared to the f1 same " hypothesis , attempting to infer stage variances did not cost much in the way of goodness of fit , while at the same time the model ' s predictions cannot be . . rejected given the variability of the S ~ j estimates . 39 References Anderson , J . A . A ' theory for the recognition of items from short memorised lists . Psychological Review , 1973 , 80 , 417 - 438 . Anderson , J . R . & Bower , G . B . Human Associative Memory . Washington , D . C . : Winston & Sons , 1973 . Atkinson , R . C . , & Wescourt , K . T . Some remarks on a theory of memory . In P . M . A . Rabbitt and S . Dornic ( EdS . ) , Attention and Performance ~ . New York , Academic Press , 1975 . Chase , W . G . , & Clark , H . H . Mental operations in the comparison of sentences and pictures . In L . W . Gregg ( Ed . ) , Cognition in learning and memory . New York : Wiley , 1972 . de Groot , A . Thought and choice in chess . New York : Basic Books , 1965 . Hayes , J . R . Problem topology and the solution process . Learning and Verbal Behavior , 1965 , 4 , 371 - 379 . Journal of Verbal Hayes , J . R . Memory , goals , and problem solving . Problem solving : research , method , and theory . In B . Kleinmuntz ( Ed . ) , New York : Wiley , 1966 . Kendall , M . G . , & Stuart , A . The advanced theory of statistics , ~ ~ . New York : Hafner , 1969 . 40 Miller , R . J . Simultaneous statistical inference . New York : McGraw - Hill , 1966 . Newell , A . , & Simon , H . A . Human problem solving . Englewood Cliffs : Prentice - Hall , 1972 . Pachella , R . G . The interpretation of reaction time i ' n information processing research . In B . Kantowitz ( Ed . ) , ~ information processing : tutorials in performance and cognition . New York : Lawrence Erlbaum Associates , 1974 . Shevell , S . K . & Atkinson , R . C . A theoretical comparison of list scanning models . Journal of Mathematical Psychology , 1974 , 11 , 79 - 106 . Sternberg , S . Memory - scanning : mental processes revealed by reaction - time experiments . American Scientist , 1969 , 4 , 421 - 457 . ( a ) Sternberg , SG The discovery of processing stages : extensions of Donder ' s method . Acta Psychologica , 1969 , 30 , 276 - 315 . ( b ) Suppes , P . , Loftus , E . F . & Jerman , M . Problem solving on a computer based teletype . Educational Studies in Mathematics , 1969 , 2 , 1 - 15 . 41 Theios , J . Reaction time measurements in the study of memory processes : theory and data . Madison , Wise . : University of Wisconsin , Report 72 - 2 , Wisconsin Mathematical Psychology Program , 1972 . Thomas , B . A . C . Sufficient conditions for monotone hazard rate : an application to latency - probability curves . Journal of Mathematical Psychology , 1971 , 8 , 303 - 332 . 42