a r X i v : 2105 . 10311v2 [ c s . C L ] 25 M a y 2021 Pretrained Language Models for Text Generation : A Survey Junyi Li 1 , 3 † , Tianyi Tang 2 † , Wayne Xin Zhao 1 , 3 ∗ and Ji - Rong Wen 1 , 2 , 3 1 Gaoling School of Artiﬁcial Intelligence , Renmin University of China 2 School of Information , Renmin University of China 3 Beijing Key Laboratory of Big Data Management and Analysis Methods { lijunyi , steven tang , jrwen } @ ruc . edu . cn , batmanﬂy @ gmail . com Abstract Text generation has become one of the most im - portant yet challenging tasks in natural language processing ( NLP ) . The resurgence of deep learning has greatly advanced this ﬁeld by neural generation models , especially the paradigm of pretrained lan - guage models ( PLMs ) . In this paper , we present an overview of the major advances achieved in the topic of PLMs for text generation . As the pre - liminaries , we present the general task deﬁnition and brieﬂy describe the mainstream architectures of PLMs for text generation . As the core content , we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text . We further summarize sev - eral important ﬁne - tuning strategies for text gen - eration . Finally , we present several future direc - tions and conclude this paper . Our survey aims to provide text generation researchers a synthesis and pointer to related research . 1 Introduction Text generation , which is often formally referred as natural language generation , has become one of the most important yet challenging tasks in natural language processing ( NLP ) . It aims to produce plausible and readable text in human lan - guage from input data ( e . g . , a sequence and keywords ) . Re - searchers have developed numerous techniques for a wide range of applications of text generation [ Li et al . , 2021a ] . For example , machine translation generates text in a different lan - guage based on the source text [ Yang et al . , 2020a ] ; summa - rization generates an abridged version of the source text to include salient information [ Guan et al . , 2020 ] . With the recent resurgence of deep learning , various works have been proposed to solve text generation tasks based on recurrent neural networks ( RNN ) [ Li et al . , 2019 ] , convolu - tional neural networks ( CNN ) [ Gehring et al . , 2017 ] , graph neural networks ( GNN ) [ Li et al . , 2020 ] , and attention mech - anism [ Bahdanau et al . , 2015 ] . One of the advantages of these neural models is that they enable end - to - end learning † Equal contribution . ∗ Corresponding author . of semantic mappings from input to output in text generation . Besides , neural models are able to learn low - dimensional , dense vectors to implicitly represent linguistic features of text , which is also useful to alleviate data sparsity . Despite the success of neural models for text generation , a major performance bottleneck lies in the availability of large - scale datasets . Existing datasets for most of supervised text generation tasks are rather small ( except machine transla - tion ) . Deep neural networks usually have a large number of parameters to learn , which are likely to overﬁt on these small datasets and do not generalize well in practice . In recent years , the paradigm of pretrained language models ( PLMs ) is thriving [ Peters et al . , 2018 ] . The idea is to ﬁrst pretrain the models in large - scale corpus and then ﬁne - tune these models in various downstream tasks to achieve state - of - the - art results . It is widely recognized that PLMs can encode a large amount of linguistic knowl - edge from corpus and induce universal representations of language . Therefore , PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [ Brown et al . , 2020 ] . Moreover , with the increas - ing of computational power and the emergence of Trans - former architecture [ Vaswani et al . , 2017 ] , PLMs have ad - vanced from shallow to deep and achieved outstanding per - formance in many tasks , such as BERT [ Devlin et al . , 2019 ] and GPT [ Radford et al . , 2019 ] . Therefore , researchers have proposed various methods to solve text generation tasks based on PLMs . Pretrained on large - scale corpus , PLMs are able to understand natural language accurately and express in human language ﬂuently , both of which are critical abilities to fulﬁll the text generation tasks . Existing surveys in this area have only partially reviewed some related topics . Zaib et al . [ 2020 ] and Guan et al . [ 2020 ] provided a synthesis to the research on some text generation subtasks , i . e . , dialogue systems and summarization , but did not go broader to the other important generation tasks . Qiu et al . [ 2020 ] summarized two gener - ations of PLMs for the whole NLP domain and introduced various extensions and adaption approaches of PLMs . To the best of our knowledge , our survey is the ﬁrst work that presents a comprehensive review of PLMs for text generation . It aims to provide text generation researchers a synthesis and pointer to related research . To start with , we ﬁrst present a general task deﬁnition with the formulations of different text generation tasks in Sec - tion 2 , and then brieﬂy describe the mainstream architectures of PLMs that are used in text generation in Section 3 . Since the core of text generation is to model the semantic mappings from input to output , we further organize the major advances with respect to the two aspects of input and output in Sec - tion 4 - 5 . For input , we mainly discuss how to adapt existing PLMs to different data types . For output , we study how to satisfy special properties for the generated text . Furthermore , we summarize several important ﬁne - tuning strategies for text generation in Section 6 . Finally , we present several future di - rections and conclude this paper in Section 7 . 2 Task and Typical Applications In what follows , we formally deﬁne the text generation task . The core of text generation is to generate a sequence of dis - crete tokens Y = h y 1 , . . . , y j , . . . , y n i , where each y j is drawn from a word vocabulary V . In most cases , text gen - eration is conditioned on input data , such as attributes , text and structured data , which is denoted as X . Formally , the text generation task can be described as : P ( Y | X ) = P ( y 1 , . . . , y j , . . . , y n | X ) . ( 1 ) According to input X , we next introduce several typical applications of text generation : • If X is not provided or a random noise vector z , this task will degenerate into language modeling or unconditional generation task [ Radford et al . , 2019 ] , which aims to gener - ate text without any constraint . • If X is a set of discrete attributes ( e . g . , topic words , sen - timent labels ) , the task becomes topic - to - text generation or attribute - based generation [ Keskar et al . , 2019 ] . The infor - mation in X plays the role of guiding the text generation pro - cess and controlling the modes of the generated text . • If X is structured data like knowledge graph or table , this task will be considered as KG - to - text or table - to - text gener - ation , called data - to - text generation [ Li et al . , 2021c ] . This task aims to generate descriptive text about structured data . • If X is multimedia input such as image and speech , the task becomes image caption [ Xia et al . , 2020 ] or speech recognition [ Fan et al . , 2019 ] . The core of image caption is to generate a description of an image , while speech recognition enables programs to process human speech into a text format . • The most common form of X is also a text sequence , and there exist several applications such as machine transla - tion , summarization and dialogue system . Machine transla - tion [ Conneau and Lample , 2019 ] aims to translate text from one language into another language automatically , summa - rization [ Zhang et al . , 2019b ] is focused on generating con - densed summary of a long document , and dialogue sys - tem [ Wolf et al . , 2019 ] is designed to converse with humans using natural language . We present the formulations for the major text generations in Table 1 . 3 Standard Architectures for Text Generation Pretrained language models ( PLMs ) are pretrained with a mass of unlabelled text data and can be ﬁne - tuned on down - stream generation tasks . Pretrained on large - scale corpus , Input X Tasks Random noise Unconditional text generation Discrete attributes Topic - to - text generation Attribute - based generation Structured data Data - to - text generation Multimedia Image caption Speech recognition Text sequence Machine translation SummarizationDialoguesystem Table 1 : Major tasks and inputs for text generation . PLMs encode massive linguistic and world knowledge into vast amounts of parameters , which can enhance the under - standing of language and improve the generation quality . The idea of pretraining is inspired by human beings , i . e . , we trans - fer and reuse our old knowledge of what we have learned in the past to understand new knowledge and handle a variety of new tasks . In this way , PLMs can successfully perform on new tasks with their old experience and knowledge . Owing to the great achievements that Trans - former [ Vaswani et al . , 2017 ] has made , almost all PLMs employ the backbone of Transformer . For the text generation tasks , some of PLMs utilize the standard Transformer architecture following basic encoder - decoder framework , while the others apply a decoder - only Transformer . Next , we will introduce these two methods successively . Encoder - decoder Transformer . A standard Transformer utilizes the encoder - decoder architecture , which is composed of two stacks of Transformer blocks . The encoder is fed with an input sequence , while the decoder aims to generate the output sequence based on encoder - decoder self - attention mechanism . Based on aforementioned architecture , models such as MASS [ Song et al . , 2019 ] , T5 [ Raffel et al . , 2020 ] , and BART [ Lewis et al . , 2020 ] have improved quality of the generated text . Decoder - only Transformer . Models such as GPT [ Radford et al . , 2019 ; Brown et al . , 2020 ] and CTRL [ Keskar et al . , 2019 ] employ a single Transformer decoder blocks , which is typically used for language modeling . They apply unidirectional self - attention masking that each token can only attend to previous tokens . Besides language modeling , several works also utilize the decoder - only achitecture to generate text conditioned on in - put text . However , these models do not have an indepen - dent module to encode input sequence . Interestingly , they concatenate the input and output sequence with a special token ( e . g . , “ [ SEP ] ” ) and employ a novel seq2seq mask - ing [ Dong et al . , 2019 ] that each token in the input sen - tence can attend to each other and generated tokens can at - tend to all input tokens and previous generate ones . Com - pared to unidirectional masking , seq2seq masking is a natural way for decoder - only PLMs to solve conditional generation tasks , which is similar to the encoder - decoder architecture . Raffel et al . [ 2020 ] has researched the performance between the above two methods and made a conclusion that the addi - tion of an explicit encoder - decoder attention is beneﬁcial . The core of text generation tasks is to learn the semantic mappings from input to output . On one hand , different tasks will correspond to a variety of input data , and we need to develop special techniques to model different data types . On the other hand , the generated text should satisfy important properties in order to cope with different task requirements . Next , we discuss the recent advances with respect to the two aspects , i . e . , input and output . 4 Modeling Different Data Types from Input As discussed in Section 2 , different text generation tasks usu - ally involve speciﬁc kinds of input . In this section , we will introduce three main kinds of input for text generation , i . e . , unstructured input , structured input , and multimedia input , and discuss how to model these input data in PLMs . 4 . 1 Unstructured Input In NLP research , most of studies focus on modeling un - structured text input ( e . g . , sentence , paragraph , and docu - ment ) . To generate satisfactory output text , it requires an excellent capacity of language understanding beyond sur - face meaning of individual words in the input text . Thus , Liu and Lapata [ 2019 ] and Zheng and Lapata [ 2019 ] em - ployed PLMs ( e . g . , BERT [ Devlin et al . , 2019 ] ) as text en - coder for condensing text into low - dimensional vectors while preserving most of its meaning . Compared with traditional shallow neural models ( e . g . , CNN ) , PLMs have a large num - ber of parameters encoding massive world knowledge , which is potentially beneﬁcial to capture the core meaning of text . In some cases , the input text might be a long document consisting of several sentences and paragraphs . For PLMs trained on sentences or short paragraphs , they are less ca - pable of accurately modeling long - range dependencies in a document . Considering this challenge , Zhang et al . [ 2019b ] and Xu et al . [ 2020b ] proposed hierarchical BERT to learn interactions between sentences with self - attention for docu - ment encoding . Besides , for capturing inter - sentential rela - tions , DiscoBERT [ Xu et al . , 2020a ] stacked graph convolu - tional network ( GCN ) on top of BERT to model structural dis - course graphs . By directly operating on the discourse units , DiscoBERT retains capacities to include more concepts or contexts , leading to more concise and informative output text . We observe that most recent PLMs are pretrained on En - glish text . While , many multilingual generation tasks such as machine translation involve multiple languages and cer - tain languages are low - resource . This challenge hinders the wide application of monolingual PLMs to multilingual text generation tasks . Therefore , Conneau and Lample [ 2019 ] proposed to learn cross - lingual language models ( XLMs ) for multilingual language understanding . Based on cross - lingual PLMs , text generation models can still obtain ef - fective input word embeddings even in a low - resource lan - guage [ Wada and Iwata , 2018 ] . 4 . 2 Structured Input Structured data ( e . g . , graph and table ) is also a critical kind of input for text generation in many real - world applications such as weather report generation . However , in real - world scenario , it is difﬁcult to collect a large amount of labelled structured data with ground - truth text for training . Since pre - trained on large - scale corpus , PLMs encode a large amount of linguistic knowledge and show excellent few - shot capabili - ties in many tasks . Motivated by this , Chen et al . [ 2020b ] and Gong et al . [ 2020 ] explored incorporating PLMs for data - to - text generation , especially in few - shot settings . When applying PLMs to structured data , a major challenge is how to feed structured data into PLMs , which are originally designed for sequential text . To adapt to the sequential na - ture of PLMs , Ribeiro et al . [ 2020 ] and Mager et al . [ 2020 ] linearized input knowledge graph ( KG ) and abstract mean - ing representation ( AMR ) graph into a sequence of triples , Li et al . [ 2021b ] introduced an additional graph encoder to encode the input KG , and Gong et al . [ 2020 ] employed a template - based method to serialize input table into text se - quence . For example , the attribute - value pair “ name : jack reynolds ” will be serialized as a sentence “ name is jack reynolds ” . However , direct linearization will lose the struc - tural information of original data , which may lead to gener - ating unfaithful text about data . Thus , in addition to gener - ating faithful text , Gong et al . [ 2020 ] proposed an auxiliary reconstruction task for recovering the structural information of input data , which can enhance the capacity of modeling structural information . In general , the output text should retain as much as impor - tant information from structured data . Therefore , to gener - ate high - ﬁdelity text adhereing to input , the pointer generator mechanism [ See et al . , 2017 ] is adopted to copy words from input knowledge data [ Chen et al . , 2020b ] . Through ground - ing PLMs on external knowledge , it is likely to endow a gen - erative model with both rich knowledge and good generaliza - tion ability . Besides , Gong et al . [ 2020 ] proposed a content matching loss for measuring the distance between the infor - mation in input data and the output text . 4 . 3 Multimedia Input In addition to the above textual data , several attempts have been made to take as input multimedia data ( e . g . , image , video , and speech ) such as image caption and speech recognition . Both VideoBERT [ Sun et al . , 2019b ] and CBT [ Sun et al . , 2019a ] conducted pretraining for the video caption task . While , they performed pretraining only for the BERT - based encoder to learn bidirectional joint distribu - tions over sequences of visual and linguistic tokens . So they have to train a separate video - to - text decoder , which tends to cause a pretrain - ﬁnetune discrepancy . In contrast , Uni - ﬁed VLP [ Zhou et al . , 2020 ] used a shared multi - layer Trans - former network for both encoding and decoding . Following UniLM [ Dong et al . , 2019 ] , they pretrained the model on two masked language modeling ( MLM ) tasks , like cloze tasks de - signed for sequence - to - sequence LM . Inspired by generative pretraining objectives in GPT , Xia et al . [ 2020 ] proposed a cross - modal pretrained model ( XGPT ) by taking images as inputs and using the image caption task as the basic genera - tive task in the pretraining stage . Besides image and video , speech recognition is also hun - gry for human - transcripted supervised data . So a number of unsupervised and semi - supervised methods are developed to integrate PLMs for weakly - supervised learning . For exam - ple , Fan et al . [ 2019 ] proposed an unsupervised approach to pretraining encoder - decoder model with unpaired speech and transcripts . Two pretraining stages are used to extract acous - tic and linguistic information with speech and transcripts , which is useful for downstream speech recognition task . 5 Satisfying Special Properties for Output Text In different text generation tasks , the generated text should satisfy several key properties . In this section , we will intro - duce three key properties in text generation , i . e . , relevance , faithfulness , and order - preservation . Relevance . According to the linguistic litera - tures [ Li et al . , 2021c ] , in text generation , relevance refers that the topics in output text is highly related to the input text . A representative example is the task of dialogue systems , which requires the generated response to be relevant to the input dialogue history . In addition to the dialogue history , a condition corresponding to the type of response might be also provided as an external input such as the topic of response and the persona of speaker . The generated responses should also be relevant to the condition . Recently , due to the absence of long - term memory , RNN - based models still tend to generate irrelevant output text and lack consistency with input . Therefore , through applying PLMs to the task of dialogue systems , TransferTransfo [ Wolf et al . , 2019 ] and DialoGPT [ Zhang et al . , 2020 ] were able to generate more relevant and context - consistent responses than traditional RNN - based models . Furthermore , to generalize to various types of conditions , Zeng and Nie [ 2020 ] utilized elaborated condition blocks to incorporate external conditions . They used BERT for both encoder and decoder by utilizing different input representa - tions and self - attention masks to distinguish the source and target sides of dialogue . On the target ( generation ) side , a new attention routing mechanism is adopted to generate context - related words . Similar approaches have been used in non - conditioned dialogue [ Bao et al . , 2020 ] . Faithfulness . Similarly , faithfulness is also a critical prop - erty of text , which means the content in generated text should not contradict the facts in input text . Sometimes , it further means the generated text is in accord with the world facts . A representative example is the task of text summarization , which aims to generate faithful text representing the most im - portant information within the original content . Pretrained on large collections of text , PLMs are potentially beneﬁcial to generate faithful text by utilizing background knowledge . Rothe et al . [ 2020 ] experimented with a large number of set - tings to initialize the encoder and decoder with three out - standing PLMs , i . e . , BERT , GPT and RoBERTa . With pre - training , the models are more aware of the domain character - istics and less prone to language model vulnerabilities . Con - sequently , they are more conﬁdent in predicting tokens from the document , hence , improving faithfulness . To improve the level of faithfulness of summary , Kryscinski et al . [ 2018 ] proposed to decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowl - edge about language generation . Also , to generate faith - ful text in different target domains , Yang et al . [ 2020b ] ﬁne - tuned PLMs on target domains through theme modeling loss . The role of the theme modeling module is to make the gener - ated summary semantically close to the original article . Order - preservation . In NLP area , order - preservation de - notes that the order of semantic units ( word , phrase , etc . ) in both input and output text is consistent . The most representative example is the task of machine translation . When translating from source language to target language , keeping the order of phrases consistent in source language and target language will ensure the accuracy of the trans - lation results to some extent . One line of research to achieve the order - preservation property is to perform seman - tic alignment in machine translation . Yang et al . [ 2020a ] proposed Code - Switching Pre - training ( CSP ) for machine translation . They extracted the word - pair alignment infor - mation from the source and target language , and then ap - plied the extracted alignment information to enhance order - preserving . Besides , it is more common to perform trans - lation across multiple languages , called multilingual ma - chine translation [ Conneau and Lample , 2019 ] . However , little work can effectively enhance order - preservation for any pairs of languages . Thus , Lin et al . [ 2020 ] proposed mRASP , an approach to pretraining a universal multilin - gual machine translation model . The key to mRASP is the technique of randomly aligned substitution , which enforces words and phrases with similar meanings across multiple languages to be aligned in the representation space . Also , Wada and Iwata [ 2018 ] focused on aligning word represen - tations of each language , making it possible to preserve the word order consistent cross multiple languages . 6 Fine - tuning Strategies for Text Generation For text generation with PLMs , a key factor is how to design suitable ﬁne - tuning strategies . In this part , we review several commonly - used ﬁne - tuning strategies from different views . 6 . 1 Data View When applying PLMs to text generation tasks especially in a new domain , how to design suitable and effective ﬁne - tuning strategies adapting to the characteristics of new domain is an important consideration . Few - shot Learning . In many text generations , it is difﬁcult and expensive to obtain sufﬁcient annotated data . Owing to the success of pretraining , PLMs can encode massive linguis - tic and world knowledge , which provides an effective solution to data scarcity . A commonly adopted approach is to plug the existing module with pretrained parameters . Then we ﬁne - tune it with a few , one , or even no examples for the studied task , which are so - called few - shot , one - shot and zero - shot , respectively . For example in multilingual translation , some low - resource languages lack sufﬁcient parallel corpus . XLM Data Categories Methods Input Unstructured BERT acts as text encoders [ Liu and Lapata , 2019 ; Zheng and Lapata , 2019 ] , hierarchical PLMs for document modeling [ Zhang et al . , 2019b ; Xu et al . , 2020b ] , and cross - lingual PLMs for multilingual input text [ Conneau and Lample , 2019 ; Wada and Iwata , 2018 ] . Structured Linearize KG and AMR graph as triple sequence [ Mager et al . , 2020 ; Ribeiro et al . , 2020 ] , graph encoder for encoding KG [ Li et al . , 2021b ] , and serialize table into template - based text sequence [ Gong et al . , 2020 ] . Multimedia Video caption [ Sun et al . , 2019b ; Sun et al . , 2019a ] , image caption [ Xia et al . , 2020 ] , and speech recognition [ Fan et al . , 2019 ] . Output Relevance Fine - tune PLMs in dialogue systems for generating more relevant and context related responses [ Wolf et al . , 2019 ; Zhang et al . , 2020 ] , and generalize to any type of input conditions based on BERT [ Zeng and Nie , 2020 ] . Faithfulness Improve faithfulness with several PLMs [ Rothe et al . , 2020 ] , retrieve relevant parts from input and incorporate prior knowledge of PLMs [ Kryscinski et al . , 2018 ] , and generate faithful text in different target domains through theme modeling loss [ Yang et al . , 2020b ] . Order - preservation Word - pair alignment [ Yang et al . , 2020a ] , a universal multilingual machine translation model [ Lin et al . , 2020 ] , and word representation alignment [ Wada and Iwata , 2018 ] . Table 2 : Categories of input types and output properties for text generation . [ Conneau and Lample , 2019 ] proposed to learn cross - lingual language models and can leverage the knowledge learned in high - resource languages to low - resource languages . Us - ing the method proposed in Section 4 , few - shot learning can also be applied in data - to - text tasks , such as table - to - text generation [ Chen et al . , 2020b ; Gong et al . , 2020 ] and KG - to - text generation [ Li et al . , 2021b ] . Chen et al . [ 2020b ] directly fed GPT - 2 with a small amount of serialized attribute - value pairs and Gong et al . [ 2020 ] further applied multi - ple tasks to better leverage structured information of tables . Moreover , Li et al . [ 2021b ] proposed representation align - ment to bridge the semantic gap between KG encodings and PLMs in order to enhance the correspondence between KG and text . Domain Transfer . Equipped with vast amounts of parame - ters and pretrained on large - scale corpus , PLMs have pow - erful generalization capability . However , they still cannot directly adapt to new domains with large distribution dis - crepency from pretraining domain [ Hendrycks et al . , 2020 ] . An effective solution is to continue training PLMs on spe - ciﬁc data with pretraining objectives before ﬁne - tuning them on target tasks . Mask prediction is a widely used method , attempting to predict the masked tokens using the remaining ones . There exist several variants of masking ways in domain transfer . Zeng and Nie [ 2020 ] proposed TF - IDF based mask - ing to select more condition - related tokens to mask , in order to focus on domain features . Document masking is usually utilized in summarization task to capture document - level fea - tures of long documents [ Zhang et al . , 2019b ] . 6 . 2 Task View Besides characteristics of new domains , it is also meaningful to consider some special concerns such as language coher - ence and text ﬁdelity in speciﬁc generation tasks when ﬁne - tuning PLMs . Enhancing Coherence . To enhance the language coher - ence , an important approach is to better model language con - text during ﬁne - tuning . Models ﬁne - tuned by contrastive learning are good at distinguishing whether a sentence pair is similar or not . Through this method , PLMs are forced to un - derstand the positional or semantic relationship between two sentences , so that they can derive better representations . Next sentence prediction ( NSP ) is a commonly adopted way to judge whether two input sentences are consec - utive segments , which can be applied to summarization [ Yang et al . , 2020b ] and dialog system [ Wolf et al . , 2019 ] . Zheng and Lapata [ 2019 ] proposed to rearrange the sen - tence order according to their semantic similarities . CBT [ Sun et al . , 2019a ] proposed noise contrastive estimation ( NCE ) in cross - modal training to encourage the model to identify the correct video - text pair compared to a set of nega - tive distractors . Denoising autoencoding ( DAE ) takes the corrupted text as input and aims to recover the original text . The model ﬁne - tuned with DAE has a strong ability to understand the overall sentences and capture longer - range correlations . For exam - ple , TED [ Yang et al . , 2020b ] utilized DAE to reﬁne essential semantic information for abstractive summarization . XGPT [ Xia et al . , 2020 ] attempted to model the underlying text - image alignments using image - conditioneddenoising autoen - coding ( IDA ) , in order to force the model to reconstruct the whole sentence . Preserving Fidelity . Text ﬁdelity refers that how the gen - erated text adheres to the original input information , which is an important aspect to consider in many text generation tasks . The universal structure in PLMs is unable to retain the text ﬁ - delity in speciﬁc text generation tasks . For the table - to - text generation task , the structure information of table is required to be encoded . Gong et al . [ 2020 ] proposed to utilize multi - task learning , in order to reconstruct from table embeddings and enforce the match between table embeddings and content embeddings . Besides , the pointer generator [ See et al . , 2017 ] can be applied to KG - to - text generation to copy the entity and relation information in KG [ Chen et al . , 2020b ] . 6 . 3 Model View To enhance the quality of generated text , a key is to well train the parameters of PLMs according to task - speciﬁc data , so that PLMs can capture the semantic characteristics specially for the generation task . However , as mentioned above , task - speciﬁc data is inadequate , thus it is likely to occur the over - ﬁtting case when ﬁne - tuned on limited data . In this part , we will introduce several ﬁne - tuning methods in view of models . Gu et al . [ 2020 ] employed a ﬁxed teacher GPT to pre - serve the knowledge encoded in another ﬁne - tuned GPT . Chen et al . [ 2020a ] proposed to utilize a BERT model ( teacher ) as supervision to guide the Seq2Seq model ( student ) for better generation performance . Besides , Liu and Lapata [ 2019 ] utilized two optimizers to update the parameters of PLM and initial module separately , in order to solve the discrepancy between two modules . There also exist other ways to guide the ﬁne - tuning pro - cess . For example , Reinforcement learning can be ap - plied to directly guide models by non - differentiable metrics [ Zhang et al . , 2019a ] , such as ROUGE . Zhao et al . [ 2020 ] utilized curriculum learning to let the model learn from easy documents to hard documents . Moreover , DIALOGPT [ Zhang et al . , 2020 ] implemented a maximum mutual infor - mation ( MMI ) scoring function to alleviate generating bland , uninformative responses . 7 Conclusion and Future Outlooks This paper presents an overview of the recent advances achieved in pretrained language models for text generation . We mainly summarize the extensions of PLMs in modeling different data types in input and satisfy special text proper - ties in output . We also discussed several useful ﬁne - tuning strategies for text generation . To advance this ﬁeld , there are several promising future directions for applying PLMs to text generation . Model Extension . Although various extensions have been proposed in Section 3 , there still exist discrepancies be - tween pretraining and downstream generation tasks . For ex - ample , the “ [ MASK ] ” token in pretraining stage will not be used in ﬁne - tuning stage , which further aggravates the pretraining - ﬁnetuning discrepancy . Thus , it further desires to design an appropriate pretraining paradigm for text gen - eration . Moreover , incorporating external knowledge into PLMs during pretraining has been shown to be effective [ Zhang et al . , 2019c ] , and it is promising to investigate how to inject related knowledge for text generation . Controllable Generation . Controllable text generation with PLMs is an interesting direction but still at a very early stage . Controlling some attributes of the generated text has many useful applications such as generating positive response to pa - tients with depression in dialogue systems . However , PLMs are usually pretrained in universal corpus , which is difﬁcult to control the multi - grained attributes of the generated text ( e . g . , sentiment , topic , and coherence ) . Keskar et al . [ 2019 ] has explored text generation with control codes that govern style , content and task - speciﬁc behavior . While , these control codes are preset and coarse - grained . Future work can explore multi - grained control and develop PLMs that are sufﬁciently steerable . Model Compression . Although PLMs with large - scale pa - rameters have achieved success in text generation , these mod - els are challenging to be deployed in resource constrained environments . As a result , it is meaningful to study how to achieve competitive performance with a small number of parameters . Several methods have been proposed to com - press PLMs , such as parameter sharing [ Lan et al . , 2020 ] and knowledge distillation [ Sanh et al . , 2019 ] , whereas most of them focused on BERT - based models , and little attention has been paid to compressing PLMs for text generation . Fine - tuning Exploration . The direct intention of pretrain - ing is to distill the linguistic knowledge learned in PLMs to downstream generation tasks . And , ﬁne - tuning is the pre - dominant transfer method at present . There could be vari - ous ways to transfer knowledge from PLMs to downstream models . For example , Chen et al . [ 2020a ] exploited knowl - edge distillation by adopting BERT as teacher model and a vanilla RNN generation model as student model . Through this method , the linguistic knowledge of BERT can be dis - tilled into the downstream model . Language - agnostic PLMs . Nowadays , almost all the PLMs for text generation are mainly based on English . These PLMs will encounter challenges when dealing with non - English generation tasks . Therefore , language - agnostic PLMs are worthy to be investigated , which need to capture universal lin - guistic and semantic features across different languages . An interesting direction is how to reuse existing English - based PLMs for text generation in non - English languages . Ethical Concern . Currently , PLMs are pretrained on large - scale corpus crawled from the web without ﬁne - grained ﬁl - tering , potentially causing ethical issues such as generating private content about users . Therefore , researchers should try their best to prevent misusing PLMs . For this purpose , we can follow the key steps provided by Blank [ 2011 ] , such as identifying threats and potential impacts and assessing likeli - hood . Besides , the text generated by PLMs might be preju - diced , which is in line with the bias in training data along the dimensions of gender , race , and religion [ Brown et al . , 2020 ] . Hence , we ought to intervene PLMs for preventing such bi - ases . The research on the general approach is extensive but still preliminary for PLMs . Acknowledgement This work was partially supported by the National Key R & D Program of China under Grant No . 2020AAA0105200 , Na - tional Natural Science Foundation of China under Grant No . 61872369 and 61832017 , Beijing Academy of Artiﬁcial In - telligence ( BAAI ) under Grant No . BAAI2020ZJ0301 , Bei - jing Outstanding Young Scientist Program under Grant No . BJJWZYJH012019100020098 , the Fundamental Research Funds for the Central Universities , and the Research Funds of Renmin University of China under Grant No . 18XNLG22 and 19XNQ047 . Xin Zhao is the corresponding author . References [ Bahdanau et al . , 2015 ] Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio . Neural machine translation by jointly learning to align and translate . In ICLR , 2015 . [ Bao et al . , 2020 ] Siqi Bao , Huang He , Fan Wang , Hua Wu , Haifeng Wang , Wenquan Wu , Zhen Guo , Zhibin Liu , and Xinchao Xu . PLATO - 2 : towards building an open - domain chatbot via curriculum learning . arXiv preprint arXiv : 2006 . 16779 , 2020 . [ Blank , 2011 ] Rebecca M Blank . Guide for conducting risk assessments . 2011 . [ Brown et al . , 2020 ] Tom B . Brown , Benjamin Mann , and Nick Ryder et al . Language models are few - shot learners . In NeurIPS , 2020 . [ Chen et al . , 2020a ] Yen - Chun Chen , Zhe Gan , Yu Cheng , Jingzhou Liu , and Jingjing Liu . Distilling knowledge learned in BERT for text generation . In ACL , 2020 . [ Chen et al . , 2020b ] Zhiyu Chen , Harini Eavani , Wenhu Chen , Yinyin Liu , and William Yang Wang . Few - shot NLG with pre - trained language model . In ACL , 2020 . [ Conneau and Lample , 2019 ] Alexis Conneau and Guil - laume Lample . Cross - lingual language model pretraining . In NeurIPS , 2019 . [ Devlin et al . , 2019 ] Jacob Devlin , Ming - Wei Chang , Ken - ton Lee , and Kristina Toutanova . BERT : pre - training of deep bidirectional transformers for language understand - ing . In NAACL - HLT , 2019 . [ Dong et al . , 2019 ] Li Dong , Nan Yang , Wenhui Wang , Furu Wei , Xiaodong Liu , Yu Wang , Jianfeng Gao , Ming Zhou , and Hsiao - Wuen Hon . Uniﬁed language model pre - training for natural language understanding and genera - tion . In NeurIPS , 2019 . [ Fan et al . , 2019 ] Zhiyun Fan , Shiyu Zhou , and Bo Xu . Un - supervised pre - training for sequence to sequence speech recognition . CoRR , arXiv preprint arXiv : 1910 . 12418 , 2019 . [ Gehring et al . , 2017 ] Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N . Dauphin . Convo - lutional sequence to sequence learning . In ICML , 2017 . [ Gong et al . , 2020 ] Heng Gong , Yawei Sun , Xiaocheng Feng , Bing Qin , Wei Bi , Xiaojiang Liu , and Ting Liu . Tablegpt : Few - shot table - to - text generation with table structure reconstruction and content matching . In COL - ING , 2020 . [ Gu et al . , 2020 ] Jing Gu , Qingyang Wu , Chongruo Wu , Weiyan Shi , and Zhou Yu . A tailored pre - training model for task - oriented dialog generation . arXiv preprint arXiv : 2004 . 13835 , 2020 . [ Guan et al . , 2020 ] Wang Guan , Ivan Smetannikov , and Man Tianxing . Survey on automatic text summarization and transformer models applicability . In CCRIS , 2020 . [ Hendrycks et al . , 2020 ] Dan Hendrycks , Xiaoyuan Liu , Eric Wallace , Adam Dziedzic , Rishabh Krishnan , and Dawn Song . Pretrained transformers improve out - of - distribution robustness . In ACL , 2020 . [ Keskar et al . , 2019 ] Nitish Shirish Keskar , Bryan McCann , Lav R . Varshney , Caiming Xiong , and Richard Socher . CTRL : A conditional transformer language model for con - trollable generation . arXiv preprint arXiv : 1909 . 05858 , 2019 . [ Kryscinski et al . , 2018 ] Wojciech Kryscinski , Romain Paulus , Caiming Xiong , and Richard Socher . Improving abstraction in text summarization . In EMNLP , 2018 . [ Lan et al . , 2020 ] Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Sori - cut . ALBERT : A lite BERT for self - supervised learning of language representations . In ICLR , 2020 . [ Lewis et al . , 2020 ] Mike Lewis , Yinhan Liu , and Na - man Goyal et al . BART : denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension . In ACL , 2020 . [ Li et al . , 2019 ] Junyi Li , Wayne Xin Zhao , Ji - Rong Wen , and Yang Song . Generating long and informative reviews with aspect - aware coarse - to - ﬁne decoding . In ACL , pages 1969 – 1979 , 2019 . [ Li et al . , 2020 ] Junyi Li , Siqing Li , Wayne Xin Zhao , Gaole He , Zhicheng Wei , Nicholas Jing Yuan , and Ji - Rong Wen . Knowledge - enhancedpersonalized review generation with capsule graph neural network . In CIKM , pages 735 – 744 , 2020 . [ Li et al . , 2021a ] Junyi Li , Tianyi Tang , Gaole He , Jinhao Jiang , Xiaoxuan Hu , Puzhao Xie , Zhipeng Chen , Zhuo - hao Yu , Wayne Xin Zhao , and Ji - Rong Wen . Textbox : A uniﬁed , modularized , and extensible framework for text generation . arXiv preprint arXiv : 2101 . 02046 , 2021 . [ Li et al . , 2021b ] Junyi Li , Tianyi Tang , Wayne Xin Zhao , Zhicheng Wei , Nicholas Jing Yuan , and Ji - Rong Wen . Few - shot knowledge graph - to - text generation with pre - trained language models . In Findings of ACL , 2021 . [ Li et al . , 2021c ] Junyi Li , Wayne Xin Zhao , Zhicheng Wei , Nicholas Jing Yuan , and Ji - Rong Wen . Knowledge - based review generation by coherence enhanced text planning . In SIGIR , 2021 . [ Lin et al . , 2020 ] Zehui Lin , Xiao Pan , Mingxuan Wang , Xipeng Qiu , Jiangtao Feng , Hao Zhou , and Lei Li . Pre - training multilingual neural machine translation by lever - aging alignment information . In EMNLP , 2020 . [ Liu and Lapata , 2019 ] Yang Liu and Mirella Lapata . Text summarization with pretrained encoders . In EMNLP , 2019 . [ Mager et al . , 2020 ] Manuel Mager , Ram´on Fernandez As - tudillo , Tahira Naseem , Md . Arafat Sultan , Young - Suk Lee , Radu Florian , and Salim Roukos . Gpt - too : A language - model - ﬁrst approach for amr - to - text generation . In ACL , 2020 . [ Peters et al . , 2018 ] Matthew E . Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . Deep contextualized word representations . In NAACL - HLT , 2018 . [ Qiu et al . , 2020 ] Xipeng Qiu , Tianxiang Sun , Yige Xu , Yunfan Shao , Ning Dai , and Xuanjing Huang . Pre - trained models for natural language processing : A survey . arXiv preprint arXiv : 2003 . 08271 , 2020 . [ Radford et al . , 2019 ] Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . Language models are unsupervised multitask learners . 2019 . [ Raffel et al . , 2020 ] Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J . Liu . Exploring the limits of transfer learning with a uniﬁed text - to - text transformer . JMLR , 2020 . [ Ribeiro et al . , 2020 ] Leonardo F . R . Ribeiro , Martin Schmitt , Hinrich Sch¨utze , and Iryna Gurevych . Inves - tigating pretrained language models for graph - to - text generation . arXiv preprint arXiv : 2007 . 08426 , 2020 . [ Rothe et al . , 2020 ] Sascha Rothe , Shashi Narayan , and Ali - aksei Severyn . Leveraging pre - trained checkpoints for se - quence generation tasks . TACL , 2020 . [ Sanh et al . , 2019 ] Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf . Distilbert , a distilled ver - sion of BERT : smaller , faster , cheaper and lighter . arXiv preprint arXiv : 1910 . 01108 , 2019 . [ See et al . , 2017 ] Abigail See , Peter J . Liu , and Christo - pher D . Manning . Get to the point : Summarization with pointer - generator networks . In ACL , 2017 . [ Song et al . , 2019 ] Kaitao Song , Xu Tan , Tao Qin , Jianfeng Lu , and Tie - Yan Liu . MASS : masked sequence to se - quence pre - training for language generation . In ICML , 2019 . [ Sun et al . , 2019a ] Chen Sun , Fabien Baradel , Kevin Mur - phy , and Cordelia Schmid . Contrastive bidirectional transformer for temporal representation learning . arXiv preprint arXiv : 1906 . 05743 , 2019 . [ Sun et al . , 2019b ] Chen Sun , Austin Myers , Carl Vondrick , Kevin Murphy , and Cordelia Schmid . Videobert : A joint model for video and language representation learning . In ICCV , 2019 . [ Vaswani et al . , 2017 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . Attention is all you need . In NIPS , 2017 . [ Wada and Iwata , 2018 ] Takashi Wada and Tomoharu Iwata . Unsupervised cross - lingual word embedding by mul - tilingual neural language models . arXiv preprint arXiv : 1809 . 02306 , 2018 . [ Wolf et al . , 2019 ] Thomas Wolf , Victor Sanh , Julien Chau - mond , and Clement Delangue . Transfertransfo : A transfer learning approach for neural network based conversational agents . arXiv preprint arXiv : 1901 . 08149 , 2019 . [ Xia et al . , 2020 ] Qiaolin Xia , Haoyang Huang , Nan Duan , Dongdong Zhang , Lei Ji , Zhifang Sui , Edward Cui , Taroon Bharti , Xin Liu , and Ming Zhou . XGPT : cross - modal gen - erative pre - training for image captioning . arXiv preprint arXiv : 2003 . 01473 , 2020 . [ Xu et al . , 2020a ] Jiacheng Xu , Zhe Gan , Yu Cheng , and Jingjing Liu . Discourse - aware neural extractive text sum - marization . In ACL , 2020 . [ Xu et al . , 2020b ] Shusheng Xu , Xingxing Zhang , Yi Wu , Furu Wei , and Ming Zhou . Unsupervised extractive sum - marization by pre - training hierarchical transformers . In EMNLP , 2020 . [ Yang et al . , 2020a ] Zhen Yang , Bojie Hu , Ambyera Han , Shen Huang , and Qi Ju . CSP : code - switching pre - training for neural machine translation . In EMNLP , 2020 . [ Yang et al . , 2020b ] Ziyi Yang , Chenguang Zhu , Robert Gmyr , Michael Zeng , Xuedong Huang , and Eric Darve . TED : A pretrained unsupervised summarization model with theme modeling and denoising . In EMNLP ( Find - ings ) , 2020 . [ Zaib et al . , 2020 ] Munazza Zaib , Quan Z . Sheng , and Wei Emma Zhang . A short survey of pre - trained lan - guage models for conversational AI - A new age in NLP . In ACSW , 2020 . [ Zeng and Nie , 2020 ] Yan Zeng and Jian - Yun Nie . General - ized conditioned dialogue generation based on pre - trained language model . arXiv preprint arXiv : 2010 . 11140 , 2020 . [ Zhang et al . , 2019a ] Haoyu Zhang , Jingjing Cai , Jianjun Xu , and Ji Wang . Pretraining - based natural language gen - eration for text summarization . In CoNLL , 2019 . [ Zhang et al . , 2019b ] Xingxing Zhang , Furu Wei , and Ming Zhou . HIBERT : document level pre - training of hierarchi - cal bidirectional transformers for document summariza - tion . In ACL , 2019 . [ Zhang et al . , 2019c ] Zhengyan Zhang , Xu Han , Zhiyuan Liu , Xin Jiang , Maosong Sun , and Qun Liu . ERNIE : en - hanced language representation with informative entities . In ACL , 2019 . [ Zhang et al . , 2020 ] Yizhe Zhang , Siqi Sun , Michel Galley , Yen - Chun Chen , Chris Brockett , Xiang Gao , Jianfeng Gao , Jingjing Liu , and Bill Dolan . DIALOGPT : Large - scale generative pre - training for conversational response generation . In ACL , 2020 . [ Zhao et al . , 2020 ] Xueliang Zhao , Wei Wu , Can Xu , Chongyang Tao , Dongyan Zhao , and Rui Yan . Knowledge - grounded dialogue generation with pre - trained language models . In EMNLP , 2020 . [ Zheng and Lapata , 2019 ] Hao Zheng and Mirella Lapata . Sentence centrality revisited for unsupervised summariza - tion . In ACL , 2019 . [ Zhou et al . , 2020 ] Luowei Zhou , Hamid Palangi , Lei Zhang , Houdong Hu , Jason J . Corso , and Jianfeng Gao . Uniﬁed vision - language pre - training for image captioning and VQA . In AAAI , 2020 .