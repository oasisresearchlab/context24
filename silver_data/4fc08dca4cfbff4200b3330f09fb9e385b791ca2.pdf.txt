D EEP L EARNING T HEORY R EVIEW : A N O PTIMAL C ONTROL AND D YNAMICAL S YSTEMS P ERSPECTIVE Guan - Horng Liu , Evangelos A . Theodorou Autonomous Control and Decision Systems Laboratory School of Aerospace Engineering Georgia Institute of Technology Atlanta , GA 30332 , USA { ghliu , evangelos . theodorou } @ gatech . edu A BSTRACT Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years , yet a uniﬁed framework re - mains relatively limited . In this article , we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control . By viewing deep neural networks as discrete - time nonlin - ear dynamical systems , we can analyze how information propagates through lay - ers using mean ﬁeld theory . When optimization algorithms are further recast as controllers , the ultimate goal of training processes can be formulated as an opti - mal control problem . In addition , we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms . This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics . It also provides a principled way for hyper - parameter tuning when optimal control theory is introduced . Our framework ﬁts nicely with super - vised learning and can be extended to other learning problems , such as Bayesian learning , adversarial training , and speciﬁc forms of meta learning , without efforts . The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory . 1 I NTRODUCTION Deep learning is one of the most rapidly developing areas in modern artiﬁcial intelligence with tremendous impact to different industries ranging from the areas of social media , health and biomed - ical engineering , robotics , autonomy and aerospace systems . Featured with millions of parameters yet without much hand tuning or domain - speciﬁc knowledge , Deep Neural Networks ( DNNs ) match and sometimes exceed human - level performance in complex problems involving visual synthesizing ( Krizhevsky et al . , 2012 ) , language reasoning ( Srivastava & Salakhutdinov , 2012 ) , and long - horizon consequential planning ( Silver et al . , 2016 ) . The remarkable practical successes , however , do not come as a free lunch . Algorithms for training DNNs are extremely data - hungry . While insufﬁcient data can readily lead to memorizing irrelevant features ( Geirhos et al . , 2018 ) , data imbalance may cause severe effects such as imposing improper priors implicitly ( Wang et al . , 2016 ) . Although automating tuning for millions of parameters alleviates inductive biases from traditional engineer - ing , it comes with the drawback of making interpretation and analysis difﬁcult . Furthermore , DNN is known for being vulnerable to small adversarial perturbations ( Goodfellow et al . , 2014 ) . In fact , researchers have struggled to improve its robustness beyond inﬁnite norm ball attacks ( Athalye & Carlini , 2018 ) . Finally , while the deep learning community has developed recipes related to the choice of the underlying organization of a DNN , the process of the overall architectural design lacks solid theoretical understanding and remains a fairly ad - hock process . Preprint . Work in progress . a r X i v : 1908 . 10920v1 [ c s . L G ] 28 A ug 2019 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective The aforementioned issues highlight the need towards the development of a theory for deep learning which will provide a scientiﬁc methodology to design DNNs architectures , robustify their perfor - mance against external attacks / disturbances , and enable the development the corresponding training algorithms . Given this need , our objective in this paper is to review and present in a systematic way work towards the discovery of deep learning theory . This work relies on concepts drawn primarily from the areas of dynamical systems and optimal control theory , and its connections to information theory and statistical physics . Theoretical understanding of DNN training from previous works has roughly followed two streams : deep latent representation and stochastic optimization . On the topic of deep representations , the composition of afﬁne functions , with element - wise nonlinear activations , plays a crucial role in au - tomatic feature extraction ( Simonyan et al . , 2013 ) by constructing a chain of differentiable process . An increase in the depth of a NN architecture has the effect of increasing its expressiveness expo - nentially ( Poole et al . , 2016 ) . This naturally yields a highly over - parametrized model , whose loss landscape is known for a proliferation of local minima and saddle points ( Dauphin et al . , 2014 ) . However , the over - ﬁtting phenomena , suggested by the bias - variance analysis , has not been ob - served during DNN training ( Zhang et al . , 2016 ) . In practice , DNN often generalizes remarkably well on unseen data when initialized properly ( Sutskever et al . , 2013 ) . Generalization of highly over - parametrized models cannot be properly explained without consider - ing stochastic optimization algorithms . Training DNN is a non - convex optimization problem . Due to its high dimensionality , most practically - used algorithms utilize ﬁrst - order derivatives with aids of adaptation and momentum mechanisms . In fact , even a true gradient can be too expensive to com - pute on the ﬂy ; therefore only an unbiased estimation is applied at each update . Despite these ap - proximations that are typically used to enable applicability , ﬁrst - order stochastic optimization is sur - prisingly robust and algorithmically stable ( Hardt et al . , 2015 ) . The stochasticity stemmed from es - timating gradients is widely believed to perform implicit regularization ( Chaudhari & Soatto , 2018 ) , guiding parameters towards ﬂat plateaus with lower generalization errors ( Keskar et al . , 2016 ) . First - order methods are also proven more efﬁcient to escape from saddle points ( Lee et al . , 2017b ) , whose number grows exponentially with model dimensionality ( Dauphin et al . , 2014 ) . Research along this line provides a fundamental understanding on training dynamics and convergence property , despite the analysis is seldom connected to the deep representation viewpoint . How do the two threads of deep latent organization and stochastic optimization interplay with each other and what are the underlying theoretical connections ? These are questions that have not been well - explored and are essential towards the development of a theory for Deep Learning . Indeed , study of stochastic optimization dynamics often treats DNN merely as a black - box . This may be insufﬁcient to describe the whole picture . When using back - propagation ( LeCun et al . , 1990 ) to obtain ﬁrst - order derivatives , the backward dynamics , characterized by the compositional structure , rescales the propagation made by optimization algorithms , which in return leads to different repre - sentation at each layer . Frameworks that are able to mathematically characterize these compounding effects will provide more nuanced statements . One of such attempts has been information bottle - neck theory ( Shwartz - Ziv & Tishby , 2017 ) , which describes the dynamics of stochastic optimization using information theory , and connects it to optimal representation via the bottleneck principle . An - other promising branch from Du et al . ( 2018a ; b ) showed that the speciﬁc representation , i . e . the Gram matrix , incurred from gradient descent ( GD ) characterizes the dynamics of the prediction space and can be used to prove global optimality . These arguments , however , have been limited to either certain architectures ( Saxe et al . , 2018 ) or noise - free optimization 1 . In this review , we provide a dynamical systems and optimal control perceptive to DNNs in an effort to systematize the alternative approaches and methodologies . This allows us to pose and answer the following questions : ( i ) at which state should the training trajectory start , i . e . how should we initialize the weights or hyper - parameters , ( ii ) through which path , in a distribution sense , may the trajectory traverse , i . e . can we give any prediction of training dynamics on average , ( iii ) to which ﬁxed point , if exists , may the trajectory converge , and ﬁnally ( iv ) the stability and generalization property at that ﬁxed point . In the context of deep learning , these can be done by recasting DNNs 1 We note that global optimality for stochastic gradient descent has been recently proven in Zou et al . ( 2018 ) ; Allen - Zhu et al . ( 2018 ) , yet their convergence theories rely on certain assumptions on the data set in order to have the Frobenius norm of the ( stochastic ) gradient lower - bounded . This is in contract the least eigenvalue of the prediction dynamics in Du et al . ( 2018a ; b ) , which is more related to the dynamical analysis in this review . 2 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective and optimization algorithms as ( stochastic ) dynamical systems . Advanced tools from signal pro - cessing , mean - ﬁeld theory , and stochastic calculus can then be applied to better reveal the training properties . We can also formulate an optimal control problem upon the derived dynamics to provide principled guidance for architecture and algorithm design . The dynamical and control viewpoints ﬁt naturally with supervised learning and can readily extend to other learning schemes , such as Bayesian learning , adversarial training , and speciﬁc forms of meta learning , This highlights the potential to provide more theoretical insights . The article is organized as follows . In Sec . 2 , we will go over recent works related to the dynamical viewpoint . Sec . 3 and 4 demonstrate how we can recast DNNs and stochastic optimizations as dynamical systems , then apply control theory to them . In Sec . 5 , we extend our analysis to other learning problems . Finally , we conclude our work and discuss some future directions in Sec . 6 . Notation : We will denote h l and x l as the ( pre - ) activation at layer l ( x 0 ≡ x for simplicity ) . Mapping to the next layer obeys h l = W ( x l ; θ l ) and x l + 1 = φ ( h l ) , where φ is a nonlinear ac - tivation functional and W is an afﬁne transform parametrized by θ l ∈ R m . The full parameter space across all layers is denoted θ ≡ { θ l } L − 1 l = 0 ∈ R ¯ m . Given a data set D : = { ( x ( i ) , y ( i ) ) } i , where x ∈ R n and y ∈ R d , we aim to minimize a loss Φ ( · ) , or equivalently the cost J ( · ) from the control viewpoint . The element of the vector / matrix are respectively denoted as a ( i ) ≡ a i and A ( i , j ) ≡ A ( i , j ) . We will follow the convention (cid:104)· , ·(cid:105) to denote the inner product of two vectors , with (cid:104) f ( · ) , g ( · ) (cid:105) µ : = (cid:82) f ( w ) T g ( w ) µ ( d w ) as its generalization to the inner product of two functionals weighted by a probability density . We will always use the subscript t to denote the dynamics . De - pending on the context , it can either mean propagation through DNN ( Sec . 3 ) or training iterations ( Sec . 4 ) . 2 R ELATED W ORK Mean Field Approximation & Gaussian Process : Mean ﬁeld theory allows us to describe distri - butions of activations and pre - activations over an ensemble of untrained DNNs in an analytic form . The connection was adapted in Poole et al . ( 2016 ) to study how signals propagate through layers at the initialization stage . It implies an existence of an order - to - chaos transition as a function of parameter statistics . While networks in the phase of order suffer from saturated information and vanished gradients , in the chaotic regime expressions of networks grow exponentially with depth , and exploded gradients can be observed . This phase transition diagram , formally characterized in Schoenholz et al . ( 2016 ) , provides a necessary condition towards network trainability and deter - mines an upper bound on the number of layers allowable for information to pass through . This analysis has been successfully applied to most commonly - used architectures for critical initializa - tion , including FCN , CNN , RNN , LSTM , ResNet ( Schoenholz et al . , 2016 ; Chen et al . , 2018a ; Gilboa et al . , 2019 ; Xiao et al . , 2018 ; Yang & Schoenholz , 2017 ) . In addition , it can also be used to provide geometric interpretation by estimating statistical properties of Fisher information ( Karakida et al . , 2018 ; Pennington et al . , 2018 ) . It is worth noticing that all aforementioned works require the limit of layer width and i . i . d . weight priors in order to utilize the Gaussian approximation . Indeed , the equivalence between DNN and Gaussian process has long been known for single - layer neural networks ( Williams , 1997 ) and extended to deeper architectures recently ( Matthews et al . , 2018 ; Garriga - Alonso et al . , 2018 ) . The resulting Bayesian viewpoint enables uncertainty estimation and accuracy prediction at test time ( Lee et al . , 2017a ) . Implicit Bias in Stochastic Gradient Descent ( SGD ) : There has been commensurate interest in studying the implicit bias stemmed from stochastic optimization . Even without stochasticity , vanilla GD algorithms are implicitly regulated as they converge to max - margin solutions for both linear predictors ( Soudry et al . , 2018 ; Gunasekar et al . , 2018a ) and over - parametrized models , e . g . multi - layers linear networks ( Gunasekar et al . , 2018b ; Ji & Telgarsky , 2018 ) . When the stochasticity is introduced , a different regularization effect has been observed ( Wu et al . , 2018 ) . This implicit regu - larization plays a key role in explaining why DNNs generalize well despite being over - parametrized ( Zhang et al . , 2016 ; Neyshabur et al . , 2017 ) . Essentially , stochasticity pushes the training dynamics away from sharp local minima ( Zhu et al . , 2018 ) and instead guides it towards ﬂat plateaus with lower generalization errors ( Keskar et al . , 2016 ) . An alternative view suggests a convolved , i . e . smoothening , effect on the loss function throughout training ( Kleinberg et al . , 2018 ) . The recent work from Chaudhari & Soatto ( 2018 ) provides mathematical intuitions by showing that SGD per - 3 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective forms variational inference under certain approximations . Algorithmically , Chaudhari et al . ( 2016 ; 2018 ) proposed a surrogate loss that explicitly biases SGD dynamics towards ﬂat local minima . The corresponding algorithm relates closely to stochastic gradient Langevin dynamics , a compu - tationally efﬁcient sampling technique originated from Markov Chain Monte Carlo ( MCMC ) for large - scale problems ( Teh et al . , 2016 ; Li et al . , 2016 ) . Information Theory & Statistical Physics : Research along this direction studies the dynamics of Markovian stochastic process and its effect on the deep representation at an ensemble level . For in - stance , the Information Bottleneck ( IB ) theory ( Tishby & Zaslavsky , 2015 ; Shwartz - Ziv & Tishby , 2017 ) studies the training dynamics on the Information Plane described by the mutual information of layer - wise activations . The principle of information bottleneck mathematically characterizes the phase transition from memorization to generalization , mimicking the critical learning periods in bi - ological neural systems ( Achille et al . , 2019 ) . Applying the same information Lagrangian to DNN’s weights has revealed intriguing properties of the deep representation , such as invariance , disentan - glement and generalization ( Achille & Soatto , 2018 ; 2019 ) , despite a recent debate in Saxe et al . ( 2018 ) arguing the inability of the ﬁndings in references ( Tishby & Zaslavsky , 2015 ; Shwartz - Ziv & Tishby , 2017 ) to generalize beyond certain architectures . In Valle - Perez et al . ( 2019 ) , similar statements on the implicit bias has been drawn from the Algorithmic Information Theory ( AIT ) , suggesting the parameter - function map of DNNs is exponentially biased towards simple functions . The information theoretic viewpoint is closely related to statistical physics . In Goldt & Seifert ( 2017a ; b ) , an upper bound mimicking the second law of stochastic thermodynamics was derived for single - layer networks on binary classiﬁcation tasks . In short , generalization of a network to unseen datum is bounded by the summation of the Shannon entropy of its weights and a term that cap - tures the total heat dissipated during training . The concept of learning efﬁciency was introduced as an alternative metric to compare algorithmic performance . Additionally , Yaida ( 2018 ) derived a discrete - time master equation at stationary equilibrium and linked it to the ﬂuctuation - dissipation theorem in statistical mechanics . Dynamics and Optimal Control Theory : The dynamical perspective has received considerable attention recently as it brings new insights to deep architectures and training processes . For in - stance , Weinan ( 2017 ) proposed to view DNNs as a discretization of continuous - time dynami - cal systems . From such , the propagating rule in the deep residual network ( He et al . , 2016 ) , x t + 1 = x t + f ( x t , θ t ) , can be thought of as an one - step discretization of the forward Euler scheme on an ordinary differential equation ( ODE ) , ˙ x t = f ( x t , θ t ) . This interpretation has been leveraged to improve residual blocks in the sense that it achieves more effective numerical approximation ( Lu et al . , 2017 ) . In the continuum limit of depth , the ﬂow representation of DNNs has made the transport analysis with Wasserstein geometry possible ( Sonoda & Murata , 2019 ) . Algorithmically , efﬁcient computational methods have been developed in Chen et al . ( 2018b ) ; Chen & Duvenaud ( 2019 ) to allow parameterization of ( stochastic ) continuous - time dynamics ( e . g . derivative of latent variables ) directly with DNNs . When the analogy between optimization algorithms and controllers is further drawn ( Hu & Lessard , 2017 ; An et al . , 2018b ) , standard supervised learning can be recast as a mean - ﬁeld optimal control problem ( Han et al . , 2018 ) . This is particularly beneﬁcial since it enables new training algorithms inspired from optimal control literature ( Li et al . , 2017a ; Li & Hao , 2018 ) . Similar analysis can be applied to SGD by viewing it as a stochastic dynamical system . In fact , most previous discussions on implicit bias formulate SGD as stochastic Langevin dynamics ( Pavliotis , 2014 ) . Other stochastic modeling , such as L ` evy process , has been recently proposed ( Simsekli et al . , 2019 ) . In parallel , stability analysis of the Gram matrix dynamics induced by DNN reveals global optimality of GD algorithms ( Du et al . , 2018b ; a ) . Applying optimal control theory to SGD dynamics results in optimal adaptive strategies for tuning hyper - parameters , such as the learning rate , momentum , and batch size ( Li et al . , 2017b ; An et al . , 2018a ) . 3 D EEP N EURAL N ETWORK AS A D YNAMICAL S YSTEM As mentioned in Sec . 2 , DNNs can be interpreted as ﬁnite - horizon nonlinear dynamical systems by viewing each layer as a distinct time step . In Sec 3 . 1 , we will discuss how to explore this connec - tion to analyze information propagation inside DNN . The formalism establishes the foundation of recent works ( Du et al . , 2018a ; Schoenholz et al . , 2016 ; Xiao et al . , 2018 ) , and we will discuss its 4 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective implications in Sec . 3 . 2 . In Sec 3 . 3 , we will draw the connection between optimization algorithms and controllers , leading to an optimal control formulation of DNN training characterized by mean - ﬁeld theory . Hereafter we will focus on fully - connected ( FC ) layers and leave remarks for other architectures , e . g . convolution layers and residual connections , in SM A . 3 . 1 I NFORMATION P ROPAGATION INSIDE DNN Recall the dynamics of a FC - DNN at time step t , i . e . at layer t - th and suppose its weights and biases are initialized by drawing i . i . d . from two zero - mean Gaussians , i . e . h t = W FC ( x t ; θ t ) : = W t x t + b t , where θ t ≡ ( W t , b t ) and W ( i , j ) t ∼ N ( 0 , σ 2 w / N t ) b ( i ) t ∼ N ( 0 , σ 2 b ) . ( 1 ) σ 2 w and σ 2 b respectively denote the variance of the weights and biases , and N t is the dimension of the pre - activation at time t . Central limit theorem ( CLT ) implies in the limit of large layer widths , N t (cid:29) 1 , the distribution of pre - activation elements , h ( i ) t , also converges to a Gaussian . It is straightforward to see the distribution also has zero mean and its variance can be estimated by matching the second moment of the empirical distribution of h ( i ) t across all N t , q t : = 1 N t N t (cid:88) i = 0 ( h ( i ) t ) 2 . ( 2 ) q t can be viewed as the normalized squared length of the pre - activation , and we will use it as the statistical quantity of the information signal . The dynamics of q t , when propagating from time step t to t + 1 , takes a nonlinear form q t + 1 = σ 2 w E h ( i ) t ∼N ( 0 , q t ) (cid:104) φ 2 ( h ( i ) t ) (cid:105) + σ 2 b , ( 3 ) with the initial condition given by q 0 = 1 N 0 x 0 · x 0 . Notice that despite starting the derivation from random neural networks , the mapping in Eq . ( 3 ) admits a deterministic process , depending only on σ w , σ b , and φ ( · ) . We highlight this determinism as the beneﬁt gained by mean - ﬁeld approximation . Schoenholz et al . ( 2016 ) showed that for any bounded φ and ﬁnite value of σ w and σ b , there exists a ﬁxed point , q ∗ : = lim t →∞ q t , regardless of the initial state q 0 . Similarly , for a pair of input ( x ( α ) , x ( β ) ) we can derive the following recurrence relation q ( x ( α ) , x ( β ) ) t + 1 ≡ q ( α , β ) t + 1 = σ 2 w E ( h ( α ) t , h ( β ) t ) T ∼N ( 0 , Σ t ) [ φ ( h ( α ) t ) φ ( h ( β ) t ) ] + σ 2 b , ( 4 ) where Σ t = (cid:32) q αt q ( α , β ) t q ( α , β ) t q βt (cid:33) ( 5 ) is the covariance matrix at time t and the initial condition is given by q ( α , β ) 0 = 1 N 0 x ( α ) 0 · x ( β ) 0 . Under the same conditions for q ∗ to exist , we also have the ﬁxed points for the covariance and correlation , respectively denoted q ∗ ( α , β ) and c ∗ = q ∗ ( α , β ) / (cid:112) q ∗ α q ∗ β = q ∗ ( α , β ) / q ∗ . The element of the covariance matrix at its ﬁxed point Σ ∗ hence takes a compact form Σ ∗ ( α , β ) = q ∗ (cid:0) δ ( α , β ) + ( 1 − δ ( α , β ) ) c ∗ (cid:1) , ( 6 ) where δ ( a , b ) is Kronecker delta . It is easy to verify that Σ ∗ has q ∗ for the diagonal entries and q ∗ c ∗ for the off - diagonal ones . In short , when the mean ﬁeld theory is applied to approximate distributions of activations and pre - activations , the statistics of the distribution follows a deterministic dynamics as propagating through layers . This statistics can be treated as the information signal and from there the dynamical system analysis can be applied , as we will show in the next subsection . 3 . 2 S TABILITY A NALYSIS & I MPLICATIONS Traditional stability analysis of dynamical systems often involves computing the Jacobian at the ﬁxed points . The Jacobian matrix describes the rate of change of system output when small dis - turbance is injected to input . In this vein , we can deﬁne the residual system , (cid:15) t : = Σ ∗ − Σ t , and 5 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 σ 2 w 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 σ 2 b Ordered Phase max ( χ q ∗ , χ c ∗ ) < 1 Chaotic Phase max ( χ q ∗ , χ c ∗ ) > 1 ( a ) 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 σ 2 w 10 20 30 40 50 60 70 80 90 100 d e p t h 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 ( b ) Figure 1 : Reproduced results 3 from Schoenholz et al . ( 2016 ) for random FC - DNNs with φ = tanh . ( a ) The phase diagram of max ( χ q ∗ , χ c ∗ ) as the function of σ w and σ b . The solid line represents the critical initialization where information inside DNN is neither saturated , as in the ordered phase , nor exploding , as in the chaotic phase . ( b ) Prediction of the network trainability given its depth and σ 2 w , with σ 2 b ﬁxed to 0 . 05 . The color bar represents the training accuracy on MINST after 200 training steps using SGD . It is obvious that the boundary at which networks become un - trainable aligns with the theoretical depth scale , denoted white dashed line , up to a constant ( ∼ 4 . 5 ξ c ∗ in this case ) . Also , notice that the peak around σ 2 w = 1 . 75 is precisely predicted by the critical line in Fig . 1 ( a ) for σ 2 b = 0 . 05 . ﬁrst - order expand 2 it at Σ ∗ . The independent evolutions of the two signal quantities , as shown in Eq . ( 3 ) and ( 4 ) , already hint that the Jacobian can be decoupled into two sub - systems . Their eigenvalues are given by , χ q ∗ = σ 2 w E h ∼N ( 0 , Σ ∗ ) [ φ (cid:48)(cid:48) ( h ( i ) ) φ ( h ( i ) ) + φ (cid:48) ( h ( i ) ) 2 ] and ( 7 ) χ c ∗ = σ 2 w E h ∼N ( 0 , Σ ∗ ) [ φ (cid:48) ( h ( i ) ) φ (cid:48) ( h ( j ) ) ] h ( i ) (cid:54) = h ( j ) . ( 8 ) This eigen - decomposition suggests the information traverses through layers in the diagonal and off - diagonal eigenspace . A ﬁxed point is stable if and only if both χ q ∗ and χ c ∗ are less than 1 . In fact , the logarithms of χ q ∗ and χ c ∗ relate to the well - known Lyapunov exponents in the dynamical system theory , i . e . | q t − q ∗ | ∼ e − t / ξ q ∗ ξ − 1 q ∗ = − log χ q ∗ and ( 9 ) | c t − c ∗ | ∼ e − t / ξ c ∗ ξ − 1 c ∗ = − log χ c ∗ , ( 10 ) where c t denotes the dynamics of the correlation . The dynamical system analysis in Eq . ( 7 - 10 ) has several important implications . Recall again that given a DNN , its propagation rule depends only on σ w and σ b . We can therefore construct a phase diagram with σ w and σ b as axes and deﬁne a critical line that separates the ordered phase , in which all eigenvalues are less than 1 to stabilize ﬁxed points , and the chaotic phase , in which either χ q ∗ or χ c ∗ exceeds 1 , leading to divergence . An example of the phase diagram for FC - DNNs is shown in Fig . 1 ( a ) . Networks initialized in the ordered phase may suffer from saturated information if the depth is sufﬁciently large . They become un - trainable since neither forward nor backward propagation is able to penetrate to the destination layer . Fig . 1 ( b ) gives an illustration of how ξ q ∗ and ξ c ∗ , named depth scale ( Schoenholz et al . , 2016 ) , predict the trainability of random DNNs . On the other hand , networks initialized along the critical line remain marginal stable , and information is able to propagate through an arbitrary depth without saturating or exploding . It is particularly interesting to note that while the traditional study on dynamical systems focuses on stability conditions , the dynamics inside DNN instead requires a form of transient chaos . The discussion of the aforementioned critical initialization , despite being crucial in the success of DNN training ( Glorot & Bengio , 2010 ) , may seem limited since once the training process begins , the i . i . d . assumptions in order to construct Eq . ( 2 ) , and all those derivations afterward , no longer hold . 2 We refer readers to the Supplementary Material in Xiao et al . ( 2018 ) for a complete treatment . 3 Code is available in https : / / github . com / ghliu / mean - field - fcdnn . 6 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Fortunately , it has been empirically observed that when DNN is sufﬁciently over - parameterized , its weight will be close to the initialization over training iterations ( Li & Liang , 2018 ) . In other words , under certain assumptions , the statistical property derived at initialization can be preserved through - out training . This has a strong implication as it can be leveraged to prove the global convergence and optimality of GD ( Du et al . , 2018b ; a ) . Below we provide the proof sketch and demonstrate how the deep information is brought into the analysis . Recalling the information deﬁned in Eq . ( 4 - 5 ) for a pair of input , we can thereby construct a Gram matrix K t ∈ R | D | × | D | , where | D | is the size of the dataset and t ∈ { 0 , 1 , · · · , T − 1 } . The element of K t represents the information quantity between the data points with the corresponding indices , i . e . K ( i , j ) t : = q ( i , j ) t . The Gram matrix can be viewed as a deep representation of a matrix form induced by the DNN architecture and dataset at random initialization . The same matrix has been derived in several concurrent works , namely the Neural Tangent Kernel ( NTK ) in Jacot et al . ( 2018 ) . Now , consider a mean squared loss , 12 (cid:107) y − u (cid:107) 22 , where y , u ∈ R | D | and each element u ( i ) : = 1 T x ( i ) T / (cid:107) 1 (cid:107) 2 denotes the scalar prediction of each data point i ∈ D . The dynamics of the prediction error governed by the GD algorithm takes an analytical form ( Du et al . , 2018b ) written as y − u ( k + 1 ) ≈ ( I − η G ( k ) ) ( y − u ( k ) ) , where G ( i , j ) ( k ) : = (cid:104) ∂ u i ( k ) ∂θ T − 1 ( k ) , ∂ u j ( k ) ∂θ T − 1 ( k ) (cid:105) . ( 11 ) k and η denote the iteration process and learning rate . Eq . ( 11 ) indicates a linear dynamics charac - terized by the matrix G , whose element at initialization is related to the one of K by G ( i , j ) ( 0 ) = K ( i , j ) T − 1 E h ∼N ( 0 , Σ T − 1 ) [ φ (cid:48) ( h ( i ) ) φ (cid:48) ( h ( j ) ) ] = : K ( i , j ) T . ( 12 ) When the width is sufﬁciently large , G ( k ) will be close to K T ( precisely (cid:107) G ( k ) − K T (cid:107) 2 is bounded ) for all iterations k ≥ 0 . This , together with the least eigenvalue of K T being lower - bounded for non - degenerate dataset , concludes the linear convergence to the global minimum . 3 . 3 T RAINING DNN WITH O PTIMAL C ONTROL To frame optimization algorithms and training processes into the dynamical system viewpoint , one intuitive way is to interpret optimization algorithms as controllers . As we will show in Sec . 3 . 3 . 1 , this can be achieved without loss of generality and naturally yields an optimal control formalism of the deep learning training process . Such a connection is useful since the optimality conditions of the former problem are well studied and characterized by the Pontryagin’s Maximum Principle ( PMP ) and the Hamilton - Jacobi - Bellman ( HJB ) equation , which we will introduce in Sec . 3 . 3 . 2 and 3 . 3 . 3 , respectively . The fact that back - propagation ( LeCun et al . , 1990 ) can be viewed as an approximation of PMP ( Li et al . , 2017a ) opens a room for new optimization algorithms inspired from the optimal control perspective . 3 . 3 . 1 M EAN - F IELD O PTIMAL C ONTROL D ERIVATION In Hu & Lessard ( 2017 ) ; An et al . ( 2018b ) , a concrete connection was derived between ﬁrst - order optimization algorithms and PID controllers . To see that , consider the formula of gradient descent and the discrete realization of integral control : θ k + 1 = θ k − ∇ f ( θ k ) ( gradient descent ) u k + 1 = (cid:88) k i = 0 e i · ∆ t ( integral control ) These two update rules are equivalent when we interpret the gradient −∇ f ( θ k ) as tracking error e ( k ) . Both modules are designed to drive certain statistics of a system , either the loss gradient or tracking error , towards zero , by iteratively updating new variables to affect system dynamics . When a momentum term is introduced , it will result in an additional lag component which helps increase the low - frequency loop gain ( Franklin et al . , 1994 ) . This suggests accelerated convergence near local minima , as observed in the previous analysis ( Qian , 1999 ) . In other words , the parameters in DNNs can be recast as the control variables in dynamical systems . With this viewpoint in mind , we can draw an interesting connection between deep learning training processes and optimal control problems ( OCP ) . In a vanilla discrete - time OCP , the minimization 7 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective problem takes the form min { θ t } T − 1 t = 0 J : = (cid:34) Φ ( x T ) + T − 1 (cid:88) t = 0 L ( x t , θ t ) (cid:35) s . t . x t + 1 = f ( x t , θ t ) , ( OCP ) where x t ∈ R n and θ t ∈ R m represent state and control vectors . f , L and Φ respectively denote the dynamics , intermediate cost and terminal cost functions . In this vein , the goal of ( supervised ) learning is to ﬁnd a set of optimal parameters at each time step ( i . e . layer ) , { θ t } T − 1 t = 0 , such that when starting from the initial state x 0 , its terminal state x T is close to the target y . Dynamical constraints in Eq . ( OCP ) are characterized by the DNNs , whereas terminal and control - dependent intermediate costs correspond to training loss and weight regularization . Though state - dependent intermediate costs are not commonly - seen in supervised problems until recently ( Nøkland & Eidnes , 2019 ) , it has been used extensively in the context of deep reinforcement learning to guide or stabilize training , e . g . the auxiliary tasks and losses ( Jaderberg et al . , 2016 ; Liu et al . , 2017 ) . Extending Eq . ( OCP ) to accept batch data requires viewing the input - output pair ( x 0 , y ) as a ran - dom variable drawn from a probability measure . This can be done by introducing the mean - ﬁeld formalism where the analysis is lifted to distribution spaces . The problem becomes searching an optimal transform that propagates the input population to the desired target distribution . The pop - ulation risk minimization problem can hence be regarded as a mean - ﬁeld optimal control problem ( MF - OCP ) ( Han et al . , 2018 ) , inf θ t ∈ L ∞ ( [ 0 , T ] , R m ) E ( x 0 , y ) ∼ µ 0 (cid:34) Φ ( x T , y ) + (cid:90) T 0 L ( x t , θ t ) dt (cid:35) s . t . ˙ x t = f ( x t , θ t ) , ( MF - OCP ) where L ∞ ( [ 0 , T ] , R m ) ≡ L ∞ denotes the set of essentially - bounded measurable functions 4 and µ 0 is the joint distribution of the initial states x 0 and terminal target y . Note that we change our formu - lation from the discrete - time realization to the continuous - time framework since it is mathematically easier to analyze and offers more ﬂexibilities . The MF - OCP formulation allows us to analyze the optimization of DNN training through two perspectives , namely the maximum principle approach and dynamic programming approach , as we will now proceed . 3 . 3 . 2 M EAN - F IELD P ONTRYAGIN ’ S M AXIMUM P RINCIPLE The necessary conditions of the OCP problem are described in the celebrated Pontryagin’s Max - imum Principle ( PMP ) ( Boltyanskii et al . , 1960 ) . It characterizes the conditions that an optimal state - control trajectory must obey locally . Han et al . ( 2018 ) derived the mean - ﬁeld extension of the theorem , which we will restate below . We will focus on its relation with standard DNN optimization , i . e . gradient descent with back - propagation , and refer to Han et al . ( 2018 ) for the concrete proof . Theorem 1 ( Mean - Field PMP ( Han et al . , 2018 ) ) . Assume the following statements are true : ( A1 ) The function f is bounded ; f , L are continuous in θ t ; and f , L , Φ are continuously differ - entiable with respect to x t . ( A2 ) The distribution µ 0 has bounded support , i . e . µ 0 (cid:0)(cid:8) ( x , y ) ∈ R n × R d : (cid:107) x (cid:107) + (cid:107) y (cid:107) ≤ M (cid:9)(cid:1) = 1 for some M > 0 . Let θ ∗ t : t (cid:55)→ R m be a solution that achieves the inﬁmum of MF - OCP . Then , there exists continuous stochastic processes x ∗ t and p ∗ t , such that ˙ x ∗ t = f ( x ∗ t , θ ∗ t ) x ∗ 0 = x 0 , ( 13a ) ˙ p ∗ t = −∇ x H ( x ∗ t , p ∗ t , θ ∗ t ) , p ∗ T = −∇ x Φ ( x ∗ T , y ) , ( 13b ) E µ 0 H ( x ∗ t , p ∗ t , θ ∗ t ) ≥ E µ 0 H ( x ∗ t , p ∗ t , θ t ) , ∀ θ t ∈ R m , a . e . t ∈ [ 0 , T ] , ( 13c ) where the Hamiltonian function H : R n × R n × R m → R is given by H ( x t , p t , θ t ) = p t · f ( x t , θ t ) − L ( x t , θ t ) ( 14 ) and p t denotes the co - state of the adjoint equation . 4 This implies θ t can be quite discontinuous in time , since the function class we consider only requires to be measurable and essentially bounded . 8 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Theorem 1 resembles the classical PMP result except that the Hamiltonian maximization condition ( 13b ) is now taken over an expectation w . r . t . µ 0 . Also , notice the optimal control trajectory θ ∗ t admits an open - loop process in the sense that it does not depend on the population distribution . This is in contrast to what we will see from the dynamic programming approach ( i . e . Theorem 2 ) . The conditions characterized by Eq . ( 13 ) can be linked to the optimization dynamics in DNN train - ing . First , Eq . ( 13a ) is simply the feed - forward pass from the ﬁrst layer to the last one . The co - state can be interpreted as the Lagrange multiplier of the objective function w . r . t . the constraint variables ( Bertsekas et al . , 1995 ) , and its backward dynamics is described in Eq . ( 13b ) . Here , we shall re - gard Eq . ( 13b ) as the back - propagation ( Li et al . , 2017a ) . To see that , consider the discrete - time Hamiltonian function , H ( x t , p t + 1 , θ t ) = p t + 1 · f ( x t , θ t ) − L ( x t , θ t ) . ( 15 ) Substituting it into Eq . ( 13b ) will lead to the chain rule used derive back - propagation , p ∗ t = ∇ x H (cid:0) x ∗ t , p ∗ t + 1 , θ ∗ t (cid:1) = p ∗ t + 1 · ∇ x f ( x ∗ t , θ ∗ t ) − ∇ x L ( x ∗ t , θ ∗ t ) , ( 16 ) where p ∗ t is the gradient of the total loss function w . r . t . the activation at layer t . Finally , the maximization in Eq . ( 13c ) can be difﬁcult to solve exactly since the dimension of the parameter is typically millions for DNNs . We can , however , apply approximated updates iteratively using ﬁrst - order derivatives ( Li et al . , 2017a ) , i . e . θ ( i + 1 ) t = θ ( i ) t + ∇ θ t E µ 0 H ( x ∗ t , p ∗ t + 1 , θ ( i ) t ) . ( 17 ) The subscript t denotes the time step in the DNN dynamics , i . e . the index of the layer , whereas the superscript ( i ) represents the iterative update of the parameters in the outer loop . The update rule in Eq . ( 17 ) is equivalent to performing gradient descent on the original objective function J in OCP , ∇ θ t H (cid:0) x ∗ t , p ∗ t + 1 , θ t (cid:1) = p ∗ t + 1 · ∇ θ t f ( x ∗ t , θ t ) − ∇ θ t L ( x ∗ t , θ t ) = ∇ θ t J . ( 18 ) When the expectation in Eq . ( 17 ) is replaced with a sample mean , Han et al . ( 2018 ) showed that if a solution of MF - OCP is stable 5 , we can ﬁnd with high probability a random variable in its neighborhood that is a stationary solution of the sampled PMP . 3 . 3 . 3 M EAN - F IELD H AMILTON - J ACOBI - B ELLMAN E QUATION The Hamilton - Jacobi - Bellman ( HJB ) equation ( Bellman & Kalaba , 1964 ) characterizes both neces - sary and sufﬁcient conditions to the OCP problem . The equation is derived from the principle of Dynamic Programming ( DP ) , which reduces the problem of minimizing over a sequence of control to a sequence of minimization over a single control at each time . This is done by recursively solving the value function ( deﬁne precisely below ) , and the obtained optimal policy is a function applied globally to the state space , i . e . a feedback controller with states as input . Han et al . ( 2018 ) adapted the analysis to mean - ﬁeld extension by considering probability measures as states . Following their derivations , we will consider the class of probability measures that is square integrable on Euclidean space with 2 - Wasserstein metrics , denoted P 2 ( R ) , throughout our analysis . Importantly , this will lead to an inﬁnite - dimensional HJB equation as we will show later . It is useful to begin with deﬁning the cost - to - go and value function , denoted as J and v ∗ : J ( t , µ , θ t ) : = E ( x t , y ) ∼ µ t subject to ( 1 ) (cid:34) Φ ( x T , y ) + (cid:90) T t L ( x t , θ t ) dt (cid:35) ( cost - to - go ) v ∗ ( t , µ ) : = inf θ t ∈ L ∞ J ( t , µ , θ t ) ( value function ) Note that the expectation is taken over the distribution evolution , starting from µ and propagating through the DNN architecture . The cost - to - go function is simply a generalization of the MF - OCP objective to varying start time , and its inﬁmum over the control space is achieved by the value func - tion , i . e . the objective in MF - OCP can be regarded as J ( 0 , µ 0 , θ 0 ) , with v ∗ ( 0 , µ 0 ) as its inﬁmum . Now , we are ready to introduce the mean - ﬁeld DP and HJB equation . 5 The mapping F : U (cid:55)→ V is said to be stable on S ρ ( x ) : = { y ∈ U : (cid:107) x − y (cid:107) U ≤ ρ } if (cid:107) y − z (cid:107) U ≤ K ρ (cid:107) F ( y ) − F ( z ) (cid:107) V for some K ρ > 0 . 9 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Theorem 2 ( Mean - Field DP & HJB ( Han et al . , 2018 ) ) . Let the following statements be true : ( A1’ ) f , L , Φ are bounded ; f , L , Φ are Lipschitz continuous with respect to x t , and the Lipschitz constants of f and L are independent of θ t . ( A2’ ) µ 0 ∈ P 2 (cid:0) R n + d (cid:1) . Then , both cost - to - go J ( t , µ , θ t ) and value function v ∗ ( t , µ ) are Lipschitz continuous on [ 0 , T ] × P 2 (cid:0) R n + d (cid:1) . For all 0 ≤ t ≤ ˆ t ≤ T , the principle of dynamic programming suggests v ∗ ( t , µ ) = inf θ t ∈ L ∞ E ( x t , y ) ∼ µ t subject to ( 1 ) (cid:34)(cid:90) ˆ t t L ( x t , θ t ) dt + v ∗ (cid:0) ˆ t , ˆ µ (cid:1)(cid:35) , ( mean - ﬁeld DP ) where ˆ µ denotes the terminal distribution at ˆ t . Furthermore , Taylor expansion of the mean - ﬁeld DP gives the mean - ﬁeld HJB equation , (cid:26) ∂ t v + inf θ t ∈ L ∞ (cid:104) ∂ µ v ( · ) , f ( · , θ t ) (cid:105) µ + (cid:104) L ( · , θ t ) (cid:105) µ = 0 , on [ 0 , T ) × P 2 (cid:0) R n + d (cid:1) v ( T , µ ) = (cid:104) Φ ( · ) (cid:105) µ , on P 2 (cid:0) R n + d (cid:1) , ( mean - ﬁeld HJB ) where we recall (cid:104) f ( · ) , g ( · ) (cid:105) µ : = (cid:82) f ( w ) T g ( w ) µ ( d w ) and accordingly denote (cid:104) f ( · ) (cid:105) µ : = (cid:82) f ( w ) µ ( d w ) . Finally , if θ ∗ : ( t , µ ) (cid:55)→ R m is a feedback policy that achieves the inﬁmum in mean - ﬁeld HJB equation , then θ ∗ is an optimal solution of the MF - OCP problem . Notice that ( A1’ ) and ( A2’ ) are much stronger assumptions as opposed to those from Theorem 1 . This is reasonable since the analysis is now adapted to take probability distributions as inputs . The - orem 2 differs from the classical HJB in that the equations become inﬁnite - dimensional . The com - putation requires the derivative of the value function w . r . t . a probability measure , which can be done by recalling the deﬁnition of the ﬁrst - order variation ( Ambrosio et al . , 2008 ) of a function F at the probability measure µ , i . e . ∂F ( µ ) ∂µ ≡ ∂ µ F , satisfying the following relation : F ( µ + (cid:15)f ) = F ( µ ) + (cid:15) (cid:104) ∂ µ F ( · ) , f ( · ) (cid:105) µ + O ( (cid:15) 2 ) , ( 19 ) where (cid:15) is taken to be inﬁnitesimally small . Note that F ( · ) and ∂ µ F ( · ) are functions respectively deﬁned on the probability space and its associated Euclidean space . In other words , the derivative w . r . t . the density is achieved by interplaying probability measures with laws of random variables , to which we can apply a suitable deﬁnition of the derivative . Due to the curse of dimensionality , classical HJB equations can be computationally intractable to solve for high - dimensional problems , let alone its mean - ﬁeld extension . However , we argue that in the literature of DNN optimization , algorithms with a DP ﬂavor , or at least an approximation of it , have not been well - explored . Research in this direction may provide a principled way to design feedback policies rather than the current open - loop solutions in which weights are ﬁxed once the training ends . This can be particularly beneﬁcial for problems related to e . g . adversarial attack and generalization analyses . 4 S TOCHASTIC O PTIMIZATION AS A D YNAMICAL S YSTEM We now turn attention to stochastic optimization . Recall that in Sec . 3 . 3 , we bridge optimization algorithms with controllers . In classical control theory , controllers alone can be characterized as separated dynamical systems . Stability analysis is conducted on the compositional system along with the plant dynamics ( Franklin et al . , 1994 ) . Similarly , the dynamical perspective can be useful to study the training evolution and convergence property of DNN optimization . Unlike the deter - ministic propagation in Sec . 3 . 1 , stochasticity plays a central role in the resulting system due to the mini - batch sampling at each iteration . Preceding of time corresponds to the propagation of training cycles instead of forwarding through DNN layers . The stochastic dynamical viewpoint forms most of the recent study on SGD ( Chaudhari & Soatto , 2018 ; Chaudhari et al . , 2018 ; An et al . , 2018a ) . This section is organized as follows . In Sec . 4 . 1 , we will review the statistical property of the mini - batch gradient , which is the foundation for deriving the SGD dynamics . Recast of SGD as a continuous - time stochastic differential equation ( SDE ) , or more generally a discrete - time master 10 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective equation , will be demonstrated in Sec . 4 . 2 , and 4 . 3 , respectively . The theoretical analysis from the dynamical framework consolidates several empirical observations , including implicit regularization on the loss landscape ( Zhang et al . , 2016 ) and phase transition from a fast memorization period to slow generalization ( Shwartz - Ziv & Tishby , 2017 ) . It can also be leveraged to design optimal adaptive strategies , as we will show in Sec . 4 . 4 . 4 . 1 P RELIMINARIES ON S TOCHASTIC M INI - B ATCH G RADIENT Slightly abuse the notation and denote the averaging training loss on the data set D as a function of parameter : Φ ( θ ; D ) ≡ Φ ( θ ) : = 1 | D | (cid:88) i ∈D J ( f ( x ( i ) , θ ) , y ( i ) ) , ( 20 ) where J is the training objective for each sample ( c . f . Eq . ( OCP ) ) and f ≡ f 0 ◦ f 1 ◦ f 2 · · · includes all compositional functionals of a DNN . We can write the full gradient of the training loss as ∇ Φ ( θ ) ≡ g ( θ ) = 1 | D | (cid:80) i ∈D g i ( θ ) , where g i ( θ ) is the gradient on each data point ( x ( i ) , y ( i ) ) . The covariance matrix of g i ( θ ) , denoted Σ D ( θ ) , is a positive - deﬁnite ( P . D . ) matrix which can be computed deterministically , given the DNN architecture , dataset , and current parameter , as Var (cid:2) g i ( θ ) (cid:3) ≡ Σ D ( θ ) : = 1 | D | (cid:88) i ∈D (cid:0) g i ( θ ) − g ( θ ) (cid:1) (cid:0) g i ( θ ) − g ( θ ) (cid:1) T . ( 21 ) Note that in practice , the eigen - spectrum of Σ D often features an extremely low rank ( < 0 . 5 % for both CIFAR - 10 and CIFAR - 100 as reported in Chaudhari & Soatto ( 2018 ) ) . Access of g ( θ ) at each iteration is computationally prohibit for large - scale problems . Instead , gra - dients can only be estimated through a mini batch of i . i . d . samples B ⊂ D . When the batch size is large enough , | B | (cid:29) 1 , CLT implies the mini - batch gradient , denoted g mb ( θ ) = 1 | B | (cid:80) i ∈B g i ( θ ) , has the sample mean and covariance mean (cid:2) g mb ( θ ) (cid:3) : = E B [ g mb ( θ ) ] ≈ g ( θ ) ( 22 ) Var (cid:2) g mb ( θ ) (cid:3) : = E B [ (cid:0) g mb ( θ ) − g ( θ ) (cid:1) (cid:0) g mb ( θ ) − g ( θ ) (cid:1) T ] ≈ 1 | B | Σ D ( θ ) . ( 23 ) The last equality in Eq . ( 23 ) holds when B is sampled i . i . d . with replacement from D . 6 For later purposes , let us also deﬁne the two - point noise matrix as ˜ Σ B : = E B [ g mb ( θ ) g mb ( θ ) T ] . Its entry ( i , j ) , or more generally the entry ( i 1 , i 2 , · · · i k ) of a higher - order noise tensor , can be written as ˜Σ B , ( i , j ) : = E B [ g mb ( θ i ) g mb ( θ j ) ] and ( 24 ) ˜Σ B , ( i 1 , i 2 , ··· i k ) : = E B [ g mb ( θ i 1 ) g mb ( θ i 2 ) · · · g mb ( θ i k ) ] , ( 25 ) where g mb ( θ i ) is the partial derivative w . r . t . θ i on the mini batch . Consequently , we can rewrite Eq . ( 23 ) as ˜ Σ B − g ( θ ) g ( θ ) T . Finally , we will denote the distribution of the parameter at training cycle t as ρ t ( z ) : = ρ ( z , t ) ∝ P ( θ t = z ) and the steady - state distribution as ρ ss : = lim t →∞ ρ t . 6 Var (cid:2) g mb ( θ ) (cid:3) = Var (cid:104) 1 | B | (cid:80) B g i ( θ ) (cid:105) = 1 | B | 2 (cid:80) B Var (cid:2) g i ( θ ) (cid:3) = 1 | B | 2 | B | Σ D ( θ ) = 1 | B | Σ D ( θ ) . The second equality holds since Cov ( g i , g j ) = 0 for i (cid:54) = j . Remarks for Vanilla and Momentum GD : Dynamics of these systems can be characterized by the spectrum of loss functions . For instance , the eigendecomposition on convex objectives gives full descriptions on convergence speed of GD . Signal residing in the eigenspace with the least eigenvalue resists longest throughout optimization . When the momentum term is introduced , the system can be thought of as a discretization of a damped harmonic oscillator ( Goh , 2017 ) , and the vanilla and momentum GD dynamics can be respectively viewed as the over - damped and under - damped systems . This implies an oscillation when the momentum is set far from the critical damping coefﬁcient . Despite focusing on SGD afterwards , most of our analyses can be extended to include momentum by considering a higher - order dynamics or augmented state space . 11 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective 4 . 2 C ONTINUOUS - TIME D YNAMICS OF SGD Derivation : The approximations from Eq . ( 22 - 23 ) allow us to replace g mb ( θ ) with a Gaussian N (cid:16) g ( θ ) , 1 | B | Σ D (cid:17) . The updated rule of SGD at each iteration can hence be recast as θ t + 1 = θ t − ηg mb ( θ t ) ≈ θ t − ηg ( θ t ) + η √ | B | Σ 12 D ( θ t ) Z t , ( 26 ) where η is the learning rate and Z t ∼ N ( 0 , I ) . Now , consider the following continuous - time SDE and its Euler discretization , d θ t = b ( θ t ) d t + σ ( θ t ) d W t ( 27 ) ⇒ θ t + 1 = θ t + b ( θ t ) ∆ t + √ ∆ t · σ ( θ t ) Z t , ( 28 ) In the standard SDE analysis ( Øksendal , 2003 ) , b ( θ t ) and σ ( θ t ) refer to the drift and diffusion function . d W t is the Wiener process , or Brownian motion , in the same dimension of θ ∈ R ¯ m . It is easy to verify that Eq . ( 28 ) resembles Eq . ( 26 ) if we set ∆ t ∼ η , b ( θ t ) ∼ − g ( θ t ) , and σ ( θ t ) ∼ (cid:112) η / | B | Σ 12 D . We have therefore derived the continuous - time limit of SGD as the following SDE , d θ t = − g ( θ t ) d t + (cid:112) 2 β − 1 Σ D ( θ t ) d W t , ( 29 ) where β = 2 | B | η is proportional to the inverse temperature in thermodynamics . Simply viewing Eq . ( 29 ) already gives us several insights . First , β controls the magnitude of the diffusion process . A similar relationship , named noise scale , between the batch size and learning rate has been proposed in Smith & Le ( 2017 ) ; Jastrzebski et al . ( 2017 ) . These two hyper - parameters , however , are not completely interchangeable since they contribute to different properties of the loss landscape ( Wu et al . , 2018 ) . Secondly , the stochastic dynamics is characterized by the drift term from the gradient descent ﬂow − g ( θ t ) and the diffusion process from Σ D ( θ t ) . When the parameter is still far from equilibrium , we can expect the drifting to dominate the propagation . As we approach ﬂat local minima , ﬂuctuations from the diffusion become signiﬁcant . This drift - to - ﬂuctuation transition has been observed on the Information Plane ( Shwartz - Ziv & Tishby , 2017 ) and can be derived exactly for convex cases ( Moulines & Bach , 2011 ) . Since the stochastic processes in Eq . ( 29 ) and ( 26 ) are different , the approximation is valid only up to the distribution level . While a more accurate approximation is possible by introducing stochastic modiﬁed equations ( Milstein , 1994 ) , we will limit the analysis to Eq . ( 29 ) and study the resulting dynamics using stochastic calculus and statistical physics . Dynamics of Training Loss : To describe the propagation of the training loss , Φ ( θ t ) , as a function of the stochastic process in Eq . ( 29 ) , we need to utilize It ˆ o lemma , an extension of the chain rule in the ordinary calculus to the stochastic setting : Lemma 1 ( Itˆo lemma ( Itˆo , 1951 ) ) . Consider the stochastic process d X t = b ( X t , t ) d t + σ ( X t , t ) d W t . Suppose b ( · , · ) and σ ( · , · ) follow appropriate smooth and growth conditions , then for a given function V ( · , · ) ∈ C 2 , 1 ( R d × [ 0 , T ] ) , V ( X t , t ) is also a stochastic process : d V ( X t , t ) = (cid:20) ∂ t V ( X t , t ) + ∇ V ( X t , t ) T b ( X t , t ) + 1 2 Tr (cid:104) σ ( X t , t ) T H V ( X t , t ) σ ( X t , t ) (cid:105)(cid:21) d t + (cid:104) ∇ V ( X t , t ) T σ ( X t , t ) (cid:105) d W t , ( Itˆo lemma ) where H V ( X t , t ) denotes the Hessian . i . e . H V , ( i , j ) = ∂ 2 V / ∂x i ∂x j . Applying Itˆo lemma to V = Φ ( θ t ) readily yields the following SDE : dΦ ( θ t ) = (cid:20) −∇ Φ ( θ t ) T g ( θ t ) + 1 2 Tr (cid:104) ˜ Σ 12 D H Φ ˜ Σ 12 D (cid:105)(cid:21) d t + (cid:104) ∇ Φ ( θ t ) T ˜ Σ 12 D (cid:105) d W t , ( 30 ) where we denote ˜ Σ 12 D = (cid:112) 2 β − 1 Σ D ( θ t ) to simplify the notation . Taking the expectation of Eq . ( 30 ) over the parameter distribution ρ t ( θ ) and recalling ∇ Φ = g , the dynamics of the expected training loss can be described as d E ρ t [ Φ ( θ t ) ] = E ρ t (cid:20) −∇ Φ T ∇ Φ + 1 2 Tr (cid:104) H Φ ˜ Σ D (cid:105)(cid:21) d t , ( 31 ) 12 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective which is also known as the backward Kolmogorov equation ( Kolmogoroff , 1931 ) , a partial differ - ential equation ( PDE ) that describes the dynamics of a conditional expectation E [ f ( X t ) | X s = x ] . Notice that d W t does not appear in Eq . ( 31 ) since the expectation of Brownian motion is zero . The term Tr [ H Φ ˜ Σ D ] draws a concrete connection between the noise covariance and the loss landscape . In Zhu et al . ( 2018 ) , this trace quantity was highlighted as a measurement of the escaping efﬁciency from poor local minima . We can also derive the dynamics of other observables following similar steps . When training converges , the left - hand side of Eq . ( 31 ) is expected to be near zero , i . e . E ρ ss [ ∇ Φ T ∇ Φ ] ≈ 1 2 E ρ ss [ Tr [ H Φ ˜ Σ D ] ] . ( 32 ) In other words , the expected magnitude of the gradient signal is balanced off by the expected Hessian - covariance product measured in trace norm . A similar relation can be founded in the discrete - time setting ( c . f . Eq . ( 42 ) ) , as we will see later in Sec . 4 . 3 . Dynamics of Parameter Distribution : The dynamics in Eq . ( 29 ) is a variant of the damped Langevin diffusion , which is widely used in statistical physics to model systems interacting with drag and random forces , e . g . the dynamics of pollen grains suspended in liquid , subjected to the friction from Navier - Stokes equations and random collisions from molecules . We synthesize the classical results for Langevin systems in the theorem below and discuss its implication in our set - tings . Theorem 3 ( Fokker - Plank equation and variational principle ( Jordan et al . , 1998 ) ) . Consider a damped Langevin dynamics with isotropic diffusion : d X t = −∇ Ψ ( X t ) d t + (cid:112) 2 β − 1 d W t , where β is the damping coefﬁcient . The temporal evolution of the probability density , ρ t ∈ C 2 , 1 ( R d × R + ) , is the solution of the Fokker - Plank equation ( FPE ) : ∂ t ρ t = ∇ · ( ρ t ∇ Ψ ) + β − 1 ∆ ρ t , ( FPE ) where ∇· , ∇ and ∆ respectively denote the divergence , gradient , and Laplacian operators . Suppose Ψ is a potential function satisfying appropriate growth conditions , FPE has a unique stationary solution given by the Gibbs distribution ρ ss ( x ; β ) ∝ exp ( − β Ψ ( x ) ) . Furthermore , the stationary Gibbs distribution satisﬁes the variational principle — it minimizes the following functional ρ ss = arg min ρ F Ψ ( ρ ; β ) : = E Ψ ( ρ ) − β − 1 S ( ρ ) , ( variational principle ) where E Ψ ( ρ ) : = (cid:82) X Ψ ( x ) ρ ( x ) d x and S ( ρ ) : = − (cid:82) X ρ ( x ) log ρ ( x ) d x . In fact , F Ψ ( ρ t ; β ) serves as a Lyapunov function for the FPE , as it decreases monotonically along the dynamics of FPE and converges to its minimum , which is zero , at ρ ss . In other words , we can rewrite FPE as a form of Wasserstein gradient ﬂow ( WGF ) : ∂ t ρ t = ∇ · ( ρ t ∇ ( ∂ ρ F Ψ ) ) , ( WGF of FPE ) where ∂ ρ F Ψ follows the same deﬁnition in Eq . ( 19 ) and is equal to log ρρ ss + 1 by simple algebra . FPE characterizes the deterministic transition of the density of an inﬁnite ensemble of particles and is also known as the forward Kolmogorov equation ( Øksendal , 2003 ) . The form of the Gibbs distri - bution at equilibrium reafﬁrms the importance of the temperature β − 1 , as it determines the sharp - ness of ρ ss . While high temperature can cause under - ﬁtting , in the asymptotic limit as β − 1 → 0 , the steady - state distribution will degenerate to point masses located at arg max Ψ ( x ) . From an information - theoretic viewpoint , F Ψ ( ρ ; β ) can be interpreted as the free energy , where E Ψ ( ρ ) and S ( ρ ) are respectively known as the energy ( or evidence ) and entropy functionals . Therefore , mini - mizing F Ψ balances between the likelihood of the observation and the diversity of the distribution . Theorem 3 focuses on the isotropic diffusion process . Generalization to general diffusion is straight - forward , and adapting the notations from our continuous limit of SGD in Eq . ( 29 ) yields ∂ t ρ t = ∇ · (cid:0) ∇ Φ ( θ t ) ρ t + β − 1 ∇ · ( Σ D ( θ t ) ρ t ) (cid:1) , ( 33 ) which is the FPE of the dynamics of the parameter distribution ρ t ( θ ) . Notice that when the analysis is lifted to the distribution space , the drift and diffusion are no longer separable as in Eq . ( 29 ) . 13 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Now , in order to apply the variational principle , Φ ( θ ) needs to be treated as a potential function under the appropriate growth conditions , which is rarely the case under the setting of deep learning . Nevertheless , assuming these assumptions hold will lead to an important implication , suggesting that the implicit regularization stemmed from SGD can be mathematically quantiﬁed as entropy maxi - mization . Another implication is that in the presence of non - isotropic diffusion during training 7 , the trajectory governed by Eq . ( 33 ) has been shown to converge to a different location from the minima of the training loss ( Chaudhari & Soatto , 2018 ) . In short , the variational inference implied from Eq . ( 29 ) takes the form arg min ρ E θ ∼ ρ t [ ˜Φ ( θ ) ] − β − 1 S ( ρ t ) , ( 34 ) which is minimized at ρ ss ( θ ) ∝ exp ( − β ˜Φ ( θ ) ) . The relationship between Φ and ˜Φ at equilibrium is given by 8 ∇ Φ = Σ D ∇ ˜Φ − β − 1 ∇ · Σ D , ( 35 ) where the divergence ∇ · Σ D is applied to the column space of the diffusion matrix . In other words , the critical points of ˜Φ differ from those of the original training loss by the quantity β − 1 ∇ · Σ D . It can be readily veriﬁed that ˜ Φ = Φ if and only if Σ D is isotropic , i . e . Σ D = c I R ¯ m × R ¯ m for some constant c . In fact , we can construct cases in which the most - likely trajectories traverse along closed loops , i . e . limit cycles , in the parameter space ( Chaudhari & Soatto , 2018 ) . Remarks on Other SDE Modeling : We should be aware that Eq . ( 29 ) , as a variant of the well - known Langevin diffusion , is only one of the possible realization of modeling stochastic mini - batch gradient . In fact , the metastability analysis of Langevin diffusion ( Bovier et al . , 2004 ) conﬂicts with empirical observations in deep learning , as the analysis suggests an escape time depending expo - nentially on the depth of the loss landscape but only polynomial with its width . In other words , theoretical study implies Brownian - like processes should stay much longer in sharp local minima . To build some intuition on why this is true , recall that an implicit assumption we made when de - riving Eq . ( 29 ) is the ﬁnite variance induced by g mb ( θ ) . Upper - bounding the second moment eventually prevents the presence of long - tail distributions , which plays a pivotal role in speeding up the exponential exit time of an SDE from narrow basins . This issue has been mitigated in Simsekli et al . ( 2019 ) by instead considering a general L ˆ evy process : d θ t = − g ( θ t ) d t + η α − 1 α σ α ( θ t ) d L αt , ( 36 ) where α ∈ ( 0 , 2 ] is the tail index and d L αt denotes the α - stable L ˆ evy motion . The mini - batch gra - dients are now drawn from a zero - mean symmetric α - stable L ˆ evy distribution , S α S - Levy ( 0 , σ α ) . Note that the moment of the distribution S α S - Levy is ﬁnite up to only α order . When α = 2 , S α S - Levy degenerates to a Gaussian and d L αt is equivalent to a scaled Brownian motion . On the other hand , for α < 2 , the stochastic process in Eq . ( 36 ) features a Markov “jump” behavior , and theo - retical study indicates a longer stay in , i . e . the process prefers , wider minima valleys ( Bovier et al . , 2004 ) . The resulting heavy - tailed density aligns better with the empirical observation ( Chaudhari & Soatto , 2018 ) . 4 . 3 D ISCRETE - TIME D YNAMICS OF SGD Despite the rich analysis by formulating SGD as a continuous - time SDE , we should remind us of the implicit assumptions for Itˆo - Stratonovich calculus to apply . Beside the smoothness conditions on the stochastic process , mini - batch gradients are replaced with Gaussian to bring Brownian motion into the picture . The fact that the mean square displacement of Brownian motion scales linearly in time , i . e . E [ d W t 2 ] = d t , leads to a quadratic expansion on the loss function , as shown in Eq . ( 30 ) . In addition , the recast between Eq . ( 26 ) and ( 29 ) requires splitting η to √ η √ d t . The continuous limit reached by sending d t → 0 + while assuming ﬁnite √ η is arguably unjustiﬁed and pointed out in Yaida ( 2018 ) . The authors instead proposed a discrete - time master equation that alleviates these 7 The non - isotropy of Σ D is expected since the dimension of the parameter is much larger than the number of data points used for training . Empirical supports can be founded in Chaudhari & Soatto ( 2018 ) . 8 The derivation of Eq . ( 35 ) is quite involved as it relies on the equivalence between It ˆ o and A - type stochastic integration for the same FPE . We refer readers to Chaudhari & Soatto ( 2018 ) for a complete treatment . 14 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective drawbacks and is able to capture higher - order structures . We will restate the result and link it to the continuous - time SDE formulation as proceeding . Derivation : Recall that ρ t ( θ ) is the distribution of the parameter at time t . Its dynamics , when propagating to the next cycle t + 1 , can be written generally as ρ t + 1 ( θ ) = E θ (cid:48) ∼ ρ t , B (cid:2) δ (cid:8) θ − (cid:2) θ (cid:48) − ηg mb ( θ (cid:48) ) (cid:3)(cid:9)(cid:3) , ( 37 ) where δ { · } is the Kronecker delta and the expectation is taken over both the current distribution and the mini - batch sampling . Given an observable O ( θ ) : R ¯ m (cid:55)→ R , its master equation at steady - state equilibrium ρ ss can be written as E ρ ss [ O ( θ ) ] = E ρ ss (cid:2) E B (cid:2) O (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3)(cid:3) . ( 38 ) Full derivations are left in SM . B . Note that the only assumption we make so far is the existence of ρ ss . Eq . ( 38 ) suggests that at equilibrium , the expectation of an observable remains unchanged when averaging over the stochasticity from mini - batch gradient updates . Fluctuation Dissipation of Training Loss : Let we proceed by plugging the training loss Φ ( θ ) to our observable of interest in Eq . ( 38 ) . Taylor expanding it at η = 0 gives E ρ ss [ Φ ( θ ) ] = E ρ ss (cid:2) E B (cid:2) Φ (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3)(cid:3) = E ρ ss (cid:34) Φ ( θ ) + ∞ (cid:88) k = 1 ( − η ) k k ! D k E B (cid:2) Φ (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3)(cid:35) , ( 39 ) where D k F denotes the k - ordered expansion on a multivariate function F . Speciﬁcally , the ﬁrst and second expansions can be written as 9 D 1 E B (cid:2) Φ (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3) = (cid:80) i ∂ θ i Φ · E B (cid:2) g mb ( θ i ) (cid:3) = ∇ Φ T ∇ Φ , and ( 40 ) D 2 E B (cid:2) Φ (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3) = (cid:80) ( i , j ) H Φ , ( i , j ) ˜ Σ B , ( i , j ) = Tr (cid:104) H Φ ˜ Σ B (cid:105) . ( 41 ) For the last equality to hold in Eq . ( 39 ) , the expectation of the inﬁnite - series summation needs to vanish . Substituting Eq . ( 40 - 41 ) to ( 39 ) , we will obtain the following relation E ρ ss (cid:2) ∇ Φ T ∇ Φ (cid:3) = η 2 E ρ ss (cid:104) Tr (cid:104) H Φ ˜ Σ B (cid:105)(cid:105) + ∞ (cid:88) k = 3 ( − η ) k k ! E ρ ss , B (cid:2) D k Φ ( θ − ηg mb ( θ ) ) (cid:3) . ( 42 ) Eq . ( 42 ) can be viewed as the ﬂuctuation - dissipation equation , a key concept rooted in statistical me - chanics for bridging microscopic ﬂuctuations to macroscopic dissipative phenomena ( Yaida , 2018 ) . It should be noted , however , that Eq . ( 42 ) is the necessary but not sufﬁcient condition to ensure stationary . Let us compare Eq . ( 42 ) with its continuous - time counterpart in Eq . ( 32 ) . First , notice the difference between the two - point noise matrix , ˜ Σ B , and the covariance matrix of the sample gradient , ˜ Σ D . In fact , these two matrices can be related by ˜ Σ B ≈ (cid:16) 1 | B | − 1 | D | (cid:17) Σ D = 1 η (cid:16) 1 − | B | | D | (cid:17) ˜ Σ D ≈ 1 η ˜ Σ D , ( 43 ) where the ﬁrst approximation is followed by Proposition 1 in Hu et al . ( 2017 ) , and the second one by assuming | B | (cid:28) | D | . In the small learning rate regime , Eq . ( 32 ) and ( 42 ) are essentially equivalent and we consolidate the analysis from the continuous - time framework in Sec . 4 . 2 . The higher - order terms in Eq . ( 42 ) measure the anharmonicity of the loss landscape , which becomes nontrivial when the learning rate is large . 9 Applying the chain rule to D k E B [ · · · ] in Eq . ( 39 ) yields a clean form as D k E B (cid:2) Φ (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3) = (cid:80) D k ( i 1 , i 2 , ··· i k ) Φ ( θ ) · ˜ Σ B , ( i 1 , i 2 , ··· i k ) , where we recall Eq . ( 25 ) and denote D k ( i 1 , i 2 , ··· i k ) as the k - ordered partial derivatives w . r . t . parameter indices i 1 , i 2 , · · · i k . For instance , D 1 ( i ) Φ ( θ ) = ∂ θ i Φ corresponds to the i - element of the full gradient . D 2 ( i , j ) Φ ( θ ) = ∂ 2 Φ / ∂θ i ∂θ j refers to the ( i , j ) - entry of the Hessian . The summation (cid:80) is taken over combinations of indices i 1 , i 2 , · · · i k . 15 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective 4 . 4 I MPROVING SGD WITH O PTIMAL C ONTROL Interpreting SGD as a stochastic dynamical system makes control theory applicable . Note that this is different from what we have derived in Sec . 3 . The state space on which we wish to impose control is the parameters space R ¯ m , instead of the activation space R n . The fact that in Sec . 3 . 3 we are managing to apply control in R ¯ m limits out capability to go beyond theoretical characterization to practical algorithmic design due to high dimensionality . In contrast , here we do not specify where the control should take place , depending on how we introduce it to the stochastic dynamical system . Such a ﬂexibility has lead to algorithmic improvement of SGD dynamics . For instance , using optimal control to derive optimal adaptive strategies for hyper - parameters ( Li et al . , 2017b ) . The literature on adaptive ( e . g . annealed ) learning rate scheduling has been well - studied for convex problems ( Moulines & Bach , 2011 ; Xu , 2011 ) . The heuristic of decaying the learning rate with training cycle , i . e . ∼ 1 / t , typically works well for DNN training , despite its non - convexity . From the optimal control perspective , we can formulate the scheduling process mathematically by introducing to Eq . ( 29 ) a rescaling factor u t ∈ ( 0 , 1 ] and its continuous - time functional u t → T : [ t , T ] (cid:55)→ ( 0 , 1 ] . Applying similar derivations using Itˆo Lemma , we obtain d θ t = − u t g ( θ t ) d t + u t (cid:112) 2 β − 1 Σ D ( θ t ) d W t , ( 44 ) and d E [ Φ ( θ t ) ] = E (cid:20) − u t ∇ Φ T ∇ Φ + u 2 t 2 Tr (cid:104) H Φ ˜ Σ D (cid:105)(cid:21) d t . ( 45 ) The stochastic optimal control problem from this new dynamics can be written as min u t → T E [ Φ ( θ T ) ] s . t . ( 45 ) , ( 46 ) where T is the maximum training cycle . Li et al . ( 2017b ) showed that when Φ ( θ ) is quadratic , solving Eq . ( 46 ) using the HJB equation ( recall Theorem 2 ) yields a closed - form policy u ∗ t = min ( 1 , E [ Φ ( θ t ) ] η ˜ Σ D ) . ( 47 ) Intuitively , this optimal strategy suggests using the maximum learning rate when far from minima and decay it whenever ﬂuctuations begin to dominate . Further expansion on the ratio E [ Φ ( θ t ) ] / η ˜ Σ D will give us the annealing schedule of O ( 1 / t ) . In other words , the strategy proposed in the previous study is indeed optimal from the optimal control viewpoint . Also , notice that Eq . ( 47 ) is a feedback policy since the optimal adaptation depends on the statistics of the current parameter . For general loss functions , Eq . ( 47 ) can serve as a well - motivated heuristic . The resulting scheduling scheme has been shown to be more robust to initial conditions when compared with other SGD variants ( Li et al . , 2017b ) . Similarly , we can derive optimal adaptation strategies for other hyper - parameters , such as the momentum and batch size ( Li et al . , 2017b ; An et al . , 2018a ) . Lastly , we note that other learning rate adaptations , such as the constant - and - cut scheme , can also be included along this line by modifying Eq . ( 44 ) to accept general Markov jump processes . 5 B EYOND S UPERVISED L EARNING The optimal control framework in Sec . 3 . 3 ﬁts with supervised learning by absorbing labels into the terminal cost or augmented state space . In this section , we demonstrate how to extend the framework to other learning problems . Speciﬁcally , by allowing standard ( i . e . risk - neutral ) OCP and MF - OCP objectives , which minimize the expected loss incurred from the stochasticity , to be risk - aware , we generalize the formulation to consider statistical behaviors from higher - order moments . Depending on the problem setting , the risk - aware optimal control problem can be recast to Bayesian learning and adversarial training , as we will show in Sec . 5 . 2 and 5 . 3 . While the former viewpoint has been leveraged to impose priors on the training dynamics ( Chaudhari et al . , 2016 ; 2018 ) , the latter seeks to optimize worst - case perturbations from an adversarial attacker . Additionally , we will interpret meta - learning algorithms with a speciﬁc structure as feedback controllers in Sec . 5 . 4 . 5 . 1 P RELIMINARIES ON R ISK - A WARE O PTIMAL C ONTROL Risk sensitivity has been widely used in Markovian decision processes ( MDPs ) that require more sophisticated criteria to reﬂect the variability - risk features of the problems ( Coraluppi & Marcus , 16 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective 1999 ) . The resulting optimal control framework is particularly suitable for stochastic dynamical systems and closely related to robust and minimax control ( Bas¸ar & Bernhard , 2008 ) . To bring risk awareness into the original training objective , i . e . the per - sample objective J in Eq . ( OCP ) , we need to consider the following generalized exponential utility function : J k ( x ) : = (cid:26) 1 k log { E ξ [ exp ( kJ ( x , ξ ) ) ] } if k (cid:54) = 0 E ξ [ J ( x , ξ ) ] for k = 0 , ( 48 ) where ξ denotes any source of stochasticity that is being averaged over the expectation . When k = 0 , J k reduces to the risk - neutral empirical mean , i . e . Φ ( θ ) in Eq . ( 20 ) . In contrast , the log partition functional for k (cid:54) = 0 has a risk - aware interpretation , which can be mathematically described as J k (cid:54) = 0 ( x ) ≈ E J + k 2Var [ J ] . ( 49 ) We left the full derivation in SM . C . For positive k , the objective is risk - averse since in addition to the expectation , we also penalize the variation of the loss . In contrast , k < 0 results in a risk - seeking behavior as we now favor higher variability . From the optimization viewpoint , the log partition functional can be thought of as an approximation of a smooth max operator . The objective in Eq . ( 48 ) therefore inherits an inner - loop max / min optimization , depending on the sign of k : min x J k (cid:54) = 0 ( x ) ≈ (cid:40) min x max ξ J ( x , ξ ) if k > 0 min x min ξ J ( x , ξ ) if k < 0 . ( 50 ) From such , it is handy to characterize the optimal policy of a min - max objective as risk - averse , whereas the one from a min - min objective often reveals a risk - seeking tendency . This interpretation will become useful as we proceed to Sec . 5 . 2 and 5 . 3 . 5 . 2 B AYESIAN L EARNING & R ISK - S EEKING C ONTROL Recall that ﬂatter minima enjoy lower generalization gap since they are less sensitive to perturbations of the data distribution ( Zhang et al . , 2016 ) . In this spirit , Chaudhari et al . ( 2016 ) proposed the following local entropy loss in order to guide the SGD dynamics towards ﬂat plateaus faster : Φ ent ( θ ; γ ) : = − log (cid:90) θ (cid:48) ∈ R m exp ( − Φ ( θ (cid:48) ) − γ 2 (cid:107) θ − θ (cid:48) (cid:107) 22 ) d θ (cid:48) , ( 51 ) where Φ ( · ) is deﬁned in Eq . ( 20 ) and the hyper - parameter γ controls the degree of trade - off between the depth and width of the loss landscape . This surrogate loss is well - motivated from the statistical physics viewpoint since the objective balances between an energetic term ( i . e . training loss ) and entropy term ( i . e . ﬂatness of local geometry ) . In addition , it can be connected to numerical analysis on nonlinear PDE ( Chaudhari et al . , 2018 ) . Here , we provide an alternative perspective from risk - aware control and its connection to Bayesian inference . We know from Sec . 5 . 1 that the log partition functional approximates the max operator . Minimizing the local entropy loss therefore becomes min θ Φ ent ( θ ; γ ) ≈ min θ − max θ (cid:48) (cid:110) − Φ ( θ (cid:48) ) − γ 2 (cid:107) θ − θ (cid:48) (cid:107) 22 (cid:111) = min θ min θ (cid:48) (cid:110) Φ ( θ (cid:48) ) + γ 2 (cid:107) θ − θ (cid:48) (cid:107) 22 (cid:111) , ( 52 ) which is a nested optimization with an inner loop minimizing the same loss with a regularization term centered at θ . For ﬁxed θ (cid:48) , the outer loop simply optimizes a locally - approximated quadratic γ 2 (cid:107) θ − θ (cid:48) (cid:107) 22 . Casting this quadratic regularization as a distribution density and recalling the risk - seeking interpretation of the min - min objective , we have min θ Φ ent ( θ ; γ ) ≈ min θ E P γ [ Φ ( θ ) ] − 1 2Var P γ [ Φ ( θ ) ] . ( 53 ) P γ denotes the Gibbs distribution , P ( θ ; γ ) ∝ Z − 1 γ exp ( − γ 2 (cid:107) θ − θ (cid:48) (cid:107) 22 ) , with Z − 1 γ as the normal - ization term . The risk - seeking objective in Eq . ( 53 ) encourages exploration on areas with higher variability . This implies a potential improvement on the convergence speed , despite the overhead incurred from additional minimization . 17 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Solving Eq . ( 52 ) requires an expensive inner - loop minimization over the entire parameter space at each iteration . This can , however , be estimated with the stochastic gradient Langevin dynamic ( Welling & Teh , 2011 ) , an MCMC sampling technique for Bayesian inference . The resulting algo - rithm , named Entropy - SGD ( Chaudhari et al . , 2016 ) , obeys the following dynamics : d z s = − (cid:2) g mb ( z s ) + γ ( z s − θ t ) (cid:3) d s + √ (cid:15) d W s z 0 = θ t , ( 54 ) d θ t = γ ( z − θ t ) d t , ( 55 ) where z takes the same space as θ ∈ R ¯ m . Notice that the two dynamical systems in Eq . ( 54 ) and ( 55 ) operate in different time scales , denoted d s and d t respectively , and they correspond to the ﬁrst - order derivatives of the inner and outer minimization in Eq . ( 52 ) . Chaudhari et al . ( 2018 ) showed that in the asymptotic limit of the non - viscous approximation , i . e . (cid:15) → 0 , gradient descent on the local entropy loss , θ t + 1 ← θ t − η Φ ent ( θ t ; γ ) , is equivalent to a forward Euler step on the original loss function , θ t + 1 ← θ t − η Φ ( θ t + 1 ) . In other words , we may interpret the dynamics in Eq . ( 54 ) as a one - step prediction ( in a Bayesian fashion with a quadratic prior ) of the gradient at the next iteration . 5 . 3 A DVERSARIAL T RAINING AS M INIMAX C ONTROL Study on adversarial properties of DNN has become increasingly popular since the landmark paper in Szegedy et al . ( 2013 ) revealed its vulnerability to human - invisible perturbations . The conse - quence can be catastrophic from a security standpoint ( Goodfellow , 2018 ) , when machine learning algorithms in real - world applications , e . g . perception systems on self - driving vehicles , are intention - ally fooled ( i . e . attacked ) to make incorrect decisions at test time . Among the attempts to robustify deep models , adversarial training proposes to solve the following optimization problem : min θ max (cid:107) δ (cid:107) p ≤ ∆ Φ adv ( θ , δ ) : = 1 | D | (cid:88) i ∈D J ( f ( x ( i ) + δ ( i ) , θ ) , y ( i ) ) . ( 56 ) Φ adv ( θ , δ ) is equivalent to the original training loss ( c . f . Eq . ( 20 ) ) subjected to sample - wise pertur - bations δ ( i ) , which are of the same dimension as the input space and constrained within a p - norm ball with radius ∆ . Essentially , adversarial training seeks to ﬁnd a minimizer of the worse - case performance when data points are adversarially distorted . The min - max objective in Eq . ( 56 ) implies a risk - averse behavior , in contrast to the risk - seeking in Eq . ( 53 ) . Classical analyses from the minimax control theory suggest a slow convergence and a con - servative optimal policy . These arguments agree with practical observations as adversarial learning usually takes much longer time to train and admits a trade - off between adversarial robustness ( e . g . proportion of data points that are adversarial ) and generalization performance ( Goodfellow , 2018 ) . Algorithmically , the inner maximization is often evaluated on a set of adversarial examples generated on the ﬂy , depending on the current parameter , and the adversarial training objective is replaced with a mixture of the original loss function and this adversarial surrogate . The min - max problem in Eq . ( 56 ) is hence lower - bounded by min θ max (cid:107) δ (cid:107) p ≤ ∆ Φ adv ( θ , δ ) ≥ min θ α Φ ( θ ) + ( 1 − α ) Φ adv ( θ , ˆ δ ) , ( 57 ) where α ∈ ( 0 , 1 ] is the mixture ratio and ˆ δ : = Proj (cid:107)·(cid:107) p ≤ ∆ [ Alg ( θ , Φ ( · ) ; D ) ] denotes the p - norm projected perturbation generated from an algorithm , Alg . The approach can be viewed as an adaptive data augmentation technique . We should note , however , that the i . i . d . assumption on the training dataset no longer holds in this scenario and we may require exponentially more data to prevent over - ﬁtting ( Schmidt et al . , 2018 ) . Lastly , it is possible to instead upper - bound the objective with a convex relaxation , which will lead to a provably robust model to any norm - bounded adversarial attack ( Wong & Kolter , 2017 ) . 5 . 4 M ETA L EARNING AS F EEDBACK C ONTROLLER Meta - learning aims to discover prior knowledge from a set of learnable tasks such that the learned initial parameter provides a favorable inductive bias for fast adaptation to unseen tasks at test time . The learning problems , often called learning to learn , is naturally applicable to those involving prior 18 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective distillation from limited data , such as few - shot classiﬁcation ( Finn et al . , 2017 ) and reinforcement learning in fast - changing environments ( Duan et al . , 2016 ) . It can also be cast to probabilistic inference in a hierarchical Bayesian model ( Grant et al . , 2018 ) . Here , we bridge a popular branch of algorithms , namely model - agnostic meta - learning ( MAML ) ( Finn et al . , 2017 ) , to the feedback control viewpoint . In the problem formulation of MAML , an agent is given a distribution of tasks , T i ∼ P T , with the task - dependent cost function , Φ T i ( · ) , and asked to ﬁnd an initialization that can continuously adapt to other unseen tasks drawn from P T . The meta - training objective and adaptation rule can be written as Φ meta ( θ ; P T ) : = E T i ∼P T (cid:2) Φ T i ( θ N adapt ) (cid:3) , ( 58 ) where θ n + 1 adapt = θ n adapt − ¯ η ∇ θ Φ T i ( θ n adapt ) and θ 0 adapt = θ . ( 59 ) θ N adapt denotes an N - step adaptation from the current parameter using gradient descent with the step size ¯ η at each update . N is a hyper - parameter that generalizes standard objectives to Φ meta ( · ) for positive N . As N increases , regularization will be imposed on the meta - training process in the sense that the agent is encouraged to ﬁnd a minimizer no more than N steps away from the local minima of each task , instead of over - ﬁtting to the one of any particular task . Now , recall the interpretation of gradient descent as integral control in Sec . 3 . 3 . Through this lens , the adaptation rule in Eq . ( 59 ) can be thought of as an N - step integral controller , and minimizing Eq . ( 58 ) is equivalent to searching an optimal initial condition for the controller . Since feedback controllers are originally designed for problems requiring on - line adaptation and integral controllers feature zero steady - state errors , we consolidate the theoretical foundation of MAML - inspired algo - rithms . Implications from this viewpoint can leverage knowledge from control literature to design more sophisticated and / or principled adaptation rules . We may also derive optimal adaptation rules for other hyper - parameters , such as the step size ¯ η and adaptation number N , similar to what we have shown in Sec . 4 . 4 . 6 C ONCLUSION This review aims to align several seemly disconnected viewpoints of deep learning theory with the line of dynamical system and optimal control . We ﬁrst observe that the compositionality of DNNs and the descending update in SGD suggest an interpretation of discrete - time ( stochastic ) dynamical systems . Rich mathematical analysis can be applied when certain assumptions are made to bring the realization to its continuous - time limit . The framework forms the basis of most recent understand - ings of deep learning , by recasting DNN as an ordinary differential equation and SGD as a stochastic differential equation . Among the available mathematical tools , we should highlight the signiﬁcance of mean - ﬁeld theory and stochastic calculus , which enable characterization of the dynamics of deep representation and stochastic functionals ( e . g . the training loss or parameter distribution ) at the ensemble level . The dynamical perspective alone has revealed valuable implications , as it success - fully gives predictions to e . g . the trainability of random networks from critical initialization , the interaction between gradient drift and noise diffusion during training , the concrete form of implicit regularization from SGD , and even the global optimality of deep learning problems , to name a few . Another appealing implication , despite receiving little attention , is to introduce the optimal control theory to the corresponding dynamics . To emphasize its importance , we note that the celebrated back - propagation algorithm is , in fact , an approximation of the Pontryagins Maximum Principle ( PMP ) , a well - known theory dated back to the 1960s that describes the necessary conditions to the optimal control problems . Limited works inspired from this viewpoint include optimal adaptive strategies for hyper - parameters and maximum principle based optimization algorithms . When the standard optimal control objective is extended to accept higher - order statistical moments , the re - sulting “risk - aware” optimal control framework generalizes beyond supervised learning , to include problems such as Bayesian learning , adversarial training , and meta learning . We wish this article stands as a foundation to open up new avenues that may bridge and beneﬁt communities from both deep learning and optimal control . For future directions , we note that the optimal control theory for DNNs training is far from being completed , and relaxing the currently presented theorems to a more realistic setting will be beneﬁ - cial . For instance , despite the thorough discussion in Sec . 3 . 3 , our derivation is mainly constructed 19 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective upon the continuous - time framework to avoid the difﬁculties incurred from the discrete - time analy - sis . Additionally , while an initial attempt to bridge other learning problems to the proposed frame - work has been taken in Sec . 5 , more are left to be explored . Speciﬁcally , Generative Adversarial Networks are closely related to minimax control , and the dynamical analysis from an SDE viewpoint has been recently discussed to reveal an intriguing variational interpretation ( Tao et al . , 2019 ) . R EFERENCES Alessandro Achille and Stefano Soatto . Emergence of invariance and disentanglement in deep representations . The Journal of Machine Learning Research , 19 ( 1 ) : 1947 – 1980 , 2018 . Alessandro Achille and Stefano Soatto . Where is the information in a deep neural network ? CoRR , abs / 1905 . 12213 , 2019 . URL http : / / arxiv . org / abs / 1905 . 12213 . Alessandro Achille , Matteo Rovere , and Stefano Soatto . Critical learning periods in deep networks . In Inter - national Conference on Learning Representations , 2019 . URL https : / / openreview . net / forum ? id = BkeStsCcKQ . Zeyuan Allen - Zhu , Yuanzhi Li , and Zhao Song . A convergence theory for deep learning via over - parameterization . arXiv preprint arXiv : 1811 . 03962 , 2018 . Luigi Ambrosio , Nicola Gigli , and Giuseppe Savar´e . Gradient ﬂows : in metric spaces and in the space of probability measures . Springer Science & Business Media , 2008 . Jing An , Jianfeng Lu , and Lexing Ying . Stochastic modiﬁed equations for the asynchronous stochastic gradient descent . arXiv preprint arXiv : 1805 . 08244 , 2018a . Wangpeng An , Haoqian Wang , Qingyun Sun , Jun Xu , Qionghai Dai , and Lei Zhang . A pid controller approach for stochastic optimization of deep networks . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 8522 – 8531 , 2018b . Anish Athalye and Nicholas Carlini . On the robustness of the cvpr 2018 white - box adversarial example de - fenses . arXiv preprint arXiv : 1804 . 03286 , 2018 . Tamer Bas¸ar and Pierre Bernhard . H - inﬁnity optimal control and related minimax design problems : a dynamic game approach . Springer Science & Business Media , 2008 . Richard Ernest Bellman and Robert E Kalaba . Selected papers on mathematical trends in control theory . Dover Publications , 1964 . Dimitri P Bertsekas , Dimitri P Bertsekas , Dimitri P Bertsekas , and Dimitri P Bertsekas . Dynamic programming and optimal control , volume 1 . Athena scientiﬁc Belmont , MA , 1995 . Vladimir Grigor’evich Boltyanskii , Revaz Valer’yanovich Gamkrelidze , and Lev Semenovich Pontryagin . The theory of optimal processes . i . the maximum principle . Technical report , TRW SPACE TECHNOLOGY LABS LOS ANGELES CALIF , 1960 . Anton Bovier , Michael Eckhoff , V ´ eronique Gayrard , and Markus Klein . Metastability in reversible diffusion processes i : Sharp asymptotics for capacities and exit times . Journal of the European Mathematical Society , 6 ( 4 ) : 399 – 424 , 2004 . Pratik Chaudhari and Stefano Soatto . Stochastic gradient descent performs variational inference , converges to limit cycles for deep networks . In 2018 Information Theory and Applications Workshop ( ITA ) , pp . 1 – 10 . IEEE , 2018 . Pratik Chaudhari , Anna Choromanska , Stefano Soatto , Yann LeCun , Carlo Baldassi , Christian Borgs , Jennifer Chayes , Levent Sagun , and Riccardo Zecchina . Entropy - sgd : Biasing gradient descent into wide valleys . arXiv preprint arXiv : 1611 . 01838 , 2016 . Pratik Chaudhari , Adam Oberman , Stanley Osher , Stefano Soatto , and Guillaume Carlier . Deep relaxation : partial differential equations for optimizing deep neural networks . Research in the Mathematical Sciences , 5 ( 3 ) : 30 , 2018 . Minmin Chen , Jeffrey Pennington , and Samuel S Schoenholz . Dynamical isometry and a mean ﬁeld theory of rnns : Gating enables signal propagation in recurrent neural networks . arXiv preprint arXiv : 1806 . 05394 , 2018a . 20 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Ricky T . Q . Chen and David Duvenaud . Neural networks with cheap differential operators . In 2019 ICML Workshop on Invertible Neural Nets and Normalizing Flows ( INNF ) , 2019 . Tian Qi Chen , Yulia Rubanova , Jesse Bettencourt , and David K Duvenaud . Neural ordinary differential equa - tions . In Advances in Neural Information Processing Systems , pp . 6572 – 6583 , 2018b . Stefano P Coraluppi and Steven I Marcus . Risk - sensitive and minimax control of discrete - time , ﬁnite - state markov decision processes . Automatica , 35 ( 2 ) : 301 – 309 , 1999 . Yann N Dauphin , Razvan Pascanu , Caglar Gulcehre , Kyunghyun Cho , Surya Ganguli , and Yoshua Bengio . Identifying and attacking the saddle point problem in high - dimensional non - convex optimization . In Ad - vances in neural information processing systems , pp . 2933 – 2941 , 2014 . Simon S Du , Jason D Lee , Haochuan Li , Liwei Wang , and Xiyu Zhai . Gradient descent ﬁnds global minima of deep neural networks . arXiv preprint arXiv : 1811 . 03804 , 2018a . Simon S Du , Xiyu Zhai , Barnabas Poczos , and Aarti Singh . Gradient descent provably optimizes over - parameterized neural networks . arXiv preprint arXiv : 1810 . 02054 , 2018b . Yan Duan , John Schulman , Xi Chen , Peter L Bartlett , Ilya Sutskever , and Pieter Abbeel . Rl 2 : Fast reinforcement learning via slow reinforcement learning . arXiv preprint arXiv : 1611 . 02779 , 2016 . Chelsea Finn , Pieter Abbeel , and Sergey Levine . Model - agnostic meta - learning for fast adaptation of deep networks . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , pp . 1126 – 1135 . JMLR . org , 2017 . Gene F Franklin , J David Powell , Abbas Emami - Naeini , and J David Powell . Feedback control of dynamic systems , volume 3 . Addison - Wesley Reading , MA , 1994 . Adri ` a Garriga - Alonso , Laurence Aitchison , and Carl Edward Rasmussen . Deep convolutional networks as shallow gaussian processes . arXiv preprint arXiv : 1808 . 05587 , 2018 . Robert Geirhos , Patricia Rubisch , Claudio Michaelis , Matthias Bethge , Felix A Wichmann , and Wieland Bren - del . Imagenet - trained cnns are biased towards texture ; increasing shape bias improves accuracy and robust - ness . arXiv preprint arXiv : 1811 . 12231 , 2018 . Dar Gilboa , Bo Chang , Minmin Chen , Greg Yang , Samuel S Schoenholz , Ed H Chi , and Jeffrey Pennington . Dynamical isometry and a mean ﬁeld theory of lstms and grus . arXiv preprint arXiv : 1901 . 08987 , 2019 . Xavier Glorot and Yoshua Bengio . Understanding the difﬁculty of training deep feedforward neural networks . In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics , pp . 249 – 256 , 2010 . Gabriel Goh . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . Sebastian Goldt and Udo Seifert . Stochastic thermodynamics of learning . Physical review letters , 118 ( 1 ) : 010601 , 2017a . Sebastian Goldt and Udo Seifert . Thermodynamic efﬁciency of learning a rule in neural networks . New Journal of Physics , 19 ( 11 ) : 113001 , 2017b . Ian Goodfellow . Defense against the dark arts : An overview of adversarial example security research and future research directions . arXiv preprint arXiv : 1806 . 04169 , 2018 . Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy . Explaining and harnessing adversarial examples . arXiv preprint arXiv : 1412 . 6572 , 2014 . Erin Grant , Chelsea Finn , Sergey Levine , Trevor Darrell , and Thomas Grifﬁths . Recasting gradient - based meta - learning as hierarchical bayes . arXiv preprint arXiv : 1801 . 08930 , 2018 . Suriya Gunasekar , Jason Lee , Daniel Soudry , and Nathan Srebro . Characterizing implicit bias in terms of optimization geometry . arXiv preprint arXiv : 1802 . 08246 , 2018a . Suriya Gunasekar , Jason D Lee , Daniel Soudry , and Nati Srebro . Implicit bias of gradient descent on linear convolutional networks . In Advances in Neural Information Processing Systems , pp . 9482 – 9491 , 2018b . Jiequn Han , Qianxiao Li , et al . A mean - ﬁeld optimal control formulation of deep learning . arXiv preprint arXiv : 1807 . 01083 , 2018 . 21 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Moritz Hardt , Benjamin Recht , and Yoram Singer . Train faster , generalize better : Stability of stochastic gradient descent . arXiv preprint arXiv : 1509 . 01240 , 2015 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 770 – 778 , 2016 . Bin Hu and Laurent Lessard . Control interpretations for ﬁrst - order optimization methods . In 2017 American Control Conference ( ACC ) , pp . 3114 – 3119 . IEEE , 2017 . Wenqing Hu , Chris Junchi Li , Lei Li , and Jian - Guo Liu . On the diffusion approximation of nonconvex stochas - tic gradient descent . arXiv preprint arXiv : 1705 . 07562 , 2017 . Kiyosi Itˆo . On stochastic differential equations , volume 4 . American Mathematical Soc . , 1951 . Arthur Jacot , Franck Gabriel , and Cl´ement Hongler . Neural tangent kernel : Convergence and generalization in neural networks . In Advances in neural information processing systems , pp . 8571 – 8580 , 2018 . Max Jaderberg , Volodymyr Mnih , Wojciech Marian Czarnecki , Tom Schaul , Joel Z Leibo , David Silver , and Koray Kavukcuoglu . Reinforcement learning with unsupervised auxiliary tasks . arXiv preprint arXiv : 1611 . 05397 , 2016 . Stanislaw Jastrzebski , Zachary Kenton , Devansh Arpit , Nicolas Ballas , Asja Fischer , Yoshua Bengio , and Amos Storkey . Three factors inﬂuencing minima in sgd . arXiv preprint arXiv : 1711 . 04623 , 2017 . Ziwei Ji and Matus Telgarsky . Gradient descent aligns the layers of deep linear networks . arXiv preprint arXiv : 1810 . 02032 , 2018 . Richard Jordan , David Kinderlehrer , and Felix Otto . The variational formulation of the fokker – planck equation . SIAM journal on mathematical analysis , 29 ( 1 ) : 1 – 17 , 1998 . Ryo Karakida , Shotaro Akaho , and Shun - ichi Amari . Universal statistics of ﬁsher information in deep neural networks : Mean ﬁeld approach . arXiv preprint arXiv : 1806 . 01316 , 2018 . Nitish Shirish Keskar , Dheevatsa Mudigere , Jorge Nocedal , Mikhail Smelyanskiy , and Ping Tak Peter Tang . On large - batch training for deep learning : Generalization gap and sharp minima . arXiv preprint arXiv : 1609 . 04836 , 2016 . Robert Kleinberg , Yuanzhi Li , and Yang Yuan . An alternative view : When does sgd escape local minima ? arXiv preprint arXiv : 1802 . 06175 , 2018 . Andrei Kolmogoroff . ¨Uber die analytischen methoden in der wahrscheinlichkeitsrechnung . Mathematische Annalen , 104 ( 1 ) : 415 – 458 , 1931 . Alex Krizhevsky , Ilya Sutskever , and Geoffrey E Hinton . Imagenet classiﬁcation with deep convolutional neural networks . In Advances in neural information processing systems , pp . 1097 – 1105 , 2012 . Yann LeCun , Bernhard E Boser , John S Denker , Donnie Henderson , Richard E Howard , Wayne E Hubbard , and Lawrence D Jackel . Handwritten digit recognition with a back - propagation network . In Advances in neural information processing systems , pp . 396 – 404 , 1990 . Jaehoon Lee , Yasaman Bahri , Roman Novak , Samuel S Schoenholz , Jeffrey Pennington , and Jascha Sohl - Dickstein . Deep neural networks as gaussian processes . arXiv preprint arXiv : 1711 . 00165 , 2017a . Jason D Lee , Ioannis Panageas , Georgios Piliouras , Max Simchowitz , Michael I Jordan , and Benjamin Recht . First - order methods almost always avoid saddle points . arXiv preprint arXiv : 1710 . 07406 , 2017b . Chunyuan Li , Changyou Chen , David Carlson , and Lawrence Carin . Preconditioned stochastic gradient langevin dynamics for deep neural networks . In Thirtieth AAAI Conference on Artiﬁcial Intelligence , 2016 . Qianxiao Li and Shuji Hao . An optimal control approach to deep learning and applications to discrete - weight neural networks . arXiv preprint arXiv : 1803 . 01299 , 2018 . Qianxiao Li , Long Chen , Cheng Tai , and E Weinan . Maximum principle based algorithms for deep learning . The Journal of Machine Learning Research , 18 ( 1 ) : 5998 – 6026 , 2017a . Qianxiao Li , Cheng Tai , et al . Stochastic modiﬁed equations and adaptive stochastic gradient algorithms . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , pp . 2101 – 2110 . JMLR . org , 2017b . 22 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Yuanzhi Li and Yingyu Liang . Learning overparameterized neural networks via stochastic gradient descent on structured data . In Advances in Neural Information Processing Systems , pp . 8157 – 8166 , 2018 . Guan - Horng Liu , Avinash Siravuru , Sai Prabhakar , Manuela Veloso , and George Kantor . Learning end - to - end multimodal sensor policies for autonomous navigation . arXiv preprint arXiv : 1705 . 10422 , 2017 . Yiping Lu , Aoxiao Zhong , Quanzheng Li , and Bin Dong . Beyond ﬁnite layer neural networks : Bridging deep architectures and numerical differential equations . arXiv preprint arXiv : 1710 . 10121 , 2017 . Alexander G de G Matthews , Mark Rowland , Jiri Hron , Richard E Turner , and Zoubin Ghahramani . Gaussian process behaviour in wide deep neural networks . arXiv preprint arXiv : 1804 . 11271 , 2018 . Grigorii Noikhovich Milstein . Numerical integration of stochastic differential equations , volume 313 . Springer Science & Business Media , 1994 . Eric Moulines and Francis R Bach . Non - asymptotic analysis of stochastic approximation algorithms for ma - chine learning . In Advances in Neural Information Processing Systems , pp . 451 – 459 , 2011 . Behnam Neyshabur , Ryota Tomioka , Ruslan Salakhutdinov , and Nathan Srebro . Geometry of optimization and implicit regularization in deep learning . arXiv preprint arXiv : 1705 . 03071 , 2017 . Arild Nøkland and Lars Hiller Eidnes . Training neural networks with local error signals . arXiv preprint arXiv : 1901 . 06656 , 2019 . Bernt Øksendal . Stochastic differential equations . In Stochastic differential equations , pp . 65 – 84 . Springer , 2003 . Grigorios A Pavliotis . Stochastic processes and applications : diffusion processes , the Fokker - Planck and Langevin equations , volume 60 . Springer , 2014 . Jeffrey Pennington , Samuel S Schoenholz , and Surya Ganguli . The emergence of spectral universality in deep networks . arXiv preprint arXiv : 1802 . 09979 , 2018 . Ben Poole , Subhaneil Lahiri , Maithra Raghu , Jascha Sohl - Dickstein , and Surya Ganguli . Exponential ex - pressivity in deep neural networks through transient chaos . In Advances in neural information processing systems , pp . 3360 – 3368 , 2016 . Ning Qian . On the momentum term in gradient descent learning algorithms . Neural networks , 12 ( 1 ) : 145 – 151 , 1999 . Andrew Michael Saxe , Yamini Bansal , Joel Dapello , Madhu Advani , Artemy Kolchinsky , Brendan Daniel Tracey , and David Daniel Cox . On the information bottleneck theory of deep learning . 2018 . Ludwig Schmidt , Shibani Santurkar , Dimitris Tsipras , Kunal Talwar , and Aleksander Madry . Adversarially robust generalization requires more data . In Advances in Neural Information Processing Systems , pp . 5014 – 5026 , 2018 . Samuel S Schoenholz , Justin Gilmer , Surya Ganguli , and Jascha Sohl - Dickstein . Deep information propaga - tion . arXiv preprint arXiv : 1611 . 01232 , 2016 . Ravid Shwartz - Ziv and Naftali Tishby . Opening the black box of deep neural networks via information . arXiv preprint arXiv : 1703 . 00810 , 2017 . David Silver , Aja Huang , Chris J Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser , Ioannis Antonoglou , Veda Panneershelvam , Marc Lanctot , et al . Mastering the game of go with deep neural networks and tree search . nature , 529 ( 7587 ) : 484 , 2016 . Karen Simonyan , Andrea Vedaldi , and Andrew Zisserman . Deep inside convolutional networks : Visualising image classiﬁcation models and saliency maps . arXiv preprint arXiv : 1312 . 6034 , 2013 . Umut Simsekli , Levent Sagun , and Mert Gurbuzbalaban . A tail - index analysis of stochastic gradient noise in deep neural networks . arXiv preprint arXiv : 1901 . 06053 , 2019 . Samuel L Smith and Quoc V Le . A bayesian perspective on generalization and stochastic gradient descent . arXiv preprint arXiv : 1710 . 06451 , 2017 . Sho Sonoda and Noboru Murata . Transport analysis of inﬁnitely deep neural network . Journal of Machine Learning Research , 20 ( 2 ) : 1 – 52 , 2019 . URL http : / / jmlr . org / papers / v20 / 16 - 243 . html . 23 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Daniel Soudry , Elad Hoffer , Mor Shpigel Nacson , Suriya Gunasekar , and Nathan Srebro . The implicit bias of gradient descent on separable data . The Journal of Machine Learning Research , 19 ( 1 ) : 2822 – 2878 , 2018 . Nitish Srivastava and Ruslan R Salakhutdinov . Multimodal learning with deep boltzmann machines . In Ad - vances in neural information processing systems , pp . 2222 – 2230 , 2012 . Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initialization and momentum in deep learning . In International conference on machine learning , pp . 1139 – 1147 , 2013 . Christian Szegedy , Wojciech Zaremba , Ilya Sutskever , Joan Bruna , Dumitru Erhan , Ian Goodfellow , and Rob Fergus . Intriguing properties of neural networks . arXiv preprint arXiv : 1312 . 6199 , 2013 . Chenyang Tao , Shuyang Dai , Liqun Chen , Ke Bai , Junya Chen , Chang Liu , Ruiyi Zhang , Georgiy Bobashev , and Lawrence Carin Duke . Variational annealing of gans : A langevin perspective . In International Confer - ence on Machine Learning , pp . 6176 – 6185 , 2019 . Yee Whye Teh , Alexandre H Thiery , and Sebastian J Vollmer . Consistency and ﬂuctuations for stochastic gradient langevin dynamics . The Journal of Machine Learning Research , 17 ( 1 ) : 193 – 225 , 2016 . Naftali Tishby and Noga Zaslavsky . Deep learning and the information bottleneck principle . In 2015 IEEE Information Theory Workshop ( ITW ) , pp . 1 – 5 . IEEE , 2015 . Guillermo Valle - Perez , Chico Q . Camargo , and Ard A . Louis . Deep learning generalizes because the parameter - function map is biased towards simple functions . In International Conference on Learning Representations , 2019 . URL https : / / openreview . net / forum ? id = rye4g3AqFm . Shoujin Wang , Wei Liu , Jia Wu , Longbing Cao , Qinxue Meng , and Paul J Kennedy . Training deep neural networks on imbalanced data sets . In 2016 international joint conference on neural networks ( IJCNN ) , pp . 4368 – 4374 . IEEE , 2016 . E Weinan . A proposal on machine learning via dynamical systems . Communications in Mathematics and Statistics , 5 ( 1 ) : 1 – 11 , 2017 . Max Welling and Yee W Teh . Bayesian learning via stochastic gradient langevin dynamics . In Proceedings of the 28th international conference on machine learning ( ICML - 11 ) , pp . 681 – 688 , 2011 . Christopher KI Williams . Computing with inﬁnite networks . In Advances in neural information processing systems , pp . 295 – 301 , 1997 . Eric Wong and J Zico Kolter . Provable defenses against adversarial examples via the convex outer adversarial polytope . arXiv preprint arXiv : 1711 . 00851 , 2017 . Lei Wu , Chao Ma , and E Weinan . How sgd selects the global minima in over - parameterized learning : A dynamical stability perspective . In Advances in Neural Information Processing Systems , pp . 8279 – 8288 , 2018 . Lechao Xiao , Yasaman Bahri , Jascha Sohl - Dickstein , Samuel S Schoenholz , and Jeffrey Pennington . Dy - namical isometry and a mean ﬁeld theory of cnns : How to train 10 , 000 - layer vanilla convolutional neural networks . arXiv preprint arXiv : 1806 . 05393 , 2018 . Wei Xu . Towards optimal one pass large scale learning with averaged stochastic gradient descent . arXiv preprint arXiv : 1107 . 2490 , 2011 . Sho Yaida . Fluctuation - dissipation relations for stochastic gradient descent . arXiv preprint arXiv : 1810 . 00004 , 2018 . Ge Yang and Samuel Schoenholz . Mean ﬁeld residual networks : On the edge of chaos . In Advances in neural information processing systems , pp . 7103 – 7114 , 2017 . Greg Yang , Jeffrey Pennington , Vinay Rao , Jascha Sohl - Dickstein , and Samuel S Schoenholz . A mean ﬁeld theory of batch normalization . arXiv preprint arXiv : 1902 . 08129 , 2019 . Chiyuan Zhang , Samy Bengio , Moritz Hardt , Benjamin Recht , and Oriol Vinyals . Understanding deep learning requires rethinking generalization . arXiv preprint arXiv : 1611 . 03530 , 2016 . Zhanxing Zhu , Jingfeng Wu , Bing Yu , Lei Wu , and Jinwen Ma . The anisotropic noise in stochastic gradient descent : Its behavior of escaping from minima and regularization effects . 2018 . Difan Zou , Yuan Cao , Dongruo Zhou , and Quanquan Gu . Stochastic gradient descent optimizes over - parameterized deep relu networks . arXiv preprint arXiv : 1811 . 08888 , 2018 . 24 Deep Learning Theory Review : An Optimal Control and Dynamical Systems Perspective Supplementary Material A E XTEND A NALYSIS IN S EC . 3 . 1 AND 3 . 2 TO O THER A RCHITECTURES Critical initialization and mean ﬁeld approximation can be applied to convolution layers as the num - ber of channels goes to limit ( Xiao et al . , 2018 ) . Similar results can be derived except (cid:15) t now traverses with a much richer dynamics through convoluted operators . For recurrent architectures , e . g . RNN and LSTM , the theory suggests that gating mechanisms facilitate efﬁcient signal propaga - tion ( Chen et al . , 2018a ; Gilboa et al . , 2019 ) . However , it also casts doubt on several practically - used modules , as the analysis suggests batch normalization causes exploding gradient signal ( Schoenholz et al . , 2016 ) and dropout destroys the order - to - chaos critical point ( Yang et al . , 2019 ) . Global opti - mality of GD for other architectures can be proved following similar derivations . Appendix E in Du et al . ( 2018a ) provides a general framework to include FC - DNN , ResNet , and convolution DNN . B D ERIVATION OF THE D ISCRETE - T IME M ASTER E Q . ( 38 ) Here we recapitulate the derivation from Yaida ( 2018 ) . First , recall Eq . ( 37 ) : ρ t + 1 ( θ ) = E θ (cid:48) ∼ ρ t , B (cid:2) δ (cid:8) θ − (cid:2) θ (cid:48) − ηg mb ( θ (cid:48) ) (cid:3)(cid:9)(cid:3) = E B (cid:20)(cid:90) d θ (cid:48) ρ t ( θ (cid:48) ) δ (cid:8) θ − (cid:2) θ (cid:48) − ηg mb ( θ (cid:48) ) (cid:3)(cid:9)(cid:21) . The steady - state distribution therefore obeys the relation ρ ss ( θ ) = E B (cid:20)(cid:90) d θ (cid:48) ρ ss ( θ (cid:48) ) δ (cid:8) θ − (cid:2) θ (cid:48) − ηg mb ( θ (cid:48) ) (cid:3)(cid:9)(cid:21) . ( 60 ) Now , substitute Eq . ( 60 ) to the expectation of an observable O ( θ ) at equilibrium E ρ ss [ O ( θ ) ] = (cid:90) d θ ρ ss ( θ ) O ( θ ) = E B (cid:20)(cid:90) d θ (cid:90) d θ (cid:48) ρ ss ( θ (cid:48) ) δ (cid:8) θ − (cid:2) θ (cid:48) − ηg mb ( θ (cid:48) ) (cid:3)(cid:9) O ( θ ) (cid:21) = E B (cid:20)(cid:90) d θ (cid:90) d θ (cid:48) ρ ss ( θ (cid:48) ) O ( θ (cid:48) − ηg mb ( θ (cid:48) ) ) (cid:21) = E ρ ss (cid:2) E B (cid:2) O (cid:0) θ − ηg mb ( θ ) (cid:1)(cid:3)(cid:3) . ( 61 ) We obtain its master equation at equilibrium Eq . ( 38 ) . C D ERIVATION OF R ISK - A WARE I NTERPRETATION E Q . ( 49 ) Recall the Taylor expansion of exp and log functions are exp ( x ) = 1 + (cid:80) ∞ k = 1 x k k ! , and log ( 1 + x ) = (cid:80) ∞ k = 1 ( − 1 ) k − 1 x k k . Expanding the objective J k (cid:54) = 0 up to second order leads to J k (cid:54) = 0 ( x ) = 1 k log E [ exp ( kJ ) ] ≈ 1 k log E (cid:20) 1 + kJ + k 2 2 J 2 (cid:21) (cid:46) Taylor expansion on exp ≈ 1 k (cid:34)(cid:18) k E J + k 2 2 E J 2 (cid:19) − 1 2 (cid:18) k E J + k 2 2 E J 2 (cid:19) 2 (cid:35) (cid:46) Taylor expansion on log = 1 k (cid:20) k E J + k 2 2 (cid:2) E J 2 − ( E J ) 2 (cid:3) + O ( k 3 ) (cid:21) = E J + k 2Var [ J ] + O ( k 2 ) . ( 62 ) For small k , the higher - order term O ( k 2 ) is negligible and we obtain the risk - aware interpretation of Eq . ( 49 ) , 25