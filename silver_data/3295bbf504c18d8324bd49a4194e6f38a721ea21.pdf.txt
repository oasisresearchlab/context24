DOI : 10 . 4018 / IJDET . 2017100105 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 ﻿ Copyright﻿©﻿2017 , ﻿IGI﻿Global . ﻿Copying﻿or﻿distributing﻿in﻿print﻿or﻿electronic﻿forms﻿without﻿written﻿permission﻿of﻿IGI﻿Global﻿is﻿prohibited . ﻿ Using Learning Analytics to Support Engagement in Collaborative Writing Ming Liu , Southwest University , Chongqing , China Abelardo Pardo , School of Electrical and Information Engineering , The University of Sydney , Sydney , Australia Li Liu , Chongqing University , Chongqing , China ABSTRACT Online﻿collaborative﻿writing﻿tools﻿provide﻿an﻿efficient﻿way﻿to﻿complete﻿a﻿writing﻿task . ﻿However , ﻿ existing﻿tools﻿only﻿focus﻿on﻿technological﻿affordances﻿and﻿ignore﻿the﻿importance﻿of﻿social﻿affordances﻿in﻿a﻿collaborative﻿learning﻿environment . ﻿This﻿article﻿describes﻿a﻿learning﻿analytic﻿system﻿that﻿analyzes﻿ writing﻿behaviors , ﻿and﻿creates﻿visualizations﻿incorporating﻿individual﻿engagement﻿awareness﻿and﻿ group﻿ranking﻿awareness﻿ ( social﻿affordance ) , ﻿and﻿review﻿writing﻿behaviour﻿history﻿ ( technological﻿ affordance ) , ﻿to﻿support﻿student﻿engagement . ﻿Studies﻿examined﻿the﻿performance﻿of﻿the﻿system﻿used﻿by﻿ university﻿students﻿in﻿two﻿collaborative﻿writing﻿activities : ﻿collaboratively﻿writing﻿a﻿project﻿proposal﻿ ( N﻿ = ﻿41 ) ﻿and﻿writing﻿tutorial﻿discussion﻿answers﻿ ( N﻿ = ﻿25 ) . ﻿Results﻿show﻿that﻿students﻿agreed﻿with﻿ what﻿the﻿visualization﻿conveys﻿and﻿visualizations﻿enhance﻿their﻿engagement﻿in﻿a﻿collaborative﻿writing﻿activity . ﻿In﻿addition , ﻿students﻿stated﻿that﻿the﻿visualizations﻿were﻿useful﻿to﻿help﻿them﻿reflect﻿on﻿the﻿ writing﻿process﻿and﻿support﻿the﻿assessment﻿of﻿individual﻿contributions . KEyWoRDS Collaborative Writing , Group Awareness , Learning Analytics , Online Collaborative Writing Tools , Student Engagement , Visualization 1 . INTRoDUCTIoN Writing﻿is﻿an﻿important﻿factor﻿of﻿teaching﻿and﻿learning﻿in﻿university﻿settings , ﻿which﻿cultivates﻿students’﻿ self - expression , ﻿construction﻿of﻿identity , ﻿understanding﻿and﻿knowledge﻿building﻿ ( Galbraith , ﻿1999 ) . ﻿ Writing﻿has﻿been﻿mostly﻿considered﻿an﻿individual﻿learning﻿activity . ﻿In﻿recent﻿years , ﻿collaborative﻿ writing﻿has﻿attracted﻿many﻿educational﻿researchers’﻿interests﻿due﻿to﻿the﻿discovery﻿of﻿new﻿pedagogical﻿benefits . ﻿As﻿recent﻿research﻿initiatives﻿illustrate , ﻿collaborative﻿writing﻿ ( CW ) ﻿can﻿encourage﻿students’﻿ initiative , ﻿creativity﻿and﻿critical﻿thinking﻿ ( Hodges , ﻿2002 ) ; ﻿and﻿help﻿students﻿to﻿work﻿jointly﻿on﻿shared﻿ objectives﻿ ( Caspi﻿and﻿Blau , ﻿2011 ) . ﻿Some﻿researchers﻿also﻿argue﻿that﻿participation﻿in﻿CW﻿activities﻿ including﻿online﻿text - based﻿discussions﻿can﻿assist﻿students﻿in﻿becoming﻿more﻿competent﻿knowledge﻿ workers﻿ ( Ellis﻿and﻿Goodyear , ﻿2010 ) . Research﻿in﻿the﻿field﻿of﻿online﻿collaborative﻿writing﻿ ( OCW ) ﻿has﻿largely﻿emphasized﻿on﻿the﻿ efficiency﻿of﻿specific﻿affordances , ﻿processes﻿and﻿conception . ﻿For﻿example , ﻿how﻿students﻿use﻿Google﻿ docs , ﻿Wikis﻿and﻿other﻿OCW﻿tools﻿ ( Wheeler﻿et﻿al . , ﻿2008 ) ; ﻿how﻿scripts﻿and﻿other﻿process﻿scaffolds﻿ improve﻿the﻿efficiency﻿of﻿group﻿writing﻿ ( Daemmrich , ﻿2000 ) ; ﻿how﻿synchronous﻿communication﻿and﻿ other﻿additional﻿technological﻿tools﻿enhance﻿coordination﻿or﻿group﻿awareness﻿in﻿OCW﻿environments﻿ ( Elola﻿and﻿Oskoz , ﻿2010 ) ; ﻿and﻿what﻿are﻿university﻿students’﻿conception﻿of﻿OCW﻿ ( Limbu﻿and﻿ Markauskaite , ﻿2015 ) . ﻿However , ﻿academics﻿who﻿attempt﻿to﻿embrace﻿Online﻿Collaborative﻿Writing﻿ 79 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 80 ( OCW ) ﻿in﻿their﻿teaching﻿often﻿report﻿challenges﻿that﻿result﻿in﻿less﻿positive﻿student﻿engagement﻿or﻿ learning﻿outcomes﻿ ( Caspi﻿and﻿Blau , ﻿2011 ; ﻿Cole , ﻿2009 ) . ﻿In﻿addition , ﻿these﻿challenges﻿include﻿the﻿ accurate﻿assessment﻿of﻿individual﻿contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups﻿toward﻿the﻿final﻿output﻿ ( Roberts﻿and﻿McInnerney , ﻿2007 ) . ﻿In﻿this﻿study , ﻿we﻿attempt﻿to﻿use﻿learning﻿analytics﻿ to﻿generate﻿useful﻿visualizations﻿to﻿address﻿these﻿issues . As﻿research﻿shows , ﻿a﻿student﻿who﻿is﻿engaged﻿and﻿intrinsically﻿motivated﻿in﻿a﻿task﻿is﻿more﻿likely﻿ to﻿learn﻿from﻿an﻿activity . ﻿Fredricks﻿et﻿al﻿ ( 2004 ) ﻿defined﻿engagement﻿in﻿three﻿dimensions : ﻿behavioral , ﻿ cognitive﻿and﻿emotional﻿engagement . ﻿‘Behavioral﻿engagement’ , ﻿which﻿is﻿the﻿focus﻿of﻿the﻿present﻿study , ﻿ refers﻿to﻿participation﻿in﻿school﻿related﻿activities﻿and﻿involvement﻿in﻿academic﻿and﻿learning﻿tasks﻿such﻿as﻿those﻿being﻿done﻿online . ﻿It﻿can﻿be﻿measured﻿by﻿observation﻿and﻿self - report . ﻿‘Cognitive﻿engagement’﻿ refers﻿to﻿motivation , ﻿thoughtfulness﻿and﻿willingness﻿to﻿make﻿an﻿effort﻿to﻿comprehend﻿ideas﻿and﻿master﻿ new﻿skills . ﻿‘Emotional﻿engagement’﻿includes﻿emotions﻿and﻿interest , ﻿such﻿as﻿affective﻿reactions﻿in﻿the﻿ classroom﻿towards﻿teachers . ﻿These﻿three﻿aspects﻿are﻿interrelated﻿and﻿helpful﻿to﻿understand﻿engagement﻿ as﻿a﻿whole . ﻿The﻿term﻿‘engagement’﻿used﻿throughout﻿the﻿paper , ﻿unless﻿otherwise﻿specified , ﻿refers﻿to﻿ ‘behavioral﻿engagement’ . Compared﻿with﻿emotional﻿and﻿cognitive﻿engagement , ﻿the﻿measurement﻿of﻿behavioral﻿engagement﻿ is﻿more﻿straightforward﻿because﻿behavioral﻿patterns﻿can﻿be﻿defined , ﻿observed﻿and﻿interpreted . ﻿ For﻿instance , ﻿when﻿a﻿student﻿participates﻿in﻿an﻿activity﻿that﻿is﻿mediated﻿by﻿technology , ﻿a﻿detailed﻿ collection﻿of﻿behavioral﻿events﻿can﻿be﻿recorded . ﻿Computer﻿keystroke - logging﻿ ( Bixler﻿and﻿D’Mello , ﻿ 2013 ; ﻿Stromqvist﻿and﻿Malmsten , ﻿1998 ) ﻿or﻿screen﻿capturing﻿ ( Latif , ﻿2008 ) ﻿allow﻿a﻿detailed﻿account﻿of﻿ the﻿behavior﻿of﻿a﻿writer﻿including﻿actions﻿such﻿as﻿starting﻿a﻿new﻿paragraph﻿or﻿deleting﻿a﻿text﻿portion﻿and﻿these﻿are﻿all﻿considered﻿indicators﻿of﻿behavioral﻿engagement . ﻿Thus , ﻿new﻿computer﻿technology﻿ permits﻿the﻿observation﻿and﻿identification﻿of﻿learning﻿events , ﻿which﻿can﻿then﻿be﻿examined﻿in﻿relation﻿ to﻿other﻿indices﻿of﻿engagement . ﻿However , ﻿in﻿order﻿to﻿collect﻿the﻿learning﻿events , ﻿these﻿computer﻿ technologies﻿required﻿some﻿special﻿software﻿applications﻿or﻿hardware , ﻿such﻿as﻿ScriptLog﻿ ( Stromqvist﻿ and﻿Malmsten , ﻿1998 ) , ﻿installed﻿in﻿the﻿student﻿computer . ﻿These﻿factors﻿present﻿a﻿barrier﻿to﻿the﻿use﻿of﻿ this﻿technology﻿in﻿the﻿education﻿sector . New﻿cloud - based﻿technologies , ﻿such﻿as﻿Google﻿Docs﻿not﻿only﻿record﻿the﻿revision﻿history﻿ ( each﻿ revision﻿contains﻿the﻿document﻿content﻿and﻿timestamp ) ﻿they﻿also﻿provide﻿application﻿programming﻿ interfaces﻿ ( API ) ﻿to﻿access﻿this﻿information﻿programmatically . ﻿In﻿addition , ﻿Google﻿Docs﻿has﻿the﻿ advantage﻿of﻿supporting﻿easy﻿system﻿integration﻿and﻿synchronous﻿collaborative﻿writing﻿and﻿it﻿has﻿been﻿successfully﻿applied﻿in﻿student﻿assignment﻿management﻿ ( Calvo﻿et﻿al . , ﻿2011 ) , ﻿collaborative﻿ writing﻿practices﻿ ( Southavilay﻿et﻿al . , ﻿2013 ) ﻿and﻿engagement﻿visualization﻿and﻿measurement﻿ ( Liu﻿et﻿ al . , ﻿2013 ) . ﻿Similar﻿to﻿Google﻿Docs , ﻿Etherpad﻿ ( www . etherpad . org ) ﻿is﻿a﻿web - based﻿collaborative﻿real - time﻿editor , ﻿which﻿was﻿acquired﻿by﻿Google﻿in﻿December﻿2009﻿and﻿released﻿as﻿open﻿source . ﻿It﻿has﻿ many﻿advantages , ﻿such﻿as﻿lightweight , ﻿quick﻿to﻿start﻿up﻿and﻿easy﻿to﻿differentiate﻿between﻿different﻿ authors . ﻿We﻿use﻿the﻿Etherpad﻿to﻿implement﻿the﻿collaborative﻿writing﻿environment . This﻿paper﻿describes﻿the﻿development﻿and﻿evaluation﻿of﻿a﻿new﻿method﻿to﻿measure﻿and﻿visualize﻿ student﻿behavioral﻿engagement﻿and﻿patterns﻿in﻿a﻿collaborative﻿writing﻿environment﻿that﻿was﻿trialed﻿with﻿university﻿students . ﻿In﻿these﻿studies﻿participants﻿were﻿required﻿to﻿collaboratively﻿complete﻿writing﻿ tasks , ﻿a﻿project﻿proposal﻿and﻿tutorial﻿discussion﻿questions , ﻿while﻿their﻿writing﻿activities﻿were﻿recorded﻿ using﻿Etherpad . ﻿Computer - generated﻿observations﻿were﻿processed﻿and﻿visualizations﻿generated﻿to﻿ yield﻿estimations﻿of﻿the﻿individual﻿writer﻿and﻿group’s﻿level﻿of﻿engagement﻿and﻿illustrate﻿the﻿writing﻿behavior﻿patterns﻿of﻿individual﻿writers . ﻿The﻿visualizations﻿for﻿formative﻿feedback﻿are﻿used﻿to﻿support﻿ student﻿engagement﻿during﻿the﻿writing﻿activity﻿while﻿the﻿visualization﻿for﻿summative﻿feedback﻿to﻿support﻿student﻿reflection﻿on﻿the﻿overall﻿writing﻿process﻿after﻿the﻿writing﻿activity . The﻿major﻿contributions﻿described﻿in﻿this﻿paper﻿are : ﻿1 ) ﻿a﻿novel﻿learning﻿analytic﻿system﻿that﻿ collects﻿behavioral﻿data﻿of﻿users’﻿collaborative﻿writing , ﻿estimates﻿the﻿level﻿of﻿engagement , ﻿and﻿ generates﻿two﻿types﻿of﻿visualizations , ﻿visualization﻿for﻿formative﻿feedback﻿and﻿visualization﻿for﻿ International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 81 summative﻿feedback ; ﻿the﻿study﻿also﻿examined﻿2 ) ﻿the﻿performance﻿of﻿the﻿system﻿in﻿two﻿types﻿of﻿ collaborative﻿writing﻿tasks , ﻿writing﻿a﻿project﻿proposal﻿and﻿tutorial﻿discussion . The﻿remainder﻿of﻿this﻿paper﻿is﻿organized﻿as﻿follows : ﻿Section﻿2﻿describes﻿the﻿relevant﻿work﻿in﻿the﻿ areas﻿of﻿behavioral﻿engagement﻿and﻿learning﻿analytics . ﻿Section﻿3﻿describes﻿the﻿architecture﻿of﻿the﻿ system﻿used﻿in﻿the﻿study . ﻿In﻿section﻿4 , ﻿the﻿algorithms﻿used﻿to﻿process﻿the﻿engagement﻿measurements﻿ and﻿the﻿creation﻿of﻿the﻿three﻿types﻿of﻿visualizations﻿is﻿described . ﻿Sections﻿5﻿describe﻿the﻿research﻿ scenario﻿and﻿experimental﻿study﻿used﻿to﻿validate﻿the﻿proposed﻿approach . ﻿The﻿paper﻿concludes﻿in﻿ Section﻿6﻿with﻿a﻿discussion﻿of﻿the﻿overall﻿approach﻿as﻿well﻿as﻿lines﻿for﻿future﻿exploration . 2 . BACKGRoUND 2 . 1 . Collaborative Writing Significant﻿efforts﻿have﻿been﻿made﻿into﻿studying﻿the﻿way﻿the﻿individual﻿writes﻿ ( Flower﻿et﻿al . , ﻿1989 ) . ﻿ Collaborative﻿writing﻿is﻿that﻿two﻿or﻿more﻿people﻿working﻿together﻿to﻿produce﻿a﻿document﻿with﻿group﻿responsibility﻿for﻿the﻿end﻿product , ﻿which﻿is﻿different﻿from﻿interactive﻿writing﻿ ( where﻿people﻿solicit﻿ others’﻿opinions﻿about﻿their﻿writings ) . ﻿The﻿interest﻿in﻿collaborative﻿writing﻿research﻿arose﻿in﻿the﻿late﻿ 1980’s﻿ ( Beck , ﻿1993 ) . ﻿Research﻿into﻿collaborative﻿writing﻿has﻿shown﻿that﻿it﻿has﻿great﻿potential﻿in﻿ scientific﻿collaboration﻿ ( Kraut﻿et﻿al . , ﻿1988 ) , ﻿in﻿second﻿language﻿learning﻿ ( Storch , ﻿2002 ) , ﻿in﻿producing﻿ technical﻿reports﻿ ( Noel﻿and﻿Robert , ﻿2004 ) ﻿and﻿in﻿the﻿development﻿of﻿scientific﻿reasoning﻿skills﻿ ( Keys , ﻿ 1994 ) . Researchers﻿have﻿investigated﻿how﻿people﻿collaboratively﻿write﻿the﻿document﻿ ( Posner﻿and﻿ Baecker , ﻿1992 ; ﻿Sharples , ﻿1993 ) . ﻿Sharples﻿et﻿al . ﻿ ( 1993 ) ﻿studied﻿two﻿types﻿of﻿collaborative﻿writing﻿ strategies : ﻿sequential﻿and﻿parallel﻿partition . ﻿In﻿the﻿sequential﻿partition , ﻿the﻿work﻿is﻿divided﻿into﻿ sequential﻿stages , ﻿and﻿each﻿stage﻿is﻿allocated﻿to﻿a﻿different﻿person﻿or﻿sub - group . ﻿In﻿parallel﻿partitioning , ﻿ the﻿document﻿is﻿divided﻿into﻿sections , ﻿and﻿each﻿person﻿or﻿sub - group﻿works﻿on﻿a﻿different﻿section﻿in﻿ parallel﻿to﻿the﻿others . ﻿Posner﻿and﻿Becker﻿investigated﻿the﻿collaborative﻿writing﻿process﻿further﻿and﻿ created﻿taxonomy﻿of﻿collaborative﻿writing﻿with﻿four﻿dimensions : ﻿roles , ﻿activities , ﻿document﻿control﻿ methods﻿and﻿writing﻿strategies . ﻿Four﻿roles﻿were﻿defined ; ﻿writer﻿ ( writes﻿the﻿document ) , ﻿consultant﻿ ( offers﻿information﻿but﻿does﻿not﻿participate﻿in﻿the﻿document﻿creation ) , ﻿editor﻿ ( modifies﻿the﻿document ) ﻿ and﻿reviewer﻿ ( gives﻿some﻿advance﻿to﻿improve﻿the﻿document ) . ﻿Activities﻿contain﻿brainstorming , ﻿ researching , ﻿planning , ﻿writing , ﻿editing , ﻿and﻿reviewing . ﻿Document﻿control﻿refers﻿to﻿who﻿manages﻿the﻿ document﻿during﻿the﻿writing . ﻿They﻿identified﻿four﻿types﻿of﻿document﻿control﻿methods , ﻿centralized﻿ ( one﻿person﻿controls﻿the﻿document ) , ﻿relay﻿ ( one﻿person﻿at﻿a﻿time﻿controls﻿the﻿document ) , ﻿independent﻿ ( each﻿person﻿controls﻿the﻿section﻿on﻿which﻿he / she﻿is﻿working ) , ﻿shared﻿ ( everyone﻿has﻿equal﻿access﻿to﻿ the﻿document ) . ﻿They﻿further﻿defined﻿four﻿types﻿of﻿writing﻿strategies : ﻿ ( 1 ) ﻿Single﻿writer : ﻿one﻿people﻿ writes﻿while﻿other﻿group﻿members﻿play﻿other﻿roles ; ﻿ ( 2 ) ﻿Separate﻿writer : ﻿each﻿group﻿member﻿works﻿ on﻿a﻿different﻿section , ﻿which﻿is﻿similar﻿to﻿parallel﻿partition ; ( 3 ) ﻿Joint﻿writing : ﻿writers﻿work﻿together﻿ synchronously﻿on﻿the﻿text ; ﻿ ( 4 ) ﻿Scribe : ﻿one﻿person﻿writes﻿in﻿a﻿group﻿meeting . ﻿They﻿found﻿that﻿separate﻿ writer﻿was﻿the﻿most﻿effective﻿collaborative﻿writing﻿strategy﻿and﻿joint﻿writing﻿the﻿least﻿effective . ﻿In﻿ our﻿experiments , ﻿students﻿mainly﻿used﻿the﻿separate﻿writer﻿or﻿parallel﻿partition﻿writing﻿strategy﻿with﻿ independent﻿or﻿shared﻿document﻿control﻿method . 2 . 2 . online Collaborative Learning and Writing Environment Literature﻿on﻿educational﻿design﻿states﻿that﻿well - designed﻿tasks﻿and﻿learning﻿environments﻿can﻿ support﻿productive﻿learner﻿behaviors﻿and﻿enhance﻿engagement , ﻿positive﻿learning﻿experiences﻿and﻿ outcomes﻿ ( Jonassen﻿and﻿Land , ﻿2012 ) . ﻿However , ﻿there﻿are﻿many﻿challenges﻿in﻿creating﻿effective﻿online﻿ collaborative﻿learning﻿and﻿writing﻿environments﻿ ( Kirschner , ﻿2004 ) . ﻿Kirschner﻿ ( 2004 ) ﻿presents﻿an﻿ affordance﻿framework , ﻿including﻿technological , ﻿educational﻿and﻿social﻿affordances , ﻿for﻿designing﻿and﻿ evaluating﻿such﻿collaborative﻿learning﻿environment . ﻿Educational﻿affordance﻿refers﻿to﻿characteristics﻿ International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 82 of﻿the﻿learning﻿environment﻿that﻿facilitate﻿collaborative﻿learning﻿behavior , ﻿which﻿include﻿group﻿ project﻿management , ﻿group﻿work﻿co - construction , ﻿information , ﻿knowledge﻿and﻿feedback﻿sharing . ﻿ Technological﻿affordance﻿refers﻿to﻿characteristics﻿that﻿enable﻿learners﻿to﻿accomplish﻿learning﻿tasks﻿in﻿an﻿efficient﻿and﻿effective﻿way . ﻿Bower﻿ ( 2008 ) ﻿further﻿defined﻿technological﻿affordances﻿in﻿a﻿second﻿ level﻿of﻿affordance﻿categories : ﻿media﻿ ( the﻿ability﻿to﻿input﻿and﻿output﻿various﻿media﻿forms , ﻿such﻿as﻿text﻿ and﻿images ) , ﻿spatial﻿ ( ability﻿to﻿resize , ﻿move﻿or﻿place﻿contents﻿within﻿an﻿interface ) , ﻿temporal﻿ ( ability﻿ to﻿access﻿anytime﻿anywhere﻿as﻿well﻿as﻿to﻿record﻿and﻿play﻿back﻿information ) , ﻿navigation﻿ ( ability﻿to﻿ see﻿other﻿member’s﻿work ) , ﻿synthesis﻿ ( ability﻿to﻿combine﻿and﻿integrate﻿group﻿members’﻿input ) , ﻿and﻿ access - control﻿ ( user﻿management ) . ﻿Our﻿current﻿system﻿captured﻿all﻿these﻿features . ﻿Social﻿affordance﻿ refers﻿to﻿features﻿that﻿offer﻿social - contextual﻿facilitation﻿in﻿relation﻿to﻿students’﻿social﻿interaction . ﻿It﻿ includes﻿group﻿communication﻿and﻿motivation . ﻿When﻿learners﻿perceive﻿social﻿affordances , ﻿they﻿are﻿ encouraged﻿to﻿engage﻿in﻿activities﻿ ( Kirschner , ﻿2004 ) . ﻿Dieberger﻿ ( 2000 ) ﻿considers﻿awareness﻿of﻿other﻿ people’s﻿activities﻿to﻿be﻿an﻿essential﻿ingredient﻿for﻿collaborative﻿work . Existing﻿online﻿collaborative﻿writing﻿tools , ﻿such﻿as﻿Google﻿Docs﻿and﻿Wiki , ﻿have﻿been﻿designed﻿to﻿ provide﻿some﻿basic﻿functions﻿of﻿these﻿three﻿affordances . ﻿Several﻿attempts﻿have﻿been﻿made﻿to﻿enhance﻿ wikis﻿with﻿learning﻿analytics﻿ ( Hoefler﻿and﻿Guetl , ﻿2011 ; ﻿Kubincová﻿et﻿al . , ﻿2012 ; ﻿Popescu﻿et﻿al . , ﻿2014 ) . ﻿ For﻿example , ﻿Popescu﻿ ( 2014 ) ﻿developed﻿a﻿learning﻿analytic﻿tool , ﻿called﻿CoLearn , ﻿which﻿provides﻿the﻿ visualization﻿of﻿timing﻿of﻿the﻿students’﻿contributions﻿to﻿helps﻿instructors﻿to﻿analyze﻿the﻿collaborative﻿writing﻿strategies . ﻿In﻿addition , ﻿it﻿allows﻿students﻿to﻿visualize﻿their﻿overall﻿progress﻿and﻿comparative﻿ statistics﻿with﻿the﻿group﻿and﻿class﻿average . ﻿However , ﻿they﻿focused﻿on﻿using﻿learning﻿analytics﻿to﻿ support﻿asynchronous﻿collaborative﻿writing﻿and﻿did﻿not﻿evaluate﻿the﻿usefulness﻿of﻿the﻿visualizations . ﻿In﻿ this﻿study , ﻿we﻿focus﻿on﻿using﻿the﻿learning﻿analytics﻿to﻿support﻿the﻿synchronous﻿collaborative﻿writing , ﻿ where﻿individual﻿writers﻿are﻿aware﻿of﻿the﻿rest﻿of﻿group﻿members﻿and﻿other﻿groups’﻿engagement . ﻿In﻿ addition , ﻿different﻿types﻿of﻿visualizations﻿were﻿evaluated﻿in﻿two﻿empirical﻿studies . 2 . 3 . Learning Analytics The﻿area﻿known﻿as﻿Learning﻿Analytics﻿ ( LA ) ﻿has﻿emerged﻿as﻿a﻿result﻿of﻿behavior - related﻿information﻿ available﻿about﻿how﻿students﻿learn . ﻿LA﻿is﻿defined﻿as﻿“the﻿measurement , ﻿collection , ﻿analysis﻿and﻿ reporting﻿data﻿about﻿learners﻿and﻿their﻿contexts , ﻿for﻿purposes﻿of﻿understanding﻿and﻿optimizing﻿ learning﻿and﻿the﻿environments﻿in﻿which﻿it﻿occurs”﻿ ( Brown , ﻿2012 ) . ﻿In﻿general , ﻿learning﻿analytic﻿ systems﻿can﻿be﻿divided﻿into﻿several﻿modules , ﻿steps﻿or﻿phases﻿ ( Campbell﻿et﻿al . , ﻿2007 ) . ﻿One﻿module﻿ captures﻿detailed﻿events﻿such﻿as﻿the﻿number﻿and﻿frequency﻿of﻿interactions﻿with﻿resources﻿in﻿a﻿learning﻿management﻿system﻿ ( Tanes﻿et﻿al . , ﻿2011 ; ﻿Waddington﻿and﻿Teasley , ﻿2016 ) . ﻿This﻿module﻿may﻿also﻿ use﻿additional﻿factors﻿such﻿as﻿a﻿student’s﻿Grade﻿Point﻿Average﻿ ( GPA ) ﻿ ( Mckay﻿et﻿al . , ﻿2012 ) , ﻿gender , ﻿ etc . ﻿An﻿algorithmic﻿module﻿then﻿analyzes﻿these﻿data﻿to﻿infer﻿some﻿conclusions . ﻿These﻿conclusions﻿ are﻿reported﻿back﻿to﻿users﻿through﻿an﻿additional﻿module . ﻿Typical﻿reports﻿include﻿visualizations﻿that﻿ can﻿range﻿from﻿a﻿simple﻿traffic﻿light - like﻿display﻿of﻿overall﻿student﻿status﻿and﻿risks﻿ ( Essa﻿and﻿Ayad , ﻿ 2012 ; ﻿Tanes﻿et﻿al . , ﻿2011 ) , ﻿to﻿more﻿sophisticated﻿dashboards﻿with﻿detailed﻿information﻿about﻿various﻿ aspects﻿derived﻿from﻿the﻿data﻿ ( Rivera - Pelayo﻿et﻿al . , ﻿2013 ; ﻿Verpoorten﻿et﻿al . , ﻿2011 ) . ﻿A﻿more﻿advanced﻿ module﻿is﻿often﻿incorporated﻿to﻿suggest﻿actions﻿to﻿modify﻿the﻿learning﻿behaviors . ﻿These﻿actions﻿are﻿ sometimes﻿referred﻿to﻿as﻿interventions﻿and﻿may﻿range﻿from﻿suggestions﻿automatically﻿proposed﻿to﻿instructors﻿or﻿other﻿academic﻿staff , ﻿to﻿automatic﻿adjustments﻿applied﻿on﻿the﻿experience . The﻿initial﻿analytics﻿tools﻿designed﻿within﻿the﻿context﻿of﻿learning﻿experiences﻿were﻿pedagogically﻿ neutral . ﻿In﻿other﻿words , ﻿they﻿simply﻿provided﻿insight﻿about﻿the﻿events﻿occurring﻿in﻿an﻿environment﻿ without﻿targeting﻿any﻿specific﻿strategy . ﻿An﻿example﻿of﻿these﻿early﻿tools﻿is﻿CourseVis﻿ ( Mazza﻿and﻿ Dimitrova , ﻿2007 ) , ﻿a﻿platform﻿to﻿visually﻿represent﻿the﻿interactions﻿of﻿students﻿in﻿the﻿context﻿of﻿ web - based﻿distance﻿education . ﻿The﻿concept﻿of﻿dashboard﻿appeared﻿as﻿a﻿proposal﻿to﻿centralize﻿the﻿ visualization﻿of﻿student﻿events﻿and﻿foster﻿self - reflection﻿and﻿sensemaking﻿for﻿both﻿students﻿and﻿teachers﻿ ( Verbert﻿et﻿al . , ﻿2013 ) . ﻿At﻿the﻿same﻿time , ﻿academic﻿institutions﻿started﻿to﻿use﻿analytics﻿to﻿tackle﻿the﻿ problem﻿of﻿student﻿retention . ﻿Numerous﻿institutions﻿have﻿created﻿platforms﻿that﻿combine﻿student﻿ International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 83 interactions﻿with﻿other﻿socio - economic﻿factors﻿to﻿calculate﻿the﻿probability﻿of﻿a﻿student﻿dropping﻿a﻿ course﻿ ( Tanes﻿et﻿al . , ﻿2011 ) . ﻿These﻿platforms﻿were﻿later﻿extended﻿to﻿cover﻿the﻿anticipation﻿of﻿other﻿ facts﻿such﻿as﻿academic﻿performance﻿and﻿are﻿generally﻿known﻿as﻿Early﻿Warning﻿Systems﻿or﻿EWAs﻿ ( see﻿ ( Fritz , ﻿2011 ; ﻿Lonn﻿et﻿al . , ﻿2012 ) ﻿for﻿two﻿examples﻿of﻿these﻿systems ) . In﻿the﻿recent﻿years , ﻿the﻿pedagogical﻿intent﻿has﻿been﻿gaining﻿influence﻿in﻿the﻿design﻿of﻿learning﻿ analytics﻿approaches . ﻿The﻿emergence﻿of﻿constructivist﻿approaches﻿to﻿education﻿prompted﻿the﻿ appearance﻿of﻿applications﻿to﻿analyze﻿the﻿interaction﻿of﻿users﻿within﻿the﻿context﻿of﻿social﻿networks . ﻿ The﻿work﻿of﻿Aviv﻿et﻿al . ﻿showed﻿how﻿to﻿connect﻿the﻿topology﻿emerging﻿in﻿a﻿network﻿with﻿knowledge﻿ construction﻿ ( Lonn﻿et﻿al . , ﻿2012 ) . ﻿Visualizations﻿are﻿also﻿used﻿in﻿this﻿context﻿to﻿identify﻿specific﻿patterns﻿ and﻿promote﻿a﻿more﻿cohesive﻿network﻿ ( Aviv﻿et﻿al . , ﻿2003 ) . ﻿More﻿advanced﻿approaches﻿have﻿been﻿ recently﻿proposed﻿in﻿the﻿context﻿of﻿discourse﻿analysis﻿ ( Ferguson﻿and﻿Shum , ﻿2012 , ﻿2011 ; ﻿Ferguson﻿ et﻿al . , ﻿2013 ) . ﻿Nowadays , ﻿learning﻿analytics﻿attracted﻿a﻿great﻿attention﻿in﻿the﻿computer﻿supported﻿ collaborative﻿learning﻿ ( CSCL ) ﻿community , ﻿such﻿as﻿the﻿use﻿of﻿learning﻿analytics﻿in﻿a﻿programming﻿ class﻿ ( Berland﻿et﻿al . , ﻿2015 ) , ﻿a﻿pedagogical﻿framework﻿for﻿learning﻿analytics﻿in﻿collaborative﻿inquiry﻿ tasks﻿ ( Koh﻿et﻿al . , ﻿2016 ) , ﻿group﻿composition﻿and﻿performance﻿prediction﻿ ( Cen﻿et﻿al . , ﻿2016 ) . Tracer﻿is﻿a﻿learning﻿analytic﻿system , ﻿which﻿analyzes﻿the﻿information﻿obtained﻿from﻿document﻿ revisions﻿using﻿Google﻿Docs , ﻿and﻿provides﻿visualization﻿and﻿measurements﻿for﻿the﻿level﻿of﻿engagement﻿ in﻿an﻿individual﻿writing﻿activity ( Liu﻿et﻿al . , ﻿2013 ) . ﻿The﻿study﻿results﻿showed﻿that﻿the﻿engagement﻿time﻿ gauged﻿by﻿Tracer﻿was﻿moderately﻿correlated﻿to﻿those﻿reported﻿by﻿the﻿students , ﻿and﻿the﻿generated﻿ visualization﻿correctly﻿conveys﻿the﻿student﻿engagement . ﻿But , ﻿Tracer﻿only﻿focus﻿on﻿supporting﻿ individual﻿writing . This﻿system﻿described﻿in﻿this﻿document﻿can﻿be﻿considered﻿as﻿an﻿extension﻿of﻿Tracer , ﻿which﻿ emphasize﻿on﻿generating﻿visualizations﻿for﻿supporting﻿student﻿engagement﻿in﻿collaborative﻿writing﻿settings . ﻿Furthermore , ﻿we﻿proposed﻿an﻿enhanced﻿engagement﻿measurement﻿and﻿a﻿novel﻿writing﻿ behavior﻿detection﻿algorithm﻿for﻿the﻿visualizations . 3 . SySTEM ARCHITECTURE Our﻿learning﻿analytics﻿system﻿ ( ‘Cooperpad’ ) ﻿captures﻿a﻿detailed﻿account﻿of﻿how﻿a﻿group﻿of﻿learners﻿ engage﻿in﻿a﻿collaborative﻿writing﻿activity , ﻿estimates﻿the﻿individual﻿and﻿group﻿engagement , ﻿detects﻿the﻿ individual﻿writing﻿behavior﻿and﻿produces﻿two﻿types﻿of﻿feedback﻿ ( formative﻿and﻿summative﻿feedback﻿ visualization ) ﻿to﻿support﻿engagement . The﻿three﻿components﻿of﻿the﻿system﻿are﻿shown﻿in﻿Figure﻿1 . ﻿The﻿first﻿is﻿the﻿Data﻿Collection﻿Module﻿ which﻿currently﻿relies﻿on﻿the﻿Etherpad . ﻿Etherpad﻿is﻿a﻿highly﻿customizable﻿open﻿source﻿online﻿editor﻿ providing﻿collaborative﻿editing﻿in﻿real﻿time . ﻿The﻿application﻿records﻿the﻿text﻿contents﻿of﻿all﻿its﻿pads , ﻿ a﻿list﻿of﻿users , ﻿their﻿preferences﻿and﻿numerous﻿intermediate﻿versions﻿of﻿the﻿documents﻿while﻿they﻿are﻿ being﻿modified , ﻿which﻿are﻿stored﻿in﻿Redis , ﻿a﻿very﻿fast﻿in - memory﻿key - value﻿store . ﻿In﻿each﻿document﻿ revision , ﻿Etherpad﻿keep﻿track﻿of﻿user﻿operations﻿in﻿the﻿changeset﻿ ( Theis﻿et﻿al . , ﻿2010 ) , ﻿which﻿is﻿used﻿ for﻿writing﻿behavior﻿detection﻿by﻿our﻿system . ﻿The﻿browser﻿sends﻿changesets﻿to﻿the﻿server , ﻿which﻿then﻿ sends﻿them﻿to﻿the﻿clients﻿to﻿update﻿them . ﻿Changesets﻿also﻿get﻿saved﻿into﻿the﻿history﻿of﻿a﻿pad﻿which﻿ allows﻿Etherpad﻿to﻿go﻿back﻿to﻿every﻿revision﻿from﻿the﻿past . ﻿The﻿application﻿programming﻿interface﻿ ( API ) ﻿is﻿used﻿by﻿Cooperpad﻿to﻿access﻿these﻿information . Currently , ﻿the﻿system﻿used﻿the﻿number﻿of﻿revisions , ﻿word﻿count﻿and﻿writing﻿behavior﻿ ( e . g . ﻿add﻿or﻿ delete﻿text ) ﻿to﻿generate﻿different﻿types﻿of﻿visualizations﻿since﻿they﻿are﻿good﻿indicators﻿of﻿behavioral﻿ engagement﻿in﻿a﻿collaborative﻿writing﻿setting﻿ ( Hoefler﻿and﻿Guetl , ﻿2011 ; ﻿Popescu﻿et﻿al . , ﻿2014 ) . ﻿For﻿ example , ﻿Hoefler﻿and﻿Guetl﻿ ( 2011 ) ﻿used﻿the﻿word﻿count﻿each﻿group﻿member﻿has﻿contributed﻿to﻿the﻿ assignment﻿wiki﻿and﻿displayed﻿this﻿information﻿in﻿a﻿contribution﻿chart﻿with﻿the﻿aim﻿of﻿motivating﻿group﻿members﻿to﻿contribute﻿more . ﻿In﻿addition , ﻿each﻿group﻿member’s﻿action﻿ ( e . g . ﻿added﻿text , ﻿removed﻿ test , ﻿edited﻿text , ﻿and﻿text﻿changed﻿style ) ﻿on﻿a﻿revision﻿of﻿the﻿assignment﻿page﻿is﻿displayed﻿to﻿maintain﻿ International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 84 task﻿awareness . ﻿Similarly , ﻿Popescu﻿et﻿al . ﻿ ( 2014 ) ﻿used﻿the﻿comparative﻿statistics﻿of﻿the﻿revision﻿history﻿ information﻿to﻿enhance﻿student﻿competitiveness﻿and﻿involvement . The﻿second﻿component﻿of﻿Cooperpad﻿is﻿the﻿Data﻿Analysis﻿Module﻿in﻿which﻿engagement﻿ measurement﻿algorithms﻿and﻿writing﻿behavior﻿detector﻿are﻿implemented﻿described﻿in﻿section﻿4 . 1﻿ and﻿4 . 2 . ﻿The﻿third﻿component﻿is﻿the﻿Feedback﻿Module﻿described﻿in﻿section﻿4 . 3﻿where﻿formative﻿and﻿ summative﻿feedback﻿visualizations﻿are﻿created﻿based﻿on﻿the﻿results﻿derived﻿from﻿the﻿analysis﻿phase . 4 . ENGAGEMENT MEASUREMENT ALGoRITHM AND VISUALIZATIoNS Due﻿to﻿the﻿complexity﻿of﻿the﻿data﻿captured﻿during﻿the﻿writing﻿activity , ﻿it﻿is﻿challenging﻿to﻿produce﻿ a﻿simple﻿and﻿meaningful﻿visualization , ﻿such﻿as﻿group﻿engagement﻿ranking﻿awareness﻿and﻿group﻿ member﻿engagement﻿awareness . ﻿Thus , ﻿raw﻿events﻿data﻿are﻿analyzed﻿by﻿the﻿analysis﻿model . ﻿This﻿ section﻿describes﻿the﻿intensity - based﻿engagement﻿algorithm﻿which﻿is﻿used﻿for﻿Individual﻿Engagement﻿ Intensity﻿Bar﻿and﻿Group﻿Engagement﻿Ranking﻿Chart﻿and﻿Group﻿Engagement﻿Contribution﻿Pie﻿Chart﻿Generation , ﻿and﻿the﻿writing﻿behavior﻿detection﻿algorithm﻿used﻿for﻿Writing﻿Behavior﻿Pattern﻿Chart﻿ generation . ﻿The﻿objective﻿of﻿these﻿components﻿was﻿to﻿explore﻿how﻿best﻿to﻿convey﻿information﻿to﻿the﻿ user﻿in﻿an﻿understandable﻿format﻿and﻿enhance﻿their﻿engagement . 4 . 1 . Engagement Measurement Algorithm The﻿engagement﻿mentioned﻿here﻿refers﻿to﻿the﻿behavioral﻿engagement , ﻿specifically﻿the﻿time﻿engagement﻿ of﻿the﻿student﻿on﻿the﻿writing﻿task , ﻿in﻿other﻿words , ﻿how﻿much﻿time﻿the﻿student﻿spent﻿on﻿the﻿task . ﻿The﻿ computation﻿of﻿the﻿engagement﻿is﻿based﻿on﻿the﻿timestamp﻿information﻿of﻿each﻿document﻿revision . ﻿Our﻿ engagement﻿measurement﻿algorithms﻿were﻿based﻿on﻿the﻿intuition﻿that﻿if﻿the﻿student﻿is﻿more﻿engaged , ﻿ the﻿system﻿generates﻿more﻿consecutive﻿revisions ; ﻿otherwise , ﻿the﻿system﻿produces﻿less﻿document﻿ revisions . ﻿In﻿the﻿previous﻿study﻿ ( Liu﻿et﻿al . , ﻿2013 ) , ﻿we﻿have﻿proposed﻿two﻿engagement﻿algorithms , ﻿ point - based﻿engagement﻿and﻿intensity - based﻿engagement﻿algorithm . ﻿The﻿point - based﻿algorithm﻿ simply﻿sum﻿up﻿each﻿data﻿cluster , ﻿where﻿each﻿data﻿cluster﻿contains﻿a﻿set﻿of﻿data﻿points﻿ ( revisions﻿with﻿ timestamp﻿information ) ﻿and﻿a﻿data﻿cluster﻿has﻿a﻿fixed﻿threshold . ﻿For﻿example , ﻿if﻿the﻿threshold﻿is﻿1﻿ minute , ﻿which﻿means﻿the﻿time﻿duration﻿between﻿the﻿first﻿data﻿point﻿and﻿last﻿data﻿point﻿is﻿less﻿than﻿1﻿ minute . ﻿However , ﻿this﻿algorithm﻿does﻿not﻿incorporate﻿engagement﻿intensity , ﻿which﻿could﻿be﻿useful﻿ for﻿generating﻿engagement﻿intensity﻿bar﻿described﻿in﻿the﻿following﻿section . The﻿intensity - based﻿algorithm﻿sum﻿up﻿weighted﻿time﻿intervals﻿between﻿two﻿adjacent﻿revisions , ﻿ where﻿the﻿weight﻿refers﻿to﻿the﻿engagement﻿intensity﻿ ( see﻿Figure﻿2 ) . ﻿If﻿the﻿time﻿interval﻿is﻿smaller , ﻿the﻿ weight﻿is﻿bigger , ﻿which﻿indicates﻿that﻿the﻿engagement﻿intensity﻿is﻿high . ﻿Based﻿on﻿our﻿experience﻿with﻿ Figure 1 . Cooperpad system architecture International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 85 writing﻿activities﻿in﻿learning﻿situations , ﻿we﻿store﻿the﻿following﻿list﻿of﻿information﻿for﻿an﻿intensive﻿ writing﻿activity : ﻿ ( 30s , ﻿1 ) , ﻿ ( 45s , ﻿0 . 8 ) , ﻿ ( 60s , ﻿0 . 6 ) , ﻿ ( 75s , ﻿0 . 4 ) , ﻿ ( 90s , ﻿0 . 2 ) . ﻿This﻿list﻿refers﻿to﻿the﻿threshMap , ﻿ where﻿each﻿element﻿includes﻿two﻿values , ﻿time﻿threshold﻿for﻿the﻿interval﻿between﻿adjacent﻿revisions﻿ and﻿its﻿corresponding﻿weight . ﻿The﻿weight﻿indicates﻿the﻿engagement﻿intensity﻿or﻿engagement﻿level . ﻿ Figure 2 . Intensity - based engagement measurement algorithm International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 86 For﻿example , ﻿ ( 30s , ﻿1 ) ﻿means﻿that﻿if﻿the﻿time﻿interval﻿between﻿adjacent﻿revisions﻿is﻿greater﻿than﻿0﻿ but﻿less﻿than﻿or﻿equal﻿to﻿30﻿seconds , ﻿the﻿engagement﻿level﻿is﻿1 . ﻿Similarly , ﻿ ( 45s , ﻿0 . 8 ) ﻿shows﻿that﻿ the﻿engagement﻿level﻿is﻿0 . 8﻿if﻿time﻿interval﻿is﻿greater﻿than﻿30﻿seconds﻿but﻿less﻿than﻿or﻿equal﻿to﻿45﻿ seconds . ﻿This﻿implies﻿that﻿the﻿user﻿almost﻿constantly﻿writes﻿and﻿the﻿engagement﻿level﻿is﻿high . ﻿But , ﻿if﻿ the﻿time﻿interval﻿is﻿greater﻿than﻿90﻿seconds , ﻿the﻿engagement﻿intensity﻿is﻿zero . ﻿In﻿order﻿to﻿display﻿the﻿ engagement﻿intensity﻿bar﻿more﻿smoothly , ﻿described﻿in﻿section﻿4 . 3 , ﻿we﻿used﻿the﻿five﻿data﻿points﻿to﻿ derive﻿a﻿linear﻿regression﻿model , ﻿which﻿further﻿generates﻿other﻿21﻿data﻿points . ﻿Thus﻿the﻿total﻿group﻿ engagement﻿score﻿is﻿calculated﻿as﻿follows : Engagement p w j m i n ji i = ∗ ∑∑ ﻿ ( 1 ) Where﻿j﻿is﻿the﻿index﻿of﻿group﻿members﻿and﻿m﻿is﻿the﻿size﻿of﻿group , ﻿i﻿is﻿the﻿index﻿of﻿an﻿adjacent﻿ revision﻿pair , ﻿pji﻿is﻿the﻿interval﻿of﻿the﻿ith﻿pair﻿of﻿adjacent﻿revisions﻿generated﻿from﻿member﻿j﻿and﻿ Wi﻿is﻿the﻿weight﻿assigned﻿to﻿this﻿interval . ﻿As﻿we﻿mentioned﻿before , ﻿the﻿weight﻿is﻿determined﻿by﻿the﻿ duration﻿of﻿neighboring﻿events﻿ ( revisions ) . ﻿A﻿small﻿duration﻿indicates﻿a﻿higher﻿engagement﻿level . 4 . 2 . Writing Behavior Detection Previous﻿work﻿in﻿educational﻿revision﻿analysis﻿ ( Fitzgerald , ﻿1987 ; ﻿Connor﻿and﻿Asenavage , ﻿1994 ) ﻿ categorized﻿revision﻿changes﻿to﻿be﻿either﻿surface﻿changes﻿or﻿text - based﻿changes . ﻿With﻿both﻿categories , ﻿ six﻿kinds﻿of﻿changes﻿were﻿defined : ﻿ ( 1 ) ﻿Addition : ﻿Adding﻿a﻿word﻿or﻿phrase ; ﻿ ( 2 ) ﻿Deletion : ﻿Omitting﻿ a﻿word﻿or﻿phrase ; ﻿ ( 3 ) ﻿Substitutions : ﻿exchange﻿words﻿with﻿synonyms ; ﻿ ( 4 ) ﻿Permutation : ﻿rearrange﻿of﻿ words﻿or﻿phrases ; ﻿ ( 5 ) ﻿Distribution : ﻿one﻿segment﻿divided﻿into﻿two ; ( 6 ) ﻿Consolidation : ﻿combine﻿two﻿ segments﻿into﻿one . Based﻿on﻿the﻿Faigley’s﻿definition﻿ ( 1987 ) , ﻿we﻿defined﻿only﻿three﻿primitives , ﻿addition , ﻿deletion﻿ and﻿modification﻿because﻿automatically﻿detecting﻿other﻿text﻿operations , ﻿substitutions , ﻿permutation , ﻿ distribution﻿and﻿consolidation , ﻿is﻿not﻿a﻿trivial﻿task , ﻿which﻿requires﻿more﻿advanced﻿technologies , ﻿ such﻿as﻿natural﻿language﻿processing﻿technology . ﻿So , ﻿we﻿currently﻿focused﻿on﻿detecting﻿these﻿three﻿ categories , ﻿which﻿is﻿more﻿feasible . ﻿Modification﻿indicates﻿substitutions , ﻿permutation , ﻿distribution﻿ and﻿consolidation . ﻿This﻿definition﻿is﻿similar﻿to﻿Bronner﻿ ( 1994 ) ﻿and﻿Zhang’s﻿work﻿ ( 2014 ) . A﻿changeset﻿ ( Theis﻿et﻿al . , ﻿2010 ) ﻿describes﻿the﻿difference﻿between﻿two﻿revisions﻿of﻿the﻿document . ﻿ In﻿the﻿changeset﻿of﻿a﻿revision , ﻿it﻿contains﻿three﻿types﻿of﻿operators : ﻿ + , ﻿ – ﻿and﻿ = . ﻿The﻿ + ﻿operator﻿adds﻿ text﻿with﻿attributes , ﻿and﻿the﻿ – ﻿operator﻿removes﻿text , ﻿while﻿the﻿ = ﻿operator﻿does﻿not﻿change﻿the﻿text , ﻿ but﻿it﻿may﻿change﻿the﻿attributes﻿of﻿the﻿text﻿ ( e . g . ﻿make﻿it﻿bold ) . ﻿In﻿addition , ﻿in﻿the﻿beginning﻿of﻿the﻿ changset﻿string , ﻿it﻿uses﻿ < ﻿or﻿ > ﻿to﻿indicate﻿if﻿the﻿word﻿counts﻿in﻿current﻿revision﻿is﻿greater﻿or﻿less﻿than﻿ that﻿of﻿previous﻿revision . ﻿For﻿instance , ﻿this﻿changeset﻿Z : 196﻿ > 1﻿ | 5 = 97﻿ = 31﻿ * 4﻿ * 5﻿ + 1﻿ $ ”x”﻿shows﻿ that﻿current﻿revision﻿has﻿one﻿more﻿character﻿than﻿previous﻿revision﻿and﻿contains﻿three﻿operators ( = , ﻿ = , ﻿ + ) . ﻿The﻿last﻿ + ﻿operator﻿add﻿x﻿to﻿the﻿document﻿and﻿make﻿it﻿bold . ﻿ * ﻿I﻿means﻿applying﻿an﻿attribute﻿ to﻿the﻿following﻿operator . ﻿Based﻿on﻿this﻿definition , ﻿three﻿regular﻿expressions﻿shown﻿in﻿Table﻿1﻿are﻿ defined﻿to﻿identify﻿the﻿writing﻿behaviors , ﻿addition , ﻿deletion﻿and﻿modification . The﻿addition﻿ / > [ ^ - \ + ] * \ + / ﻿means﻿current﻿reversion﻿has﻿more﻿text﻿than﻿previous﻿revision﻿and﻿ has﻿only﻿ + ﻿operator . ﻿The﻿deletion﻿ / < [ ^ - \ + ] * - / ﻿means﻿current﻿reversion﻿has﻿less﻿text﻿than﻿previous﻿ revision﻿and﻿has﻿only﻿ - ﻿operator . ﻿The﻿modification﻿ / [ < > ] [ ^ - \ + ] * - [ ^ - \ + ] * \ + / ﻿means﻿current﻿reversion﻿ has﻿ – ﻿operator﻿first , ﻿and﻿then﻿ + ﻿operator﻿at﻿the﻿end﻿of﻿operation . 4 . 3 . Visualizations This﻿section﻿presents﻿different﻿visualizations﻿to﻿provide﻿formative﻿feedback﻿and﻿summative﻿feedback , ﻿ which﻿incorporates﻿social﻿awareness﻿and﻿technological﻿awareness﻿features , ﻿in﻿a﻿collaborative﻿writing﻿ International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 87 environment . ﻿In﻿the﻿formative﻿feedback , ﻿the﻿visualizations﻿show﻿the﻿individual﻿engagement﻿intensity﻿ and﻿the﻿engagement﻿ranking﻿of﻿this﻿writing﻿group﻿in﻿real﻿time . In﻿the﻿summative﻿feedback , ﻿the﻿visualizations﻿show﻿the﻿overall﻿engagement﻿contributions﻿and﻿ the﻿writing﻿behaviors﻿of﻿current﻿group﻿member﻿in﻿the﻿entire﻿writing﻿activity . 4 . 3 . 1 . Formative Visualization Feedback In﻿the﻿Engagement﻿Intensity﻿Bar﻿ ( EIB ) , ﻿each﻿bar﻿represents﻿a﻿group﻿member’s﻿engagement﻿intensity . ﻿ 100 % ﻿means﻿fully﻿engagement﻿while﻿0﻿means﻿not﻿engaged﻿at﻿all . ﻿Figure﻿3﻿shows﻿the﻿current﻿engagement﻿ levels﻿of﻿three﻿writers . ﻿The﻿EIB﻿gives﻿writers﻿real - time﻿feedback﻿and﻿makes﻿them﻿aware﻿of﻿other﻿ group﻿member’s﻿engagement﻿when﻿they﻿are﻿writing . ﻿The﻿individual﻿engagement﻿intensity﻿is﻿derived﻿ from﻿the﻿engagement﻿algorithm . In﻿the﻿Group﻿Engagement﻿Ranking﻿Chart﻿ ( GERC ) , ﻿the﻿x﻿axis﻿represents﻿the﻿group﻿name﻿while﻿y﻿ axis﻿shows﻿the﻿total﻿engagement﻿time . ﻿It﻿illustrates﻿the﻿group﻿with﻿highest﻿engagement , ﻿the﻿average﻿ engagement﻿group﻿and﻿current﻿engagement﻿group . ﻿Figure﻿4﻿depicts﻿that﻿the﻿IT﻿Elite﻿Team﻿group﻿ got﻿the﻿highest﻿engagement﻿within﻿a﻿class﻿and﻿the﻿current﻿group﻿almost﻿reaches﻿the﻿average﻿group﻿engagement﻿level . ﻿The﻿GERC﻿gives﻿writers﻿real - time﻿feedback﻿and﻿makes﻿them﻿aware﻿of﻿other﻿groups’﻿ engagement﻿when﻿they﻿are﻿writing . 4 . 3 . 2 . Summative Visualization Feedback After﻿they﻿finish﻿the﻿writing﻿task , ﻿users﻿can﻿review﻿their﻿writing﻿behavior﻿patterns﻿and﻿total﻿engagement﻿ contribution﻿to﻿help﻿them﻿to﻿reflect﻿on﻿what﻿they﻿did﻿ ( Technical﻿Affordance ) . ﻿The﻿Group﻿Member﻿ Engagement﻿Contribution﻿Pie﻿Chart﻿ ( GMECPC ) ﻿shows﻿the﻿percentage﻿of﻿each﻿member﻿engagement﻿ contribution﻿in﻿an﻿entire﻿writing﻿task . ﻿Figure﻿5﻿shows﻿that﻿the﻿whole﻿writing﻿assignment﻿has﻿been﻿ contributed﻿by﻿three﻿students﻿in﻿a﻿group , ﻿where﻿a﻿student﻿Xuelian﻿Li﻿made﻿the﻿largest﻿contribution﻿ ( 51 % ) . Table 1 . Regular expression used in changeset for writing behavior detection Writing Behavior Regular Expression Addition / > [ ^ - \ + ] * \ + / Deletion / < [ ^ - \ + ] * - / Modification / [ < > ] [ ^ - \ + ] * - [ ^ - \ + ] * \ + / Figure 3 . Engagement intensity bar International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 88 Figure 4 . Group engagement ranking chart Figure 5 . Group member engagement contribution pie chart International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 89 In﻿the﻿Writing﻿Behavior﻿Pattern﻿Chart﻿ ( WBPC ) , ﻿each﻿row﻿represents﻿a﻿writer﻿behavior﻿patterns﻿ including﻿add , ﻿delete﻿and﻿modify . ﻿Figure﻿6﻿shows﻿that﻿the﻿behavioral﻿pattern﻿of﻿an﻿engineering﻿student﻿ whilst﻿completing﻿a﻿writing﻿activity , ﻿where﻿each﻿point﻿represents﻿a﻿writing﻿behavior . ﻿We﻿defined﻿a﻿ green﻿point﻿as﻿an﻿addition﻿behavior , ﻿a﻿blue﻿point﻿as﻿a﻿modification﻿behavior﻿while﻿a﻿red﻿point﻿as﻿a﻿ deletion﻿behavior . ﻿The﻿implemented﻿visualization﻿by﻿using﻿JQplot﻿has﻿a﻿zoom﻿in﻿function﻿which﻿gives﻿ more﻿detail﻿about﻿what﻿the﻿user﻿has﻿done﻿in﻿that﻿operation . In﻿short , ﻿visualizations﻿incorporated﻿social﻿affordance﻿and﻿technical﻿affordance﻿features﻿for﻿OCW . ﻿ It﻿provides﻿feedback﻿at﻿different﻿stage﻿of﻿writing . ﻿When﻿the﻿user﻿is﻿writing , ﻿it﻿provides﻿formative﻿ feedback﻿with﻿EIB﻿and﻿GERC , ﻿which﻿provides﻿each﻿group﻿member﻿with﻿their﻿current﻿engagement﻿ intensity﻿and﻿their﻿current﻿group﻿engagement﻿ranking﻿position . ﻿When﻿the﻿user﻿finished﻿the﻿writing , ﻿ it﻿gives﻿summative﻿feedback﻿with﻿GMECPC﻿and﻿WBPV , ﻿which﻿allows﻿writer﻿to﻿review﻿their﻿writing﻿ behavioral﻿patterns﻿and﻿each﻿group﻿member﻿engagement﻿contribution﻿in﻿an﻿entire﻿collaborative﻿ writing﻿activity . The﻿text﻿editor﻿is﻿located﻿in﻿the﻿middle﻿of﻿the﻿screen﻿and﻿its﻿content﻿is﻿collaboratively﻿written﻿by﻿ group﻿members﻿ ( the﻿text﻿color﻿indicates﻿which﻿member﻿wrote﻿this﻿text ) . ﻿The﻿engagement﻿intensity﻿ bars﻿shown﻿are﻿on﻿the﻿left﻿while﻿the﻿group﻿engagement﻿ranking﻿chart﻿below﻿the﻿bar . 5 . USER STUDIES Existing﻿online﻿collaborative﻿writing﻿tools﻿focus﻿on﻿technical﻿affordance﻿and﻿ignore﻿the﻿importance﻿of﻿social﻿affordance﻿to﻿support﻿engagement﻿in﻿collaborative﻿writing . ﻿The﻿system - generated﻿visualizations﻿ by﻿using﻿learning﻿analytics﻿address﻿these﻿issues . ﻿In﻿order﻿to﻿evaluate﻿the﻿effectiveness﻿of﻿the﻿system , ﻿ we﻿conducted﻿the﻿following﻿two﻿studies﻿in﻿real﻿collaborative﻿writing﻿activities﻿of﻿university﻿courses . 5 . 1 . Study 1 : Writing a Project Proposal 5 . 1 . 1 . Participants and Procedure A﻿total﻿of﻿41﻿university﻿students﻿ ( male﻿35﻿and﻿female : ﻿6 ) ﻿participated﻿in﻿this﻿study . ﻿Those﻿student﻿ participants﻿were﻿third﻿year﻿software﻿engineering﻿students﻿ ( age﻿between﻿20﻿and﻿21 ) , ﻿who﻿came﻿from﻿ Advanced﻿Web﻿Development﻿class﻿at﻿a﻿key﻿university﻿in﻿China . ﻿They﻿were﻿allocated﻿to﻿different﻿ groups﻿and﻿asked﻿to﻿collaboratively﻿write﻿a﻿project﻿proposal﻿during﻿the﻿lab﻿session﻿as﻿one﻿of﻿the﻿assessment﻿in﻿the﻿course . The﻿project﻿proposal﻿includes﻿the﻿aim﻿of﻿project , ﻿key﻿use﻿cases , ﻿selection﻿of﻿application﻿framework﻿ and﻿data﻿model . ﻿They﻿had﻿no﻿prior﻿knowledge﻿of﻿online﻿collaborative﻿writing﻿tool﻿and﻿did﻿not﻿ participated﻿in﻿any﻿previous﻿related﻿study . ﻿The﻿writing﻿activity﻿included﻿brainstorming , ﻿planning , ﻿ writing , ﻿editing﻿and﻿reviewing . ﻿In﻿the﻿brainstorming﻿stage , ﻿group﻿members﻿got﻿together﻿to﻿discuss﻿what﻿ system﻿they﻿will﻿build﻿and﻿wrote﻿down﻿the﻿key﻿features﻿of﻿the﻿system﻿in﻿the﻿Cooperpad . ﻿Then , ﻿in﻿the﻿ planning﻿section , ﻿one﻿person﻿outlined﻿the﻿structure﻿of﻿the﻿document﻿and﻿assigned﻿each﻿group﻿member﻿ to﻿work﻿on﻿that﻿section . ﻿Each﻿person﻿worked﻿on﻿one﻿use﻿case﻿so﻿that﻿they﻿can﻿work﻿in﻿parallel , ﻿which﻿ Figure 6 . Writing behavior pattern chart International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 90 is﻿quite﻿similar﻿to﻿separate﻿writer﻿strategy . ﻿After﻿finishing﻿their﻿own﻿section , ﻿they﻿gave﻿and﻿received﻿ suggestions﻿in﻿writing﻿from﻿peers . ﻿If﻿they﻿need﻿communication , ﻿they﻿can﻿just﻿talk﻿to﻿the﻿person . During﻿the﻿writing﻿activity , ﻿Cooperpad﻿generates﻿formative﻿feedback : ﻿EIB﻿and﻿GERC﻿ ( see﻿Figure﻿ 7 ) . ﻿After﻿the﻿writing﻿activity﻿was﻿finished , ﻿the﻿system﻿generated﻿summative﻿feedback : ﻿GMECPC﻿and﻿ WBPC . ﻿At﻿the﻿conclusion﻿of﻿each﻿study , ﻿each﻿participant﻿was﻿asked﻿to﻿rate﻿the﻿quality﻿of﻿visualizations﻿ using﻿a﻿Likert﻿scale , ﻿where﻿1﻿was﻿“strongly﻿disagree”﻿and﻿5﻿was﻿“strongly﻿agree” . ﻿Participants﻿were﻿ asked﻿the﻿following﻿quality﻿measure﻿ ( QM ) ﻿questions : ﻿QM1 : I﻿agree﻿with﻿what﻿the﻿visualization﻿is﻿ showing ; ﻿QM2a : ﻿The﻿visualization﻿enhances﻿my﻿engagement﻿in﻿a﻿collaborative﻿writing﻿environment﻿by﻿ being﻿aware﻿of﻿group﻿members’﻿engagement﻿ ( EIB ) ; ﻿QM2b : ﻿The﻿visualization﻿enhances﻿my﻿engagement﻿ in﻿a﻿collaborative﻿writing﻿environment﻿by﻿being﻿aware﻿of﻿group﻿engagement﻿ranking﻿in﻿the﻿class﻿ ( GERC ) ; ﻿QM3 : ﻿Useful﻿to﻿reflect﻿on﻿what﻿I﻿did ; ﻿QM4 : ﻿The﻿visualization﻿supports﻿an﻿assessment﻿of﻿ individual﻿contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups . We﻿classified﻿each﻿visualization﻿as﻿a﻿feedback﻿into﻿formative﻿feedback﻿and﻿summative﻿feedback . ﻿ EIB﻿and﻿GERC﻿focused﻿on﻿generating﻿feedback﻿for﻿enhancing﻿engagement﻿during﻿the﻿process﻿of﻿writing , ﻿so﻿quality﻿measure﻿1﻿and﻿2﻿are﻿used﻿for﻿assessing﻿this﻿type﻿of﻿feedback . ﻿GMECPC﻿and﻿WBPC﻿ emphasized﻿on﻿generating﻿feedback﻿for﻿reflection﻿and﻿work﻿assessment﻿after﻿writing , ﻿thus﻿quality﻿ measure﻿1 , ﻿3﻿and﻿4﻿are﻿used﻿for﻿evaluating﻿this﻿type﻿of﻿feedback . 5 . 1 . 2 . Results Table﻿2﻿shows﻿some﻿descriptive﻿statistics﻿about﻿the﻿writing﻿group . ﻿There﻿are﻿12﻿groups﻿containing﻿ 41﻿students﻿in﻿this﻿class , ﻿where﻿the﻿average﻿group﻿size﻿of﻿writing﻿a﻿project﻿proposal﻿is﻿3 . 42﻿persons , ﻿ while﻿the﻿average﻿word﻿count﻿of﻿a﻿project﻿proposal﻿document﻿is﻿1290 . 78﻿words . ﻿T﻿able﻿3﻿illustrates﻿the﻿ average﻿scores﻿reported﻿by﻿participants﻿to﻿the﻿visualizations﻿as﻿a﻿form﻿of﻿feedback﻿on﻿writing﻿a﻿project﻿proposal . ﻿Regarding﻿to﻿formative﻿feedback , ﻿the﻿quality﻿measure﻿scores﻿QM1﻿and﻿QM2﻿were﻿above﻿ 4 , ﻿indicating﻿that﻿most﻿participants﻿agreed﻿that﻿they﻿understood﻿what﻿the﻿visualizations﻿were﻿trying﻿ to﻿convey﻿and﻿the﻿visualization﻿enhanced﻿their﻿engagement﻿in﻿a﻿collaborative﻿writing﻿environment . ﻿ EIB﻿and﻿GERC﻿obtain﻿similar﻿high﻿scores﻿in﻿QM1﻿ ( EIB : ﻿4 . 02 , ﻿GERC : ﻿4 . 10 ) ﻿and﻿QM2﻿ ( EIB : ﻿4 . 16 , ﻿ GERC : ﻿4 . 12 ) . ﻿ANOVA﻿revealed﻿no﻿statistical﻿differences﻿between﻿the﻿two﻿visualizations﻿ ( EIB﻿and﻿ Figure 7 . A screenshot of collaborative writing on a project proposal in the Cooperpad International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 91 GERC ) , ﻿F﻿ ( 1 , ﻿39 ) ﻿ = ﻿0 . 12 , ﻿p﻿ > ﻿0 . 05﻿in﻿QM1﻿and﻿F﻿ ( 1 , ﻿39 ) ﻿ = 0 . 10 , ﻿p﻿ > ﻿0 . 05﻿in﻿QM2 . ﻿These﻿results﻿ indicated﻿that﻿the﻿EIB﻿was﻿as﻿useful﻿as﻿the﻿GERC﻿for﻿supporting﻿engagement﻿in﻿collaborative﻿writing . For﻿summative﻿feedback﻿ ( see﻿Table﻿3 ) , ﻿the﻿average﻿scores﻿for﻿GMECPC﻿and﻿WBPC﻿in﻿QM﻿1﻿ were﻿above﻿4 , ﻿indicating﻿those﻿participants﻿agreed﻿with﻿what﻿the﻿visualizations﻿are﻿showing . ﻿In﻿QM﻿ 3 , ﻿the﻿average﻿score﻿for﻿GMECPC﻿is﻿less﻿than﻿three , ﻿which﻿indicate﻿that﻿those﻿participants﻿disagree﻿ with﻿the﻿usefulness﻿for﻿reflection﻿on﻿what﻿they﻿did . ﻿Based﻿on﻿the﻿feedback﻿from﻿the﻿students , ﻿the﻿ main﻿reason﻿is﻿that﻿this﻿chart﻿only﻿simply﻿provides﻿an﻿overall﻿contribution﻿of﻿each﻿group﻿member﻿to﻿a﻿writing﻿task﻿and﻿do﻿not﻿reveal﻿help﻿them﻿to﻿reflect﻿their﻿whole﻿writing﻿process . ﻿One﻿possible﻿better﻿ chart﻿should﻿show﻿their﻿engagement﻿history﻿during﻿the﻿whole﻿writing﻿process , ﻿where﻿the﻿x - axis﻿is﻿ time﻿and﻿y - axis﻿is﻿the﻿engagement﻿level . ﻿In﻿this﻿case , ﻿the﻿student﻿can﻿see﻿their﻿engagement﻿during﻿ the﻿whole﻿writing﻿process . ﻿On﻿the﻿contrary , ﻿the﻿mean﻿score﻿for﻿WBPC﻿is﻿3 . 95 , ﻿which﻿indicates﻿that﻿ participants﻿almost﻿agreed﻿that﻿WBPC﻿was﻿useful﻿for﻿reflection . ﻿It﻿was﻿found﻿that﻿WBPC﻿significantly﻿ outperformed﻿GMECPC , ﻿F﻿ ( 1 , ﻿39 ) ﻿ = 0 . 21 , ﻿p﻿ < ﻿0 . 05 . ﻿However , ﻿GMECPC﻿ ( M = 4 . 18 ) ﻿is﻿more﻿useful﻿ than﻿WBPC﻿ ( M = 3 . 20 ) ﻿regarding﻿to﻿the﻿assessment﻿of﻿individual﻿contributions . We﻿also﻿examined﻿the﻿relationship﻿between﻿the﻿group﻿engagement﻿measure﻿by﻿the﻿system﻿and﻿ score﻿given﻿by﻿the﻿teacher﻿based﻿on﻿the﻿quality﻿of﻿this﻿group’s﻿project﻿proposal﻿ ( two﻿main﻿factors : ﻿ document﻿presentation﻿and﻿technical﻿feasibility ) . ﻿Table﻿4﻿shows﻿the﻿Pearson﻿correlation﻿coefficient﻿is﻿ . 53﻿with﻿p﻿ < ﻿0 . 05 , ﻿indicating﻿there﻿is﻿a﻿significant﻿moderate﻿correlation﻿between﻿group﻿engagement﻿ measured﻿by﻿the﻿system﻿and﻿scores﻿for﻿this﻿group﻿writing﻿task . 5 . 2 . Study 2 : Writing Tutorial Discussion Answers 5 . 2 . 1 . Participants and Procedure A﻿total﻿of﻿35﻿university﻿students﻿participated﻿ ( male : ﻿24﻿and﻿female : ﻿11 ) ﻿and﻿those﻿student﻿participants﻿ were﻿second﻿year﻿software﻿engineering﻿students﻿ ( age﻿between﻿19﻿and﻿20 ) , ﻿who﻿came﻿from﻿System﻿ Table 2 . Dataset description Collaborative Writing Task Num . of Groups Num . of Students Ave . Num . of Persons Per Group Ave . Num . of Words Per Group Project﻿Proposal 12 41 3 . 42 1290 . 78 Table 3 . Evaluation of visualization for writing a project proposal Quality Measure Formative Feedback Summative Feedback EIB GERC GMECPC WBPC QM1 : ﻿I﻿agree﻿with﻿what﻿the﻿visualization﻿is﻿showing . M = 4 . 02﻿ SD = 0 . 59﻿ N = 41 M = 4 . 10﻿ SD = 0 . 42﻿ N = 41 M = 4 . 51﻿ SD = 0 . 61﻿ N = 41 M = 4 . 13﻿ SD = 0 . 18﻿ N = 41 QM2 : ﻿The﻿visualization﻿enhances﻿my﻿engagement﻿in﻿a﻿ collaborative﻿writing﻿environment﻿by﻿being﻿aware﻿of﻿group﻿members’﻿engagement﻿or﻿group﻿engagement﻿ranking﻿in﻿the﻿class M = 4 . 16﻿ SD = 0 . 32﻿ N = 41 M = 4 . 12﻿ SD = 0 . 31﻿ N = 41 QM3 : ﻿Useful﻿to﻿reflect﻿on﻿what﻿I﻿did M = 2 . 58﻿ SD = 0 . 98﻿ N = 41 M = 3 . 95﻿ SD = 0 . 12﻿ N = 41 QM4 : ﻿The﻿visualization﻿supports﻿an﻿assessment﻿of﻿individual﻿ contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups . M = 4 . 18﻿ SD = 0 . 78﻿ N = 41 M = 3 . 20﻿ SD = 0 . 38﻿ N = 41 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 92 Analysis﻿and﻿Design﻿class﻿at﻿Southwest﻿University . ﻿They﻿were﻿allocated﻿to﻿different﻿groups﻿and﻿asked﻿ to﻿collaboratively﻿write﻿the﻿answers﻿about﻿functional﻿modeling﻿during﻿the﻿tutorial﻿session﻿about﻿80﻿minutes . ﻿Because﻿this﻿course﻿is﻿bilingual﻿teaching﻿course﻿and﻿those﻿content﻿is﻿primarily﻿taught﻿in﻿ English . ﻿Therefore , ﻿the﻿tutorial﻿discussion﻿questions﻿and﻿answers﻿were﻿both﻿in﻿English . ﻿We﻿found﻿that﻿ two﻿types﻿of﻿writing﻿strategies﻿were﻿used﻿by﻿students . ﻿Three﻿groups﻿of﻿students﻿collaboratively﻿work﻿ on﻿the﻿same﻿question﻿ ( joint﻿write ) . ﻿Each﻿person﻿first﻿tried﻿to﻿come﻿up﻿a﻿solution﻿for﻿each﻿question﻿ and﻿wrote﻿down﻿their﻿answers﻿in﻿the﻿Cooperpad . ﻿Then , ﻿they﻿combined﻿their﻿answers﻿together . ﻿Other﻿ four﻿groups﻿of﻿students﻿first﻿divided﻿the﻿task﻿and﻿work﻿on﻿individual﻿questions﻿ ( separate﻿write ) . ﻿If﻿ they﻿need﻿communication , ﻿they﻿can﻿just﻿talk﻿to﻿the﻿person . ﻿Their﻿common﻿writing﻿goal﻿is﻿to﻿find﻿the﻿ solutions﻿for﻿tutorial﻿questions . ﻿They﻿spent﻿about﻿40﻿minutes﻿in﻿writing . ﻿The﻿student﻿informational﻿ sources﻿are﻿based﻿on﻿lecture﻿slides . ﻿They﻿checked﻿the﻿lecture﻿slides﻿when﻿they﻿have﻿problems﻿in﻿ finding﻿the﻿solution . ﻿Similar﻿to﻿study﻿1 , ﻿we﻿used﻿the﻿quality﻿measurements﻿to﻿evaluate﻿the﻿system - generated﻿visualization . An﻿example﻿of﻿tutorial﻿questions﻿is﻿shown﻿below : Review﻿the﻿Amazon . com﻿Web﻿site . ﻿Develop﻿the﻿requirements﻿definition﻿for﻿the﻿site . ﻿Create﻿a﻿ list﻿of﻿functional﻿business﻿requirements﻿that﻿the﻿system﻿meets . ﻿What﻿different﻿kinds﻿of﻿nonfunctional﻿ business﻿requirements﻿does﻿the﻿system﻿meet ? ﻿Provide﻿examples﻿for﻿each﻿kind . ﻿Suppose﻿that﻿you﻿ are﻿going﻿to﻿build﻿a﻿new﻿system﻿that﻿automates﻿or﻿improves﻿the﻿interview﻿process﻿for﻿the﻿Career﻿Services﻿Department﻿of﻿your﻿school . ﻿Develop﻿a﻿requirements﻿definition﻿for﻿the﻿new﻿system . ﻿Include﻿ both﻿functional﻿and﻿nonfunctional﻿system﻿requirements . ﻿Pretend﻿you﻿will﻿release﻿the﻿system﻿in﻿three﻿ different﻿versions . ﻿Prioritize﻿the﻿requirements﻿accordingly . ﻿Suppose﻿you﻿are﻿the﻿analyst﻿charged﻿with﻿ developing﻿a﻿new﻿system﻿for﻿the﻿university﻿bookstore﻿with﻿which﻿students﻿can﻿order﻿books﻿online﻿and﻿have﻿them﻿delivered﻿to﻿their﻿dorms﻿and﻿off - campus﻿housing . ﻿What﻿requirements - gathering﻿techniques﻿ will﻿you﻿use ? ﻿Describe﻿in﻿detail﻿how﻿you﻿would﻿apply﻿the﻿techniques . 5 . 2 . 2 . Results Table﻿5﻿describes﻿some﻿descriptive﻿statistics﻿about﻿this﻿writing﻿activity . ﻿The﻿average﻿group﻿size﻿in﻿a﻿ tutorial﻿discussion﻿is﻿5﻿persons , ﻿and﻿the﻿average﻿word﻿count﻿of﻿the﻿tutorial﻿discussion﻿is﻿892﻿words . ﻿ Table﻿6﻿shows﻿the﻿average﻿scores﻿rated﻿by﻿student﻿participants﻿working﻿on﻿the﻿tutorial﻿discussion . ﻿With﻿ respect﻿to﻿formative﻿feedback , ﻿the﻿average﻿scores﻿in﻿both﻿QM1﻿and﻿QM2﻿were﻿above﻿4 , ﻿indicating﻿ that﻿most﻿participants﻿agreed﻿that﻿they﻿understood﻿what﻿the﻿visualizations﻿were﻿trying﻿to﻿convey﻿and﻿the﻿visualization﻿enhanced﻿their﻿engagement﻿in﻿a﻿collaborative﻿writing﻿environment . ﻿EIB﻿were﻿as﻿ useful﻿as﻿GERC﻿in﻿both﻿quality﻿measures . For﻿the﻿summative﻿feedback , ﻿the﻿average﻿scores﻿for﻿GMECPC﻿and﻿WBPC﻿in﻿QM﻿1﻿were﻿ above﻿4 , ﻿indicating﻿those﻿participants﻿agreed﻿with﻿what﻿the﻿visualization﻿are﻿showing . ﻿The﻿average﻿ scores﻿for﻿GMECPC﻿was﻿just﻿3 . 03 , ﻿indicating﻿that﻿those﻿participants﻿keep﻿neutral﻿opinion﻿about﻿the﻿ Table 4 . Pearson Correlation between Student Engagement and Scores for the project proposal assignment Group Engagement Score . 53﻿ ( n = 12 ) ﻿ p﻿ < ﻿0 . 05 Table 5 . Dataset description Collaborative Writing Task Num . of Groups Num . of Students Ave . Num . of Persons Per Group Ave . Num . of Words Per Group Tutorial﻿Discussion 7 35 5 892 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 93 usefulness﻿for﻿reflection . ﻿The﻿average﻿score﻿for﻿WBPC﻿ ( M = 4 . 15 ) ﻿in﻿QM﻿3﻿is﻿higher﻿than﻿which﻿of﻿ GECPR , ﻿demonstrating﻿that﻿those﻿participants﻿agree﻿with﻿the﻿usefulness﻿for﻿reflection . ﻿Regarding﻿ to﻿the﻿assessment﻿of﻿individual , ﻿the﻿average﻿score﻿ ( M = 4 . 48 ) ﻿obtained﻿for﻿GMECPC﻿was﻿higher﻿than﻿ that﻿of﻿WBPC﻿ ( M = 3 . 35 ) . ﻿These﻿results﻿were﻿almost﻿consistent﻿with﻿the﻿results﻿obtained﻿in﻿study﻿1 . In﻿short , ﻿the﻿results﻿of﻿both﻿studies﻿show﻿that : •﻿ Writers﻿agreed﻿with﻿what﻿the﻿formative﻿feedback﻿visualizations﻿showed﻿ ( agreement﻿4 . 02 - 4 . 10﻿ in﻿study﻿1﻿while﻿agreement﻿4 . 29 - 4 . 36﻿in﻿study﻿2 ) •﻿ Writers﻿agreed﻿with﻿what﻿the﻿summative﻿feedback﻿visualizations﻿showed﻿ ( agreement﻿4 . 13 - 4 . 51﻿ in﻿study﻿1﻿while﻿agreement﻿4 . 06 - 4 . 35﻿in﻿study﻿2 ) •﻿ Writers﻿agreed﻿that﻿the﻿formative﻿feedback﻿visualization﻿supported﻿my﻿engagement﻿in﻿a﻿collaborative﻿writing﻿environment﻿ ( agreement﻿4 . 12 - 4 . 16﻿in﻿study﻿1﻿while﻿agreement﻿4 . 26 - 4 . 67﻿ in﻿study﻿2 ) •﻿ Writers﻿found﻿that﻿the﻿writing﻿behavior﻿pattern﻿chart﻿was﻿useful﻿to﻿reflect﻿on﻿ ( agreement﻿3 . 95﻿ in﻿study﻿1﻿while﻿agreement﻿4 . 15﻿in﻿study﻿2 ) •﻿ Writers﻿found﻿that﻿the﻿group﻿member﻿engagement﻿contribution﻿pie﻿chart﻿was﻿useful﻿to﻿supports﻿an﻿assessment﻿of﻿individual﻿contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups . ﻿ ( Agreement﻿ 4 . 18﻿in﻿study﻿1﻿while﻿agreement﻿4 . 48﻿in﻿study﻿2 ) 5 . 3 . Student Interview In﻿addition﻿to﻿the﻿questionnaire , ﻿a﻿face - to - face﻿interview﻿with﻿student﻿participants﻿in﻿both﻿classes﻿ has﻿been﻿conducted﻿to﻿get﻿an﻿understanding﻿of﻿student﻿experience﻿of﻿using﻿Cooperpad . ﻿The﻿overall﻿ feedback﻿from﻿the﻿students﻿was﻿very﻿positive . ﻿First﻿of﻿all , ﻿most﻿of﻿students﻿like﻿the﻿efficiency﻿of﻿the﻿ collaborative﻿writing﻿tool . For﻿example , ﻿“ it is more effective to finish the task ” . ﻿“ It does not have location limitations , everyone can easily contribute to the discussion . “Compared with traditional tutorial discussion , this way to discussion is more organized and effective since you can directly write and share your thoughts with your team mates and the discussion results are clearer . ” Secondly , ﻿the﻿visualization﻿helps﻿them﻿get﻿more﻿engaged﻿in﻿the﻿task . For﻿example , ﻿ it is easy to use and motivates me to get more engaged in the task by being aware of others’ engagement . Lastly , ﻿the﻿collaborative﻿writing﻿tool﻿is﻿particularly﻿useful﻿in﻿tutorial﻿discussion﻿for﻿those﻿students﻿ with﻿poor﻿oral﻿communication﻿skill . Table 6 . Evaluation of visualization for working on tutorial discussion Quality Measure Formative Feedback Summative Feedback EIB GERC GMECPC WBPC QM1 : ﻿I﻿agree﻿with﻿what﻿the﻿visualization﻿is﻿showing . M = 4 . 36﻿ SD = 0 . 80﻿ N = 35 M = 4 . 29﻿ SD = 0 . 96﻿ N = 35 M = 4 . 35﻿ SD = 0 . 41﻿ N = 35 M = 4 . 06﻿ SD = 1 . 00﻿ N = 35 QM2 : ﻿The﻿visualization﻿enhances﻿my﻿engagement﻿in﻿a﻿ collaborative﻿writing﻿environment . M = 4 . 67﻿ SD = 0 . 49﻿ N = 35 M = 4 . 26﻿ SD = 0 . 96﻿ N = 35 QM3 : ﻿Useful﻿to﻿reflect﻿on﻿what﻿I﻿did M = 3 . 03﻿ SD = 0 . 18﻿ N = 35 M = 4 . 15﻿ SD = 1 . 00﻿ N = 35 QM4 : ﻿The﻿visualization﻿supports﻿an﻿assessment﻿of﻿ individual﻿contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups . M = 4 . 48﻿ SD = 0 . 25﻿ N = 35 M = 3 . 35﻿ SD = 0 . 15﻿ N = 35 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 94 For﻿example , ﻿ it provides equal opportunity for everyone to express their ideas through writing the answers on the Cooperpad during the joint writing , which is particularly true for those students who are very shy and not good at English . However , ﻿some﻿issues﻿were﻿raised﻿by﻿student﻿participants . ﻿For﻿example , ﻿this﻿online﻿editor﻿does﻿not﻿ support﻿drawing﻿graphs . ﻿Addition , ﻿this﻿tool﻿cannot﻿capture﻿my﻿engagement﻿if﻿I﻿am﻿orally﻿discussing﻿ with﻿my﻿teammate . 6 . DISCUSSIoN AND CoNCLUSIoN Existing﻿collaborative﻿writing﻿tools﻿cannot﻿track﻿and﻿measure﻿student﻿engagement , ﻿evaluate﻿individual﻿ contribution﻿and﻿provide﻿feedback﻿to﻿support﻿collaborative﻿writing﻿ ( Roberts﻿and﻿McInnerney , ﻿2007 ) . ﻿ This﻿study﻿attempted﻿to﻿automatically﻿capture﻿the﻿student﻿behavior﻿during﻿a﻿collaborative﻿writing﻿task﻿by﻿developing﻿Cooperpad , ﻿a﻿novel﻿Learning﻿Analytic﻿system﻿which﻿uses﻿Etherpad﻿API﻿to﻿collect﻿ the﻿document’s﻿revisions , ﻿then﻿analyses﻿them﻿and﻿generates﻿quantitative﻿and﻿visual﻿measures﻿of﻿ behavioral﻿engagement﻿and﻿pattern﻿over﻿time﻿to﻿support﻿their﻿engagement﻿and﻿individual﻿assessment . ﻿ These﻿visualizations﻿successfully﻿illustrated﻿Kirschner’s﻿an﻿affordance﻿framework , ﻿including﻿social﻿ affordance﻿and﻿technological﻿affordance , ﻿for﻿designing﻿and﻿evaluating﻿such﻿collaborative﻿learning﻿ environment﻿ ( Kirschner , ﻿2004 ) . The﻿system﻿was﻿evaluated﻿in﻿two﻿different﻿collaborative﻿writing﻿tasks , ﻿writing﻿a﻿project﻿proposal﻿ and﻿tutorial﻿discussion , ﻿from﻿university﻿courses . ﻿The﻿visualization﻿evaluation﻿results﻿show﻿that﻿the﻿ average﻿quality﻿measure﻿scores﻿QM1﻿and﻿QM2﻿were﻿above﻿4﻿in﻿both﻿studies . ﻿It﻿indicated﻿that﻿writers﻿ agreed﻿with﻿what﻿the﻿visualizations﻿conveyed﻿and﻿showed . ﻿In﻿addition , ﻿the﻿average﻿scores﻿for﻿writing﻿ behavior﻿pattern﻿chart﻿in﻿QM3﻿were﻿above﻿3 . 95﻿in﻿both﻿studies , ﻿which﻿indicates﻿that﻿the﻿participants﻿ almost﻿agree﻿that﻿the﻿visualization﻿was﻿useful﻿to﻿reflect﻿on , ﻿while﻿the﻿average﻿scores﻿for﻿group﻿member﻿ engagement﻿contribution﻿chart﻿were﻿above﻿4 . 18 , ﻿which﻿implied﻿that﻿the﻿participants﻿agree﻿that﻿the﻿ visualization﻿supports﻿an﻿assessment﻿of﻿individual﻿contributions﻿of﻿the﻿members﻿within﻿the﻿student﻿groups . ﻿Furthermore , ﻿it﻿has﻿found﻿that﻿the﻿correlation﻿between﻿the﻿engagement﻿measured﻿by﻿the﻿system﻿ and﻿the﻿scores﻿given﻿by﻿the﻿teacher﻿in﻿study﻿1﻿is﻿moderate﻿ ( r﻿ > . 50 ) . ﻿This﻿result﻿is﻿in﻿consistent﻿with﻿ previous﻿findings﻿which﻿show﻿that﻿student﻿engagement﻿is﻿positively﻿correlated﻿to﻿college﻿students’﻿grade﻿point﻿average﻿scores﻿ ( Casuso - Holgado﻿et﻿al . , ﻿2013 ) . Our﻿current﻿system﻿lacks﻿of﻿a﻿good﻿communication﻿channel , ﻿such﻿as﻿an﻿online﻿chat﻿function , ﻿ during﻿collaboration , ﻿which﻿is﻿a﻿useful﻿feature﻿for﻿social﻿affordance . ﻿Having﻿this﻿feature﻿allows﻿us﻿ to﻿capture﻿more﻿communication﻿information﻿and﻿it﻿could﻿improve﻿the﻿accuracy﻿of﻿the﻿engagement﻿measurement﻿algorithm﻿and﻿visualization . ﻿Moreover , ﻿both﻿studies﻿were﻿conducted﻿in﻿a﻿classroom , ﻿ where﻿the﻿writing﻿groups﻿were﻿co - located﻿and﻿the﻿participants﻿were﻿university﻿students﻿with﻿advanced﻿ computer﻿skills . In﻿future﻿work , ﻿we﻿will﻿evaluate﻿this﻿tool﻿from﻿various﻿aspects , ﻿such﻿as﻿user﻿experience﻿and﻿co - location﻿of﻿writing﻿groups . ﻿Besides , ﻿we﻿will﻿include﻿engagement﻿estimations﻿that﻿consider﻿additional﻿ dimensions﻿of﻿writing , ﻿including﻿the﻿quality﻿of﻿content , ﻿word﻿count﻿and﻿changes﻿made . ﻿Furthermore , ﻿ we﻿will﻿investigate﻿the﻿technological﻿and﻿social﻿awareness﻿further : ﻿how﻿the﻿tool﻿facilitates﻿social﻿ awarenesses﻿and﻿to﻿what﻿level . ACKNoWLEDGMENT This﻿work﻿is﻿supported﻿by﻿Chongqing﻿Social﻿Science﻿Planning﻿Fund﻿Program﻿under﻿grant﻿No . ﻿ 2014BS123 , ﻿Fundamental﻿Research﻿Funds﻿for﻿the﻿Central﻿Universities﻿under﻿grant﻿No . ﻿SWU114005 , ﻿ XDJK2017C024 , ﻿XDJK2017D064 , ﻿CQU903005203326 , ﻿and﻿National﻿Natural﻿Science﻿Foundation﻿ of﻿China﻿ ( 61502397 ) , ﻿and﻿the﻿Scientific﻿Research﻿Foundation﻿for﻿the﻿Returned﻿Overseas﻿Chinese﻿ Scholars , ﻿State﻿Education﻿Ministry , ﻿and﻿the﻿Research﻿Program﻿Funds﻿of﻿Faculty﻿of﻿Education , ﻿at﻿ Southwest﻿University﻿ ( Learning﻿Technology﻿Assisted﻿Writing﻿Research﻿Project ) . International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 95 REFERENCES Aviv , ﻿R . , ﻿Erlich , ﻿Z . , ﻿Ravid , ﻿G . , ﻿ & ﻿Geva , ﻿A . ﻿ ( 2003 ) . ﻿Network﻿analysis﻿of﻿knowledge﻿construction﻿in﻿asynchronous﻿ learning﻿networks . ﻿ Journal of Asynchronous Learning Networks , ﻿ 7 , ﻿1 – 23 . Beck , ﻿E . ﻿E . ﻿ ( 1993 ) . ﻿A﻿Survey﻿of﻿Experiences﻿of﻿Collaborative﻿Writing . ﻿In﻿Sharples﻿ ( Ed . ) , ﻿Computer﻿Supported﻿ Collaborative﻿Writing﻿ ( pp . ﻿87 – 112 ) . ﻿doi : 10 . 1007 / 978 - 1 - 4471 - 2007 - 0 _ 6 Berland , ﻿M . , ﻿Davis , ﻿D . , ﻿ & ﻿Smith , ﻿C . ﻿P . ﻿ ( 2015 ) . ﻿AMOEBA : ﻿Designing﻿for﻿collaboration﻿in﻿computer﻿science﻿ classrooms﻿through﻿live﻿learning﻿analytics . ﻿ Int . J . Comput . Collab . Learn . , ﻿ 10 ( 4 ) , ﻿425 – 447 . ﻿doi : 10 . 1007 / s11412 - 015 - 9217 - z Bixler , ﻿R . , ﻿ & ﻿D’Mello , ﻿S . ﻿ ( 2013 ) . ﻿Detecting﻿Boredom﻿and﻿Engagement﻿During﻿Writing﻿with﻿Keystroke﻿Analysis﻿ Task﻿Appraisals , ﻿and﻿Stable﻿Traits . ﻿ Proceedings of the ﻿ 2013 International Conference on Intelligent User Interfaces ﻿ ( pp . ﻿225 – 233 ) . ﻿doi : 10 . 1145 / 2449396 . 2449426 Bower , ﻿M . ﻿ ( 2008 ) . ﻿Affordance﻿analysis﻿ – ﻿matching﻿learning﻿tasks﻿with﻿learning﻿technologies . ﻿ Educational Media International , ﻿ 45 ( 1 ) , ﻿3 – 15 . ﻿doi : 10 . 1080 / 09523980701847115 Brown , ﻿M . , ﻿2012 . ﻿Learning﻿Analytics : ﻿Moving﻿from﻿Concept﻿to﻿Practice . ﻿ Educ . Learn . Initiat . Br . Calvo , ﻿R . ﻿A . , ﻿ORourke , ﻿S . ﻿T . , ﻿Jones , ﻿J . , ﻿Yacef , ﻿K . , ﻿ & ﻿Reimann , ﻿P . ﻿ ( 2011 ) . ﻿Collaborative﻿writing﻿support﻿tools﻿ on﻿the﻿cloud . ﻿ IEEE Trans . Learn . Technol . , ﻿ 4 ( 1 ) , ﻿88 – 97 . ﻿doi : 10 . 1109 / TLT . 2010 . 43 Campbell , ﻿J . ﻿P . , ﻿DeBlois , ﻿P . ﻿B . , ﻿ & ﻿Oblinger , ﻿D . ﻿G . ﻿ ( 2007 ) . ﻿Academic﻿Analytics : ﻿A﻿New﻿Tool﻿for﻿a﻿New﻿Era . ﻿ Educational Review , ﻿ 42 , ﻿40 – 57 . Caspi , ﻿A . , ﻿ & ﻿Blau , ﻿I . ﻿ ( 2011 ) . ﻿Collaboration﻿and﻿psychological﻿ownership : ﻿How﻿does﻿the﻿tension﻿between﻿the﻿two﻿ influence﻿perceived﻿learning ? ﻿ Social Psychology of Education , ﻿ 14 ( 2 ) , ﻿283 – 298 . ﻿doi : 10 . 1007 / s11218 - 010 - 9141 - z Casuso - Holgado , ﻿M . ﻿J . , ﻿Cuesta - Vargas , ﻿A . ﻿I . , ﻿Moreno - Morales , ﻿N . , ﻿Labajos - Manzanares , ﻿M . ﻿T . , ﻿Barón - López , ﻿ F . ﻿J . , ﻿ & ﻿Vega - Cuesta , ﻿M . ﻿ ( 2013 ) . ﻿The﻿association﻿between﻿academic﻿engagement﻿and﻿achievement﻿in﻿health﻿ sciences﻿students . ﻿ BMC Medical Education , ﻿ 13 ( 1 ) , ﻿33 . ﻿doi : 10 . 1186 / 1472 - 6920 - 13 - 33﻿PMID : 23446005 Cen , ﻿L . , ﻿Ruta , ﻿D . , ﻿Powell , ﻿L . , ﻿Hirsch , ﻿B . , ﻿ & ﻿Ng , ﻿J . ﻿ ( 2016 ) . ﻿Quantitative﻿approach﻿to﻿collaborative﻿learning : ﻿ Performance﻿prediction , ﻿individual﻿assessment , ﻿and﻿group﻿composition . ﻿ Int . J . Comput . Collab . Learn . , ﻿ 11 ( 2 ) , ﻿ 187 – 225 . ﻿doi : 10 . 1007 / s11412 - 016 - 9234 - 6 Cole , ﻿M . ﻿ ( 2009 ) . ﻿Using﻿Wiki﻿technology﻿to﻿support﻿student﻿engagement : ﻿Lessons﻿from﻿the﻿trenches . ﻿ Computers & Education , ﻿ 52 ( 1 ) , ﻿141 – 146 . ﻿doi : 10 . 1016 / j . compedu . 2008 . 07 . 003 Connor , ﻿U . , ﻿ & ﻿Asenavage , ﻿K . ﻿ ( 1994 ) . ﻿Peer﻿response﻿groups﻿in﻿esl﻿writing﻿classes : ﻿How﻿much﻿impact﻿on﻿revision ? ﻿ Journal of Second Language Writing , ﻿ 3 ( 3 ) , ﻿257 – 276 . ﻿doi : 10 . 1016 / 1060 - 3743 ( 94 ) 90019 - 1 Daemmrich , ﻿I . ﻿G . ﻿ ( 2000 ) . ﻿ An Analysis of the Current Chinese Characters Similar in Form . J . Southwest Norm . Univ ﻿ ( 26th﻿ed . , ﻿pp . ﻿12512 – 12519 ) . ﻿Humanities﻿Socail﻿Sci . Dieberger , ﻿A . ﻿2000 . ﻿Where﻿did﻿all﻿the﻿people﻿go ? ﻿A﻿collaborative﻿web﻿space﻿with﻿social﻿navigation﻿information . ﻿ Proceedings of the ﻿ 9th International World Wide Web Conference , ﻿Amsterdam . Ellis , ﻿R . ﻿A . , ﻿ & ﻿Goodyear , ﻿P . ﻿ ( 2010 ) . ﻿ Students’ experiences of E - learning in higher education : The ecology of sustainable innovation . ﻿New﻿York : ﻿Routledge . Elola , ﻿I . , ﻿ & ﻿Oskoz , ﻿A . ﻿ ( 2010 ) . ﻿Collaborative﻿writing : ﻿Fostering﻿foreign﻿language﻿and﻿writing﻿conventions﻿ development . ﻿ Language Learning & Technology , ﻿ 14 , ﻿51 – 71 . Essa , ﻿A . , ﻿ & ﻿Ayad , ﻿H . ﻿ ( 2012 ) . ﻿improving﻿student﻿success﻿using﻿predictive﻿models﻿and﻿data﻿visualisations . ﻿ Proceedings﻿of﻿the﻿19th﻿Association﻿for﻿Learning﻿Technology﻿Conference﻿ ( pp . ﻿58 – 70 ) . ﻿doi : 10 . 3402 / rlt . v20i0 . 19191 Ferguson , ﻿R . , ﻿ & ﻿Shum , ﻿S . ﻿B . ﻿ ( 2011 ) . ﻿Learning﻿analytics﻿to﻿identify﻿exploratory﻿dialogue﻿within﻿synchronous﻿ text﻿chat . ﻿ Proceedings of the 1st International Conference on Learning Analytics and Knowledge LAK ’11 . ﻿New﻿ York : ﻿ACM﻿Press . ﻿doi : 10 . 1145 / 2090116 . 2090130 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 96 Ferguson , ﻿R . , ﻿ & ﻿Shum , ﻿S . ﻿B . ﻿ ( 2012 ) . ﻿Social﻿Learning﻿Analytics : ﻿Five﻿Approaches . ﻿ Proceedings of the 2nd International Conference on Learning Analytics and Knowledge LAK ’12 . ﻿New﻿York : ﻿ACM﻿Press . ﻿ doi : 10 . 1145 / 2330601 . 2330616 Ferguson , ﻿R . , ﻿Wei , ﻿Z . , ﻿He , ﻿Y . , ﻿ & ﻿Buckingham﻿Shum , ﻿S . ﻿ ( 2013 ) . ﻿An﻿evaluation﻿of﻿learning﻿analytics﻿to﻿identify﻿ exploratory﻿dialogue﻿in﻿online﻿discussions . ﻿ Proceedings of the Third International Conference on Learning Analytics and Knowledge LAK ’13 . ﻿New﻿York : ﻿ACM﻿Press . ﻿doi : 10 . 1145 / 2460296 . 2460313 Fitzgerald , ﻿J . ﻿ ( 1987 ) . ﻿Research﻿on﻿Revision﻿in﻿Writing . ﻿ Review of Educational Research , ﻿ 57 ( 4 ) , ﻿481 – 506 . ﻿ doi : 10 . 3102 / 00346543057004481 Flower , ﻿L . , ﻿Schriver , ﻿K . A . , ﻿Carey , ﻿L . ﻿ ( 1989 ) . ﻿Planning﻿in﻿writing : ﻿The﻿cognition﻿of﻿a﻿constructive﻿process . Fredricks , ﻿J . ﻿A . , ﻿Blumenfeld , ﻿P . ﻿C . , ﻿ & ﻿Paris , ﻿A . ﻿H . ﻿ ( 2004 ) . ﻿School﻿Engagement : ﻿Potential﻿of﻿the﻿Concept , ﻿State﻿ of﻿the﻿Evidence . ﻿ Review of Educational Research , ﻿ 74 ( 1 ) , ﻿59 – 109 . ﻿doi : 10 . 3102 / 00346543074001059 Fritz , ﻿J . ﻿ ( 2011 ) . ﻿Classroom﻿walls﻿that﻿talk : ﻿Using﻿online﻿course﻿activity﻿data﻿of﻿successful﻿students﻿to﻿raise﻿ self - awareness﻿of﻿underperforming﻿peers . ﻿ The Internet and Higher Education , ﻿ 14 ( 2 ) , ﻿89 – 97 . ﻿doi : 10 . 1016 / j . iheduc . 2010 . 07 . 007 Galbraith , ﻿D . ﻿ ( 1999 ) . ﻿Writing﻿as﻿a﻿knowledge - constituting﻿process . ﻿In﻿ Knowing What to Write : Conceptual Processes in Text Production ﻿ ( pp . ﻿139 – 160 ) . ﻿Amsterdam : ﻿Amsterdam﻿University﻿Press . Hodges , ﻿G . ﻿C . ﻿ ( 2002 ) . ﻿Learning﻿through﻿collaborative﻿writing . ﻿Reading , ﻿36 , ﻿4 – 10 . Hoefler , ﻿M . , ﻿ & ﻿Guetl , ﻿C . ﻿ ( 2011 ) . ﻿Enhancing﻿Wikis﻿with﻿Visualization﻿Tools﻿to﻿Support﻿Groups﻿Production﻿ Function﻿and﻿to﻿Maintain﻿Task﻿and﻿Social﻿Awareness . ﻿Procs . ﻿ICBL﻿1 – 6 . Jonassen , ﻿D . , ﻿ & ﻿Land , ﻿S . ﻿ ( 2012 ) . ﻿Theoretical﻿Foundations﻿of﻿Learning﻿Environments﻿ ( 2nd﻿ed . ) . ﻿Routledge . Keys , ﻿C . ﻿W . ﻿ ( 1994 ) . ﻿The﻿Development﻿of﻿Scientific﻿Reasoning﻿Skills﻿in﻿Conjunction﻿with﻿Collaborative﻿Writing﻿ Assignments : ﻿An﻿Interpretive﻿Study﻿of﻿Six﻿Ninth - Grade﻿Students . ﻿ Journal of Research in Science Teaching , ﻿ 31 ( 9 ) , ﻿1003 – 1022 . ﻿doi : 10 . 1002 / tea . 3660310912 Kirschner , ﻿P . ﻿A . ﻿ ( 2004 ) . ﻿Design , ﻿development , ﻿and﻿implementation﻿of﻿electronic﻿learning﻿environments﻿for﻿ collaborative﻿learning . ﻿ Educational Technology Research and Development . ﻿doi : 10 . 1007 / BF02504674 Koh , ﻿E . , ﻿Shibani , ﻿A . , ﻿Tan , ﻿J . ﻿P . , ﻿ & ﻿Hong , ﻿H . ﻿ ( 2016 ) . ﻿A﻿Pedagogical﻿Framework﻿for﻿Learning﻿Analytics﻿in﻿ Collaborative﻿Inquiry﻿Tasks : ﻿An﻿Example﻿from﻿a﻿Teamwork﻿Competency﻿Awareness﻿Program . ﻿ Proc . LAK16 6th Int . Conf . Anal . Knowl . ﻿ ( pp . ﻿74 – 83 ) . ﻿doi : 10 . 1145 / 2883851 . 2883914 Kraut , ﻿R . , ﻿Egido , ﻿C . , ﻿ & ﻿Galegher , ﻿J . ﻿ ( 1988 ) . ﻿Patterns﻿of﻿contact﻿and﻿communication﻿in﻿scientific﻿research﻿ collaboration . ﻿ Proceedings of the 1988 ACM Conference on Computer - Supported Cooperative Work CSCW ’88 ﻿ ( pp . ﻿1 – 12 ) . ﻿doi : 10 . 1145 / 62266 . 62267 Kubincová , ﻿Z . , ﻿Homola , ﻿M . ﻿B . , ﻿ & ﻿Janajev , ﻿R . ﻿ ( 2012 ) . ﻿Tool - supported﻿assessment﻿of﻿wiki - based﻿assignments . ﻿ Proceedings of the 4th International Conference on Computer Supported Education ﻿ ( pp . ﻿58 – 67 ) . Latif , ﻿M . ﻿M . ﻿A . ﻿ ( 2008 ) . ﻿A﻿state - of - the - art﻿review﻿of﻿the﻿real - time﻿computer - aided﻿study﻿of﻿the﻿writing﻿process . ﻿ Int . J . English Stud . , ﻿ 8 , ﻿29 – 50 . Limbu , ﻿L . , ﻿ & ﻿Markauskaite , ﻿L . ﻿ ( 2015 ) . ﻿How﻿do﻿learners﻿experience﻿joint﻿writing : ﻿University﻿students﻿conceptions﻿ of﻿online﻿collaborative﻿writing﻿tasks﻿and﻿environments . ﻿ Computers & Education , ﻿ 82 , ﻿393 – 408 . ﻿doi : 10 . 1016 / j . compedu . 2014 . 11 . 024 Liu , ﻿M . , ﻿Calvo , ﻿R . ﻿A . , ﻿ & ﻿Pardo , ﻿A . ﻿ ( 2013 ) . ﻿Tracer : ﻿A﻿tool﻿to﻿measure﻿and﻿visualize﻿student﻿engagement﻿in﻿writing﻿ activities . ﻿ Proceedings of the 2013 IEEE 13th International Conference on Advanced Learning Technologies ICALT ‘13 ﻿ ( pp . ﻿421 – 425 ) . ﻿doi : 10 . 1109 / ICALT . 2013 . 129 Lonn , ﻿S . , ﻿Krumm , ﻿A . ﻿E . , ﻿Waddington , ﻿R . ﻿J . , ﻿ & ﻿Teasley , ﻿S . ﻿D . ﻿ ( 2012 ) . ﻿Bridging﻿the﻿Gap﻿from﻿Knowledge﻿to﻿ Action : ﻿Putting﻿Analytics﻿in﻿the﻿Hands﻿of﻿Academic﻿Advisors . ﻿ Proceedings of the ﻿ International Conference on Learning Analytics and Knowledge ﻿ ( pp . ﻿184 – 187 ) . ﻿ACM﻿Press . ﻿doi : 10 . 1145 / 2330601 . 2330647 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 97 Mazza , ﻿R . , ﻿ & ﻿Dimitrova , ﻿V . ﻿ ( 2007 ) . ﻿CourseVis : ﻿A﻿graphical﻿student﻿monitoring﻿tool﻿for﻿supporting﻿instructors﻿in﻿ web - based﻿distance﻿courses . ﻿ International Journal of Human - Computer Studies , ﻿ 65 ( 2 ) , ﻿125 – 139 . ﻿doi : 10 . 1016 / j . ijhcs . 2006 . 08 . 008 Mckay , ﻿T . , ﻿Miller , ﻿K . , ﻿ & ﻿Tritz , ﻿J . ﻿ ( 2012 ) . ﻿What﻿to﻿do﻿with﻿actionable﻿intelligence : ﻿E2Coach﻿As﻿An﻿Intervention﻿ Engine . ﻿ Computer . ﻿doi : 10 . 1145 / 2330601 . 2330627 Noel , ﻿S . , ﻿ & ﻿Robert , ﻿J . - M . ﻿ ( 2004 ) . ﻿Empirical﻿Study﻿on﻿Collaborative﻿Writing . ﻿ Computer Supported Cooperative Work , ﻿ 13 ( 1 ) , ﻿63 – 89 . ﻿doi : 10 . 1023 / B : COSU . 0000014876 . 96003 . be Popescu , ﻿E . , ﻿Maria , ﻿C . , ﻿ & ﻿Loredana , ﻿A . ﻿ ( 2014 ) . ﻿Fostering﻿Collaborative﻿Learning﻿with﻿Wikis : ﻿Extending﻿ MediaWiki﻿with﻿Educational﻿Features . ﻿ Proceedings of the International Conference on Web - Based Learning 2014 ﻿ ( pp . ﻿22 – 31 ) . ﻿Springer . Posner , ﻿I . ﻿R . , ﻿ & ﻿Baecker , ﻿R . ﻿M . ﻿ ( 1992 ) . ﻿How﻿People﻿Write﻿Together . ﻿ Proceedings of the Twenty - Fifth Annual Hawaii International Conference on System Sciences ﻿ ( pp . ﻿127 – 138 ) . ﻿doi : 10 . 1109 / HICSS . 1992 . 183420 Rivera - Pelayo , ﻿V . , ﻿Munk , ﻿J . , ﻿Zacharias , ﻿V . , ﻿ & ﻿Braun , ﻿S . ﻿ ( 2013 ) . ﻿Live﻿interest﻿meter : ﻿learning﻿from﻿quantified﻿ feedback﻿in﻿mass﻿lectures . ﻿ Proceedings of the ﻿ Third International Conference on Learning Analytics and Knowledge , ﻿Leuven , ﻿Belgium﻿ ( pp . ﻿23 – 27 ) . ﻿doi : 10 . 1145 / 2460296 . 2460302 Roberts , ﻿T . ﻿S . , ﻿ & ﻿McInnerney , ﻿J . ﻿M . ﻿ ( 2007 ) . ﻿Seven﻿Problems﻿of﻿Online﻿Group﻿Learning﻿ ( and﻿Their﻿Solutions ) . ﻿ Educ . Technol . Soc . , ﻿ 10 , ﻿257 – 268 . Sharples , ﻿M . ﻿ ( 1993 ) . ﻿Adding﻿a﻿Little﻿Structure﻿to﻿Collaborative﻿Writing . ﻿In﻿D . ﻿Diaper﻿ & ﻿C . ﻿Sanger﻿ ( Eds . ) , ﻿ CSCW in Practice : An Introduction and Case Studies ﻿ ( pp . ﻿51 – 67 ) . ﻿London , ﻿UK : ﻿Springer - Verlag . ﻿doi : 10 . 1007 / 978 - 1 - 4471 - 2009 - 4 _ 5 Southavilay , ﻿V . , ﻿Yacef , ﻿K . , ﻿Reimann , ﻿P . , ﻿ & ﻿Calvo , ﻿R . ﻿A . ﻿ ( 2013 ) . ﻿Analysis﻿of﻿Collaborative﻿Writing﻿Processes﻿ Using﻿Revision﻿Maps﻿and﻿Probabilistic﻿Topic﻿Models . ﻿ Proceedings of the ﻿ 3rd ACM International Conference on Learning Analytics and Knowledge ﻿ ( pp . ﻿38 – 47 ) . ﻿doi : 10 . 1145 / 2460296 . 2460307 Storch , ﻿N . ﻿ ( 2002 ) . ﻿Patterns﻿of﻿Interaction﻿in﻿ESL﻿Pair﻿Work . ﻿ Language Learning , ﻿ 52 ( 1 ) , ﻿119 – 158 . ﻿ doi : 10 . 1111 / 1467 - 9922 . 00179 Stromqvist , ﻿S . , ﻿ & ﻿Malmsten , ﻿L . ﻿ ( 1998 ) . ﻿ScriptLog﻿Pro﻿1 . 04 . Tanes , ﻿Z . , ﻿Arnold , ﻿K . ﻿E . , ﻿King , ﻿A . ﻿S . , ﻿ & ﻿Remnet , ﻿M . ﻿A . ﻿ ( 2011 ) . ﻿Using﻿Signals﻿for﻿appropriate﻿feedback : ﻿Perceptions﻿ and﻿practices . ﻿ Computers & Education , ﻿ 57 ( 4 ) , ﻿2414 – 2422 . ﻿doi : 10 . 1016 / j . compedu . 2011 . 05 . 016 Theis , ﻿J . , ﻿Wells , ﻿K . , ﻿Schmidt , ﻿J . , ﻿ & ﻿Klaetsch , ﻿L . ﻿ ( 2010 ) . ﻿Changeset﻿Protocol . ﻿ PolicyPad Doc . ﻿Retrieved﻿from﻿ http : / / policypad . readthedocs . org / en / latest / changesets . html Verbert , ﻿K . , ﻿Duval , ﻿E . , ﻿Klerkx , ﻿J . , ﻿Govaerts , ﻿S . , ﻿ & ﻿Santos , ﻿J . ﻿L . ﻿ ( 2013 ) . ﻿Learning﻿Analytics﻿Dashboard﻿Applications . ﻿ The American Behavioral Scientist , ﻿ 57 ( 10 ) , ﻿1500 – 1509 . ﻿doi : 10 . 1177 / 0002764213479363 Verpoorten , ﻿D . , ﻿Westera , ﻿W . , ﻿ & ﻿Specht , ﻿M . ﻿ ( 2011 ) . ﻿A﻿first﻿approach﻿to﻿“Learning﻿Dashboards . ”﻿ Proceedings of the ﻿ 1st International Workshop on Enhancing Learning with Ambient Displays and Visualization Techniques . Waddington , ﻿R . ﻿J . , ﻿ & ﻿Teasley , ﻿S . ﻿D . ﻿ ( 2016 ) . ﻿Improving﻿Early﻿Warning﻿Systems﻿with﻿Categorized﻿Course﻿Resource﻿ Usage . ﻿ J . Learn . Anal . , ﻿ 3 ( 3 ) , ﻿263 – 290 . ﻿doi : 10 . 18608 / jla . 2016 . 33 . 13 Wheeler , ﻿S . , ﻿Yeomans , ﻿P . , ﻿ & ﻿Wheeler , ﻿D . ﻿ ( 2008 ) . ﻿The﻿good , ﻿the﻿bad﻿and﻿the﻿wiki : ﻿Evaluating﻿student - generated﻿ content﻿for﻿collaborative﻿learning . ﻿ British Journal of Educational Technology , ﻿ 39 ( 6 ) , ﻿987 – 995 . ﻿doi : 10 . 1111 / j . 1467 - 8535 . 2007 . 00799 . x Zhang , ﻿F . , ﻿ & ﻿Litman , ﻿D . ﻿J . ﻿ ( 2014 ) . ﻿Sentence - level﻿Rewriting﻿Detection . ﻿ Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational Applications . ﻿Baltimore , ﻿Maryland : ﻿ACL . ﻿doi : 10 . 3115 / v1 / W14 - 1818 International Journal of Distance Education Technologies Volume 15 • Issue 4 • October - December 2017 98 Ming Liu is Associate Professor at the School of Computer and Information Science , Southwest Universi - ty , China . He received the PhD in Artificial Intelligence in Education at the School of Electrical and Information Engineering , The University of Sydney , Australia in 2012 . His main research interests include learning analytics and intelligent tutoring system . He participated in national and international projects funded by ARC Linkage ( Aus - tralia ) , Young and Well CRC , Office of Teaching and Learning , Google and Chinese National Fund . He is an author of over 14 publications papers in prestigious conferences and journals , such as Intelligent Tutoring Systems , IEEE transactions on Learning Technologies , Journal of Educational Technology and Society . Abelardo Pardo is Associate Professor in the School of Electrical and Information Engineering at The University of Sydney , Australia . He is the director of the Learning and Affect Technologies Engineering Research Laboratory . His research interests include the design and deployment of technology to increase the understanding and improve digital learning experiences . More specifically , his work examines the areas of learning analytics , personalized active learning , and technology for student support . Li Liu is associate professor at Chongqing University . He is also serving as a Senior Research Fellow of School of Computing at the National University of Singapore . Li received his Ph . D . in Computer Science from the Univer - sité Paris - sud XI in 2008 . He had served as an associate professor at Lanzhou University in China . His research interests are in pattern recognition , data analysis , and their applications on human behaviors . He aims to contribute in interdisciplinary research of computer science and human related disciplines . Li has published widely in conferences and journals with more than 60 peer - reviewed publications . Li has been the Principal Investigator of several funded projects from government and industry .