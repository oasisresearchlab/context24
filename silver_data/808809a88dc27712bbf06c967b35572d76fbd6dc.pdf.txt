Unboxing Default Argument Breaking Changes in Scikit Learn Jo˜ao Eduardo Montandon ∗ , Luciana Lourdes Silva † , Cristiano Politowski ‡ , Ghizlane El Boussaidi ‡ and Marco Tulio Valente ∗ ∗ Universidade Federal de Minas Gerais , Belo Horizonte , Brazil jemaf @ ufmg . br , mtov @ dcc . ufmg . br † Instituto Federal de Minas Gerais , Ouro Branco , Brazil luciana . lourdes . silva @ ifmg . edu . br ‡ ´Ecole de Technologie Sup´erieure , Montr´eal , Canada cristiano . politowski @ etsmtl . ca , ghizlane . elboussaidi @ etsmtl . ca Abstract —Machine Learning ( ML ) has revolutionized the ﬁeld of computer software development , enabling data - based predic - tions and decision - making across several domains . Following modern software development practices , developers use third - party libraries—e . g . , Scikit Learn , TensorFlow , and PyTorch—to integrate ML - based functionalities into their applications . Due to the complexity inherent in ML techniques , the models available in the APIs of these tools often require an extensive list of arguments to be set up . Library maintainers overcome this issue by deﬁning default values for most of these arguments so developers can use ML models in their client applications effortlessly . By relying on these default arguments , the clients inadvertently depend on the value deﬁned in these parameters to keep running as expected . We interpret this problem as a semantical breaking change variant , which we named Default Argument Breaking Change ( DABC ) . In this work , we leverage 77 DABCs in Scikit Learn—a well - known ML library—and investigate how 194K client applications are vulnerable to them . Our results show that 72 DABCs ( 93 % ) are responsible for exposing 67 , 747 clients ( 35 % ) . We also detected that most DABCs ( 61 , 79 % ) involve APIs used in ML model training and model evaluation stages . Finally , we discuss the importance of managing DABCs in third - party ML libraries and provide insights for developers to mitigate the potential impact of these changes in their applications . I . I NTRODUCTION Machine Learning ( ML ) has changed the landscape of creating computer software in the last decade . Thanks to their ability to make data - based predictions , ML algorithms are integrated into software systems assisting decision - making in several domains like image recognition , cybersecurity , and fraud detection applications [ 1 ] – [ 5 ] . These algorithms now recommend whom to follow in our social network , ﬁlter spam in our e - mail inboxes , and approve our credit score . Following the practices of modern software development , software developers also depend on third - party components to empower their applications with ML - based functionalities [ 6 ] . In this context , the Python ecosystem stands out from others due to the machine learning libraries available [ 7 ] – [ 9 ] , like Scikit Learn , 1 TensorFlow , 2 and PyTorch . 3 1 https : / / scikit - learn . org / 2 https : / / www . tensorﬂow . org / 3 https : / / pytorch . org / These tools provide comprehensive APIs for developers interested in reusing their implemented models . Due to the characteristics of machine learning , the models available in these APIs often require an extensive list of arguments to be set up [ 9 ] . Considering the Scikit Learn API as an example , the constructor of SVC 4 —a widely used classiﬁer based on SVM—provides 14 arguments to be deﬁned . Many of these arguments are rather speciﬁc to the model being reused , e . g . , the kernel type to be used in the classiﬁer ( argument kernel ) . Fortunately , the maintainers deﬁned a set of default values for such arguments so developers can effortlessly use these classiﬁers . In practice , developers can create a SVC model from scratch without passing any speciﬁc value . On the other hand , relying on these default arguments increases the coupling between the library and its client ap - plications ; now clients depend not only on the method syntax provided in the API but also on the values assigned to the arguments to keep running as expected . Consequently , clients’ behaviour can be affected by just changing the values of these arguments . For instance , Scikit Learn maintainers changed the Kernel coefﬁcient formula used by default in SVC — gamma argument—from " auto " to " scale " between versions 0 . 21 and 0 . 22 , which can drastically change the model’s results . The lack of backward compatibility between library versions is known as breaking changes [ 10 ] – [ 13 ] . Most studies deal with breaking changes from a syntactical perspective [ 10 ] , [ 13 ] . On the other hand , breaking changes can also encom - pass semantical modiﬁcations , i . e . , they may change library behaviour , but clients are not syntactically broken [ 11 ] , [ 14 ] . We interpret this problem as a Semantical Breaking Change variant , which we named D EFAULT A RGUMENT B REAKING C HANGE ( DABC ) . To the best of our knowledge , we did not ﬁnd any study investigating this type of breaking change . D EFAULT A RGUMENT B REAKING C HANGE s play a partic - ular role in the context of machine learning tools . First , most components provided by these libraries are hard to inspect , i . e . , machine learning engineers may spend signiﬁcant effort debugging why a given model returned a given result [ 15 ] . 4 https : / / scikit - learn . org / stable / modules / generated / sklearn . svm . SVC . html 209 2023 IEEE 23rd International Working Conference on Source Code Analysis and Manipulation ( SCAM ) DOI 10 . 1109 / SCAM59687 . 2023 . 00030 2023 I EEE 23 r d I n t e r n a ti on a l W o r k i ng C on f e r e n ce o n S ou r ce C od e A n a l y s i s a nd M a n i pu l a ti on ( S C A M ) | 979 - 8 - 3503 - 0506 - 7 / 23 / $ 3 1 . 00 © 2023 I EEE | DO I : 10 . 1109 / S C A M 59687 . 2023 . 00030 979 - 8 - 3503 - 0506 - 7 / 23 / $ 31 . 00 ©2023 IEEE Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . DABCs add another difﬁculty layer to this process as a subtle change in a model’s argument value may drastically modify its outcome . Second , numerous Jupyter Notebooks—the tool of choice for many data scientists experimenting with their ML models 5 —lack conﬁguration ﬁles declaring their module dependencies [ 16 ] , [ 17 ] . A recent study by Pimentel et al . [ 16 ] indicates that less than 14 % of public notebooks declare some dependency ﬁle , e . g . , requirements . txt . In other words , most notebooks rely on the latest default argument value assigned in their models . In this paper , we study the occurrence of DABCs in Scikit Learn , one of Python’s most used machine learning libraries , and their potential impact on client applications . We manually analyze the changes made to the default values of the arguments in Scikit Learn functions , reported in the ofﬁcial documentation , to leverage the main characteristics of DABCs . We then investigate the likely impact that DABCs have on client applications by analyzing the use of Scikit Learn functions in 194 , 099 Jupyter Notebooks , publicly available on GitHub . We propose four research questions : RQ . 1 : What are the Most Common DABCs ? In total , we identiﬁed 77 DABCs declared in S CIKIT - L EARN . From these , 56 DABCs point to class constructors , e . g . , SVC . _ _ init _ _ ( ) . This ﬁnding suggests that developers should pay particular attention when initializing and conﬁguring their models . The cv argument is the most redeﬁned one in 20 DABCs . This argument deﬁnes strategies to split data during models’ training and validation . RQ . 2 : In which Version the DABCs were Introduced ? We identiﬁed DABCs in eight major versions . Version 0 . 22 stands out with 43 occurrences , followed by 0 . 20 and 1 . 1 with 11 each ; these three versions concentrate 84 % of all DABCs . The changes in these versions are related to S CIKIT - L EARN popular features , including cross - validation training , tree - based model setup , and parallel processing . RQ . 3 : In which Modules the DABCs were Introduced ? The majority of DABCs were reported on Model Training and Model Evaluation APIs ; together , these modules hold 61 out of 77 DABC ( 79 % ) . This suggests that ML models are in the spotlight regarding DABCs . RQ . 4 : How Clients are Vulnerable to DABCs ? Overall , 67 , 747 out of 194 , 099 ( 35 % ) client applications are vulnerable to one DABC at least . These calls covered 72 out of 77 DABCs ( 93 % ) identiﬁed in RQ . 1 . The presence of DABCs does not correlate with other software metrics , such as LOC , function coupling , and cyclomatic complexity . We also observe that two - thirds of all vulnerable calls are affected by DABCs introduced from version 0 . 22 onwards . The most vulnerable calls occur during the models’ training and evaluation stages concerning the machine learning pipeline modules . We summarize the contributions of this paper as follows . • We characterize an unexplored behaviour - breaking change focused on changes performed in API arguments , 5 https : / / netﬂixtechblog . com / notebook - innovation - 591ee3221233 called D EFAULT A RGUMENT B REAKING C HANGE ( DABC ) . • We leverage the characteristics of DABCs in Scikit Learn , a well - known machine learning library in Python , and measured how real - world client applications are exposed to them . • Finally , we discuss strategies that client developers should adopt to avoid being affected by DABCs . This paper is organized as follows . Section II deﬁnes in detail what a D EFAULT A RGUMENT B REAKING C HANGE is , and how it can make client applications vulnerable . Sections III and IV describe the procedure we adopted to collect and identify the DABCs in Scikit Learn and how to map their occurrences in clients . The obtained results are described in Section V . Section VI reports the implications of this work . Section VII reports threats to validity , and Section VIII summarizes the related work . Finally , we conclude this paper in Section IX . II . B ACKGROUND A . Default Arguments in a Nutshell The Python language supports functions that , once imple - mented , can be called with only some arguments . For instance , the round ( number , digits ) function 6 has two param - eters 7 and returns the number rounded to digits decimal places . It turns out that round ( ) can be called with both one and two arguments ; if digits is omitted , the function rounds number to its nearest integer value . This means that calling round ( 3 . 1415 , 2 ) returns 3 . 14 , while calling round ( 3 . 1415 ) gives 3 . 0 . Such behavior is possible due to Default Arguments , 8 which specify values that functions will use if the caller provides no value to its corresponding argument . In the above example , the function automatically assumed digits = 0 when invoking round ( 3 . 1415 ) . One assigns a Default Argument during function deﬁnition by attributing an arbitrary value to the arguments the developer wants to become optional , as shown in Figure 1 . In this scenario , executing sum ( 10 , 20 ) will return 30 ( a = 10 , b = 20 ) , sum ( 10 ) will return 10 ( a = 10 , b = 0 ) , and calling sum ( ) will return 0 ( a = 0 , b = 0 ) . 1 def sum ( a = 0 , b = 0 ) : 2 return a + b Fig . 1 . Function deﬁnition with Default Argument Values . Default Arguments are a powerful resource as they mimic method overloading—methods with the same name but dif - ferent parameters—which is not supported in Python by default [ 18 ] . This concept promotes ﬂexibility , readability , and reusability to classes interface ; relevant to successful APIs [ 19 ] , [ 20 ] . 6 https : / / docs . python . org / 3 / library / functions . html # round 7 We use arguments and parameters interchangeably in this paper . 8 https : / / docs . python . org / 3 / tutorial / controlﬂow . html # default - argument - values 210 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . B . The S CIKIT - L EARN Library S CIKIT - L EARN is a free and open - source machine learning library for Python , released in 2011 [ 21 ] . The library im - plements well - known supervised and unsupervised machine learning algorithms , such as linear and logistic regressions , support vector machines , decision trees , and k - means cluster - ing . The library also provides techniques to manage , evaluate , and deploy the above - mentioned models . Such features contributed to its adoption in several industry and research projects . S CIKIT - L EARN is one of the most popular machine learning libraries worldwide , with more than 1 million downloads daily . 9 As of Jan 24th , 2023 , the S CIKIT - L EARN repository on GitHub has more than 52 , 7K stars , almost 30K commits , and was forked above 23 , 9K times . Despite its wide adoption , S CIKIT - L EARN ’s API is constantly changing . For instance , version 1 . 0 was released in September 2021 ; the maintainers followed with eight new versions since then . This scenario may challenge the developers whose client applications depend on S CIKIT - L EARN features . Default Arguments in S CIKIT - L EARN : S CIKIT - L EARN ’s API extensively relies on Default Arguments so users can set up the models available in the library with low effort . For example , the SVC class provides 14 parameters to be deﬁned through its constructor . 10 These arguments are responsible for conﬁguring several aspects of a SVC model , including the Kernel type used by the model ( kernel ) , its Kernel coefﬁcient ( gamma ) , random seed values ( random _ state ) , etc . Since all arguments have default values assigned , the user can quickly get started without deﬁning any parameter to the model . Figure 2 exempliﬁes this fact when creating a SVC model . Except for random _ state , all arguments rely on default values ; kernel was deﬁned to " rbf " , and gamma was assigned to " scale " . 11 1 from sklearn . svm import SVC 2 3 clf = SVC ( random _ state = 42 ) 4 clf . fit ( X _ train , y _ train ) Fig . 2 . Default Argument Values in action in S CIKIT - L EARN . C . What is a Default Argument Breaking Change ( DABC ) ? Despite the advantages of using Default Arguments , they might bring issues to client applications relying on them . Speciﬁcally , library maintainers can update the default values of some parameters to meet new conditions . Changes of this nature do not break clients’ code since the function signature ( name and arguments ) remains the same . Nevertheless , they might introduce incompatibilities as the new value change function’s behaviour . 9 According to https : / / pypistats . org / 10 https : / / scikit - learn . org / 1 . 1 / modules / generated / sklearn . svm . SVC . html 11 Default values available from version 1 . 1 . 2 , the latest stable one at the time of this work . For instance , S CIKIT - L EARN maintainers updated the value of gamma argument of the SVC classiﬁer from " auto " to " scale " in version 0 . 22 . This update clearly affects models relying on this default value as it changes the math formula used to calculate the gamma value . Consequently , the code that creates and trains a SVC model in Figure 2 outputs very different results in versions 0 . 21 and 0 . 22 . D EFAULT A RGUMENT B REAKING C HANGE Example : We illustrate this maintenance problem by implementing the min - imum working example in Listing 3 . This example classiﬁes the 20 newsgroup dataset , a popular real - world collection containing 18 , 000 newsgroup posts grouped into 20 distinct topics . The Stanford Natural Language Processing Group collected this dataset over several years , and it has become a popular alternative for experiments in text applications of machine learning techniques . Currently , it has been used as a benchmark in popular research works [ 22 ] , [ 23 ] . 1 from sklearn import datasets 2 from sklearn . model _ selection import train _ test _ split 3 from sklearn . metrics import accuracy _ score 4 from sklearn . svm import SVC 5 6 # Load dataset 7 ds = datasets . fetch _ 20newsgroups _ vectorized ( ) 8 X = ds . data [ : , 2 : ] 9 y = ds . target 10 11 # Create training / test data split 12 X _ train , X _ test , 13 y _ train , y _ test = train _ test _ split ( X , y , 14 test _ size = 0 . 3 , 15 random _ state = 42 , 16 stratify = y ) 17 18 # Create an instance of SVC Classifier 19 clf = SVC ( random _ state = 42 ) 20 21 # Fit , predict , and measure model ' s performance 22 clf . fit ( X _ train , y _ train ) 23 y _ pred = clf . predict ( X _ test ) 24 print ( ' Acc : % . 3f ' % accuracy _ score ( y _ test , y _ pred ) ) Fig . 3 . Illustrative example of DABC in S CIKIT - L EARN . In this script , we ﬁrst download the 20 newsgroup dataset and obtain their descriptive and predictive variables ( lines 7 – 9 ) . Next , as with typical ML applications , we split the data into training and test groups in lines 11 – 16 . In line 19 , we create a new SVC instance ; we intentionally did not deﬁne any argument except for random _ state to ensure the same randomness will be present in any execution . Finally , lines 21 – 24 ﬁt the model with the training data , predict it with test data , and measure its accuracy level . We execute this script using S CIKIT - L EARN in both ver - sions 0 . 21 ( gamma = " auto " ) and 0 . 22 ( gamma = " scale " ) . In version 0 . 21 , we scored 0 . 05 points for accuracy . The same script reached 0 . 82 points for accuracy in version 0 . 22 , a difference of 77 points . This issue encompasses a speciﬁc type of breaking change , which we named D EFAULT A RGUMENT B REAKING C HANGE 211 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . ( DABC ) . This paper focuses on characterizing DABCs in the S CIKIT - L EARN library and measuring its impact on client applications . III . D ATA C OLLECTION The S CIKIT - L EARN project adopts a strict contribution guide to enforce better software practices , with source code documentation conventions included . 12 The documentation section provides speciﬁc instructions to report changes in “ the default value of a parameter ” . According to the guide - line , every modiﬁcation involving the value of an argument should have its docstring’s documentation annotated with the versionchanged directive . Also , the old and new default values should be reported together with the version the change became effective . Figure 4 presents an example of this de - scription in S CIKIT - L EARN project . In this case , the parameter gamma —which belongs to the constructor of SVC class—had its value changed from " auto " to " scale " , valid from version 0 . 22 onward . We relied on this guideline to collect the D EFAULT A RGUMENT B REAKING C HANGE s studied in this work . class SVC ( BaseSVC ) : " " " C - Support Vector Classification . . . . Parameters - - - - - - - - - - . . . gamma : { ' scale ' , ' auto ' } or float , default = ' scale ' Kernel coefficient for ' rbf ' , ' poly ' and ' sigmoid ' . - if ` ` gamma = ' scale ' ` ` ( default ) . . . - if ' auto ' , . . . - if float , . . . . . versionchanged : : 0 . 22 The default value of ` ` gamma ` ` changed from ' auto ' to ' scale ' . " " " Fig . 4 . Example of a default argument changed in S CIKIT - L EARN documen - tation . Mining Changes on Functions Arguments : On October 11th , 2022 , we cloned the S CIKIT - L EARN project from GitHub , 13 and manually checked out the commit of version 1 . 1 . 2 ; the latest public release available . Next , we selected all Python ﬁles in the sklearn directory , as the library’s source ﬁles are located in this directory . We then opened each Python ﬁle and ﬁltered out the lines containing the versionchanged directive ; we did this using the regular expression “ \ . \ . versionchanged : : . + ” . This procedure initially returned 179 occurrences . Selected Attributes : For each occurrence , we collected a list of ﬁve attributes used to answer the research questions proposed in this paper . We listed these attributes below : 12 The contribution guide can be accessed at https : / / scikit - learn . org / stable / developers / contributing . html . 13 Repository available at https : / / github . com / scikit - learn / scikit - learn / . • dabc msg : This attribute contains the message used to justify each occurrence . We collected this information manually for each occurrence during the categorization we did to answer RQ . 1 . • version : This attribute keeps the version assigned to each occurrence as collected by the regex matching procedure . We used this information to answer RQ . 2 . • path : Collected during the repository analysis , this at - tribute holds the relative path from the scikit directory to the ﬁle containing the DABC . We used this attribute to leverage the modules to answer RQ . 3 . • fqn : This attribute holds the full qualiﬁed name of the function where the occurrence was found , i . e . , class name followed by the method signature . Similar to dabc msg , we manually leveraged this information to answer RQ . 4 . • dabc url : We generate the GitHub URL referring to the exact point in the source code where the DABC was declared . We refer to this URL whenever we need more context about the DABC to answer the research questions . IV . R ESEARCH Q UESTIONS In this section , we describe the methodology steps to answer the four research questions proposed in this study . A . RQ . 1 : What are the Most Common DABCs ? This RQ aims to identify the D EFAULT A RGUMENT B REAKING C HANGE s from the changes in arguments re - trieved in Section III . Due to the number of occurrences retrieved , three authors manually inspected and discussed together all 179 occurrences , selecting the ones they con - sidered valid . For this , they perform a two - step ﬁltering approach . First , the three authors read the description below each versionchanged to select the occurrences dealing only with arguments , i . e . , they ﬁltered out unrelated changes . The authors discarded 46 occurrences in this step reporting other changes , such as return values , object attributes , function refactorings , etc . Then , they analyzed each of the remaining 133 occurrences in detail to verify which of the changes can be characterized as D EFAULT A RGUMENT B REAKING C HANGE . They discarded another 56 occurrences performing other changes in argument values , such as type changes ( e . g . , the min _ samples _ split parameter started accepting ﬂoat val - ues in version 0 . 18 ) , 14 and in other values that can be passed to the parameter besides the default one ( e . g . , the metric argument no longer accepts a speciﬁc value in version 0 . 19 ) . 15 After removing these cases , we remained with 77 D EFAULT A RGUMENT B REAKING C HANGE s . B . RQ . 2 : In which Version the DABCs were Introduced ? This RQ aims to discover in which part of the release cycle these changes are introduced . To do so , we analyzed the distribution of DABCs among the S CIKIT - L EARN ’s release 14 https : / / scikit - learn . org / stable / modules / generated / sklearn . ensemble . RandomForestClassiﬁer . html 15 https : / / scikit - learn . org / stable / modules / generated / sklearn . neighbors . NearestCentroid . html 212 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Scikitdataset Calls Extraction ExternalCalls C { . . } Matroskin DABC Matching source code 3 4 M S DABCs D Grotov ' s Dataset 1 FilteringScikit - Learn Clients 2 DABC Calls 5 Fig . 5 . The data - collection pipeline adopted to answer RQ . 4 . Steps annotated in black ( e . g . , 1 ) are described in detail , while steps in gray ( e . g . , S ) are mentioned during Section IV - D . versions . As described in S CIKIT - L EARN documentation , 16 the project maintainers nominate its releases based on PEP101 . This speciﬁcation describes the library’s versions using the X . Y . Z triplet . Minor versions are tracked by the . Z sufﬁx and should include bug ﬁxes and some relevant documentation changes only , i . e . , they should not contain a behaviour change besides a bug ﬁx . On the other hand , major versions indicating new releases are annotated with the X . Y preﬁx ; these versions can contain new features and signiﬁcant maintenance that modify the library behaviour . To answer this question , we leveraged all versions contain - ing the X . Y . Z syntax released before version 1 . 1 . 2 from the git tags available in the S CIKIT - L EARN git repository . We then collected the date of the commits responsible for creating these tags and annotated them as major or minor according to the S CIKIT - L EARN convention described above . We identiﬁed 56 versions released in a 12 - year period , where 26 are major and 30 are minor versions . As the last step , we combined the release information with the version ﬁeld of each DABC identiﬁed in Section III . For example , the DABC present in Figure 4 matches with version 0 . 22 , so we considered the maintainers introduced this DABC in a major release on 2019 - 12 - 02 . C . RQ . 3 : In which Modules the DABCs were Introduced ? This RQ aims to investigate in which modules of S CIKIT - L EARN DABCs are located . We named nine modules that comprise the phases of Machine Learning [ 24 ] . For each commit , we manually inspected the changed source code ﬁles using path and dabc url ( described in Section III ) . Then , these changes were labelled into one of the following modules : • Dataset contains utilities to handle large datasets ( e . g . , functions to download and load data ) and traditional datasets ( e . g . , load and get data from a public repository ) . • Data preprocessing comprises utility functions and trans - formation techniques to apply on raw features for stan - dardizing datasets . • Data Decomposition consists of functions that implement dimensionality reduction or feature selection techniques to apply to the dataset . 16 https : / / scikit - learn . org / stable / developers / maintainer . html # releasing • Data Analysis contains the implementation of statistical techniques to support the understanding of data process . • Feature Processing includes techniques to transform arbi - trary data into usable data supported by Machine Learn - ing algorithms . • Model Training consists of algorithms’ implementations for unsupervised and supervised learning methods . • Model Evaluation contains several techniques to measure the estimator performance and evaluate the model predic - tions’ quality . • Utils comprises several utilities , such as estimate class weights for unbalanced datasets . • Pipeline consists of utilities to build a composite esti - mator . We followed S CIKIT - L EARN ’s documentation to label this module . Finally , we assigned all changes that are not directly related to ML tasks to Others . In this study , we observed that these changes are related to exception handlers . D . RQ . 4 : How Clients are Vulnerable to DABCs ? In this question , we investigate the potential impact that D EFAULT A RGUMENT B REAKING C HANGE s have on client applications that use the S CIKIT - L EARN library . For this , we implemented a data - collection pipeline that obtains a list of real - world client applications that use S CIKIT - L EARN library , extracts the method calls performed in these clients , and selects the calls vulnerable to DABCs in S CIKIT - L EARN . Figure 5 depicts this procedure ; more details about each step are described in the remainder of this section . 1 Clients Dataset : We studied some datasets containing S CIKIT - L EARN clients and data collection strategies to identify the best ﬁt for our needs [ 8 ] , [ 9 ] , [ 16 ] , [ 17 ] , [ 25 ] . We consider the dataset’s size , available documentation , and the effort to replicate and adapt it to our context . We selected Grotov et al . ’s dataset [ 8 ] due to two reasons : ( a ) it comes in a structured format that can be queried using SQL ; and ( b ) the authors also provide a tool that analyzes the source code of Jupyter Notebook and Python scripts , called Matroskin . This dataset contains 847 , 881 preprocessed Jupyter Notebooks written in Python , extracted from GitHub between September and October 2020 . 213 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . TABLE I T HE TOP 10 MOST REFERRED ARGUMENTS IN COLLECTED DABC S . Changed Argument DABCs # % cv 20 26 . 0 n jobs 8 10 . 4 max features 6 7 . 8 gamma 5 6 . 5 n estimators 5 6 . 5 n splits 4 5 . 2 init 4 5 . 2 multi class 3 3 . 9 return train score 3 3 . 9 algorithm 2 2 . 6 Total 60 77 . 9 TABLE II DABC S FOR EACH VERSION . Version DABCs # % 0 . 19 3 3 . 9 0 . 20 11 14 . 3 0 . 21 3 3 . 9 0 . 22 43 55 . 8 0 . 23 3 3 . 9 0 . 24 2 2 . 6 1 . 0 1 1 . 3 1 . 1 11 14 . 3 Total 77 100 TABLE III DABC S FOR EACH MODULE . Modules DABCs # % Dataset 1 1 . 3 Data preprocessing 2 2 . 6 Data Decomposition 5 6 . 5 Data Analysis 2 2 . 6 Feature Processing 1 1 . 3 Model Training 42 54 . 5 Model Evaluation 19 24 . 7 Utils 1 1 . 3 Pipeline 3 3 . 9 Others 1 1 . 3 Total 77 100 2 Filtering S CIKIT - L EARN Clients : We selected all notebooks relying on the S CIKIT - L EARN library from the initially obtained dataset . Speciﬁcally , the preprocessed database contains the list of imported modules for each client notebook . We then queried the database for all notebooks containing the “sklearn” string—the name of S CIKIT - L EARN module—in their import list . In total , 194 , 099 notebooks met this criterion ( S ) . 3 Calls Extraction : In this step , we extracted all existing method calls performed in each notebook . However—unlike the list of imports—this information is not available by default in the dataset . Instead , the authors provide only the number of method calls that belong to external sources , i . e . , imported and third - party modules . To obtain the actual method calls , we instrumented Matroskin ( M ) to extract all external calls during its syntactical analysis and re - executed it on the notebooks of Scikit dataset ( S ) . We ended up with 17 , 436 , 073 external calls ( C ) extracted from the 194 , 099 clients . 4 DABC Matching : In the last step of this pipeline , we selected all method calls vulnerable to DABCs . Traditionally this could be achieved by tracking down the declaration of the called method , retrieving its arguments , and checking if the default argument is assigned in the call . However , it is not straightforward to infer this information since Python is a dynamically - typed language [ 7 ] , [ 8 ] , [ 26 ] . Due to this reason , we worked on a static matching heuristic to detect calls to methods identiﬁed as DABC . The heuristic works as follows . We ﬁrst parse 17 the deﬁnition of all 77 DABCs ( D ) identiﬁed in Section IV - A and extract their class name ( if any ) , method name , and list of deﬁned arguments . Next , for each external call ( C ) , we parse and extract its method name and list of argument values ; note that we can not directly obtain class names as Python is dynamically - typed . Then we match DABCs and calls based on two conditions : ( a ) the DABC class name—if it exists—is in the same ﬁle where the call was retrieved ; and ( b ) the method name in both DABC and in 17 We used the Python gast module at ( https : / / pypi . org / project / gast / ) . the call are the same . For each successful match , we pair the call’s argument values to the DABC’s deﬁned arguments by assigning all positional arguments in sequence and assigning all keywords arguments based on the key provided . Lastly , we check if the DABC’s default argument is assigned . The call is considered vulnerable if no value is assigned to the DABC argument . In practice , the call did not provide a value for it , so it relies on the DABC’s default argument value . 5 DABCs Calls Dataset : The DABC matching procedure identiﬁed 317 , 648 calls vulnerable to DABCs in 67 , 747 client applications . We test the effectiveness of this heuristic by manually analyzing a randomly selected sample of 384 calls , equally divided among three authors . 18 They veriﬁed whether both method’s call and DABC point to the same function and if the call is vulnerable . To ensure the authors followed a similar veriﬁcation pattern , they analyzed 38 calls together ( 10 % of the sample size ) . In their evaluations , the authors identiﬁed 366 calls ( 95 . 3 % , ± 5 % ) as valid ones . From the remaining 18 , the heuristic mostly fail at detecting arguments outside the function call ; e . g . , arguments that were passed inside a Python dictionary , instead . Such issue is beyond identiﬁcation in static analysis , hence out of scope in our heuristic . V . R ESULTS A . RQ . 1 : What are the Most Common DABCs ? The 77 DABCs are spread over 61 distinct methods . From these , 19 methods are declared outside of any class— e . g . , cross _ validate ( ) , k _ means ( ) , etc—and account for 21 DABCs . All 42 remaining methods declared in classes are constructors ; they are responsible for 56 DABCs . This ﬁnding emphasizes the central role that method constructors play when conﬁguring S CIKIT - L EARN models . Individually , the methods GridSearchCV . _ _ init _ _ ( ) and RandomizedSearchCV . _ _ init _ _ ( ) lead the rank of DABCs containing three occurrences , each . Both GridSearchCV and RandomizedSearchCV classes im - plement strategies for optimizing ML models . Moreover , both methods are vulnerable to changes performed in the same 18 The sample size was determined considering 95 % conﬁdence level and 5 % conﬁdence interval . 214 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . arguments : ( i ) cv deﬁnes the cross - validation strategy ; ( ii ) n _ jobs determines the number of jobs to run in parallel ; and ( iii ) return _ train _ score determines if the method returns the computed training scores . Twelve other methods have two DABCs each . The remaining 47 methods show up with one DABC only . We also analyzed the distribution of DABCs among the changed arguments . In total , 24 arguments had their default value changed by at least one DABC . Table I lists the top 10 most modiﬁed ones . The cv argument stands out with 20 occurrences ( 26 % ) . This argument deﬁnes the cross - validation strategy to split the data during model training and validation . It is widely used in S CIKIT - L EARN , as most supervised models rely on data - splitting techniques when they are trained . Similarly , n _ jobs ( 8 occurrences , 10 . 4 % ) is also adopted in different scenarios . The remaining arguments belong to speciﬁc classes and models . For instance , max _ features ( 6 occurrences , 7 . 8 % ) and n _ estimators ( 5 occurrences , 6 . 5 % ) conﬁgure tree - based models . The 77 DABCs are spread across 61 methods . 56 DABCs occurs in class constructors . cv —responsible for deﬁning cross - validation strategies in ML models—is the most modiﬁed argument present in 20 DABCs . B . RQ . 2 : In which Version the DABCs were Introduced ? Table II presents the distribution of DABCs among each S CIKIT - L EARN ’s release . They are distributed in eight ver - sions ; the ﬁrst ones appeared on version 0 . 19 , released in November 2017 . Since then , we have identiﬁed DABCs in all major releases , i . e . , no DABC was reported in minor versions . Three versions concentrate 65 occurrences , representing 84 . 4 % of all DABCs reported in this study . Speciﬁcally , version 0 . 22 stands out with 43 modiﬁcations in default arguments ( 55 . 8 % ) ; both versions 0 . 20 and 1 . 1 appear next with 11 ( 14 . 3 % ) DABCs . The ﬁve remaining versions gather 12 DABCs in total . We inspected in detail versions 0 . 22 , 0 . 20 , and 1 . 1 to better understand the reason for such disparity . We ﬁnd that the changes in both versions deal with S CIKIT - L EARN ’s popular features . For instance , 19 out of 43 occurrences in 0 . 22 deal with cv . Other ﬁve occurrences modify n _ estimators argument . Version 0 . 20 presents a similar characteristic , as eight occurrences point to the n _ jobs argument . Differently , DABCs are regularly distributed in version 1 . 1 with four distinct changes varying between two and three occurrences . We identiﬁed DABCs in all major versions since 0 . 19 ; no DABC was found in minor ones . Version 0 . 22 alone concentrates 43 out of 77 DABCs ( 56 % ) , followed by 0 . 20 and 1 . 1 , with 11 occurrences ( 14 % ) each ; together , these three versions gather 84 % of all DABCs . C . RQ . 3 : In which Modules the DABCs were Introduced ? Table III describes the classiﬁed modules . We observe that both Model Training and Model Evaluation stand out from the other modules , with 42 and 19 DABCs , respectively ; together , they represent more than 79 % of all identiﬁed DABCs . Note that the classes implemented in both modules deal directly with machine learning algorithms , hence they are at the core of most machine learning applications . The remaining modules contain ﬁve changes or fewer . Model Training and Model Evaluation are the ones with most DABCs , with 42 and 19 occurrences , respectively . D . RQ . 4 : How Clients are Vulnerable to DABCs ? TABLE IV M OST FREQUENT DABC S CALLS IN CLIENT APPLICATIONS . Class . Method ( Default Argument ) Calls # % LogisticRegression . init ( multi class ) 38 , 323 12 . 1 LogisticRegression . init ( solver ) 31 , 290 9 . 9 RandomForestClassiﬁer . init ( max features ) 30 , 874 9 . 7 SVC . init ( decision funciton shape ) 29 , 904 9 . 4 GridSearchCV . init ( return train score ) 24 , 930 7 . 8 SVC . init ( gamma ) 22 , 258 7 . 0 KMeans . init ( algorithm ) 22 , 063 6 . 9 r2 score ( multioutput ) 20 , 805 6 . 5 GridSearchCV . init ( n jobs ) 16 , 396 5 . 2 RandomForestRegressor . init ( max features ) 14 , 970 4 . 7 Total 251 , 813 79 . 2 1 ) What are the most frequent DABCs ? : We detected vul - nerable calls for 72 out of the 77 DABCs identiﬁed previously ( 93 % ) ; Table IV lists the most frequent DABCs identiﬁed in client applications . The top 10 gathered 251 , 813 of the 317 , 648 vulnerable calls ( 79 . 2 % ) , suggesting a heavy - tail distribution . While the 10th most frequent DABC contains 14 , 970 vulnerable calls , the median value is 365 . We observe that nine vulnerable calls refer to class constructors . This highlights a common practice in S CIKIT - L EARN where most conﬁguration arguments are passed when creating the model instance . The ﬁrst two calls in the table refer to the same method ( LogisticRegression . _ _ init _ _ ( ) ) , but with different argument values ( multi _ class and solver ) . Same behavior applies for SVC . _ _ init _ _ ( ) ( decision _ function _ shape and gamma ) and GridSearchCV . _ _ init _ _ ( ) ( return _ train _ score and n _ jobs ) methods . We also observe the same default ar - gument used in two distinct classes : max _ features is used when calling RandomForestClassifier . _ _ init _ _ ( ) and RandomForestRegressor . _ _ init _ _ ( ) methods . We detected vulnerable calls in clients for 93 % of DABCs identiﬁed previously . The top 10 most frequent DABCs accounted for almost 80 % of vulnerable calls , with nine 215 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . of them referring to S CIKIT - L EARN models initialization . 2 ) How many clients are vulnerable to DABCs ? : In total , 67 , 747 out of 194 , 099 clients are vulnerable to at least one DABC ( 35 % ) . Although each had to deal with 4 . 69 DABCs on average , we identiﬁed nine clients with more than 100 vulnerable calls . The 99 % , 95 % , and 50 % client percentiles respond to 30 , 16 , and 2 vulnerable calls , respectively . TABLE V S PEARMAN CORRELATION BETWEEN THE STRUCTURAL METRICS COLLECTED BY G ROTOV ET AL . [ 8 ] AND THE NUMBER OF CALLS VULNERABLE TO DABC S IN CLIENT APPLICATIONS . T HE BULLET SHAPE QUANTIFIES THE CORRELATION LEVEL : NEGLIGIBLE ( • ) , LOW ( •• ) , AND MODERATE ( • • • ) . Metric CorrelationCoeff . Level C ODE W RITING SLOC 0 . 38 •• Blank LOC 0 . 28 • Extended comments LOC 0 . 24 • Comments LOC 0 . 19 • F UNCTION U SAGE API functions ( count ) 0 . 50 • • • API functions ( unique ) 0 . 38 •• Other functions ( count ) 0 . 32 •• Built - in functions ( count ) 0 . 29 • Built - in functions ( unique ) 0 . 19 • User - deﬁned functions ( count ) 0 . 18 • User - deﬁned functions ( unique ) 0 . 15 • C OMPLEXITY Cell coupling 0 . 41 •• Function coupling 0 . 14 • NPAVG 0 . 13 • Cyclomatic complexity 0 . 09 • We triangulated the number of vulnerable calls with the 15 structural metrics collected by Grotov et al . [ 8 ] to verify how traditional software metrics relate to DABCs . This is a ﬁrst step towards understanding how software quality inﬂuences the emergence of DABCs . For this , we executed the Spearman correlation test between the number of calls and each metric separately . We opted for Spearman due to its robustness in interpreting non - normalized distributions [ 27 ] . Following the guidelines proposed in other works [ 6 ] , [ 26 ] , [ 28 ] , we interpret its coefﬁcient according to the following : 0 . 00 ≤ negligible < 0 . 30 ≤ low < 0 . 50 ≤ moderate < 0 . 70 ≤ high < 0 . 90 ≤ veryhigh < 1 . 00 . Table V presents the correlation results ; we marked in bold the top three metrics with higher correlation coefﬁcients . Overall , we did not identify any high correlation between the structural metrics and the number of vulnerable calls in client applications . On the contrary , the correlation levels of most metrics are either low ( four ) or negligible ( ten ) . Only API functions ( count ) presented a moderate correlation with the number of vulnerable calls ( 0 . 50 ) ; the correlation with API functions ( unique ) is lower , though ( 0 . 38 , low level ) . Complexity - based are independent to the number of DABCs calls : Cyclomatic complexity , Function coupling , and NPAVG scored the lowest correlation coefﬁcients with 0 . 09 , 0 . 13 , and 0 . 14 , respectively . These ﬁndings suggest that the presence Fig . 6 . Number of vulnerable calls in each version . of DABCs is relatively independent of the codebase size , functions usage , and complexity . 35 % of client applications are vulnerable to DABCs . Besides API functions ( count ) , we did not ﬁnd any substantial correlation between source code metrics and DABCs calls . 3 ) Which versions make clients more vulnerable ? : Figure 6 depicts the vulnerable calls in each S CIKIT - L EARN ver - sion . Versions 0 . 22 , 1 . 1 , and 0 . 19 stand out with 136 , 312 ( 42 . 9 % ) , 72 , 005 ( 22 . 7 % ) , and 51 , 004 ( 16 . 1 % ) vulnerable calls , respectively ; altogether , these versions concentrate 81 . 6 % of the vulnerable calls . By contrast , versions 0 . 23 , 1 . 0 , and 0 . 24 had the most negligible impact on clients with 10 ( < 0 . 01 % ) , 1 , 190 ( 0 . 37 % ) , and 1 , 510 ( 0 . 48 % ) calls . We observe that 23 % of vulnerable calls happen in more recent versions , i . e . , version 1 . 0 onwards . The proportion goes to 66 % when we extend this analysis to version 0 . 22 . In other words , two - thirds of all vulnerable calls are due to DABCs reported from versions 0 . 22 onwards . Versions 0 . 22 , 1 . 1 , and 0 . 19 had the highest impact on clients , accounting for 81 % of vulnerable calls ; versions 0 . 23 , 1 . 0 , and 0 . 24 had the least . Two - thirds of all vulnerable calls are due to DABCs reported from version 0 . 22 onwards . 4 ) Which ML modules are more vulnerable in clients ? : Table VI presents the number of vulnerable calls located in each ML module leveraged in Section IV - C . Model Training and Model Evaluation clearly stand out with 248 , 014 and 64 , 927 calls each ; both modules condense 98 . 5 % of all vul - nerable calls . Such higher concentration indicates that most DABCs show up when dealing with the machine learning models . On the other hand , no other module gathers more than 1 % of vulnerable calls ; Pipeline is the highest remaining one with 0 . 59 % . We also veriﬁed the modules that are vulnerable together , i . e . , in the same client . In this perspective , 16 , 233 ( 24 . 0 % ) clients are simultaneously vulnerable in two modules , 216 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . TABLE VI N UMBER OF VULNERABLE CALLS FOR EACH ML MODULE . Module Calls # % Data Analysis 32 0 . 01 Data Decomposition 1 , 067 0 . 33 Feature Processing 103 0 . 03 Model Evaluation 64 , 927 20 . 43 Model Training 248 , 014 78 . 07 Utils 173 0 . 05 Dataset 1 , 070 0 . 33 Pipeline 1 , 876 0 . 59 Preprocessing 392 0 . 12 1 , 204 ( 1 . 8 % ) in three , and 172 ( 0 . 2 % ) in four modules . DABCs located in Model Training and Model Evalua - tion are responsible for most vulnerable calls ( 98 . 5 % ) . From the clients’ perspective , 74 % are vulnerable in one module , only . VI . D ISCUSSION We understand that the ﬁndings reported in this paper unfold implications for researchers , library maintainers , and library users . We discuss them in the following subsections . A . Implications for Researchers DABCs are a reality . As presented in Section V - D , we found that more than one - third of client applications are exposed to DABCs . These clients have method calls covering 93 % of all DABCs leveraged in Section V - A . These results show that DABCs do exist , yet we did not ﬁnd studies investigating this particular type of breaking change before . In this context , we believe our work is a ﬁrst step towards a better understanding of DABCs in the ML ecosystem and beyond . We lack studies that investigate breaking changes in dynamically typed languages . Public APIs frequently rely on function overloading to promote ﬂexibility , readability , and reusability [ 19 ] , [ 20 ] . Although dynamically - typed languages do not support this technique natively , 19 we can still make use of it—calling the same function with a variadic number of parameters—by using default argument values . Consequently , we can say that languages like Python , JavaScript , and PHP are especially exposed to DABCs . Therefore , we understand that dynamically - typed languages should receive more attention in breaking changes studies [ 14 ] , [ 29 ] , as more issues speciﬁc to these languages may arise . We need to understand why library maintainers rely on default argument values . In this study , we describe what D EFAULT A RGUMENT B REAKING C HANGE s are , when and where they are introduced , as well as who is vulnerable to them . However , understanding why maintainers introduce such 19 For more details see https : / / softwareengineering . stackexchange . com / questions / 425422 / do - all - dynamically - typed - languages - not - support - function - overloading modiﬁcations remains open . We claim that investigating this aspect is paramount as it could reveal typical scenarios where these values are modiﬁed . Such ﬁndings can contribute to design recommendations and good practices for using default argument values . B . Implications for Library Maintainers DABCs present a ripple effect . As the results in Section V - A show , DABCs are concentrated on a few arguments . For example , the cv argument—responsible for deﬁning cross - validation strategies during models’ training and validation stages—is the pivot of 26 % of the DABCs we detected . This ﬁnding suggests that , as with traditional breaking changes , DABCs may lead to a ripple effect among the API interface—updating the default value of one argument might affect the behaviour of other functions in the API—ultimately impacting a much larger number of client applications [ 11 ] , [ 30 ] . We argue that library maintainers should be aware of this issue when updating the default values of their APIs . Maintainers should follow semantic versioning strategies . S CIKIT - L EARN maintainers look aware of the risks involved in changing default argument values . As we can observe in Section V - B , all DABCs were reported on the library’s major versions , hence complying with semantic versioning . This ﬁnding contradicts other results reported in the literature [ 12 ] , [ 13 ] . Although adopting this approach does not suppress the occurrence of DABCs on client applications—for example , clients might not rely on versioning strategies at all [ 16 ] — adopting strict versioning guidelines is still an effective way to keep the compatibility of libraries APIs [ 14 ] . C . Implications for Library Clients Clients should work with package managers . The results reported in Section V - D show that client applications are vulnerable to DABCs ; 67 , 747 out of 194 , 099 S CIKIT - L EARN clients ( 35 % ) are exposed to at least one DABC Moreover , clients turned out to be vulnerable to multiple API versions . We reinforce a recommendation already mentioned in other works : client developers must adopt minimum versioning strategies when maintaining their dependencies , such as using package managers [ 12 ] , [ 14 ] , [ 16 ] . Clients should choose carefully when to rely on default values . As stated previously , using default argument values reduces the effort when using a given API method . Section II shows that , although the SVC constructor receives 14 parameters , it is possible to create a new model without any speciﬁc as all arguments have default values assigned . Under the hood , using default arguments introduce data dependencies in client appli - cations as the values provided to their functions are deﬁned by third parties , of which they have no control [ 11 ] , [ 14 ] . Considering the context of S CIKIT - L EARN library , our work shows these dependencies expose clients in crucial machine learning stages , such as Model Training and Model Evaluation ( see sections V - C and V - D ) . Therefore , we argue that client developers should be diligent when relying on default values . 217 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Speciﬁcally , we suggest temporarily relying on these values ; for example , during machine learning model development and experimentation . VII . T HREATS TO V ALIDITY Selected Library : We scoped this work on analyzing DABCs for one library , speciﬁcally . Despite S CIKIT - L EARN being one of the most adopted machine learning libraries , we acknowledge it is not possible to generalize our ﬁndings to other third - party components . DABCs Identiﬁcation Process : We rely on the documentation provided by S CIKIT - L EARN maintainers to leverage the DABCs investigated in this study . Naturally , this strategy may pose some threats in detecting API changes , as we are restricted to the changes properly documented in the library API . In our favour , S CIKIT - L EARN maintainers follow strict guidelines for reporting API changes , e . g . , they provide clear instructions about how to document modiﬁcations in the library’s API , including default value changes . 20 Moreover , other researchers also relied on API documentation to obtain breaking change candidates [ 9 ] . Clients Dataset : We rely on the dataset provided by Grotov et al . [ 8 ] to investigate how clients are exposed to DABCs . Although we could select other datasets to perform this analysis [ 9 ] , [ 16 ] , [ 17 ] , [ 25 ] , we take into account the documentation available to download and conﬁgure the dataset locally and the publicly available tool that—after proper adaptations—helped us in extracting the clients’ function calls . Yet , we understand it is important to expand this analysis to other artifacts besides Jupyter Notebooks , such as Python script ﬁles [ 16 ] , [ 25 ] . Function Call Heuristic : We implemented a heuristic to iden - tify DABCs calls in client applications . Ideally , we could overcome this threat by executing the source code of each client and performing a dynamic analysis over the functions called . Even though , we opted for an static - based analysis since previous works reported great difﬁculty in performing this task [ 16 ] , [ 17 ] . To ensure the reliability of our heuristic , we manually analyzed a sample of 384 calls and ﬁnd out that 95 . 3 % ( ± 5 % ) were correctly classiﬁed by the heuristic . VIII . R ELATED W ORK A . Library Updates and Breaking Changes Various studies proposed techniques to detect and under - stand breaking changes in libraries and frameworks [ 10 ] , [ 14 ] , [ 31 ] – [ 34 ] . Mezzetti et al . [ 14 ] conducted a study on breaking changes in the npm repository and introduced a technique called type regression testing to automatically detect whether a library update affects the types provided by its public interface . According to the authors , at least 5 % of all packages experienced a breaking change in a patch or minor update , with most of these changes attributed to modiﬁcations in the 20 https : / / scikit - learn . org / dev / developers / contributing . html # change - the - default - value - of - a - parameter public package API . Mostafa et al . [ 32 ] describe a large - scale regression testing performed over 68 adjacent version pairs from 15 popular Java libraries to comprehend APIs’ behavioural changes over time . For this , the authors executed each version pair and compared the output produced by them , i . e . , whether the updated code changed library behaviour . Their result reveals that behavioural backward - incompatibilities are common in Java libraries and are the root of many backward - incompatibility issues . Our study investigated an unexplored kind of behaviour - breaking change ( DABCs ) in Machine Learning libraries . B . Machine Learning APIs Smells The literature shows that Machine Learning ( ML ) systems are also prone to traditional software engineering issues [ 3 ] , [ 5 ] , [ 35 ] , [ 36 ] . For example , Tang et al . [ 37 ] study refactoring and technical debt issues in ML systems . OBrien et al . [ 2 ] also investigate technical debts in ML applications . In the context of code smells , Zhang et al . [ 38 ] identify 22 machine - learning - speciﬁc code smells , while Gesi et al . [ 36 ] investigate code smell in Deep Learning software systems . Regarding API maintenance , Haryono et al . [ 7 ] studied a list of 112 deprecated APIs from three popular Python ML libraries to better under - stand how they can be migrated . The authors identiﬁed three dimensions involving deprecated API migrations : update oper - ation , API mapping , and context dependency . Zhang et al . [ 9 ] investigated changes performed on the API documentation of multiple TensorFlow versions to analyze their evolution . Then , they classiﬁed these changes into ten categories according to the reason behind the modiﬁcations ; the most common ones are efﬁciency and compatibility . Differently , we studied a behaviour - breaking change speciﬁc for changes performed in APIs default arguments , i . e . , DABCs . IX . C ONCLUSION In this work , we investigate an unexplored type of behaviour breaking change , which we named Default Argument Breaking Change ( DABC ) . Speciﬁcally , we identiﬁed the DABCs in S CIKIT - L EARN —a well - known Machine Learning library— and analyzed how client applications are vulnerable to them . Overall , we analyzed 77 DABCs among eight major versions of S CIKIT - L EARN ; 93 % of them were detected in client applications We also discuss the implications of our ﬁndings for researchers , library maintainers , and clients . We intend to extend this work in the following directions : ( a ) reproduce this study with other machine learning libraries , such as TensorFlow , NumPy , etc ; ( b ) investigate why library maintainers introduce and modify default values in libraries’ APIs ; and ( c ) study the qualitative impact that such modiﬁcations have on clients . Replication Package : Data and scripts are publicly available at Zenodo : https : / / doi . org / 10 . 5281 / zenodo . 7868228 . A CKNOWLEDGMENT This research is supported by the National Council for Scien - tiﬁc and Technological Development ( CNPq ) . 218 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . R EFERENCES [ 1 ] F . Provost and T . Fawcett , Data Science for Business : What You Need to Know about Data Mining and Data - Analytic Thinking , 1st ed . Beijing K¨oln : O’Reilly , 2013 . [ 2 ] D . OBrien , S . Biswas , S . M . Imtiaz , R . Abdalkareem , E . Shihab , and H . Rajan , “23 Shades of Self - Admitted Technical Debt : An Empirical Study on Machine Learning Software , ” in 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering ( ESEC / FSE ) , 2022 , p . 13 . [ 3 ] H . Washizaki , F . Khomh , Y . - G . Gu´eh´eneuc , H . Takeuchi , N . Natori , T . Doi , and S . Okuda , “Software - Engineering Design Patterns for Machine Learning Applications , ” Computer , vol . 55 , no . 3 , pp . 30 – 39 , 2022 . [ 4 ] F . Ferreira , L . L . Silva , and M . T . Valente , “Software engineering meets deep learning : A mapping study , ” in 36th Annual ACM Symposium on Applied Computing ( SAC ) . ACM , 2021 , pp . 1542 – 1549 . [ 5 ] S . Amershi , A . Begel , C . Bird , R . Deline , H . Gall , E . Kamar , N . Nagap - pan , B . Nushi , and T . Zimmermann , “Software Engineering for Machine Learning : A Case Study , ” in 41st ACM / IEEE International Conference on Software Engineering ( ICSE ) , 2019 . [ 6 ] J . E . Montandon , L . Lourdes Silva , and M . T . Valente , “Identifying Experts in Software Libraries and Frameworks Among GitHub Users , ” in 16th International Conference on Mining Software Repositories ( MSR ) , 2019 , pp . 276 – 287 . [ 7 ] S . A . Haryono , F . Thung , D . Lo , J . Lawall , and L . Jiang , “Character - ization and Automatic Updates of Deprecated Machine - Learning API Usages , ” in International Conference on Software Maintenance and Evolution ( ICSME ) , Sep . 2021 , pp . 137 – 147 . [ 8 ] K . Grotov , S . Titov , V . Sotnikov , Y . Golubev , and T . Bryksin , “A Large - Scale Comparison of Python Code in Jupyter Notebooks and Scripts , ” in 19th International Conference on Mining Software Repositories ( MSR ) , 2022 , pp . 353 – 364 . [ 9 ] Z . Zhang , Y . Yang , X . Xia , D . Lo , X . Ren , and J . Grundy , “Unveiling the Mystery of API Evolution in Deep Learning Frameworks : A Case Study of Tensorﬂow 2 , ” in 43rd International Conference on Software Engineering : Software Engineering in Practice ( ICSE - SEIP ) , 2021 , pp . 238 – 247 . [ 10 ] A . Brito , M . T . Valente , L . Xavier , and A . Hora , “You broke my code : Understanding the motivations for breaking changes in APIs , ” Empirical Software Engineering , vol . 25 , pp . 1458 – 1492 , 2020 . [ 11 ] A . Ponomarenko and V . Rubanov , “Backward compatibility of software interfaces : Steps towards automatic veriﬁcation , ” Programming and Computer Software , vol . 38 , pp . 257 – 267 , 2012 . [ 12 ] D . Venturini , F . R . Cogo , I . Polato , M . A . Gerosa , and I . S . Wiese , “I depended on you and you broke me : An empirical study of manifesting breaking changes in client packages , ” ACM Transactions on Software Engineering and Methodology , 2023 . [ 13 ] L . Ochoa , T . Degueule , J . - R . Falleri , and J . Vinju , “Breaking bad ? Semantic versioning and impact of breaking changes in Maven Central , ” Empirical Software Engineering , vol . 27 , pp . 1 – 42 , 2022 . [ 14 ] G . Mezzetti , A . Møller , and M . T . Torp , “Type Regression Testing to Detect Breaking Changes in Node . js Libraries , ” in 32nd European Conference on Object - Oriented Programming ( ECOOP ) , 2018 , pp . 1 – 24 . [ 15 ] M . T . Ribeiro , S . Singh , and C . Guestrin , “”Why Should I Trust You ? ” : Explaining the Predictions of Any Classiﬁer , ” in 22nd ACM International Conference on Knowledge Discovery and Data Mining ( SIGKDD ) , 2016 , pp . 1135 – 1144 . [ 16 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire , “A Large - Scale Study About Quality and Reproducibility of Jupyter Notebooks , ” in 16th International Conference on Mining Software Repositories ( MSR ) , 2019 , pp . 507 – 517 . [ 17 ] J . Wang , T . - Y . KUO , L . Li , and A . Zeller , “Assessing and Restoring Reproducibility of Jupyter Notebooks , ” in 35th International Conference on Automated Software Engineering ( ASE ) , 2020 , pp . 138 – 149 . [ 18 ] D . Phillips , Python 3 Object - Oriented Programming : Build robust and maintainable software with object - oriented design patterns in Python 3 . 8 , 3rd ed . Packt Publishing , 2018 . [ 19 ] J . Bloch , Effective Java , 3rd ed . Addison - Wesley Professional , 2017 . [ 20 ] M . M . Lehman , “On understanding laws , evolution , and conservation in the large - program life cycle , ” Journal of Systems and Software , vol . 1 , pp . 213 – 221 , 1979 . [ 21 ] F . Pedregosa , G . Varoquaux , A . Gramfort , V . Michel , B . Thirion , O . Grisel , M . Blondel , P . Prettenhofer , R . Weiss , V . Dubourg , J . Vander - plas , A . Passos , D . Cournapeau , M . Brucher , M . Perrot , and ´E . Duch - esnay , “Scikit - learn : Machine Learning in Python , ” Journal of Machine Learning Research , vol . 12 , no . 85 , pp . 2825 – 2830 , 2011 . [ 22 ] D . Wang , P . Cui , and W . Zhu , “Structural Deep Network Embedding , ” in 22nd International Conference on Knowledge Discovery and Data Mining ( KDD ) , 2016 , pp . 1225 – 1234 . [ 23 ] F . Bianchi , S . Terragni , and D . Hovy , “Pre - training is a Hot Topic : Contextualized Document Embeddings Improve Topic Coherence , ” in 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( IJCNLP ) , 2021 , pp . 759 – 766 . [ 24 ] P . Domingos , “A few useful things to know about machine learning , ” Commun . ACM , vol . 55 , no . 10 , p . 78 – 87 , oct 2012 . [ 25 ] L . Quaranta , F . Calefato , and F . Lanubile , “KGTorrent : A Dataset of Python Jupyter Notebooks from Kaggle , ” in 18th International Conference on Mining Software Repositories ( MSR ) , 2021 , pp . 550 – 554 . [ 26 ] J . Tan , D . Feitosa , and P . Avgeriou , “Investigating the Relationship between Co - occurring Technical Debt in Python , ” in 46th Euromi - cro Conference on Software Engineering and Advanced Applications ( SEAA ) , 2020 , pp . 487 – 494 . [ 27 ] D . E . Hinkle , W . Wiersma , and S . G . Jurs , Applied Statistics for the Behavioral Sciences , 5th ed . Belmont , CA : Wadsworth Cengage Learning , 2003 . [ 28 ] H . Borges and M . Tulio Valente , “What’s in a GitHub Star ? Understand - ing Repository Starring Practices in a Social Coding Platform , ” Journal of Systems and Software , vol . 146 , pp . 112 – 129 , 2018 . [ 29 ] M . Lamothe , Y . - G . Gu´eh´eneuc , and W . Shang , “A Systematic Review of API Evolution Literature , ” ACM Computing Surveys , vol . 54 , no . 8 , pp . 171 : 1 – 171 : 36 , 2021 . [ 30 ] R . Robbes and M . Lungu , “A Study of Ripple Effects in Software Ecosystems , ” in 33rd International Conference on Software Engineering ( ICSE , NIER Track ) , 2011 , pp . 904 – 907 . [ 31 ] R . G . Kula , A . Ouni , D . M . German , and K . Inoue , “An empirical study on the impact of refactoring activities on evolving client - used apis , ” Information and Software Technology , vol . 93 , no . C , pp . 186 – 199 , jan 2018 . [ 32 ] S . Mostafa , R . Rodriguez , and X . Wang , “Experience paper : A study on behavioral backward incompatibilities of Java software libraries , ” in 26th International Symposium on Software Testing and Analysis ( ISSTA ) , Jul . 2017 , pp . 215 – 225 . [ 33 ] K . Jezek , J . Dietrich , and P . Brada , “How java apis break - an empirical study , ” Information and Software Technology , vol . 65 , no . C , pp . 129 – 146 , sep 2015 . [ 34 ] D . Dig and R . Johnson , “How do apis evolve ? a story of refactoring : Research articles , ” Journal of Software Maintenance and Evolution , vol . 18 , no . 2 , pp . 83 – 107 , mar 2006 . [ 35 ] D . Sculley , G . Holt , D . Golovin , E . Davydov , T . Phillips , D . Ebner , V . Chaudhary , M . Young , J . - F . Crespo , and D . Dennison , “Hidden Technical Debt in Machine Learning Systems , ” in 28th International Conference on Neural Information Processing Systems ( NIPS ) , 2015 . [ 36 ] J . Gesi , S . Liu , J . Li , I . Ahmed , N . Nagappan , D . Lo , E . S . de Almeida , P . S . Kochhar , and L . Bao , “Code smells in machine learning systems , ” 2022 . [ 37 ] Y . Tang , R . Khatchadourian , M . Bagherzadeh , R . Singh , A . Stewart , and A . Raja , “An empirical study of refactorings and technical debt in machine learning systems , ” in IEEE / ACM 43rd International Conference on Software Engineering ( ICSE ) , 2021 , pp . 238 – 250 . [ 38 ] H . Zhang , L . Cruz , and A . van Deursen , “Code smells for machine learning applications , ” in 1st International Conference on AI Engineer - ing : Software Engineering for AI , ser . CAIN’22 , 2022 , pp . 217 – 228 . 219 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply .