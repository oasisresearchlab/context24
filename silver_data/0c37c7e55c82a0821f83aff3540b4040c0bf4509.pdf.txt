Principled Assessment of Student Learning in High School Computer Science Eric Snow , Daisy Rutstein , Marie Bienkowski , Yuning Xu Center for Technology in Learning SRI International ACM International Computing Education Research ( ICER ) Conference August 2017  Significance & Need  Background & Related Research  Methodology  Findings  Discussion  Conclusions Overview ∗ Computer science is spreading throughout the US K - 12 system , with full , yearlong classes being offered in secondary schools ∗ Increased demand for assessments that support valid inferences about student learning Significance & Need ∗ Teachers in the introductory CS courses > > > how should I adapt instruction to meet my students’ needs ? ∗ Teachers in advanced CS courses > > > how well are students prepared for advanced work and where do they need extra help ? ∗ Principals > > > is my school offering rigorous CS courses ? ∗ Stakeholders > > > what CS knowledge and skills students are developing ? Significance & Need Development of high - quality assessments has not kept pace with the spread of CS programs / curricula throughout the US K - 12 system 2015 study by Computer Science Teacher’s Association ( CSTA ) found that teachers face a number of challenges finding valid and reliable assessments to use in their classrooms Significance & Need Challenge # 1 : Understanding the Domain > > > What is important for computer scientists to know and be able to do ? What are the important practices of CS ? Challenge # 2 : Developing Authentic Representations > > > How can we develop tasks / situations that elicit evidence of computational thinking practices ? Challenge # 3 : Eliciting Valid Evidence > > > Does the evidence support the inferences we want to make about computational thinking practices ? Assessment Challenges Challenge # 1 : Understanding the Domain What are computational thinking practices ? In the US , the new K12 CS Framework and aligned CSTA standards , and the Common Core State Standards and Next Generation Science Standards all include guidance related to computational thinking practices . Background & Related Research – Computational Thinking Practices This reflects an orientation toward not just an internal , individual “thinking” but “ways of being and doing” that students should demonstrate when learning and exhibiting computer science knowledge , skills , and attitudes . It represents the application of CS content knowledge via problem solving and inquiry - based methods . Background & Related Research – Computational Thinking Practices Challenge # 2 : Developing Authentic Representations How can we develop tasks / situations that elicit evidence of computational thinking practices ? ECD is a framework for assessment design and development Used by major testing companies and is a requirement of many states for developing assessments Views assessment as a process of gathering evidence to support inferences about what a student knows and can do Provides a structure for an approach that incorporates validity evidence into the assessment design process Particularly useful when the knowledge / skills to be measured involve complex , multistep performances , such as those required in computational thinking practices Background & Related Research – Evidence - Centered Design ( ECD ) ▪ From Mislevy & Riconscente , 2006 Assessment Delivery Students interact with tasks , performances evaluated , feedback created . Assessment Implementation Conceptual Assessment Framework Domain Modeling Domain Analysis What is important about this domain ? What work and situations are central in this domain ? What KRs are central to this domain ? How do we represent key aspects of the domain in terms of assessment argument . Design structures : Student , evidence , and task models . Manufacturing “nuts & bolts” : authoring tasks , automated scoring details , statistical models . ▪ From Mislevy & Riconscente , 2006 ECD Layer Activity Key Entities & Examples Domain analysis Gather substantive information about the domain of interest that has implications for assessment ; how knowledge is constructed , acquired , used , and communicated Computational thinking domain concepts ( e . g . , abstraction , automation ) ; terminology ( debugging ) ; tools ( programming languages ) ; representations ( storyboards ) ; situations of use ( modeling predator - prey , visual storytelling ) , and curriculum standards and mappings Domain modeling Express assessment argument in narrative form based on information from domain analysis Specification of knowledge , skills , and other attributes to be assessed ( e . g . , describe result of running a program on given data ) ; features of situations that can evoke evidence ( find errors in programs ) ; kinds of performances that convey evidence ( use of recursion ) Layers of Evidence - Centered Design and Key Entities for Computational Thinking ▪ From Mislevy & Riconscente , 2006 ECD Layer Activity Key Entities & Examples Conceptual assessment framework Express assessment argument in structures and specifications for tasks and tests , evaluation procedures , measurement models Student , evidence , and task models ; student , observable , and task variables ; rubrics ; measurement models ; test assembly specifications ; task templates and task specifications Assessment implementation Implement assessment , including presentation - ready tasks and calibrated measurement models Tasks , task materials ( including supporting materials , tools , affordances ) ; pilot test data to hone evaluation procedures and fit measurement models Assessment delivery Coordinate interactions of students and tasks : task - and test - level scoring ; reporting Tasks as presented ; work products as created ; scores as evaluated Layers of Evidence - Centered Design and Key Entities for Computational Thinking , cont’d More information about ECD can be reviewed at : https : / / ecd . sri . com / Background & Related Research – Evidence - Centered Design Challenge # 3 : Eliciting Valid Evidence Does the evidence support the inferences we want to make about computational thinking practices ? The latest thinking in test validity focuses on supporting assessment inferences through collecting and integrating different types of evidence : ∗ Test Content ∗ Internal Structure ∗ Response Processes ∗ Relations to other Variables ∗ Test Use Background & Related Research – Test Validity How can we improve CS teaching , learning , and adoption through evidence - centered assessment ? ∗ Pre - AP , introductory CS course ∗ Late middle school / early high school ∗ Six , six - week units ∗ Focus on equity ∗ A central tenet of ECS pedagogy is inquiry - based learning : core concepts learned through induction , teaching through guided inquiry , and open - ended assessments Context – Exploring Computer Science ( ECS ) Methodology – Analyzing and Modeling the CTP Domain for ECS Domain Analysis What is important about the CTP domain for ECS ? What work and situations are central in CTP domain for ECS ? What KRs are central to the CTP domain for ECS ? * The CTP domain as it is represented in ECS . Standards / Curriculum  Exploring Computer Science , Unit 1 - 4 Learning Objectives  CSTA ( 2011 ) . CSTA K - 12 Computer Science Standards  College Board ( 2010 ) . AP CS Principles : Big Ideas , Key Concepts , and Supporting Concepts  NGSS , CCSS Literature  National Academies Report : Computer Science : Reflections on the Field , Reflections from the Field  SIGCSE , CSTA , ITiCSE , Journal of Computing in Higher Education , Educational Researcher  Jeanette Wing & others ; National Academies Workshop on Pedagogical Aspects of Computational Thinking Methodology – Analyzing the CTP Domain for ECS Methodology – Analyzing and Modeling the CTP Domain for ECS ECS Units Computational Thinking Practices Unit 1 : Human - Computer Interaction • Analyze the effects of developments in computing . Unit 2 : Problem Solving • Design and implement creative solutions and artifacts . • Apply abstractions and models . • Analyze their computational work and the work of others . Unit 3 : Web Design • Design and implement creative solutions and artifacts . • Analyze their computational work and the work of others . • Connect computation with other disciplines . Unit 4 : Introduction to Programming • Design and implement creative solutions and artifacts . • Apply abstractions and models . • Analyze their computational work and the work of others . Methodology – Modeling the CTP Domain for ECS Domain Modeling How do we represent key aspects of the CTP domain for ECS in terms of an assessment argument ? Developed assessment design patterns for ECS units 1 - 4 :  Human - computer interaction  Problem solving  Web design  Introduction to programming Design patterns are high level representations of an assessment argument that lay out a design space for assessment developers Methodology – Modeling the CTP Domain for ECS Methodology – Analyzing and Modeling the CTP Domain for ECS ECS Unit / Computational Thinking Practice Example ECS Unit Focal Knowledge , Skills , Attributes ( FKSAs ) Example CTP Focal Knowledge , Skills , Attributes ( FKSAs ) Unit 1 : Human - Computer Interaction Analyze the effects of developments in computing . • Students are able to explain why an object is or is not a computer . • Ability to describe the characteristics of a computer ( including what it means for a computer to be “intelligent” ) . • Students are able to evaluate the implications of a form of data exchange on social interactions . • Ability to analyze the effects of computing on society within economic , social , and cultural contexts . • Students are able to explain how computing innovation has led to new types of legal and ethical concerns . • Ability to evaluate legal and ethical concerns raised by computing - enabled innovations . Methodology – Modeling the CTP Domain for ECS Example FKSA Example Potential Work Product ( what students produce ) Example Potential Observations ( evidence in what students produce ) Students are able to explain why an object is or is not a computer . An explanation of why an object is or is not a computer . Appropriateness of the explanation of why an object is or is not a computer . • Did the student correctly identify aspects of the object that relate to aspects of a computer ? • Did the student correctly identify aspects of a computer that the object lacks ? Methodology – Modeling the CTP Domain for ECS Example FKSA Example Characteristic Features Example Variable Features Students are able to explain why an object is or is not a computer . The student must be presented with an object The object must have clear characteristics that allow the evaluation of whether it is a computer . Whether the object could be considered a computer or not . Whether students would be able to argue either way if the object is a computer or not . The degree to which the important characteristics are explicitly stated in the problem or must be inferred by the test taker . ∗ Extended , scenario - based tasks containing 3 - 4 embedded forced - choice and short constructed response items ∗ Present students with more authentic situations to measure CTPs ∗ Focus on applying , analyzing , evaluating levels of Bloom’s taxonomy . ∗ Other Bloom levels ( e . g . , create ) and corresponding FKSAs were measured using other formats ( e . g . , portfolios ) Methodology – Developing Assessments of CTPs for ECS Methodology – Developing Assessments of CTPs for ECS Methodology – Developing Assessments of CTPs for ECS ∗ Pilot 1 2014 - 2015 , Pilot 2 2015 - 2016 ∗ ECS teachers from across the U . S . including Los Angeles , Chicago , and New York ∗ Collected validity evidence based on test content and student responses processes to help us refine and improve the assessments ∗ Test content > > > expert review of alignment between the knowledge and skills , the curriculum learning goals , and CT practices ∗ Student response processes > > > cognitive think - aloud interviews with students participating in the pilot testing activities Methodology – Piloting & Refining Assessments for ECS ∗ Researchers were trained on the rubrics ∗ Each assessment was scored by two different scorers with a third scorer scoring if there were discrepancies in the scores ∗ Inter - rater reliability was high , with over 90 % agreement between raters for most of the tasks ∗ Tasks for which the reliability was lower were revised either by modifying the item to clarify what was expected or by modifying the rubric Methodology – Scoring & Inter - Rater Reliability Analysis & Results – Descriptive Statistics Analysis & Results – Descriptive Statistics ∗ ~ 40 % female / 60 % males , ~ 50 % Hispanic / Latino ( 49 . 28 % ) ∗ Average total scores in the 60 - 70 % range across the assessments ∗ Female and male students had comparable average scores on the assessments ∗ Score distributions were slightly negatively skewed , indicating more students scored at the high end of the score distributions . Analysis & Results – Validity Evidence Based on Internal Structure ∗ Moderate to high levels of reliability ( . 66 - . 84 ) ∗ Factor analysis supported expected structure of unit and cumulative assessments ∗ Moderate task difficulty levels , with the index ranging from . 58 to . 67 ∗ High discriminating power for tasks / items with medium levels of difficulty Analysis & Results – Validity Evidence Based on Internal Structure Validity evidence based on internal structure is particularly promising : ∗ tasks within each unit assessment are all measuring one general construct ∗ assessments best suited for differentiating students of average ability Discussion Next Steps ∗ Examine whether validity results hold w / larger sample and schools from different contexts ∗ Developing additional assessment tasks , particularly those with easy and hard levels of difficulty to improve utility across wider range of ability levels ∗ Item Response Theory ( IRT ) and Testlet Response Theory ( TRT ) analyses Discussion Important effort to apply principled assessment design methods and contemporary test - validity standards to guide the development , piloting and validation of assessments of CTPs Conclusions Support use of the assessments by both educators measuring students’ CT practices and by researchers studying curriculum implementation and student learning in introductory high school computer science Conclusions ECS teachers and other groups can use design patterns to develop novel assessment items for use in their classroom or in research on the impact of the ECS curriculum on students’ CT practices Conclusions New PACT Report : Snow , E . , Tate , C . , Rutstein , D . , Bienkowski , M . ( 2017 ) . Assessment design patterns for computational thinking practices in Exploring Computer Science . * * To be released September 2017 on the PACT web site , http : / / pact . sri . com More information about PACT ? https : / / pact . sri . com / More information about the CTP Design Patterns ? http : / / bit . ly / 2u6t0Nw Review the ECS assessments and rubrics ? * https : / / pact . sri . com / ecs - assessments . html * Terms of Use & Licensing Information : https : / / pact . sri . com / assessment / term slicense . html THANK YOU ! This work was conducted with support from the National Science Foundation under contract numbers , CNS - 1132232 , CNS - 1240625 , and DRL - 1418149 . Questions ?