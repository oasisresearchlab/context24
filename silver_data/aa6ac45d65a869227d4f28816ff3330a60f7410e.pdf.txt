Using GUI Test Videos to Obtain Stakeholders’ Feedback Jianwei Shi Leibniz Universit¨at Hannover Software Engineering Group Hannover , Germany 0000 - 0001 - 6228 - 2478 Jonas M¨onnich Leibniz Universit¨at Hannover Hannover , Germany 0000 - 0002 - 6156 - 0672 Jil Kl¨under Leibniz Universit¨at Hannover Software Engineering Group Hannover , Germany 0000 - 0001 - 7674 - 2930 Kurt Schneider Leibniz Universit¨at Hannover Software Engineering Group Hannover , Germany 0000 - 0002 - 7456 - 8323 Abstract —In software projects , stakeholders can give valuable feedback on software demonstrations . Demonstrating software early and responding to feedback is crucial in agile development . However , it is difﬁcult for stakeholders who are not on - site customers but end users , marketing people , or designers , etc . to give feedback in an agile development environment . Successful Graphical User Interface ( GUI ) tests , which show the working GUI with expected software behaviors , can be documented and then demonstrated for feedback . In our new concept , GUI tests are recorded , extended , and demonstrated as videos . A GUI test is divided into several GUI unit tests , which are speciﬁed in Gherkin , a semi - structured natural language . For each GUI unit test , a video is generated during test execution . Test steps speciﬁed in Gherkin are traced and highlighted in the video . Stakeholders review these generated videos and provide feedback , e . g . , on misunderstandings of requirements or on inconsistencies . To evaluate the impact of videos in identifying inconsistencies , we asked 22 participants to identify inconsistencies between ( 1 ) given requirements in regular sentences and ( 2 ) demonstrated behav - iors from videos with Gherkin speciﬁcations or from Gherkin speciﬁcations alone . Our results show that participants tend to identify more inconsistencies from demonstrated behaviors which are not in accordance with given requirements . They tend to recognize inconsistencies more easily through videos than through Gherkin speciﬁcations alone . We conclude that GUI test videos can help stakeholders give feedback more effectively . By obtaining early feedback , inconsistencies can be resolved , thus contributing to higher stakeholder satisfaction . Index Terms —feedback , GUI test , video , agile I . I NTRODUCTION In software projects , the requirements of a software are de - ﬁned through communication among stakeholders . Stakehold - ers are people or organizations who inﬂuence the requirements of a system [ 1 ] . They can be customers , developers , end - users , user experience ( UX ) designers , or marketing people who express opinions about a software system . A common issue is the insufﬁcient communication among stakeholders in large - scale software projects [ 2 ] , [ 3 ] and in global software development [ 4 ] , [ 5 ] . This can lead to misunderstandings regarding the requirements . Thus , the implemented software can have wrong or missing functionality . This , in turn , leads to dissatisﬁed stakeholders or customers [ 2 ] . A possible solution is that developers demonstrate the soft - ware under development and get feedback from stakeholders . This feedback can be beneﬁcial , because it comes from other perspectives and could also play a signiﬁcant role on clarifying requirements . Potential end - users can give feedback on usage problems and suggest new features or extensions [ 6 ] . UX designers can give some feedback on quality related require - ments , e . g . , usability of software [ 7 ] . Moreover , marketing people can see the demonstrated software’s GUI to identify inconsistencies with the corporate design . In agile software development , continuous feedback is re - alized by regular demonstrations of the software in review meetings , e . g . , in sprint planning meetings [ 8 ] . This feedback can be addressed in future iterations [ 9 ] , [ 10 ] . However , only the on - site customer and the development team attend such meetings , and other stakeholders cannot give feedback this way . As argued above , other stakeholders can also give valuable feedback and should thus be involved in the pro - cess . Unfortunately , having more people attending the review meetings is impossible due to large and diverse groups of stakeholders , who have other responsibilities . In this paper , we want to address this problem by presenting an approach to obtain feedback from diverse stakeholders . We propose to record graphical user interface ( GUI ) tests on video , extend this video with further explanations ( if needed ) , and present this video to stakeholders who can give feedback without actually using the software themselves . The aim of this paper is to use existing successful GUI tests to create and use videos to obtain feedback from stakeholders . A video is recorded during a slowed down GUI test run while GUI interactions are highlighted in the video . The resulting video is slowed down sufﬁciently for human observers to follow . In the video , the GUI control element under interaction is highlighted to show the test step with inputs . We provide stakeholders with the video to obtain feedback . We apply this concept in Behavior - Driven Development ( BDD ) , an agile software development method . In this paper , we make the following contributions : 1 ) We provide supplementary steps beside the regular agile processes to involve stakeholders in the development . 2 ) We divide a GUI test into several GUI unit tests and organize them in a connection graph . Stakeholders see the graph and choose a continuous GUI test ﬂow to watch . 3 ) We evaluate the effect of videos with regard to ﬁnding in - consistencies between requirements and the demonstrated 35 2023 IEEE / ACM International Conference on Software and System Processes ( ICSSP ) 979 - 8 - 3503 - 1196 - 9 / 23 / $ 31 . 00 ©2023 IEEE DOI 10 . 1109 / ICSSP59042 . 2023 . 00014 20 2 3 I EEE / A C M I n t e r n a ti on a l C on f e r e n ce on S o f t w a r e a nd S y s t e m P r o ce ss e s ( I C SSP ) | 979 - 8 - 3503 - 1196 - 9 / 23 / $ 31 . 00 © 2023 I EEE | DO I : 10 . 1109 / I C SSP 59042 . 2023 . 00014 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . software from the stakeholders’ perspective . The rest of this paper is structured as follows : Section II lists related work . The concept and its exemplary application are explained in Section III . Section IV presents the experiment conducted to evaluate the concept from the stakeholders’ perspective . The collected data is analyzed and the results are discussed . The paper is concluded in Section V . II . R ELATED W ORK We list the works which reveal the necessity of involving stakeholders in testing . Tests are written based on the require - ments speciﬁcation . Requirements should be written in a way that makes the tests easily interpretable . Hence , the alignment of requirements with tests is considered in this section . Lastly , we list works that use videos as a medium to communicate requirements . A . Stakeholder Participation in Testing An industrial case study from M¨antyl¨a et al . [ 11 ] shows that not only testers are involved in testing , but also people who have a close relationship with customers . Sales personnel tend to discover additional relevant defects from the customer perspective . In another industrial case study from Bjarnason et al . [ 12 ] , customers conducted regular tests of executable code in two companies . Another company did not involve customers until shortly before the launch of the software and therefore ran into the problem of receiving too much feedback . For effective customer testing , they [ 12 ] argue that customers need to be involved in testing at an early stage of development . Chawani et al . [ 13 ] investigate the participation of stakeholders in software development for health care in Malawi . By inviting nurses as end - users during testing , requirements regarding the data format and user interface design were elicited . The managers of the health center and the ministry of health saw a system demonstration and provided feedback in more detail . In this investigation , feedback from various stakeholders complemented each other to generate concrete requirements . B . Alignment of Requirements with Tests The necessary practice of aligning requirements with tests is discussed in many studies [ 12 ] , [ 14 ] – [ 16 ] . Uusitalo et al . [ 17 ] list several practices of ﬁve Finnish companies regarding the linking of requirements and testing . Bjarnason et al . [ 12 ] propose that testers check the testability of requirements . Most of their interviewees ﬁnd the review of requirements by testers to be meaningful , because the review activates communication about requirements and thus improves their quality . Further - more , Bjarnason et al . [ 16 ] conducted an industrial case study to investigate the beneﬁts and challenges of using test cases as requirements . They summarize that this practice can help people in different roles within a company to communicate about requirements and to align their goals . A challenge of involving customers is that customer competence about GUI and quality requirements is required for communicating the related requirements . Lucassen et al . [ 18 ] explore linking user stories to test traces . Concretely , user stories are linked with methods of classes and acceptance tests . For a project of a driving assis - tance system , Pudlitz et al . [ 19 ] propose to annotate require - ments in different levels of detail . Requirements are written in natural language and annotated with concrete elements such as signal assignments , conditions , and post - conditions . They mention that the annotation and the tracing from annotation to tests can help requirements engineers to organize testable requirements . Meanwhile , testers can develop tests that are in compliance with the requirements . Based on the given test data , requirement coverage can then be calculated to check which parts of the requirements are considered in testing . C . Video for Demonstrating GUI Interactions GUI Interactions can be captured in a video to communicate requirements . Mackay et al . [ 20 ] use paper drafts of a graphical software to create and use videos for ﬁnding and detailing design ideas . In a brainstorming phase , participants acted out their ideas of software usage to create videos . In a design phase , participants reviewed these videos to ﬁnd one design scenario for every design problem . Paper mockups were drawn and used to show user interactions in a video . Designers watched the produced videos to understand how a user would use the software . To gain a shared understanding of require - ments , Stangl and Creighton [ 21 ] propose the usage of paper GUI sketches to create videos . During video creation , clickable areas are predeﬁned so that the prototype is executable through the video . Karras et al . [ 22 ] use digital mockups to visualize GUI interactions in a video . They propose to use digital mock - ups to capture and replay GUI interactions . From a developer’s perspective they evaluate how videos affect comprehension in comparison to static mockups . While the control group received only static mockups , the experimental group received a video which contains interactions on these mockups . Both groups had access to textual scenario descriptions of speciﬁed GUI interactions . The training time that participants spent on reading and familiarizing themselves with the materials was measured . Ten questions were used to check comprehension . Results show that the experimental group needed signiﬁcantly less training time than the control group . The number of correctly answered questions were almost the same for both groups . Karras et al . [ 22 ] conclude that textual scenario descriptions can be understood faster with the proposed video than with static mockups . Pham et al . [ 23 ] propose videos of GUI test executions for debugging purposes . The videos are captured during test execution , while a matching relationship between video and test code is created . A side - by - side viewer is used to replay the GUI interactions and corresponding lines in the test code . In the context of a qualitative evaluation in one company , the two interviewed developers believed that videos can help with debugging . Shi and Schneider [ 24 ] follow the concept of Pham et al . [ 23 ] and suggest the highlighting of GUI interactions . In the video , the GUI element under interaction ( e . g . , button or entry ﬁeld ) is accentuated by a colored border . Tandun [ 25 ] 36 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . implements this highlighting concept in a software suite that is used to capture and replay a GUI test run . The effect of the video in debugging was investigated among testers in a company . A semi - structured interview was conducted to collect subjective opinions of the shown videos . Shi et al . [ 26 ] have coded these opinions . According to the coding results , participants mentioned that the highlighting function can help to reveal the position of defects accurately and explicitly . The novel contributions in this paper are : 1 . Videos are used during a project to collect feedback from stakeholders , not at the beginning [ 20 ] – [ 22 ] ; 2 . Videos are viewed not only by developers [ 23 ] , [ 24 ] , but also by stakeholders , who are end - users , designers , developers , or testers ; 3 . We design an experiment to compare a classic ( textual speciﬁcations ) with a supplemented solution ( textual speciﬁcations , videos and a connection graph ) . To check the effect of the supplemented documentations , we ask participants to recognize inconsisten - cies from demonstrated behaviors which are not in accordance with given requirements . This kind of dedicated experiment is missing in [ 21 ] . III . M ETHODOLOGY A GUI test is a concrete representation of how a software should be used and what behavior is expected . When GUI tests are demonstrated to stakeholders , they may give feedback or modify their requirements . Such a demonstration is not a test in itself but rather a system demonstration to obtain feedback . In agile development , it is encouraged to write tests ﬁrst to ensure that the software should work as speciﬁed [ 27 ] . If an agile development team writes GUI tests and updates them regularly during development , these tests can be used for demonstration in order to solicit feedback and validate requirements . We want to apply this concept in this agile development . Research Question : How can existing GUI tests be used to create videos for obtaining feedback during agile development ? A . Video Production & Usage in Agile Development Our basic concepts are : 1 ) Use existing GUI tests in development to create videos ; 2 ) Demonstrate the videos to stakeholders . Figure 1 shows how our approach supplements the general agile practices . At the beginning of agile development , require - ments are elicited from stakeholders . After that , requirements are written . Then , the software and corresponding tests are programmed . We consider Implement tests and Develop soft - ware to be parallel activities , as they are not strictly ordered in agile development . Next , videos are created by using the existing tests and working software under development . In general , not only GUI tests are used , as other types of tests can also be visualized . These videos demonstrate the software to assist stakeholders in giving feedback , thus requirements are updated . (cid:2)(cid:3)(cid:4)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:7)(cid:3)(cid:6)(cid:11)(cid:7)(cid:10)(cid:12)(cid:13)(cid:14)(cid:10)(cid:9)(cid:15)(cid:16)(cid:12)(cid:3)(cid:17)(cid:18)(cid:16)(cid:19)(cid:14) (cid:20)(cid:19)(cid:7)(cid:10)(cid:16)(cid:13)(cid:3)(cid:19)(cid:13)(cid:5)(cid:21)(cid:18)(cid:9)(cid:10)(cid:16)(cid:19)(cid:16)(cid:22)(cid:5)(cid:7)(cid:19)(cid:16)(cid:4)(cid:16)(cid:6)(cid:10)(cid:14) (cid:23)(cid:4)(cid:21)(cid:17)(cid:16)(cid:4)(cid:16)(cid:6)(cid:10)(cid:10)(cid:16)(cid:14)(cid:10)(cid:14) (cid:24)(cid:16)(cid:25)(cid:16)(cid:17)(cid:3)(cid:21)(cid:14)(cid:3)(cid:26)(cid:10)(cid:11)(cid:9)(cid:19)(cid:16) (cid:27)(cid:7)(cid:18)(cid:16)(cid:3)(cid:8)(cid:19)(cid:16)(cid:9)(cid:10)(cid:7)(cid:3)(cid:6) Fig . 1 . Create and use video in agile development . B . Application in Behavior - Driven Development We apply the proposed concept in Behavior - Driven Devel - opment ( BDD ) , an agile development method . Table I shows the concrete BDD steps according to Smart [ 28 ] . TABLE I C ONCRETE BDD S TEPS A CCORDING TO S MART [ 28 ] Step Activity 1 Business owner and business analyst come together to talk about requirements ; 2 Business analyst , developers and testers deﬁne and write acceptance criteria ; 3 According to written acceptance criteria , developers develop the software , while testers write BDD unit tests ; 4 The written tests are automated and run regularly ; 5 The test results provide feedback to the development team . Acceptance criteria contain scenarios which are speciﬁed in Gherkin . The Gherkin text corresponds to the test case elements : Given ( preconditions ) , When ( actions with inputs ) , Then ( expected results ) . An example : “ Given I am on the page of ‘duckduckgo . com’ When I enter ‘University’ in the search ﬁeld And I click the search button near the search ﬁeld Then I see a list of links with short textual information” . These Gherkin speciﬁcations are stored in feature ﬁles . Step 3 ( Tab . I ) follows test - driven development ( TDD ) practice [ 27 ] . From the set of BDD tests , we use the GUI tests to create videos . These GUI tests should be unit tests . Hence , we propose the following deﬁnition : A GUI unit test is a GUI test that interacts with logically related graphical control elements on the GUI . Figure 3 shows an example of a GUI unit test and the corresponding GUI . 1 ) The Supplemented BDD Processes : Figure 2 depicts the supplemented BDD processes as a FLOW diagram . We use the FLOW notation by Stapel et al . [ 29 ] . Our supplementation to the BDD steps shown in Tab . I comprises the following : a ) After step 2 , the business analyst creates a connection graph that shows relationships between scenarios ( i . e . , GUI unit tests ) ; b ) In step 3 , GUI unit tests are implemented according to Gherkin speciﬁcations . The speciﬁcations are stored in feature ﬁles ; c ) In step 4 , executions of GUI unit tests are captured . The test execution should be modiﬁed automatically , i . e . , the test execution is slowed down and interactions in the 37 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11) (cid:12)(cid:13)(cid:10)(cid:6)(cid:8) (cid:14) (cid:14)(cid:8)(cid:15)(cid:15)(cid:16)(cid:17)(cid:13)(cid:18)(cid:4)(cid:3)(cid:13)(cid:8)(cid:17)(cid:19) (cid:4)(cid:20)(cid:8)(cid:16)(cid:3)(cid:19)(cid:11)(cid:6) (cid:22) (cid:16)(cid:13)(cid:11)(cid:6)(cid:15)(cid:6)(cid:17)(cid:3) (cid:14) (cid:21)(cid:22)(cid:6)(cid:18)(cid:16)(cid:3)(cid:6) (cid:23)(cid:24)(cid:25)(cid:19)(cid:16)(cid:17)(cid:13)(cid:3)(cid:19)(cid:3)(cid:6) (cid:14) (cid:3) (cid:14) (cid:28) (cid:11)(cid:13)(cid:3)(cid:6)(cid:19)(cid:8)(cid:11)(cid:19)(cid:16) (cid:29) (cid:10)(cid:4)(cid:3)(cid:6) (cid:4)(cid:18)(cid:18)(cid:6) (cid:29) (cid:3)(cid:4)(cid:17)(cid:18)(cid:6) (cid:18)(cid:11)(cid:13)(cid:3)(cid:6)(cid:11)(cid:13)(cid:4) (cid:30) (cid:6)(cid:4)(cid:3)(cid:16)(cid:11)(cid:6)(cid:19) (cid:31) (cid:13)(cid:9)(cid:6) (cid:14) (cid:6) ! (cid:6)(cid:9)(cid:8) (cid:29) (cid:6)(cid:11) " (cid:16) (cid:14) (cid:13)(cid:17)(cid:6) (cid:14)(cid:14) (cid:19)(cid:4)(cid:17)(cid:4)(cid:9) # (cid:14) (cid:3) $ (cid:6) (cid:14) (cid:3)(cid:6)(cid:11) (cid:23)(cid:24)(cid:25)(cid:19)(cid:24)(cid:17)(cid:13)(cid:3)(cid:19) $ (cid:6) (cid:14) (cid:3)(cid:19)(cid:12)(cid:13)(cid:6) % (cid:6)(cid:11) (cid:14)(cid:11)(cid:6)(cid:4)(cid:3)(cid:6)(cid:19) & (cid:11)(cid:4) (cid:29) (cid:7) (cid:25)(cid:15) (cid:29) (cid:9)(cid:6)(cid:15)(cid:6)(cid:17)(cid:3) (cid:31) (cid:6)(cid:4)(cid:3)(cid:16)(cid:11)(cid:6) (cid:14) (cid:19)(cid:4)(cid:17)(cid:10)(cid:19) % (cid:11)(cid:13)(cid:3)(cid:6) (cid:3)(cid:6) (cid:14) (cid:3) (cid:14) (cid:19) ’ $ ( (cid:2)(cid:8)(cid:16)(cid:11)(cid:18)(cid:6)(cid:19)(cid:18)(cid:8)(cid:10)(cid:6)(cid:19) ) (cid:23)(cid:24)(cid:25)(cid:19)(cid:16)(cid:17)(cid:13)(cid:3)(cid:19)(cid:3)(cid:6) (cid:14) (cid:3) (cid:14) * + (cid:6)(cid:11) (cid:14) (cid:8)(cid:17) , * (cid:8)(cid:18)(cid:16)(cid:15)(cid:6)(cid:17)(cid:3) - $ (cid:8)(cid:8)(cid:9) , * . (cid:18)(cid:3)(cid:13) ! (cid:13)(cid:3) # , (cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:7) (cid:4) (cid:20) (cid:18) (cid:10) (cid:18) (cid:2)(cid:3)(cid:6) (cid:29) (cid:19)(cid:17)(cid:16)(cid:15)(cid:20)(cid:6)(cid:11) (cid:6) (cid:8)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:5)(cid:16)(cid:9)(cid:17)(cid:10)(cid:18)(cid:19)(cid:16)(cid:9)(cid:11)(cid:10)(cid:12)(cid:16)(cid:20)(cid:3)(cid:11)(cid:19)(cid:10)(cid:5)(cid:16)(cid:15)(cid:5)(cid:14)(cid:10)(cid:16)(cid:13)(cid:21)(cid:14)(cid:15)(cid:22)(cid:15)(cid:14)(cid:23) (cid:24)(cid:3)(cid:5)(cid:3)(cid:11)(cid:13)(cid:14)(cid:3)(cid:19)(cid:16)(cid:15)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:5) (cid:25)(cid:3)(cid:21)(cid:3)(cid:15)(cid:22)(cid:3)(cid:19)(cid:16)(cid:15)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:5) (cid:8)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:5)(cid:16)(cid:9)(cid:17)(cid:10)(cid:18)(cid:19)(cid:16)(cid:15)(cid:5) (cid:8)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:5)(cid:16)(cid:9)(cid:17)(cid:10)(cid:18)(cid:19)(cid:16)(cid:10)(cid:26)(cid:14) Fig . 2 . FLOW diagram of the supplemented BDD processes . 1 @ gui @ iteration - 1 2 Scenario : Successful newsletter sign - up 3 Given I am logged in to the system 4 And I am on the newsletter sign - up page 5 When I enter my < name > 6 And I enter my < surname > 7 And I enter my < email > 8 And I give my approval 9 And I submit my data 10 Then I see a confirmation regarding my newsletter sign - up 11 12 Examples : 13 | name | surname | email | 14 | Jane | Doe | jane @ doe . com | (cid:2)(cid:3) (cid:4) (cid:4)(cid:5) (cid:6)(cid:7)(cid:8)(cid:9)(cid:3) (cid:4) (cid:4)(cid:5) (cid:10) (cid:4) (cid:3)(cid:11)(cid:12)(cid:5) (cid:13)(cid:14)(cid:15)(cid:3)(cid:9)(cid:16)(cid:14)(cid:16)(cid:17)(cid:14)(cid:8)(cid:4)(cid:18)(cid:4)(cid:11)(cid:19)(cid:4)(cid:14)(cid:3)(cid:14)(cid:15)(cid:4)(cid:4)(cid:20)(cid:12)(cid:21)(cid:14)(cid:9)(cid:4)(cid:15)(cid:22)(cid:12)(cid:4)(cid:16)(cid:16)(cid:4)(cid:8)(cid:23) (cid:6)(cid:7)(cid:24) (cid:4) (cid:11)(cid:16) Fig . 3 . Gherkin speciﬁcation of a GUI unit test and corresponding GUI . test are highlighted . The modiﬁcations follow the concept from Shi and Schneider [ 24 ] ; d ) After step 4 , the videos and the aforementioned graph are presented to stakeholders using our GUI Unit Test Viewer application . Stakeholders’ feedback can be obtained dur - ing the presentation ; e ) After step 5 , the obtained stakeholder feedback is con - sidered by starting again at step 1 . 2 ) Proposed Tool Support : We have prototyped an appli - cation called GUI Unit Test Viewer that supports organization of GUI unit tests in a graph , presentation of videos , and documentation of feedback . Table II shows the general usage steps . In the following paragraphs , we explain these steps in detail . In step 1 , the Gherkin speciﬁcations of GUI unit tests TABLE II G ENERAL U SAGE S TEPS FOR THE GUI U NIT T EST V IEWER Step Description 1 Business analyst imports feature ﬁles ; 2 Business analyst organizes all imported GUI unit tests in one connection graph ; 3 Business analyst loads the generated videos and links them with each scenario ; 4 Stakeholders select a path in the connection graph as a ﬂow of scenarios ; 5 Stakeholders see a consolidated video consisting of the se - lected scenarios ; 6 Stakeholders give feedback about shown scenarios . are imported . The speciﬁcation of a GUI unit test can be marked with the tag @ gui ( see Fig . 3 , line 1 ) . Developers can also mark a GUI unit test with an iteration identiﬁer ( e . g . , @ iteration - 1 , @ iteration - 2 , etc . ) . A business analyst can then recognize and import the scenarios using a tag ﬁlter . The connection graph in step 2 shows the logical and chronological relationship between the scenarios of a feature . Figure 4a shows an example connection graph for the feature “User Authentication” . This graph serves primarily as an interactive table of contents . The graph does not need to show all possible scenarios , because a stakeholder cannot view them all at once . Prior to step 3 , the videos are generated from GUI unit tests . The timestamps of each Gherkin step ( in Fig . 3 , lines 3 to 10 are each one step ) are stored as metadata together with the video . The interaction is highlighted in the test execution . The video is titled with the name of the GUI unit test . In step 3 a business analyst uses the title information to match videos with corresponding GUI unit tests . During demonstration , stakeholders may zoom in the graph to focus on a sub phase . In our example ( Fig . 4a ) , the selected scenarios from step 4 are shown as gray boxes . After a selection is conﬁrmed , the application changes to the video 38 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . GUI Unit Test Viewer Log in ( unsuccessful ) Reset password Requestaccountdeletion Conﬁrmaccountdeletion Changepassword Logout Log in ( successful ) Conﬁrmaccountcreation Requestaccountcreation Feature : User Authentication As a user I want to have a private user account So that my personal data and actions are protected from unauthorized access (cid:2) (cid:2)Feedback # 1 Verify Password Feedback title : High Priority : Description : Category : Request account creation Source scenario : Functional The chosen password should be verified by the user to avoid accidental irrevocable inputs . This can be achieved by having the user input their chosen password twice . 00 : 07 / 00 : 40 Start Screen Start Screen jane . doe @ gmail . com E - Mail Address : Password : seCur3P4ssw0rd Log out Change password A GUI unit test ( unselected ) Legend : A selected GUI unit test Executing direction Submit Create new scenario Back to overview ( b ) ( a ) Scenario Outline : Request account creation Given I am on the start screen When I choose " Create new user account " And I enter my < email > And I enter my < password > And I click the Submit button Then I see the Information " Please confirm account creation " in a new dialog window Examples : | email | password | | jane . doe @ gmail . com | seCur3P4ssw0rd | Fig . 4 . Mockup of connection graph view ( a ) and video view ( b ) . view ( Fig . 4b ) . Step 5 : In this view , the video and Gherkin speciﬁcations are displayed . If multiple scenarios were selected , their videos are played in succession ( indicated by the bold timeline markers in Fig . 4b ) . Gherkin steps are highlighted during replay . The necessary synchronization is based on timestamps that have been written into the videos’ metadata during video production . A consolidated video from our example is shown in Fig . 4b , where the ﬁrst selected scenario ( Request account creation ) is played . The password ﬁeld is highlighted because the “And I enter my < password > ” step is currently executing . In step 6 , stakeholders give feedback and express new requirements for the software . Figure 4b shows a feedback panel which contains some exemplary feedback regarding a website’s account creation process . Title , source scenario , priority , category and description can be speciﬁed , whereby the description can contain the problem ( what does the customer wish to change ? ) , the rationale ( why does the customer wish to change it ? ) , and a possible solution . This information is meant to primarily support the subsequent implementation process . Furthermore , after watching the video , stakeholders may have new ideas for requirements . These new requirements can be documented as Gherkin scenarios in another view by clicking “Create new scenario” . C . Technical Background of GUI Unit Tests in BDD This subsection describes how GUI unit tests are speciﬁed and which requirements they fulﬁll in a BDD project . This background information supports our purposed application in BDD and serves as an orientation for industrial practitioners . 1 ) Position of GUI Unit Tests in a BDD Project : Figure 5 shows the different layers of a BDD project that supports supplemented BDD processes . GUI unit tests are speciﬁed at the middle level and implemented at the bottom level . (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8) (cid:9)(cid:10)(cid:3)(cid:11)(cid:4)(cid:7)(cid:12)(cid:13)(cid:8) (cid:14)(cid:3)(cid:15)(cid:16)(cid:15)(cid:17)(cid:18)(cid:19)(cid:3)(cid:7)(cid:20)(cid:12)(cid:11)(cid:21) (cid:9)(cid:5)(cid:3)(cid:22)(cid:17)(cid:23)(cid:3)(cid:24)(cid:12)(cid:11)(cid:12)(cid:5)(cid:12)(cid:13)(cid:11)(cid:8) (cid:14)(cid:3)(cid:15)(cid:16)(cid:15)(cid:17)(cid:25)(cid:4)(cid:26)(cid:4)(cid:21) (cid:9)(cid:13)(cid:6)(cid:7)(cid:10)(cid:3)(cid:17)(cid:10)(cid:13)(cid:23)(cid:3) (cid:14)(cid:3)(cid:15)(cid:16)(cid:15)(cid:17)(cid:25)(cid:4)(cid:26)(cid:4)(cid:21) (cid:18)(cid:27)(cid:28) (cid:29)(cid:12)(cid:16)(cid:19)(cid:3)(cid:7)(cid:17)(cid:30)(cid:3)(cid:26)(cid:3)(cid:30)(cid:17)(cid:13)(cid:24)(cid:17)(cid:4)(cid:31)(cid:8)(cid:5)(cid:7)(cid:4)(cid:10)(cid:5)(cid:12)(cid:13)(cid:11) (cid:13) ! (cid:3)(cid:7)(cid:17)(cid:30)(cid:3)(cid:26)(cid:3)(cid:30)(cid:17)(cid:13)(cid:24)(cid:17) (cid:4)(cid:31)(cid:8)(cid:5)(cid:7)(cid:4)(cid:10)(cid:5)(cid:12)(cid:13)(cid:11) (cid:3)(cid:15)(cid:16)(cid:15)(cid:17)(cid:9)(cid:3)(cid:30)(cid:3)(cid:11)(cid:12)(cid:6) " (cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:28) " (cid:22)(cid:30)(cid:3) " (cid:3)(cid:11)(cid:5)(cid:12)(cid:11)(cid:16) (cid:18)(cid:27)(cid:28)(cid:17)(cid:6)(cid:11)(cid:12)(cid:5)(cid:17)(cid:5)(cid:3)(cid:8)(cid:5)(cid:8) (cid:9)(cid:22)(cid:3)(cid:10)(cid:12)(cid:24) # (cid:12)(cid:11)(cid:16) (cid:18)(cid:27)(cid:28)(cid:17)(cid:6)(cid:11)(cid:12)(cid:5)(cid:17)(cid:5)(cid:3)(cid:8)(cid:5)(cid:8) Fig . 5 . Layers of a BDD project . Layers can be implemented and connected through different frameworks and languages ( some examples are given ) . The highest , most abstract layer consists of proposed Fea - tures ( i . e . , software functionality [ 28 ] ) that can be developed and deployed independently of each other . A feature is doc - umented in a feature ﬁle , which contains title , description , and corresponding scenarios . Feature descriptions are typically documented in the same way as user stories . A number of scenarios are deﬁned for every feature . Scenarios specify the behavior of the software and are used as acceptance criteria . These scenarios are speciﬁcations of GUI unit tests , e . g . , Gherkin speciﬁcations . At the lowest level of abstraction , step deﬁnitions should be written in test code to implement speciﬁcations . This code tests functionalities in the source code or GUI control elements . GUI unit tests are implemented to execute steps which relate to the GUI . For example , Selenium 39 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . WebDriver can be used to implement GUI unit tests that automate GUI interactions for web applications . 2 ) Position of GUI Unit Tests in the BDD Test Pyramid : In Figure 6 we present an adapted test pyramid that is based on the test pyramid by Fowler [ 30 ] . It shows a possible distribution of tests in BDD . GUI unit tests are located at the bottom , just above unit tests . Generally , the higher a test type is located in the test pyramid , the more sparingly it is used in a software project . Unit tests are at the lowest level , which has the fastest execution speed . Unit tests should be strictly independent from each other . GUI unit tests and other automated acceptance tests are one level higher . These tests take longer to run than unit tests because it takes time to load and update the GUI . GUI unit tests are more integrated , i . e . , they test several parts of the system together . In addition , they are documented in a natural language , thereby supporting communication between stakeholders . GUI unit tests can be selected and arranged in such a way that they are presentable to stakeholders as a user journey ( a user’s path through the system ) . Exploratory tests , which are conducted to test all possible combinations of functionalities , are at the highest level . (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:5)(cid:8) (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:5)(cid:9)(cid:10)(cid:11)(cid:9)(cid:11) (cid:9) (cid:3)(cid:9)(cid:6)(cid:10)(cid:5) (cid:12) (cid:7)(cid:11)(cid:6) (cid:9) (cid:12)(cid:5)(cid:10)(cid:13) (cid:9) (cid:5)(cid:7)(cid:9) (cid:9) (cid:14)(cid:14)(cid:7)(cid:15)(cid:5) (cid:9) (cid:3)(cid:14)(cid:7)(cid:6)(cid:5)(cid:7)(cid:8)(cid:5)(cid:8) (cid:2)(cid:8)(cid:7)(cid:11)(cid:6)(cid:16)(cid:10)(cid:12)(cid:11)(cid:3)(cid:7)(cid:17)(cid:15)(cid:11)(cid:7)(cid:8)(cid:7)(cid:3)(cid:5) (cid:9) (cid:18)(cid:10)(cid:3) (cid:19)(cid:20)(cid:15)(cid:21)(cid:10)(cid:11) (cid:9) (cid:5)(cid:10)(cid:11)(cid:17) (cid:5)(cid:7)(cid:8)(cid:5)(cid:8) (cid:22)(cid:12)(cid:4)(cid:14)(cid:23)(cid:7) (cid:11) (cid:8)(cid:21)(cid:10)(cid:24)(cid:7)(cid:11) (cid:13)(cid:10)(cid:11)(cid:7)(cid:6)(cid:4)(cid:3)(cid:5)(cid:7)(cid:25)(cid:11) (cid:9) (cid:5)(cid:7)(cid:9) (cid:13)(cid:10)(cid:11)(cid:7)(cid:6)(cid:4)(cid:8)(cid:10)(cid:21) (cid:9) (cid:5)(cid:7)(cid:9) Fig . 6 . The adapted test pyramid . 3 ) Requirements of GUI Unit Tests in a BDD Project : A GUI unit test should satisfy the following requirements in order to provide ﬂexibility in development and clear reference in demonstration : 1 ) GUI unit tests are independent of each other , similar to unit tests . A GUI unit test should cover necessary system behaviors for a given program state , hence it can be executed on its own without being dependent on other test cases . Therefore , given steps which deﬁne preconditions should be used to set up the program state before executing the interaction . This allows developers or testers to change or delete tests without affecting the outcome of other tests . 2 ) A GUI unit test contains as few interaction steps as possible , since only a single GUI unit should be tested . According to the adapted test pyramid ( Fig . 6 ) , GUI unit tests are slower than regular unit tests , but fast enough to be executed regularly . A video is generated during GUI unit test execution . This way , the time cost of generating the videos is acceptable for developers and testers . The duration of the consolidated video is acceptable if a moderate number of GUI unit tests are selected for viewing . 3 ) For the scenario deﬁnition of a GUI unit test , an im - perative documentation style can simplify the traceability between Gherkin steps and video content . Highlighting these steps and their respective GUI elements is then possible , making it easier for stakeholders to recognize and understand GUI interactions . Figure 3 shows an example of an imperative - style Gherkin text of a GUI unit test that describes a successful sign - up for a newsletter . IV . E VALUATION In this evaluation we focus on the stakeholders’ perspective . We are interested in whether videos help stakeholders provide feedback on the software . Based on the goal template by Basili and Rombach [ 31 ] , we specify our evaluation goal as follows : Goal deﬁnition : We analyze videos of every single GUI unit test and a connection graph of these GUI unit tests for the purpose of evaluation with respect to the effectiveness of inconsistency identi - ﬁcation between given requirements and videos from the viewpoint of the stakeholders in the context of a controlled online experiment with Pro - liﬁc participants in demonstrating account management functions of an online website . A . User Study Design Based on the evaluation goal , we ask the following evalua - tion questions ( EQ ) : • EQ 1 : How can videos and the connection graph of GUI unit tests support a precise identiﬁcation of inconsisten - cies between requirements and the developed software ? • EQ 2 : How can videos and the connection graph of GUI unit tests facilitate giving feedback for stakeholders ? To answer these evaluation questions , we give all partic - ipants the same textual requirements to unify their under - standing . The behaviors of GUI unit tests are then demon - strated to the participants as Gherkin speciﬁcations or as Gherkin speciﬁcations with a video . The participants are asked to list inconsistencies between given requirements and the demonstrated behaviors . For example , if “delete account” is speciﬁed in the given requirements , but “deactivate account” is shown in the video , this difference is an inconsistency . Our second author ﬁrst familiarized himself with an English website that he had selected . He then speciﬁed behaviors for the account management section of the website as GUI unit tests in Gherkin . Afterwards , he speciﬁed those same functionalities in regular sentences ( as given requirements ) while introducing several inconsistencies . Our ﬁrst author then checked the given requirements and Gherkin speciﬁcations . Selecting these functionalities allowed us to ( 1 ) describe all functions as textual requirements in less than 350 words and 40 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . ( 2 ) describe each function in Gherkin texts of no more than 12 lines and in video of no more than 38 seconds . Thus , long viewing time due to lengthy texts or videos has been avoided . We prepared ten scenarios ( i . e . , GUI unit tests ) for the evaluation , e . g . , creating an account , resetting a password , changing a proﬁle picture . We want to investigate the effect of the videos and graph compared to the Gherkin speciﬁcations . Hence , participants are separated into two groups : 1 . The control group receives only the Gherkin speciﬁcations , as those are available and up - to - date in BDD per default ; 2 . The experimental group receives the Gherkin speciﬁcations and , in addition , the videos and connection graph of the GUI unit tests , as we propose in Sec . III . Table III shows the designed metrics for EQ 1 and EQ 2 . The metrics M 1 , M 2 are calculated by counting inconsistencies from collected answers and are thus objective . The metrics M 3 , M 4 , M 5 measure subjective perceptions and are collected using an 8 - point Likert scale , because ( 1 ) given an even num - ber of options , participants should have a clear opinion acting as customers ; and ( 2 ) eight options allow the participants to give their opinions precisely . TABLE III M ETRICS FOR THE E VALUATION Q UESTIONS EQ Metrics EQ 1 M 1 : Number of correctly identiﬁed inconsistencies M 2 : Number of incorrectly identiﬁed inconsistencies EQ 2 M 3 : Average certainty of mentioning inconsistencies ( from 1 : very uncertain to 8 : very certain ) M 4 : Difﬁculty of identifying inconsistencies ( from 1 : very easy to 8 : very difﬁcult ) M 5 : Understanding of development status ( from 1 : very limited to 8 : very extensive ) We are interested in testing the following hypotheses : There is a difference in H 1 : the number of correctly identiﬁed inconsistencies ( M 1 ) H 2 : the number of incorrectly identiﬁed inconsistencies ( M 2 ) H 3 : the average certainty of giving inconsistencies ( M 3 ) H 4 : the difﬁculty of identifying inconsistencies ( M 4 ) H 5 : the understanding of the development status ( M 5 ) between the provision of Gherkin speciﬁcations and the provi - sion of Gherkin speciﬁcations , connection graph , and videos . The corresponding null hypotheses assume that there is no difference between both provisions in the respective metrics . Figure 8 shows the steps of the experiment . In advance , par - ticipants should understand the Gherkin speciﬁcations . There - fore , Gherkin is explained in an example , followed by two multi - select multiple - choice questions to test comprehension . The questions are : “1 . Which of the following keywords are always included in a scenario ( according to the text above ) ? 2 . Which of the following statements is / are correct ? ” Afterwards , the textual requirements ( in regular sentences , not in Gherkin ) are shown , followed by one multi - select multiple - choice question : “Which of the following statements is / are correct ? ” If participants are unable to answer the test questions correctly , the experiment must not be continued , as we assume that they have not understood the basics . Next , the GUI unit tests are demonstrated . Participants in both groups receive the Gherkin speciﬁcations for every sce - nario . The experimental group receives one connection graph before seeing the ﬁrst scenario . This connection graph conveys relationships between the ten scenarios by displaying some exemplary execution sequences ( as shown in Fig . 4a ) . After that , the GUI unit tests are demonstrated ( 1 ) through Gherkin speciﬁcations for the control group , ( 2 ) through Gherkin speci - ﬁcations and GUI unit test videos for the experimental group . At the end of each scenario page , the participants of both groups are asked to state the inconsistencies they found ( M 1 , M 2 ) , as well as their answer certainty ( M 3 ) . Figure 7 shows scenario four from the translated questionnaire ( experimental group ) . The original instructions for the experiment were written in German and are available in our data set [ 32 ] . (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:9) (cid:12) (cid:7) (cid:13)(cid:14)(cid:15)(cid:16) (cid:4) (cid:13)(cid:14)(cid:17)(cid:14)(cid:18) (cid:8)(cid:10) (cid:2) (cid:3) (cid:25)(cid:4)(cid:5) (cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6) (cid:3) (cid:7)(cid:7) (cid:3) (cid:8)(cid:9)(cid:10)(cid:11)(cid:2)(cid:12)(cid:4)(cid:5)(cid:13)(cid:14)(cid:9)(cid:10)(cid:2)(cid:15)(cid:5)(cid:16)(cid:15)(cid:2)(cid:17)(cid:10)(cid:18)(cid:2)(cid:19)(cid:9)(cid:18)(cid:5) (cid:3) (cid:2)(cid:18)(cid:5)(cid:20)(cid:9)(cid:21)(cid:15)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:22)(cid:17)(cid:23)(cid:5)(cid:2)(cid:22)(cid:21)(cid:5)(cid:10)(cid:17)(cid:13)(cid:9) (cid:3) (cid:2)(cid:9)(cid:10)(cid:2)(cid:18)(cid:9)(cid:24)(cid:5)(cid:13)(cid:5)(cid:10)(cid:15)(cid:2)(cid:8)(cid:17)(cid:25)(cid:22)(cid:26)(cid:2) (cid:27) (cid:3) (cid:2)(cid:10) (cid:3) (cid:15)(cid:2)(cid:21) (cid:3) (cid:23)(cid:20)(cid:17)(cid:13)(cid:5)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:12)(cid:4)(cid:5)(cid:13)(cid:14)(cid:9)(cid:10)(cid:2)(cid:15)(cid:5)(cid:16)(cid:15)(cid:2)(cid:8)(cid:9)(cid:15)(cid:4)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:19)(cid:9)(cid:18)(cid:5) (cid:3) (cid:28)(cid:2)(cid:29)(cid:30)(cid:15)(cid:2)(cid:21) (cid:3) (cid:23)(cid:20)(cid:17)(cid:13)(cid:5)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:15)(cid:5)(cid:16)(cid:15)(cid:2)(cid:31)(cid:2)(cid:19)(cid:9)(cid:18)(cid:5) (cid:3) (cid:2)(cid:8)(cid:9)(cid:15)(cid:4)(cid:2) (cid:15)(cid:4)(cid:5)(cid:2)(cid:13)(cid:5) (cid:30)(cid:9)(cid:13)(cid:5)(cid:23)(cid:5)(cid:10)(cid:15)(cid:22)(cid:2) ! (cid:17)(cid:29) (cid:3) (cid:19)(cid:5) " (cid:26) # (cid:21)(cid:5)(cid:10)(cid:17)(cid:13)(cid:9) (cid:3) $ (cid:2) % (cid:5)(cid:22)(cid:5)(cid:15)(cid:2)(cid:20)(cid:17)(cid:22)(cid:22)(cid:8) (cid:3) (cid:13)(cid:18) (cid:2)(cid:2)(cid:12)(cid:9)(cid:19)(cid:5)(cid:10)(cid:2) & (cid:2)(cid:17)(cid:23)(cid:2) (cid:3) (cid:10)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:7) (cid:3) (cid:11)(cid:9)(cid:10)(cid:2)(cid:20)(cid:17)(cid:11)(cid:5) (cid:2)(cid:2)’(cid:10)(cid:18)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:6) (cid:3) (cid:7)(cid:7) (cid:3) (cid:8)(cid:9)(cid:10)(cid:11)(cid:2)(cid:17)(cid:21)(cid:21) (cid:3) (cid:30)(cid:10)(cid:15)(cid:2)(cid:5)(cid:16)(cid:9)(cid:22)(cid:15)(cid:22) $ (cid:2)(cid:2)(cid:2)(cid:2) ( (cid:2) ) (cid:23)(cid:17)(cid:9)(cid:7) (cid:2) ( (cid:2) * (cid:22)(cid:5)(cid:13)(cid:10)(cid:17)(cid:23)(cid:5) ( (cid:2) + (cid:17)(cid:22)(cid:22)(cid:8) (cid:3) (cid:13)(cid:18) ( (cid:2)(cid:2)(cid:2)(cid:2) ( (cid:2)(cid:23)(cid:17)(cid:16)(cid:9)(cid:26)(cid:23)(cid:30)(cid:22)(cid:15)(cid:5)(cid:13) , (cid:11)(cid:23)(cid:17)(cid:9)(cid:7)(cid:26)(cid:21) (cid:3) (cid:23) ( (cid:2) / (cid:17)(cid:16)(cid:9) / (cid:30)(cid:22)(cid:15)(cid:5)(cid:13)(cid:2) ( (cid:2) + 0 (cid:22)(cid:22)(cid:8) 1 (cid:13)(cid:15)(cid:2) ( (cid:2)(cid:2) 2 (cid:4)(cid:5)(cid:10)(cid:2) & (cid:2)(cid:22)(cid:5)(cid:7)(cid:5)(cid:21)(cid:15)(cid:2) 3 % (cid:5)(cid:22)(cid:5)(cid:15) 3 (cid:2)(cid:2)’(cid:10)(cid:18)(cid:2) & (cid:2)(cid:5)(cid:10)(cid:15)(cid:5)(cid:13)(cid:2)(cid:23)(cid:25)(cid:2)(cid:5)(cid:23)(cid:17)(cid:9)(cid:7)(cid:2) 3 (cid:23)(cid:17)(cid:16)(cid:9)(cid:26)(cid:23)(cid:30)(cid:22)(cid:15)(cid:5)(cid:13) , (cid:11)(cid:23)(cid:17)(cid:9)(cid:7)(cid:26)(cid:21) (cid:3) (cid:23) 3 (cid:2)(cid:2)’(cid:10)(cid:18)(cid:2) & (cid:2)(cid:22)(cid:30)(cid:29)(cid:23)(cid:9)(cid:15)(cid:2)(cid:23)(cid:25)(cid:2)(cid:9)(cid:10)(cid:20)(cid:30)(cid:15) (cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:10)(cid:2) & (cid:2)(cid:17)(cid:23)(cid:2)(cid:13)(cid:5)(cid:18)(cid:9)(cid:13)(cid:5)(cid:21)(cid:15)(cid:5)(cid:18)(cid:2)(cid:15) (cid:3) (cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:22)(cid:15)(cid:17)(cid:13)(cid:15)(cid:2)(cid:20)(cid:17)(cid:11)(cid:5)(cid:2)(cid:15)(cid:4)(cid:17)(cid:15)(cid:2)(cid:22)(cid:4) (cid:3) (cid:8)(cid:22)(cid:2)(cid:15)(cid:4)(cid:5)(cid:2)(cid:7)(cid:17)(cid:15)(cid:5)(cid:22)(cid:15)(cid:2)(cid:20) (cid:3) (cid:20)(cid:30)(cid:7)(cid:17)(cid:13)(cid:2) 4 (cid:7)(cid:23)(cid:22) (cid:19) (cid:9) (cid:14) (cid:10)(cid:7)(cid:10)(cid:5) (cid:14)(cid:18)(cid:20)(cid:13) (cid:4)(cid:9)(cid:7) (cid:21) (cid:7) (cid:22)(cid:23) (cid:11) (cid:14)(cid:13) (cid:6)(cid:4) (cid:18) (cid:7)(cid:8) (cid:14)(cid:24) (cid:8)(cid:7) (cid:25) (cid:7) (cid:26) (cid:4) (cid:27)(cid:14) (cid:9) (cid:28) (cid:7) (cid:17)(cid:20) (cid:8)(cid:5)(cid:11)(cid:7)(cid:8)(cid:11) (cid:14) (cid:7) (cid:13)(cid:14)(cid:15)(cid:16) (cid:4) (cid:13)(cid:14)(cid:17)(cid:14)(cid:18) (cid:8)(cid:10) (cid:29) & (cid:6)(cid:2) (cid:15)(cid:4)(cid:5)(cid:2) (cid:22)(cid:21)(cid:5)(cid:10)(cid:17)(cid:13)(cid:9) (cid:3) (cid:2) (cid:23)(cid:17)(cid:15)(cid:21)(cid:4)(cid:5)(cid:22)(cid:2) (cid:15)(cid:4)(cid:5)(cid:2) (cid:13)(cid:5) (cid:30)(cid:9)(cid:13)(cid:5)(cid:23)(cid:5)(cid:10)(cid:15)(cid:22)(cid:28)(cid:2) (cid:20)(cid:7)(cid:5)(cid:17)(cid:22)(cid:5)(cid:2) (cid:8)(cid:13)(cid:9)(cid:15)(cid:5)(cid:2) 3 / (cid:17)(cid:15)(cid:21)(cid:4)(cid:5)(cid:22)(cid:2) (cid:17)(cid:7)(cid:7)(cid:2) ! # 5 (cid:23)(cid:23)(cid:15)(cid:2) 6 (cid:29)(cid:5)(cid:13)(cid:5)(cid:9)(cid:10) " 3 (cid:26)(cid:2) 7 (cid:15)(cid:4)(cid:5)(cid:13)(cid:8)(cid:9)(cid:22)(cid:5)(cid:28)(cid:2)(cid:18)(cid:5)(cid:22)(cid:21)(cid:13)(cid:9)(cid:29)(cid:5)(cid:2)(cid:17)(cid:7)(cid:7)(cid:2)(cid:9)(cid:10)(cid:21) (cid:3) (cid:10)(cid:22)(cid:9)(cid:22)(cid:15)(cid:5)(cid:10)(cid:21)(cid:9)(cid:5)(cid:22)(cid:2)(cid:15)(cid:4)(cid:17)(cid:15)(cid:2)(cid:25) (cid:3) (cid:30)(cid:2)(cid:10) (cid:3)5 (cid:21)(cid:5)(cid:26) (cid:30) (cid:9) (cid:12) (cid:7)(cid:5) (cid:14)(cid:13) (cid:8) (cid:20) (cid:4) (cid:18) (cid:7) (cid:20)(cid:13)(cid:14) (cid:7) (cid:31) (cid:9) (cid:16) (cid:7)(cid:9) (cid:18) (cid:7) (cid:20)(cid:18) (cid:10) (cid:12)(cid:14)(cid:13) (cid:4) (cid:18) (cid:7)(cid:8)(cid:11) (cid:14) (cid:7)(cid:3) (cid:20) (cid:10)(cid:8)(cid:7) (cid:15)(cid:16)(cid:14) (cid:10) ! (cid:9) (cid:18)(cid:29) (cid:19)(cid:5)(cid:13)(cid:25)(cid:2)(cid:30)(cid:10)(cid:21)(cid:5)(cid:13)(cid:15)(cid:17)(cid:9)(cid:10) (cid:19)(cid:5)(cid:13)(cid:25)(cid:2)(cid:21)(cid:5)(cid:13)(cid:15)(cid:17)(cid:9)(cid:10) Fig . 7 . One page in the questionnaire ( scenario 4 , experimental group ) . After checking the tenth scenario , participants are asked to answer the questions “Overall , how difﬁcult was it to recognize inconsistencies ? ” ( M 4 , question ID ProcEval1 [ 32 ] ) and “After showing you ten scenarios : How do you assess your understanding of the functionality and usage of the presented functions ? ” ( M 5 , question ID ProcEval2 [ 32 ] ) . At last , participants ﬁll in their demographic data and assess their answer quality ( seriousness , distraction , understanding of instructions ) . To encourage honest responses , we inform participants that their quality assessment responses have no consequences . 41 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . (cid:2)(cid:3) (cid:2)(cid:3)(cid:4) (cid:3) (cid:5)(cid:6)(cid:7)(cid:4) (cid:3) (cid:8)(cid:9) (cid:10)(cid:11)(cid:12)(cid:4) (cid:15) (cid:13)(cid:2)(cid:6)(cid:3)(cid:4)(cid:14)(cid:13)(cid:2)(cid:13)(cid:2)(cid:7)(cid:6)(cid:14)(cid:2) (cid:18) (cid:6) (cid:15) (cid:3) (cid:16)(cid:9)(cid:4)(cid:12)(cid:11)(cid:12)(cid:2)(cid:17)(cid:13) (cid:3) (cid:2)(cid:6) (cid:3)(cid:12)(cid:17)(cid:3) (cid:18)(cid:4) (cid:3) (cid:19)(cid:13)(cid:17)(cid:13) (cid:3) (cid:2)(cid:6) (cid:3) (cid:20)(cid:6)(cid:6) (cid:4)(cid:12)(cid:21)(cid:8)(cid:13)(cid:4)(cid:12)(cid:16)(cid:12)(cid:2)(cid:17)(cid:6)(cid:13)(cid:2)(cid:6)(cid:4)(cid:12)(cid:7)(cid:8)(cid:5)(cid:14)(cid:4)(cid:6)(cid:17)(cid:12)(cid:2)(cid:3)(cid:12)(cid:2)(cid:15)(cid:12)(cid:17)(cid:6)(cid:14)(cid:2) (cid:18) (cid:6) (cid:15) (cid:3) (cid:16)(cid:9)(cid:4)(cid:12)(cid:11)(cid:12)(cid:2)(cid:17)(cid:13) (cid:3) (cid:2)(cid:6) (cid:3)(cid:12)(cid:17)(cid:3) (cid:18)(cid:4) (cid:3) (cid:19)(cid:13)(cid:17)(cid:13) (cid:3) (cid:2)(cid:6) (cid:3) (cid:20)(cid:6)(cid:10)(cid:11)(cid:12)(cid:4) (cid:15) (cid:13)(cid:2)(cid:6)(cid:17)(cid:9)(cid:12)(cid:15)(cid:22) (cid:18)(cid:4) (cid:3) (cid:19)(cid:13)(cid:17)(cid:13) (cid:3) (cid:2)(cid:6) (cid:3) (cid:20)(cid:6)(cid:10)(cid:11)(cid:12)(cid:4) (cid:15) (cid:13)(cid:2)(cid:6)(cid:17)(cid:9)(cid:12)(cid:15)(cid:22)(cid:23)(cid:6) (cid:15) (cid:3) (cid:2)(cid:2)(cid:12)(cid:15)(cid:24) (cid:3) (cid:2)(cid:6)(cid:7)(cid:4)(cid:14)(cid:9)(cid:11)(cid:23)(cid:6)(cid:14)(cid:2) (cid:18) (cid:6) (cid:19)(cid:13) (cid:18) (cid:12) (cid:3) (cid:17) (cid:25)(cid:17) (cid:15) (cid:13)(cid:2)(cid:7)(cid:6)(cid:20) (cid:3) (cid:4)(cid:6) (cid:13)(cid:2)(cid:15) (cid:3) (cid:2)(cid:17)(cid:13)(cid:17)(cid:3)(cid:12)(cid:2)(cid:15)(cid:13)(cid:12)(cid:17)(cid:6) (cid:14)(cid:2) (cid:18) (cid:6)(cid:15)(cid:12)(cid:4)(cid:3)(cid:14)(cid:13)(cid:2)(cid:3)(cid:26)(cid:6) (cid:3) (cid:20)(cid:6) (cid:3)(cid:11)(cid:12)(cid:6)(cid:14)(cid:2)(cid:17)(cid:27)(cid:12)(cid:4)(cid:6)(cid:28)(cid:20) (cid:3) (cid:4)(cid:6) (cid:12)(cid:14)(cid:15)(cid:11)(cid:6)(cid:17)(cid:15)(cid:12)(cid:2)(cid:14)(cid:4)(cid:13) (cid:3) (cid:29) (cid:18)(cid:4) (cid:3) (cid:15)(cid:12)(cid:17)(cid:17)(cid:6) (cid:14)(cid:17)(cid:17)(cid:12)(cid:17)(cid:17)(cid:16)(cid:12)(cid:2)(cid:3) (cid:25)(cid:17) (cid:15) (cid:13)(cid:2)(cid:7)(cid:6)(cid:20) (cid:3) (cid:4)(cid:6) (cid:18) (cid:12)(cid:16) (cid:3) (cid:30) (cid:7)(cid:4)(cid:14)(cid:9)(cid:11)(cid:13)(cid:15)(cid:6) (cid:18) (cid:14)(cid:3)(cid:14) (cid:31)(cid:12)(cid:5)(cid:20)(cid:6) (cid:14)(cid:17)(cid:17)(cid:12)(cid:17)(cid:17)(cid:16)(cid:12)(cid:2)(cid:3)(cid:6)(cid:13)(cid:2)(cid:6)(cid:14)(cid:2)(cid:17)(cid:27)(cid:12)(cid:4)(cid:6)(cid:21)(cid:8)(cid:14)(cid:5)(cid:13)(cid:3)(cid:26) ! (cid:9)(cid:12)(cid:4)(cid:13)(cid:16)(cid:12)(cid:2)(cid:3)(cid:14)(cid:5)(cid:6)(cid:7)(cid:4) (cid:3) (cid:8)(cid:9) Fig . 8 . General experiment design . ( Note : Spec . means Speciﬁcations ) Participation in the experiment was rewarded because we wanted to get as many potential participants as possible . The payment for one person was 5 . 00 GBP for the control group and 5 . 50 GBP for the experimental group . The number of participants in each group was limited to eleven . We have found participants through an online platform . 1 We set the following criteria on the platform to ﬁlter participants : residence in Germany , German as ﬁrst language , and very good English skills . This way , we ensured that participants can follow German instructions , understand English text in the software , and write texts in German . The platform then sent invitations to qualiﬁed participants . Participants could conduct the experiment if they received an invitation and participation spaces were still available . The experiment was conducted on an online survey platform . 2 B . Demographics For each group , 11 participants took part in the experiment . The gender distribution is almost equal ( 10 female , 12 male ) . The participants belong to different age groups , with the youngest being 19 and the oldest 60 years old . The average age is 28 . 7 . In terms of educational level , all participants have at least a vocational baccalaureate diploma ( Fachabitur in German ) . None of the participants have extensive knowledge of programming or BDD . C . Data Analysis The self assessment of answer quality is adequate for checking hypotheses . According to the submitted answers , all participants have answered the questions seriously and understood the instructions . Almost all participants reported that they performed the experiment without distraction ; only one reported a brief distraction . The second author ﬁrst counted metrics M 1 and M 2 for the submitted inconsistencies . Participants were asked to identify inconsistencies from demonstrated behaviors which are not in accordance with the given requirements . We count a correct inconsistency if a participant states a real difference between requirements and demonstrated behaviors . We count an incor - rect inconsistency if we recognize that the participant states an inconsistency which is not related to the corresponding requirement . Then , the ﬁrst author checked and corrected the counting and asked for conﬁrmation from the second author . Only one disagreement was left after the conﬁrmation : this was settled by consulting the third author . The exact counting reference is available in the survey data [ 32 ] ( sheet “Counting 1 https : / / www . proliﬁc . co / researchers 2 https : / / www . soscisurvey . de Detail” ) . The mean and standard deviation for M 1 and M 2 are calculated between all participants . To calculate M 3 , the median value of all assessments of certainty on giving inconsistencies is calculated for each participant respectively . M 4 and M 5 are collected through answers to the mentioned questions about the difﬁculty of ﬁnding inconsistencies and the understanding of the devel - opment status . As these metrics are collected via Likert scale , we calculate only their median for further interpretation . To calculate the Mann - Whitney U test for all metrics , we follow the instructions from Bortz and Schuster [ 33 ] for small samples of shared rankings . For the sake of transparency , the individual calculation steps for this test are provided in the data set [ 32 ] . According to the hypothesis deﬁnitions , all hypotheses are two - tailed . For interpretation , we set the signiﬁcance level α to 0 . 05 . Table IV shows the statistical results . TABLE IV S TATISTICAL R ESULTS FOR A LL M ETRICS Metric Control Group Exp . Group Mann - Whitney U Test Mean SD Mean SD U p M 1 1 . 91 1 . 64 5 . 73 4 . 88 26 . 5 0 . 024 M 2 0 . 36 0 . 81 0 . 36 0 . 50 53 . 5 0 . 557 Median Median U p M 3 7 7 30 . 5 0 . 022 M 4 5 4 30 . 0 0 . 037 M 5 5 6 23 . 0 0 . 011 SD : Standard Deviation Two - tailed , signiﬁcance level α = 0 . 05 D . Interpretation and Answers for the Evaluation Questions Regarding M 1 , a signiﬁcant difference between both groups is observed ( p = 0 . 024 ) . The mean number of correctly identi - ﬁed inconsistencies is higher in the experimental group than in the control group . H 1 is accepted . In contrast , results of M 2 show no signiﬁcant difference ( p = 0 . 557 ) . Most ( 16 ) participants did not falsely identify inconsistencies . Two par - ticipants in the control group stated two false inconsistencies each , while four participants in the experimental group stated one false inconsistency each . Therefore , we cannot reject the null hypothesis of H 2 . Overall , we answer EQ 1 as follows : The difference that we have found in the mean number of correctly identiﬁed inconsistencies suggests that the GUI unit test videos and connection graph help stakeholders recognize inconsistencies with stakeholders’ requirements . For M 3 , a signiﬁcant difference is measured ( p = 0 . 022 ) . H 3 is accepted . However , we cannot infer the difference direction in M 3 as the medians of both groups are the same . Results 42 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . from M 4 show a signiﬁcant difference ( p = 0 . 037 ) . Partici - pants from the control group tend to think that inconsistency recognition is difﬁcult when viewing Gherkin speciﬁcations only ( Median = 5 ) . H 4 is accepted . Regarding M 5 , another signiﬁcant difference is observed ( p = 0 . 011 ) . Participants of both groups can understand the current development status . However , the experimental group has a greater median value of M 5 than the control group . H 5 is accepted . Overall , we answer EQ 2 as follows : The videos and connection graph of GUI unit tests appear to facilitate stakeholder feedback in two ways : stakeholders can ( 1 ) recognize inconsistencies easily and ( 2 ) understand the current development status extensively . E . Discussion We discuss some other results found in the experiment and answer our research question . 1 ) Inconsistencies Found by Participants : To count M 1 and M 2 , we originally used a standard solution of 24 inconsisten - cies . We found another 5 correct inconsistencies ( shown as blue text in the survey data [ 32 ] , sheet “Counting Detail” ) in answers from participants of the experimental group . One inconsistency is “The standard avatar is grey ( instead of blue ) ” from participant 14 in the experimental group . Given the video , participants found inconsistencies which were not intentionally designed before conducting the experiment . This supports H 1 . 2 ) Time Cost on Viewing Videos : The length of the 10 videos used in the experiment ranges from 18 to 38 seconds . We measured session duration , i . e . , how long it took a par - ticipant to complete the entire online experiment . The results show that the control group took an average of 21 . 0 minutes and the experimental group took an average of 31 . 7 minutes per person . We can infer that the participants may have used the 10 . 7 minutes for viewing the 10 videos . This estimated time cost is acceptable . 3 ) Effect of the Connection Graph and Videos : In the experimental group , participants were shown the connection graph of the GUI unit tests once before the ﬁrst scenario was shown . Then they were asked to identify inconsistencies for each scenario separately , while the video was shown . The correctly identiﬁed inconsistencies ( M 1 ) should not be related to other scenarios , i . e . , other GUI unit tests . Hence , we argue that mainly the video helped participants to ﬁnd more correct inconsistencies ( M 1 ) . For other metrics , the connection diagram and videos should have a combined effect . However , the effect of the connection graph itself is not separately investigated . By selecting an individual continuous GUI unit test ﬂow , stakeholders can view a personalized video and can give feedback for a speciﬁc use case . The interaction with a connection graph and its effect on requirements elicitation is worth investigating in a future study . 4 ) Applicability in Agile or Other Software Development : We suggest an applicability matrix ( Fig . 9 ) that indicates the applicability of our approach . The crucial two dimensions are : 1 . When are GUI tests available for video creation ? ( X axis ) 2 . How fast can the development team implement updated requirements when stakeholders provide feedback on videos ? ( Y axis ) " # (cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6) (cid:9) (cid:7)(cid:8)(cid:2)(cid:9) (cid:9) (cid:10)(cid:2) (cid:11)(cid:12)(cid:13) (cid:9) (cid:8)(cid:14)(cid:13)(cid:2)(cid:15)(cid:14)(cid:16)(cid:11)(cid:17)(cid:15)(cid:14)(cid:18)(cid:14)(cid:10)(cid:8)(cid:7)(cid:2) (cid:19)(cid:14)(cid:2)(cid:17)(cid:18)(cid:12)(cid:20)(cid:14)(cid:18)(cid:14)(cid:10)(cid:8)(cid:14)(cid:13)(cid:21) (cid:22)(cid:23)(cid:2)(cid:24)(cid:25)(cid:14)(cid:10)(cid:2) (cid:9) (cid:15)(cid:14)(cid:2)(cid:26)(cid:27)(cid:28)(cid:2)(cid:8)(cid:14)(cid:7)(cid:8)(cid:7)(cid:2) (cid:9) (cid:29) (cid:9) (cid:17)(cid:20) (cid:9) (cid:19)(cid:20)(cid:14)(cid:21) (cid:14) (cid:9) (cid:15)(cid:20)(cid:30)(cid:2)(cid:18)(cid:17)(cid:13)(cid:13)(cid:20)(cid:14)(cid:2)(cid:2)(cid:14)(cid:10)(cid:13) (cid:7)(cid:20)(cid:4)(cid:5) (cid:6) (cid:9) (cid:7)(cid:8) (cid:18)(cid:17)(cid:13)(cid:13)(cid:20)(cid:14) (cid:2)(cid:3)(cid:4)(cid:5)(cid:6) (cid:22) (cid:31) Fig . 9 . Applicability matrix . In this paper , we have proposed our concept mainly for agile development . We argue that our concept is best applicable to agile development , because ( 1 ) GUI tests can be implemented during agile development and presented as videos to obtain feedback , and ( 2 ) obtained feedback can be considered or implemented in the next iterations . In the applicability matrix ( Fig . 9 ) , agile development is depicted as a dashed circle in the upper left corner . Our concept can also be applied to de - velopment methods that are located close to agile development in the applicability matrix ( green area in Fig . 9 ) , e . g . , hybrid development , where GUI tests are available in the early or middle stages of the development and changes in requirements can be considered in time . Answer to Research Question : GUI tests are divided into several GUI unit tests , which are used to create videos . Videos are recorded in a modiﬁed test run . The test steps are explained in a natural language while viewing the video . Stakeholders view a connection graph and videos of the GUI unit tests . This way , stakeholders can effec - tively give feedback to a software under development . F . Threats to Validity Our results are subject to some threats to validity which we discuss in the following according to Wohlin et al . [ 34 ] . 1 ) Construct Validity : In the online questionnaire , partici - pants could not conduct German tasks with the software , since it was only available in English . To mitigate this threat , we set criteria on the survey platform to only select participants who have the required language skills . One aspect that may have affected the validity of the results is that participants may have misunderstood the tasks . To mitigate this threat , we conducted a pilot study with two par - ticipants and adjusted the instructions based on their feedback . This way , we were able to resolve some misunderstandings . Furthermore , participants were given textual requirements which they did not collect or formulate on their own . There - fore , we could not assume that participants compared the video to the given requirements instead of their own requirements . To mitigate this threat , we presented a hint every time a new video was shown . This way , we made the given textual requirements easily accessible . 43 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Due to the nature of an online experiment , participants might have been interrupted by their environment ( e . g . , phone calls , door bells , etc . ) . To analyze the inﬂuence of such interruptions , we investigated the quality of the given answers . As only one participant experienced one short distraction and others were not distracted according to their answers to the question “Were you able to complete the questionnaire without being distracted ? ” ( question ID SelfAssess2 [ 32 ] ) , we consider this threat to be of small relevance . In addition , participants might have been tired which could have had an inﬂuence on their judgment . To reduce the risk of tiring the participants while doing the experiment , we described functionalities in short texts and opted for a number of ten scenarios , leading to an average duration of 26 . 4 min per experiment . As we assume this session duration to be adequate , we consider the loss of concentration during the experiment to have had a low inﬂuence . Nevertheless , we could not counteract tiredness caused by external factors such as participating early in the morning , or in a stressful phase . The payment for participants may threaten the validity , be - cause it cannot motivate participants intrinsically . Participants took part in the experiment if they accepted the invitations and free spaces were still available . We cannot inﬂuence the selection of subjects because this is dependent on the platform and the initiative of participants . 2 ) Internal Validity : We used the Gherkin language to specify GUI unit tests . We must not assume that participants are familiar with this language . Therefore , it is possible that they do not understand the Gherkin text . To mitigate this threat , we performed a Gherkin training with two control questions . Participants had to answer these questions correctly to con - tinue the experiment . Furthermore , we asked participants if they were able to follow the instructions . In case of difﬁculties with the given tasks of the experiment , participants were asked to communicate these difﬁculties at the end of the study . All participants understood the instructions according to question SelfAssess3 [ 32 ] and have written no comments regarding any difﬁculties . In the experimental group , videos may show participants functionalities that are not described in the given requirements . Participants may then write these functions as inconsistencies , which affect the collected identiﬁcations with respect to val - idating H 1 , H 2 , H 3 , and H 4 . To mitigate this , participants were given the following instruction : “In the videos , you may sometimes see functionalities that were not discussed in the requirements . Disregard these functionalities when looking for inconsistencies . ” We do not see these kinds of requirements in any submitted answers . 3 ) Conclusion Validity : Our results are based on insights from 22 participants . To analyze the results objectively , we applied hypotheses testing at a signiﬁcance level of α = 0 . 05 . In our experiment , the videos were created manually , not from a modiﬁed GUI unit test . This might threaten the validity of answers to research and evaluation questions . To mitigate this , the second author created the video while slowly interacting with the software so that viewers can follow . In further video editing , he added rectangles to highlight every GUI interaction , as suggested by Shi and Schneider [ 24 ] . All videos are available in our data set [ 32 ] . An imprecise counting of M 1 and M 2 might be another threat . If these metrics are counted by one person , errors may happen . The second and ﬁrst authors double - checked the results to mitigate this threat . The third author gave her opinion on one disagreement so that the counting was uniﬁed . 4 ) External Validity : Our results must not be over - interpreted and generalized . The main limitation affecting the generalizability of our results is the lack of developer perspective . In addition , we only applied our concept in an experimental setting , with limited practical relevance . Hence , future studies are required to evaluate the beneﬁts for the industry and the applicability in a real - world setting . V . C ONCLUSION AND F UTURE W ORK In agile development , feedback should be elicited in short intervals and fed into development . Stakeholders who might not use the software directly can give valuable feedback . We want to involve these stakeholders by using successful GUI tests for software demonstration . We have used existing suc - cessful GUI tests in agile development and proposed concepts to demonstrate software under development . We have applied these concepts to BDD , where acceptance criteria are written ﬁrst . GUI unit tests are speciﬁed in Gherkin text according to the acceptance criteria and then implemented . By reviewing the consolidated video , stakeholders can give feedback . We conducted a dedicated experiment from the perspective of the stakeholders to investigate the effect of videos in identi - fying inconsistencies between given requirements and demon - strated behaviors . Compared to the control group , participants in the experimental group received a connection graph and videos . Results with statistical signiﬁcance have shown that ( 1 ) videos seem to help stakeholders ﬁnd inconsistencies in the demonstrated software that do not match the given requirements and ( 2 ) videos and a connection graph of the videos seem to help stakeholders recognize inconsistencies easily and understand demonstrated functionalities extensively . Our concepts can be applied in agile or hybrid development . Moreover , the use of GUI test videos could be appropriate for remote work communication between distributed stakeholders . The effects of active selection of the connection graph and consolidated video by stakeholders can be studied in the future . Furthermore , our proposed concept should be evaluated from the developer’s perspective to check its feasibility and return on investment . Obtaining early feedback through GUI tests can contribute to stakeholder satisfaction and successful software projects . A CKNOWLEDGMENT This work is funded by Deutsche Forschungsgemeinschaft ( DFG ) - Project number 289386339 ( ViViUse ) . 44 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . R EFERENCES [ 1 ] M . Glinz and R . J . Wieringa , “Stakeholders in requirements engineering , ” IEEE Software , vol . 24 , no . 2 , p . 18 – 20 , Mar 2007 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 4118646 [ 2 ] E . Bjarnason , K . Wnuk , and B . Regnell , “Requirements are slipping through the gaps - a case study on causes & effects of communication gaps in large - scale software development , ” in 2011 IEEE 19th International Requirements Engineering Conference . Trento , Italy : IEEE , Aug 2011 , p . 37 – 46 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 6051639 / [ 3 ] U . Abelein and B . Paech , “State of practice of user - developer communication in large - scale it projects : Results of an expert interview series , ” in Requirements Engineering : Foundation for Software Quality . Cham : Springer International Publishing , 2014 , p . 95 – 111 . [ Online ] . Available : http : / / link . springer . com / 10 . 1007 / 978 - 3 - 319 - 05843 - 6 8 [ 4 ] M . Niazi , S . Mahmood , M . Alshayeb , M . R . Riaz , K . Faisal , N . Cerpa , S . U . Khan , and I . Richardson , “Challenges of project management in global software development : A client - vendor analysis , ” Information and Software Technology , vol . 80 , p . 1 – 19 , Dec 2016 . [ Online ] . Available : https : / / doi . org / 10 . 1016 / j . infsof . 2016 . 08 . 002 [ 5 ] J . Nicolas , J . M . Carrillo De Gea , B . Nicolas , J . L . Fernandez - Aleman , and A . Toval , “On the risks and safeguards for requirements engineering in global software development : Systematic literature review and quantitative assessment , ” IEEE Access , vol . 6 , p . 59628 – 59656 , 2018 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 8482263 / [ 6 ] E . C . Groen , N . Seyff , R . Ali , F . Dalpiaz , J . Doerr , E . Guzman , M . Hosseini , J . Marco , M . Oriol , A . Perini , and M . Stade , “The crowd in requirements engineering : The landscape and challenges , ” IEEE Software , vol . 34 , no . 2 , p . 44 – 52 , Mar 2017 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 7888433 [ 7 ] P . M . Bach , R . DeLine , and J . M . Carroll , “Designers wanted : participation and the user experience in open source software development , ” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . Boston MA USA : ACM , Apr 2009 , p . 985 – 994 . [ Online ] . Available : https : / / dl . acm . org / doi / 10 . 1145 / 1518701 . 1518852 [ 8 ] M . Paasivaara , S . Durasiewicz , and C . Lassenius , “Distributed agile development : Using scrum in a large project , ” in 2008 IEEE International Conference on Global Software Engineering . Bangalore : IEEE , Aug 2008 , p . 87 – 95 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 4638656 / [ 9 ] A . Cockburn , Agile Software Development . Addison Wesley , 2001 . [ 10 ] L . Williams and A . Cockburn , “Agile software development : It’s about feedback and change , ” Computer , vol . 36 , no . 06 , p . 39 – 43 , Jun 2003 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 1204373 [ 11 ] M . V . M¨antyl¨a , J . Itkonen , and J . Iivonen , “Who tested my software ? testing as an organizationally cross - cutting activity , ” Software Quality Journal , vol . 20 , no . 1 , p . 145 – 172 , Mar 2012 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s11219 - 011 - 9157 - 4 [ 12 ] E . Bjarnason , P . Runeson , M . Borg , M . Unterkalmsteiner , E . Engstr¨om , B . Regnell , G . Sabaliauskaite , A . Loconsole , T . Gorschek , and R . Feldt , “Challenges and practices in aligning requirements with veriﬁcation and validation : a case study of six companies , ” Empirical Software Engineering , vol . 19 , no . 6 , p . 1809 – 1855 , Dec 2014 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s10664 - 013 - 9263 - y [ 13 ] M . S . Chawani , J . Kaasbøll , and S . Finken , “Stakeholder participation in the development of an electronic medical record system in malawi , ” in Proceedings of the 13th Participatory Design Conference . Windhoek , Namibia : ACM Press , 2014 , p . 71 – 80 . [ Online ] . Available : http : / / dl . acm . org / citation . cfm ? doid = 2661435 . 2661444 [ 14 ] B . Ramesh and M . Jarke , “Toward reference models for requirements traceability , ” IEEE Transactions on Software Engineering , vol . 27 , no . 1 , p . 58 – 93 , Jan 2001 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 895989 / [ 15 ] D . Graham , “Requirements and testing : seven missing - link myths , ” IEEE Software , vol . 19 , no . 5 , p . 15 – 17 , Sep 2002 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 1032845 / [ 16 ] E . Bjarnason , M . Unterkalmsteiner , E . Engstr¨om , and M . Borg , “An industrial case study on test cases as requirements , ” in Agile Processes in Software Engineering and Extreme Programming . Cham : Springer International Publishing , 2015 , p . 27 – 39 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 18612 - 2 3 [ 17 ] E . J . Uusitalo , M . Komssi , M . Kauppinen , and A . M . Davis , “Linking requirements and testing in practice , ” in 2008 16th IEEE International Requirements Engineering Conference . Barcelona , Spain : IEEE , Sep 2008 , p . 265 – 270 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 4685680 / [ 18 ] G . Lucassen , F . Dalpiaz , J . M . E . van der Werf , S . Brinkkemper , and D . Zowghi , “Behavior - driven requirements traceability via automated acceptance tests , ” in 2017 IEEE 25th International Requirements Engineering Conference Workshops . Lisbon : IEEE , Sep 2017 , p . 431 – 434 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 8054891 / [ 19 ] F . Pudlitz , F . Brokhausen , and A . Vogelsang , “What am I testing and where ? Comparing testing procedures based on lightweight requirements annotations , ” Empirical Software Engineering , vol . 25 , no . 4 , p . 2809 – 2843 , Jul 2020 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s10664 - 020 - 09815 - w [ 20 ] W . E . Mackay , A . V . Ratzer , and P . Janecek , “Video artifacts for design : bridging the gap between abstraction and detail , ” in Proceedings of the conference on Designing interactive systems processes , practices , methods , and techniques . New York City , New York , United States : ACM Press , 2000 , p . 72 – 82 . [ Online ] . Available : http : / / portal . acm . org / citation . cfm ? doid = 347642 . 347666 [ 21 ] H . Stangl and O . Creighton , “Continuous demonstration , ” in 2011 Fourth International Workshop on Multimedia and Enjoyable Requirements Engineering . Trento : IEEE , Aug 2011 , p . 38 – 41 . [ Online ] . Available : https : / / ieeexplore . ieee . org / document / 6043940 / [ 22 ] O . Karras , C . Unger - Windeler , L . Glauer , and K . Schneider , “Video as a by - product of digital prototyping : Capturing the dynamic aspect of interaction , ” in 2017 IEEE 25th International Requirements Engineering Conference Workshops . Lisbon , Portugal : IEEE , Sep 2017 , p . 118 – 124 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 8054839 / [ 23 ] R . Pham , H . Holzmann , K . Schneider , and C . Br¨uggemann , “Tailoring video recording to support efﬁcient gui testing and debugging , ” Software Quality Journal , vol . 22 , no . 2 , p . 273 – 292 , Jun 2014 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s11219 - 013 - 9206 - 2 [ 24 ] J . Shi and K . Schneider , “Creation of human - friendly videos for debugging automated gui - tests , ” in Testing Software and Systems . Cham : Springer International Publishing , 2021 , p . 141 – 147 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / 978 - 3 - 031 - 04673 - 5 11 [ 25 ] M . A . Tandun , “Report - video production for quick bug - ﬁnding in the web applications , ” Hannover , Jan 2022 . [ Online ] . Available : https : / / www . pi . uni - hannover . de / ﬁleadmin / pi / se / Stud - Arbeiten / 2022 / BA Tandun 2022 . pdf [ 26 ] J . Shi , K . Schneider , M . Tandun , and O . Karras , “Supplementary material for evaluating screentracer among testers , ” Jan 2023 . [ Online ] . Available : https : / / zenodo . org / record / 7522978 [ 27 ] K . Beck , Test - Driven Development : By Example , ser . The Addison - Wesley Signature Series . Boston : Addison - Wesley , 2002 . [ 28 ] J . F . Smart , BDD in Action : Behavior - Driven Development for the whole software lifecycle . Shelter Island , NY : Manning , 2014 . [ 29 ] K . Stapel , K . Schneider , D . L¨ubke , and T . Flohr , “Improving an industrial reference process by information ﬂow analysis : A case study , ” in Product - Focused Software Process Improvement . Springer , 2007 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / 978 - 3 - 540 - 73460 - 4 15 [ 30 ] M . Fowler , “Testpyramid , ” 2012 . [ Online ] . Available : https : / / martinfowler . com / bliki / TestPyramid . html [ 31 ] V . Basili and H . Rombach , “The tame project : towards improvement - oriented software environments , ” IEEE Transactions on Software Engineering , vol . 14 , no . 6 , p . 758 – 773 , Jun 1988 . [ Online ] . Available : http : / / ieeexplore . ieee . org / document / 6156 / [ 32 ] J . Shi , J . M¨onnich , J . Kl¨under , and K . Schneider , “Supporting Data Set for Paper ‘Using GUI Test Videos to Obtain Stakeholders’ Feedback’ , ” 2023 . [ Online ] . Available : https : / / doi . org / 10 . 5281 / zenodo . 7727748 [ 33 ] J . Bortz and C . Schuster , Statistik f¨ur Human - und Sozialwissenschaftler , ser . Springer - Lehrbuch . Springer , 2010 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 12770 - 0 [ 34 ] C . Wohlin , P . Runeson , M . H¨ost , M . C . Ohlsson , B . Regnell , and A . Wessl´en , Experimentation in Software Engineering . Springer Berlin Heidelberg , 2012 . [ Online ] . Available : http : / / link . springer . com / 10 . 1007 / 978 - 3 - 642 - 29044 - 2 45 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply .