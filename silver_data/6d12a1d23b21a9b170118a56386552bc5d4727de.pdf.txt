6 . 441 Spring 2006 - 1 Paper review A Mathematical Theory of Communication Jin Woo Shin , Sang Joon Kim 1 Introduction This paper opened the new area - the information theory . Before this paper , most people believed that the only way to make the error probability of transmission as small as desired is to reduce the data rate ( such as a long repetition scheme ) . However , surprisingly this paper revealed that it does not need to reduce the data rate for achieving that much of small errors . It proved that we can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate , which means we cannot achieve the data rate with any encoding scheme that has small enough error probability over the upper bound . For reviewing this noble paper , ﬁrst , we summarize the paper at our point of view and then discuss several topics concerned with what the paper includes . 2 Summary of the paper The paper was categorized as ﬁve parts - PART I : Discrete Noiseless systems , PART II : The Discrete Channel With Noise , PART III : Mathematical Preliminaries , PART IV : The Continuous Channel , PART V : The Rate For A Continuous Source . We reconstruct this structure as four parts . We combine PART I and PART III to 2 . 1 Prelim - inary . PART I contained the important concepts for demonstrating the following parts such as entropy and ergodic source . PART III extended the concept of entropy to the continuous case for the last parts of paper . PART II , IV , V dealt with ’discrete source and discrete channel’ case , ’discrete source and continuous channel’ case and ’continuous source and continuous channel’ case respectively . We renamed each PARTs and summarize them with this point of view . 2 . 1 Preliminary There are three essential concepts which are used for developing the idea of the paper . One is entropy to measure the uncertainty of the source and another is ergodic property to apply the AEP theorem . The last is capacity to measure how much the channel allow to transfer the information . 2 . 1 . 1 Entropy To measure the quantity of information of a discrete source , the paper deﬁned ”entropy” . To deﬁne the measure H ( p 1 , p 2 , · · · , p n ) reasonably , it should satisfy the following three requirements : 1 . H should be continuous in p i . 6 . 441 Spring 2006 - 2 2 . If all p i = 1 n then , H is monotonically increasing function of n . 3 . H is the weighted sum of two H s divided from the original H . And the paper proved that the only function that satisﬁes the above three properties is : H = − K n (cid:88) i = 1 p i log p i The entropy easily can be extended in the continuous case as following : h = − (cid:90) ∞ −∞ p ( x ) log p ( x ) dx * * we use h for representing the entropy in continuous case to avoid confusing that with the entropy H in discrete case . 2 . 1 . 2 Ergodic source In the Markov process , we call the graph is ergodic if it satisﬁes the following properties : 1 . IRREDUCIBLE : the graph does not contain the case of two isolated nodes A and B , i . e . every node can reach to every node . 2 . APERIODIC : the GCD of the lengths of all cycles in the graph is 1 . If the source is ergodic then , with any initial conditions , P j ( N ) approach the equilibrium prob - ability when N → ∞ and for ∀ j . Equilibrium probability is as following : P j = (cid:88) i P i p i ( j ) 2 . 1 . 3 Capacity The paper deﬁned capacity as two diﬀerent ways and in Theorem 12 at page 25 , it proved these two deﬁnitions are identical . Two diﬀerent deﬁnitions are as following : 1 . C = lim T →∞ log N ( T ) T where N ( T ) is the number of possible signals and T is time duration . 2 . C = Max ( H ( x ) − H ( x | y ) ) 2 . 2 Discrete Source and Discrete Channel Discrete source and Discrete Channel case means that the source information is discrete value and the received data that are sent through the channel are also discrete values . In this case the paper proved the famous theorem ( Theorem 11 at the page 22 ) that there is a rate upper bound that we can achieve the error - free transmission . The detail theorem is as following : Theorem 1 . If the discrete source entropy H is less than or equal to the channel capacity C then there exists a code that can be transmitted over the channel with arbitrarily small amount of errors . If H > C then there is no method of encoding which gives equivocation less than H − C . For proving this theorem , the paper used the assumption that the source is ergodic . With this assumption , we can use the AEP theorem and the result is the above theorem . There are several examples that make sure the result of the theorem and the last one is interesting for us . The last example is a simple encoding function whose rate is equal to the capacity of the given channel , i . e . the most eﬃcient encoding scheme in that channel . The reason why this code is interesting is that it is [ 7 , 4 , 3 ] Hamming code - the basic linear code . 6 . 441 Spring 2006 - 3 2 . 3 Discrete Source and Continuous Channel 2 . 3 . 1 The capacity of a continuous channel In a continuous channel , the input or transmitted signals will be continuous functions of time f ( t ) belonging to a certain set , and the output or received signals will be perturbed versions of these . ( Although the title of this section includes ’discrete source’ , a continuous channel assumes a continuous source in general . ) The main diﬀerence from the discrete case is the domain size of the input and output of a channel becomes inﬁnite . The capacity of a continuous channel is deﬁned in a way analogous to that for a discrete channel , namely C = max P ( x ) ( h ( x ) − h ( x | y ) ) = max P ( x ) (cid:90) (cid:90) P ( x , y ) log P ( x , y ) P ( x ) P ( y ) dxdy Of course , h in the above deﬁnition is a diﬀerential entropy . 2 . 3 . 2 The transmission rate of a discrete source using a continuous channel If the logarithmic base used in computing h ( x ) and h ( x | y ) is two then C is the maximum number of binary digits that can be sent per second over the channel with arbitrarily small error , just as in the discrete case . We can check this fact in two aspects . At ﬁrst , this can be seen physically by dividing the space of signals into a large number of small cells suﬃciently small so that the probability density P ( y | x ) of signal x being perturbed to point y is substantially constant over a cell . If the cell are considered as distinct points the situation os essentially the same as a discrete channel . Secondly , in the mathematical side , it can be shown that if u is the message , x is the signal , y is the received signal and v is the recovered message , then h ( x ) − h ( x | y ) ≥ H ( u ) − H ( u | v ) regardless of what operations are performed on u to obtain x or on y to obtain v . This means no matter how we encode the binary digits to obtain the signal , no matter how we decode the received signal to recover the message , the transmission rate for the binary digits does not exceed the channel capacity we deﬁned above . 2 . 4 Continuous Source and Continuous Channel 2 . 4 . 1 Fidelity In the case of a continuous source , an inﬁnite number of binary digits needs for exact speciﬁcation of a source . This means that transmitting the output of a continuous source with exact recovery at the receiving point requires is impossible using a channel with a certain amount of noise and a ﬁnite capacity . Therefore , practically , we are not interested in exact transmission when we have a continuous source , but only in transmission to within a certain tolerance . The question is , can we assign a transmission rate to a continuous source when we require only a certain ﬁdelity of recovery , measured in a suitable way . A criterion of ﬁdelity can be represented as follows . ν ( P ( x , y ) ) = (cid:90) (cid:90) P ( x , y ) ρ ( x , y ) dxdy P ( x , y ) is a probability distribution over the input and output signals , and ρ ( x , y ) can be interpreted as a ”distance” between x and y , in other words , it measures how undesirable it is to receive y when x is transmitted . We can choose a suitable ρ ( x , y ) , for example , ρ ( x , y ) = 1 T (cid:90) T 0 [ x ( t ) − y ( t ) ] 2 dt 6 . 441 Spring 2006 - 4 , where T is the duration of the messages taken suﬃciently large . In discrete case , ρ ( x , y ) can be deﬁned as the number of symbols in the sequence y diﬀering from the corresponding symbols in x divided by the total number of symbols in x . 2 . 4 . 2 The transmission rate for a continuous source to a ﬁdelity evaluation For given a continuous source ( i . e P ( x ) ﬁxed ) and a ﬁdelity valuation ν ( i . e (cid:82) (cid:82) P ( x , y ) ρ ( x , y ) dxdy ) ﬁxed ) , we deﬁne the rate of generating information as follows . R = min P ( y | x ) ( h ( x ) − h ( x | y ) ) = min P ( y | x ) (cid:90) (cid:90) P ( x , y ) log P ( x , y ) P ( x ) P ( y ) dxdy This means that the minimum is taken over all possible channel . The justiﬁcation of this deﬁnition lies in the following result . Theorem 2 . If a source has a rate R 1 for a ﬁdelity valuation ν 1 , it is possible to encode the output of the source and transmit it over a channel of capacity C with ﬁdelity near v 1 as desired provided R 1 ≤ C . This is not possible if R 1 > C . 3 Discussion 3 . 1 Practical approach to Shannon limit This paper amazingly provided the upper limit of data rate that can be achieved by any encoding schemes . However , it only proved the ”existence” of the encoding scheme that has the data rate under the capacity . Of course , there are several kinds of provided encoding scheme in the paper but they are not practical . Therefore , ﬁnding a ”good” practical encoding scheme is another issue . There are many attempts to ﬁnd an encoding scheme to get closed to the capacity . Currently , there are two coding schemes that are most eﬃcient . One is Turbo code , and the other is LDPC ( Low Density Parity Check ) code . The encoding schemes of these two codes are quite diﬀerent . Turbo code is a kind of convolu - tional code , LDPC is a linear code . However , the decoding algorithms are very similar . They use the iterative decoding algorithm , which reduces the decoding computational complexity . These codes have a very high performance ( near Shannon limit ) but the block size should be large enough to get the performance . As this paper revealed , the message size should go to inﬁnity to get the rate that has the negligible error probability . Practically , for get P e = 10 − 7 in Low SNR ( < 2 . 5 dB ) , the block size should be larger than several tens of thousands . However , practically , the above kinds of large message block cannot be applied because of the limitation of decoding and encoding processing power . Therefore it is necessary to ﬁnd a code that has high performance with reasonable block size . 3 . 2 Assumption of ergodic process The fundamental assumption in the paper is that the source information is ergodic . With this assumption , the paper proved the AEP property and capacity theorems . Therefore , one curiosity is arisen that ”what happens if the source is not ergodic ? ” . If the information is not ergodic , it is reducible or periodic . If AEP property holds with this source ( not ergodic ) , shannon’s capacity theorem also satisﬁes in this case because capacity theorem is not based on ergodic source but on AEP property . Therefore , to ﬁnd a source that is not ergodic and holds AEP property is one of meaningful works . Following example is one of these sources . 6 . 441 Spring 2006 - 5 If a source has following transition probability , then : (cid:181) 12 12 0 1 (cid:182) It is not ergodic because of reduciblity . However it has a unique stationary probability , P = [ 01 ] , therefore it holds AEP property . 3 . 3 Discrete Source with ﬁdelity criterion For dealing with a continuous source , Shannon introduced the ’ﬁdelity’ concept , which is also called ’Rate Distortion’ . This concept can also apply to the case of a discrete source as follows . Deﬁnition 3 . The information rate R of a discrete source for a given distortion ( ﬁdelity ) ν is deﬁned as follows . R ( I ) ( ν ) = min P ( y | x ) ( H ( x ) − H ( x | y ) ) , where P ( x ) and ν = (cid:80) x , y P ( x , y ) ρ ( x , y ) ﬁxed . Consider the case when ν = 0 . In this case , the channel between X and Y becomes a noiseless channel . Therefore , H ( X | Y ) = 0 , and R = H ( X ) . Thus , the rate of a continuous source that Shan - non deﬁned in this paper turns out to be the generalized concept of entropy ( = the rate of a discrete source ) of a discrete source . In other words , entropy of a discrete source is the rate with 0 distortion . Shannon returned to it and dealt with it exhaustively in his 1959 paper ( ’Coding theorems for a discrete source with a ﬁdelity criterion’ ) , which proved the ﬁrst rate distortion theorem . To state it , deﬁne some notions . Deﬁnition 4 . A ( 2 nR , n ) rate distortion code consists of an encoding function , f n : X n → { 1 , 2 , . . . , 2 nR } and a decoding function , g n : { 1 , 2 , . . . , 2 nR } → X n , and the distortion ν associated with the code is deﬁned as follows . ν = ν ( f n , g n ) = (cid:88) x n p ( x n ) ρ ( x n , g n ( f n ( x n ) ) ) . Informally , ν means the expected distance between original messages and recovered messages . Deﬁnition 5 . For a given ν , R is said to be a acheivable rate if there exists a sequence of ( 2 nR , n ) rate distortion codes ( f n , g n ) with lim n →∞ ν ( f n , g n ) ≤ ν Theorem 6 . For an i . i . d source X with distribution P ( x ) and bounded distortion function ρ ( x , y ) , R ( I ) ( ν ) is the minimum acheivable rate at distortion ν . This main theorem implies that we can compress a source X up to R ( I ) ( ν ) ratio when allowing ν ’distortion’ . 6 . 441 Spring 2006 - 6 4 Conclusion The reason why we selected this paper for our term project thought the most contents which are contained in this paper already have been studied in the class is that we want to grab the original idea of Shannon when he opened the information theory . The organization of the paper is intu - itively clear and there are a lot of useful examples that help understand the notions and concepts . The last part about continuous source is the most diﬃcult and confusing to understand and it may be because it has not been dealt in the class . We estimate that the notion of entropy and the assumption of ergodic process are the key of this paper because all fruits are from this root . After this paper came out , many scientists are involved in this ﬁeld and there may be a great improvement from the start point . However , even if the achievement in the information theory has become plentiful it is still under the fundamental notion ”entropy” and ”ergodic” . It is a great opportunity to read this paper rigorously . We can get the fundamental idea of in - formation theory and we hope to know more about what we discussed one chapter above .