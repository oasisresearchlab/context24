A Maturity Assessment Framework for Conversational AI Development Platforms Johan Aronsson , Philip Lu Chalmers | University of Gothenburg , Sweden convaistudy @ easelab . org Daniel Strüber Radboud University , Nijmegen , Netherlands d . strueber @ cs . ru . nl Thorsten Berger Ruhr University Bochum , Germany ; Chalmers | University of Gothenburg , Sweden thorsten . berger @ rub . de ABSTRACT Conversational Artificial Intelligence ( AI ) systems have recently sky - rocketed in popularity and are now used in many applications , from car assistants to customer support . The development of con - versational AI systems is supported by a large variety of software platforms , all with similar goals , but different focus points and func - tionalities . A systematic foundation for classifying conversational AI platforms is currently lacking . We propose a framework for assessing the maturity level of conversational AI development plat - forms . Our framework is based on a systematic literature review , in which we extracted common and distinguishing features of various open - source and commercial ( or in - house ) platforms . Inspired by language reference frameworks , we identify different maturity lev - els that a conversational AI development platform may exhibit in understanding and responding to user inputs . Our framework can guide organizations in selecting a conversational AI development platform according to their needs , as well as helping researchers and platform developers improving the maturity of their platforms . CCS CONCEPTS • Human - centered computing → Interaction techniques ; KEYWORDS conversational AI ; software platforms ; assessment framework ACM Reference Format : Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger . 2021 . A Maturity Assessment Framework for , Conversational AI Development Plat - forms . In The 36th ACM / SIGAPP Symposium on Applied Computing ( SAC ’21 ) , March 22 – 26 , 2021 , Virtual Event , Republic of Korea . ACM , New York , NY , USA , 11 pages . https : / / doi . org / 10 . 1145 / 3412841 . 3442046 1 INTRODUCTION Conversational AI has recently surged in popularity and interest . A conversational AI system is an interface that can communicate and interact with users by relying on the automated processing of questions and formulating answers . In 2016 , Facebook announced a new platform to develop chatbots on their messaging application Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspecificpermission and / or a fee . Request permissions from permissions @ acm . org . SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea © 2021 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 8104 - 8 / 21 / 03 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3412841 . 3442046 [ 21 ] , which simplified the creation of AI chatbots by providing relevant toolkits [ 2 ] . After that , many other companies have im - plemented chatbots for both text and speech . Three of the most popular conversational AI systems today are Microsoft Cortana , Google Assistant , and Apple’s Siri [ 29 ] . The advances in deep learn - ing [ 33 ] and the advent of powerful language models , most recently GPT - 3 [ 4 ] , pave the way for a new generation of conversational AI systems enabling conversations with human - like qualities . To support organizations in adopting conversational AI systems , a multitude of development platforms are available . By offering numerous concepts , such as natural language understanding ( NLU ) , webhooks , and contexts , these platforms enable the user to engineer systems that can provide a rich , ideally almost human - like conver - sation experience . However , due to the large variety of available platforms , the relevance and need of each individual concept and its impact on the conversation experience is unclear . As a result , the use of such platforms may be overly complicated . To support organizations in selecting a suitable platform , and platform devel - opers in increasing the maturity of their platforms , we need to improve our empirical understanding of the state - of - the art of the domain . Specifically , we need to understand what platforms exist , what concepts they offer , what their concepts’ characteristics are , in what combinations the concepts are used , and , in sum , what level of conversation they enable . Evaluating the conversational maturity that the different platforms offer might help in changing the perception that these systems are simply task - oriented tools , and that they can hold truly social conversations . Additionally , it may help in understanding how the more functional terms of these platforms relate to the conversational ability [ 6 ] . In this paper , we provide a maturity assessment framework for conversational AI development platforms . We provide a compre - hensive overview of the features available in today’s platforms , and analyze these features to see how they relate to the quality and ability of conversational AI systems produced using them . Finally , inspired by human language development frameworks , we propose a layered framework with multiple levels of conversational maturity . With this contribution , we aim to improve our empirical understand - ing of current development platforms for creating conversational AI systems , their concepts , and the level of conversation that bots created with these systems can achieve . As a benchmark for as - sessing existing and new platforms , our framework can support and guide practitioners who engineer such platforms . Moreover , it can help researchers understand the concepts that exist , identify gaps between practice and research , and scope future research . In the long term , this could help in creating better conversational AI systems . a r X i v : 2012 . 11976v1 [ c s . H C ] 22 D ec 2020 SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger We address the following research questions : RQ1 : What platforms exist for developing conversational AI systems ? RQ2 : What are the features of these platforms ? These first two questions are aimed towards analyzing existing conversational AI development platforms and extracting informa - tion regarding their usage and features . A specific focus is on the ability of the platforms to model conversation dialogs . To this end , we performed a literature study , in which we col - lected papers presenting different platforms . We then analyzed the documentation of the platforms to identify their distinguish - ing characteristics and concepts ( features ) . To provide an intuitive , hierarchical overview on the multitude of available features , we grouped them into a feature model [ 8 , 13 ] , a common notation for modeling the variability of portfolios of software systems [ 23 ] in a domain . Feature models have also become popular in empirical studies for modeling the design space of technologies , such as model transformations [ 9 ] or language - engineering workbenches [ 10 ] . RQ3 : What are the levels of conversational maturity supported by the identified platforms ? We created a framework that can be used to evaluate the con - versational maturity ( intuitively , how “smart” an agent is in un - derstanding questions and formulating responses ) offered by the platforms . To this end , we considered existing frameworks that eval - uate the language proficiency of humans , and previous discussions on how to evaluate different conversational AI development plat - forms . We then devised a framework based on the features identified in the first research questions and their effect on the human - like performance of a conversational AI development platform . There are not many studies on the conversational maturity that different conversational AI development platforms offer . The most closely related work is the study by Venkatesh et al . [ 31 ] , who de - scribe how to evaluate the performance of conversational agents in terms of certain metrics . In contrast , our work focuses on re - cently available platforms and on how their features impact the conversational maturity of systems created upon these platforms ( cf . Section 2 for a discussion of related work ) . 2 BACKGROUND AND RELATED WORK With the recent developments in many of the sub fields of conver - sational AI , including machine learning , dialog management and NLU , many different conversational AI systems have emerged [ 22 ] . In industry , this technology has been incorporated into search engines , mobile devices , and personal computers . In search engines such as Google and Bing , conversational AI is used to create the feeling of having a conversation with the search engine , enhancing the experience . In mobile devices and personal computers , one use of conversational AI is to create virtual assistants . Some of the biggest virtual assistants on the market today are Apple’s Siri , Google Assistant , Amazon Alexa and Microsoft Cortana [ 14 ] . These assistants also have the capability of acting as chatbots where they keep a turn - based dialog ( a dialog where the user and the bot take turns in asking and responding to queries ) with the user . There also exist conversational interfaces that only focus on this type - dialog - based conversation such as XiaoIce [ 11 ] and Replika [ 11 ] . These dialogs use what is known within conversational AI as intents and entities to understand the user’s goal behind the query . In other words , an intent is what the user wants to achieve with the query , and an entity is the key information for answering the intent . Recently a number of different platforms have been made avail - able to simplify the creation and integration of conversational in - terfaces for developers . The most popular ones are : Google’s Di - alogFlow ( formerly api . ai ) 1 , IBM’s cloud - based bot service Watson Conversation 2 , Amazon Lex 3 and the Microsoft Bot Framework 4 . These platforms come equipped with several different technologies used for NLU , dialog management , response generation and other aspects [ 5 , 11 ] . Since conversational AI is a new field , systematic approaches to overview and categorize it are still in their infancy . Patil et al . [ 24 ] makes a general comparison of features and functionalities between some of the commercial platforms , giving an overview of what platform one might choose for developing a conversational AI system . There have also been more specific studies conducted which compare the NLU and conversational abilities of these types of platforms . Canonico and De Russis [ 18 ] compare the NLU per - formance of these platforms have in terms of usability , pre - built intents ( a number intents already existing in the NLU tool ) context etc . McTear [ 19 ] describe the two main conversation models " one - shot queries " and " slot - filling dialogues " . He compares different platforms’ ability to handle follow up questions in one - shot query scenarios and their mechanisms for slot - filling ( a type of conver - sation where the bot asks specific questions to fill certain slots to fulfil a user intent ) . McTear also presents a number of problems that developers may face when creating conversational interfaces with these platforms . One of the main issues is that it might be difficult to know what functionalities a specific platform offers . There is also a difficulty in interpreting what functionalities might be common between platforms since there is no standard terminology . Venkatesh et al . [ 31 ] describe a number of metrics that can be used to evaluate the overall performance of a conversational agent based on the annual competition Alexa Prize [ 25 ] made for further - ing conversational AI . They propose metrics such as conversational user experience , engagement , and conversational depth to measure the conversational abilities of entire conversational AI systems or chatbots [ 31 ] . Shawar and Atwell [ 26 ] describe metrics to specifi - cally evaluate chatbot systems , a type of conversational AI interface . They argue that metrics for evaluating the abilities of these systems should be done based on the application and its domain and not solely on a standard . One of the main issues with creating the metrics described above is the understanding of what a good conversation is . Clark et al . [ 6 ] discuss that people generally describe conversations with con - versational interfaces in terms of their performance and perceive them more as a device to be controlled . Indicating that people have a previous notion of how these systems will behave coming from a perception that infrastructure to support proper human - to - human dialogs do not exist . The maturity assessment framework presented in this paper takes inspiration from three language proficiency frameworks : Com - mon European Framework of Reference ( CEFR , [ 7 ] ) , American Council 1 https : / / dialogflow . com / 2 https : / / www . ibm . com / cloud / watson - assistant / 3 https : / / aws . amazon . com / lex / 4 https : / / dev . botframework . com / A Maturity Assessment Framework for Conversational AI Development Platforms SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea on the Teaching of Foreign Languages ( ACTFL , [ 27 ] ) , and the Intera - gency Language Roundtable ( ILR , [ 1 ] ) . The goal of these frameworks is to assess the language competency of an individual for a partic - ular language . All of these frameworks have a similar structure , distinguishing different , successive levels ( e . g . , in case of CEFR , a six - item scale A1 – C2 ) , language - relevant skills ( e . g . , for CEFR , reading , listening , speaking , and writing ) , and a number of hints for assigning an individual to a level . While the contents of the framework differ , they all share this same basic structure , which we also found useful for inspiring the design of our framework . A number of papers have scientifically investigated these frameworks , studying their validity and the possibility to use them in an automated way [ 12 , 15 , 30 ] . 3 METHODOLOGY 3 . 1 Identification of Conversational AI development platforms In the first part of our study ( RQ1 ) , we aimed to explore the variety of existing conversational AI development platforms . To this end , we used several methods as follows . 3 . 1 . 1 Literature Review . We used a systematic literature review to identify papers on conversational AI systems . We focused on papers that present and evaluate platforms used to develop such systems . Specifically , among the different methods that exist for conducting literature reviews , we used snowballing . We followed Wohlin [ 32 ] , who describes two types of snowballing and provides guidelines for performing them : forward and backwards snowballing . We per - formed backwards snowballing , to find papers describing current conversational AI development platforms . Backwards snowballing involves selecting a number of papers to be used as a start set to find more relevant papers in the same field by tracing the reference lists of the papers . The start set should include a number of differ - ent papers from different areas of the field , different authors , and different points in time . The idea is to cover the considered field or topic to the largest possible extent . The reference lists of the papers in the start set are then evaluated based on certain inclusion and exclusion criteria ( explained shortly ) . From the start set , additional papers can be found , which we also screened . Each set of reviewed papers is one iteration of the snowballing procedure , once no more papers can be found the process is over . [ 32 ] We collected the start set for snowballing through database searches , using search strings such as “Conversational AI , ” “Con - versational AI development platforms , ” and “Chatbot platforms” . We provide the full list as supplementary material ( made available in our online appendix [ 28 ] ) . The first 50 results for each search string were examined based the criteria listed below . To determine whether to include or exclude a certain paper we used the following procedure : Read the title and abstract and skim the whole paper to determine if any relevant platforms can be found . A paper could be excluded at any stage of the process based on its relevance to the study . The databases used in this search were Google Scholar , IEEExplore , arXiv , SpringerLink , and a university library database . Our inclusion criteria were : • Papers published after 2000 , after which most recent plat - forms have been developed , were candidates for inclusion . • Papers examining and presenting different conversational AI / bot platforms were included . • Papers that only examine characteristics of conversational AI and do not mention any platforms were excluded . The platforms that were found through the literature review were then examined in order to determine if enough information about them was available to fairly assess what features the platforms provided . To this end , our exclusion criteria were : • Platforms that are no longer available or heavily outdated ( no update since two years at time of search ) were excluded . • Systems that were just simple chatbots ( i . e . , responding only to simplest queries like " tell me the time " ) were excluded . • Platforms that did not have enough documentation available publicly were excluded . • Platforms that did not have a strong enough user base , either by individuals or companies or both , were excluded . In addition to conducting snowballing , we consulted with an employee from an industrial partner—a company with years of experience in conversational AI—to find platforms that we might have missed . This made us aware of several additional platforms ( detailed below ) . We conducted our analysis in the summer months of 2019 . 3 . 1 . 2 Database Searches . To find platforms outside the more for - mal channels of published literature and company expertise , we also sought via the Google search engine . This required specific care and source criticism , since the information available may be outdated or even false . We conducted the searches using search terms that try to find platforms similar to those found through previous methods , for instance , “DialogFlow competitors . ” 3 . 2 Documentation Analysis The main process for collecting information about the different conversational AI development platforms was document analysis . Document analysis involves going trough any documentation avail - able for a specific entity , such as a software platform . It allows for the collection of data that later can be evaluated and grouped based on certain criteria . Document analysis is often quite efficient and cost - effective since no new data needs to be acquired ; instead , already existing data is evaluated . However , there is a risk that the documentation may be incomplete [ 3 ] . We analyzed the documentation available for all considered plat - forms to identify their common and distinguishing features , thus ad - dressing RQ2 . Whilst the entire platforms were analyzed to be able to give an overview of the entire system structure , we put special emphasis on their conversation - defining features . The conversation - defining features build up the dialog management portion of the platforms , which define what the bot can understand and how it should respond . This process also helped us mapping similar features whose names vary between different platforms . 3 . 3 Feature Model Torepresent theidentifiedfeatures , wedevelopedafeaturemodel [ 13 ] . Feature models visualize the features of a platform by displaying them in a hierarchy , thus providing a good overview of top - level SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger and more fine - grained features . Features can be mandatory or op - tional . In our survey , we refer to common features of the considered platforms as mandatory , and to distinguishing ones as optional . The model also includes constraints between the features , such as dependencies , in which a feature needs another feature for its implementation . There are a few other models that can be used for similar purposes , such as class diagrams . However , we used feature models since they provide a compact , hierarchical overview , which is good for managing complexity in large systems [ 16 ] , and since feature models have been used in earlier empirical studies [ 9 , 10 ] on systematizing the features of systems in a particular domain . 3 . 4 Designing the Conversational AI Maturity Framework 3 . 4 . 1 Identification of Language Maturity Frameworks . As a pre - requisite for creating a maturity framework for conversational AI development platforms , we explored if any similar attempts had been made before . We performed a literature review to identify any existing frameworks , either directly related to conversational AI classification or to evaluate the conversational maturity of a hu - man . We searched using Google Scholar , IEEE , arXiv , Springer and our university’s library . The following search phrases were used when looking for these frameworks : “Common language frame - work , ” “Human language framework , ” and “Language framework . More can be found in the online appendix [ 28 ] . From these searches , the top 50 results were considered to determine their relevance for this study . We used the following exclusion and inclusion criteria : • Papers discussing different aspects of what makes good con - versational AI were included . • Papers with frameworks used to evaluate maturity of either human or bot conversation maturity were included . • Papers that have metrics for evaluating conversational AI systems were included . To determine whether a paper matched the criteria above the following procedure was followed : read the title of the papers ; read the abstract of the papers ; read discussion to determine if any frameworks are presented or characteristics of good conversational AI are mentioned . As mentioned above a paper could be excluded at any point of the process , if the title was out of scope the paper is directly excluded . 3 . 4 . 2 Designing the Framework . Our goal was to create a frame - work that describes a collection of incremental levels of conversa - tional maturity , inspired by the language proficiency frameworks CEFR , ACTFL , and ILR . We used the same structure as these ex - isting frameworks ( distinguishing various incremental levels and orthogonal skills ) , but filled the structure with entirely new con - tents , tailored to our understanding of conversational maturity . Since a definition of conversational maturity was not available in the literature , we devised an own definition : The ability of a conversational AI system to participate in a human - like conversation . To identify levels , we used the features found through the docu - mentation analysis . We decided for each feature if it contributes to conversational maturity according to this definition , and clustered those features that do into distinct , progressive levels . 4 CHARACTERISING CONVERSATIONAL AI DEVELOPMENT PLATFORMS We present the results from our literature review , the documenta - tion analysis performed on the identified conversational AI devel - opment platforms , and the obtained feature model with common and distinguishing features of these platforms . 4 . 1 Result of Literature Review The database searches resulted in 10 sources which were used as the start set for snowballing . The references of each of these papers were then screened in order to find any other papers relevant for the purpose of finding conversational AI development platforms . The snowballing was ended after three iterations of this procedure . In the first iteration , based on the start set , 13 additional papers were added . The list of potential candidates were narrowed down by using the following procedure : Read title of papers ; check where the paper is referenced in the text ; read abstract of papers ; look at full text to determine if it contains any new conversational platforms . The place of reference was checked in order to determine if it was used in conjunction with text that describe conversational platforms . All papers were matched against the same criteria that was used to put together the start set , see Section 3 . The second iteration of the snowballing procedure were done on the 13 newly found papers . From these papers another 3 were identified that describe conversational AI development platforms . The third and last iteration identified no additional relevant papers . Using snowballing , we identified a total of 56 different potential conversational AI development platforms . From these 56 platforms , we removed a number of duplicates arising from the same sys - tem appearing under different names : DialogFlow was renamed to API . ai , and IBM voice server and AT & T watson were the predeces - sors to IBM Watson Conversation . We excluded the conversational interfaces Cortana , Google assistant , and Amazon Alexa , as they are not actual development platforms . Cortana is developed by Mi - crosoft who makes its technology available through Microsoft Bot Framework . Google assistant is very much related to DialogFlow and Amazon Alexa with Amazon Lex . The remaining platforms were matched against the inclusion and exclusion criteria men - tioned in Section 3 . These criteria were used to narrow down the set to a total of 10 platforms : DialogFlow , Microsoft Bot Frame - work , Houndify , RASA , Amazon Lex , IBM Watson Conversation , VoiceXML , Recast . ai , Kore . ai , and AIML . Consulting with an employee of our partner company resulted in the addition of three new platforms that had not yet been ex - amined as well as the confirmation that the platforms found in the literature review match many of the platforms that had been found by the company . The three new platforms that were found are : Teneo , Boost . ai , and TDM . Teneo and Boost . ai were not included in further investigations as they lacked sufficient documentation of their contained features . Lastly , we used the Google search engine to identify poten - tially missed conversational AI development platforms . Our search brought forwad three new platforms that had emerged quite re - cently : Meya , Chatbot and Botpress . Chatbot and Botpress lacked A Maturity Assessment Framework for Conversational AI Development Platforms SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Table 1 : Conversational AI development platforms Platform Source Availability Modality DialogFlow closed commercial web - based Meya . ai closed commercial web - based Microsoft Bot Framework closed semi - comm . web - based Houndify closed semi - comm . web - based Amazon Lex closed commercial web - based RASA open free command - line IBM Watson Conversation closed semi - comm . web - based VoiceXML open free impl . - dependent Recast . ai closed semi - comm . web - based Kore . ai closed semi - comm . web - based AIML closed free impl . - dependent TDM closed commercial command - line URLs : https : / / cloud . google . com / dialogflow https : / / meya . ai / https : / / botframework . com / https : / / houndify . com / https : / / aws . amazon . com / lex / https : / / rasa . com / https : / / ibm . com / cloud / watson - assistant / https : / / w3 . org / Voice / https : / / recast . ai http : / / aiml . foundation / http : / / talkamatic . se / available documentation supporting a fair assessment of the avail - able functionality . For this reason both of these platforms were excluded . Meya was included since its documentation was exten - sive enough to form a full image of its features . We finally obtained a list of twelve platforms to further analyze . Table 1 shows these platforms , together with their core charac - teristics : open - vs . closed - source , ( semi - ) commercial vs . free , and web - based vs . command - line vs . implementation - dependent . A plat - form is semi - commercial if it has both free and paid variants , where the free variant typically has an upper cap ( for example , 10 , 000 messages per month in the Microsoft Bot Framework ) . 4 . 2 Results of the Documentation Analysis We thoroughly analysed the documentation of platform to pinpoint its included functionality , resulting in a list of features . These fea - tures were then reviewed to identify common and distinguishing features between the different platforms . If the same feature was found in multiple platforms under different names , we continued with the name used more often in the platforms and existing lit - erature . The consolidated features were added to a feature matrix ( made available in our online appendix [ 28 ] ) . The end result was a list of 54 different features that we grouped and organized in a hierarchy to obtain a feature model , discussed in what follows . 4 . 3 Feature Model Figure 1 shows a high - level overview of our feature model , high - lighting its four top - level feature groups : System , Conversation , Input modalities , Output modalities . These top - level feature groups and their contained features will be discussed in this section . The full list of all features with their descriptions is made available in our online appendix [ 28 ] . We use the standard syntax of feature models . Specifically , as shown in the legend , features can be marked as mandatory , meaning ConversationalAI InputModalities Conversation System OutputModalities Legend : Mandatory Optional Or Abstract Collapsed 4 25 34 4 Figure 1 : Top level view of the feature model MultiLanguage Development AutomaticUnderstanding Translation InputProcessing ContentCatalogs Content Propositionality System LanguageRecognition SpellingCorrection MultipleConversationDomains Interfaces 10 3 Figure 2 : Main system features that they exist in all or most of the analyzed platforms , or optional , meaning that they only exist in some platforms . Numbers attached to a node indicate that the node , in fact , represents a collapsed sub - tree , with the specified number of total nodes in the sub - tree . Abstract features are used for grouping purposes . " Or " features are used to specify features groups , where each considered platform had at least one of several features of the group . In what follows , we describe the most crucial features , including those deemed as particularly relevant for assessing conversational maturity . 4 . 3 . 1 System Features . From multiple System features as shown in Figure 2 , Content refers to the different conversation contents and how the platform handles them . A conversation content can be , for example , " phoning a friend " or " weather information " . Figure 2 shows two crucial sub - features of content : ContentCatalogs refers to platforms with in - built content catalogues to simplify the devel - opment of the conversational AI bot . These catalogs contain entities and intents that are common in the domain . MultipleConversation - Domains is used to distinguish platforms that support the handling of domains that are completely independent from one another , thus making it possible to have 2 different content domains in the same conversation . This is in contrast to most platforms , which only support one particular domains with no explicit separation from other domains . MultiLanguage is the conversational AI feature that regards to the number of supported languages within the platform . Development features concern the development process of sys - tems using the platform , which can be supported by features such as error feedback , debugging , and versioning tools . Input processing refers to features to processing of the user input , such as SpellingCor - rection . Different interfaces being supported may include a custom frontend , integration with social media , and other websites . SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger Entity ConversationalOutput LanguageSeparation ConversationTypes Conversation Dialogflow Clarification Intent Speech 3 3 11 3 3 2 1 Figure 3 : Conversational features Policies Sentiments ConversationalOutput DialogInitiation Figure 4 : Conversational output features 4 . 3 . 2 Conversation Features . One of the main reasons why there are so many different conversational AI development platforms is that most handle conversations differently from one another . These differences can be anything from the content of the conversation to the dialog management of the platform . The platforms consider conversations differently depending on what the intent of use is and the area of use is . Many of the different platforms in the market are focused on one specific field of expertise and are customized to fit the needs and standards of this field . An overview of these features and feature categories can be seen in Fig . 3 . These fea - tures are all regarding the conversation between the agent and a user , everything from processing to supported conversation types . Language - specific features are also in the Conversation category , since the language is a part of the conversation . LanguageSeparation istheabilityofthesystem toseparatelanguage - specific and non - language - specific information from a sentence . This allows the system to identify what parts of the sentence is crucial for the information to be received and what parts are not . This feature will simplify the translation of a conversation and the multilingual maintenance of the system . Conversational output features , depicted in Fig . 4 , affect how the conversational output is processed . DialogInitiation is one way to do so , it allows the developer to instigate a conversation . Different companies have different Policies and rules to adhere so some plat - form allows for such policies and rules to be implemented within the system it self . Sentiments allows for the conversational AI to display emotions in their response . Clarification is something that all the analyzed platforms support in some way , since not misunderstanding the user is crucial for supporting the robustness of the system . As summarized in Fig . 5 , this is done by using Affirmation , Rephrasing or FallbackActions to confirm the users intent . These features affect how the system reacts when a user input is not understood or if a user input can be Rephrasing FallbackActions Affirmation Clarification Figure 5 : Clarification features SmallTalk MultipleUserIntents YesNoQuestions ContextualDialogs Questions SlotFilling OpenQuestions SearchOrientedDialog OneShotQueries ConversationTypes TopicShifting MemoryForContext Figure 6 : Conversation types assigned to two different intents . These features also allow to give the user a second chance to change their mind or query . ConversationTypes features cover the different types of conver - sation and questions that a platform supports . These features and feature subsets can be seen in Figure 6 . SearchOrientedDialog refers to a dialog that searches through a database to find matching entities and respond to a user intent . SlotFilling is a dialog where to conversational AI ask for addi - tional information to fill certain criteria to match the correct intent to an entity . An example of this could be : User : What is the weather like today ? Bot : Which city would you like to search the weather for ? User : New York . Bot : The weather in New York is cloudy . SmallTalk is a conversation type that refers to conversations without any specific end goal . These types of conversations can be anything from asking how you feel to telling jokes . ContextualDialogs are an important component of conversational AI systems , supported by every analyzed platform . We found two different ways to support contextual dialogs : one or multiple con - texts per conversation . To support contextual dialogs a conversa - tional AI development platform must have the feature MemoryFor - Context . MemoryForContext is specially allocated memory that the AI uses in order to remember previous information . Multiple con - text are referred to as TopicShifting and can enable conversations like , for example : User : Send a text message to Peter . Bot : What would you like to text ? User : I want to book a flight to Japan for tomorrow . Bot : What time would you like to book the flight at tomorrow ? User : At 4am . Bot : Where in Japan would you like to fly ? User : I would like to text : " The temperature in New York is 20 ◦ C . " Bot : Ok , your message has been sent . User : I would like to fly to Tokyo . Bot : Ok , flight booked to Tokyo tomorrow at 4am . A Maturity Assessment Framework for Conversational AI Development Platforms SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea TrainingPhrases MultipleIntents IntentDefinition Intent Figure 7 : Intent features Entity MultipleEntities Synonyms EntityDefinition Figure 8 : Entity features ToneAnalyzer VoiceActivityDetection Speech Figure 9 : Speech features The last conversation type is Questions , these can vary from simple OneShotQueries to complex MultipleUserIntents . There are two types of OneShotQueries : YesNoQuestions and OpenQuestions . Both these types only require one response from the conversational AI to fully answer the query . MultipleUserIntents are queries with multiple intents within them , an example can be : " What is the time and weather like in New York ? " Features related to Intent are concerned with intent manipulation and intent restrictions . Intents are used to define the users goal with the query , for example : User : What is the time in New York ? Bot : The time in New York is 1pm . In this case the intent of the user is : finding out the time . These features can be seen in Figure 7 . Entity features , as shown in Figure 8 , are concerned with entity manipulation and entity restrictions . Entities are descriptive actions the conversational AI can perform after identifying the users intent . An example is : User : Can you call mom ? Bot : Calling mom . In this case the intent would be to make a call and the entity would be mom . Several platforms support features specific to Speech , one such feature is VoiceActivityDetection which allows the system to detect changes in audio level to determine whether or not the user is cur - rently speaking . Another feature is ToneAnalyzer which allows the conversational AI to identify emotions within the speech pattern . These features , depicted in Fig . 9 , are required by those platforms that support speech as an input modality . 4 . 3 . 3 Input Modalities . For a user to communicate with a conver - sational system the platform must support reading and analyzing one or several input types . The different input modalities supported by the considered platforms are shown in Figure 10 : input of text , speech , images , and URLs . The larger the number of input modali - ties a platform supports , the larger the potential areas of use and accessibility of the created systems . 4 . 3 . 4 Output Modalities . For a two - sided conversation , the con - versational AI system must be able to answer to respond to user queries by using one or several output modalities , the output modal - ities can also be seen in Fig . 10 . These different types of output , like input modalities , allow the system to be used in different types TextInput SpeechInput InputModalities URLInput ImageInput ListOutput SpeechOutput TextOutput ImageOutput OutputModalities Figure 10 : Input and output modalities of environments . Some environments only allow for one type of output to not disrupt the user’s focus . For example , in a moving car , the optimal type of output would be speech . To have several types of output to choose from will both enhance the usability and the accessibility of the developed system . 5 MATURITY ASSESSMENT FRAMEWORK To support the evaluation of the conversational maturity of a con - versational AI development platform , we created a maturity frame - work . The framework is inspired by those for human language development such as CEFR , ILR and ACTFL , which help to assess the conversational maturity of a language learner . This maturity framework can be used in the same fashion , to evaluate the conver - sational maturity level of a conversational AI development platform and how it compares to other platforms . We obtained the framework by considering all features of the state - of - the - art systems surveyed in Section 4 , using the methodology outlined in Sect . 3 . 4 . 2 . 5 . 1 Overview Our framework , as summarized in Table 3 , is divided into four main levels , where each level is further divided into two sub - levels for further differentiation ( inspired by the organization of the CEFR into three main levels with two sub - levels each ) . Orthogonally to the levels , the framework distinguishes two main skills , targeted to the capabilities of conversational AI systems : understanding and response . Understanding refers to the level of comprehension the conversational AI system has and the type of natural language processing it can perform . Response refers to the system’s response patterns and abilities to interact with the user . Table 3 further specifies the features corresponding to each level , pinpointing how certain functionality enables the conversational maturity of a conversational AI development platform . All relevant features have been introduced in Section 4 . For a conversational AI development platform to be assigned as a specific level it has to accommodate all the preceding levels of conversational maturity . For example : to be assessed as level 2a , the platform must satisfy all criteria for levels 1a and 1b , in addition to those of 2a . 5 . 2 Maturity levels A conversational AI development platform’s maturity level depends on its conversational abilities . We propose the following levels : Level 1 Indicates very limited conversational abilities and very simple comprehension . Level 1a Capabilities within the explicitly defined domain and response restricted to isolated words . Level 1b Capabilities within related domains and response restricted to sentence fragments . SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger Level 2 Indicates abilities to hold a short conversation with context and comprehension on a intermediate level . Level 2a Capabilities to understand longer queries and re - sponding with full sentences . Level 2b Capabilities to memorize context for a short con - versation and can respond with questions . Level 3 Indicates abilities to hold a long conversation with context and comprehension on an advanced level . Level 3a Capabilities to comprehend spelling mistakes and small talk . Level 3b Capabilities to comprehend multiple intents in one query . Level 4 Indicates abilities to hold multiple conversations and com - prehending complex human features . Level 4a Capabilities to comprehend multiple input lan - guages and responding in using different languages . Level 4b Capabilitiestocomprehendfeelingsandsentiments and use it for responding . Level 1 describes a platform’s ability to understand and respond to simple one - shot queries and phrases , which requires support for simple intent and entity definition as well as basic dialog manage - ment based on a definition of isolated words . It also needs to be able to provide some basic input and output processing . Level 2 is devoted to intermediate - level conversation . A feature that significantly affects the conversational maturity of a conver - sational AI is context . In conjunction with other features it plays a very crucial role in creating more fluid conversations between the AI and a user . Level 3 addresses advanced conversational ability . A complex feature of conversational AI is the ability to process and understand multiple intents within one query , corresponding to level 3b of the framework . This would not only require all the aforementioned features , but also needs features such as : multiple intents per entity and nested intents . Making it possible to handle many different user requests at once makes for a more human - like conversation . Level 4 considers one of the most important and challenging fac - tors of human conversation , the interpretation and understanding of emotions . A conversational AI system that can both read and respond with feelings creates a more natural and as such a more mature conversation , allowing a proper relationship with the user to be developed [ 17 , 20 ] . In conversational platforms this function - ality is supported through the features sentiments and tone analysis . This together with features such as context , multiple intents and multi - domains make for a platform that can support some of the most mature conversational AI systems . 5 . 3 Application of framework Our proposed framework has several use - cases , depending on the perspective of the stakeholder . Platform users can use the frame - work for informing their investment into particular platforms , a decision that might require to carefully trade off conversational ma - turity with orthogonal factors such as cost and required resources . Developers can use the framework as a benchmark to evaluate and compare their own platforms , to identify missing functionalities , and to make an informed decision on the " next steps " when ex - tending their platform or developing a new platform from scratch . Table 2 : Maturity levels of considered platforms ( based on our analysis in 2019 ; possible later updates not yet included ) Platform Lv . Features for next level DialogFlow 3b TopicShifting , Sentiments , Policies Meya . ai 1b OpenQuestions , MultiConv . Dom . Microsoft Bot Fw . 3a MultipleUserIntents Houndify 1b SearchOrientedDialog Amazon Lex 3b MultiLanguage , Policies RASA 2b SpellingCorrection IBM Watson Conv . 3a LanguageSeparation VoiceXML 3a LanguageSeparation Recast . ai 1b SearchOrientedDialog Kore . ai 1b SearchOrientedDialog AIML 1b SearchOrientedDialog TDM 2a MemoryForContext Researchers can use the framework as a tool for obtaining a quali - fied overview of the platform landscape in practice . In what follows , we describe the results of applying the framework to the twelve platforms considered in our survey . Table 2 shows an overview of the analyzed platforms together with their proposed conversational maturity level , based on a man - ual assessment by the authors . This assessment was done based on a feature matrix derived from the literature survey ( see our online appendix [ 28 ] ) . For each platform , we used the information on in - cluded features to map the platform to the corresponding level . The table also includes a hint for features required to achieve the next level , which can act as a suggestion for current developers of the platforms to specifically focus on during development . This is also a way to indicate for the platform company on what features are necessary for the platform to reach a more human - like performance if this is the end goal of the platform . The decision whether a platform has a particular feature or not is binary . One might further be interested in how well each platform supports each feature . To this end , one might combine our framework with a metrics - based approach such as the one by Venkatesh et al . [ 31 ] . Having an automated assessment tool , ideally based on a continuously updated " benchmarking bot " supported by a community consensus , is a desirable direction for future work . 6 THREATS TO VALIDITY The main threat to external validity is the question of how our framework will generalize to future platforms : Since our framework based on an analysis of the documentation of existing platforms , it is essentially an overview of “what we have” , rather than “what we want” . A promising direction for future work is to conduct a user - study to identify the limitations and unaddressed user needs in current platforms . Consequently , the framework might be aug - mented by a fifth maturity level . Nevertheless , this concern does not threaten the usefulness of our framework for assessing current and near - future platforms and guiding organizations and developers in systematically improving their platforms . A threat to internal validity are possible inaccuracies resulting from our decision to consult the considered platforms’ documen - tations : documentations might be incomplete ( i . e . , not all features A Maturity Assessment Framework for Conversational AI Development Platforms SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Table 3 : Assessment framework for conversational AI development platforms : maturity levels . Level Understanding Response Features Level 1 Very limited conversational abilities and very simple comprehension Level 1a • Can understand simple one - shot queries , like yes - or - no questions and questions for previ - ously defined names . • Can understand simple phrases like greetings or information regarding the domain at hand . • Can understand one intent per entity defined in the conversational AI . • Can respond with isolated words and numbers like “yes” , “no” and “50” . • Can initiate a dialog with the user , instead of waiting for a user input . • Intent • Entity • OneShotQueries • YesNoQuestions • InputModality • OutputModality • DialogInitiation Level 1b • Can understand queries that have been explic - itly defined for the domain at hand . • Can also understand simple queries for related domains . • Can build a short phrase or sentence using a few connected words , to pro - duce and response . • OpenQuestions • MultipleConver - sationDomains Level 2 Abilities to hold a short conversation with context and comprehension on a intermediate level Level 2a • Can understand longer queries with nested in - tents within the defined domain . • Can understand queries for related domains . • Can respond with full sentences containing more information . • Can use affirmation to confirm the users intent . • Can ask the user to repeat the query if it was not understood or mis - heard . • Affirmation • Rephrasing • FallbackActions • SearchOrientedDialog • SlotFilling Level 2b • Can understand the conversational context and keep that context memorized throughout a short conversation . • Can understand multiple intents for a single entity defined in the conversational AI . • Can comprehend propositionality . • Will ask the user for additional in - formation if there is missing infor - mation for the intent . • Can respond with follow up ques - tions to further continue the conver - sation . • ContextualDialogs • MemoryForContext Level 3 Abilities to hold a long conversation with context and comprehension on an advanced level Level 3a • Can understand context in a sentence and keep that context memorized throughout an entire conversation . • Can use spelling correction and understand policies and censorship . • Can comprehend small talk , like asking : “How are you ? ” • Can respond with sentences regard - ing small talk , like : “I’m doing very good ! ” . • SpellingCorrection • SmallTalk Level 3b • Can understand multiple intents in one query . • Can separate between language - specific and non - language - specific information . • Can respond with multiple answers to cover all intents in the query . • MultipleUserIntents • LanguageSeparation Level 4 Abilities to hold multiple conversations and comprehend complex human language features Level 4a • Can shift between different contexts within the same conversation . • Can understand at least 2 input languages . • Can have sentiments when answer - ing queries to add a more human aspect . Can respond in different lan - guages . • Can translate information for the user . • TopicShifting • MultiLanguage • Sentiments • Policies Level 4b • Can understand the sentiments and feelings . • Can analyze the users’ speech input by using linguistic analysis to detect emotion and lan - guage tones . • Can convey feelings when respond - ing to queries ; such as anger , com - fort and etc . • ToneAnalyzer • VoiceActivityDetect - ion SAC ’21 , March 22 – 26 , 2021 , Virtual Event , Republic of Korea Johan Aronsson , Philip Lu , Daniel Strüber , and Thorsten Berger are documented ) , and / or affected by overselling ( i . e . , documented features are actually not available ) . While we cannot rule out that our findings are affected by this threat , we point out that com - panies have a significant interest in keeping their documentation up - to - date and complete . Accurately depicting all available fea - tures is required to be competitive in a rising market ; conversely , overselling might affect reputation and customer trust . Further - more , one may argue that our features and levels were defined in a subjective fashion ( albeit with agreement between the authors ) ; a large - scale user - rating study might increase objectivity . 7 CONCLUSION We presented a conversational AI maturity framework for assessing conversational AI development platforms , based on the ability of the produced systems to conduct conversations . By supporting the understanding of how the features of a conversational AI develop - ment platform correspond to conversational ability , this framework can help both users with choosing and developers with developing a powerful conversational AI system . Our framework is inspired by related frameworks for human language development . Comparable to the way in which a human speaker learns a language , the levels of conversational maturity in our framework indicate the ability to conduct and engage in a natural conversation with a user . Our framework is based on , and incorporates results from an analysis of the state - of - the - art conversational AI development plat - forms , which we identified in a literature review . We considered the documentations of these platforms to extract common and unique features , which we grouped into a feature model to provide a high - level overview of all the different existing features . Each feature comes with a description to support the understanding of its use , context , and scope . We related the features to conversational matu - rity and used them to develop our framework’s maturity levels . Our results show that the various existing conversational AI de - velopment platforms share significant commonalities . In the future , to bridge different terminologies and support users in flexibility choosing a platform according to their current needs , we aim to develop a domain - specific language together with code generators for the various platform . Such an infrastructure allows developing a system on a high level , and transforming the specification into an implementation for a concrete platform . It can also support the migration between different platforms when a platform with higher conversational maturity more becomes available . REFERENCES [ 1 ] 2019 . Iteragency Language Roundtable . ( 2019 ) . https : / / www . govtilr . org / Skills / ILRscale2 . htm [ 2 ] 2019 . Messenger Platform . ( 2019 ) . https : / / developers . facebook . com / docs / messenger - platform / [ 3 ] Glenn Bowen . 2009 . Document Analysis as a Qualitative Research Method . ( 2009 ) . [ 4 ] Tom B . Brown and others . 2020 . Language Models are Few - Shot Learners . In NeurIPS . [ 5 ] Massimo Canonico and Luigi De Russis . 2018 . A Comparison and Critique of Natural Language Understanding Tools . In International Conference on Cloud Computing , GRIDs , and Virtualization . IARIA , Barcelona , 110 – 115 . [ 6 ] Leigh Clark and others . 2019 . What Makes a Good Conversation ? : Challenges in Designing Truly Conversational Agents . In CHI Conference on Human Factors in Computing Systems . [ 7 ] Council of Europe . Common European Framework for Languages : Learning , teach - ing assessment . Technical Report . 21 – 42 pages . https : / / rm . coe . int / 1680459f97 [ 8 ] Krzysztof Czarnecki and Ulrich W . Eisenecker . 2000 . Generative Programming : Methods , Tools , and Applications . Addison - Wesley . [ 9 ] Krzysztof Czarnecki and Simon Helsen . 2006 . Feature - based survey of model transformation approaches . IBM Syst . J . 45 ( July 2006 ) , 621 – 645 . Issue 3 . [ 10 ] Sebastian Erdweg and others . 2013 . The State of the Art in Language Work - benches . In International Conference on Software Language Engineering . [ 11 ] Jianfeng Gao , Michel Galley , and Lihong Li . 2018 . Neural Approaches to Con - versational AI Question Answering , Task - Oriented Dialogues and Social Chatbots . Technical Report . https : / / arxiv . org / pdf / 1809 . 08267 . pdf [ 12 ] Lan - Fen Huang , Simon Kubelec , Nicole Keng , and Lung - Hsun Hsu . 2018 . Evalu - ating CEFR rater performance through the analysis of spoken learner corpora . ( 2018 ) . [ 13 ] KyoCKang , SholomGCohen , JamesAHess , WilliamENovak , andASpencerPe - terson . 1990 . Feature - oriented domain analysis ( FODA ) feasibility study . Technical Report . Carnegie - Mellon University Pittsburgh . [ 14 ] Lorenz Cuno Klopfenstein , Saverio Delpriori , Silvia Malatini , and Alessandro Bogliolo . 2017 . The Rise of Bots : A Survey of Conversational Interfaces , Patterns , and Paradigms . In Conference on Designing Interactive Systems , DIS . 555 – 565 . [ 15 ] Dale L Lange and Jr Lowe , Pardee . 1987 . Grading Reading Passages According to the ACTFL / ETS / ILR Reading Proficiency Standard : Can It Be Learned ? Technical Report . [ 16 ] Kwanwoo Lee , Kyo Kang , and Jaejoon Lee . 2002 . Concepts and Guidelines of FeatureModelingforProductLineSoftwareEngineering . TechnicalReport . Pohang University of Science and Technology . [ 17 ] Iolanda Leite , André Pereira , Samuel Mascarenhas , Carlos Martinho , Rui Prada , and Ana Paiva . 2013 . The influence of empathy in human - robot relations . Inter - national Journal of Human Computer Studies 71 , 3 ( 2013 ) , 250 – 260 . [ 18 ] Conanico Massimo and Luigi De Russis . 2018 . A Comparison and Critique of Natural Language Understanding Tools . In Cloud Computing 2018 . Barcelona , 110 – 115 . [ 19 ] Michael McTear . 2018 . Conversational Modelling for Chatbots : Current Ap - proaches and Future Directions . In Conference on Electronic Speech Signal Pro - cessing . 175 – 185 . [ 20 ] Michael McTear , Zoraida Callejas , and David Griol . 2016 . The Conversational Interface Talking to Smart Devices . Springer International Publishing . 51 – 68 pages . [ 21 ] Ram Menon . 2017 . Council Post : The Rise Of Conversational AI . https : / / www . forbes . com / sites / forbestechcouncil / 2017 / 12 / 04 / the - rise - of - conversational - ai / # 63d2a333b91b . ( 2017 ) . [ 22 ] Eric Michiels . 2017 . Modelling Chatbots with a Cognitive System Allows for a Differentiating User Experience . Technical Report . IBM , Belgium . [ 23 ] Damir Nešić , Jacob Krüger , S , tefan Stănciulescu , and Thorsten Berger . 2019 . Prin - ciples of feature modeling . In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . ACM , 62 – 73 . [ 24 ] Amit Patil , K Marimuthu , Nagaraja Rao , and R Niranchana . 2017 . Comparative studyofcloudplatformstodevelopaChatbot . InternationalJournalofEngineering & Technology ( 2017 ) , 57 – 61 . [ 25 ] Ashwin Ram and others . 2017 . Conversational AI : The Science Behind the Alexa Prize . Technical Report . [ 26 ] Bayan Abu Shawar and Eric Atwell . 2007 . Different measurements metrics to evaluate a chatbot system . In Bridging the Gap : Academic and Industrial Research in Dialog Technologies Workshop Proceedings . Association for Computational Linguistics , Rochester , NY , 89 – 96 . [ 27 ] Elvira Swender , J . Daniel Conrad , and Robert Vicars . 2012 . ACTFL proficiency guidelines 2012 . Technical Report . American Council on the Teaching of Foreign Languages , Alexandria . https : / / www . actfl . org / sites / default / files / pdfs / public / ACTFLProficiencyGuidelines2012 _ FINAL . pdf [ 28 ] The authors . 2020 . Online appendix . ( 2020 ) . https : / / doi . org / 10 . 6084 / m9 . figshare . 13363010 . v1 [ 29 ] Peter Tsai . 2018 . Data snapshot : AI Chatbots and Intelligent Assistants in the Workplace - Spiceworks . ( 2018 ) . https : / / community . spiceworks . com / blog / 2964 - data - snapshot - ai - chatbots - and - intelligent - assistants - in - the - workplace [ 30 ] Erwing Tschierner , Olaf Bärenfänger , and Irmgard Wanner . 2012 . Assessing Evidence of Validity of Assigning CEFR Ratings to the ACTFL Oral Proficiency Interview ( OPI ) and the Oral Proficiency Interview by computers ( OPIc ) . Technical Report . Institute for Test Research and Development , Leipzig . [ 31 ] Anu Venkatesh and others . 2017 . On Evaluating and Comparing Conversational Agents . In NIPS : Conversational AI Workshop . [ 32 ] Claes Wohlin . 2014 . Guidelines for Snowballing in Systematic Literature Stud - ies and a Replication in Software Engineering . In International Conference on Evaluation and Assessment in Software Engineering . 38 : 1 – 10 . [ 33 ] Rui Yan . 2018 . " Chitty - Chitty - Chat Bot " : Deep Learning for Conversational AI . In IJCAI , Vol . 18 . 5520 – 5526 .