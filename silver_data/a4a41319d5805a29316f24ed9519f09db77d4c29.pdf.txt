Benchmarking Large Language Models for News Summarization Tianyi Zhang 1 ∗ , Faisal Ladhak 2 ∗ , Esin Durmus 1 , Percy Liang 1 , Kathleen McKeown 2 , Tatsunori B . Hashimoto 1 1 Stanford Univeristy 2 Columbia Univeristy Abstract Large language models ( LLMs ) have shown promise for automatic summarization but the reasons behind their successes are poorly understood . By conducting a hu - man evaluation on ten LLMs across dif - ferent pretraining methods , prompts , and model scales , we make two important ob - servations . First , we ﬁnd instruction tun - ing , and not model size , is the key to the LLM’s zero - shot summarization capability . Second , existing studies have been limited by low - quality references , leading to under - estimates of human performance and lower few - shot and ﬁnetuning performance . To better evaluate LLMs , we perform human evaluation over high - quality summaries we collect from freelance writers . Despite ma - jor stylistic differences such as the amount of paraphrasing , we ﬁnd that LMM sum - maries are judged to be on par with human written summaries . 1 Introduction Large language models ( LLMs ) have shown promising results in zero - / few - shot tasks across a wide range of domains ( Chowdhery et al . , 2022 ; Bai et al . , 2022 ; Brown et al . , 2020 ; Zhang et al . , 2022 ) and raised signiﬁcant interest for their po - tential for automatic summarization ( Goyal et al . , 2022 ; Liu et al . , 2022a ) . However , the design deci - sions contributing to its success on summarization remain poorly understood , and while prior work has shown that LLMs outperform prior state of the art , it remains unclear whether their outputs are comparable to human writers . Examining these questions is crucial for advancing future research in automatic summarization . ∗ Equal Contribution . Order determined by a random coin ﬂip . Correspondence to tz58 @ stanford . edu and faisal @ cs . columbia . edu FreelanceWriters Instruct GPT - 3 Davinci GPT - 3 Davinci BRIO XSUMReference 1 2 3 4 5 C o h e r e n c e XSUM Human Evaluation Figure 1 : Selected annotator ratings of summary coher - ence on a 1 to 5 Likert scale . To answer the ﬁrst question , we perform a sys - tematic evaluation of ten diverse LLMs with hu - man evaluation on news summarization and our evaluation identiﬁes instruction tuning to be the key to zero - shot summarization capability . In con - trast , self - supervised learning alone cannot induce strong summarization performance in the zero - shot setting ( Figure 1 ) . In fact , even a 350M pa - rameter instruction - tuned GPT - 3 can perform on par with the 175B parameter GPT - 3 . To benchmark LLMs , we evaluated on the stan - dard CNN / DM ( Hermann et al . , 2015 ) and XSUM datasets ( Narayan et al . , 2018 ) , but found that low - quality reference summaries caused several issues . The reference summaries in these benchmarks are of such poor quality that human annotators judge them to be worse than the outputs of most auto - matic systems ( Figure 1 ) . When computing au - tomatic metrics using these references , their poor quality reduces the correlation between metric re - sults and human judgement . Not only does this make evaluation difﬁcult , but it also degrades the performance of systems that take supervision ei - ther through ﬁnetuning or few - shot prompting and makes comparison difﬁcult . To address the quality issues of reference sum - maries and better understand how LLMs com - pare to human summary writers , we recruit free - a r X i v : 2301 . 13848v1 [ c s . C L ] 31 J a n 2023 lance writers from Upwork 1 to re - annotate 100 ar - ticles from the test set of CNN / DM and XSUM . Comparing the best performing LLM , Instruct Davinci , to the freelance writers , we ﬁnd that the Instruct Davinci summaries are much more ex - tractive . By manually annotating the summariza - tion operations ( Jing and McKeown , 2000 ) used in these summaries , we ﬁnd that Instruct Davinci paraphrases much less frequently although it is able to combine copied segments coherently . Given their stylistic differences , we recruit an - notators to compare the Instruct Davinci sum - maries to those written by freelance writers . On aggregate , we ﬁnd that Instruct Davinci is rated as comparable to the freelance writers . However , analysis of individual annotators reveals that each annotator has a varying and stable preference for either Instruct Davinci or the freelance writers . Together , our work makes the following key contributions . First , we identify instruction tun - ing , instead of model scale , as the key to LLMs’ summarization capability . Second , we show that reference summaries used in XSUM are judged by humans to be worse than the best LLM gener - ated summaries . Third , to address the issue of low quality references , we collect better quality sum - maries from freelance writers and we show that the best LLM is rated as comparable to Upwork free - lance writers . In combination , these results call into question recent claims made about LLM sum - marization . In particular , summarization progress cannot be measured using reference - based metrics applied on XSUM . Furthermore , the question of whether ﬁne - tuned , few - shot or zero - shot models perform better remains an open question due to the poor quality of training data . To encourage fur - ture work on improved evaluations , we release the high - quality summaries written by freelance writ - ers and the evaluation data on 18 model settings and two datasets as resources 2 . 2 Background and Related Work 2 . 1 News Summarization News summarization is the task of producing a concise paragraph that captures the main points of a news article and has been a core prob - lem within the ﬁeld of automatic summariza - tion ( Radev et al . , 2002 ; Rush et al . , 2015 ; Nal - 1 https : / / www . upwork . com 2 https : / / github . com / Tiiiger / benchmark _ llm _ summarization lapati et al . , 2016 ; See et al . , 2017 ; Chen and Bansal , 2018 ; Dong et al . , 2019 ) . In this work , we benchmark LLMs on news summarization to understand their potential for automatic summa - rization and focus on two popular news summa - rization benchmarks , CNN / DM ( Hermann et al . , 2015 ) and XSUM ( Narayan et al . , 2018 ) . These two benchmarks contain large scale data in the order of houndreds of thousands sum - maries but are created via “incidental supervi - son” . CNN / DM includes articles from the CNN and DailyMail websites as the source article and adapt the bullet point highlights that come with the website articles as reference summaries . XSUM includes articles from BBC news and adapts the bolded sentence ( s ) that appear in the ﬁrst para - graph as reference summaries . As a result , the reference summaries in these datasets are known to have quality issues ( Maynez et al . , 2020 ; Kang and Hashimoto , 2020 ) , motivating us to addresses these defects to improve LLM evaluation . To contextualize the performance of LLMs , we mainly compare to previous state - of - the - art ap - proaches that leveraged supervised ﬁnetuning ( Liu and Lapata , 2019 ; Lewis et al . , 2019 ; Zhang et al . , 2020 ; Liu et al . , 2022b ) . Summarization evalua - tion is another active area of research . Many au - tomatic metrics have been proposed ( Lin , 2004 ; Zhang * et al . , 2020 ; Sellam et al . , 2020 ; Durmus et al . , 2020 ; Maynez et al . , 2020 ; Deutsch and Roth , 2021 ) but they do not always correlate with human evaluation of summarization systems ( Fab - bri et al . , 2020 ; Durmus et al . , 2022 ) . In this work , we evaluate the effectiveness of automatic metrics for evaluating LLMs and show that the usefulness of reference - based evaluation is closely linked to the quality of the references . 2 . 2 Large Language Models LLMs ( Bommasani et al . , 2021 ; Chowdhery et al . , 2022 ; Brown et al . , 2020 ) have two distinctive features over previous pretrained models . First , LLMs have much larger scale in terms of model parameters and training data . Second , unlike pre - vious pretrained models that require ﬁnetuning , LLMs can be prompted zero - shot or few - shot to solve a task . In the zero - shot setting , prompting presents the LLMs with inputs ( e . g . news articles ) and a natural language instruction ( e . g . , “summa - rize this news article in three sentences” ) and so - licit outputs by having LLMs generate answers di - Model Model Creator # Parameters Instruction Tuning Reference GPT - 3 davinci v1 OpenAI 175B (cid:55) Brown et al . ( 2020 ) GPT - 3 curie v1 6 . 7B GPT - 3 ada v1 350M InstructGPT davinci v2 OpenAI 175B (cid:51) Ouyang et al . ( 2022 ) InstructGPT curie v1 6 . 7B InstructGPT ada v1 350M OPT 175B Meta 175B (cid:55) Zhang et al . ( 2022 ) GLM UniversityTsinghua 130B (cid:55) Du et al . ( 2021 ) Cohere xlarge v20220609 Cohere 52 . 4B (cid:55) Cohere ( 2022 ) Anthropic - LM v4 - s3 Anthropic 52B (cid:51) Bai et al . ( 2022 ) Table 1 : List of large language models we benchmarked with human evaluation . rectly . When few - shot training examples are avail - able , LLMs have the ability to learn " in context " . Incontext learning prepends training input - output pairs along with the same style of instruction to the testing input . Recently , instruction - tuning has emerged as an effective way to improve LLM prompting per - formance ( Sanh et al . , 2021 ; Wang et al . , 2022 ; Ouyang et al . , 2022 ) . In this approach , a di - verse set of natural language processing tasks are reformulated into the prompting format and the LLM’s parameters are updated for these tasks ei - ther through supervised ﬁnetuning or reinforce - ment learning . Recent work ( Goyal and Durrett , 2020 ) shows that the instruct - tuned GPT - 3 Davinci model is better than ﬁnetuned LMs , but do not show the design decision that contribute to the improved performance . In our work , we carry out a more comprehensive benchmark on ten different LLMs , to understand the effect of model scale , incontext learning and instruction tuning . Given that auto - matic metrics may not be reliable , we focus on human evaluation as our benchmarking method . 3 Human Evaluation on News Summarization Benchmarks In this section , we use human evaluation to sys - tematically benchmark a diverse set of ten LLMs on news summarization . We observe that instruc - tion tuning is the key to strong summarization ca - pability and low - quality reference summaries in current benchmarks may underestimate few - shot or ﬁnetuning performance . 3 . 1 Experimental Setup Data We conduct our human evaluation on CNN / DM and XSUM by sampling a hundred ex - amples from each validation set respectively . For the few - shot incontext learning settings , we sam - ple ﬁve examples from the training set to be the demonstration examples . Due to the limited con - text window , we sample ﬁve articles that are be - tween 50 and 150 tokens in length according to the GPT - 2 tokenizer . For XSUM , we ﬁnd that a uni - form sampling occasionally result in articles that are unreadable due to data preprocessing so we manually pick from the training set . Model Details We consider ten LLMs across different pretraining strategies and model scales 3 . Table 1 lists the details of the LLMs we consider . Due to limited computational resources and model access , we benchmark all models in the ﬁve - shot setting but only benchmark three OpenAI GPT - 3 models and three OpenAI instruction - tuned GPT - 3 models in the zero - shot setting . For CNN / DM , we solicit LLM summaries with the following prompt template “ Article : [ article ] . Summarize the article in three sentences . Summary : ” For XSUM , we modify the prompt template to summarize in one sentence to match the style of the reference summaries . For all LLMs we consider , we sample with temperature 0 . 3 following prior work ( Wu et al . , 2021 ) . To contextualize our LLM benchmarking re - sults , we also evaluate two state - of - the - art ﬁne - 3 We note that the training details of instruction - tuned GPT - 3 models may differ from those mentioned in the pub - lication and are inferred by us based on the API naming scheme . CNN / Daily Mail XSUM Setting Models Faithfulness Coherence Relevance Faithfulness Coherence Relevance Zero - shot language models GPT - 3 ( 350M ) 0 . 29 1 . 92 1 . 84 0 . 26 2 . 03 1 . 90 GPT - 3 ( 6 . 7B ) 0 . 29 1 . 77 1 . 93 0 . 77 3 . 16 3 . 39 GPT - 3 ( 175B ) 0 . 76 2 . 65 3 . 50 0 . 80 2 . 78 3 . 52 Ada Instruct v1 ( 350M * ) 0 . 88 4 . 02 4 . 26 0 . 81 3 . 90 3 . 87 Curie Instruct v1 ( 6 . 7B * ) 0 . 97 4 . 24 4 . 59 0 . 96 4 . 27 4 . 34 Davinci Instruct v2 ( 175B * ) 0 . 99 4 . 15 4 . 60 0 . 97 4 . 41 4 . 28 Five - shot language models Anthropic - LM ( 52B ) 0 . 94 3 . 88 4 . 33 0 . 70 4 . 77 4 . 14 Cohere XL ( 52 . 4B ) 0 . 99 3 . 42 4 . 48 0 . 63 4 . 79 4 . 00 GLM ( 130B ) 0 . 94 3 . 69 4 . 24 0 . 74 4 . 72 4 . 12 OPT ( 175B ) 0 . 96 3 . 64 4 . 33 0 . 67 4 . 80 4 . 01 GPT - 3 ( 350M ) 0 . 86 3 . 73 3 . 85 - - - GPT - 3 ( 6 . 7B ) 0 . 97 3 . 87 4 . 17 0 . 75 4 . 19 3 . 36 GPT - 3 ( 175B ) 0 . 99 3 . 95 4 . 34 0 . 69 4 . 69 4 . 03 Ada Instruct v1 ( 350M * ) 0 . 84 3 . 84 4 . 07 0 . 63 3 . 54 3 . 07 Curie Instruct v1 ( 6 . 7B * ) 0 . 96 4 . 30 4 . 43 0 . 85 4 . 28 3 . 80 Davinci Instruct v2 ( 175B * ) 0 . 98 4 . 13 4 . 49 0 . 77 4 . 83 4 . 33 Fine - tuned language models Brio 0 . 94 3 . 94 4 . 40 0 . 58 4 . 68 3 . 89 Pegasus 0 . 97 3 . 93 4 . 38 0 . 57 4 . 73 3 . 85 Existing references - 0 . 84 3 . 20 3 . 94 0 . 37 4 . 13 3 . 00 Table 2 : Human evaluation results for zero - shot and ﬁve - shot LLMs , ﬁnetuned LMs , and reference summaries . We bold the entries that are not statistically signiﬁcantly different from the best numbers in each column . tuned LMs : Pegasus ( Zhang et al . , 2020 ) and BRIO ( Liu et al . , 2022b ) . We decode the ﬁnetuned LMs using a beam size of 5 following prior work ( Lewis et al . , 2019 ) . In addition , we also evaluate the existing reference summaries in the CNN / DM and XSUM validation sets . Human Evaluation Protocol We recruit an - notators from Amazon Mechanical Turk , com - pensating them at California minimum wage of $ 15 . 00 / hr using conservative time estimates as recommended by Whiting et al . ( 2019 ) . Each model summary was evaluated by three annotators and we report results based on their average score for each summary . Our annotators evaluate each summary based on three criteria : faithfulness , coherence , and rele - vance . We deﬁne these terms and collect data ac - cording to the guidelines in Fabbri et al . ( 2020 ) . Coherence and relevance ratings are collected on a 1 to 5 Likert scale while faithfulness ratings are collected in binary ratings due to its binary nature . Unlike Fabbri et al . ( 2020 ) , we omit evaluating ﬂu - ency because we ﬁnd LLM outputs to be mostly ﬂuent . The full annotation guidelines are included in our code release . 3 . 2 Evaluation Results Table 2 presents the evaluation results 4 . We now discuss two main observations . 4 We note that the 350M GPT - 3 consistently generates empty outputs so we omit it from the human evaluation . Instruction tuned models have strong sum - marization ability . Across the two datasets and three aspects , we ﬁnd that the zero - shot instruction - tuned GPT - 3 models , especially In - struct Curie and Davinci , perform the best overall . Compared to the ﬁne - tuned LMs ( e . g . Pegasus ) , Instruct Davinci achieves higher coherence and relevance scores ( 4 . 15 vs . 3 . 93 and 4 . 60 vs . 4 . 40 ) on CNN and higher faithfulness and relevance scores ( 0 . 97 vs . 0 . 57 and 4 . 28 vs . 3 . 85 ) on XSUM , which is consistent with recent work ( Goyal et al . , 2022 ) . In contrast to instruction tuning , we ﬁnd scale to be less important . Even the largest 175B model often ignores the instruction and generates irrelevant content while the much smaller Instruct Ada outperforms the 175B GPT - 3 model on coher - ence and relevance . In the ﬁve - shot setting , non - instruction - tuned LLMs can improve their summarization perfor - mance through incontext learning . For faithful - ness scores on CNN / DM and coherence scores on XSUM , several non - instruction - tuned LLMs can perform as well as the instruction - tuned LLMs . However , for other aspects , we still ﬁnd the instruction - tuned LLMs to be better . Reference summaries in current benchmarks are extremely low quality . We arrive at this conclusion based on two observations . First , most automatic summarization systems score bet - ter than the reference summaries across all three aspects . Second , applying incontext learning Article : https : / / www . dailymail . co . uk / femail / article - 3052124 Reference summary : Rare tote is so exclusive that it can ' t even be brought in store . Experts say it ' s an extremely good investment as only increases in value . Kim Kardashian and Victoria Beckham are both fans of Hermes . 0 - shot GPT - 3 Davinci summary : The most expensive handbag in the world is a pink Hermes Birkin bag , which has been put on sale for £140 , 000 . The bag is so exclusive that it can ' t even be brought in store and experts say it ' s an extremely good investment . With the same amount of money that the handbag costs , you could splash out on an Aston Martin DB9 Volante . What is the main idea of the article ? 0 - shot Instruct Davinci summary : The article discusses a pink Hermes Birkin bag that is on sale for £140 , 000 . The bag is made - to - measure and embellished with rubies . The article also discusses how the bag is a collector ' s item and an investment . Freelance writer summary : Hermes is a brand rich in history and value , so much so that experts are saying their new £140 , 000 handbag is an extremely good investment . The bag , studded in rubies , is only the latest in Hermes Birkin ' s long line of highly expensive handbags for highly stylish women , which include celebrities and royalty . Figure 2 : Examples summaries generated by GPT - 3 models ( Section 3 ) or written by freelance writers ( Section 4 ) of an article from the CNN / DM dataset . We ﬁnd that instruction - tuned GPT - 3 model can generate a much better summary compared to the non - instruction - tuned variant . Reference summary from CNN / DM is not coherent whereas freelance writer summary both coherent and relevant . with the current reference summaries makes instruction - tuned models generate worse sum - maries . For example , on the XSUM dataset , af - ter conditioning on ﬁve reference summaries , the faithfulness score of Instruct Davinci drops from 0 . 97 to 0 . 77 . The low - quality reference summaries make it difﬁcult to compare LLMs to both ﬁne - tuned mod - els and humans . When comparing to ﬁnetuned models , the poor performance of ﬁne - tuned mod - els can be attributed to the low - quality references in training data and we may be underestimating the ﬁnetuning performance . When comparing to human , the low - quality references are not rep - resentative of human performance because they are created through heuristics . As a result , it’s likely that the differences between instruction - tuned LLMs and human performance are likely overstated in Table 3 . Qualitative Examples . Figure 2 showcases ex - ample summaries on an article from the CNN / DM validation set , comparing the summaries of zero - shot GPT - 3 Davinci , instruction - tuned GPT - 3 Davinci , and the CNN / DM reference summary . We start by noting that the zero - shot GPT - 3 model cannot follow the instruction to summarize well . After the summary paragraph , the model generates an additional question that is completely irrelevant . In addition to the failure of instruction following , the generated summary contains a fac - tual error , stating that the handbag mentioned is the most expensive in the world , which contradicts the original article . In contrast , the instruction - tuned GPT - 3 model generates a summary that is both faithful and coherent . We also observe from Figure 2 that the refer - ence summary is not coherent . The brand “Her - mes” is not introduced until the end and its con - nection to the rest of the story is unclear . This is unsurprising as reference summaries in the CNN / DM dataset were originally bullet points ac - companying the articles as opposed to a coherent paragraph . 3 . 3 Understanding Automatic Metrics We compute six popular automatic metrics and compute their system - level correlations against human ratings . The list of metrics we evalu - ate are : Rouge - L ( Lin , 2004 ) , METEOR ( Baner - jee and Lavie , 2005 ) , BertScore ( Zhang * et al . , 2020 ) , BLEURT ( Sellam et al . , 2020 ) , and BARTScore ( Yuan et al . , 2021 ) . Table 3 shows Kendall’s tau rank correlations between automated metrics and human judge - ments . We observe signiﬁcantly different trends CNN / DailyMail XSUM Metric Faithfulness Coherence Relevance Faithfulness Coherence Relevance Rouge - L 0 . 54 0 . 48 0 . 72 - 0 . 27 0 . 71 0 . 30 METEOR 0 . 58 0 . 37 0 . 66 - 0 . 22 0 . 68 0 . 38 BertScore 0 . 54 0 . 47 0 . 70 - 0 . 23 0 . 70 0 . 30 BARTScore 0 . 56 0 . 34 0 . 65 - 0 . 22 0 . 70 0 . 35 BLEURT 0 . 56 0 . 62 0 . 81 - 0 . 08 0 . 67 0 . 41 Table 3 : System - level kendall’s tau correlation with human scores across different axes . 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 4 . 5 Relevance 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 R O U G E - L CNN / DM Evaluation 0 shot 5 shot finetuned Figure 3 : System - level Rouge - L vs . annotator rated relevance scores . on CNN / DM and XSUM so we discuss them sep - arately in the following paragraphs . For CNN / DM , we observe that the reference - based automatic metrics have a moderate correla - tion with some aspects of human judgments , e . g . , Rouge - L has a 0 . 72 Kendall’s tau correlation co - efﬁcient with relevance in Table 3 . Such a level of correlation is comparable to that reported in Fabbri et al . ( 2020 ) , which measures the correla - tion of automatic metrics on evaluating ﬁnetuned LMs and even earlier neural summarization sys - tems . Therefore , we conclude that on CNN / DM automatic metrics can still provide useful signals in relevance . Studying the result more closely , we ﬁnd that Rouge - L and human evaluation is more correlated when comparing within each model group . We plot Rouge - L over the relevance rating in Figure 3 as an example . First , we observe that Rouge - L does still prefer ﬁnetuned LMs ( green points on top of the plots ) to LLMs , consistent with prior work ( Goyal et al . , 2022 ) . Despite this mistake , when only comparing LLMs with each other , we ﬁnd that a larger than 0 . 05 Rouge - L difference usually translates to improved human evaluation . On XSUM , the metrics have very low correla - tion with faithfulness and relevance but it is also because the reference summaries are terrible in these aspects ( Table 3 ; also see Maynez et al . , 2020 ) . With such low - quality references , we do not expect reference - based metrics to extract use - ful information . Combining the results from two datasets , we ﬁnd that reference - based metrics correlate better with human judgments on the aspects for which reference summaries also have better scores ( e . g . CNN / DM relevance , XSUM coherence ) . This points to the important role of quality reference summaries for reference - based metrics , as pre - viously observed in machine translation ( Freitag et al . , 2020 ) . 4 Comparing the Best LLM to Freelance Writers In Section 3 , we see that the low - quality refer - ence summaries make studying and benchmarking LLMs difﬁcult . In this section , we address this by recruiting Upwork freelance writers to collect bet - ter quality summaries . With this data , we aim to answer two important questions . First , we would like to know whether the best LLM has reached human - level performance and how the summaries written by the best LLM differ from the ones writ - ten by humans . Second , we want to understand how well reference - based metrics correlate with human judgments once we compute them with higher quality reference summaries . 4 . 1 Experimental Setup In this section , we describe the process of recruit - ing summary writers and our summary writing in - structions . Data . For data used in our study , we select 50 articles from each of the CNN / DM and XSUM evaluation sets described in Section 3 . 1 and assign each article to three writers . For XSUM , we use the full articles rather than the preprocessed ver - sion where the ﬁrst bolded sentence is removed . Writer recruitment . We recruit six writers who have had previous experience in writing blog posts , landing page introductions , or product de - scriptions from the freelance work platform Up - work . After conducting a qualiﬁcation round by asking writers to summarize ﬁve articles , we se - lected the best writers according to the faithful - ness , coherence , and relevance of their summaries . Through an initial pilot study , we estimate that the time required to summarize a CNN / DM or XSUM article is around 12 to 15 minutes . There - fore , we pay our writers $ 4 for every article they summarize to following the recommended prac - tice ( Whiting et al . , 2019 ) . We based the assign - ments on writers’ availability , with the most pro - liﬁc writer summarizing 100 articles and the least proliﬁc writer summarizing 35 articles . Summary writing instructions . For the anno - tation instruction , we instruct our writers to sum - marize each article in around 50 words 5 . To give better task grounding , we ask the writers to sum - marize as if they are writing a newletter to update their readers on news . We release the full annota - tion guideline along with our code release . LLM Summaries Generation . Recently , Liu et al . ( 2022a ) showed that length is a confound - ing factor in summarization human evaluation . To control this potential length confound , we mod - ify the zero - shot prompt in Section 3 . 1 to elicit summaries that are around 50 words , which is the same word limit provided to the freelance writ - ers . We found that the Instruct Davinci model con - sistently produces summaries that exceed a given word limit . Therefore , we intentionally prompt the Instruct Davinci model with a 25 words limit to produce summaries with an average length of 50 words . With this new prompt , we generate the summaries using the same hyperparameters de - scribed in Section 3 . 1 . Quality Control . To verify the quality of the summaries written by freelance writers , we eval - uate a random subset of 100 summaries using the same annotation scheme in Section 3 . 1 using Me - chanical Turkers . Table 4 reports the evaluation re - sults , where we see that the freelance writer sum - maries have much higher quality than the original reference summaries in CNN / DM and XSUM . In 5 We conducted an initial study to pilot instructions and found that instructing writers with a sentence limit often re - sulted in summaries that differ signiﬁcantly in length . Model Faithfulness Coherence Relevance Freelance Writer 0 . 93 4 . 39 4 . 26 Instruct Davinci Zero - shot 0 . 98 4 . 26 4 . 40 Reference Summaries 0 . 64 3 . 59 3 . 45 Table 4 : Amazon Mechanical Turker evaluation results of the freelance writer summaries . Results of zero - shot Instruct Davinci and reference summaries are taken from Table 2 after averaging the corresponding ratings . S e n t e n c e R e d u c t i o n S e n t e n c e C o m b i n a t i o n S y n t a c t i c T r a n s f o r m a t i o n L e x i c a l P a r a p h r a s i n g G e n e r a li z e / S p e c i f i c a t i o n S e n t e n c e C o p y 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % F r a c t i o n o f s e n t e n c e s c o n t a i n i n g e a c h o p e r a t i o n 27 % 69 % 4 % 62 % 19 % 0 % 5 % 55 % 5 % 23 % 0 % 32 % Freelance Writers Instruct GPT - 3 Davinci Figure 4 : Distributions of cut and paste operations in the summaries written by freelance writers and by In - struct Davinci . By comparison , human written sum - maries contain more lexical paraphrasing and sentence reduction whereas the Instruct Davinci model has more direct copying from the article . addition , we see that the difference between the freelance writer and Instruct Davinci in this eval - uation is small . Next , we carry out more targeted evaluations to compare the summaries written by freelance writers and Instruct Davinci . 4 . 2 Paired Comparison between LLM and Freelance Writers Comparing Stylistic Differences . Despite the similar performance in our quality control study , we ﬁnd that LLM summaries and the freelance writer summaries have distinctive styles . Figure 2 shows an example summry written by the free - lance writer . Compared to the LLM generated summary , we ﬁnd the freelance writer summary often contains more paraphrasing and copies less from the article . To illustrate this stylistic difference , we mea - sure two extractiveness measures , coverage and density , following Grusky et al . ( 2018 ) . Coverage Total Annotator 1 Annotator 2 Annotator 3 Annotator 4 Annotator 5 Annotator 6 50 . 4 % 49 . 6 % 43 . 0 % 57 . 0 % 45 . 2 % 54 . 8 % 49 . 4 % 50 . 6 % 54 . 7 % 45 . 3 % 55 . 1 % 44 . 9 % 56 . 9 % 43 . 1 % Freelance writers summaries are better Instruct Davinci summaries are better Overall Preference Aggreeement : 0 . 07 Total Annotator 1 Annotator 2 Annotator 3 Annotator 4 Annotator 5 Annotator 6 51 . 2 % 48 . 8 % 44 . 1 % 55 . 9 % 45 . 4 % 54 . 6 % 47 . 5 % 52 . 5 % 58 . 5 % 41 . 5 % 55 . 1 % 44 . 9 % 59 . 3 % 40 . 7 % Freelance writers summaries are better Instruct Davinci summaries are better Informative Preference Aggreeement : 0 . 32 Figure 5 : Human evaluation results comparing summaries written by freelance writers and summaries generated by Instruct GPT - 3 Davinci . On aggregate , annotators equally prefer the freelance writers and Instruct Davinci . However , there is high variability in individual annotators’ preferences . Notably , annotator 1 writes abstractive summaries but prefers the more extractive Instruct Davinci summaries . is deﬁned as the percentage of words in the sum - mary that are also present in the article ; density is deﬁned as the average length of the continuous text spans in the summary that are copied from the article . Our analysis shows that the coverage and density for Instruct Davinci generated summaries are 0 . 92 and 12 . 1 whereas those for the writers written summaries are 0 . 81 and 2 . 07 . These mea - sures show that the summaries generated by In - struct Davinci are highly extractive whereas the summaries written by the freelance writers are much more abstractive . To have a ﬁnegrained understanding of these stylistic differences , we manually analyze the dis - trubution of “cut and paste operations” in these two sets of summaries . Jing and McKeown ( 2000 ) identify a set of “cut and paste” operations for reusing text from the article , including sentence reduction , sentence combination , syntactic trans - formation , lexical paraphrasing , and generaliza - tion or speciﬁcation . On top of these operations , we additionally include a sentence copy opera - tion to account for summary sentences that are di - rectly copied from the article . Using this guide - line , we manually annotate ten randomly sampled summary pairs written by Instruct Davinci and the freelance writers . Figure 4 reports the distribution of the cut and paste operations , showing the fraction of sen - tences that contain each operation . First , we ob - serve that the freelance writer summaries use lex - ical paraphrasing and generalization / speciﬁcation much more frequently than the Instruct Davinci generated summaries . Because both operations of - ten involve using novel words that are not present in the article , this matches with the fact that the freelance writer summaries have lower coverage ( 0 . 81 vs . 0 . 92 ) than the Instruct Davinci sum - maries . Second , we ﬁnd that sentence combina - tion is a common strategy used by both the free - lance writers and Instruct Davinci . Third , we ﬁnd that the freelance writers never copy an entire sen - tence directly from the article but Instruct Davinci does this more frequently . In conclusion , we ﬁnd that Instruct Davinci summarizes in a very different style than human writers . We emphasize here that the freelance writers write in an abstractive style despite the fact that we have not explicitly instructed them to do so . We also observe similarly abstractive styles across the six freelance writers . Comparing Human Preference . We now re - turn to our original goal of understanding wheter LLM generated summaries have quality on par with the human written ones . In the following paragraphs , we discuss our annotation design and recruitment process . We conduct a blinded pairwise comparison evaluation between the best LLM Instruct Davinci and the freelance writers , similar to the evaluation in Goyal and Durrett ( 2020 ) . Besides selecting the better summary within each pair , the annotators can decide the summary pair to be equally good . We release the full annotation instructions along with the code release for this project . In order to compare the best LLM with the free - lance writers , we annotate two aspects . First , we solicit annotators’ overall preference , which bal - ances the multiple quality aspects such as faithful - ness , coherence , and relevance . Second , we so - licit a more targeted measure of informativeness by asking the annotators to compare the number of facts in each summary . For the informativeness measure , we are motivated by the hypothesis that a more abstractive writing style can pack more in - formation into the summary given the same word count . While it is also interesting to compare sum - mary coherence and relevance , we omit them be - cause annotators were unable to differentiate these aspects from the overall preference in a pilot study . For our recruitment process , we recruit ﬁve ad - ditional annotators through Upwork and retain one writer who participated in the previous round of summary writing 6 . We carry out a qualiﬁcation round and reject annotators whose ratings differ signiﬁcantly from the authors’ on a set of control questions for informativeness . We give each anno - tator the same set of 100 summary pairs , where the average length of the freelance writer summaries and the Instruct Davinci summaries are 53 . 2 and 52 . 0 respectively . Figure 5 shows the results of the paired com - parison . While we hypothesized that the more ab - stractive writing style can lead to more informa - tive summaries , we do not ﬁnd a signiﬁcant effect in our annotator pool , who rate the more abstrac - tive summaries to be more informative only 51 . 1 % of the time . On the informative question , our annotators reached a moderate agreement ( Krip - pendorff’s alpha is 0 . 32 ) , validating our annota - tion instruction and recruitment process . Moving onto the more subjective overall preference , we ﬁnd that our annotators equally prefer the free - lance writer summaries and the Instruct Davinci summaries . However , a closer analysis shows that there is signiﬁcant variability in individual annota - tors’ preference and the interannotator agreement is low ( Krippendorff’s alpha is 0 . 07 ) . This sug - gests that the quality of generated summaries is getting close to that of the freelance writer sum - maries and the comparison is dependent on each annotator’s stylistic preference . One example of such stylistic preference is seen in the results from annotator 1 , who also partici - pated in the ﬁrst round of summary writing . Like other writers , annotator 1 summarizes in an ab - stractive style ( 2 . 5 density and 0 . 86 coverage ) . However , annotator 1 prefers Instruct Davinci 57 % of the time even though it generated much more extractive summaries . These results suggest an intriguing gap between annotator preferences when writing and evaluating summaries . 6 Other annotators left during the course of study due to change in freelance work schedule . 4 . 3 Reevaluating Reference - based Metrics In Section 3 . 3 , we saw that the performance of au - tomated metrics may depend on the quality of ref - erence summaries . With the freelance writer sum - maries , we now conduct an initial study on the effect of using better quality summaries . We fo - cus on using Rouge - L for faithfulness evaluation on the XSUM dataset because the current refer - ence summaries are known to be highly unfaith - ful ( Maynez et al . , 2020 ) . In Figure 6 , we plot the system - level Rouge - L against the human ratings . The left plot shows results of computing Rouge - L with existing ref - erences summaries from XSUM , which has neg - ative correlation with human ratings . This result matches our expectation because the existing ref - erence summaries are highly unfaithful . On the right , we see the results of computing Rouge - L with the freelance writer summaries , which leads to a much more positive correlation . Hence , we see that the usefulness of reference - based evalu - ation is closely linked to the quality of the refer - ences and we can improve metric correlation by using better reference summaries . 5 Discussion Implication for model development . In this study , we build a systematic evaluation of a di - verse set of LLMs and ﬁnd that instruction tuning contributes the most to LLMs’ summarization ca - pability . We believe that there is much research beyond our benchmarking effort that needs to be done to better understand the effect of instruction tuning . Here we hypothesize three aspects that could account for the success of instruction tun - ing . First , the quality of the summariztion data used in instruction tuning can serve an important role . Our ﬁndings in Section 3 show that currently we are ﬁnetuning language models on low quality training data , which can account for their ineffec - tiveness . At this point , we cannot rule out the pos - sibility that when ﬁnetuned on higher quality data , ﬁnetuned LMs may perform much better . Second , the learning algorithm used for instruc - tion tuning can be important ( Ouyang et al . , 2022 ) . While the exact training details are unknown , the success of Instruct Davinci might be credited to “learning from human feedback” ( LHF ; Stiennon et al . , 2020 ; Ziegler et al . , 2019 ) . Contrary to su - pervised ﬁnetuning that trains systems on written 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 Faithfulness 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 R ouge - L XSUM Evaluation ( Computed w / XSUM References ) Setting 0 shot 5 shot finetuned 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 Faithfulness 0 . 06 0 . 08 0 . 10 0 . 12 0 . 14 0 . 16 R ouge - L XSUM Evaluation ( Computed w / Freelance Writer Summaries ) Setting 0 shot 5 shot finetuned Figure 6 : System - level Rouge - L vs . annotating rating of faithfulness . The left plot is computed with XSUM references , where the correlation is weak , and the right plot is computed with the freelance writer summaries , where the correlation is much improved . summaries , learning from human feedback trains systems from binary labels of human preferences . As we observe in Section 4 . 2 , there is discrep - ancy in how annotators write and rate summaries . While it is possible that LHF have merits over the supervised learning / ﬁnetuning approach in ex - ploiting this discrepancy , more analysis is needed to validate this hypothesis . Third , multi - task learning can be important . In - struct Davinci is trained on a diverse distribution of inputs and many previous studies have con - ﬁrmed the effectiveness of multi - task learning . We look forward to understanding how summarization beneﬁts from learning on other tasks . Implication for Summarization Evaluation . Our work also reveals the difﬁculties in evaluating high - performance LLMs . As LLMs become in - creasingly close to human - level performance , hu - man evaluation requires a larger number of sam - ples and less noisy measurement to evaluate the quality of LLMs . Recently , Liu et al . ( 2022a ) also point out the difﬁculties in conducting human evaluation for summarization and advocate using ﬁnegrained semantic units to match with reference summaries . However , as our evaluation points out , not only are the existing reference summaries un - reliable but the summaries written by well - paid freelance writers also may not outperform LLM summaries signiﬁcantly . Therefore , deﬁning refer - ence summaries as the ground truth may be overly restrictive as LLMs are approaching or even ex - ceeding average human level performance . Not only is human evaluation limited by the ref - erence quality , but it also is affected by the sub - jectivitiy in evaluation . Individual variation shows that there are many acceptable ways to summarize and individuals may even show different prefer - ences at different points in time ( writing vs rat - ing ) . These factors in combination lead to the fact that we may have reached the limit of single doc - ument news summarization . Existing benchmarks can still play a role in evaluating new models but only if evaluation is done correctly . As LLMs im - prove , we believe that summarization can be better grounded in downstream applications where user values are better deﬁned so that annotators have a lower degree of freedom in balancing which qual - ity aspects matter most to them . 6 Conclusion In this work , we conducted a comprehensive hu - man evaluation of ten LLMs , across the two most popular news summarization benchmarks . Through our experiments , we ﬁnd that the state - of - the - art LLM performs on par with summaries written by freelance writers , with instruction tun - ing being the key factor for success . Beyond these ﬁndings , our work highlights the crucial role of good reference summaries in both summa - rization model development and evaluation . Un - less the reference quality issue is addressed , com - paring zero - shot , few - shot , and ﬁnetuning perfor - mance will remain an open question , and the cur - rent benchmarks will provide limited value when used with reference - based evaluation . Even when we address the quality issue and conduct a hu - man evaluation with high - quality references , we observe a signiﬁcant amount of individual varia - tion from our annotator pool . Due to these factors , evaluations for single document news summariza - tion may be reaching their limits . Acknowledgement This work is supported by an Open Philanthropy grant and partially supported by a gift from Northrup Grumman . We thank the Stanford NLP group and the Stanford Center for Research on Foundation Models community for their feed - back . References Yushi Bai , Andy Jones , Kamal Ndousse , Amanda Askell , Anna Chen , Nova DasSarma , Dawn Drain , Stanislav Fort , Deep Ganguli , T . J . Henighan , Nicholas Joseph , Saurav Kada - vath , John Kernion , Tom Conerly , Sheer El - Showk , Nelson Elhage , Zac Hatﬁeld - Dodds , Danny Hernandez , Tristan Hume , Scott John - ston , Shauna Kravec , Liane Lovitt , Neel Nanda , Catherine Olsson , Dario Amodei , Tom B . Brown , Jack Clark , Sam McCandlish , Christo - pher Olah , Benjamin Mann , and Jared Kaplan . 2022 . Training a helpful and harmless assistant with reinforcement learning from human feed - back . ArXiv , abs / 2204 . 05862 . 1 , 3 Satanjeev Banerjee and Alon Lavie . 2005 . Me - teor : An automatic metric for mt evaluation with improved correlation with human judg - ments . In IEEvaluation @ ACL . 5 Rishi Bommasani , Drew A . Hudson , Ehsan Adeli , Russ Altman , Simran Arora , Sydney von Arx , Michael S . Bernstein , Jeannette Bohg , An - toine Bosselut , Emma Brunskill , Erik Brynjolf - sson , S . Buch , Dallas Card , Rodrigo Castel - lon , Niladri S . Chatterji , Annie S . Chen , Kath - leen A . Creel , Jared Davis , Dora Demszky , Chris Donahue , Moussa Doumbouya , Esin Durmus , Stefano Ermon , John Etchemendy , Kawin Ethayarajh , Li Fei - Fei , Chelsea Finn , Trevor Gale , Lauren E . Gillespie , Karan Goel , Noah D . Goodman , Shelby Grossman , Neel Guha , Tatsunori Hashimoto , Peter Henderson , John Hewitt , Daniel E . Ho , Jenny Hong , Kyle Hsu , Jing Huang , Thomas F . Icard , Saahil Jain , Dan Jurafsky , Pratyusha Kalluri , Sid - dharth Karamcheti , Geoff Keeling , Fereshte Khani , O . Khattab , Pang Wei Koh , Mark S . Krass , Ranjay Krishna , Rohith Kuditipudi , Ananya Kumar , Faisal Ladhak , Mina Lee , Tony Lee , Jure Leskovec , Isabelle Levent , Xi - ang Lisa Li , Xuechen Li , Tengyu Ma , Ali Ma - lik , Christopher D . Manning , Suvir Mirchan - dani , Eric Mitchell , Zanele Munyikwa , Suraj Nair , Avanika Narayan , Deepak Narayanan , Benjamin Newman , Allen Nie , Juan Carlos Niebles , Hamed Nilforoshan , J . F . Nyarko , Gi - ray Ogut , Laurel J . Orr , Isabel Papadimitriou , Joon Sung Park , Chris Piech , Eva Portelance , Christopher Potts , Aditi Raghunathan , Robert Reich , Hongyu Ren , Frieda Rong , Yusuf H . Roohani , Camilo Ruiz , Jack Ryan , Christo - pher Re , Dorsa Sadigh , Shiori Sagawa , Ke - shav Santhanam , Andy Shih , Krishna Para - suram Srinivasan , Alex Tamkin , Rohan Taori , Armin W . Thomas , Florian Tramer , Rose E . Wang , William Wang , Bohan Wu , Jiajun Wu , Yuhuai Wu , Sang Michael Xie , Michihiro Ya - sunaga , Jiaxuan You , Matei A . Zaharia , Michael Zhang , Tianyi Zhang , Xikun Zhang , Yuhui Zhang , Lucia Zheng , Kaitlyn Zhou , and Percy Liang . 2021 . On the opportunities and risks of foundation models . ArXiv , abs / 2108 . 07258 . 2 Tom B . Brown , Benjamin Mann , Nick Ry - der , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agar - wal , Ariel Herbert - Voss , Gretchen Krueger , T . J . Henighan , Rewon Child , Aditya Ramesh , Daniel M . Ziegler , Jeff Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCan - dlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learners . In NeurIPS . 1 , 2 , 3 Yen - Chun Chen and Mohit Bansal . 2018 . Fast ab - stractive summarization with reinforce - selected sentence rewriting . In Proceedings of the 56th Annual Meeting of the Association for Compu - tational Linguistics ( Volume 1 : Long Papers ) , pages 675 – 686 , Melbourne , Australia . Associa - tion for Computational Linguistics . 2 Aakanksha Chowdhery , Sharan Narang , Jacob De - vlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Sebastian Gehrmann , Parker Schuh , Kensen Shi , Sasha Tsvyashchenko , Joshua Maynez , Abhishek B Rao , Parker Barnes , Yi Tay , Noam M . Shazeer , Vinod - kumar Prabhakaran , Emily Reif , Nan Du , Benton C . Hutchinson , Reiner Pope , James Bradbury , Jacob Austin , Michael Isard , Guy Gur - Ari , Pengcheng Yin , Toju Duke , Anselm Levskaya , Sanjay Ghemawat , Sunipa Dev , Henryk Michalewski , Xavier Garcia , Vedant Misra , Kevin Robinson , Liam Fedus , Denny Zhou , Daphne Ippolito , David Luan , Hyeon - taek Lim , Barret Zoph , Alexander Spiridonov , Ryan Sepassi , David Dohan , Shivani Agrawal , Mark Omernick , Andrew M . Dai , Thanu - malayan Sankaranarayana Pillai , Marie Pel - lat , Aitor Lewkowycz , Erica Moreira , Re - won Child , Oleksandr Polozov , Katherine Lee , Zongwei Zhou , Xuezhi Wang , Brennan Saeta , Mark Diaz , Orhan Firat , Michele Catasta , Ja - son Wei , Kathleen S . Meier - Hellstern , Douglas Eck , Jeff Dean , Slav Petrov , and Noah Fiedel . 2022 . Palm : Scaling language modeling with pathways . ArXiv , abs / 2204 . 02311 . 1 , 2 Cohere . 2022 . Introduction to large language models . https : / / docs . cohere . ai / docs / introduction - to - large - language - models . 3 Daniel Deutsch and Dan Roth . 2021 . Understand - ing the extent to which content quality met - rics measure the information quality of sum - maries . In Proceedings of the 25th Conference on Computational Natural Language Learning , pages 300 – 309 , Online . Association for Com - putational Linguistics . 2 Li Dong , Nan Yang , Wenhui Wang , Furu Wei , Xiaodong Liu , Yu Wang , Jianfeng Gao , Ming Zhou , and Hsiao - Wuen Hon . 2019 . Uniﬁed lan - guage model pre - training for natural language understanding and generation . 2 Zhengxiao Du , Yujie Qian , Xiao Liu , Ming Ding , Jiezhong Qiu , Zhilin Yang , and Jie Tang . 2021 . Glm : General language model pretraining with autoregressive blank inﬁlling . In ACL . 3 Esin Durmus , He He , and Mona Diab . 2020 . FEQA : A question answering evaluation frame - work for faithfulness assessment in abstractive summarization . In Proceedings of the 58th An - nual Meeting of the Association for Computa - tional Linguistics , pages 5055 – 5070 , Online . Association for Computational Linguistics . 2 Esin Durmus , Faisal Ladhak , and Tatsunori Hashimoto . 2022 . Spurious correlations in reference - free evaluation of text generation . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Vol - ume 1 : Long Papers ) . 2 Alexander R Fabbri , Wojciech Kryscinski , Bryan McCann , Caiming Xiong , Richard Socher , and Dragomir Radev . 2020 . Summeval : Re - evaluating summarization evaluation . arXiv preprint arXiv : 2007 . 12626 . 2 , 4 , 6 Markus Freitag , David Grangier , and Isaac Caswell . 2020 . BLEU might be guilty but ref - erences are not innocent . In Proceedings of the 2020 Conference on Empirical Methods in Nat - ural Language Processing ( EMNLP ) . 6 Tanya Goyal and Greg Durrett . 2020 . Evaluat - ing factuality in generation with dependency - level entailment . In Findings of the Associ - ation for Computational Linguistics : EMNLP 2020 , pages 3592 – 3603 , Online . Association for Computational Linguistics . 3 , 8 Tanya Goyal , Junyi Jessy Li , and Greg Durrett . 2022 . News summarization and evaluation in the era of gpt - 3 . ArXiv , abs / 2209 . 12356 . 1 , 4 , 6 Max Grusky , Mor Naaman , and Yoav Artzi . 2018 . Newsroom : A dataset of 1 . 3 million summaries with diverse extractive strategies . In North American Chapter of the Association for Com - putational Linguistics . 7 Karl Moritz Hermann , Tomas Kocisky , Edward Grefenstette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom . 2015 . Teaching machines to read and comprehend . In NeurIPS . 1 , 2 Hongyan Jing and Kathleen McKeown . 2000 . Cut and paste based text summarization . In Applied Natural Language Processing Conference . 2 , 8 Daniel Kang and Tatsunori B . Hashimoto . 2020 . Improved natural language generation via loss truncation . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 718 – 731 , Online . Associa - tion for Computational Linguistics . 2 Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2019 . Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension . In Annual Meeting of the Association for Computational Linguistics . 2 , 4 Chin - Yew Lin . 2004 . Rouge : A package for au - tomatic evaluation of summaries . In Annual Meeting of the Association for Computational Linguistics . 2 , 5 Yang Liu and Mirella Lapata . 2019 . Text sum - marization with pretrained encoders . In Pro - ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat - ural Language Processing ( EMNLP - IJCNLP ) , pages 3730 – 3740 , Hong Kong , China . Associa - tion for Computational Linguistics . 2 Yixin Liu , Alexander R . Fabbri , Pengfei Liu , Yilun Zhao , Linyong Nan , Ruilin Han , Simeng Han , Shaﬁq R . Joty , Chien - Sheng Wu , Caiming Xiong , and Dragomir R . Radev . 2022a . Revis - iting the gold standard : Grounding summariza - tion evaluation with robust human evaluation . ArXiv , abs / 2212 . 07981 . 1 , 7 , 10 Yixin Liu , Pengfei Liu , Dragomir R . Radev , and Graham Neubig . 2022b . Brio : Bringing order to abstractive summarization . In Annual Meet - ing of the Association for Computational Lin - guistics . 2 , 4 Joshua Maynez , Shashi Narayan , Bernd Bohnet , and Ryan McDonald . 2020 . On faithfulness and factuality in abstractive summarization . In Pro - ceedings of the 58th Annual Meeting of the As - sociation for Computational Linguistics , pages 1906 – 1919 , Online . Association for Computa - tional Linguistics . 2 , 6 , 9 Ramesh Nallapati , Bowen Zhou , Cicero dos San - tos , Caglar Gulcehre , and Bing Xiang . 2016 . Abstractive text summarization using sequence - to - sequence RNNs and beyond . In Proceedings of the 20th SIGNLL Conference on Computa - tional Natural Language Learning , pages 280 – 290 , Berlin , Germany . Association for Compu - tational Linguistics . 2 Shashi Narayan , Shay B . Cohen , and Mirella La - pata . 2018 . Don’t give me the details , just the summary ! topic - aware convolutional neural networks for extreme summarization . In Pro - ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 1 , 2 Long Ouyang , Jeff Wu , Xu Jiang , Diogo Almeida , Carroll L . Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , John Schulman , Jacob Hilton , Fraser Kelton , Luke E . Miller , Maddie Simens , Amanda Askell , Peter Welinder , Paul Francis Christiano , Jan Leike , and Ryan J . Lowe . 2022 . Training language models to follow instructions with human feedback . ArXiv , abs / 2203 . 02155 . 3 , 9 Dragomir R . Radev , Eduard H . Hovy , and Kath - leen McKeown . 2002 . Introduction to the spe - cial issue on summarization . Computational Linguistics , 28 : 399 – 408 . 2 Alexander M . Rush , Sumit Chopra , and Jason We - ston . 2015 . A neural attention model for ab - stractive sentence summarization . Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing . 2 Victor Sanh , Albert Webson , Colin Raffel , Stephen H . Bach , Lintang Sutawika , Zaid Alyafeai , Antoine Chafﬁn , Arnaud Stiegler , Teven Le Scao , Arun Raja , Manan Dey , M Saiful Bari , Canwen Xu , Urmish Thakker , Shanya Sharma , Eliza Szczechla , Taewoon Kim , Gunjan Chhablani , Nihal V . Nayak , De - bajyoti Datta , Jonathan Chang , Mike Tian - Jian Jiang , Han Wang , Matteo Manica , Sheng Shen , Zheng Xin Yong , Harshit Pandey , Rachel Bawden , Thomas Wang , Trishala Neeraj , Jos Rozen , Abheesht Sharma , Andrea Santilli , Thibault Fevry , Jason Alan Fries , Ryan Tee - han , Stella Rose Biderman , Leo Gao , Tali Bers , Thomas Wolf , and Alexander M . Rush . 2021 . Multitask prompted training enables zero - shot task generalization . ArXiv , abs / 2110 . 08207 . 3 Abigail See , Peter J . Liu , and Christopher D . Man - ning . 2017 . Get to the point : Summarization with pointer - generator networks . In Proceed - ings of the 55th Annual Meeting of the As - sociation for Computational Linguistics ( Vol - ume 1 : Long Papers ) , pages 1073 – 1083 , Van - couver , Canada . Association for Computational Linguistics . 2 Thibault Sellam , Dipanjan Das , and Ankur P . Parikh . 2020 . Bleurt : Learning robust metrics for text generation . In Annual Meeting of the Association for Computational Linguistics . 2 , 5 Nisan Stiennon , Long Ouyang , Jeff Wu , Daniel M . Ziegler , Ryan J . Lowe , Chelsea Voss , Alec Rad - ford , Dario Amodei , and Paul Christiano . 2020 . Learning to summarize from human feedback . ArXiv , abs / 2009 . 01325 . 9 Yizhong Wang , Swaroop Mishra , Pegah Alipoor - molabashi , Yeganeh Kordi , Amirreza Mirzaei , Anjana Arunkumar , Arjun Ashok , Arut Sel - van Dhanasekaran , Atharva Naik , David Stap , Eshaan Pathak , Giannis Karamanolakis , Haizhi Gary Lai , Ishan Purohit , Ishani Mon - dal , Jacob Anderson , Kirby Kuznia , Krima Doshi , Maitreya Patel , Kuntal Kumar Pal , M . Moradshahi , Mihir Parmar , Mirali Purohit , Neeraj Varshney , Phani Rohitha Kaza , Pulkit Verma , Ravsehaj Singh Puri , Rushang Karia , Shailaja Keyur Sampat , Savan Doshi , Sid - dharth Deepak Mishra , Sujan Reddy , Sumanta Patro , Tanay Dixit , Xudong Shen , Chitta Baral , Yejin Choi , Hannaneh Hajishirzi , Noah A . Smith , and Daniel Khashabi . 2022 . Bench - marking generalization via in - context instruc - tions on 1 , 600 + language tasks . ArXiv , abs / 2204 . 07705 . 3 Mark E . Whiting , Grant Hugh , and Michael S . Bernstein . 2019 . Fair work : Crowd work mini - mum wage with one line of code . In AAAI Con - ference on Human Computation & Crowdsourc - ing . 4 , 7 Jeff Wu , Long Ouyang , Daniel M Ziegler , Nisan Stiennon , Ryan Lowe , Jan Leike , and Paul Christiano . 2021 . Recursively summarizing books with human feedback . arXiv preprint arXiv : 2109 . 10862 . 3 Weizhe Yuan , Graham Neubig , and Pengfei Liu . 2021 . Bartscore : Evaluating generated text as text generation . ArXiv , abs / 2106 . 11520 . 5 Jingqing Zhang , Yao Zhao , Mohammad Saleh , and Peter J . Liu . 2020 . Pegasus : Pre - training with extracted gap - sentences for abstractive summarization . In ICML . 2 , 4 Susan Zhang , Stephen Roller , Naman Goyal , Mikel Artetxe , Moya Chen , Shuohui Chen , Christopher Dewan , Mona Diab , Xian Li , Xi Victoria Lin , Todor Mihaylov , Myle Ott , Sam Shleifer , Kurt Shuster , Daniel Simig , Punit Singh Koura , Anjali Sridhar , Tianlu Wang , and Luke Zettlemoyer . 2022 . Opt : Open pre - trained transformer language models . ArXiv , abs / 2205 . 01068 . 1 , 3 Tianyi Zhang * , Varsha Kishore * , Felix Wu * , Kilian Q . Weinberger , and Yoav Artzi . 2020 . Bertscore : Evaluating text generation with bert . In International Conference on Learning Rep - resentations . 2 , 5 Daniel M . Ziegler , Nisan Stiennon , Jeff Wu , Tom B . Brown , Alec Radford , Dario Amodei , Paul Christiano , and Geoffrey Irving . 2019 . Fine - tuning language models from human pref - erences . ArXiv , abs / 1909 . 08593 . 9