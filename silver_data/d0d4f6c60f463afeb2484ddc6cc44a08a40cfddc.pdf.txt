Vamsa : Tracking Provenance in Data Science Scripts Mohammad Hossein Namaki Washington State University m . namaki @ wsu . edu Avrilia Floratou Microsoft avﬂor @ microsoft . com Fotis Psallidas Microsoft fopsalli @ microsoft . com Subru Krishnan Microsoft subru @ microsoft . com Ashvin Agrawal Microsoft asagr @ microsoft . com Yinghui Wu Case Western Reserve University yxw1650 @ case . edu ABSTRACT Machine learning ( ML ) which was initially adopted for search ranking and recommendation systems has ﬁrmly moved into the realm of core enterprise operations like sales optimization and preventative healthcare . For such ML ap - plications , often deployed in regulated environments , the standards for user privacy , security , and data governance are substantially higher . This imposes the need for tracking provenance end - to - end , from the data sources used for train - ing ML models to the predictions of the deployed models . In this work , we take a ﬁrst step towards this direction by introducing the ML provenance tracking problem in the context of data science scripts . The fundamental idea is to automatically identify the relationships between data and ML models and in particular , to track which columns in a dataset have been used to derive the features of a ML model . We discuss the challenges in capturing such prove - nance information in the context of Python , the most com - mon language used by data scientists . We then , present Vamsa , a modular system that extracts provenance from Python scripts without requiring any changes to the users’ code . Using up to 450 K real - world data science scripts from Kaggle and publicly available Python notebooks , we verify the eﬀectiveness of Vamsa in terms of coverage , and per - formance . We also evaluate Vamsa’s accuracy on a smaller subset of manually labeled data . Our analysis shows that Vamsa’s precision and recall range from 87 . 5 % to 98 . 3 % and its latency is typically in the order of milliseconds for scripts of average size . arxiv Reference Format : Mohammad Hossein Namaki , Avrilia Floratou , Fotis Psallidas , Subru Krishnan , Ashvin Agrawal , and Yinghui Wu . Vamsa : Tracking Provenance in Data Science Scripts . arxiv , 12 ( xxx ) : xxxx - yyyy , 2020 . DOI : https : / / doi . org / 10 . 14778 / xxxxxxx . xxxxxxx 1 . INTRODUCTION Machine learning ( ML ) has proven itself in multiple con - sumer applications such as web ranking and recommenda - tion systems . In the context of enterprise scenarios , ML is emerging as a compelling tool in a broad range of appli - cations such as marketing / sales optimization , process au - tomation , preventative healthcare , and automotive predic - tive maintenance , among others . For such enterprise - grade ML applications [ 13 ] , often de - ployed in regulated environments , the standards for user privacy , security , and explainability are substantially higher which now has to be extended to ML models . Consider the following scenarios : Compliance . The protection of personal data is crucial for organizations due to relatively recent compliance regulations such as HIPAA [ 5 ] and GDPR [ 4 ] . As more emerging appli - cations rely on ML , it is critical to ensure eﬀective ongoing compliance in the various pipelines deployed in an organi - zation is preserved . Thus , developing techniques that auto - matically verify whether the developer’s data science code is compliant ( e . g . , tools that determine if the features used to build a machine learning model are derived from sensitive data such as personally identiﬁable information ( PII ) [ 57 ] ) is an immediate priority in the enterprise context . Reacting to data changes . Avoiding staleness in the ML models deployed in production is a crucial concern for many applications . To this end , detecting which models are af - fected because data has become unreliable or data patterns have changed , by tracking the dependencies between data and models becomes critical . For example , it is possible that the code used to populate the data had some bug which was later discovered by an engineer . In this case , one would like to know which ML models were built based on this data and take appropriate action . Similarly , one might want to investigate whether the feature set of a ML model should be updated , once new dimensions have been added in the data . Model debugging . Diagnosis and debugging of ML mod - els deployed in production remain an open challenge . An im - portant aspect of model debugging is to understand whether the decreased model quality can be attributed to the original data sources . For example , a data scientist while debugging her code might eventually ﬁnd that the ML model is aﬀected by a subset of the data which contains 0 values for a partic - ular feature . In such scenarios , one needs to automatically track the original data sources used to produce this model and evaluate whether they also contain 0 values . The aforementioned scenarios motivate the need for track - ing provenance end - to - end , from the data sources used for training ML models to the predictions of the deployed ML models . In this paper , we take a ﬁrst step towards this direction by introducing the ML provenance tracking prob - lem . The core idea is to automatically identify the relation - ships between data and ML models in a data science script and in particular , to track which columns in a dataset have been used to derive the features ( and optionally labels ) used to train a ML model . To address this problem , we design Vamsa 1 , a system that automatically tracks coarse - grained 1 Vamsa is a Sanskrit word that means lineage . 1 a r X i v : 2001 . 01861v1 [ c s . L G ] 7 J a n 2020 provenance from scripts written in Python ( the most com - mon language used by data scientists [ 7 ] ) using a variety of static analysis techniques . Consider the Python script presented in Figure 1 that was created in the context of the Kaggle Heart Disease compe - tition [ 6 ] . The script trains a ML model using a patient dataset from a U . S . hospital . The model takes as input a set of features such as Age , Blood pressure , and Cholestoral , and predicts whether a patient might have a heart disease in the future . After performing static analysis on the script , Vamsa not only detects that this script trains a ML model but also that the columns Target and SSN from the heart disease . csv dataset are not used to derive the model’s features . Building a system that captures such provenance infor - mation is challenging : ( 1 ) As opposed to data provenance in SQL , scripting languages are not declarative and thus may not specify the logical operations that were applied to the data [ 56 ] . This is exacerbated in dynamically typed languages , such as Python . ( 2 ) Data science is still an emerging ﬁeld as exempliﬁed by popular libraries like scikit - learn [ 46 ] still evolving their APIs and growth of newly avail - able frameworks like PyTorch [ 12 ] . ( 3 ) Scripts encode vari - ous phases of the data science lifecycle including exploratory analysis [ 62 ] , visualizations , data preprocessing , training , and inference . Hence , it is nontrivial to identify the rele - vant fraction of the scripts that contribute to the answer of a speciﬁc provenance query . Vamsa is speciﬁcally designed to address the aforemen - tioned challenges without requiring any modiﬁcations to the users code by solely relying on a modular architec - ture and a knowledge base of APIs of various ML libraries . Vamsa does not make any assumption about the ML li - braries / frameworks used to train the models and is able to operate on all kinds of Python libraries as long as the ap - propriate APIs are included in the knowledge base . Addi - tionally , Vamsa’s design allows users to improve coverage by simply adding more ML APIs in the knowledge base without any further code changes . This paper makes the following contributions : 1 . Motivated by the requirements of enterprise - grade ML applications , we formally introduce the problem of ML provenance tracking in data science scripts that train ML models . To the best of our knowledge , this is the ﬁrst work that addresses this problem . 2 . We present Vamsa , a modular system that tackles the ML provenance tracking problem in data science scripts written in Python without requiring any modiﬁcations to the users’ code . We thoroughly discuss the static analysis techniques used by Vamsa to identify variable dependen - cies in a script , perform semantic annotation and ﬁnally extract the provenance information . 3 . Using real - world data science scripts from Kaggle [ 8 ] and publicly available Python notebooks [ 54 ] , we perform ex - periments using up to 450 K scripts and verify the eﬀec - tiveness of Vamsa in terms of coverage , and performance . We also evaluate Vamsa’s accuracy on a smaller subset of manually labeled data . Our analysis shows that Vamsa’s precision and recall range from 87 . 5 % to 98 . 3 % and its latency is typically in the order of milliseconds for scripts of average size . Figure 1 : A data science script written in Python The rest of the paper is organized as follows : in Section 2 we formally deﬁne the problem of ML provenance tracking and in Section 3 we give an overview of Vamsa’s architec - ture . Sections 4 , 5 , and 6 provide a detailed description of Vamsa’s major components and their corresponding algo - rithms . Section 7 presents our experimental evaluation and Section 8 discusses related work . We conclude the paper and discuss directions for future work in Section 9 . 2 . PROBLEM STATEMENT We start by deﬁning the concepts used by Vamsa , followed by the problem of ML provenance tracking in data science scripts that Vamsa targets . A Data Source D can be a database table / view , a spread - sheet , or any other external ﬁles that is typically used in Python scripts to access the input data e . g . , hdf5 , npy [ 45 ] . A common ML pipeline accesses data source D and learns a ML model M with two steps . First , feature engineering is conducted to extract a set of training samples from D to be used to train the model M . The training samples consist of features and labels that are both derived from se - lected columns in D by e . g . , transformation functions . The training process then derives the model M by optimizing a learning objective function determined by the training sam - ples and speciﬁc predictive or descriptive needs . A Data Science Script reads from a set of data sources D and trains a set of machine learning models M 2 . In this work , we focus on scripts written in Python , as this is the major language currently used by data scientists [ 7 , 11 ] . We now formally deﬁne the problem of automated ML provenance tracking which Vamsa targets . The essence is to identify which columns in a dataset have been used to derive the features ( and optionally labels in the context of 2 Note that there are also data science scripts that do not perform any model training but provide other functional - ity ( e . g . , visualization , optimization , etc . ) In this work , we focus on data science scripts that include statements that train ML models as our goal is to capture the relationships between data sources and generated ML models . 2 supervised learning ) of a particular ML model in a data sci - ence script , thus automatically capturing the relationships between data sources and models at a coarse grain level and during static analysis time . ML Provenance Tracking . Given a data science script , ﬁnd all triples (cid:104) M , D , C (cid:105) where each M ∈ M is a con - structed machine learning model trained in the script us - ing data source D . ln particular , the model is trained us - ing features ( and optionally labels ) derived from a subset of columns of data source D , denoted as C . The goal is to identify each trained model M in the script , its data source D , and the columns C that were used to train model M . Example 1 : The input of Vamsa is a data science script such as the one in Figure 1 . The script reads from heart disease . csv as a data source D and trains an ensemble of decision trees using catboost library [ 49 ] . In this script , only a single model was trained . Note that not all the columns of the data source have been used to derive the model’s fea - tures and labels . To select the features , in the script , a range of columns [ 3 , + ∞ ) from D is explicitly extracted , followed by the drop of the columns { SSN , Target } . Similarly , only the Target column was used to derive the labels . Thus , the de - sired output is a triple (cid:104) M , D , C (cid:105) where M = clf is the vari - able that contains the trained model , D = heart disease . csv is the training dataset , and C is the set : [ 3 , + ∞ ) − { SSN } . Vamsa automatically parses the script and produces this output . (cid:50) 3 . VAMSA ARCHITECTURE Vamsa takes as input a script and produces the prove - nance information that captures the relationship between data sources accessed by the script and the ML models trained in the script . It follows a modular architecture ( il - lustrated in Figure 2 ) that addresses all the above challenges without requiring manual modiﬁcations to the users’ code . At a high - level , Vamsa performs static analysis on the Python script to determine the relationships between all the variables in the script , followed by an annotation phase that assigns semantic information to the variables in the script . It then uses a generic provenance tracking algorithm that extracts the feature set for all the ML models trained in the script and stores this information in a central catalog that can be accessed by various provenance applications . More speciﬁcally , Vamsa processes data science scripts with the following three major modules : the Derivation Ex - tractor , the ML Analyzer , and the Provenance Tracker that we discuss in detail in the following sections : ( 1 ) Derivation Extractor generates a workﬂow intermediate representation ( WIR ) of the script . It extracts the major workﬂow elements including imported libraries , variables , and functions , as well as their dependencies ( Section 4 ) . ( 2 ) ML Analyzer annotates variables in WIR based on their roles in the script ( e . g . , features , labels , and models ) . To this end , it uses our proposed annotation algorithm and a knowledge base that contains information about the vari - ous APIs of diﬀerent ML libraries ( Section 5 ) . Through the knowledge base , we are able to declaratively introduce se - mantic information to Python functions which in turn allows us to track provenance in data science scripts . ( 3 ) Provenance Tracker infers a set of columns that were explicitly included in or excluded from the features / labels Figure 2 : Vamsa Architecture . by using the annotated WIR and consulting the knowledge base . We remark that acquiring labeled data is non - trivial or even infeasible [ 59 ] in real - world settings . The Provenance Tracker is able to operate in both supervised and unsuper - vised learning settings ( in the latter by tracking provenance only at the features level ) . Vamsa does not make any assumption about the ML li - braries / frameworks used to train the models . By utilizing a modular architecture combined with a knowledge base of APIs for various ML libraries , Vamsa is able to operate on all kinds of Python libraries as long as the appropriate APIs are included in the knowledge base . Additionally , this de - sign allows users to improve coverage by simply adding more ML APIs in the knowledge base , without having to modify their code or Vamsa’s other components . To evaluation Vamsa , we have populated our knowledge base with APIs from four well - established data science li - braries : scikit - learn [ 46 ] , XGBoost [ 2 ] , LightGBM [ 3 ] , and Pandas [ 37 ] . Nevertheless , Vamsa can operate on top of any other library such as CatBoost [ 49 ] , StatsModels [ 58 ] , and Graphlab [ 1 ] , among others . 4 . DERIVATION EXTRACTOR In the ﬁrst phase , Vamsa parses the Python script and by performing static analysis , builds a workﬂow model which captures the dependencies among the elements of the script including imported libraries , input arguments , operations that change the state of the program , and the derived output variables . This model is captured in a workﬂow intermediate representation ( WIR ) ( Section 4 . 1 ) . Section 4 . 2 describes how Vamsa automatically generates the model instances . 4 . 1 Workﬂow Model To formally deﬁne the WIR model , we introduce the no - tions of variables , operations , and provenance relationships ( PRs ) . We then discuss how Derivation Extractor compo - nent generates the WIR for a given script . Variable . In programming languages , variables are con - tainers for storing data values . We denote the set of all variables in the data science script as V . For instance , cat - boost , cb , train df are a few examples of variables in the script of Figure 1 . 3 Figure 3 : An example WIR Operation . An operation p ∈ P operates on an ordered set of input variables I to change the state of the program and / or to derive an ordered set of output variables O . An operation may be called by a variable , denoted as caller c . While an operation may have multiple inputs / outputs , it has at most one caller . Example 2 : In Figure 1 , the import statements , read csv ( · ) in line 4 , attribute values in line 5 , CatBoostClassiﬁer ( · ) in line 9 , and ﬁt ( · ) in line 10 are examples of operations . Con - sider the ﬁt ( · ) operation : it is invoked by the clf variable and takes three arguments namely , features and labels , and an evaluation set . While ﬁt ( · ) does not explicitly produce an output variable , it changes the state of the variable clf from model to trained model . (cid:50) Provenance relationship . An invocation of an operation p ( by an optional caller c ) depicts a provenance relation - ship ( PR ) . A PR is represented as a quadruple ( I , c , p , O ) , where I is an ordered set of input variables , ( optional ) vari - able c refers to the caller object , p is the operation , and O is an ordered set of output variables that was derived from this process . A PR can be represented as a labeled directed graph , which includes ( 1 ) a set of input edges ( labeled as ‘input edge’ ) , where there is an input edge ( v , p ) for each v ∈ I , ( 2 ) a caller edge ( labeled as ‘caller edge’ ) ( c , p ) if p is called by c , and ( 3 ) a set of output edges ( labeled as ‘output edge’ ) , where there is an output edge ( p , v ) for each v ∈ O . For consistency , we create a temporary output vari - able for the operations that do not explicitly generate one . Example 3 : Consider line 4 of the script in Figure 1 where the CSV ﬁle is read . The corresponding PR is de - picted in Figure 3 ( dashed rectangle ) and corresponds to the quadruple ( I , c , p , O ) where I = { (cid:104) heart disease . csv (cid:105) } , c = pd , and p = read csv . We create a temporary variable and set O = { tmp csv } to be used as the input by another PR . (cid:50) Workﬂow Intermediate Representation . PRs are com - posed together to form a WIR G , which is a directed graph that represents the sequence and dependencies among the Figure 4 : A fraction of an abstract syntax tree ( AST ) extracted PRs . The WIR is useful to answer queries such as : “Which variables were derived from other variables ? ” , “ What type of libraries and modules were used ? ” , and “What operations were applied to each variable ? ” . More formally , a WIR is a directed bipartite graph G = ( V ∪ P , E ) with vertices V ∪ P and edges E ⊆ ( V × P ) ∪ ( P × V ) . Each edge has an associated type from the following set : { input edge , output edge , caller edge } . Example 4 : Figure 3 illustrates a fraction of a WIR that was generated from the script of Figure 1 . The variables and operations are represented by rectangles and ovals , re - spectively . The caller , input , and output edges are marked in blue , red , and black color , respectively . Consider the op - eration ﬁt , one can tell the following from the WIR : 1 ) it is called by variable clf ; 2 ) it has two ordered input variables train x2 and train y2 ; and 3 ) a temporary variable , denoted as tmp ﬁt , was created as its output . (cid:50) 4 . 2 WIR Generation Vamsa generates workﬂows with the following three - step process . First , its Derivation Extractor component parses 3 the script to obtain a corresponding abstract syntax tree ( AST ) [ 9 , 10 ] representation . It then identiﬁes the rela - tionships between the nodes of the AST to generate the PRs . Finally , it composes the generated PRs into a directed graph . Figure 4 shows a fraction of an AST that was generated from line 4 of the script in Figure 1 . The AST is a collec - tion of nodes that are linked together based on the gram - mar of the Python language . Informally , by traversing the AST from left - to - right and top - to - bottom , we can visit the Python statements in the order presented in the script . Due to the recursive nature of AST node deﬁnitions , the WIR generation algorithm is naturally recursive . The algo - rithm , denoted as GenWIR and illustrated in Figure 5 , takes as input the root of the AST tree and traverses its chil - dren from left - to - right . For each visited AST node , in or - der to generate PRs , it invokes a recursive procedure GenPR ( Figure 5 ) . Each invocation of GenPR in line 3 of GenWIR may create multiple PRs . All the PRs are accumulated ( line 4 ) and a graph G is constructed by connecting the inputs / caller / outputs of PRs . The procedure GenPR is illustrated in Figure 5 and takes as input an AST node and a set of already generated PRs . It returns a set of WIR variables and the updated PRs . The 3 https : / / github . com / python / cpython / blob / master / Lib / ast . py 4 Algorithm GenWIR Input : AST root node r . Output : WIR G . 1 . PRs : = ∅ ; 2 . for each v in children ( r ) do 3 . ( ∅ , PRs (cid:48) ) : = GenPR ( v , PRs ) ; 4 . PRs : = PRs ∪ PRs (cid:48) ; 5 . Construct G by connecting PRs ; 6 . return G ; Procedure GenPR ( v , PRs ) Input : AST node v and PRs generated so far . Output : a set of WIR variables and updated PRs . 1 . c : = ⊥ ; p : = extract from node ( v , ‘ operation ’ ) ; 2 . if v ∈ { Str , Num , Name , NameConstant } then 3 . return ( { v } , PRs ) ; 4 . ( I , PRs ) : = GenPR ( extract from node ( v , ‘ input ’ ) , PRs ) ; 5 . ( c , PRs ) : = GenPR ( extract from node ( v , ‘ caller ’ ) , PRs ) ; 6 . ( O , PRs ) : = GenPR ( extract from node ( v , ‘ output ’ ) , PRs ) ; 7 . PRs : = PRs ∪ PR ( I , c , p , O ) ; 8 . return ( O , PRs ) ; Figure 5 : WIR generation algorithm returned WIR variables may be used as input / caller / output of other PRs . To this end , GenPR initially obtains the opera - tion from the attributes of the AST node ( line 1 ) . If the AST node is a literal or constant [ 9 , 10 ] , it returns the current PRs ( line 3 ) . Otherwise , to obtain each of input variables I , potential caller c , and potential derived variables O , GenPR recursively calls itself ( lines 4 - 6 ) . Once all the required vari - ables for a PR are found , a new PR is constructed and added to the set of so far generated PRs ( line 7 ) . It ﬁnally returns the output of the last generated PR as well as the updated set of PRs ( line 8 ) . During this process , the procedure GenPR extracts the in - put and output set and a potential caller variable for each PR ( see deﬁnition of PR in Section 4 . 1 ) . To this end , it investi - gates the AST node attributes to instantiate these variables by invoking the extract from node procedure which we sum - marize next . The procedure takes as input an AST node and and a literal parameter denoting the information requested ( input , output , caller , operation ) , and consults the abstract grammar of AST nodes [ 9 ] to return the requested informa - tion for the given node . For example , when processing the Assign node of the AST in Figure 4 , the procedure identiﬁes Assign . value as input , Assign as operation , and Assign . targets as output . It also sets the caller as ∅ , as the procedure does not return a caller for the AST node type Assign . Complexity . Each AST edge is visited at most once dur - ing the WIR generation . Thus , for a Python script whose corresponding AST contains N edges , GenWIR has O ( N ) complexity . More speciﬁcally , the extract from node pro - cedure requires constant time since for each visited AST node , it only traverses a bounded number of neighbors ( see the Python grammar [ 9 ] ) . In addition , the number of nodes / edges in a WIR is also bounded by the num - ber of nodes / edges in its corresponding AST since for each node / edge in the AST , we may generate a corresponding node / edge in WIR . 5 . MACHINE LEARNING ANALYZER The generated WIRs capture the dependencies among the variables and operations in a script . Nevertheless , WIRs alone do not provide semantic information such as the role of a variable in the script ( e . g . , ML model , features ) or the type of each object ( e . g . , CSV ﬁle , DataFrame ) . To support provenance queries , semantic information about variables should be associated to the WIRs . Such information , in turn , identiﬁes critical variables such as hyperparameters , models , and metrics for ML applications . Finding the role of each variable in a WIR is a challeng - ing task for multiple reasons : ( 1 ) One cannot accurately deduce the role / type of input and output of each operation by only looking at the name of the operation as diﬀerent ML libraries may use the same name for diﬀerent tasks ; ( 2 ) Even in the same library , an operation may accept diﬀerent number of inputs or provide diﬀerent outputs . For example , in the sklearn library [ 46 ] , the function ﬁt accepts a single in - put when creating a clustering model but two inputs when generating a classiﬁcation / regression model ; ( 3 ) The type of the caller object might also aﬀect the behavior of the oper - ation . For instance , in sklearn , invocation of the ﬁt function by a RandomForestClassiﬁer creates a model but calling it via LabelEncoder does not ; ( 4 ) The APIs of many libraries are not yet stable and change as these evolve ; ( 5 ) Some vari - ables are even harder to semantically annotate because of lack of concrete APIs associated with them . For example , identifying when a variable represents features is challenging since typically there is no speciﬁc API to load the training dataset . Instead , the common practice is to use generic func - tions such as read csv to load training data similarly to other data sources . A semantic annotation framework to be usable across var - ious data science scripts must be : ( 1 ) compatible with the various ML libraries [ 46 , 37 , 2 , 49 , 3 , 58 , 1 ] and their dif - ferent versions , and ( 2 ) extensible to accommodate new ML libraries . To this end , we propose an annotation algorithm that relies on a knowledge base of ML APIs ( KB ) that con - tains information on the APIs of various ML libraries , their modules and signatures ( Section 5 . 1 ) . The KB can be used to answer questions such as “What is the role of the in - put / output variables of a particular operation belonging to a given ML library ? ” . Our annotation algorithm annotates the WIR by querying the KB to obtain semantic information about the various variables and operations . 5 . 1 Knowledge Base of ML APIs The KB contains ﬁne - grained information about ML li - braries stored in the form of relational tables . For each li - brary , the KB stores its name ( e . g . , sklearn , xgboost ) , version , and modules ( e . g . , ensemble , svm ) . For each unique API in a library , the KB captures the corresponding library , module , caller type , and the operation name ( e . g . , train test split from the model selection module of the sklearn library or read csv from the Pandas library ) . For each potential input of an operation , the KB stores its role ( features , labels , hyperpa - rameter , and metric ) and its data type ( DataFrame , array , CSV ﬁle ) . Similarly , the KB contains semantic information about the outputs of the various operations . Example 5 : Table 1 shows three tuples in our KB . These are a subset of tuples that are utilized by the annotation algorithm to identify the variables that correspond to mod - els and features in the script of Figure 1 . The second tuple shows that when the operation ﬁt is called via a model con - structed by catboost library , its ﬁrst and second input are features and labels , respectively . It also accepts the valida - 5 Library Module Caller API Name Inputs Outputs catboost NULL NULL CatBoostClassiﬁer eval metrics : hyperparameter model catboost NULL model ﬁt featureslabels eval set : validation sets trained model sklearn model selection NULL train test split featureslabels test size : testing ratio features validation features Table 1 : Example of facts in Vamsa knowledge base tion sets as input . The output of the operation is a trained model . (cid:50) To facilitate the annotation of WIR variables , KB sup - ports two types of queries . The ﬁrst one denoted as KB ( L , L (cid:48) , c , p ) takes as input the name of a library , mod - ule , caller type , and operation name and returns a set of user - deﬁned annotations that describe the role and type for each input / output of operation p . The second one denoted as KB ( O , p ) obtains the annotations of the input variables of operation p given the annotations of its output variables . Note that in our current prototype , and similar to other eﬀorts for KB population [ 34 ] , the construction of Vamsa’s KB is manual . As such the construction and maintenance costs may seem to be non - negligible over time . As we show in our experiments , however , our manual ( yet minimal ) KB results in large coverage on big collections of data science scripts . This is primarily because many data science scripts rely on similar coding patterns . Finally , we note that an orthogonal and really interesting future work is how to pop - ulate such KBs automatically . 5 . 2 Annotation Algorithm The annotation algorithm traverses the WIR and anno - tates its variables by appropriately querying the KB when needed . After each annotation , new semantic information about a WIR node is obtained that can be used to enrich the information associated with other WIR variables , as is typ - ical in analysis of data ﬂow problems [ 61 ] . The propagation of semantic information is achieved through a combination of forward and backward traversals of the WIR . The algorithm , illustrated in Figure 6 , annotates the WIR variables by using the KB . It takes as input an extracted WIR G from a script and the knowledge base KB and com - putes an annotated WIR G + enriched with the semantic information . The algorithm starts by ﬁnding a set of PRs with p = Import as a seed set S for upcoming DFS traversals ( line 1 ) . These PRs contain the information about imported libraries and modules in the Python script . For each v s ∈ S , the algo - rithm extracts the library name L and the potential utilized module L (cid:48) ( line 3 ) . It then initiates a DFS traversal that starting from v s traverses the WIR in a forward manner i . e . , by going through the outgoing edges ( line 4 ) . For each seen PR , it obtains the annotation information for both of its inputs I and outputs O by querying the knowledge base ( lines 5 - 6 ) as described in the previous section . If a new annotation was found for an input variable v i ∈ I , the algorithm initiates a backward DFS traversal . As the input variable v i can be the output of another PR , for new information discovered for v i , we can propagate this Algorithm Annotation Input : WIR G and knowledge base KB . Output : Annotated WIR G + . 1 . Find the Import process nodes in G as the seed set S ; 2 . for each v s ∈ S do 3 . Extract library L and module L (cid:48) ; 4 . Starting from v s , follow a DFS forward traversal on PRs : 5 . for each seen PR = (cid:104) I , c , p , O (cid:105) do 6 . Obtain annotation of v i ∈ I and v o ∈ O by invoking KB ( L , L (cid:48) , c , p ) 7 . for each annotated v i ∈ I do 8 . Starting from v i , follow a DFS backward traversal on PRs : 9 . for each seen PR = (cid:104) I , c , p , O (cid:105) do 10 . Obtain annotation of v i ∈ I by invoking KB ( O , p ) 11 . return G + ; Figure 6 : Annotation algorithm information to other PRs in which v i is their output . In particular , starting from v i , the algorithm traverses the WIR in a backward manner i . e . , by going through the incoming edges ( line 8 ) . During the backward traversal , the KB is used to obtain information about the inputs of an operation given its already annotated output . In each initiated DFS traversal , each edge is visited only once . The algorithm terminates when we cannot obtain more information from initiating more forward / backward traversals . Example 6 : Operating on the WIR of Figure 3 , the anno - tation algorithm initializes the seed set S with one import op - eration and sets L = catboost and L (cid:48) = ⊥ . Once it visits the p = CatBoostClassiﬁer operation , it queries the KB to obtain the annotation of its output . Given , L , L (cid:48) , c = catboost and p , the KB annotates clf as a model . Since there exists no input edge for the CatBoostClassiﬁer node in this WIR , no backward traversal is initiated . The algorithm moves forward and vis - its the ﬁt function . It queries the KB with the same L and L (cid:48) , but updated c = model and p = ﬁt . The algorithm an - notates the output of ﬁt as trained model and then stops the forward propagation since there are no more outgoing edges in the node . However , at this time , KB successfully annotated the train x2 and train y2 as the features and labels , respectively . Thus , two backward traversals are started to propagate this information as much as possible to the pre - vious nodes in the WIR . Let us follow the DFS that was started from train x2 . By visiting the train test split node , the algorithm annotates train x as features . Similarly , it back - propagates the new annotation to train df as the caller of 6 Figure 7 : WIR with Subscript operation drop operation . The algorithm continues until we cannot obtain more annotation information . (cid:50) Complexity . In WIR G = ( V ∪ P , E ) , let V L ⊆ V be the set of nodes corresponding to the import operations and let | V L | denote its cardinality . The annotation algorithm exe - cutes | V L | rounds of forward DFS traversals . Furthermore , each forward DFS may initiate a backward DFS traversal for a newly visited input variable . Note that the backward traversal is executed only if the operation is included in the KB . Consider the set of those operations P ⊆ P and let d be the maximum in - degree of the nodes corresponding to oper - ations in P . The number of these executions is bounded by O ( d | P | ) . Note that since each DFS visits an edge at most once , it takes up to O ( | E | ) time . Thus , in the worst case , the algorithm has O ( d | V L | | P (cid:48) | | E | ) complexity . Our analysis with real - world scripts [ 54 ] shows that the average | V L | and d is typically small ( | V L | = 8 . 49 and d = 8 . 00 ) . 6 . PROVENANCE TRACKER We next introduce the provenance tracker component of Vamsa . The provenance tracker is responsible for automat - ically detecting the subset of columns in a data source that was used to train a ML model . We’d like to point out that this is only one of the various provenance / tracking applica - tions that can be built on top of the ML Analyzer . To identify the columns , we need to investigate the op - erations in the annotated WIR G + that are connected to variables that contain features and labels in their annota - tion set . There are various operations that take features ( or labels ) as their caller / input , and may apply transfor - mations , drop a set of columns from it , select a subset of rows upon satisfaction of a condition , copy it into another variable , and / or use it for visualization , etc . All these dy - namic , runtime operations , and their dependencies should be captured in G + . Following this intuition , we enrich our KB with a new ta - ble to guide our provenance tracker algorithm . The new table consists of two types of operations as follows : 1 ) op - erations from various Python libraries that exclude columns ( e . g . , drop and delete in Pandas library ) or explicitly select a subset of columns ( e . g . , iloc and ix ) , and 2 ) a few native Python operations such as Subscript , ExtSlice , Slice , Index , and Delete [ 9 , 10 ] . For each entry in this table , we set a ﬂag column exclusion = True if the corresponding operation can be used for column exclusion ( e . g . , drop and delete ) . We remark that some operations captured in the KB can be used to remove both columns and rows depending on the values of one or more input parameters . As an example , the function drop in the Pandas library is used to remove rows when the parameter axis is set to 0 , and remove columns when the value of the parameter is 1 . The parameters of the opera - tions are also captured in the WIR , and thus we can easily verify their values . The condition that needs to be checked to verify whether a particular invocation of an operation is used to remove columns is also added into the KB along with the operation . We query this table by invoking KB C ( p ) where p is the name of the operation . The query returns ∅ if there is no matching entry in the KB . However , if the operation matches to one of the entries in the table , the query re - turns the following output : ( 1 ) condition : the condition as - sociated with the operation as mentioned above ( if any ) ; 2 ) column exclusion : whether the operation can be used for col - umn exclusion ; and 3 ) traversal rule : a description on how to start a backward traversal from the node’s input edges in order to identify a set / range of indices / column names . Example 7 : Figure 7 is another fraction of WIR that was generated from line 5 of the script in Figure 1 that includes a Subscript operation . The statement in line 5 keeps all the rows but only includes the columns from index 3 to the last index in the dataset . One can ﬁnd the set of included columns by traversing backward the nodes following the in - put edge of the Subscript operation and reaching the constant values connected with the Slice operations . The traversal rule associated with the Subscript operation shows that the input edge of this node must be followed in a backward man - ner to eventually reach the selected columns . Note that this is the case for all WIRs that contain this operation . Similarly , consider the drop operation in Figure 3 . This op - eration is related to feature selection since its caller ( train df ) was annotated as features and it operates at the level of columns ( the condition axis = 1 is satisﬁed by this invo - cation of the operation ) . To ﬁnd the columns that were dropped , we again need to follow the input edge of drop backwards until we reach the constants ‘Target’ and ‘SSN’ . (cid:50) Our provenance tracking algorithm is illustrated in Fig - ure 8 . The algorithm takes as input the annotated WIR G + and the KB , and returns two column sets : the columns that from which features / labels were explicitly derived ( inclusion set C + ) and ( 2 ) the columns that are explicitly excluded from the set of features / labels ( exclusion set C − ) . The al - gorithm scans each PR to ﬁnd the ones with a variable that has been annotated as features ( or labels ) and an operation which can potentially be used for feature ( or label ) selec - tion based on the information stored in the KB ( line 2 - 3 ) . A core component of the algorithm is the GuideEval opera - tor ( shown in Figure 8 ) that starts a guided traversal of the WIR based on the information in the KB . For each of the selected PR , the GuideEval operator queries the KB and obtains the corresponding condition , 7 Algorithm PTracker Input : Annotated WIR G + , knowledge base KB . Output : Column inclusion set C + , column exclusion set C − . 1 . C + : = ∅ ; C − : = ∅ ; 2 . for each PR in PRs do 3 . if it has a variable that was annotated as features or labels and KB C ( p ) (cid:54) = ∅ 4 . GuideEval ( PR , G + , KB , C + , C − ) ; 5 . return C + , C − ; Operator GuideEval ( PR , G + , KB , C + , C − ) Input : Visited PR , annotated WIR G + , knowledge base KB , column inclusion set C + , column exclusion set C − . Output : Updated C + , C − . 1 . condition , column exclusion , traversal rule = KB C ( p ) ; 2 . if exists condition and it is False then return ∅ 3 . if PR has constant inputs cnst then 4 . if column exclusion = True then 5 . C − : = C − ∪ cnst ; 6 . else C + : = C + ∪ cnst ; 7 . return C + , C − ; 8 . Obtain new PR on G + based on traversal rule ; 9 . GuideEval ( PR , G + , KB , C + , C − ) ; Figure 8 : Provenance tracking algorithm column exclusion ﬂag , and traversal rule ( line 1 ) . If a condition exists but it is not matched by the particular op - eration , we can deduce that the operation was not used for feature ( or label ) selection and return without further action ( line 2 ) . Otherwise , the operator checks if this PR contains constant values in its input set ( line 3 ) . If so , it incorporates the discovered constant values / range of column indices into the inclusion / exclusion sets based on the column exclusion ﬂag . In case the PR does not directly contain the columns , the GuideEval operator follows the traversal rule to obtain a new PR on G + ( line 8 ) that needs to be evaluated . It then calls the GuideEval operator again for this PR ( line 9 ) . Example 8 : Continuing example 7 , the provenance track - ing algorithm ﬁnds the drop operation with a caller that was annotated as features ( Figure 3 ) and thus invokes the GuideEval operator . Since the corresponding path query is satisﬁed ( aka , axis = 1 ) , we know that the operation is used for feature selection and in particular feature exclusion ( based on the information in the KB ) . Thus , the algorithm follows the traversal rule to perform a backward traversal from its the operation’s input edge until it ﬁnds the con - stants ‘Target’ , and ‘SSN’ . These two columns are then added to the exclusion set . When the feature tracking algorithm ﬁnds the Subscript operation ( Figure 7 ) in the annotated WIR , it invokes the GuideEval operator again . Note that the Subscript operation does not have an associated path query in KB . Thus , the GuideEval operator only obtains the corresponding traversal rule from the KB and initiates a backward traversal starting from the input edge of the Subscript operation . A similar pro - cess is performed when the GuideEval operator visits ExtSlice and Slice nodes . Using the traversal rule associated with the Slice operation in the KB , the algorithm looks for a range of columns with lower bound ( respectively upper bound ) that can be found by traversing the appropriate input edges of the Slice node ( see Figure 7 ) . (cid:50) Complexity Analysis . Let V T ⊆ V be the set of variables Dataset Error - free & Python 3 compatible NTBK 2 / Kaggle 2 Scripts with selectedMLlibraries NTBK 3 / Kaggle 3 NTBK ( 807 K ) 447 K 28 . 9 K Kaggle ( 4 . 8 K ) 4 . 2 K 1 . 2 K Table 2 : Output of pre - processing pipeline that were annotated as features or labels . Let | P m | be the maximum number of operations that are directly connected to a variable in V T . The provenance tracker algorithm scans all the PRs to ﬁnd the set V T and evaluates , in constant time , whether the corresponding operations are related to feature / label selection . If an operation is indeed related to feature selection , the algorithm follows the traversal rule which in the worst case , visits all the edges of G + . The algorithm , thus , has O ( | V T | | P m | | E | ) complexity . Note that in practice | V T | (cid:28) | V | . 7 . EXPERIMENTAL EVALUATION In this section , we evaluate Vamsa on a large set of Python scripts and provide an analysis of our experimental results . Our experiments are designed to answer the following ques - tions : ( 1 ) What is the accuracy of Vamsa in identifying the features used to train ML models ? ; ( 2 ) How often is Vamsa able to extract provenance information ( coverage ) from a data science script ? ; ( 3 ) What is the latency of Vamsa ? 7 . 1 Experimental Settings Datasets . To evaluate Vamsa on a variety of data sci - ence scripts , we downloaded a large set of publicly avail - able Python scripts from two diﬀerent data sources : ( 1 ) a large - scale corpus that consists of 1 . 2 M Python notebooks published in 2017 that was crawled from Github [ 54 ] ( NTBK dataset ) . From this corpus , we excluded the notebooks that do not include any import statement resulting in a corpus of 807 K scripts , ( 2 ) a set of 4 . 8 K Python scripts that we downloaded via the public Kaggle API 4 ( Kaggle dataset ) . Dataset pre - processing pipeline . Real - world scripts may have syntax errors or may not be compatible with Python 3 ( which is the version of Python that Vamsa’s im - plementation currently targets ) . Moreover , not all of them train machine learning models . For these reasons , we cre - ated a data pre - processing pipeline that applies various ﬁl - ters to the scripts in order to capture only those that are rel - evant to the ML provenance tracking problem . The pipeline is invoked on both the NTBK and Kaggle dataset . We now show how the pipeline works using the NTBK dataset as an example . The pipeline takes as input the 807 K Python scripts and prunes the scripts for which we cannot generate the corresponding abstract syntax tree due to syn - tax errors , exceptions triggered by Python’s AST generation module or incompatibility with Python 3 . The resulting dataset is denoted as NTBK 2 . The pipeline then prunes the scripts that are not importing any of the following ML frameworks : scikit - learn , XGBoost [ 2 ] , and LightGBM as well as the scripts that do not invoke any training - related operations from these frameworks ( e . g . , ﬁt , create , and train , 4 https : / / github . com / Kaggle / kaggle - api 8 Dataset Column Exclusion Column Inclusion Annotation Precision Precision Recall Jaccard Coeﬀ . Precision Recall Jaccard Coeﬀ . Model Train Dataset Kaggle 3 93 . 82 % 94 . 97 % 92 . 78 % 87 . 57 % 89 . 22 % 87 . 49 % 100 % 99 . 33 % NTBK 3 98 . 33 % 98 . 28 % 97 . 94 % 90 . 21 % 95 . 24 % 90 . 14 % 100 % 98 . 66 % Table 3 : Accuracy of Vamsa on the labeled datasets Dataset ML Analyzer Coverage Prov . Tracker Coverage Model Train Dataset Kaggle 3 94 . 62 % 82 . 91 % 80 . 61 % NTBK 3 88 . 52 % 77 . 81 % 70 . 20 % Table 4 : Vamsa coverage in large - scale evaluation etc ) . The resulting dataset is denoted as NTBK 3 . Note that most of our experiments are performed on this dataset as we have populated our KB with APIs from the selected ML libraries discussed above . Note that it is easy to extend to other libraries by just populating the KB ( no code changes are required ) . Table 2 shows more information about the NTBK and Kaggle dataset after the pipeline has been applied to them . Experimental methodology . A challenge when evalu - ating Vamsa with such large - scale corpora is to determine the correctness of the output . Unfortunately , due to the novel nature of ML provenance tracking , there is no public benchmark available . The brute force approach would be to manually go over the corpus and determine the relationships between ML models and data sources so that we can evalu - ate Vamsa’s output . Since this is not feasible at the scale we are operating , we decided to perform two classes of experi - ments . First , we select a small subset of scripts for which we manually extract the provenance information ( ground truth ) and evaluate the accuracy of Vamsa on those 5 . The second class of experiments is performed on the large corpus . The goal is to evaluate the coverage of the system , deﬁned as how often Vamsa extracts the provenance information . We also evaluate both component - level and end - to - end system performance . Hardware and software conﬁguration . We conducted our experiments on a Linux machine powered by an Intel 2 . 30 GHz CPU with 8 GB of memory . For all the experi - ments we used Python 3 . 7 . 2 . We manually populated our knowledge base with the APIs from scikit - learn , XGBoost , LightGBM , and Pandas [ 37 ] . 7 . 2 Experiments with Labeled Datasets These experiments evaluate the accuracy of Vamsa on a set of Python scripts for which we have manually extracted the relationship between data sources and ML models . From each of the Kaggle 3 and NTBK 3 datasets , we randomly se - lected 150 scripts , ensuring that Vamsa can produce out - put for all the selected scripts . We evaluate the accuracy of Vamsa on both column exclusion and column inclusion using three metrics : precision , recall and Jaccard coeﬃ - cient . The precision shows the proportion of discovered in - cluded / excluded columns that were truly included / excluded 5 Note that we plan to open - source this dataset so that it can be used as a benchmark for future ML provenance tracking eﬀorts . columns in the feature selection process . The recall shows the proportion of the true included / excluded columns that were discovered by Vamsa . The Jaccard coeﬃcient evaluates the similarity of the two sets . The higher the values of these metrics are , the better the accuracy of Vamsa is . Given a script , the ground truth consists of two sets , namely the in - cluded columns C + T and excluded columns C − T . The metrics for column exclusion are deﬁned as follows : Precision = | C − ∩ C − T | | C − | ( 1 ) Recall = | C − ∩ C − T | | C − T | ( 2 ) Jaccard Coeﬃcient = | C − ∩ C − T | | C − ∪ C − T | ( 3 ) The metrics for column inclusion are similar but take into account the column inclusion set that Vamsa produces as well as the included columns in the ground truth . Additionally , we investigate in more detail , how often Vamsa correctly identiﬁes which variables correspond to ML models and which to training datasets as this is a prereq - uisite for correctly identifying the features / labels . To this end , we also report results that show the precision of the annotation phase ( for both models and training datasets ) . The annotation precision shows the proportion of discovered models / training datasets that were true models / training datasets according to the manual labeling we did on the two datasets . Table 3 shows the results on the two datasets . For each metric , we report the average values obtained over the 150 scripts of the dataset . As shown in the table , Vamsa achieves high precision and recall values for all the tasks evaluated . Overall , we can make the following observations : 1 . When Vamsa identiﬁes a model , its training dataset , and the corresponding features , the output is highly reliable . 2 . Vamsa reported models 100 % accurately and made a few mistakes in detecting their training datasets . We further investigated these scripts and found that the data sci - entists appended the testing data to the training data in order to perform global value transformations . The merged test data then got separated via a slicing oper - ation immediately before training . Vamsa’s annotation algorithm was not able to follow this operation , i . e merge followed by split , and mistakenly identiﬁed the testing dataset as the training dataset . 3 . Vamsa detects column exclusion sets slightly better than column inclusion ones . This is because , for column ex - clusion , data scientists typically use a set of speciﬁc APIs such as drop and pop from Pandas , or del keyword , which can be tracked more easily . 9 1 10 100 1000 10000 Kaggle 4 NTBK 4 T i m e ( m s ec s ) Vamsa Derivation Ext . ML Analyzer Prov . Tracker Figure 9 : Latency breakdown 7 . 3 Large - scale Experiments In these experiments , we use a large corpus of Python scripts both from the NTBK and Kaggle datasets extracted from the pre - processing pipeline . The goal is to evaluate the coverage of various components of Vamsa ( Derivation Extractor , ML Analyzer , Provenance Tracker ) as well as the performance / eﬃciency of the system . We also present a detailed analysis of the cases where Vamsa was not able to produce an answer . Derivation Extractor . First , we evaluate the coverage of Vamsa on generating the workﬂow intermediate representa - tion for our scripts . Note that the Derivation Extractor is a standalone component that does not rely on the KB to produce the WIR . For this reason , we perform our exper - iment using two large datasets NTBK 2 ( 447 K scripts ) and Kaggle 2 ( 4 . 2 K scripts ) that import multiple ML libraries . Our results show that Vamsa successfully generates the WIR for 95 . 70 % of the scripts in the Kaggle 2 dataset and 89 . 27 % of the scripts in the NTBK 2 dataset . The few cases where Vamsa is not able to produce a WIR are mainly due to Vamsa’s current implementation . In particular , we have not yet covered certain constructs in the Python grammar such as DictComp , SetComp , and JoinedStr . However , we’d like to note that incorporating these constructs in Vamsa is solely a matter of extending the implementation and does not require any change in Vamsa’s design or architecture . ML Analyzer . The goal of this experiment is to investigate the coverage of the ML Analyzer and in particular , how often the annotation algorithm identiﬁes ML models and training datasets . In this evaluation , we use the NTBK 3 ( 28 . 9 K scripts ) and Kaggle 3 ( 1 . 2 K scripts ) datasets . Table 4 shows the percentage of the cases where the ML Analyzer can annotate at least one variable as a model and one other variable as a training dataset . As shown in the table , Vamsa can report model and training datasets for 94 . 62 % and 82 . 91 % of the scripts in the Kaggle 3 dataset . The coverage is a bit lower for the NTBK 3 dataset . To better understand the cases where Vamsa was not able to perform the annotation , we ﬁrst investigated the cases that the ML Analyzer could not ﬁnd a model . We iden - tiﬁed the following reasons for the failure : 1 ) Some scripts called APIs commonly used for training models , such as e . g . , ﬁt , to perform other operations such as feature extraction . In these cases , the ML Analyzer correctly did not report any model . 2 ) In a few scripts , the statements used to train a model were commented out . This was not detected by our pre - processing pipeline and thus these scripts were falsely included in the ﬁnal dataset . 3 ) Some scripts im - ported modules using the * notation . In these cases , Vamsa could not relate the import statement to the API calls . 4 ) In a few other scripts , the data scientist imported a mod - ule with an alias name and used the alias when invoking the APIs . Vamsa’s implementation does not currently cover such cases . We are continuously investigating these issues and improving Vamsa’s implementation . We have also explored the cases where the ML Analyzer could not ﬁnd a training dataset . We found that : 1 ) In some scripts , hard - coded data e . g . , a large numpy array was used as the training data , and 2 ) Some APIs are not presented in our KB and thus the annotation algorithm is not able to perform back propagation . However , we’d like to point out that these cases would be covered by extending out KB to include more APIs . We note that allowing users to increase coverage by enhancing the KB as needed was one of the major requirements behind Vamsa’s design . Provenance Tracker . We evaluate the Provenance Tracker component on the NTBK 3 and Kaggle 3 datasets . Table 4 shows the percentage of the cases where the Prove - nance Tracker can identify at least one set of features . Note that the Provenance Tracker is invoked only if the ML An - alyzer can identify a model and its corresponding training dataset . We thus expect the coverage of this component to be bounded by the coverage of the ML Analyzer . As shown in Table 4 , Vamsa reports a non - empty column set for 80 . 61 % of the scripts in Kaggle 3 dataset and 70 . 20 % of the scripts in the NTBK 3 dataset . We have also analyzed the cases that Vamsa could ﬁnd both a ML model and a training dataset but did not discover the column set . The main reasons for this behavior are the following : 1 ) In some scripts , the columns have not been selected explicitly but based on a condition on their values ( e . g . , a column is in the feature set if it contains at least N non - zero values . ) , ( 2 ) Similar to the ML Analyzer , some scripts required new rules to be added into the KB for the Provenance Tracker to operate correctly , and ( 3 ) Some scripts did not include any feature selection operations and thus Vamsa did not produce any output . 7 . 4 Performance Experiments In this experiment , we evaluate the eﬃciency of each com - ponent of Vamsa as well as the end - to - end latency . To this end , we use a subset of the datasets on which Vamsa oper - ates end - to - end successfully . We call these datasets NTBK 4 ( 20 . 3 K scripts ) and Kaggle 4 ( 1 K scripts ) . Breaking down the latency . We now evaluate the per - formance of the Derivation Extraction , ML Analyzer , and Provenance Tracker , as well as the end - to - end latency . Fig - ure 9 shows the results . We observe that the time spent by each component is negligible on both datasets and on aver - age is in the order of milliseconds . Furthermore , the most time is spent on derivation extraction in comparison to the other components . Breaking down the Derivation Extractor tasks to AST generation , PR generation , and WIR compo - sition , we observed that most of the time is spent in AST generation and WIR composition . In particular , our analy - sis shows that for the Kaggle 4 dataset , AST generation takes 106 milliseconds ( msecs ) , PR generation takes 13 msecs and WIR composition takes 121 msecs . The corresponding num - bers for the NTBK 4 dataset are : 112 msecs , 11 msecs and 129 msecs . 10 1 10 100 1000 10000 150 300 450 600 750 900 T i m e ( m s ec s ) # Lines of code Derivation Ext . ML Analyzer Prov . TrackerVamsa ( a ) Kaggle 4 1 10 100 250 500 750 1000 1250 1500 T i m e ( s ec s ) # Lines of code Derivation Ext . ML Analyzer Prov . TrackerVamsa ( b ) NTBK 4 Figure 10 : Latency while varying the script size 100 1000 Kaggle 4 NTBK 4 C a r d i n a lit y Lines of Code WIR Nodes WIR Edges Figure 11 : Size of WIR Performance of Vamsa varying the lines of code . We further evaluate the performance of each component and the end - to - end performance as the lines of code in the script vary . Figure 10 ( a ) and Figure 10 ( b ) show the average la - tency of the components as the script size varies for both our datasets . We see that increasing the number of lines of code in a Python script naturally increases the latency of all Vamsa components . However , Vamsa can produce out - put within 9 seconds on a script consisting of 850 lines of code ( see Figure 10 ( a ) ) . Similarly , Vamsa requires 64 sec - onds to collect the provenance information on a script with 1 . 5 K lines of code ( see Figure 10 ( b ) ) . Size of intermediate representation . To gain more in - sight about the datasets , we evaluate the size of generated WIRs . Figure 11 shows the average size of WIR nodes and edges along with the average lines of code in the script . We see on average for each line of code , 8 . 70 nodes and 6 . 11 edges were created in a WIR . 8 . RELATED WORK We describe relevant related work from three areas : model management systems , provenance in databases , and work - ﬂow management systems . Model management systems . There has been emerging interest in developing machine learning life - cycle manage - ment systems [ 55 , 26 , 64 , 41 , 56 , 40 , 63 ] . ModelDB [ 64 ] was one of the ﬁrst open - sourced model management systems . It focused on storing trained models and their results to enable visual exploration and querying of metadata and ar - tifacts ( e . g . , hyperparameters and accuracy metrics ) . Mod - elDB requires users to change their scripts to comply with their API for logging ( e . g . , adding “sync” to function calls ) , and it works for a speciﬁc set of libraries . ModelHub [ 41 ] aimed to store model weights across diﬀerent versions with a focus on deep learning . It is a more ﬁne - grained version - ing system for the ML artifacts than other general - purpose systems such as Git . ModelHub enables querying on hyper - parameters , accuracy , and information loss during training phases . Amazons ML experiments system [ 56 ] focuses on tracking metadata and provenance of ML experimentation data . This system automated the provenance extraction for SparkML [ 39 ] and scikit - learn [ 46 ] pipelines whenever a logi - cal abstraction of operations ( e . g . , estimators / transformers ) are available . ProvDB [ 40 ] proposed a graph data model and two graph query operators to store and query the prove - nance in data science projects . It works by ingesting the provenance via shell commands similar to versioning sys - tems such as Git . The major focus of ProvDB is to eﬃciently store and query the ML provenance data . Finally , to enable the model diagnosis , Mistique [ 63 ] was developed to store model intermediates that are produced in diﬀerent stages of traditional ML pipelines or hidden representations in deep learning ( i . e . , neuron activations produced by diﬀerent lay - ers in a neural network ) . In contrast to this line of model management systems , ( 1 ) Vamsa does not require developers to modify their code , ( 2 ) Vamsa can operate on top of any library as long as it is included in the KB , ( 3 ) by focusing on the process of provenance extraction rather than eﬃciently storing the captured data , Vamsa is complementary to systems such as ProvDB [ 40 ] and Mistique [ 63 ] , and ( 4 ) none of the previous systems aimed to track features that are used to train ML models . Provenance in databases . Capturing lineage or prove - nance has been studied extensively for databases ( e . g . , sur - veys [ 60 , 31 , 20 , 17 ] . Data provenance typically describes where data came from , why an output record was pro - duced [ 18 ] , and how it was derived [ 27 ] . Provenance can be captured at diﬀerent granularities and in various levels of detail [ 40 ] . It can be as coarse - grain as datasets , ﬁles , and 11 their dependencies [ 14 ] or ﬁne - grain including dependencies between input , intermediate , and output records [ 20 , 35 , 51 ] . The value of provenance is most exaggerated by the appli - cations that it can support including , but not limited to , explanations [ 44 , 65 , 24 ] ; interactive visualizations [ 51 , 50 , 52 , 30 ] ; veriﬁcation and recomputation when data sources are outdated or not reliable [ 32 ] , debugging [ 33 , 36 , 21 ] ; data integration [ 22 ] ; auditing and compliance [ 4 ] ; and se - curity [ 19 , 35 ] . Finally , central to how provenance is uti - lized across domains is the task of provenance querying , a task complementary to the task of provenance capture . More speciﬁcally , provenance querying includes several sub - problems involving the construction of provenance querying languages [ 35 ] , provenance browsers [ 52 , 30 ] , and eﬃcient in - dexing schemes for captured provenance to streamline prove - nance queries [ 35 , 51 , 53 ] . In contrast to this line of work , Vamsa is speciﬁcally de - signed to automatically capture provenance in data science scripts written in Python . Captured provenance in Vamsa can be used to answer queries such as “Which datasets were used to train / test a model ? ” , “What libraries have been used in this script ? ” , and “What type of ML model was trained ? ” . As such , the granularity of the provenance is at the level of variables and the operations that utilize them to derive output variables . The operations can be API calls ( read csv ( · ) from Pandas library ) , accessing objects proper - ties ( ∗ . values ) , and user - deﬁned functions including their im - plementation . In other terms , the way data science logic is speciﬁed in Python has little to no similarities with how queries are structured in databases . As such , the task of ML provenance tracking requires provenance capture techniques tailored to the intrinsic semantics of how data science logic is implemented in Python . Nevertheless , however , the iden - tiﬁed provenance information from Vamsa can be stored and used by upstream applications , through provenance query - ing systems , in a similar fashion to how provenance is stored and used out of database workloads . Workﬂow management systems . Workﬂows are widely used in scientiﬁc applications [ 47 , 23 , 28 , 42 , 29 , 14 ] where scientists glue various tasks together and each task may take input data from previous tasks [ 25 ] . Workﬂow man - agement systems aid in collecting , managing , and analyzing the provenance information to enable sharing of experiments and ensure their reproduciblity [ 25 ] . Closer to our work are Starﬂow [ 16 , 15 ] , noWorkﬂow [ 43 , 48 ] , and YesWorkﬂow [ 38 ] systems . StarFlow [ 16 , 15 ] statically analyzes a Python pro - gram to build provenance traces at the level of functions for a Python script . However , it does not extract the dependen - cies inside of the functions . noWorkﬂow also transparently captures the control ﬂow information and library dependen - cies in the scripts . It extracts the provenance in three levels of deﬁnition , deployment , and execution . In order to mon - itor the evolution of scripts , it also uses abstract syntax trees ( AST ) to discover user - deﬁned functions and their ar - guments [ 43 , 48 ] . While Starﬂow [ 16 ] and YesWorkﬂow [ 38 ] require modiﬁcations to the users’ script , noWorkﬂow han - dles unmodiﬁed programs . Our work diﬀers from these works as follows : ( 1 ) By fo - cusing on data science scripts , we are able to capture more relevant provenance information to our needs such as ﬁnd - ing trained models , hyperparameters , datasets , and features ; ( 2 ) noWorkﬂow systems also generate a dependency graph among the variables . However , the workﬂow is not statically generated and the program needs to be executed . This is not always possible due to external dependencies on both the datasets and the various imported libraries . Indeed , AST was not utilized to generate a workﬂow graph but to detect the user - deﬁned functions and their arguments ; ( 3 ) As op - posed to YesWorkﬂow and Starﬂow , Vamsa does not require users to modify their code e . g . , adding tags and decorators to the function and variable deﬁnitions . To this end , it uses the KB ( Section 5 . 1 ) to automatically extract the relevant provenance information from the script . 9 . CONCLUSIONS AND FUTURE WORK ML has become a ubiquitous and integral technology across the stacks of enterprise - grade applications . Unfor - tunately , the management of machine learning logic is still in its infancy . In this direction , in this paper , we introduced the problem of ML provenance tracking with the goal to ob - tain connections between data sources and features of ML models—a fundamental type of provenance information that enables multiple upstream management applications includ - ing , but not limited to , compliance ; security ; model mainte - nance ; and model debugging . Our proposed system , Vamsa , and our experimental evaluation show that it is indeed pos - sible to recover this type of provenance with a very high precision and recall across large corpora of ML scripts au - thored imperatively in Python—even in the hard setting of static analysis time without availability of runtime informa - tion . Finally , we believe that the techniques and compo - nents of Vamsa ( e . g . , the KB and the framework for forward and backward traversals ) are broadly applicable beyond the design of Vamsa and the scope of our work . There are many areas for future work to explore both in the space of ML provenance tracking and the general space of automated management of ML pipelines . First , incorpo - rating runtime information , if available , can better help us identify connections between data sources and ML models ( e . g . , access to data sources can let us provide the exact set of excluded features ) . Second , and in - line with the ﬁrst line of future work , identifying ﬁner - grained provenance infor - mation between data sources and ML models ( e . g . , which partitions of a data source were used for training a model ) can better assist upstream applications ( e . g . , model debug - ging and compliance ) . Such type of information can be ob - tained either statically ( e . g . , identifying ﬁlters in Python scripts ) or dynamically ( e . g . , tracing the inputs to ML mod - els by inspecting program stacks and data ﬂows at runtime ) . Finally , automatically populating or decreasing the manual eﬀort for the population of the knowledge base ( e . g . , func - tions that have not changed across library releases can share the same annotations ) are technically challenging problems , yet provide an integral technology for the management of imperatively speciﬁed ML pipelines . 10 . REFERENCES [ 1 ] Graphlab . https : / / turi . com / l , 2013 . [ 2 ] Xgboost . https : / / xgboost . readthedocs . io / en / latest / index . html , 2014 . [ 3 ] lightgbm . https : / / lightgbm . readthedocs . io / en / latest / , 2017 . 12 [ 4 ] EU GDPR Regulations . https : / / ec . europa . eu / commission / priorities / justice - and - fundamental - rights / data - protection / 2018 - reform - eu - data - protection - rules / eu - data - protection - rules _ en , 2018 . [ 5 ] HIPAA Privacy Rule . https : / / www . hhs . gov / hipaa / for - professionals / privacy / index . html , 2018 . [ 6 ] Kaggle Heart Disease . https : / / www . kaggle . com / ronitf / heart - disease - uci , 2018 . [ 7 ] Kaggle survey . https : / / www . kaggle . com / kaggle / kaggle - survey - 2018 , 2018 . [ 8 ] Oﬃcial Kaggle API . https : / / github . com / Kaggle / kaggle - api , 2018 . [ 9 ] Abstract syntax trees . https : / / docs . python . org / 3 / library / ast . html , 2019 . [ 10 ] Python AST docs . https : / / greentreesnakes . readthedocs . io / en / latest / , 2019 . [ 11 ] Python language . https : / / towardsdatascience . com / programming - languages - for - data - scientists - afde2eaf5cc5 , 2019 . [ 12 ] PyTorch . https : / / pytorch . org / , 2019 . [ 13 ] A . Agrawal , R . Chatterjee , C . Curino , A . Floratou , N . Gowdal , M . Interlandi , A . Jindal , K . Karanasos , S . Krishnan , B . Kroth , J . Leeka , K . Park , H . Patel , O . Poppe , F . Psallidas , R . Ramakrishnan , A . Roy , K . Saur , R . Sen , M . Weimer , T . Wright , and Y . Zhu . Cloudy with high chance of DBMS : A 10 - year prediction for Enterprise - Grade ML , 2019 . [ 14 ] I . Altintas , O . Barney , and E . Jaeger - Frank . Provenance collection support in the kepler scientiﬁc workﬂow system . In IPAW , pages 118 – 132 , 2006 . [ 15 ] E . Angelino , U . Braun , D . A . Holland , and D . W . Margo . Provenance integration requires reconciliation . In TaPP , 2011 . [ 16 ] E . Angelino , D . Yamins , and M . Seltzer . Starﬂow : A script - centric data analysis environment . In IPAW , 2010 . [ 17 ] R . Bose and J . Frew . Lineage retrieval for scientiﬁc data processing : a survey . CSUR , pages 1 – 28 , 2005 . [ 18 ] P . Buneman , S . Khanna , and T . Wang - Chiew . Why and where : A characterization of data provenance . In ICDT , 2001 . [ 19 ] A . Chen , Y . Wu , A . Haeberlen , B . T . Loo , and W . Zhou . Data provenance at internet scale : architecture , experiences , and the road ahead . In CIDR , 2017 . [ 20 ] J . Cheney , L . Chiticariu , W . - C . Tan , et al . Provenance in databases : Why , how , and where . TRDB , pages 379 – 474 , 2009 . [ 21 ] L . Chiticariu , W . C . Tan , and G . Vijayvargiya . Dbnotes : A post - it system for relational databases based on provenance . In SIGMOD , pages 942 – 944 , 2005 . [ 22 ] Y . Cui , J . Widom , and J . L . Wiener . Tracing the lineage of view data in a warehousing environment . TODS , 25 ( 2 ) : 179 – 227 , 2000 . [ 23 ] S . B . Davidson and J . Freire . Provenance and scientiﬁc workﬂows : challenges and opportunities . In SIGMOD , 2008 . [ 24 ] D . Deutch , N . Frost , and A . Gilad . Provenance for natural language queries . PVLDB , 10 ( 5 ) : 577 – 588 , 2017 . [ 25 ] J . Freire and M . Anand . Provenance in scientiﬁc workﬂow systems . IEEE Data Engineering Bulletin , 2007 . [ 26 ] R . Garcia , V . Sreekanti , N . Yadwadkar , D . Crankshaw , J . E . Gonzalez , and J . M . Hellerstein . Context : The missing piece in the machine learning lifecycle . In KDD CMI Workshop , 2018 . [ 27 ] T . J . Green , G . Karvounarakis , and V . Tannen . Provenance semirings . In SIGMOD - SIGACT - SIGART , pages 31 – 40 , 2007 . [ 28 ] T . Guedes , V . Silva , M . Mattoso , M . V . Bedo , and D . de Oliveira . A practical roadmap for provenance capture and data analysis in spark - based scientiﬁc workﬂows . In WORKS , 2018 . [ 29 ] T . Heinis and G . Alonso . Eﬃcient lineage tracking for scientiﬁc workﬂows . In SIGMOD , 2008 . [ 30 ] M . Herschel and M . Hlawatsch . Provenance : On and behind the screens . In Proceedings of the 2016 International Conference on Management of Data , SIGMOD ’16 , 2016 . [ 31 ] R . Ikeda and J . Widom . Data lineage : A survey . Technical report , Stanford InfoLab , 2009 . [ 32 ] R . Ikeda and J . Widom . Panda : A system for provenance and data . IEEE Data Eng . Bull . , 2010 . [ 33 ] M . Interlandi , K . Shah , S . D . Tetali , M . A . Gulzar , S . Yoo , M . Kim , T . Millstein , and T . Condie . Titian : Data provenance support in spark . PVLDB , 9 ( 3 ) : 216 – 227 , 2015 . [ 34 ] Z . Ives , Y . Zhang , S . Han , and N . Zheng . Dataset relationship management . In CIDR , 2019 . [ 35 ] G . Karvounarakis , Z . G . Ives , and V . Tannen . Querying data provenance . In SIGMOD , 2010 . [ 36 ] D . Logothetis , S . De , and K . Yocum . Scalable lineage capture for debugging disc analytics . In SoCC , pages 17 : 1 – 17 : 15 , 2013 . [ 37 ] W . McKinney . pandas : a foundational Python library for data analysis and statistics . PyHPC , 2011 . [ 38 ] T . McPhillips , T . Song , T . Kolisnik , S . Aulenbach , J . Freire , et al . Yesworkﬂow : A user - oriented , language - independent tool for recovering workﬂow information from scripts . IJDC , pages 298 – 313 , 2015 . [ 39 ] X . Meng , J . Bradley , B . Yavuz , E . Sparks , S . Venkataraman , D . Liu , J . Freeman , D . Tsai , M . Amde , S . Owen , et al . Mllib : Machine learning in apache spark . JMLR , pages 1235 – 1241 , 2016 . [ 40 ] H . Miao and A . Deshpande . Provdb : Provenance - enabled lifecycle management of collaborative data analysis workﬂows . IEEE Data Eng . Bull . , pages 26 – 38 , 2018 . [ 41 ] H . Miao , A . Li , L . S . Davis , and A . Deshpande . Towards uniﬁed data and lifecycle management for deep learning . In ICDE , 2017 . [ 42 ] P . Missier , N . W . Paton , and K . Belhajjame . Fine - grained and eﬃcient lineage querying of collection - based workﬂow provenance . In EDBT , 2010 . [ 43 ] L . Murta , V . Braganholo , F . Chirigati , D . Koop , and 13 J . Freire . noworkﬂow : capturing and analyzing provenance of scripts . In IPAW , pages 71 – 83 , 2014 . [ 44 ] M . H . Namaki , Q . Song , Y . Wu , and S . Yang . Answering why - questions by exemplars in attributed graphs . In SIGMOD , pages 1481 – 1498 , 2019 . [ 45 ] F . Nelli . Python data analytics : with pandas , numpy , and matplotlib . 2018 . [ 46 ] F . Pedregosa , G . Varoquaux , A . Gramfort , V . Michel , B . Thirion , O . Grisel , M . Blondel , P . Prettenhofer , R . Weiss , V . Dubourg , et al . Scikit - learn : Machine learning in python . JMLR , pages 2825 – 2830 , 2011 . [ 47 ] J . F . Pimentel , J . Freire , L . Murta , and V . Braganholo . A survey on collecting , managing , and analyzing provenance from scripts . CSUR , page 47 , 2019 . [ 48 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire . noworkﬂow : a tool for collecting , analyzing , and managing provenance from python scripts . VLDB , 2017 . [ 49 ] L . Prokhorenkova , G . Gusev , A . Vorobev , A . V . Dorogush , and A . Gulin . Catboost : unbiased boosting with categorical features . In NIPS , 2018 . [ 50 ] F . Psallidas and E . Wu . Provenance for interactive visualizations . 2018 . [ 51 ] F . Psallidas and E . Wu . Smoke : Fine - grained lineage at interactive speed . VLDB , pages 719 – 732 , 2018 . [ 52 ] E . D . Ragan , A . Endert , J . Sanyal , and J . Chen . Characterizing provenance in visualization and data analysis : an organizational framework of provenance types and purposes . IEEE Transactions on Visualization and Computer Graphics , 22 ( 1 ) : 31 – 40 , 2016 . [ 53 ] P . Ruan , G . Chen , T . T . A . Dinh , Q . Lin , B . C . Ooi , and M . Zhang . Fine - grained , secure and eﬃcient data provenance on blockchain systems . Proc . VLDB Endow . , 12 ( 9 ) : 975 – 988 , May 2019 . [ 54 ] A . Rule , A . Tabard , and J . D . Hollan . Exploration and explanation in computational notebooks . In CHI , page 32 , 2018 . [ 55 ] S . Schelter , F . Biessmann , T . Januschowski , D . Salinas , S . Seufert , G . Szarvas , M . Vartak , S . Madden , H . Miao , A . Deshpande , et al . On challenges in machine learning model management . IEEE Data Eng . Bull . , pages 5 – 15 , 2018 . [ 56 ] S . Schelter , J . - H . B¨ose , J . Kirschnick , T . Klein , and S . Seufert . Automatically tracking metadata and provenance of machine learning experiments . In Machine Learning Systems workshop at NIPS , 2017 . [ 57 ] P . M . Schwartz and D . J . Solove . The pii problem : Privacy and a new concept of personally identiﬁable information . NYUL , page 1814 , 2011 . [ 58 ] S . Seabold and J . Perktold . Statsmodels : Econometric and statistical modeling with python . In Scipy , 2010 . [ 59 ] L . Shao , Y . Zhu , A . Eswaran , K . Lieber , J . Mahajan , M . Thigpen , S . Darbha , S . Liu , S . Krishnan , S . Srinivasan , C . Curino , and K . Karanasos . Griﬀon : Reasoning about Job Anomalies with Unlabeled Data in Cloud - based Platforms , 2019 . [ 60 ] Y . L . Simmhan , B . Plale , and D . Gannon . A survey of data provenance in e - science . Sigmod Record , pages 31 – 36 , 2005 . [ 61 ] L . Torczon and K . Cooper . Engineering A Compiler . Morgan Kaufmann Publishers Inc . , San Francisco , CA , USA , 2nd edition , 2011 . [ 62 ] J . W . Tukey . We need both exploratory and conﬁrmatory . The American Statistician , pages 23 – 25 , 1980 . [ 63 ] M . Vartak , J . M . F da Trindade , S . Madden , and M . Zaharia . Mistique : A system to store and query model intermediates for model diagnosis . In SIGMOD , 2018 . [ 64 ] M . Vartak , H . Subramanyam , W . - E . Lee , S . Viswanathan , S . Husnoo , S . Madden , and M . Zaharia . Modeldb : a system for machine learning model management . In HILDA , 2016 . [ 65 ] E . Wu and S . Madden . Scorpion : Explaining away outliers in aggregate queries . PVLDB , 6 ( 8 ) : 553 – 564 , 2013 . 14