Beyond blind faith : overcoming the obstacles to interdisciplinary evaluation The Harvard community has made this article openly available . Please share how this access benefits you . Your story matters Citation Lamont , Michèle , Grégoire Mallard , and Joshua Guetzkow . 2006 . Beyond blind faith : overcoming the obstacles to interdisciplinary evaluation . Research Evaluation 15 ( 1 ) : 43 - 55 . Published Version http : / / dx . doi . org / 10 . 3152 / 147154406781776002 Citable link http : / / nrs . harvard . edu / urn - 3 : HUL . InstRepos : 2643648 Terms of Use This article was downloaded from Harvard University’s DASH repository , and is made available under the terms and conditions applicable to Other Posted Material , as set forth at http : / / nrs . harvard . edu / urn - 3 : HUL . InstRepos : dash . current . terms - of - use # LAA Research Evaluation April 2006 0958 - 2029 / 06 / 010000 - 00 US $ 08 . 00  Beech Tree Publishing 2006 1 Research Evaluation , volume 15 , number 1 , April 2006 , pages 000 – 000 , Beech Tree Publishing , 10 Watford Close , Guildford , Surrey GU1 2EP , England Identifying the deserving Beyond blind faith : overcoming the obstacles to interdisciplinary evaluation Michèle Lamont , Grégoire Mallard and Joshua Guetzkow Knowledge about how reviewers serving on in - terdisciplinary panels produce evaluations that are perceived as fair is especially lacking . This paper draws on 81 interviews with panelists serv - ing on five multidisciplinary fellowship competi - tions . We identify how peer reviewers define “good” interdisciplinary research proposals , and how they understand the procedures for selecting such proposals . To produce an evaluation they perceive as fair , panelists must respect the pri - macy of disciplinary sovereignty , deference to expertise and methodological pluralism . These rules ensure the preponderance of the voices of experts over non - experts in interdisciplinary panels . In addition , panelists adopt a range of tactics and strategies designed to make other re - viewers who lack such expertise trust that their judgments are disinterested and unbiased . Michèle Lamont is at the Department of Sociology and African and African - American Studies , Harvard University , USA ; Email : mlamont @ wjh . harvard . edu . Grégoire Mallard is at the Department of Sociology , Princeton University , USA , and at LATTS , Ecole Nationale des Ponts et Chaussées ; Email : gmal - lard @ princeton . edu . Joshua Guetzkow is a Robert Wood John - son Scholar in the Health Policy Research Program , Harvard University , USA ; Email : jguetzkow @ rwj . harvard . edu HE EDITOR - IN - CHIEF of Duke University Press — known for publishing interdiscipli - nary works in literary studies and other fields — contends that interdisciplinary work is harder to evaluate than to produce . While scholars doing interdisciplinary research may routinely use work from other disciplines , they fail to check their disci - plinary ‘hats’ at the door when assessing research that appropriates theories , methods , and topics produced by their own discipline . Too often , when serving as evaluators , such scholars view interdisci - plinary work as “careless” or “misguided” because it uses “tools from the discipline without understand - ing their attendant histories , contexts and shortcom - ings” ( Wissoker , 2000 : B4 ) . Moreover , when they assess work from a disci - pline other than their own , they may lack the disci - plinary expertise needed to evaluate research . Because it is “difficult to identify peers whose ex - pertise fully encompasses” all the knowledge needed to evaluate interdisciplinary scholarship , the very notion of “peer” review is called into question in the case of interdisciplinarity ( Porter and Rossini , 1985 : 33 ; see also Laudel , 2004 ) . Despite such hurdles , review panels are regularly called upon to judge the value of interdisciplinary work . If evaluating inter - disciplinary work is inherently problematic , how is it accomplished ? We address this question by drawing on 81 inter - views conducted with panelists serving on 12 fund - ing panels that are part of five American multidisciplinary fellowship competitions in the humanities and the social sciences . Our analysis breaks with the standard approach to the study of peer review , which is overwhelmingly concerned with the roadblocks to distributional “fairness T Overcoming obstacles to interdisciplinary evaluation 2 Research Evaluation April 2006 caused by nonscientific influences such as politics , friendship networks , or common institutional posi - tions” ( Travis and Collins , 1991 : 324 ) . This ap - proach posits normative rules — universalism and disinterestedness — that characterize fair evaluation ( Merton , 1973 [ 1942 ] ) , and it tries to assess whether the outcome of the evaluation is consistent with this definition . Studies typically seek to determine whether judgments about substantive quality are in some way biased by the consideration of personal qualities or personal ties by measuring who gets funded or published ( Cole et al , 1978 ; GAO , 1994 ; Zuckerman and Merton , 1971 ) . Our research breaks with this standard approach in at least two ways . First , instead of positing the existence of normative rules , we identify inductively the criteria that panelists use to judge the substantive quality of interdisciplinary proposals – for instance , they value proposals that balance breadth and origi - nality with depth and empirical rigor . Second , we identify inductively the rules that they say they fol - low to produce fair evaluations , such as deferring to expertise and using “disciplinary contextualization” ( whereby proposals are to be judged first and fore - most according to the standards of the discipline from which they originate 1 ) . By examining the social sciences and humanities , where the diversity of cri - teria of evaluation is allegedly greater than in the natural sciences ( Kuhn , 1970 ) , we also provide a corrective to the literature on interdisciplinarity , which , as far as it has focused on the natural sci - ences , has tended to downplay the influence of dis - ciplinary variation in criteria of evaluation on the intersubjective production of the belief in fairness among panelists . The dimensions of interdisciplinary evaluation we focus on , the criteria and the process of evaluation , raise two analytically distinct problems that we ana - lyze in this paper . The first kind of problem faced by panelists in interdisciplinary panels consists in find - ing the relevant criteria used to define good inter - disciplinary proposals . Second , panelists meet a different kind of problem posed by the difficulty of collectively identifying the proposals that fit with their criteria in a manner they will perceive as “fair” . 2 Our two empirical sections explore succes - sively how panelists in interdisciplinary panels over - come these two problems to produce the belief , expressed without exception by the panelists we in - terviewed , that their panels were able to identify the most deserving proposals — the belief , in short , that the outcome was fair . In so doing , our paper provides an analytical de - scription of aspects of interdisciplinary evaluation that are not well - understood . As such , our contribu - tion is less to enrich our understanding of causal pro - cesses already identified , than to shed light on aspects of the process of evaluation that have been neglected to date . Our research focuses exclusively on inter - disciplinary panels and lacks direct evidence on disciplinary peer evaluation . 3 Our research does not directly test the relevance of Merton’s ( 1973 [ 1942 ] ) conventional ideas on “fairness” in disciplinary and interdisciplinary peer evaluation . Doing so would require direct evidence of the processes we describe here in both kinds of panels . Further systematic com - parative studies of peer evaluation are needed to give a clear explanation of the differences between intra - disciplinary and interdisciplinary evaluation . In the following section , we present the data and methods on which our analyses are based . Then , we discuss the criteria that panelists use to distinguish ‘good’ interdisciplinary proposals from ‘bad’ ones . Next , we describe how panelists perceive the rules they follow in producing a collective evaluation that they perceive as fair , focusing on the rules of disci - plinary sovereignty , deference to expertise and methodological pluralism . Our discussion of these rules directly builds on Mallard et al ( under review ) and Lamont ( forthcoming ) . Finally , we examine how panelists resolve the threats to fairness posed by disciplinary distance and familiarity . Data and methods We examine academic evaluation in the specific context of multidisciplinary panels that distribute fellowships to graduate students and faculty mem - bers in the social sciences and the humanities . This setting presents some distinctive characteristics as compared with the peer review of journal submis - sions or the departmental evaluation of faculty members for promotion . Most importantly , it brings together scholars who come from different discipli - nary horizons , and who therefore often have to make explicit their theoretical and methodological prefer - ences , given the low level of disciplinary knowledge they can count on sharing with other evaluators . Most of the proposals under consideration by these panels are interdisciplinary as explained below . Ap - preciation for interdisciplinary scholarship is also one of the criteria used for selecting panelists , ac - cording to the program officers we interviewed . 4 Panelists were all actively engaged in research . We conducted interviews with panelists serving on 12 funding panels attached to five different mul - tidisciplinary fellowship competitions in the social We examine academic evaluation in the specific context of multidisciplinary panels that distribute fellowships to graduate students and faculty members in the social sciences and the humanities Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 3 sciences and the humanities . We studied each panel in two successive years . The funding competitions were held by the Social Science Research Council ( SSRC ) , the American Council of Learned Societies ( ACLS ) , the Woodrow Wilson National Fellowship Foundation ( WWNFF ) , a Society of Fellows at a top research university , and an anonymous foundation in the social sciences . 5 These competitions were chosen because they cover a wide range of disciplines , and because they are all highly prestigious . While the SSRC and the WWNFF competitions are open to the social sciences and the humanities , the ACLS sup - ports research in the humanities and in humanities - related social sciences . The Society of Fellows sup - ports work across a range of fields , whereas the anonymous foundation only supports work in the so - cial sciences . The SSRC and the WWNFF programs provide support for graduate students , whereas the ACLS holds distinct competitions for assistant , as - sociate , and full professors . The Society of Fellows provides fellowships to recent PhDs , and the anonymous social science foundation supports re - search at all ranks . A total of 81 open - ended , semi - structured inter - views with panel members in charge of final delib - erations were conducted . 6 This total includes 66 interviews with 49 different panel members ( 17 pan - elists were interviewed twice , because they served on panels for the two years that the study lasted ) . In - terviews lasted approximately 90 minutes and were conducted as soon as possible after the conclusion of panel deliberations , typically within a few days or at most within a few weeks . Interviews were conducted over the phone or , where possible , in person . Fifteen additional interviews were conducted with relevant program officers and panel chairpersons for each panel , who provided details about what had hap - pened during the panel deliberation in the absence of direct observation . 7 Interviewees originated from a wide range of disciplines , reflecting the highly inter - disciplinary nature of the competitions under study ( see Table 1 ) . Drawing on interview data was the best research strategy available given that issues of confidentiality create enormous hurdles to accessing peer review panels , a circumstance which has resulted in a pau - city of research on peer review . 8 Although ideally our research would also draw on more direct obser - vation and on a larger number of panels , our empiri - cal data is optimal given the distinctive features of our topic . Saturation of information was reached with 71 interviews . Thus our project meets usual standards regarding sample size in qualitative re - search ( Ragin et al , 2004 ) . A first battery of questions concerned how panel - ists evaluated and ranked proposals prior to and dur - ing the meetings ( ie , both in isolation and in interaction ) . Respondents were asked to describe what they appreciated in the proposals they judged to be the best and the worst prior to the delibera - tions . 9 They were also asked to describe the process by which proposals that had a high ranking prior to deliberation ended up not being funded , and how some low - ranked proposals were funded . 10 The gen - eral strategy consisted of asking panelists to specify their own criteria of evaluation by producing “boundary work , ” that is , by contrasting their evaluative standards with those of others ( Lamont and Molnár , 2002 ) : panelists were asked to describe how they perceived themselves to be similar to or different from other panelists . Interviews also concerned how decisions were reached , what factors facilitated the production of consensus and fostered fairness , and what factors led to the occasional breakdown of deliberations . Asking panelists to describe exchanges surrounding Table 1 . Disciplinary composition among interviewees per interdisciplinary fellowship competition Total interviewees per competition Disciplines First competition N = Second competition N = Third competition N = Fourth competition N = Fifth competition N = Total * N = English ………… . . 2 3 2 1 0 8 Art History . . …… . . 0 1 0 1 0 2 Musicology …… . . . 0 2 0 0 0 2 Philosophy ……… 0 1 1 0 0 2 History ………… . . 1 4 2 6 1 14 Geography ……… 0 0 0 1 0 1 Anthropology …… 1 1 0 2 1 5 Sociology ……… . . 0 1 0 3 2 6 Political Science . . . 0 0 0 3 3 6 Economics ……… 0 0 0 1 1 2 Biology ………… . 0 0 1 0 0 1 Total N = 4 13 6 18 8 49 Note : * We only include here panelists whom we interviewed , and we exclude panel chairs and program officers . Overcoming obstacles to interdisciplinary evaluation 4 Research Evaluation April 2006 especially controversial proposals was particularly fruitful in revealing the diversity of arguments used by panelists , what styles they privileged and consid - ered most appealing to others , and how they be - lieved preferences for specific styles should be expressed . 11 We also asked panel chairs and program officers to comment on debates surrounding contro - versial proposals in order to learn their understand - ing of how fair outcomes are produced . Defining ‘good’ and ‘bad’ proposals The competitions we studied use appreciation for in - terdisciplinary scholarship as a criterion for selecting panelists . However , evaluators are left to decide what defines a ‘good’ or a ‘bad’ interdisciplinary proposal . According to the panelists we interviewed , the main challenge of interdisciplinary evaluation consists in assessing whether a proposal reaches the proper balance between breadth and originality on the one hand , and depth and empirical rigor on the other . What panelists appreciate most in good interdisci - plinary proposals is the intellectual breadth they offer , which is intrinsically valued , and contrasted to a ‘narrow’ concern with advancing disciplinary knowledge . For instance , for a specialist of [ In - dian ] 12 literature , good scholarship : pivots on interdisciplinarity , the ability to speak to different sets of people , albeit perhaps in fairly simple terms , where to other people aca - demic excellence is very much rooted within developing a specific discipline or even a spe - cific kind of field , a quite narrow field of knowledge . Similarly , a geographer puts a premium on interdis - ciplinary breadth and originality , which she values over disciplinary “over - specialization” and a rigorous concern with “nice little data points” : To be an artful and talented geographer … you have to be able to talk the talk of several disci - plines , [ and ] to be able to see where cutting edges are and take certain gambles in terms of advancing an idea . And so what it does as a discipline is to address the over - specialization that sometimes occurs … where you can study a quarter acre of a rain forest to death … I take a risk on what I think are interesting ideas , even if they’re bumping into different fields … [ and ] can’t always just get the numbers and the nice little data points … So it just then comes down to a call of , OK , how compelling do you think the topic is . Other scholars explicitly try to balance interdiscipli - nary breath with disciplinary standards . This stance is best represented by a China specialist who argues for combining different criteria of evaluation : I wouldn’t want us to get enthusiastic about anything that didn’t have the endorsement of specialists who felt that , just as a first question , this [ is ] going to satisfy needs in your particular discipline . I mean only after we got a yes to that [ question ] would I want to go on and say , OK , now we apply the second tier of criteria , which is , “Is it going to do anything for any - body else ? ” As these last quotes show , while panelists admire proposals that have implications for a broad range of disciplines and combined disciplinary elements in novel ways , they often fear that interdisciplinary proposals do not deploy these elements carefully enough to deepen existing knowledge . This tension is expressed by an English professor who contrasts the traditional criterion used to assess literary inter - pretation as bearing on whether a scholar could per - form a “subtle , accurate reading” ( of a sonnet , for instance ) , with the criterion used to assess interdis - ciplinary work , which is whether the argument is “plausible , persuasive , how evidence is used” . 13 Thus , panelists worry that interdisciplinary scholar - ship may be ‘looser’ than disciplinary scholarship , and that the standards by which it is evaluated are less exacting than those to be met by disciplinary scholarship . Bad interdisciplinary proposals are described as ‘fast and loose’ and not sufficiently grounded theo - retically . Thus , an English professor describes a ‘good’ proposal in the following terms : [ It was ] very ambitious intellectually and I think will contribute significantly to our under - standing to what it means to be a [ citizen ] … She wasn’t just staying focused on what I think would have seemed easiest to her . So it’s inter - disciplinary , but it’s not fast and loose as an in - terdisciplinary project . [ She wasn’t just ] throwing around some interdisciplinary vocabu - lary … pulling in Homi Bhabha and Benedict Anderson and throwing them into some kind theoretical soup , not really applying their theo - ries in any kind of deep way . What panelists appreciate most in good interdisciplinary proposals is the intellectual breadth they offer , which is intrinsically valued , and contrasted to a ‘narrow’ concern with advancing disciplinary knowledge Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 5 Others describe bad interdisciplinary proposals as “lazy” or “flashy” , and as not appropriating the theo - ries , tools and methods of other disciplines with sufficient rigor . Interdisciplinarity is perceived as presenting real dangers , in terms of using knowledge “outside your discipline , more as a rhetorical strat - egy than as something in which you really steep yourself” . Interdisciplinary proposals also face particular challenges because they are perceived as more ambi - tious , and therefore more challenging , than purely disciplinary proposals , and because they have to meet standards emanating from more than one disci - pline . Indeed , one scholar attributes the failure of some interdisciplinary proposals not to the short - comings of applicants but to their “ambition to reach beyond the person’s initial field ; they’re risky . The person might not be able to do what they want to do . Over - ambitiousness [ is ] what [ is ] both attractive and fatal about some of the projects . ” The challenge of serving more than one disciplinary master is under - scored by an art historian who deplores that : We encourage students to [ apply methods and theories across disciplines ] , but in the end they can become victims because they are seen as spreading themselves so thin that they can’t really carry out the research with enough steps and perception … For instance , you’re encour - aged to do now art history and anthropology . So if you’re an art historian who does that , very often more traditional art historians will say , well that person is a closet anthropologist . But anthropologists don’t often take you seriously because your framework is often from fine arts and from art history . So it is a problem and I think the only way to get at it is to really do good research that manages to integrate disci - plines in a compelling way . Thus , good interdisciplinary work combines differ - ent tool kits in a disciplined way . This is stated by a historian : I’m interested in , not in the kind of interdisci - plinarity that Stanley Fish once complained about , which is basically the person who makes up his own standards and therefore is bound by no one , but in consciously trying to sort of bro - ker useful relations between disciplinary tool kits … [ T ] here’s a lot of contexts where I would like to also look at other disciplinary tool kits and , when it makes sense , use them in a way that’s disciplined . In other words , don’t sort of lazily grab from whatever looks useful , but to find a way to sort of think within the paradigms that another discipline is offering . In the end , what differentiates a good and a bad in - terdisciplinary proposal is the applicant’s capacity to achieve her stated purpose while mastering research tools borrowed from one or more disciplines . This criterion , by which one can recognize good interdis - ciplinary proposals , emerges clearly from our inter - views . Less clear is how panelists understand the process by which they come to identify the proposals that fit with their definition of a good proposal , and which corresponds to their understanding of fairness in evaluation . In the next section , we analyze their understanding of such a process . Overcoming the paradoxes Panelists face additional challenges when engaging in the process of interdisciplinary evaluation . The panel - ists emanating from the same discipline as the appli - cant can lack the disciplinary distance needed to judge the wider interest of the proposal and its broader relevance . They may be too committed to disciplinary standards to spot proposals that open new vistas . On the contrary , those emanating from disciplines other than that of the applicant generally lack the special - ized knowledge needed to evaluate the proposal . The type and amount of evidence under review is generally foreign to them . Thus , they have to rely ( in blind faith ) on the expertise of their more qualified peers . They do not know if the panelists most quali - fied to judge a proposal are driven by their discipli - nary interests and biases when they judge the proposals originating from their discipline . 14 Hence , in interdisciplinary evaluation , panelists face a dilemma when defining how much weight they should give to disciplinary as opposed to inter - disciplinary concerns ; and how they should justify their evaluations , one vis - à - vis the other . We show in this section how panelists adopt different tactics to overcome these inherent challenges to interdisci - plinary evaluation . Critical evaluation and blind faith Because interdisciplinary proposals are broad by definition , it is often very challenging for panelists to determine the substantive merits of such propos - als . As pointed out by an English scholar , panelists are rarely as qualified as the applicant to make such a determination : You take your theoretical frame from some existing source , such as … Foucault , so that it would be possible to judge whether someone was pursuing a Foucaultian reading in a proper way . [ However ] , I felt that I certainly knew more about these [ government ] accounts [ I was studying ] than anybody else did , and that meant that nobody really ever could say that I was wrong , and that made me worried … They would inevitably be impressed by the fancy footwork that I performed with these sources , but … it would be difficult for someone to say that I had read them wrong . Overcoming obstacles to interdisciplinary evaluation 6 Research Evaluation April 2006 To overcome the challenge of evaluating such a pro - posal , panelists adopt customary rules that serve to limit the amount of arbitrariness that enters into the decision making process . One such rule is what we call deferring to expertise : although formally , panel - ists have equal weight in decisions , each of them claims expertise on a specific subset of topics cov - ered by the proposals . 15 The amount of authority that is accorded to them varies with their disciplinary af - filiation and their own research areas , as well as whether or not they stake a claim on a topic – in line with Weber’s ( 1978 [ 1956 ] : 99 – 100 ) analysis of the role of expertise in providing legal - rational legiti - macy . 16 A historian explained how this is accom - plished , in the context of the discussion of a proposal on modernity and [ the media in the United States ] . He mentioned to the group that he had done work on the period covered by the proposal , and that for this reason he was particularly well - positioned to assess whether it was proposing anything new . He concluded , “I think I had expert advice here and I think that’s why she got [ the funding ] , because I had expert advice . ” Because panelists are generally recognized as greater authority on proposals emanating from ‘their’ field , another , related , customary rule consists in re - specting disciplinary sovereignty . Another historian spells out this culture of disciplinary deference : For a couple of proposals , I had to ‘explicate’ a research strategy that wasn’t fully explained . Historians would say “It’s an obvious ap - proach , no problem ! ” but didn’t realize how other disciplines would not take for granted what the writer was taking for granted … Sometimes I almost felt uncomfortable about the deference that was given to my responses as a historian … [ I thought ] I’m the only one here and I hope I’m doing this right . There was a lot of credibility given to the way I responded to historical questions in particular . And I , of course , returned the deference to people in the other fields . This panelist underlines a paradox central to inter - disciplinary evaluation . While panelists tend to defer to the expertise of experts , they are generally re - quired to take position on topics that they know very little about , operating almost on blind faith . This is spelled out explicitly by an anthropologist who says that : In philosophy , I didn’t feel at all as if I were competent to evaluate those proposals . Either to say this is good or this is not good , I just did not know what was up with them . And in those cases I always deferred to the people who did have some kind of expertise in that field . Similarly , a historian explains why she deferred to a historian on Russia : There were times where I really wanted to con - test what [ she ] had to say about some propos - als , but when she comes in extremely expert and careful and a person I respect a lot , and says “No , there are eleven books on this in Russian and this is really a fairly banal pro - posal” , then I just sort of say that must be true . So you know it looks good until somebody says there’s a whole literature that you cannot reasonably be expected to know , and then you just sort of say OK , no problem . This tension between blind faith and critical colle - gial evaluation is scarcely acknowledged as delib - erations occur . Panelists construe doubts about their expert judgment as a breach of the customary norm of deferring to expertise . Breaching the rules of def - erence to expertise and respect of disciplinary sover - eignty is the most frequent source of conflict among panelists ( see Mallard et al , under review ) . A third customary rule is what we have called elsewhere “disciplinary contextualization” , which requires that panelists assess proposals using the standards that prevail in the field of scholarship of the applicant ( Mallard et al , under review ) . This rule is associated with another rule , that of methodologi - cal pluralism . Disciplinary contextualization is cap - tured by an evaluator as he described the dynamics of his panel : I noted … the differences between people who work with large datasets and do quantitative re - search . And then the very polar opposite , I sup - pose , folks who are doing community level studies in anthropology . They are such different methodologies that it’s hard to say that there’s a generalizable standard that applies to both of them . We were all , I think , willing and able to understand the projects in their own terms , for - tunately , and not try to impose a more general standard , because it would have been extremely difficult … I wouldn’t hold a candidate in po - litical science responsible for what seemed to me to be having overly instrumental or dia - grammatic ways of understanding what they’re going to do because they have to have those . Tension between blind faith and critical collegial evaluation is scarcely acknowledged as deliberations occur . Panelists construe doubts about their expert judgment as a breach of the customary norm of deferring to expertise Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 7 They have to have a certain clarity , they have to have a certain scientism , so I am not going to sit there and say “Well , where’s the fluidity , where’s the self - reflexivity , where’s the her - meneutical content ? ” Thus , panels are not a forum for critically challeng - ing other methodological or disciplinary traditions . The rules of the game require that methodological equality be recognized as a matter of principle , and that for the duration of the meeting , panelists bracket methodological boundary work , the process by which members of disciplines try to one - up one an - other to gain credibility and professional jurisdiction ( Gieryn , 1983 ; 1995 ) . The customary rule of meth - odological pluralism is further described by another political scientist who explains how he was accused of showing a lack of methodological pluralism : “I was basically being accused of being a posi - tivist . No one ever said that because obviously that’s like calling somebody a communist . But there was a sense in which I was imposing my disciplinary bias inappropriately on other disci - plines … . And my response was , ‘No I actu - ally am holding [ the applicant ] to her own standards and I’m not trying to be hegemonic on this . ’” Most evaluators think that if methods were to be critically discussed , conflicts between disciplines might render the evaluation process more tense , more polarized , and more unfair . A geographer expresses her frustration at a political scientist who refused to evaluate a proposal concerned with the analysis of “cultural meanings” with what she believed are the most appropriate standards . She says : This [ proposal ] is not about how many people are actually sick in a population , but rather how many are saying they’re sick in a population , which is about discourse . So it’s not going to fit into nice little number crunching . It’s about how people use issues to mobilize protests , and he was not willing to hear that or entertain that , and it made me mad . Similarly , a historian was critical of another historian who lacked disciplinary flexibility : She sort of has this one standard , the hypothesis , you know , “What’s the hypothesis ? ” Some things you’re not necessarily hypothesizing about . She always had this one little test that she seemed to be applying to everything . That just seemed to me to be not the most productive way . Tactics of disinterestedness and the creation of trust As we have just seen , the judgments of disciplinary experts prevail over those of evaluators who lack disciplinary familiarity . Further , the ‘non - expert panelists coming from other disciplines do not have a full understanding of these disciplinary criteria , which would provide them with a sense of how idio - syncratic these ‘expert’ judgments are . Confronted with this situation of blind faith , panelists could see the rules of evaluation ( deference to expertise , re - spect of disciplinary sovereignty and methodological pluralism ) as leading to potentially unfair , arbitrary decisions . It is actually the contrary which happens . Because these non - experts are less able to judge the intrinsic weaknesses of the proposed research as judged in disciplinary terms ( either in terms of methods , or in terms of originality of topic ) , they tend to read them in more favorable terms than the ones coming from their own discipline . Deliberations thus proceed in a way that leads panelists to be more critical of the proposals coming from their own discipline than of the ones coming from other disciplines . Many panel - ists agreed that they judged proposals from other disciplines less harshly , mainly because they did not have the required expertise . And when the proposal’s topic was one familiar to the panelist , panelists could be more opinionated , because they felt they had the relevant knowledge to form a strong opinion . 17 Most believed that panelists end up being most critical of proposals in their own field . An English professor explains : I tended to give high marks to some proposals that I had no education about the field but just sounded exciting . And then some other panelist would be able to say very quickly “This is not original work , you know . ” There’d be no way for me to know that in advance . Similarly , a musicologist acknowledges : “People were particularly critical or cast a particularly sharp eye on work from their own field , in part because they knew the field and could evaluate the claims more effectively than a non - specialist . ” Moreover , panelists often lose points by pushing their own field , as noted by a historian who describes the pan - elist he liked least as someone who is “very inter - ested in pushing her own field and is not as open to other fields . As she said herself , she’s pushing time periods , you know , that she’ll sponsor anything in the Middle Ages , kind of thing . ” Being able to sway colleagues’ opinion and gain support for a proposal depends on the overall amount of credibility that one has accumulated , and respecting all the rules de - scribed above contributes not only to maintaining the conditions for fair decision - making , but also to accumulating credibility with colleagues . In this case , panelists’ greater generosity toward proposals coming from other disciplines produced an increased sense of fairness about the decision process . Indeed , panelists could not be accused of defending their narrow self - interests by pushing their own disciplinary turfs . 18 Another important Overcoming obstacles to interdisciplinary evaluation 8 Research Evaluation April 2006 way in which a lack of disciplinary familiarity is as - sociated with an increased sense of fairness and dis - interestedness is found when panelists disclose indirect or informal ties with candidates . Whereas network connections could be viewed as an asset in so far as they can be used to get reliable knowledge on an applicant’s capacity to achieve her project ( as they are used for instance in the recruitment of stu - dents and professors in academic departments ) , too much familiarity is immediately suspected of hiding self - interest on the part of the panelist who holds a close connection to the candidate . 19 Although there is no explicit requirement to do so , panelists also volunteered information on indirect ties in a number of cases ( “This student’s mentor is a close collabora - tor of mine” or “I know this student’s advisees very well and trust her letter” ) as a way to muzzle them - selves . For instance , when asked to describe his fa - vorite proposal one historian said , That’s a little bit difficult because to be com - pletely honest about it , the one that I liked the most was by a student who quoted me at length . I recused myself . I didn’t enter the dis - cussion on that . She’s not literally my student . She did some independent work with me because she had no one else who could guide her work . 20 Panelists are aware of the importance of limiting the ‘corrupting impact’ of personal relationships that are formed in their own disciplines . Too close discipli - nary familiarity can therefore threaten the universal - istic and disinterested character of evaluations . For instance , a sociologist said that she chose not to of - fer an opinion when the work of a former colleague , which she does not appreciate , was discussed . A panel chair explains how his particular funding pro - gram tries for this reason to maximize the presence of panelists : who are not gatekeepers , and for policy reasons we actually almost always exclude people from the type of institutions that are more gate - keeping institutions . We’re not allowed to have people from the Ivy League , [ and ] you did no - tice there was nobody from Harvard , Yale , Princeton , Chicago . And we definitely do not include people from those institutions [ that produce ] large numbers of area studies types of applications . No Michigan , no Berkeley . Contrasting intellectual merit from social familiarity with the academic social elite , he adds that : It’s almost an unwritten rule that we’re looking for people who have the same intellectual level as [ people in the ] Ivy League , but are not con - nected to the networks . We look for people who will decide applications on the basis of their intellectual merits , and not on who did it , where they’re coming from . One of the panel chairs expresses this rule clearly . Asked about how panelists were to react if a panelist stated : “This is a student of a close colleague of mine and I’d love to see his work funded . ” He responded : It’s just not a consideration . It can’t be a con - sideration . You probably noticed that the panel rejected quite roundly a student of [ the presi - dent of the funding agency ] , who described him as the best student he’d had in 25 years … No - body thought about that … There are other types of biases that other people bring to the meetings , but they tend to be well camouflaged … I hear criticisms from colleagues that say , “Oh man , you’re just funding [ Berkeley ] an - thropology , it’s because you have all these [ Berkeley ] anthropologists on your panel . ” That’s the most primitive kind of interpreta - tion ! They have no idea what’s inside the black box and they make these primitive assumptions about interest . And what I can tell you is that in my experience it looks the opposite . The more specialists you have on the Middle East , the fewer Middle East proposals are going to get through . Because people tend to be really tough on their discipline , to the point where they’re too tough and we have to think of ways to make them mellow to get them to say yes . Panelists use other tactics to gain credibility and a reputation of disinterestedness . If some are obvi - ously harsh on their own discipline , others excuse themselves for being too harsh on others in anticipa - tion of potential criticisms . For instance , a panelist explains how he manages his disciplinary prejudices by being transparent about them : As I was scoring these proposals I started to be suspicious that I was giving lower scores to an - thropology and history proposals than from the other social sciences . In part , because the criteria that I think are important are somewhat disci - pline - specific . But , you know , keeping this in mind , trying not to let that influence me too much , I went through and … did some calculations to find out whether I did have a dis - Although there is no explicit requirement to do so , panelists also volunteered information on indirect ties in a number of cases as a way to muzzle themselves Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 9 ciplinary bias in scoring , and I did , as I expected . It wasn’t extreme , but it was there … And so , when we met I just fessed up . I said , “You know , I think I have a bias in terms of scoring lower for anthropology and history” … This particular panel , for whatever reason , could be just the luck of the draw , seems very open - minded and will - ing to accept the possibility that we each have our particular disciplinary process we use . This tactic , which requires that scholars hold themselves up voluntarily to not be too harsh on dis - ciplines they have not mastered , shows how a lack of disciplinary familiarity can be used to signal a high level of disinterestedness . The lack of disciplinary familiarity helps give panelists the sense that discussions proceed as though panelists were abstracted from social net - works and operating as free agents without any per - sonal agenda , and as if clientelism and particularism could not influence the decision - making process . Far from imperiling the fairness of the evaluation pro - cess , panelists’ lack of familiarity with most of the disciplines represented in interdisciplinary panels of - fers an additional guarantee of fairness of the pro - cess of decision - making . But disciplinary differences in the extent to which panelists will ‘push their disciplines’ are also very obvious when comparing the attitudes of historians with those of philosophers , classicists , or art histori - ans whose disciplines gather awards much less fre - quently , in part because these are small fields that generate far fewer applicants . The propensity to be disinterested varies according to disciplinary re - sources , as expressed by a historian who says : I generally try to give disciplines very far from me the benefit of the doubt . Maybe if there’s something I like but don’t understand , I’ll boost it up and not always successful , but also some - times I am a little bit harder on ones in my own discipline going in … a plus or minus easier or harder because I just don’t want to just be bow - ing to people in American History one hundred percent of the time … But again history does very well , it’s a luxury I can afford . Two philosophers who served on panels were de - scribed by other panelists as very eager to see their own discipline represented on the lists of awardees — philosophy proposals tend to receive few awards , according to a program officer . Conversely , histori - ans take it so much for granted that their field will be represented that their concern for diversity often fo - cuses on which geographical areas and period of his - tory the proposal covers . A panel that makes many history awards may be defined as diverse if it in - cludes proposals that concern non - Western topics . The inverse relationship between generosity toward other disciplines and scarcity may suggest important differences in the degree to which members of different disciplines engage or can afford to engage in overtly disinterested behavior . 21 When tastes matter According to Weber , rational legitimacy comes from the application of impersonal and consistent rules , which can then be regarded as producing “objective” evaluations . According to this view , panelists who believe in the fairness of the deliberation will try ex - plicitly to bracket their idiosyncratic tastes while evaluating proposals , as research on peer evaluation inspired by Merton has largely repeated ( Cole and Cole , 1981 ; Cole , 1992 ; GAO , 1994 ) . In interdisci - plinary panels , however , panelists who are not from the same discipline as an applicant lack the mastery of the disciplinary technical standards to judge the proposal . Therefore , they have to justify their evaluations in reference to other criteria than purely technical and objective standards . In particular , they allow their tastes and idiosyncrasies to play a greater role than would be expected if they adhered to Mer - tonian norms . These tastes often concern who they are and who they think the panelist is — for exam - ple , judging the “moral character” of the panelist , as shown by Guetzkow et al ( 2004 ) . For instance , an English professor argues that even though one needs to establish a distinction and hierarchy between one’s personal preferences and criteria of compe - tence , he still views the first ones as legitimate indi - cations of the academic quality of a proposal : What I found with these things , you need to use two sets of criteria . One is a sort of criteria which is your best professional judgment in as neutral a way as you can manage it , independ - ent of your taste . And the other one is allowing for your tastes , if they don’t get in the way of each other . I think one should always give up the personal one , if the arguments of other peo - ple seem sound , and not give up the other one . This scholar subordinates personal preferences to more neutral standards , while acknowledging that the latter are intrinsic to the evaluation process . Thus , he protects the legitimacy of the process while recognizing the role of idiosyncratic preferences in evaluation . Similarly , a political scientist establishes a clear distinction between evaluating the choice of topic , which is not “objective” , and evaluating the quality of the proposal , which is more unambigu - ously an object of expertise , “amenable to the can - ons of academic excellence” . Judging theoretical approaches and research designs requires expertise , whereas judging the importance of a topic is often more a matter of taste . Panelists tend to like what speaks to their own personal tastes , as one of them acknowledges : “I see scholarly excellence and ex - citement in this one project on [ food ] , possibly because I see resonance with my own life , my own interests , who I am , and other people clearly don’t . ” Overcoming obstacles to interdisciplinary evaluation 10 Research Evaluation April 2006 Multiple examples of how panelists’ idiosyncratic tastes shape their vote can be found in the inter - views , especially when they lack the technical com - petence to judge proposals . A panelist who loves modern dance could confess in an interview ( without hesitation ) : “The one on [ dance ] [ I liked a lot ] ; I’m an avid dance person … in terms of studying dance , the history of dance and vernacular dance in particu - lar . So I found that one really interesting , very good . ” Similarly , an anthropologist supported a pro - posal on [ songbirds ] , which she explains by the fact that she had just come back from Tucson where she had been observing [ songbirds ] . An English scholar supports a proposal on [ sports ] , and ties her interest to the fact that she was an elite tennis player in high school . An historian doing cross - cultural , compara - tive work is very explicit in stating that he favors proposals with a similar emphasis . Another historian doing research on non - Western societies gives extra points to proposals that are looking beyond the West . Yet another panelist ties her opposition to a proposal on [ Viagra ] to the fact that she is a lesbian : I will be very candid here , this is one place where I said , OK , in the way that I live my life and my practices , because of who I fell in love with , I have ended up living my life as a les - bian , and is there a bias here that I have … I’m so sick of hearing about [ Viagra ] … Just this focus on these men , whereas women , you know , birth control is a big problem in our country . So I think that’s what made me cranky . So in this case would you say my per - sonal preferences affected those choices . In contrast , when panelists talk about how they evaluate proposals in their own discipline , they gen - erally uphold the legitimacy of the process by fram - ing their judgment in universalistic terms , regardless of whether that might be said to be idiosyncratic or not . 22 That panelists do not generally frame their prefer - ence in terms of self - interest is illustrative of the codes they use to understand how they “produce the sacred” ( Durkheim , 1965 ) or “identify the cream” ( Lamont , forthcoming ) . They often refer to a supposedly “objective” quality of the proposal 23 and to their personal tastes , as affecting both at the same time their choice of rankings . This reference to sub - jective factors does not disrupt the perceived fairness of the process of evaluation . Liking what resembles oneself leads to self - reproduction when a proposal speaks to your discipline , but panelists who express their personal taste are not from the same discipline as the applicant , their subjectivity is less subject to suspicion of self - reproduction and gate - keeping . This distancing of personal interest may be pre - sent in part because it is difficult or even impossible to think of a system of interdisciplinary evaluation that would entirely bracket personal preferences ; panelists cannot spell out what defines an “interest - ing” proposal in the abstract , irrespective of the kinds of problems that captivate them personally . But in the absence of technical criteria , they behave as if they have no alternative but to use their own personal understanding of what constitutes a fasci - nating problem in order to do the work that is ex - pected of them . Some disciplines and scholars may also be more open to making room for personal idiosyncrasies because their members are more committed to an anti - objectivist epistemological culture according to which personal identity influences all aspects of scholarly work , as is the case for feminist standpoint theory . 24 An anthropologist , who is explicitly aware of the need to avoid idiosyncratic judgment , sums up : Excellence is in some ways is what looks most like you . It’s very hard to not do that and I’ve tried not to do that , because one of the lessons that we learn immediately as anthropologist is , there’s a lot of different ways of being in the world . So if you can apply cultural relativism to proposal writing , then you’re OK . But you never fully escape from your own interests , your own position , and so on , and so that is bound to have some impact . I don’t know that that’s necessarily a bad thing when you have a panel that’s sufficiently balanced in all sorts of ways , including academic discipline , areas of expertise , the kind of schools that you come from , and then the obvious , race , ethnicity and gender . So I do think it is important to bring all of those factors into play in creating the panel , because you aren’t going to be able to get rid of those influences that come in . Therefore , we observe a paradoxical situation in in - terdisciplinary panels . Panelists’ lack of familiarity with the technicalities of the disciplinary tools and evidence chosen by the applicants allows them to express their idiosyncratic tastes with much more confidence than expected . 25 For that matter , interdis - ciplinary panels can be said to be sites where new rules of fairness are redefined , reinvented and slowly recognized . When panelists talk about how they evaluate proposals in their own discipline , they generally uphold the legitimacy of the process by framing their judgment in universalistic terms , regardless of whether that might be said to be idiosyncratic Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 11 Conclusion Evaluators in interdisciplinary panels must walk a fine line between trust and creed to produce a fair evaluation . Panelists respect the primacy of discipli - nary sovereignty , deference to expertise and meth - odological pluralism . These latter rules ensure the preponderance of the voices of experts over non - experts in interdisciplinary panels . Panelists also adopt a range of tactics and strategies designed to make other reviewers who lack such expertise trust their judgments for being disinterested and unbiased . Experts with disciplinary familiarity tend to be harder on proposals from their own discipline . But at the same time , reviewers with disciplinary distance are not afraid to make decisions based on idiosyn - cratic tastes rather than substantive quality . The in - clusion of personal tastes in the review process does not impinge on panelists’ understanding of the deci - sion - making process as fair and the outcome as legitimate . Their lack of familiarity with most of the net - works of the candidates whose proposals are under review allows them to distance their role as knowl - edge producers ( which they often play from within a discipline ) from their role as evaluators ( which brings them to evaluate research for which they have no personal interest in funding ) . Therefore , panel - ists’ lack of disciplinary familiarity with the propos - als under review has an important effect on the decision - making process : it allows personal tastes to play a legitimate role in the evaluation process while ensuring at the same time that personal interests play no role at all in this process . For the evaluation pro - cess to be viewed as fair by panelists , idiosyncratic tastes can matter , whereas interests cannot . What we have focused on analyzing are the condi - tions that lead evaluators on interdisciplinary review panels to view their collective work as legitimate , in a context where their situation as “peers” is prob - lematic and not immediately taken for granted . What we cannot say is whether the processes of evaluation documented in this paper apply beyond multidisci - plinary panels in the humanities and social sciences , or whether alternative rules of fairness will be found in contexts other than in US panels . 26 Disciplinary panels may follow different rules , such as those described by Merton , or they be simi - larly structured by distance and familiarity , trust and creed , but centered on subfields instead of disci - plines ; in such a case , deferring to subfield expertise would take the place of deferring to disciplinary ex - pertise . This might be especially likely in fields with low levels of consensus or paradigm development ( Braxton and Hargens , 1996 ) . On the other hand , evaluators in disciplinary pan - els are more familiar with the standards and types of evidence used to assess the validity of competing theories , and fierce competition among members of the same discipline to appropriate the right to speak on a topic might be expected regardless of their expertise . Therefore , trust in the fairness of the pro - cess is more likely to be established through critical collegial evaluation and dialogue than on blind trust . Further research comparing interdisciplinary and in - tra - disciplinary evaluation would shed light on just how unique the process we have described is to in - terdisciplinary evaluation . Notes 1 . The competitions we studied were not “interdisciplinary” in the full sense of the term ; all of the panelists were trained in a discipline , and all of the proposals were rooted in a disci - pline , even though they almost all drew on methods , materi - als and theories from other disciplines . In practice , this is true of most research that goes under the rubric of “interdis - ciplinary” ( Klein , 1990 ) . 2 . We delve in the discussion of how “procedural fairness” ( the fairness of the decision - making process as opposed to the fairness of the outcome ) in Mallard et al ( under review ) . That paper analyzes how panelists respond to epistemological di - versity ( the presence of diverse epistemological styles in in - terdisciplinary panels ) , yet maintain faith in the evaluation process . The present paper builds on our previous argument to address the broader topic of interdisciplinarity . For a com - plete discussion of all these aspects , see Lamont ( forthcoming ) . 3 . Hoping to provide a direct empirical comparison between in - terdisciplinary and disciplinary panels in the social sciences and the humanities , we had secured permission to study disciplinary panels from the division of the Social and Behav - ioral Sciences at the National Science Foundation . However , invoking the Privacy Act , access was ultimately denied by NSF’s General Counsel’s office . 4 . The privileging of interdisciplinary proposals on these panels is underscored by a historian who explains that in order to be successful , applicants have to appeal to scholars with a range of interests and intellectual horizons . They have to “be able to hit a basic threshold of significance … and that has to do with ‘how is somebody outside of that field going to read this ? ’ If you can reach people outside your field , you’re inter - disciplinary , you know , you’ve reached across the discipli - nary divide . ” 5 . The specific competitions studied were the International Dis - sertation Field Research Fellowship program of the Social Science Research Council and the American Council of Learned Societies ; the Women’s Studies Dissertation Grant Program at the Woodrow Wilson National Fellowship Foun - dation ; and the Fellowship Program in the Humanities of the American Council of Learned Societies . 6 . The evaluative process adopted by most funding organiza - tions proceeds in two steps : first , individual screeners elimi - nate a large number of proposals ; then , panels of evaluators discuss finalists in face - to - face meetings and select awardees . This paper concerns only the second stage of evaluation . 7 . The competitions we studied have a program officer , a panel chair , or both . Program officers are PhD holders who may or may not have had an academic career and who are full - time employees of the funding agency . Panel chairs are generally established academics , and they preside over the panel for a few years only . 8 . We secured permission to observe the deliberations of two competitions . One of them was observed three times ; the other only once . We are excluding this data from the paper , however , because we did not gain access to all the delibera - tions . We used the field notes gathered during these delib - erations to probe panelists about specific arguments they had made at the time . The fact that the interviewer is a scholar who has served on a number of evaluation panels was essential in facilitating openness among interviewees . All respondents were guaranteed anonymity , and we made a commitment to the participating organizations to disguise all information potentially leading to the identification of panel - ists or applicants . 9 . For this purpose , we used the formal ranking of applicants Overcoming obstacles to interdisciplinary evaluation 12 Research Evaluation April 2006 produced by panelists prior to deliberations , which they pro - vided to the program officer or chair . 10 . Panelists also were asked to describe their criteria of evalua - tion beyond the context of the panel — for instance , how they recognize excellence in their graduate students , among their colleagues , and in their own work . They also were asked whether they believe in academic excellence and why . They engaged in strategies of presentation of self dur - ing the interviews , that we regard as behavioral and attitudi - nal data . Thus , we do not regard interview data as merely post - hoc , potentially distorted data . 11 . Having panelists provide several complementary accounts of ‘what happened’ is the best research strategy in the absence of deliberation transcripts ( unavailable due to confidentiality issues ) . In cases where observations were permitted , they confirmed the reliability of this retrospective interview method . 12 . We use brackets to denote details we have changed to pro - tect the identity of the panelist or proposal . 13 . Panelists adopt somewhat different definitions of a ‘good’ in - terdisciplinary proposal depending on the discipline from which proposals originate . In the case of humanistic as com - pared to social science scholarship , for instance , they put a greater premium on theoretical innovation and the originality of the topic than on methodological innovation — a contrast that we analyzed in the case of disciplinary understandings of originality ( Guetzkow et al , 2004 ) and that is explored in Abbott ( 2001 ; 2004 ) . 14 . This situation raises a specific problem in interdisciplinary evaluation in light of what Merton ( 1973 [ 1942 ] ) describes for disciplinary evaluation , particularly with respect to the obser - vance of the norms of “communalism” , which prescribes that scientists ought to make public their evidence and proce - dures concerning data gathering and analysis , and “organ - ized skepticism” , which prescribes that truth claims should be subject to critical examination based on evidence by in - formed peers . In interdisciplinary evaluation , these norms break down in practice , to the extent that panelists often lack familiarity with the evidence under consideration and the methods employed to make sense of the empirical material . 15 . Panelists vary in age , race and gender , and they represent institutions of uneven prestige ; these characteristics influ - ence how much weight their opinions have in collective deci - sion - making . 16 . Members of funding committees , which are generally part of large bureaucracies ( such as the National Science Founda - tion or the Social Science Research Council ) , usually ex - press their faith in the legitimacy of rational - legal orders ( GAO , 1994 ) . Panelists believe in the possibility of identifying the best proposals , because funding decisions are made by knowledgeable scholars who have demonstrated throughout their career an impressive command of their field . See also the large literature on cultural authority and on how scientists go about establishing their expertise ( Abbott , 1988 ; Jasanoff , 1990 ; Shapin and Schaeffer , 1985 ) . 17 . Although we sometimes describe panelists’ actions in strate - gic terms , we cannot say whether the behavior we describe here and subsequently is instrumental or not . For example , judging one’s own discipline more harshly could be a form of ‘impression management’ where panel members are instru - mentally trying to make the impression that their judgments are fair ; or it could simply be that their expertise leads them to be more critical because they feel more confident to cast judgment . We suspect that their actions combine elements of both . For example , even though their judgments are based on expertise , the fact that they announce it is done for instrumental reasons . 18 . This rule corresponds to Merton’s classical definition of uni - versal judgements ( as opposed to particularistic ones ) , which are made based on “pre - established impersonal crite - ria consonant with observations and with previously con - firmed knowledge” , and which are opposed to “particularistic” criteria such as “the personal or social attributes of their pro - tagonists ; their race , nationality , religion , class , and personal qualities are as such irrelevant” ( Merton 1973 [ 1942 ] : 270 ) . In canonical studies of peer review , such as the one of the NSF by Cole et al , ( 1978 : 33 – 34 ) , the authors propose that particularism “could refer to scientists with a common view of their fields who will appraise work only by others with similar views ; it could refer to social networks of friendship … and it could refer to social positions ; that is , those scientists who achieve eminence tend to favor the proposals of others who are similarly situated in the hierarchy of science . ” 19 . In disciplinary panels , it can be expected that applicants and referees usually include acquaintances of the evaluators , which is likely to color their judgments , given that level of trust is affected by network connections ( Burt , 2005 ; Cook , 2005 ) . The study of varying levels of inter - network connec - tion between evaluators in disciplinary and interdisciplinary panels is a path worth following . 20 . The funding agencies provide clear guidelines delineating the obligation to abstain when the work of close colleagues , friends , and direct advisees is being discussed . It is however impossible to know if disclosures about indirect personal ties are done on a systematic basis . 21 . This relationship between one’s ability to display a sense of disinterestedness and an eclectic taste on one side , and one’s scarcity of resources on the other side , has been un - earthed by sociologists of culture . Bourdieu ( 1984 [ 1979 ] ) wrote that those who lack economic and cultural resources can only make the “choice of the necessary” and cannot af - ford the luxury of disinterested behaviors , whereas those who Peterson and Kern ( 1996 ) call “cultural omnivores” have the strongest economic resources . 22 . Similarly , we find that panelists tend to define originality in ways that are in line with the type of originality that their own work exhibits . In Guetzkow et al ( 2003 ) , we detail various types of originality and we show that the types of originality that panelists attribute to their own work tend to overlap with the types of originality they attribute to the proposals under review — a phenomenon that sociologists label “homophily” , which “limits people’s social worlds in a way that has power - ful implications for the information they receive , the attitudes they form , and the interactions they experience” ( McPherson et al , 2001 ) . 23 . When attempting to “identify the cream” , evaluators often resonate with what Supreme Court Justice Potter Stewart once famously said about pornography : “I cannot define it , but I can recognize it when I see it . ” It is interesting to note that , when asked to define “academic excellence” , several of our panelists used these exact same terms . 24 . This question will be explored in Lamont ( forthcoming ) . See also Marcus and Fischer ( 1986 ) ; Smith ( 1990 ) and Mallard et al ( under review ) on the association between disciplines and epistemological styles , including the constructivist style , which is more popular in the humanities than the social sciences . 25 . An expression that standard approaches to peer review would find incompatible with the norm of universalism or / and disinterestedness . 26 . A review of the literature on peer review in the United States , the United Kingdom , and France suggests that the belief in the fairness of peer review is less widely held in France than in the USA ( Lamont and Mallard , 2005 ) . The particularistic aspects of French academia are also highlighted in many essays on French intellectuals and French academia ( see for example Debray , 1979 ) . References Abbott , Andrew 1988 . The System of Professions : An Essay on the Division of Expert Labor . Chicago : University of Chicago Press . Abbott , Andrew 2001 . Chaos of Disciplines . Chicago : University of Chicago Press . Abbott , Andrew 2004 . Methods of Discovery : Heuristics for the Social Sciences . New York : W W Norton . Bourdieu , Pierre 1984 [ 1979 ] . Distinction : A Social Critique of the Judgment of Taste . Translated by R . Nice , 1984 . Cambridge , MA : Harvard University Press . Braxton , John M and Lowell L Hargens 1996 . Variation among academic disciplines : analytical frameworks and research . In Higher Education : Handbook of Theory and Research , vol . 11 , ed . J C Smart , pp . 1 – 46 . Burt , Ronald S 2005 . Brokerage and Closure : Introduction to So - cial Capital . Chicago : University of Chicago Press . Cole , Jonathan and Stephen Cole 1981 . Peer Review in the National Science Foundation : Phase Two of a Study . Wash - ington , DC : National Academy Press . Overcoming obstacles to interdisciplinary evaluation Research Evaluation April 2006 13 Cole , Stephen 1992 . Making Science : Between Nature and Soci - ety . Cambridge , MA : Harvard University Press . Cole , Stephen , Leonard Rubin and Jonathan Cole 1978 . Peer Review in the National Science Foundation : Phase One of a Study . Washington , DC : National Academy Press . Cook , Karen S 2005 . Network , norms , and trust : the social psy - chology of social capital . Social Psychology Quarterly , 68 , 4 – 14 . Debray , Regis 1979 . Le Pouvoir Intellectual En France . Paris : Ramsay . Durkheim , E 1965 . The Elementary Forms of Religious Life . New York : Free Press . GAO 1994 . Peer Review Reforms Needed to Ensure Fairness in Federal Agency Grant Selection : Report to the Chairman , Committee on Governmental Activities , US Senate . Washing - ton , DC : General Accounting Office . Gieryn , Thomas 1983 . Boundary - work and the demarcation of science from non - science : strains and interests in professional interests of scientists . American Sociological Review , 48 , 781 – 795 . Gieryn , Thomas 1995 . Boundaries of science . In Handbook of Science and Technology Studies , eds . S Jasanoff , G M Jasanoff , James Petersen and Trevor Pinch , pp . 393 – 443 . Thousand Oaks : Sage . Guetzkow , Joshua , Michèle Lamont and Grégoire Mallard 2003 . Originality and the construction of academic worth : substantive quality and scholastic virtue in peer review . In Annual Meeting of the American Sociological Association . Atlanta , GA . Guetzkow , Joshua , Michèle Lamont and Grégoire Mallard 2004 . What is originality in the social sciences and the humanities ? American Sociological Review , 69 , 190 – 212 . Jasanoff , Sheila 1990 . The Fifth Branch : Science Advisers as Pol - icy Makers . Cambridge : Harvard University Press . Klein , Julie T 1990 . Interdisciplinarity : History , Theory , and Prac - tice . Detroit , MI : Wayne State University Press . Kuhn , Thomas 1970 . The Structure of Scientific Revolutions . Chi - cago , IL : University of Chicago Press . Lamont , Michèle . forthcoming . Cream Rising : Finding Excellence in the Social Sciences and the Humanities . Cambridge , MA : Harvard University Press . Lamont , Michèle and Grégoire Mallard 2005 . Peer Evaluation in the Social Sciences and Humanities Compared : The United States , the United Kingdom , and France . Ottawa : Social Sci - ences and Humanities Research Council of Canada , . Lamont , Michèle and Viràg Molnár 2002 . The study of boundaries across the social sciences . Annual Review of Sociology , 28 , 167 – 195 . Laudel , Grit 2004 . Conclave in the Tower of Babel : how peers re - view interdisciplinary research . In The Australian Sociology Association . La Trobe University , Beechworth , Australia . Mallard , Grégoire , Michèle Lamont and Joshua Guetzkow . Under review . Epistomological differences and fairness in peer re - view : evidence from the social sciences and the humanities . Department of Sociology , Harvard University . Marcus , George E and Michael M Fischer 1986 . Anthropology as Cultural Critique : An Experimental Moment in the Human Sci - ences . Chicago : University of Chicago Press . McPherson , Miller , Lynn Smith - Lovin and James M Cook 2001 . Birds of a feather : homophily in social networks . Annual Re - view of Sociology , 27 , 415 – 444 . Merton , Robert K 1973 [ 1942 ] . The normative structure of sci - ence . In The Sociology of Science , ed . N W Storer , pp . 267 – 278 . Chicago , IL : University of Chicago Press . Peterson , Richard A and Roger Kern 1996 . Changing highbrow taste : from snob to omnivore . American Sociological Review , 61 , 900 – 907 . Porter , Alan and Frederick Rossini 1985 . Peer review of interdis - ciplinary proposals . Science , Technology and Human Values , 10 , 33 – 38 . Ragin , Charles C , Joane Nagel and Patricia White 2004 . Report of the Workshop on Scientific Foundations of Qualitative Re - search . Arlington , VA : National Science Foundation . Shapin , Steven and Simon Schaeffer 1985 . Leviathan and the Air Pump : Hobbes , Boyle and the Experimental Life . Princeton : Princeton University Press . Smith , Dorothy E 1990 . Women’s experience as a radical critique of sociology , and The ideological practice of sociology . In The Conceptual Practices of Power : A Feminist Sociology of Knowledge , pp . 1 – 57 . Boston : Northeastern University Press . Travis , G D L and Harry M Collins 1991 . New light on old boys : cognitive and institutional particularism in the peer review system . Science , Technology and Human Values , 16 , 322 – 341 . Weber , Max 1978 [ 1956 ] . Economy and Society , vol . 1 , eds . G Roth and C Wittich . Berkeley , CA : University of California Press . Wissoker , Ken 2000 . Negotiating a passage between disciplinary borders . Chronicle of Higher Education , 46 , B4 . Zuckerman , H and R K Merton 1971 . Sociology of refereeing . Physics Today , 24 , 28 – & .