1 Journal Title : Knowledge - Based Systems Title : Information Access in Context Contact Author : Jay Budzik Intelligent Information Laboratory Northwestern University 1890 Maple Ave . Evanston , IL 60201 USA + 1 847 467 - 1771 ( phone ) + 1 847 491 - 5258 ( fax ) budzik @ infolab . nwu . edu Co - Authors : Kristian J . Hammond ( same affiliation and address ) + 1 847 467 - 1012 ( phone ) + 1 847 491 - 5258 ( fax ) hammond @ infolab . nwu . edu Larry Birnbaum ( same affiliation and address ) + 1 847 491 - 3640 ( phone ) + 1 847 491 - 5258 ( fax ) birnbaum @ infolab . nwu . edu Abstract : Our central claim is that user interactions with productivity applications ( e . g . , word processors , Web browsers , etc . ) provide rich contextual information that can be leveraged to support just - in - time access to task - relevant information . As evidence for our claim , we present Watson , a system which gathers contextual information in the form of the text of the document the user is manipulating , in order to proactively retrieve documents from distributed information repositories related to task at hand , as well as process explicit requests in the context of this task . We close by describing the results of several experiments with Watson , which show it consistently provides useful information to its users . The experiments also suggest that , contrary to the assumptions of many system designers , similar documents are not necessarily useful documents in the context of a particular task . Keywords : information access ; agents ; context ; proactive systems ; automated resource discovery ; similarity 2 1 . Introduction Information retrieval ( IR ) systems [ 1 ] have become the cornerstone of information access on the Internet ( e . g . , [ 2 - 4 ] ) and virtually all other settings in which people access large repositories of information via the computer . Such systems process requests in the form of query consisting of natural language search terms , and provide the user with a list of references to documents the system determines are relevant to the query . Very roughly , relevance is determined by comparing the words in the query with the words in the documents indexed . Given the simplicity of the process , and the domain - independence , IR systems can perform fully automatic indexing and retrieval of text documents . For huge repositories of information , they provide a key alternative to the costly process of manual indexing , arguably trading off the higher quality of manual indexing to save time , in light of the massive amount of information to be processed . Information retrieval systems were originally designed to be used by trained operators with an understanding of a particular system’s query language , the domain vocabulary of the corpus of documents being searched , and the kind of document indexed therein . Text - based terminals to IR systems were separate from the terminals and workstations used for everyday work activities . Focus in the IR community over the last decade has remained on the indexing and retrieval algorithms used by their systems , and the performance of such algorithms in terms of providing objectively relevant results given an isolated query [ 5 ] . By focusing on various algorithms used for indexing and retrieval , focus has typically remained on a very limited query - result - refinement kind of interaction in order to make statistically valid comparisons among algorithms . The context of use for information retrieval systems has changed significantly since their invention over two decades ago . The ubiquity of the personal computer , and the more recent widespread connectivity afforded by the Internet have changed the character of the situations in which IR systems are accessed . This change in situation has resulted in a change in user communities and the attitudes of those users , as well as a change in the potential information available to the systems when they process a request . Information retrieval systems are available to Internet users every day , from the comfort of their own personal computers . These information repositories are accessed by users from the same machine on which they write papers , read news , and browse the Web sites that interest them . This new setting provides new challenges and opportunities for the designers of information access systems ? both to create usable systems and also take full advantage of this new information - laden environment . Unfortunately , the kind of information communicated by the user to the system has not increased , nor has the design of human computer interaction within such systems changed very much . Users must still communicate their information requests in terms of a query to a system that is isolated from knowing the context of that need . Our aim is to improve the user’s experience with information access systems by expanding the bandwidth of information available to the system . We do this by coupling the system tightly with the tasks the user is performing in other applications via an intermediary software agent . As a result , requests for information need not be construed in an isolated setting , devoid of external influence . Rather , information requests can grounded in the context of the activities the user is performing and results can be judged by their utility in this context . We will also demonstrate that in some cases , information about what the user is doing is enough to proactively retrieve on - point information without requiring explicit requests . Given these goals , some of the key questions we will begin to address in this article are : 1 . How can information access systems benefit from knowing more about a user , her activities and her interests ? 2 . How do we build a system that knows enough about what a user is doing to be more helpful than systems that don’t ? 3 . Can we build an information access system that is useful without requiring explicit requests or user intervention ? 4 . How can such a system visually represent a user’s current task so she can form coherent expectations about her next interaction with the system ? 5 . How do we build a system that communicates implicit contextual information to traditional information systems , so that explicit requests can be made more coherent ? 6 . Does any of this actually help the user , and in what ways ? We will revisit each of these questions in turn throughout this article . Next we consider an example that demonstrates the need for information access systems to know more than they currently do about what the user is trying to accomplish when a request is made . 3 2 . The Problem of Context in Information Access Systems From the perspective of the user interface , natural language access to information seems ideal . Users need only express their request in terms of a few search terms . What could be easier ? Unfortunately , even if information retrieval systems attempted to understand and represent the concepts being expressed in the documents they index or the requests they process , the systems are divorced from critical information that is necessary to understand them . Namely , traditional information systems are isolated from the context in which a request occurs . Information requests occur for a reason , and that reason grounds the request in contextual information necessary to interpret and process it . Without access to this context , requests become highly ambiguous , resulting in incoherent results , and unsatisfied users . The following example helps illustrate this point . 2 . 1 A Query and Three Scenarios Consider the request “information about cats” and observe how drastically the utility of search results change as we manipulate the context of the request in the following scenarios . Scenario 1 : Veterinary student writing a term paper on animal cancer . In this case , the most appropriate resources probably have to do with feline cancer , its diagnosis , treatment , etc . Scenario 2 : Contractor working on a proposal for a new building . The contractor is most likely referring to Caterpillar Corporation , a major manufacturer of construction equipment , usually shortened to “Cat” by people in the construction business . Scenario 3 : Grade - school student writing a report about Egypt . In this case , we would like to see information about cat mummies , laden with pictures and descriptions that are appropriate for a grade school student . These scenarios illustrate three kinds problems associated with interpreting a request out of context . Problem 1 : Relevance of active goals . The active goals of the user contribute significantly to the interpretation of the query and to the criteria for judging a resource relevant to the query . Problem 2 : Word - sense ambiguity . The word sense of “cat” is different from the others in scenario 2 . The context of the request provides a clear choice of word sense . Problem 3 : Audience appropriateness . The audiences in each of the scenarios also constrain the choice of results . Sources appropriate for a veterinarian probably will not be appropriate for a student in grade school . The above examples uncover several major problems with current information retrieval systems that attempt to process requests out of context . Moreover , a recent study of search engine queries showed that on average , users’ queries were 2 . 21 words long [ 6 ] . Needless to say , a two - word query most likely does not contain enough information to discern the active goals of the user , or even the appropriate senses of the words in the query . These problems are not new to researchers . The following section outlines previous work in this area . 3 . Previous Efforts in Establishing Context for Information Access Previous efforts in establishing context for information access can be classified into four categories : relevance feedback in information retrieval , systems that use user profiles , approaches based on implicit and explicit techniques for word - sense disambiguation , and symbolic user modeling approaches . 3 . 1 . Relevance Feedback The technique of acquiring relevance feedback [ 7 ] to narrow the search space can be seen as a method of eliciting the context of a request from the user . In systems that support relevance feedback , the user begins with a standard query and then evaluates the results returned ( usually by judging a result as relevant or not relevant ) . The evidence gathered as a result of the user’s evaluation is used to modify the original query by adding positive or negative search terms . In the vector space model of information retrieval [ 1 ] , queries and documents are represented as vectors in a high dimensional space , where each dimension represents a word or word stem . Given a query Q , a vector in this space , documents D for which the measure d ( Q , D ) is minimized are first retrieved , where d is some measure of distance ( usually the cosine of the angle between the vectors Q and D , or equivalently the dot product of the two vectors if the space is normalized ) . When the user performs a judgment on a document D , the query Q is modified by adding the judged document’s terms and increasing the weight of those terms that already appeared in the query . This has the effect of moving the vector representing the query closer to the vector representing the document judged to be relevant , thus resulting in the retrieval of documents closer to this place in the multidimensional space ( and , hopefully , closer to what the user originally intended ) . 4 Few commercial search engines currently support relevance feedback . However , a recent study of Excite [ 2 ] , one of the only major search engines that have historically supported relevance feedback , showed that users hardly ever took advantage of it [ 8 ] . Unfortunately , a different study of the same system [ 6 ] showed that users were generally dissatisfied with their results , suggesting that the reason users did not use relevance feedback was not because the information in their initial short query of about two words was sufficient to retrieve relevant results . In addition , this study showed that search sessions did not typically include query refinements ( e . g . , the addition or deletion of terms ) , suggesting that users may not be willing to spend the time and effort necessary to use manual or semiautomatic refinement techniques . These results argue strongly for methods that automatically perform refinement up - front , instead of requiring explicit user intervention . 3 . 2 . User Profiles Efforts in building user profiles representing a user’s interests can also be seen as a method of gathering contextual information . User profiles can be collected by gathering terms based on rating documents as in relevance feedback , which was described above . Unlike the relevance feedback techniques , the information gathered in a profile persists across retrieval sessions where it may be automatically added to the user’s query [ 9 ] . Other systems use machine learning algorithms to induce a classifier for documents based on training examples gathered as a result of a user rating documents [ 10 , 11 ] . These systems typically learn binary text classifiers that classify documents as relevant or irrelevant . Systems that learn Naïve Bayes classifiers are common ( see [ 12 ] for an excellent survey of text classification techniques ) . Systems like Letizia [ 13 ] use implicit feedback in the form of user interactions ( such as book marking a page ) , to learn a user profile . Letizia uses its user profile to perform lookahead search in the locus of the page the user is currently viewing and recommend links accessible on the current page . Implicit feedback techniques seem particularly promising given that the results of the study discussed above suggest that users are unwilling to provide explicit feedback . Unfortunately , these kinds of systems have the disadvantage that while they do address the issue of communicating general interests to information systems , they lack access to the user’s active goals 1 . In our view , the active goals of the user are more important than long - term interests , especially for providing just - in - time access to relevant information . 3 . 3 . Word Sense Disambiguation Some systems attempt to reduce ambiguity by requiring the user to explicitly disambiguate among multiple possible word - senses [ 14 ] . Other systems use the popularity information intrinsic in the structure of hypertext documents [ 15 - 18 ] as a way of guessing the most likely sense of a word . Unfortunately , the disambiguation of word senses only addresses part of the problem of discerning context in an isolated setting . That is , the user’s active goals and the issue of audience are left unaddressed . In addition , requiring the user to perform explicit disambiguation may fall prey to the same user interaction problems encountered by explicit relevance feedback techniques . That is , users may be unwilling to choose among multiple senses . Systems that use reference text to index documents do have an advantage in that they require no intervention on the part of the user , yet they are limited by the fact that they typically use popularity as a metric for ordering results and fall short in taking an analysis of context any deeper . For example , at the time of writing , when Google [ 16 ] , a system which uses the text of links to index documents , processes the query “gas plasma displays , ” it will return a long list of documents about blood plasma , because the system has found references which include the word “plasma” most frequently point to documents which discuss blood plasma . Returning to our earlier example , then , just because the most common sense of the word “cat” is a noun representing feline mammals , doesn’t mean that this is what the user meant . 3 . 4 . Modeling Task and Interaction Semantics Several systems have been built that use an explicit model of user behavior in a particular application and explicitly associate queries ( or simply a user’s actions ) in a particular state of the task they are executing with resources that support that task [ 19 , 20 ] . For example , Argus [ 19 ] observes users interacting with a performance 1 Letizia is an exception because it operates on the page the user is currently viewing and attempts to offer assistance in the task of exploring the links on that page by recommending links that are related to the user’s interests . 5 support tool and uses an explicit task model in the form of a finite state automaton to detect opportunities to retrieve stories from an organizational memory system . While the performance of these systems can be impressive , they can be difficult to construct and are also usually limited in scope . In essence , they suffer the same problems as designing good hypertext documents : since related documents must be linked explicitly , the designer must have a prior knowledge of the documents to be linked . In settings in which the collection of documents is large and changing ( e . g . , the Internet ) this can be impractical . On the other hand , for situations in which user interactions are limited and regular , lexical representations of interface artifacts are unavailable , or in cases where the resources the user is able to access are fixed and limited in number , these kinds of approaches are indeed appropriate . In addition , we see an important opportunity for synergy between knowledge - intensive approaches and the approach we will describe in the next section that leverage the benefits of both . We will develop this position further in Section 7 . 4 . Information Management Assistants Our work on Information Management Assistants ( IMAs ) [ 21 ] is strongly motivated by the avenues the above approaches leave unexplored , and by what is known about the use of information systems . Our key contribution ( embodied by Information Management Assistants ) is a flexible and general architecture that allows for the coupling of implicit contextual information ? contextual information we can glean by observing the choices and actions the user takes while performing their everyday work activities ? with access to traditional information systems . The central question we are attempting to address with this architecture is whether or not user behavior in applications can be leveraged to support the retrieval of useful information from traditional information systems . We will return to an empirical treatment of this question in Section 6 . The following paragraphs describe the architecture we have developed to support the investigation of this question . Information Management Assistants observe users interacting with everyday applications and attempt to anticipate their information needs using a model of the task at hand . IMAs then automatically fulfill these needs using the text of the document the user is manipulating and a knowledge of how to form queries to traditional information retrieval systems ( e . g . , Internet search engines , abstract databases , etc . ) . IMAs embody a just - in - time information infrastructure in which information is brought to users as they need it , without requiring explicit requests . IMAs automatically query information systems on behalf of users as well as provide an interface by which the user can pose queries explicitly . Because IMAs are aware of the user’s task , they can augment the user’s explicit query with terms representative of the context of this task . In this way , IMAs provide a framework for bringing implicit task context to bear on servicing explicit information requests , in an attempt to address the problems associated with processing queries out of context . The conceptual architecture for an IMA is displayed graphically in Figure 1 . An Information Management Assistant observes users as they interact with everyday applications . The ANTICIPATOR uses an explicit task model to interpret user actions and anticipate a user’s possible information need . The CONTENT ANALYZER employs a model of the content of a document in a given application in order to produce a content representation of the document the user is currently manipulating . This representation is fed to the RESOURCE SELECTOR , which selects information sources on the basis of the perceived information need and the content of the document at hand , using a description of the available information sources . In most cases , this results in an information request being sent to external sources . A result list is returned in the form of an HTML page , which is interpreted and filtered by the RESULT PROCESSOR using a set of result analysis procedures , which may eliminate irrelevant results , or direct the result processor to pose a new request . The resulting list is presented to the user in a separate window . When the user inputs an explicit query , an IMA uses its knowledge of the user’s task context to generate an information request , which is sent to appropriate sources , as above . In cases in which it is inappropriate to automatically query information systems and filter the results , information requests can be presented to the user in the form of a button which , when pressed , executes the search and presents the results on - demand . 5 . Implementation of Watson : an Information Management Assistant A preliminary version of this architecture is realized in Watson , the first IMA we have built [ 21 , 22 ] . Watson has several application adapters , which are used to gain access to an application’s internal representation of a document , and the application - level events generated by user interactions with the application . We have developed adapters for Microsoft Word , a word processing package , Microsoft Internet Explorer , and Mozilla , a free - source 6 version of Netscape Navigator ( both WWW browsers ) . Figure 2 demonstrates Watson operating with Microsoft Word . Application adapters interpret application - level events so they can be translated into an event representation Watson can process . For example , when user types text into a document in Microsoft Word , keyboard events are generated in the application . The Word application adapter interprets these events , paying mind to their target ( in this case , the document the user is modifying ) . The adapter can then relay a message to the Watson application , in this case , indicating the document has changed . The Watson application could then query the application adapter , requesting the updated document . The adapter would then produce a document representation , which is sent to the Watson application . This is depicted graphically in Figure 3 . Watson transforms the document representation sent to it by application adapters into a query ( a process described in detail in the next section ) , and then selects sources appropriate for fulfilling this query . The selection process can be informed both by the content of the document or by a model of the task the user is performing in the application . The current implementation of Watson performs a limited analysis of actions within an application to determine the course of action it should take . For example , if a user inserts a caption with no image to fill it , Watson will direct queries to an image search engine . Future versions of Watson will have a source selection module that will determine which sources are most appropriate based on a model of the source ( see [ 23 ] for more details ) and a model of the current task . For example , if the user is manipulating a medical document , Watson might query a special - purpose search engine that specializes in information on that topic . The queries Watson produces take the form of an internal query representation capable of representing boolean combinations of terms or quoted phrases . This internal representation is sent to selected information adapters . Each information adapter translates the query into the source - specific query language , and executes a search . Information adapters are also responsible for collecting the results , which are gathered and clustered using several heuristic result similarity metrics , effectively eliminating redundant results ( due to mirrors , multiple equivalent DNS host names , etc . ) . Information adapters are relatively easy to add . We have built versions of Watson that query a local news source , and multimedia - enhanced versions of Watson that query image and video repositories ( Figure 4 ) . In parallel , Watson attempts to detect conceptually atomic , lexically regular structures in the document . Once such objects are detected , Watson presents the user with a common action for the item in the form of a button they can press . For example , if a page contains an address , Watson will present a button allowing the user to access a map for the address . Figure 5 demonstrates the Watson interface after a user has navigated to a page containing an address . The user has clicked on the button labeled “Get Map , ” which produces a map of the address generated by Yahoo ! Maps [ 24 ] . The aim of this functionality is twofold— first , to make the user aware of additional information associated with specific items , and second , to make it effortless to access and use it . The following sections describe heuristics for the analysis of the gross structure of documents ( e . g . , word emphasis , or list membership ) to the end of automatically constructing queries to information sources , as well as fine - level analysis aimed at detecting conceptually atomic , lexically regular structures . We also describe an algorithm for filtering search results aimed at eliminating redundant results and detecting broken links . 5 . 1 . Term Weighting for Query Construction In order to retrieve related documents as the user is writing or browsing , Watson must construct a query based on the content of the document at hand that will eventually be sent to external information sources in real time . Text retrieval systems typically require queries in the form of a sequence of search terms or keywords . The following heuristics were useful in constructing an algorithm that extracts search terms from a document to be included in such a query . Heuristic 1 : Remove stop words . Words included in a stop list are not good search terms . They will be automatically removed by the information systems themselves . Heuristic 2 : Value frequently used words . Words used frequently are representative of the document’s content . Heuristic 3 : Value emphasized words . Emphasized words are more representative of the document’s content than other words . Emphasized words are used in titles , section headings , etc . , and also draw more attention to the document’s reader . Heuristic 4 : Value words that appear at the beginning of a document more than words that occur at the end . Because reading is a linear process , words that occur earlier on tend to be descriptive of the rest of the document . Heuristic 5 : Punish words that appear to be intentionally de - emphasized . Words in small fonts ( de - emphasized words ) are exempt from Heuristic 4 . De - emphasized words are deliberately made smaller by the author to make them less distracting or , in some cases ( e . g . , privacy statements ) , hard to read . 7 Heuristic 6 : Ignore the ordering of words in lists . Words found in lists are a special case , because they may be ordered according to a syntactic criterion that does not reflect importance ( e . g . , alphabetically or by price ) , and are therefore exempt from Heuristic 4 . Heuristic 7 : Ignore words that occur in sections of the document that are not indicative of content . Words that occur in the navigation bar of Web page are only marginally useful , and tend to get in the way of the other text analysis heuristics we use . The above heuristics were used to construct application - specific adapters , as well as the following document representation and term weighting algorithm . Documents are represented as a stream of words or punctuations in one of four styles : normal , emphasized , de - emphasized or list item . A stop list is used to tag common words that have little information value ( e . g . , words like and , or , and the ) . Punctuations are explicitly represented in order to make the detection of phrase boundaries easier . Separate threads of execution can have simultaneous access to this stream by requesting a cursor , which represents the position in the stream . This is illustrated in Figure 6 . Words are classified into the four style groups ( or omitted , per heuristic 7 ) by detecting the appropriate structures in HTML documents ( for Internet Explorer ) , or by using the word style properties provided by Microsoft Word . Each of Watson’s application adapters sends a typed message containing a sequence of words of that type , represented as a string , to the Watson application . Watson then tokenizes the string using a basic lexical analyzer , tags stop words , and sends each term through the following weighting algorithm . Simultaneously , Watson sends tokens to an array of conceptual unit detectors described later in this paper . The pseudocode displayed in Figure 7 describes the term weighting algorithm used for constructing queries to general Internet search engines . A first pass through the terms has eliminated the stop words , and computed general statistics . At this point , maxCount is defined as the maximum number of times any one word has appeared in the document , numTerms , is defined as the total number of terms that were not stop words in the document , ? is constant factor , usually defined as 0 . 2 , and pos ( w ) contains a list of position - style pairs for a given word w . Intuitively , the preliminary weight of a term varies inversely with the square of its position on a page . This metric improves as the document length increases , prompting the addition of the numerator ( which is proportional to a fraction of the square of the total number of terms ) to reflect this fact . The term’s final weight is the sum of the preliminary weights that are less than maxCount , unless the term is a list item or de - emphasized . If a term occurring in a given position is a list item or de - emphasized , its preliminary weight is 1 . If a term is emphasized , its preliminary weight is double the original preliminary weight . The resulting term - weight pairs are sorted , and the top 20 terms are reordered in the order in which they originally occurred in the document . This ordered list is then used to form a query that is sent to selected information sources . For example , the query formed for the document in Figure 8a is displayed in Figure 8b . 5 . 1 . 1 . Other Term Extraction Techniques Techniques found in the Information Retrieval literature for automatically constructing indices [ 25 ] are closely related to the term extraction techniques we described above . These techniques rely heavily on corpus - wide statistics , such as the frequency of a word or phrase across the entire corpus [ 26 ] . The repositories Watson is intended to query do not provide such information , and hence this class of technique is not applicable . Our work on Watson has also motivated recent efforts aimed at embedding IR systems in editing applications [ 27 ] , although this work has again focused on term selection techniques that use corpus - wide statistics . Other work has focused on developing and comparing algorithms for keyphrase extraction . The Extractor system [ 28 ] couches keyphrase extraction as a supervised learning problem in which documents are treated as sets of phrases that are classified by the learning system to be included or not included in the final list of keyphrases extracted for the document . The primary goal of the Extractor system is to extract keyphrases that summarize the content of a document , although the keywords produced by Extractor have been used for “find more like this” kinds of tasks . Watson’s focus is somewhat different , in that the primary aim of the system is not to extract keywords , but to retrieve documents instrumental in fulfilling the document composition or research task at hand . An area of interest for future work is comparing the two systems’ respective performance in the context of this task . 5 . 2 . Result Clustering Because the results returned from traditional information systems often contain copies of the same page or similar pages from the same server , an IMA must filter the results so as not to add to a user’s feeling of information overload . If these similarities are not accounted for , some of the more useful pages returned by the traditional 8 information systems may be missed . Moreover , we constantly face the risk of annoying the user instead of helping her . As a result , we actively attempt to reduce the amount of irrelevant information presented . To this end , Watson collects search engine results and clusters similar pages , displaying a single representative from each cluster for the user to browse , much like Metasearch facilities do [ 29 ] . For the task of clustering redundant results , Watson uses two pieces of information that sources return for each document : the document’s title , and its URL . It employs the following heuristic similarity metrics for each of these pieces of information : Heuristic 1 : Title similarity . Two titles are similar if they overlap significantly ( e . g . , they share common substrings ) . The certainty of similarity increases as a function of the square of the length of the title in words . Heuristic 2 : URL similarity . Two URLs are similar if they have the same internal directory structure . The certainty of similarity increases proportionally as a function of the square of the length of the URL in directory units . More formally , suppose two documents have titles T 1 [ 1 . . n ] and T 2 [ 1 . . m ] , where each array element is a character . Let maxSubStr ( T 1 , T 2 ) be the maximum subsequence of T 1 that occurs in T 2 . Then the similarity of T 1 and T 2 is defined as length ( maxSubStr ( T 1 , T 2 ) ) / max { length ( T 1 ) , length ( T 2 ) } . If documents have URLs U 1 [ 1 . . n’ ] and U 2 [ 1 . . m’ ] , where each array element is a URL directory unit , the same similarity metric can be applied . The combination of these similarity metrics is generally sufficient to approximate the uniqueness of the documents returned . Note that we do not perform a clustering based on the full text of the document , as this would be far too bandwidth intensive given the goal of producing results in real - time , though it would probably be more accurate . Table 1 demonstrates the result of clustering a list of responses . In this case , over 50 % of the documents returned were eliminated . All of the documents the algorithm deleted from the final list were in fact duplicates . The clustering algorithm we use is incremental . What is different , here , is not the novelty of the algorithm , but the kind of human - computer interaction incremental clustering affords . That is , when a new response arrives from the network , it is immediately processed , and the resulting list of suggestions is updated and presented . The aim is to minimize the delay between receiving a response and updating the user interface . In general , the idea is to allow the user to access updated information as soon as it is available . As a result , the user is able to access a site in the middle of the clustering process , even if the system is waiting for further results to be returned from the information sources , unlike traditional interaction paradigms , which typically require all processing to be completed before results are displayed . As the suggestion list is being collected and incrementally computed , more detailed and expensive processing of the list ( such as URL validity checking or full document analysis ) is performed in the background , as a separate thread . We call this approach Present First , Process Later in the tradition of Riesbeck’s “Shoot first , ask questions later” paradigm of anytime reasoning [ 30 ] . Assuming the best result could take too much time to compute , the idea behind these approaches is that systems should provide access to results as they are computed , allowing the user to interact with the result of a computation at any time . 5 . 2 . 1 . Other Clustering Algorithms Other systems exist that perform clustering based on the summary texts returned by search engines , or that use the full - text of the document [ 31 - 33 ] . The aim of these systems ( unlike the clustering component in Watson ) is to cluster semantically related documents into groups that allow the user to select the most relevant one for further investigation . The assumption in such systems is that they start off with a short , ambiguous query generated by a user . It is then the system’s job to provide a way for the user to disambiguate that query by selecting clusters that best match their original intent . Watson is different in that it starts off with a very specific query that tends to produce redundant results in the same semantic category . Thus the goal is to remove redundancies , not to cluster semantically divergent documents , as it is in the previously mentioned systems . 5 . 3 . Detecting Opportunities to Provide Special - Purpose Information Watson must be able to reason about the contents of a document well enough to provide helpful suggestions . The previous section described an algorithm for computing a query that will be sent to online information sources , based on the text of a document . While this is helpful , it is only one of several things an Internet browsing or document composition assistant might do . In particular , we would like our assistant to be able to recognize opportunities to provide assistance by completing queries to special - purpose information repositories . To this end , Watson has a facility for detecting lexically regular , conceptually atomic items ( such as addresses or company names ) and providing the user with an interface to useful special - purpose information resulting from a query to specific kinds of online information sources . 9 In order to detect conceptual units for special purpose search , Watson runs an array of simple detectors in parallel . Each detector is a finite state automaton accepting a sequence of tokens representing a conceptual unit . When a conceptual unit is detected , Watson presents the user with a common action for the item in the form of a button they can press . For example , when Watson detects an address , it presents a button which , when pressed , will display a web page with a map for that address using an automated map generation service . Watson’s current facilities for extracting information of this type should be seen as a demonstration of feasibility . Further work will include integrating more robust techniques from the information extraction community [ 34 ] . Watson also detects opportunities for performing special - purpose search in the context of document composition . For example , when a user inserts a caption with no image to fill it in their Microsoft Word document , Watson uses the words in the caption to form a query to an image search engine . Users can then drag and drop the images presented directly into their document . This analysis of actions is also performed using an array of simple application - specific detectors running in each application adapter , much like the finite state automata model employed by the ARGUS system [ 19 ] . 5 . 4 . Processing Explicit Queries in Context Watson processes explicit queries in the context of the document the user is currently manipulating . Watson’s list of collected results makes its representation of the task context visible to the user . This kind of visibility is usually absent from most systems based on user profiles , yet it is extremely valuable because it allows users to form coherent expectations about the system’s performance . It is important to note that this representation not only provides the user with valuable information about otherwise hidden system states instrumental in forming expectations about their next interaction with the software , but it also represents information that useful in and of itself . In addition , note that the documents provided in the list of recommendations are concrete and comprehensible , and do not require the user to inspect a list of unassociated , weighted keywords ( as has often been the convention in many profile - based systems ) . When a user submits a query to Watson , it combines the new query terms with the previously constructed contextual query by concatenating them to form a single query . In this way , Watson brings the previously gathered information about the context of the user’s work to bear directly on the process of servicing a user’s explicit query . We have noticed that the addition of the terms Watson has gathered serves to make the user’s active goals explicit to the information service , reduce word - sense ambiguity , as well as ensure audience - appropriateness if the language of the active document is sufficient to express them . For example , if a user is viewing a page about NASA’s latest Mars probe ( Figures 9a and 9b ) and enters the query “life” , Watson will return a list of pages about life on Mars ( Figures 9c and 9d ) , not the magazine , the game , the algorithm , nor the biological definition ( as several search engines do ) . Because Watson grounds explicit queries in the context of the current document , the results returned are coherent , even for this highly ambiguous query . In cases in which a document is not available , or it is not a sufficient representation of a user’s task ( for instance if it’s all images ) , Watson acts as a standard metasearch system ( e . g . , [ 28 ] ) . Future work is aimed at addressing situations in which the user does not have some kind of document in hand . 5 . 5 . Presenting Resources from Multiple Sources The latest version of Watson queries multiple information sources for different kinds of resources . To date , Watson can search for related documents from the Internet and Intranets , images and video from publicly accessible libraries , items for sale at ecommerce sites , as well as news articles from Internet news sites . Instead of presenting the user with a long list of items ( Watson typically returns over 50 ) , a summary of the results is presented , and then the actual search results are presented in categories corresponding to their type , in order to avoid overwhelming the user . Each kind of is associated with a tab . Tabs aligned across the horizontal axis represent major content categories ; tabs aligned along the vertical axis represent system categories , including the summary pane . Figure 8 ( a ) demonstrates this interface . Each part of the summary display shows the type of resource ( e . g . , News ) , the number of resources , and an example entry . The user can then click on the corresponding tab ( displayed along the top of the interface ) to view the rest of the results of that type . Items with associated images ( movies , images , and items for sale ) are displayed as thumbnails in a grid , along with their caption and URL ( see Figure 4 ) . Image thumbnails are only downloaded if they are required for presentation , to conserve network bandwidth . Documents are displayed with title , the text extract returned by the search engine 2 , the URL , and the information source or sources that returned the item . 2 A recent study [ 35 ] has shown users are more satisfied with results when they are presented with extracts . 10 Once results are gathered , a separate thread of execution verifies whether each result is actually available . Internet information sources sometimes return URLs pointing to documents that have moved or have been deleted ( typically called “dead links” ) . Checking for dead links requires contacting the HTTP server over the network and requesting the HTTP header , a process that can be expensive over low - bandwidth connections . In addition , the only way to determine a link is dead is to wait for the connection to time out or the server to report an error , which can take several seconds . The The URLs of resources that have not been checked are displayed in gray . Those that have been checked and are valid are displayed in blue . Those that have been determined to be invalid are displayed in red . This URL verification functionality is aimed at making the process of using Internet resources less frustrating by allowing the user to easily determine whether or not a resource is there before they click on it . However , should the user want to take the risk of navigating to a ( possibly ) nonexistent site , she can still choose to do so before the verification process has finished . Our goal s to present the user with enough information about each item to quickly evaluate its utility in the context of the work she is performing without requiring additional effort that could distract the user from her primary task ( e . g . , following a dead link ) . 6 . Evaluation of Watson Most systems that make recommendations are evaluated on the basis of the objective relevance of a recommendation given the input to the system ( e . g . , [ 6 , 27 , 36 , 37 ] ) . Other systems are evaluated based on some measure of how “good” the recommendation is , a main component of which is usually relevance ( e . g . , [ 17 ] ) . Our first experiment was modeled after this general methodology . 6 . 1 . Experiment 1 For our first study , we collected a list of Web pages from other researchers at Northwestern . We asked users to choose a page from the list , look at it in a Web browser and then use Alta Vista to find similar pages . The users then judged the top 10 pages returned as relevant or irrelevant to their search task . Next , the users were asked to judge the sites Watson returned from the same page in the same way . In this experiment , Watson used Alta Vista as well . For our initial group of subjects , we drew from local computer science graduate students . All of the volunteers considered themselves expert - level searchers . This was evident in their query behavior , as most of them used long queries ( ? 4 words ) , laden with advanced features . We gathered 19 samples from a pool of 6 users . Using Alta Vista , our group of expert searchers was able to pose queries that returned , on average , 3 relevant documents out of 10 . Watson was able to do considerably better at the same task , returning , on average , 5 relevant documents out of 10 . In the samples gathered , Watson was able to do as well or better than an expert user 15 out of 19 times . A further comparison was performed of Watson and Alexa [ 38 ] in order to determine whether or not Watson provided a significant improvement over the recommendations available in current commercial systems . Alexa is a system that recommends Web pages given a URL as input , and is freely available to Internet users . It is unclear how Alexa works , because a detailed description of the system is not available . It is important to note , however , that we can only compare Alexa as a system with Watson using other information sources as a system . We cannot make claims , here , about the effectiveness of particular algorithms because we do not have control over the contents of the systems’ databases . This said , for this experiment , we gathered a collection of URLs from a volunteer’s bookmarks . We then had a volunteer evaluate the relevance of the recommendations returned by Alexa and by Watson on a 5 - point scale . The experiment was designed so the volunteer could not determine which system was providing the recommendations . The version of Watson used in this experiment was using both Alta Vista and the Google [ 16 ] search engine . The volunteer judged recommendations from 15 URLs , which resulted in him performing 316 ratings altogether . For the following summary statistics , ratings of 3 and above were considered relevant . Using this criterion for relevance , Alexa returned on average 4 relevant documents out of 10 . Watson , was able to do significantly better , returning on average more than 7 relevant documents out of 10 . In addition , Watson was able to return 118 relevant documents , while Alexa only returned 54 . The data from this study are summarized in Figure 10 . As the chart indicates , this experiment showed that as the definition of relevance increases in strictness , the gap between Watson and Alexa’s performance widens . A summary of both of the comparison studies is displayed in Figure 11 . This graph should be taken with a grain of salt , as the data displayed are gathered from two different experiments , using two different survey methodologies . 11 However the data suggest that Watson significantly outperforms both Alexa and expert users in providing relevant recommendations for the URLs in the experiments . The improvement in the second version of Watson indicates that Watson’s performance is related to the information source it queries , and that the inclusion of Google seems to improve the recommendations it gives . While the results of the experiment were favorable , users complained that they did not know on what basis they should judge relevance . Moreover , it was unclear that the similar documents were appropriate for some pages . For instance , one of the test pages was the front page of Yahoo ! [ 39 ] . It is unlikely there is any page that can objectively be considered relevant to this Internet directory site , in the absence of a knowledge of the user’s specific goal . The above observations caused us to question the methodology used to evaluate the system . Thus while this experiment was aimed at judging the relevance of Watson’s recommendations given a document , the second experiment attempted to measure the utility of results within a particular task context . The results were remarkably different . 6 . 2 . Experiment 2 This experiment was aimed at determining whether or not the sources returned by Watson were useful in the context of a particular task . Because Watson is intended to work alongside the user as she is completing a task , we were convinced that evaluating the utility of the information provided was more appropriate than the relevance - based judgments that are typical of most other evaluations of information retrieval systems and recommender agents . In addition , we were interested in investigating the degree to which the similarity of a returned document was related to the utility of that document in the context of a task . We asked researchers in our department to submit an electronic version of the last paper they wrote . Six responded . Each paper was loaded into Microsoft Word while Watson was running . The results Watson returned were then sent to the authors of the paper . Watson returned 74 documents for all of the respondents ( on average , Watson returned 12 documents per subject ) . Subjects were asked to judge the degree to which a recommended document would have been useful to them in the context of the task they were performing . Subjects were also asked to judge how similar the retrieved document was to the document they gave us . Both judgments were recorded on a 5 - point scale . For the following summary statistics , a document was counted as similar or useful if subjects gave it a numeric ranking above 2 . All of the subjects indicated that at least one of the references returned would have been useful to them . Two of the subjects indicated the references Watson provided were completely novel to them , and would be cited or used in their future work . On average , 2 . 5 of the documents Watson returned were deemed useful in the context of the task the user was performing . In contrast , almost half of the documents Watson returned were judged similar ( loosely replicating the results gathered in Experiment 1 ) . The results of this experiment are summarized in Table 2 . Figure 12 displays a histogram of the difference between similarity and utility rankings . The distribution is skewed to the positive end , indicating documents were often more similar than useful . In addition , the histogram indicates several documents were useful , but not similar . Conversely , the histogram indicates several documents were similar , but not useful . In fact , the correlation between similarity and utility was r = 0 . 51 ( if | r | = 1 , similarity and utility would be perfectly correlated , if r = 0 , they would not be related at all ) . Thus the similarity of a result accounts for r 2 = 0 . 26 , or about a quarter of the variance in the utility of a result . This weak overall relationship between similarity and utility underscores the necessity of evaluating the performance of these systems within the context of a particular task 3 . We also examined whether or not similarity was good enough for some tasks , given that it does not seem to be sufficient for all of them . Indeed , for subject 4 in our study , every similar document was also useful . The correlation of similarity and utility within a particular task context is presented in Table 3 . The data in table suggest that for some tasks , similarity is indeed a good predictor of utility ( correlations close to 1 ) , but for others , they have little to do with each other ( correlations closer to 0 ) . 3 It would be incorrect , however , to conclude that all of the unaccounted variance is due to task - sensitivity instead of something else ( e . g . , differences among subjects ) . A more rigorous experimental design is needed to make such a conclusion . 12 6 . 3 . Discussion of Results The results of the above experiment suggest that for some tasks , similar documents are not useful documents . In the aggregate Watson did consistently produce useful documents for users . However , the results suggest that improvements aimed at addressing particular classes of tasks or groups of users would be worth making . It is important to note that the above experiment only evaluated the automated results Watson returns . It did not evaluate the utility of results generated by explicit requests using Watson’s query in context facility . Our hypothesis is that use of this facility would improve the results described above , but further empirical work is needed to conclude this is in fact the case . Expanded evaluations of this and the other aspects of the system are needed to make stronger claims about the system , even though these results suggest the system is performing well on a variety of different documents , in a variety of different tasks . The differences in the attitudes of subjects in the two experiments are also worth discussing . Unlike in Experiment 1 , subjects in Experiment 2 reported it was easy for them to make judgments . We believe this is because subjects in Experiment 2 had an active context ( the task they were performing ) in which to make judgments , whereas in Experiment 1 , subjects had no such context , and were required to make judgments in the absence of any real goal . Studies have shown that similarity judgments ( and by extension , relevance judgments ) are highly context - sensitive ( e . g . , [ 40 , 41 ] ) ; the conjecture we put forth , here , is that such judgments are not only easier to make given a context , but that they are also more reliable , and more coherent . Early versions of Watson have been downloaded 4 by over 8 , 198 people ( most of the downloads were associated with press coverage about the system , we counted unique IP addresses sending HTTP GET commands requesting the installer program to arrive at the lower bound of 8 , 198 ) . Postings to an associated mailing list indicate that users are indeed using Watson , and that are generally satisfied with the way Watson performs ( although they think the installation program could be improved ) . Users have also stated they would like to be able to focus Watson on particular aspects of a document by highlighting paragraphs of text . Future work will focus on refining the level of control users have over the retrieval process using this kind of standard interface metaphor . 7 . Other Closely Related Work Software that recommends Web pages and learns user preferences has been an intense focus of must recent research . Closely related work that has not been discussed previously in the present article includes Metasearcher [ 36 ] a system which uses a collection of browser caches gathered from users working in collaboration on a common research task to form queries that are sent to search engines , the results of which are analyzed using LSI [ 42 ] ; and Remembrance Agents [ 37 , 43 ] , systems that suggest similar documents as a user composes a new document by performing IR search against a local corpus of previously written text . Watson is different from Metasearcher in that it works online and focuses on providing just - in - time access to relevant information , whereas Metasearcher performs compute - and network - intensive analyses that are inapplicable in this context . Watson is also different from the original Remembrance Agent ( RA ) in that it uses external information repositories and in its techniques for selecting representative terms . Newer versions of the RA can perform search against any locally indexed repository , and can even provide paragraph - level recommendations that are presented by aligning them next to paragraphs in the source document . This user interface technique seems particularly well suited for this kind of task because it can narrow the context of a recommendation down to a specific paragraph in the document . However , preliminary studies evaluating the Remembrance Agent’s performance [ 37 ] do suggest the quality of suggestions produced could be improved . In addition , several recent commercial offerings have functionality that is somewhat similar to Watson . Autonomy’s Kenjin system , released early in 2000 , and a system called Nano , released later that year , both make recommendations for further information based on an analysis of the active documents and activities of the user . Unlike Watson and Kenjin , and like the Alexa system mentioned previously , the Nano system appears to have a fixed set of cached recommendations . Watson goes a step beyond the above systems by providing an interface for communicating explicit requests to the system that are processed in the context of current activity , as well as by providing an architecture for recognizing opportunities to provide special - purpose information . Both of these functionalities represent a fundamental difference in belief that underlies our approach : that while it is a good start , similar information is not necessarily useful or even relevant information . The experiments in the previous section bear out this point , the 4 This early version is still available at http : / / infolab . nwu . edu / watson ; updates will also be posted at this address . 13 functionality we described has reflected it , and the work we outline in the next section is aimed at addressing it directly . 8 . Discussion and Ongoing Work Watson’s representation of a task context is a collection of words associated with the user’s current document . Future work will include augmenting this representation to include more detailed task and document models ( e . g . , knowledge of multi - section documents ) , as well as user profile and project management facilities that could allow Watson to process ambiguous requests in the absence of a discernable task . In addition , we are working on incorporating the semantics of particular kinds of tasks and domains into the system , with the aim of improving retrieval performance . Underlying this effort is the view that similar information is not always the most relevant information . For example , the Watson framework is currently being used to build a system called Point / Counterpoint , which assists users in supporting their point of view while they are developing a written argument ( for more details , see [ 44 ] ) . The system is based on the idea that when formulating an argument in support of a particular point , other documents which represent arguments both for and against that point are useful references . Point / Counterpoint uses knowledge of opposing experts in particular domains to recognize opportunities to retrieve examples of contrary points of view . For example , when a user cites Karl Marx’s idea of an ideal economic state , Point / Counterpoint will retrieve two sets of articles : one set representing Marx’s point of view , and another set representing Adam Smith’s opinion ( see Figure 13a ) . The queries Point / Counterpoint forms are composed of two distinct sets of terms— expert terms and issue terms . These queries are formed by modifying the result of the Watson query generation algorithm described above by substituting the name of an expert with his opposite while retaining the terms that represent the general topic of the argument . Issues are represented in multiple issue files , which define an issue detector and transformations to be performed on queries . Figure 13b shows portions of a sample issue file and the documents retrieved by Point / Counterpoint in the context of the issue the user is discussing in a Microsoft Word document . Although the issue file used in this example was constructed manually , we are in the process of building systems that automatically learn issues by extracting patterns of referral indicative of opposing points of view ( e . g . , “unlike ? x , ? y believes ? z” ) , taking the lead from systems like Rosetta [ 45 ] and SpinDoctor [ 46 ] . This view exemplifies the notion that strictly lexical representations ( e . g . , term vectors ) can be treated as first class representational objects that can be modified and transformed by knowledge - based systems in service of a user’s need . Vector space representations of documents are particularly good for computing document - to - document similarity in large collections . We believe that coupling such representations with semantic knowledge of a particular task or theme is an exciting avenue of future investigation . The Information Management Assistant architecture provides for such a coupling . 9 . Conclusion In summary , we have outlined several problematic issues associated with contemporary information access paradigms , the first and foremost of which is that information systems are divorced from contextual information necessary to coherently process a short request . In response to this , and in light of previous attempts , we presented an architecture for a class of systems we call Information Management Assistants . These systems observe user interactions with everyday applications , anticipate information needs , and automatically fulfill them using Internet information sources . An IMA’s query is grounded in the context of the user’s tasks . IMAs effectively turn everyday applications into intelligent , context - bearing interfaces to conventional information retrieval systems . We presented an overview of our work on Watson , a prototype of this kind of system . We then described an evaluation that underscored the effectiveness of our approach . The experiments supported a second , more theoretical point— that documents similar to the one the user is currently manipulating are not necessarily useful to her , and that progress can be made by improving upon specific kinds of document manipulation tasks . Finally , we closed with directions of ongoing and future research . Information Management Assistants embody a vision of a future in which users hardly ever form a query to request information . When an information need arises , a system like Watson has already anticipated it and provided relevant information to the user before she asks for it . Failing this , a user could explicitly express information needs to the system , which would service her request within the context of the current task . By providing a framework that allows information systems to begin to leverage the context of a user’s task , we believe IMAs represent a compelling new direction for research in intelligent systems . 14 Acknowledgements This work benefited from the efforts of Andrei Scheinkman , Cameron Marlow , and Justin Ramos who helped in the implementation of Watson . In addition , we thank Larry Birnbaum , Shannon Bradshaw , Louis Gomez , Chris Riesbeck , Henry Lieberman , members of the Northwestern Info Lab , and anonymous reviewers for candid discussions and helpful suggestions regarding this work . References [ 1 ] Salton , G . , Wong , A . , and Yang , C . S . , " A vector space model for automatic indexing , " Communications of the ACM , 18 ( 11 ) , 613 - 620 , 1971 . [ 2 ] Excite , Available at http : / / www . excite . com / . [ 3 ] Lycos , Available at http : / / www . lycos . com / . [ 4 ] AltaVista , Available at http : / / www . altavista . com / . [ 5 ] Spark Jones , K . , and Willet , P . , “Readings in Information Retrieval , ”San Francisco , CA , USA : Morgan Kaufmann , 1997 . [ 6 ] Jansen , B . , Spink , A . , and Bateman , J . , " Searchers , the Subjects they Search , and Sufficiency : A Study of a Large Sample of EXCITE Searches , " in Proceedings of The 1998 World Conference of the WWW , Internet and Intranet , AACE Press , 1998 . [ 7 ] Salton , G . , and Buckley , C . , “Improving Retrieval Performance by Relevance Feedback , ” in : Spark Jones , K . , and Willett , P . , ed . , Readings in Information Retrieval . Can Francisco , CA : Morgan Kauffman , 1997 . [ 8 ] Spink , A . , Bateman , J . , and Jansen , B . , " Users ' Searching Behavior on the EXCITE Web Search Engine , " in Proceedings of The World Conference of the WWW , Internet and Intranet , ( Orlando , FL ) , AACE Press , 1998 . [ 9 ] Chen , L . , and Sycara , K . , " WebMate : A Personal Agent for Browsing and Searching , " in Proceedings of The Second International Conference on Autonomous Agents , ACM Press , 1998 . [ 10 ] Billsus , D . , and Pazzani , M . , " A Personal News Agent that Talks , Learns and Explains , " in Proceedings of The Third International Conference on Autonomous Agents , ACM Press , 1998 . [ 11 ] Pazzani , M . , Muramatsu J . , and Billsus , D . , " Syskill & Webert : Identifying interesting Web sites , " in Proceedings of The Fourteenth National Conference on Artificial Intelligence , AAAI Press , 1996 . [ 12 ] Dumais , S . T . , Platt J . , Heckerman , D . , and Sahami , M . , " Inductive Learning Algorithms and Representations for Text Classification , " in Proceedings of ACM - CIKM98 , ACM Press , 1998 . [ 13 ] Lieberman , H . , " Letizia : An Agent That Assists Web Browsing , " in Proceedings of The International Joint Conference on Artificial Intelligence , 1995 . [ 14 ] Cheng , I . , and Wilensky , R . , An Experiment in Enhancing Information Access by Natural Language Processing , Computer Science Division , University of California , Berkeley , Berkeley , CA CSD - 97 - 963 , 1997 . [ 15 ] Bradshaw , S . , and Hammond , K . J . , " Mining Citation Text as Indices for Documents , " in Proceedings of The ASIS Annual Meeting , ( Washington , DC ) , Information Today , Inc . , 1999 . [ 16 ] Google , Available at http : / / www . google . com / . [ 17 ] Dean , J . , and Henzinger , M . R . , " Finding related pages in the World Wide Web , " in Proceedings of The Eighth International World Wide Web Conference , Elsevier , 1999 . [ 18 ] Spertus , E . , " Parasite : Mining Structural Information on the Web , " in Proceedings of The Sixth International World Wide Web Conference 1997 . [ 19 ] Johnson , C . , Birnbaum , L . , Bareiss , R . , and Hinrichs , T . , " Integrating Organizational Memory and Performance Support , " in Proceedings of The 1999 International Conference on Intelligent User Interfaces , ACM Press , 1999 . [ 20 ] Horvitz , E . , Breese , J . , Heckerman D . , Hovel , D . , and Rommelse , K . , " The Lumiere Project : Bayesian User Modeling for Inferring the Goals and Needs of Software Users , " in Proceedings of The Fourteenth Conference on Uncertainty in Artificial Intelligence , AAAI Press , 1998 . [ 21 ] Budzik , J . , Hammond , K . J . , Marlow , C . , and Scheinkman , A . , " Anticipating Information Needs : Everyday Applications as Interfaces to Internet Information Sources , " in Proceedings of The 1998 World Conference of the WWW , Internet and Intranet , ( Orlando , FL , USA ) , AACE Press , 1998 . 15 [ 22 ] Leake , D . , Scherle , R . , Budzik , J . , and Hammond , K . J . , " Selecting Task - Relevant Sources for Just - in - Time Retrieval , " in Proceedings of The AAAI - 99 Workshop on Intelligent Information Systems , AAAI Press , 1999 . [ 23 ] Yahoo ! Maps , Available at http : / / maps . yahoo . com / . [ 24 ] Salton , G . , Automatic Information Organisation and Retrieval . New York , NY : McGraw - Hill , 1968 . [ 25 ] Salton , G . , and Buckley , C . , “Term - Weighting Approaches in Automatic Text Retrieval , ” in : Spark Jones , K . , and Willett , P . , ed . , Readings in Information Retrieval . San Francisco , CA : Morgan Kaufmann , 1997 . [ 26 ] Kulyukin , V . , " Application - Embedded Retrieval from Distributed Free - Text Collections , " in Proceedings of The Sixteenth National Conference on Artificial Intelligence , AAAI Press , 1999 . [ 27 ] Turney , P . , Learning to Extract Keyphrases from Text , National Research Council Canada , Institute for Information Technology NRC - 41622 , February 17 , 1999 1999 . [ 28 ] Selberg , E . , and Etzioni , O . , " The MetaCrawler Architecture for Resource Aggregation on the Web , " IEEE Expert , November , 1996 . [ 29 ] Riesbeck , C . , “What Next ? The Future of Case - based reasoning in Post - Modern AI , ” in : Leake , D . , ed . , Case - Based Reasoning : Experiences , Lessons , and Future Directions . New York , NY : MIT Press , 1996 . [ 30 ] Hearst , M . , Pedersen , J . , and Karger , D . , " Scatter / Gather as a Tool for the Analysis of Retrieval Results , , " in Proceedings of The AAAI Fall Symposium on AI Applications in Knowledge Navigation , ( Cambridge , MA ) , AAAI Press , 1995 . [ 31 ] Fu , X . , Hammond , K . , and Burke , R . , ECHO : An Information Gathering Agent , Department of Computer Science , The University of Chicago , Chicago , IL , USA TR - 96 - 17 , 1996 . [ 32 ] Zamir , O . , and Etzioni , O . , " Grouper : A Dynamic Clustering Interface to Web Search Results , " in Proceedings of The Eighth International Conference on the World - Wide Web , Elsevier , 1999 . [ 33 ] Sundheim , B . M . , and Chinchor , N . A . , " Survey of the Message Understanding Conferences , " in Proceedings of The Human Language Technology Workshop , Morgan Kaufmann , 1993 . [ 34 ] Drori , O . , " Displaying Search Results in Textual Databases , " SIGCHI Bulletin , 32 ( 1 ) , 73 - 78 , 2000 . [ 35 ] Badue , C . , Vaz , W . , and Albuquerque , E . , " Using an Automatic Retrieval System in the Web to Assist Co - operative Learning , " in Proceedings of The 1998 World Conference of the WWW , Internet and Intranet , ( Orlando , FL , USA ) , AACE Press , 1998 . [ 36 ] Rhodes , B . , " Margin Notes : Building a Contextually Aware Associative Memory , " in Proceedings of The 2000 International Conference on Intelligent User Interfaces , ( New Orleans , Louisiana , USA ) , ACM Press , 2000 . [ 37 ] Alexa , Available at http : / / www . alexa . com / . [ 38 ] Yahoo ! , Available at http : / / www . yahoo . com / . [ 39 ] Tversky , A . , and Gati , I . , “Studies of similarity , ” in : Lloyd , E . R . a . B . , ed . , Cognition and Categorization . Hillsdale , NJ : Lawrence Erlbaum Associates , 1978 . [ 40 ] Suzuki , H . , Ohnishi , H . , and Shigemasu , K . , " Goal - Directed Processes in Similarity Judgment , " in Proceedings of The Fourteenth Annual Conference of the Cognitive Science Society , ( Bloomington , IN , USA ) , Lawrence Erlbaum Associates , 1992 . [ 41 ] Dumais , S . T . , Furnas , G . W . , Landauer , T . K . , Derrwester S . , and Harshman , R . , " Using latent semantic analysis to improve access to textual information , " in Proceedings of CHI 88 , The International Conference on Human Factors in Computing Systems , ACM Press , 1998 . [ 42 ] Rhodes , B . , and Starner , T . , " A continuously running automated information retrieval system , " in Proceedings of The First International Conference on The Practical Application of Intelligent Agents and Multi Agent Technology 1996 . [ 43 ] Budzik , J . , Hammond , K . J . , Birnbaum , L . , and Krema , M . , " Beyond Similarity , " in Proceedings of The AAAI 2000 Workshop on Artificial Intelligence for Web Search , ( Austin , TX , USA ) , AAAI Press , 2000 . [ 44 ] Bradshaw , S . , Scheinkman , A . , and Hammond , K . J . , " Guiding People to Information : Providing an Interface to a Digital Library Using Reference as a Basis for Indexing , " in Proceedings of The 2000 International Conference on Intelligent User Interfaces , ( New Orleans , Louisiana , USA ) , ACM Press , 2000 . [ 45 ] Sack , W . , " Representing and Recognizing Point of View , " in Proceedings of The American Association of Artificial Intelligence Fall Symposium on Artificial Intelligence Applications in Knowledge Navigation and Retrieval , ( Cambridge , MA . ) , AAAI Press , 1995 . 16 Table 1 Budzik Knowledge - Based Systems Before Processing : Input URLs After Processing : Titles Presented http : / / www . html . co . nz / news / 110605 . htm Sun Speaks Out On Java Standard… http : / / java . sun . com : 81 / aboutJava / … , http : / / java . sun . com / aboutJava / standard… , http : / / www . javasoft . com / aboutJava / stand… , http : / / java . sun . com / aboutJava / standard… , http : / / aidu . cs . nthu . edu . tw / java / JavaSoft / … , http : / / www . intel . se / design / news / javastand … , http : / / www . idg . net / new _ docids / find / java / … Java Standardization http : / / www . psgroup . com / snapshot / … Java Standardization Update - SnapShot http : / / xml . datachannel . com / xml / dev / XAPI … , http : / / www . datachannel . com / xml / dev / Com… Informal XML API Standardization . . . Java http : / / techweb1 . web . cerf . net / wire / news / … International Organization For Stand … http : / / techweb4 . web . cerf . net / wire / news / … Sun Moves Java Standardization Forw… http : / / www . javaworld . com / javaworld / jw… change nothing - JavaWorld - October Before Processing : ( Input URLs ) After Processing : ( Titles Presented ) http : / / www . html . co . nz / news / 110605 . htm Sun Speaks Out On Java Standardization … http : / / java . sun . com : 81 / aboutJava / … , http : / / java . sun . com / aboutJava / standard… , http : / / www . javasoft . com / aboutJava / stand… , http : / / java . sun . com / aboutJava / standard… , http : / / aidu . cs . nthu . edu . tw / java / JavaSoft / … , http : / / www . intel . se / design / news / javastand… , http : / / www . idg . net / new _ docids / find / java / … Java Standardization http : / / www . psgroup . com / snapshot / … Java Standardization Update - SnapShot http : / / xml . datachannel . com / xml / dev / XAPI… , http : / / www . datachannel . com / xml / dev / Com… Informal XML API Standardization . . . Java http : / / techweb1 . web . cerf . net / wire / news / … International Organization For Standard … http : / / techweb4 . web . cerf . net / wire / news / … Sun Moves Java Standardization Forward … http : / / www . javaworld . com / javaworld / jw… change nothing - JavaWorld - October 16 Table 1 : Results of clustering responses for documents on Java Standardization . 17 # Useful # Similar # Returned % Useful % Similar Difference Subject 1 4 8 18 0 . 22 0 . 44 0 . 22 Subject 2 1 9 11 0 . 09 0 . 82 0 . 73 Subject 3 0 1 8 0 . 00 0 . 13 0 . 13 Subject 4 8 8 13 0 . 62 0 . 62 0 . 00 Subject 5 2 6 9 0 . 22 0 . 67 0 . 44 Subject 6 5 3 14 0 . 36 0 . 21 - 0 . 14 Mean 0 . 25 ? 0 . 22 0 . 48 ? 0 . 27 0 . 23 ? 0 . 24 Table 2 : Users judged pages returned by Watson by their utility in the context of their task and by their similarity to the document they were manipulating on a 5 - point scale . Judgments n > 2 were considered similar or useful . Table 2 Budzik Knowledge Based Systems 18 Mean Difference Std . Deviation Correlation Subject 1 0 . 72 0 . 67 0 . 85 Subject 2 1 . 18 0 . 87 0 . 35 Subject 3 0 . 25 0 . 46 0 . 82 Subject 4 0 . 23 1 . 83 0 . 29 Subject 5 1 . 56 1 . 81 0 . 11 Subject 6 - 0 . 43 0 . 76 0 . 80 Table 3 : Mean difference between similarity and utility , standard deviation of the mean , and correlation between similarity and utility for 6 subjects . The overall mean is 0 . 53 , with standard deviation 1 . 29 . The overall correlation is 0 . 51 . Table 3 Budzik Knowledge Based Systems 19 Figure Captions Budzik Knowledge - Based Systems Figure 1 : The Information Management Assistant Conceptual Architecture . Figure 2 : The latest version of Watson is operating with Microsoft Word . The user is working on a paper on Socrates’ execution . Watson has returned several resources , including links to related Web sites ( e . g . , “Greek philosophy term papers for sale” , showing ) and electronic texts ( e . g . , “Plato , Apology , on line” , not shown ) , as well as books for sale ( e . g . , “The Last Days of Socrates” , not shown ) . In addition , Watson returns an image of the famous Jacques - Louis David painting of his execution ( not shown ) . Figure 3 : Message passing between the application , application adapter , and Watson . Figure 4 : The most recent version of Watson can access several kinds of sources . Section ( a ) shows images retrieved for this page about Richard Nixon ( e . g . , a picture of Nixon in China , and a picture of “Win with Dick” bubble gum cigars ) . Section ( b ) shows several items for auction ( e . g . , the former US president’s autographed business card ) . Figure 5 : Watson has detected an address on a Web page the user is browsing . It presents a button which when pressed , provides a map of the address . Figure 6 : Document Representation and Text Analysis Process . Figure 7 : Pseudocode of the term weighting algorithm we use for query construction . Figure 8 : Watson is has generated and executed queries for resources related to a document on the reunification of China and Taiwan . Section ( a ) shows the summary display for the resources Watson gathers . Section ( b ) of this figure shows the query Watson used to retrieve the results displayed . Figure 9 : Watson processes explicit queries in the context of the task the user is performing in a particular application . Here , the user is browsing a page about a NASA Mars probe , and enters the query “life . ” Watson returns documents about life on Mars . Figure 10 : Relevance ratings for suggestions provided by each system were given on a 5 - point scale . Shown above is the ratio of relevant results as a function of the threshold above which a document is considered relevant . Figure 11 : Comparison of Expert searchers , Alexa and two versions of Watson . Data for expert searchers and Watson 1 was gathered using a binary scale , while data for Alexa and Watson 2 was gathered using a 5 - point scale . In the latter case , references given a rating > 2 were counted as relevant . Figure 12 : Histogram of the difference between similarity and utility ( on a 5 - point scale ) for 6 subjects . The overall mean is 0 . 53 , with standard deviation 1 . 29 . The means and standard deviations of the individual subject rankings are shown in Table 3 . Figure 13 : Point / Counterpoint users a knowledge of opponents on a particular issue to automatically retrieve opposing arguments for an issue . Section ( a ) shows the associated interface ; section ( b ) shows the issue file used to recognize the issue and modify the query . 19 Budzik Knowledge - Based Systems User Actions Task Model ANTICIPATOR Information Need Content Model Document CONTENT ANALYZER Content Representation RESOURCE SELECTOR Explicit Request Information Source Descriptions Information Request External Information Sources Raw Search Results RESULT PROCESSOR Result Analysis Procedures Result List USER INTERFACE Figure 1 Figure should fit in a single column . 20 Budzik Knowledge - Based Systems Figure 2 Figure should fit in a single column . 21 Budzik Knowledge - Based Systems Application ApplicationAdapter Application - level Events and Content Requests Watson Update Requests Document Representation and Events InformationAdapter InformationSource Internal Query Representation Result List Source - specific Query Data Figure 3 22 Budzik Knowledge - Based Systems Figure 4 ( a ) ( b ) 23 Budzik Knowledge - Based Systems Figure 5 Figure should fit in a single column . 24 Budzik Knowledge - Based Systems CONTENT STREAM STYLE : EMPHASIZED STYLE : NORMAL february 26 1997 # p scholars say one country # p two AddressAnalyzer Related DocumentsQuery Producer # p # p … … DateDetector Figure 6 25 Budzik Knowledge - Based Systems for each word w collected for each ( position p , style s ) in pos ( w ) weight : = 1 + ( numTerms 2 ? ) / p 2 if ( weight > maxCount ) or ( s is the list item style ) or ( s is the de - emphasized style ) then weight : = 1 else if ( s is the emphasized style ) then weight : = 2 weight weight ( w ) : = weight ( w ) + weight Figure 7 Figure should fit in a single column . 26 Budzik Knowledge - Based Systems freedom AND autonomy AND taiwan AND people AND macao AND scholars AND country AND systems AND reunification AND mainland AND china AND guarantee AND principle AND uprising AND scholar AND deputy AND policy AND president ( a ) ( b ) Figure 8 27 Budzik Knowledge - Based Systems SPACE UPDATE CNN - Technology : Space Chronology of Space Exploration The Whole Mars Catalog [ tm ] SpaceViews Article : Scientists Get First … NASA To Reveal Next Mars Landing Site … Friday October 1 12 : 12 AM ET Noframes Timeline Mars Polar Lander Official Website WWW Space and Mystery – Mars CNN – Metric mishap caused loss … What’s New with Life on Mars Ancient Life on Mars ? ? ? Cydonia : A Peer - level Review Statement of Dr . Wesley T . Huntress , Jr . Douglas Isbell Headquarters , Washington … Washingtonpost . com : Mars probe to Examine … Transcript – March 31 , 1999 Lecture by … Copernicus Planetarium Astronomy Information … The Astrobiology Web SPACEZONE MARS Martian Fireworks : Earth Ship to Land on Mars … ( a ) ( b ) ( c ) ( d ) Figure 9 28 Budzik Knowledge - Based Systems Ratio of Relevant Results vs . Threshold 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4 Threshold of relevance judgment Watson Alexa Figure 10 Figure should fit in a single column . 29 Budzik Knowledge - Based Systems Ratio of Relevant Results 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 ExpertSearchers Alexa Watson 1 Watson 2 Figure 11 Figure should fit in a single column . 30 Budzik Knowledge - Based Systems 0 1 2 3 4 5 6 7 8 9 10 - 5 - 4 - 3 - 2 - 1 0 1 2 3 4 5 Difference of Similarty and Utility ( 5 point scale ) C o un t Subject 1 Subject 2 Subject 3 Subject 4 Subject 5 Subject 6 Figure 12 31 Budzik Knowledge - Based Systems NAME : capitalism / communism TERMS : capitalism TERMS : wealth of nations , adam smith TERMS : adam smith TERMS : communism , economy TERMS : marx , communism TERMS : marxism , economy TERMS : karl marx TERMS : communist manifesto RULE ( ruleset1 ) : SUBST karl marx / adam smith RULE ( ruleset1 ) : SUBST adam smith / karl marx RULE ( ruleset1 ) : SUBST marx / smith RULE ( ruleset1 ) : SUBST smith / marx RULE ( ruleset2 ) : SUBST capitalism / communism RULE ( ruleset2 ) : SUBST communism / capitalism ( a ) ( b ) Figure 13