Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas TOM HOPE * , Allen Institute for AI and The University of Washington , US RONEN TAMARI , Hebrew University of Jerusalem , Israel HYEONSU KANG , Carnegie Mellon University , US DANIEL HERSHCOVICH , University of Copenhagen , Denmark JOEL CHAN , University of Maryland , US ANIKET KITTUR , Carnegie Mellon University , US DAFNA SHAHAF , Hebrew University of Jerusalem , Israel Web - scale repositories of products , patents and scientific papers offer an opportunity for building automated systems that scour millions of existing ideas and assist users in discovering novel inspirations and solutions to problems . Yet the current way ideas in such repositories are represented is largely in the form of unstructured text , which is not amenable to the kind of user interactions required for creative innovation . Prior work has pointed to the importance of functional representations â€“ capturing the mechanisms and purposes of inventions â€“ for allowing users to discover structural connections across ideas and creatively adapt existing technologies . However , previous work exploring the use of functional representations was either very coarse - grained and limited in expressivity , or dependent on manually curated knowledge bases with poor coverage and significant manual effort from users . To help bridge this gap and unlock the potential of large - scale idea mining , we propose a novel computational representation that automatically breaks up products into fine - grained functional facets . We train a model to extract these facets from a challenging real - world corpus of invention descriptions , and represent each product as a set of facet embeddings . We design similarity metrics that support granular matching between functional facets across ideas , and use them to build a novel functional search capability that enables expressive queries for mechanisms and purposes . We construct a graph capturing hierarchical relations between purposes and mechanisms across an entire corpus of products , and use the graph to help problem - solvers explore the design space around a focal problem and view related problem perspectives . In empirical user studies , our approach leads to a significant boost in search accuracy and in the quality of creative inspirations , outperforming strong baselines and state - of - art representations of product texts by 50 - 60 % . 1 INTRODUCTION A modern - day engineer , scientist or designer has access to online repositories of millions of products , scientific papers and patents , containing descriptions of myriad technologies and their uses ; essentially , a huge database of problems and solutions . Combined with rapid advances in algorithms for extracting information from large unstructured databases , this raises the prospect of using machines to augment and scale the process of innovation , helping human problem solvers identify inspirations and solutions across domains . The human ability to detect abstract relations across ideas and find ways to creatively adapt existing tools for new uses , has been a driving force in the history of innovation [ 10 , 28 , 32 , 34 , 40 ] . Microwave ovens were discovered by repurposing radar technology developed during World War II ; Teflon , today chiefly used in non - stick cookware , was first used in armament development ; and gigantic organizations such as NASA and Procter & Gamble actively engage in searching for opportunities to adapt existing technologies for new domains and markets [ 18 ] . In a very different kind of example , a car mechanic recently invented a simple device to ease childbirths by adapting a trick for extracting a cork 2021 . Manuscript submitted to ACM 1 a r X i v : 2102 . 09761v1 [ c s . H C ] 19 F e b 2021 , , Hope et al . stuck in a wine bottle â€“ which he discovered online , in a YouTube video [ 1 ] . This award - winning vacuum device could save millions of lives in developing countries . Strikingly , according to the World Health Organization , there has been no innovation in this area of work â€œfor almost centuriesâ€ . These and many other examples suggest a future where automated systems mine web - scale repositories with myriad descriptions of inventions , surfacing pertinent inspirations or solutions to problems . But despite the immense promise for accelerating the pace of innovation , finding inspirations continues to currently mostly be a manual , trial - and - error process , or simply the result of serendipity . A key limiting factor is that these large idea repositories cannot support the kinds of user interactions that are required to support creative inspiration , because the predominant computational representation of ideas â€“ in the form of unstructured textual descriptions â€“ is unsuitable for these interactions . Human creativity often relies on detecting structural matches across distant ideas , adapting them by transferring mechanisms from one domain to another [ 12 , 13 , 25 , 26 ] â€“ but this human skill is notoriously hard to transfer to machines [ 36 ] . A primary reason is that structured representations of ideas are simply not generally available . Repositories of scientific papers , patent publications or product descriptions are typically limited to â€œstructureâ€ in the form of high - level category - focused keywords , which do not support the functional interactions we desire . For example , to identify that a contraption for extracting a cork stuck in a bottle could serve as relevant inspiration for easing childbirth , an automated system would need to figure out that a vacuum - based mechanism can serve the purpose of extraction of physical objects , and match this function to the problem of extracting babies stuck in the birth canal . At the same time , most structured knowledge bases that do provide richer , more structured representations ( e . g . , [ 6 , 62 ] ) are hand - crafted and small , and previous efforts to scale - up have been limited in expressivity [ 17 , 22 ] . General - purpose knowledge bases ( e . g . , Cyc [ 44 ] , NELL [ 53 ] , DBpedia [ 20 ] ) largely encode categorical knowledge ( e . g . , is - a , has - a ) and rarely functional knowledge ( e . g . , used - for ) , and can also suffer from poor coverage [ 29 ] . One promising recent approach [ 36 ] trains neural networks to learn one aggregate purpose and mechanism vector of products as coarse , soft â€œstructureâ€ that can be derived from raw text and used to find analogically related products with similar overall purpose but distant mechanism . The resulting matches led to increased creativity measured in an empirical ideation study . Further work used the same approach to find analogies in scientific papers [ 10 ] . However , in reality products have multiple fine - grained purposes with different mechanisms for achieving each , as demonstrated in Figure 1 . As shown in Figure 1 , the single - vector approach of [ 36 ] to search for products related to a smart pillow device cannot disentangle its different functional facets ( tracking sleep , neck support , etc . ) . The aggregate approach squashes together multiple purposes and mechanisms into one soft â€œpuddleâ€ , losing important information for retrieval of products that have only partial functional matches and limiting the ability to find diverse adaptation opportunities . Importantly , this aggregate representation does not only harm retrieval accuracy , but suffers from a fundamental limitation in terms of the interactions it enables . Prior work has demonstrated the importance of interactions for traversing and exploring granular functions . A recent study [ 29 ] showed that providing designers with computational tools to express the particular aspects of purposes that they are interested in and to traverse multiple levels of granularity and abstraction , could significantly increase the novelty and usefulness of ideas they generated . An earlier study [ 65 ] showed that representing problems in terms of multiple purposes and constraints enabled designers to search for more novel and useful inspirations . The WordTree method [ 46 ] â€“ a prominent method in creative engineering design â€” directs designers to break their problem into subfunctions , and then use the WordNet [ 51 ] database to explore abstractions and related functional facets to inspire analogies to products and designs across domains . However , to date the scope of applicability of these interactions has been limited by the lack of scalable means for modeling ideas in terms of granular purposes and mechanisms . The approach in [ 65 ] explored only manually constructed 2 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ What everyone wants to have is a comfortable way to sleep while traveling . A neck pillow ï¬lled with soft material that supports your neck . Itâ€™s unique because it has sensors to track your sleep . Track sleep Sleep Comfort Travel Support neck Neck pillow Soft material Sensors Standard search vector representation Purpose & mechanism aggregate vectors Our approach : Fine - grained purpose & mechanism facets Neck pillow with sensors for comfortable traveling Soft pillow with sensors Robotic neck brace Car seat vital signs monitor Fig . 1 . Extracting fine - grained purpose and mechanism functional facets from an online product description , to search for adaptation opportunities . Green spans are mechanisms , red spans are purposes . Left : Standard vector - based search does not enable control for partial functional matches . Retrieval results are typically highly similar to the original product , which is not helpful in creative innovation interactions . Center : The aggregate approach in previous work [ 36 ] captures only one overall , coarse purpose / mechanism , limiting the expressivity of the search and losing important information for retrieval of products that have only partial functional matches . Right : Our fine - grained functional facets enable users to discover focused matches based on specific functions , retrieving more diverse inspirations for creative adaptation . problem representations , and the WordTree method provided instructions to , but not technical scaffolds for , identification of functional facets to use for exploring a design space . The system of [ 29 ] required both manual effort from the user in specifying the different purposes , as well as a manually - curated knowledge base ( Cyc [ 45 ] ) in which those purposes were already connected in a concept graph describing their hierarchical relationships , which suffers from poor coverage [ 29 ] for real - world product description texts . In addition , even after the user manually specified granular purposes , the system was forced to use the aggregate approach of [ 36 ] to retrieve relevant matches from a corpus of products , since no automated tool for extracting granular purposes at scale across all products was available . To help close this gap and enable interactions between humans and automated systems that facilitate innovation , we develop a new computational representation of idea descriptions based on fine - grained functional facets . Our system automatically identifies multiple purposes and mechanisms within a given product description . We then construct a novel span - based representation of each product in terms of purpose and mechanism functional facets and their corresponding vector embeddings . We demonstrate the utility of our approach for supporting human creativity in two applications : ( 1 ) Fine - grained functional search for alternative uses of mechanisms , and ( 2 ) Exploring alternative problem perspectives around a focal problem for potential inspirations . Functional search for alternative uses of mechanisms . Our span - based representation enables innovators to search for ideas with expressive queries for specific functions . Figure 1 shows an example of functional facets automatically extracted by our system and their use for retrieval of potential inspirations for adaptation opportunities . In Section 3 , we 3 , , Hope et al . build a prototype fine - grained functional search tool , and evaluate its utility in an alternative uses task in which users find unconventional applications of given mechanisms , potentially leading to pathways to new markets . Exploring problem perspectives with a functional concept graph . We further use our representation to automatically generate a functional concept graph that embeds purpose / mechanism facets at different levels of granularity . While the coarse representation in [ 36 ] made it hard to pull out discrete and interpretable concepts from product texts , our fine - grained approach allow us to mine recurring functional relations , such as specific problems that are often mentioned together or specific problems and solutions associated with them . This level of detail can enable us to map the landscape of ideas â€” similarly to manually curated functional ontologies , a core tool used in engineering and design ideation [ 27 , 35 ] . By automating the graph construction , we take a step toward removing the dependence on manually - constructed KBs that limited previous work [ 29 ] . We evaluate the utility of our graph in an application involving problem reformulation [ 15 , 16 ] : construing an existing problem in terms of other structurally related problems , to explore alternative problem perspectives and the design space around a focal problem . This capability can help users â€œbreak outâ€ of fixation on the details of a specific problem and connect to parts of the design space that may superficially look unrelated [ 11 , 40 ] . In both applications , our approach leads to a significant boost of 50 - 60 % over the best - performing baselines , including the previous work of [ 36 ] . Our computational representation of idea descriptions and the interactions it enables , help address several key challenges to unlocking the potential of large scale online idea mining , including the bottlenecks in manual construction of structured idea repositories ; limited expressivity for users in searching fine - grained purposes and mechanisms ; and harnessing idea repositories to flexibly explore alternative problem formulations across levels of abstraction . We believe our representation may serve as a useful building block for novel creativity support tools that can help users find and recombine the inspirations latent in unstructured idea repositories at a scale previously impossible . A summary of our contributions : â€¢ We propose a novel computational representation of ideas with granular functional facets for purposes and mechanisms extracted automatically from product descriptions . â€¢ We use crowd workers to annotate product texts from a challenging real - world corpus , and evaluate several extraction models trained on these annotations . We represent each product as a set of span embeddings , corresponding to the multiple facets , and use similarity metrics over these sets to support partial , focused matching between ideas . â€¢ Using our similarity measures between ideas , we build a novel functional search capability that supports expressive , fine - grained queries for purposes and mechanisms . â€¢ We demonstrate the flexibility and utility of the representation for computational support of core creative tasks : ( 1 ) searching for alternative , atypical product uses for potential adaptation opportunities ; and ( 2 ) creating a functional concept graph that enables to explore the design space around a focal problem . Through two empirical user studies we demonstrate that our representation significantly outperforms both previous work and state - of - the art embedding baselines on these tasks . We achieve Mean Average Precision ( MAP ) of 87 % in the alternative product uses search , and 62 % of our inspirations for design space exploration are found to be useful and novel â€“ a relative boost of 50 - 60 % over the best - performing baselines , including the coarse representation approach of [ 36 ] . 2 LEARNING A FINE - GRAINED FUNCTIONAL REPRESENTATION Our goal in this section is to construct a representation that can support the creative innovation tasks and interactions discussed in the Introduction . Previous work [ 36 ] suggested a representation separating an idea into one purpose vector 4 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , and one mechanism vector . While that approach showed promise , the one - vector representation was coarse , mashing together many different purposes and mechanisms , and limiting interactions that require fine - grained control by the user . Figure 1 shows an example . When searching for products sharing structural relations with a smart pillow product , the aggregate purpose / mechanism vectors squash together multiple concepts such as comfort , sleep , travel , neck support ( purposes ) or neck pillow , soft material , sensors ( mechanisms ) â€“ limiting the ability to tease apart different sub - purposes and sub - mechanisms . This results in retrieval of another smart pillow , which is only slightly different in that it is not intended for travel . The aggregate vectors are also not interpretable â€“ leaving the user blind to what is truly being matched as part of the process of idea retrieval , and not enabling targeted focus on specific functional aspects . In contrast , we propose to use span representations [ 42 ] . Given a product text description , we extract tagged spans of text corresponding to purposes and mechanisms ( see Figure 2 ) , and represent the product as a set of span embeddings . By doing so , we are able to employ similarity metrics that support partial , faceted matching between ideas . Continuing our example , we can now represent the smart pillow with a set of purpose and mechanism spans ( Figure 1 , right ) . This allows to retrieve a wider range of products with faceted matches , such as a robotic neck brace for the neck support purpose , or a car seat vital signs monitor which matches on the embedding combination of travel , support neck , sensors . These retrieved products could point to new directions to explore , such as new markets where the smart pillow technology could be adapted ( e . g . , to increase comfort in robotic neck braces or car seats with sensors ) . More technically , we use a standard sequence tagging formulation , with X ğ‘ = { x 1 , x 2 , . . . , x ğ‘ } a training set of ğ‘ texts , each a sequence of tokens x ğ‘– = ( ğ‘¥ 1 ğ‘– , ğ‘¥ 2 ğ‘– , . . . , ğ‘¥ ğ‘‡ğ‘– ) , and Y ğ‘ a corresponding set of label sequences , Y ğ‘ = { y 1 , y 2 , . . . , y ğ‘ } , y ğ‘– = { ğ‘¦ 1 ğ‘– , ğ‘¦ 2 ğ‘– , . . . , ğ‘¦ ğ‘‡ğ‘– } , where each ğ‘¦ ğ‘— indicates token ğ‘— â€™s label ( purpose / mechanism / other ) . In later sections , we represent each product ğ‘– as a set of purpose span embedding vectors and a set of mechanism span embedding vectors . For the reasons discussed above , we view the span - based approach not simply as a more flexible and nuanced model , but as a potential building block that can power new interfaces and paradigms for innovation that we explore later in this paper . We start by describing our data and annotation process ; we then discuss and evaluate models to extract spans from product texts , followed by applications and experiments . 2 . 1 Data Fig . 2 . Crowdsourcing interface for fine - grained purposes and mechanisms . Boxes are predefined chunks to annotate . We use real - world product idea descriptions taken from crowdsourced innovation website Quirky . com and used in [ 36 ] , including 8500 user - generated texts describing inven - tions across diverse domains ( e . g . , kitchen products , health and fitness , clean energy ) . Texts typically include multi - ple purposes and mechanisms . Texts in Quirky use very nonstandard language , including grammatical and spelling errors ( e . g . , â€œFolds Up Perfect For Carrying . you can walk - on , put your mouth on and or hands on . numbers in any configuration 4 learning to De / Composing Numbers . â€ ) . Annotation . To create a dataset annotated with purposes and mechanisms , we collect crowdsourced annotations on Amazon Mechanical Turk ( AMT ) . We observed that in the annotation task of [ 36 ] workers tend to annotate long , often irrelevant spans . We thus guided workers to focus on shorter spans . To further improve quality and encourage more granular annotations , we limited maximal span length that could be annotated , and disabled the annotation of stopwords . 5 , , Hope et al . Fig . 2 shows our tagging interface ; rectangles are taggable chunks . For quality control , we required US - based workers with approval rate over 95 % and at least 1000 approved tasks , and filtered unreasonably fast users . Workers were paid $ 0 . 1 per task . In total , we had 400 annotating workers . Median completion time was 100 seconds . While a manual inspection of the annotations revealed they are mostly satisfactory , we observe two main issues : First , there are often multiple correct annotations . Second , workers provide partial tagging â€“ in particular , if similar spans appear in different sentences , very few workers bother tagging more than one instance ( despite instructions ) . These issues would have made computing evaluation metrics problematic . We thus decided to use the crowdsourced annotations as a bronze - standard for training and development sets only . For a reliable evaluation , we collected gold - standard test sets annotated by two CS graduate students . Annotators were instructed to mark all the relevant chunks , resulting in high inter - annotator agreement of 0 . 71 . We collect 22316 annotated training sentences and 512 gold sentences , for a total of 238 , 399 ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘  ( tag proportions : 14 . 5 % mechanism , 15 . 9 % purpose , 69 . 6 % other ) . A note on related annotated data . There has been recent work on the related topic of information extraction from scientific papers by classifying sentences , citations , or phrases . Recent supervised approaches [ 8 , 38 , 47 ] use annotations which are often provided by either paper authors themselves , NLP experts , domain experts , or involve elaborate ( multi - round ) annotation protocols . Sequence tagging models are often trained and evaluated on ( relatively ) clean , succinct sentences [ 49 , 66 ] . When trained on noisy texts , results typically suffer drastically [ 2 ] . Our corpus of product descriptions is significantly noisier than scientific papers , and our training annotations were collected in a scalable , low - cost manner by non - experts . Using noisy crowdsourced annotation for training and development only is consistent with our quest for a lightweight annotation approach that would still enable training useful models . In a closer domain than scientific texts , [ 43 ] classify product review sentences as containing a usage expression or not , over five products only . In contrast , this work focuses on extracting fine - grained purposes and mechanisms from a diverse range of products . Review texts are often written in fairly clean and coherent language , commonly appear in NLP tasks [ 61 ] , and do not typically describe in detail the mechanisms and purposes of products . In addition , sentence - level classification would not support the user interactions we explore in this paper , which require fine - grained control . 2 . 2 Extracting Spans After collecting annotations , we can now train models to extract the spans . We explore several models likely to have sufficient power to learn our proposed novel representation , with the goal of selecting the best performing one . In particular , we chose two approaches that are common for related sequence - tagging problems , such as named entity recognition ( NER ) and part - of - speech ( POS ) tagging : a common baseline and a recent state - of - the - art model . We also tried a model - enrichment approach with syntactic relational inputs . We stress that our goal in this section is to find a reasonable model whose output could support creative downstream tasks ; many other architectures are possible and could be considered in future work . â€¢ BiLSTM - CRF . A BiLSTM - CRF [ 37 ] neural network , a common baseline approach for NER tasks , enriched with semantic and syntactic input embeddings known to often boost performance [ 66 ] . We first pass the input sentence x = ( ğ‘¥ 1 , ğ‘¥ 2 , . . . , ğ‘¥ ğ‘‡ ) through an embedding module resulting in v 1 : ğ‘‡ , v ğ‘– âˆˆ R ğ‘‘ ğ‘’ , where ğ‘‘ ğ‘’ is the embedded space dimension . We adopt the â€œmulti - channelâ€ strategy as in [ 66 ] , concatenating input word embeddings ( pretrained GloVe vectors [ 56 ] ) with part - of - speech ( POS ) and NER embeddings . We additionally add an embedding corresponding to the incoming dependency relation . The sequence of token embeddings is then processed with a BiLSTM layer to obtain contextualized word representations h ( 0 ) 1 : ğ‘‡ , h ğ‘– âˆˆ R ğ‘‘ â„ , where ğ‘‘ â„ is the hidden state dimension . The outputs are fed into a 6 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Configuration P R F 1 Enriched BiLSTM 45 . 24 39 . 01 41 . 90 Pooled - Flair 53 . 30 39 . 80 45 . 50 GCN 47 . 85 47 . 93 47 . 89 GCN self - train 49 . 00 52 . 00 50 . 50 Table 1 . Raw extraction accuracy evaluation . All approaches use CRF loss . GCN with syntactic edges outperforms baselines . Self - training further improves results . Random - label achieves only 16 . 01 F 1 . linear layer ğ‘“ to obtain per - word tag scores ğ‘“ (cid:16) h ( ğ¿ ) 1 (cid:17) , ğ‘“ (cid:16) h ( ğ¿ ) 2 (cid:17) , . . . , ğ‘“ (cid:16) h ( ğ¿ ) ğ‘‡ (cid:17) . These are used as inputs to a conditional random field ( CRF ) model which maximizes the tag sequence log likelihood under a pairwise transition model between adjacent tags [ 5 ] . â€¢ Pooled Flair . A pre - trained language model [ 4 ] based on contextualized string embeddings , recently shown to outperform powerful approaches such as BERT [ 14 ] in NER and POS tagging tasks and achieve state - of - art results . Flair 1 uses a character - based language model pre - trained over large corpora , combined with a memory mechanism that dynamically aggregates embeddings of each unique string encountered during training and a pooling operation to distill a global word representation . We follow [ 4 ] and concatenate pre - trained GloVe vectors to token embeddings , add a CRF decoder , and freeze the language - model weights rather than fine - tune them [ 14 , 57 ] . â€¢ GCN . We also explore a model - enrichment approach with syntactic relational inputs . We employ a graph convolutional network ( GCN ) [ 39 ] over dependency - parse edges [ 66 ] . GCNs are known to be useful for propagating relational information and utilizing syntactic cues [ 49 , 66 ] . The linguistic cues are of special relevance and interest to us , as they are known to exist for purpose / mechanism mentions in texts [ 22 ] . We used a GCN with same token embeddings as in the BiLSTM - CRF baseline , with a BiLSTM layer for sequential context and a CRF decoder . For the graph fed into the GCN , we use a pre - computed syntactic edges with dependency parsing : For sentence x 1 : ğ‘‡ , we convert its dependency tree to A ğ‘ ğ‘¦ğ‘› where A ğ‘ ğ‘¦ğ‘›ğ‘–ğ‘— = 1 for any two tokens ğ‘¥ ğ‘– , ğ‘¥ ğ‘— connected by a dependency edge . We also add self - loops A ğ‘ ğ‘’ğ‘™ğ‘“ = ğ¼ ( to propagate from h ( ğ‘™ âˆ’ 1 ) ğ‘– to h ( ğ‘™ ) ğ‘– [ 66 ] ) . Following [ 66 ] , we normalize activations to reduce bias toward high - degree nodes . For an ğ¿ - layer GCN , denoting h ( ğ‘™ ) ğ‘– âˆˆ R ğ‘‘ â„ to be the ğ‘™ - th layer output node , the GCN operation can be written as â„ ( ğ‘™ ) ğ‘– = ğœ (cid:169)(cid:173) (cid:171) âˆ‘ï¸ ğ‘Ÿ âˆˆR ï£®ï£¯ï£¯ï£¯ï£¯ï£° ğ‘› âˆ‘ï¸ ğ‘— = 1 A ğ‘Ÿğ‘–ğ‘— W ( ğ‘™ ) ğ‘Ÿ â„ ( ğ‘™ âˆ’ 1 ) ğ‘— / ğ‘‘ ğ‘Ÿğ‘– + b ( ğ‘™ ) ğ‘Ÿ ï£¹ï£ºï£ºï£ºï£ºï£»(cid:170)(cid:174)(cid:172) where R = { syn , self } , ğœ is the ReLU activation function , W ( ğ‘™ ) ğ‘Ÿ is a linear transformation , b ( ğ‘™ ) ğ‘Ÿ is a bias term and ğ‘‘ ğ‘Ÿ ğ‘– = (cid:205) ğ‘‡ ğ‘— = 1 A ğ‘Ÿ ğ‘–ğ‘— is the degree of token ğ‘– w . r . t ğ‘Ÿ . In the GCN architecture , ğ¿ layers correspond to propagating information across ğ¿ - order neighborhoods . We set the contextualized word vectors h ( 0 ) 1 : ğ‘‡ to be the input to the GCN , and use h ( ğ¿ ) 1 : ğ‘‡ as the output word representations . Similarly to [ 49 ] , we do not model edge directions or dependency types in the GCN layers , to avoid over - parameterization in our data - scarce setting . We also attempted edge - wise gating [ 49 ] to mitigate noise propagation but did not see improvements , similarly to [ 66 ] . In our experiments , we followed standard GCN training procedures . Specifically , we base our model on the experimental setup detailed in [ 66 ] ( see also the authorsâ€™ code which we adapt for our architecture , at https : / / github . com / qipeng / gcn - over - pruned - trees ) . We pre - process the data using the spaCy ( https : / / spacy . io ) package for tokenization , dependency 1 https : / / github . com / flairNLP / flair 7 , , Hope et al . 0 10 20 30 40 50 60 70 80 90100 Top K ( % ) 40 45 50 55 60 65 70 75 P r e c i s i on mechanism purpose 40 45 50 55 60 65 70 75 Fig . 3 . Precision @ K results for the best performing model ( GCN + self - training ) . parsing , and POS / NER - tagging . We use pretrained GloVE embeddings of dimension 300 , and NER , POS and dependency relation embeddings of size 30 each , giving a total embedding dimension ğ‘‘ ğ‘’ = 390 . The bi - directional LSTM and GCN layersâ€™ hidden dimension is ğ‘‘ â„ = 200 , with 1 hidden layer for the LSTM . We find that the setting of 2 hidden layers works best for the GCNs . The semantic similarity threshold ğ¾ was tuned on the development sets , and was found to be 0 . 4 on Quirky and 0 . 3 for the patents data . We also tried training with edge label information based on syntactic relations , but found this hurts performance . The training itself was carried out using SGD with gradient clipping ( cutoff 5 ) for 100 epochs , selecting the best model on the development set . For the Pooled - Flair approach [ 4 ] , we use the FLAIR framework [ 3 ] , with the settings obtaining SOTA results for CONLL - 2003 as in [ 4 ] ( see https : / / github . com / flairNLP / flair / blob / master / resources / docs / EXPERIMENTS . md ) . We also experiment with non - pooled embeddings and obtain similar results . We experiment with initial learning rate and batch size settings described in [ 4 ] , finding 0 . 1 and 32 to work best , respectively . 2 . 3 Evaluation of Extraction Accuracy In this section we assess extraction accuracy ( whether we are able to extract purpose and mechanism spans of text ) . In the next sections , we evaluate the utility of the extracted spans for enabling creative innovation tasks . To evaluate raw accuracy of the modelâ€™s predictions , we use the standard IOB label markup to encode the purpose and mechanism spans ( 5 possible labels per token , { Beginning , Inside } x { Purpose , Mechanism } plus an " Outside " label ) . We conduct experiments using a train / development / test split of 18702 / 3614 / 512 . We pre - process the data using the spaCy package for tokenization , dependency parsing , and POS / NER - tagging . We use pretrained GloVE embeddings of dimension 300 , and NER , POS and dependency relation embeddings of size 30 each , giving a total embedding dimension ğ‘‘ ğ‘’ = 390 . The bi - directional LSTM and GCN layersâ€™ hidden dimension is ğ‘‘ â„ = 200 , with 1 hidden layer for the LSTM . We find that the setting of 2 hidden layers works best for the GCNs . The training itself was carried out using SGD with gradient clipping ( cutoff 5 ) for 100 epochs , selecting the best model and hyper - parameters based on the development set . For Flair , we experiment with initial learning rate and batch size settings described in [ 4 ] , finding 0 . 1 and 32 to work best , respectively . Due to our challenging setting , we train models on bronze - standard annotations with noisy and partial tagging done by non - experts ; for evaluation we use a curated gold - standard test set ( Section 2 ) . See Table 1 for results : GCN reaches an F 1 8 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Fig . 4 . Comparing our GCN model predictions ( right ) to human annotations ( left ) . Interestingly , our model managed to correct some errors made by the annotator ( e . g . , â€œitâ€™sâ€ , â€œheatedâ€ , â€œcoffee warmâ€ , â€œbeveragesâ€ ) . Purposes shown in pink , mechanisms in green . score of âˆ¼ 48 % , outperforming the BiLSTM - CRF model ( enriched with multi - channel GloVe , POS , NER and dependency relation embeddings ) by 6 % . GCN also surpasses the strong Pooled - Flair pre - trained language model by nearly 2 . 5 % . A random baseline guessing each token by label frequencies ( Section 2 ) achieves 16 . 01 F 1 . We interpret these results as possibly attesting to the utility of graph representations and features capturing syntactic and semantic information when labels are noisy . As a sanity check , we also computed precision @ K ( Figure 3 ) . As expected , precision is higher with low values of ğ¾ , and gradually degrades . Precision for mechanisms is higher than for purposes . Interestingly , a manual inspection revealed many cases where despite the noisy training setting , our models managed to correct mistaken or partial annotations ( see Figure 4 ) . Self - Training . According to the results , we chose GCN as our best - performing model . We experimented adding self - training [ 59 ] to GCN . Self - training is a common approach in semi - supervised learning where we iteratively re - label â€œOâ€ tags in training data with model predictions . A large portion of our training sentences are ( erroneously ) un - annotated by workers , perhaps due to annotation fatigue , introducing bias towards the â€œOâ€ label . Self - training with GCN shows an improvement in F 1 by an additional 2 . 6 % , substantially increasing recall ( more than 12 % over Flair ) , see Table 1 . Self - training stopped after 2 iterations , following no gain in F 1 on the development set . 3 CASE STUDY : FINE - GRAINED FUNCTIONAL SEARCH FOR ALTERNATIVE USES Our focus in this paper is to study the utility of the extracted purposes and mechanisms , in terms of the user interactions they enable . We explore two tasks demonstrating the value of our novel representation for supporting creative innovation . We start with a case study involving search for alternative uses . Our task is inspired by one of the most well - known divergent thinking tests [ 31 ] for measuring creative ability â€“ the alternative uses test [ 33 ] , where participants are asked to think of as many uses as possible for some object . Aside from serving as a measure of creativity , the ability to find alternative uses for technologies has important applications in engineering , science and industry . Technologies developed at NASA , the US space agency , have led to over 2 , 000 spinoffs , finding new uses in areas such as computer technology , agriculture , health , transportation , and even consumer products 2 . Procter & Gamble , the multinational consumer goods company , has invested millions of dollars in systematic search for ideas to re - purpose and adapt from other industries , such as using a compound that speeds up wound healing to treat wrinkles - an idea that led to a new line of anti - wrinkle products [ 18 ] . And very recently , the COVID - 19 pandemic 2 https : / / spinoff . nasa . gov / 9 , , Hope et al . provided a stark example of human innovation during times of crisis , with many companies actively seeking to pivot their business and re - purpose existing products to fit the new climate [ 19 ] . One teaching example is that of John Osher , creator of the popular â€œSpin Popâ€ â€“ a lollipop with a mechanism for twirling in your mouth . After selling his invention in the late 90â€™s , Osher and a group fellow inventors proactively searched for ideas â€“ â€œrather than having an idea come to usâ€ 3 . The group drew up a list of dozens of potential ideas , and eventually landed on the â€Spin Brushâ€ â€“ a cheap electric toothbrush adapted from the same mechanisms behind the twirling lollipop . This case of repurposing an existing technology is involved a systematic search process which required a rich , granular understanding of products and their designs rather than pure serendipity . However , Osher and his team still had to rely on human processing power â€“ inherently limited in its ability to scour millions of potential descriptions of problems available online , and find relevant and non - obvious candidate problems for which the twirling mechanism could be adapted . Introducing automation could help accelerate the search process , helping scale human ingenuity by sifting through millions of ideas for relevant inspirations . However , the task is challenging for existing search systems , because it requires a nuanced , multi - aspect understanding of both products and queries . Consider , for example , a company that manufactures some product ( e . g . , light bulbs ) . The company is familiar with straightforward usages of their products ( lamps , flashlights ) , and wants to identify non - standard uses and expand to new markets . Finding uses for a lightbulb that are not about the standard purpose of illuminating a space would be difficult to do with a standard search query over an idea repository . To come up with different applications of lights , one may turn to the Web to collect examples . However , it quickly turns out to be a non - trivial task , as the term â€œlightsâ€ or â€œlightingâ€ will bring back lots of results close to â€œlamps , â€ â€œflashlight , â€ and the like . The result of a quick Google search is also inundated with Christmas lights or light bulbs ( not to mention light in the sense of â€œlightweightâ€ ) . What one might want instead is finding a diverse set of applications other than just building floor lamps or decorative lights . In contrast , using our representation , each idea in the repository is associated with mechanism spans and purpose spans , and one could form a query such as mechanism = â€œlight bulbâ€ , purpose = NOT â€œlightâ€ . Using our system , the searcher adds â€œlightâ€ as a mechanism and also add â€œlightâ€ as a negative purpose ( i . e . , results should not include â€œlightâ€ purpose ) . Our engine returns interesting examples such as billiard laser instructor devices ( Table 1 , warning signs on food packages to get attention of kids with allergies and lights attached to furniture to protect your pinky toes at night ( Fig . 5 bottom ) . 3 . 1 Study Design We have built a prototype search engine supporting our representation . Figure 5 shows the top two results for the light bulb scenario : warning lights on food for kids with allergies , and lights attached to furniture to protect your pinky toe at night . These are non - standard recombinations [ 21 ] ( light + allergies , light + furniture guard ) that could lead the company to new markets . We conduct an experiment simulating scenarios where users wish to find novel / uncommon uses of mechanisms . Table 2 shows the scenarios and examples . To choose these scenarios for the experiment , we find popular / common mechanisms in the dataset and their most typical uses . For example , one frequent mechanism is RFID , which is typically used for purposes such as â€œlocatingâ€ and â€œtrackingâ€ . We then create queries searching for different uses â€“ purposes that do not include concepts related to the typical uses of a given mechanism . 4 We now describe the methods we use to retrieve results for the scenarios . 3 https : / / www . allbusiness . com / the - man - the - legend - john - osher - inventor - of - the - spin - brush - part - i - 2 - 7665547 - 1 . html 4 To automate scenario selection , we cluster mechanisms ( see Section 4 . 1 for details ) , select frequent mechanisms from the top 5 largest mechanism clusters , and identify purposes strongly co - occurring with them ( e . g . , â€œRFIDâ€ co - occurs with â€œlocatingâ€ , â€œtrackingâ€ ) to avoid . 10 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Fig . 5 . Applications for light where light is not in the purpose . Two of the results and their automatic annotations ( purposes in pink , mechanisms in green ) . 3 . 1 . 1 Our Approach . We represent each product ğ‘– as a set of purpose vectors P ğ‘– (cid:66) { p 1 ğ‘– , p 2 ğ‘– , . . . , p ğ‘ƒ ğ‘– ğ‘– } , and a set of mechanism vectors M ğ‘– (cid:66) { p 1 ğ‘– , p 2 ğ‘– , . . . , p ğ‘€ ğ‘– ğ‘– } extracted with our GCN model . Similarly , we define a set of query vectors q ğ‘ (cid:66) q 1 , q 2 , . . . q ğ‘„ ğ‘ and q ğ‘š (cid:66) q 1 , q 2 , . . . q ğ‘„ ğ‘š . Each query chunk can be negated , meaning it should not appear . Finally , we define distance metrics ğ‘‘ ğ‘ ( Â· , Â· ) , ğ‘‘ ğ‘š ( Â· , Â· ) between sets of purposes and mechanisms . For example , to locate a dog using RFID but not GPS : argmin ğ‘– ğ‘‘ ğ‘ ( { q â€œlocate dogâ€ } , P Ëœ ğ‘– ) ğ‘  . ğ‘¡ . ğ‘‘ ğ‘š ( { q â€œGPSâ€ } , M Ëœ ğ‘– ) â‰¥ threshold ğ‘‘ ğ‘š ( { q â€œRFIDâ€ } , M Ëœ ğ‘– ) â‰¤ threshold ( 1 ) We explore two alternatives for computing distance metrics ğ‘‘ ğ‘š , ğ‘‘ ğ‘ : â€¢ FineGrained - AVG . ğ‘‘ ğ‘ ( q ğ‘ , P ğ‘– ) is 1 minus the dot product between average query and purpose vectors ( normalized to unit norm ) . We define ğ‘‘ ğ‘š similarly . â€¢ FineGrained - MAXMIN . We match each element in q ğ‘ with its nearest neighbor in P ğ‘– , and then find the minimum over the distances between matches . ğ‘‘ ğ‘ is defined as 1 minus the minimum . All vectors are normalized . We define ğ‘‘ ğ‘š 11 , , Hope et al . Query Example results Mechanism : light . Purpose : NOT light Billiard laser instructor ( projector ) Mechanism : solar energy . Purpose : NOT generating power Light bulbs with built - in solar chips . Mechanism : water . Purpose : NOT cleaning , NOT drinking A lighter that burns hydrogen generated from water and sunlight . Mechanism : RFID . Purpose : NOT locating , NOT tracking A digital lock for your luggage with RFID access . Mechanism : light . Purpose : cleaning A UV box to clean and sanitize barbells at the gym . Table 2 . Scenarios and example results retrieved by our FineGrained - AVG method . All queries reflect non - trivial uses of mechanisms ( e . g . , a query for using water not for drinking / cleaning , retrieves a lighter running on hydrogen from water and sunlight ) . Fig . 6 . Results for search evaluation test case . Mean average precision ( MAP ) and Normalized Discounted Cumulative Gain ( NDCG ) by method , averaged across queries . Methods in bold use our model . similarly . This captures cases where queries match only a small subset of product chunks , erring on the side of caution with a max - min approach . 3 . 1 . 2 Baselines . We test our model against : â€¢ AvgGloVe . A weighted average of GloVe vectors of the entire text ( excluding stop words ) , similar to standard NLP approaches for retrieval and textual similarity . We average query terms and normalize to unit norm . Distance is computed via the dot product . â€¢ Aggregate purpose / mechanism . Representing each document with the model in [ 36 ] , using a BiLSTM neural network taking as input raw text and producing two vectors corresponding to aggregate purpose and mechanism . We average and normalize query vectors , and use the dot product . For all four methods , we handle negative purpose queries by filtering out all products whose distance is greater than ğœ† , where lambda is a threshold selected to be the 90 th percentile of distances . 12 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , 3 . 2 Results We recruited five engineering students to judge the retrieved product ideas . Each participant provided binary relevance ranking to the top 20 results from each of the four methods , shuffled randomly so that judges are blind to the condition 5 . See Figure 6 for results . We report Non Cummulative Discounted Gain ( NDCG ) and Mean Average Precision ( MAP ) , two common metrics in information retrieval [ 60 ] . Our FineGrained - AVG wins for both metrics , followed by FineGrained - MAXMIN . The baselines perform much worse , with the aggregate - vectors approach in [ 36 ] outperforming standard embedding - based retrieval with GloVe . Importantly , our approach achieves high MAP ( 85 % - 87 % ) in absolute terms , in addition to a large relative improvement over the baselines ( MAP of 40 % - 60 % ) . Table 2 shows example results of FineGrained - AVG . For instance , a query for using light not for lighting results in laser - based billiard instructions . A query for using RFID not for locating or tracking results in an idea for an RFID - based lock , or RFIDs used at supermarket checkouts . Looking at examples of retrieved results demonstrates the benefit of our approach . For instance , with the query for using light for the non - standard purpose of cleaning , the top ranked result retrieved by FineGrained - AVG is a UV Light Sterilizer , with purposes that include Sterilizes bacteria , Keep public and people healthy and Cleaner fresher air , and the top result from FineGrained - MAXMIN is similarly a Standalone bug zapper bulb that uses uv light / black light . Conversely , the top result for both baselines ( stan - dard search and aggregate - vectors ) is a Toilet / Bathroom Light , with a sensor light that glows around your toilet and has extra batteries if you lose electricity in the bathroom . It appears that both baselines were not able to accurately capture and disentangle purposes and mechanisms , despite the aggregate - vector being explicitly designed for that . The aggregate - vector approach squashes multiple purposes together by design into one soft , aggregate vector , which in this case includes concepts like toilet and bathroom that are somewhat topically related to cleaning . The aggregate approach had similar issues with accuracy in the next three product idea it retrieved ( Switch that glows in the dark , a Dash Light to illuminate your ash tray , and a light strip to change your water color ) . Only the fifth result ( out of the top five ) was closer to being related to the query ( a LED lamp designed to look like a window that can keep air odorless with an electrostatic air purifier ) , yet not precisely capturing the purpose of cleaning â€“ due to squashing together multiple concepts in one soft average ( this product was also ranked as the top fourth result by the standard search baseline ) . In contrast , the fifth result found by FineGrained - MAXMIN was a die grinder with a light to see inside when cleaning / fixing root welds inside steel pipe . As another example , for the query of using RFID not for locating or tracking , the top result with both FineGrained - AVG and FineGrained - MAXMIN is a walk through checkout scanner that uses RFID , a product not cap - tured by the two other baselines in their top five results . The first - ranked result found by the aggregate - vector base - line approach was a customizable luggage system with RFID protection ( also the the second result retrieved by FineGrained - AVG ) but it also retrieved products such as a wifi enabled chip for kids and pets that allows them to go in or out without tripping the alarm , and a case with laser and bluetooth to connect to smart devices , that are of weaker relatedness to RFID technology . Overall , our results demonstrate that fine - grained purposes and mechanisms lead to better functional search expressivity than approaches based on distributional representations or coarse purpose - mechanism vectors . 5 Inter - rater agreement measured across all scenarios was at 50 % by both Fleiss kappa and Krippendorffâ€™s alpha tests . 13 , , Hope et al . 4 EXPLORING THE DESIGN SPACE WITH A FUNCTIONAL CONCEPT GRAPH In this section we test the value of our novel representation for supporting users in exploring the design space for solving a given problem . We use our span - based representation to construct a corpus - wide graph of purpose / mechanism concepts . We demonstrate the utility of this approach in an ideation task , helping users identify useful inspirations in the form of problems that are related to their own . Our goal is to help users â€œbreak outâ€ of fixation on a certain domain , a well - known hindrance to innovation [ 11 , 40 ] . Doing so is challenging because it requires some level of abstraction : being able to go beyond the details of a concrete problem to connect to a part of the design space that may look dissimilar on the surface , but has abstract similarity . Numerous studies in engineering and cognitive psychology have shown the benefits of problem abstractions for ideation [ 22 , 24 , 30 , 40 , 46 , 63 , 64 ] . However , these studies either involve non - scalable methods ( relying on highly - structured annotations , or on crowd - sourcing ) or simple , syntactical pattern - matching heuristics incapable of capturing deeper abstract relations . In the work closest to ours [ 36 ] , crowd workers were given a product description from the Quirky database , and asked to come up with ideas for products that solve the same problem in a different way . Soft aggregate vectors representing purposes and mechanisms were used to find near - purpose , far - mechanism analogies . Thus , the ability to find analogs was limited by relying on having a given mechanism to control for structural distance . Unlike [ 36 ] , in our setup we assume a realistic scenario where we are given only a very short problem title â€“ e . g . , generating power for a phone , reminding to take medicine , folding laundry â€“ and aim to find inspirational stimuli [ 30 ] in the â€œsweet spotâ€ for creative ideation â€“ structurally related to the given problem , not too near yet also not too far [ 23 ] . To address this challenge , in this section we build a tool inspired by functional modeling , which we call a Functional Concept Graph . A functional model [ 35 ] is , roughly put , a hierarchical ontology of functions and ways to achieve them , and is a key concept in engineering design . Such models are especially useful for innovation , allowing problem - solvers to â€œbreak outâ€ of a fixed overly - concrete purpose or mechanism and move up and down the hierarchy . Despite their great potential , todayâ€™s functional models are constructed manually , and thus do not scale . We thus construct a ( crude ) approximation of a functional representation that would still be useful for exploring the design space and suggesting potentially useful inspirations to users . In our approach , Functional Concept Graphs consist of nodes corresponding to purposes or mechanisms , and edges encoding semantic ( not necessarily hierarchical ) relations . Our span - based representation enables us to build this graph ( Figure 7 ) â€“ products that mention certain purposes ( e . g . , " charge your phone " ) will often mention other , structurally related problems that could be more general / abstract ( e . g . , " generate power " ) or more specific ( " wireless phone charging " ) . These relations between purposes and mechanisms could help find connections across ideas at different levels of abstraction . Our approach allows us to look at fine - grained co - occurrences of concepts appearing together in products and thus infer relations between them , unlike the coarse representation in [ 36 ] that represented entire products with one aggregate purpose / mechanism vector which could not reveal important granular information needed for constructing such a graph . In other words , we can discover patterns in the form of products that solve problem ğ‘ ğ‘– , also often solve problems I , and suggest I as potential inspirations to be recommended 6 . However , naively looking for co - occurrences of problems may yield I too near to the original ğ‘ ğ‘– , as many frequently co - occurring purposes tend to be very similar , while we are interested discovering the more abstract relations . In addition , raw chunks of text extracted from our tagging model have 6 This also bears certain resemblance to collaborative filtering [ 41 ] , where recommendations are based on the pattern : people who buy item X , also often buy Y ; in our case , instead of people and items , we have ideas written by people , and the problems they solve . 14 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Fig . 7 . An example of our learned functional concept graph extracted from texts . Mechanism in green , purpose in pink . Titles are tags nearest to cluster centroids ( redacted to fit ) . countless variants that are not sufficiently abstract and are thus sparsely co - occurring . We thus design our approach to encourage abstract inspirations , as we describe next . 4 . 1 Building a Functional Concept Graph We develop a method to infer this representation from co - occurrence patterns of the fine - grained spans of text . We take the following two steps ( see more details in the next section ) : I . Concept discretization . Intuitively , nodes in our graph should correspond to groups of related spans ( â€œchargingâ€ , â€œcharging the batteryâ€ , â€œcharging a laptopâ€ ) . To achieve this , we take all purpose and mechanism spans Ë† P , Ë† M in the corpus , extracted using our GCN model , and cluster them ( separately ) , using pre - trained vector representations . We refer to the clusters C ğ‘ , C ğ‘š as concepts . II . Relations . We employ rule - mining [ 55 ] to discover a set of relations R between concepts . Relations are Antecedent = â‡’ Consequent , with weights corresponding to rule confidence . To illustrate our intuition , suppose that when â€œprevent head injuryâ€ appears in a product description , the conditional probability of â€œsafetyâ€ appearing too is large ( but not the other way around ) . In this case , we can ( weakly ) infer that preventing head injuries is a sub - purpose of â€œsafetyâ€ . Indeed , manually observing the purpose - purpose edges , the one - directional relations captured are often sub - purpose , and the bi - directional ones often encode abstract similarity . Similarly , for mechanism concepts the one - directional relations are often part of ( â€œcell phoneâ€ and â€œbatteryâ€ ) , and bi - directional are mechanisms that co - occur often . For pairs of purpose and mechanism concepts , the relation is often functionality ( â€œchargerâ€ , â€œchargeâ€ ) . Example . Figure 7 shows a subgraph from our automatically constructed functional concept graph ( showing only high - confidence edges ) . Pink nodes correspond to purposes and green nodes to mechanisms . The figure shows a part of the graph related to electricity , power and charging . A designer could go from the problem of charging batteries to the more general problem of generating power , and from there to another branch ( e . g . , solar power and mechanical stored energy ) , to get inspired by structurally related ideas . 15 , , Hope et al . Fig . 8 . A snippet from our ideation interface for â€œmorning medicine reminderâ€ . Users indicate which inspirations were useful , and what they inspired . For example , seeing â€œreal time health checkerâ€ inspired one user to suggest a monitoring device for finding the best time for reminding to take the medicine . 4 . 2 Study Design Next , we set out to test the utility of the functional concept graph , based on our nuanced representation , in an ideation task . In our setup we gave participants problems ( e . g . , reminding people to take their medication ) and asked them to think of creative solutions . Participants were also given a list of potential inspirations , and were instructed to mark whether each was novel and helpful . They were encouraged to explain the solution it inspired . See example in Figure 8 : Seeing â€œreal time health checkerâ€ inspired one user to suggest monitoring the person to find the best time to remind them to take medicine . To create a set of seed problems , a graduate student mapped between problems from WikiHow . com ( a website of how - to guides ) to purposes in our data . Using this source allowed us to collect real - world problems that are broadly familiar , with succinct and self - explanatory titles that do not require further reading to understand . The student was tasked with confirming that our Quirky dataset contains idea descriptions that mention these problems . For a given problem in WikiHow ( how to remember to take medication ) , they performed keyword search over 17 ğ¾ purpose spans gleaned by our model from Quirky , and found matching spans ( morning medicine reminder ) . We use those matching spans as our seed problem description given to users ( purple text in Figure 8 ) . We collect 25 problems this way . Table 3 shows more examples , such as Tracking distance walked , folding laundry or sensing dryness level . Inspirations are other purpose spans from our dataset ( see Table 3 ) , selected automatically using our approach and comparing to baselines . For our approach , we explore two common and powerful vector representations of spans , one based pre - trained word embeddings [ 56 ] and the other on a more recent language model representation tuned to capture semantic similarity [ 58 ] . Our method is based on clustering related purpose spans into concept nodes in the functional graph ; some of these nodes contain tens of spans in them . Thus , we also explore two approaches to â€œsummarizeâ€ each concept cluster with representative spans displayed to users . In more concrete detail , we experiment with the following approaches for selecting inspirations : 4 . 2 . 1 Our Method . We experiment with the following two span representations for building a functional concept graph : 16 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Problem Inspirations Rater explanation Track distance walked Protect children Get ideas from devices that keep track of children Folding laundry Store toilet paper Roll laundry around a tube instead of folding Dispense medicine Pet bowl that keeps ants away Based on pet bowls that can dispense food during the day Sense dryness level Voltage reading Use electric current to measure water level ( safely ) Waterproof Ideas from sensors in waterproof devices Temperature reading Morning medicine reminder Schedule coffee , coffee alarm Alarm clock with coffee and medicine reminders Send vital data , real - time health checker Health trackers to tell if medicine not taken , alert accordingly Heart rate monitoring , continuously monitor glucose Find the best time to take medicine Table 3 . Example inspirations and explanations given by human evaluators . â€¢ GloVe pre - trained word embeddings , averaged across tokens . â€¢ BERT - based contextualized vectors that have been fine - tuned for semantic similarity tasks 7 [ 58 ] . Each representation is used to cluster the spans . We cluster the spans using K - Means + + 8 [ 7 ] . We then apply the Apriori algorithm 9 to automatically mine association rules between clusters , [ 55 ] and use the confidence metric to select the top rules 10 . To use the mined rules between purpose nodes ( clusters ) for selecting inspirations shown to users , we start from the purpose node corresponding to the given problem and take its consequents ; as explained earlier , this captures a weak signal of abstract similarity . We experiment with two approaches for displaying concepts to users â€“ one that attempts to summarize the cluster independently of the seed problem , and one that takes the seed problem into account : â€¢ TextRank [ 50 ] . We construct a graph where nodes are the spans in a cluster and edges represent textual similarity . We run PageRank [ 54 ] on this graph , selecting the top ğ¾ spans to present . â€¢ Nearest spans . Following the findings in [ 23 ] , select the top ğ¾ spans in C ğ‘ that are nearest to the query ğ‘ ğ‘– . ( For both approaches , we use ğ¾ = 5 ) . 4 . 2 . 2 Baselines . â€¢ Purpose span similarity . Given a problem ğ‘ ğ‘– , we find the ğ¾ = 5 nearest purpose spans of text in our corpus ( out of 17 ğ¾ purposes ) . We experiment with the same two vector representations used by our approach : GloVe and BERT . This method is similar to applying the methodology in [ 36 ] to our setting , where in our setting we are given only a problem ğ‘ ğ‘– and no mechanism ğ‘š ğ‘– is available to control for structural distance . While this approach relies on our model for extracting purpose spans , we consider it a baseline to study the added value of our hierarchy . â€¢ Linguistic abstraction . We use the WordNet [ 52 ] lexical database to extract hypernyms ( for each token in ğ‘ ğ‘– ) , in order to capture potential abstractions . WordNet is often used in similar fashion for design - by - analogy studies [ 30 , 46 ] . 7 We use RoBERTa - large - STS - SNLI , available at github . com / UKPLab / sentence - transformers . 8 ğ¾ = 250 selected automatically with elbow - based criteria on silhouette scores . 9 http : / / www . borgelt . net / pyfim . html . 10 We use the top 3 rules in our experiment . 17 , , Hope et al . Alert / remind Making hot drinks Medicine delivery Medical monitoring Coffee machine alarm Smart medicine injector Smart medicine injector pill reminder Smart medicine injector Fig . 9 . Example from our Functional Concept Graph , explaining the inspirations shown to users in Figure 8 . Nodes represent concepts ( clusters of purposes ) , named by us for readability . Edges are annotated with products containing spans from both concepts . The problem of â€œmedicine morning reminderâ€ is mapped ( via embedding ) to the Alert / remind concept , which is linked to the concepts of medical monitoring and making hot drinks through products such as â€œsmart medicine injectorâ€ and â€œcoffee machine alarmâ€ ( among others , not displayed in the figure ) . These links serve as inspirations in our study . â€¢ Random concepts . Random inspirations are often considered as a baseline in ideation studies since diversity of examples is a known booster for creative ability [ 36 ] . For each task , we select a random cluster from C ğ‘ and display its TextRank summary . 4 . 2 . 3 Rating Collection . In our study , each method generated ğ¾ = 5 spans ( concept summaries ) , which are grouped and displayed together in a box ( Figure 8 ) . For each problem a rater views 8 boxes in randomized order , to avoid bias . We recruit 10 raters ( 8 graduate students , a senior engineering professor , and an architect ) . Raters were instructed to mark inspirations they consider useful and relevant for solving a given problem , while being not about the same problem . Raters were also encouraged to write comments , especially for non - trivial cases which they found of interest ( see Table 3 ) . In total , raters viewed 2584 boxes , or 12920 purpose descriptors . 4 . 3 Results Qualitative analysis . Table 3 and Figure 8 show examples of problems , inspirations and user explanations from our study . For instance , users facing the â€œmorning medicine reminderâ€ problem were presented with nearby concepts in the Functional Concept Graph that included health monitoring and coffee machines . To explore why these concepts are connected in our graph and why they are potentially useful as inspirations , we make use of the direct interpretability of our approach . We examine the purpose co - occurrences from which the Functional Concept Graph was constructed . Figure 9 shows the a graph with concept nodes of Making hot drinks , alerting / reminding , health monitoring , medicine delivery , and edges representing products in which two adjacent purposes were co - mentioned ( e . g . , a coffee machine alarm product that mentioned the purposes of making hot drinks and alerting / reminding , or a " smart medicine injector " that mentioned both alerting / reminding and medicine delivery ) . This explains why the concepts are nearby in the graph , as there are multiple products in our dataset that refer to purposes from both concepts . 18 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , Fig . 10 . Inspiration user study results . Left : Proportion of inspirations selected by at least 2 raters , per condition . Right : Proportion of boxes ( clusters ) with at least 2 spans marked by â‰¥ 2 raters . For example , a pill reminder product refers to the problem of forgetting to take medicine at prescribed times ( Sends notification if you forgot to take your AM or PM meds ) , while a smart injector device administers medicine on set time intervals . At the same time , both of these products of course mention purposes of medicine delivery . When our graph construction algorithm observes enough similar co - occurrence patterns between the concepts of alerting and medicine delivery , across multiple products , an edge is added between the two in the graph . Similarly , an Alarm coffee maker product mentions the purposes of time management and making coffee at a set time as well as alerting when the coffee is ready , explaining how it emerges as a potential inspiration in our graph . This type of linkage or overlap between an original problem space and inspiration problems helps get at a sweet - spot of innovation [ 12 ] by finding ideas that are not too near and not too far from the original problem , helping users break out of fixation as discussed earlier in this section . Users used these inspirations to come up with a tracker that alerts the user at the best time to take a medicine , and a coffee machine reminding the user to take their medication with their morning coffee . Those creative directions demonstrate the utility of the Functional Concept Graph for exploring the design space . Quantitative results . Figure 10 shows the results of the user study . On the left , we show the proportion of inspirations ( individual spans ) selected by at least two raters , for each method . Our approach significantly outperforms all the baselines . The effect is particularly pronounced for the BERT - based approach , with 51 % of inspirations found useful , while the best baseline reaches less than 30 % . Interestingly , for both BERT and GloVe representations , the Nearest - span summarization approach fares better , potentially due to striking a balance between being too far / near the initial problem ğ‘ ğ‘– . Figure 10 ( right ) shows the proportion of inspiration boxes that got at least 2 individual inspirations marked ( by at least 2 raters ) . This metric measures the effect of a box as one unit , as each box is meant to represent a coherent cluster . Our method is able to reach 62 % , while the best baseline ( GloVe search on purpose spans ) yields only 39 % . Again , the nearest - span summarization is prefered to TextRank . Importantly , for both individual inspiration spans and inspirations boxes , 51 % - 62 % are rated as useful â€“ high figures considering the challenging nature of the task . 19 , , Hope et al . 5 CONCLUSION In this paper we introduced a novel span - based representation of ideas in terms of their fine - grained purposes and mechanisms and used it to develop new tools for creative ideation . We trained a model to extract spans from a noisy , real - world corpus of products . We used this representation to help search for alternative , uncommon uses of products and to generate a graph capturing abstract similarities in idea repositories to help problem - solvers explore the design space around their problem . In both ideation studies , we were able to achieve high accuracies , significantly outperform baselines and help boost user creativity . In future work , we would like to further explore weak supervision approaches to augment annotation in noisy settings . Another direction is learning purposes and mechanisms in an end - to - end fashion . Another exciting prospect is deploying our search engine publicly , allowing scientists , engineers and designers to perform rich queries , discover new similarities , and boost innovation with enhanced capabilities not possible with todayâ€™s search . Beyond supporting richer search for creative inspiration , a data - driven approach to extracting functional facets and learning abstractive relationships between the facets could power much more expansive approaches to mapping out design spaces for entire domains or problem areas , identifying key subproblems and constraints and novel paths through the design space . Mapping approaches like this , such as technological roadmapping [ 9 ] , have already shown significant promise for reinvigorating research and development in real - world applications such as neural recording [ 48 ] . However , these mapping exercises are still highly manual and labor - intensive processes ; computational support for such tasks could have transformative impacts on innovation . REFERENCES [ 1 ] The car mechanic who uncorked a childbirth revolution . BBC News , 2013 . [ 2 ] G . Aguilar , S . Maharjan , A . P . L . Monroy , and T . Solorio . A multi - task approach for named entity recognition in social media data . In 3rd Workshop on Noisy User - generated Text , 2017 . [ 3 ] A . Akbik , T . Bergmann , D . Blythe , K . Rasul , S . Schweter , and R . Vollgraf . Flair : An easy - to - use framework for state - of - the - art nlp . In NAACL - HLT , 2019 . [ 4 ] A . Akbik , T . Bergmann , and R . Vollgraf . Pooled contextualized embeddings for named entity recognition . In NAACL , 2019 . [ 5 ] A . Akbik , D . Blythe , and R . Vollgraf . Contextual string embeddings for sequence labeling . In International Conference on Computational Linguistics , 2018 . [ 6 ] G . Altshuller . 40 principles : TRIZ keys to innovation . 2002 . [ 7 ] D . Arthur and S . Vassilvitskii . k - means + + : The advantages of careful seeding . In ACM - SIAM symposium on Discrete algorithms , 2007 . [ 8 ] I . Augenstein , M . Das , S . Riedel , L . Vikraman , and A . McCallum . Semeval 2017 task 10 : Scienceie - extracting keyphrases and relations from scientific publications . arXiv preprint arXiv : 1704 . 02853 , 2017 . [ 9 ] E . S . Boyden and A . H . Marblestone . Architecting Discovery : A Model for How Engineers Can Help Invent Tools for Neuroscience . Neuron , 102 ( 3 ) : 523 â€“ 525 , May 2019 . Publisher : Elsevier . [ 10 ] J . Chan , J . Chang , T . Hope , D . Shahaf , and A . Kittur . Solvent : A mixed initiative system for finding analogies between research papers . CSCW , 2018 . [ 11 ] J . Chan , S . P . Dow , and C . D . Schunn . Do The Best Design Ideas ( Really ) Come From Conceptually Distant Sources Of Inspiration ? Design Studies , 2015 . [ 12 ] J . Chan , K . Fu , C . Schunn , J . Cagan , K . Wood , and K . Kotovsky . On the benefits and pitfalls of analogies for innovative design : Ideation performance based on analogical distance , commonness , and modality of examples . Journal of mechanical design , 2011 . [ 13 ] J . Chan , T . Hope , D . Shahaf , and A . Kittur . Scaling up analogy with crowdsourcing and machine learning . In ICCBR - 16 . [ 14 ] J . Devlin , M . - W . Chang , K . Lee , and K . Toutanova . Bert : Pre - training of deep bidirectional transformers for language understanding . In NAACL - HLT , 2019 . [ 15 ] K . Dorst . The core of â€œdesign thinkingâ€ and its application . Design Studies , 2011 . [ 16 ] H . Dubberly and S . Evenson . On Modeling : The Analysis - synthesis Bridge Model . interactions , 2008 . [ 17 ] J . R . Duflou and P . - A . Verhaegen . Systematic innovation through patent based product aspect analysis . CIRP Annals - Manufacturing Technology , 2011 . [ 18 ] K . Essick . Technology scouts : hoping to find the next big thing . Science Business , Feb . 2006 . [ 19 ] K . Essick . Innovation and creativity in a time of crisis . Science Business , 2020 . 20 Scaling Creative Inspiration with Fine - Grained Functional Facets of Product Ideas , , [ 20 ] M . FÃ¤rber , F . Bartscherer , C . Menne , and A . Rettinger . Linked data quality of dbpedia , freebase , opencyc , wikidata , and yago . Semantic Web , 2018 . [ 21 ] L . Fleming . Recombinant uncertainty in technological search . Management science , 47 ( 1 ) : 117 â€“ 132 , 2001 . [ 22 ] K . Fu , J . Cagan , K . Kotovsky , and K . L . Wood . Discovering Structure In Design Databases Through Functional And Surface Based Mapping . JMD , 2013 . [ 23 ] K . Fu , J . Chan , J . Cagan , K . Kotovsky , C . Schunn , and K . Wood . The Meaning of Near and Far : The Impact of Structuring Design Databases and the Effect of Distance of Analogy on Design Output . JMD , 2013 . [ 24 ] K . Fu , J . Chan , C . Schunn , J . Cagan , and K . Kotovsky . Expert representation of design repository space : A comparison to and validation of algorithmic output . Design Studies , 2013 . [ 25 ] D . Gentner and K . J . Kurtz . Relational Categories . In Categorization inside and outside the laboratory : Essays in honor of Douglas L . Medin , APA decade of behavior series . American Psychological Association , Washington , DC , US , 2005 . [ 26 ] D . Gentner and A . B . Markman . Structure mapping in analogy and similarity . American psychologist , 1997 . [ 27 ] K . Gericke and B . Eisenbart . The integrated function modeling framework and its relation to function structures . AI EDAM , 2017 . [ 28 ] M . L . Gick and K . J . Holyoak . Analogical problem solving . Cognitive psychology , 12 ( 3 ) : 306 â€“ 355 , 1980 . [ 29 ] K . Gilon , J . Chan , F . Y . Ng , H . Lifshitz - Assaf , A . Kittur , and D . Shahaf . Analogy mining for specific design needs . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI â€™18 , pages 121 : 1 â€“ 121 : 11 . ACM , 2018 . [ 30 ] K . Goucher - Lambert and J . Cagan . Crowdsourcing inspiration : Using crowd generated inspirational stimuli to support designer ideation . Design Studies , 2019 . [ 31 ] J . P . Guilford . Three faces of intellect . American psychologist , 1959 . [ 32 ] J . P . Guilford . The nature of human intelligence . McGraw - Hill , New York , NY , 1967 . [ 33 ] J . P . Guilford . The nature of human intelligence . 1967 . [ 34 ] G . S . Halford , R . Baker , J . E . McCredden , and J . D . Bain . How many variables can humans process ? Psychological science , 2005 . [ 35 ] J . Hirtz , R . Stone , D . A . McAdams , S . Szykman , and K . Wood . A functional basis for engineering design : reconciling and evolving previous efforts . Research in engineering Design , 2002 . [ 36 ] T . Hope , J . Chan , A . Kittur , and D . Shahaf . Accelerating innovation through analogy mining . In KDD , 2017 . [ 37 ] Z . Huang , W . Xu , and K . Yu . Bidirectional lstm - crf models for sequence tagging . arXiv preprint arXiv : 1508 . 01991 , 2015 . [ 38 ] D . Jin and P . Szolovits . Hierarchical neural networks for sequential sentence classification in medical scientific abstracts . arXiv preprint arXiv : 1808 . 06161 , 2018 . [ 39 ] T . N . Kipf and M . Welling . Semi - Supervised Classification with Graph Convolutional Networks . sep 2016 . [ 40 ] A . Kittur , L . Yu , T . Hope , J . Chan , H . Lifshitz - Assaf , K . Gilon , F . Ng , R . E . Kraut , and D . Shahaf . Scaling up analogical innovation with crowds and ai . PNAS , 2019 . [ 41 ] Y . Koren and R . Bell . Advances in collaborative filtering . In Recommender systems handbook , pages 77 â€“ 118 . Springer , 2015 . [ 42 ] T . Kuribayashi , H . Ouchi , N . Inoue , P . Reisert , T . Miyoshi , J . Suzuki , and K . Inui . An empirical study of span representations in argumentation structure parsing . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4691 â€“ 4698 , Florence , Italy , July 2019 . Association for Computational Linguistics . [ 43 ] S . Lahiri , V . V . Vydiswaran , and R . Mihalcea . Identifying usage expression sentences in consumer product reviews . In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 394 â€“ 403 , 2017 . [ 44 ] D . B . Lenat . Cyc : a large - scale investment in knowledge infrastructure . In Communications of the ACM , 1995 . [ 45 ] D . B . Lenat and R . V . Guha . Building large knowledge - based systems ; representation and inference in the Cyc project . Addison - Wesley Longman Publishing Co . , Inc . , 1989 . [ 46 ] J . Linsey , A . Markman , and K . Wood . Design by analogy : a study of the wordtree method for problem re - representation . JMD , 2012 . [ 47 ] Y . Luan , L . He , M . Ostendorf , and H . Hajishirzi . Multi - task identification of entities , relations , and coreferencefor scientific knowledge graph construction . In Proc . Conf . Empirical Methods Natural Language Process . ( EMNLP ) , 2018 . [ 48 ] A . H . Marblestone , B . M . Zamft , Y . G . Maguire , M . G . Shapiro , T . R . Cybulski , J . I . Glaser , D . Amodei , P . B . Stranges , R . Kalhor , D . A . Dalrymple , D . Seo , E . Alon , M . M . Maharbiz , J . M . Carmena , J . M . Rabaey , E . S . Boyden , G . M . Church , and K . P . Kording . Physical Principles for Scalable Neural Recording . Frontiers in Computational Neuroscience , 7 , 2013 . arXiv : 1306 . 5709 . [ 49 ] D . Marcheggiani and I . Titov . Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling . 1 , 2017 . [ 50 ] R . Mihalcea and P . Tarau . Textrank : Bringing order into text . In EMNLP , 2004 . [ 51 ] G . A . Miller . WordNet : a lexical database for English . Communications of the ACM , 38 ( 11 ) : 39 â€“ 41 , 1995 . [ 52 ] G . A . Miller . Wordnet : a lexical database for english . Communications of the ACM , 1995 . [ 53 ] T . Mitchell , W . Cohen , E . Hruschka , P . Talukdar , B . Yang , J . Betteridge , A . Carlson , B . Dalvi , M . Gardner , B . Kisiel , et al . Never - ending learning . Communications of the ACM , 2018 . [ 54 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical report , Stanford InfoLab , 1999 . [ 55 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . In Database Theoryâ€”ICDTâ€™99 . 1999 . [ 56 ] J . Pennington , R . Socher , and C . D . Manning . Glove : Global vectors for word representation . In EMNLP , 2014 . [ 57 ] M . E . Peters , S . Ruder , and N . A . Smith . To tune or not to tune ? adapting pretrained representations to diverse tasks . In RepL4NLP @ ACL , 2019 . [ 58 ] N . Reimers and I . Gurevych . Sentence - bert : Sentence embeddings using siamese bert - networks . In EMNLP , 2019 . 21 , , Hope et al . [ 59 ] M . Sachan and E . Xing . Self - training for jointly learning to ask and answer questions . In NAACL - HLT , 2018 . [ 60 ] H . SchÃ¼tze , C . D . Manning , and P . Raghavan . Introduction to information retrieval . In International communication of association for computing machinery conference , 2008 . [ 61 ] R . Socher , A . Perelygin , J . Wu , J . Chuang , C . D . Manning , A . Ng , and C . Potts . Recursive deep models for semantic compositionality over a sentiment treebank . In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631 â€“ 1642 , 2013 . [ 62 ] S . Vattam , B . Wiltgen , M . Helms , A . K . Goel , and J . Yen . DANE : Fostering Creativity in and through Biologically Inspired Design . In Design Creativity 2010 . 2011 . [ 63 ] L . Yu , A . Kittur , and R . E . Kraut . Searching for analogical ideas with crowds . In CHI , 2014 . [ 64 ] L . Yu , B . Kraut , and A . Kittur . Distributed analogical idea generation : innovating with crowds . In CHIâ€™14 , 2014 . [ 65 ] L . Yu , R . E . Kraut , and A . Kittur . Distributed analogical idea generation with multiple constraints . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing . ACM , 2016 . [ 66 ] Y . Zhang , P . Qi , and C . D . Manning . Graph convolution over pruned dependency trees improves relation extraction . In EMNLP , 2018 . 22