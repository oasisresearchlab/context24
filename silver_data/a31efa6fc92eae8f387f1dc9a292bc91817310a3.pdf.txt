An Empirical Study of Span Representations in Argumentation Structure Parsing Tatsuki Kuribayashi 1 , Hiroki Ouchi 2 , 1 , Naoya Inoue 1 , 2 , Paul Reisert 2 , 1 , Toshinori Miyoshi 3 , Jun Suzuki 1 , 2 , and Kentaro Inui 1 , 2 1 Tohoku University 2 RIKEN Center for Advanced Intelligence Project ( AIP ) 3 Research & Development Group , Hitachi , Ltd . { kuribayashi , naoya - i , jun . suzuki , inui } @ ecei . tohoku . ac . jp { hiroki . ouchi , paul . reisert } @ riken . jp toshinori . miyoshi . pd @ hitachi . com Abstract For several natural language processing ( NLP ) tasks , span representation is attracting con - siderable attention as a promising new tech - nique ; a common basis for an effective de - sign has been established . With such basis , exploring task - dependent extensions for argu - mentation structure parsing ( ASP ) becomes an interesting research direction . This study in - vestigates ( i ) span representation originally de - veloped for other NLP tasks and ( ii ) a sim - ple task - dependent extension for ASP . Our ex - tensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed . 1 Introduction Argumentation structure parsing ( ASP ) is the task of identifying argumentation structures in argu - mentative text . Figure 1 shows an example of an argumentative text and its structure . The structure forms a tree , the nodes of which are referred to as argumentative discourse units ( ADUs ) and the edges represent argumentative relations between the ADUs . ASP systems must identify such edges , edge labels ( e . g . , S UPPORT and A TTACK ) , and node labels ( e . g . , P REMISE , C LAIM , and M AJOR - C LAIM ) . A key to achieving high performance is feature representation design for segmental dis - course units ( spans ) , such as ADUs . The aim of this study is to update the foundation of span rep - resentation design for ASP . Potash et al . ( 2017 ) introduced a model ex - ploiting neural network - based span representa - tion for ASP and reported state - of - the - art perfor - mance . Similarly , for other natural language pro - cessing ( NLP ) tasks , such as syntactic and seman - tic parsing , neural network - based span representa - tion design is attracting considerable attention as a promising new technique ; some effective designs ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU1 ADU2 ADU3 ADU4 : but incomes are higher too . ADU4 ADU1 ADU2 ADU3 ADU4 ( i ) Current SOTA ( ii ) LSTM + dist ( ELMo ) Gold structure ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU4 : but incomes are higher too . support attack attack Premise Premise Claim Premise support attack attack ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU4 : but incomes are higher too . support attack attack Figure 1 : Example of an argumentative text and its structure . ADUs denote argumentative discourse units , which are fundamental units of arguments . have been reported ( Wang and Chang , 2016 ; Stern et al . , 2017 ; He et al . , 2018 ) . Starting from such basis , task - dependent extensions for ASP are an interesting direction to explore . Although the neural network - based ap - proach ( Potash et al . , 2017 ; Eger et al . , 2017 ) achieves high performances for ASP , it does not explicitly take into account useful linguistic clues . However , prior works demonstrate that linguistic features , particularly discourse con - nectives , are strong clues to predict the structure for ASP ( Lawrence and Reed , 2015 ; Stab and Gurevych , 2017 ) . We integrate such linguistic properties into span representation design as task - dependent extensions for ASP . In summary , our contributions are as follows . • We investigate ( i ) span representation origi - nally developed for other NLP tasks and ( ii ) a simple task - dependent extension for ASP . • Empirical results show that such representa - tions improve the performance of ASP and yield state - of - the - art scores . • Extensive analysis reveals that such represen - tations especially improve the performance when parsing argumentative texts with a complex structure ( deeper ADU trees ) . To facilitate ASP research , our model code and scripts made publicly available 1 . 1 https : / / github . com / kuribayashi4 / span _ based _ argumentation _ parser 2 Related work Argumentation structure parsing In recent years , the persuasive essay corpus ( PEC ) , a large - scale annotated dataset for argument structures , has been published ( Stab and Gurevych , 2017 ) ; moreover , a variety of models have been pro - posed for ASP . There are two main approaches for ASP : ( i ) a discrete feature - based approach ( Stab and Gurevych , 2017 ; Nguyen and Litman , 2016 ; Peldszus and Stede , 2015 ) and ( ii ) a neural network - based approach ( Potash et al . , 2017 ; Eger et al . , 2017 ) . Because neural network - based models have achieved high performance for this task ( Potash et al . , 2017 ) , this study also explores the neural network - based approach . Discourse connectives In discourse pars - ing ( Mann and Thompson , 1987 ; Prasad et al . , 2008 ) , which is closely related to ASP , discourse connectives are strong clues for identifying discourse relations ( Marcu , 2000 ; Braud and Denis , 2016 ) . Exploring effective ways to use the discourse connective information has received wide attention in various NLP ﬁelds ( Sileo et al . , 2019 ; Pan et al . , 2018 ) . Span representation Span representation de - sign is gaining considerable attention for several NLP tasks , such as syntactic parsing ( Wang and Chang , 2016 ; Stern et al . , 2017 ; Kitaev and Klein , 2018 ) , semantic role labeling ( He et al . , 2018 ; Ouchi et al . , 2018 ) , and coreference resolution ( Lee et al . , 2017 , 2018 ) . One common practice for effective design is to use bidirectional long short - term memory networks ( BiLSTMs ) . Wang and Chang ( 2016 ) proposed span representation called “LSTM - minus , ” which represents each span ( i , j ) as the difference between the LSTM’s hidden states over time steps , i . e . h j − h i . The work most similar to ours is Li et al . ( 2016 ) , where LSTM - minus is used for discourse structure pre - diction . This study extends it by integrating dis - course properties into span representation . 3 Model This section describes ( i ) an LSTM - minus - based span representation and ( ii ) a task - speciﬁc span representation for ASP . 3 . 1 Overview The given input is a paragraph consisting of T to - kens w 1 : T = ( w 1 , w 2 , · · · , w T ) and preidentiﬁed the other reason is that consequently , in conclusion , from the above views , although ﬁrst , as you can see that in this essay , the reasons for why i agree that it is a debatable subject that unfortunatelyalthoughsome argue that furthermore , it ’s undeniable that the in short , another thing that put big cities in front of small towns is in conclusion , despite the contribution of it to the society , however , some say that Table 1 : Examples of the AMs . ( i ) No Distinction between AMs and ACs ADU1 ADU2 AM1 ( ii ) Distinction between AMs and ACs AC1 AM2 AC2 ADU1 ADU2 Some people may argue that ( AM1 ) children will be more material , neglect their study for earning money or be exploited by the employers ( AC1 ) . However , ( AM2 ) if children get good care and instructions from their parents , they can take advantages of the work to learn valuable things and avoid going in a wrong way ( AC2 ) . AM1 AC1 AM2 AC2 BiLSTM BiLSTM somepeople employers . . . h1 : T w1 : T . . . . . . some that employers . . . . . . . . . . . . children . . . . . . . . . Figure 2 : Example of a part of an argumentative essay ( AMs are highlighted in red and ACs are underlined ) , and the models with or without the AC / AM distinction . segmental discourse units . In terms of segmen - tal discourse units to be captured , we decompose ADUs into argumentative components ( ACs ) and argumentative markers ( AMs ) . ACs correspond to claims and premises relevant to the writer’s argu - mentation . AMs , which play a crucial role in con - trolling argumentative ﬂows , are conjunctive ex - pressions frequently used in argumentative texts ( see Table 1 ) . We explore span representation of ADUs by leveraging AC and AM information as task - dependent extensions for ASP . Each ADU span is denoted as ( i , j ) , where i and j are word indices ( 1 ≤ i ≤ j ≤ T ) . We contextualize each representation on all of the ADUs in a paragraph by using BiLSTMs . These vectors are then fed to each task - speciﬁc classiﬁer . 3 . 2 LSTM - minus - based span representation We ﬁrst build a model based on the LSTM - minus - based span representation ( Wang and Chang , 2016 ) , as shown in Figure 2 ( left ) . This represen - tation makes no distinction between AMs and ACs and deals with them as one combined span ( i , j ) . w 1 : T = f emb ( w 1 : T ) , h 1 : T = BiLSTM ( w 1 : T ) , h ( i , j ) = [ −→ h j − −→ h i − 1 ; ←− h i − ←− h j + 1 ; −→ h i − 1 ; ←− h j + 1 ; φ ( w i : j ) ] . ( 1 ) Here , the word embedding layer f emb maps the input tokens w 1 : T to a sequence of embeddings w 1 : T . Taking w 1 : T as input , the BiLSTM layer then outputs a sequence of hidden states h 1 : T . Fi - nally , from h 1 : T , the LSTM - minus - based repre - sentation h ( i , j ) is assigned to each combined span ( i , j ) . Following Potash et al . ( 2017 ) , we also concatenate the additional discrete feature vector φ ( w i : j ) 2 of bag - of - words ( BoW ) and position in - formation of each span . 3 . 3 Distinction between AMs and ACs As a task - speciﬁc extension of the LSTM - minus - based span model ( henceforth , the LSTM model ) , we build a model that distinguishes between AMs ( i , k ) and ACs ( k + 1 , j ) , as shown in Figure 2 ( right ) . Similar to the LSTM model , each AM ( i , k ) is assigned its span representation h ( i , k ) , and each AC ( k + 1 , j ) is assigned h ( k + 1 , j ) . Then , using two independent BiLSTMs , the model con - textualizes AM spans and AC spans , respectively . The distinction and contextualization enable the model to better capture the argumentative ﬂow ; for example , the model can learn patterns of AM sequences ( e . g . , in my opinion → for example → however ) . Finally , each contextualize AC span represen - tation h ctx ( k + 1 , j ) is concatenated with its preced - ing contextualized AM span representation h ctx ( i , k ) and discrete feature vectors φ ( w i : j ) , i . e . h ( i , j ) = [ h ctx ( i , k ) ; h ctx ( k + 1 , j ) , φ ( w i : j ) ] . 3 . 4 Output layers Based on the contextualized ADU representations , we compute the class probability for each subtask . As the output function , we use the softmax func - tion . In AC / link type classiﬁcation , the softmax function computes the probability of each type . In link identiﬁcation , it computes the probability that the m - th ADU has a directed link to the h - th ADU . 2 Details of discrete features are shown in the Ap - pendix A . 2 3 . 5 Training We assume that we have access to a training set D , D = { ( X , Y link , Y ac - type , Y link - type ) d } | D | d = 1 , X = { w 1 : T , s ac 1 : M , s am 1 : M } , Y link = { h 1 , · · · , h M } , Y ac - type = { r 1 , · · · , r M } , Y link - type = { t 1 , · · · , t M } , where s ac and s am denote AC span and AM span , respectively . Y link is the set of candidate spans ( including a root object ) to which the given span has a directed link . Y ac - type and Y link - type are the set of all possible categories deﬁned in each task , e . g . , Y link - type = { S UPPORT , A TTACK } . To train model parameters , we minimize the joint cross - entropy loss function , L = − (cid:88) ( X , Y link , Y type ) ∈D (cid:2) α (cid:96) link ( X , Y link ) + β (cid:96) ac - type ( X , Y ac - type ) + ( 1 − α − β ) (cid:96) link - type ( X , Y link - type ) (cid:3) , where the three loss functions (cid:96) link , (cid:96) ac - type , and (cid:96) link - type are interpolated with α and β . Each loss function is deﬁned as follows : (cid:96) task ( X , Y task ) = (cid:88) y ∈ Y task logP ( y | m ) , where m denotes the m - th ADU . We show the de - tails of model training and hyperparameters in the Appendix A . 3 . 4 Experiments 4 . 1 Experimental setup Task setting The ASP task consists of the fol - lowing subtasks : ( i ) AC segmentation , ( ii ) AC type classiﬁcation ( ATC ) , ( iii ) link indentiﬁcation ( LI ) , and ( iv ) link type classiﬁcation ( LTC ) . Some previous studies ( Potash et al . , 2017 ; Niculae et al . , 2017 ; Peldszus and Stede , 2015 ) skipped the AC segmentation task and used gold AC segmentation for the other three subtasks . Fol - lowing these studies , we adopted the same task setting . Dataset We used the PEC ( Stab and Gurevych , 2017 ) and arg - microtext corpus ( MTC ) ( Peldszus and Stede , 2016 ) for our experiments . The PEC consists of 402 essays ( 1 , 833 paragraphs ) posted on an online portal 3 . We used the same train / test 3 https : / / essayforum . com / split used in Stab and Gurevych ( 2017 ) and ran - domly selected 10 % of the training set as the vali - dation set in the PEC . The scores obtained for the PEC were averaged across three distinct trials us - ing different random seeds . The MTC contains 112 short texts . Because this dataset is small , we conducted 10 sets of ﬁve - fold cross - validation and obtained the average scores across all the splits . We list the statistics of the datasets in the Ap - pendix C . Argumentative marker extraction In the PEC , while ACs are annotated , AMs are not . We thus extracted AMs using simple rules . The tokens pre - ceding an AC were extracted as its AMs . Note that AMs are not beyond the sentence and do not over - lap with other ACs . We considered pairs of ACs and the preceding AMs as ADUs . In the MTC , there are annotations of ADUs , which include both ACs and AMs . First , we cre - ated an AM list from the PEC and Penn Discourse TreeBank ( Prasad et al . , 2008 ) . If an ADU began with the phrases ( choosing the longest one ) speci - ﬁed in the AM list , we extracted the phrase as the AM and the following tokens as the AC . We collected 1 , 131 AMs , which had 5 . 38 tokens on average . We show the details of extraction pro - cedures in the Appendix D . Implementation details We used GloVe ( Pen - nington et al . , 2014 ) and ELMo embeddings ( Pe - ters et al . , 2018 ) , which have different properties . To explore the performance of the three subtasks , the models jointly predicted the three subtasks . Baseline As a baseline , we used the current state - of - the - art model using span representation based on BoW ( Potash et al . , 2017 ) ( BoW span model 4 ) . 4 . 2 Results We calculated macro - F1 and individual class F1 scores for the three tasks . For the overall perfor - mance , we calculated averaged macro - F1 scores across all the three tasks . As a statistical signiﬁ - cance test , we used bootstrap resampling ( Koehn , 2004 ) . Table 2 shows the results of our experiments . We obtained the following three tendencies : ( i ) compared with the BoW span model , the LSTM models show signiﬁcantly better performance , ( ii ) 4 Details of BoW span representation are shown in the Ap - pendix A . 1 Data . Embed . Span rep . Overall LI LTC ATC PEC ELMo LSTM + dist 81 . 8 80 . 7 79 . 0 85 . 7 LSTM 81 . 8 80 . 4 † 78 . 2 † 86 . 9 † BoW 77 . 1 76 . 2 72 . 3 82 . 9 GloVe LSTM + dist 79 . 7 78 . 8 76 . 5 83 . 9 LSTM 78 . 8 77 . 7 † 75 . 0 † 83 . 7 BoW 76 . 1 74 . 2 71 . 3 82 . 8 MTC ELMo LSTM + dist 78 . 2 73 . 9 77 . 2 ‡ 83 . 4 LSTM 75 . 0 73 . 0 † 71 . 5 80 . 5 BoW 73 . 3 71 . 2 67 . 5 81 . 2 GloVe LSTM + dist 76 . 5 72 . 6 75 . 4 ‡ 81 . 5 LSTM 70 . 4 70 . 1 64 . 1 76 . 9 BoW 71 . 1 69 . 2 64 . 8 † 79 . 3 Table 2 : Comparison among the LSTM - dist , LSTM , and BoW based representation . The results of LSTM model marked with † are statistically signiﬁcant com - pared to the BoW representation ( p < 0 . 05 ) . The re - sults of LSTM + dist model marked with ‡ are statisti - cally signiﬁcant compared to the LSTM representation ( p < 0 . 05 ) 0 20 40 60 80 100 0 1 2 or more a cc u r a c y ( % ) depth in argumentation structure LSTM + dist ( ELMo ) LSTM ( ELMo ) BoW ( ELMo ) LSTM + dist ( GloVe ) current SOTA Figure 3 : Accuracy in LI according to different depths . the span representation capturing the distinction between ACs and AMs ( LSTM + dist ) improves the performance especially in LI and LTC , and ( iii ) the ELMo embeddings boost performance . We also compared the LSTM + dist model with the existing models . Table 3 shows that our span - based models achieved new state - of - the - art scores on the PEC and yielded competitive results on the MTC com - pared to the current state - of - the - art model . 4 . 3 Analysis We conducted a detailed analysis on the PEC for investigating the types of instances that were easy or difﬁcult for the span models and the current state - of - the - art model to predict , focusing on the most challenging subtask , i . e . , LI . Stab and Gurevych ( 2017 ) reported that their model tends to output shallow trees even if the corresponding gold trees are deeper . We there - fore investigated the performance according to different depths of ADU trees . Figure 3 indi - Data . Model Overall LI LTC ATC Avg . Macro Link No - Link Macro Support Attack Macro MC Claim Premise PEC LSTM + dist ( ELMo ) 81 . 8 80 . 7 67 . 8 93 . 7 79 . 0 96 . 8 61 . 1 85 . 7 91 . 6 73 . 3 92 . 1 Joint PointerNet ( Potash et al . , 2017 ) - 76 . 7 60 . 8 92 . 5 - - - 84 . 9 89 . 4 73 . 2 92 . 1 St . SVM - full ( Niculae et al . , 2017 ) - - 60 . 1 - - - - 77 . 6 78 . 2 64 . 5 90 . 2 ILP Joint ( Stab and Gurevych 2017 ) 75 . 2 75 . 1 58 . 5 91 . 8 68 . 0 94 . 7 41 . 3 82 . 6 89 . 1 68 . 2 90 . 3 MTC LSTM + dist ( ELMo ) 78 . 2 73 . 9 57 . 5 90 . 3 77 . 2 84 . 2 70 . 3 83 . 5 - 72 . 9 94 . 0 Joint PointerNet ( Potash et al . , 2017 ) - 74 . 0 57 . 7 90 . 3 - - - 81 . 3 - 69 . 2 93 . 4 New Best EG ( Afantenos et al . , 2018 ) 78 . 5 72 . 2 - - 75 . 7 - - 87 . 6 - - - ILP Joint ( Stab and Gurevych 2017 ) 76 . 2 68 . 3 48 . 6 88 . 1 74 . 5 85 . 5 62 . 8 85 . 7 - 77 . 0 94 . 3 Table 3 : Comparison with existing models on the PEC and the MTC . MC denotes M AJOR C LAIM . ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU1 ADU2 ADU3 ADU4 : but incomes are higher too . ADU4 ADU1 ADU2 ADU3 ADU4 ( i ) Current SOTA ( ii ) LSTM + dist ( ELMo ) Gold structure ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU4 : but incomes are higher too . support attack attack Premise Premise Claim Premise support attack attack ADU1 : In addition , I believe that city provides more work opportunities than the countryside . ADU2 : There are not only more jobs , but they are also well - paid . ADU3 : Of course living in a city is more expensive , ADU4 : but incomes are higher too . support attack attack Figure 4 : Example of the outputs predicted by the cur - rent state - of - the - art model and the LSTM - dist model . Model acc . LSTM + dist ( ELMo ) 71 . 0 LSTM ( ELMo ) 68 . 9 BoW ( ELMo ) 60 . 7 Joint Pointer Net . ( Current SOTA ) 50 . 3 Table 4 : Accuracy for predicting the relations which form an A TTACK relation chain . cates the accuracy of predicting outgoing links ( i . e . parents ) from each AC at each depth . Over - all , it is difﬁcult to predict the links for deeper ADUs . While the performance of the state - of - the - art model ( Joint Pointer Net ) sharply dropped for deeper ACs ( two or more ) , the LSTM mod - els ( LSTM and LSTM + dist ) could robustly predict the correct links for deeper ADUs . Figure 4 shows an example of an argu - mentation structure with a chain of relations ( ADU4 → ADU3 → ADU1 ) and the outputs pre - dicted by the state - of - the - art model and the LSTM + dist model . The writer of this text ﬁrst states an expected opposite opinion ( ADU3 ) and then re - attacks the opposite opinion ( ADU4 ) . In such a structure , while the state - of - the - art model failed to predict the higher - order relations , the LSTM + dist model succeeded . Furthermore , Ta - ble 4 shows that the distinction between ACs and AMs had a positive effect on predicting the chains of A TTACK relations . Such A TTACK re - lation chains have been regarded as an important characteristic of argumentation structure ( Peld - szus and Stede , 2013 ; Freeman , 2011 ) and fre - quently appear especially in the MTC . Thus , the performance difference between the LSTM model and the LSTM + dist model ( see Table 2 ) on the MTC is relatively greater than that of the PEC . One possible reason for this positive effect is that typical AM ﬂows ( such as of course → but ) , which the LSTM + dist model explicitly captures , can be a clue for predicting higher - order chains . 5 Conclusion and Future work This work has studied span representations for ASP . Speciﬁcally , we have investigated ( i ) an LSTM - minus - based span representation origi - nally developed for other NLP tasks and ( ii ) a task - speciﬁc extended representation capturing the AM / AC distinction for ASP . The experimen - tal results have demonstrated the effects of these representations in ASP and that the span represen - tation capturing the AM / AC distinction achieves state - of - the - art results for three subtasks . The em - pirical analysis has showed that there is room for improvement in the LI for deeper - level ADUs . One interesting line of our future work is to in - vestigate the performance of our model in an end - to - end setting ( including AC segmentation ) . An - other direction is to explore span representations in several related tasks such as RST - style dis - course parsing or new span - related argumentation mining tasks ( Trautmann et al . , 2019 ) . 6 Acknowledgements This work was supported by JST CREST Grant Number JPMJCR1513 , Japan . We would like to thank the laboratory members who gave us advice and all reviewers of this work for their useful com - ments and feedback . References Chlo´e Braud and Pascal Denis . 2016 . Learning connective - based word representations for implicit discourse relation identiﬁcation . In Proceedings of EMNLP , pages 203 – 213 . Steffen Eger , Johannes Daxenberger , and Iryna Gurevych . 2017 . Neural end - to - end learning for computational argumentation mining . In Proceed - ings of ACL , pages 11 – 22 . James B Freeman . 2011 . Argument Structure : : Repre - sentation and Theory , volume 18 . Springer Science & Business Media . Luheng He , Kenton Lee , Omer Levy , and Luke Zettle - moyer . 2018 . Jointly predicting predicates and argu - ments in neural semantic role labeling . In Proceed - ings of ACL , pages 364 – 369 . Nikita Kitaev and Dan Klein . 2018 . Constituency pars - ing with a self - attentive encoder . In Proceedings of ACL , pages 2676 – 2686 . Philipp Koehn . 2004 . Statistical signiﬁcance tests for machine translation evaluation . In Proceedings of EMNLP , pages 388 – 395 . John Lawrence and Chris Reed . 2015 . Combining argument mining techniques . In Proceedings of the 2nd Workshop on Argumentation Mining , pages 127 – 136 . Kenton Lee , Luheng He , Mike Lewis , and Luke Zettle - moyer . 2017 . End - to - end neural coreference resolu - tion . In Proceedings of EMNLP , pages 188 – 197 . Kenton Lee , Luheng He , and Luke Zettlemoyer . 2018 . Higher - order coreference resolution with coarse - to - ﬁne inference . In Proceedings of NAACL - HLT , pages 687 – 692 . Qi Li , Tianshi Li , and Baobao Chang . 2016 . Discourse parsing with attention - based hierarchical neural net - works . In Proceedings of EMNLP , pages 362 – 371 . William C Mann and Sandra A Thompson . 1987 . Rhetorical structure theory : Description and con - struction of text structures . In Natural Language Generation , pages 85 – 95 . Daniel Marcu . 2000 . The theory and practice of dis - course parsing and summarization . MIT press . Huy V Nguyen and Diane J Litman . 2016 . Context - aware Argumentative Relation Mining . Proceedings of ACL , pages 1127 – 1137 . Vlad Niculae , Joonsuk Park , and Claire Cardie . 2017 . Argument Mining with Structured SVMs and RNNs . In Proceedings of ACL , pages 985 – 995 . Hiroki Ouchi , Hiroyuki Shindo , and Yuji Matsumoto . 2018 . A span selection model for semantic role labeling . In Proceedings of EMNLP , pages 1630 – 1642 . Boyuan Pan , Yazheng Yang , Zhou Zhao , Yueting Zhuang , Deng Cai , and Xiaofei He . 2018 . Dis - course marker augmented network with reinforce - ment learning for natural language inference . In Proceedings of ACL , pages 989 – 999 . Andreas Peldszus and Manfred Stede . 2013 . From Argument Diagrams to Argumentation Mining in Texts : a survey . International Journal of Cognitive Informatics and Natural Intelligence , 7 ( 1 ) : 1 – 31 . Andreas Peldszus and Manfred Stede . 2015 . Joint pre - diction in MST - style discourse parsing for argumen - tation mining . In Proceedings of EMNLP , pages 938 – 948 . Andreas Peldszus and Manfred Stede . 2016 . An Anno - tated Corpus of Argumentative Microtexts . Studies in Logic and Argumentation . Jeffrey Pennington , Richard Socher , and Christo - pher D . Manning . 2014 . Glove : Global vectors for word representation . In Proceedings of EMNLP , pages 1532 – 1543 . Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 2018 . Deep contextualized word rep - resentations . In Proceedings of NAACL - HLT , pages 2227 – 2237 . Peter Potash , Alexey Romanov , and Anna Rumshisky . 2017 . Here’s My Point : Joint Pointer Architecture for Argument Mining . In Proceedings of EMNLP , pages 1375 – 1384 . Rashmi Prasad , Nikhil Dinesh , Alan Lee , Eleni Milt - sakaki , Livio Robaldo , Aravind K Joshi , and Bon - nie L Webber . 2008 . The penn discourse treebank 2 . 0 . In Proceedings of the Sixth Conference on Lan - guage Resources and Evaluation , pages 2961 – 2968 . Damien Sileo , Tim Van - De - Cruys , Camille Pradel , and Philippe Muller . 2019 . Mining discourse markers for unsupervised sentence representation learning . In Proceedings of NAACL - HLT . Christian Stab and Iryna Gurevych . 2017 . Parsing ar - gumentation structures in persuasive essays . In Pro - ceedings of Computational Linguistics , volume 43 , pages 619 – 659 . MIT Press . Mitchell Stern , Jacob Andreas , and Dan Klein . 2017 . A minimal span - based neural constituency parser . In Proceedings of ACL , pages 818 – 827 . Dietrich Trautmann , Johannes Daxenberger , Christian Stab , Hinrich Sch¨utze , and Iryna Gurevych . 2019 . Robust argument unit recognition and classiﬁcation . In arXiv : 1904 . 09688 . Wenhui Wang and Baobao Chang . 2016 . Graph - based dependency parsing with bidirectional lstm . In Pro - ceedings of ACL , pages 2306 – 2315 . A Model A . 1 Features of BoW models Following Potash et al . ( 2017 ) , we used the fol - lowing features as ADU representations in BoW models : ( i ) one - hot vector of BoW ; ( ii ) embed - ding representation calculated by average , max , and min pooling across the token embeddings ; ( iii ) position of the ADU in an essay and paragraph and whether ADU is in opening , body , or closing para - graph ( one - hot vector ) . In BoW models with ELMo , using all of the versions of embedding representations ( avg , min , and max pooling ) hinders the performance . We only used embedding representations calculated by max pooling in BoW models with ELMo . A . 2 Discrete features of LSTM - minus - based models As mentioned in Section 3 , we additionally used discrete features in designing ADU representation of LSTM and LSTM - dist models ( φ ( w i : j ) in equa - tion 1 ) . We used ( i ) Bag - of - words and ( iii ) posi - tion information as additional discrete features . In LSTM - dist models , while ADU span representa - tion h ( i , j ) is concatenated with its discrete features φ ( w i : j ) , AC and CE span representation ( h ( i , k ) and h ( k + 1 j ) ) has no discrete features . A . 3 Output layers AC / Link type classiﬁcation layer Taking each span vector h ( i , j ) , the AC ( or link ) type classiﬁca - tion layer computes the probability that the span m = ( i , j ) is classiﬁed into each AC / Link type r ∈ R . score type m , r = w type r · h n + b type r , P ( r | m ) = exp ( score type m , r ) (cid:80) r (cid:48) ∈R exp ( score type m , r (cid:48) ) , where w type r is a parameter vector and b type r is a bias parameter associated with a type r . In ATC , R = { P REMISE , C LAIM , M AJOR C LAIM 5 } . In LTC , R = { S UPPORT , A TTACK } . Link identiﬁcation layer Taking each span vec - tor h ( i , j ) , the link identiﬁcation layer computes the probability that span m = ( i , j ) has a directed link 5 Major claims appear only in the PEC . Name Value Word embeddings - Glove 300 dim . - ELMo 1024 dim . BiLSTMs 256 ( 300 in the LSTM models ) dim . Mini - batch size 16 Optimizer Adam Learning rate 0 . 001 Epoch 500 ( 1000 in the MTC ) Loss interpolation - α 0 . 5 - β 0 . 25 Dropout ratio - Output layer 0 . 5 ( 0 . 9 in the MTC ) - BiLSTMs 0 . 1 ( 0 . 9 in the MTC ) - ELMo 0 . 1 Table 5 : Hyperparameters for our span - based model . to span h = ( i (cid:48) , j (cid:48) ) . score link m , h = w link · [ h link m ; h link h ; h link m (cid:12) h link h ; φ ( h , m ) ] , P ( h | m ) = exp ( score link m , h ) (cid:80) Mh (cid:48) = 1 exp ( score link m , h (cid:48) ) , where w link is a parameter vector . φ ( h , m ) is one - hot vector of relative position between h - th span and m - th span . R is the set of ADU spans in the same paragraph ( including a root object ) . To de - code the tree structure , we use the maximum span - ning tree algorithm based on the probabilities cal - culated by the softmax function . B Hyperparameters Table 5 shows the hyperparameters used in our experiments . Network setup We use 300 - dimensional GloVe and 1024 - dimensional ELMo embeddings . There was concern that LSTM - dist models out - performed the other models just because they had larger parameters . For fair comparison among BoW , LSTM , and LSTM - dist models , we prelim - inary conducted experiments with increasing the number of parameters in BoW and LSTM models ( changed the dimensions of the hidden units of all the BiLSTMs from 256 to 300 and added layers in the LSTM used in ADU - level contextualization ) . While parameter increased versions of BoW mod - els did not perform better , LSTM models slightly improve by increasing their parameters . The dimensions of the hidden units of all the BiLSTMs used in BoW and LSTM - dist models PEC MTC Texts All essays 402 - Paragraphs 1 , 833 112 Sentences 7 , 116 - Tokens 147 , 271 - ACs All ACs 6 , 089 576 Major claim 751 - Claim 1 , 506 112 Premise 3 , 832 464 Links All links 3 , 832 464 Support 3 , 613 290 Attack 219 174 Table 6 : Statistics of the PEC and the MTC . are set to 256 and the BiLSTMs have one hid - den layer . In the LSTM models , the dimensions of the hidden units are set to 300 and LSTM used in ADU - level contextualization has two layers . Regularization We apply dropout to the input vectors of the output layer with a dropout ratio of 0 . 5 in the PEC . We also apply dropout to all the BiLSTMs and word embeddings with 0 . 1 in the PEC . In the MTC , we increase the ratio of dropout ( Table 5 ) Model selection We select the models that achieve the highest averaged macro - F1 across the three tasks on the validation set . C Data Table 6 shows the statistics of the PEC and the MTC . The PEC is about ten times larger than the MTC . D Argumentative Marker Extraction We distinguish because in the following examples : • A . Because B , C . • A because B . C . As a simple solution , we include a period in the AMs . Then , AMs of the examples above are . be - cause and because , respectively . If there is no lex - ical AM in AC , then only end of sentence symbols ( i . e . , . , ! , and ? ) in the preceding sentence become its AMs . In the PEC , 63 % of ADUs have AMs . Sometimes AMs are inserted in the middle of sen - tences ( e . g . Others , however , think that these chil - dren may disrupt their school work and should be allowed to leave school early to ﬁnd a job . ) . In this case , we do not extract AMs . In fact , this is a rare case ( e . g . , however , exists in 1 % of the paragraphs in the PEC . ) . Joint Tasks Performance LI LTC ATC LI LTC ATC (cid:88) (cid:88) (cid:88) 80 . 7 79 . 0 85 . 7 (cid:88) (cid:88) 80 . 2 78 . 6 - (cid:88) (cid:88) 81 . 1 - 87 . 3 (cid:88) (cid:88) - 78 . 0 86 . 0 (cid:88) 78 . 3 - - (cid:88) 79 . 6 (cid:88) - - 85 . 6 Table 7 : Joint learning results on the PEC . E Joint learning analysis We additionally analyzed the effectiveness of joint learning with the LSTM + dist model . In jointly learning three tasks , we set α = 0 . 5 , β = 0 . 25 in the joint cross - entropy loss function . In jointly learn - ing two tasks , we set 0 . 5 as each task’s weight in the joint cross - entropy loss function . Table 7 shows the result . The check mark in Table 7 means that the model jointly learns the task . The numbers denote macro - F1 scores . We found that link type prediction task does not beneﬁt from joint learn - ing .