What You See Is ( not ) What You Get : A VR Framework For Correcting Robot Errors Maciej K . Wozniak maciejw @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Rebecca Stower stower @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Patric Jensfelt patric @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Andre Pereira atap @ kth . se KTH Royal Institute of Technology Stockholm , Sweden ABSTRACT Many solutions tailored for intuitive visualization or teleoperation of virtual , augmented and mixed ( VAM ) reality systems are not robust to robot failures , such as the inability to detect and recognize objects in the environment or planning unsafe trajectories . In this paper , we present a novel virtual reality ( VR ) framework where users can ( i ) recognize when the robot has failed to detect a real - world object , ( ii ) correct the error in VR , ( iii ) modify proposed object trajectories and , ( iv ) implement behaviors on a real - world robot . Finally , we propose a user study aimed at testing the efficacy of our framework . Project materials can be found in the OSF repository 1 . CCS CONCEPTS • Human - centered computing → Virtual reality . KEYWORDS robotics , human - robot interaction , VR , AR , perception ACM Reference Format : Maciej K . Wozniak , Rebecca Stower , Patric Jensfelt , and Andre Pereira . 2023 . What You See Is ( not ) What You Get : A VR Framework For Correcting Robot Errors . In Companion of the 2023 ACM / IEEE Int’l Conference on Human - Robot Interaction ( HRI’23Companion ) , March13 – 16 , 2023 , Stockholm , Sweden . ACM , New York , NY , USA , 5 pages . https : / / doi . org / 10 . 1145 / 3568294 . 3580081 1 INTRODUCTION Robots designed for human - robot - interaction ( HRI ) , such as cobots , can increasingly be seen in homes [ 21 ] , offices [ 5 ] or common areas like restaurants or bars [ 3 ] . Nevertheless , such robots are not perfect and can suffer from many potential failures . Although deep learning models used in the robot perception modules can perform well on the datasets they are trained and evaluated on , they often fail when deployed in the real world [ 6 ] . Simple failures , such as not detecting 1 https : / / osf . io / c8wap / ? view _ only = e1c799b564b043d0aeaac289513ebff0 . The code will be added after user studies are completed . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . HRI ’23 Companion , March 13 – 16 , 2023 , Stockholm , Sweden © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9970 - 8 / 23 / 03 . https : / / doi . org / 10 . 1145 / 3568294 . 3580081 Figure1 : ExampleoftheframeworkwithFrankaPandaroboticarm . From the left : ( 1 ) the VR user interface ; ( 2 ) user interacting with the robot through Oculus Quest 2 headset ; ( 3 ) real - world environment . an object , can have severe consequences . First of all , if the robot fails a simple task , the user may get angry and annoyed , resulting in lower trust and motivation to collaborate [ 9 , 13 ] . Secondly , a robot may collide or crash with objects if it fails to detect them . Another potential issue relates to how the robot plans and exe - cutes the trajectories . Users may have different preferences about how the robot should complete a specific task . A robot reaching for an object in a collaborative task ( e . g . , joint assembly ) may be perceived as unsafe from the user’s perspective , even if the move - ment is executed successfully from the robot point of view ( i . e . , no collision occurs ) . Allowing users to see the robot’s actions in advance , and potentially modify them , could then help improve the overall user experience . In our framework , we therefore consider two main sources of failure that can occur in human - robot - interactions ; failure to de - tect an object ( potentially causing a collision in the real world ) , or failure to plan an acceptable trajectory for the user . Rather than preventing these failures entirely , we aim to provide users with the opportunity to directly correct the robot themselves when such failures occur . We provide two main functions for ( i ) correcting the robot’s understanding about its surroundings by modifying the vir - tual environment created from its perception module ( ii ) modifying the robot’s trajectory to adjust to the users’ preferences or to avoid obstacles . In doing so , we contribute a virtual reality framework for human - robot collaboration that considers that robots are not perfect and makes it easy for the user to correct their mistakes . a r X i v : 2301 . 04919v2 [ c s . R O ] 17 J a n 2023 HRI ’23 Companion , March 13 – 16 , 2023 , Stockholm , Sweden Maciej K . Wozniak , Rebecca Stower , Patric Jensfelt , & Andre Pereira 2 RELATED WORK Several recent projects have focused on improving human - robot collaboration using virtual , augmented and mixed reality ( VAM ) devices . Teleoperation ( remotely controlling the robot ) is one of the most common forms of human - robot interaction [ 17 , 18 , 22 ] . Ostatin [ 20 ] and Togias [ 24 ] both use VR to plan the robotic arm’s trajectory . Others , like Chandan et al . [ 2 ] use AR to visualize the states and intentions of the robots . Xu et al . [ 31 ] used VR to steer the robot by moving its end effector in the VR space . Kennel - Maushart et al . [ 8 ] and Barentine et al . [ 1 ] also focused on steering the robot , however , the former did so by applying force in AR by pressing virtual objects held by the virtual robot , making the real world robot move . The latter uses VR for steering the robot from a first - person view . Lastly , Wozniak and Jensfelt [ 29 ] presented a VR simulation framework for interacting with the virtual representation of a robot and modifying its trajectory . Many of these projects focus only on the execution of planned robot trajectories and do not consider the perception module ( and associated potential failures ) . In addition , the visualization and manipulation of the robot’s movements are limited to either only choosing the final position of the robot’s end - effector or adding no - go zones . Finally , none of these projects follow the whole pipeline of first visualizing , then correcting and testing the planned trajectory in simulation , followed by deploying it to a real - world robot . Our framework could also be classified as a digital twin project , which is a virtual representation of a robot’s actions and abili - ties [ 11 ] . Modern physics engines can imitate reality in great de - tail , allowing a digital twin to be an accurate test - bed for the real - world [ 14 ] . There are various applications and benefits of using VR interfaces for digital twin projects , such as improved factory safety or workers’ training [ 7 , 26 ] , collaborative tasks [ 19 ] or as - sembly process [ 4 ] . However , these methods so far focus primarily on personnel training for the manufacturing industry . In addition , what is still missing in these works is an explicit understanding of the robot’s perception of the working space and planned course of action . Our framework addresses this challenge by allowing the user to ( i ) assess how the robot perceives the envi - ronment , ( ii ) modify and correct the robot’s actions and understand - ing of the environment , and ( iii ) verify the proposed actions and trajectories before deploying them and testing in the real world . 3 TECHNICAL IMPLEMENTATION Our proposed framework integrates several systems ; the cameras and associated perception module , the VR headset and user inter - face , and the real - world robot . 3 . 1 Camera views and perception module We use two cameras to create and examine the environment : one fixed , pointing at the table ( main camera ) and one on the robot’s end effector ( secondary camera ) . The images detected by the cameras are fed into the deep learning model [ 30 ] for scene segmentation and object detection to obtain segmentation masks and bounding boxes of the detected objects 2 . These tell us what the object is and where it is located . Our system then uses this information to correctly position and add objects to the scene in VR . 2 Any other object detection architectures could be used as well . Sensordata Sensors Motors Deep Learningmodule Data transfernode VR environment User Robot controller Figure 2 : Proposed system architecture . The elements within the or - ange box correspond to nodes and parts connected with the robot’s motion and perception , whereas the ones in the green box are ex - plicitly corresponding to its hardware . Nodes inside the blue box are connected to the user and VR setting . The VR environment is then built from the output of the per - ception module . The user can only ask the robot to interact with objects shown in the VR environment . The user can switch between the VR environment view and the two distinct camera views to verify whether the robot has successfully detected all the objects . In case an object is occluded , the robot’s arm with the secondary camera can be moved ( since from another angle the object may be more clearly visible ) to detect it and add it to the environment . Alternatively , if the object is still not detectable or the perception module itself fails , the user can add the missing object to the VR environment by bringing up a menu and adding the object to the scene ( as shown in the supplementary video 1 ) . 3 . 2 Robot and data transfer We have so far tested our framework with the Franka Panda and Niryo One robotic arms ; however our framework can accommodate most robotic arms , provided the URDF files . We use the MoveIt library 3 for planning and Robot Operating System ( ROS Melodic ) 4 for transferring data and running different tasks on separate nodes . Since the user interface ( UI ) of our framework is in the VR head - set , other operations , such as planning and object detection , must be run on separate machines . This creates the challenge of efficiently transferring the data between different nodes of the system and the VR headset itself . In order to facilitate the exchange of information , improve the user experience , and minimize the necessary band - width , we only transfer the output of a detection network between the robot and the virtual environment . When the robot detects and classifies an object , it sends its class , location , and estimated size to the VR UI application . In the virtual environment , we have multiple prefabs ( 3D models of the objects we built into the project ) corresponding to the detected classes , from which we can quickly create a 3D representation of the environment when the message from the perception module ( containing detected object classes and poses ) is received . 3 https : / / moveit . ros . org / 4 https : / / www . ros . org / What You See Is ( not ) What You Get : A VR Framework For Correcting Robot Errors HRI ’23 Companion , March 13 – 16 , 2023 , Stockholm , Sweden 3 . 3 VR Interface The VR interface is built in Unity ( 2020 . 3 version ) and used the Oculus Quest 2 headset 5 . In our UI design ( shown in Fig . 3d and the supplementary video 1 ) the user can bring up two different menus with primary and secondary buttons on the left controller and choose an option by pointing at the specific button with the right - hand controller . The first menu has options for creating and modifying the environment . The second menu contains commands for the robot ( such as selecting the object or sending trajectories to the real robot ) . 4 INTERACTION SCENARIO In this section we show an example scenario of our framework with a 7 DoF Franka Panda robotic arm . While we use a real - world robot , we want to ensure that the framework is available to labs without access to a physical robot . In such a case , communication with a real - world robot can be simulated by running the same Unity project on another machine 1 . There are three main actions that users take to arrive at real - world object manipulation with the robot . First , the VR environment depicting both the robot and the objects detected and recognized by the perception module is created ( and corrected by the users , if necessary ) . Next , the users ask the virtual robot to perform an action in VR . If the users are satisfied with how the virtual robot acts , they can send the trajectory to the real – world robot . If not , they can modify the trajectory beforehand . All the steps described in this section can be found in the video in attached materials 1 . 4 . 1 Common environment understanding The VR environment corresponds to what the robot understands about the real - world environment . Objects detected by the robot’s perception module appear in the UI . Users can examine whether the objects correspond to what is in the real - world environment in two ways . First , as shown in Fig . 3c , they can activate passthrough where they will see the main camera image projected on the table where the objects are located . Secondly , they can go to camera view , where they can see and compare the VR environment with the output from both cameras’ deep learning modules as shown in Fig . 3a and Fig . 3b . However , the perception module is imperfect and often partially fails , as described in Section 3 . 1 . There are two ways to recover from these failures . The users can move the robotic arm around and try to detect the objects using the camera on the robot’s arm . This can help with , e . g . , partially occluded objects or objects that cannot be recognized from the main camera perspective ( Fig . 3a vs . Fig . 3b ) . Another way is to insert virtual objects into the environ - ment . Users can choose the object from the UI and grab and place it in the VR environment where it belongs , as shown in Fig . 3d . The passthrough view on the table allows the user to see where the ob - jects are in the real world and place a matching virtual object in the correct position . Objects in the environment are then considered for collision avoidance while planning the trajectory . 5 Our framework can also be adapted to other headsets by changing the target device while building the Unity project ( a ) Main camera view ( b ) Secondary camera view ( c ) Passthrough view ( d ) Adding missing objects to the environment Figure 3 : Different ways to see the real world view in the VR . Users can verify if the environment was correctly created and correct po - tential flaws of the robot’s perception module by comparing these views with the state of the VR environment . 4 . 2 Trajectory manipulation When the users are satisfied that the VR environment’s current state is an accurate representation of reality , they can proceed to ask the virtual robot to move objects . Whilst the virtual robot is executing a movement , it generates a trajectory and waypoints in real - time in the VR environment . After the robot finishes its move - ment , the user then has the option to either send the action to the real robot , or bring the virtual robot and the selected object back to their initial positions and edit the trajectory . While the planner considers objects in the scene and plans the trajectory so that the robotic arm will not collide with them ( except for the object we want to pick up ) , this might not always align with user’s prefer - ences . To accommodate this , we first show the user the planned trajectory containing multiple waypoints in VR . If the user is not satisfied with the proposed trajectory , they can edit it by moving the waypoints . Now , the user can ask the robot to perform the task again , following the modified trajectory ( see video for demonstra - tion 1 ) . This additional functionality could be used to e . g . , help home robots learn users’ preferences . 4 . 3 Real robot When the users are satisfied with the state of the environment and the way the robot performs the task , they can translate the movement to the real robot . After the real robot finishes the task , they can ask it to start another interaction in the same manner as before : by planning and verifying it first in VR and then sending it to the real robot . 5 PROPOSED USER STUDY In order to test the efficacy of our framework , we plan to conduct a user study comparing our VR system with a screen interface using HRI ’23 Companion , March 13 – 16 , 2023 , Stockholm , Sweden Maciej K . Wozniak , Rebecca Stower , Patric Jensfelt , & Andre Pereira a keyboard and mouse ( hereon referred to as screen interface ) . In this study , we will focus only on failures within the perception module , rather than editing robot trajectories . Our goal is to inves - tigate whether VR systems are preferred to screen - based systems for perceiving and correcting robot perceptual errors . As our independent variable , we will manipulate the type of interface participants use to interact with the robot ( VR , screen ) . We chose the screen interface as our point of comparison because ( i ) screen interfaces are often used to teleoperate robots [ 12 , 23 , 28 ] , and ( ii ) there are very few direct VR - screen comparisons in the HRI literature to date [ 10 , 16 ] . We plan to use a within - groups study design , where all participants will be exposed to both conditions . Whether participants start with the VR system first or screen first will be randomly assigned . Before beginning the study , participants will be given a pre - questionnaire assessing their familiarity with robots and VAM reality systems . Participants will first be given a training phase where they can familiarise themselves with the system setup and controls . In par - ticular , participants will be shown the two different cameras ( main and secondary ) with which they can view the real - world scene , as well as the passthrough option . Participants will then have 5 experimental trials where they interact with the robot . Each trial will consist of the robot failing to recognize an object . That is , the user can see that the object exists in the real world ( via the external cameras / passthrough ) , but it is not represented in the virtual world ( either in VR for the VR condition , or on the screen in the screen condition ) . The user can then correct this error in one of two differ - ent ways . First , they can move the secondary camera in an effort to detect the missing object . Alternatively , they can manually add the object to the environment by selecting the missing object from an array of different possible objects and placing it ( on the screen / in VR ) as close to the real - life position of the object as possible . Once the user is satisfied with their object positioning , they can submit their feedback to the virtual robot , view the planned trajectory , and finally , execute the movement in the real world ( as described in section 4 . 2 ) before moving on to the next trial . Although the overall framework also allows users to view and edit the planned trajectory , for this study we will disable this function , as we aim to focus first on how users correct the robot when an object detection failure occurs . As dependent variables , we will measure how often users choose each correction method ( moving the secondary camera , or adding virtual objects ) , the time taken per trial , and the user’s subjective perceptions of each interface . We will also measure the accuracy of the object placement by looking at where users choose to manually add the object to the environment . For the subjective perceptions , we will use the Technology Acceptance Model ( TAM ) [ 25 ] and the performance trust subscale from the Multi - Dimensional Mea - sure of Trust ( MDMT ) scale [ 15 ] . Participants will complete the questionnaires directly after interacting with each system . After participants have seen both conditions , we will also ask a binary forced - choice question about which system they preferred to use . Finally , we will interview participants about the perceived strengths and weaknesses of each system . We aim to recruit 50 participants , based on an a - priori power analysis with 𝛼 = 0 . 05 , 𝛽 = 0 . 8 and 𝑑 = 0 . 4 . All hypotheses and planned analyses will be pre - registered on the Open Science Framework 1 prior to data collection . 5 . 1 Hypotheses Based on previous studies which show that VR systems lead to better task performance [ 12 ] and have higher perceived usability [ 27 ] than 2D interactions , we formulate the following hypotheses : H1 The VR interface will be preferred over the screen interface for interacting with the robot H2 Participants will have better task performance with the VR system in terms of : 2a More accurate object placement when adding virtual ob - jects to the environment 2b Shorter average time taken per trial H3 Subjective ratings on the self – report questionnaires ( technol - ogy acceptance and trust ) will be higher for the VR system compared to the screen interface . 6 CONCLUSIONS AND FUTURE WORK This paper presents a virtual reality human - robot collaboration framework for object detection and interaction accounting for ro - bot perceptual failures . Although we are interested in using our framework to enhance human robot collaboration , it could also be used for other tasks . As briefly mentioned in Section 4 . 2 , trajectory manipulation can be adapted to the preference learning task . While future home robots might be able to successfully perform many different tasks , the users’ satisfaction will vary . For example , different groups of users might have different preferences about how to do certain things . People’s level of comfort being close to a moving robot may also vary . A similar tool to what we present can be used in the transition when a user buys a new robot and wants to collect data on how users correct a robot’s trajectory so that the system learns to perform tasks in the way that this particular user likes . The framework can also be used to collect data and improve deep learning models responsible for robot perception . When users realize a specific object is not detected , they could , e . g . , draw a bounding box around that object or even just place it in the correct place in the VR , triggering bounding box generation . Then , such images could be collected and the model retrained to perform better on the tasks it failed before . Such a solution can help the users improve the robot’s functionality , even if they do not have any programming experience . In sum , in this paper we presented a technical implementation and interaction scenario of a VR framework for correcting robot perception and planning errors , as well as proposed a future user study . Future work will continue to develop and test this framework further . 7 ACKNOWLEDGMENTS This work was partially supported by the Wallenberg AI , Au - tonomous Systems and Software Program ( WASP ) funded by the Knut and Alice Wallenberg Foundation and the Vinnova Compe - tence Center for Trustworthy Edge Computing Systems and Appli - cations . What You See Is ( not ) What You Get : A VR Framework For Correcting Robot Errors HRI ’23 Companion , March 13 – 16 , 2023 , Stockholm , Sweden REFERENCES [ 1 ] Christian Barentine , Andrew McNay , Ryan Pfaffenbichler , Addyson Smith , Eric Rosen , and Elizabeth Phillips . 2021 . A VR Teleoperation Suite with Manipu - lation Assist . In Companion of the 2021 ACM / IEEE International Conference on Human - Robot Interaction ( Boulder , CO , USA ) ( HRI ’21 Companion ) . Association for Computing Machinery , New York , NY , USA , 442 – 446 . https : / / doi . org / 10 . 1145 / 3434074 . 3447210 [ 2 ] Kishan Chandan , Vidisha Kudalkar , Xiang Li , and Shiqi Zhang . 2021 . ARROCH : Augmented reality for robots collaborating with a human . In 2021 IEEE Interna - tional Conference on Robotics and Automation ( ICRA ) . IEEE , 3787 – 3793 . [ 3 ] Mary Ellen Foster , Simon Keizer , and Oliver Lemon . 2014 . Towards action selec - tion under uncertainty for a socially aware robot bartender . In Proceedings of the 2014 ACM / IEEE international conference on Human - robot interaction . 158 – 159 . [ 4 ] Jérôme Guzzi , Gabriele Abbate , Antonio Paolillo , and Alessandro Giusti . 2022 . Interacting with a Conveyor Belt in Virtual Reality Using Pointing Gestures . In Proceedings of the 2022 ACM / IEEE International Conference on Human - Robot Interaction ( Sapporo , Hokkaido , Japan ) ( HRI ’22 ) . IEEE Press , 1194 – 1195 . [ 5 ] Nick Hawes , Christopher Burbridge , Ferdian Jovan , Lars Kunze , Bruno Lacerda , Lenka Mudrova , Jay Young , Jeremy Wyatt , Denise Hebesberger , Tobias Kortner , et al . 2017 . The strands project : Long - term autonomy in everyday environments . IEEE Robotics & Automation Magazine 24 , 3 ( 2017 ) , 146 – 156 . [ 6 ] Shanee Honig and Tal Oron - Gilad . 2018 . Understanding and resolving failures in human - robot interaction : Literature review and model development . Frontiers in psychology 9 ( 2018 ) , 861 . [ 7 ] Tero Kaarlela , Sakari Piesk , and Tomi Pitkaho . 2020 . Digital Twin and Virtual Reality for Safety Training . In 2020 11th IEEE International Conference on Cogni - tive Infocommunications ( CogInfoCom ) . 000115 – 000120 . https : / / doi . org / 10 . 1109 / CogInfoCom50765 . 2020 . 9237812 [ 8 ] Florian Kennel - Maushart , Roi Poranne , and Stelian Coros . 2022 . Multi - Arm Pay - load Manipulation via Mixed Reality . In 2022 International Conference on Robotics and Automation ( ICRA ) . 11251 – 11257 . https : / / doi . org / 10 . 1109 / ICRA46639 . 2022 . 9811580 [ 9 ] Zahra Rezaei Khavas , S Reza Ahmadzadeh , and Paul Robinette . 2020 . Modeling trust in human - robot interaction : A survey . In International Conference on Social Robotics . Springer , 529 – 541 . [ 10 ] Jamy Li . 2015 . The benefit of being physically present : A survey of experimen - tal works comparing copresent robots , telepresent robots and virtual agents . International Journal of Human - Computer Studies 77 ( 2015 ) , 23 – 37 . [ 11 ] Mengnan Liu , Shuiliang Fang , Huiyue Dong , and Cunzhi Xu . 2021 . Review of digital twin about concepts , technologies , and industrial applications . Journal of Manufacturing Systems 58 ( 2021 ) , 346 – 361 . [ 12 ] Oliver Liu , Daniel Rakita , Bilge Mutlu , and Michael Gleicher . 2017 . Understand - ing human - robot interaction in virtual reality . In 2017 26th IEEE international symposium on robot and human interactive communication ( RO - MAN ) . IEEE , 751 – 757 . [ 13 ] Gale M Lucas , Jill Boberg , David Traum , Ron Artstein , Jonathan Gratch , Alesia Gainer , Emmanuel Johnson , Anton Leuski , and Mikio Nakano . 2018 . Getting to know each other : The role of social dialogue in recovery from errors in social robots . In Proceedingsofthe2018acm / ieeeinternationalconferenceonhuman - robot interaction . 344 – 351 . [ 14 ] AliAhmadMalikandAlexanderBrem . 2021 . Digitaltwinsforcollaborativerobots : A case study in human - robot interaction . Robotics and Computer - Integrated Manufacturing 68 ( 2021 ) , 102092 . [ 15 ] Bertram F Malle and Daniel Ullman . 2021 . A multidimensional conception and measure of human - robot trust . In Trust in human - robot interaction . Elsevier , 3 – 25 . [ 16 ] Martina Mara , Jan - Philipp Stein , Marc Erich Latoschik , Birgit Lugrin , Constanze Schreiner , Rafael Hostettler , and Markus Appel . 2021 . User responses to a hu - manoid robot observed in real life , virtual reality , 3D and 2D . Frontiers in Psy - chology 12 ( 2021 ) , 633178 . [ 17 ] Abdeldjallil Naceri , Dario Mazzanti , Joao Bimbo , Domenico Prattichizzo , Dar - win G . Caldwell , Leonardo S . Mattos , and Nikhil Deshpande . 2019 . Towards a Virtual Reality Interface for Remote Robotic Teleoperation . In 2019 19th Interna - tional Conference on Advanced Robotics ( ICAR ) . 284 – 289 . https : / / doi . org / 10 . 1109 / ICAR46387 . 2019 . 8981649 [ 18 ] Federica Nenna and Luciano Gamberini . 2022 . The Influence of Gaming Ex - perience , Gender and Other Individual Factors on Robot Teleoperations in VR . In Proceedings of the 2022 ACM / IEEE International Conference on Human - Robot Interaction . 945 – 949 . [ 19 ] Valerio Ortenzi , Maija Filipovica , Diar Abdlkarim , Tommaso Pardi , Chie Taka - hashi , Alan Wing , Massimiliano Di Luca , and Katherine J Kuchenbecker . 2022 . Robot , PassMetheTool : HandleVisibilityFacilitatesTask - orientedHandovers . In Proceedings oftheACM / IEEEInternationalConferenceon Human - RobotInteraction ( HRI ) . 1 – 9 . [ 20 ] Mikhail Ostanin and Alexandr Klimchik . 2018 . Interactive robot programing using mixed reality . IFAC - PapersOnLine 51 , 22 ( 2018 ) , 50 – 55 . [ 21 ] Hayley Robinson , Bruce MacDonald , and Elizabeth Broadbent . 2014 . The role of healthcare robots for older people at home : A review . International Journal of Social Robotics 6 , 4 ( 2014 ) , 575 – 591 . [ 22 ] EricRosen , DavidWhitney , ElizabethPhillips , DanielUllman , andStefanieTellex . 2018 . TestingrobotteleoperationusingavirtualrealityinterfacewithROSreality . In Proceedings of the 1st International Workshop on Virtual , Augmented , and Mixed Reality for HRI ( VAM - HRI ) . 1 – 4 . [ 23 ] DanielSzafir . 2019 . Mediatinghuman - robotinteractionswithvirtual , augmented , and mixed reality . In International Conference on Human - Computer Interaction . Springer , 124 – 149 . [ 24 ] Theodoros Togias , Christos Gkournelos , Panagiotis Angelakis , George Michalos , and Sotiris Makris . 2021 . Virtual reality environment for industrial robot control and path design . Procedia CIRP 100 ( 2021 ) , 133 – 138 . [ 25 ] Viswanath Venkatesh and Fred D Davis . 2000 . A theoretical extension of the technologyacceptancemodel : Fourlongitudinalfieldstudies . Managementscience 46 , 2 ( 2000 ) , 186 – 204 . [ 26 ] Xi Wang , Ci Jyun Liang , Carol C Menassa , and Vineet R Kamat . 2021 . Interactive and immersive process - level digital twin for collaborative human – robot con - struction work . Journal of Computing in Civil Engineering 35 , 6 ( 2021 ) , 04021023 . [ 27 ] David Whitney , Eric Rosen , Elizabeth Phillips , George Konidaris , and Stefanie Tellex . 2020 . Comparing robot grasping teleoperation across desktop and virtual reality with ROS reality . In Robotics Research . Springer , 335 – 350 . [ 28 ] Murphy Wonsick and Taşkın Padır . 2021 . Human - humanoid robot interaction throughvirtualrealityinterfaces . In 2021IEEEAerospaceConference ( 50100 ) . IEEE , 1 – 7 . [ 29 ] Maciej K Wozniak and Patric Jensfelt . 2022 . Virtual Reality Framework for Better Human - Robot Collaboration and Mutual Understanding . ( 2022 ) . [ 30 ] Yuxin Wu , Alexander Kirillov , Francisco Massa , Wan - Yen Lo , and Ross Girshick . 2019 . Detectron2 . https : / / github . com / facebookresearch / detectron2 . [ 31 ] Shiyu Xu , Scott Moore , and Akansel Cosgun . 2022 . Shared - Control Robotic Manipulation in Virtual Reality . arXiv preprint arXiv : 2205 . 10564 ( 2022 ) .