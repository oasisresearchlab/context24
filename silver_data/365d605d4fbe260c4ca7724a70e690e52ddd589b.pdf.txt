IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 469 Correspondences Artiﬁcial Cognitive Systems That Can Answer Human Creativity Tests : An Approach and Two Case Studies Ana - Maria Olte¸teanu , Zoe Falomir , and Christian Freksa Abstract —Creative cognitive systems are rarely assessed with the same tools as human creativity . In this paper , an approach is proposed for building cognitive systems which can solve human creativity tests . The importance of using cognitively viable processes , cognitive knowledge acquisition and organization , and cognitively comparable evaluation when implementing creative problem - solving systems is emphasized . Two case studies of artiﬁcial cognitive systems evaluated with human creativity tests are reviewed . A general approach is put forward . The applicability of this general approach to other creativity tests and artiﬁcial cognitive systems , together with ways of performing cognitive knowledge acquisition for these systems are then explored . Index Terms — Cognitive knowledge acquisition , cognitive pro - cessing , cognitive systems , cognitively - comparable evaluation , computational creativity , human creativity . I . I NTRODUCTION We envision a future where artiﬁcial cognitive agents help humans solve problems in their daily routines at home and at work . In order to be helpful , those agents must ﬁnd good solutions or creative alter - natives the human can select from . Imagine you are in your house , needing to solve a particular problem . You need a particular tool or object ( e . g . , a piece of string ) or recipe ingredient ( e . g . , mince meat ) but you do not have it in the house . You would like to solve your problem using a different tool , object , or ingredient , and have a sys - tem show you what you could use instead ( e . g . , dental ﬂoss to replace the piece of string , aubergine to replace the mince meat—depending on task and recipe context ) . Or imagine you are in a situation in which you ran out of ideas on how to solve a particular problem , and would like to think of a completely different approach . Let us say that you would like the help of a system that could inspire you . Such a system would need to operate or at least communicate in a cognitive manner—understand what type of information is associated with the problem at hand , in a manner that would enhance your creative process—for example , by predisposing you to new ways of seeing the problem or rerepresenting its content . Manuscript received May 25 , 2016 ; revised August 12 , 2016 and October 6 , 2016 ; accepted November 10 , 2016 . Date of publication November 16 , 2016 ; date of current version June 8 , 2018 . The work of A . - M . Olte¸teanu was supported by the Deutsche Forschungsgemeinschaft for the Creative Cognitive Systems ( CreaCogs : http : / / creacogcomp . com / ) Project . The work of Z . Falomir was supported by the Universität Bremen through the 04 - Independent Projects for Postdocs action for the project Cognitive Qualitative Descriptions and Applications ( CogQDA : https : / / sites . google . com / site / cogqda / ) . ( Corresponding author : Ana - Maria Olte¸teanu . ) The authors are with the Bremen Spatial Cognition Center , Universität Bremen , 28359 Bremen , Germany ( e - mail : amoodu @ informatik . uni - bremen . de ) . Color versions of one or more of the ﬁgures in this paper are available online at http : / / ieeexplore . ieee . org . Digital Object Identiﬁer 10 . 1109 / TCDS . 2016 . 2629622 It could show you items ( websites , research articles , excerpts from encyclopedias , music , ﬁlms , photos , and formulas ) which could trigger for you new associations , new ways of solving or framing that problem . Furthermore , such assistive systems would need to be capable of creative problem - solving and of presenting their results in a manner that can easily be used as input by humans . They would need to be endowed with cognitive knowledge acquisition , and types of knowl - edge processing which are akin to those used by humans in their creative problem - solving . Computational creativity is a strongly emerging ﬁeld in artiﬁ - cial intelligence , with application systems ranging from poetry [ 1 ] , music [ 2 ] , painting [ 3 ] , to mathematics [ 4 ] . Empirical tests of human creativity and creative problem - solving do exist [ 5 ] – [ 10 ] . However , computational creativity systems can rarely be assessed in a com - parable manner , i . e . , by using human creativity tests . This has as consequence the fact that not many artiﬁcial cognitive systems which can be used as cognitive models exist ( e . g . , [ 11 ] ) , and thus not as much progress is made as possible in terms of understanding the cog - nitive bases of the creative process , and in building artiﬁcial cognitive creative systems . This paper argues that : 1 ) artiﬁcial cognitive systems can be used to shed light on the human creative process ; and 2 ) knowledge obtained from creativity tests can be used to inform and evaluate artiﬁcial cognitive systems , if more work is done on artiﬁcial cognitive systems that can be assessed with human creativity tests . To support this claim , this paper presents a general approach for building artiﬁcial cognitive systems that can solve human creativity tests in a cognitive manner . The types of knowledge , knowledge acquisition , cognitive processes , and cognitive evaluation which can be used are also discussed . The beneﬁts of bridging this gap are then shown in terms of : 1 ) new relations which can be observed or studied from a cognitive modeling paradigm and 2 ) knowledge and data obtained from human creativity tests which can then be used to inform artiﬁcial cognitive systems . Two case studies of systems which can solve human creativity tests are then presented , brieﬂy describing how cognitive knowledge was acquired and organized for such systems , the processes used and how the systems were evaluated compared to their human counterparts . The rest of this paper is structured as follows . Section II describes the differences between computational creativity evaluation methods and human creative evaluation . Cognitive processes relevant when implementing artiﬁcial creative cognitive systems are discussed in Section III . Sections IV - A and IV - B brieﬂy describe two case studies of artiﬁcial cognitive systems which can give comparable results to humans in creativity tests , together with the knowledge acquisition , cognitiveprocessingandevaluationofthesesystems . SectionVpresents a general approach toward making artiﬁcial cognitive systems that yield comparable results to humans in creativity tests . The applicability of this approach to other creativity tests is discussed in Section VI . 2379 - 8920 c (cid:2) 2016 IEEE . Personal use is permitted , but republication / redistribution requires IEEE permission . See http : / / www . ieee . org / publications _ standards / publications / rights / index . html for more information . 470 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 A short description of how cognitive knowledge acquisition can be performed using creativity tests is provided in Section VII . A short discussion and conclusion is presented in Section VIII . II . C OMPUTATIONAL C REATIVITY E VALUATION V ERSUS H UMAN C REATIVITY T ESTS Computational creativity is evaluated in various ways and it has been debated what the evaluation process for computational creativity systems should be [ 12 ] and [ 13 ] . Some authors aim to produce artifacts through their systems , and would like them to be assessed as having comparable creativity to that of humans . Others deﬁne creativity in terms of process [ 14 ] , not necessarily aiming for an implementation . Ritchie [ 13 ] proposed an assess - ment of computational creative systems which takes into account the inspiring set—a union of implicit and explicit knowledge— formalizing 14 criteria of evaluation , centered around typicality , quality and novelty ( see also [ 15 ] ) . Pease et al . [ 16 ] proposed an evaluation which takes into account the input , output and process of the creative system , based on measures of novelty , quality and process . The FACE model [ 17 ] describes creative acts as tuples of generative acts , including items and methods of producing items in four categories : 1 ) concepts ; 2 ) concept expressions ; 3 ) aes - thetic measures ; and 4 ) framing information . The IDEA model [ 17 ] uses the notion of the impact a creation has , rather than that of creation value , and suggests six stages of development of a com - putational creativity system , based on the difference between the information given as knowledge to the system and the artifacts it generates . Other authors use human evaluation ( e . g . , Williams and McOwan [ 18 ] ) , asking users to rate the products of computational creativity systems , or to choose words which appropriately describe their reaction to those products . Yet other thoughts on evaluation can be seen in the move against mere generation commented upon by Ventura [ 19 ] , Jordanous’ SPECS methodology [ 20 ] , the discussion on creativity versus the perception of creativity and Colton’s creative tripod [ 21 ] , etc . Although the ﬁeld of computational creativity evaluation has made signiﬁcant progress , none of the types of evaluation above shines a bright light on cognitive creative processing . Some steps in the direction of human cognition are being made , with some research addressing human tasks [ 22 ] , and models being based on a psychological theory of communicative interaction [ 23 ] . On the other disciplinary side of creativity studies—that of human cognition—human creative problem - solving , or processes considered to take part in it ( like divergent thought ) , are evaluated with various creativity tasks . Some of these are the following : 1 ) the Remote Associates Test ( RAT ) [ 5 ] ; 2 ) the Alternative Uses Test [ 6 ] ; 3 ) the Torrance Creativity Tests ( reviewed by Kim [ 7 ] ) ; 4 ) the Wallach – Kogan tests [ 8 ] ; 5 ) insight tests ( like the ones described by [ 9 ] and [ 10 ] ) . The human responses in such tasks are generally evaluated in terms of various groups of the following metrics . Some of these met - rics assess a particular expected correct answer , others are used for assessing open - ended answers . 1 ) Success : Achieving or not achieving a solution ( this metric is particularly useful for hard insight tasks ) . 2 ) Response time for a particular solution . 3 ) Difﬁculty of solving a particular problem item as a percentage of the population solving it . 4 ) Fluency : In open - ended tests , measures how many different items the participant has come up with as an answer . 5 ) Flexibility : Measures how many semantically different domains the answers to a particular item cover . 6 ) Elaboration : Assesses the amount of detail contained in the various answers . 7 ) Originality : Responses given by a small percentage of the participants are rated as unusual . 8 ) Novelty : Human judges are asked to assess answers given by human participants on a novelty scale ( used for the Alternative Uses Test [ 6 ] and the Wallach – Kogan test [ 8 ] ) . One can then check for the validity of the novelty judgement by exploring the agreement between the assessments of the various judges . Furthermore , the literature addressing these tests sheds insight into the cognitive processing behind such tasks . III . I MPORTANCE OF C OGNITIVE P ROCESSING IN C REATIVE P ROBLEM - S OLVING S YSTEMS Various cognitive processes are said to account for creative problem - solving , with one of the most modeled processes being analogy ( with models like Analogy [ 24 ] , many are called but few are chosen [ 25 ] , learning and inference with schemas and analogies [ 26 ] , structural tensor analogical reasoning [ 27 ] , Copycat [ 28 ] , etc . ) . Another creative process which has recently gathered investigative interest is that of conceptual blending [ 29 ] . Concept blending is deﬁned by Fauconnier and Turner [ 29 ] as a basic mental operation of constructing a partial match between two inputs , projecting selectively from those inputs into a “blended space” which has as a result the emergence of a new structure and of new meaning . The Coinvent project investigated and computationally instantiated this process [ 30 ] . Various theories account for insight problem - solving [ 31 ] : for example , this type of problem solving is assumed to include four stages : 1 ) preparation ; 2 ) incubation ; 3 ) illumination or insight ; and 4 ) veriﬁcation or evaluation . An important process which is considered part of insightful problem solving is that of rerepresentation . In order to build artiﬁcial cognitive creative agents , one can use this literature to build computational mechanisms akin to the cognitive processes involved in solving creativity tasks . Olte¸teanu’s opinion [ 32 ] , the cognitive processes of association , use of similarity , structure , and rerepresentation are to be always kept in mind . Associations are important in creative problem - solving due to their ability to bring new material into the problem space for the solver . Creative problem spaces are ambiguous and therefore can beneﬁt from ﬂuidity . Associations can easily be made based on similarity or context , as follows . Associations can be made by context , as encoun - tering certain items ( a , b , c , d ) constantly together generally produces the cognitive result of triggering the other items ( like c and / or d ) when some of the items ( a , b ) have been shown . Associations can be made by similarity , as similarity of features might imply similarity of affordance ( e . g . , if you know you can kick footballs , you might want to try playing in the same way with any spherical object of similar weight and material ) . All such processes can help bring more useful items into the prob - lem space and allow rerepresentation . Knowledge of structure can allow structure - mapping into different domains [ 33 ] , replacement of structure parts [ 34 ] , navigation between similar structures , and structure - based operations ( merging , overlap , removal of unnecessary parts , etc . ) [ 32 ] . These allow further ways of using old knowledge creatively in order to produce new , useful and interesting knowledge . Such cognitive processes need to be replicated in artiﬁcial cognitive systems in order to : 1 ) enable them to assist humans in processes of rerepresentation and creative problem - solving and 2 ) explain their own creative productions in ways which make sense to humans . An initial requirement in replicating and reﬁning such processes is the ability to build systems capable of giving results comparable to humans in various creativity tests . IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 471 IV . C ASE S TUDIES This section summarizes two case studies of cognitive systems which yield results comparable to human participants in creativ - ity tests . Section IV - A presents computational RAT ( comRAT ) , a cognitive system which can give comparable answers to humans in the RAT [ 35 ] . Section IV - B presents object replacement object composition ( OROC ) , a prototype system capable of giving similar answers to humans to the Alternative Uses Test [ 34 ] . A . Case 1—comRAT—Remote Associates Test Solver 1 ) Remote Associates Test : The RAT by Mednick and Mednick [ 5 ] is a creativity test in which participants are given three word items and required to produce a fourth , which is associated with all three of them in some way . For example , the words CREAM , SKATE , and WATER are given . A correct answer to this query would be the word ICE . Various types of RAT can be distinguished , depending on the type of associations at play [ 36 ] —structural or compound items reﬂect language based relationships , and functional items reﬂect relationships beyond language . The RAT has been widely used as an empirical measurement of creativity [ 37 ] , [ 38 ] and translated 1 into multiple languages [ 39 ] – [ 42 ] . The type of data acquired after empirical testing includes the per - centage of participants solving a particular test item and response speed . Useful sources of normative data are available [ 43 ] . 2 ) Description of the Knowledge Acquisition and Solving Process for the Artiﬁcial Cognitive System Solver : A comRAT was presented by Olte¸teanu and Falomir [ 35 ] , [ 44 ] , using language data and a convergence process , which will further be brieﬂy summarized . The language data used consisted of the 1048 720 most frequent 2 - grams from the Corpus of Contemporary American English . 2 The University of Lancaster—Constituent Likelihood Automatic Word - tagging System ( UCREL CLAWS7 ) tagset 3 provides grammatical tags to words in bodies of continuous text . Based on this tagset , 205602 2 - grams with relevant tags for the task were kept ( a com - plete list of the types of words kept based on their tag is provided by [ 35 ] ) . The comRAT system learned all the unique Concepts of the 2 - grams , and organized bidirectional Links between Concepts which appeared together in an Expression . When 3 - word RAT queries were given to the system , if known , the 3 Concepts were activated , together with the Links and Concepts associatively attached to them . The Concepts activated from most sides were considered as potential answers . The activation of the ini - tial Concepts thus converged upon possible answers for RAT queries through an associative process . 3 ) Description of Evaluation With Human Data : The comRAT system was given the 144 items from the Bowden and Jung - Beeman [ 43 ] normative data to solve , thus being tested with the exact same queries as humans . The compu - tational RAT found the correct answer provided in the normative data for 47 out of the 48 items for which it had known all 3 initial Expression items . The system solved another 17 queries for which it only knew 2 initial items . From a cognitive point of view , computational systems can be used to model the human process and 1 Translation of the RAT is not a trivial task , as an RAT item in English will rarely translate in an RAT item in Italian , for example , as associates between the query words and the answer might not exist , or be reasonable compounds . In that context , a translation of the RAT generally means creating a set of suitable items for the test in that language , and checking for the validity of those items . The compound RAT thus depends on language and language use , whereas the functional RAT reﬂects semantic relations within a language . 2 http : / / corpus . byu . edu / coca / 3 http : / / ucrel . lancs . ac . uk / claws7tags . html obtain similar performance , rather than solve tasks to perfection . When solving this task , comRAT is limited in its ability to come up with an answer by its existing knowledge , as human participants would also be . In 26 cases , comRAT comes up with other plausible answers—for example it answers the query M ILL , T OOTH , D UST , for which the correct answer provided by the normative data is S AW , with the plausible answer G OLD ; the query C RY , F RONT , S HIP , for which the correct answer from the normative data is B ATTLE , is given the plausible answer W AR , etc . Furthermore , using data on the frequency of 2 - grams , the proba - bility of the system to ﬁnd an answer has been found to correlate with the difﬁculty of the RAT queries for humans . The probability to ﬁnd an answer given each query word is calculated as the ratio of favorable cases in which the query word and answer word appear linked together , over the total number of cases in which the query word appears linked to all Concepts . The initial calculation consid - ered each query word to have an equal inﬂuence in determining the probability of the answer , however , this can be further modiﬁed to account for possible order effects . Difﬁculty of query for humans was understood to be represented in the normative data by human response time and percentage of participants solving the query . A sig - niﬁcant moderate correlation between difﬁculty of query for humans and comRAT’s probability of ﬁnding an answer was observed , as follows . The correlation between accuracy and comRAT’s probabil - ity was r = 0 . 49 , p < 0 . 002 for mean solution time in 30s—thus the higher the probability , the more accurate people were on average in answering the query . The inverse correlation between response times and comRAT’s probability was r = − 0 . 52 , p < 0 . 001 for mean solu - tion time in 30s—thus the higher the probability , the less time people took on average to solve the query . This shows that the process of association and convergence used by comRAT to solve the task has high chances of providing fur - ther insights into future cognitive models of the task . As the system can come up with plausible answers even when lacking the knowl - edge to respond correctly , this demonstrates possible further uses of associative processes in order to ensure ﬂexibility and robust - ness in knowledge - based systems . The system further showed an ability to give multiple answers to some queries—for example , to query H IGH , D ISTRICT , H OUSE , it could provide answers S CHOOL , C OURT , O FFICE , W ATER , and others . Answer S CHOOL ( which is also the only answer provided as correct in Bowden and Jung - Beeman’s normative dataset ) “won” via its higher probability in comRAT . This capacity for multiple answers based on knowledge acquired from human data will further allow the cognitive empirical study of the reasons why some answers are preferred by humans over others , and the links between this and frequency of known expressions . The comRAT solver can be used to generate new RAT queries . RAT items can thus be generated for future empirical work with human participants , controlling for semantic tag , position in the 2 - gram of the given and answer word , frequency of the various query items and probability of ﬁnding the answer . This allows the gener - ation of sets of RAT queries in which different variables are kept constant , thus making comRAT a future useful tool for the cognitive modeling community . The comRAT solver and its principles have also been used to translate the RAT task to the visual domain . Though a validation of these items is yet in the making , comRAT could thus help examine creativity tests in a multimodal perspective [ 45 ] . The cognitive process of association and the cognitive data on frequency of two grams and other types of associates are thus used to create an artiﬁcial cognitive system which has many future possible applications in the cognitive modeling community . Using comRAT’s ability to generate and solve queries will provide more knowledge 472 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 about how the RAT is solved by humans , which can help improve comRAT further , and possibly tackle a wider array of association - based tasks in the future . B . Case 2—Object Replacement , Object Composition System Which Can Solve the Alternative Uses Test 1 ) Alternative Uses Test : The Alternative Uses Test [ 6 ] takes the following form : participants are given the name of an object item ( e . g . , Brick ) , and asked to come up with as many different uses as they can for that item , in a set amount of time ( this amount varies from an empirical investigation to another—generally between 1 min and 3 min ) . Then , the participants proceed on doing the same with the next item , etc . The evaluation of the Alternative Uses Test is done on Fluency , Flexibility , and Originality or Novelty . Normative data for the Alternative Uses Test can be easily obtained in relation to a set of objects , for example objects from the house - hold domain . Such data should include human Fluency , Flexibility , and Originality or Novelty , as well as order of responses—which might later be useful in understanding the types of creative processes employed to generate the alternative uses . Evaluations on the same types of metrics as for human answers can be made for the artiﬁcial cognitive system ( on Fluency , Flexibility , and Originality or Novelty ) . Furthermore , the processes employed by the system can be compared to those employed by humans , as observed in think aloud protocols [ 46 ] . If normative order of responses has been obtained , then an analysis can be performed on the inﬂuence various object features have over coming up with new uses . 2 ) Description of the Knowledge Acquisition for the Artiﬁcial Cognitive System Solver : An OROC system was deemed able to give similar answers as humans to the Alternative Uses Test [ 34 ] . OROC had a knowledge base of 70 simple and 20 composed objects . These objects are described through their various features ( name , material , shape , affordance , and size ) . Their features were manually encoded from descriptions of the object , and considered common sense knowledge . OROC makes creative inferences in a cognitively - inspired process—determining similarity of affordance based on similarity of other features—e . g . , if object a has affordance aff a , than objects b and c can be proposed as having the same affor - dances or being suitable replacements if they have similar ( functional ) properties . 3 ) Description of Evaluation With Human Data : Five objects were selected from a household items domain on which OROC was to deploy its creative inference of affordance . These objects were : Cup , Newspaper , Toothbrush , Carpet , and Dental Floss . Thus , for an object like Cup , OROC could come up with creative inferences about its affordances , e . g . , A cup can be used for putting ﬂowers in ( based on its similarity of shape with known object Vase ) . The ﬂuency and ﬂexibility of OROC’s answers to these ﬁve objects—a total of 30 alternative object use statements—was assessed in the same manner in which the answer would have been assessed if provided by humans which were undergoing the Alternative Uses Test . Human judges were then employed to assess Novelty , Usefulness and Likability on a 1 - 7 Likert scale , where 1 represented the lower bound , for example “not at all novel , ” and 7 the upper bound , e . g . , “highly novel . ” Usefulness and Likability were new met - rics added in this assessment besides the normally used Novelty metric . These were meant to split the feedback from the judges in dif - ferent dimensions , and thus dispel possible inﬂuences between how much a judge liked a particular use and how much they found it to be useful or novel . Human judges were not informed that they were assessing the creative answers of an artiﬁcial system . The most novel , useful and likable items were thus classiﬁed—for example , the alterna - tive use response judged as demonstrating most novelty was Dental ﬂoss may be used to hang clothes to dry . After evaluation , the system demonstrated similar ratings as those obtained by answers given by human participants . The answers provided by OROC were rated as follows : mean Novelty 3 . 79 ( SD = 1 . 69 ) , mean Likability 3 . 31 ( SD = 1 . 68 ) , mean Usefulness 3 . 77 ( SD = 1 . 7 ) . Human answers from Gilhooly et al . [ 46 ] are rated only for Novelty : 2 . 54 ( SD = 0 . 74 ) in a think aloud group and 2 . 45 ( SD = 0 . 51 ) in a silent group , respectively . As a side result , a correlation between ratings on Usefulness and Likability was observed in human judges : r = 0 . 63 , p < 0 . 005 . Note that , when asked to provide their ratings on Novelty , Usefulness and Likability , the judges were not provided with speciﬁc deﬁnitions , thus they were guided by their own subjective deﬁnition of these dimensions , making this evaluation procedure simi - lar to the consensual assessment technique [ 47 ] , in which independent individuals make subjective judgements on products in domains they are familiar with . As deﬁnitions of these dimensions are not a priori provided , creativity assessments are more likely to mirror assessments in the real world , a more accurate measure of interjudge reliability is obtained and the deﬁnitions of the experimenter are not imposed onto the judges . OROC’s processes of proposing new object affordances were also assessed and showed comparability to the cognitive processes deployed by humans when solving the Alternative Uses Test , as explored by a think aloud protocol by [ 46 ] . More importantly , these forms of evaluation demonstrated : 1 ) the ability to apply similar tech - niques in assessing creative solving of human and artiﬁcial cognitive systems and 2 ) comparability of process which can be used for further empirical and computational investigations . It is worth noting that OROC was not implemented with the Alternative Uses Test in mind , but for testing a mechanism from a cognitive creative problem solving theoretical framework [ 32 ] , [ 48 ] . This mechanism had the general purpose of being able to replace object a with an object b with similar functionality , for cases when object a was in its knowledge base , but not in the environment . Part of OROC’s abilities , like that of composing new objects , remain presently untested via comparison to a human creativity task counterpart . V . G ENERAL A PPROACH A general approach can be formulated , based on the above examples , for producing artiﬁcial cognitive systems that can yield comparable results to humans in creativity tests , and aim for cog - nitive or cognitively - inspired processes and cognitive knowledge acquisition . This approach involves several steps : 1 ) Choosing a human creativity test , the results of which are to be replicated via a cognitive system , or choosing a cre - ative problem - solving skill that is more general and has some empirical adjacent validation possible . 2 ) Finding a source of knowledge for cognitive knowledge acqui - sition , or gathering such knowledge from human participants . 3 ) Implementing a system which uses processes similar to ( or is able to produce results similar to ) cognitive creative processes , like association , rerepresentation , use of context , structure and similarity , etc . Implementing types of knowledge organization which are cognitively inspired , or which might yield further cognitive results . 4 ) Finding or obtaining human normative data for that particular test or general task . 5 ) Evaluating the results of the artiﬁcial cognitive system using : a ) human normative data and / or b ) evaluation techniques used for assessing the human creativity task . Supplementary , com - putational creativity evaluation procedures can also be applied . IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 473 TABLE I S HOWING A PPLICATION OF THE C OMPARABILITY A PPROACH TO V ARIOUS C REATIVITY T ESTS AND T ASKS 6 ) Deploying data analysis measures which enable new possible relations of scientiﬁc interest to be observed . Such measures can be deployed on computational data , on empirical data and on the computational - empirical comparison . 7 ) Enabling the artiﬁcial cognitive system with generative abili - ties for that particular test or task ( if possible ) —i . e . , with the possibility of not only solving a particular creative or creative problem - solving task , but also with the ability of creating more tasks of this type . This will allow for new empirical testing of human participants with controlled variables , to further reﬁne hypotheses about creative processes . This approach and its essential steps are shown in Fig . 1 . VI . A PPLICABILITY OF T HESE P RINCIPLES TO O THER C REATIVITY T ESTS In this section , the Wallach – Kogan instances test [ 8 ] and insight tests [ 9 ] , [ 10 ] are proposed as two other possible creativity tests on which this approach could be applied . Sections VI - A and VI - B describe these tests and particularities of applying this approach to implementing artiﬁcial cognitive systems for each of them . Table I gives an overview of the application of the general approach to the previously mentioned case studies ( Section IV - A ) , Section IV - B , and to the other two proposed creativity tests ( Section VI - A ) , Section VI - B and corresponding skills . A . Wallach – Kogan Instances Test The Wallach – Kogan instances test [ 8 ] gives participants a speciﬁc property or component , and requires them to enumerate as many items as they can which have that property or contain that component ( e . g . , items that are green , that make noise , that have wheels , etc . ) . Fig . 1 . General approach steps . As shown in the fourth column of Table I , data can be acquired by giving the Wallach – Kogan test to human participants , with stim - uli consisting of sets of oftenly encountered properties and object parts . Thus , normative data for ﬂexibility , ﬂuency , and originality of answers can be gathered . Human judges can be used for Novelty and Elaboration ratings . Cognitive knowledge acquisition can be per - formed by either : 1 ) asking human participants to describe objects in terms of their deﬁning properties and components or 2 ) by machine learning approaches directed at acquiring such property - object or component - object co - ocurrences from text corpora . 474 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 A computational cognitive agent that solves the Wallach – Kogan instances test could also be developed to have generative abilities , thus to create queries based on controlling for the frequency of rela - tions between properties ( or components ) and objects . Giving human participants queries in which the frequency of object – properties and object – components relation is controlled would help us investigate : 1 ) whether the speed of processes like property - based search and component - based search of objects is the same or not ; 2 ) whether different types of features - based queries are easier or harder than others ; and 3 ) whether the number of components in the object inﬂuences performance , etc . This in turn will help us build more cognitively - informed creative problem - solving agents . B . Insight Tests Insight tests , like the ones by Maier [ 9 ] and Duncker [ 10 ] require a larger amount of knowledge and heuristics in order to be successfully solved and implemented . Addressing insight tests that require object knowledge before those which require abstract knowledge might be a productive scalable strategy . Such a strategy could for example employ data on objects and features that has been obtained for the Alternative Uses Test , or the Wallach – Kogan test . Object affordances are required for solving practical insight tests as well . Tasks can be given over crowdsourcing platforms to acquire affordances of sets of commonly used objects . Data mining strate - gies can be used to extract , from text corpora , sets of objects , paired with the verbs that are used in conjunction with them—a subset of which will constitute the object’s affordances . It is sensible to con - sider that at least a subset of the problem templates that pertain to objects will be constructed from such affordances . This knowledge can be complemented with the acquisition of object - related problem templates and common heuristics employed by human participants when approaching practical object insight tests . Such acquisition might require the analysis of think - aloud protocol data of human participants solving insight problems . Normative data on object insight tests can be acquired from exist - ing sources [ 49 ] , or via meta - analysis on existing results on different tests . Such meta - analysis might be necessary because insight tests generally require a large amount of time to be deployed , thus the empirical data from one source is most often restricted to only a few insight tests , or even to one . Among the multiple comparison and evaluation tools that can be employed , one of the most interesting ones is that of comparing the use , switch between , and creation of new problem - templates . An arti - ﬁcial cognitive agent performing such tasks of rerepresentation would undoubtebly prove of much interest for advancing the state of the art on creative reasoning techniques . Generative abilities in a system which can solve insight problems , even in a restricted domain , would provide a reliable way to create and modify a larger set of insight problems . VII . C OGNITIVE K NOWLEDGE A CQUISITION F ROM C REATIVITY T ESTS Some human creativity tests can be used to provide knowledge bases for artiﬁcial cognitive systems . For example , the Wallach – Kogan test will yield a set of data pertaining to object properties and object parts , as for each property or object part various objects having that property or that object part are asked for from the participant . These answers can be used in the knowledge base of artiﬁcial systems , in the context of other tests—like the Alternative Uses Test—where various properties are required to be known by the sys - tem , or the object insight tests , in which properties might be relevant for future affordances in solving object tasks . Furthermore , common answers in the Wallach – Kogan test can be considered as high - frequency associates , and models can be built to interpret the frequency of occuring answers in the Wallach – Kogan test or their ordering as weights of associative links . Similarly , giving human participants a set of object - related insight problems using the think aloud protocol will provide knowledge on problem templates used ( even if some of them are not productive for that particular problem ) , from which observations can be made on how such templates are constructed . VIII . D ISCUSSION As shown in the previous case studies ( Sections IV - A and IV - B ) , this approach is useful from both an Artiﬁcial Intelligence and a cognitive science perspective . From the AI perspective , new techniques can be developed out of the inspiration of human creative cognitive processing . From the cognitive science perspective , these systems can further be used by cognitive psychologists for the more detailed cognitive modeling of creative tasks , the implementation and testing of various theoretical hypotheses on how creative processes unfold and how knowledge organization sustains such processes . New relations can be observed during the implementation of systems capable of comparable results in data analysis , like the corre - lation between the probability of ﬁnding an answer and the difﬁculty of query for the ( RAT , comRAT ) test – system pair , or the relationship between Usefulness and Likability in the Alternative Uses Test . In order to bridge the gap between computational creativity and empirical research on human creativity , we must aim to address not only artifact creating systems , but also problem - solving and creative reasoning systems , and systems which can give answers comparable to humans in creativity tests . This will allow the further study of the creative process , in both artiﬁcial and natural cognitive systems . Steps toward bridging this gap should involve the following : 1 ) The use of cognitive processes and types of knowledge orga - nization as an inspiration to some computational creativity systems . 2 ) The use of computational creativity systems which have been tightly inspired by cognitive processes as potential models or tools to investigate human creativity processes . 3 ) The use of cognitive knowledge acquisition techniques to inform the knowledge bases of such systems . 4 ) Making results of some computational creativity systems and human creativity tasks directly comparable ; this includes both comparing the results of systems ( which can act as models ) to human normative data , and the evaluation of computa - tional creativity answers through techniques reserved for human participants . 5 ) The use of cognitive computational systems and models to gen - erate rich new sets of controlled queries , which allow a deeper investigation of the human creative problem solving process . In conclusion , this paper has presented a general approach to building cognitive computational creativity systems which can give compara - ble answers to humans when solving human creativity tests . A set of steps was laid down as possible methodology when approaching such systems . The application of these steps to various creativity tests was brieﬂy explored and two case studies of systems which realize this approach were presented . As further work we plan to : 1 ) implement and test this approach on the new cases described and 2 ) explore the applications of this approach to assistive systems—thus obtaining a deeper understanding of how natural cognitive systems can beneﬁt and learn from the input of artiﬁcial creative cognitive systems . IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS , VOL . 10 , NO . 2 , JUNE 2018 475 R EFERENCES [ 1 ] S . Colton , J . Goodwin , and T . Veale , “Full - FACE poetry generation , ” in Proc . 3rd Int . Conf . Comput . Creativity , Dublin , Ireland , 2012 , pp . 95 – 102 . [ 2 ] P . M . Todd and E . R . Miranda , “Putting some ( artiﬁcial ) life into models of musical creativity , ” in Musical Creativity : Multidisciplinary Research in Theory and Practice , I . Deliège and G . Wiggins , Eds . New York , NY , USA : Psychol . Press , 2006 , pp . 376 – 396 . [ 3 ] S . Colton , “The painting fool : Stories from building an automated painter , ” in Computers and Creativity . Heidelberg , Germany : Springer , 2012 , pp . 3 – 38 . [ 4 ] D . B . Lenat , “AM : An artiﬁcial intelligence approach to discovery in mathematics as heuristic search , ” Dept . Comput . Sci . , Stanford Univ . , Stanford , CA , USA , Tech . Rep . DTIC STAN - CS - 76 - 570 , 1976 . [ 5 ] S . A . Mednick and M . Mednick , Remote Associates Test : Examiner’s Manual . Boston , MA , USA : Houghton Mifﬂin , 1971 . [ 6 ] J . P . Guilford , The Nature of Human Intelligence . New York , NY , USA : McGraw - Hill , 1967 . [ 7 ] K . H . Kim , “Can we trust creativity tests ? A review of the torrance tests of creative thinking ( TTCT ) , ” Creativity Res . J . , vol . 18 , no . 1 , pp . 3 – 14 , 2006 . [ 8 ] M . A . Wallach and N . Kogan , Modes of Thinking in Young Children : A Study of the Creativity - Intelligence Distinction . New York , NY , USA : Rinehart and Winston , 1965 . [ 9 ] N . R . F . Maier , “Reasoning in humans . II . The solution of a problem and its appearance in consciousness , ” J . Compar . Psychol . , vol . 12 , no . 2 , pp . 181 – 194 , 1931 . [ 10 ] K . Duncker , “On problem - solving , ” Psychol . Monographs , vol . 58 , no . 5 , p . 270 , 1945 . [ 11 ] S . Hélie and R . Sun , “Incubation , insight , and creative problem solving : A uniﬁed theory and a connectionist model , ” Psychol . Rev . , vol . 117 , no . 3 , pp . 994 – 1024 , 2010 . [ 12 ] G . A . Wiggins , “Towards a more precise characterisation of cre - ativity in AI , ” in Proc . Case Based Reasoning Papers From Workshop Programme ( ICCBR ) , vol . 1 . Vancouver , BC , Canada , 2001 , pp . 113 – 120 . [ 13 ] G . Ritchie , “Assessing creativity , ” in Proc . AISB Symp . , York , U . K . , 2001 . [ 14 ] M . A . Boden , The Creative Mind : Myths and Mechanisms . New York , NY , USA : Routledge , 2003 . [ 15 ] G . Ritchie , “Some empirical criteria for attributing creativity to a computer program , ” Minds Mach . , vol . 17 , no . 1 , pp . 67 – 99 , 2007 . [ 16 ] A . Pease , D . Winterstein , and S . Colton , “Evaluating machine creativity , ” in Proc . Workshop Creative Syst . 4th Int . Conf . Case Based Reasoning , Atlanta , GA , USA , 2001 , pp . 129 – 137 . [ 17 ] S . Coltonm , J . Charnley , and A . Pease , “Computational creativity theory : The FACE and IDEA descriptive models , ” in Proc . 2nd Int . Conf . Comput . Creativity , Mexico City , Mexico , 2011 , pp . 90 – 95 . [ 18 ] H . Williams and P . W . McOwan , “Magic in the machine : A computa - tional magician’s assistant , ” Frontiers Psychol . , vol . 5 , Nov . 2014 . [ 19 ] D . Ventura , “Mere generation : Essential barometer or dated concept ? ” in Proc . 7th Int . Conf . Comput . Creativity , Paris , France , 2016 , pp . 17 – 24 . [ 20 ] A . Jordanous , “A standardised procedure for evaluating creative systems : Computational creativity evaluation based on what it is to be creative , ” Cogn . Comput . , vol . 4 , no . 3 , pp . 246 – 279 , 2012 . [ 21 ] S . Colton , “Creativity versus the perception of creativity in compu - tational systems , ” in Proc . AAAI Spring Symp . Creative Intell . Syst . , Stanford , CA , USA , 2008 , pp . 14 – 20 . [ 22 ] D . A . Joyner et al . , “Using human computation to acquire novel methods for addressing visual analogy problems on intelligence tests , ” in Proc . 6th Int . Conf . Comput . Creativity , Park City , UT , USA , Jun . / Jul . 2015 , pp . 23 – 30 . [ 23 ] E . R . Miranda , “Mimetic development of intonation , ” in Music and Artiﬁcial Intelligence . Heidelberg , Germany : Springer , 2002 , pp . 107 – 118 . [ 24 ] T . G . Evans , “A heuristic program to solve geometric - analogy problems , ” in Proc . Spring Joint Comput . Conf . , Washington , DC , USA , Apr . 1964 , pp . 327 – 338 . [ 25 ] K . D . Forbus , D . Gentner , and K . Law , “MAC / FAC : A model of similarity - based retrieval , ” Cogn . Sci . , vol . 19 , no . 2 , pp . 141 – 205 , 1995 . [ 26 ] J . E . Hummel and K . J . Holyoak , “Distributed representations of structure : A theory of analogical access and mapping , ” Psychol . Rev . , vol . 104 , no . 3 , pp . 427 – 466 , 1997 . [ 27 ] G . S . Halford et al . , “Connectionist implications for processing capacity limitations in analogies , ” in Advances in Connectionist and Neural Computation Theory , vol . 2 . Norwood , NJ , USA : Ablex , 1994 , pp . 363 – 415 . [ 28 ] D . R . Hofstadter and M . Mitchell , “The Copycat project : A model of mental ﬂuidity and analogy - making , ” in Advances in Connectionist and Neural Computation Theory , vol . 2 . Norwood , NJ , USA : Ablex , 1994 , pp . 29 – 30 . [ 29 ] G . Fauconnier and M . Turner , “Conceptual integration networks , ” Cogn . Sci . , vol . 22 , no . 2 , pp . 133 – 187 , 1998 . [ 30 ] M . Schorlemmer et al . , “COINVENT : Towards a computational concept invention theory , ” in Proc . 5th Int . Conf . Comput . Creativity , Ljubljana , Slovenia , 2014 , pp . 288 – 296 . [ 31 ] W . H . Batchelder and G . E . Alexander , “Insight problem solving : A crit - ical examination of the possibility of formal theory , ” J . Problem Solving , vol . 5 , no . 1 , pp . 56 – 100 , 2012 . [ 32 ] A . - M . Olte¸teanu , Two General Classes in Creative Problem - Solving ? An Account Based on the Cognitive Processes Involved in the Problem Structure—Representation Structure Relationship , Inst . Cogn . Sci . , Osnabrück , Germany , 2014 . [ 33 ] D . Gentner , “Structure - mapping : A theoretical framework for analogy , ” Cogn . Sci . , vol . 7 , no . 2 , pp . 155 – 170 , 1983 . [ 34 ] A . - M . Olte¸teanu and Z . Falomir , “Object replacement and object compo - sition in a creative cognitive system . Towards a computational solver of the alternative uses test , ” Cogn . Syst . Res . , vol . 39 , pp . 15 – 32 , Sep . 2016 . [ 35 ] A . - M . Olte¸teanu and Z . Falomir , “comRAT - C : A computational com - pound remote associates test solver based on language data and its comparison to human performance , ” Pattern Recognit . Lett . , vol . 67 , pp . 81 – 90 , Dec . 2015 . [ 36 ] B . R . Worthen and P . M . Clark , “Toward an improved measure of remote associational ability , ” J . Educ . Meas . , vol . 8 , no . 2 , pp . 113 – 123 , 1971 . [ 37 ] P . I . Ansburg , “Individual differences in problem solving via insight , ” Current Psychol . , vol . 19 , no . 2 , pp . 143 – 146 , 2000 . [ 38 ] J . Dorfman , V . A . Shames , and J . F . Kihlstrom , “Intuition , incubation , and insight : Implicit cognition in problem solving , ” in Implicit Cognition . Oxford , U . K . : Oxford Univ . Press , 1996 , pp . 257 – 296 . [ 39 ] Y . Baba , “An analysis of creativity by means of the remote associates test for adult revised in Japanese ( JARAT FORM A ) , ” Jpn . J . Psychol . , vol . 52 , no . 6 , pp . 330 – 336 , 1982 . [ 40 ] M . A . Hamilton , “‘Jamaicanizing the mednick remote associates test of creativity , ” Perceptual Motor Skills , vol . 55 , no . 1 , pp . 321 – 322 , 1982 . [ 41 ] B . Nevo and I . Levin , “Remote associates test : Assessment of creativity in Hebrew , ” Megamot , vol . 24 , pp . 87 – 98 , 1978 . [ 42 ] S . A . Chermahini , M . Hickendorff , and B . Hommel , “Development and validity of a Dutch version of the remote associates task : An item - response theory approach , ” Thinking Skills Creativity , vol . 7 , no . 3 , pp . 177 – 186 , 2012 . [ 43 ] E . M . Bowden and M . Jung - Beeman , “Normative data for 144 compound remote associate problems , ” Behav . Res . Methods Instrum . Comput . , vol . 35 , no . 4 , pp . 634 – 639 , 2003 . [ 44 ] A . - M . Olte¸teanu and Z . Falomir , “Towards a remote associate test solver based on language data , ” in Artiﬁcial Intelligence Research and Development ( Frontiers in Artiﬁcial Intelligence and Applications ) , vol . 269 , L . M . Cabedo , O . Pujol , and N . Agell , Eds . Amsterdam , The Netherlands : IOS Press , 2014 , pp . 249 – 252 . [ 45 ] A . - M . Olte¸teanu , B . Gautam , and Z . Falomir , “Towards a visual remote associates test and its computational solver , ” in Proc . 3rd Int . Workshop Artif . Intell . Cognition ( CEUR - Ws ) , vol . 1510 . Turin , Italy , 2015 , pp . 19 – 28 . [ 46 ] K . Gilhooly , E . Fioratou , S . Anthony , and V . Wynn , “Divergent think - ing : Strategies and executive involvement in generating novel uses for familiar objects , ” Brit . J . Psychol . , vol . 98 , no . 4 , pp . 611 – 625 , 2007 . [ 47 ] B . A . Hennessey and T . M . Amabile , “Consensual assessment , ” in Encyclopedia of Creativity , vol . 1 . San Diego , CA , USA : Academic Press , 1999 , pp . 347 – 359 . [ 48 ] A . - M . Olte¸teanu , “From simple machines to Eureka in four not - so - easy steps : Towards creative visuospatial intelligence , ” in Fundamental Issues of Artiﬁcial Intelligence ( Synthese Library ) , vol . 376 , V . C . Müller , Ed . Cham , Switzerland : Springer , 2016 , pp . 159 – 180 . [ 49 ] M . K . Jacobs and R . L . Dominowski , “Learning to solve insight problems , ” Bull . Psychonomic Soc . , vol . 17 , no . 4 , pp . 171 – 174 , 1981 . Ana - Maria Olte¸teanu photograph and biography not available at the time of publication . Zoe Falomir photograph and biography not available at the time of publication . Christian Freksa photograph and biography not available at the time of publication .