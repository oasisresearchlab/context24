a r X i v : 1503 . 01243v1 [ s t a t . M L ] 4 M a r 2015 A Diﬀerential Equation for Modeling Nesterov’s Accelerated Gradient Method : Theory and Insights Weijie Su ∗ Stephen Boyd † Emmanuel J . Cand ` es ‡ Abstract We derive a second - order ordinary diﬀerential equation ( ODE ) which is the limit of Nesterov’s accelerated gradient method . This ODE exhibits approximate equivalence to Nesterov’s scheme and thus can serve as a tool for analysis . We show that the continuous time ODE allows for a better understanding of Nesterov’s scheme . As a byproduct , we obtain a family of schemes with similar convergence rates . The ODE interpretation also suggests restarting Nesterov’s scheme leading to an algorithm , which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex . Keywords . Nesterov’s accelerated scheme , convex optimization , ﬁrst - order methods , diﬀeren - tial equation , restarting 1 Introduction In many ﬁelds of machine learning , minimizing a convex function is at the core of eﬃcient model estimation . In the simplest and most standard form , we are interested in solving minimize f ( x ) , where f is a convex function , smooth or non - smooth , and x ∈ R n is the variable . Since Newton , numerous algorithms and methods have been proposed to solve the minimization problem , notably gradient and subgradient descent , Newton’s methods , trust region methods , conjugate gradient methods , and interior point methods . First - order methods have regained popularity as data sets and problems are ever increasing in size and , consequently , there has been much research on the theory and practice of accelerated ﬁrst - order schemes . Perhaps the earliest ﬁrst - order method for minimizing a convex function f is the gradient method , which dates back to Euler and Lagrange . Thirty years ago , however , in a seminal paper Nesterov proposed an accelerated gradient method ( Nesterov , 1983 ) , which may take the following form : starting with x 0 and y 0 = x 0 , inductively deﬁne x k = y k − 1 − s ∇ f ( y k − 1 ) y k = x k + k − 1 k + 2 ( x k − x k − 1 ) . ( 1 ) ∗ Department of Statistics , Stanford University , Stanford CA 94305 wjsu @ stanford . edu † Department of Electrical Engineering , Stanford University , Stanford CA 94305 boyd @ stanford . edu ‡ Departments of Statistics and of Mathematics , Stanford University , Stanford CA 94305 candes @ stanford . edu 1 For any ﬁxed step size s ≤ 1 / L , where L is the Lipschitz constant of ∇ f , this scheme exhibits the convergence rate f ( x k ) − f ⋆ ≤ O (cid:18) k x 0 − x ⋆ k 2 sk 2 (cid:19) . ( 2 ) Above x ⋆ is any minimizer of f and f ⋆ = f ( x ⋆ ) . It is well - known that this rate is optimal among all methods having only information about the gradient of f at consecutive iterates ( Nesterov , 2004 ) . This is in contrast to vanilla gradient descent methods , which have the same computational complexity but can only achieve a rate of O ( 1 / k ) . This improvement relies on the introduction of the momentum term x k − x k − 1 as well as the particularly tuned coeﬃcient ( k − 1 ) / ( k + 2 ) ≈ 1 − 3 / k . Since the introduction of Nesterov’s scheme , there has been much work on the development of ﬁrst - order accelerated methods , see Nesterov ( 2004 , 2005 , 2007 ) for theoretical developments , and Tseng ( 2008 ) for a uniﬁed analysis of these ideas . Notable applications can be found in sparse linear regression ( Beck and Teboulle , 2009 ; Qin and Goldfarb , 2012 ) , compressed sensing ( Becker et al . , 2011 ) and , deep and recurrent neural networks ( Sutskever et al . , 2013 ) . In a diﬀerent direction , there is a long history relating ordinary diﬀerential equation ( ODEs ) to optimization , see Bloch ( 1994 ) , Helmke and Moore ( 1996 ) , Schropp and Singer ( 2000 ) , and Fiori ( 2005 ) for example . The connection between ODEs and numerical optimization is often established via taking step sizes to be very small so that the trajectory or solution path converges to a curve modeled by an ODE . The conciseness and well - established theory of ODEs provide deeper insights into optimization , which has led to many interesting ﬁndings . Notable examples include linear regression via solving diﬀerential equations induced by linearized Bregman iteration algorithm ( Osher et al . , 2014 ) , a continuous - time Nesterov - like algorithm in the context of control design ( D¨urr and Ebenbauer , 2012 ; D¨urr et al . , 2012 ) , and modeling design iterative optimization algorithms as nonlinear dynamical systems ( Lessard et al . , 2014 ) . In this work , we derive a second - order ODE which is the exact limit of Nesterov’s scheme by taking small step sizes in ( 1 ) ; to the best of our knowledge , this work is the ﬁrst to use ODEs to model Nesterov’s scheme or its variants in this limit . One surprising fact in connection with this subject is that a ﬁrst - order scheme is modeled by a second - order ODE . This ODE reads ¨ X + 3 t ˙ X + ∇ f ( X ) = 0 ( 3 ) for t > 0 , with initial conditions X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 ; here , x 0 is the starting point in Nesterov’s scheme , ˙ X ≡ d X / d t denotes the time derivative or velocity and similarly ¨ X ≡ d 2 X / d t 2 denotes the acceleration . The time parameter in this ODE is related to the step size in ( 1 ) via t ≈ k √ s . Expectedly , it also enjoys inverse quadratic convergence rate as its discrete analog , f ( X ( t ) ) − f ⋆ ≤ O (cid:18) k x 0 − x ⋆ k 2 t 2 (cid:19) . Approximate equivalence between Nesterov’s scheme and the ODE is established later in various perspectives , rigorous and intuitive . In the main body of this paper , examples and case studies are provided to demonstrate that the homogeneous and conceptually simpler ODE can serve as a tool for understanding , analyzing and generalizing Nesterov’s scheme . In the following , two understandings of Nesterov’s scheme are highlighted , the ﬁrst one on oscillations in the trajectories of this scheme , and the second on the peculiar constant 3 appearing in the ODE . 2 1 . 1 From Overdamping to Underdamping In general , Nesterov’s scheme is not monotone in the objective function value due to the introduction of the momentum term . Oscillations or overshoots along the trajectory of iterates approaching the minimizer are often observed when running Nesterov’s scheme . Figure 1 presents typical phenomena of this kind , where a two - dimensional convex function is minimized by Nesterov’s scheme . Viewing the ODE as a damping system , we obtain interpretations as follows . Small t . In the beginning , a large 3 / t leads the ODE to be an overdamped system , returning to the equilibrium without oscillating ; Large t . As t increases , the ODE with a small 3 / t behaves like an underdamped system , oscillating with the amplitude gradually decreasing to zero . As depicted in Figure 1a , in the beginning the ODE curve moves smoothly towards the origin , the minimizer x ⋆ . The second bullet provides partial explanation for the oscillations observed in Nesterov’s scheme at later stage . Although our analysis extends farther , it is similar in spirit to that carried in O’Donoghue and Cand ` es ( 2013 ) . In particular , the zoomed Figure 1b presents some butterﬂy - like oscillations for both the scheme and ODE . In relating , each overshoot in Figure 1b corresponds to a bump in Figure 1c . We observe also from Figure 1c that the periodicity captured by the bumps are very close to that of the ODE solution . In passing , it is worth mentioning that the solution to the ODE in this case can be expressed via Bessel functions , hence enabling quantitative characterizations of these overshoots and bumps , which are given in full detail in Section 3 . −0 . 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 −0 . 2 0 0 . 2 0 . 4 0 . 6 0 . 8 x 1 x 2 s = 1 s = 0 . 25 ODE ( a ) Trajectories . −0 . 06 −0 . 04 −0 . 02 0 0 . 02 0 . 04 0 . 06 0 . 08 −0 . 15 −0 . 1 −0 . 05 0 0 . 05 0 . 1 0 . 15 x 1 x 2 s = 0 . 25 s = 0 . 05 ODE ( b ) Zoomed trajectories . 0 50 100 150 200 250 300 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 t f − f * s = 1 s = 0 . 25 s = 0 . 05 ODE ( c ) Errors f − f ⋆ . Figure 1 : Minimizing f = 2 × 10 − 2 x 21 + 5 × 10 − 3 x 22 , starting from x 0 = ( 1 1 ) T . The black and solid curves correspond to the solution to the ODE . In ( c ) , for the x - axis we use the identiﬁcation between time and iterations , t = k √ s . 1 . 2 A Phase Transition The constant 3 , derived from ( k + 2 ) − ( k − 1 ) in ( 3 ) , is not haphazard . In fact , it is the smallest constant that guarantees O ( 1 / t 2 ) convergence rate . Speciﬁcally , parameterized by a constant r , the generalized ODE ¨ X + r t ˙ X + ∇ f ( X ) = 0 can be translated into a generalized Nesterov’s scheme that is the same as the original ( 1 ) except for ( k − 1 ) / ( k + 2 ) being replaced by ( k − 1 ) / ( k + r − 1 ) . Surprisingly , for both generalized ODEs and schemes , the inverse quadratic convergence is guaranteed if and only if r ≥ 3 . This phase transition suggests there might be deep causes for acceleration among ﬁrst - order methods . In particular , for r ≥ 3 , the worst case constant in this inverse quadratic convergence rate is minimized at r = 3 . 3 Figure 2 illustrates the growth of t 2 ( f ( X ( t ) ) − f ⋆ ) and sk 2 ( f ( x k ) − f ⋆ ) , respectively , for the generalized ODE and scheme with r = 1 , where the objective function is simply f ( x ) = 12 x 2 . Inverse quadratic convergence fails to be observed in both Figures 2a and 2b , where the scaled errors grow with t or iterations , for both the generalized ODE and scheme . 0 5 10 15 20 25 30 35 40 45 50 0 2 4 6 8 10 12 14 16 t t 2 ( f − f * ) ( a ) Scaled errors t 2 ( f ( X ( t ) ) − f ⋆ ) . 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 0 1 2 3 4 5 6 7 8 9 10 iterations sk 2 ( f − f * ) ( b ) Scaled errors sk 2 ( f ( x k ) − f ⋆ ) . Figure 2 : Minimizing f = 1 2 x 2 by the generalized ODE and scheme with r = 1 , starting from x 0 = 1 . In ( b ) , the step size s = 10 − 4 . 1 . 3 Outline and Notation The rest of the paper is organized as follows . In Section 2 , the ODE is rigorously derived from Nesterov’s scheme , and a generalization to composite optimization , where f may be non - smooth , is also obtained . Connections between the ODE and the scheme , in terms of trajectory behaviors and convergence rates , are summarized in Section 3 . In Section 4 , we discuss the eﬀect of replacing the constant 3 in ( 3 ) by an arbitrary constant on the convergence rate . A new restarting scheme is suggested in Section 5 , with linear convergence rate established and empirically observed . Some standard notations used throughout the paper are collected here . We denote by F L the class of convex functions f with L – Lipschitz continuous gradients deﬁned on R n , i . e . , f is convex , continuously diﬀerentiable , and obeys k∇ f ( x ) − ∇ f ( y ) k ≤ L k x − y k for any x , y ∈ R n , where k · k is the standard Euclidean norm and L > 0 is the Lipschitz constant . Next , S µ denotes the class of µ – strongly convex functions f on R n with continuous gradients , i . e . , f is continuously diﬀerentiable and f ( x ) − µ k x k 2 / 2 is convex . We set S µ , L = F L ∩ S µ . Last , we sometimes slightly abuse the notation by using x k for both the k th iterate of Nesterov’s scheme and the k th coordinate of x , depending on the context . 2 Derivation First , we sketch an informal derivation of the ODE ( 3 ) . Assume f ∈ F L for L > 0 . Combining the two equations of ( 1 ) and applying a rescaling give x k + 1 − x k √ s = k − 1 k + 2 x k − x k − 1 √ s − √ s ∇ f ( y k ) . ( 4 ) Introduce the Ansatz x k ≈ X ( k √ s ) for some smooth curve X ( t ) deﬁned for t ≥ 0 . Put k = t / √ s . Then as the step size s goes to zero , X ( t ) ≈ x t / √ s = x k and X ( t + √ s ) ≈ x ( t + √ s ) / √ s = x k + 1 , and 4 Taylor expansion gives ( x k + 1 − x k ) / √ s = ˙ X ( t ) + 1 2 ¨ X ( t ) √ s + o ( √ s ) , ( x k − x k − 1 ) / √ s = ˙ X ( t ) − 1 2 ¨ X ( t ) √ s + o ( √ s ) and √ s ∇ f ( y k ) = √ s ∇ f ( X ( t ) ) + o ( √ s ) . Thus ( 4 ) can be written as ˙ X ( t ) + 1 2 ¨ X ( t ) √ s + o ( √ s ) = (cid:16) 1 − 3 √ s t (cid:17)(cid:16) ˙ X ( t ) − 1 2 ¨ X ( t ) √ s + o ( √ s ) (cid:17) − √ s ∇ f ( X ( t ) ) + o ( √ s ) . ( 5 ) By comparing the coeﬃcients of √ s in ( 5 ) , we obtain ¨ X + 3 t ˙ X + ∇ f ( X ) = 0 . The ﬁrst initial condition is X ( 0 ) = x 0 . Taking k = 1 in ( 4 ) yields ( x 2 − x 1 ) / √ s = − √ s ∇ f ( y 1 ) = o ( 1 ) . Hence , the second initial condition is simply ˙ X ( 0 ) = 0 ( vanishing initial velocity ) . One popular alternative momentum coeﬃcient is θ k ( θ − 1 k − 1 − 1 ) , where θ k are iteratively deﬁned as θ k + 1 = (cid:16) q θ 4 k + 4 θ 2 k − θ 2 k (cid:17) / 2 , starting from θ 0 = 1 ( Nesterov , 1983 ; Beck and Teboulle , 2009 ) . A bit of analysis reveals that θ k ( θ − 1 k − 1 − 1 ) asymptotically equals 1 − 3 / k + O ( 1 / k 2 ) , thus leading to the same ODE as ( 1 ) . Classical results in ODE theory do not directly imply the existence or uniqueness of the solution to this ODE because the coeﬃcient 3 / t is singular at t = 0 . In addition , ∇ f is typically not analytic at x 0 , which leads to the inapplicability of the power series method for studying singular ODEs . Nevertheless , the ODE is well posed : the strategy we employ for showing this constructs a series of ODEs approximating ( 3 ) and then chooses a convergent subsequence by some compactness arguments such as the Arzel´a - Ascoli theorem . Theorem 2 . 1 . For any f ∈ F ∞ : = ∪ L > 0 F L and any x 0 ∈ R n , the ODE ( 3 ) with initial conditions X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 has a unique global solution X ∈ C 2 ( ( 0 , ∞ ) ; R n ) ∩ C 1 ( [ 0 , ∞ ) ; R n ) . The next theorem , in a rigorous way , guarantees the validity of the derivation of this ODE . The proofs of both theorems are deferred to the appendices . Theorem 2 . 2 . For any f ∈ F ∞ , as the step size s → 0 , Nesterov’s scheme ( 1 ) converges to the ODE ( 3 ) in the sense that for all ﬁxed T > 0 , lim s → 0 max 0 ≤ k ≤ T √ s (cid:13)(cid:13) x k − X (cid:0) k √ s (cid:1)(cid:13)(cid:13) = 0 . 2 . 1 Simple Properties We collect some elementary properties that are helpful in understanding the ODE . Time Invariance . If we adopt a linear time transformation , ˜ t = ct for some c > 0 , by the chain rule it follows that d X d˜ t = 1 c d X d t , d 2 X d˜ t 2 = 1 c 2 d 2 X d t 2 . This yields the ODE parameterized by ˜ t , d 2 X d ˜ t 2 + 3 ˜ t d X d ˜ t + ∇ f ( X ) / c 2 = 0 . 5 Also note that minimizing f / c 2 is equivalent to minimizing f . Hence , the ODE is invariant under the time change . In fact , it is easy to see that time invariance holds if and only if the coeﬃcient of ˙ X has the form C / t for some constant C . Rotational Invariance . Nesterov’s scheme and other gradient - based schemes are invariant under rotations . As expected , the ODE is also invariant under orthogonal transformation . To see this , let Y = QX for some orthogonal matrix Q . This leads to ˙ Y = Q ˙ X , ¨ Y = Q ¨ X and ∇ Y f = Q ∇ X f . Hence , denoting by Q T the transpose of Q , the ODE in the new coordinate system reads Q T ¨ Y + 3 t Q T ˙ Y + Q T ∇ Y f = 0 , which is of the same form as ( 3 ) once multiplying Q on both sides . Initial Asymptotic . Assume suﬃcient smoothness of X such that lim t → 0 ¨ X ( t ) exists . The mean value theorem guarantees the existence of some ξ ∈ ( 0 , t ) that satisﬁes ˙ X ( t ) / t = ( ˙ X ( t ) − ˙ X ( 0 ) ) / t = ¨ X ( ξ ) . Hence , from the ODE we deduce ¨ X ( t ) + 3 ¨ X ( ξ ) + ∇ f ( X ( t ) ) = 0 . Taking the limit t → 0 gives ¨ X ( 0 ) = −∇ f ( x 0 ) / 4 . Hence , for small t we have the asymptotic form : X ( t ) = −∇ f ( x 0 ) t 2 8 + x 0 + o ( t 2 ) . This asymptotic expansion is consistent with the empirical observation that Nesterov’s scheme moves slowly in the beginning . 2 . 2 ODE for Composite Optimization It is interesting and important to generalize the ODE to minimizing f in the composite form f ( x ) = g ( x ) + h ( x ) , where the smooth part g ∈ F L and the non - smooth part h : R n → ( −∞ , ∞ ] is a structured general convex function . Both Nesterov ( 2007 ) and Beck and Teboulle ( 2009 ) obtain O ( 1 / k 2 ) convergence rate by employing the proximal structure of h . In analogy to the smooth case , an ODE for composite f is derived in the appendix . 3 Connections and Interpretations In this section , we explore the approximate equivalence between the ODE and Nesterov’s scheme , and provide evidence that the ODE can serve as an amenable tool for interpreting and analyzing Nesterov’s scheme . The ﬁrst subsection exhibits inverse quadratic convergence rate for the ODE solution , the next two address the oscillation phenomenon discussed in Section 1 . 1 , and the last subsection is devoted to comparing Nesterov’s scheme with gradient descent from a numerical perspective . 3 . 1 Analogous Convergence Rate The original result from Nesterov ( 1983 ) states that , for any f ∈ F L , the sequence { x k } given by ( 1 ) with step size s ≤ 1 / L obeys f ( x k ) − f ⋆ ≤ 2 k x 0 − x ⋆ k 2 s ( k + 1 ) 2 . ( 6 ) Our next result indicates that the trajectory of ( 3 ) closely resembles the sequence { x k } in terms of the convergence rate to a minimizer x ⋆ . Compared with the discrete case , this proof is shorter and simpler . 6 Theorem 3 . 1 . For any f ∈ F ∞ , let X ( t ) be the unique global solution to ( 3 ) with initial conditions X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 . Then , for any t > 0 , f ( X ( t ) ) − f ⋆ ≤ 2 k x 0 − x ⋆ k 2 t 2 . ( 7 ) Proof . Consider the energy functional 1 deﬁned as E ( t ) = t 2 ( f ( X ( t ) ) − f ⋆ ) + 2 k X + t ˙ X / 2 − x ⋆ k 2 , whose time derivative is ˙ E = 2 t ( f ( X ) − f ⋆ ) + t 2 h∇ f , ˙ X i + 4 (cid:28) X + t 2 ˙ X − x ⋆ , 3 2 ˙ X + t 2 ¨ X (cid:29) . Substituting 3 ˙ X / 2 + t ¨ X / 2 with − t ∇ f ( X ) / 2 , the above display gives ˙ E = 2 t ( f ( X ) − f ⋆ ) + 4 h X − x ⋆ , − t ∇ f ( X ) / 2 i = 2 t ( f ( X ) − f ⋆ ) − 2 t h X − x ⋆ , ∇ f ( X ) i ≤ 0 , where the inequality follows from the convexity of f . Hence by monotonicity of E and non - negativity of 2 k X + t ˙ X / 2 − x ⋆ k 2 , the gap obeys f ( X ( t ) ) − f ⋆ ≤ E ( t ) t 2 ≤ E ( 0 ) t 2 = 2 k x 0 − x ⋆ k 2 t 2 . Making use of the approximation t ≈ k √ s , we observe that the convergence rate in ( 6 ) is essentially the same as in ( 7 ) , yet another piece of evidence for the approximate equivalence between the ODE and the scheme . We ﬁnish this subsection by showing that the number 2 appearing in the numerator of the error bound in ( 7 ) is optimal . Consider an arbitrary f ∈ F ∞ ( R ) such that f ( x ) = x for x ≥ 0 . Starting from some x 0 > 0 , the solution to ( 3 ) is X ( t ) = x 0 − t 2 / 8 before hitting the origin . Hence , t 2 ( f ( X ( t ) ) − f ⋆ ) = t 2 ( x 0 − t 2 / 8 ) has a maximum 2 x 20 = 2 | x 0 − 0 | 2 achieved at t = 2 √ x 0 . Therefore , we can not replace 2 by any smaller number , and we can expect that this tightness also applies to the discrete analog ( 6 ) . 3 . 2 Quadratic f and Bessel Functions For quadratic f , the ODE ( 3 ) admits a solution in closed form . This closed form solution turns out to be very useful in understanding the issues raised in the introduction . Let f ( x ) = 12 h x , Ax i + h b , x i , where A ∈ R n × n is a positive semideﬁnite matrix and b is in the column space of A because otherwise this function can attain −∞ . Then a simple translation in x can absorb the linear term h b , x i into the quadratic term . Since both the ODE and the scheme move within the aﬃne space perpendicular to the kernel of A , without loss of generality , we assume that A is positive deﬁnite , admitting a spectral decomposition A = Q T Λ Q , where Λ is a diagonal matrix formed by the eigenvalues . Replacing x with Qx , we assume f = 12 h x , Λ x i from now on . Now , the ODE for this function admits a simple decomposition of form ¨ X i + 3 t ˙ X i + λ i X i = 0 , i = 1 , . . . , n 1 We may also view this functional as the negative entropy . Similarly , for the gradient ﬂow ˙ X + ∇ f ( X ) = 0 , an energy function of form E gradient ( t ) = t ( f ( X ( t ) ) − f ⋆ ) + k X ( t ) − x ⋆ k 2 / 2 can be used to derive the bound f ( X ( t ) ) − f ⋆ ≤ k x 0 − x ⋆ k 2 2 t . 7 with X i ( 0 ) = x 0 , i , ˙ X i ( 0 ) = 0 . Introduce Y i ( u ) = uX i ( u / √ λ i ) , which satisﬁes u 2 ¨ Y i + u ˙ Y i + ( u 2 − 1 ) Y i = 0 . This is Bessel’s diﬀerential equation of order 1 . Since Y i vanishes at u = 0 , Y i is a constant multiple of J 1 , the Bessel function of the ﬁrst kind with order 1 . Applying the asymptotic expansion J 1 ( u ) ∼ u / 2 when u → 0 ( see e . g . Watson , 1995 ) , we obtain X i ( t ) = 2 x 0 , i t √ λ i J 1 ( t p λ i ) . ( 8 ) For large t , the Bessel function has the following asymptotic form ( see e . g . Watson , 1995 ) : J 1 ( t ) = r 2 πt (cid:16) cos ( t − 3 π / 4 ) + O ( 1 / t ) (cid:17) . ( 9 ) This asymptotic expansion yields ( note that f ⋆ = 0 ) f ( X ( t ) ) − f ⋆ = f ( X ( t ) ) = n X i = 1 2 x 2 0 , i t 2 J 1 (cid:16) t p λ i (cid:17) 2 = O (cid:18) k x 0 − x ⋆ k 2 t 3 √ min λ i (cid:19) . ( 10 ) On the other hand , ( 9 ) and ( 10 ) give a lower bound : lim sup t →∞ t 3 ( f ( X ( t ) ) − f ⋆ ) ≥ lim t →∞ 1 t Z t 0 n X i = 1 2 x 20 , i uJ 1 ( u p λ i ) 2 d u = n X i = 1 2 x 20 , i π √ λ i ≥ 2 k x 0 − x ⋆ k 2 π √ L , ( 11 ) where L = k A k 2 is the spectral norm of A . Above , ( 10 ) is interesting since it suggests that Nesterov’s scheme possibly exhibit O ( 1 / k 3 ) convergence rate for strongly convex functions . This convergence rate is consistent with the second inequality in Theorem 4 . 2 . In Section 4 . 3 , we prove the O ( 1 / t 3 ) rate for a generalized version of ( 3 ) . However , ( 11 ) rules out the possibility of a higher order convergence rate . Recall that the function considered in Figure 1 is f ( x ) = 0 . 02 x 21 + 0 . 005 x 22 , starting from x 0 = ( 1 , 1 ) . As the step size s becomes smaller , the trajectory of Nesterov’s scheme converges to the solid curve represented via the Bessel function . While approaching the minimizer x ⋆ , each trajectory displays the oscillation pattern , as well - captured by the zoomed Figure 1b . This prevents Nesterov’s scheme achieving better convergence rate . The representation ( 8 ) oﬀers excellent explanation as follows . Denote by T 1 , T 2 , respectively , the approximate periodicities of the ﬁrst component | X 1 | in absolute value and the second | X 2 | . By ( 9 ) , we get T 1 = π / √ λ 1 = 5 π and T 2 = π / √ λ 2 = 10 π . Hence , as the amplitude gradually decreases to zero , the function f = 2 x 20 , 1 J 1 ( √ λ 1 t ) 2 / t 2 + 2 x 20 , 2 J 1 ( √ λ 2 t ) 2 / t 2 has a major cycle of 10 π , the least common multiple of T 1 and T 2 . A careful look at Figure 1c reveals that within each major bump , roughly , there are 10 π / T 1 = 2 minor peaks . 3 . 3 Fluctuations of Strongly Convex f The analysis carried out in the previous subsection only applies to convex quadratic functions . In this subsection , we extend the discussion to one - dimensional strongly convex functions . The Sturm - Picone theory ( see e . g . Hinton , 2005 ) is extensively used all along the analysis . Let f ∈ S µ , L ( R ) . Without loss of generality , assume f attains minimum at x ⋆ = 0 . Then , by deﬁnition µ ≤ f ′ ( x ) / x ≤ L for any x 6 = 0 . Denoting by X the solution to the ODE ( 3 ) , we consider the self - adjoint equation , ( t 3 Y ′ ) ′ + t 3 f ′ ( X ( t ) ) X ( t ) Y = 0 , ( 12 ) 8 which , apparently , admits a solution Y ( t ) = X ( t ) . To apply the Sturm - Picone comparison theorem , consider ( t 3 Y ′ ) ′ + µt 3 Y = 0 for a comparison . This equation admits a solution e Y ( t ) = J 1 ( √ µt ) / t . Denote by ˜ t 1 < ˜ t 2 < · · · all the positive roots of J 1 ( t ) , which obey ( see e . g . Watson , 1995 ) 3 . 8317 = ˜ t 1 − ˜ t 0 > ˜ t 2 − ˜ t 3 > ˜ t 3 − ˜ t 4 > · · · > π , where ˜ t 0 = 0 . Then , it follows that the positive roots of e Y are ˜ t 1 / √ µ , ˜ t 2 / √ µ , . . . . Since t 3 f ′ ( X ( t ) ) / X ( t ) ≥ µt 3 , the Sturm - Picone comparison theorem asserts that X ( t ) has a root in each interval [ ˜ t i / √ µ , ˜ t i + 1 / √ µ ] . To obtain a similar result in the opposite direction , consider ( t 3 Y ′ ) ′ + Lt 3 Y = 0 . ( 13 ) Applying the Sturm - Picone comparison theorem to ( 12 ) and ( 13 ) , we ensure that between any two consecutive positive roots of X , there is at least one ˜ t i / √ L . Now , we summarize our ﬁndings in the following . Roughly speaking , this result concludes that the oscillation frequency of the ODE solution is between O ( √ µ ) and O ( √ L ) . Theorem 3 . 2 . Denote by 0 < t 1 < t 2 < · · · all the roots of X ( t ) − x ⋆ . Then these roots obey , for all i ≥ 1 , t 1 < 7 . 6635 √ µ , t i + 1 − t i < 7 . 6635 √ µ , t i + 2 − t i > π √ L . 3 . 4 Nesterov’s Scheme Compared with Gradient Descent The ansatz t ≈ k √ s in relating the ODE and Nesterov’s scheme is formally conﬁrmed in Theorem 2 . 2 . Consequently , for any constant t c > 0 , this implies that x k does not change much for a range of step sizes s if k ≈ t c / √ s . To empirically support this claim , we present an example in Figure 3a , where the scheme minimizes f ( x ) = k y − Ax k 2 / 2 + k x k 1 with y = ( 4 , 2 , 0 ) and A ( : , 1 ) = ( 0 , 2 , 4 ) , A ( : , 2 ) = ( 1 , 1 , 1 ) starting from x 0 = ( 2 , 0 ) . From this ﬁgure , we are fortunate to observe that x k with the same t c are very close to each other . This interesting square - root scaling has the potential to shed light on the superiority of Nes - terov’s scheme over gradient descent . Roughly speaking , each iteration in Nesterov’s scheme amounts to traveling √ s in time along the integral curve of ( 3 ) , whereas it is known that the simple gradient descent x k + 1 = x k − s ∇ f ( x k ) moves s along the integral curve of ˙ X + ∇ f ( X ) = 0 . We expect that for small s Nesterov’s scheme moves more in each iteration since √ s is much larger than s . Figure 3b illustrates and supports this claim , where the function minimized is f = | x 1 | 3 + 5 | x 2 | 3 + 0 . 001 ( x 1 + x 2 ) 2 with step size s = 0 . 05 ( The coordinates are appropri - ately rotated to allow x 0 and x ⋆ lie on the same horizontal line ) . The circles are the iterates for k = 1 , 10 , 20 , 30 , 45 , 60 , 90 , 120 , 150 , 190 , 250 , 300 . For Nesterov’s scheme , the seventh circle has already passed t = 15 , while for gradient descent the last point has merely arrived at t = 15 . A second look at Figure 3b suggests that Nesterov’s scheme allows a large deviation from its limit curve , as compared with gradient descent . This raises the question of the stable step size allowed for numerically solving the ODE ( 3 ) in the presence of accumulated errors . The ﬁnite diﬀerence approximation by the forward Euler method is X ( t + ∆ t ) − 2 X ( t ) + X ( t − ∆ t ) ∆ t 2 + 3 t X ( t ) − X ( t − ∆ t ) ∆ t + ∇ f ( X ( t ) ) = 0 , ( 14 ) 9 −0 . 5 0 0 . 5 1 1 . 5 2 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 x 1 x 2 s = 10 −2 s = 10 −3 s = 10 −4 ( a ) Square - root scaling of s . −0 . 1 −0 . 08 −0 . 06 −0 . 04 −0 . 02 0 0 . 02 −5 0 5 10 15 20x 10 −3 x 1 x 2 NesterovGradient t = 5 t = 5 t = 15 t = 15 ( b ) Race between Nesterov’s and gradient . Figure 3 : In ( a ) , the circles , crosses and triangles are x k evaluated at k = ⌈ 1 / √ s ⌉ , ⌈ 2 / √ s ⌉ and ⌈ 3 / √ s ⌉ , respectively . In ( b ) , the circles are iterations given by Nesterov’s scheme or gradient descent , depending on the color , and the stars are X ( t ) on the integral curves for t = 5 , 15 . which is equivalent to X ( t + ∆ t ) = (cid:16) 2 − 3∆ t t (cid:17) X ( t ) − ∆ t 2 ∇ f ( X ( t ) ) − (cid:16) 1 − 3∆ t t (cid:17) X ( t − ∆ t ) . ( 15 ) Assuming f is suﬃciently smooth , we have ∇ f ( x + δx ) ≈ ∇ f ( x ) + ∇ 2 f ( x ) δx for small perturbations δx , where ∇ 2 f ( x ) is the Hessian of f evaluated at x . Identifying k = t / ∆ t , the characteristic equation of this ﬁnite diﬀerence scheme is approximately det (cid:18) λ 2 − (cid:18) 2 − ∆ t 2 ∇ 2 f − 3∆ t t (cid:19) λ + 1 − 3∆ t t (cid:19) = 0 . ( 16 ) The numerical stability of ( 14 ) with respect to accumulated errors is equivalent to this : all the roots of ( 16 ) lie in the unit circle ( see e . g . Leader , 2004 ) . When ∇ 2 f (cid:22) LI n ( i . e . , LI n − ∇ 2 f is positive semideﬁnite ) , if ∆ t / t small and ∆ t < 2 / √ L , we see that all the roots of ( 16 ) lie in the unit circle . On the other hand , if ∆ t > 2 / √ L , ( 16 ) can possibly have a root λ outside the unit circle , causing numerical instability . Under our identiﬁcation s = ∆ t 2 , a step size of s = 1 / L in Nesterov’s scheme ( 1 ) is approximately equivalent to a step size of ∆ t = 1 / √ L in the forward Euler method , which is stable for numerically integrating ( 14 ) . As a comparison , note that the ﬁnite diﬀerence scheme of the ODE ˙ X ( t ) + ∇ f ( X ( t ) ) = 0 , which models gradient descent with updates x k + 1 = x k − s ∇ f ( x k ) , has the characteristic equation det ( λ − ( 1 − ∆ t ∇ 2 f ) ) = 0 . Thus , to guarantee − I n (cid:22) 1 − ∆ t ∇ 2 f (cid:22) I n in worst case analysis , one can only choose ∆ t ≤ 2 / L for a ﬁxed step size , which is much smaller than the step size 2 / √ L for ( 14 ) when ∇ f is very variable , i . e . , L is large . 4 The Magic Constant 3 Recall that the constant 3 appearing in the coeﬃcient of ˙ X in ( 3 ) originates from ( k + 2 ) − ( k − 1 ) = 3 . This number leads to the momentum coeﬃcient in ( 1 ) taking the form ( k − 1 ) / ( k + 2 ) = 1 − 3 / k + 10 O ( 1 / k 2 ) . In this section , we demonstrate that 3 can be replaced by any larger number , while maintaining the O ( 1 / k 2 ) convergence rate . To begin with , let us consider the following ODE parameterized by a constant r : ¨ X + r t ˙ X + ∇ f ( X ) = 0 ( 17 ) with initial conditions X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 . The proof of Theorem 2 . 1 , which seamlessly applies here , guarantees the existence and uniqueness of the solution X to this ODE . Interpreting the damping ratio r / t as a measure of friction 2 in the damping system , our results say that more friction does not end the O ( 1 / t 2 ) and O ( 1 / k 2 ) convergence rate . On the other hand , in the lower friction setting , where r is smaller than 3 , we can no longer expect inverse quadratic convergence rate , unless some additional structures of f are imposed . We believe that this striking phase transition at 3 deserves more attention as an interesting research challenge . 4 . 1 High Friction Here , we study the convergence rate of ( 17 ) with r > 3 and f ∈ F ∞ . Compared with ( 3 ) , this new ODE as a damping suﬀers from higher friction . Following the strategy adopted in the proof of Theorem 3 . 1 , we consider a new energy functional deﬁned as E ( t ) = 2 t 2 r − 1 ( f ( X ( t ) ) − f ⋆ ) + ( r − 1 ) (cid:13) (cid:13)(cid:13)(cid:13) X ( t ) + t r − 1 ˙ X ( t ) − x ⋆ (cid:13) (cid:13)(cid:13)(cid:13) 2 . By studying the derivative of this functional , we get the following result . Theorem 4 . 1 . The solution X to ( 17 ) obeys f ( X ( t ) ) − f ⋆ ≤ ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 t 2 , Z ∞ 0 t ( f ( X ( t ) ) − f ⋆ ) d t ≤ ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 ( r − 3 ) . Proof . Noting r ˙ X + t ¨ X = − t ∇ f ( X ) , we get ˙ E equal to 4 t r − 1 ( f ( X ) − f ⋆ ) + 2 t 2 r − 1 h∇ f , ˙ X i + 2 h X + t r − 1 ˙ X − x ⋆ , r ˙ X + t ¨ X i = 4 t r − 1 ( f ( X ) − f ⋆ ) − 2 t h X − x ⋆ , ∇ f ( X ) i ≤ − 2 ( r − 3 ) t r − 1 ( f ( X ) − f ⋆ ) , ( 18 ) where the inequality follows from the convexity of f . Since f ( X ) ≥ f ⋆ , the last display implies that E is non - increasing . Hence 2 t 2 r − 1 ( f ( X ( t ) ) − f ⋆ ) ≤ E ( t ) ≤ E ( 0 ) = ( r − 1 ) k x 0 − x ⋆ k 2 , yielding the ﬁrst inequality of this theorem . To complete the proof , from ( 18 ) it follows that Z ∞ 0 2 ( r − 3 ) t r − 1 ( f ( X ) − f ⋆ ) d t ≤ − Z ∞ 0 d E d t d t = E ( 0 ) − E ( ∞ ) ≤ ( r − 1 ) k x 0 − x ⋆ k 2 , as desired for establishing the second inequality . 2 In physics and engineering , damping may be modeled as a force proportional to velocity but opposite in direction , i . e . resisting motion ; for instance , this force may be used as an approximation to the friction caused by drag . In our model , this force would be proportional to − rt ˙ X where ˙ X is velocity and rt is the damping coeﬃcient . 11 The ﬁrst inequality is the same as ( 7 ) for the ODE ( 3 ) , except for a larger constant ( r − 1 ) 2 / 2 . The second inequality is interesting in that it measures the error f ( X ( t ) ) − f ⋆ in an average sense . Now , it is tempting to obtain such analogs for the discrete Nesterov’s scheme as well . Following the formulation of Beck and Teboulle ( 2009 ) , we wish to minimize f in the composite form f ( x ) = g ( x ) + h ( x ) , where g ∈ F L for some L > 0 and h is convex on R n possibly assuming extended value ∞ . Deﬁne the proximal subgradient G s ( x ) , x − argmin z (cid:0) k z − ( x − s ∇ g ( x ) ) k 2 / ( 2 s ) + h ( z ) (cid:1) s . Parametrizing by a constant r , we propose the generalized Nesterov’s scheme , x k = y k − 1 − sG s ( y k − 1 ) y k = x k + k − 1 k + r − 1 ( x k − x k − 1 ) , ( 19 ) starting from y 0 = x 0 . The discrete analog of Theorem 4 . 1 is below . Theorem 4 . 2 . The sequence { x k } given by ( 19 ) with 0 < s ≤ 1 / L obeys f ( x k ) − f ⋆ ≤ ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 s ( k + r − 2 ) 2 , ∞ X k = 1 ( k + r − 1 ) ( f ( x k ) − f ⋆ ) ≤ ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 s ( r − 3 ) . The ﬁrst inequality suggests that the generalized Nesterov’s schemes still achieve O ( 1 / k 2 ) con - vergence rate . However , if the error bound satisﬁes f ( x k ′ ) − f ⋆ ≥ c / k ′ 2 for some c > 0 and a dense subsequence { k ′ } , i . e . , | { k ′ } ∩ { 1 , . . . , m } | ≥ αm for any positive integer m and some α > 0 , then the second inequality of the theorem is violated . Hence , the second inequality is not trivial because it implies the error bound is in some sense O ( 1 / k 2 ) suboptimal . Now we turn to the proof of this theorem . It is worth pointing out that , though based on the same idea , the proof below is much more complicated than that of Theorem 4 . 1 . Proof . Consider the discrete energy functional , E ( k ) = 2 ( k + r − 2 ) 2 s r − 1 ( f ( x k ) − f ⋆ ) + ( r − 1 ) k z k − x ⋆ k 2 , where z k = ( k + r − 1 ) y k / ( r − 1 ) − kx k / ( r − 1 ) . If we have E ( k ) + 2 s [ ( r − 3 ) ( k + r − 2 ) + 1 ] r − 1 ( f ( x k − 1 ) − f ⋆ ) ≤ E ( k − 1 ) , ( 20 ) then it immediately yields the desired results by summing over ( 20 ) . To be speciﬁc , by recursively applying ( 20 ) , we see E ( k ) + k X i = 1 2 s [ ( r − 3 ) ( i + r − 2 ) + 1 ] r − 1 ( f ( x i − 1 ) − f ⋆ ) ≤ E ( 0 ) = 2 ( r − 2 ) 2 s r − 1 ( f ( x 0 ) − f ⋆ ) + ( r − 1 ) k x 0 − x ⋆ k 2 , which is equivalent to E ( k ) + k − 1 X i = 1 2 s [ ( r − 3 ) ( i + r − 1 ) + 1 ] r − 1 ( f ( x i ) − f ⋆ ) ≤ ( r − 1 ) k x 0 − x ⋆ k 2 . ( 21 ) 12 Noting that the left - hand side of ( 21 ) is lower bounded by 2 s ( k + r − 2 ) 2 ( f ( x k ) − f ⋆ ) / ( r − 1 ) , we thus obtain the ﬁrst inequality of the theorem . Since E ( k ) ≥ 0 , the second inequality is veriﬁed via taking the limit k → ∞ in ( 21 ) and replacing ( r − 3 ) ( i + r − 1 ) + 1 by ( r − 3 ) ( i + r − 1 ) . We now establish ( 20 ) . For s ≤ 1 / L , we have the basic inequality , f ( y − sG s ( y ) ) ≤ f ( x ) + G s ( y ) T ( y − x ) − s 2 k G s ( y ) k 2 , ( 22 ) for any x and y . Note that y k − 1 − sG s ( y k − 1 ) actually coincides with x k . Summing of ( k − 1 ) / ( k + r − 2 ) × ( 22 ) with x = x k − 1 , y = y k − 1 and ( r − 1 ) / ( k + r − 2 ) × ( 22 ) with x = x ⋆ , y = y k − 1 gives f ( x k ) ≤ k − 1 k + r − 2 f ( x k − 1 ) + r − 1 k + r − 2 f ⋆ + r − 1 k + r − 2 G s ( y k − 1 ) T (cid:16) k + r − 2 r − 1 y k − 1 − k − 1 r − 1 x k − 1 − x ⋆ (cid:17) − s 2 k G s ( y k − 1 ) k 2 = k − 1 k + r − 2 f ( x k − 1 ) + r − 1 k + r − 2 f ⋆ + ( r − 1 ) 2 2 s ( k + r − 2 ) 2 (cid:16) k z k − 1 − x ⋆ k 2 − k z k − x ⋆ k 2 (cid:17) , where we use z k − 1 − s ( k + r − 2 ) G s ( y k − 1 ) / ( r − 1 ) = z k . Rearranging the above inequality and multiplying by 2 s ( k + r − 2 ) 2 / ( r − 1 ) gives the desired ( 20 ) . In closing , we would like to point out this new scheme is equivalent to setting θ k = ( r − 1 ) / ( k + r − 1 ) and letting θ k ( θ − 1 k − 1 − 1 ) replace the momentum coeﬃcient ( k − 1 ) / ( k + r − 1 ) . Then , the equal sign “ = ” in the update θ k + 1 = ( q θ 4 k + 4 θ 2 k − θ 2 k ) / 2 has to be replaced by an inequality sign “ ≥ ” . In examining the proof of Theorem 1 ( b ) in Tseng ( 2010 ) , we can get an alternative proof of Theorem 4 . 2 . 4 . 2 Low Friction Now we turn to the case r < 3 . Then , unfortunately , the energy functional approach for proving Theorem 4 . 1 is no longer valid , since the left - hand side of ( 18 ) is positive in general . In fact , there are counterexamples that fail the desired O ( 1 / t 2 ) or O ( 1 / k 2 ) convergence rate . Let f ( x ) = 12 k x k 2 and X be the solution to ( 17 ) . Then , Y = t r − 12 X obeys t 2 ¨ Y + t ˙ Y + ( t 2 − ( r − 1 ) 2 / 4 ) Y = 0 . With the initial condition Y ( t ) ≈ t r − 1 2 x 0 for small t , the solution to the above Bessel equation in a vector form of order ( r − 1 ) / 2 is Y ( t ) = 2 r − 12 Γ ( ( r + 1 ) / 2 ) J ( r − 1 ) / 2 ( t ) x 0 . Thus , X ( t ) = 2 r − 12 Γ ( ( r + 1 ) / 2 ) J ( r − 1 ) / 2 ( t ) t r − 1 2 x 0 . For large t , the Bessel function J ( r − 1 ) / 2 ( t ) = p 2 / ( πt ) (cid:0) cos ( t − ( r − 1 ) π / 4 − π / 4 ) + O ( 1 / t ) (cid:1) . Hence , f ( X ( t ) ) − f ⋆ = O (cid:0) k x 0 − x ⋆ k 2 / t r (cid:1) , where the exponent r is tight . This rules out the possibility of inverse quadratic convergence of the generalized ODE and scheme for all f ∈ F L if r < 2 . An example with r = 1 is plotted in Figure 2 . 13 Next , we consider the case 2 ≤ r < 3 and let f ( x ) = | x | ( this also applies to multivariate f = k x k ) . 3 Starting from x 0 > 0 , we get X ( t ) = x 0 − t 2 2 ( 1 + r ) for t ≤ p 2 ( 1 + r ) x 0 . Requiring continuity of X and ˙ X at the change point 0 , we get X ( t ) = t 2 2 ( 1 + r ) + 2 ( 2 ( 1 + r ) x 0 ) r + 12 ( r 2 − 1 ) t r − 1 − r + 3 r − 1 x 0 for p 2 ( 1 + r ) x 0 < t ≤ p 2 c ⋆ ( 1 + r ) x 0 , where c ⋆ is the positive root other than 1 of ( r − 1 ) c + 4 c − r − 12 = r + 3 . Repeating this process solves for X . Note that t 1 − r is in the null space of ¨ X + r ˙ X / t and obeys t 2 × t 1 − r → ∞ as t → ∞ . For illustration , Figure 4 plot t 2 ( f ( X ( t ) ) − f ⋆ ) and sk 2 ( f ( x k ) − f ⋆ ) with r = 2 , 2 . 5 , and r = 4 for comparison 4 . It is clearly that inverse quadratic convergence does not hold for r = 2 , 2 . 5 , that is , ( 2 ) does not hold for r < 3 . Interestingly , in Figures 4a and 4d , the scaled errors at peaks grow linearly , whereas for r = 2 . 5 , the growth rate , though positive as well , seems sublinear . 0 1 2 3 4 5 6 7 8 9 10 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4 4 . 5 5 t t 2 ( f − f * ) ( a ) ODE ( 17 ) with r = 2 . 1 2 3 4 5 6 7 8 9 10 0 . 5 1 1 . 5 2 2 . 5 3 t t 2 ( f − f * ) ( b ) ODE ( 17 ) with r = 2 . 5 . 1 2 3 4 5 6 7 8 0 . 5 1 1 . 5 2 t t 2 ( f − f * ) ( c ) ODE ( 17 ) with r = 4 . 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4 4 . 5 5 iterations sk 2 ( f − f * ) ( d ) Scheme ( 19 ) with r = 2 . 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 x 10 4 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 iterations sk 2 ( f − f * ) ( e ) Scheme ( 19 ) with r = 2 . 5 . 0 1000 2000 3000 4000 5000 6000 7000 8000 0 0 . 5 1 1 . 5 2 2 . 5 iterations sk 2 ( f − f * ) ( f ) Scheme ( 19 ) with r = 4 . Figure 4 : Scaled errors t 2 ( f ( X ( t ) ) − f ⋆ ) and sk 2 ( f ( x k ) − f ⋆ ) of generalized ODEs and schemes for minimizing f = | x | . In ( d ) , the step size s = 10 − 6 , in ( e ) , s = 10 − 7 , and in ( f ) , s = 10 − 6 . However , if f possesses some additional property , inverse quadratic convergence is still guar - anteed , as stated below . In that theorem , f is assumed to be a continuously diﬀerentiable convex function . Theorem 4 . 3 . Suppose 1 < r < 3 and let X be a solution to the ODE ( 17 ) . If ( f − f ⋆ ) r − 12 is also convex , then f ( X ( t ) ) − f ⋆ ≤ ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 t 2 . Proof . Since ( f − f ⋆ ) r − 12 is convex , we obtain ( f ( X ( t ) ) − f ⋆ ) r − 1 2 ≤ h X − x ⋆ , ∇ ( f ( X ) − f ⋆ ) r − 12 i = r − 1 2 ( f ( X ) − f ⋆ ) r − 3 2 h X − x ⋆ , ∇ f ( X ) i , 3 This function does not have a Lipschitz continuous gradient . However , similar pattern as in Figure 2 can be also observed if we smooth | x | at an arbitrarily small vicinity of 0 . 4 For Figures 4d , 4e and 4f , if running generalized Nesterov’s schemes with too many iterations ( e . g . 10 5 ) , the deviations from the ODE will grow . Taking a suﬃciently small s can solve this issue . 14 which can be simpliﬁed to 2 r − 1 ( f ( X ) − f ⋆ ) ≤ h X − x ⋆ , ∇ f ( X ) i . This inequality combined with ( 18 ) leads to the monotonically decreasing of E ( t ) deﬁned for Theorem 4 . 1 . This completes the proof by noting f ( X ) − f ⋆ ≤ ( r − 1 ) E ( t ) / ( 2 t 2 ) ≤ ( r − 1 ) E ( 0 ) / ( 2 t 2 ) = ( r − 1 ) 2 k x 0 − x ⋆ k 2 / ( 2 t 2 ) . 4 . 3 Strongly Convex f Strong convexity is a desirable property for optimization . This property even allows vanilla gradient descent to achieve linear convergence . Unfortunately , the example given in the previous subsection simply rules out such possibility for Nesterov’s scheme and its generalizations ( 19 ) . However , from a diﬀerent perspective , this example suggests that O ( t − r ) convergence rate can be expected for ( 17 ) . In the next theorem , we prove a slightly weaker statement of this kind , that is , a provable O ( t − 2 r 3 ) convergence rate is established for strongly convex functions . Bridging this gap may require new tools and more careful analysis . Let f ∈ S µ , L ( R n ) and consider a new energy functional for α > 2 deﬁned as E ( t ; α ) = t α ( f ( X ( t ) ) − f ⋆ ) + ( 2 r − α ) 2 t α − 2 8 (cid:13) (cid:13)(cid:13) X ( t ) + 2 t 2 r − α ˙ X − x ⋆ (cid:13) (cid:13)(cid:13) 2 . When clear from the context , E ( t ; α ) is simply denoted as E ( t ) . For r > 3 , taking α = 2 r / 3 in the theorem stated below gives f ( X ( t ) ) − f ⋆ . k x 0 − x ⋆ k 2 / t 2 r 3 . Theorem 4 . 4 . For any f ∈ S µ , L ( R n ) , if 2 ≤ α ≤ 2 r / 3 we get f ( X ( t ) ) − f ⋆ ≤ C k x 0 − x ⋆ k 2 µ α − 2 2 t α for any t > 0 . Above , the constant C only depends on α and r . Proof . Note that ˙ E ( t ; α ) equals αt α − 1 ( f ( X ) − f ⋆ ) − ( 2 r − α ) t α − 1 2 h X − x ⋆ , ∇ f ( X ) i + ( α − 2 ) ( 2 r − α ) 2 t α − 3 8 k X − x ⋆ k 2 + ( α − 2 ) ( 2 r − α ) t α − 2 4 h ˙ X , X − x ⋆ i . ( 23 ) By the strong convexity of f , the second term of the right - hand side of ( 23 ) is bounded below as ( 2 r − α ) t α − 1 2 h X − x ⋆ , ∇ f ( X ) i ≥ ( 2 r − α ) t α − 1 2 ( f ( X ) − f ⋆ ) + µ ( 2 r − α ) t α − 1 4 k X − x ⋆ k 2 . Substituting the last display into ( 23 ) with the awareness of r ≥ 3 α / 2 yields ˙ E ≤ − ( 2 µ ( 2 r − α ) t 2 − ( α − 2 ) ( 2 r − α ) 2 ) t α − 3 8 k X − x ⋆ k 2 + ( α − 2 ) ( 2 r − α ) t α − 2 8 d k X − x ⋆ k 2 d t . Hence , if t ≥ t α : = p ( α − 2 ) ( 2 r − α ) / ( 2 µ ) , we obtain ˙ E ( t ) ≤ ( α − 2 ) ( 2 r − α ) t α − 2 8 d k X − x ⋆ k 2 d t . 15 Integrating the last inequality on the interval ( t α , t ) gives E ( t ) ≤ E ( t α ) + ( α − 2 ) ( 2 r − α ) t α − 2 8 k X ( t ) − x ⋆ k 2 − ( α − 2 ) ( 2 r − α ) t α − 2 α 8 k X ( t α ) − x ⋆ k 2 − 1 8 Z t t α ( α − 2 ) 2 ( 2 r − α ) u α − 3 k X ( u ) − x ⋆ k 2 d u ≤ E ( t α ) + ( α − 2 ) ( 2 r − α ) t α − 2 8 k X ( t ) − x ⋆ k 2 ≤ E ( t α ) + ( α − 2 ) ( 2 r − α ) t α − 2 4 µ ( f ( X ( t ) ) − f ⋆ ) . ( 24 ) Making use of ( 24 ) , we apply induction on α to ﬁnish the proof . First , consider 2 < α ≤ 4 . Applying Theorem 4 . 1 , from ( 24 ) we get that E ( t ) is upper bounded by E ( t α ) + ( α − 2 ) ( r − 1 ) 2 ( 2 r − α ) k x 0 − x ⋆ k 2 8 µt 4 − α ≤ E ( t α ) + ( α − 2 ) ( r − 1 ) 2 ( 2 r − α ) k x 0 − x ⋆ k 2 8 µt 4 − α α . ( 25 ) Then , we bound E ( t α ) as follows . E ( t α ) ≤ t αα ( f ( X ( t α ) ) − f ⋆ ) + ( 2 r − α ) 2 t α − 2 α 4 (cid:13) (cid:13)(cid:13) 2 r − 2 2 r − αX ( t α ) + 2 t α 2 r − α ˙ X ( t α ) − 2 r − 2 2 r − αx ⋆ (cid:13) (cid:13)(cid:13) 2 + ( 2 r − α ) 2 t α − 2 α 4 (cid:13)(cid:13) (cid:13) α − 2 2 r − αX ( t α ) − α − 2 2 r − αx ⋆ (cid:13)(cid:13) (cid:13) 2 ≤ ( r − 1 ) 2 t α − 2 α k x 0 − x ⋆ k 2 + ( α − 2 ) 2 ( r − 1 ) 2 k x 0 − x ⋆ k 2 4 µt 4 − α α , ( 26 ) where in the second inequality we use the decreasing property of the energy functional deﬁned for Theorem 4 . 1 . Combining ( 25 ) and ( 26 ) , we have E ( t ) ≤ ( r − 1 ) 2 t α − 2 α k x 0 − x ⋆ k 2 + ( α − 2 ) ( r − 1 ) 2 ( 2 r + α − 4 ) k x 0 − x ⋆ k 2 8 µt 4 − α α = O (cid:16) k x 0 − x ⋆ k 2 µ α − 2 2 (cid:17) . For t ≥ t α , it suﬃces to apply f ( X ( t ) ) − f ⋆ ≤ E ( t ) / t 3 to the last display . For t < t α , by Theorem 4 . 1 , f ( X ( t ) ) − f ⋆ is upper bounded by ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 t 2 ≤ ( r − 1 ) 2 µ α − 2 2 [ ( α − 2 ) ( 2 r − α ) / ( 2 µ ) ] α − 22 2 k x 0 − x ⋆ k 2 µ α − 2 2 t α = O (cid:16) k x 0 − x ⋆ k 2 µ α − 2 2 t α (cid:17) . ( 27 ) Next , suppose that the theorem is valid for some ˜ α > 2 . We show below that this theorem is still valid for α : = ˜ α + 1 if still r ≥ 3 α / 2 . By the assumption , ( 24 ) further induces E ( t ) ≤ E ( t α ) + ( α − 2 ) ( 2 r − α ) t α − 2 4 µ ˜ C k x 0 − x ⋆ k 2 µ ˜ α − 2 2 t ˜ α ≤ E ( t α ) + ˜ C ( α − 2 ) ( 2 r − α ) k x 0 − x ⋆ k 2 4 µ α − 1 2 t α for some constant ˜ C only depending on ˜ α and r . This inequality with ( 26 ) implies E ( t ) ≤ ( r − 1 ) 2 t α − 2 α k x 0 − x ⋆ k 2 + ( α − 2 ) 2 ( r − 1 ) 2 k x 0 − x ⋆ k 2 4 µt 4 − α α + ˜ C ( α − 2 ) ( 2 r − α ) k x 0 − x ⋆ k 2 4 µ α − 1 2 t α = O (cid:16) k x 0 − x ⋆ k 2 / µ α − 2 2 (cid:17) , which verify the induction for t ≥ t α . As for t < t α , the validity of the induction follows from Theorem 4 . 1 , similarly to ( 27 ) . Thus , combining the base and induction steps , the proof is com - pleted . 16 It should be pointed out that the constant C in the statement of Theorem 4 . 4 grows with the parameter r . Hence , simply increasing r does not guarantee to give a better error bound . While it is desirable to expect a discrete analogy of Theorem 4 . 4 , i . e . , O ( 1 / k α ) convergence rate for ( 19 ) , a complete proof can be notoriously complicated . That said , we mimic the proof of Theorem 4 . 4 for α = 3 and succeed in obtaining a O ( 1 / k 3 ) convergence rate for the generalized Nesterov’s schemes , as summarized in the theorem below . Theorem 4 . 5 . Suppose f is written as f = g + h , where g ∈ S µ , L and h is convex with possible extended value ∞ . Then , the generalized Nesterov’s scheme ( 19 ) with r ≥ 9 / 2 and s = 1 / L obeys f ( x k ) − f ⋆ ≤ CL k x 0 − x ⋆ k 2 k 2 p L / µ k , where C only depends on r . This theorem states that the discrete scheme ( 19 ) enjoys the error bound O ( 1 / k 3 ) without any knowledge of the condition number L / µ . In particular , this bound is much better than that given in Theorem 4 . 2 if k ≫ p L / µ . The strategy of the proof is fully inspired by that of Theorem 4 . 4 , though it is much more complicated and thus deferred to the Appendix . The relevant energy functional E ( k ) for this Theorem 4 . 5 is equal to s ( 2 k + 3 r − 5 ) ( 2 k + 2 r − 5 ) ( 4 k + 4 r − 9 ) 16 ( f ( x k ) − f ⋆ ) + 2 k + 3 r − 5 16 k 2 ( k + r − 1 ) y k − ( 2 k + 1 ) x k − ( 2 r − 3 ) x ⋆ k 2 . ( 28 ) 4 . 4 Numerical Examples We study four synthetic examples to compare ( 19 ) with the step sizes are ﬁxed to be 1 / L , as illustrated in Figure 5 . The error rates exhibits similar patterns for all r , namely , decreasing while suﬀering from local bumps . A smaller r introduces less friction , thus allowing x k moves towards x ⋆ faster in the beginning . However , when suﬃciently close to x ⋆ , more friction is preferred in order to reduce overshoot . This point of view explains what we observe in these examples . That is , across these four examples , ( 19 ) with a smaller r performs slightly better in the beginning , but a larger r has advantage when k is large . It is an interesting question how to choose a good r for diﬀerent problems in practice . Lasso with fat design . Minimizing f ( x ) = 12 k Ax − b k 2 + λ k x k 1 , in which A a 100 × 500 random matrix with i . i . d . standard Gaussian N ( 0 , 1 ) entries , b generated independently has i . i . d . N ( 0 , 25 ) entries , and the penalty λ = 4 . The plot is Figure 5a . Nonnegative least squares ( NLS ) with fat design . Minimizing f ( x ) = k Ax − b k 2 subject to x (cid:23) 0 , with the same design A and b as in Figure 5a . The plot is Figure 5b . Lasso with square design . Minimizing f ( x ) = 12 k Ax − b k 2 + λ k x k 1 , where A a 500 × 500 random matrix with i . i . d . standard Gaussian entries , b generated independently has i . i . d . N ( 0 , 9 ) entries , and the penalty λ = 4 . The plot is Figure 5c . Nonnegative least squares with sparse design . Minimizing f ( x ) = k Ax − b k 2 subject to x (cid:23) 0 , in which A is a 1000 × 10000 sparse matrix with nonzero probability 10 % for each entry and b is given as b = Ax 0 + N ( 0 , I 1000 ) . The nonzero entries of A are independently Gaussian distributed before column normalization , and x 0 has 100 nonzero entries that are all equal to 4 . The plot is Figure 5d . 17 0 500 1000 1500 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iterations f − f * r = 3 r = 4 r = 5 ( a ) Lasso with fat design . 0 50 100 150 10 −20 10 −15 10 −10 10 −5 10 0 10 5 iterations f − f * r = 3 r = 4 r = 5 ( b ) NLS with fat design . 0 50 100 150 200 250 300 350 400 450 500 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iterations f − f * r = 3 r = 4 r = 5 ( c ) Lasso with square design . 0 100 200 300 400 500 600 700 800 900 10 −20 10 −15 10 −10 10 −5 10 0 10 5 iterations f − f * r = 3 r = 4 r = 5 ( d ) NLS with square design . Figure 5 : Comparisons of generalized Nesterov’s schemes with diﬀerent r . 5 Restarting The example discussed in Section 4 . 2 demonstrates that Nesterov’s scheme and its generaliza - tions ( 19 ) are not capable of fully exploiting strong convexity . That is , this example suggests evidence that O ( 1 / poly ( k ) ) is the best rate achievable under strong convexity . In contrast , the vanilla gradient method achieves linear convergence O ( ( 1 − µ / L ) k ) . This drawback results from too much momentum introduced when the objective function is strongly convex . The derivative of a strongly convex function is generally more reliable than that of non - strongly convex functions . In the language of ODEs , at later stage a too small 3 / t in ( 3 ) leads to a lack of friction , resulting in unnecessary overshoot along the trajectory . Incorporating the optimal momentum coeﬃcient √ L − √ µ √ L + √ µ ( This is less than ( k − 1 ) / ( k + 2 ) when k is large ) , Nesterov’s scheme has convergence rate of O ( ( 1 − p µ / L ) k ) ( Nesterov , 2004 ) , which , however , requires knowledge of the condition number µ / L . While it is relatively easy to bound the Lipschitz constant L by the use of backtracking , estimating the strong convexity parameter µ , if not impossible , is very challenging . Among many approaches to gain acceleration via adaptively estimating µ / L ( see Nesterov , 2007 ) , O’Donoghue and Cand ` es ( 2013 ) proposes a procedure termed as gradient restarting for Nesterov’s scheme in which ( 1 ) is restarted with x 0 = y 0 : = x k whenever f ( x k + 1 ) > f ( x k ) . In the language of ODEs , this restarting essentially keeps h∇ f , ˙ X i negative , and resets 3 / t each time to prevent this coeﬃcient from steadily decreasing along the trajectory . Although it has been empirically observed that this method signiﬁcantly boosts convergence , there is no general theory characterizing the convergence rate . In this section , we propose a new restarting scheme we call the speed restarting scheme . The underlying motivation is to maintain a relatively high velocity ˙ X along the trajectory , similar in 18 spirit to the gradient restarting . Speciﬁcally , our main result Theorem 5 . 1 ensures linear conver - gence of the continuous version of the speed restarting . More generally , our contribution here is merely to provide a framework for analyzing restarting schemes rather than competing with other schemes . Throughout this section we assume f ∈ S µ , L for some 0 < µ ≤ L . Recall that function f ∈ S µ , L if f ∈ F L and f ( x ) − µ k x k 2 / 2 is convex . 5 . 1 A New Restarting Scheme We ﬁrst deﬁne the speed restarting time . For the ODE ( 3 ) , we call T = T ( x 0 ; f ) = sup ( t > 0 : ∀ u ∈ ( 0 , t ) , d k ˙ X ( u ) k 2 d u > 0 ) the speed restarting time . In words , T is the ﬁrst time the velocity k ˙ X k decreases . Back to the discrete scheme , it is the ﬁrst time when we observe k x k + 1 − x k k < k x k − x k − 1 k . This deﬁnition itself does not directly imply that 0 < T < ∞ , which is proven later in Lemmas 5 . 4 and . 12 . Indeed , f ( X ( t ) ) is a decreasing function before time T ; for t ≤ T , d f ( X ( t ) ) d t = h∇ f ( X ) , ˙ X i = − 3 t k ˙ X k 2 − 1 2 d k ˙ X k 2 d t ≤ 0 . The speed restarted ODE is thus ¨ X ( t ) + 3 t sr ˙ X ( t ) + ∇ f ( X ( t ) ) = 0 , ( 29 ) where t sr is set to zero whenever h ˙ X , ¨ X i = 0 and between two consecutive restarts , t sr grows just as t . That is , t sr = t − τ , where τ is the latest restart time . In particular , t sr = 0 at t = 0 . Letting X sr be the solution to ( 29 ) , we have the following observations . • X sr ( t ) is continuous for t ≥ 0 , with X sr ( 0 ) = x 0 ; • X sr ( t ) satisﬁes ( 3 ) for 0 < t < T 1 : = T ( x 0 ; f ) . • Recursively deﬁne T i + 1 = T (cid:16) X sr (cid:16)P ij = 1 T j (cid:17) ; f (cid:17) for i ≥ 1 , and e X ( t ) : = X sr (cid:16)P ij = 1 T j + t (cid:17) satisﬁes the ODE ( 3 ) , with e X ( 0 ) = X sr (cid:16) P ij = 1 T j (cid:17) , for 0 < t < T i + 1 . The theorem below guarantees linear convergence of X sr . This is a new result in the literature ( O’Donoghue and Cand ` es , 2013 ; Monteiro et al . , 2012 ) . The proof of Theorem 5 . 1 is based on Lemmas 5 . 3 and 5 . 4 , where the ﬁrst guarantees the rate f ( X sr ) − f ⋆ decays by a constant factor for each restarting , and the second conﬁrms that restartings are adequate . In these lemmas we all make a convention that the uninteresting case x 0 = x ⋆ is excluded . Theorem 5 . 1 . There exist positive constants c 1 and c 2 , which only depend on the condition number L / µ , such that for any f ∈ S µ , L , we have f ( X sr ( t ) ) − f ⋆ ≤ c 1 L k x 0 − x ⋆ k 2 2 e − c 2 t √ L . Before turning to the proof , we make a remark that this linear convergence of X sr remains to hold for the generalized ODE ( 17 ) with r > 3 . Only minor modiﬁcations in the proof below are needed , such as replacing u 3 by u r in the deﬁnition of I ( t ) in Lemma . 12 . 19 5 . 2 Proof of Linear Convergence First , we collect some useful estimates . Denote by M ( t ) the supremum of k ˙ X ( u ) k / u over u ∈ ( 0 , t ] and let I ( t ) : = Z t 0 u 3 ( ∇ f ( X ( u ) ) − ∇ f ( x 0 ) ) d u . It is guaranteed that M deﬁned above is ﬁnite , for example , see the proof of Lemma . 5 . The deﬁnition of M gives a bound on the gradient of f , k∇ f ( X ( t ) ) − ∇ f ( x 0 ) k ≤ L (cid:13)(cid:13)(cid:13) Z t 0 ˙ X ( u ) d u (cid:13)(cid:13)(cid:13) ≤ L Z t 0 u k ˙ X ( u ) k u d u ≤ LM ( t ) t 2 2 . Hence , it is easy to see that I can also be bounded via M , k I ( t ) k ≤ Z t 0 u 3 k∇ f ( X ( u ) ) − ∇ f ( x 0 ) k d u ≤ Z t 0 LM ( u ) u 5 2 d u ≤ LM ( t ) t 6 12 . To fully facilitate these estimates , we need the following lemma that gives an upper bound of M , whose proof is deferred to the appendix . Lemma 5 . 2 . For t < p 12 / L , we have M ( t ) ≤ k∇ f ( x 0 ) k 4 ( 1 − Lt 2 / 12 ) . Next we give a lemma which claims that the objective function decays by a constant through each speed restarting . Lemma 5 . 3 . There is a universal constant C > 0 such that f ( X ( T ) ) − f ⋆ ≤ (cid:18) 1 − Cµ L (cid:19) ( f ( x 0 ) − f ⋆ ) . Proof . By Lemma 5 . 2 , for t < p 12 / L we have (cid:13)(cid:13)(cid:13)(cid:13) ˙ X ( t ) + t 4 ∇ f ( x 0 ) (cid:13)(cid:13)(cid:13)(cid:13) = 1 t 3 k I ( t ) k ≤ LM ( t ) t 3 12 ≤ L k∇ f ( x 0 ) k t 3 48 ( 1 − Lt 2 / 12 ) , which yields 0 ≤ t 4 k∇ f ( x 0 ) k − L k∇ f ( x 0 ) k t 3 48 ( 1 − Lt 2 / 12 ) ≤ k ˙ X ( t ) k ≤ t 4 k∇ f ( x 0 ) k + L k∇ f ( x 0 ) k t 3 48 ( 1 − Lt 2 / 12 ) . ( 30 ) Hence , for 0 < t < 4 / ( 5 √ L ) we get d f ( X ) d t = − 3 t k ˙ X k 2 − 1 2 d d t k ˙ X k 2 ≤ − 3 t k ˙ X k 2 ≤ − 3 t (cid:18) t 4 k∇ f ( x 0 ) k − L k∇ f ( x 0 ) k t 3 48 ( 1 − Lt 2 / 12 ) (cid:19) 2 ≤ − C 1 t k∇ f ( x 0 ) k 2 , where C 1 > 0 is an absolute constant and the second inequality follows from Lemma . 12 in the appendix . Consequently , f (cid:16) X ( 4 / ( 5 √ L ) ) (cid:17) − f ( x 0 ) ≤ Z 4 5 √ L 0 − C 1 u k∇ f ( x 0 ) k 2 d u ≤ − Cµ L ( f ( x 0 ) − f ⋆ ) , 20 where C = 16 C 1 / 25 and in the last inequality we use the µ - strong convexity of f . Thus we have f (cid:18) X (cid:18) 4 5 √ L (cid:19)(cid:19) − f ⋆ ≤ (cid:18) 1 − Cµ L (cid:19) ( f ( x 0 ) − f ⋆ ) . To complete the proof , note that f ( X ( T ) ) ≤ f ( X ( 4 / ( 5 √ L ) ) ) by Lemma . 12 . With each restarting reducing the error f − f ⋆ by a constant a factor , we still need the following lemma to ensure suﬃciently many restartings . Lemma 5 . 4 . There is a universal constant ˜ C such that T ≤ 4 exp (cid:16) ˜ CLµ (cid:17) 5 √ L . Proof . For 4 / ( 5 √ L ) ≤ t ≤ T , we have d f ( X ) d t ≤ − 3 t k ˙ X ( t ) k 2 ≤ − 3 t k ˙ X ( 4 / ( 5 √ L ) ) k 2 , which implies f ( X ( T ) ) − f ( x 0 ) ≤ − Z T 4 5 √ L 3 t k ˙ X ( 4 / ( 5 √ L ) ) k 2 d t = − 3 k ˙ X ( 4 / ( 5 √ L ) ) k 2 log 5 T √ L 4 . Hence , we get an upper bound for T , T ≤ 4 5 √ L exp (cid:16) f ( x 0 ) − f ( X ( T ) ) 3 k ˙ X ( 4 / ( 5 √ L ) ) k 2 (cid:17) ≤ 4 5 √ L exp (cid:16) f ( x 0 ) − f ⋆ 3 k ˙ X ( 4 / ( 5 √ L ) ) k 2 (cid:17) . Plugging t = 4 / ( 5 √ L ) into ( 30 ) gives k ˙ X ( 4 / ( 5 √ L ) ) k ≥ C 1 √ L k∇ f ( x 0 ) k for some universal constant C 1 > 0 . Hence , from the last display we get T ≤ 4 5 √ L exp (cid:18) L ( f ( x 0 ) − f ⋆ ) 3 C 21 k∇ f ( x 0 ) k 2 (cid:19) ≤ 4 5 √ L exp L 6 C 21 µ . Now , we are ready to prove Theorem 5 . 1 by applying Lemmas 5 . 3 and 5 . 4 . Proof . Note that Lemma 5 . 4 asserts , by time t at least m : = ⌊ 5 t √ L e − ˜ CL / µ / 4 ⌋ restartings have occurred for X sr . Hence , recursively applying Lemma 5 . 3 , we have f ( X sr ( t ) ) − f ⋆ ≤ f ( X sr ( T 1 + · · · + T m ) ) − f ⋆ ≤ ( 1 − Cµ / L ) ( f ( X sr ( T 1 + · · · + T m − 1 ) ) − f ⋆ ) ≤ · · · ≤ · · · ≤ ( 1 − Cµ / L ) m ( f ( x 0 ) − f ⋆ ) ≤ e − Cµm / L ( f ( x 0 ) − f ⋆ ) ≤ c 1 e − c 2 t √ L ( f ( x 0 ) − f ⋆ ) ≤ c 1 L k x 0 − x ⋆ k 2 2 e − c 2 t √ L , where c 1 = exp ( Cµ / L ) and c 2 = 5 Cµe − ˜ Cµ / L / ( 4 L ) . In closing , we remark that we believe that estimate in Lemma 5 . 3 is tight , while not for Lemma 5 . 4 . Thus we conjecture that for a large class of f ∈ S µ , L , if not all , T = O ( √ L / µ ) . If this is true , the exponent constant c 2 in Theorem 5 . 1 can be signiﬁcantly improved . 21 5 . 3 Numerical Examples Below we present a discrete analog to the restarted scheme . There , k min is introduced to avoid having consecutive restarts that are too close . To compare the performance of the restarted scheme with the original ( 1 ) , we conduct four simulation studies , including both smooth and non - smooth objective functions . Note that the computational costs of the restarted and non - restarted schemes are the same . Algorithm 1 Speed Restarting Nesterov’s Scheme input : x 0 ∈ R n , y 0 = x 0 , x − 1 = x 0 , 0 < s ≤ 1 / L , k max ∈ N + and k min ∈ N + j ← 1 for k = 1 to k max do x k ← argmin x ( 12 s k x − y k − 1 + s ∇ g ( y k − 1 ) k 2 + h ( x ) ) y k ← x k + j − 1 j + 2 ( x k − x k − 1 ) if k x k − x k − 1 k < k x k − 1 − x k − 2 k and j ≥ k min then j ← 1 else j ← j + 1 end if end for Quadratic . f ( x ) = 12 x T Ax + b T x is a strongly convex function , in which A is a 500 × 500 random positive deﬁnite matrix and b a random vector . The eigenvalues of A are between 0 . 001 and 1 . The vector b is generated as i . i . d . Gaussian random variables with mean 0 and variance 25 . Log - sum - exp . f ( x ) = ρ log h m X i = 1 exp ( ( a Ti x − b i ) / ρ ) i , where n = 50 , m = 200 , ρ = 20 . The matrix A = ( a ij ) is a random matrix with i . i . d . standard Gaussian entries , and b = ( b i ) has i . i . d . Gaussian entries with mean 0 and variance 2 . This function is not strongly convex . Matrix completion . f ( X ) = 12 k X obs − M obs k 2 F + λ k X k ∗ , in which the ground truth M is a rank - 5 random matrix of size 300 × 300 . The regularization parameter is set to λ = 0 . 05 . The 5 singular values of M are 1 , . . . , 5 . The observed set is independently sampled among the 300 × 300 entries so that 10 % of the entries are actually observed . Lasso in ℓ 1 – constrained form with large sparse design . f ( x ) = 12 k Ax − b k 2 s . t . k x k 1 ≤ δ , where A is a 5000 × 50000 random sparse matrix with nonzero probability 0 . 5 % for each entry and b is generated as b = Ax 0 + z . The nonzero entries of A independently follow the Gaussian distribution with mean 0 and variance 0 . 04 . The signal x 0 is a vector with 250 nonzeros and z is i . i . d . standard Gaussian noise . The parameter δ is set to k x 0 k 1 . In these examples , k min is set to be 10 and the step sizes are ﬁxed to be 1 / L . If the objective is in composite form , the Lipschitz bound applies to the smooth part . Figures 6a , 6b , 6c and 6d present the performance of the speed restarting scheme , the gradient restarting scheme , the original Nesterov’s scheme and the proximal gradient method . The objective functions include strongly convex , non - strongly convex and non - smooth functions , violating the assumptions in Theorem 5 . 1 . Among all the examples , it is interesting to note that both speed restarting scheme empirically exhibit linear convergence by signiﬁcantly reducing bumps in the objective values . This leaves us an open problem of whether there exists provable linear convergence rate for the gradient restarting 22 0 200 400 600 800 1000 1200 1400 10 −6 10 −4 10 −2 10 0 10 2 10 4 10 6 10 8 iterations f − f * srNgrNoNPG ( a ) min 12 x T Ax + bx . 0 500 1000 1500 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 iterations f − f * srNgrNoNPG ( b ) min ρ log ( P mi = 1 exp ( ( a Ti x − b i ) / ρ ) ) . 0 20 40 60 80 100 120 140 160 180 200 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 iterations f − f * srNgrNoNPG ( c ) min 12 k X obs − M obs k 2F + λ k X k ∗ . 0 100 200 300 400 500 600 700 800 900 1000 1100 10 −10 10 −5 10 0 10 5 iterations f − f * srNgrNoNPG ( d ) min 12 k Ax − b k 2 s . t . k x k 1 ≤ C . Figure 6 : Numerical performance of speed restarting ( srN ) , gradient restarting ( grN ) , the original Nesterov’s scheme ( oN ) and the proximal gradient ( PG ) . scheme as in Theorem 5 . 1 . It is also worth pointing out that compared with gradient restarting , the speed restarting scheme empirically exhibits more stable linear convergence rate . 6 Discussion This paper introduces a second - order ODE and accompanying tools for characterizing Nesterov’s accelerated gradient method . This ODE is applied to study variants of Nesterov’s scheme and is capable of interpreting some empirically observed phenomena , such as oscillations along the trajectories . Our approach suggests ( 1 ) a large family of generalized Nesterov’s schemes that are all guaranteed to converge at the rate O ( 1 / k 2 ) , and ( 2 ) a restarting scheme provably achieving a linear convergence rate whenever f is strongly convex . In this paper , we often utilize ideas from continuous - time ODEs , and then apply these ideas to discrete schemes . The translation , however , involves parameter tuning and tedious calculations . This is the reason why a general theory mapping properties of ODEs into corresponding properties for discrete updates would be a welcome advance . Indeed , this would allow researchers to only study the simpler and more user - friendly ODEs . As evidenced by many examples , the viewpoint of regarding the ODE as a surrogate for Nes - terov’s scheme would allow a new perspective for studying accelerated methods in optimization . The discrete scheme and the ODE are closely connected by the exact mapping between the coeﬃ - cients of momentum ( e . g . ( k − 1 ) / ( k + 2 ) ) and velocity ( e . g . 3 / t ) . The derivations of generalized Nesterov’s schemes and the speed restarting scheme are both motivated by trying a diﬀerent veloc - ity coeﬃcient , in which the surprising phase transition at 3 is observed . Clearly , such alternatives 23 are endless , and we expect this will lead to ﬁndings of many discrete accelerated schemes . In a diﬀerent direction , a better understanding of the trajectory of the ODEs , such as curvature , has the potential to be helpful in deriving appropriate stopping criteria for termination , and choosing step size by backtracking . Acknowledgements W . S . is partially supported by a General Wang Yaowu Stanford Graduate Fellowship . S . B . is partially supported by DARPA XDATA . E . C . is partially supported by AFOSR under grant FA9550 - 09 - 1 - 0643 , by NSF under grant CCF - 0963835 and by the Math + X Award from the Simons Foundation . We would like to thank Carlos Sing - Long and Zhou Fan for helpful discussions about parts of this paper . Appendix A . Proof of Theorem 2 . 1 The proof is divided into two parts , namely , existence and uniqueness . Lemma . 1 . For any f ∈ F ∞ and any x 0 ∈ R n , the ODE ( 3 ) has at least one solution X in C 2 ( 0 , ∞ ) ∩ C 1 [ 0 , ∞ ) . Below , some preparatory lemmas are given before turning to the proof of this lemma . To begin with , for any δ > 0 consider the smoothed ODE ¨ X + 3 max ( δ , t ) ˙ X + ∇ f ( X ) = 0 ( 31 ) with X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 . Denoting by Z = ˙ X , then ( 31 ) is equivalent to d d t (cid:18) X Z (cid:19) = (cid:18) Z − 3 max ( δ , t ) Z − ∇ f ( X ) (cid:19) with X ( 0 ) = x 0 , Z ( 0 ) = 0 . As functions of ( X , Z ) , both Z and − 3 Z / max ( δ , t ) − ∇ f ( X ) ) are max ( 1 , L ) + 3 / δ - Lipschitz continuous . Hence by standard ODE theory , ( 31 ) has a unique global solution in C 2 [ 0 , ∞ ) , denoted by X δ . Note that ¨ X δ is also well deﬁned at t = 0 . Next , introduce M δ ( t ) to be the supremum of k ˙ X δ ( u ) k / u over u ∈ ( 0 , t ] . It is easy to see that M δ ( t ) is ﬁnite because k ˙ X δ ( u ) k / u = ( k ˙ X δ ( u ) − ˙ X δ ( 0 ) k ) / u = k ¨ X δ ( 0 ) k + o ( 1 ) for small u . We give an upper bound for M δ ( t ) in the following lemma . Lemma . 2 . For δ < p 6 / L , we have M δ ( δ ) ≤ k∇ f ( x 0 ) k 1 − Lδ 2 / 6 . The proof of Lemma . 2 relies on a simple lemma . Lemma . 3 . For any u > 0 , the following inequality holds k∇ f ( X δ ( u ) ) − ∇ f ( x 0 ) k ≤ 1 2 LM δ ( u ) u 2 . 24 Proof . By Lipschitz continuity , k∇ f ( X δ ( u ) ) − ∇ f ( x 0 ) k ≤ L k X δ ( u ) − x 0 k = (cid:13)(cid:13)(cid:13) Z u 0 ˙ X δ ( v ) d v (cid:13)(cid:13)(cid:13) ≤ Z u 0 v k ˙ X δ ( v ) k v d v ≤ 1 2 LM δ ( u ) u 2 . Next , we prove Lemma . 2 . Proof . For 0 < t ≤ δ , the smoothed ODE reads ¨ X δ + 3 δ ˙ X δ + ∇ f ( X δ ) = 0 , which yields ˙ X δ e 3 t / δ = − Z t 0 ∇ f ( X δ ( u ) ) e 3 u / δ d u = −∇ f ( x 0 ) Z t 0 e 3 u / δ d u − Z t 0 ( ∇ f ( X δ ( u ) ) − ∇ f ( x 0 ) ) e 3 u / δ d u . Hence , by Lemma . 3 k ˙ X δ ( t ) k t ≤ 1 t e − 3 t / δ k∇ f ( x 0 ) k Z t 0 e 3 u / δ d u + 1 t e − 3 t / δ Z t 0 1 2 LM δ ( u ) u 2 e 3 u / δ d u ≤ k∇ f ( x 0 ) k + LM δ ( δ ) δ 2 6 . Taking the supremum of k ˙ X δ ( t ) k / t over 0 < t ≤ δ and rearranging the inequality give the desired result . Next , we give an upper bound for M δ ( t ) when t > δ . Lemma . 4 . For δ < p 6 / L and δ < t < p 12 / L , we have M δ ( t ) ≤ ( 5 − Lδ 2 / 6 ) k∇ f ( x 0 ) k 4 ( 1 − Lδ 2 / 6 ) ( 1 − Lt 2 / 12 ) . Proof . For t > δ , the smoothed ODE reads ¨ X δ + 3 t ˙ X δ + ∇ f ( X δ ) = 0 , which is equivalent to d t 3 ˙ X δ ( t ) d t = − t 3 ∇ f ( X δ ( t ) ) . Hence , by integration , t 3 ˙ X δ ( t ) is equal to − Z t δ u 3 ∇ f ( X δ ( u ) ) d u + δ 3 ˙ X δ ( δ ) = − Z t δ u 3 ∇ f ( x 0 ) d u − Z t δ u 3 ( ∇ f ( X δ ( u ) ) −∇ f ( x 0 ) ) d u + δ 3 ˙ X δ ( δ ) . Therefore by Lemmas . 3 and . 2 , we get k ˙ X δ ( t ) k t ≤ t 4 − δ 4 4 t 4 k∇ f ( x 0 ) k + 1 t 4 Z t δ 1 2 LM δ ( u ) u 5 d u + δ 4 t 4 k ˙ X δ ( δ ) k δ ≤ 1 4 k∇ f ( x 0 ) k + 1 12 LM δ ( t ) t 2 + k∇ f ( X 0 ) k 1 − Lδ 2 / 6 , 25 where the last expression is an increasing function of t . So for any δ < t ′ < t , it follows that k ˙ X δ ( t ′ ) k t ′ ≤ 1 4 k∇ f ( x 0 ) k + 1 12 LM δ ( t ) t 2 + k∇ f ( x 0 ) k 1 − Lδ 2 / 6 , which also holds for t ′ ≤ δ . Taking the supremum over t ′ ∈ ( 0 , t ) gives M δ ( t ) ≤ 1 4 k∇ f ( x 0 ) k + 1 12 LM δ ( t ) t 2 + k∇ f ( X 0 ) k 1 − Lδ 2 / 6 . The desired result follows from rearranging the inequality . Lemma . 5 . The function class F = { X δ : h 0 , p 6 / L i → R n (cid:12)(cid:12) δ = p 3 / L / 2 m , m = 0 , 1 , . . . } is uniformly bounded and equicontinuous . Proof . By Lemmas . 2 and . 4 , for any t ∈ [ 0 , p 6 / L ] , δ ∈ ( 0 , p 3 / L ) the gradient is uniformly bounded as k ˙ X δ ( t ) k ≤ p 6 / LM δ ( p 6 / L ) ≤ p 6 / L max n k∇ f ( x 0 ) k 1 − 12 , 5 k∇ f ( x 0 ) k 4 ( 1 − 12 ) ( 1 − 12 ) o = 5 p 6 / L k∇ f ( x 0 ) k . Thus it immediately implies that F is equicontinuous . To establish the uniform boundedness , note that k X δ ( t ) k ≤ k X δ ( 0 ) k + Z t 0 k ˙ X δ ( u ) k d u ≤ k x 0 k + 30 k∇ f ( x 0 ) k / L . We are now ready for the proof of Lemma . 1 . Proof . By the Arzel´a - Ascoli theorem and Lemma . 5 , F contains a subsequence converging uniformly on [ 0 , p 6 / L ] . Denote by { X δ mi } i ∈ N the convergent subsequence and ˘ X the limit . Above , δ m i = p 3 / L / 2 m i decreases as i increases . We will prove that ˘ X satisﬁes ( 3 ) and the initial conditions ˘ X ( 0 ) = x 0 , ˙˘ X ( 0 ) = 0 . Fix an arbitrary t 0 ∈ ( 0 , p 6 / L ) . Since k ˙ X δ mi ( t 0 ) k is bounded , we can pick a subsequence of ˙ X δ mi ( t 0 ) which converges to a limit , denoted by X Dt 0 . Without loss of generality , assume the subsequence is the original sequence . Denote by ˜ X the local solution to ( 3 ) with X ( t 0 ) = ˘ X ( t 0 ) and ˙ X ( t 0 ) = X D t 0 . Now recall that X δ mi is the solution to ( 3 ) with X ( t 0 ) = X δ mi ( t 0 ) and ˙ X ( t 0 ) = ˙ X δ mi ( t 0 ) when δ m i < t 0 . Since both X δ mi ( t 0 ) and ˙ X δ mi ( t 0 ) approach ˘ X ( t 0 ) and X Dt 0 , respectively , there exists ǫ 0 > 0 such that sup t 0 − ǫ 0 < t < t 0 + ǫ 0 k X δ mi ( t ) − ˜ X ( t ) k → 0 as i → ∞ . However , by deﬁnition we have sup t 0 − ǫ 0 < t < t 0 + ǫ 0 k X δ mi ( t ) − ˘ X ( t ) k → 0 . Therefore ˘ X and ˜ X have to be identical on ( t 0 − ǫ 0 , t 0 + ǫ 0 ) . So ˘ X satisﬁes ( 3 ) at t 0 . Since t 0 is arbitrary , we conclude that ˘ X is a solution to ( 3 ) on ( 0 , p 6 / L ) . By extension , ˘ X can be a global solution to ( 3 ) on ( 0 , ∞ ) . It only leaves to verify the initial conditions to complete the proof . 26 The ﬁrst condition ˘ X ( 0 ) = x 0 is a direct consequence of X δ mi ( 0 ) = x 0 . To check the second , pick a small t > 0 and note that k ˘ X ( t ) − ˘ X ( 0 ) k t = lim i →∞ k X δ mi ( t ) − X δ mi ( 0 ) k t = lim i →∞ k ˙ X δ mi ( ξ i ) k ≤ lim sup i →∞ tM δ mi ( t ) ≤ 5 t p 6 / L k∇ f ( x 0 ) k , where ξ i ∈ ( 0 , t ) is given by the mean value theorem . The desired result follows from taking t → 0 . Next , we aim to prove the uniqueness of the solution to ( 3 ) . Lemma . 6 . For any f ∈ F ∞ , the ODE ( 3 ) has at most one local solution in a neighborhood of t = 0 . Suppose on the contrary that there are two solutions , namely , X and Y , both deﬁned on ( 0 , α ) for some α > 0 . Deﬁne ˜ M ( t ) to be the supremum of k ˙ X ( u ) − ˙ Y ( u ) k over u ∈ [ 0 , t ) . To proceed , we need a simple auxiliary lemma . Lemma . 7 . For any t ∈ ( 0 , α ) , we have k∇ f ( X ( t ) ) − ∇ f ( Y ( t ) ) k ≤ Lt ˜ M ( t ) . Proof . By Lipschitz continuity of the gradient , one has k∇ f ( X ( t ) ) − ∇ f ( Y ( t ) ) k ≤ L k X ( t ) − Y ( t ) k = L (cid:13)(cid:13) (cid:13) Z t 0 ˙ X ( u ) − ˙ Y ( u ) d u + X ( 0 ) − Y ( 0 ) (cid:13)(cid:13) (cid:13) ≤ L Z t 0 k ˙ X ( u ) − ˙ Y ( u ) k d u ≤ Lt ˜ M ( t ) . Now we prove Lemma . 6 . Proof . Similar to the proof of Lemma . 4 , we get t 3 ( ˙ X ( t ) − ˙ Y ( t ) ) = − Z t 0 u 3 ( ∇ f ( X ( u ) ) − ∇ f ( Y ( u ) ) ) d u . Applying Lemma . 7 gives t 3 k ˙ X ( t ) − ˙ Y ( t ) k ≤ Z t 0 Lu 4 ˜ M ( u ) d u ≤ 1 5 Lt 5 ˜ M ( t ) , which can be simpliﬁed as k ˙ X ( t ) − ˙ Y ( t ) k ≤ Lt 2 ˜ M ( t ) / 5 . Thus , for any t ′ ≤ t it is true that k ˙ X ( t ′ ) − ˙ Y ( t ′ ) k ≤ Lt 2 ˜ M ( t ) / 5 . Taking the supremum of k ˙ X ( t ′ ) − ˙ Y ( t ′ ) k over t ′ ∈ ( 0 , t ) gives ˜ M ( t ) ≤ Lt 2 ˜ M ( t ) / 5 . Therefore ˜ M ( t ) = 0 for t < min ( α , p 5 / L ) , which is equivalent to saying ˙ X = ˙ Y on [ 0 , min ( α , p 5 / L ) ) . With the same initial value X ( 0 ) = Y ( 0 ) = x 0 and the same gradient , we conclude that X and Y are identical on ( 0 , min ( α , p 5 / L ) ) , a contradiction . Given all of the aforementioned lemmas , the proof of Theorem 2 . 1 is simply combining . 1 and . 6 . 27 Appendix B . Proof of Theorem 2 . 2 Identifying √ s = ∆ t , the comparison between ( 4 ) and ( 15 ) reveals that Nesterov’s scheme is a discrete scheme for numerically integrating the ODE ( 3 ) . However , its singularity of the damping coeﬃcient at t = 0 leads to the nonexistence of oﬀ - the - shelf ODE theory for proving Theorem 2 . 2 . To address this diﬃculty , we use the smoothed ODE ( 31 ) to approximate the original one ; then bound the diﬀerence between Nesterov’s scheme and the forward Euler scheme of ( 31 ) , which may take the following form : X δk + 1 = X δk + ∆ tZ δk Z δk + 1 = (cid:16) 1 − 3∆ t max { δ , k ∆ t } (cid:17) Z δk − ∆ t ∇ f ( X δk ) ( 32 ) with X δ 0 = x 0 and Z δ 0 = 0 . Lemma . 8 . With step size ∆ t = √ s , for any T > 0 we have max 1 ≤ k ≤ T √ s k X δk − x k k ≤ Cδ 2 + o s ( 1 ) for some constant C . Proof . Let z k = ( x k + 1 − x k ) / √ s . Then Nesterov’s scheme is equivalent to x k + 1 = x k + √ sz k z k + 1 = (cid:16) 1 − 3 k + 3 (cid:17) z k − √ s ∇ f (cid:16) x k + 2 k + 3 k + 3 √ sz k (cid:17) . ( 33 ) Denote by a k = k X δk − x k k , b k = k Z δk − z k k , whose initial values are a 0 = 0 and b 0 = k∇ f ( x 0 ) k√ s . The idea of this proof is to bound a k via simultaneously estimating a k and b k . By comparing ( 32 ) and ( 33 ) , we get the iterative relationship for a k : a k + 1 ≤ a k + √ sb k . Denoting by S k = b 0 + b 1 + · · · + b k , this yields a k ≤ √ sS k − 1 . ( 34 ) Similarly , for suﬃciently small s we get b k + 1 ≤ (cid:12)(cid:12)(cid:12) 1 − 3 max { δ / √ s , k } (cid:12)(cid:12)(cid:12) b k + L √ sa k + (cid:16)(cid:12)(cid:12)(cid:12) 3 k + 3 − 3 max { δ / √ s , k } (cid:12)(cid:12)(cid:12) + 2 Ls (cid:17) k z k k ≤ b k + L √ sa k + (cid:16)(cid:12)(cid:12)(cid:12) 3 k + 3 − 3 max { δ / √ s , k } (cid:12)(cid:12)(cid:12) + 2 Ls (cid:17) k z k k . To upper bound k z k k , denoting by C 1 the supremum of p 2 L ( f ( y k ) − f ⋆ ) over all k and s , we have k z k k ≤ k − 1 k + 2 k z k − 1 k + √ s k∇ f ( y k ) k ≤ k z k − 1 k + C 1 √ s , which gives k z k k ≤ C 1 ( k + 1 ) √ s . Hence , (cid:16) (cid:12) (cid:12)(cid:12) 3 k + 3 − 3 max { δ / √ s , k } (cid:12) (cid:12)(cid:12) + 2 Ls (cid:17) k z k k ≤ ( C 2 √ s , k ≤ δ √ s C 2 √ s k < C 2 sδ , k > δ √ s . 28 Making use of ( 34 ) gives b k + 1 ≤ ( b k + LsS k − 1 + C 2 √ s , k ≤ δ / √ s b k + LsS k − 1 + C 2 sδ , k > δ / √ s . ( 35 ) By induction on k , for k ≤ δ / √ s it holds that b k ≤ C 1 Ls + C 2 + ( C 1 + C 2 ) √ Ls 2 √ L ( 1 + √ Ls ) k − 1 − C 1 Ls + C 2 − ( C 1 + C 2 ) √ Ls 2 √ L ( 1 − √ Ls ) k − 1 . Hence , S k ≤ C 1 Ls + C 2 + ( C 1 + C 2 ) √ Ls 2 L √ s ( 1 + √ Ls ) k + C 1 Ls + C 2 − ( C 1 + C 2 ) √ Ls 2 L √ s ( 1 − √ Ls ) k − C 2 L √ s . Letting k ⋆ = ⌊ δ / √ s ⌋ , we get lim sup s → 0 √ sS k ⋆ − 1 ≤ C 2 e δ √ L + C 2 e − δ √ L − 2 C 2 2 L = O ( δ 2 ) , which allows us to conclude that a k ≤ √ sS k − 1 = O ( δ 2 ) + o s ( 1 ) ( 36 ) for all k ≤ δ / √ s . Next , we bound b k for k > k ⋆ = ⌊ δ / √ s ⌋ . To this end , we consider the worst case of ( 35 ) , that is , b k + 1 = b k + LsS k − 1 + C 2 s δ for k > k ⋆ and S k ⋆ = S k ⋆ + 1 = C 3 δ 2 / √ s + o s ( 1 / √ s ) for some suﬃciently large C 3 . In this case , C 2 s / δ < sS k − 1 for suﬃciently small s . Hence , the last display gives b k + 1 ≤ b k + ( L + 1 ) sS k − 1 . By induction , we get S k ≤ C 3 δ 2 / √ s + o s ( 1 / √ s ) 2 (cid:16) ( 1 + p ( L + 1 ) s ) k − k ⋆ + ( 1 − p ( L + 1 ) s ) k − k ⋆ (cid:17) . Letting k ⋄ = ⌊ T / √ s ⌋ , we further get lim sup s → 0 √ sS k ⋄ ≤ C 3 δ 2 ( e ( T − δ ) √ L + 1 + e − ( T − δ ) √ L + 1 ) 2 = O ( δ 2 ) , which yields a k ≤ √ sS k − 1 = O ( δ 2 ) + o s ( 1 ) for k ⋆ < k ≤ k ⋄ . Last , combining ( 36 ) and the last display , we get the desired result . Now we turn to the proof of Theorem 2 . 2 . 29 Proof . Note the triangular inequality k x k − X ( k √ s ) k ≤ k x k − X δk k + k X δk − X δ ( k √ s ) k + k X δ ( k √ s ) − X ( k √ s ) k , where X δ ( · ) is the solution to the smoothed ODE ( 31 ) . The proof of Lemma . 1 implies that , we can choose a sequence δ m → 0 such that sup 0 ≤ t ≤ T k X δ m ( t ) − X ( t ) k → 0 . The second term k X δ m k − X δ m ( k √ s ) k will uniformly vanish as s → 0 and so does the ﬁrst term k x k − X δ m k k if ﬁrst s → 0 and then δ m → 0 . This completes the proof . Appendix C . ODE for Composite Optimization In analogy to ( 3 ) for smooth f in Section 2 , we develop an ODE for composite optimization , minimize f ( x ) = g ( x ) + h ( x ) , ( 37 ) where g ∈ F L and h is a general convex function possibly taking on the value + ∞ . Provided it is easy to evaluate the proximal of h , Beck and Teboulle ( 2009 ) propose a proximal gradient version of Nesterov’s scheme for solving ( 37 ) . It is to repeat the following recursion for k ≥ 1 , x k = y k − 1 − sG t ( y k − 1 ) y k = x k + k − 1 k + 2 ( x k − x k − 1 ) , where the proximal subgradient G s has been deﬁned in Section 4 . 1 . If the constant step size s ≤ 1 / L , it is guaranteed that ( Beck and Teboulle , 2009 ) f ( x k ) − f ⋆ ≤ 2 k x 0 − x ⋆ k 2 s ( k + 1 ) 2 , which in fact is a special case of Theorem 4 . 2 . Compared to the smooth case , it is not as clear to deﬁne the driving force as ∇ f in ( 3 ) . At ﬁrst , it might be a good try to deﬁne G ( x ) = lim s → 0 G s ( x ) = lim s → 0 x − argmin z (cid:0) k z − ( x − s ∇ g ( x ) ) k 2 / ( 2 s ) + h ( z ) (cid:1) s , if it exists . However , as implied in the proof of Theorem . 11 stated below , this deﬁnition fails to capture the directional aspect of the subgradient . To this end , we deﬁne the subgradients through the following lemma . Lemma . 9 . ( Rockafellar , 1997 ) For any convex function f and any x , p ∈ R n , the directional derivative lim t → 0 + ( f ( x + sp ) − f ( x ) ) / s exists , and can be evaluated as lim s → 0 + f ( x + sp ) − f ( x ) s = sup ξ ∈ ∂f ( x ) h ξ , p i . Note that the directional derivative is semilinear in p because sup ξ ∈ ∂f ( x ) h ξ , cp i = c sup ξ ∈ ∂f ( x ) h ξ , p i for any c > 0 . 30 Deﬁnition . 10 . A Borel measurable function G ( x , p ; f ) deﬁned on R n × R n is said to be a directional subgradient of f if G ( x , p ) ∈ ∂f ( x ) , h G ( x , p ) , p i = sup ξ ∈ ∂f ( x ) h ξ , p i for all x , p . Convex functions are naturally locally Lipschitz , so ∂f ( x ) is compact for any x . Consequently there exists ξ ∈ ∂f ( x ) which maximizes h ξ , p i . So Lemma . 9 guarantees the existence of a directional subgradient . The function G is essentially a function deﬁned on R n × S n − 1 in that we can deﬁne G ( x , p ) = G ( x , p / k p k ) , and G ( x , 0 ) to be any element in ∂f ( x ) . Now we give the main theorem . However , note that we do not guarantee the existence of solution to ( 38 ) . Theorem . 11 . Given a convex function f ( x ) with directional subgradient G ( x , p ; f ) , assume that the second order ODE ¨ X + 3 t ˙ X + G ( X , ˙ X ) = 0 , X ( 0 ) = x 0 , ˙ X ( 0 ) = 0 ( 38 ) admits a solution X ( t ) on [ 0 , α ) for some α > 0 . Then for any 0 < t < α , we have f ( X ( t ) ) − f ⋆ ≤ 2 k x 0 − x ⋆ k 22 t 2 . Proof . It suﬃces to establish that E , ﬁrst deﬁned in the proof of Theorem 3 . 1 , is monotonically decreasing . The diﬃculty comes from that E may not be diﬀerentiable in this setting . Instead , we study ( E ( t + ∆ t ) − E ( t ) ) / ∆ t for small ∆ t > 0 . In E , the second term 2 k X + t ˙ X / 2 − x ⋆ k 2 is diﬀerentiable , with derivative 4 h X + t 2 ˙ X − x ⋆ , 32 ˙ X + t 2 ¨ X i . Hence , 2 k X ( t + ∆ t ) + t 2 ˙ X ( t + ∆ t ) − x ⋆ k 2 − 2 k X ( t ) + t 2 ˙ X ( t ) − x ⋆ k 2 = 4 h X + t 2 ˙ X − x ⋆ , 3 2 ˙ X + t 2 ¨ X i ∆ t + o ( ∆ t ) = − t 2 h ˙ X , G ( X , ˙ X ) i ∆ t − 2 t h X − x ⋆ , G ( X , ˙ X ) i ∆ t + o ( ∆ t ) . ( 39 ) For the ﬁrst term , note that ( t + ∆ t ) 2 ( f ( X ( t + ∆ t ) ) − f ⋆ ) − t 2 ( f ( X ( t ) ) − f ⋆ ) = 2 t ( f ( X ( t + ∆ t ) ) − f ⋆ ) ∆ t + t 2 ( f ( X ( t + ∆ t ) ) − f ( X ( t ) ) ) + o ( ∆ t ) . Since f is locally Lipschitz , o ( ∆ t ) term does not aﬀect the function in the limit , f ( X ( t + ∆ t ) ) = f ( X + ∆ t ˙ X + o ( ∆ t ) ) = f ( X + ∆ t ˙ X ) + o ( ∆ t ) . ( 40 ) By Lemma . 9 , we have the approximation f ( X + ∆ t ˙ X ) = f ( X ) + h ˙ X , G ( X , ˙ X ) i ∆ t + o ( ∆ t ) . ( 41 ) 31 Combining all of ( 39 ) , ( 40 ) and ( 41 ) , we obtain E ( t + ∆ t ) − E ( t ) = 2 t ( f ( X ( t + ∆ t ) ) − f ⋆ ) ∆ t + t 2 h ˙ X , G ( X , ˙ X ) i ∆ t − t 2 h ˙ X , G ( X , ˙ X ) i ∆ t − 2 t h X − x ⋆ , G ( X , ˙ X ) i ∆ t + o ( ∆ t ) = 2 t ( f ( X ) − f ⋆ ) ∆ t − 2 t h X − x ⋆ , G ( X , ˙ X ) i ∆ t + o ( ∆ t ) ≤ o ( ∆ t ) , where the last inequality follows from the convexity of f . Thus , lim sup ∆ t → 0 + E ( t + ∆ t ) − E ( t ) ∆ t ≤ 0 , which along with the continuity of E , concludes that E ( t ) is a non - increasing function of t . We give a simple example as follows . Consider the Lasso problem minimize 1 2 k y − Ax k 2 + λ k x k 1 . Any directional subgradients admits the form G ( x , p ) = − A T ( y − Ax ) + λ sgn ( x , p ) , where sgn ( x , p ) i =    sgn ( x i ) , x i 6 = 0 sgn ( p i ) , x i = 0 , p i 6 = 0 ∈ [ − 1 , 1 ] , x i = 0 , p i = 0 . To encourage sparsity , for any index i with x i = 0 , p i = 0 , we let G ( x , p ) i = sgn (cid:0) A Ti ( Ax − y ) (cid:1) (cid:0) | A Ti ( Ax − y ) | − λ (cid:1) + . Appendix D . Proof of Theorem 4 . 5 Proof . To begin with , for f = g + h , with g a µ – strongly convex function and h convex , the inequality ( 22 ) can be strengthened to f ( y − sG s ( y ) ) ≤ f ( x ) + G s ( y ) T ( y − x ) − s 2 k G s ( y ) k 2 − µ 2 k y − x k 2 . ( 42 ) Summing ( 4 k − 3 ) × ( 42 ) with x = x k − 1 , y = y k − 1 and ( 4 r − 6 ) × ( 42 ) with x = x ⋆ , y = y k − 1 yields ( 4 k + 4 r − 9 ) f ( x k ) ≤ ( 4 k − 3 ) f ( x k − 1 ) + ( 4 r − 6 ) f ⋆ + G s ( y k − 1 ) T [ ( 4 k + 4 r − 9 ) y k − 1 − ( 4 k − 3 ) x k − 1 − ( 4 r − 6 ) x ⋆ ] − s ( 4 k + 4 r − 9 ) 2 k G s ( y k − 1 ) k 2 − µ ( 4 k − 3 ) 2 k y k − 1 − x k − 1 k 2 − µ ( 2 r − 3 ) k y k − 1 − x ⋆ k 2 ≤ ( 4 k − 3 ) f ( x k − 1 ) + ( 4 r − 6 ) f ⋆ + G s ( y k − 1 ) T [ ( 4 k + 4 r − 9 ) ( y k − 1 − x ⋆ ) − ( 4 k − 3 ) ( x k − 1 − x ⋆ ) ] − µ ( 2 r − 3 ) k y k − 1 − x ⋆ k 2 , ( 43 ) which amounts to a lower bound of G s ( y k − 1 ) T [ ( 4 k + 4 r − 9 ) y k − 1 − ( 4 k − 3 ) x k − 1 − ( 4 r − 6 ) x ⋆ ] . De - note by ∆ k the second term of ˜ E ( k ) in ( 28 ) , namely , ∆ k , k + d 8 k ( 2 k + 2 r − 2 ) ( y k − x ⋆ ) − ( 2 k + 1 ) ( x k − x ⋆ ) k 2 , 32 where d : = 3 r / 2 − 5 / 2 . Then by ( 43 ) , we get ∆ k − ∆ k − 1 = − k + d 8 D s ( 2 r + 2 k − 5 ) G s ( y k − 1 ) + k − 2 k + r − 2 ( x k − 1 − x k − 2 ) , ( 4 k + 4 r − 9 ) ( y k − 1 − x ⋆ ) − ( 4 k − 3 ) ( x k − 1 − x ⋆ ) E + 1 8 k ( 2 k + 2 r − 4 ) ( y k − 1 − x ⋆ ) − ( 2 k − 1 ) ( x k − 1 − x ⋆ ) k 2 ≤ − s ( k + d ) ( 2 k + 2 r − 5 ) 8 [ ( 4 k + 4 r − 9 ) ( f ( x k ) − f ⋆ ) − ( 4 k − 3 ) ( f ( x k − 1 ) − f ⋆ ) + µ ( 2 r − 3 ) k y k − 1 − x ⋆ k 2 ] − ( k + d ) ( k − 2 ) 8 ( k + r − 2 ) h x k − 1 − x k − 2 , ( 4 k + 4 r − 9 ) ( y k − 1 − x ⋆ ) − ( 4 k − 3 ) ( x k − 1 − x ⋆ ) i + 1 8 k 2 ( k + r − 2 ) ( y k − 1 − x ⋆ ) − ( 2 k − 1 ) ( x k − 1 − x ⋆ ) k 2 . Hence , ∆ k + s ( k + d ) ( 2 k + 2 r − 5 ) ( 4 k + 4 r − 9 ) 8 ( f ( x k ) − f ⋆ ) ≤ ∆ k − 1 + s ( k + d ) ( 2 k + 2 r − 5 ) ( 4 k − 3 ) 8 ( f ( x k − 1 ) − f ⋆ ) − sµ ( 2 r − 3 ) ( k + d ) ( 2 k + 2 r − 5 ) 8 k y k − 1 − x ⋆ k 2 + Π 1 + Π 2 , ( 44 ) where Π 1 , − ( k + d ) ( k − 2 ) 8 ( k + r − 2 ) h x k − 1 − x k − 2 , ( 4 k + 4 r − 9 ) ( y k − 1 − x ⋆ ) − ( 4 k − 3 ) ( x k − 1 − x ⋆ ) i and Π 2 , 1 8 k 2 ( k + r − 2 ) ( y k − 1 − x ⋆ ) − ( 2 k − 1 ) ( x k − 1 − x ⋆ ) k 2 . By the iterations deﬁned in ( 19 ) , one can show that Π 1 = − ( 2 r − 3 ) ( k + d ) ( k − 2 ) 8 ( k + r − 2 ) ( k x k − 1 − x ⋆ k 2 − k x k − 2 − x ⋆ k 2 ) − ( k − 2 ) 2 ( 4 k + 4 r − 9 ) ( k + d ) + ( 2 r − 3 ) ( k − 2 ) ( k + r − 2 ) ( k + d ) 8 ( k + r − 2 ) 2 k x k − 1 − x k − 2 k 2 and Π 2 = ( 2 r − 3 ) 2 8 k y k − 1 − x ⋆ k 2 + ( 2 r − 3 ) ( 2 k − 1 ) ( k − 2 ) 8 ( k + r − 2 ) ( k x k − 1 − x ⋆ k 2 − k x k − 2 − x ⋆ k 2 ) + ( k − 2 ) 2 ( 2 k − 1 ) ( 2 k + 4 r − 7 ) + ( 2 r − 3 ) ( 2 k − 1 ) ( k − 2 ) ( k + r − 2 ) 8 ( k + r − 2 ) 2 k x k − 1 − x k − 2 k 2 . Although this is a little tedious , it is straightforward to check that ( k − 2 ) 2 ( 4 k + 4 r − 9 ) ( k + d ) + ( 2 r − 3 ) ( k − 2 ) ( k + r − 2 ) ( k + d ) ≥ ( k − 2 ) 2 ( 2 k − 1 ) ( 2 k + 4 r − 7 ) + ( 2 r − 3 ) ( 2 k − 1 ) ( k − 2 ) ( k + r − 2 ) for any k . Therefore , Π 1 + Π 2 is bounded as Π 1 + Π 2 ≤ ( 2 r − 3 ) 2 8 k y k − 1 − x ⋆ k 2 + ( 2 r − 3 ) ( k − d − 1 ) ( k − 2 ) 8 ( k + r − 2 ) ( k x k − 1 − x ⋆ k 2 − k x k − 2 − x ⋆ k 2 ) , 33 which , together with the fact that sµ ( 2 r − 3 ) ( k + d ) ( 2 k + 2 r − 5 ) ≥ ( 2 r − 3 ) 2 when k ≥ p ( 2 r − 3 ) / ( 2 sµ ) , reduces ( 44 ) to ∆ k + s ( k + d ) ( 2 k + 2 r − 5 ) ( 4 k + 4 r − 9 ) 8 ( f ( x k ) − f ⋆ ) ≤ ∆ k − 1 + s ( k + d ) ( 2 k + 2 r − 5 ) ( 4 k − 3 ) 8 ( f ( x k − 1 ) − f ⋆ ) + ( 2 r − 3 ) ( k − d − 1 ) ( k − 2 ) 8 ( k + r − 2 ) ( k x k − 1 − x ⋆ k 2 − k x k − 2 − x ⋆ k 2 ) . This can be further simpliﬁed as ˜ E ( k ) + A k ( f ( x k − 1 ) − f ⋆ ) ≤ ˜ E ( k − 1 ) + B k ( k x k − 1 − x ⋆ k 2 − k x k − 2 − x ⋆ k 2 ) ( 45 ) for k ≥ p ( 2 r − 3 ) / ( 2 sµ ) , where A k = ( 8 r − 36 ) k 2 + ( 20 r 2 − 126 r + 200 ) k + 12 r 3 − 100 r 2 + 288 r − 281 > 0 since r ≥ 9 / 2 and B k = ( 2 r − 3 ) ( k − d − 1 ) ( k − 2 ) / ( 8 ( k + r − 2 ) ) . Denote by k ⋆ = ⌈ max { p ( 2 r − 3 ) / ( 2 sµ ) , 3 r / 2 − 3 / 2 } ⌉ ≍ 1 / √ sµ . Then B k is a positive increasing sequence if k > k ⋆ . Summing ( 45 ) from k to k ⋆ + 1 , we obtain E ( k ) + k X i = k ⋆ + 1 A i ( f ( x i − 1 ) − f ⋆ ) ≤ E ( k ⋆ ) + k X i = k ⋆ + 1 B i ( k x i − 1 − x ⋆ k 2 − k x i − 2 − x ⋆ k 2 ) = E ( k ⋆ ) + B k k x k − 1 − x ⋆ k 2 − B k ⋆ + 1 k x k ⋆ − 1 − x ⋆ k 2 + k − 1 X i = k ⋆ + 1 ( B j − B j + 1 ) k x j − 1 − x ⋆ k 2 ≤ E ( k ⋆ ) + B k k x k − 1 − x ⋆ k 2 . Similarly , as in the proof of Theorem 4 . 4 , we can bound E ( k ⋆ ) via another energy functional deﬁned from Theorem 4 . 1 , E ( k ⋆ ) ≤ s ( 2 k ⋆ + 3 r − 5 ) ( k ⋆ + r − 2 ) 2 2 ( f ( x k ⋆ ) − f ⋆ ) + 2 k ⋆ + 3 r − 5 16 k 2 ( k ⋆ + r − 1 ) y k ⋆ − 2 k ⋆ x k ⋆ − 2 ( r − 1 ) x ⋆ − ( x k ⋆ − x ⋆ ) k 2 ≤ s ( 2 k ⋆ + 3 r − 5 ) ( k ⋆ + r − 2 ) 2 2 ( f ( x k ⋆ ) − f ⋆ ) + 2 k ⋆ + 3 r − 5 8 k 2 ( k ⋆ + r − 1 ) y k ⋆ − 2 k ⋆ x k ⋆ − 2 ( r − 1 ) x ⋆ k 2 + 2 k ⋆ + 3 r − 5 8 k x k ⋆ − x ⋆ k 2 ≤ ( r − 1 ) 2 ( 2 k ⋆ + 3 r − 5 ) 2 k x 0 − x ⋆ k 2 + ( r − 1 ) 2 ( 2 k ⋆ + 3 r − 5 ) 8 sµ ( k ⋆ + r − 2 ) 2 k x 0 − x ⋆ k 2 . k x 0 − x ⋆ k 2 √ sµ . ( 46 ) For the second term , it follows from Theorem 4 . 2 that B k k x k − 1 − x ⋆ k 2 ≤ ( 2 r − 3 ) ( 2 k − 3 r + 3 ) ( k − 2 ) 8 µ ( k + r − 2 ) ( f ( x k − 1 ) − x ⋆ ) ≤ ( 2 r − 3 ) ( 2 k − 3 r + 3 ) ( k − 2 ) 8 µ ( k + r − 2 ) ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 s ( k + r − 3 ) 2 ≤ ( 2 r − 3 ) ( r − 1 ) 2 ( 2 k ⋆ − 3 r + 3 ) ( k ⋆ − 2 ) 16 sµ ( k ⋆ + r − 2 ) ( k ⋆ + r − 3 ) 2 k x 0 − x ⋆ k 2 . k x 0 − x ⋆ k 2 √ sµ . ( 47 ) 34 For k > k ⋆ , ( 46 ) together with ( 47 ) this gives f ( x k ) − f ⋆ ≤ 16 E ( k ) s ( 2 k + 3 r − 5 ) ( 2 k + 2 r − 5 ) ( 4 k + 4 r − 9 ) ≤ 16 ( E ( k ⋆ ) + B k k x k − 1 − x ⋆ k 2 ) s ( 2 k + 3 r − 5 ) ( 2 k + 2 r − 5 ) ( 4 k + 4 r − 9 ) . k x 0 − x ⋆ k 2 s 32 µ 12 k 3 . To conclusion , note that by Theorem 4 . 2 the gap f ( x k ) − f ⋆ for k ≤ k ⋆ is bounded by ( r − 1 ) 2 k x 0 − x ⋆ k 2 2 s ( k + r − 2 ) 2 = ( r − 1 ) 2 √ sµk 3 2 ( k + r − 2 ) 2 k x 0 − x ⋆ k 2 s 32 µ 12 k 3 . √ sµk ⋆ k x 0 − x ⋆ k 2 s 32 µ 12 k 3 . k x 0 − x ⋆ k 2 s 32 µ 12 k 3 . Appendix E . Proof of Lemmas in Section 5 First , we prove Lemma 5 . 2 . Proof . To begin with , note that the ODE ( 3 ) is equivalent to d ( t 3 ˙ X ( t ) ) / d t = − t 3 ∇ f ( X ( t ) ) , which by integration leads to t 3 ˙ X ( t ) = − t 4 4 ∇ f ( x 0 ) − Z t 0 u 3 ( ∇ f ( X ( u ) ) − ∇ f ( x 0 ) ) d u = − t 4 4 ∇ f ( x 0 ) − I ( t ) . ( 48 ) Dividing ( 48 ) by t 4 and applying the bound on I ( t ) , we obtain k ˙ X ( t ) k t ≤ k∇ f ( x 0 ) k 4 + k I ( t ) k t 4 ≤ k∇ f ( x 0 ) k 4 + LM ( t ) t 2 12 . Note that the right - hand side of the last display is monotonically increasing in t . Hence , by taking the supremum of the left - hand side over ( 0 , t ] , we get M ( t ) ≤ k∇ f ( x 0 ) k 4 + LM ( t ) t 2 12 , which completes the proof by rearrangement . Next , we prove the lemma used in the proof of Lemma 5 . 3 . Lemma . 12 . The speed restarting time T obeys T ( x 0 , f ) ≥ 4 5 √ L . Proof . The proof is based on studying h ˙ X ( t ) , ¨ X ( t ) i . Dividing ( 48 ) by t 3 , we get an expression for ˙ X , ˙ X ( t ) = − t 4 ∇ f ( x 0 ) − 1 t 3 Z t 0 u 3 ( ∇ f ( X ( u ) ) − ∇ f ( x 0 ) ) d u . ( 49 ) Diﬀerentiating the above , we also obtain an expression for ¨ X : ¨ X ( t ) = −∇ f ( X ( t ) ) + 3 4 ∇ f ( x 0 ) + 3 t 4 Z t 0 u 3 ( ∇ f ( X ( u ) ) − ∇ f ( x 0 ) ) d u . ( 50 ) 35 Using the two equations we can show that d k ˙ X k 2 / d t = 2 h ˙ X ( t ) , ¨ X ( t ) i > 0 for 0 < t < 4 / ( 5 √ L ) . Continue by observing that ( 49 ) and ( 50 ) yield h ˙ X ( t ) , ¨ X ( t ) i = D − t 4 ∇ f ( x 0 ) − 1 t 3 I ( t ) , − ∇ f ( X ( t ) ) + 3 4 ∇ f ( x 0 ) + 3 t 4 I ( t ) E ≥ t 4 h∇ f ( x 0 ) , ∇ f ( X ( t ) ) i − 3 t 16 k∇ f ( x 0 ) k 2 − 1 t 3 k I ( t ) k (cid:16) k∇ f ( X ( t ) ) k + 3 2 k∇ f ( x 0 ) k (cid:17) − 3 t 7 k I ( t ) k 2 ≥ t 4 k∇ f ( x 0 ) k 2 − t 4 k∇ f ( x 0 ) kk∇ f ( X ( t ) ) − ∇ f ( x 0 ) k − 3 t 16 k∇ f ( x 0 ) k 2 − LM ( t ) t 3 12 (cid:16) k∇ f ( X ( t ) ) − ∇ f ( x 0 ) k + 5 2 k∇ f ( x 0 ) k (cid:17) − L 2 M ( t ) 2 t 5 48 ≥ t 16 k∇ f ( x 0 ) k 2 − LM ( t ) t 3 k∇ f ( x 0 ) k 8 − LM ( t ) t 3 12 (cid:16) LM ( t ) t 2 2 + 5 2 k∇ f ( x 0 ) k (cid:17) − L 2 M ( t ) 2 t 5 48 = t 16 k∇ f ( x 0 ) k 2 − LM ( t ) t 3 3 k∇ f ( x 0 ) k − L 2 M ( t ) 2 t 5 16 . To complete the proof , applying Lemma 5 . 2 , the last inequality yields h ˙ X ( t ) , ¨ X ( t ) i ≥ (cid:16) 1 16 − Lt 2 12 ( 1 − Lt 2 / 12 ) − L 2 t 4 256 ( 1 − Lt 2 / 12 ) 2 (cid:17) k∇ f ( x 0 ) k 2 t ≥ 0 for t < min { p 12 / L , 4 / ( 5 √ L ) } = 4 / ( 5 √ L ) , where the positivity follows from 1 16 − Lt 2 12 ( 1 − Lt 2 / 12 ) − L 2 t 4 256 ( 1 − Lt 2 / 12 ) 2 > 0 , which is valid for 0 < t ≤ 4 / ( 5 √ L ) . References A . Beck and M . Teboulle . A fast iterative shrinkage - thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2 ( 1 ) : 183 – 202 , 2009 . S . Becker , J . Bobin , and E . J . Cand ` es . NESTA : a fast and accurate ﬁrst - order method for sparse recovery . SIAM Journal on Imaging Sciences , 4 ( 1 ) : 1 – 39 , 2011 . A . Bloch . Hamiltonian and gradient ﬂows , algorithms , and control , volume 3 . American Mathematical Soc . , 1994 . H . - B . D¨urr and C . Ebenbauer . On a class of smooth optimization algorithms with applications in control . In Nonlinear Model Predictive Control , volume 4 , pages 291 – 298 , 2012 . H . - B . D¨urr , E . Saka , and C . Ebenbauer . A smooth vector ﬁeld for quadratic programming . In 51st IEEE Conference on Decision and Control , pages 2515 – 2520 , 2012 . S . Fiori . Quasi - geodesic neural learning algorithms over the orthogonal group : A tutorial . Journal of Machine Learning Research , 6 : 743 – 781 , 2005 . U . Helmke and J . Moore . Optimization and dynamical systems . Proceedings of the IEEE , 84 ( 6 ) : 907 , 1996 . D . Hinton . Sturm’s 1836 oscillation results evolution of the theory . In Sturm - Liouville theory , pages 1 – 27 . Birkh¨auser , Basel , 2005 . J . J . Leader . Numerical Analysis and Scientiﬁc Computation . Pearson Addison Wesley , 2004 . 36 L . Lessard , B . Recht , and A . Packard . Analysis and design of optimization algorithms via integral quadratic constraints . arXiv preprint arXiv : 1408 . 3595 , 2014 . R . Monteiro , C . Ortiz , and B . Svaiter . An adaptive accelerated ﬁrst - order method for convex optimization . Technical report , ISyE , Gatech , 2012 . Y . Nesterov . A method of solving a convex programming problem with convergence rate O ( 1 / k 2 ) . In Soviet Mathematics Doklady , volume 27 , pages 372 – 376 , 1983 . Y . Nesterov . Introductory lectures on convex optimization : A basic course , volume 87 of Applied Optimization . Kluwer Academic Publishers , Boston , MA , 2004 . Y . Nesterov . Smooth minimization of non - smooth functions . Mathematical programming , 103 ( 1 ) : 127 – 152 , 2005 . Y . Nesterov . Gradient methods for minimizing composite objective function . CORE Discussion Papers , 2007 . B . O’Donoghue and E . J . Cand ` es . Adaptive restart for accelerated gradient schemes . Found . Comput . Math . , 2013 . S . Osher , F . Ruan , J . Xiong , Y . Yao , and W . Yin . Sparse recovery via diﬀerential inclusions . arXiv preprint arXiv : 1406 . 7728 , 2014 . Z . Qin and D . Goldfarb . Structured sparsity via alternating direction methods . Journal of Machine Learning Research , 13 ( 1 ) : 1435 – 1468 , 2012 . R . T . Rockafellar . Convex analysis . Princeton Landmarks in Mathematics . Princeton University Press , 1997 . Reprint of the 1970 original . J . Schropp and I . Singer . A dynamical systems approach to constrained minimization . Numerical functional analysis and optimization , 21 ( 3 - 4 ) : 537 – 551 , 2000 . I . Sutskever , J . Martens , G . Dahl , and G . Hinton . On the importance of initialization and momentum in deep learning . In Proceedings of the 30th International Conference on Machine Learning , pages 1139 – 1147 , 2013 . P . Tseng . On accelerated proximal gradient methods for convex - concave optimization . submitted to SIAM Journal on Optimization . 2008 . P . Tseng . Approximation accuracy , gradient methods , and error bound for structured convex optimization . Mathematical Programming , 125 ( 2 ) : 263 – 295 , 2010 . G . N . Watson . A treatise on the theory of Bessel functions . Cambridge Mathematical Library . Cambridge University Press , 1995 . Reprint of the second ( 1944 ) edition . 37