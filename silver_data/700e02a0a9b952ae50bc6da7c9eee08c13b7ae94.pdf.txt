8 Autonomous Authoring Tools for Hypertext MARK TRURAN University of Teesside and JAMES GOULDING and HELEN ASHMAN University of Nottingham Autonomous authoring tools are routinely used to expedite the translation of large document collections into functioning hypertexts . They are also used to add hyperlinks to pre - existing hypertext structures . In this survey we describe a taxonomy of autonomous hypertext authoring tools . The classiﬁcation of any given system is determined by the type and nature of the document analysis it performs . Categories and Subject Descriptors : A . 1 [ Introductory and Survey ] ; H . 3 . 1 [ Information Storage and Retrieval ] : Content analysis and Indexing ; H . 3 . 3 [ Information Storage and Retrieval ] : Information Search and Retrieval ; I . 2 . 1 [ Artiﬁcial intelligence ] : Applications and Expert Systems ; I . 2 . 7 [ Artiﬁcial intelligence ] : Natural language Processing General Terms : Theory , performance , reliability AdditionalKeyWordsandPhrases : Authoringtools , automatichypertextgenerationsystems , linkgeneration ACM Reference Format : Truran , M . , Goulding , J . , and Ashman , H . 2007 . Autonomous authoring tools for hypertext . ACM Comput . Surv . 39 , 3 , Article 8 ( August 2007 ) , 30 pages DOI = 10 . 1145 / 1267070 . 1267072 http : / / doi . acm . org / 10 . 1145 / 1267070 . 1267072 1 . INTRODUCTION The automatic , as opposed to manual , creation of hypertexts has many documented advantages [ Agosti and Allan 1997 ] . One key beneﬁt is the soundness of the hyperlinks which an automatic system can produce . Whereas hyperlinks created by human oper - ators lack much in fundamental rigour and robustness , link creation when executed by some software process is typically exact and predictable , providing that the algorithm informing the link creation process is demonstrably sound [ Ellis et al . 1996 ; Ingham et al . 1996 ] . The relevant literature tends to agree that this quality of predictability is This work was partly funded by the EPSRC—grant number 20164 . Authors’ addresses : M . Truran , School of Computing , University of Teesside , Middlesbrough , Tees Valley , TS1 3BA , UK ; email : m . a . truran @ tees . ac . uk ; J . Goulding and H . Ashman , Web Technology Research Group , Jubilee Campus , University of Nottingham , Wollaton Road , Nottingham , NG8 1BB , UK . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or direct commercial advantage and that copies show this notice on the ﬁrst page or initial screen of a display along with the full citation . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , to republish , to post on servers , to redistribute to lists , or to use any component of this work in other works requires prior speciﬁc permission and / or a fee . Permissions may be requested from Publications Dept . , ACM , Inc . , 2 Penn Plaza , Suite 701 , New York , NY 10121 - 0701 , USA , fax + 1 ( 212 ) 869 - 0481 , or permissions @ acm . org . c (cid:2) 2007 ACM 0360 - 0300 / 2007 / 08 - ART8 $ 5 . 00 . DOI 10 . 1145 / 1267070 . 1267072 http : / / doi . acm . org / 10 . 1145 / 1267070 . 1267072 ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 2 M . Truran et al . Fig . 1 . A taxonomy of autonomous hypertext authoring tools . a positive factor in favour of the end user : “ . . . given the same set of initial conditions , any automatic link generation algorithm will always proceed in the same way . The sets of links created by a particular method will thus be produced in a consistent manner to which users might become accustomed as they browse . ” [ Ellis et al . 1999 ] The attraction of a predictable process governing the authoring process provides one solid justiﬁcation for the development of automatic hypertext generation systems . A second rationale is the magnitude of the authoring task at hand , and the limits and costs of human effort . At the time of writing there is an overabundance of unlinked electronic documents available to the public that could conceivably be authored into a hypertext , but there is a comparitive shortfall of the human time and effort required to do so . There are also huge catalogues of hypertext documents that may beneﬁt from additional links authored by some automatic process . Again , the literature tends to agree , the applications available for any high quality hypertext authoring process that required little human intervention would be manifold [ Agosti et al . 1996 ] . Such a tool could be used by governments and organizations alike to publish this nascent hypertext in an efﬁcient and cost effective manner , providing “the hypothetical end - user with access to a large depository of knowledge for reading , browsing and retrieving” [ Agosti and Smeaton 1996 ] . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 3 1 . 1 . Structure of this Article This article surveys three distinct methodologies employed in the automatic authoring of hypertext links . Systems are broadly categorized according to the loci of their analyt - ical focus , and are discussed in turn . In § 2 we examine those systems that exploit the internal structure of a the document to author links ( structural systems ) . In § 3 we move on to those systems that address the frequency of words inside the document as a guide to link authoring ( statistical systems ) . In § 4 we address those systems that attempt to glean some meaning from the text itself ( semantic systems ) . Following this discussion , in § 5 we examine the theory and development of several hybrid systems , which combine various aspects from each of the methodologies listed . In § 6 we examine some general factors affecting the autonomous authoring of hypertext , which apply to any authoring tool irrespective of its high level categorization . Finally , in § 7 we outline likely future developments in the ﬁeld , including a possible trend towards community based solutions for automatic authoring . 1 . 2 . Scope of this Article This survey does not discuss the techniques employed when translating or decom - posing large text ﬁles into workable sections or segments ( would - be hypertext nodes ) . Although an interesting topic in its own right , it is better documented elsewhere [ Riner 1991 ; Franke and Wahl 1995 ] . Our primary focus is that of automatic hyperlink au - thoring . The systems described herein typically receive as their input a large collection of documents , which can be either “ﬂat” text ﬁles ( with or without markup ) or doc - uments with some preliminary linking structures already in place . The system then performs some manner of document analysis , and hopefully transmutes this input into some corresponding hypertext of the same material , authoring links between docu - ments or parts of documents according to some underlying associative rationale . Links created by an autonomous authoring tool system are therefore precomputed links ac - cording to the nomenclature suggested by Ashman et al . [ 1997 ] . This survey does not address dynamically computed links , that is , those links that remain uninstan - tiated until some later system event ( e . g . traversal by a user ) . Interested parties are directed to Davis et al . [ 1992 ] ; Thistlewaite [ 1997 ] ; and Ashman [ 2000 ] for further discussion . 2 . STRUCTURAL AUTHORING TOOLS Structural authoring tools utilize the internal logical structure of documents to gen - erate links . Structural information can be loosely deﬁned as any part of a text ﬁle indicating the array of its logical components , for example the disposition and bound - aries of sections or chapters . Typically , structural authoring systems are designed to link one recognizable structural element ( A ) to another ( B ) , although B is occasionally derived by parsing the text contained within A . The link A → B can obviously be of either interdocument or intradocument type . Raymond and Tompa [ 1987 ] used a structural approach when implementing a “hypertext - like front - end” for an electronic version of the Oxford English Dictionary [ Murray 1928 ] . Combing the text ﬁles for appropriate link source anchors , the authors selected the notation indicating a cross - reference as “a substantial source of hypertext links . ” However , they were unable to automatically determine a suitable link target by parsing the actual text of the cross - reference . This was , they concluded rather gloomily , a problem requiring “complex semantic analysis for resolution . ” Glushko [ 1989 ] documented a similar large - document translation in the paper De - sign Issues for Multi - Document Hypertexts . The task at hand was the integration of ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 4 M . Truran et al . a very large engineering encyclopedia ( the Compendium ) , with a 388 page military speciﬁcation for human engineering design ( the Standard ) . Glushko exploited the explicit structure of the documents to create a synthesized table of contents , hy - brid indices , and other “composite access points . ” A rationale for the creation of links was sketched which favoured intradocument links : links that join one part of a document to another part of the same document ( for example , footnotes , notations etc . ) . “These links are the easiest to identify when converting existing texts to hypertext and they are probably the most usable and useful as well . Since good writers use these structural and rhetorical devices in predictable ways , it follows that a reader of a hypertext will ﬁnd them predictable as well . ” [ Glushko 1989 ] Glushko considered the prospect for identifying and creating links between two differ - ent documents , in this case the Compendium and the Standard , to be a different matter altogether . He claimed the difﬁculties that arose when contemplating the creation of implicit interdocument links were quite fundamental , and that the task of identifying these links should be left to the hypertext reader . The problem , he declared , was the subjective nature of that association [ Ellis et al . 1994 ] . One might construct a clever process for identifying interdocument links automatically . The links produced by this process might completely satisfy a well - reasoned set of assessment criteria ( relevance , conciseness , coverage etc ) . However , a subsequent user of the hypertext , with different ideas about how topics “ﬁt” together , might still ﬁnd the links irrelevant : “Like a used textbook , some of the highlighting and margin notes may be useful to another student , but may be distracting or misleading at other times . ” [ Glushko 1989 ] Importantly , Glushko had recognized quite early on , that one person‘s idea of an ap - propriately linked text might not satisfy a subsequent user ( see further , § 6 . 2 ) . There - fore , he limited himself to safer territory , and emphasized the importance of links within documents . The construction of interdocument links is occasionally facilitated by some unusual aspect of the hypertext material . When Andersen et al . [ 1989 ] constructed a prototype for reading the Unix network news through a hypertext browser , they were able to utilize the unique Unix identiﬁers imprinted on each article . By creating hypertext links between articles and follow - up articles ( determined by scrutinising the ID number ) , a hypertext was created that reﬂected the ancestor - descendant relationships of the raw materials . Another example of media particularly well suited to automatic conversion into hy - pertext is that of system or software documentation . Complex or highly technical hard - ware / software systems are often accompanied by large instruction manuals . Often these manuals have a very deﬁnite internal structure designed to improve information re - trieval speeds . In addition , they typically feature a self - referential framework of cita - tions ( for more information about full text parsing , see § 67 . 2 ) . When considered together , these features describe a type of document ideally suited to automatic processing into some hypertext equivalent . Also profoundly self - referential , legal materials provide a rich medium for automatic translation , and have been the focus for several research projects . For example , Wilson [ 1988 ] assembled law cases and secondary materials for the GUIDE hypertext system [ Brown 1986 ] utilizing structural data . In later work , Agosti et al . [ 1991 ] memorably characterized the connections that exist among legal documents in a collection as un - usually cohesive : “The handling of legal documents has a rather peculiar feature : the logical connections between them are very tight binding . For example , a law text can almost never be properly interpreted without making reference to other normative law documents . . . It is therefore practically impos - sible to satisfy one’s information requirements by consulting a single document . ” ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 5 Franke and Wahl [ 1995 ] made similar observations when developing a hypertext version of the UNIX man pages ( see also Cawley [ 1993 ] ) . Stored as nroff text ﬁles , man pages provide a user of the UNIX operating environment with explanation and usage instructions for commands understood by the system . Converting these ﬁles into a hypertext entailed translating each ﬁle into a number of separate nodes , each identiﬁed by the structural clues contained within the text ( section , sub - section etc . ) These nodes were then processed for links using a simple cross - referential rule . If node A cites a help topic contained in node B , then a new link A → B was created . The authors observed that the quality of the links created using this process scored a “high degree of satisfaction” with test users , but admit that some manual intervention and / or editing was required in a supervisory capacity . “To create hypertext that is easy to read requires some human expertize . A reasonable ap - proximation may be generated automatically but an ideal solution requires associative thinking and subjective decisions . In fact , an optimal solution is impossible to deﬁne because the decisions made by the editor are subject to his or her interpretation . ” [ Franke and Wahl 1995 ] In this respect the authors are in accordance with Rearick [ 1991 ] , who suggested that systems capable of automatically generating hypertext should work in a suggestive capacity . That is to say , they should routinely defer to a human editor charged with ﬁnal approval . Further examples of media particularly amenable to automatic conversion to some hypertext equivalent because of their explicit / regular internal structure can routinely be found in the literature : Frisse converted the prestigious Manual of Modern Therapeu - tics [ Orland and Saltman 1986 ] into a card metaphor searchable hypertext [ Frisse 1988 ; Frisse and Cousins 1989 ] ; Nunn et al . [ 1988 ] used automatic transformation techniques to create anonline manual for REXX ( restructured extended executor ) , a scripting lan - guage for IBM VM and MVS systems ; and the WHURLE system utilized structural information to provide auto - navigation for its learners [ Brailsford et al . 2002 ] . Providing a valuable synthesis , Furuta et al . [ 1989b , 1989a ] advanced a distinct and describable class of “medium grained information sources” particularly suited to auto - matic transformation . A membership prerequisite of this class is a structured document representation [ Furuta 1989 ] : “The structured document representation identiﬁes the logical components of the document , separating out the speciﬁcation of the physical placement of the component of the page . The representation is object - based : higher level objects are formed by composing lower level objects . ” [ Furuta et al . 1989a ] Members of this class include text documents deﬁned using some generic mark - up languages ( e . g . hypertext documents authored using ( x ) HTML ) , database representa - tions of information ( which necessarily identify separate logical components ) , and other documents exhibiting a tightly constrained syntax ( e . g . computer programs , card cata - logues ) [ Drakos 1994 ] . Interestingly , it would appear that paper - based documents can belong to this class providing they contain adequate and detectable representational structures to inform later transformations [ Mamrak et al . 1987 , 1989 ] . Bibliographic references provide a rich seam of explicit links that can be explored during the authoring process . A simple authoring tool that exploits such references would create a link between documents A and B if the former cited the latter . 1 A more sophisticated technique is the practice of “bibliographic coupling” [ Kessler 1963 ] . Savoy 1 Although , as Savoy pointed out , unsophisticated citation based schemes like this example are not without their problems : “Sometimes the writing of bibliographic references does not support citation indexing goals be - cause of excessive self - citing , the generation of long lists , citations’ ampliﬁcation of details , bib - liographic mutual back scratching etc . ” [ Savoy 1996 ] ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 6 M . Truran et al . [ 1996 ] states the acme of this theory quite succinctly : “if two papers have a similar bibliography , they must have a similar content , and thus deal with similar subjects . ” Accordingly , two documents with a similar bibliography would make good candidates for linking together . One further permutation of this theory concerns the incidence of cocitation of two given articles , stating that the more frequently those articles are cocited by other articles in the collection , the more similar in content they are likely to be [ Small 1973 ] . A useful illustration of the utility of bibliographic references can be found in NEC’s Citeseer system , on which see § 5 for further discussion . One slightly unusual structural authoring tool should be mentioned at this junc - ture . Nentwich et al . [ 2002 ] developed a rule based link generation application service known as xlinkit . Given a set of resources and a set of rules , both described in XML ( Extensible Mark - up Language ) [ Bray et al . 2004 ] , xlinkit veriﬁed the consistency of the information contained in the former according to the constraints expressed in the latter . A constraint identiﬁes some relationship that should exist in the data . So , for example , where a hypothetical set of University resources contains one list of modules offered to undergraduate students , and one list of modules offered to postgraduate stu - dents , we might wish to enforce a constraint that asserts the module description should be identical in both lists . The xlinkit service used a “set - based rule language” to “express consistency con - straints between distributed documents . ” XPath [ DeRose et al . 2002 ] expressions were used to select and compare sets of elements from comparable resources . Link genera - tion was dependent upon the results of these comparisons . For each comparison , rather than a Boolean true | false statement , xlinkit returned a link : “We use hyper - links called consistency links to connect consistent or inconsistent elements . If a number of elements form a consistent relationship , they will be connected via a consistent link . If they form an undesirable relationship , with respect to some rule , they are connected via an inconsistent link . ” [ DeRose et al . 2002 ] Consistency links were stored in an XLink [ DeRose et al . 2001 ; DeRose 1999 ] link - base and provided “diagnostic information” for the individuals responsible for maintain - ing the resources . Subsequent actions could include resolving any detected inconsisten - cies by editing the resources that are “pointed at” by inconsistent links , or allowing the inconsistency to persist . The consistency links could even be “folded - in” to the resources using a link - based processor known as XtooX , thereby producing “a web of inconsistency information between ﬁles . ” The xlinkit consistency checker clearly differs from the applications we have previ - ously discussed . The consistency links it manufactures fulﬁll a different role from the interdocument links produced by Raymond , Glushko , Frisse and others . These authors designed systems that produced navigational links for some equivalent of a hypertext end - user , hoping to make a large corpus of information more accessible . Nentwich et al . [ 2002 ] , on the other hand , produced links of interest chieﬂy to the actual owners / editors of the hypertext resources , who are perforce interested in any application that shortens routine tasks designed to enforce consistency . However , this difference in link end - use does not affect its system classiﬁcation . 3 . STATISTICAL AUTHORING TOOLS In the previous section we described how certain structural features of documents in a collection could be identiﬁed and used as the source or destination points of a hypertext link . In this section we discuss the statistical approach to automatic hypertext authoring . This method does not involve analysis of the syntactic constructs present in a given document , but instead revolves around mathematical functions relating to the occurrence of each word ( or term ) in the documents concerned . These functions ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 7 Fig . 2 . Comparison of an information retrieval system and a statistical authoring tool . are resolved into some determination of what a particular document is concerned with ( sometimes described as aboutness ) , and this determination is subsequently used to inform the construction of hypertext links . 3 . 1 . Relationship to Information Retrieval The statistical approach described here represents a convergence of the ﬁelds of infor - mation retrieval and hyperlink authoring . Agosti et al . [ 1997 ] have commented that the development of this approach was quite logical , given the type of operations required for autonomous authoring : “Since the most difﬁcult part of the automatic construction of a hypertext is the building up of links that connect semantically related documents or document fragments , it is natural to concentrate on IR techniques , that have always dealt with the construction of relationships dependent on the mutual relevance of objects . . . ” Automatic hypertext generation systems that use statistical methods borrow many of the techniques developed for classic IR tasks , but differ slightly in terms of input and output speciﬁcations . An IR system accepts a query , and returns a list of related documents from a given collection . A statistical authoring tool typically accepts a doc - ument or a segment of a document and returns that same input with a number of links inserted . These subtle differences in requirements are illustrated in Figure 2 : 3 . 2 . Underlying Theory Having explained the differences and similarities between an IR system and a statisti - cal authoring tool , it is now time to turn to the underlying theory . Luhn [ 1958 ] provided the theoretical underpinnings of information retrieval . He famously observed : “ . . . that the frequency of word occurrence in an article furnishes a useful measure of word signiﬁcance . It is further proposed that the relative position within a sentence of words having given values of signiﬁcance furnish a useful measurement for determining the signiﬁcance of sentences . ” Luhn used this assertion to develop a technique for the automatic creation of ab - stracts . In his scheme , the term frequency t f for every term in document d was calcu - lated . Analyzing the distribution of t f in d , an upper and lower limit was established to exclude those terms that occurred too frequently , or very infrequently , in the docu - ment . These terms were categorized as nonresolving function words ; that is to say , they were deemed unlikely candidates for inclusion in an abstract because they could not contribute signiﬁcantly to the article [ van Rijsbergen 1980 ] . The remaining terms were weighted . Luhn proposed that a term was more likely to be signiﬁcant if its frequency count occurred roughly equidistant from the upper and ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 8 M . Truran et al . lower cutoffs . According , the t f values for these “ideal” terms were increased at this point . Finally , each sentence was scored for signiﬁcance ( so that the score of sentence s of length l in terms was equal to t f 1 + t f 2 + t f 3 . . . t f l ) . By selecting the sentences with the highest total score , Luhn was able to effectively abstract the contents of a document with no prior knowledge of them . Adjusting term frequency data with a weighting scheme is quite common in infor - mation retrieval . One of the simpler weighting schemes normalizes against document length [ Salton and Buckley 1988 ] . A set of documents of unequal length will produce term frequency distributions that do not fairly represent their respective contents . Therefore , the term frequency t f is usually divided by document length l , so that the weighted score for term x in document d is t f x / l d . The precise manner in which the length of each document is actually represented , whether it be in characters , bytes , or some other measurable unit , is irrelevant , provided the scheme is applied uniformly across the collection . Sometimes , a weighting scheme is directly related to the logical structure of the documents being evaluated . For example , Anderson et al . [ 1989 ] designed a similar - ity function that weighted term frequency data using node - speciﬁc criteria . Each term occurrence was statistically important , but words occurring in certain structurally sig - niﬁcant areas of the document ( subject ﬁeld , keyword ﬁeld , header etc . ) were considered more likely to represent document content . Accordingly , those terms were given signif - icantly higher weightings than terms occurring in other parts of the document . Sometimes the weighting scheme implemented by a particular information retrieval system is related to the underlying hypertext model . Frisse [ 1988 ] hypothesized that in the context of a rigidly hierarchical card - metaphor hypertext system , an informative weighting of a card c was composed using two factors : —The intrinsic value of the terms in c itself . —The extrinsic value of the cards immediately descendant to c . This extended weighting scheme was designed to target : “ . . . parent cards that might not contain any of the query terms , but which might be ideal starting points because of the high fraction of query terms present within their immediate de - scendants . ” [ Frisse 1988 ] Obviously this extended weighting scheme raises signiﬁcant questions regarding processing costs and response time . To calculate the extrinsic weight of a card c , d further weightings must be ﬁgured , where d is the number of descendant nodes of c . Given a hypertext of sufﬁcient depth , a formerly inexpensive and simple weighting operation rapidly becomes quite dear . Fuller et al . [ 1993 ] examined these questions when constructing hyperbases of large structured document collections , concluding that the solution could lie in appropriate preprocessing . Each node could contain , alongside its primary media , metainformation such as its node level in the structure and a digest of its descendants . This metain - formation could then form part of the node - weighting scheme , removing the need for expensive leaf - bound traversals . Although not extensively tested with real users , this scheme was reported as producing ‘sensible’ results in experimental trials [ Fuller et al . 1993 ] . A more common , general - purpose weighting scheme is known as inverse document frequency [ Jones 1972 ; Salton et al . 1975 ; Aizawa 2003 ] . IDF measures the number of occurrences of a word across a document collection . It provides a simple method of diminishing the importance of words that are statistically common , and therefore less useful when attempting the task of document disambiguation . Another way of stating this is that IDF is related to a term’s speciﬁcity . Where D is the number of documents in ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 9 the collection , and d is the number of documents that contain a term t , the IDF factor of t in the collection is log ( D / d ) . The higher the IDF value of a term , the more unique that word is across a document collection . The term frequency of a particular term is often multiplied by its inverse document frequency to provide an appropriate composite weighting . This scheme , tf × idf , provides the backbone of many common information retrieval systems [ Salton 1991b ; Allan 1995 ; Sugiyama et al . 2003 ] . There are three important text operations that are usually carried out prior to the calculating of term frequency . The ﬁrst is the application of a stop list . This is a ﬁle of frequently used terms that lend nothing to the actual meaning of the document , such as propositions , conjunctions , pronouns , and so on . This ﬁle is applied as a ﬁlter , allowing the extraction of common words from a document before further analysis takes place . Stop lists typically contain between 40 to 300 features , and their application can signiﬁcantly reduce the size of a document prior to processing [ Salton and McGill 1986 ] . The second measure is known as stemming . Prior to the calculation of tf , each term is passed through an algorithm that reduces it to some agreed morphological minimum . A typical stemming operation would , for example , reduce the terms mountaineer and mountaineering to the same root . Subsequently , these conﬂated features can be counted as two instances of the same term . Stemming reduces document length and typically improves the quality of the results . There are several published algorithms that can be used [ Lovins 1968 ; Paice 1990 ; Melucci and Orio 2003 ] , and a corresponding list of techniques for evaluating their output [ Harman 1991 ; Paice 1994 , 1996 ; Kantrowitz et al . 2000 ] . The third text operation is known as term expansion . The purpose of expansion is to maximize the probability of successful retrieval by inﬂating the query document terms . One common and quick method of expanding a query term t is to search the document collection for t and all synonyms of t [ Harman 1988 ] . So , for example , if a query document contained the string car clubs , the expanded query document might read automobile car vehicle association club federation . Once these text operations are completed and the term frequencies of the query docu - ment have been calculated , we can compare that document with every other document in the collection . Rather than calculate tf values for every document in the collection each time a query document is received , most statistical authoring tools generate some kind of index . An index ( in information retrieval parlance ) shares many properties with the paper index we could ﬁnd in the back of a book . For example , it will feature value - location mapping . That is to say , it will provide a list of text strings ( terms ) , and the locations in which those terms can be found . However , rather than display a page number , as in the case of a book index , this location value is more likely to take the form of some internally - understood ﬁle location or pointer . The electronic index will also be inverted , which means that it will be internally sorted on the terms that it contains , rather than the locations it points to . This inversion is crucial . Just as inversion speeds the rate at which a human user can look up an entry in a paper index , so it enables a computer to quickly ﬁnd an entry in an electronic index . Searching through an inverted index , a computer can employ the binary chop algorithm to locate the required entry . This algorithm reduces the complexity of the search task from n ( in the case of a simple sequential search ) , to log ( n ) ( where n is the number of items to search ) . Building an index takes place in advance of submitting the ﬁrst query document to the system , and can be undertaken during periods of low computer demand . Having a prebuilt index signiﬁcantly reduces the response time of a statistical authoring tool system , and requires little more than the application of the various measures we have described . For a document collection c of size n we build an index in the following way : For 1 to n do : ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 10 M . Truran et al . ( 1 ) Apply the stop list to a single document d . ( 2 ) Stem all features . ( 3 ) Derive tf for each unique term . ( 4 ) Apply any applicable weighting scheme . ( 5 ) Where the tf for any term t exceeds a speciﬁed threshold add the pairing ( t , d ) to the index . Where t is already present in the index , conﬂate that entry into a one - to - many relationship ( t − d 1 , d 2 ) . Determining the threshold value for an index is often a trial and error process re - quiring hands - on adjustment . Sometimes an arbitrary threshold value is set and then “ﬁne - tuned” depending on the size and quality of the index produced . In other situa - tions , the threshold value may be determined with reference to the number of index entries , so that the threshold is gradually incremented until the generated index con - tains a subjectively ideal number of entries . Construction of an index may also involve a compression stage , to account for the uneven distribution of terms in the text ( see further Zobel et al . [ 1993 ] ; Zipf [ 1949 ] ) . In those situations where the document collection is already a hypertext and the au - thoring process is merely adding hyperlinks to an interlinked set of nodes , a statistical authoring tool can use the existing links as a valid factor in the creation of an index . As observed : “ . . if two nodes are connected by links , they must deal with similar topics or present related concepts . . . For example , if the representative of hypertext node D j contains the term T i , and this node is linked to node D k , then there is a probability that the node D k should also be indexed by the term T i . ” [ Savoy 1996 ] This approach , of course , mirrors those IR techniques that exploit existing link structures to determine the relevance of a document to a query [ Brin and Page 1998 ; Kleinberg 1998 ] . Once indexing is complete , a simple procedure can be employed to match the query document with one or more documents from the collection . Signiﬁcant terms featured in the query document can be looked up in the index , and candidate documents featuring these terms located . Hypertext links can then created , so that the occurrence of term t in the query document is associated with term t in the candidate document ( with the proviso that some mechanism exists for controlling the number of links created per node ) . These procedures represent a particularly simple scheme for statistical authoring . There are , of course , more sophisticated methods available that can be utilized to com - pare a collection of candidate documents to a given query document . One good example of a more sophisticated matching function can be found in the vector space model , backbone of Salton’s [ 1991a ] SMART retrieval system ( see also Raghavan and Wong [ 1986 ] , Lee et al . [ 1997 ] ) . In this scheme , the submitted query and a candidate docu - ment are represented by an array of term - weight pairs . These arrays are converted to n - dimensional vectors , where n is the number of unique terms present in the collec - tion . Final query - candidate comparison is achieved using the cosine measure ( that is , the angle between the query vector and the document vector ) [ Salton et al . 1996 ] . The cosine measure is deﬁned as sim ( Q j , D i ) = t (cid:2) k = 1 ω jk × ω ik , ( 1 ) where t expresses all unique terms in the collection , ω jk is the weight of term k in the query , and ω ik represents the weight of k in the candidate document . Both terms’ ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 11 weights are derived using a tf × idf scheme normalized against document length . This measure produces a numerical value ( usually between 0 and 1 ) , which represents the similarity of the query and the candidate document . 3 . 3 . Early Statistical Systems We now move on to speciﬁc examples of statistical authoring tools , as opposed to the generalized theory behind them . We begin with Bernstein [ 1992 ] and his hypertext ap - prentice . The apprentice was a “best - effort” construct . It used computationally “meagre” textual and statistical analysis to discover links in a text . Its understanding of this text was fundamentally shallow —it involved nothing more than the simple manipulation of hash tables to deﬁne a similarity measure between the page being viewed and every other page in the hypertext . This measure was then used to derive the twenty most sim - ilar candidate nodes and suggest them to the hypertext user as possible or ‘plausible’ targets of links . A discussion of the performance of the hypertext apprentice revealed that many of the links suggested by the system were “strikingly relevant , ” and that the apprentice was able to construct convincing hypertext “tours” given an initial subject . On the other hand , the apprentice was unable to discover structural links , and occasionally produced rather “bizarre” or unforeseeable associations . Bernstein [ 1992 ] concluded that the shortcomings of the system were compensated for by a negligible impact on system resources ( 32 msec / page on a 68000 - based Macintosh computer ) . Next we consider the Microcosm link service [ Fountain et al . 1991 ] . One of the more interesting features of this system was known as a retrieval - link : “Microcosm has a facility that allows a user to batch a set of text ﬁles and to index these documents . Once this indexing has been done , a block of text may be selected and the action , compute link , may be chosen . The system will very rapidly return a number of other documents within the system that have a similar vocabulary to the selected block , in the order of the best match . ” [ Hall et al . 1993 ] Following this procedure , a user of the Microcosm system could browse a document collection by repeatedly identifying topics for further investigation and then following the computed links . This would allow them to construct a potentially unique route through the hypertext material , a path personal to them and contoured by their inter - ests [ Bush 1945 ] . 3 . 4 . Reﬁnements of the Early Statistical Approach Some of the early statistical processes that were developed to match query documents to members of a collection were easily led astray . Salton et al . [ 1996 ] recognized this fact when they observed that global vector similarity functions could be misled by documents with overlapping vocabularies but distinct subjects ( see also Salton and Buckley [ 1989 ] ) . 2 As a remedial measure , they developed an approach grounded on the proposition advanced by Wittgenstein [ 1953 ] , that “text meanings are related . . when sufﬁciently large overlaps exist in the global vocabularies , and the overlapping terms occur in reasonably similar local environments” [ Salton and Buckley 1991 ] . This sug - gested the need for an “additional step” designed to capture context : “This is accomplished by insisting on certain locally matching substructures , such as text sentences and text paragraphs , in addition to the global vector match , before accepting two texts as legitimately similar . ” [ Salton et al . 1996 ] 2 As an example , they cited the biographies of John Fitzgerald Kennedy and Antony M . Kennedy , Supreme Court Justice , who shared a religion , an alma mater ( Harvard ) and an intensely public life . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 12 M . Truran et al . How does this text matching strategy work in practice ? The ﬁrst stage breaks the collection of documents into more manageable portions , what Salton and Buckley [ 1991 ] refer to as ‘individual text units’ of differing granularities ( e . g . sections , paragraphs and sentences ) . The following stage is a standard indexing operation of these text units , whose output for each text fragment is a representative vector of weighted terms . The identiﬁcation of similarities among text fragments now occurs , with the matching function operating on “several contexts of different scope” : “In practice , two text sections might then be related when the similarity between the vec - tors describing the text sections exceeds a stated threshold , and in addition the sections also contain at least one paragraph pair with sufﬁciently large paragraph similarity . Analogously , paragraphs may be related when the global paragraph similarity is sufﬁciently large , and ad - ditionally sentence pairs are present with sufﬁciently large sentence similarities . ” [ Salton and Buckley 1991 ] The authors describe an experiment using the the 29 - volume Funk and Wagnalls New Encyclopedia [ 1979 ] in which single entries were used as queries against the remaining 24 , 900 articles . The experiment used the atc similarity measure for larger text excerpts and the atn similarity measure for shorter textual units , such as sentences and para - graphs [ Salton and Buckley 1990 ] . Larger articles were manually broken down into smaller subdocuments for simpliﬁed processing , and then recombined after text com - parison . The relevance of retrieved articles to the query fragment was then scored using the presence or absence of manually authored cross - references previously inserted by the Encyclopedia editors . The results of the experiment validated the variable scope approach , despite a recorded ( and unusual ) lack of cross - references in the test material which obviously impacted upon the recall and precision scorings : 3 “The available evidence indicates that when searching an encyclopedia , the use of the combined global and local similarity computations improves retrieval effectiveness by about 10 % over the use of global vector similartiy measurements alone . ” [ Salton et al . 1996 ] In a similar vein , Hearst and Plaunt [ 1993 ] discussed the technique of TextTiling , an attempt to partition “full - length documents into multiparagraph units as an approxi - mation their underlying subtopic structure . ” The motivation for this experiment relates to the relationship between a document as a whole and subtopics occurring within that document : “One way to view an expository text is as a sequence of subtopics set against a ‘backdrop’ of one or two ‘main’ topics . A long text will be comprised of many different subtopics which may be related to one another and to the backdrop in many different ways . . . Therefore , instead of querying against the entire content of a document , a user should be able to issue a query about a query subpart , or subtopic . ” [ Hearst and Plaunt 1993 ] TextTiling was in fact an attempt to develop a different kind of query , enabling a user to search for a subtopic occurring as part of a main topic . However , its main interest ( at least in regard to this article ) lies in respect to the procedures used to identify topic boundaries . Previous work had examined creating text segments by chopping the documents into regular sized blocks [ Jacobs 1992 ] , but the TextTiling system explored a technique based on similarity scoring and graph analysis . The procedure worked as follows : ( 1 ) The candidate document is divided into blocks of k length , where k is the average paragraph length ( in sentences ) . ( 2 ) Block to adjacent block similarity is established using a minor variation of the standard tf × idf scheme , where idf is calculated according to the frequency of the 3 There were less than 80 , 000 cross - references in the 29 volume set . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 13 term under consideration within the parent document , rather than the collection as a whole . 4 ( 3 ) Relative differences between the block to adjacent block similarity scorings are then plotted as a graph and smoothed using the algorithm suggested by Rabiner and Schafer [ 1978 ] . ( 4 ) Segment boundaries are then ﬁxed so that they correspond with the lowermost portions of the graph plot . The intuition here is that high similarity scorings between adjacent blocks means that those blocks correspond to the same subtopic , whereas low similarity scorings between adjacent blocks indicate a change in topic . The authors describe an experiment using the TextTiling method in which each de - rived segment was indexed as a separate unit , and the scores for any parent document was derived by adding the top - scoring segments taken from that document . This experi - ment used the Ziff subset of the TIPSTER test collection , and Salton’s SMART retrieval system . The results were impressive : “We found . . . that by indexing full - text documents at the level of motivated segments , perform - ing retrieval against these segments , and summing the results of these subdocument retrievals , we could increase both recall and precision over conventional query / document retrieval consis - tently and substantially . Our experiments obtained improvements of from 18 . 9 % to 28 . 2 % ( for both precision and recall ) over normally indexed documents . ” [ Hearst and Plaunt 1993 ] A longer article concerning the same subject can be found in Hearst [ 1997 ] . Another author to experiment with document matching functions that addressed subglobal vectors was James Allan [ Allan 1995 ; 1996a ] . Citing several studies support - ing the view that the utility of a hyperlink is increased by some indication of its function [ Conklin 1987 ; Parunak 1989 ; Marshall and Shipman 1993 ] , Allan implemented a sta - tistical authoring tool ( using the vector space model ) along with with novel procedures for determining the nature of the relationship between two linked items . With visual - ization techniques also used in the ﬁeld of database analysis [ Davidson 1993 ] , Allan was able to successfully “label” each “automatic link” according to his own taxonomy of possible link types . 5 Despite this notable success , Allan freely admitted a category of relationships not susceptible to automated analysis / linking techniques—links that could not be located without human intervention . He classiﬁed these problem cases as “manual links” : ‘Manual links include those which connect documents which describe circumstances under which one document occurred , those which collect the various components of a debate or argu - ment , and those which describe forms of logical implication ( caused - by , purpose , warning , and so on ) . ’ [ Allan 1996a ] Allan concluded that the automatic identiﬁcation of these “manual” links was a task better suited to the natural language processing ( NLP ) and artiﬁcial intelligence re - search communities ( see further § 4 ) . The tight - knit relationship between the ﬁelds of information retrieval and statisti - cal authoring tools means that any signiﬁcant advances in the former tend to trickle down to the latter . This was evidenced by work completed by Blustein [ 2000 ] , the pur - pose of whose experiment was to compare the utility of , and user satisfaction with , sets of hypertext links authored using structural procedures with sets of links cre - ated using statistical methods . Creation of the structural links exploited the section 4 This modiﬁcation is related to the hunt for self - contained blocks : “if a term is discussed frequently but within a localized cluster ( thus indicating a cohesive passage ) , then it will be weighted more heavily than if it appears frequently but scattered evenly through the entire document” [ Hearst and Plaunt 1993 ] . 5 Automatic links types identiﬁed were revision , summary , expansion , equivalence , comparison , contrast , tangent and aggregate [ Allan 1995 ] . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 14 M . Truran et al . headings of the scientiﬁc articles using simple pattern matching rules , automati - cally generating tables of contents linking pages , sections and ﬁgures . By compar - ison , the authored statistical links were intended to “connect related but separate passages : ” ‘Two main kinds of . . . links were made : links from summary sections to summarized sections , and links creating related but distant passages . For example , a sentence in the abstract may have a link leading to the beginning of a passage which occurs later in the document and is summarized in the abstract . ’ [ Blustein 2000 ] Blustein authored hypertext versions of several scholarly articles using the SMART engine and a relatively recent innovation , Bellcore’s latent semantic indexing system [ Deerwester et al . 1990 ] . Latent semantic indexing is an attempt to address a funda - mental problem faced by information retrieval systems , namely that “users want to retrieve on the basis on conceptual content , and individual words provide unreliable evidence about the conceptual topic or meaning of a document” [ Deerwester et al . 1990 ] ( see also Furnas et al . [ 1988 ] ) . The logical repercussion of this statement is that a user query , couched in terms and not concepts , may not adequately represent the user’s in - formation need , leading to search results that are of a poor overall quality . Bellcore’s response to this observed problem was to tackle the “unreliability of observed term - document association data as a statistical problem : ” ‘We take a large matrix of term - document association data and construct a “semantic” space wherein terms and documents that are closely associated are placed near one another . Singular value decomposition allows the arrangement of the space to reﬂect the major associative patterns in the data . . . . Position in the space then serves as the new kind of semantic indexing , and retrieval proceeds by using the terms in a query to identify a point in the space , and documents in its neighborhood are returned to the user . ” [ Deerwester et al . 1990 ] The link authoring process ( for both the SMART and LSI inspired tools ) compared word groups and sentences to progressively smaller grained ‘blocks’ of the same article ( e . g . sections , subsections , paragraphs , sentences ) . Candidate selection proceeded in a manner quite similar to the earlier work of Salton on local document vectors [ Salton and Buckley 1991 ] , so that links were created to article blocks with a vocabulary match to the source of the link if and only if one subcomponent of that block also had a match . The authoring process was governed by several empirical rules designed to prevent the creation of too many links in one section of the article , and favoured the smallest possible matching article block . Subsequently , the statistically authored articles were compared to what the authors identiﬁed as “primitive hypertext , ” that is , hypertext authored using structural pro - cedures . This user - based evaluation led to a conclusion that favoured structural links over those created using IR - based techniques : “Readers followed more of the available deﬁnition links than available semantic or structural links . . . Structural and deﬁnition links may provide the necessary mental scaffolding to help readers make sense of articles in the absence of the familiar paper medium . ’ [ Deerwester et al . 1990 ] However , it must be said that while the evaluation that supported this conclusion was rigorous , it suffered from many of the same problems encountered by other documented attempts to evaluate the worth or usefulness of a hypertext in a controlled environment . This theme , genuinely a topic in its own right , is discussed further in § 6 . 3 . Other systems in the literature using statistical methods to automatically create links in a text are too numerous to discuss in detail . For example , Kellogg and Subhas [ 1996 ] used a cluster - based approach to create links for a dynamic digital library ( see also Can [ 1993 ] ) ; Lelu [ 1991 ] employed a spreading activation technique to construct “hyper - paths” in a document database represented ( at least conceptually ) as a city - scape ; Tebbutt [ 1999 ] produced a hypertext version of a very large service manual ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 15 known as POMS ( Program Operations Manual System ) for the United States Social Security Administration ( SSA ) ; and Macedo et al . [ 2002 ] developed “an infrastructure allowing the use of latent semantic analysis and open hypermedia concepts in the au - tomatic identiﬁcation of relationships among Web pages . ” It sufﬁces to say that almost any novel technique developed by researchers in the ﬁeld of information retrieval has an almost immediate potential application as a component of a statistical authoring tool . For this reason , any competent review of recent trends in IR doubles as an introduction to novel statistical authoring methods . 4 . SEMANTIC AUTHORING TOOLS In the previous section we reviewed those systems that examine the distribution of document terms to inform the creation of links . Implicit in that discussion were prob - lems related to the open texture of natural language . Terms have a number of different meanings , resolved quite naturally by human readers who ( unconsciously ) refer to the context in which those terms are used . A system that treats each occurrence of a term in exactly the same way , regardless of this underlying conceptual sense , inevitably makes mistakes . For example , consider the purely hypothetical news article : Congress Backs Missile The United States Navy today conﬁrmed that a test ﬁring of their A / U / RGM - 84 Harpoon missile has validated improvements to their radar guidance systems . A White House aide informed a member of the Associated Press that several ﬁscal initiatives had also kept the improvements within the Congressional budget . This news came as a relief to several high - ranking doves who nevertheless used the occasion to campaign for greater transparency on large military contracts . Assuming this article was one of a number of documents being analyzed by a primitive statistical authoring tool , the words highlighted in italics indicate terms that might be indexed as signiﬁcant . Any links generated by this system associating these terms with another occurrence of said term may or may not capture the particular word - sense as it used here . For example , an unsophisticated statistical authoring tool may link this document with another , citing dove - like leanings in the American cabinet ( which would be a correct association ) ; however , it may also link the document to an article describing the illegal hunting of Minke whales using harpoons ( which would be an incorrect association ) . It is these word sense problems that semantic authoring tools address . The basic semantic approach is to instill within the authoring system some understanding of each context ( or domain ) in which a particular term can be used . For example , the term “dove” is a member of at least three separate domains . It indicates an anti - war stance ( when used in a political context ) . It signiﬁes a winged animal of the family Columbidae ( when read within a context describing the natural world or fauna in general ) . It further connotes the concept of peace , ( if used as an abstract idea or symbol ) . Having identiﬁed the various domains to which a term may or may not belong , the next stage of the process is to deﬁne rules that enable each word sense to be dis - tinguished . One excellent case study of this process can be found in “ A News Story Categorisation Problem , ” published by Hayes et al . [ 1988 ] . The system described in this article used a modiﬁed version of the Carnegie Group’s Language Craft product [ Hayes et al . 1985 ] coupled with “knowledge - based” rules to enable the automatic clas - siﬁcation of news stories into “broad topic categories . ” The system attempted a topic categorization of unseen material in two distinct stages . Hypothesization described “an attempt to pick out all the categories into which the stories might fall on the basis of the words or phrases it ( sic . ) might contain . ” Conﬁrmation described “an attempt to ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 16 M . Truran et al . ﬁnd additional evidence in support of a hypothesized topic or to determine whether or not the language that led to the topics being hypothesized was used in a way that mis - led the system . ” Both stages are primarily pattern matching exercises—the difference between them is the ‘directness’ with which they are carried out : “Hypothesization always looks for the same words and phrases . Conﬁrmation looks for different words or phrases using speciﬁc knowledge - based rules associated with each of the topics that have been hypothesised . ” [ Hayes et al . 1988 ] Crucial to this two - stage process is the notion of a patternset , which is a descriptive term applied to a collection of patterns . A pattern describes one or more words , and that pattern is matched if these words occur in the news article being processed . If enough patterns from one patternset are matched then that patternset is hypothetically matched against an article . 6 One article may be matched against several patternsets at this stage . The next stage reduces these candidate patternsets to the most likely to apply . This is the stage that ﬁlters for false positives , that is , words that may initially satisfy partic - ular patterns but only because they are used in a “misleading” way . These “misleading” usages are identiﬁed using topic - speciﬁc rules . For example , an article describing a lengthy title prizeﬁght may satisfy a number of patterns in the war patternset , leading to that particular patternset to be hypothesized , but this hypothesis would ultimately be discounted during the conﬁrmation stage because —It mentions no countries , individuals , or organizations associated with wars . —It mentions no wars by name ( e . g . Hundred Years War ) . One problem discussed by the authors and other researchers is the burden of “tedious” effort required to “train” the system ( see also Callan and Croft [ 1993 ] ) . Time taken to develop the rulebase ( patternset , hypothesis , and conﬁrmation rules ) was estimated at six person - months . Subsequent to the creation of the rulebase there was also the continuing problem of updating the rules to reﬂect changes in a particular domain . For example , a patternset governing war would have to be updated in the event of a serious conﬂagration , and new patterns written describing the combatants , the name of the conﬂict , casus belli , principal individuals , and so on . Hayes and Pepper [ 1989 ] used these techniques described to assist in the creation of a system known as IMAD . IMAD , or “integrated maintenance advisor , ” was described as a “conceptually indexed hypertext representation of written documentation along with tightly integrated diagnostic subsystems . ” IMAD was intended for use alongside highly complex systems that provide voluminous sets of documentation ( relating to use , servicing , and trouble - shooting ) for the engineers tasked with their servicing and upkeep . An example of such a complex system might be a commercial airliner , or a military grade weapons platform . IMAD featured an “expert librarian” function . The “librarian” automatically con - structed hypertext links within a documentation set , using the procedures discussed here ( in relation to news articles ) , thereby improving user navigation . This authoring function was augmented by diagnostic / repair subsystems that assisted engineers with speciﬁc faults to determine the appropriate remedial action ( see further Kahn et al . [ 1987 ] ) . Prior to the creation of an IMAD rulebase , the developer annotated all relevant doc - umentation . When the system was used to assist in the documentation of the HAWK 6 This is somewhat of a simpliﬁcation . There is in fact a pattern weighting that inﬂuences the hypothesization phase of the categorization process . Matched patterns can carry both a “possible” and a “probable” weight . These weights contribute to a cumulative patternset weighting . If this cumulative weighting satisﬁes some threshold set arbitrarily by the rule developer , then the patternset is hypothetically matched . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 17 missile system [ Keller and Knutilla 1989 ] , a “standard dictionary of terms” describing all major and minor components , systems and subsystems , was created . “Each component in the component hierarchy is labeled with the words and phrases that can be used to describe it both speciﬁcally ( spar 7 ( in the left wing ) , an ft76 - 903 fastener ) and generically ( the spar , the bolt ) . The generic descriptions may be supplemented by information about the type of component ( fastener , electrical , electronic , hydraulic , subsystem , etc ) . ” [ Keller and Knutilla 1989 ] Using these annotations as reference points , a rulebase was created . This enabled “appropriate references to be found and index entries to be made on a conceptual basis , rather than based on the occurrence of speciﬁc key words or phrases . ” As with the news classiﬁcation project , signiﬁcant human effort was expended upon this annotation of the hypertext material and the creation of applicable patternsets . Lenat et al . [ 1990 ] later asserted two major problems with expert systems like IMAD . The ﬁrst is the “knowledge - acquisition bottleneck . ” This describes the lengthy and it - erative process of collecting expert opinion and installing it into some given system , a process usually measured in months and years . The second problem is that of domain dependence , whereby the expert system is restricted to one carefully delineated area of competence [ Smeaton 1992 ] : “Carefully selecting just the fragments of relevant knowledge leads to adequate but brittle performance : when confronted by some anticipated situation the program is likely to reach the wrong conclusion—a skin disease diagnosis system is told about a rusty old car , and concludes it has measles . ” [ Lenat et al . 1990 ] As an antidote Lenat [ 1995 ] and others labored on the creation of a massive knowl - edge base that embodied common sense . The result , after one century of person - effort , was a system comprised of around 10 5 general concepts / terms and 10 6 commonsense axioms . These concepts and axioms provide a general body of knowledge that could background or augment expert system level rules , a “common sense substrate” that describes the world as we unconsciously accept it . A typical CYC assertion might be this— one object cannot rotate clockwise and anticlockwise at the same time : “Such assertions are unlikely to be published in textbooks , dictionaries , magazines , or ency - clopedias , even those designed for children . . . These assertions are so fundamental that stating them to another person , aloud or in print , would likely be confusing or insulting . ” [ Lenat 1995 ] Multiples uses for the CYC system have been proposed [ Lenat 1998 ] . It could supple - ment the basic grammar / spelling correction features of a word processor ; assist in the construction of richer user models for adaptive hypertext systems ; automatically check for logical inconsistencies in the content of databases ; and much , much more . Of course , one further and quite interesting possible application for CYC would be its involvement in the automatic creation of hypertexts , possibly alongside the descendants of some of the expert systems already discussed . As with the development of statistical authoring tools , the evolution of semantic systems shadows its parent research ﬁeld . For this reason , the literature discloses an abundance of authoring tools implementing novel or recombined NLP techniques . For example , Green [ 1999 ] used lexical chains to associate sections of text concerning sim - ilar subjects ( see also Morris and Hirst [ 1991 ] ) . Basili et al . [ 2003 ] used “knowledge based Information Extraction techniques” to identify typed links ; Pilzecker [ 2002 ] de - scribed his experiences with “semantically driven automatic hyperlinking” ; and Valle et al . [ 2003 ] developed EIPs ( Enterprise Information Portals ) that ‘understood’ meta - data descriptions of corporate resources , improving user navigation with automat - ically created links . However , developing a successful semantic authoring tool has proved problematic . One author has commented that the modest successes published in relation to these systems may stem from the ancestry of the techniques being applied : ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 18 M . Truran et al . “ . . . the NLP techniques currently developed from NLP applications have evolved over decades for applications like MT and text understanding . It may be that IR is an intrinsically different application because of the imprecision involved , whereas MT is exact , and these require different approaches to NLP . ” [ Agosti and Smeaton 1996 ] 5 . HYBRID AUTHORING TOOLS While the majority of published authoring tools rely exclusively on just one of the methodologies discussed above , some researchers have implemented what is best de - scribed ( within our classiﬁcation systems ) as a hybrid model . A good discussion of such a combinatorial approach can be found in Rada [ 1992a ] . In this paper the author discusses the automatic transformation of a 250 page textbook ( Hypertext : from Text to Expertext [ Rada 1992b ] ) into formats suitable for four hypertext systems ( namely Emacs - Info , Guide , HyperTies , and SuperBook [ Remde et al . 1987 ; Gosling 1981 ; Brown and Russell 1988 ; Shneiderman 1987 ] ) . Discussing key aspects of the conversion process , the au - thor deﬁnes two descriptive classes of hypertext output . A ﬁrst order hypertext is one in which the markup of the original documents translates directly into the associative structure of an equivalent hypertext . “ . . . every link in the ﬁrst - order hypertext is one which was explicitly present in the markup of the text . For many documents , the most signiﬁcant ﬁrst - order links are those connecting outline headings . . . bibliographic citations , indices , footnotes , and cross - references . ” 7 [ Rada 1992a ] This type of output can be contrasted with second - order hypertext , which is com - posed of links not explicitly present in the original text but discernible through some automatic procedure . 8 Citing Mili and Rada [ 1985 ] , Rada demonstrated this secondary type of hypertext with an implementation of a link generation system based upon the distribution of terms across a collection of text blocks . Interestingly , the ﬁrst and sec - ond order hypertexts were then combined to complete Rada’s translation process . A subsequent trial revealed that users favored the structural links , but discussion of the results was cursory and did not broach the advantages of a mixed approach vs . a single methodology . A second example of a hybridized authoring tool is the NEC’s Citeseer system . Cite - seer was developed to “automate the tedious , repetitive , and slow process of ﬁnding and retrieving Web based publications” [ Bollacker et al . 1998 ] : “ . . . scientiﬁc articles on the Web are largely disorganized , with research articles being spread across archive sites , institution sites , journal sites , and researcher home pages . No index covers all of the available literature , and the major Web search engines typically do not index the content of the Postscript / PDF documents at all . ” [ Lawrence et al . 1999 ] To ameliorate this situation , Citeseer was designed to provide the following facilities to its users : ( 1 ) A set of machine learning heuristics that use conventional search engines to ﬁnd scientiﬁc articles on the web . Typical targets for such a search include Web pages that contain the words “publication” or “article , ” and ﬁles with Postscript extensions . Duplicated Postscript ﬁles and URLs are detected [ Bollacker et al . 1998 ] . ( 2 ) A translation service that converts PDF and PS ﬁles into ASCII text , combined with a veriﬁcation stage that conﬁrms that the text represents a valid research document ( using the presence or absence of a list of references near the end of the document ) . 7 In other words , this particular type of hypertext closely matches the work already discussed under the general heading of structural systems . 8 This label broadly covers those systems disuccused under the headings of statistical and semantic tools . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 19 ( 3 ) Various information extraction tools that parse key ﬁelds from the extracted text , such as the document header ( usually consisting of author name , institution , title etc . ) , the abstract and a list of citations . Once this process is complete , individual citations are analyzed to determine both the target of the citation , and the context in which it was made : “Each citation is parsed using heuristics to extract the following ﬁelds : title , author , year of publication , page numbers , and citation tag . The citation tag is the information in the citation that is used to cite that citation in the body of the document . . . this allows us to extract the context of the citations during database browsing . ” [ Bollacker et al . 1998 ] This process , repeated for each and every identiﬁed research document , facilitates the creation of a citation index , which “indexes the citations that research articles make , allowing , for example , the location of papers that cite a given paper” [ Lawrence et al . 1999 ] . This index is exploited by the document data browser , a “query processing subagent that takes a user query of proper syntax and returns an HTML formatted response , ” usually accessed through a Web interface [ Bollacker et al . 1998 ] . Automat - ically authored links ( which are presented through the data browser and not “written into” the research documents themselves ) , associate articles according to citation - based criteria ( which are of course structural in nature ) and statistical document similarity measures such as term frequency and edit distances [ Giles et al . 1998 ] . Viewed together , these tools provide a powerful mechanism for browsing the available scientiﬁc literature , permitting the user an unusual navigational ﬂexibility : “Papers can be located independent of language , and words in the title , keywords , or document . A citation index allows navigation backward in time ( the list of cited articles ) , and forward in time ( which subsequent articles cite the current article ) . . . ” [ Giles et al . 1998 ] For this reason , Citeseer has quickly become a valuable reference tool for “re - searchers , funding agents , promotion and tenure committees” [ Councill et al . 2005 ] , fur - nishing some measure of the impact of a publication on a particular ﬁeld , and providing the public at large with a free digital library of around 750 , 000 scientiﬁc documents . 9 Recent improvements to the Citeseer system have included a plugin that enables the mining of acknowledgments from research documents , thereby capturing more fully the “network of inﬂuence underlying primary scientiﬁc communication” [ Councill et al . 2005 ] . This plugin associates a given document not with some set of similar documents , but with one or more formative external inﬂuences that have somehow contributed to its creation , whether that be through academic inspiration , editorial support , or ﬁnancial backing of a parent project : 9 Interestingly , Petricek et al . [ 2005 ] have commented that the composition of the Citeseer document database exhibits a statistical bias towards research papers with multiple authors . This bias was established by examining the population of the Citeseer digital library and comparing it to the DBLP , a rival computer science bibliographic tool that is manually maintained . The authors wrote : “It is clear that Citeseer has far fewer single - authored papers . In fact , Citeseer has relatively fewer papers published by one to three authors . . . the frequency of single - authored papers in Citeseer is only 77 % of that occuring in DBLP . ” [ Petricek et al . 2005 ] This author bias is possibly an artifact of the process through which Citeseer acquires research papers for its citation index . The two main vectors into the citation index for a research paper are direct submission through a Web interface or indirect submission via the Web crawling techniques described here . These mechanisms favor multiauthor collaborations because : ( 1 ) The greater the number of authors , the more likely it is that one of them will submit the paper directly to Citeseer . ( 2 ) Even if none of the authors submits the paper directly , it is likely that the research paper will appear on more Web pages than those papers authored by single authors . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 20 M . Truran et al . “Acknowledgments may be made for a number of reasons , but often imply signiﬁcant intel - lectual debt to fellow researchers . Many funders of scientiﬁc work require acknowledgments in scientiﬁc publications , thus acknowledgments can be used to identify relationships of government and corporate entities to researchers and research output . ” [ Councill et al . 2005 ] Our ﬁnal example of a hybridized authoring tool is the TACHIR system , developed at the University of Padua [ Agosti et al . 1996 ; 1997 ] . TACHIR was a tool developed for the creation of IR hypertexts from a collection of ﬂat documents ( e . g . ASCII text ﬁles , troff documents , L A TEXmark - up etc . ) , where an IR hypertext as deﬁned by the authors is : “ . . a document base that allows access to documents mainly by navigation and browsing . An IR hypertext is composed of nodes , stores of information , and links , connections between nodes . The user navigates from node to node using Links . The series of navigational choices which are made , leads hopefully , through the document base to the desired information . ” 10 [ 1996 ] . TACHIR evolved from ideas ﬁrst offered in relation to the HyperLaw system , a soft - ware tool developed to assist in the retrieval of information from collections of legal documents and reference textbooks . The keystone supporting TACHIR was the notion that the user of an IR hypertext should be permitted to “actively browse through the indexing term structure in order to acquire a proper understanding of the semantic context in which each term has been deﬁned by the indexers” [ Agosti et al . 1991 ; 1990 ] . To enable this facility , the authors developed a dual - layer model called EXPLICIT , so named because it its goal was the explicit presentation to the user of the index terms being used to represent the document collection . The ﬁrst layer held “networks of doc - uments of interest , ” while the second level was populated by auxiliary data , forming a “plane of abstraction where indexing terms . . . are placed” [ Agosti et al . 1991 ] . Impor - tantly , this two layer model was extended some years later when Agosti et al . revisited the theme . An overarching third layer was grafted to the conceptual architecture , a layer that captured the “sets or classes of index terms” that represent “a classiﬁcation system or thesaurus for the particular application domain of the document collection” [ Agosti et al . 1996 ] . Practical application of this model invokes a set of standard text - based operations on a collection of documents ( stemming , stop lists , frequency scoring etc . ) to populate the index term level . This is followed by the rather more demanding construction of the concepts level , a process that unfortunately echoes some of the knowledge capture problems discussed in relation to semantic systems ( § 4 ) : “Most of the time it is necessary either to build manually the set of concepts , a job that is very time consuming and that needs to be performed by experts of the application domain , or use an ( or part of an ) existing thesaurus , if available . ” [ Agosti et al . 1996 ] The next stage sees the formation of links between selected IR objects occupying the same conceptual layer and IR objects occupying different but adjacent layers . This authoring process draws upon a mixture of statistical , structural and semantic tech - niques , leveraging bibliographic citations , term distribution / co - occurrence weightings , and a technique known as semantic association [ Agosti and Marchetti 1992 ] . There is little doubt that the extended EXPLICIT model provides one of the more comprehen - sive formulations of a hybrid methodology to date . Latter applied to the slightly more problematic tasks of the the automatic construction of hypermedia [ Agosti et al . 1995 ] , the inﬂuence of this model is considerable . 10 The term is associated with the concept of hypertext information retrieval ( HIR ) . HIR describes those information management tools which combine query based searching and search through the navigation and browsingfunctionsofahypertext [ DunlopandvanRijsbergen1993 ] ( seealso [ LucarellaandZanzi1993 ] ) . The combinationofthesetwotechniquesaimsto‘provideamoreefﬁciencyandeffectiveenvironmentforaccessingandretrievinginformation’ChiaramellaandKheirbek [ 1996 ] . Obviously , the ﬁeld of HIR encompasses the production of automatic authoring tools . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 21 6 . CATEGORY - INDEPENDENT ISSUES As we have seen , structural systems exploit explicit references between one part of the document collection and another part , while semantic and statistical systems revolve around the implicit references that can be found in the text of the documents [ Agosti and Smeaton 1996 ] . Despite these obvious differences in focus and technique , there are certain issues that apply equally to all automatic authoring tools . 6 . 1 . Adaptive Linking As discussed in § 2 , the decision to link one object to another in a hypertext is inherently subjective . In a study of the process through which users decide to create manual links , it was found that little similarity existed between link sets created over the same documents by different users . Another way to state this fact is to say that interlinker consistency for manually authored links is low [ Ellis et al . 1999 ] . This has obvious implications for the development of authoring tools , whatever the underlying methodology . Where links are authored automatically , those links embody rules encoded by the programmer ( s ) of the authoring tool . The rules themselves repre - sent the programmer’s conception of how documents should be joined together , and are just as subjective in nature . Some later user of the authored material might ﬁnd the links useful . Equally as probable , they may not . 11 One possible solution to this problem could be adaptive link authoring . Adaptive link authoring involves tailoring the authored links to identiﬁable and relevant character - istics of the end user of the hypertext . Possible inspiration for this line of thought comes from Lucarella and Zanzi [ 1996 ] , who , in the context of discussing a graph - based object model for visual IR systems , explored the idea of a perspective : “a form of data abstrac - tion that acts as a user interface , providing control over the visibility of the system objects” : “A reasonable way to present complex information is to produce multiple views of the same information , each focusing on different aspects and thus conforming to different needs . The cognitive overhead required to face tangled information structures can be alleviated if the system presents only the relevant pieces of the stored information while hiding the rest” [ Lucarella and Zanzi 1996 ] . However , applying this suggestion to an automatically authored document raises considerable problems . If links are to be hidden or revealed according to how “relevant” the linked object is to the user , then what mechanisms should we employ to determine “relevance” ? Furthermore , if the estimate of the relevance of a document to a particular user revolves around a collection of information proﬁling that user , then the difﬁcul - ties of compiling and using such proﬁles effectively has to be taken into consideration [ Brown et al . 2006 ; Brusilovsky 2001 ] . 6 . 2 . Overauthoring A related problem is known as overauthoring . Overauthoring occurs when the author - ing tool creates too many links , presenting the user with a meaningless “spaghetti - document” [ Foss 1988 ] : “The capability of easy linking of different pieces of information , which is considered very im - portant in the development of effective hypertext information retrieval applications , can produce information retrieval hypertexts that are very difﬁcult to use for the end - user because these same capabilities can generate user disorientation and cognitive overload” [ Agosti and Smeaton 1996 ] . 11 In support of this point see Blustein [ 2000 ] , who similarly commented upon the limited usefulness of statistically created links within single articles . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 22 M . Truran et al . Various techniques that could be used to control the number of automatically au - thored links have been suggested . For example , in Smeaton [ 1995 ] a topologically de - rived measure of hypertext known as “compactness” was used to limit link generation ( see also Botafogo et al . [ 1992 ] ) . Latter , in [ Allan 1996b ] , a technique of merging node - node links ( based on graph simpliﬁcation theory ) was proposed . Agosti et al . [ 1996 ] suggested implementing a form of preference relation between the source and destina - tion nodes , making it “possible to cut off the number of links that are presented to the user by setting up a threshold on the document - document and term - term similarity . ” One ﬁnal suggestion advocates tolerating the overlinked collection , but providing the user with navigation aids : for example , topic and document maps of the hypertext [ Zizi and Beaudouin - Lafon 1994 ] . 6 . 3 . Evaluation Evaluation of any type of automatic authoring tool is highly problematic , since the va - lidity of any authored link is a matter of opinion and not of fact . Possible solutions come from the manner in which IR systems are evaluated , since in that ﬁeld the dilemma of determining relevance between two arbitrary objects has to a certain extent already been tackled . The typical IR approach to evaluating a system involves the following : ( 1 ) A large corpus known as a test collection . 12 ( 2 ) A set of queries to be run against the collection . ( 3 ) A set of judgments that can be used to assess the results , written and applied by third party adjudicators ( National Institute of Science and Technology ) . Organizations test their systems by searching the test collection using the set of ofﬁ - cial queries . They then submit their search results to the third party adjudicators , who score the submitted work for correctness and publish their ﬁndings . 13 This approach has one major advantage : since an independent party is responsible for determining relevance , their contribution acts to create a level playing ﬁeld for all participants , neatly avoiding the impossible task of calculating relevance absolute . This strategy can be adapted for the automatic authoring community with only slight modiﬁcations . This process contemplates procedure through which the relevance of query q to document d is decided in advance by the adjudicators . If the same procedure is enacted with each document , or segment of a document , in the collection considered as a query , then a set of authoring judgments could be created for active researchers . 12 Examples includes the TREC conference materials [ Harman 1992b , 1993 ] , the TIPSTER collection [ Harman 1992a ] , the Reuter - 21578 text categorisation test collection [ Lewis 2004 ] , and the RCV1 ( Reuters Corpus Volume 1 ) [ Rose et al . 2002 ] . 13 The most common metrics used in this area are recall and precision . Recall measures how efﬁcient the search system is at retrieving documents from the collections . The recall of a system in relation to a query a is equal to the number of documents about a retrieved / number of documents about a in the collection . Precision measures the accuracy of the search system . The precision of a system in relation to a query b is equal to number of documents about b retrieved / total number of documents retrieved . Various studies have illustrated that these two metrics exhibit oppositional properties , so that an increase in one invariably leads to a decrease in the other [ Jones 1981 ; Cleverdon 1972 ; Buckland and Gey 1994 ] . As Rada states , it seems that this relationship holds true when evaluating authoring tools : “We expect that the typical relationships between thresholds and recall - precision would be ger - mane to the hypertext case : namely , the lower the indexing thresholds , the higher the retrieval recall and the lower the retrieval precision” [ Rada 1992a ] . Thus , the designer of an authoring tool will usually have to make the choice between a smaller set of possibly incomplete links and a larger set of links more likely to contain errors . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 23 Given the appropriate backing and support , such a testbed is indeed possible today . Evaluation of an authored hypertext was discussed to good effect in Blustein [ 2000 ] . The task at hand was a user - centric comparison of a set of scientiﬁc articles that had been authored using both statistical and structural tools . The central hypothe - sis , namely that the users would favour structural links over statistical links , was assessed using a protocol that examined the readers comprehension of the authored text . Blustein made the valid point that any evaluation of a hypertext should address the context of use : “Hypertext versions of discursive works ( such as this article describes ) should be evaluated not only for user satisfaction but also how useful they are . When we ask how useful hypertext is then we must take into account the tasks it is being used for , the people who are performing those tasks , and the context in which they perform them” [ Blustein 2000 ] . As the author states , this approach was entirely in accord with the earlier work of Chen and Rada [ 1996 ] and Dillon [ 1996 ] , who afﬁrmed a “contextually determined view of usability . ” Therefore , it seems prudent that any assessment metric for an authoring tool should incorporate end use as a consideration . As a ﬁnal note on this subject , Agosti et al . [ 1996 ] have commented that they are skep - tical about the use of IR evaluation techniques to judge the performance of an automatic authoring system . Instead , they recommend a more holistic approach , centering on an evaluation of “overall system performance through interaction test , comparative stud - ies and controlled experiments” [ Agosti et al . 1996 ] ( see also Hancock - Beaulieu et al . [ 1991 ] ) . Other techniques that can be used to evaluate an authoring tool have been published—interested readers are referred to Blustein et al . [ 1997 ] ; Blustein [ 1999 ] ; Croft and Turtle [ 1993 ] ; and Melucci [ 1999 ] for further discussion . 7 . THE FUTURE : THIRD ORDER AUTHORING TOOLS ? In § 5 we referred to the logical division between those systems that exploit explicit references in the document collection ( known as ﬁrst order authoring tools ) and those systems that consider implicit references in the text ( known as second order authoring tools ) . It is possible that a shift towards third order hypertext authoring tools is now im - minent . A third order authoring tool does not exclusively rely upon references ( whether explicit or implicit ) contained within the document collection . Instead , its analytical fo - cus addresses the human users of that collection , the opinions they have and their personal view of the material . Such a tool assumes that a group of users , browsing and searching a document collection , acts with intelligence . This intelligence , repre - sented by navigational histories and logs relating to document access , can be exploited to author hyperlinks . The development of third order authoring tools would coincide with a more general move in the information retrieval and hypermedia ﬁelds towards more user - centric solutions . Many recent innovations in both ﬁelds have relied upon harnessing the col - lective intelligence of users for some later value - giving process . The exact forms of this intelligence is diverse indeed , but has included countable hyperlinks or bibliographic references ( as indicators of user esteem ) [ Kleinberg 1998 ; Brin and Page 1998 ; Giles et al . 1998 ] , the development of folksonomies ( collaborative labeling systems ) [ Volkmer et al . 2005 ] , social bookmarking [ Stoilova et al . 2005 ] , peer production news , blogging , and collaborative content in the form of encyclopedia articles [ Bryant et al . 2005 ] ( see also Adafre and de Rijke [ 2005 ] ) . Despite the differences in the manner and form in which the user judgments are supplied , there is an obvious theme in common—an underlying rationale that values the contribution and discernment of the user commu - nity considered as a whole . Given that it seems possible that the next generation of ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 24 M . Truran et al . autonomous authoring tools will use some form of collective user intelligence to guide link production . 8 . CONCLUSION In this survey we have introduced a taxonomy of automatic hypertext authoring tools . Systems that rely upon the internal logical structure of documents to generate links have been labeled as structural authoring tools . Systems that rely upon mathematical functions relating to term frequencies to instant links have been identiﬁed as statistical authoring tools . Last , applications that depend upon some fundamental understanding of natural language to guide link creation we have categorized as semantic authoring tools . The remainder of the discussion in this article has focused upon work that does not readily conform to our tripartite classiﬁcation system . Best described as hybrid authoring tools , these amalgamative systems adopt cross - category procedures , shoring up the perceived liabilities of one type of approach with the strengths of another . In passing , we also outlined several key factors that will affect any autonomous authoring tool , whatever its theoretical classiﬁcation . ACKNOWLEDGMENTS Many thanks to our colleague Duncan Martin and his knowledge of L A TEX . REFERENCES A DAFRE , S . F . AND DE R IJKE , M . 2005 . Discovering missing links in wikipedia . In LinkKDD ’05 : Proceedings of the 3rd International Workshop on Link Discovery . ACM Press , New York , NY , 90 – 97 . A GOSTI , M . AND A LLAN , J . 1997 . Methods and tools for the construction of hypertexts . Inf . Process . Manage . 33 , 2 , 129 – 271 . A GOSTI , M . , C OLOTTI , R . , AND G RADENIGO , G . 1991 . A two - level hypertext retrieval model for legal data . In Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval ( SIGIR ’91 ) . ACM Press , New York , NY , USA , 316 – 325 . A GOSTI , M . , C RESTANI , F . , G RADENIGO , G . , AND M ATTIELLO , P . 1990 . An approach for the conceptual modelling of IR auxiliary data . In Ninth Annual IEEE International Phoenix Conference on Computers and Com - munications . Scottsdale , Arizona , 500 – 505 . A GOSTI , M . , C RESTANI , F . , AND M ELUCCI , M . 1996 . Design and implementation of a tool for the automatic construction of hypertexts for information retrieval . Inf . Process . Manage . 32 , 4 , 459 – 476 . A GOSTI , M . , C RESTANI , F . , AND M ELUCCI , M . 1997 . On the use of information retrieval techniques for the automatic construction of hypertext . Inf . Process . Manage . 33 , 2 , 133 – 144 . A GOSTI , M . AND M ARCHETTI , P . G . 1992 . User navigation in the IRS conceptual structure through a semantic association function . Comput . J . 35 , 3 , 194 – 199 . A GOSTI , M . , M ELUCCI , M . , AND C RESTANI , F . 1995 . Automatic authoring and construction of hypermedia for information retrieval . Multimedia Syst . 3 , 1 , 15 – 24 . A GOSTI , M . AND S MEATON , A . F . , Eds . 1996 . Hypertext and Information Retrieval . Kluwer , Boston , U . S . A . A IZAWA , A . 2003 . Aninformation - theoreticperspectiveoftf - idfmeasures . Inf . Process . Manage . 39 , 1 , 45 – 65 . A LLAN , J . 1995 . Automatic hypertext construction . Ph . D . thesis , Cornell University . A LLAN , J . 1996a . Automatic hypertext link typing . In Proceedings of the the Seventh ACM Conference on Hypertext . ACM Press , New York , NY , USA , 42 – 52 . A LLAN , J . 1996b . Automatic hypertext link typing . In Proceedings of the the Seventh ACM Conference on Hypertext . ACM Press , New York , NY , USA , 42 – 52 . A NDERSON , M . H . , N IELSEN , J . , AND R ASMUSSEN , H . 1989 . A similarity - based hypertext browser for reading the unix network news . Hypermedia 1 , 3 , 255 – 265 . A SHMAN , H . 2000 . Electronic document addressing : dealing with change . ACM Comput . Surv . 32 , 3 , 201 – 212 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 25 A SHMAN , H . , G ARRIDO , A . , AND O INAS - K UKKONEN , H . 1997 . Hand - made and computed links , precomputed and dynamic links . In Proceedings of Hypertext - Information Retrieval - Multimedia Conference ( HIM ’97 ) . N . Fuhr , G . Dittrich , and K . Tochtermann , Eds . Universit¨atsverlag Konstanz , Germany , Dortmund , Germany . B ASILI , R . , P AZIENZA , M . T . , AND Z ANZOTTO , F . M . 2003 . Inducing hyperlinking rules in text collections . In RANLP . Borovets , Bulgaria , 131 – 140 . B ERNSTEIN , M . 1992 . An apprentice that discovers hypertext links . In Hypertext : Concepts , Systems and Applications , A . Rizk , N . Streitz , and J . Andr´e , Eds . Cambridge University Press , New York , NY , 212 – 223 . B LUSTEIN , J . 2000 . Automatically generated hypertext versions of scholarly articles and their evaluation . In Proceedings of the Eleventh ACM Conference on Hypertext and Hypermedia . ACM Press , New York , NY , 201 – 210 . B LUSTEIN , J . , W EBBER , R . E . , AND T AGUE - S UTCLIFFE , J . 1997 . Methods for evaluating the quality of hypertext links . Inf . Process . Manage . 33 , 2 , 255 – 271 . B LUSTEIN , W . J . 1999 . Hypertext versions of journal articles : Computer - aided linking and realistic human - basedevaluation . Ph . D . thesis , DepartmentofComputerScience , UniversityofWesternOntario , Canada . B OLLACKER , K . , L AWRENCE , S . , AND G ILES , C . L . 1998 . CiteSeer : An autonomous web agent for automatic retrieval and identiﬁcation of interesting publications . In Proceedings of the Second International Con - ference on Autonomous Agents , K . P . Sycara and M . Wooldridge , Eds . ACM Press , New York , 116 – 123 . B OTAFOGO , R . A . , R IVLIN , E . , AND S HNEIDERMAN , B . 1992 . Structural analysis of hypertexts : identifying hier - archies and useful metrics . ACM Trans . Inf . Syst . 10 , 2 , 142 – 180 . B RAILSFORD , T . J . , S TEWART , C . D . , Z AKARIA , M . R . , AND M OORE , A . 2002 . Auto - navigation , links and narrative in an adaptive Web - based integrated learning environment . In Proceedings of the Eleventh International World Wide Web Conference . ACM , New York , NY . B RAY , T . , P AOLI , J . , S PERBOURG - M C Q UEEN , C . M . , M ALER , E . , AND Y ERGEAU , F . 2004 . Extensible mark up lan - guage ( xml ) 1 . 0 ( third edition ) . http : / / www . w3 . org / TR / REC - xml / . W3C . B RIN , S . AND P AGE , L . 1998 . The anatomy of a large - scale hypertextual web search engine . In Proceedings of the seventh international conference on World Wide Web 7 ( WWW7 ) . Elsevier Science Publishers B . V . , Amsterdam , The Netherlands , 107 – 117 . B ROWN , E . , B RAILSFORD , T . , F ISHER , T . , M OORE , A . , AND A SHMAN , H . 2006 . Reappraisingcognitivestylesinadap - tive Web applications . In Proceedings of the 15th International World Wide Web Conference ( WWW2006 ) . ACM Press , New York , NY . B ROWN , P . J . 1986 . Interactive documentation . Softw . Pract . Exper . 16 , 3 , 292 – 299 . B ROWN , P . J . AND R USSELL , M . T . 1988 . Converting help systems to hypertext . Softw . Pract . Exper . 18 , 2 , 163 – 165 . B RUSILOVSKY , P . 2001 . User modeling and user adapted interaction . Adapt . Hyperm . 11 , 1 / 2 , 87 – 110 . B RYANT , S . L . , F ORTE , A . , AND B RUCKMAN , A . 2005 . Becoming Wikipedian : transformation of participation in a collaborative online encyclopedia . In Proceedings of the 2005 international ACM SIGGROUP Conference on Supporting Group Work ( GROUP ’05 ) . ACM Press , New York , NY , 1 – 10 . B UCKLAND , M . AND G EY , F . 1994 . The relationship between recall and precision . J . Amer . Soc . Inform . Sci . 45 , 1 , 12 – 19 . B USH , V . 1945 . As we may think . The Atlantic Monthly 176 , 1 ( July ) , 101 – 108 . C ALLAN , J . P . AND C ROFT , W . B . 1993 . An approach to incorporating CBR concepts in IR systems . In AAAI Spring Symposium Series : Case - Based Reasoning and Information Retrieval—Exploring the Opportuni - ties for Information Sharing . AAAI Press , Menlo Park , CA , USA . C AN , F . 1993 . Incremental clustering for dynamic information processing . ACM Trans . Inform . Proc . 11 , 2 , 143 – 164 . C AWLEY , J . W . T . 1993 . Flinkman . Honours Thesis , Discipline of Computer Science , Flinders University of South Australia . C HEN , C . AND R ADA , R . 1996 . Interacting with hypertext : A meta - analysis of experimental studies . Hum . Comput . Inter . 11 , 2 , 125 – 156 . C HIARAMELLA , Y . AND K HEIRBEK , A . 1996 . An integrated model for hypermedia and information retrieval . In Information Retrieval and Hypertext , M . Agosti and A . Smeaton , Eds . Kluwer , Amsterdam , NL , 139 – 178 . C LEVERDON , C . W . 1972 . On the inverse relationship of recall and precision . J . Docum . 28 , 195 – 201 . C ONKLIN , J . 1987 . Hypertext : An introduction and survey . IEEE Computing Society Press 20 , 9 , 17 – 41 . C OUNCILL , I . G . , G ILES , C . L . , H AN , H . , AND M ANAVOGLU , E . 2005 . Automatic acknowledgement indexing : expanding the semantics of contribution in the citeseer digital library . In Proceedings of the 3rd Inter - national Conference on Knowledge Capture ( KCAP ’05 ) . ACM Press , New York , NY , 19 – 26 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 26 M . Truran et al . C ROFT , W . B . AND T URTLE , H . R . 1993 . Retrieval strategies for hypertext . Inf . Process . Manage . 29 , 3 , 313 – 324 . D AVIDSON , C . 1993 . What your database hides away . New Scientist , 28 – 31 . January 9th . D AVIS , H . , H ALL , W . , H EATH , I . , H ILL , G . , AND W ILKINS , R . 1992 . Towards an integrated information envi - ronment with open hypermedia systems . In European Conference on Hypertext Technology ( ECHT ’92 ) . ACM Press , New York , NY , 181 – 190 . D EERWESTER , S . , D UMAIS , S . T . , F URNAS , G . W . , L ANDAUER , T . K . , AND H ARSHMAN , R . 1990 . Indexing by latent semantic analysis . J . Amer . Soc . Inform . Sci . 41 , 391 – 407 . D E R OSE , S . , D ANIEL , R . , G ROSSO , P . , M ALER , E . , M ARSH , J . , AND W ALSH , N . 2002 . Xml pointer language ( xpointer ) . http : / / www . w3 . org / TR / xptr / . W3C . D E R OSE , S . , M ALER , E . , AND O RCHARD , D . 2001 . Xml linking language ( xlink ) version 1 . 0 . http : / / www . w3 . org / TR / xlink / . W3C . D E R OSE , S . J . 1999 . Xml linking . ACM Comput . Surv . 31 , 4 ( Dec . ) . Article No . 21 . D ILLON , A . 1996 . TIMS : A framework for the design of usable electronic text . In Cognitive Aspects of Elec - tronic Text Processing , H . van Oostendorp and S . DeMul Eds . Ablex , Norwood , NJ , 99 – 120 . D RAKOS , N . 1994 . From text to hypertext : a post - hoc rationalisation of latex2html . In Selected Papers of the First Conference on World - Wide Web . Elsevier Science Publishers B . V . , Amsterdam , The Netherlands , The Netherlands , 215 – 224 . D UNLOP , M . D . AND VAN R IJSBERGEN , C . J . 1993 . Hypermedia and free text retrieval . Inf . Process . Manage . 29 , 3 , 287 – 298 . E LLIS , D . , F URNER , J . , AND W ILLETT , P . 1999 . Inter - linker consistency in the manual construction of hypertext documents . ACM Comput . Surv . 31 , 18 . E LLIS , D . , F URNER - H INES , J . , AND W ILLETT , P . 1994 . On the creation of hypertext links in full - text documents : Measurement of inter - linker consistency . J . Docum . 50 , 67 – 98 . E LLIS , D . , F URNER - H INES , J . , AND W ILLETT , P . 1996 . On the creation of hypertext links in full - text documents : measurement of retrieval effectiveness . J . Amer . Soc . Inform . Sci . 47 , 4 , 287 – 300 . F OSS , C . L . 1988 . Effective browsing in hypertext . In Proceedings of the Conference on User - Oriented Context - based Text and Image Handling ( RIAO ’88 ) . Le centre de Hautes Etudes Internationales d‘Informatique Documentaire , Paris , France , 82 – 98 . F OUNTAIN , A . M . , H ALL , W . , H EATH , I . , AND D AVIS , H . 1991 . MICROCOSM : An open model for hypermedia with dynamic linking . In Hypertext : Concepts , Systems and Applications : Proceedings of the European Conference on Hypertext ( ECHT ’91 ) . A . Rizk , N . A . Streitz , and J . Andr´e , Eds . Cambridge University Press , Cambrisge , UK , 298 – 311 . F RANKE , III , C . H . AND W AHL , N . J . 1995 . Authoring a hypertext unix help manual . In Proceedings of the 1995 ACM Twenty - Third Annual Conference on Computer Science . ACM Press , New York , NY , 238 – 245 . F RISSE , M . E . 1988 . Searching for information in a hypertext medical handbook . Comm . ACM 31 , 7 , 880 – 886 . F RISSE , M . E . AND C OUSINS , S . B . 1989 . Information retrieval from hypertext : update on the dynamic medical handbook project . In Proceedings of the Second Annual ACM Conference on Hypertext ( HYPERTEXT ’89 ) . ACM Press , New York , NY , USA , 199 – 212 . F ULLER , M . , M ACKIE , E . , S ACKS - D AVIS , R . , AND W ILKINSON , R . 1993 . Structured answers for a large structured document collection . In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’93 ) . ACM Press , New York , NY , 204 – 213 . F UNK AND W AGNALLS . 1979 . Funk and Wagnalls New Encyclopedia . Funk and Wagnalls , New York , NY . F URNAS , G . W . , D EERWESTER , S . , D UMAIS , S . T . , L ANDAUER , T . K . , H ARSHMAN , R . A . , S TREETER , L . A . , AND L OCHBAUM , K . E . 1988 . Information retrieval using a singular value decomposition model of latent semantic structure . In Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’88 ) . ACM Press , New York , NY , 465 – 480 . F URUTA , R . 1989 . Concepts and models for structured documents . In Structured Documents , J . Andre , R . Furuta , and V . Quint , Eds . Cambridge University Press , Cambridge , UK , 7 – 38 . F URUTA , R . , P LAISANT , C . , AND S HNEIDERMAN , B . 1989a . Automatically transforming regularly structured documents into hypertext . Electronic Publishing—Origination , Dissemination , and Design 2 , 4 , 211 – 229 . F URUTA , R . , P LAISANT , C . , AND S HNEIDERMAN , B . 1989b . A spectrum of automatic hypertext constructions . Hypermedia 1 , 2 , 179 – 195 . G ILES , C . L . , B OLLACKER , K . D . , AND L AWRENCE , S . 1998 . Citeseer : an automatic citation indexing system . In Proceedings of the Third ACM Conference on Digital Libraries ( DL ’98 ) . ACM Press , New York , NY , 89 – 98 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 27 G LUSHKO , R . J . 1989 . Design issues for multi - document hypertexts . In Proceedings of the Second Annual ACM Conference on Hypertext . ACM Press , New York , NY , 51 – 60 . G OSLING , J . 1981 . A redisplay algorithm . In Proceedings of the ACM SIGPLAN SIGOA Symposium on Text manipulation . ACM Press , New York , NY , USA , 123 – 129 . G REEN , S . J . 1999 . Building hypertext links by computing semantic similarity . IEEE Trans . Knowl . Data Eng . 11 , 5 , 713 – 730 . H ALL , W . , H ILL , G . , AND D AVIS , H . 1993 . The microcosm link service . In Proceedings of the Fifth ACM Con - ference on Hypertext . ACM Press , New York , NY , 256 – 259 . H ANCOCK - B EAULIEU , M . , R OBERTSON , S . , AND N EILSON , C . 1991 . Evaluation of online catalogues : eliciting in - formation from the user . Inf . Process . Manage . 27 , 5 , 523 – 532 . H ARMAN , D . K . 1988 . Towards interactive query expansion . In Proceedings of the Eleventh International Conference on Research and Development in Information Retrieval ( SIGIR ’88 ) . Y . Chinramella , Ed . ACM , New York , NY , 321 – 332 . H ARMAN , D . K . 1991 . How effective is sufﬁxing ? J . Amer . Soc . Inform . Sci . 42 , 1 , 7 – 51 . H ARMAN , D . K . 1992a . The DARPA TIPSTER project . SIGIR Forum 26 , 2 , 26 – 28 . H ARMAN , D . K . , Ed . 1992b . The ﬁrst Text REtrieval Conference . NIST Special Publication 500 - 207 . National Institute of Standards and Technology , Department of Commerce , Gaithersburg , Maryland . NTIS Order Code : PB93 - 191641 . H ARMAN , D . K . 1993 . Overview of the ﬁrst TREC conference . In Proceedings of the Sixteenth Annual ACM Conference on Research and Development in Information Retrieval ( SIGIR ’93 ) . ACM , New York , NY , 36 – 47 . H AYES , P . AND P EPPER , J . 1989 . Towards an integrated maintenance advisor . In Proceedings of the Second Annual ACM Conference on Hypertext . ACM Press , New York , NY , 119 – 127 . H AYES , P . J . , A NDERSEN , P . , AND S AFIER , S . 1985 . Semantic case frame parsing and syntactic generality . In Proceedings of the Twenty - Third Annual Meeting of the Association for Computing Linguistics . MIT Press , Cambridge , MA . H AYES , P . J . , K NECHT , L . E . , AND C ELLIO , M . J . 1988 . A news story categorization system . In Proceedings of the Second Conference on Applied Natural Language Processing . ACM Press , New York , NY , 9 – 17 . H EARST , M . 1997 . Texttiling : Segmenting text into multi - paragraph subtopic passages . Computat . Ling . 23 , 1 . H EARST , M . A . AND P LAUNT , C . 1993 . Subtopic structuring for full - length document access . In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’93 ) . ACM Press , New York , NY , USA , 59 – 68 . I NGHAM , D . , C AUGHEY , S . , AND L ITTLE , M . 1996 . Fixing the ‘broken - link’ problem : the w3objects approach . In Proceedings of the Fifth International World Wide Web Conference on Computer Networks and ISDN Systems . Elsevier Science Publishers B . V . , Paris , France , 1255 – 1268 . J ACOBS , P . S . , Ed . 1992 . Text - based Intelligent Systems : Current Research and Practice in Information Ex - traction and Retrieval . Lawrence Erlbaum Associates , Inc . , Mahwah , NJ . J ONES , K . S . 1972 . A statistical interpretation of term speciﬁcity and its application in retrieval . J . Docum . 28 , 1 , 11 – 20 . J ONES , K . S . 1981 . The Cranﬁeld tests . In Information Retrieval Experiment , K . S . Jones , Ed . Butterworths , London , UK , 256 – 284 . K AHN , G . , K EPNER , A . , AND P EPPER , J . 1987 . Test , a model - driven application shell . In Proceedings of the Sixth National Conference of the American Association for AI . AAAI Press , Menlo Park , CA , 814 – 818 . K ANTROWITZ , M . , M OHIT , B . , AND M ITTAL , V . 2000 . Stemming and its effects on TF - IDF ranking ( poster ses - sion ) . In Proceedings of the Twenty - third Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , New York , NY , 357 – 359 . K ELLER , B . AND K NUTILLA , T . 1989 . Building an expert diagnostic system . Ordnance , 44 – 45 . K ELLOGG , R . B . AND S UBHAS , M . 1996 . Text to hypertext : can clustering solve the problem in digital libraries ? In Proceedings of the First ACM International Conference on Digital Libraries . ACM Press , New York , NY , USA , 144 – 150 . K ESSLER , M . M . 1963 . Bibliographic coupling between scientiﬁc papers . Amer . Docum . 14 , 10 – 25 . K LEINBERG , J . M . 1998 . Authoritative sources in a hyperlinked environment . In Proceedings of the Ninth Annual ACM - SIAM Symposium on Discrete Algorithms ( SODA ’98 ) . Society for Industrial and Applied Mathematics , Philadelphia , PA , USA , 668 – 677 . L AWRENCE , S . , B OLLACKER , K . , AND G ILES , C . L . 1999 . Indexing and retrieval of scientiﬁc literature . In Pro - ceedings of the Eighth International Conference on Information and Knowledge Management ( CIKM ’99 ) . ACM Press , New York , NY , USA , 139 – 146 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 28 M . Truran et al . L EE , D . L . , C HUANG , H . , AND S EAMONS , K . 1997 . Document ranking and the vector - space model . IEEE Softw . 14 , 2 , 67 – 75 . L ELU , A . 1991 . Automatic generation of ‘hyper - paths’ in information retrieval systems : a stochastic and an incremental algorithm . In Proceedings of the Fourteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , New York , NY , 326 – 335 . L ENAT , D . B . 1995 . CYC : A large - scale investment in knowledge infrastructure . Comm . ACM 38 , 11 , 33 – 38 . L ENAT , D . B . 1998 . From 2001 to 2001 : Common sense and the mind of HAL . In HAL‘s Legacy : 2001’s Computer as Dream and Reality , D . G . Stork , Ed . MIT Press , Cambridge , MA . L ENAT , D . B . , G UHA , R . V . , P ITTMAN , K . , P RATT , D . , AND S HEPHERD , M . 1990 . Cyc : towardprogramswithcommon sense . Comm . ACM 33 , 8 , 30 – 49 . L EWIS , D . 2004 . Reuters - 21578 text categorisation collection . http : / / www . daviddlewis . com / resources / testcollections / reuters21578 . Distribution 1 . 0 . , Copyright Reuters Ltd . and Carngie , 1987 . L OVINS , J . B . 1968 . Development of a stemming algorithm . Mech . Transl . Comput . Ling . 11 , 1 – 2 , 11 – 31 . L UCARELLA , D . AND Z ANZI , A . 1993 . Information retrieval from hypertext : an approach using plausible in - ference . Inf . Process . Manage . 29 , 3 , 299 – 312 . L UCARELLA , D . AND Z ANZI , A . 1996 . A visual retrieval environment for hypermedia information systems . ACM Trans . Inf . Syst . 14 , 1 , 3 – 29 . L UHN , H . P . 1958 . The automatic creation of literature abstracts . IBM J . Res . Devel . 2 , 159 – 165 . M ACEDO , A . A . , DA G RACA C AMPOS P IMENTEL , M . , AND C AMACHO - G UERRERO , J . A . 2002 . An infrastructure for open latent semantic linking . In Proceedings of the Thirteenth ACM Conference on Hypertext and Hyper - media ( HYPERTEXT ’02 ) . ACM Press , New York , NY , USA , 107 – 116 . M AMRAK , S . , K AEBLING , M . , N ICHOLAS , C . , AND S HARE , M . 1987 . A software architecture for supporting the exchange of electronic manuscripts . Comm . ACM 30 , 5 , 408 – 414 . M AMRAK , S . A . , K AEBLING , M . J . , N ICHOLAS , C . K . , AND S HARE , M . 1989 . Chameleon : A system for solving the data - transportation problem . IEEE Trans . Softw . Eng . 15 , 9 , 1090 – 1108 . M ARSHALL , C . C . AND S HIPMAN III , F . M . 1993 . Searching for the missing link : Discovering implicit structure in spatial hypertext . In Proceedings of the Fifth ACM Conference on Hypertext ( HYPERTEXT ’93 ) . ACM Press , New York , NY , USA , 217 – 230 . M ELUCCI , M . 1999 . An evaluation of automatically constructed hypertexts for information retrieval . Inf . Retr . 1 , 1 – 2 , 91 – 114 . M ELUCCI , M . AND O RIO , N . 2003 . A novel method for stemmer generation based on hidden Markov models . In Proceedings of the Twelfth International Conference on Information and Knowledge Management . ACM Press , New York , NY , 131 – 138 . M ILI , H . AND R ADA , R . 1985 . A statistically built knowledge base . In Proceedings of the Expert System Government Conference . IEEE Computer Society Press , Washington , DC , 457 – 463 . M ORRIS , J . AND H IRST , G . 1991 . Lexical cohesion computed by theasaural relations as an indicator of the structure of text . Computat . Ling . 17 , 1 , 21 – 48 . M URRAY , H . , Ed . 1928 . The Oxford English Dictionary . Oxford University Press , Oxford , UK . N ENTWICH , C . , C APRA , L . , E MMERICH , W . , AND F INKELSTEIN , A . 2002 . xlinkit : a consistency checking and smart link generation service . ACM Trans . Inter . Tech . 2 , 2 , 151 – 185 . N UNN , D . , L EGGETT , J . , B OYLE , C . , AND H ICKS , D . 1988 . The rexx project : A case study of automatic hypertext construction . Tech . rep . , Hypertex Research Lab , Texas A & M University . TAMU 88 – 021 . O RLAND , M . AND S ALTMAN , R . , Eds . 1986 . Manual of Medical Therapeutics , 25 ed . Little , Bown and Co . , Boston , MA , USA . P AICE , C . D . 1990 . Another stemmer . SIGIR Forum 24 , 3 , 56 – 61 . P AICE , C . D . 1994 . An evaluation method for stemming algorithms . In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . Springer - Verlag , New York , NY , 42 – 50 . P AICE , C . D . 1996 . Method for evaluation of stemming algorithms based on error counting . J . Amer . Soc . Inf . Sci . 47 , 8 , 632 – 649 . P ARUNAK , H . V . D . 1989 . Hypermedia topologies and user navigation . In Proceedings of the Second Annual ACM Conference on Hypertext ( HYPERTEXT ’89 ) . ACM Press , New York , NY , USA , 43 – 50 . P ETRICEK , V . , C OX , I . J . , H AN , H . , C OUNCILL , I . G . , AND G ILES , C . L . 2005 . Modeling the author bias be - tween two on - line computer science citation databases . In Special Interest Tracks and Posters of the 14th International Conference on World Wide Web ( WWW ’05 ) . ACM Press , New York , NY , USA , 1062 – 1063 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Autonomous Authoring Tools for Hypertext Article 8 / 29 P ILZECKER , M . 2002 . Semantically driven automatic hyperlinking . In 15th European Conference on Artiﬁ - cial Intelligence , ( ECAI 2002 ) — Workshop 22 : Semantic Authoring , Annotation and Knowledge Markup . IOS Press , Lyon , France , 99 – 102 . R ABINER , L . AND S CHAFER , R . 1978 . Digital Processing of Speech Signals . Prentice - Hall , Englewood Cliffs , N . J . R ADA , R . 1992a . Converting a textbook to hypertext . ACM Trans . Inf . Syst . 10 , 3 , 294 – 315 . R ADA , R . 1992b . Hypertext : from Text to Expertext . McGraw - Hill ; Inc . , New York , NY . R AGHAVAN , V . V . AND W ONG , S . K . M . 1986 . A critical analysis of vector space model for information retrieval . J . Amer . Soc . Inf . Sci . 37 , 5 , 279 – 287 . R AYMOND , D . R . AND T OMPA , F . W . 1987 . Hypertext and the new Oxford English Dictionary . In Proceedings of the ACM conference on Hypertext . ACM Press , New York , NY , USA , 143 – 153 . R EARICK , T . C . 1991 . Automating the Conversion of Text into Hypertext . McGraw - Hill , Inc . , Hightstown , NJ , 113 – 140 . R EMDE , J . R . , G OMEZ , L . M . , AND L ANDAUER , T . K . 1987 . Superbook : an automatic tool for information explo - ration u2014hypertext ? In Proceedings of the ACM Conference on Hypertext ( HYPERTEXT ’87 ) . ACM Press , New York , NY , USA , 175 – 188 . R INER , R . 1991 . Automated conversion . In Hypertext / Hypermedia Handbook . Intertext Publication , McGraw Hill , Hightstown , New Jersey , 95 – 111 . R OSE , T . G . , S TEVENSON , M . , AND W HITEHEAD , M . 2002 . The Reuters corpus volume 1—from yesterday’s news to tomorrow’s language resources . In Proceedings of the Third International Conference on Language Resources and Evaluation ( LREC ’02 ) . Evaluation and Language Resource Distribution Agency ( ELDA ) , Paris , France . S ALTON , G . 1991a . Developments in automatic text retrieval . Science 253 , 974 – 97 . S ALTON , G . 1991b . The smart document retrieval project . In Proceedings of the Fourteenth Annual Inter - national ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , New York , NY , 356 – 358 . S ALTON , G . AND B UCKLEY , C . 1988 . Term weighting approaches in automatic text retrieval . Inf . Proc . Manage . 24 , 5 , 513 – 523 . S ALTON , G . AND B UCKLEY , C . 1989 . On the automatic generation of content links in hypertext . Tech . rep . , Cornell University , Ithaca , NY , USA . S ALTON , G . AND B UCKLEY , C . 1990 . Flexible text matching for information retrieval . Tech . rep . , Cornell University , Ithaca , NY , USA . TR 90 - 1158 . S ALTON , G . AND B UCKLEY , C . 1991 . Automatic text structuring and retrieval - experiments in automatic en - cyclopedia searching . In Proceedings of the 14th Annual International ACM SIGIR Conference on Re - search and Development in Information Retrieval ( SIGIR ’91 ) . ACM Press , New York , NY , USA , 21 – 30 . S ALTON , G . AND M C G ILL , M . J . 1986 . Introduction to Modern Information Retrieval . McGraw - Hill , Inc . , Hightstown , NJ . S ALTON , G . , S INGHAL , A . , B UCKLEY , C . , AND M ITRA , M . 1996 . Automatictextdecompositionusingtextsegments and text themes . In Proceedings of the the Seventh ACM Conference on Hypertext ( HYPERTEXT ’96 ) . ACM Press , New York , NY , USA , 53 – 65 . S ALTON , G . , Y ANG , C . S . , AND Y U , C . T . 1975 . A theory of term importance in automatic text analysis . J . Amer . Soc . Inf . Sci . 26 , 1 , 33 – 44 . S AVOY , J . 1996 . Citation schemes in hypertext information retrieval . In Information Retrieval and Hypertext , M . Agosti and A . Smeaton , Eds . Kluwer , Amsterdam , NL , 99 – 120 . S HNEIDERMAN , B . 1987 . User interface design for the hyperties electronic encyclopedia ( panel session ) . In Proceeding of the ACM Conference on Hypertext ( HYPERTEXT ’87 ) . ACM Press , New York , NY , USA , 189 – 194 . S MALL , H . 1973 . Co - citation in the scientiﬁc literature : A new measure of the relationship between two documents . J . Amer . Soc . Inf . Sci . 24 , 4 , 265 – 269 . S MEATON , A . F . 1992 . Progress in the application of natural language processing to information retrieval tasks . Comput . J . 35 , 3 , 268 – 278 . S MEATON , A . F . 1995 . Building hypertext under the inﬂuence of topology metrics . In Proceedings of the IWHD Conference . S TOILOVA , L . , H OLLOWAY , T . , M ARKINES , B . , M AGUITMAN , A . G . , AND M ENCZER , F . 2005 . Givealink : mining a semantic network of bookmarks for web search and recommendation . In Proceedings of the 3rd Interna - tional Workshop on Link Discovery ( LinkKDD ’05 ) . ACM Press , New York , NY , 66 – 73 . ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 . Article 8 / 30 M . Truran et al . S UGIYAMA , K . , H ATANO , K . , Y OSHIKAWA , M . , AND U EMURA , S . 2003 . Reﬁnement of TF - IDF schemes for Web pages using their hyperlinked neighbouring pages . In Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia . ACM Press , New York , NY , USA , 198 – 207 . T EBBUTT , J . 1999 . User evaluation of automatically generated semantic hypertext links in a heavily used procedural manual . Inf . Proc . Manage . 35 , 1 , 1 – 18 . T HISTLEWAITE , P . 1997 . Automatic construction and management of large open Webs . Inf . Proc . Manage . 33 , 2 , 161 – 173 . V ALLE , E . D . , C ASTAGNA , P . , AND B RIOSCHI , M . 2003 . Towards a semantic enterprise information portal . In Second International Conference on Knowledge Capture ( Workshop on Knowledge Management and the Semantic Web ) ( KCAP ’03 ) . ACM Press , New York , NY , USA . VAN R IJSBERGEN , C . J . 1980 . Information Retrieval , 2 ed . Butterworths , London , UK . V OLKMER , T . , S MITH , J . R . , AND N ATSEV , A . P . 2005 . A Web - based system for collaborative annotation of large image and video collections : an evaluation and user study . In Proceedings of the 13th Annual ACM International Conference on Multimedia ( MULTIMEDIA ’05 ) . ACM Press , New York , NY , USA , 892 – 901 . W ILSON , E . 1988 . Integrated information retrieval for law in a hypertext environment . In Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , New York , NY , 663 – 677 . W ITTGENSTEIN , L . 1953 . Philosophical Investigations . OUP , Oxford , United Kingdom . Z IPF , G . K . 1949 . Human Behaviour and the Principle of Least Effort . Addison - Wesley , Reading , MA . Z IZI , M . AND B EAUDOUIN - L AFON , M . 1994 . Accessing hyperdocuments through interactive dynamic maps . In Proceedings of the 1994 ACM European Conference on Hypermedia Technology ( ECHT ’94 ) . ACM Press , New York , NY , USA , 126 – 135 . Z OBEL , J . , M OFFAT , A . , AND S ACKS - D AVIS , R . 1993 . Searching large lexicons for partially speciﬁed terms using compressed inverted ﬁles . In Proceedings of the 19th International Conference on Very Large Data Bases ( VLDB ’93 ) . Morgan Kaufmann Publishers Inc . , San Francisco , CA , 290 – 301 . Received June 2004 ; revised June 2006 ; accepted March 2007 ACM Computing Surveys , Vol . 39 , No . 3 , Article 8 , Publication date : August 2007 .