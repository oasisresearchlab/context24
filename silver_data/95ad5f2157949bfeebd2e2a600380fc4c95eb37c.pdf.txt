An optimal ﬁrst order method based on optimal quadratic averaging Dmitriy Drusvyatskiy ∗ Maryam Fazel † Scott Roy ‡ Abstract In a recent paper , Bubeck , Lee , and Singh introduced a new ﬁrst order method for minimizing smooth strongly convex functions . Their geometric descent algorithm , largely inspired by the ellipsoid method , enjoys the optimal linear rate of convergence . Motivated by their work , we propose a close variant that iteratively maintains a quadratic global under - estimator of the objective function , whose minimal value approaches the true min - imum at an optimal rate . The resulting intuitive scheme comes equipped with a natural stopping criterion and can be numerically accelerated by using accumulated information . 1 Introduction Consider a function f : R n → R that is β - smooth and α - strongly convex . Thus each point x yields a quadratic upper estimator and a quadratic lower estimator of the function . Namely , the inequality q ( y ; x ) ≤ f ( y ) ≤ Q ( y ; x ) holds for all x , y ∈ R n , where we set q ( y ; x ) : = f ( x ) + (cid:104)∇ f ( x ) , y − x (cid:105) + α 2 (cid:107) y − x (cid:107) 2 , Q ( y ; x ) : = f ( x ) + (cid:104)∇ f ( x ) , y − x (cid:105) + β 2 (cid:107) y − x (cid:107) 2 . Classically , one step of the steepest descent algorithm decreases the distance of the iterate to the minimizer of f by the fraction 1 − α / β . This linear convergence rate is suboptimal from a computational complexity viewpoint . Optimal ﬁrst - order methods , originating in Nesterov’s work [ 8 ] achieve the superior ( and the best possible ) linear rate of 1 − (cid:112) α / β ; see also the discussion in [ 7 , Section 2 . 2 ] . Such accelerated schemes , on the other hand , are notoriously diﬃcult to analyze . Numerous recent papers ( e . g . [ 1 , 3 , 6 , 9 ] ) have aimed to shed new light on optimal algorithms . This manuscript is motivated by the novel geometric descent algorithm of Bubeck , Lee , and Singh [ 3 ] . Their scheme is highly geometric , sharing some aspects with the ellipsoid method , and it achieves the optimal linear rate of convergence . Moreover , the geometric descent algorithm often has much better practical performance than accelerated gradient ∗ University of Washington , Department of Mathematics , Seattle , WA 98195 ; ddrusv @ uw . edu . Research of Drusvyatskiy was partially supported by the AFOSR YIP award FA9550 - 15 - 1 - 0237 . † University of Washington , Department of Electrical Engineering , Seattle , WA 98195 ; mfazel @ uw . edu . Research partially supported by ONR award N00014 - 12 - 1 - 1002 and NSF award CIF - 1409836 . ‡ University of Washington , Department of Mathematics , Seattle , WA 98195 ; scottroy @ uw . edu 1 methods ; see the discussion in [ 3 ] . In this paper , we propose an intuitive variant of the geo - metric descent algorithm that maintains a quadratic lower model of the objective function , whose minimum value converges to the the true minimum at an optimal linear rate . The algorithm has a number of virtues . First , it comes equipped with a natural stopping criterion . Second , a formal comparison with the original accelerated gradient method [ 7 , 8 ] is immedi - ate . Finally , one can signiﬁcantly speed up the method in practice by utilizing accumulated information – a limited memory version of the scheme . The outline of the paper is as follows . In Section 2 , we describe the optimal quadratic averaging framework ( Algorithm 1 ) – the focal point of the manuscript . In Section 3 , we pro - pose a limited memory version of Algorithm 1 , based on iteratively solving small dimensional quadratic programs . Section 4 discusses the close connection between Algorithm 1 and the geometric descent method of [ 3 ] . Section 5 is devoted to numerical illustrations , in particular showing that the optimal quadratic averaging algorithm with memory can be competitive with L - BFGS . We ﬁnish the paper within Section 6 , where we discuss the challenges that must be overcome in order to derive proximal extensions . 1 . 1 Notation We follow the notation of [ 3 ] – the motivation for the current work . Given a point x ∈ R n , we deﬁne a short step x + : = x − 1 β ∇ f ( x ) and a long step x + + : = x − 1 α ∇ f ( x ) . Setting y = x + in the quadratic bound f ( y ) ≤ Q ( y ; x ) yields the standard inequality f ( x + ) + 1 2 β (cid:107)∇ f ( x ) (cid:107) 2 ≤ f ( x ) . ( 1 ) We denote the unique minimizer of f by x ∗ , its minimal value by f ∗ , and its condition number by κ : = β / α . Throughout , the symbol B ( x , R 2 ) stands for the Euclidean ball of radius R around x . For any points x , y ∈ R n , we let line search ( x , y ) be the minimizer of f on the line between x and y . 2 Optimal quadratic averaging The starting point for our development is the elementary observation that every point ¯ x provides a quadratic under - estimator of the objective function , having a canonical form . Indeed , completing the square in the strong convexity inequality f ( x ) ≥ q ( x ; ¯ x ) yields f ( x ) ≥ (cid:32) f ( ¯ x ) − (cid:107)∇ f ( ¯ x ) (cid:107) 2 2 α (cid:33) + α 2 (cid:13)(cid:13) x − ¯ x + + (cid:13)(cid:13) 2 . ( 2 ) Suppose we have now available two quadratic lower - estimators : f ( x ) ≥ Q A ( x ) : = v A + α 2 (cid:107) x − x A (cid:107) 2 and f ( x ) ≥ Q B ( x ) : = v B + α 2 (cid:107) x − x B (cid:107) 2 . Clearly , the minimal values of Q A and of Q B lower - bound the minimal value of f . For any λ ∈ [ 0 , 1 ] , the average Q λ : = λQ A + ( 1 − λ ) Q B is again a quadratic lower - estimator of f . Thus we are led to the question : 2 What choice of λ yields the tightest lower - bound on the minimal value of f ? To answer this question , observe the equality Q λ ( x ) : = λQ A ( x ) + ( 1 − λ ) Q B ( x ) = v λ + α 2 (cid:107) x − c λ (cid:107) 2 , where c λ = λx A + ( 1 − λ ) x B and v λ = v B + (cid:16) v A − v B + α 2 (cid:107) x A − x B (cid:107) 2 (cid:17) λ − (cid:16) α 2 (cid:107) x A − x B (cid:107) 2 (cid:17) λ 2 . ( 3 ) In particular , the average Q λ has the same canonical form as Q A and Q B . A quick compu - tation now shows that v λ ( the minimum of Q λ ) is maximized by setting ¯ λ : = proj [ 0 , 1 ] (cid:18) 1 2 + v A − v B α (cid:107) x A − x B (cid:107) 2 (cid:19) . With this choice of λ , we call the quadratic function Q = ¯ v + α 2 (cid:107) · − ¯ c (cid:107) 2 the optimal averaging of Q A and Q B . See Figure 1 for an illustration . Figure 1 : The optimal averaging of Q A ( x ) = 1 + 0 . 5 ( x + 2 ) 2 and Q B ( x ) = 3 + 0 . 5 ( x − 4 ) 2 . An algorithmic idea emerges . Given a current iterate x k , form the quadratic lower - model Q ( · ) in ( 2 ) with ¯ x = x k . Then let Q k be the optimal averaging of Q and the quadratic lower model Q k − 1 from the previous step . Finally deﬁne x k + 1 to be the minimizer of Q k , and repeat . Though attractive , the scheme does not converge at an optimal rate . Indeed , this algorithm is closely related to the suboptimal method in [ 3 ] ; see Section 4 . 1 for a discussion . The main idea behind acceleration , natural in retrospect , is a separation of roles : one must maintain two sequences of points x k and c k . The points x k will generate quadratic lower models as above , while c k will be the minimizers of the quadratics . We summarize the proposed method in Algorithm 1 . The rule for determining the iterate x k by a line search is entirely motivated by the geometric descent method in [ 3 ] . 3 Algorithm 1 : Optimal Quadratic Averaging Input : Starting point x 0 and strong convexity constant α > 0 . Output : Final quadratic Q K ( x ) = v K + α 2 (cid:107) x − c K (cid:107) 2 and x + K . Set Q 0 ( x ) = v 0 + α 2 (cid:107) x − c 0 (cid:107) 2 , where v 0 = f ( x 0 ) − (cid:107)∇ f ( x 0 ) (cid:107) 2 2 α and c 0 = x + + 0 ; for k = 1 , . . . , K do Set x k = line search (cid:0) c k − 1 , x + k − 1 (cid:1) ; Set Q ( x ) = (cid:16) f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α (cid:17) + α 2 (cid:13)(cid:13) x − x + + k (cid:13)(cid:13) 2 ; Let Q k ( x ) = v k + α 2 (cid:107) x − c k (cid:107) 2 be the optimal averaging of Q and Q k − 1 ; end Remark 2 . 1 . When implementing Algorithm 1 , we set x + k = line search ( x k , x k − ∇ f ( x k ) ) . This does not impact the analysis as x + k still satisﬁes the key inequality ( 1 ) . With this mod - iﬁcation , the algorithm does require β as part of the input , and we have observed that the algorithm performs better numerically . To aid in the analysis of the scheme , we record the following easy observation . Lemma 2 . 2 . Suppose that Q = ¯ v + α 2 (cid:107) · − ¯ c (cid:107) 2 is the optimal averaging of the quadratics Q A = v A + α 2 (cid:107) · − x A (cid:107) 2 and Q B = v B + α 2 (cid:107) · − x B (cid:107) 2 . Then the quantity ¯ v is nondecreasing in both v A and v B . Moreover , whenever the inequality | v A − v B | ≤ α 2 (cid:107) x A − x B (cid:107) 2 holds , we have ¯ v = α 8 (cid:107) x A − x B (cid:107) 2 + 1 2 ( v A + v B ) + 1 2 α (cid:18) v A − v B (cid:107) x A − x B (cid:107) (cid:19) 2 . Proof . Deﬁne ˆ λ : = 12 + v A − v B α (cid:107) x A − x B (cid:107) 2 . Notice that we have ˆ λ ∈ [ 0 , 1 ] if and only if | v A − v B | ≤ α 2 (cid:107) x A − x B (cid:107) 2 . If ˆ λ lies in [ 0 , 1 ] , equality ¯ λ = ˆ λ holds , and then from ( 3 ) we deduce ¯ v = v ¯ λ = α 8 (cid:107) x A − x B (cid:107) 2 + 1 2 ( v A + v B ) + 1 2 α (cid:18) v A − v B (cid:107) x A − x B (cid:107) (cid:19) 2 . If ˆ λ does not lie in [ 0 , 1 ] , then an easy argument shows that ¯ v is linear in v A either with slope one or zero . If ˆ λ lies in ( 0 , 1 ) , then we compute ∂ ¯ v ∂v A = 1 2 + 1 α (cid:107) x A − x B (cid:107) 2 ( v A − v B ) , which is nonnegative because | v A − v B | α (cid:107) x A − x B (cid:107) 2 ≤ 12 . Since ¯ v is clearly continuous , it follows that ¯ v is nondecreasing in v A , and by symmetry also in v B . We now show that Algorithm 1 achieves the optimal linear rate of convergence . 4 Theorem 2 . 3 ( Convergence of optimal quadratic averaging ) . In Algorithm 1 , for every index k ≥ 0 , the inequalities v k ≤ f ∗ ≤ f ( x + k ) hold and we have f ( x + k ) − v k ≤ (cid:18) 1 − 1 √ κ (cid:19) k ( f ( x + 0 ) − v 0 ) . Proof . Since in each iteration , the algorithm only averages quadratic minorants of f , the inequalities v k ≤ f ∗ ≤ f ( x + k ) hold for every index k . Set r 0 = 2 α ( f ( x + 0 ) − v 0 ) and deﬁne the quantities r k : = (cid:16) 1 − 1 √ κ (cid:17) k r 0 . We will show by induction that the inequality v k ≥ f ( x + k ) − α 2 r k holds for all k ≥ 0 . The base case k = 0 is immediate , and so assume we have v k − 1 ≥ f ( x + k − 1 ) − α 2 r k − 1 for some index k − 1 . Next set v A ( x ) : = f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α and v B : = v k − 1 . Then the function Q k ( x ) = v k + α 2 (cid:107) x − c k (cid:107) 2 , is the optimal averaging of Q A ( x ) = v A + α 2 (cid:13)(cid:13) x − x + + k (cid:13)(cid:13) 2 and Q B ( x ) = v B + α 2 (cid:107) x − c k − 1 (cid:107) 2 . An application of ( 1 ) yields the lower bound ˆ v A on v A : v A = f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α ≥ f ( x + k ) − α 2 (cid:107)∇ f ( x k ) (cid:107) 2 α 2 (cid:18) 1 − 1 κ (cid:19) : = ˆ v A . The induction hypothesis and the choice of x k yield a lower bound ˆ v B on v B : v B ≥ f ( x + k − 1 ) − α 2 r k − 1 ≥ f ( x k ) − α 2 r k − 1 ≥ f ( x + k ) + 1 2 β (cid:107)∇ f ( x k ) (cid:107) 2 − α 2 r k − 1 = f ( x + k ) − α 2 (cid:18) r k − 1 − 1 α 2 κ (cid:107)∇ f ( x k ) (cid:107) 2 (cid:19) : = ˆ v B . Deﬁne the quantities d : = (cid:13)(cid:13) x + + k − c k − 1 (cid:13)(cid:13) and h : = (cid:107)∇ f ( x k ) (cid:107) α . We now split the proof into two cases . First assume h 2 ≤ r k − 1 2 . Then we deduce v k ≥ v A ≥ ˆ v A = f ( x + k ) − α 2 h 2 (cid:18) 1 − 1 κ (cid:19) ≥ f ( x + k ) − α 2 r k − 1 (cid:32) 1 − 1 κ 2 (cid:33) ≥ f ( x + k ) − α 2 r k − 1 (cid:18) 1 − 1 √ κ (cid:19) = f ( x + k ) − α 2 r k . Hence in this case , the proof is complete . 5 Next suppose h 2 > r k − 1 2 and let v + α 2 (cid:107)·− c (cid:107) 2 be the optimal average of the two quadratics ˆ v A + α 2 (cid:107) · − x + + k (cid:107) 2 and ˆ v B + α 2 (cid:107) · − c k − 1 (cid:107) 2 . By Lemma 2 . 2 , the inequality v k ≥ v holds . We claim that equality v = ˆ v B + α 8 ( d 2 + 2 α ( ˆ v A − ˆ v B ) ) 2 d 2 holds . ( 4 ) From Lemma 2 . 2 , it suﬃces to show 12 ≥ | ˆ v A − ˆ v B | αd 2 . Note the equality | ˆ v A − ˆ v B | αd 2 = | r k − 1 − h 2 | 2 d 2 . The choice of x k ensures the inequality h 2 ≤ d 2 . Thus we have h 2 − r k − 1 < h 2 ≤ d 2 . Finally , the assumption h 2 > r k − 1 2 implies r k − 1 − h 2 < r k − 1 2 < h 2 ≤ d 2 . Hence we can be sure that ( 4 ) holds . Plugging in ˆ v A and ˆ v B we conclude v = f ( x + k ) − α 2 (cid:18) r k − 1 − 1 κ h 2 − ( d 2 + r k − 1 − h 2 ) 2 4 d 2 (cid:19) . Hence the proof is complete once we show the inequality r k − 1 − 1 κh 2 − ( d 2 + r k − 1 − h 2 ) 2 4 d 2 ≤ (cid:18) 1 − 1 √ κ (cid:19) r k − 1 . After rearranging , our task simpliﬁes to showing the inequality r k − 1 √ κ ≤ h 2 √ κ + ( d 2 + r k − 1 − h 2 ) 2 4 d 2 . Minimizing the right - hand side over d satisfying h 2 < d 2 , we deduce h 2 √ κ + ( d 2 + r k − 1 − h 2 ) 2 4 d 2 ≥ h 2 √ κ + r 2 k − 1 4 h 2 . Minimizing the right - hand side over h satisfying h 2 ≥ r k − 1 2 yields h 2 √ κ + r 2 k − 1 4 h 2 ≥ r k − 1 √ κ . The proof is complete . It is instructive to compare optimal averaging ( Algorithm 1 ) with Nesterov’s optimal methods in [ 7 , 8 ] . For convenience , we record the optimal gradient method following [ 7 ] , in Algorithm 2 . Comparing Algorithms 1 and 2 , we see that • x k is some point on the line between c k − 1 and x + k − 1 , and • Q k is an average of the previous quadratic Q k − 1 and the strong convexity quadratic lower bound Q based at x k . As we discuss in Appendix A , we can modify Nesterov’s method so that like in optimal quadratic averaging , we set x k = line search (cid:0) c k − 1 , x + k − 1 (cid:1) in each iteration . After this change , only two diﬀerences remain between the schemes : 6 Algorithm 2 : General scheme of an optimal method [ Nesterov ] Input : Starting points x 0 and c 0 , strong convexity constant α > 0 , smoothness parameter β > 0 , and initial quadratic curvature γ 0 ≥ α . Output : Final quadratic Q K ( x ) = v K + γ K 2 (cid:107) x − c K (cid:107) 2 . Set Q 0 ( x ) = v 0 + γ 0 2 (cid:107) x − c 0 (cid:107) 2 , where v 0 = f ( x 0 ) − 12 β (cid:107)∇ f ( x 0 ) (cid:107) 2 ; for k = 1 , . . . , K do Compute averaging parameter λ k ∈ ( 0 , 1 ) from βλ 2 k = ( 1 − λ k ) γ k − 1 + λ k α ; Set γ k = ( 1 − λ k ) γ k − 1 + λ k α . ; Set x k = ( 1 − θ k ) c k − 1 + θ k x + k − 1 where θ k = γ k γ k − 1 + λ k α ; Set Q ( x ) = (cid:16) f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α (cid:17) + α 2 (cid:13)(cid:13) x − x + + k (cid:13)(cid:13) 2 ; Let c k be the minimizer of the quadratic Q k ( x ) = ( 1 − λ k ) Q k − 1 ( x ) + λ k Q ( x ) ; end / * If we set γ 0 = α , then we have γ k = α , λ k = 1 √ κ , and θ k = √ κ 1 + √ κ . * / • the initial quadratic Q 0 is diﬀerent , and • the averaging parameter is computed diﬀerently . These diﬀerences , however , are fundamental . In Algorithm 1 , the quadratic Q 0 lower bounds f and therefore optimal averaging makes sense ; in the accelerated gradient method , Q 0 does not lower bound f , and the idea of optimal averaging does not apply . 3 Optimal quadratic averaging with memory Each iteration of Algorithm 1 forms an optimal average of the current lower quadratic model with the one from the previous iteration ; that is , as stated the scheme has a memory size of one . We next show how the scheme easily adapts to maintaining limited memory , i . e . by averaging multiple quadratics in each iteration . We mention in passing that the authors of [ 3 ] left open the question of eﬃciently speeding up their geometric descent algorithm in practice . One approach of this ﬂavor has recently appeared in [ 2 , Section 4 ] . The optimal averaging viewpoint , developed here , provides a direct and satisfying alternative . Indeed , computing the optimal average of several quadratics is easy , and amounts to solving a small dimensional quadratic optimization problem . To see this , ﬁx t quadratics Q i ( x ) : = v i + α 2 (cid:107) x − c i (cid:107) 2 , with i ∈ { 1 , . . . , t } , and a weight vector λ in the t - dimensional simplex ∆ t : = (cid:8) x ∈ R t : (cid:80) ti = 1 x i = 1 , x ≥ 0 (cid:9) . The average quadratic Q λ ( x ) : = t (cid:88) i = 1 λ i Q i ( x ) maintains the same canonical form as each Q i . Proposition 3 . 1 . Deﬁne the matrix C = (cid:2) c 1 c 2 . . . c t (cid:3) and the vector v = (cid:2) v 1 v 2 . . . v t (cid:3) T . Then we have Q λ ( x ) = v λ + α 2 (cid:107) x − c λ (cid:107) 2 , 7 where c λ = Cλ and v λ = (cid:68) α 2 diag ( C T C ) + v , λ (cid:69) − α 2 (cid:107) Cλ (cid:107) 2 . Proof . The Hessian of Q λ is simply α 2 I , and therefore the quadratic Q λ ( x ) has the form v λ + α 2 (cid:107) x − c λ (cid:107) 2 for some v λ and c λ . Notice that c λ is the minimizer of Q λ , and by diﬀerentiating , we determine that c λ = (cid:80) ti = 1 λ i c i = Cλ . We then compute v λ = Q λ ( c λ ) = t (cid:88) i = 1 (cid:18) λ i v i + λ i α 2 (cid:107) Cλ − c i (cid:107) 2 (cid:19) = (cid:104) v , λ (cid:105) + α 2 t (cid:88) i = 1 λ i (cid:16) (cid:107) Cλ (cid:107) 2 − 2 (cid:104) Cλ , c i (cid:105) + (cid:107) c i (cid:107) 2 (cid:17) = (cid:104) v , λ (cid:105) + α 2 (cid:107) Cλ (cid:107) 2 − α (cid:42) Cλ , t (cid:88) i = 1 λ i c i (cid:43) + α 2 t (cid:88) i = 1 λ i (cid:107) c i (cid:107) 2 = (cid:68) α 2 diag (cid:0) C T C (cid:1) + v , λ (cid:69) − α 2 (cid:107) Cλ (cid:107) 2 . The proof is complete . Naturally , we deﬁne the optimal averaging of the quadratics Q i , with i ∈ { 1 , 2 , . . . , t } , to be Q ¯ λ , where ¯ λ is the maximizer of the concave quadratic v λ = (cid:68) α 2 diag (cid:0) C T C (cid:1) + v , λ (cid:69) − α 2 (cid:107) Cλ (cid:107) 2 over the simplex ∆ t . There is no closed form expression for ¯ λ , but one can quickly ﬁnd it by solving a quadratic program in t variables , for example by an active set method . Moreover , some thought shows that the matrix C T C can be eﬃciently updated if one of the centers changes ; we omit the details . We propose an optimal averaging scheme with memory in Algorithm 3 . As we see in Section 5 , the method performs well numerically . Moreover , the scheme enjoys the same convergence guarantees as Algorithm 1 ; that is , Theorem 2 . 3 applies to Algorithm 3 , with nearly the same proof ( which we omit ) . The reader may notice that Algorithm 3 shows some similarity to the classical Kelley’s method for minimizing nonsmooth convex functions [ 5 ] . In the simplest case of minimizing a smooth convex function f on R n , Kelley’s method iterates the following steps x k + 1 = argmin x f k ( x ) for the functions f k ( x ) : = max i = 1 , . . . , k { f ( x i ) + (cid:104)∇ f ( x i ) , x − x i (cid:105) } . 8 Algorithm 3 : Optimal Quadratic Averaging with Memory Input : Starting point x 0 , strong convexity constant α > 0 , and memory size t ≥ 1 . Output : Final quadratic Q K ( x ) = v K + α 2 (cid:107) x − c K (cid:107) 2 and x + K . Set Q 0 ( x ) = v 0 + α 2 (cid:107) x − c 0 (cid:107) 2 , where v 0 = f ( x 0 ) − (cid:107)∇ f ( x 0 ) (cid:107) 2 2 α and c 0 = x + + 0 ; for k = 1 , . . . , K do Set x k = line search (cid:0) c k − 1 , x + k − 1 (cid:1) ; Set M k ( x ) = f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α + α 2 (cid:107) x − c k (cid:107) 2 ; Let Q k ( x ) : = v k + α 2 (cid:107) x − c k (cid:107) 2 be the optimal averaging of the k + 1 quadratics Q k − 1 , M k , M k − 1 , . . . , M 1 if k ≤ t , or of the t + 1 quadratics Q k − 1 , M k , M k − 1 , . . . , M k − t + 1 if k ≥ t + 1 ; end In other words , the scheme iteratively minimizes the lower - models f k of f . Coming back to the optimal averaging viewpoint , suppose that Q ¯ λ is an optimal average of the lower - bounding quadratics Q i , for i = 1 , . . . , k . Then we may write v ¯ λ = max λ ∈ ∆ k min x (cid:88) i λ i Q i ( x ) = min x max λ ∈ ∆ k (cid:88) i λ i Q i ( x ) = min x (cid:18) max i = 1 , . . . , k Q i ( x ) (cid:19) Thus v ¯ λ is the minimal value of the now diﬀerent lower - model , max i = 1 , . . . , k Q i , of f . Kelly’s method is known to have poor numerical performance and convergence guarantees ( e . g . [ 7 , Section 3 . 3 . 2 ] ) , while Algorithm 3 achieves the optimal linear convergence rate . This disparity is of course based on the two key distinctions : ( 1 ) using quadratic lower - models coming from strong convexity instead of linear functions , and ( 2 ) maintaining two separate sequences c k ( centers ) and x k ( sources of lower model updates ) . 4 Connection to geometric descent Algorithm 1 is largely motivated by the geometric descent method introduced by Bubeck , Lee , and Singh [ 3 ] . In this section , we describe the close connection between the two schemes . 4 . 1 Suboptimal geometric descent method The basic idea of geometric descent is that for each point x ∈ R n , the strong convexity lower bound f ∗ ≥ q ( x ∗ ; x ) deﬁnes a ball containing x ∗ : x ∗ ∈ B (cid:32) x + + , (cid:107)∇ f ( x ) (cid:107) 2 α 2 − 2 α ( f ( x ) − f ∗ ) (cid:33) . In turn , taking into account ( 1 ) yields the guarantee x ∗ ∈ B (cid:32) x + + , (cid:18) 1 − 1 κ (cid:19) (cid:107)∇ f ( x ) (cid:107) 2 α 2 − 2 α (cid:0) f ( x + ) − f ∗ (cid:1)(cid:33) . ( 5 ) 9 A crude upper estimate of the radius above is obtained simply by ignoring the nonnegative term 2 α ( f ( x + ) − f ∗ ) . The suboptimal geometric descent method proceeds as follows . Suppose we have available some ball B (cid:0) c 0 , R 20 (cid:1) containing x ∗ . As discussed , the quadratic lower bound at the center c 0 , namely f ∗ ≥ q ( x ∗ , c 0 ) , yields another ball B (cid:16) c + + 0 , (cid:0) 1 − 1 κ (cid:1) (cid:107)∇ f ( c 0 ) (cid:107) 2 α 2 (cid:17) containing x ∗ . Geometrically it is clear that the intersection of these two balls must be signiﬁcantly smaller than either of the individual balls . The following lemma from [ 3 ] makes this observation precise ; see Figure 2 for an illustration . Lemma 4 . 1 ( Minimal enclosing ball of the intersection ) . Fix a center x ∈ R n , square radius R 2 > 0 , step h ∈ R n , and (cid:15) ∈ ( 0 , 1 ) . Then there exists a new center c ∈ R n with B (cid:0) x , R 2 (cid:1) ∩ B (cid:16) x + h , ( 1 − (cid:15) ) (cid:107) h (cid:107) 2 (cid:17) ⊂ B (cid:0) c , ( 1 − (cid:15) ) R 2 (cid:1) . An application of Lemma 4 . 1 yields a new center c 1 with B (cid:0) c 0 , R 20 (cid:1) ∩ B (cid:32) c + + 0 , (cid:18) 1 − 1 κ (cid:19) (cid:107)∇ f ( c 0 ) (cid:107) 2 α 2 (cid:33) ⊂ B (cid:18) c 1 , (cid:18) 1 − 1 κ (cid:19) R 20 (cid:19) . Repeating the procedure with the new ball B (cid:0) c 1 , (cid:0) 1 − 1 κ (cid:1) R 20 (cid:1) yields a sequence of centers c k satisfying (cid:107) c k − x ∗ (cid:107) 2 ≤ (cid:18) 1 − 1 κ (cid:19) k R 20 . We note that the centers c k and R 20 of the minimal enclosing balls in Lemma 4 . 1 are easy to compute ; see Algorithm 1 in [ 3 ] . x x + h c ( 1   ✏ ) k h k 2 Figure 2 : Minimal enclosing ball of the intersection . There is a very close connection between ﬁnding the minimal enclosing ball of the inter - section of two balls and of optimally averaging quadratics . To see this , consider again two quadratics f ( x ) ≥ Q A ( x ) : = v A + α 2 (cid:107) x − x A (cid:107) 2 and f ( x ) ≥ Q B ( x ) : = v B + α 2 (cid:107) x − x B (cid:107) 2 . 10 Let Q be the optimal average of Q A and Q B . Notice that since Q A , Q B , and Q lower bound f , the minimizer x ∗ of f is guaranteed to lie in the three balls : B (cid:0) x A , R 2 A (cid:1) where R 2 A = 2 α (cid:16) ˆ f − v A (cid:17) , B (cid:0) x B , R 2 B (cid:1) where R 2 B = 2 α (cid:16) ˆ f − v B (cid:17) , B (cid:0) ¯ c , R 2 (cid:1) where R 2 = 2 α (cid:16) ˆ f − ¯ v (cid:17) , where ˆ f is any upper bound on f ∗ . The following elementary fact is true . Proposition 4 . 2 ( Minimal enclosing ball and optimal averaging ) . The ball B (cid:0) ¯ c , R 2 (cid:1) is precisely the minimal enclosing ball of the intersection B (cid:0) x A , R 2 A (cid:1) ∩ B (cid:0) x B , R 2 B (cid:1) . Proof . Deﬁne the quantity ˆ λ = 12 + v A − v B α (cid:107) x A − x B (cid:107) 2 . If ˆ λ lies in the unit interval [ 0 , 1 ] , then a quick computation using Lemma 2 . 2 shows the expressions R 2 = R 2 B − (cid:16) (cid:107) x A − x B (cid:107) 2 + R 2 B − R 2 A (cid:17) 2 4 (cid:107) x A − x B (cid:107) 2 and ¯ c = ¯ λx A + ( 1 − ¯ λ ) x B = 1 2 ( x A + x B ) − R 2 A − R 2 B 2 (cid:107) x A − x B (cid:107) 2 ( x A − x B ) . Now observe ˆ λ < 0 if and only if (cid:107) x A − x B (cid:107) 2 < R 2 A − R 2 B ˆ λ ∈ [ 0 , 1 ] if and only if (cid:107) x A − x B (cid:107) 2 ≥ | R 2 A − R 2 B | , and ˆ λ > 1 if and only if (cid:107) x A − x B (cid:107) 2 < R 2 B − R 2 A . Comparing with the recipe [ 3 , Algorithm 1 ] for computing the minimal enclosing ball , we see that B (cid:0) ¯ c , R 2 (cid:1) is the minimal enclosing ball of the intersection B (cid:0) x A , R 2 A (cid:1) ∩ B (cid:0) x B , R 2 B (cid:1) . 4 . 2 Optimal geometric descent method To obtain an optimal method , the authors of [ 3 ] observe that the term 2 α ( f ( x + ) − f ∗ ) in the inclusion ( 5 ) cannot be ignored . Exploiting this term will require maintaining two sequences c k ( the centers of the balls ) and x k ( points for generating new balls ) . Suppose in iteration k , we know that x ∗ lies in the ball B (cid:18) c k , R 2 k − 2 α (cid:0) f ( x + k ) − f ∗ (cid:1)(cid:19) . Consider now an arbitrary point , denoted suggestively by x k + 1 . Then ( 5 ) implies the inclusion x ∗ ∈ B (cid:32) x + + k + 1 , (cid:18) 1 − 1 κ (cid:19) (cid:107)∇ f ( x k + 1 ) (cid:107) 2 α 2 − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:33) . ( 6 ) 11 If we choose x k + 1 to satisfy f ( x k + 1 ) ≤ f ( x + k ) and apply inequality ( 1 ) with x = x k + 1 , we can get a new upper estimate of the initial ball , x ∗ ∈ B (cid:32) c k , R 2 k − 1 κ (cid:107)∇ f ( x k + 1 ) (cid:107) 2 α 2 − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:33) . ( 7 ) It seems clear that if the centers c k and x + + k + 1 of the two balls in ( 6 ) and ( 7 ) are “suﬃciently far apart” , then their intersection is contained in an even smaller ball . This is the content of following lemma from [ 3 ] . Lemma 4 . 3 ( Two balls shrinking ) . Fix centers x A , x B ∈ R n and square radii r 2 A , r 2 B > 0 . Also ﬁx (cid:15) ∈ ( 0 , 1 ) and suppose (cid:107) x A − x B (cid:107) 2 ≥ r 2 B . Then there exists a new center c ∈ R n such that for any δ > 0 , we have B (cid:0) x A , r 2 A − (cid:15)r 2 B − δ (cid:1) ∩ B (cid:0) x B , ( 1 − (cid:15) ) r 2 B − δ (cid:1) ⊂ B (cid:0) c , ( 1 − √ (cid:15) ) r 2 A − δ (cid:1) . A quick application of this result shows that provided the estimate (cid:13)(cid:13) x + + k + 1 − c k (cid:13)(cid:13) 2 ≥ (cid:107)∇ f ( x k + 1 ) (cid:107) 2 α 2 ( 8 ) holds , there exists a new center c k + 1 with x ∗ ∈ B (cid:18) c k + 1 , (cid:18) 1 − 1 √ κ (cid:19) R 2 k − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:19) . One way to ensure that x k + 1 satisﬁes the two key conditions , f ( x k + 1 ) ≤ f ( x + k ) and inequality ( 8 ) , is to simply let x k + 1 be the minimizer of f along the line between c k and x + k . Trivially this guarantees the inequality f ( x k + 1 ) ≤ f ( x + k ) , while the univariate optimality condition ∇ f ( x k + 1 ) ⊥ ( c k − x k + 1 ) ensures that ( 8 ) holds . This is exactly the motivation for the line - search procedure in Algorithm 1 . Repeating the process yields iterates c k that satisfy the optimal linear rate of convergence (cid:107) c k − x ∗ (cid:107) 2 ≤ (cid:18) 1 − 1 √ κ (cid:19) k R 20 . The precise method is described in Algorithm 4 . 5 Numerical examples In this section , we numerically illustrate optimality gap convergence in Algorithm 1 , and explore how Algorithm 3 , the variant of Algorithm 1 with memory , aids performance . To this end , we focus on minimizing two functions : the regularized logistic loss function L ( w ) : = 1 N N (cid:88) i = 1 log (cid:16) 1 + e − y i w T x i (cid:17) + α 2 (cid:107) w (cid:107) 2 , where x i ∈ R n and y i ∈ { ± 1 } are labeled training data , and the Rosenbrock function f ( x ) = B 2 (cid:32) ( 1 − x 1 ) 2 + n − 1 (cid:88) i = 1 ( x i − x i + 1 ) 2 + x 2 n (cid:33) + 1 2 n (cid:88) i = 1 x 2 i . For the logistic regression examples , we use the LIBSVM [ 4 ] data sets a1a ( N = 1605 , n = 123 ) and colon - cancer ( N = 62 , n = 2000 ) . 12 Algorithm 4 : Geometric Descent Method [ Bubeck , Lee , Singh ] Input : Starting point x 0 , strong convexity constant α > 0 . Output : x + K Set c 0 = x + + 0 and R 20 = (cid:107)∇ f ( x 0 ) (cid:107) 2 α 2 − 2 α (cid:0) f ( x 0 ) − f ( x + 0 ) (cid:1) ; for k = 1 , . . . , K do Set x k = line search (cid:0) x + k − 1 , c k − 1 (cid:1) ; Set x A = x k − α − 1 ∇ f ( x k ) and R 2 A = (cid:107)∇ f ( x k ) (cid:107) 2 α 2 − 2 α (cid:0) f ( x k ) − f ( x + k ) (cid:1) ; Set x B = c k − 1 and R 2 B = R 2 k − 1 − 2 α (cid:0) f ( x + k − 1 ) − f ( x + k ) (cid:1) ; Let B (cid:0) c k , R 2 k (cid:1) be the smallest enclosing ball of B (cid:0) x A , R 2 A (cid:1) ∩ B (cid:0) x B , R 2 B (cid:1) ; end 5 . 1 Optimality gap convergence From inequality ( 2 ) , we get the well - known optimality gap estimate for strongly convex functions f ( x ) − f ∗ ≤ (cid:107)∇ f ( x ) (cid:107) 2 2 α . ( 9 ) How does this estimate compare with the gaps g k : = f ( x + k ) − v k generated by Algorithm 1 ? Obviously the answer depends on the point where we evaluate the gap estimate in ( 9 ) . Nonetheless , we can say that the gaps g k are tighter than the gaps G k : = (cid:107)∇ f ( x k ) (cid:107) 2 2 α . Indeed , by the deﬁnition of v k , we trivially have v k ≥ f ( x k ) − G k and thus g k = f ( x + k ) − v k ≤ f ( x k ) − v k ≤ G k . On a relative scale , the diﬀerence between g k and G k is striking ; see Figure 3 . Notice that G k is an optimality gap estimate before averaging , and g k is an optimality gap estimate after averaging ; the plots in Figure 3 show that optimal quadratic averaging makes great relative progress per iteration . In Figure 4 , we plot g k , the true gaps f ( x + k ) − f ∗ , and the gap estimate in ( 9 ) at x k , x + k , and c k for the Rosenbrock function and the logistic loss function . The true gaps are the tightest , albeit unknown at runtime . Surprisingly , the gaps (cid:107)∇ f ( c k ) (cid:107) 2 2 α are quite bad : several orders of magnitude larger than g k . So even though the centers c k may appear to be the focal points of the algorithm , the points x + k are the ones to monitor in practice . Finally we note that the gaps g k and (cid:107) ∇ f ( x + k ) (cid:107) 2 2 α are comparable , even though g k does not rely on gradient information at x + k . 5 . 2 Optimal quadratic averaging with memory To demonstrate the eﬀectiveness of optimal quadratic averaging with memory , we use it to minimize the logistic loss ( see Figure 5 ) . The speedup over the memoryless method is signiﬁcant , even when taking into account the extra work per iteration needed to solve the small dimensional quadratic subproblems . In Figure 6 , we compare Algorithm 3 with L - BFGS . The two schemes are on par with each other , and neither is better than the other in all cases . L - BFGS with memory size m actually stores m pairs of vectors , whereas Algorithm 3 13 Figure 3 : Relative diﬀerences in gaps G k − g k G k on the Rosenbrock function ( B = 10 6 , n = 200 ) , and on the logistic loss on the colon - cancer data set with regularization α = 0 . 0001 . Figure 4 : Comparison of various optimality gaps on the Rosenbrock function ( B = 10 6 , n = 200 ) , and on the logistic loss on the a1a data set with regularization α = 0 . 0001 . with memory size t only stores t vectors . Thus it is perhaps fairer to compare L - BFGS with memory size m to Algorithm 3 with memory size t = 2 m ( see Figure 7 ) . Empirically , we’ve noticed that the small dimensional quadratic program in Algorithm 3 must be solved to high accuracy , especially on poorly conditioned problems . In Figure 8 , we again compare L - BFGS and Algorithm 3 on logisitic regression , but with less regularization . 14 Figure 5 : Algorithm 3 with various memory sizes t . The case t = 1 corresponds to the memoryless optimal averaging method in Algorithm 1 . The task is logistic regression , with regularization α = 0 . 0001 , on data sets a1a and colon - cancer . Figure 6 : Algorithm 3 with memory size t versus L - BFGS with memory size m . The task is logistic regression , with regularization α = 0 . 0001 , on data sets a1a and colon - cancer . 6 Comments on proximal extensions It is natural to try to extend geometric descent and optimal quadratic averaging to a proximal setting . For the sake of concreteness , let us focus on geometric descent . We can easily extend the suboptimal version of the algorithm to the proximal setting , but some diﬃculties arise when accelerating the method . Suppose we are interested in solving the problem min x f ( x ) : = g ( x ) + h ( x ) , 15 Figure 7 : A fairer ( equal memory ) comparison of Algorithm 3 and L - BFGS . The task is still logistic regression , with regularization α = 0 . 0001 , on data sets a1a and colon - cancer . We focus on lower accuracy than we did in Figure 6 . where g : R n → R is β - smooth and α - strongly convex , and h : R n → R ∪ { + ∞ } is closed , convex , and is such that the proximal mapping prox th ( x ) : = argmin z { h ( z ) + 1 2 t (cid:107) z − x (cid:107) 2 } is easily computable . In the analysis of ﬁrst - order methods for such problems , the gradient mapping G t ( x ) : = 1 t ( x − prox th ( x − t ∇ g ( x ) ) ) plays the role of the usual gradient . The following is a standard estimate ; see for example [ 7 , Section 2 . 2 . 3 ] . We provide a proof for completeness . Lemma 6 . 1 . Fix a step length t > 0 and deﬁne a proximal gradient step x + : = x − tG t ( x ) . Then for every y ∈ R n the inequality holds : f ( y ) ≥ f ( x + ) + (cid:104) G t ( x ) , y − x (cid:105) + t (cid:18) 1 − βt 2 (cid:19) (cid:107) G t ( x ) (cid:107) 2 + α 2 (cid:107) y − x (cid:107) 2 . Proof . Appealing to β - smoothness of g , we deduce f ( x + ) ≤ g ( x ) − t (cid:104)∇ g ( x ) , G t ( x ) (cid:105) + βt 2 2 (cid:107) G t ( x ) (cid:107) 2 + h ( x + ) . Furthermore , strong convexity of g implies f ( x + ) ≤ g ( y ) + (cid:10) ∇ g ( x ) , x + − y (cid:11) − α 2 (cid:107) y − x (cid:107) 2 + βt 2 2 (cid:107) G t ( x ) (cid:107) 2 + h ( x + ) . Finally , using the observation that G t ( x ) − ∇ g ( x ) belongs to ∂h ( x + ) , we have f ( x + ) ≤ f ( y ) + (cid:10) G t ( x ) , x + − y (cid:11) − α 2 (cid:107) y − x (cid:107) 2 + βt 2 2 (cid:107) G t ( x ) (cid:107) 2 . Rearrangement completes the proof . 16 Figure 8 : Algorithm 3 with memory size t versus L - BFGS with memory size m . The task is logistic regression on data sets a1a and colon - cancer , with α = 10 − 6 ( top row ) and α = 10 − 8 ( bottom row ) . If we let y = x ∗ in Lemma 6 . 1 and rearrange we get x ∗ ∈ B (cid:18) x − 1 αG t ( x ) , (cid:18) 1 α 2 − 2 αt + β αt 2 (cid:19) (cid:107) G t ( x ) (cid:107) 2 − 2 α (cid:0) f ( x + ) − f ∗ (cid:1)(cid:19) . How should we choose the step length t ? A simple approach is to choose t to minimize the quantity 1 α 2 − 2 α t + βα t 2 , i . e . , set t = 1 β . With this choice of t , we deduce the inclusion x ∗ ∈ B (cid:32) x + + , (cid:18) 1 − 1 κ (cid:19) (cid:13)(cid:13) G 1 / β ( x ) (cid:13)(cid:13) 2 α 2 − 2 α (cid:0) f ( x + ) − f ∗ (cid:1)(cid:33) , where x + + = x − 1 α G 1 / β ( x ) is a long step and x + = x − 1 β G 1 / β ( x ) is a short step . A proximal version of the suboptimal geometric descent follows easily from Lemma 4 . 1 . 17 To accelerate the proximal geometric descent algorithm we assume in iteration k that x ∗ lies in some ball B (cid:18) c k , R 2 k − 2 α ( f ( y k ) − f ∗ ) (cid:19) . We then consider a second minimizer enclosing ball derived from information at some point x k + 1 : x ∗ ∈ B (cid:32) x + + k + 1 , (cid:18) 1 − 1 κ (cid:19) (cid:13)(cid:13) G 1 / β ( x k + 1 ) (cid:13)(cid:13) 2 α 2 − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:33) . Following the same pattern as in Section 4 . 2 , if we choose x k + 1 to satisfy f ( x k + 1 ) ≤ f ( y k ) and appeal to the smoothness inequality f ( x + k + 1 ) ≤ f ( x k + 1 ) − 12 β (cid:13)(cid:13) G 1 / β ( x k + 1 ) (cid:13)(cid:13) 2 , we deduce the inclusion x ∗ ∈ B (cid:32) c k , R 2 k − 1 κ (cid:13)(cid:13) G 1 / β ( x k + 1 ) (cid:13)(cid:13) 2 α 2 − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:33) . By Lemma 4 . 3 there is a new center c k + 1 with x ∗ ∈ B (cid:18) c k + 1 , (cid:18) 1 − 1 √ k (cid:19) R 2 k − 2 α (cid:0) f ( x + k + 1 ) − f ∗ (cid:1)(cid:19) , provided the old centers x + + k + 1 and c k are far apart ; speciﬁcally , we must be sure that the inequality (cid:13)(cid:13) x + + k + 1 − c k (cid:13)(cid:13) 2 ≥ (cid:13)(cid:13) G 1 / β ( x k + 1 ) (cid:13)(cid:13) 2 α 2 holds . How do we choose x k + 1 to satisfy both f ( x k + 1 ) ≤ f ( y k ) and (cid:13)(cid:13) x + + k + 1 − c k (cid:13)(cid:13) 2 ≥ (cid:107) G 1 / β ( x k + 1 ) (cid:107) 2 α 2 ? The desired x k + 1 does exist ; for example , x k + 1 = x ∗ is such a point . In the proximal setting , it is not clear how to choose x k + 1 to ensure these two inequalities ( even for speciﬁc problem classes ) . This is an interesting topic for future research . References [ 1 ] H . Attouch , J . Peypouquet , and P . Redont . On the fast convergence of an inertial gradient - like dynamics with vanishing viscosity . Preprint , arXiv : 1507 . 04782 , 2015 . [ 2 ] S . Bubeck and Y . T . Lee . Black - box optimization with a politician . Preprint , arXiv : 1602 . 04847 , 2016 . [ 3 ] S . Bubeck , Y . T . Lee , and M . Singh . A geometric alternative to nesterov’s accelerated gradient descent . Preprint , arXiv : 1506 . 08187 , 2015 . [ 4 ] C . - C . Chang and C . - J . Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2 : 27 : 1 – 27 : 27 , 2011 . [ 5 ] J . E . Kelley , Jr . The cutting - plane method for solving convex programs . J . Soc . Indust . Appl . Math . , 8 : 703 – 712 , 1960 . 18 [ 6 ] L . Lessard , B . Recht , and A . Packard . Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints . SIAM J . Optim . , 26 ( 1 ) : 57 – 95 , 2016 . [ 7 ] Y . Nesterov . Introductory lectures on convex optimization , volume 87 of Applied Opti - mization . Kluwer Academic Publishers , Boston , MA , 2004 . A basic course . [ 8 ] Yu . E . Nesterov . A method for solving the convex programming problem with convergence rate O ( 1 / k 2 ) . Dokl . Akad . Nauk SSSR , 269 ( 3 ) : 543 – 547 , 1983 . [ 9 ] W . Su , S . Boyd , and E . Candes . A diﬀerential equation for modeling nesterovs acceler - ated gradient method : Theory and insights . In Z . Ghahramani , M . Welling , C . Cortes , N . d . Lawrence , and K . q . Weinberger , editors , Advances in Neural Information Processing Systems 27 , pages 2510 – 2518 . Curran Associates , Inc . , 2014 . A Exact line search in accelerated gradient descent Nesterov’s method is based on an estimate sequence ; that is , a sequence of functions Q k and nonnegative numbers Λ k with Λ k → 0 and Q k ( x ) ≤ ( 1 − Λ k ) f ( x ) + Λ k Q 0 ( x ) . Estimate sequences are useful because if y k satisﬁes f ( y k ) ≤ v k : = min x ∈ R n Q k ( x ) , then f ( y k ) − f ∗ ≤ Λ k ( Q 0 ( x ∗ ) − f ∗ ) ; that is , f ( y k ) approaches f ∗ with error proportional to Λ k , see [ 7 ] . The quadratics in Algorithm 2 ( with appropriately chosen Λ k ) form an estimate sequence . To explain , for k ≥ 1 , pick vectors x k and numbers λ k ∈ ( δ , 1 ) with δ > 0 . Next , recursively deﬁne Q 0 ( x ) = v 0 + γ 0 2 (cid:107) x − c 0 (cid:107) 2 and Q k ( x ) = ( 1 − λ k ) Q k − 1 ( x ) + λ k (cid:32) f ( x k ) − (cid:107)∇ f ( x k ) (cid:107) 2 2 α + α 2 (cid:13)(cid:13) x − x + + k (cid:13)(cid:13) 2 (cid:33) . Then the quadratics Q k and numbers Λ k = (cid:81) kj = 1 ( 1 − λ j ) are an estimate sequence for f . Nesterov’s method is designed to ensure the inequality f ( x + k ) ≤ v k with the added optimal rate condition λ k ≥ (cid:113) αβ . The scheme in Algorithm 2 with x k = line search (cid:0) c k − 1 , x + k − 1 (cid:1) also guarantees these conditions . Trivially we have f ( x + 0 ) ≤ v 0 . Assume , for induction , that we have f ( x + k − 1 ) ≤ v k − 1 . From [ 7 , Lemma 2 . 2 . 3 ] , we know v k = ( 1 − λ k ) v k − 1 + λ k f ( x k ) − λ 2 k 2 γ k (cid:107)∇ f ( x k ) (cid:107) 2 + + λ k ( 1 − λ k ) γ k − 1 γ k (cid:16) α 2 (cid:107) x k − c k − 1 (cid:107) 2 + (cid:104)∇ f ( x k ) , c k − 1 − x k (cid:105) (cid:17) . 19 Since x k = line search (cid:0) c k − 1 , x + k − 1 (cid:1) , we have f ( x k ) ≤ f ( x + k − 1 ) ≤ v k − 1 and (cid:104)∇ f ( x k ) , c k − 1 − x k (cid:105) = 0 , and therefore v k ≥ f ( x k ) − λ 2 k 2 γ k (cid:107)∇ f ( x k ) (cid:107) 2 = f ( x k ) − 1 2 β (cid:107)∇ f ( x k ) (cid:107) 2 ≥ f ( x + k ) . Provided we set γ 0 ≥ α , we get the optimal rate condition λ k = (cid:113) γ k β ≥ (cid:113) αβ . 20