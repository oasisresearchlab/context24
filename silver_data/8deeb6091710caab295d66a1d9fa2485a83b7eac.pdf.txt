Probabilistic Topic Models David M . Blei Department of Computer Science Princeton University September 26 , 2013 Probabilistic topic models As more information becomes available , it becomes more difﬁcult to ﬁnd and discover what we need . We need new tools to help us organize , search , and understand these vast amounts of information . Probabilistic topic models Topic modeling provides methods for automatically organizing , understanding , searching , and summarizing large electronic archives . 1 Discover the hidden themes that pervade the collection . 2 Annotate the documents according to those themes . 3 Use annotations to organize , summarize , search , form predictions . Probabilistic topic models “Genetics” “Evolution” “Disease” “Computers” human evolution disease computer genome evolutionary host models dna species bacteria information genetic organisms diseases data genes life resistance computers sequence origin bacterial system gene biology new network molecular groups strains systems sequencing phylogenetic control model map living infectious parallel information diversity malaria methods genetics group parasite networks mapping new parasites software project two united new sequences common tuberculosis simulations Probabilistic topic models 1880 1900 1920 1940 1960 1980 2000 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o 1880 1900 1920 1940 1960 1980 2000 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o RELATIVITY LASER FORCE NERVE OXYGEN NEURON " Theoretical Physics " " Neuroscience " Probabilistic topic models wild type mutantmutationsmutants mutation plantsplantgenegenes arabidopsis p53 cell cycle activitycyclinregulation amino acids cdna sequenceisolatedprotein genediseasemutationsfamiliesmutation rnadna rna polymerasecleavagesite cellscell expressioncell lines bone marrow united states women universitiesstudentseducation sciencescientistssaysresearchpeople researchfundingsupportnihprogram surfacetipimagesampledevice laseropticallightelectronsquantum materialsorganicpolymerpolymersmolecules volcanicdepositsmagma eruptionvolcanism mantlecrust upper mantle meteoritesratios earthquake earthquakesfaultimagesdata ancientfoundimpact million years ago africa climateoceanicechanges climate change cellsproteins researchersproteinfound patientsdiseasetreatmentdrugsclinical geneticpopulationpopulationsdifferencesvariation fossil recordbirdsfossilsdinosaursfossil sequencesequencesgenomednasequencing bacteriabacterialhostresistanceparasite developmentembryosdrosophilagenesexpression speciesforestforestspopulationsecosystems synapsesltpglutamatesynapticneurons neuronsstimulusmotorvisualcortical ozone atmosphericmeasurementsstratosphere concentrations sun solar wind earthplanetsplanet co2 carbon carbon dioxide methanewater receptorreceptorsligandligandsapoptosis proteinsproteinbindingdomaindomains activated tyrosine phosphorylationactivation phosphorylationkinase magneticmagnetic ﬁeld spin superconductivitysuperconducting physicistsparticlesphysicsparticleexperiment surfaceliquidsurfacesﬂuidmodel reactionreactions molecule molecules transition state enzymeenzymesironactive site reduction pressure high pressure pressurescoreinner core brainmemorysubjectslefttask computerprobleminformationcomputersproblems stars astronomersuniversegalaxiesgalaxy virushivaids infectionviruses miceantigent cells antigens immune response Probabilistic topic models the , of a , is and n algorithmtimelogbound n functionspolynomiallogalgorithm logic programs systemslanguagesets systemsystems performanceanalysis distributed graphgraphs edge minimumvertices proofpropertyprogramresolutionabstract consensusobjectsmessagesprotocolasynchronous networksqueuingasymptoticproductformserver approximationspointsdistanceconvex databaseconstraintsalgebrabooleanrelational formulasﬁrstorderdecisiontemporal queries treesregulartreesearch compression machinedomaindegreedegreespolynomials routingadaptivenetworknetworksprotocols networksprotocolnetworkpacketslink databasetransactionsretrievalconcurrencyrestrictions learninglearnablestatisticalexamplesclasses m mergingnetworkssortingmultiplication constraintdependencieslocalconsistency tractable logiclogicsquerytheorieslanguages quantumautomatancautomatonlanguages online schedulingtaskcompetitivetasks learningknowledgereasoningveriﬁcationcircuit An optimal algorithm for intersecting line segments in the plane Recontamination does not help to search a graph A new approach to the maximum - flow problem The time complexity of maximum matching by simulated annealing Quantum lower bounds by polynomials On the power of bounded concurrency I : finite automata Dense quantum coding and quantum finite automata Classical physics and the Church - - Turing Thesis Nearly optimal algorithms and bounds for multilayer channel routing How bad is selfish routing ? Authoritative sources in a hyperlinked environment Balanced sequences and optimal routing Single - class bounds of multi - class queuing networks The maximum concurrent flow problem Contention in shared memory algorithms Linear probing with a nonuniform address distribution Magic Functions : In Memoriam : Bernard M . Dwork 1923 - - 1998 A mechanical proof of the Church - Rosser theorem Timed regular expressions On the power and limitations of strictness analysis Module algebra On XML integrity constraints in the presence of DTDs Closure properties of constraints Dynamic functional dependencies and database aging Probabilistic topic models Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 SKY WATER TREE MOUNTAIN PEOPLE Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 SCOTLAND WATER FLOWER HILLS TREE Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 SKY WATER BUILDING PEOPLE WATER Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 FISH WATER OCEAN TREE CORAL Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 PEOPLE MARKET PATTERN TEXTILE DISPLAY Automatic image annotation birds nest leaves branch tree predicted caption : predicted caption : people market pattern textile display sky water tree mountain people predicted caption : fish water ocean tree coral sky water buildings people mountain predicted caption : predicted caption : predicted caption : scotland water flowers hills tree Probabilisticmodelsoftextandimages – p . 5 / 53 BIRDS NEST TREE BRANCH LEAVES Probabilistic topic models Year W e i gh t e d I n f l uen c e 0 . 000 0 . 005 0 . 010 0 . 015 0 . 020 0 . 025 0 . 030 1880 1900 1920 1940 1960 1980 2000 Jared M . Diamond , Distributional Ecology of New Guinea Birds . Science ( 1973 ) [ 296 citations ] W . B . Scott , The Isthmus of Panama in Its Relation to the Animal Life of North and South America , Science ( 1916 ) [ 3 citations ] William K . Gregory , The New Anthropogeny : Twenty - Five Stages of Vertebrate Evolution , from Silurian Chordate to Man , Science ( 1933 ) [ 3 citations ] Derek E . Wildman et al . , Implications of Natural Selection in Shaping 99 . 4 % Nonsynonymous DNA Identity between Humans and Chimpanzees : Enlarging Genus Homo , PNAS ( 2003 ) [ 178 citations ] Probabilistic topic models 16 J . CHANG AND D . BLEI Table 2 Top eight link predictions made by RTM ( ψ e ) and LDA + Regression for two documents ( italicized ) from Cora . The models were ﬁt with 10 topics . Boldfaced titles indicate actual documents cited by or citing each document . Over the whole corpus , RTM improves precision over LDA + Regression by 80 % when evaluated on the ﬁrst 20 documents retrieved . Markov chain Monte Carlo convergence diagnostics : A comparative review Minorization conditions and convergence rates for Markov chain Monte Carlo R T M ( ψ e ) Rates of convergence of the Hastings and Metropolis algorithms Possible biases induced by MCMC convergence diagnostics Bounding convergence time of the Gibbs sampler in Bayesian image restoration Self regenerative Markov chain Monte Carlo Auxiliary variable methods for Markov chain Monte Carlo with applications Rate of Convergence of the Gibbs Sampler by Gaussian Approximation Diagnosing convergence of Markov chain Monte Carlo algorithms Exact Bound for the Convergence of Metropolis Chains L D A + R e g r e ss i o n Self regenerative Markov chain Monte Carlo Minorization conditions and convergence rates for Markov chain Monte Carlo Gibbs - markov models Auxiliary variable methods for Markov chain Monte Carlo with applications Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Models Mediating instrumental variables A qualitative framework for probabilistic inference Adaptation for Self Regenerative MCMC Competitive environments evolve better solutions for complex tasks Coevolving High Level Representations R T M ( ψ e ) A Survey of Evolutionary Strategies Genetic Algorithms in Search , Optimization and Machine Learning Strongly typed genetic programming in evolving cooperation strategies Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . Evolutionary Module Acquisition An Empirical Investigation of Multi - Parent Recombination Operators . . . A New Algorithm for DNA Sequence Assembly L D A + R e g r e ss i o n Identiﬁcation of protein coding regions in genomic DNA Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . A genetic algorithm for passive management The Performance of a Genetic Algorithm on a Chaotic Objective Function Adaptive global optimization with local search Mutation rates as adaptations Table 2 illustrates suggested citations using RTM ( ψ e ) and LDA + Regres - sion as predictive models . These suggestions were computed from a model ﬁt on one of the folds of the Cora data . The top results illustrate suggested links for “Markov chain Monte Carlo convergence diagnostics : A comparative re - Probabilistic topic models dod , defense , defense and appropriation , military , subtitle veteran , veterans , bills , care , injury people , woman , american , nation , school producer , eligible , crop , farm , subparagraph coin , inspector , designee , automobile , lebanon bills , iran , official , company , sudan human , vietnam , united nations , call , people drug , pediatric , product , device , medical child , fire , attorney , internet , bills surveillance , director , court , electronic , flood energy , bills , price , commodity , market land , site , bills , interior , river child , center , poison , victim , abuse coast guard , vessel , space , administrator , requires science , director , technology , mathematics , bills computer , alien , bills , user , collection head , start , child , technology , award loss , crop , producer , agriculture , trade bills , tax , subparagraph , loss , taxable cover , bills , bridge , transaction , following transportation , rail , railroad , passenger , homeland security business , administrator , bills , business concern , loan defense , iraq , transfer , expense , chapter medicare , medicaid , child , chip , coverage student , loan , institution , lender , school energy , fuel , standard , administrator , lamp housing , mortgage , loan , family , recipient bank , transfer , requires , holding company , industrial county , eligible , ballot , election , jurisdiction tax credit , budget authority , energy , outlays , tax Probabilistic topic models Probabilistic topic models • What are topic models ? • What kinds of things can they do ? • How do I compute with a topic model ? • How do I evaluate and check a topic model ? • What are some unanswered questions in this ﬁeld ? • How can I learn more ? Probabilistic models • This is a case study in data analysis with probability models . • Our agenda is to teach about this kind of analysis through topic models . • Note : We are being “Bayesian” in this sense : “ [ By Bayesian inference , ] I simply mean the method of statistical inference that draws conclusions by calculating conditional distributions of unknown quantities given ( a ) known quantities and ( b ) model speciﬁcations . ” ( Rubin , 1984 ) • ( The Bayesian versus Frequentist debate is not relevant to this talk . ) Probabilistic models • Specifying models • Directed graphical models • Conjugate priors and nonconjugate priors • Time series modeling • Hierarchical methods • Mixed - membership models • Prediction from sparse and noisy inputs • Model selection and Bayesian nonparametric methods • Approximate posterior inference • MCMC • Variational inference • Using and evaluating models • Exploring , describing , summarizing , visualizing data • Evaluating model ﬁtness Probabilistic models Make assumptions Infer the posterior Explore Collect data Predict Check Organization of these lectures 1 Introduction to topic modeling : Latent Dirichlet allocation 2 Beyond latent Dirichlet allocation • Correlated and dynamic models • Supervised models • Modeling text and user data 3 Bayesian nonparametrics : A brief tutorial 4 Posterior computation • Scalable variational inference • Nonconjugate variational inference 5 Checking and evaluating models • Using the predictive distribution • Posterior predictive checks 6 Discussion , open questions , and resources Introduction to Topic Modeling Latent Dirichlet allocation ( LDA ) Simple intuition : Documents exhibit multiple topics . Latent Dirichlet allocation ( LDA ) gene 0 . 04 dna 0 . 02 genetic 0 . 01 . , , life 0 . 02 evolve 0 . 01 organism 0 . 01 . , , brain 0 . 04 neuron 0 . 02 nerve 0 . 01 . . . data 0 . 02 number 0 . 02 computer 0 . 01 . , , Topics Documents Topic proportions and assignments • Each topic is a distribution over words • Each document is a mixture of corpus - wide topics • Each word is drawn from one of those topics Latent Dirichlet allocation ( LDA ) Topics Documents Topic proportions and assignments • In reality , we only observe the documents • The other structure are hidden variables • Topic modeling algorithms infer these variables from data . Latent Dirichlet allocation ( LDA ) Topics Documents Topic proportions and assignments • Our goal is to infer the hidden variables • I . e . , compute their distribution conditioned on the documents p ( topics , proportions , assignments | documents ) LDA as a graphical model θ d Z d , n W d , n N D K β k α η Proportionsparameter Per - document topic proportions Per - word topic assignment Observedword Topics Topic parameter • Encodes assumptions • Deﬁnes a factorization of the joint distribution • Connects to algorithms for computing with data LDA as a graphical model θ d Z d , n W d , n N D K β k α η Proportionsparameter Per - document topic proportions Per - word topic assignment Observedword Topics Topic parameter • Nodes are random variables ; edges indicate dependence . • Shaded nodes are observed ; unshaded nodes are hidden . • Plates indicate replicated variables . LDA as a graphical model θ d Z d , n W d , n N D K β k α η Proportionsparameter Per - document topic proportions Per - word topic assignment Observedword Topics Topic parameter p ( β , θ , z , w ) = (cid:32) K (cid:89) i = 1 p ( β i | η ) (cid:33)(cid:32) D (cid:89) d = 1 p ( θ d | α ) N (cid:89) n = 1 p ( z d , n | θ d ) p ( w d , n | β 1 : K , z d , n ) (cid:33) LDA as a graphical model θ d Z d , n W d , n N D K β k α η • This joint deﬁnes a posterior , p ( θ , z , β | w ) . • From a collection of documents , infer • Per - word topic assignment z d , n • Per - document topic proportions θ d • Per - corpus topic distributions β k • Then use posterior expectations to perform the task at hand : information retrieval , document similarity , exploration , and others . LDA as a graphical model θ d Z d , n W d , n N D K β k α η Approximate posterior inference algorithms • Mean ﬁeld variational methods ( Blei et al . , 2001 , 2003 ) • Expectation propagation ( Minka and Lafferty , 2002 ) • Collapsed Gibbs sampling ( Grifﬁths and Steyvers , 2002 ) • Distributed sampling ( Newman et al . , 2008 ; Ahmed et al . , 2012 ) • Collapsed variational inference ( Teh et al . , 2006 ) • Online variational inference ( Hoffman et al . , 2010 ) • Factorization based inference ( Arora et al . , 2012 ; Anandkumar et al . , 2012 ) Example inference • Data : The OCR’ed collection of Science from 1990 – 2000 • 17K documents • 11M words • 20K unique terms ( stop words and rare words removed ) • Model : 100 - topic LDA model using variational inference . Example inference 1 8 16 26 36 46 56 66 76 86 96 Topics P r obab ili t y 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 Example inference “Genetics” “Evolution” “Disease” “Computers” human evolution disease computer genome evolutionary host models dna species bacteria information genetic organisms diseases data genes life resistance computers sequence origin bacterial system gene biology new network molecular groups strains systems sequencing phylogenetic control model map living infectious parallel information diversity malaria methods genetics group parasite networks mapping new parasites software project two united new sequences common tuberculosis simulations 1 analysis dna gene genes genetic genome human sequence sequences two 6 article card circle end letters news readers science service start 11 age ago early evidence fig million north record university years 16 aaas advertising associate fax manager member recruitment sales science washington 2 activation activity binding cell cells fig kinase protein proteins receptor 7 data different fig model number rate results system time two 12 biology evolution evolutionary genetic natural population populations species studies university 17 biology cell cells development expression fig gene genes mice mutant 3 atmosphere atmospheric carbon changes climate global ocean surface temperature water 8 chemical fig high materials molecular molecules structure surface temperature university 13 acid amino binding molecular protein proteins residues structural structure two 18 electron electrons energy high laser light magnetic physics quantum state 4 first just like new researchers says science university work years 9 binding dna protein proteins rna sequence sequences site specific transcription 14 antigen cell cells hiv human immune infected infection viral virus 19 health national new research science scientific scientists states united university 5 crust earth earthquakes earths high lower mantle pressure seismic temperature 10 cancer disease drug drugs gene human medical normal patients studies 15 astronomers earth mass observations solar space stars sun telescope university 20 activity brain cells channels cortex fig neuronal neurons university visual Aside : The Dirichlet distribution • The Dirichlet distribution is an exponential family distribution over the simplex , i . e . , positive vectors that sum to one p ( θ | (cid:126)α ) = Γ (cid:128)(cid:80) i α i (cid:138) (cid:81) i Γ ( α i ) (cid:89) i θ α i − 1 i . • It is conjugate to the multinomial . Given a multinomial observation , the posterior distribution of θ is a Dirichlet . • The parameter α controls the mean shape and sparsity of θ . • The topic proportions are a K dimensional Dirichlet . The topics are a V dimensional Dirichlet . α = 1 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 10 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 100 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 1 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 0 . 1 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 0 . 01 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 α = 0 . 001 item v a l u e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 l l l l l l l l l l 6 l l l l l l l l l l 11 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 2 l l l l l l l l l l 7 l l l l l l l l l l 12 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 3 l l l l l l l l l l 8 l l l l l l l l l l 13 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 4 l l l l l l l l l l 9 l l l l l l l l l l 14 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 5 l l l l l l l l l l 10 l l l l l l l l l l 15 l l l l l l l l l l 1 2 3 4 5 6 7 8 9 10 Why does LDA “work” ? • LDA trades off two goals . 1 For each document , allocate its words to as few topics as possible . 2 For each topic , assign high probability to as few terms as possible . • These goals are at odds . • Putting a document in a single topic makes # 2 hard : All of its words must have probability under that topic . • Putting very few words in each topic makes # 1 hard : To cover a document’s words , it must assign many topics to it . • Trading off these goals ﬁnds groups of tightly co - occurring words . LDA summary θ d Z d , n W d , n N D K β k α η • LDA is a probabilistic model of text . It casts the problem of discovering themes in large document collections as a posterior inference problem . • It lets us visualize the hidden thematic structure in large collections , and generalize new data to ﬁt into that structure . • Builds on latent semantic analysis ( Deerwester et al . , 1990 ; Hofmann , 1999 ) It is a mixed - membership model ( Erosheva , 2004 ) . It relates to PCA and matrix factorization ( Jakulin and Buntine , 2002 ) . Was independently invented for genetics ( Pritchard et al . , 2000 ) LDA summary by emerging groups . Both modalities are driven by the common goal of increasing data likelihood . Consider the voting example again ; resolutions that would have been as - signed the same topic in a model using words alone may beassignedtodiﬀerenttopicsiftheyexhibitdistinctvotingpatterns . Distinct word - based topics may be merged if the entities vote very similarly on them . Likewise , multiple dif - ferent divisions of entities into groups are made possible by conditioning them on the topics . Theimportanceofmodelingthe language associatedwith interactionsbetweenpeoplehasrecentlybeendemonstratedintheAuthor - Recipient - Topic ( ART ) model [ 16 ] . In ART the words in a message between people in a network are generated conditioned on the author , recipient and a set of topics that describes the message . The model thus cap - tures both the network structure within which the people interact as well as the language associated with the inter - actions . In experiments with Enron and academic email , the ART model is able to discover role similarity of people betterthanSNAmodelsthatconsidernetworkconnectivityalone . However , theARTmodel does notexplicitlycapture groups formed by entities in the network . The GT model simultaneously clusters entities to groups and clusters words into topics , unlike models that gener - atetopicssolelybasedonworddistributionssuchasLatentDirichletAllocation [ 4 ] . In this way the GT model discov - ers salient topics relevant to relationships between entities in the social network—topics which the models that only examine words are unable to detect . We demonstrate the capabilities of the GT model by ap - plying it to two large sets of voting data : one from US Sen - ate and the other from the General Assembly of the UN . The model clusters voting entities into coalitions and si - multaneouslydiscoverstopicsforwordattributesdescribing the relations ( bills or resolutions ) between entities . We ﬁnd that the groups obtained from the GT model are signiﬁ - cantly more cohesive ( p - value < . 01 ) than those obtained from the Blockstructures model . The GT model also dis - coversnewandmoresalienttopicsinboththeUNandSen - ate datasets—in comparison with topics discovered by only examining the words of the resolutions , the GT topics are either split or joined together as inﬂuenced by the voters’ patterns of behavior . 2 . GROUP - TOPIC MODEL TheGroup - TopicModelisadirectedgraphicalmodelthat clusters entities with relations between them , as well as at - tributes of those relations . The relations may be either di - rected or undirected and have multiple attributes . In this paper , we focus on undirected relations and have words as the attributes on relations . In the generative process for each event ( an interaction between entities ) , the model ﬁrst picks the topic t of the event and then generates all the words describing the event where each word is generated independently according to a multinomial distribution φ t , speciﬁc to the topic t . To generate the relational structure of the network , ﬁrst the group assignment , g st for each entity s is chosen condition - allyonthetopic , fromaparticularmultinomialdistribution θ t overgroupsforeachtopic t . Giventhegroupassignments on an event b , the matrix V ( b ) is generated where each cell V ( b ) g i g j represents how often the groups of two senators be - haved the same or not during the event b , ( e . g . , voted the SYMBOL DESCRIPTION g it entity i ’s group assignment in topic t t b topic of an event b w ( b ) k the k th token in the event b V ( b ) ij entity i and j’s groups behaved same ( 1 ) or diﬀerently ( 2 ) on the event b S number of entities T number of topics G number of groups B number of events V number of unique words N b number of word tokens in the event b S b number of entities who participated in the event b Table 1 : Notation used in this paper ! " w v # $ t g % & N b S b 2 T BG 2 S T B Figure 1 : The Group - Topic model sameornotonabill ) . Theelementsof V aresampledfrom a binomial distribution γ ( b ) g i g j . Our notation is summarized in Table 1 , and the graphical model representation of the model is shown in Figure 1 . Without considering the topic of an event , or by treat - ing all events in a corpus as reﬂecting a single topic , the simpliﬁed model ( only the right part of Figure 1 ) becomes equivalent to the stochastic Blockstructures model [ 17 ] . To match the Blockstructures model , each event deﬁnes a re - lationship , e . g . , whether in the event two entities’ groups behave the same or not . On the other hand , in our model a relation may have multiple attributes ( which in our exper - iments are the words describing the event , generated by a per - topic multinomial ) . When we consider the complete model , the dataset is dy - namically divided into T sub - blocks each of which corre - sponds to a topic . The complete GT model is as follows , t b ∼ Uniform ( 1 T ) w it | φ t ∼ Multinomial ( φ t ) φ t | η ∼ Dirichlet ( η ) g it | θ t ∼ Multinomial ( θ t ) θ t | α ∼ Dirichlet ( α ) V ( b ) ij | γ ( b ) g i g j ∼ Binomial ( γ ( b ) g i g j ) γ ( b ) gh | β ∼ Beta ( β ) . We want to perform joint inference on ( text ) attributes andrelationstoobtaintopic - wisegroupmemberships . Since inferencecannotbedoneexactlyonsuchcomplicatedprob - abilisticgraphicalmodels , weemployGibbssamplingtocon - duct inference . Note that we adopt conjugate priors in our IndianBuffetProcessCompoundDirichletProcess B selectsasubsetofatomsforeachdistribution , andthe gammarandomvariables φ determinetherelativemasses associatedwiththeseatoms . 2 . 4 . FocusedTopicModels Suppose H parametrizes distributions over words . Then , theICDdeﬁnesagenerativetopicmodel , whereitisused togenerateasetofsparsedistributionsoveraninﬁnitenum - ber of components , called “topics . ” Each topic is drawn fromaDirichletdistributionoverwords . Inordertospecify afullygenerativemodel , wesamplethenumberofwords for each document from a negative binomial distribution , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . 2 Thegenerativemodelfor M documentsis 1 . for k = 1 , 2 , . . . , ( a ) Samplethesticklength π k accordingtoEq . 1 . ( b ) Sampletherelativemass φ k ∼ Gamma ( γ , 1 ) . ( c ) Drawthetopicdistributionoverwords , β k ∼ Dirichlet ( η ) . 2 . for m = 1 , . . . , M , ( a ) Sampleabinaryvector b m accordingtoEq . 1 . ( b ) Drawthetotalnumberofwords , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . ( c ) Samplethedistributionovertopics , θ m ∼ Dirichlet ( b m · φ ) . ( d ) Foreachword w mi , i = 1 , . . . , n ( m ) · , i . Drawthetopicindex z mi ∼ Discrete ( θ m ) . ii . Drawtheword w mi ∼ Discrete ( β z mi ) . We call this the focused topic model ( FTM ) because the inﬁnite binary matrix B serves to focus the distribution overtopicsontoaﬁnitesubset ( seeFigure1 ) . Thenumber oftopicswithinasingledocumentisalmostsurelyﬁnite , thoughthetotalnumberoftopicsisunbounded . Thetopic distribution for the m th document , θ m , is drawn from a Dirichletdistributionoverthetopicsselectedby b m . The Dirichlet distribution models uncertainty about topic pro - portionswhilemaintainingtherestrictiontoasparsesetoftopics . TheICDmodelsthedistributionovertheglobaltopicpro - portionparameters φ separatelyfromthedistributionover thebinarymatrix B . Thiscapturestheideathatatopicmay appearinfrequentlyinacorpus , butmakeupahighpropor - tionofthosedocumentsinwhichitoccurs . Conversely , a topicmayappearfrequentlyinacorpus , butonlywithlow proportion . 2 Notation n ( m ) k isthenumberofwordsassignedtothe k th topicofthe m thdocument , andweuseadotnotationtorepresent summation - i . e . n ( m ) · = P k n ( m ) k . Figure1 . Graphicalmodelforthefocusedtopicmodel 3 . RelatedModels Titsias ( 2007 ) introducedtheinﬁnitegamma - Poissonpro - cess , a distribution over unbounded matrices of non - negativeintegers , anduseditasthebasisforatopicmodel of images . In this model , the distribution over features forthe m thimageisgivenbyaDirichletdistributionover the non - negative elements of the m th row of the inﬁnite gamma - Poisson process matrix , with parameters propor - tionaltothevaluesattheseelements . Whilethisresultsin asparsematrixofdistributions , thenumberofzeroentries inanycolumnofthematrixiscorrelatedwiththevaluesofthenon - zeroentries . Columnswhichhaveentrieswith large values will not typically be sparse . Therefore , this modelwillnotdecoupleacross - dataprevalenceandwithin - dataproportionsoftopics . IntheICDthenumberofzero entriesiscontrolledbyaseparateprocess , theIBP , from thevaluesofthenon - zeroentries , whicharecontrolledby thegammarandomvariables . The sparse topic model ( SparseTM , Wang & Blei , 2009 ) usesaﬁnitespikeandslabmodeltoensurethateachtopic is represented by a sparse distribution over words . The spikesaregeneratedbyBernoullidrawswithasingletopic - wideparameter . Thetopicdistributionisthendrawnfroma symmetricDirichletdistributiondeﬁnedoverthesespikes . TheICDalsousesaspikeandslabapproach , butallows anunboundednumberof“spikes” ( duetotheIBP ) anda moregloballyinformative“slab” ( duetothesharedgamma randomvariables ) . WeextendtheSparseTM’sapproxima - tionoftheexpectationofaﬁnitemixtureofDirichletdis - tributions , toapproximatethemorecomplicatedmixtureof DirichletdistributionsgiveninEq . 2 . RecentworkbyFoxetal . ( 2009 ) usesdrawsfromanIBP toselectsubsetsofaninﬁnitesetofstates , tomodelmulti - pledynamicsystemswithsharedstates . ( Astateinthedy - namicsystemislikeacomponentinamixedmembershipmodel . ) Theprobabilityoftransitioningfromthe i thstate tothe j thstateinthe m thdynamicsystemisdrawnfroma Dirichletdistributionwithparameters b mj γ + τδ i , j , where Chang , Blei ! N d " d w d , n z d , n K # k y d , d ' $ N d ' " d ' w d ' , n z d ' , n Figure2 : Atwo - documentsegmentoftheRTM . Thevariable y indicateswhetherthetwodocumentsarelinked . Thecompletemodel containsthisvariableforeachpairofdocuments . Theplatesindicatereplication . Thismodelcapturesboththewordsandthelink structureofthedatashowninFigure1 . formulation , inspiredbythesupervisedLDAmodel ( Blei andMcAuliffe2007 ) , ensuresthatthesamelatenttopicas - signmentsusedtogeneratethecontentofthedocumentsalsogeneratestheirlinkstructure . Modelswhichdonot enforcethiscoupling , suchasNallapatietal . ( 2008 ) , might dividethetopicsintotwoindependentsubsets—oneforlinksandtheotherforwords . Suchadecompositionpre - ventsthesemodelsfrommakingmeaningfulpredictions aboutlinksgivenwordsandwordsgivenlinks . InSec - tion4wedemonstrateempiricallythattheRTMoutper - formssuchmodelsonthesetasks . 3 INFERENCE , ESTIMATION , AND PREDICTION Withthemodeldeﬁned , weturntoapproximateposte - riorinference , parameterestimation , andprediction . We developavariationalinferenceprocedureforapproximat - ingtheposterior . Weusethisprocedureinavariational expectation - maximization ( EM ) algorithmforparameter estimation . Finally , weshowhowamodelwhoseparame - tershavebeenestimatedcanbeusedasapredictivemodelofwordsandlinks . Inference Inposteriorinference , weseektocompute the posterior distribution of the latent variables condi - tionedontheobservations . Exactposteriorinferenceisin - tractable ( Bleietal . 2003 ; BleiandMcAuliffe2007 ) . We appealtovariationalmethods . Invariationalmethods , wepositafamilyofdistributions overthelatentvariablesindexedbyfreevariationalpa - rameters . Thoseparametersareﬁttobeclosetothetrue posterior , whereclosenessismeasuredbyrelativeentropy . SeeJordanetal . ( 1999 ) forareview . Weusethefully - factorizedfamily , q ( Θ , Z | γ , Φ ) = ￿ d [ q θ ( θ d | γ d ) ￿ n q z ( z d , n | φ d , n ) ] , ( 3 ) where γ isasetofDirichletparameters , oneforeachdoc - ument , and Φ isasetofmultinomialparameters , onefor eachwordineachdocument . Notethat E q [ z d , n ] = φ d , n . Minimizingtherelativeentropyisequivalenttomaximiz - ingtheJensen’slowerboundonthemarginalprobabilityoftheobservations , i . e . , theevidencelowerbound ( ELBO ) , L = ￿ ( d 1 , d 2 ) E q [ log p ( y d 1 , d 2 | z d 1 , z d 2 , η , ν ) ] + ￿ d ￿ n E q [ log p ( w d , n | β 1 : K , z d , n ) ] + ￿ d ￿ n E q [ log p ( z d , n | θ d ) ] + ￿ d E q [ log p ( θ d | α ) ] + H ( q ) , ( 4 ) where ( d 1 , d 2 ) denotesalldocumentpairs . Theﬁrstterm oftheELBOdifferentiatestheRTMfromLDA ( Bleietal . 2003 ) . Theconnectionsbetweendocumentsaffecttheob - jectiveinapproximateposteriorinference ( and , below , in parameterestimation ) . Wedeveloptheinferenceprocedureundertheassumptionthatonlyobservedlinkswillbemodeled ( i . e . , y d 1 , d 2 isei - ther 1 orunobserved ) . 1 Wedothisfortworeasons . First , whileonecanﬁx y d 1 , d 2 = 1 wheneveralinkisob - servedbetween d 1 and d 2 andset y d 1 , d 2 = 0 otherwise , this approachisinappropriateincorporawheretheabsenceofalinkcannotbeconstruedasevidencefor y d 1 , d 2 = 0 . In thesecases , treatingtheselinksasunobservedvariablesis morefaithfultotheunderlyingsemanticsofthedata . For example , inlargesocialnetworkssuchasFacebooktheab - senceofalinkbetweentwopeopledoesnotnecessarilymeanthattheyarenotfriends ; theymayberealfriends whoareunawareofeachother’sexistenceinthenetwork . Treatingthislinkasunobservedbetterrespectsourlackofknowledgeaboutthestatusoftheirrelationship . Second , treatingnon - linkslinksashiddendecreasesthe computationalcostofinference ; sincethelinkvariablesare leavesinthegraphicalmodeltheycanberemovedwhen - 1 Sumsoverdocumentpairs ( d 1 , d 2 ) areunderstoodtorange overpairsforwhichalinkhasbeenobserved . ! ! T " # k $ k % M & d ! D ' Parse trees grouped into M documents ( a ) OverallGraphicalModel w1 : laid w2 : phrases w6 : for w5 : his w4 : some w5 : mind w7 : years w3 : in z1 z2 z3 z4 z5 z5 z6 z7 ( b ) SentenceGraphicalModel Figure 1 : In the graphical model of the STM , a document is made up of a number of sentences , represented by a tree of latent topics z which in turn generate words w . These words’ topics are chosen by the topic of their parent ( as encoded by the tree ) , the topic weights for a document θ , and the node’s parent’s successor weights π . ( For clarity , not all dependencies of sentence nodes are shown . ) The structure of variables for sentences within the document plate is on the right , as demonstratedbyanautomaticparseofthesentence“Somephraseslaidinhismindforyears . ” The STMassumesthatthetreestructureandwordsaregiven , butthelatenttopics z arenot . isgoingtobeanounconsistentastheobjectofthepreposition“of . ” Thematically , becauseitisin atravelbrochure , wewouldexpecttoseewordssuchas“Acapulco , ” “CostaRica , ” or“Australia” more than “kitchen , ” “debt , ” or “pocket . ” Our model can capture these kinds of regularities and exploittheminpredictiveproblems . Previouseffortstocapturelocalsyntacticcontextincludesemanticspacemodels [ 6 ] andsimilarity functions derived from dependency parses [ 7 ] . These methods successfully determine words that sharesimilarcontexts , butdonotaccountforthematicconsistency . Theyhavedifﬁcultywithpol - ysemouswordssuchas“ﬂy , ” whichcanbeeitheraninsectoratermfrombaseball . Withasense of document context , i . e . , a representation of whether a document is about sports or animals , the meaningofsuchtermscanbedistinguished . Other techniques have attempted to combine local context with document coherence using linear sequence models [ 8 , 9 ] . While these models are powerful , ordering words sequentially removes the important connections that are preserved in a syntactic parse . Moreover , these models gener - ate words either from the syntactic or thematic context . In the syntactic topic model , words are constrainedtobeconsistentwithboth . The remainder of this paper is organized as follows . We describe the syntactic topic model , and develop an approximate posterior inference technique based on variational methods . We study its performance both on synthetic data and hand parsed data [ 10 ] . We show that the STM captures relationshipsmissedbyothermodelsandachieveslowerheld - outperplexity . 2 Thesyntactictopicmodel Wedescribethesyntactictopicmodel ( STM ) , adocumentmodelthatcombinesobservedsyntactic structure and latent thematic structure . To motivate this model , we return to the travel brochure sentence “In the near future , you could ﬁnd yourself in . ” . The word that ﬁlls in the blank is constrainedbyitssyntacticcontextanditsdocumentcontext . Thesyntacticcontexttellsusthatitis anobjectofapreposition , andthedocumentcontexttellsusthatitisatravel - relatedword . The STM attempts to capture these joint inﬂuences on words . It models a document corpus as exchangeable collections of sentences , each of which is associated with a tree structure such as a 2 Thisprovidesaninferentialspeed - upthatmakesit possibletoﬁtmodelsatvaryinggranularities . Asex - amples , journalarticlesmightbeexchangeablewithin anissue , anassumptionwhichismorerealisticthan onewheretheyareexchangeablebyyear . Otherdata , suchasnews , mightexperienceperiodsoftimewithout anyobservation . WhilethedDTMrequiresrepresent - ingalltopicsforthediscretetickswithintheseperiods , thecDTMcananalyzesuchdatawithoutasacriﬁceofmemoryorspeed . WiththecDTM , thegranularity canbechosentomaximizemodelﬁtnessratherthantolimitcomputationalcomplexity . WenotethatthecDTManddDTMarenottheonlytopicmodelstotaketimeintoconsideration . Topics overtimemodels ( TOT ) [ 23 ] anddynamicmixture models ( DMM ) [ 25 ] alsoincludetimestampsinthe analysisofdocuments . TheTOTmodeltreatsthe timestampsasobservationsofthelatenttopics , while DMMassumesthatthetopicmixtureproportionsofeachdocumentisdependentonprevioustopicmix - tureproportions . InbothTOTandDMM , thetopics themselvesare constant , andthetimeinformationis usedtobetterdiscoverthem . Inthesettinghere , we areinterestedininferringevolvingtopics . Therestofthepaperisorganizedasfollows . Insec - tion2wedescribethedDTManddevelopthecDTM indetail . Section3presentsaneﬃcientposteriorin - ferencealgorithmforthecDTMbasedonsparsevaria - tionalmethods . Insection4 , wepresentexperimental resultsontwonewscorpora . 2 Continuoustimedynamictopicmodels Inatimestampeddocumentcollection , wewouldlike to model its latent topics as changing through the courseofthecollection . Innewsdata , forexample , a singletopicwillchangeasthestoriesassociatedwith itdevelop . Thediscrete - timedynamictopicmodel ( dDTM ) buildsontheexchangeabletopicmodelto providesuchmachinery [ 2 ] . InthedDTM , documents aredividedintosequentialgroups , andthetopicsof eachsliceevolvefromthetopicsofthepreviousslice . Documentsinagroupareassumedexchangeable . Morespeciﬁcally , atopicisrepresentedasadistribu - tionovertheﬁxedvocabularyofthecollection . The dDTMassumesthatadiscrete - timestatespacemodel governstheevolutionofthenaturalparametersofthe multinomial distributions that represent the topics . ( Recallthatthenaturalparametersofthemultino - mialarethelogsoftheprobabilitiesofeachitem . ) Thisisatime - seriesextensiontothelogisticnormal distribution [ 26 ] . Figure 1 : Graphical model representation of the cDTM . Theevolutionofthetopicparameters β t is governedbyBrownianmotion . Thevariable s t isthe observedtimestampofdocument d t . AdrawbackofthedDTMisthattimeisdiscretized . Iftheresolutionischosentobetoocoarse , thenthe assumptionthatdocumentswithinatimestepareex - changeablewillnotbetrue . Iftheresolutionistoo ﬁne , thenthenumberofvariationalparameterswillex - plodeasmoretimepointsareadded . Choosingthedis - cretizationshouldbeadecisionbasedonassumptionsaboutthedata . However , thecomputationalconcerns mightpreventanalysisattheappropriatetimescale . Thus , wedevelopthecontinuoustimedynamictopic model ( cDTM ) for modeling sequential time - series datawitharbitrarygranularity . ThecDTMcanbe seenasanaturallimitofthedDTMatitsﬁnestpos - sibleresolution , theresolutionatwhichthedocument timestampsaremeasured . InthecDTM , westillrepresenttopicsintheirnatural parameterization , butweuseBrownianmotion [ 14 ] to modeltheirevolutionthroughtime . Let i , j ( j > i > 0 ) betwoarbitrarytime indexes , s i and s j bethetime stamps , and∆ s j , s i betheelapsedtimebetweenthem . Ina K - topiccDTMmodel , thedistributionofthe k th ( 1 ≤ k ≤ K ) topic’sparameteratterm w is : β 0 , k , w ∼ N ( m , v 0 ) β j , k , w | β i , k , w , s ∼ N ￿ β i , k , w , v ∆ s j , s i ￿ , ( 1 ) wherethevarianceincreaseslinearlywiththelag . Thisconstructionisusedasacomponentinthefullgenerativeprocess . ( Note : if j = i + 1 , wewrite∆ s j , s i as∆ s j forshort . ) 1 . Foreachtopic k , 1 ≤ k ≤ K , ( a ) Draw β 0 , k ∼N ( m , v 0 I ) . ( a ) ( b ) Figure1 : ( a ) LDAmodel . ( b ) MG - LDAmodel . isstillnotdirectlydependentonthenumberofdocumentsand , therefore , themodelisnotexpectedtosuﬀerfromover - ﬁtting . AnotherapproachistouseaMarkovchainMonte CarloalgorithmforinferencewithLDA , asproposedin [ 14 ] . Insection3wewilldescribeamodiﬁcationofthissamplingmethodfortheproposedMulti - grainLDAmodel . BothLDAandPLSAmethodsusethebag - of - wordsrep - resentation of documents , therefore they can only explore co - occurrencesatthedocumentlevel . Thisisﬁne , provided the goal is to represent an overall topic of the document , but our goal is diﬀerent : extracting ratable aspects . The maintopicofallthereviewsforaparticularitemisvirtu - allythesame : areviewofthisitem . Therefore , whensuch topic modeling methods are applied to a collection of re - viewsfordiﬀerentitems , theyinfertopicscorrespondingto distinguishingpropertiesoftheseitems . E . g . whenapplied toacollectionofhotelreviews , thesemodelsarelikelytoin - fertopics : hotelsinFrance , NewYorkhotels , youthhostels , or , similarly , when applied to a collection of Mp3 players’ reviews , these models will infertopics like reviews of iPod or reviewsofCreativeZenplayer . Thoughtheseareallvalid topics , theydonotrepresentratableaspects , butratherde - ﬁneclusteringsoftherevieweditemsintospeciﬁctypes . In furtherdiscussionwewillrefertosuchtopicsas global topics , becausetheycorrespondtoaglobalpropertyoftheobjectinthereview , suchasitsbrandorbaseofoperation . Dis - coveringtopicsthatcorrelatewithratableaspects , suchas cleanliness and location forhotels , ismuchmoreproblem - aticwithLDAorPLSAmethods . Mostofthesetopicsare presentinsomewayineveryreview . Therefore , itisdiﬃcult todiscoverthembyusingonlyco - occurrenceinformationat thedocumentlevel . Inthiscaseexceedinglylargeamounts oftrainingdataisneededandaswellasaverylargenum - ber of topics K . Even in this case there is a danger that themodelwillbeoverﬂownbyveryﬁne - grainglobaltopics or theresulting topics will be intersection of global topics and ratable aspects , like location for hotels in New York . WewillshowinSection4thatthishypothesisisconﬁrmedexperimentally . Onewaytoaddressthisproblemwouldbetoconsiderco - occurrencesatthesentencelevel , i . e . , applyLDAorPLSAto individualsentences . Butinthiscasewewillnothaveasuf - ﬁcientco - occurrencedomain , anditisknownthatLDAand PLSAbehavebadlywhenappliedtoveryshortdocuments . Thisproblemcanbeaddressedbyexplicitlymodelingtopictransitions [ 5 , 15 , 33 , 32 , 28 , 16 ] , but these topic n - gram models are considerably more computationally expensive . Also , likeLDAandPLSA , theywillnotbeabletodistin - guish betweentopics correspondingtoratableaspectsand global topics representingproperties of the reviewed item . Inthefollowing section wewill introduceamethod which explicitlymodelsbothtypesoftopicsandeﬃcientlyinfersratableaspectsfromlimitedamountoftrainingdata . 2 . 2 MG - LDA WeproposeamodelcalledMulti - grainLDA ( MG - LDA ) , whichmodelstwodistincttypesoftopics : globaltopicsand localtopics . AsinPLSAandLDA , thedistributionofglobal topicsisﬁxedforadocument . However , thedistributionof localtopicsisallowedtovaryacrossthedocument . Aword inthedocumentissampledeitherfromthemixtureofglobaltopicsorfromthemixtureoflocaltopicsspeciﬁcforthelocalcontextoftheword . The hypothesis is that ratable aspects will be captured by local topics and global topics willcapturepropertiesofrevieweditems . Forexamplecon - sideranextractfromareviewofaLondonhotel : “ . . . public transportinLondonisstraightforward , thetubestationis aboutan8minutewalk . . . oryoucangetabusfor £ 1 . 50” . It can be viewed as a mixture of topic London shared by the entire review ( words : “London” , “tube” , “ £ ” ) , and the ratableaspect location , speciﬁcforthelocalcontextofthe sentence ( words : “transport” , “walk” , “bus” ) . Local topics are expected to be reused between very diﬀerent types of items , whereasglobaltopicswillcorrespondonlytopartic - ulartypesofitems . Inordertocaptureonlygenuinelocal topics , weallowalargenumberofglobaltopics , eﬀectively , creatingabottleneckattheleveloflocaltopics . Ofcourse , this bottleneck is speciﬁc to our purposes . Otherapplica - tions of multi - grain topic models conceivably might prefer thebottleneckreversed . Finally , wenotethatourdeﬁnition ofmulti - grainissimplyfortwo - levelsofgranularity , global and local . Inprinciplethough , thereisnothingpreventing themodeldescribedinthissectionfromextendingbeyondtwolevels . Onemightexpectthatforothertasksevenmore levelsofgranularitycouldbebeneﬁcial . Werepresentadocumentasasetofslidingwindows , each covering T adjacentsentenceswithinit . Eachwindow v in document d hasanassociateddistributionoverlocaltopics θ locd , v and a distribution deﬁning preference for local topics versusglobaltopics π d , v . Awordcanbesampledusingany windowcoveringitssentence s , wherethewindowischosen accordingtoacategoricaldistribution ψ s . Importantly , the fact that the windows overlap , permits to exploit a larger co - occurrencedomain . Thesesimpletechniquesarecapable ofmodelinglocaltopicswithoutmoreexpensivemodelingof topicstransitionsusedin [ 5 , 15 , 33 , 32 , 28 , 16 ] . Introduction ofasymmetricalDirichletprior Dir ( γ ) forthedistribution ψ s permitstocontrolsmoothnessoftopictransitionsinour model . The formal deﬁnition of themodel with K gl global and K loc local topics is the following . First , draw K gl word distributions for global topics ϕ glz from a Dirichlet prior Dir ( β gl ) and K loc word distributions for local topics ϕ locz ! from Dir ( β loc ) . Then , foreachdocument d : • Chooseadistributionofglobaltopics θ gld ∼ Dir ( α gl ) . • For each sentence s choose a distribution ψ d , s ( v ) ∼ Dir ( γ ) . • Foreachslidingwindow v 113 WWW 2008 / Refereed Track : Data Mining - Modeling April 21 - 25 , 2008 · Beijing , China McCallum , Wang , & Corrada - Emmanuel ! " z w Latent Dirichlet Allocation ( LDA ) [ Blei , Ng , Jordan , 2003 ] N D x z w Author - Topic Model ( AT ) [ Rosen - Zvi , Grifﬁths , Steyvers , Smyth 2004 ] N D " # ! $ T A # $ T x z w Author - Recipient - Topic Model ( ART ) [ This paper ] N D " # ! $ T A , A z w Author Model ( Multi - label Mixture Model ) [ McCallum 1999 ] N D # $ A a d a d r d a d d d d d Figure 1 : Three related models , and the ART model . In all models , each observed word , w , isgeneratedfromamultinomialworddistribution , φ z , speciﬁctoaparticular topic / author , z , however topics are selected diﬀerently in each of the models . In LDA , the topic is sampled from a per - document topic distribution , θ , which in turn is sampled from a Dirichlet over topics . In the Author Model , there is one topic associated with each author ( or category ) , and authors are sampled uniformly . In the Author - Topic model , the topic is sampled from a per - author multinomialdistribution , θ , andauthorsaresampleduniformlyfromtheobserved list of the document’s authors . In the Author - Recipient - Topic model , there is a separate topic - distribution for each author - recipient pair , and the selection of topic - distributionisdeterminedfromtheobservedauthor , andbyuniformlysam - pling a recipient from the set of recipients for the document . its generative process for each document d , a set of authors , a d , is observed . To generate each word , an author x is chosen uniformly from this set , then a topic z is selected from a topic distribution θ x that is speciﬁc to the author , and then a word w is generated from a topic - speciﬁc multinomial distribution φ z . However , as described previously , none of these models is suitable for modeling message data . An email message has one sender and in general more than one recipients . We could treatboththesenderandtherecipientsas“authors”ofthemessage , andthenemploythe ATmodel , butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage , which isundesirableinmanyreal - worldsituations . Amanagermaysendemailtoasecretaryand vice versa , but the nature of the requests and language used may be quite diﬀerent . Even more dramatically , consider the large quantity of junk email that we receive ; modeling the topicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotreﬂectourexpertiseorroles . AlternativelywecouldstillemploytheATmodelbyignoringtherecipientinformation of email and treating each email document as if it only has one author . However , in this case ( whichissimilartotheLDAmodel ) wearelosingallinformationabouttherecipients , and the connections between people implied by the sender - recipient relationships . 252 • LDA is a simple building block that enables many applications . • It is popular because organizing and ﬁnding patterns in data has become important in the sciences , humanties , industry , and culture . • Further , algorithmic improvements let us ﬁt models to massive data . Example : LDA in R ( Jonathan Chang ) docs < - read . documents ( " mult . dat " ) K < - 20 alpha < - 1 / 20 eta < - 0 . 001 model < - lda . collapsed . gibbs . sampler ( documents , K , vocab , 1000 , alpha , eta ) 245 1897 : 1 1467 : 1 1351 : 1 731 : 2 800 : 5 682 : 1 315 : 6 3668 : 1 14 : 1 260 4261 : 2 518 : 1 271 : 6 2734 : 1 2662 : 1 2432 : 1 683 : 2 1631 : 7 279 2724 : 1 107 : 3 518 : 1 141 : 3 3208 : 1 32 : 1 2444 : 1 182 : 1 250 : 1 266 2552 : 1 1993 : 1 116 : 1 539 : 1 1630 : 1 855 : 1 1422 : 1 182 : 3 2432 : 1 233 1372 : 1 1351 : 1 261 : 1 501 : 1 1938 : 1 32 : 1 14 : 1 4067 : 1 98 : 2 148 4384 : 1 1339 : 1 32 : 1 4107 : 1 2300 : 1 229 : 1 529 : 1 521 : 1 2231 : 1 193 569 : 1 3617 : 1 3781 : 2 14 : 1 98 : 1 3596 : 1 3037 : 1 1482 : 12 665 : 2 . . . . perspective identifying tumor suppressor genes in human . . . letters global warming report leslie roberts article global . . . . research news a small revolution gets under way the 1990s . . . . a continuing series the reign of trial and error draws to a close . . . making deep earthquakes in the laboratory lab experimenters . . . quick ﬁx for freeways thanks to a team of fast working . . . feathers ﬂy in grouse population dispute researchers . . . . . . . 1 analysis dna gene genes genetic genome human sequence sequences two 6 article card circle end letters news readers science service start 11 age ago early evidence fig million north record university years 16 aaas advertising associate fax manager member recruitment sales science washington 2 activation activity binding cell cells fig kinase protein proteins receptor 7 data different fig model number rate results system time two 12 biology evolution evolutionary genetic natural population populations species studies university 17 biology cell cells development expression fig gene genes mice mutant 3 atmosphere atmospheric carbon changes climate global ocean surface temperature water 8 chemical fig high materials molecular molecules structure surface temperature university 13 acid amino binding molecular protein proteins residues structural structure two 18 electron electrons energy high laser light magnetic physics quantum state 4 first just like new researchers says science university work years 9 binding dna protein proteins rna sequence sequences site specific transcription 14 antigen cell cells hiv human immune infected infection viral virus 19 health national new research science scientific scientists states united university 5 crust earth earthquakes earths high lower mantle pressure seismic temperature 10 cancer disease drug drugs gene human medical normal patients studies 15 astronomers earth mass observations solar space stars sun telescope university 20 activity brain cells channels cortex fig neuronal neurons university visual Open source document browser ( with Allison Chaney ) Beyond Latent Dirichlet Allocation Extending LDA θ d Z d , n W d , n N D K β k α η • LDA is a simple topic model . • It can be used to ﬁnd topics that describe a corpus . • Each document exhibits multiple topics . • How can we build on this simple model of text ? Extending LDA Make assumptions Infer the posterior Explore Collect data Predict Check Extending LDA by emerging groups . Both modalities are driven by the common goal of increasing data likelihood . Consider the voting example again ; resolutions that would have been as - signed the same topic in a model using words alone may be assigned to diﬀerent topics if they exhibit distinct voting patterns . Distinct word - based topics may be merged if the entities vote very similarly on them . Likewise , multiple dif - ferent divisions of entities into groups are made possible by conditioning them on the topics . The importanceof modelingthe language associated with interactionsbetweenpeoplehasrecentlybeendemonstratedintheAuthor - Recipient - Topic ( ART ) model [ 16 ] . In ART the words in a message between people in a network are generated conditioned on the author , recipient and a set of topics that describes the message . The model thus cap - tures both the network structure within which the people interact as well as the language associated with the inter - actions . In experiments with Enron and academic email , the ART model is able to discover role similarity of people better than SNA models that consider network connectivity alone . However , the ART model does not explicitly capture groups formed by entities in the network . The GT model simultaneously clusters entities to groups and clusters words into topics , unlike models that gener - ate topics solely based on word distributions such as Latent Dirichlet Allocation [ 4 ] . In this way the GT model discov - ers salient topics relevant to relationships between entities in the social network—topics which the models that only examine words are unable to detect . We demonstrate the capabilities of the GT model by ap - plying it to two large sets of voting data : one from US Sen - ate and the other from the General Assembly of the UN . The model clusters voting entities into coalitions and si - multaneously discovers topics for word attributes describing the relations ( bills or resolutions ) between entities . We ﬁnd that the groups obtained from the GT model are signiﬁ - cantly more cohesive ( p - value < . 01 ) than those obtained from the Blockstructures model . The GT model also dis - covers new and more salient topics in both the UN and Sen - ate datasets—in comparison with topics discovered by only examining the words of the resolutions , the GT topics are either split or joined together as inﬂuenced by the voters’ patterns of behavior . 2 . GROUP - TOPIC MODEL TheGroup - TopicModelisadirectedgraphicalmodelthat clusters entities with relations between them , as well as at - tributes of those relations . The relations may be either di - rected or undirected and have multiple attributes . In this paper , we focus on undirected relations and have words as the attributes on relations . In the generative process for each event ( an interaction between entities ) , the model ﬁrst picks the topic t of the event and then generates all the words describing the event where each word is generated independently according to a multinomial distribution φ t , speciﬁc to the topic t . To generate the relational structure of the network , ﬁrst the group assignment , g st for each entity s is chosen condition - ally on the topic , from a particular multinomial distribution θ t overgroupsforeachtopic t . Giventhegroupassignments on an event b , the matrix V ( b ) is generated where each cell V ( b ) g i g j represents how often the groups of two senators be - haved the same or not during the event b , ( e . g . , voted the SYMBOL DESCRIPTION g it entity i ’s group assignment in topic t t b topic of an event b w ( b ) k the k th token in the event b V ( b ) ij entity i and j’s groups behaved same ( 1 ) or diﬀerently ( 2 ) on the event b S number of entities T number of topics G number of groups B number of events V number of unique words N b number of word tokens in the event b S b number of entities who participated in the event b Table 1 : Notation used in this paper ! " w v # $ t g % & N b S b 2 T BG 2 S T B Figure 1 : The Group - Topic model same or not on a bill ) . The elements of V are sampled from a binomial distribution γ ( b ) g i g j . Our notation is summarized in Table 1 , and the graphical model representation of the model is shown in Figure 1 . Without considering the topic of an event , or by treat - ing all events in a corpus as reﬂecting a single topic , the simpliﬁed model ( only the right part of Figure 1 ) becomes equivalent to the stochastic Blockstructures model [ 17 ] . To match the Blockstructures model , each event deﬁnes a re - lationship , e . g . , whether in the event two entities’ groups behave the same or not . On the other hand , in our model a relation may have multiple attributes ( which in our exper - iments are the words describing the event , generated by a per - topic multinomial ) . When we consider the complete model , the dataset is dy - namically divided into T sub - blocks each of which corre - sponds to a topic . The complete GT model is as follows , t b ∼ Uniform ( 1 T ) w it | φ t ∼ Multinomial ( φ t ) φ t | η ∼ Dirichlet ( η ) g it | θ t ∼ Multinomial ( θ t ) θ t | α ∼ Dirichlet ( α ) V ( b ) ij | γ ( b ) g i g j ∼ Binomial ( γ ( b ) g i g j ) γ ( b ) gh | β ∼ Beta ( β ) . We want to perform joint inference on ( text ) attributes andrelationstoobtaintopic - wisegroupmemberships . Since inferencecannotbedoneexactlyonsuchcomplicatedprob - abilisticgraphicalmodels , weemployGibbssamplingtocon - duct inference . Note that we adopt conjugate priors in our IndianBuffetProcessCompoundDirichletProcess B selects a subset of atoms for each distribution , and the gammarandomvariables φ determinetherelativemasses associatedwiththeseatoms . 2 . 4 . FocusedTopicModels Suppose H parametrizes distributions over words . Then , theICDdeﬁnesagenerativetopicmodel , whereitisused togenerateasetofsparsedistributionsoveraninﬁnitenum - ber of components , called “topics . ” Each topic is drawn fromaDirichletdistributionoverwords . Inordertospecify afullygenerativemodel , wesamplethenumberofwords for each document from a negative binomial distribution , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . 2 Thegenerativemodelfor M documentsis 1 . for k = 1 , 2 , . . . , ( a ) Samplethesticklength π k accordingtoEq . 1 . ( b ) Sampletherelativemass φ k ∼ Gamma ( γ , 1 ) . ( c ) Drawthetopicdistributionoverwords , β k ∼ Dirichlet ( η ) . 2 . for m = 1 , . . . , M , ( a ) Sampleabinaryvector b m accordingtoEq . 1 . ( b ) Drawthetotalnumberofwords , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . ( c ) Samplethedistributionovertopics , θ m ∼ Dirichlet ( b m · φ ) . ( d ) Foreachword w mi , i = 1 , . . . , n ( m ) · , i . Drawthetopicindex z mi ∼ Discrete ( θ m ) . ii . Drawtheword w mi ∼ Discrete ( β z mi ) . We call this the focused topic model ( FTM ) because the inﬁnite binary matrix B serves to focus the distribution overtopicsontoaﬁnitesubset ( seeFigure1 ) . Thenumber of topics within a single document is almost surely ﬁnite , thoughthetotalnumberoftopicsisunbounded . Thetopic distribution for the m th document , θ m , is drawn from a Dirichletdistributionoverthetopicsselectedby b m . The Dirichlet distribution models uncertainty about topic pro - portionswhilemaintainingtherestrictiontoasparsesetoftopics . TheICDmodelsthedistributionovertheglobaltopicpro - portionparameters φ separatelyfromthedistributionover thebinarymatrix B . Thiscapturestheideathatatopicmay appearinfrequentlyinacorpus , butmakeupahighpropor - tionofthosedocumentsinwhichitoccurs . Conversely , a topicmayappearfrequentlyinacorpus , butonlywithlow proportion . 2 Notation n ( m ) k is the number of words assigned to the k th topicofthe m thdocument , andweuseadotnotationtorepresent summation - i . e . n ( m ) · = P k n ( m ) k . Figure1 . Graphicalmodelforthefocusedtopicmodel 3 . RelatedModels Titsias ( 2007 ) introduced the inﬁnite gamma - Poisson pro - cess , a distribution over unbounded matrices of non - negativeintegers , anduseditasthebasisforatopicmodel of images . In this model , the distribution over features forthe m thimageisgivenbyaDirichletdistributionover the non - negative elements of the m th row of the inﬁnite gamma - Poisson process matrix , with parameters propor - tionaltothevaluesattheseelements . Whilethisresultsin asparsematrixofdistributions , thenumberofzeroentries in any column of the matrix is correlated with the values ofthenon - zeroentries . Columnswhichhaveentrieswith large values will not typically be sparse . Therefore , this modelwillnotdecoupleacross - dataprevalenceandwithin - dataproportionsoftopics . IntheICDthenumberofzero entries is controlled by a separate process , the IBP , from thevaluesofthenon - zeroentries , whicharecontrolledby thegammarandomvariables . The sparse topic model ( SparseTM , Wang & Blei , 2009 ) usesaﬁnitespikeandslabmodeltoensurethateachtopic is represented by a sparse distribution over words . The spikesaregeneratedbyBernoullidrawswithasingletopic - wideparameter . Thetopicdistributionisthendrawnfroma symmetricDirichletdistributiondeﬁnedoverthesespikes . The ICD also uses a spike and slab approach , but allows an unbounded number of “spikes” ( due to the IBP ) and a moregloballyinformative“slab” ( duetothesharedgamma randomvariables ) . WeextendtheSparseTM’sapproxima - tionoftheexpectationofaﬁnitemixtureofDirichletdis - tributions , toapproximatethemorecomplicatedmixtureof DirichletdistributionsgiveninEq . 2 . RecentworkbyFoxetal . ( 2009 ) usesdrawsfromanIBP toselectsubsetsofaninﬁnitesetofstates , tomodelmulti - pledynamicsystemswithsharedstates . ( Astateinthedy - namicsystemislikeacomponentinamixedmembershipmodel . ) Theprobabilityoftransitioningfromthe i thstate tothe j thstateinthe m thdynamicsystemisdrawnfroma Dirichletdistributionwithparameters b mj γ + τδ i , j , where Chang , Blei ! N d " d w d , n z d , n K # k y d , d ' $ N d ' " d ' w d ' , n z d ' , n Figure2 : Atwo - documentsegmentoftheRTM . Thevariable y indicateswhetherthetwodocumentsarelinked . Thecompletemodel containsthisvariableforeachpairofdocuments . Theplatesindicatereplication . Thismodelcapturesboththewordsandthelink structureofthedatashowninFigure1 . formulation , inspiredbythesupervisedLDAmodel ( Blei andMcAuliffe2007 ) , ensuresthatthesamelatenttopicas - signmentsusedtogeneratethecontentofthedocumentsalsogeneratestheirlinkstructure . Modelswhichdonot enforcethiscoupling , suchasNallapatietal . ( 2008 ) , might dividethetopicsintotwoindependentsubsets—oneforlinksandtheotherforwords . Suchadecompositionpre - ventsthesemodelsfrommakingmeaningfulpredictionsaboutlinksgivenwordsandwordsgivenlinks . InSec - tion4wedemonstrateempiricallythattheRTMoutper - formssuchmodelsonthesetasks . 3 INFERENCE , ESTIMATION , AND PREDICTION With the model deﬁned , we turn to approximate poste - riorinference , parameterestimation , andprediction . We developavariationalinferenceprocedureforapproximat - ingtheposterior . Weusethisprocedureinavariational expectation - maximization ( EM ) algorithm for parameter estimation . Finally , weshowhowamodelwhoseparame - tershavebeenestimatedcanbeusedasapredictivemodelofwordsandlinks . Inference In posterior inference , we seek to compute the posterior distribution of the latent variables condi - tionedontheobservations . Exactposteriorinferenceisin - tractable ( Bleietal . 2003 ; BleiandMcAuliffe2007 ) . We appealtovariationalmethods . Invariationalmethods , wepositafamilyofdistributions overthelatentvariablesindexedbyfreevariationalpa - rameters . Thoseparametersareﬁttobeclosetothetrue posterior , whereclosenessismeasuredbyrelativeentropy . SeeJordanetal . ( 1999 ) forareview . Weusethefully - factorizedfamily , q ( Θ , Z | γ , Φ ) = ￿ d [ q θ ( θ d | γ d ) ￿ n q z ( z d , n | φ d , n ) ] , ( 3 ) where γ isasetofDirichletparameters , oneforeachdoc - ument , and Φ isasetofmultinomialparameters , onefor eachwordineachdocument . Notethat E q [ z d , n ] = φ d , n . Minimizingtherelativeentropyisequivalenttomaximiz - ingtheJensen’slowerboundonthemarginalprobabilityoftheobservations , i . e . , theevidencelowerbound ( ELBO ) , L = ￿ ( d 1 , d 2 ) E q [ log p ( y d 1 , d 2 | z d 1 , z d 2 , η , ν ) ] + ￿ d ￿ n E q [ log p ( w d , n | β 1 : K , z d , n ) ] + ￿ d ￿ n E q [ log p ( z d , n | θ d ) ] + ￿ d E q [ log p ( θ d | α ) ] + H ( q ) , ( 4 ) where ( d 1 , d 2 ) denotesalldocumentpairs . Theﬁrstterm oftheELBOdifferentiatestheRTMfromLDA ( Bleietal . 2003 ) . Theconnectionsbetweendocumentsaffecttheob - jectiveinapproximateposteriorinference ( and , below , in parameterestimation ) . Wedeveloptheinferenceprocedureundertheassumptionthatonlyobservedlinkswillbemodeled ( i . e . , y d 1 , d 2 isei - ther 1 orunobserved ) . 1 Wedothisfortworeasons . First , whileonecanﬁx y d 1 , d 2 = 1 wheneveralinkisob - servedbetween d 1 and d 2 andset y d 1 , d 2 = 0 otherwise , this approachisinappropriateincorporawheretheabsenceofalinkcannotbeconstruedasevidencefor y d 1 , d 2 = 0 . In thesecases , treatingtheselinksasunobservedvariablesis morefaithfultotheunderlyingsemanticsofthedata . For example , inlargesocialnetworkssuchasFacebooktheab - senceofalinkbetweentwopeopledoesnotnecessarilymeanthattheyarenotfriends ; theymayberealfriends whoareunawareofeachother’sexistenceinthenetwork . Treatingthislinkasunobservedbetterrespectsourlackofknowledgeaboutthestatusoftheirrelationship . Second , treatingnon - linkslinksashiddendecreasesthe computationalcostofinference ; sincethelinkvariablesare leavesinthegraphicalmodeltheycanberemovedwhen - 1 Sumsoverdocumentpairs ( d 1 , d 2 ) areunderstoodtorange overpairsforwhichalinkhasbeenobserved . ! ! T " # k $ k % M & d ! D ' Parse trees grouped into M documents ( a ) OverallGraphicalModel w1 : laid w2 : phrases w6 : for w5 : his w4 : some w5 : mind w7 : years w3 : in z1 z2 z3 z4 z5 z5 z6 z7 ( b ) SentenceGraphicalModel Figure 1 : In the graphical model of the STM , a document is made up of a number of sentences , represented by a tree of latent topics z which in turn generate words w . These words’ topics are chosen by the topic of their parent ( as encoded by the tree ) , the topic weights for a document θ , and the node’s parent’s successor weights π . ( For clarity , not all dependencies of sentence nodes are shown . ) The structure of variables for sentences within the document plate is on the right , as demonstratedbyanautomaticparseofthesentence“Somephraseslaidinhismindforyears . ” The STMassumesthatthetreestructureandwordsaregiven , butthelatenttopics z arenot . is going to be a noun consistent as the object of the preposition “of . ” Thematically , because it is in a travel brochure , we would expect to see words such as “Acapulco , ” “Costa Rica , ” or “Australia” more than “kitchen , ” “debt , ” or “pocket . ” Our model can capture these kinds of regularities and exploittheminpredictiveproblems . Previouseffortstocapturelocalsyntacticcontextincludesemanticspacemodels [ 6 ] andsimilarity functions derived from dependency parses [ 7 ] . These methods successfully determine words that share similar contexts , but do not account for thematic consistency . They have difﬁculty with pol - ysemous words such as “ﬂy , ” which can be either an insect or a term from baseball . With a sense of document context , i . e . , a representation of whether a document is about sports or animals , the meaningofsuchtermscanbedistinguished . Other techniques have attempted to combine local context with document coherence using linear sequence models [ 8 , 9 ] . While these models are powerful , ordering words sequentially removes the important connections that are preserved in a syntactic parse . Moreover , these models gener - ate words either from the syntactic or thematic context . In the syntactic topic model , words are constrainedtobeconsistentwithboth . The remainder of this paper is organized as follows . We describe the syntactic topic model , and develop an approximate posterior inference technique based on variational methods . We study its performance both on synthetic data and hand parsed data [ 10 ] . We show that the STM captures relationshipsmissedbyothermodelsandachieveslowerheld - outperplexity . 2 The syntactic topic model Wedescribethesyntactictopicmodel ( STM ) , adocumentmodelthatcombinesobservedsyntactic structure and latent thematic structure . To motivate this model , we return to the travel brochure sentence “In the near future , you could ﬁnd yourself in . ” . The word that ﬁlls in the blank is constrainedbyitssyntacticcontextanditsdocumentcontext . Thesyntacticcontexttellsusthatitis anobjectofapreposition , andthedocumentcontexttellsusthatitisatravel - relatedword . The STM attempts to capture these joint inﬂuences on words . It models a document corpus as exchangeable collections of sentences , each of which is associated with a tree structure such as a 2 This provides an inferential speed - up that makes it possibletoﬁtmodelsatvaryinggranularities . Asex - amples , journalarticlesmightbeexchangeablewithin anissue , anassumptionwhichismorerealisticthan onewheretheyareexchangeablebyyear . Otherdata , suchasnews , mightexperienceperiodsoftimewithout anyobservation . WhilethedDTMrequiresrepresent - ingalltopicsforthediscretetickswithintheseperiods , thecDTMcananalyzesuchdatawithoutasacriﬁceofmemoryorspeed . WiththecDTM , thegranularity canbechosentomaximizemodelﬁtnessratherthantolimitcomputationalcomplexity . WenotethatthecDTManddDTMarenottheonlytopicmodelstotaketimeintoconsideration . Topics over time models ( TOT ) [ 23 ] and dynamic mixture models ( DMM ) [ 25 ] also include timestamps in the analysis of documents . The TOT model treats the timestampsasobservationsofthelatenttopics , while DMMassumesthatthetopicmixtureproportionsofeachdocumentisdependentonprevioustopicmix - tureproportions . InbothTOTandDMM , thetopics themselvesare constant , andthetimeinformationis usedtobetterdiscoverthem . Inthesettinghere , we areinterestedininferringevolvingtopics . Therestofthepaperisorganizedasfollows . Insec - tion2wedescribethedDTManddevelopthecDTMindetail . Section3presentsaneﬃcientposteriorin - ferencealgorithmforthecDTMbasedonsparsevaria - tionalmethods . Insection4 , wepresentexperimental resultsontwonewscorpora . 2 Continuoustimedynamictopic models Inatimestampeddocumentcollection , wewouldlike to model its latent topics as changing through the courseofthecollection . Innewsdata , forexample , a singletopicwillchangeasthestoriesassociatedwith it develop . The discrete - time dynamic topic model ( dDTM ) builds on the exchangeable topic model to providesuchmachinery [ 2 ] . InthedDTM , documents aredividedintosequentialgroups , andthetopicsof eachsliceevolvefromthetopicsofthepreviousslice . Documentsinagroupareassumedexchangeable . Morespeciﬁcally , atopicisrepresentedasadistribu - tionovertheﬁxedvocabularyofthecollection . The dDTMassumesthatadiscrete - timestatespacemodel governstheevolutionofthenaturalparametersofthemultinomialdistributionsthatrepresentthetopics . ( Recall that the natural parameters of the multino - mial are the logs of the probabilities of each item . ) Thisisatime - seriesextensiontothelogisticnormal distribution [ 26 ] . Figure 1 : Graphical model representation of the cDTM . The evolution of the topic parameters β t is governedbyBrownianmotion . Thevariable s t isthe observedtimestampofdocument d t . AdrawbackofthedDTMisthattimeisdiscretized . Iftheresolutionischosentobetoocoarse , thenthe assumptionthatdocumentswithinatimestepareex - changeablewillnotbetrue . Iftheresolutionistoo ﬁne , thenthenumberofvariationalparameterswillex - plodeasmoretimepointsareadded . Choosingthedis - cretizationshouldbeadecisionbasedonassumptionsaboutthedata . However , thecomputationalconcerns mightpreventanalysisattheappropriatetimescale . Thus , wedevelopthecontinuoustimedynamictopic model ( cDTM ) for modeling sequential time - series datawitharbitrarygranularity . ThecDTMcanbe seenasanaturallimitofthedDTMatitsﬁnestpos - sibleresolution , theresolutionatwhichthedocument timestampsaremeasured . InthecDTM , westillrepresenttopicsintheirnatural parameterization , butweuseBrownianmotion [ 14 ] to modeltheirevolutionthroughtime . Let i , j ( j > i > 0 ) betwoarbitrarytime indexes , s i and s j bethetime stamps , and∆ s j , s i betheelapsedtimebetweenthem . Ina K - topiccDTMmodel , thedistributionofthe k th ( 1 ≤ k ≤ K ) topic’sparameteratterm w is : β 0 , k , w ∼ N ( m , v 0 ) β j , k , w | β i , k , w , s ∼ N ￿ β i , k , w , v ∆ s j , s i ￿ , ( 1 ) wherethevarianceincreaseslinearlywiththelag . Thisconstructionisusedasacomponentinthefullgenerativeprocess . ( Note : if j = i + 1 , wewrite∆ s j , s i as∆ s j forshort . ) 1 . Foreachtopic k , 1 ≤ k ≤ K , ( a ) Draw β 0 , k ∼N ( m , v 0 I ) . ( a ) ( b ) Figure 1 : ( a ) LDA model . ( b ) MG - LDA model . isstillnot directlydependentonthenumberofdocuments and , therefore , themodelisnotexpectedtosuﬀerfromover - ﬁtting . Anotherapproach is to use a Markov chain Monte CarloalgorithmforinferencewithLDA , asproposedin [ 14 ] . Insection3wewilldescribeamodiﬁcationofthissamplingmethodfortheproposedMulti - grainLDAmodel . BothLDAandPLSAmethodsusethebag - of - wordsrep - resentation of documents , therefore they can only explore co - occurrencesatthedocumentlevel . Thisisﬁne , provided the goal is to represent an overall topic of the document , but our goal is diﬀerent : extracting ratable aspects . The main topic of all the reviews for a particular item is virtu - ally thesame : areviewof thisitem . Therefore , whensuch topic modeling methods are applied to a collection of re - viewsfordiﬀerentitems , theyinfertopicscorrespondingto distinguishingpropertiesoftheseitems . E . g . whenapplied toacollectionofhotelreviews , thesemodelsarelikelytoin - fertopics : hotels inFrance , New Yorkhotels , youthhostels , or , similarly , when applied to a collection of Mp3 players’ reviews , these models will infer topics like reviews of iPod or reviewsofCreativeZenplayer . Thoughtheseareallvalid topics , theydonotrepresentratableaspects , butratherde - ﬁneclusteringsoftherevieweditemsintospeciﬁctypes . In furtherdiscussionwewillrefertosuchtopicsas global topics , because theycorrespond toaglobal propertyof theobject in the review , such as its brand or base of operation . Dis - covering topics thatcorrelate with ratable aspects , such as cleanliness and location for hotels , is much more problem - aticwithLDAorPLSAmethods . Most ofthesetopicsare presentinsomewayineveryreview . Therefore , itisdiﬃcult todiscoverthembyusingonlyco - occurrenceinformationat thedocumentlevel . Inthiscaseexceedinglylargeamounts of trainingdataisneededandaswell asaverylargenum - ber of topics K . Even in this case there is a danger that themodelwillbeoverﬂownbyveryﬁne - grainglobaltopics or the resulting topics will be intersection of global topics and ratable aspects , like location for hotels in New York . WewillshowinSection4thatthishypothesisisconﬁrmedexperimentally . Onewaytoaddressthisproblemwouldbetoconsiderco - occurrencesatthesentencelevel , i . e . , applyLDAorPLSAto individualsentences . Butinthiscasewewillnothaveasuf - ﬁcientco - occurrencedomain , anditisknownthatLDAand PLSAbehavebadlywhenappliedtoveryshortdocuments . Thisproblemcanbeaddressedbyexplicitlymodelingtopictransitions [ 5 , 15 , 33 , 32 , 28 , 16 ] , but these topic n - gram models are considerably more computationally expensive . Also , like LDA and PLSA , they will not be able to distin - guish between topics corresponding to ratable aspects and global topics representing properties of the reviewed item . In the following section we will introduce a method which explicitly models both types of topics and eﬃciently infers ratableaspectsfromlimitedamountoftrainingdata . 2 . 2 MG - LDA WeproposeamodelcalledMulti - grainLDA ( MG - LDA ) , whichmodelstwodistincttypesoftopics : globaltopicsand localtopics . AsinPLSAandLDA , thedistributionofglobal topicsisﬁxedforadocument . However , thedistributionof localtopicsisallowedtovaryacrossthedocument . Aword inthedocumentissampledeitherfromthemixtureofglobaltopicsorfromthemixtureoflocaltopicsspeciﬁcforthelocalcontextoftheword . The hypothesis is that ratable aspects will be captured by local topics and global topics willcapturepropertiesofrevieweditems . Forexamplecon - sideranextractfromareviewofaLondonhotel : “ . . . public transport in London is straightforward , the tubestation is aboutan8minutewalk . . . oryoucangetabusfor £ 1 . 50” . It can be viewed as a mixture of topic London shared by the entire review ( words : “London” , “tube” , “ £ ” ) , and the ratable aspect location , speciﬁc for thelocal context of the sentence ( words : “transport” , “walk” , “bus” ) . Local topics are expected to be reused between very diﬀerent types of items , whereasglobaltopicswillcorrespondonlytopartic - ular typesof items . In order to captureonly genuinelocal topics , weallowalargenumberofglobaltopics , eﬀectively , creatingabottleneckattheleveloflocaltopics . Ofcourse , this bottleneck is speciﬁc to our purposes . Other applica - tions of multi - grain topic models conceivably might prefer thebottleneckreversed . Finally , wenotethatourdeﬁnition of multi - grain is simply for two - levels of granularity , global and local . In principle though , there is nothing preventing themodel described in this section from extendingbeyond twolevels . Onemightexpectthatforothertasksevenmore levelsofgranularitycouldbebeneﬁcial . Werepresentadocumentasasetofslidingwindows , each covering T adjacent sentenceswithin it . Each window v in document d hasanassociated distributionoverlocaltopics θ locd , v and a distribution deﬁning preference for local topics versusglobaltopics π d , v . Awordcanbesampledusingany windowcoveringitssentence s , wherethewindowischosen accordingtoacategoricaldistribution ψ s . Importantly , the fact that the windows overlap , permits to exploit a larger co - occurrencedomain . Thesesimpletechniquesarecapable ofmodelinglocaltopicswithoutmoreexpensivemodelingof topicstransitionsusedin [ 5 , 15 , 33 , 32 , 28 , 16 ] . Introduction ofasymmetricalDirichletprior Dir ( γ ) forthedistribution ψ s permitstocontrolsmoothnessoftopictransitionsinour model . The formal deﬁnition of the model with K gl global and K loc local topics is the following . First , draw K gl word distributions for global topics ϕ glz from a Dirichlet prior Dir ( β gl ) and K loc word distributions for local topics ϕ locz ! from Dir ( β loc ) . Then , foreachdocument d : • Chooseadistributionofglobaltopics θ gld ∼ Dir ( α gl ) . • For each sentence s choose a distribution ψ d , s ( v ) ∼ Dir ( γ ) . • Foreachslidingwindow v 113 WWW 2008 / Refereed Track : Data Mining - Modeling April 21 - 25 , 2008 · Beijing , China McCallum , Wang , & Corrada - Emmanuel ! " z w Latent Dirichlet Allocation ( LDA ) [ Blei , Ng , Jordan , 2003 ] N D x z w Author - Topic Model ( AT ) [ Rosen - Zvi , Grifﬁths , Steyvers , Smyth 2004 ] N D " # ! $ T A # $ T x z w Author - Recipient - Topic Model ( ART ) [ This paper ] N D " # ! $ T A , A z w Author Model ( Multi - label Mixture Model ) [ McCallum 1999 ] N D # $ A a d a d r d a d d d d d Figure 1 : Three related models , and the ART model . In all models , each observed word , w , is generated from a multinomial word distribution , φ z , speciﬁc to a particular topic / author , z , however topics are selected diﬀerently in each of the models . In LDA , the topic is sampled from a per - document topic distribution , θ , which in turn is sampled from a Dirichlet over topics . In the Author Model , there is one topic associated with each author ( or category ) , and authors are sampled uniformly . In the Author - Topic model , the topic is sampled from a per - author multinomialdistribution , θ , andauthorsaresampleduniformlyfromtheobserved list of the document’s authors . In the Author - Recipient - Topic model , there is a separate topic - distribution for each author - recipient pair , and the selection of topic - distributionisdeterminedfromtheobservedauthor , andbyuniformlysam - pling a recipient from the set of recipients for the document . its generative process for each document d , a set of authors , a d , is observed . To generate each word , an author x is chosen uniformly from this set , then a topic z is selected from a topic distribution θ x that is speciﬁc to the author , and then a word w is generated from a topic - speciﬁc multinomial distribution φ z . However , as described previously , none of these models is suitable for modeling message data . An email message has one sender and in general more than one recipients . We could treat both the sender and the recipients as “authors” of the message , and then employ the ATmodel , butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage , which is undesirable in many real - world situations . A manager may send email to a secretary and vice versa , but the nature of the requests and language used may be quite diﬀerent . Even more dramatically , consider the large quantity of junk email that we receive ; modeling the topicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotreﬂectourexpertiseorroles . Alternatively we could still employ the AT model by ignoring the recipient information of email and treating each email document as if it only has one author . However , in this case ( whichissimilartotheLDAmodel ) wearelosingallinformationabouttherecipients , and the connections between people implied by the sender - recipient relationships . 252 • LDA can be embedded in more complicated models , embodying further intuitions about the structure of the texts . • E . g . , it can be used in models that account for syntax , authorship , word sense , dynamics , correlation , hierarchies , and other structure . Extending LDA by emerging groups . Both modalities are driven by the common goal of increasing data likelihood . Consider the voting example again ; resolutions that would have been as - signed the same topic in a model using words alone may be assigned to diﬀerent topics if they exhibit distinct voting patterns . Distinct word - based topics may be merged if the entities vote very similarly on them . Likewise , multiple dif - ferent divisions of entities into groups are made possible by conditioning them on the topics . The importanceof modelingthe language associated with interactionsbetweenpeoplehasrecentlybeendemonstratedintheAuthor - Recipient - Topic ( ART ) model [ 16 ] . In ART the words in a message between people in a network are generated conditioned on the author , recipient and a set of topics that describes the message . The model thus cap - tures both the network structure within which the people interact as well as the language associated with the inter - actions . In experiments with Enron and academic email , the ART model is able to discover role similarity of people better than SNA models that consider network connectivity alone . However , the ART model does not explicitly capture groups formed by entities in the network . The GT model simultaneously clusters entities to groups and clusters words into topics , unlike models that gener - ate topics solely based on word distributions such as Latent Dirichlet Allocation [ 4 ] . In this way the GT model discov - ers salient topics relevant to relationships between entities in the social network—topics which the models that only examine words are unable to detect . We demonstrate the capabilities of the GT model by ap - plying it to two large sets of voting data : one from US Sen - ate and the other from the General Assembly of the UN . The model clusters voting entities into coalitions and si - multaneously discovers topics for word attributes describing the relations ( bills or resolutions ) between entities . We ﬁnd that the groups obtained from the GT model are signiﬁ - cantly more cohesive ( p - value < . 01 ) than those obtained from the Blockstructures model . The GT model also dis - covers new and more salient topics in both the UN and Sen - ate datasets—in comparison with topics discovered by only examining the words of the resolutions , the GT topics are either split or joined together as inﬂuenced by the voters’ patterns of behavior . 2 . GROUP - TOPIC MODEL TheGroup - TopicModelisadirectedgraphicalmodelthat clusters entities with relations between them , as well as at - tributes of those relations . The relations may be either di - rected or undirected and have multiple attributes . In this paper , we focus on undirected relations and have words as the attributes on relations . In the generative process for each event ( an interaction between entities ) , the model ﬁrst picks the topic t of the event and then generates all the words describing the event where each word is generated independently according to a multinomial distribution φ t , speciﬁc to the topic t . To generate the relational structure of the network , ﬁrst the group assignment , g st for each entity s is chosen condition - ally on the topic , from a particular multinomial distribution θ t overgroupsforeachtopic t . Giventhegroupassignments on an event b , the matrix V ( b ) is generated where each cell V ( b ) g i g j represents how often the groups of two senators be - haved the same or not during the event b , ( e . g . , voted the SYMBOL DESCRIPTION g it entity i ’s group assignment in topic t t b topic of an event b w ( b ) k the k th token in the event b V ( b ) ij entity i and j’s groups behaved same ( 1 ) or diﬀerently ( 2 ) on the event b S number of entities T number of topics G number of groups B number of events V number of unique words N b number of word tokens in the event b S b number of entities who participated in the event b Table 1 : Notation used in this paper ! " w v # $ t g % & N b S b 2 T BG 2 S T B Figure 1 : The Group - Topic model same or not on a bill ) . The elements of V are sampled from a binomial distribution γ ( b ) g i g j . Our notation is summarized in Table 1 , and the graphical model representation of the model is shown in Figure 1 . Without considering the topic of an event , or by treat - ing all events in a corpus as reﬂecting a single topic , the simpliﬁed model ( only the right part of Figure 1 ) becomes equivalent to the stochastic Blockstructures model [ 17 ] . To match the Blockstructures model , each event deﬁnes a re - lationship , e . g . , whether in the event two entities’ groups behave the same or not . On the other hand , in our model a relation may have multiple attributes ( which in our exper - iments are the words describing the event , generated by a per - topic multinomial ) . When we consider the complete model , the dataset is dy - namically divided into T sub - blocks each of which corre - sponds to a topic . The complete GT model is as follows , t b ∼ Uniform ( 1 T ) w it | φ t ∼ Multinomial ( φ t ) φ t | η ∼ Dirichlet ( η ) g it | θ t ∼ Multinomial ( θ t ) θ t | α ∼ Dirichlet ( α ) V ( b ) ij | γ ( b ) g i g j ∼ Binomial ( γ ( b ) g i g j ) γ ( b ) gh | β ∼ Beta ( β ) . We want to perform joint inference on ( text ) attributes andrelationstoobtaintopic - wisegroupmemberships . Since inferencecannotbedoneexactlyonsuchcomplicatedprob - abilisticgraphicalmodels , weemployGibbssamplingtocon - duct inference . Note that we adopt conjugate priors in our IndianBuffetProcessCompoundDirichletProcess B selects a subset of atoms for each distribution , and the gammarandomvariables φ determinetherelativemasses associatedwiththeseatoms . 2 . 4 . FocusedTopicModels Suppose H parametrizes distributions over words . Then , theICDdeﬁnesagenerativetopicmodel , whereitisused togenerateasetofsparsedistributionsoveraninﬁnitenum - ber of components , called “topics . ” Each topic is drawn fromaDirichletdistributionoverwords . Inordertospecify afullygenerativemodel , wesamplethenumberofwords for each document from a negative binomial distribution , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . 2 Thegenerativemodelfor M documentsis 1 . for k = 1 , 2 , . . . , ( a ) Samplethesticklength π k accordingtoEq . 1 . ( b ) Sampletherelativemass φ k ∼ Gamma ( γ , 1 ) . ( c ) Drawthetopicdistributionoverwords , β k ∼ Dirichlet ( η ) . 2 . for m = 1 , . . . , M , ( a ) Sampleabinaryvector b m accordingtoEq . 1 . ( b ) Drawthetotalnumberofwords , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . ( c ) Samplethedistributionovertopics , θ m ∼ Dirichlet ( b m · φ ) . ( d ) Foreachword w mi , i = 1 , . . . , n ( m ) · , i . Drawthetopicindex z mi ∼ Discrete ( θ m ) . ii . Drawtheword w mi ∼ Discrete ( β z mi ) . We call this the focused topic model ( FTM ) because the inﬁnite binary matrix B serves to focus the distribution overtopicsontoaﬁnitesubset ( seeFigure1 ) . Thenumber of topics within a single document is almost surely ﬁnite , thoughthetotalnumberoftopicsisunbounded . Thetopic distribution for the m th document , θ m , is drawn from a Dirichletdistributionoverthetopicsselectedby b m . The Dirichlet distribution models uncertainty about topic pro - portionswhilemaintainingtherestrictiontoasparsesetoftopics . TheICDmodelsthedistributionovertheglobaltopicpro - portionparameters φ separatelyfromthedistributionover thebinarymatrix B . Thiscapturestheideathatatopicmay appearinfrequentlyinacorpus , butmakeupahighpropor - tionofthosedocumentsinwhichitoccurs . Conversely , a topicmayappearfrequentlyinacorpus , butonlywithlow proportion . 2 Notation n ( m ) k is the number of words assigned to the k th topicofthe m thdocument , andweuseadotnotationtorepresent summation - i . e . n ( m ) · = P k n ( m ) k . Figure1 . Graphicalmodelforthefocusedtopicmodel 3 . RelatedModels Titsias ( 2007 ) introduced the inﬁnite gamma - Poisson pro - cess , a distribution over unbounded matrices of non - negativeintegers , anduseditasthebasisforatopicmodel of images . In this model , the distribution over features forthe m thimageisgivenbyaDirichletdistributionover the non - negative elements of the m th row of the inﬁnite gamma - Poisson process matrix , with parameters propor - tionaltothevaluesattheseelements . Whilethisresultsin asparsematrixofdistributions , thenumberofzeroentries in any column of the matrix is correlated with the values ofthenon - zeroentries . Columnswhichhaveentrieswith large values will not typically be sparse . Therefore , this modelwillnotdecoupleacross - dataprevalenceandwithin - dataproportionsoftopics . IntheICDthenumberofzero entries is controlled by a separate process , the IBP , from thevaluesofthenon - zeroentries , whicharecontrolledby thegammarandomvariables . The sparse topic model ( SparseTM , Wang & Blei , 2009 ) usesaﬁnitespikeandslabmodeltoensurethateachtopic is represented by a sparse distribution over words . The spikesaregeneratedbyBernoullidrawswithasingletopic - wideparameter . Thetopicdistributionisthendrawnfroma symmetricDirichletdistributiondeﬁnedoverthesespikes . The ICD also uses a spike and slab approach , but allows an unbounded number of “spikes” ( due to the IBP ) and a moregloballyinformative“slab” ( duetothesharedgamma randomvariables ) . WeextendtheSparseTM’sapproxima - tionoftheexpectationofaﬁnitemixtureofDirichletdis - tributions , toapproximatethemorecomplicatedmixtureof DirichletdistributionsgiveninEq . 2 . RecentworkbyFoxetal . ( 2009 ) usesdrawsfromanIBP toselectsubsetsofaninﬁnitesetofstates , tomodelmulti - pledynamicsystemswithsharedstates . ( Astateinthedy - namicsystemislikeacomponentinamixedmembershipmodel . ) Theprobabilityoftransitioningfromthe i thstate tothe j thstateinthe m thdynamicsystemisdrawnfroma Dirichletdistributionwithparameters b mj γ + τδ i , j , where Chang , Blei ! N d " d w d , n z d , n K # k y d , d ' $ N d ' " d ' w d ' , n z d ' , n Figure2 : Atwo - documentsegmentoftheRTM . Thevariable y indicateswhetherthetwodocumentsarelinked . Thecompletemodel containsthisvariableforeachpairofdocuments . Theplatesindicatereplication . Thismodelcapturesboththewordsandthelink structureofthedatashowninFigure1 . formulation , inspiredbythesupervisedLDAmodel ( Blei andMcAuliffe2007 ) , ensuresthatthesamelatenttopicas - signmentsusedtogeneratethecontentofthedocumentsalsogeneratestheirlinkstructure . Modelswhichdonot enforcethiscoupling , suchasNallapatietal . ( 2008 ) , might dividethetopicsintotwoindependentsubsets—oneforlinksandtheotherforwords . Suchadecompositionpre - ventsthesemodelsfrommakingmeaningfulpredictionsaboutlinksgivenwordsandwordsgivenlinks . InSec - tion4wedemonstrateempiricallythattheRTMoutper - formssuchmodelsonthesetasks . 3 INFERENCE , ESTIMATION , AND PREDICTION With the model deﬁned , we turn to approximate poste - riorinference , parameterestimation , andprediction . We developavariationalinferenceprocedureforapproximat - ingtheposterior . Weusethisprocedureinavariational expectation - maximization ( EM ) algorithm for parameter estimation . Finally , weshowhowamodelwhoseparame - tershavebeenestimatedcanbeusedasapredictivemodelofwordsandlinks . Inference In posterior inference , we seek to compute the posterior distribution of the latent variables condi - tionedontheobservations . Exactposteriorinferenceisin - tractable ( Bleietal . 2003 ; BleiandMcAuliffe2007 ) . We appealtovariationalmethods . Invariationalmethods , wepositafamilyofdistributions overthelatentvariablesindexedbyfreevariationalpa - rameters . Thoseparametersareﬁttobeclosetothetrue posterior , whereclosenessismeasuredbyrelativeentropy . SeeJordanetal . ( 1999 ) forareview . Weusethefully - factorizedfamily , q ( Θ , Z | γ , Φ ) = ￿ d [ q θ ( θ d | γ d ) ￿ n q z ( z d , n | φ d , n ) ] , ( 3 ) where γ isasetofDirichletparameters , oneforeachdoc - ument , and Φ isasetofmultinomialparameters , onefor eachwordineachdocument . Notethat E q [ z d , n ] = φ d , n . Minimizingtherelativeentropyisequivalenttomaximiz - ingtheJensen’slowerboundonthemarginalprobabilityoftheobservations , i . e . , theevidencelowerbound ( ELBO ) , L = ￿ ( d 1 , d 2 ) E q [ log p ( y d 1 , d 2 | z d 1 , z d 2 , η , ν ) ] + ￿ d ￿ n E q [ log p ( w d , n | β 1 : K , z d , n ) ] + ￿ d ￿ n E q [ log p ( z d , n | θ d ) ] + ￿ d E q [ log p ( θ d | α ) ] + H ( q ) , ( 4 ) where ( d 1 , d 2 ) denotesalldocumentpairs . Theﬁrstterm oftheELBOdifferentiatestheRTMfromLDA ( Bleietal . 2003 ) . Theconnectionsbetweendocumentsaffecttheob - jectiveinapproximateposteriorinference ( and , below , in parameterestimation ) . Wedeveloptheinferenceprocedureundertheassumptionthatonlyobservedlinkswillbemodeled ( i . e . , y d 1 , d 2 isei - ther 1 orunobserved ) . 1 Wedothisfortworeasons . First , whileonecanﬁx y d 1 , d 2 = 1 wheneveralinkisob - servedbetween d 1 and d 2 andset y d 1 , d 2 = 0 otherwise , this approachisinappropriateincorporawheretheabsenceofalinkcannotbeconstruedasevidencefor y d 1 , d 2 = 0 . In thesecases , treatingtheselinksasunobservedvariablesis morefaithfultotheunderlyingsemanticsofthedata . For example , inlargesocialnetworkssuchasFacebooktheab - senceofalinkbetweentwopeopledoesnotnecessarilymeanthattheyarenotfriends ; theymayberealfriends whoareunawareofeachother’sexistenceinthenetwork . Treatingthislinkasunobservedbetterrespectsourlackofknowledgeaboutthestatusoftheirrelationship . Second , treatingnon - linkslinksashiddendecreasesthe computationalcostofinference ; sincethelinkvariablesare leavesinthegraphicalmodeltheycanberemovedwhen - 1 Sumsoverdocumentpairs ( d 1 , d 2 ) areunderstoodtorange overpairsforwhichalinkhasbeenobserved . ! ! T " # k $ k % M & d ! D ' Parse trees grouped into M documents ( a ) OverallGraphicalModel w1 : laid w2 : phrases w6 : for w5 : his w4 : some w5 : mind w7 : years w3 : in z1 z2 z3 z4 z5 z5 z6 z7 ( b ) SentenceGraphicalModel Figure 1 : In the graphical model of the STM , a document is made up of a number of sentences , represented by a tree of latent topics z which in turn generate words w . These words’ topics are chosen by the topic of their parent ( as encoded by the tree ) , the topic weights for a document θ , and the node’s parent’s successor weights π . ( For clarity , not all dependencies of sentence nodes are shown . ) The structure of variables for sentences within the document plate is on the right , as demonstratedbyanautomaticparseofthesentence“Somephraseslaidinhismindforyears . ” The STMassumesthatthetreestructureandwordsaregiven , butthelatenttopics z arenot . is going to be a noun consistent as the object of the preposition “of . ” Thematically , because it is in a travel brochure , we would expect to see words such as “Acapulco , ” “Costa Rica , ” or “Australia” more than “kitchen , ” “debt , ” or “pocket . ” Our model can capture these kinds of regularities and exploittheminpredictiveproblems . Previouseffortstocapturelocalsyntacticcontextincludesemanticspacemodels [ 6 ] andsimilarity functions derived from dependency parses [ 7 ] . These methods successfully determine words that share similar contexts , but do not account for thematic consistency . They have difﬁculty with pol - ysemous words such as “ﬂy , ” which can be either an insect or a term from baseball . With a sense of document context , i . e . , a representation of whether a document is about sports or animals , the meaningofsuchtermscanbedistinguished . Other techniques have attempted to combine local context with document coherence using linear sequence models [ 8 , 9 ] . While these models are powerful , ordering words sequentially removes the important connections that are preserved in a syntactic parse . Moreover , these models gener - ate words either from the syntactic or thematic context . In the syntactic topic model , words are constrainedtobeconsistentwithboth . The remainder of this paper is organized as follows . We describe the syntactic topic model , and develop an approximate posterior inference technique based on variational methods . We study its performance both on synthetic data and hand parsed data [ 10 ] . We show that the STM captures relationshipsmissedbyothermodelsandachieveslowerheld - outperplexity . 2 The syntactic topic model Wedescribethesyntactictopicmodel ( STM ) , adocumentmodelthatcombinesobservedsyntactic structure and latent thematic structure . To motivate this model , we return to the travel brochure sentence “In the near future , you could ﬁnd yourself in . ” . The word that ﬁlls in the blank is constrainedbyitssyntacticcontextanditsdocumentcontext . Thesyntacticcontexttellsusthatitis anobjectofapreposition , andthedocumentcontexttellsusthatitisatravel - relatedword . The STM attempts to capture these joint inﬂuences on words . It models a document corpus as exchangeable collections of sentences , each of which is associated with a tree structure such as a 2 This provides an inferential speed - up that makes it possibletoﬁtmodelsatvaryinggranularities . Asex - amples , journalarticlesmightbeexchangeablewithin anissue , anassumptionwhichismorerealisticthan onewheretheyareexchangeablebyyear . Otherdata , suchasnews , mightexperienceperiodsoftimewithout anyobservation . WhilethedDTMrequiresrepresent - ingalltopicsforthediscretetickswithintheseperiods , thecDTMcananalyzesuchdatawithoutasacriﬁceofmemoryorspeed . WiththecDTM , thegranularity canbechosentomaximizemodelﬁtnessratherthantolimitcomputationalcomplexity . WenotethatthecDTManddDTMarenottheonlytopicmodelstotaketimeintoconsideration . Topics over time models ( TOT ) [ 23 ] and dynamic mixture models ( DMM ) [ 25 ] also include timestamps in the analysis of documents . The TOT model treats the timestampsasobservationsofthelatenttopics , while DMMassumesthatthetopicmixtureproportionsofeachdocumentisdependentonprevioustopicmix - tureproportions . InbothTOTandDMM , thetopics themselvesare constant , andthetimeinformationis usedtobetterdiscoverthem . Inthesettinghere , we areinterestedininferringevolvingtopics . Therestofthepaperisorganizedasfollows . Insec - tion2wedescribethedDTManddevelopthecDTMindetail . Section3presentsaneﬃcientposteriorin - ferencealgorithmforthecDTMbasedonsparsevaria - tionalmethods . Insection4 , wepresentexperimental resultsontwonewscorpora . 2 Continuoustimedynamictopic models Inatimestampeddocumentcollection , wewouldlike to model its latent topics as changing through the courseofthecollection . Innewsdata , forexample , a singletopicwillchangeasthestoriesassociatedwith it develop . The discrete - time dynamic topic model ( dDTM ) builds on the exchangeable topic model to providesuchmachinery [ 2 ] . InthedDTM , documents aredividedintosequentialgroups , andthetopicsof eachsliceevolvefromthetopicsofthepreviousslice . Documentsinagroupareassumedexchangeable . Morespeciﬁcally , atopicisrepresentedasadistribu - tionovertheﬁxedvocabularyofthecollection . The dDTMassumesthatadiscrete - timestatespacemodel governstheevolutionofthenaturalparametersofthemultinomialdistributionsthatrepresentthetopics . ( Recall that the natural parameters of the multino - mial are the logs of the probabilities of each item . ) Thisisatime - seriesextensiontothelogisticnormal distribution [ 26 ] . Figure 1 : Graphical model representation of the cDTM . The evolution of the topic parameters β t is governedbyBrownianmotion . Thevariable s t isthe observedtimestampofdocument d t . AdrawbackofthedDTMisthattimeisdiscretized . Iftheresolutionischosentobetoocoarse , thenthe assumptionthatdocumentswithinatimestepareex - changeablewillnotbetrue . Iftheresolutionistoo ﬁne , thenthenumberofvariationalparameterswillex - plodeasmoretimepointsareadded . Choosingthedis - cretizationshouldbeadecisionbasedonassumptionsaboutthedata . However , thecomputationalconcerns mightpreventanalysisattheappropriatetimescale . Thus , wedevelopthecontinuoustimedynamictopic model ( cDTM ) for modeling sequential time - series datawitharbitrarygranularity . ThecDTMcanbe seenasanaturallimitofthedDTMatitsﬁnestpos - sibleresolution , theresolutionatwhichthedocument timestampsaremeasured . InthecDTM , westillrepresenttopicsintheirnatural parameterization , butweuseBrownianmotion [ 14 ] to modeltheirevolutionthroughtime . Let i , j ( j > i > 0 ) betwoarbitrarytime indexes , s i and s j bethetime stamps , and∆ s j , s i betheelapsedtimebetweenthem . Ina K - topiccDTMmodel , thedistributionofthe k th ( 1 ≤ k ≤ K ) topic’sparameteratterm w is : β 0 , k , w ∼ N ( m , v 0 ) β j , k , w | β i , k , w , s ∼ N ￿ β i , k , w , v ∆ s j , s i ￿ , ( 1 ) wherethevarianceincreaseslinearlywiththelag . Thisconstructionisusedasacomponentinthefullgenerativeprocess . ( Note : if j = i + 1 , wewrite∆ s j , s i as∆ s j forshort . ) 1 . Foreachtopic k , 1 ≤ k ≤ K , ( a ) Draw β 0 , k ∼N ( m , v 0 I ) . ( a ) ( b ) Figure 1 : ( a ) LDA model . ( b ) MG - LDA model . isstillnot directlydependentonthenumberofdocuments and , therefore , themodelisnotexpectedtosuﬀerfromover - ﬁtting . Anotherapproach is to use a Markov chain Monte CarloalgorithmforinferencewithLDA , asproposedin [ 14 ] . Insection3wewilldescribeamodiﬁcationofthissamplingmethodfortheproposedMulti - grainLDAmodel . BothLDAandPLSAmethodsusethebag - of - wordsrep - resentation of documents , therefore they can only explore co - occurrencesatthedocumentlevel . Thisisﬁne , provided the goal is to represent an overall topic of the document , but our goal is diﬀerent : extracting ratable aspects . The main topic of all the reviews for a particular item is virtu - ally thesame : areviewof thisitem . Therefore , whensuch topic modeling methods are applied to a collection of re - viewsfordiﬀerentitems , theyinfertopicscorrespondingto distinguishingpropertiesoftheseitems . E . g . whenapplied toacollectionofhotelreviews , thesemodelsarelikelytoin - fertopics : hotels inFrance , New Yorkhotels , youthhostels , or , similarly , when applied to a collection of Mp3 players’ reviews , these models will infer topics like reviews of iPod or reviewsofCreativeZenplayer . Thoughtheseareallvalid topics , theydonotrepresentratableaspects , butratherde - ﬁneclusteringsoftherevieweditemsintospeciﬁctypes . In furtherdiscussionwewillrefertosuchtopicsas global topics , because theycorrespond toaglobal propertyof theobject in the review , such as its brand or base of operation . Dis - covering topics thatcorrelate with ratable aspects , such as cleanliness and location for hotels , is much more problem - aticwithLDAorPLSAmethods . Most ofthesetopicsare presentinsomewayineveryreview . Therefore , itisdiﬃcult todiscoverthembyusingonlyco - occurrenceinformationat thedocumentlevel . Inthiscaseexceedinglylargeamounts of trainingdataisneededandaswell asaverylargenum - ber of topics K . Even in this case there is a danger that themodelwillbeoverﬂownbyveryﬁne - grainglobaltopics or the resulting topics will be intersection of global topics and ratable aspects , like location for hotels in New York . WewillshowinSection4thatthishypothesisisconﬁrmedexperimentally . Onewaytoaddressthisproblemwouldbetoconsiderco - occurrencesatthesentencelevel , i . e . , applyLDAorPLSAto individualsentences . Butinthiscasewewillnothaveasuf - ﬁcientco - occurrencedomain , anditisknownthatLDAand PLSAbehavebadlywhenappliedtoveryshortdocuments . Thisproblemcanbeaddressedbyexplicitlymodelingtopictransitions [ 5 , 15 , 33 , 32 , 28 , 16 ] , but these topic n - gram models are considerably more computationally expensive . Also , like LDA and PLSA , they will not be able to distin - guish between topics corresponding to ratable aspects and global topics representing properties of the reviewed item . In the following section we will introduce a method which explicitly models both types of topics and eﬃciently infers ratableaspectsfromlimitedamountoftrainingdata . 2 . 2 MG - LDA WeproposeamodelcalledMulti - grainLDA ( MG - LDA ) , whichmodelstwodistincttypesoftopics : globaltopicsand localtopics . AsinPLSAandLDA , thedistributionofglobal topicsisﬁxedforadocument . However , thedistributionof localtopicsisallowedtovaryacrossthedocument . Aword inthedocumentissampledeitherfromthemixtureofglobaltopicsorfromthemixtureoflocaltopicsspeciﬁcforthelocalcontextoftheword . The hypothesis is that ratable aspects will be captured by local topics and global topics willcapturepropertiesofrevieweditems . Forexamplecon - sideranextractfromareviewofaLondonhotel : “ . . . public transport in London is straightforward , the tubestation is aboutan8minutewalk . . . oryoucangetabusfor £ 1 . 50” . It can be viewed as a mixture of topic London shared by the entire review ( words : “London” , “tube” , “ £ ” ) , and the ratable aspect location , speciﬁc for thelocal context of the sentence ( words : “transport” , “walk” , “bus” ) . Local topics are expected to be reused between very diﬀerent types of items , whereasglobaltopicswillcorrespondonlytopartic - ular typesof items . In order to captureonly genuinelocal topics , weallowalargenumberofglobaltopics , eﬀectively , creatingabottleneckattheleveloflocaltopics . Ofcourse , this bottleneck is speciﬁc to our purposes . Other applica - tions of multi - grain topic models conceivably might prefer thebottleneckreversed . Finally , wenotethatourdeﬁnition of multi - grain is simply for two - levels of granularity , global and local . In principle though , there is nothing preventing themodel described in this section from extendingbeyond twolevels . Onemightexpectthatforothertasksevenmore levelsofgranularitycouldbebeneﬁcial . Werepresentadocumentasasetofslidingwindows , each covering T adjacent sentenceswithin it . Each window v in document d hasanassociated distributionoverlocaltopics θ locd , v and a distribution deﬁning preference for local topics versusglobaltopics π d , v . Awordcanbesampledusingany windowcoveringitssentence s , wherethewindowischosen accordingtoacategoricaldistribution ψ s . Importantly , the fact that the windows overlap , permits to exploit a larger co - occurrencedomain . Thesesimpletechniquesarecapable ofmodelinglocaltopicswithoutmoreexpensivemodelingof topicstransitionsusedin [ 5 , 15 , 33 , 32 , 28 , 16 ] . Introduction ofasymmetricalDirichletprior Dir ( γ ) forthedistribution ψ s permitstocontrolsmoothnessoftopictransitionsinour model . The formal deﬁnition of the model with K gl global and K loc local topics is the following . First , draw K gl word distributions for global topics ϕ glz from a Dirichlet prior Dir ( β gl ) and K loc word distributions for local topics ϕ locz ! from Dir ( β loc ) . Then , foreachdocument d : • Chooseadistributionofglobaltopics θ gld ∼ Dir ( α gl ) . • For each sentence s choose a distribution ψ d , s ( v ) ∼ Dir ( γ ) . • Foreachslidingwindow v 113 WWW 2008 / Refereed Track : Data Mining - Modeling April 21 - 25 , 2008 · Beijing , China McCallum , Wang , & Corrada - Emmanuel ! " z w Latent Dirichlet Allocation ( LDA ) [ Blei , Ng , Jordan , 2003 ] N D x z w Author - Topic Model ( AT ) [ Rosen - Zvi , Grifﬁths , Steyvers , Smyth 2004 ] N D " # ! $ T A # $ T x z w Author - Recipient - Topic Model ( ART ) [ This paper ] N D " # ! $ T A , A z w Author Model ( Multi - label Mixture Model ) [ McCallum 1999 ] N D # $ A a d a d r d a d d d d d Figure 1 : Three related models , and the ART model . In all models , each observed word , w , is generated from a multinomial word distribution , φ z , speciﬁc to a particular topic / author , z , however topics are selected diﬀerently in each of the models . In LDA , the topic is sampled from a per - document topic distribution , θ , which in turn is sampled from a Dirichlet over topics . In the Author Model , there is one topic associated with each author ( or category ) , and authors are sampled uniformly . In the Author - Topic model , the topic is sampled from a per - author multinomialdistribution , θ , andauthorsaresampleduniformlyfromtheobserved list of the document’s authors . In the Author - Recipient - Topic model , there is a separate topic - distribution for each author - recipient pair , and the selection of topic - distributionisdeterminedfromtheobservedauthor , andbyuniformlysam - pling a recipient from the set of recipients for the document . its generative process for each document d , a set of authors , a d , is observed . To generate each word , an author x is chosen uniformly from this set , then a topic z is selected from a topic distribution θ x that is speciﬁc to the author , and then a word w is generated from a topic - speciﬁc multinomial distribution φ z . However , as described previously , none of these models is suitable for modeling message data . An email message has one sender and in general more than one recipients . We could treat both the sender and the recipients as “authors” of the message , and then employ the ATmodel , butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage , which is undesirable in many real - world situations . A manager may send email to a secretary and vice versa , but the nature of the requests and language used may be quite diﬀerent . Even more dramatically , consider the large quantity of junk email that we receive ; modeling the topicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotreﬂectourexpertiseorroles . Alternatively we could still employ the AT model by ignoring the recipient information of email and treating each email document as if it only has one author . However , in this case ( whichissimilartotheLDAmodel ) wearelosingallinformationabouttherecipients , and the connections between people implied by the sender - recipient relationships . 252 • The data generating distribution can be changed . We can apply mixed - membership assumptions to many kinds of data . • E . g . , we can build models of images , social networks , music , purchase histories , computer code , genetic data , and other types . Extending LDA by emerging groups . Both modalities are driven by the common goal of increasing data likelihood . Consider the voting example again ; resolutions that would have been as - signed the same topic in a model using words alone may be assigned to diﬀerent topics if they exhibit distinct voting patterns . Distinct word - based topics may be merged if the entities vote very similarly on them . Likewise , multiple dif - ferent divisions of entities into groups are made possible by conditioning them on the topics . The importanceof modelingthe language associated with interactionsbetweenpeoplehasrecentlybeendemonstratedintheAuthor - Recipient - Topic ( ART ) model [ 16 ] . In ART the words in a message between people in a network are generated conditioned on the author , recipient and a set of topics that describes the message . The model thus cap - tures both the network structure within which the people interact as well as the language associated with the inter - actions . In experiments with Enron and academic email , the ART model is able to discover role similarity of people better than SNA models that consider network connectivity alone . However , the ART model does not explicitly capture groups formed by entities in the network . The GT model simultaneously clusters entities to groups and clusters words into topics , unlike models that gener - ate topics solely based on word distributions such as Latent Dirichlet Allocation [ 4 ] . In this way the GT model discov - ers salient topics relevant to relationships between entities in the social network—topics which the models that only examine words are unable to detect . We demonstrate the capabilities of the GT model by ap - plying it to two large sets of voting data : one from US Sen - ate and the other from the General Assembly of the UN . The model clusters voting entities into coalitions and si - multaneously discovers topics for word attributes describing the relations ( bills or resolutions ) between entities . We ﬁnd that the groups obtained from the GT model are signiﬁ - cantly more cohesive ( p - value < . 01 ) than those obtained from the Blockstructures model . The GT model also dis - covers new and more salient topics in both the UN and Sen - ate datasets—in comparison with topics discovered by only examining the words of the resolutions , the GT topics are either split or joined together as inﬂuenced by the voters’ patterns of behavior . 2 . GROUP - TOPIC MODEL TheGroup - TopicModelisadirectedgraphicalmodelthat clusters entities with relations between them , as well as at - tributes of those relations . The relations may be either di - rected or undirected and have multiple attributes . In this paper , we focus on undirected relations and have words as the attributes on relations . In the generative process for each event ( an interaction between entities ) , the model ﬁrst picks the topic t of the event and then generates all the words describing the event where each word is generated independently according to a multinomial distribution φ t , speciﬁc to the topic t . To generate the relational structure of the network , ﬁrst the group assignment , g st for each entity s is chosen condition - ally on the topic , from a particular multinomial distribution θ t overgroupsforeachtopic t . Giventhegroupassignments on an event b , the matrix V ( b ) is generated where each cell V ( b ) g i g j represents how often the groups of two senators be - haved the same or not during the event b , ( e . g . , voted the SYMBOL DESCRIPTION g it entity i ’s group assignment in topic t t b topic of an event b w ( b ) k the k th token in the event b V ( b ) ij entity i and j’s groups behaved same ( 1 ) or diﬀerently ( 2 ) on the event b S number of entities T number of topics G number of groups B number of events V number of unique words N b number of word tokens in the event b S b number of entities who participated in the event b Table 1 : Notation used in this paper ! " w v # $ t g % & N b S b 2 T BG 2 S T B Figure 1 : The Group - Topic model same or not on a bill ) . The elements of V are sampled from a binomial distribution γ ( b ) g i g j . Our notation is summarized in Table 1 , and the graphical model representation of the model is shown in Figure 1 . Without considering the topic of an event , or by treat - ing all events in a corpus as reﬂecting a single topic , the simpliﬁed model ( only the right part of Figure 1 ) becomes equivalent to the stochastic Blockstructures model [ 17 ] . To match the Blockstructures model , each event deﬁnes a re - lationship , e . g . , whether in the event two entities’ groups behave the same or not . On the other hand , in our model a relation may have multiple attributes ( which in our exper - iments are the words describing the event , generated by a per - topic multinomial ) . When we consider the complete model , the dataset is dy - namically divided into T sub - blocks each of which corre - sponds to a topic . The complete GT model is as follows , t b ∼ Uniform ( 1 T ) w it | φ t ∼ Multinomial ( φ t ) φ t | η ∼ Dirichlet ( η ) g it | θ t ∼ Multinomial ( θ t ) θ t | α ∼ Dirichlet ( α ) V ( b ) ij | γ ( b ) g i g j ∼ Binomial ( γ ( b ) g i g j ) γ ( b ) gh | β ∼ Beta ( β ) . We want to perform joint inference on ( text ) attributes andrelationstoobtaintopic - wisegroupmemberships . Since inferencecannotbedoneexactlyonsuchcomplicatedprob - abilisticgraphicalmodels , weemployGibbssamplingtocon - duct inference . Note that we adopt conjugate priors in our IndianBuffetProcessCompoundDirichletProcess B selects a subset of atoms for each distribution , and the gammarandomvariables φ determinetherelativemasses associatedwiththeseatoms . 2 . 4 . FocusedTopicModels Suppose H parametrizes distributions over words . Then , theICDdeﬁnesagenerativetopicmodel , whereitisused togenerateasetofsparsedistributionsoveraninﬁnitenum - ber of components , called “topics . ” Each topic is drawn fromaDirichletdistributionoverwords . Inordertospecify afullygenerativemodel , wesamplethenumberofwords for each document from a negative binomial distribution , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . 2 Thegenerativemodelfor M documentsis 1 . for k = 1 , 2 , . . . , ( a ) Samplethesticklength π k accordingtoEq . 1 . ( b ) Sampletherelativemass φ k ∼ Gamma ( γ , 1 ) . ( c ) Drawthetopicdistributionoverwords , β k ∼ Dirichlet ( η ) . 2 . for m = 1 , . . . , M , ( a ) Sampleabinaryvector b m accordingtoEq . 1 . ( b ) Drawthetotalnumberofwords , n ( m ) · ∼ NB ( ￿ k b mk φ k , 1 / 2 ) . ( c ) Samplethedistributionovertopics , θ m ∼ Dirichlet ( b m · φ ) . ( d ) Foreachword w mi , i = 1 , . . . , n ( m ) · , i . Drawthetopicindex z mi ∼ Discrete ( θ m ) . ii . Drawtheword w mi ∼ Discrete ( β z mi ) . We call this the focused topic model ( FTM ) because the inﬁnite binary matrix B serves to focus the distribution overtopicsontoaﬁnitesubset ( seeFigure1 ) . Thenumber of topics within a single document is almost surely ﬁnite , thoughthetotalnumberoftopicsisunbounded . Thetopic distribution for the m th document , θ m , is drawn from a Dirichletdistributionoverthetopicsselectedby b m . The Dirichlet distribution models uncertainty about topic pro - portionswhilemaintainingtherestrictiontoasparsesetoftopics . TheICDmodelsthedistributionovertheglobaltopicpro - portionparameters φ separatelyfromthedistributionover thebinarymatrix B . Thiscapturestheideathatatopicmay appearinfrequentlyinacorpus , butmakeupahighpropor - tionofthosedocumentsinwhichitoccurs . Conversely , a topicmayappearfrequentlyinacorpus , butonlywithlow proportion . 2 Notation n ( m ) k is the number of words assigned to the k th topicofthe m thdocument , andweuseadotnotationtorepresent summation - i . e . n ( m ) · = P k n ( m ) k . Figure1 . Graphicalmodelforthefocusedtopicmodel 3 . RelatedModels Titsias ( 2007 ) introduced the inﬁnite gamma - Poisson pro - cess , a distribution over unbounded matrices of non - negativeintegers , anduseditasthebasisforatopicmodel of images . In this model , the distribution over features forthe m thimageisgivenbyaDirichletdistributionover the non - negative elements of the m th row of the inﬁnite gamma - Poisson process matrix , with parameters propor - tionaltothevaluesattheseelements . Whilethisresultsin asparsematrixofdistributions , thenumberofzeroentries in any column of the matrix is correlated with the values ofthenon - zeroentries . Columnswhichhaveentrieswith large values will not typically be sparse . Therefore , this modelwillnotdecoupleacross - dataprevalenceandwithin - dataproportionsoftopics . IntheICDthenumberofzero entries is controlled by a separate process , the IBP , from thevaluesofthenon - zeroentries , whicharecontrolledby thegammarandomvariables . The sparse topic model ( SparseTM , Wang & Blei , 2009 ) usesaﬁnitespikeandslabmodeltoensurethateachtopic is represented by a sparse distribution over words . The spikesaregeneratedbyBernoullidrawswithasingletopic - wideparameter . Thetopicdistributionisthendrawnfroma symmetricDirichletdistributiondeﬁnedoverthesespikes . The ICD also uses a spike and slab approach , but allows an unbounded number of “spikes” ( due to the IBP ) and a moregloballyinformative“slab” ( duetothesharedgamma randomvariables ) . WeextendtheSparseTM’sapproxima - tionoftheexpectationofaﬁnitemixtureofDirichletdis - tributions , toapproximatethemorecomplicatedmixtureof DirichletdistributionsgiveninEq . 2 . RecentworkbyFoxetal . ( 2009 ) usesdrawsfromanIBP toselectsubsetsofaninﬁnitesetofstates , tomodelmulti - pledynamicsystemswithsharedstates . ( Astateinthedy - namicsystemislikeacomponentinamixedmembershipmodel . ) Theprobabilityoftransitioningfromthe i thstate tothe j thstateinthe m thdynamicsystemisdrawnfroma Dirichletdistributionwithparameters b mj γ + τδ i , j , where Chang , Blei ! N d " d w d , n z d , n K # k y d , d ' $ N d ' " d ' w d ' , n z d ' , n Figure2 : Atwo - documentsegmentoftheRTM . Thevariable y indicateswhetherthetwodocumentsarelinked . Thecompletemodel containsthisvariableforeachpairofdocuments . Theplatesindicatereplication . Thismodelcapturesboththewordsandthelink structureofthedatashowninFigure1 . formulation , inspiredbythesupervisedLDAmodel ( Blei andMcAuliffe2007 ) , ensuresthatthesamelatenttopicas - signmentsusedtogeneratethecontentofthedocumentsalsogeneratestheirlinkstructure . Modelswhichdonot enforcethiscoupling , suchasNallapatietal . ( 2008 ) , might dividethetopicsintotwoindependentsubsets—oneforlinksandtheotherforwords . Suchadecompositionpre - ventsthesemodelsfrommakingmeaningfulpredictionsaboutlinksgivenwordsandwordsgivenlinks . InSec - tion4wedemonstrateempiricallythattheRTMoutper - formssuchmodelsonthesetasks . 3 INFERENCE , ESTIMATION , AND PREDICTION With the model deﬁned , we turn to approximate poste - riorinference , parameterestimation , andprediction . We developavariationalinferenceprocedureforapproximat - ingtheposterior . Weusethisprocedureinavariational expectation - maximization ( EM ) algorithm for parameter estimation . Finally , weshowhowamodelwhoseparame - tershavebeenestimatedcanbeusedasapredictivemodelofwordsandlinks . Inference In posterior inference , we seek to compute the posterior distribution of the latent variables condi - tionedontheobservations . Exactposteriorinferenceisin - tractable ( Bleietal . 2003 ; BleiandMcAuliffe2007 ) . We appealtovariationalmethods . Invariationalmethods , wepositafamilyofdistributions overthelatentvariablesindexedbyfreevariationalpa - rameters . Thoseparametersareﬁttobeclosetothetrue posterior , whereclosenessismeasuredbyrelativeentropy . SeeJordanetal . ( 1999 ) forareview . Weusethefully - factorizedfamily , q ( Θ , Z | γ , Φ ) = ￿ d [ q θ ( θ d | γ d ) ￿ n q z ( z d , n | φ d , n ) ] , ( 3 ) where γ isasetofDirichletparameters , oneforeachdoc - ument , and Φ isasetofmultinomialparameters , onefor eachwordineachdocument . Notethat E q [ z d , n ] = φ d , n . Minimizingtherelativeentropyisequivalenttomaximiz - ingtheJensen’slowerboundonthemarginalprobabilityoftheobservations , i . e . , theevidencelowerbound ( ELBO ) , L = ￿ ( d 1 , d 2 ) E q [ log p ( y d 1 , d 2 | z d 1 , z d 2 , η , ν ) ] + ￿ d ￿ n E q [ log p ( w d , n | β 1 : K , z d , n ) ] + ￿ d ￿ n E q [ log p ( z d , n | θ d ) ] + ￿ d E q [ log p ( θ d | α ) ] + H ( q ) , ( 4 ) where ( d 1 , d 2 ) denotesalldocumentpairs . Theﬁrstterm oftheELBOdifferentiatestheRTMfromLDA ( Bleietal . 2003 ) . Theconnectionsbetweendocumentsaffecttheob - jectiveinapproximateposteriorinference ( and , below , in parameterestimation ) . Wedeveloptheinferenceprocedureundertheassumptionthatonlyobservedlinkswillbemodeled ( i . e . , y d 1 , d 2 isei - ther 1 orunobserved ) . 1 Wedothisfortworeasons . First , whileonecanﬁx y d 1 , d 2 = 1 wheneveralinkisob - servedbetween d 1 and d 2 andset y d 1 , d 2 = 0 otherwise , this approachisinappropriateincorporawheretheabsenceofalinkcannotbeconstruedasevidencefor y d 1 , d 2 = 0 . In thesecases , treatingtheselinksasunobservedvariablesis morefaithfultotheunderlyingsemanticsofthedata . For example , inlargesocialnetworkssuchasFacebooktheab - senceofalinkbetweentwopeopledoesnotnecessarilymeanthattheyarenotfriends ; theymayberealfriends whoareunawareofeachother’sexistenceinthenetwork . Treatingthislinkasunobservedbetterrespectsourlackofknowledgeaboutthestatusoftheirrelationship . Second , treatingnon - linkslinksashiddendecreasesthe computationalcostofinference ; sincethelinkvariablesare leavesinthegraphicalmodeltheycanberemovedwhen - 1 Sumsoverdocumentpairs ( d 1 , d 2 ) areunderstoodtorange overpairsforwhichalinkhasbeenobserved . ! ! T " # k $ k % M & d ! D ' Parse trees grouped into M documents ( a ) OverallGraphicalModel w1 : laid w2 : phrases w6 : for w5 : his w4 : some w5 : mind w7 : years w3 : in z1 z2 z3 z4 z5 z5 z6 z7 ( b ) SentenceGraphicalModel Figure 1 : In the graphical model of the STM , a document is made up of a number of sentences , represented by a tree of latent topics z which in turn generate words w . These words’ topics are chosen by the topic of their parent ( as encoded by the tree ) , the topic weights for a document θ , and the node’s parent’s successor weights π . ( For clarity , not all dependencies of sentence nodes are shown . ) The structure of variables for sentences within the document plate is on the right , as demonstratedbyanautomaticparseofthesentence“Somephraseslaidinhismindforyears . ” The STMassumesthatthetreestructureandwordsaregiven , butthelatenttopics z arenot . is going to be a noun consistent as the object of the preposition “of . ” Thematically , because it is in a travel brochure , we would expect to see words such as “Acapulco , ” “Costa Rica , ” or “Australia” more than “kitchen , ” “debt , ” or “pocket . ” Our model can capture these kinds of regularities and exploittheminpredictiveproblems . Previouseffortstocapturelocalsyntacticcontextincludesemanticspacemodels [ 6 ] andsimilarity functions derived from dependency parses [ 7 ] . These methods successfully determine words that share similar contexts , but do not account for thematic consistency . They have difﬁculty with pol - ysemous words such as “ﬂy , ” which can be either an insect or a term from baseball . With a sense of document context , i . e . , a representation of whether a document is about sports or animals , the meaningofsuchtermscanbedistinguished . Other techniques have attempted to combine local context with document coherence using linear sequence models [ 8 , 9 ] . While these models are powerful , ordering words sequentially removes the important connections that are preserved in a syntactic parse . Moreover , these models gener - ate words either from the syntactic or thematic context . In the syntactic topic model , words are constrainedtobeconsistentwithboth . The remainder of this paper is organized as follows . We describe the syntactic topic model , and develop an approximate posterior inference technique based on variational methods . We study its performance both on synthetic data and hand parsed data [ 10 ] . We show that the STM captures relationshipsmissedbyothermodelsandachieveslowerheld - outperplexity . 2 The syntactic topic model Wedescribethesyntactictopicmodel ( STM ) , adocumentmodelthatcombinesobservedsyntactic structure and latent thematic structure . To motivate this model , we return to the travel brochure sentence “In the near future , you could ﬁnd yourself in . ” . The word that ﬁlls in the blank is constrainedbyitssyntacticcontextanditsdocumentcontext . Thesyntacticcontexttellsusthatitis anobjectofapreposition , andthedocumentcontexttellsusthatitisatravel - relatedword . The STM attempts to capture these joint inﬂuences on words . It models a document corpus as exchangeable collections of sentences , each of which is associated with a tree structure such as a 2 This provides an inferential speed - up that makes it possibletoﬁtmodelsatvaryinggranularities . Asex - amples , journalarticlesmightbeexchangeablewithin anissue , anassumptionwhichismorerealisticthan onewheretheyareexchangeablebyyear . Otherdata , suchasnews , mightexperienceperiodsoftimewithout anyobservation . WhilethedDTMrequiresrepresent - ingalltopicsforthediscretetickswithintheseperiods , thecDTMcananalyzesuchdatawithoutasacriﬁceofmemoryorspeed . WiththecDTM , thegranularity canbechosentomaximizemodelﬁtnessratherthantolimitcomputationalcomplexity . WenotethatthecDTManddDTMarenottheonlytopicmodelstotaketimeintoconsideration . Topics over time models ( TOT ) [ 23 ] and dynamic mixture models ( DMM ) [ 25 ] also include timestamps in the analysis of documents . The TOT model treats the timestampsasobservationsofthelatenttopics , while DMMassumesthatthetopicmixtureproportionsofeachdocumentisdependentonprevioustopicmix - tureproportions . InbothTOTandDMM , thetopics themselvesare constant , andthetimeinformationis usedtobetterdiscoverthem . Inthesettinghere , we areinterestedininferringevolvingtopics . Therestofthepaperisorganizedasfollows . Insec - tion2wedescribethedDTManddevelopthecDTMindetail . Section3presentsaneﬃcientposteriorin - ferencealgorithmforthecDTMbasedonsparsevaria - tionalmethods . Insection4 , wepresentexperimental resultsontwonewscorpora . 2 Continuoustimedynamictopic models Inatimestampeddocumentcollection , wewouldlike to model its latent topics as changing through the courseofthecollection . Innewsdata , forexample , a singletopicwillchangeasthestoriesassociatedwith it develop . The discrete - time dynamic topic model ( dDTM ) builds on the exchangeable topic model to providesuchmachinery [ 2 ] . InthedDTM , documents aredividedintosequentialgroups , andthetopicsof eachsliceevolvefromthetopicsofthepreviousslice . Documentsinagroupareassumedexchangeable . Morespeciﬁcally , atopicisrepresentedasadistribu - tionovertheﬁxedvocabularyofthecollection . The dDTMassumesthatadiscrete - timestatespacemodel governstheevolutionofthenaturalparametersofthemultinomialdistributionsthatrepresentthetopics . ( Recall that the natural parameters of the multino - mial are the logs of the probabilities of each item . ) Thisisatime - seriesextensiontothelogisticnormal distribution [ 26 ] . Figure 1 : Graphical model representation of the cDTM . The evolution of the topic parameters β t is governedbyBrownianmotion . Thevariable s t isthe observedtimestampofdocument d t . AdrawbackofthedDTMisthattimeisdiscretized . Iftheresolutionischosentobetoocoarse , thenthe assumptionthatdocumentswithinatimestepareex - changeablewillnotbetrue . Iftheresolutionistoo ﬁne , thenthenumberofvariationalparameterswillex - plodeasmoretimepointsareadded . Choosingthedis - cretizationshouldbeadecisionbasedonassumptionsaboutthedata . However , thecomputationalconcerns mightpreventanalysisattheappropriatetimescale . Thus , wedevelopthecontinuoustimedynamictopic model ( cDTM ) for modeling sequential time - series datawitharbitrarygranularity . ThecDTMcanbe seenasanaturallimitofthedDTMatitsﬁnestpos - sibleresolution , theresolutionatwhichthedocument timestampsaremeasured . InthecDTM , westillrepresenttopicsintheirnatural parameterization , butweuseBrownianmotion [ 14 ] to modeltheirevolutionthroughtime . Let i , j ( j > i > 0 ) betwoarbitrarytime indexes , s i and s j bethetime stamps , and∆ s j , s i betheelapsedtimebetweenthem . Ina K - topiccDTMmodel , thedistributionofthe k th ( 1 ≤ k ≤ K ) topic’sparameteratterm w is : β 0 , k , w ∼ N ( m , v 0 ) β j , k , w | β i , k , w , s ∼ N ￿ β i , k , w , v ∆ s j , s i ￿ , ( 1 ) wherethevarianceincreaseslinearlywiththelag . Thisconstructionisusedasacomponentinthefullgenerativeprocess . ( Note : if j = i + 1 , wewrite∆ s j , s i as∆ s j forshort . ) 1 . Foreachtopic k , 1 ≤ k ≤ K , ( a ) Draw β 0 , k ∼N ( m , v 0 I ) . ( a ) ( b ) Figure 1 : ( a ) LDA model . ( b ) MG - LDA model . isstillnot directlydependentonthenumberofdocuments and , therefore , themodelisnotexpectedtosuﬀerfromover - ﬁtting . Anotherapproach is to use a Markov chain Monte CarloalgorithmforinferencewithLDA , asproposedin [ 14 ] . Insection3wewilldescribeamodiﬁcationofthissamplingmethodfortheproposedMulti - grainLDAmodel . BothLDAandPLSAmethodsusethebag - of - wordsrep - resentation of documents , therefore they can only explore co - occurrencesatthedocumentlevel . Thisisﬁne , provided the goal is to represent an overall topic of the document , but our goal is diﬀerent : extracting ratable aspects . The main topic of all the reviews for a particular item is virtu - ally thesame : areviewof thisitem . Therefore , whensuch topic modeling methods are applied to a collection of re - viewsfordiﬀerentitems , theyinfertopicscorrespondingto distinguishingpropertiesoftheseitems . E . g . whenapplied toacollectionofhotelreviews , thesemodelsarelikelytoin - fertopics : hotels inFrance , New Yorkhotels , youthhostels , or , similarly , when applied to a collection of Mp3 players’ reviews , these models will infer topics like reviews of iPod or reviewsofCreativeZenplayer . Thoughtheseareallvalid topics , theydonotrepresentratableaspects , butratherde - ﬁneclusteringsoftherevieweditemsintospeciﬁctypes . In furtherdiscussionwewillrefertosuchtopicsas global topics , because theycorrespond toaglobal propertyof theobject in the review , such as its brand or base of operation . Dis - covering topics thatcorrelate with ratable aspects , such as cleanliness and location for hotels , is much more problem - aticwithLDAorPLSAmethods . Most ofthesetopicsare presentinsomewayineveryreview . Therefore , itisdiﬃcult todiscoverthembyusingonlyco - occurrenceinformationat thedocumentlevel . Inthiscaseexceedinglylargeamounts of trainingdataisneededandaswell asaverylargenum - ber of topics K . Even in this case there is a danger that themodelwillbeoverﬂownbyveryﬁne - grainglobaltopics or the resulting topics will be intersection of global topics and ratable aspects , like location for hotels in New York . WewillshowinSection4thatthishypothesisisconﬁrmedexperimentally . Onewaytoaddressthisproblemwouldbetoconsiderco - occurrencesatthesentencelevel , i . e . , applyLDAorPLSAto individualsentences . Butinthiscasewewillnothaveasuf - ﬁcientco - occurrencedomain , anditisknownthatLDAand PLSAbehavebadlywhenappliedtoveryshortdocuments . Thisproblemcanbeaddressedbyexplicitlymodelingtopictransitions [ 5 , 15 , 33 , 32 , 28 , 16 ] , but these topic n - gram models are considerably more computationally expensive . Also , like LDA and PLSA , they will not be able to distin - guish between topics corresponding to ratable aspects and global topics representing properties of the reviewed item . In the following section we will introduce a method which explicitly models both types of topics and eﬃciently infers ratableaspectsfromlimitedamountoftrainingdata . 2 . 2 MG - LDA WeproposeamodelcalledMulti - grainLDA ( MG - LDA ) , whichmodelstwodistincttypesoftopics : globaltopicsand localtopics . AsinPLSAandLDA , thedistributionofglobal topicsisﬁxedforadocument . However , thedistributionof localtopicsisallowedtovaryacrossthedocument . Aword inthedocumentissampledeitherfromthemixtureofglobaltopicsorfromthemixtureoflocaltopicsspeciﬁcforthelocalcontextoftheword . The hypothesis is that ratable aspects will be captured by local topics and global topics willcapturepropertiesofrevieweditems . Forexamplecon - sideranextractfromareviewofaLondonhotel : “ . . . public transport in London is straightforward , the tubestation is aboutan8minutewalk . . . oryoucangetabusfor £ 1 . 50” . It can be viewed as a mixture of topic London shared by the entire review ( words : “London” , “tube” , “ £ ” ) , and the ratable aspect location , speciﬁc for thelocal context of the sentence ( words : “transport” , “walk” , “bus” ) . Local topics are expected to be reused between very diﬀerent types of items , whereasglobaltopicswillcorrespondonlytopartic - ular typesof items . In order to captureonly genuinelocal topics , weallowalargenumberofglobaltopics , eﬀectively , creatingabottleneckattheleveloflocaltopics . Ofcourse , this bottleneck is speciﬁc to our purposes . Other applica - tions of multi - grain topic models conceivably might prefer thebottleneckreversed . Finally , wenotethatourdeﬁnition of multi - grain is simply for two - levels of granularity , global and local . In principle though , there is nothing preventing themodel described in this section from extendingbeyond twolevels . Onemightexpectthatforothertasksevenmore levelsofgranularitycouldbebeneﬁcial . Werepresentadocumentasasetofslidingwindows , each covering T adjacent sentenceswithin it . Each window v in document d hasanassociated distributionoverlocaltopics θ locd , v and a distribution deﬁning preference for local topics versusglobaltopics π d , v . Awordcanbesampledusingany windowcoveringitssentence s , wherethewindowischosen accordingtoacategoricaldistribution ψ s . Importantly , the fact that the windows overlap , permits to exploit a larger co - occurrencedomain . Thesesimpletechniquesarecapable ofmodelinglocaltopicswithoutmoreexpensivemodelingof topicstransitionsusedin [ 5 , 15 , 33 , 32 , 28 , 16 ] . Introduction ofasymmetricalDirichletprior Dir ( γ ) forthedistribution ψ s permitstocontrolsmoothnessoftopictransitionsinour model . The formal deﬁnition of the model with K gl global and K loc local topics is the following . First , draw K gl word distributions for global topics ϕ glz from a Dirichlet prior Dir ( β gl ) and K loc word distributions for local topics ϕ locz ! from Dir ( β loc ) . Then , foreachdocument d : • Chooseadistributionofglobaltopics θ gld ∼ Dir ( α gl ) . • For each sentence s choose a distribution ψ d , s ( v ) ∼ Dir ( γ ) . • Foreachslidingwindow v 113 WWW 2008 / Refereed Track : Data Mining - Modeling April 21 - 25 , 2008 · Beijing , China McCallum , Wang , & Corrada - Emmanuel ! " z w Latent Dirichlet Allocation ( LDA ) [ Blei , Ng , Jordan , 2003 ] N D x z w Author - Topic Model ( AT ) [ Rosen - Zvi , Grifﬁths , Steyvers , Smyth 2004 ] N D " # ! $ T A # $ T x z w Author - Recipient - Topic Model ( ART ) [ This paper ] N D " # ! $ T A , A z w Author Model ( Multi - label Mixture Model ) [ McCallum 1999 ] N D # $ A a d a d r d a d d d d d Figure 1 : Three related models , and the ART model . In all models , each observed word , w , is generated from a multinomial word distribution , φ z , speciﬁc to a particular topic / author , z , however topics are selected diﬀerently in each of the models . In LDA , the topic is sampled from a per - document topic distribution , θ , which in turn is sampled from a Dirichlet over topics . In the Author Model , there is one topic associated with each author ( or category ) , and authors are sampled uniformly . In the Author - Topic model , the topic is sampled from a per - author multinomialdistribution , θ , andauthorsaresampleduniformlyfromtheobserved list of the document’s authors . In the Author - Recipient - Topic model , there is a separate topic - distribution for each author - recipient pair , and the selection of topic - distributionisdeterminedfromtheobservedauthor , andbyuniformlysam - pling a recipient from the set of recipients for the document . its generative process for each document d , a set of authors , a d , is observed . To generate each word , an author x is chosen uniformly from this set , then a topic z is selected from a topic distribution θ x that is speciﬁc to the author , and then a word w is generated from a topic - speciﬁc multinomial distribution φ z . However , as described previously , none of these models is suitable for modeling message data . An email message has one sender and in general more than one recipients . We could treat both the sender and the recipients as “authors” of the message , and then employ the ATmodel , butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage , which is undesirable in many real - world situations . A manager may send email to a secretary and vice versa , but the nature of the requests and language used may be quite diﬀerent . Even more dramatically , consider the large quantity of junk email that we receive ; modeling the topicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotreﬂectourexpertiseorroles . Alternatively we could still employ the AT model by ignoring the recipient information of email and treating each email document as if it only has one author . However , in this case ( whichissimilartotheLDAmodel ) wearelosingallinformationabouttherecipients , and the connections between people implied by the sender - recipient relationships . 252 • The posterior can be used in creative ways . • E . g . , we can use inferences in information retrieval , recommendation , similarity , visualization , summarization , and other applications . Extending LDA • These different kinds of extensions can be combined . • ( Really , these ways of extending LDA are a big advantage of using probabilistic modeling to analyze data . ) • To give a sense of how LDA can be extended , I’ll describe several examples of extensions that my group has worked on . • We will discuss • Correlated topic models • Dynamic topic models & measuring scholarly impact • Supervised topic models • Relational topic models • Ideal point topic models • Collaborative topic models Correlated and Dynamic Topic Models Correlated topic models • The Dirichlet is a distribution on the simplex , positive vectors that sum to 1 . • It assumes that components are nearly independent . • In real data , an article about fossil fuels is more likely to also be about geology than about genetics . Correlated topic models • The logistic normal is a distribution on the simplex that can model dependence between components ( Aitchison , 1980 ) . • The log of the parameters of the multinomial are drawn from a multivariate Gaussian distribution , X ∼ (cid:78) K ( µ , Σ ) θ i ∝ exp { x i } . Correlated topic models Z d , n W d , n N D K β k µ , Σ η θ d Logistic normal prior • Draw topic proportions from a logistic normal • This allows topic occurrences to exhibit correlation . • Provides a “map” of topics and how they are related • Provides a better ﬁt to text data , but computation is more complex wild type mutantmutationsmutantsmutation plantsplantgenegenes arabidopsis p53 cell cycle activitycyclinregulation amino acids cdna sequenceisolatedprotein genediseasemutationsfamiliesmutation rnadna rna polymerasecleavagesite cellscell expression cell lines bone marrow united states women universitiesstudentseducation sciencescientistssaysresearchpeople researchfundingsupportnihprogram surfacetipimagesampledevice laseropticallightelectronsquantum materialsorganicpolymerpolymersmolecules volcanicdepositsmagmaeruption volcanism mantlecrust upper mantle meteoritesratios earthquakeearthquakesfaultimages data ancient foundimpact million years ago africa climateoceanicechanges climate change cellsproteins researchersprotein found patientsdiseasetreatmentdrugsclinical geneticpopulationpopulationsdifferencesvariation fossil recordbirdsfossilsdinosaursfossil sequencesequencesgenomednasequencing bacteriabacterialhostresistanceparasite developmentembryosdrosophilagenesexpression speciesforestforestspopulationsecosystems synapsesltpglutamatesynapticneurons neuronsstimulusmotorvisualcortical ozone atmosphericmeasurementsstratosphereconcentrations sun solar wind earthplanetsplanet co2carbon carbon dioxide methane water receptorreceptorsligandligandsapoptosis proteinsproteinbindingdomaindomains activated tyrosine phosphorylationactivation phosphorylationkinase magneticmagnetic ﬁeld spin superconductivitysuperconducting physicistsparticlesphysicsparticleexperiment surfaceliquidsurfacesﬂuidmodel reactionreactionsmoleculemolecules transition state enzymeenzymesironactive site reduction pressure high pressure pressurescoreinner core brainmemorysubjectslefttask computerprobleminformationcomputersproblems stars astronomersuniversegalaxiesgalaxy virushivaids infectionviruses miceantigent cells antigens immune response Dynamic topic models AMONG the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order . . . 1789 My fellow citizens : I stand here today humbled by the task before us , grateful for the trust you have bestowed , mindful of the sacrifices borne by our ancestors . . . 2009 Inaugural addresses • LDA assumes that the order of documents does not matter . • Not appropriate for sequential corpora ( e . g . , that span hundreds of years ) • Further , we may want to track how language changes over time . • Dynamic topic models let the topics drift in a sequence . D θ d Z d , n W d , n N K α D θ d Z d , n W d , n N α D θ d Z d , n W d , n N α β k , 1 β k , 2 β k , T . . . Topics drift through time Dynamic topic models β k , 1 β k , 2 β k , T . . . • Use a logistic normal distribution to model topics evolving over time . • Embed it in a state - space model on the log of the topic distribution β t , k | β t − 1 , k ∼ (cid:78) ( β t − 1 , k , I σ 2 ) p ( w | β t , k ) ∝ exp (cid:8) β t , k (cid:9) • As for CTMs , this makes computation more complex . But it lets us make inferences about sequences of documents . Dynamic topic models Original article Topic proportions Dynamic topic models sequencegenomegenessequenceshuman gene dnasequencingchromosomeregionsanalysisdatagenomicnumber devicesdevicematerialscurrenthigh gate lightsiliconmaterialtechnologyelectricalﬁberpowerbased datainformationnetworkwebcomputer language networkstimesoftwaresystemwordsalgorithmnumberinternet Original article Most likely words from top topics Dynamic topic models 1880 electricmachinepowerenginesteamtwomachinesironbatterywire 1890 electricpowercompanysteamelectricalmachinetwosystemmotorengine 1900 apparatussteampowerengineengineeringwaterconstructionengineerroomfeet 1910 airwater engineeringapparatusroomlaboratoryengineermadegastube 1920 apparatustubeairpressurewaterglassgasmadelaboratorymercury 1930 tube apparatusglassairmercurylaboratorypressuremadegassmall 1940 airtube apparatusglasslaboratoryrubberpressuresmallmercurygas 1950 tube apparatusglassairchamberinstrument small laboratorypressurerubber 1960 tubesystem temperatureairheatchamber powerhigh instrumentcontrol 1970 airheatpowersystem temperaturechamber highﬂowtubedesign 1980 highpowerdesignheatsystemsystems devices instrumentscontrollarge 1990 materialshighpowercurrentapplicationstechnology devicesdesigndeviceheat 2000 devicesdevicematerialscurrentgatehigh lightsiliconmaterial technology Dynamic topic models 1880 1900 1920 1940 1960 1980 2000 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o 1880 1900 1920 1940 1960 1980 2000 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o RELATIVITY LASER FORCE NERVE OXYGEN NEURON " Theoretical Physics " " Neuroscience " Dynamic topic models • Time - corrected similarity shows a new way of using the posterior . • Consider the expected Hellinger distance between the topic proportions of two documents , d ij = E   K (cid:88) k = 1 ( (cid:112) θ i , k − (cid:112) θ j , k ) 2 | w i , w j   • Uses the latent structure to deﬁne similarity • Time has been factored out because the topics associated to the components are different from year to year . • Similarity based only on topic proportions Dynamic topic models The Brain of the Orang ( 1880 ) Dynamic topic models Representation of the Visual Field on the Medial Wall of Occipital - Parietal Cortex in the Owl Monkey ( 1976 ) Measuring scholarly impact History of Science I m p a c t Einstein ' s Theory of Relativity My crackpot theory Relativity paper # 1 Relativity paper # 2 Relativity paper # 3 Relativity paper # 4 • We built on the DTM to measure scholarly impact with sequences of text . • Inﬂuential articles reﬂect future changes in language use . • The “inﬂuence” of an article is a latent variable . • Inﬂuential articles affect the drift of the topics that they discuss . • The posterior gives a retrospective estimate of inﬂuential articles . θ d Z d , n W d , n K α θ d Z d , n W d , n α β k , 1 β k , 2 . . . I d I d θ d Z d , n W d , n α β k , 2 I d Topic drift biased by inﬂuential articles Per - document inﬂuence Measuring scholarly impact W d , n K β k , 1 β k , 2 I d θ d Z d , n α • Each document has an inﬂuence score I d . • Each topic drifts in a way that is biased towards the documents with high inﬂuence . • We can examine the posterior of the inﬂuence scores to retrospectively ﬁnd articles that best explain the changes in language . Measuring scholarly impact Number of topics C o rr e l a t i on t o c i t a t i on 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 0 . 40 pnas lll l l l l 20 40 60 80 100 nature ll l l l l l 20 40 60 80 100 acl l l l l l l l 20 40 60 80 100 • This measure of impact only uses the words of the documents . It correlates strongly with citation counts . • High impact , high citation : “The Mathematics of Statistical Machine Translation : Parameter Estimation” ( Brown et al . , 1993 ) • “Low” impact , high citation : “Building a large annotated corpus of English : the Penn Treebank” ( Marcus et al . , 1993 ) Measuring scholarly impact Year W e i gh t ed I n f l uen c e 0 . 000 0 . 005 0 . 010 0 . 015 0 . 020 0 . 025 0 . 030 1880 1900 1920 1940 1960 1980 2000 Jared M . Diamond , Distributional Ecology of New Guinea Birds . Science ( 1973 ) [ 296 citations ] W . B . Scott , The Isthmus of Panama in Its Relation to the Animal Life of North and South America , Science ( 1916 ) [ 3 citations ] William K . Gregory , The New Anthropogeny : Twenty - Five Stages of Vertebrate Evolution , from Silurian Chordate to Man , Science ( 1933 ) [ 3 citations ] Derek E . Wildman et al . , Implications of Natural Selection in Shaping 99 . 4 % Nonsynonymous DNA Identity between Humans and Chimpanzees : Enlarging Genus Homo , PNAS ( 2003 ) [ 178 citations ] • PNAS , Science , and Nature from 1880 – 2005 • 350 , 000 Articles • 163M observations • Year - corrected correlation is 0 . 166 Summary : Correlated and dynamic topic models • The Dirichlet assumption on topics and topic proportions makes strong conditional independence assumptions about the data . • The correlated topic model uses a logistic normal on the topic proportions to ﬁnd patterns in how topics tend to co - occur . • The dynamic topic model uses a logistic normal in a linear dynamic model to capture how topics change over time . • What’s the catch ? These models are harder to compute with . ( Stay tuned . ) Supervised Topic Models Supervised LDA • LDA is an unsupervised model . How can we build a topic model that is good at the task we care about ? • Many data are paired with response variables . • User reviews paired with a number of stars • Web pages paired with a number of “likes” • Documents paired with links to other documents • Images paired with a category • Supervised LDA are topic models of documents and responses . They are ﬁt to ﬁnd topics predictive of the response . Supervised LDA θ d Z d , n W d , n N D K β k α Y d η , δ Regression parameters Document response 1 Draw topic proportions θ | α ∼ Dir ( α ) . 2 For each word • Draw topic assignment z n | θ ∼ Mult ( θ ) . • Draw word w n | z n , β 1 : K ∼ Mult ( β z n ) . 3 Draw response variable y | z 1 : N , η , σ 2 ∼ N (cid:128) η (cid:62) ¯ z , σ 2 (cid:138) , where ¯ z = ( 1 / N ) (cid:80) N n = 1 z n . Supervised LDA θ d Z d , n W d , n N D K β k α Y d η , δ Regression parameters Document response • Fit sLDA parameters to documents and responses . This gives : topics β 1 : K and coefﬁcients η 1 : K . • Given a new document , predict its response using the expected value : E (cid:148) Y | w 1 : N , α , β 1 : K , η , σ 2 (cid:151) = η (cid:62) E (cid:148) ¯ Z | w 1 : N (cid:151) • This blends generative and discriminative modeling . Supervised LDA bothmotionsimpleperfectfascinatingpowercomplex howevercinematographyscreenplayperformancespictures effectivepicture histheircharactermanywhileperformancebetween − 30 − 20 − 10 0 10 20 ● ● ● ● ●● ● ● ● ● morehasthanﬁlmsdirectorwillcharacters onefromtherewhichwho muchwhat awfulfeaturingroutinedryofferedcharlieparis notaboutmovieallwould theyits havelikeyouwasjust someout badguyswatchableitsnotonemovie leastproblemunfortunatelysupposedworseﬂatdull • 10 - topic sLDA model on movie reviews ( Pang and Lee , 2005 ) . • Response : number of stars associated with each review • Each component of coefﬁcient vector η is associated with a topic . Supervised LDA Number of topics C o rr e l a t i on 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 5 10 15 20 25 30 Model lda slda Supervised LDA θ d Z d , n W d , n N D K β k α Y d η , δ Regression parameters Document response • SLDA enables model - based regression where the predictor is a document . • It can easily be used wherever LDA is used in an unsupervised fashion ( e . g . , images , genes , music ) . • SLDA is a supervised dimension - reduction technique , whereas LDA performs unsupervised dimension reduction . Supervised LDA θ d Z d , n W d , n N D K β k α Y d η , δ Regression parameters Document response • SLDA has been extended to generalized linear models , e . g . , for image classiﬁcation and other non - continuous responses . • We will discuss two extensions of sLDA • Relational topic models : Models of networks and text • Ideal point topic models : Models of legislative voting behavior Relational topic models 52 478 430 2487 75 288 1123 2122 2299 1354 1854 1855 89 635 92 2438 136 479 109 640 119 686 120 1959 1539 147 172 177 965 911 21921489885 178 378 286 208 1569 2343 1270 218 1290 223 227 236 1617 254 1176 256 634 264 1963 2195 1377 303 426 2091 313 1642 534 801 335 344 585 1244 2291 2617 1627 22901275 375 1027 396 1678 24472583 1061 692 1207 960 1238 20121644 2042 381 418 17921284 651 524 1165 2197 1568 25931698 547 683 21371637 2557 2033 632 1020 436442 449 474 649 2636 2300 539 541 603 1047 722 660 806 1121 1138 831 837 1335 902 964 966 981 1673 114014811432 1253 1590 1060 992 994 1001 1010 1651 1578 1039 1040 1344 1345 1348 1355 1420 1089 1483 11881674 1680 2272 1285 1592 1234 1304 13171426 1695 1465 1743 1944 2259 2213 We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high - accuracy concepts . . . Irrelevant features and the subset selection problem In many domains , an appropriate inductive bias is the MIN - FEATURES bias , which prefers consistent hypotheses definable over as few features as possible . . . Learning with many irrelevant features In this introduction , we define the term bias as it is used in machine learning systems . We motivate the importance of automated methods for evaluating . . . Evaluation and selection of biases in machine learning The inductive learning problem consists of learning a concept given examples and nonexamples of the concept . To perform this learning task , inductive learning algorithms bias their learning method . . . Utilizing prior concepts for learning The problem of learning decision rules for sequential tasks is addressed , focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile . . . Improving tactical plans with genetic algorithms Evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots . In the approach described here , evolutionary . . . An evolutionary approach to learning in robots Navigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles . One way to produce robust behavior . . . Using a genetic algorithm to learn strategies for collision avoidance and local navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . • Many data sets contain connected observations . • For example : • Citation networks of documents • Hyperlinked networks of web - pages . • Friend - connected social network proﬁles Relational topic models 52 478 430 2487 75 288 1123 2122 2299 1354 1854 1855 89 635 92 2438 136 479 109 640 119 686 120 1959 1539 147 172 177 965 911 21921489885 178 378 286 208 1569 2343 1270 218 1290 223 227 236 1617 254 1176 256 634 264 1963 2195 1377 303 426 2091 313 1642 534 801 335 344 585 1244 2291 2617 1627 22901275 375 1027 396 1678 24472583 1061 692 1207 960 1238 20121644 2042 381 418 17921284 651 524 1165 2197 1568 25931698 547 683 21371637 2557 2033 632 1020 436442 449 474 649 2636 2300 539 541 603 1047 722 660 806 1121 1138 831 837 1335 902 964 966 981 1673 114014811432 1253 1590 1060 992 994 1001 1010 1651 1578 1039 1040 1344 1345 1348 1355 1420 1089 1483 11881674 1680 2272 1285 1592 1234 1304 13171426 1695 1465 1743 1944 2259 2213 We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high - accuracy concepts . . . Irrelevant features and the subset selection problem In many domains , an appropriate inductive bias is the MIN - FEATURES bias , which prefers consistent hypotheses definable over as few features as possible . . . Learning with many irrelevant features In this introduction , we define the term bias as it is used in machine learning systems . We motivate the importance of automated methods for evaluating . . . Evaluation and selection of biases in machine learning The inductive learning problem consists of learning a concept given examples and nonexamples of the concept . To perform this learning task , inductive learning algorithms bias their learning method . . . Utilizing prior concepts for learning The problem of learning decision rules for sequential tasks is addressed , focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile . . . Improving tactical plans with genetic algorithms Evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots . In the approach described here , evolutionary . . . An evolutionary approach to learning in robots Navigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles . One way to produce robust behavior . . . Using a genetic algorithm to learn strategies for collision avoidance and local navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . • Research has focused on ﬁnding communities and patterns in the link - structure of these networks . But this ignores content . • We adapted sLDA to pairwise response variables . This leads to a model of content and connection . • Relational topic models ﬁnd related hidden structure in both types of data . Relational topic models β k α Z i , n Z j , n W i , n W j , n θ i θ j Y i , j η Pairwise response This structure repeats for every i , j pair • Adapt ﬁtting algorithm for sLDA with binary GLM response • RTMs allow predictions about new and unlinked data . • These predictions are out of reach for traditional network models . Relational topic models 16 J . CHANG AND D . BLEI Table 2 Top eight link predictions made by RTM ( ψ e ) and LDA + Regression for two documents ( italicized ) from Cora . The models were ﬁt with 10 topics . Boldfaced titles indicate actual documents cited by or citing each document . Over the whole corpus , RTM improves precision over LDA + Regression by 80 % when evaluated on the ﬁrst 20 documents retrieved . Markov chain Monte Carlo convergence diagnostics : A comparative review Minorization conditions and convergence rates for Markov chain Monte Carlo R T M ( ψ e ) Rates of convergence of the Hastings and Metropolis algorithms Possible biases induced by MCMC convergence diagnostics Bounding convergence time of the Gibbs sampler in Bayesian image restoration Self regenerative Markov chain Monte Carlo Auxiliary variable methods for Markov chain Monte Carlo with applications Rate of Convergence of the Gibbs Sampler by Gaussian Approximation Diagnosing convergence of Markov chain Monte Carlo algorithms Exact Bound for the Convergence of Metropolis Chains L D A + R e g r e ss i o n Self regenerative Markov chain Monte Carlo Minorization conditions and convergence rates for Markov chain Monte Carlo Gibbs - markov models Auxiliary variable methods for Markov chain Monte Carlo with applications Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Models Mediating instrumental variables A qualitative framework for probabilistic inference Adaptation for Self Regenerative MCMC Competitive environments evolve better solutions for complex tasks Coevolving High Level Representations R T M ( ψ e ) A Survey of Evolutionary Strategies Genetic Algorithms in Search , Optimization and Machine Learning Strongly typed genetic programming in evolving cooperation strategies Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . Evolutionary Module Acquisition An Empirical Investigation of Multi - Parent Recombination Operators . . . A New Algorithm for DNA Sequence Assembly L D A + R e g r e ss i o n Identiﬁcation of protein coding regions in genomic DNA Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . A genetic algorithm for passive management The Performance of a Genetic Algorithm on a Chaotic Objective Function Adaptive global optimization with local search Mutation rates as adaptations Table 2 illustrates suggested citations using RTM ( ψ e ) and LDA + Regres - sion as predictive models . These suggestions were computed from a model ﬁt on one of the folds of the Cora data . The top results illustrate suggested links for “Markov chain Monte Carlo convergence diagnostics : A comparative re - Given a new document , which documents is it likely to link to ? Relational topic models 16 J . CHANG AND D . BLEI Table 2 Top eight link predictions made by RTM ( ψ e ) and LDA + Regression for two documents ( italicized ) from Cora . The models were ﬁt with 10 topics . Boldfaced titles indicate actual documents cited by or citing each document . Over the whole corpus , RTM improves precision over LDA + Regression by 80 % when evaluated on the ﬁrst 20 documents retrieved . Markov chain Monte Carlo convergence diagnostics : A comparative review Minorization conditions and convergence rates for Markov chain Monte Carlo R T M ( ψ e ) Rates of convergence of the Hastings and Metropolis algorithms Possible biases induced by MCMC convergence diagnostics Bounding convergence time of the Gibbs sampler in Bayesian image restoration Self regenerative Markov chain Monte Carlo Auxiliary variable methods for Markov chain Monte Carlo with applications Rate of Convergence of the Gibbs Sampler by Gaussian Approximation Diagnosing convergence of Markov chain Monte Carlo algorithms Exact Bound for the Convergence of Metropolis Chains L D A + R e g r e ss i o n Self regenerative Markov chain Monte Carlo Minorization conditions and convergence rates for Markov chain Monte Carlo Gibbs - markov models Auxiliary variable methods for Markov chain Monte Carlo with applications Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Models Mediating instrumental variables A qualitative framework for probabilistic inference Adaptation for Self Regenerative MCMC Competitive environments evolve better solutions for complex tasks Coevolving High Level Representations R T M ( ψ e ) A Survey of Evolutionary Strategies Genetic Algorithms in Search , Optimization and Machine Learning Strongly typed genetic programming in evolving cooperation strategies Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . Evolutionary Module Acquisition An Empirical Investigation of Multi - Parent Recombination Operators . . . A New Algorithm for DNA Sequence Assembly L D A + R e g r e ss i o n Identiﬁcation of protein coding regions in genomic DNA Solving combinatorial problems using evolutionary algorithms A promising genetic algorithm approach to job - shop scheduling . . . A genetic algorithm for passive management The Performance of a Genetic Algorithm on a Chaotic Objective Function Adaptive global optimization with local search Mutation rates as adaptations Table 2 illustrates suggested citations using RTM ( ψ e ) and LDA + Regres - sion as predictive models . These suggestions were computed from a model ﬁt on one of the folds of the Cora data . The top results illustrate suggested links for “Markov chain Monte Carlo convergence diagnostics : A comparative re - Given a new document , which documents is it likely to link to ? Ideal point topic models YYNNNYYYYY NNYYNYNNNY YYYYNNYNYN YYYYNNYNYN NYNNYY NYYN YNNYYNYNNN a j x i v ij p ( v ij ) = f ( d ( x i , a j ) ) • The ideal point model uncovers voting patterns in legislative data • We observe roll call data v ij . • Bills attached to discrimination parameters a j . Senators attached to ideal points x i . Ideal point topic models Ip1 La s t Udall Gillibrand Enzi Carper Feinstein Klobuchar Akaka Whitehouse Inouye MurrayCaseySalazar Webb Mikulski Schumer Lincoln Levin Rockefeller Kohl Reed DurbinNelson Cardin Leahy Conrad McCaskill Harkin Kerry Clinton Bingaman Pryor JohnsonBrown Stabenow BoxerBiden Lautenberg Dorgan Menendez Byrd Feingold Bayh LiebermanLandrieu Cantwell Baucus Sanders Obama Snowe Dodd Collins TesterWyden Coleman Reid Smith Specter KennedyLugar Murkowski Stevens Bennett McCain Warner Lott Sununu GrassleyDomenici HatchHagel CochranRoberts Corker Martinez Bond Isakson Alexander McConnell Dole Craig Hutchison Voinovich Thune Burr Chambliss Crapo ShelbyThomas Brownback Vitter GrahamEnsignCornyn Gregg Sessions Kyl BunningBarrassoAllard WickerInhofeDeMintCoburn - 5 0 5 10 Party Democrat Democrat , Independent Independent Republican Republican , Democrat . . . Ip1 La s t Udall Gillibrand Enzi Carper Feinstein Klobuchar Akaka Whitehouse Inouye MurrayCaseySalazar Webb Mikulski Schumer Lincoln Levin Rockefeller Kohl Reed DurbinNelson Cardin Leahy Conrad McCaskill Harkin Kerry Clinton Bingaman Pryor JohnsonBrown Stabenow BoxerBiden Lautenberg Dorgan Menendez Byrd Feingold Bayh LiebermanLandrieu Cantwell Baucus Sanders Obama Snowe Dodd Collins TesterWyden Coleman Reid Smith Specter KennedyLugar Murkowski Stevens Bennett McCain Warner Lott Sununu GrassleyDomenici HatchHagel CochranRoberts Corker Martinez Bond Isakson Alexander McConnell Dole Craig Hutchison Voinovich Thune Burr Chambliss Crapo ShelbyThomas Brownback Vitter GrahamEnsignCornyn Gregg Sessions Kyl BunningBarrassoAllard WickerInhofeDeMintCoburn - 5 0 5 10 Party Democrat Democrat , Independent Independent Republican Republican , Democrat Ip1 La s t Udall Gillibrand Enzi Carper Feinstein Klobuchar Akaka Whitehouse Inouye MurrayCaseySalazar Webb Mikulski Schumer Lincoln Levin Rockefeller Kohl Reed DurbinNelsonCardin Leahy Conrad McCaskill Harkin Kerry Clinton Bingaman Pryor JohnsonBrown Stabenow BoxerBiden Lautenberg Dorgan Menendez Byrd Feingold Bayh LiebermanLandrieu Cantwell Baucus Sanders Obama Snowe Dodd Collins TesterWyden Coleman Reid Smith Specter KennedyLugar Murkowski Stevens Bennett McCain Warner Lott Sununu GrassleyDomenici HatchHagel CochranRoberts Corker Martinez Bond Isakson Alexander McConnell Dole Craig Hutchison Voinovich Thune Burr Chambliss Crapo ShelbyThomas Brownback Vitter GrahamEnsignCornyn Gregg Sessions Kyl BunningBarrassoAllard WickerInhofeDeMintCoburn - 5 0 5 10 Party Democrat Democrat , Independent Independent Republican Republican , Democrat . . . • Posterior inference reveals the political spectrum of senators • Widely used in quantitative political science . Ideal point topic models NYNNYY N YYN ? YYNNNYY YYY ? NNYYNYN NNY ? YYYYNNY NYN ? YYYYNNY NYN ? YNNYYNY NNN ? a j x i v ij p ( v ij ) = f ( d ( x i , a j ) ) • We can predict a missing vote . • But we cannot predict all the missing votes from a bill . • Cf . the limitations of collaborative ﬁltering Ideal point topic models probabilistictopic model NYNNYY N YYNY YYNNNYY YYYY NNYYNYN NNYY YYYYNNY NYNN YYYYNNY NYNN YNNYYNY NNNN predicted discrimination • Use supervised LDA to predict bill discrimination from bill text . • But this is a latent response . Ideal point topic models θ d N D K β k α W dn Z dn η X u Ideal points Votes U V ud A d , B d σ 2 u σ 2 d Bill content Bill sentiment Ideal point topic models dod , defense , defense and appropriation , military , subtitle veteran , veterans , bills , care , injury people , woman , american , nation , school producer , eligible , crop , farm , subparagraph coin , inspector , designee , automobile , lebanon bills , iran , official , company , sudan human , vietnam , united nations , call , people drug , pediatric , product , device , medical child , fire , attorney , internet , bills surveillance , director , court , electronic , flood energy , bills , price , commodity , market land , site , bills , interior , river child , center , poison , victim , abuse coast guard , vessel , space , administrator , requires science , director , technology , mathematics , bills computer , alien , bills , user , collection head , start , child , technology , award loss , crop , producer , agriculture , trade bills , tax , subparagraph , loss , taxable cover , bills , bridge , transaction , following transportation , rail , railroad , passenger , homeland security business , administrator , bills , business concern , loan defense , iraq , transfer , expense , chapter medicare , medicaid , child , chip , coverage student , loan , institution , lender , school energy , fuel , standard , administrator , lamp housing , mortgage , loan , family , recipient bank , transfer , requires , holding company , industrial county , eligible , ballot , election , jurisdiction tax credit , budget authority , energy , outlays , tax In addition to senators and bills , IPTM places topics on the spectrum . Summary : Supervised topic models • Many documents are associated with response variables . • Supervised LDA embeds LDA in a generalized linear model that is conditioned on the latent topic assignments . • Relational topic models use sLDA assumptions with pair - wise responses to model networks of documents . • Ideal point topic models demonstrates how the response variables can themselves be latent variables . In this case , they are used downstream in a model of legislative behavior . • ( SLDA , the RTM , and others are implemented in the R package “lda . ” ) Modeling User Data and Text Topic models for recommendation ( Wang and Blei , 2011 ) Introduction to Variational Methods for Graphical Models Conditional Random Fields Maximum likelihood from incomplete data via the EM algorithm The Mathematics of Statistical Machine Translation Users P a p e r s In - matrix prediction Out - of - matrix prediction Topic Models for Recommendation • In many settings , we have information about how people use documents . • With new models , this can be used to • Help people ﬁnd documents that they are interested in • Learn about what the documents mean to the people reading them • Learn about the people reading ( or voting on ) the documents . • ( We also saw this in ideal point topic models . ) Topic models for recommendation ( Wang and Blei , 2011 ) Introduction to Variational Methods for Graphical Models Conditional Random Fields Maximum likelihood from incomplete data via the EM algorithm The Mathematics of Statistical Machine Translation Users P a p e r s In - matrix prediction Out - of - matrix prediction Topic Models for Recommendation • Online communities of scientists’ allow for new ways of connecting researchers to the research literature . • With collaborative topic models , we recommend scientiﬁc articles based both on other scientists’ preferences and their content . • We can form both “in - matrix” and “out - of - matrix” predictions . We can learn about which articles are important , and which are interdisciplinary . • Consider EM ( Dempster et al . , 1977 ) . The text lets us estimate its topics : Statistics Vision (cid:20)(cid:3) (cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:3)(cid:47)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:44)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:89)(cid:76)(cid:68)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:36)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3) (cid:37)(cid:92)(cid:3)(cid:36)(cid:17)(cid:3)(cid:51)(cid:17)(cid:3) (cid:39)(cid:40)(cid:48)(cid:51)(cid:54)(cid:55)(cid:40)(cid:53)(cid:15)(cid:3)(cid:49)(cid:17)(cid:3) (cid:48)(cid:17)(cid:3) (cid:47)(cid:36)(cid:44)(cid:53)(cid:39)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)(cid:39)(cid:17)(cid:3)(cid:37)(cid:17)(cid:3)(cid:53)(cid:56)(cid:37)(cid:44)(cid:49)(cid:3) (cid:43)(cid:68)(cid:85)(cid:89)(cid:68)(cid:85)(cid:71)(cid:3)(cid:56)(cid:81)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:40)(cid:71)(cid:88)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:55)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:54)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:3) (cid:62)(cid:53)(cid:72)(cid:68)(cid:71)(cid:3)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:53)(cid:50)(cid:60)(cid:36)(cid:47)(cid:3)(cid:54)(cid:55)(cid:36)(cid:55)(cid:44)(cid:54)(cid:55)(cid:44)(cid:38)(cid:36)(cid:47)(cid:3)(cid:54)(cid:50)(cid:38)(cid:44)(cid:40)(cid:55)(cid:60)(cid:3)(cid:68)(cid:87)(cid:3)(cid:68)(cid:3)(cid:80)(cid:72)(cid:72)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:85)(cid:74)(cid:68)(cid:81)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:53)(cid:40)(cid:54)(cid:40)(cid:36)(cid:53)(cid:38)(cid:43)(cid:3) (cid:54)(cid:40)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3) (cid:82)(cid:81)(cid:3)(cid:58)(cid:72)(cid:71)(cid:81)(cid:72)(cid:86)(cid:71)(cid:68)(cid:92)(cid:15)(cid:3)(cid:39)(cid:72)(cid:70)(cid:72)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:27)(cid:87)(cid:75)(cid:15)(cid:3)(cid:20)(cid:28)(cid:26)(cid:25)(cid:15)(cid:3)(cid:51)(cid:85)(cid:82)(cid:73)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:54)(cid:17)(cid:3)(cid:39)(cid:17)(cid:3)(cid:54)(cid:44)(cid:47)(cid:57)(cid:40)(cid:60)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:38)(cid:75)(cid:68)(cid:76)(cid:85)(cid:64)(cid:3) (cid:54)(cid:56)(cid:48)(cid:48)(cid:36)(cid:53)(cid:60)(cid:3) (cid:36)(cid:3)(cid:69)(cid:85)(cid:82)(cid:68)(cid:71)(cid:79)(cid:92)(cid:3)(cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:76)(cid:86)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:87)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:79)(cid:72)(cid:89)(cid:72)(cid:79)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:76)(cid:87)(cid:92)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:82)(cid:85)(cid:92)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:82)(cid:81)(cid:82)(cid:87)(cid:82)(cid:81)(cid:72)(cid:3)(cid:69)(cid:72)(cid:75)(cid:68)(cid:89)(cid:76)(cid:82)(cid:88)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:72)(cid:85)(cid:74)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:71)(cid:72)(cid:85)(cid:76)(cid:89)(cid:72)(cid:71)(cid:17)(cid:3)(cid:48)(cid:68)(cid:81)(cid:92)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:86)(cid:78)(cid:72)(cid:87)(cid:70)(cid:75)(cid:72)(cid:71)(cid:15)(cid:3)(cid:76)(cid:81)(cid:70)(cid:79)(cid:88)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:76)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:86)(cid:76)(cid:87)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:15)(cid:3)(cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:83)(cid:72)(cid:71)(cid:15)(cid:3)(cid:70)(cid:72)(cid:81)(cid:86)(cid:82)(cid:85)(cid:72)(cid:71)(cid:3)(cid:82)(cid:85)(cid:3)(cid:87)(cid:85)(cid:88)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:15)(cid:3)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:72)(cid:3)(cid:80)(cid:76)(cid:91)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)(cid:15)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3)(cid:75)(cid:92)(cid:83)(cid:72)(cid:85)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:79)(cid:92)(cid:3)(cid:85)(cid:72)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:3)(cid:79)(cid:72)(cid:68)(cid:86)(cid:87)(cid:3)(cid:86)(cid:84)(cid:88)(cid:68)(cid:85)(cid:72)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:73)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:68)(cid:81)(cid:68)(cid:79)(cid:92)(cid:86)(cid:76)(cid:86)(cid:17)(cid:3) (cid:46)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:86)(cid:29)(cid:3)(cid:48)(cid:36)(cid:59)(cid:44)(cid:48)(cid:56)(cid:48)(cid:3)(cid:47)(cid:44)(cid:46)(cid:40)(cid:47)(cid:44)(cid:43)(cid:50)(cid:50)(cid:39)(cid:30)(cid:3)(cid:44)(cid:49)(cid:38)(cid:50)(cid:48)(cid:51)(cid:47)(cid:40)(cid:55)(cid:40)(cid:3)(cid:39)(cid:36)(cid:55)(cid:36)(cid:30)(cid:3)(cid:40)(cid:48)(cid:3)(cid:36)(cid:47)(cid:42)(cid:50)(cid:53)(cid:44)(cid:55)(cid:43)(cid:48)(cid:30)(cid:3)(cid:51)(cid:50)(cid:54)(cid:55)(cid:40)(cid:53)(cid:44)(cid:50)(cid:53)(cid:3)(cid:48)(cid:50)(cid:39)(cid:40)(cid:3) (cid:20)(cid:17)(cid:3)(cid:44)(cid:49)(cid:55)(cid:53)(cid:50)(cid:39)(cid:56)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3) (cid:55)(cid:43)(cid:44)(cid:54)(cid:3)(cid:83)(cid:68)(cid:83)(cid:72)(cid:85)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:68)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:68)(cid:83)(cid:83)(cid:85)(cid:82)(cid:68)(cid:70)(cid:75)(cid:3)(cid:87)(cid:82)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:70)(cid:68)(cid:81)(cid:3)(cid:69)(cid:72)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:72)(cid:71)(cid:3)(cid:68)(cid:86)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:17)(cid:3)(cid:54)(cid:76)(cid:81)(cid:70)(cid:72)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:70)(cid:82)(cid:81)(cid:86)(cid:76)(cid:86)(cid:87)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:81)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:70)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:73)(cid:82)(cid:79)(cid:79)(cid:82)(cid:90)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:90)(cid:72)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:3)(cid:76)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:40)(cid:48)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:40)(cid:48)(cid:3)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3)(cid:76)(cid:86)(cid:3)(cid:85)(cid:72)(cid:80)(cid:68)(cid:85)(cid:78)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:87)(cid:3)(cid:69)(cid:72)(cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:76)(cid:87)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:82)(cid:85)(cid:92)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:87)(cid:3)(cid:69)(cid:72)(cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:76)(cid:71)(cid:72)(cid:3)(cid:85)(cid:68)(cid:81)(cid:74)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:73)(cid:68)(cid:79)(cid:79)(cid:3)(cid:88)(cid:81)(cid:71)(cid:72)(cid:85)(cid:3)(cid:76)(cid:87)(cid:86)(cid:3)(cid:88)(cid:80)(cid:69)(cid:85)(cid:72)(cid:79)(cid:79)(cid:68)(cid:17)(cid:3)(cid:58)(cid:75)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:88)(cid:81)(cid:71)(cid:72)(cid:85)(cid:79)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:70)(cid:82)(cid:80)(cid:72)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:68)(cid:81)(cid:3)(cid:72)(cid:91)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:90)(cid:75)(cid:82)(cid:86)(cid:72)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:72)(cid:68)(cid:86)(cid:76)(cid:79)(cid:92)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:71)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:81)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:81)(cid:3)(cid:40)(cid:48)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:90)(cid:76)(cid:86)(cid:72)(cid:3)(cid:72)(cid:68)(cid:86)(cid:76)(cid:79)(cid:92)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:71)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:87)(cid:72)(cid:85)(cid:80)(cid:3)(cid:5)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:5)(cid:3)(cid:76)(cid:81)(cid:3)(cid:76)(cid:87)(cid:86)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:76)(cid:80)(cid:83)(cid:79)(cid:76)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:91)(cid:76)(cid:86)(cid:87)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:90)(cid:82)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:86)(cid:83)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:48)(cid:60)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:59)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:16)(cid:82)(cid:81)(cid:72)(cid:3)(cid:80)(cid:68)(cid:83)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:27)(cid:3)(cid:87)(cid:82)(cid:3)(cid:58)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:92)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:68)(cid:3)(cid:85)(cid:72)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:58)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:86)(cid:83)(cid:82)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:91)(cid:3)(cid:76)(cid:81)(cid:3)(cid:59)(cid:3)(cid:76)(cid:86)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:79)(cid:92)(cid:15)(cid:3)(cid:69)(cid:88)(cid:87)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:76)(cid:81)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:79)(cid:92)(cid:3)(cid:87)(cid:75)(cid:85)(cid:82)(cid:88)(cid:74)(cid:75)(cid:3)(cid:92)(cid:17)(cid:3)(cid:48)(cid:82)(cid:85)(cid:72)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:79)(cid:79)(cid:92)(cid:15)(cid:3)(cid:90)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:88)(cid:80)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:83)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:91)(cid:3)(cid:16)(cid:33)(cid:92)(cid:11)(cid:91)(cid:12)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:59)(cid:3)(cid:87)(cid:82)(cid:3)(cid:38)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:91)(cid:3)(cid:76)(cid:86)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:81)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:79)(cid:76)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:27)(cid:11)(cid:92)(cid:12)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:88)(cid:69)(cid:86)(cid:72)(cid:87)(cid:3)(cid:82)(cid:73)(cid:3)(cid:11)(cid:3)(cid:71)(cid:72)(cid:87)(cid:72)(cid:85)(cid:80)(cid:76)(cid:81)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:84)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:92)(cid:3)(cid:32)(cid:3)(cid:92)(cid:11)(cid:91)(cid:12)(cid:15)(cid:3)(cid:90)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:92)(cid:3)(cid:76)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:17)(cid:3)(cid:58)(cid:72)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:91)(cid:3)(cid:68)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:72)(cid:89)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:3)(cid:76)(cid:81)(cid:3)(cid:70)(cid:72)(cid:85)(cid:87)(cid:68)(cid:76)(cid:81)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:91)(cid:3)(cid:76)(cid:81)(cid:70)(cid:79)(cid:88)(cid:71)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:68)(cid:87)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:72)(cid:71)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3)(cid:58)(cid:72)(cid:3) (cid:83)(cid:82)(cid:86)(cid:87)(cid:88)(cid:79)(cid:68)(cid:87)(cid:72)(cid:3) (cid:68)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3) (cid:71)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3) (cid:73)(cid:11)(cid:91)(cid:3) (cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3) (cid:82)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:70)(cid:45)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:71)(cid:72)(cid:85)(cid:76)(cid:89)(cid:72)(cid:3) (cid:76)(cid:87)(cid:86)(cid:3)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:86)(cid:83)(cid:82)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:71)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3) (cid:44)(cid:3) (cid:70)(cid:45)(cid:12)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:73)(cid:11)(cid:17)(cid:17)(cid:17)(cid:3) (cid:77)(cid:3) (cid:17)(cid:17)(cid:17)(cid:12)(cid:3) (cid:76)(cid:86)(cid:3)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:74)(cid:11)(cid:17)(cid:17)(cid:17)(cid:3) (cid:44)(cid:3) (cid:17)(cid:17)(cid:17)(cid:12)(cid:3) (cid:69)(cid:92)(cid:3) (cid:74)(cid:11)(cid:92)(cid:77)(cid:70)(cid:77)(cid:23)(cid:12)(cid:12)(cid:3) (cid:73)(cid:11)(cid:91)(cid:3) (cid:20)(cid:3) (cid:12)(cid:12)(cid:3) (cid:11)(cid:17)(cid:20)(cid:12)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:87)(cid:3)(cid:73)(cid:76)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:70)(cid:45)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:72)(cid:86)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3)(cid:20)(cid:3)(cid:70)(cid:12)(cid:12)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:68)(cid:81)(cid:3) (cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:92)(cid:15)(cid:3)(cid:69)(cid:88)(cid:87)(cid:3)(cid:76)(cid:87)(cid:3)(cid:71)(cid:82)(cid:72)(cid:86)(cid:3)(cid:86)(cid:82)(cid:3)(cid:69)(cid:92)(cid:3)(cid:80)(cid:68)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:72)(cid:86)(cid:86)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3)(cid:19)(cid:12)(cid:17)(cid:3) (cid:49)(cid:82)(cid:87)(cid:76)(cid:70)(cid:72)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3) (cid:44)(cid:70)(cid:12)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:3)(cid:83)(cid:82)(cid:86)(cid:86)(cid:76)(cid:69)(cid:79)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3) (cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:44)(cid:3) (cid:70)(cid:45)(cid:12)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:90)(cid:76)(cid:79)(cid:79)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3)(cid:79)(cid:3) (cid:23)(cid:12)(cid:17)(cid:3) (cid:54)(cid:82)(cid:80)(cid:72)(cid:87)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3)(cid:68)(cid:3)(cid:81)(cid:68)(cid:87)(cid:88)(cid:85)(cid:68)(cid:79)(cid:3)(cid:70)(cid:75)(cid:82)(cid:76)(cid:70)(cid:72)(cid:3)(cid:90)(cid:76)(cid:79)(cid:79)(cid:3)(cid:69)(cid:72)(cid:3)(cid:82)(cid:69)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:15)(cid:3) (cid:68)(cid:87)(cid:3)(cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:87)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:80)(cid:68)(cid:92)(cid:3)(cid:69)(cid:72)(cid:3)(cid:86)(cid:72)(cid:89)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:71)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:90)(cid:68)(cid:92)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:71)(cid:72)(cid:73)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:44)(cid:19)(cid:17)(cid:3) (cid:40)(cid:68)(cid:70)(cid:75)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:81)(cid:89)(cid:82)(cid:79)(cid:89)(cid:72)(cid:86)(cid:3)(cid:87)(cid:90)(cid:82)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:90)(cid:72)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:70)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3) (cid:11)(cid:40)(cid:16)(cid:86)(cid:87)(cid:72)(cid:83)(cid:12)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:11)(cid:48)(cid:16)(cid:86)(cid:87)(cid:72)(cid:83)(cid:12)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:83)(cid:85)(cid:72)(cid:70)(cid:76)(cid:86)(cid:72)(cid:3)(cid:71)(cid:72)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:86)(cid:72)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:76)(cid:85)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:75)(cid:72)(cid:88)(cid:85)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:83)(cid:85)(cid:72)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:15)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:54)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:21)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:86)(cid:88)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:76)(cid:89)(cid:72)(cid:79)(cid:92)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)(cid:17)(cid:3)(cid:43)(cid:72)(cid:85)(cid:72)(cid:3)(cid:90)(cid:72)(cid:3)(cid:86)(cid:75)(cid:68)(cid:79)(cid:79)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:68)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:72)(cid:85)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:87)(cid:82)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:73)(cid:79)(cid:68)(cid:89)(cid:82)(cid:88)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)(cid:17)(cid:3) • With user data , we adjust the topics to account for who liked it : Statistics Vision • We can then recommend to users : Statistics Vision STATISTICIAN VISION RESEARCHER Topic models for recommendation θ d N D K β k α W dn Z dn X u User Preferences Ratings U V ud ζ d σ 2 u σ 2 d Article content Topic correction Topic models for recommendation • Big data set from Mendeley . com • Fit the model with stochastic optimization • The data— • 261K documents • 80K users • 10K vocabulary terms • 25M observed words • 5 . 1M entries ( sparsity is 0 . 02 % ) (cid:20)(cid:3) (cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:3)(cid:47)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:44)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:89)(cid:76)(cid:68)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:36)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3) (cid:37)(cid:92)(cid:3)(cid:36)(cid:17)(cid:3)(cid:51)(cid:17)(cid:3) (cid:39)(cid:40)(cid:48)(cid:51)(cid:54)(cid:55)(cid:40)(cid:53)(cid:15)(cid:3)(cid:49)(cid:17)(cid:3) (cid:48)(cid:17)(cid:3) (cid:47)(cid:36)(cid:44)(cid:53)(cid:39)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)(cid:39)(cid:17)(cid:3)(cid:37)(cid:17)(cid:3)(cid:53)(cid:56)(cid:37)(cid:44)(cid:49)(cid:3) (cid:43)(cid:68)(cid:85)(cid:89)(cid:68)(cid:85)(cid:71)(cid:3)(cid:56)(cid:81)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:40)(cid:71)(cid:88)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:55)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:54)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:3) (cid:62)(cid:53)(cid:72)(cid:68)(cid:71)(cid:3)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:53)(cid:50)(cid:60)(cid:36)(cid:47)(cid:3)(cid:54)(cid:55)(cid:36)(cid:55)(cid:44)(cid:54)(cid:55)(cid:44)(cid:38)(cid:36)(cid:47)(cid:3)(cid:54)(cid:50)(cid:38)(cid:44)(cid:40)(cid:55)(cid:60)(cid:3)(cid:68)(cid:87)(cid:3)(cid:68)(cid:3)(cid:80)(cid:72)(cid:72)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:85)(cid:74)(cid:68)(cid:81)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:53)(cid:40)(cid:54)(cid:40)(cid:36)(cid:53)(cid:38)(cid:43)(cid:3) (cid:54)(cid:40)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3) (cid:82)(cid:81)(cid:3)(cid:58)(cid:72)(cid:71)(cid:81)(cid:72)(cid:86)(cid:71)(cid:68)(cid:92)(cid:15)(cid:3)(cid:39)(cid:72)(cid:70)(cid:72)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:27)(cid:87)(cid:75)(cid:15)(cid:3)(cid:20)(cid:28)(cid:26)(cid:25)(cid:15)(cid:3)(cid:51)(cid:85)(cid:82)(cid:73)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:54)(cid:17)(cid:3)(cid:39)(cid:17)(cid:3)(cid:54)(cid:44)(cid:47)(cid:57)(cid:40)(cid:60)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:38)(cid:75)(cid:68)(cid:76)(cid:85)(cid:64)(cid:3) (cid:54)(cid:56)(cid:48)(cid:48)(cid:36)(cid:53)(cid:60)(cid:3) (cid:36)(cid:3)(cid:69)(cid:85)(cid:82)(cid:68)(cid:71)(cid:79)(cid:92)(cid:3)(cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:76)(cid:86)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:87)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:79)(cid:72)(cid:89)(cid:72)(cid:79)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:76)(cid:87)(cid:92)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:82)(cid:85)(cid:92)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:82)(cid:81)(cid:82)(cid:87)(cid:82)(cid:81)(cid:72)(cid:3)(cid:69)(cid:72)(cid:75)(cid:68)(cid:89)(cid:76)(cid:82)(cid:88)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:72)(cid:85)(cid:74)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:71)(cid:72)(cid:85)(cid:76)(cid:89)(cid:72)(cid:71)(cid:17)(cid:3)(cid:48)(cid:68)(cid:81)(cid:92)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:86)(cid:78)(cid:72)(cid:87)(cid:70)(cid:75)(cid:72)(cid:71)(cid:15)(cid:3)(cid:76)(cid:81)(cid:70)(cid:79)(cid:88)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:76)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:86)(cid:76)(cid:87)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:15)(cid:3)(cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:83)(cid:72)(cid:71)(cid:15)(cid:3)(cid:70)(cid:72)(cid:81)(cid:86)(cid:82)(cid:85)(cid:72)(cid:71)(cid:3)(cid:82)(cid:85)(cid:3)(cid:87)(cid:85)(cid:88)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:15)(cid:3)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:72)(cid:3)(cid:80)(cid:76)(cid:91)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)(cid:15)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3)(cid:75)(cid:92)(cid:83)(cid:72)(cid:85)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:79)(cid:92)(cid:3)(cid:85)(cid:72)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:3)(cid:79)(cid:72)(cid:68)(cid:86)(cid:87)(cid:3)(cid:86)(cid:84)(cid:88)(cid:68)(cid:85)(cid:72)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:73)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:68)(cid:81)(cid:68)(cid:79)(cid:92)(cid:86)(cid:76)(cid:86)(cid:17)(cid:3) (cid:46)(cid:72)(cid:92)(cid:90)(cid:82)(cid:85)(cid:71)(cid:86)(cid:29)(cid:3)(cid:48)(cid:36)(cid:59)(cid:44)(cid:48)(cid:56)(cid:48)(cid:3)(cid:47)(cid:44)(cid:46)(cid:40)(cid:47)(cid:44)(cid:43)(cid:50)(cid:50)(cid:39)(cid:30)(cid:3)(cid:44)(cid:49)(cid:38)(cid:50)(cid:48)(cid:51)(cid:47)(cid:40)(cid:55)(cid:40)(cid:3)(cid:39)(cid:36)(cid:55)(cid:36)(cid:30)(cid:3)(cid:40)(cid:48)(cid:3)(cid:36)(cid:47)(cid:42)(cid:50)(cid:53)(cid:44)(cid:55)(cid:43)(cid:48)(cid:30)(cid:3)(cid:51)(cid:50)(cid:54)(cid:55)(cid:40)(cid:53)(cid:44)(cid:50)(cid:53)(cid:3)(cid:48)(cid:50)(cid:39)(cid:40)(cid:3) (cid:20)(cid:17)(cid:3)(cid:44)(cid:49)(cid:55)(cid:53)(cid:50)(cid:39)(cid:56)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3) (cid:55)(cid:43)(cid:44)(cid:54)(cid:3)(cid:83)(cid:68)(cid:83)(cid:72)(cid:85)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:68)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:68)(cid:83)(cid:83)(cid:85)(cid:82)(cid:68)(cid:70)(cid:75)(cid:3)(cid:87)(cid:82)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:70)(cid:68)(cid:81)(cid:3)(cid:69)(cid:72)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:72)(cid:71)(cid:3)(cid:68)(cid:86)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:17)(cid:3)(cid:54)(cid:76)(cid:81)(cid:70)(cid:72)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:70)(cid:82)(cid:81)(cid:86)(cid:76)(cid:86)(cid:87)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:81)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:70)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:73)(cid:82)(cid:79)(cid:79)(cid:82)(cid:90)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:90)(cid:72)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:3)(cid:76)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:40)(cid:48)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:40)(cid:48)(cid:3)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3)(cid:76)(cid:86)(cid:3)(cid:85)(cid:72)(cid:80)(cid:68)(cid:85)(cid:78)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:87)(cid:3)(cid:69)(cid:72)(cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:76)(cid:87)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:82)(cid:85)(cid:92)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:87)(cid:3)(cid:69)(cid:72)(cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:76)(cid:71)(cid:72)(cid:3)(cid:85)(cid:68)(cid:81)(cid:74)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:73)(cid:68)(cid:79)(cid:79)(cid:3)(cid:88)(cid:81)(cid:71)(cid:72)(cid:85)(cid:3)(cid:76)(cid:87)(cid:86)(cid:3)(cid:88)(cid:80)(cid:69)(cid:85)(cid:72)(cid:79)(cid:79)(cid:68)(cid:17)(cid:3)(cid:58)(cid:75)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:88)(cid:81)(cid:71)(cid:72)(cid:85)(cid:79)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:70)(cid:82)(cid:80)(cid:72)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:68)(cid:81)(cid:3)(cid:72)(cid:91)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:90)(cid:75)(cid:82)(cid:86)(cid:72)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:88)(cid:80)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:72)(cid:68)(cid:86)(cid:76)(cid:79)(cid:92)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:71)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:81)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:81)(cid:3)(cid:40)(cid:48)(cid:3)(cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:90)(cid:76)(cid:86)(cid:72)(cid:3)(cid:72)(cid:68)(cid:86)(cid:76)(cid:79)(cid:92)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:71)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:87)(cid:72)(cid:85)(cid:80)(cid:3)(cid:5)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:5)(cid:3)(cid:76)(cid:81)(cid:3)(cid:76)(cid:87)(cid:86)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:76)(cid:80)(cid:83)(cid:79)(cid:76)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:91)(cid:76)(cid:86)(cid:87)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:90)(cid:82)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:86)(cid:83)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:48)(cid:60)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:59)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:16)(cid:82)(cid:81)(cid:72)(cid:3)(cid:80)(cid:68)(cid:83)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:27)(cid:3)(cid:87)(cid:82)(cid:3)(cid:58)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:92)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:68)(cid:3)(cid:85)(cid:72)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:58)(cid:17)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:86)(cid:83)(cid:82)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:91)(cid:3)(cid:76)(cid:81)(cid:3)(cid:59)(cid:3) (cid:76)(cid:86)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:79)(cid:92)(cid:15)(cid:3)(cid:69)(cid:88)(cid:87)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:76)(cid:81)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:79)(cid:92)(cid:3)(cid:87)(cid:75)(cid:85)(cid:82)(cid:88)(cid:74)(cid:75)(cid:3)(cid:92)(cid:17)(cid:3) (cid:48)(cid:82)(cid:85)(cid:72)(cid:3) (cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:79)(cid:79)(cid:92)(cid:15)(cid:3)(cid:90)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:88)(cid:80)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)(cid:80)(cid:68)(cid:83)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:91)(cid:3)(cid:16)(cid:33)(cid:92)(cid:11)(cid:91)(cid:12)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:59)(cid:3)(cid:87)(cid:82)(cid:3)(cid:38)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:91)(cid:3)(cid:76)(cid:86)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:81)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:79)(cid:76)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:27)(cid:11)(cid:92)(cid:12)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:88)(cid:69)(cid:86)(cid:72)(cid:87)(cid:3)(cid:82)(cid:73)(cid:3)(cid:11)(cid:3)(cid:71)(cid:72)(cid:87)(cid:72)(cid:85)(cid:80)(cid:76)(cid:81)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:84)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:92)(cid:3)(cid:32)(cid:3)(cid:92)(cid:11)(cid:91)(cid:12)(cid:15)(cid:3)(cid:90)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:92)(cid:3)(cid:76)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:17)(cid:3)(cid:58)(cid:72)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:91)(cid:3)(cid:68)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:72)(cid:89)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:3)(cid:76)(cid:81)(cid:3)(cid:70)(cid:72)(cid:85)(cid:87)(cid:68)(cid:76)(cid:81)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:91)(cid:3)(cid:76)(cid:81)(cid:70)(cid:79)(cid:88)(cid:71)(cid:72)(cid:86)(cid:3)(cid:90)(cid:75)(cid:68)(cid:87)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:72)(cid:71)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3) (cid:58)(cid:72)(cid:3)(cid:83)(cid:82)(cid:86)(cid:87)(cid:88)(cid:79)(cid:68)(cid:87)(cid:72)(cid:3)(cid:68)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:71)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:81)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:70)(cid:45)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:71)(cid:72)(cid:85)(cid:76)(cid:89)(cid:72)(cid:3) (cid:76)(cid:87)(cid:86)(cid:3)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:86)(cid:83)(cid:82)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:71)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3) (cid:44)(cid:3) (cid:70)(cid:45)(cid:12)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:73)(cid:11)(cid:17)(cid:17)(cid:17)(cid:3) (cid:77)(cid:3) (cid:17)(cid:17)(cid:17)(cid:12)(cid:3) (cid:76)(cid:86)(cid:3)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:74)(cid:11)(cid:17)(cid:17)(cid:17)(cid:3) (cid:44)(cid:3) (cid:17)(cid:17)(cid:17)(cid:12)(cid:3) (cid:69)(cid:92)(cid:3) (cid:74)(cid:11)(cid:92)(cid:77)(cid:70)(cid:77)(cid:23)(cid:12)(cid:12)(cid:3) (cid:73)(cid:11)(cid:91)(cid:3) (cid:20)(cid:3) (cid:12)(cid:12)(cid:3) (cid:11)(cid:17)(cid:20)(cid:12)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:87)(cid:3)(cid:73)(cid:76)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:70)(cid:45)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:72)(cid:86)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3)(cid:20)(cid:3)(cid:70)(cid:12)(cid:12)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:68)(cid:81)(cid:3) (cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:71)(cid:3)(cid:92)(cid:15)(cid:3)(cid:69)(cid:88)(cid:87)(cid:3)(cid:76)(cid:87)(cid:3)(cid:71)(cid:82)(cid:72)(cid:86)(cid:3)(cid:86)(cid:82)(cid:3)(cid:69)(cid:92)(cid:3)(cid:80)(cid:68)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:72)(cid:86)(cid:86)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:88)(cid:86)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:73)(cid:68)(cid:80)(cid:76)(cid:79)(cid:92)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:19)(cid:12)(cid:17)(cid:3) (cid:49)(cid:82)(cid:87)(cid:76)(cid:70)(cid:72)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:76)(cid:81)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3) (cid:44)(cid:70)(cid:12)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:3)(cid:83)(cid:82)(cid:86)(cid:86)(cid:76)(cid:69)(cid:79)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:16)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3) (cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:44)(cid:3) (cid:70)(cid:45)(cid:12)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:90)(cid:76)(cid:79)(cid:79)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)(cid:74)(cid:11)(cid:92)(cid:3)(cid:79)(cid:3) (cid:23)(cid:12)(cid:17)(cid:3) (cid:54)(cid:82)(cid:80)(cid:72)(cid:87)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3)(cid:68)(cid:3)(cid:81)(cid:68)(cid:87)(cid:88)(cid:85)(cid:68)(cid:79)(cid:3)(cid:70)(cid:75)(cid:82)(cid:76)(cid:70)(cid:72)(cid:3)(cid:90)(cid:76)(cid:79)(cid:79)(cid:3)(cid:69)(cid:72)(cid:3)(cid:82)(cid:69)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:15)(cid:3) (cid:68)(cid:87)(cid:3)(cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:87)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3)(cid:80)(cid:68)(cid:92)(cid:3)(cid:69)(cid:72)(cid:3)(cid:86)(cid:72)(cid:89)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:71)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:90)(cid:68)(cid:92)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:71)(cid:72)(cid:73)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:73)(cid:11)(cid:91)(cid:3) (cid:44)(cid:19)(cid:17)(cid:3) (cid:40)(cid:68)(cid:70)(cid:75)(cid:3)(cid:76)(cid:87)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:40)(cid:48)(cid:3) (cid:68)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:76)(cid:81)(cid:89)(cid:82)(cid:79)(cid:89)(cid:72)(cid:86)(cid:3)(cid:87)(cid:90)(cid:82)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:3)(cid:90)(cid:75)(cid:76)(cid:70)(cid:75)(cid:3)(cid:90)(cid:72)(cid:3)(cid:70)(cid:68)(cid:79)(cid:79)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:70)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3) (cid:11)(cid:40)(cid:16)(cid:86)(cid:87)(cid:72)(cid:83)(cid:12)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3)(cid:11)(cid:48)(cid:16)(cid:86)(cid:87)(cid:72)(cid:83)(cid:12)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:83)(cid:85)(cid:72)(cid:70)(cid:76)(cid:86)(cid:72)(cid:3)(cid:71)(cid:72)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:86)(cid:72)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:76)(cid:85)(cid:3)(cid:68)(cid:86)(cid:86)(cid:82)(cid:70)(cid:76)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:75)(cid:72)(cid:88)(cid:85)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:83)(cid:85)(cid:72)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:15)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:54)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:21)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:86)(cid:88)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:76)(cid:89)(cid:72)(cid:79)(cid:92)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)(cid:17)(cid:3)(cid:43)(cid:72)(cid:85)(cid:72)(cid:3)(cid:90)(cid:72)(cid:3)(cid:86)(cid:75)(cid:68)(cid:79)(cid:79)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:3)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:68)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:72)(cid:85)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:87)(cid:82)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:73)(cid:79)(cid:68)(cid:89)(cid:82)(cid:88)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)(cid:17)(cid:3) 0 . 0 0 . 1 0 . 2 0 . 3 0 100 200 300 400 500 Topic P r opo r t i on estimates , likelihood , maximum , parameters algorithm , algorithms , optimization , problem , efﬁcient general , examples , presented , discussed , 0 . 0 0 . 1 0 . 2 0 . 3 0 100 200 300 400 500 Topic P r opo r t i on bayesian , model , inference , models , probability image , images , algorithm , registration , segmentation web , search , semantic , text , ontology 0 . 0 0 . 1 0 . 2 0 . 3 0 100 200 300 400 500 Topic P r opo r t i on algorithm , algorithms , optimization , problem , efﬁcient problem , problems , solution , solving , solve time , timing , duration , interval , times 0 . 0 0 . 1 0 . 2 0 . 3 0 100 200 300 400 500 Topic P r opo r t i on matrix , sparse , kernel , matrices , linear image , images , algorithm , registration , segmentation sensor , network , networks , wireless , security Topic models for recommendation number of recommended articles r e c a ll 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 in−matrix l l l l l l l l l l 50 100 150 200 out−of−matrix 50 100 150 200 method l CF CTR LDA Can make predictions about current articles and new articles More than recommendation Introduction to Variational Methods for Graphical Models Conditional Random Fields Maximum likelihood from incomplete data via the EM algorithm The Mathematics of Statistical Machine Translation P a p e r s • The users also tell us about the data . • We can look at posterior estimates to ﬁnd • Widely read articles in a ﬁeld • Articles in a ﬁeld that are widely read in other ﬁelds • Articles from other ﬁelds that are widely read in a ﬁeld • These kinds of explorations require interpretable dimensions . They are not possible with classical matrix factorization . Maximum Likelihood Estimation Topic In - topic , read in topic In - topic , read in other topics Out - of - topic , read in topic estimates , likelihood , maximum , parameters , method Maximum Likelihood Estimation of Population Parameters Bootstrap Methods : Another Look at the Jackknife R . A . Fisher and the Making of Maximum Likelihood Maximum Likelihood from Incomplete Data with the EM Algorithm Bootstrap Methods : Another Look at the Jackknife Tutorial on Maximum Likelihood Estimation Random Forests Identiﬁcation of Causal Effects Using Instrumental Variables Matrix Computations Network Science networks , topology , connected , nodes , links , degree Assortative Mixing in Networks Characterizing the Dynamical Importance of Network Nodes and Links Subgraph Centrality in Complex Networks Assortative Mixing in Networks The Structure and Function of Complex Networks Statistical Mechanics of Complex Networks Power Law Distributions in Empirical Data Graph Structure in the Web The Orgins of Bursts and Heavy Tails in Human Dynamics Topic In - topic , read in topic In - topic , read in other topics Out - of - topic , read in topic Issue - adjusted ideal points • Our earlier ideal point model uses topics to predict votes from new bills . • Alternatively , we can use the text to characterize how legislators diverge from their usual ideal points . • For example : A senator might be left wing , but vote conservatively when it comes to economic matters . Issue - adjusted ideal points θ d N D K β k α W dn Z dn η X u Global ideal point Observed votes U V ud A d , B d σ 2 u σ 2 d Bill content Bill sentiment K V k Issue adjustments Issue - adjusted ideal points −2 −1 0 1 2 3 4 R ona l d P au l J i m C oope r R obe r t B e rr y M i c hae l M c C au l La m a r S m i t h T ho m a s E d w a r d s C ha r l e s D j ou T o m M c C l i n t o ck J e ff r e y | J e ff F o r t enbe rr y J oe D onne ll y D enn i s K u c i n i c h A da m S m i t h D ona l d | D on M an z u l l o A nh C ao Ideal point Taxationideal point −2 −1 0 1 2 3 4 Ideal point Healthideal point Extending LDA New applications — • Syntactic topic models • Topic models on images • Topic models on social network data • Topic models on music data • Topic models for recommendation systems Testing and relaxing assumptions — • Spike and slab priors • Models of word contagion • N - gram topic models Extending LDA θ d Z d , n W d , n K α θ d Z d , n W d , n α β k , 1 β k , 2 . . . I d I d θ d Z d , n W d , n α β k , 2 I d θ d N D K β k α W dn Z dn X u U V ud ζ d σ 2 u σ 2 d β k α Z i , n Z j , n W i , n W j , n θ i θ j Y i , j η θ d Z d , n W d , n N D K β k α Y d η , δ Z d , n W d , n N D K β k µ , Σ η θ d θ d N D K β k α W dn Z dn η X u U V ud A d , B d σ 2 u σ 2 d θ d N D K β k α W dn Z dn η X u U V ud A d , B d σ 2 u σ 2 d • Each of these models is tailored to solve a problem . • Some problems arise from new kinds of data . • Others arise from an issue with existing models . • Probabilistic modeling is a ﬂexible and modular language for designing solutions to speciﬁc problems . Latent Dirichlet Allocation Ideal Point Models Probabilistic Matrix Factorization Graph Models Generalized Linear Models SupervisedLDA Relational Topic Models Collaborative Topic Models Ideal Point Topic Models Issue - Adjusted Ideal Points Dynamic Topic Models The Impact Model Correlated Topic Models State Space Models Logistic Normal Extending LDA Make assumptions Infer the posterior Explore Collect data Predict Check Bayesian Nonparametric Models Bayesian nonparametric models • Why Bayesian nonparametric models ? • The Chinese restaurant process • Chinese restaurant process mixture models • The Chinese restaurant franchise • Bayesian nonparametric topic models • Random measures and stick - breaking constructions Why Bayesian nonparametric models ? • Topic models assume that the number of topics is ﬁxed . • It is a type of regularization parameter . It can be determined by cross validation and other model selection techniques . • Bayesian nonparametric methods skirt model selection— • The data determine the number of topics during inference . • Future data can exhibit new topics . • ( This is a ﬁeld unto itself , but has found wide application in topic modeling . ) The Chinese restaurant process ( CRP ) 1 2 3 4 5 6 7 8 9 10 • N customers arrive to an inﬁnite - table restaurant . Each sits down according to how many people are sitting at each table , p ( z i = k | z 1 : ( i − 1 ) , α ) ∝ (cid:168) n k for k ≤ K α for k = K + 1 . • The resulting seating plan provides a partition • This distribution is exchangeable : Seating plan probabilities are the same regardless of the order of customers ( Pitman , 2002 ) . CRP mixture models 1 2 3 4 5 6 7 8 9 10 β ∗ 1 β ∗ 2 β ∗ 3 β ∗ 4 β ∗ 5 • Associate each table with a topic ( β ∗ ) . Associate each customer with a data point ( grey node ) . • The number of clusters is inﬁnite a priori ; the data determines the number of clusters in the posterior . • Further : the next data point might sit at new table . • Exchangeability makes inference easy ( Escobar and West , 1995 ; Neal , 2000 ) . The CRP is not a mixed - membership model θ d Z d , n W d , n N D K β k α η • Mixture models draw each data point from one component . • The advantage of LDA is that it’s a mixed - membership model . • This is addressed by the Chinese restaurant franchise . The Chinese restaurant franchise ( Teh et al . , 2006 ) 1 2 3 4 5 6 7 8 9 10 β ∗ 1 β ∗ 2 β ∗ 3 β ∗ 4 β ∗ 5 β ∗ 2 β ∗ 1 β ∗ 1 β ∗ 1 β ∗ 2 β ∗ 3 β ∗ 4 β ∗ 1 β ∗ 4 At the corpus level , topics are drawn from a prior . Each document - level table is associated with a customer at the corpus level restaurant . Each word is associated with a customer at the document ' s restuarant . It is drawn from the topic that its table is associated with . Corpus level restaurant Document level restaurants The CRF selects the “right” number of topics ( Teh et al . , 2006 ) 10 20 30 40 50 60 70 80 90 100 110 120 750 800 850 900 950 1000 1050 P e r p l e x it y Number of LDA topics Perplexity on test abstacts of LDA and HDP mixture LDAHDP Mixture 61 62 63 64 65 66 67 68 69 70 71 72 73 0 5 10 15 Number of topics N u m b e r o f s a m p l e s Posterior over number of topics in HDP mixture Figure 3 : ( Left ) Comparison of latent Dirichlet allocation and the hierarchical Dirichlet process mixture . Results are averaged over 10 runs ; the error bars are one standard error . ( Right ) Histogram of the number of topics for the hierarchical Dirichlet process mixture over 100 posterior samples . ! G j G 0 H " 0 # G j VS Training documents x ji ji VS Test documents x ji ji ! ! G j G 0 H " 0 # G j G j VS Training documents Additional training documents x ji ji x ji ji VS Test documents x ji ji ! ! ! G j G 1 G 2 " H 0 # # 1 G 0 G j G j VS Training documents Additional training documents x ji ji x ji ji VS Test documents x ji ji ! ! M1 M2 M3 Figure 4 : Three models for the NIPS data . From left to right : M1 , M2 and M3 . 39 Extended to ﬁnd hierarchies ( Blei et al . , 2010 ) the , of a , is and n algorithmtimelogbound n functionspolynomiallogalgorithm logic programssystemslanguage sets systemsystems performanceanalysisdistributed graphgraphsedgeminimumvertices proofproperty program resolutionabstract consensusobjectsmessagesprotocolasynchronous networksqueuingasymptoticproductformserver approximationspointsdistanceconvex databaseconstraints algebrabooleanrelational formulas ﬁrstorder decisiontemporalqueries treesregulartreesearch compression machinedomaindegreedegreespolynomials routingadaptivenetworknetworksprotocols networksprotocolnetworkpacketslink databasetransactionsretrievalconcurrencyrestrictions learninglearnablestatisticalexamplesclasses m mergingnetworkssortingmultiplication constraintdependencieslocalconsistencytractable logiclogicsquerytheorieslanguages quantumautomatancautomatonlanguages online schedulingtaskcompetitivetasks learningknowledgereasoningveriﬁcationcircuit An optimal algorithm for intersecting line segments in the plane Recontamination does not help to search a graph A new approach to the maximum - flow problem The time complexity of maximum matching by simulated annealing Quantum lower bounds by polynomials On the power of bounded concurrency I : finite automata Dense quantum coding and quantum finite automata Classical physics and the Church - - Turing Thesis Nearly optimal algorithms and bounds for multilayer channel routing How bad is selfish routing ? Authoritative sources in a hyperlinked environment Balanced sequences and optimal routing Single - class bounds of multi - class queuing networks The maximum concurrent flow problem Contention in shared memory algorithms Linear probing with a nonuniform address distribution Magic Functions : In Memoriam : Bernard M . Dwork 1923 - - 1998 A mechanical proof of the Church - Rosser theorem Timed regular expressions On the power and limitations of strictness analysis Module algebra On XML integrity constraints in the presence of DTDs Closure properties of constraints Dynamic functional dependencies and database aging BNP correlated topic model ( Paisley et al . , 2011 ) { population female male } { emperor reign imperial } { site town wall } { language culture spanish } { son father brother } { church catholic roman } { language letter sound } { william lord earl } { god greek ancient } { calendar month holiday } { empire ottoman territory } { noun verb language } { colony slave independence } { building wall design } { island ship islands } { political society argue } { social theory cultural } { kill prisoner arrest } { president party elect } { international china union } { art painting artist } { battle army fight } { math function define } { mathematician numeral decimal } { capitalist socialism capitalism } { host centre football } { motion law relativity } { law convention international } { earth planet solar } { law legal court } { military army armed } { universe destroy series } { music instrument musical } { university prize award } { student university school } { wave light field } { county home population } { report fbi investigation } { sport competition event } { weapon gun design } { heat pressure mechanical } { water sub metal } { technology information organization } { jersey york uniform } { publish story publication } { company car engine } { game player character } { film scene movie } { film award director } { album song music } { game sell video } Random measures X n G α G 0 N • The CRP metaphors are the best ﬁrst way to understand BNP methods . • BNP models were originally developed as random measure models . • E . g . , data drawn independently from a random distribution : G ∼ DP ( α G 0 ) X n ∼ G • The random measure perspective helps with certain applications ( such as the BNP correlated topic model ) and for some approaches to inference . The Dirichlet process ( Ferguson , 1973 ) X n G α G 0 N • The Dirichlet process is a distribution of distributions , G ∼ DP ( α , G 0 ) • concentration parameter α ( a positive scalar ) • base distribution G 0 . • It produces distributions deﬁned on the same space as its base distribution . The Dirichlet process ( Ferguson , 1973 ) X n G α G 0 N • Consider a partition of the probability space ( A 1 , . . . , A K ) . • Ferguson : If for all partitions , 〈 G ( A 1 ) , . . . , G ( A k ) 〉 ∼ Dir ( α G 0 ( A 1 ) , . . . , α G 0 ( A K ) ) then G is distributed with a Dirichlet process . • Note : In this process , the random variables G ( A k ) are indexed by the Borel sets of the probability space . The Dirichlet process ( Ferguson , 1973 ) X n G α G 0 N • G is discrete ; it places its mass on a countably inﬁnite set of atoms . • The distribution of the locations is the base distribution G 0 . • As α gets large , G looks more like G 0 . • The conditional P ( G | x 1 : N ) is a Dirichlet process . The Dirichlet process ( Ferguson , 1973 ) X n G α G 0 N • Marginalizing out G reveals the clustering property . • The joint distribution of X 1 : N will exhibit fewer than N unique values . • These unique values are drawn from G 0 . • The distribution of the partition structure is a CRP ( α ) . The Dirichlet process mixture ( Antoniak , 1974 ) X n G α G 0 N θ n • The draw from G can be a latent parameter to an observed variable : G ∼ DP ( α , G 0 ) θ n ∼ G x n ∼ p ( · | θ n ) . • This smooths the random discrete distribution to a DP mixture . • Because of the clustering property , marginalizing out G reveals that this model is the same as a CRP mixture . Hierarchical Dirichlet processes ( Teh et al . , 2006 ) α G 0 N G m X mn H γ M θ mn • The hierarchical Dirichlet process ( HDP ) models grouped data . G 0 ∼ DP ( γ , H ) G m ∼ DP ( α , G 0 ) θ mn ∼ G m x mn ∼ p ( · | θ mn ) • Marginalizing out G 0 and G m reveals the Chinese restaurant franchise . Hierarchical Dirichlet processes ( Teh et al . , 2006 ) α G 0 N G m X mn H γ M θ mn • In topic modeling— • The atoms of G 0 are all the topics . • Each G m is a document - speciﬁc distribution over those topics • The variable θ mn is a topic drawn from G m . • The observation x mn is a word drawn from the topic θ mn . • Note that in the original topic modeling story , we worked with pointers to topics . Here the θ mn variables are distributions over words . Summary : Bayesian nonparametrics • Bayesian nonparametric modeling is a growing ﬁeld ( Hjort et al . , 2011 ) . • BNP methods can deﬁne priors over latent combinatorial structures . • In the posterior , the documents determine the particular form of the structure that is best for the corpus at hand . • Recent innovations : • Improved inference ( Blei and Jordan , 2006 , Wang et al . 2011 ) • BNP models for language ( Teh , 2006 ; Goldwater et al . , 2011 ) • Dependent models , such as time series models ( MacEachern 1999 , Dunson 2010 , Blei and Frazier 2011 ) • Predictive models ( Hannah et al . 2011 ) • Factorization models ( Grifﬁths and Ghahramani , 2011 ) Posterior Inference Posterior inference Make assumptions Infer the posterior Explore Collect data Predict Check • We can express many kinds of assumptions . • How can we analyze the collection under those assumptions ? Posterior inference Topics Documents Topic proportions and assignments • Posterior inference is the main computational problem . • Inference links observed data to statistical assumptions . • Inference on large data is crucial for topic modeling applications . Posterior inference Topics Documents Topic proportions and assignments • Our goal is to compute the distribution of the hidden variables conditioned on the documents p ( topics , proportions , assignments | documents ) Posterior inference for LDA θ d Z d , n W d , n N D K β k α η • The joint distribution of the latent variables and documents is (cid:81) K i = 1 p ( β i | η ) (cid:81) D d = 1 p ( θ d | α ) (cid:16)(cid:81) N n = 1 p ( z d , n | θ d ) p ( w d , n | β 1 : K , z d , n ) (cid:17) . • The posterior of the latent variables given the documents is p ( β , θ , z | w ) . Posterior inference for LDA θ d Z d , n W d , n N D K β k α η • This is equal to p ( β , θ , z , w ) (cid:82) β (cid:82) θ (cid:80) z p ( β , θ , z , w ) . • We can’t compute the denominator , the marginal p ( w ) . • This is the crux of the inference problem . Posterior inference for LDA θ d Z d , n W d , n N D K β k α η • There is a large literature on approximating the posterior , both within topic modeling and Bayesian statistics in general . • We will focus on mean - ﬁeld variational methods . • We will derive stochastic variational inference , a generic approximate inference method for very large data sets . Variational inference • Variational inference turns posterior inference into optimization . • The main idea— • Place a distribution over the hidden variables with free parameters , called variational parameters . • Optimize the variational parameters to make the distribution close ( in KL divergence ) to the true posterior • Variational inference can be faster than sampling - based approaches . • It is easier to handle nonconjugate models with variational inference . ( This is important in the CTM , DTM , and legislative models . ) • It can be scaled up to very large data sets with stochastic optimization . Stochastic variational inference • We want to condition on large data sets and approximate the posterior . • In variational inference , we optimize over a family of distributions to ﬁnd the member closest in KL divergence to the posterior . • Variational inference usually results in an algorithm like this : • Infer local variables for each data point . • Based on these local inferences , re - infer global variables . • Repeat . Stochastic variational inference • This is inefﬁcient . We should know something about the global structure after seeing part of the data . • And , it assumes a ﬁnite amount of data . We want algorithms that can handle data sources , information arriving in a constant stream . • With stochastic variational inference , we can condition on large data and approximate the posterior of complex models . Stochastic variational inference • The structure of the algorithm is : • Subsample the data—one data point or a small batch . • Infer local variables for the subsample . • Update the current estimate of the posterior of the global variables . • Repeat . • This is efﬁcient —we need only process one data point at a time . • We will show : Just as easy as “classical” variational inference Stochastic variational inference for LDA Sample one document Update the model Analyze it 1 Sample a document w d from the collection 2 Infer how w d exhibits the current topics 3 Create intermediate topics , formed as though the w d is the only document . 4 Adjust the current topics according to the intermediate topics . 5 Repeat . Stochastic variational inference for LDA 4096 systemshealth communication servicebillionlanguagecareroad 8192 servicesystemshealth companiesmarket communicationcompanybillion 12288 servicesystemscompanies businesscompanybillionhealthindustry 16384 service companiessystems businesscompanyindustrymarketbillion 32768 businessservicecompanies industrycompanymanagementsystemsservices 49152 businessservicecompanies industryservicescompanymanagementpublic 2048 systemsroadmade service announcednationalwestlanguage 65536 businessindustryservice companiesservicescompanymanagementpublic Documentsanalyzed Top eight words Documents seen ( log scale ) P e r p l e x i t y 600 650 700 750 800 850 900 10 3 . 5 10 4 10 4 . 5 10 5 10 5 . 5 10 6 10 6 . 5 Batch 98K Online 98K Online 3 . 3M Stochastic variational inference for LDA Sample one document Update the model Analyze it We have developed stochastic variational inference algorithms for • Latent Dirichlet allocation • The hierarchical Dirichlet process • The discrete inﬁnite logistic normal • Mixed - membership stochastic blockmodels • Bayesian nonparametric factor analysis • Recommendation models and legislative models Organization • Describe a generic class of models • Derive mean - ﬁeld variational inference in this class • Derive natural gradients for the variational objective • Review stochastic optimization • Derive stochastic variational inference Organization n x i z i β Global variables Local variables • We consider a generic model . • Hidden variables are local or global . • We use variational inference . • Optimize a simple proxy distribution to be close to the posterior • Closeness is measured with Kullback - Leibler divergence • Solve the optimization problem with stochastic optimization . • Stochastic gradients are formed by subsampling from the data . Generic model n x i z i β Global variables Local variables p ( β , z 1 : n , x 1 : n ) = p ( β ) n (cid:89) i = 1 p ( z i | β ) p ( x i | z i , β ) • The observations are x = x 1 : n . • The local variables are z = z 1 : n . • Th global variables are β . • The i th data point x i only depends on z i and β . • Our goal is to compute p ( β , z | x ) . Generic model n x i z i β Global variables Local variables p ( β , z 1 : n , x 1 : n ) = p ( β ) n (cid:89) i = 1 p ( z i | β ) p ( x i | z i , β ) • A complete conditional is the conditional of a latent variable given the observations and other latent variable . • Assume each complete conditional is in the exponential family , p ( z i | β , x i ) = h ( z i ) exp { η (cid:96) ( β , x i ) (cid:62) z i − a ( η (cid:96) ( β , x i ) ) } p ( β | z , x ) = h ( β ) exp { η g ( z , x ) (cid:62) β − a ( η g ( z , x ) ) } . Generic model n x i z i β Global variables Local variables p ( β , z 1 : n , x 1 : n ) = p ( β ) n (cid:89) i = 1 p ( z i | β ) p ( x i | z i , β ) • Bayesian mixture models • Time series models ( variants of HMMs , Kalman ﬁlters ) • Factorial models • Matrix factorization ( e . g . , factor analysis , PCA , CCA ) • Dirichlet process mixtures , HDPs • Multilevel regression ( linear , probit , Poisson ) • Stochastic blockmodels • Mixed - membership models ( LDA and some variants ) Mean - ﬁeld variational inference n x i z i β β nz i λ φ i ELBO • Introduce a variational distribution over the latent variables q ( β , z ) . • We optimize the evidence lower bound ( ELBO ) with respect to q , log p ( x ) ≥ E q [ log p ( β , Z , x ) ] − E q [ log q ( β , Z ) ] . • Up to a constant , this is the negative KL between q and the posterior . Mean - ﬁeld variational inference n x i z i β β nz i λ φ i ELBO We can derive the ELBO with Jensen’s inequality : log p ( x ) = log (cid:90) p ( β , Z , x ) dZd β = log (cid:90) p ( β , Z , x ) q ( β , Z ) q ( β , Z ) dZd β ≥ (cid:90) q ( β , Z ) log p ( β , Z , x ) q ( β , Z ) dZd β = E q [ log p ( β , Z , x ) ] − E q [ log q ( β , Z ) ] . Mean - ﬁeld variational inference n x i z i β β nz i λ φ i ELBO • We specify q ( β , z ) to be a fully factored variational distribution , q ( β , z ) = q ( β | λ ) (cid:81) n i = 1 q ( z i | φ i ) . • Each instance of each variable has its own distribution . • Each component is in the same family as the model conditional , p ( β | z , x ) = h ( β ) exp { η g ( z , x ) (cid:62) β − a ( η g ( z , x ) ) } q ( β | λ ) = h ( β ) exp { λ (cid:62) β − a ( λ ) } ( And , same for the local variational parameters . ) Mean - ﬁeld variational inference n x i z i β β nz i λ φ i ELBO • We optimize the ELBO with respect to these parameters , (cid:76) ( λ , φ 1 : n ) = E q [ log p ( β , Z , x ) ] − E q [ log q ( β , Z ) ] . • Same as ﬁnding the q ( β , z ) that is closest in KL divergence to p ( β , z | x ) • The ELBO links the observations / model to the variational distribution . Mean - ﬁeld variational inference n x i z i β β nz i λ φ i ELBO • Coordinate ascent : Iteratively update each parameter , holding others ﬁxed . • With respect to the global parameter , the gradient is ∇ λ (cid:76) = a (cid:48)(cid:48) ( λ ) ( E φ [ η g ( Z , x ) ] − λ ) . This leads to a simple coordinate update λ ∗ = E φ (cid:148) η g ( Z , x ) (cid:151) . • The local parameter is analogous . Mean - ﬁeld variational inference Initialize λ randomly . Repeat until the ELBO converges 1 For each data point , update the local variational parameters : φ ( t ) i = E λ ( t − 1 ) [ η (cid:96) ( β , x i ) ] for i ∈ { 1 , . . . , n } . 2 Update the global variational parameters : λ ( t ) = E φ ( t ) [ η g ( Z 1 : n , x 1 : n ) ] . Mean - ﬁeld variational inference for LDA θ d Z d , n W d , n N D K β k γ d φ d , n λ k • Document variables : Topic proportions θ and topic assignments z 1 : N . • Corpus variables : Topics β 1 : K • The variational distribution is q ( β , θ , z ) = K (cid:89) k = 1 q ( β k | λ k ) D (cid:89) d = 1 q ( θ d | γ d ) N (cid:89) n = 1 q ( z d , n | φ d , n ) Mean - ﬁeld variational inference for LDA θ d Z d , n W d , n N D K β k γ d φ d , n λ k • In the “local step” we iteratively update the parameters for each document , holding the topic parameters ﬁxed . γ ( t + 1 ) = α + (cid:80) N n = 1 φ ( t ) n φ ( t + 1 ) n ∝ exp { (cid:69) q [ log θ ] + (cid:69) q [ log β . , w n ] } . Mean - ﬁeld variational inference for LDA 1 8 16 26 36 46 56 66 76 86 96 Topics P r obab ili t y 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 Mean - ﬁeld variational inference for LDA θ d Z d , n W d , n N D K β k γ d φ d , n λ k • In the “global step” we aggregate the parameters computed from the local step and update the parameters for the topics , λ k = η + (cid:88) d (cid:88) n w d , n φ d , n . Mean - ﬁeld variational inference for LDA “Genetics” “Evolution” “Disease” “Computers” human evolution disease computer genome evolutionary host models dna species bacteria information genetic organisms diseases data genes life resistance computers sequence origin bacterial system gene biology new network molecular groups strains systems sequencing phylogenetic control model map living infectious parallel information diversity malaria methods genetics group parasite networks mapping new parasites software project two united new sequences common tuberculosis simulations Mean - ﬁeld variational inference for LDA 1 : Initialize topics randomly . 2 : repeat 3 : for each document do 4 : repeat 5 : Update the topic assignment variational parameters . 6 : Update the topic proportions variational parameters . 7 : until document objective converges 8 : end for 9 : Update the topics from aggregated per - document parameters . 10 : until corpus objective converges . Mean - ﬁeld variational inference Initialize λ randomly . Repeat until the ELBO converges 1 Update the local variational parameters for each data point , φ ( t ) i = E λ ( t − 1 ) [ η (cid:96) ( β , x i ) ] for i ∈ { 1 , . . . , n } . 2 Update the global variational parameters , λ ( t ) = E φ ( t ) [ η g ( Z 1 : n , x 1 : n ) ] . • Note the relationship to existing algorithms like EM and Gibbs sampling . • But we must analyze the whole data set before completing one iteration . Mean - ﬁeld variational inference Initialize λ randomly . Repeat until the ELBO converges 1 Update the local variational parameters for each data point , φ ( t ) i = E λ ( t − 1 ) [ η (cid:96) ( β , x i ) ] for i ∈ { 1 , . . . , n } . 2 Update the global variational parameters , λ ( t ) = E φ ( t ) [ η g ( Z 1 : n , x 1 : n ) ] . To make this more efﬁcient , we need two ideas : • Natural gradients • Stochastic optimization The natural gradient R IEMANNIAN C ONJUGATE G RADIENTFOR VB ! " # $ % & ' ( ) * + ' ( , % ) ) ' % ) - # $ % & ' ( ) * Figure1 : Gradient and Riemannian gradient directions are shown for the mean of distribution q . VBlearningwithadiagonalcovarianceisappliedtotheposterior p ( x , y ) ! exp [ − 9 ( xy − 1 ) 2 − x 2 − y 2 ] . The Riemannian gradient strengthens the updates in the directions where theuncertaintyislarge . the conjugate gradient algorithm with their Riemannian counterparts : Riemannian inner products and norms , parallel transport of gradient vectors between different tangent spaces as well as line searchesandstepsalonggeodesicsintheRiemannianspace . Inpracticalalgorithmssomeofthese can be approximated by their ﬂat - space counterparts . We shall apply the approximate Riemannian conjugategradient ( RCG ) methodwhichimplementsRiemannian ( natural ) gradients , innerproducts andnormsbutusesﬂat - spaceapproximationsoftheothersasouroptimisationalgorithmofchoice throughoutthepaper . AsshowninAppendixA , theseapproximationsdonotaffecttheasymptotic convergence properties of the algorithm . The difference between gradient and conjugate gradient methodsisillustratedinFigure2 . In this paper we propose using the Riemannian structure of the distributions q ( " " " | # # # ) to derive more efﬁcient algorithms for approximate inference and especially VB using approximations with aﬁxedfunctionalform . ThisdiffersfromthetraditionalnaturalgradientlearningbyAmari ( 1998 ) which uses the Riemannian structure of the predictive distribution p ( XXX | " " " ) . The proposed method can be used to jointly optimise all the parameters # # # of the approximation q ( " " " | # # # ) , or in conjunction withVBEMforsomeparameters . 3239 ( fromHonkelaetal . , 2010 ) • In natural gradient ascent , we premultiply the gradient by the inverse of a Riemannian metric . Amari ( 1998 ) showed this is the steepest direction . • For distributions , the Riemannian metric is the Fisher information . The natural gradient n x i z i β β nz i λ φ i ELBO • In the exponential family , the Fisher information is the second derivative of the log normalizer , G = a (cid:48)(cid:48) ( λ ) . • So , the natural gradient of the ELBO is ˆ ∇ λ (cid:76) = E φ [ η g ( Z , x ) ] − λ . • We can compute the natural gradient by computing the coordinate updates in parallel and subtracting the current variational parameters . Stochastic optimization Institute of Mathematical Statistics is collaborating with JSTOR to digitize , preserve , and extend access to The Annals of Mathematical Statistics . www . jstor . org ® • Why waste time with the real gradient , when a cheaper noisy estimate of the gradient will do ( Robbins and Monro , 1951 ) ? • Idea : Follow a noisy estimate of the gradient with a step - size . • By decreasing the step - size according to a certain schedule , we guarantee convergence to a local optimum . Stochastic optimization n x i z i β β nz i λ φ i ELBO • We will use stochastic optimization for global variables . • Let ∇ λ (cid:76) t be a realization of a random variable whose expectation is ∇ λ (cid:76) . • Iteratively set λ ( t ) = λ ( t − 1 ) + ε t ∇ λ (cid:76) t • This leads to a local optimum when (cid:80) ∞ t = 1 ε t = ∞ (cid:80) ∞ t = 1 ε 2 t < ∞ • Next step : Form a noisy gradient . A noisy natural gradient n x i z i β β nz i λ φ i ELBO • We need to look more closely at the conditional distribution of the global hidden variable given the local hidden variables and observations . • The form of the local joint distribution is p ( z i , x i | β ) = h ( z i , x i ) exp { β (cid:62) f ( z i , x i ) − a ( β ) } . This means the conditional parameter of β is η g ( z 1 : n , x 1 : n ) = 〈 α 1 + (cid:80) n i = 1 f ( z i , x i ) , α 2 + n 〉 . • See the discussion of conjugacy in Bernardo and Smith ( 1994 ) . A noisy natural gradient • With local and global variables , we decompose the ELBO (cid:76) = E [ log p ( β ) ] − E [ log q ( β ) ] + (cid:80) n i = 1 E [ log p ( z i , x i | β ) ] − E [ log q ( z i ) ] • Sample a single data point t uniformly from the data and deﬁne (cid:76) t = E [ log p ( β ) ] − E [ log q ( β ) ] + n ( E [ log p ( z t , x t | β ) ] − E [ log q ( z t ) ] ) . 1 . The ELBO is the expectation of (cid:76) t with respect to the sample . 2 . The gradient of the t - ELBO is a noisy gradient of the ELBO . 3 . The t - ELBO is like an ELBO where we saw x t repeatedly . A noisy natural gradient • Deﬁne the conditional as though our whole data set is n replications of x t , η t ( z t , x t ) = 〈 α 1 + n · f ( z t , x t ) , α 2 + n 〉 • The noisy natural gradient of the ELBO is ∇ λ ˆ (cid:76) t = E φ t [ η t ( Z t , x t ) ] − λ . • This only requires the local variational parameters of one data point . • In contrast , the full natural gradient requires all local parameters . Stochastic variational inference Initialize global parameters λ randomly . Set the step - size schedule ε t appropriately . Repeat forever 1 Sample a data point uniformly , x t ∼ Uniform ( x 1 , . . . , x n ) . 2 Compute its local variational parameter , φ = E λ ( t − 1 ) [ η (cid:96) ( β , x t ) ] . 3 Pretend its the only data point in the data set , ˆ λ = E φ [ η t ( Z t , x t ) ] . 4 Update the current global variational parameter , λ ( t ) = ( 1 − ε t ) λ ( t − 1 ) + ε t ˆ λ . Stochastic variational inference in LDA θ d Z d , n W d , n N D K β k γ d φ d , n λ k 1 Sample a document 2 Estimate the local variational parameters using the current topics 3 Form “fake topics” from those local parameters 4 Update the topics to be a weighted average of “fake” and current topics Stochastic variational inference in LDA 1 : Deﬁne ρ t (cid:172) ( τ 0 + t ) − κ 2 : Initialize λ randomly . 3 : for t = 0 to ∞ do 4 : Choose a random document w t 5 : Initialize γ tk = 1 . ( The constant 1 is arbitrary . ) 6 : repeat 7 : Set φ t , n ∝ exp { (cid:69) q [ log θ t ] + (cid:69) q [ log β · , w n ] } 8 : Set γ t = α + (cid:80) n φ t , n 9 : until 1 K (cid:80) k | change in γ t , k | < ε 10 : Compute ˜ λ k = η + D (cid:80) n w t , n φ t , n 11 : Set λ k = ( 1 − ρ t ) λ k + ρ t ˜ λ k . 12 : end for Stochastic variational inference in LDA 4096 systemshealth communication servicebillionlanguagecareroad 8192 servicesystemshealth companiesmarket communicationcompanybillion 12288 servicesystemscompanies businesscompanybillionhealthindustry 16384 service companiessystems businesscompanyindustrymarketbillion 32768 businessservicecompanies industrycompanymanagementsystemsservices 49152 businessservicecompanies industryservicescompanymanagementpublic 2048 systemsroadmade service announcednationalwestlanguage 65536 businessindustryservice companiesservicescompanymanagementpublic Documentsanalyzed Top eight words Documents seen ( log scale ) P e r p l e x i t y 600 650 700 750 800 850 900 10 3 . 5 10 4 10 4 . 5 10 5 10 5 . 5 10 6 10 6 . 5 Batch 98K Online 98K Online 3 . 3M Stochastic variational inference n x i z i β β nz i λ φ i ELBO We deﬁned a generic algorithm for scalable variational inference . • Bayesian mixture models • Time series models ( variants of HMMs , Kalman ﬁlters ) • Factorial models • Matrix factorization ( e . g . , factor analysis , PCA , CCA ) • Dirichlet process mixtures , HDPs • Multilevel regression ( linear , probit , Poisson ) • Stochastic blockmodels • Mixed - membership models ( LDA and some variants ) Stochastic variational inference n x i z i β β nz i λ φ i ELBO • See Hoffman et al . ( 2010 ) for LDA ( and code ) . • See Wang et al . ( 2010 ) for Bayesian nonparametric models ( and code ) . • See Sato ( 2001 ) for the original stochastic variational inference . • See Honkela et al . ( 2010 ) for natural gradients and variational inference . Stochastic variational inference Make assumptions Infer the posterior Explore Collect data Predict Check • Many applications posit a model , condition on data , and use the posterior . • We can now apply this kind of data analysis to very large data sets . Nonconjugate variational inference • The class of conditionally conjugate models is very ﬂexible . • However , some models—like the CTM and DTM—do not ﬁt in . • In the past , researchers developed tailored optimization procedures for ﬁtting the variational objective . • We recently developed a more general approach that subsumes many of these strategies . Nonconjugate variational inference • Bishop ( 2006 ) showed that the optimal mean - ﬁeld variational distribution is q ∗ ( z ) ∝ exp (cid:166) E q ( β ) [ log p ( z | β , x ) ] (cid:169) q ∗ ( β ) ∝ exp (cid:166) E q ( z ) [ log p ( β | z , x ) ] (cid:169) • In conjugate models , we can compute these expectations . This determines the form of the optimal variational distribution . • In nonconjugate models we can’t compute the expectations . • But , under certain conditions , we can use Taylor approximations . This leads to Gaussian variational distributions . Using and Checking Topic Models Using and checking topic models Make assumptions Infer the posterior Explore Collect data Predict Check • We have collected data , selected a model , and inferred the posterior . • How do we use the topic model ? Using and checking topic models Make assumptions Infer the posterior Explore Collect data Predict Check • Using a model means doing something with the posterior inference . • E . g . , visualization , prediction , assessing document similarity , using the representation in a downstream task ( like IR ) Using and checking topic models number of recommended articles r e c a ll 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 in−matrix l l l l l l l l l l 50 10 0 15 0 20 0 out−of−matrix 50 10 0 15 0 20 0 method l CF CTR LDA • Questions we ask when evaluating a model : • Does my model work ? Is it better than another model ? • Which topic model should I choose ? Should I make a new one ? • These questions are tied up in the application at hand . • Sometimes evaluation is straightforward , especially in prediction tasks . Using and checking topic models • But a promise of topic models is that they give good exploratory tools . Evaluation is complicated , e . g . , is this a good navigator of my collection ? • And this leads to more questions : • How do I interpret a topic model ? • What quantities help me understand what it says about the data ? Using and checking topic models • How to interpret and evaluate topic models is an active area of research . • Visualizing topic models • Naming topics • Matching topic models to human judgements • Matching topic models to external ontologies • Computing held out likelihoods in different ways • I will discuss two components : • Predictive scores for evaluating topic models • Posterior predictive checks for topic modeling The predictive score • Assess how well a model can predict future data • In text , a natural setting is one where we observe part of a new document and want to predict the remainder . • The predictive distribution is a distribution conditioned on the corpus and the partial document , p ( w | (cid:68) , w obs ) = (cid:90) β (cid:90) θ (cid:16)(cid:80) K k = 1 θ k β k , w (cid:17) p ( θ | w obs , β ) p ( β | (cid:68) ) ≈ (cid:90) β (cid:90) θ (cid:16)(cid:80) K k = 1 θ k β k , w (cid:17) q ( θ ) q ( β ) = E q [ θ | w obs ] (cid:62) E q [ β · , w | (cid:68) ] . The predictive score • The predictive score evaluates the remainder of the document independently under this distribution . s = (cid:88) w ∈ w heldout log p ( w | (cid:68) , w obs ) ( 1 ) • In the predictive distribution , q is any approximate poterior . This puts various models and inference procedures on the same scale . • ( In contrast , perplexity of entire held out documents requires different approximations for each inference method . ) The predictive score Nature New York Times Wikipedia LDA 100 - 7 . 26 - 7 . 66 - 7 . 41 LDA 200 - 7 . 50 - 7 . 78 - 7 . 64 LDA 300 - 7 . 86 - 7 . 98 - 7 . 74 HDP - 6 . 97 - 7 . 38 - 7 . 07 The predictive score on large corpora using stochastic variational inference Posterior predictive checks • The predictive score and other model selection criteria are good for choosing among several models . • But they don’t help with the model building process ; they don’t tell us how a model is misﬁt . ( E . g . should I go from LDA to a DTM or LDA to a CTM ? ) • Further , prediction is not always important in exploratory or descriptive tasks . We may want models that capture other aspects of the data . • Posterior predictive checks are a technique from Bayesian statistics that help with these issues . Posterior predictive checks – This feels even more relevant today . I think of modeling as piecing together various modules , rather than choosing among a population of models . – Machine learning has given us many new building blocks , but has little to say about how to diagnose models . – This is especially important in exploratory analysis , e . g . , to form hypotheses or organize data . Many exploratory tasks do not have clear measures of quality . • Automating model building is a tall order . Even BNP methods do not automate it . – They help deﬁne ﬂexible models , but it is up to the modeler to deﬁne likelihood functions , dependencies between the observed data and latent variables , etc . 2 The predictive check • Box ( 1980 ) describes a predictive check , which tells the story . ( Though this story will be reﬁned in a posterior predictive check . ) • All the intuitions about how to assess a model are in this picture : • The set up from Box ( 1980 ) is the following . – The data are y ; the hidden variables are θ ; the model is M . – Each point of the hidden variable θ yields a distribution of data . – The joint distribution combines the prior and the likelihood p ( y , θ | M ) = p ( y | θ ) p ( θ | M ) ( 1 ) 2 This is a predictive check from Box ( 1980 ) . Posterior predictive checks • Three stages to model building : estimation , criticism , and revision . • In criticism , the model “confronts” our data . • Suppose we observe a data set y . The predictive distribution is the distribution of data if the model is true : p ( y | M ) = (cid:90) θ p ( y | θ ) p ( θ ) • Locating y in the predictive distribution indicates if we can “trust” the model . • Or , locating a discrepancy function g ( y ) in its predictive distribution indicates if what is important to us is captured in the model . Posterior predictive checks • Rubin ( 1984 ) located the data y in the posterior p ( y | y , M ) . • Gelman , Meng , Stern ( 1996 ) expanded this idea to “realized discrepancies” that include hidden variables g ( y , z ) . • We might make modeling decisions based on a variety of simplifying considerations ( e . g . , algorithmic ) . But we can design the realized discrepancy function to capture what we really care about . • Further , realized discrepancies let us consider which parts of the model ﬁt well and which parts don’t . This is apt in exploratory tasks . Posterior predictive checks in topic models • Consider a decomposition of a corpus into topics , i . e . , { w d , n , z d , n } . Note that z d , n is a latent variable . • For all the observations assigned to a topic , consider the variable { w d , n , d } . This is the observed word and the document it appeared in . • One measure of how well a topic model ﬁts the LDA assumptions is to look at the per - topic mutual information between w and d . • If the words from the topic are independently generated then we expect lower mutual information . • What is “low” ? To answer that , we can shufﬂe the words and recompute . This gives values of the MI when the words are independent . Posterior predictive checks in topic models 80 COMMUNICATIONS OF THE ACM | APRIL 2012 | VOL . 55 | NO . 4 review articles observed variables . This conditional distribution is also called the posterior distribution . LDA falls precisely into this frame - work . The observed variables are the words of the documents ; the hidden variables are the topic structure ; and the generative process is as described here . The computational problem of inferring the hidden topic structure from the documents is the problem of computing the posterior distribution , the conditional distribution of the hid - den variables given the documents . We can describe LDA more formally with the following notation . The topics are b 1 : K , where each b k is a distribution over the vocabulary ( the distributions over words at left in Figure 1 ) . The topic proportions for the d th document are q d , where q d , k is the topic proportion for topic k in document d ( the car - toon histogram in Figure 1 ) . The topic assignments for the d th document are z d , where z d , n is the topic assignment for the n th word in document d ( the colored coin in Figure 1 ) . Finally , the observed words for document d are w d , where w d , n is the n th word in document d , which is an element from the ﬁxed vocabulary . With this notation , the generative process for LDA corresponds to the fol - lowing joint distribution of the hidden and observed variables , ( 1 ) Notice that this distribution speciﬁes a number of dependencies . For example , the topic assignment z d , n depends on the per - document topic proportions q d . As another example , the observed word w d , n depends on the topic assign - ment z d , n and all of the topics b 1 : K . ( Operationally , that term is deﬁned by looking up as to which topic z d , n refers to and looking up the probability of the word w d , n within that topic . ) These dependencies deﬁne LDA . They are encoded in the statistical assumptions behind the generative process , in the particular mathemati - cal form of the joint distribution , and— in a third way—in the probabilistic graphical model for LDA . Probabilistic graphical models provide a graphical language for describing families of probability distributions . e The graphi - cal model for LDA is in Figure 4 . These three representations are equivalent ways of describing the probabilistic assumptions behind LDA . In the next section , we describe the inference algorithms for LDA . However , we ﬁrst pause to describe the short history of these ideas . LDA was developed to ﬁx an issue with a previ - ously developed probabilistic model probabilistic latent semantic analysis ( pLSI ) . 21 That model was itself a prob - abilistic version of the seminal work on latent semantic analysis , 14 which revealed the utility of the singular value decomposition of the document - term matrix . From this matrix factorization perspective , LDA can also be seen as a type of principal component analysis for discrete data . 11 , 12 Posterior computation for LDA . We now turn to the computational e The ﬁeld of graphical models is actually more than a language for describing families of distributions . It is a ﬁeld that illuminates the deep mathematical links between probabi - listic independence , graph theory , and algo - rithms for computing with probability distri - butions . 35 Figure 3 . A topic model ﬁt to the Yale Law Journal . Here , there are 20 topics ( the top eight are plotted ) . Each topic is illustrated with its top - most frequent words . Each word’s position along the x - axis denotes its speciﬁcity to the documents . For example “estate” in the ﬁrst topic is more speciﬁc than “tax . ” 4 consumption earnings estate exemption funds income organizations revenue subsidies tax taxation taxes taxpayers treasury year 6 crime crimes defendantdefendants evidence guilty judge judges jurors jury offense punishment sentence sentencing trial 10 bargaining collective employee employees employeremployersemployment industrial job labor union unions work worker workers 15 amendment conduct content contextcultureequality expression free freedom ideas information protect protected speech values 3 child children discrimination family female gender male marriage men parents sex sexual social woman women 1 assets capital corporate cost efficient firm firms insurance market offer price share shareholders stock value 13 agreement bargaining breach contract contracting contracts contractual creditors debtexchange liability limited parties party terms 16 amendment article citizens constitution constitutional fourteenth government history justice legislative majority opinion people political republican • This realized discrepancy measures model ﬁtness • Can use it to measure model ﬁtness per topic . • Helps us explore parts of the model that ﬁt well . Discussion Probabilistic topic models • What are topic models ? • What kinds of things can they do ? • How do I compute with a topic model ? • How do I evaluate and check a topic model ? • What are some unanswered questions in this ﬁeld ? • How can I learn more ? Introduction to topic modeling gene 0 . 04 dna 0 . 02 genetic 0 . 01 . , , life 0 . 02 evolve 0 . 01 organism 0 . 01 . , , brain 0 . 04 neuron 0 . 02 nerve 0 . 01 . . . data 0 . 02 number 0 . 02 computer 0 . 01 . , , Topics Documents Topic proportions and assignments • LDA assumes that there are K topics shared by the collection . • Each document exhibits the topics with different proportions . • Each word is drawn from one topic . • We discover the structure that best explain a corpus . Extensions of LDA θ d Z d , n W d , n K α θ d Z d , n W d , n α β k , 1 β k , 2 . . . I d I d θ d Z d , n W d , n α β k , 2 I d θ d N D K β k α W dn Z dn X u U V ud ζ d σ 2 u σ 2 d β k α Z i , n Z j , n W i , n W j , n θ i θ j Y i , j η θ d Z d , n W d , n N D K β k α Y d η , δ Z d , n W d , n N D K β k µ , Σ η θ d θ d N D K β k α W dn Z dn η X u U V ud A d , B d σ 2 u σ 2 d θ d N D K β k α W dn Z dn η X u U V ud A d , B d σ 2 u σ 2 d Topic models can be adapted to many settings • relax assumptions • combine models • model more complex data Posterior inference 4096 systemshealth communication service billion languagecareroad 8192 servicesystemshealth companies market communicationcompanybillion 12288 servicesystemscompanies business companybillionhealthindustry 16384 service companiessystems business companyindustrymarketbillion 32768 businessservicecompanies industry companymanagementsystemsservices 49152 businessservicecompanies industry servicescompanymanagementpublic 2048 systemsroadmade service announcednationalwestlanguage 65536 businessindustryservice companies servicescompanymanagementpublic Documentsanalyzed Top eight words Documents seen ( log scale ) P e r p l e x i t y 600 650 700 750 800 850 900 10 3 . 5 10 4 10 4 . 5 10 5 10 5 . 5 10 6 10 6 . 5 Batch 98K Online 98K Online 3 . 3M • Posterior inference is the central computational problem . • Stochastic variational inference is a scalable algorithm . • We can handle nonconjugacy with Laplace inference . • ( Note : There are many types of inference we didn’t discuss . ) Posterior predictive checks 80 COMMUNICATIONS OF THE ACM | APRIL 2012 | VOL . 55 | NO . 4 review articles observed variables . This conditional distribution is also called the posterior distribution . LDA falls precisely into this frame - work . The observed variables are the words of the documents ; the hidden variables are the topic structure ; and the generative process is as described here . The computational problem of inferring the hidden topic structure from the documents is the problem of computing the posterior distribution , the conditional distribution of the hid - den variables given the documents . We can describe LDA more formally with the following notation . The topics are b 1 : K , where each b k is a distribution over the vocabulary ( the distributions over words at left in Figure 1 ) . The topic proportions for the d th document are q d , where q d , k is the topic proportion for topic k in document d ( the car - toon histogram in Figure 1 ) . The topic assignments for the d th document are z d , where z d , n is the topic assignment for the n th word in document d ( the colored coin in Figure 1 ) . Finally , the observed words for document d are w d , where w d , n is the n th word in document d , which is an element from the ﬁxed vocabulary . With this notation , the generative process for LDA corresponds to the fol - lowing joint distribution of the hidden and observed variables , ( 1 ) Notice that this distribution speciﬁes a number of dependencies . For example , the topic assignment z d , n depends on the per - document topic proportions q d . As another example , the observed word w d , n depends on the topic assign - ment z d , n and all of the topics b 1 : K . ( Operationally , that term is deﬁned by looking up as to which topic z d , n refers to and looking up the probability of the word w d , n within that topic . ) These dependencies deﬁne LDA . They are encoded in the statistical assumptions behind the generative process , in the particular mathemati - cal form of the joint distribution , and— in a third way—in the probabilistic graphical model for LDA . Probabilistic graphical models provide a graphical language for describing families of probability distributions . e The graphi - cal model for LDA is in Figure 4 . These three representations are equivalent ways of describing the probabilistic assumptions behind LDA . In the next section , we describe the inference algorithms for LDA . However , we ﬁrst pause to describe the short history of these ideas . LDA was developed to ﬁx an issue with a previ - ously developed probabilistic model probabilistic latent semantic analysis ( pLSI ) . 21 That model was itself a prob - abilistic version of the seminal work on latent semantic analysis , 14 which revealed the utility of the singular value decomposition of the document - term matrix . From this matrix factorization perspective , LDA can also be seen as a type of principal component analysis for discrete data . 11 , 12 Posterior computation for LDA . We now turn to the computational e The ﬁeld of graphical models is actually more than a language for describing families of distributions . It is a ﬁeld that illuminates the deep mathematical links between probabi - listic independence , graph theory , and algo - rithms for computing with probability distri - butions . 35 Figure 3 . A topic model ﬁt to the Yale Law Journal . Here , there are 20 topics ( the top eight are plotted ) . Each topic is illustrated with its top - most frequent words . Each word’s position along the x - axis denotes its speciﬁcity to the documents . For example “estate” in the ﬁrst topic is more speciﬁc than “tax . ” 4 consumption earnings estate exemption funds income organizations revenue subsidies tax taxation taxes taxpayers treasury year 6 crime crimes defendantdefendants evidence guilty judge judges jurors jury offense punishment sentence sentencing trial 10 bargaining collective employee employees employeremployersemployment industrial job labor union unions work worker workers 15 amendment conduct content contextcultureequality expression free freedom ideas information protect protected speech values 3 child children discrimination family female gender male marriage men parents sex sexual social woman women 1 assets capital corporate cost efficient firm firms insurance market offer price share shareholders stock value 13 agreement bargaining breach contract contracting contracts contractual creditors debtexchange liability limited parties party terms 16 amendment article citizens constitution constitutional fourteenth government history justice legislative majority opinion people political republican Probabilistic models Make assumptions Infer the posterior Explore Collect data Predict Check Implementations of LDA There are many available implementations of topic modeling . Here is an incomplete list— LDA - C ∗ A C implementation of LDA HDP ∗ A C implementation of the HDP ( “inﬁnite LDA” ) Online LDA ∗ A python package for LDA on massive data LDA in R ∗ Package in R for many topic models LingPipe Java toolkit for NLP and computational linguistics Mallet Java toolkit for statistical NLP TMVE ∗ A python package to build browsers from topic models ∗ available at www . cs . princeton . edu / ∼ blei / Research opportunities in topic modeling • New applications of topic modeling What methods should we develop to solve problems in the computational social sciences ? The digital humanties ? Digital medical records ? • Interfaces and downstream applications of topic modeling What can I do with an annotated corpus ? How can I incorporate latent variables into a user interface ? How should I visualize a topic model ? • Model interpretation and model checking Which model should I choose for which task ? What does the model tell me about my corpus ? Research opportunities in topic modeling • Incorporating corpus , discourse , or linguistic structure How can our knowledge of language help inform better topic models ? • Prediction from text What is the best way to link topics to prediction ? • Theoretical understanding of approximate inference What do we know about variational inference ? Can we analyze it from either the statistical or learning perspective ? What are the relative advantages of the many inference methods ? • And many speciﬁc problems E . g . , sensitivity to the vocabulary , modeling word contagion , modeling complex trends in dynamic models , robust topic modeling , combining graph models with relational models , . . . “We should seek out unfamiliar summaries of observational material , and establish their useful properties . . . And still more novelty can come from ﬁnding , and evading , still deeper lying constraints . ” ( J . Tukey , The Future of Data Analysis , 1962 ) “Despite all the computations , you could just dance to the rock ’n’ roll station . ” ( The Velvet Underground , Rock & Roll , 1969 )