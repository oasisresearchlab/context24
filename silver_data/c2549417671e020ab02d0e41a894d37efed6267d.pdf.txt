Survey : Finite - State Technology in Natural Language Processing 1 Andreas Maletti a a Institute for Natural Language Processing , Universität Stuttgart , Pfa ﬀ enwaldring 5b , 70569 Stuttgart , Germany Abstract In this survey , we will discuss current uses of ﬁnite - state information in several statistical natural language processing tasks . To this end , we will review standard approaches in tokenization , part - of - speech tagging , and parsing , and illustrate the utility of ﬁnite - state information and technology in these areas . The particular problems were chosen to allow a natural progression from simple prediction to structured prediction . We aim for a su ﬃ ciently formal presentation suitable for readers with a background in automata theory that allows to appreciate the contribution of ﬁnite - state approaches , but we will not discuss practical issues outside the core ideas . We provide instructive examples and pointers into the relevant literature for all constructions . We close with an outlook on ﬁnite - state technology in statistical machine translation . Keywords : ﬁnite - state automaton , tree automaton , context - free grammar , natural language processing , tokenization , part - of - speech tagging , parsing , machine translation 1 . Introduction Finite - state technology has played a major part in the development of several state - of - the - art tools in natural language processing [ 1 , 2 ] . In this survey , we will recall three major problems in the domain of natural language processing , whose state - of - the - art solutions have successfully utilized ﬁnite - state technology . Several other areas ( e . g . , compu - tational morphology ) have beneﬁtted from ﬁnite - state technology as well , so we necessarily had to select . We chose the mentioned topics because they allow a nice progression from simple prediction to structured prediction , and in particular , require no deep linguistic background . In general , this survey is written for the theoretical computer scien - tist , so concepts from the area of formal languages are introduced rather tersely . Additional room is provided for the explanation of the application problems , their solutions in terms of automata , and their evaluation . References to the relevant literature are provided in the content sections , which are supposed to be rather self - contained , although we reuse some methodology in the parsing section . In all three applications , an inital model is extracted from annotated ( positive ) training data using standard methods in supervised training . This initial model is then transformed into an automaton for further processing . In the area of tokenization , the ﬁnite - state technology essentially delivers the operations required to work with the extracted model . We illustrate this on the basic approach , in which we model a scorer for fully annotated data . Using standard ﬁnite - state operations we can obtain a scorer for unannotated data from this scorer . In the area of part - of - speech tagging we follow a similar approach and ﬁrst model a scorer for the fully annotated data . However , in the next step we show how such a model can be optimized to given unannotated training data with the help of the well - known forward - backward algorithm . In this unsupervised optimization step , the structure of the automaton remains the same , but the transition weights are adjusted to better ﬁt the unannotated data . Practical considerations like overﬁtting the data ( i . e . , the over - adjustment of the automaton such that it exactly represents only the training data ) will not be discussed to allow a clean presentation . In the last part , we will discuss the classical parsing problem for natural languages . Whereas we discuss weighted ﬁnite - state automata in the ﬁrst two application areas , we will mostly talk about weighted context - free grammars and Email address : maletti @ ims . uni - stuttgart . de ( Andreas Maletti ) 1 Research ﬁnancially supported by the German Research Foundation ( DFG ) grant MA 4959 / 1 - 1 . Preprint submitted to Theoretical Computer Science April 19 , 2016 weighted tree automata in this section . A preliminary section is provided to recall those basic notions . As before , we use supervised training to obtain an initial ( local ) model from annotated training data . Next we demonstrate a successful approach that combines the weight adjustment procedure , which we already discussed for part - of - speech tagging , with a change of the structure of the automaton . In this case , the process derives truly hidden ﬁnite - state information that is not present in the fully annotated training data , so this optimization is again unsupervised . This automatic deduction of hidden ﬁnite - state information outperforms all comparable e ﬀ orts to manually provide addi - tional annotation for parsing models in the training data . We conclude with a short outlook on the use of ﬁnite - state information in statistical machine translation . In this area , ﬁnite - state models are used to model the translation pro - cess , but typically local variants , which do not actually use the power of “hidden” ﬁnite - state information ( i . e . , they only use clues from the part of the input currently under consideration ) , are actually used . Some minor uses have been demonstrated in the literature , but the majority of the used models remains essentially local . 2 . String automata 2 . 1 . Basic notions We denote the set of nonnegative integers by N . An alphabet is simply a ﬁnite set . The set of all sequences ( or words ) over the alphabet Σ is denoted by Σ ∗ , of which ε ∈ Σ ∗ is the empty word . The length of a word w ∈ Σ ∗ is denoted by | w | , and we write w ( i ) to denote the i - th letter in w for all 1 ≤ i ≤ | w | . Given 1 ≤ i ≤ j ≤ | w | , we let w [ i , j ] = w ( i ) · · · w ( j ) be the substring from position i to j in w . We will use semirings [ 3 , 4 ] as weight structures . A semiring is an algebraic structure ( S , + , · , 0 , 1 ) that consists of two monoids ( S , + , 0 ) and ( S , · , 1 ) such that ( i ) the additive monoid ( S , + , 0 ) is commutative and ( ii ) the generalized distributivity law holds for both directions ; i . e . , (cid:16) k (cid:88) i = 1 s i (cid:17) · s = k (cid:88) i = 1 ( s i · s ) and s · (cid:16) k (cid:88) i = 1 s i (cid:17) = k (cid:88) i = 1 ( s · s i ) for all k ∈ N and s , s 1 , . . . , s k ∈ S . The semiring ( S , + , · , 0 , 1 ) is commutative if its multiplicative monoid ( S , · , 1 ) is commutative . Two commutative semirings will be particularly relevant for this contribution : • the semiring ( Q ≥ 0 , + , · , 0 , 1 ) of nonnegative rational numbers together with the usual operations and • the semiring ( [ 0 , 1 ] , max , · , 0 , 1 ) of the ( rational ) unit interval together with the maximum ( as addition ) and the usual multiplication . In the following , let ( S , + , · , 0 , 1 ) be a semiring . Given a mapping f : A → S , we let supp ( f ) = { a ∈ A | f ( a ) (cid:44) 0 } . Finite - state automata [ 5 ] have been extended already in the 1960s to handle weights [ 6 ] . The recent handbook [ 7 ] provides an in - depth discussion of the developed theory and its applications ( also NLP applications in chapter 14 [ 8 ] ) . A weighted string automaton [ over the semiring ( S , + , · , 0 , 1 ) ] is a tuple A = ( Q , Σ , I , wt , F ) , where Q is a ﬁnite set of states , Σ an alphabet , I : Q → S is an initial weight assignment , wt : Q × Σ × Q → S is a transition weight assign - ment , and F : Q → S is a ﬁnal weight assignment . The labeled graph underlying the transition weight assignment is supp ( wt ) = { (cid:104) q , σ , p (cid:105) | wt ( q , σ , p ) (cid:44) 0 } . The automaton A is acyclic if the underlying graph supp ( wt ) has no cycle ; i . e . , there exists no sequence (cid:104) q 0 , σ 1 , q 1 (cid:105) , . . . , (cid:104) q k − 1 , σ k , q k (cid:105) of elements of supp ( wt ) such that q 0 = q k . Moreover , it is deterministic if I ( q ) (cid:44) 0 for at most one q ∈ Q and for every q ∈ Q and σ ∈ Σ , there exists at most one p ∈ Q such that (cid:104) q , σ , p (cid:105) ∈ supp ( wt ) . Finally , the automaton A is unambiguous if for every k ∈ N and σ 1 , . . . , σ k ∈ Σ there exists at most one sequence (cid:104) q 0 , σ 1 , q 1 (cid:105) , . . . , (cid:104) q k − 1 , σ k , q k (cid:105) of elements of supp ( wt ) such that I ( q 0 ) (cid:44) 0 and F ( q k ) (cid:44) 0 . For such a weighted string automaton A and q , p ∈ Q , we deﬁne the mapping A q , p : Σ ∗ → S recursively by A q , p ( ε ) =  1 if q = p 0 otherwise and A q , p ( σ w ) = (cid:88) r ∈ Q (cid:16) wt ( q , σ , r ) · A r , p ( w ) (cid:17) ( 1 ) for all σ ∈ Σ and w ∈ Σ ∗ . The weighted string automaton A generates the mapping A : Σ ∗ → S given by A ( w ) = (cid:80) q , p ∈ Q (cid:0) I ( q ) · A q , p ( w ) · F ( p ) (cid:1) for every w ∈ Σ ∗ . The such generated mappings are called the regular weighted string languages , which are closed under several important operations . For example , when the semiring is commuta - tive , they are closed under the H adamard product [ 9 ] , which generalizes the classical intersection and for two given mappings f , g : Σ ∗ → S returns the mapping ( f · g ) : Σ ∗ → S given by (cid:0) f · g (cid:1) ( w ) = f ( w ) · g ( w ) for all w ∈ Σ ∗ . Given 2 two weighted string automata A 1 = ( Q 1 , Σ , I 1 , wt 1 , F 1 ) and A 2 = ( Q 2 , Σ , I 2 , wt 2 , F 2 ) , their H adamard product is the weighted string automaton A 1 · A 2 = ( Q 1 × Q 2 , Σ , I , wt , F ) given by I (cid:0) (cid:104) q 1 , q 2 (cid:105) (cid:1) = I 1 ( q 1 ) · I 2 ( q 2 ) , wt (cid:0) (cid:104) q 1 , q 2 (cid:105) , σ , (cid:104) q (cid:48) 1 , q (cid:48) 2 (cid:105) (cid:1) = wt ( q 1 , σ , q (cid:48) 1 ) · wt ( q 2 , σ , q (cid:48) 2 ) , and F (cid:0) (cid:104) q 1 , q 2 (cid:105) (cid:1) = F 1 ( q 1 ) · F 2 ( q 2 ) for all (cid:104) q 1 , q 2 (cid:105) , (cid:104) q (cid:48) 1 , q (cid:48) 2 (cid:105) ∈ Q 1 × Q 2 and σ ∈ Σ . As suggested by the notation , provided that the semiring ( S , + , · , 0 , 1 ) is commutative , the weighted string automaton A 1 · A 2 indeed generates the weighted language A 1 · A 2 . Similarly , regular weighted string languages are closed under alphabet change [ 9 ] . Let r : Σ → ∆ be a symbol relabeling , which we extend to strings r : Σ ∗ → ∆ ∗ by r ( σ 1 · · · σ k ) = r ( σ 1 ) · · · r ( σ k ) for all k ∈ N and σ 1 , . . . , σ k ∈ Σ . If f : Σ ∗ → S is ( e ﬀ ectively ) regular , then also the weighted string language g : ∆ ∗ → S given by g ( v ) = (cid:80) w ∈ Σ ∗ , r ( w ) = v f ( w ) is ( e ﬀ ectively ) regular . Given a weighted string automaton A = ( Q , Σ , I , wt , F ) generating f , we can construct a weighted string automaton B = ( Q , ∆ , I , wt (cid:48) , F ) generating g as follows : For every q , p ∈ Q and δ ∈ ∆ , we let wt (cid:48) ( q , δ , p ) = (cid:80) σ ∈ Σ , r ( σ ) = δ wt ( q , σ , p ) . 2 . 2 . Supervised training and tokenization Two of the most basic tasks that are performed early in many natural language processing applications [ 1 , 2 ] are tokenization and sentence boundary disambiguation . In general , the input to a natural language processing application is simply a character stream in some encoding ( e . g . , UTF - 8 or ASCII ) . During tokenization this character stream is separated into tokens , which are the basic units for further processing . Similarly , in applications that receive documents ( i . e . , character streams representing multiple sentences ) as input , the token ( or character ) stream often needs to be further subdivided to indicate sentence boundaries . The goal of sentence boundary disambiguation is the identiﬁcation of sentences in such streams . For many applications , tokens correspond roughly to words , but the task of sentence boundary disambiguation can be understood as tokenization , where the desired tokens are sentences . Due to this close connection , in the following we will focus on the general task of tokenization , which is similar to the task of lexical analysis [ 10 ] used in compilers or other text processors . While programming languages are designed such that token identiﬁcation is simple , natural language input can - not be strongly constrained ( e . g . , 50 years ago the sequence ‘ : - ) ’ was probably considered illegal in English ) and applications have to deal the ambiguities present in natural language . However , many natural languages o ﬀ er strong clues indicating tokens ( e . g . , whitespace to indicate word boundaries or full stops to indicate sentence boundaries ) , but some ambiguity typically remains . For example , in English a full stop is also used to denote ordinal numbers ( e . g . , ‘1 . ’ for ‘ﬁrst’ and ‘42 . ’ for ‘fourty - second’ ) or at the end of an abbreviation ( e . g . , ‘M . S . ’ for ‘Master of Science’ and ‘Hon . ’ for ‘Honorable’ ) . In Chinese , the sentence end marker 。 is unambiguous , but words can consist of several characters and word boundaries are not indicated by whitespace . For example , the sentence [ 11 ] 上 海 计 划到 本 世 纪 末 实 现 人 均 国 内 生 产 总 值 五 千 美 元 。 P inyin : 2 Shàng hˇai jì huà dào bˇen shì jì mò shí xiàn rén j¯un guó nèi sh¯eng chˇan zˇong zhí wˇu qi¯an mˇei yuán . [ Shanghai plans to reach the goal of 5 , 000 dollars in per capita GDP by the end of the century . ] should be word - segmented ( i . e . , tokenized into words ) according to the Penn Chinese tree bank standard [ 13 , 14 ] into 上 海 计 划 到 本 世 纪 末 实 现 人 均 国 内 生 产 总 值 五 千 美 元 。 The standard baseline for Chinese word - segmentation uses a pre - compiled ( ﬁnite ) lexicon of known Chinese words and matches ( beginning - to - end ) the longest possible lexicon entry ( or an individual character if there is no match ) . Given a tokenization D ( e . g . , the sequence of tokens output by a tokenizer ) and a reference tokenization D gold ( e . g . , the tokenization performed by a human ) , precision p , recall r , and the F 1 - score F 1 are deﬁned as follows : p = c | D | and r = c | D gold | and F 1 = 2 pr p + r , where c is the number of correctly recognized tokens in D . In other words , the F 1 - score is the harmonic mean between precision p and recall r . The simple longest - match strategy already achieves almost 90 % F 1 - score [ 11 ] . For example , 2 O ﬃ cial romanization system used in mainland China and Taiwan . Pinyin [ 12 ] essentially renders a Chinese character as the syllable ( or the sound ) that it represents . Unfortunately , this translation from character to syllable is not injective . 3 ε 日 章 文 鱼 文 鱼 章 (cid:3) / L ( 日文 ) (cid:3) / L ( 日 ) (cid:3) / L ( 文 章 ) (cid:3) / L ( 鱼 ) (cid:3) / L ( 章 鱼 ) (cid:104) 0 , 1 (cid:105) (cid:104) 4 , 1 (cid:105) 日 文 章 鱼 文 章 鱼 (cid:3) (cid:3) (cid:3) Figure 1 : Illustration of the lexicon representation and the interspersed input . States are ( mostly ) unnamed here and unless indicated otherwise transitions have weight 1 . it correctly tokenizes the sentence [ 15 ] : 日文 章 鱼 怎 么 说 ？ [ P inyin : Rì wén zh¯ang yú zˇen me shu¯o ? G loss : How to say octopus in Japanese ? ] as indicated , although the tokenization into lexicon entries is not unique in this example . The ( wrong ) tokenization 日 文 章 鱼 怎 么 说 ？ [ G loss : Japan ( article ) ﬁsh how say ? ] would also match lexicon entries . The main drawback of the static heuristic is that it consistently makes mistakes and simply cannot tokenize certain sentences according to standard . S proat et al . [ 15 ] suggest a weighted ﬁnite - state approach to Chinese word segmen - tation , in which they represent their scoring function ( or model ) as a weighted string automaton . 3 More precisely , they turn their probabilistic lexicon L : Σ + → [ 0 , 1 ] with ﬁnite support , where Σ + = Σ ∗ \ { ε } and the probabilities represent occurrence probabilities , into a weighted string automaton A = ( Q , Σ ∪ { (cid:3) } , I , wt , F ) over ( [ 0 , 1 ] , max , · , 0 , 1 ) such that • (cid:3) (cid:60) Σ is a new distinguished symbol indicating a token boundary , • Q = { ε } ∪ { w [ 1 , j ] | w ∈ supp ( L ) , 1 ≤ j ≤ | w | } contains a distinguished state ε and a state for each ( non - empty ) preﬁx of the strings represented by the lexicon L , • I ( ε ) = F ( ε ) = 1 and I ( q ) = F ( q ) = 0 for all q ∈ Q \ { ε } , and • the weights of the transitions are as follows : – wt (cid:0) w , σ , w σ (cid:1) = 1 for all σ ∈ Σ , w ∈ Q such that w σ ∈ Q , – wt (cid:0) w , (cid:3) , ε (cid:1) = L ( w ) for all w ∈ supp ( L ) , and – all remaining transitions have weight 0 . The structure of the automaton A is presented in Figure 1 . In other words , the automaton essentially recognizes a word w in the support of the lexicon L character - by - character and then recognizes a blank - symbol (cid:3) behind w with the occurrence weight L ( w ) . This last transition resets the automaton into the initial state , which allows it to recognize the next word . A wealth of literature exists on the e ﬃ cient representation of lexicons [ 16 ] that may also include morphological derivations of lemmas . This automaton A can score a given tokenization z ∈ ( Σ ∪ { (cid:3) } ) ∗ by simply computing A ( z ) = A ε , ε ( z ) . In other words , each candidate tokenization z can be scored by A using essentially the unigram language model score [ 1 ] for z ( i . e . , the product of the occurrence probabilities for the tokens in z ) . To determine the best tokenization ( i . e . , the one with the highest probability ) for the input w , we can now exploit the nice closure properties of the mappings generated by weighted string automata . More precisely , since we already have a model that can score tokenized text , it remains to construct a weighted string automaton representing the possible 3 They actually use a weighted ﬁnite - state transducer . For each input it outputs the separation into tokens , the phonetic realization of the characters in Pinyin [ 12 ] , and the part - of - speech tag of the recognized word . These outputs are necessary to deal with homographs ( i . e . , characters with several phonetic realisations potentially indicating di ﬀ erent meanings ) and similar phenomena . We drop the outputs since most of that information ( except for the separation naturally ) is irrelevant for our exposition . 4 tokenizations for the input string . To this end , the input string w ∈ Σ + is simply interspersed with (cid:3) - symbols . Let A w = ( Q w , Σ ∪ { (cid:3) } , I w , wt w , F w ) be the weighted string automaton such that • Q w = (cid:8) (cid:104) i , j (cid:105) | 0 ≤ i ≤ | w | , j ∈ { 1 , 2 } (cid:9) with initial weights I w (cid:0) (cid:104) 0 , 1 (cid:105) (cid:1) = 1 and I w ( q ) = 0 for all q ∈ Q w \ (cid:8) (cid:104) 0 , 1 (cid:105) (cid:9) , • ﬁnal weights F w (cid:0) (cid:104) | w | , 1 (cid:105) (cid:1) = 1 and F w ( q ) = 0 for all q ∈ Q \ (cid:8) (cid:104) | w | , 1 (cid:105) (cid:9) , and • the transition weights are as follows : – wt w (cid:0) (cid:104) i , j (cid:105) , w ( i + 1 ) , (cid:104) i + 1 , 1 (cid:105) (cid:1) = 1 for all 0 ≤ i < | w | and j ∈ { 1 , 2 } , – wt w (cid:0) (cid:104) i , 1 (cid:105) , (cid:3) , (cid:104) i , 2 (cid:105) (cid:1) = 1 for all 1 ≤ i < | w | , and – all remaining transitions have weight 0 . The structure of this automaton for the input w = 日文 章 鱼 is displayed in Figure 1 . The H adamard product A · A w then trivially represents a scorer for all possible tokenizations of w . In the example of Figure 1 only the tokeniza - tions 日 (cid:3) 文 章 (cid:3) 鱼 and 日文 (cid:3) 章 鱼 are permitted by the lexicon . Their scores are L ( 日 ) · L ( 文 章 ) · L ( 鱼 ) and L ( 日文 ) · L ( 章 鱼 ) , respectively , which coincide with their traditional unigram language model scores . The automa - ton A · A w , which represents the H adamard product , scores all tokenizations of w according to the lexicon model A and prunes out all other strings ( i . e . , other strings are scored with 0 ) . This automaton is always acyclic ( because the automaton A w is acyclic ) and typically unambigious . Since the automaton A w only uses the units { 0 , 1 } of the semi - ring , which always commute with all other elements , the commutativity of the semiring is not actually needed , so also non - commutative semirings can be used to model the lexicon . 4 To extract the best ( i . e . , highest - scoring ) tokenization from A · A w we can use D ijkstra ’s algorithm [ 17 ] , which runs in time O (cid:0) | A | · | w | + | Q | · | w | · ( log | Q | + log | w | ) (cid:1) . As demonstrated in [ 15 ] the framework of weighted string automata lends itself perfectly to the inclusion of additional knowledge sources such as models for morphological derivation or proper name detection . Later approaches for the Chinese word - segmentation problem ( e . g . , those derived from X ue ’s work [ 11 ] ) rein - terpret the problem as a character tagging problem assigning essentially word - start , word - internal , and word - ending tags ( or even more tags [ 18 ] ) to each character . We omit a detailed presentation of those approaches here , since we will cover the similar part - of - speeching tagging problem in the next section . The shared tasks organized by SIGHAN ( the special interest group for all aspects of Chinese language processing ) consistently demonstrate that these tagging models based on hidden M arkov models or conditional random ﬁelds outperform the longest - match baseline and the approach presented here . In addition , they demonstrate that the main remaining problem concerns out - of - vocabulary items ( i . e . , characters not seen during training ) and the recognition of named entities . 2 . 3 . Unsupervised training and part - of - speech tagging After the basic segmentation of the text is performed , and thus the common notions of ‘sentence’ and ‘word’ have been established , many applications require further processing of the token stream . For example , we might be interested in ﬁnding representative keywords for the token stream in order to provide an index into a text database [ 19 ] ( also called terminology mining or term extraction ) . Such keywords are usually taken to be nouns , which requires us to detect noun occurrences . The goal of sentiment analysis [ 20 ] ( or opinion mining ) is detecting whether a text overall speaks positively or negatively about its subject . In this context , we are typically interested in content words ( nouns , adjectives , verbs , negations , etc . ) and might not be interested in function words ( articles , pronouns , auxiliary verbs , etc . ) since they often just serve a grammatical function and carry little lexical meaning . Also in this setup we would need a process that identiﬁes occurrences of such word categories . Part - of - speech tagging [ 21 ] is the process of assigning a word category ( noun , adjective , verb , etc . ) to each word of a sentence . The word categories are also called part - of - speech . Again , the most straightforward solution to this problem uses a lexicon , in which the possible word categories of a word might be listed ( potentially with their frequencies ) . However , in many languages words can have several categories . For example , the English word ‘well’ has common uses in at least 5 word categories ( adverb , adjective , interjection , noun , and verb ) and the W ikipedia page for ‘like’ lists 10 categories ( noun , verb , adverb , adjective , preposition , particle , conjunction , hedge , ﬁller , and quotative ) . This ambiguity makes the task non - trivial because especially common words often have several associated categories . In fact we can form ( artiﬁcial ) sentences [ 22 ] containing the same word multiple times . Bu ﬀ alo bu ﬀ alo (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) Bison from Bu ﬀ alo Bu ﬀ alo bu ﬀ alo bu ﬀ alo (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) that are bullied by bison from Bu ﬀ alo bu ﬀ alo Bu ﬀ alo bu ﬀ alo . (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) themselves bully bison from Bu ﬀ alo . 4 Non - commutative semirings can be used to simulate weighted ﬁnite - state transducer behavior [ 15 ] . If we additionally want to output part - of - speech tags or the P inyin romanization , then the multiplicative operation will use some form of concatenation and will thus be non - commutative . 5 (cid:104) the , DT (cid:105) (cid:104) a , DT (cid:105) (cid:104) can , MD (cid:105) (cid:104) can , NN (cid:105) (cid:104) the , DT (cid:105) 0 0 0 . 1 0 . 9 (cid:104) a , DT (cid:105) 0 0 0 . 1 0 . 9 (cid:104) can , MD (cid:105) 0 . 5 0 . 5 0 0 (cid:104) can , NN (cid:105) 0 . 1 0 . 1 0 . 8 0 ( the , DT ) ( can , MD ) ( a , DT ) ( can , NN ) 0 0 . 1 0 0 . 9 0 . 5 0 0 . 5 0 0 0 . 1 0 0 . 9 0 . 1 0 . 8 0 . 1 0  0 . 3 0 . 4 0 . 2 0 . 1  (cid:104) the , DT (cid:105) (cid:104) a , DT (cid:105) (cid:104) can , MD (cid:105) (cid:104) can , NN (cid:105) Figure 2 : Transition matrix and initial state vector of a Markov model . In this case , the associated sequence of assigned word categories would be NNP ( proper noun ) , NN ( noun ) , NNP , NP , VBP ( verb , non - 3rd person singular present ) , VBP , NNP , NN , where we used the word category abbreviations ( or tags ) from the P enn tree bank [ 23 ] . Similarly , the sentence He (cid:124)(cid:123)(cid:122)(cid:125) PRP lived (cid:124)(cid:123)(cid:122)(cid:125) VBD a (cid:124)(cid:123)(cid:122)(cid:125) DT long (cid:124)(cid:123)(cid:122)(cid:125) JJ and (cid:124)(cid:123)(cid:122)(cid:125) CC fruitful (cid:124) (cid:32) (cid:123)(cid:122) (cid:32) (cid:125) JJ life (cid:124)(cid:123)(cid:122)(cid:125) NN demonstrates some other standard word categories . We represent such annotated sentences as sentences over the alphabet L × S , where L is the ﬁnite word inventory ( i . e . , lexicon ) and S is the ﬁnite set of word categories . Both sets L and S can , for example , be obtained from the P enn tree bank . In this representation , the annotated sentence above , would be (cid:104) He , PRP (cid:105) (cid:104) lived , VBD (cid:105) (cid:104) a , DT (cid:105) (cid:104) long , JJ (cid:105) (cid:104) and , CC (cid:105) (cid:104) fruitful , JJ (cid:105) (cid:104) life , NN (cid:105) . These sequences of word and assigned category pairs follow certain regularities [ 21 ] that can be modelled statisti - cally . Some combinations are extremely infrequent ( e . g . , adjective followed by a determiner as in “as tall a tree as” ) , whereas others are very frequent ( e . g . , determiner followed by an adjective as in “a tall tree” ) . The stochastic process , which generates the sequences , can be modelled as a ( homogeneous ) Markov chain , which was originally devised by M arkov [ 24 ] for sequences of letters in Russian texts . Note that we will again start in the supervised setting , where we follow the general strategy of building a model for the fully annotated data , which we will then modify to our needs using simple transformations . We ﬁrst formally model the likelihood of a particular sequence t = w 1 · · · w n with w i ∈ W = L × S for all 1 ≤ i ≤ n . Additionally , let X i be a random variable associated to position i . Using the general product rule , we obtain the probability of t as p (cid:0) t (cid:1) = p (cid:0) X 1 = w 1 , . . . , X n = w n (cid:1) = n (cid:89) i = 1 p (cid:0) X i = w i | X 1 = w 1 , . . . , X i − 1 = w i − 1 (cid:1) . Thus , we see that , in general , the probability of an element in the sequence depends on all previous positions . We could now assume that the variables X 1 , . . . , X n are statistically independent , but we have already seen that this is an unrealistic assumption . Instead M arkov assumed that the probability of an element only depends on the previous element . This assumption is the deﬁning feature of M arkov chains and demands that there exists an | W | × | W | - matrix M such that p (cid:0) X i = w i | X 1 = w 1 , . . . , X i = 1 = w i − 1 (cid:1) = p ( X i = w i | X i − 1 = w i − 1 (cid:1) = M w i − 1 , w i for all 2 ≤ i ≤ n . An example illustration of such a matrix is presented in Figure 2 . Under this assumption , the probability of t simpliﬁes to p ( t ) = (cid:126) v w 1 · (cid:81) ni = 2 M w i − 1 , w i for a stochastic vector (cid:126) v of length | W | with (cid:126) v w = p ( X 1 = w ) for every w ∈ W . A M arkov chain can easily be modelled as a weighted string automaton as follows : Given the 6 q 1 q 2 ε q 3 q 4 ( can , MD ) 0 . 1 ( can , NN ) 0 . 9 ( the , DT ) 0 . 5 ( a , DT ) 0 . 5 ( can , MD ) 0 . 1 ( can , NN ) 0 . 9 ( the , DT ) 0 . 1 ( can , MD ) 0 . 8 ( a , DT ) 0 . 1 ( the , DT ) 0 . 3 ( can , MD ) 0 . 2 ( a , DT ) 0 . 4 ( can , NN ) 0 . 1 Figure 3 : Illustration of the weighted string automaton corresponding to a M arkov chain . deﬁning parameters W , M , and (cid:126) v of the M arkov chain , we construct the automaton A = ( W ∪ { ε } , W , I , wt , F ) over ( Q ≥ 0 , + , · , 0 , 1 ) , where I ( ε ) = F ( ε ) = 1 , I ( w ) = 0 , and F ( w ) = 1 for all w ∈ W , and wt ( ε , w , w (cid:48)(cid:48) ) =  (cid:126) v w if w = w (cid:48)(cid:48) 0 otherwise and wt ( w (cid:48) , w , w (cid:48)(cid:48) ) =  M w , w (cid:48) if w = w (cid:48)(cid:48) 0 otherwise for all w , w (cid:48) ∈ W and w (cid:48)(cid:48) ∈ W ∪ { ε } . The general structure is illustrated in Figure 3 , which shows the weighted string automaton for the M arkov chain of Figure 2 . Note that the automaton A is deterministic . Clearly , A ( t ) = p ( t ) for all t ∈ W ∗ . We have arrived at a visible M arkov model for the annotated sentences . It is called ‘visible’ because the input string actually dictates the state behavior of the automaton . If the i - th letter of the input string is w ∈ W , then the automaton A needs to switch into state w during the i - th transition ( since the weight of all other transitions is 0 ) . Moreover , given annotated sentences as training data ( e . g . , the P enn tree bank ) , we can easily determine W and esti - mate the parameters (cid:126) v and M by maximum likelihood estimation [ 25 ] . Assuming that the training data ( t 1 , . . . , t k ) of sequences t 1 , . . . , t k ∈ W ∗ is representative for the general distribution , we have that (cid:126) v w ≈ c 1 ( w ) k and M w , w (cid:48) ≈ c ( ww (cid:48) ) c ( w (cid:48) ) − c 1 ( w (cid:48) ) for every w , w (cid:48) ∈ W , where c 1 ( w (cid:48)(cid:48) ) is the number of occurrences of w (cid:48)(cid:48) at the ﬁrst position and c ( t ) is the number of occurrences of the sequence t ∈ W ∗ ( at any position ) in the training examples t 1 , . . . , t k . An excellent and more detailed introduction into these models can be found in the book [ 1 ] . At this point , we managed to extract a weighted string automaton from fully annotated data . The automaton runs on ( and scores essentially the bigrams in ) sentences that are already annotated with word categories , but in the standard use - case outside training those categories are not available . To determine the best annotation for an unannotated string , we can again construct a weighted string automaton A u for the given input string u ∈ L ∗ that generates all possible word category annotations for u . Then the H adamard product A · A u scores all word category annotations and extracting the best derivation ( using , for example , the D ijkstra algorithm ) yields the best annotation . Alternatively , the V iterbi algorithm [ 26 ] essentially combines these two steps and can directly be employed . With these approaches we can determine that the most likely annotation for “can the” under the model of Figure 3 is (cid:104) can , MD (cid:105) (cid:104) the , DT (cid:105) ( probability : 0 . 2 · 0 . 5 ) , whereas (cid:104) can , NN (cid:105) (cid:104) the , DT (cid:105) is scored only 0 . 1 · 0 . 1 . Since we already demonstrated this approach , we omit a detailed exposition here , but note that the H adamard product can also be used to introduce additional knowledge into the model . For example , in English certain word endings ( e . g . , “ - ly” , “ - ing” , “ - ize” , and “ - al” ) are strongly indicative for some word categories ( adverbs , nouns , verbs , and adjectives , respectively ) . Another model that encodes such additional information can easily be combined with A with the help of the H adamard product . 7 To showcase the power of the ﬁnite - state approach , we switch to the unsupervised setting and demonstrate how we can tune ( or train ) our model to additional training material that is not annotated ( i . e . , tokenized English text ) . The annotated training data ( e . g . , P enn tree bank ) might not be representative for the texts that we want to use our model on . In such setups , we would like to adjust our model to accommodate the particularities of our evaluation texts . The special variant used here is typically called weight training , and we will illustrate it on the forward - backward ( or B aum - W elch ) algorithm [ 27 ] . The next goal is to ﬁne - tune our model A to a ﬁnite sequence ( u 1 , . . . , u n ) with u i ∈ L ∗ of unannotated training examples ( i . e . , tokenized English sentences ) . To this end , we ﬁrst note that our model A also assigns a probability to unannotated sentences . More precisely , the probability of a sentence u ∈ L ∗ is given by summing over all the possible annotations for u ; i . e . , p ( u ) = (cid:80) t ∈ W ∗ , π 1 ( t ) = u p ( t ) = (cid:80) t ∈ W ∗ , π 1 ( t ) = u A ( t ) , where π 1 : W → L is the projection on the ﬁrst component ( i . e . , π 1 ( (cid:104) (cid:96) , s (cid:105) ) = (cid:96) for all (cid:96) ∈ L and s ∈ S ) and π 1 : W ∗ → L ∗ is its homomorphic extension to strings . Fortunately , regular weighted string languages are closed under alphabet change , so we can easily compute a weighted string automaton B from A that computes the probability for each unannotated string . For the particular automaton of Figure 3 , the corresponding automaton after the projection looks exactly alike with the exception that all input labels only contain the ﬁrst component . Note that this automaton is typically no longer deterministic . With it , we can e ﬃ ciently compute the probability B ( can the ) of the unannotated string “can the” , which is 0 . 11 ( 0 . 1 = 0 . 2 · 0 . 5 along the states q 2 q 1 and 0 . 01 = 0 . 1 · 0 . 1 along the states q 4 q 1 ) , but the internal state changes are no longer observable , which justiﬁes the name ‘hidden M arkov model’ . The guiding principle of the forward - backward algorithm is the maximization of the training data ( u 1 , . . . , u n ) . Consequently , for each wt : Q × L × Q → Q ≥ 0 , let B wt = ( Q , L , I , wt , F ) be the weighted string automaton B with transition weights ‘wt’ . To keep the presentation simple , we assume that there is a unique initial state q 0 ( i . e . , there exists a unique q ∈ Q with I ( q ) (cid:44) 0 ) , a unique ﬁnal state q f ( i . e . , there exists a unique state q ∈ Q with F ( q ) (cid:44) 0 ) . Finally , we assume that the training data ( u 1 , . . . , u n ) is independently identically distributed . Given the last assumption , we want to determine (cid:98) wt = arg max wt : Q × L × Q → Q ≥ 0 (cid:18) n (cid:89) i = 1 B wt ( u i ) (cid:19) , ( 2 ) which is the transition weight assignment that maximizes the probability of the training data U ( maximum likelihood estimation ) . Unfortunately , no analytic method is known that solves this problem in general , but the forward - backward algorithm will improve a given transition weight assignment ‘wt’ such that the probability of the data does not de - crease . Iterating the algorithm then yields a ( classical hill - climbing ) approximation procedure for (cid:98) wt that might get stuck in local maxima . 5 We let B = B wt be the automaton with the current transition weight assignment . The al - gorithm consists of two steps since it is an instance of the famous expectation - maximization ( EM ) algorithm [ 32 ] : ( a ) an expectation step , in which the contribution of each transition is calculated , and ( b ) a maximization step . In the expectation step , we compute the “contribution” p τ of each transition τ = (cid:104) q , (cid:96) , q (cid:48) (cid:105) ∈ Q × L × Q in the product of ( 2 ) as p τ = n (cid:88) i = 1 (cid:18) (cid:88) u i = v (cid:96) w v , w ∈ L ∗ B ( v ) q 0 , q · wt ( q , (cid:96) , q (cid:48) ) · B ( w ) q (cid:48) , q f (cid:19) . The name ‘forward - backward algorithm’ is derived from the e ﬃ cient calculation of the probabilities B ( v ) q 0 , q [ forward step ] and B ( w ) q (cid:48) , q f [ backward step ] for the preﬁxes v and su ﬃ xes w of each sentence u i of the training data ( u 1 , . . . , u n ) . We note that B ( v ) q 0 , q and B ( w ) q (cid:48) , q f can be e ﬃ ciently computed using ( 1 ) by storing the intermediate results . Next , in the maximization step , we set the transition probabilities relative to their contribution . We let wt (cid:48) ( q , (cid:96) , q (cid:48) ) = p (cid:104) q , (cid:96) , q (cid:48) (cid:105) (cid:80) q (cid:48)(cid:48) ∈ Q , (cid:96) (cid:48) ∈ L p (cid:104) q , (cid:96) (cid:48) , q (cid:48)(cid:48) (cid:105) for all q , q (cid:48) ∈ Q and (cid:96) ∈ L . It was shown [ 33 ] that this procedure works in the sense that the probability of the training data improves ; i . e . , (cid:81) ni = 1 B wt ( u i ) ≤ (cid:81) ni = 1 B wt (cid:48) ( u i ) . Iterating the two steps , yields better and better transition weights 5 Due to this behavior , the optimization should be started on several initial weight assignments and techniques like simulated annealing [ 28 , 29 ] should be used to avoid getting stuck and potentially identify a global maximum . There is a wealth of literature [ 30 , 31 ] on the selection of initial weights as those can be chosen randomly , can be obtained by other methods ( such as our initial weights ) , or can be obtained based on knowledge of the search space . 8 converging towards a local maximum . Applied to our example model of Figure 3 and the training sentences “can the” and “the can” , we obtain the contributions p (cid:104) ε , can , q 2 (cid:105) = 0 . 2 · 0 . 5 and p (cid:104) ε , can , q 4 (cid:105) = 0 . 1 · 0 . 1 and p (cid:104) ε , the , q 1 (cid:105) = 0 . 3 · ( 0 . 1 + 0 . 9 ) . With these contributions , the new transition weights are wt (cid:48) ( ε , can , q 2 ) = 0 . 1 0 . 1 + 0 . 01 + 0 . 3 = 0 . 24 and wt (cid:48) ( ε , can , q 4 ) = 0 . 01 0 . 1 + 0 . 01 + 0 . 3 = 0 . 02 . Clearly , their weights increase because we have evidence in the training data for sentences starting with “can” . Sim - ilarly , the weight of the transition (cid:104) ε , the , q 1 (cid:105) will increase from 0 . 3 to 0 . 73 . To compensate , the transition weight for (cid:104) ε , a , q 3 (cid:105) is set to 0 since this transition was not useful in the small training sentences “can the” and “the can” . This “removal” of the transition (cid:104) ε , a , q 3 (cid:105) already indicates a potential generalization problem of the approach to maximize the probability of the training data , but we omit a detailed discussion . 6 Whereas the states in the modelling of the tokenization problem were mostly tools to represent a graph and were necessary to model long strings inside an automaton , the states in M arkov models have a much more pronounced role . They again serve a clear representation role in visible M arkov models , but in a hidden M arkov model the states and the transitions indeed encode the unknown behavior of a system ( or process ) . Optimization algorithms like the forward - backward algorithm assign transition weights and in this sense assign some “meaning” to the states . This use of states as a model of unknown behavior of an ill - understood process that generates the desired distribution is a major approach in many application areas ( e . g . , speech processing [ 34 ] ) , physics [ 35 ] , and biology [ 36 , 37 ] . Modern approaches to part - of - speech tagging also use reﬁnements of the presented approach such as conditional random ﬁelds [ 38 ] , which are strongly related to hidden M arkov models [ 39 ] . 3 . Context - free grammars and tree automata 3 . 1 . Basic notions From now on , we assume that ( S , + , · , 0 , 1 ) is a commutative semiring . A weighted context - free grammar [ 39 , 40 ] is a tuple G = ( N , T , S , P , wt ) , where N is a ﬁnite set of nonterminals , T is an alphabet of terminal symbols such that N ∩ T = ∅ , S ⊆ N is a set of initial nonterminals , P ⊆ N × ( N ∪ T ) ∗ is a ﬁnite set of productions , and wt : P → S . Each production ( n → w ) ∈ P induces a rewrite relation n → w = ⇒ ⊆ ( N ∪ T ) ∗ × ( N ∪ T ) ∗ , which is given by n → w = ⇒ = { ( unv , uwv ) | u ∈ T ∗ , v ∈ ( N ∪ T ) ∗ } . We let = ⇒ G = (cid:83) ρ ∈ P ρ = ⇒ , and to keep the presentation simple , we additionally assume that for every n ∈ N , the statement n = ⇒ + G w implies that ( i ) w (cid:44) ε ( no ε - productions ) and ( ii ) w (cid:44) n ( no cyclic chains ) . A ( ξ 0 , ξ k ) - derivation of G is a sequence ( ξ 0 , ρ 1 , ξ 1 , . . . , ρ k , ξ k ) of sentential forms ξ 0 , . . . , ξ k ∈ ( N ∪ T ) ∗ and productions ρ 1 , . . . , ρ k ∈ P such that ξ i − 1 ρ i = ⇒ ξ i for each 1 ≤ i ≤ k . The set of all ( ξ 0 , ξ k ) - derivations of G is denoted by Der G ( ξ 0 , ξ k ) . The weighted context - free grammar G generates the weighted language G : T ∗ → S , which is deﬁned for every w ∈ T ∗ by G ( w ) = (cid:88) n ∈ S (cid:18) (cid:88) ( n , ρ 1 , ξ 1 , . . . , ρ k , w ) ∈ Der G ( n , w ) wt ( ρ 1 ) · . . . · wt ( ρ k ) (cid:19) . Let Σ be an alphabet and X be a set such that X ∩ Σ = ∅ . The set of Σ - trees indexed by X , denoted by T Σ ( X ) , is the smallest set T such that X ⊆ T and σ ( t 1 , . . . , t k ) ∈ T for all k ∈ N , σ ∈ Σ , and t 1 , . . . , t k ∈ T . In - stead of T Σ ( ∅ ) we also write T Σ , and σ ( ) will also be written just σ for all σ ∈ Σ . Given a tree t ∈ T Σ ( X ) , its ( rank - extended ) positions pos ( t ) ⊆ N ∗ × N are inductively deﬁned by pos ( x ) = { ( ε , 0 ) } for every x ∈ X and pos (cid:0) σ ( t 1 , . . . , t k ) (cid:1) = { ( ε , k ) } ∪ { ( i . w , j ) | 1 ≤ i ≤ k , ( w , j ) ∈ pos ( t i ) } for all k ∈ N , σ ∈ Σ , and t 1 , . . . , t k ∈ T Σ ( X ) . 6 Adding the unannotated sentences from our annotated training data would mitigate the problem in our setup . 9 σ γ α σ x 1 x 2 ( ε , 2 ) ( 1 , 1 ) ( 1 . 1 , 0 ) ( 2 , 2 ) ( 2 . 1 , 0 ) ( 2 . 2 , 0 ) Figure 4 : Tree t [ left ] and the same tree , in which each label was replaced by the rank - extended position of the node . The subtree t | 2 is σ ( x 1 , x 2 ) . Each position ( w , k ) ∈ pos ( t ) in a tree t ∈ T Σ ( X ) has a label , denoted by t ( w ) , and induces a subtree , denoted by t | w . Formally , these notions can be deﬁned by x ( ε ) = x | ε = x for all x ∈ X and (cid:0) σ ( t 1 , . . . , t k ) (cid:1) ( w ) =  σ if w = ε t i ( v ) if w = i . v for 1 ≤ i ≤ k and v ∈ pos ( t i ) (cid:0) σ ( t 1 , . . . , t k ) (cid:1) | w =  σ ( t 1 , . . . , t k ) if w = ε t i | v if w = i . v for 1 ≤ i ≤ k and v ∈ pos ( t i ) for all k ∈ N , σ ∈ Σ , and t 1 , . . . , t k ∈ T Σ ( X ) . These notions are illustrated in Figure 4 . The weighted context - free grammar G = ( N , T , S , P , wt ) also generates derivation trees . More formally , the set of derivation trees generated by G is Der ( G ) = { t ∈ T N ( T ) | t ( ε ) ∈ S , ∀ ( w , k ) ∈ pos ( t ) : t ( w ) ∈ T or (cid:0) t ( w ) → t ( w . 1 ) · · · t ( w . k ) (cid:1) ∈ P } . In other words , each derivation tree t has a root label t ( ε ) that is from S , and at each position ( w , k ) ∈ pos ( t ) the label t ( w ) is either a terminal symbol from T or the labels t ( w ) → t ( w . 1 ) · · · t ( w . k ) form a production of P . The derivation tree d ∈ Der ( G ) is assigned the weight wt ( d ) = (cid:89) ( w , k ) ∈ pos ( t ) t ( w ) (cid:60) T wt (cid:0) t ( w ) → t ( w . 1 ) · · · t ( w . k ) (cid:1) ; i . e . , the product of the weights of the productions occuring in the derivation tree , where each production ( and its weight ) is counted as often as it occurs . A weighted tree automaton is a tuple A = ( Q , Σ , Q 0 , R , wt ) , where Q is the ﬁnite set of its states , Σ is its ( terminal ) alphabet , Q 0 ⊆ Q is its set of initial states , R is its ﬁnite set of rules , where each is of the form q → σ ( q 1 , . . . , q k ) for some k ∈ N , q , q 1 , . . . , q k ∈ Q , and σ ∈ Σ , and wt : R → S is a weight assignment . For such a weighted tree automaton A , we deﬁne a function h A : T Σ × Q → S recursively as follows : h A (cid:0) σ ( t 1 , . . . , t k ) , q (cid:1) = (cid:88) ( q → σ ( q 1 , . . . , q k ) ) ∈ R wt (cid:0) q → σ ( q 1 , . . . , q k ) (cid:1) · k (cid:89) i = 1 h A ( t i , q i ) for all q ∈ Q , k ∈ N , σ ∈ Σ , and t 1 , . . . , t k ∈ T Σ . A weighted tree language is simply a mapping ϕ : T ∆ → S for some alphabet ∆ . The weighted tree automaton A recognizes the weighted tree language A : T Σ → S such that A ( t ) = (cid:80) q ∈ Q 0 h A ( t , q ) for all t ∈ T Σ . The weighted tree language ϕ : T Σ → S is regular [ 41 ] if there exists a weighted tree automaton A such that ϕ = A . Let us provide a quick example using the commutative semiring ( Q , + , · , 0 , 1 ) of rational numbers . Consider the weighted tree automaton B = ( { q , (cid:62) } , Σ , { q } , R , wt ) such that Σ = { σ , α } , the set R of rules contains exactly the following rules q → α q → σ ( (cid:62) , (cid:62) ) q → σ ( q , (cid:62) ) q → σ ( (cid:62) , q ) (cid:62) → α (cid:62) → σ ( (cid:62) , (cid:62) ) , and wt ( ρ ) = 1 for all ρ ∈ R . Then h B ( t , q ) = | pos ( t ) | and h B ( t , (cid:62) ) = 1 for all t ∈ T Σ such that for every position ( w , k ) ∈ pos ( t ) we have either ( i ) t ( w ) = σ and k = 2 or ( ii ) t ( w ) = α and k = 0 . Let T ⊆ T Σ be the set of trees fulﬁlling the previous condition , so for all other trees t ∈ T Σ \ T we have h B ( t , q ) = h B ( t , (cid:62) ) = 0 . Consequently , 10 the automaton B recognizes the weighted tree language B that assigns the size to the trees only using binary ‘ σ ’ and nullary α and weight 0 to all other trees ; i . e . , for every t ∈ T Σ B ( t ) =  | pos ( t ) | if t ∈ T 0 otherwise . A detailed introduction into regular weighted tree languages can be found in the book chapter [ 42 ] . 3 . 2 . Supervised training and parsing A number of applications needs more detailed information about the structure ( syntax ) of the sentence . For example , in the analysis of the semantics [ 43 , 44 ] of a sentence the grammatical subject , verb , and its objects are of paramount importance . To detect and relate mentions [ 45 ] or to resolve pronouns , the noun phrases are typically identiﬁed with mentions . Finally , it was also demonstrated that syntax helps in certain machine translation tasks [ 46 ] . Let us start with an informal discussion . Given a tokenized sentence , which might already be part - of - speech tagged , the task of syntactical analysis ( also called ‘parsing’ ) is to determine the grammatical parts ( subject , verb with its objects , etc . ) of the sentence and their relations . The principal grammatical parts and their potential relations are typically more or less agreed upon by linguists . The syntactical analysis shall identify the parts and their relation for the given sentence according to a ﬁxed standard . Many di ﬀ erent theories for the syntax of natural languages ( e . g . , constituent syntax [ 47 ] and dependency syntax [ 48 ] ) exist , and we present an example analysis of the sentence “We must bear in mind the Community as a whole” as a constituent tree and a dependency tree in Figure 5 . In the following , we will focus on the classic constituent syntax in the form of non - crossing trees . The P enn tree bank contains this annotation for roughly 50 , 000 English sentences . Let us go into a little more detail on the constituent syntax of English . A sentence is typically built from a subject and a verbal complex and possibly modiﬁers . The verbal complex contains the verb and its objects . These grammatical parts are called constituents and the corresponding continuous substring is called a phrase . The main observation in constituent syntax is that constituents are hierarchically organized ( larger constituents are formed from smaller ones ) , so that they can be organized in a tree . For example , in Figure 5 the substring “the Community” forms a noun phrase ( NP ) and the substring “the Community as a whole” forms another noun phrase that consists of the noun phrase “the Community” and the prepositional phrase ( PP ) “as a whole” , which itself decomposes into the preposition ( IN ) “as” and the noun phrase “a whole” . The phrases and their constituent types were hand - annotated in the P enn tree bank , so we again start in the supervised setting with fully annotated examples and extract a model directly from that . Following the historical development , we start with the extraction of a weighted context - free grammar ( N , T , S , P , wt ) , whose derivation trees will model the syntactical analysis . Let ( t 1 , . . . , t n ) be the training data . First of all , the constituent types and part - of - speech tags will form the set N of nonterminals , of which those that occur ( in some tree ) at the root are made initial nonterminals S . Similarly , all the lexical items that occur in the training data will form the set T of terminals . 7 Consequently , we have t i ∈ T N ( T ) and t i ( ε ) ∈ S for all 1 ≤ i ≤ n . Since the trees in the training data are supposed to be derivation trees of the extracted grammar , also the used productions are evident and we set P = (cid:91) 1 ≤ i ≤ n (cid:8) t i ( w ) → t i ( w . 1 ) · · · t i ( w . k ) | ( w , k ) ∈ pos ( t i ) , k (cid:44) 0 (cid:9) . Finally , we need to set the production weights . We again chose the maximum likelihood approach and simply count in c ( ρ ) for each production ρ ∈ P , how often it occurs in the training data . More formally , for every production ( s → s 1 · · · s k ) ∈ P with s ∈ N and s 1 , . . . , s k ∈ N ∪ T , let c ( s → s 1 · · · s k ) = (cid:88) 1 ≤ i ≤ n (cid:12)(cid:12)(cid:12)(cid:12)(cid:8) ( w , k ) ∈ pos ( t i ) | t i ( w ) = s , ∀ 1 ≤ j ≤ k : t i ( w . j ) = s j (cid:9)(cid:12)(cid:12)(cid:12)(cid:12) . We can then normalize ( e . g . , by left - hand side ) those counts to obtain probabilities and our production weights ; namely , for every production ( s → s 1 · · · s k ) ∈ P , we let wt ( s → s 1 · · · s k ) = c ( s → s 1 · · · s k ) (cid:80) ( s → w ) ∈ P c ( s → w ) . 11 S NP PRP We VP MD must VP VB bear PP IN in NP NN mind NP NP DT the NN Community PP IN as NP DT a NN whole We must bear in mind the Community as a whole subj aux prep dobj pobj det prep pobj det Figure 5 : A constituency and dependency parse of an English sentence . Before we proceed , let us explain P arse E val [ 49 ] quickly . 8 Note that in a tree t ∈ T N ( T ) each nonterminal occurrence w heads a certain substring , namely the string that is obtained by concatenating from left - to - right the lexical elements of the subtree t | w . Moreover , constituents are the elements of N that are not part - of - speech tags . We indicated constituents in black in Figure 6 . P arse E val essentially computes labeled precision p and recall r of constituents between two trees : a parse tree t and a reference tree u . Roughly speaking , labeled precision p is the number of correct constituents in t ( i . e . , the number of occurrences of constituents in t that head the same phrase as in the reference tree u ) divided by the number of all constituent occurrences in t . Similarly , labeled recall r is the number of correct constituents in t divided by the number of all constituent occurrences in u . P arse E val is then the F 1 - score F 1 = 2 prp + r of p and r . This approach can trivially be extended to several trees forming a test set . To illustrate the notion , let us consider the two trees of Figure 6 , in which the constituents are black . Given the right tree as reference , we obtain labeled precision p = 0 . 9 = 90 % ( because there are 10 occurrences of constituents in the parse tree and only 9 are correct ; the NP heading “the Community as a whole” cannot be conﬁrmed in the reference ) and labeled recall r = 1 = 100 % ( because all 9 occurrences of constituents in the reference head the same substring in the parse tree ) . Overall , this yields 2 prp + r = 0 . 95 = 95 % F 1 - score . If we extract a weighted context - free grammar ( N , T , S , P , wt ) from the training data ( Sections 2 – 21 of the P enn tree bank ) as previously described , then with small modiﬁcations ( unknown word handling and binarization ) we al - ready achieve 65 – 70 % F 1 - score ( on Section 23 ) using P arse E val . 9 Clearly , 70 % F 1 - score does not compare favorably to scores of state - of - the - art parsers [ 54 ] , which exceed 90 % . The evaluation scores of the weighted context - free gram - mar approach presented above have been boosted by additional manual or automatic annotation of the training data . Many constituent types ( like ‘NP’ and ‘PP’ ) have been subdivided into more speciﬁc types ( like ‘NP - SBJ’ for the subject noun phrase or ‘PP - TMP’ for temporal prepositional phrases ) . This process is called subcategorization , and many subcategorizations have been proposed [ 55 ] . However , it is important to recall that in the evaluation ( i . e . , on the test set ) the subcategorization is not performed . In other words , the parsers are still evaluated on the “base” categories 7 We will not deal with unknown words in this survey . 8 The actual description of P arse E val is slightly more involved . For example , P arse E val actually eliminates punctuation and unary nodes ( i . e . , nodes with exactly one child ) , and the original description does not take the node label into account . Although P arse E val received a lot of criticism [ 50 , 51 ] over the years , it remains the dominant evaluation metric for statistical constituent parsers . 9 Using the B it P ar decoder [ 52 , 53 ] with the V iterbi - parse option we obtain 69 . 84 % F 1 - score . Note that B it P ar internally binarizes the grammar and adds a default unknown word handling . 12 S NP PRP We VP MD must VP VB bear PP IN in NP NN mind NP NP DT the NN Community PP IN as NP DT a NN whole S NP PRP We VP MD must VP VB bear PP IN in NP NN mind NP DT the NNP Community PP IN as NP DT a NN whole Figure 6 : Parse tree ( left ) and reference tree ( right ) . S PP - TMP IN For NP CD six NNS years , , NP - SBJ NNP T . NNP Marshall NNP Hahn NNP Jr . VP VBZ has VP VBN made NP JJ corporate NNS acquisitions PP - MNR IN in NP NP DT the NML NNP George NNP Bush NN mode : : ADJP JJ kind CC and JJ gentle . . Figure 7 : Actual parse tree from the P enn tree bank . ( constituent types ) . However , since the parser now operates with more ﬁne - grained nonterminals , the derivation trees also contain the subcategorized nonterminals . Thus , a relabeling is applied to the derivation tree , which removes the additional information before evaluation . Let t ∈ T Σ ( X ) be a tree . Since the lexical elements are typically expressed as index X and not relabeled , a deterministic relabeling is a mapping χ : Σ → ∆ . It is extended to trees χ : T Σ ( X ) → T ∆ ( X ) as follows : χ ( x ) = x for all x ∈ X and χ (cid:0) σ ( t 1 , . . . , t k ) (cid:1) = χ ( σ ) (cid:0) χ ( t 1 ) , . . . , χ ( t k ) (cid:1) for all k ∈ N , σ ∈ Σ , and t 1 , . . . , t k ∈ T Σ ( X ) . We can extend it once more to weighted tree languages χ : S T Σ ( X ) → S T ∆ ( X ) , where as usual A B de - notes the set of all mappings f : B → A , for every ϕ : T Σ ( X ) → S and u ∈ T ∆ ( X ) by (cid:0) χ ( ϕ ) (cid:1) ( u ) = (cid:80) t ∈ T Σ ( X ) , χ ( t ) = u ϕ ( t ) . Note that the sum in the previous sentence is always ﬁnite . In the parsing domain , the additional annotation is typ - ically separated by a hyphen ‘ - ’ or equals sign ‘ = ’ from the base category ( see Figure 7 ) . The standard scorer [ 56 ] implements the relabeling that removes all information behind those symbols and the hyphen or equals sign itself . It is well - known [ 42 ] that relabeling the weighted derivations of a weighted context - free grammar yields a regular weighted tree language , and moreover , each regular weighted tree language can be obtained in this fashion . More formally , let G = ( N , T , S , P , wt ) be a weighted context - free grammar and χ : N → N (cid:48) be a relabeling . The weighted derivation language is Der G : T N ( T ) → S given by Der G ( d ) = wt ( d ) for all d ∈ Der ( G ) . Then χ ( Der G ) : T N (cid:48) ( T ) → S is a regular weighted tree language . Vice versa , every regular weighted tree language ϕ : T Σ ( X ) → S can be presented as ϕ = χ (cid:48) ( Der G (cid:48) ) for some relabeling χ (cid:48) and weighted context - free grammar G (cid:48) . Consequently , irrespective of the chosen subcategorization , the model can actually be expressed as a weighted tree automaton . This even applies more generally to , for example , weighted tree substitution grammars and arbitrary subcategorizations [ 54 ] . Let us now model the relabeled derivations of a weighted context - free grammar with the help of a weighted tree automaton . We assume that all productions generating lexical items have a part - of - speech tag on the left - hand side . Moreover , we allow weighted tree automata rules of the form q → σ ( w ) for a state q , a part - of - speech tag σ , and lexical item w . This can easily be simulated by two “proper” rules q → σ ( q w ) and q w → w for some new state q w at 13 the expense of an additional state per lexical item . Let G = ( N , T , S , P , wt ) be a weighted context - free grammar and χ : N → N (cid:48) be a deterministic relabeling . We construct the weighted tree automaton A = ( N , N ∪ T , S , P (cid:48) , wt (cid:48) ) , where P (cid:48) = (cid:8) s → χ ( s ) ( s 1 , . . . , s k ) | ( s → s 1 · · · s k ) ∈ P , ∀ 1 ≤ i ≤ k : s i ∈ N ∪ T (cid:9) and wt (cid:0) s → χ ( s ) ( s 1 , . . . , s k ) (cid:1) = wt ( s → s 1 · · · s k ) for all ( s → s 1 · · · s k ) ∈ P with s i ∈ N ∪ T for all 1 ≤ i ≤ k . It is routine to check that A = χ ( Der G ) . So far , this supervised approach mirrors the approach used for part - of - speech taggers : The states encode the constituent types ( part - of - speech tags in part - of - speech tagging ) , which is the information that is generally not available outside the annotated training examples . The rules ( or productions ) and their weights encode how the states can be combined . Note that the parsing task also solves the part - of - speech tagging task since the part - of - speech tags are also derived , although they are excluded from the evaluation . Indeed the best part - of - speech taggers are actually parsers [ 57 ] , which comes at the expense of a more potent and computationally more expensive model ( weighted ﬁnite - state automaton vs . weighted context - free grammar ) . 3 . 3 . Unsupervised training and the Berkeley parser The introduction of fully automatic subcategorization [ 58 – 60 ] without external resources ( i . e . , unsupervised training ) marked a fundamental change in direction . Beforehand most of the proposed subcategorizations were either manually annotated or automatically annotated with the help of other resources such as head information . The fully automatic approaches were later reﬁned by P etrov [ 61 , 62 ] , whose approach we will present here . Essentially , we split each state into two almost identical copies and retrain the rule weights of the obtained weighted tree automaton using , for example , expectation maximization . To this end , we will use the original training material ( as opposed to the approach presented for part - of - speech tagging ) , in which the desired subcategorization is naturally not present . Let us present the approach on an example . Assume that the two trees of Figure 8 are given as training data . From them we can extract the weighted tree automaton A = ( N , N ∪ T , S , R , wt ) , where • N = { S , NP , VB , DT , NN } and T = { the , a , dragon , sleeps } and S = { S } , and • the following rules are in R with their weight ‘wt’ indicated above the arrow : S 1 → S (cid:0) NP , VB (cid:1) VB 1 → VB (cid:0) sleeps (cid:1) DT . 5 → DT (cid:0) the (cid:1) NP 1 → NP (cid:0) DT , NN (cid:1) NN 1 → NN (cid:0) dragon (cid:1) DT . 5 → DT (cid:0) a (cid:1) . Now we split every state into two copies . For the two rules DT . 5 → DT (cid:0) the (cid:1) and DT . 5 → DT (cid:0) a (cid:1) , we create the four rules DT - 1 . 51 → DT (cid:0) the (cid:1) DT - 1 . 49 → DT (cid:0) a (cid:1) DT - 2 . 49 → DT (cid:0) the (cid:1) DT - 2 . 51 → DT (cid:0) a (cid:1) , where the weights were chosen slightly di ﬀ erent to break the symmetry . 10 Mind that all states are split into two states at the same time . Now suppose that we arrived at the following weighted tree automaton A (cid:48) = ( N (cid:48) , N (cid:48) ∪ T , S , R (cid:48) , wt (cid:48) ) , where • N (cid:48) = { S - 1 , NP - 1 , NP - 2 , VB - 1 , DT - 1 , DT - 2 , NN - 1 } and T = { the , a , dragon , sleeps } , • S = { S - 1 } , and • the following rules with their weight indicated above the arrow : S - 1 . 25 → S (cid:0) NP - 1 , VB - 1 (cid:1) NP - 1 1 → NP (cid:0) DT - 1 , NN - 1 (cid:1) VB - 1 1 → VB (cid:0) sleeps (cid:1) DT - 1 . 9 → DT (cid:0) the (cid:1) S - 1 . 75 → S (cid:0) NP - 2 , VB - 1 (cid:1) NP - 2 1 → NP (cid:0) DT - 2 , NN - 1 (cid:1) NN - 1 1 → NN (cid:0) dragon (cid:1) DT - 2 . 2 → DT (cid:0) the (cid:1) DT - 2 . 8 → DT (cid:0) a (cid:1) . Using the provided training trees , we train the rule weights using the expectation maximization algorithm in much the same fashion as in Section 2 . 3 . First , we consider all derivations of the training trees with any additional annotation , 10 Setting the weights to equal values makes it di ﬃ cult ( if not impossible ) for the learning procedure to distinguish the states . 14 S NP DT the NN dragon VB sleeps S NP DT a NN dragon VB sleeps Weight : 0 . 25 · 0 . 9 S - 1 NP - 1 DT - 1 the NN - 1 dragon VB - 1 sleeps Weight : 0 . 75 · 0 . 2 S - 1 NP - 2 DT - 2 the NN - 1 dragon VB - 1 sleeps Weight : 0 . 75 · 0 . 8 S - 1 NP - 2 DT - 2 a NN - 1 dragon VB - 1 sleeps Figure 8 : Two simple training trees ( left ) and three derivations of them ( right ) . which we display in Figure 8 together with their weights . Consequently , we obtain the following “expected” counts for two rules : c (cid:0) S - 1 → S ( NP - 1 , VB - 1 ) (cid:1) = 0 . 25 · 0 . 9 = 0 . 225 and c (cid:0) S - 1 → S ( NP - 2 , VB - 1 ) (cid:1) = 0 . 75 · 0 . 2 + 0 . 75 · 0 . 8 = 0 . 75 In the maximization step , we again normalize the rule weight , so we obtain the new rule weights wt (cid:48)(cid:48) (cid:0) S - 1 → S ( NP - 1 , VB - 1 ) (cid:1) = 0 . 225 0 . 225 + 0 . 75 = 0 . 23 and wt (cid:48)(cid:48) (cid:0) S - 1 → S ( NP - 2 , VB - 1 ) (cid:1) = 0 . 75 0 . 225 + 0 . 75 = 0 . 77 for the mentioned rules . Intuitively , the second rule was more useful in the training data , so its weight was increased with the goal to increase the overall likelihood of the training data . At this point we can re - evaluate the probability of the training corpus , and if we do not observe signiﬁcant improvements , then we remerge certain splits . In fact , a simple and direct way to evaluate the approximate gain from an individual split has been obtained [ 61 ] , but we refer to the thesis [ 62 ] for the details . This completes one iteration , and we can now either stop the training process or continue with another iteration by splitting all remaining states again . It was demonstrated [ 61 , 62 ] that the automatic splits capture interesting linguistic phenomena . Moreover , the B erkeley parser , which is trained in the described manner , obtains an F 1 - score of 90 . 7 % on the standard data from the P enn tree bank . We note that the states here capture completely unobserved information ( i . e . , information that is not even present in the fully annotated training data ) . The result also demonstrates that even hand - crafted subcate - gorizations are outperformed by the automatic approach , so the automatically aquired ﬁnite - state information has a higher utility than the manual or human - designed subcategorization . A case in point is the subcategorization manually annotated in the P enn tree bank . As demonstrated [ 55 ] these annotations typically have negative utility for parsing the test set when used directly . This shows that the utility of subcategorizations is hard to predict , and given su ﬃ ciently large training resources it seems best to acquire them automatically [ 54 ] . 4 . Conclusion and outlook to machine translation We have demonstrated the usefulness of the ﬁnite - state approach in several natural language processing tasks . The ﬁnitely many states can often nicely and compactly encode the desired annotation ( as in the tokenization and part - of - speech tagging tasks ) or even completely unknown ﬁne - grained distinctions that go well beyond the annotation provided in the supervised setting ( as in the B erkeley parser scenario ) . The availability of toolkits [ 63 , 64 ] that im - plement ﬁnite - state models together with e ﬃ cient manipulation procedures allows us to focus on the modelling task , in which we derive a ﬁnite - state model that directly or partially encodes the desired solution . The ﬁnal representation of the solution can then be extracted from the built model using generic algorithms provided by the toolkits as demon - strated by the examples in this contribution . In addition , the ﬁnite - state approach is also ideally suited to compactly represent n - best lists or lattices of solutions , which allows to e ﬃ ciently provide access to several weighted solutions to further post - processing tasks . Finally , the availability of composition procedures [ 65 ] for ﬁnite - state models with output allows the modular development of a pipeline for the task at hand without the disadvantages usually encoun - tered when evaluating such pipelines . The latter approach was extremely successful in speech recognition and is used in most commercially available systems . Finally , let us provide a quick outlook on ﬁnite - state technology in statistical machine translation . The task of machine translation is to transfer an input sentence given in some natural language into another natural language with 15 I would like your advice about Rule 143 concerning inadmissibility Könnten Sie mir eine Auskunft zu Artikel 143 im Zusammenhang mit der Unzulässigkeit geben KOUS PPER PPER ART NN APPR NN CD AART NN APPR ART NN VV PP PP PP NP S Figure 9 : Source string and translation with annotated syntax tree , which are linked via the word alignment . the constraint that the same meaning should be conveyed by the sentences . Most current statistical machine translation systems [ 66 ] again use supervised training and thus annotated training data to derive the rules used in the derivation process in addition to maximum likelihood estimates as rule weights . 11 Most modern translation systems are trained on a word - aligned parallel corpus , which is a resource that contains sentence - by - sentence translations in the two languages in question together with a bipartite relation on the sentence positions , called the word alignment [ 67 ] . Roughly speaking , the word alignment relates the words of the sentences and ideally indicates the translation on the word level . A typical training example for a system that uses syntactic annotations on the target side is provided in Figure 9 . Synchronous grammars [ 68 ] are often used as models that perform the translation . The rules extracted from the training data typically consist of segments ( substrings in case of strings and subtrees in the case of trees ) of the annotated training example . The number of extracted rules is often extremely large and modern systems have several hundred million rules , so adding even more “hidden” information ( e . g . , by state splitting ) seems di ﬃ cult . Although ﬁnite - state information was successfully used in many di ﬀ erent areas of natural language processing , it remains open whether similar approaches can be made tractable for statistical machine translation . The ﬁnite - state information could either be automatically derived as in the B erkeley parser or imported from processes that perform annotations ( e . g . , the parser annotating the syntax tree or the aligner yielding the word alignment ) . Currently , ﬁnite - state constructions are used to translate weighted tree languages representing all or a selection of the likely parses of an input sentence instead of just the best syntax tree [ 69 ] or similarly extract rules from weighted tree languages annotated to an input sentence in the training data [ 70 ] . Whether automatically derived ﬁnite - state information can be used successfully also these highly complex systems with millions of surprisingly simple individual rules remains a key challenge , but recent successes [ 71 ] with deep neural networks seem to indicate that hidden ﬁnite - state information only waits to be discovered . References [ 1 ] C . Manning , H . Schütze , Foundations of Statistical Natural Language Processing , MIT Press , 1999 . 11 Modern systems use additional features besides the pure translation weights to encourage ﬂuent translations and o ﬀ set certain particularities of the translation model . 16 [ 2 ] D . Jurafsky , J . H . Martin , Speech and Language Processing , 2nd Edition , Prentice Hall , 2008 . [ 3 ] U . Hebisch , H . J . Weinert , Semirings — Algebraic Theory and Applications in Computer Science , World Scientiﬁc , 1998 . [ 4 ] J . S . Golan , Semirings and their Applications , Kluwer Academic , Dordrecht , 1999 . [ 5 ] S . Yu , Regular languages , in : G . Rozenberg , A . Salomaa ( Eds . ) , Handbook of Formal Languages , Vol . 1 , Springer , 1997 , Ch . II , pp . 41 – 110 . [ 6 ] M . P . Schützenberger , On the deﬁnition of a family of automata , Inform . and Control 4 ( 2 – 3 ) ( 1961 ) 245 – 270 . [ 7 ] M . Droste , W . Kuich , H . Vogler ( Eds . ) , Handbook of Weighted Automata , EATCS Monographs on Theoret . Comput . Sci . , Springer , 2009 . [ 8 ] K . Knight , J . May , Applications of weighted automata in natural language processing , in : Droste et al . [ 7 ] , Ch . XIV , pp . 571 – 596 . [ 9 ] J . Sakarovitch , Rational and recognisable power series , in : Droste et al . [ 7 ] , Ch . IV , pp . 105 – 174 . [ 10 ] N . Wirth , Compiler Construction , Addison Wesley , 1996 . [ 11 ] N . Xue , Chinese word segmentation as character tagging , Computational Linguistics and Chinese Language Processing 8 ( 1 ) ( 2003 ) 29 – 48 . [ 12 ] ISO Technical Committee 46 , Romanization of Chinese , Tech . Rep . ISO 7098 : 1991 , International Organization for Standardization ( 1991 ) . [ 13 ] F . Xia , The segmentation guidelines for the Penn Chinese treebank ( 3 . 0 ) , Tech . Rep . IRCS 00 - 06 , University of Pennsylvania ( 2000 ) . [ 14 ] N . Xue , F . Xia , F . - D . Chiou , M . Palmer , The Penn Chinese treebank : Phrase structure annotation of a large corpus , J . Natur . Lang . Engrg . 11 ( 2 ) ( 2002 ) 207 – 238 . [ 15 ] R . Sproat , C . Shih , W . Gale , N . Chang , A stochastic ﬁnite - state word - segmentation algorithm for Chinese , Comput . Linguist . 22 ( 3 ) ( 1996 ) 377 – 404 . [ 16 ] R . Sproat , Lexical analysis , in : R . Dale , H . Moisl , H . Somers ( Eds . ) , Handbook of Natural Language Processing , 2nd Edition , Marcel Dekker Inc . , 2000 , Ch . 3 , pp . 37 – 57 . [ 17 ] E . W . Dijkstra , A note on two problems in connexion with graphs , Numer . Math . 1 ( 1 ) ( 1959 ) 269 – 271 . [ 18 ] H . Zhao , C . Kit , Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recogni - tion , in : Proc . SIGHAN Workshop on Chinese Language Processing , Association for Computational Linguistics , 2008 , pp . 106 – 111 . [ 19 ] R . Navigli , P . Velardi , Learning domain ontologies from document warehouses and dedicated web sites , Comput . Linguist . 30 ( 2 ) ( 2004 ) 151 – 179 . [ 20 ] B . Pang , L . Lee , S . Vaithyanathan , Thumbs up ? sentiment classiﬁcation using machine learning techniques , in : Proc . EMNLP , Association for Computational Linguistics , 2002 , pp . 79 – 86 . [ 21 ] S . J . DeRose , Grammatical category disambiguation by statistical optimization , Comput . Linguist . 14 ( 1 ) ( 1988 ) 31 – 39 . [ 22 ] D . A . Borgmann , Beyond Language — Adventures in Word and Thought , Charles Scribner’s Sons , 1967 . [ 23 ] M . P . Marcus , B . Santorini , M . A . Marcinkiewicz , Building a large annotated corpus of English — the Penn treebank , Comput . Linguist . 19 ( 2 ) ( 1993 ) 313 – 330 . [ 24 ] A . A . Markov , An example of statistical investigation of the text of ‘Eugene Onegin’ concerning the connection of samples in chains , in : Proc . Royal Academy of Sciences , St . Petersburg , Vol . VI of Ser . 7 , 1913 , pp . 153 – 162 . [ 25 ] I . J . Myung , Tutorial on maximum likelihood estimation , J . Math . Psych . 47 ( 2003 ) 90 – 100 . [ 26 ] A . J . Viterbi , Error bounds for convolutional codes and an asymptotically optimum decoding algorithm , IEEE Trans . Inform . Theory 13 ( 2 ) ( 1967 ) 260 – 269 . [ 27 ] L . E . Baum , T . Petrie , Statistical inference for probabilistic functions of ﬁnite state markov chains , Ann . Math . Stat . 37 ( 6 ) ( 1966 ) 1554 – 1563 . [ 28 ] S . Kirkpatrick , C . D . Gelatt Jr . , M . P . Vecchi , Optimization by simulated annealing , Science 220 ( 4598 ) ( 1983 ) 671 – 680 . [ 29 ] V . ˇCerný , Thermodynamical approach to the traveling salesman problem : An e ﬃ cient simulation algorithm , J . Optim . Theory Appl . 45 ( 1 ) ( 1985 ) 41 – 51 . [ 30 ] S . Ravi , K . Knight , Minimized models for unsupervised part - of - speech tagging , in : Proc . ACL , Association for Computational Linguistics , 2009 , pp . 504 – 512 . [ 31 ] D . Garrette , J . Baldridge , Learning a part - of - speech tagger from two hours of annotation , in : Proc . NAACL , Association for Computational Linguistics , 2013 , pp . 138 – 147 . [ 32 ] A . P . Dempster , N . M . Laird , D . B . Rubin , Maximum likelihood from incomplete data via the EM algorithm , J . R . Stat . Soc . Ser . B Stat . Methodol . 39 ( 1 ) ( 1977 ) 1 – 38 . [ 33 ] L . E . Baum , T . Petrie , G . Soules , N . Weiss , A maximization technique occurring in the statictical analysis of probabilistic functions of Markov chains , Ann . Math . Stat . 41 ( 1 ) ( 1970 ) 164 – 171 . [ 34 ] J . Baker , The DRAGON system — an overview , IEEE Trans . Acoust . Speech Signal Process . 23 ( 1 ) ( 1975 ) 24 – 29 . [ 35 ] C . Nicolai , Solving ion channel kinetics with the QuB software , Biophys . Rev . Letters 8 ( 03n04 ) ( 2013 ) 191 – 211 . [ 36 ] M . J . Bishop , E . A . Thompson , Maximum likelihood alignment of DNA sequences , J . Mol . Biol . 190 ( 2 ) ( 1986 ) 159 – 165 . [ 37 ] J . Stigler , F . Ziegler , A . Gieseke , J . C . M . Gebhardt , M . Rief , The complex folding network of single calmodulin molecules , Science 334 ( 6055 ) ( 2011 ) 512 – 516 . [ 38 ] J . La ﬀ erty , A . McCallum , F . C . N . Pereira , Conditional random ﬁelds — probabilistic models for segmenting and labeling sequence data , in : Proc . ICML , 2001 , pp . 282 – 289 . [ 39 ] N . A . Smith , M . Johnson , Weighted and probabilistic context - free grammars are equally expressive , Comput . Linguist . 33 ( 4 ) ( 2007 ) 477 – 491 . [ 40 ] A . Salomaa , Probabilistic and weighted grammars , Inform . and Control 15 ( 6 ) ( 1969 ) 529 – 544 . [ 41 ] J . Berstel , C . Reutenauer , Recognizable formal power series on trees , Theoret . Comput . Sci . 18 ( 1982 ) 115 – 148 . [ 42 ] Z . Fülöp , H . Vogler , Weighted tree automata and tree transducers , in : Droste et al . [ 7 ] , Ch . 9 , pp . 313 – 403 . [ 43 ] Y . Wilks , E . Charniak , Computational Semantics — An Introduction to Artiﬁcial Intelligence and Natural Language Understanding , North - Holland , 1976 . [ 44 ] P . Blackburn , J . Bos , Representation and Inference for Natural Language — A First Course in Computational Semantics , CSLI publications , 2005 . [ 45 ] A . Haghighi , D . Klein , Coreference resolution in a modular , entity - centered model , in : Proc . NAACL , Association for Computational Lin - guistics , 2010 , pp . 385 – 393 . [ 46 ] O . Bojar , C . Buck , C . Federmann , B . Haddow , P . Koehn , J . Leveling , C . Monz , P . Pecina , M . Post , H . Saint - Amand , R . Soricut , L . Specia , A . Tamchyna , Findings of the 2014 workshop on statistical machine translation , in : Proc . WMT , Association for Computational Linguistics , 17 2014 , pp . 12 – 58 . [ 47 ] A . Carnie , Constituent Structure , Oxford University , 2010 . [ 48 ] L . Tesnière , Elements of structural syntax , John Benjamins , 2015 . [ 49 ] E . Black , S . Abney , D . Flickenger , C . Gdaniec , R . Grishman , P . Harrison , D . Hindle , R . Ingria , F . Jelinek , J . Klavans , M . Liberman , M . Marcus , S . Roukos , B . Santorini , T . Strzalkowski , A procedure for quantitatively comparing the syntactic coverage of English grammars , in : Proc . HLT , Morgan - Kaufmann , 1991 , pp . 306 – 311 . [ 50 ] J . Carroll , E . Briscoe , A . Sanﬁlippo , Parser evaluation : a survey and a new proposal , in : Proc . LREC , 1998 , pp . 447 – 454 . [ 51 ] G . Sampson , A . Babarczy , A test of the leaf - ancestor metric for parse accuracy , J . Natur . Lang . Engrg . 9 ( 4 ) ( 2003 ) 365 – 380 . [ 52 ] H . Schmid , E ﬃ cient parsing of highly ambiguous context - free grammars with bit vectors , in : Proc . CoLing , Association for Computational Linguistics , 2004 , pp . 162 – 168 . [ 53 ] H . Schmid , Trace prediction and recovery with unlexicalized pcfgs and slash features , in : Proc . ACL , Association for Computational Lin - guistics , 2006 , pp . 177 – 184 . [ 54 ] H . Shindo , Y . Miyao , A . Fujino , M . Nagata , Bayesian symbol - reﬁned tree substitution grammars for syntactic parsing , in : Proc . ACL , Association for Computational Linguistics , 2012 , pp . 440 – 448 . [ 55 ] D . Klein , C . D . Manning , Accurate unlexicalized parsing , in : Proc . ACL , Association for Computational Linguistics , 2003 , pp . 423 – 430 . [ 56 ] S . Sekine , Evalb — bracket scoring program , online at : http : / / nlp . cs . nyu . edu / evalb / ( 2013 ) . [ 57 ] B . Bohnet , J . Nivre , A transition - based system for joint part - of - speech tagging and labeled non - projective dependency parsing , in : Proc . EMNLP , Association for Computational Linguistics , 2012 , pp . 1455 – 1465 . [ 58 ] J . Henderson , Discriminative training of a neural network statistical parser , in : Proc . ACL , Association for Computational Linguistics , 2004 , pp . 95 – 102 . [ 59 ] T . Matsuzaki , Y . Miyao , J . Tsujii , Probabilistic CFG with latent annotations , in : Proc . ACL , Association for Computational Linguistics , 2005 , pp . 75 – 82 . [ 60 ] D . Prescher , Inducing head - driven PCFGs with latent heads — reﬁning a tree - bank grammar for parsing , in : Proc . ECML , Association for Computational Linguistics , 2005 , pp . 292 – 304 . [ 61 ] S . Petrov , L . Barrett , R . Thibaux , D . Klein , Learning accurate , compact , and interpretable tree annotation , in : Proc . ACL , Association for Computational Linguistics , 2006 , pp . 433 – 440 . [ 62 ] S . Petrov , Coarse - to - ﬁne natural language processing , Ph . D . thesis , University of California at Bekeley , Berkeley , CA , USA ( 2009 ) . [ 63 ] C . Allauzen , M . Riley , J . Schalkwyk , W . Skut , M . Mohri , Openfst : A general and e ﬃ cient weighted ﬁnite - state transducer library , in : Proc . CIAA , Vol . 4783 of LNCS , Springer , 2007 , pp . 11 – 23 . [ 64 ] J . May , K . Knight , Tiburon : A weighted tree automata toolkit , in : Proc . CIAA , Vol . 4094 of LNCS , Springer , 2006 , pp . 102 – 113 . [ 65 ] M . Mohri , Finite - state transducers in language and speech processing , Comput . Linguist . 23 ( 2 ) ( 1997 ) 269 – 311 . [ 66 ] P . Koehn , Statistical Machine Translation , Cambridge University Press , 2010 . [ 67 ] P . F . Brown , S . A . Della Pietra , V . J . Della Pietra , R . L . Mercer , The mathematics of statistical machine translation — parameter estimation , Comput . Linguist . 19 ( 2 ) ( 1993 ) 263 – 311 . [ 68 ] D . Chiang , An introduction to synchronous grammars , Tech . rep . , ISI , University of Southern California , part of a tutorial given with Kevin Knight at ACL ( 2006 ) . [ 69 ] H . Mi , L . Huang , Q . Liu , Forest - based translation , in : Proc . ACL , Association for Computational Linguistics , 2008 , pp . 192 – 199 . [ 70 ] H . Mi , L . Huang , Forest - based translation rule extraction , in : Proc . EMNLP , Association for Computational Linguistics , 2008 , pp . 206 – 214 . [ 71 ] K . Cho , B . van Merrienboer , C . Gulcehre , D . Bahdanau , F . Bougares , H . Schwenk , Y . Bengio , Learning phrase representations using RNN encoder - decoder for statistical machine translation , in : Proc . EMNLP , Association for Computational Linguistics , 2014 , pp . 1724 – 1734 . 18