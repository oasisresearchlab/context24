a r X i v : 1903 . 10676v3 [ c s . C L ] 10 S e p 2019 S CI B ERT : A Pretrained Language Model for Scientiﬁc Text Iz Beltagy Kyle Lo Arman Cohan Allen Institute for Artiﬁcial Intelligence , Seattle , WA , USA { beltagy , kylel , armanc } @ allenai . org Abstract Obtaining large - scale annotated data for NLP tasks in the scientiﬁc domain is challeng - ing and expensive . We release S CI B ERT , a pretrained language model based on B ERT ( Devlin et al . , 2019 ) to address the lack of high - quality , large - scale labeled scientiﬁc data . S CI B ERT leverages unsupervised pretraining on a large multi - domain corpus of scientiﬁc publications to improve perfor - mance on downstream scientiﬁc NLP tasks . We evaluate on a suite of tasks including sequence tagging , sentence classiﬁcation and dependency parsing , with datasets from a variety of scientiﬁc domains . We demon - strate statistically signiﬁcant improvements over B ERT and achieve new state - of - the - art results on several of these tasks . The code and pretrained models are available at https : / / github . com / allenai / scibert / . 1 Introduction The exponential increase in the volume of scien - tiﬁc publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these docu - ments . Recent progress in NLP has been driven by the adoption of deep neural models , but train - ing such models often requires large amounts of labeled data . In general domains , large - scale train - ing data is often possible to obtain through crowd - sourcing , but in scientiﬁc domains , annotated data is difﬁcult and expensive to collect due to the ex - pertise required for quality annotation . As shown through E LM o ( Peters et al . , 2018 ) , GPT ( Radford et al . , 2018 ) and B ERT ( Devlin et al . , 2019 ) , unsupervised pre - training of language models on large corpora signiﬁcantly improves performance on many NLP tasks . These models return contextualized embeddings for each token which can be passed into minimal task - speciﬁc neural architectures . Leveraging the success of unsupervised pretrain - ing has become especially important especially when task - speciﬁc annotations are difﬁcult to obtain , like in scientiﬁc NLP . Yet while both B ERT and E LM o have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia . In this work , we make the following contribu - tions : ( i ) We release S CI B ERT , a new resource demon - strated to improve performance on a range of NLP tasks in the scientiﬁc domain . S CI B ERT is a pre - trained language model based on B ERT but trained on a large corpus of scientiﬁc text . ( ii ) We perform extensive experimentation to investigate the performance of ﬁnetuning ver - sus task - speciﬁc architectures atop frozen embed - dings , and the effect of having an in - domain vo - cabulary . ( iii ) We evaluate S CI B ERT on a suite of tasks in the scientiﬁc domain , and achieve new state - of - the - art ( SOTA ) results on many of these tasks . 2 Methods Background The B ERT model architecture ( Devlin et al . , 2019 ) is based on a multilayer bidi - rectional Transformer ( Vaswani et al . , 2017 ) . In - stead of the traditional left - to - right language mod - eling objective , B ERT is trained on two tasks : pre - dicting randomly masked tokens and predicting whether two sentences follow each other . S CI B - ERT follows the same architecture as B ERT but is instead pretrained on scientiﬁc text . Vocabulary B ERT uses WordPiece ( Wu et al . , 2016 ) for unsupervised tokenization of the input text . The vocabulary is built such that it contains the most frequently used words or subword units . We refer to the original vocabulary released with B ERT as B ASE V OCAB . We construct S CI V OCAB , a new WordPiece vo - cabulary on our scientiﬁc corpus using the Sen - tencePiece 1 library . We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of B ASE V OCAB . The re - sulting token overlap between B ASE V OCAB and S CI V OCAB is 42 % , illustrating a substantial dif - ference in frequently used words between scien - tiﬁc and general domain texts . Corpus We train S CI B ERT on a random sample of 1 . 14M papers from Semantic Scholar ( Ammar et al . , 2018 ) . This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain . We use the full text of the papers , not just the abstracts . The average paper length is 154 sentences ( 2 , 769 tokens ) resulting in a corpus size of 3 . 17B tokens , similar to the 3 . 3B tokens on which B ERT was trained . We split sentences using ScispaCy ( Neumann et al . , 2019 ) , 2 which is optimized for scientiﬁc text . 3 Experimental Setup 3 . 1 Tasks We experiment on the following core NLP tasks : 1 . Named Entity Recognition ( NER ) 2 . PICO Extraction ( PICO ) 3 . Text Classiﬁcation ( CLS ) 4 . Relation Classiﬁcation ( REL ) 5 . Dependency Parsing ( DEP ) PICO , like NER , is a sequence labeling task where the model extracts spans describing the Partici - pants , Interventions , Comparisons , and Outcomes in a clinical trial paper ( Kim et al . , 2011 ) . REL is a special case of text classiﬁcation where the model predicts the type of relation expressed be - tween two entities , which are encapsulated in the sentence by inserted special tokens . 3 . 2 Datasets For brevity , we only describe the newer datasets here , and refer the reader to the references in Ta - ble 1 for the older datasets . EBM - NLP ( Nye et al . , 2018 ) annotates PICO spans in clinical trial ab - stracts . SciERC ( Luan et al . , 2018 ) annotates en - tities and relations from computer science ab - 1 https : / / github . com / google / sentencepiece 2 https : / / github . com / allenai / SciSpaCy stracts . ACL - ARC ( Jurgens et al . , 2018 ) and Sci - Cite ( Cohan et al . , 2019 ) assign intent labels ( e . g . Comparison , Extension , etc . ) to sentences from scientiﬁc papers that cite other papers . The Paper Field dataset is built from the Microsoft Academic Graph ( Sinha et al . , 2015 ) 3 and maps paper titles to one of 7 ﬁelds of study . Each ﬁeld of study ( i . e . geography , politics , economics , business , so - ciology , medicine , and psychology ) has approxi - mately 12K training examples . 3 . 3 Pretrained B ERT Variants B ERT - Base We use the pretrained weights for B ERT - Base ( Devlin et al . , 2019 ) released with the original B ERT code . 4 The vocabulary is B ASE - V OCAB . We evaluate both cased and uncased ver - sions of this model . S CI B ERT We use the original B ERT code to train S CI B ERT on our corpus with the same con - ﬁguration and size as B ERT - Base . We train 4 different versions of S CI B ERT : ( i ) cased or un - cased and ( ii ) B ASE V OCAB or S CI V OCAB . The two models that use B ASE V OCAB are ﬁnetuned from the corresponding B ERT - Base models . The other two models that use the new S CI V OCAB are trained from scratch . Pretraining B ERT for long sentences can be slow . Following the original B ERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing . We then continue training the model allowing sen - tence lengths up to 512 tokens . We use a single TPU v3 with 8 cores . Training the S CI V OCAB models from scratch on our corpus takes 1 week 5 ( 5 days with max length 128 , then 2 days with max length 512 ) . The B ASE V OCAB models take 2 fewer days of training because they aren’t trained from scratch . All pretrained B ERT models are converted to be compatible with PyTorch using the pytorch - transformers library . 6 All our models ( Sec - tions 3 . 4 and 3 . 5 ) are implemented in PyTorch us - ing AllenNLP ( Gardner et al . , 2017 ) . Casing We follow Devlin et al . ( 2019 ) in using the cased models for NER and the uncased models 3 https : / / academic . microsoft . com / 4 https : / / github . com / google - research / bert 5 B ERT ’s largest model was trained on 16 Cloud TPUs for 4 days . Expected 40 - 70 days ( Dettmers , 2019 ) on an 8 - GPU machine . 6 https : / / github . com / huggingface / pytorch - transformers for all other tasks . We also use the cased models for parsing . Some light experimentation showed that the uncased models perform slightly better ( even sometimes on NER ) than cased models . 3 . 4 Finetuning B ERT We mostly follow the same architecture , opti - mization , and hyperparameter choices used in Devlin et al . ( 2019 ) . For text classiﬁcation ( i . e . CLS and REL ) , we feed the ﬁnal B ERT vector for the [ CLS ] token into a linear classiﬁcation layer . For sequence labeling ( i . e . NER and PICO ) , we feed the ﬁnal B ERT vector for each token into a linear classiﬁcation layer with softmax output . We differ slightly in using an additional condi - tional random ﬁeld , which made evaluation eas - ier by guaranteeing well - formed entities . For DEP , we use the model from Dozat and Manning ( 2017 ) with dependency tag and arc embeddings of size 100 and biafﬁne matrix attention over B ERT vec - tors instead of stacked BiLSTMs . In all settings , we apply a dropout of 0 . 1 and optimize cross entropy loss using Adam ( Kingma and Ba , 2015 ) . We ﬁnetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e - 6 , 1e - 5 , 2e - 5 , or 5e - 5 with a slanted triangu - lar schedule ( Howard and Ruder , 2018 ) which is equivalent to the linear warmup followed by lin - ear decay ( Devlin et al . , 2019 ) . For each dataset and B ERT variant , we pick the best learning rate and number of epochs on the development set and report the corresponding test results . We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e - 5 . While task - dependent , op - timal hyperparameters for each task are often the same across B ERT variants . 3 . 5 Frozen B ERT Embeddings We also explore the usage of B ERT as pre - trained contextualized word embeddings , like ELMo ( Peters et al . , 2018 ) , by training simple task - speciﬁc models atop frozen B ERT embed - dings . For text classiﬁcation , we feed each sentence of B ERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hid - den size 200 ) on the concatenated ﬁrst and last BiLSTM vectors . For sequence labeling , we use the same BiLSTM layers and use a condi - tional random ﬁeld to guarantee well - formed pre - dictions . For DEP , we use the full model from Dozat and Manning ( 2017 ) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks . We did not ﬁnd changing the depth or size of the BiLSTMs to sig - niﬁcantly impact results ( Reimers and Gurevych , 2017 ) . We optimize cross entropy loss using Adam , but holding B ERT weights frozen and applying a dropout of 0 . 5 . We train with early stopping on the development set ( patience of 10 ) using a batch size of 32 and a learning rate of 0 . 001 . We did not perform extensive hyperparameter search , but while optimal hyperparameters are go - ing to be task - dependent , some light experimenta - tion showed these settings work fairly well across most tasks and B ERT variants . 4 Results Table 1 summarizes the experimental results . We observe that S CI B ERT outperforms B ERT - Base on scientiﬁc tasks ( + 2 . 11 F1 with ﬁnetuning and + 2 . 43 F1 without ) 8 . We also achieve new SOTA results on many of these tasks using S CI B ERT . 4 . 1 Biomedical Domain We observe that S CI B ERT outperforms B ERT - Base on biomedical tasks ( + 1 . 92 F1 with ﬁnetun - ing and + 3 . 59 F1 without ) . In addition , S CI B - ERT achieves new SOTA results on BC5CDR and ChemProt ( Lee et al . , 2019 ) , and EBM - NLP ( Nye et al . , 2018 ) . S CI B ERT performs slightly worse than SOTA on 3 datasets . The SOTA model for JNLPBA is a BiLSTM - CRF ensemble trained on multi - ple NER datasets not just JNLPBA ( Yoon et al . , 2018 ) . The SOTA model for NCBI - disease is B IO B ERT ( Lee et al . , 2019 ) , which is B ERT - Base ﬁnetuned on 18B tokens from biomedi - cal papers . The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from Dozat and Manning ( 2017 ) with part - of - speech ( POS ) features , which we do not use . In Table 2 , we compare S CI B ERT results with reported B IO B ERT results on the subset of datasets included in ( Lee et al . , 2019 ) . Interest - ing , S CI B ERT outperforms B IO B ERT results on 7 The SOTA paper did not report a single score . We compute the average of the reported results for each class weighted by number of examples in each class . 8 For rest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS . Field Task Dataset SOTA B ERT - Base S CI B ERT Frozen Finetune Frozen Finetune Bio NER BC5CDR ( Li et al . , 2016 ) 88 . 85 7 85 . 08 86 . 72 88 . 73 90 . 01 JNLPBA ( Collier and Kim , 2004 ) 78 . 58 74 . 05 76 . 09 75 . 77 77 . 28 NCBI - disease ( Dogan et al . , 2014 ) 89 . 36 84 . 06 86 . 88 86 . 39 88 . 57 PICO EBM - NLP ( Nye et al . , 2018 ) 66 . 30 61 . 44 71 . 53 68 . 30 72 . 28 DEP GENIA ( Kim et al . , 2003 ) - LAS 91 . 92 90 . 22 90 . 33 90 . 36 90 . 43 GENIA ( Kim et al . , 2003 ) - UAS 92 . 84 91 . 84 91 . 89 92 . 00 91 . 99 REL ChemProt ( Kringelum et al . , 2016 ) 76 . 68 68 . 21 79 . 14 75 . 03 83 . 64 CS NER SciERC ( Luan et al . , 2018 ) 64 . 20 63 . 58 65 . 24 65 . 77 67 . 57 REL SciERC ( Luan et al . , 2018 ) n / a 72 . 74 78 . 71 75 . 25 79 . 97 CLS ACL - ARC ( Jurgens et al . , 2018 ) 67 . 9 62 . 04 63 . 91 60 . 74 70 . 98 Multi CLS Paper Field n / a 63 . 64 65 . 37 64 . 38 65 . 71 SciCite ( Cohan et al . , 2019 ) 84 . 0 84 . 31 84 . 85 85 . 42 85 . 49 Average 73 . 58 77 . 16 76 . 01 79 . 27 Table 1 : Test performances of all B ERT variants on all tasks and datasets . Bold indicates the SOTA result ( multiple results bolded if difference within 95 % bootstrap conﬁdence interval ) . Keeping with past work , we report macro F1 scores for NER ( span - level ) , macro F1 scores for REL and CLS ( sentence - level ) , and macro F1 for PICO ( token - level ) , and micro F1 for ChemProt speciﬁcally . For DEP , we report labeled ( LAS ) and unlabeled ( UAS ) attachment scores ( excluding punctuation ) for the same model with hyperparameters tuned for LAS . All results are the average of multiple runs with different random seeds . Task Dataset B IO B ERT S CI B ERT NER BC5CDR 88 . 85 90 . 01 JNLPBA 77 . 59 77 . 28 NCBI - disease 89 . 36 88 . 57 REL ChemProt 76 . 68 83 . 64 Table 2 : Comparing S CI B ERT with the reported B IO B ERT results on biomedical datasets . BC5CDR and ChemProt , and performs similarly on JNLPBA despite being trained on a substan - tially smaller biomedical corpus . 4 . 2 Computer Science Domain We observe that S CI B ERT outperforms B ERT - Base on computer science tasks ( + 3 . 55 F1 with ﬁnetuning and + 1 . 13 F1 without ) . In addition , S CI B ERT achieves new SOTA results on ACL - ARC ( Cohan et al . , 2019 ) , and the NER part of SciERC ( Luan et al . , 2018 ) . For relations in Sci - ERC , our results are not comparable with those in Luan et al . ( 2018 ) because we are performing re - lation classiﬁcation given gold entities , while they perform joint entity and relation extraction . 4 . 3 Multiple Domains We observe that S CI B ERT outperforms B ERT - Base on the multidomain tasks ( + 0 . 49 F1 with ﬁnetuning and + 0 . 93 F1 without ) . In addi - tion , S CI B ERT outperforms the SOTA on Sci - Cite ( Cohan et al . , 2019 ) . No prior published SOTA results exist for the Paper Field dataset . 5 Discussion 5 . 1 Effect of Finetuning We observe improved results via B ERT ﬁnetuning rather than task - speciﬁc architectures atop frozen embeddings ( + 3 . 25 F1 with S CI B ERT and + 3 . 58 with B ERT - Base , on average ) . For each scientiﬁc domain , we observe the largest effects of ﬁnetun - ing on the computer science ( + 5 . 59 F1 with S CI B - ERT and + 3 . 17 F1 with B ERT - Base ) and biomed - ical tasks ( + 2 . 94 F1 with S CI B ERT and + 4 . 61 F1 with B ERT - Base ) , and the smallest effect on mul - tidomain tasks ( + 0 . 7 F1 with S CI B ERT and + 1 . 14 F1 with B ERT - Base ) . On every dataset except BC5CDR and SciCite , B ERT - Base with ﬁnetuning outperforms ( or performs similarly to ) a model us - ing frozen S CI B ERT embeddings . 5 . 2 Effect of S CI V OCAB We assess the importance of an in - domain sci - entiﬁc vocabulary by repeating the ﬁnetuning ex - periments for S CI B ERT with B ASE V OCAB . We ﬁnd the optimal hyperparameters for S CI B ERT - B ASE V OCAB often coincide with those of S CI B - ERT - S CI V OCAB . Averaged across datasets , we observe + 0 . 60 F1 when using S CI V OCAB . For each scientiﬁc do - main , we observe + 0 . 76 F1 for biomedical tasks , + 0 . 61 F1 for computer science tasks , and + 0 . 11 F1 for multidomain tasks . Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over B ERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , S CI B ERT beneﬁts most from the scientiﬁc corpus pretraining . 6 Related Work Recent work on domain adaptation of BERT in - cludes B IO B ERT ( Lee et al . , 2019 ) and C LIN - ICAL B ERT ( Alsentzer et al . , 2019 ; Huang et al . , 2019 ) . B IO B ERT is trained on PubMed ab - stracts and PMC full text articles , and C LIN - ICAL B ERT is trained on clinical text from the MIMIC - III database ( Johnson et al . , 2016 ) . In contrast , S CI B ERT is trained on the full text of 1 . 14M biomedical and computer science papers from the Semantic Scholar corpus ( Ammar et al . , 2018 ) . Furthermore , S CI B ERT uses an in - domain vocabulary ( S CI V OCAB ) while the other above - mentioned models use the original B ERT vocab - ulary ( B ASE V OCAB ) . 7 Conclusion and Future Work We released S CI B ERT , a pretrained language model for scientiﬁc text based on B ERT . We evalu - ated S CI B ERT on a suite of tasks and datasets from scientiﬁc domains . S CI B ERT signiﬁcantly outper - formed B ERT - Base and achieves new SOTA re - sults on several of these tasks , even compared to some reported B IO B ERT ( Lee et al . , 2019 ) results on biomedical tasks . For future work , we will release a version of S CI B ERT analogous to B ERT - Large , as well as ex - periment with different proportions of papers from each domain . Because these language models are costly to train , we aim to build a single resource that’s useful across multiple domains . Acknowledgment We thank the anonymous reviewers for their com - ments and suggestions . We also thank Waleed Ammar , Noah Smith , Yoav Goldberg , Daniel King , Doug Downey , and Dan Weld for their help - ful discussions and feedback . All experiments were performed on beaker . org and supported in part by credits from Google Cloud . References Emily Alsentzer , John R . Murphy , Willie Boag , Wei - Hung Weng , Di Jin , Tristan Naumann , and Matthew B . A . McDermott . 2019 . Publicly available clini - cal bert embeddings . In ClinicalNLP workshop at NAACL . Waleed Ammar , Dirk Groeneveld , Chandra Bhagavat - ula , Iz Beltagy , Miles Crawford , Doug Downey , Ja - son Dunkelberger , Ahmed Elgohary , Sergey Feld - man , Vu Ha , Rodney Kinney , Sebastian Kohlmeier , Kyle Lo , Tyler Murray , Hsu - Han Ooi , Matthew Pe - ters , Joanna Power , Sam Skjonsberg , Lucy Lu Wang , Chris Wilhelm , Zheng Yuan , Madeleine van Zuylen , and Oren Etzioni . 2018 . Construction of the litera - ture graph in semantic scholar . In NAACL . Arman Cohan , Waleed Ammar , Madeleine van Zuylen , and Field Cady . 2019 . Structural scaffolds for citation intent classiﬁcation in scientiﬁc publications . In NAACL - HLT , pages 3586 – 3596 , Minneapolis , Minnesota . Association for Computational Linguis - tics . Nigel Collier and Jin - Dong Kim . 2004 . Introduction to the bio - entity recognition task at jnlpba . In NLP - BA / BioNLP . Tim Dettmers . 2019 . TPUs vs GPUs for Transformers ( BERT ) . http : / / timdettmers . com / 2018 / 10 / 17 / tpus - vs - gpus - for - transformers - bert / . Accessed : 2019 - 02 - 22 . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In NAACL - HLT . Rezarta Islamaj Dogan , Robert Leaman , and Zhiyong Lu . 2014 . NCBI disease corpus : A resource for dis - ease name recognition and concept normalization . Journal of biomedical informatics , 47 : 1 – 10 . Timothy Dozat and Christopher D . Manning . 2017 . Deep biafﬁne attention for neural dependency pars - ing . ICLR . Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F . Liu , Matthew Peters , Michael Schmitz , and Luke S . Zettlemoyer . 2017 . Allennlp : A deep semantic natural language processing platform . In arXiv : 1803 . 07640 . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model ﬁne - tuning for text classiﬁcation . In ACL . Kexin Huang , Jaan Altosaar , and Rajesh Ranganath . 2019 . Clinicalbert : Modeling clinical notes and pre - dicting hospital readmission . arXiv : 1904 . 05342 . Alistair E . W . Johnson , Tom J . Pollard aand Lu Shen , Liwei H . Lehman , Mengling Feng , Moham - mad Ghassemi , Benjamin Moody , Peter Szolovits , Leo Anthony Celi , , and Roger G . Mark . 2016 . Mimic - iii , a freely accessible critical care database . In Scientiﬁc Data , 3 : 160035 . David Jurgens , Srijan Kumar , Raine Hoover , Daniel A . McFarland , and Daniel Jurafsky . 2018 . Measuring the evolution of a scientiﬁc ﬁeld through citation frames . TACL , 06 : 391 – 406 . Jin - Dong Kim , Tomoko Ohta , Yuka Tateisi , and Jun’ichi Tsujii . 2003 . GENIA corpus - a semanti - cally annotated corpus for bio - textmining . Bioinfor - matics , 19 : i180i182 . Su Kim , David Mart´ınez , Lawrence Cavedon , and Lars Yencken . 2011 . Automatic classiﬁcation of sen - tences to support evidence based medicine . In BMC Bioinformatics . Diederik P . Kingma and Jimmy Ba . 2015 . Adam : A method for stochastic optimization . ICLR . Jens Kringelum , Sonny Kim Kjærulff , Søren Brunak , Ole Lund , Tudor I . Oprea , and Olivier Taboureau . 2016 . ChemProt - 3 . 0 : a global chemical biology dis - eases mapping . In Database . Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho So , and Jaewoo Kang . 2019 . BioBERT : a pre - trained biomedical language representation model for biomedical text mining . In arXiv : 1901 . 08746 . Jiao Li , Yueping Sun , Robin J . Johnson , Daniela Sci - aky , Chih - Hsuan Wei , Robert Leaman , Allan Peter Davis , Carolyn J . Mattingly , Thomas C . Wiegers , and Zhiyong Lu . 2016 . BioCreative V CDR task corpus : a resource for chemical disease relation extraction . Database : the journal of biological databases and curation . Yi Luan , Luheng He , Mari Ostendorf , and Hannaneh Hajishirzi . 2018 . Multi - task identiﬁcation of enti - ties , relations , and coreference for scientiﬁc knowl - edge graph construction . In EMNLP . Mark Neumann , Daniel King , Iz Beltagy , and Waleed Ammar . 2019 . ScispaCy : Fast and robust mod - els for biomedical natural language processing . In arXiv : 1902 . 07669 . Dat Quoc Nguyen and Karin M . Verspoor . 2019 . From pos tagging to dependency parsing for biomedical event extraction . BMC Bioinformatics , 20 : 1 – 13 . Benjamin Nye , Junyi Jessy Li , Roma Patel , Yinfei Yang , Iain James Marshall , Ani Nenkova , and By - ron C . Wallace . 2018 . A corpus with multi - level an - notations of patients , interventions and outcomes to support language processing for medical literature . In ACL . Matthew E . Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke S . Zettlemoyer . 2018 . Deep contextualized word representations . In NAACL - HLT . Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 . Improving language under - standing by generative pre - training . Nils Reimers and Iryna Gurevych . 2017 . Optimal hy - perparameters for deep lstm - networks for sequence labeling tasks . In EMNLP . Arnab Sinha , Zhihong Shen , Yang Song , Hao Ma , Dar - rin Eide , Bo - June Paul Hsu , and Kuansan Wang . 2015 . An overview of microsoft academic service ( MAS ) and applications . In WWW . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . In NIPS . Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V . Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Jeff Klingner , Apurva Shah , Melvin Johnson , Xiaobing Liu , Lukasz Kaiser , Stephan Gouws , Yoshikiyo Kato , Taku Kudo , Hideto Kazawa , Keith Stevens , George Kurian , Nishant Patil , Wei Wang , Cliff Young , Jason Smith , Jason Riesa , Alex Rudnick , Oriol Vinyals , Gregory S . Corrado , Macduff Hughes , and Jeffrey Dean . 2016 . Google’s neural machine translation system : Bridging the gap between human and ma - chine translation . abs / 1609 . 08144 . Wonjin Yoon , Chan Ho So , Jinhyuk Lee , and Jaewoo Kang . 2018 . CollaboNet : collaboration of deep neu - ral networks for biomedical named entity recogni - tion . In DTMBio workshop at CIKM .