This article was downloaded by : [ The University of Manchester Library ] On : 26 November 2014 , At : 06 : 13 Publisher : Taylor & Francis Informa Ltd Registered in England and Wales Registered Number : 1072954 Registered office : Mortimer House , 37 - 41 Mortimer Street , London W1T 3JH , UK International Journal of Human - Computer Interaction Publication details , including instructions for authors and subscription information : http : / / www . tandfonline . com / loi / hihc20 A study of human performance in computer ‐ aided architectural design Donna L . Cuomo a & Joseph Sharif b a MITRE Corporation , Bedford , MA b State University of New York , Buffalo , Amherst , N . Y . Published online : 23 Sep 2009 . To cite this article : Donna L . Cuomo & Joseph Sharif ( 1989 ) A study of human performance in computer ‐ aided architectural design , International Journal of Human - Computer Interaction , 1 : 1 , 69 - 107 , DOI : 10 . 1080 / 10447318909525958 To link to this article : http : / / dx . doi . org / 10 . 1080 / 10447318909525958 PLEASE SCROLL DOWN FOR ARTICLE Taylor & Francis makes every effort to ensure the accuracy of all the information ( the “Content” ) contained in the publications on our platform . However , Taylor & Francis , our agents , and our licensors make no representations or warranties whatsoever as to the accuracy , completeness , or suitability for any purpose of the Content . Any opinions and views expressed in this publication are the opinions and views of the authors , and are not the views of or endorsed by Taylor & Francis . The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information . Taylor and Francis shall not be liable for any losses , actions , claims , proceedings , demands , costs , expenses , damages , and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with , in relation to or arising out of the use of the Content . This article may be used for research , teaching , and private study purposes . Any substantial or systematic reproduction , redistribution , reselling , loan , sub - licensing , systematic supply , or distribution in any form to anyone is expressly forbidden . Terms & Conditions of access and use can be found at http : / / www . tandfonline . com / page / terms - and - conditions A Study of Human Performance in Computer - Aided Architectural Design Donna L . Cuomo MITRE CorporationBedford , MA Joseph Sharif State University of New York at Buffalo , Amherst , N . Y . This paper describes the development and application of a cognitively - based performance methodology for assessing human performance on computer - aided architectural design ( CAAD ) tasks . Two CAAD tasks were employed that were hypothesized to be different in terms of the under - lying cognitive processes required for these tasks to be performed . Meth - ods of manipulating task complexity within each of these tasks were then developed . Six architectural graduate students were trained on a com - mercially available CAAD system . Each student performed the two ex - perimental design tasks at one of three levels of complexity . The data collected included protocols , video recordings of the computer screen , and an interactive script ( time - stamped record of every command input and the computers textual response ) . Performance measures and meth - ods of analysis were developed which reflected the cognitive processes used by the human during design ( including problem - solving techniques , planning times , heuristics employed , etc . ) and the role of the computer as a design aid . The analysis techniques used included graphical tech - niques , Markov process analysis , protocol analysis , and error classifi - cation and analysis . The results of the study indicated that some measures more directly reflected human design activity while others more directly reflected the efficiency of interaction between the computer and the human . The discussion of the results focuses primarily on the usefulness of the various measures comprising the performance methodology , the usefulness of the tasks employed including methods for manipulating task complexity , and the effectiveness of this system as well as CAAD systems in general for aiding human design processes . Acknowledgements : This research was supported in part by the Prime Computer Corporation and through a grant from the Diamond Research Fund of the State University of New York at Buffalo . The authors are grateful to Paul Beiter and Jim Kiser for their technical assistance with the hardware , and to Kevin M . Cuomo for his assistance with the data analysis . International Journal of Human - Computer Interaction 1 ( 1 ) 69 - 107 ( 1989 ) 69 D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 70 D . L Cuomo and J . Shan ' t INTRODUCTION Many areas of human - machine interaction have well - developed human perform - ance measures . These measures have often proved effective for evaluating the usefulness of various system - related attributes or features for task performance . Unfortunately , this is not the case for computer - aided design ( CAD ) . Consequently , it is not always obvious whether software / hardware developments in CAD systems are productive or counterproductive from the standpoint of the human ' s ability to incorporate these features into the decision - making processes that underlie design activities . To gain insight into the effects computer aids can have on human design activities , we need to better understand the cognitive processes at work during design and the role of external computer representations—graphic representations in partic - ular—in mediating such processes ( Goumain & Sharit , 1988 ) . In order to understand the human ' s design process , the factors which affect this process and the manner in which they affect it must be known . For example , one factor which influences the human—CAD interactive process is the type of design task to be performed . While it is intuitive that certain types of CAD systems would be better suited for certain types of design problems , there is currently no framework for matching the aiding capabilities of a CAD system to the needs of the designer for a particular type of task . The particular area of CAD that is the focus of this paper is architectural design . This field , often referred to as computer - aided architectural design ( CAAD ) , was selected for two reasons . First , while a large number of computer aids have been developed in many engineering fields , for example , concrete beam design , me - chanical part design , and drainage layout , architects have shown great reluctance to use the computer for design purposes . Second , although most CAAD systems are currently rich in low - level operations and limited in their high - level " automated " design capabilities ( Teicholtz , 1985 ) , it is believed that for a CAAD system to be effective in architectural design , the system must act as an intelligent knowledge - based design assistant ( Kalay , 1985 ) . However , we do not know how this CAAD feature affects human design capabilities , especially when it is made available within the context of other CAAD tools and for a wide variety of architectural tasks . The goals of the study described in this paper were to : ( 1 ) classify CAAD tasks in terms of the mental representations designers have of them and of the resulting underlying cognitive processes used during these tasks ; ( 2 ) derive performance measures , data summarization techniques , and mod - eling methods that reflect meaningful changes that occur during the human ' s performance of these tasks ; and ( 3 ) formulate measures of task complexity in order to determine how sensitive the human performance measures are to changes in task complexity across each of the tasks . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 71 These ideas are consistent with various long - term research goals advocated by a group of workshop participants ( Atwood , 1984 ) that concern the more general problem of human - computer interactive task performance . Meeting these goals would give us the potential to evaluate : ( 1 ) human performance on a given task across systems or system features and thereby allow us to determine the suitability of certain classifications of problems for certain types of CAAD systems ; and ( 2 ) human performance across different levels of complexity within a single task and the effectiveness of new CAAD techniques applied to these different levels of complexity . The performance measures must therefore be sensitive enough to reflect signif - icant differences in patterns of human performance both between and within tasks , and ultimately across computer systems as well . The section that follows provides the cognitive basis for the performance methodology that was developed to po - tentially serve this purpose . This section together with the subsequent sections on " Method " and " Analytic Techniques " provide the details of the overall methodology developed for investigating human performance of CAAD tasks . A COGNITIVE FRAMEWORK The performance methodology developed for this study was based on the idea that the mental or information - processing load on the designer during performance of the CAAD task stems from two components ( Sharit & Cuomo , 1988 ) : ( 1 ) the load due to the application task ( independent of the CAAD system ) ; and ( 2 ) the load due to the interactive techniques ( CAAD system ) . It was hypothesized that not only do each of these components affect the designer ' s mental load independently but that they also interact in a way such that the sum of the two loads is less than the total load imposed ; that is , there is an interaction between the components . Based on this idea it was believed that to effectively develop performance measures , the loads due to task and interactive technique must be controlled to a degree that allows the output effects ( i . e . , human performance ) to be predicted and tested . Since the mental workload imposed by the CAAD system is fixed when using a single CAAD system , the focus in this study was on manipulating the application task . The application task is predominantly responsible for the relative distribution of perceptual , decision - making , memory , and motor loads . Stimulus - Central Processing Component - Response Compatibility One way to manipulate the workload associated with the application task is to vary the compatibility between the application task and the interactive technique used to D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 72 D . L . Cuomo and J . Shan ' t perform the task . This can be accomplished by utilizing the concept of stimulus - central processing component - response ( S - C - R ) compatibility ( Wickens , 1980 ; Wick - ens , Sandry , & Vidulich , 1983 ; Wickens , 1984 ) . This concept represents an expansion of the S - R research of Paul Fitts in the 1950s and is based on a multiple resource theory of attention . According to this theory , humans are viewed as possessing several different processing resources with corresponding limited capacities which may be defined by three relatively independent dimensions . The concept of S - C - R compatibility suggests unique compatibility relationships between modalities of input ( auditory or visual ) , output ( manual or speech ) , and codes of central processing ( spatial or verbal ) . The benefits of the visual / manual modalities are best realized when associated with the spatial task , although it is perhaps more appropriate to view the verbal - spatial dichotomy as endpoints on a continuum rather than as mu - tually exclusive categories ( Wickens et al . , 1983 ) . The compatibility between the application task ( which defines the goals ) and the interactive techniques available for accomplishing the goals can potentially im - pact human information - processing capabilities . The concept of compatibility can therefore help explain potential differences in performance resulting from manip - ulations of task and system related factors . Although the inputs in a CAAD task occur through the visual modality , the processing codes utilized will be both verbal and spatial , with the complexity and nature of the application task governing the interplay between verbal and spatial codes prior to the execution of a response . By increasing the demands on the various processing resources , the effects of compatibility should become more pronounced ( Wickens , 1984 ) , and inferences can be made concerning the interplay and relative distribution of the verbal and spatial processing codes . With this knowledge , the link between inputs and outputs that reflect the human ' s underlying design process in performing CAAD tasks can be better understood . Problem Solving In the theory of human problem solving described by Simon and Newell ( 1971 ) , the basic characteristics of the human information processing system that shape its problem - solving efforts are the following : ( 1 ) a working memory ( WM ) that is limited in capacity ; ( 2 ) a large long - term memory ( LTM ) from which information can be accessed ; and ( 3 ) an essentially serial processing system . These properties are important as they impose strong constraints on the ways in which the system can seek solutions to problems in large problem spaces . The information processing system is adaptive ; its behavior is therefore determined by the demands of the task environment rather than by its own internal characteristics . Only when the environment stresses its capacities along some dimension ( e . g . over - load ) do we discover the capabilities and limits of the information processing system , allowing their parameters to be measured ( Simon & Newell , 1971 ) . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 73 Subjects faced with problem solving tasb represent the problem environment in internal memory as a space of possible situations to be searched in order to find a solution . Each node in a problem space may be thought of as a possible state of knowledge a problem solver may attain . A state of knowledge is simply what the problem solver knows about the problem at a particular point in time . The problem solver ' s search for a solution is an odyssey through the problem space until his or her current knowledge state includes the problem solution . Problem spaces are enormous . The human , as a serial processor , can visit only a modest number of knowledge states in the search for a problem solution . The power of heuristics resides in their capability for examining small , promising regions of the entire space and ignoring the rest . Thus , to understand the problem solving behavior of a human , we must turn to the structure of problem spaces and see how information that is imbedded in such spaces can be extracted by heuristic processes and used to guide search to a problem solution ( Simon & Newell , 1971 ) . Selection of the correct problem space may make information available which can reduce the search by orders of magnitude in comparison with what is required if a less sophisticated space is used . Although no one has really observed the process of generating a problem space , from their research on problem solving Simon and Newell ( 1 971 ) have derived the sources of information which can be used to help construct a problem space . These include the task instructions , previous experience of a past similar task , previous experience with a past analogous task , programs stored in LTM which generalize overa range of tasks , programs in LTM for combining task instructions with other information in memory to construct new problem spaces and problem solving programs , and information accumulated while solving a prob - lem which may suggest changing the problem space . Simon ( 1973 ) notes that creative design is a special type of problem solving which lies toward the ill - structured end of the problem continuum . Any problem solving process will appear ill - structured if the problem solver is a serial machine that has access to a very large LTM of potentially relevant information and / or access to a very large external memory that provides information about the actual real - world consequences of problem solving actions . The design process can be consid - ered to be an ill - structured problem if there is a large long term - memory of potentially relevant information . As the knowledge required to solve a problem grows smaller , the problem becomes more well - structured . Therefore , design problems can vary in their degree of structuredness ( ill vs . well ) , thereby affecting the human ' s problem - solving strategies and resultant cognitive load . Task Complexity Human performance can be assumed to vary with the complexity of the design task . For this variation to become manifest , more control of the information - processing aspects of the design process is needed . This control relies on relatively well - defined criteria for describing task complexity which would then allow methods of varying task complexity as a function of the application task to be employed . While the concept of task complexity may appear intuitive , it is one that is extremely difficult D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 74 D . L . Cuomo and J . Sharif to define . With respect to the CAAD task domain , task complexity is being proposed to be a function of three attributes : criteria , entities , and relationships . Examples of possible criteria are dimensions or tolerances , spatial position and orientation constraints ( e . g . in architectural design ) , manufacturing constraints ( cost , material , etc . ) , and time allowed for designing the product . Entities can also affect task complexity . From an information - theoretic standpoint , increasing the number of entities in a task or decreasing the similarity between entities ( at any level ) would serve to increase the entropy in accordance with the basic axiom of syntactic in - formation theory ( Corley & Allan , 1976 ) . The last attribute noted refers to the re - lationships between entities . These relationships imply that more entities need to be considered simultaneously . Changes made to one entity must propagate to related entities . These attributes of task complexity as operationally defined above were applied to this study as a means for varying the complexity of the architectural design tasks that subjects performed . Performance Measures To analyze human performance on CAAD tasks , a relevant subset of the numerous measures of human performance generic to the area of human—computer inter - action can be used to compliment measures that are unique to human performance in the CAAD task domain . Examples of measures of the former type are the number and type of errors occurring during task performance . The basis for the occurrence of errors during the performance of CAAD tasks is discussed below . The human ' s interface to the system is represented by an input language structure that includes conceptual , semantic , syntactic , and lexical levels of input ( Foley , Wal - lace , & Chan , 1984 ) . The conceptual level refers to the user ' s mental model of how the system operates . A conceptual level error would represent the most serious type of error in the sense that it would likely result in an inability to pursue or significantly progress on any design problem . The semantic level corresponds to the actual functionality of the CAAD commands available on the computer system . These com - mands reflect the human ' s purpose of interaction , and can potentially demonstrate actions directed at reducing cognitive loads . An error at this level would result from the belief that a command performs a function that is entirely different from the one actually assigned to it . For example , a user may wish to temporarily enlarge a portion of the screen and believe the command " Magnify " performs this function when in fact the command " Zoom " is required . An error at the syntactical level refers to the units or words conveying the semantics of the language as being either ordered or utilized improperly to define a parameter . Interactive techniques become a factor at the lexical level where the actual ( physical ) interaction between the human and the CAAD system occurs . Errors that occur at this level are therefore very much a function of the attributes associated with these techniques . Although errors at different levels presumably reflect the processing level at which the user encounters difficulty , errors from all levels are detected at the lexical level . Human performance measures should also reflect cognitive loads , or equiva - D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 75 lently , limitations in information - processing resources . The information processing model proposed by Wickens ( 1984 ) provides a useful framework for examining potential limitations in human performance by drawing attention to the relationships between the human ' s processing resources . This approach requires that consideration be given to how capacity limitations in the various processing resources might affect human performance for the particular CAAD application task . Also , an understanding of the various components of the information - processing model is needed in order to anticipate the task or system related factors which could lead to performance difficulties that are cognitively based . The relationship between the components of the information processing model and human performance measures in the CAAD task domain are briefly discussed below . Perception . Various actions exist that the human would likely resort to under conditions of high perceptual load in order to meet the objectives of a CAAD task . These actions can serve as indicators of perceptual difficulty , and through their analysis measures of performance sensitive to perceptual load in the CAAD task domain can be developed . Some actions taken by the user that suggest attempts to reduce perceptual load are zooming , redrawing the screen , turning off layers , and panning . Memory . The knowledge necessary for initiating a CAAD task must be stored in the human ' s long - term memory ( LTM ) . A lack of knowledge in LTM should be revealed when information is required for processes being operated on in working memory ( WM ) . The limitations of WM will result in complex tasks being broken up into subtasks and / or tasks with many elements being " chunked . " Working on subsets of the prob - lem or chunking elements may then be indicative of high memory load . Similar inferences could be made through observing the human ' s use of the " Query " com - mand to obtain information on an entity . Decision Making . In a CAAD task , the human must first encode the problem prior to formulating a solution . If the problem is extremely well - defined , the overall decision - making load imposed by the task will likely be lower than that for a more ill - defined problem . In the latter situation , interpretation and solution of the problem will necessarily require many more hypotheses to be entertained and more inferences to be drawn . It is at the problem definition and solution formulation stage that the human may attempt to reduce the decision / memory load by decomposing the task into subtasks . The use of heuristics , an adaptive mechanism for coping with a complex , dynamic environment , and the generation of problem constraints by the designer may be viewed as an adaptive mechanism for coping with a complex decision - making environment . The use of heuristics can , however , lead to severe and system - atic errors or biases ( Tversky & Kahneman , 1974 ) . Norman ( 1983 ) has classified human errors that are likely to occur during human - computer interaction . The subset known as slips are sufficiently related to decision processes to qualify as errors in decision making and therefore as reflecting high decision or decision / memory loads . According to Norman , these errors can yield insight into the cognitive mechanisms involved and therefore can guide the examination of the human - machine interface . Motor Processes . The motor process is initiated once a stimulus has been re - D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 76 D . L Cuomo and J . Sharit ceived , recognized , and a decision to act upon it made . The response may involve reaching for a mouse , moving it to a tablet and picking a menu command , or moving the cursor to a particular location on the screen . In any case , it is a function of the particular decision and the interactive techniques available . Generally , motor proc - esses consider not only the input devices but also the total user interface including menu design . The use of CAAD commands that effectively decrease the positioning accuracy or reduce the number of inputs in the design process could reflect attempts at reducing the motor load . Planning A plan can be thought of as a set of directions used for guiding problem solving . More efficient problem solving is likely to result from effort invested at the planning stage . Humans have often been observed to be working in terms more abstract than those which characterize the initial problem space , a process described as abstracting from the concrete in order to construct a plan in a simpler abstract space ( Simon & Newell , 1971 ) . The effectiveness of this mapping between concrete and abstract objects derives from the human ' s ability to avoid treating the latter as a separate problem space . Verbal protocol techniques may be needed in order to identify the construction of abstract planning spaces . This data , even if in generalized form , can help provide information concerning how the human is generating and modifying problem spaces . CAAD Functions Most CAAD systems have the CAAD functional groupings ( Knox , 1 984 ) listed and defined in Table 1 . In attempting to analyze data collected during an interactive CAAD session , it could prove useful to group the commands into logical , functional higher - level tasks based on these CAAD functions . Added to this list are some func - tions specific to the Prime Medusa CAAD system . METHOD Task Development The first problem approached was that of establishing a general taxonomy of CAAD tasks . There are several ways to classify problems , such as by type of building , function of building , etc . However , as discussed earlier , a cognitively - based classification was desired since the focus was to be on the information - processing functions used by the designer in a CAAD task . The methods used for classifying tasks were in terms of their stimulus - central processing component - response ( S - C - R ) compatibility ( Wick - ens et al . , 1983 ) and the structuredness ( or definedness ) of the problem . Based on the classification ideas alluded to above , two CAAD tasks were de - veloped . This approach potentially allowed for more diverse patterns of design behavior to emerge than if just one task were used , and consequently offered a better opportunity for evaluating the computer ' s capabilities for aiding architectural design . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 77 Table 1 . CAAD Functions Function Definition Control functions Geometric definition Geometric manipulation Drafting functions Analysis functions Data management functions Programming language Supersyntax geometric definition This includes commands such as zooming , changing views , generating auxiliary views , and layering . These are commands which perform the drawing of points , lines , circles and arcs , and the generation of complex curves and shapes using extended geometry . These functions offer the ability to blank out or alter parts of drawings and change line types . It also includes the ability to translate , rotate , scale , and duplicate a design entity , and to group two design entities . These functions keep track of the scale and dimension of each part . It also includes automatic scribe lines and arrowheads , and the ability to store special patterns ( such as threads , nuts , or bolts ) . These allow various properties of the design to be computed such as perimeters , centers of gravity , moments of inertia , volumes , and weights . These functions control the storage , classification , and retrieval of drawings . Programming languages are for programming drawings ( macroability ) . Macros which draw a specified object based on parameters and / or locations input by the user . They include doors , double doors , windows , window wall , expand wall , and symbols . Application Tasks The two architectural application tasks that were developed were a master plan or site design task ( layout new buildings in an existing industrial park ) and a single building design task ( a branch bank ) with relationships between departments . There were no relationships between the buildings in the first task . The second task , a single building design , focused on the internal layout of a single building . Here , detail was importatnt and the function of the building was of great importance ; it strongly influenced the internal layout and determined the needs of the building . Exact placement and dimensions of internal components were im - portant in this task , whereas the elements of importance in the first task ( surroundings , site , traffic patterns ) were only important initially , primarily for deciding on the ori - entation of the building . A summarization of task differences and the hypothesized effects on human performance is given in Table 2 . Increasing Task Complexity Within a Task Varying the complexity of these two tasks was accomplished by manipulating the components of task complexity discussed earlier , namely , criteria , entities , and re - lationships . Three levels of complexity within each task were chosen based on the belief that this was the maximum numberforwhich sensitive measures of performance could be obtained within the period of time allocated for the tasks ( 3—5 hours ) . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 78 D . L . Cuomo and J . Shan ' t Table 2 . Summary of Differences Between the Site Layout ( Task 1 ) and the Bank ( Task 2 ) Design Tasks Task Differences Hypothesized Effects on Human Performance Task Ts program specifications are By lacking in qualitative information , Task 1 needs quantitative . criteria to be derived to base decisions on . Task 2 ' s program specifications are Task 2 is represented verbally in WM . Area square qualitative . footages must be derived . Verbal specifications need to be transformed to spatial positions . Task 1 is ill - defined . Problem spaces need to be constructed to work within . A high decision - making load may result , leading to the use of heuristics . Task 2 is well - defined . There will be more evaluation against program specifications . The subjects are searching for a correct solution . Task 1 elements are not related . This may result in independence in planning and editing . This would induce a lower memory load . Task 2 elements have relationships This will require planning ahead , inducing a high among elements at complexity levels 2 memory load , and 3 . Also , by having three levels , complexity can be varied in at least two different ways for each task . Summary of Task One . The site layout task ( refered to as task 1 ) consisted of laying out a 47 , 000 square foot micro - device factory and a 10 , 000 square foot office manufacturing building , along with their respective parking lots , on a rectan - gular site . The second complexity level was reached by increasing the number of entities in the scene , through the addition of a 20 , 000 square foot office building and warehouse , while the other components of complexity were kept constant . The site was increased in size and L - shaped . In the third level of complexity , the number of entities was kept constant from level 2 , however a spatial criteria—an oddly shaped site—was included to make the problem harder to solve . The relationship component of task complexity was kept constant throughout task one . Summary of Task Two . The base task of the branch bank consisted of a list of elements the bank should possess . This included the number of teller stations , a drive - in window , a vault area , officer areas , etc . The first increase in complexity in the bank task ( refered to as task two ) relative to the base task description was accomplished through the addition of nine relationships between the departments . An example of a relationship was that during slow periods , tellers would be required to work in the bookkeeping area . The task complexity attribute manipulated for the third level of complexity was criteria , with the relationship component being identical to level 2 of the task . A sample criterion was that the teller waiting line should accommodate a maximum of 20 people comfortably . The entities component of task complexity was held constant throughout . The evaluation of the effects of task complexity was based on the performances observed between subjects within each task . The evaluation of the effects deriving from differences between the two tasb was based on the performance of each of the subjects within each level of complexity . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 79 Task Validation Architectural programs represent a formal compendium of the essential information architects require in order to perform an architectural design task . For this study , the architectural programs were developed for both tasks 1 and 2 . These programs were based , in part , on information taken from real architectural programs . The site layouts for task 1 , for example , were site plans from an existing industrial park . The programs for both tasks were reviewed by two architects before the experiment . Both architects agreed the tasks were representative of architectural tasb and were reasonable . The estimated times for task completion were adjusted upward as a result of this review and a few other minor changes were also made . Subjects Six subjects were used for this study . Although this number appears small , it is actually consistent with research studies on architectural design processes , as noted by Akin ( 1978 ) . The limited availability of qualified subjects , the imposition this study placed on their schedules , and the nature of the analysis , particularly the decoding and synchronization of the verbal protocol and interactive script ( discussed below ) data , made it that much more difficult to employ a large sample size . Five of the six subjects were fourth and fifth year architectural students from the State University of New York at Buffalo ' s School of Architecture . Of these five , one was female and four were male . Three of the subjects had previous CAAD experience but not with the computer system being used in this experiment . The sixth subject was a recent graduate of the same school ( he had two years of work experience ) . Each subject received up to 24 hours of personal CAAD training and practice on the Prime Medusa CAAD system , and received five dollars per hour during both the training and testing sessions . Apparatus The following apparatus was used in this study : ( 1 ) An IBM PC running a data collection program written in compiled Basic ; ( 2 ) A Prime 6350 super minicomputer with 32 megabytes of memory ; ( 3 ) Video camera , recorder , monitor and video cassettes ; ( 4 ) Prime Medusa Architectural package , version 2 . 42 ; ( 5 ) Prime Medusa workstation including a color graphics monitor ( 780 by 1024 pixels ) ; and ( 6 ) Menu tablet , stylus and joystick . An illustration of the Prime Medusa graphics equipment and menu is shown in Figure 1 . Data Collection Techniques The data collection techniques used in this experiment were ( 1 ) protocol analysis , whereby the subject talks aloud and voice recordings are made ( Akin , 1985 ) , ( 2 ) D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 80 D . L . Cuomo and J . Shan ' t Graphics Display - Unit Screen Joystick . Book Menu , on the Data Tablet ^ Stylus ( Pen Probe ) Keyboard FTCME MEDUSA . . , — - : — = Figure 1 . Prime Medusa graphics equipment and menus ( Borge & Benevides , 1986 ) . The depiction of the menus is intended to illustrate the overall complexity of the CAAD System . It is not intended that the details of this information be legible in this figure . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 81 videotaping the CAAD workstation screen to record the output of the design process ( Waern , 1 986 ) , and ( 3 ) the interactive script file . The interactive script file is a time - stamped record of both the subject ' s command input and the computer ' s textual response or prompts ( Figure 2 ) . This interactive script file was obtained by tapping a wire into the Prime computer line and sending a copy of the information being input to the work station from the mainframe to an IBM PC . The IBM PC ran a compiled Basic program which filtered out the graphics characters that are mixed in with the text characters , and reconstructed the textual interactions . This included prompts , echoing of commands input by the subject , and error messages generated by the computer . The time at which each interaction is produced was also recorded . For reasons given below , the Prime computer ' s built - in techniques for storing user - computer interactions , namely , the command output files ( COMO files ) and the Monitor function , could not be used . Although the COMO files record the subject ' s input and the resulting computer output , they can only be used when graphics are not involved ( the graphical output gets printed into the file as nonreadable char - acters ) . The monitor file , on the other hand , records only subject input commands and would have worked quite nicely except for the fact that the architectural add - on package to the base Medusa utilizes " supersyntax " programs ( macros ) . When these supersyntax commands are chosen by the subject ( e . g . " door " ) , the whole program which is executed to generate the door is written to the monitorfile . Another problem with the monitor file was that the computer ' s responses to input commands ( e . g . error messages ) would not have been recorded as they are with the IBM PC data collection method . It must be emphasized that the resolution of the various problems associated with obtaining the interactive script file was a key to this study . Without this data , studies of this type are necessarily limited to design tasks of an artificial nature that are not only of little interest to system designers but are also not very meaningful in terms of gaining insight into how the computer affects human design activities . The videotaping of the screen is an important data collection method because it is the only way to determine how many alternative designs have been generated and how many segments have been deleted during backtracking . The interactive script would show something was made current and deleted but not how many segments were in that current element . Only by reviewing the videotape could this information be determined . The protocols are important for several reasons . The protocols explain what the drawn entities represent ; for example , do the lines represent a sidewalk or a parking space ? Also , during the delay times ( times when commands are not being input ) , the protocols are needed for determining whether planning or evaluation activities are occuring . Procedure The testing procedure was as follows . The subjects ( tested individually ) each received the architectural program for task 1 or task 2 , at its preassigned level of complexity . The subjects were given instructions ( e . g . told to read the program and talk aloud D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 82 D . L . Cuomo and J . Sharit 02 : 01 : 39 [ 6 ] FLO > ARE > ROM . . O : SEG $ RP 02 : 01 : 49 [ 6 ] FLO > ARE > ROM . . O : CONL LCN LAYN < 100 + 27 > 02 : 01 : 52 [ 6 ] FLO > ARE > ROM > LCN . . 127 : RUN MED400 * > AECWORK > U . SS > WALL _ EXT . PROG 02 : 02 : 13 Wall construction is currently 10 . To change wall 02 : 02 : 13 construction enter " Wall L , C or R and thickness ( es ) " 02 : 02 : 14 Probe start of wall : < INT $ RP > 02 : 02 : 14 NEA $ RP 02 : 02 : 22 Probe next point of wall , CLOSE or END LINE : < INT $ RP > 02 : 02 : 22 PER $ RP 02 : 02 : 22 Probe next point of wall , CLOSE or END LINE : < INT $ RP > 02 : 02 : 25 PER $ RP 02 : 02 : 26 Probe next point of wall , CLOSE or END LINE : < INT $ RP > 02 : 02 : 30 ENDL 02 : 02 : 33 Wall construction is currently 10 . To change wall 02 : 02 : 33 construction enter " Wall L , C or R and thickness ( es ) " 02 : 02 : 33 Probe start of wall : < INT $ RP > 02 : 02 : 33 DONE 02 : 02 : 34 [ 6 ] FLO > ARE > ROM . . O : DELA LCN 02 : 03 : 14 »REDRA 02 - 03 ' 28 * SAV S TASK2 02 : 03 : 28 CB78 : Say YES if OK to overwrite existing filed sheet with name : 02 : 03 : 29 < USRO > IE > SHAR1T > DONNA > CHUCKTEST > S . TASK2 02 : 03 : 29 * YES 02 : 03 : 30 CB73 : Sheet saved on file : < USR0 > ffi > SHARIT > DONNA > CHUCKTEST > S . TASK2 02 : 03 : 30 »RUN MED400 * > AECWORK > U . SS > EXPAND _ WALL . PROG 02 : 03 : 31 Enter expanded wall line type : < SL0 > 02 : 03 : 34 Probe wall to be expanded : < SEG $ RP > 02 : 03 : 37 SEG $ RP 02 : 03 : 42 Probe wall to be expanded : < SEG $ RP > 02 : 03 : 42 SEG $ RP 02 : 03 : 46 Probe wall to be expanded : < SEG $ RP > 02 : 03 : 46 SEG $ RP 02 : 03 : 52 Probe wall to be expanded : < SEG $ RP > 02 : 03 : 53 SEG $ RP 02 : 03 : 58 Probe wall to be expanded : < SEG $ RP > 02 : 03 : 59 SEG $ RP 02 : 04 : 01 Probe wall to be expanded : < SEG $ RP > 02 : 04 : 05 DONE 02 : 04 : 06 [ 6 ] FLO > ARE > ROM . . O : WIN $ RP Now use RH field to set opposite comer 02 : 04 : 50 [ 6 ] FLO > ARE > ROM . . O 02 : 04 : 58 [ 6 ] FLO > ARE > ROM . . O 02 : 05 : 46 [ 6 ] FLO > ARE > ROM . . O DY = - 60 02 : 05 : 53 [ 6 ] FLO > ARE > ROM > LCN . . 127 : SEG $ RP 02 : 06 : 00 [ 6 ] FLO > ARE > ROM > LCN . . 127 : CONL LCN LAYN < 100 + 27 > LAS VER DX = 96 02 : 06 : 13 [ 6 ] FLO > ARE > ROM > LCN . . 127 : CONL LCN LAYN < 100 + 27 > LAS VER DX = 84 Probe LHS first $ RP REDRA SEG $ RP CONL LCN LAYN < 100 + 27 > LAS HOR Figure 2 . A sample of a partial interactive script . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 83 as they work ) , and given an approximate upper limit for performing the task . The time limits were considered as being sufficient based on the opinions of the two architects who reviewed the architectural programs . Next , the video camera built - in clock was set to 00 : 00 : 00 and the data collection program was started . The clock on the video camera was synchronized as closely as possible to the IBM PC clock . This enabled easy matching of protocol times with the times associated with the interactive script . The subject then performed the design task for the next three , four , or five hours with a 10 - minute break every 2 hours . During the break the videotape was replaced with a new one . The entire procedure was repeated twice for each subject ( each subject performed both tasks 1 and 2 ) , with the order of tasks being randomized between subjects . The six subjects were assigned subject numbers ( 1 —6 ) . Subjects 1 and 2 were assigned to level of complexity 1 for both tasks ( least difficult level ) , Subjects 3 and 4 were assigned to level of complexity 2 for both tasks , and Subjects 5 and 6 were assigned to level of complexity 3 for both tasks ( most difficult level ) . Before proceeding with the actual experiment , a pilot study was performed in order to evaluate the sensitivity of the proposed methodology and the feasibility of the data collection techniques . ANALYTIC TECHNIQUES Performance measures and methods of analyzing these measures were developed that were believed to reflect meaningful changes in human behavior during the performance on CAAD tasks . Due to the complexity of the CAAD process , several different types of analyses were used . For the most part , these analytic techniques complement each other , providing the overall characterization of design behavior that is necessary for evaluating human performance in this problem domain . The analysis techniques used included identification of phases in the CAAD task , graphical summarization techniques ( design functions , design movement graphs , and design rate graphs ) , analysis of inter - event intervals , Markov process analysis , protocol analysis , and error analysis . Each of these techniques and the rationale for using them are discussed below . Identification of Phases in the CAAD Process One of the first steps in analyzing the data was to identify stages or phases in the CAAD process . Phases are defined here as large identifiable subproblems potentially capable of producing human CAAD performance idiosyncratic to that stage . For instance , design behavior at the beginning of the CAAD process is necessarily dif - ferent from CAAD behavior at the end of the design process . By looking at CAAD performance within phases of the overall process , in addition to focusing on the entire interactive CAAD process , changes in performance within a single task can be identified . These changes can help us better understand the relationship between the application task and the usefulness of the CAAD tool . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 84 D . L . Cuomo and J . Sharit Coding the Design Data for Data Summarization Different ways of coding the design data to allow the CAAD process to be quantified for the purposes of modeling and data summarization were explored . The technique developed for coding is based on the idea that during CAAD , the actual drawing of elements on the screen can be perceived as forward movement in terms of moving through design states toward a goal in the design process ( Kalay , 1985 ) . In contrast , non - drawing commands or system commands , while necessary in the overall CAAD process , are not directly moving the designer closer to his or her goal . Deleting elements already drawn on the screen can be perceived as moving the designer back to some previous state in the design process , and thereby can be considered as backward movement or backtracking . The 1 , 0 , - N coding technique described below derives from this idea and from the CAAD functions given in Table 1 . This technique involves classifying each subject input in the interactive script as being either a move forward in the design process ( represented graphically as a move upward and assigned a + 1 ) , a nonmovement in the design process ( represented graphically as a horizontal movement and as - signed a 0 ) , or a backward movement in the design process ( represented graphically as a downward movement and assigned a negative number [ - N ] corresponding to the number of elements deleted ) . Each subject input is correspondingly assigned a number of either + 1 for a forward movement , 0 for a horizontal movement , or a negative number , depending on the number of elements deleted fora backward movement . This number is referred to as " design movement . " The cumulative design movement , that is , the sum of the design movements over the entire task period , is referred to as the " design height . " Graphical Summarization Techniques With the design process quantified according to the scheme described above , a plot of design height versus subject inputs to the computer provides a useful data sum - marization technique . This plot essentially depicts the overall movement through the CAAD process and is referred to as a " design function " . Two such design functions are illustrated in Figures 3a and 3b . To display the amount of movement associated with each subject input requires plotting design movement , as opposed to design height , against these inputs . This graph facilitates the identification of forward , horizontal , and especially backward movements as illustrated in Figure 4 , and is referred to as a " design movement graph " . Analysis of Inter - Event Intervals As was noted earlier , the interactive script included prompts , echoing of commands by the subject , and error messages generated by the computer . Therefore , in addition to the subject inputs to the computer , the interactive script also contained other events , specifically those that occurred in response to the user ' s input to the computer ( Figure 2 ) . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 85 o . o 400 . 0 BOO . O SUBJECT INPUT NUMBER 1003 . C Figure 3a . Design function for Subject 4 during performance of task 2 . CD CT ) LJ 200 . 0 400 . 0 600 . 0 SUBJECT INPUT NUMBER s : o . o 1000 . 0 Figure 3b . Design function for Subject 6 during performance of task 2 . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 86 D . L . Cuomo and J . Sharit •fl t / l O 0 . 0 200 . 0 400 . 0 600 . 0 800 . 0 SUBJECT INPUT NUMBER 1000 . 0 Figure 4 . Illustration of a design movement graph . The analysis of inter - event intervals examines the time intervals between these events , and is referred to as " inter - event interval analysis . " It identifies those periods of time when computer design activity is not occuring . These periods of time are referred to as " delay times . " Each delay time has a corresponding " event number " representing the sequential ordering of events in the interactive script . The delay times are found by subtracting the adjacent times recorded on the interactive script . For example , the time difference between the first interactive script line and the second interactive line is the delay time for event # 1 . Graphical plots of delay time vs . event number can indicate patterns in delay times , and thereby enable patterns between type of task , between complexity levels within a task , and between design phases within a task to be identified . These patterns are considered important since relatively long delay times potentially reflect planning and evaluatory activities in the design process . Design Rate Graphs To integrate the preceding sections on design functions , inter - event interval analysis , and design phases , and to bring the concept of time duration into the analysis , another graphical summarization technique was developed . The intent was to show how the design movements progressed over time ( rather than over subject input number ) and within design phases . In the design functions the concept of time du - ration is lacking . For instance , the first half of all of the subject inputs could have been performed in three hours and the last half in one hour . In the design rate graph , this would be indicated by a low design rate followed by a high design rate . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 87 XCD CDLJ a 0 . 0 3 . 6 7 . 2 TIME I SEC ) 1C . B Figure 5 . Design rate graph for Subject 4 during performance of task ! . The design rate graphs also show total task time and the length of design start - up times . While it is impractical to plot every design movement on a graph with time on the x - axis ( due to the length of the data and the way it was collected—both inputs and computer outputs ) , it is possible to find the absolute distance moved , in terms of design height , for a particular design phase and to calculate the length of time of that design phase . Performing this procedure involves using design phase infor - mation , design heights from the design functions , and times from the interactive script . An example of a design rate graph is shown in Figure 5 . Each labeled increment on the x - axis equals one hour . Finite Markov Chain Another CAAD related process which will be explored is the way in which the designer moves between different classes of computer commands . These transitions can indicate that different design behaviors are occurring . One way of exploring this process is to model the CAAD process as a finite Markov chain , which is defined by Kemeny and Snell ( 1960 ) as " a stochastic process which moves through a finite number of states , and for which the probability of entering a certain state depends only on the last state occupied " ( p . 207 ) . Modeling user behavior as a Markov process has been done for other types of computer tasks such as freeform text editing ( Hammer & Rouse , 1979 ) and search D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 88 D . L Cuomo and J . Sharit activity on an on - line retrieval system ( Penniman , 1975 ) . State transition matrices can show patterns of computer commands or classes of commands selected to complete a particular task . In this study , commands were classified based on higher level functional groupings that are meaningful in terms of cognitive effects induced by the tasks . These higher level functional groupings are given in Table 3 . As this table indicates , with the exception of the draw ( + 1 ) , delete point ( — 1 ) , and backtrack ( - N ) states , the remaining states all represent commands that were previously associated with horizontal movement and coded as 0 . What was primarily accomplished by this classification scheme was the further differentiation of com - mands associated with horizontal movement . Overall , this type of classification scheme allowed for a more precise definition of states and consequently for the use of concepts and analyses associated with state transitions . The overall percentage of useage of particular functions represents the steady state probabilities derived from this analysis , and will potentially reflect differences between tasks , complexity level , or individual differences . Utilizing this classification scheme and the resultant steady state probabilities , we would interpret a higher probability of being in a forward design movement ( 1 - coded ) state than in a non - movement ( 0 - coded ) state as indicative of more efficient performance . The prob - abilities associated with being in a large negative - number state implies that more alternatives were tried , and suggests a higher decision - making load . Table 3 . Definitions of the 10 CAAD States State Definition Error Any input by the user which results in an error message from the computer . Drafting Elements which are temporary and used to reduce the motor accuracy needed in locating points . These include grid functions , scales , and construction line commands . Split / Join Inputs which are used to redefine the hierarchy of elements . These include splitting or joining line segments and shifting elements into other clumps . Perceptual All inputs which concern viewing the drawing . These include panning , zooming , windowing , redrawing the screen , etc . Motor Inputs that put elements into files for the purpose of copying these elements on the screen . They reduce motor load in terms of the number of elements to be drawn . Query Inputs which request information from the computer . Draw All inputs which result in an element , which will be part of the design drawing , being put on the drawing or manipulated . These include text , lines , symbols , unloading files , geometric manipulations , and supersyntax geometric definitions . System Miscellaneous computer commands which do not fall into any other category and do not contribute directly to the design process . These include commands such as moving between clumps , starting or ending segments , making elements current , saving the drawing , etc . Delete point A backward movement ( deletion ) of an element which took one draw command to input , and includes commands that delete a point , symbol , etc . Backtrack Includes all backward movements ( deletions ) which are greater than one unit . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 89 Statistical Tests on Markov Transition Matrices Statistical tests can be used to compute the probability that the Markov transition matrices could be from the same Markov process . This can be done using asymptotic chi - squared tests which have been shown to simulate chi - squared tests in contingency tables ( Anderson & Goodman , 1957 ; Gold , 1963 ) . Such tests on matrices were used in the studies by Hammer & Rouse ( 1979 ) and Penniman ( 1975 ) . Protocol Analysis Protocol analysis is defined as " a technique devised to infer the information pro - cessing mechanisms underlying human problem solving behavior " ( Akin , 1 985 , p . 23 ) . A protocol is the recorded behavior of the problem solver which is obtained as a result of instructing a subject to " think aloud . " Svenson ( 1985 ) noted that " human decision making cannot be understood by simply studying final decisions , " but that the cognitive processes which ultimately lead to the choice of a decision alternative must also be studied ( in Schweiger , Anderson , & Locke , 1985 ) . The protocols in this experiment were intended to be used in conjunction with the interactive script to clarify the reason for certain actions in situations where ambiguity existed . For example , if lines are drawn on a site , do they represent a parking lot or a sidewalk ? The protocols will also supply information on plans , heuristics , decision making , problem - solving strategies , and , in general , information that cannot be obtained from the other data collected . Error Analysis The error analysis consists of identifying that an error has occurred ( see Table 3 ) and identifying where in the task errors occur most frequently . This information can be obtained from the steady - state probabilities resulting from the Markov analysis and the use of the Markov analysis by design phase , respectively . The cause of errors can be determined based on Norman ' s ( 1983 ) scheme or from the levels of input interaction discussed earlier . This knowledge provides insight into the cognitive mechanisms underlying the errors . Factors affecting error rate ( e . g . , task complexity , type of task , subjective factors ) can also be determined . This part of the error analysis can be performed using the information in the interactive script , which contains the computer - generated error messages , and the probabilities from the Markov analysis . Design errors , defined here as not meeting a specification in the architectural pro - gram , were also examined . This type of information can be obtained from judging the final design drawings . Errors are important to this study because an error can be the result of basic human cognitive limitations ( e . g . , capacity exceeded in WM ) , subject limitations ( e . g . , lack of knowledge in LTM or an incomplete mental model of the computer system ) , or a poor computer design ( such as an interface design flaw or the incompatibility of the computer with the task ) . Identifying the causes of errors can lead to recom - mendations for further training on the part of the user , areas where the computer D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 90 D . L Cuomo and J . Shan ' t interface needs redesign , or how the computer should aid the human in the design process . The effects of task complexity on error rates can be filtered out by looking at the error rate in those design phases uninfluenced by task complexity ( e . g . , text , symbols , and doors and windows ) . For example , placing text on a drawing or furniture in an office should not be more difficult at higher complexity levels than at lower ones . If a subject has high error rates in low mental workload phases such as these , it is probably intrinsic to the subject rather than due to task complexity . The other possible reason for the above scenario in which a subject has a high error rate in a low mental workload phase could be because of a computer design flaw or bug . This can be identified by first looking at the error rate for the subject in other design phases and at the error rates of other subjects for that design phase . If the subject has low error rates in other phases and other subjects have a high error rate only in that one phase , there may be a computer problem with commands used in that phase . If the subject has low error rates in other phases and other subjects do not have a problem with the phase in question , it may indicate a lack of knowledge on the part of that one subject on using commands in that phase . To test the effects of complexity and type of task , the obvious comparisons can be made . The analytic techniques discussed above are summarized in Table 4 . Table 4 . Summary of Analytic Techniques Analytic Technique Contributions of the Technique Phase identification Inter - event interval analysis Design functions Design movement graphs Design rate graphs Markov analysis Protocol analysis The phases of the design process within a task sensitive to task complexity and type of task are isolated . Phases not affected by task manipulation reflect subjective differences . The changes in subject behavior within a single task can be observed . The analysis of delay times between events identifies the location and duration of planning / evaluatory activities in a task . Design functions show the directness of translating design ideas from WM into the computer . The ratio of nonmovement commands to forward design movement commands is seen . The frequency , location , and magnitude of forward , horizontal , and backward movements in the design process are shown . Design rate graphs add the concept of time to the analysis . They illustrate the changes in design rate across phases . When used in conjunction with the delay times and the design function , the causes of design slow - downs can be identified . Markov analysis shows the patterns of movements between high - level command groups ( states ) and the overall probability of being in any one state . This provides information on the subject ' s attempt to reduce certain cognitive loads induced by the task and the frequency of errors . Statistical tests on the transition matrices can identify whether they are similar or different . Protocol analysis provides unique information on heuristics used , and problem - solving and planning strategies , which are not available from other sources . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design » ' RESULTS AND ANALYSIS The purpose of the performance methodology was to derive performance measures and data summarization techniques that reflect meaningful changes occurring during the human ' s performance of architectural design tasks that differ in their cognitive requirements . These measures must also be sensitive to changes in task complexity within these tasks . Through this methodology , we hoped to learn about the human ' s design capabilities and limitations and the role of the computer system as a design aid . As the ensuing sections will attempt to show , the performance methodology was characterized by a relatively high degree of completeness , with some measures sensitive to task complexity , some to task differences , and some to individual differ - ences . The human design process and the usefulness of the computer as a tool were capable of being evaluated somewhat independently . The following sections will discuss the effectiveness of each of the analytic tech - niques and provide one or two examples from the results of the experiment . Due to the tremendous amount of data generated during the experiment , only a sample of the results will be presented here . A more complete treatment of the data can be found in Cuomo ( 1988 ) . It should be pointed out , however , that with the exception of the Markov Process Analysis , the analysis of the data was primarily descriptive , and often based on visual analysis of the various graphical techniques that were developed for this study . In general , the complexity associated with analyzing CAAD tasks and the need for developing measures unique to performance of these tasks precluded the meaningful use of formal statistical methods . As will hopefully become clear in the remainder of this paper , the usefulness of the analysis derived primarily from the integration of the results obtained from the various complementary techniques employed . The 12 final designs that were produced by the architectural subjects during their performance of the experimental tasks can be found in Cuomo ( 1988 ) . Design Phases Sequential design phases for each task were identified from the interactive scripts , the protocols , and the videotape of the computer screen . In task 1 , for instance , the general phases identified were the CAAD start - up phase , drawing the site on the computer , exterior building design , parking lot design / site work , site symbols , interior building design , and doors and windows . As mentioned previously , the identification of phases was necessary because the behaviors induced by each phase varied greatly . For example , in the " drawing the site " phase the user is defining rather than designing . There is very little design decision making during this phase . The exterior building design phase , however , is characterized by a long planning period . A high decision making load exists due to the need to consider many variables such as orientation of buildings , department layout within a building , traffic patterns , service access , and so forth . Evaluation is also necessary to determine if building size spec - ifications are being met . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 92 D . L Cuomo and J . Sharit For each design phase , the percent of time and the percent of subject inputs were calculated . The design phase data served an important supportive role for many of the other analytic techniques . In general , task 1 had higher variability across subjects as to which phase the maximum amount of time was spent in as compared to task 2 . This was believed to reflect the ill - defined nature of the task in the sense that subjects were free to emphasize different parts of the design . In task 2 , however , the emphasis clearly had to be on the interior / exterior bank layout . From the information provided in the architectural program , the problem space was more clearly defined for task 2 . Design Functions Various types of behavioral response during CAAD performance can be identified from the design functions . A high sloping linear function is indicative of few non - movement ( O - coded ) computer commands relative to the total number of forward design movement ( 1 - coded ) commands . The subject is moving more directly towards his or her goal with few extraneous computer commands ( e . g . , by demonstrating very little editing , few computer errors , and a limited use of commands to reduce perceptual or motor workload , etc . ) . A low slope may be indicative of a high com - puter load where many O - coded commands are needed . Causes potentially con - tributing to this condition are high perceptual load , high drafting load , many com - puter errors , or miscellaneous system commands . Problems with computer set - up and initial design phases are also recognizable from the design functions as well as the location , degree , and frequency of back - tracking in the design process . Backward movements are even more discernible on the design movement graphs . For example , as his design function illustrates ( Figure 3b ) , S6 had start - up problems in Task 2 , performed at complexity level 3 . In contrast , S4 had no start - up problems and a much higher sloping design function for task 2 when performed at complexity level 2 ( Figure 3a ) . Analysis of Inter - Event Intervals The inter - event interval analysis was useful for showing patterns of noncomputer - use times . As illustrated in Figures 6a and 6b , both the magnitude of these delay times and where in the design process they occur can be observed . Of most interest were the " long delay times , " defined here as delays greater than or equal to one minute . The frequency and the total time within these long delays were analyzed both between complexity levels within a task and between tasks . Several interesting results were noted from this analysis . One was that the mag - nitude and frequency of the time delays increased with increasing complexity level . Also , the total time spent in long delays showed a slightly different pattern between tasks . Total times in long delays remained higher for all complexity levels of task 1 as compared to task 2 ( Figure 7 ) . These differences between tasks , however , de - creased with increasing complexity levels . Overall , the time - based measures , i . e . , patterns of delay times , frequency of long delay times , total time spent in long time delays , and total task time all appeared D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 93 LJ in S 8 - LJCD 0 . 0 500 . 0 1000 . 0 EVENT NUMBER 15GO . 0 2000 . 0 Figure 6a . Inter - event interval analysis for Subject 1 performing task 1 at complexity level 1 . LJ en LJ o . o 500 . 0 ÍOOO . O EVENT NUMBER Figure 6b . Inter - event interval analysis for Subject 6 performing task 1 at complexity level 3 . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 94 D . L Cuomo and J . Sharit 3 4 SUBJECT NUMBER Figure 7 . Total time in long delays for tasks 1 and 2 . Task 2 - solidbars sensitive to task complexity with some characteristic differences appearing between tasks . It should be noted that the degree of complexity change between levels is not necessarily constant , especially in task 1 . Design Rate Graphs The design rate graph was the performance measure most sensitive to increases in task complexity . The overall slowdown in design rates and the increase in start - up times as complexity increased is apparent for both tasks as illustrated in Figures 8a and 8b . This result was not apparent in the design functions where time is not represented . Differences in design rate within a single task between design phases is also illustrated in the design rate graphs . The reasons for the design rate slow - downs can be determined by analyzing the Markov process data , the design function , and the time delays for that particular phase or task . Markov Process Analysis There were two aspects to the Markov process analysis . The first concerned the direct use of the transition probabilities and the derivation of steady - state vectors from the state transition matrices . This analysis was descriptive and proved to be D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 95 o . o 3 . 6 7 . 2 TIME ( SEC ) 10 . 8 H . 4 3 Level 1 - dotted , Level 2 - solid Figure 8a . Task 1 design rate graphs for Sublets 1 - 6 . Level 3 - dashed 7 . 2 TIME ( SEC ) Level 1 - dotted Figure 8b . Task 2 design rate graphs for Subjects 1—6 . Level 3 - dashed D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 96 D . L . Cuomo and J . Sharit very useful . The second aspect of this analysis involved performing statistical tests on the state transition matrices . Although this analysis did not prove to be as useful , it did allow an important issue to be raised concerning the nature of the CAAD system employed . In any case , it is believed that such an analysis has potentially important implications for this and similar types of studies on human design processes , and its details will therefore be presented . Both aspects of this analysis are discussed below . The steady - state vector derived from this analysis represents the probability of being in a state ( see Table 3 ) independent of the state it came from ; that is , it is the long - term probability of being in a given state . The calculation of steady - state vectors by design phase allowed changes in computer - use behavior over design phases to be identified . The error probability changes identified were particularly interesting and will be used below in the section on computer error analysis as part of a method for identifying causes of error . Overall , the steady - state vectors for a task reflected the percentage of command inputs that were used to reduce certain cognitive loads ( e . g . , the percent of perceptual load reduction commands ) . The steady - state vectors for Subjects 3 and 4 , derived from the performance of task 1 are presented in Table 5 . The state transition matrices provide the probabilities of moving between states ( see Table 6 ) . Statistical tests were performed to test whether ( 1 ) the matrices could have come from the same Markov process , and ( 2 ) if a type of task , task complexity , or subject effect on the patterns of command movements was present . Six state transition matrices were used with infrequently occurring states being collapsed back into the " system " state ( see Table 3 ) to avoid overestimating the degrees of freedom . Use of the chi - squared statistic for testing whether groups of matrices were the same or different revealed significant differences between subjects within each task , be - tween all subjects within both tasks , and between subjects within levels of complexity for both tasks . Comparing pairs of matrices within subjects for both tasks revealed only one significant difference . Table 5 . Illustration of the Steady State Vectors as Derived from Markov Process Analysis for Subject 3 and Subject 4 based on Performance of Task 1 Subject 3 Subject 4 . 080 ( Error ) . 019 ( Error ) . 039 ( Draft ) . 077 ( Draft ) . 049 ( Perceptual ) . 042 ( Perceptual ) . 004 ( Motor ) . 464 ( Draw ) . 389 ( Draw ) . 371 ( System ) . 407 ( System ) . 014 ( Delete Pt . ) . 017 ( Delete Pt . ) . 013 ( Backtrack ) . 014 ( Backtrack ) D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design Table 6 . The State Transitiont Matrices for Subject 3 and Subject 4 Based on Performance of Task 2 97 ( Err ) ( Draft ) ( Per ) ( Mot ) ( Draw ) ( Sys ) ( Del ) ( Back ) ( Err ) ( Draft ) ( Per ) ( Draw ) ( Sys ) ( Del ) ( Back ) ( Err ) . 175 . 071 . 143 . 000 . 058 . 073 . 083 . 200 ( Err ) . 000 . 000 . 000 . 017 . 029 . 000 . 000 ( Draft ) . 035 . 429 . 142 . 000 . 000 . 024 . 167 . 000 ( Draft ) . 000 . 629 . 105 . 000 . 062 . 000 . 083 ( Per ) . 070 . 107 . 057 . 000 . 043 . 045 . 083 . 000 ( Per ) . 000 . 086 . 158 . 000 . 071 . 000 . 167 Subject 3 ( Mot ) . 018 . 000 . 000 . 000 . 007 . 000 . 000 . 000 Subject 4 ( Draw ) . 294 . 000 . 000 . 667 . 389 . 308 . 000 ( Draw ) . 246 . 071 . 057 1 . 000 . 580 . 322 . 167 . 000 ( Sys ) . 706 . 286 . 737 . 302 . 395 . 615 . 750 ( Sys ) . 456 , . 321 . 600 . 000 . 283 . 488 . 500 . 800 ( Del ) . 000 . 000 . 000 . 014 . 018 . 077 . 000 ( Del ) . 000 . 000 . 000 . 000 . 022 . 021 . 000 . 000 ( Back ) . 000 . 000 . 000 . 000 . 035 . 000 . 000 ( Back ) . 000 . 000 . 000 . 000 . 007 . 028 . 000 . 000 tProbability transitions between states as represented in the matrices are occurring from row to column . Testing for task effects and complexity effects ( both between and within tasks ) revealed no significant effects . Using subject number as a treatment did , however , indicate that a significant difference existed between subjects . Therefore , it appears the factor most influencing patterns of command movements appears to be the individual ( see Cuomo , 1988 for more details concerning this analysis ) . The results of the statistical tests were remarkably similar to the results obtained by Hammer & Rouse ( 1 979 ) , who did the same type of analysis on transition matrices collected during user program and document editing sessions . The usefulness of the statistical tests on state transition matrices as an analysis technique for interpreting human performance in CAAD is not entirely clear . The tests appeared to be sensitive to the magnitude of the expected cell frequencies . This point is brought up in Hines & Montgomery ( 1 980 ) : " If these expected fre - quencies are too small , the chi - square will not reflect the departure of observed from expected , but only the smallness of the expected frequencies " ( p . 299 ) . Another possible explanation for the lack of task or level of complexity effects on the transition matrices could be the relative simplicity of the CAAD system . Since the system was capable of performing only basic drawing types of functions , the level of complexity and task - induced effects were likely to be reflected more so in the human ' s cognitive activities ( e . g . , as reflected in long planning times ) than in the human - computer interaction per se . This argument is supported by the complexity effects observed from the design rate graphs , and will be further supported in the D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 98 D . L Cuomo and J . Shan ' t following section on verbal protocols . It is believed that a system with more advanced aiding capabilities would have produced patterns of human—computer interaction that would be more sensitive to complexity and task - related differences since each task would likely have required very different types of computer aiding . These ideas are brought up again in the discussion section . Finally , it should be pointed out that other types of statistical analyses such as the analysis of variance procedure were not considered appropriate for this study . Although this was partly due to the small sample size used , more importantly , uni - variate measures analyzed individually are not very meaningful when the primary interest is in the patterns of response underlying human design processes . Also , the changes in complexity as defined in this study were task specific , thereby significantly limiting the meaningfulness of analyzing the effects of complexity and task concur - rently . This criticism does not apply to the statistical analysis of transition matrices since the goal in this analysis was to determine whether patterns in human—computer interactive response differed . Pairwise Comparisons Utilizing Protocol Analyses Through the analysis of the protocols , an in - depth comparison of subjects ' CAAD behavior was performed , usually pairwise , using meaningful differences in the ob - jective measures collected as the criteria for being selected for comparison . The purpose of these comparisons was to learn more about design behavior by finding causes for differences in behavior ( such as differences attributable to the task , com - plexity , or the individual ) . It was also of interest to determine if the protocols would reveal differences in the problem solving / planning / information processing presumed to underlie the differences observed in the objective measures . In this respect , the protocol analyses turned out to be very useful , and provided unique information on design processes not available from the objective techniques . The primary reasons for differences in the objective measures were traced to fundamental differences in cognitive processing between subjects . For example , the performances of Subject 3 ( S3 ) and Subject 4 ( S4 ) , who were assigned to the same complexity level , were compared for task 2 . Some measures they differed along were error rate , goodness of the design , and time spent in design phases . Their total task times were similar as were their number of backtracks . While their frequency of long delays were close , S4 had a higher total time in these delays ( 49 min and 11 s as compared to 34 min and 48 s ) . The difference in goodness of design was traced , in part , to the way each subject anchored their design . S4 let the design of the interior determine the exterior shape and size while S3 began with the exterior wall design , thereby constraining the as yet undesigned interior . S4 adapted his problem - solving strategy to work more efficiently with the computer . Another design error made by S3 was found to be attributable to a problem in the memory chunks she created to deal with the com - plexity of the relationships provided in the architectural program . S4 increased the complexity of his design by adding his own criteria and ele - ments . By comparing the percentages of time spent in different design phases , it was found that S4 spent 54 % of his time in the interior / exterior layout of the bank , while D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 99 53 spent only 21 . 8 % of her time in this phase ( Cuomo , 1988 ) . S4 , therefore , focused a significant amount of attention on the most difficult part of the design . In terms of computer efficiency , from the data in the " detailing " design phases , it would appear 54 was more efficient in using the computer . The design rate functions illustrate that S3 had a slightly slower design rate in the initial phases than S4 , probably due to inefficiencies in the use of the computer . S4 ' s high objective workload was probably offset by his design experience . A change in design rate was noticed between S4 ' s initial high cognitive load phases and the final low cognitive load design phases . Inferences on the effectiveness of the computer as a design aid were also made from information gained by the protocol analysis technique . For instance , in the example above it was found that the computer did not serve as an aid in determining the size and layout of the bank or in evaluating whether the program criteria were met . The computer also did not serve as an aid in grouping related areas together or in indicating that relationships existed between areas . In the remainder of this section , due to the large amount of protocol data collected and the enormous amount of information that was generated when this data was analyzed in conjunction with the other performance measures , only a brief sampling of the information derived from the use of the protocol technique will be presented . One of the important conclusions drawn from this analysis was that how a design is initiated ( anchored ) can affect the resultant goodness of the design ; certain design strategies were clearly more efficient when implemented on the com - puter than others . The attributes of the bank task were found to result in subjects chunking elements together as a memory aid . The specified program requirements in levels 2 and 3 helped determine the chunking scheme used . Errors in chunking lead to errors in design . As task complexity increased , this chunking process grew more complex . Based on the subjects ' verbalizations , the percent of time spent in design phases as affected by task complexity appeared to reflect the degree of mental workload . Total time in long delays also appeared to reflect the degree of mental workload . The percent of time spent in detailing design phases did not necessarily reflect task complexity ( particularly if long delay times were not present ) , and the design rate typically increased during these phases . In general , the amount of time spent in detailing phases was found to be characteristic of the individual . In task 1 , the building layout decision appeared to be based on one variable as were other design decisions ( e . g . , " By putting the buildings here , each will have it ' s own identity . " ) . Since no criteria were given in the program on which to evaluate design decisions in task 1 , the subjects had to create their own problem constraints to reduce the decision making load . The lack of generation of many alternatives was surprising . When performing task 1 , most subjects showed indications of using previous knowledge and general planning strategies to construct a problem space to work within . This behavior was not as apparent in task 2 although the subjects obviously used knowledge about banks to create their designs . Some other interesting results based on the analysis of protocols included the following : • Since task times and time in long delays increased as complexity increased , D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 100 D . L Cuomo and J . Shan ' t it appeared that the added task workload was assumed by the human rather than the computer . • An overestimation error by S6 in initial area calculations resulted in computer start - up problems and subsequent edits to the bank design . A final bank design that was still too large was believed to be a result of biases stemming from employing the anchoring heuristic . • Perceptual load appeared to be related to sheet size and scale selected ; a poor choice of these items can increase the number of nondrawing commands . • S2 , performing at complexity level 1 , drew a bank with approximately the same level of detail as S6 , performing at complexity level 3 , but in half the time . This illustrates the difference between designing and drawing . • Some subjects experienced problems with specifying rotation angles to get the desired symbol position . This finding was not surprising since this is not an operation architects do in traditional design methods . Computer Error Analysis An analysis of computer errors ( as defined in Table 3 ) was undertaken to determine the likely causes of these errors and ultimately to classify these errors based on their sources . The potential sources of computer errors were believed to be an inappro - priate mental model of the computer system , type of task , task complexity , design phase effects , and computer interface design including command syntax and menu design . From the data collected , it was possible to classify subjects in terms of their computer expertise , which is presumed to reflect the appropriateness of their mental models of the system they utilized . This classification was made more reliable due to the length of the design tasks and the existence of phases characterized by low cognitive load ; that is phases unaffected by task complexity or type of task . In general , the identification of design phases within a task enabled phases characterized by high error rates to be located and for the sources of these errors to be determined . Comparisons between subjects allowed errors due to computer design flaws to be distinguished from errors based on limitations due to cognitive load . Error classification schemes ( e . g . , Norman , 1983 ; Foley et al . , 1984 ) have proved useful not only for explaining the causes of errors , such as failure to identify the correct mode or being " captured " when inputting an infrequently used command sequence , but also for indicating the level of processing at which they occur ( con - ceptual vs . syntactic , etc . ) . Once this information is known , training can be reintro - duced and / or the computer interface can be improved . This is especially important in CAAD systems due to the complexity of the systems and the disruption errors cause in the design process . Based on the analysis , there did not appear to be a difference in error rate due to the type of task . Differences in error rate due to task complexity were also not apparent . Some of the errors in the phases characterized by high cognitive load were thought to be due to the subject attending intently to the design task rather than to the required computer inputs . These types of errors can be identified by looking at the inputs preceding and following the error in order to determine whether D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design this same command was just used correctly or was used correctly immediately after the error message . In general , this type of information can be found from the error - to - error transition probabilities , where a high probability can indicate problems associated with error recovery . The overall probabilities in this experiment ranged from 1 . 7 % to 6 . 4 % for task 1 and 1 . 9 % to 8 . 0 % for task 2 . DISCUSSION The purpose of this study was to develop performance measures capable of reflecting changes in human performance in CAAD tasks , and to use these measures to gain an understanding of the human cognitive processes at work during design . The role of the computer in aiding the human ' s limited information - processing capabilities was also of interest . In general , the results of this study indicated that the cognitive processes used by the human in design can be manipulated by selectively varying the task inputs . The changes in the CAAD process were observed , measured , and summarized using the various techniques developed for this purpose and discussed in this paper . As implied in the section " A Cognitive Framework , " the human actually performed two simultaneous tasks . One task addressed the cognitive design activities associated with the application task while the other task involved using the computer as a design aid to represent the outputs of the design process . The results of the analysis indicated that some of the measures , for example , the design functions , more directly reflected the computer - interaction aspects of the task while other measures , for example , those based on inter - event interval analysis , protocols , and percent of time in a design phase , more directly reflected the human ' s design activities that occurred for the most part independently of the use of the computer . Other measures such as the design rate graph attempted to integrate these two aspects of human performance . The Markov analysis provided a detailed examination of the movements be - tween computer states , where each computer state represented a significant type of human CAAD behavior . Some states were attributable to using the computer as a tool . For example , " Zooming " was not directly part of the design process but was performed because of limitations in perceptibility caused by certain task components when using the computer . It was therefore part of the CAAD process . Other states more directly represented changes in design behavior independent of using the computer as a tool , although the function was necessarily performed on the com - puter . Overall , the measures of performance specifically developed for this study were very effective at meeting the research objectives , especially when integrated with the data from the videotapes of the screen and the verbal protocols . It should be noted that the sensitivity of these measures were improved by virtue of identifying design phases within the individual tasks since all phases were not equally affected by task manipulations . Based on the detailed analysis of the results ( Cuomo , 1988 ) , the following 101 D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 102 D . L . Cuomo and J . Shan ' t conclusions were drawn concerning the overall effectiveness of the performance methodology developed for assessing human performance in CAAD : • Total task time serves as a gross indicator of task complexity . This variable includes the time associated with computer interaction and should be used in conjunction with design height . • Planning time in a design phase is a useful indicator of mental workload during design . Delay time graphs illustrate the pattern of delay times during a task . The total time in delays greater than or equal to one minute as well as the frequency of these delays also reflect differences in planning patterns . The total time in these long delays appears most indicative of task complexity . Unlike task time , this measure is not confounded with computer interaction time . • The percent of time allocated to a design phase reflects the distribution of time during task performance . In phases where actual design activity is oc - curring , the time allocated is a reflection of workload . In phases involving detailing activities , this allocation is a function of individual differences since some subjects prefer the detailing while others spend little time in these phases . • Design functions indicate the directness with which the computer commands are used for movement toward the design goal . Non - drawing commands all serve to detract from moving toward the goal and are an artifact of using the computer as a tool . Miscellaneous system commands are mostly a function of the design of the computer system . A goal of system designers should be to minimize these types of inputs in order to reduce the human resources needed when performing a task on a CAAD system . Design functions also indicate the presence of start - up problems , as well as the pattern of forward , horizontal , and backward movements . • Design movement graphs reflect the frequency and magnitude of backward movements in the design process . A large number of backward movements usually indicates that many alternatives at solving the design problem are being attempted . Some backwawrd movements are due to design errors while others are the result of conceptual changes in the design . • Design rate graphs indicate the rate of forward movement in a design task , and serve to highlight the differences in rates between design phases . The time duration associated with precomputer phases and start - up problems can also be observed from these graphs . In both tasb , as complexity increased , start - up times increased and design rates decreased . Design rate graphs pro - vide a good estimate of task complexity during CAAD performance that is representative of planning , designing , and computer interaction behaviors . When used in conjunction with the design functions , inter - event interval anal - ysis , and steady - state vectors as evaluated by design phase , the reasons for design rate changes can be determined . • Steady - state vectors reflect the number of times certain computer functions were entered . Each state provides relatively unique information concerning the cognitive processes being used by the designer . The steady - state vectors as evaluated by design phase indicate the changes in types of computer D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 103 activities required over a task . The transition matrices show the patterns of movement between states . • The perceptual load on a subject appears to be related to the sheet size and the scale selected . A poor choice can increase the number of nondrawing commands . • Errors are primarily a function of the following : the individual as reflected by his or her mental model of the system , the computer interface design , and the mental workload the user is experiencing . Error - to - error transitions can in - dicate ease of error recovery . From these transitions , we can infer whether errors were due to a poor mental model ( i . e . , when the individual does not recover easily from an error ) or to a momentary lapse in attention due to design workload ( e . g . , various types of slips ) . • From the steady - state vectors as evaluated by design phase , a high probability of error in specific design phases across subjects indicates a possible computer interface and functionality problem . If a high error probability exists for only a single subject in that phase , further training is indicated . • Norman ' s ( 1983 ) error classification of slips is applicable to the CAAD prob - lem domain and is useful for identifying the underlying causes of errors . The classification scheme of Foley et al . ( 1984 ) could help to identify the depth of processing at which an error occurred . This knowledge can result in an improved interface design . • Protocol analysis is the only way to discover the cognitive strategies ( plans , problem - solving techniques , heuristics used , etc . ) underlying the design proc - ess . This information is critical for improving the aiding capabilities of CAAD . The user ' s dissatisfaction with certain aspects of the computer interface or functionality can also be discovered through protocols . The problem with objective measures alone is that they can look the same for different types of behavior—for example , a backtrack due to a design change appears the same as a backtrack due to a design error . Protocols therefore clarify am - biguities in the design process . Other information gained from the protocols was that certain design strategies work more efficiently on the computer than others ( e . g . , designing the interior layout before the exterior walls ) . Also , certain subjects appeared to have some difficulty with operations not used in traditional design methods such as specifying rotation angles to achieve the desired symbol position . This could point to the need for the retraining of architects when switching from traditional design methods to a CAAD tool , in addition to the training required to use the computer . Classification of CAAD Tasks The two CAAD tasks employed were developed with the purpose of inducing dif - ferent problem solving strategies in the human and requiring differing degrees of compatibility with the computer . Overall , this approach appeared successful at pro - viding a broader base of results for evaluating human CAAD performance . However , there was not as strong a difference in all the the measures as was expected . One reason for this relative lack of sensitivity between tasks was that the computer system D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 104 D . L Cuomo and J . Sharit used for the study was a low - level design aid—essentially , it was a drawing aid . This conclusion was based in part on the design functions derived between tasks for each subject ( which were often quite similar ) , and was also consistent with the results of the chi - square tests as noted earlier . If the computer system was capable of higher level aiding functions , more noticeable differences would have likely been observed in terms of what functions the user chose for aiding purposes . The effects of the task on CAAD performance were most apparent from the protocols and somewhat less apparent from the objective measures that were developed . With respect to the issue of compatibility between task and the CAAD system , the finding that subjects performing task 1 had , in each case , a higher percentage of subject inputs dedicated to drawing commands could indicate that the site layout task was more directly applicable to this computer system . The bank task had more conceptual design errors resulting from the failure to correctly convert the specified verbal relationships into corresponding spatial relationships . Since these verbal re - lationships were not directly representable in the computer , subjects resorted to chunking as a memory aid . The results of the analysis indicated that areas not specified by a relationship were not chunked into a group as quickly as those with relationships specified in the architectural program . The performance methodology employed was capable of detecting differences between the two tasks in behavior indicative of planning activities . In task 2 , subjects appeared to plan farther ahead in order to account for where all the bank areas would go . In contrast , when performing task 1 , most subjects generated partial solutions without considering other parts of the problem . Task effects on CAAD performance were also noticeable in the generation of constraints and the use of heuristics . In task 1 , subjects had to create their own problem space to work in and narrow down the number of possible solutions to be considered . Very little of this type of behavior was observed in task 2 where the goals and constraints were much more well - defined . The large pool of potentially useful knowledge associated with task 1 is one of the attributes of ill - structured problems . The measure demonstrating this difference in ¡ll - structuredness between tasks was the variability across subjects as to which design phase the maximum amount of time was spent in , with less variability reflecting increased structure . Overall , the bank task actually appeared to be the easier of the two tasks , with the difference in task difficulty narrowing at higher levels of complexity . Another effect of human CAAD performance stemming from differences be - tween the tasks was based on the type of information presented . Task 1 had quan - titative information provided while task 2 did not . This factor caused minor problems during task 2 for at least two subjects . S6 overcalculated the total bank size and had to adjust several times . S3 underestimated her total size and was forced to add - on to the existing structure . S3 also did poorly on area calculations during task 1 , resulting in buildings having to be deleted and redrawn even though the building area dimensions were provided . Complexity Levels in CAAD Tasks The adjustments made to the " base " tasks by increasing their levels of complexity were relatively successful in terms of eliciting changes in performance measures . The D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 105 graphs of delay times indicated that delays between events increased in frequency for both tasks as complexity increased . Individual factors also influenced the apparent complexity of a task . S5 , for instance , decreased the complexity of task 1 by virtually ignoring the difficulty of planning the parking and simplified this phase of his task . The analysis was , however , able to reflect this behavior . In general , the more inter - esting behavior differences seemed to occur at levels two and three of complexity . As the task demands increased , human information processing limitations became more pronounced . Computer Aiding Although task 1 required the generation of alternative designs and site layouts , most of the subjects did not generate many alternatives and instead used a heuristic approach to choose just one layout . The subjects also did not fully utilize the computer capability of manipulating the positions of buildings . In principle , computer - aiding would have been helpful for generating design alternatives . Woodbury ( 1986 ) postulates that a computer should act as an extension of the human information - processing system . Unlike computers , humans are forced to use more selective heuristic search strategies due to search time limitations . The use of computer capabilities can allow designers to adopt a more effective method for generating solutions than heuristic search , thereby overcoming structural limi - tations associated with the human information processing system ( Woodbury , 1 986 ) . With the capability for the computer to generate many solutions rapidly , the biases resulting from heuristic methods should decrease , in turn increasing the power of the overall human - computer system . Task 2 could have benefitted from computer - aiding through the representation of relationships in the design . Although subjects attempted to remember all the relationships through chunking , errors were still made . This problem would be ac - centuated in larger design problems of this type . Woodbury ( 1 986 ) addressed this issue more generally by noting that the ability to represent abstractions ( partial representations of an object that expresses some information about the object and suppresses all other information , for example , a bubble diagram which expresses adjacencies but not shape ) should also be present in a true design system . This would call for the capability to input , in some manner , relationships into the computer that could be represented efficiently and quickly . Overall , this could increase the number of program specifications that are met . Aiding would also have been desirable for helping determine room sizes , the sizes of parking lots required to accommodate the number of employees ( e . g . , a database of common architectural square footages ) , and for calculating overall bank square footages . Problems at this design stage were apparent for both subjects 3 and 6 . For both tasks , the computer should aid in choosing the optimal sheet size and scale such that the drawing fills the screen and perceptual inputs are minimized . The use of windows could also be effective . A window on the screen could always display a zoom of the area near the cursor , while the rest of the screen shows the overall design layout . This would increase the efficiency of the CAAD system by minimizing the inputs required of a user to perform a task . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 106 D . L Cuomo and J . Sharit In the early phases of the design , the current system requires formal inputs to draw preliminary design ideas . A " sketch mode " is needed to allow users to quickly sketch ideas with the minimum of inputs . Easy editing and manipulation is required during this phase . Woodbury ( 1986 ) notes that in design , precision is not always necessary . An object can be placed beside other objects without determining a precise location for them . The formal representations of objects at early design phases is actually a hindrance to the designer . The computer load is excessive at too early a stage . The effort of editing the formal representations is too much work as was illustrated in S3 ' s bank design , where she decided not to redesign the exterior to accomodate a vestibule and , in general , in the few number of design alternatives that were attempted . To be an efficient external memory aid , internal representations must be rapidly translated to external ones , so that it could effectively increase the capacity of WM ( Woodbury , 1986 ) . Kalay ( 1985 ) has pointed out that CAAD ' s failure to improve architectural design practices and products can be attributed to the fact that most systems are used as drafting tools rather than design assistants . This study firmly supports this statement . The actual design processes were still occuring within the human with little aid from the computer . As the complexity increased , the power of the design ( i . e . , goodness of the design ) decreased , even though the utilization of human information - pro - cessing resources appeared to increase . The efficiency also decreased since more time was required to complete the design tasks at higher levels of complexity . New " design assistant " types of computer systems are currently being developed . The measures formulated here can be used to ensure that such systems are indeed aids rather than additional burdens on the human ' s information processing system . The design methods extracted from the protocols , for example , can be implemented through artificial intelligence techniques in a CAAD database . As the CAAD system interfaces will become more complex , it will become extremely important that we have methods available that allow us to determine whether these systems are sup - porting human design processes . REFERENCES Akin , O . , ( 1978 ) How do architects design ? In Latombe , J . C . ( Ed . ) . Artificial Intelligence and pattern recognition in computer - aided design , Amsterdam : North - Holland Publishing Co . Akin , O . , ( 1985 ) Exploration of the design process CRIT , 15 , Summer Anderson , T . W . , & Goodman , L . A . , ( 1957 ) Statistical inference about Markov Chains . Annals of Mathematical Statistics , 28 , 89 - 110 . Atwood , M . E . , ( 1984 ) A report on the Vail workshop on Human Factors in Computer Systems . IEEE Computer Graphics and Applications , 4 ( 12 ) , 48 - 66 . Borge , T . , & Benevides , A . , ( 1986 ) Prime Medusa AEC User ' s Guide , DOC9280 - 1 LA . Natick , MA , Prime Computer Inc . Corley , M . R . & Allan , J . J . , ( 1976 ) Pragmatic information processing aspects of graphically accessed computer - aided design . IEEE Transactions on Systems , Man , and Cybernetics , 6 ( 6 ) . Cuomo , D . L . , ( 1988 ) A study of human performance in computer - aided architectural design : Methods and analysis . Unpublished doctoral dissertation , State University of New York at Buffalo , Amherst , NY . Foley , J . D . , Wallace , V . L . , & Chan , P . , ( 1984 ) The human factors of computer graphics interaction techniques . IEEE Computer Graphics and Applications , 4 ( 11 ) , 13 - 48 . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014 Human Performance in Computer - Aided Architectural Design 107 Gold , R . Z . , ( 1963 ) . Tests auxiliary to Chi - squared tests in a Markov Chain . The Annals of Mathematical Statistics , 34 ( 1 ) , 56 - 74 . Goumain , P . , & Sharit , J . , ( 1988 ) . Human - computer interaction in architectural design . In M . G . Helander ( Ed . ) . Handbook of Human - Computer Interaction , Amsterdam : North - Hol - land . Hammer , J . M . , & Rouse , W . B . , ( 1979 ) . Analysis and modelling of freedom text editing behavior , ( pp . 659 - 664 ) Proceedings of the International Conference on Cybernetics and Society , Denver , CO . Hines , W . W . , & Montgomery , D . C . , ( 1980 ) . Probability and statistics in engineering and management science New York : Wiley . Kalay , Y . E . , ( 1985 ) . Redefining the role of computers in architecture : from drafting / modelling tools to knowledge - based design assistants . Computer - Aided Design , 17 , 319 - 328 . Kemeny , J . G . , & Snell , J . L . , ( 1960 ) . Finite Markov chains . New York , NY : Van Nostrand Co . Knox , C . , ( 1984 ) . CAD / CAM Systems : Planning and implementation . New York : Dekker Inc . Norman , D . A . , ( 1983 ) . Design rules based on analyses of human error . Communications of the ACM , 26 ( 4 ) , 254 - 258 . Penniman , W . D . , ( 1975 , October ) . A stochastic process analysis of on - line user behavior . ( pp . 147 - 148 ) . Proceedings of the 38th Annual Meeting . Boston : ASIS . Schweiger , D . M . , Anderson , C . R . , & Locke , E . A . , ( 1985 ) . Complex decision making : A longitudinal study of process and performance . Organizational Behavior and Human Decision Processes , 36 , 245 - 272 . Sharit , J . , & Cuomo , D . L . , ( 1 988 ) . A cognifively - based methodology for evaluating human performance in the computer - aided design task domain . Behavior and Information Tech - nology , 7 ( 4 ) , 373 - 397 . Simon , H . A . , ( 1973 ) . The structure of ill - structured problems . Artificial Intelligence , 4 , 181 - 201 Simon , H . A . , & Newell , A . , ( 1971 ) . Human problem solving : The state of the theory in 1970 . American Psychologist , 26 , 145 - 159 . Svenson , O . , ( 1985 ) . Cognitive strategies in a complex judgement task : Analysis of concurrent verbal reports and judgements of cumulated risk over different exposure times . Org . Beh . and Hum . Dec . Proc , 36 , 1 - 15 . Teicholtz , E . , ( 1985 ) . CAD / CAM handbook . New York : McGraw - Hill . Tversky , A . , & Kahneman , D . , ( 1974 ) . Judgement under uncertainty : Heuristics and biases . Science , 185 , 1124 - 1131 . Waern , Y . , ( 1986 ) . Problem solving in computer - aided design . HUFACIT Series , Dept . of Psychology , University of Stockholm , Stockholm , Sweden . Wickens , C . , ( 1980 ) . The structure of attentional resources . In R . Nickerson ( Ed . ) . Attention and performance ( Vol . VIII ) , Hillsdale , NJ . : Erlbaum . Wickens , C . , ( 1984 ) . Engineering psychology and human performance . Columbus : Merrill . Wickens , C . , Sandry , D . , & Vidulich , M . , ( 1983 ) . Compatibility and resource competition be - tween modalities of input , central processing , and output . Human Factors , 25 ( 2 ) , 227— 248 . Woodbury , R . F . , ( 1986 ) . Strategies for interactive design systems . In A . C . Harfmann , Y . E . Kalay , B . R . Majkowski , & L . Swerdloff , ( Eds . ) . The compatibility of design , SUNY at Buffalo Symposium on CAAD . D o w n l o a d e d by [ T h e U n i v e r s it y o f M a n c h e s t e r L i b r a r y ] a t 06 : 13 26 N ov e m b e r 2014