OPERATIONS RESEARCH CENTER Working Paper MASSACHUSETTS INSTITUTE OF TECHNOLOGY by An Efficient Rescaled Perceptron Algorithm for Conic Systems OR 379 - 06 Alexandre Belloni Robert M . Freund Santosh Vempala September 2006 An Efﬁcient Re - scaled Perceptron Algorithm for Conic Systems Alexandre Belloni MIT Operations Research Center , E40 - 129 , 77 Massachusetts Ave . , Cambridge , Massachusetts 02139 email : belloni @ mit . edu http : / / web . mit . edu / belloni / www Robert M . Freund MIT Sloan School of Management , E53 - 357 , 50 Memorial Drive , Cambridge , Massachusetts 02142 email : rfreund @ mit . edu http : / / web . mit . edu / rfreund / www Santosh Vempala Georgia Institute of Technology and MIT , 2 - 363C , 77 Massachusetts Ave . , Cambridge , Massachusetts 02139 email : vempala @ math . mit . edu http : / / www - math . mit . edu / ~ vempala / The classical perceptron algorithm is an elementary row - action / relaxation algorithm for solving a homogeneous linear inequality system Ax > 0 . A natural condition measure associated with this algorithm is the Euclidean width τ of the cone of feasible solutions , and the iteration complexity of the perceptron algorithm is bounded by 1 / τ 2 , see Rosenblatt 1962 [ 14 ] . Dunagan and Vempala [ 4 ] have developed a re - scaled version of the perceptron algorithm with an improved complexity of O ( n ln ( 1 / τ ) ) iterations ( with high probability ) , which is theoretically eﬃcient in τ , and in particular is polynomial - time in the bit - length model . We explore extensions of the concepts of these perceptron methods to the general homogeneous conic system Ax ∈ int K where K is a regular convex cone . We provide a conic extension of the re - scaled perceptron algorithm based on the notion of a deep - separation oracle of a cone , which essentially computes a certiﬁcate of strong separation . We give a general condition under which the re - scaled perceptron algorithm is itself theoretically eﬃcient ; this includes the cases when K is the cross - product of half - spaces , second - order cones , and the positive semi - deﬁnite cone . Key words : Convex Cones ; Perceptron ; Conic System ; Separation Oracle MSC2000 Subject Classiﬁcation : Primary : 52A20 , 90C60 ; Secondary : 90C25 OR / MS subject classiﬁcation : Primary : Convexity ; Secondary : Random Walk 1 . Introduction . We consider the problem of computing a solution of the following conic system (cid:26) Ax ∈ int K x ∈ X ( 1 ) where X and Y are n - and m - dimensional Euclidean subspaces , respectively , A : X → Y is a linear operator and K ⊂ Y is a regular closed convex cone . We refer to this problem as the “conic inclusion” problem , we call K the inclusion cone and we call F : = { x ∈ X : Ax ∈ K } the feasibility cone . The goal is to compute an interior element of the feasibility cone F . Important special cases of this format include feasibility problem instances for linear programming ( LP ) , second - order cone programming ( SOCP ) and positive semi - deﬁnite programming ( SDP ) . The ellipsoid method ( [ 9 ] ) , the random walk method ( [ 2 ] ) , and interior - point methods ( IPMs ) ( [ 8 ] , [ 10 ] ) are examples of methods which solve ( 1 ) in polynomial - time . Nonetheless , these methods diﬀer substantially in their representation requirement as well as in their practical performance . For example , a membership oracle suﬃces for the ellipsoid method and the random walk method , while a special barrier function for K is required to implement an IPM . The latter is by far the most successful algorithm for conic programming in practice : for example , applications of SDP range over several ﬁelds including optimal control , eigenvalue optimization , combinatorial optimization and many others , see [ 15 ] . In the case when X = IR n and K = IR m + , we recover the original setting of a homogeneous system of linear inequalities . Within this context , another alternative method is the perceptron algorithm [ 14 ] . It is well - known that this simple method terminates after a ﬁnite number of iterations which can be bounded by the square of the inverse of the width τ of the feasibility cone F . Although occasionally attractive from a practical point of view due to its simplicity , the perceptron algorithm is not considered theoretically eﬃcient since the width τ can be exponentially small in the size of the instance in the bit - length model . Dunagan and Vempala ( [ 4 ] ) combined the perceptron algorithm with a sequence of re - scalings constructed from near - feasible solutions . These re - scalings gradually increase τ on average and the resulting re - scaled perceptron algorithm has complexity O ( n ln ( 1 / τ ) ) iterations ( with high probability ) , which is theoretically 1 2 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS eﬃcient . Herein we extend the re - scaled perceptron algorithm proposed in [ 4 ] to the conic setting of ( 1 ) . Al - though the probabilistic analysis is similar , this is not the case for the remainder of the analysis . In particular , we show that the improvement obtained in [ 4 ] arises from a clever use of a deep - separation oracle , which is stronger than the usual separation oracle used in the classical perceptron algorithm . In the case of a system of linear inequalities studied in [ 4 ] , there is no diﬀerence between the implementation of both oracles . However , this diﬀerence is signiﬁcant for more general cones . We investigate in detail ways to construct a deep - separation oracle for several classes of cones , since it is the driving force of the re - scaled perceptron algorithm . We establish important properties of the deep - separation oracle and its implementation for several classes ( including the case when K is the cross - product of second - order cones ) . Based on these properties , we propose a scheme for general convex cones which exploits the particular structure of the deep - separation oracle . This scheme yields a deep - separation oracle in polynomial - time and requires only a deep - separation oracle for the dual cone of K ( which is readily available for many cones of interest such as the cone of positive semi - deﬁnite matrices ) , and includes the case when K is the cross - product of positive semi - deﬁnite cones . We start in Section 2 with properties of convex cones , oracles , and the deﬁnition of a deep - separation oracle . Section 3 generalizes the classical perceptron algorithm to the conic setting , and Section 4 extends the re - scaled perceptron algorithm of [ 4 ] to the conic setting . Section 5 contains the probabilistic and complexity analysis of the re - scaled perceptron algorithm , which reviews some material from [ 4 ] for completeness . Section 6 is devoted to methods for constructing a deep - separation oracle for both speciﬁc and general cones . We conclude this section with an informal discussion of the main ideas and technical diﬃculties encountered in obtaining our results . The perceptron algorithm is a greedy procedure that updates the current proposed solution by using any violated inequality . The number of iterations is ﬁnite but can be exponential . The modiﬁed percep - tron algorithm ( proposed in [ 3 ] , used in [ 4 ] ) is a similar updating procedure that only uses inequalities that are violated by at least some ﬁxed threshold . Although this procedure is not guaranteed to ﬁnd a feasible solution , it ﬁnds a near - feasible solution with the guarantee that no constraint is violated by more than the threshold and the number of steps to convergence is proportional to the inverse square of the threshold , independent of the conditioning of the initial system . The key idea in [ 4 ] is that such a near - feasible solution can be used to improve the width of the original system by a multiplicative factor . As we show in this paper , this analysis extends naturally to the full generality of conic systems . The main diﬃculty is in identifying a constraint that is violated by more than a ﬁxed threshold by the current proposed solution , precisely what we call a deep - separation oracle . This is not an issue in the linear setting ( one simply checks each constraint ) . For conic systems , the deep - separation itself is a conic feasibility problem ! It has the form : ﬁnd w ∈ K ∗ , the dual of the original inclusion cone , such that w satisﬁes a single second - order conic constraint . Our idea is to apply the re - scaled percepron algorithm to this system which is considerably simpler than F . What we can prove is that provided K ∗ has a deep - separation oracle , the method is theoretically eﬃcient . For many interesting inclusion cones , including the cone of positive semi - deﬁnite matrices , such a deep - separation oracle is readily available . 2 . Preliminaries 2 . 1 Notation For simplicity we conﬁne our analysis to ﬁnite dimensional Euclidean spaces . Let X and Y denote Euclidean spaces with ﬁnite dimension n and m , respectively . Denote by k·k their Euclidean norms , and h· , ·i their Euclidean inner products . For ¯ x ∈ X , B ( ¯ x , r ) will denote the ball centered at ¯ x with radius r , and analogously for Y . Let A : X → Y denote a linear operator , and A ∗ : Y → X denote the adjoint operator associated with A . 2 . 2 Convex Cones Let C be a convex cone . The dual cone of C is deﬁned as C ∗ = { d : h x , d i ≥ 0 , for all x ∈ C } ( 2 ) and ext C denote the set of extreme rays of C . A cone is pointed if it contains no lines . We say that C is a regular cone if C is a pointed closed convex cone with non - empty interior . It is elementary to show that C is regular if and only if C ∗ is regular . Given a regular convex cone C , we use the following geometric ( condition ) measure : Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 3 Definition 2 . 1 If C is a regular cone in X , the width of C is given by τ C , max x , r (cid:26) r k x k : B ( x , r ) ⊂ C (cid:27) . Furthermore the center of C is any vector ¯ z that attains the above maximum , normalized so that k ¯ z k = 1 . We will be particularly interested in the following three classes of cones : the non - negative orthant IR m + : = { x ∈ IR m : x ≥ 0 } , the second order cone denoted by Q n : = { x ∈ IR n : k ( x 1 , x 2 , . . . , x n − 1 ) k ≤ x n } , and the cone of positive semi - deﬁnite matrices S k × k + : = { X ∈ S k × k : h v , Xv i ≥ 0 for all v ∈ IR k } where S k × k : = { X ∈ IR k × k : X = X T } . These three cones are self - dual and their widths are 1 / √ m , 1 / √ 2 , and 1 / √ k , respectively . The following characterization will be used in our analysis . Lemma 2 . 1 Let G = { x : Mx ∈ C } and Let T = { M ∗ λ : λ ∈ C ∗ } . Then G ∗ = cl ( T ) . Proof . ( ⊆ ) Let λ ∈ C ∗ . Then for every x satisfying Mx ∈ C , h x , A ∗ λ i = h Ax , λ i ≥ 0 , since Mx ∈ C and λ ∈ C ∗ . Thus , cl ( T ) ⊆ G ∗ since G ∗ is closed . ( ⊇ ) Assume that there exists y ∈ G ∗ \ cl ( T ) . Thus there exists h 6 = 0 satisfying h h , y i < 0 and h h , w i ≥ 0 for all w ∈ cl ( T ) . Notice that h h , M ∗ λ i ≥ 0 for all λ ∈ C ∗ , which implies that Mh ∈ C and so h ∈ G . On the other hand , since y ∈ G ∗ , it follows that h h , y i ≥ 0 , contradicting h h , y i < 0 . (cid:3) The question of sets of the form T being closed has been recently studied by Pataki [ 11 ] . Necessary and suﬃcient conditions for T to be a closed set are given in [ 11 ] when C ∗ belongs to a class called “nice cones , ” a class which includes polyhedra and self - scaled cones . Nonetheless , the set T may fail to be closed even in simple cases , as the following example shows . Example 2 . 1 Let C ∗ = Q 3 = { ( λ 1 , λ 2 , λ 3 ) | k ( λ 1 , λ 2 ) k ≤ λ 3 } and M =   − 1 0 0 1 1 0   . In this case , T = { M ∗ λ | λ ∈ C ∗ } = { ( − λ 1 + λ 3 , λ 2 ) | k ( λ 1 , λ 2 ) k ≤ λ 3 } . It is easy to verify that ( 0 , 1 ) / ∈ T but ( ε , 1 ) ∈ T for every ε > 0 ( set λ 1 = 12 ε − ε 2 , λ 2 = 1 , and λ 3 = 12 ε + ε 2 ) , which shows that T is not closed . The following property of convex cones are well - known , but is presented and proved herein both for completeness as well as for conformity to our notation . Lemma 2 . 2 B ( z , r ) ⊆ C if and only if h d , z i ≥ r k d k for all d ∈ C ∗ . Proof . Suppose B ( z , r ) ⊂ C . Let d ∈ C ∗ . Then , z − r d k d k ∈ C and since d ∈ C ∗ , D d , z − r d k d k E ≥ 0 . Thus , h d , z i ≥ r h d , d i k d k = r k d k . Conversely , suppose h d , z i ≥ r k d k for every d ∈ C ∗ . Let v satisfy k v k ≤ r . Assume z + v / ∈ C , then there exists d ∈ C ∗ , h d , z + v i < 0 . Therefore h d , z i < − h d , v i ≤ r k d k , which contradicts h d , z i ≥ r k d k . (cid:3) 2 . 3 Oracles In our algorithms and analysis we will distinguish two diﬀerent types of oracles . Definition 2 . 2 An interior separation oracle for a convex set S ⊂ IR n is a subroutine that given a point x ∈ IR n , identiﬁes if x ∈ int S or returns a vector d ∈ IR n , k d k = 1 , such that h d , x i ≤ h d , y i for all y ∈ S . Definition 2 . 3 For a ﬁxed positive scalar t , a deep - separation oracle for a cone C ⊂ IR n is a subroutine that given a non - zero point x ∈ IR n , either ( I ) correctly identiﬁes that h d , x i k d kk x k ≥ − t for all d ∈ ext C ∗ or ( II ) returns a vector d ∈ C ∗ , k d k = 1 satisfying h d , x i k d kk x k ≤ − t . 4 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS Deﬁnition 2 . 2 is standard in the literature , whereas Deﬁnition 2 . 3 is new as far as we know . Our motivation for this deﬁnition arises from a relaxation of the orthogonality characterization of a convex cone . For d , x 6 = 0 let cos ( d , x ) denote the cosine of the angle between d and x , i . e . , cos ( d , x ) = h d , x i k d kk x k . Notice that x ∈ C if and only if cos ( d , x ) ≥ 0 for all d ∈ C ∗ if and only if cos ( d , x ) ≥ 0 for all d ∈ ext C ∗ . The latter characterization states that h d , x i k d kk x k ≥ 0 for all d ∈ ext C ∗ . Condition ( I ) of the deep - separation oracle relaxes the cosine condition from 0 to − t . The following example illustrates that the perceptron improvement algorithm described in [ 4 ] corresponds to a deep - separation oracle for a linear inequality system . Example 2 . 2 Let C = { x ∈ IR n : Mx ≥ 0 } where M is an m × n matrix none of whose rows are zero . Notice that C ∗ = { M ∗ λ : λ ≥ 0 } is the conic hull of the rows of M , and the extreme rays of C ∗ are a subset of the rows of M . Therefore a deep - separation oracle for C can be constructed by identifying for a given x 6 = 0 if there is an index i ∈ { 1 , . . . , m } for which h M i , x i k M i kk x k ≤ − t and returning M i / k M i k in such a case . Notice that we do not need to know which vectors M i are extreme rays of C ∗ ; if m is not excessively large it is suﬃcient to simply check the aforementioned inequality for every row index i . Remark 2 . 1 It might seem odd that condition ( I ) involves “only” the extreme rays of C ∗ . However , in many particular conic structures arising in practice , a super - set of the extreme rays of the dual cone C ∗ is at least partially accessible , as is the case when C = { x : Mx ≥ 0 } where this super - set is comprised of the row vectors of M . Indeed , suppose we replace condition ( I ) by the seemingly more convenient condition “ h d , x i k d kk x k ≥ − t for all d ∈ C ∗ . ” Utilizing Lemma 2 . 1 , this condition is met by checking − t ≤ min λ { h M ∗ λ , x / k x ki : k M ∗ λ k ≤ 1 , λ ≥ 0 } , and taking a dual yields − t ≤ max w { −k w − x / k x kk : Mw ≥ 0 } . We see that this latter optimization problem simply tests if x / k x k is at most distance t from the cone C , which itself is at least as hard as computing a non - trivial point in C . Remark 2 . 2 It turns out that conditions ( I ) and ( II ) might each be strictly satisﬁable . Let C = { x : Mx ≥ 0 } where M =   − 2 3 3 − 2 0 1   . Then C has an interior solution , and let t = 3 / 4 . It is straight - forward to check that x = ( − 1 , − 1 ) satisﬁes h M i , x i k M i kk x k > − t for every i , whereby condition ( I ) is satisﬁed strictly . Furthermore , ¯ d = ( 1 , 1 ) ∈ C ∗ and satisﬁes h ¯ d , x i k ¯ d kk x k < − t , thus showing that condition ( II ) is also satisﬁed strictly . Of course ¯ d / ∈ ext C ∗ , thus highlighting the importance of the role of extreme rays . 3 . Perceptron Algorithm for a Conic System The classical perception algorithm was proposed to solve a homogeneous system of linear inequalities ( 1 ) with K = IR m + . It is well - known that the algorithm has ﬁnite termination in at most (cid:4) 1 / τ 2 F (cid:5) iterations , see Rosenblatt 1962 [ 14 ] . This complexity bound can be exponential in the bit - model . Our starting point herein is to show that the classical perceptron algorithm can be easily extended to the case of a conic system of the form ( 1 ) . Perceptron Algorithm for a Conic System ( a ) Let x be the origin in X . Repeat : ( b ) If Ax ∈ int K , Stop . Otherwise , call interior separation oracle for F at x , returning d ∈ F ∗ , k d k = 1 , such that h d , x i ≤ 0 , and set x ← x + d . This algorithm presupposes the availability of a separation oracle for the feasibility cone F . In the typical case when the inclusion cone K has an interior separation oracle , this oracle can be used to construct an interior separation oracle for F : if x / ∈ int F , then Ax / ∈ int K and there exists λ ∈ K ∗ satisfying h λ , Ax i ≤ 0 , whereby d = A ∗ λ / k A ∗ λ k satisﬁes h d , x i ≤ 0 , d ∈ F ∗ , and k d k = 1 . Exactly as in the case of linear inequalities , we have Lemma 3 . 1 The perceptron algorithm for a conic system will compute a solution of ( 1 ) in at most (cid:4) 1 / τ 2 F (cid:5) iterations . Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 5 Proof . Consider the potential function π ( x ) = h x , ¯ z i / k x k , and note that π ( x ) ≤ 1 for all x 6 = 0 , where τ F is the width of the feasibility cone F and ¯ z is the center of F . If the algorithm does not stop at ( b ) , we update x to x + d , whereby h x + d , ¯ z i = h x , ¯ z i + h d , ¯ z i ≥ h x , ¯ z i + τ F , and k x + d k 2 = h x , x i + 2 h x , d i + h d , d i ≤ h x , x i + 1 , since h x , d i ≤ 0 , h d , d i = 1 , and h d , ¯ z i ≥ τ F from Lemma 2 . 2 . After k iterations , the potential function is at least kτ F / √ k . After more than (cid:4) 1 / τ 2 F (cid:5) iterations , the potential function would be greater than one , a contradiction . Thus , the algorithm must terminate after at most (cid:4) 1 / τ 2 F (cid:5) iterations , having computed a solution of ( 1 ) . (cid:3) Example 3 . 1 Consider the semideﬁnite cone K = S k × k + and the linear operator A : IR n → S k × k . Suppose that Ax / ∈ int K . In order to compute a direction d ∈ F ∗ , we start by computing any eigenvector v of the symmetric matrix Ax associated with a non - positive eigenvalue . Then the vector d = A ∗ ( vv T ) will satisfy h d , x i = (cid:10) A ∗ ( vv T ) , x (cid:11) = (cid:10) vv T , Ax (cid:11) = tr ( vv T Ax ) = v T ( Ax ) v ≤ 0 , and for all y ∈ F we have : h d , y i = (cid:10) vv T , Ay (cid:11) = v T ( Ay ) v ≥ 0 , i . e . , d ∈ F ∗ , and h d , x i ≤ 0 . If ( 1 ) has a solution it easily follows that d 6 = 0 whereby d / k d k can be used in ( b ) of the perceptron algorithm for a conic system . 4 . Re - scaled Conic Perceptron Algorithm In this section we construct a version of the per - ceptron algorithm whose complexity depends only logarithmically on 1 / τ F . To accomplish this we will systematically re - scale the system ( 1 ) using a linear transformation related to a suitably constructed random vector that approximates the center ¯ z of F . The linear transformation we use was ﬁrst proposed in [ 4 ] for the case of linear inequality systems ( i . e . , K = IR m + ) . Herein we extend these ideas to the conic setting . Table 1 contains a description of our algorithm , which is a structural extension of the algorithm in [ 4 ] . Note that the perceptron improvement phase requires a deep - separation oracle for F instead of the interior separation oracle for F as required by the perceptron algorithm . For the remainder of this section we presuppose that a deep - separation for F is indeed available . In Section 6 we will show that for most standard cones K a deep - separation oracle for F can be eﬃciently constructed . We now present our analysis of the re - scaled perceptron algorithm . The following lemma quantiﬁes the impact of the re - scaling ( Step 6 ) on the width of the feasibility cone F . Lemma 4 . 1 Let ¯ z denote the center of the feasibility cone F , normalized so that k ¯ z k = 1 . Let A , ˆ A denote the linear operators and τ F , τ ˆ F denote the widths of the feasibility cones F , ˆ F of two consecutive iterations of the re - scaled perception algorithm . Then τ ˆ F ≥ ( 1 − σ ) √ 1 + 3 σ 2 k ˆ z k τ F where ˆ z = ¯ z + 12 (cid:16) τ F − D x k x k , ¯ z E(cid:17) x k x k , and x is the output of the perceptron improvement phase . Proof . At the end of the perception improvement phase , we have a vector x satisfying h d , x i k d kk x k ≥ − σ for all d ∈ ext F ∗ . Let ¯ x = x / k x k . Then h d , ¯ x i ≥ − σ k d k for all d ∈ ext F ∗ . From Lemma 2 . 2 , it holds that h d , ¯ z i k d kk ¯ z k = h d , ¯ z i k d k ≥ τ F for all d ∈ F ∗ , i . e . h d , ¯ z i ≥ τ F k d k for all d ∈ F ∗ . 6 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS Re - scaled Perceptron Algorithm for a Conic System Step 1 Initialization . Set B = I and σ = 1 / ( 32 n ) . Step 2 Perceptron Algorithm for a Conic System . ( a ) Let x be the origin in X . Repeat at most (cid:4) ( 1 / σ 2 ) (cid:5) times : ( b ) If Ax ∈ int K , Stop . Otherwise , call interior separation oracle for F at x , returning d ∈ F ∗ , k d k = 1 , such that h d , x i ≤ 0 , and set x ← x + d . Step 3 Stopping Criteria . If Ax ∈ int K then output Bx and Stop . Step 4 Perceptron Improvement Phase . ( a ) Let x be a random unit vector in X . Repeat at most (cid:4) ( 1 / σ 2 ) ln ( n ) (cid:5) times : ( b ) Call deep - separation oracle for F at x with t = σ . If h d , x i ≥ − σ k d kk x k for all d ∈ ext F ∗ ( condition I ) , End Step 4 . Otherwise , oracle returns d ∈ F ∗ , k d k = 1 , such that h d , x i ≤ − σ k d kk x k ( condition II ) , and set x ← x − h d , x i d . If x = 0 restart at ( a ) . ( c ) Call deep - separation oracle for F at x with t = σ . If oracle returns condition ( II ) , restart at ( a ) . Step 5 Stopping Criteria . If Ax ∈ int K then output Bx and Stop . Step 6 Re - scaling . A ← A ◦ (cid:18) I + xx T h x , x i (cid:19) , B ← B ◦ (cid:18) I + xx T h x , x i (cid:19) , and Goto Step 2 . Table 1 : One iteration of the re - scaled perceptron algorithm is one pass of Steps 2 - 6 . From Lemma 2 . 1 it therefore holds that h λ , A ¯ z i = h A ∗ λ , ¯ z i ≥ τ F k A ∗ λ k for all λ ∈ K ∗ . Note that ˆ z = ¯ z + 12 ( τ F − h ¯ x , ¯ z i ) ¯ x , and let ˆ τ : = ( 1 − σ ) √ 1 + 3 σ 2 τ F . We want to show that h v , ˆ z i ≥ ˆ τ k v k for all v ∈ ext F ∗ . ( 3 ) If ( 3 ) is true , then by convexity of the function f ( v ) = ˆ τ k v k − h v , ˆ z i it will also be true that h v , ˆ z i ≥ ˆ τ k v k for any v ∈ F ∗ . Then from Lemma 2 . 2 it would follow that B ( ˆ z , ˆ τ ) ⊂ F , whereby τ ˆ F ≥ ˆ τ k ˆ z k as desired . Let v be an extreme ray of F ∗ . Using Lemma 2 . 1 , there exist a sequence { λ i } i ≥ 1 , λ i ∈ K ∗ , A ∗ λ i → v as i → ∞ . Since ( 3 ) is trivially true for v = 0 , we can assume that v 6 = 0 and hence A ∗ λ i 6 = 0 for i large enough . Next note that k ˆ A ∗ λ i k 2 = k A ∗ λ i k 2 + 2 (cid:10) A ∗ λ i , ¯ x (cid:11) 2 + h ¯ x , ¯ x i (cid:10) A ∗ λ i , ¯ x (cid:11) 2 = k A ∗ λ i k 2   1 + 3 (cid:10) A ∗ λ i , ¯ x (cid:11) k A ∗ λ i k ! 2   and D ˆ A ∗ λ i , ˆ z E = (cid:10) A ∗ λ i , ˆ z (cid:11) + h ¯ x , ˆ z i (cid:10) A ∗ λ i , ¯ x (cid:11) = (cid:10) A ∗ λ i , ¯ z (cid:11) + ( τ F − h ¯ x , ¯ z i ) (cid:10) A ∗ λ i , ¯ x (cid:11) + h ¯ x , ¯ z i (cid:10) A ∗ λ i , ¯ x (cid:11) ≥ τ F k A ∗ λ i k + τ F (cid:10) A ∗ λ i , ¯ x (cid:11) = τ F 1 + (cid:10) A ∗ λ i , ¯ x (cid:11) k A ∗ λ i k ! k A ∗ λ i k . ( 4 ) Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 7 Therefore D ˆ A ∗ λ i , ˆ z E k ˆ A ∗ λ i k ≥ τ F 1 + t i p 1 + 3 t 2 i where t i = h A ∗ λ i , ¯ x i k A ∗ λ i k . Note that t i ≤ 1 and h v , ¯ x i ≥ − σ k v k since v ∈ ext F ∗ , and so h v , ¯ x i k v k ≥ − σ . By continuity , for any ε > 0 it holds that t i ≥ − σ − ε for i suﬃciently large . Thus , t i ∈ [ − σ − ε , 1 ] for i large enough . For t ∈ [ 0 , 1 ] , we have 1 + t √ 1 + 3 t 2 ≥ 1 + t √ 1 + 2 t + t 2 = 1 , and for t ∈ [ − σ − ε , 0 ] , the function g ( t ) = 1 + t √ 1 + 3 t 2 ≥ 1 − σ − ε √ 1 + 3 ( σ + ε ) 2 since dg ( t ) dt = 1 − 3 t ( 1 + 3 t 2 ) 3 / 2 ≥ 0 for t ∈ [ − σ − ε , 0 ] , that is , g ( t ) is increasing on [ − σ − ε , 0 ] . Therefore , for i large enough we have D ˆ Aλ i , ˆ z E k ˆ A ∗ λ i k ≥ τ F ( 1 − σ − ε ) p 1 + 3 ( σ + ε ) 2 . Passing to the limit as λ i → v obtain h v , ˆ z i k v k ≥ τ F ( 1 − σ − ε ) p 1 + 3 ( σ + ε ) 2 whereby h v , ˆ z i k v k ≥ τ F ( 1 − σ ) √ 1 + 3 σ 2 = ˆ τ . (cid:3) 5 . Probabilistic Analysis . As mentioned before , the probabilistic analysis of our conic framework is similar to the analysis with linear inequalities in [ 4 ] . Although a few changes are required , all the main ideas are still valid . For the sake of completeness , we go over some results of [ 4 ] . Our exposition intentionally separates the probabilistic analysis from the remaining sections . The ﬁrst lemma of this section was established in [ 3 ] for the case of linear inequalities , and here is generalized to the conic framework . Roughly speaking , it shows that the perceptron improvement phase generates near - feasible solutions if started at a good initial point , which happens with at least a ﬁxed probability p = 1 / 8 . Lemma 5 . 1 Let z be a feasible solution of ( 1 ) of unit norm . With probability at least 18 , the perception improvement phase returns a vector x satisfying : ( i ) h d , x i ≥ − σ k x k for every d ∈ ext F ∗ , k d k = 1 , and ( ii ) h z , x / k x ki ≥ 1 √ n . Proof . Let x 0 be the random unit vector in IR n that is the starting value of the perceptron im - provement phase . Then with probability at least 1 / 8 we have (cid:10) z , x 0 (cid:11) ≥ 1 / √ n , see [ 4 ] . Notice that in the perceptron improvement phase we have h x − h d , x i d , z i = h x , z i − h d , x i h d , z i ≥ h x , z i since h d , x i ≤ 0 and h d , z i ≥ 0 ( since d ∈ F ∗ and z ∈ F ) . Thus , the inner product in h z , x i does not decrease at each inner iteration of the perceptron improvement phase ( Step 4 ) . Also , in each inner iteration of the perceptron improvement phase the norm of x decreases by at least a constant factor : h x − h x , d i d , x − h x , d i d i = h x , x i − 2 h d , x i 2 + h d , x i 2 h d , d i = h x , x i − h d , x i 2 = h x , x i − h d , x / k x ki 2 h x , x i ≤ h x , x i ( 1 − σ 2 ) , since h d , x / k x ki ≤ − σ < 0 and k d k = 1 . 8 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS Thus , after more than (cid:4) ( 1 / σ 2 ) ln ( n ) (cid:5) iterations , we would have h x , z i k x k > 1 , which is a contradiction since z is a unit vector . Therefore we terminate with a vector x satisfying ( i ) and ( ii ) with probability at least 1 / 8 . (cid:3) Lemma 5 . 1 establishes that points obtained after the perceptron improvement phase are near - feasible for the current conic system . The next lemma clariﬁes the implications of using these near - feasible points to re - scale the conic system . Lemma 5 . 2 Suppose that n ≥ 2 , τ F , σ ≤ 1 / 32 n and A is the linear operator of the current iteration . Let ˆ A be the linear operator obtained after one iteration of the perceptron improvement phase . Let τ ˆ F denote the width of the cone of feasible solutions ˆ F of the updated conic system associated with ˆ A . Then ( i ) τ ˆ F ≥ (cid:18) 1 − 1 32 n − 1 512 n 2 (cid:19) τ F ; ( ii ) With probability at least 18 , τ ˆ F ≥ (cid:18) 1 + 1 3 . 02 n (cid:19) τ F . Proof . Let x be the output of the perceptron improvement phase . For simplicity , let τ : = τ F , ˆ τ : = τ ˆ F , and ¯ x = x / k x k . Using Lemma 4 . 1 , we have ˆ τ ≥ ( 1 − σ ) √ 1 + 3 σ 2 k ˆ z k τ where ˆ z = ¯ z + 12 ( τ − h ¯ x , ¯ z i ) ¯ x . Next note that k ˆ z k 2 = 1 + ( τ − h ¯ x , ¯ z i ) + 1 4 ( τ − h ¯ x , ¯ z i ) 2 = 1 + τ 2 4 + h ¯ z , ¯ x i (cid:18) τ 2 − 3 4 h ¯ z , ¯ x i (cid:19) . Following [ 4 ] , consider two cases . First assume that | h ¯ z , ¯ x i | < 1 / √ n which happens with probability at most 7 / 8 . Then viewing the above as a quadratic function in h ¯ z , ¯ x i which is maximized when h ¯ z , ¯ x i = τ / 3 , we obtain k ˆ z k 2 ≤ 1 + τ 2 4 + τ 2 12 = 1 + τ 2 3 . Thus , we have ˆ τ ≥ τ ( 1 − σ ) (cid:18) 1 − 3 σ 2 2 (cid:19) (cid:18) 1 − τ 2 6 (cid:19) ≥ τ (cid:18) 1 − 1 32 n − 1 512 n 2 (cid:19) , since τ and σ are less or equal to 132 n , and 1 √ 1 + t ≥ 1 − t 2 . The second case assumes that | h ¯ z , ¯ x i | ≥ 1 / √ n , which happens with probability at least 1 / 8 . In this case , the quadratic function in h ¯ z , ¯ x i will be maximized at h ¯ z , ¯ x i = 1 √ n which yields k ˆ z k 2 ≤ 1 − 3 4 n + τ 2 √ n + τ 2 4 . Again using 1 √ 1 + t ≥ 1 − t 2 , we obtain ˆ τ ≥ τ ( 1 − σ ) (cid:18) 1 − 3 σ 2 2 (cid:19) (cid:18) 1 + 3 8 n − τ 4 √ n − τ 2 8 (cid:19) ≥ τ (cid:18) 1 + 1 3 . 02 n (cid:19) . (cid:3) The following theorem bounds the number of overall iterations and the number of oracle calls made by the algorithm . Theorem 5 . 1 Suppose that n ≥ 2 . If ( 1 ) has a solution , the re - scaled perceptron algorithm will compute a solution in at most T = max (cid:26) 4096 ln (cid:18) 1 δ (cid:19) , 139 n ln (cid:18) 1 32 nτ F (cid:19)(cid:27) = O (cid:18) n ln (cid:18) 1 τ F (cid:19) + ln (cid:18) 1 δ (cid:19)(cid:19) iterations , with probability at least 1 − δ . Moreover , the algorithm makes at most O ( T n 2 ln ( n ) ) calls of a deep - separation oracle for F and at most O ( T n 2 ) calls of a separation oracle for F with probability at least 1 − δ . Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 9 Proof . Our proof is slightly diﬀerent than that of Theorem 3 . 4 in [ 4 ] . Let T denote the number of times that the re - scaled perceptron algorithm calls Step 4 ( a ) , and let i index these calls . After each visit to Step 4 ( a ) exactly one of three cases can occur : ( i ) the algorithm ends Step 4 in Step 4 ( b ) with the resulting update in Step 6 satisfying conclusion ( ii ) of Lemma 5 . 2 , ( ii ) the algorithm ends Step 4 with the resulting update in Step 6 not satisfying the conclusion ( ii ) of Lemma 5 . 2 , or ( iii ) the algorithm does not end Step 4 and therefore restarts Step 4 ( a ) . Let V i be the binary random variable whose value is 1 if the perceptron improvement phase ends as in case ( i ) and is 0 otherwise , and let V = P Ti = 1 V i . Letting τ i denote the width of the feasibility cone after i calls to Step 4 ( a ) , we see from Lemma 5 . 2 that V i = 1 implies τ i + 1 ≥ τ i ( 1 + 1 / ( 3 . 02 n ) ) . Furthermore , Lemma 5 . 2 implies that P ( V i = 1 ) ≥ 1 / 8 whereby E [ V ] ≥ T / 8 . The Chernoﬀ bound yields P ( V < ( 1 − (cid:15) ) E [ V ] ) ≤ e − (cid:15) 2 E [ V ] / 2 ≤ e − (cid:15) 2 V / 16 . In order to bound this probability by δ and setting (cid:15) = 1 / 16 , we need T ≥ 4096 ln ( 1 / δ ) . Next note that V i = 0 if either case ( ii ) or case ( iii ) above occur , the former yielding τ i + 1 ≥ τ i (cid:0) 1 − 132 n − 1 512 n 2 (cid:1) from Lemma 5 . 2 , and the latter yielding τ i + 1 = τ i ( i . e . , no update is performed ) . Thus , with probability at least 1 − δ , using Lemma 5 . 2 , we have τ T ≥ τ 0 (cid:0) 1 + 1 3 . 02 n (cid:1) V (cid:0) 1 − 132 n − 1 512 n 2 (cid:1) T − V ≥ τ 0 (cid:0) 1 + 1 3 . 02 n (cid:1) T ( 1 − (cid:15) ) / 8 (cid:0) 1 − 1 32 n − 1 512 n 2 (cid:1) T − T ( 1 − (cid:15) ) / 8 ≥ τ 0 (cid:0) 1 + 1 3 . 02 n (cid:1) 15 T 128 (cid:0) 1 − 132 n − 1 512 n 2 (cid:1) 113 T 128 ≥ τ 0 e T / 139 n . Setting T ≥ 139 n ln ( 1 / ( 32 nτ 0 ) ) we obtain τ T ≥ 1 / ( 32 n ) . Therefore it suﬃces for the algorithm to visit Step 4 ( a ) at most T = max n 4096 ln (cid:0) 1 δ (cid:1) , 139 n ln (cid:16) 1 32 nτ F (cid:17)o times to ensure that the algorithm succeeds with probability at least 1 − δ . Also , the number of iterations of the re - scaled perceptron algorithm , i . e . , the number of calls to Step 2 , is at most T , the number of calls to the separation oracle for F is at most b 1024 n 2 T c , and the number of calls to the deep - separation oracle for F is at most b 1024 n 2 ln ( n ) T c . (cid:3) Remark 5 . 1 It is instructive to compare the complexity bound in Theorem 5 . 1 with that of the ellipsoid method ( see [ 7 ] ) . Let W s and W d denote the number of operations needed for an oracle call to an interior separation oracle and a deep - separation oracle , respectively , for the feasibility cone F . The complexity of the ellipsoid method for computing a solution of ( 1 ) is O ( n 2 ln ( 1 / τ F ) ) iterations , with each iteration requiring ( i ) one call to an interior separation oracle for F , and ( ii ) O ( n 2 ) additional operations , yielding a total operation count of O ( ( n 4 + n 2 W s ) ln ( 1 / τ F ) ) . The corresponding complexity bound for the re - scaled perceptron algorithm is O ( n ln ( 1 / τ F ) + ln ( 1 / δ ) ) iterations , where each iteration requires ( i ) O ( n 2 ) calls to an interior separation oracle , ( ii ) O ( n 2 ln n ) calls to a deep - separation oracle , and O ( n 2 ) additional operations , yielding a total operation count of O ( ( n 2 W s + n 2 ln nW d + n 2 ) ( n ln ( 1 / τ F ) + ln ( 1 / δ ) ) ) . If we make the reasonable presumption that either δ is a ﬁxed scalar or τ F < < δ , and that W d ≥ W s , we see that the ellipsoid method has superior complexity by a factor of at least n ln n , with this advantage growing to the extent that W d > > W s ( as is the case when K is either composed of second - order or positive semi - deﬁnite cones , see Section 6 ) . However , the re - scaled perceptron algorithm is still attractive for at least two reasons . First , it has the possibility of acceleration beyond its worst - case bound . And second , we believe that the method is of independent interest for its ability to re - dilate the space in a way that improves the width of the feasibility cone . It may be possible to exploit the mechanisms underlying this phenomenon in other algorithms yet to be developed . In certain applications , it will useful to amend Deﬁnition 2 . 3 of the deep - separation oracle as follows : Definition 5 . 1 For a ﬁxed positive scalar σ , a half - deep - separation oracle for a cone C ⊂ IR n is a subroutine that given a non - zero point x ∈ IR n , either ( I ) correctly identiﬁes that h d , x i k d kk x k ≥ − σ for all d ∈ ext C ∗ or ( II ) returns a vector d ∈ C ∗ , k d k = 1 satisfying h d , x i k d kk x k ≤ − σ / 2 . 10 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS Remark 5 . 2 Deﬁnition 5 . 1 only diﬀers from Deﬁnition 2 . 3 in the inequality in condition ( II ) , where now σ / 2 is used instead of σ . This minor change only aﬀects the iteration bound in Step 4 of the re - scaled perceptron algorithm , which needs to be changed to (cid:4) ( 4 / σ 2 ) ln ( n ) (cid:5) ; all other analysis in this Section remains valid . 6 . Deep - separation Oracles for F , for Some Inclusion Cones K The re - scaled perceptron algorithm presupposes the availability of a deep - separation oracle for the feasibility cone F . Herein we show that such a deep - separation oracle is fairly easy to construct when ( 1 ) has the format :   A L x ∈ int IR m + A i x ∈ int Q n i i = 1 , . . . , q x s ∈ int S k × k + , ( 5 ) where x is composed as the cartesian product x = ( x s , x p ) . Note that ( 5 ) is an instance of ( 1 ) for K = IR m + × Q n 1 × · · · × Q n q × S k × k + and the only special structure on A is that the semi - deﬁnite inclusion is of the simple format “ Ix s ∈ S k × k + . ” In Section 6 . 4 we show how to construct a deep - separation oracle for problems like ( 5 ) that also include the more general semi - deﬁnite inclusion “ A s x ∈ S k × k + , ” but this construction is much less straightforward to develop . The starting point of our analysis is a simple observation about intersections of feasibility cones . Suppose we have available deep - separation oracles for each of the feasibility cones F 1 and F 2 of instances : (cid:26) A 1 x ∈ int K 1 x ∈ X and (cid:26) A 2 x ∈ int K 2 x ∈ X ( 6 ) and consider the problem of ﬁnding a point that simultaneously satisﬁes both conic inclusions :   A 1 x ∈ int K 1 A 2 x ∈ int K 2 x ∈ X . ( 7 ) Let F = { x : A 1 x ∈ K 1 , A 2 x ∈ K 2 } = { x : Ax ∈ K } where K = K 1 × K 2 and A is deﬁned analogously . Then F = F 1 ∩ F 2 where F i = { x : A i x ∈ K i } for i = 1 , 2 . It follows from the calculus of convex cones that F ∗ = F ∗ 1 + F ∗ 2 , and therefore ext F ∗ ⊂ ( ext F ∗ 1 ∪ ext F ∗ 2 ) . ( 8 ) This observation leads to an easy construction of a deep - separation oracle for F 1 ∩F 2 if one has available deep - separation oracles for F 1 and F 2 : Deep - separation Oracle for F 1 ∩ F 2 Given : scalar t > 0 and x 6 = 0 , call the deep - separation oracles for F 1 and F 2 at x . If both oracles report Condition I , return Condition I . Otherwise at least one oracle reports Condition II and provides d ∈ F ∗ i ⊂ F ∗ , k d k = 1 , such that h d , x i ≤ − t k d kk x k ; return d and Stop . Remark 6 . 1 If deep - separation oracles for F i are available and their eﬃciency is O ( T i ) operations for i = 1 , 2 , then the deep - separation oracle for F 1 ∩ F 2 given above is valid and its eﬃciency is O ( T 1 + T 2 ) operations . Utilizing Remark 6 . 1 , in order to construct a deep - separation oracle for the feasibility cone of ( 5 ) it will suﬃce to construct deep - separation oracles for each of the conic inclusions therein , which is what we now examine . 6 . 1 Deep - separation Oracle for F when K = IR m + We consider F = { x : Ax ∈ IR m + } . Example 2 . 2 has already described a deep - separation oracle for F when the inclusion cone is IR m + . It is easy to see that this oracle can be implemented in O ( mn ) operations . Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 11 6 . 2 Deep - separation Oracle for F when K = Q k For convenience we amend our notation so that F = { x : k Mx k ≤ g T x } for a given real ( k − 1 ) × n matrix M and a real n - vector g , so that F = { x : Ax ∈ Q k } where the linear operator A is speciﬁed by Ax : = (cid:20) Mx g T x (cid:21) . We will construct an eﬃcient half - deep - separation oracle ( Deﬁnition 5 . 1 ) by considering the following optimization problem : t ∗ : = min d d T x s . t . k d k = 1 d ∈ F ∗ . ( 9 ) If x ∈ F , then t ∗ ≥ 0 and clearly condition I of Deﬁnition 5 . 1 is satisﬁed . If x / ∈ F , then t ∗ < 0 and we can replace the equality constraint in ( 9 ) with an inequality constraint . We obtain the following primal / dual pair of convex problems with common optimal objective function value t ∗ : t ∗ : = min d x T d = max y −k y − x k s . t . k d k ≤ 1 s . t . y ∈ F d ∈ F ∗ ( 10 ) Now consider the following half - deep - separation oracle for F when K = Q k . Half - Deep - Separation Oracle for F when K = Q k , for x 6 = 0 and relaxation parameter σ > 0 If k Mx k ≤ g T x , return Condition I , and Stop . Solve ( 10 ) for feasible primal and dual solutions ¯ d , ¯ y with duality gap ¯ g satisfying ¯ g / k x k ≤ σ / 2 If x T ¯ d / k x k ≥ − σ / 2 , report Condition ( I ) , and Stop . If x T ¯ d / k x k ≤ − σ / 2 , then return d = ¯ d , report Condition ( II ) , and Stop . To see the validity of this method , note that if k Mx k ≤ g T x , then x ∈ F and clearly Condition ( I ) of Deﬁnition 5 . 1 is satisﬁed . Next , suppose that x T ¯ d / k x k ≥ − σ / 2 , then t ∗ ≥ −k ¯ y − x k = x T ¯ d − ¯ g ≥ −k x k σ / 2 − k x k σ / 2 = −k x k σ . Therefore x T d k x kk d k ≥ − σ for all d ∈ F ∗ , and it follows that Condition ( I ) of Deﬁnition 5 . 1 is satisﬁed . Finally , if x T ¯ d / k x k ≤ − σ / 2 , then ¯ d T x k ¯ d kk x k ≤ − σ / 2 and ¯ d ∈ F ∗ , whereby Condition ( II ) of Deﬁnition 5 . 1 is satisﬁed using ¯ d . The computational eﬃciency of this deep - separation oracle depends on the ability to eﬃciently solve ( 10 ) for feasible primal / dual solutions with duality gap ¯ g ≤ σ k x k / 2 . For the case when K = Q k , it is shown in [ 1 ] that ( 10 ) can be solved very eﬃciently to this desired duality gap , namely in O ( n 3 + n ln ln ( 1 / σ ) + n ln ln ( 1 / min { τ F , τ F ∗ } ) ) operations in practice , using a combination of Newton’s method and binary search . Using σ = 1 / ( 32 n ) this is O ( n 3 + n ln ln ( 1 / min { τ F , τ F ∗ } ) ) operations for the relaxation parameter σ needed by the re - scaled perceptron algorithm . 6 . 3 Deep - separation Oracle for S k × k + Let C = S k × k + , and for convenience we alter our notation herein so that X ∈ S k × k is a point under consideration . A deep - separation oracle for C at X 6 = 0 for the scalar t > 0 is constructed by simply checking the condition “ X + t k X k I (cid:23) 0 . ” If X + t k X k I (cid:23) 0 , then condition I of the deep - separation oracle is satisﬁed . This is true because the extreme rays of C are the collection of rank - 1 matrices vv T , and (cid:10) vv T , X (cid:11) k X kk vv T k = v T Xv k X kk vv T k ≥ − t k X k v T v k X kk vv T k = − t for any v 6 = 0 . On the other hand , if X + t k X k I 6(cid:31) 0 , then compute any nonzero v satisfying v T Xv + t k X k v T v ≤ 0 , and return D = vv T / v T v , which will satisfy h D , X i k X kk D k = v T Xv k X k v T v ≤ − t , 12 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS thus satisfying condition II . Notice that the work per oracle call is simply to check the eigenvalue condition X (cid:23) − t k X k I and possibly to compute an appropriate vector v , which is typically O ( k 3 ) operations in practice . 6 . 4 Methodology for a Deep - separation Oracle for F when K ∗ has a Deep - Separation Oracle In this subsection we present a general result on how to construct a deep - separation oracle for any feasibility cone F = { x ∈ IR n : Ax ∈ K } whose dual inclusion cone K ∗ has an eﬃciently - computable deep - separation oracle . We therefore formally deﬁne our working premise for this subsection as follows : Premise : K ∗ has an eﬃciently - computable deep - separation oracle . Furthermore , τ K and τ K ∗ are known . Remark 6 . 2 The results herein specify to the case when K = S k × k + . We know from the results in Section 6 . 3 and the self - duality of S k × k + ( ( S k × k + ) ∗ = S k × k + ) that K ∗ has an eﬃciently computable deep - separation oracle when K = S k × k + . Furthermore , we have τ K = τ K ∗ = 1 / √ k . The complexity analysis that we develop in this subsection uses the data - perturbation condition mea - sure model of Renegar [ 12 ] , which we now brieﬂy review . Considering ( 1 ) as a system with ﬁxed cone K and ﬁxed spaces X and Y , let M denote those operators A : X → Y for which ( 1 ) has a solution . For A ∈ M , let ρ ( A ) denote the “distance to infeasibility” for ( 1 ) , namely : ρ ( A ) : = min ∆ A { k ∆ A k : A + ∆ A / ∈ M } . Then ρ ( A ) denotes the smallest perturbation of our given operator A which would render the system ( 1 ) infeasible . Next let C ( A ) denote the condition measure of ( 1 ) , namely C ( A ) = k A k / ρ ( A ) , which is a scale - invariant reciprocal of the distance to infeasibility . ln ( C ( A ) ) is tied to the complexity of interior - point methods and the ellipsoid method for computing a solution of ( 1 ) , see [ 13 ] and [ 5 ] . Given a regular inclusion cone K , the feasibility cone for ( 1 ) is F = { x : Ax ∈ K } . Given the relaxation parameter t > 0 and a non - zero vector x ∈ IR n , consider the following conic feasibility system in the variable d : ( S t , x ) :   h x , d i k x kk d k < − t d ∈ F ∗ ( 11 ) It follows from Deﬁnition 2 . 3 that if d is feasible for ( S t , x ) , then Condition II of Deﬁnition 2 . 3 is satisﬁed ; however , if ( S t , x ) has no solution , then Condition I is satisﬁed . Utilizing Lemma 2 . 1 and rearranging terms yields the equivalent system in variables w : ( S t , x ) :   t k x kk A ∗ w k + h w , Ax i < 0 w ∈ int K ∗ ( 12 ) Note that if ˜ w solves ( 12 ) , then ˜ d = A ∗ ˜ w solves ( 11 ) from Lemma 2 . 1 . This leads to the following approach to constructing a deep - separation oracle for F : given x 6 = 0 and t : = σ , compute a solution ˜ w of ( 12 ) or certify that no solution exists . If ( 12 ) has no solution , report Condition I and Stop ; otherwise ( 12 ) has a solution ˜ w , return d : = A ∗ ˜ w / k A ∗ ˜ w k , report Condition II , and Stop . In order to implement this deep - separation oracle we need to be able to compute a solution ˜ w of ( 12 ) if such a solution exists , or be able to provide a certiﬁcate of infeasibility of ( 12 ) if no solution exists . Now notice that ( 12 ) is a homogeneous conic feasibility problem of the form ( 5 ) , as it is comprised of a single second - order cone inclusion constraint ( ( t k x k A ∗ w , h w , − Ax i ) ∈ Q n ) plus a constraint that the variable w must lie in K ∗ . Therefore , using Remark 6 . 1 and the premise that K ∗ has an eﬃciently - computable deep - separation oracle , it follows that ( 12 ) itself can be eﬃciently solved by the re - scaled perceptron algorithm , under the proviso that it has a solution . However , in the case when ( 12 ) has no solution , it will be necessary to develop a means to certify this infeasibility . To do so , we ﬁrst analyze its feasibility cone , denoted as ˜ F ( t , x ) : = { w : t k x kk A ∗ w k + h w , Ax i ≤ 0 , w ∈ K ∗ } . We have : Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS 13 Proposition 6 . 1 For a given σ ∈ ( 0 , 1 / 2 ) and x 6 = 0 , suppose that S ( σ , x ) has a solution and let t ∈ ( 0 , σ ) . Then τ ˜ F ( t , x ) ≥ τ K ∗ ( σ − t ) 3 C ( A ) . Proof . For simplicity we assume with no loss of generality that k x k = 1 and k A k = 1 . Since S ( σ , x ) has a solution , let ˆ w satisfy σ k A ∗ ˆ w k + h ˆ w , Ax i ≤ 0 , ˆ w ∈ K ∗ , and k ˆ w k = 1 . It follows directly from Theorem 2 of [ 6 ] that k A ∗ ˆ w k ≥ ρ ( A ) . Let w ◦ be the center of K ∗ , whereby B ( w ◦ , τ K ∗ ) ⊂ K ∗ . Consider the vector ˆ w + βw ◦ + αd where k d k ≤ 1 and β > 0 will be speciﬁed shortly . Then ˆ w + βw ◦ + αd ∈ K ∗ so long as α ≤ βτ K ∗ . Also , t k A ∗ ( ˆ w + βw ◦ + αd ) k + h ˆ w + βw ◦ + αd , Ax i ≤ t k A ∗ ˆ w k + βt + αt + h ˆ w , Ax i + β + α ≤ ( t − σ ) k A ∗ ˆ w k + βt + αt + β + α ≤ ( t − σ ) ρ ( A ) + βt + αt + β + α ≤ 0 so long as α ≤ ˆ α : = ( σ − t ) ρ ( A ) t + 1 − β . Therefore τ ˜ F ( t , x ) ≥ min n ( σ − t ) ρ ( A ) t + 1 − β , βτ K ∗ o k ˆ w + βw ◦ k ≥ min n ( σ − t ) ρ ( A ) t + 1 − β , βτ K ∗ o 1 + β . Let β : = ( σ − t ) ρ ( A ) 2 ( t + 1 ) and substituting in this last expression yields τ ˜ F ( t , x ) ≥ ( σ − t ) ρ ( A ) τ K ∗ 2 + 2 t + ( σ − t ) ρ ( A ) ≥ ( σ − t ) ρ ( A ) τ K ∗ 3 = ( σ − t ) τ K ∗ 3 C ( A ) since ρ ( A ) ≤ k A k = 1 and 0 < t ≤ σ ≤ 1 / 2 . (cid:3) Now consider the following half - deep - separation oracle for F ( recall Deﬁnition 5 . 1 ) which takes as input an estimate L of C ( A ) : Half - deep - separation Oracle for F , for x 6 = 0 , relaxation parameter σ , and estimate L Set t : = σ / 2 , and run the re - scaled perceptron algorithm to compute a solution ˜ w of ( 12 ) for at most ˆ T : = max n 4096 ln (cid:0) 1 δ (cid:1) , 139 n ln (cid:16) 6 L τ K ∗ (cid:17)o iterations . If a solution ˜ w of ( 12 ) is computed , return d : = A ∗ ˜ w / k A ∗ ˜ w k , report Condition II , and Stop . If no solution is computed within ˆ T iterations , report “either Condition I is satisﬁed , or L < C ( A ) , ” and Stop . We now validate that this constitutes a half - deep - separation oracle for F , with high probability . If the oracle computes a solution ˜ w of ( 12 ) , then it is trivial to show that d : = A ∗ ˜ w / k A ∗ ˜ w k satisﬁes d ∈ F ∗ and h d , x i k d kk x k ≤ − t = − σ / 2 , thus satisfying condition II of Deﬁnition 5 . 1 . Suppose instead that the oracle does not compute a solution within ˆ T iterations . It follows from Theorem 5 . 1 that with probability at least 1 − δ the re - scaled perceptron algorithm would compute a solution of ( 12 ) in at most T : = max ( 4096 ln (cid:18) 1 δ (cid:19) , 139 n ln 1 32 nτ ˜ F ( t , x ) ! ) iterations . However , if L ≥ C ( A ) and ˜ F ( σ , x ) 6 = ∅ , then it follows from Proposition 6 . 1 that 1 32 nτ ˜ F ( t , x ) ≤ 3 C ( A ) 32 nτ K ∗ ( σ / 2 ) ≤ 6 L τ K ∗ , whereby T ≤ ˆ T . Therefore , it follows that with probability at least 1 − δ that either L < C ( A ) or ˜ F ( σ , x ) = ∅ , the latter then implying condition I of Deﬁnition 5 . 1 . We note that the above - outlined method for constructing a deep - separation oracle is inelegant in many respects . Nevertheless , it is theoretically eﬃcient , i . e . , it is polynomial - time in n , ln ( 1 / τ K ∗ ) , ln ( L ) , and ln ( 1 / δ ) . It is an interesting and open question whether , in the case of K = S k × k + , a more straightforward and more eﬃcient deep - separation oracle for F can be constructed . 14 Belloni , Freund , and Vempala : Re - scaled Conic Perceptron Mathematics of Operations Research xx ( x ) , pp . xxx – xxx , c (cid:13) 200x INFORMS Finally , it follows from Theorem 7 of [ 6 ] that the width of F can be lower - bounded by Renegar’s condition measure : τ F ≥ τ K C ( A ) . ( 13 ) This can be used in combination with binary search ( for bounding C ( A ) ) and the half - deep - separation oracle above to produce a complexity bound for computing a solution of ( 1 ) in time polynomial in n , ln ( C ( A ) ) , ln ( 1 / δ ) , ln ( 1 / τ K ) , and ln ( 1 / τ K ∗ ) . References [ 1 ] Alexandre Belloni and Robert M . Freund , On the second - order feasibility cone : Primal - dual represen - tation and eﬃcient projection , Technical Report , MIT Operations Research Center , in preparation 2006 . [ 2 ] D . Bertsimas and S . Vempala , Solving convex programs by random walks , Journal of the ACM 51 ( 2004 ) , no . 4 , 540 – 556 . [ 3 ] A . Blum , A . Frieze , R . Kannan , and S . Vempala , A polynomial - time algorithm for learning noisy linear threashold functions , Algorithmica 22 ( 1998 ) , no . 1 , 35 – 52 . [ 4 ] J . Dunagan and S . Vempala , A simple polynomial - time rescaling algorithm for solving linear pro - grams , Proceedings of STOC’04 ( 2004 ) . [ 5 ] R . M . Freund and J . R . Vera , Condition - based complexity of convex optimization in conic linear form via the ellipsoid algorithm , SIAM Journal on Optimization 10 ( 1999 ) , no . 1 , 155 – 176 . [ 6 ] , Some characterizations and properties of the “distance to ill - posedness” and the condition measure of a conic linear system , Mathematical Programming 86 ( 1999 ) , no . 2 , 225 – 260 . [ 7 ] M . Gr¨otschel , L . Lov´asz , and A . Schrijver , Geometric algorithms and combiantorial optimization , second ed . , Springer - Verlag , Berlin , 1994 . [ 8 ] N . Karmarkar , A new polynomial - time algorithm for linear programming , Combinatorica 4 ( 1984 ) , no . 4 , 373 – 395 . [ 9 ] L . G . Khachiyan , A polynomial algorithm in linear programming , Soviet Math . Dokl . 20 ( 1979 ) , no . 1 , 191 – 194 . [ 10 ] Y . Nesterov and A Nemirovskii , Interior - point polynomial algorithms in convex programming , Society for Industrial and Applied Mathematics ( SIAM ) , Philadelphia , 1993 . [ 11 ] G . Pataki , On the closedness of the linear image of a closed convex cone , Technical Report , University of North Carolina , TR - 02 - 3 Department of Operations Research 1992 . [ 12 ] J . Renegar , Some perturbation theory for linear programming , Mathematical Programming 65 ( 1994 ) , no . 1 , 73 – 91 . [ 13 ] , Linear programming , complexity theory , and elementary functional analysis , Mathematical Programming 70 ( 1995 ) , no . 3 , 279 – 351 . [ 14 ] F . Rosenblatt , Principles of neurodynamics , Spartan Books , Washington , DC , 1962 . [ 15 ] H . Wolkowicz , R . Saigal , and L . Vandenberghe , Handbook of semideﬁnite programming , Kluwer Academic Publishers , 2000 .