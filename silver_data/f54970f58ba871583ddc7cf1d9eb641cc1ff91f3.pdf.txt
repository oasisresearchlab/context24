a r X i v : 2304 . 09175v1 [ c s . L G ] 17 A p r 2023 Memento : Facilitating Eﬀortless , Eﬃcient , and Reliable ML Experiments Zac Pullar - Strecker 1 , Xinglong Chang 1 , Liam Brydon 1 , Ioannis Ziogas 1 , 2 , Katharina Dost 1 , and Jörg Wicker 1 ( (cid:0) ) 1 University of Auckland , Auckland , New Zealand 2 University of Mississippi , Oxford , MS , USA { zpul156 , xcha011 , lbry121 , izio995 } @ aucklanduni . ac . nz { katharina . dost , j . wicker } @ auckland . ac . nz Abstract . Running complex sets of machine learning experiments is challenging and time - consuming due to the lack of a uniﬁed framework . This leaves researchers forced to spend time implementing necessary fea - tures such as parallelization , caching , and checkpointing themselves in - stead of focussing on their project . To simplify the process , in this paper , we introduce Memento , a Python package that is designed to aid re - searchers and data scientists in the eﬃcient management and execution of computationally intensive experiments . Memento has the capacity to streamline any experimental pipeline by providing a straightforward conﬁguration matrix and the ability to concurrently run experiments across multiple threads . A demonstration of Memento is available at : wickerlab . org / publication / memento . Keywords : Experimental pipeline · Parallel computing · Reliable ML 1 Introduction As machine learning ( ML ) and data science continue to shape decision - making across a wide range of domains [ 6 ] , researchers and practitioners are facing the ever - growing challenge of designing and executing complex experiments eﬃ - ciently and reliably [ 8 ] . This challenge is particularly acute in machine learning , where the traditional experimental pipeline involves a complex set of intermedi - ate steps , such as data pre - processing , model selection , hyperparameter tuning , and model evaluation . Benchmarking multiple options for each of these steps easily results in an overwhelming number of individual experiments , long wait - ing times when run sequentially , and the need to adjust scripts or restart entire sets of experiments when errors occur . Aiming to avoid these challenges in practice , automated machine learning ( AutoML ) tools can help choose the best combination of steps in the ML pipeline automatically [ 1 , 3 , 4 , 7 ] , but they may not be ﬂexible enough to integrate new algorithms or match multiple libraries as is necessary for research . In Python , scikit - learn’s pipeline functionality [ 5 ] allows users to conveniently choose and 2 Z . Pullar - Strecker et al . User specify config _ matrix deﬁne exp _ func start Memento collect results Memento Memento Runner Create Parameter Options Caching and Checkpointing Parallel Run 1 Parallel Run 2 Parallel Run n ··· Gather Metrics Create Results and Send Notiﬁcations Fig . 1 . The roles of the user and Memento when running a complex and large set of experiments with Memento . chain the steps involved in a single experiment but falls short in handling the setup of entire experiment sets . To date , there exists no uniﬁed framework for the parallelization of experi - mental setups beyond a narrow range of machine learning applications [ 2 ] . Most often , researchers have to manually implement workﬂow features , such as caching and checkpointing , which introduce an added layer of complexity . Moreover , re - medial corrections to potential errors occurring partway through the execution of a series of long experiments require tedious debugging . To ﬁll this gap , we present Memento ( Marvelous ExperiMENt TOol ) , a sim - ple and ﬂexible Python library designed to help researchers , data analysts , and machine learning practitioners streamline the experimental pipeline , parallelize experiments across threads , save and restore results , checkpoint in - progress ex - periments , and receive notiﬁcations when experiments fail or ﬁnish . By providing a uniﬁed framework for the parallelization of experimental setups , Memento can signiﬁcantly reduce the amount of time and coding expertise required to structure and execute experimental workﬂows , as Figure 1 shows . In the following sections , we present the main features of Memento , explain how it can help towards streamlining any experimental workﬂow , and conclude the paper , respectively . 2 The Memento Package : Features and Strengths Memento is a modular , ﬂexible , and easily conﬁgurable Python library that allows for the parallel execution of multiple experimental setups with the added beneﬁts of automatic checkpoint creation , output caching , and error tracing . At Memento 3 the core of Memento ’s design lies a user - friendly and customizable conﬁgura - tion matrix outlining the building blocks of the intended experiments within a pipeline , while avoiding writing repetitive code . Memento automatically trans - lates the matrix to distinct experimental tasks that can be allocated to diﬀerent CPUs , thus eﬀectively parallelizing the experimental pipeline and signiﬁcantly reducing the time required for large - scale experiments . When running computationally intensive experiments , saving intermediate results and resuming the process from where it left oﬀ in case of unexpected failures or interruptions ( e . g . , power outages , hardware failures , or errors in a single task ) is of utmost importance . Memento provides an easy way to do this through automated checkpointing . When checkpointing is enabled , Memento saves the experiment output at regular intervals , allowing for resumption without costly manual intervention , with the additional option of storing each result for post - evaluation . The source code and documentation are available on GitHub 3 , and the pack - age can be installed via PyPI using pip install memento - ml . 3 Demonstration and Conﬁguration To run a set of experiments with Memento , the user only needs to do the steps outlined in Figure 1 ( left ) . After installing and importing Memento using import memento # . . . the user needs to deﬁne a conﬁguration matrix ( config _ matrix ) . This matrix is the core of Memento and describes the list of experiments the user wants to run . One example is # The configuration matrix conveniently s p e ci f i e s the experiments to be run . config _ matrix = { " parameters " : { " dataset " : [ load _ digits , load _ wine , load _ breast _ cancer ] , " feature _ engineering " : [ DummyImputer , SimpleImputer ] , " preprocessing " : [ DummyPreprocessor , MinMaxScaler , StandardScaler ] , " model " : [ AdaBoost , RandomForest , SVC ] } , " settings " : { " n _ fold " : 5 } , " exclude " : [ { " dataset " : load _ digits , " feature _ engineering " : SimpleImputer } ] } Memento automatically constructs tasks using every combination of deﬁned parameters , e . g . , for all combinations of datasets , feature engineering methods , preprocessing techniques , and models above resulting in 3 × 2 × 3 × 3 = 54 tasks . The settings keyword stores constants that can be accessed by each task , re - moving the need to access global constants . The keyword exclude is used as a lookup table to skip any unwanted combinations during the task generation 3 Memento source code and documentation : github . com / wickerlab / memento 4 Z . Pullar - Strecker et al . allowing for a more ﬁne - grained speciﬁcation of required experiments . The name and number of parameters can be fully customized , making Memento compat - ible with any type of machine - learning pipeline . Each task is self - isolated and can be run in parallel . Once the conﬁguration matrix is deﬁned , Memento passes every created task individually as a set of parameters into the user - deﬁned experiment function ( exp _ func ) , for example , # The experiment function i s called for each indi vid ual set of parameters . def exp _ func ( context : memento . Context , config : memento . Config ) − > any : i f context . checkpoint _ exist ( ) : # Based on the hashing value . resu lt = context . restore ( ) # Recover resul t s from cache . else : # access the parameters for an indi vi dual experiment X , y = config . dataset ( ) model = config . model ( ) n _ fold = config . settings [ " n _ fold " ] # prepare the experimental pipeline and run pipeline = make _ pipeline ( config . feature _ engineering , config . preprocessing , model ) resu lt = cross _ val _ scores ( pipeline , X , y , cv = n _ fold ) context . checkpoint ( resul t ) # Caching resul t s return resul t Memento is designed with fault tolerance and exception handling in mind . Each parameter is assigned a hash value when generating the tasks . If an error occurs during the execution , we can update the code and rerun it . To avoid running duplicate experiments , we specify to restore checkpoints if available . If not , we access the input parameters for this task specify the individual experiment that needs to be run . Lastly , we specify the outputs that should be checkpointed . Once both the conﬁguration matrix and the experiment function are set up , the user can start Memento : # start MEMENTO notif _ provider = memento . ConsoleNotificationProvider ( ) resu lt s = memento . Memento ( exp _ func , notif _ provider ) . run ( config _ matrix ) and relax while it runs exp _ func for each task in parallel based on the number of threads available on the computer , caching results , and creating checkpoints . The notiﬁcation provider speciﬁes the notiﬁcation sent to the user once Memento completes the tasks . 4 Conclusion In this paper , we introduced Memento , a Python library that helps researchers and data scientists run and manage computationally expensive experiments eﬃ - ciently and reliably . Memento ’s simple conﬁguration matrix and parallelization capabilities allow for running large numbers of experiments quickly . The abil - ity to checkpoint in - progress experiments adds convenience and reliability to the process , making it easier to manage long - running experiments . While Me - mento is particularly well - suited to running machine learning experiments , it can be used for any type of laborious and time - consuming experimental pipeline . Memento 5 References 1 . Feurer , M . , Klein , A . , Eggensperger , K . , Springenberg , J . T . , Blum , M . , Hutter , F . : Auto - sklearn : Eﬃcient and Robust Automated Machine Learning , pp . 113 – 134 . Springer International Publishing , Cham ( 2019 ) . https : / / doi . org / 10 . 1007 / 978 - 3 - 030 - 05318 - 5 _ 6 2 . Idowu , S . , Strüber , D . , Berger , T . : Asset management in machine learning : State - of - research and state - of - practice . ACM Comput . Surv . 55 ( 7 ) ( dec 2022 ) . https : / / doi . org / 10 . 1145 / 3543847 3 . Karmaker , S . K . , Hassan , M . M . , Smith , M . J . , Xu , L . , Zhai , C . , Veeramachaneni , K . : AutoML to date and beyond : Challenges and opportunities . ACM Comput . Surv . 54 ( 8 ) ( oct 2021 ) . https : / / doi . org / 10 . 1145 / 3470918 4 . Brent Komer , James Bergstra , Chris Eliasmith : Hyperopt - Sklearn : Automatic Hy - perparameter Conﬁguration for Scikit - Learn . In : Stéfan van der Walt , James Bergstra ( eds . ) Proceedings of the 13th Python in Science Conference . pp . 32 – 37 ( 2014 ) . https : / / doi . org / 10 . 25080 / Majora - 14bd3278 - 006 5 . Pedregosa , F . , Varoquaux , G . , Gramfort , A . , Michel , V . , Thirion , B . , Grisel , O . , Blondel , M . , Prettenhofer , P . , Weiss , R . , Dubourg , V . , Vanderplas , J . , Passos , A . , Cournapeau , D . , Brucher , M . , Perrot , M . , Duchesnay , E . : Scikit - learn : Machine learning in python . J . Mach . Learn . Res . 12 , 2825 – 2830 ( nov 2011 ) 6 . Sarker , I . H . : Machine learning : Algorithms , real - world applica - tions and research directions . SN Computer Science 2 ( 3 ) ( 2021 ) . https : / / doi . org / 10 . 1007 / s42979 - 021 - 00592 - x 7 . Thornton , C . , Hutter , F . , Hoos , H . H . , Leyton - Brown , K . : Auto - weka : Combined selection and hyperparameter optimization of classiﬁcation algorithms . In : Proceed - ings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . p . 847 – 855 . KDD ’13 , Association for Computing Machinery , New York , NY , USA ( 2013 ) . https : / / doi . org / 10 . 1145 / 2487575 . 2487629 8 . Zöller , M . A . , Huber , M . F . : Benchmark and survey of automated ma - chine learning frameworks . J . Artif . Int . Res . 70 , 409 – 472 ( 2021 ) . https : / / doi . org / 10 . 1613 / jair . 1 . 11854