Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows Madeleine Grunde - McLaughlin † , Michelle S . Lam § , Ranjay Krishna †‡ , Daniel S . Weld †‡ , Jeffrey Heer † † University of Washington § Stanford University ‡ Allen Institute for AI { mgrunde , ranjay , jheer } @ cs . washington . edu , mlam4 @ cs . stanford . edu , danw @ allenai . org OUTCOME QUALITY Verifiable creativity STRATEGIES Diversity for creativity RESPONSE DIVERSITY User revision QUALITY CONTROL TACTICS DESIGN SPACE Parallel generation Evaluate candidates Step to select focus OBJECTIVES TASK Shortening text to specified lengths while preserving meaning RESULTING WORKFLOW Verification step Transform outputs INPUT CALLS TO WORKERS OUTPUT Find Fix 1 . 2 . Long text Shorter text with length slider Verify Figure 1 : We contribute ( 1 ) a design space , ( 2 ) case studies , and ( 3 ) a discussion of what techniques can transfer from crowd - sourcing workflows to LLM chains . For example , ( Left ) given a task of shortening text , as in Soylent [ 15 ] , our design space aids an LLM chain designer in identifying relevant high - level objectives . This objective could be “verifiable creativity , ” i . e . , creatively shortening input text while verifying its faithfulness to the original . Once the designer selects the relevant objective , they can browse concrete strategies , such as user revision for quality control . These strategies in turn point to lower - level tactics , such as allowing users to transform outputs . ( Right ) The designer can produce LLM chains by selecting the strategies and tactics outlined in our design space . For example , generating diverse responses with the parallel generation tactic employed by Soylent creates an LLM chain that gives users control over the length of the shortened text via a directly manipulable slider . ABSTRACT LLM chains enable complex tasks by decomposing work into a sequence of sub - tasks . Crowdsourcing workflows similarly decom - pose complex tasks into smaller tasks for human crowdworkers . Chains address LLM errors analogously to the way crowdsourcing workflows address human error . To characterize opportunities for LLM chaining , we survey 107 papers across the crowdsourcing and chaining literature to construct a design space for chain develop - ment . The design space connects an LLM designer’s objectives to strategies they can use to achieve those objectives , and tactics to implement each strategy . To explore how techniques from crowd - sourcing may apply to chaining , we adapt crowdsourcing workflows to implement LLM chains across three case studies : creating a tax - onomy , shortening text , and writing a short story . From the design space and our case studies , we identify which techniques transfer from crowdsourcing to LLM chaining and raise implications for future research and development . ACM Reference Format : MadeleineGrunde - McLaughlin † , MichelleS . Lam § , RanjayKrishna †‡ , Daniel S . Weld †‡ , Jeffrey Heer † . 2023 . Designing LLM Chains by Adapting Tech - niques from Crowdsourcing Workflows . In Proceedings of Conference ( ArXiv ) . ACM , New York , NY , USA , 22 pages . 1 INTRODUCTION People use Large Language Models ( LLMs ) for assistance with both single - step tasks , like suggesting synonyms , and more complex multi - step tasks , like writing stories , editing text , and organizing ArXiv , preprint , 2023 2023 . ideas . Despite their widespread adoption , LLMs suffer from qual - ity deficits like hallucinations [ 40 , 106 , 111 ] , brittleness to prompt changes [ 12 , 45 , 55 , 114 , 152 ] , and user interventions limited to prompt changes [ 116 , 143 ] . These deficits are exacerbated for com - plex tasks , which require multiple steps in which errors can arise and propagate [ 39 ] . Unfortunately , many common use cases require multiple reasoning steps . For example , the use case of shortening text requires first deciding which portions of the text to edit and then determining how to revise each one [ 15 ] . To tackle complex tasks , recent research has turned to LLM chaining techniques . Chaining decomposes a task into multiple calls to an LLM , in which the output of one call affects the input to the next call [ 144 ] . For example , when shortening text , an LLM chain could identify verbose sentences , edit each one , and propose outputs of variable lengths by composing multiple edits . Chain - ing has many benefits : this strategy can enable new abilities like long - form story writing [ 108 , 148 ] , increase accuracy on logical reasoning questions [ 32 , 146 , 156 ] , and enable greater transparency and debuggability [ 57 , 122 , 141 ] . Although initial evidence suggests that chaining can help mitigate LLM quality deficits , current solu - tions are imperfect [ 32 , 97 , 108 , 110 , 117 ] . LLM chaining is nascent and challenging , with a seemingly intractable design space . The challenge of decomposing complex work , however , is not new . In the more established field of crowdsourcing , researchers have faced a similar problem setup . Groups of people ( requesters ) outsource tasks to other people ( crowdworkers ) on online platforms . Crowdworkers have their own limitations : approximately 30 % of crowdworkers’ outputs are low quality [ 15 ] , small changes in in - structions affect crowdworker responses [ 71 , 107 ] , and requesters a r X i v : 2312 . 11681v1 [ c s . H C ] 18 D ec 2023 ArXiv , preprint , 2023 Grunde - McLaughlin , et al . have limited modes of interaction with crowdworkers [ 124 ] . To compensate for these limitations , crowdsourcing researchers have developed a variety of structured workflows . Crowdsourcing work - flows decompose a task into a series of smaller microtasks for crowdworkers to complete . These workflows account for crowd - worker limitations by reducing task complexity and leveraging redundancy , aggregation , and explicit verification [ 6 , 15 , 25 , 151 ] . Given the parallels between these fields , we investigate how crowdsourcing techniques may be adapted to design LLM chains . Selecting relevant strategies from over a decade of crowd - sourcing research is challenging , especially as LLMs and crowd - workers have salient differences [ 143 ] . For example , crowdworkers may be slow to generate large amounts of text , whereas LLMs may lack common sense and factual grounding , making them more sen - sitive to instructions and more likely to “hallucinate” information . To guide LLM chain development , we first construct a design space based on a systematic review of the crowdsourcing workflow and LLM chaining literatures . We analyze 107 papers and perform open coding to identify core design space dimensions . Our design space is organized into three tiers : objectives , strategies , and tactics ( Figure 2 ) . The first tier outlines the objectives for which a designer can optimize a workflow . Then , strategies describe ap - proaches to achieve an objective , and tactics are mechanisms that implement strategies . We then describe how LLM chain designers can navigate this design space . For example , consider a user who wants to shorten input text while still maintaining faithfulness to the original text ( Figure 1 ) . This task is analogous to one explored by the Find - Fix - Verify crowdsourcing workflow [ 15 ] . Using our design space , the designer would classify their task’s objective as “verifiable creativ - ity” ; i . e . , a creative task ( shortening text ) that has to obey some set verifiable constraint ( faithfulness ) . Next , they can browse potential strategies that might help with this objective . They will come across the “response diversity” strategy , which suggests a chain design that redundantly produces multiple shortened outputs that can later be analyzed for faithfulness . To implement this strategy , our design space provides the “parallel generation” tactic by prompting LLMs to produce multiple parallel generations . To better assess the impact of similarities and differences be - tween crowdsourcing and LLMs , we implement three case stud - ies adapting crowdsourcing workflows to use LLMs . We chose workflows that enable direct manipulation of the output , an in - stance of the user revision strategy for quality control that has been explored in crowdsourcing but is underexplored in chaining . The case studies implement tasks that vary in their degrees of creativity , adapting Cascade [ 28 ] to control the precision of a generated tax - onomy , Soylent [ 15 ] to shorten text , and Mechanical Novel [ 68 ] to control the narrative elements of a fictional story . We evaluate these chains in terms of the quality and controllability of their outputs . For example , compared to zero - shot prompting , chain - generated stories are preferred by crowdsourced raters and the story elements can be more precisely controlled . These case study implementations provide insights into how differences between LLMs and crowd - workers impact chain design to achieve the same objectives . In Mechanical Novel , for instance , we find that the optimal chain re - quires more granular sub - tasks and extra verification steps than the original crowdsourcing workflow . Based on our literature survey , design space , and case studies , we identify techniques that do and do not transfer from crowd - sourcing and share implications for future work . We find that the objectives , strategies and types of tactics of our design space trans - fer . We recommend approaches to the user revision strategy and creative work objective applied in crowdsourcing as particularly worth exploration in chaining . However , to achieve a given strategy , tactic design decisions may not transfer , due to differences in the abilities , strengths , and weak - nesses of crowdworkers and LLMs . For example , LLMs can more quickly process and generate fluent text than crowdworkers , dif - ferent subtask complexities perform best on LLM chains , and LLM chains require more oversight to generate and validate diverse re - sponses . Consequently , future work is needed to study optimal tactic designs , such as tactic selection and implementation , espe - cially within the context of effectively employing certain strategies . Finally , we explore how the relative speed and reduced cost of LLMs relative to crowdworkers presents an opportunity to cre - ate higher quality chains , and we recommend improved tools and methods to augment this design process . 2 DESIGN SPACE METHODOLOGY We survey papers that involve the design and implementation of crowdsourcing workflows or LLM chains . Although crowdsourcing workflows and LLM chains have many overlapping concepts , the terms used to denote these concepts differ . In this paper , we refer to both LLMs and crowdworkers as “workers . ” We use the term “workflow” to refer to both crowdsourcing workflows and LLM chains . We refer to intermediate steps of workflows or chains as “subtasks . ” We use the term “user” to describe the person receiving workflow outputs ( e . g . , the “requester” in crowdsourcing terms ) . 2 . 1 Paper collection We run two paper collection processes , one for the crowdsourcing literature and another for the LLM chaining literature . We use snowball sampling from the relevant citations in and of these papers , including every citation of the initial AI Chains paper [ 142 ] . We consider papers available as of August 2023 . Workflow literature . We collect papers through search of the key - words : crowd ( sourcing ) workflow , crowd ( sourcing ) pipeline , crowd - sourcing complex work and microtask . We search the ACM CHI Con - ference on Human Factors in Computing Systems , ACM Conference on Computer - supported Cooperative Work and Social Computing , the ACM Symposium on User Interface Software and Technology , and the AAAI Conference on Human Computation and Crowd - sourcing , as well as Semantic Scholar [ 44 ] and Google Scholar . Chaining literature . We collect papers through search of the key - words : LLM chain ( ing ) , prompt chain ( ing ) , LLM pipeline , and AI chain ( s / ing ) . We search the International Conference on Learning Representations , the Conference on Neural Information Process - ing Systems , the International Conference on Machine Learning , the Association for Computational Linguistics proceedings , the Association for Computing Machinery proceedings , as well as Se - mantic Scholar [ 44 ] and Google Scholar . We include preprints in our literature search . Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Redundant User revision Objectives Outcome quality Resource constraints Accuracy Verifiable creativity Creativity Expense Latency Strategies Quality control Response diversity Appropriate subtasks Worker instructions Diversity for accuracy Verificationsteps Diversity for creativity Workflow flexibility Task specificity Architecture rigidity Disagreement resolution Communicative Tactics Architecture Subtasks Sequential Branching Workers Crowdworkers LLMs Dynamic Transform Partition Generate Evaluate Focus User Figure 2 : LLM chain design space summary . We present a design space that guides LLM chain designers from their high - level objectives to workflow design strategies to grounded implementation - oriented tactics . These design space components synthesize practices from the established domain of crowdsourcing workflow design and the emerging area of LLM chaining . Paper inclusion criteria . We include papers that implement , or support the implementation of , a workflow with at least two steps ; i . e . , workflows in which the outputs from one worker inform the input to another worker . We include papers that focus on building an interface or domain specific language for other designers to build workflows . We restrict our search to academic papers , rather than software like LangChain [ 81 ] . We exclude works that call multiple workers in parallel but never in a chain of steps ( e . g . , ma - jority voting of labels ) . We also exclude techniques such as Chain of Thought [ 137 ] which issue multiple tasks to a worker in one step instead of incorporating multiple worker calls [ 26 ] . We include instances in which workers synchronously work on the same in - terface , viewing and editing each other’s outputs . Finally , the task at hand must have a user working towards some end goal , so we do not consider open - ended collaborative projects like Wikipedia . We make one exception for a paper that explicitly draws from crowdsourcing strategies to complete data aggregation tasks with LLMs [ 116 ] . Although they do not implement chains , the connec - tion to crowdsourcing resonates with our design space goals . Results . Our initial keyword search found 313 papers for workflows and 180 papers for chaining . One author determined whether papers were out of scope given our collection criteria , and any ambiguous cases were discussed among multiple authors . Our final set of papers resulted in 107 papers : 68 for workflows , and 39 for chains . 2 . 2 Coding strategy We conduct a thematic analysis [ 19 ] , following the methodolo - gies of similar design spaces in HCI [ 13 , 79 , 127 ] . First , one author extracted 1 ) the outcomes and 2 ) the elements of the workflows from the paper set . We then employed iterative open coding on the extracted data , in which two authors separately coded each data extraction , sorted resulting codes into sub - categories , then discussed these categorizations . In three further rounds of itera - tion , the same authors refined these categorizations . All authors reviewed the resulting design space . Two authors categorized every paper within the finalized design space . 1 1 https : / / github . com / madeleinegrunde / designingchains 3 DESIGN SPACE Our design space consists of three tiers that cover work in both the crowdsourcing workflow and LLM chaining literatures . First , we cover objectives for which designers may want to optimize : to promote outcome qualities such as accuracy or creativity , while respecting resource constraints . We then delineate strategies by which designers can achieve these objectives : response diversity , quality control mechanisms , and workflow flexibility . Finally , we categorize tactics for implementing these processes , at the levels of workflow architectures , subtasks , and choice of workers . Finally , we demonstrate how to navigate connections between design space tiers , with the objectives guiding design decisions at every level . We arrived at the tiers through an open coding process . Two of the high level tiers that emerged matched the concept types we extracted from the literature : objectives ( outcomes ) and tactics ( elements ) . If you were to sketch a workflow on a piece of paper , tactics would be the visible elements in the sketch and the workers who complete each subtask . The process of coding also surfaced a set of abstract concepts that connect the tactics to the objectives . These concepts provide the third intermediate tier : strategies . 3 . 1 Objectives in the design space The objectives of a workflow design consist of achieving a desired outcome quality within resource constraints ( Figure 3 ) . 3 . 1 . 1 Outcome Quality . Workflows support a range of output qual - ity objectives , from highly accurate to highly creative . To aid dis - cussion , we discretize this spectrum into three categories : tasks that prioritize accuracy , verifiable creativity , or creativity . Accuracy . Accuracy - focused tasks have one correct answer . Some crowdsourcing papers in this space include TurKit [ 93 ] and Second Opinion [ 109 ] . Examples of these tasks include math problems [ 38 , 88 , 101 , 145 , 158 ] , blurry text recognition [ 92 , 93 ] , and matching images with the same content [ 74 , 109 ] . For these tasks , workflows can improve performance on benchmarks [ 31 , 42 , 51 , 74 , 87 , 107 , 126 , 145 , 156 ] , enable new tasks [ 47 , 50 , 58 , 62 , 93 ] , classify examples for which machine learning approaches struggle [ 4 , 15 , 25 , 109 ] , improve instructional clarity [ 17 ] , and reduce hallucinations [ 32 , 33 , 108 , 110 , 111 , 116 , 140 ] . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . 0 % 50 % 100 % P r eva l e n ce Accuracy VeriﬁableCreativity Creativity C r o w d LL M C r o w d LL M C r o w d LL M Outcome Quality 0 % 50 % 100 % Expense Latency C r o w d LL M C r o w d LL M Resource Constraints Objectives Figure 3 : Objectives literature distributions . LLM chaining papers ( LLM ) most often pursue accuracy - based objectives , while a higher rate of crowdsourcing papers pursue verifiable creativity objectives . Crowdsourcing papers ( Crowd ) more often address methods for reducing expense and latency . Verifiable creativity . In some tasks , there is not one correct an - swer , but there are restrictions on what outputs are correct . Crowd - sourcing papers for these tasks include Cascade [ 28 ] , Flock [ 25 ] , Revolt [ 23 ] , and Storia [ 66 ] . Example tasks include categorizing data ( categories must be accurate , but there are multiple options and unclear edge cases ) [ 8 , 18 , 23 , 25 , 27 , 28 ] , summarizing text ( summaries must stay faithful , but have degrees of freedom in how to write the summary ) [ 15 , 66 , 110 , 111 , 118 ] , and coding ( programs must work , but there are multiple approaches to problem solv - ing ) [ 56 , 57 , 78 , 84 , 140 ] . The goal of workflows for these tasks are to align with expert outputs [ 28 , 43 , 67 , 69 , 95 , 117 , 124 ] or to enable users to choose among multiple correct possible output options . Using workflows for these tasks can result in an output that is human - understandable [ 25 ] , that better represents what would be important for people to know [ 125 ] , and that surfaces the uncertainty among the different possible correct options [ 23 ] . Creativity . Creative tasks have the widest degree of output free - dom . Representative crowdsourcing papers for creative objec - tives include Flash Teams [ 123 ] , VisiBlends [ 29 ] , and Mechanical Novel [ 68 ] . These papers tackle creative writing [ 7 , 15 , 68 , 108 , 148 , 150 , 155 ] , idea generation [ 69 , 92 , 143 ] , and prototyping [ 78 , 85 , 117 , 123 , 133 ] . Prior work finds that on creative tasks , workflows can help achieve higher quality [ 7 , 68 , 69 , 85 , 108 , 117 , 148 , 150 ] and less constrained outputs [ 37 , 53 , 85 , 95 , 117 , 135 ] . Workflows may enable novice crowdworkers [ 120 ] or users [ 8 , 29 , 66 , 90 , 133 , 135 , 143 ] to perform a creative task with an otherwise high barrier of entry . Im - portant qualities for creative tasks include coherence [ 68 , 148 , 150 ] and aligning to the user’s creative vision [ 124 ] . On creative tasks , a workflow is often better suited for creating and lengthening the output , rather than proofreading [ 61 , 68 , 93 , 108 ] , so these outputs are sometimes seen as a starting point for editing , rather than a finished product [ 66 , 97 , 108 , 113 , 117 ] . 3 . 1 . 2 Resource constraints . Workflow objectives are constrained by both monetary expense and latency requirements . Expense . The expense of a workflow refers to paying crowdwork - ers or paying for an API call to an LLM . Workflows are often more expensive than simpler approaches ( e . g . , Chain of Thought prompt - ing ) [ 35 , 38 , 45 , 53 , 62 , 90 , 104 , 105 , 110 , 134 , 150 , 157 ] . However , the cost is often less than hiring professionals [ 6 , 30 , 43 , 89 , 151 ] , pretraining models [ 31 , 57 , 69 , 86 , 150 ] , or completing the task yourself [ 135 , 156 ] . Latency . Another common constraint is the time for the workflow to complete [ 78 , 82 , 104 , 105 ] . Many workflows have lower latency than performing the same task manually , since work can be paral - lelized across multiple workers [ 8 , 27 , 47 , 49 , 78 , 82 , 85 , 90 , 120 , 131 ] . 3 . 2 Strategies in the design space Several strategies can be applied to achieve workflow objectives . We categorize the strategies used in prior work into three groups : response diversity , quality control , and workflow flexibility ( Figure 4 ) . 3 . 2 . 1 Response diversity . Workflows can source multiple , diverse responses for the same subtask , which can promote either accuracy or creativity [ 7 , 68 , 84 , 113 , 123 , 131 ] . Crowdworkers have different viewpoints [ 6 , 24 , 83 ] and sourcing perspectives from expert crowd - workers [ 27 ] or from workers with various skill levels [ 102 ] can extend this variety . For LLMs , leveraging their stochastic nature ( e . g . , with a higher temperature parameter setting ) or using differ - ent models or prompt variants ( including role - based prompts ) can source a variety of responses [ 12 , 117 , 143 ] . In some cases one can automatically calculate the optimal mix of LLMs for a task [ 100 ] . Diversity for accuracy . When aggregated or compared , diverse responses can improve accuracy and robustness to low quality outputs , outliers , and spammers [ 6 , 38 , 53 , 59 , 62 , 83 , 89 , 122 , 151 ] . Diversity for creativity . Sourcing diverse responses encourages divergent and diverse thinking , helping improve the quality of creative outputs and to rapidly ideate [ 61 , 82 , 88 , 98 , 113 , 131 , 140 ] . Furthermore , different workers can fill in each other’s deficits to create a more rounded final product [ 66 , 153 ] . 3 . 2 . 2 Quality control . Errors can occur from the worker’s capa - bilities [ 142 , 146 , 148 ] or can be induced by the workflow itself . Workflows can suffer from cascading errors [ 14 , 15 , 38 , 46 , 73 , 92 , 93 , 141 , 142 ] or produce incoherent outputs [ 53 , 53 , 68 , 108 , 141 , 148 ] . Aggregations may also induce disfluencies [ 15 , 53 , 66 ] or omit in - tended parts of a task [ 148 ] . While all elements of a workflow should support quality outputs , some support quality control more directly : appropriate subtasks , worker instructions , verification steps , disagreement resolution mechanisms , and user revision . Appropriate subtasks . Compared to entire tasks , subtasks are of - ten easier to complete , verify , and correct [ 6 , 32 , 33 , 69 , 126 , 142 , 148 ] . Simplifying tasks can increase quality [ 28 , 83 , 120 , 142 ] . Another strategy is to adapt the subtask to fit the worker’s capabilities [ 12 , 23 , 109 , 120 ] . For example , researchers found that crowdworkers were better at generating predictive features than at estimating if a feature is predictive ; so they adapted the workflow accordingly [ 25 ] . Worker instructions . Properly instructing workers and identify - ing “good” workers can improve quality [ 17 , 94 ] . Both crowdwork - ers and LLMs benefit from task examples , namely screening and tutorial tasks for crowdworkers [ 15 , 24 , 37 , 82 , 83 , 120 , 131 ] and few - shot examples for LLMs [ 45 , 110 ] . A key difficulty is communicating Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 0 % 50 % 100 % P r eva l e n ce DiversityForAccuracy DiversityForCreativity C r o w d LL M C r o w d LL M Response Diversity 0 % 50 % 100 % AppropriateSubtasks Worker Instruction VeriﬁcationStep DisagreementResolution UserRevision C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M Quality Control 0 % 50 % 100 % Task Flexible Task Speciﬁc Dynamic Rigid C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M Workﬂow Flexibility Strategies Figure 4 : Strategies literature distributions . Crowdsourcing papers ( Crowd ) have a higher prevalence of workflows incorporating response diversity , disagreement resolution , and user revision strategies , while LLM chaining papers ( LLM ) more often explicitly incorporate strategies to improve worker instruction and verification steps . the user’s intent through language . Crowdworkers tend to need less explicitly defined instruction than LLMs , but can still struggle with vague or ambiguous instructions [ 143 ] . For both crowdworkers and LLMs , small changes in instructions or prompts can lead to dramatically different outcomes , impacting the accuracy of the final result [ 12 , 45 , 58 , 71 , 107 , 108 , 116 , 141 – 143 ] . Additionally , different LLMs may respond differently to the same input prompt [ 116 ] . Communicating context to workers can help with improving the quality of the output [ 6 , 15 , 29 , 53 , 77 , 98 , 131 , 141 ] . The crowdsourc - ing literature contains many forms of contextual communication , from input text [ 6 , 15 , 77 ] , to workflow goal tracking [ 68 , 108 , 148 ] , to explicit communication between users and workers [ 124 ] . For LLMs , context may be included as part of the prompt , although LLMs have the additional barrier of an input token limit [ 108 , 117 ] . Verification steps . Workflows can incorporate their own trou - bleshooting by explicitly dedicating a step for verification [ 6 , 77 , 126 , 140 , 143 , 157 ] . Others instead make all production editable at all steps so workers can fix errors in a flexible manner [ 84 , 95 ] . Automated methods of flagging malicious workers , gold - standard validation steps , or requiring a certain agreement threshold among workers can also provide verification [ 62 , 82 , 116 ] . Disagreement resolution . Resolving disagreements among di - verse responses can improve the quality of the aggregate outcome . In parallel and iterative workflows , errors from prior passes can be found and fixed or rejected [ 53 , 151 ] and differing opinions can be merged [ 73 , 98 ] . Debate - based workflows can promote reflection on disagreements [ 24 , 38 , 74 , 88 ] . Using different models and roles in debate shows promise [ 38 ] , as an LLM judge in a debate was found to more likely agree with its same underlying model [ 88 ] . User revision . Incorporating the user ( the requester , in crowd - sourcing terms ) into some subtasks can also improve quality . Users can dynamically answer questions and provide feedback [ 124 ] or monitor dynamic workflows to catch cascading errors [ 77 ] . Users can be called upon when necessary with a “panic button” that workers can invoke [ 77 ] , or by automatically surfacing ties to a user [ 120 ] . With a shared interface between users and all work - ers , users can intervene when needed [ 82 , 113 ] . Users may review at intermediate steps to edit or regenerate sections of the work - flow [ 69 , 117 , 155 ] . A workflow may also include multiple iterations with user revisions between each iteration [ 25 , 69 , 78 , 117 , 139 ] . Users can also directly manipulate the final output to improve qual - ity . This approach surfaces uncertainty in data as a feature for the user to explore , rather than hiding it in aggregation [ 23 , 25 , 139 ] . They can sort and filter the returned data [ 25 , 109 , 139 ] , use a slider to control output characteristics like text length [ 15 ] , and consider category labels at multiple levels of precision [ 23 ] . 3 . 2 . 3 Workflow flexibility . Workflows may or may not be task - specific , with either rigid or dynamic architectures . Task specificity . Some workflows are highly specialized to the specifications of the task ( e . g . , visual blends [ 29 , 135 ] , masking faces in images [ 4 , 65 ] , and programming [ 57 , 84 ] ) . Others generalize to a variety of tasks [ 32 , 33 , 77 , 111 , 140 ] . For example , AutoGen [ 140 ] uses the same conversation - based workflow for coding as well as chess games , math problems , and retrieval - based question answer - ing . Task - specific workflows support fewer outcome types but can incorporate more tailored support of specific outcome qualities . Architecture rigidity . Rigid workflows do not change their ar - chitecture depending on the input , while dynamic workflows gen - erate parts of the workflow architecture at runtime [ 24 , 57 , 58 , 66 , 73 , 78 , 133 ] . Changes in dynamic workflows may be made by the crowd [ 49 , 84 , 123 , 133 ] , user [ 77 , 113 , 123 , 133 ] , or an algo - rithm [ 18 , 24 , 34 , 46 ] . Generally , architecture rigidity supports ob - jectives that require more transparency and consistency , while architecture flexibility especially supports quality outcomes when tasks are difficult to predefine . 3 . 3 Tactics in the design space While strategies help reach workflow objectives , more granular tactics are needed to implement them . Tactics need to consider the workflow architecture , the subtasks they contain , and the choice of workers for those subtasks ( Figure 5 ) . 3 . 3 . 1 Workflow architecture . Workflows combine subtasks into larger architectures that may be sequential , branching , redundant , dynamic , or communicative . One workflow may employ multiple architectural patterns . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . 0 % 50 % 100 % P r eva l e n ce Sequential Branching Redundant Dynamic Communicative C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M Architecture 0 % 50 % 100 % Generate Evaluate Transform Focus Partition C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M C r o w d LL M Subtasks 0 % 50 % 100 % Crowdworkers LLMs User C r o w d LL M C r o w d LL M C r o w d LL M Workers Tactics Figure 5 : Tactics literature distributions . Both the crowdsourcing workflow ( Crowd ) and LLM chaining ( LLM ) literatures often incorporate sequential and redundant architectures and generate , evaluate , and transform subtasks . Crowdsourcing workflows more often incorporate communicative architectures and the user as a worker than LLM chains . Sequential architectures order subtasks in a chain in which the output of one worker passes forward to another worker [ 42 , 45 , 56 , 57 , 97 , 110 , 122 , 135 , 142 , 143 , 158 ] . Branching workflows contain conditionals that , depending on the output of a prior step , choose different execution paths [ 57 , 59 , 71 , 141 , 157 ] . Branching architectures can grant flexibility for workflows to address a variety of tasks . Redundant architectures require different workers to complete the same task in parallel or iteratively . In parallel generation , mul - tiple workers perform the same subtask separately [ 5 , 21 , 31 , 51 , 61 , 90 , 138 ] . This subtask is often , but not always , followed by an aggregation or evaluation subtask to merge or select among the different parallel generations . Iterative architectures run iteratively over a changing input . For example , iterative improvement takes the output of a subtask and iteratively gives it to a series of workers to improve , sometimes with interspersed decision tasks choosing between the current and prior iterations [ 7 , 34 , 46 , 82 , 84 , 98 , 139 ] . The number of iterations can be determined to be a fixed num - ber of steps [ 62 , 68 , 93 , 143 , 151 ] , by including a subtask to check for sufficiency [ 32 , 33 , 49 , 78 , 88 , 101 , 111 , 124 , 134 , 145 ] , until the user stops calling iterations [ 25 , 29 , 78 , 122 , 139 ] , until the out - puts converge [ 38 , 45 , 89 , 89 , 126 ] , until all quality checks have all been addressed [ 18 , 28 , 68 , 104 , 105 , 131 , 148 ] , from increas - ing complexity levels [ 4 , 5 , 20 ] , and through decision - theoretic ap - proaches [ 35 , 117 ] . Redundant architectures most explicitly support the diverse responses strategy , but they can also be used for quality control ( e . g . determining if a disagreement has been resolved by checking similarity across parallel workstreams ) and for surfacing uncertainty for user revision . Dynamic architectures support changing the workflow architec - ture as it is executed . Map - reduce is a dynamic architecture in which workers break down a task recursively until the subtasks are an appropriate size [ 58 , 65 , 66 , 73 , 77 , 89 , 89 , 133 , 143 ] . The literature defines such workflows as a hierarchy [ 57 , 73 ] . Other dynamic workflow architectures include doing beam search across multiple options [ 32 , 146 , 150 ] and allowing for workers to flex - ibly create tasks on a shared to - do list before choosing which of those to complete [ 29 , 73 , 84 , 102 , 113 , 153 ] . Some other ar - chitecture methods may have dynamic elements , such as paral - lelizing an input into chunks and running the workflow for each chunk [ 8 , 15 , 47 , 65 , 67 , 118 , 151 ] or dynamically determining the number of iterative steps [ 24 , 27 , 101 , 111 , 158 ] . Dynamic archi - tectures support the workflow flexibility strategy , and can be used to fit within resource constraints , create appropriate subtasks , and incorporate user revision . Communicative architectures can facilitate debate and conver - sation among workers [ 23 , 24 , 37 , 38 , 74 , 88 , 125 , 131 , 140 ] and allow workers to return feedback to the user [ 77 , 82 , 113 ] . Some workflows create roles for different workers , often based on roles held in real - life organizations ( e . g . , journalists , software develop - ment teams , etc . ) [ 1 , 117 , 123 , 133 ] . Some papers integrate these roles into a hierarchy of communication , with managers , midlevel managers , and different forms of communication between differ - ent levels of the hierarchy [ 1 , 123 ] . Communication may be re - strained through subtasks , free - form though email [ 1 , 133 ] , or en - abled through a shared interface [ 78 , 82 , 83 , 85 , 102 , 113 ] . Commu - nication among workers may be synchronous or asynchronous . Synchronous workflows help enable collaboration [ 7 , 27 , 29 , 125 ] and can reduce latency [ 14 ] . With asynchronous workflows or workflows with limited communication architectures , a working memory or state management can help workers access progress reports , what other workers think are good next steps , and the over - arching goals [ 53 , 53 , 83 , 84 , 111 , 133 , 140 , 148 , 153 ] . Communicative architectures can serve as a method of disagreement resolution and workflow flexibility . They often encourage diverse responses though worker communication . 3 . 3 . 2 Subtasks . Subtasks are singular tasks in a workflow . Al - though highly varied , they follow several thematic functions : to generate , evaluate , transform , focus , and partition . Generate . Most workflows involve at least one step in which work - ers generate content . This generation may be writing words or code [ 87 , 118 , 138 , 155 , 156 ] , answering a question or solving a problem [ 42 , 74 , 145 , 146 ] , or brainstorming and ideation [ 29 , 59 , 83 , 97 , 102 , 150 ] . Generation tasks may also ask for labels or anno - tations on given content [ 18 , 23 , 27 , 50 , 51 ] , to translate between languages [ 6 , 151 ] , to contribute to a conversation with other work - ers [ 38 , 88 , 102 , 134 ] , to demonstrate a behavior [ 16 , 85 ] , or to source content from external sources [ 29 , 30 , 53 , 60 , 66 , 67 , 98 , 120 ] . Some subtasks also require an explanation along with the generated content [ 23 , 25 , 37 , 43 , 45 , 68 , 126 ] . Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Evaluate . One of the most common evaluation tasks is choosing the best among multiple possible options [ 25 , 50 , 51 , 71 , 95 , 138 ] . Other subtasks may include rating and ranking among multiple options [ 62 , 102 , 104 , 118 , 124 , 139 ] , a pairwise comparison [ 20 , 92 , 93 , 104 , 105 , 109 , 116 ] . Evaluation subtasks may also determine if another round of generation needs to occur [ 84 , 88 , 134 , 142 ] . Workflows can decide what moves forward through majority vote [ 35 , 51 , 61 , 67 , 82 , 95 ] , other aggregations of voting tasks ( e . g . , expectation maximization [ 37 , 116 ] , weighted mean [ 109 ] , weak supervision [ 12 ] ) , returning the highest ranked or rated items [ 62 , 118 ] , or automated methods . These automated meth - ods include measuring agreement from the generation step [ 14 , 23 , 53 , 60 , 62 , 95 , 111 ] and using non - AI checks like semantic em - beddings and rule - based heuristics [ 6 , 116 , 135 ] . There are also subtasks that seek critiques and reviews from the user [ 113 , 126 ] or another worker [ 30 , 68 , 78 , 88 , 101 , 102 , 116 , 126 , 133 ] . These subtasks may ask for feedback or verify if errors or other qualities exist [ 8 , 15 , 18 , 21 , 29 , 30 , 45 , 57 , 60 , 61 , 67 , 73 , 77 , 88 , 138 , 143 , 148 ] . The outputs from these subtasks can disqualify information or pass into another subtask to be addressed . Evaluate subtasks primarily resolve disagreements and serve the verification step strategy . Transform . Other subtasks transform given data by merging or im - proving it . Some subtasks merge multiple streams of incoming con - tent . This merge could be algorithmic [ 2 , 4 , 5 , 29 , 73 , 131 ] or accom - plished by the worker [ 6 , 71 , 73 , 77 , 84 , 85 , 98 , 120 , 131 , 142 , 143 , 143 ] . Other subtasks improve content generated by previous steps , some - times by incorporating feedback from validation steps . Improving on previously generated content can come in many forms : updat - ing the format of the content [ 12 , 42 , 57 , 134 , 142 ] , pruning bad results [ 45 , 95 ] , adding missing results [ 57 , 95 , 138 ] , correcting er - rors [ 37 , 47 , 57 , 74 , 148 ] , and performing direct improvements [ 15 , 30 , 35 , 51 , 53 , 62 , 68 , 85 , 89 , 93 , 101 , 102 , 107 , 113 , 126 , 131 , 143 ] . Transform subtasks primarily support the strategies of resolving disagreements among diverse responses . Focus . One set of subtasks looks to select the attention of the upcoming tasks within a broader context . For example , this step may find patches of the input that need editing , extract information from context , or focus on common keywords [ 14 , 32 , 33 , 45 , 62 , 68 , 111 ] . Focus subtasks address the strategies of workflow flexibility , worker instructions for other subtasks , and creating appropriate subtasks . Partition . Some subtasks transform a task by breaking it up into smaller pieces [ 58 , 73 , 77 , 84 , 133 , 142 , 143 , 158 ] . For example , the Divide step in the Turkomatic workflow asks workers to divide a given subtask into simpler subtasks . The workflow calls this step recursively until the subtask complexity is valued ( by a separate crowdworker ) at a specified cost [ 77 ] . Partition subtasks create appropriate subtasks and support workflow flexibility . 3 . 3 . 3 Workers . Workers that complete the subtasks can be crowd - workers , LLMs , or the user . Crowdworkers are usually recruited from crowdworking plat - forms like Amazon Mechanical Turk or UpWork ( formerly known as oDesk ) . Platforms allow for requests of worker qualifications , although these credentials are sometimes hard to validate [ 2 , 78 , 133 , 151 ] . Methods like retainer models can reduce the latency of accessing crowdworkers on these platforms [ 14 ] . Workers can also be recruited from the public , including community members [ 102 ] , conference committees [ 8 , 27 ] , and online deaf communities [ 16 ] . Crowdworkers have limitations that overlap with LLM limita - tions . Prior work suggests that approximately 30 % of outcomes from Amazon Mechanical Turk are low quality [ 15 ] , and may include spam or off - topic responses [ 4 , 21 , 59 , 83 ] . Crowdworkers also strug - gle with following long and complex instructions , as people can overly focus on the beginning or end of a given text [ 24 , 28 , 53 , 82 ] . Crowdworkers can be biased ; for example , in one study , workers performed significantly better in identifying photos of their own race compared to other races [ 109 ] . Crowdworkers have unique limitations . Anchoring bias can lead to less creativity in generation or divergent thinking [ 142 , 143 ] . Since workers are paid when they complete a task , there is incen - tive to work quickly even if that means satisficing [ 15 , 54 , 116 ] . Additionally , since rejected work can hurt access to future jobs on a platform [ 68 ] , workers may overcompensate to a detrimental level in an attempt to make their efforts clear [ 15 , 67 ] . Crowdworkers may fatigue [ 4 , 34 ] , and they may lack task - relevant knowledge and delete jargon [ 15 , 113 ] . On synchronous tasks , other limitations become prevalent . Social blocking can make workers so cautious to change others’ work that not enough work gets done [ 53 , 82 ] , and territoriality can motivate edit wars in which workers’ attempts to establish their own work over others degrade the final product quality [ 7 , 133 ] . Crowdworkers also may be affected by diffusion of responsibility and social loafing in which there is incentive to wait for others to do tasks [ 7 , 82 ] . Crowdworkers have their comparative strengths as well , includ - ing the ability to perform high - level structural evaluation and to quickly identify outliers [ 60 , 98 ] . People are also currently better at selectively choosing among subtasks than LLMs [ 143 ] . LLMs . There are many possible LLMs of different sizes and ex - penses [ 20 ] . Models have differing strengths and respond differently to prompts [ 88 , 108 , 116 , 143 ] . LLM outputs can be adjusted through parameter changes ( e . g . , temperature ) [ 116 , 117 ] , fine - tuning [ 20 , 31 – 33 , 87 ] , and expressing roles through prompts [ 38 , 117 ] . Some LLM limitations overlap with those of crowdworkers . LLMs can provide erroneous outputs : they are known to confidently out - put incorrect answers [ 38 , 116 , 135 ] or hallucinations [ 32 , 33 , 108 , 110 , 111 , 116 , 140 ] and for struggling with self - consistency and logical reasoning [ 32 , 33 , 88 , 116 , 146 , 150 ] . LLMs can ignore large fractions of long prompts or focus mostly on recent parts of the given context [ 38 , 101 , 116 , 143 ] . LLMs also output biased results , a widespread concern [ 56 , 97 , 117 , 140 ] . For example , work on screen - play generation found predictable and stereotypical outputs , with multiple instances of gender biases [ 108 ] . Furthermore , LLM train - ing on human data increases the risk of accidental plagiarism [ 108 ] . LLMs also have unique limitations . On creative tasks , LLMs sometimes produce outputs that are perceived as robotic [ 97 ] and that can lack the understanding , nuance , and subtext that people can provide [ 108 ] . Furthermore , models can get stuck in generative loops , degenerating text quality [ 116 ] . LLMs also hallucinate more than crowds [ 32 , 33 , 108 , 110 , 111 , 116 , 140 ] , and may struggle to change their outputs through self - reflection [ 88 ] . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . LLMs have comparative strengths as well . They may be better at divergent thinking [ 88 , 140 , 143 ] and comparison - based instruc - tions [ 142 , 143 ] than crowdworkers , who are affected more by anchoring bias . LLMs may also be better than crowdworkers at tasks requiring recollection of large amounts of data [ 116 ] . User . Finally , users of the workflow can also be involved to revise or guide the process . Users can be novices using the workflow to adapt to a steep learning curve [ 29 , 78 , 90 , 120 , 135 ] , or experts using the workflow as a support tool [ 108 , 109 , 113 ] . Involving users in completing subtasks grants them greater control at the expense of higher effort [ 129 ] . However , even with user involvement , prior work finds mixed results on how well tools retain the user’s sense of agency and ownership [ 69 , 108 ] . 3 . 4 Using the design space All categories and subcategories of the design space , which emerged through open coding , exist in both the crowdsourcing workflow and LLM chaining literatures . This section covers how designers can use the design space’s interdependent tiers to build LLM chains with an objectives - centered approach . We also discuss gaps in the literature , suggesting future work . To exemplify this process , we follow the design of two hypothet - ical workflows as running examples : Workflow A performing an accuracy - based task with high resource constraints , and Workflow C performing a creativity - based task with few resource constraints . By following an objectives based approach , the workflows traverse the design space differently . 3 . 4 . 1 Objectives and the quality - cost tradeoff . With a task in mind , designers can define their outcome qualities and resource constraints . Defining these objectives also defines how the chain must respect a quality - cost tradeoff . Mechanisms for improving output quality can be more expensive [ 62 , 150 ] and take longer [ 23 , 47 ] . Some workflows have parameters for tuning this quality - cost tradeoff , such as giving the user control over the number of redundant generations [ 37 , 62 , 116 , 150 ] , parallelization of work on different parts of the input [ 84 , 98 , 157 ] , beam search [ 32 , 146 , 150 ] and decision - theoretic choices of the number of responses to obtain [ 5 , 18 , 35 ] . However , this tradeoff is inherent to building a workflow due to the cost of querying for responses , and it impacts the degree to which strategies can be implemented . Workflow A must be more mindful of the quality - cost tradeoff and prioritize consistently correct outcome qualities , while Work - flow C prioritizes unconstrained idea generation as its primary creativity - focused outcome quality . 3 . 4 . 2 Objective - informed strategy choices . The objectives inform how to utilize strategies within given resource constraints . An accuracy - focused outcome quality will often use aggregated di - verse responses [ 38 , 88 , 140 ] and focus quality control on avoiding cascading errors and factual inaccuracies [ 92 , 93 , 146 ] . Workflows for high - stakes accuracy tasks , which prioritize consistency , trans - parency , and debuggability , can prefer rigid and task - specific de - signs [ 122 , 140 , 141 ] . With limited resources , prioritizing the quality control strategy over the diverse responses and workflow flexibility strategies better suits the needs of tasks requiring high accuracy . Workflows for creative outcomes use response diversity to improve divergent thinking and enable richer user interactions [ 15 ] , and they use quality control methods to prioritize the coherency and qualities of the output . For example , verification steps in creative tasks focus on whether or not outputs meet given quality requirements , rather than if they are correct [ 29 , 30 , 60 , 61 ] . Flexible workflows provide the ability to run creative tasks without pre - defining all edge cases , although the work must be easily decomposed [ 73 ] . With limited resources , prioritizing reducing architecture rigidity , if possible , and sourcing a creative diversity of responses can support creative outcome quality goals . When considering strategies informed by objectives , Workflow A prioritizes many verification steps for quality control , then ensures enough workflow rigidity for high consistency . With remaining resources , it can source diverse responses for accuracy . In contrast , Workflow C generates diverse responses as its primary strategy . 3 . 4 . 3 Connecting strategies to tactics . With strategies in mind , de - signers then turn to tactics : how to implement and parameterize a workflow . Many connections between strategies and tactics are intuitive : redundant architectures source diverse responses , evalu - ate subtasks can be a verification step , and dynamic architectures create non - rigid workflows . Others are less intuitive . For exam - ple , consistency between parallel workflows serve as a verification step [ 89 , 116 ] , redundant architectures help with user revision [ 15 ] , and communicative architectures source and evaluate diverse cre - ative responses [ 8 , 27 , 123 ] . Approaching strategies with objectives in mind also informs tactics . For example , sourcing diverse responses for a creative task with coherency as a quality goal would prioritize iterative , rather than parallel , redundant architectures . Workflow A’s architecture uses rigid subtasks to ensure consis - tency , but , to increase efficiency , incorporates a dynamic element controlling the number of redundant generations based on available resources . Workflow A uses evaluate subtasks to verify correctness . Workflow C also uses redundant architectures to source diverse responses but instead resolves disagreements in those generations with transform subtasks to iteratively refine and improve the ideas . 3 . 4 . 4 Gaps between literatures in the design space . Another util - ity of the design space is to compare literatures to see if there are elements of crowdsourcing that can better inform LLM chains . Some parts of the design space include more varied approaches in the crowdsourcing literature , including tasks’ outcome qualities , approaches to reducing workflow rigidity such as free - form com - municative architectures , and subsets of approaches to user revision such as creating controllable outputs . Crowdsourcing research also studies a wider array of specific tactical designs , such as the ef - fects of parallel and iterative redundant architectures with different parameters [ 7 , 46 , 62 , 73 , 92 , 98 ] . To take advantage of this exten - sive crowdsourcing workflow history , it would be useful to know if designers can incorporate findings from these studies into chaining . Since LLMs are different from crowdworkers , it is unclear from the literature alone what will effectively transfer to chaining and what will not . Some studies begin to compare LLM chains and crowdsourcing workflows [ 116 , 143 ] , but much remains unknown . To better understand the extent to which crowdsourcing can in - form a more expansive and detailed design space for chaining , we adapt crowdsourcing workflows into LLM chains ( Section 4 ) . We Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 … Generate SelectBest Categorize Worker - completed Taxonomize parameters Parameters Vote threshold Taxonomize Automated Remove underspecified Format verification Cascade - LLM Crowdsourced Cascade Legend 1→t1→t m - t→m m - t→m 11 11 … … 11 11 … 11 11 … … 11 11 … … 11 11 … 11 11 … 1→t m - t→m 1 m 1 m 1 n 1 m 1 m 1 m 1 m Ensure every input goes through entire workflow Verification step … Generate for each individual item Dynamically adapt parameters Figure 6 : Case study 1 : Cascade [ 28 ] for taxonomy creation . Crowdsourcing workflows can inspire LLM workflows , but the differing capabilities of crowdworkers and LLMs requires changes to the design . We adapt the crowdsourcing workflow Cascade with elements from our design space to better work for LLMs . For example , we found that generating responses for individual items , rather than sets of items , is a more appropriate subtask because it created better response diversity . Through these adaptations , we can control the output taxonomy shape with parameters . then synthesize our findings from the literature and our case study experiences to propose what does ( Section 5 . 1 ) and does not ( Sec - tion 5 . 2 ) transfer from crowdsourcing to chaining and the resulting implications for future work . 4 CASE STUDIES In order to better assess how crowdsourcing methods transfer to LLM chain development , we adapt crowdsourcing workflows to LLM chains in three case studies . We document insights from this process and map our decisions into the design space . The case stud - ies foray into an underexplored part of the LLM space : supporting quality control through user revision in which the user can directly manipulate workflow outputs . With directly manipulable outputs , users can make “rapid , incremental , reversible operations whose impact on the object of interest is immediately visible” [ 128 ] , in - creasing control over output elements and helping users explore alternative results [ 63 ] . By adapting this subset of the user revision strategy from crowdsourcing , we explore how and if such a con - cept can transfer to chains and what modifications are necessary . Prior non - chaining LLM work explores controllability as a helpful attribute [ 70 , 99 ] , so here we seek to incorporate it within chains . We choose three popular crowdsourcing tasks that span the ver - ifiable creativity and creativity sections of the design space quality objectives . We implement Cascade [ 28 ] to build taxonomies with manipulable structures , Soylent [ 15 ] to shorten text to a variety of target lengths , and Mechanical Novel [ 68 ] to control the elements of a story . Diagrams of the workflows , as well as our adaptations to use LLMs , can be found in Figures 6 , 8 , and 10 . For each case study , we first document how the workflow was changed to incorporate LLMs and map those changes to the design space . Common limitations we faced include LLM context lengths , inconsistent output formatting , lack of diversity in model responses , and models struggling to perform the workflow - defined subtasks . We adapted to these limitations with more granular subtasks , more diverse prompts , comparative tasks , and hard - coded verification steps . Across all three case studies , we use different prompts at each parallel generation step to increase response diversity , update worker instructions through trial and error to get the right formatting from ArXiv , preprint , 2023 Grunde - McLaughlin , et al . 2 4 6 8 10 12 14 16 18 20 Target number of categories 100 50 0 50 100 E rr o r ( % ) Reaching category targets Zero - shot - target Cascade - LLM Figure 7 : Controllability of Cascade - LLM versus zero - shot taxonomies . Manipulating Cascade - LLM outputs through given parameters can more precisely achieve a wide range of category sizes than indicating a target value within a zero - shot prompt . Across three datasets , Cascade - LLM exhibits low deviation from the target number of categories , while prompting with zero - shot gives noisy responses , often with too few categories . The shaded region indicates the 95 % con - fidence interval . the LLM outputs , and structure voting evaluation subtasks to choose among numbered options to minimize the impact of hallucinations . We compare our LLM workflows to zero - shot baselines in terms of output control precision , salient qualities for each task , and the latency and number of API calls . To prioritize human - preferred LLM outputs , we use an instruction - tuned model : OpenAI’s API gpt - 3 . 5 - turbo with default parameters [ 115 ] . Compared to zero - shot prompting , chained workflows use more resources but can improve output quality . These workflows also provide a greater precision of output control than parameterized zero - shot baselines . 4 . 1 Case study 1 : Taxonomy creation For our first case study , we explore taxonomy creation , the task of organizing a set of items into a category tree to aid sensemaking . This task falls under the verifiable creativity aspect of the design space : categories need to be correct , but most items can fit into mul - tiple categorizations of different levels of precision . In our design , we focus on the ability to control the level of granularity of these categorizations by controlling the taxonomy shape . 4 . 1 . 1 Workflow design . We adapt the Cascade [ 28 ] crowdsourc - ing workflow ( Figure 6 ) , which takes as input a list of items and outputs a tree mapping items to categories . The workflow begins by selecting a subset of 32 items , generating category suggestions for each item , selecting the best categories among the generated options , and confirming which among all categories works for each item . The next step turns the item - category assignments into a taxonomy , deleting small categories and merging similar categories . The workflow then updates the categorizations for the entire list of items with this new assignment . The workflow iterates with a new subset of items until all items are categorized , building a progressively larger set of item categorizations . To build Cascade - LLM , our version of Cascade powered by LLMs , we made multiple changes to the original workflow . We highlight some of the most salient modifications . To achieve diverse cat - egorizations , we improved response diversity by giving different prompts to parallel generations and ensuring that every item is in - put into a category generation step . At first , the categories were too broad . Specifying category qualities via targeted prompts proved unsuccessful , so instead we added a programmatic validation step to delete categories that included “and , ” “or , ” commas , or slashes . As this hard - coded validation step worked , it was not necessary to include another LLM call . The original workflow gives workers sets of 8 items for which to generate 8 categories . However , the LLM would try categorizing multiple items at once , consistent with prior findings that LLMs struggle to disambiguate multiple subtasks in a prompt [ 101 , 143 ] . To increase diversity , we found the appropriate subtask design includes one item categorization . Finally , the origi - nal workflow also requires parameters for the minimum category size and a merge threshold . To support user revision through direct manipulation , we dynamically apply these parameters at the last taxonomization stage . 4 . 1 . 2 Evaluation setup . We compare Cascade - LLM against zero - shot baselines on three lists of nouns . Datasets . Our experiments use the labels from the scenes in MIT Indoor Scenes [ 121 ] , object categories in MSCOCO [ 91 ] , and in CIFAR100 [ 76 ] datasets . These label sets are of size 67 , 80 , and 100 , respectively . The original Cascade paper explores three datasets sourced from Quora . com question responses . Our LLM workflow is capable of processing these larger datasets , as only individual items need to be within the model’s context size ; however , the Quora . com data exceeds the context limit for the zero - shot baselines . Baselines . We compare our workflow to two baselines . The zero - shot baseline directly prompts the model to create a taxonomy . This baseline establishes how the model completes the task given no restrictions . The zero - shot - target baseline further specifies the target number of categories in the prompt . With this baseline , we compare the ability to control workflow outputs in a single prompt . 4 . 1 . 3 Workflows enable direct manipulation . To measure the con - trollability of the taxonomy’s shape , we measure how well methods can change the taxonomy size ( the number of categories ) . Given a target number of categories ( range : 2 – 20 ) , we measure the percent error between the method’s closest option and the target value . As seen in Figure 7 , one run of Cascade - LLM almost always achieves a precise match with the desired number of categories . In contrast , zero - shot - target is close but usually unable to produce a taxonomy with the target number of categories . The zero - shot condition produces small taxonomies the size of which is not con - trollable ( MIT Indoor Scenes : 2 , MSCOCO : 8 , CIFAR100 : 4 ) . 4 . 1 . 4 Workflows reduce hallucinated and forgotten items . We also evaluate the quality of the outputs through qualitative evaluation . Similar to the original paper , we hand - code taxonomies without knowledge of which method created it . First , we look for errors in Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Table 1 : Cascade - LLM versus zero - shot performance across three datasets . Though the quality of categorizations is similar between methods , the Cascade - LLM generated taxonomies include more categories and have greater taxonomy depth . MIT Indoor Scenes [ 121 ] MSCOCO [ 91 ] CIFAR100 [ 76 ] Zero - shot Cascade - LLM Zero - shot Cascade - LLM Zero - shot Cascade - LLM # of Categories 2 21 8 24 4 19 Tree Depth 1 3 1 4 1 3 Duplicate Categories 0 0 0 0 0 0 Missing Category - Item 0 2 1 3 7 2 Incorrect Category - Item 0 0 1 2 2 2 Missing Parent - Child N / A 0 N / A 1 N / A 0 Incorrect Parent - Child N / A 0 N / A 1 N / A 1 items across all baselines . Specifically , we see if the output halluci - nates or forgets any items . We then code the zero - shot and default - parameter Cascade - LLM baselines for the same mistakes considered in the original paper : duplicate categories , missing parent - child re - lationships , and incorrect parent - child relationships . Cascade - LLM does not forget any items , but 83 . 72 % of zero - shot taxonomies do not include all items . Cascade - LLM cannot halluci - nate new items , while zero - shot hallucinates 51 items across the 129 taxonomies . Cascade - LLM taxonomies have similar error rates to zero - shot taxonomies on duplicate categories and missing / incorrect category - item and parent - child pairs , yet are richer taxonomies with more categories and depth ( Table 1 ) . 4 . 1 . 5 Workflows have higher costs . We measure generation time and the number of API calls to an LLM . Cascade - LLM takes more time ( mean : 2 , 642 . 58 sec . , std : 655 . 81 ) than the zero - shot ( mean : 24 . 35 sec . , std : 5 . 04 ) and zero - shot - target ( mean : 21 . 69 sec . , sd : 4 . 49 ) baselines . Crowdsourced Cascade took 43 hours and 3 minutes to taxonomize three item sets [ 28 ] . Cascade - LLM makes more model calls ( mean : 18 , 898 . 33 , std : 4472 . 61 ) than the one call from the zero - shot and zero - shot - target baselines . Most of these calls are short in terms of token length , as they compare one item to one category . Crowdsourced Cascade reports 1760 calls to workers per iteration . For our goals , the increased cost in time and computation enables direct manipulation on more detailed categorizations without item forgetting or hallucination . 4 . 2 Case study 2 : Text shortening The next case study is the task of shortening text . Unlike sum - marization , shortening the text involves making edits to create a condensed version of the text that attempts to preserve the full meaning . Shortening is a task with verifiable creativity , as short - ened versions must stay faithful to the original text , but there are many options of what phrases to shorten and how to shorten them . By implementing a workflow to generate many shortening options , an end user can control the final length of a paragraph dynamically . 4 . 2 . 1 Workflow design . To shorten text we adapt the Find - Fix - Verify workflow ( Figure 8 ) introduced in Soylent [ 15 ] . In the “Find” step , crowdworkers select areas that might be shortened . If at least two workers agree on a text span , that span progresses to the “Fix” step . In the “Fix” step , crowdworkers make edits to shorten the text . In the final “Verify” step , more crowdworkers verify the possible changes by rejecting by majority vote any edits deemed grammat - ically incorrect or that change the meaning of the sentence . The workflow outputs edit options for multiple spans in the paragraph , which in turn may have multiple possible edits . Soylent defaults to showing the shortest possible option , but also provides a slider for the user to adjust the final output as desired . We made multiple adjustments to produce our Soylent - LLM workflow . First , we ran into context limits and parallelization con - straints with longer texts . In response , we apply a dynamic approach that splits the workflow into chunks of 10 sentences and applies the workflow to each . Second , Soylent allows workers to fix inputs by editing , merging , and deleting phrases . LLMs performed better when we split these tasks into separate prompts , so we further decompose Find and Fix into finer grained editing , merging , and deleting steps to get an appropriate subtask size . Finally , in the Find step the LLM would sometimes “hallucinate” phrases to shorten that do not exist in the text or would change quotes . To mitigate these errors , we add several hard - coded verification steps : we en - sure that the phrase to replace exists in the input text , that the replacement is shorter , and that no quotes were altered or added . Recent work also implemented three unique variations of a Find - Fix - Verify workflow using LLM chains [ 143 ] . Their evaluation , us - ing student preferences , found mixed results on effectiveness for creating better shortenings than a zero - shot baseline . We evaluate our workflow along different dimensions : controllability of output length , fluency , grammaticality , what type of content is deleted , and workflow resource usage . Unlike this prior work , we also retain the parallel generation and validation steps from the original paper . 4 . 2 . 2 Evaluation setup . For evaluating Soylent - LLM , we use texts from newspapers and from the original Soylent paper [ 15 ] . We compare Soylent - LLM with three zero - shot baselines . Datasets . We use the XSUM summarization dataset [ 112 ] . We only use the input texts because our task is shortening , not matching gold - standard summarizations . We use 100 randomly sampled texts from XSUM , with the requirement that examples are more than 500 words long . For qualitative analysis , we use the texts from the Soylent evaluation [ 15 ] . Through this dataset , we can compare the Soylent - LLM shortenings to Crowdsourced Soylent shortenings . Baselines . The zero - shot baseline prompts the model to shorten the paragraph directly . For the zero - shot - target baseline , we specify the target output length in a zero - shot prompt . In the zero - shot - ffv baseline , we ask the LLM to explicitly perform Find - Fix - Verify steps in one prompt . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . Target Length Parameters Soylent - LLM Crowdsourced Soylent Overlap Threshold Format verification Automated Fix Find Verify Worker - completed Legend … … … delete merge edit delete edit … … Split into chunks Verificationstep Split subtasks into individual queries Figure 8 : Case study 2 : Soylent [ 15 ] for text shortening . When using LLMs , the crowdsourcing workflow Soylent initially output low quality responses and ran over context limits . In response , we build Soylent - LLM to have appropriately specified subtasks , to work within context limits , and to verify there were no hallucinated quotes . 4 . 2 . 3 Workflows enable length manipulation . We first compare how well the various LLM approaches reach a desired output length . We represent target lengths through word counts of 100 − 500 words . In initial experiments , we found that zero - shot baselines perform more competitively when asked to shorten to a given word count rather than to a length percentage and hence use word counts in targeted prompts . Nevertheless , we also measure accuracy in terms of a target percentage of length . We calculate the target percent length by dividing the target number of words by the input text’s number of words , and we calculate the actual percent length as the output text’s number of words divided by the input text’s number of words . We compare the percent difference ( i . e . , error ) between the actual and target percent lengths . We find that as text length increases , Soylent - LLM can more precisely meet the target length ( Figure 9 ) . When shortening text , the zero - shot prompt condition consistently outputs approximately 100 word long shortenings ( mean : 126 . 14 , std : 52 . 63 ) . Thus , for longer target lengths , this method achieves lower performance . Zero - shot - target is similarly best at lengths close to 100 words , but it still shows a lack of precision in reaching longer target lengths . Soylent - LLM and zero - shot - ffv both struggle to hit shorter targets . This result matches the Soylent paper , in which output texts were between 78 - 90 % of the original text length [ 15 ] . Soylent - LLM can more precisely hit the larger target lengths than any of the baselines . We find similar trends measuring percent error with the percent length reduction required to meet the target length . Soylent - LLM and zero - shot - ffv can perform poorly with large reductions , in which the target % is a small fraction of the input text , while zero - shot and zero - shot - target perform worse with smaller reductions . In summary , Soylent - LLM does a better job of tightening long texts but is less well - suited to extreme shortenings that cross over into summarization . In addition , Soylent - LLM is unique in providing options among all these different targets without having to requery the model . Moreover , at each target length , there are often multiple possible output texts . 4 . 2 . 4 Workflow outputs have abrupt transitions but lower perplexity . We measure the average perplexity of output texts as a proxy for fluency . We use a different language model ( Facebook’s OPT - 1 . 3b ) than the worker model ( OpenAI’s gpt - 3 . 5 - turbo ) for an external evaluation . We also perform a qualitative analysis of the shortening procedure . We use the five texts used for evaluation in the original Soylent paper [ 15 ] . We compare the edits made with the shortest output from Soylent - LLM , the baseline conditions , and the crowd - sourced condition’s output provided by the authors of the original paper . Without knowing the condition , we coded the outputs for fluency and grammatical errors , following the qualitative evalua - tion procedure of the original paper . We additionally coded what content changed during shortening . Soylent - LLM has the better result of a lower perplexity ( mean : 11 . 69 , std : 3 . 77 ) than zero - shot ( mean : 15 . 37 , std : 5 . 73 ) , zero - shot - target ( mean : 12 . 31 , std : 3 . 91 ) , and zero - shot - ffv ( mean : 16 . 19 , std : 5 . 68 ) . This result surprised us , as zero - shot is fully generated by a large language model fine - tuned to provide fluent responses . As with the original crowdsourcing workflow , some shortenings diminished fluency or introduced grammatical errors . Zero - shot , zero - shot - ffv and Soylent - LLM shortenings all included passive voice , but were otherwise grammatical . However , Soylent - LLM and zero - shot - ffv both could contain abrupt endings or transi - tions . This abruptness is due to separately editing parts of the input text . Crowdsourced Soylent induced grammatical errors as well as abrupt transitions . We found that zero - shot and Soylent - LLM both shorten by removing unnecessary phrasing and deleting examples and justifications . Zero - shot more successfully merges sentences than Soylent - LLM , while Soylent - LLM changes less text and more successfully retains the user’s voice , rather than formal - izing it . Soylent - LLM’s changes affect a larger portion of the text than Crowdsourced Soylent . In sum , Crowdsourced Soylent made 42 edits to phrases and 12 to entire sentences , while Soylent - LLM made 12 edits to phrases and 36 to entire sentences . Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Figure 9 : Text shortening results for Soylent - LLM and zero - shot baselines . A 0 % error indicates outputs that exactly match a target length , and the shaded region ( left ) indicates the 95 % confidence interval . Naïve zero - shot prompting without length guidance averages between 100 - 200 words , regardless of the desired target length . With length guidance added to the prompt , zero - shot - target does substantially better at reaching all targets , especially when the target length is short . Soylent - LLM struggles to hit short targets , but is quite precise at longer lengths and when reducing only a short amount relative to the original length . The zero - shot - ffv baseline , which instructs the model to perform Find - Fix - Verify steps within one prompt , behaves more similarly to the Soylent - LLM workflow but is consistently less accurate . 4 . 2 . 5 Workflows have higher costs than zero - shot prompting . We measure the time it takes to generate the outputs , as well as the number of API calls to the model . The number of API calls varies depending on the length of the input text and the number of items that are selected at each step . Soylent - LLM takes much longer ( mean : 105 . 44 sec . , std : 60 . 16 ) than the zero - shot ( mean : 9 . 09 sec . , std : 2 . 92 ) , zero - shot - target ( mean : 13 . 65 sec . , std : 5 . 47 ) , and zero - shot - ffv ( mean : 43 . 55 sec . , std : 23 . 16 ) baselines . Crowdsourced Soylent takes longest with a reported range of 2 , 640 – 29 , 340 sec . ( 44 - 489 min . ) [ 15 ] . Compared to the one call to the model for the zero - shot baselines , one Soylent - LLM run averages 127 . 11 calls to the model ( std . 63 . 56 ) . Crowd - sourced Soylent reports incorporating 158 – 362 workers . 4 . 3 Case study 3 : Short story generation The final case study concerns story generation given a short text prompt . This task lies on the open - ended creativity side of the output quality spectrum . Short stories have a wide design space of possible plots , characters , messages , imagery , and lessons . In this task , user revision of the story involves higher level control of different plot elements , with the ability to see counterfactuals ( e . g . , if a story element is included or not ) . We implement a workflow that , compared to zero - shot approaches , enables this exploration more precisely , while also creating stories that are preferred by crowdsourced raters for imagery , originality , and style . 4 . 3 . 1 Workflow design . We adapt the Mechanical Novel ( MN ) [ 68 ] workflow for this task . This workflow involves three main steps ( Figure 10 ) . The first step “initializes” a story with multiple scenes based on a given story prompt ( e . g . , “Malcolm finds himself alone in a runaway hot air balloon and accidentally travels to a city in the sky . ” ) . Then , the “reflect” and “revise” steps are iterated five times in a loop . In the “reflect” step , workers consider what changes might helpfully adjust the story . In the “revise” step , workers implement those changes . Each step has subtasks for parallel generation of suggestions and edits , as well as voting for the best generation . We adapted the Crowdsourced MN to MN - LLM by incorporat - ing fine - grained user revision with manipulatable outputs . At the conclusion of the workflow , instead of receiving just one or more separate short stories , the user can interact with different combina - tions of edits to compare stories with different quality combinations and compare counterfactuals on editing decisions . To enable these outputs , we run the “revise” stage on texts of all combinations of suggestions from the “reflect” stage . Beyond introducing control of story elements , the largest chal - lenge was managing token context limitations . Getting diversity for creativity with iterative improvement on texts often increases the text length [ 92 ] . To account for context length , we generate stories with four scenes rather than six and iterate through scenes rather than informing workers to select scenes . At the beginning of the “revise” step , we add a generation subtask that summarizes the cur - rent story to provide abbreviated context in the worker instructions . We included a hard - coded validation step to disqualify edits that were more than 1 . 5 times the input text length . Unfortunately , this disqualified many highly descriptive texts and sometimes meant that no edits were made because all suggestions were too long . Our initial pilots generated editing suggestions that were very generic ( e . g . , “Add more detail to the story” ) and were repetitive across iteration rounds . To improve the diversity of suggestion out - puts , we tried changing the worker instructions to be contrastive by providing previously used “banned” suggestions . However , the out - puts anecdotally seemed more likely to follow those of prior rounds after this change . We hypothesize that the length of the context and request to do two tasks in one ( generate a suggestion and avoid the ArXiv , preprint , 2023 Grunde - McLaughlin , et al . Initialize Write scenes from prompt Reflect Suggestion for story change Revise Edits to make the suggested change Write scene Vote among options Verify quality Propose change Summarize Worker - completed Vote threshold Iteration check Verify length Automated Combination of story elements Parameters Mechanical Novel - LLM Crowdsourced Mechanical Novel Initialize Write scenes from prompt Reflect Suggestion for story change Revise Edits to make the suggested change Legend Revise all suggestion combinations Dynamically adapt plot elements Summarize for worker instructions Length verification step Suggestion quality verification step Figure 10 : Case study 3 : Mechanical Novel [ 68 ] for short story generation . We alter the crowdsourcing workflow Mechanical Novel for LLMs . These adaptations include a new summarization step , more verification steps , and iterations over different input subsets . These changes fit the task within context limits , encourage diverse outputs , and enable control of plot elements . prior suggestions ) led to a lack of diversity . Instead , we add several verification steps to promote suggestion diversity and quality . This subtask granularity worked better . In these evaluate subtasks , we prompted the LLM to ensure that 1 ) the suggestion did not overlap with prior suggestions or two seeded generic suggestions ( “Generic recommendation to introduce conflict . ” and “Generic recommendation to add detail . ” ) , 2 ) the suggestion has not already been implemented in the story , and 3 ) that the suggestion is worded as a suggestion , not new text for the story . 4 . 3 . 2 Evaluation setup . We generate one story in the zero - shot and workflow conditions for each of the five short story prompts in the Mechanical Novel paper [ 68 ] , following their evaluation procedure . Baselines . The zero - shot baseline method prompts the model to generate five suggestions , then write one story that incorporates these suggestions . The zero - shot - combo baseline method prompts the model to generate five suggestions , then write different versions of a story based on combinations of those suggestions . 4 . 3 . 3 Workflows enable control of plot elements . We assess each output to evaluate the number and quality of suggestion combina - tions that emerge from MN - LLM and zero - shot - combo . To measure quality , we calculate the length of texts , as well as the precision and recall of the included suggestions . To measure precision , we calculate what percentage of included suggestions were supposed to be included . To measure recall , we calculate what percentage of suggestions that were supposed to be included were included . As MN - LLM is iterative , some later suggestions reference elements initialized from earlier suggestions , so when coding , we consider the parts of each suggestion that differ from one another . As shown in Table 3 , MN - LLM generates more , longer , and higher quality suggestion combinations than the zero - shot baseline . MN - LLM does not always generate the 33 story options afforded by the initial story and combinations of 5 suggestions , as sometimes no generated suggestions passed our verification steps , or all generated fixes were too long . However , it still generates more than 20 options on average , in comparison to zero - shot’s average of 3 . 8 . 4 . 3 . 4 Workflows generate a wider variety of suggestions . Without knowing the condition , we coded the story suggestions in the MN - LLM and both zero - shot conditions by theme . MN - LLM suggestions were more varied in type than the zero - shot suggestions ( Table 4 ) . The MN - LLM validation steps removed nearly half of the generated suggestions and most often voted for adding plot , characters , or emotional details as the final suggestions . 4 . 3 . 5 The MN - LLM workflow improves imagery , originality , and style . Following the evaluation methods of the MN paper , for each story prompt we gave the output from MN - LLM and the zero - shot baseline , in randomized order , to 200 workers on Amazon Mechanical Turk using EasyTurk [ 75 ] . We asked crowdworkers to vote between stories from the Soylent - LLM and zero - shot baselines along seven story writing dimensions [ 68 ] . We paid workers $ 1 . 50 USD , the equivalent of $ 18 per hour . The Mechanical Novel workflow improved the quality of the output narratives , although sometimes at the expense of a concise plot . We find that MN - LLM stories were more highly rated along the Imagery , Originality , and Style , and Overall dimensions . Zero - shot - generated stories gained more votes in the Coherency , Plot , and Technical dimensions ( Table 2 ) . Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Table 2 : Crowdworker preferences between MN - LLM and zero - shot baseline generated stories . More crowdworkers voted for MN - LLM stories in terms of imagery , originality , style and overall preference . However , crowdworkers judged the zero - shot stories to have a more coherent and complete plot , reflecting challenges with outputting coherent answers from workflows . Imagery Coherency Plot Originality Style Technical Overall MN - LLM 124 97 93 137 118 97 112 Zero - shot 76 103 107 63 82 103 88 Table 3 : Controllability of story plot elements with MN - LLM and zero - shot baselines . We find that the MN - LLM work - flow creates more combinations of long story options . Both approaches incorporate intended story elements at similar rates ( Recall ) , but the zero - shot baseline more often includes unintended story elements ( Precision ) . Zero - shot MN - LLM Avg . # combinations 3 . 80 20 . 40 Avg . length in words 276 . 38 642 . 93 Precision 77 . 05 97 . 74 Recall 88 . 68 90 . 00 Table 4 : MN - LLM and zero - shot generate suggestion types for story elements . The MN - LLM workflow first generates a suggestion , which must pass a series of verification steps . The workflow then votes on valid suggestions to decide which to incorporate . At all stages , MN - LLM provides a wider variety of story suggestions than zero - shot . Suggestion type Zero - shot MN - LLM Passed Voted Add plot events 25 30 13 6 Introduce character 14 18 13 3 Introduce significant object 6 1 1 0 Add plot twist 4 11 10 3 Add detail to interactions 1 6 3 1 Add emotional detail 0 17 8 3 Add background details 0 8 5 0 Add a different perspective 0 5 2 2 Add a moral lesson 0 2 1 1 Add story detail 0 2 1 0 4 . 3 . 6 Workflows can struggle with coherence . Without knowing the condition , we also reviewed the MN - LLM and zero - shot conditions for plot irregularities . Long - form plot coherence of creative LLM outputs is difficult to achieve [ 108 , 148 ] , and we find some plot irregularities in the MN - LLM outputs . Three stories revealed the same information multiple times . Two plots had disjointed elements , one in which the same pronoun “she” referred to two characters in confusing ways and one with a time and location skip due to a scene that was never selected for editing . Such plot irregularities occur in the MN - LLM outputs because of the iteration through scenes and the use of a story summary as context . Both of these design choices were made to avoid exceeding LLM context limits . 4 . 3 . 7 Workflows have higher costs . We measure the time and num - ber of API calls required to run the workflow compared to running the baselines . Compared to zero - shot ( mean : 21 . 88 sec . , std : 5 . 89 ) , MN - LLM takes much longer in seconds to complete ( mean : 1 , 452 . 97 sec . , std : 930 . 26 ) . Compared to zero - shot’s one call , MN - LLM makes 2 , 398 LLM calls on average . The crowdsourced Mechanical Novel paper does not report latency or number of workers [ 68 ] . 5 DISCUSSION The approach of decomposing a task into a series of subtasks is conceptually the same for both crowdsourcing workflows and LLM chains . Through overlap in the literature and our case study obser - vations , we find that the high - level concepts of the design space apply to both , with the exception of the choice of workers . However , LLMs and people have different abilities , strengths , and weaknesses , so we may need different approaches for tactic designs to effectively implement the same strategies that achieve desired objectives . Thus , objectives , strategies , and some types of tactics transfer di - rectly from crowdsourcing to LLMs . However , tactical design decisions can vary significantly , and could benefit from im - proved development and testing processes . We first discuss transferrable approaches to user revision and creative work from crowdsourcing that could inspire a broader range of LLM chains . We then delve into findings from the literature and our case studies on how LLMs chain designs can differ from workflows powered by crowdworkers . Finally , the process itself of designing LLM chains is different because the running of LLMs is cheaper and faster . Using our experience from the case studies , we identify future directions to support chain design and prevent design tunneling . Finally , we summarize several concurrent papers investigating the intersection of LLMs and crowdsourcing . 5 . 1 What transfers from crowdsourcing From the literature , we see that many tasks and all categories of the design space appear for both crowdsourcing workflows and LLM chains . The quality - cost tradeoff also appears in both literatures , although LLMs are less costly to query [ 132 ] . Through our case studies , we find that we can also transfer new tasks ( e . g . , short - ening and taxonomy creation ) and underexplored strategies ( e . g . , direct manipulation , a subset of user revision ) from crowdsourcing to chaining . The transferrability of these concepts can help build new chains from scratch using a broader swath of literature and past experience as guidance . Given past success in the literature survey and our case studies , we expect more approaches to tasks , objectives , strategies , and some tactics to transfer from crowdsourc - ing to chaining . In particular , transferring approaches to user revision and creative work from crowdsourcing could expand chaining capabilities . 5 . 1 . 1 Incorporating more user revision . Although many chaining papers discuss involving humans into the chain hypothetically [ 126 , ArXiv , preprint , 2023 Grunde - McLaughlin , et al . 134 , 140 , 145 , 148 ] , few implement this user oversight [ 69 , 108 , 117 , 155 ] . Our case studies showcase one example of interactions through directly manipulable outputs . Crowdsourcing workflows integrate the user in more varied ways , including providing results without aggregation that the user can sort and filter [ 90 , 109 ] , user guidance for prototyping [ 25 , 78 , 123 ] , and worker - initiated user revision [ 124 ] . Intervention - based user revision is especially helpful for accuracy tasks . Users can break ties [ 120 ] and intervene if there are cascading errors [ 77 ] . Sometimes disagreements may not need to be resolved within the workflow and can instead support an explorable interface for users [ 23 , 25 , 109 , 139 ] . In creative tasks , quick iteration and choos - ing among uncertain options reduces the effort it takes for the user to prototype creative outputs and allows the user to guide ideation . We recommend incorporating more approaches to user revision such as explorable interfaces , worker - initiated revision , and guided prototyping . 5 . 1 . 2 Supporting a broader range of approaches for creative work . Crowdsourcing studies currently support a wider range of tasks for creative work , under the verifiable creativity and creativity objec - tives , with a wider range of approaches . We expect these approaches to transfer to chaining and bring myriad benefits . Fewer studies include creative tasks in chaining ( 15 papers or 38 . 5 % ) than in crowdsourcing ( 51 papers or 75 % ) . Future work could attempt creative tasks that are under - explored in chaining , such as : prototyping [ 78 , 85 , 133 ] , categorizing [ 18 , 23 , 62 ] , topic concept maps [ 53 , 95 , 98 ] , project development [ 102 ] , or more specific tasks like itinerary creation [ 153 ] and conceptual blends [ 29 ] . Crowdsourcing work also uses a broader range of communicative and dynamic architectures to support creative work . Giving workers roles during communication can assist both crowdsourcing work - flows [ 1 , 123 , 133 ] and LLM chains [ 140 ] . In crowdsourcing work , some studies go beyond giving a role in instructions and instead create free - form role - based [ 113 , 133 , 154 ] hierarchical [ 1 , 123 ] and state - sharing [ 78 , 82 , 83 , 85 , 102 , 113 ] communicative architectures . Dynamic architectures provide more flexibility when contingen - cies arise [ 66 , 73 , 77 , 133 ] and adapt to edge cases [ 32 , 53 , 60 , 67 , 69 , 71 , 77 , 111 , 117 , 126 , 141 , 143 , 150 , 158 ] . LLM chains rarely imple - ment dynamic architectures for creative work [ 69 , 143 , 150 , 155 ] . Crowdsourcing work applies dynamic architectures to a wider vari - ety of creative tasks [ 29 , 73 , 82 , 123 , 133 ] and includes more complex dynamic workflows such as multi - level map - reduce [ 73 ] and dynam - ically choosing between workflows using their maximum expected value of utility [ 46 ] . Decision theoretic techniques , such as Partially Observable Markov Decision Processes , yield great efficiencies in crowdsourcing [ 34 , 35 ] , but have thus far barely been considered for LLM chaining [ 100 ] . We recommend pursuing a broader range of creative tasks with more communicative ( e . g . establishing a hierarchy of roles ) and dy - namic ( e . g . multi - level map reduce ) architectures . 5 . 2 What does not transfer from crowdsourcing Although strategic aspects of workflow task decomposition transfer , the differences between crowdworkers and LLMs lead to different optimal tactical design choices in the selection and implementation of architectures and subtasks . Examples of tactical design decisions include determining the number of optimal parallel generations , performing a series of single verification subtasks instead of one multi - faceted subtask , and the text used for worker instructions in a subtask . Compared to crowdworkers , LLMs can process and generate fluent text more quickly per subtask . They best re - spond to different , often more constrained , task instructions and require more direction to generate and verify with di - verse responses . We highlight several instances in which these differences affect optimal tactical design choices and studies that could guide future work towards understanding how LLMs respond to different tactical design choices . 5 . 2 . 1 Sufficient context must not overcomplicate subtasks . Provid - ing global context to subtasks is widely agreed to be a critical element of subtask design and output coherence , but it can increase instruction complexity [ 6 , 98 , 124 , 141 ] . However , initial findings from the literature survey and case studies suggest that LLMs can respond poorly to long , complicated instructions [ 38 , 101 , 116 , 143 ] . This context limitation should be re - evaluated over time as LLM context windows grow . For example , we wanted to provide Mechanical Novel the entire short story thus far as context , as occurs in Crowdsourced Mechan - ical Novel . To generate diverse suggestions for improvement we gave the LLM previous suggestions as context for what it should avoid . However , adding this requirement to the overall prompt anecdotally increased the output’s chance of anchoring to prior suggestions . Therefore , we made a separate evaluate subtask that determined if the new suggestion matched previous suggestions out of context from the rest of the story . We recommend designers take into consideration that LLMs may not adequately process all the nuances of complicated instructions . Simplifying the subtask size in turn simplifies instructions . 5 . 2 . 2 LLMs need different , often simpler , subtasks . Throughout our case studies , we often needed to break crowdsourcing subtasks into further subtasks . Multiple factors necessitated these changes : nat - ural language is difficult to structure ( e . g . , thus , we query for one item category at a time rather than eight in Cascade ) , LLMs have dif - ficulty balancing multiple requirements in a subtask [ 101 , 143 ] ( e . g . , in response , we split Soylent’s “Fix” step into separate edit , merge , and delete queries ) , and LLMs do not understand length - related con - straints ( e . g . , to address this , we add a hard - coded verification step after a generate subtask in Mechanical Novel to check for length ) . In our case studies , LLMs performed better on highly specified subtasks for chains rather than fewer , more complicated subtasks . 5 . 2 . 3 Sourcing diverse responses requires more overhead but may offer more control . Crowdsourcing workflows get diverse responses by querying the crowdsourcing platform multiple times and re - ceiving the suggestion of a different worker each time . LLMs have the capability to output diverse responses , as their training data includes information produced by a wide variety of people . Al - though LLMs give stochastic responses , our initial case study expe - riences suggest that querying an LLM multiple times may output responses with less diversity than querying multiple crowdworkers . Methods for sourcing diverse responses include prompt diversity , verification checks to judge uniqueness , temperature adjustments , and using a variety of models . Parallel and iterative architectures Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 source different degrees of diversity and quality of responses in crowdsourcing [ 92 ] . Although these takeaways may not transfer directly , redundant architecture design choices will also impact LLM response diversity . Although sourcing diverse responses requires more specific prompt and workflow design with LLMs , initial results from role - based prompts suggest an opportunity for finer - grained control of LLMs [ 136 , 140 , 154 ] . Critical challenges remain around what tactical design decisions can generate usefully diverse responses with LLMs . Future studies can measure the impact of these design decisions to more effectively harness the response diversity strategy . To source diverse responses , we recommend further studies on how prompt diversity , model choices , role - based prompts , and architectures can source diverse responses from LLMs . 5 . 2 . 4 Simple verification methods are often insufficient . Providing verification in crowdsourcing workflows often involves requesting a different worker to verify if errors or desirable qualities exist in outputs . Some studies show that prompting a model to reflect can help achieve higher accuracy outputs [ 45 , 101 , 146 , 150 ] . However , others caution against the same model verifying its own outputs , as LLMs exhibit high confidence for incorrect prior outputs [ 38 , 88 ] . We found mixed results through our case studies on the effective - ness of a model verifying its own output . These mixed results from the literature and our case studies imply that more robust verifica - tion methods could be especially useful when working with LLMs . Some approaches for more robust verification include : verifying with a different model [ 88 ] , debate - based structures [ 38 , 88 ] , struc - tural verification such as similarity across parallel streams [ 38 , 89 ] , sets of specific evaluate subtasks , and hard - coded verification . Compared to crowdsourcing , we recommend more robust meth - ods for verifying quality , such as model variety and debate - based communicative architectures . 5 . 2 . 5 LLM chains need new design studies . These observations are some of many differences in design requirements . More systematic studies on optimal chain design would be valuable . Several studies already approach questions around model choice [ 38 , 100 ] , incor - porating roles into prompts [ 117 , 140 ] , and using self - reflection or debate to validate outputs [ 38 , 88 ] . Although the outcomes may be different , systematic crowdsourcing studies could provide in - spiration for some of these explorations . Prior studies investigate the effect of context timing , architecture , and content on task out - put [ 98 , 124 ] . Others measure trade - offs of iterative and parallel workflows ( e . g . , effect on latency , quality averages and variance , risk of cascading errors ) in various task contexts , as well as good prac - tices for each ( e . g . , appropriate number of branches or iterations and what context to share amongst branches ) [ 7 , 23 , 46 , 62 , 73 , 92 , 98 ] . By specifying a strategy of interest , the design space can orient studies for understanding optimal tactical design decisions . For example , studying debate designs could look at debate to estab - lish flexibility for tasks with many edge cases , debate to resolve disagreements , or debate to encourage a diversity of creative ideas . Evaluating the use of tactic designs within the context of specific strategies will lead to more generalizable findings than evaluating solely on task accuracy . We recommend systematic tests for tactical design decisions in - formed by prior crowdsourcing work and the design space . 5 . 3 New opportunities for LLM chain design Since LLMs are cheaper and faster to query [ 132 ] , there are more opportunities to find optimal designs through systematic testing . While the lower barrier to entry may allow LLM chain designers to iterate more quickly , it doesn’t guarantee that they focus their attention on the right things while iterating . Since LLMs display unpredictable outputs in response to low - level changes , designers may end up fixating on minor tweaks like prompt rewordings rather than exploring the design space of strategies and tactics . To avoid potential design fixation that tunnels into technical detail rather than exploring higher - level design directions , LLM chain designers would benefit from tools that scaffold their design and iteration process . Running an LLM chain is cheaper and faster than running a crowdsourcing workflow , but the design process is unstructured . There is a need for design support . We see many promising directions to help build chains efficiently and effectively , given that iterations are often based on extensive qualitative analysis and enlightened trial - and - error . As there are many overlapping tactics among workflows , a repos - itory of “battle - tested” workflow operators could allow sharing of quality parts for reuse in new workflows . Such a platform could in - clude operator - level test suites to assist with validation—and could draw on crowdsourcing to create such test suites—that would mon - itor operator output quality in new content areas or with newly released LLMs . Additionally , corresponding workflow design tools could suggest different options to achieve a goal by collecting mul - tiple approaches for sub - parts of the workflow . Other work engineers more optimal LLM prompts by iteratively generating and scoring test prompt candidates using mathematical or natural language optimization functions [ 147 , 159 ] . Future work could adapt these approaches within the context of chaining , for example to optimize for diverse answers . Comparing many workflow designs can be difficult because of the requirement that the output of a previous step match the in - put of the next step . Therefore , removing or adding steps can be a non - trival process . More formalized input and output indicators , potentially with matching prompts , could facilitate easier chain de - velopment . This approximates the approach of Flash Teams which documents the inputs and outputs of steps [ 123 ] . Sketching aids practitioners in rapidly instantiating ideas to explore the possible solution space rather than committing too early to one design direction [ 22 , 36 , 149 ] . Incorporating sketching could aid LLM chain designers in arriving at better solutions , rather than getting stuck in local optima . Sketches are quick , but they also should be low - fidelity and minimal in detail . While LLMs af - ford quick instantiation , they produce high - fidelity output that can distract designers from their primary prototyping goals [ 64 , 80 ] . Op - portunities to support this designing perspective include providing interfaces amenable to rapid chain construction and reconfigura - tion [ 11 , 141 ] and suggesting strategies and tactics when a user may be tunneling on technical details . We recommend continued work on chain design support tools such as repositories of workflow operators , prompting optimization meth - ods for chaining , formalized input - output indicators , and sketching - inspired workflow design approaches . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . 5 . 4 Related work on LLMs and crowdsourcing Concurrent to this paper , a few other groups investigate the in - tersection of crowdsourcing and LLMs . One paper explores the impact of incorporating LLMs into crowdsourcing workflows on requester and crowdworker stakeholders [ 3 ] . Another implements three strategies from crowdsourcing using LLMs for data process - ing tasks [ 116 ] . Closest to our work , the final paper has students implement six crowdsourcing workflows . The paper discusses the effectiveness of these workflows and the impact of the difference between LLM and crowdworker abilities on recommendations for chaining [ 143 ] . We build on this approach of providing recommen - dations based on case study implementations . We widen the scope of our recommendations by first integrating a broad survey of LLM chaining and crowdsourcing workflow literature into a design space and then using case studies to explore open questions in the design space . This broader investigation enables us to build a design space and informs more comprehensive recommendations for navigating design decisions and for orienting future work . These recommen - dations span from higher - level guidance , such as adhering to the quality - cost tradeoff , to lower - level suggestions , such as pursuing architecture - based methods for quality verification . 6 LIMITATIONS Our work addresses crowdsourcing workflows and LLM chains , but it is limited in scope beyond these fields and in discussion of broader impacts . Additionally , our recommendations are contingent upon an imperfect understanding of LLM capabilities , and we support the utility of our case studies through theory instead of a user study . Scope . Our work targets LLM chain design by adapting crowdsourc - ing workflow techniques . However , there are opportunities to look beyond LLMs to multi - modal models as the workers [ 52 , 96 , 130 ] and beyond crowdsourcing to other task decomposition methods as a source of inspiration [ 9 , 10 , 103 ] . Additionally , there may be limitations to the scope of work cov - ered by our review of the LLM chaining literature . Since LLM chain - ing is a relatively new area , researchers have not converged upon established terminology as in the crowdsourcing domain . Our key - word search may not have captured some work that did not use one of our keyword search terms . Broader impacts . We focus on the ability of workflows to serve a beneficial role in supporting various tasks . However , there are potential risks in using LLMs and crowdworkers as support tools that we do not explicitly consider . Several papers in the design space cite concerns around the loss of human jobs and unintended consequences of using LLMs , especially for creative tasks [ 108 , 117 , 140 ] . Crowdsourcing work similarly addressed concerns around replacing expert workers and labor rights for crowdworkers [ 72 ] . Broader discussions about privacy [ 119 , 140 ] , ownership [ 69 , 108 ] , and perceived sense of agency we also leave to future work . Finally , although workflows can improve output quality , results will still be imperfect , so errors from implemented chains could still have harmful impacts [ 41 , 48 ] . Rapid changes of LLM capabilities . The capabilities of LLMs are rapidly changing , so researchers’ understanding of and subsequent recommendations to work with these capabilities may fall out of sync with new model developments . We recommend future work to monitor the efficacy of chain designs as new models are released , with potentially differing capabilities and failure modes . Lack of user evaluation . We build on longstanding HCI theory and practice that demonstrates the benefit of direct manipulation in - terfaces [ 63 , 128 ] . However , our work does not conduct a user evaluation to explicitly test the benefits and drawbacks of our di - rect manipulation case study implementations . Future work may more deeply explore how users interact with LLM chains via direct manipulation and whether these interactions positively impact user control and task outcomes . 7 CONCLUSION We argue that crowdsourcing workflows can be a fruitful source of inspiration for designing LLM chains . We contribute a design space for LLM chain designers that incorporates lessons from the crowd - sourcing workflow literature . This design space both encourages an objectives - based approach to chain design and opens questions about what crowdsourcing decomposition strategies can transfer to LLMs . Through three case studies , we address this knowledge gap about the design space by building an understanding for how crowdsourcing workflows need to be adapted for LLM chains . Fi - nally , we synthesize our findings from the literature and our case study to propose that the categories of the design space transfer , but that different tactical design decisions can be needed to support LLM chains . We find LLM chains need additional support in sub - task design and verification , but also that their comparatively low cost and high speed can allow for faster iteration of designs than in crowdsourcing . Through these findings , we guide future work towards more approaches to user revision and creative tasks , sys - tematic discovery of good practices for tactic design , and artifacts to support the chain design process . ACKNOWLEDGMENTS We thank Michael Bernstein and Joy Kim for providing outputs from the original Soylent and Mechanical Novel papers . We also thank the Allen Institute for Artificial Intelligence for access to Amazon Mechanical Turk resources . REFERENCES [ 1 ] Elena Agapie , Jaime Teevan , and Andrés Monroy - Hernández . 2015 . Crowd - sourcing in the field : A case study using local crowds for event reporting . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 3 . 2 – 11 . [ 2 ] Salman Ahmad , Alexis Battle , Zahan Malkani , and Sepander Kamvar . 2011 . The jabberwocky programming environment for structured social computing . In Proceedings of the 24th annual ACM symposium on User interface software and technology . 53 – 64 . [ 3 ] GARRETT ALLEN , GAOLE HE , and UJWAL GADIRAJU . 2023 . Power - up ! What Can Generative Models Do for Human Computation Workflows ? ( 2023 ) . [ 4 ] Abdullah Alshaibani , Sylvia Carrell , Li - Hsin Tseng , Jungmin Shin , and Alexan - der Quinn . 2020 . Privacy - preserving face redaction using crowdsourcing . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 8 . 13 – 22 . [ 5 ] Abdullah Alshaibani and Alexander J Quinn . 2021 . Pterodactyl : Two - Step Redaction of Images for Robust Face Deidentification . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 9 . 27 – 34 . [ 6 ] Vamshi Ambati , Stephan Vogel , and Jaime Carbonell . 2012 . Collaborative work - flow for crowdsourcing translation . In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work . 1191 – 1194 . [ 7 ] PaulAndré , RobertEKraut , andAniketKittur . 2014 . Effectsofsimultaneousand sequential work structures on distributed collaborative interdependent tasks . In Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 139 – 148 . [ 8 ] Paul André , Haoqi Zhang , Juho Kim , Lydia Chilton , Steven Dow , and Robert Miller . 2013 . Community clustering : Leveraging an academic crowd to form coherent conference sessions . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 1 . 9 – 16 . [ 9 ] Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Dan Klein . 2016 . Neural module networks . In Proceedings of the IEEE conference on computer vision and pattern recognition . 39 – 48 . [ 10 ] Obinna Anya . 2015 . Bridge the gap ! What can work design in crowdwork learn from work design theories ? . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing . 612 – 627 . [ 11 ] Ian Arawjo , Priyan Vaithilingam , Martin Wattenberg , and Elena Glassman . 2023 . ChainForge : An open - source visual programming environment for prompt engineering . In Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology . 1 – 3 . [ 12 ] Simran Arora , Avanika Narayan , Mayee F Chen , Laurel J Orr , Neel Guha , Kush Bhatia , InesChami , FredericSala , andChristopherRé . 2022 . Askmeanything : A simple strategy for prompting language models . arXiv preprint arXiv : 2210 . 02441 ( 2022 ) . [ 13 ] SSandraBae , ClementZheng , MaryEttaWest , EllenYi - LuenDo , SamuelHuron , and Danielle Albers Szafir . 2022 . Making data tangible : A cross - disciplinary design space for data physicalization . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . 1 – 18 . [ 14 ] Michael S Bernstein , Joel Brandt , Robert C Miller , and David R Karger . 2011 . Crowdsintwoseconds : Enablingrealtimecrowd - poweredinterfaces . In Proceed - ingsofthe24thannualACMsymposiumonUserinterfacesoftwareandtechnology . 33 – 42 . [ 15 ] Michael S Bernstein , Greg Little , Robert C Miller , Björn Hartmann , Mark S Ackerman , David R Karger , David Crowell , and Katrina Panovich . 2010 . Soylent : a word processor with a crowd inside . In Proceedings of the 23nd annual ACM symposium on User interface software and technology . 313 – 322 . [ 16 ] Danielle Bragg , Naomi Caselli , John W Gallagher , Miriam Goldberg , Courtney J Oka , and William Thies . 2021 . ASL sea battle : gamifying sign language data col - lection . In Proceedings of the 2021 CHI conference on human factors in computing systems . 1 – 13 . [ 17 ] Jonathan Bragg , Mausam , and Daniel S . Weld . 2018 . Sprout : Crowd - Powered TaskDesignforCrowdsourcing . Proceedingsofthe31stAnnualACMSymposium on User Interface Software and Technology ( 2018 ) . https : / / api . semanticscholar . org / CorpusID : 51948377 [ 18 ] Jonathan Bragg , Daniel Weld , et al . 2013 . Crowdsourcing multi - label classifica - tion for taxonomy creation . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 1 . 25 – 33 . [ 19 ] VirginiaBraunandVictoriaClarke . 2006 . Usingthematicanalysisinpsychology . Qualitative research in psychology 3 , 2 ( 2006 ) , 77 – 101 . [ 20 ] Victor S Bursztyn , David Demeter , Doug Downey , and Larry Birnbaum . 2022 . Learning to Perform Complex Tasks through Compositional Fine - Tuning of Language Models . arXiv preprint arXiv : 2210 . 12607 ( 2022 ) . [ 21 ] Crystal Butler , Stephanie Michalowicz , Lakshmi Subramanian , and Winslow Burleson . 2017 . More than a Feeling : The MiFace Framework for Defining Facial Communication Mappings . In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology . 773 – 786 . [ 22 ] Bill Buxton . 2010 . Sketching User Experiences : Getting the Design Right and the Right Design . Morgan Kaufmann . [ 23 ] Joseph Chee Chang , Saleema Amershi , and Ece Kamar . 2017 . Revolt : Collabora - tive crowdsourcing for labeling machine learning datasets . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . 2334 – 2346 . [ 24 ] Quanze Chen , Jonathan Bragg , Lydia B Chilton , and Dan S Weld . 2019 . Cicero : Multi - turn , contextual argumentation for accurate crowdsourcing . In Proceed - ings of the 2019 chi conference on human factors in computing systems . 1 – 14 . [ 25 ] Justin Cheng and Michael S Bernstein . 2015 . Flock : Hybrid crowd - machine learning classifiers . In Proceedings of the 18th ACM conference on computer supported cooperative work & social computing . 600 – 611 . [ 26 ] Justin Cheng , Jaime Teevan , Shamsi T Iqbal , and Michael S Bernstein . 2015 . Break it down : A comparison of macro - and microtasks . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 4061 – 4064 . [ 27 ] Lydia B Chilton , Juho Kim , Paul André , Felicia Cordeiro , James A Landay , Daniel S Weld , Steven P Dow , Robert C Miller , and Haoqi Zhang . 2014 . Frenzy : collaborative data organization for creating conference sessions . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1255 – 1264 . [ 28 ] Lydia B Chilton , Greg Little , Darren Edge , Daniel S Weld , and James A Landay . 2013 . Cascade : Crowdsourcing taxonomy creation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1999 – 2008 . [ 29 ] Lydia B Chilton , Savvas Petridis , and Maneesh Agrawala . 2019 . VisiBlends : A flexible workflow for visual blends . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 30 ] Maria Christoforaki and Panagiotis Ipeirotis . 2014 . Step : A scalable testing and evaluation platform . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 2 . 41 – 49 . [ 31 ] Karl Cobbe , Vineet Kosaraju , Mohammad Bavarian , Mark Chen , Heewoo Jun , Lukasz Kaiser , Matthias Plappert , Jerry Tworek , Jacob Hilton , Reiichiro Nakano , et al . 2021 . Training verifiers to solve math word problems . arXiv preprint arXiv : 2110 . 14168 ( 2021 ) . [ 32 ] Antonia Creswell and Murray Shanahan . 2022 . Faithful reasoning using large language models . arXiv preprint arXiv : 2208 . 14271 ( 2022 ) . [ 33 ] Antonia Creswell , Murray Shanahan , and Irina Higgins . 2022 . Selection - inference : Exploiting large language models for interpretable logical reasoning . arXiv preprint arXiv : 2205 . 09712 ( 2022 ) . [ 34 ] Peng Dai , Christopher H Lin , Daniel S Weld , et al . 2013 . POMDP - based control of workflows for crowdsourcing . Artificial Intelligence 202 ( 2013 ) , 52 – 85 . [ 35 ] Peng Dai , Daniel Weld , et al . 2010 . Decision - theoretic control of crowd - sourced workflows . In ProceedingsoftheAAAIConferenceonArtificialIntelligence , Vol . 24 . 1168 – 1174 . [ 36 ] Steven P . Dow , Alana Glassco , Jonathan Kass , Melissa Schwarz , Daniel L . Schwartz , and Scott R . Klemmer . 2011 . Parallel Prototyping Leads to Better Design Results , More Divergence , and Increased Self - Efficacy . ACM Trans . Comput . - Hum . Interact . 17 , 4 , Article 18 ( dec 2011 ) , 24 pages . https : / / doi . org / 10 . 1145 / 1879831 . 1879836 [ 37 ] RyanDrapeau , LydiaChilton , JonathanBragg , andDanielWeld . 2016 . Microtalk : Using argumentation to improve crowdsourcing accuracy . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 4 . 32 – 41 . [ 38 ] Yilun Du , Shuang Li , Antonio Torralba , Joshua B Tenenbaum , and Igor Mor - datch . 2023 . Improving Factuality and Reasoning in Language Models through Multiagent Debate . arXiv preprint arXiv : 2305 . 14325 ( 2023 ) . [ 39 ] NouhaDziri , XimingLu , MelanieSclar , XiangLorraineLi , LiweiJian , BillYuchen Lin , Peter West , Chandra Bhagavatula , Ronan Le Bras , Jena D Hwang , et al . 2023 . FaithandFate : LimitsofTransformersonCompositionality . arXivpreprint arXiv : 2305 . 18654 ( 2023 ) . [ 40 ] Nouha Dziri , Sivan Milton , Mo Yu , Osmar R Zaiane , and Siva Reddy . 2022 . On the Origin of Hallucinations in Conversational Models : Is it the Datasets or the Models ? ArXiv abs / 2204 . 07931 ( 2022 ) . https : / / api . semanticscholar . org / CorpusID : 248227301 [ 41 ] Florin Eggmann , Roland Weiger , Nicola U Zitzmann , and Markus B Blatz . 2023 . Implications of large language models such as ChatGPT for dental medicine . Journal of Esthetic and Restorative Dentistry ( 2023 ) . [ 42 ] Jacob Eisenstein , Daniel Andor , Bernd Bohnet , Michael Collins , and David Mimno . 2022 . Honest students from untrusted teachers : Learning an inter - pretable question - answering pipeline from a pretrained language model . arXiv preprint arXiv : 2210 . 02498 ( 2022 ) . [ 43 ] Scott L Fleming , Keith Morse , Aswathi M Kumar , Chia - Chun Chiang , Birju Patel , Emma P Brunskill , and Nigam Shah . 2023 . Assessing the Potential of USMLE - Like Exam Questions Generated by GPT - 4 . medRxiv ( 2023 ) , 2023 – 04 . [ 44 ] Suzanne Fricke . 2018 . Semantic scholar . Journal of the Medical Library Associa - tion : JMLA 106 , 1 ( 2018 ) , 145 . [ 45 ] Zelalem Gero , Chandan Singh , Hao Cheng , Tristan Naumann , Michel Galley , Jianfeng Gao , and Hoifung Poon . 2023 . Self - Verification Improves Few - Shot Clinical Information Extraction . arXiv preprint arXiv : 2306 . 00024 ( 2023 ) . [ 46 ] Shinsuke Goto , Toru Ishida , and Donghui Lin . 2016 . Understanding crowd - sourcing workflow : modeling and optimizing iterative and parallel processes . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 4 . 52 – 58 . [ 47 ] Sai Gouravajhala , Jinyeong Yim , Karthik Desingh , Yanda Huang , Odest Jenkins , and Walter Lasecki . 2018 . Eureca : Enhanced understanding of real environ - ments via crowd assistance . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 6 . 31 – 40 . [ 48 ] Kai Greshake , Sahar Abdelnabi , Shailesh Mishra , Christoph Endres , Thorsten Holz , and Mario Fritz . 2023 . Not what you’ve signed up for : Compromising Real - World LLM - Integrated Applications with Indirect Prompt Injection . arXiv preprint arXiv : 2302 . 12173 ( 2023 ) . [ 49 ] Anhong Guo , Xiang’Anthony’ Chen , Haoran Qi , Samuel White , Suman Ghosh , Chieko Asakawa , and Jeffrey P Bigham . 2016 . Vizlens : A robust and interactive screen reader for interfaces in the real world . In Proceedings of the 29th annual symposium on user interface software and technology . 651 – 664 . [ 50 ] Anhong Guo , Jeeeun Kim , Xiang’Anthony’ Chen , Tom Yeh , Scott E Hudson , Jennifer Mankoff , and Jeffrey P Bigham . 2017 . Facade : Auto - generating tactile interfaces to appliances . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . 5826 – 5838 . [ 51 ] Anhong Guo , Junhan Kong , Michael Rivera , Frank F Xu , and Jeffrey P Bigham . 2019 . Statelens : A reverse engineering solution for making existing dynamic touchscreens accessible . In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology . 371 – 385 . [ 52 ] Tanmay Gupta and Aniruddha Kembhavi . 2023 . Visual programming : Com - positional visual reasoning without training . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 14953 – 14962 . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . [ 53 ] Nathan Hahn , Joseph Chang , Ji Eun Kim , and Aniket Kittur . 2016 . The Knowl - edge Accelerator : Big picture thinking in small pieces . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 2258 – 2270 . [ 54 ] Kenji Hata , Ranjay Krishna , Li Fei - Fei , and Michael S Bernstein . 2017 . A glimpse far into the future : Understanding long - term crowd worker quality . In Proceed - ings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . 889 – 901 . [ 55 ] Ari Holtzman , Peter West , Vered Shwartz , Yejin Choi , and Luke Zettlemoyer . 2021 . Surface form competition : Why the highest probability answer isn’t always right . arXiv preprint arXiv : 2104 . 08315 ( 2021 ) . [ 56 ] Qing Huang , Yishun Wu , Zhenchang Xing , He Jiang , Yu Cheng , and Huan Jin . 2023 . Adaptive Intellect Unleashed : The Feasibility of Knowledge Transfer in Large Language Models . arXiv preprint arXiv : 2308 . 04788 ( 2023 ) . [ 57 ] Qing Huang , Jiahui Zhu , Zhilong Li , Zhenchang Xing , Changjing Wang , and Xiwei Xu . 2023 . Pcr - chain : Partial code reuse assisted by hierarchical chaining of prompts on frozen copilot . In 2023 IEEE / ACM 45th International Conference on Software Engineering : Companion Proceedings ( ICSE - Companion ) . IEEE , 1 – 5 . [ 58 ] Qing Huang , Zhou Zou , Zhenchang Xing , Zhenkang Zuo , Xiwei Xu , and Qinghua Lu . 2023 . AI Chain on Large Language Model for Unsupervised Con - trol Flow Graph Generation for Statically - Typed Partial Code . arXiv preprint arXiv : 2306 . 00757 ( 2023 ) . [ 59 ] Ting - Hao Huang , Joseph Chee Chang , and Jeffrey P Bigham . 2018 . Evorus : A crowd - powered conversational assistant built to automate itself over time . In Proceedings of the 2018 CHI conference on human factors in computing systems . 1 – 13 . [ 60 ] Ting - Hao Huang , Walter Lasecki , and Jeffrey Bigham . 2015 . Guardian : A crowd - powered spoken dialog system for web apis . In Proceedings of the AAAI Confer - ence on Human Computation and Crowdsourcing , Vol . 3 . 62 – 71 . [ 61 ] Yi - Ching Huang , Jiunn - Chia Huang , Hao - Chuan Wang , and Jane Hsu . 2017 . Supporting ESL writing by prompting crowdsourced structural feedback . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 5 . 71 – 78 . [ 62 ] Jordan S Huffaker , Jonathan K Kummerfeld , Walter S Lasecki , and Mark S Ackerman . 2020 . Crowdsourceddetectionofemotionallymanipulativelanguage . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 14 . [ 63 ] Edwin L Hutchins , James D Hollan , and Donald A Norman . 1985 . Direct manip - ulation interfaces . Human – computer interaction 1 , 4 ( 1985 ) , 311 – 338 . [ 64 ] Ellen Jiang , Kristen Olson , Edwin Toh , Alejandra Molina , Aaron Donsbach , Michael Terry , and Carrie J Cai . 2022 . PromptMaker : Prompt - Based Prototyping with Large Language Models . In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems ( New Orleans , LA , USA ) ( CHI EA ’22 ) . Association for Computing Machinery , New York , NY , USA , Article 35 , 8 pages . https : / / doi . org / 10 . 1145 / 3491101 . 3503564 [ 65 ] Harmanpreet Kaur , Mitchell Gordon , Yiwei Yang , Jeffrey Bigham , Jaime Teevan , Ece Kamar , and Walter Lasecki . 2017 . Crowdmask : Using crowds to preserve privacy in crowd - powered systems via progressive filtering . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 5 . 89 – 98 . [ 66 ] Joy Kim and Andres Monroy - Hernandez . 2016 . Storia : Summarizing social media content based on narrative theory using crowdsourcing . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing . 1018 – 1027 . [ 67 ] Juho Kim , Phu Tran Nguyen , Sarah Weir , Philip J Guo , Robert C Miller , and Krzysztof Z Gajos . 2014 . Crowdsourcing step - by - step information extraction to enhance existing how - to videos . In Proceedings of the SIGCHI conference on human factors in computing systems . 4017 – 4026 . [ 68 ] Joy Kim , Sarah Sterman , Allegra Argent Beal Cohen , and Michael S Bernstein . 2017 . Mechanical novel : Crowdsourcing complex work through reflection and revision . In Proceedings of the 2017 acm conference on computer supported cooperative work and social computing . 233 – 245 . [ 69 ] Jeongyeon Kim , Sangho Suh , Lydia B Chilton , and Haijun Xia . 2023 . Metapho - rian : Leveraging Large Language Models to Support Extended Metaphor Cre - ation for Science Writing . In Proceedings of the 2023 ACM Designing Interactive Systems Conference . 115 – 135 . [ 70 ] Tae Soo Kim , Yoonjoo Lee , Minsuk Chang , and Juho Kim . 2023 . Cells , genera - tors , and lenses : Design framework for object - oriented interaction with large language models . In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology . 1 – 18 . [ 71 ] Aniket Kittur , Susheel Khamkar , Paul André , and Robert Kraut . 2012 . Crowd - Weaver : visually managing complex crowd work . In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work . 1033 – 1036 . [ 72 ] Aniket Kittur , Jeffrey V Nickerson , Michael Bernstein , Elizabeth Gerber , Aaron Shaw , JohnZimmerman , MattLease , andJohnHorton . 2013 . Thefutureofcrowd work . In Proceedings of the 2013 conference on Computer supported cooperative work . 1301 – 1318 . [ 73 ] Aniket Kittur , Boris Smus , Susheel Khamkar , and Robert E Kraut . 2011 . Crowd - forge : Crowdsourcing complex work . In Proceedings of the 24th annual ACM symposium on User interface software and technology . 43 – 52 . [ 74 ] Masaki Kobayashi , Hiromi Morita , Masaki Matsubara , Nobuyuki Shimizu , and Atsuyuki Morishima . 2018 . An empirical study on short - and long - term ef - fects of self - correction in crowdsourced microtasks . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 6 . 79 – 87 . [ 75 ] Ranjay Krishna . 2019 . EasyTurk : A Wrapper for Custom AMT Tasks . https : / / github . com / ranjaykrishna / easyturk . [ 76 ] Alex Krizhevsky , Geoffrey Hinton , et al . 2009 . Learning multiple layers of features from tiny images . ( 2009 ) . [ 77 ] Anand Kulkarni , Matthew Can , and Björn Hartmann . 2012 . Collaboratively crowdsourcing workflows with turkomatic . In Proceedings of the acm 2012 con - ference on computer supported cooperative work . 1003 – 1012 . [ 78 ] Anand Kulkarni , Prayag Narula , David Rolnitzky , and Nathan Kontny . 2014 . Wish : Amplifyingcreativeabilitywithexpertcrowds . In ProceedingsoftheAAAI Conference on Human Computation and Crowdsourcing , Vol . 2 . 112 – 120 . [ 79 ] Vivian Lai , Chacha Chen , Alison Smith - Renner , Q Vera Liao , and Chenhao Tan . 2023 . Towards a Science of Human - AI Decision Making : An Overview of Design Space in Empirical Human - Subject Studies . In Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency . 1369 – 1385 . [ 80 ] Michelle S . Lam , Zixian Ma , Anne Li , Izequiel Freitas , Dakuo Wang , James A . Landay , and Michael S . Bernstein . 2023 . Model Sketching : Centering Concepts in Early - Stage Machine Learning Model Design . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( Hamburg , Germany ) ( CHI ’23 ) . Association for Computing Machinery , New York , NY , USA , Article 741 , 24 pages . https : / / doi . org / 10 . 1145 / 3544548 . 3581290 [ 81 ] LangChain . 2022 . LangChain . ( 2022 ) . https : / / www . langchain . com / [ 82 ] Walter S Lasecki , Juho Kim , Nick Rafter , Onkur Sen , Jeffrey P Bigham , and Michael S Bernstein . 2015 . Apparition : Crowdsourced user interfaces that come to life as you sketch them . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 1925 – 1934 . [ 83 ] WalterSLasecki , RachelWesley , JeffreyNichols , AnandKulkarni , JamesFAllen , and Jeffrey P Bigham . 2013 . Chorus : a crowd - powered conversational assistant . In Proceedings of the 26th annual ACM symposium on User interface software and technology . 151 – 162 . [ 84 ] Thomas D LaToza , W Ben Towne , Christian M Adriano , and André Van Der Hoek . 2014 . Microtask programming : Building software with a crowd . In Proceedings of the 27th annual ACM symposium on User interface software and technology . 43 – 54 . [ 85 ] Sang Won Lee , Yujin Zhang , Isabelle Wong , Yiwei Yang , Stephanie D O’Keefe , and Walter S Lasecki . 2017 . Sketchexpress : Remixing animations for more effective crowd - powered prototyping of interactive interfaces . In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology . 817 – 828 . [ 86 ] YoavLevine , ItayDalmedigos , OriRam , YoelZeldes , DanielJannai , DorMuhlgay , Yoni Osin , Opher Lieber , Barak Lenz , Shai Shalev - Shwartz , et al . 2022 . Standing ontheshouldersofgiantfrozenlanguagemodels . arXivpreprintarXiv : 2204 . 10019 ( 2022 ) . [ 87 ] Cheng Li , Mingyang Zhang , Qiaozhu Mei , Yaqing Wang , Spurthi Amba Hom - baiah , Yi Liang , and Michael Bendersky . 2023 . Teach LLMs to Personalize – An ApproachinspiredbyWritingEducation . arXivpreprintarXiv : 2308 . 07968 ( 2023 ) . [ 88 ] Tian Liang , Zhiwei He , Wenxiang Jiao , Xing Wang , Yan Wang , Rui Wang , Yujiu Yang , Zhaopeng Tu , and Shuming Shi . 2023 . Encouraging Divergent ThinkinginLargeLanguageModelsthroughMulti - AgentDebate . arXivpreprint arXiv : 2305 . 19118 ( 2023 ) . [ 89 ] Beatrice Liem , Haoqi Zhang , and Yiling Chen . 2011 . An iterative dual pathway structure for speech - to - text transcription . ( 2011 ) . [ 90 ] Chi - Chin Lin , Yi - Ching Huang , and Jane Yung - jen Hsu . 2014 . Crowdsourced explanations for humorous internet memes based on linguistic theories . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 2 . 143 – 150 . [ 91 ] Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Dollár , and C Lawrence Zitnick . 2014 . Microsoft coco : Common objects in context . In Computer Vision – ECCV 2014 : 13th European Conference , Zurich , Switzerland , September 6 - 12 , 2014 , Proceedings , Part V 13 . Springer , 740 – 755 . [ 92 ] GregLittle , LydiaBChilton , MaxGoldman , andRobertCMiller . 2010 . Exploring iterative and parallel human computation processes . In Proceedings of the ACM SIGKDD workshop on human computation . 68 – 76 . [ 93 ] Greg Little , Lydia B Chilton , Max Goldman , and Robert C Miller . 2010 . Turkit : human computation algorithms on mechanical turk . In Proceedings of the 23nd annual ACM symposium on User interface software and technology . 57 – 66 . [ 94 ] AngliLiu , StephenSoderland , JonathanBragg , C . H . Lin , XiaoLing , andDanielS . Weld . 2016 . Effective Crowd Annotation for Relation Extraction . In North American Chapter of the Association for Computational Linguistics . https : / / api . semanticscholar . org / CorpusID : 10705630 [ 95 ] Ching Liu , Juho Kim , and Hao - Chuan Wang . 2018 . ConceptScape : Collaborative concept mapping for video learning . In Proceedings of the 2018 CHI conference on human factors in computing systems . 1 – 12 . Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows ArXiv , preprint , 2023 [ 96 ] Vivian Liu , Han Qiao , and Lydia Chilton . 2022 . Opal : Multimodal image genera - tion for news illustration . In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology . 1 – 17 . [ 97 ] Tao Long , Dorothy Zhang , Grace Li , Batool Taraif , Samia Menon , Kynnedy Si - mone Smith , Sitong Wang , Katy Ilonka Gero , and Lydia B Chilton . 2023 . Twee - torial Hooks : Generative AI Tools to Motivate Science on Social Media . arXiv preprint arXiv : 2305 . 12265 ( 2023 ) . [ 98 ] Kurt Luther , Nathan Hahn , Steven Dow , and Aniket Kittur . 2015 . Crowdlines : Supporting synthesis of diverse information sources through crowdsourced outlines . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 3 . 110 – 119 . [ 99 ] StephenMacNeil , AndrewTran , JoanneKim , ZihengHuang , SethBernstein , and Dan Mogil . 2023 . Prompt Middleware : Mapping Prompts for Large Language Models to UI Affordances . arXiv preprint arXiv : 2307 . 01142 ( 2023 ) . [ 100 ] Aman Madaan , Pranjal Aggarwal , Ankit Anand , Srividya Pranavi Potharaju , Swaroop Mishra , Pei Zhou , Aditya Gupta , Dheeraj Rajagopal , Karthik Kappa - ganthu , Yiming Yang , Shyam Upadhyay , Mausam , and Manaal Faruqui . 2023 . AutoMix : Automatically Mixing Language Models . https : / / api . semanticscholar . org / CorpusID : 264306222 [ 101 ] Aman Madaan , Niket Tandon , Prakhar Gupta , Skyler Hallinan , Luyu Gao , Sarah Wiegreffe , Uri Alon , Nouha Dziri , Shrimai Prabhumoye , Yiming Yang , et al . 2023 . Self - refine : Iterative refinement with self - feedback . arXiv preprint arXiv : 2303 . 17651 ( 2023 ) . [ 102 ] Narges Mahyar , Michael R James , Michelle M Ng , Reginald A Wu , and Steven P Dow . 2018 . CommunityCrit : inviting the public to improve and evaluate urban design ideas through micro - activities . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 103 ] Jiayuan Mao , Chuang Gan , Pushmeet Kohli , Joshua B Tenenbaum , and Jiajun Wu . 2019 . The neuro - symbolic concept learner : Interpreting scenes , words , and sentences from natural supervision . arXiv preprint arXiv : 1904 . 12584 ( 2019 ) . [ 104 ] Adam Marcus , Eugene Wu , David Karger , Samuel Madden , and Robert Miller . 2011 . Human - powered sorts and joins . The 38th International Conference on Very Large Data Bases ( 2011 ) . [ 105 ] Adam Marcus , Eugene Wu , David R Karger , Samuel Madden , and Robert C Miller . 2011 . Demonstration of qurk : a query processor for humanoperators . In Proceedings of the 2011 ACM SIGMOD International Conference on Management of data . 1315 – 1318 . [ 106 ] Joshua Maynez , Shashi Narayan , Bernd Bohnet , and Ryan McDonald . 2020 . On faithfulness and factuality in abstractive summarization . arXiv preprint arXiv : 2005 . 00661 ( 2020 ) . [ 107 ] Tyler McDonnell , Matthew Lease , Mucahid Kutlu , and Tamer Elsayed . 2016 . Whyisthatrelevant ? collectingannotatorrationalesforrelevancejudgments . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 4 . 139 – 148 . [ 108 ] Piotr Mirowski , Kory W Mathewson , Jaylen Pittman , and Richard Evans . 2023 . Co - Writing Screenplays and Theatre Scripts with Language Models : Evaluation by Industry Professionals . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1 – 34 . [ 109 ] Vikram Mohanty , Kareem Abdol - Hamid , Courtney Ebersohl , and Kurt Luther . 2019 . Second opinion : Supporting last - mile person identification with crowd - sourcing and face recognition . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 7 . 86 – 96 . [ 110 ] Varun Nair , Elliot Schumacher , and Anitha Kannan . 2023 . Generating medically - accurate summaries of patient - provider dialogue : A multi - stage approach using large language models . arXiv preprint arXiv : 2305 . 05982 ( 2023 ) . [ 111 ] Varun Nair , Elliot Schumacher , Geoffrey Tso , and Anitha Kannan . 2023 . DERA : enhancing large language model completions with dialog - enabled resolving agents . arXiv preprint arXiv : 2303 . 17071 ( 2023 ) . [ 112 ] Shashi Narayan , Shay B Cohen , and Mirella Lapata . 2018 . Don’t give me the de - tails , just the summary ! topic - aware convolutional neural networks for extreme summarization . arXiv preprint arXiv : 1808 . 08745 ( 2018 ) . [ 113 ] Michael Nebeling , Alexandra To , Anhong Guo , Adrian A de Freitas , Jaime Teevan , Steven P Dow , and Jeffrey P Bigham . 2016 . WearWrite : Crowd - assisted writing from smartwatches . In Proceedings of the 2016 CHI conference on human factors in computing systems . 3834 – 3846 . [ 114 ] Feng Nie , Meixi Chen , Zhirui Zhang , and Xu Cheng . 2022 . Improving few - shot performanceoflanguagemodelsvianearestneighborcalibration . arXivpreprint arXiv : 2212 . 02216 ( 2022 ) . [ 115 ] LongOuyang , JeffreyWu , XuJiang , DiogoAlmeida , CarrollWainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , et al . 2022 . Traininglanguagemodelstofollowinstructionswithhumanfeedback . Advances in Neural Information Processing Systems 35 ( 2022 ) , 27730 – 27744 . [ 116 ] Aditya G Parameswaran , Shreya Shankar , Parth Asawa , Naman Jain , and Yujie Wang . 2023 . Revisiting Prompt Engineering via Declarative Crowdsourcing . arXiv preprint arXiv : 2308 . 03854 ( 2023 ) . [ 117 ] Joon Sung Park , Lindsay Popowski , Carrie Cai , Meredith Ringel Morris , Percy Liang , and Michael S Bernstein . 2022 . Social simulacra : Creating populated prototypes for social computing systems . In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology . 1 – 18 . [ 118 ] Amy Pavel , Colorado Reed , Björn Hartmann , and Maneesh Agrawala . 2014 . Video digests : a browsable , skimmable format for informational lecture videos . . In UIST , Vol . 10 . Citeseer , 2642918 – 2647400 . [ 119 ] Charith Peris , Christophe Dupuy , Jimit Majmudar , Rahil Parikh , Sami Smaili , RichardZemel , andRahulGupta . 2023 . Privacyinthe TimeofLanguageModels . In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1291 – 1292 . [ 120 ] Mary Pietrowicz , Danish Chopra , Amin Sadeghi , Puneet Chandra , Brian Bai - ley , and Karrie Karahalios . 2013 . CrowdBand : An Automated Crowdsourcing Sound Composition System . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 1 . 121 – 129 . [ 121 ] Ariadna Quattoni and Antonio Torralba . 2009 . Recognizing indoor scenes . In 2009 IEEE conference on computer vision and pattern recognition . IEEE , 413 – 420 . [ 122 ] Justin Reppert , Ben Rachbach , Charlie George , Luke Stebbing Jungwon Byun , Maggie Appleton , and Andreas Stuhlmüller . 2023 . Iterated Decomposition : Improving Science Q & A by Supervising Reasoning Processes . arXiv preprint arXiv : 2301 . 01751 ( 2023 ) . [ 123 ] Daniela Retelny , Sébastien Robaszkiewicz , Alexandra To , Walter S Lasecki , Jay Patel , Negar Rahmati , Tulsee Doshi , Melissa Valentine , and Michael S Bernstein . 2014 . Expert crowdsourcing with flash teams . In Proceedings of the 27th annual ACM symposium on User interface software and technology . 75 – 85 . [ 124 ] Niloufar Salehi , Jaime Teevan , Shamsi Iqbal , and Ece Kamar . 2017 . Communi - cating context to the crowd for complex writing tasks . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing . 1890 – 1901 . [ 125 ] Elliot Salisbury , Ece Kamar , and Meredith Morris . 2017 . Toward scalable social alt text : Conversational crowdsourcing as a tool for refining vision - to - language technology for the blind . In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , Vol . 5 . 147 – 156 . [ 126 ] Timo Schick , Jane Dwivedi - Yu , Zhengbao Jiang , Fabio Petroni , Patrick Lewis , Gautier Izacard , Qingfei You , Christoforos Nalmpantis , Edouard Grave , and Sebastian Riedel . 2022 . Peer : A collaborative language model . arXiv preprint arXiv : 2208 . 11663 ( 2022 ) . [ 127 ] Yang Shi , Xingyu Lan , Jingwen Li , Zhaorui Li , and Nan Cao . 2021 . Communicat - ing with motion : A design space for animated visual narratives in data videos . In Proceedings of the 2021 CHI conference on human factors in computing systems . 1 – 13 . [ 128 ] Ben Shneiderman . 1982 . The future of interactive systems and the emergence of direct manipulation . Behaviour & Information Technology 1 , 3 ( 1982 ) , 237 – 256 . [ 129 ] Otilia Stretcu , Edward Vendrow , Kenji Hata , Krishnamurthy Viswanathan , Vit - torio Ferrari , Sasan Tavakkol , Wenlei Zhou , Aditya Avinash , Emming Luo , Neil Gordon Alldrin , MohammadHossein Bateni , Gabriel Berger , Andrew Bun - ner , Chun - Ta Lu , Javier Rey , Giulia DeSalvo , Ranjay Krishna , and Ariel Fuxman . 2023 . Agile Modeling : From Concept to Classifier in Minutes . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 22323 – 22334 . [ 130 ] Dídac Surís , Sachit Menon , and Carl Vondrick . 2023 . Vipergpt : Visual inference via python execution for reasoning . arXiv preprint arXiv : 2303 . 08128 ( 2023 ) . [ 131 ] Jaime Teevan , Shamsi T Iqbal , and Curtis Von Veh . 2016 . Supporting collab - orative writing with microtasks . In Proceedings of the 2016 CHI conference on human factors in computing systems . 2657 – 2668 . [ 132 ] Christian Terwiesch and Karl Ulrich . [ n . d . ] . M . B . A . Students vs . ChatGPT : Who Comes Up With More Innovative Ideas ? The Wall Street Journal ( [ n . d . ] ) . https : / / www . wsj . com / tech / ai / mba - students - vs - chatgpt - innovation - 679edf3b [ 133 ] Melissa A Valentine , Daniela Retelny , Alexandra To , Negar Rahmati , Tulsee Doshi , and Michael S Bernstein . 2017 . Flash organizations : Crowdsourcing complex work by structuring crowds as organizations . In Proceedings of the 2017 CHI conference on human factors in computing systems . 3523 – 3537 . [ 134 ] Boshi Wang , Xiang Deng , and Huan Sun . 2022 . Iteratively prompt pre - trained language models for chain of thought . arXiv preprint arXiv : 2203 . 08383 ( 2022 ) . [ 135 ] Sitong Wang , Savvas Petridis , Taeahn Kwon , Xiaojuan Ma , and Lydia B Chilton . 2023 . PopBlends : Strategiesforconceptualblendingwithlargelanguagemodels . In Proceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems . 1 – 19 . [ 136 ] Zekun Moore Wang , Zhongyuan Peng , Haoran Que , Jiaheng Liu , Wangchunshu Zhou , Yuhan Wu , Hongcheng Guo , Ruitong Gan , Zehao Ni , Man Zhang , et al . 2023 . Rolellm : Benchmarking , eliciting , and enhancing role - playing abilities of large language models . arXiv preprint arXiv : 2310 . 00746 ( 2023 ) . [ 137 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Fei Xia , Ed Chi , Quoc V Le , Denny Zhou , et al . 2022 . Chain - of - thought prompting elicits rea - soning in large language models . Advances in Neural Information Processing Systems 35 ( 2022 ) , 24824 – 24837 . [ 138 ] Sarah Weir , Juho Kim , Krzysztof Z Gajos , and Robert C Miller . 2015 . Learn - ersourcing Subgoal Labels for How - to Videos . 405 – 416 . DOI : https : / / doi . org / 10 . 1145 / 2675133 . 2675219 ( 2015 ) . [ 139 ] Wesley Willett , Jeffrey Heer , and Maneesh Agrawala . 2012 . Strategies for crowd - sourcing social data analysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 227 – 236 . ArXiv , preprint , 2023 Grunde - McLaughlin , et al . [ 140 ] Qingyun Wu , Gagan Bansal , Jieyu Zhang , Yiran Wu , Shaokun Zhang , Erkang Zhu , BeibinLi , LiJiang , XiaoyunZhang , andChiWang . 2023 . AutoGen : Enabling Next - Gen LLM Applications via Multi - Agent Conversation Framework . arXiv preprint arXiv : 2308 . 08155 ( 2023 ) . [ 141 ] Tongshuang Wu , Ellen Jiang , Aaron Donsbach , Jeff Gray , Alejandra Molina , Michael Terry , and Carrie J Cai . 2022 . Promptchainer : Chaining large language model prompts through visual programming . In CHI Conference on Human Factors in Computing Systems Extended Abstracts . 1 – 10 . [ 142 ] Tongshuang Wu , Michael Terry , and Carrie Jun Cai . 2022 . Ai chains : Transpar - ent and controllable human - ai interaction by chaining large language model prompts . In Proceedingsofthe2022CHIconferenceonhumanfactorsincomputing systems . 1 – 22 . [ 143 ] Tongshuang Wu , Haiyi Zhu , Maya Albayrak , Alexis Axon , Amanda Bertsch , Wenxing Deng , Ziqi Ding , Bill Guo , Sireesh Gururaja , Tzu - Sheng Kuo , et al . 2023 . LLMs as Workers in Human - Computational Algorithms ? Replicating Crowdsourcing Pipelines with LLMs . arXiv preprint arXiv : 2307 . 10168 ( 2023 ) . [ 144 ] TongshuangSherryWu , MichaelTerry , andCarrieJ . Cai . 2021 . AIChains : Trans - parent and Controllable Human - AI Interaction by Chaining Large Language Model Prompts . Proceedings of the 2022 CHI Conference on Human Factors in ComputingSystems ( 2021 ) . https : / / api . semanticscholar . org / CorpusID : 238353829 [ 145 ] YiranWu , FeiranJia , ShaokunZhang , QingyunWu , HangyuLi , ErkangZhu , Yue Wang , Yin Tat Lee , Richard Peng , and Chi Wang . 2023 . An Empirical Study on Challenging Math Problem Solving with GPT - 4 . arXiv preprint arXiv : 2306 . 01337 ( 2023 ) . [ 146 ] Yuxi Xie , Kenji Kawaguchi , Yiran Zhao , Xu Zhao , Min - Yen Kan , Junxian He , and Qizhe Xie . 2023 . Decomposition enhances reasoning via self - evaluation guided decoding . arXiv preprint arXiv : 2305 . 00633 ( 2023 ) . [ 147 ] ChengrunYang , XuezhiWang , YifengLu , HanxiaoLiu , QuocVLe , DennyZhou , and Xinyun Chen . 2023 . Large language models as optimizers . arXiv preprint arXiv : 2309 . 03409 ( 2023 ) . [ 148 ] Kevin Yang , Nanyun Peng , Yuandong Tian , and Dan Klein . 2022 . Re3 : Gener - ating longer stories with recursive reprompting and revision . arXiv preprint arXiv : 2210 . 06774 ( 2022 ) . [ 149 ] Qian Yang , Justin Cranshaw , Saleema Amershi , Shamsi T . Iqbal , and Jaime Teevan . 2019 . Sketching NLP : A Case Study of Exploring the Right Things To Design with Language Intelligence . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300415 [ 150 ] ShunyuYao , DianYu , JeffreyZhao , IzhakShafran , ThomasLGriffiths , YuanCao , and Karthik Narasimhan . 2023 . Tree of thoughts : Deliberate problem solving with large language models . arXiv preprint arXiv : 2305 . 10601 ( 2023 ) . [ 151 ] OmarZaidanandChrisCallison - Burch . 2011 . Crowdsourcingtranslation : Profes - sional quality from non - professionals . In Proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies . 1220 – 1229 . [ 152 ] JD Zamfirescu - Pereira , Richmond Y Wong , Bjoern Hartmann , and Qian Yang . 2023 . Why Johnny can’t prompt : how non - AI experts try ( and fail ) to design LLM prompts . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1 – 21 . [ 153 ] Haoqi Zhang , Edith Law , Rob Miller , Krzysztof Gajos , David Parkes , and Eric Horvitz . 2012 . Human computation tasks with global constraints . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 217 – 226 . [ 154 ] Jieyu Zhang , Ranjay Krishna , Ahmed H Awadallah , and Chi Wang . 2023 . EcoAs - sistant : Using LLM Assistant More Affordably and Accurately . arXiv preprint arXiv : 2310 . 03046 ( 2023 ) . [ 155 ] ZhengZhang , JieGao , RanjodhSinghDhaliwal , andTobyJia - JunLi . 2023 . VISAR : A Human - AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping . arXiv preprint arXiv : 2304 . 07810 ( 2023 ) . [ 156 ] Zhuosheng Zhang , Aston Zhang , Mu Li , and Alex Smola . 2022 . Automatic chain of thought prompting in large language models . arXiv preprint arXiv : 2210 . 03493 ( 2022 ) . [ 157 ] Diliara Zharikova , Daniel Kornev , Fedor Ignatov , Maxim Talimanchuk , Dmitry Evseev , Ksenya Petukhova , Veronika Smilga , Dmitry Karpov , Yana Shishkina , Dmitry Kosenko , et al . 2023 . DeepPavlov Dream : Platform for Building Genera - tive AI Assistants . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Volume 3 : System Demonstrations ) . 599 – 607 . [ 158 ] Denny Zhou , Nathanael Schärli , Le Hou , Jason Wei , Nathan Scales , Xuezhi Wang , Dale Schuurmans , Claire Cui , Olivier Bousquet , Quoc Le , et al . 2022 . Least - to - most prompting enables complex reasoning in large language models . arXiv preprint arXiv : 2205 . 10625 ( 2022 ) . [ 159 ] Yongchao Zhou , Andrei Ioan Muresanu , Ziwen Han , Keiran Paster , Silviu Pitis , Harris Chan , and Jimmy Ba . 2022 . Large language models are human - level prompt engineers . arXiv preprint arXiv : 2211 . 01910 ( 2022 ) .