Changing the Mind of Transformers for Topically - Controllable Language Generation Haw - Shiuan Chang Jiaming Yuan Mohit Iyyer Andrew McCallum CICS , University of Massachusetts Amherst hschang @ cs . umass . edu , jiamingyuan @ umass . edu , { mccallum , miyyer } @ cs . umass . edu Abstract Large Transformer - based language models can aid human authors by suggesting plausi - ble continuations of text written so far . How - ever , current interactive writing assistants do not allow authors to guide text generation in desired topical directions . To address this lim - itation , we design a framework that displays multiple candidate upcoming topics , of which a user can select a subset to guide the gener - ation . Our framework consists of two compo - nents : ( 1 ) a method that produces a set of can - didate topics by predicting the centers of word clusters in the possible continuations , and ( 2 ) a text generation model whose output adheres to the chosen topics . The training of both compo - nents is self - supervised , using only unlabeled text . Our experiments demonstrate that our topic options are better than those of standard clustering approaches , and our framework of - ten generates ﬂuent sentences related to the chosen topics , as judged by automated metrics and crowdsourced workers . 1 Introduction Recently , Transformer - based language models ( LMs ) have achieved impressive performance in language generation tasks ( Radford et al . , 2019 ; Dai et al . , 2019 ) such as open - domain story genera - tion ( See et al . , 2019a ) . When writing with the LM , users often desire an intuitive and effective way to control what a LM is going to generate ( Keskar et al . , 2019 ) . To address this need , interactive writ - ing assistants provide options to reveal possible developments of the story and generate continua - tions guided by the user - selected options . Interactive writing assistants have wide applica - tions in creative writing ( Roemmele and Gordon , 2015 ; Clark et al . , 2018 ; Akoury et al . , 2020 ) , ed - ucation ( Luo et al . , 2015 ) , and gaming ( Walton , 2020 ) . Nevertheless , the existing systems’ options usually do not provide ﬁne - grained control and / or Step 2 : Might say these topics Step 1 : Let’s see what language models would say Step 3 : Please talk more about these topics 1 book books novels 2 Essays Perspectives Perspective 3 University faculty undergraduate 4 Reid Sen . McConnell 5 humanity life spirituality 6 2011 2010 2009 7 know sure want 8 insistence disdain dismissive 9 election elections Democratic 10 U . S . States United Input Prompt : “Barack Obama writes a new book” Output Continuation : “ : The Future of a Democratic Election . The book tells the story of the 2008 election . ” Transformer - based Language Models User Step 4 : Let me try . What does this continuation sound ? Figure 1 : Given an input prompt , the Transformer - based LM provides K = 10 topics that might be men - tioned next and each topic is represented by M = 3 words . The user could guide the generation process by choosing a subset of topics . require substantial human labor . In some prior work ( Keskar et al . , 2019 ; Tu et al . , 2019 ) , users choose among a static set of predeﬁned attributes ( e . g . , sentiment ) that only provide coarse - grained control . Other work ( Roemmele and Gordon , 2015 ; Clark et al . , 2018 ) presents users with multiple generated continuations , which requires substan - tial reading effort and might not contain topics that users want to see . Finally , options could be nodes in a plot graph that are handcrafted ( Luo et al . , 2015 ) or derived from a collaboration between humans and machine ( Li et al . , 2013 ) , but such choices are usually limited due to the high cost of preparing the options . To address these limitations , we propose an in - teractive writing framework that provides a set of topics and guides the text generation by the user - chosen topics . The topic options are generated dynamically based on the input prompt to pro - a r X i v : 2103 . 15335v1 [ c s . C L ] 29 M a r 2021 Input Prompt : Barack Obama writes a new book : The Future of a Democratic Election . The book tells the story of the 2008 election . Topic : election , elections , Democratic Topic : book , books , novels Topic : humanity , life , spirituality on spirituality and the role of religion in society Topic : God , Christ , eternal , entitled My Living With God , and writes that he will give the gift of grace . In it he describes why many Americans believe in political parties . Topic : understand , know , realize Word : story Word : zombie about the United States entitled I Don ' t Care . . . You Bet I ' m a Zombie . Topic : American , America , U . S . Topic : political , ideology , politics . In the United States , many people know the story of the human race Figure 2 : Examples of our generated options and continuations . We highlight the words in the continuation that are related to the chosen topics or to the speciﬁed word . vide ﬁne - grained control , and our models are self - supervised without the need to deﬁne the attributes or collect annotations . As depicted in Figure 1 , a user can peek at the most probable K topics ( shown as bags of words ) appearing after the input prompt and control the generation by choosing the topics . In Figure 2 , we compare multiple generated sen - tences conditioned on different chosen topic ( s ) or speciﬁed word ( s ) . For example , if the user chooses a topic about humanity , life , and spirituality , our system continues the input prompt “ Barack Obama writes a new book ” with “ on spirituality and the roles of religion in society ” . Then , we can use the generated text as the new input prompt and update the set of topics to include other more relevant top - ics such as God , Christ , and eternal . The process can be repeated to create a plot tree . A user can also control the generation by spec - ifying word ( s ) if the user wants to see the words that are not in the topic list or seeks a transition to a word that is not directly related to the input prompt . For example , a user can ask our system to generate a sentence about zombie . Consequently , the continuation of “Barack Obama writes a new book” becomes “about the United States entitled I Don’t Care . . . You Bet I’m a Zombie” . The system is realized by two components : an option generator and a conditional text generator . Given a prompt , the option generator suggests a set of K topics . After a user chooses a subset of the topics and speciﬁes some words , the embedding of every word or topic will guide the conditional text generator to produce the continuation that is both consistent with the existing prompt and relevant to the chosen topics and words . Both components are self - supervised and use pretrained GPT2 models ( Radford et al . , 2019 ) to encode the input prompt . During training , the op - tion generator predicts the cluster centers of fu - ture words , which are in the continuation of the prompt , based on the contextualized embeddings from GPT2 . The conditional text generator ﬁne - tunes GPT2 to predict the next words given the prompt and a few subsequent words . Since both components’ input and output only come from the prompt and its continuation , training the system only requires a raw corpus , word tokenizers , and a list of stop words . This makes the proposed method suitable for open - domain story generation and eas - ily being ﬁne - tuned for a speciﬁc domain . In experiments , we demonstrate that our system recommends high - quality topics and often generate sentences that follow the chosen topics . We com - pare our option generator with global topic models such as LDA ( Blei et al . , 2001 ) or local topic mod - els such as clustering the words in the input prompt . The results show that the proposed method gener - ates signiﬁcantly more topics that are plausible and promote the narrative . Moreover , we compare our conditional text generator with PPLM ( Plug and Play Language Models ) ( Dathathri et al . , 2020 ) and demonstrate that our generation is more ﬂuent and relevant to the chosen topics . Our code is available at https : / / github . com / iesl / interactive _ LM . 2 Method The proposed framework consists of two compo - nents : option generator and conditional text gen - erator . In Figure 3 , we illustrate the two compo - nents and their interaction . First , given the prompt x 1 , . . . , x I inputted by a user , the option generator at the bottom of the ﬁgure outputs K topics . After the user chooses two topics about book and elec - tion and speciﬁes one extra word story , the topics GPT2 Encoder book , books , novels t 5 Linear Layer Weighted average of GloVe t 7 election , elections , Democratic t w story Sample based on probability GloVe GPT2 Encoder Transformer L 1 L 2 L 3 L 4 L 5 L 6 L 7 L 8 L 9 L 10 3 . write T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 T 10 3 . choose 2 . show 1 . write 4 . show ( b ) Option Generator ( a ) Conditional Text Generator User book : new Barack … … book Obama new Barack writes a Softmax p w5 p w6 … p w1 p f6 p f6 p f6 + + + + + + x 1 x 2 x 3 x 4 x 5 x 6 ̂ y 1 c 1 c 2 c 3 c 4 c 5 c 6 c 7 c 8 c 9 c 10 Closest M Words Figure 3 : Our model architectures for ( a ) conditional text generator and ( b ) option generator . During testing , the information ﬂows from the bottom to the top . and word are passed to our text generator as the generation guidance . Accordingly , the generator continues to write the next token (cid:98) y 1 . 1 In the following subsections , we introduce our model designs and the way to train each compo - nent . More implementation details are described in Appendix B . 2 . 1 Option Generator When we do not have labeled attributes in a corpus , we can create options by clustering all the words in a corpus into topics ( Tu et al . , 2019 ) . The clustering could be done by topic modeling approaches such as LDA ( Blei et al . , 2001 ) . The resulting topics are static ( i . e . , the clustering is performed globally 1 The framework is ﬂexible . For example , the GPT2 en - coders in the two components could be shared . Besides topics , the option generator could be extended to predict likely at - tributes in the continuation such as positive sentiment and event frames ( Tu et al . , 2019 ) if the corresponding label data are available in the training corpus . without considering the prompt ) . However , the prompt might have a narrow focus and the related words of interest are all clustered into a single topic . A simple remedy is to cluster only the words in the prompt rather than all the words in the cor - pus . The topics are created dynamically and locally given a prompt and can capture more ﬁne - grained aspects in the continuations . However , the top - ics derived from the prompt might provide less inspiration because the users have seen the prompt . Another major drawback of the approach is that the generated topics might encourage the LM to generate repetitive sentences or make a narrative circle inside a loop . Motivated by the challenges , we propose an op - tion generator that predicts the cluster centers based on the prompt instead of clustering the words in the prompt during testing . 2 . 1 . 1 Model Prediction The goal of our option generator is to predict the K cluster centers of words in the possible continu - ations and use the cluster centers as the topics user could choose from . As in Figure 3 ( b ) , the option generator uses GPT2 to encode the input prompt x 1 , . . . , x I and passes the output embedding to K different linear layers L 1 , . . . , L K . To model the dependency of clusters , a Transformer ( Vaswani et al . , 2017 ) takes the K embeddings as input and predicts the cluster centers c 1 , . . . c K in GloVe ( Pen - nington et al . , 2014 ) space . During testing , each predicted cluster center is normalized by its L2 norm , and we use the M closest words in the normalized GloVe space to represent the topic T i , which users can choose . We choose to learn the cluster centers in GloVe space rather than GPT2 or BERT ( Devlin et al . , 2019 ) space because the non - contextualized word embeddings are easier to visualize . Users can eas - ily understand the meaning of a cluster center by seeing nearby words . We normalize GloVe space in this work to make the squared L2 distance equal to twice the cosine distance between two embeddings . Our architecture is similar to the one in Chang et al . ( 2021 ) , but we use a pretrained GPT2 en - coder rather than train a BERT - like Transformer from scratch . Another difference is that we ignore the connection between the second Transformer and the output of GPT2 to save GPU memory for handling a longer input prompt . GloVe - null null null - American … 2008 … election … of severe . . . African . . . Leak information Randomly selected words GPT2 Encoder + L k + Transformer c 1 ( b ) Option Generator African Obama first Barack becomes the ( a ) Conditional Text Generator - American president in 2008 , in an election held against the backdrop of severe economic problems caused by policies started or worsened under … African Tell GPT2 that the selected words will appear in the future 2008 severe . . . . . . first Barack … American Americans America election elections 2008 2009 2007 voters Republicans Democrats economic north bus A randomly sampled word Push away Pull closer Push away c 2 c 3 c 4 c 5 c 6 c 7 c 8 c 9 c 10 Pull closer x 1 x 2 x 3 x 4 x 5 x 6 y 1 y 2 y 3 y 4 y 5 y 6 y 7 y 8 y 9 y 10 GPT2 Encoder + Softmax A word in continuation Figure 4 : Training our two components using the same sentence . ( a ) We randomly pick n = 3 words in the actual continuation as our conditions for the text generator , and the null labels mean their predicted probabilities are ignored in our loss . ( b ) We visualize 5 out of K = 10 generated topics in a normalized GloVe space . Red words are the ones that appear in the continuation and pull the nearby cluster centers closer during training . 2 . 1 . 2 Model Training In Figure 4 ( b ) , we visualize our training proce - dure . For each input prompt in the training corpus , we run a forward pass through the Transformers and get predicted cluster centers c 1 , . . . c K . Next , we collect 50 words in the continuation ( except stop words ) as positive examples and match the words with cluster centers as in the E - step of the EM algorithm ( Dempster et al . , 1977 ) . We mini - mize the distances between the centers and their nearby positive examples by backpropagating the gradients through the matching and updating our Transformer models . Furthermore , we randomly sample some words as negative examples and max - imize the distances between the cluster centers and their nearby embeddings from negative examples . Using Figure 4 ( b ) as an example , the orange cluster center is pulled closer toward the embed - ding of 2008 , which appears in the continuation . The green cluster center is pushed away from the embedding of north , a randomly sampled word . Since each output embedding c k is pulled by only the nearby embeddings of words in the continua - tion , the output embedding will naturally become the cluster center of the nearby continuation word embeddings . Notice that the related topics like Democrats and Republicans are not observed in the prompt and continuation , but our model can predict a red cluster center close to them because the model can learn from other similar input prompts whose continuation mentions words like Democrats . Chang et al . ( 2021 ) discover that non - negative sparse coding ( NNSC ) ( Hoyer , 2002 ) could en - courage the Transformers to predict more diverse and relevant topics compared with Kmeans , so we adopt NNSC as our clustering loss , and its formu - lation could be found in Chang et al . ( 2021 ) . 2 . 2 Conditional Text Generator After the user chooses topic ( s ) or speciﬁes word ( s ) , each topic or word is converted to a GloVe em - bedding . The component aims to generate the text given the input prompt and the GloVe embeddings of the topics or words we prefer to see in the con - tinuation . Users only see the M words closest to the k th predicted cluster center c k from our option genera - tor , so we compute the k th topic embedding as t k = (cid:80) Mm = 1 cos ( e wm , c k ) e wm | | (cid:80) Mm = 1 cos ( e wm , c k ) e wm | | , ( 1 ) where e wm is the normalized GloVe embedding of the m th closet word and cos ( e wm , c k ) is the cosine similarities between the m th word embedding and the embedding c k . 2 . 2 . 1 Model Prediction During testing , the topic embeddings t k or em - bedding of the speciﬁed words are inserted into GPT2 encoder before x I , the last word piece in the prompt . The inserted embeddings nudge the GPT2 to generate the sentences containing the desired words with a higher probability . As Figure 3 ( a ) shows , the GloVe embeddings are ﬁrst passed through a linear layer to make their dimension become the same as the hidden state size of GPT2 . Then , the transformed embeddings are added with special positional embeddings p fI , which are different from those for the prompt p wi . The special positional embedding tells GPT2 that the inserted embeddings have a different meaning and where the conditional generation starts . The GPT2 encoder’s output goes through a soft - max layer , which computes the probability of each token being observed as the ﬁrst word piece in the continuation y 1 . We adopt top - k sampling ( Fan et al . , 2018 ) , which reduces the chance of sampling words with low probability , to pick the next word , and autoregressively sample one token (cid:98) y o at a time to generate the continuation (cid:98) y 1 , . . . , (cid:98) y O . 2 . 2 . 2 Model Training We train the generator using the continuation of a prompt and some randomly selected non - stop words in the continuation as its generation conditions . Since the continuation contains the randomly - selected words , the generator would be heavily penalized if it ignores the conditions by assigning low probabilities to the selected words in all the continuation positions . An example is illustrated in Figure 4 ( a ) . Given an input prompt in the training set , we randomly pick a number n from 0 to K and sample n words from the next O = 25 words ( except stop words ) . Next , the normalized GloVe embeddings of n words are inserted to the GPT2 encoder before the last word piece in the prompt , and we ignore the output probabilities corresponding to the inserted positions during training . To speed up the training , we conduct the future word insertion in multiple positions of each training text sequence . We insert the future words just before the text that might contain the words rather than at the be - ginning as in the classic seq2seq model , because we do not want the model to learn to generate the continuation based on the future topics that have not yet be speciﬁed by the users ( e . g . , The GPT2 should not know that it will see election in the fu - ture when it learns to generate Barack Obama . . . during training ) . By allowing the LM to see the upcoming words earlier , we leak partial label information to the LM input . Consequently , GPT2 learns to utilize the information and generate the sentence containing the desired words to achieve a lower perplexity loss . Notice that the training method allows us to specify our topical preference without signiﬁcantly scarifying generation efﬁciency and ﬂuency , but it cannot guarantee to generate all the desired topics , especially when we specify multiple ones . One concern of the method is that the LM cannot see all possible sets of topics or words users might specify during training . Besides , each GloVe em - bedding used to supervise LM comes from a single word , but we ask the LM to condition on average GloVe embedding of the top M words during test - ing . Nevertheless , we observe that the LM is often able to generalize well in our experiments because similar words have similar GloVe embeddings , lots of training instances could be easily prepared by the self - supervised method , and our option gener - ator usually provides the topics mentioned in the continuation in our training corpus . 3 Experiments We evaluate two components separately , and both evaluations include automated metrics and human judgment . Throughout the evaluation , the number of topics K = 10 and the length of generations is 50 word pieces . We ﬁnd that ﬁxing K = 10 works well in our experiments . If the possible continuations cover more than 10 topics , our option generator tends to output the important topics . If they cover fewer topics , our option generator tends to output the related topics that are not explicitly mentioned in the prompt or the duplicated topics . More experiment setup details could be found in Appendix C . 3 . 1 Datasets We use 90 % of English Wikipedia 2016 as our train - ing set for both components , 5 % as our validation set to determine the hyperparameters such as the number of epochs , and the remaining 5 % as our test set to perform the automated evaluation . For human evaluation , we collect labels from Amazon Mechanical Turk ( MTurk ) . We randomly sample sentences from the training set of STS benchmark ( STSb ) ( Cer et al . , 2017 ) as our input prompts . Compared with Wikipedia , the sentences from STSb are easier to understand for annotators because a large portion of sentences in Wikipedia involves terminologies , depends on a longer con - text , or might even just be a list of names . In STSb , we sample 24 sentences as our prompts , and each method generates one continuation for each input prompt . Each generated continuation or topics will be scored by three different workers . 3 . 2 Option Generator Evaluation We evaluate the topics from different option genera - tors by judging whether the topics will appear in the continuation and whether the topics would promote the narrative . The goal is to have topics that are relevant and provide new information . The topics that are too similar to the prompt words might be redundant and not helpful because the users have already seen the prompt . 3 . 2 . 1 Automatic Evaluation Metrics • Sim : If the generated topics T can help users to write the continuation , the embedding of every non - stop word in the actual continuation should be similar to the embeddings of a generated topic . Thus , we compute Sim ( ¯ Y , T ) = O (cid:48) (cid:88) o = 1 K max k = 1 ( t k ) T e ¯ yo , ( 2 ) where ¯ Y = { ¯ y o } O (cid:48) o = 1 is a set of non - stop words in the continuation and O (cid:48) = 25 . t k is the normal - ized embedding of k th topic in T from equation 1 and e ¯ yo is the o th word in ¯ Y . • Sim Short : When computing Sim , we use the in - put prompts containing around 180 words on av - erage . To examine the topic quality at the start of writing , where the authors might need assistance the most , we also report Sim ( ¯ Y , T ) on short in - put prompts ( with 35 words on average ) . • Sim Diff : The options that are helpful to users should be sufﬁciently different from the words in the input prompt to promote the narrative and avoid generating repeated content . Thereby , we also evaluate methods using Sim Diff = Sim ( ¯ Y , T ) - Sim ( ¯ X , T ) , where ¯ X = { ¯ x i } I (cid:48) i = 1 are the non - stop words in the input prompt . 3 . 2 . 2 Human Evaluation Our questionnaire shows the prompt and asks which generated topics are likely to appear in Scope Method Sim Sim Short Sim Diff Global Sample 14 . 63 14 . 42 0 . 16 LDA 36 . 86 36 . 02 - 2 . 82 Kmeans 40 . 65 39 . 91 - 3 . 40 Local Sample 41 . 50 41 . 23 - 12 . 51 NNSC 46 . 70 42 . 80 - 15 . 94 Kmeans 47 . 94 43 . 89 - 16 . 12 Ours 48 . 38 46 . 29 0 . 45 Table 1 : Comparison of the option generators using au - tomatic metrics . The best numbers within each scope are highlighted . Scope Method L TP L & TP Global LDA 5 . 76 ± 0 . 50 6 . 24 ± 0 . 33 5 . 26 ± 0 . 31 Kmeans 6 . 94 ± 0 . 36 6 . 13 ± 0 . 30 5 . 96 ± 0 . 31 Local Kmeans 8 . 65 ± 0 . 16 5 . 31 ± 0 . 50 5 . 14 ± 0 . 50 Ours 7 . 85 ± 0 . 25 6 . 96 ± 0 . 26 6 . 75 ± 0 . 28 Table 2 : Comparison of option generators using human judgment ( mean ± standard error ) . L and TP refer to likelihood and topic promotion , respectively . a reasonable continuation and which topics pro - mote the narrative . For each method , we re - port the average number of its topics that are likely to appear ( L ) , promote the topic ( TP ) , and both ( L & TP ) . For example , an MTurk worker is shown three topics generated by a method given a prompt : ABC . The worker thinks A is likely to appear in the continuation and AB promote the topic . Then , L = | { A } | = 1 , TP = | { AB } | = 2 , and L & TP = | { A } ∩ { AB } | = | { A } | = 1 for this prompt . 3 . 2 . 3 Option Generator Baselines We compare our generator with two types of meth - ods . 2 The ﬁrst type performs the clustering glob - ally and selects the most relevant topics to the input prompt from the static set of clusters . We cluster all the words into J = 150 topics by LDA ( Blei et al . , 2001 ) ( LDA - global ) and into J = 1000 topics by Kmeans on the normalized GloVe em - bedding space ( Tu et al . , 2019 ) ( Kmeans - global ) . We also randomly sample K words from the whole vocabulary as our cluster centers ( Sample - global ) . Similar to equation 1 , we ﬁnd the M words with the closest embeddings to each cluster center to represent the topic and compute the topic embed - ding t j as the weighted average embedding of M words in the j th topic . Among all J cluster cen - ters , we pick the K topics with the closest t j to the 2 Another alternative is to generate many continuations and cluster the words in the generation . However , the method takes time , which might be prohibited by limited computational resources and the real - time interaction requirement . Input Prompt The study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s . LDA - global Kmeans - local Ours 1 population , households 6 company , companies 1 Norway , Sweden 6 also , however 1 research , scientiﬁc 6 1980s , 1970s 2 patients , treatment 7 Norwegian , Norway 2 tripled , doubled 7 since , Since 2 tissues , tissue 7 even , though 3 psychology , research 8 story , book 3 nearly , almost 8 Sweden , Finland 3 patients , diagnosis 8 susceptibility , pathogenic 4 police , prison 9 hospital , Hospital 4 cancer , skin 9 study , studies 4 DNA , gene 9 decreased , increased 5 chemical , carbon 10 Icelandic , Iceland 5 1950s , 1940s 10 found , discovered 5 orange , purple 10 Sweden , Norway Table 3 : Comparison of all K topics for the input prompt using M = 2 words closest to each topic . Input Prompt The study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s . Generator Generated Text Option Text LDA - global Ours A study of the Norwegian police has conﬁrmed the cancer case . The law in Norway was the subject of the Kmeans - local Ours The study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s . As well , skin Ours PPLM In this study , a study was conducted conducted in Italy and in Finland . From the 1990s to the 1970s , there None GPT2 The study also revealed that only 20 % of the deaths in Norway were caused by a sudden cardiac response Ours Ours Recent studies have shown that melanin causes a decrease in genetic susceptibility in people in Norway , Table 4 : The continuations that are generated by conditioning on all of K topics from different option generators . The input prompt comes from STSb . prompt embedding , where the prompt embedding is the average embedding of all words in the input prompt . The second type of methods discovers the K topics from the input prompt . We cluster non - stop words in the prompt using non - negative sparse coding ( Hoyer , 2002 ) ( NNSC - local ) and Kmeans ( Kmeans - local ) . We also sample K non - stop words from the prompt and call it Sample - local . Similar to equation 1 , we represent each topic us - ing M words and compute the weighted average of their embeddings t k as the input of our text gen - erator . Notice that the locally clustering methods produce similar results when the prompts come from STSb due to their short lengths , so we only test Kmeans - local in our human evaluation . 3 . 2 . 4 Results In Table 1 , we show that local methods generate the options more relevant to the input prompt than the global methods due to signiﬁcantly higher Sim and Sim Short . Our method performs better com - pared to other local methods , especially in Sim Diff , which highlights the high novelty of our generated topics . The improvement on Sim Short is larger than that on Sim because our method could suggest the related topics that are not explicitly mentioned in the short prompt ( e . g . , U . S . in Figure 1 ) . The human evaluation results are presented in Table 2 . Our method wins in terms of generat - ing relevant topics that promote the narrative . The Kmeans - local performs better in L because most of the words in the input prompts could be men - tioned again in the next sentence . However , it often leads to the redundant topics that are too similar to the prompt . Table 3 compares the options generated by dif - ferent methods while Table 4 compares the text generated using different option generators and text generators . More examples are presented in Ap - pendix D . In Table 3 , we can see that most topics in Kmeans - local do not promote the narrative , which makes the generated continuation become a copy of the input prompt in Table 4 . We will quantitatively evaluate the generated continuations using different option generators in Appendix A . Notice that the high redundancy problem is hard to be solved by a conditional text generator because the relatedness between the prompt and the generated text is hard to be controlled ( See et al . , 2019b ) . 3 . 3 Conditional Text Generator Evaluation To demonstrate our text generator’s effectiveness , we use our option generator to prepare the topic embeddings and randomly select n topics as our conditions to simulate the user’s choice , where n is a random number from 1 to K . The sentences generated by different methods are compared . 3 . 3 . 1 Automatic Evaluation Metrics We match the union of M × K top words in the chosen topics with the words in the generated con - tinuations and count the number of tokens that are matched exactly ( token ) , the number of matched word types ( word ) , and the number of topics that contain at least one matched word ( topic ) to mea - sure the relevancy between the continuations and the chosen topics . Notice that the scores are under - estimated because the generation might mention words in different morphological variations or other Text Automatic Metrics Inference Human Judgement Generation Relevancy Hit Quality Time Relevancy Fluency Method Token Word Topic PPL ( ↓ ) Dist - 1 Dist - 2 s ( ↓ ) Recall Precision Score PPLM 1 . 48 0 . 99 0 . 77 18 . 49 40 . 29 80 . 83 17 . 74 30 . 56 ± 2 . 96 56 . 01 ± 4 . 41 3 . 83 ± 0 . 13 Ours 2 . 36 1 . 79 1 . 40 16 . 39 37 . 98 79 . 65 1 . 02 41 . 46 ± 3 . 47 56 . 41 ± 4 . 41 4 . 07 ± 0 . 10 GPT2 1 . 27 0 . 84 0 . 64 14 . 24 39 . 80 80 . 22 1 . 00 24 . 49 ± 2 . 77 48 . 69 ± 4 . 61 4 . 15 ± 0 . 11 Table 5 : Comparison of conditional text generators . The numbers in Dist - 1 , Dist - 2 , Recall , and Precision are percentages . Lower perplexity ( PPL ) and inference time are better . The better performances between PPLM and our method are highlighted . In human evaluation , we report the mean ± standard error of each method . words related to the topics . The ﬂuency of the generated text is measured using the perplexity ( Serban et al . , 2016 ) of the original GPT2 ( with 345M parameters ) without being ﬁne - tuned on Wikipedia . Dist - n ( Li et al . , 2016 ) is the ratio between the number of unique n - grams and the number of all n - grams in the con - tinuations , where n = 1 or 2 . Higher Dist - n implies more diverse generations . The average inference time per input prompt is also presented . 3 . 3 . 2 Human Evaluation We present the prompt and the generated continu - ation and ask the worker to score the generation’s ﬂuency from 1 ( not ﬂuent at all ) to 5 ( very ﬂuent ) . Next , we show K topics and ask which topics are mentioned in the generation . Treating the worker’s choices as prediction and the topics our model con - ditions on as ground truth , we report the average precision and recall of the prediction . 3 . 3 . 3 Conditional Text Generator Baselines We compare our method with PPLM ( Plug and Play Language Models ) ( Dathathri et al . , 2020 ) due to its strong performance against the weighted de - coding approach from Ghazvininejad et al . ( 2017 ) when the condition is a bag of words . The condition for PPLM is the union of the top M words in the chosen topics and each word’s weight is neglected . We use our generation model without conditioning on any word ( i . e . , n = 0 ) during testing 3 as the base model of PPLM . We also present the performance of the base model itself as a reference to know the signiﬁcance of our improvement ( denoted as GPT2 ) . 3 . 3 . 4 Results Table 5 indicates that our model outperforms PPLM in all metrics except in Dist - 1 and Dist - 2 . We suspect that our model generates slightly less 3 We ﬁnd the model performs similarly compared with the GPT2 with no condition during training . diverse sentences in order to make the generation more relevant to the given topics . The generation might mention a topic even if it is not chosen as a condition , so we achieve similar precision compared to PPLM in human evalua - tion . The recall of PPLM means that only around 30 % of given topics are mentioned . The low recall indicates the difﬁculty of mentioning multiple ran - domly selected topics in the next 50 word pieces while keeping the sentence ﬂuent . By contrast , achieving 40 % on recall demonstrates the effective - ness of our conditional text generator . Compared with PPLM , our model requires an additional training step but achieves low inference time and high relevancy to the given topics / words once the training is ﬁnished . The beneﬁts make it preferable in our interactive writing application . 4 Related Work Different interactive writing assistants provide dif - ferent forms of options to let users express their preferences . The options could be manually de - ﬁned classes ( e . g . , sentiment ) ( Keskar et al . , 2019 ; Dathathri et al . , 2020 ) , semantic frames ( Tu et al . , 2019 ) , or event structures such as ( subject , verb , object , modiﬁer ) ( Martin et al . , 2018 ; Tambwekar et al . , 2019 ; Ammanabrolu et al . , 2020 ) . The forms of options allow users to control the attributes of the generated text but require labels or classiﬁers that map the text to the attributes / options . The options could also be a single query word at the beginning ( Austin , 2019 ) , the article title ( Yan , 2016 ) , politeness ( Niu and Bansal , 2018 ) or speci - ﬁcity ( See et al . , 2019b ) of the text , or the length of the generated sentence ( Tu et al . , 2019 ) . However , the options cannot provide ﬁne - grained control on topical directions of the generated contents . A related research direction is the multi - stage story generation . To make a long story more co - herent , recent work proposes to generate a skele - ton and then generate the full text guided by the skeleton . The skeleton could be a sequence of SRL frames ( Fan et al . , 2019 ) , a sequence of event structure ( subject , verb , object , prepo - sition , modiﬁer ) ( Ammanabrolu et al . , 2020 ) , a story premise ( Fan et al . , 2018 ) , or a story sum - mary ( Chen et al . , 2019 ) . Users can revise the skeleton to control the generated text , but the ap - proaches assume the existence of the skeleton ex - tractor or labels in the training corpus . Besides , the systems cannot suggest options given the par - tial text , which is one of the main focuses of our interactive writing assistant . The skeleton could also be multiple keyphrases . The keyphrases are extracted based on word fre - quency ( Ippolito et al . , 2019 ; Tan et al . , 2020 ; Wu et al . , 2020 ) , an off - the - shelf keyword extraction method ( Peng et al . , 2018 ; Goldfarb - Tarrant et al . , 2019 ; Yao et al . , 2019 ; Rashkin et al . , 2020 ; Zhang et al . , 2020 ) , a sentence compression dataset and reinforcement learning ( Xu et al . , 2018 ) , or image caption datasets and ConceptNet ( Lin et al . , 2020 ) . Most of the studies focus on modeling the long - term dependency among the keyphrases and / or forcing the generation to contain the keyphrases . Instead , we focus on allowing users to determine the topical directions of the generation . Compared with conditioning on keyphrases , our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics . 5 Conclusion We propose an interactive writing assistant that generates topic options given an input prompt and generates the continuation of the prompt given the topics chosen by a user . We decompose the frame - work into two components and propose a novel model for each component . The automated evalua - tion and human evaluation indicate that our system generates many topics that are related to but differ - ent from the prompt , and generates the sentences that are ﬂuent and relevant to the chosen topics . Acknowledgements We thank Ao Liu for his preliminary exploration of this project and Nader Akoury for his helpful feedbacks . We also thank the anonymous reviewers for their constructive feedback . This work was supported in part by the Cen - ter for Data Science and the Center for Intelligent Information Retrieval , in part by the Chan Zucker - berg Initiative under the project Scientiﬁc Knowl - edge Base Construction , in part using high per - formance computing equipment obtained under a grant from the Collaborative R & D Fund managed by the Massachusetts Technology Collaborative , in part by the National Science Foundation ( NSF ) grant numbers DMR - 1534431 and IIS - 1514053 . Any opinions , ﬁndings , conclusions , or recom - mendations expressed in this material are those of the authors and do not necessarily reﬂect those of the sponsor . References Nader Akoury , Shufan Wang , Josh Whiting , Stephen Hood , Nanyun Peng , and Mohit Iyyer . 2020 . STO - RIUM : A Dataset and Evaluation Platform for Machine - in - the - Loop Story Generation . In Proceed - ings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6470 – 6484 , Online . Association for Computational Linguistics . Prithviraj Ammanabrolu , Ethan Tien , Wesley Cheung , Zhaochen Luo , William Ma , Lara J . Martin , and Mark O . Riedl . 2020 . Story realization : Expand - ing plot events into sentences . In The Thirty - Fourth AAAI Conference on Artiﬁcial Intelligence , AAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 , pages 7375 – 7382 . AAAI Press . John Austin . 2019 . The book of endless history : Au - thorial use of GPT2 for interactive storytelling . In Interactive Storytelling - 12th International Confer - ence on Interactive Digital Storytelling , ICIDS 2019 , Little Cottonwood Canyon , UT , USA , November 19 - 22 , 2019 , Proceedings , volume 11869 , pages 429 – 432 . Springer . David M . Blei , Andrew Y . Ng , and Michael I . Jordan . 2001 . Latent dirichlet allocation . In Advances in Neural Information Processing Systems 14 [ Neural Information Processing Systems : Natural and Syn - thetic , NIPS 2001 , December 3 - 8 , 2001 , Vancouver , British Columbia , Canada ] , pages 601 – 608 . MIT Press . Daniel Cer , Mona Diab , Eneko Agirre , Iñigo Lopez - Gazpio , and Lucia Specia . 2017 . SemEval - 2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation . In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval - 2017 ) , pages 1 – 14 , Vancouver , Canada . Association for Computational Linguistics . Haw - Shiuan Chang , Amol Agrawal , and Andrew Mc - Callum . 2021 . Extending multi - sense word embed - ding to phrases and sentences for unsupervised se - mantic applications . In Proceedings of the Twenty - Seventh AAAI Conference on Artiﬁcial Intelligence . Boxing Chen and Colin Cherry . 2014 . A systematic comparison of smoothing techniques for sentence - level BLEU . In Proceedings of the Ninth Workshop on Statistical Machine Translation , pages 362 – 367 , Baltimore , Maryland , USA . Association for Compu - tational Linguistics . Gang Chen , Yang Liu , Huanbo Luan , Meng Zhang , Qun Liu , and Maosong Sun . 2019 . Learning to predict explainable plots for neural story generation . arXiv preprint arXiv : 1912 . 02395 . Elizabeth Clark , Anne Spencer Ross , Chenhao Tan , Yangfeng Ji , and Noah A Smith . 2018 . Creative writ - ing with a machine in the loop : Case studies on slo - gans and stories . In 23rd International Conference on Intelligent User Interfaces . Zihang Dai , Zhilin Yang , Yiming Yang , Jaime Car - bonell , Quoc Le , and Ruslan Salakhutdinov . 2019 . Transformer - XL : Attentive language models beyond a ﬁxed - length context . In Proceedings of the 57th Annual Meeting of the Association for Computa - tional Linguistics , pages 2978 – 2988 , Florence , Italy . Association for Computational Linguistics . Sumanth Dathathri , Andrea Madotto , Janice Lan , Jane Hung , Eric Frank , Piero Molino , Jason Yosinski , and Rosanne Liu . 2020 . Plug and play language models : A simple approach to controlled text generation . In 8th International Conference on Learning Represen - tations , ICLR 2020 , Addis Ababa , Ethiopia , April 26 - 30 , 2020 . OpenReview . net . Arthur P Dempster , Nan M Laird , and Donald B Rubin . 1977 . Maximum likelihood from incomplete data via the em algorithm . Journal of the Royal Statisti - cal Society : Series B ( Methodological ) , 39 ( 1 ) : 1 – 22 . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171 – 4186 , Minneapolis , Minnesota . Associ - ation for Computational Linguistics . Angela Fan , Mike Lewis , and Yann Dauphin . 2018 . Hi - erarchical neural story generation . In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 889 – 898 , Melbourne , Australia . Association for Computational Linguistics . Angela Fan , Mike Lewis , and Yann Dauphin . 2019 . Strategies for structuring story generation . In Pro - ceedings of the 57th Annual Meeting of the Asso - ciation for Computational Linguistics , pages 2650 – 2660 , Florence , Italy . Association for Computa - tional Linguistics . Marjan Ghazvininejad , Xing Shi , Jay Priyadarshi , and Kevin Knight . 2017 . Hafez : an interactive poetry generation system . In Proceedings of ACL 2017 , System Demonstrations , pages 43 – 48 , Vancouver , Canada . Association for Computational Linguistics . Seraphina Goldfarb - Tarrant , Haining Feng , and Nanyun Peng . 2019 . Plan , write , and revise : an interactive system for open - domain story generation . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( Demonstrations ) , pages 89 – 97 , Minneapolis , Minnesota . Association for Computational Linguistics . Patrik O Hoyer . 2002 . Non - negative sparse coding . In Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing . Daphne Ippolito , David Grangier , Chris Callison - Burch , and Douglas Eck . 2019 . Unsupervised hier - archical story inﬁlling . In Proceedings of the First Workshop on Narrative Understanding , pages 37 – 43 , Minneapolis , Minnesota . Association for Com - putational Linguistics . Nitish Shirish Keskar , Bryan McCann , Lav R Varshney , Caiming Xiong , and Richard Socher . 2019 . CTRL : A conditional transformer language model for controllable generation . arXiv preprint arXiv : 1909 . 05858 . Boyang Li , Stephen Lee - Urban , George Johnston , and Mark Riedl . 2013 . Story generation with crowd - sourced plot graphs . In Proceedings of the Twenty - Seventh AAAI Conference on Artiﬁcial Intelligence , July 14 - 18 , 2013 , Bellevue , Washington , USA . AAAI Press . Jiwei Li , Michel Galley , Chris Brockett , Jianfeng Gao , and Bill Dolan . 2016 . A diversity - promoting ob - jective function for neural conversation models . In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies , pages 110 – 119 , San Diego , California . Association for Computational Linguistics . Bill Yuchen Lin , Wangchunshu Zhou , Ming Shen , Pei Zhou , Chandra Bhagavatula , Yejin Choi , and Xiang Ren . 2020 . CommonGen : A constrained text gen - eration challenge for generative commonsense rea - soning . In Findings of the Association for Computa - tional Linguistics : EMNLP 2020 , pages 1823 – 1840 , Online . Association for Computational Linguistics . Ilya Loshchilov and Frank Hutter . 2019 . Decou - pled weight decay regularization . In 7th Inter - national Conference on Learning Representations , ICLR 2019 , New Orleans , LA , USA , May 6 - 9 , 2019 . OpenReview . net . Linbo Luo , Wentong Cai , Suiping Zhou , Michael Lees , and Haiyan Yin . 2015 . A review of interactive narra - tive systems and technologies : a training perspective . Simulation , 91 ( 2 ) : 126 – 147 . Lara J . Martin , Prithviraj Ammanabrolu , Xinyu Wang , William Hancock , Shruti Singh , Brent Harrison , and Mark O . Riedl . 2018 . Event representations for au - tomated story generation with deep neural nets . In Proceedings of the Thirty - Second AAAI Conference on Artiﬁcial Intelligence , ( AAAI - 18 ) , New Orleans , Louisiana , USA , February 2 - 7 , 2018 , pages 868 – 875 . AAAI Press . Tong Niu and Mohit Bansal . 2018 . Polite dialogue gen - eration without parallel data . Transactions of the As - sociation for Computational Linguistics , 6 : 373 – 389 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic eval - uation of machine translation . In Proceedings of the 40th Annual Meeting of the Association for Com - putational Linguistics , pages 311 – 318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics . Nanyun Peng , Marjan Ghazvininejad , Jonathan May , and Kevin Knight . 2018 . Towards controllable story generation . In Proceedings of the First Workshop on Storytelling , pages 43 – 49 , New Orleans , Louisiana . Association for Computational Linguistics . Jeffrey Pennington , Richard Socher , and Christopher Manning . 2014 . GloVe : Global vectors for word representation . In Proceedings of the 2014 Confer - ence on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532 – 1543 , Doha , Qatar . Association for Computational Linguistics . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 . Language models are unsupervised multitask learners . Hannah Rashkin , Asli Celikyilmaz , Yejin Choi , and Jianfeng Gao . 2020 . PlotMachines : Outline - conditioned generation with dynamic plot state tracking . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process - ing ( EMNLP ) , pages 4274 – 4295 , Online . Associa - tion for Computational Linguistics . Melissa Roemmele and Andrew S . Gordon . 2015 . Cre - ative help : A story writing assistant . In Interac - tive Storytelling - 8th International Conference on Interactive Digital Storytelling , ICIDS 2015 , Copen - hagen , Denmark , November 30 - December 4 , 2015 , Proceedings , volume 9445 , pages 81 – 92 . Springer . Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf . 2019 . Distilbert , a distilled version of bert : smaller , faster , cheaper and lighter . In Pro - ceedings of the 5th Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing . Abigail See , Aneesh Pappu , Rohun Saxena , Akhila Yerukola , and Christopher D . Manning . 2019a . Do massively pretrained language models make better storytellers ? In Proceedings of the 23rd Confer - ence on Computational Natural Language Learning ( CoNLL ) , pages 843 – 861 , Hong Kong , China . Asso - ciation for Computational Linguistics . Abigail See , Stephen Roller , Douwe Kiela , and Ja - son Weston . 2019b . What makes a good conver - sation ? how controllable attributes affect human judgments . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1702 – 1723 , Minneapolis , Minnesota . Associ - ation for Computational Linguistics . Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 . Neural machine translation of rare words with subword units . In Proceedings of the 54th An - nual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 – 1725 , Berlin , Germany . Association for Computa - tional Linguistics . Iulian Vlad Serban , Alessandro Sordoni , Yoshua Ben - gio , Aaron C . Courville , and Joelle Pineau . 2016 . Building end - to - end dialogue systems using gener - ative hierarchical neural network models . In Pro - ceedings of the Thirtieth AAAI Conference on Arti - ﬁcial Intelligence , February 12 - 17 , 2016 , Phoenix , Arizona , USA , pages 3776 – 3784 . AAAI Press . Pradyumna Tambwekar , Murtaza Dhuliawala , Lara J . Martin , Animesh Mehta , Brent Harrison , and Mark O . Riedl . 2019 . Controllable neural story plot generation via reward shaping . In Proceedings of the Twenty - Eighth International Joint Conference on Artiﬁcial Intelligence , IJCAI 2019 , Macao , China , August 10 - 16 , 2019 , pages 5982 – 5988 . ijcai . org . Bowen Tan , Zichao Yang , Maruan AI - Shedivat , Eric P Xing , and Zhiting Hu . 2020 . Progressive generation of long text . arXiv preprint arXiv : 2006 . 15720 . Tijmen Tieleman and Geoffrey Hinton . 2012 . Lecture 6 . 5 - rmsprop : Divide the gradient by a running aver - age of its recent magnitude . COURSERA : Neural networks for machine learning , 4 ( 2 ) : 26 – 31 . Lifu Tu , Xiaoan Ding , Dong Yu , and Kevin Gimpel . 2019 . Generating diverse story continuations with controllable semantics . In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 44 – 58 , Hong Kong . Association for Compu - tational Linguistics . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . In Advances in Neural Information Pro - cessing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , December 4 - 9 , 2017 , Long Beach , CA , USA , pages 5998 – 6008 . Nick Walton . 2020 . AI dungeon . Zeqiu Wu , Michel Galley , Chris Brockett , Yizhe Zhang , Xiang Gao , Chris Quirk , Rik Koncel - Kedziorski , Jianfeng Gao , Hannaneh Hajishirzi , Mari Osten - dorf , et al . 2020 . A controllable model of grounded response generation . arXiv preprint arXiv : 2005 . 00613 . Jingjing Xu , Xuancheng Ren , Yi Zhang , Qi Zeng , Xi - aoyan Cai , and Xu Sun . 2018 . A skeleton - based model for promoting coherence among sentences in narrative story generation . In Proceedings of the 2018 Conference on Empirical Methods in Natu - ral Language Processing , pages 4306 – 4315 , Brus - sels , Belgium . Association for Computational Lin - guistics . Rui Yan . 2016 . i , poet : Automatic poetry composition through recurrent neural networks with iterative pol - ishing schema . In Proceedings of the Twenty - Fifth International Joint Conference on Artiﬁcial Intelli - gence , IJCAI 2016 , New York , NY , USA , 9 - 15 July 2016 , pages 2238 – 2244 . IJCAI / AAAI Press . Lili Yao , Nanyun Peng , Ralph M . Weischedel , Kevin Knight , Dongyan Zhao , and Rui Yan . 2019 . Plan - and - write : Towards better automatic storytelling . In The Thirty - Third AAAI Conference on Artiﬁcial Intelligence , AAAI 2019 , Honolulu , Hawaii , USA , January 27 - February 1 , 2019 , pages 7378 – 7385 . AAAI Press . Yizhe Zhang , Guoyin Wang , Chunyuan Li , Zhe Gan , Chris Brockett , and Bill Dolan . 2020 . POINTER : constrained progressive text generation via insertion - based generative pre - training . In Proceedings of the 2020 Conference on Empirical Methods in Nat - ural Language Processing , EMNLP 2020 , Online , November 16 - 20 , 2020 , pages 8649 – 8670 . Associ - ation for Computational Linguistics . Yaoming Zhu , Sidi Lu , Lei Zheng , Jiaxian Guo , Weinan Zhang , Jun Wang , and Yong Yu . 2018 . Texy - gen : A benchmarking platform for text generation models . In The 41st International ACM SIGIR Con - ference on Research & Development in Information Retrieval , SIGIR 2018 , Ann Arbor , MI , USA , July 08 - 12 , 2018 , pages 1097 – 1100 . ACM . Scope Method F NP A Global LDA 3 . 07 ± 0 . 17 2 . 82 ± 0 . 16 3 . 06 ± 0 . 13 Kmeans 3 . 65 ± 0 . 13 3 . 42 ± 0 . 14 3 . 42 ± 0 . 12 Local Kmeans 3 . 71 ± 0 . 13 3 . 56 ± 0 . 15 3 . 39 ± 0 . 13 Ours 3 . 85 ± 0 . 14 3 . 64 ± 0 . 15 3 . 67 ± 0 . 14 Table 6 : Comparison of the continuations generated by different option generators using human judgment ( mean ± standard error ) . F , NP , and A refer to ﬂuency , narrative promotion , and overall , respectively . A Option Generator Comparison Using Generated Continuations To see whether the proposed option generator im - proves the quality of the continuations , we use all of K topics from different methods to guide our conditional text generator and compare their gener - ated continuations . In addition to all the methods we described in Section 3 . 2 . 3 , we also present the results of our text generator without conditioning on any topics ( i . e . , n = 0 ) as a reference and call the method None . A . 1 Automatic Evaluation Metrics • BLEU : For each generated text guided by the set of K topics , we report BLEU - 2 ( Papineni et al . , 2002 ) between the generation and the actual con - tinuation containing O = 25 words . We adopt the smoothing method 3 in Chen and Cherry ( 2014 ) because there is sometimes no bigram overlapping between the predicted continuation and the actual continuation . • BLEU Diff : Similar to Sim Diff , BLEU Diff is the BLEU score between the generation and the continuation minus the BLEU score between the generation and the input prompt . • Word Hit : If the generated topics are not rel - evant to the input prompt , our conditional text generator might have difﬁculty in mentioning the related words in the continuation . We report how many unique word types representing K topics are mentioned in the generated continuation . • Self - BLEU : The metric computes the average pairwise BLEU scores of 3 generations ( Zhu et al . , 2018 ) . Lower Self - BLEU implies the op - tions encourage more diverse generations . A . 2 Human Evaluation We show the continuation guided by all topics and ask how ﬂuent the sentence is ( F ) , how helpful the sentence can promote the narrative ( NP ) , and the overall quality of the generation ( A ) . The worker Scope Method BLEU BLEU Diff Word Hit Self - BLEU ( ↓ ) Dist - 1 Dist - 2 Global Sample 7 . 39 5 . 66 0 . 34 9 . 45 47 . 60 86 . 79 LDA 7 . 19 4 . 87 2 . 01 13 . 06 36 . 02 78 . 73 Kmeans 7 . 12 4 . 65 1 . 30 12 . 23 36 . 62 81 . 49 Local Sample 8 . 38 2 . 71 2 . 93 18 . 03 35 . 76 77 . 00 NNSC 8 . 44 3 . 24 2 . 94 17 . 20 35 . 43 76 . 71 Kmeans 8 . 32 3 . 06 2 . 96 16 . 97 35 . 39 77 . 10 Ours 8 . 38 5 . 55 3 . 02 15 . 97 36 . 18 78 . 71 NA None 8 . 50 5 . 59 - 13 . 17 39 . 69 80 . 17 Table 7 : Comparison of the continuations generated by different option generators using automatic metrics . The values are percentages except in Word Hit . Higher numbers are better except in Self - BLEU . The best numbers within each scope are highlighted . can choose from 5 options , and 5 means very ﬂuent , very helpful , and excellent , respectively . A . 3 Results The automatic evaluation results are presented in Table 7 . As expected , the options generated by the local methods lead to the continuations that are more similar to the actual continuation ( i . e . , higher BLEU score ) compared to that generated by the global methods . Global topics encourage the generated text to be unrelated to the input prompt , so leading to more diverse sentences ( i . e . , lower Self - BLEU and higher Dist - 1 and Dist - 2 ) . Our method performs better in most metrics than the other local methods , especially in BLEU Diff , while achieving comparable BLEU , which means our generated options often result in the relevant and diverse continuations that are sufﬁciently dif - ferent from the prompt . Furthermore , the human evaluation results in Table 6 show that our method outperforms other baselines in all metrics . B Implementation Details The training algorithm for our option generator could be seen in Algorithm 1 . The algorithm is sim - ilar to the training method in Chang et al . ( 2021 ) . For each non - stop word in the continuation ¯ y o , we linearly combine all the cluster centers c 1 , . . . c K to reconstruct the word embedding of ¯ y o . We only allow positive weights , a 1 , . . . , a K ≥ 0 , and incor - porate L1 loss K (cid:80) k = 1 a k to encourage the weights of the irrelevant cluster centers to be 0 , so the clus - tering method is called non - negative sparse cod - ing ( NNSC ) ( Hoyer , 2002 ) . Estimating a 1 , . . . , a K could be viewed as E - step , which matches the clus - ters and the word embedding in the continuation . In the M - step , we ﬁx the estimated weights ˆ a 1 , . . . , ˆ a K and use backpropagation to encourage the cluster centers to be closer to the embedding of ¯ y o . To encourage the cluster centers to be context depen - dent , we also use the same EM optimization to push away the clusters centers from negative samples’ embeddings . During training , the input prompt is tokenized into word pieces , and the actual continuation is tokenized into words . We run the byte pair en - coding ( Sennrich et al . , 2016 ) to get word pieces required by GPT2 and run Spacy tokenizer 4 to get words required by GloVe . The two tokenization results are aligned to collect the training examples . When training our option generator , we sample a word piece sequence with length 512 as the in - put of the GPT2 encoder . We randomly select a number from 1 to 199 as the size of the ﬁrst in - put prompt and the next prompt always contains 200 more word pieces than the previous one . Each continuation includes 50 words ( not including stop words ) after the corresponding prompt . In the same text sequence , the last output embedding in every prompt receives gradients together from a single backward pass . We initialize our encoder using distilled GPT2 ( Sanh et al . , 2019 ) to save GPU memory and the parameters are trained using SGD as in Chang et al . ( 2021 ) . When training our conditional text generator , the size of the input to the GPT2 encoder is 256 . We randomly select 5 positions from the input se - quence to insert the future words sampled from the continuation containing 25 words ( after removing stop words ) . Although we insert future words into multiple positions to speed up the training , we in - sert the future words once ( only before the end of the prompt ) during testing . We initialize our en - coder using the GPT2 with 117M parameters and train the parameters using AdamW ( Loshchilov 4 spacy . io / Algorithm 1 : Training procedure for our option generator ( using batch size = 1 ) Input : Training corpus , stop word list , pretrained GPT2 encoder , and pre - trained word embeddings . Output : Neural option generator Initialize our encoder using a pretrained GPT2 model and randomly initialize the other parameters foreach x 1 , . . . , x I in training corpus do Run forward pass of our model given x 1 , . . . , x I to compute the cluster centers c 1 , . . . c K Collect the positive examples ¯ y 1 , . . . , ¯ y O ( i . e . , non - stop words after x I ) and their word embeddings e ¯ yo Collect the negative examples ¯ y (cid:48) 1 , . . . , ¯ y (cid:48) O ( i . e . , a randomly sampled continuation without stop words ) and their word embeddings e ¯ y (cid:48) o L = 0 foreach ¯ y o in the positive example do Estimate ˆ a 1 , . . . , ˆ a K = arg min 0 ≤ a 1 , . . . , a K ≤ 1 | | K (cid:80) k = 1 a k c k − e ¯ yo | | 2 + λ K (cid:80) k = 1 a k using RMSprop L = L + | | K (cid:80) k = 1 ˆ a k c k − e ¯ yo | | 2 end foreach ¯ y (cid:48) o in the negative example do Estimate ˆ b 1 , . . . , ˆ b K = arg min 0 ≤ b 1 , . . . , b K ≤ 1 | | K (cid:80) k = 1 b k c k − e ¯ y (cid:48) o | | 2 + λ K (cid:80) k = 1 b k using RMSprop L = L − | | K (cid:80) k = 1 ˆ b k c k − e ¯ y (cid:48) o | | 2 end Update our neural model by backpropagation through cluster centers c 1 , . . . c K to minimize L end and Hutter , 2019 ) . Notice that we insert at most K words before each position during training . There - fore , the number of speciﬁed words plus the num - ber of chosen topics cannot be greater than K dur - ing testing . We use the cased version ( 840B ) of GloVe em - bedding . The GloVe embedding in both compo - nents is ﬁxed to allow the two components that are trained parallelly to communicate during testing . To simplify our method , we train the two compo - nents separately and bridge the components using GloVe . 5 Training separately also allows the lan - guage generator to use a larger model on a GPU with limited memory . We use a GTX TITAN X and train the option generator for around three weeks and train the conditional text generator for about ﬁve weeks . 5 If we want to let the text generator directly condition on the topics rather than words during training , we need to know what topics that are mentioned by the actual continuation and how often our option generator predicts the topics . Trying the achieve this will complicate the method , so we leave this direction as future work . C Experiment Details We truncate the probabilities after the top 40 in top - k sampling ( Fan et al . , 2018 ) . In all the ex - periments , we set M = 5 words to represent each topic , although the ﬁgures and tables use M = 2 or M = 3 due to the space limit . We set K = 10 because K = 10 seems to work well in Chang et al . ( 2021 ) . Our Transformer decoder for option generation has 5 layers . In the following subsections , we describe the de - tails about our baselines , the automatic evaluation , and human evaluation . C . 1 Baselines We adopt the default hyper - parameters of LDA in gensim 6 . The cluster centers of Kmeans are op - timized using random initialization and EM algo - rithm for at most 300 iterations . 7 We use RM - 6 https : / / radimrehurek . com / gensim / models / ldamulticore . html 7 https : / / scikit - learn . org / stable / modules / generated / sklearn . cluster . KMeans . html Input Prompt defense chiefs from estonia , latvia , lithuania , germany , italy , spain and slovakia signed the agreement . LDA - global Kmeans - local Ours 1 police , prison 6 Draft , NCAA 1 defense , defenses 6 signed , signing 1 century , Roman 6 1754 , 1744 2 football , basketball 7 League , league 2 chiefs , chieftains 7 defense , defenses 2 constitutional , mandate 7 knew , wished 3 Nations , Foreign 8 company , subsidiary 3 signed , signing 8 agreement , agreements 3 king , prince 8 Bulgars , Magyars 4 company , companies 9 baseball , Baseball 4 agreement , agreements 9 signed , signing 4 Romanian , Hungarian 9 troops , war 5 party , Party 10 game , games 5 chiefs , chieftains 10 defense , defenses 5 kingdom , kings 10 Slovakia , Latvia Input Prompt The two Democrats on the ﬁve - member FCC panel held a news conference to sway opinion against Powell . LDA - global Kmeans - local Ours 1 Republican , Democratic 6 company , companies 1 conference , conferences 6 Democrats , Republicans 1 CNN , news 6 said , stated 2 party , Party 7 psychology , research 2 news , headlines 7 member , held 2 Committee , Legislative 7 know , sure 3 election , elections 8 football , basketball 3 panel , panels 8 opinion , opinions 3 party , Party 8 culminated , protested 4 television , show 9 Nations , Foreign 4 FCC , CRTC 9 sway , sways 4 Smith , Thompson 9 election , ballot 5 police , prison 10 James , Robert 5 Powell , Thompson 10 three , four 5 telecommunications , corporations 10 Obama , Barack Input Prompt The MSN Messenger 6 software will be available from 11 a . m . PST on Wednesday , according to Microsoft . LDA - global Kmeans - local Ours 1 software , user 6 California , Disney 1 PST , PDT 6 Messenger , messenger 1 integration , development 6 2012 , February 2 company , companies 7 cards , dog 2 a . m . , p . m . 7 9 , 6 2 conﬁguration , interface 7 provide , available 3 television , show 8 company , subsidiary 3 available , Available 8 according , According 3 websites , web 8 6 , 9 4 game , games 9 party , Party 4 will , must 9 Wednesday , Tuesday 4 released , release 9 IPv6 , InﬁniBand 5 Education , College 10 radio , FM 5 software , Microsoft 10 MSN , Yahoo 5 smartphones , smartphone 10 Windows , Desktop Input Prompt Schools that fail to meet state goals for three years in a row must offer tutoring in addition to transfers . LDA - global Kmeans - local Ours 1 company , companies 6 software , user 1 must , meet 6 fail , failing 1 learning , concepts 6 funded , nonproﬁt 2 football , basketball 7 cards , dog 2 row , rows 7 years , year 2 Education , Curriculum 7 need , able 3 psychology , research 8 patients , treatment 3 three , four 8 tutoring , tutor 3 students , student 8 ﬁve , six 4 game , games 9 police , prison 4 goals , goal 9 transfers , offer 4 applicant , stipulated 9 tax , taxes 5 Education , College 10 population , households 5 addition , additional 10 Schools , School 5 school , kindergarten 10 State , Missouri Input Prompt Declining issues outnumbered advancers slightly more than 3 to 1 on the New York Stock Exchange . LDA - global Kmeans - local Ours 1 County , Historic 6 Rhode , Connecticut 1 New , York 6 Stock , stock 1 economic , economy 6 1848 , 1859 2 California , Disney 7 Angeles , Los 2 1 , 2 7 York , NY 2 Investment , Financial 7 even , enough 3 Canada , Ontario 8 Australian , Melbourne 3 3 , 4 8 Declining , Decline 3 bank , loans 8 4 , 3 4 company , companies 9 Nations , Foreign 4 Exchange , exchange 9 outnumbered , outnumbering 4 , US 9 % , percent 5 China , Hong 10 Education , College 5 issues , issue 10 slightly , somewhat 5 market , trading 10 York , New Input Prompt The Portuguese weather service said Europe’s heatwave was caused by a mass of hot , dry air moving from the southeast . LDA - global Kmeans - local Ours 1 chemical , carbon 6 police , prison 1 hot , sexy 6 northeast , weather 1 population , estimates 6 October , February 2 company , companies 7 island , Island 2 mass , masses 7 caused , causing 2 temperature , heat 7 seemed , just 3 park , Park 8 restaurant , food 3 heatwave , downpours 8 air , Air 3 storm , storms 8 35 , 10 4 plant , plants 9 River , river 4 Europe , European 9 dry , drying 4 Pedro , Vicente 9 caused , severe 5 engine , aircraft 10 brown , grey 5 moving , said 10 Portuguese , Spanish 5 north , south 10 Portugal , Spain Input Prompt tibet suspects chinese government of creating the virus to spy on tibetan exiles and the dalai lama . LDA - global Kmeans - local Ours 1 police , prison 6 story , book 1 suspects , suspect 6 spy , spies 1 ﬁlm , movie 6 tells , asks 2 African , Africans 7 Iranian , Iran 2 chinese , japanese 7 exiles , exile 2 government , governmental 7 want , know 3 psychology , research 8 Nations , Foreign 3 government , governments 8 lama , Lama 3 military , government 8 insurrectionists , reactionaries 4 software , user 9 party , Party 4 creating , create 9 creating , create 4 Lai , Ying 9 killed , killing 5 cards , dog 10 China , Hong 5 virus , viruses 10 lama , Lama 5 creatures , creature 10 Thailand , Malaysia Input Prompt I have years of " Neener Neener " rights Usually I get pretty decent care . LDA - global Kmeans - local Ours 1 cards , dog 6 psychology , research 1 years , year 6 decent , good 1 ﬁlm , ﬁlms 6 said , told 2 company , companies 7 television , show 2 rights , Rights 7 care , health 2 song , lyrics 7 really , know 3 story , book 8 software , user 3 Usually , Normally 8 years , year 3 album , albums 8 downright , cynical 4 game , games 9 football , basketball 4 get , getting 9 get , getting 4 Sommer , Steffen 9 expressive , portrayal 5 patients , treatment 10 African , Africans 5 pretty , quite 10 get , getting 5 girl , teenage 10 Germany , Berlin Table 8 : Comparison of all K topics for the input prompts using M = 2 words closest to each topic . Sprop ( Tieleman and Hinton , 2012 ) to optimize NNSC for 2 , 000 iterations . PPLM uses the default hyperparameters for con - ditioning on a bag of words in its GitHub reposi - tory 8 . We try several different hyperparameters in PPLM and also try to apply PPLM to the original GPT2 with 117M parameters and to the GPT2 that is ﬁne - tuned on Wikipedia . They produce similar relevancy and perplexity , which are signiﬁcantly worse than ours in automated evaluation . The code of PPLM can only condition on a sin - gle word piece , so we need to remove the rare words that contain multiple word pieces . We ﬁlter out the input prompt in the test set if PPLM cannot condition on any word in the randomly sampled topics . 8 https : / / github . com / uber - research / PPLM C . 2 Automated Evaluation Similar to training , we ﬁrst randomly sample a word piece sequence with a length of 512 in the testing set and call the sequence a paragraph . We randomly choose a number from 1 to 79 as the number of word pieces that the ﬁrst input prompt include and append 80 more word pieces to create the next input prompt until all the word pieces in the paragraph are added to the prompt . When we compute Sim Short in Table 1 , only the ﬁrst input prompt in the paragraph is used , while all prompts are included when we compute Sim . In every automatic evaluation , we sample 300 paragraphs . We do not train our model using < | startoftext | > or < | endoftext | > because a paragraph might not start with the beginning of the ﬁrst sen - tence , and a paragraph might contain multiple Wikipedia pages . The maximal input size of our conditional text generator is 256 , and it needs to generate 50 word pieces , so we only consider the Input Prompt defense chiefs from estonia , latvia , lithuania , germany , italy , spain and slovakia signed the agreement . Generator Generated Text Option Text LDA - global Ours After the talks , the League of Nations allowed the German Democratic Republic’s representatives to negotiate the deal . Kmeans - local Ours These agreements were based on the agreement signed by the German king Frederick Barbarossa between 870 and 873 Ours PPLM For the period of ﬁve years in Lithuania were the chief ministers ( procurator princeps or jevgadirs ) and the chief None GPT2 ( This treaty would come under Royal Decree 1282 on 8 September 1725 . ) On 9 December 1725 , Russian armies entered Ours Ours These agreements were signed in 1756 by the sovereigns of Moldavia ( Moorish ) and the princely states of the Romanian Input Prompt The two Democrats on the ﬁve - member FCC panel held a news conference to sway opinion against Powell . Generator Generated Text Option Text LDA - global Ours He and his former friend and fellow Democrat , James H . Jim White , were arrested on charges of corruption and child Kmeans - local Ours He and his three other Democrats had no time to discuss the other four ; this changed at the conference and at the FCC Ours PPLM She responded , The House has decided , ’When the other candidates say , ’Let Democrats take over the FCC , ’ it’s kind of None GPT2 When questioned in the news , Powell stated The fact that she does not want to get a job with a group that includes me Ours Ours As a result , a Senate committee investigation by the Senate said that the Democratic party had been involved in the Input Prompt The MSN Messenger 6 software will be available from 11 a . m . PST on Wednesday , according to Microsoft . Generator Generated Text Option Text LDA - global Ours Thessaloniki Business Center - a business school that was built in 1995 and that provides jobs in business , technology , Kmeans - local Ours The MSN Messenger 8 software will be available from 9 a . m . PST on Thursday , according to Microsoft . The MSN Ours PPLM On Tuesday January 22 , 2016 , Microsoft announced that the Internet Mail service , the Messenger Plus service , is going None GPT2 Microsoft plans to expand the coverage of MSN Messenger in the United States . . nbc . org ; November 8 , 2008 . In its Ours Ours The Windows Messenger 6 web app now has a new web service for mobile devices to download the Windows product . Input Prompt Schools that fail to meet state goals for three years in a row must offer tutoring in addition to transfers . Generator Generated Text Option Text LDA - global Ours Students at the School of Business , Computer , and information science programs must complete their coursework in Kmeans - local Ours Tuition on non - residential loans , and the availability of tutoring for at least three years , will be phased out before Ours PPLM The school also provides scholarships to students from the other districts who apply for the school to receive free or None GPT2 The program is an outgrowth of the Tisch School’s efforts to build the academic program required for graduate programs , Ours Ours Additionally , it must also provide a forum to discuss the learning needs of its students . California school districts a Input Prompt Declining issues outnumbered advancers slightly more than 3 to 1 on the New York Stock Exchange . Generator Generated Text Option Text LDA - global Ours New York City San Mateo County Windsor County , Ontario Windsor , Ontario ( convention ) Windsor , On Kmeans - local Ours New York Stock Exchange had been down for issues outnumbered 7 to 1 on the New York Exchange . Declining issues Ours PPLM , Financial Times According to Bloomberg’s Financial Times , the ﬁrm’s current capital ﬂows to the Securities None GPT2 On December 19 , 2006 , and February 10 , 2007 , respectively , and February 21 , 2007 , respectively , and February 5 Ours Ours In response the Bank of New York announced $ 100 million in loan interest . The Bank withdrew its $ 5 million offer , Input Prompt The Portuguese weather service said Europe’s heatwave was caused by a mass of hot , dry air moving from the southeast . Generator Generated Text Option Text LDA - global Ours The police said it is likely that the heat wave is coming from the ocean around the park . There are a number of other Kmeans - local Ours The North American weather service said Europe’s heatwave was caused by a mass of hot , dry air moving from the Ours PPLM It was a record in the European part of the Western Hemisphere . At 2AM Eastern Europe will see two nights a week of None GPT2 On January 1 , 2014 , the station’s digital channel was shut down as digital television began broadcasting , ending Ours Ours The most common weather to affect Portugal was the August , which began with an approaching hurricane , causing severe Input Prompt tibet suspects chinese government of creating the virus to spy on tibetan exiles and the dalai lama . Generator Generated Text Option Text LDA - global Ours In 2003 , Chinese Nationalist Party leader Deng Xiaoping visited Taiwan and told the Chinese Nationalist Party Kmeans - local Ours He warns Tulku and Chinkua against spreading it because it is a secret operation by the Chinese and is a spy on the Ours PPLM They then begin to research the chinese hukou and chinta . They ﬁnd that the Chinese believe that they are agents of None GPT2 Although not suspected , one person known of the virus to be infected with it in Thailand is unknown . The virus Ours Ours He threatens to reveal his plans with the aid of Malaysia government with all the help of Japan and China to the Input Prompt I have years of " Neener Neener " rights Usually I get pretty decent care . Generator Generated Text Option Text LDA - global Ours A computer games system or computer science is a game system that is designed in a way that a computer is capable of Kmeans - local Ours In later years , my patients never get very good care . The rights are only a couple to a day so I’ve just had a few year Ours PPLM I hope you can bring in more people . We’ve a good relationship . We’ve got a good relationship with our customers since None GPT2 I’ve no idea how much care I get with the NERVIC _ CORE _ GILAS _ WELL _ CORE _ GILAS _ WELL _ CORPS _ VIGILUS ) Ours Ours The album ’Nederlands Kort Eindhoven , which , when I mentioned , was a pop - rock ﬁlm , was downright cynical - a song Table 9 : The continuations that are generated by conditioning on all of K topics from different option generators . The input prompts comes from STSb . last 206 word pieces when the input prompt is long . For each input prompt , we sample 3 sentences us - ing our conditional text generator or PPLM . When computing Sim , Sim Short , Sim Diff , BLEU , and BLEU Diff , we remove the ﬁrst word piece in the continuation and last word piece in the input prompt because the word pieces might not form complete words in the evaluation . Fur - thermore , we ignore the input prompt in the test set if the length of continuation in the paragraph is smaller than O = O (cid:48) = 25 . When computing Dist - 1 and Dist - 2 , we count unigram and bigram within each paragraph . C . 3 Human Evaluation In STSb , we discard the sentences containing less than K = 10 words after removing stop words to ensure that Kmeans - local could generate 10 non - repetitive topics . GPT2 ﬁne - tuned on English Wikipedia some - times generate sentences containing special charac - ters ( e . g . , UTF - 8 characters for other languages ) , which crowdsourcing workers might not under - stand . Thus , we ﬁlter out the input prompt in the STSb for human evaluation if the input prompt or the continuation generated by any method contains a character that cannot be encoded using the ASCII code . On Amazon Mechanical Turk ( MTurk ) , we pre - pare one task to evaluate the option generators and another task to evaluate the conditional text gener - ators . In the ﬁrst task , we show the input prompt and the K = 10 topics generated by a method . Be - fore seeing the generated continuation , the worker needs to answer • " Which topics do NOT promote the narrative ? " ( TP ) , and • " Which topics are NOT very likely to appear in the reasonable continuations ? " ( L ) . 9 Then , we show the generated continuation and ask • " How ﬂuent is the generated continuation ? ( Not ﬂuent at all - Very ﬂuent ) " ( F ) , • " How helpful is this generated continuation in terms of promoting the narrative ? ( Not helpful at all - Very helpful ) " ( NP ) , and • " Overall , how good is the generated continua - tion ? ( Terrible - Excellent ) " ( A ) . In the second task , we show the input prompt and the generated continuation . The worker needs to answer • " How ﬂuent is the generated continuation ? ( Not ﬂuent at all - Very ﬂuent ) " ( Fluency ) , and 9 We reverse the question because there are often more topics that are likely to appear . • " Whether the sentence is related to the speciﬁed topics ? " ( Relevancy ) . We allow only masters on MTurk ( the worker with a good reputation ) to do our tasks . The work - ers are rewarded 0 . 4 or 0 . 5 dollars for each of the ﬁrst tasks and 0 . 2 dollars for each of the second tasks . In our instruction , we deﬁne the reasonable con - tinuation as what the author might say next given only the input prompt , and what the author said in the real word is not important . The average performance of generated text is between the score 3 and 4 . That is , the quality of generated sentences are between somewhat ﬂuent and ﬂuent ( F ) , somewhat helpful and helpful ( NP ) , and medium and good ( A ) . The results suggest the difﬁculty of generating the continuation for a sentence ( mostly from the news in the ﬁltered STSb ) . D More Examples We randomly select 8 examples with less than 130 letters from STSb as our input prompts . The top - ics of different option generators are visualized in Table 8 . The continuations of different text gen - erators are visualized in Table 9 . You can down - load our code from https : / / github . com / iesl / interactive _ LM and test our models using your own prompts via IPython notebook .