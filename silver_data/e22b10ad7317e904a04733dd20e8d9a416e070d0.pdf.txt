Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 I’m home : Deﬁning and evaluating a gesture set for smart - home control Christine K u¨ hnel a , (cid:2) , Tilo Westermann b , Fabian Hemmert a , Sven Kratz c , Alexander M u¨ ller a , Sebastian M ¨oller a a Deutsche Telekom Laboratories , Technische Universit¨at Berlin , 10587 Berlin , Germany b Universit¨at Hamburg , interactive media . virtual environments , 22527 Hamburg , Germany c Ludwig - Maximilians - Universit¨at Mu¨nchen , Medieninformatik und Mensch - Maschine - Interaktion , 80333 Mu¨nchen , Germany Received 22 February 2010 ; received in revised form 31 March 2011 ; accepted 17 April 2011 Communicated by J . O . Wobbrock Available online 21 June 2011 Abstract Mobile phones seem to present the perfect user interface for interacting with smart environments , e . g . smart - home systems , as they are nowadays ubiquitous and equipped with an increasing amount of sensors and interface components , such as multi - touch screens . After giving an overview on related work this paper presents the adapted design methodology proposed by Wobbrock et al . ( 2009 ) for the development of a gesture - based user interface to a smart - home system . The ﬁndings for the new domain , device and gesture space are presented and compared to ﬁndings by Wobbrock et al . ( 2009 ) . Three additional steps are described : A small pre - test survey , a mapping and a memory test and a performance test of the implemented system . This paper shows the adaptability of the approach described by Wobbrock et al . ( 2009 ) for three - dimensional gestures in the smart - home domain . Elicited gestures are described and a ﬁrst implementation of a user interface based on these gestures is presented . & 2011 Elsevier Ltd . All rights reserved . Keywords : Gesture - based interaction ; Smart - home ; User - centered design ; Mobile device 1 . Introduction In developed countries , mobile phones are an almost ubiquitous feature of the environment – they are in almost everybody’s hands or pockets and thus usually in easy reach . With ever - increasing computing power and new functionalities emerging , people no longer use their mobile phones for communication purposes only . Mobile phone users are becoming used to look beyond the phone and messaging functionalities , and are also get accustomed to the diverse interaction possibilities offered by touch screens and motion sensors . The mobile phone has thus become one of the most promising user interfaces and is already being used for a range of mobile applications , for example mobile tourist guides ( Cheverst et al . , 2000 ) , map interaction ( Sch ¨oning et al . , 2009 ) or games ( Pasman et al . , 2004 ) . Even though an increasing amount of features is covered by the mobile phone , greatly facilitating life on the move , when we come home , we again have to deal with a multitude of separate applications through different interfaces . Domestic interfaces can be simple but distributed , such as light switches , or complicated ( and occasionally well - hidden ) such as remote controls for TV and hi - ﬁ systems . This situation has con - tinued to inspire research on smart environments for a long time ( Cook and Das , 2004 ; Gieselmann and Denecke , 2003 ; Koskela and V ¨a ¨an ¨anen - Vainio - Mattila , 2004 ) . Solving the inconveniences of domestic life with the possibilities offered by the – constantly activated – mobile phone seems a logical step and has already been tackled for example by the Pebbles research project . This project studies the use of handhold devices as remote controls for computers and household and ofﬁce appliances ( Myers , 2005 ) . www . elsevier . com / locate / ijhcs 1071 - 5819 / $ - see front matter & 2011 Elsevier Ltd . All rights reserved . doi : 10 . 1016 / j . ijhcs . 2011 . 04 . 005 (cid:2) Corresponding author . Tel . : þ 49 30 835358282 ; fax : þ 49 30 835358409 . E - mail addresses : christine . kuehnel @ telekom . de , christine . kuehnel @ posteo . de , spooper @ gmx . de ( C . K u¨ hnel ) , tilo . westermann @ gmail . com ( T . Westermann ) , fabian . hemmert @ telekom . de ( F . Hemmert ) , sven . kratz @ iﬁ . lmu . de ( S . Kratz ) , a . mueller @ telekom . de ( A . M u¨ ller ) , sebastian . moeller @ telekom . de ( S . M ¨oller ) . URL : http : / / www . qu . tlabs . tu - berlin . de ( C . K u¨ hnel ) . Koskela and V ¨a ¨an ¨anen - Vainio - Mattila ( 2004 ) found that users prefer a global remote control for instant control when studying interaction in a smart - home with interactive house - hold objects such as lamps , curtains , and information appli - ances . They describe instant control in terms of ‘‘right now’’ and ‘‘right here’’ , which requires centralized and mobile means and concluded with the following statement : ‘‘The mobile phone could become the primary centralized remote control while its personalization capabilities could add to its usability . ’’ A study of mobile phone GUI - based interaction operating a range of appliances by Roduner et al . ( 2007 ) suggested a combination of traditional , essentially button - based interfaces located on the device or a remote control , with extended user interfaces on a mobile device as an optimal solution . The latter should offer additional help and provide further options which are used less often , proﬁting from both screen size and number of buttons on the mobile device . Johnston et al . ( 2007 ) concentrated on a subset of the previous ideas , allowing the search and browsing of a movie database with tablet display supporting pen input . They found that users generally preferred the more familiar remote control but appreciated the value of handwriting as back - up input method . Contributing to this work our current research focuses on building a gesture - based interface to a smart - home system using a mobile device . We implemented our interface for the Apple iPhone , using accelerometer data for gesture recogni - tion and the touch screen to provide a simple GUI for device selection . Apart from designing the gesture vocabulary , our goal was to answer the following questions : (cid:2) Are users interested in gesture control in their home ? (cid:2) Which devices would they like to control via gestures ? (cid:2) Would they prefer a given set of gestures or would they like to design their own gestures , as proposed by Schlomer et al . ( 2008 ) ? Following the encouraging results of a small survey ( reported below ) we set off by ﬁrst deﬁning a gesture vocabulary that meets the requirements proposed by Nielsen et al . ( 2003 ) : the gestures should be easy to perform and remember , intuitive , metaphorically and iconically logical towards the functionality , and ergonomic . As suggested by Wobbrock et al . ( 2009 ) , we followed a participatory design approach ( Schuler and Namioka , 1993 ) involving possible end users in the design process to achieve this goal . Expanding upon Wobbrock’s ( Wobbrock et al . , 2009 ) work on surface gestures for tabletop interaction , the initial contribution of our work is the adaptation of that approach to a different domain ( smart - home ) , a different interaction device ( a mobile phone ) and a different gesture space ( three - dimensional gestures ) as well as the veriﬁcation of the approach itself . In the following study , we tested our gesture vocabulary for its degree of self - explanation by asking participants in a second study to map gestures back to functions . In a further study , we tested the proposed gestures for their memorability , asked users how well a gesture ﬁts the corresponding function , and assessed the ease of perform - ing the gesture . These steps are motivated by the work of Nielsen et al . ( 2003 ) . Finally , we implemented a ﬁrst version of the system for which we compared the recogni - tion rate of a user - trained system to the one of a pre - trained system . Thus , we were able to show the feasibility of our design . The remainder of this paper gives a short overview on related work , describes the preliminary survey and three studies – namely the design process , the test of self - explanation capability and the test of memorability – as well as the ﬁnal performance test . 2 . Related work In this section we give a short overview on previous work on gesture recognition , especially in the context of smart environments . It can be roughly divided according to the input devices , which could be one or more cameras , or a device equipped with motion sensors like the Nintendo Wii Remote ( Wii ) . Data gloves are mostly used in augmented reality systems or for recognition of sign language ( Pradhan et al . , 2008 ) and are found as being ‘‘too bulky , expensive , and awkward with the cables’’ in a study done by Tsukada and Yasumura ( 2002 ) . 2 . 1 . Camera - based gestural interaction Gieselmann and Denecke ( 2003 ) worked on an intelligent room , offering control of lamps and a video recorder by speech and pointing gestures . They argue that the possibility of using gestures ‘‘leads to a more natural and easier commu - nication’’ but seem not to have done any kind of evaluation . Richarz et al . ( 2008 ) relied on computer vision as well , when researching deictic gestures in a smart environment . They presented the results of a performance test for a two - camera setting but did not report on any subjective evaluation . Chang et al . ( 2005 ) compared vision - based recognition of hand gestures with mouse gestures in a home entertainment system , using a predeﬁned set of gestures , each mapped to several tasks . Again , no account is given of any evaluation , although the authors claim that the proposed interface is ‘‘intuitive’’ . A comparison of different vision - based systems , as well as the description and performance test of their own system – which allows pointing and simple predeﬁned gestures to control the TV , the curtains , and the lamps – is given by Do et al . ( 2006 ) . Although they assert that their system ‘‘complements the inconvenience of conventional remote controller’’ and is thus ‘‘useful to people without disabilities , as well as the aged people and persons with disabilities’’ no study involving potential users is carried out . 2 . 2 . Motion sensor - based gestural interaction The XWand supports pointing and simple gestures for turning on lights or adjusting the volume of a stereo ampliﬁer ( Wilson and Shafer , 2003 ) . The sensor data is C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 694 recorded via a two - axis accelerometer , a magnetometer and a gyroscope . Results of a user study concerning the inﬂuence of audio feedback and the usage of a tracking system on pointing accuracy and completion time are reported , but participants’ opinion about the interface itself has not been assessed . Another example is the Magic Wand which allows the control of home appliances with six simple gestures ( up , down , left , right , circle , counter - clockwise circle ) recorded via two three - axis accelerometers ( Ouchi et al . , 2005 ) . No evaluation whatsoever is reported . Schlomer et al . ( 2008 ) reported on performing gesture recognition with a Wii Controller . They tested their system using ﬁve 3D gestures ( square , circle , roll , ‘Z’ and tennis serve movement ) concerning recognition rate only . They propose that users should be allowed to choose and train their own gestures . As we could see in the last two sections , extensive work has been conducted in the area of gesture - controlled smart environments . More often then not , systems are evaluated concerning technical performance , e . g . recognition rate . But rarely , subjective evaluation is reported , and no indication is found that users were included in the design process . This research gap – the missing evaluation methodology for mobile device - enabled gesture - based interaction – is what the reported project attempts to close . It combines and adapts the approach of Wobbrock et al . ( 2009 ) and Nielsen et al . ( 2003 ) to mobile devices . 3 . Methodology When involving possible end users in the design process the ﬁrst question to answer is : Who are the users ? According to an extensive survey on mobile phone usage conducted by IFAK ( 2009 ) more than 80 % of the German population owned a mobile phone in the year 2008 . About half of them are already using their mobile phone for services beyond simple phone and message functionalities and are interested in new mobile phone - based applications . Among the seven user types classiﬁed by the IFAK survey Mobile Scan 2009 , three – namely the mobile innovative type ( 11 % , mostly male , aged 30 – 39 ) , the multi entertainment type ( 13 % , mostly between 14 and 29 ) , and the open minded type ( 15 % mostly female , aged 20 – 39 ) – form the potential user group for the new interface . Individuals belonging to these groups possess one of the newest mobile phones – often a smartphone – and are interested in innovative applications . For the studies reported below we tried to ﬁnd young adults ﬁtting these groups . Due to budget restrictions most of the participants were students , often with a technical background , and young professionals . While it might be argued that studies relying on students are biased ( Remus , 1989 ) , we are conﬁdent that this does not apply in our case . Firstly , students ﬁt at least part of the potential user group , that is the multi entertainment type . Secondly , a number of studies comparing students to professionals showed that studies relying on students as subjects may indeed yield correct results ( H ¨ost et al . , 2000 ; Liyanarachchi , 2007 ; Yavas , 1994 ) . For the development of the gesture interface to our smart - home system we started with a small survey . This survey aimed at obtaining a ﬁrst impression of whether or not a mobile phone - based remote control has any appeal to possible end users , and which home appliances people would like to control with their mobile phone . As the results were promising , we followed the approach of Nielsen et al . ( 2003 ) and Wobbrock et al . ( 2005 , 2009 ) to design an appropriate gesture vocabulary . Three studies and a performance test are described and results presented . 3 . 1 . Preliminary survey We asked 17 adults , aged between 18 and 28 ( M = 24 . 82 , SD = 2 . 46 ) to take part in our survey . Amongst the eight women and nine men were students of architecture , social pedagogy , economics , and design . Twelve participants were either owner of an iPhone or a Wii , or at least used one or both regularly . Eleven of the participants appreciated the idea of controlling home appliances via gestures . The remaining six participants – apart from one – were those who seldom or never used either of both devices . While two could not imagine using a gesture control system at all , four had problems to imagine using a gesture control . Amongst the devices they would like to control , the TV ( 10 votes ) , lamps ( 8 votes ) , hi - ﬁ system ( 6 votes ) , and blinds ( 3 votes ) were mentioned most often . The functions stated were turning on or off the device , adjusting the sound volume and the lighting conditions , or changing the channel on the TV and lowering or raising the blinds . Finally , we asked whether they would prefer a prede - ﬁned gesture vocabulary or design their own gestures . Interestingly , six participants answered that they would prefer a predeﬁned set that they could adjust or enhance , four preferred a given set and three would like to design their own gestures . Four participants gave no answer to this question . 3 . 2 . Study 1 – Finding a gesture vocabulary The next step follows the approach of Wobbrock et al . ( 2009 ) , ﬁrst described in Wobbrock et al . ( 2005 ) . Where necessary , due to the different domain , device and gesture space , adjustments are made . 3 . 2 . 1 . Participants Eighteen participants ( six women and 12 men , M age ¼ 26 . 5 , SD age ¼ 4 . 99 ) took part in our study and were rewarded with a chocolate bar . Two of them were left - handed , four owned an iPhone or iPod Touch and ﬁve regularly played with the Wii . Most of them were recruited from the lab environment and their background was in psychology , communication studies , computer science or design research . 3 . 2 . 2 . Procedure The study was conducted inside a fully functional living room where the participants could experience the effects C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 695 directly . The participants were seated on a sofa from which they could see all devices in the room . Via a GUI interface to the smart - home system , we triggered the effect ( e . g . shutting the blinds ) and described it verbally afterwards ( ‘ shut the blinds ’ ) . Then , the participant was asked to carry out a gesture she would deem appropriate for the given task , while thinking aloud . We did not include list naviga - tion amongst the referents as we decided to rely on GUI interaction for the selection of items . The movements of the participants were recorded with two video cameras , one located sideways of the participant at an angle of 45 1 and one at the ceiling directly above the participant . The video data was later manually annotated to calculate planning and articulation time of each gesture . See Fig . 1 for the layout of the room . After completing each gesture , the participant was asked to rate the suitability of the gesture ( ‘ The gesture I chose is a good match for the intended purpose ’ ) and the ease of performing the gesture ( ‘ The gesture I picked is easy to perform ’ ) on two 7 - point Likert scales ( (cid:3) 3 ¼ strongly disagree , 0 ¼ undecided , 3 ¼ strongly agree ) . The 23 effects – referred to as ‘referents’ in the following ( see Wobbrock et al . ( 2009 ) ) – were presented in a predeﬁned order grouped by device ( blinds , lamps , TV , Electronic Program Guide ( EPG ) , video recorder , answering machine , general ) , resulting in 23 (cid:4) 18 ¼ 414 recorded gestures ( see Table 1 ) of which 174 were of different types . Each referent was rated by the authors for its conceptual complexity ; i . e . how difﬁcult it would be to come up with an intuitive gesture to trigger this effect . 3 . 2 . 3 . Classiﬁcation of gestures Wobbrock et al . ( 2009 ) proposed a taxonomy of surface gestures that classiﬁes the gestures along four dimensions : form , nature , binding and ﬂow . While Wobbrock et al . ( 2009 ) studied two - dimensional gestures for desktop and TV answering machine blinds wand lamps sliding door door door smart room control participant experimenter 45° camera 1 camera 2 , 3m above the floor Fig . 1 . The layout of the living room where study 1 had been conducted . Table 1 The 23 referents and their complexity as rated by the authors ( 1 ¼ simple , 5 ¼ complex ) , resulting in an average complexity of M ¼ 1 . 97 with a standard deviation of SD ¼ 1 . 03 . Referents Complexity Referents Complexity by device Mean SD by device Mean SD Blinds Electronic Program Guide DOWN 1 . 00 0 . 00 SHOW 1 . 50 0 . 87 UP 1 . 00 0 . 00 RECORD MOVIE 3 . 75 0 . 43 STOP 2 . 50 0 . 50 SET REMINDER 3 . 25 1 . 30 Lamps Video Recorder TURN ON 1 . 25 0 . 43 SHOW LIST 4 . 00 0 . 71 DIM 1 . 25 0 . 50 DELETE MOVIE 2 . 50 0 . 50 TURN OFF 2 . 50 0 . 50 PLAY MOVIE 1 . 75 0 . 43 TV Answering Machine TURN ON 1 . 50 0 . 87 PLAY MESSAGE 2 . 50 0 . 50 NEXT CHANNEL 1 . 25 0 . 43 NEXT MESSAGE 1 . 00 0 . 00 PREVIOUS CHANNEL 1 . 25 0 . 43 PREVIOUS MESSAGE 1 . 00 0 . 00 VOLUME DOWN 2 . 50 0 . 50 VOLUME UP 1 . 00 0 . 00 General TURN OFF 1 . 50 0 . 87 HELP 2 . 25 1 . 09 ABORT 2 . 00 0 . 00 C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 696 tabletop interaction using a tabletop system , we are working with three - dimensional gestures using a smartphone as input device for smart - home interaction . The possible gesture vocabulary is restricted , compared to gesture recognition with a data glove or by computer vision . Using a data glove for example , information about ﬁnger and hand position would be provided , which would allow to use ﬁnger ﬂexure , hand orientation and movement for distinguishing between gestures . The form dimension distinguishes between ‘static’ and ‘dynamic’ ‘pose’ and ‘path’ of one hand . In the case of wizard - style ‘wand’ gestures performed with a mobile device , the ‘static pose’ can be interpreted as the orientation of the device ( upright , sideways , facing towards user , etc . ) . A ‘dynamic pose’ is described as changing from one orientation to another ( ‘device rotation’ ) . A movement of the device while holding one orientation is classiﬁed as a ‘static path’ ( ‘device movement’ ) while the movement of the device and the simultaneously changed orientation is a ‘dynamic path’ ( ‘movement and rotation’ ) . Most of the performed gestures ( 310 of 414 ¼ 74 . 9 % ) can be described as a ‘device movement’ . Only the repeatedly appearing gesture ‘button click’ was done by a ‘static pose’ : 58 ( 14 % ) times the participants held the iPhone towards the device and pressed a button . In 8 . 9 % ( ¼ 37 ) of the cases the gesture could be described as a ‘device rotation’ and even fewer ( 9 of 414 ¼ 2 . 2 % ) can be categorized ‘movement and rotation’ . The ‘device rotation’ was used most often for referents like NEXT and PREVIOUS ( roll right / left with iPhone ) , VOLUME UP and VOLUME DOWN , and DIM and BRIGHTEN ( tilting the iPhone up / down ) . In general , none of the participants used gestures that differed only on this dimension . In the nature dimension , gestures are either symbolic ( e . g . writing a question mark into the air ) , physical ( e . g . pulling down the blinds ) , metaphorical ( e . g . tapping an imaginary button ) or abstract ( in case of an arbitrary mapping ) . This dimension can be directly adapted from the taxonomy proposed by Wobbrock et al . ( 2009 ) . Their ﬁnding that the conceptual complexity C of the referents as rated by experts ( i . e . the authors ) signiﬁcantly inﬂuenced the nature of the gestures could be repeated ( F ( 3 , 410 ) ¼ 73 . 41 , p o 0 : 01 ) ; with the more complex referents resulting in symbolic gestures . Furthermore , the nature of gestures has a signiﬁcant inﬂuence also on gesture planning time ( F ( 3 , 410 ) ¼ 18 . 91 , p o 0 : 01 ) , gesture articulation time ( F ( 3 , 187 ) ¼ 4 . 60 , p o 0 : 01 ) and number of matches with other participants ( F ( 3 , 410 ) ¼ 89 . 08 , p o 0 : 01 ) . We found that gestures classi - ﬁed as physical led to the highest agreement between participants and that abstract gestures – on average – were preceded by the longest planning time and took the longest to be conducted ( cf . Table 2 ) . Especially the last two parameters are characterized by a high variance : Some participants very quickly picked any gesture that came to their mind ( for example , moving the arm in a long curve to the right for ‘show EPG’ ) – apparently giving up when confronted with a complex referent . Other participants took longer carefully choosing an adequate gesture they considered easy to remember . Binding describes whether the gesture only requires infor - mation about the object they affect or whether they refer to the environment . This dimension cannot be applied easily to the case of wizard - style gestures . The gestures studied here are by deﬁnition acting on devices or – in the case of the HELP and ABORT referent – are device - independent . A few gestures ( 2 . 7 % ) referred to the participant issuing the gesture : ‘remind me’ ( pointing to himself ) and some gestures ( 1 . 4 % ) were compounds of two separate gestures used previously , for example NEXT MESSAGE ( ¼ ‘ M ’ for message and a movement to the right for ‘next’ ) . The last dimension , ﬂow , categorizes the gestures depending on them being continuous ( to dim the lights while the mobile device is being turned ) or discrete ( recognize the movement to the right as ‘next’ ) . Only a small percentage ( 11 . 8 % ) can be categorized as continuous gestures and all of them occurred for the referents DIM the light , VOLUME DOWN and VOLUME UP . 3 . 2 . 4 . Analysis of gesture vocabulary Apart from simply counting the number of matches with other participants for each gesture and participant , we adopted the agreement measure A from Wobbrock et al . ( 2009 ) : A r ¼ X P i D P r j P i j j P r j (cid:2) (cid:3) 2 ð 1 Þ Eq . ( 1 ) computes the agreement A r for one referent r from the set of 23 referents . P r is the complete set of 18 proposed Table 2 Gesture nature , and mean and standard deviation of conceptual complexity ( C ) , planning time ( t p ) , articulation time ( t a ) and number of matches with other participants ( N matches ) , with the highest mean values printed in bold face . Nature Occurrences C t p ( s ) t a ( s ) N matches M SD M SD M SD M SD Physical 34 ( 8 . 2 % ) 1 . 22 0 . 54 2 . 38 3 . 74 0 . 89 0 . 27 11 . 71 5 . 05 Metaphorical 221 ( 53 . 4 % ) 1 . 55 0 . 70 6 . 00 9 . 93 0 . 97 1 . 23 5 . 68 4 . 30 Abstract 115 ( 27 . 8 % ) 2 . 63 1 . 04 16 . 21 19 . 53 1 . 56 1 . 04 1 . 02 1 . 89 Symbolic 44 ( 10 . 6 % ) 2 . 93 0 . 90 13 . 43 14 . 18 1 . 50 0 . 64 2 . 41 2 . 70 Total 414 ( 100 % ) C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 697 gestures for this referent and P i is the subset of identical gestures from P r . The agreement found for all referents is depicted in Fig . 2 . As found by Wobbrock et al . ( 2009 ) , the conceptual complexity of the referents is negatively corre - lated with the agreement between participants ( r ¼ (cid:3) 0 . 67 , p o 0 : 01 ) . The more complex the referent , the fewer participants chose the same gesture . When analyzing planning and articulation time and user ratings we could again conﬁrm most of the ﬁndings of Wobbrock et al . ( 2009 ) listed below . The correlation coefﬁcient or results of an ANOVA analysis and the level of signiﬁcance ( given in brackets ) are indicated in Table 2 . (cid:2) Average gesture planning time correlated signiﬁcantly with the conceptual complexity of the referents ( r ¼ 0 . 41 , p o 0 : 01 Þ . The more complex the referent , the longer the participants took to choose a gesture . On average , it took the participants 26 s ( SD ¼ 19 . 30 ) to ﬁnd a gesture for the most complex referent ( SHOW EPG ) , while the least complex referents ( blinds DOWN / UP , VOLUME UP and NEXT / PREVIOUS MESSAGE ) elicited a gesture after an average of 1 . 52 s ( SD ¼ 4 . 12 ) . (cid:2) The referents’ conceptual complexity did not correlate with gestural articulation time . (cid:2) The higher the number of matches of one gesture with the gestures made by other participants were , the higher the suitability rating of this gesture was ( F ( 4 , 404 ) ¼ 12 . 78 , p o 0 : 01 Þ . (cid:2) Suitability ratings of the gestures correlated strongly with referents’ conceptual complexity ( r ¼ (cid:3) 0 . 923 , p o 0 : 01 ) . Gestures associated with complex referents were rated poorly ( M ¼ (cid:3) 0 . 41 , SD ¼ 1 . 28 ) , while simple referents led to gestures rated better ( M ¼ 1 . 64 , SD ¼ 1 . 63 ) . (cid:2) In contrast to Wobbrock et al . ( 2009 ) we found a small correlation of gesture ease with referents’ complexity ( r ¼ (cid:3) 0 . 174 , p o 0 : 01 ) . This indicates that more complex referents elicited gestures that were more difﬁcult to perform . (cid:2) The longer the participants thought about which gesture to make , the worse they rated the suitability of this gesture ( F ( 251 , 162 ) ¼ 1 . 48 , p o 0 : 01 ) and the ease of performing the gesture ( F ( 251 , 162 ) ¼ 1 . 51 , p o 0 : 01 Þ . (cid:2) Gesture articulation time affected the rating of ease negatively ( F ( 60 , 130 ) ¼ 2 . 07 , p o 0 : 01 ) , which is in contrast to the ﬁndings of Wobbrock et al . ( 2009 ) , who found that gestures that took longer to perform were rated better . Articulation time did not affect the suitability ratings . Relying on a smartphone , the gestures do not need to be unique across referents . The same gesture can be used to lower the blinds and the volume , for example . The device could be selected either by pointing or by pressing a certain area on the mobile device while performing the gesture . Thus , the gesture vocabulary can be reduced to a number smaller than the number of referents . Indeed , participants tended to reuse gestures . Concerning the mental model of the participants we found the following when analyzing the think - aloud data : 3 . 2 . 5 . Mental model – Dichotomous referents Most dichotomous referents ( BLINDS UP / DOWN , NEXT / PREVIOUS CHANNEL , VOLUME UP / DOWN , NEXT / PREVIOUS MES - SAGE ) led to reversible gestures as found by Wobbrock et al . ( 2009 ) . Most of the gestures chosen for the referents TURN ON / OFF TV ( 15 of 18 gestures ) and TURN ON / OFF light ( half of the gestures ) were identical , based on the metaphor of an on / off button of the remote control , or the switch , respectively . In 30 of 34 cases a movement to the right meant NEXT ( CHANNEL or MESSAGE ) and in 15 of 18 cases a movement downwards was used to turn the VOLUME DOWN . Only two participants used an upwards motion to change to the next Fig . 2 . Agreement A for each referent in descending order . C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 698 channel and a movement to the right to adjust the volume . To reduce the brightness of the lights 10 participants moved either arm or iPhone downwards , four moved left and four picked a counter - clockwise circular movement . According to Hurtienne et al . ( 2008 ) the concept of up – down as an image schema – a form of knowledge representa - tion – correlates with quantity . This could be one explanation why these gestures are mapped to turn up , respectively , reduce light and volume . The spatial representation of time is found to be inﬂuenced by the writing direction ( see Tversky et al . ( 1991 ) ) , which for the German participants would produce a timeline from left ( ¼ past ) to right ( ¼ future ) . This could explain why the gestures left and right have been chosen by nearly all participants for the referents PREVIOUS / NEXT MESSAGE , which – on the time axis – are positioned before and after the current message . 3 . 2 . 6 . Mental model – Gestural form and ﬂow Participants did not distinguish between ‘device rotation’ ( movement of mobile device ) and ‘device movement’ ( movement of arm ) . They chose either one depending on their tendency to make large or small movements . This is especially apparent for the dichotomous referents named above . Continuous gestures were generally performed slower than discrete gestures . Apart from this , articulation time was inﬂuenced by the gesture’s nature and by participants’ disposition . 3 . 2 . 7 . Inﬂuence of expertise Half of the participants had either often played with the Wii or owned an iPhone and were therefore considered as experts . When comparing experts to non - experts we found that experts were more conﬁdent in having chosen a good gesture ( F ( 1 , 16 ) ¼ 7 . 41 , p o 0 : 05 ) and faster in performing the gesture ( F ( 1 , 189 ) ¼ 18 . 05 , p o 0 : 01 ) ( see Table 3 ) . 3 . 3 . Study 2 – Distinctness of the gesture vocabulary As a ﬁrst step to evaluate the gesture set the level of distinction of each gesture can be assessed : seeing the gesture , which referent would the participants choose as being possibly triggered by the gesture ( cf . Nielsen et al . ( 2003 ) ) . Ideally , a gesture should evoke exactly the referent in the mind of the participants the gesture has been designed for . Thus , the gestures linked most often to the correct referent would be the more suitable gestures . Based on the number of matches among participants we tentatively selected 26 from the 174 gestures found in the previously reported study for a ﬁrst gesture vocabulary . The chosen gestures and the gesture – referent mapping are described in Table 4 . Some gestures ( left , right , up , down , circle ) were included twice – once as ‘device rotation’ ( for example , turn mobile phone along length clockwise ) and once as ‘device movement’ ( clockwise circle ) or as small ( movement from the wrist ) and wide ( movement of the arm ) . In a second study , we asked 22 adults who had neither participated in the survey nor in the gesture ﬁnding study to Table 3 Comparison of experts and non - experts concerning gesture suitability and articulation time , with relevant mean values in bold . Experts Non - experts M SD M SD Suitability 1 . 48 0 . 44 0 . 51 0 . 86 Articulation time ( s ) 0 . 86 0 . 62 1 . 51 1 . 35 Table 4 Gesture – referent mapping , with number of correct mappings indicated in brackets . Gesture Description Referents Physical Swing upwards Move arm upwards / turn mobile phone upwards BLINDS UP ( 11 ) , VOLUME UP ( 8 ) , BRIGHTNESS UP ( 7 ) Swing downwards Move arm downwards / turn mobile phone downwards BLINDS DOWN ( 13 ) , VOLUME DOWN ( 9 ) , DIM LIGHT ( 8 ) Move down and up Move mobile phone as if pulling cord TURN ON LAMP ( 8 ) , TURN OFF LAMP ( 8 ) Metaphorical Swing right Move from left to right NEXT CHANNEL ( 15 ) , NEXT MESSAGE ( 14 ) Swipe Horizontal movement , directionless STOP BLINDS ( 5 ) Swing left Move from right to left PREVIOUS CHANNEL ( 10 ) , PREVIOUS MESSAGE ( 11 ) Circle Draw vertical clockwise circle TURN ON LAMP ( 3 ) , BRIGHTEN LIGHT ( 7 ) Anti - circle Draw vertical counter - clockwise circle TURN OFF LAMP ( 3 ) , DIM LIGHT ( 8 ) Point forward Move mobile phone forward STOP BLINDS ( 3 ) Remote control Press button while pointing TURN ON TV ( 10 ) / LAMP ( 6 ) , TURN OFF TV ( 5 ) , TURN OFF LAMP ( 3 ) , PLAY MESSAGE ( 6 ) Alarm clock Shake mobile device quickly next to ear SET REMINDER ( 5 ) Phone Move mobile phone towards ear PLAY MESSAGES ( 3 ) Symbolic O Draw record symbol RECORD MOVIE ( 4 ) L Draw ‘L’ SHOW LIST ( 6 ) 4 Draw play - symbol PLAY MOVIE ( 3 ) X Draw ‘X’ DELETE MOVIE ( 13 ) ? Draw question mark HELP ( 14 ) Abstract Landscape Turn mobile phone 90 1 SHOW EPG ( 5 ) Remind me Point to myself SET REMINDER ( 2 ) Point down Point towards ﬂoor ABORT ( 5 ) Wipe Wiping movement ( horizontal shake ) ABORT ( 15 ) , DELETE MOVIE ( 11 ) Wave Waving movement ( vertical shake ) ABORT ( 6 ) , DELETE MOVIE ( 6 ) C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 699 choose one or more referents for a given gesture shown to them in a short video ( see Fig . 3 for a screenshot ) . We used a set of 24 referents : the 23 referents named above , plus a new referent BRIGHTEN the lights as an inverse to DIM the lights . The gestures were shown in randomized sequence to the partici - pants and the order of referents’ blocks ( by device ) was randomized as well . According to our mapping , some of the gestures could be used for several referents ( M ¼ 2 . 26 , SD ¼ 1 . 53 ) and for some referents several gestures ( for example ‘device rota - tion’ and ‘device movement’ ) are applicable ( M ¼ 2 . 54 , SD ¼ 1 . 53 ) . Especially metaphorical gestures ( for example , move arm or mobile phone from left to right ) found for the less complex referents were mapped to up to ﬁve referents , while the symbolic and abstract gestures ( for example the question mark ) were mapped to only one referent . Simple metaphoric gestures seem to be less distinct and are therefore mapped to many different referents . On average , each gesture was 56 . 07 times ( SD ¼ 7 . 97 ) assigned to one or more referents . Given the number of 22 participants , each gesture was assigned to 2 . 55 referents on average . The gesture ‘alarm clock’ ( shaking the mobile phone close to one’s ear in resemblance of the ringing of an alarm clock ) was assigned to the smallest number of referents ( 39 times ) . Seven participants selected the referent ABORT for this gesture and only ﬁve picked SET REMINDER . The gesture ‘right’ ( moving the mobile phone from left to right ) was assigned most frequently ( 76 times ) ; 14 times it was assigned to the referent NEXT CHANNEL and 13 time the referent NEXT MESSAGE . It was also chosen for STOP ( 6 ) , VOLUME UP ( 5 ) , DELETE MOVIE ( 5 ) and PLAY MOVIE ( 5 ) . Each referent in turn was selected 65 . 42 times on average ( SD ¼ 9 . 67 ) . Again , we computed an agreement score A r ( see Eq . ( 1 ) ) for each referent . The agreement between participants on the gestures expected to be chosen for the referents with low complexity was higher than the agreement for gestures mapped to referents with high complexity . This results in a negative correlation between the conceptual complexity and the agreement score with Pearson’s r ¼ (cid:3) 0 . 490 , signiﬁcant at the 0 . 05 level . The agreement measure calculated after the ﬁrst experiment correlates with the agreement found in this second experiment ( r ¼ 0 . 457 , p o 0 : 05 ) . As a result of this study we further reduced the gesture – referent mapping ( see Fig . 4 for the remaining gestures ) to the gesture – referent pairs selected most often . 3 . 4 . Study 3 – Memorability In a third study , inspired by Nielsen et al . ( 2003 ) , 10 participants ( ﬁve female , ﬁve male ) who had not partici - pated in any of the previous studies were asked to perform the reduced set of gestures by memory after a brief training . The participants were aged between 22 and 35 ( M ¼ 26 , SD ¼ 4 ) . The study was conducted according to the following structure : Step 1 ( repeated for each gesture ) 1 . A video of each gesture is shown to the participant . At the same time , the respective mapping ( one to three referents ) is displayed . 2 . The participant repeats the gesture ﬁve times while the video is repeated . The sensor data is recorded and used for training the recognizer . 3 . The participant is asked to rate the gesture concerning gesture suitability ( ‘ The gesture is well - suited for the intended purpose ’ ) on a 7 - point Likert scale for each corresponding referent . Each gesture is also rated for demand on the SEA scale ( scale for assessing subjec - tively experienced effort , Eilers et al . ( 1986 ) ) . The SEA scale is a single rating scale ranging from 0 to 220 – an adapted German version of the subjective mental effort questionnaire ( SMEQ , Zijlstra ( 1993 ) ; see also Arnold ( 1999 ) ) . Verbal anchors such as not demanding at all or exceedingly demanding facilitate the rating process . Step 2 ( continued until all corresponding gestures were correctly performed ) 1 . Stimulus presentation ( referents grouped by device , sequence of devices randomly ) : Each referent is displayed Fig . 3 . Screenshot of the user interface employed in Study 2 – ‘Distinctness of the Gesture Vocabulary’ . C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 700 for 5 s . During this time the participant is asked to perform the corresponding gesture . 2 . ( a ) The presentation continues if the gesture is correct . ( b ) The presentation is stopped if the gesture was not remembered correctly . Whether or not the gesture is remembered correctly is judged by one observer . The decision is not based on the correct recognition of the gesture . If the observer is not able to recognize the gesture or if the gesture belongs to a different referent the correct gesture is again shown to the participant . The referent is added to the end of the slide show which continues . For each gesture , the number of necessary repetitions was counted as a measure for its memorability . The higher the number , the more difﬁcult it was to remember the gesture for the corresponding referent . In all cases a repetition was the result of a wrong gesture performed for a given referent , not a blank ( no gesture remembered ) . Memorability was negatively correlated with gesture suitability ( r ¼ (cid:3) 0 . 678 , p o 0 : 01 ) . Gestures rated unsuitable for a certain referent were also more difﬁcult to remember . In general , the gesture – referent mapping can , at this point of the process , be considered functional : On average , only 1 of 23 referents did not elicit the correct gesture . Gesture suitability was correlated with the suitability ratings achieved in the ﬁrst study ( r ¼ 0 . 631 , p o 0 : 01 ) and nega - tively correlated with referents’ conceptual complexity C ( r ¼ (cid:3) 0 . 647 , p o 0 : 01 ) . The gesture – referent pairs most difﬁcult to memorize were TURN OFF TV ( seven participants did not remember ) and RECORD MOVIE ( six participants did not remember ) . The referents for which the gestures were rated to be the most demanding ones on the SEA scale were DIM LAMP ( M ¼ 76 . 5 ) , VOLUME DOWN ( M ¼ 76 . 5 ) , VOLUME UP ( M ¼ 73 . 5 ) , and DELETE ( M ¼ 88 ) . The experienced effort is negatively correlated with suitability ( r ¼ (cid:3) 0 . 45 , p o 0 : 05 ) but not with memorability . Fig . 4 . The ﬁnal gesture set . ( a ) Swing to the right ; ( b ) swing to the left ; ( c ) swing upwards ; ( d ) swing downwards ; ( e ) tilt ; ( f ) move down and up ; and ( g ) point forward . C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 701 3 . 5 . Performance test We used Fast Dynamic Time Warping ( FastDTW , Salvador and Chan ( 2007 ) ) to analyze the recorded accel - erometer data for a ﬁrst implementation of the gesture recognition . The basic DTW algorithm’s complexity is quadratic , as for time and memory usage , whereas FastDTW uses a multilevel approach that results in a linear time and space complexity . The DTW algorithm is able to ﬁnd an optimal temporal alignment of two given data sequences ( Sakoe and Chiba , 1990 ) . The initial recorded samples of a single gesture are matched , produ - cing a mean cost . Upon entry of a gesture , a database comparison determines which , if any , trained gesture has been performed . For a prototype we decided to use a simple touch - based GUI to allow the user to select the device to be controlled . The recognition process is thus started once the user selects the device and ends , when she releases the button . Based on the results obtained so far we decided to combine a gesture recognition for simple and often repeated gestures with a GUI for more complex interactions and implemen - ted this on an Apple iPhone which communicated via wireless LAN with our smart - home system . For the performance test of the gesture recognition , we used seven gestures that were repeated by 10 participants 10 times ( see Fig . 4 ) . Over all 90 . 76 % of the gestures were correctly recognized while 5 . 38 % were incorrectly recognized ( false positives ) and 3 . 86 % were not recognized at all ( cf . Table 5 ) . 4 . Discussion The results of the presented survey , the three studies and the performance test are discussed separately below . 4 . 1 . Survey The preliminary survey has been conducted with the small number of 17 participants . The results drawn from this can only be seen as a careful hint of what the participants would like to have without considering restrictions such as recogni - tion rates . Nevertheless , results suggest that users aged between 18 and 28 might be interested in gesture control in their home for daily tasks as turning on and off the TV and lamps and controlling the radio , the stereo and the blinds . Those users that had prior experiences with motion - controlled devices were more open to the notion of gesture control . Our ﬁndings indicate that users – given a near - to - perfect recognition rate – might prefer a pre - deﬁned set for less complex referents but would like to design their own gestures for complex referents , for which no gesture is immediately apparent . This ﬁnding points to a demand for user - adaptable gesture - based interfaces , furthermore , user - trained gestures may facilitate the achievement of high recognition accuracy . 4 . 2 . Study 1 – Deﬁning a gesture vocabulary Similar to the ﬁndings of Wobbrock et al . ( 2009 ) , our results indicate that a large group of average users is helpful in designing a coherent set of simple gestures . Our study also indicates that a small expert group ( in this case the authors ) may ﬁnd a more diverse set of gestures , that may , however , be less intuitive – as discussed before by Morris et al . ( 2010 ) . At the same time , experts were well able to estimate the complexity of the proposed referents . This may be helpful , as the preliminary study pointed to a need for both pre - deﬁned and custom - trained gestures : This distinction can , as indicated , be made by experts . The data gathered in the study showed that neither the size of a gesture , nor its articulation time is a suitable means of distinction between gestures for users – a circumstance that should be taken into consideration during the training phase . Expert users have shown more self - conﬁdence in choosing and performing the gestures . It has become apparent that users’ mental models might be inﬂuenced by experience , i . e . with previously known user interfaces ( e . g . existing rotatory dimmers for lightings ) , culture ( e . g . writing direction ) and knowledge representation . 4 . 3 . Study 2 – Distinctness of gesture vocabulary The reverse - mapping of the gestures to the referents has conﬁrmed the ﬁndings of Study 1 . In Study 1 , simpler referents elicited apparently more intuitive gestures , as these were , in Study 2 , more successfully mapped back to their original referents . Besides the physically inspired gestures Table 5 Gesture – command mapping and recognition results . Gesture Success rate ( % ) Command Device Swing upwards 87 . 50 Volume up TV , Radio Brighter Lamps Open Blinds Swing downwards 92 . 38 Volume down TV , Radio Dim Lamps Close Blinds Point forward 97 . 22 Turn on / off TV , Radio Swing to the right 91 . 51 Next channel TV , Radio Stop Blinds Swing to the left 92 . 45 Previous channel TV , Radio Move down and up 87 . 50 Turn on / off Lamps Tilt 89 . 52 Show EPG TV Show available channels Radio C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 702 like ‘Up’ and ‘Down’ , it was the highly symbolic gestures ( e . g . ‘Question Mark’ ) that were easily mapped back . This points to a comparably high intuitiveness of these gestures . 4 . 4 . Study 3 – Memorability Study 3 has shown as well that users are able to distinguish intuitive mappings of referents to gestures from less intuitive ones . This was further conﬁrmed by the circumstance that the more intuitive mappings were also remembered with a higher success rate . By and large , all mappings at this stage of the process were remembered at a comparably high success rate . It has to be stated though that the setup only allows for testing how well participants are able to recall the gestures stored in their intermediate - term memory which has a duration of minutes to hours ( cf . Kesner ( 2007 ) ) . The three studies yield consistent results concerning the suitability of the gestures for certain referents . Gestures that were brought up by numerous participants were later rated as well - suited and remembered well . For referents rated as complicated by the authors , no apparent best - match could be found and the gestures selected were rated to be less well - suited . We conclude that gestures could be used for often repeated interactions for which most users would have similar concepts ( physical , symbolic or metaphorical ) . Examples for this are turning on or off devices , reducing or increasing volume or lightning and going backwards or forward in a list of TV channels . For more complex interactions like recording a movie GUI interaction might be more appropriate . 4 . 5 . Performance test The seven gestures selected for the ﬁrst prototype were used to test the recognition algorithm for performance . The recognition rates line up well with previous work using similar hardware ( e . g . Schlomer et al . ( 2008 ) with a recognition rate of 85 – 95 % for a set of ﬁve gestures ) . Most false positives or not recognized gestures may be explained by the variety of execution during the performance test . For example , an early release of the button while perform - ing a ‘down and up’ gesture could either lead to the recognition of a ‘down’ gesture or no recognition result . Furthermore , some instances were small , others big and participants would start their gestures at different levels and angles . This also ensues in a reduced recognition rate . 5 . Conclusion and future work We have presented an extended methodology for the deﬁnition and evaluation of gestures in mobile phone - controlled smart - home environments . The methodology has originally been proposed by Wobbrock et al . ( 2009 ) and Nielsen et al . ( 2003 ) for tabletop interaction . One outcome of the work reported here is the adaptation of the approach to the smart - home domain , a mobile phone as interaction device and three - dimensional gesture space as well as the veriﬁcation of the approach itself . The set of gestures , which the methodology elicited , may also be easily transferable to other domains , due to their character of universality . It seems to be a robust step forward , toward future explorations in this ﬁeld of research . This said , we can draw the following conclusions con - cerning the design and the training of the recognizer : As users do not differentiate between wide and small gestures , nor between starting points of gestures , the recognizer should be robust against this and trained with a multitude of variations of one gesture . It became apparent that gestures are well - suited for actions for which physical or metaphorical and thus intuitive gestures can be found . For this gestures the recognizer might be pre - trained and the gestures can then be shown via video to the users or using similar pictures as in Fig . 4 . For complicated actions that are performed less often two possible approaches apply : ( 1 ) Offering the user the possibility to design her own gesture set ( this might require a sort of self - writing help functionality ) or ( 2 ) sidestepping the problem offering additional interfaces , such as a GUI . As mentioned in Section 4 . 4 we took the second approach and combined the gesture recognition for simple and often repeated gestures with a GUI for more complex interactions and implemented this on an Apple iPhone which commu - nicated via wireless LAN with our smart - home system . In a user study the new interface was compared to a voice - based interface and the combination of voice , gesture and GUI – similar to the study reported by Brumitt and Cadiz ( 2001 ) . First results reported in K u¨ hnel et al . ( 2010 ) indicate that about half of the users prefer a voice - based interaction while the other half favor a mobile phone - based interface . In another study a smartphone - based GUI interface is com - pared to a gesture interface and a voice - based interface . The results are not analyzed yet . On the long run device selection should be done either by pointing at the device or using the build - in camera of the mobile phone and image recognition . Thus , the GUI interface would be even less cluttered with buttons and the whole interaction would be more intuitive . Another question that needs investigation is how to design a help functionality for a gesture interface . The challenge of course is to design a gesture interface that is intuitive enough not to require any introduction or help functionality . But there might be users who prefer reading some sort of manual when using a new interface for the ﬁrst time or when coming across problems . In the case of a smartphone , this manual could consist of video clips showing pre - trained gestures or an instruction how to train their own gestures . Acknowledgments The project was ﬁnancially supported by the Deutsche For - schungsgemeinschaft DFG ( German Research Com - munity ) , grant MO 1038 / 6 - 1 . C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 703 References Arnold , A . G . , 1999 . Mental effort and evaluation of user - interfaces : a questionnaire approach . In : Proceedings of HCI International on Human – Computer Interaction : Ergonomics and User Interfaces , vol . I . L . Erlbaum Associates Inc . , Hillsdale , NJ , USA , pp . 1003 – 1007 . Brumitt , B . , Cadiz , J . , 2001 . Let there be light examining interfaces for homes of the future . In : Proceedings of Interact , pp . 375 – 382 . Chang , J . S . , Kim , S . H . , Kim , H . J . , 2005 . Vision - based interface for integrated home entertainment system . In : Proceedings of Computer Vision in Human – Computer Interaction , pp . 176 – 183 . Cheverst , K . , Davies , N . , Mitchell , K . , Friday , A . , Efstratiou , C . , 2000 . Developing a context - aware electronic tourist guide : some issues and experiences . In : Proceedings of CHI , pp . 17 – 24 . Cook , D . , Das , S . , 2004 . Smart Environments – Technology , Protocols and Applications . John Wiley and Sons . Do , J . - H . , Jung , J . - W . , Jung , S . H . , Jang , H . , Bien , Z . , 2006 . Advanced soft remote control system using hand gesture . In : Proceedings of MICAI : Advances in Artiﬁcial Intelligence , pp . 745 – 755 . Eilers , K . , Nachreiner , F . , H ¨anecke , K . , 1986 . Entwicklung und U¨ ber - pr u¨ fung einer Skala zur Erfassung subjektiv erlebter Anstrengung . Zeitschrift f u¨ r Arbeitswissenschaften 40 , 215 – 244 . Gieselmann , P . , Denecke , M . , 2003 . Towards multimodal interaction with an intelligent room . In : Proceedings of Eurospeech , pp . 2229 – 2232 . H ¨ost , M . , Regnell , B . , Wohlin , C . , 2000 . Using students as subjects – a comparative study of students and professionals in lead - time impact assessment . Empirical Software Engineering , 201 – 214 . Hurtienne , J . , Weber , K . , Blessing , L . , 2008 . Prior experience and intuitive use : image schemas in user centred design . In : Langdon , P . , Clarkson , J . , Robinson , P . ( Eds . ) , Designing Inclusive FuturesSpringer , London , pp . 107 – 116 . IFAK - Institut f u¨ r Markt - und Sozialforschung , Apr . 2009 . Mobile scan 2009 – mobile market segmentation / http : / / www . mobile - research . ifak . com / ? p = 3 S ( last accessed 2010 / 01 / 18 ) . Johnston , M . , D’Haro , L . - F . , Levine , M . , Renger , B . , 2007 . A multimodal interface for access to content in the home . In : Proceedings of ACL , pp . 376 – 383 . Kesner , R . , 2007 . Neurobiological views of memory . In : Kesner , R . P . , Martinez , J . L . ( Eds . ) , Neurobiology of learning and memoryElsevier , Oxford , pp . 271 – 298 . Koskela , T . , V ¨a ¨an ¨anen - Vainio - Mattila , K . , 2004 . Evolution towards smart home environments : empirical evaluation of three user inter - faces . Personal and Ubiquitous Computing 8 ( 3 – 4 ) , 234 – 240 . K u¨ hnel , C . , Westermann , T . , Weiss , B . , M ¨oller , S . , 2010 . Evaluating multimodal systems – a comparison of established questionnaires and interaction parameters . In : Proceedings of NordiCHI , pp . 286 – 294 . Liyanarachchi , G . A . , 2007 . Feasibility of using student subjects in accounting experiments : a review . Paciﬁc Accounting Review , 47 – 67 . Morris , M . , Wobbrock , J . , Wilson , A . , 2010 . Understanding users’ preferences for surface gestures . In : Proceedings of Graphics Interface , pp . 261 – 268 . Myers , B . A . , 2005 . Using handhelds for wireless remote control of PCs and appliances . Interacting with Computers 17 , 251 – 264 . Nielsen , N . , Moeslund , T . , St ¨orring , M . , Granum , E . , 2003 . A procedure for developing intuitive and ergonomic gesture interfaces for HCI . In : Proceedings of Gesture - Based Communication in Human – Computer Interaction , 5th International Gesture Workshop . Springer , pp . 409 – 420 . Ouchi , K . , Esaka , N . , Tamura , Y . , Hirahara , M . , Doi , M . , 2005 . Magic Wand : an intuitive gesture remote control for home appliances . In : International Conference on Active Media Technology . Pasman , W . , Woodward , C . , Hakkarainen , M . , Honkamaa , P . , Hyv ¨akk ¨a , J . , 2004 . Augmented reality with large 3D models on a PDA – implemen - tation , performance and use experiences . In : Proceedings of VRCAI , pp . 344 – 351 . Pradhan , G . , Prabhakaran , B . , Li , C . , 2008 . Hand - gesture computing for the hearing and speech impaired . IEEE MultiMedia 15 ( 2 ) , 20 – 27 . Remus , W . , 1989 . Using students as subjects in experiments on decision support systems . In : Proceedings of the Twenty - Second Annual Hawaii International Conference on System Sciences . IEEE , pp . 176 – 180 . Richarz , J . , Pl ¨otz , T . , Fink , G . A . , 2008 . Real - time detection and interpretation of 3D deictic gestures for interaction with an intelligent environment . In : Proceedings of International Conference on Pattern Recognition , pp . 1 – 4 . Roduner , C . , Langheinrich , M . , Floerkemeier , C . , Schwarzentrub , B . , 2007 . Operating appliances with mobile phones – Strengths and limits of a universal interaction device . Pervasive Computing , 198 – 215 . Sakoe , H . , Chiba , S . , 1990 . Dynamic programming algorithm optimiza - tion for spoken word recognition . Readings in Speech Recognition , 159 – 165 . Salvador , S . , Chan , P . , 2007 . Toward accurate dynamic time warping in linear time and space . Intelligent Data Analysis 11 ( 5 ) , 561 – 580 . Schlomer , T . , Poppinga , B . , Henze , N . , Boll , S . , 2008 . Gesture recognition with a Wii Controller . In : Proceedings of the 2nd international Conference on Tangible and Embedded Interaction , pp . 11 – 14 . Sch ¨oning , J . , Cheverst , K . , L ¨ochtefeld , M . , Kr u¨ ger , A . , Rohs , M . , Taher , F . , 2009 . PhotoMap : Using spontaneously taken images of public maps for pedestrian navigation tasks on mobile devices . In : Proceed - ings of MobileHCI , pp . 1 – 10 . Schuler , D . , Namioka , A . , 1993 . Participatory design : principles and practices . Erlbaum . Tsukada , K . , Yasumura , M . , 2002 . Ubi - Finger : Gesture input device for mobile use . Information Processing Society of Japan , 43 . Tversky , B . , Kugelmass , S . , Winter , A . , 1991 . Cross - cultural and developmental trends in graphic productions . Cognitive Psychology 23 ( 4 ) , 515 – 557 . Wilson , A . , Shafer , S . , 2003 . XWand : UI for intelligent spaces . In : Proceedings of CHI . ACM Press , New York , NY , USA , pp . 545 – 552 . Wobbrock , J . O . , Aung , H . H . , Rothrock , B . , Myers , B . A . , 2005 . Max - imizing the guessability of symbolic input . In : Proceedings of CHI . ACM Press , New York , NY , USA , pp . 1869 – 1872 . Wobbrock , J . O . , Morris , M . R . , Wilson , A . D . , 2009 . User - deﬁned gestures for surface computing . In : Proceedings of CHI . ACM Press , New York , pp . 1083 – 1092 . Yavas , U . , 1994 . Research note : students as subjects in advertising and marketing research . International Marketing Review 11 , 35 – 43 . Zijlstra , R . , 1993 . Efﬁciency in work behaviour . A design approach for modern tools . Ph . D . Thesis , Delft University Press , Delft . C . Ku¨hnel et al . / Int . J . Human - Computer Studies 69 ( 2011 ) 693 – 704 704