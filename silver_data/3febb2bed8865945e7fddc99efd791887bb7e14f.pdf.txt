Proceedings of NAACL - HLT 2018 , pages 2227 – 2237 New Orleans , Louisiana , June 1 - 6 , 2018 . c (cid:13) 2018 Association for Computational Linguistics Deep contextualized word representations Matthew E . Peters † , Mark Neumann † , Mohit Iyyer † , Matt Gardner † , { matthewp , markn , mohiti , mattg } @ allenai . org Christopher Clark ⇤ , Kenton Lee ⇤ , Luke Zettlemoyer †⇤ { csquared , kentonl , lsz } @ cs . washington . edu † Allen Institute for Artiﬁcial Intelligence ⇤ Paul G . Allen School of Computer Science & Engineering , University of Washington Abstract We introduce a new type of deep contextual - ized word representation that models both ( 1 ) complex characteristics of word use ( e . g . , syn - tax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i . e . , to model polysemy ) . Our word vectors are learned func - tions of the internal states of a deep bidirec - tional language model ( biLM ) , which is pre - trained on a large text corpus . We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems , including question answering , tex - tual entailment and sentiment analysis . We also present an analysis showing that exposing the deep internals of the pre - trained network is crucial , allowing downstream models to mix different types of semi - supervision signals . 1 Introduction Pre - trained word representations ( Mikolov et al . , 2013 ; Pennington et al . , 2014 ) are a key compo - nent in many neural language understanding mod - els . However , learning high quality representa - tions can be challenging . They should ideally model both ( 1 ) complex characteristics of word use ( e . g . , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i . e . , to model polysemy ) . In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and signiﬁcantly improves the state of the art in every considered case across a range of challenging language un - derstanding problems . Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence . We use vectors derived from a bidirec - tional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text cor - pus . For this reason , we call them ELMo ( Em - beddings from Language Models ) representations . Unlike previous approaches for learning contextu - alized word vectors ( Peters et al . , 2017 ; McCann et al . , 2017 ) , ELMo representations are deep , in the sense that they are a function of all of the in - ternal layers of the biLM . More speciﬁcally , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer . Combining the internal states in this manner al - lows for very rich word representations . Using in - trinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e . g . , they can be used with - out modiﬁcation to perform well on supervised word sense disambiguation tasks ) while lower - level states model aspects of syntax ( e . g . , they can be used to do part - of - speech tagging ) . Simultane - ously exposing all of these signals is highly bene - ﬁcial , allowing the learned models select the types of semi - supervision that are most useful for each end task . Extensive experiments demonstrate that ELMo representations work extremely well in practice . We ﬁrst show that they can be easily added to existing models for six diverse and challenging language understanding problems , including tex - tual entailment , question answering and sentiment analysis . The addition of ELMo representations alone signiﬁcantly improves the state of the art in every case , including up to 20 % relative error reductions . For tasks where direct comparisons are possible , ELMo outperforms CoVe ( McCann et al . , 2017 ) , which computes contextualized rep - resentations using a neural machine translation en - coder . Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform 2227 those derived from just the top layer of an LSTM . Our trained models and code are publicly avail - able , and we expect that ELMo will provide simi - lar gains for many other NLP problems . 1 2 Related work Due to their ability to capture syntactic and se - mantic information of words from large scale un - labeled text , pretrained word vectors ( Turian et al . , 2010 ; Mikolov et al . , 2013 ; Pennington et al . , 2014 ) are a standard component of most state - of - the - art NLP architectures , including for question answering ( Liu et al . , 2017 ) , textual entailment ( Chen et al . , 2017 ) and semantic role labeling ( He et al . , 2017 ) . However , these approaches for learning word vectors only allow a single context - independent representation for each word . Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword informa - tion ( e . g . , Wieting et al . , 2016 ; Bojanowski et al . , 2017 ) or learning separate vectors for each word sense ( e . g . , Neelakantan et al . , 2014 ) . Our ap - proach also beneﬁts from subword units through the use of character convolutions , and we seam - lessly incorporate multi - sense information into downstream tasks without explicitly training to predict predeﬁned sense classes . Other recent work has also focused on learning context - dependent representations . context2vec ( Melamud et al . , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word . Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system ( CoVe ; McCann et al . , 2017 ) or an unsupervised lan - guage model ( Peters et al . , 2017 ) . Both of these approaches beneﬁt from large datasets , although the MT approach is limited by the size of parallel corpora . In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences ( Chelba et al . , 2014 ) . We also generalize these approaches to deep contextual representations , which we show work well across a broad range of diverse NLP tasks . 1 http : / / allennlp . org / elmo Previous work has also shown that different lay - ers of deep biRNNs encode different types of in - formation . For example , introducing multi - task syntactic supervision ( e . g . , part - of - speech tags ) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing ( Hashimoto et al . , 2017 ) or CCG super tagging ( Søgaard and Goldberg , 2016 ) . In an RNN - based encoder - decoder machine trans - lation system , Belinkov et al . ( 2017 ) showed that the representations learned at the ﬁrst layer in a 2 - layer LSTM encoder are better at predicting POS tags then second layer . Finally , the top layer of an LSTM for encoding word context ( Melamud et al . , 2016 ) has been shown to learn representations of word sense . We show that similar signals are also induced by the modiﬁed language model objective of our ELMo representations , and it can be very beneﬁcial to learn models for downstream tasks that mix these different types of semi - supervision . Dai and Le ( 2015 ) and Ramachandran et al . ( 2017 ) pretrain encoder - decoder pairs using lan - guage models and sequence autoencoders and then ﬁne tune with task speciﬁc supervision . In con - trast , after pretraining the biLM with unlabeled data , we ﬁx the weights and add additional task - speciﬁc model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model . 3 ELMo : Embeddings from Language Models Unlike most widely used word embeddings ( Pen - nington et al . , 2014 ) , ELMo word representations are functions of the entire input sentence , as de - scribed in this section . They are computed on top of two - layer biLMs with character convolutions ( Sec . 3 . 1 ) , as a linear function of the internal net - work states ( Sec . 3 . 2 ) . This setup allows us to do semi - supervised learning , where the biLM is pre - trained at a large scale ( Sec . 3 . 4 ) and easily incor - porated into a wide range of existing neural NLP architectures ( Sec . 3 . 3 ) . 3 . 1 Bidirectional language models Given a sequence of N tokens , ( t 1 , t 2 , . . . , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - 2228 ken t k given the history ( t 1 , . . . , t k   1 ) : p ( t 1 , t 2 , . . . , t N ) = N Y k = 1 p ( t k | t 1 , t 2 , . . . , t k   1 ) . Recent state - of - the - art neural language models ( J´ozefowicz et al . , 2016 ; Melis et al . , 2017 ; Mer - ity et al . , 2017 ) compute a context - independent to - ken representation x LMk ( via token embeddings or a CNN over characters ) then pass it through L lay - ers of forward LSTMs . At each position k , each LSTM layer outputs a context - dependent repre - sentation   ! h LMk , j where j = 1 , . . . , L . The top layer LSTM output ,   ! h LMk , L , is used to predict the next token t k + 1 with a Softmax layer . A backward LM is similar to a forward LM , ex - cept it runs over the sequence in reverse , predict - ing the previous token given the future context : p ( t 1 , t 2 , . . . , t N ) = N Y k = 1 p ( t k | t k + 1 , t k + 2 , . . . , t N ) . It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations   h LMk , j of t k given ( t k + 1 , . . . , t N ) . A biLM combines both a forward and backward LM . Our formulation jointly maximizes the log likelihood of the forward and backward directions : N X k = 1 ( log p ( t k | t 1 , . . . , t k   1 ; ⇥ x ,   ! ⇥ LSTM , ⇥ s ) + log p ( t k | t k + 1 , . . . , t N ; ⇥ x ,   ⇥ LSTM , ⇥ s ) ) . We tie the parameters for both the token represen - tation ( ⇥ x ) and Softmax layer ( ⇥ s ) in the forward and backward direction while maintaining sepa - rate parameters for the LSTMs in each direction . Overall , this formulation is similar to the approach of Peters et al . ( 2017 ) , with the exception that we share some weights between directions instead of using completely independent parameters . In the next section , we depart from previous work by in - troducing a new approach for learning word rep - resentations that are a linear combination of the biLM layers . 3 . 2 ELMo ELMo is a task speciﬁc combination of the in - termediate layer representations in the biLM . For each token t k , a L - layer biLM computes a set of 2 L + 1 representations R k = { x LMk ,   ! h LMk , j ,   h LMk , j | j = 1 , . . . , L } = { h LMk , j | j = 0 , . . . , L } , where h LMk , 0 is the token layer and h LMk , j = [   ! h LMk , j ;   h LMk , j ] , for each biLSTM layer . For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E ( R k ; ⇥ e ) . In the simplest case , ELMo just selects the top layer , E ( R k ) = h LMk , L , as in TagLM ( Peters et al . , 2017 ) and CoVe ( Mc - Cann et al . , 2017 ) . More generally , we compute a task speciﬁc weighting of all biLM layers : ELMo taskk = E ( R k ; ⇥ task ) =   task L X j = 0 s taskj h LMk , j . ( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter   task allows the task model to scale the entire ELMo vector .   is of practical im - portance to aid the optimization process ( see sup - plemental material for details ) . Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization ( Ba et al . , 2016 ) to each biLM layer before weighting . 3 . 3 Using biLMs for supervised NLP tasks Given a pre - trained biLM and a supervised archi - tecture for a target NLP task , it is a simple process to use the biLM to improve the task model . We simply run the biLM and record all of the layer representations for each word . Then , we let the end task model learn a linear combination of these representations , as described below . First consider the lowest layers of the super - vised model without the biLM . Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , uniﬁed manner . Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre - trained word em - beddings and optionally character - based represen - tations . Then , the model forms a context - sensitive representation h k , typically using either bidirec - tional RNNs , CNNs , or feed forward networks . To add ELMo to the supervised model , we ﬁrst freeze the weights of the biLM and then 2229 concatenate the ELMo vector ELMo taskk with x k and pass the ELMo enhanced representation [ x k ; ELMo taskk ] into the task RNN . For some tasks ( e . g . , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the out - put of the task RNN by introducing another set of output speciﬁc linear weights and replacing h k with [ h k ; ELMo taskk ] . As the remainder of the supervised model remains unchanged , these addi - tions can happen within the context of more com - plex neural models . For example , see the SNLI experiments in Sec . 4 where a bi - attention layer follows the biLSTMs , or the coreference resolu - tion experiments where a clustering model is lay - ered on top of the biLSTMs . Finally , we found it beneﬁcial to add a moder - ate amount of dropout to ELMo ( Srivastava et al . , 2014 ) and in some cases to regularize the ELMo weights by adding   k w k 2 2 to the loss . This im - poses an inductive bias on the ELMo weights to stay close to an average of all biLM layers . 3 . 4 Pre - trained bidirectional language model architecture The pre - trained biLMs in this paper are similar to the architectures in J´ozefowicz et al . ( 2016 ) and Kim et al . ( 2015 ) , but modiﬁed to support joint training of both directions and add a residual con - nection between LSTM layers . We focus on large scale biLMs in this work , as Peters et al . ( 2017 ) highlighted the importance of using biLMs over forward - only LMs and large scale training . To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in J´ozefowicz et al . ( 2016 ) . The ﬁnal model uses L = 2 biLSTM lay - ers with 4096 units and 512 dimension projections and a residual connection from the ﬁrst to second layer . The context insensitive type representation uses 2048 character n - gram convolutional ﬁlters followed by two highway layers ( Srivastava et al . , 2015 ) and a linear projection down to a 512 repre - sentation . As a result , the biLM provides three lay - ers of representations for each input token , includ - ing those outside the training set due to the purely character input . In contrast , traditional word em - bedding methods only provide one layer of repre - sentation for tokens in a ﬁxed vocabulary . After training for 10 epochs on the 1B Word Benchmark ( Chelba et al . , 2014 ) , the average for - ward and backward perplexities is 39 . 7 , compared to 30 . 0 for the forward CNN - BIG - LSTM . Gener - ally , we found the forward and backward perplex - ities to be approximately equal , with the backward value slightly lower . Once pretrained , the biLM can compute repre - sentations for any task . In some cases , ﬁne tuning the biLM on domain speciﬁc data leads to signiﬁ - cant drops in perplexity and an increase in down - stream task performance . This can be seen as a type of domain transfer for the biLM . As a result , in most cases we used a ﬁne - tuned biLM in the downstream task . See supplemental material for details . 4 Evaluation Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks . In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error re - ductions ranging from 6 - 20 % over strong base models . This is a very general result across a di - verse set model architectures and language under - standing tasks . In the remainder of this section we provide high - level sketches of the individual task results ; see the supplemental material for full ex - perimental details . Question answering The Stanford Question Answering Dataset ( SQuAD ) ( Rajpurkar et al . , 2016 ) contains 100K + crowd sourced question - answer pairs where the answer is a span in a given Wikipedia paragraph . Our baseline model ( Clark and Gardner , 2017 ) is an improved version of the Bidirectional Attention Flow model in Seo et al . ( BiDAF ; 2017 ) . It adds a self - attention layer af - ter the bidirectional attention component , simpli - ﬁes some of the pooling operations and substitutes the LSTMs for gated recurrent units ( GRUs ; Cho et al . , 2014 ) . After adding ELMo to the baseline model , test set F 1 improved by 4 . 7 % from 81 . 1 % to 85 . 8 % , a 24 . 9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1 . 4 % . A 11 member ensem - ble pushes F 1 to 87 . 4 , the overall state - of - the - art at time of submission to the leaderboard . 2 The increase of 4 . 7 % with ELMo is also signiﬁcantly larger then the 1 . 8 % improvement from adding CoVe to a baseline model ( McCann et al . , 2017 ) . 2 As of November 17 , 2017 . 2230 T ASK P REVIOUS SOTA O UR BASELINE ELM O + BASELINE I NCREASE ( ABSOLUTE / RELATIVE ) SQuAD Liu et al . ( 2017 ) 84 . 4 81 . 1 85 . 8 4 . 7 / 24 . 9 % SNLI Chen et al . ( 2017 ) 88 . 6 88 . 0 88 . 7 ± 0 . 17 0 . 7 / 5 . 8 % SRL He et al . ( 2017 ) 81 . 7 81 . 4 84 . 6 3 . 2 / 17 . 2 % Coref Lee et al . ( 2017 ) 67 . 2 67 . 2 70 . 4 3 . 2 / 9 . 8 % NER Peters et al . ( 2017 ) 91 . 93 ± 0 . 19 90 . 15 92 . 22 ± 0 . 10 2 . 06 / 21 % SST - 5 McCann et al . ( 2017 ) 53 . 7 51 . 4 54 . 7 ± 0 . 5 3 . 3 / 6 . 8 % Table 1 : Test set comparison of ELMo enhanced neural models with state - of - the - art single model baselines across six benchmark NLP tasks . The performance metric varies across tasks – accuracy for SNLI and SST - 5 ; F 1 for SQuAD , SRL and NER ; average F 1 for Coref . Due to the small test sizes for NER and SST - 5 , we report the mean and standard deviation across ﬁve runs with different random seeds . The “increase” column lists both the absolute and relative improvements over our baseline . Textual entailment Textual entailment is the task of determining whether a “hypothesis” is true , given a “premise” . The Stanford Natu - ral Language Inference ( SNLI ) corpus ( Bowman et al . , 2015 ) provides approximately 550K hypoth - esis / premise pairs . Our baseline , the ESIM se - quence model from Chen et al . ( 2017 ) , uses a biL - STM to encode the premise and hypothesis , fol - lowed by a matrix attention layer , a local infer - ence layer , another biLSTM inference composi - tion layer , and ﬁnally a pooling operation before the output layer . Overall , adding ELMo to the ESIM model improves accuracy by an average of 0 . 7 % across ﬁve random seeds . A ﬁve member ensemble pushes the overall accuracy to 89 . 3 % , exceeding the previous ensemble best of 88 . 9 % ( Gong et al . , 2018 ) . Semantic role labeling A semantic role label - ing ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering “Who did what to whom” . He et al . ( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following Zhou and Xu ( 2015 ) . As shown in Table 1 , when adding ELMo to a re - implementation of He et al . ( 2017 ) the single model test set F 1 jumped 3 . 2 % from 81 . 4 % to 84 . 6 % – a new state - of - the - art on the OntoNotes benchmark ( Pradhan et al . , 2013 ) , even improving over the previous best ensemble result by 1 . 2 % . Coreference resolution Coreference resolution is the task of clustering mentions in text that re - fer to the same underlying real world entities . Our baseline model is the end - to - end span - based neu - ral model of Lee et al . ( 2017 ) . It uses a biLSTM and attention mechanism to ﬁrst compute span representations and then applies a softmax men - tion ranking model to ﬁnd coreference chains . In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task ( Pradhan et al . , 2012 ) , adding ELMo improved the average F 1 by 3 . 2 % from 67 . 2 to 70 . 4 , establish - ing a new state of the art , again improving over the previous best ensemble result by 1 . 6 % F 1 . Named entity extraction The CoNLL 2003 NER task ( Sang and Meulder , 2003 ) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) . Following recent state - of - the - art systems ( Lample et al . , 2016 ; Peters et al . , 2017 ) , the base - line model uses pre - trained word embeddings , a character - based CNN representation , two biLSTM layers and a conditional random ﬁeld ( CRF ) loss ( Lafferty et al . , 2001 ) , similar to Collobert et al . ( 2011 ) . As shown in Table 1 , our ELMo enhanced biLSTM - CRF achieves 92 . 22 % F 1 averaged over ﬁve runs . The key difference between our system and the previous state of the art from Peters et al . ( 2017 ) is that we allowed the task model to learn a weighted average of all biLM layers , whereas Pe - ters et al . ( 2017 ) only use the top biLM layer . As shown in Sec . 5 . 1 , using all layers instead of just the last layer improves performance across multi - ple tasks . Sentiment analysis The ﬁne - grained sentiment classiﬁcation task in the Stanford Sentiment Tree - bank ( SST - 5 ; Socher et al . , 2013 ) involves select - ing one of ﬁve labels ( from very negative to very positive ) to describe a sentence from a movie re - view . The sentences contain diverse linguistic phenomena such as idioms and complex syntac - 2231 Task Baseline Last Only All layers   = 1   = 0 . 001 SQuAD 80 . 8 84 . 7 85 . 0 85 . 2 SNLI 88 . 1 89 . 1 89 . 3 89 . 5 SRL 81 . 6 84 . 1 84 . 6 84 . 8 Table 2 : Development set performance for SQuAD , SNLI and SRL comparing using all layers of the biLM ( with different choices of regularization strength   ) to just the top layer . Task Input Only Input & Output Output Only SQuAD 85 . 1 85 . 6 84 . 8 SNLI 88 . 9 89 . 5 88 . 7 SRL 84 . 7 84 . 3 80 . 9 Table 3 : Development set performance for SQuAD , SNLI and SRL when including ELMo at different lo - cations in the supervised model . tic constructions such as negations that are difﬁ - cult for models to learn . Our baseline model is the biattentive classiﬁcation network ( BCN ) from McCann et al . ( 2017 ) , which also held the prior state - of - the - art result when augmented with CoVe embeddings . Replacing CoVe with ELMo in the BCN model results in a 1 . 0 % absolute accuracy improvement over the state of the art . 5 Analysis This section provides an ablation analysis to vali - date our chief claims and to elucidate some inter - esting aspects of ELMo representations . Sec . 5 . 1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regard - less of whether they are produced from a biLM or MT encoder , and that ELMo representations pro - vide the best overall performance . Sec . 5 . 3 ex - plores the different types of contextual informa - tion captured in biLMs and uses two intrinsic eval - uations to show that syntactic information is better represented at lower layers while semantic infor - mation is captured a higher layers , consistent with MT encoders . It also shows that our biLM consis - tently provides richer representations then CoVe . Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec . 5 . 2 ) , training set size ( Sec . 5 . 4 ) , and visualize the ELMo learned weights across the tasks ( Sec . 5 . 5 ) . 5 . 1 Alternate layer weighting schemes There are many alternatives to Equation 1 for com - bining the biLM layers . Previous work on con - textual representations used only the last layer , whether it be from a biLM ( Peters et al . , 2017 ) or an MT encoder ( CoVe ; McCann et al . , 2017 ) . The choice of the regularization parameter   is also important , as large values such as   = 1 effec - tively reduce the weighting function to a simple average over the layers , while smaller values ( e . g . ,   = 0 . 001 ) allow the layer weights to vary . Table 2 compares these alternatives for SQuAD , SNLI and SRL . Including representations from all layers improves overall performance over just us - ing the last layer , and including contextual rep - resentations from the last layer improves perfor - mance over the baseline . For example , in the case of SQuAD , using just the last biLM layer im - proves development F 1 by 3 . 9 % over the baseline . Averaging all biLM layers instead of using just the last layer improves F 1 another 0 . 3 % ( comparing “Last Only” to   = 1 columns ) , and allowing the task model to learn individual layer weights im - proves F 1 another 0 . 2 % (   = 1 vs .   = 0 . 001 ) . A small   is preferred in most cases with ELMo , al - though for NER , a task with a smaller training set , the results are insensitive to   ( not shown ) . The overall trend is similar with CoVe but with smaller increases over the baseline . For SNLI , av - eraging all layers with   = 1 improves development accuracy from 88 . 2 to 88 . 7 % over using just the last layer . SRL F 1 increased a marginal 0 . 1 % to 82 . 2 for the   = 1 case compared to using the last layer only . 5 . 2 Where to include ELMo ? All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN . However , we ﬁnd that including ELMo at the output of the biRNN in task - speciﬁc architec - tures improves overall results for some tasks . As shown in Table 3 , including ELMo at both the in - put and output layers for SNLI and SQuAD im - proves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer . One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM’s internal representations . In the SRL case , 2232 Source Nearest Neighbors GloVe play playing , game , games , played , players , plays , player , Play , football , multiplayer biLM Chico Ruiz made a spec - tacular play on Alusik ’s grounder { . . . } Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all - round excellent play . Olivia De Havilland signed to do a Broadway play for Garson { . . . } { . . . } they were actors who had been handed fat roles in a successful play , and had talent enough to ﬁll the roles competently , with nice understatement . Table 4 : Nearest neighbors to “play” using GloVe and the context embeddings from a biLM . Model F 1 WordNet 1st Sense Baseline 65 . 9 Raganato et al . ( 2017a ) 69 . 9 Iacobacci et al . ( 2016 ) 70 . 1 CoVe , First Layer 59 . 4 CoVe , Second Layer 64 . 7 biLM , First layer 67 . 4 biLM , Second layer 69 . 0 Table 5 : All - words ﬁne grained WSD F 1 . For CoVe and the biLM , we report scores for both the ﬁrst and second layer biLSTMs . the task - speciﬁc context representations are likely more important than those from the biLM . 5 . 3 What information is captured by the biLM’s representations ? Since adding ELMo improves task performance over word vectors alone , the biLM’s contextual representations must encode information gener - ally useful for NLP tasks that is not captured in word vectors . Intuitively , the biLM must be disambiguating the meaning of words using their context . Consider “play” , a highly poly - semous word . The top of Table 4 lists near - est neighbors to “play” using GloVe vectors . They are spread across several parts of speech ( e . g . , “played” , “playing” as verbs , and “player” , “game” as nouns ) but concentrated in the sports - related senses of “play” . In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM’s context representation of “play” in the source sen - tence . In these cases , the biLM is able to disam - biguate both the part of speech and word sense in the source sentence . These observations can be quantiﬁed using an Model Acc . Collobert et al . ( 2011 ) 97 . 3 Ma and Hovy ( 2016 ) 97 . 6 Ling et al . ( 2015 ) 97 . 8 CoVe , First Layer 93 . 3 CoVe , Second Layer 92 . 8 biLM , First Layer 97 . 3 biLM , Second Layer 96 . 8 Table 6 : Test set POS tagging accuracies for PTB . For CoVe and the biLM , we report scores for both the ﬁrst and second layer biLSTMs . intrinsic evaluation of the contextual representa - tions similar to Belinkov et al . ( 2017 ) . To isolate the information encoded by the biLM , the repre - sentations are used to directly make predictions for a ﬁne grained word sense disambiguation ( WSD ) task and a POS tagging task . Using this approach , it is also possible to compare to CoVe , and across each of the individual layers . Word sense disambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1 - nearest neighbor approach , similar to Melamud et al . ( 2016 ) . To do so , we ﬁrst use the biLM to compute representations for all words in Sem - Cor 3 . 0 , our training corpus ( Miller et al . , 1994 ) , and then take the average representation for each sense . At test time , we again use the biLM to com - pute representations for a given target word and take the nearest neighbor sense from the training set , falling back to the ﬁrst sense from WordNet for lemmas not observed during training . Table 5 compares WSD results using the eval - uation framework from Raganato et al . ( 2017b ) across the same suite of four test sets in Raganato et al . ( 2017a ) . Overall , the biLM top layer rep - 2233 resentations have F 1 of 69 . 0 and are better at WSD then the ﬁrst layer . This is competitive with a state - of - the - art WSD - speciﬁc supervised model using hand crafted features ( Iacobacci et al . , 2016 ) and a task speciﬁc biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags ( Raganato et al . , 2017a ) . The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher overall performance at the second layer compared to the ﬁrst ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet ﬁrst sense baseline . POS tagging To examine whether the biLM captures basic syntax , we used the context repre - sentations as input to a linear classiﬁer that pre - dicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) ( Marcus et al . , 1993 ) . As the linear classiﬁer adds only a small amount of model capacity , this is direct test of the biLM’s representations . Similar to WSD , the biLM rep - resentations are competitive with carefully tuned , task speciﬁc biLSTMs ( Ling et al . , 2015 ; Ma and Hovy , 2016 ) . However , unlike WSD , accuracies using the ﬁrst biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi - task training ( Søgaard and Gold - berg , 2016 ; Hashimoto et al . , 2017 ) and MT ( Be - linkov et al . , 2017 ) . CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder . Implications for supervised tasks Taken to - gether , these experiments conﬁrm different layers in the biLM represent different types of informa - tion and explain why including all biLM layers is important for the highest performance in down - stream tasks . In addition , the biLM’s representa - tions are more transferable to WSD and POS tag - ging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks . 5 . 4 Sample efﬁciency Adding ELMo to a model increases the sample ef - ﬁciency considerably , both in terms of number of parameter updates to reach state - of - the - art perfor - mance and the overall training set size . For ex - ample , the SRL model reaches a maximum devel - opment F 1 after 486 epochs of training without ELMo . After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach Figure 1 : Comparison of baseline vs . ELMo perfor - mance for SNLI and SRL as the training set size is var - ied from 0 . 1 % to 100 % . Figure 2 : Visualization of softmax normalized biLM layer weights across tasks and ELMo locations . Nor - malized weights less then 1 / 3 are hatched with hori - zontal lines and those greater then 2 / 3 are speckled . the same level of performance . In addition , ELMo - enhanced models use smaller training sets more efﬁciently than mod - els without ELMo . Figure 1 compares the per - formance of baselines models with and without ELMo as the percentage of the full training set is varied from 0 . 1 % to 100 % . Improvements with ELMo are largest for smaller training sets and signiﬁcantly reduce the amount of training data needed to reach a given level of performance . In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set . 5 . 5 Visualization of learned weights Figure 2 visualizes the softmax - normalized learned layer weights . At the input layer , the task model favors the ﬁrst biLSTM layer . For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks . The output layer weights are relatively balanced , with a slight preference for the lower layers . 2234 Task GloVe ELMo ELMo ELMo + type GloVe SQuAD 80 . 8 81 . 4 85 . 3 85 . 6 SNLI 88 . 1 88 . 5 89 . 1 89 . 5 SRL 81 . 6 81 . 7 84 . 5 84 . 7 Table 7 : Development set ablation analysis for SQuAD , SNLI and SRL comparing different choices for the context - independent type representation and contextual representation . From left to right , the table compares systems with only GloVe vectors ; only the ELMo context - independent type representation with - out the ELMo biLSTM layers ; full ELMo representa - tions without GloVe ; both GloVe and ELMo . 5 . 6 Contextual vs . sub - word information In addition to the contextual information cap - tured in the biLM’s biLSTM layers , ELMo rep - resentations also contain sub - word information in the fully character based context insensitive type layer , x LMk . To analyze the relative contribu - tion of the contextual information compared to the sub - word information , we ran an additional ab - lation that replaced the GloVe vectors with just the biLM character based x LMk layer without the biLM biLSTM layers . Table 7 summarizes the re - sults for SQuAD , SNLI and SNLI . Replacing the GloVe vectors with the biLM character layer gives a slight improvement for all tasks ( e . g . from 80 . 8 to 81 . 4 F 1 for SQuAD ) , but overall the improve - ments are small compared to the full ELMo model . From this , we conclude that most of the gains in the downstream tasks are due to the contextual in - formation and not the sub - word information . 5 . 7 Are pre - trained vectors necessary with ELMo ? All of the results presented in Sec . 4 include pre - trained word vectors in addition to ELMo repre - sentations . However , it is natural to ask whether pre - trained vectors are still necessary with high quality contextualized representations . As shown in the two right hand columns of Table 7 , adding GloVe to models with ELMo generally provides a marginal improvement over ELMo only models ( e . g . 0 . 2 % F 1 improvement for SRL from 84 . 5 to 84 . 7 ) . 6 Conclusion We have introduced a general approach for learn - ing high - quality deep context - dependent represen - tations from biLMs , and shown large improve - ments when applying ELMo to a broad range of NLP tasks . Through ablations and other controlled experiments , we have also conﬁrmed that the biLM layers efﬁciently encode different types of syntactic and semantic information about words - in - context , and that using all layers improves over - all task performance . References Jimmy Ba , Ryan Kiros , and Geoffrey E . Hinton . 2016 . Layer normalization . CoRR abs / 1607 . 06450 . Yonatan Belinkov , Nadir Durrani , Fahim Dalvi , Has - san Sajjad , and James R . Glass . 2017 . What do neu - ral machine translation models learn about morphol - ogy ? In ACL . Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov . 2017 . Enriching word vectors with subword information . TACL 5 : 135 – 146 . Samuel R . Bowman , Gabor Angeli , Christopher Potts , and Christopher D . Manning . 2015 . A large an - notated corpus for learning natural language infer - ence . In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) . Association for Computational Linguis - tics . Ciprian Chelba , Tomas Mikolov , Mike Schuster , Qi Ge , Thorsten Brants , Phillipp Koehn , and Tony Robin - son . 2014 . One billion word benchmark for mea - suring progress in statistical language modeling . In INTERSPEECH . Qian Chen , Xiao - Dan Zhu , Zhen - Hua Ling , Si Wei , Hui Jiang , and Diana Inkpen . 2017 . Enhanced lstm for natural language inference . In ACL . Jason Chiu and Eric Nichols . 2016 . Named entity recognition with bidirectional LSTM - CNNs . In TACL . Kyunghyun Cho , Bart van Merrienboer , Dzmitry Bah - danau , and Yoshua Bengio . 2014 . On the properties of neural machine translation : Encoder - decoder ap - proaches . In SSST @ EMNLP . Christopher Clark and Matthew Gardner . 2017 . Sim - ple and effective multi - paragraph reading compre - hension . CoRR abs / 1710 . 10723 . Kevin Clark and Christopher D . Manning . 2016 . Deep reinforcement learning for mention - ranking corefer - ence models . In EMNLP . Ronan Collobert , Jason Weston , L´eon Bottou , Michael Karlen , Koray Kavukcuoglu , and Pavel P . Kuksa . 2011 . Natural language processing ( almost ) from scratch . In JMLR . Andrew M . Dai and Quoc V . Le . 2015 . Semi - supervised sequence learning . In NIPS . 2235 Greg Durrett and Dan Klein . 2013 . Easy victories and uphill battles in coreference resolution . In EMNLP . Yarin Gal and Zoubin Ghahramani . 2016 . A theoret - ically grounded application of dropout in recurrent neural networks . In NIPS . Yichen Gong , Heng Luo , and Jian Zhang . 2018 . Nat - ural language inference over interaction space . In ICLR . Kazuma Hashimoto , Caiming Xiong , Yoshimasa Tsu - ruoka , and Richard Socher . 2017 . A joint many - task model : Growing a neural network for multiple nlp tasks . In EMNLP 2017 . Luheng He , Kenton Lee , Mike Lewis , and Luke S . Zettlemoyer . 2017 . Deep semantic role labeling : What works and what’s next . In ACL . Sepp Hochreiter and J¨urgen Schmidhuber . 1997 . Long short - term memory . Neural Computation 9 . Ignacio Iacobacci , Mohammad Taher Pilehvar , and Roberto Navigli . 2016 . Embeddings for word sense disambiguation : An evaluation study . In ACL . Rafal J´ozefowicz , Oriol Vinyals , Mike Schuster , Noam Shazeer , and Yonghui Wu . 2016 . Exploring the lim - its of language modeling . CoRR abs / 1602 . 02410 . Rafal J´ozefowicz , Wojciech Zaremba , and Ilya Sutskever . 2015 . An empirical exploration of recur - rent network architectures . In ICML . Yoon Kim , Yacine Jernite , David Sontag , and Alexan - der M Rush . 2015 . Character - aware neural language models . In AAAI 2016 . Diederik P . Kingma and Jimmy Ba . 2015 . Adam : A method for stochastic optimization . In ICLR . Ankit Kumar , Ozan Irsoy , Peter Ondruska , Mohit Iyyer , Ishaan Gulrajani James Bradbury , Victor Zhong , Romain Paulus , and Richard Socher . 2016 . Ask me anything : Dynamic memory networks for natural language processing . In ICML . John D . Lafferty , Andrew McCallum , and Fernando Pereira . 2001 . Conditional random ﬁelds : Prob - abilistic models for segmenting and labeling se - quence data . In ICML . Guillaume Lample , Miguel Ballesteros , Sandeep Sub - ramanian , Kazuya Kawakami , and Chris Dyer . 2016 . Neural architectures for named entity recognition . In NAACL - HLT . Kenton Lee , Luheng He , Mike Lewis , and Luke S . Zettlemoyer . 2017 . End - to - end neural coreference resolution . In EMNLP . Wang Ling , Chris Dyer , Alan W . Black , Isabel Tran - coso , Ramon Fermandez , Silvio Amir , Lu´ıs Marujo , and Tiago Lu´ıs . 2015 . Finding function in form : Compositional character models for open vocabu - lary word representation . In EMNLP . Xiaodong Liu , Yelong Shen , Kevin Duh , and Jian - feng Gao . 2017 . Stochastic answer networks for machine reading comprehension . arXiv preprint arXiv : 1712 . 03556 . Xuezhe Ma and Eduard H . Hovy . 2016 . End - to - end sequence labeling via bi - directional LSTM - CNNs - CRF . In ACL . Mitchell P . Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz . 1993 . Building a large annotated corpus of english : The penn treebank . Computa - tional Linguistics 19 : 313 – 330 . Bryan McCann , James Bradbury , Caiming Xiong , and Richard Socher . 2017 . Learned in translation : Con - textualized word vectors . In NIPS 2017 . Oren Melamud , Jacob Goldberger , and Ido Dagan . 2016 . context2vec : Learning generic context em - bedding with bidirectional lstm . In CoNLL . G´abor Melis , Chris Dyer , and Phil Blunsom . 2017 . On the state of the art of evaluation in neural language models . CoRR abs / 1707 . 05589 . Stephen Merity , Nitish Shirish Keskar , and Richard Socher . 2017 . Regularizing and optimizing lstm lan - guage models . CoRR abs / 1708 . 02182 . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Cor - rado , and Jeff Dean . 2013 . Distributed representa - tions of words and phrases and their compositional - ity . In NIPS . George A . Miller , Martin Chodorow , Shari Landes , Claudia Leacock , and Robert G . Thomas . 1994 . Us - ing a semantic concordance for sense identiﬁcation . In HLT . Tsendsuren Munkhdalai and Hong Yu . 2017 . Neural tree indexers for text understanding . In EACL . Arvind Neelakantan , Jeevan Shankar , Alexandre Pas - sos , and Andrew McCallum . 2014 . Efﬁcient non - parametric estimation of multiple embeddings per word in vector space . In EMNLP . Martha Palmer , Paul Kingsbury , and Daniel Gildea . 2005 . The proposition bank : An annotated corpus of semantic roles . Computational Linguistics 31 : 71 – 106 . Jeffrey Pennington , Richard Socher , and Christo - pher D . Manning . 2014 . Glove : Global vectors for word representation . In EMNLP . Matthew E . Peters , Waleed Ammar , Chandra Bhaga - vatula , and Russell Power . 2017 . Semi - supervised sequence tagging with bidirectional language mod - els . In ACL . Sameer Pradhan , Alessandro Moschitti , Nianwen Xue , Hwee Tou Ng , Anders Bj¨orkelund , Olga Uryupina , Yuchen Zhang , and Zhi Zhong . 2013 . Towards ro - bust linguistic analysis using ontonotes . In CoNLL . 2236 Sameer Pradhan , Alessandro Moschitti , Nianwen Xue , Olga Uryupina , and Yuchen Zhang . 2012 . Conll - 2012 shared task : Modeling multilingual unre - stricted coreference in ontonotes . In EMNLP - CoNLL Shared Task . Alessandro Raganato , Claudio Delli Bovi , and Roberto Navigli . 2017a . Neural sequence learning models for word sense disambiguation . In EMNLP . Alessandro Raganato , Jose Camacho - Collados , and Roberto Navigli . 2017b . Word sense disambigua - tion : A uniﬁed evaluation framework and empirical comparison . In EACL . Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang . 2016 . Squad : 100 , 000 + questions for machine comprehension of text . In EMNLP . Prajit Ramachandran , Peter Liu , and Quoc Le . 2017 . Improving sequence to sequence learning with unla - beled data . In EMNLP . Erik F . Tjong Kim Sang and Fien De Meulder . 2003 . Introduction to the CoNLL - 2003 shared task : Language - independent named entity recognition . In CoNLL . Min Joon Seo , Aniruddha Kembhavi , Ali Farhadi , and Hannaneh Hajishirzi . 2017 . Bidirectional attention ﬂow for machine comprehension . In ICLR . Richard Socher , Alex Perelygin , Jean Y Wu , Jason Chuang , Christopher D Manning , Andrew Y Ng , and Christopher Potts . 2013 . Recursive deep mod - els for semantic compositionality over a sentiment treebank . In EMNLP . Anders Søgaard and Yoav Goldberg . 2016 . Deep multi - task learning with low level tasks supervised at lower layers . In ACL 2016 . Nitish Srivastava , Geoffrey E . Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdi - nov . 2014 . Dropout : a simple way to prevent neural networks from overﬁtting . Journal of Machine Learning Research 15 : 1929 – 1958 . Rupesh Kumar Srivastava , Klaus Greff , and J¨urgen Schmidhuber . 2015 . Training very deep networks . In NIPS . Joseph P . Turian , Lev - Arie Ratinov , and Yoshua Ben - gio . 2010 . Word representations : A simple and gen - eral method for semi - supervised learning . In ACL . Wenhui Wang , Nan Yang , Furu Wei , Baobao Chang , and Ming Zhou . 2017 . Gated self - matching net - works for reading comprehension and question an - swering . In ACL . John Wieting , Mohit Bansal , Kevin Gimpel , and Karen Livescu . 2016 . Charagram : Embedding words and sentences via character n - grams . In EMNLP . Sam Wiseman , Alexander M . Rush , and Stuart M . Shieber . 2016 . Learning global features for coref - erence resolution . In HLT - NAACL . Matthew D . Zeiler . 2012 . Adadelta : An adaptive learn - ing rate method . CoRR abs / 1212 . 5701 . Jie Zhou and Wei Xu . 2015 . End - to - end learning of semantic role labeling using recurrent neural net - works . In ACL . Peng Zhou , Zhenyu Qi , Suncong Zheng , Jiaming Xu , Hongyun Bao , and Bo Xu . 2016 . Text classiﬁcation improved by integrating bidirectional lstm with two - dimensional max pooling . In COLING . 2237