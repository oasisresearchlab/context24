1 Distributed Online Learning via Cooperative Contextual Bandits Cem Tekin * , Member , IEEE , Mihaela van der Schaar , Fellow , IEEE Abstract —In this paper we propose a novel framework for decentralized , online learning by many learners . At each moment of time , an instance characterized by a certain context may arrive to each learner ; based on the context , the learner can select one of its own actions ( which gives a reward and provides information ) or request assistance from another learner . In the latter case , the requester pays a cost and receives the reward but the provider learns the information . In our framework , learners are modeled as cooperative contextual bandits . Each learner seeks to maximize the expected reward from its arrivals , which involves trading off the reward received from its own actions , the information learned from its own actions , the reward received from the actions requested of others and the cost paid for these actions - taking into account what it has learned about the value of assistance from each other learner . We develop distributed online learning algorithms and provide analytic bounds to compare the efﬁciency of these with algorithms with the complete knowledge ( oracle ) benchmark ( in which the expected reward of every action in every context is known by every learner ) . Our estimates show that regret - the loss incurred by the algorithm - is sublinear in time . Our theoretical framework can be used in many practical applications including Big Data mining , event detection in surveillance sensor networks and distributed online recommendation systems . Index Terms —Online learning , distributed learning , multi - user learning , cooperative learning , contextual bandits , multi - user bandits . I . I NTRODUCTION In this paper we propose a novel framework for online learning by multiple cooperative and decentralized learners . We assume that an instance ( a data unit ) , characterized by a context ( side ) information , arrives at a learner ( processor ) which needs to process it either by using one of its own pro - cessing functions or by requesting another learner ( processor ) to process it . The learner’s goal is to learn online what is the best processing function which it should use such that it maximizes its total expected reward for that instance . A data stream is an ordered sequence of instances that can be read only once or a small number of times using limited computing and storage capabilities . For example , in a stream mining application , an instance can be the data unit extracted by a sensor or camera ; in a wireless communication application , an instance can be a packet that needs to be transmitted . The context can be anything that provides information about the rewards to the learners . For example , in stream mining , the context can be the type of the extracted instance ; in wireless communications , the context can be the channel Signal to Noise Ratio ( SNR ) . The processing functions in the stream C . Tekin and M . van der Schaar are with the Department of Electrical Engineering , UCLA . Email : cmtkn @ ucla . edu , mihaela @ ee . ucla . edu . A preliminary version of this work appeared in Allerton 2013 . The work is partially supported by the grants NSF CNS 1016081 and AFOSR DDDAS . mining application can be the various classiﬁcation functions , while in wireless communications they can be the transmission strategies for sending the packet ( Note that the selection of the processing functions by the learners can be performed based on the context and not necessarily the instance ) . The rewards in the stream mining can be the accuracy associated with the selected classiﬁcation function , and in wireless communica - tion they can be the resulting goodput and expended energy associated with a selected transmission strategy . To solve such distributed online learning problems , we deﬁne a new class of multi - armed bandit solutions , which we refer to as cooperative contextual bandits . In the considered scenario , there is a set of cooperative learners , each equipped with a set of processing functions ( arms 1 ) which can be used to process the instance . By deﬁnition , cooperative learners agree to follow the rules of a prescribed algorithm provided by a designer given that the prescriped algorithm meets the set of constraints imposed by the learners . For instance , these constraints can be privacy constraints , which limits the amount of information a learner knows about the arms of the other learners . We assume a discrete time model t = 1 , 2 , . . . , where different instances and associated context information arrive to a learner . 2 Upon the arrival of an instance , a learner needs to select either one of its arms to process the instance or it can call another learner which can select one of its own arms to process the instance and incur a cost ( e . g . , delay cost , communication cost , processing cost , money ) . Based on the selected arm , the learner receives a random reward , which is drawn from some unknown distribution that depends on the context information characterizing the instance . The goal of a learner is to maximize its total undiscounted reward up to any time horizon T . A learner does not know the expected reward ( as a function of the context ) of its own arms or of the other learners’ arms . In fact , we go one step further and assume that a learner does not know anything about the set of arms available to other learners except an upper bound on the number of their arms . The learners are cooperative because they obtain mutual beneﬁts from cooperation - a learner’s beneﬁt from calling another learner may be an increased reward as compared to the case when it uses solely its own arms ; the beneﬁt of the learner asked to perform the processing by another learner is that it can learn about the performance 1 We use the terms action and arm interchangeably . 2 Assuming synchronous agents / learners is common in the decentralized multi - armed bandit literature [ 1 ] , [ 2 ] . Although our formulation is for syn - chronous learners , our results directly apply to the asynchronous learners , where times of instance and context arrivals can be different . A learner may not receive an instance and context at every time slot t . Then , instead of the ﬁnal time T , our performance bounds for learner i will depend on the total number of arrivals to learner i by time T . 2 of its own arm based on its reward for the calling learner . This is especially beneﬁcial when certain instances and associated contexts are less frequent , or when gathering labels ( observing the reward ) is costly . The problem deﬁned in this paper is a generalization of the well - known contextual bandit problem [ 3 ] – [ 8 ] , in which there is a single learner who has access to all the arms . However , the considered distributed online learning problem is signiﬁcantly more challenging because a learner cannot observe the arms of other learners and cannot directly estimate the expected rewards of those arms . Moreover , the heterogeneous contexts arriving at each learner lead to different learning rates for the various learners . We design distributed online learning algorithms whose long - term average rewards converge to the best distributed solution which can be obtained if we assumed complete knowledge of the expected arm rewards of each learner for each context . To rigorously quantify the learning performance , we deﬁne the regret of an online learning algorithm for a learner as the difference between the expected total reward of the best decentralized arm selection scheme given complete knowl - edge about the expected arm rewards of all learners and the expected total reward of the algorithm used by the learner . Simply , the regret of a learner is the loss incurred due to the unknown system dynamics compared to the complete knowledge benchmark . We prove a sublinear upper bound on the regret , which implies that the average reward converges to the optimal average reward . The upper bound on regret gives a lower bound on the convergence rate to the optimal average reward . We show that when the contexts arriving to a learner are uniformly distributed over the context space , the regret depends on the dimension of the context space , while when the contexts arriving to the same learner are concentrated in a small region of the context space , the regret is independent of the dimension of the context space . The proposed framework can be used in numerous applica - tions including the ones given below . Example 1 : Consider a distributed recommender system in which there is a group of agents ( learners ) that are connected together via a ﬁxed network , each of whom experiences inﬂows of users to its page . Each time a user arrives , an agent chooses from among a set of items ( arms ) to offer to that user , and the user will either reject or accept each item . When choosing among the items to offer , the agent is uncertain about the user’s acceptance probability of each item , but the agent is able to observe speciﬁc background information about the user ( context ) , such as the user’s gender , location , age , etc . Users with different backgrounds will have different probabilities of accepting each item , and so the agent must learn this probability over time by making different offers . In order to promote cooperation within this network , we let each agent also recommend items of other agents to its users in addition to its own items . Hence , if the agent learns that a user with a particular context is unlikely to accept any of the agent’s items , it can recommend to the user items of another agent that the user might be interested in . The agent can get a commission from the other agent if it sells the item of the other agent . This provides the necessary incentive to cooperate . However , since agents are decentralized , they do not directly share the information that they learn over time about user preferences for their own items . Hence the agents must learn about other agent’s acceptance probabilities through their own trial and error . Example 2 : Consider a network security scenario in which autonomous systems ( ASs ) collaborate with each other to detect cyber - attacks [ 9 ] . Each AS has a set of security solutions which it can use to detect attacks . The contexts are the characteristics of the data trafﬁc in each AS . These contexts can provide valuable information about the occurrence of cyber - attacks . Since the nature of the attacks are dynamic , non - stochastic and context dependent , the efﬁciency of the various security solutions are dynamically varying , context dependent and unknown a - priori . Based on the extracted contexts ( e . g . key properties of its trafﬁc , the originator of the trafﬁc etc . ) , an AS i may route its incoming data stream ( or only the context information ) to another AS j , and if AS j detects a malicious activity based on its own security solutions , it warns AS i . Due to the privacy or security concerns , AS i may not know what security applications AS j is running . This problem can be modeled as a cooperative contextual bandit problem in which the various ASs cooperate with each other to learn online which actions they should take or which other ASs they should request to take actions in order to accurately detect attacks ( e . g . minimize the mis - detection probability of cyber - attacks ) . The remainder of the paper is organized as follows . In Section II we describe the related work and highlight the differences from our work . In Section III we describe the choices of learners , rewards , complete knowledge benchmark , and deﬁne the regret of a learning algorithm . A cooperative contextual learning algorithm that uses a non - adaptive partition of the context space is proposed and a sublinear bound on its regret is derived in Section IV . Another learning algorithm that adaptively partitions the context space of each learner is proposed in Section V , and its regret is bounded for different types of context arrivals . In Section VI we discuss the necessity of training phase which is a property of both algorithms and compare them . Finally , the concluding remarks are given in Section VII . II . R ELATED W ORK Contextual bandits have been studied before in [ 5 ] – [ 8 ] in a single agent setting , where the agent sequentially chooses from a set of arms with unknown rewards , and the rewards depend on the context information provided to the agent at each time slot . The goal of the agent is to maximize its reward by balancing exploration of arms with uncertain rewards and exploitation of the arm with the highest estimated reward . The algorithms proposed in these works are shown to achieve sublinear in time regret with respect to the complete knowledge benchmark , and the sublinear regret bounds are proved to match with lower bounds on the regret up to logarithmic factors . In all the prior work , the context space is assumed to be large and a known similarity metric over the contexts is exploited by the algorithms to estimate arm rewards together for groups of similar contexts . Groups of contexts are created by partitioning the context space . For 3 example , [ 7 ] proposed an epoch - based uniform partition of the context space , while [ 5 ] proposed a non - uniform adaptive partition . In [ 10 ] , contextual bandit methods are developed for personalized news articles recommendation and a variant of the UCB algorithm [ 11 ] is designed for linear payoffs . In [ 12 ] , contextual bandit methods are developed for data mining and a perceptron based algorithm that achieves sublinear regret when the instances are chosen by an adversary is proposed . To the best of our knowledge , our work is the ﬁrst to provide rigorous solutions for online learning by multiple cooperative learners when context information is present and propose a novel framework for cooperative contextual bandits to solve this problem . Another line of work [ 3 ] , [ 4 ] considers a single agent with a large set of arms ( often uncountable ) . Given a similarity structure on the arm space , they propose online learning algorithms that adaptively partition the arm space to get sublinear regret bounds . The algorithms we design in this paper also exploits the similarity information , but in the context space rather than the action space , to create a partition and learn through the partition . However , distributed problem formulation , creation of the partitions and how learning is performed is very different from related prior work [ 3 ] – [ 8 ] . Previously , distributed multi - user learning is only consid - ered for multi - armed bandits with ﬁnite number of arms and no context . In [ 1 ] , [ 13 ] distributed online learning algorithms that converge to the optimal allocation with logarithmic regret are proposed for the i . i . d . arm reward model , given that the optimal allocation is an orthogonal allocation in which each user selects a different arm . Considering a similar model but with Markov arm rewards , logarithmic regret algorithms are proposed in [ 14 ] , [ 15 ] , where the regret is with respect to the best static policy which is not generally optimal for Markov rewards . This is generalized in [ 2 ] to dynamic resource sharing problems and logarithmic regret results are also proved for this case . A multi - armed bandit approach is proposed in [ 16 ] to solve decentralized constraint optimization problems ( DCOPs ) with unknown and stochastic utility functions . The goal in this work is to maximize the total cumulative reward , where the cumulative reward is given as a sum of local utility functions whose values are controlled by variable assignments made ( actions taken ) by a subset of agents . The authors propose a message passing algorithm to efﬁciently compute a global upper conﬁdence bound on the joint variable assignment , which leads to logarithmic in time regret . In contrast , in our formulation we consider a problem in which rewards are driven by contexts , and the agents do not know the set of actions of the other agents . In [ 17 ] a combinatorial multi - armed bandit problem is proposed in which the reward is a linear combination of a set of coefﬁcients of a multi - dimensional action vector and an instance vector generated by an unknown i . i . d . process . They propose an upper conﬁdence bound algorithm that computes a global conﬁdence bound for the action vector which is the sum of the upper conﬁdence bounds computed separately for each dimension . Under the proposed i . i . d . model , this algorithm achieves regret that grows logarithmically in time and polynomially in the dimension of the vector . We provide a detailed comparison between our work and related work in multi - armed bandit learning in Table I . Our cooperative contextual learning framework can be seen as an important extension of the centralized contextual bandit framework [ 3 ] – [ 8 ] . The main differences are : ( i ) training phase which is required due to the informational asymmetries between learners , ( ii ) separation of exploration and exploita - tion over time instead of using an index for each arm to balance them , resulting in three - phase learning algorithms with training , exploration and exploitation phases , ( iii ) coordinated context space partitioning in order to balance the differences in reward estimation due to heterogeneous context arrivals to the learners . Although we consider a three - phase learning structure , our learning framework can work together with index - based policies such as the ones proposed in [ 5 ] , by restricting the index updates to time slots that are not in the training phase . Our three - phase learning structure separates exploration and exploitation into distinct time slots , while they take place concurrently for an index - based policy . We will discuss the differences between these methods in Section VI . We will also show in Section VI that the training phase is necessary for the learners to form correct estimates about each other’s rewards in cooperative contextual bandits . Different from our work , distributed learning is also con - sidered in online convex optimization setting [ 18 ] – [ 20 ] . In all of these works local learners choose their actions ( parameter vectors ) to minimize the global total loss by exchanging messages with their neighbors and performing subgradient descent . In contrast to these works in which learners share information about their actions , the learners in our model does not share any information about their own actions . The information shared in our model is the context information of the calling learner and the reward generated by the arm of the called learner . However , this information is not shared at every time slot , and the rate of information sharing between learners who cannot help each other to gain higher rewards goes to zero asymptotically . In addition to the aforementioned prior work , in our recent work [ 21 ] we consider online learning in a decentralized social recommender system . In this related work , we address the challenges of decentralization , cooperation , incentives and privacy that arises in a network of recommender systems . We model the item recommendation strategy of a learner as a combinatorial learning problem , and prove that learning is much faster when the purchase probabilities of the items are independent of each other . In contrast , in this work we pro - pose the general theoretical model of cooperative contextual bandits which can be applied in a variety of decentralized online learning settings including wireless sensor surveillance networks , cognitive radio networks , network security applica - tions , recommender systems , etc . We show how context space partition can be adapted based on the context arrival process and prove the necessity of the training phase . III . P ROBLEM F ORMULATION The system model is shown in Fig . 1 . There are M learners which are indexed by the set M = { 1 , 2 , . . . , M } . Let 4 [ 5 ] – [ 8 ] [ 2 ] , [ 13 ] , [ 22 ] This work Multi - user no yes yes Cooperative N / A yes yes Contextual yes no yes Context arrival arbitrary N / A arbitrary processsynchronous ( syn ) / N / A syn both asynchronous ( asn ) Regret sublinear logarithmic sublinear TABLE I C OMPARISON WITH RELATED WORK IN MULTI - ARMED BANDITS M − i : = M − { i } be the set of learners learner i can choose from to receive a reward . Let F i denote the set of arms of learner i . Let F : = ∪ j ∈M F j denote the set of all arms . Let K i : = F i ∪ M − i . We call K i the set of choices for learner i . We use index k to denote any choice in K i , f to denote arms of the learners , j to denote other learners in M − i . Let M i : = | M − i | , F i : = | F i | and K i : = | K i | , where | · | is the cardinality operator . A summary of notations is provided in Appendix B . The learners operate under the following privacy constraint : A learner’s set of arms is its private information . This is im - portant when the learners want to cooperate to maximize their rewards , but do not want to reveal their technology / methods . For instance in stream mining , a learner may not want to reveal the types of classiﬁers it uses to make predictions , or in network security a learner may not want to reveal how many nodes it controls in the network and what types of security protocols it uses . However , each learner knows an upper bound on the number of arms the other learners have . Since the learners are cooperative , they can follow the rules of any learning algorithm as long as the proposed learning algorithm satisﬁes the privacy constraint . In this paper , we design such a learning algorithm and show that it is optimal in terms of average reward . These learners work in a discrete time setting t = 1 , 2 , . . . , T , where the following events happen sequentially , in each time slot : ( i ) an instance with context x i ( t ) arrives to each learner i ∈ M ; ( ii ) based on x i ( t ) , learner i either chooses one of its arms f ∈ F i or calls another learner and sends x i ( t ) ; 3 ( iii ) for each learner who called learner i at time t , learner i chooses one of its arms f ∈ F i ; ( iv ) learner i observes the rewards of all the arms f ∈ F i it had chosen both for its own contexts and for other learners ; ( v ) learner i either obtains directly the reward of its own arm it had chosen , or a reward that is passed from the learner that it had called for its own context . 4 The contexts x i ( t ) come from a bounded D dimensional space X , which is taken to be [ 0 , 1 ] D without loss of general - ity . When selected , an arm f ∈ F generates a random reward sampled from an unknown , context dependent distribution G f ( x ) with support in [ 0 , 1 ] . 5 The expected reward of arm f ∈ F for context x ∈ X is denoted by π f ( x ) . Learner i 3 An alternative formulation is that learner i selects multiple choices from K i at each time slot , and receives sum of the rewards of the selected choices . All of the ideas / results in this paper can be extended to this case as well . 4 Although in our problem description the learners are synchronized , our model also works for the case where instance / context arrives asynchronously to each learner . We discuss more about this in [ 9 ] . 5 Our results can be generalized to rewards with bounded support [ b 1 , b 2 ] for −∞ < b 1 < b 2 < ∞ . This will only scale our performance bounds by a constant factor . Fig . 1 . System model from the viewpoint of learners i and j . Here i exploits j to obtain a high reward while helping j to learn about the reward of its own arm . incurs a known deterministic and ﬁxed cost d ik for selecting choice k ∈ K i . 6 For example for k ∈ F i , d ik can represent the cost of activating arm k , while for k ∈ M − i , d ik can represent the cost of communicating with learner k and / or the payment made to learner k . Although in our system model we assume that each learner i can directly call another learner j , our model can be generalized to learners over a network where calling learners that are away from learner i has a higher cost for learner i . Learner i knows the set of other learners M − i and costs of calling them , i . e . , d ij , j ∈ M − i , but does not know the set of arms F j , j ∈ M − i , but only knows an upper bound on the number of arms that each learner has , i . e . , F max on F j , j ∈ M − i . Since the costs are bounded , without loss of generality we assume that costs are normalized , i . e . , d i k ∈ [ 0 , 1 ] for k ∈ K i , i ∈ M . The net reward of learner i from a choice is equal to the obtained reward minus cost of selecting the choice . The net reward of a learner is always in [ − 1 , 1 ] . The learners are cooperative which implies that when called by learner i , learner j will choose one of its own arms which it believes to yield the highest expected reward given the context of learner i . The expected reward of an arm is similar for similar contexts , which is formalized in terms of a H¨older condition given in the following assumption . Assumption 1 : There exists L > 0 , α > 0 such that for all f ∈ F and for all x , x (cid:48) ∈ X , we have | π f ( x ) − π f ( x (cid:48) ) | ≤ L | | x − x (cid:48) | | α , where | | · | | denotes the Euclidian norm in R D . 6 Alternatively , we can assume that the costs are random variables with bounded support whose distribution is unknown . In this case , the learners will not learn the reward but they will learn reward minus cost which is essentially the same thing . However , our performance bounds will be scaled by a constant factor . 5 We assume that α is known by the learners . In the contextual bandit literature this is referred to as similarity information [ 5 ] , [ 23 ] . Different from prior works on contextual bandit , we do not require L to be known by the learners . However , L will appear in our performance bounds . The goal of learner i is to maximize its total expected reward . In order to do this , it needs to learn the rewards from its choices . Thus , learner i should concurrently explore the choices in K i to learn their expected rewards , and exploit the best believed choice for its contexts which maximizes the reward minus cost . In the next subsection we formally deﬁne the complete knowledge benchmark . Then , we deﬁne the regret which is the performance loss due to uncertainty about arm rewards . A . Optimal Arm Selection Policy with Complete Information We deﬁne learner j ’s expected reward for context x as π j ( x ) : = π f ∗ j ( x ) ( x ) , where f ∗ j ( x ) : = arg max f ∈F j π f ( x ) . This is the maximum expected reward learner j can provide when called by a learner with context x . For learner i , µ ik ( x ) : = π k ( x ) − d ik denotes the net reward of choice k ∈ K i for context x . Our benchmark when evaluating the performance of the learning algorithms is the optimal solution which selects the choice with the highest expected net reward for learner i for its context x . This is given by k ∗ i ( x ) : = arg max k ∈K i µ ik ( x ) ∀ x ∈ X . ( 1 ) Since knowing µ ij ( x ) requires knowing π f ( x ) for f ∈ F j , knowing the optimal solution means that learner i knows the arm in F that yields the highest expected reward for each x ∈ X . B . The Regret of Learning Let a i ( t ) be the choice selected by learner i at time t . Since learner i has no a priori information , this choice is only based on the past history of selections and reward observations of learner i . The rule that maps the history of learner i to its choices is called the learning algorithm of learner i . Let a ( t ) : = ( a 1 ( t ) , . . . , a M ( t ) ) be the choice vector at time t . We let b i , j ( t ) denote the arm selected by learner i when it is called by learner j at time t . If j does not call i at time t , then b i , j ( t ) = ∅ . Let b i ( t ) = { b i , j ( t ) } j ∈M − i and b ( t ) = { b i ( t ) } i ∈M . The regret of learner i with respect to the complete knowledge benchmark k ∗ i ( x i ( t ) ) given in ( 1 ) is given by R i ( T ) : = T (cid:88) t = 1 (cid:16) π k ∗ i ( x i ( t ) ) ( x i ( t ) ) − d ik ∗ i ( x i ( t ) ) (cid:17) − E (cid:34) T (cid:88) t = 1 r ia i ( t ) ( x i ( t ) , t ) − d ia i ( t ) (cid:35) where r ia i ( t ) ( x i ( t ) , t ) denotes the random reward of choice a i ( t ) ∈ K i for context x at time t for learner i , and the expectation is taken with respect to the selections made by the distributed algorithm of the learners and the statistics of the rewards . For example , when a i ( t ) = j and b j , i ( t ) = f ∈ F j , this random reward is sampled from the distribution of arm f . Regret gives the convergence rate of the total expected reward of the learning algorithm to the value of the optimal solution given in ( 1 ) . Any algorithm whose regret is sublinear , i . e . , R ( T ) = O ( T γ ) such that γ < 1 , will converge to the optimal solution in terms of the average reward . In the subsequent sections we will propose two different distributed learning algorithms with sublinear regret . IV . A DISTRIBUTED UNIFORM CONTEXT PARTITIONING ALGORITHM The algorithm we consider in this section forms at the beginning a uniform partition of the context space for each learner . Each learner estimates its choice rewards based on the past history of arrivals to each set in the partition inde - pendently from the other sets in the partition . This distributed learning algorithm is called Contextual Learning with Uniform Partition ( CLUP ) and its pseudocode is given in Fig . 2 , Fig . 3 and Fig . 4 . For learner i , CLUP is composed of two parts . The ﬁrst part is the maximization part ( see Fig . 3 ) , which is used by learner i to maximize its reward from its own contexts . The second part is the cooperation part ( see Fig . 4 ) , which is used by learner i to help other learners maximize their rewards for their own contexts . Let m T be the slicing parameter of CLUP that determines the number of sets in the partition of the context space X . When m T is small , the number of sets in the partition is small , hence the number of contexts from the past observations which can be used to form reward estimates in each set is large . However , when m T is small , the size of each set is large , hence the variation of the expected choice rewards over each set is high . First , we will analyze the regret of CLUP for a ﬁxed m T and then optimize over it to balance the aforementioned tradeoff . CLUP forms a partition of [ 0 , 1 ] D consisting of ( m T ) D sets where each set is a D - dimensional hypercube with dimensions 1 / m T × 1 / m T × . . . × 1 / m T . We use index p to denote a set in P T . For learner i let p i ( t ) be the set in P T which x i ( t ) belongs to . 7 First , we will describe the maximization part of CLUP . At time slot t learner i can be in one of the three phases : training phase in which learner i calls another learner with its context such that when the reward is received , the called learner can update the estimated reward of its selected arm ( but learner i does not update the estimated reward of the selected learner ) , exploration phase in which learner i selects a choice in K i and updates its estimated reward , and exploitation phase in which learner i selects the choice with the highest estimated net reward . Recall that the learners are cooperative . Hence , when called by another learner , learner i will choose its arm with the highest estimated reward for the calling learner’s context . To gain the highest possible reward in exploitations , learner i must have an accurate estimate of other learners’ expected rewards without observing the arms selected by them . In order to do this , before forming estimates about the expected reward of learner j , learner i needs to make sure that learner j will almost always select its best arm when called by learner i . 7 If x i ( t ) is an element of the boundary of multiple sets , then it is randomly assigned to one of these sets . 6 CLUP for learner i : 1 : Input : D 1 ( t ) , D 2 ( t ) , D 3 ( t ) , T , m T 2 : Initialize sets : Create partition P T of [ 0 , 1 ] D into ( m T ) D identical hypercubes 3 : Initialize counters : N ip = 0 , ∀ p ∈ P T , N ik , p = 0 , ∀ k ∈ K i , p ∈ P T , N tr , i j , p = 0 , ∀ j ∈ M − i , p ∈ P T 4 : Initialize estimates : ¯ r ik , p = 0 , ∀ k ∈ K i , p ∈ P T 5 : while t ≥ 1 do 6 : Run CLUPmax to get choice a i , p = p i ( t ) and train 7 : If a i ∈ M − i call learner a i and pass x i ( t ) 8 : Receive C i ( t ) , the set of learners who called i , and their contexts 9 : if C i ( t ) (cid:54) = ∅ then 10 : Run CLUPcoop to get arms to be selected b i : = { b i , j } j ∈C i ( t ) and sets that the contexts lie in p i : = { p i , j } j ∈C i ( t ) 11 : end if 12 : if a i ∈ F i then 13 : Pay cost d ia i , receive random reward r drawn from G a i ( x i ( t ) ) 14 : else 15 : Pay cost d ia i , receive random reward r drawn from G b ai , i ( x i ( t ) ) 16 : end if 17 : if train = 1 then 18 : N tr , i a i , p + + 19 : else 20 : ¯ r ia i , p = ¯ r iai , p N iai , p + r N iai , p + 1 21 : N ip + + , N ia i , p + + 22 : end if 23 : if C i ( t ) (cid:54) = ∅ then 24 : for j ∈ C i ( t ) do 25 : Observe random reward r drawn from G b i , j ( x j ( t ) ) 26 : ¯ r ib i , j , p i , j = ¯ r ibi , j , pi , j N ibi , j , pi , j + r N ibi , j , pi , j + 1 27 : N ip i , j + + , N ib i , j , p i , j + + 28 : end for 29 : end if 30 : t = t + 1 31 : end while Fig . 2 . Pseudocode for CLUP algorithm . Thus , the training phase of learner i helps other learners build accurate estimates about rewards of their arms , before learner i uses any rewards from these learners to form reward estimates about them . In contrast , the exploration phase of learner i helps it to build accurate estimates about rewards of its choices . These two phases indirectly help learner i to maximize its total expected reward in the long run . Next , we deﬁne the counters learner i keeps for each set in P T for each choice in K i , which are used to decide its current phase . Let N ip ( t ) be the number of context arrivals to learner i in p ∈ P T by time t ( its own arrivals and arrivals to other learners who call learner i ) except the training phases of learner i . For f ∈ F i , let N if , p ( t ) be the number of times arm f is selected in response to a context arriving to set p by learner i by time t ( including times other learners select learner i for their contexts in set p ) . Other than these , learner i keeps two counters for each other learner in each set in the partition , which it uses to decide training , exploration or exploitation . The ﬁrst one , i . e . , N tr , i j , p ( t ) , is an estimate on the number of context arrivals to learner j from all learners except the training phases of learner j and exploration , exploitation phases of learner i . This is an estimate because learner i CLUPmax ( maximization part of CLUP ) for learner i : 1 : train = 0 2 : Find the set in P T that x i ( t ) belongs to , i . e . , p i ( t ) 3 : Let p = p i ( t ) 4 : Compute the set of under - explored arms F ue i , p ( t ) given in ( 2 ) 5 : if F ue i , p ( t ) (cid:54) = ∅ then 6 : Select a i randomly from F ue i , p ( t ) 7 : else 8 : Compute the set of training candidates M ct i , p ( t ) given in ( 3 ) 9 : / / Update the counters of training candidates 10 : for j ∈ M ut i , p ( t ) do 11 : Obtain N jp from learner j , set N tr , i j , p = N jp − N ij , p 12 : end for 13 : Compute the set of under - trained learners M ut i , p ( t ) given in ( 4 ) 14 : Compute the set of under - explored learners M ue i , p ( t ) given in ( 5 ) 15 : if M ut i , p ( t ) (cid:54) = ∅ then 16 : Select a i randomly from M ut i , p ( t ) , train = 1 17 : else if M ue i , p ( t ) (cid:54) = ∅ then 18 : Select a i randomly from M ue i , p ( t ) 19 : else 20 : Select a i randomly from arg max k ∈K i ¯ r ik , p − d ik 21 : end if 22 : end if Fig . 3 . Pseudocode for the maximization part of CLUP algorithm . CLUPcoop ( cooperation part of CLUP ) for learner i : 1 : for j ∈ C i ( t ) do 2 : Find the set in P T that x j ( t ) belongs to , i . e . , p i , j 3 : Compute the set of under - explored arms F ue i , p i , j ( t ) given in ( 2 ) 4 : if F ue i , p i , j ( t ) (cid:54) = ∅ then 5 : Select b i , j randomly from F ue i , p i , j ( t ) 6 : else 7 : b i , j = arg max f ∈F i ¯ r if , p i , j 8 : end if 9 : end for Fig . 4 . Pseudocode for the cooperation part of CLUP algorithm . updates this counter only when it needs to train learner j . The second one , i . e . , N ij , p ( t ) , counts the number of context arrivals to learner j only from the contexts of learner i in set p at times learner i selected learner j in its exploration and exploitation phases by time t . Based on the values of these counters at time t , learner i either trains , explores or exploits a choice in K i . This three - phase learning structure is one of the major components of our learning algorithm which makes it different than the algorithms proposed for the contextual bandits in the literature which assigns an index to each choice and selects the choice with the highest index . At each time slot t , learner i ﬁrst identiﬁes p i ( t ) . Then , it chooses its phase at time t by giving highest priority to exploration of its own arms , second highest priority to training of other learners , third highest priority to exploration of other learners , and lowest priority to exploitation . The reason that exploration of own arms has a higher priority than training of other learners is that it can reduce the number of trainings required by other learners , which we will describe below . First , learner i identiﬁes its set of under - explored arms : F ue i , p ( t ) : = { f ∈ F i : N if , p ( t ) ≤ D 1 ( t ) } ( 2 ) where D 1 ( t ) is a deterministic , increasing function of t which 7 is called the control function . We will specify this function later , when analyzing the regret of CLUP . The accuracy of reward estimates of learner i for its own arms increases with D 1 ( t ) , hence it should be selected to balance the tradeoff between accuracy and the number of explorations . If this set is non - empty , learner i enters the exploration phase and randomly selects an arm in this set to explore it . Otherwise , learner i identiﬁes the set of training candidates : M ct i , p ( t ) : = { j ∈ M − i : N tr , i j , p ( t ) ≤ D 2 ( t ) } ( 3 ) where D 2 ( t ) is a control function similar to D 1 ( t ) . Accuracy of other learners’ reward estimates of their own arms increase with D 2 ( t ) , hence it should be selected to balance the possible reward gain of learner i due to this increase with the reward loss of learner i due to number of trainings . If this set is non - empty , learner i asks the learners j ∈ M ct i , p ( t ) to report N jp ( t ) . Based in the reported values it recomputes N tr , i j , p ( t ) as N tr , i j , p ( t ) = N jp ( t ) − N ij , p ( t ) . Using the updated values , learner i identiﬁes the set of under - trained learners : M ut i , p ( t ) : = { j ∈ M − i : N tr , i j , p ( t ) ≤ D 2 ( t ) } . ( 4 ) If this set is non - empty , learner i enters the training phase and randomly selects a learner in this set to train it . 8 When M ct i , p ( t ) or M ut i , p ( t ) is empty , this implies that there is no under - trained learner , hence learner i checks if there is an under - explored choice . The set of learners that are under - explored by learner i is given by M ue i , p ( t ) : = { j ∈ M − i : N ij , p ( t ) ≤ D 3 ( t ) } ( 5 ) where D 3 ( t ) is also a control function similar to D 1 ( t ) . If this set is non - empty , learner i enters the exploration phase and randomly selects a choice in this set to explore it . Otherwise , learner i enters the exploitation phase in which it selects the choice with the highest estimated net reward , i . e . , a i ( t ) ∈ arg max k ∈K i ¯ r ik , p ( t ) − d ik ( 6 ) where ¯ r ik , p ( t ) is the sample mean estimate of the rewards learner i observed ( not only collected ) from choice k by time t , which is computed as follows . For j ∈ M − i , let E i j , p ( t ) be the set of rewards collected by learner i at times it selected learner j while learner i ’s context is in set p in its exploration and exploitation phases by time t . For estimating the rewards of its own arms , learner i can also use the rewards obtained by other learner at times they called learner i . In order to take this into account , for f ∈ F i , let E if , p ( t ) be the set of rewards collected by learner i at times it selected its arm f for its own contexts in set p union the set of rewards observed by learner i when it selected its arm f for other learners calling it with contexts in set p by time t . Therefore , sample mean 8 Most of the regret bounds proposed in this paper can also be achieved by setting N tr , i j , p ( t ) to be the number of times learner i trains learner j by time t , without considering other context observations of learner j . However , by recomputing N tr , i j , p ( t ) , learner i can avoid many unnecessary trainings especially when own context arrivals of learner j is adequate for it to form accurate estimates about its arms for set p or when learners other than learner i have already helped learner j to build accurate estimates for its arms in set p . reward of choice k ∈ K i in set p for learner i is deﬁned as ¯ r ik , p ( t ) = ( (cid:80) r ∈E ik , p ( t ) r ) / | E ik , p ( t ) | . An important observation is that computation of ¯ r ik , p ( t ) does not take into account the costs related to selecting choice k . Reward generated by an arm only depends on the context it is selected at but not on the identity of the learner for whom that arm is selected . However , the costs incurred depend on the identity of the learner . Let ˆ µ ik , p ( t ) : = ¯ r ik , p ( t ) − d ik be the estimated net reward of choice k for set p . Of note , when there is more than one maximizer of ( 6 ) , one of them is randomly selected . In order to run CLUP , learner i does not need to keep the sets E ik , p ( t ) in its memory . ¯ r ik , p ( t ) can be computed by using only ¯ r ik , p ( t − 1 ) and the reward at time t . The cooperation part of CLUP operates as follows . Let C i ( t ) be the learners who call learner i at time t . For each j ∈ C i ( t ) , learner i ﬁrst checks if it has any under - explored arm f for p j ( t ) , i . e . , f such that N if , p j ( t ) ( t ) ≤ D 1 ( t ) . If so , it randomly selects one of its under - explored arms and provides its reward to learner j . Otherwise , it exploits its arm with the highest estimated reward for learner j ’s context , i . e . , b i , j ( t ) ∈ arg max f ∈F i ¯ r if , p j ( t ) ( t ) . ( 7 ) A . Analysis of the Regret of CLUP Let β a : = (cid:80) ∞ t = 1 1 / t a , and let log ( . ) denote logarithm in base e . For each set ( hypercube ) p ∈ P T let π f , p : = sup x ∈ p π f ( x ) , π f , p : = inf x ∈ p π f ( x ) , for f ∈ F , and µ ik , p : = sup x ∈ p µ ik ( x ) , µ ik , p : = inf x ∈ p µ ik ( x ) , for k ∈ K i . Let x ∗ p be the context at the center ( center of symmetry ) of the hypercube p . We deﬁne the optimal choice of learner i for set p as k ∗ i ( p ) : = arg max k ∈K i µ ik ( x ∗ p ) . When the set p is clear from the context , we will simply denote the optimal choice for set p with k ∗ i . Let L ip ( t ) : = (cid:110) k ∈ K i such that µ ik ∗ i ( p ) , p − µ ik , p > At θ (cid:111) be the set of suboptimal choices for learner i for hypercube p at time t , where θ < 0 , A > 0 are parameters that are only used in the analysis of the regret and do not need to be known by the learners . First , we will give regret bounds that depend on values of θ and A and then we will optimize over these values to ﬁnd the best bound . Also related to this let F jp ( t ) : = (cid:110) f ∈ F j such that π f ∗ j ( p ) , p − π f , p > At θ (cid:111) be the set of suboptimal arms of learner j for hypercube p at time t , where f ∗ j ( p ) = arg max f ∈F j π f ( x ∗ p ) . Also when the set p is clear from the context we will just use f ∗ j . The arms in F jp ( t ) are the ones that learner j should not select when called by another learner . The regret given in ( 1 ) can be written as a sum of three components : R i ( T ) = E [ R e i ( T ) ] + E [ R s i ( T ) ] + E [ R n i ( T ) ] , where R ei ( T ) is the regret due to trainings and explorations by time T , R si ( T ) is the regret due to suboptimal choice selections in exploitations by time T and R ni ( T ) is the regret due to near optimal choice selections in exploitations by time T , which are all random variables . In the following lemmas we will bound each of these terms separately . The following lemma bounds E [ R ei ( T ) ] . 8 Lemma 1 : When CLUP is run by all learners with param - eters D 1 ( t ) = t z log t , D 2 ( t ) = F max t z log t , D 3 ( t ) = t z log t and m T = (cid:100) T γ (cid:101) , 9 where 0 < z < 1 and 0 < γ < 1 / D , we have E [ R ei ( T ) ] ≤ ( m T ) D (cid:88) p = 1 2 ( F i + M i ( F max + 1 ) ) T z log T + 2 ( F i + 2 M i ) ( m T ) D ≤ 2 D + 1 Z i T z + γD log T + 2 D + 1 ( F i + 2 M i ) T γD where Z i : = ( F i + M i ( F max + 1 ) ) . ( 8 ) Proof : Since time slot t is a training or an exploration slot for learner i if and only if M ut i , p i ( t ) ( t ) ∪ M ue i , p i ( t ) ( t ) ∪ F ue i , p i ( t ) ( t ) (cid:54) = ∅ , up to time T , there can be at most (cid:100) T z log T (cid:101) exploration slots in which an arm in f ∈ F i is selected by learner i , (cid:100) F max T z log T (cid:101) training slots in which learner i selects learner j ∈ M − i , (cid:100) T z log T (cid:101) exploration slots in which learner i selects learner j ∈ M − i . Since µ ik ( x ) = π ik ( x ) − d ik ∈ [ − 1 , 1 ] for all k ∈ K i , the realized ( hence expected ) one slot loss due to any choice is bounded above by 2 . Hence , the result follows from summing the above terms and multiplying by 2 , and the fact that ( m T ) D ≤ 2 D T γD for any T ≥ 1 . From Lemma 1 , we see that the regret due to explorations is linear in the number of hypercubes ( m T ) D , hence exponential in parameter γ and z . For any k ∈ K i and p ∈ P T , the sample mean ¯ r ik , p ( t ) represents a random variable which is the average of the independent samples in set E ik , p ( t ) . Let Ξ ij , p ( t ) be the event that a suboptimal arm f ∈ F j is selected by learner j ∈ M − i , when it is called by learner i for a context in set p for the t th time in the exploitation phases of learner i . Let X ij , p ( t ) denote the random variable which is the number of times learner j selects a suboptimal arm when called by learner i in exploitation slots of learner i when the context is in set p ∈ P T by time t . Clearly , we have X i j , p ( t ) = | E ij , p ( t ) | (cid:88) t (cid:48) = 1 I ( Ξ i j , p ( t (cid:48) ) ) ( 9 ) where I ( · ) is the indicator function which is equal to 1 if the event inside is true and 0 otherwise . The following lemma bounds E [ R si ( T ) ] . Lemma 2 : Consider all learners running CLUP with param - eters D 1 ( t ) = t z log t , D 2 ( t ) = F max t z log t , D 3 ( t ) = t z log t and m T = (cid:100) T γ (cid:101) , where 0 < z < 1 and 0 < γ < 1 / D . For any 0 < φ < 1 if t − z / 2 + t φ − z + LD α / 2 t − γα ≤ At θ / 2 holds for all t ≤ T , then we have E [ R si ( T ) ] ≤ 4 ( M i + F i ) β 2 + 4 ( M i + F i ) M i F max β 2 T 1 − φ 1 − φ . Proof : Consider time t . Let W i ( t ) : = { M ut i , p i ( t ) ( t ) ∪ M ue i , p i ( t ) ( t ) ∪ F ue i , p i ( t ) ( t ) = ∅ } be the event that learner i exploits at time t . 9 For a number r ∈ R , let (cid:100) r (cid:101) be the smallest integer that is greater than or equal to r . First , we will bound the probability that learner i selects a suboptimal choice in an exploitation slot . Then , using this we will bound the expected number of times a suboptimal choice is selected by learner i in exploitation slots . Note that every time a suboptimal choice is selected by learner i , since µ ik ( x ) = π ik ( x ) − d ik ∈ [ − 1 , 1 ] for all k ∈ K i , the realized ( hence expected ) loss is bounded above by 2 . Therefore , 2 times the expected number of times a suboptimal choice is selected in an exploitation slot bounds E [ R si ( T ) ] . Let V ik ( t ) be the event that choice k is chosen at time t by learner i . We have R si ( T ) ≤ 2 (cid:80) Tt = 1 (cid:80) k ∈L ipi ( t ) ( t ) I ( V ik ( t ) , W i ( t ) ) . Adopting the standard probabilistic notation , for two events E 1 and E 2 , I ( E 1 , E 2 ) is equal to I ( E 1 ∩ E 2 ) . Taking the expectation E [ R si ( T ) ] ≤ 2 T (cid:88) t = 1 (cid:88) k ∈L ipi ( t ) ( t ) P ( V ik ( t ) , W i ( t ) ) . ( 10 ) Let B ij , p ( t ) be the event that at most t φ samples in E ij , p ( t ) are collected from suboptimal arms of learner j in hypercube p . Let B i ( t ) : = (cid:84) j ∈M − i B ij , p i ( t ) ( t ) . For a set A , let A c denote the complement of that set . For any k ∈ L ip i ( t ) ( t ) , we have { V ik ( t ) , W i ( t ) } ⊂ (cid:110) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , W i ( t ) , B i ( t ) (cid:111) ∪ (cid:110) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , W i ( t ) , B i ( t ) c (cid:111) ⊂ (cid:110) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:111) ∪ (cid:110) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:111) ∪ (cid:110) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t , ˆ µ ik ∗ i , p i ( t ) ( t ) > µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:111) ∪ { B i ( t ) c , W i ( t ) } ( 11 ) for some H t > 0 . This implies that P (cid:0) V ik ( t ) , W i ( t ) (cid:1) ≤ P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:17) + P (cid:16) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ i k ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) + P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t , ˆ µ ik ∗ i , p i ( t ) ( t ) > µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) + P ( B i ( t ) c , W i ( t ) ) . Since for any k ∈ K , ¯ µ ik , p i ( t ) = sup x ∈ p i ( t ) µ ik ( x ) , we have for any suboptimal choice k ∈ L ip i ( t ) ( t ) , P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:17) ≤ exp ( − 2 H 2 t t z log t ) ( 12 ) by Chernoff - Hoeffding bound since on event W i ( t ) at least t z log t samples are taken from each choice . Similarly , we have P (cid:16) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) ≤ exp ( − 2 ( H t − t φ − z − LD α / 2 m − α T ) 2 t z log t ) ( 13 ) 9 which follows from the fact that the maximum variation of expected rewards within p i ( t ) is at most LD α / 2 m − α T and on event B i ( t ) at most t φ observations from any choice comes from a suboptimal arm of the learner corresponding to that choice . For k ∈ L ip i ( t ) ( t ) , when 2 H t ≤ At θ ( 14 ) the three inequalities given below µ k ∗ i , p i ( t ) − µ ik , p i ( t ) > At θ ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t ˆ µ ik ∗ i , p i ( t ) ( t ) > µ ik ∗ i , p i ( t ) − H t together imply that ˆ µ ik , p i ( t ) ( t ) < ˆ µ ik ∗ i , p i ( t ) ( t ) , which implies that P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t , ˆ µ i k ∗ i , p i ( t ) ( t ) > µ i k ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) = 0 . ( 15 ) Using the results of ( 12 ) and ( 13 ) and by setting H t = t − z / 2 + t φ − z + LD α / 2 t − γα ( 16 ) ≥ t − z / 2 + t φ − z + LD α / 2 m − α T we get P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:17) ≤ t − 2 ( 17 ) and P (cid:16) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) ≤ t − 2 . ( 18 ) All that is left is to bound P ( B i ( t ) c , W i ( t ) ) . Applying the union bound , we have P ( B i ( t ) c , W i ( t ) ) ≤ (cid:88) j ∈M − i P ( B ij , p i ( t ) ( t ) c , W i ( t ) ) . We have { B ij , p i ( t ) ( t ) c , W i ( t ) } = { X ij , p i ( t ) ( t ) ≥ t φ } ( Recall X ij , p i ( t ) ( t ) from ( 9 ) ) . Applying the Markov inequality we have P ( B ij , p i ( t ) ( t ) c , W i ( t ) ) ≤ E [ X ij , p i ( t ) ( t ) ] / t φ . Recall that X ij , p i ( t ) ( t ) = (cid:80) | E ij , p i ( t ) ( t ) | t (cid:48) = 1 I ( Ξ ij , p i ( t ) ( t (cid:48) ) ) , and P (cid:16) Ξ ij , p i ( t ) ( t ) (cid:17) ≤ (cid:88) m ∈F jpi ( t ) ( t ) P (cid:16) ¯ r jm , p i ( t ) ( t ) ≥ ¯ r jf ∗ j , p i ( t ) ( t ) (cid:17) ≤ (cid:88) m ∈F jpi ( t ) ( t ) (cid:16) P (cid:16) ¯ r jm , p i ( t ) ( t ) ≥ π m , p i ( t ) + H t , W i ( t ) (cid:17) + P (cid:16) ¯ r jf ∗ j , p i ( t ) ( t ) ≤ π f ∗ j , p i ( t ) − H t , W i ( t ) (cid:17) + P (cid:16) ¯ r jm , p i ( t ) ( t ) ≥ ¯ r jf ∗ j , p i ( t ) ( t ) , ¯ r jm , p i ( t ) ( t ) < π m , p i ( t ) + H t , ¯ r j f ∗ j , p i ( t ) ( t ) > π f ∗ j , p i ( t ) − H t , W i ( t ) (cid:17)(cid:17) . When ( 14 ) holds , the last probability in the sum above is equal to zero while the ﬁrst two probabilities are upper bounded by e − 2 ( H t ) 2 t z log t . This is due to the training phase of CLUP by which it is guaranteed that every learner samples each of its own arms at least t z log t times before learner i starts forming estimates about learner j . Therefore for any p ∈ P T , we have P (cid:0) Ξ ij , p ( t ) (cid:1) ≤ (cid:80) m ∈F jp ( t ) 2 e − 2 ( H t ) 2 t z log t ≤ 2 F j t − 2 for the value of H t given in ( 16 ) . These together imply that E [ X ij , p ( t ) ] ≤ (cid:80) ∞ t (cid:48) = 1 P ( Ξ ij , p ( t (cid:48) ) ) ≤ 2 F j (cid:80) ∞ t (cid:48) = 1 t − 2 . Therefore from the Markov inequality we get P ( B ij , p ( t ) c , W i ( t ) ) = P ( X ij , p ( t ) ≥ t φ ) ≤ 2 F j β 2 t − φ for any p ∈ P T and hence , P ( B i ( t ) c , W i ( t ) ) ≤ 2 M i F max β 2 t − φ . ( 19 ) Then , using ( 15 ) , ( 17 ) , ( 18 ) and ( 19 ) , we have P (cid:0) V ik ( t ) , W i ( t ) (cid:1) ≤ 2 t − 2 + 2 M i F max β 2 t − φ , for any k ∈ L ip i ( t ) ( t ) . By ( 10 ) , and by the result of Appendix A , we get the stated bound for E [ R si ( T ) ] . Each time learner i calls learner j , learner j selects one of its own arms in F j . There is a positive probability that learner j will select one of its suboptimal arms , which implies that even if learner j is near optimal for learner i , selecting learner j may not yield a near optimal outcome . We need to take this into account , in order to bound E [ R ni ( T ) ] . The next lemma bounds the expected number of such happenings . Lemma 3 : Consider all learners running CLUP with param - eters D 1 ( t ) = t z log t , D 2 ( t ) = F max t z log t , D 3 ( t ) = t z log t and m T = (cid:100) T γ (cid:101) , where 0 < z < 1 and 0 < γ < 1 / D . For any 0 < φ < 1 if t − z / 2 + t φ − z + LD α / 2 t − γα ≤ At θ / 2 holds for all t ≤ T , then we have E [ X ij , p ( t ) ] ≤ 2 F max β 2 for j ∈ M − i . Proof : The proof is contained within the proof of the last part of Lemma 2 . We will use Lemma 3 in the following lemma to bound E [ R ni ( T ) ] . Lemma 4 : Consider all learners running CLUP with param - eters D 1 ( t ) = t z log t , D 2 ( t ) = F max t z log t , D 3 ( t ) = t z log t and m T = (cid:100) T γ (cid:101) , where 0 < z < 1 and 0 < γ < 1 / D . For any 0 < φ < 1 if t − z / 2 + t φ − z + LD α / 2 t − γα ≤ At θ / 2 holds for all t ≤ T , then we have E [ R ni ( T ) ] ≤ 2 A 1 + θT 1 + θ + 6 LD α / 2 T 1 − αγ + 4 M i F max β 2 2 D T γD . Proof : At any time t , for any k ∈ K i − L ip ( t ) and x ∈ p , we have µ ik ∗ i ( x ) ( x ) − µ ik ( x ) ≤ At θ + 3 LD α / 2 T − αγ . Similarly for any j ∈ M , f ∈ F j −F jp ( t ) and x ∈ p , we have π f ∗ j ( x ) ( x ) − π f ( x ) ≤ At θ + 3 LD α / 2 T − αγ . Let p = p i ( t ) . Due to the above inequalities , if a near optimal arm in F i ∩ ( K i −L ip ( t ) ) is chosen by learner i at time t , the contribution to the regret is at most At θ + 3 LD α / 2 T − αγ . If a near optimal learner j ∈ M − i ∩ ( K i − L ip ( t ) ) is called by learner i at time t , and if learner j selects one of its near optimal arms in F j −F jp ( t ) , then the contribution to the regret is at most 2 ( At θ + 3 LD α / 2 T − αγ ) . Therefore , the total regret due to near optimal choices of learner i by time T is upper 10 bounded by 2 T (cid:88) t = 1 ( At θ + 3 LD α / 2 T − αγ ) ≤ 2 A 1 + θT 1 + θ + 6 LD α / 2 T 1 − αγ by using the result in Appendix A . Each time a near optimal learner in j ∈ M − i ∩ ( K i −L ip ( t ) ) is called in an exploitation step , there is a small probability that the arm selected by learner j is a suboptimal one . Given in Lemma 3 , the expected number of times a suboptimal arm is chosen by learner j for learner i in each hypercube p is bounded by 2 F max β 2 . For each such choice , the one - slot regret of learner i can be at most 2 , and the number of such hypercubes is bounded by 2 D T γD . In the next theorem we bound the regret of learner i by combining the above lemmas . Theorem 1 : Consider all learners running CLUP with parameters D 1 ( t ) = t 2 α / ( 3 α + D ) log t , D 2 ( t ) = F max t 2 α / ( 3 α + D ) log t , D 3 ( t ) = t 2 α / ( 3 α + D ) log t and m T = (cid:6) T 1 / ( 3 α + D ) (cid:7) . Then , we have R i ( T ) ≤ 4 ( M i + F i ) β 2 + T 2 α + D 3 α + D (cid:18) 4 ( LD α / 2 + 2 ) + 4 ( M i + F i ) M i F max β 2 ( 2 α + D ) / ( 3 α + D ) + 6 LD α / 2 + 2 D + 1 Z i log T (cid:17) + T D 3 α + D ( 2 D + 1 ( F i + 2 M i ) + 2 D + 2 M i F max β 2 ) . for any sequence of context arrivals { x i ( t ) } t ∈ 1 , . . . , T , i ∈ M . Hence , R i ( T ) = ˜ O (cid:16) MF max T 2 α + D 3 α + D (cid:17) , for all i ∈ M , where Z i is given in ( 8 ) . Proof : The highest orders of regret that come from trainings , explorations , suboptimal and near optimal arm se - lections are ˜ O ( T γD + z ) , O ( T 1 − φ ) and O ( T max { 1 − αγ , 1 + θ } ) . We need to optimize them with respect to the constraint t − z / 2 + t φ − z + LD α / 2 t − γα ≤ At θ / 2 , t ≤ T which is assumed in Lemmas 2 and 4 . The values that minimize the regret for which this constraint holds are z = 2 α / ( 3 α + D ) , φ = z / 2 , θ = − z / 2 , γ = z / ( 2 α ) and A = 2 LD α / 2 + 4 . Result follows from summing the bounds in Lemmas 1 , 2 and 4 . Remark 1 : Although the parameter m T of CLUP depends on T and hence we require T as an input to the algorithm , we can make CLUP run independently of the ﬁnal time T and achieve the same regret bound by using a well known doubling trick ( see , e . g . , [ 5 ] ) . Consider phases τ ∈ { 1 , 2 , . . . } , where each phase has length 2 τ . We run a new instance of algorithm CLUP at the beginning of each phase with time parameter 2 τ . Then , the regret of this algorithm up to any time T will be ˜ O (cid:0) T ( 2 α + D ) / ( 3 α + D ) (cid:1) . Although doubling trick works well in theory , CLUP can suffer from cold - start problems . The algorithm we will deﬁne in the next section will not require T as an input parameter . The regret bound proved in Theorem 1 is sublinear in time which guarantees convergence in terms of the average reward , i . e . , lim T →∞ E [ R i ( T ) ] / T = 0 . For a ﬁxed α , the regret becomes linear in the limit as D goes to inﬁnity . On the contrary , when D is ﬁxed , the regret decreases , and in the limit , as α goes to inﬁnity , it becomes O ( T 2 / 3 ) . This is intuitive since increasing D means that the dimension of the context increases and therefore the number of hypercubes to explore increases . While increasing α means that the level of similarity between any two pairs of contexts increases , i . e . , knowing the expected reward of arm f in one context yields more information about its accuracy in another context . B . Computational Complexity of CLUP For each set p ∈ P T , learner i keeps the sample mean of rewards from F i + M i choices , while for a centralized bandit algorithm , the sample mean of the rewards of | ∪ j ∈M F j | arms needs to be kept in memory . Since the number of sets in P T is upper bounded by 2 D T D / ( 3 α + D ) , the memory requirement is upper bounded by ( F i + M i ) 2 D T D / ( 3 α + D ) . This means that the memory requirement is sublinearly increasing in T and thus , in the limit T → ∞ , required memory goes to inﬁnity . However , CLUP can be modiﬁed so that the available memory provides an upper bound on m T . However , in this case the regret bound given in Theorem 1 may not hold . Also the actual number of hypercubes with at least one context arrival depends on the context arrival process , hence can be very small compared to the worst - case scenario . In that case , it is enough to keep the reward estimates for these hypercubes . The follow - ing example illustrates that for a practically reasonable time frame , the memory requirement is not very high for a learner compared to a non - contextual centralized implementation ( that uses partition { X } ) . For example for α = 1 , D = 1 , we have 2 D T D / ( 3 α + D ) = 2 T 1 / 4 . If learner i learned through T = 10 8 samples , and if M = 100 , F j = 100 , for all j ∈ M , learner i using CLUP only needs to store at most 40000 sample mean estimates , while a standard bandit algorithm which does not exploit any context information requires to keep 10000 sample mean estimates . Although , the memory requirement is 4 times higher than the memory requirement of a standard bandit algorithm , CLUP is suitable for a distributed implementation , learner i does not require any knowledge about the arms of other learners ( except an upper bound on the number of arms ) , and it is shown to converge to the best distributed solution . V . A DISTRIBUTED ADAPTIVE CONTEXT PARTITIONING ALGORITHM Intuitively , the loss due to selecting a suboptimal choice for a context can be further minimized if the learners inspect the regions of X with large number of context arrivals more carefully , instead of using a uniform partition of X . We do this by introducing the Distributed Context Zooming Algorithm ( DCZA ) . A . The DCZA Algorithm In the previous section , the partition P T is formed by CLUP at the beginning by choosing the slicing parameter m T . Differently , DCZA adaptively generates the partition based on how contexts arrive . Similar to CLUP , using DCZA a learner forms reward estimates for each set in its partition based only on the history related to that set . Let P i ( t ) be learner i ’s partition of X at time t and p i ( t ) denote the set in P i ( t ) that contains x i ( t ) . Using DCZA , learner i starts with P i ( 1 ) = { X } , then divides X into sets with smaller 11 sizes as time goes on and more contexts arrive . Hence the cardinality of P i ( t ) increases with t . This division is done in a systematic way to ensure that the tradeoff between the variation of expected choice rewards inside each set and the number of past observations that are used in reward estimation for each set is balanced . As a result , the regions of the context space with a lot of context arrivals are covered with sets of smaller sizes than regions of contexts space with few context arrivals . In other words , DCZA zooms into the regions of context space with large number of arrivals . An illustration that shows partition of CLUP and DCZA is given in Fig . 5 for D = 1 . As we discussed in the Section II the zooming idea have been used in a variety of multi - armed bandit problems [ 3 ] – [ 8 ] , but there are differences in the problem structure and how zooming is done . The sets in the adaptive partition of each learner are chosen from hypercubes with edge lengths coming from the set { 1 , 2 − 1 , 2 − 2 , . . . } . 10 We call a D - dimensional hypercube which has edges of length 2 − l a level l hypercube ( or level l set ) . For a hypercube p , let l ( p ) denote its level . Different from CLUP , the partition of each learner in DCZA can be different since context arrivals to learners can be different . In order to help each other , learners should know about each other’s partition . For this , whenever a new set of hypercubes is activated by learner i , learner i communicates this by sending the center and edge length of one of the hypercubes in the new set of hypercubes to other learners . Based on this information , other learners update their partition of learner i . Thus , at any time slot t all learners know P i ( t ) . This does not require a learner to keep M different partitions . It is enough for each learner to keep P ( t ) : = (cid:83) i ∈M P i ( t ) , which is the set of hypercubes that are active for at least one learner at time t . For p ∈ P ( t ) let τ ( p ) be the ﬁrst time p is activated by one of the learners and for p ∈ P i ( t ) , let τ i ( p ) be the ﬁrst time p is activated for learner i ’s partition . We will describe the activation process later , after deﬁning the counters of DCZA which are initialized and updated differently than CLUP . N ip ( t ) , p ∈ P i ( t ) counts the number of context arrivals to set p of learner i ( from its own contexts ) from times { τ i ( p ) , . . . , t − 1 } . For f ∈ F i , N i f , p ( t ) counts the number of times arm f is selected in response to contexts arriving to set p ∈ P ( t ) ( from learner i ’s own contexts or contexts of calling learners ) from times { τ ( p ) , . . . , t − 1 } . Similarly N tr , i j , p ( t ) , p ∈ P i ( t ) is an estimate on the context arrivals to learner j in set p from all learners except the training phases of learner j and exploration , exploitation phases of learner i from times { τ ( p ) , . . . , t − 1 } . Finally , N ij , p ( t ) counts the number of context arrivals to learner j from exploration and exploitation phases of learner i from times { τ i ( p ) , . . . , t − 1 } . Let E if , p ( t ) , f ∈ F i be the set of rewards ( received or observed ) by learner i at times that contribute to the increase of counter N i f , p ( t ) and E ij , p ( t ) , j ∈ M − i be the set of rewards received by learner i at times that contribute to the increase of counter N ij , p ( t ) . We have ¯ r ik , p ( t ) = ( (cid:80) r ∈E ik , p ( t ) r ) / | E ik , p ( t ) | for k ∈ K i . Training , 10 Hypercubes have advantages in cooperative contextual bandits because they are disjoint and a learner can pass information to another learner about its partition by only passing the center and edge length of its hypercubes . Fig . 5 . An illustration showing how the partition of DCZA differs from the partition of CLUP for D = 1 . As contexts arrive , DCZA zooms into regions of high number of context arrivals . exploration and exploitation within a hypercube p is controlled by control functions D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t , which depend on the level of hypercube p unlike the control functions D 1 ( t ) , D 2 ( t ) and D 3 ( t ) of CLUP , which only depend on the current time . DCZA separates training , exploration and exploitation the same way as CLUP but using control functions D 1 ( p , t ) , D 2 ( p , t ) , D 3 ( p , t ) instead of D 1 ( t ) , D 2 ( t ) , D 3 ( t ) . Learner i updates its partition P i ( t ) as follows . At the end of each time slot t , learner i checks if N ip i ( t ) ( t + 1 ) exceeds a threshold 2 ρl ( p i ( t ) ) , where ρ is the parameter of DCZA that is common to all learners . If N ip i ( t ) ( t + 1 ) ≥ 2 ρl ( p i ( t ) ) , learner i will divide p i ( t ) into 2 D level l ( p i ( t ) ) + 1 hypercubes and will note the other learners about its new partition P i ( t + 1 ) . With this division p i ( t ) is de - activated for learner i ’s partition . For a set p , let τ ﬁn i ( p ) be the time it is de - activated for learner i ’s partition . Similar to CLUP , DCZA also have maximization and coop - eration parts . The maximization part of DCZA is the same as CLUP with training , exploration and exploitation phases . The only differences are that which phase to enter is determined by comparing the counters deﬁned above with the control functions and in exploitation phase the best choice is selected based on the sample mean estimates deﬁned above . In the cooperation part at time t , learner i explores one of its under - explored arms or chooses its best arm for p j ( t ) for learner j ∈ C i ( t ) using the counters and sample mean estimates deﬁned above . Since the operation of DCZA is the same as CLUP except the differences mentioned in this section , we omitted its pseudocode to avoid repetition . B . Analysis of the Regret of DCZA Our analysis for CLUP in Section IV was for worst - case context arrivals . This means that the bound in Theorem 1 holds even when other learners never call learner i to train it , or other learners never learn by themselves . In this section we analyze the regret of DCZA under different types of context arrivals . Let K i , l ( T ) be the number of level l hypercubes of learner i that are activated by time T . In the following we deﬁne two extreme cases of correlation between the contexts arriving to different learners . Deﬁnition 1 : We call the context arrival process , solo ar - rivals if contexts only arrive to learner i , identical arrivals if 12 x i ( t ) = x j ( t ) for all i , j ∈ M , t = 1 , . . . , T . We start with a simple lemma which gives an upper bound on the highest level hypercube that is active at any time t . Lemma 5 : All the active hypercubes p ∈ P ( t ) at time t have at most a level of ρ − 1 log 2 t + 1 . Proof : Let l (cid:48) + 1 be the level of the highest level active hypercube . We must have (cid:80) l (cid:48) l = 0 2 ρl < t , otherwise the highest level active hypercube’s level will be less than l (cid:48) + 1 . We have , ( 2 ρ ( l (cid:48) + 1 ) − 1 ) / ( 2 ρ − 1 ) < t ⇒ 2 ρl (cid:48) < t ⇒ l (cid:48) < ρ − 1 log 2 t . In order to analyze the regret of DCZA , we ﬁrst bound the regret due to trainings and explorations in a level l hypercube . We do this for the solo and identical context arrival cases separately . Lemma 6 : Consider all learners that run DCZA with pa - rameters D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t . Then , for any level l hypercube the regret of learner i due to trainings and explorations by time T is bounded above by ( i ) 2 Z i ( 2 2 αl log T + 1 ) for solo context arrivals , ( ii ) 2 K i ( 2 2 αl log T + 1 ) for identical context arrivals ( given F i ≥ F j , j ∈ M − i ) . 11 Proof : The proof is similar to Lemma 1 . Note that when the context arriving to each learner is the same and | F i | ≥ | F j | , j ∈ M − i , we have N i , tr j , p ( t ) > D 2 ( p , t ) for all j ∈ M − i whenever N if , p ( t ) > D 1 ( p , t ) for all f ∈ F i . We deﬁne the set of suboptimal choices and arms for learner i in DCZA a little differently than CLUP ( suboptimality depends on the level of the hypercube but not on time ) , using the same notation as in the analysis of CLUP . Let L ip : = (cid:110) k ∈ K i : µ ik ∗ i ( p ) , p − µ ik , p > A ∗ LD α / 2 2 − l ( p ) α (cid:111) ( 20 ) be the set of suboptimal choices of learner i for a hypercube p , and F jp : = (cid:110) f ∈ F j : π f ∗ j ( p ) , p − π f , p > A ∗ LD α / 2 2 − l ( p ) α (cid:111) ( 21 ) be the set of suboptimal arms of learner j for hypercube p , where A ∗ = 2 + 4 / ( LD α / 2 ) . In the next lemma we bound the regret due to choosing suboptimal choices in the exploitation steps of learner i . Lemma 7 : Consider all learners running DCZA with pa - rameters ρ > 0 , D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t . Then , we have E [ R si ( T ) ] ≤ 4 ( M i + F i ) β 2 + 4 ( M i + F i ) M i F max β 2 T (cid:88) t = 1 2 − αl ( p i ( t ) ) . Proof : The proof of this lemma is similar to the proof of Lemma 7 , thus some steps are omitted . W i ( t ) and V ik ( t ) are deﬁned the same way as in Lemma 7 . B ij , p i ( t ) ( t ) denotes the event that at most 2 αl ( p i ( t ) ) samples in E i j , p i ( t ) ( t ) are collected from the suboptimal arms of learner j in F jp i ( t ) , and B i ( t ) : = (cid:84) j ∈M − i B ij , p i ( t ) ( t ) . We have E [ R si ( T ) ] ≤ 11 In order for the bound for identical context arrivals to hold for learner i we require that F i ≥ F j , j ∈ M − i . Hence , in order for the bound for identical context arrivals to hold for all learners , we require F i = F j for all i , j ∈ M . 2 (cid:80) Tt = 1 (cid:80) k ∈L ipi ( t ) P ( V ik ( t ) , W i ( t ) ) . Similar to Lemma 7 , we have P (cid:0) V ik ( t ) , W i ( t ) (cid:1) ≤ P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:17) + P (cid:16) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) + P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t , ˆ µ ik ∗ i , p i ( t ) ( t ) > µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) + P ( B i ( t ) c , W i ( t ) ) . Letting H t = ( LD α / 2 + 2 ) 2 − αl ( p i ( t ) ) we have P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ µ ik , p i ( t ) + H t , W i ( t ) , B i ( t ) (cid:17) ≤ t − 2 P (cid:16) ˆ µ ik ∗ i , p i ( t ) ( t ) ≤ µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) ≤ t − 2 . Since 2 H t ≤ A ∗ LD α / 2 2 − l ( p i ( t ) ) α , P (cid:16) ˆ µ ik , p i ( t ) ( t ) ≥ ˆ µ ik ∗ i , p i ( t ) ( t ) , ˆ µ ik , p i ( t ) ( t ) < µ ik , p i ( t ) + H t , ˆ µ ik ∗ i , p i ( t ) ( t ) > µ ik ∗ i , p i ( t ) − H t , W i ( t ) , B i ( t ) (cid:17) = 0 . Similar to the proof of Lemma 7 , we have P ( Ξ ij , p i ( t ) ) ≤ 2 F j t − 2 E [ X ij , p i ( t ) ] ≤ 2 F j β 2 P ( B ij , p i ( t ) ( t ) c , W i ( t ) ) ≤ 2 F j β 2 2 − αl ( p i ( t ) ) P ( B i ( t ) c , W i ( t ) ) ≤ 2 M i F max β 2 2 − αl ( p i ( t ) ) . Hence , P (cid:0) V ik ( t ) , W i ( t ) (cid:1) ≤ 2 t − 2 + 2 M i F max β 2 2 − αl ( p i ( t ) ) . In the next lemma we bound the regret of learner i due to selecting near optimal choices . Lemma 8 : Consider all learners running DCZA with pa - rameters ρ > 0 , D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t . Then , we have E [ R ni ( T ) ] ≤ 4 M i F max β 2 + 2 ( 3 + A ∗ ) LD α / 2 T (cid:88) t = 1 2 − αl ( p i ( t ) ) . Proof : For any k ∈ K i − L ip i ( t ) and x ∈ p i ( t ) , we have µ ik ∗ i ( x ) ( x ) − µ ik ( x ) ≤ ( 3 + A ∗ ) LD α / 2 2 − l ( p i ( t ) ) α . Similarly for any j ∈ M , f ∈ F j − F jp i ( t ) ( t ) and x ∈ p i ( t ) , we have π f ∗ j ( x ) ( x ) − π f ( x ) ≤ ( 3 + A ∗ ) LD α / 2 2 − l ( p i ( t ) ) α . As in the proof of Lemma 7 , we have P ( Ξ i j , p i ( t ) ( t ) ) ≤ 2 F max t − 2 . Thus , when a near optimal learner j ∈ M − i ∩ ( K i − L ip ) is called by learner i at time t , the contribu - tion to the regret from suboptimal arms of j is bounded by 4 F max t − 2 . The one - slot regret of any near optimal arm of any near optimal learner j ∈ M − i ∩ ( K i − L ip ) is bounded by 2 ( 3 + A ∗ ) LD α / 2 2 − l ( p ) α . The one - step regret of 13 any near optimal arm f ∈ F i ∩ ( K i − L ip ) is bounded by ( 3 + A ∗ ) LD α / 2 2 − l ( p ) α . The result is obtained by taking the sum up to time T . Next , we combine the results from Lemmas 6 , 7 and 8 to obtain regret bounds as a function of the number of hypercubes of each level that are activated up to time T . Theorem 2 : Consider all learners running DCZA with pa - rameters ρ > 0 , D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t . Then , for solo arrivals , we have R i ( T ) ≤ 2 C 1 ( log 2 T / ρ ) + 1 (cid:88) l = 0 K i , l ( T ) 2 2 αl log T + C 2 ( log 2 T / ρ ) + 1 (cid:88) l = 0 K i , l ( T ) 2 ( ρ − α ) l + 2 C 1 ( log 2 T / ρ ) + 1 (cid:88) l = 0 K i , l ( T ) + C 0 where C 0 = 4 β 2 ( M i + F i + M i F max ) , C 1 = Z i for solo arrivals and C 1 = K i for identical arrivals and C 2 = 4 ( M i + F i ) M i F max β 2 + 2 ( 3 + A ∗ ) LD α / 2 . Proof : The result follows from summing the results of Lemmas 6 , 7 and 8 and using Lemma 5 . Although the result in Theorem 2 bounds the regret of DCZA for an arbitrary context arrival process in terms of K i , l ( T ) ’s , it is possible to obtain context arrival process inde - pendent regret bounds by considering the worst - case context arrivals . The next corollary shows that the worst - case regret bound of DCZA matches with the worst - case regret bound of CLUP derived in Theorem 1 . Corollary 1 : Consider all learners running DCZA with pa - rameters ρ = 3 α , D 1 ( p , t ) = D 3 ( p , t ) = 2 2 αl ( p ) log t and D 2 ( p , t ) = F max 2 2 αl ( p ) log t . Then , the worst - case regret of learner i is bounded by R i ( T ) ≤ 2 2 ( D + 2 α ) ( 2 C 1 log T + C 2 ) T 2 α + D 3 α + D + 2 C 1 2 2 D T D 3 α + D + C 0 where C 0 , C 1 and C 2 are given in Theorem 2 . Proof : Since hypercube p remains active for at most 2 ρl ( p ) context arrivals within that hypercube , combining the results of Lemmas 7 and 8 , the expected loss in hypercube p in exploitation slots is at most C 2 2 ( ρ − α ) l ( p ) , where C 2 is deﬁned in Theorem 2 . However , the expected loss in hypercube p due to trainings and explorations is at least C 2 2 αl ( p ) for some constant C > 0 , and is at most 2 Z i ( 2 2 αl ( p ) log T + 1 ) as given in Lemma 6 . In order to balance the regret due to trainings and explorations with the regret incurred in exploitation within p we set ρ = 3 α . In the worst - case context arrivals , contexts arrive in a way that all level l hypercubes are divided into level l + 1 hypercubes before contexts start arriving to any of the level l + 1 hypercubes . In this way , the number of hypercubes to train and explore is maximized . Let l max be the hypercube with the maximum level that had at least one context arrival on or before T in the worst - case context arrivals . We must have l max − 1 (cid:88) l = 0 2 Dl 2 3 αl < T . Otherwise , no hypercube with level l max will have a context arrival by time T . From the above equation we get l max < 1 + ( log 2 T ) / ( D + 3 α ) . Thus , R i ( T ) ≤ 2 C 1 l max (cid:88) l = 0 2 Dl 2 2 αl log T + C 2 l max (cid:88) l = 0 2 Dl 2 2 αl + 2 C 1 l max (cid:88) l = 0 2 Dl + C 0 . VI . D ISCUSSION A . Necessity of the Training Phase In this subsection , we prove that the training phase is neces - sary to achieve sublinear regret for the cooperative contextual bandit problem for algorithms of the type CLUP and DCZA ( without the training phase ) which use ( i ) exploration control functions of the form Ct z log t , for constants C > 0 , z > 0 ; ( ii ) form a ﬁnite partition of the context space ; and ( iii ) use the sample mean estimator within each hypercube in the partition . We call this class of algorithms Simple Separation of Exploration and Exploitation ( SSEE ) algorithms . In order to show this , we consider a special case of expected arm rewards and context arrivals and show that independent of the rate of explorations , the regret of an SSEE algorithm is linear in time for any exploration control function D i ( t ) 12 of the form Ct z log t for learner i ( exploration functions of learners can be different ) . Although , our proof does not consider index - based learning algorithms , we think that similar to our construction in Theorem 3 , problem instances which will give linear regret can be constructed for any type of index policy without the training phase . Theorem 3 : Without the training phase , the regret of any SSEE algorithm is linear in time . Proof : We will construct a problem instance for which the statement of the theorem is valid . Assume that all costs d ik , k ∈ K i , i ∈ M are zero . Let M = 2 . Consider a hypercube p . We assume that at all time slots context x ∗ ∈ p arrives to learner 1 , and all the contexts that are arriving to learner 2 are outside p . Learner 1 has only a single arm m , learner 2 has two arms b and g . With an abuse of notation , we denote the expected reward of an arm f ∈ { m , b , g } at context x ∗ as π f . Assume that the arm rewards are drawn from { 0 , 1 } and the following is true for expected arm rewards : π b + C K δ < π m < π g − δ < π m + δ ( 22 ) for some δ > 0 , C K > 0 , where the value of C K will be speciﬁed later . Assume that learner 1’s exploration control function is D 1 ( t ) = t z log t , and learner 2’s exploration control 12 Here D i ( t ) is the control function that controls when to explore or exploit the choices in K i for learner i . 14 function is D 2 ( t ) = t z log t / K for some K ≥ 1 , 0 < z < 1 . 13 When we have K = 1 , when called by learner 1 in its explorations , learner 2 may always choose its suboptimal arm b since it is under - explored for learner 2 . If this happens , then in exploitations learner 1 will almost always choose its own arm instead of learner 2 , because it had estimated the accuracy of learner 2 for x ∗ incorrectly because the random rewards in explorations of learner 2 came from b . By letting K ≥ 1 , we also consider cases where only a fraction of reward samples of learner 2 for learner 1 comes from the suboptimal arm b . We will show that for any value of K ≥ 1 , there exists a problem instance of the form given in ( 22 ) such that learner 1’s regret is linear in time . Let E t be the event that time t is an exploitation slot for learner 1 . Let ˆ π m ( t ) , ˆ π 2 ( t ) be the sample mean reward of arm m and learner 2 for learner 1 at time t respectively . Let ξ τ be the event that learner 1 exploits for the τ th time by choosing its own arm . Denote the time of the τ th exploitation of learner 1 by τ ( t ) . We will show that for any ﬁnite τ , P ( ξ τ , . . . , ξ 1 ) ≥ 1 / 2 . We have by the chain rule P ( ξ τ , . . . , ξ 1 ) = P ( ξ τ | ξ τ − 1 , . . . , ξ 1 ) Pr ( ξ τ − 1 | ξ τ − 2 , . . . , ξ 1 ) . . . P ( ξ 1 ) . ( 23 ) We will continue by bounding P ( ξ τ | ξ τ − 1 , . . . , ξ 1 ) . When the event E τ ( t ) ∩ ξ τ − 1 ∩ . . . ∩ ξ 1 happens , we know that at least (cid:100) τ ( t ) z log τ ( t ) / K (cid:101) of (cid:100) τ ( t ) z log τ ( t ) (cid:101) reward samples of learner 2 for learner 1 comes from b . Let A t : = { ˆ π m ( t ) > π m − (cid:15) 1 } , B t : = { ˆ π 2 ( t ) < π g − (cid:15) 2 } and C t : = { ˆ π 2 ( t ) < ˆ π m ( t ) } , for (cid:15) 1 > 0 , (cid:15) 2 > 0 . Given (cid:15) 2 ≥ (cid:15) 1 + 2 δ , we have ( A t ∩ B t ) ⊂ C t . Consider the event { A Ct , E t } . Since on E t , learner 1 selected m at least t z log t times ( given that z is large enough such that the reward estimate of learner 1’s own arm is accurate ) , we have P ( A Ct , E t ) ≤ 1 / ( 2 t 2 ) , using a Chernoff bound . Let N g ( t ) ( N b ( t ) ) be the number of times learner 2 has chosen arm g ( b ) when called by learner 1 by time t . Let r g ( t ) ( r b ( t ) ) be the random reward of arm g ( b ) when it is chosen for the t th time by learner 2 . For η 1 > 0 , η 2 > 0 , let Z 1 ( t ) : = { ( (cid:80) N g ( t ) t (cid:48) = 1 r g ( t (cid:48) ) ) / N g ( t ) < π g + η 1 } and Z 2 ( t ) : = { ( (cid:80) N b ( t ) t (cid:48) = 1 r b ( t (cid:48) ) ) / N b ( t ) < π b + η 2 } . On the event E τ ( t ) ∩ ξ τ − 1 ∩ . . . ∩ ξ 1 , we have N g ( τ ( t ) ) / N b ( τ ( t ) ) ≤ K . Since ˆ π 2 ( t ) = (cid:16)(cid:80) N b ( t ) t (cid:48) = 1 r b ( t (cid:48) ) + (cid:80) N g ( t ) t (cid:48) = 1 r g ( t (cid:48) ) (cid:17) / ( N b ( t ) + N g ( t ) ) , We have Z 1 ( t ) ∩ Z 2 ( t ) ⇒ ˆ π 2 ( t ) < N g ( t ) π g + N b ( t ) π b + η 1 N g ( t ) + η 2 N b ( t ) N b ( t ) + N g ( t ) . ( 24 ) If π g − π b > N g ( t ) N b ( t ) ( η 1 + (cid:15) 2 ) + ( η 2 + (cid:15) 2 ) ( 25 ) then , it can be shown that the right hand side of ( 24 ) is less than π g − (cid:15) 2 . Thus given that ( 25 ) holds , we have Z 1 ( t ) ∩ Z 2 ( t ) ⊂ B t . But on the event E τ ( t ) ∩ ξ τ − 1 ∩ . . . ∩ ξ 1 , 13 Given two control functions of the form C i t z log t , i ∈ { 1 , 2 } , we can always normalize them such that one of them is t z log t and the other one is t z log t / K , and then construct the problem instance that gives linear regret based on the normalized control functions . ( 25 ) holds at τ ( t ) when π g − π b > K ( η 1 + (cid:15) 2 ) + ( η 2 + (cid:15) 2 ) . Note that if we take (cid:15) 1 = η 1 = η 2 = δ / 2 , and (cid:15) 2 = (cid:15) 1 + 2 δ = 5 δ / 2 the statement above holds for a problem instance with C K > 3 K + 3 . Since at any exploitation slot t , at least (cid:100) t z log t / K (cid:101) samples are taken by learner 2 from both arms b and g , we have P ( Z 1 ( τ ( t ) ) C ) ≤ 1 / ( 4 τ ( t ) 2 ) and P ( Z 2 ( τ ( t ) ) C ) ≤ 1 / ( 4 τ ( t ) 2 ) by a Chernoff bound ( again for z large enough as in the proofs of Theorems 1 and 2 ) . Thus P ( B τ ( t ) ) C ≤ P ( Z 1 ( τ ( t ) ) C ) + P ( Z 2 ( τ ( t ) ) C ) ≤ 1 / ( 2 τ ( t ) 2 ) . Hence P ( C Cτ ( t ) ) ≤ P ( A Cτ ( t ) ) + P ( B Cτ ( t ) ) ≤ 1 / ( τ ( t ) 2 ) , and P ( C τ ( t ) ) > 1 − 1 / ( τ ( t ) 2 ) . Continuing from ( 23 ) , we have P ( ξ τ , . . . , ξ 1 ) = (cid:0) 1 − 1 / ( τ ( t ) 2 ) (cid:1) (cid:0) 1 − 1 / ( ( τ − 1 ) ( t ) 2 ) (cid:1) . . . (cid:0) 1 − 1 / ( ( 1 ) ( t ) 2 ) (cid:1) ≥ Π τ ( t ) t (cid:48) = 2 (cid:0) 1 − 1 / ( t (cid:48) ) 2 (cid:1) > 1 / 2 ( 26 ) for all τ . This result implies that with probability greater than one half , learner 1 chooses its own arm at all of its exploitation slots , resulting in an expected per - slot regret of π g − π m > δ . Hence the regret is linear in time . B . Comparison of CLUP and DCZA In this subsection we assess the computation and memory requirements of DCZA and compare it with CLUP . DCZA needs to keep the sample mean reward estimates of K i choices for each active hypercube . A level l active hypercube becomes inactive if the context arrivals to that hypercube exceeds 2 ρl . Because of this , the number of active hypercubes at any time T may be much smaller than the number of activated hypercubes by time T . In the best - case , only one level l hypercube expe - riences context arrivals , then when that hypercube is divided into level l + 1 hypercubes , only one of these hypercubes experiences context arrivals and so on . In this case , DCZA run with ρ = 3 α creates at most 1 + ( log 2 T ) / ( 3 α ) hypercubes ( using Lemma 5 ) . In the worst - case ( given in Corollary 1 ) , DCZA creates at most 2 2 D T D / ( 3 α + D ) hypercubes . Recall that for any D and α , the number of hypercubes of CLUP creates is O ( T D / ( 3 α + D ) ) . Hence , in practice the memory requirement of DCZA can be much smaller than CLUP which requires to keep the estimates for every hypercube at all times . Finally DCZA does not require ﬁnal time T as in input while CLUP requires it . Although CLUP can be combined with the doubling trick to make it independent of T , this makes the constants that multiply the time order of the regret large . VII . C ONCLUSION In this paper we proposed a novel framework for decen - tralized , online learning by many learners . We developed two novel online learning algorithms for this problem and proved sublinear regret results for our algorithms . We discussed some implementation issues such as complexity and the memory requirement under different instance and context arrivals . Our theoretical framework can be applied to many practical settings including distributed online learning in Big Data mining , recommendation systems and surveillance applications . Co - operative contextual bandits opens a new research direction in online learning and raises many interesting questions : What are the lower bounds on the regret ? Is there a gap in the time 15 order of the lower bound compared to centralized contextual bandits due to informational asymmetries ? Can regret bounds be proved when cost of calling learner j is controlled by learner j ? In other words , what happens when a learner wants to maximize both the total reward from its own contexts and the total reward from the calls of other learners . A PPENDIX A A BOUND ON DIVERGENT SERIES For ρ > 0 , ρ (cid:54) = 1 , (cid:80) Tt = 1 1 / ( t ρ ) ≤ 1 + ( T 1 − ρ − 1 ) / ( 1 − ρ ) . Proof : See [ 24 ] . A PPENDIX B F REQUENTLY USED EXPRESSIONS Mathematical operators • O ( · ) : Big O notation . • ˜ O ( · ) : Big O notation with logarithmic terms hidden . • I ( A ) : indicator function of event A . • A c or A C : complement of set A . Notation related to underlying system • M : Set of learners . M = | M | . • F i : Set of arms of learner i . F i = | F i | . • M − i : Set of learners except i . M i = | M − i | . • K i : Set of choices of learner i . K i = | K i | . • F : Set of all arms . • X = [ 0 , 1 ] D : Context space . • D : Dimension of the context space . • π f ( x ) : Expected reward of arm f ∈ F for context x . • π j ( x ) : Expected reward of learner j ’s best arm for context x . • d ik : Cost of selecting choice k ∈ K i for learner i . • µ ik ( x ) = π k ( x ) − d ik : Expected net reward of learner i from choice k for context x . • k ∗ i ( x ) : Best choice ( highest expected net reward ) for learner i for context x . • f ∗ i ( x ) : Best arm ( highest expected reward ) of learner j for context x . • L : H ¨ older constant . α : H ¨ older exponent . Notation related to algorithms • D 1 ( t ) , D 2 ( t ) , D 3 ( t ) : Control functions . • p : Index for set of contexts ( hypercube ) . • m T : Number of slices for each dimension of the context for CLUP . • P T : Partition of X for CLUP . • P i ( t ) : Learner i ’s adaptive partition of X at time t for DCZA . • P ( t ) : Union of partitions of X of all learners for DCZA . • p i ( t ) : The set in P i ( t ) that contains x i ( t ) . • M uc i , p ( t ) : Set of learners who are training candidates of learner i at time t for set p of learner i ’s partition . • M ut i , p ( t ) : Set of learners who are under - trained by learner i at time t for set p of learner i ’s partition . • M ue i , p ( t ) : Set of learners who are under - explored by learner i at time t for set p of learner i ’s partition . • M uc i , p ( t ) : Set of learners who are training candidates of learner i at time t for set p of learner i ’s partition . R EFERENCES [ 1 ] K . Liu and Q . Zhao , “Distributed learning in multi - armed bandit with multiple players , ” IEEE Trans . Signal Process . , vol . 58 , no . 11 , pp . 5667 – 5681 , 2010 . [ 2 ] C . Tekin and M . Liu , “Online learning in decentralized multi - user spec - trum access with synchronized explorations , ” in Proc . IEEE MILCOM , 2012 . [ 3 ] R . Kleinberg , A . Slivkins , and E . Upfal , “Multi - armed bandits in metric spaces , ” in Proc . 40th Annual ACM Symposium on Theory of Computing , 2008 , pp . 681 – 690 . [ 4 ] S . Bubeck , R . Munos , G . Stoltz , and C . Szepesvari , “X - armed bandits , ” J . Mach . Learn . Res . , vol . 12 , pp . 1655 – 1695 , 2011 . [ 5 ] A . Slivkins , “Contextual bandits with similarity information , ” in Proc . 24th Annual Conf . on Learning Theory ( COLT ) , vol . 19 , June 2011 , pp . 679 – 702 . [ 6 ] M . Dudik , D . Hsu , S . Kale , N . Karampatziakis , J . Langford , L . Reyzin , and T . Zhang , “Efﬁcient optimal learning for contextual bandits , ” arXiv preprint arXiv : 1106 . 2369 , 2011 . [ 7 ] J . Langford and T . Zhang , “The epoch - greedy algorithm for contextual multi - armed bandits , ” Advances in Neural Information Processing Sys - tems , vol . 20 , pp . 1096 – 1103 , 2007 . [ 8 ] W . Chu , L . Li , L . Reyzin , and R . E . Schapire , “Contextual bandits with linear payoff functions , ” in Proc . 14th International Conf . on Artiﬁcial Intelligence and Statistics ( AISTATS ) , vol . 15 , April 2011 , pp . 208 – 214 . [ 9 ] C . Tekin and M . van der Schaar , “Decentralized online big data classiﬁcation - a bandit framework , ” Preprint : arXiv : 1308 . 4565 , 2013 . [ 10 ] L . Li , W . Chu , J . Langford , and R . E . Schapire , “A contextual - bandit approach to personalized news article recommendation , ” in Proc . 19th International Conf . on World Wide Web , 2010 , pp . 661 – 670 . [ 11 ] P . Auer , N . Cesa - Bianchi , and P . Fischer , “Finite - time analysis of the multiarmed bandit problem , ” Machine Learning , vol . 47 , pp . 235 – 256 , 2002 . [ 12 ] K . Crammer and C . Gentile , “Multiclass classiﬁcation with bandit feedback using adaptive regularization , ” Machine Learning , vol . 90 , no . 3 , pp . 347 – 383 , 2013 . [ 13 ] A . Anandkumar , N . Michael , and A . Tang , “Opportunistic spectrum access with multiple players : Learning under competition , ” in Proc . IEEE INFOCOM , March 2010 . [ 14 ] C . Tekin and M . Liu , “Online learning of rested and restless bandits , ” IEEE Trans . Inf . Theory , vol . 58 , no . 8 , pp . 5588 – 5611 , 2012 . [ 15 ] H . Liu , K . Liu , and Q . Zhao , “Learning in a changing world : Restless multiarmed bandit with unknown dynamics , ” IEEE Trans . Inf . Theory , vol . 59 , no . 3 , pp . 1902 – 1916 , 2013 . [ 16 ] R . Stranders , L . Tran - Thanh , F . M . D . Fave , A . Rogers , and N . R . Jennings , “DCOPs and bandits : Exploration and exploitation in decen - tralised coordination , ” in Proc . 11th International Conf . on Autonomous Agents and Multiagent Systems - Volume 1 , 2012 , pp . 289 – 296 . [ 17 ] Y . Gai , B . Krishnamachari , and R . Jain , “Combinatorial network op - timization with unknown variables : Multi - armed bandits with linear rewards and individual observations , ” IEEE / ACM Trans . Netw . , vol . 20 , no . 5 , pp . 1466 – 1478 , 2012 . [ 18 ] S . S . Ram , A . Nedi´c , and V . V . Veeravalli , “Distributed stochastic subgradient projection algorithms for convex optimization , ” Journal of Optimization Theory and Applications , vol . 147 , no . 3 , pp . 516 – 545 , 2010 . [ 19 ] F . Yan , S . Sundaram , S . Vishwanathan , and Y . Qi , “Distributed au - tonomous online learning : regrets and intrinsic privacy - preserving prop - erties , ” IEEE Trans . Knowl . Data Eng . , vol . 25 , no . 11 , pp . 2483 – 2493 , 2013 . [ 20 ] M . Raginsky , N . Kiarashi , and R . Willett , “Decentralized online con - vex programming with local information , ” in Proc . American Control Conference ( ACC ) , 2011 , pp . 5363 – 5369 . [ 21 ] C . Tekin , S . Zhang , and M . van der Schaar , “Distributed online learning in social recommender systems , ” IEEE J . Sel . Topics Signal Process . , vol . 8 , no . 4 , pp . 638 – 652 , Aug 2014 . [ 22 ] H . Liu , K . Liu , and Q . Zhao , “Learning in a changing world : Non - Bayesian restless multi - armed bandit , ” Techinal Report , UC Davis , October 2010 . [ 23 ] R . Ortner , “Exploiting similarity information in reinforcement learning , ” Proc . 2nd ICAART , pp . 203 – 210 , 2010 . [ 24 ] E . Chlebus , “An approximate formula for a partial sum of the divergent p - series , ” Applied Mathematics Letters , vol . 22 , no . 5 , pp . 732 – 737 , 2009 .