Show of Hands : Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions Jung In Koh Texas A & M University College Station , USA jungin @ tamu . edu Samantha Ray Texas A & M University College Station , USA sjr45 @ tamu . edu Josh Cherian Texas A & M University College Station , USA jcherian14 @ tamu . edu Paul Taele Texas A & M University College Station , USA ptaele @ tamu . edu Tracy Hammond Texas A & M University College Station , USA hammond @ tamu . edu ABSTRACT Increased virtual meeting software usage has allowed people to meet remotely in a more seamless fashion . However , compared to in - person meetings , valuable interaction cues such as impromptu group polling are less optimally executed due to increased difficulty in gauging remote participants , while also requiring prior meeting setup for automated counting with built - in polling tools . We propose a novel intelligent user interface approach for virtual meeting software that supports impromptu polling interactions by leveraging real - time hand gesture recognition and video filter feedback . We conducted studies to design and evaluate this intuitive gesture - based polling system with visual feedback . Our results demonstrated that our system was able to recognize attendees’ gestures and poll responses with reasonable accuracy , and showed improvements in hosts’ task workload performance . From our findings , our interface informs hosts of valuable results while maintaining organic gestural interaction cues with attendees similar to in - person meetings . CCS CONCEPTS • Human - centered computing → Interactive systems and tools ; User studies ; Mixed / augmented reality ; Gestural input . KEYWORDS hand gestures , virtual meetings , impromptu polling , educational interfaces , gesture elicitation ACM Reference Format : Jung In Koh , Samantha Ray , Josh Cherian , Paul Taele , and Tracy Hammond . 2022 . ShowofHands : LeveragingHandGesturalCuesinVirtualMeetingsfor Intelligent Impromptu Polling Interactions . In 27th International Conference on Intelligent User Interfaces ( IUI ’22 ) , March 22 – 25 , 2022 , Helsinki , Finland . ACM , NewYork , NY , USA , 18pages . https : / / doi . org / 10 . 1145 / 3490099 . 3511153 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland © 2022 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 9144 - 3 / 22 / 03 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3490099 . 3511153 1 INTRODUCTION The recent demand for and interest in synchronous online classes and alternative professional meeting opportunities has led to the growing reliability and ubiquity of video conferencing technology . As a result , people can now attend meetings both remotely and virtually while still doing many of the same tasks they would in a traditional in - person meeting . As an example , impromptu polling is an interaction activity where a host can gauge the opinions of attendees by collecting and interpreting their prompted responses so that they are better informed of the virtual meeting participants’ state of mind and so that attendees feel more engaged and empowered because their feedback is heard . However , conducting impromptu polling in virtual meetings poses certain challenges that are not present in traditional in - person interactions . That is , unlike in - person meeting environments where attendees are co - located such that hosts can more immediately gauge the collective opinions of their peers through visual cues ( e . g . , raising of hands , performing hand gestures , facial expressions ) and audio cues ( e . g . , verbal responses , group volume levels ) , the lack of co - located presence in remote environments makes such visual and audio cues more difficult to gauge through video conferencing technology [ 12 , 97 ] . Current online polling websites such as Poll Everywhere [ 26 ] and Kahoot ! [ 41 ] provide solutions that allow hosts to conduct structured polling in a more seamless and entertaining way , respectively , but such solutions are constrained to polls that are prepared in advance of the meetings ( e . g . , online quizzes , formal surveys ) . Furthermore , developers of popular video conferencing software such as Zoom [ 98 ] , Google Meet [ 29 ] , and Microsoft Teams [ 59 ] have been motivated by the need to bridge the gap between traditional in - person meetings and virtual remote meetings , introducing engaging visual communication markers ( e . g . , text messaging emoji , video filters ) to supplement users’ live - streamed body cues . However , such approaches still require that hosts manually view these digital visual cues . The cognitive load required for comprehending this feedback goes significantly up as the size of the meeting audience increases . The challenge of this situation grows when considering that A ) attendees may choose to turn off their microphones and / or cameras at any point during the meeting , and B ) all attendees may not be visible on the host’s screen at once , being spread across pages of small video windows . Worse still , existing systems cannot convey all the backchanneling behavior of nods , murmurs , and gestures that are vital to the 292 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . orchestration of spontaneous conversation and the building of rapport . The hurdles faced by hosts are multifaceted , but the problems involving assessing the state of the attendees can be addressed by leveraging intelligent user interfaces . Using artificial intelligence and machine learning , an intelligent user interface can assist hosts in prompting the attendees to provide responses to impromptu polling questions by enabling attendees to naturally respond to questions . As video conferencing interfaces already enable verbal communication via chat and audio interfaces , we target nonverbal communication as the best medium to expand the possible interactions in intelligent user interfaces . Specifically , we focus on hand gestures due to the individual’s ability to freely control their hands’ visibility in the camera view and because hand gestures are a natural form of communication both within and outside of video conferencing . The research goal for this research work is to better support host and attendee interactions in impromptu polling activities during virtual remote meetings with the following research questions : • R1 . What are the hand gesture sets representing impromptu polling responses that reaches high agreement ? • R2 . What type of visual cues do hosts prefer to gauge attendees’ engagement ? • R3 . How quickly and robustly can a gesture recognition interface detect specific hand poses across a group to facilitate virtual impromptu polling ? • R4 : How does a system designed to summarize polling results reduce the mental workload on the host ? In order to address these request questions , we conducted the following set of studies ( all of which received IRB approval ) : • Gesture Elicitation Study . This study addresses R1 by eliciting gestures from meeting attendees , in order to determine gestures for communicating in a virtual meeting setting ( Section 3 ) . We refer to this study as the gesture elicitation study throughout this paper . The outcomes of this study identified gestures for conveying visual cues to be studied in the subsequent study . • Visual Feedback Study . This study addresses R2 by discovering the best means of conveying attendees’ responses to hosts in a virtual meeting setting ( Section 4 ) . We refer to this study as the visual feedback study throughout this paper . The outcomes of this study and the first study identified preferred communicative gestures and visual feedback for designing interface features that leverage impromptu polling interactions in a virtual meeting platform . • System Evaluation Study . This study – consisting of a collection of three related smaller studies – addresses R3 and R4 by evaluating an interface that incorporates the gesture interactions and visual feedback from the first and second studies ( Section 6 ) . 2 RELATED WORK 2 . 1 The Usage of Gestures in Intelligent User Interfaces In the field of user intelligent interfaces , there have been three main streams of gesture interaction widely investigated : detecting non - verbal cues [ 47 , 53 , 54 , 82 , 91 ] , communicating sign languages [ 5 , 16 , 52 , 60 , 75 ] , and controlling interfaces as a natural input modality [ 48 , 50 , 70 , 81 , 86 ] . There has been a considerable amount of research conducted on subsequently recognizing these gestures ( whether or not they are defined by the researcher or elicited from participants ) . This is most commonly done using cameras ( e . g . , RGB video cameras , depth sensors ) [ 3 , 5 , 45 , 45 , 47 , 50 , 60 ] ; however , other researchers have used wearable sensors [ 14 , 32 , 40 , 52 ] and environmental sensors [ 51 , 90 ] . In addition to the variety in sensing modalities , researchers have also explored a wide range of input modalities , including facial expressions [ 36 , 57 , 83 ] , multi - modal gestures [ 34 , 35 ] , and body postures [ 23 , 80 , 92 ] . Studies on gesture interaction primarily use research - designed gestures or user - defined gestures . There has been a significant effort toward the latter , which is commonly referred to as gesture elicitation [ 10 , 42 , 64 , 68 , 74 , 86 , 89 ] . Gesture elicitation , first introduced in 2005 by Wobbrock et al . , is a human - centric method for designing gesture sets that involves observing the users’ behavior in response to the given referents , or functions [ 94 ] . This method determines what gestures users find most intuitive , enabling designers to create gesture sets that require minimal effort to learn and execute . Based on Villarreal - Narvaez et al . ’s work , these studies can be open or closed and use formal / informal and structured / unstructured gesture representations [ 89 ] . Open elicitation gives users full freedom to perform whatever gesture they find intuitive . Closed elicitation provides the participants with a pre - defined set of gestures and asks them to select the best match between their opinion and the given referent . Formal vs . informal refers to whether the gesture data is collected objectively ( e . g . , sensor - driven ) or subjectively ( e . g . , observer - driven ) , respectively . Structured representations define gestures using a model such as a human pose framework with keypoints while unstructured ones lack such a precise definition . Elicited gesture sets can be derived by a combination of agreement and dissimilarity measures to create a consensus gesture set for the referents . Agreement refers to how much participants agree with each other , i . e . , whether they perform the same gesture in response to the same referent [ 63 , 87 , 88 , 95 ] . Dissimilarity metrics measure how dissimilar two gestures are in order to determine how many unique gestures were performed across the participants . In the case of formal studies , this metric can be a function directly measuring the dissimilarity of the collected data . In the case of informal studies , this metric can be the taxonomic category the gesture belongs to and / or similarity judgements [ 2 ] . 2 . 2 Technology in Virtual Remote Meetings Studies that have looked at virtual remote meetings have focused on bridging the gap between physical and virtual meetings by repli - cating or augmenting the experience and atmosphere of in - person 293 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland meeting environments . Creating immersive virtual environments requires enabling natural forms of human communication . That said , despite the fact that gestures are ubiquitous in face - to - face communication and are the main paradigm by which we interact with many forms of technology , there have not been many studies that investigate their utility in virtual remote meetings . Perhaps the gesture that has received the most attention is raising one’s hand , with studies primarily utilizing computer vision techniques to detect the gesture in virtual classrooms [ 8 , 33 , 39 , 44 ] . As our work focuses specifically on enhancing the virtual classroom experience , we focus our literature review specifically on works targeting virtual classrooms . It is worth noting , however , that there is a large body of work that has looked at understanding and enhancing the physical classroom experience [ 1 , 9 , 46 , 71 , 73 ] . These approaches have largely focused on understanding and utilizing existing visual and auditory cues , whereas studies that have looked at virtual classrooms have focused more on bridging the gap between physical and virtual classrooms by replicating or augmenting the experience and atmosphere of in - person learning environments . Studies that investigate how to foster virtual classroom environments that are conducive to learning have explored a variety of techniques over the years . These techniques can be classified into three categories : automatic , semi - automatic , and manual [ 19 ] . Manual approaches include self - reporting of the students’ thoughts and feelings [ 22 , 65 ] and observational checklists that require an observer to watch and evaluate students’ behaviors [ 43 , 67 ] . Semi - automatic approaches use engagement tracking techniques such as measuring the time it takes the student to respond and the accuracy of the response to measure the student’s engagement [ 7 ] . Automatic approaches rely on sensors such as cameras to detect facial expressions , gestures , or posture [ 61 , 84 , 93 ] or physiological and neurological sensors ( e . g . , heart rate , EEG ) to identify the students’ affective state [ 27 ] . Facial expression recognition has been commonly used for tracking engagement and frustration in order to track factors that correlate with student learning [ 21 , 30 , 55 ] . Over the past year in particular there has been a significant interest in developing solutions for virtual classes as COVID - 19 sharply increased the number of classes that occurred online [ 4 , 13 , 15 , 20 , 66 , 97 ] . It’s worth noting that historically there has been less focus on developing practical systems for virtual classes . Some of the earliest works on the topic involved the implementation of an activity indicator system that can visualize classroom interaction dynamics such as speech activity , hand motion , and body motion to help instructors to read their classrooms and make deeper connections with students [ 11 , 12 ] . This thrust has continued with developments in recognizing attention via detecting posture , emotion , and drowsiness in virtual classes [ 72 ] . More recent works have focused on improving the sense of being in a physical classroom while residing in a virtual space . For example , UniVResity [ 69 ] allows students to connect to virtual reality based classroom with a VR head set , see their instructor’s movement as an avatar , and hear the instructor’s voice transmitted over the network . However , this progress can have the consequences of replicating stressful environments such as in [ 96 ] where students who “scanned” the classroom by rotating their heads reported greater anxiety about the virtual social partners in the room ( i . e . , they reported thinking more about what the others students thought of them ) . Erazo et al . built a touchless hand gesture system that allows students to participate in in - class activities remotely via mid - air gestures such as hand - n - hold , tap , swipe but a researcher - designed gesture set and specifically targeted a Data Structures course [ 24 , 25 ] . Grafsgaard et al . used automated tools to track posture and hand - to - face movements to explore posture and gesture as non - verbal cues in computer - mediated tutorial dialogue [ 31 ] . A few studies have also explored the use of virtual reality technologies that recognize instructor’s gestures to facilitate practice before a live class [ 6 , 56 , 72 ] . To our knowledge no study has explored the practicality and utility of using a gesture set to enhance virtual classroom experience . Furthermore , while the studies described above use or recognize predefined gestures we take an open gesture elicitation approach to learn the most intuitive gesture set for the virtual classroom . 3 GESTURE ELICITATION The aim of our gesture elicitation study was to determine what gestures people preferred for communicating in a virtual meeting setting . For this system to work smoothly , the interface’s supported gesture set needs to be expansive enough to cover answer options as well as be easy to learn and perform . To that end , we conducted a gesture elicitation study to discover what gestures people would naturally use to respond to various impromptu question types . Gesture elicitation is a human - centric method for designing gesture sets that involves observing the users’ behavior in response to the given referents , or functions [ 89 , 94 ] . These gestures are analyzed and organized to identify common answers and find the best candidates for the final gesture set ; this process is explained in more detail in Subsection 3 . 3 . To discover what gestures people find intuitive to answer impromptu questions and keep the study setting as close to a real - world scenario as possible , we conducted an open , informal gesture elicitation study . Additionally , we used an unstructured gesture representation , using natural language descriptions of the participants’ behavior and transcribing any think - aloud comments during the study . This design allowed maximum creativity in the answers and enables the creation of a consensus gesture set that best matches the attendees’ behavior in an actual virtual meeting . Our study focused on three question - answering referents : answering binary questions , answering multiple choice questions , and answering scaled questions . Binary questions are characterized as “true or false . ” Multiple choice questions are characterized as “A , B , C , or D” or “1 , 2 , 3 , or 4 . ” Scaled questions are characterized by representing a continuous scale of values , e . g . , from “nothing” to “all . ” 3 . 1 Participants We recruited our participants through convenience sampling at a large public university in North America through university - wide bulk mail and direct email contact . Our recruitment efforts yielded 20 participants—nine women—from varying backgrounds who consisted mostly of graduate students from a large public university in North America . The majority of the participants were between the ages of 25 – 34 years old . All but one participant were 294 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . right handed . Eleven people were from North America / Central America , seven were from Asia , one was from Europe , and one had mixed heritage from North America and the Middle East . All had extensive experience with virtual meeting platforms . 3 . 2 Process Each user study was conducted on Zoom so that the participant could see themselves in their camera views like they would in a remote class setting . During the study , participants were asked to sit near the center of their camera view and have their upper body visible . Meetings were recorded to keep a record of exactly how the participant performed the gestures , any think - aloud comments they shared , and how much time it took the participant to think of each gesture set . To start the study , the conductor primed the participant with the definition of a gesture and examples of common gestures to remind the participant that they had full freedom of motion within their screens and could interact with the dimensions of the screen . The participants were encouraged to try other gestures as they saw fit and not to feel limited to these examples . Next , the conductor described the scenario to establish the context where the elicited gestures would be used : “ You are a student in a class of over fifty students in a virtual class meeting . The instructor has asked an impromptu question to which you need to respond with a physical gesture . ” Participants were instructed that they would be asked to perform multiple sets of gestures and rank each set on its suitability of match and ease of execution on a 7 - point Likert scale from strongly disagree to strongly agree . The elicitation for each question type followed the same process . First , the conductor defines the referent and its options , e . g . , binary having an option for answering “true” and an option for answering “false” . Participants were instructed to provide a set of gestures that they felt was representative of each option . Second , participants were asked to provide three gesture sets for each of the three referents using only their bodies and the bounds of their screens . Participants were shown example images of the range of motion they could utilize when creating their gesture sets . Third , participants were asked to provide three gesture sets for each of the three referents still using their bodies and the bounds of their screens but now incorporating a visual cue that could be used in an augmented reality system . Participants were shown example images illustrating that visual cues could be static or dynamic and could be interacted with however the participant saw fit . The images shown at the beginning of this and the previous step can be found in Appendix A . After each gesture set , the participant was asked to score the appropriateness of their gesture set in terms of their match and ease on an inclusive scale of - 3 to 3 . At the end , the participant had provided three gesture sets for each referent twice for a total of six gesture sets for each referent across two conditions . In total , the participant had provided 3 × 2 × 3 = 18 gesture sets across all three referents . At the end of the study , the participant was asked to select their favorite gesture set for each referent . By the nature of the referents in this work , elicited gestures were kept as sets and were not further divided into atomic gestures . Some of the gestures required the context of the other options in the set , e . g . , selecting one of four possible directions to point . 3 . 3 Gesture Taxonomy The elicited gestures were quite varied due to participants having full range of freedom within their screens , freedom of movement of their upper body , head , arms , and hands , and the option to include AR elements that they could interact with . To further analyze the responses beyond finding exact matches ( e . g . , “thumbs - up / down” ) , we categorized each gesture using a taxonomy of form , nature , and binding following the gesture vocabulary definition approach in Kühnel et al . [ 49 ] based on Wobbrock et al . ’s methodology [ 95 ] . The dimension flow , i . e . , whether the response occurs during or after the action , from Wobbrock et al . [ 95 ] was not used in this taxonomy because it did not add significant information due to the nature of the referents of answering impromptu questions . The response always occurs after the gesture’s completion . We briefly define each of the categorizations below . • Form captures whether the gestures involve motion or not , i . e . , whether they involve static poses , dynamic poses , or both . Interactions like hand poses , holding or hovering a body part , and pointing are considered static . Interactions like swiping , tapping , and moving a body part or visual cue are considered dynamic . If an answered set contains static and dynamic gestures for different responses , then it has a form of “both . ” For example , one gesture set for the multiple choice referent involved nodding ( dynamic ) , holding up one’s left hand ( static ) , holding up one’s right hand ( static ) , and creating an “X” with one’s arms ( static ) . • Nature refers to the style of the gesture and includes the options of symbolic , physical , metaphorical , and abstract . Symbolic gestures represent human language and nonverbal communication gestures , e . g . , creating an “X” with the hands or making the OK hand gesture . These gestures generally would make sense to an outside viewer without the context of a computer interface . Physical gestures involve interacting with an object , e . g . , an AR button , or other interface element . Metaphorical gestures emulate computer - mediated communication ( CMC ) gestures and AR controls , e . g . , swiping and tapping . Participants generally would describe UI elements and interacting with their screens for metaphorical gestures . Abstract gestures do not satisfy the conditions of the other three categories . For example , a participant proposed holding one’s hand up with an open or closed palm for binary questions . • Binding defines the frame of reference for the gesture . For this domain , the bindings include screen , body - centered , and neutral . If a gesture uses the position of the gesture on the screen to determine its meaning ( e . g . , pointing at the corners of the screen ) , then it has a screen binding . If a gesture has to be performed in a specific area relative to the body to determine its meaning ( e . g . , performed on the left or right side of the body ) , then it has a body - centered binding . Otherwise , the gesture has a neutral binding , i . e . , it can be performed anywhere in the camera view and maintain the same meaning . To give an example of how a gesture set was classified using this taxonomy , consider a participant answering holding up one , two , three , or four fingers to answer each option for the multiple choice 295 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland referent . Finger counting is a common nonverbal hand gesture for communicating numbers , so this gesture set has a symbolic nature . The location of the hand in the screen or relative to the body does not impact the answer , so it has a neutral binding . Holding up a specific number of fingers is a static hand pose , so the set has a static form . Each elicited gesture was classified into this taxonomy by two researchers ( who are authors on this work ) to reduce subjective bias . These categorizations were based on identifying the logic and mapping of the gestures in conjunction with the participants’ think - aloud comments . After standardizing the names of the gestures to find matches , duplicate gestures from the same participant were reduced to a count of one as participants were not allowed to repeat answers . Similarly , if a participant treated the scaled question as a multiple choice question and insisted on their answers in the same fashion , then the associated gestures were not included . As given in Table 1 , the final number of gesture sets for each referent totaled to 116 for binary , 115 for multiple choice , and 89 for scaled . Based on the taxonomic definition and natural language description of each gesture set , unique gesture sets within each referent were identified and had their frequencies calculated . Binary and multiple choice had the most variety with 49 and 50 unique gesture sets , respectively , while scaled only had 38 unique sets . Agreement is a standard metric for determining the most intuitive gestures for a given referent [ 62 , 95 ] . The original definition for this metric had the context that a participant could only answer a single gesture per referent , but our study asks the participants to answer a total of m gestures per referent . As such , the maximum agreement score would be bounded by 1 m if all participants answered the same m gestures which does not match the intuition of the metric . To address this problem , we modified the gesture agreement calculation to normalize the equation to be bounded between 0 and 1 again ( Eq . 1 ) . P r is the set of gestures for referent r , P i is the subset of identical gestures for that referent , and n is the number of participants . For each unique gesture set within each referent , we also computed the consensus score [ 62 ] , the percentage of participants who answered the given gesture set , as another metric of intuitiveness . The agreement score , maximum gesture consensus , average time to think of the gesture set , average suitability of match , and average ease of execution are given in Table 1 . A r = | P r | n (cid:213) P i ⊆ P r (cid:18) | P i | | P r | (cid:19) 2 ( 1 ) Table 1 : Metrics Per Referent METRIC BINARY MULTIPLE CHOICE SCALED Total Gesture Sets 116 115 89 Unique Gesture Sets 49 50 38 Agreement 0 . 27 0 . 30 0 . 25 Max . Consensus 0 . 7 1 0 . 5 Avg . Time ( s ) 27 . 9 29 . 1 38 . 1 Avg . Match [ - 3 : 3 ] 2 . 34 2 . 01 2 . 25 Avg . Ease [ - 3 : 3 ] 2 . 43 1 . 86 1 . 85 To illustrate the most common gestures , we have included the gestures for each referent that over 20 % ( i . e . , at least 5 participants ) agreed on in Figures 1 and 3 . Unsurprisingly , the most common binary gesture sets involved two discrete gestures , two buttons , or dividing the screen in half and performing CMC gestures such as hovering ( static ) or swiping ( dynamic ) . Likewise , the most common multiple choice gesture sets involved communicating the prompt verbatim , e . g . , counting or using sign language , or selecting one of four options inherent to the screen , i . e . , the corners . Lastly , the most common scaled gesture sets involved communicating a relative distance to communicate concepts like “none , ” “less than half , ” “about half , ” “more than half , ” and “all . ” 3 . 4 Analysis Chi - squared analysis was used to determine the significance of the differences of the proposed gestures for each referent with respect to their taxonomic nature , taxonomic binding , whether the favorites had visual cues , the match scores , and the ease scores . The patterns for taxonomic nature or binding indicate whether participants conceptualized each question type in significantly different fashions . The other metrics capture how difficult the referents are to communicate via gesture . In short , this analysis determines the need for independent support in the final gesture set to be intuitive . The results of the Chi - squared analysis are given in Table 2 . Table 2 : Chi - Squared Analysis METRIC CHI - SQUARE DF p Nature 24 . 71 6 < 0 . 05 Binding 23 . 81 6 < 0 . 05 Visual Cue 6 . 94 2 < 0 . 05 Match 17 . 95 12 0 . 12 Ease 31 . 66 12 < 0 . 05 We discuss the combinations of referents and values that had the most impact on the overall sum for each significant category . All of the categories except for the match score had a significant score . These results indicate that participants conceptualized the answers to each of the referents significantly differently but found all of their answers intuitive . For example , binary had more symbolic , fewer abstract , and more neutral gestures . In other words , participants generally leveraged commonly used hand gestures to communicate the concepts of “positive” and “negative” and did not rely on the screen or visual cues to improve the clarity of their answer . In contrast , multiple choice had more screen and body - centered gestures . Many participants visualized quadrants , took advantage of the natural mapping of using the four corners or four sides of their screen , or directly communicated their selection based on the prompt with a symbolic gesture . Scaled had fewer symbolic , more physical , and more object gestures . Distances and angles were used most often to communicate continuous values for the scaled questions ; these questions also saw the most variety in the proposed visual cues to use an manipulable object to answer the question . Following these patterns , participants tended to prefer gestures without visual cues for binary questions and preferred visual cues for scaled questions . Many of the proposed gestures directly drew on common hand gestures used for nonverbal communication in daily life . 296 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . ( a ) ( b ) ( c ) ( d ) Figure 1 : Binary Gestures With More Than 20 % Consensus . In ( a ) participants raised and held up their right or left hands . In ( b ) participants selected one of two buttons on an augmented reality interface . In ( c ) participants created an O or an X with their arms . In ( d ) participants moved their hand either right - to - left or left - to - right . Thumbs Up / Down and Nodding Up - Down / Left - Right are not pictured . ( a ) ( b ) ( c ) ( d ) Figure 2 : Multiple Choice Gestures With More Than 20 % Consensus . In ( a ) participants held up one , two , three , or four fingers . In ( b ) participants pointed to the top - left , top - right , bottom - left , or bottom - right corners of the screen . In ( c ) participants used the sign language hand gestures for a , b , c , or d . In ( d ) participants raised and held up their hand in the top - left , top - right , bottom - left , or bottom - right corners of the screen . Many of the proposed gestures directly drew on common hand gestures used for nonverbal communication in daily life . Each cate - gory saw participants propose using finger counting , likely due to its natural mapping to selecting one of a number of options . Finger counting had complete agreement on multiple choice questions and appeared a significant number of times in the binary and scaled question types . As such , we conclude that finger counting is one of the most widely applicable of the elicited gestures across referents . 4 VISUAL FEEDBACK Having identified what gestures people prefer when communi - cating in virtual meetings , we conducted a visual feedback study to discover the best means of conveying attendee responses to the host . In other words , while the aim of the gesture elicitation study was to determine what gestures that people would prefer for communicating in a virtual meeting setting , the aim of this visual feedback study was to determine what visual feedback that hosts Figure 1 : Binary Gestures With More Than 20 % Consensus . In ( a ) participants raised and held up their right or left hands . In ( b ) participants selected one of two buttons on an augmented reality interface . In ( c ) participants created an O or an X with their arms . In ( d ) participants moved their hand either right - to - left or left - to - right . Thumbs Up / Down and Nodding Up - Down / Left - Right are not pictured . IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . ( a ) ( b ) ( c ) ( d ) Figure 1 : Binary Gestures With More Than 20 % Consensus . In ( a ) participants raised and held up their right or left hands . In ( b ) participants selected one of two buttons on an augmented reality interface . In ( c ) participants created an O or an X with their arms . In ( d ) participants moved their hand either right - to - left or left - to - right . Thumbs Up / Down and Nodding Up - Down / Left - Right are not pictured . ( a ) ( b ) ( c ) ( d ) Figure 2 : Multiple Choice Gestures With More Than 20 % Consensus . In ( a ) participants held up one , two , three , or four fingers . In ( b ) participants pointed to the top - left , top - right , bottom - left , or bottom - right corners of the screen . In ( c ) participants used the sign language hand gestures for a , b , c , or d . In ( d ) participants raised and held up their hand in the top - left , top - right , bottom - left , or bottom - right corners of the screen . Many of the proposed gestures directly drew on common hand gestures used for nonverbal communication in daily life . Each cate - gory saw participants propose using finger counting , likely due to its natural mapping to selecting one of a number of options . Finger counting had complete agreement on multiple choice questions and appeared a significant number of times in the binary and scaled question types . As such , we conclude that finger counting is one of the most widely applicable of the elicited gestures across referents . 4 VISUAL FEEDBACK Having identified what gestures people prefer when communi - cating in virtual meetings , we conducted a visual feedback study to discover the best means of conveying attendee responses to the host . In other words , while the aim of the gesture elicitation study was to determine what gestures that people would prefer for communicating in a virtual meeting setting , the aim of this visual feedback study was to determine what visual feedback that hosts Figure 2 : Multiple Choice Gestures With More Than 20 % Consensus . In ( a ) participants held up one , two , three , or four fingers . In ( b ) participants pointed to the top - left , top - right , bottom - left , or bottom - right corners of the screen . In ( c ) participants used the sign language hand gestures for a , b , c , or d . In ( d ) participants raised and held up their hand in the top - left , top - right , bottom - left , or bottom - right corners of the screen . Each category saw participants propose using finger counting , likely due to its natural mapping to selecting one of a number of options . Finger counting had complete agreement on multiple choice questions and appeared a significant number of times in the binary and scaled question types . As such , we conclude that finger counting is one of the most widely applicable of the elicited gestures across referents . 4 VISUAL FEEDBACK Having identified what gestures people prefer when communicating in virtual meetings , we conducted a visual feedback study to discover the best means of conveying attendee responses to the host . In other words , while the aim of the gesture elicitation study was to determine what gestures that people would prefer for communicating in a virtual meeting setting , the aim of this visual 297 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland ( a ) ( b ) ( c ) Figure 3 : Scaled Gestures With More Than 20 % Consensus . In ( a ) selected a percentage on a augmented reality scale . In ( b ) participants moved their hands apart with a virtual bar appearing between their hands to indicate the percentage reflected by the distance between their hands . In ( c ) participants moved their fingers along a virtual slide . feedback study was to determine what visual feedback that hosts would prefer for quickly interpreting the relative counts of those attendees’ performed gestures . 4 . 1 Participants We recruited our participants through convenience sampling similarly to the previous study ( Section 3 ) . Our recruitment efforts yielded 66 volunteer respondents from varying backgrounds , 36 of those—54 . 6 % in total—self - reported prior teaching experience ( e . g . , professor , lecturer , teaching assistant ) . In terms of experience with virtual meeting platforms , 100 % had used Zoom , 65 . 0 % had used Skype , 50 . 0 % had used Microsoft Teams , 47 . 0 % had used Google Meet , and 25 . 8 % had used WebEx . Lastly , 69 . 8 % were in STEM , 7 . 58 % were in liberal arts , 4 . 55 % were in business , and 3 . 03 % were in education ( participants self - reported their academic field ) . 4 . 2 Process The study setup for the online survey respondents focused on viewing and evaluating four different interface visual feedback types ( Figure 4 ) , which would potentially be displayed after an attendee performed an hand gesture response to a host’s impromptu polling question . The different interface visual feedback types and the conditions for displaying the feedback types were as follows : 4 . 2 . 1 Visual Feedback Types . The survey respondents viewed and directly evaluated four visual feedback types ( Figure 4a ) . • No feedback . Default camera view found in virtual meeting platforms [ 18 , 29 , 58 , 98 ] . • Color feedback . Simple semi - transparent augmented reality color filter and similarly supported in video chat interface features and extensions [ 28 , 78 ] . • Emoji feedback . Displayable Unicode - supported emoji [ 85 ] and similarly supported in virtual meeting platforms [ 29 , 98 ] and as chat message polls [ 77 ] . • Window feedback . Pop - up window with poll response counts and similarly supported in virtual meeting plat - forms [ 18 , 29 , 98 ] and audience response tools [ 17 , 26 , 41 ] . 4 . 2 . 2 Hand Gesture Responses . The visual feedback types were displayed after a hand gesture response is performed in the camera view ( Figure 4b ) . These responses consist of five hand poses that were selected as example catalysts to trigger the visual feedback . While the five emojis do not fully correspond to hand poses that communicate type of agreement and finger counts , we believe that the emojis were visually representative enough to convey such communications . • Thumbs up . “Yes” response for binary question , “option 1” for multiple - choice question . We chose this emoji since it represents a hand pose that internationally expresses agreement to represent the former usage , while the hand pose consists of a single extended finger to represent the first option for the latter usage . • Thumbs down . “No” response for binary question . We chose this emoji since it represents a hand pose that internationally expresses non - agreement . • V sign . “Option 2” response for multiple - choice question . We chose this emoji since the hand pose consists of two extended fingers to represent the second option . • OK sign . “Option 3” response for multiple - choice question . We chose this emoji since the hand pose consists of three extended fingers to represent the third option . • Hand wave . “Option 4” response for multiple - choice question . We chose this emoji since the hand pose is the closest emoji that visually looks like there are four extended fingers to represent the fourth option . 4 . 2 . 3 Meeting Size Configurations . The visual feedback types were performed in an abstract virtual meeting interface view of different meeting sizes ( Figure 4c ) . We utilized the different meeting configurations as two distinct grid layouts on a single page of visible camera views , in order to display the camera view windows of the different virtual meeting participants with larger and smaller sizes . • 3 × 3 meeting size . Shown with camera views of a meeting of 8 attendees and of the host in the top - left corner to represent a relatively smaller meeting size . • 5 × 5 meeting size . Shown with camera views of a meeting of 24 attendees and of the host in the top - left corner to represent a relatively larger meeting size . 4 . 2 . 4 Survey Content . The online survey first introduced respon - dents to the study’s purpose of viewing and evaluating different interface visual feedback types for a simulated virtual meeting meeting , along with details of the interface visual feedback types , hand gesture responses , and meeting size configurations . Afterwards , they were shown four videos in randomized order that demonstrate each of the visual feedback types . Each visual feedback type was shown for each meeting size configuration for two question types ( i . e . , binary and multiple - choice ) , yielding 298 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . ( a ) visual feedback types ( L → R ) : none , color , emoji , window . ( b ) hand gesture responses ( L → R ) : thumbs up , thumbs down , V sign , OK sign , open palm . ( c ) meeting sizes ( L → R ) : 3 × 3 attendees , 5 × 5 attendees Figure 4 : Study setup : ( a ) interface visual feedback , ( b ) hand gestures , ( c ) meeting sizes . 2 × 2 = 4 variations per type . The two meeting size configurations— 3 × 3 meeting size and 5 × 5 meeting size—were chosen to provide variation of presenting the different visual feedback types . The two question types—binary and multiple - choice—were chosen to minimize the number of conditions and simplify the study for the respondents . Lastly , our focus was on the visual feedback design and scaled questions would not significantly impact the results . The simulated camera views in the demonstration videos were of people performing the hand gesture responses from publicly - available stock footage on Shutterstock [ 76 ] , which were trimmed to 6 . 5 seconds for uniformity . The hand gesture responses for each variation were also proportionally shown for each question type ( e . g . , “yes” responses are gestured in four camera views for a 3 × 3 meeting size of eight attendees and one host ) for consistency among the variations . After viewing the demonstration videos together , respondents were shown questions for providing additional freeform responses and preference rankings on the four visual feedback types . 4 . 3 Analysis 4 . 3 . 1 Quantitative Results . As stated , we recruited 66 participants , 36 of whom self - reported as instructors of some kind . We analyzed the survey responses’ preference ranking counts—from first to fourth—for the visual feedback types ( Figure 5 ) , both with all 66 respondents ( Figure 5a ) and with the 36 instructor respondents only ( Figure 5b ) . Based on the raw numerical ranking counts on the four conditions for all and instructor - only responses , we saw that color had the most first preference selections , while none had the most last preference selections . We also similarly analyzed these responses on preference between any visual feedback , which summed together color , emoji , and window conditions ; and no feedback ( Figure 5c ) . For the raw numerical preference counts on either any or no visual feedback for both all and instructor - only responses , there were relatively fewer preference responses for no visual feedback compared to any . To determine if the conditions for the feedback statistically significantly improved the clarity of the responses , we applied the Friedman Test on the rankings of preference for the color , emoji , window , and no feedback conditions . When considering all responses regardless of teaching experience , the Friedman test ( n = 66 , k = 4 ) determined the feedback conditions to be impactful with a calculated F r statistic of 73 . 0 and a critical value of 7 . 82 with an alpha of 0 . 05 . When considering only the responses from participants with teaching experience , the significance held with the Friedman test ( n = 36 , k = 4 ) produced a F r statistic of 32 . 3 , still above the critical value . By extension of these results , the consensus ranking of the four conditions is not random . That is , color was most preferred , emoji and window were tied for second choice , and none was least preferred . 4 . 3 . 2 Qualitative Results . We performed a qualitative analysis of the survey’s freeform response question , where 51 of the 66 respondents provided remarks that were segmented into 76 initial open codes . The codes were then grouped into four conditions ( i . e . , the visual feedback types ) and an additional fifth condition ( i . e . , any visual feedback ) , along with their corresponding positive and negative response summaries ( Table 3 ) . Overall freeform responses summarized explained that no visual feedback led to lack of any indicator cue or clarity from the attendee responses , while visual feedback of some kind helped determine attendee responses and encourage engagement more easily . However , a few participants expressed concerns with visual feedback such as potentially being gimmicky and having a cluttered appearance . Additional remarks included alternatives ( e . g . , border visual feedback ) , additions ( e . g . , an “unsure” response ) , and 299 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland ( a ) All rankings . ( b ) Instructor rankings . ( c ) Preferences for any vs . none . Figure 5 : Total counts for the rankings and preferences of the four visual feedback types . The counts revealed color feedback as the first ranked visual feedback as supported by the Friedman Test . ( a - b ) Rankings count of the four visual feedback types for all respondents and instructor respondents , respectively . ( c ) Preferences count between any feedback versus no visual feedback . Table 3 : Summary of survey freeform responses for Study 3 . CONDITION POSITIVE NEGATIVE None - Lacks indicator , Not clear Color Easy to determine Not color blind - friendly , Lacks anonymity , Lacks contrast Emoji Comfortable to view , Understandable Too cluttered , Too childish Window More informed , Tracks directly Obfuscates students , Weak design Any Visual Easy to determine , Helps engagement Gestures gimmicky , Not anonymous , Too cluttered interest in incorporating multiple visual feedback types together for combining their respective benefits . 5 IMPLEMENTATION From the findings of our gesture elicitation study ( Section 3 ) for eliciting gestures from attendees , we discovered preferred gestures for communicating attendee responses in impromptu virtual polling . From the findings for our visual feedback study ( Section 4 , we discovered visual feedback of attendees’ performed gestures for more optimally informing hosts during impromptu virtual polling Based on the findings from these first two studies , we developed a system to facilitate impromptu virtual polling that leveraged finger counting and color feedback . The system further provides hosts with the ability to view the aggregated results in a simple bar graph . Below we describe the main components of our system . 5 . 1 Hand Gesture Recognition To recognize how many fingers an individual is holding up we created a Snap Camera lens [ 38 ] using Lens Studio [ 37 ] . Lens Studio is an integrated development environment / platform that facilitates the development of AR filters . We based ours lens on a template lens that is able to recognize how many fingers were being held up based on the ratio between the angles between the three joints of each fingers and the joint of the wrist . We modified the default thresholds for the detection of three and four fingers to improve recognition performance . We will share our code with interested researchers to enable the replication and expansion of our work . We further modified the lens such that the entire screen was tinted a certain color based on the number of fingers being held up ( shown in Figure 7 ) . This system hierarchy is shown in detail in Figure 6 . 5 . 2 Result Aggregation To show the aggregated results to a host , we wrote a Python script that allowed an individual to take snapshots of the virtual meeting interface and then view the aggregated results in a bar graph . As shown in Figure Figure 8 , the host can take snapshots of every page of users ( for meetings with a large number of attendees ) and then once they have taken snapshots of all users holding up fingers and triggering the filter , they can view the poll results . Our script aggregates attendee responses using contour finding to identify the various camera feeds and then finding the smallest Euclidean distance between the dominant color in a camera feed and the five colors using K - means clustering to detect which colored filter is being applied . Note that our script was written based on the Zoom virtual meeting interface . We will share our code with interested researchers on request . 6 SYSTEM EVALUATION In order to evaluate our system , we conducted three separate studies . The first two studies were quantitative studies for evaluating the system’s gesture recognition and color filter detection performance on the meeting attendee’s gestured responses to the impromptu polls , and the third was a qualitative study for evaluating the meeting host’s mental workload in interpreting the virtual meeting’s impromptu polling responses from the attendees . 300 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . Figure 6 : System Hierarchy . A custom filter was created in Lens Studio to recognize how many fingers an individual is holding up ( shown in the first image ) . This filter was uploaded to Snap Camera , allowing users to search for and load the filter in the Snap Camera app ( shown in the second image ) . In Zoom , users could then switch their camera to Snap Camera , and use the loaded filter in Zoom meetings ( shown in the third image ) [ 79 ] . ( a ) One Finger Triggered a Blue Filter ( b ) Two Fingers Triggered an Orange Filter ( c ) Three Fingers Triggered a Green Filter ( d ) Four Fingers Triggered a Pink Filter ( e ) Five Fingers Triggered a Yellow Filter Figure 7 : Finger Gestures and the Color Filters Used in this Study . We chose these colors to be accessible to color - blind individuals . 6 . 1 Study 1 : Attendees Gesture Recognition The first study evaluated our system’s performance in automatically recognizing attendees’ gestured responses to impromptu polling questions . This evaluation focused on the system’s functionality in terms of how many gestures were detected , classified correctly , and remained classified correctly while continuing to be performed . Participants were recruited through convenience sampling through a large public research university . This study consisted of two rounds of virtual meetings , and participants were randomly invited to attend one of two rounds of virtual meetings on the Zoom meeting software platform : 14 participants—4 women—for the first round , and 21 participants—8 women—in the second round ( note : the 21st participants who was not counted in the first study due to arriving late and having technical issues throughout the study ) . Participants ranged in age from 21 to 35 years , and all self - reported familiarity with using virtual meeting software such as Zoom . At the start of both rounds , a meeting host—a volunteer for the first round and a research study personnel for the second round— was present to introduce the study and ask a set of questions to all participants . All participants participated in the study with commercially - available webcams that are built into laptop devices or as separate peripherals ( i . e . , all cameras were color - based and lacked motion sensors ) . The questions were scripted in order to control the study , but hosts presented the questions to the participants as impromptu polling questions to receive spontaneous gesture responses . The 14 participants in the first round were given a total of 8 prompts . The first five prompts instructed the participants to sequentially answer with each option to test the system’s recognition of each of those five gestures . The last three prompts were questions where participants answered one of the five options . The 21 participants in the second round were asked a scaled - up total of sixteen questions : five prompts to answer the five gesture options individually , five multiple choice questions , three 5 point - scaled questions , and three questions with binary response 301 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Figure 8 : Host interface and sample results options ( i . e . , option 1 or option 2 ) . The system’s performance for this study was evaluated on four metrics for each asked question , where the metrics were averaged on all participants based on the round ( i . e . , first or second ) and question types ( i . e . , multiple choice / scaled or binary ) : • Initial . The ratio of all participants whose gesture was recognized on the initial attempt . • Synchronous . The maximum ratio of all participants who synchronously performed the correct gesture with others . • Completed . The ratio of all participant who successfully performed the gesture . • Stable . The ratio of all participants whose gesture did not change for more than 0 . 5 seconds while gesturing . The results of the study for both round can be found in Figure 9 and Table 4 . The first three metrics—Initial , Synchronous , Completed—scored at least 90 % for both rounds , with the second round performed better than 95 % . The fourth metric—stable— performed above 80 % for both rounds , with the second round performed better than 85 % . Overall , the system performed reasonably well such that attendees all were able to perform the gesture successfully during the response period and all performed their intended gestures synchronously for almost all questions . The recognition errors that we encountered in the study stemmed from two factors : 1 ) several participants were streaming in environments that were either brightly lit or very dim , and 2 ) the three finger - count gesture was not performed uniformly but consisted of two other variations not yet trained for the system . This evaluation demonstrated that the hand gesture recognition system was functioning adequately . In other words , the system’s performance from the attendees’ side of the interaction was acceptable . Next , we needed to evaluate the performance of the host’s result aggregation system to complete the system functionality evaluation . 6 . 2 Study 2 : Attendees Color Recognition We analyzed the data from the second round of the previous study to evaluate the color lens recognition accuracy . This evaluation focused on the system’s functionality with respect to correctly summarizing the visual cues produced by the hand gesture recognition system . The main metric of interest was how often the aggregation system would have at least one error , i . e . , all - or - nothing accuracy . From there , we wanted to identify the causes for the errors to address the issues in future iterations . In this study we asked 16 questions ( 5 prompts , 5 multiple choice , 3 scaled , and 3 binary ) . Our system correctly identified every color in 13 of these questions . In the remaining 3 questions our system incorrectly recognized the color of one user’s lens . These errors are explained in Figure 10 . With an all - or - nothing approach , the aggregation system had 81 . 3 % accuracy . However , it is worthwhile to note that only one user’s lens was incorrectly classified in each of the 3 questions with errors and that the errors were caused by technical problems in contrast to the aggregation system misidentifying a color . That said , these issues need to be considered in future efforts to improve the viability of the system . With the system functionality evaluation complete , the evaluation of the system usability from the host’s perspective remains . 6 . 3 Study 3 : Host Mental Workload Study To evaluate the utility of our system in facilitating impromptu virtual polling we conducted a study in which a host asked several impromptu polling questions . We employed a case - control study to determine whether the system makes it easier for hosts to conduct polls in virtual settings in terms of their mental workload . The control was an exam review session for a freshman level computer science course at a major university . The session consisted of over 100 students and two instructors . The instructors asked several impromptu questions over the course of the exam review and had students respond with their choices in chat . The case was a lab meeting consisting of 15 students , in which one of the students was asked to serve as the host . In this portion of the study , the host was provided with the system to elicit polling results from the attendee gesture responses and instructed on how the system works . The experimenters asked several questions such as “What is your favorite season ? ” and “Pick a random number from 1 – 5” to demo the system to the participants before the host took over the meeting . The impact of our system on the amount of effort required by the host to run their meeting was measured using the NASA TLX Task Load Index . This tool measures the workload on the individual as a ranking across the following dimensions : mental demand , physical demand , temporal demand , performance , effort , and frustration . Lower scores are better , i . e . , high scores indicate a high workload . Treating the hosts from the study as a representative for the host perspective in general , we performed a paired t - test on the difference in the average ranking for each dimension . The results were statistically significant with p < 0 . 05 with 5 degrees of freedom . The average NASA TLX rankings are given in Table 5 . 302 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . Table 4 : Gesture recognition accuracy numbers for both meetings in Study 1 . Round 1 Round 2 Multiple Choice / Scaled Binary Initial 0 . 9107 ± 0 . 0633 0 . 9563 ± 0 . 075 0 . 9538 ± 0 . 0803 0 . 9667 ± 0 . 0577 Synchronous 0 . 9184 ± 0 . 0643 1 ± 0 1 ± 0 1 ± 0 Completed 1 ± 0 . 027 1 ± 0 1 ± 0 1 ± 0 Stable 0 . 8163 ± 0 . 0382 0 . 8781 ± 0 . 0752 0 . 8808 ± 0 . 083 0 . 8667 ± 0 . 0289 ( a ) First meeting ( all questions ) . ( b ) Second meeting ( all questions ) . ( c ) Second meeting ( scaled ques - tions only ) . ( d ) Second meeting ( binary ques - tions only ) . Figure 9 : Gesture recognition accuracy for both meetings in Study 1 . ( a ) Chat Occlusion Error ( b ) Snap Camera Error Figure 10 : Our color detection algorithm encountered two types of errors . The first type of error occurred when the chat pop - up occluded part of a user’s camera feed as in the case of the user in the bottom - most row of ( a ) . This error occurred twice . The second type of error occurred when a user had an issue with Snap Camera and the user’s camera feed was replaced with the Snap Camera logo . This occurred once and can be seen in ( b ) . Table 5 : Average NASA TLX Task Load Index Rankings of Control and Case Conditions . Lower Is Better . NASA TLX METRIC CONTROL CASE Mental Demand 4 2 Physical Demand 4 2 Temporal Demand 4 . 5 4 Performance 4 3 Effort 4 3 Frustration 4 3 7 FURTHER DISCUSSION AND FUTURE WORK The outcomes from our proposed intelligent user interface approach—which leverages hand gesture interactions and color - driven visual feedback for improved impromptu polling activities in virtual meetings—motivates our work to address open design and technical challenges for improved experiences . The following subsections address specific limitations of our work , initial approaches we took to address those limitations , and the potential for future work designed specifically around expanding this work to overcome these limitations . 7 . 1 Broader Communicative Hand Gesture Sets From our initial studies , we leveraged finger counting hand gestures for their ease - of - use , prevalent cultural usage , and prior successes in other domains ( e . g . , virtual reality ) . With this gesture set , our system demonstrated that attendees were successfully able to intuitively provide immediate responses to hosts’ impromptu polling - type questions for binary and scaled questions . However , the pool of available participants was limited by the fact that our studies were conducted at an American university in education or education - adjacent scenarios and looked at three types of questions . Future work should look at a broader range of settings and a broader range 303 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland of question types to develop a larger set of communicative hand gestures that allow virtual meeting participants to more naturally engage in virtual meetings . 7 . 2 Enhanced Anonymity Capabilities The filter system used in our study required participants to have their camera on . However in many virtual meetings , a number of participants do not enable their webcams , choosing to attend anonymously due to reasons such as privacy and / or less - than - ideal background environments . Future versions of this work should take this into account and allow users to maintain their anonymity while still allowing them to engage in virtual meetings . 7 . 3 Wider Deployment in Virtual Classroom and Professional Meetings In addition to further settings and broader gesture set , a major next step for our work is to deploy our system more widely in virtual classrooms and professional meetings . This involves working more closely with instructors and organizers to understand barriers to adoption and the system’s effects on engagement . An additional limitation of our work is that the largest meeting in our work had 21 participants . Many virtual meetings ( e . g . , college classes , seminars , conference talks ) have significantly more attendees , a fact that video conferencing software clients typically handle by showing the camera feeds of attendees across multiple pages . Future work should look specifically at handling responses to impromptu polling questions in large meetings . 7 . 4 More Intelligent Automated Polling Count Our current approach for achieving automated polling count involves a simple but effective approach that takes a snapshot of the virtual meeting’s gallery view , which can be achieved similarly with multiple snapshots if the number of participants extend into multiple screens . However , we discovered the asynchronous nature of participants expressing gestures , which may lead to participant interactions or automated color cues not entirely occurring synchronously for polling to capture the most accurate polling count in a single snapshot . We would like to investigate other potential solutions that compensate for this situation such as exploring a sequence of snapshots that more effectively determine the intentions of participants’ responses during impromptu polling time . 7 . 5 Platform Variability By requiring users to download a specific Snap Camera lens for use on Zoom , we indirectly required participants to have a computer capable of running this software . This eliminated the possibility of users joining meetings from their mobile devices or from less powerful computers . Future work should look at expanding accessibility to these gestures by allowing for the use of a broader range of devices . 7 . 6 User Engagement One of the primary motivations behind this work is that virtual meetings often make it difficult to gauge participant engagement . In this work we presented a system designed to facilitate engagement via responding to impromptu polling questions . Thus , a key next step is understanding how our system affects engagement : both whether it affects how engaged participants are and if it provides hosts with a deeper understanding of attendee engagement levels . 8 CONCLUSION Utilizing impromptu questions during meetings is a valuable tool for hosts to better engage with attendees ( e . g . , especially as a pedagogical tool for instructors with students and for improving engagement with participants in professional meetings ) , and be better informed of the group’s current mindset without prior preparation . However , for in - person meetings shifting remotely to online and hosted on virtual meeting platforms , hosts have experienced challenges in utilizing impromptu questions effectively in these virtual environments . These challenges can range from limitations with existing virtual meeting platforms and audience response systems , whose built - in features for prepared polls do not seamlessly adapt for impromptu questions ; to the virtual medium itself , where remote interactions have led to outcomes such as decreased engagement and less optimal experiences in distance communications modalities . In this work , we propose that leveraging nonverbal cues as a way to engage with hosts in impromptu polling activities—such as performing physical gestures within attendees’ camera views—can alleviate open challenges in virtual meeting environments . • Gesture elicitation study . Our gesture elicitation study discovers what hand gestures participants prefer to perform in virtual meetings for different types of impromptu questions . • Interpreting hand gestures . Our visual feedback study involved asking respondents to view various types of visual feedback—which adapted existing interface feature designs— that triggered after hand gestures were performed , and provide their preferences for more effectively interpreting those responses as a collective . • Designing a gesture polling system . Our system evaluation study consisting of three studies focusing on attendee and host experiences determines several design considerations for a gesture polling system with respect to both the participant’s ability to reliably communicate their selection and the host’s ability to efficiently interpret the results . Our system demonstrates the feasibility of using gestures to facilitate clear and intuitive communication between meeting hosts and attendees by leveraging the power of intelligent user interfaces . REFERENCES [ 1 ] Karan Ahuja , Dohyun Kim , Franceska Xhakaj , Virag Varga , Anne Xie , Stanley Zhang , Jay Eric Townsend , Chris Harrison , Amy Ogan , and Yuvraj Agarwal . 2019 . EduSense : Practical Classroom Sensing at Scale . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . 3 , 3 , Article 71 ( Sept . 2019 ) , 26 pages . https : / / doi . org / 10 . 1145 / 3351229 [ 2 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . 2018 . Crowdsourcing Similarity Judgments for Agreement Analysis in End - User Elicitation Studies . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology ( Berlin , Germany ) ( UIST ’18 ) . Association for Computing Machinery , New York , NY , USA , 177— - 188 . https : / / doi . org / 10 . 1145 / 3242587 . 3242621 304 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . [ 3 ] Juan David Arango Paredes , Beatriz Muñoz , Wilfredo Agredo , Yoseth Ariza - Araújo , Jorge Luis Orozco , and Andres Navarro . 2015 . A reliability assessment software using Kinect to complement the clinical evaluation of Parkinson’s disease . In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society ( EMBC ) ( Milan , Italy ) ( EMBC ’15 ) . IEEE , New York , NY , USA , 6860 – 6863 . https : / / doi . org / 10 . 1109 / EMBC . 2015 . 7319969 [ 4 ] John Arquilla and Mark Guzdial . 2020 . Transitioning to Distance Learning and Virtual Conferencing . Commun . ACM 63 , 7 ( June 2020 ) , 10 – 11 . https : / / doi . org / 10 . 1145 / 3398386 [ 5 ] Kshitij Bantupalli and Ying Xie . 2018 . American Sign Language Recognition using Deep Learning and Computer Vision . In 2018 IEEE International Conference on Big Data ( Big Data ) ( Seattle , WA , USA ) ( BIG DATA ’02 ) . IEEE , New York , NY , USA , 4896 – 4899 . https : / / doi . org / 10 . 1109 / BigData . 2018 . 8622141 [ 6 ] Roghayeh Barmaki and Charles E . Hughes . 2018 . Embodiment analytics of practicing teachers in a virtual immersive environment . Journal of Computer Assisted Learning 34 , 4 ( 2018 ) , 387 – 396 . https : / / doi . org / 10 . 1111 / jcal . 12268 arXiv : https : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1111 / jcal . 12268 [ 7 ] Joseph E . Beck . 2005 . Engagement Tracing : Using Response Times to Model Student Disengagement . In Proceedings of the 2005 Conference on Artificial Intelligence in Education : Supporting Learning through Intelligent and Socially Informed Technology . IOS Press , NLD , 88 – 95 . [ 8 ] NyanBoBo , PeterVanHese , DimitriVanCauwelaert , PeterVeelaert , andWilfried Philips . 2011 . Detection of a hand - raising gesture by locating the arm . In 2011 IEEE International Conference on Robotics and Biomimetics . IEEE , New York , NY , USA , 976 – 980 . https : / / doi . org / 10 . 1109 / ROBIO . 2011 . 6181414 [ 9 ] Jane E . Caldwell . 2007 . Clickers in the Large Classroom : Current Research and Best - Practice Tips . CBE—Life Sciences Education 6 , 1 ( 2007 ) , 9 – 20 . https : / / doi . org / 10 . 1187 / cbe . 06 - 12 - 0205 PMID : 17339389 . [ 10 ] Jessica R . Cauchard , Jane L . E , Kevin Y . Zhai , and James A . Landay . 2015 . Drone & Me : An Exploration into Natural Human - Drone Interaction . In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing ( Osaka , Japan ) ( UbiComp ’15 ) . Association for Computing Machinery , New York , NY , USA , 361 – 365 . https : / / doi . org / 10 . 1145 / 2750858 . 2805823 [ 11 ] Milton Chen . 2002 . Achieving Effective Floor Control with a Low - Bandwidth Gesture - Sensitive Videoconferencing System . In Proceedings of the Tenth ACM International Conference on Multimedia ( Juan - les - Pins , France ) ( MULTIMEDIA ’02 ) . Association for Computing Machinery , New York , NY , USA , 476 – 483 . https : / / doi . org / 10 . 1145 / 641007 . 641109 [ 12 ] Milton Chen . 2003 . Visualizing the Pulse of a Classroom . In Proceedings of the Eleventh ACM International Conference on Multimedia ( Berkeley , CA , USA ) ( MULTIMEDIA ’03 ) . Association for Computing Machinery , New York , NY , USA , 555— - 561 . https : / / doi . org / 10 . 1145 / 957013 . 957130 [ 13 ] Xinyue Chen , Si Chen , Xu Wang , and Yun Huang . 2021 . “I Was Afraid , but Now I Enjoy Being a Streamer ! " : Understanding the Challenges and Prospects of Using Live Streaming for Online Education . Proc . ACM Hum . - Comput . Interact . 4 , CSCW3 , Article 237 ( Jan . 2021 ) , 32 pages . https : / / doi . org / 10 . 1145 / 3432936 [ 14 ] Yuqing Chen and Yang Xue . 2015 . A Deep Learning Approach to Human Activity RecognitionBasedonSingleAccelerometer . In 2015IEEEInternationalConference on Systems , Man , and Cybernetics ( Hong Kong , China ) . IEEE , New York , NY , USA , 1488 – 1492 . https : / / doi . org / 10 . 1109 / SMC . 2015 . 263 [ 15 ] Zhilong Chen , Hancheng Cao , Yuting Deng , Xuan Gao , Jinghua Piao , Fengli Xu , Yu Zhang , and Yong Li . 2021 . Learning from Home : A Mixed - Methods Analysis of Live Streaming Based Remote Education Experience in Chinese Colleges during the COVID - 19 Pandemic . Association for Computing Machinery , New York , NY , USA , Chapter 0 , 1 – 16 . https : / / doi . org / 10 . 1145 / 3411764 . 3445428 [ 16 ] Necati Cihan Camgöz , Oscar Koller , Simon Hadfield , and Richard Bowden . 2020 . Sign Language Transformers : Joint End - to - End Sign Language Recognition and Translation . In 2020 IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) ( Seattle , WA , USA ) ( CVPR ’20 ) . IEEE , New York , NY , USA , 10020 – 10030 . https : / / doi . org / 10 . 1109 / CVPR42600 . 2020 . 01004 [ 17 ] Cisco . 2021 . Slido . https : / / www . sli . do / ( accessed 2021 / 08 / 01 ) . [ 18 ] Cisco . 2021 . WebEx . https : / / www . webex . com / ( accessed 2021 / 08 / 01 ) . [ 19 ] M Ali Akber Dewan , Mahbub Murshed , and Fuhua Lin . 2019 . Engagement detection in online learning : a review . Smart Learning Environments 6 , 1 ( 2019 ) , 1 – 20 . [ 20 ] Nana Diana , Suhendra Suhendra , and Yohannes Yohannes . 2020 . Teachers’ Difficulties in Implementing Distance Learning during Covid - 19 Pandemic . In 2020 12th International Conference on Education Technology and Computers ( London , United Kingdom ) ( ICETC’20 ) . Association for Computing Machinery , New York , NY , USA , 105 – 109 . https : / / doi . org / 10 . 1145 / 3436756 . 3437029 [ 21 ] Abhilash Dubbaka and Anandha Gopalan . 2020 . Detecting Learner Engagement in MOOCs using Automatic Facial Expression Recognition . In 2020 IEEE Global Engineering Education Conference ( EDUCON ) . IEEE , New York , NY , USA , 447 – 456 . https : / / doi . org / 10 . 1109 / EDUCON45650 . 2020 . 9125149 [ 22 ] Sidney D’Mello , Blair Lehman , Reinhard Pekrun , and Art Graesser . 2014 . Confusion can be beneficial for learning . Learning and Instruction 29 ( 2014 ) , 153 – 170 . https : / / doi . org / 10 . 1016 / j . learninstruc . 2012 . 05 . 003 [ 23 ] Don Samitha Elvitigala , Denys J . C . Matthies , Chamod Weerasinghe , and Suranga Nanayakkara . 2021 . GymSoles + + : Combining Google Glass with Smart Insoles to Improve Body Posture When Performing Squats . In The 14th PErvasive Technologies Related to Assistive Environments Conference ( Corfu , Greece ) ( PETRA 2021 ) . Association for Computing Machinery , New York , NY , USA , 48 – 54 . https : / / doi . org / 10 . 1145 / 3453892 . 3453898 [ 24 ] OrlandoErazo , NelsonBaloian , JoséA . Pino , andSergioF . Ochoa . 2017 . Designing hand gesture interfaces for easing students participation from their spot . In 2017 IEEE 21st International Conference on Computer Supported Cooperative Work in Design ( CSCWD ) . IEEE , New York , NY , USA , 133 – 138 . https : / / doi . org / 10 . 1109 / CSCWD . 2017 . 8066683 [ 25 ] Orlando Erazo , Nelson Baloian , José A . Pino , and Gustavo Zurita . 2016 . Easing Students’ Participation in Class with Hand Gesture Interfaces . In Ubiquitous Computing and Ambient Intelligence , Carmelo R . García , Pino Caballero - Gil , Mike Burmester , and Alexis Quesada - Arencibia ( Eds . ) . Springer International Publishing , Cham , 393 – 399 . [ 26 ] Poll Everywhere . 2021 . Poll Everywhere . https : / / www . polleverywhere . com / ( accessed 2021 / 08 / 01 ) . [ 27 ] Benjamin S . Goldberg , Robert A . Sottilare , Keith W . Brawner , and Heather K . Holden . 2011 . Predicting Learner Engagement during Well - Defined and Ill - Defined Computer - Based Intercultural Interactions . In Affective Computing and Intelligent Interaction , Sidney D’Mello , Arthur Graesser , Björn Schuller , and Jean - Claude Martin ( Eds . ) . Springer Berlin Heidelberg , Berlin , Heidelberg , 538 – 547 . [ 28 ] Google . 2021 . Google Duo . https : / / duo . google . com / ( accessed 2021 / 08 / 01 ) . [ 29 ] Google . 2021 . Google Meet . https : / / meet . google . com / ( accessed 2021 / 08 / 01 ) . [ 30 ] Joseph F . Grafsgaard , Joseph B . Wiggins , Kristy Elizabeth Boyer , Eric N . Wiebe , and James C . Lester . 2013 . Automatically Recognizing Facial Indicators of Frustration : ALearning - centricAnalysis . In 2013HumaineAssociationConference on Affective Computing and Intelligent Interaction . Association for Computing Machinery , New York , NY , USA , 159 – 165 . https : / / doi . org / 10 . 1109 / ACII . 2013 . 33 [ 31 ] Joseph F . Grafsgaard , Joseph B . Wiggins , Kristy Elizabeth Boyer , Eric N . Wiebe , andJamesC . Lester . 2013 . EmbodiedAffectinTutorialDialogue : StudentGesture andPosture . In ArtificialIntelligenceinEducation , H . ChadLane , KalinaYacef , Jack Mostow , and Philip Pavlik ( Eds . ) . Springer Berlin Heidelberg , Berlin , Heidelberg , 1 – 10 . https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 39112 - 5 _ 1 [ 32 ] Saurabh Gupta . 2021 . Deep learning based human activity recognition ( HAR ) using wearable sensor data . International Journal of Information Management DataInsights 1 , 2 , Article100046 ( 2021 ) , 18pages . https : / / doi . org / 10 . 1016 / j . jjimei . 2021 . 100046 [ 33 ] BalajiHariharan , S . Padmini , andUmaGopalakrishnan . 2014 . Gesturerecognition using Kinect in a virtual classroom environment . In 2014 Fourth International Conference on Digital Information and Communication Technology and its Applications ( DICTAP ) . IEEE , New York , NY , USA , 118 – 124 . https : / / doi . org / 10 . 1109 / DICTAP . 2014 . 6821668 [ 34 ] Javaria Hassan , Jovin Leong , and Bertrand Schneider . 2021 . Multimodal Data Collection Made Easy : The EZ - MMLA Toolkit : A Data Collection Website That Provides Educators and Researchers with Easy Access to Multimodal Data Streams . . In LAK21 : 11th International Learning Analytics and Knowledge Conference ( Irvine , CA , USA ) ( LAK21 ) . Association for Computing Machinery , New York , NY , USA , 579 – 585 . https : / / doi . org / 10 . 1145 / 3448139 . 3448201 [ 35 ] Jinying He , Anouk van Maris , and Praminda Caleb - Solly . 2020 . Investigating the Effectiveness of Different Interaction Modalities for Spatial Human - Robot Interaction . In Companion of the 2020 ACM / IEEE International Conference on Human - Robot Interaction ( Cambridge , United Kingdom ) ( HRI ’20 ) . Association for Computing Machinery , New York , NY , USA , 239 – 241 . https : / / doi . org / 10 . 1145 / 3371382 . 3378273 [ 36 ] Su - Tzu Hsieh and Chin - Ta Chen . 2021 . Facial Recognition with Mask during Pandemic Period by Big Data Technical of GMM . In 2021 3rd International ConferenceonAdvancedInformationScienceandSystem ( AISS2021 ) ( Sanya , China ) ( AISS 2021 ) . Association for Computing Machinery , New York , NY , USA , Article 39 , 3 pages . https : / / doi . org / 10 . 1145 / 3503047 . 3503090 [ 37 ] Snap Inc . 2021 . Lens Studio . https : / / lensstudio . snapchat . com / ( accessed 2021 / 10 / 09 ) . [ 38 ] Snap Inc . 2021 . Snap Camera . https : / / snapcamera . snapchat . com / ( accessed 2021 / 10 / 09 ) . [ 39 ] J . Jesna , Athi S . Narayanan , and Kamal Bijlani . 2018 . Automatic Hand Raise Detection by Analyzing the Edge Structures . In Emerging Research in Computing , Information , Communication and Applications , N . R . Shetty , L . M . Patnaik , N . H . Prasad , and N . Nalini ( Eds . ) . Springer Singapore , Singapore , 171 – 180 . [ 40 ] Wenchao Jiang and Zhaozheng Yin . 2015 . Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks . In Proceedings of the 23rd ACM International Conference on Multimedia ( Brisbane , Australia ) ( MM ’15 ) . Association for Computing Machinery , New York , NY , USA , 1307 – 1310 . https : / / doi . org / 10 . 1145 / 2733373 . 2806333 [ 41 ] Kahoot ! 2021 . Kahoot ! https : / / kahoot . com / ( accessed 2021 / 08 / 01 ) . [ 42 ] Shaun K . Kane , Jacob O . Wobbrock , and Richard E . Ladner . 2011 . Usable Gestures for Blind People : Understanding Preference and Performance . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Vancouver , BC , Canada ) ( CHI ’11 ) . Association for Computing Machinery , New York , NY , USA , 305 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland 413 – 422 . https : / / doi . org / 10 . 1145 / 1978942 . 1979001 [ 43 ] Ashish Kapoor and Rosalind W . Picard . 2005 . Multimodal Affect Recognition in Learning Environments . In Proceedings of the 13th Annual ACM International Conference on Multimedia ( Hilton , Singapore ) ( MULTIMEDIA ’05 ) . Association for Computing Machinery , New York , NY , USA , 677 – 682 . https : / / doi . org / 10 . 1145 / 1101149 . 1101300 [ 44 ] Bill Kapralos , Andrew Hogue , and Hamed Sabri . 2007 . Recognition of Hand Raising Gestures for a Remote Learning Application . In Eighth International Workshop on Image Analysis for Multimedia Interactive Services ( WIAMIS ’07 ) . IEEE , Santorini , Greece , 38 – 38 . https : / / doi . org / 10 . 1109 / WIAMIS . 2007 . 72 [ 45 ] YugoKatsuki , YujiYamakawa , andMasatoshiIshikawa . 2015 . High - SpeedHuman / Robot Hand Interaction System . In Proceedings of the Tenth Annual ACM / IEEE InternationalConferenceonHuman - RobotInteractionExtendedAbstracts ( Portland , Oregon , USA ) ( HRI’15ExtendedAbstracts ) . AssociationforComputingMachinery , New York , NY , USA , 117 – 118 . https : / / doi . org / 10 . 1145 / 2701973 . 2701984 [ 46 ] Rehan Ahmed Khan , Komal Atta , Madiha Sajjad , and Masood Jawaid . 2021 . Twelve tips to enhance student engagement in synchronous online teaching and learning . Medical Teacher 0 , 0 ( 2021 ) , 1 – 6 . https : / / doi . org / 10 . 1080 / 0142159X . 2021 . 1912310 PMID : 33877950 . [ 47 ] Jung In Koh , Josh Cherian , Paul Taele , and Tracy Hammond . 2019 . Developing a Hand Gesture Recognition System for Mapping Symbolic Hand Gestures to Analogous Emojis in Computer - Mediated Communication . ACM Trans . Interact . Intell . Syst . 9 , 1 , Article 6 ( mar 2019 ) , 35 pages . https : / / doi . org / 10 . 1145 / 3297277 [ 48 ] Yuki Kubo , Yuto Koguchi , Buntarou Shizuki , Shin Takahashi , and Otmar Hilliges . 2019 . AudioTouch : Minimally Invasive Sensing of Micro - Gestures via Active Bio - Acoustic Sensing . In Proceedings of the 21st International Conference on Human - ComputerInteractionwithMobileDevicesandServices ( Taipei , Taiwan ) ( MobileHCI ’19 ) . Association for Computing Machinery , New York , NY , USA , Article 36 , 13 pages . https : / / doi . org / 10 . 1145 / 3338286 . 3340147 [ 49 ] Christine Kühnel , Tilo Westermann , Fabian Hemmert , Sven Kratz , Alexander Müller , and Sebastian Möller . 2011 . I’m home : Defining and evaluating a gesture set for smart - home control . International Journal of Human - Computer Studies 69 , 11 ( 2011 ) , 693 – 704 . https : / / doi . org / 10 . 1016 / j . ijhcs . 2011 . 04 . 005 [ 50 ] Arun Kulshreshth and Joseph J . LaViola . 2014 . Exploring the Usefulness of Finger - Based3DGestureMenuSelection . In ProceedingsoftheSIGCHIConference on Human Factors in Computing Systems ( Toronto , Ontario , Canada ) ( CHI ’14 ) . Association for Computing Machinery , New York , NY , USA , 1093 – 1102 . https : / / doi . org / 10 . 1145 / 2556288 . 2557122 [ 51 ] Xinyu Li , Yanyi Zhang , Ivan Marsic , Aleksandra Sarcevic , and Randall S . Burd . 2016 . Deep Learning for RFID - Based Activity Recognition . In Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD - ROM ( Stanford , CA , USA ) ( SenSys ’16 ) . Association for Computing Machinery , New York , NY , USA , 164 – 175 . https : / / doi . org / 10 . 1145 / 2994551 . 2994569 [ 52 ] Yun Li , Xiang Chen , Jianxun Tian , Xu Zhang , Kongqiao Wang , and Jihai Yang . 2010 . Automatic Recognition of Sign Language Subwords Based on Portable Accelerometer and EMG Sensors . In International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction ( Beijing , China ) ( ICMI - MLMI ’10 ) . Association for Computing Machinery , New York , NY , USA , Article 17 , 7 pages . https : / / doi . org / 10 . 1145 / 1891903 . 1891926 [ 53 ] Jingxian Liao and Hao - Chuan Wang . 2019 . Gestures as Intrinsic Creativity Support : Understanding the Usage and Function of Hand Gestures in Computer - Mediated Group Brainstorming . Proc . ACM Hum . - Comput . Interact . 3 , GROUP , Article 243 ( dec 2019 ) , 16 pages . https : / / doi . org / 10 . 1145 / 3361124 [ 54 ] Kuan - Yu Lin , Seraphina Yong , Shuo - Ping Wang , Chien - Tung Lai , and Hao - Chuan Wang . 2016 . HandVis : Visualized Gesture Support for Remote Cross - Lingual Communication . In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI EA ’16 ) . Association for Computing Machinery , New York , NY , USA , 1236 – 1242 . https : / / doi . org / 10 . 1145 / 2851581 . 2892431 [ 55 ] Gwen Littlewort , Jacob Whitehill , Tingfan Wu , Ian Fasel , Mark Frank , Javier Movellan , and Marian Bartlett . 2011 . The computer expression recognition toolbox ( CERT ) . In 2011 IEEE International Conference on Automatic Face Gesture Recognition ( FG ) . IEEE , New York , NY , USA , 298 – 305 . https : / / doi . org / 10 . 1109 / FG . 2011 . 5771414 [ 56 ] Jean - Luc Lugrin , Marc Erich Latoschik , Michael Habel , Daniel Roth , Christian Seufert , and Silke Grafe . 2016 . Breaking Bad Behaviors : A New Tool for Learning Classroom Management Using Virtual Reality . Frontiers in ICT 3 ( 2016 ) , 26 . https : / / doi . org / 10 . 3389 / fict . 2016 . 00026 [ 57 ] Chengwen Luo , Zhongru Yang , Xingyu Feng , Jin Zhang , Hong Jia , Jianqiang Li , Jiawei Wu , and Wen Hu . 2022 . RFaceID : Towards RFID - Based Facial Recognition . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . 5 , 4 , Article 170 ( dec 2022 ) , 21 pages . https : / / doi . org / 10 . 1145 / 3494985 [ 58 ] Microsoft . 2021 . Skype . = https : / / www . skype . com / ( accessed 2021 / 08 / 01 ) . [ 59 ] Microsoft . 2021 . Teams . = https : / / www . microsoft . com / en - us / microsoft - teams ( accessed 2021 / 08 / 01 ) . [ 60 ] Anshul Mittal , Pradeep Kumar , Partha Pratim Roy , Raman Balasubramanian , and Bidyut B . Chaudhuri . 2019 . A Modified LSTM Model for Continuous Sign Language Recognition Using Leap Motion . IEEE Sensors Journal 19 , 16 ( 2019 ) , 7056 – 7063 . https : / / doi . org / 10 . 1109 / JSEN . 2019 . 2909837 [ 61 ] Hamed Monkaresi , Nigel Bosch , Rafael A . Calvo , and Sidney K . D’Mello . 2017 . Automated Detection of Engagement Using Video - Based Estimation of Facial Expressions and Heart Rate . IEEE Transactions on Affective Computing 8 , 1 ( 2017 ) , 15 – 28 . https : / / doi . org / 10 . 1109 / TAFFC . 2016 . 2515084 [ 62 ] Meredith Ringel Morris . 2012 . Web on the Wall : Insights from a Multimodal Interaction Elicitation Study . In Proceedings of the 2012 ACM International ConferenceonInteractiveTabletopsandSurfaces ( Cambridge , Massachusetts , USA ) ( ITS ’12 ) . Association for Computing Machinery , New York , NY , USA , 95 – 104 . https : / / doi . org / 10 . 1145 / 2396636 . 2396651 [ 63 ] Meredith Ringel Morris , Andreea Danielescu , Steven Drucker , Danyel Fisher , Bongshin Lee , m . c . schraefel , and Jacob O . Wobbrock . 2014 . Reducing Legacy Bias in Gesture Elicitation Studies . Interactions 21 , 3 ( May 2014 ) , 40 – 45 . https : / / doi . org / 10 . 1145 / 2591689 [ 64 ] MiguelA . Nacenta , YemlihaKamber , YizhouQiang , andPerOlaKristensson . 2013 . Memorability of Pre - Designed and User - Defined Gesture Sets . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Paris , France ) ( CHI ’13 ) . Association for Computing Machinery , New York , NY , USA , 1099 – 1108 . https : / / doi . org / 10 . 1145 / 2470654 . 2466142 [ 65 ] Heather L . O’Brien and Elaine G . Toms . 2010 . The development and evaluation of a survey to measure user engagement . Journal of the American Society for Information Science and Technology 61 , 1 ( 2010 ) , 50 – 69 . https : / / doi . org / 10 . 1002 / asi . 21229 arXiv : https : / / asistdl . onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / asi . 21229 [ 66 ] Abiodun Ogunyemi , Merja Bauters , Jana Pejoska , and James Sunney Quaicoe . 2021 . Designing for Active Engagement in Online Learning Environments . In 3rd African Human - Computer Interaction Conference : Inclusiveness and Empowerment ( Maputo , Mozambique ) ( AfriCHI 2021 ) . Association for Computing Machinery , New York , NY , USA , 176 – 179 . https : / / doi . org / 10 . 1145 / 3448696 . 3448744 [ 67 ] Jim Parsons and Leah Taylor . 2012 . Student Engagement : What do we know and what should we do ? University of Alberta , Edmonton , Alberta , Canada . [ 68 ] Jorge - Luis Pérez - Medina , Santiago Villarreal , and Jean Vanderdonckt . 2020 . A Gesture Elicitation Study of Nose - Based Gestures . Sensors 20 , 24 , Article 7118 ( 2020 ) , 21 pages . https : / / doi . org / 10 . 3390 / s20247118 [ 69 ] Krzysztof Pietroszek and Chao Cheng Lin . 2019 . UniVResity : Face - to - Face Class Participation for Remote Students Using Virtual Reality . In 25th ACM Symposium on Virtual Reality Software and Technology ( Parramatta , NSW , Australia ) ( VRST ’19 ) . Association for Computing Machinery , New York , NY , USA , Article 97 , 2 pages . https : / / doi . org / 10 . 1145 / 3359996 . 3364730 [ 70 ] RichardPinsenschaumandFlaithriNeff . 2016 . EvaluatingGestureCharacteristics When Using a Bluetooth Handheld Music Controller . In Proceedings of the Audio Mostly 2016 ( Norrköping , Sweden ) ( AM ’16 ) . Association for Computing Machinery , New York , NY , USA , 209 – 214 . https : / / doi . org / 10 . 1145 / 2986416 . 2986443 [ 71 ] Andrew Quinn . 2010 . An Exploratory Study of Opinions on Clickers and Class Participation From Students of Human Behavior in the Social Environment . Journal of Human Behavior in the Social Environment 20 , 6 ( 2010 ) , 721 – 731 . https : / / doi . org / 10 . 1080 / 10911351003749102 [ 72 ] Abhishek Revadekar , Shreya Oak , Aumkar Gadekar , and Pramod Bide . 2020 . Gauging attention of students in an e - learning environment . In 2020 IEEE 4th Conference on Information Communication Technology ( CICT ) . IEEE , Chennai , India , 1 – 6 . https : / / doi . org / 10 . 1109 / CICT51604 . 2020 . 9312048 [ 73 ] Steven Robbins . 2011 . Beyond Clickers : Using ClassQue for Multidimensional Electronic Classroom Interaction . In Proceedings of the 42nd ACM Technical Symposium on Computer Science Education ( Dallas , TX , USA ) ( SIGCSE ’11 ) . Association for Computing Machinery , New York , NY , USA , 661 – 666 . https : / / doi . org / 10 . 1145 / 1953163 . 1953347 [ 74 ] Jaime Ruiz , Yang Li , and Edward Lank . 2011 . User - Defined Motion Gestures for Mobile Interaction . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Vancouver , BC , Canada ) ( CHI ’11 ) . Association for Computing Machinery , New York , NY , USA , 197 – 206 . https : / / doi . org / 10 . 1145 / 1978942 . 1978971 [ 75 ] Hirohiko Sagawa and Masaru Takeuchi . 2002 . A Teaching System of Japanese Sign Language Using Sign Language Recognition and Generation . In Proceedings of the Tenth ACM International Conference on Multimedia ( Juan - les - Pins , France ) ( MULTIMEDIA ’02 ) . Association for Computing Machinery , New York , NY , USA , 137 – 145 . https : / / doi . org / 10 . 1145 / 641007 . 641035 [ 76 ] Shutterstock . 2021 . Shutterstock . https : / / www . shutterstock . com / ( accessed 2021 / 08 / 01 ) . [ 77 ] Slack . 2021 . Create a Poll in Slack . https : / / slack . com / help / articles / 229002507 - Create - a - poll - in - Slack ( accessed 2021 / 08 / 21 ) . [ 78 ] Snap , Inc . 2021 . Snap Camera . https : / / snapcamera . snapchat . com / ( accessed 2021 / 08 / 01 ) . [ 79 ] Snapchat . 2020 . How to use Snap Camera . https : / / www . youtube . com / watch ? v = k2n3mG8YHDE [ 80 ] Gaoyue Sun . 2021 . Golf Swing Correction Based on Deep Learning Body Posture Recognition . In 2021 3rd International Conference on Pattern Recognition and Intelligent Systems ( Bangkok , Thailand ) ( PRIS 2021 ) . Association for Computing Machinery , New York , NY , USA , 72 – 76 . https : / / doi . org / 10 . 1145 / 3480651 . 3480713 306 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . [ 81 ] Wei Sun , Franklin Mingzhe Li , Congshu Huang , Zhenyu Lei , Benjamin Steeper , SongyunTao , FengTian , andChengZhang . 2021 . ThumbTrak : RecognizingMicro - Finger Poses Using a Ring with Proximity Sensing . Association for Computing Machinery , New York , NY , USA , 1 – 9 . https : / / doi . org / 10 . 1145 / 3447526 . 3472060 [ 82 ] Chiew Seng Sean Tan , Johannes Schöning , Kris Luyten , and Karin Coninx . 2013 . Informing Intelligent User Interfaces by Inferring Affective States from Body Postures in Ubiquitous Computing Environments . In Proceedings of the 2013 International Conference on Intelligent User Interfaces ( Santa Monica , California , USA ) ( IUI ’13 ) . Association for Computing Machinery , New York , NY , USA , 235 – 246 . https : / / doi . org / 10 . 1145 / 2449396 . 2449427 [ 83 ] Tian Xiang Tee and Hee Kooi Khoo . 2020 . Facial Recognition Using Enhanced Facial Features K - Nearest Neighbor ( k - NN ) for Attendance System . In Proceedingsofthe20202ndInternationalConferenceonInformationTechnologyandComputerCommunications ( Kuala Lumpur , Malaysia ) ( ITCC 2020 ) . Association for Computing Machinery , New York , NY , USA , 14 – 18 . https : / / doi . org / 10 . 1145 / 3417473 . 3417475 [ 84 ] Ghassem Tofighi , Haisong Gu , and Kaamraan Raahemifar . 2016 . Vision - based engagement detection in Virtual Reality . In 2016 Digital Media Industry Academic Forum ( DMIAF ) . IEEE , New York , NY , USA , 202 – 206 . https : / / doi . org / 10 . 1109 / DMIAF . 2016 . 7574933 [ 85 ] Unicode . 2021 . Emoji Frequency . https : / / home . unicode . org / emoji / emoji - frequency / ( accessed 2021 / 08 / 01 ) . [ 86 ] Radu - Daniel Vatavu . 2012 . User - Defined Gestures for Free - Hand TV Control . In Proceedings of the 10th European Conference on Interactive TV and Video ( Berlin , Germany ) ( EuroITV ’12 ) . Association for Computing Machinery , New York , NY , USA , 45 – 48 . https : / / doi . org / 10 . 1145 / 2325616 . 2325626 [ 87 ] Radu - Daniel Vatavu and Jacob O . Wobbrock . 2015 . Formalizing Agreement Analysis for Elicitation Studies : New Measures , Significance Test , and Toolkit . Association for Computing Machinery , New York , NY , USA , 1325 – 1334 . https : / / doi . org / 10 . 1145 / 2702123 . 2702223 [ 88 ] Radu - Daniel Vatavu and Jacob O . Wobbrock . 2016 . Between - Subjects Elicitation Studies : Formalization and Tool Support . Association for Computing Machinery , New York , NY , USA , 3390 – 3402 . https : / / doi . org / 10 . 1145 / 2858036 . 2858228 [ 89 ] Santiago Villarreal - Narvaez , Jean Vanderdonckt , Radu - Daniel Vatavu , and Jacob O . Wobbrock . 2020 . A Systematic Review of Gesture Elicitation Studies : What Can We Learn from 216 Studies ? Association for Computing Machinery , New York , NY , USA , 855 – 872 . https : / / doi . org / 10 . 1145 / 3357236 . 3395511 [ 90 ] Aiguo Wang , Guilin Chen , Cuijuan Shang , Miaofei Zhang , and Li Liu . 2016 . Human Activity Recognition in a Smart Home Environment with Stacked Denoising Autoencoders . In Web - Age Information Management , Shaoxu Song and Yongxin Tong ( Eds . ) . Springer International Publishing , Cham , 29 – 40 . [ 91 ] Hao - Chuan Wang and Chien - Tung Lai . 2014 . Kinect - Taped Communication : Using Motion Sensing to Study Gesture Use and Similarity in Face - to - Face and Computer - Mediated Brainstorming . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Toronto , Ontario , Canada ) ( CHI ’14 ) . Association for Computing Machinery , New York , NY , USA , 3205 – 3214 . https : / / doi . org / 10 . 1145 / 2556288 . 2557060 [ 92 ] JiaXin Wang and Hai Cheng . 2020 . Human Posture Recognition Based On Convolutional Neural Network . In Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering ( Xiamen , China ) ( EITCE 2020 ) . Association for Computing Machinery , New York , NY , USA , 475 – 480 . https : / / doi . org / 10 . 1145 / 3443467 . 3443801 [ 93 ] Jacob Whitehill , Zewelanji Serpell , Yi - Ching Lin , Aysha Foster , and Javier R . Movellan . 2014 . The Faces of Engagement : Automatic Recognition of Student Engagement from Facial Expressions . IEEE Transactions on Affective Computing 5 , 1 ( 2014 ) , 86 – 98 . https : / / doi . org / 10 . 1109 / TAFFC . 2014 . 2316163 [ 94 ] Jacob O Wobbrock , Htet Htet Aung , Brandon Rothrock , and Brad A Myers . 2005 . Maximizing the guessability of symbolic input . In CHI’05 extended abstracts on Human Factors in Computing Systems . Association for Computing Machinery , Portland , Oregon , USA , 1869 – 1872 . [ 95 ] Jacob O . Wobbrock , Meredith Ringel Morris , and Andrew D . Wilson . 2009 . User - Defined Gestures for Surface Computing . Association for Computing Machinery , New York , NY , USA , 1083 – 1092 . https : / / doi . org / 10 . 1145 / 1518701 . 1518866 [ 96 ] Andrea Stevenson Won , Brian Perone , Michelle Friend , and Jeremy N Bailenson . 2016 . Identifyinganxietythroughtrackedheadmovementsinavirtualclassroom . Cyberpsychology , Behavior , and Social Networking 19 , 6 ( 2016 ) , 380 – 387 . [ 97 ] Matin Yarmand , Jaemarie Solyst , Scott Klemmer , and Nadir Weibel . 2021 . “It Feels Like I Am Talking into a Void " : Understanding Interaction Gaps in Synchronous Online Classrooms . Association for Computing Machinery , New York , NY , USA , 1 – 9 . https : / / doi . org / 10 . 1145 / 3411764 . 3445240 [ 98 ] Zoom . 2021 . Zoom . https : / / zoom . us / ( accessed 2021 / 08 / 01 ) . 307 Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland A PRIMING IMAGES ( a ) Starting Position ( b ) Move the arm on the left side of the camera view ( c ) Move the arm on the right side of the camera view ( d ) Move both arms ( e ) Move hands close together ( f ) Move hands far apart ( g ) Movehand ( s ) tothetopofthecameraview ( h ) Move hand ( s ) to the bottom of the camera view ( i ) Extending a variable number of fingers ( j ) Form different kinds of poses with a single hand ( k ) Form different kinds of poses with both hands Figure 11 : Participants were shown these images to illustrate the freedom of movement they had when creating gestures . 308 IUI ’22 , March 22 – 25 , 2022 , Helsinki , Finland Koh , et al . ( a ) Example gesture with a static visual cue the user does not interact with ( b ) Example gesture with a static visual cue the user performs gesture over ( c ) Examplegesturewithadynamicvisualcue the user does not interact with ( d ) Example visual cue the user manipulates while performing the gesture Figure 12 : Participants were shown these images to illustrate how they could use visual cues in addition to physical gestures . 309