1 A Survey Of Regression Algorithms And Connections With Deep Learning Yunpeng Tai Abstract —Regression has attracted immense interest lately due to its effectiveness in tasks like predicting values . And Regression is of widespread use in multiple ﬁelds such as Economics , Finance , Business , Biology and so on . While considerable studies have proposed some impressive models , few of them have provided a whole picture regarding how and to what extent Regression has developed . With the aim of aiding beginners in understanding the relationships among different Regression algorithms , this paper characterizes a broad and thoughtful selection of recent regression algorithms , providing an organized and comprehensive overview of existing work and models utilized frequently . In this paper , the relationship between Regression and Deep Learning is also discussed and a conclusion can be drawn that Deep Learning can be more powerful as an combination with Regression models in the future . Index Terms —Regression , Survey , Comparison Between Algorithms , A Different Vision Of Ordinary Least Squares , Insight Of Regression Future . (cid:70) 1 I NTRODUCTION R EGRESSION is an approach of obtaining a relationship between input space and output space . The relationship is represented by a function f : X (cid:55)−→ Y and X is known as the independent variable , and Y as the dependent variable . It was originally put forward by Legendre [ 1 ] in 1805 , who applied least squares in Regression . And then Gauss [ 2 ] published a further development of the theory of least squares featuring ordinary least squares in 1821 . Regression belongs to supervised learning and Y is con - tinuous , i . e . Y ∈ R . Undoubtedly , Regression is powerful and has made tremendous impact in enormous ﬁelds . As such , an increasing number of research has made funda - mental improvement to Regression models over the past few decades . There has been such a surge of Regression models pro - posed recently , that researchers and beginners may ﬁnd it challenging to ﬁgure out what exactly every model means and the relationship between them . Thus , a survey of the existing Regression models is beneﬁcial both to beginners who just want to scratch the surface of Regression and researchers willing to have a systematic view of Regression models and gain insight from those smart models . The key component of Regression is ordinary least squares . It is capable of producing an unbiased linear model of minimum variance as long as six necessary assumptions is satisﬁed according to Gauss - Markov Theorem [ 2 ] . How - ever , if OLS is applied in speciﬁc areas , some of the assump - tions are likely to be violated so that OLS fails to play its part in predicting values . Hence , it is essential to grasp those assumptions and ﬁgure out possible solutions when one of them is broken . And those well - known concerns with OLS contributes to extensive models designed to ﬁx those violations such as Ridge [ 3 ] [ 4 ] , Lasso [ 5 ] , Elastic Net [ 6 ] and so on . • Yunpeng Tai is a freshman of Suzhou University of Science and Technol - ogy . E - mail : yunpengtai @ foxmail . com . What distinguishes this paper from others is the earlier part of this paper is OLS - centered and alternative solutions provided by different models are discussed at length when some assumptions are broken ( Figure 1 ) . This paper views the relationship between models on the whole and discusses the details of distinct models speciﬁcally and explicitly . And this paper also provides a walk - through of some uncommon Regression models . What’s more , a possible direction to which Regression is going to develop is covered . Fig . 1 : Problems With OLS And Possible Solutions Provided By Distinct Models . N is the size of input space . P is the number of features of every sample in input space . This paper is organized as follows . A brief introduction of the Regression task and convention in this paper is included in Section 2 . In Section 3 , OLS , its assumptions and possible solutions for violation are comprehensively explained . Section 4 is composed of a number of Regres - sion models which enable OLS’s potential to be stimulated although the real data challenges its assumptions and some unexpected situations happen . Generalized Linear Models and an uncommon Regression named as Step - Wise are explored in Section 5 . Section 6 sets out to provide a quick overview of the strong bond between Regression and Deep a r X i v : 2104 . 12647v1 [ c s . L G ] 26 A p r 2021 2 ( a ) X T = log ( x ) ( b ) X T = √ x ( c ) X T = exp ( x ) ( d ) X T = 1 x ( e ) X T = ( x − 3 ) 2 ( f ) X T = x 3 ( g ) X T = x 4 ( h ) X T = x 5 ( i ) X T = x + 1 ( j ) X T = 1 x 2 Fig . 2 : Visualize Transformed Results . Every ﬁgure corresponds to a function . Learning . In Section 7 , conclusions about Regression are drawn and possible combination of Regression and Deep Learning in the future is discussed . In a nutshell , this well - established paper is an overview of Regression models and the relationship between Regression and Deep Learning , and hopefully this paper does make sense . 2 T HE R EGRESSION T ASK Our data looks like { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x n , y n ) } , which y i ∈ R . We intend to train a model from our data set and implement it in unknown test sets . A standard for machine performs well is that low residuals ( distance from predicted values to labels ) . When it comes to regression task , it is common way to implement Linear Regression . y = ˆ y + (cid:15) ( 1 . 1 ) = θx + b + (cid:15) ( 1 . 2 ) θ is called slope ( gradient ) or coefﬁcient and b is called intercept . θ explains when x changes to what extent ˆ y is going to change . X = { x 1 , x 2 , . . . , x n } is known as input space and Y = { y 1 , y 2 , . . . , y n } as output space . x i is called a sample , and x ij means the j - th feature of the i - th sample . y is the label and ˆ y is the prediction . And (cid:15) is the error accompanied by every prediction and is also called the distance from ˆ y to y ( residual ) . In ordinary least squares , the model assumes that y is actually sampled from Gaussian Distribution and every sample is with noise . Thus , it can also called noise in statistics . But in this paper , I am going to use error for that . 3 O RDINARY L EAST S QUARES In Machine Learning , we always ﬁgure out the best model by minimizing our objective function , which is also known as cost function . OLS ( Ordinary Least Squares ) serves as an effective loss function as long as the model satisﬁes six necessary assumptions . Then it can choose an unbiased model of minimum variance by minimizing the function below . And J ( θ ) is convex . Thus , set partial derivative of θ zero and then we can get the best parameter θ ∗ . Note that only when X T X is full rank , equation ( 3 ) does make sense . Some books may multiply J ( θ ) by 1 / n , which is convenient for computation . Note that X and Y are matrices . J ( θ ) = n (cid:88) i ( y i − ( θx i + b ) ) 2 ( 2 ) θ ∗ = ( X T X ) − 1 X T Y ( 3 ) 3 . 1 Prior Assumptions In this section , six necessary assumptions is studied . And possible solutions for unsatisﬁed situations are also covered . • Linearity . In other words , only Straight Line models are permitted . If the relationship between X and y is a non - linear model , e . g . y = X 4 , the whole regres - sion model crashes . And the recipe for this situation is applying feature transformation . By doing so , the whole relationship between X and y is changed for good . Hence , we must also take the correlation between X and y into consideration . Correlation can be told by calculating R 2 ( Coefﬁcient Of Determi - nation ) , which stands for the ability to predict y by observing X . When R 2 = 1 , it means the loss of this predictor is 0 . If R 2 = 0 , it is equivalent to that the predictor is constant , which indicates X has nothing to do with y . As shown in Table 1 , in Column R 2 , the value in bracket stands for the original coefﬁcient . And the same goes for Column Linearity . Note that I use 10000 random points from y = 2 ( X − 3 ) 2 + 5 + N , which N stands for noise and it varies from 0 to 1 . X follows random and even distribution . y = 1 n n (cid:88) i = 1 y i ( 4 ) R 2 = 1 − n (cid:80) i = 1 ( y i − ˆ y i ) 2 n (cid:80) i = 1 ( y i − y ) 2 ( 5 ) As shown in Table 1 , R 2 changes when feature transformation is applied . And the purpose is to ﬁnd linear model with the best R 2 . Note that linear means y w . r . t transformed X . And if so , scatter plot of 3 ( a ) Errors ( b ) Norml Distribu - tion ( c ) Q - Q of ND Fig . 3 : In Figure ( a ) , errors are not symmetrical and Q - Q plot doesn’t look like a line , which indicates errors don’t follow Normal Distribution . Figure ( b ) and Figure ( c ) show when errors follow Normal Distribution , the hist plot should look like bell curve and Q - Q plot should be a line . TABLE 1 : Transformation Results About R 2 And Linearity X y Transformation R 2 ( - 1 . 602 ) Linearity ( Non ) 1 . 450 9 . 994 X T = log 10 ( x ) 0 . 764 Non 1 . 003 13 . 475 X T = √ x 0 . 693 Non 3 . 179 5 . 935 X T = exp ( x ) 0 . 281 Non 3 . 801 6 . 726 X T = 1 x 0 . 870 Non 1 . 294 11 . 704 X T = ( x − 3 ) 2 0 . 983 Linear 1 . 399 10 . 915 X T = x 3 0 . 344 Non 3 . 591 5 . 951 X T = x 4 0 . 253 Non 2 . 987 6 . 742 X T = x 5 0 . 188 Non 3 . 161 5 . 906 X T = x + 1 0 . 616 Non 2 . 065 7 . 647 X T = 1 x 2 0 . 904 Non original X and predicted y should ﬁt the original dis - tribution plot . This assumption is the most signiﬁcant for Linear Regression and it may explain why Ridge or Lasso also performs badly when the relationship is nonlinear . • Constant Error Variance [ 7 ] . It means errors are uni - formly distributed , which in statistics is called no Heteroscedasticity . When we apply our model , we can get a bunch of predicted values via observing X . Then we can calculate errors between true values and predicted values . And we can also calculate the variance of errors . If errors follow normal distribu - tion ( equation 5 ) , thus its variance is constant ( σ 2 ) . Also , its distribution is symmetrical . In turn , the distribution of errors should also be uniform and symmetrical . So we can use error plot to detect it ( Figure 3 ) . And Q - Q Plot can detect whether the errors follow normal distribution . The data I use can be downloaded here . I choose X2 house age as X and Y house price of unit area for y . Note that I remove points that X equals 0 . And I choose 200 points for study . f ( x ) = 1 σ √ 2 π e − 12 ( x − µσ ) 2 ( 6 ) f ( x ) = 1 √ 2 π e ( − x 2 2 ) ( 7 ) equation ( 6 ) represents standard normal distribution , µ = 0 , σ = 1 . As shown in Figure 3 , if errors follow normal distribution , the distribution should be uni - form just like Figure ( b ) and histogram should be like bell curve . We can also draw a safe conclusion that if ( a ) Before Log ( b ) After Log Fig . 4 : The Relationship Between Residuals And Log errors follow normal distribution , their distribution should ﬁt the line in Q - Q Plot . After ﬁguring out heteroscedasticity , we can come up with a question that how it inﬂuences our model and how to improve it . The most common way is to try feature transformation e . g . Log . As shown in Figure 4 , it can , to some degree , make our errors’ distribu - tion slightly more stable . It’s always an option to try , but not an effective method to handle the problem . What’s more , we can apply Box Cox Transforma - tion , which can make data more close to normal distribution . In statistics , if data follows normal dis - tribution and then the variance of noise ( error ) is a constant ( σ 2 ) . Thus , normality of data is likely to relieve heteroscedasticity . And data can be down - loaded here . I choose total sulfur dioxide for X and quality for y . In Figure 5 , it may relieve heteroscedas - ticity . Note you can’t always depend on it . Also , it can be worse ( Figure 6 ) . • Independent Errors ( no autocorrelation , AC for short ) . For instance , you want to predict the shares in stock market . But the errors are correlated while they should be i . i . d ( independent identically distributed ) . When a ﬁnancial crisis happens , the shares is going to be extremely unstable in next few months , which means errors are going to increase sharply . It can be detected by Durbin Watson Test ( Table 3 ) or drawing AC Plot . And if values of y axis in AC Plot vary from ( 0 , 1 ] , they mean Positive AC . If values equal 0 , they mean Non AC . Otherwise , they mean Negative AC . TABLE 2 : Durbin Watson Test V alue Relationship 2 . 0 no Ac 0 . 0 - 2 . 0 positive AC 2 . 0 - 4 . 0 negative AC The autocorrelation can affect errors’ standard de - viation while it’s unlikely to have an inﬂuence on model’s coefﬁcient and intercept [ 8 ] . There’re two common ways to ﬁx it . The ﬁrst is to add omitted variables . For example , you want to predict stock performaces by time . Undoubtedly , the model is of hight autocorrelation . We can , however , add S & P 500 . And hopefully , it may relieve autocor - relation . The second is to switch another function . You can transform your linear model into a squared model . 4 • ( a ) Density Distribu - tion ( b ) Original Residual ( c ) Final Residual Fig . 5 : Box Cox Transformation May Help • No Multicollinearity . If independent variables are related to each other , there’s Multicollinearity in data . We can use Variance Inﬂation Factor ( VIF ) to detect it ( R 2 is Coefﬁcient Of Determination ) . If value = 1 , it implies that there is no Multicollinearity among the predictors . If value > 5 , it implies there’s potential Multicollinearity . If value > 10 , it implies apparent Multicollinearity . softmax ( x ) = e x i (cid:80) n i = 1 e x i ( 8 ) Our goal of Regression model is to ﬁgure out the re - lationship between independent variable ( X ) and de - pendent variable ( y ) by ﬁnding a proper coefﬁcient . But when there’s Multicollinearity , the coefﬁcient is unable to interpret . We actually don’t know what exactly the relationship is . However , if we just want to make good predictions , it’s still effective [ 7 ] . And if the degree of Multicollinearity is moderate , you don’t have to care about it too much . We can remove highly correlated variables or increase sample size . • Normality Of Data [ 7 ] . Box - Cox is the efﬁcient trans - formation to make data more close to normal distri - bution . Normalization and some basic feature trans - formation may help . And also try increasing data size . • No Exogeneity . If X we choose iteself is of little inﬂuence on y , which means the real prediction is not based on X , then there’s exogeneity [ 7 ] . And the best solution is to make a good analysis about what on earth affects our predicted values and choose a suitable X . 4 A LTERNATIVE M ODELS In this section , OLS’s weaknesses are going to appear in Background . And each Background stands for a speciﬁc problem . 4 . 1 Ridge Regression • Background : high variance . OLS enables the predic - tor to perform good on training sets , however , this is an invitation to poor performance on testing sets , which is also known as overﬁtting . Generally speak - ing , the more complicated a model is , the poorer per - formance of the model on unknown sets . According to Occam’s rule , a model is more likely to do a good job on unknown sets if the model is simple . And the model is of high generalization ability . • Shrinkage . When OLS is applied in real life , the predictor’s coefﬁcients can be too large in absolute . The coefﬁcient of variable more correlated to y is large while the one of variable less correlated to y is also large , which is misleading to ﬁgure out the relationship between X and y [ 3 ] . This phenomenon can account for poor performance on unknown sets . Thus , Ridge is going to implement shrinkage on coefﬁcients and the extent of shrinkage counts on the degree of correlation . Typically , if a variable holds much predicting power , its coefﬁcient is more likely to be big [ 4 ] . • Nonorthogonal Solution [ 3 ] . Whether OLS can be directly computed just by derivative relies mainly on if X T X is orthogonal . If X T X is nonorthogonal , this means X T X is not reversible and the direct computation can’t work . I is a unit matrix . There’re only small positive quantity on the diagonal of kI , the diagonal looks like a ridge in comparison with zero’s distribution ( equation 9 ) . ( a ) Density Distribu - tion ( b ) Original Residual ( c ) Final Residual Fig . 6 : Box Cox Transformation May Suck • Biased Model . The potential assumptions of Ridge are that X T X is nonorthogonal and coefﬁcients need shrinkage . Hence , Ridge actually do a trade - off between bias and variance and it uses increased bias to obtain reduced variance . Because Ridge’s assumptions are correct in most cases , it is capable of producing a model with low variance . • L2 - Penalty . Equation 10 is Ridge’s loss function . The loss function means not only an accurate model is required , its coefﬁcients should be small . And λ / 2 | | θ i | | 2 is called L2 - norm . This kind of method is known as regularization in the sense that the model generated by regularization is of high generalization ability . Except direct computation , gradient descent is a common way to get the best parameter . Ridge always minuses the coefﬁcient vector ( equation 11 . 1 & 11 . 2 ) . θ ∗ = ( X T X + kI ) − 1 X T Y ( 9 ) J ( θ ) = 1 n n (cid:88) i ( y i − ˆ y i ) 2 + λ 2 | | θ i | | 2 ( 10 ) 5 ∂J ( θ ) ∂θ = ∂MSE ∂θ + λθ ( 11 . 1 ) =   ∂M ( θ ) ∂θ 1 ∂M ( θ ) ∂θ 2 . . . ∂M ( θ ) ∂θ n   + λ   θ 1 θ 2 . . . θ n   ( 11 . 2 ) 4 . 2 Lasso Regression • Background : Ridge’s poor performance on outliers . Compared to OLS , Ridge is quite powerful but shrinkage means it just cuts down on coefﬁcients’ ability of affecting the result . As such , each coefﬁ - cient still has inﬂuence on the result which further indicates Ridge still cares about every sample’s loss . However , when outliers appear in the data , Ridge fails to deal with them in that it is sensitive to outliers . • Feature Selection [ 5 ] . Unlike Ridge , Lasso imple - ments feature selection because only one coefﬁcient is saved and others are set zero , which is known as sparse solution . Therefore , only one sample has effect on the prediction in the sense that Lasso is insensi - tive to outliers and robust to small changes . Feature selection results in oscillation in optimization , which means Ridge is more stable than Lasso in gradient descent ( Figure 7 ) . What’s more , although Lasso has looked at all the data , only one sample does make sense . Thus , Lasso is also able to avoid overﬁtting . If performance of Lasso is excellent , then we can say which sample does work . So Lasso is an interpretable model compared to OLS and Ridge . Fig . 7 : Ridge is more stable than Lasso in optimization process ( 2016 . Deep Learning . MIT Press ) . • L1 - Penalty . Lasso minuses a constant in gradient descent ( equation 13 . 1 & 13 . 2 ) . Suppose we’re on the top of a mountain , what Lasso does is just move a bitter farther while Ridge just goes where seems more steap . Hence , Ridge is more faster than Lasso . So when values are quite large , Ridge should be a better choice than Lasso . But when values are small , Lasso should be a better choice . J ( θ ) = 1 n n (cid:88) i ( y i − ˆ y i ) 2 + λ | | θ i | | ( 12 ) ∂J ( θ ) ∂θ = ∂MSE ∂θ + λsign ( θ ) ( 13 . 1 ) =   ∂M ( θ ) ∂θ 1 ∂M ( θ ) ∂θ 2 . . . ∂M ( θ ) ∂θ n   + λ   sign ( θ 1 ) sign ( θ 2 ) . . . sign ( θ n )   ( 13 . 2 ) 4 . 3 Support Vector Regression • Margin Maximization . SVM ( Support Vector Ma - chine ) is originally invented for classiﬁcation prob - lems [ 9 ] . Unlike other algorithms , SVM not only needs to classify all the data correctly but also re - quires the distance of data to the Hyper plane to be the biggest , which is known as widest street . Our linear model is y = θ T X + b . Among all the data points , the distance of the closest positive point to the hyper plane pluses the same distance of closest negative point is Margin ( γ ) . Then SVM is turned into maximizing Margin . And the same for SVR . We can turn the maximizing 2 | | θ | | into minimizing | | θ | | 2 2 . Because | | θ | | is bigger than 0 , | | θ | | 2 is proportional to | | θ | | . Then the problem for SVR is like equation 15 . In equation 15 , C is a coefﬁcient for regularization and L ( x ) is an undeﬁned loss function . min θ , b 1 2 | | θ | | 2 + C n (cid:88) i = 1 L ( y i − ˆ y i ) ( 14 ) Fig . 8 : (cid:15) - insensitive loss . Smola and Sch ¨ o lkopf , 2002 • (cid:15) - insensitive loss . SVR can tolerate mistakes which are no more than (cid:15) , if those points predicted wrongly in dashed area [ f ( x ) − (cid:15) , f ( x ) + (cid:15) ] , then the losses of those points equal zero , which is called (cid:15) - insensitive loss ( Figure 8 ) . In equation 16 , z stands for loss . Ac - tually , it just makes a trade - off between errors and complexity of the model . Hence , SVR is unlikely to overﬁt the data . Thus , SVR can reduce the variance of OLS . l (cid:15) ( z ) = (cid:40) 0 , if | z | ≤ (cid:15) | z | − (cid:15) otherwise ( 15 ) 6 • Dual Problem . Slack variables can be introduced to optimization problem ( equation 18 ) . They means how many errors are allowed to make beyond (cid:15) . min θ , b 1 2 | | θ | | 2 + C n (cid:88) i = 1 L ( ξ i + ˆ ξ i ) ( 16 ) y i − ˆ y i ≤ (cid:15) + ξ i ( 17 ) ˆ y i − y i ≤ (cid:15) + ˆ ξ i ( 18 ) ξ i ≥ 0 , ˆ ξ i ≥ 0 ( 19 ) We can introduce Lagrange Multiplier µ i ≥ 0 , ˆ µ i ≥ 0 , α i ≥ 0 , ˆ α i ≥ 0 . L ( θ , b , α , ˆ α , ξ , ˆ ξ , µ , ˆ µ ) = 1 2 | | θ | | 2 + C n (cid:88) i = 1 ( ξ i + ˆ ξ i ) − n (cid:88) i = 1 µ i ξ i − n (cid:88) i = 1 ˆ µ i ˆ ξ i + n (cid:88) i = 1 α i ( y i − ˆ y i − (cid:15) − ξ i ) + n (cid:88) i = 1 ˆ α i ( ˆ y i − y i − (cid:15) − ˆ ξ i ) ( 20 ) And set partial derivative of w , b , ξ i and ˆ ξ i zero . w = n (cid:88) i = 1 ( ˆ α i − α i ) x i ( 21 ) 0 = n (cid:88) i = 1 ( ˆ α i − α i ) ( 22 ) C = α i + µ i ( 23 ) C = ˆ α i + ˆ µ i ( 24 ) Thus , we can get Dual Problem for SVR . max α , ˆ α n (cid:88) i = 1 ˆ y i ( ˆ α i − α i ) − (cid:15) ( ˆ α i + α i ) − 1 2 n (cid:88) i = 1 n (cid:88) j = 1 ( ˆ α i − α i ) ( ˆ α j − α j ) ( x i ) T x j s . t . n (cid:88) i = 1 ( ˆ α i − α i ) = 0 ( 25 ) • Kernel Trick . Typically , the model is nonlinear in 1D space . Thus , we use φ ( x ) to represent the trans - formed x . Some common kernel functions are listed in table 3 . w = n (cid:88) i = 1 ( ˆ α i − α i ) φ ( x ) ( 26 ) f ( x ) = n (cid:88) i = 1 ( ˆ α i − α i ) k ( x , x i ) + b ( 27 ) 4 . 4 Random Forest Regression • Tree Structure [ 12 ] . A walk - through of tree structure is going to be given in this section . In regression tree , there’re root node , internal nodes and leaf node . For instance , if we’re going to predict a man’s height . And we get men’s and women’s heights . Thus , the root node is man or not . If a point which we need to predict is ( 68 , 171 ) . It means the height of a man TABLE 3 : Common Kernel Functions Name Expression Parameters Linear Kernel k ( x i , x j ) = ( x i ) T x j d = 1 Polynomial Kernel k ( x i , x j ) = (cid:16) ( x i ) T x j (cid:17) d ≥ 1 Gaussian Kernel k ( x i , x j ) = exp (cid:18) −(cid:107) x i − x j (cid:107) 2 2 σ 2 (cid:19) σ > 0 Sigmoid Kernel k ( x i , x j ) = tanh (cid:16) β ( x i ) T x j + θ (cid:17) β > 0 , θ < 0 weighing 68kg is 171cm . If we use tree regression , then our internal node is > 60 ? And the second internal node is < 70 ? and so on . And at last , the leaf node is the result we predict . Hence , the leaf node is the predicted height . • Classiﬁcation And Regression Tree [ 13 ] . CART in - cludes feature selection , generating trees and prun - ing . CART assumes that decision tree is a Bi - nary tree . The decision tree is equivalent to di - viding features into two groups recursively . It di - vides the input space into limited units and pre - dict the distribution . Suppose our data D = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x n , y n ) } . Decision Tree splits input space into M units R 1 , R 2 , . . . , R M and at the end of every unit is the output value c m . And we can minimize squared errors of output values and true values . And it’s obvious that the best output value in every unit should be the average value . But the question is that how to split the input space . And we choose x j randomly and its output value s . And we split the input space into two space by their output values . R 1 ( j , s ) = { x | x j ≤ s } and R 2 ( j , s ) = { x | x j > s } x j is called splitting variable and s is splitting point . And we ﬁnd c 1 , c 2 in R 1 , R 2 via squared least errors . We also want c 1 + c 2 to be small enough . That’s how we ﬁnally get j , s . min n (cid:88) i = 1 ( y i − ˆ y i ) 2 ( 28 ) ˆ c m = average ( y i | x i ∈ R m ) ( 29 ) min j , s ( min c 1 (cid:88) x i ∈ R 1 ( y i − c 1 ) 2 + min c 2 (cid:88) x i ∈ R 2 ( y i − c 2 ) 2 ) ( 30 ) • Pruning [ 13 ] . Decision Tree implements recursive binary splitting to make more accurate predictions . And if the size of data and the input space is quite large . The structure of the tree is complicated . Sadly , a complex model easily gives birth to Overﬁtting . So it’s necessary to make our model more simple . Therefore , from the bottom of the tree , we cut down some child trees . Then we can get a tree sequence { T 0 , T 1 , . . . , T n } ( T 0 is the root node ) . Then we em - ploy cross - validation to choose the best child tree from it . At the same time , we also expect our model to perform well . Hence , we apply a loss function to measure the differences of performances in the pro - cess of pruning . In equation ( 32 ) , T is arbitrary child tree . C ( T ) means errors on the training data . | T | is the 7 number of leaf nodes in a child tree . α is a parameter , which decides the regularization term . If α is big , it means hard punishment and this results in a simple tree . If α is small , it means soft punishment and this leads to a more complicated tree respectively . C α ( T ) = C ( T ) + α | T | ( 31 ) • Ensemble Learning [ 14 ] . We use 1 / 3 of our data to evaluate our model , which is called out of bag data . And 2 / 3 of our data to be a new data set . Then we select subsets randomly from the new data set , which is known as Bagging . Every time we select one subset of the complete data set and then the subset is placed back . The number of points in different subsets is the same . We train different tree models for every different data . Finally we make an average of all trees’ variables as our ﬁnal model variable in order to cut down on the variance . Hence , it’s an accurate model . And it is able to maintain accuracy although most of data is missing in that the model only randomly select a subset to train . However , it may overﬁt data when there’re some outliers in data . 4 . 5 Boosted Regression Tree • Biased Feature Selection [ 15 ] . It’s almost the same as Random Forest . They both select the random subset and then a new tree is generated . In Random Forest , the probability of data is selected is the same , which is almost unbiased . But in Boosted Regression Tree , it is going to give weights to every data point . For instance , ﬁrst time we select a subset and we build a tree model for it . Before this subset is placed back , prediction errors are calculated for every point . If the error is high , this point is likely to be given large weights , which indicates its probability of being se - lected is higher than others . To summarize , Boosted Regression Tree focuses on the errors and is going to mix it . Whereas , if there’re many outliers in data , it just sucks . But it is robust to missing values just like Random Forest because they both select subsets to ﬁt . 4 . 6 Elastic Net Regression • Background : Multicollinearity . One of OLS’s assump - tions is that no multicollinearity . However , in multi - variate regression , X can be sometimes dependent . If this happens , OLS , Ridge and Lasso fail to play their part . • Encourage Group Effect [ 6 ] . In multivariate regres - sion , if some samples is correlated to each other , OLS is likely to take one sample , not caring which one is selected while strongly correlated samples are on the same boat in Elastic Net Regression . And it view them as a whole , which is called group effect . It does automatic variable selection and continuous shrinkage , and it select groups of correlated samples [ 6 ] , which is similar to clustering methods . However , this model doesn’t reduce the variance and extra bias increases . • Generation Of Lasso And Ridge . Elastic Net is a mid - dle ground between Ridge Regression and Lasso Re - gression . It mixes Lasso’s loss function with Ridge’s . It has the parameter r to control the mix ratio . If r = 0 , it’s Ridge . If r = 1 , it’s Lasso . • N (cid:29) P . N is the number of samples and P is the number of features of every sample . When in multivariate regression , Elastic Net Regression can play a key role in N (cid:29) P cases . J ( θ ) = 1 n n (cid:88) i ( y i − ˆ y i ) 2 + rλ | | θ i | | + 1 − r 2 λ | | θ i | | 2 ( 32 ) 4 . 7 Least Angle Regression • Joint Least Squares Direction [ 16 ] . To begin with , the model selects the coefﬁcient β j and calculate errors . When some other sample x k has more correlation with errors than x j has . Hence , the model increases ( β j , β k ) in their joint least squares direction until x m has more correlation with errors . Thus , the model increases ( β j , β k , β m ) in their joint least squares di - rection . The model comes to its end until all samples in the model . Therefore , it can also settle samples’ autocorrelation in high dimension . OLS is the special case of LARS . When LARS doesn’t increase in joint least squares direction , the model becoms OLS . By the way , LARS is also powerful when N (cid:29) P just like Elastic Net Regression . • Sensitive To Outliers . LARS is a method which itera - tively reﬁt the errors . When there’re outliers in data , then LARS doesn’t make sense . • Easily Modiﬁed . It’s simple for LARS to join with other models such as Lasso . LARS - Lasso employs the Lasso’s loss function and applies the LARS’s method of coefﬁcient selection . 4 . 8 RANSAC Regression • Background . Although Lasso is insensitive to a few outliers , Lasso doesn’t work when data is ﬁlled with a large number of outliers , let alone OLS . • Random Sample Consensus Set [ 17 ] . RANSAC is an iterative method . And it has a error threshold (cid:15) . To begin with , it select a subset of the whole data and ﬁnd a model to ﬁt it . Then , use the model to test the rest of the data . If point’ loss on the model is no more than (cid:15) , then add it to the consensus set , which is full of inliers . And this process is iterative . When iterative times is reached , the process comes to its end . RANSAC is going to make the number of points in consensus set as large as possible . • Voting Scheme . RANSAC is kind of like voting pro - cess . A subset of data claims its idea and then the rest of the data votes for the idea . In every independent process , there are two kinds of data . One agrees with the idea while the other is against it . And RANSAC is going to select a process where the number of supporters is max . Thus , RANSAC is biased . If the 8 Algorithm 1 : Random Sample Consensus iteration times n i = 0 while i < n do Randomly Select Inliers From Data Find A Model M To Fit Test Other data Via The Model if points ﬁt M then Inliers Set ← points else Ouliers Set ← points model is going to be robust to outliers , then there must be enough good features which vote for correct model and outliers can’t vote consistently . • Disadvantages . When there’re few outliers in the data , RANSAC can’t make sense in that the differ - ence between every process is little . Only when the data is heavily contaminated , RANSAC can play its part . Besides , the threshold must be set by hand , which requires users to decide speciﬁc threshold on different data . Last but not least , the cost of computa - tion is high because it is an iterative method and the number of times required in the model is unknown . 4 . 9 Theil - Sen Regression • Median Method [ 18 ] . There’re many data pairs used to calculate coefﬁcient . θ = y a − y b / x a − x b And θ is the median of all θs . b = y − θx b is also the median of bs . Hence , it’s a nonparametric technique . However , complete computation leads to low speed . • Breaking Point . In particular , Theil - Sen only can tol - erate 29 . 3 % of data is outliers . And when the model is applied in high - dimensional regression , the rate is going to decrease . 4 . 10 Huber Regression • Huber Loss [ 19 ] . It transforms its loss function when faced with different values . When values are large , which is of high possibility of being outliers , Huber turns their loss function into Linear Loss in order to minimize their inﬂuence on the model . δ serves as a threshold , deciding how large data is to need a linear loss . And it is fastest in three robust regression . HuberLoss = (cid:40) 12 a 2 , if | a | ≤ δ δ ( | a | − 12 δ ) otherwise ( 33 ) 4 . 11 Multivariate Adaptive Regression Splines • Partitioning [ 20 ] . MARS begins with partitioning data and then runs linear regression on each different partition . And it makes no assumptions about the relationship between the labels and samples . MARS originally has a large collection of basis functions . Each meeting point of two linear models is called a knot . And each knot has a pair of basis functions . And these functions are used to describe the rela - tionship between x and y . The ﬁrst basis function is max ( 0 , x − y ) . The second is max ( 0 , y − x ) . • Remove Basis Functions [ 20 ] . After MARS parti - tions data and builds models , it applies least - squares model to ﬁt data . And each knot has two basis functions . The results of them can be viewed as input variables . Least - Squares model estimate the loss of each basis function’s output value . If a basis function has little inﬂuence on model ﬁtting , then it is going to be removed . • Advantages And Disadvantages . It can ﬁt a large number of predictor variables . And it is an effective and fast algorithm . Also , it is robust to outliers . However , it begins with a large set of models and this easily leads to overﬁtting . And it is vulnerable to missing data problems . 4 . 12 Polynomial Regression • Background : When the relationship between X and y is nonlinear , OLS sucks . • Polynomial Transformation [ 21 ] . Polynomial Regres - sion replaces original X with Polynomial in order to attain a more linear relationship than before or change features for some reason . Hence , it is not interpretable . Interestingly , it is somewhat like Talyor Extend . When your model breaks the assumption of linearity , then you can try all polynomial regression to ﬁnd a best one , which is a good recipe . If feature’s dimension is 2 , • order = 1 ⇒ [ 1 , X 1 , X 2 ] • order = 2 ⇒ [ 1 , X 1 , X 2 , X 21 , X 1 X 2 , X 22 ] • order = 3 ⇒ [ 1 , X 1 , X 2 , X 21 , X 1 X 2 , X 22 , X 31 , X 21 X 2 , X 1 X 22 , X 32 ] 4 . 13 Weighted Least Squares • Background . One of the OLS’s assumptions is con - stant error variance . In section 3 , I put forward log method . However , it is ineffective . • Transformed Weights [ 22 ] . When w i all equals 1 , it should be the OLS . In OLS , the model gives every point the same attention . But it’s under homoskedas - ticity while we come across more heteroskedastic scenarios . The idea is that we gives more attention to those points of which error is small . Thus , the model gives those points bigger weights . • Stable Intercept and Sensitive Coefﬁcient [ 23 ] . Note I choose different values for the ﬁrst 20 weights and others are always 1 . Different weights are equivalent for errors’ abnormal distribution . As shown in Table 4 , the intercept is right regardless . However , the coefﬁcient changes sharply . Hence , we can’t use it to draw inferences and test our hypotheses with regard to coefﬁcient . WLS = n (cid:88) i = 1 w i ( y i − ˆ y i ) 2 ( 34 ) 9 ( a ) w = 1 ( b ) w = 10 ( c ) w = 20 Fig . 9 : The middle line shows OLS and MLS ﬁts the data . And others show the range of predicted values of different algorithms TABLE 4 : Slopes And Intercepts Weights Slopes Intercepts 1 . 0 1 . 444 0 . 059 10 . 0 1 . 4887 0 . 059 20 . 0 1 . 5146 0 . 058 40 . 0 1 . 5407 0 . 058 80 . 0 1 . 5615 0 . 057 160 . 0 1 . 5755 0 . 057 As shown in Figure 9 , Figure ( a ) shows that OLS is the special case of WLS when weights = 1 . And Fig - ure ( b ) and Figure ( c ) show when weights change , the intercept almost stays the same . 4 . 14 Generalized Least Squares • Background . OLS assumes that the error must be independent in the sense that one error can’t be correlated to others . But when error autocorrelation happens , OLS has no way but to fail . • Better OLS [ 24 ] . GLS is similar to OLS on a linearly transformed version of the data . And GLS is un - baised , consistent and effective . WLS is the special case of GLS , which means GLS can also solve het - eroskedasticity [ 25 ] . 4 . 15 Feasible Generalized Least Squares • Implementable GLS . While GLS sounds powerful , but it can’t be applied in speciﬁc regression tasks . FGLS is an implementable version of GLS . And FGLS needs some crucial assumptions to ensure a consistent estimator for errors covariance matrix . • Inefﬁciency On Little Data . Whereas GLS is more powerful than OLS under heteroscedasticity or au - tocorrelation , this is not the case for FGLS . When the size of data is quite small , FGLS is ineffctive than OLS . Thus , some people prefer FGLS over OLS under small data . But when the size of data becomes large , FGLS is a better choice . 4 . 16 Bayesian Regression • Background . As is known to all , OLS exactly makes an estimation of the mean of the values , which fails to provide a whole picture of the relationship between independent variables and dependent variables . And in some cases , we want to obtain a possible distribu - tion of labels instead of a mean value . • Bayesian Theorem [ 27 ] . For instance , we’re going to employ a model to distinguish whether a email is normal or spam . So what our model faces is that it has to make predictions about the unknown email . Our data has 100 emails and 10 % of them is spam . Hence , the percentage of spam is 10 % . But that’s absolutely not the whole story . In Bayesian , it’s called Prior Probability , which means the Basic Assumption of the distribution and that’s where Bayesian Begins . At the beginning of the algorithm , Bayesian is biased in return the model is easily affected by the original distribution . For example , if we only have all 10 normal emails , it’s impossible that we wouldn’t get any spam emails in future . In other words , if the size of our data is quite small , it’s not incentive for us to implement Bayesian . However , when training times keep increasing , we should get ideal results ultimately . In the equation below , P ( B ) is a Normal - ization term and P ( A ) is Prior Probability . P ( A | B ) is called Posterior Probability ( Conditional Probability ) . To conclude , when we have much data , Bayesian may be a good choice to try while it exactly performs like other algorithms . P ( A | B ) = P ( B | A ) P ( A ) P ( B ) ( 35 ) • Maximum Likelihood Estimation [ 28 ] . Generally speaking , our goal is to ﬁgure out the real data distribution , which is almost impossible . There - fore , we want a data distribution which is close to our data distribution from a problem domain . MLE ( Maximum Likelihood Estimation ) indicates that we want to maximize the probability that real data is sampled from the Hypothesis Distribution . β ∗ = arg max β P β ( D ) ( 36 ) • Maximum Posterior Estimation [ 29 ] . Typically , we can use MAP ( Maximum A Posterior Estimation ) to replace MLE . It’s based on Bayesian Theorem . And MAP is fundamental to Bayesian Regres - sion ( equation 37 ) . Rather than other standard algo - rithms , Bayesian Regression doesn’t produce a single value but a range of possible distribution . And in most cases , MLE and MAP are likely to get the same results . However , when the hypothesis of MAP is different from MLE , they fail to reach the same destination . When Prior Probability is uniformly dis - tributed , they can make it . From another point of view , if we have some precise understanding of data , Bayesian Regression is a excellent choice in that it serves as Prior Probablity or we can weigh every different choice just like Weighted Least Errors . Interestingly , prior can be kind of Regularization or bias of the model , as such prior can be interpreted as L2 norm , which is also called Bayesian Ridge Regression . Equation ( 38 ) means given a model m , the probability of output y . And β ( Coefﬁcients ) and σ ( Standard Deviation ) are arbitrary values . 10 P ( β | D ) = P ( D | β ) P ( β ) P ( D ) ( 37 ) P ( y | m ) = P ( β , σ | m ) P ( y | X , β , σ , m ) P ( β , σ | y , X , m ) ( 38 ) y ∼ ( β T X , σ 2 ) ( 39 ) 4 . 17 Quantile Regression • Transformed Loss Function [ 30 ] . QR has a parameter q which decides the proportion to split the data . One is q % of the data and the other is ( 1 - q ) % of the data . For instance , if q = 0 . 5 , then data is split in two . We minimize the squared loss in OLS while we now minimize the absolute loss in QR . J ( θ ) = n (cid:88) i = 1 q | y i − ˆ y i | + n (cid:88) i = 1 ( 1 − q ) | y i − ˆ y i | ( 40 ) • Advantages . It goes without saying that QR can provide a more complete view of the relationship than OLS . What’s more , it is also robust to outliers and situations where the variance of errors is not a constant . 4 . 18 Ordinal Regression • Background . In some cases , the values for labels are ranking numbers . For instance , 0 - 5 can represent his ability of communicating with others in social sci - ence . And OLS can’t make accurate prediction about them . • Ranking Learning [ 31 ] . In OR , the model has a set of thresholds θ 1 , θ 2 , . . . , θ n , which is used to split pre - dictions into independent intervals and every inter - val corresponds to a y . The model can be represented by sigmoid function of which output values stand for possibility . P ( y ≤ i | x ) = σ ( θ i − ˆ y i ) ( 41 ) 5 E XTRA M ODELS 5 . 1 Generalized Linear Models • Generalized Functions [ 32 ] . Just as the name implies , it is generalization of different functions . And it consists of two signiﬁcant parts . The ﬁrst part is the probability distribution of y such as normal distri - bution ( OLS ) . The second is linear predictor , which decides how the coefﬁcients combine with indepen - dent variables . And GLM includes several regression models such as Binomial Regression , Bernoulli Re - gression , Poisson Regression and so on . Their appli - cation is not so wide and they just swift the two parts compared to OLS . Hence , it’s left out in this paper . • Advantages And Disadvantages . It can absolutely deal with situations where y doesn’t follow normal distribution . However , it needs large data sets and it is sensitive to outliers . 5 . 2 Step - Wise Regression • Forward Selection [ 33 ] . This method begins with no variables . And it involves testing the addition of the variable in an iterative method which is of great use to the improvement of the accuracy . The model repeats until no improvement . • Backward Elimination . This method starts with many candidate variables . It involves testing the loss of the model with the deletion of variables . If the loss is small , then the variable is going to be deleted . The model repeats until no variable can be deleted . • Bidirectional Elimination . This is an combination of the above two methods . Whether adding or deleting a variable is decided on every step . To summarize , Step - Wise Regression contains a big space of possible models , which can lead to overﬁtting . • Reasons For Stopping [ 34 ] . First , the tests such as F - tests and t - tests are biased , thus it may not be accurate . Second , widespread incorrect usage and availability of alternative models such as ensemble learning have led to calls to stop the use of this algorithm . 6 R ELATIONSHIP W ITH D EEP L EARNING 6 . 1 General Regression Neural Network Fig . 10 : GRNN Structure . Source : https : / / www . mdpi . com / 1424 - 8220 / 20 / 9 / 2625 . • Network Structure [ 35 ] . GRNN includes input , pat - tern , summation and output layers . The input and output layers are independent vector and dependent vector . The pattern layer can be seen as a vector full of coefﬁcients . For instance , if we want to apply y = θx . Then one pattern neuron stands for θ i x i . And output layer can be formulated as the equation below . And this model can maintain its accuracy with small data and it’s robust to outliers . However , the structure of the network is complicated so that it is computationally expensive . 11 Y ( x ) = (cid:80) ni = 1 y i K ( x , x i ) (cid:80) ni = 1 K ( x , x i ) ( 42 ) K ( x , x i ) = e − d i / 2 σ 2 ( 43 ) d i = ( x − x i ) T ( x − x i ) ( 44 ) • Widespread Application [ 36 ] . Many regression mod - els like Poisson Regression and Ordinal Regression have succeeded in using GRNN . And we can draw a safe conclusion that a complicated network structure can represent any kind of regression . But only few of them are proved successful . Neural Network is powerful and classic regression algorithms are well - structured . Maybe regression can be applied in neu - ral network without missing its original function . Humans have made fundamental progress in Re - gression . If we can combine Regression with neural network perfectly , then it’s another picture . 7 C ONCLUSIONS In this paper , I set out necessary assumptions with OLS and little tricks to ﬁx the problems when assumptions are violated . Amazingly , it seems that OLS is the beginning of almost evey regression model . And a large number of Regression models are designed to be a better OLS . They can play their part in situations where OLS fails to work . I hold the belief that not evey algorithm needs to be introduced in details . Hence , the widespread algorithms are given enough attention and others are quickly illustrated . Finally , I give a quick overview of GRNN . From this paper , I can draw three conclusions . • Know Your Model . Note that regression algorithms aren’t plug - and - play . You must know evey model’s range of application and are able to deal with situa - tions where the model’s assumptions are unsatisﬁed . • Regression In the Future . Regression is older com - pared to Deep Learning and great ideas behind every classic algorithm is never out of date . And people always want to predict unknown values and regres - sion task is really fascinating . Deep Learning is quite powerful . If regression can learn from Deep Learning and keeps its excellent part , I do believe regression can be more powerful in the near future . • It seems that regression algorithms are out of date . However , as far as I am concerned , beautiful ideas behind every algorithm are shared . In other words , dipping into these old algorithms can enable us to gain insight and intuition about algorithms and put forward exciting algorithms which share the same ideas with regression algorithms and are just dif - ferent implementations of awesome ideas to handle new problems . R EFERENCES [ 1 ] A . M . Legendre . Nouvelles m´ethodes pour la d´etermination des orbites des com ` etes , Firmin Didot , Paris , 1805 . “Sur la M´ethode des moindres quarr´es” appears as an appendix . [ 2 ] C . F . Gauss . Theoria combinationis observationum erroribus min - imis obnoxiae [ 3 ] Arthur E . Hoerl & Robert W . Kennard ( 1970 ) Ridge Regression : Biased Estimation for Nonorthogonal Problems , Technometrics , 12 : 1 , 55 - 67 , DOI : 10 . 1080 / 00401706 . 1970 . 10488634 [ 4 ] Arthur E . Hoerl & Robert W . Kennard ( 1970 ) Ridge Regression : Applications to Nonorthogonal Problems , Technometrics , 12 : 1 , 69 - 82 , DOI : 10 . 1080 / 00401706 . 1970 . 10488635 [ 5 ] Tibshirani , R . ( 1996 ) , Regression Shrinkage and Selection Via the Lasso . Journal of the Royal Statistical Society : Series B ( Methodological ) , 58 : 267 - 288 . https : / / doi . org / 10 . 1111 / j . 2517 - 6161 . 1996 . tb02080 . x [ 6 ] Zou , H . and Hastie , T . ( 2005 ) , Regularization and variable selection via the elastic net . Journal of the Royal Statisti - cal Society : Series B ( Statistical Methodology ) , 67 : 301 - 320 . https : / / doi . org / 10 . 1111 / j . 1467 - 9868 . 2005 . 00503 . x [ 7 ] Hayashi , Fumio ( 2000 ) . Econometics . Princeton University Press . p . 15 . [ 8 ] Gubner , John A . ( 2006 ) . Probability and Random Processes for Electrical and Computer Engineers . Cambridge University Press . ISBN 978 - 0 - 521 - 86470 - 1 . [ 9 ] V . Vapnik , “The support vector method of function estimation , ” in J . A . K . Suykens and J . Vandewalle ( Eds ) Nonlinear Modeling : Advanced Black - Box Techniques , Kluwer Academic Publishers , Boston , pp . 55 – 85 , 1998 . [ 10 ] Drucker , Harris ; Burges , Christ . C . ; Kaufman , Linda ; Smola , Alexander J . ; and Vapnik , Vladimir N . ( 1997 ) ; ”Support Vector Re - gression Machines” , in Advances in Neural Information Processing Systems 9 , NIPS 1996 , 155 – 161 , MIT Press . [ 11 ] Smola , A . J . , Sch¨olkopf , B . A tutorial on support vec - tor regression . Statistics and Computing 14 , 199 – 222 ( 2004 ) . https : / / doi . org / 10 . 1023 / B : STCO . 0000035301 . 49549 . 88 [ 12 ] S . R . Safavian and D . Landgrebe , ”A survey of decision tree classiﬁer methodology , ” in IEEE Transactions on Systems , Man , and Cybernetics , vol . 21 , no . 3 , pp . 660 - 674 , May - June 1991 , doi : 10 . 1109 / 21 . 97458 . [ 13 ] Loh , W . - Y . ( 2011 ) , Classiﬁcation and regression trees . WIREs Data Mining Knowl Discov , 1 : 14 - 23 . https : / / doi . org / 10 . 1002 / widm . 8 [ 14 ] Liaw A , Wiener M . Classiﬁcation and regression by randomFor - est [ J ] . R news , 2002 , 2 ( 3 ) : 18 - 22 . [ 15 ] Elith J , Leathwick J R , Hastie T . A working guide to boosted regression trees [ J ] . Journal of Animal Ecology , 2008 , 77 ( 4 ) : 802 - 813 . [ 16 ] Efron , Bradley ; Hastie , Trevor ; Johnstone , Iain ; Tibshirani , Robert ( 2004 ) . ”Least Angle Regression” ( PDF ) . Annals of Statistics . 32 ( 2 ) : pp . 407 – 499 . arXiv : math / 0406456 . doi : 10 . 1214 / 009053604000000067 . MR 2060166 . [ 17 ] Fischler , M . & Bolles , R . ( 1981 ) . Random Sample Consensus : A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography . Communications of the ACM , 24 , 381 - 395 . [ 18 ] Theil , H . ( 1950 ) , ”A rank - invariant method of linear and polyno - mial regression analysis . I , II , III” , Nederl . Akad . Wetensch . , Proc . , 53 : 386 – 392 , 521 – 525 , 1397 – 1412 , MR 0036489 [ 19 ] Huber , Peter J . ( 1964 ) . ”Robust Estimation of a Lo - cation Parameter” . Annals of Statistics . 53 ( 1 ) : 73 – 101 . doi : 10 . 1214 / aoms / 1177703732 . JSTOR 2238020 . [ 20 ] Friedman J H . Multivariate adaptive regression splines [ J ] . The annals of statistics , 1991 : 1 - 67 . [ 21 ] Stigler , Stephen M . ( November 1974 ) . ”Gergonne’s 1815 paper on the design and analysis of polynomial regression experi - ments” . Historia Mathematica . 1 ( 4 ) : 431 – 439 . doi : 10 . 1016 / 0315 - 0860 ( 74 ) 90033 - 0 . [ 22 ] Ruppert D , Wand M P . Multivariate locally weighted least squares regression [ J ] . The annals of statistics , 1994 : 1346 - 1370 . [ 23 ] Suykens J A K , De Brabanter J , Lukas L , et al . Weighted least squares support vector machines : robustness and sparse approxi - mation [ J ] . Neurocomputing , 2002 , 48 ( 1 - 4 ) : 85 - 105 . [ 24 ] Amemiya , Takeshi ( 1985 ) . ”Generalized Least Squares Theory” . Advanced Econometrics . Harvard University Press . ISBN 0 - 674 - 00560 - 0 . [ 25 ] Kmenta , Jan ( 1986 ) . ”Generalized Linear Regression Model and Its Applications” . Elements of Econometrics ( Second ed . ) . New York : Macmillan . pp . 607 – 650 . ISBN 0 - 472 - 10886 - 7 . [ 26 ] Kariya T , Kurata H . Generalized least squares [ M ] . John Wiley & Sons , 2004 . [ 27 ] Bernardo J M , Smith A F M . Bayesian theory [ M ] . John Wiley & Sons , 2009 . 12 [ 28 ] Myung I J . Tutorial on maximum likelihood estimation [ J ] . Journal of mathematical Psychology , 2003 , 47 ( 1 ) : 90 - 100 . [ 29 ] Gauvain J L , Lee C H . Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains [ J ] . IEEE transactions on speech and audio processing , 1994 , 2 ( 2 ) : 291 - 298 . [ 30 ] Koenker R , Hallock K F . Quantile regression [ J ] . Journal of eco - nomic perspectives , 2001 , 15 ( 4 ) : 143 - 156 . [ 31 ] Harrell Jr F E . Regression modeling strategies : with applications to linear models , logistic and ordinal regression , and survival analysis [ M ] . Springer , 2015 . [ 32 ] Faraway J J . Extending the linear model with R : generalized linear , mixed effects and nonparametric regression models [ M ] . CRC press , 2016 . [ 33 ] Efroymson , MA ( 1960 ) ”Multiple regression analysis . ” In Ralston , A . and Wilf , HS , editors , Mathematical Methods for Digital Com - puters . Wiley . [ 34 ] Flom , P . L . and Cassell , D . L . ( 2007 ) ”Stopping stepwise : Why stepwise and similar selection methods are bad , and what you should use , ” NESUG 2007 . [ 35 ] Specht , D . F . ( 2002 - 08 - 06 ) . ”A general regression neural net - work” . IEEE Transactions on Neural Networks . 2 ( 6 ) : 568 – 576 . doi : 10 . 1109 / 72 . 97934 . PMID 18282872 . [ 36 ] Dreiseitl S , Ohno - Machado L . Logistic regression and artiﬁcial neural network classiﬁcation models : a methodology review [ J ] . Journal of biomedical informatics , 2002 , 35 ( 5 - 6 ) : 352 - 359 . [ 37 ] Hutcheson G D . Ordinary least - squares regression [ J ] . L . Moutinho and GD Hutcheson , The SAGE dictionary of quantitative manage - ment research , 2011 : 224 - 228 . [ 38 ] Dismuke C , Lindrooth R . Ordinary least squares [ J ] . Methods and Designs for Outcomes Research , 2006 , 93 : 93 - 104 . [ 39 ] Kiers H A L . Weighted least squares ﬁtting using ordinary least squares algorithms [ J ] . Psychometrika , 1997 , 62 ( 2 ) : 251 - 266 . [ 40 ] Marquardt D W , Snee R D . Ridge regression in practice [ J ] . The American Statistician , 1975 , 29 ( 1 ) : 3 - 20 . [ 41 ] Hoerl A E , Kannard R W , Baldwin K F . Ridge regression : some simulations [ J ] . Communications in Statistics - Theory and Methods , 1975 , 4 ( 2 ) : 105 - 123 . [ 42 ] Le Cessie S , Van Houwelingen J C . Ridge estimators in logistic regression [ J ] . Journal of the Royal Statistical Society : Series C ( Ap - plied Statistics ) , 1992 , 41 ( 1 ) : 191 - 201 . [ 43 ] Osborne M R , Presnell B , Turlach B A . On the lasso and its dual [ J ] . Journal of Computational and Graphical statistics , 2000 , 9 ( 2 ) : 319 - 337 . [ 44 ] Zou H . The adaptive lasso and its oracle properties [ J ] . Journal of the American statistical association , 2006 , 101 ( 476 ) : 1418 - 1429 . [ 45 ] Park T , Casella G . The bayesian lasso [ J ] . Journal of the American Statistical Association , 2008 , 103 ( 482 ) : 681 - 686 . [ 46 ] Zhao P , Yu B . On model selection consistency of Lasso [ J ] . The Journal of Machine Learning Research , 2006 , 7 : 2541 - 2563 . [ 47 ] Meinshausen N . Relaxed lasso [ J ] . Computational Statistics & Data Analysis , 2007 , 52 ( 1 ) : 374 - 393 . [ 48 ] Zou H , Hastie T . Regression shrinkage and selection via the elastic net , with applications to microarrays [ J ] . JR Stat Soc Ser B , 2003 , 67 : 301 - 20 . [ 49 ] Ogutu J O , Schulz - Streeck T , Piepho H P . Genomic selection using regularized linear regression models : ridge regression , lasso , elastic net and their extensions [ C ] / / BMC proceedings . BioMed Central , 2012 , 6 ( 2 ) : 1 - 6 . [ 50 ] Ceperic E , Ceperic V , Baric A . A strategy for short - term load fore - casting by support vector regression machines [ J ] . IEEE Transactions on Power Systems , 2013 , 28 ( 4 ) : 4356 - 4364 . [ 51 ] Angiulli G , Cacciola M , Versaci M . Microwave devices and an - tennas modelling by support vector regression machines [ J ] . IEEE Transactions on Magnetics , 2007 , 43 ( 4 ) : 1589 - 1592 . [ 52 ] Xu S , An X , Qiao X , et al . Multi - output least - squares support vector regression machines [ J ] . Pattern Recognition Letters , 2013 , 34 ( 9 ) : 1078 - 1084 . [ 53 ] Segal M R . Machine learning benchmarks and random forest regression [ J ] . 2004 . [ 54 ] Svetnik V , Liaw A , Tong C , et al . Random forest : a classiﬁcation and regression tool for compound classiﬁcation and QSAR mod - eling [ J ] . Journal of chemical information and computer sciences , 2003 , 43 ( 6 ) : 1947 - 1958 . [ 55 ] Cootes T F , Ionita M C , Lindner C , et al . Robust and accurate shape model ﬁtting using random forest regression voting [ C ] / / European Conference on Computer Vision . Springer , Berlin , Heidelberg , 2012 : 278 - 291 . [ 56 ] J¨oreskog K G , Goldberger A S . Factor analysis by generalized least squares [ J ] . Psychometrika , 1972 , 37 ( 3 ) : 243 - 260 . [ 57 ] Orsini N , Bellocco R , Greenland S . Generalized least squares for trend estimation of summarized dose – response data [ J ] . The stata journal , 2006 , 6 ( 1 ) : 40 - 57 . [ 58 ] Browne M W . Generalized least squares estimators in the analysis of covariance structures [ J ] . South African statistical journal , 1974 , 8 ( 1 ) : 1 - 24 . [ 59 ] Hao L , Naiman D Q , Naiman D Q . Quantile regression [ M ] . Sage , 2007 . [ 60 ] Yu K , Lu Z , Stander J . Quantile regression : applications and current research areas [ J ] . Journal of the Royal Statistical Society : Series D ( The Statistician ) , 2003 , 52 ( 3 ) : 331 - 350 . [ 61 ] Meinshausen N , Ridgeway G . Quantile regression forests [ J ] . Jour - nal of Machine Learning Research , 2006 , 7 ( 6 ) . [ 62 ] Bishop C M , Tipping M E . Bayesian regression and classiﬁcation [ J ] . Nato Science Series sub Series III Computer And Systems Sciences , 2003 , 190 : 267 - 288 . [ 63 ] Gelman A , Goodrich B , Gabry J , et al . R - squared for Bayesian regression models [ J ] . The American Statistician , 2019 . [ 64 ] Koop G M . Bayesian econometrics [ M ] . John Wiley & Sons Inc . , 2003 . [ 65 ] Yu K , Moyeed R A . Bayesian quantile regression [ J ] . Statistics & Probability Letters , 2001 , 54 ( 4 ) : 437 - 447 . [ 66 ] Willett J B , Singer J D . Another cautionary note about R 2 : Its use in weighted least - squares regression analysis [ J ] . The American Statistician , 1988 , 42 ( 3 ) : 236 - 238 . [ 67 ] Chang P T , Lee E S . A generalized fuzzy weighted least - squares regression [ J ] . Fuzzy Sets and Systems , 1996 , 82 ( 3 ) : 289 - 298 . [ 68 ] Blatman G , Sudret B . Adaptive sparse polynomial chaos expan - sion based on least angle regression [ J ] . Journal of computational Physics , 2011 , 230 ( 6 ) : 2345 - 2367 . [ 69 ] Khan J A , Van Aelst S , Zamar R H . Robust linear model selection based on least angle regression [ J ] . Journal of the American Statisti - cal Association , 2007 , 102 ( 480 ) : 1289 - 1299 . [ 70 ] Hesterberg T , Choi N H , Meier L , et al . Least angle and l1 penalized regression : A review [ J ] . Statistics Surveys , 2008 , 2 : 61 - 93 . [ 71 ] Christensen R H B . ordinal—regression models for ordinal data [ J ] . R package version , 2015 , 28 : 2015 . [ 72 ] Elith J , Leathwick J . Boosted Regression Trees for ecological mod - eling [ J ] . R Documentation . Available online : https : / / cran . r - project . org / web / packages / dismo / vignettes / brt . pdf ( accessed on 12 June 2011 ) , 2017 . [ 73 ] Tyree S , Weinberger K Q , Agrawal K , et al . Parallel boosted regression trees for web search ranking [ C ] / / Proceedings of the 20th international conference on World wide web . 2011 : 387 - 396 . [ 74 ] Choi S , Kim T , Yu W . Performance evaluation of RANSAC fam - ily [ J ] . Journal of Computer Vision , 1997 , 24 ( 3 ) : 271 - 300 . [ 75 ] Derpanis K G . Overview of the RANSAC Algorithm [ J ] . Image Rochester NY , 2010 , 4 ( 1 ) : 2 - 3 . [ 76 ] Wilcox R . A note on the Theil - Sen regression estimator when the regressor is random and the error term is heteroscedastic [ J ] . Bio - metrical Journal : Journal of Mathematical Methods in Biosciences , 1998 , 40 ( 3 ) : 261 - 268 . [ 77 ] Fernandes R , Leblanc S G . Parametric ( modiﬁed least squares ) and non - parametric ( Theil – Sen ) linear regressions for predicting biophysical parameters in the presence of measurement errors [ J ] . Remote Sensing of Environment , 2005 , 95 ( 3 ) : 303 - 316 . [ 78 ] Sun Q , Zhou W X , Fan J . Adaptive huber regression [ J ] . Journal of the American Statistical Association , 2020 , 115 ( 529 ) : 254 - 265 . [ 79 ] Fox J , Weisberg S . Robust regression [ J ] . An R and S - Plus compan - ion to applied regression , 2002 , 91 . [ 80 ] Ostertagov´a E . Modelling using polynomial regression [ J ] . Proce - dia Engineering , 2012 , 48 : 500 - 506 . [ 81 ] Theil H . A rank - invariant method of linear and polynomial regres - sion analysis [ M ] / / Henri Theil’s contributions to economics and econometrics . Springer , Dordrecht , 1992 : 345 - 381 . [ 82 ] Bendel R B , Aﬁﬁ A A . Comparison of stopping rules in forward “stepwise” regression [ J ] . Journal of the American Statistical associ - ation , 1977 , 72 ( 357 ) : 46 - 53 . [ 83 ] Zheng B , Agresti A . Summarizing the predictive power of a generalized linear model [ J ] . Statistics in medicine , 2000 , 19 ( 13 ) : 1771 - 1781 . [ 84 ] Graybill F A . Theory and application of the linear model [ M ] . North Scituate , MA : Duxbury press , 1976 .