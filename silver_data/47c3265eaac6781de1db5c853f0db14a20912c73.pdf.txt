Distributed Momentum for Byzantine - resilient Learning El - Mahdi El - Mhamdi 1 Rachid Guerraoui 1 S´ebastien Rouault 1 Abstract Momentum is a variant of gradient descent that has been proposed for its beneﬁts on convergence . In a distributed setting , momentum can be im - plemented either at the server or the worker side . When the aggregation rule used by the server is linear , commutativity with addition makes both deployments equivalent . Robustness and privacy are however among motivations to abandon lin - ear aggregation rules . In this work , we demon - strate the beneﬁts on robustness of using mo - mentum at the worker side . We ﬁrst prove that computing momentum at the workers reduces the variance - norm ratio of the gradient estimation at the server , strengthening Byzantine resilient ag - gregation rules . We then provide an extensive ex - perimental demonstration of the robustness effect of worker - side momentum on distributed SGD . 1 . Introduction Gradient descent is the driving force of the recent successes in machine learning . Large - scale deployment of gradient descent relies on two ideas : stochastic approximation and distribution . Stochastic approximation ( drastically ) reduces the computation time , at the price of introducing variance in the gradient estimations . Distribution alleviates the work - load on a single machine but , as we discuss below , the multiplicity of elements inevitably increases the likelihood of ( malicious ) faults . In the distributed parameter server setting , the training of a model is basically performed as follows . A central machine , called the server , sends the current model ( the vector of parameters ) to other machines , called workers . These use their share of data ( either their local and private data , or data provided by the server for training purpose ) to compute a gradient estimate which is in turn sent to the server . As the server receives the gradients from different workers , the Author list in alphabetical order . 1 Distributed Computing Laboratory ( DCL ) , ´ Ecole Polytechnique F´ed´erale de Lausanne ( EPFL ) , Switzerland . Correspondence to : S ´ ebastien Rouault < se - bastien . rouault @ epﬂ . ch > . Parameterserver Byzantineworkers Honestworkers Figure 1 . A parameter server setup with n = 8 workers , among which f = 3 are Byzantine ( i . e . , adversarial ) workers . A black line represents a bidirectional communication channel . server typically averages their values to update the model if the setting is synchronous , or updates the model as individ - ual gradients are received if the model is asynchronous . When all the workers are reliable and provide correct es - timates of the gradient , this setting has close to optimal behavior ( Lian et al . , 2015 ; Zhang et al . , 2016 ; Dean et al . , 2012 ) . Many practical factors could however make the cor - rectness assumption of the workers doubtful . These factors span a large spectrum of causes , from software bugs , noisy or poisonous data , stale machines or worse , malicious at - tackers controlling some machines . The Byzantine abstraction is a very general fault model in distributed systems ( Lamport et al . , 1982 ) . The standard , golden solution for Byzantine fault tolerance is the state machine replication approach ( Schneider , 1990 ) . This ap - proach is however based on replication , which is unsuitable for distributed machine learning and stochastic gradient de - scent , such as federated learning ( Konecn´y et al . , 2015 ) . Workers could be independent entities , who could not be replicated for obvious privacy , scalability or legal reasons . For instance , in the context of federated learning , recent work has shown that Byzantine fault tolerance serves as a good basis to study poisoning ( Bagdasaryan et al . , 2018 ; Sun et al . , 2019 ) . In that same context , recent results show that Byzantine - resilient aggregation rules are effective against distributed backdoor attacks ( Xie et al . , 2020 ) . The key vulnerability in the standard parameter server rests upon how gradients are aggregated . Since 2017 , many al - ternatives to averaging have been proposed : ( Alistarh et al . , a r X i v : 2003 . 00010v2 [ c s . L G ] 9 M a r 2020 Distributed Momentum for Byzantine - resilient Learning 2018 ; Chen et al . , 2018 ; Yin et al . , 2018 ; Xie et al . , 2018a ; Blanchard et al . , 2017 ; El - Mhamdi et al . , 2018 ; Damaskinos et al . , 2018 ; Yang & Bajwa , 2019b ; TianXiang et al . , 2019 ; Bernstein et al . , 2019 ; Xie et al . , 2018a ; Yang & Bajwa , 2019a ; Chen et al . , 2018 ; Xie et al . , 2018b ; Yang et al . , 2019 ; Rajput et al . , 2019 ; Mu˜noz - Gonz´alez et al . , 2019 ) to list a few . In synchronous settings , these solutions consists in replacing the averaging of gradients by a robust alterna - tive such as the median and its variants ( Blanchard et al . , 2017 ; El - Mhamdi et al . , 2018 ; Xie et al . , 2018a ) or redun - dancy schemes ( Chen et al . , 2018 ; Rajput et al . , 2019 ) . In asynchronous settings , since no aggregation can be made , gradients are ( ideally ) used individually as they are deliv - ered , the robust alternatives are less diverse and are mostly consisting of a ﬁltering scheme ( Damaskinos et al . , 2018 ) . One common aspect underlying these methods is their re - liance on “quality gradients” from the non - Byzantine work - ers . Technically : the variance between non - Byzantine gra - dient estimates must be bounded below a factor of their average norm . This requirement is not new in machine learning ( Bottou , 1998 ) , and is actually independent from Byzantine considerations as an unbounded variance - norm ratio would prevent convergence . Is there a way to guarantee “quality gradient” at the non Byzantine workers ? Addressing this question is crucial to put Byzantine - resilient gradient descent to work . We provide a positive answer to this question by using mo - mentum ( Rumelhart et al . , 1986 ) . Momentum consists in summing a series of past gradients with the new one using an exponential decay factor µ ( 0 < µ < 1 ) , instead of using the new gradient alone . Momentum can be computed at the server side , when the update is performed , or at the workers’ side , when gradients are still computed ( Lin et al . , 2018 ) . In non Byzantine - resilient settings , both deployments are equivalent , as the gradient aggregation used at the server is linear and commutes with addition . In practice , momentum is typically employed at the server side . In this work , we propose to use momentum at the workers’ side since none of the existing Byzantine - resilient aggregation rules is linear . We ﬁrst show theoretically that indeed we can guarantee “quality gradient” by using momentum at the workers . Then we report on an extensive experimental assessment of this claim . In particular , and while using momentum at the workers has no additional overhead over momentum at the server , this technique led to an observed × 11 reduction on cross - accuracy drop due to Byzantine actors ( Section 4 . 3 ) . Contributions . Essentially , we show for the ﬁrst time that applying momentum at the workers signiﬁcantly boosts ro - bustness against Byzantine behavior . We prove that comput - ing momentum at the workers reduces the variance - norm ratio of the honest gradient estimations at the server , a key quantity for any robust alternative to averaging which ap - proximates a high - dimensional median ; for instance ( Blan - chard et al . , 2017 ; Xie et al . , 2018a ; El - Mhamdi et al . , 2018 ) . In particular , we show that combining now - standard defense mechanisms ( Blanchard et al . , 2017 ; Xie et al . , 2018a ; El - Mhamdi et al . , 2018 ) with momentum ( at the worker side ) ensures previously unavailable safety guarantees and coun - ters state - of - the - art attacks such as ( Baruch et al . , 2019 ; Xie et al . , 2019 ) . We report on an extensive experimental evaluation of this claim with 88 different tested sets of hy - perparameters ( 440 trained models in total ) , spanning the 2 mentioned state - of - the - art attacks and 3 defenses . Paper Organization . Section 2 formalizes the problem and provides the necessary background . Section 3 presents our theoretical contribution and compares the usage of mo - mentum at the workers versus at the server . Section 4 describes our experimental settings in details , before pre - senting and analysing our experimental results . Section 5 discusses related and future work . Due to space limitation , only a representative fraction of the experimental results is presented in the main paper . The supplementary material reports on the entirety of our exper - iments , along with the code and procedure to reproduce all of our results ( including the graphs ) . 2 . Background 2 . 1 . Byzantine Distributed SGD Stochastic Gradient Descent ( SGD ) . We consider the classical problem of optimizing a non - convex , differen - tiable loss function Q : R d → R , where Q ( θ t ) (cid:44) E x ∼D [ q ( θ t , x ) ] for a ﬁxed data distribution D . Namely , we seek a θ ∗ ∈ R d such that : ∇ Q ( θ ∗ ) = 0 ( 1 ) Using SGD , we initially pick a random θ 0 ∈ R d . Then at every step t ≥ 0 , we uniformly sample b datapoints x 1 . . . x b from D to estimate a gradient g t (cid:44) 1 b (cid:80) b k = 1 ∇ q ( θ t , x k ) ≈ ∇ Q ( θ t ) . Finally , for step t , we update the parameter vector using θ t + 1 = θ t − η t g t , where η t > 0 is the learning rate . One ﬁeld - tested amendment to this update rule is momen - tum ( Rumelhart et al . , 1986 ) , where each gradient has an exponentially - decreasing effect on every subsequent update . Formally : θ t + 1 = θ t − η t (cid:80) tu = 0 µ t − u g u , with 0 < µ < 1 . Distributed SGD with Byzantine workers . We follow the parameter server model ( Li et al . , 2014 ) : 1 process ( the parameter server ) holding the parameter vector θ t ∈ R d , and n other processes ( the workers ) estimating gradients . Among these n workers , up to f < n are said Byzantine , i . e . , adversarial . Unlike the other n − f honest workers , these f Byzantine workers can send arbitrary gradients ( Figure 1 ) . Distributed Momentum for Byzantine - resilient Learning At each step t , the parameter server receives n different gradients g ( 1 ) t . . . g ( n ) t , among which f are arbitrary ( sent by the Byzantine workers ) . So the update equation becomes : θ t + 1 = θ t − η t G t where : G t (cid:44) t (cid:88) u = 0 µ t − u F (cid:16) g ( 1 ) u , . . . , g ( n ) u (cid:17) ( 2 ) and where : F : (cid:0) R d (cid:1) n → R d The function F is called a Gradient Aggregation Rule ( GAR ) . If we assume no Byzantine worker , averaging is sufﬁcient ; formally : F (cid:16) g ( 1 ) t , . . . , g ( n ) t (cid:17) = 1 n (cid:80) ni = 1 g ( i ) t . In the pres - ence of Byzantine workers , a more complex aggregation is performed with a Byzantine - resilient GAR . Section 2 . 2 presents the three Byzantine - resilient GARs studied in this paper , along with their own theoretical requirements . Adversarial Model . The goal of the adversary is to im - pede the learning process , which can generally be deﬁned as the maximization of the loss Q or , more judiciously in the image classiﬁcation tasks used in this paper , as the mini - mization 1 of the model’s top - 1 cross - accuracy . The adversary cannot directly overwrite θ t at the parameter server . The adversary only submits f arbitrary gradients to the server per step , via the f Byzantine workers it controls 2 . We assume an omniscient adversary . In particular , the ad - versary knows the GAR used by the parameter server and can generate Byzantine gradients dependent on the honest gradients submitted at the same step . 2 . 2 . Byzantine - resilient GARs We formally present below the 3 GARs studied in this paper . These GARs are Byzantine - resilience , a notion ﬁrst intro - duced by ( Blanchard et al . , 2017 ) under the name ( α , f ) - Byzantine - resilience . When used within its operating as - sumptions , a Byzantine - resilient GAR guarantees conver - gence ( in the sense of ( 1 ) ) even in an adversarial setting . Deﬁnition 1 . Let ( α , f ) ∈ (cid:2) 0 . . π 2 (cid:2) × [ 0 . . n ] , with n the total number of workers . Let (cid:16) g ( 1 ) t . . . g ( n ) t (cid:17) ∈ (cid:0) R d (cid:1) n , among which n − f are independent ( “honest” ) vectors following the same distribution G t ; the f other vectors are arbitrary , each possibly dependent on G t and the “honest” vectors . A GAR F is said to be ( α , f ) - Byzantine resilient iff : g t (cid:44) F (cid:16) g ( 1 ) t , . . . , g ( n ) t (cid:17) satisﬁes : 1 I . e . , with 10 classes , the worst possible ﬁnal accuracy is 0 . 1 . 2 Said otherwise , the f Byzantine workers can collude . 1 . (cid:104) E g t , E G t (cid:105) ≥ ( 1 − sin α ) · (cid:107) E G t (cid:107) > 0 2 . ∀ r ∈ { 2 , 3 , 4 } , E (cid:107) g t (cid:107) r is bounded above by a linear combination of the terms E (cid:107)G t (cid:107) r 1 . . . E (cid:107)G t (cid:107) r k , with ( k , r 1 . . . r k ) ∈ ( N ∗ ) k + 1 and r 1 + . . . + r k = r . 2 . 2 . 1 . K RUM ( B LANCHARD ET AL . , 2017 ) Let ( f , m ) ∈ N 2 , with n ≥ 2 f + 3 and 1 ≤ m ≤ n − f − 2 . Krum works by assigning a score to each input gradient . The score of g ( i ) t is the sum of the distances between g ( i ) t and its n − f − 2 closest neighbor gradients . Krum outputs the arithmetic mean of the m smallest – scoring gradients 3 . In our experiments , we set m to its maximum : n − f − 2 . To be proven ( α , f ) - Byzantine resilient , besides the stan - dard convergence conditions in non - convex optimization ( Bottou , 1998 ) , Krum requires the honest gradients’ vari - ance E (cid:107)G t − E G t (cid:107) 2 to be bounded above as follows : 2 · κ ( n , f ) · E (cid:107)G t − E G t (cid:107) 2 < (cid:107) E G t (cid:107) 2 ( 3 ) with : κ ( n , f ) (cid:44) n − f + f ( n − f − 2 ) + f 2 ( n − f − 1 ) n − 2 f − 2 2 . 2 . 2 . M EDIAN ( X IE ET AL . , 2018 A ) Let f ∈ N with n ≥ 2 f + 1 . Median computes the coordinate - wise median of the input gradients g ( 1 ) t . . . g ( n ) t . Formally for the real - valued median : median ( x 1 . . . x n ) (cid:44) arg min x ∈ R n (cid:88) i = 1 | x i − x | And so , formally for the coordinate – wise Median : Median (cid:16) g ( 1 ) t . . . g ( n ) t (cid:17) (cid:44)   median (cid:16) g ( 1 ) t [ 1 ] . . . g ( n ) t [ 1 ] (cid:17) . . . median (cid:16) g ( 1 ) t [ d ] . . . g ( n ) t [ d ] (cid:17)   The condition of ( α , f ) - Byzantine resilience is : ( n − f ) · E (cid:107)G t − E G t (cid:107) 2 < (cid:107) E G t (cid:107) 2 ( 4 ) 2 . 2 . 3 . B ULYAN ( E L - M HAMDI ET AL . , 2018 ) Bulyan uses another Byzantine - resilient GAR to aggregate the input gradients . In the remaining of this paper we will consider Bulyan of Krum , that we will simply call Bulyan . Let ( f , m ) ∈ N 2 , with n ≥ 4 f + 3 and 1 ≤ m ≤ n − f − 2 . Bulyan ﬁrst selects n − 2 f − 2 gradients by iterating n − 2 f − 2 times over Krum , each time removing the highest scoring gradient from the input gradient set . From these n − 2 f − 2 ≥ 2 f + 1 selected gradients , Bulyan outputs 3 The original paper called the GAR Multi - Krum when m > 1 . Distributed Momentum for Byzantine - resilient Learning the coordinate - wise average of the n − 4 f − 2 ≥ 1 closest coordinate values to the coordinate - wise median . The theoretical requirements for the ( α , f ) - Byzantine re - silience of Bulyan are the same as the ones of Krum . 2 . 3 . Studied Attacks The two , state - of - the - art attacks studied in this paper follow the same core algorithm , that we identify below . Let ε t ∈ R ≥ 0 be a non - negative factor , and a t ∈ R d an attack vector which value depends on the actual attack used . At each step t , each of the f Byzantine workers submits the same Byzantine gradient : g t + ε t · a t ( 5 ) , where g t is an approximation of the real gradient ∇ Q ( θ t ) at step t . For both of the studied attacks , the value of ε t is ﬁxed . 2 . 3 . 1 . A L ITTLE IS E NOUGH ( B ARUCH ET AL . , 2019 ) In this attack , a Byzantine worker submits g t + ε t · a t , with a t (cid:44) − σ t the opposite of the coordinate - wise standard deviation of the honest gradient distribution G t . 2 . 3 . 2 . F ALL OF E MPIRES ( X IE ET AL . , 2019 ) A Byzantine worker submits ( 1 − ε t ) g t , i . e . , a t (cid:44) − g t . 3 . Momentum at the Workers The Byzantine - resilience of Krum , Median and Bulyan rely on the honest gradients being sufﬁciently clumped . For the GARs we study , this is formalized in equations ( 3 ) and ( 4 ) . This requirement is theoretically important . When the vari - ance of the honest gradients is too high compared to their norms ( e . g . , Equation ( 3 ) unsatisﬁed ) , the Byzantine gradi - ents can induce aggregated gradients having negative dot - products with the real gradient , preventing convergence ( as such aggregated gradients would locally increase the loss ) . This requirement is also not satisﬁed in practice . When reproducing the attacks ( ﬁgures 2 to 5 ) , we measured and observed that the honest gradients’ variance is often at least one order of magnitude too large for all the studied GARs . In the 400 experiments we performed under attack , we mea - sured the theoretical requirement of Equation ( 3 ) is never satisﬁed , not even for a single step , in 394 of them . Among the 6 other experiments ( all with the CIFAR - 10 model ) , equations ( 3 ) or ( 4 ) were never veriﬁed for more than 4 steps ( out of 3000 ) per experiment . Nevertheless , our empirical evidences show that reducing the honest gradients’ variance relative to their norm can be enough to defend the training against the two presented attacks . In this section we present a technique aiming at decreasing the variance - norm ratio of the honest gradients , reducing or even cancelling at negligible computational costs the effects of the attacks studied in this paper . 3 . 1 . Formulation From the formulation of momentum SGD ( Equation ( 2 ) ) : G t (cid:44) t (cid:88) u = 0 µ t − u F (cid:16) g ( 1 ) u , . . . , g ( n ) u (cid:17) we instead confer the momentum operation on the workers : G t (cid:44) F (cid:18) t (cid:88) u = 0 µ t − u g ( 1 ) u (cid:124) (cid:123)(cid:122) (cid:125) G ( 1 ) t , . . . , t (cid:88) u = 0 µ t − u g ( n ) u (cid:124) (cid:123)(cid:122) (cid:125) G ( n ) t (cid:19) ( 6 ) Notations . In the remaining of this paper , we call the original formulation ( momentum ) at the server , and the proposed , revised formulation ( momentum ) at the workers . 3 . 2 . Effects We compare the variance - norm ratio when momentum is computed at the server versus at the workers . Let λ t (cid:44) (cid:107) E G t (cid:107) > 0 be the real gradient’s norm at step t . Let σ t (cid:44) (cid:113) E (cid:107)G t − E G t (cid:107) 2 be the standard deviation of the real gradient at step t . The variance - norm ratio , when momentum is computed at the server , is : r ( s ) t (cid:44) σ t 2 λ t 2 We will now compute this ratio when momentum is applied at the workers . Let G ( i ) t , with G ( i ) − 1 (cid:44) 0 , be the gradient sent by any honest worker i at step t , i . e . : G ( i ) t (cid:44) t (cid:88) u = 0 µ t − u g ( i ) u Then , for any two honest worker identiﬁers i (cid:54) = j : E (cid:13)(cid:13)(cid:13) G ( i ) t − G ( j ) t (cid:13)(cid:13)(cid:13) 2 = E (cid:13)(cid:13)(cid:13) g ( i ) t + µ G ( i ) t − 1 − g ( j ) t − µ G ( j ) t − 1 (cid:13)(cid:13)(cid:13) 2 = E (cid:13)(cid:13)(cid:13) g ( i ) t − g ( j ) t (cid:13)(cid:13)(cid:13) 2 + µ 2 E (cid:13)(cid:13)(cid:13) G ( i ) t − 1 − G ( j ) t − 1 (cid:13)(cid:13)(cid:13) 2 + 2 µ   E g ( i ) t − E g ( j ) t (cid:124) (cid:123)(cid:122) (cid:125) = E G t − E G t   · (cid:16) E G ( i ) t − 1 − E G ( j ) t − 1 (cid:17) (cid:124) (cid:123)(cid:122) (cid:125) = 0 = E (cid:13)(cid:13)(cid:13) g ( i ) t − g ( j ) t (cid:13)(cid:13)(cid:13) 2 + µ 2 E (cid:13)(cid:13)(cid:13) G ( i ) t − 1 − G ( j ) t − 1 (cid:13)(cid:13)(cid:13) 2 = 2 σ t 2 + µ 2 (cid:0) 2 σ t − 12 + µ 2 (cid:0) 2 σ t − 22 + µ 2 ( . . . ) (cid:1)(cid:1) Distributed Momentum for Byzantine - resilient Learning = 2 t (cid:88) u = 0 µ 2 ( t − u ) σ u 2 ( 7 ) = 2 E (cid:13)(cid:13)(cid:13) G ( i ) t − E G ( i ) t (cid:13)(cid:13)(cid:13) 2 (cid:13)(cid:13)(cid:13) E G ( i ) t (cid:13)(cid:13)(cid:13) 2 = (cid:13)(cid:13)(cid:13) E g ( i ) t + µ E G ( i ) t − 1 (cid:13)(cid:13)(cid:13) 2 = (cid:13)(cid:13)(cid:13) E g ( i ) t (cid:13)(cid:13)(cid:13) 2 + 2 µ E g ( i ) t · E G ( i ) t − 1 + µ 2 (cid:13)(cid:13)(cid:13) E G ( i ) t − 1 (cid:13)(cid:13)(cid:13) 2 = λ t 2 + 2 µ E g ( i ) t · (cid:16) E g ( i ) t − 1 + µ (cid:16) E g ( i ) t − 2 + µ ( . . . ) (cid:17)(cid:17) + µ 2 (cid:16) λ t − 12 + 2 µ E g ( i ) t − 1 · (cid:16) E g ( i ) t − 2 + µ ( . . . ) (cid:17) + µ 2 E (cid:13)(cid:13)(cid:13) G ( i ) t − 2 (cid:13)(cid:13)(cid:13) 2 (cid:19) = t (cid:88) u = 0 µ 2 ( t − u )    λ u 2 + 2 u − 1 (cid:88) v = 0 µ u − v E g ( i ) u · E g ( i ) v (cid:124) (cid:123)(cid:122) (cid:125) = E G u · E G v    Thus , assuming honest gradients E G ( i ) t do not become null : r ( w ) t (cid:44) Ω t 2 Λ t 2 = (cid:80) tu = 0 µ 2 ( t − u ) σ u 2 (cid:80) tu = 0 µ 2 ( t − u ) (cid:0) λ u 2 + s u (cid:1) where the expected “straightness” of the gradient computed by an honest worker at step u is deﬁned by : s u (cid:44) 2 u − 1 (cid:88) v = 0 µ u − v E G u · E G v s u quantiﬁes what can be thought as the curvature of the honest gradient trajectory . Straight trajectories can make s u grow up to ( 1 − µ ) − 1 > 1 times the expected squared - norm of the honest gradients , while highly “curved” trajectories ( e . g . , close to a local minimum ) tend to make s u negative . This observation stresses that this formulation of momentum can sometimes be harmful for the purpose of Byzantine resilience . We measured s u for every step u > 0 in our experiments , and we always observed that this quantity is positive and increases for a short window of ( dozen ) steps ( depending on η t ) , and then oscillates between positive and negative values . These two phases are noticeable in the ﬁrst steps of Figure 5 . While the empirical impact ( decreased or cancelled loss in accuracy ) is concrete , we believe there is room for further improvements , discussed in Section 5 . The purpose of using momentum at the workers is to reduce the variance - norm ratio r ( w ) t , compared to r ( s ) t . Since g ( i ) 0 = G ( i ) 0 , we verify that r ( u ) 0 = r ( w ) 0 . Then , ∀ t > 0 : r ( w ) t ≤ r ( s ) t ⇔ σ t 2 + µ 2 Ω t − 12 λ t 2 + s t + µ 2 Λ t − 12 ≤ σ t 2 λ t 2 ⇔ µ 2 Ω t − 12 λ t 2 ≤ (cid:0) s t + µ 2 Λ t − 12 (cid:1) σ t 2 ⇔ s t ≥ µ 2 Λ t − 12 (cid:32) r ( w ) t − 1 r ( s ) t − 1 (cid:33) ( 8 ) The condition for decreasing r ( w ) t can be obtained similarly : r ( w ) t ≤ r ( w ) t − 1 ⇔ s t ≥ λ t 2 (cid:32) r ( s ) t r ( w ) t − 1 − 1 (cid:33) To study the impact of a lower learning rate η t on s t , we will assume that the real gradient ∇ Q is l - Lipschitz . Namely : ∀ ( t , u ) ∈ N 2 , u < t , (cid:107) E G t − E G u (cid:107) 2 ≤ l 2 (cid:107) θ t − θ u (cid:107) ≤ l 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 (cid:88) v = u η v G v (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Then , ∀ ( t , u ) ∈ N 2 , u < t , we can rewrite : (cid:107) E G t − E G u (cid:107) 2 = (cid:107) E G t (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) λ t 2 + (cid:107) E G u (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) λ u 2 − 2 E G t · E G u And ﬁnally , we can lower - bound s t as : t − 1 (cid:88) u = 0 µ t − u (cid:107) E G t − E G u (cid:107) 2 = t − 1 (cid:88) u = 0 µ t − u (cid:0) λ t 2 + λ u 2 (cid:1) − 2 t − 1 (cid:88) u = 0 µ t − u E G t · E G u (cid:124) (cid:123)(cid:122) (cid:125) s t ≤ t − 1 (cid:88) u = 0 µ t − u l 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 (cid:88) v = u η v G v (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ⇔ s t ≥ t − 1 (cid:88) u = 0 µ t − u (cid:32) λ t 2 + λ u 2 − l 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 (cid:88) v = u η v G v (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:33) ( 9 ) ≥ 1 − µ t 1 − µ λ t 2 + t − 1 (cid:88) u = 0 µ t − u (cid:32) λ u 2 − l 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 (cid:88) v = u η v G v (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:33) When the real gradient ∇ Q is ( locally ) Lipschitz continuous , reducing the learning rate η t can sufﬁce to ensure s t satisﬁes the conditions laid above for decreasing the variance - norm ratio r ( w ) t ; the purpose of momentum at the workers . Importantly this last lower bound , namely Equation ( 9 ) , sets how the practitioner should choose two hyperparameters , µ and η t , for the purpose of Byzantine - resilience . Basi - cally , and as long as it does not harm the training without adversary , µ should be set as high and η t as low as possible . As a side note , ( Xie et al . , 2019 ) is prone to increasing the lower bound on s t . Indeed , this attack submits gradients smaller or opposed to the honest gradient ( Section 2 . 3 . 2 ) . Such an attack can shorten the parameter trajectory , and so can improve Byzantine - resilience in the ensuing step ( s ) . Distributed Momentum for Byzantine - resilient Learning 4 . Experiments The goal of this section is to empirically verify our theo - retical results , measuring the evolution of both the top - 1 cross - accuracy and variance - norm ratio over the training . Our experiments cover every possible combinations of 5 key hyperparameters , including combinations used by ( Baruch et al . , 2019 ; Xie et al . , 2019 ) . For reproducibility and conﬁ - dence in the results , each combination of hyperparameters is repeated 5 times with seeds 1 to 5 , totalling 440 different runs . Besides observing the beneﬁt of lower learning rates , our results show tangible mitigation of both attacks . 4 . 1 . Experimental Setup We use a compact notation to deﬁne the models : L ( # outputs ) for a fully - connected linear layer , R for ReLU activation , S for log - softmax , C ( # channels ) for a fully - connected 2D - convolutional layer ( kernel size 3 , padding 1 , stride 1 ) , M for 2D - maxpool ( kernel size 2 ) , N for batch - normalization , and D for dropout ( with ﬁxed probability 0 . 25 ) . We use the model and dataset from ( Baruch et al . , 2019 ) : Model ( 784 ) - L ( 100 ) - R - L ( 10 ) - R - S Dataset MNIST ( 83 training points / gradient ) # workers n = 51 f ∈ { 24 , 12 } We also use the model and dataset from ( Xie et al . , 2019 ) : Model ( 3 , 32 × 32 ) - C ( 64 ) - R - B - C ( 64 ) - R - B - M - D - - C ( 128 ) - R - B - C ( 128 ) - R - B - M - D - - L ( 128 ) - R - D - L ( 10 ) - S Dataset CIFAR - 10 ( 50 training points / gradient ) # workers n = 25 f ∈ { 11 , 5 } For model training , we use the negative log likelihood loss and respectively 10 − 4 and 10 − 2 (cid:96) 2 - regularization for the MNIST and CIFAR - 10 models . We also clip gradients , ensuring their norms remain respectively below 2 and 5 for the MNIST and CIFAR - 10 models . For model evaluation , we use the top - 1 cross - accuracy on the whole testing set . Both datasets are pre - processed before training . For MNIST we apply the same pre - processing as in ( Baruch et al . , 2019 ) : an input image normalization with mean 0 . 1307 and stan - dard deviation 0 . 3081 . For CIFAR - 10 , besides including horizontal ﬂips of the input pictures , we also apply a per - channel normalization with means 0 . 4914 , 0 . 4822 , 0 . 4465 and standard deviations 0 . 2023 , 0 . 1994 , 0 . 2010 ( Liu , 2019 ) . We set f the number of Byzantine workers either to the maximum for which Krum can be used ( roughly an half : f = (cid:4) n − 3 2 (cid:5) ) , or the maximum for Bulyan ( roughly a quarter , f = (cid:4) n − 3 4 (cid:5) ) . The attack factors ε t ( Section 2 . 3 ) are set to constants proposed in the literature , namely ε t = 1 . 5 for ( Baruch et al . , 2019 ) and ε t = 1 . 1 for ( Xie et al . , 2019 ) . Guided by our theoretical study on the impact of the learning rate on the variance - norm ratio , for every pair model - attack in our experiments we select two different learning rates . The ﬁrst and largest is selected so as to maximize the per - formance ( highest ﬁnal cross - accuracy and accuracy gain per step ) of the model trained without Byzantine workers . The second and smallest is chosen so as to minimize the per - formance loss under attack , without substantially impacting the ﬁnal accuracy when trained without Byzantine workers . The MNIST and CIFAR - 10 model are trained respectively with µ = 0 . 9 and µ = 0 . 99 . These values were obtained by trial and error , to maximize overall accuracy gain per step . Our theoretical analysis highlights two metrics : the top - 1 cross - accuracy , measuring the performance of the model , and the variance - norm ratio , i . e . either r ( s ) t or r ( w ) t in ac - cordance with where momentum was carried out . Each experiment is run 5 times . We present the average and standard deviation of the two metrics over these 5 runs . 4 . 2 . Reproducibility Particular care has been taken to make our results repro - ducible . Each of the 5 runs per experiment are respectively seeded with seed 1 to 5 . For instance , this implies that two experiments with same seed and same model also starts with the same parameters θ 0 . To further reduce the sources of non - determinism , the CuDNN backend is conﬁgured in deterministic mode ( our experiments ran on a GeForce GTX 1080 Ti ) with benchmark mode turned off . We also used log - softmax + nll loss , which is equal to softmax + cross - entropy loss , but with improved numerical stability on PyTorch . We provide our code along with a script reproducing all of our results , both the experiments and the graphs , in one command . Details , including software and hardware depen - dencies , are available in the supplementary material . 4 . 3 . Experimental Results For each of the pair model - dataset , we consider 5 variable hyperparameters : which attack to test ( ( Baruch et al . , 2019 ) or ( Xie et al . , 2019 ) ) , which defense to run ( Krum , Median or Bulyan ) , how many Byzantine workers f to use ( an half or a quarter ) , where momentum is computed ( at the server or at the workers ) and which learning rate η t to apply . We report on every possible combination of these hyperpa - rameters , along with baselines that use averaging without attack . With 5 repetitions per setup , the experiments consist in 440 runs , aggregated and studied in the supplementary material . In this section we report on a representative subset . We made a concerning observation : one of the theoretical requirement for Byzantine - resilience , equations ( 3 ) or ( 4 ) , is actually rarely satisﬁed in practice . In less than 2 % of the runs under attack was this theoretical condition satisﬁed for Distributed Momentum for Byzantine - resilient Learning ( a ) Momentum at the server , η t = 0 . 5 ( b ) Momentum at the workers , η t = 0 . 5 Figure 2 . MNIST and the associated model ( Section 4 . 1 ) , n = 51 and f = 12 . The attack , ( Baruch et al . , 2019 ) , has a tangible impact ( − 15 % on the maximum accuracy ) in this setup . Using momentum at the workers diminishes the effect of the attack , even substantially when Bulyan is used ( only 1 % loss in accuracy ) . ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise Figure 3 . CIFAR - 10 and the associated model ( Section 4 . 1 ) , with n = 25 and f = 5 . As in Figure 2 , ( Baruch et al . , 2019 ) has a strong impact on the training . The positive effect of momentum at the workers ( Figure 3b ) is conspicuous and , as predicted by the theory ( Section 3 . 2 ) , ampliﬁed with a lower learning rate . ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise Figure 4 . CIFAR - 10 and the associated model ( Section 4 . 1 ) , with n = 25 and f = 11 ( Figure 3 uses f = 5 ) . The attack , ( Xie et al . , 2019 ) , is extremely efﬁcient when f ≈ n 2 , but had almost no effect with f ≈ n 4 ; the supplementary material has a more com - plete range of experiments . Momentum at the workers noticeably improves the cross - accuracy when Median is used in this setting . ( a ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 5 . CIFAR - 10 and the associated model ( Section 4 . 1 ) , with n = 25 and f = 5 , showing a decreased variance - norm ratio when momentum is computed at the workers . The beneﬁt of a lower η t is clearly visible in ( a ) , when its value is reduced at step 1500 . Distributed Momentum for Byzantine - resilient Learning at least 1 step , and none for more than 4 steps ( out of 3000 ) . In most of the experiments , the observed variance - norm ratio was often between 1 and 2 orders of magnitude too high ( e . g . , Figure 5 ) . Since our hyperparameters ( model , dataset , mini - batch size , n , f ) are very close , if not equal , to those used in the experiments of ( Baruch et al . , 2019 ; Xie et al . , 2019 ) , such a substantial margin ( 1 to 2 orders of magnitude ) lets us think the theoretical requirements for Byzantine resilience were actually not satisﬁed either in ( Baruch et al . , 2019 ; Xie et al . , 2019 ) . ( Baruch et al . , 2019 ) reached the same conclusion , using a different experiment . Result Analysis . With the largest , optimal learning rate , we obtained with Krum and for both models very similar maximum top - 1 cross - accuracies to the ones obtained in ( Baruch et al . , 2019 ) 4 , namely ∼ 80 % for MNIST’s model ( Figure 2a ) and ∼ 20 % for CIFAR - 10’s model ( Figure 3a ) . A similar observation can be made for ( Xie et al . , 2019 ) : on the CIFAR - 10’s model , the maximum accuracies of Krum and Median are respectively ∼ 10 % and ∼ 20 % ( Figure 4a ) . The beneﬁt of computing momentum at the workers is vis - ible in all the ﬁgures . Regarding the impact on the top - 1 cross - accuracy , we systematically 5 observe an increase com - pared to when momentum is computed at the server ( ﬁgures 2 – 4 ) . The empirical increase ranges from + 5 % , on the MNIST model attacked by ( Baruch et al . , 2019 ) ( supple - mentary material , Figure 3 ) , to + 50 % on the CIFAR - 10 model , defended by Median ( Figure 3 ) . Regarding the ef - fect on the variance - norm ratio , we comparatively observe a decrease of this ratio before approaching convergence . As predicted by our theoretical analysis , this decrease can be ampliﬁed by reducing the learning rate ( Figure 5a ) . 5 . Concluding Remarks Momentum - based Variance Reduction . Our algorithm is different from ( Cutkosky & Orabona , 2019 ) , as instead of reducing the variance of the gradients , we actually increase it ( Equation ( 7 ) ) . What we seek to reduce is the variance - norm ratio , which is the key quantity for any Byzantine - resilient GAR approximating a high - dimensional median , e . g . Krum , Median , Bulyan as well as in ( Yang & Bajwa , 2019b ; a ; Chen et al . , 2017 ; Mu˜noz - Gonz´alez et al . , 2019 ) 6 . Some of the ideas introduced in ( Cutkosky & Orabona , 2019 ) could nevertheless help further improve Byzantine resilience . For instance , introducing an adaptive learning rate which decreases depending on the curvature of the parameter trajectory is an appealing approach to further reduce the variance - norm ratio ( Equation ( 9 ) ) . 4 Although ( Baruch et al . , 2019 ) uses Krum with m = 1 ( see Section 2 . 2 . 1 ) , and not the exact same CIFAR - 10 model . 5 Except for Krum against ( Xie et al . , 2019 ) when f = (cid:4) n − 3 2 (cid:5) . 6 This list is not exhaustive . Further Work . The theoretical condition for ratio reduc - tion , in Section 3 . 2 , shows that momentum at the workers is a double - edged sword . The intuition can be gained with the classic analogy from physics : without Byzantine work - ers , momentum makes the parameters θ t somehow like a particle travelling down the loss function with inertia . When inside a “straight valley” , past estimation errors are on average compensated in the next steps , dampening os - cillations and accumulating the average descent direction . The variance - norm ratio of the momentum gradient is then reduced , mostly because its norm increases , which is quan - tiﬁed by s t . The problem is that s t can become negative . Intuitively with the particle analogy , this happens when the loss is locally “curved” , for instance when approaching a local minimum . The particle may start continuing “uphill” instead of following the “valley” , and so , losing momen - tum . The norm of the momentum gradient then decreases , increasing the variance - norm ratio . While the ability to cross narrow , local minima is recognized as an accelerator ( Goh , 2017 ) , for the purpose of Byzantine - resilience we want to ensure momentum at the workers does not increase the variance - norm ratio compared to the classi - cal , momentum at the server . The theoretical condition for this purpose is given in Equation ( 8 ) . One simple amend - ment would then be to use momentum at the workers when Equation ( 8 ) is satisﬁed , and fallback to computing it at the server otherwise . Also , a more complex , possible future ap - proach could be to dynamically adapt the momentum factor µ , decreasing it as the curvature increases . Asynchronous SGD . We focused in this work on the syn - chronous setting , which received most of the attention in the Byzantine - resilient literature . Yet , we believe our work can be applied to asynchronous settings , as momentum is agnos - tic to the question of synchrony . Speciﬁcally , combining our idea with a ﬁltering scheme such as Kardam ( Damaskinos et al . , 2018 ) is in principle possible , as the ﬁlter commutes with the basic operations of momentum . However , further analysis of the interplay between the dynamics of stale gra - dients and the dynamics of momentum remain necessary . Byzantine Servers . While most of the research on Byzantine - resilience gradient descent has focused on the workers’ side , assuming a reliable server , recent efforts have started tackling Byzantine servers ( El - Mhamdi et al . , 2019 ) . Our reduction of the variance - norm ratio strengthens the gradient aggregation phase , which is necessary whether we deal with Byzantine workers or Byzantine servers . An in - teresting open question is how the momentum dynamics affects the models drift between different parameter servers . Any quantitative answer to this question will enable the use of our method in fully decentralised Byzantine resilient gradient descent . Distributed Momentum for Byzantine - resilient Learning References Alistarh , D . , Allen - Zhu , Z . , and Li , J . Byzantine stochastic gradient descent . In Advances in Neural Information Processing Systems 31 : Annual Conference on Neural Information Processing Systems 2018 , NeurIPS 2018 , 3 - 8 December 2018 , Montr´eal , Canada , pp . 4618 – 4628 , 2018 . Bagdasaryan , E . , Veit , A . , Hua , Y . , Estrin , D . , and Shmatikov , V . How to backdoor federated learning . CoRR , abs / 1807 . 00459 , 2018 . Baruch , M . , Baruch , G . , and Goldberg , Y . A little is enough : Circumventing defenses for distributed learning . In Ad - vances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , 8 - 14 December 2019 , Long Beach , CA , USA , 2019 . Bernstein , J . , Zhao , J . , Azizzadenesheli , K . , and Anandku - mar , A . signsgd with majority vote is communication efﬁcient and fault tolerant . In 7th International Con - ference on Learning Representations , ICLR 2019 , New Orleans , LA , USA , May 6 - 9 , 2019 , 2019 . Blanchard , P . , El - Mhamdi , E . - M . , Guerraoui , R . , and Stainer , J . Machine learning with adversaries : Byzantine tolerant gradient descent . In Advances in Neural Infor - mation Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , 4 - 9 Decem - ber 2017 , Long Beach , CA , USA , pp . 119 – 129 , 2017 . Bottou , L . Online learning and stochastic approximations . Online learning in neural networks , 17 ( 9 ) : 142 , 1998 . Chen , L . , Wang , H . , Charles , Z . B . , and Papailiopoulos , D . S . DRACO : byzantine - resilient distributed training via redundant gradients . In Proceedings of the 35th Inter - national Conference on Machine Learning , ICML 2018 , Stockholmsm¨assan , Stockholm , Sweden , July 10 - 15 , 2018 , pp . 902 – 911 , 2018 . Chen , Y . , Su , L . , and Xu , J . Distributed statistical machine learning in adversarial settings : Byzantine gradient de - scent . CoRR , abs / 1705 . 05491 , 2017 . Cutkosky , A . and Orabona , F . Momentum - based vari - ance reduction in non - convex SGD . In Wallach , H . M . , Larochelle , H . , Beygelzimer , A . , d’Alch´e - Buc , F . , Fox , E . B . , and Garnett , R . ( eds . ) , Advances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Sys - tems 2019 , NeurIPS 2019 , 8 - 14 December 2019 , Vancouver , BC , Canada , pp . 15210 – 15219 , 2019 . URL http : / / papers . nips . cc / paper / 9659 - momentum - based - variance - reduction - in - non - convex - sgd . Damaskinos , G . , El - Mhamdi , E . - M . , Guerraoui , R . , Patra , R . , and Taziki , M . Asynchronous byzantine machine learning ( the case of SGD ) . In Proceedings of the 35th International Conference on Machine Learning , ICML 2018 , Stockholmsm¨assan , Stockholm , Sweden , July 10 - 15 , 2018 , pp . 1153 – 1162 , 2018 . Dean , J . , Corrado , G . , Monga , R . , Chen , K . , Devin , M . , Mao , M . , Senior , A . , Tucker , P . , Yang , K . , Le , Q . V . , et al . Large scale distributed deep networks . In NIPS , pp . 1223 – 1231 , 2012 . El - Mhamdi , E . - M . , Guerraoui , R . , and Rouault , S . The hid - den vulnerability of distributed learning in byzantium . In Proceedings of the 35th International Conference on Ma - chine Learning , ICML 2018 , Stockholmsm¨assan , Stock - holm , Sweden , July 10 - 15 , 2018 , pp . 3518 – 3527 , 2018 . El - Mhamdi , E . - M . , Guerraoui , R . , Guirguis , A . , and Rouault , S . Sgd : Decentralized Byzantine resilience . arXiv preprint arXiv : 1905 . 03853 , 2019 . Goh , G . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . Konecn´y , J . , McMahan , B . , and Ramage , D . Federated opti - mization : Distributed optimization beyond the datacenter . CoRR , abs / 1511 . 03575 , 2015 . Lamport , L . , Shostak , R . E . , and Pease , M . C . The byzantine generals problem . ACM Trans . Program . Lang . Syst . , 4 ( 3 ) : 382 – 401 , 1982 . doi : 10 . 1145 / 357172 . 357176 . Li , M . , Andersen , D . G . , Park , J . W . , Smola , A . J . , Ahmed , A . , Josifovski , V . , Long , J . , Shekita , E . J . , and Su , B . Scaling distributed machine learning with the parameter server . In 11th USENIX Symposium on Operating Systems Design and Implementation , OSDI ’14 , Broomﬁeld , CO , USA , October 6 - 8 , 2014 , pp . 583 – 598 , 2014 . Lian , X . , Huang , Y . , Li , Y . , and Liu , J . Asynchronous parallel stochastic gradient for nonconvex optimization . In NIPS , pp . 2737 – 2745 , 2015 . Lin , Y . , Han , S . , Mao , H . , Wang , Y . , and Dally , B . Deep gra - dient compression : Reducing the communication band - width for distributed training . In International Confer - ence on Learning Representations , 2018 . URL https : / / openreview . net / forum ? id = SkhQHMW0W . Liu , K . Train cifar - 10 with pytorch , 2019 . URL https : / / github . com / kuangliu / pytorch - cifar / blob / ab908327d44bf9b1d22cd333a4466e85083d3f21 / main . py # L33 . Distributed Momentum for Byzantine - resilient Learning Mu˜noz - Gonz´alez , L . , Co , K . T . , and Lupu , E . C . Byzantine - robust federated machine learning through adaptive model averaging . arXiv preprint arXiv : 1909 . 05125 , 2019 . Rajput , S . , Wang , H . , Charles , Z . , and Papailiopoulos , D . Detox : A redundancy - based framework for faster and more robust gradient aggregation . Neural Information Processing Systems , 2019 . Rumelhart , D . E . , Hinton , G . E . , and Williams , R . J . Learn - ing representations by back - propagating errors . Nature , 323 ( 6088 ) : 533 – 536 , Oct 1986 . doi : 10 . 1038 / 323533a0 . Schneider , F . B . Implementing fault - tolerant services using the state machine approach : A tutorial . ACM Computing Surveys ( CSUR ) , 22 ( 4 ) : 299 – 319 , 1990 . Sun , Z . , Kairouz , P . , Suresh , A . T . , and McMahan , H . B . Can you really backdoor federated learning ? CoRR , abs / 1911 . 07963 , 2019 . TianXiang , W . , ZHENG , Z . , ChangBing , T . , and Hao , P . Aggregation rules based on stochastic gradient descent in byzantine consensus . In 2019 IEEE 8th Joint Interna - tional Information Technology and Artiﬁcial Intelligence Conference ( ITAIC ) , pp . 317 – 324 . IEEE , 2019 . Xie , C . , Koyejo , O . , and Gupta , I . Generalized Byzantine - tolerant sgd . arXiv preprint arXiv : 1802 . 10116 , 2018a . Xie , C . , Koyejo , O . , and Gupta , I . Phocas : dimensional byzantine - resilient stochastic gradient descent . arXiv preprint arXiv : 1805 . 09682 , 2018b . Xie , C . , Koyejo , O . , and Gupta , I . Fall of empires : Breaking byzantine - tolerant SGD by inner product manipulation . In Proceedings of the Thirty - Fifth Conference on Uncer - tainty in Artiﬁcial Intelligence , UAI 2019 , Tel Aviv , Israel , July 22 - 25 , 2019 , pp . 83 , 2019 . Xie , C . , Huang , K . , Chen , P . - Y . , and Li , B . { DBA } : Dis - tributed backdoor attacks against federated learning . In International Conference on Learning Representations , 2020 . URL https : / / openreview . net / forum ? id = rkgyS0VFvr . Yang , Z . and Bajwa , W . U . Bridge : Byzantine - resilient decentralized gradient descent . arXiv preprint arXiv : 1908 . 08098 , 2019a . Yang , Z . and Bajwa , W . U . Byrdie : Byzantine - resilient distributed coordinate descent for decentralized learning . IEEE Transactions on Signal and Information Processing over Networks , 2019b . Yang , Z . , Gang , A . , and Bajwa , W . U . Adversary - resilient inference and machine learning : From distributed to de - centralized . arXiv preprint arXiv : 1908 . 08649 , 2019 . Yin , D . , Chen , Y . , Ramchandran , K . , and Bartlett , P . L . Byzantine - robust distributed learning : Towards optimal statistical rates . In Proceedings of the 35th International Conference on Machine Learning , ICML 2018 , Stock - holmsm¨assan , Stockholm , Sweden , July 10 - 15 , 2018 , pp . 5636 – 5645 , 2018 . Zhang , R . , Zheng , S . , and Kwok , J . T . Asynchronous dis - tributed semi - stochastic gradient optimization . In AAAI , pp . 2323 – 2329 , 2016 . Distributed Momentum for Byzantine - resilient Learning A . Reproducing the results The codebase is available at https : / / github . com / LPD - EPFL / ByzantineMomentum . A . 1 . Dependencies Software dependencies . Python 3 . 7 . 3 has been used to run our scripts . Besides the standard libraries associated with Python 3 . 7 . 3 , our scripts also depend on : Library Version numpy 1 . 17 . 2 torch 1 . 2 . 0 torchvision 0 . 4 . 0 pandas 0 . 25 . 1 matplotlib 3 . 0 . 2 tqdm 4 . 40 . 2 PIL 6 . 1 . 0 Library Version six 1 . 12 . 0 pytz 2019 . 3 dateutil 2 . 7 . 3 pyparsing 2 . 2 . 0 cycler 0 . 10 . 0 kiwisolver 1 . 0 . 1 cfﬁ 1 . 13 . 2 We list below the OS on which our scripts have been tested : • Debian 10 ( GNU / Linux 4 . 19 . 0 - 6 x86 64 ) • Ubuntu 18 . 04 . 3 LTS ( GNU / Linux 4 . 15 . 0 - 58 x86 64 ) Hardware dependencies . Although our experiments are time - agnostic , we list below the hardware components used : • 1 Intel ( R ) Core ( TM ) i7 - 8700K CPU @ 3 . 70GHz • 2 Nvidia GeForce GTX 1080 Ti • 64 GB of RAM A . 2 . Command Our results , i . e . the experiments and graphs , are reproducible in one command . In the root directory , please run : $ python3 reproduce . py On our hardware , reproducing the results takes ∼ 24 hours . B . Experimental results For every pair model - dataset , the following parameters vary : • Which attack : ( Baruch et al . , 2019 ) or ( Xie et al . , 2019 ) • Which defense : Krum , Median or Bulyan • How many Byzantine workers ( an half or a quarter ) • Where momentum is computed ( server or workers ) • Which learning rate is used ( larger or smaller ) Every possible combination is tested 7 , leading to a total of 88 different experiment setups . Each setup is tested 5 times , each run with a ﬁxed seed from 1 to 5 , enabling verbatim reproduction of our results 8 . We then report the average and standard deviation for two metrics : top - 1 cross - accuracy and variance - norm ratio over the training steps . The results regarding the cross - accuracy are layed out by “blocks” of 4 experiment setups presenting the same model , dataset , number of Byzantine workers and attack . These results are presented from ﬁgures 6 to 13 . In each “block” , the 2 top experiments use the larger learning rate and the 2 bottom ones the smaller , so looking below correspond to looking to the same experiment but with a smaller learning rate ( and vice versa ) . Similarly , the 2 left experiments use momentum at the server , while the 2 right ones use momen - tum at the workers , which allows for handy comparison of the effect of using one technique over the other . The results regarding the variance - norm ratio are also layed out by “blocks” of 4 experiment setups presenting the same model , dataset , number of Byzantine workers and defense . These results are presented from ﬁgures 14 to 23 . In each “block” , the attack from ( Baruch et al . , 2019 ) is use on the left column and ( Xie et al . , 2019 ) on the right . As for the cross - accuracy , the top row use the larger learning rate and the bottom row shows the effect of using a smaller one . For ﬁgures 6 to 23 , the captions present to the reader the hyperparameters used in each of the experiments , along with comments about the observed behaviors . 7 Along with baselines using averaging without attack . 8 Despite our best efforts , there may still exist minor sources of non - determinism , like race - conditions in the evaluation of certain functions ( e . g . , parallel additions ) in a GPU . Nevertheless we believe these should not affect the results in any signiﬁcant way . Distributed Momentum for Byzantine - resilient Learning ( a ) Momentum at the server , η t = 0 . 5 ( b ) Momentum at the workers , η t = 0 . 5 ( c ) Momentum at the server , η t = 0 . 02 ( d ) Momentum at the workers , η t = 0 . 02 Figure 6 . MNIST using n = 51 workers , including f = 24 Byzantine workers implementing ( Baruch et al . , 2019 ) . This is the maximum number of Byzantine workers Krum can support . No matter where the momentum is computed , reducing the learning rate decreases the effect of the attack against all the GARs . This is not observed , in the same setting , when ( Xie et al . , 2019 ) is used instead ( Figure 8 ) . No matter the learning rate , using momentum at the workers always leads in these settings to an increase of the ﬁnal accuracy ( + 5 % ) . ( a ) Momentum at the server , η t = 0 . 5 ( b ) Momentum at the workers , η t = 0 . 5 ( c ) Momentum at the server , η t = 0 . 02 ( d ) Momentum at the workers , η t = 0 . 02 Figure 7 . MNIST using n = 51 workers , including f = 12 Byzantine workers implementing ( Baruch et al . , 2019 ) . This is the maximum number of Byzantine workers Bulyan can support . With momentum at the server and the largest learning rate , the impact of the attack remains unchanged compared to Figure 6 and despite the reduced number of Byzantine workers . Bulyan , which combines Krum and Median , achieves the same performance as both Krum and Median . When momentum is computed at the workers , Bulyan achieves in these settings better resilience than its parts Krum and Median , and the attack has no tangible effect anymore . Distributed Momentum for Byzantine - resilient Learning ( a ) Momentum at the server , η t = 0 . 5 ( b ) Momentum at the workers , η t = 0 . 5 ( c ) Momentum at the server , η t = 0 . 02 ( d ) Momentum at the workers , η t = 0 . 02 Figure 8 . MNIST using n = 51 workers , including f = 24 Byzantine workers implementing ( Xie et al . , 2019 ) . This is the maximum number of Byzantine workers Krum can support . Contrary to Figure 6 , this attack has very different impacts on the training depending on the Byzantine - resilient GAR used . With Krum and momentum at the server , the model parameters quickly ( after 30 steps ) reach a point where the output class becomes independent from the input ; the model is driven useless . Momentum at the workers with Krum only avoids obtaining such a model . Median shows substantially more resilience than Krum in this setup , and when momentum is computed at the workers , the maximum cross - accuracy is consistently increased , between 15 % to 25 % additional points . ( a ) Momentum at the server , η t = 0 . 5 ( b ) Momentum at the workers , η t = 0 . 5 ( c ) Momentum at the server , η t = 0 . 02 ( d ) Momentum at the workers , η t = 0 . 02 Figure 9 . MNIST using n = 51 workers , including f = 12 Byzantine workers implementing ( Xie et al . , 2019 ) . This is the maximum number of Byzantine workers Bulyan can support . With a quarter of Byzantine workers , the attack does not have any tangible impact on Krum ( and thus none on Bulyan either ) for this model and dataset anymore . Using momentum at the workers further reduces the impact on Median , to the point of ﬁltering out the adversarial effect of this attack . Distributed Momentum for Byzantine - resilient Learning ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Momentum at the server , η t = 0 . 001 ( d ) Momentum at the workers , η t = 0 . 001 Figure 10 . CIFAR - 10 using n = 25 workers , including f = 11 Byzantine workers implementing ( Baruch et al . , 2019 ) . This is the maximum number of Byzantine workers Krum can support . Compared to Figure 6 , the impact of the attack is substantial . Even with a reduced learning rate , the model maximum cross - accuracy barely reaches 20 % . Using momentum at the workers , while having virtually no computational cost , positively impacts the performance of the model . ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Momentum at the server , η t = 0 . 001 ( d ) Momentum at the workers , η t = 0 . 001 Figure 11 . CIFAR - 10 using n = 25 workers , including f = 5 Byzantine workers implementing ( Baruch et al . , 2019 ) . This is the maximum number of Byzantine workers Bulyan can support . The effect of going to only a quarter of Byzantine workers , compared to Figure 10 , did not made the attack less effective . Conversely , momentum at the workers leads to a conspicuous improvement of the model performance . Distributed Momentum for Byzantine - resilient Learning ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Momentum at the server , η t = 0 . 001 ( d ) Momentum at the workers , η t = 0 . 001 Figure 12 . CIFAR - 10 using n = 25 workers , including f = 11 Byzantine workers implementing ( Xie et al . , 2019 ) . This is the maximum number of Byzantine workers Krum can support . As in Figure 8 , the attack is extremely effective on Krum and slightly less on Median . Momentum at the workers has a substantial positive effect when Median is used . ( a ) Momentum at the server , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Momentum at the workers , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Momentum at the server , η t = 0 . 001 ( d ) Momentum at the workers , η t = 0 . 001 Figure 13 . CIFAR - 10 using n = 25 workers , including f = 5 Byzantine workers implementing ( Xie et al . , 2019 ) . This is the maximum number of Byzantine workers Bulyan can support . With a reduced fraction of Byzantine workers to a quarter compared to Figure 12 , the effect of the attack is almost void . Notably in this setting , Bulyan improves the cross - accuracy gain per step over Krum and Median . Distributed Momentum for Byzantine - resilient Learning ( a ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 5 ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 5 ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 02 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 02 Figure 14 . MNIST using n = 51 workers , including f = 24 Byzantine workers defended against by Krum . This setting contains the full range of behaviors one can observe in the subsequent ﬁgures . One ﬁrst , notable behavior was predicted by the theory : until convergence is reached , reducing the learning rate decreases the variance - norm ratio . The second , recurrent behavior is that , when the model is driven useless ( Figure 8 ) , the ratio reaches low values . Such decreasing curves ( 14b and 14d ) are empirical , distinctive signal of a very successful attack . Indeed for a successful defense , one should expect the ratio to grow to inﬁnity , as the norm of the honest gradient goes toward 0 . ( a ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 5 ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 5 ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 02 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 02 Figure 15 . MNIST using n = 51 workers , including f = 12 Byzantine workers defended against by Krum . See Figure 14 . Distributed Momentum for Byzantine - resilient Learning ( a ) Attack ( Baruch et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 001 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 16 . CIFAR - 10 using n = 25 workers , including f = 11 Byzantine workers defended against by Krum . See Figure 14 . ( a ) Attack ( Baruch et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 001 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 17 . CIFAR - 10 using n = 25 workers , including f = 5 Byzantine workers defended against by Krum . The “fracture” is due to the fact that the learning rate is decreased at step 1500 . See Figure 14 . Distributed Momentum for Byzantine - resilient Learning ( a ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 5 ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 5 ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 02 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 02 Figure 18 . MNIST using n = 51 workers , including f = 24 Byzantine workers defended against by Median . See Figure 14 . ( a ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 5 ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 5 ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 02 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 02 Figure 19 . MNIST using n = 51 workers , including f = 12 Byzantine workers defended against by Median . See Figure 14 . Distributed Momentum for Byzantine - resilient Learning ( a ) Attack ( Baruch et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 001 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 20 . CIFAR - 10 using n = 25 workers , including f = 11 Byzantine workers defended against by Median . See Figure 14 . ( a ) Attack ( Baruch et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 001 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 21 . CIFAR - 10 using n = 25 workers , including f = 5 Byzantine workers defended against by Median . See Figure 14 . Distributed Momentum for Byzantine - resilient Learning ( a ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 5 ( b ) Attack ( Xie et al . , 2019 ) , η t = 0 . 5 ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 02 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 02 Figure 22 . MNIST using n = 51 workers , including f = 12 Byzantine workers defended against by Bulyan . See Figure 14 . ( a ) Attack ( Baruch et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( b ) Attack ( Xie et al . , 2019 ) , η t = (cid:40) 0 . 01 if t < 1500 0 . 001 otherwise ( c ) Attack ( Baruch et al . , 2019 ) , η t = 0 . 001 ( d ) Attack ( Xie et al . , 2019 ) , η t = 0 . 001 Figure 23 . CIFAR - 10 using n = 25 workers , including f = 5 Byzantine workers defended against by Bulyan . See Figure 14 .