Information Systems Research Vol . 15 , No . 3 , September 2004 , pp . 216 – 235 issn 1047 - 7047 (cid:1) eissn 1526 - 5536 (cid:1) 04 (cid:1) 1503 (cid:1) 0216 inf orms ® doi 10 . 1287 / isre . 1040 . 0026 ©2004 INFORMS DSS Effectiveness in Marketing Resource Allocation Decisions : Reality vs . Perception Gary L . Lilien , Arvind Rangaswamy The Smeal College of Business , Pennsylvania State University , University Park , Pennsylvania 16802 { glilien @ psu . edu , arvindr @ psu . edu } Gerrit H . Van Bruggen Rotterdam School of Management , Erasmus University Rotterdam , P . O . Box 1738 , 3000 DR Rotterdam , The Netherlands Katrin Starke The Smeal College of Business , Pennsylvania State University , University Park , Pennsylvania 16802 , wazblue @ hotmail . com W e study the process by which model - based decision support systems ( DSSs ) inﬂuence managerial decision making in the context of marketing budgeting and resource allocation . We focus on identifying whether and how DSSs inﬂuence the decision process ( e . g . , cognitive effort deployed , discussion quality , and decision alternatives considered ) and , as a result , how these DSSs inﬂuence decision outcomes ( e . g . , proﬁt and satisfaction both with the decision process and the outcome ) . We study two speciﬁc marketing resource allocation decisions in a laboratory context : sales effort allocation and customer targeting . We ﬁnd that decision makers who use high - quality , model - based DSSs make objectively better decisions than do decision makers who only have access to a generic decision tool ( Microsoft Excel ) . However , their subjective evaluations ( perceptions ) of both their decisions and the processes that lead to those decisions do not necessarily improve as a result of DSS use . And expert judges , serving as surrogates for top management , have a difﬁcult time assessing the objective quality of those decisions . Our results suggest that what managers get from a high - quality DSS may be substantially better than what they see . To increase the inclination for managerial adoption and use of DSS , we must get users to “see” the beneﬁts of using a DSS . Our results also suggest two ways to bridge the perception - reality gap : ( 1 ) improve the perceived value of the decision process by designing DSSs both to encourage discussion ( e . g . , by providing explanation and support for alternative recommendations ) as well as to reduce the perceived complexity of the problem so that managers invest more cognitive effort in exploring additional options and ( 2 ) provide feedback on the likely market / business outcomes of various decision options . Key words : DSS ; marketing models ; decision quality ; decision process ; resource allocation History : Peter Todd , Associate Editor . This paper was received on February 5 , 2003 , and was with the authors 9 months for 2 revisions . Introduction The determination and allocation of a budget ( of time or resources , ﬁnancial or otherwise ) is a pervasive human activity . For example , we must all determine our budget for food , necessities , and leisure activ - ities and allocate those budgets within those cate - gories . We must also determine how much of our time we will work each week and how much of our remaining time we will spend with our children , surf - ing the Internet , watching television , and the like . Firms continually face such resource allocation chal - lenges . They must determine how much to spend on new product development and how to allocate those funds across projects and time . Charitable organiza - tions must determine what their development bud - get should be and what past donors or prospects to target . Manufacturers must decide how much plant capacity to invest in and where that capacity should be placed . The determination of the budget and the alloca - tion of that budget are tasks that are straightforward to deﬁne conceptually and mathematically , but not at all that simple for humans to perform “optimally” without some decision aid . Indeed , such decisions 216 Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 217 helped form Simon’s ( 1955 ) view of satisﬁcing behav - ior , where he states that “there is a complete lack of evidence that in actual human choice situations these computations can be or are in fact performed” ( p . 105 ) . It is perhaps not surprising , therefore , that a search on Google on June 14 , 2004 , for “resource allocation” and “software” turned up nearly 390 , 000 links . While it would seem then , that in an area of such importance , we would have substantial and deﬁnitive evidence about the beneﬁts and costs of using decision support aids for resource allocation in various application domains , such is not the case . For example , Agarwal et al . ( 1992 ) describe critical ele - ments missing in systems to help support the choice of management information system ( MIS ) projects under resource constraints , a resource allocation task ; Muckstadt et al . ( 2001 ) describe at least ﬁve key ele - ments that they claim are missing in the design of systems to support resource allocation decisions in a supply chain . Indeed , there appears to be only mod - est evidence to support the belief that model - based DSSs can help improve business decisions of any sort ( Sharda et al . 1988 , Benbasat and Nault 1990 , Todd and Benbasat 1999 ) . We focus here on one domain for speciﬁcity : orga - nizational resource allocation decisions in marketing : how large the marketing budget should be ( e . g . , for advertising , sales promotion , and sales force effort ) and how that budget should be allocated over geogra - phies , products , market segments , and time . And while there is some evidence of the effectiveness of model - based systems to support such decisions , the adoption rate of such systems by ﬁrms remains far below potential ( Wierenga and Van Bruggen 2000 ) . A study by Accenture ( 2001 ) points out that more than two - thirds of the more than $ 1 trillion spent by the Global 1000 on marketing is allocated without any return on investment ( ROI ) justiﬁcation , much less supported by a DSS . Is the apparent low level of adoption of decision support models in marketing because of their inher - ent lack of value , because their value ( perceived or actual ) is not sufﬁciently high for the adopting orga - nization to incur the costs that the adopting indi - viduals may be forced to bear , or because of some combination of these factors ? We study these issues by exploring how DSSs inﬂuence the decision process ( e . g . , cognitive effort deployed , discussion quality , and decision alternatives considered ) and , as a result , how these DSSs inﬂuence decision outcomes ( e . g . , proﬁt and satisfaction both with the process and the outcome ) . We study two speciﬁc marketing resource allocation decisions , i . e . , sales effort allocation , and customer targeting . We deﬁne a DSS as a packaged software application that uses analytical models to transform business data into numerical and graphical reports to help users make business decisions more easily and effectively . In our conceptualization of DSSs the presence of built - in analytical decision models is essential , distin - guishing a DSS from a more general - purpose tool like Excel . Also , DSSs for resource allocation differ on ana - lytical model sophistication , ranging from relatively simple descriptive response models to sophisticated normative optimization models providing problem - speciﬁc recommendations . In this study , we investi - gate the effects of two quite sophisticated DSSs for resource allocation . There have been several studies on the effects and effectiveness of marketing DSSs , including DSSs designed for resource allocation . 1 Most have focused primarily on exploring whether the use of a DSS improves the performance of decision makers as mea - sured by decision quality ( typically based on outcome variables such as sales , proﬁt , or market share com - puted endogenously from the model ) or by decision makers’ satisfaction and conﬁdence in the results of using the DSS . Only a few studies have examined how a DSS affects the decision process , and the few that have , have not investigated how the DSS inﬂu - ences both the process and the outcomes . The studies report mixed results regarding DSS effects on outcomes . Most studies in the marketing lit - erature report that DSSs improve marketing resource allocation decisions , with the notable exception of the study by Chakravarti et al . ( 1979 ) , which concluded that the use of a DSS had a detrimental effect on decision quality . However , the broader DSS research reports mixed ﬁndings in laboratory studies on the effects of DSSs on decision outcomes ( see Sharda et al . 1 See , e . g . , Fudge and Lodish 1977 ; Chakravarti et al . 1979 ; McIntyre 1982 ; Lodish et al . 1988 ; Gensch et al . 1990 ; Hoch and Schkade 1996 ; Van Bruggen et al . 1996 , 1998 ; Eisenstein and Lodish ( 2002 ) provide a review . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 218 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 1988 , Benbasat and Nault 1990 ) . Of the 11 studies that Sharda et al . ( 1988 ) reviewed , 6 showed improved performance because of DSS use , 4 showed no differ - ence , and in 1 study performance actually decreased for DSS users . We note three issues with respect to the past studies . ( 1 ) Most studies have not tracked the decision pro - cesses associated with DSS use . ( 2 ) It is possible that DSS can improve objective decision outcomes without having a positive effect on the subjective evaluations of these decisions and vice versa , and it would be useful to understand the sep - arate nature of these two effects . ( 3 ) Field studies have used DSSs to address real managerial problems , but have lacked effective exper - imental control ( e . g . , Fudge and Lodish 1977 ) , making it difﬁcult to demarcate the drivers of DSS perfor - mance , while lab studies have imposed sound experi - mental controls , but have addressed relatively simple and contrived problems . We designed our research to balance the beneﬁts of experimental control ( internal validity ) with those of real - world applicability ( external validity ) by con - ducting a laboratory study where we used ﬁeld - tested DSSs and real - world cases , for which actual outcomes are both known and have been reported in the aca - demic literature . In view of the limitations of past studies , we designed our research to incorporate the following three features . ( 1 ) Broad assessment of DSS impact . We incorpo - rate objective measures , exogenous expert judgments , and subjective perceptions of DSS impact , including both multiple dependent ( outcome ) and mediating ( process ) variables . We evaluate whether a DSS inﬂu - ences outcomes , and also study how that inﬂuence is moderated by changes in the decision process . ( 2 ) Two different DSSs . We study the effects of two DSSs that have different foci , although both address resource allocation problems : customer targeting for ABB Electric and sales force allocation for Syntex Labs ( described in subsequent sections ) . Having two dif - ferent DSSs should allow us to more readily iden - tify results / relationships that are both common and unique across these two types of resource allocation models . ( 3 ) Realistic study context . Unlike most previous studies , we do not compare a DSS versus a non - DSS treatment , an unrealistic comparison . Instead , subjects in our non - DSS condition ( which we will refer to as the Excel - only condition throughout for clarity ) have access to and can manipulate ( through Excel ) the same data as the DSS subjects , as is the case in natural settings . Excel is a DSS generator that enables our subjects to do estimation and optimization tasks , both of which are useful for addressing the resource allocation problems faced by our subjects . However , Excel does not automatically enable people to develop problem representations to fully exploit the avail - able information . Our DSSs , unlike a more general - purpose tool like Excel , build in the DSS developers’ expertise in the design , potentially helping users to both better understand how to represent the prob - lem as well as how to develop speciﬁc , defensible recommendations . The theoretical underpinnings of our study rest on the cost - beneﬁt framework of cognition ( Payne et al . 1993 ) , applied to the ﬁeld of decision support by Todd and Benbasat ( 1999 ) , and on the ﬁt - appropriation model ( FAM ) developed to understand the effects of group support systems ( Dennis et al . 2001 ) . From these theo - ries , we postulate that DSSs that have task - technology ﬁt ( i . e . , are of high quality ) can improve decision quality / accuracy and / or improve decision outcomes . However , these theories also suggest that decision makers prefer less effort to more effort . Blending these theoretical perspectives , we develop hypotheses about the kinds of effects the DSSs will have on mar - keting resource allocation decisions . Our results show that decision makers with access to high - quality , model - based DSSs make objectively better decisions than do those with access only to Excel . However , subjective evaluations of both the decisions made and the decision processes that lead to those decisions , do not necessarily improve as a result of DSS use . In particular , even expert judges have difﬁculty assessing the objective quality of those decisions . We also ﬁnd that DSSs can lead to better objective decision outcomes without seeming to affect some aspects of the decision process , such as the num - ber of decision alternatives considered . However , to improve subjective evaluations of decision quality , a necessary precondition for DSS adoption , the decision process must be inﬂuenced in such a way that it is viewed favorably . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 219 The Impact of Model - Based DSS : Hypothesis Development To assess whether and how a DSS leads to different decision outcomes , we must distinguish the effects of a DSS on various impact measures and separate the effects of a DSS on the decision process from its effects on decision outcomes . DeLone and McLean ( 1992 ) , seeking a measure of general information sys - tems ( IS ) impact or success , report “rather than ﬁnd - ing none , there are nearly as many measures as there are studies” ( p . 61 ) . They identify several categories of IS impact and make an important distinction between the effect of the IS on the individual and the effect on the organization . The individual - level impact of an IS is closely related to the user’s perceived perfor - mance given IS use , but the impact can also involve providing a user with a better understanding of the decision problem or changing the user’s perception of the usefulness of the IS . The organizational impact of an IS relates to its effect on organizational perfor - mance , which can be measured in terms of ﬁrm proﬁt , sales , or related performance metrics , an area in need of research ( DeLone and McLean 1992 ) . This taxon - omy of IS success measures applies to the DSSs we study : We address both the individual and the orga - nizational impact of DSSs and distinguish two cate - gories of impact variables—DSS impact on decision Figure 1 Research Framework Decision Context Availability of a Model - Based Decision Support System Decision Process • Effort • Discussion Quality • Decision Alternatives Considered Decision Process DSS Impact Decision Outcome Objective Decision Outcomes • Incremental Return / Profit • Expert Evaluation Subjective Evaluation of • Satisfaction with Decision Decision Outcome Subjective Evaluation of the • Complexity • Learning • Perceived Userfulness of DSS Decision Process process and their impact on decision outcomes . Within both impact categories , we distinguish between objec - tive outcomes and subjective evaluations . Combining these two dimensions , we study the impact of DSSs on four groups of variables : ( 1 ) the characteristics of the team decision process , ( 2 ) the ( individual ) subjec - tive evaluation of the team decision process , ( 3 ) objec - tive decision outcomes for the team , and ( 4 ) the ( individual ) subjective evaluation of the decision out - comes ( see Figure 1 ) . We investigate the impact of DSS on speciﬁc measures within each of these four classes of variables . Decisions emerge from an underlying decision pro - cess , which can be characterized by the amount of cognitive effort that people devote to problem solv - ing , the quality of the discussions they have during the decision process , the decision alternatives they consider , and so on . Both the decision outcomes and the decision process , in turn , will be inﬂuenced by the context in which decision makers operate . This context can be described by the characteristics of the decision environment , the characteristics of the decision mak - ers who must resolve problems , and the characteristics of the available DSSs , especially how appropriate they are for the tasks faced by the decision makers . A priori , we can expect decision models to have a positive effect on decision outcomes for several Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 220 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS reasons . Decision makers have cognitive limitations in acquiring and processing information ( Tversky and Kahneman 1974 , Hogarth and Makridakis 1981 , Bazerman 1998 ) . When confronted with large amounts of information in short timeframes , they use heuristic approaches to solve problems , which trigger various cognitive biases that could diminish decision quality . An example heuristic is anchoring and adjustment . Decision makers who apply this heuristic start from an initial “anchor” point and adjust it to arrive at a decision . When there are strong anchors , adjustments from an anchor point tend to be sub - optimal ( Slovic and Lichtenstein 1971 , Mowen and Gaeth 1992 ) . Many techniques have been proposed in the literature to “debias” the decision - making pro - cess , including task simpliﬁcation , providing training and feedback to decision makers , and providing sim - ulators to generate multiple alternative explanations ( Croskerry 2003 ) . A DSS can potentially be a debias - ing tool to reduce several types of bias ( Arnott 2002 ) . In resource allocation tasks , DSSs can help man - agers cope with large amounts of information and integrate that information in a consistent way ( Dawes 1979 ) . In particular , a DSS can help managers choose good resource allocation strategies by consistently weighting the available options according to speciﬁed criteria , whereas humans tend to alter the weights they assign to different variables by using heuristics . At the same time , a DSS can underweight important idiosyncratic elements ( e . g . , the strategic desirability of an option ) relevant to a particular resource allo - cation problem . Given these advantages and limita - tions of DSSs , it is perhaps not surprising that several researchers have demonstrated that a combination of a DSS and human decision making outperforms unaided decision makers ( Blattberg and Hoch 1990 , Hoch 1994 , Hoch and Schkade 1996 ) . The main expla - nation for this ﬁnding is that DSSs cause changes in the ( imperfect ) processes by which decisions are made ( Silver 1990 ) . Thus , good decision support technolo - gies should be designed to provide decision makers with capabilities needed to extend their bounds of rationality ( Todd and Benbasat 1999 ) . However , the mere availability or use of DSSs will not automatically lead to better decisions because decision makers make effort - accuracy trade - offs ( Payne et al . 1993 ) in their decision processes , and these trade - offs affect the quality of the decision outcomes . The literature suggests that effort is the most important factor inﬂuencing strategy selection ( Todd and Benbasat 1999 ) . If a DSS enables a higher quality decision process with no more effort than the current process , that higher quality DSS process is more likely to be adopted . However , if a DSS allows the decision maker’s existing decision process to be executed with less effort , then the decision quality may not improve . It is generally easier for decision makers to assess efﬁciency gains ( e . g . , savings in cog - nitive effort ) from DSS use than it is for them to assess decision - quality improvements , especially if they are inexperienced and unfamiliar with the DSS . Only if a DSS is intrinsically of high quality and makes it easy to deploy higher quality decision processes will DSS use improve both objective decision outcomes and decision efﬁciency . However , if users do not rec - ognize the intrinsic quality of the DSS or the value of the outcomes it helps generate , they may not be satisﬁed . The ﬁt - appropriation model ( FAM ) ( Dennis et al . 2001 ) proposes that the effects of ( group ) DSSs are inﬂuenced by two factors . The ﬁrst is the ﬁt between the task and the DSS , i . e . , the task - technology ﬁt . The second is the appropriation support the group mem - bers receive in the form of training , facilitating , rou - tinizing , or software restrictions to help them incorpo - rate the system effectively into their decision - making process . FAM proposes that task - technology ﬁt is a necessary , but not sufﬁcient , condition to improve decision performance . Without proper appropriation support , performance is less likely to improve signif - icantly even when task - technology ﬁt is high . That is , the effect of task - technology ﬁt on performance will be moderated by appropriation support . Appropria - tion itself , in turn , is affected by the ﬁt ( a good ﬁt is more likely to lead to faithful appropriation ) . Empiri - cal results show that even without appropriation sup - port performance may still be inﬂuenced positively , whereas the subjective evaluations ( e . g . , satisfaction with the decision ) may not be ( positively ) inﬂuenced ( Dennis et al . 2001 ) . Hence , the FAM model suggests that DSSs can be expected to improve objective decision outcomes if they show a sufﬁciently high level of task - technology ﬁt . Given decision makers’ natural tendency to prefer Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 221 effort reduction , and the fact that merely following the recommendation of a high - quality DSS offers both low effort and high decision quality , we can expect high - quality DSSs to improve objective out - comes ( incremental return / proﬁt ) . For our study , we selected DSSs that have high task - technology ﬁt , a nec - essary condition for improving objective decision per - formance . We also chose the level of appropriation support to reﬂect the conditions under which resource allocation software is typically used by marketing managers , as few companies today maintain a large analytic staff to provide extensive support for man - agers . Rather they operate in environments character - ized by moderate appropriation support for software ( e . g . , telephone / web support , remote diagnostics , and so on ) . We investigate the effects of the availability of model - based DSSs relative to the use of the general purpose decision tool , Microsoft Excel . Excel is a DSS generator that enables estimation and optimiza - tion , both of which are useful for the tasks faced by our subjects . However , Excel does not help users with problem representation ( i . e . , what to estimate and how , or what to optimize and how ) . Without the appropriate problem representation , the available data may not be exploited to the fullest . In contrast , our DSSs provide a sort of blueprint , embedding an analytical process for problem formulation and representation that allows the users to more fully exploit the available data . In a sense , Excel is just a toolkit whereas our DSSs are toolkits that come with a blueprint , permitting users to exploit the toolkit most effectively . Thus , although Excel is less restrictive than our DSSs , it offers no guidance to fully exploit its capabilities in a speciﬁc problem context . Therefore , the Excel - only aid has a lower task - technology ﬁt than the two DSSs used in the study . The two DSSs in our study are similar to each other in that they both support resource allocation deci - sions , and both show a high level of task - technology ﬁt and moderate appropriation support . However , they differ in important ways : the DSS for ABB is nondirective ( i . e . , it gives no feedback , nor does it generate speciﬁc recommendations ) whereas the Syntex DSS provides both a speciﬁc recommendation and a projected proﬁt impact of that recommendation , relative to the current allocation . Goodman ( 1998 ) and Wigton et al . ( 1986 ) show that such feedback can play both an informational role ( promoting knowledge acquisition ) as well as a motivational role ( providing a reward cue for increasing cognitive effort investment ) . In the framework of Balzer et al . ( 1989 ) , user inter - actions with the Syntex DSS ( but not with the ABB DSS ) provide “cognitive feedback , ” linking the task with the environmental performance measures . Note that the Syntex DSS allows users to conduct “what - if” analyses , experimenting with different constraints and observing their impact on expected proﬁts . The ABB DSS only offers users additional information in terms of computed probabilities , but does not include options to explicitly encourage users to experiment with or analyze multiple scenarios , as was the case with the Syntex DSS . Even though both DSS have moderate levels of appropriation support ( e . g . , no direct training ) , Syntex provides more appropriation support than ABB , and could lead to higher effort deployment , which in turn could lead to better out - comes . However , we do not postulate these differ - ences as formal hypotheses . Based on the above reasoning , we propose the fol - lowing hypotheses about the impact that the use of high - quality DSS will have on various measures of performance . 2 Hypothesis 1 . Model - based DSSs will improve objec - tive decision outcomes relative to the Excel - only tool . Hypothesis 1a . DSSs will generate more incremen - tal returns / proﬁts relative to the Excel - only tool . Hypothesis 1b . DSSs will result in more favorable experts’ evaluation of the decisions relative to the Excel - only tool . Because we have a moderate level of appropriation support , we posit the following . Hypothesis 2 . DSSs will have no effect on the sub - jective evaluation of the decision outcomes relative to the Excel - only tool . Hypothesis 2a . DSSs will have no differential effects on decision satisfaction relative to the Excel - only tool . 2 In these hypotheses , the term “use of DSSs” implies ( 1 ) high task - technology ﬁt , ( 2 ) a moderate level of appropriation , and ( 3 ) use for resource allocation decisions . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 222 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS With only moderate appropriation support , it would be difﬁcult for decision makers to judge the value of the available DSS , and therefore they would not be able to fully assess how their decision - making efforts improve decision quality . Therefore , decision makers are more likely to reduce effort at the expense of deci - sion accuracy . It is also possible that the availability of a model to facilitate discussions could lead decision makers to ( incorrectly ) pursue quick consensus when , in fact , it may be in their best interest to generate more alternatives , and evaluate them more autonomously ( Miranda and Saunders 1995 ) . In view of the above arguments , we expect less effort , but more focused and higher quality discussions ( because of the high - quality DSS ) , which also means that fewer decision alternatives will be generated or evaluated carefully . We , therefore , hypothesize the following . Hypothesis 3 . DSSs will have mixed effects on the decision process . Hypothesis 3a . DSSs will lead to less effort devoted to problem solving than the Excel - only tool . Hypothesis 3b . DSSs will enhance the quality of the discussions as compared to the Excel - only tool . Hypothesis 3c . DSSs will lead to fewer decision alternative generated than the Excel - only tool . Appropriation support involves training , facilitat - ing , routinizing , or software restrictions to help users incorporate a DSS within their decision - making pro - cess . We can expect moderate levels of appropriation support to lead to less intensive decision processes with mixed effects on the subjective evaluation of these processes . On the one hand , if decision mak - ers spend less effort on the task because of the facil - itation offered by the DSS , they may not discover the full complexity of the task . On the other hand , they may view the decision problem as complex , and because they do not spend much effort devel - oping a full understanding of the problem , the com - plexity remains even after DSS use . Given these two countervailing effects , we hypothesize no signiﬁcant net effect of the DSS on perceived problem complex - ity . And because decision makers may use the DSS for effort reduction , it may reduce the amount of learning that occurs because of DSS use . Finally , the DSS should help decision makers improve along at least one dimension : lower effort or higher decision quality . Therefore , we expect DSSs will be perceived as being useful . Hypothesis 4 . DSSs will have mixed effects on the subjective evaluation of the decision process . Hypothesis 4a . DSSs will have no effect on per - ceived problem complexity relative to the Excel - only tool . Hypothesis 4b . DSSs will lead to less perceived learning relative to the Excel - only tool . Hypothesis 4c . DSSs will be perceived as useful rel - ative to the Excel - only tool . Methodology To test the hypotheses , we applied six criteria to select our methodology . First , we conducted our research in a laboratory setting because we wanted our decision context to be replicable to permit statistical model building and hypothesis testing . Second , our study required a realistic decision context to enhance exter - nal validity . Third , we wanted our hypotheses to be testable across DSS designs . Therefore , we included two realistic resource allocation scenarios , which had DSSs associated with them : the ABB Electric case and the Syntex Labs ( A ) case , both of which received the Edelman Prize from INFORMS as outstanding exam - ples of the practice of management science . Papers describing these models ( Gensch et al . 1990 , Lodish et al . 1988 ) include the actual market response to the resource allocation decisions implemented by the respective ﬁrms . Therefore , it is possible , ex post , to estimate likely decision effectiveness . Fourth , our subjects should be real decision makers or have had sufﬁcient training in the domain to understand the issues associated with resource allocation decisions . Fifth , subjects should have the background and capa - bility to understand and use spreadsheets and mar - ket response models . Sixth , our subjects must not be experts ( e . g . , analysts ) in the use of DSSs , because our hypotheses concern decision making by typical managers . These criteria lead us to consider business school undergraduates , MBAs , and company executives . Pilot tests with undergraduates showed that they did not have sufﬁcient background to understand the problem context . We were not able to locate a large Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 223 enough group of executives who were sufﬁciently homogeneous in background and skill level to meet our needs . Pilot studies with MBA students who had taken core marketing and management science courses showed that such students not only were able to understand both the context ( marketing resource allocation ) and the approach ( spreadsheet tools and response model - based decision support ) , but were also sufﬁciently homogenous along other dimensions to make them appropriate subjects . We adapted software implementations of the ABB and Syntex model from Lilien and Rangaswamy ( 1998 ) . To mimic the organizational reality and group decision process associated with such decisions in practice , we used two - person teams as the exper - imental unit . We randomly assigned each team to one of eight experimental conditions to analyze and develop recommendations for both cases . All groups received identical data ( described in the cases ) in the form of spreadsheets and had the full functionality of Microsoft Excel available to them . The groups differed in ( 1 ) whether their spreadsheet included an embed - ded DSS model that allowed them to analyze the data ( if they chose to ) using a resource allocation model and ( 2 ) the order in which they analyzed the cases— ABB followed by Syntex or Syntex followed by ABB . We brieﬂy describe these two cases and the associated models . ABB Electric Case . The decision problem was to allocate a supplementary marketing budget to the “top 20” customers ( out of 88 ) to be recommended by the subjects . The data reported how each of these customers rated the four suppliers ( including ABB ) on criteria such as invoice price , technical specs of the products , availability of spare parts , and so on . Subjects who had access to the DSS were also able to run a multinomial logit analysis to determine the probability of each customer buying from each of the four suppliers . The subjects could then use the results of the model analysis in any way they thought was appropriate ( e . g . , sort customers according to their probability of purchasing from ABB and construct an index of vulnerability or attractiveness ) in identifying the target customers . To provide a common decision anchor , all subjects were told the company had historically targeted its marketing programs at its largest customers , but that a company consultant had introduced the concept of targeting customers by “switchability . ” The new idea was to target those customers whose likelihood of purchase indicated that they were “sitting on the fence” with respect to purchasing from ABB ( i . e . , where ABB was either a narrow ﬁrst choice or was the second choice by a narrow margin ) , and pay less attention to those customers who were already either loyal to competitors or were loyal to ABB . Switch - ability segmentation conﬂicted with the prior com - pany resource allocation strategy , which was to target purely based on sales potential of the customers , putting more effort on customers with higher sales potential . Figure 2 summarizes the key data as well as the results from running the multinomial logit model ( available to groups that had access to the DSS ) . Syntex Labs ( A ) Case . The Syntex case describes the situation that Syntex Labs faced in 1982 , when it had 430 sales representatives in the United States and were adding 40 reps per year . The company had 7 different products and the stated management plan was to continue adding 40 reps per year , and to allo - cate those reps to those 7 products proportionally to the current allocation of representatives . The company was concerned both about the total size of its sales force and the allocation of the sales effort across prod - ucts , because a relatively new product , Naprosyn , was popular and appeared to be underpromoted relative to the resources allocated to other products . The case describes the concept of a response model and the hiring of a consultant who led a team of Syntex exec - utives through the calibration of that response model . All subjects received data on the current level of effort , the allocation of that sales effort to products , the current sales of these products , the proﬁtability of the products , the current overall proﬁtability of the ﬁrm , and the results of the response model cal - ibration session . This latter information—the answer to questions such as “What would the percentage sales of Naprosyn be with a 50 % increase in its sales force allocation from ( current ) 96 . 8 sales reps to 145 . 2 ? ” Answer : “26 % increase” ( Cell I9 in Fig - ure 3 ) —was provided to the respondents in Excel template format as presented in Cells G9 – J15 in Fig - ure 3 . The DSS - supported group also had access to an optimization model that generates the recommen - dations in Columns D and E in Figure 3 . That model Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 224 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS Figure 2 ABB DSS—Resource Allocation Model , Giving Purchase Likelihood by Brand for Each Potential Customer Note . The model - supported groups could run a multinomial logit ( MNL ) model to obtain the choice probabilities of each supplier for each customer . The Model menu option in the program enables the subjects to access the MNL model and obtain the result above . Figure 3 Syntex DSS Output—Unconstrained Optimization , Showing What the Model Recommends with No Restrictions on the Amount of Selling Effort Note 1 . Base selling effort = current sales force allocation in number of representatives . Base sales = expected sales in 1985 with base selling effect . Unit margin = group proﬁt / unit before allocating sales costs . Base response estimates = % increase / decrease in sales with noted increase / decrease in selling effort ( both relative to “base” ) . Note 2 . The Excel - only group had access only to base selling effort , base sales data and base market response estimates . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 225 allowed subjects to determine the “optimal” ( short - term proﬁt maximizing ) sales force size and effort allocation , either on an unconstrained basis or under user - speciﬁed constraints . Those constraints could be placed either on the total size of the sales force or on individual products . Subjects were given a common decision anchor to allocate any increase in sales effort proportionally to the most recent level of effort allo - cation , a widely used process in practice . The ABB DSS and the Syntex DSS differ both in their designs and in respect to the problem context in which each is used . The ABB DSS does not make speciﬁc recommendations about which customers to target under various user - selected criteria ( the user has to develop those criteria ) , nor does it provide any expected outcomes associated with a resource allo - cation decision . In contrast , the Syntex DSS , through a proﬁt optimization tool , makes speciﬁc recommen - dations for the sizes of the sales force and effort allocation , and also provides the resulting level of expected proﬁt ( computed from sales response func - tions ) . Thus , Syntex is a directive DSS , which pro - vides its users with speciﬁc feedback on the expected sales and proﬁt outcomes of alternative resource allocations . Experimental Procedure . Subjects analyzed both the ABB and the Syntex cases in two - person teams . We systematically manipulated the availability of a DSS as shown below . To address order effects , half the groups did the ABB case ﬁrst followed by the Syntex case , while the other groups did the reverse . Table 1 summarizes the resulting eight experimental conditions . Our experimental procedure consisted of the fol - lowing ﬁve steps . Step 1 . Background and Qualiﬁcations . After entering the lab , each subject completed a pre - experimental Table 1 Experimental Conditions DSS Task order Task order condition ( ABB ﬁrst , Syntex second ) ( Syntex ﬁrst , ABB second ) No DSS Group 1 Group 2 DSS followed by Group 3 Group 4 No DSS No DSS followed Group 5 Group 6 by DSS DSS Group 7 Group 8 questionnaire with questions about demographics ( age , gender , and so on ) , work background , and computer and Excel experience . We used this information to check ( post hoc ) whether the teams in the various experimental conditions had similar back - grounds and qualiﬁcations . Step 2 . Case 1 . Subjects as a group received their ﬁrst case and a tutorial illustrating how the related software worked . The tutorials that were given to sub - jects with the DSS contained additional information about running the DSS . After they had analyzed the case , all groups completed forms summarizing their recommendations and their justiﬁcations for those recommendations . Step 3 . Postanalysis Questionnaire 1 . After complet - ing their recommendation form for the ﬁrst case , all subjects ( individually ) completed a postanalysis ques - tionnaire that asked for their subjective evaluations of their case analysis , the associated discussions , their recommendations , and their assessment of the avail - able software . Step 4 . Case 2 . The same as Step 2 , but now for the second case . Step 5 . Postanalysis Questionnaire 2 . The same as Step 3 , but now for the second case . At the end of the exercise , the subjects were debriefed and asked not to discuss the case with anyone . Subjects . There were 112 ﬁrst year MBA students who participated in our study , making 56 groups , with 7 groups per experimental condition . We paid each subject $ 25 for their ( approximately ) 3 hours of participation . We told all groups they were eligible to win one of three group prizes , depending on their performance . Measures . We classify the variables used in the study as ( 1 ) experimental factors ( independent variables ) and the following dependent DSS impact variables : ( 2 ) group decision process variables , ( 3 ) subjective evaluation of the group decision process variables , ( 4 ) objective decision outcome variables , and ( 5 ) subjective evaluations of decision outcomes variables . ( We also collected information on problem - solving style and computer and Excel efﬁciency and found no signiﬁcant differences between experimen - tal groups . ) Below , we describe these variables and their measurement . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 226 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS Experimental Factors . We systematically manipu - lated the following two experimental factors . ( 1 ) DSS Availability . Yes—1 or No—0 , for the two DSSs being used ; namely , ABB and Syntex . Unobtru - sively collected tracking data showed that the deci - sion groups that had the DSSs available always used them . ( 2 ) Order . ABB ﬁrst / Syntex second = 0 ; Syntex ﬁrst / ABB second = 1 . To control for order effects , we had half the teams start with the ABB case and other half start with the Syntex case in a manner that made order independent of the two experimental factors overall . DSS Impact . In Tables 2a – d , we summarize the process and outcome variables that we used to mea - sure DSS impact . Wherever feasible , as noted in the tables , we used or adapted scales from previous research , although for several constructs , as indicated in Tables 2b – d , we had to develop new measures because well - tested scales either did not exist or were inappropriate for our context . We used LISREL 8 . 3 ( Jöreskög and Sörbom 1993 ) to assess the quality of the seven subjectively measured multiple - item mea - surement instruments ( e . g . , subjective evaluations of effort , discussion quality , decision alternatives con - sidered , complexity , learning , perceived usefulness Table 2a Summary of Objective Decision Outcome Variables and Measures Construct Deﬁnition / Description ABB Syntex Incremental return Estimated incremental sales ( ABB ) or incremental Mean = 4 (cid:1) 135 Mean = 260 (cid:1) 368 proﬁt ( Syntex ) associated with a recommended course of action . Expert rater’s evaluation Single item overall judgment provided Mean = 57 (cid:2) 6 Mean = 48 (cid:2) 9 by experts ( 1 – 100 scale ) . Table 2b Summary of Subjective Evaluation of Decision Outcome Variables and Measures Construct Description ABB Syntex Decision satisfaction The self - reported subject’s summary affective Mean = 4 . 00 Mean = 3 . 14 response to the decision that the team Alpha = 0 . 90 Alpha = 0 . 94 reached . We developed the scale . Five - item Likert scale (cid:3) normalized 1 – 5 (cid:4) I am satisﬁed with it . It is of high quality . I am in full agreement with it . I like it . I am conﬁdent that it will work out well . of the DSS , and satisfaction with the decision ) . We speciﬁed one conﬁrmatory factor analysis model . The chi square for this model is 171 . 09 (cid:1)p = 0 (cid:3) 81 (cid:4) . The com - parative ﬁt index is 1 , above the generally accepted level of 0 . 90 . The value of the standardized root mean square residual is 0 . 058 . All factor loadings are highly signiﬁcant ( minimum t - value is 4 . 40 , p < 0 (cid:3) 001 , and most t - values are above 10 ) and larger than 0 . 50 ( except for one loading , which was 0 . 35 ) . These ﬁnd - ings support the convergent validity of the items . The correlations between the seven constructs are signiﬁ - cantly different from unity , which supports their dis - criminant validity . In view of the small sample size of our data set , we also developed conﬁrmatory factor models separately for each group of impact measures ( see Figure 1 ) , i . e . , for decision process , subjective evaluation of decision process , and subjective eval - uation of decision outcome ( Churchill 1979 ) . These analyses yielded results similar to that of the overall conﬁrmatory factor model . The Cronbach alpha relia - bilities of the factors are presented in Tables 2b – d and range from 0 . 56 to 0 . 96 . As objective outcomes of using the DSSs , we mea - sured the incremental revenue obtained by the teams and the quality of the recommendations and their jus - tiﬁcations as judged by outside experts . The items listed in Table 2a require additional description . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 227 Table 2c Summary of Group Decision Process Variables and Measures Construct Description ABB Syntex Effort The self - reported extent of mental effort deployed by the subject in making the decisions Mean = 4 . 36 Mean = 4 . 11 ( note that almost all effort in this task is mental ; there is little physical effort involved ) . Alpha = 0 . 73 Alpha = 0 . 79 Scale developed by us . Three - item Likert scale (cid:3) normalized to 1 – 5 (cid:4) We were totally immersed in resolving this problem . We took this task seriously . We put in a lot of effort . Discussion quality The self - reported extent to which discussions by subjects were relevant for resolving the Mean = 4 . 32 Mean = 3 . 95 problems . Scale was adapted from Miranda and Saunders ( 1995 ) . Alpha = 0 . 58 Alpha = 0 . 59 Three - item Likert scale (cid:3) normalized to 1 – 5 (cid:4) Our discussions were well organized . We had discussions about what criteria to use to select amongst the various decision alternatives . We both participated actively in our deliberations . Decision alternatives The self - reported extent to which the decision process involved detailed consideration of Mean = 3 . 54 Mean = 3 . 66 considered various decision alternatives . Scale was adapted from Miranda and Saunders ( 1995 ) . Alpha = 0 . 56 Alpha = 0 . 65 Two - item Likert scale (cid:3) normalized to 1 – 5 (cid:4) We had discussions about many decision alternatives that were not part of the ﬁnal recommendation . We considered several alternatives carefully . Table 2d Summary of Subjective Evaluation of the Group Process Variables and Measures Construct Description ABB Syntex Process complexity The self - reported complexity of the decision - making process faced by the team in resolving Mean = 3 . 57 Mean = 4 . 05 the decision problem . We developed the scale . Alpha = 0 . 87 Alpha = 0 . 91 Three - item Likert scale (cid:3) normalized to 1 – 5 (cid:4) It was a complex process . It was a challenging process . It was a difﬁcult process . Learning This is the self - reported change in the subject’s skill and knowledge as a result of Mean = 3 . 59 Mean = 3 . 27 completing the problem - solving exercise . Scale adapted from Alavi ( 1994 ) . Alpha = 0 . 82 Alpha = 0 . 86 Three - item Likert scale (cid:3) normalized to 1 – 5 (cid:4) It increased my skills in critical thinking . It increased my ability to integrate facts . It showed me how to focus on identifying the central issues . Perceived usefulness The self - reported degree to which the subject believes that using a DSS would enhance his Mean = 4 . 15 Mean = 3 . 44 or her performance . Scale adapted from Davis ( 1989 ) . Alpha = 0 . 91 Alpha = 0 . 96 Three - item Likert scale (cid:3) normalized 1 – 5 (cid:4) It enabled us to make decisions more quickly . It increased our productivity . It improved our performance . Incremental Return Computation . For both cases , there is information in the research papers cited earlier about the resource allocation plans actually adopted by the ﬁrms and the incremental return ( proﬁts in the Syntex case and incremental sales rev - enue for ABB ) that can be attributed to these plans . That information allows us to calibrate a scoring rule to determine what the incremental return would be for any recommendation made by a team . While the Syntex model is an “optimization model , ” it also permits the user to specify different constraints on the total amount to spend and the upper and Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 228 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS lower bounds on per - product spending , to change projected product proﬁt margins , to run sensitivity analyses on different response functions , and the like . Hence , no single “optimization” is right , and the sub - jects are urged to run scenarios and consider orga - nizational and resource constraints in making their recommendations . The ABB model is not an opti - mization model ; rather it provides information about the likely response of individually targeted customers and prospects . The test condition , in which the DSS provides purchase probabilities , allows knowledge - able users to develop better targeting plans rule than in the control condition , where those probabilities are not provided . It is important to note that in this research , as in management practice , there is no sin - gle optimal solution ; hence , we frame our assessment procedure both in terms of incremental return calcu - lation ( for objective results ) and expert judgments ( for subjective assessments ) . ABB Incremental Return Calculation . For ABB , we used the market results reported in Gensch et al . ( 1990 , Table 3 , p . 16 ) . • There is no impact on incremental revenue from additional effort deployed on customers who are loyal to ABB or loyal to competitors . Speciﬁcally , if either ABB or a competitor had a purchase likelihood statis - tically signiﬁcantly higher than that of its closest com - petitor , ABB saw no gain in targeting these customers . • There is a 30 % gain from customers who had a slightly lower probability of purchasing from ABB ( but not signiﬁcantly so ) than from their most pre - ferred supplier . ABB would then see a 30 % gain , on average , from targeting these customers ( called switchables ) . • There is a 31 % gain from customers who had a slightly higher probability of purchasing from ABB ( but not signiﬁcantly so ) than from their next most preferred supplier . ABB would then see a 31 % gain , on average , from targeting these customers ( called competitives ) . We used the choice probabilities computed by the model to identify the largest 20 of the vulnerable customers ( switchables and competitives ) . We then computed the expected incremental sales from each targeted customer as : zero if not a switchable or competitive customer , and otherwise equal to adjust - ment factor ∗ ( ( 1 − P ( buying from ABB ) ) ∗ max sales potential ) . This formula determines the incremental return that ABB gets either by retaining a cus - tomer who would otherwise likely have switched to a competitor , or by gaining a new customer who currently marginally prefers one of the competitors . We computed the adjustment factor ( = 0 . 40 ) to obtain the overall sales increase from switchables and com - petitives of 30 . 5 % to be consistent with the actual results that ABB realized . DSS users had the informa - tion in Figure 2 to work with ; they had to develop their own ﬁnancial targeting rule , albeit with better information than the Excel - only users . Syntex Incremental Return Calculation . Syntex’s actual market performance ( three years forward ) closely matched what the managerially generated judgmental response functions had predicted . Hence , we used the following estimate of proﬁt per product : Proﬁt for Product i = (cid:1) base sales i × Response i (cid:2) X i base X i (cid:3) × margin i (cid:4) − (cid:10)X i × salesman unit cost (cid:11)(cid:12) where X i is the sales force effort level deployed on product i , and Response i (cid:1)X i / base X i (cid:4) is the judgmentally cali - brated response function assessed at X i (cid:3) We summed these proﬁt ﬁgures over all products to yield an overall company proﬁt for a team’s rec - ommendation . As an example for Naprosyn , if the recommendation is for 145 reps ( approximately 1 . 5 × 96 . 8 reps ) , then , from Row 9 of Figure 3 , we get Naprosyn proﬁt = $ 214 (cid:12) 400 (cid:12) 000 × 1 (cid:3) 26 × 0 (cid:3) 70 − $ 63 (cid:12) 000 × 145 = $ 179 (cid:12) 965 (cid:12) 000 (cid:3) Note that the DSS automates the estimation of the response function and invokes Excel’s Solver opti - mization module to help with such calculations ( i . e . , the estimation of the 1 . 26 response factor above result - ing from the 50 % increase in the sales force allocation to Naprosyn ) . As noted earlier , the DSS also permits the user to impose upper or lower limits on over - all sales force spending or on spending on individual products . Excel - only users had all the input data needed to build the response function ( i . e . , data in cells G9 – J15 in Figure 3 ) , but not the response func - tions themselves . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 229 Table 3 Overall , Main Experimental Results on DSS vs . No DSS Conditions ABB Syntex No DSS DSS Total No DSS DSS Total DSS impact measure mean ( std . dev . ) mean ( std . dev . ) mean ( std . dev . ) mean ( std . dev . ) mean ( std . dev . ) mean ( std . dev . ) Objective decision outcomes Incremental revenue 3 (cid:1) 219 (cid:3) 1 (cid:1) 945 (cid:4) 5 (cid:1) 052 (cid:3) 1 (cid:1) 821 (cid:4) 4 (cid:1) 135 (cid:3) 2 (cid:1) 084 (cid:4) 252 (cid:1) 918 (cid:3) 16 (cid:1) 477 (cid:4) 267 (cid:1) 553 (cid:3) 13 (cid:1) 535 (cid:4) 260 (cid:1) 368 (cid:3) 16 (cid:1) 638 (cid:4) Expert evaluation 56 (cid:2) 43 (cid:3) 13 (cid:2) 53 (cid:4) 58 (cid:2) 75 (cid:3) 12 (cid:2) 87 (cid:4) 57 (cid:2) 59 (cid:3) 13 (cid:2) 13 (cid:4) 47 (cid:2) 86 (cid:3) 18 (cid:2) 31 (cid:4) 50 (cid:2) 00 (cid:3) 16 (cid:2) 91 (cid:4) 48 (cid:2) 93 (cid:3) 17 (cid:2) 51 (cid:4) Subjective evaluation of decision outcomes Decision satisfaction 3 (cid:2) 79 (cid:3) 0 (cid:2) 87 (cid:4) 4 (cid:2) 20 (cid:3) 0 (cid:2) 64 (cid:4) 4 (cid:2) 00 (cid:3) 0 (cid:2) 78 (cid:4) 2 (cid:2) 78 (cid:3) 1 (cid:2) 13 (cid:4) 3 (cid:2) 47 (cid:3) 0 (cid:2) 92 (cid:4) 3 (cid:2) 14 (cid:3) 1 (cid:2) 07 (cid:4) Decision process Effort 4 (cid:2) 49 (cid:3) 0 (cid:2) 53 (cid:4) 4 (cid:2) 23 (cid:3) 0 (cid:2) 62 (cid:4) 4 (cid:2) 36 (cid:3) 0 (cid:2) 59 (cid:4) 3 (cid:2) 92 (cid:3) 0 (cid:2) 78 (cid:4) 4 (cid:2) 30 (cid:3) 0 (cid:2) 58 (cid:4) 4 (cid:2) 11 (cid:3) 0 (cid:2) 71 (cid:4) Discussion quality 4 (cid:2) 30 (cid:3) 0 (cid:2) 65 (cid:4) 4 (cid:2) 34 (cid:3) 0 (cid:2) 37 (cid:4) 4 (cid:2) 32 (cid:3) 0 (cid:2) 52 (cid:4) 3 (cid:2) 75 (cid:3) 0 (cid:2) 68 (cid:4) 4 (cid:2) 14 (cid:3) 0 (cid:2) 45 (cid:4) 3 (cid:2) 95 (cid:3) 0 (cid:2) 60 (cid:4) Decision alternatives considered 3 (cid:2) 64 (cid:3) 0 (cid:2) 97 (cid:4) 3 (cid:2) 43 (cid:3) 0 (cid:2) 77 (cid:4) 3 (cid:2) 54 (cid:3) 0 (cid:2) 87 (cid:4) 3 (cid:2) 57 (cid:3) 0 (cid:2) 98 (cid:4) 3 (cid:2) 75 (cid:3) 0 (cid:2) 74 (cid:4) 3 (cid:2) 66 (cid:3) 0 (cid:2) 86 (cid:4) Subjective evaluation of the decision process Complexity 3 (cid:2) 77 (cid:3) 0 (cid:2) 77 (cid:4) 3 (cid:2) 36 (cid:3) 0 (cid:2) 70 (cid:4) 3 (cid:2) 57 (cid:3) 0 (cid:2) 76 (cid:4) 4 (cid:2) 21 (cid:3) 0 (cid:2) 82 (cid:4) 3 (cid:2) 89 (cid:3) 0 (cid:2) 80 (cid:4) 4 (cid:2) 05 (cid:3) 0 (cid:2) 82 (cid:4) Learning 3 (cid:2) 77 (cid:3) 0 (cid:2) 84 (cid:4) 3 (cid:2) 41 (cid:3) 0 (cid:2) 79 (cid:4) 3 (cid:2) 59 (cid:3) 0 (cid:2) 83 (cid:4) 3 (cid:2) 29 (cid:3) 1 (cid:2) 01 (cid:4) 3 (cid:2) 26 (cid:3) 0 (cid:2) 99 (cid:4) 3 (cid:2) 27 (cid:3) 0 (cid:2) 99 (cid:4) Perceived usefulness of DSS 4 (cid:2) 11 (cid:3) 0 (cid:2) 89 (cid:4) 4 (cid:2) 19 (cid:3) 0 (cid:2) 94 (cid:4) 4 (cid:2) 15 (cid:3) 0 (cid:2) 91 (cid:4) 3 (cid:2) 05 (cid:3) 1 (cid:2) 40 (cid:4) 3 (cid:2) 82 (cid:3) 1 (cid:2) 20 (cid:4) 3 (cid:2) 44 (cid:3) 1 (cid:2) 35 (cid:4) Expert Rater’s Evaluations . All groups completed a recommendation form for each case , along with their justiﬁcations for these recommendations . We transcribed and typed these recommendation forms to make them appear uniform , and gave them to three expert raters for evaluation . We removed references to the form of the DSS that the respondents had avail - able so that the raters would not know whether the groups had had access to a DSS to aid their decisions . The raters were senior faculty members in marketing and management science at two leading universities and were knowledgeable about the speciﬁc problem context and resource allocation issues in general . We provided the raters with the cases and the accompa - nying software , but provided no indication of “right” answers . We then asked the raters to score the overall quality of the recommendations on a scale of 1 – 100 . Results To test the signiﬁcance of the effects of our experimen - tal manipulations , we conducted a series of analysis of variances ( ANOVAs ) with repeated measurements . Within each team , we treated measures for the two subjects as the repeated measures for the same dependent variables . As independent variables , we included DSS availability and order . Further , we included the interaction between DSS availability and order to analyze whether the effect of a model would be different if a case were analyzed ﬁrst or second . The structure of the ANOVA model is as follows : DSS impact variable = (cid:14) 0 + (cid:14) 1 ∗ DSS availability + (cid:14) 2 ∗ order + (cid:14) 3 ∗ DSS availability ∗ order (cid:3) The signiﬁcance levels we report in this section are based on an analysis of the complete model that includes the two main effects and their interactions . DSS Impact on Objective Decision Outcomes We start by testing Hypothesis 1 ; namely , that the use of a DSS improves objective decision outcomes . Incremental Return . The results in Table 3 show that for both ABB ( F = 12 (cid:3) 82 , p 3 < 0 (cid:3) 001 ) and Syntex ( F = 12 (cid:3) 63 , p < 0 (cid:3) 001 ) , DSS - aided groups achieved higher incremental returns than did Excel - only aided groups . No signiﬁcant order effects appeared . These results support Hypothesis 1a . Expert Evaluation . In neither of the two cases did the experts rate the recommendations of the DSS - aided teams higher than those of the Excel - only aided teams ( ABB : F = 0 (cid:3) 43 , p = 0 (cid:3) 257 ; Syntex : F = 0 (cid:3) 20 , p = 0 (cid:3) 329 ) . Thus , we did not ﬁnd support 3 All signiﬁcance levels in the “Results” section represent one - tailed tests . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 230 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS for Hypothesis 1b . To understand why , we ran exploratory regression analyses , regressing expert rat - ings against different explanatory variables . We found that report length—the number of words in the writ - ten explanations provided by the subjects for their recommendations—was , by far , the most signiﬁcant factor explaining expert ratings for both ABB and Syntex ( ABB : (cid:7) (cid:14) = 0 (cid:3) 52 , Syntex : (cid:7) (cid:14) = 0 (cid:3) 73 ) . That is , the more detailed the explanation for a recommendation , the better the raters evaluated that recommendation , regardless of the experimental condition to which a team belonged . For Syntex , there is a marginally sig - niﬁcant ( positive ) effect of deanchoring , but that effect is much less important than the report - length effect . We hypothesize that in the absence of objective per - formance indicators , expert raters employ potentially biasing cues , such as the length of the report , in mak - ing an assessment of the quality of the recommen - dations . To test for this possibility , we estimated a regression model of expert ratings as a function of ( 1 ) the DSS availability and ( 2 ) performance cues— Table 4a Determinants of Expert Ratings ( Standardized Regression Coefﬁcient , t - Value in Parentheses ) Variable Syntex Case ABB Case DSS availability 0 . 11 ( 1 . 05 ) − 0 (cid:2) 00 (cid:3) − 0 (cid:2) 03 (cid:4) Cue : Report length 1 0 . 73 ( 7 . 85 ) 0 . 52 ( 4 . 40 ) Cue : Deanchoring 1 0 . 22 ( 2 . 20 ) 0 . 15 ( 1 . 16 ) F F(cid:3) 3 (cid:1) 51 (cid:4) = 23 (cid:2) 44 F(cid:3) 3 (cid:1) 52 (cid:4) = 7 (cid:2) 22 p = 0 (cid:2) 00 p = 0 (cid:2) 00 R - square 0 . 58 0 . 29 Table 4b Determinants of Report Length ( Standardized Regression Coefﬁcient , t - Value in Parentheses ) Variable Syntex Case ABB Case DSS availability − 0 (cid:2) 36 (cid:3) − 2 (cid:2) 84 (cid:4) − 0 (cid:2) 30 (cid:3) − 2 (cid:2) 00 (cid:4) 0 . 05 ( 0 . 35 ) 0 . 07 ( 0 . 45 ) Incremental return 0 . 27 ( 2 . 12 ) 0 . 22 ( 1 . 49 ) − 0 (cid:2) 16 (cid:3) − 1 (cid:2) 15 (cid:4) − 0 (cid:2) 01 (cid:3) − 0 (cid:2) 03 (cid:4) “Group’s tendency to write 0 . 52 ( 4 . 57 ) 0 . 52 ( 4 . 01 ) lengthy reports” 2 F F(cid:3) 3 (cid:1) 51 (cid:4) = 9 (cid:2) 01 F(cid:3) 2 (cid:1) 52 (cid:4) = 2 (cid:2) 23 F(cid:3) 3 (cid:1) 52 (cid:4) = 5 (cid:2) 47 F(cid:3) 2 (cid:1) 53 (cid:4) = 0 (cid:2) 14 p = 0 (cid:2) 00 p = 0 (cid:2) 12 p = 0 (cid:2) 00 p = 0 (cid:2) 87 R - square 0 . 35 0 . 08 0 . 24 0 . 01 Note . Differences signiﬁcant at the 0 . 05 level ( one tailed ) are shown in bold . 1 The factor report length is the number of words used in the team’s recommendation for Syntex and ABB , respec - tively . Deanchoring , i . e . , deviation of decision from anchor point , is calculated as follows : ( 1 ) ABB : 20—number of targeted ﬁrms that belong to the set of the 20 ﬁrms with the highest purchase volume and ( 2 ) Syntex : Euclidean distance from the base allocation . The variables deanchoring ( Syntex only ) and report length ( both cases ) were log transformed . 2 To approximate the team trait of writing extensively , independent of any performance measures , we used the report length ( log transformed ) of ABB in the case of Syntex and vice versa . i . e . , report length and the extent of deanchoring—that may or may not be associated with actual decision quality . Table 4a summarizes our results . Given the high level of signiﬁcance of report length as a cue in both cases , we also explored the potential deter - minants of report length , summarized in Table 4b . Our analyses suggest that there is an underlying trait , namely , the tendency to write long reports , that is not only distinct from team performance ( Table 4b ) , but is also the main driver for report length ( ABB : (cid:7) (cid:14) = 0 (cid:3) 52 , Syntex : (cid:7) (cid:14) = 0 (cid:3) 52 ) . Thus , report length seems to be a primary cue that leads to judgmental bias on the part of the expert raters . Summarizing , we ﬁnd ( not surprisingly ) that high - quality , model - based DSSs signiﬁcantly improve the quality of decision making , i . e . , teams using the DSS signiﬁcantly improve their ﬁrm’s proﬁtability . How - ever , somewhat surprisingly , the use of the DSS does not signiﬁcantly affect the way their recommenda - tions are perceived by the ( surrogate ) senior man - agers , represented by the expert judges . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 231 DSS Impact on Subjective Evaluations of Decision Outcomes Next , we test Hypothesis 2 , which states that the DSS has no effect on the decision makers’ subjective eval - uations of their decisions . Decision Satisfaction . For ABB , the effect of the DSS on decision satisfaction is highly dependent on whether this case was analyzed ﬁrst or second ( after the Syntex case ) ( F = 15 (cid:3) 82 , p < 0 (cid:3) 001 ) . On average , DSS availability does not increase decision satisfac - tion ( F = 1 (cid:3) 16 , p = 0 (cid:3) 144 ) . However , overall ( including those with and without DSS ) , subjects were more sat - isﬁed with their recommendation if they analyzed the ABB case after they had ﬁrst analyzed the Syn - tex case ( 4 . 12 versus 3 . 87 , F = 4 (cid:3) 33 , p = 0 (cid:3) 022 ) . If the ABB case is analyzed ﬁrst , the DSS was effective in increasing decision satisfaction ( 4 . 17 versus 3 . 44 for the DSS group versus the Excel - only aided group , F = 8 (cid:3) 826 , p = 0 (cid:3) 003 ) . The satisfaction enhancing effect did not show up if the ABB case was analyzed after the Syntex case . For Syntex , we do ﬁnd a signiﬁcant main effect of DSS availability ( F = 4 (cid:3) 095 , p = 0 (cid:3) 024 ) . DSS - aided subjects report more satisfaction with their recommendations than do Excel - only aided subjects ( 3 . 47 versus 2 . 78 ) . This DSS effect holds irrespective of the order in which the cases are analyzed . Overall , subjects are more satisﬁed with their recommenda - tions for ABB than for Syntex ( 4 . 00 versus 3 . 14 ) . Summarizing , for the Syntex case , DSS use increases decision satisfaction , which leads us to reject Hypoth - esis 2a , whereas we ﬁnd partial support for Hypoth - esis 2a for the ABB DSS . DSS Impact on Decision Process Next , we test Hypothesis 3 , which states that the DSSs will have mixed effects on the decision process . Effort . For the ABB case , we ﬁnd that DSS avail - ability does not have a signiﬁcant main effect on perceived effort . In fact , there is a tendency for the DSS to reduce the amount of effort ( 4 . 49 versus 4 . 23 ; F = 2 (cid:3) 02 , p = 0 (cid:3) 081 ) . However , similar to the effect for decision satisfaction , the ABB - DSS effect depends on whether the case is analyzed ﬁrst or second ( F = 3 (cid:3) 78 , p = 0 (cid:3) 029 ) . If the ABB case is analyzed ﬁrst , the DSS only marginally affects the amount of effort expended . If the ABB case is analyzed after the Syntex case , the DSS leads to a substantial reduction in effort ( 4 . 17 versus 4 . 57 ) , which supports Hypothesis 3a . For Syntex , while we ﬁnd that DSS use increases the amount of effort deployed in analyzing the case ( 3 . 92 versus 4 . 30 ) , the effect is not signiﬁcant ( F = 1 (cid:3) 82 , p = 0 (cid:3) 092 ) . Hence , we ﬁnd some support for Hypoth - esis 3a , but only for the ABB case . Discussion Quality . Overall , the ABB DSS does not signiﬁcantly improve the perceived quality of the dis - cussions ( F = 0 (cid:3) 35 , p = 0 (cid:3) 228 ) . However , as before , the effect of the ABB DSS differs depending on order ( F = 2 (cid:3) 99 , p = 0 (cid:3) 045 ) . If the ABB case was analyzed ﬁrst , DSS availability improves discussion quality ( 4 . 14 versus 4 . 32 ) ; if the ABB case was analyzed second , the DSS had little impact on discussion quality . For the Syntex DSS , we ﬁnd that discussion quality improves with the availability of the DSS ( 4 . 14 versus 3 . 75 ; F = 3 (cid:3) 23 , p = 0 (cid:3) 039 ) . This effect does not depend on the order in which the cases were analyzed . In summary , with respect to discussion quality , we ﬁnd support for Hypothesis 3b for the Syntex DSS and partial support for the ABB DSS . Decision Alternatives Considered . We ﬁnd no support for Hypothesis 3c for either case : DSSs did not signiﬁcantly affect the number of decision alterna - tives generated or evaluated ( ABB : F = 0 (cid:3) 19 , p = 0 (cid:3) 333 ; Syntex : F = 0 (cid:3) 019 , p = 0 (cid:3) 446 ) . The order in which the cases were analyzed also did not inﬂuence the num - ber of alternatives generated . It appears that DSS can inﬂuence objective outcomes even without altering some key aspects of the decision process , such as the number of alternatives evaluated during decision making . Miranda and Saunders ( 1995 ) report a similar result . DSS Impact on Subjective Evaluations of Decision Process Lastly , we test Hypothesis 4 , which states that a DSS will have mixed effects on the evaluation of the deci - sion process . Complexity of the Decision Problem . Hypothe - sis 4a hypothesizes a net zero effect of DSS use . How - ever , for both the ABB and the Syntex case , we ﬁnd that the perceived complexity of the problem - solving task is reduced with the availability of a DSS . Overall , the Syntex case is perceived to be more complex than Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 232 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS the ABB case ( 4 . 05 versus 3 . 57 ) . Analyzing the ABB case is perceived to be signiﬁcantly less complex if it is analyzed second than if it is analyzed ﬁrst ( F = 3 (cid:3) 85 , p = 0 (cid:3) 028 ) . Additionally , the ABB DSS reduced the per - ceived complexity of the decision problem ( F = 5 (cid:3) 29 , p = 0 (cid:3) 013 ) . For the Syntex case , we do not ﬁnd an order effect , and further , the effect of the DSS is not as strong ( F = 2 (cid:3) 76 , p = 0 (cid:3) 052 ) . For Syntex , there was no difference in perceived effort between the DSS and Excel - only conditions ( Hypothesis 3a ) , and yet we ﬁnd perceived problem complexity is lower with DSS use . This suggests that the directive Syntex DSS ( i . e . , a DSS that offers speciﬁc feedback ) has a different type of effect on the decision process than does the nondi - rective ABB DSS . Overall , we reject Hypothesis 4a . Learning . The ABB DSS does not enhance learning as perceived by the participants . In fact , DSS users report a lower learning experience than do Excel - only aided users ( Excel : 3 . 77 versus DSS aided 3 . 41 ; F = 2 (cid:3) 24 , p = 0 (cid:3) 141 ) , as hypothesized in Hypothesis 4b , and this effect is more prominent if the Syntex case is analyzed ﬁrst ( F = 2 (cid:3) 24 , p = 0 (cid:3) 071 ) . However , the effect is not signiﬁcant . The Syntex DSS also had no impact on the amount of perceived learning ( F = 0 (cid:3) 026 , p = 0 (cid:3) 436 ) . Thus , overall , we ﬁnd no support for Hypothe - sis 4b . Presumably , to the extent that a DSS automates parts of the decision process and hides complexity , it may well lead to a reduction in perceived learning . Perceived Usefulness of the DSS . On average , the DSS for ABB is not perceived as signiﬁcantly more useful than the Excel - only aid ( F = 0 (cid:3) 17 , p = 0 (cid:3) 341 ) . However , there was an interaction between DSS avail - ability and order ( F = 5 (cid:3) 501 , p = 0 (cid:3) 012 ) , such that the DSS is perceived as useful if the ABB case is ana - lyzed ﬁrst . If the Syntex case is analyzed ﬁrst , the ABB DSS is not perceived as useful . For Syntex , we ﬁnd a strong effect of DSS availability ( F = 6 (cid:3) 618 , p = 0 (cid:3) 007 ) : The DSS is perceived as useful ( 3 . 82 versus 3 . 05 ) . This result does not depend on the order in which the cases were analyzed . Hence , we ﬁnd support for Hypoth - esis 4c for the Syntex model and partial support for Hypothesis 4c for the ABB model . Again , this result shows the importance of deploying a more directive DSS , such as Syntex , to inﬂuence perceived usefulness of a DSS . In summary , our results show that both the ABB and the Syntex DSS improve objective performance by leading to higher incremental returns . The effects of the DSSs on objective performance variables are more pronounced than their effects on subjective vari - ables , especially for the ABB DSS . The ABB DSS only improves decision satisfaction if it is analyzed ﬁrst , before the subjects had gained any experience in addressing a resource allocation problem . In such sit - uations , subjects also perceived the ABB DSS as more useful , and as improving discussion quality . When the ABB case is analyzed second ( i . e . , after the Syn - tex case ) , the DSS was not perceived as more useful than the Excel - only tool , DSS use reduced perceived effort , there was no improvement in perceived discus - sion quality , and the subjects reported less satisfaction with their decisions . The Syntex DSS was perceived as useful regardless of whether the Syntex case was ana - lyzed ﬁrst or second . The Syntex DSS improved dis - cussion quality , improved decision satisfaction , and appeared to have increased effort as well . Discussion and Conclusion Our results show that two well - designed decision models for marketing resource allocation improve objective outcomes , primarily because those DSSs enabled subjects using them to move away from historic anchors ( the base - case decision scenarios ) , toward decisions that improve organizational proﬁts . Speciﬁcally , for ABB , DSS users targeted more “switchable and competitive” customers ( average 12 . 8 customers out of the best 20 to be targeted ) than did Excel - only users ( average 6 . 3 customers ) , com - pared to the anchor ( targeting large customers ) , which included 6 switchables and competitives . For Syntex , DSS users recommended 270 incremental salespeo - ple on average , versus 175 by Excel - only users and 120 under the current management plan ( the anchor ) . The Syntex DSS users also recommended more effort deployment on the proﬁtable product , Naprosyn , than did either the Excel - only group or the current management plan ( 38 % of total sales resource effort in the DSS group versus 30 % for the Excel - only group and 23 % under the current management plan ) . Hence , a DSS appears to provide users with the conﬁdence and the support to propose decisions farther away from the status quo than those without a DSS . DSS effects on subjective perceptions of achieved outcomes , however , were mixed and somewhat Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 233 surprising . For Syntex , DSS use enhanced perceived satisfaction with the outcome and the perceived use - fulness of the DSS . In contrast , for ABB , the DSS did not always increase satisfaction with the outcome and the perceived usefulness of the model . Even though the use of DSSs reduced subjects’ perception of problem complexity , they had no signiﬁcant pos - itive impact on perceived learning and may even have reduced it . By investigating DSS effects on the decision process separate from its direct effects on objective performance , we ﬁnd a surprising discon - nect between objective performance measures that are favorable and subjective performance measures ( e . g . , satisfaction and usefulness ) that are mixed or unfavorable . As discussed earlier , DSSs can alter the trade - offs that decision makers make between effort deployment and quality of decisions . A DSS could just improve efﬁciency ( save effort ) . Or it could also lead to greater effectiveness if the user is motivated by the DSS to deploy more cognitive effort to the task ( Moore and Chang 1983 ) . Our results indicate that both the Syntex DSS and the ABB DSS reduced the perceived complexity of the problem . For the directive Syntex DSS , there was no reduction in effort even though perceived complexity did decrease ; whereas for the nondirective ABB DSS , we did not ﬁnd similar effects , especially if the ABB DSS was used after the subjects analyzed the Syntex case . These results suggest that the design of a DSS ( e . g . , directive versus nondirec - tive ) inﬂuences whether decision makers choose effort reduction over decision - quality improvement . The mixed results with respect to subjective and objective outcomes also offer insights about why we do not see widespread use of DSS in tasks such as resource allocation . It is not enough to simply promise , or even deliver , improved objective out - comes through DSS use . It is equally important to design DSSs so they give users cues to help them per - ceive that improved outcomes are likely to occur with their use ( as was the case with the Syntex DSS ) . And expert evaluations of decisions seem to be driven as much by style as by substance . Hence , if DSSs gener - ate a cost for the organization without a clearly per - ceived beneﬁt ( improved perceived decision quality ) , they are unlikely to be widely used , even when their use is likely to be beneﬁcial . The effect of the order variable highlights the poten - tial differences between directive and nondirective DSS ( Syntex versus ABB ) . Our postexperimental ques - tionnaire supports our observation that feedback from the Syntex DSS inﬂuenced the decision process in a different way than in the ABB case . In the Syntex case , the means for the item “The DSS narrowed our focus” were statistically signiﬁcantly different between DSS and Excel - only groups , whereas they were not dif - ferent for the ABB case groups . In the framework of Balzer et al . ( 1989 ) , user interactions with the Syntex DSS ( but not with the ABB DSS ) provided “cognitive feedback , ” linking the task with the environmental performance measures . Our study also shows that DSSs can help reduce the perceived cognitive complexity of a resource allo - cation task , suggesting that their use is more likely when the problem , such as resource allocation , is intrinsically complex . While we believe our results will generalize beyond the resource allocation context , we also note that resource allocation is an important and signiﬁcant context in itself . For example , Sinha and Zoltners ( 2001 ) recount more than 2 , 000 appli - cations similar to that of Syntex in their consulting practice alone . While our results suggest the need for further research , especially concerning the role of discussion facilitation and feedback , they also offer the following insights for DSS design . ( 1 ) Design DSSs to encourage discussion . Although designing DSSs that lead to improved objective out - comes should be a primary criterion , that criterion alone is not enough to encourage use of the DSS , or to help users feel good about DSS use ; what decision makers perceive ( i . e . , subjective assessment ) is not what they actually achieve ( objective outcomes ) . To make subjective outcomes more commensurate with objective outcomes , it is important to design in fea - tures that encourage interaction with the DSS , offer explanations for recommendations , generate visual outputs , and provide structured cognitive and out - come feedback , all of which can facilitate managerial discussion quality about the decision problem and enhance perceived outcomes from DSS use ( satisfac - tion , learning , and usefulness of the DSS ) . ( 2 ) Design DSSs to reduce problem complexity and encourage consideration of additional alternatives . When Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions 234 Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS a DSS reduces problem complexity and facilitates the assessment of multiple alternatives , decision quality improves ( as was the case with Syntex DSS ) . This improvement appears to occur because such DSSs induce users to deploy more cognitive effort toward problem solving . ( 3 ) Design in feedback . Users experience improved decision processes and better outcomes when the DSS ﬁts well with the decision context , and provides spe - ciﬁc feedback on the likely outcomes of alternative courses of action . Users are likely to prefer DSSs that they understand and trust—to increase perceived use - fulness of the DSS , it is important to make the oper - ation and the logic of the DSS more transparent ( as with the Syntex DSS versus the ABB DSS ) . The above suggestions for DSS design are , natu - rally , incomplete . Other factors , such as ease of use , compatibility with existing systems , alignment of per - sonal and organizational reward metrics , and the like that have been identiﬁed in the adoption literature ( e . g . , Rogers 2003 ) are also required to increase the intent to adopt . We conclude by noting some limitations of this research that suggest opportunities for future research questions . Our study is based on a laboratory exper - iment , with limited duration , and without all the political complexities associated with DSS use in orga - nizational settings . We did not manipulate or explore several contextual aspects of DSS use that are likely to inﬂuence the process and outcomes associated with DSS . For example , we ﬁxed the ( same ) max - imum experimental duration time for all subjects , preventing us from fully understanding the drivers of ( objective ) effort expenditure in a more uncon - strained environment . In addition , in practice , people are trained speciﬁcally in the use of a DSS , which we did not do here to avoid inducing another strong anchor point for the decisions to follow . And , it may be that managers in real situations are better able to distinguish good recommendations from poorer ones , in contrast with our laboratory situation . These issues suggest the need for ﬁeld research ( preferably using experimental techniques ) in the context of the intro - duction of a DSS in real organizations . And while we attempted to study learning and likely adoption in a single laboratory setting , it may be much more realistic and appropriate to study learning , feedback , and their effects in a repetitive decision environment . In recent years , companies have invested tens of billions of dollars in implementing software systems , such as customer relationship management , to facil - itate marketing resource allocation decisions . With - out a careful understanding of how such systems inﬂuence individual and organizational decision mak - ing and performance , it is likely that these invest - ments will not be optimally leveraged . This situation presents many rich research opportunities for devel - oping generalizations about what types of DSS will work well in such software environments and why . We hope that the work presented here can play a part in helping to affect both the theory and practice of DSS implementation . References Accenture Research Report . 2001 . Insight Driven Marketing , Chicago , IL . Agarwal , R . , M . R . Tanniru , M . R . Dacruz . 1992 . Knowledge - based support for combining qualitative and quantitative judgments in resource allocation decisions . J . Management Inform . Systems 9 ( Summer ) 165 – 184 . Alavi , M . 1994 . Computer - mediated collaborative learning : An empirical evaluation . MIS Quart . 18 ( 2 ) 159 – 175 . Arnott , D . 2002 . Decision biases and decision support systems development . Working paper , Decision Support Systems Lab - oratory , Monash University , Melbourne , Australia . Balzer , W . K . , M . E . Doherty , R . O’Connor , Jr . 1989 . Effects of cog - nitive feedback on performance . Psych . Bull . 106 ( 3 ) 410 – 433 . Bazerman , M . 1998 . Judgment in Managerial Decision Making . John Wiley and Sons , New York . Benbasat , I . , B . R . Nault . 1990 . An evaluation of empirical research in managerial support systems . Decision Support Systems 6 ( 3 ) 203 – 226 . Blattberg , R . C . , S . J . Hoch . 1990 . Database models and manage - rial intuition : 50 % model + 50 % manager . Management Sci . 36 ( 8 ) 887 – 899 . Chakravarti , D . , A . Mitchell , R . Staelin . 1979 . Judgment - based mar - keting decision models : An experimental investigation of the decision calculus approach . Management Sci . 25 ( 3 ) 251 – 263 . Churchill , G . A . , Jr . 1979 . A paradigm for developing better mea - sures of marketing constructs . J . Marketing Res . 16 ( 1 ) 64 – 73 . Croskerry , P . 2003 . The importance of cognitive errors in diagnosis and strategies to minimize them . Acad . Medicine 78 ( 8 ) 775 – 780 . Davis , F . D . 1989 . Perceived usefulness , perceived ease of use , and user acceptance of information technology . MIS Quart . 13 ( 3 ) 319 – 340 . Dawes , R . M . 1979 . The robust beauty of improper linear models in decision making . Amer . Psychologist 34 ( 7 ) 571 – 582 . DeLone , W . , E . R . McLean . 1992 . Information systems success : The quest for the dependent variable . Inform . Systems Res . 3 ( 1 ) 60 – 95 . Lilien et al . : DSS Effectiveness in Marketing Resource Allocation Decisions Information Systems Research 15 ( 3 ) , pp . 216 – 235 , ©2004 INFORMS 235 Dennis , A . R . , B . Wixom , R . J . Vandenberg . 2001 . Understanding ﬁt and appropriation effects in group support systems via meta - analysis . MIS Quart . 25 ( 2 ) 167 – 197 . Eisenstein , Eric M . , Leonard P . Lodish . 2002 . Precisely Worthwhile or Vaguely Worthless : Are Marketing Decision Support and Intelligent Systems “Worth It” ? Sage Publications , Thousand Oaks , CA , 436 – 456 . Fudge , W . K . , L . M . Lodish . 1977 . Evaluation of the effectiveness of a model - based salesman’s planning by ﬁeld experimentation . Interfaces 8 ( 1 ) 97 – 106 . Gensch , D . , N . Aversa , S . P . Moore . 1990 . A choice modeling market information system that enabled ABB Electric to expand its market share . Interfaces 20 ( 1 ) 6 – 25 . Goodman , J . S . 1998 . The interactive effects of task and external feedback on practice performance and learning . Organ . Behav - ior Human Decision Processes 76 ( 3 ) 223 – 252 . Hoch , S . J . 1994 . Experts and models in combination . R . C . Blattberg , R . Glazer , J . D . C . Little , eds . The Marketing Information Revolution . Harvard Business School Press , Boston , MA . Hoch , S . J . , D . A . Schkade . 1996 . A psychological approach to deci - sion support systems . Management Sci . 42 ( 1 ) 51 – 64 . Hogarth , R . M . , S . Makridakis . 1981 . Forecasting and planning : An evaluation . Management Sci . 27 ( 2 ) 115 – 138 . Jöreskög , K . , D . Sörbom . 1993 . LISREL 8 : Structural Equation Model - ing with the SIMPLIS Command Language . SSI Scientiﬁc Software International , Chicago , IL . Lilien , G . L . , A . Rangaswamy . 1998 . Marketing Engineering : Computer - Assisted Marketing Analysis and Planning . Addison - Wesley , Reading , MA . Lodish , L . M . , E . Curtis , M . Ness , M . K . Simpson . 1988 . Sales force sizing and deployment using a decision calculus model at Syntex Laboratories . Interfaces 18 ( 1 ) 5 – 20 . McIntyre , S . H . 1982 . An experimental study of the impact of judgment - based marketing models . Management Sci . 28 ( 1 ) 17 – 33 . Miranda , S . M . , C . Saunders . 1995 . Group support systems : An organization development intervention to combat groupthink . Public Admin . Quart . 19 ( 2 ) 193 – 216 . Moore , J . H . , M . G . Chang . 1983 . Meta - design considerations in building DSS . J . L . Bennet , ed . Building Decision Support Systems . Addison - Wesley , Reading , MA , 173 – 204 . Mowen , J . C . , G . J . Gaeth . 1992 . The evaluation stage in marketing decision making . J . Acad . Marketing Sci . 20 ( 2 ) 177 – 187 . Muckstadt , J . A . , D . H . Murray , J . A . Rappold , D . E . Collins . 2001 . Guidelines for collaborative supply chain system design and operation . Inform . Systems Frontiers 3 ( 4 ) 427 – 453 . Payne , J . W . , J . R . Bettman , E . J . Johnson . 1993 . The Adaptive Decision Maker . Cambridge University Press , Cambridge , U . K . Rogers , E . M . 2003 . The Diffusion of Innovations , 5th ed . The Free Press , New York . Sharda , R . , S . H . Barr , J . C . McDonnell . 1988 . Decision support sys - tem effectiveness : A review and an empirical test . Management Sci . 34 ( 2 ) 139 – 159 . Silver , M . S . 1990 . Decision support systems : Directed and nondi - rected change . Inform . Systems Res . 1 ( 1 ) 47 – 70 . Simon , H . 1955 . A behavioral model of rational choice . Quart . J . Econom . 69 99 – 118 . Sinha , P . , A . A . Zoltners . 2001 . Salesforce decision models : Insights from 25 years of implementation . Interfaces 31 ( 3 ) S8 – S44 . Slovic , P . , S . Lichtenstein . 1971 . Comparison of Bayesian and regres - sion approaches to the study of information processing in judg - ment . Organ . Behavior Human Performance 6 ( 6 ) 694 – 744 . Todd , P . , I . Benbasat . 1999 . Evaluating the impact of DSS , cognitive effort , and incentives on strategy selection . Inform . Systems Res . 10 ( 4 ) 356 – 374 . Tversky , A . , D . Kahneman . 1974 . Judgment under uncertainty : Heuristics and biases . Science 185 1124 – 1131 . Van Bruggen , G . H . , A . Smidts , B . Wierenga . 1996 . The impact of the quality of a marketing decision support system : An exper - imental study . Internat . J . Res . Marketing 13 ( 4 ) 331 – 343 . Van Bruggen , G . H . , A . Smidts , B . Wierenga . 1998 . Improving deci - sion making by means of a marketing decision support system . Management Sci . 44 ( 5 ) 645 – 658 . Wierenga , B . , G . Van Bruggen . 2000 . Marketing Management Support Systems : Principles , Tools , and Implementation . Kluwer Academic Publishers , Boston , MA . Wigton , R . S . , K . D . Patti , V . L . Hoellerich . 1986 . The effect of feed - back in learning clinical diagnosis . J . Medical Ed . 61 ( October ) 816 – 822 .