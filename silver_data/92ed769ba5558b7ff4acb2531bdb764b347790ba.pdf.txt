Measuring Agreement on Set - Valued Items ( MASI ) for Semantic and Pragmatic Annotation Rebecca Passonneau Columbia University becky @ cs . columbia . edu Introduction Semantic annotation faces an inherent dilemma : in annotation judgments , as in usage , there are few blacks and whites and numerous shadings of gray . To capture gradations in meaning or function , annotation projects can allow annotators to make multiple selections when no single selection seems sufficient [ 1 ] . Another approach is to create weighted categories where frequency across multiple annotators represents stronger or weaker presence of pragmatic units [ 3 ] , or semantic ones [ 4 ] . To assess the quality of such annotations requires metrics that can grade the similarity of annotators ' judgments , in contrast to conventional inter - annotator measures . In [ 6 ] , a set - based weighting metric for use in measuring inter - annotator agreement was described , and results were compared with other measures of agreement on coreference annotation . This paper will further develop the proposed metric , referred to here as MASI ( Measuring Agreement on Set - Valued Items ) , by illustrating its application to a semantic annotation task pertaining to the evaluation of summarization [ 4 ] . The proposal presented in [ 6 ] consisted of two parts . First , it was shown that agreement on coreference annotations could be evaluated by treating sets of coreferential expressions as the annotation values . The second part of the proposal used Krippendorff ' s Alpha [ 5 ] , an agreement coefficient that can be weighted so as to capture differences in the size of disagreements . However , a set - based weighting was needed . MASI was proposed as a metric of similarity between sets that represent semantic or pragmatic judgments . Section two of the full paper will describe the pyramid annotation data so as to motivate the use of MASI , and present agreement results . As noted in [ 7 ] , it seems necessary to rely on weighted metrics for semantic and pragmatic data , but it is not clear how to compare performance from different weightings , or across projects . Section three of the full paper will discuss results from two other semantic annotation efforts in order to present general guidelines for assessing gradations in semantic and pragmatic annotation . Pyramid Annotation Data To create a pyramid , annotators begin with model summaries of the same source texts , and select sets of text spans that express the same meaning across summaries [ 4 ] . Each set is referred to as a Summary Content Unit ( SCU ) , and receives a label for mnemonic purposes . An SCU has a weight corresponding to the number of summaries that express the SCUâ€™s meaning . Figure 1 illustrates an overlay of matching SCUs from two annotators , from summaries a through f . Boldface indicates text selected by both annotators . Text spans in italics are labeled A1 or A2 to indicate which single annotator selected them . SCU : The cause of an airline crash over Nova Scotia has not been determined a . The cause of the Sept . 2 , 1998 A2 crash has not been determined . b . searched for clues as to a cause A2 but refrained from naming one . c . The cause has not been determined , d . The specific cause of the tragedy was never determined e . but investigators remain unsure of its cause . f . A final determination of the crashes cause is still far off . A1 Figure 1 . Overlay of matching SCUs from two annotators In [ 8 ] , agreement on different dimensions of factoid annotation , which is similar to SCU annotation , are calculated separately using a conventional agreement metric . Here a single measure is computed for the entire annotation , and is computed on a per token basis . Each token is selected to be in at most one SCU ; typically , all but a few words are selected . When annotator i assigns token j to SCU k , the coding value for token j is SCU k - { j } . For comparing any pair of coding values selected by different annotators , MASI uses a weighting involving the Jaccard coefficient , the ratio of the cardinalities of the set intersection and set union , and a parameter for semantic monotonicity that ranges from zero to one , depending on whether two sets A and B are equal , A subsumes B , A and B have a non - empty intersection set C distinct from A and B , or finally , A and B are disjoint . Details will be provided in the full paper . Table 1 shows the inter - annotator agreement on 5 pyramids for two annotators ; each pyramid consists of seven model summaries , for a total of about 725 words each . Pyramid Alpha / MASI D30016 0 . 79 D30040 0 . 80 D31001 0 . 68 D31010 0 . 69 D31038 0 . 71 Table 1 . Interannotator agreement on 5 pyramids using Alpha / MASI . The results show good agreement . In addition , correlation of agreement values with other characteristics of the pyramids provides independent evidence for semantic differences across the different sets of model summaries . Conclusion The question generally arises as to how much agreement is sufficient . By pointing to the use of MASI on a distinct annotation task involving [ 1 ] , we show that there are important functions of agreement measures apart from showing whether the annotators reach a target threshold . By comparing MASI with other weightings where there is an extrinsic means of assessing annotations [ 2 ] , we argue that identification of an appropriate threshold agreement value can be contingent on the purpose of the annotation . References [ 1 ] Interlingual Annotation of Multilingual Text Corpora . http : / / aitc . aitcnet . org / nsf / iamtc / . [ 2 ] Passonneau , Rebecca ; Nenkova , Ani ; McKeown , Kathleen ; Sigelman , Sergey . 2005 . Applying the pyramid method in DUC 2005 . Document Understanding Conference Workshop . [ 3 ] Passonneau , Rebecca and Diane Litman . 1997 . Discourse Segmentation by Human and Automated Means . Computational Linguistics 23 . 1 : 103 - 139 . [ 4 ] Nenkova , Ani and Rebecca Passonneau . 2004 . Evaluating Content Selection in Summarization : The Pyramid Method . Proceedings of the Joint Annual Meeting of Human Language Technology ( HLT ) and the North American chapter of the Association for Computation al Linguistics ( NACL ) . [ 5 ] Krippendorff , Klaus . 1980 . Content Analysis . Sage Publications . [ 6 ] Passonneau , Rebecca . 2004 . Computing Reliability for Coreference Annotation . Proceedings of the International Conference on Language Resources and Evaluation ( LREC ) . Portugal . [ 7 ] Artstein , Ron and Massimo Poesio . 2005 . Kappa 3 = Alpha ( or Beta ) . University of Essex NLE Technote 2005 - 01 . [ 8 ] Teufel , Simone and Hans van Halteren , 2004 : Evaluating information content by factoid analysis : human annotation and stability EMNLP - 04 , Barcelona .