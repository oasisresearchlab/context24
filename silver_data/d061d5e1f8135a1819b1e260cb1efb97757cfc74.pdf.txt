Seed - Guided Topic Discovery with Out - of - Vocabulary Seeds Yu Zhang 1 , Yu Meng 1 , Xuan Wang 1 , Sheng Wang 2 , Jiawei Han 1 1 University of Illinois at Urbana - Champaign , IL , USA 2 University of Washington , Seattle , WA , USA { yuz9 , yumeng5 , xwang174 , hanj } @ illinois . edu swang @ cs . washington . edu Abstract Discovering latent topics from text corpora has been studied for decades . Many existing topic models adopt a fully unsupervised setting , and their discovered topics may not cater to users’ particular interests due to their inability of leveraging user guidance . Although there exist seed - guided topic discovery approaches that leverage user - provided seeds to discover topic - representative terms , they are less concerned with two factors : ( 1 ) the existence of out - of - vocabulary seeds and ( 2 ) the power of pre - trained language models ( PLMs ) . In this paper , we generalize the task of seed - guided topic discovery to allow out - of - vocabulary seeds . We propose a novel framework , named S EE - T OPIC , wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually beneﬁt each other . Experiments on three real datasets from differ - ent domains demonstrate the effectiveness of S EE T OPIC in terms of topic coherence , accu - racy , and diversity . 1 1 Introduction Automatically discovering informative and coher - ent topics from massive text corpora is central to text analysis through helping users efﬁciently di - gest a large collection of documents ( Grifﬁths and Steyvers , 2004 ) and advancing downstream appli - cations such as summarization ( Wang et al . , 2009 , 2022 ) , classiﬁcation ( Chen et al . , 2015 ; Meng et al . , 2020b ) , and generation ( Liu et al . , 2021 ) . Unsupervised topic models have been the main - stream approach to topic discovery since the pro - posal of pLSA ( Hofmann , 1999 ) and LDA ( Blei et al . , 2003 ) . Despite their encouraging perfor - mance in ﬁnding informative latent topics , these topics may not reﬂect user preferences well , mainly due to their unsupervised nature . For example , given a collection of product reviews , a user may be speciﬁcally interested in product categories 1 The code and datasets are available at https : / / github . com / yuzhimanhua / SeeTopic . Table 1 : Three datasets ( Cohan et al . , 2020 ; McAuley and Leskovec , 2013 ; Zhang et al . , 2017 ) from different domains and their topic categories ( i . e . , seeds ) . Red : Seeds never seen in the corpus ( i . e . , out - of - vocabulary ) . In all three datasets , a large proportion of seeds are out - of - vocabulary . Dataset Category Names ( Seeds ) SciDocs ( ScientiﬁcPapers ) cardiovascular diseases chronic kidney disease chronic respiratory diseases diabetes mellitus digestive diseases hiv / aids hepatitis a / b / c / e mental disorders musculoskeletal disorders neoplasms ( cancer ) neurological disorders Amazon ( ProductReviews ) apps for android books cds and vinyl clothing , shoes and jewelry electronics health and personal care home and kitchen movies and tv sports and outdoors video games Twitter ( SocialMediaPosts ) food shop and service travel and transport college and university nightlife spot residence outdoors and recreation arts and entertainment professional and other places ( e . g . , “ books ” , “ electronics ” ) , but unsupervised topic models may generate topics containing dif - ferent sentiments ( e . g . , “ good ” , “ bad ” ) . To con - sider users’ interests and needs , seed - guided topic discovery approaches ( Jagarlamudi et al . , 2012 ; Gallagher et al . , 2017 ; Meng et al . , 2020a ) have been proposed to ﬁnd representative terms for each category based on user - provided seeds or category names . 2 However , there are still two less concerned factors in these approaches . The Existence of Out - of - Vocabulary Seeds . Pre - vious studies ( Jagarlamudi et al . , 2012 ; Gallagher et al . , 2017 ; Meng et al . , 2020a ) assume that all user - provided seeds must be in - vocabulary ( i . e . , appear at least once in the input corpus ) , so that they can utilize the occurrence statistics or Skip - Gram embedding methods ( Mikolov et al . , 2013 ) to model seed semantics . However , user - interested categories can have speciﬁc or composite descrip - tions , which may never appear in the corpus . Table 1 shows three datasets from different domains : sci - 2 In this paper , we use “seeds” and “category names” inter - changeably . a r X i v : 2205 . 01845v1 [ c s . C L ] 4 M a y 2022 entiﬁc papers , product reviews , and social media posts . In each dataset , documents can belong to one or more categories , and we list the category names provided by the dataset collectors . These seeds should reﬂect their particular interests . In all three datasets , we have a large proportion of seeds ( 45 % in SciDocs , 60 % in Amazon , and 78 % in Twitter ) that never appear in the corpus . Some category names are too speciﬁc ( e . g . , “ chronic respiratory diseases ” , “ nightlife spot ” ) to be exactly matched , others are the composition of multiple entities ( e . g . , “ hepatitis a / b / c / e ” , “ neoplasms ( cancer ) ” , “ clothing , shoes and jewelry ” ) . 3 The Power of Pre - trained Language Models . Techniques used in previous studies are mainly based on LDA variants ( Jagarlamudi et al . , 2012 ) or context - free embeddings ( Meng et al . , 2020a ) . Re - cently , pre - trained language models ( PLMs ) such as BERT ( Devlin et al . , 2019 ) have achieved signif - icant improvement in a wide range of text mining tasks . In topic discovery , the generic representation power of PLMs learned from web - scale corpora ( e . g . , Wikipedia or PubMed ) may complement the information a model can obtain from the input cor - pus . Moreover , out - of - vocabulary seeds usually have meaningful in - vocabulary components ( e . g . , “ night ” and “ life ” in “ nightlife spot ” , “ health ” and “ care ” in “ health and personal care ” ) . The opti - mized tokenization strategy of PLMs ( Sennrich et al . , 2016 ; Wu et al . , 2016 ) can help segment the seeds into such meaningful components ( e . g . , “ nightlife ” → “ night ” and “ # # life ” ) , and the contex - tualization power of PLMs can help infer the cor - rect meaning of each component ( e . g . , “ # # life ” and “ care ” ) in the category name . Therefore , PLMs are much needed in handling out - of - vocabulary seeds and effectively learning their semantics . Contributions . Being aware of these two factors , in this paper , we study seed - guided topic discovery in the presence of out - of - vocabulary seeds . Our proposed S EE T OPIC framework consists of two modules : ( 1 ) The general representation module 3 One possible idea to deal with composite seeds is to split them into multiple seeds . However , there are many possible ways to express the conjunctions ( e . g . , “ / ” , “ ( ) ” , “ , ” and “ and ” in Table 1 ) , which may require manual tuning . Besides , simple chunking rules will induce splits that break the semantics of the original composition ( e . g . , “ professional and other places ” may be split into “ professional ” and “ other places ” ) . Moreover , even after the split , some seeds are still out - of - vocabulary . Therefore , we propose to use PLMs to tackle out - of - vocabulary seeds in a uniﬁed way . In experiments , we will show that our model is able to tackle composite seeds . For example , given the seed “ hepatitis a / b / c / e ” , we can ﬁnd terms relevant to “ hepatitis b ” and “ hepatitis c ” ( see Table 4 ) . uses a PLM to derive the representation of each term ( including out - of - vocabulary seeds ) based on the general linguistic knowledge acquired through pre - training . ( 2 ) The seed - guided local representa - tion module learns in - vocabulary term embeddings speciﬁc to the input corpus and the given seeds . In order to optimize the learned representations for topic coherence , which is commonly reﬂected by pointwise mutual information ( PMI ) ( Newman et al . , 2010 ) , our objective implicitly maximizes the PMI between each word and its context , the documents it appears , as well as the category it belongs to . The learning of the two modules is connected through an iterative ensemble ranking process , in which the general knowledge of PLMs and the term representations speciﬁcally learned from the target corpus conditioned on the seeds can complement each other . To summarize , this study makes three contri - butions . ( 1 ) Task : we propose to study seed - guided topic discovery in the presence of out - of - vocabulary seeds . ( 2 ) Framework : we design a uni - ﬁed framework that jointly models general knowl - edge through PLMs and local corpus statistics through embedding learning . ( 3 ) Experiment : ex - tensive experiments on three datasets demonstrate the effectiveness of S EE T OPIC in terms of topic coherence , accuracy , and diversity . 2 Problem Deﬁnition As shown in Table 1 , we assume a seed can be either a single word or a phrase . Given a corpus D , we use V D to denote the set of terms appearing in D . In accordance with the assumption of category names , each term can also be a single word or a phrase . In practice , given a raw corpus , one can use existing phrase chunking tools ( Manning et al . , 2014 ; Shang et al . , 2018 ) to detect phrases in it . After phrase chunking , if a category name is still not in V D , we deﬁne it as out - of - vocabulary . Problem Deﬁnition . Given a corpus D = { d 1 , . . . , d | D | } and a set of category names C = { c 1 , . . . , c | C | } where some category names are out - of - vocabulary , the task is to ﬁnd a set of in - vocabulary terms S i = { w 1 , . . . , w S } ⊆ V D for each category c i such that each term in S i is se - mantically close to c i and far from other categories c j ( ∀ j (cid:54) = i ) . 3 The S EE T OPIC Framework In this section , we ﬁrst introduce how we model general and local text semantics using a PLM mod - ule and a seed - guided embedding learning module , respectively . Then , we present the iterative ensem - ble ranking process and our overall framework . 3 . 1 Modeling General Text Semantics using a PLM PLMs such as BERT ( Devlin et al . , 2019 ) aim to learn generic language representations from web - scale corpora ( e . g . , Wikipedia or PubMed ) that can be applied to a wide variety of text - related applications . To transfer such general knowledge to our topic discovery task , we employ a PLM to encode each category name and each in - vocabulary term to a vector . To be speciﬁc , given a term w ∈ C ∪V D , we input the sequence “ [ CLS ] w [ SEP ] ” into the PLM . Here , w can be a phrase containing multiple words , and each word can be out of the PLM’s vocabulary . To deal with this , most PLMs use a pre - trained tokenizer ( Sennrich et al . , 2016 ; Wu et al . , 2016 ) to segment each unseen word into frequent subwords . Then , the contextualization power of PLMs will help infer the correct meaning of each word / subword , so as to provide a more precise representation of the whole category . After LM encoding , following ( Sia et al . , 2020 ; Thompson and Mimno , 2020 ; Li et al . , 2020 ) , we take the output of all tokens from the last layer and average them to get the term embedding e w . In this way , even if a seed c i is out - of - vocabulary , we can still obtain its representation e c i . 3 . 2 Modeling Local Text Semantics in the Input Corpus The motivation of topic discovery is to discover latent topic structures from the input corpus . There - fore , purely relying on general knowledge in the PLM is insufﬁcient because topic discovery results should adapt to the input corpus D . Now , we in - troduce how we learn another set of embeddings { u w | w ∈ V D } from D . Previous studies on embedding learning assume that the semantic of a term is similar to its local context ( Mikolov et al . , 2013 ) , the document it appears ( Tang et al . , 2015 ; Xun et al . , 2017a ) , and the category it belongs to ( Meng et al . , 2020a ) . Inspired by these studies , we propose the following embedding learning objective . J = (cid:88) d ∈D (cid:88) w i ∈ d (cid:88) w j ∈C ( w i , h ) p ( w j | w i ) (cid:124) (cid:123)(cid:122) (cid:125) context + (cid:88) d ∈D (cid:88) w ∈ d p ( d | w ) (cid:124) (cid:123)(cid:122) (cid:125) document + (cid:88) c i ∈C (cid:88) w ∈S i p ( c i | w ) (cid:124) (cid:123)(cid:122) (cid:125) category , ( 1 ) where p ( z | w ) = exp ( u Tw v z ) (cid:80) z (cid:48) exp ( u Tw v z (cid:48) ) , ( z can be w j , d , or c i ) . ( 2 ) In this objective , u w i ( and v w j ) , v d , v c i are the embedding vectors of terms , documents , and cate - gories , respectively . C ( w i , h ) is the set of context terms of w i in d . Speciﬁcally , if d = w 1 w 2 . . . w L , then C ( w i , h ) = { w j | i − h ≤ j ≤ i + h , j (cid:54) = i } , where h is the context window size . Note that the last term in Eq . ( 1 ) encourages the similarity between each category c i and its rep - resentative terms S i . Here , we adopt an iterative process to gradually update category - representative terms . Initially , S i consists of just a few in - vocabulary terms similar to c i according to the PLM . At each iteration , the size of S i will increase to contain more category - discriminative terms ( the selection criterion of these terms will be introduced in the next section ) , and we need to encourage their proximity with c i in the next iteration . Directly optimizing the full softmax in Eq . ( 2 ) is costly . Therefore , we adopt the negative sam - pling strategy ( Mikolov et al . , 2013 ) for efﬁcient approximation . Interpreting the Objective . In topic modeling studies , pointwise mutual information ( PMI ) ( New - man et al . , 2010 ) is a standard evaluation metric for topic coherence ( Lau et al . , 2014 ; Röder et al . , 2015 ) . Levy and Goldberg ( 2014 ) prove that the Skip - Gram embedding model is implicitly factoriz - ing the PMI matrix . Following their proof , we can show that maximizing Eq . ( 1 ) is implicitly doing the following factorization : U Tw [ V w ; V d ; V c ] = [ X ww ; X wd ; X wc ] , ( 3 ) where the columns of U w , V w , V d , V c are u w i , v w j , v d , v c i , respectively ( w i , w j ∈ V D , d ∈ D , c i ∈ C ) ; X ww , X wd , and X wc are PMI matrices . X ww = (cid:34) log (cid:16) # D ( w i , w j ) · λ D # D ( w i ) · # D ( w j ) · b (cid:17)(cid:35) w i , w j ∈V D , X wd = (cid:34) log (cid:16) # d ( w ) · λ D # D ( w ) · λ d · b (cid:17)(cid:35) w ∈V D , d ∈D , X wc = (cid:2) x w , c i (cid:3) w ∈V D , c i ∈C , where x w , c i = (cid:26) log | C | b , if w ∈ S i , −∞ , if w ∈ S j ( ∀ j (cid:54) = i ) . ( 4 ) Here , # D ( w i , w j ) denotes the number of co - occurrences of w i and w j in a context window in D ; # D ( w ) denotes the number of occurrences of w in D ; λ D is the total number of terms in D ; # d ( w ) denotes the number of times w occurs in d ; λ d is the total number of terms in d ; b is the number of negative samples . ( For the derivation of Eq . ( 3 ) , please refer to Appendix A . ) To summarize , the learned local representations u w are implicitly optimized for topic coherence , where term co - occurrences are measured in context , document , and category levels . 3 . 3 Ensemble Ranking We have obtained two sets of term embeddings that model text semantics from different angles : { e w | w ∈ C ∪ V D } carries the PLM’s knowledge , while { u w | w ∈ V D } models the input corpus as well as user - provided seeds . We now propose an ensemble ranking method to leverage information from both sides to grab more discriminative terms for each category . Given a category c i and its current term set S i , we ﬁrst calculate the scores of each term w ∈ V D . score G ( w | S i ) = 1 | S i | (cid:88) w (cid:48) ∈S i cos ( e w , e w (cid:48) ) , score L ( w | S i ) = 1 | S i | (cid:88) w (cid:48) ∈S i cos ( u w , u w (cid:48) ) . ( 5 ) The subscript “ G ” here means “general” , while “ L ” means “local” . Then , we sort all terms by these two scores , respectively . Each term w will hence get two rank positions rank G ( w ) and rank L ( w ) . We propose the following ensemble score based on the reciprocal rank : score ( w | S i ) = (cid:18) 1 2 (cid:16) 1 rank G ( w ) (cid:17) ρ + 1 2 (cid:16) 1 rank L ( w ) (cid:17) ρ (cid:19) 1 / ρ . ( 6 ) Here , 0 < ρ ≤ 1 is a constant . In practice , in - stead of ranking all terms in the vocabulary , we only check the top - M results in the two ranking lists . If a term w is not among the top - M ac - cording to score G ( w ) ( resp . , score L ( w ) ) , we set rank G ( w ) = + ∞ ( resp . , rank L ( w ) = + ∞ ) . In fact , when ρ = 1 , Eq . ( 6 ) becomes the arith - metic mean of the two reciprocal ranks 1 rank G ( w ) and 1 rank L ( w ) . This is essentially the mean recip - rocal rank ( MRR ) commonly used in ensemble ranking , where a high position in one ranking list can largely compensate a low position in the other . In contrast , when ρ → 0 , Eq . ( 6 ) becomes the geometric mean of the two reciprocal ranks ( see Appendix B ) , where two ranking lists both have the “veto power” ( i . e . , a term needs to be ranked as top - M in both ranking lists to obtain a non - zero Algorithm 1 : S EE T OPIC Input : A text corpus D = { d 1 , . . . , d | D | } , a set of seeds C = { c 1 , . . . , c | C | } , and a PLM . Output : ( S 1 , . . . , S | C | ) , where each S i is a set of category - discriminative terms for c i . 1 Compute { e w | w ∈ C ∪ V D } using the PLM ; 2 / / Initialize S i ; 3 S 1 , . . . , S | C | ← ∅ ; 4 for n ← 1 to N do 5 for i ← 1 to | C | do 6 w n ← arg max w ∈V D \ ( S 1 ∪ . . . ∪S | C | ) cos ( e w , e c i ) ; 7 S i ← S i ∪ { w n } ; 8 / / Update S i for T iterations ; 9 for t ← 1 to T do 10 Learn { u w | w ∈ V D } from the input corpus D and the up - to - date representative terms S 1 , . . . , S | C | according to Eq . ( 1 ) ; 11 score G ( w | S i ) and score L ( w | S i ) ← Eq . ( 5 ) ; 12 score ( w | S i ) ← Eq . ( 6 ) ; 13 S 1 , . . . , S | C | ← ∅ ; 14 for n ← 1 to ( t + 1 ) N do 15 for i ← 1 to | C | do 16 S i ← Eq . ( 7 ) ; 17 Return ( S 1 , . . . , S | C | ) ; ensemble score ) . In experiment , we set ρ = 0 . 1 and show it outperforms MRR ( i . e . , ρ = 1 ) in our topic discovery task . After computing the ensemble score score ( w | S i ) for each w , we update S i . To guarantee that each S i is category - discriminative , we do not allow any term to belong to more than one category . There - fore , we gradually expand each S i by turns . At the beginning , we reset S 1 = . . . = S | C | = ∅ . When it is S i ’s turn , we add one term S i according to the following criterion : S i ← S i ∪ { arg max w ∈V D \ ( S 1 ∪ . . . ∪S | C | ) score ( w | S i ) } . ( 7 ) 3 . 4 Overall Framework We summarize the entire S EE T OPIC framework in Algorithm 1 . To deal with out - of - vocabulary cat - egory names , we ﬁrst utilize a PLM to ﬁnd their nearest in - vocabulary terms as the initial category - discriminative term set S i ( Lines 1 - 7 ) . After ini - tialization , | S i | = N ( ∀ 1 ≤ i ≤ | C | ) . Note that for an in - vocabulary category name c i ∈ V D , itself will be added to the initial S i as the top - 1 similar in - vocabulary term . After getting the initial S i , we update it by T it - erations ( Lines 8 - 16 ) . At each iteration , according to the up - to - date S 1 , S 2 , . . . , S | C | , we relearn embed - dings u w , v w , v d , and v c i using Eq . ( 1 ) ( Line 10 ) . The two set of embeddings , { e w | w ∈ C ∪ V D } ( computed at Line 1 ) and { u w | w ∈ V D } ( up - dated at Line 10 ) , are then leveraged to perform ensemble ranking ( Lines 11 - 12 ) . Based on the ensemble score score ( w | S i ) , we update S i using Eq . ( 7 ) ( Lines 13 - 16 ) . After the t - th iteration , | S i | = ( t + 1 ) N ( ∀ 1 ≤ i ≤ | C | ) . Complexity Analysis . The time complexity of using the PLM is O ( ( | C | + | V D | ) α PLM ) , where α PLM is the complexity of encoding one term via the PLM . The total complexity of local embed - ding is O ( T λ D ( h + | C | ) b ) because in each iteration 1 ≤ t ≤ T , every w ∈ D interacts with every other term in the context window of size h , its belong - ing document , and each category c i ∈ C , and each update involves b negative samples . The total com - plexity of ensemble ranking is O ( T | V D | | C | | S i | ) as in each iteration 1 ≤ t ≤ T , we compute scores between each w ∈ V D and each w (cid:48) ∈ S i ( ∀ c i ∈ C ) . 4 Experiments 4 . 1 Experimental Setup Datasets . We conduct experiments on three pub - lic datasets from different domains : ( 1 ) SciDocs ( Cohan et al . , 2020 ) 4 is a large collection of sci - entiﬁc papers supporting diverse evaluation tasks . For the MeSH classiﬁcation task ( Coletti and Ble - ich , 2001 ) , about 23K medical papers are collected , each of which is assigned to one of the 11 common disease categories derived from the MeSH vocabu - lary . We use the title and abstract of each paper as documents and the 11 category names as seeds . ( 2 ) Amazon ( McAuley and Leskovec , 2013 ) 5 contains product reviews from May 1996 to July 2014 . Each Amazon review belongs to one or more product cat - egories . We use the subset sampled by Zhang et al . ( 2020 , 2022 ) , which contains 10 categories and 100K reviews . ( 3 ) Twitter ( Zhang et al . , 2017 ) 6 is a crawl of geo - tagged tweets in New York City from August 2014 to November 2014 . The dataset collectors link these tweets with Foursquare’s POI database and assign them to 9 POI categories . We take these category names as input seeds . Seeds used in the three datasets are shown in Table 1 . Dataset statistics are summarized in Ta - ble 2 . For all three datasets , we use AutoPhrase ( Shang et al . , 2018 ) 7 to perform phrase chunking in the corpus , and we remove words and phrases occurring less than 3 times . Previous studies ( Jagarlamudi et al . , 2012 ; Meng et al . , 2020a ) have tried some other datasets ( e . g . , RCV1 , 20 Newsgroups , NYT , and Yelp ) . However , the category names they use in these datasets are 4 https : / / github . com / allenai / scidocs 5 http : / / jmcauley . ucsd . edu / data / amazon / index . html 6 https : / / github . com / franticnerd / geoburst 7 https : / / github . com / shangjingbo1226 / AutoPhrase Table 2 : Dataset Statistics . Dataset SciDocs Amazon Twitter # Documents 23 , 473 100 , 000 135 , 529 # In - vocabulary Terms ( After Phrase Chunking ) 55 , 897 56 , 942 17 , 577 Avg Doc Length 239 . 8 119 . 0 6 . 7 # Seeds 11 10 9 # Out - of - vocabulary Seeds ( After Phrase Chunking ) 5 6 7 all picked from in - vocabulary terms . Therefore , we do not consider these datasets for evaluation in our task settings . Following ( Sia et al . , 2020 ) , we adopt a 60 - 40 train - test split for all three datasets . The training set is used as the input corpus D , and the testing set is used for calculating topic coherence metrics ( see evaluation metrics for details ) . Compared Methods . We compare our S EE T OPIC framework with the following methods , includ - ing seed - guided topic modeling methods , seed - guided embedding learning methods , and PLMs . ( 1 ) SeededLDA ( Jagarlamudi et al . , 2012 ) 8 is a seed - guided topic modeling method . It improves LDA by biasing topics to produce input seeds and by biasing documents to select topics relevant to the seeds they contain . ( 2 ) Anchored CorEx ( Gallagher et al . , 2017 ) 9 is a seed - guided topic modeling method . It incorporates user - provided seeds by balancing between compressing the in - put corpus and preserving seed - related informa - tion . ( 3 ) Labeled ETM ( Dieng et al . , 2020 ) 10 is an embedding - based topic modeling method . It in - corporates distributed representation of each term . Following ( Meng et al . , 2020a ) , we retrieve repre - sentative terms according to their embedding sim - ilarity with the category name . ( 4 ) CatE ( Meng et al . , 2020a ) 11 is a seed - guided embedding learn - ing method for discriminative topic discovery . It takes category names as input and jointly learns term embedding and speciﬁcity from the input cor - pus . Category - discriminative terms are then se - lected based on both embedding similarity with the category and speciﬁcity . ( 5 ) BERT ( Devlin et al . , 2019 ) 12 is a PLM . Following Lines 1 - 7 in Algorithm 1 , we use BERT to encode each input category name and each term to a vector , and then perform similarity search to directly ﬁnd all repre - 8 https : / / github . com / vi3k6i5 / GuidedLDA 9 https : / / github . com / gregversteeg / corex _ topic 10 https : / / github . com / adjidieng / ETM 11 https : / / github . com / yumeng5 / CatE 12 https : / / huggingface . co / bert - base - uncased Table 3 : NPMI , LCP , MACC , and Diversity of compared algorithms on three datasets . NPMI and LCP measure topic coherence ; MACC measures term accuracy ; Diversity ( abbreviated to Div . ) measures topic diversity . Bold : the highest score . Underline : the second highest score . ∗ : signiﬁcantly worse than S EE T OPIC ( p - value < 0 . 05 ) . ∗∗ : signiﬁcantly worse than S EE T OPIC ( p - value < 0 . 01 ) . Methods SciDocs Amazon Twitter NPMI LCP MACC Div . NPMI LCP MACC Div . NPMI LCP MACC Div . SeededLDA 0 . 056 ∗∗ - 0 . 616 0 . 156 ∗∗ 0 . 451 ∗∗ 0 . 070 ∗∗ - 0 . 753 0 . 147 ∗∗ 0 . 393 ∗∗ 0 . 013 ∗∗ - 2 . 254 ∗∗ 0 . 195 ∗∗ 0 . 696 ∗∗ Anchored CorEx 0 . 106 ∗∗ - 1 . 090 ∗∗ 0 . 264 ∗∗ 1 . 000 0 . 134 ∗∗ - 0 . 982 ∗ 0 . 333 ∗∗ 1 . 000 0 . 090 ∗∗ - 2 . 192 ∗∗ 0 . 233 ∗∗ 1 . 000 Labeled ETM 0 . 334 ∗ - 0 . 775 ∗∗ 0 . 458 ∗∗ 0 . 961 ∗ 0 . 308 ∗∗ - 1 . 051 ∗∗ 0 . 585 ∗∗ 1 . 000 0 . 305 ∗ - 1 . 098 ∗∗ 0 . 268 ∗∗ 0 . 989 CatE 0 . 345 ∗ - 0 . 725 ∗∗ 0 . 633 ∗∗ 1 . 000 0 . 317 ∗∗ - 0 . 844 ∗∗ 0 . 856 ∗ 1 . 000 0 . 356 - 0 . 827 0 . 483 ∗∗ 1 . 000 BERT 0 . 313 ∗∗ - 0 . 841 ∗∗ 0 . 740 ∗∗ 0 . 891 ∗∗ 0 . 294 ∗∗ - 1 . 093 ∗∗ 0 . 832 ∗∗ 1 . 000 0 . 313 ∗∗ - 1 . 044 ∗∗ 0 . 627 0 . 944 ∗∗ BioBERT 0 . 309 ∗∗ - 0 . 852 ∗∗ 0 . 938 0 . 982 ∗∗ – – – – – – – – S EE T OPIC - NoIter 0 . 341 ∗∗ - 0 . 768 ∗∗ 0 . 887 1 . 000 0 . 322 ∗∗ - 0 . 986 ∗∗ 0 . 892 1 . 000 0 . 318 - 1 . 004 ∗∗ 0 . 618 1 . 000 S EE T OPIC 0 . 358 - 0 . 634 0 . 909 1 . 000 0 . 342 - 0 . 696 0 . 904 1 . 000 0 . 320 - 0 . 907 0 . 633 1 . 000 sentative terms . ( 6 ) BioBERT ( Lee et al . , 2020 ) 13 is a PLM . It is used in the same way as BERT . Since BioBERT is speciﬁcally trained for biomedi - cal text mining tasks , we report its performance on the SciDocs dataset only . ( 7 ) S EE T OPIC - NoIter is a variant of our S EE T OPIC framework . In Algo - rithm 1 , after initialization ( Lines 1 - 7 ) , it executes Lines 9 - 16 only once ( i . e . , T = 1 ) to ﬁnd all repre - sentative terms . Here , all seed - guided topic modeling and em - bedding baselines ( i . e . , SeededLDA , Anchored CorEx , CatE , and Labeled ETM ) can only take in - vocabulary seeds as input . For a fair compar - ison , we run Lines 1 - 7 in Algorithm 1 to get the initial representative in - vocabulary terms for each category , and input these terms as seeds into the baselines . In other words , all compared methods use BERT / BioBERT to initialize their term sets . Evaluation Metrics . We evaluate topic discovery results from three different angles : topic coherence , term accuracy , and topic diversity . ( 1 ) NPMI ( Lau et al . , 2014 ) is a standard metric in topic modeling to measure topic coherence . Within each topic , it calculates the normalized pointwise mutual information for each pair of terms in S i . NPMI = 1 | C | | C | (cid:88) i = 1 1 (cid:0) | S i | 2 (cid:1) (cid:88) w j , w k ∈S i log P ( w j , w k ) P ( w j ) P ( w k ) − log P ( w j , w k ) , ( 8 ) where P ( w j , w k ) is the probability that w j and w k co - occur in a document ; P ( w j ) is the marginal probability of w j . 14 ( 2 ) LCP ( Mimno et al . , 2011 ) is another standard metric to measure topic coherence . It calculates the pairwise log conditional probability of top - ranked 13 https : / / huggingface . co / dmis - lab / biobert - v1 . 1 14 When calculating Eqs . ( 8 ) and ( 9 ) , to avoid log 0 , we use P ( w j , w k ) + (cid:15) and P ( w ) + (cid:15) to replace P ( w j , w k ) and P ( w ) , respectively , where (cid:15) = 1 / | D | . terms . LCP = 1 | C | | C | (cid:88) i = 1 1 (cid:0) | S i | 2 (cid:1) (cid:88) w j , w k ∈S i j < k log P ( w j , w k ) P ( w j ) . ( 9 ) Note that PMI ( Newman et al . , 2010 ) is also a stan - dard metric for topic coherence . We do observe that S EE T OPIC outperforms baselines in terms of PMI in most cases . However , since our local em - bedding step is implicitly optimizing a PMI - like objective , we no longer use it as our evaluation metric . ( 3 ) MACC ( Meng et al . , 2020a ) measures term ac - curacy . It is deﬁned as the proportion of retrieved terms that actually belong to the corresponding category according to the category name . MACC = 1 | C | | C | (cid:88) i = 1 1 | S i | (cid:88) w j ∈S i 1 ( w j ∈ c i ) , ( 10 ) where 1 ( w j ∈ c i ) is the indicator function of whether w j is relevant to category c i . MACC re - quires human evaluation , so we invite ﬁve anno - tators to perform independent annotation . The re - ported MACC score is the average MACC of the ﬁve annotators . A high inter - annotator agreement is observed , with Fleiss’ kappa ( Fleiss , 1971 ) being 0 . 856 , 0 . 844 , and 0 . 771 on SciDocs , Amazon , and Twitter , respectively . ( 4 ) Diversity ( Dieng et al . , 2020 ) measures the mutual exclusivity of discovered topics . It is the percentage of unique terms in all topics , which cor - responds to our task requirement that each retrieved term is discriminatively close to one category and far from the others . Diversity = | (cid:83) | C | i = 1 S i | (cid:80) | C | i = 1 | S i | . ( 11 ) Experiment Settings . We use BioBERT as the PLM on SciDocs , and BERT - base - uncased as the PLM on Amazon and Twitter . The embedding dimension of u w is 768 ( the same as e w ) ; the number of negative samples b = 5 . In ensem - ble ranking , the length of the general / local ranking list M = 100 ; the hyperparameter ρ in Eq . ( 6 ) is set as 0 . 1 ; the number of iterations T = 4 ; after each iteration , we increase the size of S i by N = 3 . We use the top - 10 ranked terms in each topic for ﬁnal evaluation ( i . e . , | S i | = 10 in Eqs . ( 8 ) - ( 11 ) ) . Experiments are run on Intel Xeon E5 - 2680 v2 @ 2 . 80GHz and one NVIDIA GeForce GTX 1080 . 4 . 2 Performance Comparison Table 3 shows the performance of all methods . We run each experiment 3 times with the average score reported . To show statistical signiﬁcance , we con - duct a two - tailed unpaired t - test to compare S EE - T OPIC and each baseline . ( The performance of BERT and BioBERT is deterministic according to our usage . When comparing S EE T OPIC with them , we conduct a two - tailed Z - test instead . ) The signif - icance level is also marked in Table 3 . We have the following observations from Table 3 . ( 1 ) Our S EE T OPIC model performs consistently well . In fact , it achieves the highest score in 8 columns and the second highest in the remaining 4 columns . ( 2 ) Classical seed - guided topic modeling baselines ( i . e . , SeededLDA and Anchored CorEx ) perform not well in respect of NPMI ( topic coher - ence ) and MACC ( term accuracy ) . Embedding - based topic discovery approaches ( i . e . , Labeled ETM and CatE ) make some progress , but they still signiﬁcantly underperform the PLM - empowered S EE T OPIC model on SciDocs and Amazon . ( 3 ) S EE T OPIC consistently performs better than S EE - T OPIC - NoIter on all three datasets , indicating the positive contribution of the proposed iterative pro - cess . ( 4 ) S EE T OPIC guarantees the mutual exclu - sivity of S 1 , . . . , S | C | . In comparison , SeededLDA , Labeled ETM , and BERT cannot guarantee such mutual exclusivity . In - vocabulary vs . Out - of - vocabulary . Figure 1 compares the MACC scores of different seed - guided topic discovery methods on in - vocabulary categories and out - of - vocabulary categories . We ﬁnd that the performance improvement of S EE - T OPIC upon baselines on out - of - vocabulary cat - egories is larger than that on in - vocabulary ones . For example , on Amazon , S EE T OPIC underper - forms CatE on in - vocabulary categories but outper - forms CatE on out - of - vocabulary ones ; on Twit - ter , the gap between S EE T OPIC and baselines be - 0 0 . 2 0 . 4 0 . 6 0 . 8 1 In - Vocab Out - of - Vocab M A CC SeededLDA Anchored CorEx ( a ) SciDocs 0 0 . 2 0 . 4 0 . 6 0 . 8 1 In - Vocab Out - of - Vocab M A CC Labeled ETM CatE SeeTopic ( b ) Amazon 0 0 . 2 0 . 4 0 . 6 0 . 8 In - Vocab Out - of - Vocab M A CC ( c ) Twitter Figure 1 : MACC of seed - guided topic discovery meth - ods on in - vocabulary categories and out - of - vocabulary categories . 0 . 3 0 . 32 0 . 34 0 . 36 0 . 1 0 . 3 0 . 5 0 . 7 0 . 9 N P M I ρ SciDocs Amazon Twitter ( a ) Effect of ρ on NPMI - 1 . 1 - 1 - 0 . 9 - 0 . 8 - 0 . 7 - 0 . 6 0 . 1 0 . 3 0 . 5 0 . 7 0 . 9 L C P ρ SciDocs Amazon Twitter ( b ) Effect of ρ on LCP 0 . 3 0 . 32 0 . 34 0 . 36 0 . 38 1 2 3 4 5 N P M I T SciDocs Amazon Twitter ( c ) Effect of T on NPMI - 1 . 1 - 1 - 0 . 9 - 0 . 8 - 0 . 7 - 0 . 6 1 2 3 4 5 L C P T SciDocs Amazon Twitter ( d ) Effect of T on LCP Figure 2 : Parameter study of S EE T OPIC measured by topic coherence . comes much more evident on out - of - vocabulary categories . Note that all baselines in Figure 1 do not utilize the power of PLMs , so this observation validates our claim that PLMs are helpful in tack - ling out - of - vocabulary seeds . 4 . 3 Parameter Study We study the effect of two important hyperparame - ters : ρ ( the hyperparameter in ensemble ranking ) and T ( the number of iterations ) . We vary the value of ρ in { 0 . 1 , 0 . 3 , 0 . 5 , 0 . 7 , 0 . 9 , 1 } ( S EE T OPIC uses ρ = 0 . 1 by default ) and the value of T in { 1 , 2 , 3 , 4 , 5 } ( S EE T OPIC uses T = 4 by default , and S EE T OPIC - NoIter is the case when T = 1 ) . Figure 2 shows the change of model performance measured by NPMI and LCP . Table 4 : Top - 5 representative terms retrieved by different algorithms for three out - of - vocabulary categories from SciDocs , Amazon , and Twitter . (cid:51) : at least 3 of the 5 annotators judge the term as relevant to the seed . (cid:55) : at most 2 of the 5 annotators judge the term as relevant to the seed . Method Top - 5 Representative Terms Dataset : SciDocs , Category Name : hepatitis a / b / c / e SeededLDA patients ( (cid:55) ) , treatment ( (cid:55) ) , placebo ( (cid:55) ) , study ( (cid:55) ) , group ( (cid:55) ) Anchored CorEx expression ( (cid:55) ) , gene ( (cid:55) ) , cells ( (cid:55) ) , genes ( (cid:55) ) , genetic ( (cid:55) ) Labeled ETM hepatitis b virus hbv dna ( (cid:51) ) , serum hbv dna ( (cid:51) ) , serum alanine aminotransferase ( (cid:55) ) , alanine aminotransferase alt ( (cid:55) ) , below detection limit ( (cid:55) ) CatE chronic hepatitis b virus hbv infection ( (cid:51) ) , hepatitis b e antigen hbeag ( (cid:51) ) , hepatitis b virus hbv dna ( (cid:51) ) , normal alanine aminotransferase ( (cid:55) ) , hbeag - negative chronic hepatitis b ( (cid:51) ) BioBERT hepatitis b virus hbv dna ( (cid:51) ) , chronic hepatitis b virus hbv infection ( (cid:51) ) , hepatitis b e antigen hbeag ( (cid:51) ) , hepatitis b virus hbv infection ( (cid:51) ) , chronic hepatitis c virus hcv ( (cid:51) ) S EE T OPIC - NoIter hepatitis b virus hbv dna ( (cid:51) ) , hepatitis b e antigen hbeag ( (cid:51) ) , chronic hepatitis b virus hbv infection ( (cid:51) ) , hepatitis b surface antigen hbsag ( (cid:51) ) , hbeag - negative chronic hepatitis b ( (cid:51) ) S EE T OPIC chronic hepatitis b virus hbv infection ( (cid:51) ) , hbeag - negative chronic hepatitis b ( (cid:51) ) , hepatitis c virus hcv - infected ( (cid:51) ) , hepatitis b virus hbv dna ( (cid:51) ) , chronic hepatitis c virus hcv ( (cid:51) ) Dataset : Amazon , Category Name : sports and outdoors SeededLDA use ( (cid:55) ) , good ( (cid:55) ) , one ( (cid:55) ) , product ( (cid:55) ) , like ( (cid:55) ) Anchored CorEx sports ( (cid:51) ) , use ( (cid:55) ) , size ( (cid:55) ) , wear ( (cid:55) ) , ﬁt ( (cid:51) ) Labeled ETM cars and tracks ( (cid:51) ) , tracks and cars ( (cid:51) ) , search options ( (cid:55) ) , championships ( (cid:55) ) , cool bosses ( (cid:55) ) CatE outdoorsmen ( (cid:51) ) , outdoor activities ( (cid:51) ) , cars and tracks ( (cid:51) ) , foot support ( (cid:51) ) , offers plenty ( (cid:55) ) BERT cars and tracks ( (cid:51) ) , outdoor activities ( (cid:51) ) , outdoorsmen ( (cid:51) ) , sports ( (cid:51) ) , sporting events ( (cid:51) ) S EE T OPIC - NoIter outdoorsmen ( (cid:51) ) , outdoor activities ( (cid:51) ) , cars and tracks ( (cid:51) ) , indoor soccer ( (cid:51) ) , bike riding ( (cid:51) ) S EE T OPIC canoeing ( (cid:51) ) , picnics ( (cid:51) ) , bike rides ( (cid:51) ) , bike riding ( (cid:51) ) , rafting ( (cid:51) ) Dataset : Twitter , Category Name : travel and transport SeededLDA nyc ( (cid:55) ) , new york ( (cid:55) ) , line ( (cid:51) ) , high ( (cid:55) ) , time square ( (cid:51) ) Anchored CorEx new york ( (cid:55) ) , post photo ( (cid:51) ) , new ( (cid:55) ) , day ( (cid:55) ) , today ( (cid:55) ) Labeled ETM tourism ( (cid:51) ) , theview ( (cid:51) ) , ﬁle ( (cid:55) ) , morning view ( (cid:51) ) , gma ( (cid:55) ) CatE maritime ( (cid:51) ) , tourism ( (cid:51) ) , natural history ( (cid:55) ) , scenery ( (cid:51) ) , elevate ( (cid:55) ) BERT maritime ( (cid:51) ) , tourism ( (cid:51) ) , natural history ( (cid:55) ) , olive oil ( (cid:55) ) , baggage claim ( (cid:51) ) S EE T OPIC - NoIter maritime ( (cid:51) ) , tourism ( (cid:51) ) , natural history ( (cid:55) ) , scenery ( (cid:51) ) , navy ( (cid:55) ) S EE T OPIC wildlife ( (cid:51) ) , scenery ( (cid:51) ) , maritime ( (cid:51) ) , highlinepark ( (cid:55) ) , aquarium ( (cid:51) ) According to Figures 2 ( a ) and 2 ( b ) , in most cases , the performance of S EE T OPIC deteriorates as ρ increases from 0 . 1 to 0 . 9 . Thus , setting ρ = 0 . 1 always leads to competitive NPMI and LCP scores on the three datasets . Although ρ = 1 is better than ρ = 0 . 9 , its performance is still suboptimal in comparison with ρ = 0 . 1 . This ﬁnding indicates that replacing the mean reciprocal rank ( i . e . , ρ = 1 ) with our proposed Eq . ( 6 ) is reasonable . According to Figures 2 ( c ) and 2 ( d ) , S EE T OPIC usually per - forms better when there are more iterations . On SciDocs and Twitter , the scores start to converge after T = 4 . Besides , more iterations will result in longer running time . Overall , we believe setting T = 4 strikes a good balance . 4 . 4 Case Study Finally , we show the terms retrieved by different methods as a case study . From each of the three datasets , we select an out - of - vocabulary category and show its topic discovery results in Table 4 . We mark a retrieved term as correct ( (cid:51) ) if at least 3 of the 5 annotators judge the term as relevant to the seed . Otherwise , we mark the term as incorrect ( (cid:55) ) . For the category “ hepatitis a / b / c / e ” from Sci - Docs , SeededLDA and Anchored CorEx can only ﬁnd very general medical terms , which are relevant to all seeds in SciDocs and thus inaccurate ; Labeled ETM and CatE ﬁnd terms about “ alanine amino - transferase ” , whose elevation suggest not only hep - atitis but also other diseases like diabetes and heart failure , thus not discriminative either ; BioBERT and S EE T OPIC , with the power of a PLM , can ac - curately pick terms relevant to “ hepatitis b ” and “ hepatitis c ” . For the category “ sports and out - doors ” from Amazon , SeededLDA and Anchored CorEx again ﬁnd very general terms , most of which are not category - discriminative ; Labeled ETM and CatE are able to pick more speciﬁc terms such as “ cars and tracks ” , but they still make mistakes ; BERT , as a PLM , can accurately ﬁnd terms that have lexical overlap with the category name ( e . g . , “ outdoorsmen ” , “ sporting events ” ) , meanwhile such terms are less diverse ; S EE T OPIC - NoIter starts to discover more concrete terms than BERT ( e . g . , “ in - door soccer ” , “ bike riding ” ) by leveraging local text semantics ; the full S EE T OPIC model , with an iterative updating process , can ﬁnd more speciﬁc and informative terms ( e . g . , “ canoeing ” , “ picnics ” , and “ rafting ” ) . For the category “ travel and trans - port ” from Twitter , both BERT and CatE make mistakes by including the term “ natural history ” ; S EE T OPIC - NoIter , without an iterative update pro - cess , also includes this error ; the full S EE T OPIC model ﬁnally excludes this error and achieves the highest accuracy in the retrieved top - 5 terms among all compared methods . 5 Related Work Seed - Guided Topic Discovery . Seed - guided topic models aim to leverage user - provided seeds to dis - cover underlying topics according to users’ inter - ests . Early studies take LDA ( Blei et al . , 2003 ) as the backbone and incorporate seeds into model learning . For example , Andrzejewski et al . ( 2009 ) consider must - link and cannot - link constraints among seeds as priors . SeededLDA ( Jagarlamudi et al . , 2012 ) encourages topics to contain more seeds and encourages documents to select topics relevant to the seeds they contain . Anchored CorEx ( Gallagher et al . , 2017 ) extracts maximally informa - tive topics by jointly compressing the corpus and preserving seed relevant information . Recent stud - ies start to utilize embedding techniques to learn better word semantics . For example , CatE ( Meng et al . , 2020a ) explicitly encourages distinction among retrieved topics via category - name guided embedding learning . However , all these models require the provided seeds to be in - vocabulary , mainly because they focus on the input corpus only and are not equipped with general knowledge of PLMs . Embedding - Based Topic Discovery . A number of studies extend LDA to involve word embed - ding . The common strategy is to adapt distribu - tions in LDA to generate real - valued data ( e . g . , Gaussian LDA ( Das et al . , 2015 ) , LFTM ( Nguyen et al . , 2015 ) , Spherical HDP ( Batmanghelich et al . , 2016 ) , and CGTM ( Xun et al . , 2017b ) ) . Some other studies think out of the LDA backbone . For example , TWE ( Liu et al . , 2015 ) uses topic struc - tures to jointly learn topic embeddings and improve word embeddings . CLM ( Xun et al . , 2017a ) col - laboratively improves topic modeling and word embedding by coordinating global and local con - texts . ETM ( Dieng et al . , 2020 ) models word - topic correlations via word embeddings to improve the expressiveness of topic models . More recently , Sia et al . ( 2020 ) show that directly clustering word em - beddings ( e . g . , word2vec or BERT ) also generates good topics ; Thompson and Mimno ( 2020 ) further ﬁnd that BERT and GPT - 2 discover high - quality topics , but RoBERTa does not . These models are unsupervised and hard to be applied to seed - guided settings . In contrast , our S EE T OPIC framework joint leverages PLMs , word embeddings , and seed information . 6 Conclusions and Future Work In this paper , we study seed - guided topic discov - ery in the presence of out - of - vocabulary seeds . To understand and make use of in - vocabulary com - ponents in each seed , we utilize the tokenization and contextualization power of PLMs . We pro - pose a seed - guided embedding learning framework inspired by the goal of maximizing PMI in topic modeling , and an iterative ensemble ranking pro - cess to jointly leverage general knowledge of the PLM and local signals learned from the input cor - pus . Experimental results show that S EE T OPIC outperforms seed - guided topic discovery baselines and PLMs in terms of topic coherence , term accu - racy , and topic diversity . A parameter study and a case study further validate some design choices in S EE T OPIC . In the future , it would be interesting to extend S EE T OPIC to seed - guided hierarchical topic dis - covery , where parent and child information in the input category hierarchy may help infer the mean - ing of out - of - vocabulary nodes . Acknowledgments We thank anonymous reviewers for their valu - able and insightful feedback . Research was sup - ported in part by US DARPA KAIROS Program No . FA8750 - 19 - 2 - 1004 , SocialSim Program No . W911NF - 17 - C - 0099 , and INCAS Program No . HR001121C0165 , National Science Foundation IIS - 19 - 56151 , IIS - 17 - 41317 , and IIS 17 - 04532 , and the Molecule Maker Lab Institute : An AI Re - search Institutes program supported by NSF under Award No . 2019897 , and the Institute for Geospa - tial Understanding through an Integrative Discov - ery Environment ( I - GUIDE ) by NSF under Award No . 2118329 . Any opinions , ﬁndings , and con - clusions or recommendations expressed herein are those of the authors and do not necessarily rep - resent the views , either expressed or implied , of DARPA or the U . S . Government . References David Andrzejewski , Xiaojin Zhu , and Mark Craven . 2009 . Incorporating domain knowledge into topic modeling via dirichlet forest priors . In ICML’09 , pages 25 – 32 . Kayhan Batmanghelich , Ardavan Saeedi , Karthik Narasimhan , and Sam Gershman . 2016 . Nonpara - metric spherical topic modeling with word embed - dings . In ACL’16 , pages 537 – 542 . David M Blei , Andrew Y Ng , and Michael I Jordan . 2003 . Latent dirichlet allocation . JMLR , 3 : 993 – 1022 . Xingyuan Chen , Yunqing Xia , Peng Jin , and John Car - roll . 2015 . Dataless text classiﬁcation with descrip - tive lda . In AAAI’15 , pages 2224 – 2231 . Arman Cohan , Sergey Feldman , Iz Beltagy , Doug Downey , and Daniel S Weld . 2020 . Specter : Document - level representation learning using citation - informed transformers . In ACL’20 , pages 2270 – 2282 . Margaret H Coletti and Howard L Bleich . 2001 . Med - ical subject headings used to search the biomedical literature . JAMIA , 8 ( 4 ) : 317 – 323 . Rajarshi Das , Manzil Zaheer , and Chris Dyer . 2015 . Gaussian lda for topic models with word embed - dings . In ACL’15 , pages 795 – 804 . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . Bert : Pre - training of deep bidirectional transformers for language understand - ing . In NAACL - HLT’19 , pages 4171 – 4186 . Adji B Dieng , Francisco JR Ruiz , and David M Blei . 2020 . Topic modeling in embedding spaces . TACL , 8 : 439 – 453 . Joseph L Fleiss . 1971 . Measuring nominal scale agree - ment among many raters . Psychological Bulletin , 76 ( 5 ) : 378 . Ryan J Gallagher , Kyle Reing , David Kale , and Greg Ver Steeg . 2017 . Anchored correlation explanation : Topic modeling with minimal domain knowledge . TACL , 5 : 529 – 542 . Thomas L Grifﬁths and Mark Steyvers . 2004 . Finding scientiﬁc topics . PNAS , 101 ( suppl 1 ) : 5228 – 5235 . Thomas Hofmann . 1999 . Probabilistic latent semantic indexing . In SIGIR’99 , pages 50 – 57 . Jagadeesh Jagarlamudi , Hal Daumé , and Raghavendra Udupa . 2012 . Incorporating lexical priors into topic models . In EACL’12 , pages 204 – 213 . Jey Han Lau , David Newman , and Timothy Baldwin . 2014 . Machine reading tea leaves : Automatically evaluating topic coherence and topic model quality . In EACL’14 , pages 530 – 539 . Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho So , and Jaewoo Kang . 2020 . Biobert : a pre - trained biomed - ical language representation model for biomedical text mining . Bioinformatics , 36 ( 4 ) : 1234 – 1240 . Omer Levy and Yoav Goldberg . 2014 . Neural word em - bedding as implicit matrix factorization . NIPS’14 , pages 2177 – 2185 . Bohan Li , Hao Zhou , Junxian He , Mingxuan Wang , Yiming Yang , and Lei Li . 2020 . On the sentence embeddings from pre - trained language models . In EMNLP’20 , pages 9119 – 9130 . Yang Liu , Zhiyuan Liu , Tat - Seng Chua , and Maosong Sun . 2015 . Topical word embeddings . In AAAI’15 , pages 2418 – 2424 . Zequn Liu , Shukai Wang , Yiyang Gu , Ruiyi Zhang , Ming Zhang , and Sheng Wang . 2021 . Graphine : A dataset for graph - aware terminology deﬁnition gen - eration . In EMNLP’21 , pages 3453 – 3463 . Christopher D Manning , Mihai Surdeanu , John Bauer , Jenny Rose Finkel , Steven Bethard , and David Mc - Closky . 2014 . The stanford corenlp natural language processing toolkit . In ACL’14 , System Demonstra - tions , pages 55 – 60 . Julian McAuley and Jure Leskovec . 2013 . Hidden fac - tors and hidden topics : understanding rating dimen - sions with review text . In RecSys’13 , pages 165 – 172 . Yu Meng , Jiaxin Huang , Guangyuan Wang , Zihan Wang , Chao Zhang , Yu Zhang , and Jiawei Han . 2020a . Discriminative topic mining via category - name guided text embedding . In WWW’20 , pages 2121 – 2132 . Yu Meng , Yunyi Zhang , Jiaxin Huang , Chenyan Xiong , Heng Ji , Chao Zhang , and Jiawei Han . 2020b . Text classiﬁcation using label names only : A language model self - training approach . In EMNLP’20 , pages 9006 – 9017 . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Cor - rado , and Jeff Dean . 2013 . Distributed representa - tions of words and phrases and their compositional - ity . In NIPS’13 , pages 3111 – 3119 . David Mimno , Hanna Wallach , Edmund Talley , Miriam Leenders , and Andrew McCallum . 2011 . Optimizing semantic coherence in topic models . In EMNLP’11 , pages 262 – 272 . David Newman , Jey Han Lau , Karl Grieser , and Tim - othy Baldwin . 2010 . Automatic evaluation of topic coherence . In NAACL - HLT’10 , pages 100 – 108 . Dat Quoc Nguyen , Richard Billingsley , Lan Du , and Mark Johnson . 2015 . Improving topic models with latent feature word representations . TACL , 3 : 299 – 313 . Jiezhong Qiu , Yuxiao Dong , Hao Ma , Jian Li , Kuansan Wang , and Jie Tang . 2018 . Network embedding as matrix factorization : Unifying deepwalk , line , pte , and node2vec . In WSDM’18 , pages 459 – 467 . Michael Röder , Andreas Both , and Alexander Hinneb - urg . 2015 . Exploring the space of topic coherence measures . In WSDM’15 , pages 399 – 408 . Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 . Neural machine translation of rare words with subword units . In ACL’16 , pages 1715 – 1725 . Jingbo Shang , Jialu Liu , Meng Jiang , Xiang Ren , Clare R Voss , and Jiawei Han . 2018 . Automated phrase mining from massive text corpora . IEEE TKDE , 30 ( 10 ) : 1825 – 1837 . Suzanna Sia , Ayush Dalmia , and Sabrina J Mielke . 2020 . Tired of topic models ? clusters of pretrained word embeddings make for fast and good topics too ! In EMNLP’20 , pages 1728 – 1736 . Jian Tang , Meng Qu , and Qiaozhu Mei . 2015 . Pte : Pre - dictive text embedding through large - scale heteroge - neous text networks . In KDD’15 , pages 1165 – 1174 . Laure Thompson and David Mimno . 2020 . Topic mod - eling with contextualized word representation clus - ters . arXiv preprint arXiv : 2010 . 12626 . Dingding Wang , Shenghuo Zhu , Tao Li , and Yihong Gong . 2009 . Multi - document summarization us - ing sentence - based topic models . In ACL’09 , pages 297 – 300 . Mu - Chun Wang , Zixuan Liu , and Sheng Wang . 2022 . Textomics : A dataset for genomics data summary generation . In ACL’22 . Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 . Google’s neural machine translation system : Bridging the gap between hu - man and machine translation . arXiv preprint arXiv : 1609 . 08144 . Guangxu Xun , Yaliang Li , Jing Gao , and Aidong Zhang . 2017a . Collaboratively improving topic dis - covery and word embeddings by coordinating global and local contexts . In KDD’17 , pages 535 – 543 . Guangxu Xun , Yaliang Li , Wayne Xin Zhao , Jing Gao , and Aidong Zhang . 2017b . A correlated topic model using word embeddings . In IJCAI’17 , pages 4207 – 4213 . Chao Zhang , Keyang Zhang , Quan Yuan , Fangbo Tao , Luming Zhang , Tim Hanratty , and Jiawei Han . 2017 . React : Online multimodal embedding for recency - aware spatiotemporal activity modeling . In SI - GIR’17 , pages 245 – 254 . Yu Zhang , Shweta Garg , Yu Meng , Xiusi Chen , and Jiawei Han . 2022 . Motifclass : Weakly supervised text classiﬁcation with higher - order metadata infor - mation . In WSDM’22 , pages 1357 – 1367 . Yu Zhang , Yu Meng , Jiaxin Huang , Frank F Xu , Xuan Wang , and Jiawei Han . 2020 . Minimally supervised categorization of text with metadata . In SIGIR’20 , pages 1231 – 1240 . A The Embedding Learning Objective In Section 3 . 2 , we propose the following embed - ding learning objective : J = (cid:88) d ∈D (cid:88) w i ∈ d (cid:88) w j ∈C ( w i , h ) exp ( u Tw i v w j ) (cid:80) w (cid:48) ∈V D exp ( u Tw i v w (cid:48) ) (cid:124) (cid:123)(cid:122) (cid:125) J context + (cid:88) d ∈D (cid:88) w ∈ d exp ( u Tw v d ) (cid:80) d (cid:48) ∈D exp ( u Tw v d (cid:48) ) (cid:124) (cid:123)(cid:122) (cid:125) J document + (cid:88) c i ∈C (cid:88) w ∈S i exp ( u Tw v c i ) (cid:80) c (cid:48) ∈C exp ( u Tw v c (cid:48) ) (cid:124) (cid:123)(cid:122) (cid:125) J category . ( 12 ) Now we prove that maximizing J is implicitly performing the factorization in Eq . ( 3 ) . Levy and Goldberg ( 2014 ) have proved that max - imizing J context is implicitly doing the following factorization . u Tw i v w j = log (cid:16) # D ( w i , w j ) · λ D # D ( w i ) · # D ( w j ) · b (cid:17) , i . e . , U Tw V w = X ww . ( 13 ) We follow their approach to consider the other two terms J document and J category in Eq . ( 12 ) . Using the negative sampling strategy to rewrite J document , we get (cid:88) w ∈V D (cid:88) d ∈D # d ( w ) (cid:16) log σ ( u Tw v d ) + b E d (cid:48) (cid:2) log σ ( − u Tw v d (cid:48) ) (cid:3)(cid:17) , ( 14 ) where σ ( · ) is the sigmoid function . Following ( Levy and Goldberg , 2014 ; Qiu et al . , 2018 ) , we assume the negative sampling distribution ∝ λ d . 15 Then , the objective becomes (cid:88) w ∈V D (cid:88) d ∈D # d ( w ) log σ ( u Tw v d ) + (cid:88) w ∈V D # D ( w ) (cid:88) d (cid:48) ∈D b · λ d (cid:48) λ D log σ ( − u T w v d (cid:48) ) . ( 15 ) For a speciﬁc term - document pair ( w , d ) , we con - sider its effect in the objective : J w , d = # d ( w ) log σ ( u Tw v d ) + # D ( w ) b · λ d λ D log σ ( − u Tw v d ) . ( 16 ) Let x w , d = u Tw v d . To maximize J w , d , we should have 0 = ∂ J w , d ∂x w , d = # d ( w ) σ ( − x w , d ) − # D ( w ) · b · λ d λ D σ ( x w , d ) . ( 17 ) 15 In practice , the negative sampling distribution ∝ λ 3 / 4 d , but related studies ( Levy and Goldberg , 2014 ; Qiu et al . , 2018 ) usually assume a linear relationship in their derivation . That is , e 2 x w , d − (cid:16) # d ( w ) · λ D # D ( w ) · b · λ d − 1 (cid:17) e x w , d − # d ( w ) · λ D # D ( w ) · b · λ d = 0 . ( 18 ) Therefore , e x w , d = − 1 ( which is invalid ) or e x w , d = # d ( w ) · λ D # D ( w ) · b · λ d . In other words , u Tw v d = x w , d = log (cid:16) # d ( w ) · λ D # D ( w ) · b · λ d (cid:17) , i . e . , U Tw V d = X wd . ( 19 ) Similarly , for J category , the objective can be rewritten as (cid:88) w ∈V D (cid:88) c i ∈C 1 w ∈S i log σ ( u Tw v c i ) + (cid:88) w ∈V D 1 w ∈S 1 ∪ . . . ∪S | C | (cid:88) c (cid:48) ∈C b | C | log σ ( − u Tw v c (cid:48) ) . ( 20 ) Following the derivation of J document , we get u Tw v c i = x w , c i = log (cid:16) 1 w ∈S i | C | 1 w ∈S 1 ∪ . . . ∪S | C | · b (cid:17) , i . e . , U Tw V c i = X wc . ( 21 ) Putting Eqs . ( 13 ) , ( 19 ) , and ( 21 ) together gives us Eq . ( 3 ) . B The Ensemble Ranking Function In Section 3 . 3 , we propose the following ensemble ranking function : score ( w | S i ) = (cid:18) 1 2 (cid:16) 1 rank G ( w ) (cid:17) ρ + 1 2 (cid:16) 1 rank L ( w ) (cid:17) ρ (cid:19) 1 / ρ . ( 22 ) Now we prove this ranking function is a general - ization of the arithmetic mean reciprocal rank ( i . e . , MRR ) and the geometric mean reciprocal rank : lim ρ → 1 score ( w | S i ) = 1 2 (cid:16) 1 rank G ( w ) + 1 rank L ( w ) (cid:17) ; lim ρ → 0 score ( w | S i ) = (cid:16) 1 rank G ( w ) · 1 rank L ( w ) (cid:17) 1 / 2 . ( 23 ) The case of ρ → 1 is trivial . When ρ → 0 , we aim to show that lim ρ → 0 log score ( w | S i ) = log (cid:16) 1 rank G ( w ) · 1 rank L ( w ) (cid:17) 1 / 2 . ( 24 ) In fact , let r G = 1 rank G ( w ) and r L = 1 rank L ( w ) . lim ρ → 0 log score ( w | S i ) = lim ρ → 0 log (cid:16) 1 2 r ρG + 1 2 r ρL (cid:17) 1 / ρ = lim ρ → 0 log (cid:0) 12 r ρG + 12 r ρL (cid:1) ρ = lim ρ → 0 12 r ρG log r G + 12 r ρL log r L 12 r ρG + 12 r ρL 1 = lim ρ → 0 (cid:16) r ρG log r G + r ρL log r L (cid:17) lim ρ → 0 (cid:16) r ρG + r ρL (cid:17) = log r G + log r L 2 = log ( r G · r L ) 1 / 2 . ( 25 ) The third line is obtained by applying L’Hopital’s rule .