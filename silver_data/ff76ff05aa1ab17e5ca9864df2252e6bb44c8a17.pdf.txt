Gender Recognition or Gender Reductionism ? The Social Implications of Automatic Gender Recognition Systems Foad Hamidi Morgan Klaus Scheuerman Stacy M . Branham University of Maryland , University of Maryland , University of Maryland , Baltimore County ( UMBC ) Baltimore County ( UMBC ) Baltimore County ( UMBC ) Baltimore , MD , USA Baltimore , MD , USA Baltimore , MD , USA foadhamidi @ umbc . edu morgan . klaus @ umbc . edu sbranham @ umbc . edu ABSTRACT Automatic Gender Recognition ( AGR ) refers to various computational methods that aim to identify an individual’s gender by extracting and analyzing features from images , video , and / or audio . Applications of AGR are increasingly being explored in domains such as security , marketing , and social robotics . However , little is known about stakeholders’ perceptions and attitudes towards AGR and how this technology might disproportionately affect vulnerable communities . To begin to address these gaps , we interviewed 13 transgender individuals , including three transgender technology designers , about their perceptions and attitudes towards AGR . We found that transgender individuals have overwhelmingly negative attitudes towards AGR and fundamentally question whether it can accurately recognize such a subjective aspect of their identity . They raised concerns about privacy and potential harms that can result from being incorrectly gendered , or misgendered , by technology . We present a series of recommendations on how to accommodate gender diversity when designing new digital systems . Author Keywords Automatic gender recognition ; gender identity ; transgender ; autonomy ; user - centered design . ACM Classification Keywords H . 5 . 3 . Information interfaces and presentation ( e . g . , HCI ) INTRODUCTION Gender is a significant social construct in human cultures ; it permeates both our offline , physical worlds , and increasingly our online , virtual spaces and digital devices [ 5 , 40 ] . Whether it be through social networks or video games , users increasingly encounter embedded gender Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from Permissions @ acm . org . CHI 2018 , April 21 – 26 , 2018 , Montreal , QC , Canada © 2018 Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 5620 - 6 / 18 / 04… $ 15 . 00 https : / / doi . org / 10 . 1145 / 3173574 . 3173582 representations in technology . For example , social networks such as Facebook use information about users’ gender for targeted marketing , a practice that potentially impacts millions of users worldwide [ 2 ] . Another outgrowth of this trend is the development of automatic gender recognition ( AGR ) , a class of algorithms that use various techniques , including facial recognition [ 27 , 32 ] and body recognition [ 6 , 45 ] , to classify an individual’s gender . While AGR technology is still in its infancy , the recent integration of facial recognition into already pervasive technologies suggest it could impact large numbers of people in the near future . As technologists continue to develop AGR applications , it is important to understand the social and ethical implications of widespread adoption . However , there has been little research into the perceptions and attitudes of end users about this emerging technology . Gender is a complex concept with important roles both as a cultural construct and a core aspect of an individual’s identity [ 5 , 40 , 42 ] . Research into gender is increasingly revealing its multifaceted internal aspects , which exhibit much more diversity and fluidity than thought before [ 5 , 40 ] . For example , currently an estimated 0 . 06 % of Americans ( or 1 . 4 million people ) identify as transgender ( including gender non - binary ) [ 8 ] . Transgender or trans denotes when an individual’s gender identity differs from the one they were assigned at birth based on sex [ 47 ] ( as opposed to cisgender or cis , a person who does identify with the sex they were assigned at birth ) . Sensitivity to one’s gender identity is crucially important , as the act of misgendering – – whereby one’s gender is incorrectly identified , leading to the use of incorrect gendered words – – is a form of “structural violence” that can have a significant negative impact on trans individuals [ 21 , 31 , 35 ] . The complexity of gender constructs motivates the current exploration , which aims to complement and problematize the technical work that is going into implementing AGR . By looking at AGR from the perspective of transgender individuals , we seek to study the impact of technologies that make assumptions about what can or should be automatically inferred without consultation with human agents . Given the multiple aspects of gender and its CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 1 subjective dimension , can it be detected accurately based on external features ? What happens when the system misgenders an individual , either by mistaking their gender or by failing to include their non - conforming gender in the model ? What would be the cost of these failures ? Finally , as AGR is frequently enacted upon an individual without consent , what are the ethical implications regarding one’s autonomy and privacy ? To address these questions , we conducted qualitative interviews with transgender - identifying individuals , including transgender technologists ( developers and researchers ) . We asked participant to share their general impressions of AGR and to respond to scenarios where AGR makes mistakes or misclassifies gender , leading to misgendering . We found that our participants had overwhelmingly negative impressions of AGR and had serious concerns about how it would impact their autonomy and privacy . Based on our findings , we offer insights into the development of AGR moving forward . The contributions of this paper are three - fold . First , it provides insight into the potentially harmful impact of an emerging technology on an understudied and vulnerable population ( i . e . , transgender individuals ) , as well as the wider population . Second , it offers recommendations into how gender and other complex human characteristics can be mindfully incorporated into technology . Finally , it tackles some of the sociotechnical issues that arise when notions of identity , autonomy , and diversity intersect in relation to technology design . RELATED WORK Automatic Gender Recognition ( AGR ) and its Applications Automatic Gender Recognition ( AGR ) ( also known as gender classification ) refers to algorithmic methods , including automatic facial recognition [ 27 , 32 ] and body recognition [ 6 , 45 ] technologies , that extract features from images , video , or audio of one or more individuals in order to identify their gender . AGR often leverages computer vision algorithms and / or voice recognition modules . A common method is to extract features ( e . g . , facial hair ) from an individual’s visual and / or audio data ( e . g . , a video showing their face ) and compare them with ground - truth samples ( e . g . , videos of faces for which the gender is known ) in an existing database . If the input features are found to be similar to those in the database , a match is declared . AGR has been developed since at least the early 1990’s [ 12 ] . Prior research has explored the technical capabilities of AGR and its applications , including gendered marketing , human - robot interaction , and security surveillance [ 32 ] . There are several motivations for using AGR : it is believed to improve user experience by providing a digital system with more information about the user , such that it can better adapt to them [ 32 , 39 , 45 ] ; it is also believed to have the ability to enhance surveillance or marketing research by analyzing user data and providing results to marketers [ 33 ] or authorities ( i . e . , police ) [ 18 , 41 ] . Researchers have focused on several methods for implementing AGR and improving its accuracy , including : voice recognition using fundamental frequency and MFCC coefficients [ 46 ] , facial recognition using image texture extraction [ 27 ] and face alignment [ 30 ] , body recognition using a part - based gender recognition algorithm [ 6 ] , analysis of breast shape [ 45 ] , and gait - based gender recognition using the gait energy image ( GEI ) [ 49 ] . Researchers have also studied the effects of racial facial features on gender recognition accuracy [ 37 ] . In a 2013 paper , Kosinski et al . presented a method to analyze digital records of online behavior ( e . g . , Facebook Likes ) to predict user gender , as well as other attributes including sexual orientation , ethnicity and age [ 24 ] . Recently , there has been an increasing corpus of research discussing AGR and transgender individuals [ 26 , 30 , 48 ] . This work is focused primarily on the accuracy challenges , which transgender faces present to AGR algorithms . Mahalingam and Ricanek created the first transgender dataset for facial recognition technology usage [ 28 ] . This dataset is used in biometric research examining transgender facial recognition [ 26 , 29 ] . The authors also claim hormone replacement therapy ( HRT ) could be used for the purpose of “biometric obfuscation” or disguise [ 29 , 48 ] . This work which positions transgender faces as problematic to facial recognition accuracy , also raised ethical issues related to user privacy as the data for the database was scraped from transgender individuals’ videos without their consent or knowledge [ 20 ] . To our knowledge , there is a lack of research on AGR that seeks consultation from members of the transgender community . Algorithmic Fairness , Human Autonomy and Automatic Recognition Systems Previous research has identified the possibility of unintentionally replicating human biases in automatic systems [ 23 , 36 ] . For example , Kannabiran and Petersen discussed the interaction between power and the politics of ingrained biases in user interfaces [ 23 ] , while O’Neil explains that algorithms and big data increase inequality by “masquerading as neutral technology” [ 36 ] . Researchers have further called into question the concept of “fairness” in machine learning , calling for frameworks for inclusion of vulnerable communities [ 44 ] . Activist organizations and projects have formed to protest algorithmic bias [ 1 ] , with some efforts specifically targeting facial recognition technology [ 4 ] . While some research has discussed the interaction between gender , race and encoded biases [ 2 , 17 ] , there is still a need to understand how gendered bias can be encoded in AGR and the possible implications of such encoding on vulnerable gender minorities . CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 2 In automatic recognition systems , issues regarding bias are compounded by concerns for human autonomy . Human autonomy can be defined as the ability “to be one ' s own person , to be directed by considerations , desires , conditions , and characteristics that are not simply imposed externally upon one , but are part of what can somehow be considered one ' s authentic self” [ 7 ] . Previous research in HCI has long identified the need to support human autonomy as a central ethical value [ 9 , 10 ] . While some of the researchers developing AGR systems have briefly discussed concerns of privacy for their users ( e . g . , [ 39 ] ) , most of the previous research in this area has focused on addressing technical issues of the algorithms themselves and how to improve their accuracy . Additionally , recent news stories have reported the use of AGR - capable facial recognition systems for advertising in public spaces without user knowledge or consent ; systems whose use became only apparent to passersby after a billboard screen malfunction [ 34 ] . In the face of these stories and concerns about the possibility for algorithmic bias and threats to user autonomy posed by automatic recognition systems , it is important to study and better understand the ethical and social implications of these systems . Technology Design and Transgender Individuals A recent survey of the last thirty - five years of CHI proceedings found only three papers that included input from trans technology users [ 43 ] . However , there is a growing interest in understanding this community , and previous research has found that failing to include the perspectives of this population in technology design can have harmful and adverse impacts on them [ 2 , 15 , 16 ] . Using survey results from 283 trans Facebook users , Haimson et al . found that trans individuals use different strategies , including editing self - representational data from prior identifies and conducting detailed access management for their profiles , to deal with gender transitioning while on the social network [ 16 ] . They found that these strategies were time - consuming and sometimes emotionally painful and could potentially be facilitated by better user experience design . In another study , Bivens and Haimson analyzed the gender options on member sign up pages , profile pages , and advertising portals of the 10 most popular social networks and found that most of the sites embedded gender binaries in their underlying categorization mechanisms , an approach that might lead to the implicit or explicit misgendering of users [ 2 ] . In this study , we focus on the expereince of being misgendered by technology from the transgender individuals’ perspective . As mentioned in the introduction , misgendering refers to the experience of being incorrectly gendered by others . Transgender individuals often experience misgendering by other individuals in their daily lives . Previous research has shown that being misgendered can have a significant negative impact on an individual’s mental health , including feelings of stigmatization [ 31 ] , social exclusion [ 14 ] , and oppression [ 22 ] . Findings from the National Transgender Discrimination Survey conducted in 2014 found that 41 % of respondents attempted suicide compared to the 4 . 6 % of the average U . S . population [ 8 ] . Of those who were regularly misgendered in their workplace , 56 % had attempted suicide . These data suggest that misgendering may have severe implications for the mental and physical health of trans individuals [ 13 ] . It is therefore important to investigate how being misgendered by technologies like AGR may impact trans people . In summary , there is a small but growing body of research in the HCI community that focuses on digital technology design for transgender individuals [ 2 , 3 , 15 , 16 ] . This paper builds on this research by investigating gender representation within AGR systems and offering design recommendations that are inclusive of trans voices . METHODS Participants In this study , we focused on understanding the perspective of individuals who identify as transgender ( i . e . , a different gender identity than the one assigned at birth ) [ 47 ] . In addition to binary - identifying transgender individuals , we have included people with non - binary and gender non - conforming trans identities , which we also refer to as “transgender” and “trans” throughout this paper . We recruited 13 trans - identifying participants , three of whom were technologists , or professionals working in a field related to digital technology , such as software engineering . We chose to seek out trans technologists to allow that individuals with technical expertise may have different attitudes towards technological innovation . We denote technologist participants by using a T as their Participant ID ( e . g . , T1 ) and denote non - technologist participants by adding a T to their ID ( e . g . , P1 ) . Please see Table 1 on the next page for the participants’ demographic information . Participants were recruited using an online survey distributed using Facebook Groups . To identify trans technologists , authors asked personal contacts to distribute the online survey in their professional circles . Finally , in an effort to include a more racially and socioeconomically diverse participant pool , physical fliers with the link to the survey and contact information were posted in trans - oriented community centers in the authors’ city . Study Protocol We conducted semi - structured interviews with the participants over the phone . We developed and refined the interview protocols by conducting pilots with two representatives of our population . We used different protocols for technologists and non - technologists . Both protocols included questions about the participants’ perceptions and attitudes towards AGR and their previous experience with similar automatic recognition technology , such as facial recognition software . CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 3 Participant ID Gender Identity Pronouns Racial Identity P1 Non - binary trans masculine He / him Black P2 Non - binary trans woman She / her and they / them White P3 Non - binary male He / him or they / them Black P4 Trans feminine She / her and they / them Black / mix P5 Genderqueer They / them Black Jamaican P6 ( Trans ) woman She / her White P7 Trans male He / him White P8 Non - binary He / him White / Latine P9 ( Trans ) female She / her White P10 Bigender Ey / em or they / them Japanese / White T1 Trans / gender non - conforming They / them or she / her White T2 Trans woman She / her Middle Eastern / South Asian T3 Non - binary They / them White Table 1 . Participant information . Participant IDs beginning with ' T ' represent a technology developer or researcher . Additionally , we presented participants with a variety of possible AGR use scenarios based on existing examples from literature and news ( i . e . , for marketing , security , and human - robot interaction ) . We chose examples that were not overtly positive or negative . If participants offered examples of their own , we would ask them to elaborate on these scenarios rather than our examples . We also asked participants to imagine future scenarios in which they may encounter AGR , including scenarios where they might be gendered correctly or incorrectly . For the technologists , we also asked questions about their professional experience with technology design , especially the design of recognition systems that use automatic training . Interviews took on average approximately 85 minutes , ranging from 36 to 91 minutes . We transcribed all the interviews and conducted an iterative thematic analysis to identify and synthesize themes within the data . We identified seven themes and eight subthemes . FINDINGS Previous Experiences of Misgendering : “The Base Alienation that Comes with Transphobia” Misgendering in Physical Spaces Participants discussed the negative impact of misgendering on their mental and emotional wellbeing . Some ( P1 , P2 , P3 , P4 , P7 , T2 ) reported being more often misgendered offline . Participants who identified as non - binary ( P5 , P8 , P10 ) reported never being gendered correctly by strangers . Others ( P2 , P6 ) who said they usually “pass” ( i . e . , are correctly gendered by others ) in person , reported instances where they were misgendered on the phone or through voice chat where people cannot see them . Participants ( P1 , P2 , P7 ) described feelings of frustration and emotional exhaustion that come with trying to avoid being misgendered by others : “It can be exhausting to have to go out and be misgendered . [ It makes ] me dysphoric . What about my face is like this gender that I don ' t identify with ? … [ It ] makes me try to hide that trait from other people . . . If I feel like my chest is sticking out too much , I might … wear longer clothes or try to … lean forward . ” – P1 The frustration stems not only from being misgendered , but also from an awareness that others simply do not know their gender identity exists : “It’s annoying that [ people ] don’t think that , ‘Oh ! Non - binary is a thing” ( P5 ) . Misgendering in Virtual Spaces Participants ( P1 , P2 , P3 , P4 , P7 ) said that technology and especially online spaces presented them with more control over how others see their identities and interact with them . P3 said he can control his avatar or image online , but he “can ' t control everything in real life . ” P7 said he would actively manage his gender presentation online with “things like the angle of [ his ] jaw when [ he ] would take a picture” to “masculinize” his facial features . These mechanisms were particularly valued by people who did not pass as their preferred gender in face - to - face situations . Further , ease of gender presentation online supported more fluid , day - to - day expression along a gender spectrum ( P4 ) . However , participants ( P7 , P10 ) also noted that online systems could reinforce problematic gender expectations they usually faced offline . P10 said online surveys that forced them to pick male or female are “terrible” and that the lack of pronoun options on some sites is “frustrating . ” “When I see the language of male and female … as the only two options , … that’s an indicator that they haven’t done one of the most basic things to accommodate trans people , so I don’t know if I can trust the rest of the experience . ” – P7 While online profiles support more control over gender presentation and thus reduce misgendering by other users , CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 4 the system itself can misgender users by embedding inflexible binary gender categories . Can AGR Really Work ? : “I Would Show Up as a Blip or an Error” When asked about their impression of AGR , all 13 participants had serious concerns about the assumptions these systems make about gender and how they might reflect on the trans community . Some participants ( P2 , P8 , T2 , P10 , T3 ) disagreed with the assumption AGR systems make about the nature of gender as something that can be classified using external features ; they stated that gender is an internal identity not necessarily tied to physical features : “The very premise is flawed . It’s not even a matter of adding more categories [ of gender ] … You [ cannot ] map a sort of appearance or map a presentation onto a gender with anything approaching accuracy . ” – T3 Other concerns about accuracy were related to fluctuating gender presentations ( P7 , P9 , T3 ) achieved with makeup , hormone replacement therapy ( HRT ) , and / or gender affirmation surgery . On the flip side , lack of access to these options as well as realistic limitations of changing one’s physical body to match their identity also raised concerns : “Whose gonna change their wrists ? Or whose gonna change … their bone structure to the point where they’re either going to look male or look female ? Are you going to change your rib cage , are you going to change your hips ? ” – P9 Here , P9 explains how AGR algorithms based on physical form might make it difficult to accurately identify one’s gender while simultaneously placing unrealistic expectations on transgender bodies to conform . This quote captures the stress that can result when socially - constructed standards are materialized and imposed on users through technology implementation . Finally , P7 noted that trans people are often mistakenly accused of “catfishing , ” or luring others into relationships using false and constructed online personas . He was therefore worried that AGR systems might flag transgender individuals , lumping them in with ill - intentioned people trying to commit fraud or deception . Impact of Being Misgendered by AGR All participants acknowledged the potential negative impact of being misgendered by AGR . They differed on their reading of the severity of misgendering perpetrated by an algorithm as compared to a human being . Worse than a Human : “It’s the Worst Social Exclusion” Most of the participants ( P1 , P7 , P8 , T1 , P9 , P10 , T3 ) considered misgendering by AGR worse than being misgendered by another human being . This stemmed in part from the fact that AGR simply introduced another potential source of invalidation . P7 , T1 , P10 , and T3 were concerned that it would add to the regular exhaustion and impact of being misgendered that they already experience . “I get misgendered enough by . . . human beings , why on Earth would I want a robot to help in that ? … Programmatic misgendering , it sort of just adds to the ocean we all swim in of constant small comments … [ Misgendering ] is death by a thousand paper cuts . ” – T3 Participants also foresaw that increased misgendering would lead to an increase in its negative effects : “That would just increase trans people’s dysphoria [ i . e . , the distress or discomfort some transgender people experience when their physical body does not match their gender identity ] , as well as increase the amount that they’re getting misgendered , which is terrible . ” – P10 For others , the distinction between human and computer mistakes was significant . One set of concerns was rooted in the belief that AGR systems might not allow users to perceive and therefore correct gender classification errors . P9 and T3 expressed being more tolerant of human mistakes , because “people you can correct” ( T3 ) . In the long - term , it would be “really demoralizing” to consistently be seen as “something that you’re not” ( T1 ) . Other participants ( P1 , P4 , P7 , P8 ) expressed that being misgendered by a computer was worse due to the perceived objectivity of computer systems : “Computers are said to be a lot smarter than people … I would feel a little bit worse if there was a software that looked at everything about me [ and misgendered me ] . ” – P1 “Not only is human error getting my identity false , it ' s computers and AIs and technology also messing up too . It ' s not a person ' s uncertain perception , it ' s a more precise mathematical analysis of me that led to this conclusion , which kinda would rub it in my face even more . ” – P7 In contrast , P8 was aware that computers carry the biases of their human developers . But , as a result , he interpreted being misgendered by a computer as a more severe act committed by many people as opposed to just one : “It would probably feel shittier if this million - dollar piece of software developed by however many people decides that I’m this thing that I’m not . ” – P8 Finally , both P1 and P7 thought being misgendered by AGR technology would reinforce gendered standards that transgender individuals would then internalize and hold themselves to . We saw that the perception of computers as somehow being more “objective” or as a synthesis of general human standards led participants to a sort of insult - to - injury mindset because they interpreted the gender label assigned by the computer as more definitive and exacting . The Same as a Human : “A Misunderstanding of Gender” Other participants ( P3 , P5 , P6 ) said the impact of being misgendered by AGR would be the same as being misgendered by a human being because , like humans , machines are “subjective” . P3 also attributed AGR misgendering them to its designers having a “misunderstanding of gender as a whole . ” However , they thought the impact of misgendering was “basically the CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 5 same , because a person would be responsible ultimately for making and designing [ AGR ] . ” ( P3 ) . Better than a Human : “Oh , this Machine is Stupid” P2 , P4 , and T2 thought being misgendered by AGR would be better than being misgendered by a human being . P2 and T2 both said they would view the mistake as a flaw in the technology , for which they would judge the designers : “I’d be like , ‘Oh , this machine is stupid , ’ you know ? It would tell me something about the assumptions of gender that were being put into the design . ” – T2 P4 , however , was more focused on the sophistication of human versus machine classifiers . He suggested that being misgendered by AGR would be less concerning because there would be less perceptivity and intentionality behind the mistake : “A human being misgendering a person can be a lot more nuanced and it can mean a lot more or less depending on the person it’s coming from … Robots misgendering me is kind of like a fake objective ‘you look like a man , ’ but there is no objective look of masculinity . ” – P4 Regardless of where participants stood on the spectrum of AGR being worse , better , or the same as human beings misgendering them , none of them expressed that AGR getting their gender wrong would be viewed positively . Questioning AGR’s Necessity : “What Benefit Would this Provide to Society ? ” Participants were skeptical of useful or necessary applications of AGR and they all questioned whether implementing it would offer any benefit to end users . Most of the participants were familiar with other automatic recognition technologies : nine out of 13 participants had used automatic facial recognition technology before . However , of the 10 trans non - technologists and 3 trans technologists , none of them could imagine any benefit that AGR would offer its users . Put bluntly by P9 : “It has no social redeeming value … I either would totally ignore the [ AGR ] robot , [ or ] if it were possible , kick the robot in the balls and knock it over and get out of my way . ” – P9 P2 , P3 , P4 , P7 , and P10 were against the development of AGR due to its potential negative impact . “I don’t know if [ AGR ] would intersect well with transness … It sounds like it could be bio - essentialist [ i . e . , reducing gender information to biological characteristics ] . ” – P3 Several participants related their concerns about the similarity of AGR to facial recognition software . P10 described how she was “wary” of facial recognition software because of how it could be used for “gendered marketing” . P10 described the face tagging functionality of Facebook as “creepy . ” P4 , P5 , T1 , and P9 all stated that automatically gendering others on sight was undesirable . “Why in the first 30 seconds that you meet someone , whether it’s a robot or human , [ would you need to know their gender ] ? The only reason we have to establish gender really is so we can use the right pronouns . ” – T1 P6 , P7 , and P8 questioned what the benefit would be , both in the context of society and to developers . “Particularly with the range of expression of gender now , I just wonder … if that’s actually valuable information . ” – P6 P9 was the most optimistic of the participants . Although she was concerned that the potential “negatives outweigh the positives , ” she also expressed hope that if designers could adjust for the negatives AGR could have potential . Nonetheless , both P9 and P10 did not feel it would work out in the current cultural and political context . “Right now , I don’t see programmers having the ability to screen out or prevent the negative use , the hostile use… . I don’t think we’re progressive enough as humanity to successfully navigate the use of that kind of a program for only good . ” – P9 All three technologists ( T1 , T2 , T3 ) also disliked the concept of AGR . T1 and T2 had used facial recognition software in lab contexts before and were familiar with technology ( e . g . , Microsoft’s Face API ) that has features to detect age , gender , emotion , pose , smile , and facial hair on images . T3 had experience researching facial recognition , and stated that they had encountered it in airports before . T2 said that technology that could serve cisgender people may be unable to serve transgender people , because in technology design the “treatment of identity [ is ] poor . ” “ [ These ] algorithms … they’re looking for certain things . They’re looking for masculine landmarks or … feminine contours , or whatever … it’s not clear how that stuff is made . Who’s making those decisions ? That’s binarist as fuck . ” – T2 Technologist participants expressed a sense of resignation about their lack of agency in designing AGR : “The work that they’re doing has no provable meaningful benefit . It does have provable meaningful harm … I think ( trans people ) should absolutely be concerned … Yeah , this is bad technology , and yeah this doesn’t work , but . . . there will always be someone willing to deploy it . ” – T3 AGR as a Tool for Oppression : “It’s Just Going to Exacerbate what’s Already There” Participants expressed fear that AGR could be used , whether intentionally or not , as a tool for renewing oppressive structures that already affect the trans community . Specifically , they were concerned that it would reinforce gender binaries , override user autonomy , and impose surveillance that undermines privacy and safety . P4 articulated the experience of oppression that trans individuals already face and how AGR would intensify it : “There ' s already so many eyes on every trans person navigating through the world and we all feel those eyes … It ' s just going to exacerbate what’s already there . ” AGR Reinforces Gender Binarism Participants ( P1 , P4 , P8 , T1 , T2 , T3 ) viewed current AGR implementations , which classify targets into male or CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 6 female categories , as reinforcing a binary gender system . Adopting a binary male / female scheme excludes , invalidates , and assures the misgendering of non - binary identities . Further , as T2 pointed out , binary AGR systems would likely give preference to transgender people who “pass” as their gender identity by conforming to binary categories of gender expression . Participants ( P4 , P8 , T1 ) noted the incongruous pairing of futuristic AGR technology with old - fashioned conceptualizations of gender and its value to society : “I don’t think [ AGR is ] good or necessary moving forward in a world where we’re caring less about gender . ” – P4 “For legal IDs , we put sex … but we don ' t have blood type on IDs , which doesn’t make sense because when EMTs [ i . e . , Emergency Medical Technicians ] open the wallet they don’t always need to know what your genitals are but they do need to know what blood type you are … It doesn’t need to be gendered , that doesn’t need to be the system of classification . ” – P4 P4 , P7 , T1 , T2 , P10 , and T3 expressed that technological futures should not simply replicate archaic gender systems , they should drive them forward : “People are raised to be really , really cissexist … whereas a robot or a screen or some kind of technology , they’re not raised in society , so they have no reason to misgender you except that someone specifically programmed them to do so . ” – P10 From this perspective , AGR in its current implementations represents a “missed opportunity” for progress ( T1 ) . Consequently , AGR is a missed opportunity for including the trans community : “We ' re excluded from the direction of the future … that’s sorta what it feels like . ” – P7 AGR Undermines User Autonomy Concerns about violations of user autonomy in relation to AGR and a lack of trust that emerged as an important topic in interviews . Specifically , several participants ( P2 , P4 , P7 , T3 ) described AGR as non - consensual : “ [ AGR ] is ( 1 ) completely unnecessary , and ( 2 ) undesired by many people who would be interacting with the software without their consent . ” – P4 P2 described an incident in which an “electronic billboard that crashed and revealed the program running beneath it” was being used without user consent to identify the age and gender of people for targeted advertisements . In addition to general disapproval of being gendered by a machine , P2 and T3 worried about whether their personal data would be stored and potentially sold to third parties . AGR is a Tool for Surveillance Participants ( P2 , P4 , P7 , P9 , T3 ) were fearful that AGR could and would be used as a tool for surveillance . Lack of privacy was a common misgiving among participants , for whom AGR was perceived as uncomfortable and invasive : “I would certainly be uncomfortable … it’s a [ high ] level of invasiveness of photographing or recording your face . ” – P2 But even more salient was the fear that surveillance infrastructures erected by AGR systems could be used to persecute the trans community . P2 was “sketched out” by AGR as well as facial recognition technology , believing they open new “dystopian surveillance state potentials . ” The consequences cited ranged from being the target of yet another source of scrutiny , to being physically brutalized : “We don ' t need to feel another robot overlord set of eyes … If security cameras were constantly on the hunt for my gender , I think that I’d be brutalized . . . I think that I’d be exposed to a lot of violences that are unnecessary . ” – P4 P1 , P8 , P9 , and P10 also drew lines between AGR and other surveillance technologies that have been used by those in power to harm queer communities . P8 recalled a surveillance program run by their city without residents’ knowledge , leading to skepticism about AGR technology : “A lot has been illuminated in recent years about abuses of power and what can happen when people who have whatever bias are in control of certain surveillance technologies … I’m also very aware of histories of surveillance being used against queer communities or communities of people of color . ” – P8 AGR Threatens Safety Participants identified several ways in which gender binarism , lack of autonomy , and surveillance imposed by AGR systems might present threats to emotional and physical wellbeing ( P1 , T1 , P9 ) and civil liberties ( all participants ) of trans individuals . From the unrelenting emotional toll of daily microaggressions , to losing your job , or to being physically attacked , a future with AGR was interpreted as highly consequential : “It’s easier for a cis person to be like , “Oh , that ' s wrong . ” But it’s more of a daily fight for a lot of trans folks , so I could see that being … harmful . And also , it could be a safety issue . . . Like the fear of being outed … That could have job consequences or physical safety consequences . ” – T1 All participants raised concerns about the possibility of AGR being misused to perpetrate discriminatory acts . P4 and P7 expressed fear that the system would be used in “malicious” ( P7 ) ways to target trans people , especially those whose physical features do not conform to expectations of the AGR system : “People who don’t fit promptly into the gender binary would be highly brutalized . ” – P4 P2 imagined such a system preventing trans people from entering bathrooms that match their gender : “You could see in some state , like if North Carolina’s still insistent on passing bathroom laws , detectors that try and gauge your gender based on your face every time you want to enter a restroom . ” – P2 Similarly , P2 , P4 , T2 , and P9 expressed concern that AGR could stand in the way of trans people gaining traction in CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 7 the legislation that directly affects them , including healthcare and bathroom laws : “ [ People think ] ‘God forbid , trans folks win and consolidate power and have full authority over what gets made and what gets done’ , I think that [ for anti - trans people , AGR ] would be the worst . Being able to out any trans person , being able to track trans people . ” – T2 P9 expressed her fears about the current presidential administration in the United States using AGR in conjunction with tracking or a registry to exclude trans individuals from government employment : “If you think about it politically now , if Hitler had had that ability , there would be a lot of dead people . If Trump has that ability , there’s a way to exclude trans individuals from government , from employment . ” – P9 Finally , P9 explained that AGR technology is situated in a cultural and historical context that augment the probable uses and impacts of technology adoption . While AGR might someday be acceptable , in the current time it is a dangerous proposition for the trans community : “Maybe in the future [ possibilities for misuse become ] less , but at this point in time [ AGR is ] dangerous , because I think it can be misused much more than it can be used appropriately . ” – P9 Most participants ( P1 , P4 , P7 , T1 , T2 , P10 , T3 ) were concerned that AGR will perpetuate and potentially amplify systems of oppression for transgender people . AGR’s Impact Beyond the Transgender Community While participants’ concerns were primarily about how AGR could negatively impact the trans community , they also expressed concerns about impacts to society at large : “ [ AGR is not just a trans issue , it is ] a misogyny and patriarchy issue because we’ve created these narrow boxes around policing male and policing female . ” – T1 P1 , P4 , and T1 brought up the possible harm that misgendering cis people might cause . “Guessing people ' s gender wrong [ is ] very bad … People would react badly to that … You could equally misread my wife for her gender using that facial technology . ” – P6 Some participants ( P2 , P4 , T2 ) also described how in their opinion biases in software design are not limited to gender and extend to other issues , such as race . P4 mentioned disliking Snapchat because its filters are “racist” and the filters “always give you blue eyes” and “change your face shape to be more European . ” “ [ AGR ] may have trouble with the way that … different races have different facial patterns . ” – P2 P4 and T2 both blamed this on limited representation in technology design . They attributed bias to limiting datasets to data about white people , insinuating this could impact how gender is predicted . “Based on… the proportion of people [ tech companies ] hire being trans or not , being women or not , the fact that tech is still majority white and male … What kinda people are the bases for these predictions of ‘what gender are you ? ’” - T2 Incorporating Gender into Technology While participants did not have a positive view of AGR , they expressed positive views of other forms of technology in general . Several participants had suggestions on how designers could adopt more inclusive practices to incorporate gender into technology . T1 and P9 suggested giving people autonomy over the way they are gendered by technology . T1 suggested that technology should “just ask” people for their pronouns , while P9 said to avoid pronouns all together and choose to use names given by users . “If somebody’s developing a robot that’s going to walk up to me and start interacting with me , the robot should say , ‘Hello . How are you ? I’m R2D2 . What’s your name ? ’ and once they get the input of my name’s [ P9 ] , the robot should respond with , ‘ [ P9 ] , would you like a cup of coffee ? ’” – P9 P5 , P7 , and T1 recommended allowing users to explicitly consent and confirm their own identities , supporting their choice and autonomy in the interaction . “I would definitely recommend having an option … for the person to be able to confirm their identity or have an option … for people to address that before it affects them . ” – P7 T1 conveyed hope that , if technology designers implemented AGR well , it could be “empowering : ” “If technology were more inclusive , it could normalize a lot of things for the trans community , and for other folks , especially because it’s so ubiquitous . ” – T1 P7 , P8 , T1 , T2 , and T3 advocated for including diverse voices in technology design . T1 suggested designers should stop “making assumptions about wanting to help the majority . ” P8 thought it was important to have “amateurs meddling” in the creation of new technology . “ [ When ] making new technology … there should be a team … to proof it or check it for these different things that would be really beneficial to people of marginalized identities . ” – P7 “It’s about where you source the pictures you’re using from , it’s about how they are identified , it’s about what the categorical labels you use are , it’s about how the result is exposed to the user , and it’s about saying no to applications of the software that … are provably harmful . ” – T3 Finally , P4 and T3 suggested the effort put into AGR be used to create something more positive instead . “Instead of investing energy into inventing a technology that genders people … You could use that software to understand how gender exists in world . ” – P4 DISCUSSION : ENGAGING GENDER DIVERSITY IN HCI Results from our study show that transgender individuals have serious concerns about the possible negative impact of AGR and similar technologies that incorporate gender . Both technologist and non - technologist participants expressed doubts about whether AGR could ever be CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 8 successfully implemented and wondered about the negative consequences of its inaccuracy both on the transgender community and society as a whole . Further , participants were concerned that if AGR is used for surveillance and non - consensual monitoring of individuals , it could result in discrimination and oppression . These findings augment prior research on transgender stigmatization [ 19 , 25 ] and the impact of misgendering [ 31 ] in non - technical contexts and paint a picture of transgender individuals’ perceptions , attitudes and concerns , towards emerging technologies . Gender ( Mis ) Representations Can Do Harm One of the most prominent themes arising in interviews was that AGR incorporates flawed representations of gender that are consequential . Notably , gender is not something that can be accurately read through physical features ( face , body , or voice ) by either humans or digital algorithms . This concern was present regardless of whether the participants were technology professionals or not . Much of the current research on AGR is geared towards improving the accuracy of gender recognition as a function of physical features presumed to connote gender [ 26 , 29 , 48 ] . However , there is a large body of research that confirms participants’ view that gender identity is primarily subjective and internal ( e . g . , [ 5 , 40 ] ) . This fact poses significant challenges to the notion that gender can be recognized automatically , at least with contemporary recognition strategies [ 26 , 29 , 48 ] . Even if AGR designers allow that their systems will make mistakes only as often as any other human might , the concern remains that misgendering perpetrated by a machine is distinct . Our findings show that , for many individuals , automatic misgendering is perceived to be more harmful than being misgendered by another person . There is no apparent evidence that the internal and subjective aspects of gender identity are discussed when designing AGR algorithms ( see , e . g . , [ 26 , 29 , 48 ] ) . We believe this is an unfortunate omission that leads to the re - materializing of misconceptions about gender in technical systems and scientific reports . For example , AGR designers that understand trans users’ concerns regarding “catfishing” might avoid describing the gender transition process as “face disguise” ( i . e . , deceptive modification of facial features ) in their scientific publications [ 29 , 48 ] . Intersecting with gender , participants expressed unease about AGR’s implications for racial bias . Previous work has identified embedded bias in algorithms that result in , for example , face recognition algorithms failing to detect dark skin or Asian eyes [ 17 ] . While algorithmic bias has been documented for over a decade [ 11 ] , the present study is the first to explore gender representation bias whereby binary conceptualizations of gender are embedded in automatic recognition systems . Threats to User Autonomy and Privacy Another central concern of our participants was the possibility that the deployment of AGR technology would challenge their autonomy and compromise their privacy . Ethical questions about the deployment of technologies that can be used for surveillance are an increasingly prominent concern in HCI research [ 23 ] especially in relation to emerging ubiquitous systems and the Internet of Things [ 21 , 38 ] . Our participants’ concerns echo arguments for protecting human autonomy and user choice when interacting with digital systems [ 9 , 10 ] . In this context , participants were worried that AGR technology could be deployed and used on them without their explicit consent or knowledge , evoking the idea that users might be used by technology and not vice versa . These concerns often translated to a lack of trust towards systems that are not transparent about their functionality . The participants did not expect to know everything about how a system functions but to have enough information to know that it does not compromise their autonomy and privacy . This sense of mistrust can be a barrier to technology adoption and needs to be addressed to support positive user experiences . This might signal a design opportunity to explore ways to create more trust in users by incorporating mechanisms and cues that assure them that the system will not undermine its users’ autonomy . Supporting Gender Diversity : Towards a Trans Inclusive HCI Our results underline a need to support gender diversity when designing digital systems . Our participants were concerned about AGR algorithms materializing and reifying dominant gender binaries that are harmful not only to the trans community , but to people of all gender identities . They were concerned that limited gender representation ( i . e . , excluding transgender identities from datasets ) could impact system functionality and reinforce stereotypical expectations of “male” and “female” in society . Participants noted that their concerns would not be mitigated by “adding more categories , ” but rather by supporting user flexibility and autonomy . The participants’ concerns extended to cisgender people who might look or act differently from the majority : individuals of different races , cisgender individuals who present androgynously and individuals with unique facial structures . Additionally , many participants questioned why more input from their community has not been included in the design and development of systems that interpret gender . This question is reminiscent of the concerns of other minorities who are historically left out of conversations about large scale technology deployment , including ethnic minorities and people with disabilities [ 21 , 44 ] . These observations align with current recommendations for avoiding gender reductionism and ethnocentrism in psychology and medical clinical research practice [ 14 ] . The CHI community has long recognized the importance CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 9 of studying gender in various contexts , including technology education , gaming culture and online virtual representation , among others ; it is time that this recognition is extended to the study of the experiences of individuals with diverse gender identities , creating a space for trans HCI or “TransCHI . ” DESIGN RECOMMENDATIONS In this section , we present a series of recommendations based on our participants’ input on possible ways to incorporate gender in digital systems in an inclusive and sensitive manner . We present these recommendations not as end - all - be - all solutions but rather as a starting set of considerations that need to be applied after careful examination of each system’s specific context and application to avoid harming users in any way . Inform users if and how they might be gendered and let them opt out . Our first recommendation is that designers carefully consider the potentially harmful impact of incorporating gender in their designs and avoid including it if unnecessary . If they decide to include it , we recommend that systems explicitly inform users if they are going to be gendered and provide them with an option to decline . Beyond letting users know if they are going to be gendered , systems should provide transparency on how users will be gendered . This can be implemented by allowing users to access ( and edit ) their personal data that is used to make gendering decisions . Systems should also let users know if gender information will be stored or used beyond each current application . For example , Facebook currently collects binary gender information when users register for new accounts , then allows users to customize the gender identity visible in their profile . However , only the original binary distinction is used to serve advertisements . Applying our recommendation , users should be informed that their response to the gender prompt will be used to adapt site content , that they can edit it in the future , and that it is optional for using the service . Let users define their own gender identity . Since it is difficult ( if not impossible ) to automatically recognize gender accurately for all users and the consequences of misgendering are significant , we recommend that designers who want to incorproate gender provide for users themselves to define their own gender identity . This choice should go beyond limited categories and allow users to define their gender identity according to their own experiences [ 42 ] . Additionally , to support gender transition and fluidity , as well as the dynamicity of language describing gender identities , we recommend that such systems allow users to modify and update their gender information ( or opt out ) over time . One benefit of this approach is to reduce the occurrence of misgendering while supporting user self - expression and autonomy . Additioanlly , leveraging diverse gender information , as opposed to reducing to a binary , can allow digital platforms to provide better service to more customers ( e . g . , by providing approporiate ads to transgender individuals ) . Incorporate gender diversity when designing systems . When developing new systems with modules that enable gendering ( e . g . , AGR systems ) , we recommend that careful attention be given to how they are implemented and whether the underlying databases are representative of the user population . While inclusivity is important , it is critical that it is not gained at the expense of the autonomy and privacy of historically marginalized groups ( e . g . , by scraping images of trans people without consent [ 20 , 29 ] ) . Further , we recommend including the perspectives of individuals with a variety of gender identities early in the design process , by working with diverse teammembers and adopting participatory design approaches . LIMITATIONS AND FUTURE WORK In this study , participants did not actually experience being gendered by AGR ; their perspectives were based on possible future scenarios . We limited our participants to transgender individuals . In the future , we would like to interview cisgender individuals ( including cisgender technologists ) to compare and contrast responses . We also would like to explore participatory processes and design outcomes that include transgender participants and values . CONCLUSION With emerging technologies attempting to incorporate complex human attributes , such as gender , it is important to take into account the perspectives of diverse populations who might be directly or indirectly impacted . We studied the perceptions and attitudes of transgender individuals towards automatic gender recognition ( AGR ) , a technology that aims to classify a person’s gender based on their physical characteristics . We found that participants had overwhelmingly negative attitudes towards AGR and questioned if it can offer any beneficial applications to end users . They also expressed doubt about whether AGR can accurately identify gender and described the harm of being misgendered by it . Finally , participants expressed serious concerns about threats that it can pose to their autonomy and privacy . We presented several recommendations for incorporating gender in system design , including informing users if their gender information would be used , giving them the option to opt out and allowing them to communicate their own gender identity to systems . With respect to AGR , we are not necessarily arguing for the elimination of gender recognition from technology , but a careful consideration of the implications of incorporating it . In totality , these recommendations point towards an approach to gender that is more inclusive , collaborative and sensitive to human autonomy and choice . ACKNOWLEDGMENTS We would like to thank our participants for sharing their experiences , insights , and opinions with us . More broadly , we would like to thank the transgender community , and the resilience , strength , and wisdom of individuals within it . CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 10 REFERENCES 1 . AJL - Algorithmic Justice League . Retrieved September 7 , 2017 from https : / / www . ajlunited . org / 2 . Rena Bivens and Oliver L . Haimson . 2016 . Baking Gender Into Social Media Design : How Platforms Shape Categories for Users and Advertisers . Social Media + Society 2 , 4 : 1 – 12 . 3 . Lindsay Blackwell , Jean Hardy , Tawfiq Ammari , Tiffany Veinot , Cliff Lampe , and Sarita Schoenebeck . 2016 . LGBT Parents and Social Media : Advocacy , Privacy , and Disclosure During Shifting Social Movements . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) , 610 – 622 . 4 . Zach Blas . 2011 . Facial Weaponization Suite | zach blas . Zac Blas . Retrieved September 7 , 2017 from http : / / www . zachblas . info / works / facial - weaponization - suite / 5 . Judith Butler . 1988 . Performative Acts and Gender Constitution : An Essay in Phenomenology and Feminist Theory . Theatre Journal 40 , 4 : 519 . 6 . Liangliang Cao , Mert Dikmen , Yun Fu , and Thomas S . Huang . 2008 . Gender Recognition from Body . In Proceeding of the 16th ACM International Conference on Multimedia ( MM ’08 ) , 725 – 728 . 7 . John Christman . 2015 . Autonomy in Moral and Political Philosophy . The Stanford Encyclopedia of Philosophy . 8 . Andrew R . Flores , Jody L . Herman , Gary J . Gates , and Taylor N . T . Brown . 2016 . How Many Adults Identify As Transgender in the United States ? Los Angeles , CA . Retrieved August 22 , 2017 from https : / / williamsinstitute . law . ucla . edu / wp - content / uploads / How - Many - Adults - Identify - as - Transgender - in - the - United - States . pdf 9 . Batya Friedman and Peter H . Kahn . 2008 . Human values , ethics , and design . Lawrence Erlbaum Associates . 10 . Batya Friedman . 1996 . Value - sensitive design . Interactions 3 , 6 : 16 – 23 . 11 . Batya Friedman , Eric Brok , Susan King Roth , and John Thomas . 1996 . Minimizing bias in computer systems . ACM SIGCHI Bulletin 28 , 1 : 48 – 51 . 12 . Beatrice A . Golomb , David T . Lawrence , Terrence J . Sejnowski . 1990 . Sexnet : A Neural Network Identifies Sex from Human Faces . Advances in Neural Information Processing Systems 3 : 572 – 577 . 13 . Jaime M . Grant , Lisa A . Mottet , Justin Tanis , Jack Harrison , Jody L . Herman , and Mara Keisling . 2011 . Injustice at Every Turn : A Report of the National Transgender Discrimination Survey . Washington National Center for Transgender Equality and National Gay and Lesbian Task Force . 14 . Ansara Y . Gavriel and Peter Hegarty . 2014 . Methodologies of misgendering : Recommendations for reducing cisgenderism in psychological research . Feminism & Psychology 24 , 2 : 259 – 270 . 15 . Oliver L . Haimson , Jed R . Brubaker , Lynn Dombrowski , and Gillian R . Hayes . 2015 . Disclosure , Stress , and Support During Gender Transition on Facebook . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing ( CSCW ’15 ) , 1176 – 1190 . 16 . Oliver L . Haimson , Jed R . Brubaker , Lynn Dombrowski , and Gillian R . Hayes . 2016 . Digital Footprints and Changing Networks During Online Identity Transitions . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) , 2895 – 2907 . 17 . David Hankerson , Andrea R Marshall , Jennifer Booker , Houda El Mimouni , Imani Walker , and Jennifer A Rode . 2016 . Does Technology Have Race ? In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems ( CHI EA ’16 ) , 473 – 486 . 18 . Lucas D Introna and David Wood . 2004 . Picturing Algorithmic Surveillance : The Politics of Facial Recognition Systems . Surveillance & Society 2 , 2 / 3 : 177 – 198 . 19 . Sandy E . James , Jody L . Herman , Susan Rankin , Mara Keisling , Lisa Mottet , and Ma’ayan Anafi . 2016 . The Report of the 2015 U . S . Transgender Survey . Retrieved August 22 , 2017 from http : / / www . transequality . org / sites / default / files / docs / us ts / USTS Full Report - FINAL 1 . 6 . 17 . pdf 20 . James Vincent . 2017 . Transgender YouTubers had their videos grabbed to train facial recognition software . The Verge . Retrieved August 28 , 2017 from https : / / www . theverge . com / 2017 / 8 / 22 / 16180080 / transg ender - youtubers - ai - facial - recognition - dataset 21 . Haiyan Jia , Mu Wu , Eunhwa Jung , Alice Shapiro , and S . Shyam Sundar . 2012 . Balancing Human Agency and Object Agency : An End - User Interview Study of the Internet of Things . In Proceedings of the 2012 ACM Conference on Ubiquitous Computing ( UbiComp ’12 ) , 1185 - 1188 . 22 . Stephanie Julia Kapusta . 2016 . Misgendering and Its Moral Contestability . Hypatia 31 , 3 : 502 – 519 . 23 . Gopinaath Kannabiran and Marianne Graves Petersen . 2010 . Politics at the Interface : A Foucauldian Power Analysis . In Proceedings of the 6th Nordic Conference on Human - Computer Interaction : Extending Boundaries ( NordiCHI ’10 ) , 695 - 698 . 24 . Michal Kosinski , David Stillwell , and Thore Graepel . 2013 . Private traits and attributes are predictable from digital records of human behavior . In Proceedings of CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 11 the National Academy of Sciences of the United States of America 110 , 15 : 5802 – 5 . 25 . Irwin Krieger . 2017 . The Impact of Stigma on Transgender and Non - Binary Youth . In Counseling Transgender and Non - Binary Youth : The Essential Guide . Jessica Kingsley Publisher , 46 – 47 . 26 . Vijay Kumar , R . Raghavendra , Anoop Namboodiri , and Christoph Busch . 2016 . Robust transgender face recognition : Approach based on appearance and therapy factors . In IEEE Conference on Identity , Security and Behavior Analysis ( ISBA 2016 ) , 1 – 7 . 27 . Chien - Cheng Lee and Chung - Shun Wei . 2013 . Gender Recognition Based On Combining Facial and Hair Features . In Proceedings of International Conference on Advances in Mobile Computing & Multimedia ( MoMM ’13 ) , 537 – 540 . 28 . Gayathri Mahalingam and Karl Ricanek . HRT Transgender Dataset . Retrieved August 23 , 2017 from http : / / www . faceaginggroup . com / hrt - transgender / 29 . Gayathri Mahalingam and Karl Ricanek . 2013 . Is the eye region more reliable than the face ? A preliminary study of face - based recognition on a transgender dataset . In IEEE Conference on Biometrics : Theory , Applications and Systems ( BTAS 2013 ) , 1 – 7 . 30 . Erno Makinen and Roope Raisamo . 2008 . Evaluation of gender classification methods with automatically detected and aligned faces . IEEE Transactions on Pattern Analysis and Machine Intelligence 30 , 3 : 541 – 547 . 31 . Kevin A . McLemore . 2015 . Experiences with Misgendering : Identity Misclassification of Transgender Spectrum Individuals . Self and Identity 14 , 1 : 51 – 74 . 32 . Choon Boon Ng , Yong Haur Tay , and Bok Min Goi . 2015 . A review of facial gender recognition . Pattern Analysis and Applications 18 , 4 : 739 – 755 . 33 . Mei Ngan and Patrick Grother . 2015 . Face Recognition Vendor Test ( FRVT ) - Performance of Automated Gender Classification Algorithms . NIST Interagency / Internal Report ( NISTIR ) - 8052 . 34 . Nick Whigham . 2017 . Glitch in digital pizza advert goes viral , shows disturbing future of facial recognition tech . news . com . au . Retrieved September 18 , 2017 from http : / / www . news . com . au / technology / innovation / desig n / glitch - in - digital - pizza - advert - goes - viral - shows - disturbing - future - of - facial - recognition - tech / news - story / 3b43904b6dd5444a279fd3cd6f8551db 35 . Y . Avriel Ansara . 2012 . Cisgenderism in medical settings : How collaborative partnerships can challenge structural violence . In Out of the Ordinary : LGBT Lives . Cambridge Scholars Publishing , Cambridge , 102 – 122 . 36 . Cathy O’Neil . 2016 . Weapons of Math Destruction : How Big Data Increases Inequality and Threatens Democracy . Crown Random House . 37 . Özlem Özbudak , Mürvet Kirci , Yüksel Çakir , and Ece Olcay Güne ş . 2010 . Effects of the Facial and Racial Features on Gender Classification . In Proceedings of the Mediterranean Electrotechnical Conference ( MELECON ) , 26 – 29 . 38 . Joyojeet Pal , Anandhi Viswanathan , Priyank Chandra , Anisha Nazareth , Vaishnav Kameswaran , Hariharan Subramonyam , Aditya Johri , Mark S . Ackerman , and Sile O’Modhrain . 2017 . Agency in Assistive Technology Adoption : Visual Impairment and Smartphone Use in Bangalore . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( CHI ’17 ) , 5929 – 5940 . 39 . Arnaud Ramey and Miguel A . Salichs . 2014 . Morphological Gender Recognition by a Social Robot and Privacy Concerns . In Proceedings of the 2014 ACM / IEEE Iternational conference on Human - Robot Interaction ( HRI ’14 ) : 272 – 273 . 40 . Jemima Repo . 2017 . The Biopolitics of Gender . Oxford University Press . 41 . Bridget A Sarpu . 2015 . Google : The Endemic Threat to Privacy . Journal of High Technology Law XV , April 2012 . Retrieved September 7 , 2017 from https : / / sites . suffolk . edu / jhtl / files / 2014 / 12 / Sarpu - Google - The - Endemic - Threat - to - Privacy . pdf 42 . Christine Satchell . 2010 . Women are people too : The problem of designing for gender . Retrieved December 18 , 2017 from https : / / www . cl . cam . ac . uk / events / experiencingcriticalth eory / Satchell - WomenArePeople . pdf 43 . Ari Schlesinger , W . Keith Edwards , and Rebecca E . Grinter . 2017 . Intersectional HCI : Engaging Identity through Gender , Race , and Class . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( CHI ’17 ) , 5412 – 5427 . 44 . Michael Skirpan and Micha Gorelick . 2017 . The Authority of “Fair” in Machine Learning . Retrieved September 7 , 2017 from https : / / arxiv . org / pdf / 1706 . 09976 . pdf 45 . Jinshan Tang , Xiaoming Liu , Huaining Cheng , and Kathleen M . Robinette . 2011 . Gender recognition using 3 - D human body shapes . IEEE Transactions on Systems , Man and Cybernetics Part C : Applications and Reviews 41 , 6 : 898 – 908 . 46 . Jisha C . Thankappan and Sumam M . Idicula . 2010 . Language Independent Voice - Based Gender Identification System . In Proceedings of the 1st Amrita ACM - W Celebration on Women in Computing in India ( A2CWiC ’10 ) , 1996 : 1 – 6 . 47 . Transgender ( 1974 ) . In Merriam - Webster ' s CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 12 Dictionary . Retrieved September 3 , 2017 from https : / / www . merriam - webster . com / dictionary / transgender 48 . Archana Vijayan , Shyma Kareem , and Jubilant J . Kizhakkethottam . 2016 . Face Recognition Across Gender Transformation Using SVM Classifier . Procedia Technology 24 : 1366 – 1373 . 49 . Shiqi Yu , Tieniu Tan , Kaiqi Huang , Kui Jia , and Xinyu Wu . 2009 . A Study on Gait - Based Gender Classification . IEEE Transactions on Image Processing 18 , 8 : 1905 – 1910 . CHI 2018 Best Paper Award CHI 2018 , April 21 – 26 , 2018 , Montréal , QC , Canada Paper 8 Page 13