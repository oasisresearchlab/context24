Comprehension Strategies of End - User Programmers in an Event - Driven Application Susan Wiedenbeck College of IST Drexel University Philadelphia , PA 19104 USA susan . wiedenbeck @ cis . drexel . edu Alec Engebretson Department of IST Doane College Crete , NE 68333 USA aengebretson @ doane . edu Abstract Teachers may engage in end - user programming to support student learning or administrative activities associated with teaching . The objective of this research is to understand strategies used by teachers in program comprehension and to identify specific problems they face . A think - aloud study was conducted of teachers comprehending an event - driven application , consisting of a graphical user interface and the scripts controlling it . We found that end users followed a strongly top - down strategy and breadth - wise exploration of the application . Depth - wise exploration was observed in half the teachers . Teachers varied greatly in their motivations and persistence to dig deeply into the code . Problems of the teachers included difficulties comprehending the event - driven application , given the distributed nature of the code , choosing appropriate inputs for running the program , and reasoning about the results of their test runs . 1 . Introduction Teachers are one among many populations of end - user programmers . While most teachers in elementary and secondary school do not program , some subset of teachers create programs in aid of teaching their students or to support their administrative duties as teachers . Teachers who are end - user programmers are not limited to computer teachers , nor to science and mathematics teachers . GUI applications , simulations , and interactive web pages are all examples of programs that can be useful in teaching subjects ranging from history to mathematics . Although they may create programs , teachers are far from being traditional programmers [ 4 ] . They have little or no formal training in programming . They create programs only to enhance their work or the learning of their students . They create programs intermittently , as the need arises and their time permits . Their emphasis is on writing a program to fulfill their need in a timely manner . Thus , they are rarely interested in increasing their programming knowledge and skills beyond the minimum necessary to write the programs they need [ 8 ] . They trade off learning in favor of achievement of their immediate goal [ 3 ] . Teachers , as typical end users , have little interest in the elegance of the code or even its readability . They also may pay little attention to dependability , even though it has been shown that end - user programs are often filled with errors [ 1 ] . Clearly , a program with errors can do more harm than good in the classroom , for example , a science simulation based on incorrect calculations . However , teachers may simply assume that their programs are correct if the results look correct or if they can find a few confirming test cases [ 12 ] . Teachers generally write applications for their own use . However , in the increasingly computerized schoolroom , end - user programs may be passed from teacher to teacher . One teacher may modify a program that another created , either alone or in collaboration with the original programmer , a phenomenon previously recognized in end - user programming [ 8 ] . The teacher has to understand the program sufficiently in order to modify it . Thus , the comprehension of the teacher becomes an important issue . A good case can be made that end users are best served by task - specific languages that are designed for particular kinds of end - user tasks [ 8 ] . However , in many cases task - specific tools are not readily available . End - users are compelled to use the tools at hand . In schools a kind of programming tool likely to be available is rapid application development ( RAD ) environments , which can be used to build GUI applications . Examples are Visual Basic , Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE Hypercard 1 and its descendents , Supercard , Metacard , and Hyperstudio . Although the basic interface building features of RADs are typically relatively easy to master , the end user is faced with writing code to enable interface actions using a textual scripting language . Applications developed with such tools are highly event - driven because every program sequence is directly invoked by user actions in the interface . In this paper we report on a study of teachers comprehending a GUI application developed in Hypercard . Our goal is to understand their comprehension strategies at a detailed level as well as particular problems they face . Since program modification is dependent on program comprehension , this may lead to better insight into ways to support end users who modify programs written by others . 2 . Program comprehension studies 2 . 1 Direction of comprehension The direction of comprehension refers to whether the strategic approach to program comprehension is mostly top - down , mostly bottom - up , or a mix . Pennington [ 9 ] describes a bottom - up theory of program comprehension , based on text theory . She proposes that the programmer forms two program abstractions during comprehension : the program model and the domain model . The program model is a low - level abstraction consisting of detailed knowledge of operations at a level close to the program code and of control flow relations representing the order of execution . The programmer forms the program - level abstraction early during program understanding by gathering information in the program text . The domain model is a higher - level abstraction containing knowledge of data flow and function . The programmer forms this abstraction after forming the program model . The domain model is built upon the detailed knowledge in the program model , along with the programmer’s domain knowledge and knowledge of stereotypical programming plans [ 11 ] . Pennington’s studies [ 9 ] provide support for her bottom - up theory . Brooks [ 2 ] proposes a top - down theory of program comprehension . In Brooks’s model program understanding is hypothesis - driven . The programmer begins by making a general hypothesis about the program’s function , based on information other than the program code itself , such as a title , a brief 1 Hypercard is no longer supported by its developer , Apple Computer , Inc . , but has been succeeded by a similar program , Supercard . description , or examples of the run - time behavior . This hypothesis leads the programmer to expect to find certain objects and operations in the program , leading to another level of more specific hypotheses . At this point the programmer has concrete things to look for in the program code , so hypothesis verification is attempted . The programmer scans the program text searching for “beacons , ” which are defined as typical indicators of the presence of a particular structure or operation . If such beacons are found , the programmer may conclude with high probability that the particular structure or operation is present . This strengthens the current hypothesis . However , if no beacons for the hypothesized structures and operations are found , the programmer must study the program text more carefully and may ultimately have to revise or reject the hypothesis . Comprehension continues through rounds of deepening and refinement of hypotheses and hypothesis verification until the program has been understood sufficiently . While Brooks’s theory has not been tested in its entirety , the use of beacons in comprehension has been shown empirically [ 14 ] . Mixed models of program comprehension have also been proposed , e . g . , [ 6 , 13 ] . These models consider programmers to behave opportunistically in program understanding , using top - down or bottom - up comprehension strategies depending on the situation . von Mayrhauser and Vans [ 13 ] argue that programmers use a top - down approach to understanding when they are working in a familiar domain where they know a large number of programming plans [ 11 ] . On the other hand , when they encounter code that is new to them and in an unfamiliar domain , they use a bottom - up approach . 2 . 2 Breadth and depth of comprehension The breadth of familiarity with the program also affects programmers’ comprehension . Littman et al . [ 7 ] found that programmers use two strategies concerning scope of comprehension : systematic and as - needed . Using the systematic strategy , the programmer attempts to gain a broad understanding of the program before carrying out modifications . The goal is to understand the design of the original programmer so that modifications fit with the existing code . On the other hand , using the as - needed strategy , the programmer attempts to minimize the amount of code that has to be understood . The programmer does not attempt to understand the overall design of the program but concentrates instead on the functioning of selected local parts of the code that may be critically involved in the modification . Littman et al . found that programmers who used the systematic approach Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE carried out modifications more successfully , probably because systematic study increased ability to detect interactions between the modification and existing code . The depth of comprehension refers to whether the programmer digs deeply into the code . Digging deeply involves reading the code in detail , reasoning about it , and perhaps simulating it to understand exactly what is happening . By contrast , less deep comprehension may result from merely running the program , reading program comments and skimming the code . While depth of comprehension is not necessary in every programming task , having sufficient in - depth knowledge should help a programmer modify a program in an expeditious manner . 3 . Methodology 3 . 1 Participants The participants were 10 secondary school teachers in the humanities , social sciences , and science who were current users of Hypercard and its associated Hypertalk scripting language . They had basic knowledge of one other programming language , such as would be gained in an introductory programming course ; however , they had not studied programming beyond one introductory course . Furthermore , they had not programmed professionally or programmed extensively for their own purposes . None of the participants were computer teachers or taught programming . The average participant was a 38 - year - old male who had used a Macintosh for 8 . 5 years . He had used Hypercard / Hypertalk , weekly or monthly for an average of 5 years . He had authored an average of 10 original programs , containing an average of 32 interface screen , with a few hundred lines of scripting . In addition to authoring programs , he used Hypercard to run programs and to modify programs authored by himself or others . 3 . 2 Materials A Hypercard application for grade recording and reporting was developed for the study . Although the participants used Hypercard mostly to create applications for use in teaching their students , this administrative application was chosen because grade administration was familiar to all the teachers . In the Gradebook application , student scores were recorded along with the student name and identification number . A set - up option allowed users to customize the grade book for a course , assigning specific grade categories ( e . g . , tests , quizzes , etc . ) , the number of grade entries in each category ( e . g . , 2 tests , 5 quizzes ) , and the weights for the categories ( e . g . , each test weighted as 25 percent of the final grade and each quiz as 10 percent of the final grade ) . Set - up options also included adding a student or removing a student from the grade book . Grade recording options included recording a score for all students in a course or recording a score for a single student . Once scores were entered for a student , the user could click the “recalculate” button to update the student grade record , calculate the average of each category ( e . g . , the average of the tests ) , and also calculate the weighted average of all grades entered for a student . Grade reporting options allowed the user to produce a report for all students in a class or a report for an individual student . Whether reporting grades for a class or for an individual , the user could choose a summary report giving only the averages or a detail report including the raw scores . The grade book application consisted of 7 interface screens . The screens contained fields , in which data could be written , and buttons , which were used to navigate the application and to invoke actions , such as saving student data entered in a field , calculating averages , and creating and displaying a report . There were 21 scripts in the application , each associated with some event , most often a button click by the user . Of the 21 scripts , 8 were key scripts that contained most of the functionality of the program . The remaining scripts were mostly navigational , i . e . , scripts that displayed a different screen if a certain button was pressed . The amount of scripting was similar to the upper limit that the study participants reported having written in their own Hypercard applications , approximately 400 to 500 lines of scripting . The key parts of the code were based strongly on sorting , searching , data entry , and report generation tasks , areas of programming in which the participants had experience . The scripts were well structured and did not use programming “tricks” that would be confusing to the participants . There was a small amount of documentation in the scripts . The program had no known bugs ( and none was discovered by the participants ) . However , to make the application similar to an end user’s code , there was little checking of constraints on input data . Furthermore , Hypertalk is an untyped language , so type errors were not automatically detected . Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE 3 . 3 Procedure Each participant took part in one session that lasted approximately three hours . After an overview and short practice session , the participant was given 45 minutes to study and comprehend the Gradebook application . To explain the comprehension task , a scenario was given in which a fellow teacher had developed the grade book and had given it to the participant , but the participant needed to understand the application in order to customize it to his or her own needs . The participant was allowed to read and execute the program but not make changes to the interface or the scripts . All reading of scripts was done online ; hardcopy was not provided . Participants had access to a manual , The Complete Hypercard Handbook by Danny Goodman , in addition to help provided in the development environment . The participant was allowed to take notes . The think - aloud method was used [ 5 ] . While studying the program , the participant was asked to speak aloud everything that he or she was thinking . The investigator prompted the participant to speak if more than 30 seconds passed without a verbalization . The session was videotaped . The video camera was focused on the computer screen . The participants were given practice on thinking aloud during the practice session . The objective of using the think - aloud method was to gather detailed data about how these end users understand a program . The think - aloud method allows the researcher to have a window into the participant’s thought processes and behavior , something that is not possible in purely quantitative studies of performance . After the comprehension phase ended , the participant modified the program , given a set of specifications . In this paper we are concerned with the first phase , the initial comprehension of the grade book . We do not report results of the modification phase . 4 . Results For the analysis , the verbalizations were transcribed and annotated with the participant’s physical actions . The verbalizations were then divided into episodes , and the episodes were classified according to the type of activity . 4 . 1 Results on direction of comprehension Table 1 shows that the participants spent the largest part of their time on the interface , an average of approximately 25 minutes out of the total time of 45 minutes . Interface activities included viewing the 7 screens , reading instructions on the screens , entering data , and testing the grade book functions . These activities were dominated by entering data and testing grade book functions . The participants were strongly focused on understanding the manifest behaviors of the application . The mean time spent on the interface before looking at scripts was 12 : 48 minutes , i . e . , over one - quarter of the total time . This suggests that participants wanted a firm understanding of how the application worked from a user’s viewpoint before delving into the scripting . However , it is also evident from the difference between the mean interface total time and the mean interface time before first use of the scripts that participants also returned to the interface while studying the scripts . Although activities in the interface predominated , the mean time spent on the scripts was substantial , accounting for 34 % of the total . Table 1 : Time on interface and scripts ( N = 10 ) Interface total time Scripts total time Interface time before first use of a script Mean 25 : 24 15 : 12 12 : 48 Median 25 : 05 16 : 12 11 : 54 Min 08 : 30 03 : 28 04 : 30 Max 37 : 42 28 : 48 28 : 00 As previously discussed , the literature on human behavior in programming debates whether programmers predominantly take a top - down , bottom - up , or mixed approach to comprehension . The evidence in this study supports a top - down approach in this population of end users . A top - down approach involves generating hypotheses about the program initially from sources outside the code , then attempting to verify the hypotheses in the code . To verify an hypothesis the programmer normally has to search the code , generating lower - level sub - hypotheses , until the hypothesis is specific enough to be verified or rejected with respect to the code . The verification may take place by recognition of beacons and plans in the code or by careful reading . Two results in this study that suggest a top - down approach are the predominance of interface activities over script activities and the relatively long period of time using the interface before viewing scripts . Both of these measures indicate that the participants first approached the comprehension task by gaining an understanding of the program’s overt behavior from sources external to the code . Their sources included instructions available in the interface and the behavior of the program when they ran it . Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE A top - down theory of comprehension predicts hypothesis generation and verification as primary activities in program comprehension . To investigate hypotheses and hypothesis verification , we counted the number of hypotheses that participants made . A hypothesis was defined as any explicit verbalization that predicted or stated an expectation about the program . We also counted attempts at hypothesis verification , since a top - down method implies that the programmer must carry out activities to accept or reject the hypotheses . We included all hypothesis verification attempts , whether successful or not , because an attempt at verification of hypotheses indicates behavior that conforms with Brooks’s theory , regardless of its success . Our protocols indicated that 256 hypotheses were made . The mean per participant was 25 . 6 ; the median was 24 . 3 . The highest number of hypotheses of any participant was 38 and the lowest was 5 . In terms of verification , we first determined that of the 256 hypotheses , 121 could be verified by running functions in the interface with appropriate test data . By contrast , almost all the hypotheses , 245 in all , could be verified from the scripts . ( A few hypotheses could not be verified from the scripts because they involved properties of interface objects that were not stored in the scripts . ) Our results on attempts at hypothesis verification showed that verification of hypotheses that could be answered via the interface was attempted in 102 of the 121 hypotheses , or 84 % . Participants had a preference for verifying hypotheses from the interface , if possible . Leaving out hypotheses that participants verified from running interface functions , participants attempted to verify 94 of the remaining 143 hypotheses , or 66 % , via the scripts . Only a few hypotheses were redundantly verified by a participant via both the interface and the scripts . The most obviously reason for not attempting verification was that participants got distracted and forgot about the hypothesis if they did not act on it immediately . This was more likely to happen when the participant had to verify a hypothesis from the scripts because there was overhead in deciding which script to look at and physically opening the script . Another reason for lack of verification was that the participant did not want to stop what he or she was currently doing ( for example , the participant verbalization : “I’m busy here . I’ll look at that later . ” ) or did not want to sift through the scripts ( “I don’t want to spend too much time on this…I assume that must be how it works . ” ) . The verification itself was done in the interface by comparing the result of an action to the participant’s expectation . This sometimes involved the participant comparing interface screens ( “Did this get copied to there ? …no , what’s going on ? ” ) or doing a mental calculation ( “So I add the exams and divide by three and the same for the daily work . . . How do they [ i . e . , the program ] get 4 . 58 ? ” ) . In verification from the scripts the participants first found the correct script ( sometimes difficult in itself ) , then skimmed through the code to find the right area . They usually based their verification on recognition of a plan or beacon , i . e . , stereotypical code that they expected to find if the hypothesis was correct . The scripting language contained a rich set of built in functions , e . g . , sort , search , round . Recognizing these beacons , and sometimes examining parameters , constituted the participants’ verification ( “I see the Search function and it looks like it searches for a student by id” ) . 4 . 2 Results on breadth of comprehension Table 2 shows data about the breadth of exploration in this study . Table 2 : Coverage of scripts ( N = 10 ) Total script visits Unique scripts visited ( out of 21 ) Key scripts visited ( out of 8 ) Mean 17 . 5 11 . 4 6 . 8 Median 19 . 0 12 . 0 7 . 0 Min 2 2 2 Max 33 21 8 There were 21 scripts in the application . In this event - driven program , scripts were associated with actions , usually clicks on buttons in the interface . Each script was accessed through the button it was attached to . There was no central repository of scripts where the user could scan them linearly . In spite of the distributed nature of the code , Table 2 suggests that the coverage of the scripts was broad . While on average only 54 % of the unique scripts ( 11 . 4 out of 21 ) were visited , many of the scripts were trivial , such as a script to advance to the next screen on a button click , or a script to quit the program . We therefore identified 8 scripts in which the main work of the program was done . On average the participants visited 6 . 8 of these key scripts , and 8 of the 10 participants visited all the key scripts . Therefore , they did view the important code . On the other hand , only one person made the point of exhaustively opening all the scripts ; others ignored stereotyped scripts and scripts of navigational buttons , sometimes explicitly stating the reason ( “That’s not going to be anything interesting . ” or “Just another ‘go to somewhere , ’ no doubt . ” ) . The ‘total script visits’ in Table 2 are substantially higher than Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE unique script visited . This suggests that scripts were comprehended incrementally , with multiple visits to the same script . 4 . 3 Results on depth of comprehension Depth of understanding of scripts is also important in program comprehension . Table 3 gives some indirect data on depth , the amount of time spent in scripts . The mean time per script visit is short , indicating that participants tended to move quickly between scripts . However , scripts were often visited more than once , as suggested by the second column in Table 3 . It shows that the mean time on the 21 unique scripts was double the mean time of all script visits . The most revealing data , however , is the time spent on the 8 key scripts . Here the mean time is over 14 minutes . Comparing to Table 1 , we see that the mean time on all scripts was 15 : 12 minutes . This indicates that participants spent a very high portion of their time examining the important parts of the program . Table 3 : Time on scripts ( N = 10 ) Time per script visit Time on unique scripts ( 21 scripts ) Time on key scripts ( 8 scripts ) Mean 0 : 57 1 : 59 14 : 44 Median 0 : 46 1 : 22 14 : 07 Min 0 : 10 0 : 19 03 : 44 Max 2 : 50 6 : 59 25 : 29 In the videotapes we also examined skimming scripts vs . close reading of the scripts . Most of the reading was code skimming , often reading aloud without pausing to articulate the purpose of the code . In many cases this was appropriate behavior , because the code was implementing stereotypical plans , for example , prompting for data or writing out labels for a report . To investigate depth more fully , we analyzed in detail a complex part of one script that calculated a weighted average of grades within deeply nested loops . In this segment 5 of the participants read the code closely , verbalized their understandings , and reasoned about the code ( “But this isn’t doing the average the way I would do an average…But , but these numbers look like the weights…Are there weights ? ” ) . Four participants merely skimmed this part , and one never looked at this script . Other indications of efforts to gain depth of understanding were simulation of code and comparisons of scripts . While close reading during walkthrough of the code did occur , hand simulation of code segments with values was very rare . We observed this only two times in the study , both in the calculation segment mentioned above . Some participants explicitly stated their low motivation for simulating the text ( “I don’t want to get involved in plugging in numbers unless I have to debug something . I guess I’m fundamentally lazy . ” ) . Comparison of scripts was observed in 5 participants . Three of them opened up scripts that they thought might be related side - by - side in order to compare them . Two other participants looked at scripts sequentially and commented on their similar structure and operations without a direct comparison of code . Close - coupling of the participant’s interface behavior and script examination was observed 6 times among 4 people . In close - coupling the participant moved quickly between interface actions and code in an extended sequences lasting between 10 and 18 iterations . Typically , the participant began by closely reading the code , switched to the interface to carry out an action that exercised that code , then evaluated whether the results of the action were what he or she expected . Failure to confirm expectations led to further rounds of reasoning about and running the program . In three of the cases the participant moved the script so that it was side - by - side with the relevant interface screen in order to facilitate quick shifts of attention between them . Participants ran the program with their own inputs and , as indicated above , spent a substantial amount of time understanding the functions in the interface . However , in general the participants used poor test data and procedures . A surprising problem was failure to pay attention to the results of tests , for example , calculating a student’s grade , then barely glancing at the result , just noting that a number appeared but not attending to whether it made sense for the data input ( “There’s the average…good , it’s working” ) . Frequently they used too little data , for example , testing the calculation of student grades with only two scores , in a single grade category , for a single student . Some participants saw inputting data as useless ( “I’ll put in two [ scores ] . I’m not wasting my time here . ” ) . An opposite problem was putting in too much data . Some participants filled a student’s grade record with 15 or more scores in different grade categories . When they clicked the button to recalculate the average , they wanted to compare the result to their expectation but were not able to carry the mental calculation through with so much data . We observed only two participants who took an iterative approach . They started with limited data , clicked the recalculate button , then compared the result to their expectations . If it was correct , they iterated by adding more data in different Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE categories , and so on . We noted that only 3 participants verified special cases , for example , boundary conditions ( “Can I put in a score over 100 ? Let’s try…Looks like it accepted it . ” ) . 4 . 4 Other observations The event - driven nature of the application posed comprehension problems to these end - users . Since the code is attached to events , it is scattered and there is no central place to view it online . They frequently got lost in their exploration of the program , not being sure what scripts they had already seen ( “I’m getting lost ; I need to write them down . ” ) . More importantly , participants were unsure which event caused a certain output . For example , if data were input on one screen and displayed on the next , the participants tended to look for a ‘save’ script associated with the field where the data was displayed on the second screen . They had difficulty realizing that the significant scripts were likely to be associated with actions , such as the button press when the user finished entering data , not with passive containers , like data display fields . There were many questions about data , as a result . These were manifested as data flow questions ( “How did that data get into that field ? ” or “It’s strange that the fields don’t have scripts , because they actually do contain data . ” ) . This problem of understanding data flow was compounded because Hypercard has background fields that may appear on multiple screens but are actually located on a common background , not the individual screens . Participants were familiar with the background and knew how to view it , but the frequently failed to remember that fields could be located in the background . They also had difficulty relating variable names in the scripts to data fields in the interface . The lack of visual highlighting of such relationships was a detriment to the participants . The participants had different views of themselves . Some saw themselves as programmers in this task ( “I am look at this not from the point of a user , but from the point of someone who wants to understand the script . ” ) , while others saw themselves primarily as teachers ( “As an educator I want their grade to be correct…if it is , I’m happy . ” ) . All the participants were educators by profession , and they cast themselves in that role at least part of the time . This was manifested by the many critiques they gave of the interface : its ease of use , the flexibility of the grade book , and its format . In the course of these verbalizations they expressed many user and data requirements , making certain episodes sound like a task analysis . Half of the participants expressed that they were not highly motivated about studying the code ( “I’m not , to be honest , willing to sit and think through all of these [ scripts ] . . . ” ) . Four of the participants expressed surprise or dismay when they saw a long script of approximately 60 lines ( “It’s hard for me to focus on scripts that are quite so long . ” or “Gosh , it’s long ! ” ) . As is typical [ 3 ] , the participants did not want to learn more about the programming language by consulting the manual ( “I could look in the book , but…um…I don’t want to look in the book . ” ) . Only 3 participants used the manual , and then only for 1 to 3 minutes . 5 . Discussion and conclusions The results show that the teachers took a strongly top - down approach , gained good breadth of familiarity with the program , and varied highly in the depth of comprehension they achieved . They are teachers first , have moderate programming knowledge , and know the domain of administering student grades . This may explain the top - down approach . The teachers preferred strongly to exploit the knowledge they could gain via the interface before digging into the code , a reasonable approach since much important information about the program was visible by running it . Their domain knowledge helped them reason about and interpret what they saw . This is consistent with arguments that domain expertise facilitates a top - down approach [ 13 ] . The teachers ran the program , usually many times , to comprehend it . However , in doing so , they chose simple inputs and rarely attempted to explore the limits of its functioning . They assumed the program worked correctly and ran it with inputs that were likely to confirm the assumption [ 12 ] . When teachers found omissions , such as lack of range checking , they usually found it by accident , not from a systematic effort to identify test cases . Their simple , and essentially unplanned inputs , interfered with understanding the program’s behavior . This underlines the importance of testing methodologies and tools for end users [ 10 , 15 ] . The breadth of comprehension of the teachers was evident in the high coverage of the key scripts . With the exception of one or two teachers , they got a good overview of the scripts and generally only ignored scripts that appeared trivial . They did not know what specific modifications they would ultimately be asked to carry out , but they learned where the key functions were located and the conventions used in the scripts . A more striking difference among teachers was the variability in their depth of comprehension of the scripts . While significant time was spent viewing the scripts , only half the teachers did detailed walkthroughs of important scripts , and close - coupled Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE their study of the code with running tests of the interface . They were clearly aided by beacons [ 14 ] and plan - like code [ 11 ] in recognizing program functions , but they went beyond recognition to verify specifically how key segments of code worked . Compared to these teachers who “dug into” the code , half the teachers stayed at rather a higher level . They skimmed code , recognized beacons and plans , but rarely made attempts to understand completely a code segment once its general purpose had been identified . Most teachers were hindered by the “scattered” code associated with buttons ( one teacher referring to it as “hide - and - seek with the code” ) and the lack of tools to visually highlight relationships between elements in the interface and the scripts . Some of these teachers expressed a general lack of motivation for reading code . The phenomenon of low motivation for certain aspects of programming has been identified previously in end users [ 8 ] , and current research is targeting this problem [ 16 ] . In our case , the lack of motivation for reading is likely to affect the efficiency of modifying the more complex parts of the program . Interestingly , after the comprehension period , all the teachers expressed confidence in their understanding of the program and their ability to modify it . Research indicates that such confidence may be misplaced [ 15 ] . This population of teachers would be aided by tools to help them choose appropriate inputs to test a program , support for reasoning about the results generated in testing the program , and visual mechanisms to link elements in the scripts to objects in the interface . In addition , innovative , ‘just enough , just in time’ training should be considered to support teachers effectively . 6 . Acknowledgments This work was supported in part by the EUSES Consortium via NSF grant CCR - 0324844 . Any opinions , findings and conclusions or recommendations expressed in this material are those of the author ( s ) and do not necessarily reflect the views of the National Science Foundation ( NSF ) . 7 . References [ 1 ] Boehm , B . and Basili , V . Software defect reduction top 10 list . Computer , 34 ( 1 ) , 2000 , pp . 135 - 137 . [ 2 ] Brooks , R . Towards a theory of the comprehension of computer programs . International Journal of Man - Machine Studies , 18 , 1983 , pp . 543 - 554 . [ 3 ] Carroll , J . M . and Rosson , M . B . Paradox of the active user , Interfacing Thought . MIT Press , Cambridge , MA , 1987 . [ 4 ] Engebretson , A . and Wiedenbeck , S . Novice comprehension of programs using task - specific and non - task - specific constructs . IEEE Symposia on Human - Centric Computing and Environments , IEEE , NY , 2002 , pp . 11 - 18 . [ 5 ] Ericsson , K . A . and Simon , H . A . Protocol Analysis : Verbal Reports As Data . MIT Press , Cambridge , MA , 1993 . [ 6 ] Letovsky , S . Cognitive processes in program comprehension . In E . Soloway and S . Iyengar ( Eds . ) , Empirical Studies of Programmers , Ablex , Norwood , NJ , 1986 , pp . 80 - 98 . [ 7 ] Littman , D . C . , Pinto , J . , Letovsky , S . , and Soloway , E . Mental models and software maintenance . In Soloway , E . and Iyengar , S . ( Eds . ) , Empirical Studies of Programmers . Ablex , Norwood , NJ , 1986 , pp . 80 - 98 . [ 8 ] Nardi , B . A Small Matter of Programming . MIT Press , Cambridge , MA , 1993 . [ 9 ] Pennington , N . Stimulus structures and mental representations in expert comprehension of computer programs . Cognitive Psychology , 19 , 1987 , pp . 295 - 341 . [ 10 ] Rothermel , K . , Cook , C . H . , Burnett , M . M . , Schonfeld , J . , Green , T . R . G . , and Rothermel , G . WUSIWYT testing in the spreadsheet paradigm : an empirical evaluation . ICSE2000 , ACM , NY , 2000 , pp . 230 - 239 . [ 11 ] Soloway , E . and Ehrlich , K . Empirical studies of programming knowledge . IEEE Transactions on Software Engineering , SE - 10 , 1984 , pp . 595 - 609 . [ 12 ] Teasley , B . , Leventhal , L . M , and Rohlman , D . S . Positive test bias in software testing by professionals : what’s right and what’s wrong . In C . R . Cook , J . C . Scholtz , and J . C . Spohrer ( Eds . ) , Empirical Studies of Programmers : Fifth Workshop . Ablex , Norwood , NJ , 1993 , pp . 206 - 221 . [ 13 ] von Mayrhauser , A . and Vans , A . M . Program understanding during software adaptation tasks . International Conference on Software Maintenance , IEEE , NY , 1998 , pp . 316 - 325 . [ 14 ] Wiedenbeck , S . The initial stage of program comprehension . International Journal of Man - Machine Studies , 35 , 517 - 540 . [ 15 ] Wilcox , E . , Atwood , J . , Burnett , M . , Cadiz , J . , and Cook , C . Does continuous visual feedback aid debugging in direct - manipulation programming systems ? CHI’97 , ACM , NY , 1997 , pp . 22 - 27 . [ 16 ] Wilson , A . , Burnett , M . , Beckwith , L , Granatir , O . , Casburn , L . , Cook , C . , Durham , M . , and Rothermel , G . Harnessing curiosity to increase correctness in end - user programming . CHI 2003 , ACM , NY , 2003 , pp . 305 - 312 . Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing ( VLHCC’04 ) 0 - 7803 - 8696 - 5 / 04 $ 20 . 00 IEEE