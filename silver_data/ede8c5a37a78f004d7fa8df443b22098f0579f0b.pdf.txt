Budget - Optimal Task Allocation for Reliable Crowdsourcing Systems Citation Karger , David R . , Sewoong Oh , and Devavrat Shah . “Budget - Optimal Task Allocation for Reliable Crowdsourcing Systems . ” Operations Research 62 , no . 1 ( February 2014 ) : 1 – 24 . As Published http : / / dx . doi . org / 10 . 1287 / opre . 2013 . 1235 Publisher Institute for Operations Research and the Management Sciences ( INFORMS ) Version Author ' s final manuscript Accessed Thu Jan 24 00 : 11 : 34 EST 2019 Citable Link http : / / hdl . handle . net / 1721 . 1 / 87088 Terms of Use Creative Commons Attribution - Noncommercial - Share Alike Detailed Terms http : / / creativecommons . org / licenses / by - nc - sa / 4 . 0 / The MIT Faculty has made this article openly available . Please share how this access benefits you . Your story matters . Budget - Optimal Task Allocation for Reliable Crowdsourcing Systems David R . Karger ∗ , Sewoong Oh † , and Devavrat Shah ‡ March 27 , 2013 Abstract Crowdsourcing systems , in which numerous tasks are electronically distributed to numerous “information piece - workers” , have emerged as an eﬀective paradigm for human - powered solving of large scale problems in domains such as image classiﬁcation , data entry , optical character recognition , recommendation , and proofreading . Because these low - paid workers can be un - reliable , nearly all such systems must devise schemes to increase conﬁdence in their answers , typically by assigning each task multiple times and combining the answers in an appropriate manner , e . g . majority voting . In this paper , we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price ( i . e . , number of task assignments ) that must be paid to achieve a target overall reliability . We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers’ answers . We show that our algorithm , inspired by belief propagation and low - rank matrix approximation , signiﬁcantly outperforms majority voting and , in fact , is optimal through comparison to an oracle that knows the reliability of every worker . Further , we compare our approach with a more general class of algorithms which can dynamically assign tasks . By adaptively deciding which questions to ask to the next arriving worker , one might hope to reduce uncertainty more eﬃciently . We show that , perhaps surprisingly , the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and non - adaptive scenarios . Hence , our non - adaptive approach is order - optimal under both scenarios . This strongly relies on the fact that workers are ﬂeeting and can not be exploited . Therefore , architecturally , our results suggest that building a reliable worker - reputation system is essential to fully harnessing the potential of adaptive designs . ∗ Computer Science and Artiﬁcial Intelligence Laboratory and Department of EECS at Massachusetts Institute of Technology . Email : karger @ mit . edu † Department of Industrial and Enterprise Systems Engineering at University of Illinois at Urbana - Champaign . Email : swoh @ illinois . edu ‡ Laboratory for Information and Decision Systems and Department of EECS at Massachusetts Institute of Tech - nology . Email : devavrat @ mit . edu . This work was supported in parts by NSF EMT project , AFOSR Complex Networks project and Army Research Oﬃce under MURI Award 58153 - MA - MUR . 1 a r X i v : 1110 . 3564v4 [ c s . L G ] 26 M a r 2013 1 Introduction Background . Crowdsourcing systems have emerged as an eﬀective paradigm for human - powered problem solving and are now in widespread use for large - scale data - processing tasks such as im - age classiﬁcation , video annotation , form data entry , optical character recognition , translation , recommendation , and proofreading . Crowdsourcing systems such as Amazon Mechanical Turk 1 , establish a market where a “taskmaster” can submit batches of small tasks to be completed for a small fee by any worker choosing to pick them up . For example a worker may be able to earn a few cents by indicating which images from a set of 30 are suitable for children ( one of the beneﬁts of crowdsourcing is its applicability to such highly subjective questions ) . Because these crowdsourced tasks are tedious and the pay is low , errors are common even among workers who make an eﬀort . At the extreme , some workers are “spammers” , submitting arbitrary answers independent of the question in order to collect their fee . Thus , all crowdsourcers need strategies to ensure the reliability of their answers . When the system allows the crowdsourcers to identify and reuse particular workers , a common approach is to manage a pool of reliable workers in an explore / exploit fashion . However in many crowdsourcing platforms such as Amazon Mechanical Turk , the worker crowd is large , anonymous , and transient , and it is generally diﬃcult to build up a trust relationship with particular workers . 2 It is also diﬃcult to condition payment on correct answers , as the correct answer may never truly be known and delaying payment can annoy workers and make it harder to recruit them to your task next time . Instead , most crowdsourcers resort to redundancy , giving each task to multiple workers , paying them all irrespective of their answers , and aggregating the results by some method such as majority voting . For such systems there is a natural core optimization problem to be solved . Assuming the taskmaster wishes to achieve a certain reliability in her answers , how can she do so at minimum cost ( which is equivalent to asking how she can do so while asking the fewest possible questions ) ? Several characteristics of crowdsourcing systems make this problem interesting . Workers are neither persistent nor identiﬁable ; each batch of tasks will be solved by a worker who may be completely new and who you may never see again . Thus one cannot identify and reuse particularly reliable workers . Nonetheless , by comparing one worker’s answer to others’ on the same question , it is possible to draw conclusions about a worker’s reliability , which can be used to weight their answers to other questions in their batch . However , batches must be of manageable size , obeying limits on the number of tasks that can be given to a single worker . Another interesting aspect of this problem is the choice of task assignments . Unlike many inference tasks which makes inferences based on a ﬁxed set of signals , our algorithm can choose which signals to measure by deciding which questions to include in which batches . In addition , there are several plausible options : for example , we might choose to ask a few “pilot questions” to each worker ( just like a qualifying exam ) to decide on the reliability of the worker . Another possibility is to ﬁrst ask few questions and based on the answers decide to ask more questions or not . We would like to understand the role of all such variations in the overall optimization of budget for reliable task processing . In the remainder of this section , we will deﬁne a formal probabilistic model that captures these aspects of the problem . We consider both a non - adaptive scenario , in which all questions are 1 http : / / www . mturk . com 2 For certain high - value tasks , crowdsourcers can use entrance exams to “prequalify” workers and block spammers , but this increases the cost of the task and still provides no guarantee that the workers will try hard after qualiﬁcation . 2 asked simultaneously and all the responses are collected simultaneously , and an adaptive scenario , in which one may adaptively choose which tasks to assign to the next arriving worker based on all the previous answers collected thus far . We provide a non - adaptive task allocation scheme and an inference algorithm based on low - rank matrix approximations and belief propagation . We will then show that our algorithm is order - optimal : for a given target error rate , it spends only a constant factor times the minimum necessary to achieve that error rate . The optimality is established through comparisons to the best possible non - adaptive task allocation scheme and an oracle estimator that can make optimal decisions based on extra information provided by an oracle . In particular , we derive a parameter q that characterizes the ‘collective’ reliability of the crowd , and show that to achieve target reliability ε , it is both necessary and suﬃcient to replicate each task Θ ( 1 / q log ( 1 / ε ) ) times . This leads to the next question of interest : by using adaptive task assignment , can we ask fewer questions and still achieve the same error rate ? We , somewhat surprisingly , show that the optimal costs under this adaptive scenario scale in the same manner as the non - adaptive scenario ; asking questions adaptively does not help ! Setup . We consider the following probabilistic model for crowdsourcing . There is a set of m binary tasks which is associated with unobserved ‘correct’ solutions : { t i } i ∈ [ m ] ∈ { ± 1 } m . Here and after , we use [ N ] to denote the set of ﬁrst N integers . In the image categorization example stated earlier , a set of tasks corresponds to labeling m images as suitable for children ( + 1 ) or not ( − 1 ) . We will be interested in ﬁnding the true solutions by querying noisy workers who arrive one at a time in an on - line fashion . An algorithmic solution to crowdsourcing consists of two components : a task allocation scheme and an inference algorithm . At task allocation phase queries are made sequentially according to the following rule . At j - th step , the task assignment scheme chooses a subset T j ⊆ [ m ] of tasks to be assigned to the next arriving noisy worker . The only constraint on the choice of the batch is that the size | T j | must obey some limit on the number of tasks that can be given to a single worker . Let r denote such a limit on the number of tasks that can be assigned to a single worker , such that all batches must satisfy | T j | ≤ r . Then , a worker j arrives , whose latent reliability is parametrized by p j ∈ [ 0 , 1 ] . For each assigned task , this worker gives a noisy answer such that A ij = (cid:26) t i with probability p j , − t i otherwise , and A ij = 0 if i / ∈ T j . ( Throughout this paper , we use boldface characters to denote random variables and random matrices unless it is clear from the context . ) The next assignment T j + 1 can be chosen adaptively , taking into account all of the previous assignments and the answers collected thus far . This process is repeated until the task assignment scheme decides to stop , typically when the total number of queries meet a certain budget constraint . Then , in the subsequent inference phase , an inference algorithm makes a ﬁnal estimation of the true answers . We say a task allocation scheme is adaptive if the choice of T j depends on the answers collected on previous steps , and it is non - adaptive if it does not depend on the answers . In practice , one might prefer using a non - adaptive scheme , since assigning all the batches simultaneously and having all the batches of tasks processed in parallel reduces latency . However , by switching to an adaptive task allocation , one might be able to reduce uncertainty more eﬃciently . We investigate this possibility in Section 2 . 4 , and show that the gain from adaptation is limited . Note here that at the time of assigning tasks T j for a next arriving worker j , the algorithm is not aware of the latent reliability of the worker . This is consistent with how real - world crowdsourcing 3 works , since taskmasters typically have no choice over which worker is going to pick up which batch of tasks . Further , we make the pessimistic assumption that workers are neither persistent nor identiﬁable ; each batch of tasks T j will be solved by a worker who may be completely new and who you may never see again . Thus one cannot identify and reuse particularly reliable workers . This is a diﬀerent setting from adaptive games [ LW89 ] , where you have a sequence of trials and a set of predictions is made at each step by a pool of experts . In adaptive games , you can identify reliable experts from their past performance using techniques like multiplicative weights , whereas in crowdsourcing you cannot hope to exploit any particular worker . The latent variable p j captures how some workers are more diligent or have more expertise than others , while some other workers might be trying to cheat . The random variable A ij is independent of any other event given p j . The underlying assumption here is that the error probability of a worker does not depend on the particular task and all the tasks share an equal level of diﬃculty . Hence , each worker’s performance is consistent across diﬀerent tasks . We discuss a possible generalization of this model in Section 2 . 7 . We further assume that the reliability of workers { p j } are independent and identically dis - tributed random variables with a given distribution on [ 0 , 1 ] . As one example we deﬁne the spammer - hammer model , where each worker is either a ‘hammer’ with probability q or is a ‘spam - mer’ with probability 1 − q . A hammer answers all questions correctly , meaning p j = 1 , and a spammer gives random answers , meaning p j = 1 / 2 . It should be noted that the meaning of a ‘spammer’ might be diﬀerent from its use in other literature . In this model , a spammer is a worker who gives uniformly random labels independent of the true label . In other literature in crowdsourcing , the word ‘spammer’ has been used , for instance , to refer to a worker who always gives ‘ + ’ labels [ RY12 ] . Another example is the beta distribution with some parameters α > 0 and β > 0 ( f ( p ) = p α − 1 ( 1 − p ) β − 1 / B ( α , β ) for a proper normalization B ( α , β ) ) [ Hol11 , RYZ + 10b ] . A distribution of p j characterizes a crowd , and the following parameter plays an important role in capturing the ‘collective quality’ of this crowd , as will be clear from our main results : q ≡ E [ ( 2 p j − 1 ) 2 ] . A value of q close to one indicates that a large proportion of the workers are diligent , whereas q close to zero indicates that there are many spammers in the crowd . The deﬁnition of q is consistent with use of q in the spammer - hammer model and in the case of beta distribution , q = 1 − ( 4 αβ / ( ( α + β ) ( α + β + 1 ) ) ) . We will see later that our bound on the achievable error rate depends on the distribution only through this parameter q . When the crowd population is large enough such that we do not need to distinguish whether the workers are ‘sampled’ with or without replacement , then it is quite realistic to assume the existence of a prior distribution for p j . In particular , it is met if we simply randomize the order in which we upload our task batches , since this will have the eﬀect of randomizing which workers perform which batches , yielding a distribution that meets our requirements . The model is therefore quite general . On the other hand , it is not realistic to assume that we know what the prior is . To execute our inference algorithm for a given number of iterations , we do not require any knowledge of the distribution of the reliability . However , q is necessary in order to determine how many times a task should be replicated and how many iterations we need to run to achieve a certain target reliability . We discuss a simple way to overcome this limitation in Section 2 . 2 . The only assumption we make about the distribution is that there is a bias towards the right answer , i . e . E [ p j ] > 1 / 2 . Without this assumption , we can have a ‘perfect’ crowd with q = 1 , but 4 everyone is adversarial , p j = 0 . Then , there is no way we can correct for this . Another way to justify this assumption is to deﬁne the “ground truth” of the tasks as what the majority of the crowd agrees on . We want to learn this consensus eﬃciently without having to query everyone in the crowd for every task . If we use this deﬁnition of the ground truth , then it naturally follows that the workers are on average more likely to be correct . Throughout this paper , we are going to assume that there is a ﬁxed cost you need to pay for each response you get regardless of the quality of the response , such that the total cost is proportional to the total number of queries . When we have a given target accuracy we want to achieve , and under the probabilistic crowdsourcing model described in this section , we want to design a task allocation scheme and an inference algorithm that can achieve this target accuracy with minimal cost . Possible deviations from our model . Some of the main assumptions we make on how crowd - sourcing systems work are ( a ) workers are neither identiﬁable nor reusable , ( b ) every worker is paid the same amount regardless of their performance , and ( c ) each worker completes only one batch and she completes all the tasks in that batch . In this section , we discuss common strategies used in real crowdsourcing that might deviate from these assumptions . First , there has been growing interest recently in developing algorithms to eﬃciently identify good workers assuming that worker identities are known and workers are reusable . Imagine a crowdsourcing platform where there are a ﬁxed pool of identiﬁable workers and we can assign the tasks to whichever worker we choose to . In this setting , adaptive schemes can be used to signiﬁcantly improve the accuracy while minimizing the total number of queries . It is natural to expect that by ﬁrst exploring to ﬁnd better workers and then exploiting them in the following rounds , one might be able to improve performance signiﬁcantly . Donmez et al . [ DCS09 ] proposed IEThresh which simultaneously estimates worker accuracy and actively selects a subset of workers with high accuracy . Zheng et al . [ ZSD10 ] proposed a two - phase approach to identify good workers in the ﬁrst phase and utilize the best subset of workers in the second phase . Ertekin et al . [ EHR11 ] proposed using a weighted majority voting to better estimate the true labels in CrowdSense , which is then used to identify good workers . The power of such exploration / exploitation approaches were demonstrated on numerical ex - periments , however none of these approaches are tested on real - world crowdsourcing . All the experiments are done using pre - collected datasets . Given these datasets they simulate a labor mar - ket where they can track and reuse any workers they choose to . The reason that the experiments are done on such simulated labor markets , instead of on popular crowdsourcing platforms such as Amazon Mechanical Turk , is that on real - world crowdsourcing platforms it is almost impossible to track workers . Many of the popular crowdsourcing platforms are completely open labor markets where the worker crowd is large and transient . Further , oftentimes it is the workers who choose which tasks they want to work on , hence the taskmaster cannot reuse particular workers . For these reasons , we assume in this paper that the workers are ﬂeeting and provide an algorithmic solution that works even when workers are not reusable . We show that any taskmaster who wishes to outperform our algorithm must adopt complex worker - tracking techniques . Furthermore , no worker - tracking technique has been developed that has been proven to be foolproof . In particu - lar , it is impossible to prevent a worker from starting over with a new account . Many tracking algorithms are susceptible to this attack . Another important and closely related question that has not been formally addressed in crowd - 5 sourcing literature is how to diﬀerentiate the payment based on the inferred accuracy in order to incentivize good workers . Regardless of whether the workers are identiﬁable or not , when all the tasks are completed we get an estimate of the quality of the workers . It would be desirable to pay the good workers more in order to incentivize them to work for us in the future tasks . For example , bonuses are built into Amazon Mechanical Turk to be granted at the taskmaster’s discretion , but it has not been studied how to use bonuses optimally . This could be an interesting direction for future research . It has been observed that increasing the cost on crowdsourcing platforms does not directly lead to higher quality of the responses [ MW10 ] . Instead , increasing the cost only leads to faster responses . Mason and Watts [ MW10 ] attributes this counterintuitive ﬁndings to an “anchoring” eﬀect . When the ( expected ) payment is higher , workers perceive the value of their work to be greater as well . Hence , they are no more motivated than workers who are paid less . However , these studies were done in isolated experiments , and the long term eﬀect of taskmasters’ keeping a good reputation still needs to be understood . Workers of Mechanical Turk can manage reputation of the taskmasters using for instance Turkopticon 3 , a Firefox extension that allows you to rate taskmasters and view ratings from other workers . Another example is Turkernation 4 , an on - line forum where workers and taskmasters can discuss Mechanical Turk and leave feedback . Finally , in Mechanical Turk , it is typically the workers who choose which tasks they want to work on and when they want to stop . Without any regulations , they might respond to multiple batches of your tasks or stop in the middle of a batch . It is possible to systematically prevent the same worker from coming back and repeating more than one batch of your tasks . For example , on Amazon’s Mechanical Turk , a worker cannot repeat the same task more than once . However , it is diﬃcult to guarantee that a worker completes all the tasks in a batch she started on . In practice , there are simple ways to ensure this by , for instance , conditioning the payment on completing all the tasks in a batch . A problem with restricting the number of tasks assigned to each worker ( as we propose in Section 2 . 1 ) is that it might take a long time to have all the batches completed . Letting the workers choose how many tasks they want to complete allows a few eager workers to complete enormous amount of tasks . However , if we restrict the number of tasks assigned to each worker , we might need to recruit more workers to complete all the tasks . This problem of tasks taking long time to ﬁnish is not just restricted to our model , but is a very common problem in open crowdsourcing platforms . Ipeirotis [ Ipe10 ] studied the completion time of tasks on Mechanical Turk and observed that it follows a heavy tail distribution according to a power law . Hence , for some tasks it takes signiﬁcant amount of time to ﬁnish . A number of strategies have been proposed to complete tasks on time . This includes optimizing pricing policy [ FHI11 ] , continuously posting tasks to stay on the ﬁrst page [ BJJ + 10 , CHMA10 ] , and having a large amount of tasks available [ CHMA10 ] . These strategies are eﬀective in attracting more workers fast . However , in our model , we assume there is no restrictions on the latency and we can wait until all the batches are completed , and if we have good strategies to reduce worker response time , such strategies could be incorporated into our system design . Prior work . Previous crowdsourcing system designs have focused on developing inference algo - rithms assuming that the task assignments are ﬁxed and the workers’ responses are already given . 3 http : / / turkopticon . diﬀerenceengines . com 4 http : / / turkernation . com 6 None of the prior work on crowdsourcing provides any systematic treatment of task assignment under the crowdsourcing model considered in this paper . To the best of our knowledge , we are the ﬁrst to study both aspects of crowdsourcing together and , more importantly , establish optimality . A naive approach to solve the inference problem , which is widely used in practice , is majority voting . Majority voting simply follows what the majority of workers agree on . When we have many spammers in the crowd , majority voting is error - prone since it gives the same weight to all the responses , regardless of whether they are from a spammer or a diligent workers . We will show in Section 2 . 3 that majority voting is provably sub - optimal and can be signiﬁcantly improved upon . If we know how reliable each worker is , then it is straightforward to ﬁnd the maximum likelihood estimates : compute the weighted sum of the responses weighted by the log - likelihood . Although , in reality , we do not have this information , it is possible to learn about a worker’s reliability by comparing one worker’s answer to others’ . This idea was ﬁrst proposed by Dawid and Skene , who introduced an iterative algorithm based on expectation maximization ( EM ) [ DS79 ] . They con - sidered the problem of classifying patients based on labels obtained from multiple clinicians . They introduce a simple probabilistic model describing the clinicians’ responses , and gave an algorithmic solution based on EM . This model , which is described in Section 2 . 7 , is commonly used in modern crowdsourcing settings to explain how workers make mistakes in classiﬁcation tasks [ SPI08 ] . This heuristic algorithm iterates the following two steps . In the M - step , the algorithm estimates the error probabilities of the workers that maximizes the likelihood using the current estimates of the answers . In the E - step , the algorithm estimates the likelihood of the answers using the current estimates of the error probabilities . More recently , a number of algorithms followed this EM approach based on a variety of probabilistic models [ SFB + 95 , WRW + 09 , RYZ + 10a ] . The crowdsourcing model we consider in this paper is a special case of these models , and we discuss their relationship in Section 2 . 7 . The EM approach has also been widely applied in classiﬁcation problems , where a set of labels from low - cost noisy workers is used to ﬁnd a good classiﬁer [ JG03 , RYZ + 10a ] . Given a ﬁxed budget , there is a trade - oﬀ between acquiring a larger training dataset or acquiring a smaller dataset but with more labels per data point . Through extensive experiments , Sheng , Provost and Ipeirotis [ SPI08 ] show that getting repeated labeling can give considerable advantage . Despite the popularity of the EM algorithms , the performance of these approaches are only empirically evaluated and there is no analysis that gives performance guarantees . In particular , EM algorithms are highly sensitive to the initialization used , making it diﬃcult to predict the quality of the resulting estimate . Further , the role of the task assignment is not at all understood with the EM algorithm ( or for that matter any other algorithm ) . We want to address both questions of task allocation and inference together , and devise an algorithmic solution that can achieve minimum error from a ﬁxed budget on the total number of queries . When we have a given target accuracy , such an algorithm will achieve this target accuracy with minimum cost . Further , we want to provide a strong performance guarantee for this approach and show that it is close to a fundamental limit on what the best algorithm can achieve . Contributions . In this work , we provide the ﬁrst rigorous treatment of both aspects of designing a reliable crowdsourcing system : task allocation and inference . We provide both an order - optimal task allocation scheme ( based on random graphs ) and an order - optimal algorithm for inference ( based on low - rank approximation and belief propagation ) on that task assignment . We show that our algorithm , which is non - adaptive , performs as well ( for the worst - case worker distribution ) as 7 the optimal oracle estimator which can use any adaptive task allocation scheme . Concretely , given a target probability of error ε and a crowd with collective quality q , we show that spending a budget which scales as O ( ( 1 / q ) log ( 1 / ε ) ) is suﬃcient to achieve probability of error less than ε using our approach . We give a task allocation scheme and an inference algorithm with runtime which is linear in the total number of queries ( up to a logarithmic factor ) . Conversely , we also show that using the best adaptive task allocation scheme together with the best inference algorithm , and under the worst - case worker distribution , this scaling of the budget in terms of q and ε is unavoidable . No algorithm can achieve error less than ε with number of queries smaller than ( C / q ) log ( 1 / ε ) with some positive universal constant C . This establishes that our algorithm is worst - case optimal up to a constant factor in the required budget . Our main results show that our non - adaptive algorithm is worst - case optimal and there is no signiﬁcant gain in using an adaptive strategy . We attribute this limit of adaptation to the fact that , in existing platforms such as Amazon’s Mechanical Turk , the workers are ﬂeeting and the system does not allow for exploiting good workers . Therefore , a positive message of this result is that a good rating system for workers is essential to truly beneﬁt from crowdsourcing platforms using adaptivity . Another novel contribution of our work is the analysis technique . The iterative inference al - gorithm we introduce operates on real - valued messages whose distribution is a priori diﬃcult to analyze . To overcome this challenge , we develop a novel technique of establishing that these mes - sages are sub - Gaussian and compute the parameters recursively in a closed form . This allows us to prove the sharp result on the error rate . This technique could be of independent interest in analyzing a more general class of message - passing algorithms . 2 Main result To achieve a certain reliability in our answers with minimum number of queries , we propose using random regular graphs for task allocation and introduce a novel iterative algorithm to infer the correct answers . While our approach is non - adaptive , we show that it is suﬃcient to achieve an order - optimal performance when compared to the best possible approach using adaptive task allocations . Precisely , we prove an upper bound on the resulting error when using our approach and a matching lower bound on the minimax error rate achieved by the best possible adaptive task allocation together with an optimal inference algorithm . This shows that our approach is minimax optimal up to a constant factor : it requires only a constant factor times the minimum necessary budget to achieve a target error rate under the worst - case worker distribution . We then present the intuitions behind our inference algorithm through connections to low - rank matrix approximations and belief propagation . 2 . 1 Algorithm Task allocation . We use a non - adaptive scheme which makes all the task assignments before any worker arrives . This amounts to designing a bipartite graph with one type of nodes corresponding to each of the tasks and another set of nodes corresponding to each of the batches . An edge ( i , j ) indicates that task i is included in batch T j . Once all T j ’s are determined according to the graph , these batches are submitted simultaneously to the crowdsourcing platform . Each arriving worker 8 will pick up one of the batches and complete all the tasks in that batch . We denote by j the worker working on j - th batch T j . To design a bipartite graph , the taskmaster ﬁrst makes a choice of how many workers to assign to each task and how many tasks to assign to each worker . The task degree (cid:96) is typically determined by how much resources ( e . g . money , time , etc . ) one can spend on the tasks . The worker degree r is typically determined by how many tasks are manageable for a worker depending on the application . The total number of workers that we need is automatically determined as n = m(cid:96) / r , since the total number of edges has to be consistent . We will show that with such a regular graph , you can achieve probability of error which is quite close to a lower bound on what any inference algorithm can achieve with any task assignment . In particular , this includes all possible graphs which might have irregular degrees or have very large worker degrees ( and small number of workers ) conditioned on the total number of edges being the same . This suggests that , among other things , there is no signiﬁcant gain in using an irregular graph . We assume that the total cost that must be paid is proportional to the total number of edges and not the number of workers . If we have more budget we can increase (cid:96) . It is then natural to expect the probability of error to decrease , since we are collecting more responses . We will show that the error rate decreases exponentially in (cid:96) as (cid:96) grows . However , increasing r does not incur increase in the cost and it is not immediately clear how it aﬀects the performance . We will show that with larger r we can learn more about the workers and the error rate decreases as r increases . However , how much we can gain by increasing the worker degree is limited . Given the task and worker degrees , there are multiple ways to generate a regular bipartite graph . We want to choose a graph that will minimize the probability of error . Deviating slightly from regular degrees , we propose using a simple random construction known as conﬁguration model in random graph literature [ RU08 , Bol01 ] . We start with [ m ] × [ (cid:96) ] half - edges for task nodes and [ n ] × [ r ] half - edges for the worker nodes , and pair all the half - edges according to a random permutation of [ m(cid:96) ] . The resulting graph might have multi - edges where two nodes are connected by more than one edges . However , they are very few in thus generated random graph as long as (cid:96) (cid:28) n , whence we also have r (cid:28) m . Precisely , the number of double - edges in the graph converges in distribution to Poisson distribution with mean ( (cid:96) − 1 ) ( r − 1 ) / 2 [ Bol01 , Page 59 Exercise 2 . 12 ] . The only property that we need for the main result to hold is that the resulting random graph converges locally to a random tree in probability in the large system limit . This enables us to analyze the performance of our inference algorithm and provide sharp bounds on the probability of error . The intuition behind why random graphs are good for our inference problem is related to the spectral gap of random matrices . In the following , we will use the ( approximate ) top singular vector of a weighted adjacency matrix of the random graph to ﬁnd the correct labels . Since , sparse random graphs are excellent expanders with large spectral gaps , this enables us to reliably separate the low - rank structure from the data matrix which is perturbed by random noise . Inference algorithm . We are given a task allocation graph G (cid:0) [ m ] ∪ [ n ] , E (cid:1) where we connect an edge ( i , j ) if a task i is assigned to a worker j . In the following , we will use indexes i for a i - th task node and j for a j - th worker node . We use ∂i to denote the neighborhood of node i . Each edge ( i , j ) on the graph G has a corresponding worker response A ij . To ﬁnd the correct labels from the given responses of the workers , we introduce a novel iterative algorithm . This algorithm is inspired by the celebrated belief propagation algorithm and low - rank 9 matrix approximations . The connections are explained in detail in Section 2 . 5 and 2 . 6 , along with mathematical justiﬁcations . The algorithm operates on real - valued task messages { x i → j } ( i , j ) ∈ E and worker messages { y j → i } ( i , j ) ∈ E . A task message x i → j represents the log - likelihood of task i being a positive task , and a worker mes - sage y j → i represents how reliable worker j is . We start with the worker messages initialized as independent Gaussian random variables , although the algorithm is not sensitive to a speciﬁc ini - tialization as long as it has a strictly positive mean . We could also initialize all the messages to one , but then we need to add extra steps in the analysis to ensure that this is not a degenerate case . At k - th iteration , the messages are updated according to x ( k ) i → j = (cid:88) j (cid:48) ∈ ∂i \ j A ij (cid:48) y ( k − 1 ) j (cid:48) → i , for all ( i , j ) ∈ E , and ( 1 ) y ( k ) j → i = (cid:88) i (cid:48) ∈ ∂j \ i A i (cid:48) j x ( k ) i (cid:48) → j , for all ( i , j ) ∈ E , ( 2 ) where ∂i is the neighborhood of the task node i and ∂j is the neighborhood of the worker node j . At task update , we are giving more weight to the answers that came from more trustworthy workers . At worker update , we increase our conﬁdence in that worker if the answers she gave on another task , A i (cid:48) j , has the same sign as what we believe , x i (cid:48) → j . Intuitively , a worker message represents our belief on how ‘reliable’ the worker is . Hence , our ﬁnal estimate is a weighted sum of the answers weighted by each worker’s reliability : ˆ t ( k ) i = sign (cid:16) (cid:88) j ∈ ∂i A ij y ( k − 1 ) j → i (cid:17) . Iterative Algorithm Input : E , { A ij } ( i , j ) ∈ E , k max Output : Estimate ˆ t ∈ { ± 1 } m 1 : For all ( i , j ) ∈ E do Initialize y ( 0 ) j → i with random Z ij ∼ N ( 1 , 1 ) ; 2 : For k = 1 , . . . , k max do For all ( i , j ) ∈ E do x ( k ) i → j ← (cid:80) j (cid:48) ∈ ∂i \ j A ij (cid:48) y ( k − 1 ) j (cid:48) → i ; For all ( i , j ) ∈ E do y ( k ) j → i ← (cid:80) i (cid:48) ∈ ∂j \ i A i (cid:48) j x ( k ) i (cid:48) → j ; 3 : For all i ∈ [ m ] do x i ← (cid:80) j ∈ ∂i A ij y ( k max − 1 ) j → i ; 4 : Output estimate vector ˆ t ( k ) = { sign ( x i ) } . While our algorithm is inspired by the standard Belief Propagation ( BP ) algorithm for approx - imating max - marginals [ Pea88 , YFW03 ] , our algorithm is original and overcomes a few limitations of the standard BP for this inference problem under the crowdsourcing model . First , the iterative algorithm does not require any knowledge of the prior distribution of p j , whereas the standard BP requires it as explained in detail in Section 2 . 6 . Second , the iterative algorithm is provably order - optimal for this crowdsourcing problem . We use a standard technique , known as density evolution , to analyze the performance of our message - passing algorithm . Although we can write down the density evolution equations for the standard BP for crowdsourcing , it is not trivial to describe or 10 compute the densities , analytically or numerically . It is also very simple to write down the density evolution equations ( cf . ( 13 ) and ( 14 ) ) for our algorithm , but it is not a priori clear how one can analyze the densities in this case either . We develop a novel technique to analyze the densities for our iterative algorithm and prove optimality . This technique could be of independent interest to analyzing a broader class of message - passing algorithms . 2 . 2 Performance guarantee and experimental results We provide an upper bound on the probability of error achieved by the iterative inference algorithm and task allocation according to the conﬁguration model . The bound decays as e − C(cid:96)q with a universal constant C . Further , an algorithm - independent lower bound that we establish suggests that such a dependence of the error on (cid:96)q is unavoidable . 2 . 2 . 1 Bound on the average error probability To lighten the notation , let ˆ (cid:96) ≡ (cid:96) − 1 and ˆ r ≡ r − 1 , and recall that q = E [ ( 2 p j − 1 ) 2 ] . Using these notations , we deﬁne σ 2 k to be the eﬀective variance in the sub - Gaussian tail of our estimates after k iterations of our inference algorithm : σ 2 k ≡ 2 q µ 2 ( q 2 ˆ (cid:96) ˆ r ) k − 1 + (cid:16) 3 + 1 q ˆ r (cid:17) 1 − ( 1 / q 2 ˆ (cid:96) ˆ r ) k − 1 1 − ( 1 / q 2 ˆ (cid:96) ˆ r ) . With this , we can prove the following upper bound on the probability of error when we run k iterations of our inference algorithm with ( (cid:96) , r ) - regular assignments on m tasks using a crowd with collective quality q . We refer to Section 3 . 1 for the proof . Theorem 2 . 1 . For ﬁxed (cid:96) > 1 and r > 1 , assume that m tasks are assigned to n = m(cid:96) / r workers according to a random ( (cid:96) , r ) - regular graph drawn from the conﬁguration model . If the distribution of the worker reliability satisﬁes µ ≡ E [ 2 p j − 1 ] > 0 and q 2 > 1 / ( ˆ (cid:96) ˆ r ) , then for any t ∈ { ± 1 } m , the estimate after k iterations of the iterative algorithm achieves 1 m m (cid:88) i = 1 P (cid:0) t i (cid:54) = ˆ t ( k ) i (cid:1) ≤ e − (cid:96)q / ( 2 σ 2 k ) + 3 (cid:96)r m ( ˆ (cid:96) ˆ r ) 2 k − 2 . ( 3 ) The second term , which is the probability that the resulting graph is not locally tree - like , vanishes for large m . Hence , the dominant term in the error bound is the ﬁrst term . Further , when q 2 ˆ (cid:96) ˆ r > 1 as per our assumption and when we run our algorithm for large enough number of iterations , σ 2 k converges linearly to a ﬁnite limit σ 2 ∞ ≡ lim k →∞ σ 2 k such that σ 2 ∞ = (cid:16) 3 + 1 q ˆ r (cid:17) q 2 ˆ (cid:96) ˆ r q 2 ˆ (cid:96) ˆ r − 1 . With linear convergence of σ 2 k , we only need a small number of iterations to achieve σ k close to this limit . It follows that for large enough m and k , we can prove an upper bound that does not dependent on the problem size or the number of iterations , which is stated in the following corollary . 11 Corollary 2 . 2 . Under the hypotheses of Theorem 2 . 1 , there exists m 0 = 3 (cid:96)re (cid:96)q / 4 σ 2 ∞ ( ˆ (cid:96) ˆ r ) 2 ( k − 1 ) and k 0 = 1 + (cid:0) log ( q / µ 2 ) / log ( ˆ (cid:96) ˆ rq 2 ) (cid:1) such that 1 m m (cid:88) i = 1 P (cid:0) t i (cid:54) = ˆ t ( k ) i (cid:1) ≤ 2 e − (cid:96)q / ( 4 σ 2 ∞ ) , ( 4 ) for all m ≥ m 0 and k ≥ k 0 . Proof . For ˆ (cid:96) ˆ rq 2 > 1 as per our assumption , k = 1 + log ( q / µ 2 ) / log ( ˆ (cid:96) ˆ rq 2 ) iterations suﬃce to ensure that σ 2 k ≤ ( 2 q / µ 2 ) ( ˆ (cid:96) ˆ rq 2 ) − k + 1 + q ˆ (cid:96) ( 3 q ˆ r + 1 ) / ( q 2 ˆ (cid:96) ˆ r − 1 ) ≤ 2 σ 2 ∞ . Also , m = 3 (cid:96)re (cid:96)q / 4 σ 2 ∞ ( ˆ (cid:96) ˆ r ) 2 ( k − 1 ) suﬃces to ensure that ( ˆ (cid:96) ˆ r ) 2 k − 2 ( 3 (cid:96)r ) / m ≤ exp { − (cid:96)q / ( 4 σ 2 ∞ ) } . (cid:3) The required number of iterations k 0 is small ( only logarithmic in (cid:96) , r , q , and µ ) and does not depend on the problem size m . On the other hand , the required number of tasks m 0 in our main theorem is quite large . However , numerical simulations suggest that the actual performance of our approach is not very sensitive to the number of tasks and the bound still holds for tasks of small size as well . For example , in Figure 1 ( left ) , we ran numerical experiment with m = 1000 , q = 0 . 3 , and k = 20 , and the resulting error exhibits exponential decay as predicted by our theorem even for large (cid:96) = r = 30 . In this case , theoretical requirement on the number of tasks m 0 is much larger than what we used in the experiment . Consider a set of worker distributions { F | E F [ ( 2 p − 1 ) 2 ] = q } that have the same collective quality q . These distributions that have the same value of q can give diﬀerent values for µ ranging from q to q 1 / 2 . Our main result on the error rate suggests that the error does not depend on the value of µ . Hence , the eﬀective second moment q is the right measure of the collective quality of the crowd , and the eﬀective ﬁrst moment µ only aﬀects how fast the algorithm converges , since we need to run our inference algorithm k = Ω ( 1 + log ( q / µ 2 ) / log ( ˆ (cid:96) ˆ rq 2 ) ) iterations to guarantee the error bound . The iterative algorithm is eﬃcient with run - time comparable to that of majority voting which requires O ( m(cid:96) ) operations . Each iteration of the iterative algorithm requires O ( m(cid:96) ) operations , and we need O ( 1 + log ( q / µ 2 ) / log ( q 2 ˆ (cid:96) ˆ r ) ) iterations to ensure an error bound in ( 4 ) . By deﬁnition , we have q ≤ µ ≤ √ q . The run - time is the worst when µ = q , which happens under the spammer - hammer model , and it is the smallest when µ = √ q which happens if p j = ( 1 + √ q ) / 2 deterministically . In any case , we only need extra logarithmic factor that does not increase with compared to majority voting , and this Notice that as we increase the number of iterations , the messages converge to an eigenvector of a particular sparse matrix of dimensions m(cid:96) × m(cid:96) . This suggests that we can alternatively compute the messages using other algorithms for computing the top singular vector of large sparse matrices that are known to converge faster ( e . g . Lanczos algorithm [ Lan50 ] ) . Next , we make a few remarks on the performance guarantee . First , the assumption that µ > 0 is necessary . If there is no assumption on µ , then we cannot distinguish if the responses came from tasks with { t i } i ∈ [ m ] and workers with { p j } j ∈ [ n ] or tasks with { − t i } i ∈ [ m ] and workers with { 1 − p j } j ∈ [ n ] . Statistically , both of them give the same output . The hypothesis on µ allows us to distinguish which of the two is the correct solution . In the case when we know that µ < 0 , we can use the same algorithm changing the sign of the ﬁnal output and get the same performance guarantee . Second , our algorithm does not require any information on the distribution of p j . However , in order to generate a graph that achieves an optimal performance , we need the knowledge of q for 12 selecting the degree (cid:96) = Θ ( 1 / q log ( 1 / ε ) ) . Here is a simple way to overcome this limitation at the loss of only additional constant factor , i . e . scaling of cost per task still remains Θ ( 1 / q log ( 1 / ε ) ) . To that end , consider an incremental design in which at step a the system is designed assuming q = 2 − a for a ≥ 1 . At step a , we design two replicas of the task allocation for q = 2 − a . Now compare the estimates obtained by these two replicas for all m tasks . If they agree amongst m ( 1 − 2 ε ) tasks , then we stop and declare that as the ﬁnal answer . Otherwise , we increase a to a + 1 and repeat . Note that by our optimality result , it follows that if 2 − a is less than the actual q then the iteration must stop with high probability . Therefore , the total cost paid is Θ ( 1 / q log ( 1 / ε ) ) with high probability . Thus , even lack of knowledge of q does not aﬀect the order optimality of our algorithm . Further , unlike previous approaches based on Expectation Maximization ( EM ) , the iterative algorithm is not sensitive to initialization and converges to a unique solution from a random initial - ization with high probability . This follows from the fact that the algorithm is essentially computing a leading eigenvector of a particular linear operator . Finally , we observe a phase transition at ˆ (cid:96) ˆ rq 2 = 1 . Above this phase transition , when ˆ (cid:96) ˆ rq 2 > 1 , we will show that our algorithm is order - optimal and the probability of error is signiﬁcantly smaller than majority voting . However , perhaps surprisingly , when we are below the threshold , when ˆ (cid:96) ˆ rq 2 < 1 , we empirically observe that our algorithm exhibit a fundamentally diﬀerent behavior ( cf . Figure 1 ) . The error we get after k iterations of our algorithm increases with k . In this regime , we are better oﬀ stopping the algorithm after 1 iteration , in which case the estimate we get is essentially the same as the simple majority voting , and we cannot do better than majority voting . This phase transition is universal and we observe similar behavior with other inference algorithms including EM approaches . We provide more discussions on the choice of (cid:96) and the limitations of having small r in the following section . 2 . 2 . 2 Minimax optimality of our approach For a task master , the natural core optimization problem of her concern is how to achieve a certain reliability in the answers with minimum cost . Throughout this paper , we assume that the cost is proportional to the total number of queries . In this section , we show that if a taskmaster wants to achieve a target error rate of ε , she can do so using our approach with budget per task scaling as O ( ( 1 / q ) log ( 1 / ε ) ) for a broad range of worker degree r . Compared to the necessary condition which we provide in Section 2 . 3 , this is within a constant factor from what is necessary using the best non - adaptive task assignment and the best inference algorithm . Further , we show in Section 2 . 4 that this scaling in the budget is still necessary if we allow using the best adaptive task assignment together with the best inference algorithm . This proves that our approach is minimax optimal up to a constant factor in the budget . Assuming for now that there is no restrictions on the worker degree r and we can assign as many tasks to each worker as we want , we can get the following simpliﬁed upper bound on the error that holds for all r ≥ 1 + 1 / q . To simplify the resulting bound , let us assume for now that ˆ (cid:96) ˆ rq ≥ 2 . Then , we get that σ 2 ∞ ≤ 2 ( 3 + 1 / ˆ rq ) . Then from ( 4 ) , we get the following bound : 1 m (cid:88) i ∈ [ m ] P ( t i (cid:54) = ˆ t ( k ) i ) ≤ 2 e − (cid:96)q / 32 , for large enough m ≥ m 0 . In terms of the budget or the number of queries necessary to achieve a target accuracy , we get the following suﬃcient condition as a corollary . 13 Corollary 2 . 3 . Using the non - adaptive task assignment scheme with r ≥ 1 + 1 / q and the iterative inference algorithm introduced in Section 2 . 1 , it is suﬃcient to query ( 32 / q ) log ( 2 / ε ) times per task to guarantee that the probability of error is at most ε for any ε ≤ 1 / 2 and for all m ≥ m 0 . We provide a matching minimax necessary condition up to a constant factor for non - adaptive algorithms in Section 2 . 3 . When the nature can choose the worst - case worker distributions , no non - adaptive algorithm can achieve error less than ε with budget per task smaller than ( C (cid:48) / q ) log ( 1 / 2 ε ) with some universal positive constant C (cid:48) . This establishes that under the non - adaptive scenario , our approach is minimax optimal up to a constant factor for large enough m . With our approach you only need to ask ( and pay for ) a constant factor more than what is necessary using the best non - adaptive task assignment scheme together with the best inference algorithm under the worst - case worker distribution . Perhaps surprisingly , we will show in Section 2 . 4 that the necessary condition does not change even if we allow adaptive task assignments . No algorithm , adaptive or non - adaptive , can achieve error less than ε without asking ( C (cid:48)(cid:48) / q ) log ( 1 / 2 ε ) queries per task with some universal positive constant C (cid:48)(cid:48) . Hence , our non - adaptive approach achieves minimax optimal performance that can be achieved by the best adaptive scheme . In practice , we might not be allowed to have large r depending on the application . For diﬀerent regimes of the restrictions on the allowed worker degree r , we need diﬀerent choices of (cid:96) . When we have a target accuracy ε , the following corollary establishes that we can achieve probability of error ε with (cid:96) ≥ C ( 1 + 1 / ˆ rq ) ( 1 / q ) log ( 1 / ε ) for any value of r . Corollary 2 . 4 . Using the non - adaptive task assignment scheme with any r and the iterative in - ference algorithm introduced in Section 2 . 1 , it is suﬃcient to query ( 24 + 8 / ˆ rq ) ( 1 / q ) log ( 2 / ε ) times per task to guarantee that the probability of error is at most ε for any ε ≤ 1 / 2 and for all m ≥ m 0 . Proof . We will show that for (cid:96) ≥ max { 1 + 2 / ( ˆ rq 2 ) , 8 ( 3 + 1 / ˆ rq ) ( 1 / q ) log ( 1 / ε ) } , the probability of error is at most ε . Since , 1 + 2 / ( ˆ rq 2 ) ≤ 8 ( 3 + 1 / ˆ rq ) ( 1 / q ) log ( 1 / ε ) for ε ≤ 1 / 2 , this proves the corollary . Since ˆ (cid:96) ˆ rq 2 ≥ 2 from the ﬁrst condition , we get that σ 2 ∞ ≤ 2 ( 3 + 1 / ˆ rq ) . Then , the probability of error is upper bounded by 2 exp { − (cid:96)q / ( 24 + 8 / ˆ rq ) } . This implies that for (cid:96) ≥ ( 24 + 8 / ˆ rq ) ( 1 / q ) log ( 2 / ε ) the probability of error is at most ε . (cid:3) For r ≥ C (cid:48) / q , this implies that our approach requires O ( ( 1 / q ) log ( 1 / ε ) ) queries and it is minimax optimal . However , for r = O ( 1 ) , our approach requires O ( ( 1 / q 2 ) log ( 1 / ε ) ) queries . This is due to the fact that when r is small , we cannot eﬃciently learn the quality of the workers and need signiﬁcantly more questions to achieve the accuracy we desire . Hence , in practice , we want to be able to assign more tasks to each worker when we have low - quality workers . 2 . 2 . 3 Experimental results Figure . 1 shows the comparisons between probabilities of error achieved by diﬀerent inference algorithms , but on the same task assignment using regular bipartite random graphs . We ran 20 iterations of EM and our iterative algorithm , and also the spectral approach of using leading left singular vector of A for estimation . The spectral approach , which we call Singular Vector in the graph , is explained in detail in Section 2 . 5 . The error rates are compared with those of majority voting and the oracle estimator . The oracle estimator performance sets a lower bound on what any inference algorithm can achieve , since it knows all the values of p j ’s . For the numerical simulation 14 1e - 05 0 . 0001 0 . 001 0 . 01 0 . 1 1 5 10 15 20 25 30 P r ob a b ilit y o f e rr o r Number of queries per task Majority Voting Expectation MaximizationSingular Vector Iterative Algorithm Oracle Estimator 1e - 06 1e - 05 0 . 0001 0 . 001 0 . 01 0 . 1 1 0 0 . 1 0 . 2 0 . 3 0 . 4 P r ob a b ilit y o f e rr o r Collective quality of the crowd = q Majority Voting Expectation MaximizationSingular Vector Iterative Algorithm Oracle Estimator Figure 1 : The iterative algorithm improves over majority voting and EM algorithm . Using the top singular vector for inference has similar performance as our iterative approach . on the left - hand side , we set m = 1000 , (cid:96) = r and used the spammer hammer model for the distribution of the workers with q = 0 . 3 . According to our theorem , we expect a phase transition at (cid:96) = 1 + 1 / 0 . 3 = 4 . 3333 . From the ﬁgure , we observe that the iterative inference algorithm starts to perform better than majority voting at (cid:96) = 5 . For the ﬁgure on the right - hand side , we set (cid:96) = 25 . For fair comparisons with the EM approach , we used an implementation of the EM approach in Java by Sheng et al . [ SPI08 ] , which is publicly available . We also ran two experiments with real crowd using Amazon Mechanical Turk . In our experi - ments , we created tasks for comparing colors ; we showed three colors on each task , one on the top and two on the bottom . We asked the crowd to indicate “if the color on the top is more similar to the color on the left or on the right” . The ﬁrst experiment conﬁrms that the ground truth for these color comparisons tasks are what is expected from pairwise distances in the Lab color space . The distances in the Lab color space between the a pair of colors are known to be a good measure of the perceived distance between the pair [ WS67 ] . To check the validity of this Lab distance we collected 210 responses on each of the 10 color comparison tasks . As shown in Figure . 2 , for all 10 tasks , the majority of the 210 responses were consistent with the Lab distance based ground truth . Next , to test our approach , we created 50 of such similarity tasks and recruited 28 workers to answer all the questions . Once we have this data , we can subsample the data to simulate what would have happened if we collected smaller number of responses per task . The resulting average probability of error is illustrated in Figure . 3 . For this crowd from Amazon Mechanical Turk , we can estimate the collective quality from the data , which is about q (cid:39) 0 . 175 . Theoretically , this indicates that phase transition should happen when ( (cid:96) − 1 ) ( ( 50 / 28 ) (cid:96) − 1 ) q 2 = 1 , since we set r = ( 50 / 28 ) (cid:96) . With this , we expect phase transition to happen around (cid:96) (cid:39) 5 . In Figure . 3 , we see that our iterative algorithm starts to perform better than majority voting around (cid:96) = 8 . 2 . 3 Fundamental limit under the non - adaptive scenario Under the non - adaptive scenario , we are allowed to use only non - adaptive task assignment schemes which assign all the tasks a priori and collect all the responses simultaneously . In this section , we 15 151 59 123 87 141 69 126 84 109 101 121 89 141 69 141 69 149 61 159 51 Figure 2 : Experimental results on color comparison using real data from Amazon’s Mechanical Turk . The color on the left is closer to the one on the top in Lab distance for each triplet . The votes from 210 workers are shown below each triplet . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 4 8 12 16 20 24 28 P r ob a b ilit y o f e rr o r Number of queries per task Majority Voting Expectation Maximization Iterative Algorithm Figure 3 : The average probability of error on color comparisons using real data from Amazon’s Mechanical Turk . 16 investigate the fundamental limit on how small an error can be achieved using the best possible non - adaptive task assignment scheme together with the best possible inference algorithm . In particular , we are interested in the minimax optimality : What is the minimum error that can be achieved under the worst - case worker distribution ? To this end , we analyze the performance of an oracle estimator when the workers’ latent qualities are drawn from a speciﬁc distribution and provide a lower bound on the minimax rate on the probability of error . Compared to our main result , this establishes that our approach is minimax optimal up to a constant factor . In terms of the budget , the natural core optimization problem of our concern is how to achieve a certain reliability in our answers with minimum cost . Let us assume that the cost is proportional to the total number of queries . We show that for a given target error rate ε , the total budget suﬃcient to achieve this target error rate using our algorithm is within a constant factor from what is necessary using the best non - adaptive task assignment and the best inference algorithm . Fundamental limit . Consider a crowd characterized by worker distribution F such that p j ∼ F . Let F q be a set of all distributions on [ 0 , 1 ] , such that the collective quality is parametrized by q : F q = (cid:8) F | E F [ ( 2 p j − 1 ) 2 ] = q (cid:9) . We want to prove a lower bound on the minimax rate on the probability of error , which only depends on q and (cid:96) . Deﬁne the minimax rate as min τ ∈T (cid:96) , ˆ t max t ∈ { ± 1 } m , F∈F q 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) , where ˆ t ranges over all estimators which are measurable functions over the responses , and τ ranges over the set T (cid:96) of all task assignment schemes which are non - adaptive and ask m(cid:96) queries in total . Here the probability is taken over all realizations of p j ’s , A ij ’s , and the randomness introduced in the task assignment and the inference . Consider any non - adaptive scheme that assigns (cid:96) i workers to the i - th task . The only constraint is that the average number of queries is bounded by ( 1 / m ) (cid:80) i ∈ [ m ] (cid:96) i ≤ (cid:96) . To get a lower bound on the minimum achievable error , we consider an oracle estimator that has access to all the p j ’s , and hence can make an optimal estimation . Further , since we are proving minimax optimality and not instance - optimality , the worst - case error rate will always be lower bounded by the error rate for any choice of worker distribution . In particular , we prove a lower bound using the spammer - hammer model . Concretely , we assume the p j ’s are drawn from the spammer - hammer model with perfect hammers : p j = (cid:26) 1 / 2 with probability 1 − q , 1 otherwise . Notice that the use of q is consistent with E [ ( 2 p j − 1 ) 2 ] = q . Under the spammer - hammer model , the oracle estimator only makes a mistake on task i if it is only assigned to spammers , in which case we ﬂip a fair coin to achieve error probability of half . Formally , P ( ˆ t i (cid:54) = t i ) = 1 2 ( 1 − q ) (cid:96) i . By convexity and using Jensen’s inequality , the average probability of error is lower bounded by 1 m (cid:88) i ∈ [ m ] P ( ˆ t i (cid:54) = t i ) ≥ 1 2 ( 1 − q ) (cid:96) . 17 Since we are interested in how many more queries are necessary as the quality of the crowd dete - riorates , we are going to assume q ≤ 2 / 3 , in which case ( 1 − q ) ≥ e − ( q + q 2 ) . As long as total m(cid:96) queries are used , this lower bound holds regardless of how the actual tasks are assigned . And since this lower bound holds for a particular choice of F , it holds for the worst case F as well . Hence , for the best task assignment scheme and the best inference algorithm , we have min τ ∈T (cid:96) , ˆ t max t ∈ { ± 1 } m , F∈F q 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≥ 1 2 e − ( q + q 2 ) (cid:96) . This lower bound on the minimax rate holds for any positive integer m , and regardless of the number of workers or the number of queries , r , assigned to each worker . In terms of the average number of queries necessary to achieve a target accuracy of ε , this implies the following necessary condition . Lemma 2 . 5 . Assuming q ≤ 2 / 3 and the non - adaptive scenario , if the average number of queries per task is less than ( 1 / 2 q ) log ( 1 / 2 ε ) , then no algorithm can achieve average probability of error less than ε for any m under the worst - case worker distribution . To prove this worst - cased bound , we analyzed a speciﬁc distribution of the spammer - hammer model . However , the result ( up to a constant factor ) seems to be quite general and can also be proved using diﬀerent distributions , e . g . when all workers have the same quality . The assumption on q can be relaxed as much as we want , by increasing the constant in the necessary budget . Compared to the suﬃcient condition in Corollary 2 . 3 this establishes that our approach is minimax optimal up to a constant factor . With our approach you only need to ask ( and pay for ) a constant factor more than what is necessary for any algorithm . Majority voting . As a comparison , we can do similar analysis for the simple majority voting and show that the performance is signiﬁcantly worse than our approach . The next lemma provides a bound on the minimax rate of majority voting . A proof of this lemma is provided in Section 3 . 4 . Lemma 2 . 6 . For any C < 1 , there exists a positive constant C (cid:48) such that when q ≤ C , the error achieved by majority voting is at least min τ ∈T (cid:96) max t ∈ { ± 1 } m , F∈F q 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≥ e − C (cid:48) ( (cid:96)q 2 + 1 ) . In terms of the number of queries necessary to achieve a target accuracy ε using majority voting , this implies that we need to ask at least ( c / q 2 ) log ( c (cid:48) / ε ) queries per task for some universal constants c and c (cid:48) . Hence , majority voting is signiﬁcantly more costly than our approach in terms of budget . Our algorithm is more eﬃcient in terms of computational complexity as well . Simple majority voting requires O (cid:0) ( m / q 2 ) log ( 1 / ε ) (cid:1) operations to achieve target error rate ε in the worst case . From Corollary 2 . 2 , together with (cid:96) = O ( ( 1 / q ) log ( 1 / ε ) ) and (cid:96)rq 2 = Ω ( 1 ) , we get that our approach requires O ( ( m / q ) log ( 1 / q ) log ( 1 / ε ) ) operations in the worst case . 2 . 4 Fundamental limit under the adaptive scenario In terms of the scaling of the budget necessary to achieve a target accuracy , we established that using a non - adaptive task assignment , no algorithm can do better than our approach . One might 18 prefer a non - adaptive scheme in practice because having all the batches of tasks processed in parallel reduces the latency . This is crucial in many applications , especially in real - time applications such as searching , visual information processing , and document processing [ BJJ + 10 , BLM + 10 , YKG10 , BBMK11 ] . However , by switching to an adaptive task assignment , one might hope to be more eﬃcient and still obtain a desired accuracy from fewer questions . On one hand , adaptation can help improve performance . But on the other hand , it can signiﬁcantly complicate system design due to careful synchronization requirements . In this section , we want to prove an algorithm - independent upper bound on how much one can gain by using an adaptive task allocation . When the identities of the workers are known , one might be tempted to ﬁrst identify which workers are more reliable and then assign all the tasks to those workers in an explore / exploit manner . However , in typical crowdsourcing platforms such as Amazon Mechanical Turk , it is unrealistic to assume that we can identify and reuse any particular worker , since typical workers are neither persistent nor identiﬁable and batches are distributed through an open - call . Hence , exploiting a reliable worker is not possible . However , we can adaptively resubmit batches of tasks ; we can dynamically choose which subset of tasks to assign to the next arriving worker . In particular , we can allocate tasks to the next batch based on all the information we have on all the tasks from the responses collected thus far . For example , one might hope to reduce uncertainty more eﬃciently by adaptively collecting more responses on those tasks that she is less certain about . Fundamental limit . In this section , we show that , perhaps surprisingly , there is no signiﬁcant gain in switching from our non - adaptive approach to an adaptive strategy when the workers are ﬂeeting . We ﬁrst prove a lower bound on the minimax error rate : the error that is achieved by the best inference algorithm ˆ t using the best adaptive task allocation scheme τ under a worst - case worker distribution F and the worst - case true answers t . Let (cid:101) T (cid:96) be the set of all task assignment schemes that use at most m(cid:96) queries in total . Then , we can show the following lower bound on the minimax rate on the probability of error . A proof of this theorem is provided in Section 3 . 5 . Theorem 2 . 7 . When q ≤ C for any constant C < 1 , there exists a positive constant C (cid:48) such that min τ ∈ (cid:101) T (cid:96) , ˆ t max t ∈ { ± 1 } m , F∈F q 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≥ 1 2 e − C (cid:48) (cid:96)q , ( 5 ) for all m where the task assignment scheme τ ranges over all adaptive schemes that use at most m(cid:96) queries and ˆ t ranges over all estimators that are measurable functions over the responses . We cannot avoid the factor of half in the lower bound , since we can always achieve error prob - ability of half without asking any queries ( with (cid:96) = 0 ) . In terms of the budget required to achieve a target accuracy , the above lower bound proves that no algorithm , adaptive or non - adaptive , can achieve an error rate less than ε with number of queries per task less than ( C (cid:48) / q ) log ( 2 / ε ) in the worst case of worker distribution . Corollary 2 . 8 . Assuming q ≤ C for any constant C < 1 and the iterative scenario , there exists a positive constant C (cid:48) such that if the average number of queries is less than ( C (cid:48) / q ) log ( 1 / 2 ε ) , then no algorithm can achieve average probability of error less than ε for any m under the worst - case worker distribution . Compared to Corollary 2 . 3 , we have a matching suﬃcient and necessary conditions up to a constant factor . This proves that there is no signiﬁcant gain in using an adaptive scheme , and our 19 approach achieves minimax - optimality up to a constant factor with a non - adaptive scheme . This limitation of adaptation strongly relies on the fact that workers are ﬂeeting in existing platforms and can not be reused . Therefore , architecturally our results suggest that building a reliable reputation system for workers would be essential to harnessing the potential of adaptive designs . A counter example for instance - optimality . The above corollary establishes minimax - optimality : for the worst - case worker distribution , no algorithm can improve over our approach other than improving the constant factor in the necessary budget . However , this does not imply instance - optimality . In fact , there exists a family of worker distributions where all non - adaptive algorithms fail to achieve order - optimal performance whereas a trivial adaptive algorithm succeeds . Hence , for particular instances of worker distributions , there exists a gap between what can be achieved using non - adaptive algorithms and adaptive ones . We will prove this in the case of the spammer - hammer model where each new worker is a hammer ( p j = 1 ) with probability q or a spammer ( p j = 1 / 2 ) otherwise . We showed in Section 2 . 3 that no non - adaptive algorithm can achieve an error less than ( 1 / 2 ) e − C (cid:48) (cid:96)q for any value of m . In particular , this does not vanish even if we increase m . We will introduce a simple adaptive algorithm and show that this algorithm achieves an error probability that goes to zero as m grows . The algorithm ﬁrst groups all the tasks into √ m disjoint sets of size √ m each . Starting with the ﬁrst group , the algorithm assigns all √ m tasks to new arriving workers until it sees two workers who agreed on all √ m tasks . It declares those responses as its estimate for this group and moves on to the next group . This process is repeated until it reaches the allowed number of queries . This estimator makes a mistake on a group if ( a ) there were two spammers who agreed on all √ m tasks or ( b ) we run out of allowed number of queries before we ﬁnish the last group . Formally , we can prove the following upper bound on the probability of error . Lemma 2 . 9 . Under the spammer - hammer model , when the allowed number of queries per task (cid:96) is larger than 2 / q , there is an adaptive task allocation scheme and an inference algorithm that achieves average probability of error at most m(cid:96) 2 2 −√ m + e − ( 2 / (cid:96) ) ( (cid:96)q − 2 ) 2 √ m . Proof . Recall that we are only allowed (cid:96)m queries . Since we are allocating √ m queries per worker , we can only ask at most (cid:96) √ m workers . First , the probability that there is at least one pair of spammers ( among all possible pairs from (cid:96) √ m workers ) who agreed an all √ m responses is at most m(cid:96) 2 2 −√ m . Next , given that no pairs of spammers agreed on all their responses , the probability that we run out of all m(cid:96) allowed queries is the probability that the number of hammers in (cid:96) √ m workers is strictly less than 2 √ m ( which is the number of hammers we need in order to terminate the algorithm , conditioned on that no spammers agree with one another ) . By standard concentration results , this happens with probability at most e − ( 2 / (cid:96) ) ( (cid:96)q − 2 ) 2 √ m . (cid:3) This proves the existence of an adaptive algorithm which achieves vanishing error probability as m grows for a board range of task degree (cid:96) . Comparing the above upper bound with the known lower bound for non - adaptive schemes , this proves that non - adaptive algorithms cannot be instance optimal : there is a family of distributions where adaptation can signiﬁcantly improve performance . This is generally true when there is a strictly positive probability that a worker is a hammer ( p j = 1 ) . One might be tempted to apply the above algorithm in more general settings other than the spammer - hammer model . However , this algorithm fails when there are no perfect workers in the 20 crowd . If we apply this algorithm in such a general setting , then it produces useless answers : the probability of error approaches half as m grows for any ﬁnite (cid:96) . 2 . 5 Connections to low - rank matrix approximation In this section , we ﬁrst explain why the top singular vector of the data matrix A reveals the true answers of the tasks , where A is the m × n matrix of the responses and we ﬁll in zeros wherever we have no responses collected . This naturally deﬁnes a spectral algorithm for inference which we present next . It was proven in [ KOS11 ] that the error achieved by this spectral algorithm is upper bounded by C / ( (cid:96)q ) with some constant C . But numerical experiments ( cf . Figure 1 ) suggest that the error decays much faster , and that the gap is due to the weakness of the analysis used in [ KOS11 ] . Inspired by this spectral approach , we introduced a novel inference algorithm that performs as well as the spectral algorithm ( cf . Figure 1 ) and proved a much tighter upper bound on the resulting error which scales as e − C (cid:48) (cid:96)q with some constant C (cid:48) . Our inference algorithm is based on power iteration , which is a well - known algorithm for computing the top singular vector of a matrix , and Figure 1 suggests that both algorithms are equally eﬀective and the resulting errors are almost identical . The data matrix A can be viewed as a rank - 1 matrix that is perturbed by random noise . Since , E [ A ij | t i , p j ] = ( r / m ) t i ( 2 p j − 1 ) , the conditional expectation of this matrix is E (cid:2) A | t , p (cid:3) = (cid:16) r m (cid:17) t ( 2 p − 1 ) T , where 1 is the all ones vector , the vector of correct solutions is t = { t i } i ∈ [ m ] and the vector of worker reliability is p = { p j } j ∈ [ n ] . Notice that the rank of this conditional expectation matrix is one and this matrix reveals the correct solutions exactly . We can decompose A into a low - rank expectation plus a random perturbation : A = (cid:16) r m (cid:17) t ( 2 p − 1 ) T + Z , where Z ≡ A − E (cid:2) A | t , p (cid:3) is the random perturbation with zero mean . When the spectral radius of the noise matrix Z is much smaller than the spectral radius of the signal , we can correctly extract most of t using the leading left singular vector of A . Under the crowdsourcing model considered in this paper , an inference algorithm using the top left singular vector of A was introduced and analyzed by Karger et al . [ KOS11 ] . Let u be the top left singular vector of A . They proposed estimating ˆ t i = sign ( u i ) and proved an upper bound on the probability of error that scales as O ( 1 / (cid:96)q ) . The main technique behind this result is in analyzing the spectral gap of A . It is not diﬃcult to see that the spectral radius of the conditional expectation matrix is ( r / m ) (cid:107) t ( 2 p − 1 ) T (cid:107) 2 = √ (cid:96)rq , where the operator norm of a matrix is denoted by (cid:107) X (cid:107) 2 ≡ max a (cid:107) Xa (cid:107) / (cid:107) a (cid:107) . Karger et al . proved that the spectral radius of the perturbation (cid:107) Z (cid:107) 2 is in the order of ( (cid:96)r ) 1 / 4 . Hence , when (cid:96)rq 2 (cid:29) 1 , we expect a separation between the conditional expectation and the noise . One way to compute the leading singular vector is to use power iteration : for two vectors u ∈ R m and v ∈ R n , starting with a randomly initialized v , power iteration iteratively updates u and v by repeating u = Av and v = A T u . It is known that normalized u ( and v ) converges linearly to the leading left ( and right ) singular vector . Then we can use the sign of u i to estimate t i . Writing the 21 update rule for each entry , we get u i = (cid:88) j ∈ ∂i A ij v j , v j = (cid:88) i ∈ ∂j A ij u i . Notice that this power iteration update rule is almost identical to those of message passing updates in ( 1 ) and ( 2 ) . The (cid:96) task messages { x i → j } j ∈ ∂i from task i are close in value to the entry u i of the power iteration . The r worker messages { y j → i } i ∈ ∂j from worker j are close in value to the entry v j of the power iteration . Numerical simulations in Figure 1 suggest that the quality of the estimates from the two algorithms are almost identical . However , the known performance guarantee for the spectral approach is weak . We developed novel analysis techniques to analyze our message passing algorithm , and provide an upper bound on the error that scales as e − C(cid:96)q . It might be possible to apply our algorithm , together with the analysis techniques , to other problems where the top singular vector of a data matrix is used for inference . 2 . 6 Connections to belief propagation The crowdsourcing model described in this paper can naturally be described using a graphical model . Let G ( [ m ] × [ n ] , E , A ) denote the weighted bipartite graph , where [ m ] is the set of m task nodes , [ n ] is the set of n worker nodes , E is the set of edges connecting a task to a worker who is assigned that task , and A is the set of weights on those edges according to the responses . Given such a graph , we want to ﬁnd a set of task answers that maximize the following posterior distribution F ( ˆ t , p ) : { ± 1 } m × [ 0 , 1 ] n → R + . max ˆ t , p (cid:89) a ∈ [ n ] F ( p a ) (cid:89) ( i , a ) ∈ E (cid:110) p a I ( ˆ t i = A ia ) + ( 1 − p a ) I ( ˆ t i (cid:54) = A ia ) (cid:111) , where with a slight abuse of notation we use F ( · ) to denote the prior probability density function of p a ’s and we use i and j to denote task nodes and a and b to denote worker nodes . For such a problem of ﬁnding the most probable realization in a graphical model , the celebrated belief propagation ( BP ) gives a good approximate solution . To be precise , BP is an approximation for maximizing the marginal distribution of each variable , and a similar algorithm known as min - sum algorithm approximates the most probable realization . However , the two algorithms are closely related , and in this section we only present standard BP . There is a long line of literature providing the theoretical and empirical evidences supporting the use BP [ Pea88 , YFW03 ] . Under the crowdsourcing graphical model , standard BP operates on two sets of messages : the task messages { (cid:101) x i → a } ( i , a ) ∈ E and the worker messages { (cid:101) y a → i } ( i , a ) ∈ E . In our iterative algorithm the messages were scalar variables with real values , whereas the messages in BP are probability density functions . Each task message corresponds to an edge and each worker message also corresponds to an edge . The task node i corresponds to random variable ˆ t i , and the task message from task i to worker a , denoted by (cid:101) x i → a , represents our belief on the random variable ˆ t i . Then (cid:101) x i → a is a probability distribution over { ± 1 } . Similarly , a worker node a corresponds to a random variable p a . The worker message (cid:101) y a → i is a probability distribution of p a over [ 0 , 1 ] . Following the standard BP framework , we iteratively update the messages according to the following rule . We start with 22 randomly initialized (cid:101) x i → a ’s and at k - th iteration , (cid:101) y ( k ) a → i ( p a ) ∝ F ( p a ) (cid:89) j ∈ ∂a \ i (cid:110) ( p a + ¯ p a + ( p a − ¯ p a ) A ja ) (cid:101) x ( k ) j → a ( + 1 ) + ( p a + ¯ p a − ( p a − ¯ p a ) A ja ) (cid:101) x ( k ) j → a ( − 1 ) (cid:111) , (cid:101) x ( k + 1 ) i → a ( ˆ t i ) ∝ (cid:89) b ∈ ∂i \ a (cid:90) (cid:16)(cid:101) y ( k ) b → i ( p b ) (cid:0) p b I ( A ib = ˆ t i ) + ¯ p b I ( A ib (cid:54) = ˆ t i ) (cid:1)(cid:17) dp b , for all ( i , a ) ∈ E and for ¯ p = 1 − p . The above update rule only determines the messages up to a scaling , where ∝ indicates that the left - hand side is proportional to the right - hand side . The algorithm produces the same estimates in the end regardless of the scaling . After a predeﬁned number of iterations , we make a decision by computing the decision variable (cid:101) x i ( ˆ t i ) ∝ (cid:89) b ∈ ∂i (cid:90) (cid:16)(cid:101) y ( k ) b → i ( p b ) (cid:0) p b I ( A ib = ˆ t i ) + ¯ p b I ( A ib (cid:54) = ˆ t i ) (cid:1)(cid:17) dp b , and estimating ˆ t i = sign (cid:0)(cid:101) x i ( + ) − (cid:101) x i ( − ) (cid:1) . In a special case of a Haldane prior , where a worker either always tells the truth or always gives the wrong answer , p j = (cid:26) 0 with probability 1 / 2 , 1 otherwise , the above BP updates boils down to our iterative inference algorithm . Let x i → a = log (cid:0)(cid:101) x i → a ( + ) / (cid:101) x i → a ( − ) (cid:1) denote the log - likelihood of (cid:101) x i → a ( · ) . Under Haldane prior , p a is also a binary random variable . We can use y a → i = log (cid:0)(cid:101) y a → i ( 1 ) / (cid:101) y a → i ( 0 ) (cid:1) to denote the log - likelihood of (cid:101) y a → i ( · ) . After some simpliﬁ - cations , the above BP update boils down to y ( k ) a → i = (cid:88) j ∈ ∂a \ i A ja x ( k − 1 ) j → a , x ( k ) i → a = (cid:88) b ∈ ∂i \ a A ib y ( k ) b → i . This is exactly the same update rule as our iterative inference algorithm ( cf . Eqs . ( 1 ) ad ( 2 ) ) . Thus , our algorithms is belief propagation for a very speciﬁc prior . Despite this , it is surprising that it performs near optimally ( with random regular graph for task allocation ) for all priors . This robustness property is due to the models assumed in this crowdsourcing problem and is not to be expected in general . 2 . 7 Discussion In this section , we discuss several implications of our main results and possible future research directions in generalizing the model studied in this paper . Below phase transition . We ﬁrst discuss the performance guarantees in the below threshold regime when ˆ (cid:96) ˆ rq 2 < 1 . As we will show , the bound in ( 4 ) always holds even when ˆ (cid:96) ˆ rq 2 ≤ 1 . However , numerical experiments suggest that we should stop our algorithm at ﬁrst iteration when we are below the phase transition as discussed in Section 2 . 2 . We provide an upper bound on 23 the resulting error when only one iteration of our iterative inference algorithm is used ( which is equivalent as majority voting algorithm ) . Notice that the bound in ( 4 ) is only meaningful when it is less than a half . When ˆ (cid:96) ˆ rq 2 ≤ 1 or (cid:96)q < 24 log 2 , the right - hand side of inequality ( 4 ) is always larger than half . Hence the upper bound always holds , even without the assumption that ˆ (cid:96) ˆ rq 2 > 1 , and we only have that assumption in the statement of our main theorem to emphasize the phase transition in how our algorithm behaves . However , we can also try to get a tighter bound than a trivial half implied from ( 4 ) in the below threshold regime . Speciﬁcally , we empirically observe that the error rate increases as the number of iterations k increases . Therefore , it makes sense to use k = 1 . In which case , the algorithm essentially boils down to the majority rule . We can prove the following error bound which generally holds for any regime of (cid:96) , r and the worker distribution F . A proof of this statement is provided in Section 3 . 6 . Lemma 2 . 10 . For any value of (cid:96) , r , and m , and any distribution of workers F , the estimates we get after ﬁrst step of our algorithm achieve 1 m m (cid:88) i = 1 P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≤ e − (cid:96)µ 2 / 4 , ( 6 ) where µ = E F [ 2 p j − 1 ] . Since µ is always between q and q 1 / 2 , the scaling of the above error exponent is always worse than what we have after running our algorithm for a long time ( cf . Theorem 2 . 1 ) . This suggests that iterating our inference algorithm helps when ˆ (cid:96) ˆ rq 2 > 1 and especially when the gap between µ and q is large . Under these conditions , our approach does signiﬁcantly better than majority voting ( cf . Figure 1 ) . The gain of using our approach is maximized when there exists both good workers and bad workers . This is consistent with our intuition that when there is a variety of workers , our algorithm can identify the good ones and get better estimates . Golden standard units . Next , consider the variation where we ask questions to workers whose answers are already known ( also known as ‘gold standard units’ ) . We can use these to assess the quality of the workers . There are two ways we can use this information . First , we can embed ‘seed gold units’ along with the standard tasks , and use these ‘seed gold units’ in turn to perform more informed inference . However , we can show that there is no gain in using such ‘seed gold units’ . The optimal lower bound of 1 / q log ( 1 / ε ) essentially utilizes the existence of oracle that can identify the reliability of every worker exactly , i . e . the oracle has a lot more information than what can be gained by such embedded golden questions . Therefore , clearly ‘seed gold units’ do not help the oracle estimator , and hence the order optimality of our approach still holds even if we include all the strategies that can utilize these ‘seed gold units’ . However , in practice , it is common to use the ‘seed gold units’ , and this can improve the constant factor in the required budget , but not the scaling . Alternatively , we can use ‘pilot gold units’ as qualifying or pilot questions that the workers must complete to qualify to participate . Typically a taskmaster do not have to pay for these qualifying questions and this provides an eﬀective way to increase the quality of the participating workers . Our approach can beneﬁt from such ‘pilot gold units’ , which has the eﬀect of increasing the eﬀective collective quality of the crowd q . Further , if we can ‘measure’ how the distribution of workers change 24 when using pilot questions , then our main result fully describes how much we can gain by such pilot questions . In any case , pilot questions only change the distribution of participating workers , and the order - optimality of our approach still holds even if we compare all the schemes that use the same pilot questions . How to optimize over a multiple choices of crowds . We next consider the scenario where we have a choice over which crowdsourcing platform to use from a set of platforms with diﬀerent crowds . Each crowd might have diﬀerent worker distributions with diﬀerent prices . Speciﬁcally , suppose there are K crowds of workers : the k - th crowd has collective quality q k and requires payment of c k to perform a task . Now our optimality result suggests that the per - task cost scales as c k / q k log ( 1 / ε ) if we only used workers of class k . More generally , if we use a mix of these workers , say α k fraction of workers from class k , with (cid:80) k α k = 1 , then the eﬀective parameter q = (cid:80) k α k q k . And subject to this , the optimal per task cost scales as ( (cid:80) k α k c k ) / ( (cid:80) k α k q k ) log ( 1 / ε ) . This immediately suggests that the optimal choice of fraction α k must be such that α k > 0 only if c k / q k = min i c i / q i . That is , the optimal choice is to select workers only from the classes that have maximal quality per cost ratio of q k / c k over k ∈ [ K ] . One implication of this observation is that it suggests a pricing scheme for crowdsourcing platforms . If you are managing a crowdsourcing platform with the collective quality q and the cost c and there is another crowdsourcing platform with q (cid:48) and c (cid:48) , you want to choose the cost such that the quality per cost ratio is at least as good as the other crowd : q / c ≥ q (cid:48) / c (cid:48) . General crowdsourcing models . Finally , we consider possible generalizations of our model . The model assumed in this paper does not capture several factors : tasks with diﬀerent level of diﬃculties or workers who always answer positive or negative . In general , the responses of a worker j to a binary question i may depend on several factors : ( i ) the correct answer to the task ; ( ii ) the diﬃculty of the task ; ( iii ) the expertise or the reliability of the worker ; ( iv ) the bias of the worker towards positive or negative answers . Let t i ∈ { + 1 , − 1 } represent the correct answer and r i ∈ [ 0 , ∞ ) represents the level of diﬃculty . also , let α j ∈ [ −∞ , ∞ ] represent the reliability and β j ∈ ( −∞ , ∞ ) represent the bias of worker j . In formula , a worker j ’s response to a binary task i can be modeled as A ij = sign ( Z i , j ) , where Z i , j is a Gaussian random variable distributed as Z i , j ∼ N ( α j t i + β j , r i ) and sign ( Z ) = 1 almost surely for Z ∼ N ( ∞ , 1 ) . A task with r i = 0 is an easy task and large r i is a diﬃcult task . A worker with large positive α j is more likely to give the right answer and large negative α j is more likely to give the wrong answer . When α j = 0 , the worker gives independent answers regardless of what the correct answer is . A worker with large β j is biased towards positive responses and if β j = 0 then the worker is unbiased . A similar model with multi - dimensional latent variables was studied in [ WBBP10 ] . Most of the models studied in the crowdsourcing literature can be reduced to a special case of this model . For example , the early patient - classiﬁcation model introduced by Dawid and Skene [ DS79 ] is equivalent to the above Gaussian model with r i = 1 . Each worker is represented by two latent quality parameters p + j and p − j , such that A ij = (cid:26) t i with probability p t i j , − t i otherwise . 25 This model captures the bias of workers . More recently , Whitehill et al . [ WRW + 09 ] introduced another model where P ( A ij = t i | a i , b j ) = 1 / ( 1 + e − a i b j ) , with worker reliability a i and task diﬃculty b j . This is again a special case of the above Gaussian model if we set β j = 0 . The model we study in this paper has an underlying assumption that all the tasks share an equal level of diﬃculty and the workers are unbiased . It is equivalent to the above Gaussian model with β j = 0 and r i = 1 . In this case , there is a one - to - one relation between the worker reliability p j and α j : p j = Q ( α j ) , where Q ( · ) is the tail probability of the standard Gaussian distribution . 3 Proof of main results In this section , we provide proofs of the main results . 3 . 1 Proof of the main result in Theorem 2 . 1 By symmetry , we can assume that all t i ’s are + 1 . Let ˆ t ( k ) i denote the resulting estimate of task i after k iterations of our iterative inference algorithm deﬁned in Section 2 . 1 . If we draw a random task I uniformly in [ m ] , then we want to compute the average error probability , which is the probability that we make an error on this randomly chosen task : 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t ( k ) i (cid:1) = P (cid:0) t I (cid:54) = ˆ t ( k ) I (cid:1) . ( 7 ) We will prove an upper bound on the probability of error in two steps . First , we prove that the local neighborhood of a randomly chosen task node I is a tree with high probability . Then , assuming that the graph is locally tree - like , we provide an upper bound on the error using a technique known as density evolution . We construct a random bipartite graph G ( [ m ] ∪ [ n ] , E ) according to the conﬁguration model . We start with [ m ] × [ (cid:96) ] half - edges for task nodes and [ n ] × [ r ] half - edges for the worker nodes , and pair all the m(cid:96) task half - edges to the same number of worker half - edges according to a random permutation of [ m(cid:96) ] . Let G i , k denote a subgraph of G ( [ m ] ∪ [ n ] , E ) that includes all the nodes whose distance from the ‘root’ i is at most k . At ﬁrst iteration of our inference algorithm , to estimate the task i , we only use the responses provided by the workers who were assigned to task i . Hence we are performing inference on the local neighborhood G i , 1 . Similarly , when we run k iterations of our ( message - passing ) inference algorithm to estimate a task i , we only run inference on local subgraph G i , 2 k − 1 . Since we update both task and worker messages , we need to grow the subgraph by distance two at each iteration . When this local subgraph is a tree , then we can apply density evolution to analyze the probability of error . When this local subgraph is not a tree , we can make a pessimistic assumption that an error has been made to get an upper bound on the actual error probability . P (cid:0) t I (cid:54) = ˆ t ( k ) I (cid:1) ≤ P (cid:0) G I , 2 k − 1 is not a tree (cid:1) + P (cid:0) G I , 2 k − 1 is a tree and t I (cid:54) = ˆ t ( k ) I (cid:1) . ( 8 ) Next lemma bounds the ﬁrst term and shows that the probability that a local subgraph is not a tree vanishes as m grows . A proof of this lemma is provided in Section 3 . 2 . 26 Lemma 3 . 1 . For a random ( (cid:96) , r ) - regular bipartite graph generated according to the conﬁguration model , P (cid:0) G I , 2 k − 1 is not a tree (cid:1) ≤ (cid:0) ( (cid:96) − 1 ) ( r − 1 ) (cid:1) 2 k − 2 3 (cid:96)r m . ( 9 ) Then , to bound the second term of ( 8 ) , we provide a sharp upper bound on the error probability conditioned on that G I , 2 k − 1 is a tree . Let x ( k ) i denote the decision variable for task i after k iterations of the iterative algorithm such that ˆ t ( k ) i = sign ( x ( k ) i ) . Then , we make an error whenever this decision variable is negative . When this is exactly zero , we make a random decision , in which case we make an error with probability half . Then , P (cid:0) t I (cid:54) = ˆ t ( k ) I (cid:12)(cid:12) G I , k is a tree (cid:1) ≤ P (cid:0) x ( k ) I ≤ 0 (cid:12)(cid:12) G I , k is a tree (cid:1) . ( 10 ) To analyze the distribution of the decision variable on a locally tree - like graph , we use a standard probabilistic analysis technique known as ‘density evolution’ in coding theory or ‘recursive distri - butional equations’ in probabilistic combinatorics [ RU08 , MM09 ] . Precisely , we use the following equality that P (cid:0) x ( k ) I ≤ 0 (cid:12)(cid:12) G I , k is a tree (cid:1) = P (cid:0) ˆ x ( k ) ≤ 0 (cid:1) , ( 11 ) where ˆ x ( k ) is deﬁned through density evolution equations ( 13 ) , ( 14 ) and ( 15 ) in the following . We will prove in the following that when ˆ (cid:96) ˆ rq 2 > 1 , P (cid:0) ˆ x ( k ) ≤ 0 (cid:1) ≤ e − (cid:96)q / ( 2 σ 2 k ) . ( 12 ) Together with equations ( 11 ) , ( 10 ) , ( 9 ) , ( 8 ) , and ( 7 ) , this ﬁnishes the proof of Theorem 2 . 1 . Density evolution . At iteration k the algorithm operates on a set of messages { x ( k ) i → j } ( i , j ) ∈ E and { y ( k ) j → i } ( i , j ) ∈ E . If we chose an edge ( i , j ) uniformly at random , the values of x and y messages on that randomly chosen edge deﬁne random variables whose randomness comes from random choice of the edge , any randomness introduced by the inference algorithm , the graph , and the realizations of p j ’s and A ij ’s . Let x ( k ) denote this random variable corresponding to the message x ( k ) i → j and y ( k ) p denote the random variable corresponding to y ( k ) j → i conditioned on the latent worker quality being p for randomly chosen edge ( i , j ) . As proved in Lemma 3 . 1 , the ( (cid:96) , r ) - regular random graph locally converges in distribution to a ( (cid:96) , r ) - regular tree with high probability . On a tree , there is a recursive way of deﬁning the distribution of messages x ( k ) and y ( k ) p . At initialization , we initialize the worker messages with Gaussian random variable with mean one and variance one . The corresponding random variable y ( 0 ) p ∼ N ( 1 , 1 ) , which at initial step is independent of the worker quality p , fully describes the distribution of y ( 0 ) j → i for all ( i , j ) . At ﬁrst iteration , the task messages are updated according to x ( 1 ) i → j = (cid:80) j (cid:48) ∈ ∂i \ j A ij (cid:48) y ( 0 ) j (cid:48) → i . If we know the distribution of A ij (cid:48) ’s and y ( 0 ) j (cid:48) → i ’s , we can update the distribution of x ( 1 ) i → j . Since we are assuming a tree , all x ( 1 ) i → j are independent . Further , because of the symmetry in the way we construct our random graph , all x ( 1 ) i → j ’s are identically distributed . Precisely , they are distributed according to x ( 1 ) deﬁned in ( 13 ) . This recursively deﬁnes x ( k ) and y ( k ) through the density evolution equations in ( 13 ) and ( 14 ) [ MM09 ] . 27 Let us ﬁrst introduce a few deﬁnitions ﬁrst . Here and after , we drop the superscript k denoting the iteration number whenever it is clear from the context . Let x b ’s and y p , a ’s be independent ran - dom variables distributed according to x and y p respectively . Also , z p , a ’s and z p , b ’s are independent random variables distributed according to z p , where z p = (cid:26) + 1 with probability p , − 1 with probability 1 − p . This represents the answer given by a worker conditioned on the worker having quality parameter p . Let p ∼ F be a random variable distributed according to the distribution of the worker’s quality F over [ 0 , 1 ] . Then p a ’s are independent random variable distributed according to p . Further , z p , b ’s and x b ’s are independent , and z p a , a ’s and y p a , a ’s are conditionally independent conditioned on p a . We initialize y p with a Gaussian distribution , whence it is independent of the latent variable p : y ( 0 ) p ∼ N ( 1 , 1 ) . Let d = denote equality in distribution . Then , for k ∈ { 1 , 2 , . . . } , the task messages are distributed as the sum of (cid:96) − 1 incoming messages that are independent and identically distributed according to y ( k − 1 ) p and weighted by i . i . d . responses : x ( k ) d = (cid:88) a ∈ [ (cid:96) − 1 ] z p a , a y ( k − 1 ) p a , a . ( 13 ) Similarly , the worker messages ( conditioned on the latent worker quality p ) are distributed as the sum of r − 1 incoming messages that are independent and identically distributed according to x ( k ) and weighted by i . i . d . responses : y ( k ) p d = (cid:88) b ∈ [ r − 1 ] z p , b x ( k ) b . ( 14 ) For the decision variable x ( k ) I on a randomly chosen task I , we have ˆ x ( k ) d = (cid:88) i ∈ [ (cid:96) ] z p i , i y ( k − 1 ) p i , i . ( 15 ) Numerically or analytically computing the densities in ( 13 ) and ( 14 ) exactly is not computation - ally feasible when the messages take continuous values as is the case for our algorithm . Typically , heuristics are used to approximate the densities such as quantizing the messages , approximating the density with simple functions , or using Monte Carlo method to sample from the density . A novel contribution of our analysis is that we prove that the messages are sub - Gaussian using recursion , and we provide an upper bound on the parameters in a closed form . This allows us to prove the sharp result on the error bound that decays exponentially . Mean and variance computation . To give an intuition on how the messages behave , we describe the evolution of the mean and the variance of the random variables in ( 13 ) and ( 14 ) . Let p be a random variable distributed according to the measure F . Deﬁne m ( k ) ≡ E [ x ( k ) ] , ˆ m ( k ) p ≡ E [ y ( k ) p | p ] , v ( k ) ≡ Var ( x ( k ) ) , and ˆ v ( k ) p ≡ Var ( y ( k ) p | p ) . Also let ˆ (cid:96) = (cid:96) − 1 and ˆ r = r − 1 to 28 simplify notation . Then , from ( 13 ) and ( 14 ) we get that m ( k ) = ˆ (cid:96) E p (cid:2) ( 2 p − 1 ) ˆ m ( k − 1 ) p (cid:3) , ˆ m ( k ) p = ˆ r ( 2 p − 1 ) m ( k ) , v ( k ) = ˆ (cid:96) (cid:26) E p [ ˆ v ( k − 1 ) p + (cid:0) ˆ m ( k − 1 ) p (cid:1) 2 ] − (cid:16) E p (cid:2) ( 2 p − 1 ) ˆ m ( k − 1 ) p (cid:3)(cid:17) 2 (cid:27) , ˆ v ( k ) p = ˆ r (cid:110) v ( k ) + ( m ( k ) ) 2 − (cid:0) ( 2 p − 1 ) m ( k ) (cid:1) 2 (cid:111) . Recall that µ = E [ 2 p − 1 ] and q = E [ ( 2 p − 1 ) 2 ] . Substituting ˆ m p and ˆ v p we get the following evolution of the ﬁrst and the second moment of the random variable x ( k ) . m ( k + 1 ) = ˆ (cid:96) ˆ rqm ( k ) , v ( k + 1 ) = ˆ (cid:96) ˆ rv ( k ) + ˆ (cid:96) ˆ r ( m ( k ) ) 2 ( 1 − q ) ( 1 + ˆ rq ) . Since ˆ m ( 0 ) p = 1 and ˆ v ( 0 ) = 1 as per our assumption , we have m ( 1 ) = µ ˆ (cid:96) and v ( 1 ) = ˆ (cid:96) ( 4 − µ 2 ) . This implies that m ( k ) = µ ˆ (cid:96) ( ˆ (cid:96) ˆ rq ) k − 1 , and v ( k ) = av ( k − 1 ) + bc k − 2 , with a = ˆ (cid:96) ˆ r , b = µ 2 ˆ (cid:96) 3 ˆ r ( 1 − q ) ( 1 + ˆ rq ) , and c = ( ˆ (cid:96) ˆ rq ) 2 . After some algebra , it follows that v ( k ) = v ( 1 ) a k − 1 + bc k − 2 (cid:80) k − 2 (cid:96) = 0 ( a / c ) (cid:96) . For ˆ (cid:96) ˆ rq 2 > 1 , we have a / c < 1 and v ( k ) = ˆ (cid:96) ( 4 − µ 2 ) ( ˆ (cid:96) ˆ r ) k − 1 + ( 1 − q ) ( 1 + ˆ rq ) µ 2 ˆ (cid:96) 2 ( ˆ (cid:96) ˆ rq ) 2 k − 2 1 − 1 / ( ˆ (cid:96) ˆ rq 2 ) k − 1 ˆ (cid:96) ˆ rq 2 − 1 . The ﬁrst and second moment of the decision variable ˆ x ( k ) in ( 15 ) can be computed using a similar analysis : E [ ˆ x ( k ) ] = ( (cid:96) / ˆ (cid:96) ) m ( k ) and Var ( ˆ x ( k ) ) = ( (cid:96) / ˆ (cid:96) ) v ( k ) . In particular , we have Var ( ˆ x ( k ) ) E [ ˆ x ( k ) ] 2 = ˆ (cid:96) ( 4 − µ 2 ) (cid:96) ˆ (cid:96)µ 2 ( ˆ (cid:96) ˆ rq 2 ) k − 1 + ˆ (cid:96) ( 1 − q ) ( 1 + ˆ rq ) (cid:96) ( ˆ (cid:96) ˆ rq 2 − 1 ) (cid:16) 1 − 1 ( ˆ (cid:96) ˆ rq 2 ) k − 1 (cid:17) . Applying Chebyshev’s inequality , it immediately follows that P ( ˆ x ( k ) < 0 ) is bounded by the right - hand side of the above equality . This bound is weak compared to the bound in Theorem 2 . 1 . In the following , we prove a stronger result using the sub - Gaussianity of x ( k ) . But ﬁrst , let us analyze what this weaker bound gives for diﬀerent regimes of (cid:96) , r , and q , which indicates that the messages exhibit a fundamentally diﬀerent behavior in the regimes separated by a phase transition at ˆ (cid:96) ˆ rq 2 = 1 . In a ‘good’ regime where we have ˆ (cid:96) ˆ rq 2 > 1 , the bound converges to a ﬁnite limit as the number of iterations k grows . Namely , lim k →∞ P ( ˆ x ( k ) < 0 ) ≤ ˆ (cid:96) ( 1 − q ) ( 1 + ˆ rq ) (cid:96) ( ˆ (cid:96) ˆ rq 2 − 1 ) . Notice that the upper bound converges to ( 1 − q ) / ( (cid:96)q ) as ˆ (cid:96) ˆ rq 2 grows . This scales in the same way as the known bounds for using the left singular vector directly for inference ( cf . [ KOS11 ] ) . In the case when ˆ (cid:96) ˆ rq 2 < 1 , the same analysis gives Var ( ˆ x ( k ) ) E [ ˆ x ( k ) ] 2 = e Θ ( k ) . 29 Finally , when ˆ (cid:96) ˆ rq 2 = 1 , we get v ( k ) = ( ˆ (cid:96) ˆ r ) k + ˆ (cid:96) ˆ r ( 1 − q ) ( 1 + ˆ rq ) ( ˆ (cid:96) ˆ rq ) 2 k − 2 k , which implies Var ( ˆ x ( k ) ) E [ ˆ x ( k ) ] 2 = Θ ( k ) . Analyzing the density . Our strategy to provide a tight upper bound on P ( ˆ x ( k ) ≤ 0 ) is to show that ˆ x ( k ) is sub - Gaussian with appropriate parameters and use the Chernoﬀ bound . A random variable z with mean m is said to be sub - Gaussian with parameter ˜ σ if for all λ ∈ R the following inequality holds : E [ e λ z ] ≤ e mλ + ( 1 / 2 ) ˜ σ 2 λ 2 . Deﬁne ˜ σ 2 k ≡ 2ˆ (cid:96) ( ˆ (cid:96) ˆ r ) k − 1 + µ 2 ˆ (cid:96) 3 ˆ r ( 3 q ˆ r + 1 ) ( q ˆ (cid:96) ˆ r ) 2 k − 4 1 − ( 1 / q 2 ˆ (cid:96) ˆ r ) k − 1 1 − ( 1 / q 2 ˆ (cid:96) ˆ r ) , and m k ≡ µ ˆ (cid:96) ( q ˆ (cid:96) ˆ r ) k − 1 for k ∈ Z . We will ﬁrst show that , x ( k ) is sub - Gaussian with mean m k and parameter ˜ σ 2 k for a regime of λ we are interested in . Precisely , we will show that for | λ | ≤ 1 / ( 2 m k − 1 ˆ r ) , E [ e λ x ( k ) ] ≤ e m k λ + ( 1 / 2 ) ˜ σ 2 k λ 2 . ( 16 ) By deﬁnition , due to distributional independence , we have E [ e λ ˆ x ( k ) ] = E [ e λ x ( k ) ] ( (cid:96) / ˆ (cid:96) ) . Therefore , it follows from ( 16 ) that ˆ x ( k ) satisﬁes E [ e λ ˆ x ( k ) ] ≤ e ( (cid:96) / ˆ (cid:96) ) m k λ + ( (cid:96) / 2ˆ (cid:96) ) ˜ σ 2 k λ 2 . Applying the Chernoﬀ bound with λ = − m k / ( ˜ σ 2 k ) , we get P (cid:0) ˆ x ( k ) ≤ 0 (cid:1) ≤ E (cid:2) e λ ˆ x ( k ) (cid:3) ≤ e − (cid:96)m 2 k / ( 2 ˆ (cid:96) ˜ σ 2 k ) , ( 17 ) Since m k m k − 1 / ( ˜ σ 2 k ) ≤ µ 2 ˆ (cid:96) 2 ( q ˆ (cid:96) ˆ r ) 2 k − 3 / ( 3 µ 2 q ˆ (cid:96) 3 ˆ r 2 ( q ˆ (cid:96) ˆ r ) 2 k − 4 ) = 1 / ( 3ˆ r ) , it is easy to check that | λ | ≤ 1 / ( 2 m k − 1 ˆ r ) . This implies the desired bound in ( 12 ) . Now we are left to prove that x ( k ) is sub - Gaussian with appropriate parameters . We can write down a recursive formula for the evolution of the moment generating functions of x and y p as E (cid:2) e λ x ( k ) (cid:3) = (cid:16) E p (cid:104) p E [ e λ y ( k − 1 ) p | p ] + ¯p E [ e − λ y ( k − 1 ) p | p ] (cid:105)(cid:17) ˆ (cid:96) , ( 18 ) E (cid:2) e λ y ( k ) p (cid:3) = (cid:16) p E (cid:2) e λ x ( k ) (cid:3) + ¯ p E (cid:2) e − λ x ( k ) (cid:3)(cid:17) ˆ r , ( 19 ) where ¯ p = 1 − p and ¯p = 1 − p . We can prove that these are sub - Gaussian using induction . First , for k = 1 , we show that x ( 1 ) is sub - Gaussian with mean m 1 = µ ˆ (cid:96) and parameter ˜ σ 21 = 2ˆ (cid:96) , where µ ≡ E [ 2 p − 1 ] . Since y p is initialized as Gaussian with unit mean and variance , we have E [ e λ y ( 0 ) p ] = e λ + ( 1 / 2 ) λ 2 regardless of p . Substituting this into ( 18 ) , we get for any λ , E (cid:104) e λ x ( 1 ) (cid:105) = (cid:16) E [ p ] e λ + ( 1 − E [ p ] ) e − λ (cid:17) ˆ (cid:96) e ( 1 / 2 ) ˆ (cid:96)λ 2 ≤ e ˆ (cid:96)µλ + ˆ (cid:96)λ 2 , ( 20 ) where the inequality follows from the fact that ae z + ( 1 − a ) e − z ≤ e ( 2 a − 1 ) z + ( 1 / 2 ) z 2 for any z ∈ R and a ∈ [ 0 , 1 ] ( cf . [ AS08 , Lemma A . 1 . 5 ] ) . 30 Next , assuming E [ e λ x ( k ) ] ≤ e m k λ + ( 1 / 2 ) ˜ σ 2 k λ 2 for | λ | ≤ 1 / ( 2 m k − 1 ˆ r ) , we show that E [ e λ x ( k + 1 ) ] ≤ e m k + 1 λ + ( 1 / 2 ) ˜ σ 2 k + 1 λ 2 for | λ | ≤ 1 / ( 2 m k ˆ r ) , and compute appropriate m k + 1 and ˜ σ 2 k + 1 . Substituting the bound E [ e λ x ( k ) ] ≤ e m k λ + ( 1 / 2 ) ˜ σ 2 k λ 2 in ( 19 ) , we get E [ e λ y ( k ) p ] ≤ ( pe m k λ + ¯ pe − m k λ ) ˆ r e ( 1 / 2 ) ˆ r ˜ σ 2 k λ 2 . Further applying this bound in ( 18 ) , we get E (cid:104) e λ x ( k + 1 ) (cid:105) ≤ (cid:16) E p (cid:104) p ( p e m k λ + ¯p e − m k λ ) ˆ r + ¯p ( p e − m k λ + ¯p e m k λ ) ˆ r (cid:105)(cid:17) ˆ (cid:96) e ( 1 / 2 ) ˆ (cid:96) ˆ r ˜ σ 2 k λ 2 . ( 21 ) To bound the ﬁrst term in the right - hand side , we use the next key lemma . A proof of this lemma is provided in Section 3 . 3 . Lemma 3 . 2 . For any | z | ≤ 1 / ( 2ˆ r ) and p ∈ [ 0 , 1 ] such that q = E [ ( 2 p − 1 ) 2 ] , we have E p (cid:104) p ( p e z + ¯p e − z ) ˆ r + ¯p ( ¯p e z + p e − z ) ˆ r (cid:105) ≤ e q ˆ rz + ( 1 / 2 ) ( 3 q ˆ r 2 + ˆ r ) z 2 . Applying this inequality to ( 21 ) gives E [ e λ x ( k + 1 ) ] ≤ e q ˆ (cid:96) ˆ rm k λ + ( 1 / 2 ) (cid:0) ( 3 q ˆ (cid:96) ˆ r 2 + ˆ (cid:96) ˆ r ) m 2 k + ˆ (cid:96) ˆ r ˜ σ 2 k (cid:1) λ 2 , for | λ | ≤ 1 / ( 2 m k ˆ r ) . In the regime where q ˆ (cid:96) ˆ r ≥ 1 as per our assumption , m k is non - decreasing in k . At iteration k , the above recursion holds for | λ | ≤ min { 1 / ( 2 m 1 ˆ r ) , . . . , 1 / ( 2 m k − 1 ˆ r ) } = 1 / ( 2 m k − 1 ˆ r ) . Hence , we get the following recursion for m k and ˜ σ k such that ( 16 ) holds for | λ | ≤ 1 / ( 2 m k − 1 ˆ r ) . m k + 1 = q ˆ (cid:96) ˆ rm k , ˜ σ 2 k + 1 = ( 3 q ˆ (cid:96) ˆ r 2 + ˆ (cid:96) ˆ r ) m 2 k + ˆ (cid:96) ˆ r ˜ σ 2 k . With the initialization m 1 = µ ˆ (cid:96) and ˜ σ 21 = 2ˆ (cid:96) , we have m k = µ ˆ (cid:96) ( q ˆ (cid:96) ˆ r ) k − 1 for k ∈ { 1 , 2 , . . . } and ˜ σ 2 k = a ˜ σ 2 k − 1 + bc k − 2 for k ∈ { 2 , 3 , . . . } , with a = ˆ (cid:96) ˆ r , b = µ 2 ˆ (cid:96) 2 ( 3 q ˆ (cid:96) ˆ r 2 + ˆ (cid:96) ˆ r ) , and c = ( q ˆ (cid:96) ˆ r ) 2 . After some algebra , it follows that ˜ σ 2 k = ˜ σ 21 a k − 1 + bc k − 2 (cid:80) k − 2 (cid:96) = 0 ( a / c ) (cid:96) . For ˆ (cid:96) ˆ rq 2 (cid:54) = 1 , we have a / c (cid:54) = 1 , whence ˜ σ 2 k = ˜ σ 21 a k − 1 + bc k − 2 ( 1 − ( a / c ) k − 1 ) / ( 1 − a / c ) . This ﬁnishes the proof of ( 16 ) . 3 . 2 Proof of Lemma 3 . 1 Consider the following discrete time random process that generates the random graph G I , 2 k − 1 starting from the root I . At ﬁrst step , we connect (cid:96) worker nodes to node I according to the conﬁguration model , where (cid:96) half - edges are matches to a randomly chosen subset of nr worker half - edges of size (cid:96) . Let α 1 denote the probability that the resulting graph is a tree , that is no pair of edges are connected to the same worker node . Since there are (cid:0) (cid:96) 2 (cid:1) pairs and each pair of half - edges are connected to the same worker node with probability ( r − 1 ) / ( nr − 1 ) : α 1 ≤ (cid:18) (cid:96) 2 (cid:19) r − 1 nr − 1 . Similarly , deﬁne α t ≡ P ( G I , 2 t − 1 is not a tree | G I , 2 t − 2 is a tree ) , and β t ≡ P ( G I , 2 t − 2 is not a tree | G I , 2 t − 3 is a tree ) . 31 Then , P ( G I , 2 k − 1 is not a tree ) ≤ α 1 + k (cid:88) t = 2 (cid:0) α t + β t (cid:1) . ( 22 ) We can upper bound α t ’s and β t ’s in a similar way . For generating G I , 2 t − 1 conditioned on G I , 2 t − 2 being a tree , there are (cid:96) ( ˆ (cid:96) ˆ r ) t − 1 half - edges , where ˆ (cid:96) = (cid:96) − 1 and ˆ r = r − 1 . Among (cid:0) (cid:96) ( ˆ (cid:96) ˆ r ) t − 1 2 (cid:1) pairs of these half - edges , each pair will be connected to the same worker with probability at most ( r − 1 ) / ( r ( n − (cid:80) t − 1 a = 1 (cid:96) ( ˆ (cid:96) ˆ r ) a − 1 ) − 1 ) , where (cid:80) t − 1 a = 1 (cid:96) ( ˆ (cid:96) ˆ r ) a − 1 is the total number of worker nodes that are assigned so far in G I , 2 t − 2 . Then , α t ≤ (cid:96) 2 ( ˆ (cid:96) ˆ r ) 2 t − 2 2 r − 1 r ( n − ( (cid:96) ( ( ˆ (cid:96) ˆ r ) t − 2 − 1 ) ) / ( ˆ (cid:96) ˆ r − 1 ) ) − 1 ≤ (cid:96) 2 ( ˆ (cid:96) ˆ r ) 2 t − 2 2 ( n − (cid:96) ( ˆ (cid:96) ˆ r ) t − 2 / 2 ) ≤ (cid:96) 2 ( ˆ (cid:96) ˆ r ) 2 t − 2 n + (cid:96) ( ˆ (cid:96) ˆ r ) t − 2 n ≤ 3 (cid:96) 2 ( ˆ (cid:96) ˆ r ) 2 t − 2 2 n , where the second inequality follows from the fact that ( a − 1 ) / ( b − 1 ) ≤ a / b for all a ≤ b and ˆ (cid:96) ˆ r ≥ 2 as per our assumption , and in the third inequality we used the fact that α t is upper bounded by one and the fact that for f ( x ) = b / ( x − a ) which is upper bounded by one , we have f ( x ) ≤ ( 2 b / x ) + ( 2 a / x ) . Similarly , we can show that β t ≤ 3 (cid:96) 2 ( ˆ (cid:96) ˆ r ) 2 t − 2 ˆ (cid:96) 2 m . Substituting α t and β t into ( 22 ) , we get that P ( G I , 2 k − 1 is not a tree ) ≤ ( ˆ (cid:96) ˆ r ) 2 k − 2 3 (cid:96)r m . 3 . 3 Proof of Lemma 3 . 2 By the fact that ae b + ( 1 − a ) e − b ≤ e ( 2 a − 1 ) b + ( 1 / 2 ) b 2 for any b ∈ R and a ∈ [ 0 , 1 ] , we have p e z + ¯p e − z ≤ e ( 2 p − 1 ) z + ( 1 / 2 ) z 2 almost surely . Applying this inequality once again , we get E (cid:104) p ( p e z + ¯p e − z ) ˆ r + ¯p ( ¯p e z + p e − z ) ˆ r (cid:105) ≤ E (cid:104) e ( 2 p − 1 ) 2 ˆ rz + ( 1 / 2 ) ( 2 p − 1 ) 2 ˆ r 2 z 2 (cid:105) e ( 1 / 2 ) ˆ rz 2 . Using the fact that e a ≤ 1 + a + 0 . 63 a 2 for | a | ≤ 5 / 8 , E (cid:104) e ( 2 p − 1 ) 2 ˆ rz + ( 1 / 2 ) ( 2 p − 1 ) 2 ˆ r 2 z 2 (cid:105) ≤ E (cid:104) 1 + ( 2 p − 1 ) 2 ˆ rz + ( 1 / 2 ) ( 2 p − 1 ) 2 ˆ r 2 z 2 + 0 . 63 ( ( 2 p − 1 ) 2 ˆ rz + ( 1 / 2 ) ( 2 p − 1 ) 2 ˆ r 2 z 2 ) 2 (cid:105) ≤ 1 + q ˆ rz + ( 3 / 2 ) q ˆ r 2 z 2 ≤ e q ˆ rz + ( 3 / 2 ) q ˆ r 2 z 2 , for | z | ≤ 1 / ( 2ˆ r ) . This proves Lemma 3 . 2 . 32 3 . 4 Proof of a bound on majority voting in Lemma 2 . 6 Majority voting simply follows what the majority of workers agree on . In formula , ˆ t i = sign ( (cid:80) j ∈ W i A ij ) , where W i denotes the neighborhood of node i in the graph . It makes a random choice when there is a tie . We want to compute a lower bound on P ( ˆ t i (cid:54) = t i ) . Let x i = (cid:80) j ∈ W i A ij . Assuming t i = + 1 without loss of generality , the error rate is lower bounded by P ( x i < 0 ) . After rescaling , ( 1 / 2 ) ( x i + (cid:96) ) is a standard binomial random variable Binom ( (cid:96) , α ) , where (cid:96) is the number of neighbors of the node i , α = E [ p j ] , and by assumption each A ij is one with probability α . It follows that P ( x i = − l + 2 k ) = ( ( (cid:96) ! ) / ( (cid:96) − k ) ! k ! ) α k ( 1 − α ) l − k . Further , for k ≤ α(cid:96) − 1 , the probability distribution function is monotonically increasing . Precisely , P ( x i = − (cid:96) + 2 ( k + 1 ) ) P ( x i = − (cid:96) + 2 k ) ≥ α ( (cid:96) − k ) ( 1 − α ) ( k + 1 ) ≥ α ( (cid:96) − α(cid:96) + 1 ) ( 1 − α ) α(cid:96) > 1 , where we used the fact that the above ratio is decreasing in k whence the minimum is achieved at k = α(cid:96) − 1 under our assumption . Let us assume that (cid:96) is even , so that x i take even values . When (cid:96) is odd , the same analysis works , but x i takes odd values . Our strategy is to use a simple bound : P ( x i < 0 ) ≥ k P ( x i = − 2 k ) . By assumption that α = E [ p j ] ≥ 1 / 2 , For an appropriate choice of k = √ l , the right - hand side closely approximates the error probability . By deﬁnition of x i , it follows that P (cid:16) x i = − 2 √ (cid:96) (cid:17) = (cid:18) (cid:96) (cid:96) / 2 + √ (cid:96) (cid:19) α (cid:96) / 2 −√ l (cid:0) 1 − α (cid:1) (cid:96) / 2 + √ (cid:96) . ( 23 ) Applying Stirling’s approximation , we can show that (cid:18) (cid:96) (cid:96) / 2 + √ (cid:96) (cid:19) ≥ C 2 √ (cid:96) 2 l , ( 24 ) for some positive constant C 2 . We are interested in the case where worker quality is low , that is α is close to 1 / 2 . Accordingly , for the second and third terms in ( 23 ) , we expand in terms of 2 α − 1 . log (cid:18) α (cid:96) / 2 −√ (cid:96) (cid:16) 1 − α (cid:17) (cid:96) / 2 + √ (cid:96) (cid:19) = (cid:16) (cid:96) 2 − √ (cid:96) (cid:17)(cid:16) log ( 1 + ( 2 α − 1 ) ) − log ( 2 ) (cid:17) + (cid:16) (cid:96) 2 + √ (cid:96) (cid:17)(cid:16) log ( 1 − ( 2 α − 1 ) ) − log ( 2 ) (cid:17) = − (cid:96) log ( 2 ) − (cid:96) ( 2 α − 1 ) 2 2 + O ( (cid:112) (cid:96) ( 2 α − 1 ) 4 ) . ( 25 ) We can substitute ( 24 ) and ( 25 ) in ( 23 ) to get the following bound : P ( x i < 0 ) ≥ exp (cid:8) − C 3 ( (cid:96) ( 2 α − 1 ) 2 + 1 ) (cid:9) , ( 26 ) for some positive constant C 3 . Now , let (cid:96) i denote the degree of task node i , such that (cid:80) i (cid:96) i = (cid:96)m . Then for any { t i } ∈ { ± 1 } m , any distribution of p such that µ = E [ 2 p − 1 ] = 2 α − 1 , and any non - adaptive task assignment for m tasks , the following lower bound is true . 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≥ 1 m m (cid:88) i = 1 e − C 3 ( (cid:96) i µ 2 + 1 ) ≥ e − C 3 ( (cid:96)µ 2 + 1 ) , 33 where the last inequality follows from convexity of the exponential function . Under the spammer - hammer model , where µ = q this gives min τ ∈T (cid:96) max t ∈ { ± 1 } m , F∈F q 1 m (cid:88) i ∈ [ m ] P (cid:0) t i (cid:54) = ˆ t i (cid:1) ≥ e − C 3 ( (cid:96)q 2 + 1 ) . This ﬁnishes the proof of lemma . 3 . 5 Proof of a bound on the adaptive schemes in Theorem 2 . 7 In this section , we prove that , even with the help of an oracle , the probability of error cannot decay faster than e − C(cid:96)q . We consider an labeling algorithm which has access to an oracle that knows the reliability of every worker ( all the p j ’s ) . At k - th step , after the algorithm assign T k and all the | T k | answers are collected from the k - th worker , the oracle provides the algorithm with p k . Using all the previously collected answers { A ij } j ≤ k and the worker reliability { p j } j ≤ k , the algorithm makes a decision on the next task assignment T k + 1 . This process is repeated until a stopping criterion is met , and the algorithm outputs the optimal estimate of the true labels . The algorithm can compute the maximum likelihood estimates , which is known to minimize the probability of making an error . Let W i be the set of workers assigned to task i , then ˆ t i = sign (cid:16) (cid:88) j ∈ W i log (cid:16) p j 1 − p j (cid:17) A ij (cid:17) . ( 27 ) We are going to show that there exists a family of distributions F such that for any stopping rule and any task assignment scheme , the probability of error is lower bounded by e − C(cid:96)q . We deﬁne the following family of distributions according to the spammer - hammer model with imperfect hammers . We assume that q ≤ a 2 and p j = (cid:26) 1 / 2 with probability 1 − ( q / a 2 ) , 1 / 2 ( 1 + a ) with probability q / a 2 , such that E [ ( 2 p j − 1 ) 2 ] = q . Let W i denote the set of workers assigned to task i when the algorithm has stopped . Then | W i | is a random variable representing the total number of workers assigned to task i . The oracle estimator knows all the values necessary to compute the error probability of each task . Let E i = E [ I ( t i (cid:54) = ˆ t i ) | { A ij } , { p j } ] be the random variable representing the error probability as computed by the oracle estimator , conditioned on the | W i | responses we get from the workers and their reliability p j ’s . We are interested in identifying how the average budget ( 1 / m ) (cid:80) i E (cid:2) | W i | (cid:3) depends on the achieve average error rate ( 1 / m ) (cid:80) i E [ E i ] . In the following we will show that for any task i , independent of which task allocation scheme is used , it is necessary that E (cid:2) | W i | (cid:3) ≥ 0 . 27 q log (cid:16) 1 2 E [ E i ] (cid:17) . ( 28 ) By convexity of log ( 1 / x ) and Jensen’s inequality , this implies that 1 m m (cid:88) i = 1 E (cid:2) | W i | (cid:3) ≥ 0 . 27 q log (cid:16) 1 2 ( 1 / m ) (cid:80) mi = 1 E [ E i ] (cid:17) . 34 Since the total number of queries has to be consistent , we have (cid:80) j | T j | = (cid:80) i | W i | ≤ m(cid:96) . Also , by deﬁnition E [ E i ] = P ( t i (cid:54) = ˆ t i ) . Then , from the above inequality , we get ( 1 / m ) (cid:80) i ∈ [ m ] P ( t i (cid:54) = ˆ t i ) ≥ ( 1 / 2 ) e − ( 1 / 0 . 27 ) q(cid:96) , which ﬁnishes the proof of the theorem . Note that this bound holds for any value of m . Now , we are left to prove that the inequality ( 28 ) holds . Focusing on a single task i , since we know who the spammers are and spammers give us no information about the task , we only need the responses from the reliable workers in order to make an optimal estimate as per ( 27 ) . The conditional error probability E i of the optimal estimate depends on the realizations of the answers { A ij } j ∈ W i and the worker reliability { p j } j ∈ W i . The following lower bound on the error only depends on the number of reliable workers , which we denote by (cid:96) i . Without loss of generality , let t i = + 1 . Then , if all the reliable workers provide ‘ − ’ answers , the maximum likelihood estimation would be ‘ − ’ for this task . This leads to an error . Therefore , E i ≥ P ( all (cid:96) i reliable workers answered − ) = 1 2 (cid:16) 1 − a 2 (cid:17) (cid:96) i , for all the realizations of { A ij } and { p j } . The scaling by half ensures that the above inequality holds even when (cid:96) i = 0 . By convexity and Jensen’s inequality , it follows that E (cid:2) (cid:96) i (cid:3) ≥ log (cid:0) 2 E [ E i ] (cid:1) log (cid:0) ( 1 − a ) / 2 (cid:1) . When we recruit | W i | workers , we see (cid:96) i = ( q / a 2 ) | W i | reliable ones on average . Formally , we have E [ (cid:96) i ] = ( q / a 2 ) E [ | W i | ] . Again applying Jensen’s inequality , we get E (cid:2) | W i | (cid:3) ≥ 1 q a 2 log (cid:0) ( 1 − a ) / 2 (cid:1) log (cid:0) 2 E [ E i ] (cid:1) . Maximizing over all choices of a ∈ ( 0 , 1 ) , we get E (cid:2) | W i | (cid:3) ≥ − log (cid:0) 2 E [ E i ] (cid:1) 0 . 27 q , which in particular is true with a = 0 . 8 . For this choice of a , the result holds in the regime where q ≤ 0 . 64 . Notice that by changing the constant in the bound , we can ensure that the result holds for any values of q . This ﬁnishes the proof of ( 28 ) . 3 . 6 Proof of a bound with one iteration in Lemma 2 . 10 The probability of making an error after one iteration of our algorithm for node i is P ( t i (cid:54) = ˆ t ( 1 ) i ) ≤ P ( ˆ x i ≤ 0 ) , where ˆ x i = (cid:80) j ∈ ∂i A ij y ( 1 ) j → i . Assuming t i = + , without loss of generality , A ij is + 1 with probability E [ p ] and − 1 otherwise . All y ( 1 ) j → i ’s are initialized as Gaussian random variables with mean one and variance one . All these random variables are independent of one another at this initial step . Hence , the resulting random variable ˆ x i is a sum of a shifted binomial random variable 2 ( Binom ( (cid:96) , E [ p ] ) − (cid:96) ) and a zero - mean Gaussian random variable N ( 0 , (cid:96) ) . From calculations similar to ( 20 ) , it follows that E (cid:104) e λ ˆ x ( 1 ) (cid:105) ≤ e (cid:96)µλ + (cid:96)λ 2 ≤ e − ( 1 / 4 ) (cid:96)µ 2 , where we choose λ = − µ / 2 . By Chernoﬀ’s inequality , this implies the lemma for any value of m . 35 4 Conclusion We conclude with some limitations of our results and interesting research directions . 1 . More general models . In this paper , we provided an order - optimal task assignment scheme and an order - optimal inference algorithm for that task assignment assuming a probabilistic crowd - sourcing model . In this model , we assumed that each worker makes a mistake randomly according to a worker speciﬁc quality parameter . Two main simpliﬁcations we make here is that , ﬁrst , the worker’s reliability does not depend on whether the task is a positive task or a negative task , and second , all the tasks are equally easy or diﬃcult . The main remaining challenges in developing inference algorithms for crowdsourcing is how to develop a solution for more generic models for - mally described in Section 2 . 7 . When workers exhibit bias and can have heterogeneous quality parameters depending on the correct answer to the task , spectral methods using low - rank matrix approximations nicely generalize to give an algorithmic solution . Also , it would be interesting to ﬁnd algorithmic solutions with performance guarantees for the generic model where tasks diﬃculties are taken into account . 2 . Improving the constant . We prove our approach is minimax optimal up to a constant factor . However , there might be another algorithm with better constant factor than our inference algorithm . Some modiﬁcation of the expectation maximization or the belief propagation might achieve a better constant compared to our inference algorithm . It is an interesting research direction to ﬁnd such an algorithm and give an upper bound on the error probability that is smaller than what we have in our main theorem . 3 . Instance - optimality . The optimality of our approach is proved under the worst - case worker distribution . However , it is not known whether our approach is instance - optimal or not under the non - adaptive scenario . It would be important to prove lower bounds for all worker distributions or to ﬁnd a counter example where another algorithm achieves a strictly better performance for a particular worker distribution in terms of the scaling of the required budget . 4 . Phase transition . We empirically observe that there is a phase transition around ˆ (cid:96) ˆ rq 2 = 1 . Below this , no algorithm can do better than majority voting . This phase transition seems to be an algorithm - independent and fundamental property of the problem ( and the random graph ) . It might be possible to formally prove the fundamental diﬀerence in the way information propagates under the crowdsourcing model . Such phase transition has been studied for a simpler model of broadcasting on trees in information theory and statistical mechanics [ EKPS00 ] . References [ AS08 ] N . Alon and J . H . Spencer , The probabilistic method , John Wiley , 2008 . [ BBMK11 ] M . S . Bernstein , J . Brandt , R . C . Miller , and D . R . Karger , Crowds in two seconds : enabling realtime crowd - powered interfaces , Proceedings of the 24th annual ACM sym - posium on User interface software and technology , UIST ’11 , 2011 , pp . 33 – 42 . [ BJJ + 10 ] J . P . Bigham , C . Jayant , H . Ji , G . Little , A . Miller , R . C . Miller , R . Miller , A . Tatarow - icz , B . White , S . White , and T . Yeh , Vizwiz : nearly real - time answers to visual ques - tions , Proceedings of the 23nd annual ACM symposium on User interface software and technology , UIST ’10 , 2010 , pp . 333 – 342 . 36 [ BLM + 10 ] M . S . Bernstein , G . Little , R . C . Miller , B . Hartmann , M . S . Ackerman , D . R . Karger , D . Crowell , and K . Panovich , Soylent : a word processor with a crowd inside , Proceed - ings of the 23nd annual ACM symposium on User interface software and technology ( New York , NY , USA ) , ACM UIST , 2010 , pp . 313 – 322 . [ Bol01 ] B . Bollob´as , Random Graphs , Cambridge University Press , January 2001 . [ CHMA10 ] L . B . Chilton , J . J . Horton , R . C . Miller , and S . Azenkot , Task search in a human computation market , Proceedings of the ACM SIGKDD Workshop on Human Compu - tation , HCOMP ’10 , 2010 , pp . 1 – 9 . [ DCS09 ] P . Donmez , J . G . Carbonell , and J . Schneider , Eﬃciently learning the accuracy of label - ing sources for selective sampling , Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , ACM , 2009 , pp . 259 – 268 . [ DS79 ] A . P . Dawid and A . M . Skene , Maximum likelihood estimation of observer error - rates using the em algorithm , Journal of the Royal Statistical Society . Series C ( Applied Statistics ) 28 ( 1979 ) , no . 1 , 20 – 28 . [ EHR11 ] S¸ . Ertekin , H . Hirsh , and C . Rudin , Approximating the wisdom of the crowd , Proceed - ings of the Second Workshop on Computational Social Science and the Wisdom of Crowds ( NIPS 2011 ) , 2011 . [ EKPS00 ] W . Evans , C . Kenyon , Y . Peres , and L . J . Schulman , Broadcasting on trees and the ising model , The Annals of Applied Probability 10 ( 2000 ) , no . 2 , pp . 410 – 433 . [ FHI11 ] S . Faradani , B . Hartmann , and P . G . Ipeirotis , What’s the right price ? pricing tasks for ﬁnishing on time , Human Computation’11 , 2011 . [ Hol11 ] S . Holmes , Crowd counting a crowd , 2011 , March 2011 , Statistics Seminar , Stanford University . [ Ipe10 ] P . G . Ipeirotis , Analyzing the amazon mechanical turk marketplace , XRDS 17 ( 2010 ) , no . 2 , 16 – 21 . [ JG03 ] R . Jin and Z . Ghahramani , Learning with multiple labels , Advances in neural informa - tion processing systems , 2003 , pp . 921 – 928 . [ KOS11 ] D . R . Karger , S . Oh , and D . Shah , Budget - optimal crowdsourcing using low - rank matrix approximations , Proc . of the Allerton Conf . on Commun . , Control and Computing , 2011 . [ Lan50 ] C . Lanczos , An iteration method for the solution of the eigenvalue problem of linear diﬀerential and integral operators , Journal of Research of The National Bureau Of Standards 45 ( 1950 ) , no . 4 , 255 – 282 . [ LW89 ] N . Littlestone and M . K . Warmuth , The weighted majority algorithm , Foundations of Computer Science , 1989 . , 30th Annual Symposium on , oct 1989 , pp . 256 – 261 . [ MM09 ] M . Mezard and A . Montanari , Information , physics , and computation , Oxford Univer - sity Press , Inc . , New York , NY , USA , 2009 . 37 [ MW10 ] W . Mason and D . J . Watts , Financial incentives and the “performance of crowds” , SIGKDD Explor . Newsl . 11 ( 2010 ) , no . 2 , 100 – 108 . [ Pea88 ] J . Pearl , Probabilistic Reasoning in Intelligent Systems , Morgan Kaufmann Publ . , San Mateo , Califonia , 1988 . [ RU08 ] T . Richardson and R . Urbanke , Modern Coding Theory , Cambridge University Press , march 2008 . [ RY12 ] V . C . Raykar and S . Yu , Eliminating spammers and ranking annotators for crowd - sourced labeling tasks , J . Mach . Learn . Res . 13 ( 2012 ) , 491 – 518 . [ RYZ + 10a ] V . C . Raykar , S . Yu , L . H . Zhao , G . H . Valadez , C . Florin , L . Bogoni , and L . Moy , Learning from crowds , J . Mach . Learn . Res . 99 ( 2010 ) , 1297 – 1322 . [ RYZ + 10b ] V . C . Raykar , S . Yu , L . H . Zhao , G . H . Valadez , C . Florin , L . Bogoni , L . Moy , and D . Blei , Learning from crowds , Journal of Machine Learning Research ( 2010 ) , no . 11 , 1297 – 1322 . [ SFB + 95 ] P . Smyth , U . Fayyad , M . Burl , P . Perona , and P . Baldi , Inferring ground truth from subjective labelling of venus images , Advances in neural information processing systems , 1995 , pp . 1085 – 1092 . [ SPI08 ] V . S . Sheng , F . Provost , and P . G . Ipeirotis , Get another label ? improving data quality and data mining using multiple , noisy labelers , Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’08 , ACM , 2008 , pp . 614 – 622 . [ WBBP10 ] P . Welinder , S . Branson , S . Belongie , and P . Perona , The multidimensional wisdom of crowds , Advances in Neural Information Processing Systems , 2010 , pp . 2424 – 2432 . [ WRW + 09 ] J . Whitehill , P . Ruvolo , T . Wu , J . Bergsma , and J . Movellan , Whose vote should count more : Optimal integration of labels from labelers of unknown expertise , Advances in Neural Information Processing Systems , vol . 22 , 2009 , pp . 2035 – 2043 . [ WS67 ] G . Wyszecki and W . S . Stiles , Color science : Concepts and methods , quantitative data and formulae , Wiley - Interscience , 1967 . [ YFW03 ] J . S . Yedidia , W . T . Freeman , and Y . Weiss , Understanding belief propagation and its generalizations , pp . 239 – 269 , Morgan Kaufmann Publishers Inc . , San Francisco , CA , USA , 2003 . [ YKG10 ] T . Yan , V . Kumar , and D . Ganesan , Crowdsearch : exploiting crowds for accurate real - time image search on mobile phones , Proceedings of the 8th international conference on Mobile systems , applications , and services , MobiSys ’10 , 2010 , pp . 77 – 90 . [ ZSD10 ] Y . Zheng , S . Scott , and K . Deng , Active learning from multiple noisy labelers with varied costs , Data Mining ( ICDM ) , 2010 IEEE 10th International Conference on , dec . 2010 , pp . 639 – 648 . 38