Evaluation and Improvement of Chatbot Text Classiﬁcation Data Quality Using Plausible Negative Examples Kit Kuksenok jobpal Ltd . Berlin , Germany kit @ jobpal . ai Andriy Martyniv jobpal Ltd . Berlin , Germany andriy @ jobpal . ai Abstract We describe and validate a metric for estimat - ing multi - class classiﬁer performance based on cross - validation and adapted for improve - ment of small , unbalanced natural - language datasets used in chatbot design . Our expe - riences draw upon building recruitment chat - bots that mediate communication between job - seekers and recruiters by exposing the ML / NLP dataset to the recruiting team . Eval - uation approaches must be understandable to various stakeholders , and useful for improving chatbot performance . The metric , nex - cv , uses negative examples in the evaluation of text classiﬁcation , and fulﬁls three require - ments . First , it is actionable : it can be used by non - developer staff . Second , it is not overly optimistic compared to human ratings , mak - ing it a fast method for comparing classiﬁers . Third , it allows model - agnostic comparison , making it useful for comparing systems de - spite implementation differences . We vali - date the metric based on seven recruitment - domain datasets in English and German over the course of one year . 1 Introduction Smart conversational agents are increasingly used across business domains ( Jain et al . , 2018 ) . We focus on recruitment chatbots that connect re - cruiters and job - seekers . The recruiter teams we work with are motivated by reasons of scale and accessibility to build and maintain chatbots that provide answers to frequently asked questions ( FAQs ) based on ML / NLP datasets . Our enter - prise clients may have up to 100 K employees , and commensurate hiring rate . We have found that al - most 50 % of end - user ( job - seeker ) trafﬁc occurs outside of working hours ( Liu , 2019 ) , which is consistent with the anecdotal reports of our clients that using the chatbot helped reduce email and ticket inquiries of common FAQs . The usefulness of these question - answering conversational UIs depends on building and maintaining the ML / NLP components used in the overall ﬂow ( see Fig . 1 ) . In practice , the use of NLP does not improve the experience of many chatbots ( Pereira and D´ıaz , 2018 ) , which is unsurprising . Although transparency ( being “honest and transparent when explaining why something doesn’t work” ) is a core design recommendation ( DialogFlow , 2018 ) , the most commonly available higher - level plat - forms ( Canonico and De Russis , 2018 ) do not pro - vide robust ways to understand error and commu - nicate its implications . Interpretability is a chal - lenge beyond chatbots , and is a prerequisite for trust in both individual predictions and the over - all model ( Ribeiro et al . , 2016 ) . The development of the nex - cv metric was driven by a need for a quantiﬁcation useful to developers , as well as both vendor and client non - developer staff . The nex - cv metric uses plausible n egative ex amples to perform actionable , model - agnostic evaluation of text classiﬁcation as a component in a chatbot system . It was developed , validated , and used at jobpal , a recruiting chatbot company , in projects where a client company’s recruiting team trains and maintains a semi - automated conversa - tional agent’s question - answering dataset . Use of ML and NLP is subject to conversation ﬂow design considerations , and internal and external transparency needs ( Kuksenok and Praß , 2019 ) . The chatbots do not generate answers , but provide all responses from a bank that can be managed by client staff . Each of about a dozen live chatbots answers about 70 % of incoming questions without having to defer to a human for an answer . About two thirds of the automated guesses are conﬁrmed by recruiters ; the rest are corrected ( Fig . 3 ) . In “Background” , we relate our work to prior research on curated ML / NLP datasets and evalua - tion in chatbots . In “Approach” , we describe the Figure 1 : Each incoming message from an end - user is subject to ( 1 ) a general intent classiﬁer speciﬁc to a language ; and , if none of the roughly 20 intents are the recognized , ( 2 ) a company - speciﬁc FAQ classiﬁer . Custom ﬂow affects the speciﬁcs of this behavior . metric and provide its application and data con - text of use . In “Validation Datasets” , we describe the datasets with which this metric has been val - idated . In “Validation” , we provide results from experiments conducted while developing and us - ing the metric for over a year , addressing each of the needs of the metric , which make it a useful tool for multiple stakeholders in the chatbot design and maintenance process . 1 . enable data quality improvements ( Fig . 4 ) 2 . not be overly - optimistic ( Fig . 5 ) 3 . enable model - agnostic comparison ( Fig . 6 ) We contribute a metric deﬁnition , its validation with six real projects over the course of one year ( 2018 . Q2 through 2019 . Q1 ) , as well as an exten - sible implementation 1 and testing plan , which is described in “Metric Deﬁnition” below . 2 Background Chatbots , or “text messaging - based conversa - tional agents” , have received particular attention in 2010s ( Jain et al . , 2018 ) . Many modern text - based chatbots use relatively simple NLP tools ( Abdul - Kader and Woods , 2015 ) , or avoid ML / NLP alto - gether ( Pereira and D´ıaz , 2018 ) , relying on conver - 1 http : / / github . com / jobpal / nex - cv sation ﬂow design and non - NLP inputs like but - tons and quick - replies . Conversational natural - language interfaces for question - answering have an extensive history , which distinguishes open - domain and closed - domain systems ( Mishra and Jain , 2016 ) . ML - based chatbots rely on cu - rated data to provide examples for classes ( com - monly , “intents” ) , and must balance being widely - accessible to many end - users , but typically spe - cialized in the domain and application goal ( Ser - ban et al . , 2015 ) . In practice , design and devel - opment of a chatbot might assume a domain more focused , or different , than real use reveals . In the chatbot application context , the training dataset of a text classiﬁer may be modiﬁed to im - prove that classiﬁer’s performance . The classes — “intents” — are trained with synthetic data and constitute anticipated , rather than actual , use . Ex - isting general - purpose platforms include this syn - thetic data step as part of design and mainte - nance ( Canonico and De Russis , 2018 ) . For ex - ample , when it comes to invocations for a voice agent ( Ali et al . , 2018 ) , dataset construction en - codes ﬁndings about how users might imagine asking for some action : the authors use a crowd - sourcing mechanism to achieve both consistency useful for classiﬁcation , and reﬂection of user expectations in the dataset . We adopt a simi - lar approach : enabling domain - experts ( recruiters ) to maintain the dataset helps map end - user ( job - seeker ) needs to recruiters’ goals . Data cleaning is not only relevant to chatbots . Model - agnostic systems for understanding ma - chine learning can help iteratively develop ma - chine learning models ( Zhang et al . , 2019 ) . De - velopers tend to overlook data quality in favor of focusing on algorithmic improvements in building ML systems ( Patel et al . , 2008 ) . Feature engineer - ing can be made accessible to non - developers or domain experts , e . g . ( Ribeiro et al . , 2016 ) . We make use of representative examples in the process that surfaces nex - cv to non - developers ; in de - scribing this process in “Metric Application” , we map it to the inspection - explanation - reﬁnement process employed in ( Zhang et al . , 2019 ) . En - abling non - developers to perform data cleaning ef - fectively allows developers to focus on model ad - justments and feature engineering . There are many ways to measure overall chatbot quality , such as manual check - lists of high - level feature presence ( Kuligowska , 2015 ; Pereira and D´ıaz , 2018 ) . Static analysis and formal veriﬁca - tion may be used with a speciﬁed ﬂow ( Porﬁrio et al . , 2018 ) . User behavior measurements — both explicit , like ratings or feedback , and implicit , like timing or sentiment — are explored in ( Hung et al . , 2009 ) . During metric development , we used qualitative feedback from domain - expert users , and key performance indicators ( KPIs ) , such as automatic response rate . Regardless of overall evaluation approach , the use of a classiﬁer as a component in a complex ﬂow demands robust and actionable evaluation of that component . 3 Approach The nex - cv algorithm selects some classes as plausible sources of negative examples , and then uses those to partition the given dataset into train - ing and test data ( Alg . 1 ) . Negative examples are useful in chatbot component evaluation : the end - user interaction with a chatbot is open - ended , so the system is expected to encounter input that it should recognize as outside its domain . Low - membership classes are candidates for be - ing ignored in training and used as negative ex - amples in testing . Two mutually - exclusive varia - tions use the K parameter for cutoff - based nega - tive example selection ( Alg . 2 ) ; and the P param - eter for proportional negative example selection ( Alg . 2 ) . We focus on three settings , with ( K , P ) set to ( 0 , 0 ) , ( 0 , 0 . 15 ) , and ( 5 , 0 ) . The values were tuned for typical distributions ( see “Valida - tion Datasets” ) , and the ( 0 , 0 ) is a validating mea - sure that is comparable to 5 - fold CV ( see “Metric Deﬁnition” ) . We assume that low - population classes are all in the same domain as the rest . There may be excep - tions : in some cases a new , small category may be created in response to new questions on an emer - gent topic well outside of the core domain . In our experience , this happens when there is a technical issue elsewhere on the site and the chatbot chan - nel is used as an alternative to escalate the issue . In practice , our system handles this case well , even if the evaluation is less applicable . Such emergent categories either disappear over time : the topic is temporary ; or grow : the topic becomes part of the domain . 3 . 1 System Overview A chatbot ( Fig . 2 ) is based on two datasets ( Fig . 1 ) , each maintained using a data management tool Result : ( X train , y train , X test , y test ) Require data X , y s . t . x i is the input text that has gold standard label y i ∀ i ; Require label sets L SM , L LG s . t . L SM ∪ L LG = { y i | y } Require test fraction 0 < t < 1 and function split t ( L ) which randomly splits out two lists L 1 , L 2 s . t . | L 2 | | L | = t and L 1 ∪ L 2 = L ; for L j ∈ L LG do T R , T S = split t ( i | y i ∈ y ∧ y i = = L ) ; X train , y train ← x i , y i s . t . i ∈ T R ; T R , T S = split t ( i | y i ∈ y ∧ y i = = L ) ; X test , y test ← x i , y i s . t . i ∈ T S ; end T R L , T S L = split t ( { j | y j ∈ L SM } ) ; X train , y train ← x i , y i s . t . y i ∈ T R L ; X test , y test ← x i , Ø s . t . y i ∈ T S L ; Algorithm 1 : Negative Example Data Provision ( Fig . 3 ) . Trafﬁc varies widely between projects , but is typically consistent within a project . To pro - vide a range : in one quarter in 2018 , the high - est trafﬁc chatbot had about 2000 active users , of which about 250 ( ca . 12 % ) asked questions . The lowest - trafﬁc chatbot saw ˜65 weekly active users , of which 15 ( ca . 23 % ) asked questions . In both cases , a small number ( 2 - 4 ) of recruiters were re - sponsible for maintaining the dataset . The training set of the FAQ portion of each project contains between 1 K and 12 K training examples across between 100 and 200 distinct classes , usually starting with about 50 − 70 classes and creating new classes after the system goes live and new , unanticipated user needs are en - countered . To build classiﬁers on datasets of this size , we use spaCy ( Honnibal and Montani , 2017 ) and fastText ( Bojanowski et al . , 2016 ) for vector - ization , with transformation for improved perfor - mance ( Arora et al . , 2016 ) , and logistic regression with L2 regularization ( Pedregosa et al . , 2011 ) . The dataset for shared general intents is main - tained through the data management tool by jobpal staff . One such classiﬁer is shared by all compa - nies that use a particular language ; projects span English , German , Chinese , and French . About 20 general intents are trained with a total of about 1 K to 1 . 5 K training examples per language . These include intents that control the conversation ( e . g . , ‘stop’ , ‘help’ ) . This shared language - speciﬁc clas - siﬁcation step includes entity extraction of profes - Figure 2 : Here , the job - seeker’s question receives an immediate answer , based on the ML / NLP classiﬁer . If conﬁdence is too low , chatbot will defer to a human . Figure 3 : Even if the chatbot responds , recruiters can use a data management tool to review the answer . sion and city of interest to job - seekers ; for exam - ple , statements like ‘I want a [ profession ] job in [ city ] ‘ and ‘do you have any [ profession ] open - ings ? ’ should all resolve to ‘job search’ along with extracted keywords . Lastly , this classiﬁer also identiﬁes very common questions that affect all chatbots 2 , but which are not in the recruitment domain : e . g . , ‘how are you ? ’ and ‘is this a robot ? ’ . The dialog in Fig . 2 shows the FAQ functional - ity of the chatbots , powered by classiﬁcation using company - speciﬁc FAQ datasets ( see also Fig . 1 ) . 2 This was another outcome of the case study summarized in Fig . 4 : we identiﬁed four categories of questions that we could anticipate in all projects , but that were not in the ex - pert domain of the FAQ , so we made modiﬁcations to the ﬂow , the way the existing classiﬁers were used , and the gen - eral intents training data , to help keep company - speciﬁc FAQ datasets more focused . In most projects , users who ask question ask be - tween 1 and 2 questions . The FAQ functionality is typically an addition to any existing informa - tion displays . Many of our chatbots also feature job discovery , including search and subscriptions . Job search may be triggered by clicking on the button [ Look for a job ] , or writing some - thing like “I would like a [ profession ] job in [ location ] ” at almost any point in the ﬂow . If either of location or profession is not speciﬁed , the user is prompted , and the responses are used to search current openings , which are then shown . The user may submit application or follow exter - nal links ; the user may also ask questions about speciﬁc jobs or the employer more generally . 3 . 2 Metric Deﬁnition The code available online 3 provides the evalua - tion implementation , an abstract black - box deﬁ - nition for a classiﬁer , and two strategies to help test an implementation . For integration testing , CustomClassifier . test ( ) can be used to check consistency of classiﬁer wrapper . For func - tional testing , nex - cv both K = 0 ( Alg . 2 ) and P = 0 ( Alg . 2 ) should yield comparable results to 5 - fold cross - validation . Result : L SM , L LG Require data X , y s . t . x i is the input text that has gold standard label y i ∀ i ; Require cutoff parameter K > 0 ; L SM = { y i | y i in y , occurs < K } ; L LG = { y i | y i in y , occurs ≥ K } ; Algorithm 2 : Cutoff Selection of Plausible Neg - ative Example Classes In k - fold cross - validation , data is partitioned into k sets of ( X train , y train , X test , y test ) such that | X test | | X train | = 1 / k ( let the test fraction t = 1 / k ) , and the training sets do not overlap . Then , each set of training data is evaluated using the correspond - ing test set . Evaluation can include many possible measures : accuracy or F 1 ; representative exam - ples ; confusion matrix ; timing data ; etc . In nex - cv , test fraction t is a setting ( 0 . 2 for all reported experiments ) , and data partitions may overlap . As shown in Alg . 1 , representation of high - population classes is enforced . Then , low - population classes are also split using t , and in - cluded either in the training set with their ground 3 http : / / github . com / jobpal / nex - cv Result : L SM , L LG Require data X , y s . t . x i is the input text that has gold standard label y i ∀ i ; Require proportion parameter 0 ≤ P < 1 ; L SM = { } ; Let Q = { y i | y i ∈ y } , as queue sorted from least to most occurring in X ; while | { i | x i ∈ X ∧ y i ∈ L SM } | | X | < P do Pop element L from Q ; L SM ← L ; end L LG = { y i | y i in y , not in L SM } ; Algorithm 3 : Proportional selection of Plausible Negative Example Classes truth label ; or in the test set as a negative ex - ample . In practice , this results in about t of the data being in training . Some low - population classes in the training set should be included as this is representative of the dataset shape ; many low - population classes may affect the classiﬁca - tion and conﬁdence overall , depending on classiﬁ - cation approach . Low - population classes are typ - ically rare or relatively recent topics , so interpret - ing them as plausible negative examples helps to test the classiﬁer , and its measure of conﬁdence . 3 . 3 Validation Datasets The seven datasets to which we report having ap - plied the nex - cv metric are in the recruitment domain . Each dataset has about 50 − 200 classes , and most have classes with 5 - 10 members as well as classes with over a hundred . To characterize the content , we trained a classiﬁer on an anony - mous benchmark dataset 4 and used it to classify a random recent sample of 6 K English - language questions . About 25 % of recent end - user queries in En - glish fall into 5 categories : ( 1 ) Application Pro - cess ; ( 2 ) Salary ; ( 3 ) Professional Growth and De - 4 The clean , anonymized recruitment - domain - speciﬁc dataset in English was built by anonymizing and aggregat - ing all FAQ datasets ; using pairwise similarity between cate - gories to group them . For an initial clustering , we used Jac - card index with a minimum of 0 . 09 , which balanced the goals of high coverage of example data ( 74 ) and reasonable sizes of classes ( 15 examples per class ) ; then , this dataset was sub - ject to iterative data quality improvements as described fur - ther and exempliﬁed in Fig . 4 until a ﬁnal set of about 800 examples over about 47 categories was developed . This ini - tial domain - speciﬁc clustering was performed on English , but has since been extended to other supported languages ; the re - sults reported are speciﬁc to English , however . Figure 4 : Change in classiﬁer performance as a re - sult of data quality intervention . Averages of daily 10 - retry evaluations shown . velopment ; ( 4 ) Internships ; ( 5 ) Contact a Human . Another 25 % of end - user queries fall into 14 categories : Application Evaluation ; Application Deadline ; Application Delete or Modify ; How Long to Apply and Hear Back ; Qualiﬁcation ; Application Documents ; Language Expectations ; Thesis ; Working Hours ; Location ; Starting at the Company ; Commute ; Equipment ; Beneﬁts . About 40 % of overall requests were not recog - nized ( with a conﬁdence of 0 . 5 or higher ) as any of the categories in the anonymous benchmarking set . Upon manual inspection , some of these test questions were noise , and many were topics spe - ciﬁc to particular company FAQs , such as concern - ing speciﬁc work - study programs ; details of the application software ; and other more niche topics . The classiﬁcation datasets share some overlap - ping topics ; each also has a speciﬁc set of addi - tional topics . Each dataset has the typical shape of a few larger classes , and many smaller ones , which have an indirect relationship to what data is expected . The use of low - population classes as plausible negative examples takes advantage of both the content of the data ( closed - domain , with a topic - speciﬁc core but a considerable number of additional , outlying topics ) and the membership distribution of the classes ( a few well - populated ones , and many much smaller classes ) . The nex - cv metric may apply in other prob - lems or domains , but we developed and validated it in a series of experiments with six live datasets , in English and German ( see Fig . 5 , of which chat - bot E is also the subject of Fig . 4 ) , in addition to the seventh aggregate anonymous benchmark dataset described above , which was used for the comparison in Fig . 6 . 4 Validation The following case studies validate the metric rel - ative to each of three requirements : ( 1 ) enable data quality improvements , as in Fig . 4 , ( 2 ) not be overly - optimistic , as in Fig . 5 , ( 3 ) enable model - agnostic comparison , as in Fig . 6 . 4 . 1 Metric Application The goal of usefulness includes interpretability : “provid [ ing ] qualitative understanding between the input variables and the response . . . [ taking ] into account the users limitations in ( Ribeiro et al . , 2016 ) . Usefulness combines this with actionable support of chatbot design . The users include , in this case , non - developer staff on both vendor and client side : recruiters and project managers . Through iteration on internal tools , we found that displaying performance information in the form of , “which 2 - 3 topics are the biggest prob - lem ? ” was most effective for understanding , com - munication , and action . Over the course of a year , the nex - cv metric informed this analysis . Dur - ing this time , both qualitative feedback and KPIs have validated that it was effective both for trust and for the end - user experience . The automation rate KPI — proportion of incoming queries that did not need deferral to a human , but answered immediately , as in Fig . 2 — has risen to and re - mained at 70 − 75 % across projects mainly 5 due to data quality support during both design and main - tenance . 5 The data training UI design contributes to data quality ; in the months following the intervention shown in Fig . 4 , the UI was redesigned to address outstanding usability prob - lems , with very positive feedback from domain - expert users . A more in - depth discussion of the role of human factors in human - in - the - loop systems is out of scope for this paper . In one illustrative project ( Fig . 4 ) the automa - tion rate had become as low as 40 % . The recruiters responsible for dealing with escalated questions became frustrated to see questions come up that had been asked before . Action needed to be taken , and this project became one of the ﬁrst case studies for developing the application of nex - cv inter - nally . After intervention , automated response rate rose into the desirable 70 s range and remained . The quality improvements were explained and im - plemented by an internal project manager , who pro - actively included client domain - expert users in explanations over calls and emails over what improvements were made and why . Initially , 200 classes were trained with 1 K examples , with long tail of low - population classes . Following interven - tion , dataset grew by 25 % and , despite concept drift risk , did not deteriorate . To use nex - cv , we aggregate the confusion matrix from the K = 0 ; P = 0 . 15 setting and rank how confused a pair of categories is . The most confused 2 - 3 pairs of classes are then the focus of conceptual , manual review in the dataset . Evalu - ation is performed again , producing a new rank - ing that guides the next 2 - 3 classes to focus on , until the metric falls below an acceptable thresh - old . There are other sources of classiﬁcation error , but overlap between conceptually related pairs of classes accounts for most of the data quality prob - lems we encounter in the datasets in practice , and are particularly understandable than other forms of error . This relatively simple approach is imple - mented as a Jupyter notebook accessible to non - developers ( internal project managers ) . The details of pairwise measures and accept - ability threshold were developed iteratively based on project manager feedback . The project man - agers also honed processes and intuitions for com - municating this information to clients effectively . In extreme situations as that shown in Fig . 4 the project managers made a presentation to get buy - in and implemented data quality improvements on their own . However , the typical practice now is to provide explanations , in calls and emails , of the “confusions” between one or few pairs of spe - ciﬁc categories to the client . This practice builds awareness of data quality across stakeholders , and the domain - experts ( recruiters ) are better able to use the system to create the envisioned chatbot functionality without major intervention . As the number of projects grows , the metric can be used Figure 5 : Comparison of nex - cv and Human - Rater Accuracy . The six datasets from pseudonymous chatbots tested had a different number of questions ( examples ) and categories ( classes ) , as shown in the bottom row . The human - rater estimate of accuracy ( top left , blue ) is consistently more lenient than any of the automated measures ( top right ) . The ( 0 ; 0 . 15 ) setting ( top right , blue ) is not consistently more or less optimistic than the other settings . Figure 6 : Comparison Against Leading Chatbot NLP Engines on Recruitment - Domain Data . Engine C wraps jobpal’s system ; Engines A and B wrap external general - purpose chatbot platforms . by project managers to monitor and prioritize data quality improvement tasks . 4 . 2 Metric is not Overly Optimistic One of the practical motivations for a new metric was the sense that the existing metrics were too optimistic to be useful to improve chatbot behav - ior in response to overall qualitative feedback . As shown in Fig . 4 , for example , the typical F 1 metric is more optimistic than nex - cv . As an initial step of validating the metric , we applied it in the case of six under - performing datasets that required some intervention . Fig . 4 shows the differences in data abundance and classiﬁer quality across these six pseudonymized snapshots . Internal QA staff gave the human rating scores by considering whether a question - answer pairs seemed reasonable : they could pick “yes” “no” and “can’t tell” ; in most cases , the appropri - ateness was not ambiguous . As shown in Fig . 5 , the human - rater estimate of quality is consistently more lenient than any of the automated measures . The Chatbot E in this case is the same project as shown in Fig . 4 , prior to improvements . Four of the six datasets analyzed had a very big difference between the human estimate of quality and the automated estimate , which , upon inves - tigation , revealed that there were signiﬁcant con - ceptual overlaps in the classes that the recruiters had trained , and the answers given . So , indeed , the classiﬁer was making surprisingly adequate guesses , but which were very low - conﬁdence . Fol - lowing the intervention described in the previous section , which includes ongoing communication of any outstanding problems by project managers to recruiter teams , this type of error became rare and quickly - addressed . 4 . 3 Metric can be used for Internal and External Comparison We used the nex - cv metric to help compare the performance of our classiﬁcation component with two leading vendors for general - purpose chatbot development . Fig . 6 shows the comparison be - tween jobpal and 2 leading vendors in the space . The three settings of the metric 6 were aggregated to provide a plausible range of estimated per - formance . The range of accuracy was signiﬁ - cantly higher for our domain - speciﬁc classiﬁer , than those trained using general - purpose tools . 6 Where ( K , P ) are ( 0 , 0 ) , ( 0 , 0 . 15 ) , and ( 5 , 0 ) , respec - tively , as differentiated in both Fig . 4 and Fig . 5 . Aside from being useful to classify into known classes , the metric must account for fallback or es - calation . This may be modeled as a separate class ( as one of the external engines does with the “fall - back” intent ) , or by relying on conﬁdence scores from classiﬁers that produce measures of conﬁ - dence ( all engines provide some estimate of conﬁ - dence that may be used ) . The “carefulness” score was included to represent how useful the conﬁ - dence score is for deciding when to decline an an - swer : the number of incorrect guesses that were rejected due to too - low conﬁdence scores divided by total no - answer - given cases ( no guess or low - conﬁdence guess ) . Fig . 6 shows that the performance of our ML / NLP component on our domain - speciﬁc dataset is better than that of two popular general - purpose platforms , both in terms of classiﬁca - tion accuracy , and rate of deferral due to low - conﬁdence answers . This comparison mechanism validates our system relative to existing external services in a way that is interpretable by various internal stakeholders , not only the developer staff . 5 Conclusion We described and validated the nex - cv metric , which is a modiﬁcation of cross - validation that makes use of plausible negative examples from low - population classes in the datasets typical of our application area and domain . Existing chatbot guidelines leave error handling to the designer : “transparency” is included as an important topic ( DialogFlow , 2018 ) , but , in prac - tice , why something does not work , and under what conditions , can puzzle designers and devel - opers , not just end - users . We presented on a metric that can be used by a variety of relevant stakehold - ers to understand , communicate , and improve text classiﬁer performance by improving data quality . In future work , we aim to explore other text classiﬁer and chatbot evaluation strategies , keep - ing in mind the needs for understandability and transparency in this multi - stakeholder design pro - cess and maintenance practice . References Sameera A Abdul - Kader and JC Woods . 2015 . Survey on chatbot design techniques in speech conversation systems . International Journal of Advanced Com - puter Science and Applications , 6 ( 7 ) . Abdullah X Ali , Meredith Ringel Morris , and Jacob O Wobbrock . 2018 . Crowdsourcing similarity judg - ments for agreement analysis in end - user elicitation studies . In The 31st Annual ACM Symposium on User Interface Software and Technology , pages 177 – 188 . ACM . Sanjeev Arora , Yingyu Liang , and Tengyu Ma . 2016 . A simple but tough - to - beat baseline for sentence em - beddings . Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov . 2016 . Enriching word vec - tors with subword information . arXiv preprint arXiv : 1607 . 04606 . Massimo Canonico and Luigi De Russis . 2018 . A com - parison and critique of natural language understand - ing tools . Cloud Computing 2018 , page 120 . DialogFlow . 2018 . Dialogﬂow design guide - lines . conversational components - error han - dling . designguidelines . withgoogle . com / conversation / conversational - components / er - rors . html . Accessed : 2018 - 09 - 02 . Matthew Honnibal and Ines Montani . 2017 . spacy 2 : Natural language understanding with bloom embed - dings , convolutional neural networks and incremen - tal parsing . To appear . Victor Hung , Miguel Elvir , Avelino Gonzalez , and Ronald DeMara . 2009 . Towards a method for eval - uating naturalness in conversational dialog systems . In Systems , Man and Cybernetics , 2009 . SMC 2009 . IEEE International Conference on , pages 1236 – 1241 . IEEE . Mohit Jain , Pratyush Kumar , Ramachandra Kota , and Shwetak N Patel . 2018 . Evaluating and informing the design of chatbots . In Proceedings of the 2018 on Designing Interactive Systems Conference 2018 , pages 895 – 906 . ACM . Kit Kuksenok and Nina Praß . 2019 . Transparency in maintenance of recruitment chatbots . arXiv preprint arXiv : 1905 . 03640 . Karolina Kuligowska . 2015 . Commercial chatbot : per - formance evaluation , usability metrics and quality standards of embodied conversational agents . Ching - Ju Liu . 2019 . Behind the screen : When do applicants approach you ? https : / / jobpal . ai / en / blog / when - do - applicants - approach - you / . Accessed : 2019 - 05 - 01 . Amit Mishra and Sanjay Kumar Jain . 2016 . A survey on question answering systems with classiﬁcation . Journal of King Saud University - Computer and In - formation Sciences , 28 ( 3 ) : 345 – 361 . Kayur Patel , James Fogarty , James A Landay , and Bev - erly L Harrison . 2008 . Examining difﬁculties soft - ware developers encounter in the adoption of statis - tical machine learning . In AAAI , pages 1563 – 1566 . F . Pedregosa , G . Varoquaux , A . Gramfort , V . Michel , B . Thirion , O . Grisel , M . Blondel , P . Pretten - hofer , R . Weiss , V . Dubourg , J . Vanderplas , A . Pas - sos , D . Cournapeau , M . Brucher , M . Perrot , and E . Duchesnay . 2011 . Scikit - learn : Machine learning in Python . Journal of Machine Learning Research , 12 : 2825 – 2830 . Juanan Pereira and Oscar D´ıaz . 2018 . A quality analy - sis of facebook messenger’s most popular chatbots . In Proceedings of the 33rd Annual ACM Symposium on Applied Computing , pages 2144 – 2150 . ACM . David Porﬁrio , Allison Saupp´e , Aws Albarghouthi , and Bilge Mutlu . 2018 . Authoring and verifying human - robot interactions . In The 31st Annual ACM Sym - posium on User Interface Software and Technology , pages 75 – 86 . ACM . Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 . Why should i trust you ? : Explain - ing the predictions of any classiﬁer . In Proceed - ings of the 22nd ACM SIGKDD international con - ference on knowledge discovery and data mining , pages 1135 – 1144 . ACM . Iulian Vlad Serban , Ryan Lowe , Peter Henderson , Lau - rent Charlin , and Joelle Pineau . 2015 . A survey of available corpora for building data - driven dialogue systems . arXiv preprint arXiv : 1512 . 05742 . Jiawei Zhang , Yang Wang , Piero Molino , Lezhi Li , and David S Ebert . 2019 . Manifold : A model - agnostic framework for interpretation and diagnosis of ma - chine learning models . IEEE transactions on visu - alization and computer graphics , 25 ( 1 ) : 364 – 373 .