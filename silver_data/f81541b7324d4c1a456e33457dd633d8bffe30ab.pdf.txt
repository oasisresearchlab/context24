BlueSky : Crowd - Powered Uniform Sampling of Idea Spaces Gaoping Huang , Alexander J . Quinn Purdue University School of Electrical & Computer Engineering West Lafayette , Indiana { huang , aq } @ purdue . edu ABSTRACT Design contests and group creativity support systems have demonstrated the value of crowds for producing a solution to a design or engineering problem . However , when the goal is not one idea , but many ideas , an uncoordinated crowd effort would likely lead to a set of ideas clustered around those that are eas - iest to think of . We present BlueSky , a crowd - powered system that coordinates microtask workers to enumerate a uniform sample of textual ideas on a given topic . Through the process of soliciting ideas , an ontology is developed , which is used to categorize the ideas in the list . That categorization then reveals combinations of attributes that have not yet been covered . Our evaluation with four list topics compared BlueSky to freeform solicitation of ideas with respect to comprehensiveness ( cover - age over the entire idea space ) and uniformity ( mitigating the tendency to emphasize ideas that are easy to think of ) . Author Keywords Human computation ; crowdsourcing ; innovation ; brainstorming ACM Classiﬁcation Keywords H . 5 . 3 Group and Organization Interfaces : Crowdsourcing INTRODUCTION With the rise of crowdsourcing came a recognition of its value of a distributed workforce for creative applications [ 28 , 11 , 14 ] . Web sites such as 99designs and InnoCentive have had success engaging crowds on design and engineering problems through design contests . By posting many such contests on one site , would - be contributors can match their individual knowledge and experience with problems to which they are well - suited . With a sufﬁciently large and diverse set of participants , the strategy can lead to a pool of candidate ideas from which a single winner is chosen . Design contests—and idea contests , in general—depend on chance . Finding a great idea requires that among the people Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspeciﬁcpermission and / or a fee . Request permissions from permissions @ acm . org . C & C ’17 June 27 - 30 , 2017 , Singapore , Singapore © 2017 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ISBN 978 - 1 - 4503 - 4403 - 6 / 17 / 06 . . . $ 15 . 00 DOI : http : / / dx . doi . org / 10 . 1145 / 3059454 . 3059481 who view the contest , at least one will have just the right combination of inspiration or background knowledge to lead them to a winner [ 2 ] . Most other forms of group creativity support tools are also primarily participant - directed , and thus effectively dependent on chance [ 19 ] . Moreover , these assume that the motivating goal is to ﬁnd a winner . This work engages crowds differently , in the form of mech - anized labor—or , more aptly , mechanized creativity—to achieve a broader goal : Enumerate a uniform sample of ideas in an idea space . For example , a few examples of textual ideas spaces ( i . e . , ideas expressed as text ) are below : 1 . The developer of a new virtual reality system wants to understand the full range of possible applications . 2 . A political campaign wants to prepare its candidate for the full range of questions that might arise in a debate . 3 . A marketing ﬁrm wants to test a large and diverse set of text ad blurbs in an online experiment . A truly exhaustive listing of all ideas for such topics would of course be impossible to produce . Many items on such a listing would be deemed duplicate , or at least “equivalent” by any reasonable standard . For example , given the prompt , “Ways to use a brick , ” the ideas “break a window” and “throw a brick through a glass pane” would probably be deemed equivalent . However , “break a window” and “crack a window” would depend on what one deﬁnes as distinct . Given such a deﬁnition , then a comprehensive sample could be achieved . One possible standard for distinctness is suggested by legal theories of originality in the United States , which demand at least a modicum of creative thought [ 1 ] . Purely mechanical transformations of existing knowledge are deemed unoriginal ( and thus ineligible for copyright ) . We take this a step further by requiring that for a new idea to be considered distinct , it must add creative value relative to the needs of the requester who solicits the list of ideas . Even then , there could still be many possible deﬁnitions of “distinct” for a given ideation task . The choice depends on the intended purpose of the list . For example , a building supply marketer might treat two ideas as effectively equivalent if they would appeal to the same segment of customers , or are used in a different setting . In contrast , a sculpture artist might differentiate based on the physical characteristics of the brick that are exploited . More concretely , we consider two ideas Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 119 distinct if they differ with respect to any dimension that is deemed important by the requester . For example , “break a window” and “break a vase” differ by the dimension , “target of action” ( e . g . , window vs . vase ) . Ideally , the deﬁnition of distinctness would come from the requester for whom the list of ideas is being created ( e . g . , the developer of the virtual reality system ) . That way , the kinds of variations could be aligned with what they need and value . One could imagine a system in which requesters ac - tively participate , guiding crowd workers toward the avenues of variation that will produce the most value for that requester . However , for this research , we will aim for a process that is as self - contained as possible . The objective of this work can be now stated as follows : Enumerate a sample of ideas spanning an idea space that are distinct with respect to some set of k dimensions . This paper presents BlueSky , a system we developed to demon - strate a novel method for enumerating idea spaces ( hereafter referred to as the BlueSky method ) . The inputs consist of a noun phrase describing the list topic ( e . g . , ”political debate questions , ” ”ways to use a brick , ” etc . ) , accompanied by a brief explanation of the context , to help workers understand . BlueSky then coordinates workers on Amazon Mechanical Turk ( AMT ) , together with intermittent feedback from the requester , to generate a list of ideas covering every meaningful combination of values on the dimensions . A dimension consists of a label and a set of val - ues that can be used to partition the ideas in a list . Some dimensions are inherently discrete and ﬁnite ( e . g . , “ is destructive ” ⇒ { “ yes ” , “ no ” } ) . The others will be clus - tered into bins or representative values ( e . g . , “ location ” ⇒ { “ indoor ” , “ outdoor ” , “ underground ” , “ outerspace ” } ) . Any item in an idea space can be mapped to a set of values for each of the dimensions . Two ideas that map to the same set of values are treated as equivalent . One of the primary goals of BlueSky is comprehensiveness : covering every compatible combination of values . The other primary goal is uniformity : controlling redundancy so that ( ideally ) only one idea is ac - quired for each combination of values . As we will see , perfect uniformity is hard to achieve given the constraints of Amazon Mechanical Turk , the platform which we have used for this research . However , our evaluation demonstrates that BlueSky produces a much more uniform list of ideas ( i . e . , less redun - dancy ) than freeform solicitation of ideas . The key contributions can be summarized as follows : 1 . We show how mechanized creativity —driving the initiative and discovery from the system—can engage the creative intellect of crowd workers with less dependence on chance than unconstrained idea solicitation . 2 . BlueSky , a system we developed , applies mechanized cre - ativity to enumerate a comprehensive sample of an idea space efﬁciently . 3 . Our evaluation demonstrates that a uniform and diverse sam - ple can be more efﬁciently acquired by synthesizing a set of dimensions from an initial batch of unconstrained ideas and using those dimensions to guide further ideation—versus collecting the same number of ideas without constraints . RELATED WORK BlueSky builds from prior work in morphological concept generation , creativity support tools and sensemaking with crowds . It also considers some factors that may inﬂuence the creativity in the ideation process . Crowd - powered innovation Creative tasks can be made less dependent on chance by guid - ing the process with some element of the idea space . Yu et al . have shown that crowds can ﬁnd analogies from within a large set of ideas [ 27 ] , and that such analogies can provide the seed for new and valuable ideas [ 26 ] . The type of semantics lever - aged are quite different from BlueSky , but both use semantic connections between ideas to enhance the diversity of ideas produced by the crowd ideators . Dimension - driven ideation Dimensions and values have been used for ideation of graphi - cal designs , by individuals and crowds . Talton et al . demon - strated one of the ﬁrst interfaces that allow individual users to explore an idea space by manipulating sliders to control various dimensions [ 23 ] . A similar strategy is embodied by At - tribit , which also engages workers on Mechanical Turk to help describe relationships between resulting objects ( e . g . , “The right part makes the shape look than the left part . ” ) [ 4 ] . Enumeration tasks with crowds Recent work relating to the enumeration of unbounded sets with crowds has established a foundation for our work [ 18 , 24 ] . However , enumerating creative idea spaces is fundamentally different from drawing ideas from the Internet at large . Quasi - comprehensive enumeration of an idea space A recent effort enumerates ideas for inventions using ma - chine computation . The website’s stated goal is “to algo - rithmically create and publicly publish all possible new prior art , thereby making the published concepts not patentable” ( http : / / allpriorart . com ) . Here is an example : “Devices and methods for therapeutic photodynamic modulation of neural function in a human . Product gas Figure 1 . Voluntary hint interface with ideation ( left ) and voting ( right ) . Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 120 in the vapor phase is drawn from the head space above the liquid level and condensed to form the product fuel . The adapter segment is positioned at the ﬁrst end and is conﬁgured to be coupled to another component . ” All have been computationally generated using algorithms that are not yet publicly disclosed . Morphological Concept Generation Morphological concept generation is a method from engineer - ing design that involves mapping an ideas space in a way that could potentially be used to enumerate the ideas . It generates concepts based on a morphological chart . A morphological chart in essence is a table of functions ( desired capabilities for the object being designed ) and an array of means ( ways to implement or achieve those capabilities ) [ 20 ] . The space of possible designs can then be viewed as the combination of available means for each function [ 21 ] . Exploring that space of combinations is a popular method of engineering design [ 17 , 9 , 7 , 25 ] . Web - based morphological charts have been demon - strated , including support for functional analysis , conception generation , and concept evaluation [ 12 ] . Since morphologi - cal charts are typically used in a large idea space , they can naturally lead to combinations of features that would make the design impractical or nonsensical . Compatibility matrices can be used to evaluate whether functions can reasonably be combined [ 8 , 17 ] . BlueSky could , in a very loose sense , be considered an adapta - tion of the morphological chart method to distributed ideation by crowds . The notions of functions and means are analogous to dimensions and values , though the latter is more broad , since BlueSky and the ideas it embodies , supports other kinds of ideation tasks , as well . BlueSky also automates the process of eliminating incompatible combinations . Creativity Support Tools Key ideas from creativity support tools [ 19 ] have been applied to crowdsourcing to amplify the potential for creative output . Crowds inherit previous examples and combine atypical fea - tures to produce more creative ideas [ 28 , 29 ] . Challenges , however , stem from the vague requirements for discovery and innovation , as well as from the unorthodox user behaviors and unclear measures of success [ 19 ] . BlueSky could gradually un - fold the important dimensions that constitute clear constraints and clear measures of success . Sensemaking with Crowds Enumeration of ideas can be thought of as exploring an as - yet - unknown information space . In that way , it is closely related to work in sensemaking of existing information sources [ 16 , 6 ] . Sensemaking is how people construct insights and make use of existing data , especially in quantities that would otherwise overwhelm human working memory . Recently , distributed sensemaking platforms have been developed to leverage the value of crowds , either alone or blended with automated tech - niques . Crowdlines [ 15 ] , for example , uses crowdsourcing to synthesize diverse online information to better describe the essential knowledge . Cascade [ 5 ] creates taxonomies for user - contributed datasets ( e . g . , answers to questions ) through crowd workers’ generation , selection , and categorization . Figure 2 . Attribute taxonomy interface . The column headers show the ideas and the row headers show the attributes . Other approaches have had success combining machine learn - ing algorithms , such as partial clustering , which discovers categories and later classiﬁes data sets [ 10 ] , and SWIRL ( sam - pling with reduced replacement ) , which models human list production [ 13 ] . Researchers have compared different sense - making approaches ( between human and machine ) for large - scale ideation [ 3 ] . BlueSky combines the user - generated lists with requester’s prior understanding of the idea space to identify the impor - tant dimensions . Based on those dimensions , it also has the information about the covering status of the idea space . INITIAL APPROACHES The central strategy of BlueSky is to discover the important dimensions and then continue soliciting ideas until all com - patible combinations of dimension - values have been covered . However , developing this strategy into a working method and system was an iterative process . In this section , we detail some of our early approaches . These illustrate the design challenges , which had to be overcome in order to develop our high - level strategy into the eventual BlueSky method and implementation , which will be presented in the following section . Voluntary Hints Pointing Solution Path To explore a wider idea space , our ﬁrst strategy was to let workers ideate and voluntarily share the promising solution paths with future workers ( Fig . 1 ) . Participants were asked to enter ideas and rate existing ideas generated by others . While typing , they might see some similar ideas generated by others and be suggested to avoid duplicate ones . Once got stuck , they could click the “See some hints” button to acquire some hints which were shared voluntarily by other participants who had multiple solution paths . We evaluated it with a speciﬁc list topic—“things that humans can do , but computers cannot“—and got 191 ideas from 12 distinct AMT workers . We found that the strategy of voluntary hints suffered from two main problems : 1 . Freeform ideation may lead to many redundant ideas . Workers received no strict / tight constraint in the whole pro - cess of ideation . Instead , they were suggested , not required , to think a different idea once their ideas were duplicated with existing ones . There were only 167 distinct ideas ( out of 191 in total ) after de - duplication ; meanwhile , a lot of Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 121 them are similar . For instance , “make food” and “cook” are basically the same ; likewise , “play” , “play go” and “play sports” are very similar . The possible reason is that workers often think in their familiar ways on the lack of compulsory constraint . 2 . Hoping to obtain hints from workers may be inefﬁcient and infeasible . During the whole process of ideation , no hint was added by workers . One reason could be that the task was so loosely restricted that workers might feel unnec - essary to provide hints for next workers . We further realized that workers would not provide hints even in a tougher task but simply ﬁnish the task by their own . We also observed creativity ﬁxation effects [ 22 ] . Therefore , we decided to 1 ) add constraints to achieve less duplication and 2 ) reduce the exposure of others’ ideas in the ideation session ( e . g . , removing the voting section and auto - suggestion ) . Attribute Taxonomy The second strategy was to create a taxonomy of attributes that are contained in most ideas . Figure 2 shows the interface that allows workers to propose new attributes and extract details from ideas for each attribute . After the extraction , workers should merge the details into several values that could form a parent - child relationship with each attribute . We tested it with four graduate students and one undergraduate student by giving a list of ideas for the topic “ways to use a brick” . Under face - to - face environment , they could ask questions if they were confused about the process . Similar to Cascade [ 5 ] , we observed that they often gave overly broad attribute names such as “build” or “tools” . Besides , the details they extracted were often in different levels of abstraction . Take attribute “build” for example , participants extracted both higher - level details like “something” and lower - level details like “house” and “pond for ﬁsh” , from the ideas “build a house” , “build a pond for ﬁsh” , and “use to build something” . This is because the ideas contained different levels of details . Furthermore , participants had great difﬁculty in merging those details into values . We thus decided to ask workers to provide values directly , instead of merging from details with unpredictable , different levels . DEFINITIONS Before describing the system design , we deﬁne some key terms , as they are used in this paper . A dimension ( d ) is comprised of a ﬁnite set of values ( V ) . We denote this relationship as d ⇒ V . Some dimensions are inherently discrete and ﬁnite ( e . g . , “ is destructive ” ⇒ { “ yes ” , “ no ” } ) . For others , the values represent clus - ters . For example , given the dimension location , there could potentially be an inﬁnite number of locations in the universe , but for our purposes , the set of val - ues will always be a ﬁnite set ( e . g . , “ location ” ⇒ { “ indoor ” , “ outdoor ” , “ underground ” , “ outerspace ” } ) . Together , a set of dimensions form the basis of an idea space . As in linear algebra , this basis is not unique . We use the term dimension , not category , because a dimen - sion is intended to cover most ( or even all ) of the ideas ( also referred to as list items ) while a category may only cover partially . Also , in early testing , we found that some people interpret category like dimension , while others interpret it like value . The terms dimension and value , while perhaps less intuitive , do not suffer from this ambiguity . To chart an idea space , the values of a dimension should meet the following criteria as much as possible : • Orthogonal ( no overlap between each other ) . For example , “ occurrence time ” ⇒ { “ in January ” , “ in February ” , . . . } are orthogonal , but { “ in January ” , “ in the spring ” , . . . } are not , because “January” has overlap with ”spring” . • Evenly - spaced ( nearly the same differences between each other ) . For example , “ occurrence time ” ⇒ { “ in January ” , “ in February ” , “ in March ” , . . . } are evenly - spaced , but { “ in January ” , “ in June ” , “ in July ” , . . . } are not . • Complete ( all representative aspects ) . For example , “color” ⇒ { “red” , “green” , “blue” } and { “ cold ” , “ warm ” } are both complete , but { “ red ” , “ green ” } are not . Idea Space and Cell In theory , every idea space is charting by an inﬁnite number of dimensions . However , in a practical application , a requester with a speciﬁc list topic may only care about a ﬁnite represen - tation of the idea space . Suppose an idea space is represented by n vital dimen - sions ( d 1 , d 2 , . . . , d n ) along with corresponding sets of val - ues ( V 1 , V 2 , . . . , V n ) where d i ⇒ V i , then a cell of the idea space is a combination of one value from each dimension { ( v 1 , v 2 , v 3 , . . . , v n ) : v 1 ∈ V 1 , v 2 ∈ V 2 , . . . , v n ∈ V n } . For example , suppose the list topic ”ways to a brick” has an idea space of n = 2 dimensions : “ is destructive ” ⇒ { “ yes ” , “ no ” } ( V 1 ) and “ location ” ⇒ { “ indoor ” , . . . } ( V 2 ) , then one of the cells will be { ( “ yes ” , “ indoor ” ) : “ yes ” ∈ V 1 , “ indoor ” ∈ V 2 } . By deﬁni - tion , each cell represents a distinct idea . Thus the maximum number of distinct ideas is equal to the number of cells : NumberOfDistinctIdeas ≤ n ∏ i = 1 | | V i | | , where | | V i | | is the number of values for the dimension d i . An idea space is completely covered when each cell is covered by at least one idea , and uniformly covered if the number of ideas covering each cell is roughly the same . Constraint An empty cell of the idea space means one cell that is not covered by any idea . To cover it , we need to guide participants with constraints . It is straight - forward to convert an empty cell { ( v 1 , v 2 , v 3 , . . . , v n ) : v 1 ∈ V 1 , v 2 ∈ V 2 , . . . , v n ∈ V n } into a batch of constraints ( C = ( c 1 , c 2 , . . . , c n ) ) by replacing v i with a constraint c i , where c i is in the format of “ v i , and not ∀ v ∈ V i \ v i , −− w . r . t d i ” . Here , ∀ v ∈ V i \ v i means every value in the set V i except v i . For example , if v i is a value “ indoor ” from dimension “ location ” ⇒ { “ indoor ” , . . . } , then it will be con - verted to a constraint “Indoor , and not outdoor , underground , Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 122 Figure 3 . Ideation task interface with constraints or outerspace – w . r . t location” . In particular , for a dimension with Boolean values ( e . g . “ is destructive ” ⇒ { “ yes ” , “ no ” } ) , its values will be converted into human - readable constraints like “ is destructive ” and “ is not destructive ” . BLUESKY We now discuss BlueSky ( the system ) and the algorithms that it uses to coordinate microtask workers to enumerate idea spaces . BlueSky is implemented as a Python web application that interoperates with Amazon Mechanical Turk by way of the CrowdLib library . A requester enters a list topic as a noun phrase ( e . g . , ”interview questions” ) accompanied by a brief de - scription of the context ( e . g . , “Suppose you are the interviewer of an innovative company . . . . ) . Algorithm Steps BlueSky’s overall control ﬂow consists of three HITs that are shown to participants . Here are the concrete instances of these HITs— ideation ( Fig . 3 ) , synthesis ( Fig . 4 ) , and map ( Fig . 5 ) . The system runs through a circular way , from ideation to synthesis , to map and then back to ideation . BlueSky also includes a dashboard to assist requester to moni - tor the process and prune dimensions of no interest . We ﬁrst present an outline of the steps of Bluesky and then describe the steps in full detail . 1 ) Show list topic , description , and constraints ( if any ) to par - ticipants whom each ideates 1 list item based on the con - straints ( or 2 without constraints ) , or mark the constraints as N / A ( not applicable ) . 2 ) Show list items to participants whom each synthesizes one dimension and 2 - 8 values that could be used to categorize the existing and future list items . 3 ) Show list items and dimensions to participants whom each maps the list items to their best ﬁt value of each dimension . Figure 4 . ( Top ) The tutorial for synthesis with four lessons . Area 1 teaches participants how to categorize ideas by given dimensions and values , while area 3 and 2 teach them how to generate dimensions and values , respectively . Area 4 teaches them how to combine all the skills in the previous lessons . ( Bottom ) The task interface for synthesis . 4 ) Compute and generate constraints for empty cells . Repeat the algorithm ( from step 1 ) with these constraints . 5 ) Prune ( by requester ) the dimensions of no interest in the dashboard during the whole process . The algorithm continues until each cell in the idea space is either covered by list items or marked as N / A . Following are the implementation details for each step . Step 1 . Ideation with or without Constraints This step has two phases , depending on whether participants are shown constraints or not . Phase 1 : The ﬁrst several ideation HITs have no constraints as we have no inference on the idea space . Participants are asked to enter two ideas freely in each HIT . Phase 2 : After one round , participants will be guided by a batch of constraints that represents an empty cell in the idea space ( Fig . 3 ) . As mentioned in Deﬁnitions , the batch of constraints is denoted by C = ( c 1 , c 2 , . . . , c n ) . Participants are asked to enter one idea to meet every c i ∈ C . For example , in Fig . 3 , the HIT has a batch of two constraints ( C = ( c 1 , c 2 ) ) in which c 2 : “Statement ( of topic ) , and not question – w . r . t question or statement” . Note that c 2 is converted from a value “ statement ( of topic ) ” of dimension “ question or statement ” ⇒ { “ statement ( of topic ) ” , “ question ” } . If participants have difﬁculty in generating ideas for the given constraints , they have two other options . One is to start over with another batch of constraints ( e . g . , C = ( c (cid:48) 1 , c (cid:48) 2 , . . . , c (cid:48) n ) ) , Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 123 namely , another empty cell . The other is to mark the current constraints as N / A if they are impossible to generate an idea , which in our case may be caused by incompatible combina - tions of dimensions or unclear values ; meanwhile , participants need to write down the reason . If at least two participants mark the same batch of constraints as N / A , the corresponding cell in the idea space is marked as N / A and will not appear in the future HITs . Based on the usability feedback of Phase 2 , we added several optimizations . First , participants will start with a simpler batch of constraints . Speciﬁcally , they will be given a subset of two constraints from the batch C , e . g . , ( c i , c j ) ⊂ C , as a stepping stone . After participants have done all the subsets , they will be given the full batch C . A subset contains two constraints because it represents a minimum combination of dimensions and can be used to test compatibility . If a subset is compatible , the ideas generated for it will be used as examples for any batch C containing it ( when participants click the button “Show example” ) . On the other hand , if the subset is incompatible , any batch C containing it will be automatically marked as N / A . Second , to motivate participants to generate ideas covering all of the constraints in the batch , we provide them a bonus of $ 0 . 15 ( US dollars ) if their ideas are being categorized ( in step 3 ) by the same values as the constraints . The bonus incents them to try hard while leaving us leeway to approve HITs where a worker makes a nominal effort but fails to meet all of the constraints . Step 2 . Synthesize Dimensions and Values This step needs participants to enter one dimension and 2 - 8 values . Since the dimensions are fundamental for the idea space , we provided a tutorial with four lessons to explain the concepts of dimensions and values , as shown in the top of Fig . 4 . The ﬁrst lesson ( marked by number 1 ) asks participants to categorize a list of “ways to use a brick” by given dimensions and values . The second lesson ( marked by number 2 ) asks participants to enter values for given dimensions and already categorized list items . Likewise , the next lesson ( marked by number 3 ) lets them enter dimensions for given values and already categorized list items . The last lesson ( marked by number 4 ) wraps up the previous lessons : it ﬁrst gives several list items ( uncategorized ) and requires participants to think of dimensions , values and then categorize those items . We partition the necessary skills—categorization , generating dimensions , and generating values—into the ﬁrst three lessons so that participants can build up skills gradually . Each lesson has the same list topic but different dimensions , values and list items , which makes the skills more generalizable . Once participants complete any of the ﬁrst three lessons , their re - sults will be compared with gold standard and prompted with explanation if incorrect . The results of the fourth lesson will be manually checked by a qualiﬁed participant who has passed the tutorial . The task interface is similar to the fourth lesson of the tutorial , except that there is only one dimension ( the bottom of Fig . 4 ) . Only those who have passed the tutorial can take this HIT . Figure 5 . Mapping task interface Participants can see the tutorial they have done and thus have a strong connection between the tutorial and real task . This step will be skipped when all of the important dimen - sions have been ﬁgured out . In case of pruning an important dimension by the requester , this HIT will restart after that . Step 3 . Map with Multi - judgment This step needs participants to map the list items from step 1 to a set of values for each of the dimensions from step 2 ( Fig . 5 ) . It is essentially categorization . Participants will see a conﬁrmed dimension and several un - categorized list items ( excluding ones that they entered in step 1 ) in each HIT . Conﬁrmed means that the dimension is one of the vital dimensions ( dependent on the requester ) that could represent the idea space . If dimensions are provided by the requester , they are automatically conﬁrmed . If they are provided by AMT workers , the requester may prune some nonsense dimensions or less interesting ones . In case of ab - sence of the requester , dimensions provided by workers could be conﬁrmed through multiple judgments : workers will see all of the dimensions and select the best one to categorize with ; then the dimensions selected by three or more workers are conﬁrmed . In our test , we found that the dimensions selected by three workers were usually selected by a fourth worker , so it is reasonable to ask three workers to perform the multiple judgment . At any point in time , the requester may come in and modify ( or even prune ) a conﬁrmed dimension to ﬁt their interests . Similarly , a list item is categorized by a speciﬁc value if at least two workers categorize it with the same value . A list item can be categorized by only one value of a dimension , as the values Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 124 are orthogonal to each other . Participants may also categorize a list item as N / A ; if two or more workers categorize an item as N / A , it is marked as N / A for this dimension . To reduce the number of mapping workers required , we use a simple optimization . For each dimension in the constraint at the ideation step , we count the ideation worker’s assertion that the idea conforms the same as if a worker in the mapping step had evaluated it . However , since some contributed ideas do not truly conform to all of the values in the constraints , we still require one judgment from a mapping worker before we consider that cell in the idea space as ﬁlled . Step 4 . Idea Space Computation After categorizing existing list items , BlueSky computes the status of the idea space . It stops running if all of the cells have been covered or marked as N / A , or posts more ideation HITs ( step 1 ) for empty cells . This step is done automatically without participants . Step 5 . Dashboard for Requester Through the dashboard , a requester can see the whole picture of the idea space , such as the covering status of each cell . Moreover , a requester can prune dimensions that are trivial or unlikely to generate useful ideas , and thus ﬁgure out the important ones . MAIN EXPERIMENTS To test the performance of BlueSky , we ran the algorithm on four list topics and analyzed the ideas it produced . Recruitment of Participants In step 1 and step 3 , participants were AMT workers with min - imal approval rate of 90 % , while in step 2 , participants were AMT workers who have passed the tutorial . As mentioned earlier , workers’ tutorial results ( i . e . , the fourth lesson ) will be judged by a qualiﬁed participant who has passed the tutorial . In this experiment , we asked two graduate students to take the tutorial and then recruited one who passed to judge workers’ results . In principle , it is also plausible to let AMT workers be the judge once they have passed the tutorial . List Topics We picked four open - ended list topics that belong to different types . Two of them were related to tangible things , such as HoloLens ( Microsoft augmented reality devices ) and a gadget for ﬁnding keys . The others were intangible concepts like interview questions and debate questions . The list topics are summarized in Table 1 . These list topics represented three kinds of user goals in which a user : 1 ) needs lots of distinct ideas ( not necessarily com - plete ) , such as mathematical test questions ; 2 ) needs com - pleteness for technical reasons and / or future - prooﬁng , such as test cases for machine learning ; or 3 ) needs inspiration to ultimately ﬁnd the best idea , such as business names . Method For each list topic , we conducted an experiment with two conditions : a treatment condition that gave the constraints generated by BlueSky and a control condition in which no Abbreviation List topic Types Application use - HoloLens “ways to use HoloLens” T needs lots of dis - tinct list items ( not necessarily complete ) interview “interview questions to examine one’s creativity” IT needs complete - ness for technical reasons and / or future - prooﬁng debate “debate ques - tions for the US election” IT needs complete - ness for technical reasons and / or future - prooﬁng textAd “text ad blurbs for a new gad - get for ﬁnding lost keys to ap - pear in Google Ads” T needs inspiration to ultimately ﬁnd the best idea Table 1 . Two types ( T : tangible , IT : intangible ) of list topics with their potential applications . constraints were given . The list items from the control were categorized by the same dimensions as the treatment condition . Participants were randomly assigned to one group or the other and were prevented from participating in both . Results In phase 1 of step 1 , both the treatment and control posted the same number of HITs . However , in phase 2 , the treatment HITs were twice as the control because if the constraints were given , workers needed to enter one list item instead of two . Those list items were categorized by the same dimensions . We stopped both the treatment and control groups when the idea space was completely covered or when the coverage remained the same for a long time ( e . g . , longer than two iterations ) . Therefore , we ended up with approximately the same number of list items for both conditions . For simplicity , we truncated the list items of two conditions to the same number . The results are shown in Table 2 . useHoloLens produced 309 list items in the treatment condi - tion by 96 workers and 308 items in the control condition by 40 workers . We truncated them into 308 items for simplicity . In both conditions , some list items were literally duplicate , such as ”playing video games . ” Some were phrased differently but had the same meaning , such as “virtual museum tours” and “To give a tour of a museum to people remotely . ” Some were phrased differently with different meanings , such as “Ex - plore places on Earth where humans can’t survive . ” ( L 1 ) and “Simulate real world emergencies to respond to” ( L 2 ) . useHoloLens also produced 16 dimensions based on the list items by 6 workers who have passed the tutorial . In step 3 , workers conﬁrmed four dimensions by multi - judgment , such as “Age of user” with 4 values { “child” , “teenager” , “Adults” and “All ages” } . The other three dimensions had 2 , 4 , and 4 values , respectively . Thus the maximum number of cells in the idea space was 128 ; namely , there would be at most 128 distinct list items . In such case , even some items Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 125 List topic # of ideas # of dimensions ( conﬁrmed / total ) # of total cells Sample dimension & values useHoloLens 308 4 / 16 128 “ Age of users ” ⇒ { “ child ” , . . . } interview 106 3 / 13 36 “ Types of questions ” ⇒ { “ Creative ability ” , . . . } debate 136 4 / 16 40 “ Foreign vs . domestic ” ⇒ { “ foreign policy ” , . . . } textAd 134 5 / 10 32 “ Mentions keys ? ” ⇒ { “ yes ” , “ no ” } Table 2 . Results for four list topics ( for both BlueSky and control ) . phrased differently with different meanings were treated as duplicate if they are categorized by the same value of each dimension . For example , L 1 and L 2 mentioned above were duplicate because they were categorized by the same value of each of four dimensions , such as “ Age of user ” ⇒ “ Adults ” . interview produced 106 list items in treatment and control con - ditions , such as “What would you do if it would start raining ﬁsh ? ” , by 59 and 15 workers , respectively . Eight workers pro - duced 13 dimensions and three dimensions were conﬁrmed , such as “Types of questions” with 3 values { “Creative abil - ity” , “Personal experiences” , and “Professional experiences” } . The other two dimensions had 3 and 4 values , respectively , charting an idea space of 36 cells . debate produced 136 list items in treatment and control con - ditions ( after truncation ) , such as “Will you force all public washrooms to be inclusive to all genders , including transgen - dered persons ? ” , by 79 and 18 workers , respectively . Seven workers generated 16 dimensions and four dimensions were conﬁrmed , such as “foreign vs domestic issues” with 2 values { “foreign policy” , “domestic” } . The other three had 2 , 2 , and 5 values , respectively , charting an idea space of 40 cells . textAd produced 134 list items in treatment and control con - ditions , such as “How rich would you be if you had a dollar for every time you lost your keys ? ” , by 58 and 28 workers , respectively . Three workers produced 10 dimensions in which ﬁve were conﬁrmed , such as “Mentions keys ? ” with 2 values { “yes” , “no” } . The other four dimensions all had 2 values , charting an idea space of 32 cells . The size of the idea spaces is determined not by the topic , but by the chosen dimensions and values . Workers decide how many values to partition each dimension by . Requesters control which dimensions to accept . Together , they determine the number of cells into which the idea space is partitioned . Time and Cost Step 1 was paid $ 0 . 25 per HIT while step 3 was paid $ 0 . 30 per HIT . After the system stopped , the average cost per list item under the treatment condition was $ 0 . 56 , $ 0 . 58 , $ 0 . 53 , and $ 0 . 55 for useHoloLens , interview , debate and textAd , respectively . Correspondingly , for the control , the average cost per list item was $ 0 . 44 , $ 0 . 33 , $ 0 . 31 , and $ 0 . 39 . The control group was slightly cheaper than the treatment because each control ideation HIT got twice ideas as the treatment with the same amount of payment . Under the treatment condition , participants spent a combined total of 17 hours 9 minutes , 5 hours 31 minutes , 6 hours 49 minutes , and 7 hours 10 minutes for useHoloLens , interview , debate and textAd , respectively . Correspondingly , for the con - trol , the combined work time was 7 hours 37 minutes , 2 hours List topic Group Efﬁciency # covered # of entries use - HoloLens BlueSky 36 % = 111 ÷ 308 control 19 % = 59 ÷ 308 interview BlueSky 34 % = 36 ÷ 106 control 28 % = 30 ÷ 106 debate BlueSky 26 % = 36 ÷ 136 control 12 % = 16 ÷ 136 textAd BlueSky 20 % = 27 ÷ 134 control 15 % = 20 ÷ 134 Table 3 . Efﬁciency of BlueSky versus the control . Efﬁciency is deﬁned as the number of distinct ideas ( “ # covered” ) divided by the number of idea entries in the ideation stage ( “ # of entries” ) . 13 minutes , 2 hours 11 minutes , and 2 hours 38 minutes , re - spectively . The control group was faster than the treatment group because workers in the treatment spent more time in ideation to meet the given constraints . Due to the same reason , there were more unique workers in the treatment ( 73 on aver - age ) than in the control ( 25 on average ) where workers were able to add ideas quickly and continuously in multiple HITs . The average hourly rate for the BlueSky condition was $ 10 . 46 / hour . For the control , it was $ 18 . 07 / hour . Evaluation The goal of BlueSky is to produce a list of items that com - pletely and uniformly cover the entire idea space . To evaluate it , we ask and answer two key questions : 1 . How comprehensively is the idea space covered ? 2 . How uniformly is the idea space covered ? Comprehensiveness The systems stopped when the idea spaces of the treatment were covered by 111 , 36 , 36 and 27 cells for useHoloLens , interview , debate and textAd , respectively ( Table 3 ) . It indi - cates that the topic interview stopped after the idea space was completely covered , while others stopped after the idea space coverage remained the same for three iterations . We stopped the control once the treatment stopped because it would take lots more time to cover some uncommon cells in the idea space . For instance , useHoloLens had a cell with the combination between “User category ⇒ government” and “Age of user ⇒ child” . The treatment succeeded in covering it with creative list items ( e . g . , “ Children could use this to mimic a court room to learn about the judicial branch . ” ) while the control did not . Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 126 Figure 6 . Value density of four dimensions on the list topic debate . The height of each column segment indicates the proportion of the total list items that were categorized with the value shown . If perfect uniformity could be achieved , then all segments within a given column would have equal height . BlueSky ( left ) is more uniform than the control ( right ) . Correspondingly , in the end , the control groups had completely covered 59 , 30 , 16 , and 20 cells , respectively . We perform a two - way ANOVA to compare the coverage ( normalized by total cells ) between treatment and control ( p = 0 . 06 ) , which is ( barely ) not statistically signiﬁcant . To measure how well a method covers the space while avoiding duplicates ( same value for every dimension ) , the efﬁciency is calculated by the ratio of covered cells to the total list items ( Ta - ble 3 ) . Take useHoloLens as an example , the treatment had ef - ﬁciency 111 / 308 = 36 % while the control had 59 / 308 = 19 % . The average efﬁciency of BlueSky ( over four topics ) was 29 % , which was about 53 % higher than that of the control ( 19 % ) . Uniformity The uniformity of an idea space can be illustrated by the distribution of list items covering each cell . Some cells in the idea space were more clustered than others and thus had more duplicate list items . The most clustered cells had 14 , 8 , 14 and 20 list items for useHoloLens , interview , debate , and textAd , respectively , and 53 , 14 , 30 , and 30 for the four control groups . This indicates that the BlueSky reduced the heaviest duplication to about half of the control . To illustrate how the BlueSky method enhances uniformity , the density of list items with respect to individual dimensions can be analyzed ( Fig . 6 ) . Note that uniformity on each dimension does not guarantee uniformity across all cells . For the control , the dimension “question or statement” had a value “statement ( of topic ) ” with the smallest density . It indicates that only a few list items were categorized by this value , and implies that this value might be a “blind spot” in the freeform ideation . On the other hand , this dimension had a value “question” with the largest density , which implies the participants’ common thinking habits for the idea space . This situation is similar to other three list topics where BlueSky is more uniform than the control . By increasing uniformity , we could explore more uncommon ( often valuable ) list items as well as reduce the common ( often boring ) list items . Match Rate of Step 1 and Step 3 In the treatment , a list item is generated w . r . t a batch of con - straints and then categorized by one value of each dimension . Recall that each constraint ( c i ) is converted from a value ( v i ) of a dimension . If the categorization results ( e . g . , ( v 1 , v 2 , . . . , v n ) ) match all the constraints C = ( c 1 , c 2 , . . . , c n ) accordingly , this list item is called matched between step 1 and step 3 . Workers could get a bonus of $ 0 . 15 if their list item is matched . Figure 7 shows the number of matched list items over the number of total constrained list items ( i . e . , list items which have been given constraints ) . Interestingly , the number of matched ideas increased approximately linearly with the number of total constrained list items in most cases , except textAd . The rea - son might be that the difﬁculty of each cell was similar , and workers generated a similar number of ideas to fully meet the constraints of each cell . Note that the constrained list items were fewer than the total list items because some list items were generated with no con - straints ( phase 1 of step 1 ) . Besides , they exceeded the number of total cells in an idea space because some constraints ap - peared more than once in the whole experiment . As mentioned above , each batch of constraints represents an empty cell and appears once in each iteration . If the empty cell is not ﬁlled in one iteration , the corresponding constraints will be posted again in the next iteration . The constraints will not appear when participants manage to generate a list item to meet the constraints ( i . e . , matched ) or the empty cell is unintentionally ﬁlled by other list items . Apparently , match rate is essential to cover an idea space efﬁciently . Ideally , if every list item is matched , the efﬁciency would be 100 % . The actual match rate was about 20 % on average . There are several possible reasons that could cause the relatively low match rate : 1 . A list item was generated correctly ( i . e . , could meet all the constraints ) but was categorized by different values in step 3 . Since some dimensions have subjective values ( e . g . , “ Question answer length ” ⇒ { “ short ” , “ medium ” , “ long ” } for the list topic interview ) , the participants in step 1 may have different opinions of “ short ” from those in step 3 . This discrepancy could be mitigated , but not eliminated , by the multi - judgment in step 3 . 2 . A list item was generated incorrectly ( i . e . , could meet part or none of the constraints ) . It would either cover a non - empty cell ( thus duplicate ) or happen to cover an empty cell . It is not uncommon as some participants have difﬁculty thinking a list item to meet all the constraints . On average , 94 % of list items could meet at least one constraint among the given batch , and 63 . 5 % of list items could meet at least two constraints . Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 127 Figure 7 . The number of matched list items over the number of total list items which have been given constraints for list topics under the treatment condition . DISCUSSION BlueSky is a transformation from divergent to convergent process . The divergent process includes the freeform ideation with no constraints ( phase 1 of step 1 ) and the dimension - values synthesis ( step 2 ) . After all the dimensions and values are proposed , the convergent process leads the constrained ideation ( phase 2 of step 1 ) . Most prior crowd - powered ideation systems aimed to maxi - mize quality or creativity , or to ﬁnd one great idea . In contrast , we aim to cover the space completely and uniformly . Thus , comprehensiveness and uniformity are the metrics that we used to measure the success of BlueSky . The examples and con - straints provided in step 1 may affect the creativity , especially when the constraints are difﬁcult . There was no signiﬁcant difference of match rate between workers who chose to see the examples or not . The categorization task is also a common type of task on AMT . It is seen in other crowd workﬂows , such as Cascade [ 5 ] . Unlike Cascade , we only allow a list item to ﬁt in one value , not multiple values because values are orthogonal ( no overlap ) . If values are subjective or vague , it will depend on multi - judgment to ﬁnd the best ﬁt value for a list item . On average , 2 . 1 workers were needed to agree that a list item was categorized by one value ( of a dimension ) , in which about 85 % of categorizations were agreed by two workers and the rest were agreed by more than two workers . In step 1 , an option is to mark a batch of constraints as N / A , and less than 1 % of workers chose it ( then they would be asked to write down why the constraints were N / A ) . The low N / A rate is consistent with the fact that the entire space was ( almost ) completely covered and is also a sign of good dimensions . Similarly , in step 3 , we allow workers to categorize ideas as N / A if they do not ﬁt any of the values . Only about 1 % of list items were categorized as N / A by more than two workers ( multi - judgment ) . Most N / A list items were irrelevant to the list topic and / or made no sense ( e . g . , incomplete sentences ) . LIMITATIONS AND FUTURE WORK The total running time of the system is long because it is se - quential , not parallel . Participants have to wait until others ﬁnish the previous steps . If any participant spends an atypi - cally long time ( probably due to the confusion of instruction or holding the HIT but leaving ) , the whole system will be in idle . To speed up the system , a real - time ideation - mapping workﬂow might help ; namely , once a participant enters a new idea , another participant is notiﬁed to categorize it , and then new constraints will be given back . To that end , we might need to retain a group of workers . Moreover , the more constraints in one HIT , the harder to meet them all . If participants choose to ignore some of the constraints , the system will slow down while the time and cost will increase since more HITs are needed . To increase match rate , we might ﬁlter / rank workers based on their performance . For example , we could invite the workers who have high match rates while block the ones with low rates . Another challenge is efﬁcient discovery of dimensions . One potential avenue would be to use machine learning to extract keywords or propose potential dimensions to inspire partici - pants ( including requesters ) . We could also maintain a pool of dimensions for each type of list topics . Participants could reuse and iterate existing dimensions or add new ones . In this paper , we did not consider the hierarchy of dimensions . The values of a dimension might also be sub - dimensions . For example , “ location ” ⇒ { “ indoor ” , . . . } could be further bro - ken down into “ indoor ” ⇒ { “ living room ” , . . . } . It might be interesting to allow zoom - in or zoom - out of the idea space . CONCLUSION This paper presents a crowd - powered algorithm that could me - chanically enumerate a uniform and comprehensive sample of ideas via ideation - synthesis - mapping process . By testing with four list topics ( useHoloLens , interview , debate and textAd ) under the treatment ( BlueSky method ) and control conditions , we showed that the treatment could cover the entire space with 29 % efﬁciency while the control could do with 19 % . More - over , the treatment covered the idea space more uniformly than the control , and reduced the heaviest duplication to about half of the control . The match rate between ideation and mapping steps was about 20 % , suggesting that participants need to enter approximately ﬁve ideas to meet a given batch of constraints . ACKNOWLEDGMENTS We gratefully acknowledge John Boyer , Jordan Huffaker , and Meng - Han Wu for thoughtful feedback on the interface de - sign at several junctures , and to the 1087 workers who used the system , including many previous iterations of the system . In particular , Chris Tucci provided essential feedback and assistance with attracting workers to our strange tasks . Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 128 REFERENCES 1 . Howard B Abrams . 1992 . Originality and Creativity in Copyright Law . Law and Contemporary Problems 55 , 2 ( 1992 ) , 3 – 44 . 2 . Daren C Brabham . 2008 . Crowdsourcing as a model for problem solving an introduction and cases . Convergence : the international journal of research into new media technologies 14 , 1 ( 2008 ) , 75 – 90 . 3 . Joel Chan , Steven Dang , and Steven P Dow . 2016 . Comparing Different Sensemaking Approaches for Large - Scale Ideation . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , New York , 2717 – 2728 . 4 . Siddhartha Chaudhuri , Evangelos Kalogerakis , Stephen Giguere , and Thomas Funkhouser . 2013 . Attribit : Content Creation with Semantic Attributes . In Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology ( UIST ’13 ) . ACM , New York , NY , USA , 193 – 202 . DOI : http : / / dx . doi . org / 10 . 1145 / 2501988 . 2502008 5 . Lydia B Chilton , Greg Little , Darren Edge , Daniel S Weld , and James A Landay . 2013 . Cascade : Crowdsourcing taxonomy creation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , New York , 1999 – 2008 . 6 . Nelson Cowan . 2010 . The magical mystery four how is working memory capacity limited , and why ? Current directions in psychological science 19 , 1 ( 2010 ) , 51 – 57 . 7 . Nigel Cross . 2008 . Engineering design methods : strategies for product design . John Wiley & Sons , New York . 8 . Clive L Dym and Patrick Little . 2000 . Engineering design : A project based approach . ( 2000 ) . 9 . Michael J French and Design Council . 1985 . Conceptual design for engineers . Springer , Berlin . 10 . Ryan G Gomes , Peter Welinder , Andreas Krause , and Pietro Perona . 2011 . Crowdclustering . In Advances in neural information processing systems . MIT Press , Cambridge MA , 558 – 566 . 11 . Jeff Howe . 2008 . Crowdsourcing : How the power of the crowd is driving the future of business . Random House , New York . 12 . George Q Huang and Kai - Ling Mak . 1999 . Web - based morphological charts for concept design in collaborative product development . Journal of Intelligent Manufacturing 10 , 3 - 4 ( 1999 ) , 267 – 278 . 13 . Kwang - Sung Jun , Jerry Zhu , Burr Settles , and Timothy Rogers . 2013 . Learning from human - generated lists . In Proceedings of The 30th International Conference on Machine Learning . ICML , Madison WI , 181 – 189 . 14 . Aaron Michael Koblin . 2009 . The sheep market . In Proceedings of the seventh ACM conference on Creativity and cognition . ACM , New York , 451 – 452 . 15 . Kurt Luther , Nathan Hahn , Steven P Dow , and Aniket Kittur . 2015 . Crowdlines : Supporting Synthesis of Diverse Information Sources through Crowdsourced Outlines . In Third AAAI Conference on Human Computation and Crowdsourcing . AAAI , Palo Alto CA , 110 – 119 . 16 . George A Miller . 1956 . The magical number seven , plus or minus two : some limits on our capacity for processing information . Psychological review 63 , 2 ( 1956 ) , 81 . 17 . Gerhard Pahl and Wolfgang Beitz . 2013 . Engineering design : a systematic approach . Springer Science & Business Media , Berlin . 18 . Aditya Parameswaran , Ming Han Teh , Hector Garcia - Molina , and Jennifer Widom . 2013 . Datasift : An expressive and accurate crowd - powered search toolkit . In First AAAI Conference on Human Computation and Crowdsourcing . AAAI , Palo Alto CA , 112 – 120 . 19 . Ben Shneiderman . 2007 . Creativity Support Tools : Accelerating Discovery and Innovation . Commun . ACM 50 , 12 ( Dec . 2007 ) , 20 – 32 . DOI : http : / / dx . doi . org / 10 . 1145 / 1323688 . 1323689 20 . Gregory Smith . 2007 . Morphological charts : a systematic exploration of qualitative design space . Master’s thesis . Clemson University , Clemson , SC . 21 . Gregory Smith , Jenkins Richardson , Joshua D Summers , and Gregory M Mocko . 2012 . Concept exploration through morphological charts : an experimental study . Journal of mechanical design 134 , 5 ( 2012 ) , 051004 . 22 . Steven M . Smith , Thomas B . Ward , and Jay S . Schumacher . 1993 . Constraining effects of examples in a creative generation task . Memory & Cognition 21 , 6 ( Nov . 1993 ) , 837 – 845 . DOI : http : / / dx . doi . org / 10 . 3758 / BF03202751 23 . Jerry O . Talton , Daniel Gibson , Lingfeng Yang , Pat Hanrahan , and Vladlen Koltun . 2009 . Exploratory Modeling with Collaborative Design Spaces . ACM Trans . Graph . 28 , 5 , Article 167 ( Dec . 2009 ) , 10 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 1618452 . 1618513 24 . Beth Trushkowsky , Tim Kraska , Michael J Franklin , and Pradyut Sarkar . 2013 . Crowdsourced enumeration queries . In Data Engineering ( ICDE ) , 2013 IEEE 29th International Conference on . IEEE , Piscataway NJ , 673 – 684 . 25 . Karl T Ulrich . 2003 . Product design and development . Tata McGraw - Hill Education , Delhi . 26 . Lixiu Yu , Aniket Kittur , and Robert E . Kraut . 2014a . Distributed Analogical Idea Generation : Inventing with Crowds . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1245 – 1254 . DOI : http : / / dx . doi . org / 10 . 1145 / 2556288 . 2557371 27 . Lixiu Yu , Aniket Kittur , and Robert E . Kraut . 2014b . Searching for Analogical Ideas with Crowds . In Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 129 Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1225 – 1234 . DOI : http : / / dx . doi . org / 10 . 1145 / 2556288 . 2557378 28 . Lixiu Yu and Jeffrey V . Nickerson . 2011 . Cooks or cobblers ? : crowd creativity through combination . In Proceedings of the SIGCHI conference on human factors in computing systems . ACM , New York , 1393 – 1402 . http : / / dl . acm . org / citation . cfm ? id = 1979147 29 . Lixiu Yu and Yasuaki Sakamoto . 2011 . Feature Selection in Crowd Creativity . In Foundations of Augmented Cognition . Directing the Future of Adaptive Systems , Dylan D . Schmorrow and Cali M . Fidopiastis ( Eds . ) . Number 6780 in Lecture Notes in Computer Science . Springer , Berlin Heidelberg , 383 – 392 . http : / / link . springer . com / chapter / 10 . 1007 / 978 - 3 - 642 - 21852 - 1 _ 45 Session : Crowd Ideation C & C 2017 , June 27 – 30 , 2017 , Singapore 130