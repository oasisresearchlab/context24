Ambient Intelligence for Next - Generation AR Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova Abstract Next - generation augmented reality ( AR ) promises a high degree of context - awareness – a detailed knowledge of the environmental , user , social and system conditions in which an AR experience takes place . This will facilitate both the closer integration of the real and virtual worlds , and the provision of context - speciﬁc content or adaptations . However , environmental awareness in particular is challenging to achieve using AR devices alone ; not only are these mobile devices’ view of an environment spatially and temporally limited , but the data obtained by onboard sensors is frequently inaccurate and incomplete . This , combined with the fact that many aspects of core AR functionality and user experiences are impacted by properties of the real environment , motivates the use of ambient IoT devices , wireless sensors and actuators placed in the surrounding environment , for the measurement and optimization of environment properties . In this book chapter we categorize and examine the wide variety of ways in which these IoT sensors and actuators can support or enhance AR experiences , including quantitative insights and proof - of - concept systems that will inform the development of future solutions . We outline the challenges and opportunities associated with several important research directions which must be addressed to realize the full potential of next - generation AR . 1 Introduction While virtual content in the metaverse is designed to be immersive , it will not be experienced in isolation . In particular for augmented reality ( AR ) , in which virtual content is integrated with our real - world surroundings , an accurate and complete understanding of the real environment is a prerequisite for high quality experiences . Obtaining this using AR devices alone is infeasible in many scenarios , raising the potential for employing external sensors placed in the surrounding environment , a form of ambient intelligence for AR . As well as sensing the properties of an environment , it is also desirable to control them , for example to optimize the performance of core AR algorithms , or to generate stimuli in sensory modalities that are beyond the capabilities of AR devices . In this book chapter we explore the potential for wireless Internet of Things ( IoT ) devices to provide this type of ambient intelligence , and thereby support next - generation AR experiences in the metaverse . More well - studied than AR is virtual reality ( VR ) , in which users are immersed in a fully virtual environment through visual and auditory stimuli , usually delivered via a headset ( e . g . , Meta Quest 2 , HTC Vive ) , or alternatively a desktop or mobile device . To provide tactile feedback and interaction methods to the user , VR headsets are frequently used in combination with specialized controllers or other handheld devices , and less commonly with other peripherals such as gloves , body suits , helmets [ 76 ] and mouth accessories [ 186 ] . There is work on integrating external devices into VR experiences to further increase the levels of immersion , beyond the capabilities of wearable devices . For example , fans have been used for representations of wind [ 43 , 62 ] , while multiple works ( e . g . , [ 170 ] ) have used ‘climate chambers’ with HVAC systems to study thermal perception Tim Scargill Electrical and Computer Engineering Department , Duke University , Durham , NC , USA e - mail : ts352 @ duke . edu Sangjun Eom Electrical and Computer Engineering Department , Duke University , Durham , NC , USA , e - mail : sangjun . eom @ duke . edu Ying Chen Electrical and Computer Engineering Department , Duke University , Durham , NC , USA , e - mail : ying . chen151 @ duke . edu Maria Gorlatova Electrical and Computer Engineering Department , Duke University , Durham , NC , USA , e - mail : maria . gorlatova @ duke . edu 1 a r X i v : 2303 . 12968v2 [ c s . H C ] 24 M a r 2023 2 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova in immersive virtual environments . External devices have also been employed to capture contextual data from users or the ambient environment to enhance VR experiences ( e . g . , [ 109 , 161 ] ) . Here we focus on the less - studied topic of ambient intelligence to support or enhance AR , in which virtual content is overlaid onto and integrated with a user’s real - world surroundings . Speciﬁcally , we examine this in the context of mobile AR , when virtual content is presented through handheld devices such as a smartphone or tablet , or wearable devices such as a headset . As in VR , current AR devices deliver virtual content primarily in the form of visual and auditory stimuli , with limited tactile feedback sometimes available in the case of handheld devices or headsets with handheld controllers ( e . g . , the Magic Leap 2 ) . This again raises the possibility of using external devices to adjust the real environment in other sensory modalities or to extend possible interaction methods . However , there is also an important distinction between two methods of delivering visual content in AR ; handheld devices and some headsets ( e . g . , the Varjo XR - 3 ) use video see - through ( VST , sometimes referred to as video pass - through ) displays , while other headsets ( e . g . , the Microsoft HoloLens 2 and the Magic Leap 2 ) use optical see - through ( OST ) HMDs . These diﬀerent designs have important implications for how the properties of real environments aﬀect a user’s perception of virtual visual content . In general , a knowledge of how the real environment aﬀects both system functionality and human perception , along with the ability to sense and control environment properties , will enable the provision of optimal AR experiences in diverse scenarios . Further motivation for pursuing ambient intelligence comes from the nature of next - generation AR . The coming years hold the promise of virtual content that is not only more closely integrated with our real surroundings , but which also adapts to the current context in which it is presented . This context - awareness is key to realizing the full potential of AR ; to deliver virtual content that provides a high - quality user experience , and enables optimal task performance , we require information on the speciﬁc circumstances in which it is presented [ 69 ] . At a high level , we can break down this contextual information into environmental , user , social , and system awareness . Environmental awareness , including the environment understanding required for close integration of real and virtual content , is particularly challenging to obtain using the sensors onboard mobile AR devices alone because their view of an environment is spatially and temporally limited ; they typically only capture a small portion of an environment for a short period of time . Furthermore , due to restrictions on the quality of onboard sensors , and the fact that they are frequently in motion , the data they capture is often inaccurate . External devices on the other hand can help to address these deﬁciencies and generate more accurate , more complete environmental awareness , across a wider range of conditions . The desire to both improve the accuracy and completeness of environmental awareness in AR , and control environment properties , motivates the use of Internet of Things ( IoT ) devices . These wirelessly connected devices have become ubiquitous across diverse settings ( globally there were 11 billion active IoT devices at the end of 2021 , and this is forecast to be 29 billion by 2030 [ 137 ] ) and include a wide range of sensors and actuators that are suitable for detecting and adjusting environmental conditions . The widespread availability and relatively low cost of IoT sensors like smart cameras and IoT actuators like smart lights and displays , as well as the prevalence of supporting infrastructure ( e . g . , Wi - Fi networks ) , means that they can be readily leveraged for AR . Our overall vision , illustrated in Figure 1 , is for multi - device AR architectures , in which experiences on mobile AR devices are enhanced or supported by a set of connected devices . Contextual data are provided by IoT sensors such as smart cameras and wearables , while IoT actuators such as smart lights , shades , and displays optimize environmental conditions for AR , or enhance AR experiences by providing additional stimuli . An edge server or the cloud provides the storage and resources to aggregate these data , compute context , and control IoT devices . Fig . 1 : A high - level view of a multi - device next - generation AR architecture , in which data from mobile AR devices , wearables , and IoT sensors like smart cameras are aggregated to compute context ( e . g . , environment properties ) on an edge or cloud server . These data are also used to inform the control of IoT actuators such as smart lights , shades , and displays , which optimize environmental conditions for AR experiences . Ambient Intelligence for Next - Generation AR 3 We deﬁne the scope of the IoT devices we cover in this book chapter through two important distinctions . Firstly , our focus is on IoT devices that support or enhance AR , rather than simply any IoT device whose data could be displayed in AR or be controlled through an AR interface . For example , while there are interesting and useful ways in which devices such as air quality sensors and robotic arms could be combined with AR , they are excluded here . Examples of works which proposed the use of AR as a tool for visualizing IoT - generated data and interacting with IoT nodes include [ 84 , 127 , 151 , 211 ] . Secondly , we only consider truly external or ambient IoT devices ; we exclude sensors that are attached to AR devices , such as inertial sensors or eye tracking cameras , as well as wearable sensors that may capture additional biometrics from humans in an AR environment . For a recent review of wearable sensors for AR applications , see [ 92 ] . Given the placement of ambient IoT devices in the wider environment they are particularly beneﬁcial for environment awareness , though we note cases in which they can supply other types of context - awareness ; for example , while user context data is most often captured through on - device or wearable sensors , ambient IoT cameras can also capture visual data pertinent to activity or emotion recognition [ 175 ] . The contents of this book chapter are as follows . In Section 2 , we cover related work on combining AR and IoT devices , methods of communication between them , and a network architecture that will support the implementation of ambient intelligence for AR , edge computing . In Section 3 , we categorize the diﬀerent ways in which ambient IoT devices can support or enhance AR experiences , including relevant sensors and actuators for each use case ; then in Sections 4 and Sections 5 , we discuss in more detail the possibilities for IoT sensors and actuators respectively , organized by use case . In Section 6 we cover open challenges and research directions , and in Section 7 we provide a conclusion . 2 Related Work In this section we review existing work on systems which incorporate both AR and IoT devices , diﬀerent methods of commu - nication that have been used to connect AR and the IoT , and a network architecture that will support the implementation of ambient intelligence for next - generation AR . Systems incorporating AR and IoT devices : AR can be used to enhance interactions with IoT devices , including both the visualization of IoT data and the provision of an immersive interface to access and control IoT devices . For example , in [ 154 ] the authors described an AR - IoT system that displays real - time IoT data as holographic content to enhance object interactions ; this system is applied to crop monitoring , to provide object coordinates of plants and information collected from IoT devices such as the fertilizers used . Similarly , Sun et al . [ 190 ] presented MagicHand , an AR system that allows users to interact with IoT devices by detecting , localizing , and enabling augmented hand controls ; the system was implemented using a 2D convolutional neural network ( CNN ) and achieved high gesture recognition accuracy . In [ 191 ] IoT sensor data were overlaid onto industrial machines using AR , with more accurate pose estimates ( and thereby better aligned overlays ) obtained by applying deep learning to RGB and depth images of the machine . Visualizing and identifying IoT objects using an AR interface has also been shown to improve shopping experiences , by increasing perceived usability and satisfaction in user interactions [ 85 ] . Communication between AR and IoT devices : Prior works have demonstrated how AR devices can communicate with IoT devices through the Internet ; examples include an AR application that displays agricultural data from temperature or moisture sensors for crop monitoring [ 115 ] , and a web - based AR application framework to visualize the state changes of a coﬀeemaker through a visual tag [ 104 ] . Others have highlighted the importance of scalable systems with eﬃcient data management for AR applications [ 31 , 158 ] ; to this end the authors of [ 211 ] proposed an AR - based browsing architecture that identiﬁes new IoT devices and enables immediate interactions with easily controllable user interfaces . Similarly , Blanco - Novoa et al . proposed a framework that allows AR and IoT devices to communicate dynamically in real time through standard and open - source protocols such as MQTT , HTTPS , or Node - RED [ 24 ] . Developing systems which incorporate AR devices into a network of IoT devices remains an important topic of research ; for example , VisIoT [ 151 ] supports tracking and visualizing the location of IoT nodes in real time through a combination of data collected from camera , inertial measurement unit ( IMU ) , and radio frequency ( RF ) modules , while Scenariot [ 80 ] integrates the discovery and localization of IoT devices into an AR system by embedding RF modules into both AR and IoT devices . These works highlight the need for further research on device localization and calibration , as well as system scalability ( with respect to e . g . , bandwidth consumption ) , to inform the development of IoT - supported AR systems . Edge computing for AR : In order to leverage the large amounts of data from AR and ambient IoT devices , context - aware AR systems will require storage and computational resources beyond the constraints of these devices alone . Given the low latency requirements of many aspects of context - aware AR , along with the privacy concerns associated with transferring sensitive information about users or environments to the cloud , many have deemed edge computing a particularly promising network 4 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova architecture [ 70 , 114 , 201 , 217 , 223 ] . In this architecture , a server is placed physically close to mobile AR devices and ambient IoT devices ( e . g . , in the same building ) , helping to address the aforementioned latency and privacy requirements . Existing work has already demonstrated the beneﬁts of oﬄoading tasks to the edge for various aspects of AR system functionality , including elements of SLAM pipelines [ 5 , 204 , 207 ] , lighting estimation [ 223 ] and object detection [ 74 , 112 , 162 ] , and in our ongoing work we have developed multiple edge architectures for context - aware AR . For example , in [ 178 ] we developed a system to predict the quality of virtual content positioning ( a function of AR device pose estimate error ) from environment properties , in which we transmitted data collected on the AR device to the edge for the computationally expensive pre - processing and model inference . In [ 114 ] we presented an edge - assisted collaborative image recognition system , in [ 179 ] we demonstrated an edge - supported AR application that analyzed user eye movements to recognize common activities in a museum scenario , and we have developed multiple systems that provide edge - based provisioning of contextual virtual content [ 63 ] . We see the incorporation of IoT devices that provide additional contextual data as a natural extension to these edge architectures for context - aware AR , and we recently presented an example of this in [ 177 ] ( see Section 5 . 1 for further details ) . 3 Ambient IoT for AR In this section , we categorize the diﬀerent ways in which ambient IoT devices can support or enhance AR experiences ( Section 3 . 1 ) . We then provide an overview of the diﬀerent types of IoT devices that may be employed , along with their associated uses , and examples of their use for AR or VR ( Section 3 . 2 ) . 3 . 1 Uses of Ambient IoT for AR Central to next - generation AR is the incorporation of context - awareness – adapting virtual content according to the environ - mental , user , social , and system context in which it is presented . Ambient IoT sensors are able to collect environment data that is more accurate and complete than AR devices alone , while ambient IoT actuators can be used to adjust environment properties for better system performance or a higher quality user experience . In order to categorize the ways in which detecting and adjusting environment properties using IoT devices can support or enhance AR , here we deﬁne ﬁve high - level aspects of AR which contribute to the quality of a user’s experience : • Spatial understanding concerns information about the physical properties of the environment , which includes representations of the real - world surfaces present , the detection of ﬁducial or image - based markers , and real - time pose estimates for the AR device ; this information is required for accurate spatial registration of virtual content . • Semantic understanding takes this environmental awareness a step further , and provides a knowledge of the type , poses , and states of objects and surfaces present , which may be used to inform spatial understanding or display context - aware content . • Contextualized content refers to the ways in which either the spatial or semantic understanding obtained through IoT devices , or environment properties such as light and visual texture directly , may be used to inform adaptations to virtual content . • Interaction covers how current interaction methods in AR ( e . g . , hand gestures , eye tracking ) may be enhanced or extended using IoT devices . • Immersion relates to how IoT actuators can be used to increase an AR user’s sense of immersion ( i . e . , the sense that virtual content is truly present in their real environment ) . 3 . 2 Ambient IoT Devices for AR In Table 1 , we present an overview of ambient IoT devices that may be used to support or enhance AR experiences . We group included devices by the type of information they collect from or convey to AR devices or users ( e . g . , visual , auditory ) , rather than the underlying functionality of the device . For example , many motion sensors detect changes in thermal energy , but given that in this case they provide information regarding the state of the visual environment ( i . e . , human presence ) , we classify them here under ‘visual’ . Similarly , strain gauges , which detect changes in electrical resistance , are classiﬁed as visual because they provide information about the deformation of an object , eﬀectively enhancing the visual perception and acuity of an AR user . For each type of information , we divide IoT devices into sensors or actuators , list example devices , state their possible uses from the categories we deﬁned in Section 3 . 1 , and provide examples of related work for these use cases . Ambient Intelligence for Next - Generation AR 5 Table 1 : Overview of ambient IoT devices ( sensors and actuators ) with potential uses for AR . Sensors and actuators are grouped by the type of information they collect from or convey to AR devices or users ( e . g . , visual , auditory ) . Type of informationprovided Sensors Actuators Devices Uses Examples of use for AR or VR Devices Uses Examples of use for AR or VR Visual Cameras , depth sensors , motion sensors , light sensors , strain gauges , pressure sensors Spatial understanding , semantic understanding , contextualized content , interaction [ 111 , 123 , 132 , 167 , 178 , 210 ] Light bulbs , projectors , electronic displays ( e . g . , LCD , E - Ink ) Spatial understanding , semantic understanding , interaction , immersion [ 4 , 177 ] Auditory Microphones Semantic understanding , contextualized content , interaction [ 60 , 136 ] Speakers Immersion [ 101 ] Haptic Wind sensors , physical buttons and proxies , touchscreens Contextualized content , interaction [ 21 , 44 , 94 ] Haptic surfaces , fans Interaction , immersion [ 43 , 62 ] Olfactory Nanomechnicalsensors Semantic understanding - Diﬀusers , olfactometers Immersion [ 16 ] Thermal Thermocouples , resistancetemperature detectors , infrared temperaturesensors Semantic understanding , contextualized content [ 25 , 140 , 154 ] Heaters , HVAC systems Immersion [ 43 , 170 ] Considering ﬁrst IoT sensors , there is a wide variety of useful information we can collect from the surrounding environment , and we identify four key ways in which these data may be leveraged . Firstly , we can enhance the spatial understanding , semantic understanding , and interaction capabilities of AR systems by sensing from additional and more advantageous poses . Secondly , with suﬃcient data on how environmental conditions impact AR systems , we can predict the current level of performance and overall quality of an AR experience . Thirdly , we can use the data collected from sensors to adapt AR system functionality or the presentation of virtual content for the current environment . Finally , we can contextualize virtual content , in that it is related to or reacts to current conditions . Throughout Section 4 , we discuss in more detail how IoT sensors can be used to support or enhance AR . Beyond detecting the properties of the real environment using IoT sensors , we can also adjust the properties of the real environment using IoT actuators . The motivation for this is threefold . Firstly , because key aspects of AR system performance and user perception are aﬀected by environment properties , we can optimize environments to achieve the best possible performance . For example , the accuracy of spatial understanding is dependent on suﬃcient levels of light and visual texture , light levels aﬀect the performance of algorithms related to semantic understanding and interaction , and human perception of virtual content is also aﬀected by the properties of both light and textures . Secondly , we can improve the quality of a user’s interactions in AR , by providing alternative interaction methods ( e . g . , electronic displays ) or enhancing existing ones ( e . g . , adding tactile feedback using haptic surfaces ) . Thirdly , we can enhance a user’s sense of immersion in an AR experience . This may involve the generation of visual and auditory stimuli to extend what is possible on AR devices ( using e . g . , light projectors or speakers ) , or the generation of sensory stimuli in modalities that cannot be generated on AR devices ( using e . g . , fans , diﬀusers , or heaters ) . We discuss the possibilities for ambient IoT actuators in more detail in Section 5 . 4 IoT - based Sensing for AR In this section we discuss in more detail how ambient IoT sensors could be used to support or enhance AR experiences . Each subsection examines a diﬀerent use which we deﬁned in Section 3 . 1 and listed in Table 1 ; for sensors we cover spatial understanding ( Section 4 . 1 ) , semantic understanding ( Section 4 . 2 ) , contextualized content ( Section 4 . 3 ) , and interaction ( Section 4 . 4 ) . 6 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 4 . 1 Spatial Understanding A fundamental component of all AR systems which position virtual content relative to the real world is spatial understanding . Indeed an accurate and detailed knowledge of our physical surroundings is essential for next - generation AR systems which aim to closely integrate the real and virtual worlds . In this subsection , we ﬁrst provide background information on the techniques behind spatial understanding in AR . We then examine how environment properties aﬀect spatial understanding , and hence why obtaining knowledge of these properties through IoT sensors is useful , before exploring how IoT sensors may be used directly in spatial understanding algorithms . 4 . 1 . 1 Background on Spatial Understanding in AR Understanding one’s physical surroundings is a fundamental component of both marker - based and markerless AR , in order to accurately overlay virtual content onto a view of the real - world environment . A marker - based AR system uses a marker , commonly a paper - printed static image with distinct features , to obtain information about the position and orientation of an object in the surrounding space . On the other hand , a markerless AR system is based on the use of simultaneous localization and mapping ( SLAM ) to understand the surrounding space without the use of markers . Marker - based AR : Marker detection based on image processing has been a popular approach to enable marker - based applications in AR due to its ease of use and accurate tracking of an object [ 59 ] . A multitude of markers with diﬀerent patterns and features has been used in marker - based AR systems , including binary ﬁducial markers ( e . g . , ARToolkit [ 17 ] , ArUco [ 18 ] , ARTag [ 51 ] ) , a variation of ﬁducial markers specialized for robotics applications ( e . g . , AprilTag [ 146 ] ) , and image - based markers with a high number of unique feature points ( e . g . , Vuforia [ 200 ] ) . The detection of these markers is based on the processing of the image frames captured by the camera on an AR device – the pose of the device is estimated by ﬁnding contours , features , or lines [ 75 ] within the marker . However , the accuracy and reliability of marker detection in AR are largely determined by the performance of the camera capturing images of the scene and the environmental conditions of the scene where the marker is located . Poor camera calibration or focus , or low image resolution , can potentially result in low pose estimation accuracy of the marker in the scene [ 218 ] . Additionally , environmental properties such as lighting or the distance from the camera to the marker are other factors that can aﬀect pose estimation accuracy . We discuss the use of IoT sensors and actuators to address challenges related to marker detection in Section 4 . 2 . 3 for the use of IoT sensors in object state detection , and in Section 5 . 1 . 1 for environment optimization using IoT actuators . Markerless AR : SLAM is a key enabling technology for markerless AR . Visual and visual - inertial SLAM ( V - SLAM and VI - SLAM ) , using cameras either alone or in combination with inertial sensors , have demonstrated remarkable progress over the last three decades [ 27 ] . Due to the aﬀordability of cameras and the richness of information provided by them , V - SLAM using monocular [ 138 , 168 ] , RGB - D [ 138 ] , and stereo [ 138 ] cameras has been widely studied . To provide robustness to textureless areas , motion blur , illumination changes , there is a growing trend of employing VI - SLAM , that assists cameras with an IMU [ 28 , 159 , 203 ] ; VI - SLAM has become the de - facto standard SLAM method for modern augmented reality platforms [ 13 , 65 ] . In VI - SLAM , visual information is fused with IMU data to achieve more accurate and robust localization and mapping performance [ 28 , 159 , 203 ] . Due to the high computational demands incurred by V - and VI - SLAM on mobile devices , oﬄoading parts of the workload to edge servers has recently emerged as a promising solution for lessening the loads on mobile devices and improving overall performance [ 5 , 33 , 207 , 208 ] . A standard approach [ 5 , 33 , 208 ] is to oﬄoad computationally expensive tasks , such as place recognition , the process of taking a sensor snapshot ( e . g . , an image ) of a location and querying it in a large , geotagged database gathered from prior measurements , and loop closings , the process of determining whether AR users are revisiting the same place . Both the aforementioned papers [ 5 , 28 , 33 , 159 , 207 , 208 ] and commercial AR devices that employ markerless AR ( e . g . , Android devices with ARCore [ 65 ] , iOS devices with ARKit [ 13 ] , or headsets such as the Microsoft HoloLens 2 [ 129 ] ) implement VI - SLAM using sensor data captured onboard mobile devices , and these data are both spatially and temporally limited . To address this limitation , in Section 4 . 1 . 3 we discuss methods to increase the accuracy and completeness of spatial understanding by integrating the sensor data obtained onboard AR devices with data obtained from IoT sensors . 4 . 1 . 2 Estimating the Quality of Spatial Understanding Using IoT Sensors Because of the role of vision - based sensing in spatial understanding in AR , and the nature of VI - SLAM algorithms in particular , the properties of the visual environment impact the accuracy and completeness of spatial understanding . Therefore a thorough Ambient Intelligence for Next - Generation AR 7 knowledge of those properties , obtained through ambient IoT sensors , is highly useful in identifying problematic regions for tracking , estimating current levels of spatial understanding , and informing system adaptations for current environmental con - ditions . Until recently , knowledge of the impact of environmental properties was limited to qualitative guidelines ; however , our recent work [ 175 , 176 ] provides quantitative insights on the impact of both light and visual texture on VI - SLAM performance . To obtain these quantitative insights on the impact of environmental conditions on VI - SLAM performance , we developed and implemented two methodologies . In one [ 176 ] , we measure the pose estimate error of open - source VI - SLAM algorithms ( e . g . , ORB - SLAM3 [ 28 ] ) using a game engine - based emulator ; we use the trajectories from existing VI - SLAM datasets ( e . g . TUM VI [ 181 ] , SenseTime [ 83 ] ) to create new camera images in realistic virtual environments , and combine that with the original inertial data from those datasets . In the other [ 174 ] , we measure virtual object position error ( determined by pose estimate error ) on commercial AR platforms ( e . g . , ARCore [ 65 ] , ARKit [ 13 ] ) , by aligning virtual objects with a real world reference point using our open - source AR app . Our game engine - based methodology facilitates ﬁne - grained control of environment properties ( i . e . , the exact properties of the light sources and textures in a virtual environment ) , while our method - ology for commercial AR supports monitoring of environment conditions with either AR device sensors or ambient IoT sensors . Light : Illuminance , the amount of light incident on environment surfaces per unit area , determines the accuracy with which environment surfaces can be mapped or tracked , because it determines the extent to which visual features are detectable for tracking . We illustrate an example of this in Figure 2 ; in these experiments we used our game engine - based emulator to run two SenseTime [ 83 ] trajectories in a 6m × 6m × 4m virtual concrete room , with 10 diﬀerent overhead light intensities [ 176 ] , and 10 trials per light setting . We then plotted VI - SLAM pose estimation performance ( in terms of relative error , the translational component of relative pose error ) against the 10 light intensity settings . Figure 2a shows the results for SenseTime A1 , a trajectory involving slow side - to - side motion facing a wall ( as if inspecting a virtual object at head height ) , followed by repeated walking away and returning with the camera angled more towards the ﬂoor ( described in [ 83 ] as ‘inspect + patrol’ ) . Figure 2b shows the results for SenseTime A4 , with slow motion focused on a small area of the ﬂoor , followed by the same slow side - to - side motion facing the wall ( described as ‘aiming + inspect’ ) . For SenseTime A1 , optimal performance is obtained at a medium light intensity ( 750 lumens ) , at which there is suﬃcient light to ensure recognizable features are visible , but those features are not obscured by specular reﬂections – this is particularly a factor during the ‘walking away and returning’ portion of the sequence . For the less challenging inertial data in SenseTime A4 , performance is largely determined by the illuminance of the small area of ﬂoor at the start of the sequence , and specular reﬂections are not a major factor ; as such performance is poor at low light levels when the small area of ﬂoor is too dark , and optimal performance is obtained at the highest light intensity . These results illustrate that optimal environment illuminance diﬀers depending on which regions of an environment user trajectories cover , and that monitoring of illuminance in speciﬁc environment regions of interest will be informative for the estimation of current VI - SLAM performance . ( a ) SenseTime A1 ( b ) SenseTime A4 Fig . 2 : VI - SLAM pose estimation error for two SenseTime [ 83 ] trajectories in a 6m × 6m × 4m virtual concrete room [ 176 ] , showing the translational component of relative error when diﬀerent light intensities are emitted by a light source ( 100 trials , 10 at each light intensity ) . Optimal light intensity for VI - SLAM performance depends on the structure , textures , and reﬂectance properties of an environment , as well as the camera trajectory . While a medium light intensity will be optimal when specular reﬂections due to high illuminance are a factor ( e . g . , Figure 2a ) , trajectories with views of environment regions where light sources are distant or occluded will result in optimal performance at high light intensities ( e . g . , Figure 2b ) . 8 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova We envision multiple IoT ambient light sensors being employed to accurately measure lighting properties in diﬀerent parts of the environment , without requiring an AR device . This will enable both the proactive identiﬁcation of environment regions where lighting may cause tracking errors , and a more complete understanding of how lighting conditions change over time . Data on the position of virtual content or user trajectories will inform the most relevant positions for ambient light sensors to be placed . The output from these sensors is also highly useful for environment optimization systems which control properties such as illuminance based on occupant needs , as we show in Section 5 . 1 . Visual texture : Given the reliance of feature - based SLAM on recognizable visual textures in the surrounding environment , estimating the properties of visual textures present is also a valuable predictor of device tracking performance in markerless AR . In quantitative experiments on a state - of - the - art open - source VI - SLAM algorithm ( ORB - SLAM3 [ 28 ] ) using our game engine - based emulator , we showed that both the edge strength and complexity of a texture , as well as how a texture is impacted by motion blur , aﬀect pose estimate error magnitude [ 176 ] . For example , Figure 3 shows the VI - SLAM pose estimation performance ( in terms of relative error , the translational component of relative pose error , and robustness , the mean percentage of tracked frames ) we obtained when running the ‘room5’ trajectory from the TUM VI dataset [ 181 ] in 6m × 6m × 4m cuboid environments covered with diﬀerent visual textures . The textures are ordered according to their edge strength ( variance of the Laplacian ) , with higher numbers indicating higher edge strength values . Three out of four of the textures with high edge strength , stone ( R7 ) , plants wallpaper ( R8 ) , and brick ( R9 ) resulted in median relative error ≤ 5cm , compared to > 20cm for all other textures . The notable exception was the speckled marble texture ( R10 ) , with a median relative error of 95cm . In this case , the ﬁne texture was greatly aﬀected by motion blur , resulting in less recognizable texture ( low edge strength ) in camera images from dynamic portions of the trajectory . Fig . 3 : VI - SLAM pose estimate performance for TUM VI room5 [ 181 ] with various visual textures , ordered by the edge strength of the texture . The interior of a 6m × 6m × 4m virtual room was covered in each texture , and 100 trials were conducted for each ( 10 at each of 10 diﬀerent light levels , light levels shown in Figure 2 ) . Performance was measured using relative error , the translational component of relative pose error , and robustness , the mean percentage of tracked frames over all trials . In our experiments on commercial AR platforms we have demonstrated that in terms of virtual object position error ( a function of pose estimate error magnitude ) , some visual textures are more robust to low illuminance than others [ 177 ] . In these experiments we measured virtual object position error , the 3D Euclidean distance between where a virtual object was originally placed and where it appears after walking away approximately 7m and returning , in diﬀerent environmental conditions using Ambient Intelligence for Next - Generation AR 9 our open - source app [ 174 ] . We conducted our experiments in a university lab , and tested two textures where the virtual object was placed , a checkerboard and an academic paper , at three ambient light levels ( low , 50 - 100 lux ; medium , 150 - 450 lux ; high , 500 - 1000 lux ) , with 10 trials for each of the six settings . Figure 4 shows our results for these experiments on the Samsung Galaxy Note 10 + smartphone ( ARCore v1 . 28 ) . Our results illustrate how error increases at lower ambient light levels , but that the checkerboard texture was more robust to this eﬀect than the academic paper ( mean errors of 4 . 1cm and 12 . 0cm respectively at the medium light level ) . We observed that at the medium light level , noise in smartphone camera images has minimal eﬀect on the checkerboard texture , but obscures the ﬁner texture of the academic paper , making VI - SLAM - based place recognition more challenging , and resulting in greater error . Fig . 4 : Virtual object position error on the Samsung Galaxy Note 10 + smartphone after walking away approximately 7m and returning , when the virtual object was placed on a checkerboard or academic paper texture , with a low ( 50 – 100 lux ) , medium ( 150 – 450 lux ) or high ( 500 – 1000 lux ) ambient light level . The ﬁne texture of the academic paper is less robust to lower light levels due to it being more easily obscured by noise in the AR device camera image [ 177 ] . These relationships between tracking quality and visual texture motivate the proactive monitoring of texture properties ; images of the environment periodically captured by IoT cameras can be transmitted to an edge server for the required image processing . Again , not only does this allow us to identify and manually address problematic regions that might cause poor tracking quality before they are encountered by AR devices , but we can automatically optimize illuminance or texture based on the current types of visual texture present ( see Section 5 . 1 . 2 ) . Another promising direction is implementing a form of adaptive SLAM by adjusting feature extraction parameters based on visual texture ; for example , it may be beneﬁcial to lower corner detection thresholds when low - contrast textures are detected . 4 . 1 . 3 Enhancing Spatial Understanding Using IoT Sensors As well as measuring environment conditions to estimate current levels of spatial understanding , we can also use the data from ambient IoT sensors as input to spatial understanding algorithms directly . There are several promising possibilities in this area , which we discuss below . Collaborative SLAM : The visual data obtained from cameras onboard AR devices is both spatially and temporally limited , and frequently suﬀers from distortions such as noise or motion blur . To help alleviate some of these issues , we can additionally employ the visual data from ambient IoT cameras . This use of multiple vantage points can be seen as a type of collaborative SLAM , where captures of diﬀerent devices can be combined to obtain a higher - quality overall map , that leads to higher - quality pose estimation by the AR device [ 89 , 180 , 208 ] . To further extend the sensing capabilities using external sensors , in our planned work we will use IoT sensors ( e . g . , a surveillance camera ) located in the vicinity of AR devices to provide extra information for pose estimation in SLAM . Speciﬁcally , we will use deep learning - based object detection and person re - identiﬁcation for obtaining semantic information , that is , localizing and tracking users wearing mobile AR devices in the surveillance camera 10 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova frames . We will fuse the semantic information obtained from the surveillance camera with VI - SLAM running onboard AR devices to achieve more robust and accurate pose estimation . Collaborative depth mapping : In addition to VI - SLAM , modern AR devices are increasingly relying on time - of - ﬂight depth sensors to aid with spatial understanding , and sensing from multiple vantage points can also be advantageous for obtaining accurate depth maps . Time - of - ﬂight sensors struggle to obtain reliable data when the observed scene contains materials with low reﬂectivity , strongly specular objects , and reﬂections from multiple objects [ 73 , 195 ] . Not only that , but indirect time - of - ﬂight depth sensors such as those on the Microsoft HoloLens 2 , Magic Leap 2 and some high - end Android smartphones are limited in range , while the direct time - of - ﬂight depth sensors ( marketed as LiDAR ) on high - end iOS devices produce sparse readings that may be inaccurate at shorter distances . This leads to depth maps that are missing valid estimates in large parts of a frame , and incomplete spatial understanding . In our evaluation of depth estimates obtained by a Microsoft HoloLens 2 ( in the long throw mode ) across a range of representative indoor environments , we found that on average 30 % of depth pixels in a frame were missing [ 220 ] . We also collected an indoor dataset of 18 . 6K depth maps on a Samsung Galaxy Note 10 + smartphone , of which 58 % had greater than 40 % missing pixels [ 220 ] . Based on the properties of these missing pixels , we determined that the range of indirect time - of - ﬂight depth sensors on current AR devices is approximately 5m . We also observed that smaller angles ( 60 ◦ or less ) between the sensor’s optical axis and the target surface resulted in large numbers of missing pixels . These problematic conditions are prevalent in AR scenarios – in larger rooms a distance of greater than 5m between an AR device and the nearest surface is common , while a depth sensor naturally faces outward toward a wall ( due to how a smartphone is usually held or a headset is worn ) , but target surfaces in AR are often horizontal planes such as a table . Ambient IoT depth sensors can help to increase the completeness of raw depth maps by capturing data from poses that are not accessible to or normally covered by AR devices . To address limitations in sensor range , ambient depth sensors can be positioned closer to surfaces that are only observed from large distances by AR devices . Horizontal planes , frequently incom - plete due to their similar angle of orientation to the optical axis of AR device sensors , can be captured by downward - facing IoT depth sensors . Even some challenging reﬂections may be avoided from diﬀerent viewpoints . We envision this collaborative sensing approach being combined with existing techniques for depth map completion such as [ 126 , 160 , 219 , 220 ] , with the more complete depth data from multiple sensors combined on the edge server for a less challenging depth inpainting task . Scene change detection : Ambient IoT cameras can also be used to establish whether the environment has changed between diﬀerent AR sessions , to trigger SLAM remapping as required to improve the quality of spatial scene understanding ( and conversely to avoid unnecessary time - and resource - consuming remapping if the environment did not change ) . Scene change detection based on stationary cameras’ inputs is a long - examined , well - formulated problem , for which many solutions have been proposed [ 202 ] . Extending existing solutions to incorporate the speciﬁc constraints of heterogeneous multi - device plat - forms we envision ( IoT and AR , stationary and mobile , devices ) , for the speciﬁc case of scene change detection in the context of VI - SLAM , has the potential to signiﬁcantly reduce the extent of mapping that would be required to achieve high - quality , spatially - aware AR experiences . 4 . 2 Semantic Understanding A key element of next - generation AR is the use of semantic algorithms that detect the type , position , orientation , and even the current state of objects and surfaces within an environment . Not only can they be combined with SLAM to extend or enhance spatial understanding ( e . g . , [ 215 ] ) , but they also enable the delivery of various types of content to the user . Firstly , directly annotating a visual display with semantic information has a wide variety of applications , from language learning [ 81 ] to ﬁre - ﬁghting [ 23 ] . Secondly , this knowledge can be used to inform user interactions , e . g . , suggesting appropriate places for the user to position virtual content , or objects that can be interacted with in a speciﬁc application . Finally , semantic understanding also enables the provision of more intelligent content , such as avatars or virtual characters that interact naturally and autonomously with real - world objects . Here we focus on the topic of object detection to illustrate the role of ambient IoT sensors , however the techniques we describe may also be applied to other types of semantic understanding , such as semantic segmentation . Ambient Intelligence for Next - Generation AR 11 4 . 2 . 1 Background on Object Detection in AR By running object detection models on images captured by AR devices , we can detect the type , pose and extents of common objects that are present in real world environments , which provides us with a more in - depth understanding of environmental context and informs the rendering of virtual content . Although current advancements in deep neural networks ( DNNs ) have shown superior performance in object detection [ 12 , 78 , 99 , 105 ] , executing large networks on computation - constrained devices such as AR devices and IoT sensors with low latency remains a challenge . To address this , edge - supported architectures are needed to oﬄoad computation from the AR devices and IoT sensors and improve the end - to - end latency [ 70 , 114 , 201 , 205 , 216 ] . As the pervasive deployment of mobile AR will oﬀer numerous opportunities for multi - user collaboration , prior works have also studied object detection that exploits the visual information captured by diﬀerent AR devices [ 37 , 217 ] . Nevertheless , collaborative object detection for AR devices and IoT sensors , where visual information is captured from highly distinct vantage points , is an unexplored area of research . The depth information that is obtained by specialized AR headsets and high - end smartphones equipped with time - of - ﬂight depth sensors may also be employed to aid in object detection [ 147 ] , while recent work has investigated the use of point clouds generated on these types of AR devices as input to object detection models [ 37 , 74 ] . Another approach is to detect and track objects in the environment by matching current input data – either 2D feature points in captured images , or the 3D data in a generated point cloud – with a predeﬁned reference . In the case of images , the marker detection techniques we described in Section 4 . 1 . 1 may be used by attaching a printed marker to a real - world object . Alternatively , feature points can be matched across input and reference images [ 45 , 102 ] . By comparing the features extracted on reference images ( i . e . , images of objects that need to be detected or tracked ) to the features detected on input images ( i . e . , images of AR scenes where the desired objects are present ) using descriptors such as ORB , SIFT , or SURF [ 122 ] , the pose estimation of a desired object can be found by computing the homography matrix . Eﬀorts to enhance this markerless registration through improved feature matching include [ 32 , 98 , 103 ] . Similarly for point clouds , input data is matched with the 3D points captured in a prior scan of the object , through point cloud registration [ 79 ] . This approach is used to enable the detection of previously scanned objects on ARKit [ 15 ] . In general , feature matching approaches require fewer computational resources when compared to neural - network - based object detection , but are less robust to environmental factors such as lighting , image distortion , the distance of the AR device to an object , and background textures . 4 . 2 . 2 Enhancing Object Detection Using IoT Sensors Current techniques for semantic understanding , including both object detection and semantic segmentation , rely on inputs of 2D images [ 112 , 162 ] , image and depth data [ 147 ] , or 3D point clouds [ 74 ] . When these data are collected from sensors onboard AR devices , they are frequently subject to distortions due to device motion , occlusions , and resolution limitations ( i . e . , targets must be observed at an appropriate distance to capture informative data ) , resulting in incomplete and incorrect knowledge of the environment . Thankfully , just as for spatial understanding , ambient IoT sensors such as cameras and depth sensors can help address these issues by capturing data from additional and more favorable vantage points ( e . g . , a stationary downward - facing camera ) . Given that many semantic algorithms are computationally expensive and often best oﬄoaded from AR devices to an edge server , data from IoT sensors can also be transmitted to the edge for processing . One way in which the data sourced from IoT sensors can be used is to combine them with the data from AR devices , to achieve more robust detection . For example , in [ 114 ] we developed CollabAR , a collaborative image recognition system in which the camera image from an AR device is combined with spatially and temporally correlated images stored on the edge . The architecture for this system is shown in Figure 5 . Initial inference results based on the device image are provided by the distortion - tolerant image recognizer , then aggregated with the inference results from spatially and temporally correlated images by our auxiliary - assisted multi - view ensembler module , which outputs the ﬁnal object detection result . This enables CollabAR to achieve over 96 % recognition accuracy for images with severe distortions , with an end - to - end system latency as low as 17 . 8ms . With ambient IoT cameras , we can quickly provision a large set of high - quality images for the correlated image lookup step , without having to rely on data from AR devices . In general IoT sensors can supplement the data obtained by AR devices to achieve more robust object detection , by providing images or point clouds of the same environment region that are of higher quality , or that contain a more complete view of an object . Alternatively , the data sourced from IoT sensors may be used to extend the perceptual ﬁeld of the AR device , to detect objects or surfaces which are outside the ﬁeld of view or range of device sensors , or objects or surfaces which are entirely occluded . This gives rise to exciting possibilities regarding the extension of the human perceptual ﬁeld to the entire environment in which an AR user is located . Not only can these IoT sensor data include data from cameras and depth sensors , but one can also incorporate other modalities , towards more reliable recognition that is robust to conditions in which vision - based sensors may perform poorly . For example , passive infrared motion sensors which detect heat can detect human or animal presence in dark environments , while recent works have demonstrated tactile - olfactory - based detection of humans [ 113 ] and chemical sensing 12 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova Distortionclassifier Pristine expert Motion blur expert Gaussian blur expert Gaussian noise expert Recognition module Edge Server Resized Image Distortion - tolerant image recognizer Multi - view ensemble learning Resultscache Correlated Image lookup Spatial - temporal image lookup Anchor - time cache Cloudanchors R e s u l t Auxiliary - assisted multi - view ensembler anchors timestamps image IDs Pose estimation Mobile client Fig . 5 : An example of an edge computing - based architecture we have developed for semantic understanding in AR , CollabAR [ 114 ] , which provides distortion - tolerant , collaborative image recognition . As an example of combining data from AR devices and IoT sensors to achieve better robustness , we can use high - quality data from IoT cameras to provide the existing viewpoints used in the auxiliary - assisted multi - view ensembler step . of illegal drugs [ 57 ] and explosives [ 106 ] , which could all be incorporated into ambient IoT sensors . IoT - based monitoring of ambient acoustic signals [ 141 ] could be used to improve acoustic - based context detection for audio sources that are located far from the microphones of the AR devices . Central to realizing this vision of semantic understanding through an ambient multimodal sensor network will be solving challenges related to device localization and calibration , variable signal quality , and combining multimodal signals , which we discuss further in Section 6 . 3 . 4 . 2 . 3 Enhancing Object State Awareness Using IoT Sensors In addition to established types of semantic understanding – i . e . , a knowledge of the type and position of objects present in an environment – we propose that a knowledge of the current state of objects present can also enhance AR applications . This is particularly useful for AR applications that directly interact with physical objects in the surroundings , e . g . , overlaying holograms related to an object’s position , orientation , or other properties . These types of AR applications can be enhanced by the understanding of object states , and reﬂecting changes in real time . While information about some properties ( e . g . , pose ) can be gathered through the processing of images captured by an AR device , IoT sensors incorporated into objects can provide greater robustness to environment properties , as well as data on other types of object states ( e . g . , strain ) . Below , we cover several properties of objects that can be obtained through ambient IoT sensors , and discuss potential use cases in AR applications . Pose : While information about the position and orientation of objects in the surrounding environment can be obtained through marker detection ( see Section 4 . 1 . 1 ) or object detection ( see Section 4 . 2 . 1 ) , these vision - based approaches are often dependent on environmental factors ( e . g . , lighting conditions or image resolution ) . Incorporating ambient inertial sensors ( e . g . , the accelerometer and gyroscope in an IMU ) into objects on the other hand provides position and orientation estimates for an object without this dependency on environmental factors . Use cases for integrating inertial sensors into AR for understanding object pose can be seen across various applications that use wearable devices ; examples include tracking the orientation of a glove [ 132 ] , estimating a user’s location and orientation through IMUs in earphones [ 210 ] , detecting head movements for face - related gestures through smart glasses [ 123 ] , and enabling sensing through haptic devices [ 150 , 187 ] . Strain : Strain of an object refers to its deformation due to stress , and can be measured by strain gauges . Strain gauges change the electrical resistance based on the magnitude of the deformation , thus providing knowledge about the deformation Ambient Intelligence for Next - Generation AR 13 Fig . 6 : The registration of a catheter hologram in an AR - assisted external ventricular drain ( EVD ) procedure can beneﬁt from the use of IoT sensors for understanding of object state . We used strain gauges to detect the degree of bending on the catheter to align the catheter hologram in AR . of an object such as the bending or external pressure applied . In our recent work , we have been examining this property to enhance the image registration of a catheter hologram in AR - assisted neurosurgery , as shown in Figure 6 . We have developed an AR - assisted guidance system for neurosurgery by tracking the position and orientation of the catheter and overlaying a catheter hologram to guide surgeons in targeting the brain ventricle [ 46 , 47 ] . However , due to the deformable shape of the catheter , there were often misalignments between the hologram and the catheter object . Use of ﬁber Bragg grating ( FBG ) sensors has been proposed in multiple lines of work for shape detection in medical applications [ 108 , 182 , 183 ] , however the integration of FBG sensors into an AR system is challenging due to its high cost and requirement of an additional measurement device . To address this challenge , we are currently experimenting with strain gauges as low - cost IoT sensors that can be used to estimate the deformed shape of the catheter . The strain data collected from the catheter are sent to the HoloLens 2 from an edge server to display and align the correct shape of the catheter hologram onto the object . Figure 6 shows the enhancement of catheter hologram alignment by detecting the bending of the catheter using strain gauges . We believe that this will reduce the misguidance that can occur from the misalignment of the catheter hologram in our AR - assisted neurosurgical guidance system , and further enhance the accuracy of catheter placement during the external ventricular drain procedure . Other properties : Ambient IoT sensors could also be employed to detect other ‘non - spatial’ aspects of object state , as well as pose and strain . This goes beyond simply visualizing the data from existing IoT sensors , to enhancing semantic under - standing for AR in new ways . For example , one AR use case is cleaning applications , which inform users if an area requires attention , even if that area does not appear dirty to the human eye . Nanomechanical and electrochemical sensors have been developed which detect pathogens [ 29 , 157 ] , and if integrated into IoT devices , these sensors could provide information about whether objects or surfaces in an environment are contaminated . Similarly , recent works [ 91 , 135 , 143 ] have shown that food spoilage can be detected using a variety of diﬀerent devices , from gas , humidity , and temperature sensors to more novel designs using nanomaterials . Deployed in distribution , retail , or culinary environments , these sensors could increase the speed and accuracy of AR - assisted food inventory management , by quickly indicating to workers which items are unsafe for consumption . Extending the perceptive capabilities of users to a wider range of object properties in this manner will provide opportuni - ties to improve both productivity and safety in many industries , and is likely to be a key motivation for the wider adoption of AR . 4 . 3 Contextualized Content In the previous two subsections , we explored how ambient IoT sensors may be used to support or enhance two core aspects of next - generation AR , spatial and semantic understanding . We now consider how the virtual content that is presented to the user may be adapted according to the data obtained from IoT sensors . We start by covering adaptive user interfaces , the adjustment of virtual content to improve visibility and intelligibility for the user . We then cover the established ﬁeld of photometric registration in AR , the matching of virtual content lighting to real environmental conditions , before extending this to environment - aware intelligent virtual content , the provision of virtual avatars or characters which are aware of and respond to a wide range of environmental conditions . 14 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 4 . 3 . 1 Adaptive User Interfaces The development of adaptive user interfaces , the properties of which are adjusted according to the context they are presented in and the needs of the user , is a long - standing ﬁeld of research in human - computer interaction ( for a recent review see [ 133 ] ) . While to a large extent the literature on traditional 2D interfaces has focused on contextual information related to the user ( in - cluding social and cultural context ) or system capabilities , the impact of the diverse and dynamic real environments which will host AR on both human perception of virtual content and system functionality means that environment - aware user interfaces are a vital consideration [ 54 , 110 , 117 ] . Below we examine diﬀerent properties which will inform environment - adaptive user interfaces in AR , and how this will be enabled by ambient IoT sensors . Spatial and semantic understanding : A number of works on environment - adaptive AR have considered adaptations based on spatial or semantic properties . For example , in [ 56 ] the authors developed a rule - based framework that enables designers to ﬁt their application components to environments with diﬀerent geometries , in this case , sourced from the RGB and depth streams of a Microsoft Kinect camera . Similarly , [ 145 ] presented a prototype system that allows users to ‘snap’ virtual content to planar surfaces and edges , with environment data again extracted using RGB and depth images from the Kinect . More recently , the work of Lindlbauer et al . [ 110 ] considers adapting AR user interfaces based on a combination of environment geometry ( checking whether a virtual object will be occluded using depth data ) , user task ( which may in part be determined by semantic understanding ) , and user cognitive load . Regarding semantic understanding speciﬁcally , we may wish to place user interface elements near real - world objects that are semantically related , in order to either anticipate a user’s current needs , or to place AR - based tasks in context for a smoother experience , as proposed in [ 34 ] . For example , access to AR - guided recipes may appear above the stove , or an in - progress packing list might appear next to a suitcase . Alternatively , we may wish to block the view of real objects with virtual objects – for instance , in the context of just - in - time adaptive interventions ( JITAI ) [ 139 ] it may support a user’s personal development and change to cover a cigarette packet or mobile phone with another object such as a plant . A mock - up of this on the Magic Leap One headset is shown in Figure 7 , and we discuss this topic further in [ 175 ] . Incorporating ambient IoT cameras and depth sensors will enable suﬃcient levels of spatial and semantic understanding to realize these visions reliably in practical scenarios ; accurate , information on the contents of an environment , along with data gathered on how users interact with that environment , will enable the provision of optimally positioned virtual content for AR users . Fig . 7 : A Magic Leap - based mock - up of an environment - adaptive user interface , which uses semantic understanding of the environment to identify a distracting item ( a phone ) , then covers it with a virtual plant . To further motivate the user to study , a motivational hologram ( a diploma ) is presented [ 175 ] . Light , texture , and color : However , the physical attributes of an environment are not the only factors which should inform the positioning and properties of AR user interface elements – the light , texture , and colors present also aﬀect visibility and intelligibility . For example , on the OST displays employed on a number of state - of - the - art AR headsets , visibility of virtual content ( and as a result usability , user comfort , and sense of presence ) is lower at higher levels of illuminance , even those Ambient Intelligence for Next - Generation AR 15 commonly encountered in indoor environments [ 49 , 86 , 96 ] . Not only does the additive nature of these displays mean that any virtual content added is less perceptible , but real - world background textures that are visible through dark , transparent regions of virtual content are more visible . Therefore , we may wish to detect when high ambient light levels or highly textured backgrounds are present and increase the brightness ( pixel intensity ) of virtual content , or adjust the amount of ambient light that is allowed through the headset lenses , to improve content visibility . While state - of - the - art headsets support automatic adjustment of display brightness based on ambient light settings , and the Magic Leap 2 allows users to reduce the amount of ambient light that passes through all or parts of the display [ 119 ] , the ultimate goal here is automatic , ﬁne - grained adjustment based on both the nature of virtual content and the presence of background textures . Furthermore , existing literature has established the eﬀect of contrast ratio and color combinations on content legibility and aesthetics for both traditional ( e . g . , [ 107 ] ) and AR displays ( e . g . , [ 213 ] ) . Given that virtual content may be presented in front of a wide variety of real - world backgrounds in AR , content that is automatically optimized for the current background color and texture is naturally of interest , and was proposed in [ 54 ] . On OST displays , blending eﬀects occur in that the perceived color of virtual content is aﬀected by the color of the real - world background [ 53 , 55 ] ; the automatic selection of virtual content colors that will either be minimally aﬀected , or aﬀected towards a desired result by blending with the current background , is an important direction for future work . While light levels , background textures , and colors can be detected through the ambient light sensors or cameras on an AR device , this requires user interfaces ( or other content ) to be optimized in real - time . This is feasible for the types of back - grounds and content that have been studied so far in AR ( the vast majority of existing works consider text readability against backgrounds with a single color or pattern , e . g . , [ 40 , 54 ] ) , but for more complex cases it will likely be advantageous to prepare and provision optimized content in advance . Especially when we consider the complex ways in which light , texture , and color interact to determine visibility , and the potential for combining this information with spatial and semantic understanding of an environment , the use of ambient IoT sensors becomes a necessary addition . To this end , we envision the installation of IoT cameras that capture the texture and color of the real - world surfaces which frequently appear behind virtual content from the user’s perspective , with camera poses informed by both analyses of user trajectories and the positions of virtual content . Automatic content optimization , informed by existing work on deep learning for adaptive user interfaces ( e . g . , [ 188 ] ) can then be performed on the edge , and the result provisioned to an AR user when they enter an environment . Spatial and semantic algorithm performance : Less considered in the literature is how user interfaces might be adapted based on the current performance of algorithms for spatial and semantic understanding , which is essential to ensure users do not rely on incorrect virtual content . Several pioneering works examined how virtual content may be adjusted in the presence of registration ( tracking ) error ; for example , MacIntyre et al . introduced and applied a level - of - error rendering technique [ 116 , 117 ] , in which a virtual convex hull outlining a real object was expanded according to estimated registration error . This was extended in [ 38 ] to show 2D convex hulls representing possible registration error of virtual objects . In [ 72 ] , Hallaway et al . demonstrated switching between spatially registered object labels and unregistered augmentations depending on the level of tracking accuracy currently available ( this AR system employed a hybrid tracking solution incorporating both ultrasonic and GPS - based tracking , rather than VI - SLAM ) . Since then , other works have proposed and evaluated visualization techniques to mitigate the eﬀect of registration error [ 165 ] , as well as alternative ways of visualizing error in navigation tasks ( e . g . , colored arrows , virtual character expressions ) [ 149 ] . We are building upon this line of research by exploring new methods of displaying registration error , such as 3D convex hulls around virtual objects . In particular , our ongoing work examines how to establish and convey the relationship between environmental conditions and tracking or registration error – as we covered in Sections 4 . 1 and 4 . 2 , properties such as light and visual texture can impact the performance of the underlying algorithms . Beyond visualizing an estimate of current error , this will enable users and environment designers to take steps to reduce error by altering their environment , before the main AR experience commences . While commercial AR platforms such as ARKit [ 13 ] and ARCore [ 65 ] indicate when tracking results are unavailable or questionable , along with possible high - level causes , these causes lack granularity in terms of environmental conditions . To address this we developed an interpretable predictive model ( a decision tree ) for binary classiﬁcation of tracking performance in [ 178 ] , along with an example of how environment ratings might be displayed ( Figure 8a ) . We have since extended the design of this user interface to use the model output to provide the user with extra guidance , as shown in Figure 8b . In these cases the input data for the tracking quality prediction were obtained by the AR device alone , but we envision them being sourced from ambient IoT sensors in the future . We are now developing prediction models which provide more ﬁne - grained estimates of error magnitude , as well as methods to visualize the error estimates associated with diﬀerent environment regions . 16 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova ( a ) ARuserinterfaceshowingbinaryclassiﬁcationoftrackingperformanceusing symbols and colored planes [ 178 ] . ( b ) AR user interface showing colored binary classiﬁ - cation of tracking performance , plus user guidance . Fig . 8 : Two of our prototype adaptive user interfaces for communicating the current level of spatial understanding ( device tracking performance ) in an environment when placing a virtual object ( hologram ) . Notiﬁcation of tracking performance classiﬁcation ( a ) can be extended to include user guidance on how to improve unsuitable environments ( b ) . 4 . 3 . 2 Photometric Registration Currently , the most established ( and well - supported ) type of environment - based virtual content adaption in AR is photometric registration – rendering virtual content such that it is consistent with the lighting in the real environment . Lighting properties of interest include illuminance , light direction , and color temperature , and can be used to more accurately render reﬂections , specular highlights ( the bright regions of a surface that reﬂect a light source directly ) , and directional shadows , which in turn increases the level of realism , a user’s sense that a virtual object is really present in an environment . Photometric registration for AR has been an active research topic for more than two decades ( e . g . , [ 68 , 88 ] ) , enabling its recent support in commercial AR platforms such as ARCore and ARKit . State - of - the - art methods in commercial AR ( e . g . , [ 66 ] ) use machine learning to analyze camera images and synthesize environment lighting , similar to recent work on using CNNs for this task ( e . g . , [ 121 ] ) . Other recent work includes the development of an edge - assisted framework to support real - time light estimation [ 223 ] , and the use of physical , geometrically - tracked reﬂective light probes that are placed in the environment [ 155 ] . We envisage ambient IoT cameras and light sensors being employed to enhance photometric registration for two main reasons . Firstly , IoT sensors are able to provide views of the environment not available to AR devices due to occlusions or a limited ﬁeld of view . This will enable a more complete understanding of lighting within an environment , such as the exact position of light sources which are not visible from the perspective of the AR device . Secondly , unlike AR devices , IoT devices are able to obtain relevant information on environment lighting prior to an AR experience . This means that all the computation related to light estimation does not have to be done in real - time on the AR device ; instead , it can be precomputed ( and continuously updated ) on the edge , both saving resources on the AR device and allowing for more complex light estimation methods to be implemented . Indeed , an example of this type of solution was developed in [ 167 ] , which the authors termed ‘distributed illumination’ , to address the lack of computing power in mobile devices at that time ( 2014 ) . In this implementation high dynamic range cameras were connected to a stationary PC via a wired connection ; we propose extending this to wireless connections using the IoT sensors now available , as well as incorporating more advanced light rendering techniques such as indirect illumination ( reﬂections from the virtual object onto the environment ) and the use of spherical harmonics ( e . g . , [ 222 ] ) . 4 . 3 . 3 Environment - Aware Intelligent Virtual Content Environment awareness in next - generation AR should also extend to the provision of intelligent virtual humans , animals , or other characters that respond naturally to environmental conditions , including the presence and state of real humans or objects , as well as properties such as light and temperature . This has the potential to make virtual content appear even more realistic to users , and further increase a user’s sense of presence or immersion . Indeed , recent work has shown that virtual humans whose gaze is directed towards real physical objects supported more eﬀective communication with real participants [ 8 ] , and Ambient Intelligence for Next - Generation AR 17 that a virtual human that is able to inﬂuence physical objects ( e . g . , turning oﬀ a light ) may be seen as more trustworthy by users , and can result in users perceiving a greater degree of social presence [ 93 ] . In [ 193 ] the authors looked to develop a more sophisticated understanding of semantic context to inform this type of intelligent virtual content , with semantic information ﬁrst extracted from RGB and depth images , then represented as a 3D scene graph . While less explored , a virtual character’s apparent awareness of and ability to respond to environmental properties such as light , noise , wind , or temperature also holds great potential for increasing levels of realism and immersion . For example , one can imagine a virtual cat that basks in a patch of sunshine , a virtual human that turns its head in the direction of a slammed door , or a virtual character that warms itself next to a heater . In [ 94 ] the authors demonstrated the use of IoT wind sensors for AR , with virtual papers that ﬂuttered in response to the airﬂow generated by a real fan , along with a virtual human that put their hand out to stop the ﬂuttering . IoT sensors will be particularly useful in cases such as this , when detecting or localizing these types of contextual information is beyond the capabilities of the sensors onboard an AR device . Contextual data from diﬀerent parts of an environment may also be required in advance in order to provision speciﬁc animations associated with responsive or intelligent content ( e . g . , the virtual cat rolling around in a patch of sunshine ) . 4 . 4 Interaction Currently , the primary methods of interaction for user input in AR are tactile , via a touchscreen on the device ( e . g . , a smartphone ) , a controller ( e . g . , the Magic Leap 2 ) , or mid - air gestures via hand tracking ( e . g . , the Microsoft HoloLens 2 , the Varjo XR - 3 ) . Some devices also facilitate gaze - based interactions , supported via video - oculography - based eye tracking , or auditory interactions via speech recognition . One important property of all these methods is that their performance can be aﬀected by noise sources in the surrounding environment ; for example , the presence of high intensity light sources or high illuminance in general can cause issues for infrared sensor - based depth sensing [ 195 ] , resulting in lower quality eye tracking [ 172 , 177 ] and hand tracking [ 71 ] . 3D gaze point estimation may also be inaccurate at low illuminance due to lower quality spatial mapping [ 71 , 177 ] , while acoustic noise is known to be problematic for speech recognition [ 221 ] . Another limitation of tactile and mid - air interaction methods at present is that they only capture gestures made using the hands , and in the case of mid - air gestures captured via hand tracking , gestures are only recognized when the hands are in the ﬁeld of view of the head - mounted depth sensor . If we compare this to natural human interactions , this is severely restricted ; humans frequently express themselves with hand gestures outside this region , as well as gestures with the head , upper and lower body , and facial expressions . There are two ways in which ambient IoT sensors could be used to manage or address the reduced performance of AR interaction methods due to environmental conditions . One option is to use ambient light sensors or microphones to predict the current level of performance for each interaction method ; the most appropriate method could then be suggested to the user , or virtual content could even be adapted to address current limitations ( e . g . , elements might be increased in size to support gaze - based interactions when gaze estimation accuracy is lower ) . Another option particularly applicable to speech - based interaction is to use ambient IoT sensors to inform noise cancellation techniques . Such a method was developed in [ 185 ] , the core idea being that an IoT microphone captures ambient sounds and forwards them to the speaker over its wireless radio , with the information arriving faster than if the sound was captured next to the speaker . This solution is readily applicable to AR audio systems , especially if IoT microphones are already placed in the environment for other purposes . There are also a number of possibilities for extending the interaction methods available in AR using IoT sensors . For example , cameras and depth sensors have the potential to greatly expand the range of gestures AR users can use for interaction by capturing a view of a user’s entire body . Building upon existing work in human - robot interaction ( for a review see [ 111 ] ) and human activity recognition ( for a review see [ 39 ] ) , we can develop solutions to recognize natural human gestures as well as relevant activities in AR scenarios . Alternatively , tangible user interfaces allow users to manipulate virtual content using a physical proxy , tracked using optical or inertial sensors – for example in [ 44 ] a Vive Tracker [ 77 ] equipped with infrared and inertial sensors was placed inside a physical sphere used for interaction . One could also leverage IoT tactile sensors present in the environment , such as touchscreens , physical buttons , or pressure sensors ; a recent example of the latter , particularly relevant to robotic and biomedical applications , was developed in [ 36 ] , and is able to detect contact pressure , contact shape , and shear direction . Finally , higher - quality signals available from ambient sensors will also be useful in some circumstances ; for example , although the microphones embedded in AR devices are of suﬃcient quality for speech - based interactions , we may require microphones with a better dynamic range and frequency response for AR applications that present feedback on musical performances to support learning . 18 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 5 IoT - based Actuation for AR Next , we detail how ambient IoT actuators may be used to support or enhance AR experiences . As in Section 4 , we examine the diﬀerent uses which we deﬁned in Section 3 . 1 , though given that the role of IoT actuators for AR is less studied than IoT sensors , this section is more exploratory in nature . In this section we combine related uses into one of two subsections ; the ﬁrst covers how IoT actuators can enhance the performance of spatial and semantic understanding algorithms by optimizing environmental conditions , and the second covers how IoT actuators can be used to extend interaction methods or enhance immersion . 5 . 1 Spatial and Semantic Understanding Optimizing environment properties in order to achieve higher quality spatial understanding is an important direction for both marker - based and markerless AR , that will enable both more accurate spatial registration of virtual content and more stable virtual content . Here we cover the use of IoT actuators for marker - based AR in our dynamic marker system ( Section 5 . 1 . 1 ) , and for markerless AR in our environment illuminance optimization system ( Section 5 . 1 . 2 ) . One alternative to these spatial understanding methods was recently introduced in [ 4 ] , and involves the modulation of ambient light sources in a predeﬁned manner ; this could be achieved using ambient IoT actuators such as light bulbs , however we focus on established techniques for spatial understanding here . Finally , we also cover the possible use of IoT actuators to optimize environments for semantic understanding ( Section 5 . 1 . 3 ) . 5 . 1 . 1 Environment Optimization for Marker - based AR As introduced in Section 4 . 1 . 1 , marker detection is a common technique used for object detection , with the position and orientation of an object obtained through the detection of a printed marker attached to the physical object . To address the challenges of environmental factors ( e . g . , environment lighting , the distance and angle between an AR device and the marker ) , we developed a dynamic marker system that controls IoT actuators including an E - Ink display and a smart light bulb to optimize the environment for marker detection . Fig . 9 : The IoT - based architecture of our dynamic marker system , an example of environment optimization for marker - based AR . Environment images are captured by the AR headset and processed on the edge server for feature detection and matching , to inform environment optimization through IoT actuators . Dynamic markers were ﬁrst explored by manipulating the size of projections in a projection - based AR system [ 124 ] , however with recent innovations in low - cost and low - power display hardware ( e . g . , an E - Ink display ) , dynamic markers became more practical for many applications . In prior studies , dynamic markers were used in robotic applications such as enhancing human - robot swarm management [ 131 ] , or aiding Unmanned Aerial Vehicle ( UAV ) landing by scaling the size of a marker based on the distance to the UAV [ 1 ] . Similarly , a dynamic marker has the potential to enhance marker - based AR applications by changing the properties of the marker . Compared to a static printed marker , dynamic markers using an E - Ink display can enhance user interactions by adapting to various situations ( e . g . , changing the size , shape , or pattern of the marker on the display ) [ 152 ] . However , there are challenges associated with a dynamic marker , such as the glare observed on the E - Ink display depending on the environment lighting , which may result in the marker being undetectable by the AR system . Our dynamic marker system Ambient Intelligence for Next - Generation AR 19 for AR further enables the system to control the properties of IoT actuators such as the brightness of a smart light bulb , and the size and shape of a marker shown on an E - Ink display , in order to optimize the environment for marker detection ( as shown in Figure 9 ) . The hardware setup of our system ( illustrated in Figure 10 ) includes a 7 . 5 - inch Waveshare E - Ink display that can show images in 8 - bit grayscale with a Raspberry Pi 3 , a HoloLens 2 AR headset with Vuforia marker detection , and a LIFX smart light bulb for controlling environment illuminance . As shown in Figure 9 , we collect scene ( environment ) images from the HoloLens 2 and process them on an edge server , to characterize the environment with feature points , an important metric for marker detection algorithms , that provides information about the quality of the marker image . Due to the relatively low computational complexity of this task , we use a Raspberry Pi 3 as our edge server . The scene images are ﬁrst cropped to a region of interest through image processing by detecting the square shape of the marker . We then detect the feature points in the cropped scene image and match them with the reference marker image to quantify the percentage of features available in the scene image . Our system considers three environmental properties ( the lighting condition of the scene , the distance from the camera to the marker , and the viewing angle of the camera to the marker ) to optimize for marker detection . Environment lighting impacts the quality of the printed marker , with marker detection less robust in darker or brighter conditions . The distance and angle of the camera on the AR headset , determined by the user’s movement , impacts marker shape - related factors such as the black and white ratio , edge sharpness , or the information complexity of the marker [ 87 , 90 ] . To optimize the environment , we control the IoT actuators to change the brightness of the LIFX bulb , and the size and shape of the marker shown on the E - Ink display , until the percentage of matched feature points reaches the optimal level for marker detection . The latency associated with changing a new image based on the environment characterization in our dynamic marker system takes about one second , due to updating all pixels on the E - Ink display . Fig . 10 : Hardware setup of our environment optimzation system for marker - based AR : an E - Ink - based dynamic marker , a HoloLens 2 , and IoT actuators ( a ) , an undetected dynamic marker in an unoptimized environmental condition ( b ) , an overlay of virtual content through detection of the dynamic marker in an optimized environmental condition ( c ) . To inform this system , we ﬁrst investigated the relationship between the three aforementioned environment properties and characterization metrics of the scene ( i . e . , results of feature point matching ) . We conducted experiments by computing feature matching with four diﬀerent marker patterns under diﬀerent environmental conditions . The four markers comprised two ﬁducial markers in 1 - bit grayscale with a similar number of available features from ARToolkit [ 17 ] and ArUco [ 18 ] open - source libraries , and two image markers ; one with a uniform pattern of features and another with an inconsistent pattern of features . We analyzed the changes in the percentage of matched feature points by testing each marker pattern at 9 diﬀerent viewing distances ( 20 - 90cm with an increment of 10cm ) , 5 diﬀerent viewing angles ( 0 - 60 degrees with an increment of 15 degrees ) , and 9 diﬀerent illuminance levels . Each set of environmental conditions was run for 20 trials . Our results showed that there was a correlation between the number of matched feature points and the environment properties . More feature points were matched on the dynamic marker when the viewing distance and angle were lower . On the other hand , fewer feature points were matched on the dynamic marker at lower illuminance levels . We also observed a dissimilitude in the changes in the percentage of the matched feature points among diﬀerent marker patterns . The matched feature points on the ﬁducial markers ( those from ARToolkit and ArUco ) were more dependent on environmental conditions than image - based markers due to their lower number of available feature points . This prompts further investigation to discover the diﬀerent optimal conditions required for each marker pattern , to achieve more robust and accurate marker detection in AR . These relationships between environment properties and marker detection performance motivate the use of dynamic markers with IoT - based actuators for various marker - based AR applications . Our dynamic marker system achieves more accurate and robust marker detection by optimizing the environment . For instance , we can change the brightness of a smart light bulb to optimize the level of illuminance , and change the size of the marker shown on the E - Ink display based on the distance from the 20 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova user to the marker . This environmental optimization enhances user interactions in marker - based AR applications by allowing the system to detect a marker at ease even under initially challenging conditions . Furthermore , a dynamic marker provides much greater ﬂexibility in changing the marker pattern , shape , and size , when compared to a conventional printed marker . This holds potential for marker - based image registration in medical applications by showing diﬀerent anatomies in AR holograms based on the marker type [ 46 , 52 ] or in human - robot applications by providing diﬀerent feedback on the E - Ink display to enable simpler debugging for users [ 131 ] . 5 . 1 . 2 Environment Optimization for Markerless AR Given the increasing popularity of markerless AR applications ( e . g . , [ 6 , 82 , 144 ] ) , and the fact that virtual object instability due to incorrect spatial understanding is still a prevalent issue on state - of - the - art platforms and devices [ 174 ] , it is also of interest to consider how IoT actuators can be used to optimize environments for better spatial understanding in markerless AR . As we covered in Section 4 . 1 . 2 , environment illuminance ( ambient light level ) plays a central role in determining the performance of VI - SLAM , the method underpinning spatial understanding in markerless AR , because it determines the extent to which visual textures are distinguishable for feature - based mapping and tracking . If the tracking cameras on an AR device are pointed towards regions of an environment with suﬃciently low or ( less commonly in indoor scenarios ) high levels of ambient light , device tracking may fail to initialize , be of lower quality , or be lost , resulting in the incorrect spatial registration of virtual content . Thankfully , illuminance is also readily controlled through IoT actuators which emit light ( e . g . , smart bulbs ) or block light ( e . g . , smart blinds ) , leading us to focus our initial eﬀorts on environment illuminance optimization . Additional complexity comes from the need to consider the impact of light on other AR system elements , including the quality of semantic understanding , the performance of eye and hand - tracking algorithms , and the visibility of virtual content on OST displays . In [ 177 ] , we developed a proof - of - concept environment illuminance optimization system for AR , which automatically maintains illuminance at a suﬃcient level for high virtual object stability , and where possible , accurate and precise eye tracking . It uses both IoT sensors to detect current levels of illuminance and visual texture in an environment , and an IoT actuator to control the level of illuminance in that environment . Our results on virtual object position error for diﬀerent visual textures and illuminance levels ( see Section 4 . 1 . 2 ) showed that the robustness of spatial understanding of diﬀerent illuminance levels is dependent on the properties of the visual textures present , with ﬁne textures requiring greater illuminance to support good performance . Therefore , while our system’s default optimum illuminance level is 300 lux ( to support accurate eye tracking and low virtual object position error for coarse textures ) , when the environment contains ﬁne visual textures the core AR functionality , virtual object stability , is prioritized and that optimal level is increased to 750 lux . To oﬄoad the computationally expensive environment texture characterization task , while avoiding the transfer of potentially sensitive images to the cloud , we implement an edge - computing architecture , with optimization controlled by a server on the same wireless local area network , as shown in Figure 11 . Fig . 11 : The edge - based architecture for our environment illuminance optimization system for markerless AR [ 177 ] . Environ - ment images and illuminance levels are periodically captured by IoT sensors ( a Raspberry Pi camera and an ambient light sensor ) connected to a Raspberry Pi , which transfers these data to the edge server . Environment characterization is performed on the edge , and instructions are sent to an IoT actuator , a LIFX bulb , to optimize illuminance levels . Our environment illuminance optimization system employs two IoT sensors , an 8MP Raspberry Pi Camera Module 2 and an ambient light sensor ( TSL25911FN ) , connected to a Raspberry Pi 4 ( as shown in Figure 12 ) , to record images of the environment and illuminance every 5s . This environment data is saved to local storage and then sent via an HTTP PUT request to the edge server ( a Lenovo ThinkCentre M910T desktop computer ) . The characterization module determines the optimal Ambient Intelligence for Next - Generation AR 21 light level by detecting the number of FAST corners in the environment image using OpenCV : if more than 250 corners are detected the environment texture is classiﬁed as ﬁne , and the optimal light level is raised . If a light level change is required , the characterization module sends the optimal and current light levels to the light level control module . Our system employs one IoT actuator , a LIFX bulb , and the light level control module adjusts the brightness value of the LIFX bulb accordingly , at low latency ( < 0 . 5s ) . Environment characterization metrics ( illuminance , plus image brightness , contrast , edge strength , and corners ) are also stored on the edge for long - term trend analysis , while the latest metrics can be requested by an AR device via an HTTP GET request . The computational and storage requirements of this long - term environment characterization motivated our use of a higher class of edge server ( the desktop computer ) than the Raspberry Pi 3 used in our optimization system for marker - based AR ( Section 5 . 1 . 1 ) . In future work , we will extend our environment illuminance optimization system to employ multiple sets of sensors and smart bulbs to control illuminance in diﬀerent environment regions . One associated challenge will be device localization and calibration , which we discuss further in Section 6 . 2 . It will be desirable for the pose of IoT cameras to be automatically adjustable , such that they capture regions where AR users often point their device camera , or where device tracking is lost . To this end , while our current system uses a ﬁxed camera mount , future implementations could be replaced by a programmable camera mount , controlled according to data gathered on the edge server ( the Arducam B0227 Pan Tilt Platform for example provides a low - cost solution for a Raspberry Pi Camera ) . We will also investigate how other IoT actuators such as smart blinds can be incorporated , as alternative methods of controlling illuminance . Fig . 12 : A proof - of - concept installation of our IoT - enabled environment illuminance optimization system [ 177 ] , viewed through a custom AR application on a Samsung Galaxy Note 10 + smartphone . In order to capture the appropriate region of the environment with the Raspberry Pi Camera Module 2 we manufactured a custom 3D - printed camera mount , which could be replaced by a programmable mount in future implementations . Another direction for future work is to explore how the visual texture in an environment could be adjusted to enhance spatial understanding in markerless AR . VI - SLAM algorithms rely on suﬃcient texture for accurate and robust pose tracking [ 176 ] , but this is often not present in built environments , due to human preferences for minimalism in architecture and interior design . One possible solution would be to detect when an AR experience is taking place in an environment , and use ambient IoT actuators such as electronic or E - Ink displays , or light projectors , to provide additional visual texture during that period . In some cases , additional textures provided by ambient actuators might even be matched with the contents of the AR application , towards greater user immersion ( see Section 5 . 2 . 2 ) . In ongoing work , we are investigating methods that automatically determine where , for a given environment and application scenario , the application of visual texture will be most beneﬁcial . However , any environment texture optimization system must also take into account the impact texture can have on the visual perception of AR users – greater visual texture can impair visibility on OST displays or distract users from a task [ 176 ] . This is particularly pertinent because some of the environment regions in which applying visual texture is likely to be most beneﬁcial for spatial understanding ( i . e . , the regions which a device camera is most often facing towards ) , are frequently the regions where AR content is placed , so adjustment of textures could interfere with human perception . To inform the optimization of visual texture in AR environments , further work is required to investigate the trade - oﬀs between spatial understanding and human visual perception of diﬀerent visual textures . 22 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 5 . 1 . 3 Environment Optimization for Semantic Understanding In a similar manner as for spatial understanding , one can also use ambient IoT actuators to optimize environments for semantic understanding . Speciﬁcally , IoT light bulbs and blinds can be used to adjust environment illuminance to levels which maximize the performance of vision - based object detection and semantic segmentation algorithms . For example , the guidelines for detecting previously scanned objects on ARKit ( described in Section 4 . 2 . 1 ) recommend an illuminance of 250 to 400 lux [ 15 ] . At low levels of illuminance , noise is introduced to the camera images used as input for semantic understanding , especially on mobile devices with a small camera sensor size [ 7 ] – even small amounts of noise have been shown to result in large accuracy drops for leading image recognition algorithms [ 114 ] . Although we covered techniques to obtain images with less distortion using ambient IoT cameras in Section 4 . 2 . 2 , one can make this task less challenging by ensuring IoT light bulbs provide suﬃcient illuminance . On the other hand , high intensity light sources may cause specular reﬂections on objects or surfaces made of glossy or metallic materials ; not only do these artifacts reduce the performance of image - based object detection or semantic segmentation algorithms [ 10 ] , but they also degrade the quality and completeness of data available from time - of - ﬂight depth sensors used to support semantic understanding [ 195 ] . IoT light bulbs should be set to intensities that minimize these issues , and IoT blinds should be employed when necessary to block strong sunlight . Critical to implementing environment optimization systems for semantic understanding will be establishing ‘regions of interest’ , where illuminance - related issues are likely to occur . Regions of interest will be areas of an environment where both ( 1 ) illuminance - based distortion or artifacts could occur ( e . g . , dark corners , or a metallic object close to a light source ) , and ( 2 ) AR users are likely to view , particularly with the expectation of semantic information being provided ( e . g . , an object of interest ) . These regions of interest can then be monitored by ambient IoT sensors to inform the control of nearby IoT actuators . For example , if an ambient light sensor on a desk or workbench detects a low light level , the light intensity of a nearby IoT light bulb could be raised . Alternatively , one could detect illuminance - based issues in IoT sensor data directly ; for example , specular reﬂection detection [ 10 ] could be run on the images captured by an IoT camera focused on a metallic object near a window , and when specular reﬂections are detected the IoT blind on that window would be lowered . We envision these optimization systems being readily combined with the illuminance optimization systems for spatial understanding that we described in Sections 5 . 1 . 1 and 5 . 1 . 2 ; not only do they share the common goal of minimizing distortion and artifacts in visual data , but regions of interest in need of IoT sensor - based monitoring will frequently overlap . 5 . 2 Interaction and Immersion As well as being used to optimize the performance of spatial understanding algorithms , IoT actuators may also be leveraged to enhance the user - facing elements of an AR system directly . Below we consider the ways in which a user’s interactions with virtual content ( Section 5 . 2 . 1 ) and their immersion or sense of presence in it ( Section 5 . 2 . 2 ) may be improved using ambient IoT actuators . 5 . 2 . 1 Interaction Environmental conditions ( e . g . , lighting ) can impact both the performance of interaction methods such as hand and eye tracking in AR ( see Section 4 . 4 ) , and an AR user’s perception of visual virtual content ( see Section 4 . 3 ) , which in turn aﬀects their ability to interact with it . Ideally , we wish to incorporate these constraints into the type of environment optimization systems we introduced in Section 5 . 1 , although this will not be without challenges due to conﬂicting requirements , as we discuss in Section 6 . 4 . In certain cases , for example when conditions cannot be adjusted , or in safety - critical scenarios where the accuracy and speed of task completion are paramount , it may be beneﬁcial to present information on traditional electronic displays that are not as aﬀected by environmental conditions . Users could choose to switch to a nearby ambient visual actuator ( e . g . , a tablet , smartphone , or smart display such as an Echo Show ) to help them complete a speciﬁc task , before switching back to the main AR display . Indeed , similar to how screen mirroring is used to share content with a wider audience , this can also be used to support interactions that involve non - AR users . We envision that knowledge of alternative displays that can currently be leveraged will become an important element of environmental awareness in AR . Another limitation of current AR headset designs is that they facilitate the delivery of visual and auditory sensory information to users , but are not well suited to providing realistic tactile feedback when users interact with AR content . The controllers included with some devices ( e . g . , the Magic Leap 2 ) provide uniform tactile feedback and support vibration but do not allow users to actually ‘touch’ AR content , while hand tracking - based interactions allow for natural gestures but do not provide any haptic feedback when users touch virtual content . Recent work has sought to address this latter issue by providing haptic Ambient Intelligence for Next - Generation AR 23 feedback through a vibrotactile ring [ 125 , 192 ] or ﬁnger - side or nail actuators [ 118 , 156 ] ; however , all solutions involving wearables place extra requirements on user equipment and setup that may not be practical in many scenarios . One potential alternative for visual AR content that is overlaid on a real - world surface is the use of surface haptics , to produce variable tactile sensations and perceived levels of friction on the same surface through electroadhesion [ 134 , 206 ] . By incorporating these devices into an AR environment as ambient IoT actuators , we can program them according to the virtual content that is displayed in a particular region , and thereby provide more realistic haptic feedback to users without the need for wearables . 5 . 2 . 2 Immersion There are a variety of possibilities for using ambient IoT actuators to enhance an AR user’s sense of immersion . One type of enhancement stems from the fact that current AR devices are limited to delivering visual and auditory sensory information ; this can lead to a mismatch with other sensory inputs such as the sensation of temperature ( thermoception ) , for example , if a virtual ﬁre is present , but the room is cold . This limits a user’s sense of immersion , the feeling that virtual AR content is truly present in the real environment – research indicates that increasing the modalities of sensory input in virtual environments increases a user’s sense of presence in that environment ( e . g . , [ 42 , 64 , 153 ] ) , consistent with observations that the human perceptual system evolved in multisensory environments [ 189 ] . Recent work in VR has explored the potential for diﬀerent types of external actuators to deliver a range of stimuli speciﬁc to the current environment , including speakers for 3D audio [ 101 ] , heat lamps for temperature control , and fans for haptic wind representations [ 43 ] ( the authors use the term ‘smart substitutional reality’ to describe their system here ) , and an olfactometer that provides diﬀerent scents or smells [ 16 ] . We propose applying and evaluating these types of systems in AR ; for example , if we imagine a therapeutic AR application that relaxes users by placing virtual ﬂowers in their environment , incorporating the scents associated with those plants may increase the application’s eﬀectiveness . Given the variable level of virtual content that may be incorporated into an AR experience , an important consideration will be how a sense of virtual content realism in AR ( i . e . , a sense that a virtual object is present in a user’s physical environment ) diﬀers from a user’s sense of presence in VR ( the feeling that one is actually present in a new virtual environment ) . Alternatively , another type of possible enhancement using IoT actuators involves increasing immersion through the addition of visual and auditory stimuli beyond the current limitations of AR device displays and speakers . In particular , the limited ﬁeld of view on OST displays and smartphones may be counteracted somewhat by adding visual stimuli , or controlling visual conditions , in the surrounding environment to match AR content . For example , while an AR application displaying the planets of the solar system is being used , the environment light level might be lowered using IoT bulbs , and distant stars represented on the surrounding surfaces using IoT light projectors . Existing immersive art experiences which oﬀer separate projection and VR exhibits ( e . g . , [ 50 ] ) could provide AR experiences which combine 3D virtual content with 2D projections , while still allowing visitors to interact naturally in the same space . For auditory information , although spatial audio using headphones or the speakers available on some headsets is well - developed , there will be cases ( e . g . , for a smartphone without headphones , or a headset with a smaller form factor ) in which ambient IoT speakers can replicate environmental sound sources much more realistically . Indeed , the opportunities to support smaller AR device form factors by using external devices for virtual content presentation ( as well as for computation ) , in this type of ‘mixed media AR , ’ is another important direction for future work . 6 Challenges and Research Directions In this section we lay out a set of research directions associated with employing ambient IoT devices to support or enhance AR experiences . First , we describe the use of a game engine - based emulator , required to gather suﬃcient data on the eﬀect of environment properties to inform the design of IoT - supported AR systems , and prototype them in known , controlled conditions ( Section 6 . 1 ) . Second , we lay out the challenges associated with AR and IoT device localization and calibration that arise once we implement these systems in the real world , along with potential solutions ( Section 6 . 2 ) . We then consider how , given a set of localized and calibrated devices , one might tackle combining the sensor data from those devices ( Section 6 . 3 ) . Next , we discuss the challenges of implementing the IoT actuator - based environment optimization systems we described in Section 5 , with a focus on the conﬂicting requirements of diﬀerent aspects of AR experiences , as well as those that come about once we introduce heterogeneous users and devices ( Section 6 . 4 ) . Finally , we cover security and privacy issues related to platforms which combine AR and ambient IoT devices ( Section 6 . 5 ) , a vital consideration for the wider acceptance and deployment of ambient IoT - supported AR systems . 24 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 6 . 1 Game Engine - based Emulations of IoT - supported AR Systems The vision we have laid out in this book chapter requires both an in - depth understanding of how environment properties impact AR algorithm performance in diverse scenarios , and the development of deep learning models to predict AR algorithm perfor - mance . This is challenging for two reasons . Firstly , we require large amounts of data with accurate ground truth information in a diverse set of environments , which is time - consuming or even infeasible to obtain . For example , obtaining ground truth pose data for VI - SLAM algorithm evaluations in real environments necessitates the use of optical tracking systems such as OptiTrack [ 148 ] and Vicon [ 199 ] , which involves considerable setup and calibration time for each new environment . Secondly , we require ﬁne - grained , systematic manipulation of environment properties in order to perform experiments in repeatable and controlled conditions ; this is also diﬃcult to achieve in most real environments because they are often subject to external inﬂuences , such as daylight entering through a window , or objects being moved . Indeed , these challenges are why existing datasets for VI - SLAM evaluations ( e . g . , EuRoC [ 26 ] , TUM VI [ 181 ] , SenseTime [ 83 ] ) or object detection in AR ( e . g . , [ 2 , 105 ] ) cover a small range of environments or provide no information on environment properties . Instead , we propose the use of highly realistic synthetic data , recently made possible through high - deﬁnition rendering techniques in game engines ( e . g . , Unity [ 197 ] and Unreal [ 48 ] ) and other rendering software ( e . g . , V - Ray [ 30 ] ) . We developed a methodology for using virtual environments for VI - SLAM evaluations in [ 176 ] , and the process for generating synthetic datasets for semantic understanding was described in [ 164 ] . We cover the key elements of this approach for ambient IoT - supported AR below . AR environment and AR user emulation : In ongoing work we are using the Unity and Unreal game engines , de - facto standard AR and VR development platforms , to create emulators of AR environments and users . We are using these emulators to generate diverse photorealistic scenes , represent mobile AR user behaviors , and test the performance of a wide range of IoT - supported AR solutions . An example of a scene we have generated in our initial Unreal - based emulator is shown in Figure 13 . Within the generated scenes , we have ﬁne - grained control over multiple parameters related to AR environments ( e . g . , physical layouts , lighting , reﬂections , textures ) , as well as parameters related to AR users and devices ( e . g . , trajectories , camera properties , motion blur levels ) . In future we envision the inclusion of other factors to emulate AR environments and users at higher ﬁdelity , such as animating AR users to move and behave like real humans . (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:9) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:10) Fig . 13 : An image captured by an IoT camera in the Unreal - based emulator we have created . The IoT camera overlooks a photorealistic scene with three AR users . IoT sensor and actuator emulation : To enhance the capabilities of our game engine - based emulators , we will also emulate ambient IoT sensors and actuators in AR environments . For example , we will emulate light sensors to measure lighting proper - ties ( illuminance , light direction , and color temperature ) of the virtual environment . In our preliminary investigations we have added an IoT camera in our emulator : Figure 13 shows an image captured by this IoT camera , overlooking a space with three AR users . The camera has also identiﬁed and tracked the AR users in its view . We are currently exploiting the emulation of IoT cameras to develop algorithms that increase the accuracy of spatial and semantic understanding , by fusing the images from these IoT cameras with the sensor data captured by the AR devices worn by emulated AR users . Adding the emulation of IoT actuators ( e . g . , by creating and modifying light sources ) will enable the modeling and prototyping of environment optimization Ambient Intelligence for Next - Generation AR 25 systems , such as those we described in Section 5 . 1 . Ground truth generation : As well as controlling parameters related to environmental conditions , we envision building pipelines that automatically generate the ground truth data required for algorithm evaluation . We obtained the ground truth of the states ( e . g . , camera poses , sensor positions ) of AR devices and IoT sensors in our prior work [ 176 ] , and will use the pipelines to generate pixel - wise semantic instance annotations for images captured by AR devices and IoT sensors . For example , in [ 176 ] our VI - SLAM evaluation methodology used existing SLAM dataset ground truth trajectories to generate sequences in new virtual environments , while preserving the use of real inertial data , as shown in Figure 14 . In our future work , we envision exploiting the automatic generation of ground truth data , especially ﬁne - grained pixel - wise data , to evaluate IoT - supported AR systems under diverse environment settings , such as assessing mapping accuracy in SLAM , which is otherwise diﬃcult to evaluate in real environments [ 97 ] . Fig . 14 : An overview of our methodology ( left ) and a screenshot of the SLAM sequence execution step ( right ) for our game engine - based emulator for VI - SLAM using virtual environments [ 176 ] . 6 . 2 AR and IoT Device Localization and Calibration In the game engine - based emulators described above we have access to the ground truth poses of AR and IoT devices , along with the parameters of IoT sensors and actuators ( e . g . , correct information on environment regions captured by sensors and aﬀected by actuators such as light sources ) . However , a critical challenge for multi - device , IoT - supported AR in real environments is how we establish all AR and IoT devices’ poses within the same frame of reference , and determine the environment regions either in view of sensors or which will be adjusted by an actuator . Existing literature provides us with potential ways to address this problem ; for example , the localization of IoT devices is a known issue , and a variety of diﬀerent solutions have been proposed , including the use of ultra - wideband signals [ 22 ] and retroreﬂectors [ 184 ] – for a survey of indoor localization systems and technologies see [ 212 ] , and for discussion of the challenges speciﬁc to IoT devices , see [ 61 ] . Established camera calibration techniques are also an option for IoT cameras , such as the OCamCalib toolbox [ 173 ] employed in [ 167 ] for photometric registration using multiple cameras . For localizing light sources a possible approach is to build on the work in [ 58 ] , and apply DNNs to images obtained by AR devices or IoT cameras . Methods are also available to localize multiple AR devices ; cross - platform solutions include Azure Spatial Anchors [ 128 ] and Google Cloud Anchors [ 67 ] , while the ‘ARWorldMap’ feature can be used on ARKit [ 14 ] , and ‘Shared Spaces’ on the Magic Leap [ 120 ] . The above techniques provide both inspiration and readily implementable solutions for systems which employ AR and IoT devices . Based on our analysis of existing techniques , we identify a number of possible approaches for localizing and calibrating both AR and IoT devices , and consider their advantages and disadvantages . The solution that is perhaps most readily implemented for current commercial AR devices is to identify IoT devices within the frame of reference established by an AR platform . For example , the pose of devices could be recognized by displaying markers on them or attaching printed markers to them ( see Section 4 . 1 . 1 for background on spatial understanding using markers ) , although this may not be feasible for many small devices , and has downsides in terms of environment aesthetics . Alternatively , spatial anchors could be manually aligned with IoT devices [ 130 ] ; however , this approach could be both labor - intensive and time - consuming , and the accuracy that can be achieved requires further investigation , as it may be susceptible to human error or easily impacted by environmental factors . On the other hand , a diﬀerent research direction is to identify AR device poses in the views obtained by IoT cameras , aided by motion and visual cues from the AR device – this may be particularly useful for collaborative SLAM techniques ( e . g . , to inform loop closing ) . For the mapping of available environment control actions ( e . g . , increasing the brightness of a light bulb or lowering a smart blind ) to environment properties as detected by an IoT sensor or AR device , we plan to develop 26 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova eﬃcient sampling strategies for long - term monitoring of environment properties , such that we can automatically determine the necessary control actions to achieve speciﬁc conditions , and thereby optimal AR experiences . 6 . 3 Combining Data from Multiple AR Users and IoT Sensors Along with localizing and calibrating AR devices , IoT sensors and actuators , we must also consider how to combine the data from the variety of diﬀerent sensors that are available in the environments that host AR experiences . This sensor data is diverse , including multimodal sensor data of AR devices and IoT sensors captured from diﬀerent vantage points . Below we discuss three challenges related to combining data from multiple AR users and IoT sensors , and outline the associated research directions . Communication - eﬃcient AR with multiple users and multiple IoT sensors : In a multi - user , multi - sensor AR scenario , as the number of AR users and IoT sensors grows , the operational overhead of the IoT - supported system such as bandwidth consumption also scales [ 163 ] , due to the transmission of large amounts of contextual data from multiple users or their ambient environments . The system also needs to maintain a low user - perceived latency [ 11 ] to ensure seamless integration with the real world and synchronous perception of contextual information among diﬀerent users . To improve communication eﬃciency and reduce user - perceived latency , we envision developing intelligent network - adaptive AR - IoT solutions that adapt the size and frequency of data transmissions under changing network conditions . Since multiple users and IoT sensors that are in close proximity usually exhibit a strong correlation in the perceived contextual information , we also envision exploiting the collaboration among multiple AR users and IoT sensors for communication redundancy elimination . Assessment and processing of data with diﬀerent signal quality levels : Another research direction in this space is the quality assessment of data from diﬀerent devices and the development of robust approaches for combining this data . The signal quality levels obtained from diﬀerent vantage points will change over time due to a variety of factors ; for example , other noise sources will be interfering with signals captured by diﬀerent microphones , users of AR devices will move closer and farther from diﬀerent IoT - based cameras , and IoT sensors in unusual poses will cause the misbehavior of the object detection models that perform well for IoT sensors in canonical poses . We envision the real - time and in - situ evaluation of the quality levels of data captured by diﬀerent devices at diﬀerent times . After the data quality assessment , we will seek to combine signals with diﬀerent quality levels ; we envision designing algorithms that will adaptively amplify ‘good’ signals and place less weight on signals that are less useful , e . g . , by integrating the existing design of appropriate attention mechanisms [ 198 ] . Multimodal data : In IoT - supported AR systems , AR devices and IoT sensors continuously capture and process multimodal data ( e . g . , visual , auditory , haptic , olfactory , and thermal sensory information ) . One potential research direction is multimodal sensing and learning in which the visual modality enhances , or is enhanced by , other sensing modalities ; another is fusing sensor data with diﬀerent temporal and spatial traits . Given the heterogeneity of the collected data , we envision capturing correspondences and transferring knowledge between modalities to improve the performance of various aspects of AR func - tionality , including using gesture and speech for precise multimodal interaction , and using thermal , light , and visual sensors for detecting scene changes throughout the day . 6 . 4 Optimizing Environments with Conﬂicting Requirements Environment optimization systems which automatically adjust properties such as light and visual texture ( see Section 5 . 1 ) have the potential to facilitate AR experiences of both higher and more consistent quality . However , before these types of systems are ready for deployment , consideration must be given to how we manage conﬂicting environmental requirements . These conﬂicts , already a challenge for designers of spaces which host AR , will be commonplace in practical scenarios , as we describe below . Conﬂicting environmental requirements : Even in an environment in which just one AR user is present , adjusting envi - ronmental conditions to optimize one aspect of an AR experience may have negative eﬀects on another element of system functionality or the overall user experience . For example , the addition of visual texture to an environment may improve spatial understanding , but be distracting for the user [ 176 ] . For headsets with an OST display , the low level of illuminance which maximizes content visibility may result in a reduced level of performance for spatial understanding or eye - tracking Ambient Intelligence for Next - Generation AR 27 algorithms [ 177 ] . Furthermore , environmental requirements related to AR may also be at odds with constraints related to energy eﬃciency or the comfort of human occupants in general . Any environment optimization system will need to ( 1 ) deﬁne the characteristics of the AR devices , applications , and users to be served , in order to deﬁne a reasonable ‘operating range’ in which the system is functional and usable ; ( 2 ) apply other environmental constraints , e . g . , those imposed by building managers ; ( 3 ) implement real - time analysis of the system elements and environment regions users are interacting with , in order to determine and prioritize environment adjustments . This latter element is a particularly complex challenge , necessitating innovative solutions to manage continuously updating environment maps and device poses , and to avoid oscillations between environment states that may occur due to conﬂicting adjustments . Some of these issues were recently tackled in the context of shared control of public IoT actuators [ 95 ] , including device discovery , establishing ‘areas of interest’ for IoT devices , and aggregating conﬂicting requirements ; this work may serve as a valuable starting point for developing AR - speciﬁc systems . Heterogeneous AR users and devices : The environment optimization challenge becomes even more complex when we consider the diﬀering requirements of heterogeneous AR devices and users . Diﬀerent devices are equipped with diﬀerent sensors , run diﬀerent marker detection or VI - SLAM algorithms , and employ diﬀerent types of displays : smartphones may require greater levels of illuminance and texture to achieve acceptable tracking performance [ 174 ] , while OST displays may require lower illuminance than VST displays to achieve the same level of content visibility [ 20 ] . Heterogeneous AR users will have diﬀerent behavior patterns ( e . g . , mobility characteristics , eye gaze patterns , virtual content interactions ) , as well as diﬀerent expectations and requirements for the properties of virtual content . For example , school children exploring an AR science exhibit may move rapidly , prompting the addition of environmental textures to maintain high - quality pose tracking . On the other hand , an academic carefully examining virtual content in the same environment might require a view that is uninhibited by a textured background , and also be more concerned with the stability of virtual content . While knowledge of the type of AR devices present in an environment is likely to be relatively straightforward in most cases , capturing and analyzing information on the properties of AR users is a more complex task , which will require ongoing analysis of both individual users and user populations . This in turn will have signiﬁcant security and privacy implications , which we discuss next in Section 6 . 5 . 6 . 5 Secure and Privacy - Preserving AR - IoT Platforms The development of secure and privacy - preserving AR - IoT platforms is paramount for achieving widespread societal accep - tance of AR systems incorporated with ambient intelligence technology . Security : The increase in the number of connected AR and IoT devices and the richness of data collected by them bring security concerns to AR - IoT platforms . Building upon the categorization adopted in [ 100 , 166 ] , we classify concerns as related to security of AR devices [ 41 , 100 , 166 , 169 ] and IoT sensors and actuators [ 9 , 142 ] as related to input security or output security . Input security challenges are well known in traditional arenas of networked and computing systems , but become even more important for AR - IoT platforms with rich multimodal inputs ( e . g . , RGB , depth , inertial , haptic , auditory sensor readings , and even gaze tracking data ) of AR and IoT devices . Compromised inputs will degrade the performance of AR - IoT platforms , including localization and tracking accuracy , rendering quality , and the accuracy and completeness of environmental aware - ness . To mitigate input security risks , potential research directions include designing input validation and sanitization methods that apply to multimodal data from AR and IoT devices , and building fault - tolerant AR - IoT platforms robust to corrupted input data . AR outputs produced by malicious or bug - ridden applications can also be potentially harmful or distracting . For example , in the case of industrial AR in a smart warehouse , it would be dangerous for a visual overlay to obstruct the operator’s view ; IoT actuators that generate ﬂashing visuals , shrill sounds , or intense haptic signals could cause physiological damage to the user . To mitigate output security risks , inspired by existing works on adaptive policies to secure visual outputs in AR devices [ 3 , 41 ] , we envision the development of reinforcement learning - based policies that prevent distraction due to tam - pered content of AR devices or IoT actuators , with these policies able to adapt to dynamic environments through trial - and - error . Privacy : AR - IoT systems’ need for rich , continuous sensor data ( from both AR and IoT devices ) raises privacy concerns for both users and bystanders in the AR environment . Buggy or malicious applications may record privacy - sensitive information on user states or the surrounding AR environment by compromising the AR devices [ 209 ] or IoT sensors [ 194 ] . To mitigate privacy risks , we envision enforcing privacy - preserving policies to limit access to potentially sensitive sensor data . For example , we will anonymize human faces with an extremely low resolution before feeding images to object detection applications running on AR or IoT devices ; we will seek to prevent applications from inferring sensitive information about an AR environment ( e . g . , the contents of a home ) by concealing information contained in the point cloud constructed by VI - SLAM algorithms . 28 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 7 Conclusion In this book chapter , we explored the variety of ways in which ambient IoT sensors and actuators could be used to support next - generation AR . We categorized these uses by deﬁning ﬁve diﬀerent aspects of AR system functionality or user experience that may be enhanced – spatial understanding , semantic understanding , contextualized content , interaction , and immersion – and provided an overview of relevant IoT devices ( Section 3 ) . We then examined the possibilities for each of these categories in detail , when employing IoT sensors ( Section 4 ) and actuators ( Section 5 ) . Finally , we discussed a number of research directions related to implementing ambient IoT - supported AR systems , along with associated challenges and opportunities for future work ( Section 6 ) . One important point to make , having reviewed multiple diﬀerent uses of IoT sensors and actuators , is that the devices physically deployed in a given environment will likely be used for multiple purposes concurrently ( e . g . , images from a single IoT camera may serve as input to spatial understanding , semantic understanding , photometric registration , and visual texture estimation algorithms ) , so consideration will need to be given to balancing the needs of each use in these cases . Similarly , our vision for ambient IoT for AR in general , is that it is combined with the use of ambient IoT devices for other purposes ; for example , IoT - based systems incorporating devices also applicable to AR have been proposed for energy eﬃciency [ 171 ] , occupant comfort and productivity [ 19 ] , surveillance [ 214 ] and building ventilation [ 35 ] , while in [ 196 ] the authors consider the role of ubiquitous displays in environment aesthetics . Moreover , as AR becomes more and more integrated into our lives in the coming years , we envision that built environments , products and materials will be designed with ambient intelligence for AR in mind , in order to support the use cases we have laid out in this book chapter . 8 Acknowledgements We thank Guohao Lan , Zida Liu , Yunfan Zhang , Jovan Stojkovic , Achilles Dabrowski , Alex Xu , Ritvik Janamsetty , Tiﬀany Ma , Owen Gibson , Michael Glushakov and Joseph DeChicchis for their contributions to this work . This work was supported in part by NSF CAREER Award IIS - 2046072 , NSF grants CNS - 2112562 and CNS - 1908051 , Facebook Research Award , IBM Research Award , and a Thomas Lord Educational Innovation Grant . References 1 . R . Acuna and V . Willert . Dynamic markers : UAV landing proof of concept . In Proceedings of IEEE Latin American Robotic Symposium , Brazilian Symposium on Robotics ( SBR ) and Workshop on Robotics in Education ( WRE ) , 2018 . 2 . A . Ahmadyan , L . Zhang , A . Ablavatski , J . Wei , and M . Grundmann . Objectron : A large scale dataset of object - centric videos in the wild with pose annotations . In Proceedings of IEEE / CVF CVPR , 2021 . 3 . S . Ahn , M . Gorlatova , P . Naghizadeh , M . Chiang , and P . Mittal . Adaptive fog - based output security for augmented reality . In Proceedings of ACM SIGCOMM VR / AR Network Workshop , 2018 . 4 . K . Ahuja , S . Pareddy , R . Xiao , M . Goel , and C . Harrison . LightAnchors : Appropriating point lights for spatially - anchored augmented reality interfaces . In Proceedings of ACM UIST , 2019 . 5 . A . J . B . Ali , Z . S . Hashemifar , and K . Dantu . Edge - SLAM : Edge - assisted visual simultaneous localization and mapping . In Proceedings of ACM MobiSys , 2020 . 6 . Amazon . Amazon AR view . https : / / www . amazon . com / adlp / arview , 2023 . 7 . J . Anaya and A . Barbu . Renoir – a dataset for real low - light image noise reduction . Journal of Visual Communication and Image Representation , 51 : 144 – 154 , 2018 . 8 . S . Andrist , M . Gleicher , andB . Mutlu . Lookingcoordinated : Bidirectionalgazemechanismsforcollaborativeinteractionwithvirtualcharacters . In Proceedings of ACM CHI , 2017 . 9 . M . Antonakakis , T . April , M . Bailey , M . Bernhard , E . Bursztein , J . Cochran , Z . Durumeric , J . A . Halderman , L . Invernizzi , M . Kallitsis , et al . Understanding the Mirai botnet . In Proceedings of USENIX Security , 2017 . 10 . A . Anwer , S . Ainouz , M . N . M . Saad , S . S . A . Ali , and F . Meriaudeau . Specseg network for specular highlight detection and segmentation in real - world images . Sensors , 22 ( 17 ) : 6552 , 2022 . 11 . K . Apicharttrisorn , B . Balasubramanian , J . Chen , R . Sivaraj , Y . - Z . Tsai , R . Jana , S . Krishnamurthy , T . Tran , and Y . Zhou . Characterization of multi - user augmented reality over cellular networks . In Proceedings of IEEE SECON , 2020 . 12 . K . Apicharttrisorn , X . Ran , J . Chen , S . V . Krishnamurthy , and A . K . Roy - Chowdhury . Frugal following : Power thrifty object detection and tracking for mobile augmented reality . In Proceedings of ACM SenSys , 2019 . 13 . Apple . ARKit . https : / / developer . apple . com / augmented - reality / , 2023 . 14 . Apple . ARWorldMap . https : / / developer . apple . com / documentation / arkit / arworldmap , 2023 . 15 . Apple . Scanning and detecting 3D objects . https : / / developer . apple . com / documentation / arkit / content _ anchors / scanning _ and _ detecting _ 3d _ objects , 2023 . 16 . N . S . Archer , A . Bluﬀ , A . Eddy , C . K . Nikhil , N . Hazell , D . Frank , and A . Johnston . Odour enhances the sense of presence in a virtual reality environment . Plos one , 17 ( 3 ) : e0265039 , 2022 . Ambient Intelligence for Next - Generation AR 29 17 . ARToolkit . ARToolkit . http : / / www . artoolkitx . org / , 2023 . 18 . ArUco . ArUco . https : / / pypi . org / project / aruco / , 2023 . 19 . A . Aryal , F . Anselmo , and B . Becerik - Gerber . Smart IoT desk for personalizing indoor environmental conditions . In Proceedings of ACM IoT , 2018 . 20 . G . Ballestin , F . Solari , and M . Chessa . Perception and action in peripersonal space : A comparison between video and optical see - through augmented reality devices . In Proceedings of ISMAR - Adjunct , 2018 . 21 . J . Baumeister , S . Y . Ssin , N . A . ElSayed , J . Dorrian , D . P . Webb , J . A . Walsh , T . M . Simon , A . Irlitti , R . T . Smith , M . Kohler , et al . Cognitive cost of using augmented reality displays . IEEE Transactions on Visualization and Computer Graphics , 23 ( 11 ) : 2378 – 2388 , 2017 . 22 . P . N . Beuchat , H . Hesse , A . Domahidi , and J . Lygeros . Enabling optimization - based localization for IoT devices . IEEE Internet of Things Journal , 6 ( 3 ) : 5639 – 5650 , 2019 . 23 . M . Bhattarai , A . R . Jensen - Curtis , and M . Martínez - Ramón . An embedded deep learning system for augmented reality in ﬁreﬁghting applications . In Proceedings of IEEE ICMLA , 2020 . 24 . Ó . Blanco - Novoa , P . Fraga - Lamas , M . A Vilar - Montesinos , and T . M . Fernández - Caramés . Creating the Internet of augmented things : An open - source framework to make IoT devices and augmented and mixed reality systems talk to each other . Sensors , 20 ( 11 ) : 3328 , 2020 . 25 . L . Bonanni , C . - H . Lee , and T . Selker . Attention - based design of augmented reality interfaces . In CHI Extended Abstracts , 2005 . 26 . M . Burri , J . Nikolic , P . Gohl , T . Schneider , J . Rehder , S . Omari , M . W . Achtelik , and R . Siegwart . The EuRoC micro aerial vehicle datasets . The International Journal of Robotics Research , 35 ( 10 ) : 1157 – 1163 , 2016 . 27 . C . Cadena , L . Carlone , H . Carrillo , Y . Latif , D . Scaramuzza , J . Neira , I . Reid , and J . J . Leonard . Past , present , and future of simultaneous localization and mapping : Toward the robust - perception age . IEEE Transactions on Robotics , 32 ( 6 ) : 1309 – 1332 , 2016 . 28 . C . Campos , R . Elvira , J . J . G . Rodríguez , J . M . Montiel , and J . D . Tardós . ORB - SLAM3 : An accurate open - source library for visual , visual – inertial , and multimap SLAM . IEEE Transactions on Robotics , pp . 1 – 17 , 2021 . 29 . L . M . Castle , D . A . Schuh , E . E . Reynolds , and A . L . Furst . Electrochemical sensors to detect bacterial foodborne pathogens . ACS sensors , 6 ( 5 ) : 1717 – 1730 , 2021 . 30 . Chaos . V - Ray 3D rendering software . https : / / www . chaos . com / 3d - rendering - software , 2022 . 31 . D . Chatzopoulos , C . Bermejo , Z . Huang , and P . Hui . Mobile augmented reality survey : From where we are to where we go . IEEE Access , 5 : 6917 – 6950 , 2017 . 32 . P . Chen , Z . Peng , D . Li , and L . Yang . An improved augmented reality system based on AndAR . Journal of Visual Communication and Image Representation , 37 : 63 – 69 , 2016 . 33 . Y . Chen , H . Inaltekin , and M . Gorlatova . AdaptSLAM : Edge - assisted adaptive SLAM with resource constraints via uncertainty minimization . In Proceedings of IEEE INFOCOM , 2023 . 34 . Y . Cheng , Y . Yan , X . Yi , Y . Shi , and D . Lindlbauer . SemanticAdapt : Optimization - based adaptation of mixed reality layouts leveraging virtual - physical semantic connections . In Proceedings of ACM UIST , 2021 . 35 . B . Chhaglani , C . Zakaria , A . Lechowicz , J . Gummeson , and P . Shenoy . FlowSense : Monitoring airﬂow in building ventilation systems using audio sensing . Proceedings of ACM IMWUT , 6 ( 1 ) : 1 – 26 , 2022 . 36 . E . Choi , S . Kim , J . Gong , H . Sun , M . Kwon , H . Seo , O . Sul , and S . - B . Lee . Tactile interaction sensor with millimeter sensing acuity . Sensors , 21 ( 13 ) : 4274 , 2021 . 37 . S . Choudhary , N . Sekhar , S . Mahendran , and P . Singhal . Multi - user , scalable 3D object detection in AR cloud . In Proceedings of IEEE CVPR Workshop on Computer Vision for Augmented and Virtual Reality , 2020 . 38 . E . M . Coelho , S . Julier , and B . Maclntyre . Osgar : A scene graph with uncertain transformations . In Proceedings of IEEE / ACM ISMAR , 2004 . 39 . L . M . Dang , K . Min , H . Wang , M . J . Piran , C . H . Lee , andH . Moon . Sensor - basedandvision - basedhumanactivityrecognition : Acomprehensive survey . Pattern Recognition , 108 : 107561 , 2020 . 40 . S . Debernardis , M . Fiorentino , M . Gattullo , G . Monno , and A . E . Uva . Text readability in head - worn displays : Color and style optimization in video versus optical see - through devices . IEEE Transactions on Visualization and Computer Graphics , 20 ( 1 ) : 125 – 139 , 2013 . 41 . J . DeChicchis , S . Ahn , and M . Gorlatova . Adaptive AR visual output security using reinforcement learning trained policies : Demo abstract . In Proceedings of ACM SenSys , 2019 . 42 . H . Q . Dinh , N . Walker , L . F . Hodges , C . Song , and A . Kobayashi . Evaluating the importance of multi - sensory input on memory and the sense of presence in virtual environments . In Proceedings of IEEE VR , 1999 . 43 . B . Eckstein , E . Krapp , A . Elsässer , and B . Lugrin . Smart substitutional reality : Integrating the smart home into virtual reality . Entertainment Computing , 31 : 100306 , 2019 . 44 . D . Englmeier , J . Dörner , A . Butz , and T . Höllerer . A tangible spherical proxy for object manipulation in augmented reality . In IEEE VR , 2020 . 45 . S . Eom , M . Hadziahmetovic , M . Pajic , and M . Gorlatova . Demo Abstract : Through an AR lens : Augmented reality magniﬁcation through feature detection and matching . In Proceedings of ACM SenSys , 2022 . 46 . S . Eom , S . Kim , S . Rahimpour , and M . Gorlatova . AR - assisted surgical guidance system for ventriculostomy . In Proceedings of IEEE VR Workshops , 2022 . 47 . S . Eom , D . Sykes , S . Rahimpour , and M . Gorlatova . NeuroLens : Augmented reality - based contextual guidance through surgical tool tracking in neurosurgery . In Proceedings of IEEE ISMAR , 2022 . 48 . Epic Games . Unreal engine . https : / / www . unrealengine . com / , 2023 . 49 . A . Erickson , K . Kim , G . Bruder , and G . F . Welch . Exploring the limitations of environment lighting on optical see - through head - mounted displays . In Proceedings of ACM SUI , 2020 . 50 . Fever . Van Gogh : The immersive experience . https : / / vangoghexpo . com / , 2023 . 51 . M . Fiala . Designing highly reliable ﬁducial markers . IEEE Transactions on Pattern Analysis and Machine Intelligence , 32 ( 7 ) : 1317 – 1324 , 2009 . 52 . T . Frantz , B . Jansen , J . Duerinck , and J . Vandemeulebroucke . Augmenting Microsoft’s HoloLens with Vuforia tracking for neuronavigation . Healthcare Technology Letters , 5 ( 5 ) : 221 – 225 , 2018 . 53 . J . L . Gabbard , M . Smith , C . Merenda , G . Burnett , and D . R . Large . A perceptual color - matching method for examining color blending in augmented reality head - up display graphics . IEEE Transactions on Visualization and Computer Graphics , 28 : 2834 – 2851 , 2022 . 54 . J . L . Gabbard , J . E . Swan , D . Hix , S . - J . Kim , and G . Fitch . Active text drawing styles for outdoor augmented reality : A user - based study and design implications . In Proceedings of IEEE VR , 2007 . 30 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 55 . J . L . Gabbard , J . E . Swan , and A . Zarger . Color blending in outdoor optical see - through AR : The eﬀect of real - world backgrounds on user interface color . In Proceedings of IEEE VR , 2013 . 56 . R . Gal , L . Shapira , E . Ofek , and P . Kohli . FLARE : Fast layout for augmented reality applications . In Proceedings of IEEE ISMAR , 2014 . 57 . J . Gao , N . Zhang , D . Ji , H . Song , Y . Liu , L . Zhou , Z . Sun , J . M . Jornet , A . C . Thompson , R . L . Collins , et al . Superabsorbing metasurfaces with hybrid ag – au nanostructures for surface - enhanced raman spectroscopy sensing of drugs and chemicals . Small Methods , 2 ( 7 ) : 1800045 , 2018 . 58 . M . - A . Gardner , K . Sunkavalli , E . Yumer , X . Shen , E . Gambaretto , C . Gagné , and J . - F . Lalonde . Learning to predict indoor illumination from a single image . arXiv preprint arXiv : 1704 . 00090 , 2017 . 59 . S . Garrido - Jurado , R . Muñoz - Salinas , F . J . Madrid - Cuevas , and M . J . Marín - Jiménez . Automatic generation and detection of highly reliable ﬁducial markers under occlusion . Pattern Recognition , 47 ( 6 ) : 2280 – 2292 , 2014 . 60 . F . Ghorbani , A . Ahmadi , M . Kia , Q . Rahman , and M . Delrobaei . A decision - aware ambient assisted living system with IoT embedded device for in - home monitoring of older adults . Sensors , 23 ( 5 ) : 2673 , 2023 . 61 . S . Ghorpade , M . Zennaro , and B . Chaudhari . Survey of localization for Internet of Things nodes : Approaches , challenges and open issues . Future Internet , 13 ( 8 ) : 210 , 2021 . 62 . G . Giraldo , M . Servières , and G . Moreau . Towards a sensitive urban wind representation in virtual reality . ISPRS International Journal of Geo - Information , 11 ( 4 ) : 239 , 2022 . 63 . M . Glushakov , Y . Zhang , Y . Han , T . J . Scargill , G . Lan , and M . Gorlatova . Edge - based provisioning of holographic content for contextual and personalized augmented reality . In Proceedings of IEEE PerCom Workshops , 2020 . 64 . D . Gooch . An Investigation into Communicating Social Presence With Thermal Devices . PhD thesis , MSc Dissertation , 2009 . 65 . Google . ARCore . https : / / arvr . google . com / arcore / , 2023 . 66 . Google . ARCore : Get the lighting right . https : / / developers . google . com / ar / develop / lighting - estimation , 2023 . 67 . Google . Cloud Anchors . https : / / developers . google . com / ar / develop / cloud - anchors , 2023 . 68 . L . Gruber , T . Richter - Trummer , and D . Schmalstieg . Real - time photometric registration from arbitrary geometry . In Proceedings of IEEE ISMAR , 2012 . 69 . J . Grubert , T . Langlotz , S . Zollmann , and H . Regenbrecht . Towards pervasive augmented reality : Context - awareness in augmented reality . IEEE Transactions on Visualization and Computer Graphics , 23 ( 6 ) : 1706 – 1724 , 2017 . 70 . Y . Guan , X . Hou , N . Wu , B . Han , and T . Han . DeepMix : Mobility - aware , lightweight , and hybrid 3D object detection for headsets . In Proceedings of ACM MobiSys , 2022 . 71 . H . - J . Guo and B . Prabhakaran . HoloLens 2 technical evaluation as mixed reality guide . arXiv preprint arXiv : 2207 . 09554 , 2022 . 72 . D . Hallaway , S . Feiner , andT . Höllerer . Bridgingthegaps : Hybridtrackingforadaptivemobileaugmentedreality . AppliedArtiﬁcialIntelligence , 18 ( 6 ) : 477 – 500 , 2004 . 73 . M . Hansard , S . Lee , O . Choi , and R . Horaud . Time - of - ﬂight Cameras : Principles , Methods and Applications . Springer , 2012 . 74 . S . Herrmann . Object detection with Microsoft HoloLens 2 . A comparison between image and point cloud based algorithms . PhD thesis , TU Wien , 2021 . 75 . M . Hirzer . Marker detection for augmented reality applications . In Seminar / Project Image Analysis Graz , vol . 25 , 2008 . 76 . M . Hoppe , D . Oskina , A . Schmidt , and T . Kosch . Odin’s helmet : A head - worn haptic feedback device to simulate g - forces on the human body in virtual reality . Proceedings of ACM on Human - Computer Interaction , 5 ( EICS ) : 1 – 15 , 2021 . 77 . HTC . Vive tracker . https : / / www . vive . com / us / accessory / tracker3 / , 2023 . 78 . M . Hu , D . Weng , F . Chen , and Y . Wang . Object detecting augmented reality system . In Proceedings of IEEE ICCT , 2020 . 79 . X . Huang , G . Mei , J . Zhang , and R . Abbas . A comprehensive survey on point cloud registration . arXiv preprint arXiv : 2103 . 02690 , 2021 . 80 . K . Huo , Y . Cao , S . H . Yoon , Z . Xu , G . Chen , and K . Ramani . Scenariot : Spatially mapping smart things within augmented reality scenes . In Proceedings of ACM CHI , 2018 . 81 . B . Huynh , J . Orlosky , and T . Höllerer . In - situ labeling for augmented reality language learning . In Proceedings of IEEE VR , 2019 . 82 . IKEA . IKEA Place app . https : / / www . ikea . com / us / en / customer - service / mobile - apps / , 2023 . 83 . L . Jinyu , Y . Bangbang , C . Danpeng , W . Nan , Z . Guofeng , and B . Hujun . Survey and evaluation of monocular visual - inertial SLAM algorithms for augmented reality . Virtual Reality & Intelligent Hardware , 1 ( 4 ) : 386 – 410 , 2019 . 84 . D . Jo and G . J . Kim . ARIoT : Scalable augmented reality framework for interacting with Internet of Things appliances everywhere . IEEE Transactions on Consumer Electronics , 62 ( 3 ) : 334 – 340 , 2016 . 85 . D . Jo and G . J . Kim . IoT + AR : Pervasive and augmented environments for “Digi - log” shopping experience . Human - centric Computing and Information Sciences , 9 ( 1 ) : 1 – 17 , 2019 . 86 . D . Kahl , M . Ruble , and A . Krüger . The inﬂuence of environmental lighting on size variations in optical see - through tangible augmented reality . In Proceedings of IEEE VR , 2022 . 87 . M . Kalaitzakis , B . Cain , S . Carroll , A . Ambrosi , C . Whitehead , and N . Vitzilaios . Fiducial markers for pose estimation . Journal of Intelligent & Robotic Systems , 101 ( 4 ) : 1 – 26 , 2021 . 88 . M . Kanbara and N . Yokoya . Real - time estimation of light source environment for photorealistic augmented reality . In Proceedings of ICPR , 2004 . 89 . M . Karrer , P . Schmuck , and M . Chli . CVI - SLAM - collaborative visual - inertial SLAM . IEEE Robotics and Automation Letters , 3 ( 4 ) : 2762 – 2769 , 2018 . 90 . D . Khan , S . Ullah , and I . Rabbi . Factors aﬀecting the design and tracking of ARToolKit markers . Computer Standards & Interfaces , 41 : 56 – 66 , 2015 . 91 . D . Kim , Y . Cao , D . Mariappan , M . S . Bono Jr , A . J . Hart , and B . Marelli . A microneedle technology for sampling and sensing bacteria in the food supply chain . Advanced Functional Materials , 31 ( 1 ) : 2005370 , 2021 . 92 . H . Kim , Y . - T . Kwon , H . - R . Lim , J . - H . Kim , Y . - S . Kim , and W . - H . Yeo . Recent advances in wearable sensors and integrated functional devices for virtual and augmented reality applications . Advanced Functional Materials , 31 ( 39 ) : 2005692 , 2021 . 93 . K . Kim , L . Boelling , S . Haesler , J . Bailenson , G . Bruder , and G . F . Welch . Does a digital assistant need a body ? the inﬂuence of visual embodiment and social behavior on the perception of intelligent virtual agents in AR . In Proceedings of IEEE ISMAR , 2018 . 94 . K . Kim , G . Bruder , and G . F . Welch . Blowing in the wind : Increasing copresence with a virtual human via airﬂow inﬂuence in augmented reality . In Proceedings of ICAT - EGVE , 2018 . 95 . W . Kim , S . Lee , Y . Chang , T . Lee , I . Hwang , and J . Song . Hivemind : Social control - and - use of IoT towards democratization of public spaces . In Proceedings of ACM MobiSys , 2021 . Ambient Intelligence for Next - Generation AR 31 96 . Y . - J . Kim , R . Kumaran , E . Sayyad , A . Milner , T . Bullock , B . Giesbrecht , and T . Höllerer . Investigating search among physical and virtual objects under diﬀerent lighting conditions . IEEE Transactions on Visualization and Computer Graphics , 28 ( 11 ) : 3788 – 3798 , 2022 . 97 . K . Koide , J . Miura , M . Yokozuka , S . Oishi , and A . Banno . Interactive 3D graph SLAM for map correction . IEEE Robotics and Automation Letters , 6 ( 1 ) : 40 – 47 , 2021 . 98 . D . Kurz and S . Benhimane . Gravity - aware handheld augmented reality . In Proceedings of IEEE ISMAR , 2011 . 99 . H . Le , M . Nguyen , W . Q . Yan , and H . Nguyen . Augmented reality and machine learning incorporation using YOLOv3 and ARKit . Applied Sciences , 11 ( 13 ) : 6006 , 2021 . 100 . K . Lebeck , K . Ruth , T . Kohno , and F . Roesner . Towards security and privacy for multi - user augmented reality : Foundations with end users . In Proceedings of IEEE S & P , 2018 . 101 . C . H . Lee . Location - aware speakers for the virtual reality environments . IEEE Access , 5 : 2636 – 2640 , 2017 . 102 . J . Lee , J . Bang , and S . - I . Yang . Object detection with sliding window in images including multiple similar objects . In Proceedings of IEEE ICTC , 2017 . 103 . T . Lee and T . Hollerer . Hybrid feature tracking and user interaction for markerless augmented reality . In Proceedings of IEEE VR , 2008 . 104 . T . Leppänen , A . Heikkinen , A . Karhu , E . Harjula , J . Riekki , and T . Koskela . Augmented reality web applications with mobile agents in the Internet of Things . In Proceedings of IEEE International Conference on Next Generation Mobile Apps , Services and Technologies , 2014 . 105 . X . Li , Y . Tian , F . Zhang , S . Quan , and Y . Xu . Object detection in the context of mobile augmented reality . In Proceedings of IEEE ISMAR , 2020 . 106 . Y . Li , W . Zhou , B . Zu , and X . Dou . Qualitative detection toward military and improvised explosive vapors by a facile tiO2 nanosheet - based chemiresistive sensor array . Frontiers in Chemistry , 8 : 29 , 2020 . 107 . C . - C . Lin . Eﬀects of contrast ratio and text color on visual performance with TFT - LCD . International journal of industrial ergonomics , 31 ( 2 ) : 65 – 72 , 2003 . 108 . M . A . Lin , A . F . Siu , J . H . Bae , M . R . Cutkosky , and B . L . Daniel . HoloNeedle : Augmented reality guidance system for needle placement investigating the advantages of three - dimensional needle shape reconstruction . IEEE Robotics and Automation Letters , 3 ( 4 ) : 4156 – 4162 , 2018 . 109 . P . Lin , Q . Song , D . Wang , F . R . Yu , L . Guo , and V . C . Leung . Resource management for pervasive - edge - computing - assisted wireless VR streaming in industrial Internet of Things . IEEE Transactions on Industrial Informatics , 17 ( 11 ) : 7607 – 7617 , 2021 . 110 . D . Lindlbauer , A . M . Feit , and O . Hilliges . Context - aware online adaptation of mixed reality interfaces . In Proceedings of ACM UIST , 2019 . 111 . H . LiuandL . Wang . Gesturerecognitionforhuman - robotcollaboration : Areview . InternationalJournalofIndustrialErgonomics , 68 : 355 – 367 , 2018 . 112 . L . Liu , H . Li , and M . Gruteser . Edge assisted real - time object detection for mobile augmented reality . In Proceedings of ACM MobiCom , 2019 . 113 . M . Liu , Y . Zhang , J . Wang , N . Qin , H . Yang , K . Sun , J . Hao , L . Shu , J . Liu , Q . Chen , et al . A star - nose - like tactile - olfactory bionic sensing array for robust object recognition in non - visual environments . Nature Communications , 13 ( 1 ) : 1 – 10 , 2022 . 114 . Z . Liu , G . Lan , J . Stojkovic , Y . Zhang , C . Joe - Wong , and M . Gorlatova . CollabAR : Edge - assisted collaborative image recognition for mobile augmented reality . In Proceedings of IEEE IPSN , 2020 . 115 . J . A . G . Macias , J . Alvarez - Lozano , P . Estrada , and E . A . Lopez . Browsing the Internet of Things with sentient visors . Computer , 44 ( 5 ) : 46 – 52 , 2011 . 116 . B . MacIntyre and E . M . Coelho . Adapting to dynamic registration errors using level of error ( LOE ) ﬁltering . In Proceedings of IEEE / ACM ISMAR , 2000 . 117 . B . MacIntyre , E . M . Coelho , and S . J . Julier . Estimating and adapting to registration errors in augmented reality systems . In Proceedings of IEEE VR , 2002 . 118 . T . Maeda , S . Yoshida , T . Murakami , K . Matsuda , T . Tanikawa , and H . Sakai . Fingeret : A wearable ﬁngerpad - free haptic device for mixed reality . In Proceedings of ACM SUI , 2022 . 119 . Magic Leap . Magic Leap 2 device . https : / / www . magicleap . com / magic - leap - 2 , 2023 . 120 . Magic Leap . Shared Spaces . https : / / www . magicleap . com / spatial - mapping - ml2 , 2023 . 121 . D . Mandl , K . M . Yi , P . Mohr , P . M . Roth , P . Fua , V . Lepetit , D . Schmalstieg , and D . Kalkofen . Learning lightprobes for mixed reality illumination . In Proceedings of IEEE ISMAR , 2017 . 122 . E . Marchand , H . Uchiyama , and F . Spindler . Pose estimation for augmented reality : A hands - on survey . IEEE Transactions on Visualization and Computer Graphics , 22 ( 12 ) : 2633 – 2651 , 2015 . 123 . K . Masai , K . Kunze , D . Sakamoto , Y . Sugiura , and M . Sugimoto . Face commands - user - deﬁned facial gestures for smart glasses . In Proceedings of IEEE ISMAR , 2020 . 124 . K . Matkovic , T . Psik , I . Wagner , and D . Gracanin . Dynamic texturing of real objects in an augmented reality system . IEEE , 2005 . 125 . L . Meli , C . Pacchierotti , G . Salvietti , F . Chinello , M . Maisto , A . DeLuca , andD . Prattichizzo . Combiningwearableﬁngerhapticsandaugmented reality : User evaluation using an external camera and the Microsoft HoloLens . IEEE Robotics and Automation Letters , 3 ( 4 ) : 4297 – 4304 , 2018 . 126 . N . Merrill , P . Geneva , and G . Huang . Robust monocular visual - inertial depth completion for embedded systems . In Proceedings of IEEE ICRA , 2021 . 127 . K . Michalakis , J . Aliprantis , and G . Caridakis . Visualizing the Internet of Things : Naturalizing human - computer interaction by incorporating AR features . IEEE Consumer Electronics Magazine , 7 ( 3 ) : 64 – 72 , 2018 . 128 . Microsoft . Azure Spatial Anchors . https : / / azure . microsoft . com / en - us / services / spatial - anchors / , 2023 . 129 . Microsoft . Microsoft HoloLens . https : / / www . microsoft . com / en - us / hololens , 2023 . 130 . Microsoft . Spatial Anchors . https : / / learn . microsoft . com / en - us / windows / mixed - reality / design / spatial - anchors , 2023 . 131 . A . G . Millard , R . A . Joyce , and I . Gray . Human - swarm interaction via E - Ink displays . In Proceedings of ICRA Workshops , 2020 . 132 . V . T . Minh , N . Katushin , and J . Pumwa . Motion tracking glove for augmented reality and virtual reality . Paladyn , Journal of Behavioral Robotics , 10 ( 1 ) : 160 – 166 , 2019 . 133 . M . H . Miraz , M . Ali , and P . S . Excell . Adaptive user interfaces and universal usability through plasticity of user interface design . Computer Science Review , 40 : 100363 , 2021 . 134 . A . Mishra , P . Kumar , J . Shukla , and A . Parnami . HaptiDrag : A device with the ability to generate varying levels of drag ( friction ) eﬀects on real surfaces . Proceedings of ACM IMWUT , 6 ( 3 ) : 1 – 26 , 2022 . 135 . Z . Mohammadi and S . M . Jafari . Detection of food spoilage and adulteration by novel nanomaterial - based sensors . Advances in Colloid and Interface Science , 286 : 102297 , 2020 . 32 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 136 . E . Morotti , L . Donatiello , and G . Marﬁa . Fostering fashion retail experiences through virtual reality and voice assistants . In Proceedings of IEEE VR Abstracts and Workshops , 2020 . 137 . J . Morrish and M . Arnott . Global IoT forecast report , 2021 - 2030 . https : / / transformainsights . com / research / reports / global - iot - forecast - report - 2030 , 2022 . 138 . R . Mur - Artal and J . D . Tardós . ORB - SLAM2 : An open - source SLAM system for monocular , stereo , and RGB - D cameras . IEEE Transactions on Robotics , 33 ( 5 ) : 1255 – 1262 , 2017 . 139 . I . Nahum - Shani , S . N . Smith , B . J . Spring , L . M . Collins , K . Witkiewitz , A . Tewari , and S . A . Murphy . Just - in - time adaptive interventions ( JITAIs ) in mobile health : Key components and design principles for ongoing health behavior support . Annals of Behavioral Medicine , 52 ( 6 ) : 446 – 462 , 2018 . 140 . W . Natephra and A . Motamedi . Live data visualization of IoT sensors using augmented reality ( AR ) and BIM . In Proceedings of International Symposium on Automation and Robotics in Construction ( ISARC ) , 2019 . 141 . B . W . Nelson and N . B . Allen . Extending the passive - sensing toolbox : Using smart - home technology in psychological science . Perspectives on Psychological Science , 13 ( 6 ) : 718 – 733 , 2018 . 142 . N . Neshenko , E . Bou - Harb , J . Crichigno , G . Kaddoum , and N . Ghani . Demystifying IoT security : An exhaustive survey on IoT vulnerabilities and a ﬁrst empirical look on internet - scale IoT exploitations . IEEE Communications Surveys & Tutorials , 21 ( 3 ) : 2702 – 2733 , 2019 . 143 . L . H . Nguyen , S . Naﬁcy , R . McConchie , F . Dehghani , and R . Chandrawati . Polydiacetylene - based sensors to detect food spoilage at low temperatures . Journal of Materials Chemistry C , 7 ( 7 ) : 1919 – 1926 , 2019 . 144 . Niantic . Pokemon GO app . https : / / www . pokemongo . com / , 2023 . 145 . B . Nuernberger , E . Ofek , H . Benko , and A . D . Wilson . SnapToReality : Aligning augmented reality to the real world . In Proceedings of ACM CHI , 2016 . 146 . E . Olson . AprilTag : A robust and ﬂexible visual ﬁducial system . In Proceedings of IEEE ICRA , 2011 . 147 . T . Ophoﬀ , K . Van Beeck , and T . Goedemé . Exploring RGB + Depth fusion for real - time object detection . Sensors , 19 ( 4 ) : 866 , 2019 . 148 . OptiTrack . OptiTrack . https : / / optitrack . com / , 2023 . 149 . F . Pankratz , A . Dippon , T . Coskun , and G . Klinker . User awareness of tracking uncertainties in ar navigation scenarios . In Proceedings of IEEE ISMAR , 2013 . 150 . G . Paolocci , T . L . Baldi , D . Barcelli , and D . Prattichizzo . Combining wristband display and wearable haptics for augmented reality . In Proceedings of IEEE VR Workshops , 2020 . 151 . Y . Park , S . Yun , and K . - H . Kim . When IoT met augmented reality : Visualizing the source of the wireless signal in AR view . In Proceedings of ACM MobiSys , 2019 . 152 . R . L . Peiris , O . N . N . Fernando , C . S . Bee , A . D . Cheok , A . G . Ganesan , and P . Kumarasinghe . dMarkers : Ubiquitous dynamic makers for augmented reality . In Proceedings of ACM VRCAI , 2011 . 153 . S . Persky and A . P . Dolwick . Olfactory perception and presence in a virtual reality food environment . Frontiers in Virtual Reality , 1 : 571812 , 2020 . 154 . P . Phupattanasilp and S . - R . Tong . Augmented reality in the integrative Internet of Things ( AR - IoT ) : Application for precision farming . Sustainability , 11 ( 9 ) : 2658 , 2019 . 155 . S . Prakash , A . Bahremand , L . D . Nguyen , and R . LiKamWa . GLEAM : An illumination estimation framework for real - time photorealistic augmented reality on mobile devices . In Proceedings of ACM MobiSys , 2019 . 156 . P . Preechayasomboon and E . Rombokas . Haplets : Finger - worn wireless and low - encumbrance vibrotactile haptic feedback for virtual and augmented reality . Frontiers in Virtual Reality , 2 : 738613 , 2021 . 157 . F . Pujol - Vila , R . Villa , and M . Alvarez . Nanomechanical sensors as a tool for bacteria detection and antibiotic susceptibility testing . Frontiers in Mechanical Engineering , 6 : 44 , 2020 . 158 . X . Qiao , P . Ren , S . Dustdar , L . Liu , H . Ma , and J . Chen . Web AR : A promising future for mobile augmented reality—state of the art , challenges , and insights . Proceedings of the IEEE , 107 ( 4 ) : 651 – 666 , 2019 . 159 . T . Qin , P . Li , and S . Shen . VINS - Mono : A robust and versatile monocular visual - inertial state estimator . IEEE Transactions on Robotics , 34 ( 4 ) : 1004 – 1020 , 2018 . 160 . J . Qiu , Z . Cui , Y . Zhang , X . Zhang , S . Liu , B . Zeng , and M . Pollefeys . DeepLiDAR : Deep surface normal guided depth prediction for outdoor scene from sparse LiDAR data and single color image . In Proceedings of IEEE CVPR , 2019 . 161 . F . Rabbi , T . Park , B . Fang , M . Zhang , and Y . Lee . When virtual reality meets Internet of Things in the gym : Enabling immersive interactive machine exercises . Proceedings of ACM IMWUT , 2 ( 2 ) : 1 – 21 , 2018 . 162 . X . Ran , H . Chen , X . Zhu , Z . Liu , and J . Chen . DeepDecision : A mobile deep learning framework for edge video analytics . In Proceedings of IEEE INFOCOM , 2018 . 163 . X . Ran , C . Slocum , Y . - Z . Tsai , K . Apicharttrisorn , M . Gorlatova , and J . Chen . Multi - user augmented reality with communication eﬃcient and spatially consistent virtual objects . In Proceedings of ACM CoNEXT , 2020 . 164 . M . Roberts , J . Ramapuram , A . Ranjan , A . Kumar , M . A . Bautista , N . Paczan , R . Webb , and J . M . Susskind . Hypersim : A photorealistic synthetic dataset for holistic indoor scene understanding . In Proceedings of IEEE / CVF ICCV , 2021 . 165 . C . M . Robertson . Using graphical context to reduce the eﬀects of registration error in augmented reality . PhD thesis , Georgia Institute of Technology , 2007 . 166 . F . Roesner , T . Kohno , and D . Molnar . Security and privacy for augmented reality systems . Communications of the ACM , 57 ( 4 ) : 88 – 96 , 2014 . 167 . K . Rohmer , W . Büschel , R . Dachselt , and T . Grosch . Interactive near - ﬁeld illumination for photorealistic augmented reality on mobile devices . In Proceedings of IEEE ISMAR , 2014 . 168 . E . Rublee , V . Rabaud , K . Konolige , and G . Bradski . ORB : An eﬃcient alternative to SIFT or SURF . In Proceedings of IEEE ICCV , 2011 . 169 . K . Ruth , T . Kohno , and F . Roesner . Secure multi - user content sharing for augmented reality applications . In Proceedings of USENIX Security Symposium , 2019 . 170 . S . Saeidi , G . Rentala , T . Rizzuto , T . Hong , N . Johannsen , and Y . Zhu . Exploring thermal state in mixed immersive virtual environments . Journal of Building Engineering , 44 : 102918 , 2021 . 171 . L . Salman , S . Salman , S . Jahangirian , M . Abraham , F . German , C . Blair , and P . Krenz . Energy eﬃcient IoT - based smart home . In Proceedings of IEEE WF - IoT , 2016 . 172 . T . Santini , W . Fuhl , and E . Kasneci . Pure : Robust pupil detection for real - time pervasive eye tracking . Computer Vision and Image Understanding , 170 : 40 – 50 , 2018 . Ambient Intelligence for Next - Generation AR 33 173 . D . Scaramuzza . OCamCalib : Omnidirectional camera calibration toolbox for Matlab . https : / / sites . google . com / site / scarabotix / ocamcalib - omnidirectional - camera - calibration - toolbox - for - matlab , 2023 . 174 . T . Scargill , J . Chen , and M . Gorlatova . Here to stay : Measuring hologram stability in markerless smartphone augmented reality . arXiv preprint arXiv : 2109 . 14757 , 2021 . 175 . T . Scargill , Y . Chen , S . Eom , J . Dunn , and M . Gorlatova . Environmental , user , and social context - aware augmented reality for supporting personal development and change . In Proceedings of IEEE VR Workshops , 2022 . 176 . T . Scargill , Y . Chen , N . Marzen , and M . Gorlatova . Integrated design of augmented reality spaces using virtual environments . In Proceedings of IEEE ISMAR , 2022 . 177 . T . Scargill , A . Dabrowski , A . Xu , and M . Gorlatova . IoT - enabled environment illuminance optimization for augmented reality . In Proceedings of ACM UbiComp , 2022 . 178 . T . Scargill , S . Hurli , J . Chen , and M . Gorlatova . Demo : Will it move ? Indoor scene characterization for hologram stability in mobile AR . In Proceedings of ACM HotMobile , 2021 . Demo video available at https : / / sites . duke . edu / timscargill / sceneit - prototype / . 179 . T . Scargill , G . Lan , and M . Gorlatova . Demo abstract : Catch my eye : Gaze - based activity recognition in an augmented reality art gallery . In Proceedings of ACM / IEEE IPSN , 2022 . Demo video available at https : / / sites . duke . edu / timscargill / catchmyeye - demo / . 180 . P . Schmuck and M . Chli . CCM - SLAM : Robust and eﬃcient centralized collaborative monocular simultaneous localization and mapping for robotic teams . Journal of Field Robotics , 36 ( 4 ) : 763 – 781 , 2019 . 181 . D . Schubert , T . Goll , N . Demmel , V . Usenko , J . Stückler , and D . Cremers . The TUM VI benchmark for evaluating visual - inertial odometry . In Proceedings of IEEE / RSJ IROS , 2018 . 182 . S . Sefati , C . Gao , I . Iordachita , R . H . Taylor , and M . Armand . Data - driven shape sensing of a surgical continuum manipulator using an uncalibrated ﬁber bragg grating sensor . IEEE Sensors Journal , 21 ( 3 ) : 3066 – 3076 , 2020 . 183 . S . Sefati , R . Hegeman , F . Alambeigi , I . Iordachita , P . Kazanzides , H . Khanuja , R . H . Taylor , and M . Armand . A surgical robotic system for treatment of pelvic osteolysis using an FBG - equipped continuum manipulator and ﬂexible instruments . IEEE / ASME Transactions on Mechatronics , 26 ( 1 ) : 369 – 380 , 2020 . 184 . S . Shao , A . Khreishah , and I . Khalil . RETRO : Retroreﬂector based visible light indoor localization for real - time tracking of IoT devices . In Proceedings of IEEE INFOCOM , 2018 . 185 . S . Shen , N . Roy , J . Guan , H . Hassanieh , and R . R . Choudhury . MUTE : Bringing IoT to noise cancellation . In Proceedings of ACM SIGCOMM , 2018 . 186 . V . Shen , C . Shultz , and C . Harrison . Mouth haptics in VR using a headset ultrasound phased array . In Proceedings of ACM CHI , 2022 . 187 . Y . Shi , H . Zhang , K . Zhao , J . Cao , M . Sun , and S . Nanayakkara . Ready , steady , touch ! sensing physical contact with a ﬁnger - mounted IMU . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies , 4 ( 2 ) : 1 – 25 , 2020 . 188 . H . Soh , S . Sanner , M . White , and G . Jamieson . Deep sequential recommendation for personalized adaptive user interfaces . In Proceedings of ACM IUI , 2017 . 189 . J . Steuer . Deﬁning virtual reality : Dimensions determining telepresence . Journal of communication , 42 ( 4 ) : 73 – 93 , 1992 . 190 . Y . Sun , A . Armengol - Urpi , S . N . R . Kantareddy , J . Siegel , and S . Sarma . MagicHand : Interact with IoT devices in augmented reality environment . In Proceedings of IEEE VR , 2019 . 191 . Y . Sun , S . N . R . Kantareddy , J . Siegel , A . Armengol - Urpi , X . Wu , H . Wang , and S . Sarma . Towards industrial IoT - AR systems using deep learning - based object pose estimation . In Proceedings of IEEE IPCCC , 2019 . 192 . Z . Sun , M . Zhu , X . Shan , and C . Lee . Augmented tactile - perception and haptic - feedback rings as human - machine interfaces aiming for immersive interactions . Nature communications , 13 ( 1 ) : 1 – 13 , 2022 . 193 . T . Tahara , T . Seno , G . Narita , and T . Ishikawa . Retargetable AR : Context - aware augmented reality in indoor scenes based on 3D scene graph . In Proceedings of IEEE ISMAR - Adjunct , 2020 . 194 . L . Tawalbeh , F . Muheidat , M . Tawalbeh , and M . Quwaider . IoT privacy and security : Challenges and solutions . Applied Sciences , 10 ( 12 ) : 4102 , 2020 . 195 . Texas Instruments . Introduction to time - of - ﬂight long range proximity and distance sensor system design ( Rev . B ) . https : / / www . ti . com / lit / pdf / sbau305 , 2019 . 196 . N . Tractinsky and E . Eytam . Considering the aesthetics of ubiquitous displays . In Ubiquitous Display Environments , pp . 89 – 104 . Springer , 2012 . 197 . Unity . Unity real - time development platform . https : / / unity . com / , 2023 . 198 . A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , Ł . Kaiser , and I . Polosukhin . Attention is all you need . In Proceedings of NIPS , 2017 . 199 . Vicon . Vicon . https : / / www . vicon . com / , 2023 . 200 . Vuforia . Vuforia . https : / / library . vuforia . com / objects / image - targets , 2023 . 201 . H . Wang , B . Kim , J . L . Xie , and Z . Han . LEAF + AIO : Edge - assisted energy - aware object detection for mobile augmented reality . IEEE Transactions on Mobile Computing , 2022 . 202 . Y . Wang , P . - M . Jodoin , F . Porikli , J . Konrad , Y . Benezeth , and P . Ishwar . CDnet 2014 : An expanded change detection benchmark dataset . In Proceedings of IEEE CVPR Workshops , 2014 . 203 . S . Weiss , M . W . Achtelik , S . Lynen , M . Chli , and R . Siegwart . Real - time onboard visual - inertial state estimation and self - calibration of MAVs in unknown environments . In Proceedings of IEEE ICRA , 2012 . 204 . S . Wen , J . Chen , F . R . Yu , F . Sun , Z . Wang , and S . Fan . Edge computing - based collaborative vehicles 3D mapping in real time . IEEE Transactions on Vehicular Technology , 69 ( 11 ) : 12470 – 12481 , 2020 . 205 . D . Xu , A . Zhou , G . Wang , H . Zhang , X . Li , J . Pei , and H . Ma . Tutti : Coupling 5G RAN and mobile edge computing for latency - critical video analytics . In Proceedings of ACM MobiCom , 2022 . 206 . H . Xu , M . A . Peshkin , and J . E . Colgate . UltraShiver : Lateral force feedback on a bare ﬁngertip via ultrasonic oscillation and electroadhesion . IEEE Transactions on Haptics , 12 ( 4 ) : 497 – 507 , 2019 . 207 . J . Xu , H . Cao , D . Li , K . Huang , C . Qian , L . Shangguan , and Z . Yang . Edge assisted mobile semantic visual SLAM . In Proceedings of IEEE INFOCOM , 2020 . 208 . J . Xu , H . Cao , Z . Yang , L . Shangguan , J . Zhang , X . He , and Y . Liu . SwarmMap : Scaling up real - time collaborative visual SLAM at the edge . In Proceedings of USENIX NSDI , 2022 . 34 Tim Scargill , Sangjun Eom , Ying Chen , Maria Gorlatova 209 . X . Yang and X . Zhang . A study of user privacy in Android mobile AR apps . In Proceedings of IEEE / ACM International Conference on Automated Software Engineering , 2022 . 210 . Z . Yang , Y . - L . Wei , S . Shen , and R . R . Choudhury . Ear - AR : Indoor acoustic augmented reality on earphones . In Proceedings of ACM MobiCom , 2020 . 211 . T . Zachariah and P . Dutta . Browsing the web of things in mobile augmented reality . In Proceedings of ACM HotMobile , 2019 . 212 . F . Zafari , A . Gkelias , and K . K . Leung . A survey of indoor localization systems and technologies . IEEE Communications Surveys & Tutorials , 21 ( 3 ) : 2568 – 2599 , 2019 . 213 . T . Zhan , K . Yin , J . Xiong , Z . He , and S . - T . Wu . Augmented reality and virtual reality displays : Perspectives and challenges . Iscience , 23 ( 8 ) : 101397 , 2020 . 214 . T . Zhang , A . Chowdhery , P . Bahl , K . Jamieson , and S . Banerjee . The design and implementation of a wireless video surveillance system . In Proceedings of ACM MobiCom , 2015 . 215 . T . Zhang , H . Zhang , Y . Li , Y . Nakamura , and L . Zhang . FlowFusion : Dynamic dense RGB - D SLAM based on optical ﬂow . In Proceedings of IEEE ICRA , 2020 . 216 . W . Zhang , B . Han , and P . Hui . On the networking challenges of mobile augmented reality . In Proceedings of ACM Workshop on Virtual Reality and Augmented Reality Network , 2017 . 217 . W . Zhang , B . Han , and P . Hui . SEAR : Scaling experiences in multi - user augmented reality . IEEE Transactions on Visualization and Computer Graphics , 28 ( 5 ) : 1982 – 1992 , 2022 . 218 . X . Zhang , S . Fronz , and N . Navab . Visual marker detection and decoding in AR systems : A comparative study . In Proceedings of IEEE ISMAR , 2002 . 219 . Y . Zhang and T . Funkhouser . Deep depth completion of a single RGB - D image . In Proceedings of IEEE CVPR , 2018 . 220 . Y . Zhang , T . Scargill , A . Vaishnav , G . Premsankar , M . Di Francesco , and M . Gorlatova . InDepth : Real - time depth inpainting for mobile augmented reality . In Proceedings of ACM IMWUT , 2022 . 221 . Z . Zhang , J . Geiger , J . Pohjalainen , A . E . - D . Mousa , W . Jin , and B . Schuller . Deep learning for environmentally robust speech recognition : An overview of recent developments . ACM Transactions on Intelligent Systems and Technology ( TIST ) , 9 ( 5 ) : 1 – 28 , 2018 . 222 . Y . Zhao and T . Guo . PointAR : Eﬃcient lighting estimation for mobile augmented reality . In Proceedings of ECCV , 2020 . 223 . Y . Zhao and T . Guo . Xihe : A 3D vision - based lighting estimation framework for mobile augmented reality . In Proceedings of ACM MobiSys , 2021 .