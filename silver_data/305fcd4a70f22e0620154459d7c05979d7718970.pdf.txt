C ODE C ODE E VOLUTION : U NDERSTANDING H OW P EOPLE C HANGE D ATA S CIENCE N OTEBOOKS O VER T IME Deepthi Raghunandan University of Maryland College Park MarylandUSA draghun1 @ umd . edu Aayushi Roy University of Maryland College Park MarylandUSA aroy2530 @ umd . edu Shenzhi Shi University of Maryland College Park MarylandUSA sshi1234 @ umd . edu Niklas Elmqvist University of Maryland College Park MarylandUSA elm @ umd . edu Leilani Battle University of Washington SeattleWashingtonUSA leibatt @ cs . washington . edu A BSTRACT Sensemaking is the iterative process of identifying , extracting , and explaining insights from data , where each iteration is referred to as the “ sensemaking loop . ” Although recent work observes snap - shots of the sensemaking loop within computational notebooks , none measure shifts in sensemaking behaviors over time—between exploration and explanation . This gap limits our ability to under - stand the full scope of the sensemaking process and thus our ability to design tools to fully support sensemaking . We contribute the ﬁrst quantitative method to characterize how sensemaking evolves within data science computational notebooks . To this end , we conducted a quantitative study of 2 , 574 Jupyter notebooks mined from GitHub . First , we identify data science - focused notebooks that have undergone signiﬁcant iterations . Second , we present regression models that automatically characterize sensemaking activity within individual notebooks by assigning them a score representing their position within the sensemaking spectrum . Finally , we use our regression models to calculate and analyze shifts in notebook scores across GitHub versions . Our results show that notebook authors participate in a diverse range of sensemaking tasks over time , such as annotation , branching analysis , and documentation . Finally , we propose design recommendations for extending notebook environments to support the sensemaking behaviors we observed . K eywords Computational Notebooks , machine learning , sensemaking , data science , data exploration , analysis . 1 Introduction Sensemaking is “the process of searching for a representation and encoding data in that representation to answer task - speciﬁc questions” [ Russell et al . ( 1993 ) ] . In each iteration of this “sensemaking loop” [ Pirolli and Card ( 2005 ) ] , data scientists reﬁne their code , visualizations , and annotations in pursuit of a deeper understanding of their data [ Thomas and Cook ( 2005 ) ] . In the process , data scientists often oscillate between exploring the data and ex - plaining what they have learned ( to themselves or stakeholders ) [ Rule et al . ( 2018 ) , Pirolli and Card ( 2005 ) ] , leading to more of “spiral” of activity than a true “loop” . Computational notebooks such as Jupyter [ Kluyver et al . ( 2016 ) ] , R Markdown , or Observable are especially popular for documenting the complexities of the sensemaking process given the ease with which code can be interleaved with descriptive text and illustrative images [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) ] . However , notebooks still fall short of the a r X i v : 2209 . 02851v2 [ c s . H C ] 8 S e p 2022 Code Code Evolution ideal for sensemaking , particularly in tracking changes to notebooks over time [ Kery et al . ( 2017 ) ] , frustrating many notebook users [ Chattopadhyay et al . ( 2020 ) ] . In order to improve notebooks for sensemaking we must ﬁrst characterize users’ common interaction patterns so that we can ( re ) design notebook environments to better support them [ Kery et al . ( 2017 ) ] . However , the evolving nature of sensemaking suggests that these patterns may vary depending on where users are within the progression between exploration and explanation [ Rule et al . ( 2018 ) ] . Thus , we need to determine where a user is along this exploration - explanation spectrum before we can design appropriate solutions . Recent work posits that we can infer where a user is within the exploration - explanation spectrum directly from computational notebooks [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Wang et al . ( 2020 ) ] . However , these prior works rely on small - scale user studies to investigate sensemaking within notebooks . Furthermore , they treat notebooks as static outputs of sensemaking rather than a core medium for iteration . This limits our understanding of notebooks as living documents of scientiﬁc inquiry . Without more rigorous validation , it is still unclear whether current theory can accurately detect sensemaking within real - world notebook environments . This paper proposes a new approach to analyzing how computational notebooks are revised over time . The key idea is that many analysts already track their notebook iterations using public version control infrastructure such as GitHub . We contribute a pipeline to collect , model , and quantify sensemaking behaviors across GitHub commits . This pipeline allows us to ( 1 ) characterize observed shifts in sensemaking behavior within notebooks , such as whether notebooks become more explanatory or exploratory over time , and to ( 2 ) understand why these shifts occur . To do this , we randomly sampled and downloaded 2 , 574 Jupyter notebooks stored on GitHub’s public repositories . We report on their content , revision history , and evolution . Our analysis has three parts : 1 . Identifying Relevant Notebooks – ﬁnding the data science notebooks that were actively reﬁned overtime on GitHub , as well as quantitative metrics to analyze them ; 2 . Measuring Exploration vs . Explanation – leveraging prior work [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] to distinguish between the exploratory vs . explanatory nature of data science notebooks ; and 3 . Measuring Evolution – drawing on GitHub revision history to understand how notebooks , and in turn authors’ positions in the sensemaking loop , shifted over time . We acknowledge that these quantitative approximations may not reﬂect the author’s complete sensemaking pro - cess . This discrepancy is due in part to authors’ selective reporting as well as to limitations inherent to notebook platforms themselves [ Liu et al . ( 2020 ) , Wang et al . ( 2019 ) , Muller et al . ( 2019a ) , Kery et al . ( 2018 ) , Wang et al . ( 2020 ) , Rule et al . ( 2018 ) , Chattopadhyay et al . ( 2020 ) ] . In other words , the use of GitHub as a data source likely biases the type of notebooks we collect for our sample . Regardless , we believe that measuring notebooks as they publicly change over time still provides a unique perspective on the sensemaking process that qualitative analyses of singular notebook versions cannot . We make the following contributions in this paper : ( 1 ) we develop a rubric to show how to quantify the explanatory or exploratory nature of a Jupyter Notebook , enabling us to analyze data - science notebooks at scale ; ( 2 ) we track the evolution of notebooks over time by calculating our quantitative measure across multiple notebook versions ; ( 3 ) we characterize the way analysts iterate on their notebooks ; and ( 4 ) we use these insights to make design recommendations to better support the different notebook - based sensemaking behavior we observed . More broadly , we contribute a more nuanced view of the data science process that brings notebook analysis methodology closer alignment with established theories of sensemaking and data exploration . This , in combination with a quantitative approach , can be directly applied towards teaching , guiding , and implementing tools for best practices in data science . Beyond the overview of our method and results presented in this paper , we have also provided supplementary ma - terial with the full details in the following anonymous OSF repository : https : / / osf . io / 9q4wp / ? view _ only = 61e6f58d29194742a0aaed328afdea4d 2 Related Work In this section , we introduce key concepts and terminology that we use in our work to map signs of exploration and explanation in computational notebooks to corresponding shifts within the sensemaking loop . 2 . 1 Computational Notebooks As an embodiment of the literate programming paradigm [ Knuth ( 1984 ) ] , where traditional source code is em - bedded in descriptive natural language , computational notebooks are an ideal medium for studying the sense - 2 Code Code Evolution making process [ Google ( 2019 ) , ObservableHQ ( 2011 ) , Kluyver et al . ( 2016 ) , Rädle et al . ( 2017 ) , Tabard et al . ( 2008 ) , Wickham and Grolemund ( 2017 ) ] . A notebook is a linear sequence of executable code that perfectly captures the procedural nature of sensemaking . The ability to inspect intermediate results by generating visualizations and tables scaffolds the exploratory process . The rich annotation features scaffold the pivoting of data rep - resentations towards explanation . Notebooks also allow for easy sharing of data , code , and analyses all in one [ Rule et al . ( 2018 ) , Chattopadhyay et al . ( 2020 ) ] . As a result , computational notebooks have quickly become an essential part of conducting data science [ Kery et al . ( 2018 ) , Kluyver et al . ( 2016 ) , Rule et al . ( 2018 ) , Wang et al . ( 2019 ) ] . Data scientists utilize computational notebooks—speciﬁcally their ﬂexible cell structure—to iterate on different branches of exploration and create narratives surrounding their analyses [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Wang et al . ( 2021 ) , Wang et al . ( 2022 ) , Chattopadhyay et al . ( 2020 ) , Pimentel et al . ( 2019 ) , Wenskovitch et al . ( 2019 ) , Koenzen et al . ( 2020 ) , Källén and Wrigstad ( 2021 ) , Dong et al . ( 2021 ) , Rehman ( 2019 ) , Wang et al . ( 2020 ) ] . 2 . 2 Sensemaking in Computational Notebooks Pirolli and Card describe the sensemaking loop as cumulative iterations by which analysts develop an understanding of the data [ Pirolli and Card ( 2005 ) ] . Each iteration informs the next . Our work enriches this deﬁnition of sensemaking with notebook - oriented notions of exploration and explanation from the literature [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] . Speciﬁcally , we deﬁne the “sensemaking spectrum” as a two - dimensional representation of the sensemaking loop from early - stage exploration to late - stage explanation [ Pirolli and Card ( 2005 ) ] . Qualitative studies from Rule et al . [ Rule et al . ( 2018 ) ] , Kery et al . [ Kery et al . ( 2018 ) ] , and Wang et al . [ Wang et al . ( 2020 ) ] show that we can observe sensemaking within computational notebooks in the form of explo - ration and explanation . For example , Kery et al . observed that some analysts created many small code cells while performing exploratory data analysis to optimize iteration , and later grouped code into individual “logical units” to communicate analytical steps [ Kery et al . ( 2018 ) ] . Rule et al . noted that analysts place text that serve different purposes in different parts of the notebook [ Rule et al . ( 2018 ) ] . They found that nearly all code comments help explain the methods employed by code , headers labeled the analyses , and most non - header text explained analytical steps . Wang et al . extended this ﬁnding by showing that highly readable ( explanatory ) notebooks use a variety of descriptors to attract a broader audience [ Wang et al . ( 2021 ) ] . Based on these ﬁndings , it seems evident that different notebook characteristics , such as types of documentation or distribution of code across cells , can indicate an analyst’s current position within the sensemaking spectrum between exploration and explanation . Some previous work contribute to our understanding of how notebooks are used but do not identify these steps in the context of sensemaking . For example , Dong et al . ﬁnd that code cleaning is an integral part of sharing a notebook [ Dong et al . ( 2021 ) ] . They characterize cleaning as renaming variables , generating functions , reordering code cells , adding pertinent annotations , moving content between ﬁles , and removing extraneous content . We leverage Dong et al . ’s work to construct a comprehensive model of sensemaking in notebooks . 2 . 3 From Exploration to Explanation In data exploration , analysts seek to proﬁle their data , deﬁne their goals , and become comfortable with potential analytic methods [ Kery et al . ( 2018 ) , Pimentel et al . ( 2019 ) , Guo ( 2012 ) ] . As Alspaugh et al . explain , data analysis exists within a spectrum between “exploratory " and “directed " analysis , wherein the nature of analysis changes as goals become more concrete [ Alspaugh et al . ( 2018 ) ] . Analysts seek to understand their dataset , look for exciting patterns , and identify assumptions as a means to inform next steps [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Muller et al . ( 2019b ) , Wongsuphasawat et al . ( 2019 ) ] . The ultimate objective of this process is to inform decisions . The process of explaining data insights entails shaping explorations into a narrative to communicate the process and results [ Kery et al . ( 2018 ) ] . This type of explanation provides clarity on how the analysis process yielded particular insights to an audience ( including oneself ) . Analysts can describe their analyses and ﬁndings in varying levels of detail and clarity , ranging from reporting all avenues of exploration and ensuing insights to saving only the most critical decisions and ﬁndings [ Kery et al . ( 2018 ) , Mathisen et al . ( 2019 ) ] . The level of detail they choose depends mainly on the audience . When the audience is oneself or fellow technical team members , the analyst focuses on retaining code and branches of exploration and formatting them in a comprehensible manner [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] . When presenting results to a broader , perhaps non - technical , audience , analysts may remove details that appear confusing or uninteresting and add more explanatory text—shifting the focus from the code to the narrative [ Liu et al . ( 2020 ) , Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] . Our deﬁnitions are also grounded in literature on notebook reproducibility—a common motivation for authoring notebooks [ Pimentel et al . ( 2019 ) , Wang et al . ( 2020 ) , Rehman ( 2019 ) , Wenskovitch et al . ( 2019 ) , 3 Code Code Evolution Chattopadhyay et al . ( 2020 ) ] . Like explanatory notebooks , reproducible notebooks allow for communication , reuse and reproduction—enabling a clear linear structure [ Rehman ( 2019 ) , Wenskovitch et al . ( 2019 ) ] and presenting clean code [ Dong et al . ( 2021 ) , Pimentel et al . ( 2019 ) ] . 3 Dataset Given our aim to use revision histories to measure how data science notebooks evolve , we chose to analyze publicly available Jupyter Notebooks found on GitHub for the following reasons : 1 ) it provides an extensive repository of Jupyter Notebook documents , and 2 ) they are particularly amenable to meta - analysis due to the ease of accessing their underlying JSON metadata structure . In July 2019 , we identiﬁed 4 . 7 million notebooks on GitHub and downloaded a random sample of 60 , 000 . Using the above criteria , we selected 2 , 574 notebooks of these for further analysis . We queried for GitHub commit information to ensure we could examine all versions of notebooks . We anticipated that , given that GitHub contains a large variety of notebooks in terms of quality and purpose , not all notebooks would be suited for this analysis . Thus , we sought to programmatically ﬁlter for data science notebooks to automatically scale our analysis to any sample size . Here , we describe our methods for collecting , sampling , and ﬁltering notebooks for subsequent meta - analysis . 3 . 1 Data Collection Method To mine Jupyter Notebooks from GitHub , we used Rule et al . ’s approach [ Rule et al . ( 2018 ) ] . We ﬁrst downloaded and accessed a total of 59 , 887 notebooks . For the sake of project feasibility , we chose to observe only Python notebooks annotated in English . Python and R are the most popular languages used for data science , but we made this choice on the basis that Python is signiﬁcantly more common in Jupyter than R ( more than 96 % of all Jupyter Notebooks are written in Python [ Rule et al . ( 2018 ) ] ) . This eliminated 3 , 642 notebooks from our sample . To further select notebooks suitable for our analysis , we deﬁned a standard to identify Jupyter notebooks that use data science and were stored on GitHub . To meet our standards : 1 ) notebooks must demonstrate data analysis activity , 2 ) some subset of changes must be observable across multiple versions found in the GitHub repository , and 3 ) changes to the notebook must be made by the original owner of the notebook . We brieﬂy outline our ﬁltering criteria below ; further details about our criteria and our methods can be found in our supplemental material . Data Science Notebooks First , we used the number of popular data science Python libraries as a heuristic to determine whether the notebooks were data science - oriented . We manually derived a list of data science libraries by reviewing 28 online Python tutorials . The full list contains the following packages : numpy , scipy , pandas , scikit - learn , matplotlib , pytorch , and tensorflow [ Pimentel et al . ( 2019 ) ] . We searched and ﬁltered for notebooks with these libraries and the associated API calls . A total of 35 , 692 notebooks out of 59 , 887 were eliminated for not meeting this criterion . Versioned Notebooks Second , we used the number of notebook versions within the repositories and the number of changes within them as a heuristic to measure whether changes to data analysis were observable . We found that we could observe changes to notebook cells and lines when there were at least 4 revisions . We required that notebook versions contained at least 2 additions or deletions of cells , along with at least 20 additions or deletions to lines across cells . A total of 5 , 082 were eliminated for not meeting this criterion . Original Content When there are multiple contributors to a notebook , it is unclear whether both authors had the same intent in making changes . We restrict our analysis to only include notebooks updated by the owner of the notebooks’ respository to eliminate this ambiguity . An additional 12 , 350 notebooks were eliminated based on this criterion . 3 . 2 Summary & Dataset Considerations 2 , 574 notebooks passed the above criteria . A majority of these notebooks were data - science oriented , contained a rich amount of data science activity , and were written in English . We acknowledge that our quantitative approach may not be perfectly accurate . Even with our rigorous criteria , the dataset may still contain notebooks that do not demonstrate traditional sensemaking . We believe these will be in the minority given our threshold on data science API usage combined with the precision of our commit ﬁlters . 4 Code Code Evolution We also acknowledge that not all analysts may commit all their iterations to GitHub repositories . However , given the relatively large size of our dataset ( 2 , 574 notebooks and 26 , 474 notebook versions ) , the noise in our dataset is signiﬁcantly diminished . 4 Measuring Exploration vs . Explanation We ﬁrst seek to answer the following research question : Can we apply previous ﬁndings to quantitatively measure exploration and explanation in computational notebooks ? To answer this question , we ﬁrst manually curated a reference dataset of 244 notebooks ( 10 % of our sample ) using a manual rubric that maps notebook characteristics to points within the sensemaking spectrum . This rubric scores builds directly upon ﬁndings of previous work regarding how sensemaking manifests within computational notebooks . We then used our manual reference dataset to develop and validate a model to automate the manual classiﬁcation . 4 . 1 Constructing the Reference Dataset Here , we describe a new rubric to score data science notebooks according to their position along the sensemaking spec - trum , which we aim to automate . The goal of this analysis was to determine whether each notebook appears to be more ex - ploratory or explanatory in nature . As observed in prior work [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Wang et al . ( 2021 ) ] , notebooks with good narrative structure generally tell a compelling story of both the data analysis process and the insights derived from this process . These notebooks clearly communicate the analyst’s motivations and insights , and can appeal to a wide audience via instructional text or explanations of ﬁeld - speciﬁc terminol - ogy [ Alspaugh et al . ( 2018 ) , Chattopadhyay et al . ( 2020 ) ] . Therefore , the better the narrative structure of a notebook , the higher the sensemaking score it should receive . Based on the literature , one of the authors developed a rubric for scoring notebooks . Two other authors provided feedback on the rubric between iterations . All coders were knowledgeable in data science . Inspired by methods for reaching agreement on qualitative codes [ McDonald et al . ( 2019 ) ] , our scoring process involved three iterations with one coder to converge to scores consistent with our rubric . The iterations included a preliminary scoring iteration where scores were assigned based on an initial rubric , a second one where scores were reﬁned in parallel with the rubric , and a ﬁnal one where scores were reviewed for consistency with the ﬁnal rubric . We represented the positions of notebooks on the sensemaking spectrum using a score between 0 . 1 and 1 . 0 , where 0 . 1 represents the most exploratory notebooks and 1 . 0 the most explanatory ones . Scores were assigned in increments of 0 . 1 . We wanted our range of scores to be evenly distributed such that the ﬁrst ﬁve scores ( 0 . 1 - 0 . 5 ) characterize mostly exploratory notebooks and the last ﬁve scores ( 0 . 6 - 1 . 0 ) characterize mostly explanatory notebooks . To form an impression of the notebooks , coders considered the following criteria : code abstraction methods such as functions , classes , and code distribution within cells ; the clarity of code based variable names and in - line code comments ; the use of markdown headers to create sections ; the cohesiveness of the analytical workﬂow ; and the types of descriptors ( analytical , procedural and context ) included in the document . We identify exploratory notebooks as ones which leverage code to explore data . These notebooks place little focus on explaining insights , reasoning , or analytical methods , and instead focus on fast iteration resulting in duplicated , messy code [ Kery et al . ( 2018 ) , Chattopadhyay et al . ( 2020 ) , Koenzen et al . ( 2020 ) ] . In explanatory notebooks , on the other hand , the intent to outline , document , or explain previous data exploration is clear [ Rule et al . ( 2018 ) , Wang et al . ( 2021 ) , Chattopadhyay et al . ( 2020 ) , Alspaugh et al . ( 2018 ) ] . The rubric in Table 1 speciﬁes how each notebook was evaluated . Coders reached a ﬁnal inter - rater reliability score of 0 . 88 after converging on this rubric . 4 . 2 Automating the Scoring Process To scale up our analysis , we needed a way to programmatically calculate a notebook’s sensemaking score . We observed characteristics of notebooks from our reference set to understand how they contributed to a notebook’s position on the sensemaking spectrum . We chose to observe particularly quantitative characteristics which were highlighted by previous literature on sensemaking in notebooks and used our own observations to understand how other metrics correlated with our manually assigned sensemaking scores . 4 . 2 . 1 Analyzing Notebook Characteristics We analyzed all parts of a notebook as follows . 5 Code Code Evolution Code Given that notebook authors perform their analyses primarily through code , we were interested in how an author’s approach to writing their code may contribute to the exploratory nature of the notebook . The amount of code in the notebook tends to increase as notebook authors explore their dataset [ Head et al . ( 2019 ) , Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Pimentel et al . ( 2019 ) , Chattopadhyay et al . ( 2020 ) , Dong et al . ( 2021 ) , Källén and Wrigstad ( 2021 ) , Koenzen et al . ( 2020 ) ] . To this end , we computed the following measures for each notebook in our reference set : the number of code cells and the total lines of code across all code cells . Non - Code Data science notebooks often contain content beyond just code , such as markdown text , table outputs , text outputs , visualization outputs , and code comments . This content is of particular interest , because they are primarily explanation - oriented , and thus increase a notebook’s sensemaking score [ Rule et al . ( 2018 ) , Wang et al . ( 2022 ) , Rule et al . ( 2019 ) , Kery et al . ( 2018 ) ] . To gauge the impact of non - code content within notebook cells , we computed the following measures : the number of markdown cells ; the total lines across markdown cells ; the number of text outputs produced by code cells ; the number of tables produced by code cells ; the number of visualizations produced by code cells ; and the number of individual code comments within code cells . Whitespace Negative space can have a profound impact on how information is organized and presented for communi - cation [ Tufte ( 2001 ) ] . Notebook authors can control the negative space in their notebooks by adding newline and space characters . Rule et al . suggest that the number of spacing characters in text and code cells could point towards more of an explanation focus for a notebook [ Rule et al . ( 2018 ) ] . Therefore , we also measure the total spacing characters observed across code cells , and spacing characters across markdown cells . 4 . 2 . 2 Measure Normalization To ensure that different quantitative measures could be compared fairly across notebooks , we normalized each measure with respect to individual notebooks . Our normalization process translates each measure to a domain of 0 . 0 to 1 . 0 . We normalized cell counts by dividing them by the total number of cells found in the notebook ( e . g . , the number of code cells were divided by the number of all cells ) . We normalized the output ( text , table , and visualization ) counts by dividing them by the total number of outputs in the notebook . We normalized the number of code comments by the number of lines found within the code cell . Finally , we normalized the number of spaces for a given cell type by dividing by the total spaces across all notebook cells . 4 . 2 . 3 Combining Measures We tested two different combinations of the measures based on our understanding of how notebooks are used in practice . The ﬁrst combination focused on the outputs generated by a notebook , which may indicate a more explanatory notebook . This combination consists of total markdown cells , total tabular outputs , total visualization outputs , and total text outputs . We refer to this combination as “Output - Focused . ” Rather than emphasizing the outputs of code cells , our second combination of measures gauged the proportions of different cell types and their structure , where notebooks with more markdown cells and / or better organized cells were likely to be more explanatory . This combination includes the following measures : total code cells ; total markdown cells ; total spaces in markdown cells ; and total lines in markdown cells . We refer to this combination as “Organization - Focused . ” Synthesizing a “Hybrid” It is possible that cell outputs and cell organization together play important roles in assessing the sensemaking score of a notebook . In response , we formulated a new “Hybrid” combination , incorporating measures from both of the above combinations : total code cells ; total markdown cells ; total markdown spacing ; total text outputs ; total visualization outputs ; and total tabular outputs . We refer to this combination as “Hybrid . ” 4 . 2 . 4 Comparing Combinations of Quantitative Measures We used each combination of measures as parameters in a multi - linear regression analysis against the manually assigned sensemaking scores . We leveraged a k - fold cross - validation technique to ensure the strength of each model . The models were trained in 5 folds on 20 % of the data , and tested on the rest . R 2 values were calculated for each model , within each fold . A mean and median R 2 value were generated for each model . Median R 2 values were compared to assess correlations . 6 Code Code Evolution 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Score assigned through qualitative study 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 S c o r e ass i gn e d t h r ough li n ea r r e g r ess i on a n a l ys i s Auto - generated scores from regression testing vs . manual scores ( R² = 0 . 591 ) Figure 1 : Score comparison . Comparison of our manually assigned sensemaking scores ( x - axis ) and hybrid automated scores ( y - axis ) , using a combination of quantitative measures . These data points are from our reference - set containing 244 notebooks . ( Multi - linear regression model , R 2 = 0 . 591 . ) 4 . 2 . 5 Hybrid - Focused Combination Performance We found that our Output - Focused and Organization - Focused combinations correlate positively with increases in sensemaking score . The Organization - Focused combination has a higher correlation value than our Output - Focused combination . However , it is unclear whether cell outputs and cell organization measure redundant information , or are complemen - tary . To assess this relationship , we performed the same analysis with our “Hybrid” combination , which produces a multi - linear regression model with a correlation value ( R 2 = 0 . 591 ) : Y = 0 . 426 × totalMarkdownCells + 0 . 145 × totalMarkdownSpace − 0 . 077 × totalCodeCells + 0 . 176 × totalVisualizations + 0 . 125 × totalTextOutputs + 0 . 172 × totalTableOutputs + 0 . 395 . Thus , it seems that cell types , cell outputs , and cell organization capture separate but complementary facets of a notebook author’s sensemaking process . For this reason , we use the hybrid combination for all subsequent analyses in this paper . The results for the “Hybrid” combination are provided in Figure 1 , where the y - axis represents the range of automated sensemaking scores , and the x - axis the manually assigned sensemaking scores . 4 . 3 Results and Takeaways In this study , we used prior observations of how analysts interact with the sensemaking process as a whole [ Pirolli and Card ( 2005 ) , Kery et al . ( 2018 ) , Kross and Guo ( 2019 ) , Russell et al . ( 1993 ) , Alspaugh et al . ( 2018 ) , Kandel et al . ( 2012 ) ] and computational notebooks in particular [ Rule et al . ( 2018 ) ] to derive a new rubric for assigning a sensemaking score to an individual notebook . To scale up the application of our rubric , we ﬁrst formed a manually - labeled reference dataset containing 244 notebooks . These were used to develop our regression models . We then analyzed relevant quantitative measures that may predict these sensemaking scores . We found a strong correlation between increases in sensemaking scores and increases in organization - focused measures , such as having more lines or more spacing in markdown cells . We also found a positive correlation between output - focused measures and sensemaking score ( i . e . , more explanatory notebooks ) . Thus , the content and structure of a notebook may be indicative of the sensemaking goals of the notebook’s author . We acknowledge that it is unrealistic to extract exact quantitative measures for sensemaking scores and that many combinations of notebook attributes could ultimately predict sensemaking behavior . However , our scoring mechanism aims to establish sufﬁcient placement along the exploration - explanation spectrum , allowing us to observe shifts in the spectrum over time . Although not exact , our computed scores still provide a valuable signal for studying notebook evolution . We encourage the community to extend our initial feature set with new attributes in Section 6 . 7 Code Code Evolution Score Stage Concrete Examples 0 . 1 Understanding the data [ Kandel et al . ( 2012 ) , Muller et al . ( 2019b ) , Pirolli and Card ( 2005 ) , Wongsuphasawat et al . ( 2019 ) , Alspaugh et al . ( 2018 ) ] Just code ( Few unorganized cells [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) ] , duplicated code [ Källén and Wrigstad ( 2021 ) ] , no out - put [ Pimentel et al . ( 2019 ) ] . ) 0 . 2 Iterative data wran - gling [ Kandel et al . ( 2012 ) , Guo ( 2012 ) , Muller et al . ( 2019b ) , Pirolli and Card ( 2005 ) , Wongsuphasawat et al . ( 2019 ) , Alspaugh et al . ( 2018 ) ] Just code , some output from exploration [ Pimentel et al . ( 2019 ) ] ( dis - joint , duplicated code cells ; lots of code cells with individual lines of code [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Head et al . ( 2019 ) , Chattopadhyay et al . ( 2020 ) ] ) . 0 . 3 Deﬁning goals using it - erative exploratory anal - ysis [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) ] Lots of code [ Kery et al . ( 2018 ) ] , some output from exploration ( some code cells are grouped by functionality , text headers and code com - ments are used to label groups ) [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Dong et al . ( 2021 ) , Head et al . ( 2019 ) , Chattopadhyay et al . ( 2020 ) ] . 0 . 4 Beginning goal - oriented exploratory analysis [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) ] Code and visual output address some goals ( some code cells are linearly grouped by text headers and code comments are used to label and annotate analysis ) [ Rehman ( 2019 ) , Wenskovitch et al . ( 2019 ) , Kery et al . ( 2018 ) , Head et al . ( 2019 ) , Dong et al . ( 2021 ) , Rule et al . ( 2018 ) , Wang et al . ( 2020 ) ] . 0 . 5 Exploratory analysis with clear goals [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) ] Code and visual output address goals ( majority of code cells are linearly grouped by text headers and code comments ) [ Rehman ( 2019 ) , Wenskovitch et al . ( 2019 ) , Kery et al . ( 2018 ) , Head et al . ( 2019 ) , Dong et al . ( 2021 ) , Rule et al . ( 2018 ) , Wang et al . ( 2020 ) ] . 0 . 6 Analytical steps are commu - nicated [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) , Guo ( 2012 ) ] Code and code output are interwoven with text headers and code comments for the sake of outlining the logical steps which were taken . [ Kery et al . ( 2018 ) , Head et al . ( 2019 ) , Rule et al . ( 2018 ) , Wang et al . ( 2021 ) ] . 0 . 7 Some insights of analy - sis tracked and communi - cated [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) , Guo ( 2012 ) ] Analysts are also using text to brieﬂy annotate their code with insights from individual logical steps . [ Wang et al . ( 2021 ) , Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] . 0 . 8 Some motivations and in - sights tracked and commu - nicated [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) , Guo ( 2012 ) ] Analysts are explaining their motivations behind individual analytical steps , and more thoroughly annotating their logical steps with insights . [ Rule et al . ( 2018 ) , Wang et al . ( 2021 ) , Kery et al . ( 2018 ) ] . 0 . 9 Motivations and insights of analysis clearly communi - cated [ Kandel et al . ( 2012 ) , Alspaugh et al . ( 2018 ) , Wongsuphasawat et al . ( 2019 ) , Pirolli and Card ( 2005 ) , Guo ( 2012 ) ] Analysts introduce their analytical reasoning behind the work overall . In addition , they are illustrating links between logical steps using text in the form of headers , insights and motivations . Together the text forms a narrative of the methods and the results . [ Rule et al . ( 2018 ) , Wang et al . ( 2021 ) , Kery et al . ( 2018 ) ] . 1 . 0 Analysis workﬂow com - municated to a wide audi - ence [ Pirolli and Card ( 2005 ) ] Text may provide instructions on how to interact with the notebook , provide context behind the work , motivations on the methodology , in - sights from individual logical steps and insights from the entire ex - ercise [ Rule et al . ( 2019 ) ] . If code and code output are present , they align with the narrative being outlined by the text . [ Wang et al . ( 2021 ) , Rule et al . ( 2018 ) , Kery et al . ( 2018 ) ] . Table 1 : Notebook scoring rubric . This rubric leverages existing observations from the literature to characterize notebooks along the sensemaking spectrum . For example , Tukey’s deﬁnition of exploratory data analysis motivates our deﬁnition of stages 0 . 1 - 0 . 5 [ Tukey ( 1977 ) ] , and we deﬁned stages 0 . 6 - 1 . 0 using existing deﬁnitions of narrative structure [ Rule et al . ( 2018 ) ] and types of descriptors [ Wang et al . ( 2021 ) ] . Our supplementary material ( https : / / osf . io / 9q4wp / ? view _ only = 61e6f58d29194742a0aaed328afdea4d ) includes the full rubric . 8 Code Code Evolution 5 Measuring Notebook Evolution Rule et al . observe that “the process used to collect , explore , and model data has a signiﬁcant impact on the sense made . ” In other words , the process of authoring a notebook affects the insights derived . Given that a single snapshot of a notebook represents only one point within this process , it stands to reason that analyzing only one version of a notebook is insufﬁcient to fully comprehend the sensemaking process behind it . For example , it is impossible to know from a single notebook version whether a user’s analysis shifted from exploration towards explanation , as hypothesized in prior work , or followed a different path . However , a more complete view of the user’s sensemaking process could be gained by considering how the notebook has changed over time , i . e . , across multiple git versions . To this end , we analyze how our sensemaking scores from Section 4 change across notebook versions by treating them as individual time series . We seek to answer the following research question through this analysis : How does the sensemaking score of a notebook change over time , and what factors ( if any ) may explain any observed changes in the score ? 5 . 1 Measuring Sensemaking Scores Across Versions To understand how notebooks change over time , we chose to characterize notebooks by their respective and available versions . This was done in two steps . First , we used public GitHub commits as a proxy for notebook versions . We downloaded all available GitHub versions for each notebook . For each version , we generated the notebook metrics needed to apply the “hybrid - focused " formula from Section 4 . 2 . 5 . We used these metrics to calculate the sensemaking score of each version , compiling a list of scores for each notebook . This transformation allowed us to view each notebook as a time - series of sensemaking scores ( i . e . , a series of notebook scores ordered based on the time of each commit ) . For example , we would represent a notebook with 5 versions with a list of 5 numbers , each ranging from 0 . 1 to 1 . 0 . Each number indicated the position of each version within the sensemaking spectrum . We viewed changes in the time - series to indicate the evolution of a notebook across versions . Second , we normalize the time - series data to enable comparison across notebooks . The number of versions and thus the length of our representations varied across notebooks , ranging from 4 to 94 versions . To do this , we generated a simple , best - ﬁt linear regression model for each notebook representing points as a linear relationship between version numbers and sensemaking scores . A linear model is an appropriate choice because we focus on general shifts across entire notebook histories , which is a noisy time series . Linear Models Many time series exhibit different patterns at different levels of granular - ity [ Heer and Agrawala ( 2006 ) ] , where some of the observed variation may be due to noise [ Barnett et al . ( 2005 ) ] . The stock market is a classic example . The gyrations of the stock market vary non - linearly at a granular level , but a linear model can overcome the effects of noise to reveal overall trends of stock market prices over time , e . g . , market booms and busts and phenomena such as “regression to the mean " [ Barnett et al . ( 2005 ) ] . A linear model is simple but still appropriate for assessing these kinds of trends in noisy time series [ Foster et al . ( 1992 ) ] . We also attempted to analyze this data using more sophisticated time series analysis methods such as dynamic time warping . However , we soon realized these methods were unsuccessful due to noise ; example time series are shown in Figure 3 . We observed consistent overall shifts across time series , but no consistent patterns between consecutive pairs of commits . Hence we adopted a more traditional time series analysis method , i . e . , a linear model [ Foster et al . ( 1992 ) ] . 5 . 2 Grouping Time - Series Now that we had a means of comparing notebook time - series , we chose to group notebooks by major shifts in sensemaking score as a way to identify common user behaviors . We wanted to asses whether these behaviors matched our current understanding of the sensemaking spectrum . For example , if users generally follow the pattern hypothesized in prior work [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Kery et al . ( 2017 ) , Dong et al . ( 2021 ) , Head et al . ( 2019 ) ] , then we would expect to see notebooks shifting upward from exploration towards explanation . However , the notion of a sensemaking loop suggests that users might also do the reverse , corresponding to shifts from explanation toward exploration . In the remainder of this section , we describe our process for grouping the time series and qualitatively analyzing each group , and discuss the major shifts that users tended to make along the sensemaking spectrum . Grouping Methods We focus our analysis on how notebooks shift along the sensemaking spectrum , represented by three variables : initial score ( i . e . , time - series starting with high / low sensemaking scores ) , ﬁnal score ( i . e . , time - series ending with high / low sensemaking scores ) , and direction of slope from respective linear regression model ( i . e . , 9 Code Code Evolution increasing or decreasing scores ) . Using the rubric established in section 4 , we labeled scores ≤ 0 . 5 as exploratory and > 0 . 5 as explanatory . We identiﬁed four main groups of notebook shifts : exploration to exploration , exploration to explanation , explanation to explanation , and explanation to exploration . For example , time - series that began and ended with exploratory notebook versions were grouped as ’exploration to exploration’ . Qualitative Analysis Methods Three of the authors qualitatively examined a random sample of 5 % of all notebooks ( 142 total ) and their version histories using the following guidelines : 1 . We analyzed the ﬁrst version to form a hypothesis for the notebook author’s initial intent in creating the notebook . 2 . We observed changes in the type of text , code , and visualizations across individual version deltas and how these changes contributed to the notebook’s narrative . 3 . We paid special attention to changes in the structure of the notebook across versions—e . g . , markdown , comments , or visualizations demarcating different analytical steps . 4 . The frequency of commits , the commit window , and the commit messages gave our coders clues into how authors leveraged GitHub to meet their analysis goals . We derived qualitative codes ( words or short phrases ) to describe our observations with respect to these guidelines . We used these codes to identify broader behavioral themes within each of the four sensemaking groups . Themes focus on structural elements commonly used to track the narrative and ﬂow of sensemaking , including code comments , objectives , sections , templates , and cleaning [ Rule et al . ( 2018 ) ] . Details are provided in our supplementary material . 5 . 3 Results Here we discuss our observations for each group of sensemaking shifts , summarized in Table 2 and Figure 2 : exploration to exploration , exploration to explanation , exploration to explanation , and explanation to exploration . 5 . 3 . 1 Exploration to Exploration 22 . 6 % of our sample contains notebooks that begin as exploratory ( 0 . 31 - 0 . 49 ) and remain exploratory after subsequent changes ( scores of 0 . 31 - 0 . 49 ) . Notebooks in this group tend to have a relatively ﬂat slope , suggesting “slow” progress along the sensemaking spectrum . Although they remain exploratory , we still observe both positive ( towards explanation ) and negative ( towards exploration ) shifts within this group . Edit Behavior Authors of these notebooks often depend on code comments to organize , annotate and save code [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Kery et al . ( 2017 ) ] . Code is commented as a way to control the ﬂow of the analysis [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Kery et al . ( 2017 ) ] . Authors also add text within code comments to label analyses and describe insights . These notebooks organize code based on their purpose . Code in loops and functions are sometimes found in separate cells from code that outputs text , tables , or visualizations . Code that outputs are generally found in smaller sections to facilitate quick iteration [ Kery et al . ( 2018 ) ] . The edit behavior within negatively and positively sloping notebooks are the same . Negatively sloping notebooks often capture the removal of visualizations and positively sloping notebooks their additions . 5 . 3 . 2 Exploration to Explanation 15 . 1 % of our sample contains notebooks that begin as exploratory ( scores of 0 . 31 - 0 . 49 ) and become explanatory ( 0 . 50 - 0 . 91 ) . These notebooks had a relatively steep slope , which could be interpreted as “rapid” shifts in sensemaking . They tend to start within a narrow range of exploration scores and end within a wider range of explanatory scores . Edit Behavior The ﬁrst versions of these notebooks typically contain just code or code and visualizations . In subsequent versions , there are two main methods of iteration authors employ . The ﬁrst method consists of adding markdown and code in tandem , such as including annotations and headers into sections as they create and edit code cells . In the second method , authors focus on code iteration ﬁrst , and add markdown and headers in their last few commits [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Dong et al . ( 2021 ) ] . Some authors explicitly label a cleaning phase within their GitHub versions where they prep their notebook for communication purposes . This cleaning phase often involves reordering , splitting , and reformatting code cells as well as adding observations within markdown cells [ Dong et al . ( 2021 ) , Head et al . ( 2019 ) ] . 10 Code Code Evolution 5 . 3 . 3 Explanation to Explanation 60 . 1 % of our sample contains notebooks that begin as explanatory notebooks ( scores of 0 . 50 - 1 . 00 ) and remain explanatory after changes ( 0 . 50 - 1 . 00 ) . Although both positive and negative shifts are observed within this group , these notebooks tend to have relatively ﬂat slopes , similar to the ‘exploration to exploration’ group . This is by far the largest group observed , suggesting that data scientists may prioritize clarity and reproducibility during sensemaking within notebooks , consistent with prior work [ Rule et al . ( 2018 ) , Pimentel et al . ( 2019 ) , Kery et al . ( 2018 ) , Dong et al . ( 2021 ) , Head et al . ( 2019 ) ] . Edit Behavior Notebooks that became less explanatory ( sloped negatively ) often began with a template , to - do list , or a statement of objective at the top of the page . In other words , they started as highly explanatory , which we see reﬂected in these notebooks’ ﬁrst scores , averaging 0 . 7 . Notebooks that became more explanatory ( sloped positively ) lacked an explicit statement of objectives . Objectives , implicit ( in positively sloping notebooks ) or explicit ( in negatively sloping notebooks ) , seemed to drive the construction of the rest of the notebook . For example , if a template speciﬁed three goals , we observed authors attempt each goal sequentially across versions . Some authors even added commit messages about the goal being achieved . When authors implemented each goal , they often added annotations to describe and explain their workﬂow as it progressed . For example , if authors added code and visualizations to the end of the notebook , they also added markdown text to describe their process and results . 5 . 3 . 4 Explanation to Exploration Perhaps not surprisingly , only 2 . 09 % of notebooks started explanatory ( scores of 0 . 50 - 0 . 74 ) and became exploratory ( 0 . 33 - 0 . 49 ) . Relative to other groups , these notebooks shift negatively from within a narrow explanatory range to a narrow exploratory range . Edit Behavior These notebooks progress towards exploration through the removal of explanatory elements . For example , several notebook authors commented code producing visualizations and deleted markdown cells in later versions . This reduction of visualizations and markdown in favor of code may be indicative of authors preparing for new iterations of sensemaking with existing ( and likely duplicate ) code as a starting point [ Koenzen et al . ( 2020 ) , Kery et al . ( 2017 ) ] . 5 . 4 Summary Our qualitative ﬁndings suggest that GitHub commits can capture shifts in notebook editing behaviors over time , which we successfully mapped to corresponding shifts in the authors’ sensemaking . Thus , our results support the idea that one can automatically detect a variety of sensemaking activities within computational notebooks . Although we do see the shift from exploration to explanation emphasized in prior work [ Kery et al . ( 2018 ) , Rule et al . ( 2018 ) , Dong et al . ( 2021 ) , Head et al . ( 2019 ) ] , our analysis also reveals a variety of shifts along the en - tire sensemaking spectrum which were previously unobserved . For example , data scientists may explain their ﬁndings in tandem with exploring their data , as seen through our analysis of the ‘explanation to explanation’ group of notebooks . Furthermore , the ‘exploration to exploration’ group shows that some notebooks have yet to reach the explanatory stage , suggesting that some authors are content to keep certain analyses exploratory . We also observed shifts away from explanation towards exploration . Though previously unobserved ( and in some ways , counter - intuitive ) , this result is consistent with our understanding of the sensemaking spectrum . We believe that some of these notebooks demonstrate the beginning of a new sensemaking iteration . The fact that some of these notebooks start with explicit objectives suggests that authors begin these notebooks with prior knowledge and analysis goals and likely leverage them to streamline their analysis of the data . 6 Discussion We have presented an analysis of 60 , 000 Jupyter Notebooks and their respective GitHub histories . With this corpus , we isolate 2 , 574 notebooks that appear to be data science - oriented , characterize their organization and structure , quantita - tively measure various properties to situate them within the overall sensemaking process [ Pirolli and Card ( 2005 ) ] , and observe how sensemaking within these notebooks shifts across GitHub commits . 11 Code Code Evolution Explore - Explore Explain - Explain Explore - Explain Explain - Explore # of notebooks 582 1549 390 54 % of sample 22 . 6 60 . 1 15 . 1 2 . 00 avg # of versions 9 10 10 11 avg ﬁrst score 0 . 438 0 . 683 0 . 438 0 . 567 avg last score 0 . 453 0 . 695 0 . 618 0 . 469 avg slope value 0 . 002 0 . 0015 0 . 021 - 0 . 012 % positively sloping 58 . 4 54 . 4 96 . 9 5 . 55 % negatively sloping 38 . 3 45 . 5 3 . 00 94 . 4 % neutral sloping 3 . 20 . 06 0 0 Table 2 : General statistics of each notebook group . Explore signiﬁes that the score ( ﬁrst - last ) in the time - series corresponding to the notebooks are in exploratory side of the sensemaking spectrum . Explain signiﬁes that the score is in explanatory side of the spectrum . 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 S e n se m ak i ng S c o r e 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Exploration to Exploration Exploration to Explanation Explanation to Explanation Explanation to Exploration Figure 2 : Visual summary of notebook time - series . Summary within each notebook group , in order : exploration to exploration , exploration to explanation , explanation to explanation , and exploration to exploration . Version Number S e n s e m a k i n g S c o r e Exploration to Explanation Figure 3 : Evolution of the Explore - Explain group . This group includes all notebooks that shifted from exploration to explanation , with the average shift plotted in red . 12 Code Code Evolution 6 . 1 Explaining and Generalizing the Results Our results demonstrate that we can apply qualitative observations from the literature ( e . g . , [ Rule et al . ( 2018 ) , Kery et al . ( 2018 ) , Head et al . ( 2019 ) , Dong et al . ( 2021 ) , Wang et al . ( 2021 ) , Wang et al . ( 2022 ) ] ) to automatically mea - sure sensemaking within Jupyter Notebooks . By measuring how each data science notebook evolves across multiple GitHub commits , we can estimate how and whether subsequent revisions correlate with a notebook becoming more explanatory over time as hypothesized in prior work . Shifts in scores enabled us to identify the discrete steps of analysts as they iterate within the sensemaking spectrum . As Pirolli and Card describe , analysts appear to exhibit a cycle of sensemaking activities . Our ﬁndings also reveal a range of distinct notebook edit behaviors , such as setting analysis goals prior to notebook development . These behaviors align with existing observations of data exploration behaviors with visualization tools [ Battle and Heer ( 2019 ) ] , suggesting that there may be core patterns to sensemaking that seem to transcend particular tools and environments . As a result , our work opens the door to gaining a deeper quantitative understanding of the sensemaking loop itself through the lens of data science tools and practices . 6 . 2 Implications for Data Science Tool Design Our ﬁndings show that authors often use structural aspects of the notebook to track and manage the evolution of their analysis ( Section 5 ) . For example , notebook authors will use markdown cells to annotate sections of analysis with their objectives . We believe that this ﬁnding highlights a need for tools that help data scientists manage their goals while they perform analysis in a notebook [ Head et al . ( 2019 ) ] . Track Multiple Analysis Paths GitHub versioning of computational notebooks does not help to track what data scientists do . For example , an analyst may pursue a particular line of inquiry , realize that a few analysis steps were dead ends , and backtrack to an earlier point to continue their analysis—introducing an alternative branch of investigation . It is hard to represent this non - linear ﬂow with GitHub commits . We need mechanisms that track the actual non - linear and iterative practices of data scientists [ Kery et al . ( 2017 ) , Head et al . ( 2019 ) , Kery and Myers ( 2018 ) ] . We suggest that an extension to current computational notebooks could remedy this problem—an extension that versions and manages cell dependencies . This enhancement would enable notebook users to better track their sensemaking processes and enable researchers to study sensemaking ( and its evolution ) in notebook environments . Generate Relevant Recommendations Using the techniques we have demonstrated , notebook platforms can auto - matically calculate the position of a notebook document within the sensemaking spectrum while it is being edited . Platforms could use this information to support or even enforce best practices . For example , having detected that the author is in the exploratory phase of analysis , the platform may choose to automatically version the document to comprehensively capture competing branches of exploration . We believe this information can be particularly pertinent to data science engines that wish to guide analysts with recommendations on analysis tools and techniques . For example , having detected that an author is performing exploratory analysis , a recommendation engine can cull recommendations from a group of curated exploratory notebooks . Our ideas can direct how recent work , done by Yan et al . [ Yan and He ( 2020 ) ] and Raghunandan et al . [ Raghunandan et al . ( 2021 ) ] , generate recommendations found in Jupyter Notebooks . They can use the context of a notebook to provide more targeted data science recommendations to authors . 6 . 3 Limitations and Future Work Although our techniques produce a relatively small sample compared to the original corpus , our study is still one of the largest analyses of Jupyter Notebooks from GitHub ( e . g . , compared to [ Wang et al . ( 2021 ) , Dong et al . ( 2021 ) ] ) . Part of the problem is the inconsistent notebook quality on GitHub [ Wang et al . ( 2021 ) ] . We combat this challenge by proposing a method to identify data science notebooks suitable for quantitative analysis . This methodology could easily be extended to collect larger notebook corpora in the future ; for example , by curating data science notebooks from all of the millions of notebooks on GitHub . We approached our dataset with an understanding that many authors selectively report their analysis [ Liu et al . ( 2020 ) ] . As our ﬁndings indicate , many notebooks on GitHub are skewed towards the explanatory side of the spectrum , suggesting that some authors may wait until later in the sensemaking process to share their notebooks . Coupled with a lack of ground truth for the mental models of the notebook authors , our ability to infer user intent was limited . We note that this is a fundamental limitation of surveying computational notebooks stored in a public repository such as GitHub , but that the beneﬁts of getting the kind of insight demonstrated here far outweighs this drawback . We address this limitation in part through a mixed - methods analysis strategy in section 4 and section 5 . 13 Code Code Evolution Nevertheless , it would be interesting to develop new strategies for collecting richer notebook metadata to ﬁll observed gaps in GitHub histories and to infer user intent from this metadata . We view our work in this paper as the ﬁrst of many to explore mixed methods towards understanding sensemaking in computational notebooks . Acknowledgments We gratefully acknowledge the classic 1998 Konami game Dance Dance Revolution for inspiring our paper title . Allow us to close with the following immortal words drawn from this game : “ You’re a rockstar . You’re the one they came to see . I’m crying , buckets of tears . ” References [ Alspaugh et al . ( 2018 ) ] Sara Alspaugh , Nava Zokaei , Andrea Liu , Cindy Jin , and Marti A Hearst . 2018 . Futzing and moseying : Interviews with professional data analysts on exploration practices . IEEE Transactions on Visualization and Computer Graphics 25 , 1 ( 2018 ) , 22 – 31 . https : / / doi . org / 10 . 1109 / tvcg . 2018 . 2865040 [ Barnett et al . ( 2005 ) ] Adrian G . Barnett , Jolieke C . Van Der Pols , and Annette J . Dobson . 2005 . Regression to the mean : what it is and how to deal with it . International Journal of Epidemiology 34 , 1 ( 2005 ) , 215 – 220 . https : / / doi . org / 10 . 1093 / ije / dyh299 [ Battle and Heer ( 2019 ) ] Leilani Battle and Jeffrey Heer . 2019 . Characterizing Exploratory Visual Analysis : A Litera - ture Review and Evaluation of Analytic Provenance in Tableau . Computer Graphics Forum 38 , 3 ( 2019 ) , 145 – 159 . https : / / doi . org / 10 . 1111 / cgf . 13678 [ Chattopadhyay et al . ( 2020 ) ] Souti Chattopadhyay , Ishita Prasad , Austin Z Henley , Anita Sarma , and Titus Barik . 2020 . What’s Wrong with Computational Notebooks ? Pain Points , Needs , and Design Opportunities . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3313831 . 3376729 [ Dong et al . ( 2021 ) ] Helen Dong , Shurui Zhou , Jin LC Guo , and Christian Kästner . 2021 . Splitting , renaming , removing : A study of common cleaning activities in Jupyter notebooks . In Proceedings of the IEEE / ACM Conference on Automated Software Engineering Workshops . IEEE , Piscataway , NJ , USA , 114 – 119 . https : / / doi . org / 10 . 1109 / ASEW52652 . 2021 . 00032 [ Foster et al . ( 1992 ) ] WR Foster , F Collopy , and LH Ungar . 1992 . Neural network forecasting of short , noisy time series . Computers & Chemical Engineering 16 , 4 ( 1992 ) , 293 – 297 . https : / / doi . org / 10 . 1016 / 0098 - 1354 ( 92 ) 80049 - F [ Google ( 2019 ) ] Google . 2019 . Colaboratory . https : / / colab . research . google . com / . [ Guo ( 2012 ) ] Philip Jia Guo . 2012 . Software Tools to Facilitate Research Programming . Ph . D . Dissertation . Stanford University . [ Head et al . ( 2019 ) ] Andrew Head , Fred Hohman , Titus Barik , Steven M . Drucker , and Robert DeLine . 2019 . Managing Messes in Computational Notebooks . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , Article 270 , 12 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300500 [ Heer and Agrawala ( 2006 ) ] Jeffrey Heer and Maneesh Agrawala . 2006 . Multi - scale banking to 45 degrees . IEEE Transactions on Visualization and Computer Graphics 12 , 5 ( 2006 ) , 701 – 708 . https : / / doi . org / 10 . 1109 / TVCG . 2006 . 163 [ Källén and Wrigstad ( 2021 ) ] Malin Källén and Tobias Wrigstad . 2021 . Jupyter Notebooks on GitHub : Characteristics and Code Clones . The Art , Science , and Engineering of Programming 5 , 3 , Article 15 ( 2021 ) , 31 pages . arXiv : 2007 . 10146 https : / / arxiv . org / abs / 2007 . 10146 [ Kandel et al . ( 2012 ) ] Sean Kandel , Andreas Paepcke , Joseph M . Hellerstein , and Jeffrey Heer . 2012 . Enterprise Data Analysis and Visualization : An Interview Study . IEEE Transactions on Visualization and Computer Graphics 18 , 12 ( 2012 ) , 2917 – 2926 . https : / / doi . org / 10 . 1109 / TVCG . 2012 . 219 [ Kery et al . ( 2017 ) ] Mary Beth Kery , Amber Horvath , and Brad A Myers . 2017 . Variolite : Supporting Exploratory Programming by Data Scientists . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1265 – 1276 . https : / / doi . org / 10 . 1145 / 3025453 . 3025626 [ Kery and Myers ( 2018 ) ] Mary Beth Kery and Brad A . Myers . 2018 . Interactions for Untangling Messy History in a Computational Notebook . In Proceedings of the IEEE Symposium on Visual Languages and Human - Centric Computing . IEEE , Piscataway , NJ , USA , 147 – 155 . https : / / doi . org / 10 . 1109 / VLHCC . 2018 . 8506576 14 Code Code Evolution [ Kery et al . ( 2018 ) ] Mary Beth Kery , Marissa Radensky , Mahima Arya , Bonnie E . John , and Brad A . Myers . 2018 . The Story in the Notebook : Exploratory Data Science using a Literate Programming Tool . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 174 : 1 – 174 : 11 . https : / / doi . org / 10 . 1145 / 3173574 . 3173748 [ Kluyver et al . ( 2016 ) ] Thomas Kluyver , Benjamin Ragan - Kelley , Fernando Pérez , Brian E . Granger , Matthias Bus - sonnier , Jonathan Frederic , Kyle Kelley , Jessica B . Hamrick , Jason Grout , Sylvain Corlay , Paul Ivanov , Damián Avila , Saﬁa Abdalla , and Carol Willing . 2016 . Jupyter Notebooks – A publishing format for reproducible compu - tational workﬂows . In Positioning and Power in Academic Publishing : Players , Agents and Agendas . IOS Press , Amsterdam , Netherlands , 87 – 90 . https : / / doi . org / 10 . 3233 / 978 - 1 - 61499 - 649 - 1 - 87 [ Knuth ( 1984 ) ] Donald E . Knuth . 1984 . Literate Programming . Comput . J . 27 , 2 ( 1984 ) , 97 – 111 . https : / / doi . org / 10 . 1093 / comjnl / 27 . 2 . 97 [ Koenzen et al . ( 2020 ) ] Andreas P Koenzen , Neil A Ernst , and Margaret - Anne D Storey . 2020 . Code duplication and reuse in Jupyter notebooks . In Proceedings of the IEEE Symposium on Visual Languages and Human - Centric Computing . IEEE , Piscataway , NJ , USA , 1 – 9 . https : / / doi . org / 10 . 1109 / VL / HCC50065 . 2020 . 9127202 [ Kross and Guo ( 2019 ) ] Sean Kross and Philip J Guo . 2019 . Practitioners teaching data science in industry and academia : Expectations , workﬂows , and challenges . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3290605 . 3300493 [ Liu et al . ( 2020 ) ] Yang Liu , Tim Althoff , and Jeffrey Heer . 2020 . Paths explored , paths omitted , paths obscured : Decision points & selective reporting in end - to - end data analysis . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 14 . [ Mathisen et al . ( 2019 ) ] Andreas Mathisen , Tom Horak , Clemens Nylandsted Klokmose , Kaj Grønbæk , and Niklas Elmqvist . 2019 . InsideInsights : Integrating Data - Driven Reporting in Collaborative Visual Analytics . Computer Graphics Forum 38 , 3 ( 2019 ) , 649 – 661 . https : / / doi . org / 10 . 1111 / cgf . 13717 [ McDonald et al . ( 2019 ) ] Nora McDonald , Sarita Schoenebeck , and Andrea Forte . 2019 . Reliability and Inter - rater Reliability in Qualitative Research : Norms and Guidelines for CSCW and HCI Practice . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 23 . https : / / doi . org / 10 . 1145 / 3359174 [ Muller et al . ( 2019a ) ] Michael Muller , Melanie Feinberg , Timothy George , Steven J Jackson , Bonnie E John , Mary Beth Kery , and Samir Passi . 2019a . Human - centered study of data science work practices . In Extended Abstracts of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 8 . https : / / doi . org / 10 . 1145 / 3290607 . 3299018 [ Muller et al . ( 2019b ) ] Michael Muller , Ingrid Lange , Dakuo Wang , David Piorkowski , Jason Tsay , Q Vera Liao , Casey Dugan , and Thomas Erickson . 2019b . How data science workers work with data : Discovery , capture , curation , design , creation . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 15 . https : / / doi . org / 10 . 1145 / 3290605 . 3300356 [ ObservableHQ ( 2011 ) ] ObservableHQ . 2011 . Observable . https : / / observablehq . com . [ Pimentel et al . ( 2019 ) ] Joao Felipe Pimentel , Leonardo Murta , Vanessa Braganholo , and Juliana Freire . 2019 . A large - scale study about quality and reproducibility of jupyter notebooks . In Proceedings of the IEEE / ACM Conference on Mining Software Repositories . IEEE , Piscataway , NJ , USA , 507 – 517 . https : / / doi . org / 10 . 1109 / MSR . 2019 . 00077 [ Pirolli and Card ( 2005 ) ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage points for analyst technology as identiﬁed through cognitive task analysis . In Proceedings of the International Conference on Intelligence Analysis , Vol . 5 . The MITRE Corporation , McLean , VA , USA , 2 – 4 . [ Rädle et al . ( 2017 ) ] Roman Rädle , Midas Nouwens , Kristian Antonsen , James R . Eagan , and Clemens N . Klokmose . 2017 . Codestrates : Literate Computing with Webstrates . In Proceedings of the ACM Symposium on User Interface Software and Technology . ACM , New York , NY , USA , 715 – 725 . https : / / doi . org / 10 . 1145 / 3126594 . 3126642 [ Raghunandan et al . ( 2021 ) ] Deepthi Raghunandan , Zhe Cui , Kartik Krishnan , Segen Tirfe , Shenzhi Shi , Tejaswi Dar - shan Shrestha , Leilani Battle , and Niklas Elmqvist . 2021 . Lodestar : Supporting Independent Learning and Rapid Experimentation Through Data - Drive Analysis Recommendations . In Proceedings of the IEEE Symposium on Viusalization in Data Science . IEEE , Piscataway , NJ , USA , 8 pages . [ Rehman ( 2019 ) ] Mohammed Suhail Rehman . 2019 . Towards understanding data analysis workﬂows using a large notebook corpus . In Proceedings of the ACM Conference on Management of Data . ACM , New York , NY , USA , 1841 – 1843 . https : / / doi . org / 10 . 1145 / 3299869 . 3300107 15 Code Code Evolution [ Rule et al . ( 2019 ) ] Adam Rule , Amanda Birmingham , Cristal Zuniga , Ilkay Altintas , Shih - Cheng Huang , Rob Knight , Niema Moshiri , Mai H . Nguyen , Sara Brin Rosenthal , Fernando Pérez , and Peter W . Rose . 2019 . Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks . PLoS Computational Biology 15 , 7 ( 2019 ) , 8 pages . https : / / doi . org / 10 . 1371 / journal . pcbi . 1007007 [ Rule et al . ( 2018 ) ] Adam Rule , Aurélien Tabard , and James D . Hollan . 2018 . Exploration and Explanation in Compu - tational Notebooks . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 32 : 1 – 32 : 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3173606 [ Russell et al . ( 1993 ) ] Daniel M . Russell , Mark J . Steﬁk , Peter Pirolli , and Stuart K . Card . 1993 . The cost structure of sensemaking . In Proceedings of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 269 – 276 . https : / / doi . org / 10 . 1145 / 169059 . 169209 [ Tabard et al . ( 2008 ) ] Aurélien Tabard , Wendy E . Mackay , and Evelyn Eastmond . 2008 . From individual to collabora - tive : the evolution of Prism , a hybrid laboratory notebook . In Proceedings of the ACM Conference on Computer Supported Cooperative Work . ACM , New York , NY , USA , 569 – 578 . https : / / doi . org / 10 . 1145 / 1460563 . 1460653 [ Thomas and Cook ( 2005 ) ] James J . Thomas and Kristin A . Cook . 2005 . Illuminating the Path : The Research and Development Agenda for Visual Analytics . IEEE Computer Society Press , Los Alamitos , CA , USA . [ Tufte ( 2001 ) ] Edward R Tufte . 2001 . The Visual Display of Quantitative Information . Vol . 2 . Graphics Press , Cheshire , CT , USA . [ Tukey ( 1977 ) ] John W . Tukey . 1977 . Exploratory Data Analysis . Addison - Wesley , Reading , MA , USA . [ Wang et al . ( 2019 ) ] April Yi Wang , Anant Mittal , Christopher Brooks , and Steve Oney . 2019 . How Data Scientists Use Computational Notebooks for Real - Time Collaboration . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 39 : 1 – 39 : 30 . https : / / doi . org / 10 . 1145 / 3359141 [ Wang et al . ( 2021 ) ] April Yi Wang , Dakuo Wang , Jaimie Drozdal , Xuye Liu , Soya Park , Steve Oney , and Christopher Brooks . 2021 . What makes a well - documented notebook ? A case study of data scientists’ documentation practices in Kaggle . In Extended Abstracts of the ACM Conference on Human Factors in Computing Systems . ACM , New York , NY , USA , 1 – 7 . https : / / doi . org / 10 . 1145 / 3411763 . 3451617 [ Wang et al . ( 2022 ) ] April Yi Wang , Dakuo Wang , Jaimie Drozdal , Michael Muller , Soya Park , Justin D Weisz , Xuye Liu , Lingfei Wu , and Casey Dugan . 2022 . Documentation Matters : Human - Centered AI System to Assist Data Science Code Documentation in Computational Notebooks . ACM Transactions on Computer - Human Interaction 29 , 2 ( 2022 ) , 1 – 33 . https : / / doi . org / 10 . 1145 / 3489465 [ Wang et al . ( 2020 ) ] Jiawei Wang , KUO Tzu - Yang , Li Li , and Andreas Zeller . 2020 . Assessing and restoring repro - ducibility of Jupyter notebooks . In Proceedings of the IEEE / ACM Conference on Automated Software Engineering . IEEE , Piscataway , NJ , USA , 138 – 149 . https : / / doi . org / 10 . 1145 / 3324884 . 3416585 [ Wenskovitch et al . ( 2019 ) ] John Wenskovitch , Jian Zhao , Scott Carter , Matthew Cooper , and Chris North . 2019 . Albireo : An interactive tool for visually summarizing computational notebook structure . In Proceedings of the IEEE Symposium on Visualization in Data Science . IEEE , Piscataway , NJ , USA , 1 – 10 . https : / / doi . org / 10 . 1109 / VDS48975 . 2019 . 8973385 [ Wickham and Grolemund ( 2017 ) ] Hadley Wickham and Garrett Grolemund . 2017 . R for Data Science : Import , Tidy , Transform , Visualize , and Model Data . O’Reilly Media , Sebastopol , CA , USA . [ Wongsuphasawat et al . ( 2019 ) ] Kanit Wongsuphasawat , Yang Liu , and Jeffrey Heer . 2019 . Goals , Process , and Challenges of Exploratory Data Analysis : An Interview Study . CoRR abs / 1911 . 00568 ( 2019 ) , 11 pages . arXiv : 1911 . 00568 http : / / arxiv . org / abs / 1911 . 00568 [ Yan and He ( 2020 ) ] Cong Yan and Yeye He . 2020 . Auto - Suggest : Learning - to - Recommend Data Preparation Steps Using Data Science Notebooks . In Proceedings of the ACM Conference on Management of Data . ACM , New York , NY , USA , 1539 – 1554 . https : / / doi . org / 10 . 1145 / 3318464 . 3389738 16