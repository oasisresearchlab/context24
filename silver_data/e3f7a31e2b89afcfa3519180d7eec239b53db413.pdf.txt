Review of Economic Studies ( 2005 ) 72 , 1 – 19 0034 - 6527 / 05 / 00010000001 $ 02 . 00 c (cid:13) 2005 The Review of Economic Studies Limited Semiparametric Difference - in - Differences Estimators ALBERTO ABADIE Harvard University and NBER First version received June 2001 ; ﬁnal version accepted October 2003 ( Eds . ) The difference - in - differences ( DID ) estimator is one of the most popular tools for applied research in economics to evaluate the effects of public interventions and other treatments of interest on some relevant outcome variables . However , it is well known that the DID estimator is based on strong identifying assumptions . In particular , the conventional DID estimator requires that , in the absence of the treatment , the average outcomes for the treated and control groups would have followed parallel paths over time . This assumption may be implausible if pre - treatment characteristics that are thought to be associated with the dynamics of the outcome variable are unbalanced between the treated and the untreated . That would be the case , for example , if selection for treatment is inﬂuenced by individual - transitory shocks on past outcomes ( Ashenfelter’s dip ) . This article considers the case in which differences in observed characteristics create non - parallel outcome dynamics between treated and controls . It is shown that , in such a case , a simple two - step strategy can be used to estimate the average effect of the treatment for the treated . In addition , the estimation framework proposed in this article allows the use of covariates to describe how the average effect of the treatment varies with changes in observed characteristics . A good way to do econometrics is to look for good natural experiments and use statistical methods that can tidy up the confounding factors that nature has not controlled for us . ( Daniel McFadden , Econometric Tools ) 1 . INTRODUCTION The use of natural experiments to evaluate treatment effects in the absence of truly experimental data has gained wide acceptance in empirical research in economics and other social sciences . Simple comparisons of pre - treatment and post - treatment outcomes for those individuals exposed to a treatment are likely to be contaminated by temporal trends in the outcome variable or by the effect of events , other than the treatment , that occurred between both periods . However , when only a fraction of the population is exposed to the treatment , an untreated comparison group can be used to identify temporal variation in the outcome that is not due to treatment exposure . The difference - in - differences ( DID ) estimator is based on this simple idea . Card and Krueger ( 1994 ) assess the employment effects of a raise in the minimum wage in New Jersey using a neighbouring state , Pennsylvania , to identify the variation in employment that New Jersey would have experienced in the absence of a raise in the minimum wage . Other applications of DID include studies of the effects of immigration on native wages and employment ( Card , 1990 ) , the effects of temporary disability beneﬁts on time out of work after an injury ( Meyer , Viscusi and Durbin , 1995 ) , and the effect of anti - takeover laws on ﬁrms’ leverage ( Garvey and Hanka , 1999 ) . It is well known that the conventional DID estimator is based on strong assumptions . In particular , the conventional DID estimator requires that in absence of the treatment , the 1 2 REVIEW OF ECONOMIC STUDIES average outcomes for treated and controls would have followed parallel paths over time . This assumption may be implausible if pre - treatment characteristics that are thought to be associated with the dynamics of the outcome variable are unbalanced between the treated and the untreated group . This study considers the case in which differences in observed characteristics create non - parallel outcome dynamics for the treated and untreated groups . It is shown that , in such a case , a simple two - step strategy can be used to estimate the average effect of the treatment for the treated . In addition , the estimation framework proposed in this article allows the use of covariates to describe how the average effect of the treatment varies with changes in observed characteristics . Despite the proliﬁc literature on semiparametric and non - parametric methods , few articles have been devoted to studying and relaxing the DID identiﬁcation restrictions . Some exceptions are Besley and Case ( 1994 ) , Meyer ( 1995 ) , Heckman , Ichimura and Todd ( 1997 ) , Imbens , Liebman and Eissa ( 1997 ) , Heckman , Ichimura , Smith and Todd ( 1998 ) , Angrist and Krueger ( 1999 ) , Blundell and MaCurdy ( 1999 ) , Blundell , Costa Dias , Meghir and van Reenen ( 2001 ) and Athey and Imbens ( 2002 ) . The identiﬁcation procedure used in this article originated with Heckman et al . ( 1997 , 1998 ) . However , the estimation procedure differs from the earlier literature in three ways . First , it does not require repeated observations for the same individuals . The proposed estimators are feasible under the data requirements for traditional DID estimators when applied to repeated cross - sections . Second , it allows the estimation of parsimonious parametric approximations to the average effect of the treatment on the treated conditional on selected covariates of interest . Finally , the framework can accommodate multilevel treatment variables ( that is , different treatment intensities ) . The rest of the article is organized as follows . Section 2 describes the conventional DID model and discusses some of its limitations . Section 3 presents the main identiﬁcation results of the article , followed by some extensions . The estimation strategy along with asymptotic distribution theory is provided in Section 4 . Section 5 concludes . Proofs are presented in the Appendix . 2 . THE DID ESTIMATOR The basic DID framework can be described as follows . Let Y ( i , t ) be the outcome of interest for individual i at time t . The population is observed in a pre - treatment period t = 0 , and in a post - treatment period t = 1 . Between these two periods , some fraction of the population is exposed to the treatment . We denote D ( i , t ) = 1 if individual i has been exposed to the treatment previous to period t , D ( i , t ) = 0 otherwise . We call those individuals with D ( i , 1 ) = 1 treated , and those with D ( i , 1 ) = 0 controls ( or untreated ) . Since individuals are only exposed to treatment after the ﬁrst period , D ( i , 0 ) = 0 for all i . The conventional DID estimator is often derived using a linear parametric model . It is useful to consider this formulation of the DID model ﬁrst , to ﬁx ideas , before studying non - parametric identiﬁcation in Section 3 . The following formulation of the DID model is based on that given in Ashenfelter and Card ( 1985 ) . Suppose that the outcome variable is generated by a components of variance process Y ( i , t ) = δ ( t ) + α · D ( i , t ) + η ( i ) + υ ( i , t ) , ( 1 ) where δ ( t ) is a time - speciﬁc component , α represents the impact of the treatment , η ( i ) is an individual - speciﬁc component , and υ ( i , t ) is an individual - transitory shock that has mean zero at each period , t = 0 , 1 , and is possibly correlated in time . Only Y ( i , t ) and D ( i , t ) are observed . The effect of the treatment , α , is not identiﬁed without further restrictions . A sufﬁcient condition ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 3 for identiﬁcation is that selection for treatment does not depend on the individual - transitory shocks , that is P ( D ( i , 1 ) = 1 | υ ( i , t ) ) = P ( D ( i , 1 ) = 1 ) ( 2 ) for t = 0 , 1 . Adding and subtracting E [ η ( i ) | D ( i , 1 ) ] in equation ( 1 ) , we obtain Y ( i , t ) = δ ( t ) + α · D ( i , t ) + E [ η ( i ) | D ( i , 1 ) ] + ε ( i , t ) , ( 3 ) where ε ( i , t ) = η ( i ) − E [ η ( i ) | D ( i , 1 ) ] + υ ( i , t ) . Notice that δ ( t ) = δ ( 0 ) + ( δ ( 1 ) − δ ( 0 ) ) t , and E [ η ( i ) | D ( i , 1 ) ] = E [ η ( i ) | D ( i , 1 ) = 0 ] + ( E [ η ( i ) | D ( i , 1 ) = 1 ] − E [ η ( i ) | D ( i , 1 ) = 0 ] ) D ( i , 1 ) . Let µ = E [ η ( i ) | D ( i , 1 ) = 0 ] + δ ( 0 ) , τ = E [ η ( i ) | D ( i , 1 ) = 1 ] − E [ η ( i ) | D ( i , 1 ) = 0 ] and δ = δ ( 1 ) − δ ( 0 ) . We obtain Y ( i , t ) = µ + τ · D ( i , 1 ) + δ · t + α · D ( i , t ) + ε ( i , t ) . ( 4 ) The restriction in equation ( 2 ) for t = 0 , 1 implies E [ ( 1 , D ( i , 1 ) , t , D ( i , t ) ) · ε ( i , t ) ] = 0 , so all the parameters in equation ( 4 ) , including the treatment impact α , are estimable by least squares . Notice that the model allows any kind of dependence between selection for treatment , D ( i , 1 ) = 1 , and the individual - speciﬁc component , η ( i ) . This model is called “difference - in - differences” because under the identifying condition in equation ( 2 ) we have α = { E [ Y ( i , 1 ) | D ( i , 1 ) = 1 ] − E [ Y ( i , 1 ) | D ( i , 1 ) = 0 ] } − { E [ Y ( i , 0 ) | D ( i , 1 ) = 1 ] − E [ Y ( i , 0 ) | D ( i , 1 ) = 0 ] } , ( 5 ) and the least squares estimator of α is the sample counterpart of equation ( 5 ) . This formulation of the problem is useful when repeated cross sections of ( Y ( i , t ) , D ( i , 1 ) ) for t = 0 , 1 are available . If a sample with repeated pre - treatment and post - treatment observations of the outcome variable , Y ( i , 1 ) and Y ( i , 0 ) , is available , then α is estimable by least squares regression of Y ( i , 1 ) − Y ( i , 0 ) on D ( i , 1 ) : α = E [ Y ( i , 1 ) − Y ( i , 0 ) | D ( i , 1 ) = 1 ] − E [ Y ( i , 1 ) − Y ( i , 0 ) | D ( i , 1 ) = 0 ] . Note that equation ( 2 ) for t = 0 , 1 implies that υ ( i , 1 ) − υ ( i , 0 ) is mean independent of D ( i , 1 ) , and therefore that , in absence of the treatment , the average outcome for the treated would have experienced the same variation as the average outcome for the untreated . This restriction , implied by the model , may be too stringent if treated and controls are unbalanced in covariates that are thought to be associated with the dynamics of the outcome variable . For example , it has been documented that participants in training programmes experience a decline in earnings prior to the training period ( Ashenfelter’s dip , Ashenfelter ( 1978 ) ) . This fact suggests that selection for training may be affected by individual - transitory shocks in pre - training earnings . To accommodate this conjecture , Ashenfelter and Card ( 1985 ) propose the following model for the selection process : D ( i , 1 ) = ( 1 if Y ( i , 1 − κ ) + u ( i ) < ¯ Y 0 otherwise , ( 6 ) where κ is a positive integer , ¯ Y is a constant , and u ( i ) is a random variable independent of any variance component . Under this formulation for the selection process , those individuals with low earning κ periods before training are more likely to participate in the training programme . The identiﬁcation condition in equation ( 2 ) does not hold in general for this example . The reason is that the individual - speciﬁc components , υ ( i , t ) , are allowed to be correlated in time . However , if the selection process can be represented by equation ( 6 ) , then P ( D ( i , 1 ) = 1 | Y ( i , 1 − κ ) , v ( i , t ) ) = P ( D ( i , 1 ) = 1 | Y ( i , 1 − κ ) ) . The DID model holds conditional on 4 REVIEW OF ECONOMIC STUDIES Y ( i , 1 − κ ) , so the impact of the treatment is given by { E [ Y ( i , 1 ) | X ( i ) , D ( i , 1 ) = 1 ] − E [ Y ( i , 1 ) | X ( i ) , D ( i , 1 ) = 0 ] } − { E [ Y ( i , 0 ) | X ( i ) , D ( i , 1 ) = 1 ] − E [ Y ( i , 0 ) | X ( i ) , D ( i , 1 ) = 0 ] } ( 7 ) where X ( i ) = Y ( i , 1 − κ ) . 1 More generally , in this article X ( i ) is a vector of observed characteristics , such as demographic attributes , predetermined at t = 0 . A conditional identiﬁcation restriction is appealing in the DID framework when the variables in X ( i ) are believed to be related to the outcome dynamics and their distributions differ between treated and controls . The traditional way to accommodate covariates in the DID model is to introduce them linearly in equation ( 4 ) : Y ( i , t ) = µ + X ( i ) 0 π ( t ) + τ · D ( i , 1 ) + δ · t + α · D ( i , t ) + ε ( i , t ) , ( 8 ) where X ( i ) is assumed uncorrelated with ε ( i , t ) . Because the coefﬁcients on X ( i ) change with t , this formulation of the DID model allows the use of covariates to represent heterogeneity in outcome dynamics . Differencing the last equation with respect to t , we obtain Y ( i , 1 ) − Y ( i , 0 ) = δ + X ( i ) 0 π + α · D ( i , 1 ) + ( ε ( i , 1 ) − ε ( i , 0 ) ) , where π = π ( 1 ) − π ( 0 ) . This alternative formulation is useful when a sample with repeated observations is available . However , as noticed by Meyer ( 1995 ) , introducing covariates in this linear fashion may not be appropriate if the treatment has different effects for different groups in the population . Heterogeneity in treatment effects can be studied by specifying α in equation ( 8 ) as a function of X ( i ) ( i . e . by including interactions between X ( i ) and D ( i , t ) in equation ( 8 ) ) . Ideally , covariates should be treated non - parametrically , as in equation ( 7 ) , so that any potential inconsistency created by functional form misspeciﬁcation is avoided . However , when the number of covariates required to attain identiﬁcation is large , some kind of integration over X ( i ) is required in order to obtain interpretable results . The next section proposes a ﬂexible new procedure to control for the effect of covariates in the DID model which is based on conditional identiﬁcation restrictions . As in the conventional DID model , the role of the covariates in this new approach is twofold . First , by using covariates we extend identiﬁcation to those instances in which observed compositional differences between treated and controls cause non - parallel dynamics in the outcome variable . In addition , the effect of the treatment is allowed to differ among individuals , so covariates can be used to describe the effect of the treatment for different groups of the population . A distinctive feature of the methods proposed in this article is that , while covariates are treated non - parametrically for identiﬁcation , the estimators provide parsimonious parametric approximations to the average effect of the treatment on the treated conditional on selected covariates of interest . 2 A related way to accommodate covariates in a DID estimator has been explored by Heckman et al . ( 1997 , 1998 ) who propose a DID estimator of the average treatment effect on the treated based also on conditional identiﬁcation restrictions . Their estimator is constructed by matching differences in pre - treatment and post - treatment outcomes for the treated to weighted averages of differences in pre - treatment and post - treatment outcomes for the untreated . The differences are matched on the probability of treatment exposure conditional on the covariates 1 . In fact , for Ashenfelter’s dip model in equation ( 6 ) , the second term in equation ( 7 ) is zero and estimation could be based solely on conditional averages of post - treatment outcomes . 2 . In other words , while identiﬁcation does not hinge on parametric restrictions , it allows the use of parametric functions to describe how the effect of the treatment varies with covariates . This approach follows the spirit of White ( 1981 ) and Roehrig ( 1988 ) among others who propose treating parametric models as convenient approximations to unknown functions of interest , so identiﬁcation can be studied non - parametrically . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 5 ( the propensity score ) and the weights are determined non - parametrically using local linear regression . 3 This article , however , proposes a direct weighting scheme on the propensity score that can be used to estimate the effect of the treatment on the treated without estimating weights non - parametrically in a previous step . Estimators of treatment effects that weight on functions of the probability of treatment are based on the Horvitz – Thompson statistic ( Horvitz and Thompson , 1952 ) . Non - parametric generalizations of the Horvitz – Thompson statistic have been studied in Imbens , Hirano and Ridder ( 2003 ) . Ichimura and Linton ( 2002 ) have developed higher - order asymptotic expansions for this class of estimators to guide bandwidth selection . Robins and co - authors have proposed related estimators in the context of parametric models for time - varying treatments ( see , e . g . Hern´an , Brumback and Robins , 2001 ) . In these studies it is assumed that all other factors , aside from the treatment , that affect the outcome variable are either observed , or their distribution is the same for treated and untreated . Consequently , all factors which confound simple comparisons of the outcome distribution between treated and untreated are observed . In contrast , the identiﬁcation conditions used in this article allow for the distribution of both observed and unobserved factors to differ between treated and untreated , as long as the effect of unobserved factors on the outcome does not vary with time ( or , more generally , if it experiences the same variation , on average , for treated and untreated ) . 3 . NON - PARAMETRIC IDENTIFICATION In the previous section , we referred to α as the “impact of the treatment” or the “treatment effect” , but the exact meaning of these terms was left undeﬁned . As in Rubin ( 1974 ) and Heckman ( 1990 ) , the effect of the treatment will be deﬁned in terms of potential outcomes . Y 0 ( i , t ) represents the outcome that individual i would attain at time t in absence of the treatment . In the same fashion , Y 1 ( i , t ) represents the outcome that individual i would attain at time t if exposed to the treatment . The effect of the treatment on the outcome for individual i at time t is then naturally deﬁned as Y 1 ( i , t ) − Y 0 ( i , t ) . The fundamental identiﬁcation problem is that for any particular individual i and time period t , we do not observe both potential outcomes Y 0 ( i , t ) and Y 1 ( i , t ) ; so we cannot compute the individual treatment effect Y 1 ( i , t ) − Y 0 ( i , t ) . We only observe the realized outcome , Y ( i , t ) that can be expressed as Y ( i , t ) = Y 0 ( i , t ) · ( 1 − D ( i , t ) ) + Y 1 ( i , t ) · D ( i , t ) . Since , in the simple scenario considered here , the treatment is only administered after period t = 0 , we can denote D ( i ) = D ( i , 1 ) , then we have that Y ( i , 0 ) = Y 0 ( i , 0 ) and Y ( i , 1 ) = Y 0 ( i , 1 ) · ( 1 − D ( i ) ) + Y 1 ( i , 1 ) · D ( i ) . Given the impossibility of computing individual treatment effects , researchers often focus on estimating some average effect , like the average effect of the treatment on the treated E [ Y 1 ( i , 1 ) − Y 0 ( i , 1 ) | D ( i ) = 1 ] ( see , e . g . Heckman , 1990 ) . Sometimes , when the desired level of aggregation is lower , researchers try to learn about some conditional version of the average effect on the treated E [ Y 1 ( i , 1 ) − Y 0 ( i , 1 ) | X ( i ) , D ( i ) = 1 ] . For the rest of the article , the individual argument i will be dropped to reduce notation . I will take the next assumption to hold throughout this article . Assumption 3 . 1 . E [ Y 0 ( 1 ) − Y 0 ( 0 ) | X , D = 1 ] = E [ Y 0 ( 1 ) − Y 0 ( 0 ) | X , D = 0 ] . Assumption 3 . 1 is the crucial identifying restriction in DID models . It states that , conditional on the covariates , the average outcomes for treated and controls would have followed parallel 3 . Blundell et al . ( 2001 ) propose a related estimator which combines DID and matching on the propensity score . 6 REVIEW OF ECONOMIC STUDIES paths in absence of the treatment . 4 Notice that when E [ Y 0 ( 0 ) | X , D = 1 ] = E [ Y 0 ( 0 ) | X , D = 0 ] , Assumption 3 . 1 collapses to a “selection on observables” restriction ( E [ Y 0 ( 1 ) | X , D = 1 ] = E [ Y 0 ( 1 ) | X , D = 0 ] ) which can be used in cross - sectional studies to identify the effect of the treatment on the treated . 5 Therefore , the results in this article also apply to this particular situation . This is the case for Ashenfelter’s dip example , discussed in Section 2 . I will come back to this point in Section 3 . 2 . Existence of the expectations is assumed throughout . Under Assumption 3 . 1 , the effect of the treatment on the treated conditional on X can be expressed as ( Heckman et al . , 1997 ) : 6 E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] = { E [ Y ( 1 ) | X , D = 1 ] − E [ Y ( 1 ) | X , D = 0 ] } − { E [ Y ( 0 ) | X , D = 1 ] − E [ Y ( 0 ) | X , D = 0 ] } . ( 9 ) Even when Assumption 3 . 1 holds unconditionally , if it also holds conditional on some predetermined covariates of interest ( e . g . gender ) , we may still use the conditional identiﬁcation result to evaluate the effect of the treatment for different groups of the population ( e . g . women vs . men ) . In principle , the identiﬁcation result in equation ( 9 ) can be used to estimate E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] by producing non - parametric estimates of each one of the four expectations on the R . H . S . of equation ( 9 ) . In practice , the number of observations required to attain an acceptable precision for this type of non - parametric estimator increases very rapidly with the dimension of X . This problem , often called the curse of dimensionality , may prevent us from using non - parametric estimators for E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] in many practical instances . In addition , a simple non - parametric estimator of E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] , directly based on equation ( 9 ) , may be difﬁcult to interpret if the dimension of X is larger than two , so we cannot summarize the result in a graph . In those cases , some integration over the distribution of X for the treated is required in order to produce summary statistics . Even then , the estimation process is cumbersome . An estimator of average treatment effects for the treated directly based on equation ( 9 ) requires estimating four conditional expectations non - parametrically ( or two if a sample with repeated outcomes , Y ( 0 ) and Y ( 1 ) , for the same individuals is available ) and then integrating the estimates to the desired level of aggregation . This article proposes simple weighting schemes to produce estimators of the average effect on the treated E [ Y 1 ( 1 ) − Y 0 ( 1 ) | D = 1 ] and parsimonious parametric approximations to its conditional version E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] , where X k is a function of X ( for example , 4 . Using experimental data , Heckman et al . ( 1997 , 1998 ) have shown the plausibility of this identifying assumption in the context of the evaluation of a subsidized training programme . 5 . In that case , Assumption 3 . 1 implies E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] = E [ Y ( 1 ) | X , D = 1 ] − E [ Y ( 1 ) | X , D = 0 ] , and pre - treatment data are not required to identify the average effect of the treatment on the treated . If , in addition , E [ Y 1 ( 1 ) | X , D = 1 ] = E [ Y 1 ( 1 ) | X , D = 0 ] , then E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X ] = E [ Y ( 1 ) | X , D = 1 ] − E [ Y ( 1 ) | X , D = 0 ] . Selection on observables implies that all factors which confound simple comparisons of outcomes between treated and controls are observed . This is a too stringent assumption if the distribution of unobserved variables which affect the outcome is believed to differ between treated and controls . See , e . g . Rubin ( 1977 ) and Heckman et al . ( 1997 ) . 6 . A researcher could be interested in estimating E [ Y 1 ( 1 ) | X , D = 1 ] and E [ Y 0 ( 1 ) | X , D = 1 ] separately . Since Y 1 ( 1 ) is observed for the treated , both conditional expectations are identiﬁed . In fact , given that Y 1 ( 1 ) is observed for the treated , identiﬁcation results on E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] are equivalent to identiﬁcation results on E [ Y 0 ( 1 ) | X , D = 1 ] . Here , I concentrate on the difference E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] because it has been the object of interest in the difference - in - differences literature ( see , e . g . Heckman et al . ( 1997 , 1998 ) ) . Notice also that Assumption 3 . 1 by itself does not identify the average treatment effects conditional only on the covariates ( E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X ] ) , unless conditional average effects coincide for treated and untreated E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] = E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 0 ] . The reason is that Assumption 3 . 1 identiﬁes E [ Y 0 ( 1 ) | X , D = 1 ] , and therefore , the effect of the treatment on the treated . However , Assumption 3 . 1 leaves E [ Y 1 ( 1 ) | X , D = 0 ] totally unrestricted ; so the effect of the treatment on the untreated is also unrestricted . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 7 a subset of the variables in X ) . The weighting scheme is directly based on the propensity score , P ( D = 1 | X ) , which is the only function which needs to be estimated in a ﬁrst step . As a result , the proposed method reduces the ﬁrst step estimation burden and allows the researcher to use four or two times more observations for ﬁrst step estimation , relative to direct estimation of equation ( 9 ) . In practice , this feature may be an important advantage if non - parametric estimation is carried out in the ﬁrst step . When the number of observations is too small for non - parametric estimation in the ﬁrst step , the proposed method allows the researcher to circumvent the curse of dimensionality by placing parametric restrictions on the propensity score , which leaves E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] unrestricted , rather than on each one of the conditional means of equation ( 9 ) , which may impose unwanted restrictions on E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] . The estimation of parametric approximations to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] has some beneﬁts . First , it provides a simple method to produce estimation results at the level of aggregation desired by the analyst . In addition , the results are parsimoniously summarized by the estimates of the parameters that deﬁne the approximation to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] . However , the quality of the information provided by our estimators will be low if the quality of the approximation to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] is poor . Since the object of study is the effect of the treatment on the treated , the minimal requirement for the problem to be well deﬁned is that some fraction of the population is exposed to the treatment . In addition , since identiﬁcation is attained after controlling for the effect of some covariates X , it will be required that for each given value of the covariates there is some fraction of the population that remains untreated and can be used as controls . Assumption 3 . 2 . P ( D = 1 ) > 0 and with probability one P ( D = 1 | X ) < 1 . Note that Assumption 3 . 2 implies that the support of the propensity score for the treated is a subset of the support of the propensity score for the untreated . This is a well - known condition for identiﬁcation of the average impact on the treated under selection on covariates ( see , e . g . Heckman et al . , 1997 ) . 3 . 1 . Random sample with repeated outcomes In this section , I introduce the identiﬁcation strategy proposed in this article by considering the situation in which we can observe both pre - treatment and post - treatment outcomes for a random sample of the population of interest . Examples of applications of DID estimators to data on repeated outcomes are Card and Krueger ( 1994 ) , Heckman et al . ( 1997 , 1998 ) , Garvey and Hanka ( 1999 ) and Blundell et al . ( 2001 ) . Under this sampling scheme , for each individual in our sample we observe ( Y ( 1 ) , Y ( 0 ) , D , X ) . Later in the article , the identiﬁcation procedure is extended to repeated cross sections . Lemma 3 . 1 . If Assumption 3 . 1 holds , and for values of X such that 0 < P ( D = 1 | X ) < 1 , we have E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] = E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X ] , where ρ 0 = D − P ( D = 1 | X ) P ( D = 1 | X ) · ( 1 − P ( D = 1 | X ) ) . For notational convenience , let ρ 0 = − 1 if P ( D = 1 | X ) = 0 ( this choice is inconsequential since the objects of interest will be integrals over the distribution of the X conditional on D = 1 ) . The average effect of the treatment for the treated is given by 8 REVIEW OF ECONOMIC STUDIES E [ Y 1 ( 1 ) − Y 0 ( 1 ) | D = 1 ] = Z E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] d P ( X | D = 1 ) = Z E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X ] d P ( X | D = 1 ) = E (cid:20) ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) · P ( D = 1 | X ) P ( D = 1 ) (cid:21) = E (cid:20) Y ( 1 ) − Y ( 0 ) P ( D = 1 ) · D − P ( D = 1 | X ) 1 − P ( D = 1 | X ) (cid:21) . ( 10 ) In words , under Assumptions 3 . 1 and 3 . 2 , a simple weighted average of temporal differences in the outcome variable recovers the average effect of the treatment for the treated . The weights depend on the propensity score . On an intuitive level , this scheme works by weighting - down the distribution of Y ( 1 ) − Y ( 0 ) for the untreated for those values of the covariates which are over - represented among the untreated ( that is , with low P ( D = 1 | X ) / P ( D = 0 | X ) ) , and weighting - up Y ( 1 ) − Y ( 0 ) for those values of the covariates under - represented among the untreated ( that is with high P ( D = 1 | X ) / P ( D = 0 | X ) ) . In this way the same distribution of the covariates is imposed for treated and untreated . 7 Equation ( 10 ) suggests a simple two - step method to estimate the average effect of the treatment on the treated under Assumptions 3 . 1 and 3 . 2 : ( i ) estimate the propensity score , P ( D = 1 | X ) , and compute the ﬁtted values for the sample ; ( ii ) plug the ﬁtted values into the sample analogue of equation ( 10 ) to obtain an estimate of E [ Y 1 ( 1 ) − Y 0 ( 1 ) | D = 1 ] . In many practical instances , the desired level of aggregation is lower than the entire treated population and the analyst wants to study how the treatment affects the treated for different groups of the population . As explained above , a non - parametric estimator of E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] may be difﬁcult to interpret , especially if the dimension of X is large . Such a problem is circumvented here by focusing on parametric approximations to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] . More generally , consider the situation in which we need to condition on some vector of random variables X to attain identiﬁcation , but we are interested in E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] , where X k is some deterministic function of X . 8 This situation is relevant when the number of covariates needed in order to attain identiﬁcation is large , so the analyst may be willing to allow for a higher level of aggregation in the second step in order to obtain parsimonious results . Consider a class of approximating functions G = { g ( X k ; θ ) : θ ∈ 2 ⊂ R k } , square - integrable with respect to P ( X k | D = 1 ) . Then , a least squares approximation from G to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] is given by g ( X k ; θ 0 ) where θ 0 = arg min θ ∈ 2 E [ { E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) } 2 | D = 1 ] . ( 11 ) For example , if G = { X 0 k θ : θ ∈ 2 ⊂ R k } , then θ 0 deﬁnes a linear least squares approximation to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] . It is assumed that θ 0 exists uniquely . Proposition 3 . 1 . If Assumptions 3 . 1 and 3 . 2 hold , then θ 0 = arg min θ ∈ 2 E [ P ( D = 1 | X ) · { ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − g ( X k ; θ ) } 2 ] . 7 . Similarly , Heckman et al . ( 1997 ) use matching on the propensity score to account for imbalances in the distribution of the covariates between treated and untreated . Matching on the propensity score works because it imposes the same distribution of the covariates for the two groups ( see Rosenbaum and Rubin , 1983 ) . 8 . For example , X k may contain a subset of the variables in X . In other instances , X may contain indicators for all different values of a discrete variable included in X k . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 9 As before , it is easy to construct a two - step estimator based on the sample analogue of the result in the last proposition . After some algebra , the result in equation ( 10 ) can be obtained applying Proposition 3 . 1 to the case when g ( X k , θ ) is constant . 3 . 2 . Some extensions and particular cases This section contains extensions to repeated cross sections , and multilevel treatments . It also discusses the case of selection on observables . 3 . 2 . 1 . Repeated cross sections . Often , a random sample with repeated outcomes is not available . In such a case , repeated cross - section data - sets ( pre - treatment and post - treatment ) may be used to construct DID estimators . However , the use of repeated cross sections for DID presents some issues of data availability . First , treatment status ( in the post - treatment period ) must be known for the individuals in the pre - treatment sample . This requirement is satisﬁed , for example , if treatment exposure can be determined from some individual characteristic observed in both periods . 9 , 10 In addition , covariates must be observed in the post - treatment sample . Since covariates are often pre - treatment variables , this second requirement may prove a problem when covariates are time - varying and there are not retrospective covariate data available . Examples of applications of DID estimators with covariates to repeated cross sections are Card ( 1990 , 1992 ) , Meyer et al . ( 1995 ) , Eissa and Liebman ( 1996 ) , Acemoglu and Angrist ( 2001 ) , Corak ( 2001 ) and Finkelstein ( 2002 ) . Here , I show how to apply the methods proposed in this article to repeated cross sections . The data requirements are the same as for traditional difference - in - differences estimators which use cross - sectional data and covariates . Assume that random samples are available for the pre - treatment and the post - treatment periods . For each individual in the pooled sample ( post - treatment and pre - treatment ) , we observe Z = ( Y , D , T , X ) where T is a time indicator that takes value one if the observation belongs to the post - treatment sample . Assumption 3 . 3 . Conditional on T = 0 , the data are i . i . d . from the distribution of ( Y ( 0 ) , D , X ) ; conditional on T = 1 , the data are i . i . d . from the distribution of ( Y ( 1 ) , D , X ) . This sampling scheme produces the following mixture distribution : P M ( Y = y , D = d , X = x , T = t ) = λ · t · P ( Y ( 1 ) = y , D = d , X = x ) + ( 1 − λ ) · ( 1 − t ) · P ( Y ( 0 ) = y , D = d , X = x ) , where λ ∈ ( 0 , 1 ) reﬂects the proportion of the observations sampled in the post - treatment period . 11 Let E M [ · ] denote expectations with respect to P M ( · ) . Lemma 3 . 2 . If Assumptions 3 . 1 and 3 . 3 hold , and for values of X such that 0 < P ( D = 1 | X ) < 1 , we have E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] = E M [ ϕ 0 · Y | X ] , where ϕ 0 = T − λ λ · ( 1 − λ ) · D − P ( D = 1 | X ) P ( D = 1 | X ) · P ( D = 0 | X ) . 9 . Note that identiﬁcation requires in turn that such individual characteristic is excluded from X . Otherwise , the support condition in Assumption 3 . 2 would be violated . This exclusion restriction is problematic if the excluded variable inﬂuences the dynamics of the outcome variable , so Assumption 3 . 1 is not plausible . 10 . The requirement may also be satisﬁed in other cases , for example when the pre - treatment sample can be linked to administrative data records on treatment participation . 11 . For simplicity , I do not consider more complicated situations in which the data may be generated by stratiﬁed sampling ( on X or D ) . In such a case , the results in this section apply for a suitably reweighted sample ( see , e . g . Wooldridge , 2002 ) . 10 REVIEW OF ECONOMIC STUDIES Then , following a reasoning similar to that of the previous section we have that the average treatment effect on the treated is identiﬁed by E M (cid:20) P ( D = 1 | X ) P ( D = 1 ) · ϕ 0 · Y (cid:21) = E [ Y 1 ( 1 ) − Y 0 ( 1 ) | D = 1 ] . ( 12 ) The following proposition is analogous to Proposition 3 . 1 . Proposition 3 . 2 . If Assumptions 3 . 1 , 3 . 2 , and 3 . 3 hold , then for θ 0 deﬁned in equation ( 11 ) we have θ 0 = arg min θ ∈ 2 E M [ P ( D = 1 | X ) · { ϕ 0 · Y − g ( X k ; θ ) } 2 ] . ( 13 ) The result in equation ( 12 ) can be obtained by considering a constant g ( X k , θ ) in equation ( 13 ) . 3 . 2 . 2 . Selection on observables . In the previous section , it is shown how to approximate conditional average treatment effects by ﬁrst weighting temporal differences in the outcome variable on the propensity score , and then projecting the weighted differences on a set of parametric functions of the covariates . The interpretation of the resulting functionals as approximations to conditional average treatment effects comes from the difference - in - differences condition in Assumption 3 . 1 . However , it should be noticed that the same “ﬁrst weight , then project” strategy can be applied in other contexts . In particular , the results in the previous section carry over naturally to “selection on observables” : E [ Y 0 ( 1 ) | X , D = 1 ] = E [ Y 0 ( 1 ) | X , D = 0 ] . ( 14 ) The reason is that “selection on observables” can be expressed as a particular case of Assumption 3 . 1 ( when E [ Y 0 ( 0 ) | X , D = 1 ] = E [ Y 0 ( 0 ) | X , D = 0 ] ) . As a result , if equation ( 14 ) holds , then θ 0 = arg min θ ∈ 2 E [ P ( D = 1 | X ) · { ρ 0 · Y − g ( X k ; θ ) } 2 ] , for θ 0 deﬁned in equation ( 11 ) and Y = Y ( 1 ) . 12 , 13 Similar estimators have been considered in Wooldridge ( 2001 ) . Imbens et al . ( 2003 ) consider the situation in which the average treatment effect is estimated for a given distribution of the covariates . Abadie ( 2003 ) applies similar approximation methods in the context of instrumental variable models for treatment effects . 3 . 2 . 3 . Multilevel treatments . So far , we have considered only the case of a binary treatment , which is the usual focus of DID estimators . However , the same ideas can be applied when individuals may be exposed to different levels ( or doses ) of the treatment . Let W represent the level of the treatment . For untreated individuals , let W = 0 . For the treated , suppose that W takes on a ﬁnite number of positive values w 1 < · · · < w J , with positive probability . 14 Let 12 . For this case , it is useful to deﬁne Y = Y ( 1 ) because equation ( 14 ) may be adopted as an identiﬁcation restriction in absence of measures on the outcome variable in a pretreatment period . However , as shown for Ashenfelter’s dip example , it may be necessary to condition on the values of the outcome variable in a pre - treatment period in order for equation ( 14 ) to hold . 13 . Alternatively , if the objects of interest are average treatment effects conditional only on the covariates , θ 0 can be redeﬁned as θ 0 = argmin θ ∈ 2 E [ { E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k ] − g ( X k , θ ) } 2 ] . However , equation ( 14 ) does not identify the effect of the treatment on the untreated because it leaves E [ Y 1 ( 1 ) | X , D = 0 ] completely unrestricted . If we assume in addition that E [ Y 1 ( 1 ) | X , D = 1 ] = E [ Y 1 ( 1 ) | X , D = 0 ] , then θ 0 is identiﬁed by θ 0 = argmin θ ∈ 2 E [ ( ρ 0 · Y − g ( X k ; θ ) ) 2 ] . 14 . Here , I assume that treatment levels are ordered ( e . g . number of weeks in a training programme ) . For expositional simplicity and since it is often the case in applications , I consider only a ﬁnite number of treatment levels . However , the analysis presented in this section can be generalized to continuous treatments by substituting densities for W for probabilities for W , and integrals over those densities for sums . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 11 W + = { w 1 , . . . , w J } , then D = 1 W + ( W ) , where 1 A ( · ) is the indicator function for the set A ( that is , 1 A ( w ) = 1 if w ∈ A , zero otherwise ) . For w ∈ { 0 } ∪ W + and t ∈ { 0 , 1 } , let Y w ( t ) be the potential outcome for treatment level w and period t . Suppose that Assumption 3 . 1 holds for each treatment level : E [ Y 0 ( 1 ) − Y 0 ( 0 ) | X , W = w ] = E [ Y 0 ( 1 ) − Y 0 ( 0 ) | X , W = 0 ] , for w ∈ W + . In words , this assumption requires that , in absence of the treatment , the average outcomes for all treatment groups would have followed parallel trends , conditional on the covariates . As in the usual DID case with a binary treatment variable , the assumption allows the levels of average potential outcomes without the treatment to differ arbitrarily between treatment groups . For w ∈ W + , let ρ w 0 = ( 1 { w } ( W ) / P ( W = w | X ) ) − ( 1 { 0 } ( W ) / P ( W = 0 | X ) ) . ( Note that , for ρ 0 deﬁned in Lemma 3 . 1 , ρ 0 = ρ 10 . ) Then , following the same reasoning as for Lemma 3 . 1 , we obtain E [ ρ w 0 ( Y ( 1 ) − Y ( 0 ) ) | X ] = E [ Y w ( 1 ) − Y 0 ( 1 ) | X , W = w ] . In addition , for some class of square - integrable approximating functions G = { g ( W , X k ; θ ) : θ ∈ 2 } , redeﬁne θ 0 = arg min θ ∈ 2 E [ ( E [ Y W ( 1 ) − Y 0 ( 1 ) | X k , W ] − g ( W , X k ; θ ) ) 2 | D = 1 ] . ( 15 ) The parameters θ 0 deﬁne a least squares approximation to a function describing average effects for the treated E [ Y w ( 1 ) − Y 0 ( 1 ) | X k , W = w ] . Then , these parameters are identiﬁed by θ 0 = arg min θ ∈ 2 E (cid:20)X J j = 1 P ( W = w j | X ) ( ρ w j 0 ( Y ( 1 ) − Y ( 0 ) ) − g ( w j , X k ; θ ) ) 2 (cid:21) . This result collapses to the result in Proposition 3 . 1 if W is binary , and can be proven using the same argument as for Proposition 3 . 1 . 4 . ESTIMATION AND ASYMPTOTIC DISTRIBUTION For concreteness , I will concentrate here on linear approximations to E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] where X k is a deterministic function of X . In addition , only the case of repeated cross sections is explicitly considered here . However , the analysis is also valid for the case of repeated observations if Y ( 1 ) − Y ( 0 ) is substituted for ( ( T − λ ) / λ ( 1 − λ ) ) · Y , and expectations are taken with respect to the distribution of ( Y ( 1 ) , Y ( 0 ) , D , X ) . Consider β 0 = arg min β ∈ 2 E M [ π 0 · { ϕ 0 Y − X 0 k β } 2 ] where π 0 ( X ) = P ( D = 1 | X ) , and ϕ 0 = T − λ λ · ( 1 − λ ) · D − π 0 ( X ) π 0 ( X ) · ( 1 − π 0 ( X ) ) . Consider also the following estimator of β 0 : b β = (cid:18) 1 n X n i = 1 X ki b π ( X i ) X 0 ki (cid:19) − 1 1 n X n i = 1 X ki b π ( X i ) b ϕ i Y i , where b π ( X i ) is an estimator of π 0 ( X i ) , and b ϕ i = T i − λ λ · ( 1 − λ ) · D i − b π ( X i ) b π ( X i ) · ( 1 − b π ( X i ) ) , for λ = n 1 / ( n 0 + n 1 ) . Under the conditions of the theorems stated below , b β is well deﬁned with probability approaching one . 4 . 1 . Non - parametric ﬁrst step estimation of the propensity score Here , I consider the case in which non - parametric ( power series ) regression is used in a ﬁrst step to estimate π 0 . Let ζ = ( ζ 1 , . . . , ζ r ) 0 be a vector of non - negative integers where r is 12 REVIEW OF ECONOMIC STUDIES the dimension of X . Also let X ζ = Q rj = 1 X ζ j j and | ζ | = P rj = 1 ζ j . Let { ζ ( k ) } ∞ k = 1 be a sequence containing all distinct vectors ζ , with | ζ | non - decreasing . For a positive integer K , let p K ( X ) = ( p 1 K ( X ) , . . . , p KK ( X ) ) 0 where p kK ( X ) = X ζ ( k ) . Then , for K = K ( n ) → ∞ a power series non - parametric estimator of π 0 is given by b π ( X ) = p K ( X ) 0 b γ ( 16 ) where b γ = ( P ni = 1 p K ( X i ) p K ( X i ) 0 ) − ( P ni = 1 p K ( X i ) D i ) and A − denotes any symmetric generalized inverse of the matrix A . Assumption 4 . 1 . ( i ) K 6 / n = o ( 1 ) , π 0 ( X ) is continuously differentiable of order s , and nK − 2 s / r = O ( 1 ) ; ( ii ) the support of X is a Cartesian product of compact intervals on which X has density that is bounded away from zero ; ( iii ) π 0 ( X ) is bounded away from zero and one ; ( iv ) β 0 is an interior point of a compact set 2 ⊂ R k ; ( v ) E M Y 2 < ∞ , k X k k is bounded , and E [ X k X 0 k | D = 1 ] is non - singular . Let δ ( X ) = E M (cid:20) X k (cid:18) T − λ λ · ( 1 − λ ) · D − 1 ( 1 − π 0 ) 2 · Y − X 0 k β 0 (cid:19) (cid:12)(cid:12)(cid:12)(cid:12) X (cid:21) . Theorem 4 . 1 . If n 0 , n 1 → ∞ , n 1 / ( n 0 + n 1 ) = λ ∈ ( 0 , 1 ) and Assumptions 3 . 1 , 3 . 3 and 4 . 1 hold , then n 1 / 2 ( b β − β 0 ) d → N ( 0 , V ) , where V = Q − 1 6 Q − 1 , Q = E [ X k DX 0 k ] , 6 = E M [ ψψ 0 ] , ψ = X k π 0 ( X ) ( ϕ 0 Y − X 0 k β 0 ) + δ ( X ) · ( D − π 0 ( X ) ) . To construct an estimator of the asymptotic variance , let b V = b Q − 1 b 6 b Q − 1 , where b Q = 1 n X n i = 1 X ki D i X 0 ki , b 6 = 1 n X n i = 1 b ψ i b ψ 0 i , b δ ( X i ) = (cid:18)X n i = 1 X ki (cid:18) T i − λ λ · ( 1 − λ ) · D i − 1 ( 1 − b π ( X i ) ) 2 · Y i − X 0 ki b β (cid:19) p K ( X i ) 0 (cid:19) × (cid:16)X n i = 1 p K ( X i ) p K ( X i ) 0 (cid:17) − p K ( X i ) and b ψ i = X ki b π ( X i ) ( b ϕ i Y i − X 0 ki b β ) + b δ ( X i ) · ( D i − b π ( X i ) ) . The following theorem establishes the consistency of b V . Theorem 4 . 2 . If the assumptions of Theorem 4 . 1 hold and K 7 / n → 0 , then b V p → V . 4 . 2 . Parametric ﬁrst step estimation of the propensity score Often , samples are too small to use a non - parametric estimator in the ﬁrst step . This is particularly likely when the analyst deals with longitudinal data - sets . In such cases , it may be convenient to use a parametric restriction in the ﬁrst step and estimate the propensity score by maximum likelihood . This section provides distribution theory for that case . The results include probit , logit and linear probability ﬁrst step estimation of π 0 as special cases . Assumption 4 . 2 . ( i ) γ 0 is an interior point of a compact set 0 ⊂ R r ; ( ii ) the support of X is a subset of a compact set S ; E [ X X 0 ] is non - singular ; ( iii ) there is a ( known ) function π : R 7→ [ 0 , 1 ] such that π 0 ( X ) = π ( X 0 γ 0 ) ; ( iv ) let V = { x 0 γ : x ∈ S , γ ∈ 0 } ; for v ∈ V , π ( v ) is bounded away from zero and one , strictly increasing and continuously differentiable with ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 13 derivative bounded away from zero and inﬁnity ; ( v ) β 0 is an interior point of a compact set 2 ⊂ R k ; ( vi ) E M Y 2 < ∞ , k X k k is bounded , and E [ X k X 0 k | D = 1 ] is non - singular . Under this assumption , γ 0 can be estimated by maximum likelihood : b γ = arg max γ ∈ 0 1 n X n i = 1 D i log π ( X 0 i γ ) + ( 1 − D i ) log ( 1 − π ( X 0 i γ ) ) . Then , b π ( X i ) = π ( X 0 i b γ ) . Boundedness of X is not a necessary restriction but allows Assumption 4 . 2 to encompass maximum likelihood estimation of the logit , probit and linear probability models . For simplicity , and since in most cases X k is some subset of the variables in X , X k is also assumed to be bounded . If the covariates are discrete and the vector X is saturated with indicators of all the possible values of the covariates , then the functional form of π is not restrictive and the estimation of b π ( X ) is completely non - parametric . Let ˙ π = ∂π ( v ) / ∂v and ˙ π 0 = ˙ π ( X 0 γ 0 ) . Under standard regularity conditions ( e . g . Assumption 4 . 2 ( i ) – ( iv ) ) , b γ is asymptotically linear , that is n 1 / 2 ( b γ − γ 0 ) = n − 1 / 2 P ni = 1 ψ γ 0 ( Z i ) + o p ( 1 ) , where ψ γ 0 ( Z ) = E " ˙ π 20 π 0 ( 1 − π 0 ) X X 0 # − 1 X ˙ π 0 π 0 ( 1 − π 0 ) ( D − π 0 ) . ( 17 ) Let M γ 0 = E M (cid:20) X k (cid:18) T − λ λ ( 1 − λ ) D − 1 ( 1 − π 0 ) 2 Y − X 0 k β 0 (cid:19) ˙ π 0 X 0 (cid:21) . Theorem 4 . 3 . If n 0 , n 1 → ∞ , n 1 / ( n 0 + n 1 ) = λ ∈ ( 0 , 1 ) and Assumptions 3 . 1 , 3 . 3 and 4 . 2 hold , then √ n ( b β − β 0 ) d → N ( 0 , V ) , where V = Q − 1 6 Q − 1 , Q = E [ X k DX 0 k ] , 6 = E M [ ψψ 0 ] , ψ = m ( Z , β 0 , γ 0 ) + M γ 0 ψ γ 0 , m ( Z , β 0 , γ 0 ) = X k π 0 [ ϕ 0 · Y − X 0 k β 0 ] . Let b V = b Q − 1 b 6 b Q − 1 , where b Q = 1 n X n i = 1 X ki D i X 0 ki , b 6 = 1 n X n i = 1 b ψ i b ψ 0 i , b M b γ = 1 n X n i = 1 X ki (cid:18) T i − λ λ ( 1 − λ ) D i − 1 ( 1 − b π ( X i ) ) 2 Y i − X 0 ki b β (cid:19) ˙ π ( X 0 i b γ ) X 0 i , b ψ b γ ( Z i ) = 1 n X n i = 1 ˙ π ( X 0 i b γ ) 2 b π ( X i ) ( 1 − b π ( X i ) ) X i X 0 i ! − 1 X i ˙ π ( X 0 i b γ ) b π ( X i ) ( 1 − b π ( X i ) ) ( D i − b π ( X i ) ) , and b ψ i = X ki b π ( X i ) ( b ϕ i Y i − X 0 ki b β ) + b M b γ b ψ b γ ( Z i ) . The following theorem establishes consistency of b V . Theorem 4 . 4 . If the assumptions of Theorem 4 . 3 hold and π ( v ) is twice differentiable with bounded second derivative in V , then b V p → V . 5 . CONCLUSIONS In this article , I have introduced a family of semiparametric difference - in - differences estimators of treatment effects based on conditional identiﬁcation restrictions . These estimators may be particularly appropriate when the distribution of observed characteristics that are thought to be related to the dynamics of the outcome variable differs between treated and untreated . 14 REVIEW OF ECONOMIC STUDIES Identiﬁcation does not entail parametric restrictions . However , the methods presented here can be used to estimate parsimonious parametric approximations to conditional versions of the average treatment effect for the treated . APPENDIX . PROOFS Proof of Lemma 3 . 1 . For 0 < P ( D = 1 | X ) < 1 , we have that E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X ] = E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X , D = 1 ] · P ( D = 1 | X ) + E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X , D = 0 ] · P ( D = 0 | X ) = E [ Y ( 1 ) − Y ( 0 ) | X , D = 1 ] − E [ Y ( 1 ) − Y ( 0 ) | X , D = 0 ] . Applying equation ( 9 ) we obtain the result . k Proof of Proposition 3 . 1 . Let G ( θ ) = E [ P ( D = 1 | X ) · { ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − g ( X k ; θ ) } 2 ] . Adding and subtracting E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] , we obtain G ( θ ) = E [ P ( D = 1 | X ) · { ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] } 2 ] + E [ P ( D = 1 | X ) · { E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) } 2 ] + 2 E [ P ( D = 1 | X ) · ( ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] ) × ( E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) ) ] . ( A . 1 ) The ﬁrst term on the R . H . S . of equation ( A . 1 ) does not depend on θ . By the law of iterated expectations , the second term on the R . H . S . of equation ( A . 1 ) is equal to E [ D · { E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) } 2 ] = E [ { E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) } 2 | D = 1 ] P ( D = 1 ) . Therefore , by Assumption 3 . 2 and equation ( 11 ) , the second term on the R . H . S . of equation ( A . 1 ) is minimized at θ 0 . The expectation in the third term on the R . H . S . of equation ( A . 1 ) is equal to E [ E [ P ( D = 1 | X ) · ( ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] ) | X k ] × ( E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] − g ( X k ; θ ) ) ] . Applying the law of iterated expectations and Lemma 3 . 1 : E [ P ( D = 1 | X ) · ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X k ] = E [ P ( D = 1 | X ) E [ ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X ] | X k ] = E [ P ( D = 1 | X ) E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X , D = 1 ] | X k ] = E [ E [ D ( Y 1 ( 1 ) − Y 0 ( 1 ) ) | X ] | X k ] = E [ D ( Y 1 ( 1 ) − Y 0 ( 1 ) ) | X k ] = P ( D = 1 | X k ) · E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] . Therefore , E [ P ( D = 1 | X ) · ( ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) − E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] ) | X k ] = E [ P ( D = 1 | X ) · ρ 0 · ( Y ( 1 ) − Y ( 0 ) ) | X k ] − P ( D = 1 | X k ) E [ Y 1 ( 1 ) − Y 0 ( 1 ) | X k , D = 1 ] = 0 . Consequently , the third term on the R . H . S . of equation ( A . 1 ) is equal to zero , and the result of the proposition holds . k Proof of Lemma 3 . 2 . Notice that E M [ ϕ 0 · Y | X ] = E M [ E M [ ϕ 0 · Y | X , T ] | X ] = E M [ E [ ϕ 0 · Y | X , T ] | X ] = E [ ρ 0 · Y | X , T = 1 ] − E [ ρ 0 · Y | X , T = 0 ] = E [ ρ 0 · Y ( 1 ) | X ] − E [ ρ 0 · Y ( 0 ) | X ] = { E [ Y ( 1 ) | X , D = 1 ] − E [ Y ( 1 ) | X , D = 0 ] } − { E [ Y ( 0 ) | X , D = 1 ] − E [ Y ( 0 ) | X , D = 0 ] } , and the result follows from equation ( 9 ) . k Proof of Proposition 3 . 2 . The proof follows the same steps as the proof of Proposition 3 . 1 . Proof of Theorem 4 . 1 . Let | · | ∞ denote the supremum norm . By Assumption 4 . 1 ( i ) , ( ii ) and Theorem 4 in Newey ( 1997 ) , it follows that | b π − π 0 | ∞ = O p ( K · [ ( K / n ) 1 / 2 + K − s / r ] ) = o p ( 1 ) . Let m ( Z , β , π ) = X k π ( ϕ ( π ) · Y − X 0 k β ) . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 15 Assumption 4 . 1 ( v ) guarantees that E M [ m ( Z , β , π 0 ) ] has a unique zero at β 0 = ( E [ X k π 0 X 0 k ] ) − 1 E M [ X k π 0 ϕ 0 Y ] . In addition , sup β ∈ 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , b π ) − E M [ m ( Z , β , π 0 ) ] (cid:13)(cid:13)(cid:13)(cid:13) ≤ sup β ∈ 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , b π ) − m ( Z i , β , π 0 ) (cid:13)(cid:13)(cid:13)(cid:13) + sup β ∈ 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , π 0 ) − E M [ m ( Z , β , π 0 ) ] (cid:13)(cid:13)(cid:13)(cid:13) . ( A . 2 ) Let ϕ 0 i = T i − λ λ · ( 1 − λ ) · D i − π 0 ( X i ) π 0 ( X i ) · ( 1 − π 0 ( X i ) ) . Notice that , m ( Z i , β , b π ) − m ( Z i , β , π 0 ) = X ki { ( b π ( X i ) b ϕ i − π 0 ( X i ) ϕ 0 i ) Y i − ( b π ( X i ) − π 0 ( X i ) ) X 0 ki β } , and b π ( X i ) b ϕ i − π 0 ( X i ) ϕ 0 i = T i − λ λ ( 1 − λ ) · D i − 1 ( 1 − b π ( X i ) ) ( 1 − π 0 ( X i ) ) · ( b π ( X i ) − π 0 ( X i ) ) . By Assumption 4 . 1 ( iii ) and uniform convergence of b π , with probability approaching one b π is bounded away from one and there is a constant C such that sup β ∈ 2 (cid:13)(cid:13)(cid:13) (cid:13) 1 n X n i = 1 m ( Z i , β , b π ) − m ( Z i , β , π 0 ) (cid:13)(cid:13)(cid:13) (cid:13) ≤ C · 1 n X n i = 1 { k X ki Y i k + k X ki k 2 k β k } | b π − π 0 | ∞ . Therefore , the ﬁrst term of the R . H . S . of equation ( A . 2 ) is o p ( 1 ) . Assumption 4 . 1 also implies that k m ( Z , β , π 0 ) k is dominated by an integrable function . Then , since m ( Z , β , π 0 ) is continuous at each β ∈ 2 compact , Lemma 2 . 4 in Newey and McFadden ( 1994 ) implies that the second term of the R . H . S . of equation ( A . 2 ) is also o p ( 1 ) and that E [ m ( Z , β , π 0 ) ] is continuous . Now , the usual consistency argument for estimators based on moment equations ( e . g . van der Vaart ( 1998 , Theorem 5 . 9 ) ) proves that b β p → β 0 . Since | b π − π 0 | ∞ converges in probability to zero , with probability approaching one n − 1 P ni = 1 X ki b π ( X i ) X 0 ki is non - singular and n 1 / 2 ( b β − β 0 ) = (cid:18) 1 n X n i = 1 X ki b π ( X i ) X 0 ki (cid:19) − 1 · 1 √ n X n i = 1 X ki b π ( X i ) ( b ϕ i Y i − X 0 ki β 0 ) . Since X k has second moments , 1 n X n i = 1 X ki b π ( X i ) X 0 ki = E [ X k DX 0 k ] + o p ( 1 ) . Now , let us prove that 1 √ n X n i = 1 m ( Z i , β 0 , b π ) = 1 √ n X n i = 1 X ki b π ( X i ) ( b ϕ i Y i − X 0 ki β 0 ) = 1 √ n X n i = 1 ψ ( Z i ) + o p ( 1 ) . Consider 3 ( Z , π , β , e π ) = (cid:18) ∂ ∂π m ( Z , β , π ) | π = e π (cid:19) · π = X k (cid:20) T − λ λ · ( 1 − λ ) · D − 1 ( 1 − e π ) 2 · Y − X 0 k β (cid:21) · π . It can be easily seen that for π , e π bounded away from one we have m ( Z , β , π ) − m ( Z , β , e π ) − 3 ( Z , π − e π , β , e π ) = X k · T − λ λ · ( 1 − λ ) · D − 1 ( 1 − π ) ( 1 − e π ) 2 · Y · ( π − e π ) 2 . Therefore , since , with probability approaching one , ( 1 − b π ) − 1 is bounded by some constant C , we have (cid:13)(cid:13)(cid:13)(cid:13) 1 √ n X n i = 1 m ( Z i , β 0 , b π ) − m ( Z i , β 0 , π 0 ) − 3 ( Z i , b π − π 0 , β 0 , π 0 ) (cid:13)(cid:13)(cid:13)(cid:13) ≤ n 1 / 2 | b π − π 0 | 2 ∞ C · 1 n X n i = 1 (cid:13)(cid:13)(cid:13)(cid:13) X ki · T i − λ λ · ( 1 − λ ) · D i − 1 ( 1 − π 0 ) 2 · Y i (cid:13)(cid:13)(cid:13)(cid:13) = o p ( 1 ) . This result holds because Assumption 4 . 1 ( i ) implies that n 1 / 2 K 2 · ( K / n + K − 2 s / r ) → 0 , and therefore n 1 / 2 | b π − π 0 | 2 ∞ = o p ( 1 ) . The assumptions E M | Y | < ∞ , k X k k bounded and π 0 bounded away from one take care of the sample average term . Therefore 1 √ n X n i = 1 m ( Z i , β 0 , b π ) = 1 √ n X n i = 1 m ( Z i , β 0 , π 0 ) + 1 √ n X n i = 1 3 ( Z i , b π − π 0 , β 0 , π 0 ) + o p ( 1 ) . 16 REVIEW OF ECONOMIC STUDIES Note that k 3 ( Z , π , β 0 , π 0 ) k ≤ b ( Z ) · | π | ∞ with b ( Z ) = k X k [ ( T − λ ) / ( λ · ( 1 − λ ) ) ] · [ ( D − 1 ) / ( 1 − π 0 ) 2 ] · Y − X 0 k β 0 k . By E M Y 2 < ∞ and k X k k bounded , we have that E M b ( Z ) 2 < ∞ . Then , it follows from the proof of Theorem 6 . 1 in Newey ( 1994 ) that 1 √ n X n i = 1 { 3 ( Z i , b π − π 0 , β 0 , π 0 ) − E M [ 3 ( Z , b π − π 0 , β 0 , π 0 ) ] } = O p ( K − s / r ) + O p ( K · [ ( K / n ) 1 / 2 + K − s / r ] ) = o p ( 1 ) . Note that E M [ 3 ( Z , π , β 0 , π 0 ) ] = E [ δ ( X ) π ( X ) ] . If : ( a ) For each K , there exists ξ K such that E [ k δ ( X ) − ξ K p K ( X ) k 2 ] → 0 and nK − 2 s / r E [ k δ ( X ) − ξ K p K ( X ) k 2 ] → 0 , ( b ) K 5 / n → 0 , ( c ) K 2 · K − 2 s / r → 0 , then , Assumption 6 . 6 in Newey ( 1994 ) holds for 3 ( Z , π , β 0 , π 0 ) . Condition ( a ) holds by Assumption 4 . 1 ( i ) and by square - integrability of δ ( X ) . Conditions ( b ) and ( c ) follow from Assumption 4 . 1 ( i ) . Under such conditions , Newey ( 1994 ) shows that n 1 / 2 E M [ 3 ( Z , b π − π 0 , β 0 , π 0 ) ] = 1 √ n X n i = 1 δ ( X i ) · ( D i − π 0 ( X i ) ) + o p ( 1 ) . So the result of the theorem follows from existence of second moments of ψ . k Proof of Theorem 4 . 2 . By the law of large numbers b Q converges in probability to Q , which is non - singular . Notice that X ki b π ( X i ) ( b ϕ i Y i − X 0 ki b β ) − X ki π 0 ( X i ) ( ϕ 0 i Y i − X 0 ki β 0 ) = X ki (cid:18) T i − λ λ ( 1 − λ ) · D i − 1 ( 1 − b π ( X i ) ) ( 1 − π 0 ( X i ) ) (cid:19) Y i ( b π ( X i ) − π 0 ( X i ) ) − X ki X 0 ki ( b β − β 0 ) . Now , since k X ki k is bounded , E M Y 2 < ∞ , π 0 is bounded away from zero , and | b π − π 0 | ∞ and k b β − β 0 k are o p ( 1 ) , we obtain 1 n X n i = 1 k X ki b π ( X i ) ( b ϕ i Y i − X 0 ki b β ) − X ki π 0 ( X i ) ( ϕ 0 i Y i − X 0 ki β 0 ) k 2 = o p ( 1 ) . ( A . 3 ) For π bounded away from one , we have that 3 ( Z , ˜ π , β , π ) − 3 ( Z , ˜ π , β 0 , π 0 ) = (cid:26)(cid:18) X k T − λ λ ( 1 − λ ) · ( 1 − π 0 ) + ( 1 − π ) ( 1 − π ) 2 ( 1 − π 0 ) 2 ( D − 1 ) Y (cid:19) ( π − π 0 ) − X k X 0 k ( β − β 0 ) (cid:27) ˜ π · Therefore , there exists some function , b ( · ) , such that k 3 ( Z , ˜ π , β , π ) − 3 ( Z , ˜ π , β 0 , π 0 ) k ≤ b ( Z ) ( | π − π 0 | ∞ + k β − β 0 k ) | ˜ π | ∞ with E M b ( Z ) < ∞ . This result , along with K 7 / n → 0 guarantees that 1 n X n i = 1 k b δ ( X i ) ( D i − b π ( X i ) ) − δ ( X i ) ( D i − π 0 ( X i ) ) k 2 = o p ( 1 ) ( A . 4 ) ( see Newey , 1994 ) . Equations ( A . 3 ) and ( A . 4 ) , along with the Triangle and H¨older’s inequalities , imply b 6 p → 6 . As a result , b V p → V . k Proof of Theorem 4 . 3 . First , let us prove that under Assumption 4 . 2 ( i ) – ( iv ) , b γ is asymptotically linear with inﬂuence function given by equation ( 17 ) . By Assumption 4 . 2 ( ii ) , E [ XX 0 ] is non - singular , therefore ( γ − γ 0 ) 0 E [ XX 0 ] ( γ − γ 0 ) > 0 for γ 6 = γ 0 . As a result , for γ 6 = γ 0 , X 0 γ 6 = X 0 γ 0 in a set of positive probability . Since π ( v ) is strictly increasing in v ∈ V , we have that π ( X 0 γ ) 6 = π ( X 0 γ 0 ) in a set of positive probability . Let m γ = D log ( π ( X 0 γ ) ) + ( 1 − D ) log ( 1 − π ( X 0 γ ) ) . E [ m γ − m γ 0 | X ] = E [ D log ( π ( X 0 γ ) / π ( X 0 γ 0 ) ) + ( 1 − D ) log ( ( 1 − π ( X 0 γ ) ) / ( 1 − π ( X 0 γ 0 ) ) ) | X ] = π ( X 0 γ 0 ) log ( π ( X 0 γ ) / π ( X 0 γ 0 ) ) + ( 1 − π ( X 0 γ 0 ) ) log ( ( 1 − π ( X 0 γ ) ) / ( 1 − π ( X 0 γ 0 ) ) ) ≤ 0 . ( A . 5 ) The last inequality follows from log λ ≤ λ − 1 , for λ > 0 , which is strict for λ 6 = 1 . Since π ( X 0 γ ) 6 = π ( X 0 γ 0 ) with positive probability , then the inequality in equation ( A . 5 ) is strict with positive probability and E [ m γ − m γ 0 ] < 0 for γ 6 = γ 0 . Since π ( v ) is bounded away from zero and one in V , then the absolute value of m γ is bounded by a constant ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 17 for any γ ∈ 0 . This last fact , along with compactness of 0 and S , continuity of π ( · ) , and Lemma 2 . 4 in Newey and McFadden ( 1994 ) guarantees that E [ m γ ] is a continuous function of γ and that sup γ ∈ 0 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m γ ( Z i ) − E [ m γ ( Z ) ] (cid:13)(cid:13)(cid:13)(cid:13) p → 0 . These conditions are sufﬁcient for b γ p → γ 0 ( see , e . g . Newey and McFadden ( 1994 , Theorem 2 . 1 ) ) . Now , let us study the asymptotic distribution of √ n ( b γ − γ 0 ) . Assumption 4 . 2 along with Lemma 7 . 6 in van der Vaart ( 1998 ) guarantees that , for p γ = π ( X 0 γ ) D ( 1 − π ( X 0 γ ) ) ( 1 − D ) , the map γ 7→ p 1 / 2 γ is differentiable in quadratic mean with derivative ˙ l γ = X ˙ π ( X 0 γ ) π ( X 0 γ ) ( 1 − π ( X 0 γ ) ) ( D − π ( X 0 γ ) ) . Therefore , the map γ 7→ p 1 / 2 γ is differentiable in quadratic mean at γ 0 with derivative ˙ l γ 0 . Take a convex open neighbourhood N γ 0 of γ 0 contained in 0 . By Assumption 4 . 2 ( ii ) and ( iv ) , k ∂ m γ / ∂γ k is bounded by a constant on N γ 0 . Therefore , by Theorem 9 . 19 in Rudin ( 1976 ) , | m γ 1 − m γ 2 | ≤ M k γ 1 − γ 2 k for any γ 1 , γ 2 ∈ N γ 0 and for some constant M . Finally notice that , by Assumption 4 . 2 ( ii ) – ( iv ) , E [ ˙ l γ 0 ˙ l 0 γ 0 ] = E [ { ˙ π 20 / ( π 0 ( 1 − π 0 ) ) } XX 0 ] is non - singular . Therefore , by Theorem 5 . 39 in van der Vaart ( 1998 ) , we obtain n 1 / 2 ( b γ − γ 0 ) = n − 1 / 2 P ni = 1 ψ γ 0 ( Z i ) + o p ( 1 ) . Let m ( Z , β , γ ) = X k π ( X 0 γ ) [ ϕ ( Z , γ ) Y − X 0 k β ] . By Assumption 4 . 2 ( vi ) , E M [ m ( Z , β , γ 0 ) ] has a unique zero at β 0 = ( E [ X k π 0 X 0 k ] ) − 1 E M [ X k π 0 ϕ 0 Y ] . Under the assumptions of the theorem , the function m ( Z , β , γ ) is continuous at each ( β , γ ) ∈ 2 × 0 . Since π ( X 0 γ ) is bounded away from zero and one , and both Y and k X k k have ﬁnite ﬁrst moments , then k m ( Z , β , γ ) k is dominated by a variable with ﬁnite ﬁrst moment . Therefore , by Lemma 2 . 3 in Newey and McFadden ( 1994 ) sup ( β , γ ) ∈ 2 × 0 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , γ ) − E M [ m ( Z , β , γ ) ] (cid:13)(cid:13)(cid:13)(cid:13) p → 0 , ( A . 6 ) and E [ m ( Z , β , γ ) ] is continuous at each ( β , γ ) ∈ 2 × 0 . By the Triangle inequality , sup β ∈ 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , b γ ) − E [ m ( Z , β , γ 0 ) ] (cid:13)(cid:13)(cid:13)(cid:13) ≤ sup β ∈ 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 n X n i = 1 m ( Z i , β , b γ ) − E [ m ( Z , β , b γ ) ] (cid:13)(cid:13)(cid:13)(cid:13) + sup β ∈ 2 k E [ m ( Z , β , b γ ) ] − E [ m ( Z , β , γ 0 ) ] k . ( A . 7 ) Equation ( A . 6 ) implies that the ﬁrst term of the R . H . S . of equation ( A . 7 ) is o p ( 1 ) . Continuity of E [ m ( Z , β , γ ) ] and consistency of b γ give pointwise convergence for the second term of the R . H . S . of equation ( A . 7 ) , uniform convergence holds by 2 being compact . These conditions guarantee b β p → β 0 . It can be shown that k m ( Z , β 1 , γ 1 ) − m ( Z , β 2 , γ 2 ) k ≤ k X k k 2 · k β 1 − β 2 k + ( C · | Y | · k X k k + k X k k 2 k β 2 k ) · k π ( X 0 γ 1 ) − π ( X 0 γ 2 ) k , for some C > 0 and β 1 , β 2 ∈ 2 . Using Theorem 9 . 19 in Rudin ( 1976 ) , it is easy to show that for γ 1 , γ 2 in some open ball containing γ 0 , and X ∈ S , k π ( X 0 γ 1 ) − π ( X 0 γ 2 ) k ≤ M k γ 1 − γ 2 k , for some M > 0 . This Lipschitz property , along with the existence of ﬁnite second moments for Y and X k , implies that the class of functions { m ( Z , β , γ ) : k β − β 0 k < c , k γ − γ 0 k < c } is Donsker for some c > 0 ( see , e . g . van der Vaart ( 1998 , p . 271 ) ) . In addition , E M k m ( Z , β , γ ) − m ( Z , β 0 , γ 0 ) k 2 → 0 as ( β , γ ) → ( β 0 , γ 0 ) . Existence of ﬁnite second moments for k X k k also implies that ∂ E M [ m ( Z , β , γ ) ] / ∂β = − E [ X k π ( X 0 γ ) X 0 k ] which is non - singular in a neighbourhood of γ 0 by continuity . Notice that M γ 0 is the derivative of E M [ m ( Z , β 0 , γ ) ] at γ 0 . Applying the delta method : n 1 / 2 E M [ m ( Z , β 0 , b γ ) ] = n 1 / 2 M γ 0 ( b γ − γ 0 ) + o p ( 1 ) . Now , since b γ is asymptotically linear , apply Theorem 5 . 31 in van der Vaart ( 1998 ) to get n 1 / 2 ( b β − β 0 ) = Q − 1 1 n 1 / 2 X n i = 1 m ( Z i , β 0 , γ 0 ) + M γ 0 ψ γ 0 ( Z i ) + o p ( 1 ) . Then , existence of second moments of ψ implies the result of the theorem . k Proof of Theorem 4 . 4 . Like in the proof of Theorem 4 . 2 , notice that X ki b π ( X i ) ( b ϕ i Y i − X 0 ki b β ) − X ki π 0 ( X i ) ( ϕ 0 i Y i − X 0 ki β 0 ) = X ki (cid:18) T i − λ λ ( 1 − λ ) · D i − 1 ( 1 − b π ( X i ) ) ( 1 − π 0 ( X i ) ) (cid:19) Y i ( b π ( X i ) − π 0 ( X i ) ) − X ki X 0 ki ( b β − β 0 ) . By the Lipschitz property shown above for π ( · ) , and E M Y 2 < ∞ , we obtain 1 n X n i = 1 k m ( Z i , b β , b γ ) − m ( Z i , β 0 , γ 0 ) k 2 = o p ( 1 ) . ( A . 8 ) 18 REVIEW OF ECONOMIC STUDIES Continuity of ˙ π and Lemma 4 . 3 in Newey and McFadden ( 1994 ) imply that k b M b γ − M γ 0 k = o p ( 1 ) . Using similar arguments , it can be easily seen that boundedness of ¨ π implies that n − 1 P ni = 1 k b ψ b γ ( Z i ) − ψ γ 0 k 2 = o p ( 1 ) . Apply the Triangle and H¨older’s inequalities to obtain the desired result . k Acknowledgements . I thank Joshua Angrist , Jinyong Hahn , Guido Imbens , Jos´e Machado , seminar participants at Harvard / MIT , ITAM , the 2000 American Statistical Association Meetings , the 2003 Econometric Society North American Winter Meetings , and the 2003 Bank of Portugal Conference on Labour Market Reform , the referees , and two editors ( Orazio Attanasio and Bernard Salani´e ) for helpful comments . REFERENCES ABADIE , A . ( 2003 ) , “Semiparametric Instrumental Variable Estimation of Treatment Response Models” , Journal of Econometrics , 113 , 231 – 263 . ACEMOGLU , D . and ANGRIST , J . D . ( 2001 ) , “Consequences of Employment Protection ? The Case of the Americans with Disabilities Act” , Journal of Political Economy , 109 , 915 – 957 . ANGRIST , J . D . and KRUEGER , A . B . ( 1999 ) , “Empirical Strategies in Labor Economics” , in O . Ashenfelter and D . Card ( eds . ) Handbook of Labor Economics , Vol . 3A ( Amsterdam : Elsevier Science ) 1277 – 1366 . ASHENFELTER , O . ( 1978 ) , “Estimating the Effect of Training Programs on Earnings” , Review of Economics and Statistics , 60 , 47 – 57 . ASHENFELTER , O . and CARD , D . ( 1985 ) , “Using the Longitudinal Structure of Earnings to Estimate the Effects of Training Programs” , Review of Economics and Statistics , 67 , 648 – 660 . ATHEY , S . and IMBENS , G . ( 2002 ) , “Identiﬁcation and Inference in Nonlinear Difference - in - Differences Models” ( Mimeo , Stanford University ) . BESLEY , T . and CASE , A . ( 1994 ) , “Unnatural Experiments ? Estimating the Incidence of Endogenous Policies” ( Working Paper No . 4956 , National Bureau of Economic Research ) . BLUNDELL , R . , COSTA DIAS , M . , MEGHIR , C . and VAN REENEN , J . ( 2001 ) , “Evaluating the Employment Impact of a Mandatory Job Search Assistance Program” ( Mimeo , UCL ) . BLUNDELL , R . and M A CURDY , T . ( 1999 ) , “Labor Supply : A Review of Alternative Approaches” , in O . Ashenfelter and D . Card ( eds . ) Handbook of Labor Economics , Vol . 3A ( Amsterdam : Elsevier Science ) 1559 – 1695 . CARD , D . ( 1990 ) , “The Impact of the Mariel Boatlift on the Miami Labour Market” , Industrial and Labor Relations Review , 44 , 245 – 257 . CARD , D . ( 1992 ) , “Do Minimum Wages Reduce Employment ? A Case Study of California , 1987 – 1989” , Industrial and Labor Relations Review , 46 , 38 – 54 . CARD , D . and KRUEGER , A . B . ( 1994 ) , “Minimum Wages and Employment : A Case Study of the Fast - Food Industry in New Jersey and Pennsylvania” , American Economic Review , 84 , 772 – 793 . CORAK , M . ( 2001 ) , “Death and Divorce : The Long Term Consequences of Parental Loss on Adolescents” , Journal of Labor Economics , 19 , 682 – 715 . EISSA , N . and LIEBMAN , J . B . ( 1996 ) , “Labor Supply Response to the Earned Income Tax Credit” , Quarterly Journal of Economics , 111 , 605 – 637 . FINKELSTEIN , A . ( 2002 ) , “The Effect of Tax Subsidies to Employer - Provided Supplementary Health Insurance : Evidence from Canada” , Journal of Public Economics , 84 , 305 – 339 . GARVEY , G . T . and HANKA , G . ( 1999 ) , “Capital Structure and Corporate Control : The Effect of Antitakeover Statutes on Firm Leverage” , Journal of Finance , 54 , 519 – 546 . HECKMAN , J . J . ( 1990 ) , “Varieties of Selection Bias” , American Economic Review , 80 , 313 – 318 . HECKMAN , J . J . , ICHIMURA , H . , SMITH , J . and TODD , P . E . ( 1998 ) , “Characterizing Selection Bias using Experimental Data” , Econometrica , 66 , 1017 – 1098 . HECKMAN , J . J . , ICHIMURA , H . and TODD , P . E . ( 1997 ) , “Matching as an Econometric Evaluation Estimator : Evidence from Evaluating a Job Training Programme” , Review of Economic Studies , 64 , 605 – 654 . HERN ´AN , M . A . , BRUMBACK , B . and ROBINS , J . M . ( 2001 ) , “Marginal Structural Models to Estimate the Joint Causal Effect of Nonrandomized Treatments” , Journal of the American Statistical Association , 96 , 440 – 448 . HORVITZ , D . G . and THOMPSON , D . J . ( 1952 ) , “A Generalization of Sampling without Replacement from a Finite Universe” , Journal of the American Statistical Association , 47 , 663 – 685 . ICHIMURA , H . and LINTON , O . ( 2002 ) , “Asymptotic Expansions for some Semiparametric Program Evaluation Estimators” ( Mimeo , London School of Economics ) . IMBENS , G . , LIEBMAN , J . B . and EISSA , N . ( 1997 ) , “The Econometrics of Difference in Differences” ( Mimeo , Harvard University ) . IMBENS , G . W . , HIRANO , K . and RIDDER , G . ( 2003 ) , “Efﬁcient Estimation of Average Treatment Effects Using the Estimated Propensity Score” , Econometrica , 71 ( 4 ) , 1161 – 1189 . MEYER , B . D . ( 1995 ) , “Natural and Quasi - Experiments in Economics” , Journal of Business & Economic Statistics , 13 , 151 – 161 . MEYER , B . D . , VISCUSI , W . K . and DURBIN , D . L . ( 1995 ) , “Worker’s Compensation and Injury Duration : Evidence from a Natural Experiment” , American Economic Review , 85 , 322 – 340 . NEWEY , W . K . ( 1994 ) , “The Asymptotic Variance of Semiparametric Estimators” , Econometrica , 62 , 1349 – 1382 . NEWEY , W . K . ( 1997 ) , “Convergence Rates and Asymptotic Normality for Series Estimators” , Journal of Econometrics , 79 , 147 – 168 . ABADIE SEMIPARAMETRIC DIFFERENCE - IN - DIFFERENCES 19 NEWEY , W . K . and M C FADDEN , D . ( 1994 ) , “Large Sample Estimation and Hypothesis Testing” , in R . F . Engle and D . McFadden ( eds . ) Handbook of Econometrics , Vol . 4 ( Amsterdam : Elsevier Science ) . ROEHRIG , C . S . ( 1988 ) , “Conditions for Identiﬁcation in Nonparametric and Parametric Models” , Econometrica , 56 , 433 – 447 . ROSENBAUM , P . and RUBIN , D . ( 1983 ) , “The Central Role of the Propensity Score in Observational Studies for Causal Effects” , Biometrika , 70 , 41 – 55 . RUBIN , D . B . ( 1974 ) , “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies” , Journal of Educational Psychology , 66 , 688 – 701 . RUBIN , D . B . ( 1977 ) , “Assignment to Treatment of the Basis of a Covariate” , Journal of Educational Statistics , 2 , 1 – 26 . RUDIN , W . ( 1976 ) Principles of Mathematical Analysis ( New York : McGraw - Hill ) . VAN DER VAART , A . W . ( 1998 ) Asymptotic Statistics ( Cambridge : Cambridge University Press ) . WHITE , H . ( 1981 ) , “Consequences and Detection of Misspeciﬁed Nonlinear Regression Models” , Journal of the American Statistical Association , 76 , 419 – 433 . WOOLDRIDGE , J . M . ( 2001 ) , “Estimating Average Partial Effects under Conditional Moment Independence Restrictions” ( Mimeo , Michigan State University ) . WOOLDRIDGE , J . M . ( 2002 ) Econometric Analysis of Cross Section and Panel Data ( Cambridge : MIT Press ) .