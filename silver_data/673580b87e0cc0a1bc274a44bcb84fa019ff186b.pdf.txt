UC Berkeley Connected Communities Title Structuring , Aggregating , and Evaluating Crowdsourced Design Critique Permalink https : / / escholarship . org / uc / item / 7ck2f48g Authors Luther , Kurt Tolentino , Jari - Lee Wu , Wei et al . Publication Date 2015 - 03 - 01 Peer reviewed eScholarship . org Powered by the California Digital Library University of California Structuring , Aggregating , and Evaluating Crowdsourced Design Critique Kurt Luther 1 , Jari - Lee Tolentino 2 , Wei Wu 3 , Amy Pavel 3 , Brian P . Bailey 4 , Maneesh Agrawala 3 , Bj¨orn Hartmann 3 , Steven P . Dow 1 1 Carnegie Mellon 2 University of 3 University of 4 University of Illinois University California , Irvine California , Berkeley at Urbana – Champaign { kluther , spdow } jltolent @ gmail . com { amypavel , maneesh , bjoern } bpbailey @ illinois . edu @ cs . cmu . edu @ eecs . berkeley . edu ABSTRACT Feedback is an important component of the design process , but gaining access to high - quality critique outside a class - room or ﬁrm is challenging . We present CrowdCrit , a web - based system that allows designers to receive design critiques from non - expert crowd workers . We evaluated CrowdCrit in three studies focusing on the designer’s experience and bene - ﬁts of the critiques . In the ﬁrst study , we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique . In a second study , we found that designers who got crowd feedback perceived that it improved their design process . The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs . We conclude with implications for the design of crowd feedback services . Author Keywords Design ; critique ; feedback ; social computing ; crowdsourcing ACM Classiﬁcation Keywords H . 5 . 3 Group and Organizational Interfaces : Computer sup - ported cooperative work , web - based interaction INTRODUCTION For centuries , critique has provided a foundational exercise for art and design , and also more recently , for project - based education in computing and engineering [ 23 ] . The traditional studio critique is a co - located communication activity where someone presents preliminary work and then critics—often teachers and peers—provide feedback to improve the design [ 5 , 2 ] . Critiques can help novices to understand key prin - ciples in a domain [ 12 ] , to compare alternatives [ 7 , 27 ] , to articulate the goals and assumptions behind their work , and to recognize how others perceive their work [ 15 ] . The cri - tique providers also learn by developing domain - speciﬁc vo - cabulary [ 4 ] and rehearsing the mechanics of the crit process . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Request permissions from permissions @ acm . org . CSCW 2015 , March 14 – 18 , 2015 , Vancouver , BC , Canada . Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 2922 - 4 / 15 / 03 . . . $ 15 . 00 http : / / dx . doi . org / 10 . 1145 / 2675133 . 2675283 Figure 1 . CrowdCrit allows designers to submit preliminary designs to be critiqued by crowds and clients . The system then aggregates and visualizes the critiques for designers . Critique leads to knowledge sharing and helps inculcate im - portant values and aesthetics . Feedback can be valuable to a wide audience , from pro - fessional designers to design students to everyday design - ers working on slide decks and ﬂyers . Unfortunately , high - quality critique can be difﬁcult to obtain outside of a de - sign ﬁrm or classroom . There are numerous online design communities where people seek feedback ( e . g . Forrst [ 14 ] , Photosig [ 32 ] ) , but these often yield sparse , superﬁcial com - ments [ 32 ] . Some ( e . g . Dribbble [ 18 ] ) are invitation only , and most require members to build a reputation and feel comfort - able sharing preliminary work . Novice designers in particular may experience evaluation apprehension and avoid sharing their works - in - progress alongside experts [ 18 ] . Recently , a number of academic and commercial efforts have explored how to leverage paid crowdsourcing platforms like Amazon Mechanical Turk ( MTurk ) within a design process . These platforms are attractive because they provide fast , scal - able access to human intelligence at a reasonable cost , but most crowd workers lack design knowledge and cannot pro - vide useful critiques . Most of these efforts , therefore , focus on using crowds to provide high - level impressions [ 11 , 13 ] or learn more about a target audience [ 8 ] . Others ( e . g . Voy - ant [ 33 ] ) decompose the feedback process into microtasks but do not focus on the process and language of design critique . In this paper , we propose and evaluate a different approach . We built a web - based system called CrowdCrit ( Figure 1 ) that allows designers to submit preliminary designs , receive critiques from many crowd workers , and explore the aggre - gated results using an interactive visualization . Rather than searching a large crowd for design experts , CrowdCrit pro - vides a structured interface , inspired by scaffolding theory , to help non - expert crowds submit critiques . CrowdCrit inte - grates with MTurk to leverage the speed , scale , and cost ben - eﬁts of paid crowds , but is designed to work with any type of crowd ( classmates , coworkers , user communities , etc . ) . We evaluated CrowdCrit with three studies , focusing on the experience of designers and the characteristics of crowd cri - tiques . The ﬁrst study compared the characteristics of crowd critiques ( N = 14 ) to expert critiques ( N = 3 ) of the same set of poster designs . Crowd and expert critiques had comparable internal consistency , though it was low . The results also show that aggregating crowd critiques trends towards approximat - ing expert critiques , with 10 – 14 crowd workers ﬁnding 40 – 60 % of design issues identiﬁed by experts . The second study ( N = 14 ) used interviews to explore design - ers’ reactions to crowd critiques and examples and patterns of use in the context of a poster design contest . We found that designers of varying experience levels considered crowd critiques valuable and used them to make changes to their designs . We describe how designers explored and interpret crowd critiques , weighing attributes like crowd expertise and critique frequency against their own intuitions . The third study ( N = 18 ) is , to our knowledge , the ﬁrst con - trolled experiment to examine the speciﬁc impact of crowd - generated design feedback against a baseline . In a second contest , half of designers received crowd critiques and half re - ceived generic feedback . The study found that with crowd cri - tiques , designers reported noticing more issues , making more changes , and producing an overall better design . However , third - party ratings of designs found few conditional differ - ences , suggesting that CrowdCrit may lead designers to focus on reﬁnement , rather than broader changes . In the following sections , we review related work on crowd - sourcing and feedback and describe the CrowdCrit system and our evaluations . We conclude with design implications for crowd feedback services . RELATED WORK Theories of learning and assessment To inform our interface design , we draw on learning theory literature that explores how best to teach new concepts to stu - dents . Scaffolding refers to a teaching technique where learn - ers are given signiﬁcant support — by teachers or computer - based tutors , for example — to help them learn new mate - rial [ 25 , 3 ] . Our system , inspired by scaffolding theory , struc - tures the visual design critique process for the crowd by sug - gesting design issues and concepts as a set of pre - authored critique statements . Sadler reviewed different feedback approaches and argued that good feedback must help a student grasp the concept of a standard ( conceptual ) , compare the actual level of per - formance with this standard ( speciﬁc ) , and engage in action that closes this gap ( actionable ) [ 22 ] . Our system structures crowd workers to include all three components in the context of visual design critique . Peer critique vs . external feedback Recently , researchers have explored the viability of online critique within an educational context . Tinapple et al . [ 26 ] developed CritViz , a system which enables peer critiques of designs . Kulkarni and Klemmer [ 16 ] also developed a peer critique system for use in a Massive Open Online Course ( MOOC ) , and Easterday et al . [ 10 ] developed a mixed face - to - face and online critique system for design students in a classroom environment . These systems were shown to be ef - fective in a formal educational setting , with students of fairly uniform levels of knowledge and motivation . Alternatively , Dow et al . [ 8 ] explored the utility of external crowds to con - tribute outside perspectives to add authenticity to a design course . CrowdCrit seeks to leverage the diverse capabilities of crowd workers and focuses their efforts on producing vi - sual design critique . Reputation vs . anonymity Many commercial sites like Dribbble [ 18 ] and Forrst [ 14 ] allow members to upload their own creative projects and provide feedback on each other’s work . However , novices are likely to experience evaluation apprehension and feel in - timidated sharing their work alongside professionals [ 18 ] . CrowdCrit operates in a double - blind fashion , so that design - ers do not feel reluctant to share and critique providers are not inﬂuenced by the designer’s reputation . Payment vs . reciprocity Systems that follow the reciprocity model do not require a ﬁ - nancial transaction , but they often fail to produce enough use - ful feedback in a timely manner . Xu and Bailey [ 32 ] studied critique behaviors in the online photography community Pho - toSIG , where members voluntarily critique each other’s pho - tos and hope for reciprocity . They found that 80 % of photos received less than four critiques , which most users found in - sufﬁcient . The paid crowdsourcing paradigm used by Crowd - Crit provides a novel opportunity to deliver large quantities of feedback quickly and at a reasonable cost . Impressions vs . structured feedback Sites like Five Second Test [ 13 ] and Feedback Army [ 11 ] al - low users to pay crowds to provide feedback on their designs or websites , typically in the form of overall impressions or general reactions . This kind of feedback is valuable for get - ting an audience’s perspective on a design , but it does not il - luminate why a design works well , or suggest what to change . Even in online communities for professional designers , shal - low reactions , rather than detailed critique , are the norm [ 32 ] . Xu et al . [ 33 ] created the Voyant system to obtain crowd feed - back on visual designs by creating structured micro - tasks . Most of the feedback types supported by Voyant centered around audience reactions ( e . g . which elements are noticed ﬁrst , general impressions of the design ) . The system also paid the crowd to rate , on a Likert scale , how well the design meets the guidelines of alignment , contrast , proximity , and repeti - tion . CrowdCrit builds on these ideas by structuring not only the crowd’s ability to recognize a wide range of design issues , but also the process and language of constructing a critique . Aggregating crowd feedback Most commercial and academic critique software is not de - signed to scale to large quantities of critique . CritViz [ 26 ] supported peer critique in a college course of 75 students , but each assignment received only ﬁve critiques . Voyant [ 33 ] supports large quantities of crowd feedback , using visual - izations such as word clouds and histograms . Our research builds on this idea by using a stacked bar chart visualiza - tion to aggregate rich critique data , including valence ( pos - itive / negative ) , text comments and graphical annotations , and expertise across seven design principles . General solutions for aggregating crowd - generated content are also germane . For example , CommentSpace [ 30 ] pro - vided interactive visualizations and structure to help crowds perform social data analysis . We implemented several of these papers’ recommendations , such as linked discussions and structured comments to support deeper analysis . Our sys - tem differs in that it focuses on the unique requirements of design critique and operates as a service for designers , pro - viding distinct interfaces for critique collection ( from crowds ) and critique aggregation ( for designers ) . PRELIMINARY STUDIES To better understand the challenges of crowd - generated cri - tique and the appropriate type of structure , we conducted a series of preliminary studies on MTurk ( Figure 2 ) to see what type of feedback crowds provided , given minimal training or structure . In our ﬁrst pilot , we paid three crowd workers to write open - ended feedback on a set of eight designs . A quick analysis showed that only ﬁve of 65 responses fulﬁlled Sadler’s [ 22 ] requirements for high - quality feedback . Next , we added some structure to the task , asking three different workers to provide feedback on four designs using two text boxes : one for describing the problem and one for explaining why it was a problem . However , these workers still failed to identify and justify signiﬁcant issues with the designs . In our third pilot , we embedded design knowledge into the structure , asking three new workers to critique nine designs by consid - ering a list of pre - authored critique statements and checking the box next to ones that applied to the given design . This approach showed promise ; the three workers agreed on 65 % and 52 % of the issues in the two checklists we provided . Overall , these pilots suggested that eliciting open - ended cri - tique without appropriate task structuring led to predomi - nantly low - quality responses that lacked conceptual ground - ing and action - oriented advice . Introducing structured check - lists was more successful and suggested that given this type of domain - speciﬁc structure , crowd workers might identify legitimate design problems and generate reasonably useful feedback . These early ﬁndings , along with the prior work re - viewed above , informed the design of our CrowdCrit system , described in the next section . Figure 2 . Crowdsourcing interfaces used for the ﬁrst three pilot stud - ies that indicated some drawbacks of open - ended designs feedback and beneﬁts of additional structure . THE CROWDCRIT SYSTEM CrowdCrit is a web - based system built in Python and JavaScript that allows a user to request , receive , and review design critiques generated by an online crowd . To use the system , a designer uploads an image and provide some con - textual information , such as a title , brief description , and in - tended audience . Next , crowd workers provide feedback on the design using a scaffolded critique interface . Finally , the designer can explore the crowd critiques using an aggregation interface which includes an interactive visualization . This process can be repeated whenever the designer seeks a new round of feedback . Eliciting critique CrowdCrit is designed to elicit valuable critiques from any online crowd , even those with limited design experience . Our preliminary studies and prior work suggested a struc - tured interface could help novice crowds adopt the process and language of visual design critique . This required solving two problems : ﬁrst , generating structured design knowledge based on best practices ; and second , embedding this material in an interface suitable for non - expert crowd workers . Critique statements There is no universally agreed - upon reference for visual de - sign critique , but our survey of widely cited design textbooks and other resources revealed signiﬁcant overlap in terminol - ogy , concepts , and best practices . Therefore , two authors Figure 4 . The Overview ( left ) and Design Principle view ( right ) for the CrowdCrit aggregation interface . Figure 3 . The CrowdCrit critique interface . with design experience performed a bottom - up analysis of key sources [ 21 , 31 , 17 , 6 ] to generate material for structur - ing our critique interface . They began by reviewing each text for lists of best practices or principles and extracting these along with deﬁnitions and examples . Next , they grouped related concepts together based on overlapping terminology , similar examples , their own knowledge and experience , and direct references in the text . After several iterations , they produced a list of seven key design principles : Readability , Layout , Balance , Simplicity , Emphasis , Consistency , and Ap - propriateness . The principles are distinct , but not mutually exclusive , as there are often multiple effective ways to iden - tify issues with a design . This general approach is inspired by similar efforts in the HCI community to produce other types of design principles ( e . g . [ 20 , 28 , 29 ] ) . Each principle is composed of a set of pre - authored “critique statements” about more speciﬁc issues . CrowdCrit follows from traditional critique [ 12 ] and provides both positive and negative critique statements . For example , critique providers can praise the appropriateness of the design ( “Reaches in - tended audience” ) or raise concerns about it ( “Wrong audi - ence” ) . A total of 70 critique statements are available across the seven principles . Finally , each critique statement has a short name and a longer , more detailed description . The description identiﬁes the spe - ciﬁc issue , connects it to a broader design principle , and ( in most cases ) offers a generic solution . This format is meant to embody Sadler’s [ 22 ] criteria for good feedback and pro - vide a basic level of utility . For example , within the Sim - plicity principle , there is a critique statement with the name “Too much text” and the description : “The abundance of text makes this difﬁcult for viewers to comprehend . Consider con - densing this text by focusing on the essential message . ” Critique interface Crowd workers begin by completing a survey of self - reported design experience and a design knowledge quiz , used to cate - gorize their expertise as low , moderate , or high . Next , work - ers are presented with the critique interface ( Figure 3 ) . The design’s contextual information ( a ) , including intended audi - ence , along with the design itself ( b ) , appears on 1 side , and the critique tools are shown on the other . The interface uses progressive disclosure [ 24 ] to avoid overwhelming novice cri - tique providers with information about design . There are tabs for each of the seven design principles , plus an “Other” tab for critiques that our list may overlook ( c ) . Crowd workers click a tab to reveal the critique statements for that principle and hide the others ( d ) . Workers can mouseover the short state - ment names to display the longer description as a tooltip . Af - ter selecting a statement , the worker can use annotation tools , such as markers or polygons , to indicate the relevant area of the design ( e ) . Workers can also elaborate on the critique us - ing a text box ( f ) . Finally , the worker saves the critique , and it appears on the comment list ( g ) . The worker can repeat this process for as many critiques as they desire . Aggregating and presenting critique Once workers have critiqued the design , the designer needs to be able to review the feedback in a meaningful way . This poses interesting challenges compared to a traditional studio critique , which unfolds linearly , in real time , from a small number of co - located participants . In contrast , CrowdCrit generates potentially large quantities ( 50 + ) of individual cri - tiques from online , distributed providers in a nonlinear , asyn - chronous format . We needed to design an interface that would handle these requirements and allow the user to identify the most signiﬁcant issues with a design . Aggregation interface The CrowdCrit aggregation is shown in Figure 4 . Like the critique interface , it leverages progressive disclosure to avoid overwhelming the user with details . The design and contex - tual information appear on one side ( a , b ) , and an interac - tive visualization providing an overview of all critiques as a stacked bar chart ( c ) appears on the right ( Figure 4 , left ) . Each bar represents a design principle , and each bar segment indicates a critique statement chosen by workers . Bars are or - dered by decreasing critique count so that the principle with the most critiques appears ﬁrst . Color encodes critique va - lence ; positive critiques are green and negative critiques are purple . We use lightness values to encode crowd expertise ; darker shades indicate critiques from workers with higher ex - pertise . Below the visualization , a “Top Feedback” section lists the most frequently used critique statements ( d ) . The designer can click a bar or segment to drill down on the detailed critiques for that principle ( e ) , ordered by decreas - ing frequency ( Figure 4 , right ) . The interface displays each critiquer’s text comment and expertise ( f ) ; we label workers in the bottom quartile of design expertise as novices , workers in the middle quartiles as competent , and workers in the top quartile as experienced . Hovering over a critique causes any corresponding annotations to appear on the design itself ( g ) . STUDY 1 : COMPARING CROWD VS . EXPERT CRITIQUE Our ﬁrst study focuses on how crowd critiques compare to ex - pert critiques . We consider the following research questions : • How internally consistent are crowd vs . expert critiques ? • How similar are individual and aggregated crowd critiques to expert critiques ? Methods We designed a controlled experiment using three event poster designs gathered online ( Figure 5 ) . Each poster received cri - tiques from a group of 14 crowd workers and a group of three expert designers . The crowd workers came from MTurk and were recruited with the same criteria described above . The expert designers all had degrees in design and had worked full - time as professional designers . Both crowd workers and experts provided critiques independently , using the Crowd - Crit interface . The crowd produced between 38 and 58 critiques for each of the three designs ( approximately four crits per design ) , using between 25 and 33 unique critique statements . Experts gener - ated between 21 and 24 critiques for the designs , using 18 – 20 different statements . Finally , we generated a set of random critiques as another baseline . We generated 14 groups of four random critiques per design , to approximate the quantity of crowd - generated critiques . Design 1 Design 2 Design 3 Figure 5 . Poster designs for Study 1 , used to advertise real events ( col - lected from the web ) . To measure consistency among more than two raters , we used Fleiss’s κ . For each of the 70 possible critique statements of - fered by CrowdCrit , we calculated how many critiquers used that statement , and how many didn’t . To compare crowd and random critiques to experts , we use an approach inspired by Nielsen and Molich [ 20 ] to validate us - ability heuristics . For each design , we produce a “gold stan - dard” list of expert critiques which is the union of the state - ments chosen by each expert independently . We chose the union rather than intersection due to the low levels of agree - ment observed among experts , described below . This yielded 18 , 19 , and 20 “correct” statements for designs 1 , 2 , and 3 , respectively . If crowd or random critiques include the same statements as experts , we can think of this as a “match” or as both groups identifying the same issues and problems . Finally , to understand the impact of false positive , we calcu - late precision , recall , and f - measure ( combined precision and recall ) for crowd and random critiques vs . expert critiques . Results and discussion Internal consistency of crowd vs . expert critiques The crowd’s κ scores ranged from 0 . 04 to 0 . 09 , while experts ranged from - 0 . 05 to 0 . 20 . Random critique κ scores ranged from - 0 . 01 to 0 . 01 . All of these scores are considered poor or slight agreement . Yet , across all designs , crowd workers converged on a subset of statements ( ≤ 33 of the available 70 ) , and on average each statement that was used received two different crits . The highest crowd agreement occurred in design 1 , where six of 14 workers ( 43 % ) chose the statement , “Lacks background contrast . ” Individual crowd workers vs . experts We found the average worker identiﬁed 6 – 8 % of expert cri - tiques , though workers provided only about four crits per de - sign . The best workers produced crits that matched 17 – 25 % of the expert list . Random critiques fare worse , with aver - age matches ranging between 3 – 6 % , and a maximum of 11 % , even with equal or higher numbers of critiques vs . the crowd condition . Aggregate crowd workers vs . experts While individual crowd workers provide relatively few matching critiques , one of the beneﬁts of crowdsourcing plat - forms is that we can easily recruit more workers and generate Figure 6 . Proportion of expert critiques matched by aggregated crowd vs . random critiques . more critiques . However , we ﬁrst need to understand the re - lationship between more workers and better expert matches . For design 1 , the crowd identiﬁed 60 . 3 % of the experts’ selec - tions , compared to 56 . 9 % of random crits . For design 2 , the crowd found 55 . 3 % of the expert issues , compared to 52 . 2 % for random crits . For design 3 , the crowd found 45 % of the issues , compared to 30 % for random crits . These results sug - gest that more workers produce better results ( 8x to 10x more matches ) . Data on > 14 workers is needed to determine if higher matches are possible . Figure 6 shows how match percentage changes with aggrega - tion . For each design , we ran 500 trials randomizing the order in which the crowd critiques were aggregated . Overall , the trend suggests a linear relationship between number of crowd critiquers and proportion of expert issues identiﬁed . An ag - gregate of 10 workers yields 40 - 50 % of the issues , while 14 workers ﬁnd 45 – 60 % of the problems . Precision / recall scores for crowd critiques The above analyses consider how well crowds match the re - sults produced by experts , but they don’t address crowd cri - tiques that don’t match the experts , i . e . , false positives . Figure 7 graphs the precision , recall , and f - measure scores for crowd and random crits across all designs . The crowd achieves bet - ter scores across the board , on average 0 . 12 points higher ( out of 1 . 0 ) for precision and recall . This supports the above evi - dence that crowd critiques match experts better than random , and also suggests that crowds miss fewer expert crits and of - fer fewer false positives . However , not all false positives are bad critiques . Nielsen and Molich [ 20 ] observed few false positives , and in fact revised their expert list post - hoc to include new issues identiﬁed by novices . Similarly , our informal analysis ﬁnds many crowd critiques that point out legitimate , if minor , issues . Overall , compared to random critiques , crowd critiques show higher internal consistency , individual workers identify a greater percentage of expert issues , and aggregate percent - ages are also higher . Additionally , crowd critiques may pro - Figure 7 . Precision , recall , and f - measures for crowd and random cri - tiques vs . expert critiques . vide richer feedback , in the form of optional annotations and text comments , that random critiques lack . The ﬁrst study suggests that crowd critiques could beneﬁt designers , but we know little about how designers might respond to crowd cri - tiques or incorporate them into their actual design process . We address these issues in the following two studies . STUDY 2 : DESIGNER REACTIONS TO CROWD CRITIQUE This exploratory study examines how designers leverage CrowdCrit in the context of a visual design contest . It seeks to answer three key research questions : • How do participants react to the critiques and the aggrega - tion interface ? • How do participants react to the source of crowd critiques ? • How do participants use crowd critiques ? To facilitate rich data collection and comparison across par - ticipants , we organized a poster design contest with real clients ( organizers of a local music festival ) and real prize money ( $ 250 for the best poster ) . Participants used their own laptops and software and were provided with crowd feedback from CrowdCrit . Method We recruited 14 participants ( six female ) ranging in age from 20 to 33 ( µ = 25 ) by posting ﬂyers at a university campus and advertising on neighborhood email lists , Craigslist , and Red - dit . We sought to recruit participants with a wide range of ex - perience , which we later categorized into three groups : novice designers , who had little or no design experience ; competent designers who had taken a design course or had done some freelance design work ; and experienced designers who had earned a degree in design or had worked full - time as a de - signer . To motivate participants while acknowledging their primary role as research participants , we compensated each participant $ 10 / hour ( $ 30 total ) , the standard rate for research participation at our institution , and awarded the $ 250 contest prize to the winning participant . The study consisted of two design sessions separated by a week to provide time to gen - erate feedback from both the crowd and client . Design session 1 In the ﬁrst design session , each participant came to our lab and completed a pre - survey about their design experience . We then explained the goals of the contest and presented a design brief , written in collaboration with the clients , which included information about the event and other requirements , such as using the festival logo . Participants then had one hour to design a draft of their posters . Crowd and client critique After all participants had completed the ﬁrst design session , we used CrowdCrit to gather crowd and client critiques for each design . To gather crowd critiques , we recruited 50 crowd workers on MTurk and paid each the US minimum wage ( $ 7 . 25 / hour , or $ 3 . 50 for this 25 - minute task ) . All workers were US - based to reduce the impact of cultural differences on perceptions of design . After completing a pre - survey and a tutorial video , each worker critiqued three randomly selected posters from the group , completed a brief post - survey about their opinions of the system , and received payment . To gather client critiques , we arranged an hour - long , face - to - face meeting with two of the festival organizers . They dis - cussed each of the 14 initial designs and provided feedback using the same critique interface as the crowd . We also asked them to share their impressions of the critique and aggrega - tion interfaces . Design session 2 After one week , each participant returned to the lab for a sec - ond design session . We demonstrated the aggregation inter - face and asked participants to explore the crowd and client critiques using a think - aloud protocol . Participants then had another hour to iterate on their original version and submit a ﬁnal design , using the aggregation interface and critiques as much or as little as they wanted . We then conducted a semi - structured interview covering design process , thoughts on the crowd and client critiques , and experience with the in - terface . The combination of think - aloud and interview lasted , on average , 27 minutes per participant . Pre - and post - critique versions of all 14 designs can be viewed in Figure 8 . Client judging We set up a second face - to - face meeting with the clients where we showed them each participant’s ﬁrst draft , the client and crowd critiques , and the ﬁnal design . We asked clients to comment on how the designs had changed and how well the participants had responded to their feedback . The clients also voted on a winning poster ( P6 ) . All client and participant interview data was audio - recorded , fully transcribed , and analyzed using a bottom - up approach . In our ﬁrst pass , we annotated quotes which addressed one of our research questions above , and in several subsequent passes , we grouped related quotes together to elicit broader themes , presented in the next section . Results and discussion How do participants react to the critiques and the aggregation interface ? Quality : On the whole , participants found the majority of critiques to be helpful , especially given the crowd’s minimal qualiﬁcations and the lack of moderation . They mentioned speciﬁc reasons such as causing them to notice issues they hadn’t considered ( P5 ) , appreciating the concreteness and rich detail ( P6 ) , and feeling like the critiques were carefully considered ( P8 ) . Some participants particularly appreciated positive critiques ; P6 noted , “I tend to always second guess my work , so hearing afﬁrmation , hearing people say ‘This is a strength , ’ it helps a lot . ” Quantity : Overall , each poster received an average of 69 ( σ = 12 ) critiques , where 26 ( σ = 17 ) critiques were positive and 43 ( σ = 20 ) critiques were negative . Clients contributed 4 % of all critiques . The majority of participants were satisﬁed by the number of critiques they received , with some express - ing pleasant surprise at receiving more than they expected . P7 characterized the general attitude towards quantity , say - ing , “The more feedback , in my opinion , the better , because there’s a chance that you’ll be able to see a larger trend . ” P1 agreed , “It feels like if it’s only two people [ critiquing ] , that’s just their taste . [ With CrowdCrit ] you get a sense of , ‘Overall , people like the alignment , ’ [ or ] ‘Overall , people like this . ”’ Speed : We asked participants how quickly they’d prefer to receive crowd critiques , and most responses ranged between one day and several days for each round of feedback , depend - ing on the project . For this study , the CrowdCrit system gen - erated critiques for all 14 posters in less than two hours , or on average , less than 10 minutes per design . We shared this with participants , and most found the turnaround time to be surprisingly fast . In P1’s words , “If you can do it in an hour , it’s perfect—I think that will really justify the price . ” Cost : The total cost to collect critiques for all 14 designs was $ 175 , or $ 12 . 50 per design on average ; workers were paid an hourly rate equivalent to minimum wage in our location . We asked participants how much they’d be willing to pay for the critiques of their poster drafts . For novices , who typically designed for themselves , the most common response was a ﬂat rate of around $ 5 – 10 per round of feedback . Given the abundance of feedback provided in this study , we speculate that a service like CrowdCrit can provide value at the $ 5 – 10 price point . Interface Participants generally found the aggregation inter - face easy to use . They described it using phrases such as “easy to understand” ( P1 ) , “simple and clean” ( P5 ) , “clear - cut” ( P6 ) , and “intuitive” ( P7 ) . P12 praised the “straightfor - ward” design , saying , “the brilliance of this interface is the fact that it’s really simple . ” One of the most popular features was the ability to hover over individual critique text to reveal the corresponding graphical annotations ( e . g . markers or selected regions ) on the design itself . P6 liked that annotations added speciﬁcity to ambigu - ous comments ; without them , critique statements like “Lacks balance” would seem , in her words , “fake . ” How do participants react to the source of crowd critiques ? Crowd expertise : When exposed to critiques from a vari - ety of sources , participants usually considered client critiques ﬁrst and crowd critiques second , often commenting that client satisfaction trumped everything else . Among crowd critiques , participants paid closer attention to expert critiques because they perceived them to be higher quality . Responses to novice feedback were more varied . Some experienced designers Figure 8 . All drafts ( left ) and ﬁnal designs ( right ) from the Study 2 . P6 won the contest . tended to disregard novice feedback because the experienced designers felt more knowledgeable . P6 said , “Not to sound pompous or anything , but I feel like I do know a little bit more than general people who don’t really know much about de - sign . ” Other participants felt that novice critiques were valu - able in their own distinct way . P3 felt that novices provided “good emotional feedback” while experts offered “a higher - level technical critique . ” P1 saw novices as representative of the target audience of her design : “ [ Novices ] are the cus - tomer , too—they are the ones that are going to the show . ” Crowd vs . client critiques : Participants found both client and crowd feedback to be valuable , albeit in different ways . The crowd , as mentioned above , provided both designerly critique and audience reactions . The clients , on the other hand , were sources of domain - speciﬁc knowledge , provid - ing input on the music festival and the goals for the poster . For our design contest , participants needed to consider both types of feedback to succeed . P5 noted that clients provided feedback speciﬁc to the festival ( e . g . add a photo of the band Royal Teeth ) , while the crowd provided “more generic” feedback about effective design ( e . g . “the font size should change” ) . The clients also saw their role as complementary to the crowd’s . Their “general feedback” supplemented the crowd’s “high - end visual feedback” ( Client ) . Some participants viewed the clients’ lack of design expertise as problematic , and crowd critique as an exciting possible so - lution . They imagined “hav [ ing ] a crowd behind you , backing you up” ( P1 ) , i . e . , using the crowd’s feedback as empirical evidence to persuade clients to abandon a poor idea ( P1 ) or embrace one favored by the designer ( P3 ) . Crowd vs . other critique sources : Participants also com - pared the crowd critiques they received to other feedback sources . Participants who worked as freelancers ( P1 ) or as the lone designer within an organization , such as a univer - sity department ( P6 , P8 ) , struggled to ﬁnd reliable sources of meaningful feedback beyond their clients , and these clients frequently lacked design backgrounds . Some isolated partic - ipants turned to online feedback sources , like Dribbble [ 18 ] and design forums , but were generally unsatisﬁed with the results . P6 praised the crowd critique for being “more spe - ciﬁc , like it addressed very , very speciﬁc issues better than just vague ‘I like it’ or ‘I don’t like it”’ comments typical of design websites . She added that the “guiding points” and “categories” of the crowd critique system seemed effective for preventing the misunderstandings and mean - spirited re - marks which deterred her from these other sites . Privacy and identity : Participants generally expressed little concern over sharing their works - in - progress with the crowd for the purposes of critique . Several mentioned that any ap - prehensions they might have were eased because the system was double - blind ; designers were anonymous to critiquers and vice - versa . P1 said , “I like anonymous . . . They don’t see my name , I don’t see theirs . That’s good . ” She added that she was grateful that the system did not support direct interaction between designers and crowds , fearing that direct communication could lead to arguments and defensiveness . How do participants use crowd critiques ? Most participants used the feedback to make changes to their design . For example , P1 increased the legibility of the top of the design and removed a distracting image ; P5 changed the placement of a photo and made the background less “ﬂashy” ; and P9 changed an inappropriate typeface and added color to create visual interest . Participants also used the feedback to decide what not to change . For example , P6 saw that the clients “liked how clean and simple it was so instead of changing my design to make it more ﬂowery or whatever , I just decided to mostly keep it the way it was . ” Participants also mentioned situations where they agreed with feedback , but chose not to act on it . For ex - ample , P13 agreed with a crowd worker’s suggestion that cre - ating an illustration would help tie together two images , but he felt he lacked the drawing skill to execute it . The clients gave P8 negative feedback on the “stars” motif he had cho - sen , but he kept it because he felt he was too far along with the original concept . Summary This study offered evidence that crowd feedback provides value to designers across a range of backgrounds and skill levels . It demonstrated how a crowdsourcing platform could be leveraged to provide design critique for a contest sce - nario . Participants were generally enthusiastic about the qual - ity , quantity , cost , and speed of crowd critiques , and found the aggregation tool straightforward and helpful . Participants also described their reactions to this novel source of critique . They compared crowds favorably to more familiar feedback sources and identiﬁed unique beneﬁts of crowds , such as pro - viding a broader range of perspectives , preserving limited so - cial capital , and protecting privacy and anonymity . Finally , the study revealed how participants used crowd critiques in their design processes , weighed expertise against their own intuitions , and even used crowds to justify design decisions . STUDY 3 : IMPACT OF CROWD CRITIQUE ON DESIGN PROCESS AND RESULTS The second study provides qualitative evidence for the efﬁ - cacy of gathering crowd critique for a realistic design sce - nario . However , it did not directly compare crowd critique to the feedback typically available on a design contest platform . Therefore , we conducted a controlled between - subjects ex - periment to isolate these effects . Speciﬁcally , we investigated the following research questions : • How do designers perceive crowd critique in comparison to the generic feedback typical on contest platforms ? • How does crowd critique affect designers’ ﬁnal products in comparison to generic feedback ? Method Site We ran our experiment on 99designs [ 1 ] , a popular host of online design contests . The site has hosted nearly 300 , 000 contests since it was founded in 2008 . Participants Eighteen designers ( ﬁve female , ages 19 – 55 ) participated in the experiment . All but three reported their occupations as designers . They had won , on average , 5 . 6 contests on 99de - signs ( min : 0 ; max : 33 ) . All 18 designers were paid $ 10 for participating , and the winner ( D18 ) received an additional $ 599 in prize money . Experimental conditions We randomly assigned half ( nine ) of the designers to the ex - perimental condition , where they received crowd critiques via URL . The remaining nine designers , assigned to the control condition , received the following “generic” feedback : “We checked out your design and wanted to tell you that you’re on the right track . Keep up the good work ! ” This type of feed - back is typical of contest sites like 99designs , where many hosts provide little or no feedback to designers . Procedure We initiated a 99designs contest to design a poster advertising an upcoming lecture series hosted by a U . S . university . We worked with the lecture series coordinator to develop a design brief , which contained information about the lecture series and target audience , as well as more speciﬁc requirements like size and sponsor logos . The contest lasted one week from start to ﬁnish . Participants ( hereafter “designers” for sake of clarity ) completed a demo - graphics pre - survey and submitted a draft of their poster de - sign by the end of Day 3 of the contest . On Day 4 , we gen - erated crowd critiques for all 18 designs from a total of 30 crowd workers on MTurk , and distributed these crowd cri - tiques to designers in the experimental condition . Design - ers in the control condition were sent generic feedback . All designers were asked to consider their feedback , iterate , and submit a ﬁnal poster design on Day 7 . Pre - and post - critique versions of all 18 posters , totalling 36 unique designs , are shown in Figure 9 . Following the contest deadline , all designers completed a post - survey about their impressions of the contest and feed - back . Designers who had received only the generic feedback were given the URL for the crowd critiques , which we gen - erated on Day 4 but did not reveal until this time ( Day 8 ) . These designers completed a second post - survey about their reactions to the crowd critiques . The ﬁnal posters were evaluated by both the client ( the lec - ture series coordinator ) and crowd workers from MTurk . We sought evaluations from people with design knowledge , so we required crowd workers to score 80 % or higher on a true / false design knowledge quiz that was previously vali - dated [ 9 ] . Thirty - seven crowd workers passed the quiz and participated in the evaluation . D1 D2 D Received crowd critique between versions D Received generic feedback between versions D3 D4 D5 D7 D8 D9 D10 D11 D12 D13 D17 D18 D14 D15 D16 D6 Figure 9 . All drafts ( left ) and ﬁnal designs ( right ) from Study 3 . D18 won the contest . Figure 10 . The rating interface used by crowd workers and the client to evaluate the 18 poster designs along 10 dimensions . Raters viewed both poster versions from all 18 participants ( 36 designs total ) , unlabeled and in random order . They com - pared two versions across 10 dimensions : adherence to the seven design principles , originality , similarity , and overall quality ( see Figure 10 ) . Raters moved a slider left or right , more towards one design or the other , to indicate which de - sign embodied each dimension more effectively . Analysis To analyze the non - parametric survey data , we ran indepen - dent samples Mann - Whitney U - tests . We also analyzed de - signers’ responses to open - ended questions about the feed - back and share illustrative quotes below . To analyze the posters , we ﬁrst calculated intraclass corre - lation ( ICC ) scores to establish the reliability of the crowd’s ratings ( see Table 1 . Eight of the 10 dimensions scored 0 . 75 or higher ( considered “excellent” on a scale of 0 – 1 ) , and the remaining two dimensions , Balance and Appropriateness , scored 0 . 60 or higher ( considered “good” ) . Thus , there was high agreement among crowd raters . For each designer , we computed the average change in score between his or her initial and ﬁnal designs ( delta ) , as rated by the crowd workers on a 0 – 10 Likert scale . The maximum delta was ± 5 points , except for Similarity , which was a 0 – 10 scale . Positive deltas indicate the post - feedback design improved along that dimension . We ran independent samples t - tests comparing the crowd feedback deltas across all 10 dimensions to the generic feed - back deltas . these deltas across conditions . We also examine poster ratings by design experience . For the latter analysis , we divided the participant pool into thirds according to the number of 99designs contests they had won . Figure 11 . Post - survey results . Only the ﬁrst three questions ( marked with * ) have statistically signiﬁcant different responses between condi - tions ( p < 0 . 05 ) . Results and discussion Survey responses are shown in Figure 11 , and poster evalua - tion results are shown in Table 1 . In general , designers rated the feedback from CrowdCrit higher than the generic feed - back . Designs improved along most dimensions between the initial and ﬁnal iterations , but there was no signiﬁcant differ - ence between conditions . Designers receiving crowd crits felt they noticed more issues Designers who got crowd critiques reported noticing signif - icantly more issues they would have ordinarily missed ( 6 . 13 vs . 3 . 67 , p < 0 . 05 ) . Some attributed this to the higher quantity and variety of critiques available on CrowdCrit . D10 wrote , “I didn’t realise that in a design there are so many points to look for . . . now I know what to look for in my future designs . ” D2 wrote , “The feedback about lacking contrast , imagery and the bottom text being too small were right on . ( I can’t believe I didn’t see it myself ) . ” Designers receiving crowd crits felt they made made better designs , but third - party ratings show little difference Compared to generic feedback , designers felt that crowd cri - tiques helped them make a signiﬁcantly better overall design ( 6 . 25 vs . 3 . 67 , p < 0 . 05 ) . Many provided speciﬁc examples of crowd feedback that led them to make improvements . D11 described how , after reviewing critiques in the Balance tab , he “noticed that the bottom part of my entry lacks something to balance the whole thing . . . so I added edited photos of the uni - versity to ﬁll in the gap . ” D15 described “changing the layout of the dates to make it less confusing” , and D13 “ [ c ] hanged a lot of things that weren’t making sense at all like using dark colour fonts under the dark background . ” However , despite designers’ greater sense of improvement in the crowd feedback condition , the third - party ratings show no signiﬁcant differences between conditions for nine of the 10 dimensions . Only ﬁnal designs receiving generic feedback scored signiﬁcantly higher on Simplicity than designs getting crowd feedback ( + 0 . 63 vs . + 0 . 09 , p < 0 . 05 ) . Designs tended to improve modestly ( ≤ 1 point ) across all dimensions follow - ing either type of feedback . Dimension ICC score Crowd avg . ∆ Generic avg . ∆ * Simplicity 0 . 94 ‡ + 0 . 09 + 0 . 63 Balance 0 . 71 † + 0 . 44 + 0 . 67 Consistency 0 . 84 ‡ + 0 . 25 + 0 . 40 Appropriateness 0 . 64 † + 0 . 20 + 0 . 26 Readability 0 . 88 ‡ + 0 . 74 + 0 . 81 Layout 0 . 86 ‡ + 0 . 65 + 0 . 69 Emphasis 0 . 88 ‡ + 0 . 54 + 0 . 52 Originality 0 . 95 ‡ + 0 . 20 + 0 . 12 Similarity 0 . 99 ‡ 6 . 51 6 . 19 Quality 0 . 84 ‡ + 0 . 69 + 1 . 00 Table 1 . Average change ( ∆ ) between pre - critique and post - critique de - signs for the 10 dimensions evaluated . Only Simplicity ( marked with * ) was signiﬁcantly different between conditions ( p < 0 . 05 ) . ICC scores with ‡ have “excellent” agreement ; scores with † have “good” agreement . Designers receiving crowd crits felt they made bigger changes , but ratings show only inexperienced designers did The third signiﬁcant survey result helps to explain the dis - crepancy between designers’ perceptions and the poster rat - ings . Compared to generic feedback , designers reported mak - ing signiﬁcantly more changes to their designs as a result of crowd critiques ( 5 . 25 vs . 2 . 89 , p < 0 . 05 ) . Some mentioned that the higher speciﬁcity and concreteness of crowd critiques made it easier to recognize problems and make changes . D1 appreciated that crowd feedback “showed where the change was needed . . . in a graphical manner . ” D4 wrote , “ [ Crowd ] feedback was precise , detailed and to the point . Even though it didn’t favor my design it was better than someone saying they like it , or nice and not giving any more details . ” The poster ratings partially support these claims . As men - tioned above , there was no signiﬁcant difference in Similar - ity when designers of all experience levels are analyzed to - gether . Separating the analysis by designers’ contest expe - rience , however , tells a different story . High - experience de - signers who received crowd critique produced more similar ﬁnal designs ( 8 . 47 vs . 8 . 01 , p < 0 . 05 ) , while low - experience designers getting crowd critique produced less similar ﬁnal designs ( 4 . 81 vs . 5 . 44 , p < 0 . 05 ) . As these means suggest , inexperienced designers made bigger changes between iter - ations , regardless of feedback type . Generic feedback was correlated with higher Simplicity scores , regardless of expe - rience ( p < 0 . 05 ) . There were no other signiﬁcant differences across dimensions . IMPLICATIONS Evidence from the ﬁrst study suggested that aggregated crowd critiques can address many of the issues raised by ex - pert critique providers . Designers from the 99designs con - test site expressed enthusiasm for crowd critiques and offered concrete examples of how they inﬂuenced their design pro - cess , echoing the ﬁndings of our second study . Compared to the generic feedback condition , designers who got crowd cri - tiques reported signiﬁcantly better recognition of issues , more changes , and better overall designs . By mitigating “good sub - ject” effects [ 19 ] through our controlled experiment , these results show that CrowdCrit provides value over the generic feedback typical in design contests . Yet , when we examine independent ratings of their designs , we see no signiﬁcant difference in quality or most other di - mensions . One interpretation of these results is that crowd critique leads designers to think they are making signiﬁcant improvements , when in fact , they tend to make minor revi - sions . The opportunity to respond to speciﬁc issues , provided by CrowdCrit , may ﬁxate designers on addressing minor , easily - addressed problems rather than contemplating broader , more substantive changes . The aggregation interface pro - vides a “Top Feedback” section aimed to mitigate this prob - lem , but this orientation towards simple frequency may have oriented designers towards issues that were popular or easily identiﬁed ( e . g . a typo ) rather than important . Further , pre - senting critiques in list form may have led to changes that were idiosyncratic , rather than contributing to a more syner - gistic improvement . This could have resulted in designs that were “busier” but not necessarily better , possibly explaining why crowd critique led to more complex designs . Our ﬁrst study , which found that many designers approached their re - visions by working their way through the list of critiques , also supports this interpretation . Another partial explanation may come from the nature of our study site , a host of online design contests . Typically only the winner is paid , so designers may ﬁnd that submitting multi - ple revisions to fewer contests is a less effective strategy than submitting a single reasonable version to many contests . The culture of 99designs may have discouraged designers from making signiﬁcant changes , even if the crowd critiques were considered valuable . Our ﬁnding that designers with more contest experience made fewer changes , regardless of critique type , also supports this interpretation . Crowd critique led in - experienced designers to make bigger changes , possibly be - cause they had not yet embraced this “shotgun approach” as more productive . Systems like CrowdCrit may be most appropriate for domains in which quality is somewhat subjective , experts frequently disagree , and a rich diversity of feedback is valued . Although usability heuristics have now been widely adopted as a foun - dation of HCI practice , Nielsen and Molich found little corre - lation between evaluators , experts adjusted their rubrics in re - sponse to novice feedback , and the average novice found only 20 – 50 % of issues identiﬁed by experts . They acknowledge that usability evaluation is challenging , yet argue convinc - ingly that it is still worthwhile because “even ﬁnding some problems is of course better than ﬁnding no problems” [ 20 ] . We propose that a similar argument may apply to crowd - sourced design critique and possibly other domains of crowd - sourced feedback . FUTURE WORK Enhance scaffolding Scaffolding allows novices to quickly become productive within an unfamiliar domain to gradually learn to do new things without support . As such , scaffolding can come in many forms . Our critique statements provided a ﬁrst approx - imation at scaffolding visual design critique , but we did not measure the effect on crowd worker learning , or implement techniques such as progressively “fading” scaffolds , requir - ing the learner to draw on memory to ﬁll new gaps [ 25 ] . Crowd feedback systems could be modiﬁed to monitor cri - tique providers and , as they gain experience , remove supports for structured feedback . For example , CrowdCrit could adapt the pre - authored critique statements to match a worker’s skill level , perhaps starting with surface - level elements ( like fonts , text , and images ) , moving to more abstract principles ( like balance and emphasis ) , eventually dropping all of these in fa - vor of more open - ended critique . Support new principles For crowd feedback services to generalize across domains , they should offer ﬂexibility in how they structure novice be - havior . For CrowdCrit , this could mean allowing designers to author their own set of visual design principles or to modify the ones provided in our system . This feature could also make CrowdCrit useful in other domains , such as interface design or architectural design , and to other types of crowds , such as user communities or concerned citizens . Explore new power dynamics This paper focused on the practical scenario of introducing feedback in design contests . As a result , we had the oppor - tunity to observe interactions between designers , crowds , and clients ( or contest hosts ) . The introduction of crowd critique created the potential to challenge the traditional relationship between the designer and client . Instead of merely respond - ing to the clients’ whims , designers saw an opportunity to use crowd feedback as empirical support for certain design deci - sions . The crowd , in a sense , represents both the target audi - ence and a body of knowledgeable designers . Future work on crowd feedback systems can explore new interaction models where clients also view and interpret crowd feedback . CONCLUSION We presented CrowdCrit , a system which uses crowdsourc - ing to provide designers with a fast , scalable , and reason - ably priced source of feedback . We built a working proto - type of CrowdCrit and deployed it in three studies . This pa - per makes the following contributions . First , we present the CrowdCrit system itself , demonstrating how to structure the complex task of design critique for crowd workers , and how to aggregate and present large , diverse quantities of crowd feedback for review . Second , we compared crowd and expert critiques , showing how aggregated crowd critiques converges towards expert feedback , and demonstrating that agreement trends low among both novice and expert critiquers . Our sec - ond study used interviews to provide rich descriptions of a di - verse group of designers’ attitudes , concerns , and enthusiasm towards crowd feedback and patterns of use in a real - world design context . Our third study showed that crowd feedback led designers to report noticing more issues , enacting more changes , and producing better designs that the generic feed - back more typical of online contest hosts . It also identiﬁed a gap between designers’ perceptions and independent rat - ings of changes to their ﬁnal designs . These evaluations are among the ﬁrst to attempt to describe and quantify the value of crowd feedback . Finally , we discuss design implications for building crowd feedback systems that provide valuable and equitable services to both designers and crowd workers . ACKNOWLEDGMENTS The authors wish to thank our designer participants , crowd - workers , Allegheny County ( PA ) Parks Department , 99de - signs . com staff , and our anonymous reviewers . Financial sup - port provided by the National Science Foundation under IIS grants 1210836 , 1208382 , and 1217096 . REFERENCES 1 . 99designs . http : / / www . 99designs . com / . 2 . Barrett , T . A comparison of the goals of studio professors conducting critiques and art education goals for teaching criticism . Studies in art education ( 1988 ) , 22 – 27 . 3 . Bransford , J . D . , Brown , A . L . , and Cocking , R . R . , Eds . How people learn : brain , mind , experience , and school . National Academy Press , Washington , D . C . , 2000 . 4 . Dannels , D . , Gaffney , A . , and Martin , K . Beyond content , deeper than delivery : What critique feedback reveals about communication expectations in design education . Int . J . Schol . Teach . & Learn . 2 , 2 ( 2008 ) . 5 . Dannels , D . P . , and Martin , K . N . Critiquing critiques a genre analysis of feedback across novice to expert design studios . Jo . Bus . & Tech . Comm . 22 , 2 ( 2008 ) , 135 – 159 . 6 . Dondis , D . A . A primer of visual literacy . MIT Press , Cambridge , Mass . , 1973 . 7 . Dow , S . , Fortuna , J . , Schwartz , D . , Altringer , B . , Schwartz , D . , and Klemmer , S . Prototyping dynamics : sharing multiple designs improves exploration , group rapport , and results . In Proc . CHI 2011 ( 2011 ) , 2807 – 2816 . 8 . Dow , S . , Gerber , E . , and Wong , A . A pilot study of using crowds in the classroom . In Proc . CHI 2013 , CHI ’13 ( New York , NY , USA , 2013 ) , 227 – 236 . 9 . Dow , S . P . , Glassco , A . , Kass , J . , Schwarz , M . , Schwartz , D . L . , and Klemmer , S . R . Parallel prototyping leads to better design results , more divergence , and increased self - efﬁcacy . ACM Trans . Comput . - Hum . Interact . 17 , 4 ( Dec . 2010 ) , 18 : 1 – 18 : 24 . 10 . Easterday , M . W . , Rees Lewis , D . , Fitzpatrick , C . , and Gerber , E . M . Computer supported novice group critique . In Proc . of DIS ’14 ( 2014 ) . 11 . Feedback army . http : / / www . feedbackarmy . com / . 12 . Feldman , E . Practical Art criticism . Prentice Hall , Englewood Cliffs , NJ , 1994 . 13 . Five second test . http : / / www . fivesecondtest . com / . 14 . Forrst . http : / / www . forrst . com / . 15 . Klemmer , S . R . , Hartmann , B . , and Takayama , L . How bodies matter : ﬁve themes for interaction design . In Proceedings of DIS , ACM ( 2006 ) , 140 – 149 . 16 . Kulkarni , C . , and Klemmer , S . R . Learning design wisdom by augmenting physical studio critique with online self - assessment . Tech . rep . , Stanford U . , 2012 . 17 . Lidwell , W . , Holden , K . , and Butler , J . Universal principles of design . Rockport Pub , 2010 . 18 . Marlow , J . , and Dabbish , L . From rookie to all - star : Professional development in a graphic design community of practice . In Proc . CSCW 2014 ( 2014 ) . 19 . Nichols , A . L . , and Maner , J . K . The good - subject effect : investigating participant demand characteristics . The Journal of general psychology 135 , 2 ( Apr . 2008 ) , 151 – 165 . PMID : 18507315 . 20 . Nielsen , J . , and Molich , R . Heuristic evaluation of user interfaces . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’90 , ACM ( New York , NY , USA , 1990 ) , 249 – 256 . 21 . Reynolds , G . Presentation Zen : Simple ideas on presentation design and delivery . New Riders , 2011 . 22 . Sadler , D . R . Formative assessment and the design of instructional systems . Instr . Sci . 18 , 2 ( 1989 ) , 119 – 144 . 23 . Sch ¨ on , D . Educating the Reﬂective Practitioner . Jossey – Bass Publishers , San Francisco , 1990 . 24 . Shneiderman , B . The eyes have it : a task by data type taxonomy for information visualizations . In Proc . VL / HCC 1996 ( 1996 ) , 336 – 343 . 25 . Soloway , E . , Guzdial , M . , and Hay , K . E . Learner - centered design : the challenge for HCI in the 21st century . interactions 1 ( April 1994 ) , 36 – 48 . 26 . Tinapple , D . , Olson , L . , and Sadauskas , J . Critviz : Web - based software supporting peer critique in large creative classrooms . Bulletin of the IEEE Technical Committee on Learning Technology 15 , 1 ( 2013 ) , 29 . 27 . Tohidi , M . , Buxton , W . , Baecker , R . , and Sellen , A . Getting the right design and the design right . In Proceedings of CHI , ACM ( 2006 ) , 1243 – 1252 . 28 . Wattenberg , M . , and Kriss , J . Designing for social data analysis . IEEE Transactions on Visualization and Computer Graphics 12 , 4 ( 2006 ) , 549 – 557 . 29 . Willett , W . , Heer , J . , and Agrawala , M . Strategies for crowdsourcing social data analysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’12 , ACM ( New York , NY , USA , 2012 ) , 227 – 236 . 30 . Willett , W . , Heer , J . , Hellerstein , J . , and Agrawala , M . Commentspace : structured support for collaborative visual analysis . In Proc . CHI 2011 ( 2011 ) , 3131 – 3140 . 31 . Williams , R . The Non - Designer’s design book . Peachpit Press , 2008 . 32 . Xu , A . , and Bailey , B . What do you think ? : a case study of beneﬁt , expectation , and interaction in a large online critique community . In Proc . CSCW 2012 ( 2012 ) , 295 – 304 . 33 . Xu , A . , Bailey , B . P . , and Huang , S . - W . Voyant : Generating structured feedback on visual designs using a crowd of non - experts . In Proc . CSCW 2014 ( 2014 ) .