“I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI Jaemarie Solyst jsolyst @ andrew . cmu . edu Shixian Xie shixianx @ andrew . cmu . edu Ellia Yang elliay @ andrew . cmu . edu Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University Pittsburgh , PA , USA Pittsburgh , PA , USA Pittsburgh , PA , USA Angela E . B . Stewart Motahhare Eslami Jessica Hammer angelas @ pitt . edu meslami @ andrew . cmu . edu hammerj @ andrew . cmu . edu University of Pittsburgh Carnegie Mellon University Carnegie Mellon University Pittsburgh , PA , USA Pittsburgh , PA , USA Pittsburgh , PA , USA Amy Ogan aeo @ andrew . cmu . edu Carnegie Mellon University Pittsburgh , PA , USA ABSTRACT Artifcial intelligence ( AI ) literacy is especially important for those who may not be well - represented in technology design . We worked with ten Black girls in ffth and sixth grade from a predominantly Black school to understand their perceptions around fair and ac - countable AI and how they can have an empowered role in the creation of AI . Thematic analysis of discussions and activity ar - tifacts from a summer camp and after - school session revealed a number of fndings around how Black girls : perceive AI , primarily consider fairness as niceness and equality ( but may need support considering other notions , such as equity ) , consider accountability , and envision a just future . We also discuss how the learners can be positioned as decision - making designers in creating AI technology , as well as how AI literacy learning experiences can be empowering . CCS CONCEPTS • Social and professional topics → Computing education ; Informal education ; • Human - centered computing → Human computer interaction ( HCI ) . KEYWORDS AI literacy , AI ethics , artifcial intelligence , Black girls , design ACM Reference Format : Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan . 2023 . “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI ’23 ) , April 23 – 28 , 2023 , Hamburg , Germany . ACM , New York , NY , USA , 14 pages . https : / / doi . org / 10 . 1145 / 3544548 . 3581378 This work is licensed under a Creative Commons Attribution International 4 . 0 License . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9421 - 5 / 23 / 04 . https : / / doi . org / 10 . 1145 / 3544548 . 3581378 1 INTRODUCTION Today , youth are one of the main audiences of Artifcial Intelligence ( AI ) systems , from social media platforms that utilize algorithms to determine what content to show , to AI tools used in education for grading [ 2 ] and guiding instructors [ 49 ] . Such systems are usu - ally designed to make more efcient decisions and even reduce the potential for human biases . Yet , recent years have witnessed social inequities and biases introduced or exacerbated via AI systems [ 3 , 14 , 29 , 30 , 50 ] – and youth are one of the main afected stakehold - ers . For example , prior work has shown racial disparities in child maltreatment prediction tools which unfairly predicted that Black families may have instances of child maltreatment [ 14 , 19 , 92 ] . This has caused families to undergo stress , afecting children’s , especially Black children’s , well - being . In response to inequities in AI systems , young people have seen a role for themselves , showing great potential to combat inficted harm on them and their communities . For example , when the UK government developed a grading algorithm at the beginning of the COVID - 19 to rectify the teacher shortage , the bias in this algorithm against the working class resulted in nationwide protests , called “Fuck the Algorithm” by students , and furthermore , the termination of the algorithm [ 2 ] . To support youth , learners could be equipped with even deeper knowledge to understand the complexities of AI systems in order to best advocate for their rights . AI literacy is becoming increasingly important , with the CHI community’s recent interest and investment in defning and explor - ing AI literacy [ 64 ] . This calls for educating about AI to foster their literacy around potential ( negative ) impacts of AI - driven systems and equip them with the right tools to support their arguments and advocacy [ 1 , 21 , 77 , 82 ] . Further , this is particularly of great importance for underrepresented and underserved groups who may be most afected by algorithmic harms . In this paper , we focus on fostering AI literacy among Black girls who are not well - represented in AI and technology design [ 89 ] , yet are signifcantly impacted by algorithmic injustice ( e . g . , [ 12 ] ) . For example , youth [ 23 ] and users with accents , including people CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan of color [ 63 ] , are less likely to be understood by voice recogni - tion technology . Computer vision is less likely to recognize Black users in real time [ 12 ] and mislabel photos of Black people [ 69 ] . Companies like Google have been faulted for having racial bias in their search results for Black youth [ 42 ] . Feminine voices are less likely to be understood by voice recognition software [ 5 ] . There remains gender disparity in those who work in tech . Recent reports show that women are underrepresented , making up only 26 % of AI practitioners [ 33 ] and 14 % of authors on AI research papers [ 88 ] . Particularly high stakes contexts include bias in hiring tools , which have held bias against female applicants [ 20 ] , or predictive policing being biased against Black citizens [ 72 ] . With multiple facets of identity underrepresented in AI and tech - nology design , including gender , race , and age , it is necessary for young Black girls to have efective computing and AI educational resources that are built around their ways of being and knowing , and that leverage their unique experiences with the world . However , there is a lack of culturally responsive computing and AI oppor - tunities that resonate with learners [ 82 ] , as well as prior research focusing on Black girls . Therefore , to create such a curriculum , it is important to understand Black girls’ ideas , perceptions , and knowledge gaps in fairness and accountability in AI systems . In line with Scott et al . [ 82 ] , an efective curriculum aims to position girls of color as ‘technosocial change agents’ , poised to critique and push for justice in technology design , employment , and refnement . In this paper , we work with Black girls in ffth and sixth grade , which is a prime age to develop STEM identity right before the shift to secondary education [ 90 ] , and therefore an ideal time for efective educational experiences and interventions focused on computing and AI . We strive to answer the following research questions . How do Black girls : • RQ1 : Defne fairness in AI ? • RQ2 : Consider the type of AI in their perceptions of AI ? • RQ3 : Imagine future procedures , including processes and accountability of stakeholders , to create fair AI ? In our study , we focus on diferent forms of AI ( public and private , as well as tangible and intangible ) , since these systems afect users in diferent ways and are created with diferent motivations in mind . Users may have very diferent perceptions of various AI . We have a further interest in how Black girls see themselves as part ( or not ) of the AI creation process as a means to understand how to help position them as powerful agents of change . We believe that these RQs will lead to understanding an un - derrepresented group and support creating efective AI literacy learning experiences . Our fndings span how learners primarily view fairness as equality and niceness , how learners may trust computers more than humans , and how moral framings and their own lived experiences may impact their understanding of which AI is fair . We further fnd that learners view people with power and infuence as accountable for making AI fair , alongside the original developers . We lastly describe learner ideas about the role they hope to play , specifcally designing , in the future of AI creation . 2 RELATED WORK 2 . 1 Children’s Perceptions of AI Children are exposed to AI earlier and more frequently than ever before . Previous work has found that children’s direct interactions with home digital assistants , such as Alexa , have increased over the past few years [ 79 ] . Understanding how this shift changes children’s perception of technology and how it impacts their lives is vital for researchers in developing efective AI education . There are multiple factors that infuence children’s views of AI . While age is one of these deciding factors of how children view AI [ 24 , 32 ] , increasing in age does not always mean a better understanding of the abilities and constraints of computers and AI [ 93 ] . Prior work has found that compared to younger children ( 3 and 4 years old ) whose answers varied , older children ( 6 - 10 years old ) in general were more likely to believe that AI was smarter than them [ 23 ] . Cultural background is another factor that afects how children view AI . Children who are from cultures that enable them to have more exposure to AI assistants tend to be less skeptical about them [ 22 ] . Gender can also become an infuencing factor . Although no direct research has been conducted on the impacts of gender , girls in general have less exposure to computers than boys [ 94 ] . Lack of prior experience can inhibit children’s ability to accurately assess what types of problems a computer can solve [ 93 ] . Therefore , it is critical to understand how girls perceive and interact with AI technology . The form of AI might make a diference in perception as well [ 65 ] . Previous work indicates that children think robots belong to a completely diferent category compared to other technologies including computers [ 23 ] . Furthermore , much previous work ex - ploring children’s perceptions of intelligent technology is about tangible AI agents , e . g . , [ 22 , 24 , 96 ] , while this aims to also explore perceptions of a number of intangible algorithms . Findings from this work can contribute to understanding how diferent types of AI technologies can infuence youth users’ understanding and judgments about fairness . 2 . 2 Children’s Perspectives of Fairness and Applications to AI As AI technologies continue to afect and shape the lives of an increasingly large amount of people , the topic of Fairness , Account - ability , Transparency , and Ethics ( FATE ) in AI has subsequently gained importance and relevance , especially as algorithmic systems have sometimes been found to reproduce and / or amplify social stereotypes , biases , and inequalities - and in other cases , reduce them [ 11 , 12 , 71 ] . Prior studies have proposed methods to formalize measurements of algorithmic fairness , including mathematical and computational defnitions [ 26 , 40 , 44 , 60 ] . Other Prior studies have focused on perspectives on fairness in AI in various populations including a general public adult population [ 39 , 95 ] , or specifc groups , such as university students , computer scientists , or tradi - tionally marginalized communities [ 56 , 57 , 67 , 98 ] . However , less is known about children’s perspectives on algorithmic fairness . There - fore , below we frst discuss previous work examining children’s developing notions of fairness in general and the need to extend these notions in the AI domain . “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Early theories in this space focused on the development of moral judgment in children by age cohort . According to Piaget’s Theory of Moral Development , the stage of heteronomous morality applies to children ages 5 - 11 and is the time in which their judgment of the morality of behavior is based on outcomes rather than inten - tions , and rules are applied infexibly and absolutely . The next stage , autonomous morality , typically emerges from ages 11 to adoles - cence . This is when children are able to view situations from the perspectives of others and decide whether or not a rule is fair [ 76 ] . Later , Kohlberg’s Theory of Moral Development built upon and reconceived these stages . In this theory , children advance between the preconventional morality and conventional morality stages . At preconventional morality , children’s moral decisions are shaped by the consequences that they might receive from breaking rules . This can lead to revenge - based philosophies ( an “eye for an eye” ) , and a focus on the individual self . In the transition to conventional morality , decisions begin to be infuenced by the moral codes of the adults in a child’s life and the norms their social groups hold , including respecting authority and the law [ 58 ] . Our study focuses on children aged 9 - 12 , a time when children may be in transition in their morality judgments as they begin to acquire a less egocentric view of the world . As the focus of our work is girls , it is also important to consider theories of moral development that incorporate gender . Gilligan’s theory is based on Kohlberg’s and was developed after Gilligan began to critique Kohlberg’s male - centered studies , concluding that his theory was gender - biased . According to Gilligan , young women have been socialized to emphasize interpersonal relation - ships , while young men emphasize the importance of lawfulness and order , making it sexist to center law and order as a more “ad - vanced” stage of moral development . According to her theory , girls aged 9 - 12 should be transitioning from the simplest level of moral reasoning , selfshness and strict egocentrism to having responsibil - ity to others [ 70 ] . As a specifc subgenre of moral development , there has been a long tradition of examining children’s developing notions of fair - ness . According to literature on organizational justice , there are two main categories of fairness : distributive and procedural . Dis - tributive refers to fairness in the results of decision - making ; this is also known as resource distribution or resource allocation [ 100 ] . Procedural , on the other hand , refers to fairness in the process of decision - making [ 40 ] . The most extensive previous work on chil - dren’s perceptions of fairness has been in the context of distributive fairness ( resource allocation ) . This work has shown that children have a deep aversion to inequality , and are able to identify behavior that they view as unfair starting from infancy [ 87 ] . However , other studies have shown that while younger children are able to recog - nize and state actions that they think would be fair in a scenario , older children ( age 8 and onwards ) were more likely to follow those actions while younger children only endorsed equality without actually implementing it [ 9 ] . Children aged 8 and onwards even demonstrate advantageous inequity aversion , which means that they reject unfair resource allocation , even if it favors themselves [ 68 ] , and will even throw away extra resources if they cannot be distributed equally [ 8 , 84 ] . Younger children preferred equality over equity and were more likely to distribute resources equally [ 6 ] , although fndings support the idea that children tend to distribute resources more equitably as they grow older [ 51 , 52 , 78 , 81 ] . For instance , older children tend to allocate more resources to recipi - ents who need them most , and are infuenced by wealth diferences , merit , and empathy [ 51 , 55 , 74 ] . The concept of procedural fairness has even stronger connec - tions to AI or algorithmic fairness . While less work has been done on understanding children’s perceptions of this type of fairness , existing research does demonstrate that as children grow older they are increasingly likely to favor fair processes ( such as distribut - ing fun stickers via an ‘impartial’ coin fip versus advantageous distribution ) and the general appearance of impartiality in how resources are distributed , over fair outcomes [ 41 ] . Alternatively , one recent study found that when children were presented with an existing unequal distribution between two third parties ( one party had 2 stickers and the other party had 1 sticker ) , on average the older age group chose to give a resource to the disadvantaged party rather than fip a coin , while the younger age group did not show a preference between the two processes [ 25 ] . They also found that girls were more likely to rectify the inequality than boys , who chose more often to fip the coin . Finally , there are several studies that have focused specifcally on marginalized populations and algorithmic fairness . In [ 85 ] , the participants were children of color between ages 9 and 14 who engaged in designing a fair AI Librarian through co - designing ses - sions , role - playing , table - top polling , and storyboarding . Findings show that the children tended to equate fairness with kindness , and the results of the co - design were that the AI technology should never be “a bigot , gendered , sexist , racist , uncultured , homophobic or a bully , ” which was essential for ensuring that the AI wasn’t rude and therefore unfair [ 85 ] . Workshops investigating perspectives on algorithmic fairness from traditionally marginalized groups that have used similar methods to ours ( presenting algorithmic fair - ness scenarios and discussing perspectives with participants ) have seemingly only worked with adults ( e . g . , [ 98 ] ) . They found that participants drew many connections between the scenarios given and negative experiences with discrimination and stereotyping that they had experienced in their personal lives . Participants in this study also trusted human decision - making over algorithmic decision - making . 2 . 3 AI Education Opportunities Currently , few children have the opportunity to learn about AI due the relatively new emphasis on computing education in schools , with an even smaller emphasis on AI in nationwide standards for computing [ 4 ] . Children from lower - resourced backgrounds may have even less access to AI education . Most research on K - 12 AI has a main focus on technical aspects of AI as opposed to the ethical or social implications , e . g . , [ 46 , 47 , 54 , 97 , 101 ] . Recent trends in AI literacy also include non - technical aspects , for example , Touretzsky et al . [ 91 ] describe a mix of both technical knowledge of AI and analysis of social impacts as important factors in AI education , including understanding that AI applications can have positive and negative impacts on society . Dipaola et al . [ 21 ] applied ethical aspects in a learning activity , showing that middle school children were able to analyze artifcially intelligent technologies through an ethical lens , with a focus on CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan stakeholders and their values . Girls from minoritized backgrounds , in particular , have been found to enjoy applying an ethical lens to AI [ 62 ] . However , few AI curricula have a signifcant emphasis on learners’ lived experiences or cultural background , even though prior research has shown that understanding and addressing learn - ers’ identities and backgrounds through culturally responsive or Black feminist approaches can empower girls of color to fully en - gage with computing and STEM topics [ 53 , 83 ] . Scott et al . [ 82 ] presented Culturally Responsive Computing ( CRC ) as supporting girls of color in learning computing . Based on intersectional feminism with a focus on Black girls and cultur - ally responsive teaching and pedagogy [ 37 , 61 ] , CRC takes into account learners’ funds of knowledge and aims to embolden them as technosocial change agents , ready to make change and play a part in shifting power to marginalized users . CRC has been found to support girls of color , specifcally Black girls , in developing a STEM identity and interest in computing , as they are positioned as justice - oriented knowledge and technology producers [ 83 ] . Core to culturally responsive pedagogy is developing learners’ cultural competence and critical consciousness [ 61 ] , such that they can en - gage in thought and conversation about power , identity , and justice [ 38 ] . Prior work by Everson et al . [ 31 ] describes how a group of diverse high schoolers co - created a course to engage in critically conscious counternarratives about computing . More work needs to be done to learn about how specifcally Black girls , a group excluded from prior work on AI education , can become AI literate . We aim to contribute to flling this gap by understanding Black girls’ perceptions , opinions , and knowledge gaps about fair AI as a way to begin creating efective learning experiences , frst taking inspiration from Touretzky et al . ’s [ 91 ] fve big ideas with a focus on ethics and society . In our camp and workshops , we aimed to take an asset - based approach [ 13 , 36 ] to center the learners’ prior knowledge and interests as a base for further learning and engagement [ 16 ] . This approach is particularly appropriate in a culturally responsive context for Black girls [ 66 , 82 ] . 3 METHODS We next describe two data collection sessions , a camp - style work - shop that took place over three consecutive sessions within a week - long summer computing and technology program and a single after - school session at the same school . Workshops as a methodology are used within HCI , including within the CHI community [ 98 ] , and are often used to explore values , needs , and design . Workshops have a history of helping to position participants as having agency in sharing their insights about their experiences with technology , shifting power away from the researchers and to the participants [ 99 ] . 3 . 1 Participants & Recruitment Data collection took place in a city with a mid - sized population ( approximately 300 , 000 inhabitants ) on the East coast of the United States ( U . S . ) . Relevant to the participants’ experiences , the city has a history of and still is currently challenged with gentrifcation and segregation mostly between Black and White inhabitants . We conducted two programs , a three - session workshop within a summer camp ( N = 8 ) and one after - school session ( N = 2 ) . For both data collections , participants ( Table 1 ) were recruited from ffth and sixth grades at the school where the camp and after - school session took place . Female and non - binary students in these grades were informed about the camp from in - school announcements , and their families received emails about joining the program . For the camp , one researcher on our team also ran an informational table during the normal school pickup time , so parents could ask questions about the program and the research consent forms . There were no exclusions other than the gender criteria ; all learners who wanted to join the camp and the after - school session were included . The school demographics were described as “99 % ” Black learners by the headmaster of the school . We note that these programs were conducted in the context of ongoing challenges with the COVID - 19 pandemic which had signifcant impacts on participation and recruitment , with over half of our scheduled data collection sessions over the course of a year being cancelled or reducing attendance . The pandemic had disproportionately high impacts on minoritized communities . 3 . 2 Data Collection Sessions 3 . 2 . 1 Camp Data . We conducted three two - hour - long workshop sessions over two days , as part of a fve - day computing camp . Our sessions were conducted on the frst two days , after which learners went on to explore additional ideas of power in computing and tech - nology . Each session started with a 30 - minute icebreaker activity ( e . g . , a name game or an activity supporting thought around power and identity ) before we got into our workshop material on AI and fairness . Learners were compensated $ 100 for their participation in the week - long camp . Sessions were conducted by the research team , with two primary teachers . The frst teacher was an Asian American woman with a background in designing and teaching inclusive computing cur - riculum for youth and undergraduates . The second teacher was a Black American woman with a background in culturally responsive pedagogy and over 20 years of experience teaching STEM topics to middle schoolers in the United States and Africa . Other team mem - bers primarily took notes or assisted in supporting activities and data collection . They were all Asian , Asian American , and White women in undergraduate and master’s degree programs . In session 1 of the three - session workshop , we ran a survey at the start of the session to gain more information on learner’s back - grounds where we collected demographic information ( see Table 1 ) , asking about their technology interests and knowledge , including if they agreed with the statement “I am curious about programming or computer science . ” We began by discussing what AI is and what fairness means , including examples of AI technology , including Alexa , video flters , Netfix recommendations , and Google Trans - late . To better support a more technical understanding of AI for the rest of the camp , we then talked through a high - level description of what AI is and how it is diferent from non - AI technology . Next , we ran a short activity called AI Stories , in which we asked learners to develop and illustrate or write one story where AI is fair and one story where AI is not fair . Finally , we ended this session with a more detailed conversation about fve specifc algorithms and an example of potential unfairness or harmful aspects in each . Table 2 lists these algorithms in order of discussion . “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Table 1 : Participant demographics and IDs . Most learners did not have prior experience . ID Group Age Self - Described Race Prior Computing Exp . Curious About Prog . L1 Camp 12 Black Yes ( in - class activities ) Agree L2 Camp 10 Black No Disagree L3 Camp 11 Black No Disagree L4 Camp 9 Did not say No Agree L5 Camp 10 Black Yes ( robotics class ) Agree L6 Camp 11 Black & Hispanic No Neutral L7 Camp 11 Black No Agree L8 Camp 10 Did not say Yes ( some programming ) Agree L9 After - school 11 Black No n . a . L10 After - school 11 Black No n . a . Table 2 : AI examples were a mix of diferent types of algorithms . AI Technology Example of Unfairness or Harm ( 1 ) Voice recognition , algorithms that can un - derstand what people say based on voiced input , e . g . , as used in assistant technology such as Siri or Alexa ( 2 ) Public School Lottery , an algorithm that decides whether students should be admitted to certain schools ( 3 ) “FamilyBoost” ( anonymized name ) , an algo - rithm that fags in - need families to be contacted by social workers and ofered more resources ( e . g . , child - care , food stamps , etc . ) ( 4 ) YouTube’s video recommendation system , an algorithm that chooses which videos to show to diferent users ( 5 ) Stray Animal Feeder , a fctitious , public , computer - vision - based AI that feeds stray dogs and cats by determining what type of feed they need Might fail more for people with certain accents Might advantage students who already have a sibling at a school or are experiencing poverty Might not accurately fag some families in the learners’ neighborhoods while accu - rately identifying those in another nearby neighborhood Might provide potentially inappropriate or hateful content to some users , as well as not recommend some hard - working content creators Might misrecognize animals and give them the wrong food , inducing sickness or poor nutrition These algorithms were chosen due to the relevance we thought they may have to the learners , with voice recognition being ubiqui - tous in a number of technologies , a school lottery having afected them or children like them , FamilyBoost being an existing local algorithm to make decisions about families with children in the city where the data collection took place , YouTube as a popular platform for a range of users including children , and the feeding machine being inspired by girls’ strong interest in future AI helping animals [ 86 ] . To prompt discussion about unfairness , we presented a potentially unfair or harmful scenario for each of these algorithms ( see Table 2 ) . Learners were asked to consider who should be re - sponsible for fxing the unfair AI and who should help design such systems . In session 2 , we engaged learners in a technical defnition of algorithms to facilitate a deeper discussion around training data and bias . We ended this session with a tangible card game , designed to teach more about bias in training data . The game’s learning goals included 1 ) understanding that training data is an input that has an impact on the output of AI - driven algorithms and 2 ) understanding that human bias can impact training data to result in algorithmic bias . For the scope of this paper , we do not focus on the learners’ interactions with this game but note that after this session , learners had more exposure to technical aspects of AI and machine learning . Lastly , in session 3 , we led an individual fnal project , in which learners came up with an idea about an AI that they wanted to exist , scafolded as a booklet with spaces to write or draw . While the booklet activity was individual , learners were seated at tables in groups . Some groups engaged in conversation about the activity or other topics as they worked , while others remained quiet and focused on their projects . The booklet prompts ( see Figure 1 ) in order are : Fairness in AI means , My AI technology idea is called , What it does is , What it looks like , Fair interactions my technology could have are , The people who help with the creation of AI are , They could help by , Unfair interactions I want my technology to avoid are , I want to avoid this unfair interaction because it could , If this were to happen the people who could help are , They could help by . Finally , we CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan discussed learners’ work and ideas by open - endedly asking them to walk us through their booklet progress in one - on - one conversations . They were asked to defne fairness in AI and then come up with an imaginative AI system to solve a problem in their community . Then , the learners were prompted to consider if there were any unfair interactions that could occur , who should be involved in the design , and who should be responsible for fxing unfairness . For learners who did not fnish the booklet ( e . g . , due to attention to time spent drawing ) , we discussed the rest of the prompts in a conversation before fnishing the session . 3 . 2 . 2 Afer - school Session . We conducted an hour - and - a - half after - school session in the same school with two new learners . Data from this session was used to understand if our main fndings from the camp achieved data saturation . This session took place in a research workshop series , taking place in weekly after - school sessions dedicated to exposure to social robots . By the time of the AI and fairness session , learners had some sessions about robots and designing technology - based companions but not about concepts in AI or a focus on ethics . In the shorter time block , we ran a subset of the activities from the camp , and no survey questions were asked , since most demographic information about learners ( e . g . , age , race , prior experience ) was gotten from data collected in a prior session . Because of this , learners in this session ( L9 and L10 ) were not asked if they were curious about programming . We started by discussing what AI is and what fairness means , followed by the AI Stories activity , and then covered the technical defnition of algorithms . We then discussed the voice recognition algorithm and the public school lottery algorithm , with the same unfair scenarios as presented in the camp . Learners were compensated $ 100 for their engagement in the entire after - school series over the course of a semester . The main teacher for this data collection session was the frst author of this paper , the same Asian American woman researcher who co - taught the camp . Since we saw that no major new themes emerged from this session , we chose to conclude data collection for this project . 3 . 2 . 3 Data Capture . The camp sessions and the after - school ses - sion were recorded with audio , and shorter conversations with learners about their artifacts included video and audio recordings . For all recordings , we reminded learners that we would like to record and confrmed that they were aware of and approved of it . Research assistants took detailed notes as they observed the ses - sions , as well as in follow - up conversations with learners about their artifacts . With the learners’ approval , we additionally took pic - tures of their artifacts . Audio and video recordings were transcribed for analysis . 3 . 3 Data Analysis For data analysis , our team took an iterative and consensus - based approach [ 43 ] with all authors on the paper . Three researchers ( frst , second , and third authors ) conducted thematic analysis using inductive open coding [ 17 ] to fnd themes in the learners’ artifacts ( AI stories activity and fnal project ) , transcribed recordings , and research notes of the session . The researchers separately created a list of codes and themes , then they met to create afnity diagrams [ 7 ] . The full team then reviewed the afnity diagrams , as well as raw data from the artifacts and research notes , and provided feedback . Taking into account the team feedback , the three researchers then combined themes and discussed , coming up with a shorter set of themes . The full team of researchers then discussed the themes again , justifed the themes , returned to the data to tie back the themes once more , and made iterations for the fnal set of fndings that we present next . The results describe the main themes that address the research questions . We illustrate our fndings and tie them back to the data with quotes from the learners , as storytelling and highlighting participant voices is core to centering people of color in work that addresses race [ 73 ] . 3 . 4 Research Approach & Positionality Statement In this research , we worked with under - resourced and marginalized young learners to understand the opportunities and challenges in providing efective educational experiences and interventions in AI computing and fairness issues . Previous work has brought up sev - eral risks for conducting research with underserved communities without creating a sustainable relationship with those communities to beneft them in the long term , which has resulted in calls for providing resources to underserved communities over time that they can leverage and use after the research activities are done [ 45 ] . To achieve this goal , we aimed to provide our partner school with the right tools and materials that can help them create continuous learning opportunities in the domain of AI and technology , which go beyond our camp sessions . As such , we situated the data collec - tion sessions within our ongoing collaborative eforts to support the development of technology programs at the school . We frst interacted with the school when they replied to a social media re - cruiting post , which ofered research engagement and educational experiences at the intersection of computing , technology , and de - sign . The headmaster and coordinating staf asked for assistance in building programming aimed at computing and technology , as they didn’t have enough staf with the relevant expertise to do so on the scale they desired . Based on this need , we developed the camp and after - school workshop series as our initial engagement with the school community , with plans to support teacher profes - sional development , provide additional learning resources , and ofer family - friendly design and computing learning experiences in the future . We recognize that this work is impacted by our personal expe - riences and backgrounds . Our team of authors is composed of a diverse range of backgrounds and expertise , including academic backgrounds in computer science , learning science , design , cul - turally responsive computing , Black feminism , and ethics in AI . The racial demographics of our team include Asian , Black , Middle Eastern , and White , with American , Asian , and Jewish cultural back - grounds and a range of socioeconomic upbringings . All authors identify as women . Following Gary & Holmes [ 48 ] , the authors of this paper bring “insider” and “outsider” perspectives in relation to our learner pop - ulation , Black American girls in the United States . Our insider per - spectives include insight into being a woman in STEM and familiar - ity with American culture . One team member identifes as a Black American woman , with a background in computing as well as in “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Figure 1 : Images of a few of the fnal project booklet prompts . Black feminism . In terms of outsider perspectives , we acknowledge that diferent minority experiences are unique . Nonetheless , we believe that solidarity between people of color further supports critical insight into the work . Although the last author grew up in the city and has a deep understanding of its history and context , no researchers come from the neighborhood where the school is located . Additionally , our afliation was with an elite private insti - tution , which could have resulted in varied power dynamics in the collaboration . The team also consists of all adults ( 18 + ) , while our participants were children . 4 FINDINGS In the following sections , we present fndings about learners’ per - ceptions of fairness and how they applied fairness to AI . We then share fndings related to their views on fve diferent example algo - rithms , contextualizing these with prior work on children’s moral development and reasoning . And lastly , we describe how learners envisioned the AI creation and correction processes , and how they saw themselves as a part of the future of AI technology . 4 . 1 Defning Fairness and AI 4 . 1 . 1 Fairness as Primarily Equality . In asking the learners about what fairness means to them , we saw several patterns in how they defned the concept . First , the most common idea we saw was fair - ness as equality , e . g . , people getting the same amount of resources or the same access to opportunities . “Everyone is equal . ” L2 and L6 simply described . L1 illustrated her belief in a story , “When I was outside , my mom gave everyone one popsicle and everyone got the same amount " , and L5 added , " Fairness means like equal . Equal to me means when we’re all treated the same . Unfair means not fair at all , if somebody gets something and the other person doesn’t . " More complex notions of fairness , such as equity ( e . g . , people getting diferent resources based on their backgrounds and unique needs ) , took longer to be considered during the sessions and even then were not always endorsed . For example , in later conversations about the specifc algorithms ( see Table 2 ) , L9 suggested that the public school lottery should not prioritize students facing poverty or those who have a sibling at another school . When further prompted about poverty , she replied , “This one’s kind of hard , but I have to say no , because they poor , so they probably ain’t get that much . Maybe if they work hard and stuf like that and start practicing more by buying supplies and stuf like that . Just because they are poor , they don’t need anything less from us . That’s like saying a rich person can already get into school just because they are rich , except they didn’t get into school because they are poor . ” 4 . 1 . 2 Fairness as Niceness . The second most common theme was niceness . We found that when participants in our study were prompted about fairness in AI , learners mentioned scenarios in which being nice played a part : for example , L8 suggested that , “Fair is if there were two people , and if there was another person , but they didn’t have anybody to play with , and we just invited them over to play with us , that’d be fair . ” , while L2 stated , “People not being rude to certain people . " L5 noted that fair is “treating others the way you want to be treated” . However , for our participants , this conceptualization of fairness was less settled . When a facilitator probed deeper on the idea of rudeness as unfair by asking if pushing someone was fair , the girls’ response returned to equality , “it’s only fair if they pushed you frst” ( L5 ) . In fact , some girls then purported that sometimes fair is in fact not nice , before the group consensus turned to explicitly including both niceness and equality in their defnition of fairness . In the end , the concept of niceness resonated strongly as their later activities focused almost exclusively on this defnition . 4 . 1 . 3 Applications to AI . When we frst asked the girls to pro - vide examples of AI , they did not diferentiate general technology from AI technology , suggesting TV and video games as possible AI . Other non - specifc technologies that can contain AI , such as phones , computers , and tablets were also mentioned , e . g , “An iPod . You can call , and you can text , " ( L5 ) . However , when scafolded with more examples , including Siri , Google Translate , and Snapchat face recognition , they were quick to agree that these were AI . When learners engaged in the AI stories activity about fair and unfair AI , we saw that learners’ examples were in line with their initial defnitions of fairness . Of the six fair stories , four were about niceness , one was about equality , and one included notions of both . Similarly , of the six unfair stories , three were about rudeness , two were about inequality , and one was about a technology malfunction . However , the majority of the stories , fair or unfair , did not directly involve AI - powered technology . L3 , L5 , and L6 described fair or unfair AI interactions within Roblox , a popular online game ; for CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan instance , where a player on the other side was kind ( fair ) or rude ( unfair ) . Learners that did include AI described interactions that were pleasant as fair : for instance , a digital assistant playing a song along with her story about a ‘nice’ interaction between sisters ( L4 ) , or unpleasant as unfair : Google translate mistranslating a message and causing a miscommunication ( L5 ) . 4 . 2 Cognitive and Moral Framings Elicited by AI Algorithms We further explored the idea of fairness in AI ( see Table 2 ) , as well as an impromptu discussion about autocorrect in word processors that learners brought up themselves , we found the following patterns emerge related to the technology and framing of scenarios on : access and merit , users fnding workarounds , potential egocentrism , and trust . 4 . 2 . 1 Equal Access and Merit as Fair . Some of the algorithms pre - sented appeared to elicit a framing aligned with resource allocation . For instance , the FamilyBoost algorithm was introduced with the possible unfair scenario that it might not fag all families that need help based on the neighborhood they resided in , where the dis - advantaged families were in the area of the city that the learners were from . We expected that this might induce an in - group bias efect in which girls would identify with the families from their own neighborhood . However , when we asked if the learners thought FamilyBoost was overall helpful or harmful , L5 said “Maybe not [ helpful ] if it doesn’t include people , but there are a ton of families [ that need help ] . ” When asked if necessary , learners overall agreed with one another that the algorithm was not needed if it did not include everyone ; as L3 articulated , “If you are going to just pick certain people , then it’s not needed . ” L5 added , " A lot of people might not have a person to support them . It’s not needed if it doesn’t include a lot of people . " In discussing the possible advantages the public school algorithm could give , learners felt strongly that algorithms should not give an advantage to students who already have a sibling at a school . For example , L3 posed the question , “If your sibling was lucky enough to get in there and they went by themselves without anyone helping them why should their sibling be able to get in there without doing the work ? " L1 followed up with , " What [ L3 ] said , ‘cause one of them already got in and was working towards it that the other one shouldn’t be able to just get in , because they haven’t worked as hard as the other . " L9 reasoned that , “No , it’s not fair because other people probably worked hard . But they’re just like , ‘Oh the sibling is there already . ’ That’s not fair at all . ” L2 was the only learner to raise using poverty and socioeco - nomic status as fair inputs to a school algorithm , but other learners ignored this comment in favor of continuing to discuss siblings . L2 further suggested that race was important to consider as a factor to support students in being admitted to certain schools , and L3 agreed . However , when asked why the government would make such an algorithm , they recognized the disadvantage experienced by certain learners . For instance , L3 said , “For people that can’t af - ford certain schools , it gets them to places or schools that they never thought they would be able to go to . ” 4 . 2 . 2 AI Users Can Find Alternative Solutions . Although voice recog - nition not understanding accents is a staple unfair example of AI [ 63 ] , all learners present in the camp at the time at frst agreed that this was still a fair interaction . Trying to encourage learners to think again , instructors had to come up with impromptu scenarios in which this could cause problems , including a grandmother with an accent who needed assistance . One participant expressed that she did have a grandmother with non - mainstream accented English , confrming this example as relevant to young learners . However , learners ofered a number of alternative solutions for this issue , e . g . , L2 suggested “She can call me , and I understand her . . . She can write , ” and avoid using the voice recognition technology entirely , further adding that “There are workarounds . All you gotta do is change the setting of the Alexa . ” Similarly , L10 suggested that the technology was “a little bit fair , because we don’t need a whole bunch of technol - ogy , so it’s a little bit alright if that messes up . . . if they are not really from there , [ they should ] learn the language . ” This was also the case for YouTube’s recommendation system suggesting inappropriate content to viewers . They did not view it as unfair or problematic , instead suggesting ways humans could have workaround solutions , such as voluntarily moving to “YouTube Kids , ” ( L1 ) which censors content . The facilitators had to probe extensively on these examples . In a debriefng following a peer pair conversation about voice recog - nition in the camp , learners slowly began to consider how the situation may be unfair , rethinking some of the solutions they had previously provided for the user to accommodate the technology . Towards the end of the conversation , about half of the learners remained unchanged and agreed that this situation was fair , while the other half concluded that it was not . Some learners continued to agree with previously mentioned solutions , e . g . , L5 said , “Yes , it’s fair . You can just change the setting . Learners who now thought the situation was unfair responded to workaround solutions that were ofered earlier , e . g . , L1 said , “She said change the setting , but what if you couldn’t change the setting . ” Accessibility issues were also brought up , e . g . , L3 reasoned , “If you couldn’t reach your phone and you said ‘Hey Siri’ or ‘Hey Alexa’ it would be unfair , ” and L4 added , “We think it’s not fair ‘cause of the connection and the battery . ” Learners’ frst instincts were for users in these contexts to adapt to or abandon the technology and suggest workarounds instead of critically questioning fairness . 4 . 2 . 3 Egocentrism in Perceptions of Fairness . Overall , we saw that the two example algorithms that resonated most with the learners were when the unfairness was targeted very directly at them , rather than their neighborhood or even their family . For instance , when facilitators suggested that learners could change their own voices for the voice recognition technology to understand words better , the learners strongly protested this idea and agreed that the devel - oper should fx the technology rather than suggest or reintroduce workarounds . In another instance , when autocorrect came up as an example of AI , learners were very passionate about this technology , introducing the example of how autocorrect often changed their name . L1 com - mented that “When I try to type my name , it spells my name wrong . When I type it it’s right and then when I send it it’s wrong . " Multi - ple learners subsequently shared the experiences of their names “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany being wrongly corrected both during typing and after sending . The learners refected on the extent to which they had felt personally frustrated when this happened , although they did not necessarily connect fairness to potential bias , e . g . , bias toward culturally White western names . 4 . 2 . 4 Algorithms Trusted Over Humans in Decision - Making . The conversation on resource allocation brought up a critical concept of how learners thought about fairness processes , which is trust - worthiness . When asked to compare algorithms and humans in deciding school admissions , all but one determined that computers should make the decision . Specifcally , they viewed computers as more trustworthy than humans . When asked about their reasoning , learners all expressed concern that humans could easily lie and this would be difcult to detect . In preferring algorithmic decision - making to humans , L2 raised a more specifc issue that afected her perception of trust . She expressed concern that “If people want to put [ students ] in a diferent school , they will make all the Black people go to one school and all the White people go to the other . Computers would mix them all up . People would separate them . " Other girls also agreed with this statement , suggesting that learners believed algorithms to be free of holding human bias . The only learner ( L5 ) that disagreed with computers being more trustworthy than humans described a lived experience of an online scam : “Computers aren’t always honest like , because sometimes com - puters can lie . My friend said she got a prize online and she put all her information in , and she never got anything . ” While participants trusted computers over humans , they did acknowledge that they could learn to trust a human if given enough time to get to know them . For example , L3 suggested that a person has to “earn” her trust , which was not the case for computers . This process could take time : “I’m not saying computers are better , it just takes a while for me to trust someone . ” ( L3 ) . Learners did not explicitly make any connections to human creators underlying the technology . 4 . 3 Processes for Creating Fair AI Finally , when we explored the ways in which learners imagined futures of fair AI processes in the fnal project and discussions about algorithms , we saw that learners considered accountability as involving the developers , the users , and those with powerful plat - forms . They further diferentiated between designers vs . builders of AI technology , with most having a preference toward roles in designing . 4 . 3 . 1 Perceptions of Accountability . Learners displayed relatively homogeneous opinions on accountability in AI . Most or all agreed that the original creator of the technology should be mainly re - sponsible for fxing any problems and making the technology fair . Additionally , some learners agreed that powerful fgures repre - senting their community , specifcally “the mayor” ( L8 ) , should be responsible for making fair AI . Others brought up prominent po - litical fgures at a more national level , such as “Joe Biden” ( L3 ) or other leaders with responsibility . One learner proposed a popular celebrity , “Chris Pratt , ” because “people listen to celebrities” ( L5 ) . This suggests that the girls understood that infuencers and po - litical fgures have power , and that their power could be used for change . Two learners in the camp , L1 and L3 , suggested that the general public could play a role by engaging in civic action , such as protests , to push back against unfairness in AI . 4 . 3 . 2 Learners as Designers vs . Builders . The girls’ excitement to - ward using technology for fairness and justice was refected in the fnal projects , as the girls were scafolded in ideating an AI tech - nology that would support a cause they cared about . Most learners thought of AI that would help with solving challenges relevant to them . For instance , some examples include L6 , who designed an app called “Shy - Disqualify , ” which “makes shy people not shy anymore , ” encouraging personal development and improvement . Notably , L6 was quite shy herself , and a facilitator supported her at times in the hallway , as she was sometimes uncertain about being in the classroom with the other learners . L5 created a technology called the “Hypnotism Place of Kindness , ” which could hypnotize boys to make them kinder to girls , explaining the confict that she had personally experienced of boys being rude to girls in sports class . L7 proposed a machine shaped like an expandable bubble called “The Prisoner” that “traps people . . . and you can let them out if you feel like it . ” She explained that this machine would trap bad people but wouldn’t let them “worry , starve , or die alone . ” . L8 came up with a robot idea that helped with sanitizing , to help with preventing illness , potentially inspired by recent events with COVID - 19 . Not all learners walked away with a clear idea of what fairness in AI means , since in the fnal project , two learners ( L1 , L8 ) felt that they were not sure , and other learners came up with ideas that were similar to the initial defnitions from the AI Stories . However , this did not stop them from coming up with solutions for issues that they wanted to solve . And although their fnal project ideas were not necessarily always tied to AI , as L1 proposed a non - technical protest for wildlife conservation , agency to be a part of change was fully demonstrated in our session . When asked if they wanted to design AI , learners quickly and adamantly answered that they would . The girls wanted their ideas and feedback to be integrated into new and current AI , e . g . , for YouTube and FamilyBoost . Furthermore , they saw people similar to them , including their friends or people who they liked ( such as some adult facilitators ) , as also ft to be involved . However , learners often mentioned adults specifcally due to their perceived technical competency while not preferring to include them in making design decisions . For example , L3 stated ( and many agreed ) that websites like YouTube would have strong censorship on the content that they want to see if designed by adults in their lives , suggesting that “some of the videos are violent , and they might have knives and stuf , and if a parent or teacher are [ involved in design ] , it would be blocked . ” L3 further suggested , " Some of the stuf that [ programmers ] recommend might be boring or something . They might not know what we want . " Learners in the camp agreed that designers need to know user wants and needs , as they articulated the diferences between designers and builders without being prompted to do so . Four learners ( L5 , L6 , L4 , L2 ) diferentiated between designers and builders . The girls did not necessarily see themselves as builders , due to a self - described lack of technical background and perhaps a greater interest in the decision - making role of a designer . A moti - vation to be a part of technology creation was also shown in their interest in social good , with L5 stating , “I would like to design a new algorithm to support other people” in response to prompting CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan about the FamilyBoost algorithm . Most of the help they wanted in creating AI technology was related to building , proposing that some adults and people with technical backgrounds , including big companies , should be a part of actually building and fxing the tech - nology from a technical perspective . For example , L1 said , “People who make robots and all that can help with all that , ” and L5 added , “The technology people , the iPhone people . ” We saw that when learners took on a designer role , facets of their identity and values were refected in their AI ideas , adding representation in the technology they thought of . Specifcally , hair has been documented to be important to Black girls’ identity [ 75 ] , and we saw this show through in their designs . L4 and L8’s fnal projects were humanoid AI - powered robots ( Figure 2 ) , which dis - played hairstyles similar to the learners’ , a robot representation not often shown in popular media and current children’s material . Figure 2 : Images of L4 ( left ) and L8’s ( right ) fnal project featuring AI - powered robots . 5 DISCUSSION We now discuss how our fndings connect to prior literature on chil - dren’s perceptions of fairness in AI , as well as consider connections to Black feminist theory and critical consciousness in positioning learners in empowered roles in AI creation . Our fndings suggest design recommendations for future interventions and programs . 5 . 1 Children’s Moral Development and Their Perceptions of Fairness in AI Broadly speaking , our fndings are in line with existing work on children’s moral development . We saw kindness and niceness re - fected in our participants’ view of fairness , in line with the prior work specifcally focused on children of color [ 85 ] , alongside the “eye for an eye” philosophy [ 59 ] , a potentially contrasting idea com - pared to niceness but in line with the notion of equality as fairness [ 8 , 84 ] . In our examples , we found that girls were most easily able to identify unfair practices when they involved distributive fairness , or the unequal distribution of resources . Two of our examples clearly triggered this conceptual framing : the Public School Lottery and the FamilyBoost algorithm , which provoked strong feelings about fairness in the girls . The literature predicts that youth of this age would reject distributional inequality [ 100 ] , which we observed with our participants going so far as to endorse the removal of additional resources for families if equality can’t be achieved . This is also about the age when children begin to pay attention to how deserving recipients are of resources , and we found a strong en - dorsement from our participants for merit - based allocations — in particular a focus on hard work . A minority of participants also encouraged consideration of wealth and race as important features , which could have opened up a space to further explore with the group how algorithms account for identity and context . Beyond equality of outcome , procedural fairness , i . e . , fairness in process , was also discussed in relation to these examples . Because the outcomes involved limited resources ( e . g . , seats in particular schools ) , the learners were able to identify the unfairness and clearly discuss options for making the process fairer and , as prior litera - ture would predict [ 41 ] , preferred fair processes . In particular , they believed that algorithms would produce a fairer process than hav - ing humans in charge , who would be untrustworthy . This fnding connects to prior work on fairness showing that children had a stronger negative emotional reaction to unfair ofers from humans than to the same ofers from a computer [ 80 ] . Interestingly , we saw evidence that girls both did and did not internalize the idea that algorithms , and particularly AI systems , are created by human beings . On the one hand , girls were confdent that an algorithm would treat them more fairly than a potentially racially - biased human . This contrasts with prior work on adults’ views on computers vs . human decision - making [ 98 ] . They further did not seem concerned that systems could be created by biased humans , whose biases might be refected in the system . On the other hand , they clearly understood that infuential people were key to accountability in AI , e . g . , they knew that people were behind creating and supporting the AI systems , and they could imagine themselves in designer roles . How do we reconcile this diference ? We may understand this through the lens of niceness , which girls at this age see as a critical part of fairness . When an AI system treats Black girls unfairly , it is not doing so with interpersonal cruelty or intent to harm . This may be why girls felt that algorithms would be fairer to them even when they refected the biases of their creators . Our other examples of algorithms did not trigger the resource allocation framing and learners were correspondingly less defnitive about their unfairness . Instead , these examples connected more strongly to a moral judgment frame in which the morality of a behavior is based on outcomes rather than intentions - and they did not see the outcomes of the biased voice recognition algorithm or the YouTube recommendation algorithm as particularly harmful given their ability to design around them . Alternatively , as predicted by the theories of moral development , the girls were at an age to be more motivated by egocentric concerns for fairness , that is , how the consequences of biased technologies harmed them specifcally . These examples had an impact on their fairness perceptions when they touched the girls’ lives directly - it wasn’t sufcient for them to negatively afect their community or even their family . With this in mind , we recommend choosing examples of distri - butional and procedural fairness . Ideally , distributive justice may be posed as an introduction , where learners are willing to see clear unfairness , which can later be used to scafold procedural under - standing and application of fairness . When we ran our programs , we did the opposite , since distributive justice seemed more seri - ous and complex , but based on our fndings , we think that even a complex distributive justice example may be more accessible for girls’ learning . When building examples for youth of this age that “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany address moral judgments and not resources , fairness discussions will go deeper when they are connected to egocentric concerns . Further , we suggest addressing niceness critically . What if someone treated the learners fairly but was mean ? Versus someone who treated them unfairly but was nice on the surface ? “Niceness” is not just important in the context of AI fairness , it’s also a tactic used by White people to maintain the racial status quo [ 10 ] . 5 . 2 Critical Consciousness to Support AI Literacy Our fndings show that the learners were not consistently able to identify systemic racism embedded in AI . For example , when discussing a voice assistant , they proposed workarounds for a grand - mother whose voice was not recognized due to an accent rather than acknowledging the situation as unfair . We initially hypothe - sized that this might be due to egocentricity [ 58 ] , since the issue did not afect the girls directly . However , we saw similar issues arise with the question of autocorrect , which the girls themselves raised . Unlike the voice recognition example , the girls had personal experi - ence with the issue , and they cared about it a great deal . While this led to emotional engagement , it did not lead them to identify the systemic issue , which is that autocorrect is biased toward White Western names and often makes mistakes on others [ 27 ] . This fnding connects with Black feminist notions of critical consciousness . Critical consciousness refers to a person’s ability to recognize and critique systems of oppression around them , as well as how they are equipped to take action against those oppressive systems [ 34 ] . Critical consciousness does not come automatically . It must be cultivated through refection and analysis , for example , by discussing the lived experiences of Black women and connecting them to larger systemic forces . The girls in our study were able to identify the experiences that they were having , as well as the unfairness in the context of these experiences . However , connecting these examples to larger systemic injustices did not happen by itself without some support from facilitators . On the other hand , at least some girls used the creative portion of the workshop on generating their own AI ideas to engage critically with social injustice . For example , L5’s idea , “Hypnotism Place of Kindness” engages with gender dynamics , as it illuminates how AI could empower girls in a space where injustice exists in a sports context . L7’s “bubble” technology , which acts as a “jail” was de - scribed by her as having the potential to cause unfairness if it does “not let them out for eternity over something small . ” She regarded people being incarcerated for longer than the crime called for as a problematic issue . L7’s fnal project showed us that the learner has had exposure to the justice system and has strong enough opinions about it to make changes using technology . Based on this analysis , we suggest ways to support Black girls in building their critical consciousness . First , girls were able to gen - erate examples of unfairness more easily than they could identify unfairness in existing systems . Exploration of unfairness can start with generative activities and then show the parallels to other ex - amples that are best for teaching . Further , activities should scafold participants with language to foster their ability to refect on expe - riences ( their own , or stories presented about others ) [ 28 ] . Current work uses individual , internalized , and institutional racism as the major categories of analysis . These categories can be incorporated into the discussion of AI systems , both in terms of analyzing current injustices and defning mechanisms of accountability . Finally , a key element of supporting critical consciousness is positioning learners as change - makers in their environments [ 28 ] . We therefore turn to this aspect of our work next . 5 . 3 Change - making and STEM Leadership As the girls’ creative projects suggest , they see themselves as people who can change how AI works , including making it fairer and introducing accountability mechanisms . However , our fndings also show that girls did not primarily did not see themselves as the developers of such technology ( e . g . , doing hands - on coding ) . Instead , they saw themselves in designerly roles , where they were defning what the project should be and how it should work . We see this fnding not as evidence of girls’ disinterest in STEM but rather as evidence of emerging technosocial change agency [ 82 ] . Becoming a technosocial change agent is a broader approach that focuses on how to shape technology , rather than assuming any particular method will be efective in doing so . We distinguish it from more conventional ways of measuring engagement in STEM , e . g . , an emphasis on acquiring technical skills [ 35 ] . In this case , we believe that our participants recognized that the person who implements a technology is not necessarily the person making critical decisions about how that technology works . In reality , technical teams include product managers , designers , and others in leadership roles who defne the purpose of the system . This is how our Black girl participants saw themselves . They knew that technical knowledge was needed to construct future AI systems , and they envisioned having technical collaborators who would help them realize their visions ( e . g . , “people who make robots” ) . In other words , girls did not see themselves as needing to be hands - on workers with the technology in order to be tech leaders . We can see further evidence of girls’ agency in how they re - peatedly proposed workarounds for unfair technologies , such as voice assistants not recognizing an elder’s voice . Their workarounds made it hard for them to recognize the technology as unfair , but it also demonstrated that they saw themselves as people with agency and power in regard to technology . Their suggestions included purely technical ideas , such as changing the settings for the voice assistant but also recognized technologies as embedded in a social context where they could intervene ( e . g . , using their own voices to repeat their grandmother’s request ) . Historically , Black and ac - tivist communities have taken this type of remixing approach to technology , making do with what is available even when it is not constructed with their needs in mind [ 18 ] . The learners can be understood as participating in this tradition . Based on these fndings , we recommend that AI education pro - grams additionally emphasize the role of STEM leadership , rather than focusing narrowly on teaching girls to code . Black girls are already able to imagine themselves as designers , creators , and lead - ers of interdisciplinary teams . Rather than pushing them toward technical roles that are primarily about implementation , culturally responsive AI education programs can support learners by see - ing technical contributions as one way to be technosocial change agents related to AI - but not the only way . As part of this work , CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan we can emphasize the importance of creative roles in STEM , which are often coded as feminine and therefore devalued [ 15 ] . Choosing domains such as games or art can help with this , as can emphasizing the many ways to make contributions to AI systems beyond simply writing code . 5 . 4 Limitations We have several threats to validity in our study . While we found data saturation across the camp and workshop and within each program , our fndings center on participants’ personal opinions , which are not intended to generalize to Black girls in the U . S . as a whole . Additionally , due to the out - of - school time structure of the program , some of our data is incomplete , e . g . , as one participant left after the frst two sessions and one joined in the second and third sessions . This issue , in addition to the sample size , was exacerbated by the residual impacts and shutdowns caused by COVID - 19 . As is expected with a workshop - based method , it is possible that some of the facilitator prompting led the girls to focus on specifc issues , or that given the group setting , learners may have been afected by social compliance and agreed with one another more often than they would in an individual setting . These limitations are counterbalanced by the value that a workshop - based method brings in increasing participants’ agency and voices . 6 CONCLUSION In this work , we ran workshops to understand Black girls’ per - ceptions of and ideas for fair AI . We saw that while sometimes their concepts of fairness to AI were emergent and followed de - velopmental expectations for their age group , they brought their lived experiences to the discussion and were already engaged with themes of social justice . They envisioned themselves in powerful designer roles for future AI and identifed authorities who should be responsible for creating the fair AI of today . We believe that these fndings and recommendations will help support the creation of learner - centered AI literacy for Black girls , an underrepresented group in AI development . ACKNOWLEDGMENTS We thank the learners who participated in this study , as well as the school that we partnered with and the supportive staf . This work is funded by NSF DRL - 1811086 and DRL - 1935801 . REFERENCES [ 1 ] Safnah Ali , Blakeley H Payne , Randi Williams , Hae Won Park , and Cynthia Breazeal . 2019 . Constructionism , ethics , and creativity : Developing primary and middle school artifcial intelligence education . In International workshop on education in artifcial intelligence k - 12 ( eduai’19 ) . 1 – 4 . [ 2 ] Louise Amoore . 2020 . Why ‘Ditch the algorithm’is the future of political protest . The Guardian 19 ( 2020 ) . [ 3 ] Joshua Asplund , Motahhare Eslami , Hari Sundaram , Christian Sandvig , and Karrie Karahalios . 2020 . Auditing race and gender discrimination in online housing markets . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 14 . 24 – 35 . [ 4 ] Computer Science Teachers Association . 2017 . CS Standards . Standards ( 2017 ) . [ 5 ] Joan Palmiter Bajorek . 2019 . Voice recognition still has signifcant race and gender biases . Harvard Business Review 10 ( 2019 ) . [ 6 ] Nicolas Baumard , Olivier Mascaro , and Coralie Chevallier . 2012 . Preschoolers are able to take merit into account when distributing goods . Developmental psychology 48 , 2 ( 2012 ) , 492 . [ 7 ] Hugh Beyer and Karen Holtzblatt . 1999 . Contextual design . interactions 6 , 1 ( 1999 ) , 32 – 42 . [ 8 ] Peter R Blake and Katherine McAulife . 2011 . “I had so much it didn’t seem fair” : Eight - year - olds reject two forms of inequity . Cognition 120 , 2 ( 2011 ) , 215 – 224 . [ 9 ] Peter R Blake , Katherine McAulife , and Felix Warneken . 2014 . The develop - mental origins of fairness : The knowledge – behavior gap . Trends in cognitive sciences 18 , 11 ( 2014 ) , 559 – 561 . [ 10 ] Eduardo Bonilla - Silva . 2006 . Racism without racists : Color - blind racism and the persistence of racial inequality in the United States . Rowman & Littlefeld Publishers . [ 11 ] Engin Bozdag . 2013 . Bias in algorithmic fltering and personalization . Ethics and information technology 15 , 3 ( 2013 ) , 209 – 227 . [ 12 ] Joy Buolamwini and Timnit Gebru . 2018 . Gender shades : Intersectional accu - racy disparities in commercial gender classifcation . In Conference on fairness , accountability and transparency . PMLR , 77 – 91 . [ 13 ] Sylvia Celedón - Pattichis , Lunney Lisa Borden , Stephen J Pape , Douglas H Clements , Susan A Peters , Joshua R Males , Olive Chapman , and Jacqueline Leonard . 2018 . Asset - based approaches to equitable mathematics education research and practice . Journal for Research in Mathematics Education 49 , 4 ( 2018 ) , 373 – 389 . [ 14 ] Alexandra Chouldechova , Diana Benavides - Prado , Oleksandr Fialko , and Rhema Vaithianathan . 2018 . A case study of algorithm - assisted decision making in child maltreatment hotline screening decisions . In Conference on Fairness , Ac - countability and Transparency . PMLR , 134 – 148 . [ 15 ] Jazmine Christen . 2018 . Devaluation of Feminized Work . ( 2018 ) . [ 16 ] Sean T Coleman and Julius Davis . 2020 . Using asset - based pedagogy to facilitate STEM learning , engagement , and motivation for Black middle school boys . Journal of African American Males in Education ( JAAME ) 11 , 2 ( 2020 ) , 76 – 94 . [ 17 ] Juliet M Corbin and Anselm Strauss . 1990 . Grounded theory research : Proce - dures , canons , and evaluative criteria . Qualitative sociology 13 , 1 ( 1990 ) , 3 – 21 . [ 18 ] Sasha Costanza - Chock . 2018 . Design justice : Towards an intersectional feminist framework for design theory and practice . Proceedings of the Design Research Society ( 2018 ) . [ 19 ] Tim Dare and Eileen Gambrill . 2017 . Ethical analysis : Predictive risk models at call screening for Allegheny County . Alleghany County Analytics ( 2017 ) . [ 20 ] Jefrey Dastin . 2018 . Amazon scraps secret AI recruiting tool that showed bias against women . In Ethics of Data and Analytics . Auerbach Publications , 296 – 299 . [ 21 ] Daniella DiPaola , Blakeley H Payne , and Cynthia Breazeal . 2020 . Decoding de - sign agendas : an ethical design activity for middle school students . In Proceedings of the Interaction Design and Children Conference . 1 – 10 . [ 22 ] Stefania Druga , Sarah T Vu , Eesh Likhith , and Tammy Qiu . 2019 . Inclusive AI literacy for kids around the world . In Proceedings of FabLearn 2019 . 104 – 111 . [ 23 ] Stefania Druga , Randi Williams , Cynthia Breazeal , and Mitchel Resnick . 2017 . " Hey Google is it OK if I eat you ? " : Initial Explorations in Child - Agent Interaction . Proceedings of the 2017 Conference on Interaction Design and Children ( 2017 ) . [ 24 ] Stefania Druga , Randi Williams , Hae Won Park , and Cynthia Breazeal . 2018 . How smart are the smart toys ? : children and parents’ agent interaction and intelligence attribution . Proceedings of the 17th ACM Conference on Interaction Design and Children ( 2018 ) . [ 25 ] Yarrow Dunham , Allison Durkin , and Tom R Tyler . 2018 . The development of a preference for procedural justice for self and others . Scientifc reports 8 , 1 ( 2018 ) , 1 – 8 . [ 26 ] Cynthia Dwork , Moritz Hardt , Toniann Pitassi , Omer Reingold , and Richard Zemel . 2012 . Fairness through awareness . In Proceedings of the 3rd innovations in theoretical computer science conference . 214 – 226 . [ 27 ] Rashmi Dyal - Chand . 2021 . Autocorrecting for Whiteness . BUL Rev . 101 ( 2021 ) , 191 . [ 28 ] Aaliyah El - Amin , Scott Seider , Daren Graves , Jalene Tamerat , Shelby Clark , Madora Soutter , Jamie Johannsen , and Saira Malhotra . 2017 . Critical conscious - ness : A key to student achievement . Phi Delta Kappan 98 , 5 ( 2017 ) , 18 – 23 . [ 29 ] Motahhare Eslami , Kristen Vaccaro , Karrie Karahalios , and Kevin Hamilton . 2017 . “Be careful ; things can be worse than they appear” : Understanding Biased Algorithms and Users’ Behavior around Them in Rating Platforms . In Proceedings of the international AAAI conference on web and social media , Vol . 11 . 62 – 71 . [ 30 ] Virginia Eubanks . 2018 . Automating inequality : How high - tech tools profle , police , and punish the poor . St . Martin’s Press . [ 31 ] Jayne Everson , F Megumi Kivuva , and Amy J Ko . 2022 . " A Key to Reducing Inequities in Like , AI , is by Reducing Inequities Everywhere First " Emerging Critical Consciousness in a Co - Constructed Secondary CS Classroom . In Pro - ceedings of the 53rd ACM Technical Symposium on Computer Science Education V . 1 . 209 – 215 . [ 32 ] Jodi Forlizzi and Carl F . DiSalvo . 2006 . Service robots in the domestic environ - ment : a study of the roomba vacuum in the home . In HRI ’06 . [ 33 ] World Economic Forum . 2020 . Global gender gap report 2020 . Insight Report ( 2020 ) . [ 34 ] Paulo Freire . 2021 . Education for critical consciousness . Bloomsbury Publishing . [ 35 ] Francisco J García - Peñalvo , Angela Marie Rees , Jenny Hughes , Ilkka Jor - manainen , Tapani Toivonen , and Jens Vermeersch . 2016 . A survey of resources for introducing coding into schools . In Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality . 19 – 26 . “I Would Like to Design” : Black Girls Analyzing and Ideating Fair and Accountable AI CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany [ 36 ] Lisa Garoutte and Kate McCarthy - Gilmore . 2014 . Preparing students for community - based learning using an asset - based approach . Journal of the Schol - arship of Teaching and Learning ( 2014 ) , 48 – 61 . [ 37 ] Geneva Gay . 2002 . Preparing for culturally responsive teaching . Journal of teacher education 53 , 2 ( 2002 ) , 106 – 116 . [ 38 ] Geneva Gay . 2018 . Culturally responsive teaching : Theory , research , and practice . teachers college press . [ 39 ] Nina Grgic - Hlaca , Elissa M Redmiles , Krishna P Gummadi , and Adrian Weller . 2018 . Human perceptions of fairness in algorithmic decision making : A case study of criminal risk prediction . In Proceedings of the 2018 World Wide Web Conference . 903 – 912 . [ 40 ] Nina Grgić - Hlača , Muhammad Bilal Zafar , Krishna P Gummadi , and Adrian Weller . 2018 . Beyond distributive fairness in algorithmic decision making : Feature selection for procedurally fair learning . In Proceedings of the AAAI Conference on Artifcial Intelligence , Vol . 32 . [ 41 ] Patricia Grocke , Federico Rossano , and Michael Tomasello . 2015 . Procedural justice in children : Preschoolers accept unequal resource distributions if the procedure provides equal opportunities . Journal of experimental child psychology 140 ( 2015 ) , 197 – 210 . [ 42 ] Ben Guarino . 2016 . Google faulted for racial bias in image search results for black teenagers . Washington Post 6 ( 2016 ) , 2016 . [ 43 ] David Hammer and Leema K Berland . 2014 . Confusing claims for data : A critique of common practices for presenting qualitative research on learning . Journal of the Learning Sciences 23 , 1 ( 2014 ) , 37 – 46 . [ 44 ] Moritz Hardt , Eric Price , and Nati Srebro . 2016 . Equality of opportunity in supervised learning . Advances in neural information processing systems 29 ( 2016 ) . [ 45 ] Christina Harrington , Sheena Erete , and Anne Marie Piper . 2019 . Deconstructing community - based collaborative design : Towards more equitable participatory design engagements . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 25 . [ 46 ] Tom Hitron , Yoav Orlev , Iddo Wald , Ariel Shamir , Hadas Erel , and Oren Zuck - erman . 2019 . Can children understand machine learning concepts ? The efect of uncovering black boxes . In Proceedings of the 2019 CHI conference on human factors in computing systems . 1 – 11 . [ 47 ] Tom Hitron , Iddo Wald , Hadas Erel , and Oren Zuckerman . 2018 . Introducing chil - dren to machine learning concepts through hands - on experience . In Proceedings of the 17th ACM conference on interaction design and children . 563 – 568 . [ 48 ] Andrew Gary Darwin Holmes . 2020 . Researcher Positionality – A Consideration of Its Infuence and Place in Qualitative Research – A New Researcher Guide . Shanlax International Journal of Education 8 , 4 ( 2020 ) , 1 – 10 . [ 49 ] Kenneth Holstein , Bruce M McLaren , and Vincent Aleven . 2018 . Student learning benefts of a mixed - reality teacher awareness tool in AI - enhanced classrooms . In International conference on artifcial intelligence in education . Springer , 154 – 168 . [ 50 ] Kenneth Holstein , Jennifer Wortman Vaughan , Hal Daumé III , Miro Dudik , and Hanna Wallach . 2019 . Improving fairness in machine learning systems : What do industry practitioners need ? . In Proceedings of the 2019 CHI conference on human factors in computing systems . 1 – 16 . [ 51 ] Elizabeth Huppert , Jason M Cowell , Yawei Cheng , Carlos Contreras - Ibáñez , Na - talia Gomez - Sicard , Maria Luz Gonzalez - Gadea , David Huepe , Agustin Ibanez , Kang Lee , Randa Mahasneh , et al . 2019 . The development of children’s prefer - ences for equality and equity across 13 individualistic and collectivist cultures . Developmental science 22 , 2 ( 2019 ) , e12729 . [ 52 ] Jillian J Jordan , Katherine McAulife , and Felix Warneken . 2014 . Development of in - group favoritism in children’s third - party punishment of selfshness . Pro - ceedings of the National Academy of Sciences 111 , 35 ( 2014 ) , 12710 – 12715 . [ 53 ] N Joseph and Jakita O Thomas . 2020 . Designing STEM Learning Environments to Support Middle School Black Girls’ Computational Algorithmic Thinking : A Possibility Model for Disrupting STEM Neoliberal Projects . In International Conference of the Learning Sciences . [ 54 ] K Megasari Kahn , Rani Megasari , Erna Piantari , and Enjun Junaeti . 2018 . AI programming by children using snap ! block programming in a developing country . ( 2018 ) . [ 55 ] Patricia Kanngiesser and Felix Warneken . 2012 . Young children consider merit when sharing resources with others . ( 2012 ) . [ 56 ] Maria Kasinidou , Styliani Kleanthous , Kalia Orphanou , and Jahna Otterbacher . 2021 . Educating Computer Science Students about Algorithmic Fairness , Ac - countability , Transparency and Ethics . In Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V . 1 . 484 – 490 . [ 57 ] Styliani Kleanthous , Maria Kasinidou , Pınar Barlas , and Jahna Otterbacher . 2022 . Perception of fairness in algorithmic decisions : Future developers’ perspective . Patterns 3 , 1 ( 2022 ) , 100380 . [ 58 ] Lawrence Kohlberg . 1958 . The development of modes of moral thinking and choice in the years 10 to 16 . Ph . D . Dissertation . The University of Chicago . [ 59 ] Lawrence Kohlberg . 1974 . Education , moral development and faith . Journal of Moral Education 4 , 1 ( 1974 ) , 5 – 16 . [ 60 ] Matt J Kusner , Joshua Loftus , Chris Russell , and Ricardo Silva . 2017 . Counter - factual fairness . Advances in neural information processing systems 30 ( 2017 ) . [ 61 ] Gloria Ladson - Billings . 1995 . Toward a theory of culturally relevant pedagogy . American educational research journal 32 , 3 ( 1995 ) , 465 – 491 . [ 62 ] Irene Lee , Safnah Ali , Helen Zhang , Daniella DiPaola , and Cynthia Breazeal . 2021 . Developing Middle School Students’ AI Literacy . In Proceedings of the 52nd ACM Technical Symposium on Computer Science Education . 191 – 197 . [ 63 ] Lanna Lima , Vasco Furtado , Elizabeth Furtado , and Virgilio Almeida . 2019 . Empirical analysis of bias in voice - based personal assistants . In Companion Proceedings of The 2019 World Wide Web Conference . 533 – 538 . [ 64 ] Duri Long and Brian Magerko . 2020 . What is AI literacy ? Competencies and design considerations . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 16 . [ 65 ] Duri Long and Brian Magerko . 2020 . What is AI Literacy ? Competencies and Design Considerations . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 16 . https : / / doi . org / 10 . 1145 / 3313831 . 3376727 [ 66 ] Francesca López . 2016 . Culturally responsive pedagogies in Arizona and Latino students’ achievement . Teachers College Record 118 , 5 ( 2016 ) , 1 – 42 . [ 67 ] Frank Marcinkowski , Kimon Kieslich , Christopher Starke , and Marco Lünich . 2020 . Implications of AI ( un - ) fairness in higher education admissions : the efects of perceived AI ( un - ) fairness on exit , voice and organizational reputation . In Proceedings of the 2020 conference on fairness , accountability , and transparency . 122 – 130 . [ 68 ] Katherine McAulife , Peter R Blake , Grace Kim , Richard W Wrangham , and Felix Warneken . 2013 . Social infuences on inequity aversion in children . PloS one 8 , 12 ( 2013 ) , e80966 . [ 69 ] Alexander Monea . 2019 . Race and computer vision . ( 2019 ) . [ 70 ] Rolf E Muuss . 1988 . Carol Gilligan’s theory of sex diferences in the development of moral reasoning during adolescence . Adolescence 23 , 89 ( 1988 ) , 229 . [ 71 ] Ziad Obermeyer , Brian Powers , Christine Vogeli , and Sendhil Mullainathan . 2019 . Dissecting racial bias in an algorithm used to manage the health of populations . Science 366 , 6464 ( 2019 ) , 447 – 453 . [ 72 ] Renata M O’Donnell . 2019 . Challenging racist predictive policing algorithms under the equal protection clause . NYUL Rev . 94 ( 2019 ) , 544 . [ 73 ] Ihudiya Finda Ogbonnaya - Ogburu , Angela DR Smith , Alexandra To , and Kentaro Toyama . 2020 . Critical race theory for HCI . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 16 . [ 74 ] Markus Paulus . 2014 . The early origins of human charity : Developmental changes in preschoolers’ sharing with poor and wealthy individuals . Frontiers in Psychology 5 ( 2014 ) , 344 . [ 75 ] Robin J Phelps - Ward and Crystal T Laura . 2016 . Talking back in cyberspace : self - love , hair care , and counter narratives in Black adolescent girls’ YouTube vlogs . Gender and Education 28 , 6 ( 2016 ) , 807 – 820 . [ 76 ] Jean Piaget . 1965 . The stages of the intellectual development of the child . Educational psychology in context : Readings for future teachers 63 , 4 ( 1965 ) , 98 – 106 . [ 77 ] Yim Register and Amy J Ko . 2020 . Learning machine learning with personal data helps stakeholders ground advocacy arguments in model mechanics . In Proceedings of the 2020 ACM Conference on International Computing Education Research . 67 – 78 . [ 78 ] Michael T Rizzo , Laura Elenbaas , Shelby Cooley , and Melanie Killen . 2016 . Children’s recognition of fairness and others’ welfare in a resource allocation task : Age related changes . Developmental psychology 52 , 8 ( 2016 ) , 1307 . [ 79 ] Michael S Rosenwald . 2017 . How millions of kids are being shaped by know - it - all voice assistants . Washington Post 2 ( 2017 ) . [ 80 ] Alan G Sanfey , James K Rilling , Jessica A Aronson , Leigh E Nystrom , and Jonathan D Cohen . 2003 . The neural basis of economic decision - making in the ultimatum game . Science 300 , 5626 ( 2003 ) , 1755 – 1758 . [ 81 ] Marco FH Schmidt , Margarita Svetlova , Jana Johe , and Michael Tomasello . 2016 . Children’s developing understanding of legitimate reasons for allocating re - sources unequally . Cognitive Development 37 ( 2016 ) , 42 – 52 . [ 82 ] Kimberly A Scott , Kimberly M Sheridan , and Kevin Clark . 2015 . Culturally responsive computing : A theory revisited . Learning , Media and Technology 40 , 4 ( 2015 ) , 412 – 436 . [ 83 ] Kimberly A Scott and Mary Aleta White . 2013 . COMPUGIRLS’standpoint : Culturally responsive computing and its efect on girls of color . Urban Education 48 , 5 ( 2013 ) , 657 – 681 . [ 84 ] Alex Shaw and Kristina R Olson . 2012 . Children discard a resource to avoid inequity . Journal of Experimental Psychology : General 141 , 2 ( 2012 ) , 382 . [ 85 ] Zoe Skinner , Stacey Brown , and Greg Walsh . 2020 . Children of Color’s Percep - tions of Fairness in AI : An Exploration of Equitable and Inclusive Co - Design . In Extended Abstracts of the 2020 CHI Conference on Human Factors in Com - puting Systems ( Honolulu , HI , USA ) ( CHI EA ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 8 . https : / / doi . org / 10 . 1145 / 3334480 . 3382901 [ 86 ] Jaemarie Solyst , Alexis Axon , Angela E . B . Stewart , Motahhare Eslami , and Amy Ogan . 2022 . Investigating Girls’ Perspectives and Knowledge Gaps on Ethics and Fairness in Artifcial Intelligence in a Lightweight Workshop . Proceedings of the 16th International Society of the Learning Sciences ( ICLS ) ( 2022 ) , 807 – 814 . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Jaemarie Solyst , Shixian Xie , Ellia Yang , Angela E . B . Stewart , Motahhare Eslami , Jessica Hammer , and Amy Ogan [ 87 ] Jessica A Sommerville . 2018 . Infants’ understanding of distributive fairness as a test case for identifying the extents and limits of infants’ sociomoral cognition and behavior . Child Development Perspectives 12 , 3 ( 2018 ) , 141 – 145 . [ 88 ] Konstantinos Stathoulopoulos and Juan C Mateos - Garcia . 2019 . Gender diversity in AI research . Available at SSRN 3428240 ( 2019 ) . [ 89 ] Ulrika Sultan , Cecilia Axell , and Jonas Hallström . 2018 . Girls’ engagement in technology education : A systematic review of the literature . In 36th International Pupils’ Attitudes Towards Technology Conference Athlone Institute of Technology , Co . Westmeath , Ireland , 18th – 21st June 2018 . Technology Education Research Group , 231 – 238 . [ 90 ] Robert H Tai , Christine Qi Liu , Adam V Maltese , and Xitao Fan . 2006 . Planning early for careers in science . Science 312 , 5777 ( 2006 ) , 1143 – 1144 . [ 91 ] David Touretzky , Christina Gardner - McCune , Fred Martin , and Deborah See - horn . 2019 . Envisioning AI for K - 12 : What should every child know about AI ? . In Proceedings of the AAAI Conference on Artifcial Intelligence , Vol . 33 . 9795 – 9799 . [ 92 ] Rhema Vaithianathan , Emily Putnam - Hornstein , Nan Jiang , Parma Nand , and Tim Maloney . 2017 . Developing predictive models to support child maltreatment hotline screening decisions : Allegheny County methodology and implementa - tion . Center for Social data Analytics ( 2017 ) . [ 93 ] Mike Van Duuren , Barbara Dossett , and Dawn Robinson . 1998 . Gauging chil - dren’s understanding of artifcially intelligent objects : a presentation of “counter - factuals” . International Journal of Behavioral Development 22 , 4 ( 1998 ) , 871 – 889 . [ 94 ] Greg Walsh and Eric Wronsky . 2019 . AI + co - design : developing a novel computer - supported approach to inclusive design . In Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing . 408 – 412 . [ 95 ] Ruotong Wang , F Maxwell Harper , and Haiyi Zhu . 2020 . Factors infuencing perceived fairness in algorithmic decision - making : Algorithm outcomes , devel - opment procedures , and individual diferences . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 96 ] Randi Williams , Christian Vázquez Machado , Stefania Druga , Cynthia Breazeal , and Pattie Maes . 2018 . " My Doll Says It’s Ok " : A Study of Children’s Conformity to a Talking Doll . In Proceedings of the 17th ACM Conference on Interaction Design and Children ( Trondheim , Norway ) ( IDC ’18 ) . Association for Computing Machinery , New York , NY , USA , 625 – 631 . https : / / doi . org / 10 . 1145 / 3202185 . 3210788 [ 97 ] Randi Williams , Hae Won Park , Lauren Oh , and Cynthia Breazeal . 2019 . Popbots : Designing an artifcial intelligence curriculum for early childhood education . In Proceedings of the AAAI Conference on Artifcial Intelligence , Vol . 33 . 9729 – 9736 . [ 98 ] Allison Woodruf , Sarah E Fox , Steven Rousso - Schindler , and Jefrey Warshaw . 2018 . A qualitative exploration of perceptions of algorithmic fairness . In Pro - ceedings of the 2018 chi conference on human factors in computing systems . 1 – 14 . [ 99 ] Brian Wynne . 1991 . Knowledges in context . Science , Technology , & Human Values 16 , 1 ( 1991 ) , 111 – 121 . [ 100 ] Tan Fee Yean et al . 2016 . Organizational justice : A conceptual discussion . Procedia - Social and Behavioral Sciences 219 ( 2016 ) , 798 – 803 . [ 101 ] Abigail Zimmermann - Niefeld , Makenna Turner , Bridget Murphy , Shaun K Kane , and R Benjamin Shapiro . 2019 . Youth learning machine learning through building models of athletic moves . In Proceedings of the 18th ACM International Conference on Interaction Design and Children . 121 – 132 .