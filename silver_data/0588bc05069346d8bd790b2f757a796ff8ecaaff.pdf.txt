29 How to Support Users in Understanding Intelligent Systems ? An Analysis and Conceptual Framework of User Questions Considering User Mindsets , Involvement , and Knowledge Outcomes DANIEL BUSCHEK , Department of Computer Science , University of Bayreuth , Germany MALIN EIBAND and HEINRICH HUSSMANN , LMU Munich , Germany The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human - computer interaction . Research in the field therefore highlights the need for trans - parency , scrutability , intelligibility , interpretability and explainability , among others . While all of these terms carry a vision of supporting users in understanding intelligent systems , the underlying notions and assump - tions about users and their interaction with the system often remain unclear . We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets , user involvement , and knowledge outcomes to reveal , differentiate and classify current notions in prior work . This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work . We further discuss related aspects such as stakeholders and trust , and also provide material to apply our framework in practice ( e . g . , ideation / design sessions ) . We thus hope to advance and structure the dialogue on supporting users in understanding intelligent systems . CCS Concepts : • Human - centered computing → HCI theory , concepts and models ; Additional Key Words and Phrases : Review , intelligent systems , scrutability , interpretability , transparency , explainability , intelligibility , accountability , interactive machine learning , end - user debugging ACM Reference format : Daniel Buschek , Malin Eiband , and Heinrich Hussmann . 2022 . How to Support Users in Understanding In - telligent Systems ? An Analysis and Conceptual Framework of User Questions Considering User Mindsets , Involvement , and Knowledge Outcomes . ACM Trans . Interact . Intell . Syst . 12 , 4 , Article 29 ( November 2022 ) , 27 pages . https : / / doi . org / 10 . 1145 / 3519264 1 INTRODUCTION Interactive intelligent systems violate core interface design principles such as predictable output and easy error correction [ 6 , 34 ] . This makes them hard to design , understand , and use [ 119 ] —an This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation ( bidt ) . Authors’ addresses : D . Buschek , Department of Computer Science , University of Bayreuth , Universitätsstraße 30 , Bayreuth , 95447 , Germany ; email : daniel . buschek @ uni - bayreuth . de ; M . Eiband and H . Hussmann , LMU Munich , Frauenlobstraße 7a , Munich , Germany , 80337 ; emails : { malin . eiband , hussmann } @ ifi . lmu . de . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2022 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . 2160 - 6455 / 2022 / 11 - ART29 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3519264 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 2 D . Buschek et al . Fig . 1 . Our framework for structuring the discussion of how to support users in understanding intelligent sys - tems : We examine user questions in the literature ( left , examples ) to synthesise three categories overarching prior work ( centre ) , namely assumed user mindsets , user involvement , and knowledge outcomes . We discuss di - vergent instances of each category ( right ) to differentiate approaches and solution principles in the literature . observation that has already been made decades earlier [ 54 ] , but it is only in the last years that ma - chine learning has increasingly penetrated everyday applications and thus refuelled the discussion on how we want interaction with such systems to be shaped . One particularly challenging property of intelligent systems is their opaqueness . As a result , re - searchers [ 3 , 10 , 48 ] , practitioners [ 23 ] , policy - makers [ 91 ] and the general public [ 68 ] increasingly call for intelligent systems to be transparent [ 39 ] , scrutable [ 63 ] , explainable [ 94 ] , intelligibile [ 78 ] and interactive [ 34 ] , among others , which we will henceforth refer to as system qualities . Work on the system qualities follows a joint and urgent maxim : Designing interaction in a way that sup - ports users in understanding and dealing with intelligent systems despite their often complex and black - box nature . Linked by this shared goal , the diverse terms are often employed interchangeably – and yet , prior work implies divergent assumptions about how users may best be supported . For instance , work on interpretability has recently been criticised for unclear use of the term [ 32 , 82 ] , a survey on explain - ability in recommenders found incompatible existing taxonomies [ 88 ] , and discussions about sys - tem transparency and accountability revealed diverging assumptions ( i . e . disclosing source code vs system auditing through experts ) [ 35 ] . A recent HCI survey shows the fractured terminological landscape in the field [ 1 ] . In particular , for supporting user understanding of intelligent systems , clarifying concepts , and connecting diverse approaches is crucial to advance scholarship , as pointed out in a recent “roadmap” towards a rigorous science of interpretability [ 32 ] . More generally speaking , a lack of conceptual clarity impedes scientific thinking [ 55 ] and presents challenging problems for re - searchers in the respective field : First , a lack of overarching conceptual frameworks renders new ideas difficult to develop and discuss in a structured way . Second , blurred terminological bound - aries impede awareness of existing work , for example , through varying use of keywords . Third , new prototypes often remain disconnected from the existing body of design solutions . To address this , we need a clearer conceptual understanding of the assumptions that underlie how prior work envisions to foster user understanding of intelligent systems . In this article , we thus aim to answer the following research questions : RQ1 : Which assumptions about users and interaction with intelligent systems do researchers make when referring to the system qualities ? RQ2 : How can we structure and differentiate these assumptions ? 2 CONTRIBUTION AND SUMMARY We analyse both theoretical concepts and prototype solutions through the lens of implied user questions and synthesise a conceptual framework integrating user mindsets , user involvement , and knowledge outcomes to reveal , differentiate , and classify notions of supporting user understanding of intelligent systems in prior work . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 3 Our analysis revealed three categories that capture and differentiate current assumptions about users and interaction with intelligent systems from an HCI perspective ( also see Figure 1 ) : ( 1 ) User Mindsets —What users seek to know ( e . g . , do they want to know why the system did X , what it was developed for , how trustworthy it is , etc . ) , ( 2 ) User Involvement — How users gain knowledge ( e . g . , do they actively inquire into the system or do they passively obtain information presented by the system ) , and ( 3 ) Knowledge Outcomes What kind of knowledge users gain ( e . g . , about a specific output or how to correct errors ) . In particular , as we will describe later in more detail , we argue that these three categories are linked to users’ intentions when using a system , influence the direction of information transfer between user and system , and reflect the envisioned outcome of the system qualities , respectively . Our view helps to resolve conceptual ambiguity in the field and provides researchers with a framework to clarify their assumptions and become aware of those made in prior work . We also provide material – a set of cards and activities – to translate the framework to ( design ) practice . We thus hope to advance and structure the dialogue in the HCI community on supporting users in understanding intelligent systems . 3 SCOPE AND FOUNDATIONS Before we present the results of our analysis in detail , we first discuss fundamental conceptual prerequisites . Note that this is neither intended as an overview of related work , nor as a detailed conceptual treatment of these aspects , which is beyond the scope of this article . We only give a brief characterisation to locate our perspective for our analysis here . 3 . 1 Intelligent Systems Our work focuses on interaction with intelligent systems . Following Singh [ 103 ] , a system is in - telligent if we need to “attribute cognitive concepts such as intentions and beliefs to it in order to characterize , understand , analyze , or predict its behavior” . While we are aware of the fact that many other definitions of intelligent systems exist , Singh’s definition hints at the potential com - plexity of intelligent systems and the resulting challenges for users to understand them , and thus motivates work on supporting users in doing so , including this article . It also takes a perspective that hints at people’s perception of a system . While this seems not ideal as a general definition , it aligns with our focus on the users’ perceptions of such systems in this article , as opposed to at - tempting an ( often difficult and contestable ) objective judgement of the technically implemented level of “intelligence” . We return to this in the discussion . 3 . 2 A Pragmatic View on Supporting User Understanding One can identify two opposing perspectives in the larger discussion of supporting users in under - standing intelligent systems : A normative and a pragmatic one [ 38 ] . The normative perspective is visible in the ethical discourse about intelligent systems or re - flected in legislation . It provides users with what has been called a “right to explanation” [ 46 ] , such as recently articulated in the GDPR [ 91 ] , and ties lawful use of intelligent systems to the abil - ity to make users understand their decision - making process [ 51 ] . While highly valuable for a legal basis for interaction with intelligent systems , this perspective stems from ethical and moral reflec - tion , not from user needs for interaction with a concrete system . As such , it often lacks specifics on how to implement its claims in a way that benefits users in practice . In this article , we therefore adopt the pragmatic perspective , which strives to best support users during interaction with intelligent systems . We define the purpose of this support by transferring ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 4 D . Buschek et al . a statement by Lynham from philosophy of science [ 83 ] to the context of our work : Supporting users in understanding intelligent systems means helping people to use a system better and in more informed ways , and to better ends and outcomes . We argue that this perspective captures well and articulates a core assumption of work on the system qualities : We as HCI researchers in the field of intelligent systems strive to create interfaces and interactions that are explainable , understandable , scrutable , transparent , accountable , intelli - gible , and so on , precisely because we envision users to then interact with these systems in more informed , effective , and efficient ways . 3 . 3 User Knowledge and Understanding In general , HCI has widely adopted mental models [ 61 ] as representations of the knowledge users possess about a system [ 87 ] , and this is no different in work on the system qualities ( e . g . , [ 38 , 70 , 111 ] ) . Mental models originate from a constructivist perspective on knowledge , where knowledge is seen as individually constructed , subjective interpretations of the world , based on previous experiences and assumptions [ 114 ] . While we do not mean to exclude other perspectives in HCI , in this article , we follow the described view for our framework . We also use knowledge in - terchangeably with understanding here . Moreover , we assume that knowledge is gained through the transmission of information between user and system . 4 METHOD Here , we shortly describe the process of paper collection and interpretation through which we derived our framework . 4 . 1 Theoretical Sampling Our paper set was collected using theoretical sampling , an approach to collection of qualitative data introduced by Glaser and Strauss as a part of Grounded Theory [ 43 ] . In contrast to statistical sampling , where the sample size is fixed a priori , theoretical sampling gradually defines the sample during the interpretation process until theoretical saturation is reached ( i . e . , the point where further data and interpretation does not further enrich the emerging categories ) . We started our sampling by looking for a set of the most widely adopted system qualities in the field . We did so first through collecting search terms for system qualities based on our experiences as researchers working at the intersection of HCI and AI for several years and then expanded our set of system qualities through the topic networks presented by Abdul et al . [ 1 ] in their 2018 survey of over 12 , 000 papers at this intersection . Their analysis surfaced many system qualities that we sought to address a priori ( e . g . , interpretability , scrutability , explainability ) , but also related topics ( e . g . , accountability and different types of transparency ) . With keyword searches on the ACM Digital Library , we then iteratively collected papers on these terms , following the above sampling method . We started out with the most cited papers in the field , which we interpreted as described in Section 4 . 2 to create first categories . Papers were selected to represent the diversity of approaches in the field , but also according to the following criteria : ( 1 ) The presented contribution focuses on the system qualities and is linked to intelligent sys - tems , and ( 2 ) The contribution involves an HCI perspective ( e . g . , via a prototype , user study , design guide - lines , etc . ) . Papers were re - visited if necessary , and we integrated new data through snowball searches as well as through updated keyword searches when we extended our system qualities set . Overall , 222 papers contributed to this process before we considered our categories to be theoretically ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 5 Table 1 . The system qualities in Focus of our Work Through the Lens of our Framework , Along with Example user Questions and Implied user Mindsets , user Involvement , and Knowledge Outcomes Systemquality Userquestions ( examples ) Usermindsets Userinvolvement Knowledgeoutcomesandqualities Accountability How fair and controllable is thesystem ? [ 94 ] Howfairare system decisions ? [ 15 ] How will algorithmic - decision making impact my ( social ) work practices ? [ 19 ] How can I as a designer / developer of decision systems support human values ? [ 53 , 112 ] Users critique the system , often in a wider context beyond use ( e . g . , legal , ethical concerns ) . Users get informed by the system , or by a third party reporting on the system ( e . g . , journalists ) [ 28 , 30 ] . People discuss the system and / or challenge its implications beyond use [ 100 ] , e . g . , in its organisationalcontext [ 19 , 53 , 112 ] . Users seek reflection on outputs , processes , as well as on reasons behind and implications of the system ( meta ) . What is relevant—and to what extent—may depend on the context of the system’s deployment . Debuggability ( end - user debugging ) How can I correct system errors ? How can I tell the system why it was wrong ? [ 73 ] How can I give effective feedback to the system ? Which objects do I have to change ? How do changes affect the rest of the system ? What does the system’s feedback mean ? How can I detect a system error ? How can I find out the cause for an error ? How can I solve this error ? [ 74 ] Users gain insight into the system to fix its errors . Users fix the system’s errors . Users need to understand outputs , processes , and interactions to give good feedback and correct the system . Users make the system more relevant to them by correcting system errors . Explainability Can I trust this model ? [ 96 ] Should I trust this prediction ? [ 49 , 96 ] What are the strengths and limitations of the system ? How can I add my knowledge and skills to the decision process ? [ 49 ] How are input and output related ? [ 60 ] Why does the system think that I want / need X ? [ 14 , 31 ] Why is this recommendation ranked at the top ? [ 110 ] Users gain insight into the system to better use it . Users get informed by the system . Users get information about outputs and processes . Explanations should be relevant to the user . They should be “sound and complete” , but not overwhelming [ 69 , 72 ] . Intelligibility Why did the system do X ? How / under what conditions does it do Y ? Why did it not do Y ? What ( else ) is it doing ? What if there is a change in conditions , what would happen ? [ 24 , 79 ] Users want to use the system in better ways or to gain trust [ 81 , 118 ] . Users actively inquire into the system’s inner workings . Users seek information about outputs and processes . Users’ demand informs what is relevant . Factors related to system and context influence this [ 79 ] . Note that we do not attempt to redefine the terms here but rather provide a guiding overview of our coding , which includes some overlap between terms , as found in the literature . This table is continued in Table 2 . saturated and had defined our framework dimensions . The final set of system qualities emerg - ing from this process included scrutability , interpretability , transparency , explainability , intel - ligibility , interactivity ( interactive Machine Learning ) , debuggability ( end - user debugging ) , and accountability . The paper set used is available on the project website : https : / / www . medien . ifi . lmu . de / howtosupport / . 4 . 2 Paper Coding and Interpretation We followed the coding process as suggested by Glaser and Strauss [ 43 ] and Strauss and Cobin [ 105 ] to code the papers . The coding was done by the first two authors , in joint work on the texts and discussion . 4 . 2 . 1 Open Coding : Opening Up the Text . To keep the coding process manageable , we mainly focused on the motivation , contribution , and conclusion of a paper , making use of the flexibility ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 6 D . Buschek et al . Table 2 . Continuation of Table 1 Systemquality Userquestions ( examples ) Usermindsets Userinvolvement Knowledgeoutcomesandqualities Interactivity ( interactivemachinelearning ) How can I assess the state of the learned concept ? Where does the model fail ? Why did the system fail in this specific instance ? [ 34 ] How well does the system know the domain ? How sure is the system that a given output is correct ? Did the system do a simple or complex thing to arrive at the output ? [ 98 ] How to combine models ? [ 106 ] Which model works best ? [ 7 ] Users inspect the system state to refine it or guide its training [ 34 ] . Users iteratively refine the system and guide its training by giving feedback [ 34 ] . Users need to understand outputs , processes , and interactions to guide the system . What is relevant to know is defined by the machine learning task that users and system solve together . Interpretability How sensible and not arbitrary or random – is the system ? [ 4 , 94 ] Why ? questions [ 42 ] Can you trust your model ? What else can it tell you about the world ? [ 82 ] Users gain utilitarian and interpretative insight into the system to bridge the gap between the system’s criteria and full real - world context [ 32 , 64 ] . Users get information about the system’s inner workings [ 32 , 82 ] . Users can access information about outputs and processes , which may include low - level ( expert ) information ( e . g . , on inner states [ 64 ] ) . What is relevant to know depends on the user’s task with the system . Explicit call for rigorous evaluation [ 32 ] . Scrutability Why / How did the system do X ? Whatelsedoesthesystem think I ( don’t ) know ? What would the system do if I did Y ? What does the system do for other people ? How can I tell the system what I ( don’t ) want ? [ 63 ] How can I efficiently improve recommendations ? [ 11 ] Users want to be able to interpret the system’s decisions . They may analyse and control it for more efficient use . System decision and behaviour is based on a user model , which users can adequately access and control . Users make “real effort” [ 63 ] . Users gain understanding of outputs and processes . They may also learn about interactions to influence how the system uses the user model . Information should be relevant to users , yet they may also learn about what the system considers relevant . Transparency How does the system produce an output ( i . e . , data sources , reasoning steps ) ? Why did the system do sth ( i . e . , justification , motivation behind the system ) ? What is informed by the intelligent system ( i . e . , reveal existence of intelligent processing ) ? How was the system developed and how is it continually being improved ? [ 94 ] How did the system produce the model ? [ 33 ] Users interpret the system’s output and question the underlying mechanisms . Users get informed by the system . Users seek understanding of outputs and processes , also beyond use ( meta ) . What is relevant—and to what extent – may depend on the context of the user’s inquiry . of the approach to include other sections as needed for a clearer understanding . We first collected open codes about the assumptions on supporting user understanding made in the text . Open coding was guided by so - called basic questions [ 43 ] ( e . g . , Who ? , What ? , How ? , etc . ) . This step resulted in a first set of ( sub - ) categories , namely Motivation ( of the work ) , Key Phe - nomena and Constructs , Support as Property or Process , Reference to Specific System Part , Main Goal of Support , Main Challenge of Support , User Role , System Role , Interaction Design Implications , and Concrete Realisation . 4 . 2 . 2 Axial Coding : Eliciting User Questions . We then created axial codes to refine and differen - tiate our categories . During this process , we realised that suitable axial codes could be expressed in the form of questions users have about the workings of intelligent systems ( e . g . , Why did the system ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 7 do X ? ) . This is in line with prior work : First established by Lim and Dey [ 79 , 81 ] as a manifestation for users’ information demand , such user questions have gained popularity in related work on the system qualities as a way of anchoring design suggestions and solutions , even if these questions are not always elicited from actual users ( e . g . , cf . work by Kay and Kummerfeld [ 63 ] ( scrutability ) , Kulesza et al . [ 73 ] ( end - user debugging ) , or Rader et al . [ 94 ] ( transparency / accountability ) ) . In our case , coding these questions helped us to extract and contrast underlying perspectives : On the one hand , they connected prior work on the system qualities , on the other they revealed conceptual differences , and thus refined our preliminary categories . During this process , we kept close to the text of a paper . For example , “Accountability [ . . . ] the extent to which participants think the system is fair and they can control the outputs the system produced” [ 94 ] yielded “How fair and controllable is the system ? ” in our question set . We continuously moved back and forth between the texts and our preliminary categories when integrating new user questions to test our categories against the text passages . As the outcome of this coding step , we ( 1 ) refined the categories elicited in the open coding , and ( 2 ) discovered differences and commonalities between the system qualities by grouping user questions according to the quality in focus of a paper ( see Tables 1 and 2 for an excerpt ; please note that one question could be relevant for multiple system qualities ) . 4 . 2 . 3 Selective Coding : Building the Framework . In the last step , we identified core concepts overarching our categories . Practically , we wrote our user questions on cards we re - arranged on a large wall , plus example papers and stickers for possible core concepts . This set - up was then discussed extensively over several weeks and extended through further paper collection and reviewing . As a result , we identified and defined our framework dimensions presented in Section 5 . 4 . 3 Limitations Our framework should be understood and applied with these limitations and focus aspects in mind : First , as work on interactive intelligent systems has evolved into a rapidly developing field in the last decade , future approaches might uncover solutions and principles that the presented frame - work does not cover in its current state . Second , this work focuses on eight system qualities , listed in Section 4 . 1 . Since this set was shaped through our sampling process , we are confident that it covers a representative part of the diverse approaches and reflections in the field . However , it is not comprehensive . For a detailed overview of possible system qualities , see Abdul et al . [ 1 ] . Third , our paper set is mainly based on work accessible through the ACM Digital Library . We also included work published through IEEE , Springer , and Elsevier , among others , but only through snowball searches , not as a main database , to keep our paper set at a manually manageable size . Overall , these limitations might create the need to build on , extend , and adapt the framework in future work . Intentionally , we do not attempt to provide an explicit definition for each of the system qualities in focus here . However , our analysis can be used to gain a guiding overview ( Tables 1 and 2 ) and our framework helps to make explicit the assumptions for particular uses of these terms , as we discuss later on . Finally , most of this work focuses on HCI researchers as a target audience . However , in Section 8 . 4 . 1 , we suggest ideas on how our framework might be applied to commercial systems . Moreover , we provide a version of the framework as printable cards for a set of ( design ) activi - ties ( see Section 7 ) . Such translations into practice could be further examined and validated , for example , through interviews with designers and developers , or by prototyping future applications using the framework dimensions and evaluating them with end - users . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 8 D . Buschek et al . 5 A USER - CENTERED FRAMEWORK FOR SUPPORTING USERS IN UNDERSTANDING INTELLIGENT SYSTEMS Our analysis revealed three categories that capture and differentiate current assumptions about users and interaction with intelligent systems . They thus serve as a conceptual framework for structuring the discussion about supporting users in understanding intelligent systems from an HCI perspective , as presented in the next sections . 5 . 1 What Do Users Seek to Know ? User Mindsets The user questions addressed in work on different system qualities imply different assumptions about what users seek to know : For example , searching for information about why a certain system output came into being [ 81 ] implies a very different kind of interest than wanting to know how well a program knows a given domain [ 98 ] or how a system was developed and continually improved [ 94 ] . Likewise , this is true for users asking how to correct system errors [ 73 ] compared to users that want to be merely informed about the presence of algorithmic decision - making [ 94 ] . To capture these differences , we introduce the category user mindsets . In psychological research , mindsets describe the “cognitive orientation” of people that precede the formation of intentions and planning of successive actions towards reaching a goal [ 45 ] . In the same manner , for this work we define user mindsets as users’ cognitive orientation that guides concrete intentions to interact with an intelligent system . Note that , in this view , the mindset can precede use of the system but of course this might change through interaction with the system or its presentation . From our analysis emerged three such mindsets that we find in prior work on the system qualities : utilitarian , interpretive , and critical , described in detail in the next subsections . 5 . 1 . 1 Utilitarian Mindset . A utilitarian mindset aims to predict and control system behaviour to reach a practical goal . This mindset carries a strong notion of utility and / or usability . Consequently , a utilitarian mindset is reflected by many examples in work on system qualities that imply a very practical view on user inquiry , such as in work on explainability and intelligi - bility . For example , users might want to understand system recommendations to better compare and find products they are interested in ( Why was this recommended to me ? ) [ 93 ] . Moreover , users might want to train more effectively with an intelligent fitness coach [ 38 ] , or understand a system ranking they financially depend on , as observed in AirBnB [ 59 ] or social media [ 20 ] . In another ex - ample , users worked more efficiently with a system feedforward based on What if ? questions [ 24 ] . Similarly , user questions in work on scrutability such as How can I efficiently improve recommen - dations ? [ 11 ] imply a utilitarian mindset . On a meta - level , this mindset can also be found in work on interactive machine learning and end - user debugging . Research in these areas addresses user questions such as How can I assess the state of the learned concept ? Where does the model fail ? [ 34 ] , How sure is the system that a given output is correct ? [ 98 ] , How to combine models ? [ 106 ] , Which model works best ? [ 7 ] , or How do changes affect the rest of the system ? [ 74 ] . These and similar questions imply a focus on recognising and handling system error , giving feedback , or analysing the system to better work with it in the future . 5 . 1 . 2 Interpretive Mindset . An interpretive mindset strives to interpret system actions based on one’s perception of and experience with the system and its output . This mindset embraces the notion of user experience . When users adopt this mindset , they do not necessarily want to reach a particular practical goal , but rather to understand the system based on a certain experience they have had . For example , a social media user might want to understand why posts of particular friends are not shown [ 20 ] . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 9 Moreover , an interpretive mindset might be adopted by users who do not understand how they are being profiled , when they believe their feedback is not being considered or feel they lack control over system output [ 20 ] . Examples for an ( implied ) interpretive mindset can be found in work on transparency ( e . g . , How sensible—and not arbitrary or random—is the system ? [ 94 ] ) and interpretability ( e . g . , What else can the model tell me about the world ? [ 82 ] ) . Moreover , it is reflected in many user questions articulated in work on scrutability , for example What else does the system think I ( don’t ) know ? , What would the system do if I did Y ? , or What does the system do for other people ? [ 63 ] . Although control also plays an important role in research in this field , the underlying perspective is framed in terms of experience with and perception of the system and its output rather than a practical goal . 5 . 1 . 3 Critical Mindset . A critical mindset stresses normative , ethical , and legal reflection about intelligent systems . This echoes the wider discussion about the system qualities , such as transparency , explainability and accountability ( e . g . , [ 39 , 51 , 84 , 115 , 116 ] ) . For example , a user might critique a system’s missing social intelligence [ 19 , 20 ] or might want to know why it was developed in a certain way [ 94 ] . A critical mindset may thus be decoupled from system use . Calls for support of critical inquiry are mainly found in work on system accountability . User questions include How was the system developed and how is it continually being improved ? , What is informed by the intelligent system ( i . e . , reveal existence of intelligent decision - making and process - ing ) ? , How fair and controllable is the system ? [ 94 ] , How fair is a system decision ? [ 15 ] or Can I trust this model ? [ 96 ] and Should I trust this prediction ? [ 49 , 96 ] . 5 . 2 How Do Users Gain Knowledge ? User Involvement As introduced in the Scope and Foundations sections of this article , we assume that user under - standing is built through the transmission of information between the user and the system . Our analysis revealed that the great majority of work on the system qualities envisions this transmission of information in the form of a dialogue , that is as a “cycle of communication acts channelled through input / output from the machine perspective , or perception / action from the human perspective” [ 55 ] . Dialogue as an interaction concept inherently stresses the need for users to understand the system ( and vice - versa ) [ 55 ] . It has even been argued that the characteristics of intelligent systems necessarily involve some sort of dialogue in order for users to understand them [ 98 ] . Elements of dialogue , such as structuring interaction as stages [ 55 ] , are commonly found in work on the system qualities . Most notably , end - user debugging [ 69 ] and interactive machine learning [ 62 ] make use of mixed - initiative interfaces [ 56 ] . It thus seems that the system qualities almost imply this concept of interaction , so closely are they interwoven with a dialogue structure . To support users in understanding intelligent systems , information may thus be transferred in two directions , either from user to system , or from system to user . From a user perspective , this determines how users gain knowledge —either through action ( active ) or perception ( passive ) . Our second framework category , user involvement , captures these two ways of gaining knowledge . User involvement thus describes interaction possibilities to transfer information to or receive information from the system as a basis for user understanding . In the following sections , we distinguish work on the system qualities according to these two directions . In general , a system ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 10 D . Buschek et al . might support involvement in multiple ways ( e . g . , via explanation , controls , visualisations ) and thus imply transitioning between both directions during use , for example , through interactive visual explanations [ 69 ] . As a clarification of the terms used , note that “user involvement” here refers to interaction with the system , not its design ( e . g . , not “user involvement” in the sense of participatory design ) . 5 . 2 . 1 Active User Involvement ( User - to - System ) . User questions such as How can I tell the system what I want ? [ 63 ] , How can I detect system errors ? [ 73 ] , or What do I have to change to correct the system ? [ 73 ] point to active users whose corrections and feedback are utilised by the system . The dialogue between system and user may be initiated by both sides and is then indeed based on turn - taking , as described earlier . For example , Alkan et al . [ 2 ] presented a career goal recommender that literally employs a dialogue structure to suggest items and incorporate user feedback . Work on scrutability by Kay and Kummerfeld [ 63 ] emphasises the “real effort” users must make when inquiring into a system . Other work on the system qualities , in particular in end - user - debugging and interactive machine learning , sees users in active roles as debuggers [ 71 ] or teachers [ 6 ] . Systems support an active user by offering interface controls tied to aspects of their “intelligence” ( e . g . , data processing , user model ) . These controls may enable users to experiment with the intel - ligent system ( e . g . , user model controls [ 63 ] ) . For example , if a user interface offers switches for certain data sources ( e . g . , in a settings view ) , users can actively experiment with the way that these data sources influence system output ( e . g . , switch off GPS to see how recommendations change in a city guide app ; also see [ 63 ] ) . Moreover , Coppers et al . [ 24 ] introduced widgets for system feedfor - ward that allow for active inquiry to answer What if ? user questions such as What will happen if I click this checkbox ? . Another example is a separate , dedicated GUI for such experimentation , which allows users to directly set the values of certain system inputs and check the resulting output ( e . g . , “intelligibility testing” [ 81 ] ) . Moreover , many visual explanations offer direct manipulation that also puts users into an ac - tive role : For instance , work in interactive machine learning proposed interactions with classifier confusion matrices to express desired changes in resulting decisions [ 62 , 106 ] . Similarly , work on explanations for spam filtering enabled users to influence the classifier via interactive bar charts of word importance [ 69 ] . 5 . 2 . 2 Passive User Involvement ( System - to - User ) . User questions such as Why does the system think that I want / need X ? [ 14 ] , Why did the system do X ? [ 79 , 94 ] , Why did it not do Y ? [ 81 ] or How does the system produce an output ? [ 94 ] suggest that users want to get informed about the systems inner workings , but do not actively provide the system with feedback and corrections . Users may still initiate the dialogue with the system , but are then restricted to be recipients of information . This way of user involvement is typically assumed by work on transparency and ex - plainability , where displaying information about a system’s inner workings is a common tool for user support . For example , related work proposed visual and textual explanations that show how recommendations are influenced by data from customers with similar preferences [ 41 ] . Further examples of supporting user understanding in a passive way include icons that indicate “intelli - gent” data processing [ 38 ] , interaction history [ 57 ] , annotations for specific recommendations [ 16 ] , and plots and image highlighting for classification decisions [ 96 ] or recommendations [ 109 ] . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 11 5 . 3 Which Knowledge Do Users Gain ? Knowledge Outcomes The envisioned result of the different system qualities is knowledge that users gain about an intelli - gent system . However , this knowledge may refer to different aspects of the system and interaction , such as a specific recommendation [ 93 ] or the “reasoning” of a system [ 96 ] . To account for this vari - ety of which knowledge users gain , we introduce our third framework category , knowledge outcomes . These characterise the nature of user understanding developed about an intelligent system . Overall , our analysis surfaced four different knowledge outcomes currently addressed in the literature ( out - put , process , interaction , and meta ) . Since output and process knowledge are often confronted in the literature , we present them in one subsection here , too . These knowledge outcomes are not unique to HCI or intelligent systems . For example , output and process knowledge can be found in work on theory on gaining knowledge in practice [ 83 ] . Moreover , work on complex problem solving articulates output , process and structural know - ledge [ 101 ] , the latter being similar to our interaction knowledge . We also introduce two qualities of knowledge emerging from the reviewed literature . Borrowing established terms for knowledge qualities in applied research theory [ 50 , 83 ] , we summarise them as rigour and relevance of knowledge . 5 . 3 . 1 Output and Process Knowledge . Output knowledge targets individual instances of an intel - ligent system ( e . g . , understanding a specific movie recommendation ) . In contrast , process knowl - edge targets the system’s underlying model and reasoning steps ( e . g . , the workings of a neural net - work that processes movie watching behaviour ) . Related , the terms local and global explanations / interpretability are also used in this context ( e . g . , see [ 85 ] ) . Explainability research in particular distinguishes between explanations for instances and mod - els . For example , Ribeiro et al . [ 96 ] explain classifiers with regard to two questions , Should I trust this prediction ? and Can I trust this model ? . Therefore , they design for both output and pro - cess knowledge . These two knowledge types also motivate the what and how questions posed by Lim and Dey in their work on intelligibility [ 79 ] ( e . g . , What did the system do ? ) . Also , Rana and Bridge [ 95 ] introduced chained explanations ( called “Recommendation - by - Explanation” ) to explain a specific output to users . Moreover , work on accountability makes system reasoning ac - cessible to users to support the development of process knowledge [ 15 ] . 5 . 3 . 2 Interaction Knowledge . Our knowledge type , interaction knowledge , describes knowing how to do something in an interactive intelligent system . For example , supporting users in gaining this type of knowledge motivates questions in work on scrutability ( e . g . , How can I tell the system what I want to know ( or not ) ? [ 63 ] ) , interactive machine learning ( e . g . , How to experiment with model inputs ? [ 6 ] ) , and end - user debugging ( e . g . , How can I tell the system why it was wrong ? [ 73 ] , How can I correct system errors ? [ 71 ] ) . 5 . 3 . 3 Meta Knowledge . Meta knowledge captures system - related knowledge beyond interaction situations , such as information from a developer blog . For example , meta knowledge motivates some questions in work on transparency , such as How is the system developed and how is it contin - ually improved ? by Rader et al . [ 94 ] . They also explicitly add Objective explanations that inform users about how “a system comes into being” that result in meta knowledge ( e . g . , development practices and contexts ) . Moreover , this knowledge type is a main driver of work on accountability , in which computer science overlaps with journalism : For instance , Diakopoulus “seeks to articu - late the power structures , biases , and influences” of intelligent systems [ 28 ] . 5 . 3 . 4 Rigour and Relevance of Knowledge . Rigour : Kulesza et al . propose the concepts of sound - ness and completeness in their work on explanations in intelligent systems [ 69 , 72 ] — soundness is ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 12 D . Buschek et al . truthful explanation , and completeness means explaining the whole system . Gilpin et al . [ 42 ] also refer to completeness , yet understand it as supporting anticipation of system behaviour in more situations . For an overarching view , we generalise this to a broader level : We regard soundness and completeness as facets of rigour . Linked back to the work by Kulesza et al . [ 69 , 72 ] , this means that a rigorous explanation , and the resulting understanding of a system , should be sound and complete . Relevance . A rigorous understanding does not need to be useful . We argue that this aspect should be of explicit interest for a pragmatic HCI perspective . We thus consider relevance as another general quality of knowledge [ 83 ] that is crucial to make explicit in the specific context of user understanding of intelligent systems . This quality highlights our pragmatic view : Elements like explanations are valuable if they add utility , that is , if they help users to gain knowledge that is relevant for using the system in better ways and towards better outcomes . In this pragmatic sense , this quality echoes Kulesza et al . ’s suggestion to “not overwhelm” users with ( irrelevant ) information [ 69 , 72 ] . What is relevant to know , and to which extent , may also depend on factors such as task and complexity of the system [ 21 ] . 6 FRAMEWORK APPLICATION IN RESEARCH : STRUCTURING PAST & FUTURE WORK We have presented three categories for supporting user understanding of intelligent systems as emerged from our analysis of the literature— user mindsets , user involvement , and knowledge out - comes . These categories highlight differences and commonalities between work on the system qualities and serve as a conceptual framework of supporting users in understanding intelligent systems . Our framework introduces an overarching user - centric structure to the field that abstracts from the fractured terminological landscape . We now propose to use our framework categories as a means for researchers in HCI and adjoining fields to clarify and make explicit the assumptions of their work , and to structure past and future work and discussions about how to support users in understanding intelligent systems . The boxes presented throughout this article provide inspiration on what to consider . The following sections highlight further applications of our framework . 6 . 1 Structuring Existing Approaches and Solution Principles Here we analyse three concrete examples from the literature using our framework dimensions : First , in Figure 2 , we demonstrate the application of our framework based on an interactive intel - ligent mood board creation tool by Koch et al . [ 66 ] as an example for system - aided design ideation . In their prototype , system and user collaborate to find suitable imagery , the system making sug - gestions which the user can accept or discard . For each suggestion , the system offers a textual explanation and different feedback options . In terms of user involvement , this approach thus sup - ports both an active user ( via options for feedback and correction and turn - taking ) , as well as a passive one ( via information about why an image was suggested ) . With regard to knowledge out - comes , users gain relevant knowledge about a specific output ( a specific image suggestion ) and interaction knowledge about how to control future system suggestions ( by telling the system what they like or not ) . Overall , the prototype is designed so as to best support users in the creation of a mood board as an ideation activity , and thus implies a utilitarian mindset . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 13 Fig . 2 . Application of our framework using Koch et al . ’s system - aided mood board creation tool [ 66 ] as an example . System image from Koch et al . [ 66 ] used with the authors’ permission . As a second example , we consider an interactive intelligent meeting scheduling assistant by Kocielnik et al . [ 67 ] which extracts meeting requests from free - text emails . One of their interface variants ( in their paper referred to as variant B , example - based explanation ) provides users with an explanation of how the assistant arrives at a decision ( i . e . , classifying text as a meeting request or not ) . Users are presented with a general description of how the text classification works as well as with four concrete text examples and their respective likelihood to be classified as a meeting request ( from “very likely” to “very unlikely” ) . In these text examples , meeting - related keywords are visually highlighted . While the overall context of the assistant can be categorised as utilitarian ( users want their meeting requests to be recognised automatically and accurately ) , this particular interface supports an interpretive mindset : According to the authors , “ [ this ] design is meant to increase user understanding of how the AI component operates” , that is , understanding how sys - tem detection works is in focus . The interface not only fosters output knowledge ( a concrete text example is linked to a concrete output ) but is specifically designed to support process knowledge as well : It communicates that decisions are made on a sentence - level , that meeting - related keywords increase the chances of detection , and that the system may err due to sparse or lacking keywords . Finally , user involvement is passive since users receive information but have no means to act on their part . As a third and last example , we discuss Silva , a system for fairness assessment in machine learn - ing [ 117 ] . It takes a dataset as an input , and then computes a causal model of the data attributes , as well as several fairness metrics when used for training a set of standard machine learning models . While we cannot go into every detail of Silva’s interface here due to its complexity , we summarise its main components : ( 1 ) A list of data attributes considered for training ( e . g . , age , sex , occupation , etc . ) , which can be grouped as needed , ( 2 ) a ( system - created ) graph that visualises the causal re - lationship between attributes , ( 3 ) a tabular overview of user - defined attribute groups along with several fairness metrics , and ( 4 ) a bar - chart visualisation of the fairness metrics for different at - tribute groups and across different machine learning models . Categorising Silva using our frame - work , we clearly see a critical user mindset reflected in its purpose and design—it enables users to diagnose sources of bias in the data and corresponding models and thus find issues with machine learning fairness . Moreover , users are passively and actively involved : On the one hand , they re - ceive information and recommendations from the system . On the other hand , they can explore the ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 14 D . Buschek et al . system through What if ? questions and influencing system output by ( de ) selecting and grouping data attributes . Overall , users acquire interaction knowledge within the tool itself ( e . g . , via interact - ing with data attributes ) as well as output knowledge for the investigated dataset and model ( e . g . , via exploring how different attributes or attribute groups impact the fairness metrics in this case ) . Interestingly , likely due to its focus on serving as an analysis tool for ( other ) ML systems , Silva does not provide the users with explanations about its own underlying workings , for example , regarding its recommendations and the generation of the causal attribute graph . Beyond these three concrete in - depth examples , Table 3 presents other exemplary solution prin - ciples from the literature to illustrate how user mindsets , user involvement , and knowledge out - comes may be used to structure past work . We do not claim to provide a comprehensive survey in our work , but selected these examples to show the diversity of approaches in our paper set . Studying the approaches as arranged in Table 3 reveals interesting structures : For example , ex - planations appear across the charted space and thus could be seen as the go - to building block for many solution principles . They commonly provide output and process knowledge via text and / or plots [ 41 , 57 , 65 , 69 , 73 , 80 , 96 ] . Sometimes these representations also allow for interactivity and user corrections [ 69 , 73 , 81 ] , in particular when explanations are referred to in work on scrutability , end - user debugging , and interactive machine learning [ 27 , 62 , 63 , 106 ] . Explanations commonly arise from utilitarian mindsets , yet they also appear in work with interpretive and critical ques - tions [ 16 , 29 , 63 , 94 ] . 6 . 2 Reflecting on Your Own Approach 6 . 2 . 1 Reframing User Questions . User questions are a helpful way to uncover users’ informa - tion needs . Our framework can be used to re - frame such questions in related work , in particular by reconsidering the underlying mindsets to view questions from a novel angle : For example , a question such as Why did the system do X ? is currently mostly tied to a context implying a util - itarian mindset [ 79 ] . However , this question could also reflect other mindsets and thus different underlying user motives for inquiry , depending on the envisioned context . Design solutions to this question could then foster utilitarian ( e . g . , explain feature influences ) , interpretive ( e . g . , explain in terms of a user’s daily life context ) , critical ( e . g . , explain system decision given a community’s norms ) , or all three mindsets . 6 . 2 . 2 Explicating Perspectives with Mixed Mindsets . Related , reflecting on the three mindsets presented here can help to discover structure in perspectives that mix multiple mindsets : For ex - ample , a recent set of 18 guidelines for interaction with AI [ 8 ] contained mostly utilitarian guide - lines ( e . g . , “Support efficient dismissial” , “Provide global controls” ) , yet two stand out as following a critical mindset ( “Match relevant social norms” , “Mitigate social biases” ) . Our lens allows to clar - ify and explicate this mix , revealing , in this example , that the guidelines already follow a broader perspective than the related work itself alluded to with its stated focus “on AI design guidelines that [ . . . ] could be easily evaluated by inspection of a system’s interface” . Such analysis could help to structure discussions about similarly mixed perspectives . 6 . 2 . 3 Explicitly Determining Relevance . What is relevant to know for users is considered dif - ferently across the system qualities . Highlighting relevance and rigour ( see section on knowledge outcomes and Tables 1 and 2 , last column ) thus helps to reflect on what we consider important , for whom , and to what extent—and how we choose to determine it . For example , explanation design often involves non - expert users , possibly via a user - centred design process [ 38 ] or scenario - based elicitation [ 81 ] to inform what is relevant , what should be explained , and to what extent . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 15 Table 3 . Examples of Approaches and Solution Principles for Supporting User Understanding of Intelligent Systems , Structured Through our Framework KnowledgeOutcomes Output Process Interaction Meta U s e r I n v o l v e m e n t A c t i v e Visualisationsexplainclassificationviafeatures , user“explainsback”correctionsbymanipulatingtheseplots [ 69 , 73 ] . “Reasons”tabsshowsensor - specificvisualisations thatexplaincurrentprediction [ 80 ] . Usersbuildif - rulesinan editor , systemdetects problemsviasimulation ( e . g . , loops ) , usercorrects them [ 27 ] . “Experimentationview”allowsuserstotryoutinputsandseesystemoutput [ 81 ] . Confusionmatricesshowcurrentstateof classifier [ 62 , 106 ] . Naturallanguagedialogueenablesuserstoasksystemwhattheycoulddonext [ 107 ] . Usersmanipulateconfusionmatrixtochangemodel [ 62 ] , including re - combiningmultiplemodels [ 106 ] . Beyondruntime : Opensourceenables codeauditsandfacilitatesunderstandingofsystem [ 22 ] . U t i l i t a r i a n M i n d s e t P a ss i v e Barchartsandimageregionsshowimportanceofpredictorstoexplainspecificclassification [ 96 ] . Visualisationsshowsimilarusers’inputtoexplainrecommendation [ 41 ] . Treeofimagesoftraininginstancesexplainsclassification [ 118 ] . Explainingaclassifierbyexplainingmultiplespecificclassifications [ 96 ] . Animationshowslearningalgorithmatwork [ 58 ] . Texthighlightingshowswhichwordscontributedtomeetingdetectioninemails . [ 67 ] Listshowsuser’spastinteractionstoexplainspecificrecommendation [ 57 ] . Step - by - stepexplanationsof trigger - actionrulesbysimulatinguser andsystemactions [ 25 ] . Beyondsinglesystems : Educatepublic incomputationalskillstofacilitatesystemunderstandingoverall [ 22 ] . A c t i v e Rule - basedreasoningsystem verbalisessystemdecisionsinnaturallanguagedialoguewithuser [ 107 ] . Separateprofilepagedisplayscurrentusermodel [ 63 ] . Naturallanguagedialogueenablesuserstoasksystemwhyithasnotdecideddifferently [ 107 ] . Constructivistlearning : user manipulatessystemandupdatesmentalmodelofitbasedonresultingchangesinoutput [ 99 ] . Separateprofilepageenablesuserstoeditwhatthesystemknowsaboutthem [ 63 ] . “Algorithmicprofilingmanagement” : Profilepagerevealswhatsystemknowsandhowthisinfluencescontent , includingpastinteractions ; controlsenablemodifications [ 5 ] . “AlgorithmicUX”beyondinteraction : Usersengageincommunicationandrelationshipbuildingwithintelligentagents [ 89 ] . I n t e r p r e t i v e P a ss i v e IconindicateswhichsystemoutputisinfluencedbyAI [ 38 ] . Textanalysisextractsrelevantsentencesfromreviewstoshowalongwithrecommendation [ 31 ] . Outputshownwithtextualexplanationofthedecisionprocess [ 65 ] . Animationindicateshowsystemoutputisgenerated ( e . g . , dicerollfor randomness ) [ 38 ] . ExplainRecommendationswithusagestatistics ( e . g . , globalpopularity , repeatedinterest ) [ 16 ] . Beyondcode : Educate / sensitise developersanddecisionmakerstoconsequencesofsystems [ 22 ] . Iconsindicatewhenandforwhichhigh - levelgoal ( e . g . , ads ) userdatais processedbythesystem [ 102 ] . A c t i v e “Algorithmicaccountabilityreporting” : Journalistsreport onblackboxsystems , e . g . , by tryingoutinputssystematically [ 28 , 30 ] . Beyondsystemunderstanding : Societymust looknotintosystemsbut acrossthem , thatis , seetheir rolewithinalargernetworkofactors ( incl . humansand institutions ) [ 9 ] . Challengingthesystem : Peoplelearn frompastinteractionsandoutputhow tochallengesystemintelligenceanditsnormativeimplicationsthroughunexpectedormaliciousinput ( e . g . , manipulatingpublicchatbotviatwitter ) [ 86 ] . Beyondsystemuse : Peoplediscussand reflectonsocialimplicationsand contextofthesystem’soutput [ 19 , 100 , 112 ] . C r i c t i c a l P a ss i v e Recordmodels , algorithms , data , decisionsforlater audit [ 10 ] . Annotaterecommendedcontentpieceswithindicatorsforquality / reliabilityoftheir source ( e . g . , fornews ) [ 76 ] . Textualexplanationsofsystemintelligenceonahighlevel , notintegratedintothe system ( e . g . , articlesabout system ) [ 94 ] . Explainingthelogicbehindanalgorithmwithanotheralgorithm [ 18 ] . “AlgorithmicImaginary” : People developunderstandingofintelligentsystemsandhowtoinfluencethembasedonhow“theyarebeingarticulated , experiencedandcontested inthepublicdomain” [ 20 ] . Textualexplanationsofdevelopers’intentionsonahighlevel , not integratedintothesystem ( e . g . , articles aboutsystem ) [ 94 ] . This is not a comprehensive survey ; examples were selected to illustrate the diversity of approaches in the literature . In contrast , interactive machine learning focuses on information that is relevant to the task of the system ( e . g . , to improve a classifier ) , which is often operated by experts [ 6 ] . Therefore , what is relevant here ( and to what extent ) is foremost informed by the machine learning task , and less so by studying or asking end - users . This can be seen , for example , in the UI elements derived in ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 16 D . Buschek et al . a recent survey on interactive machine learning [ 34 ] , which are closely coupled to the machine learning task ( e . g . , they serve “sample review” , “feedback assignment” , etc . ) . As another example , work on end - user debugging presents an action - focused middle - ground , between user - focused ( as explainability ) and system - focused ( as interactive machine learning ) : Here , resulting knowledge should help users to make the system more relevant to them , for exam - ple , by correcting system errors from the users’ point of view ; users may be both experts [ 7 ] or non - experts [ 73 ] . 6 . 2 . 4 Informing Methodology . Our framework may be used to motivate methodological choices , for example , when informing or evaluating the design of a new approach for supporting user understanding : For instance , work catering to a utilitarian mindset might benefit from a controlled environment and precise measurements in a lab study . Even simulation of system decisions might be a ( first ) option [ 32 ] . A lab study might also be a suitable choice to evaluate support for developing inter - action knowledge , since users can be directly observed during interaction ( e . g . , see [ 63 , 106 ] ) . A related but different idea in this context is the use of a controlled interactive ( utilitarian or inter - pretive ) task to examine underlying machine learning models with regard to the system qualities ( e . g . , interpretability , cf . [ 97 ] ) . Moreover , if a design or research question targets an interpretive mindset and / or meta knowledge , it might be worthwhile or required to study user and system in their daily contexts of use ( e . g . , see lab vs . field study in education context in [ 63 ] ) . The same holds for work motivated by a critical mindset , yet other methods exist here as well , such as online sur - veys or data analyses of views expressed through mass media and social network discussions [ 76 ] , or policy and legal texts [ 10 , 46 , 91 ] . 7 TRANSFER TO PRACTICE In order to support translation to practical use we created a set of user question cards ( see project material 1 ) . Figure 3 gives an overview of this material . These cards are meant to support several group activities and could be used , for example , by teams of practitioners and researchers , working on interactive explainable AI systems or other topics related to the system qualities . While we leave an empirical evaluation of this material for future work , we describe an illustrative scenario here as guidance : Consider a team of UI / UX practitioners involved in developing explainability features for a recommendation component of a larger business system . They might use the cards for three activities , as described next . 7 . 1 Activity A : Eliciting the Team’s Assumptions The team collects users questions that they aim to support with their ongoing work . The team realises this as a brainstorming session where each member first comes up with user questions individually before discussing them and converging on the most central ones as a group . The team writes these key questions on blank cards . They then place the questions in the centre of their table , one at a time . For each framework category ( i . e . , for each card colour ) , the team goes through the different possible values ( i . e . , cards of that colour ) and discusses which one best fits the team’s understanding of the question . To keep track , they put those framework cards next to the question card . As a concrete example , the team might think about the user question “Why did the system rec - ommend X and not Y ? ” . The team members might discuss that they see this question mainly as 1 https : / / www . medien . ifi . lmu . de / howtosupport / . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 17 Fig . 3 . Overview of the user question cards set available for practical use of our framework in group activities ( e . g . , for design sessions ) . It includes coloured cards for the framework categories and values ( cf . Figure 1 ) and cards with example user questions ( cf . Tables 1 and 2 ) . We encourage extensions of this set by writing further question cards . See Section 7 for activity examples . an interpretative mindset in their current context ( e . g . , customers know how to find good recom - mendations in the system already but their user experience around trust and transparency could be improved ) . They further decide on the question being asked with a passive user involvement in mind , and they agree on output knowledge as the intended main outcome . Throughout their dis - cussion , they might call upon information such as existing customer feedback , early user research , experiences from previous projects , current strategic goals , practical feasibility , and the like . Parts of this might be speculative ( i . e . , it is not clear if their users really would ask this question with this mindset ) yet the main point here is to reveal and capture the team’s own assumptions —which then could be checked later , for example , with empirical user research . In summary , for each relevant user question , the team gets to discuss their detailed understand - ing of that question with regard to the framework categories . Thus , in this activity , the team mem - bers make explicit ( 1 ) which user questions they want to support with their work , and ( 2 ) which mindsets , user involvements , and knowledge outcomes they assume for each such user question . As such , they establish common ground for working towards concrete design solutions . 7 . 2 Activity B : Rethinking the User Questions In this activity , the team could pick a question card and agree on that card from each colour that to the members seems to be emphasised the most through the chosen question . Alternatively , here our example team uses one of the arrangements from activity A ( i . e . , user question plus assumed mindset , user involvement , and knowledge outcome ) . Now one person swaps one of the coloured cards . The team now enters a new discussion : Could the question also be understood in this light , and what would this mean for supporting users here ? Is that something they ( also ) want to design for ? Why ( not ) ? This can be repeated as often as the team likes , ideally at least once per framework category ( colour ) . As a result , team members get to rethink their assumptions around that user question , which might solidify their earlier plans or reveal uncertainty they might want to address . 7 . 3 Activity C : Inspiring Design Ideas Finally , in this activity , the team could pick one card of each colour and one user question card , or again use an arrangement resulting from their earlier activities . Now the team ideates design direc - tions , for example , by asking : How might we design a system / UI that supports users in answering this question , considering the given mindset , involvement , and knowledge outcome ? ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 18 D . Buschek et al . In a focused variant , they would pick the arrangement that represents their assumptions or plans ; in an open variant , they could also shuffle the cards and thus create a random combination of question , mindset , involvement , and outcome . 7 . 4 Extending the Question Set As a concluding remark here , note that the material includes cards with concrete user questions ( three examples are shown in Figure 3 ) . Across all activities , these concrete questions can be used to provide inspiration for questions that users might have , based on the literature surveyed in this article ( cf . Tables 1 and 2 ) . We selected these questions to match those in that table , yet this is clearly only a small subset of possibly relevant questions . As also outlined in the illustrative example above , we encourage teams using our cards to create further question cards , depending on what seems most relevant in their specific contexts . 8 DISCUSSION AND OUTLOOK Here we discuss broader aspects of the framework regarding its foundations and potential further directions . 8 . 1 Supporting Understanding of Different Stakeholders The analysed user questions point at “users” that ask them . These people could be seen as stake - holders with their own agendas , including end users but also other groups . So far , we have not examined these people more closely and we did not separate the collected questions by such dif - ferent groups of people who might ask them . Overall , we can consider many kinds of user groups or stakeholders , which we might want to support in understanding intelligent systems . For exam - ple , Tomsett et al . [ 108 ] describe a role - based model for analysing interpretable ML systems which includes the roles of creator , data - subject , examiner , operator , executor , and decision - subject . Re - lated interdisciplinary work on explainable AI by Langer et al . [ 75 ] , following Barredo Arrieta et al . [ 12 ] , distinguished five classes of stakeholders : users , developers , affected parties , deployers , and regulators . This focus on the “who ? ” , the people involved in the context of explainable and transparent AI systems , is also highlighted and expanded on in further recent research in the HCI community by Ehsan et al . [ 36 ] . In light of this awareness of different stakeholders , we expect that the framework derived here can prove useful when reflecting on the involved or targeted groups : For example , explanation design might benefit from considering the type of user involvement , knowledge outcome and mindset a stakeholder group has , wants , and / or needs . Practitioners , on the one hand , could then transfer the insights from these reflections into concrete design goals , such as designing support for an examiner role with a critical mindset or a more active involvement of data subjects . Researchers , on the other hand , might use the combination of our framework dimensions with , for example , a role - based stakeholder model [ 108 ] to identify gaps and / or position new ideas in the context of the system qualities for specific groups . 8 . 2 Do Users Always Formulate Questions ? Trust and Implicit Knowledge As explained in Section 4 . 2 . 2 , user questions have proven useful both in prior research and in the work presented here as a manifestation of users’ information demand in the context of the system qualities . However , one implied assumption of user questions is that users actually formulate them , at least sometimes . While user studies such as conducted by Lim and Dey [ 79 ] show that this often happens , there is ongoing discussion about the role of trust and implicit knowledge as well , which we examine here . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 19 In general , trust has been recognised as a crucial aspect of interaction with intelligent sys - tems [ 54 , 104 ] . Similar to other research fields , there is no common definition of trust in HCI . Exemplary definitions refer to goal alignment between user and system [ 26 , 77 ] or the perceived expertise , or competence , of the system [ 90 ] . There are at least two opposing perspectives on the role of trust in the context of interaction with intelligent systems [ 104 ] : One perspective on trust regards it as an argument against the importance of system qualities , such as transparency , intelligibility , explainability , and so on , for example , by drawing on analogies like microwaves or cars [ 52 ] , which people happily use as black - boxes . So why not simply trust an intelligent ( software ) system in the same way ? Related , some argue that verbalised explanations of AI systems might even be fundamentally misleading [ 13 ] . In contrast , the opposing perspective on trust holds the basic assumption that users indeed benefit from inquiry and that trusting a black - box is not enough . This view fundamentally motivates research on the various system qualities and assumes that users formulate such questions as the ones collected in this article . Sometimes , these properties are further motivated by in turn fostering trust in the system ( e . g . , see work on intelligibility [ 78 ] , explainability [ 92 , 96 ] , transparency [ 44 , 65 ] ) . In this way , trust is seen as a desiderata to be reached through design and as an outcome of work on the system qualities . In this reflection here , we argue that an emphasis on a “pragmatic user” ( cf . Section 3 . 2 ) and their questions can reconcile both views : First , taking the user questions that we collected and analysed in this article seriously emphasises the value and relevance of explicit inquiry as an approach for users to better understand an intelligent system . However , at the same time , the pragmatic perspective offers to view trust not ( only ) as a goal of the system qualities but also as a pragmatic alternative approach ( to asking explicit questions ) that users might take to gain knowledge of a system—i . e . , experience . For instance , instead of relying on UI elements created with system qualities such as explainability in mind , a user might trust a system ( enough ) to “try it out” and implicitly learn something about it in this way . In summary , following the pragmatic perspective taken here , and with our focus on user ques - tions , we accommodate both trust and inquiry as possible and relevant ways for users to approach an intelligent interactive system . We regard this as an adequate and encompassing stance for HCI research , since it seems likely that users might trust some systems ( or parts of it ) while inquiring into others . This is also supported by voices from journalism and social information systems re - search , which called the idea “persistent fiction” that people will always seek out and debate in detail on information about intelligent systems , even if available [ 9 ] . 8 . 3 Perceived vs . Technical System “Intelligence” In this work , we mostly consider systems as intelligent as perceived by the users ( cf . Section 3 . 1 ) . Whether the underlying technical implementation can be called “intelligent” or not is not so rele - vant for the framework here , which focuses on examining support for understanding by users . In this sense , the framework could also fit other relatively complex systems that might not be deemed “intelligent” , although we do not examine this in particular here . According to cognitive science , a system is complex if it serves ill - defined user - goals and contains a large number of interrelated variables whose dependencies are opaque and change dynamically over time [ 40 ] . In fact , it has been argued that intelligent systems exhibit the same properties and can therefore be classified as complex systems [ 37 ] . With this in mind , we also do not attempt to objectively assess or explic - itly determine whether a specific technical approach can be called “intelligent” or not , which is often difficult , debatable , and can change over time [ 113 ] . However , the technical implementation in a concrete case examined with our framework here can become relevant , for example , with ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 20 D . Buschek et al . regards to designing support to achieve a certain knowledge outcome . For instance , the informa - tion required to provide relevant and rigorous process knowledge will be different depending on whether a system is implemented , for example , with a decision tree or a neural network . 8 . 4 Going Beyond Here we outline further concrete directions for opening up the discussion based on aspects of our framework . 8 . 4 . 1 Inspiring New Approaches in Research and Practice . Our framework may provide inspi - ration for new approaches and solution principles . While the target audience of this article are foremost researchers in the field , we believe that the framework might also be used by practition - ers , for example , as a brainstorming tool for prototype development ( also see Section 7 ) . We illustrate this using the proposed mindsets : UIs could support users in examining the sys - tem with different mindsets via “modes” for explanation views . Users could then switch between utilitarian explanations ( e . g . , explain a recommendation with product features ) and interpretive or critical ones ( e . g . , explain system beliefs about a user , such as that the user is part of a cer - tain target group ; reveal that recommendations are assembled by an AI and not by a human , cf . [ 38 ] ) . A more radical solution could offer three different views of the system that display or hide UI elements depending on the respective “mode” . Or the mindsets might simply help to decide which user approach to support in a system , and to identify those remaining unaddressed so far . Prototypes could then be tested with the respective end - users of the application . Yet , the actual generative power of our framework has to be validated in the future . 8 . 4 . 2 Understanding Systems beyond Interaction Situations . The critical mindset and meta knowledge capture a crucial difference between traditional ( non - intelligent ) systems and what we see today and what is yet to come : Systems are increasingly interwoven with our lives , be it in everyday applications or in areas of consequential decision - making ( e . g . , financial , medical , or legal ) . Their effects thus do not remain limited to a particular interaction situation . It is important that we as researchers reflect on the impact of the systems we design beyond the duration of direct use . This also includes reflections on when and how intelligent systems can learn compared to hu - mans in the same roles [ 3 ] . Examples for work in such a larger context are presented in Table 3 , in particular in the critical and meta areas ( e . g . , [ 9 , 22 , 28 , 30 ] ) . Connections with HCI in such work commonly refer to accountability and transparency of intelligent systems . 8 . 4 . 3 Motivating Connections beyond HCI & Machine Learning / AI . We see recent calls for more joint research at the intersection of HCI and AI to improve system understanding [ 1 ] . However , this is mostly motivated by utilitarian or interpretive mindsets . Thus , another related key take - away is to draw attention to interdisciplinary connections via the critical of the three mindsets proposed in this article : As is evident from recent “AI and data scandals” ( e . g . , [ 47 , 76 , 86 ] ) , devel - oping more understandable ( and accountable ) intelligent systems also needs to be addressed in a wider view ( cf . third wave HCI [ 17 ] ) , for example , across networks of human and AI actors [ 9 ] . More generally , fruitful connections could span considerations from fields like journalism [ 28 , 30 ] and communication [ 76 , 86 ] , policy [ 3 ] , sociology [ 9 ] and education [ 22 ] , and ethical and legal con - cerns [ 18 , 39 ] . As an example of related efforts on expanding the view here , social transparency is a recently proposed concept in the HCI community in this context that emphasises “socially - situated XAI” [ 36 ] . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 21 9 CONCLUSION Intelligent systems tend to violate UI principles , such as predictable output [ 6 , 34 ] , which makes them difficult to understand and use . To address this , researchers , practitioners , policy - makers and the general public call for system qualities such as transparency , scrutability , explainability , interpretability , interactivity , and so on . However , these terms are often blurred and employed with varying interpretations . This impedes conceptual clarity of the very properties that are envisioned to foster users’ understanding of intelligent systems . This review responds to this lack of conceptual clarity with an analysis and discussion of the - oretical concepts and prototype solutions from the literature : We make explicit the diversity of different implied views on user mindsets , user involvement , and knowledge outcomes . In conclusion , we provide researchers with a framework to ( 1 ) clearly motivate and frame their work , ( 2 ) draw connections across work on different system qualities and related design solutions , and ( 3 ) articulate explicitly their underlying assumptions and goals . Moreover , we provide a print - able card - based version of the framework along with suggested activities to facilitate integration of the framework into ( design ) practice . With our work , we thus hope to facilitate , structure , and advance further discussions on supporting users’ understanding of intelligent systems . REFERENCES [ 1 ] Ashraf Abdul , Jo Vermeulen , Danding Wang , Brian Y . Lim , and Mohan Kankanhalli . 2018 . Trends and trajectories for explainable , accountable and intelligible systems : an HCI research agenda . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 582 , 18 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3174156 [ 2 ] Oznur Alkan , Elizabeth M . Daly , Adi Botea , Abel N . Valente , and Pablo Pedemonte . 2019 . Where can my ca - reer take me ? : Harnessing dialogue for interactive career goal recommendations . In Proceedings of the 24th Inter - national Conference on Intelligent User Interfaces ( Marina del Ray , California ) ( IUI’19 ) . ACM , New York , 603 – 613 . https : / / doi . org / 10 . 1145 / 3301275 . 3302311 [ 3 ] Ali Alkhatib and Michael Bernstein . 2019 . Street - level algorithms : A theory at the gaps between policy and decisions . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , Article 530 , 13 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300760 [ 4 ] AhmedAlqaraawi , MartinSchuessler , PhilippWeiß , EnricoCostanza , andNadiaBerthouze . 2020 . Evaluatingsaliency map explanations for convolutional neural networks : A user study . In Proceedings of the 25th International Con - ference on Intelligent User Interfaces ( Cagliari , Italy ) ( IUI’20 ) . ACM , New York , 275 – 285 . https : / / doi . org / 10 . 1145 / 3377325 . 3377519 [ 5 ] Oscar Alvarado and Annika Waern . 2018 . Towards algorithmic experience : Initial efforts for social media contexts . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 286 , 12 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3173860 [ 6 ] Saleema Amershi , Maya Cakmak , William Bradley Knox , and Todd Kulesza . 2014 . Power to the people : The role of humans in interactive machine learning . AI Magazine 35 , 4 ( December 2014 ) , 105 . https : / / doi . org / 10 . 1609 / aimag . v35i4 . 2513 [ 7 ] Saleema Amershi , Max Chickering , Steven M . Drucker , Bongshin Lee , Patrice Simard , and Jina Suh . 2015 . Mod - elTracker : Redesigning performance analysis tools for machine learning . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems ( Seoul , Republic of Korea ) ( CHI’15 ) . ACM , New York , 337 – 346 . https : / / doi . org / 10 . 1145 / 2702123 . 2702509 [ 8 ] Saleema Amershi , Dan Weld , Mihaela Vorvoreanu , Adam Fourney , Besmira Nushi , Penny Collisson , Jina Suh , Shamsi Iqbal , Paul N . Bennett , Kori Inkpen , Jaime Teevan , Ruth Kikin - Gil , and Eric Horvitz . 2019 . Guidelines for human - AI interaction . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , NY , Article 3 , 13 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300233 [ 9 ] Mike Ananny and Kate Crawford . 2018 . Seeing without knowing : Limitations of the transparency ideal and its ap - plication to algorithmic accountability . New Media & Society 20 , 3 ( March 2018 ) , 973 – 989 . https : / / doi . org / 10 . 1177 / 1461444816676645 [ 10 ] Association for Computing Machinery US Public Policy Council ( USACM ) . 2017 . Statement on Algorith - mic Transparency and Accountability . https : / / www . acm . org / binaries / content / assets / public - policy / 2017 _ usacm _ statement _ algorithms . pdf . ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 22 D . Buschek et al . [ 11 ] Krisztian Balog , Filip Radlinski , and Shushan Arakelyan . 2019 . Transparent , scrutable and explainable user models for personalized recommendation . In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval ( Paris France ) ( SIGIR’19 ) . ACM , New York , 265 – 274 . https : / / doi . org / 10 . 1145 / 3331184 . 3331211 [ 12 ] Alejandro Barredo Arrieta , Natalia Díaz - Rodríguez , Javier Del Ser , Adrien Bennetot , Siham Tabik , Alberto Barbado , Salvador Garcia , Sergio Gil - Lopez , Daniel Molina , Richard Benjamins , Raja Chatila , and Francisco Herrera . 2020 . Explainable artificial intelligence ( XAI ) : Concepts , taxonomies , opportunities and challenges toward responsible AI . Information Fusion 58 ( 2020 ) , 82 – 115 . https : / / doi . org / 10 . 1016 / j . inffus . 2019 . 12 . 012 [ 13 ] Daniel Bear . 2017 . Artificial Intelligence isn’t a “Black Box” . It’s a Key to Studying the Brain . https : / / massivesci . com / articles / artificial - intelligence - human - brain - black - box - algorithm / . Accessed : 05 . 10 . 2020 . [ 14 ] Daniel Billsus and Michael J . Pazzani . 1999 . A personal news agent that talks , learns and explains . In Proceedings of the 3rd Annual Conference on Autonomous Agents ( Seattle , Washington ) ( AGENTS’99 ) . ACM , New York , 268 – 275 . https : / / doi . org / 10 . 1145 / 301136 . 301208 [ 15 ] ReubenBinns , MaxVanKleek , MichaelVeale , UlrikLyngs , JunZhao , andNigelShadbolt . 2018 . “It’sreducingahuman being to a percentage” : Perceptions of justice in algorithmic decisions . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 377 , 14 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3173951 [ 16 ] RoiBlanco , DiegoCeccarelli , ClaudioLucchese , RaffaelePerego , andFabrizioSilvestri . 2012 . Youshouldreadthis ! Let me explain you why : Explaining news recommendations to users . In Proceedings of the 21st ACM International Con - ference on Information and Knowledge Management ( Maui , Hawaii ) ( CIKM’12 ) . ACM , New York , 1995 – 1999 . https : / / doi . org / 10 . 1145 / 2396761 . 2398559 [ 17 ] Susanne Bødker . 2015 . Third - wave HCI , 10 years later – participation and sharing . Interactions 22 , 5 ( Aug . 2015 ) , 24 – 31 . https : / / doi . org / 10 . 1145 / 2804405 [ 18 ] Maja Brkan . 2017 . AI - supported decision - making under the general data protection regulation . In Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law ( London , UK ) ( ICAIL’17 ) . ACM , New York , 3 – 8 . https : / / doi . org / 10 . 1145 / 3086512 . 3086513 [ 19 ] Anna Brown , Alexandra Chouldechova , Emily Putnam - Hornstein , Andrew Tobin , and Rhema Vaithianathan . 2019 . Toward algorithmic accountability in public services : A qualitative study of affected community perspectives on algorithmic decision - making in child welfare services . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , Article 41 , 12 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300271 [ 20 ] Taina Bucher . 2017 . The algorithmic imaginary : Exploring the ordinary affects of facebook algorithms . Information Communication and Society 20 , 1 ( 2017 ) , 30 – 44 . https : / / doi . org / 10 . 1080 / 1369118X . 2016 . 1154086 [ 21 ] AndreaBunt , MatthewLount , andCatherineLauzon . 2012 . Areexplanationsalwaysimportant ? : Astudyofdeployed , low - cost intelligent interactive systems . In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces ( Lisbon , Portugal ) ( IUI’12 ) . ACM , New York , 169 – 178 . https : / / doi . org / 10 . 1145 / 2166966 . 2166996 [ 22 ] Jenna Burrell . 2016 . How the machine “thinks” : Understanding opacity in machine learning algorithms . Big Data & Society 3 , 1 ( 2016 ) , 2053951715622512 . https : / / doi . org / 10 . 1177 / 2053951715622512 [ 23 ] Ajay Chander , Ramya Srinivasan , Suhas Chelian , Jun Wang , and Kanji Uchino . 2018 . Working with beliefs : AI transparency in the enterprise . In Explainable Smart Systems Workshop at IUI 2018 . http : / / ceur - ws . org / Vol - 2068 / exss14 . pdf . [ 24 ] Sven Coppers , Kris Luyten , Davy Vanacken , David Navarre , Philippe Palanque , and Christine Gris . 2019 . Fortunettes : Feedforward about the future state of GUI widgets . In Proceedings of the ACM on Human - Computer Interaction 3 , ( EICS ) , Article 20 ( June 2019 ) , 20 pages . https : / / doi . org / 10 . 1145 / 3331162 [ 25 ] Fulvio Corno , Luigi De Russis , and Alberto Monge Roffarello . 2019 . Empowering end users in debugging trigger - action rules . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , Article 388 , 13 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300618 [ 26 ] Henriette Cramer , Bob Wielinga , Satyan Ramlal , Vanessa Evers , Lloyd Rutledge , and Natalia Stash . 2009 . The effects of transparency on perceived and actual competence of a content - based recommender . CEUR Workshop Proceedings 543 ( 2009 ) , 1 – 10 . https : / / doi . org / 10 . 1007 / s11257 - 008 - 9051 - 3 [ 27 ] Luigi De Russis and Alberto Monge Roffarello . 2018 . A debugging approach for trigger - action programming . In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI EA’18 ) . ACM , New York , Article LBW105 , 6 pages . https : / / doi . org / 10 . 1145 / 3170427 . 3188641 [ 28 ] Nicholas Diakopoulos . 2015 . Algorithmic accountability . Digital Journalism 3 , 3 ( 2015 ) , 398 – 415 . https : / / doi . org / 10 . 1080 / 21670811 . 2014 . 976411 [ 29 ] Nicholas Diakopoulos . 2016 . Accountability in algorithmic decision making . Commun . ACM 59 , 2 ( Jan . 2016 ) , 56 – 62 . https : / / doi . org / 10 . 1145 / 2844110 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 23 [ 30 ] NicholasDiakopoulos . 2017 . EnablingAccountabilityofAlgorithmicMedia : TransparencyasaConstructiveandCritical Lens . Springer International Publishing , Cham , 25 – 43 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 54024 - 5 _ 2 [ 31 ] Tim Donkers , Timm Kleemann , and Jürgen Ziegler . 2020 . Explaining recommendations by means of aspect - based transparent memories . In Proceedings of the 25th International Conference on Intelligent User Interfaces ( Cagliari , Italy ) ( IUI’20 ) . ACM , New York , 166 – 176 . https : / / doi . org / 10 . 1145 / 3377325 . 3377520 [ 32 ] Finale Doshi - Velez and Been Kim . 2017 . Towards a rigorous science of interpretable machine learning . ( February 2017 ) . arXiv : 1702 . 08608 http : / / arxiv . org / abs / 1702 . 08608 [ 33 ] JaimieDrozdal , JustinWeisz , DakuoWang , GauravDass , BingshengYao , ChangruoZhao , MichaelMuller , LinJu , and Hui Su . 2020 . Trust in AutoML : Exploring information needs for establishing trust in automated machine learning systems . In Proceedings of the 25th International Conference on Intelligent User Interfaces ( Cagliari , Italy ) ( IUI’20 ) . ACM , New York , 297 – 307 . https : / / doi . org / 10 . 1145 / 3377325 . 3377501 [ 34 ] JohnJ . DudleyandPerOlaKristensson . 2018 . Areviewofuserinterfacedesignforinteractivemachinelearning . ACM Transactions on Interactive Intelligent Systems 8 , 2 , Article 8 ( June 2018 ) , 37 pages . https : / / doi . org / 10 . 1145 / 3185517 [ 35 ] Lilian Edwards and Michael Veale . 2017 . Slave to the Algorithm ? Why a “Right to an Explanation” Is Probably Not the Remedy You Are Looking For . ( 2017 ) . https : / / doi . org / 10 . 2139 / ssrn . 2972855 arXiv : arXiv : 1802 . 01557v1 [ 36 ] UpolEhsan , Q . VeraLiao , MichaelMuller , MarkO . Riedl , andJustinD . Weisz . 2021 . ExpandingExplainability : Towards Social Transparency in AI Systems . ACM , New York . https : / / doi . org / 10 . 1145 / 3411764 . 3445188 [ 37 ] Malin Eiband . 2019 . Supporting Users in Understanding Intelligent Everyday Systems . http : / / nbn - resolving . de / urn : nbn : de : bvb : 19 - 256754 [ 38 ] Malin Eiband , Hanna Schneider , Mark Bilandzic , Julian Fazekas - Con , Mareike Haug , and Heinrich Hussmann . 2018 . Bringing transparency design into practice . In Proceedings of the 2018 Conference on Intelligent User Interfaces ( IUI’18 ) , 211 – 223 . https : / / doi . org / 10 . 1145 / 3172944 . 3172961 [ 39 ] Malin Eiband , Hanna Schneider , and Daniel Buschek . 2018 . Normative vs . pragmatic : Two perspectives on the design of explanations in intelligent systems . In Explainable Smart Systems Workshop at IUI 2018 . [ 40 ] Andreas Fischer , Samuel Greiff , and Joachim Funke . 2012 . The process of solving complex problems . Journal of Problem Solving ( 2012 ) . https : / / doi . org / 10 . 7771 / 1932 - 6246 . 1118 [ 41 ] Fatih Gedikli , Dietmar Jannach , and Mouzhi Ge . 2014 . How should i explain ? A comparison of different explanation types for recommender systems . International Journal of Human Computer Studies ( 2014 ) . https : / / doi . org / 10 . 1016 / j . ijhcs . 2013 . 12 . 007 [ 42 ] Leilani H . Gilpin , David Bau , Ben Z . Yuan , Ayesha Bajwa , Michael Specter , and Lalana Kagal . 2018 . Explain - ing explanations : An approach to evaluating interpretability of machine learning . ( 2018 ) . arXiv : 1806 . 00069 http : / / arxiv . org / abs / 1806 . 00069 [ 43 ] Barney G . Glaser and Anselm L . Strauss . 2017 . Discovery of Grounded Theory : Strategies for Qualitative Research . Routledge . [ 44 ] Alyssa Glass , Deborah L . McGuinness , and Michael Wolverton . 2008 . Toward establishing trust in adaptive agents . In Proceedings of the 13th International Conference on Intelligent User Interfaces ( Gran Canaria , Spain ) ( IUI’08 ) . ACM , New York , 227 – 236 . https : / / doi . org / 10 . 1145 / 1378773 . 1378804 [ 45 ] Peter M . Gollwitzer . 1993 . Goal achievement : The role of intentions . European Review of Social Psycholog 4 , 1 ( 1993 ) , 141 – 185 . https : / / doi . org / 10 . 1080 / 14792779343000059 [ 46 ] BryceGoodmanandSethFlaxman . 2017 . EuropeanUnion RegulationsonAlgorithmic Decision - Makinganda“Right to Explanation” . AI Magazine , 38 , 3 ( 2017 ) , 50 – 57 . https : / / doi . org / 10 . 1609 / aimag . v38i3 . 2741 [ 47 ] Hannes Grassegger and Mikael Krogerus . 2017 . The Data that Turned the World Upside Down . https : / / motherboard . vice . com / en _ us / article / mg9vvn / how - our - likes - helped - trump - win . Accessed : 01 . 10 . 2020 . [ 48 ] Gregory D . Hager , Randal Bryant , Eric Horvitz , Maja J . Mataric , and Vasant G . Honavar . 2017 . Advances in artificial intelligence require progress across all of computer science . CoRR abs / 1707 . 04352 ( 2017 ) . arXiv : 1707 . 04352 http : / / arxiv . org / abs / 1707 . 04352 [ 49 ] Jonathan L . Herlocker , Joseph A . Konstan , and John Riedl . 2000 . Explaining collaborative filtering recommendations . In Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work ( Philadelphia , PA ) ( CSCW’00 ) . ACM , New York , 241 – 250 . https : / / doi . org / 10 . 1145 / 358916 . 358995 [ 50 ] Alan R . Hevner , Salvatore T . March , Jinsoo Park , and Sudha Ram . 2004 . Design science in information systems re - search . MIS Q . 28 , 1 ( March 2004 ) , 75 – 105 . http : / / dl . acm . org / citation . cfm ? id = 2017212 . 2017217 [ 51 ] Mireille Hildebrandt . 2016 . The new imbroglio : Living with machine algorithms . The Art of Ethics in the Information Society . ( 2016 ) , 55 – 60 . [ 52 ] Rebecca Hill . 2017 . Transparent algorithms ? Here’s why that’s a bad idea , Google tells MPs . https : / / www . theregister . co . uk / 2017 / 11 / 07 / google _ on _ commons _ algorithm _ inquiry / . Accessed : 05 . 10 . 2020 . [ 53 ] Kenneth Holstein , Jennifer Wortman Vaughan , Hal Daumé , III , Miro Dudik , and Hanna Wallach . 2019 . Improving fairnessinmachinelearningsystems : Whatdoindustrypractitionersneed ? . In Proceedingsofthe2019CHIConference ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 24 D . Buschek et al . on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , Article 600 , 16 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300830 [ 54 ] Kristina Höök . 2000 . Steps to take before intelligent user interfaces become real . Interacting with Computers 12 , 4 ( 2000 ) , 409 – 426 . https : / / doi . org / 10 . 1016 / S0953 - 5438 ( 99 ) 00006 - 5 [ 55 ] Kasper Hornbæk and Antti Oulasvirta . 2017 . What is interaction ? . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( Denver , Colorado , ) ( CHI’17 ) . ACM , New York , 5040 – 5052 . https : / / doi . org / 10 . 1145 / 3025453 . 3025765 [ 56 ] Eric Horvitz . 1999 . Principles of mixed - initiative user interfaces . In Proceedings of the 1999 CHI Conference on Human Factors in Computing Systems ( CHI’99 ) . ACM , New York , 159 – 166 . https : / / doi . org / 10 . 1145 / 302979 . 303030 [ 57 ] Tim Hussein and Sebastian Neuhaus . 2010 . Explanation of spreading activation based recommendations . In Pro - ceedings of the 1st International Workshop on Semantic Models for Adaptive Interactive Systems ( Hong Kong , China ) ( SEMAIS’10 ) . ACM , New York , 24 – 28 . https : / / doi . org / 10 . 1145 / 2002375 . 2002381 [ 58 ] David Jackson and Andrew Fovargue . 1997 . The use of animation to explain genetic algorithms . In Proceedings of the 28th SIGCSE Technical Symposium on Computer Science Education ( San Jose , CA ) ( SIGCSE’97 ) . ACM , New York , 243 – 247 . https : / / doi . org / 10 . 1145 / 268084 . 268175 [ 59 ] Shagun Jhaver , Yoni Karpfen , and Judd Antin . 2018 . Algorithmic anxiety and coping strategies of Airbnb hosts . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( 2018 ) , 1 – 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3173995 [ 60 ] Hilary Johnson and Peter Johnson . 1993 . Explanation facilities and interactive systems . In Proceedings of the 1st InternationalConferenceonIntelligentUserInterfaces ( Orlando , FL ) ( IUI’93 ) . ACM , NewYork , 159 – 166 . https : / / doi . org / 10 . 1145 / 169891 . 169951 [ 61 ] Philip N . Johnson - Laird . 1989 . Mental models . In Foundations of Cognitive Science . The MIT Press , Cambridge , MA . [ 62 ] Ashish Kapoor , Bongshin Lee , Desney Tan , and Eric Horvitz . 2010 . Interactive optimization for steering machine classification . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Atlanta , GA ) ( CHI’10 ) . ACM , New York , 1343 – 1352 . https : / / doi . org / 10 . 1145 / 1753326 . 1753529 [ 63 ] Judy Kay and Bob Kummerfeld . 2013 . Creating personalized systems that people can scrutinize and control : Drivers , principles and experience . ACM Transactions on Interactive Intelligent Systems 2 , 4 , Article 24 ( January 2013 ) , 42 pages . https : / / doi . org / 10 . 1145 / 2395123 . 2395129 [ 64 ] Been Kim . 2015 . Interactive and Interpretable Machine Learning Models for Human Machine Collaboration . Ph . D . Dis - sertation . Massachusetts Institute of Technology ( MIT ) . [ 65 ] René F . Kizilcec . 2016 . How much information ? : Effects of transparency on trust in an algorithmic interface . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( San Jose , CA ) ( CHI’16 ) . ACM , New York , 2390 – 2395 . https : / / doi . org / 10 . 1145 / 2858036 . 2858402 [ 66 ] Janin Koch , Andrés Lucero , Lena Hegemann , and Antti Oulasvirta . 2019 . May AI ? Design ideation with cooperative contextual bandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , New York , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300863 [ 67 ] Rafal Kocielnik , Saleema Amershi , and Paul N . Bennett . 2019 . Will you accept an imperfect AI ? : Exploring designs for adjusting end - user expectations of AI systems . In Proceedings of the 2019 CHI Conference on Human Factors in ComputingSystems ( Glasgow , Scotland , UK ) ( CHI’19 ) . ACM , NewYork , Article 411 , 14pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300641 [ 68 ] Cliff Kuang . 2017 . Can A . I . Be Taught to Explain Itself ? https : / / www . nytimes . com / 2017 / 11 / 21 / magazine / can - ai - be - taught - to - explain - itself . html / . Accessed : 01 . 10 . 2020 . [ 69 ] Todd Kulesza , Margaret Burnett , Weng - Keen Wong , and Simone Stumpf . 2015 . Principles of explanatory debugging to personalize interactive machine learning . In Proceedings of the 20th International Conference on Intelligent User Interfaces ( Atlanta , GA ) ( IUI’15 ) . ACM , New York , 126 – 137 . https : / / doi . org / 10 . 1145 / 2678025 . 2701399 [ 70 ] Todd Kulesza , Simone Stumpf , Margaret Burnett , and Irwin Kwan . 2012 . Tell me more ? The effects of mental model soundness on personalizing an intelligent agent . In Proceedings of the 2012 ACM Conference on Human Factors in Computing Systems ( 2012 ) , 1 . https : / / doi . org / 10 . 1145 / 2207676 . 2207678 [ 71 ] Todd Kulesza , Simone Stumpf , Margaret Burnett , Weng - Keen Wong , Yann Riche , Travis Moore , Ian Oberst , Amber Shinsel , and Kevin McIntosh . 2010 . Explanatory debugging : Supporting end - user debugging of machine - learned programs . In Proceedings of the 2010 IEEE Symposium on Visual Languages and Human - Centric Computing . IEEE , 41 – 48 . https : / / doi . org / 10 . 1109 / VLHCC . 2010 . 15 [ 72 ] Todd Kulesza , Simone Stumpf , Margaret Burnett , Sherry Yang , Irwin Kwan , and Weng - Keen Wong . 2013 . Too much , too little , or just right ? Ways explanations impact end users’ mental models . In Proceedings of the 2013 IEEE Sympo - sium on Visual Languages and Human Centric Computing . 3 – 10 . https : / / doi . org / 10 . 1109 / VLHCC . 2013 . 6645235 [ 73 ] Todd Kulesza , Simone Stumpf , Weng - Keen Wong , Margaret M . Burnett , Stephen Perona , Andrew Ko , and Ian Oberst . 2011 . Why - oriented end - user debugging of naive Bayes text classification . ACM Transactions on Interactive Intelligent Systems 1 , 1 , Article 2 ( October 2011 ) , 31 pages . https : / / doi . org / 10 . 1145 / 2030365 . 2030367 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 25 [ 74 ] Todd Kulesza , Weng - Keen Wong , Simone Stumpf , Stephen Perona , Rachel White , Margaret M . Burnett , Ian Oberst , and Andrew J . Ko . 2008 . Fixing the program my computer learned . In Proceedings of the 13th International Conference on Intelligent User Interfaces ( IUI’09 ) . 187 – 196 . https : / / doi . org / 10 . 1145 / 1502650 . 1502678 [ 75 ] Markus Langer , Daniel Oster , Timo Speith , Holger Hermanns , Lena Kästner , Eva Schmidt , Andreas Sesing , and Kevin Baum . 2021 . What do we want from explainable artificial intelligence ( XAI ) ? - A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research . Artificial Intelligence 296 ( 2021 ) , 103473 . https : / / doi . org / 10 . 1016 / j . artint . 2021 . 103473 [ 76 ] David M . J . Lazer , MatthewA . Baum , Yochai Benkler , Adam J . Berinsky , Kelly M . Greenhill , FilippoMenczer , MiriamJ . Metzger , Brendan Nyhan , Gordon Pennycook , David Rothschild , Michael Schudson , Steven A . Sloman , Cass R . Sunstein , Emily A . Thorson , Duncan J . Watts , and Jonathan L . Zittrain . 2018 . The science of fake news . Science 359 , 6380 ( 2018 ) , 1094 – 1096 . https : / / doi . org / 10 . 1126 / science . aao2998 arXiv : http : / / science . sciencemag . org / content / 359 / 6380 / 1094 . full . pdf [ 77 ] J . D . Lee and K . A . See . 2004 . Trust in automation : Designing for appropriate reliance . Human Factors : The Journal of the Human Factors and Ergonomics Society 46 , 1 ( 2004 ) , 50 – 80 . https : / / doi . org / 10 . 1518 / hfes . 46 . 1 . 50 _ 30392 [ 78 ] Brian Y . Lim . 2010 . Improving trust in context - aware applications with intelligibility . In Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing ( Copenhagen , Denmark ) ( UbiComp’10 Adjunct ) . ACM , New York , NY , USA , 477 – 480 . https : / / doi . org / 10 . 1145 / 1864431 . 1864491 [ 79 ] Brian Y . Lim and Anind K . Dey . 2009 . Assessing demand for intelligibility in context - aware applications . In Proceed - ings of the 11th International Conference on Ubiquitous Computing ( UbiComp’09 ) . 195 – 204 . https : / / doi . org / 10 . 1145 / 1620545 . 1620576 [ 80 ] Brian Y . Lim and Anind K . Dey . 2011 . Design of an intelligible mobile context - aware application . In Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services ( MobileHCI’11 ) . 157 – 166 . https : / / doi . org / 10 . 1145 / 2037373 . 2037399 [ 81 ] Brian Y . Lim , Anind K . Dey , and Daniel Avrahami . 2009 . Why and why not explanations improve the intelligibility of context - aware intelligent systems . In Proceedings of the 27th International Conference on Human Factors in Computing Systems , 2119 – 2128 . https : / / doi . org / 10 . 1145 / 1518701 . 1519023 [ 82 ] Zachary C . Lipton . 2018 . The mythos of model interpretability . Queue 16 , 3 , Article 30 ( June 2018 ) , 27 pages . https : / / doi . org / 10 . 1145 / 3236386 . 3241340 [ 83 ] Susan A . Lynham . 2002 . The general method of theory - building research in applied disciplines . Advances in Devel - oping Human Resources 4 , 3 ( 2002 ) , 221 – 241 . https : / / doi . org / 10 . 1177 / 1523422302043002 [ 84 ] Brent D . Mittelstadt , Patrick Allo , Mariarosaria Taddeo , Sandra Wachter , and Luciano Floridi . 2016 . The ethics of algorithms : Mapping the debate . Big Data & Society 3 , 2 ( 2016 ) , 205395171667967 . https : / / doi . org / 10 . 1177 / 2053951716679679 [ 85 ] Christoph Molnar . 2020 . Interpretable Machine Learning . https : / / christophm . github . io / interpretable - ml - book / . [ 86 ] Gina Neff and Peter Nagy . 2016 . Automation , algorithms , and politics | talking to bots : Symbiotic agency and the case of tay . International Journal of Communication 10 ( 2016 ) . http : / / ijoc . org / index . php / ijoc / article / view / 6277 . [ 87 ] Donald A . Norman . 2013 . The Design of Everyday Things . https : / / doi . org / 10 . 1002 / hfm . 20127 arXiv : arXiv : 1011 . 1669v3 [ 88 ] Ingrid Nunes and Dietmar Jannach . 2017 . A systematic review and taxonomy of explanations in decision support and recommender systems . User Modeling and User - Adapted Interaction 27 , 3 ( 01 December 2017 ) , 393 – 444 . https : / / doi . org / 10 . 1007 / s11257 - 017 - 9195 - 0 [ 89 ] Changhoon Oh , Taeyoung Lee , Yoojung Kim , SoHyun Park , Sae bom Kwon , and Bongwon Suh . 2017 . Us vs . them : Understanding artificial intelligence technophobia over the Google Deepmind challenge match . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( Denver , CO ) ( CHI’17 ) . ACM , New York , 2523 – 2534 . https : / / doi . org / 10 . 1145 / 3025453 . 3025539 [ 90 ] Raja Parasuraman and Christopher A . Miller . 2004 . Trust and etiquette in high - criticality automated systems . Com - mun . ACM 47 , 4 ( 2004 ) , 51 . https : / / doi . org / 10 . 1145 / 975817 . 975844 [ 91 ] The European Parliament and the Council of the European Union . 2016 . General Data Protection Regulation . https : / / gdpr - info . eu / . Accessed : 09 . 10 . 2020 . [ 92 ] WolterPieters . 2011 . Explanationandtrust : whattotelltheuserinsecurityandAI ? EthicsandInformationTechnology 13 , 1 ( 01 Mar 2011 ) , 53 – 64 . https : / / doi . org / 10 . 1007 / s10676 - 010 - 9253 - 3 [ 93 ] Pearl Pu and Li Chen . 2006 . Trust building with explanation interfaces . In Proceedings of the 11th International Con - ference on Intelligent User Interfaces ( IUI’06 ) . 93 – 100 . https : / / doi . org / 10 . 1145 / 1111449 . 1111475 [ 94 ] Emilee Rader , Kelley Cotter , and Janghee Cho . 2018 . Explanations as mechanisms for supporting algorithmic trans - parency . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 103 , 13 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3173677 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . 29 : 26 D . Buschek et al . [ 95 ] Arpit Rana and Derek Bridge . 2018 . Explanations that are intrinsic to recommendations . In Proceedings of the 26th Conference on User Modeling , Adaptation and Personalization ( Singapore , Singapore ) ( UMAP’18 ) . ACM , New York , 187 – 195 . https : / / doi . org / 10 . 1145 / 3209219 . 3209230 [ 96 ] Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 . “Why should I trust you ? ” : Explaining the predictions of any classifier . In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( San Francisco , CA ) ( KDD’16 ) . ACM , New York , 1135 – 1144 . https : / / doi . org / 10 . 1145 / 2939672 . 2939778 [ 97 ] Andrew Ross , Nina Chen , Elisa Zhao Hang , Elena L . Glassman , and Finale Doshi - Velez . 2021 . Evaluating the inter - pretability of generative models by interactive reconstruction . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . ACM , New York , Article 80 , 15 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445296 [ 98 ] Advait Sarkar . 2015 . Confidence , command , complexity : Metamodels for structured interaction with machine intelligence . In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group . 23 – 36 . http : / / www . ppig . org / library / paper / confidence - command - complexity - metamodels - structured - interaction - machine - intelligence . [ 99 ] Advait Sarkar . 2016 . Constructivist design for interactive machine learning . In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems ( San Jose , CA ) ( CHI EA’16 ) . ACM , New York , 1467 – 1475 . https : / / doi . org / 10 . 1145 / 2851581 . 2892547 [ 100 ] Ari Schlesinger , Kenton P . O’Hara , and Alex S . Taylor . 2018 . Let’s talk about race : Identity , chatbots , and AI . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 315 , 14 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3173889 [ 101 ] Wolfgang Schoppek . 2002 . Examples , rules , and strategies in the control of dynamic systems . Cognitive Science Quar - terly 2 , 1 ( 2002 ) , 63 – 92 . [ 102 ] Johanneke Siljee . 2015 . Privacy transparency patterns . In Proceedings of the 20th European Conference on Pattern Languages of Programs ( Kaufbeuren , Germany ) ( EuroPLoP’15 ) . ACM , New York , Article 52 , 11 pages . https : / / doi . org / 10 . 1145 / 2855321 . 2855374 [ 103 ] Munindar P . Singh . 1994 . Multiagent Systems . Springer Berlin Heidelberg , Berlin , Heidelberg , 1 – 14 . https : / / doi . org / 10 . 1007 / BFb0030532 [ 104 ] Steffen Staab , Bharat Bhargava , Leszek Lilien , Arnon Rosenthal , Marianne Winslett , Morris Sloman , Tharam S . Dillon , Elizabeth Chang , Farookh Khadeer Hussain , Wolfgang Nejdl , Daniel Olmedilla , and Vipul Kashyap . 2004 . The pudding of trust . IEEE Intelligent Systems 19 , 5 ( 2004 ) , 74 – 88 . https : / / doi . org / 10 . 1109 / MIS . 2004 . 52 [ 105 ] Anselm Strauss and Juliet Corbin . 1998 . Basics of Qualitative Research Techniques . SAGE . [ 106 ] Justin Talbot , Bongshin Lee , Ashish Kapoor , and Desney S . Tan . 2009 . EnsembleMatrix : Interactive visualization to support machine learning with multiple classifiers . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Boston , MA , ) ( CHI’09 ) . ACM , New York , 1283 – 1292 . https : / / doi . org / 10 . 1145 / 1518701 . 1518895 [ 107 ] Nava Tintarev and Roman Kutlak . 2014 . Demo : Making plans scrutable with argumentation and natural language generation . In Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Inter - faces ( Haifa , Israel ) ( IUI Companion’14 ) . ACM , New York , 29 – 32 . https : / / doi . org / 10 . 1145 / 2559184 . 2559202 [ 108 ] Richard Tomsett , Dave Braines , Dan Harborne , Alun Preece , and Supriyo Chakraborty . 2018 . Interpretable to whom ? A role - based model for analyzing interpretable machine learning systems . arXiv preprint arXiv : 1806 . 07552 ( 2018 ) . [ 109 ] Chun - Hua Tsai and Peter Brusilovsky . 2019 . Evaluating visual explanations for similarity - based recommendations : User perception and performance . In Proceedings of the 27th ACM Conference on User Modeling , Adaptation and Personalization ( Larnaca , Cyprus ) ( UMAP’19 ) . ACM , New York , 22 – 30 . https : / / doi . org / 10 . 1145 / 3320435 . 3320465 [ 110 ] Chun - Hua Tsai and Peter Brusilovsky . 2019 . Explaining recommendations in an interactive hybrid social recom - mender . In Proceedings of the 24th International Conference on Intelligent User Interfaces ( Marina del Ray , CA ) ( IUI’19 ) . ACM , New York , 391 – 396 . https : / / doi . org / 10 . 1145 / 3301275 . 3302318 [ 111 ] Joe Tullio , Anind K . Dey , Jason Chalecki , and James Fogarty . 2007 . How it works : A field study of non - technical users interacting with an intelligent system . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI’07 ) . 31 – 40 . https : / / doi . org / 10 . 1145 / 1240624 . 1240630 [ 112 ] Michael Veale , Max Van Kleek , and Reuben Binns . 2018 . Fairness and accountability design needs for algorithmic support in high - stakes public sector decision - making . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI’18 ) . ACM , New York , Article 440 , 14 pages . https : / / doi . org / 10 . 1145 / 3173574 . 3174014 [ 113 ] Sarah Theres Völkel , Christina Schneegass , Malin Eiband , and Daniel Buschek . 2020 . What is “Intelligent” in intelli - gent user interfaces ? A meta - analysis of 25 years of IUI . In Proceedings of the 25th International Conference on Intel - ligent User Interfaces ( Cagliari , Italy ) ( IUI’20 ) . ACM , New York , 477 – 487 . https : / / doi . org / 10 . 1145 / 3377325 . 3377500 [ 114 ] Ernst von Glasersfeld . 1989 . Cognition , construction of knowledge , and teaching . Synthese ( 1989 ) . https : / / doi . org / 10 . 1007 / BF00869951 arXiv : arXiv : 1011 . 1669v3 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 . How to Support Users in Understanding Intelligent Systems ? 29 : 27 [ 115 ] Sandra Wachter , Brent Mittelstadt , and Luciano Floridi . 2017 . Transparent , explainable , and accountable AI for ro - botics . Science Robotics 2 , 6 ( 2017 ) . https : / / doi . org / 10 . 1126 / scirobotics . aan6080 arXiv : 2794335 [ 116 ] Sandra Wachter , Brent Mittelstadt , and Luciano Floridi . 2017 . Why a right to explanation of automated decision - making does not exist in the general data protection regulation . SSRN ( 2017 ) , 1 – 47 . https : / / doi . org / 10 . 2139 / ssrn . 2903469 arXiv : 1606 . 08813 [ 117 ] Jing Nathan Yan , Ziwei Gu , Hubert Lin , and Jeffrey M . Rzeszotarski . 2020 . Silva : Interactively Assessing Machine Learning Fairness Using Causality . ACM , New York , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376447 [ 118 ] Fumeng Yang , Zhuanyi Huang , Jean Scholtz , and Dustin L . Arendt . 2020 . How do visual explanations foster end users’ appropriate trust in machine learning ? . In Proceedings of the 25th International Conference on Intelligent User Interfaces ( Cagliari , Italy ) ( IUI’20 ) . Association for Computing Machinery , New York , NY , USA , 189 – 201 . https : / / doi . org / 10 . 1145 / 3377325 . 3377480 [ 119 ] QianYang , AaronSteinfeld , CarolynRosé , andJohnZimmerman . 2020 . Re - examiningwhether , why , andhowhuman - AIinteractionisuniquelydifficulttodesign . In Proceedingsofthe2020CHIConferenceonHumanFactorsinComputing Systems ( Honolulu , HI , USA ) ( CHI’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376301 Received 28 July 2021 ; revised 26 November 2021 ; accepted 15 February 2022 ACM Transactions on Interactive Intelligent Systems , Vol . 12 , No . 4 , Article 29 . Publication date : November 2022 .