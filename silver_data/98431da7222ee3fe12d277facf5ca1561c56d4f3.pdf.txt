32 IERE TRANSACTIONS ON INFORMATION THEORY , VOL . IT - 21 , NO . 1 , JANUARY 1975 T h e E stim ation of the Gradient of a D e n sity F u n ction , with A p plications in P attern - R e c o g nition KEINOSUKE FUKUNAGA , SENIOR MEMBER , IEEE , AND LARRY D . HOSTETLER , MEMBER , IEEE Abstract - Nonparametric density gradient estimation using a gen - eralized kernel approach is investigated . Conditions on the kernel func - tions are derived to guarantee asymptotic unbiasedness , consistency , and uniform consistenby of the estimates . The results are generalized to obtain a simple mean - shift estimate that can be extended in a k - nearest - neighbor approach . Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems , with the ultimate goal of providing further understanding of these problems in terms of density gradients . I . INTRODUCTION N ONPARAMETRIC estimation of probability density functions is based on the concept that the value of a density function at a continuity point can be estimated using the sample observations that fall within a small region around that point . This idea was perhaps first used in Fix and Hodges’ [ l ] original work . Rosenblatt [ 2 ] , Parzen [ 3 ] , and Cacoullos [ 4 ] generalized these results atid developed the Parzen kernel class of density estimates . These estimates were shown to be asymptotically unbiased , consistent in a mean - square sense , and uniformly consistent ( in probability ) . Additional work by Nadaraya [ S ] , later extended by Van Ryzin [ 6 ] to n dimensions , showed strong and uniformly strong consistency ( with probability one ) . Similarly , the gradient of a probability density function can be estimated using the sample observations within a small region . In this paper we will use these concepts and results to derive a general kernel class of density gradient estimates . Although Bhattacharyya [ 7 ] and Schuster [ 8 ] considered estimating the density and all its derivatives in the univariate case , we are interested in the multivariate gradient , and our results follow more closely the Parzen kernel estimates previously mentioned . In Section II , we will develop the general form of the kernel gradient estimates and derive conditions on the kernel functions to assure asymptotically unbiased , con - sistent , and uniformly consistent estimates . By examining the form of the gradient estimate when a Gaussian kernel is used , in Section III we will be able to develop a simple mean - shift class of estimates . The approach uses the fact that the expected value of the observations within a small region about a point can be related to the density gradient Manuscript received February 1 , 1973 ; revised June 13 , 1974 . This work was supported in part by the National Science Foundation under Grant GJ - 35722 and in part by Bell Laboratories , Madison , N . J . K . Fukunaga is with the School of Electrical Engineering , Purdue University , Lafayette , Ind . 47907 . L . D . I ; Iostetler was with Bel ! Laboratories , Whippany and Madison , fj ; lFe 1s now with the Sandra Laboratones , Albuquerque , N . Mex . at that point . A simple modification results in a k - nearest - neighbor mean - shift class of estimates . This modification is similar to Loftsgaarden and Quesenberry’s [ 9 ] k - nearest - neighbor density estimates , in that the number of observa - tions within the region is fixed whereas in the previous estimates the volume of the region was fixed . In Section IV , applications of density gradient estimation to pattern recognition problems are investigated . By taking a mode - seeking approach , a recursive clustering algorithm is developed , and its properties studied . Interpretations of our results are presented along with examples . Applications to data filtering and ilitrinsic dimensionality determination are also . discussed . Section V is a summary . II . ESTIMATION OF THE DENSITY GRADIENT In most pattern recognition problems , very little if any information is available as to the true probability density function or even as to its form . Due to this lack of know - ledge about the density , we have to rely on nonparametric techniques to obtain density gradient estimates . A straight - forward approach for estimating a density gradient would be to first dbtain a differentiable nonparametric estimate of the probability density function and then take its gradient . The nonparametric density estimates we will use are Cacoullos’ [ 4 ] multivariate extensions of Parzen’s [ 3 ] univariate kernel estimates . A . Proposed Gradient Estimates Let X1 , X , , * * - , X , be a set of N independent and identi - cally distributed n - dimensional random vectors . Cacoullos [ 4 ] investigated multivariate kernel density estimators of the form MO = W” ) - l j $ l W + ( X where k ( X ) is a scalar function satisfying sup IW ) l < 00 YeR” s Ik ( Y ) I dY < 00 R” lim ) I Y link ( Y ) = 0 IIYII + m s k ( Y ) dY = 1 R” - xj > > ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) and where II * II is the ordinary Euclidean norm , and R” is the n - dimensional feature space . The parameter h , which FUKUNAGA AND HOSTETLER : GRADIENT OF DENSITY FUNCTION 33 is a function of the sample size N , is chosen to satisfy lim h ( N ) = 0 ( 6 ) N - W functions that satisfy lixq , p ( X ) = 0 , Ilxll - ~ ( 15 ) to guarantee asymptotic unbiasedness of the estimate . Mean - square consistency of the estimate is assured by the condition lim A % “ ( N ) = co . ( 7 ) N - tm Although the condition ( 15 ) must be imposed on the density function , practically all density functions satisfy this condition . Thus lim E { Q , p , ( X ) ) = V , p ( X ) N - CO ( 16 ) Uniform consistency ( in probability ) is assured by the at the points of continuity of V , p ( X ) , provided that 1 ) condition p ( X ) satisfies ( 15 ) ; 2 ) h ( N ) goes to zero as N - + co ( see lim FIJI ? “ ( N ) = co , ( 8 ) ( 6 ) ) ; 3 ) k ( X ) satisfies ( 2 ) - - ( 5 ) ; and 4 ) sR” Iap ( Y ) / ayil dY < CO , N’G ) for i = 1 , 2 ; * * , n . provided the true density p ( X ) is uniformly continuous . Additional conditions were found by Van Ryzin [ 6 ] to assure strong and uniformly strong consistency ( with probability one ) . The proof is given in the Appendix . C . Consistency Motivated by this general class of nonparametric density estimates , we use the differentiable kernel function and then estimate the density gradient as the gradient of the density estimate ( 1 ) . This gives the density gradient estimate The estimate vxpN ( X ) is consistent in quadratic mean , lim E { IIQ , PN ( X ) - V , P < X ) II ~ ) = 0 ( 17 ) N - too at the points of continuity of V , p ( X ) , provided that the following conditions are . satisfied 6 , p , ( X ) E ( Nh” ) - 1 5 V , k ( h - l ( X - X , ) ) j = l ( 9 ) 1 ) lim h ( N ) = 0 ( 18 ) N + CX lim N / I” + ‘ ( N ) = co ( 19 ) N - + CC = ( Nh” + l ) - ’ j $ l Vk ( h - ‘ ( X - X , ) ) ( 10 ) Vk ( Y ) = ak ( Y ) ak ( Y ) ay , ‘ - ‘“” 8Y2 ( 11 ) and V , is the usual gradient operator with respect to x1 , x2 , * * * , x , . Equation ( 10 ) is the general form of our density gradient estimates . As in density estimation , this is a kernel class of estimates , and various kernel functions k ( Y ) may be used . Conditions on the kernel functions and h ( N ) will now be derived to guarantee asymptotic unbiasedness , consistency , and uniform consistency of the estimate . B . Asymptotic Unbiasedness The basic result needed for the proof of asymptotic unbiasedness is [ 4 , theorem 2 . 11 . This theorem , given here without proof , states that if the function k ( X ) satisfies conditions ( 2 ) - ( 4 ) , h ( N ) satisfies condition ( 6 ) , and g ( X ) is any other function such that s Is ( dY < * , ( 12 ) R” then the sequence of functions gN ( X ) defined by gN ( X ) = h - “ ( N ) s k ( h - ‘ ( N ) Y ) g ( X - Y ) dY ( 13 ) R” converges at every point of continuity of g ( X ) to lim gNcX ) = dx ) s k ( Y ) dY . ( 14 ) N - t03 R” Using this we will be able to show that the estimate 6 , pN ( X ) is asymptotically unbiased for a class of density and in addition to ( 2 ) - ( 5 ) the kernel function is such that 2 ) sup Iki’ ( Y ) I < 00 YER” WY s Iki’ ( Y ) l dY ~ 00 ( 21 ) R” where lim I [ Y / “k , ‘ ( Y ) = 0 ( 22 ) IIYII - rm k , ‘ ( Y ) E y . 1 ( 23 ) The proof is given in the Appendix . The additional power of two in the condition ( 19 ) on h ( N ) comes from estimating gradients and not just the density so that in ( 17 ) , 2 , k ( h - l ( X - ’ Y ) ) 1 = h - 2 [ ki’ ( h - ‘ ( X - Y ) ) 12 ( 24 ) . b with the additional h - ’ being generated . This additional power requires h ( N ) to go to zero slightly slower than in the previous case ( 7 ) for density estimation alone . Since we may want to use the estimate of the density gradient over the ent . ire space in the same application and not just be satisfied with pointwise properties , we would like to determine the conditions under which the gradient estimate is uniformly consistent . D . Uniform Consistency The gradient estimate QxpN ( X ) is uniformly consistent . That is , for every F > 0 , lim Pr { sup II $ , pN ( X ) - V , p ( X ) ll > E } = 0 . ( 25 ) N - + m R” 34 IEEE TRANSACTIONS ON INFORMATION THEORY , JANUARY 1975 The conditions to be satisfied are listed as follows 0 lim h ( N ) * 0 N - C . 3 lim Nh2” + 2 ( N ) = cc N - tCO 2 ) the characteristic function G ( W ) = [ exp ( jWrX ) V , k ( X ) dX JR " of V & ( X ) is absolutely integrable ( i . e . , s IIWVII dW < ~ 0 , where W = [ or - R " 3 ) V , p ( X ) is uniformly continuous . . . ( 26 ) ( 27 ) % lT ) ( 28 ) ( 29 ) The proof follows from the definition of Vx & X ) and the properties of characteristic functions , the details of which are given in the Appendix . Again we see that the additional power of two in the condition ( 27 ) on h ( N ) is present . III . SPECIAL KERNEL FUNCTIONS Having derived a general class of probability density gradient estimates and shown it to have certain desirable properties , we will now investigate specific kernel functions in order to better understand the underlying process involved in gradient estimation . Ultimately this results in a simple and very intuitive mean - shift estimate for the density gradient . A simple modification then extends this in a k - nearest - neighbor approach to gradient estimation much in the same manner as Loftsgaarden and Quesenberry’s [ 9 ] extension of kernel density estimates . A . Mean - Shift Gradient Estimates The Gaussian kernel function is perhaps the best known differentiable multivariate kernel function satisfying the conditions for asymptotic unbiasedness , consistency , and uniform consistency of the density gradient estimate . The Gaussian probability density kernel function with zero mean and identity covariance matrix is k ( X ) = ( 27 ~ ) - “ / ~ exp ( - + XTX ) . ( 30 ) Taking the gradient of ( 30 ) and substituting it into ( 10 ) for Vk ( X ) , we obtain as the estimate of the density gradient ? xPN ( x ) = N - ’ 2 ( xi _ X ) ( 2n ) - “ / 2h - ( “ + 2 ) i = l * exp [ - ( X - Xi ) T ( y ) ] * ( 31 ) An intuitive interpretation of ( 31 ) is that it is essentially a weighted measure of the mean shift of the observations about the point X . The shift Xi - X of each point from X is calculated and multiplied by the weighting factor h - ( “ + 2 ) . w - ‘1’ exp ( - ( X - Xi ) T ( X - Xi ) / 2h2 ) . The sample mean of these weighted shifts is then taken as the gradient estimate . The same general form will result if the kernel function is of the form k ( X ) = g ( XTX ) . ( 32 ) The most simple kernel with this form is k ( X ) = 1 c ( l - XTX ) , XTX I ; 1 o xTx > 1 ( 33 ) 3 where ( 34 ) is the normalizing constant required to make the kernel function integrate to one and l ? ( w ) is the gamma function . This kernel function can easily be shown to satisfy all the conditions for asymptotic unbiasedness , consistency , and uniform consistency of the gradient estimate and is , in a sense , quite similar to the asymptotic optimum product kernel discussed by Epanechnikov [ lo ] . Substituting ( 33 ) into ( lo ) , we obtain as the gradient estimate oxpNtx ) = = where ( Nh” + 2 ) - 12c C ( Xi - X ) xi Es & v s n n / 2 Q ( X ) = dy = hs’c & m I - ( n + 2 / 2 ) ( 36 ) is the volume of the region S , ( X ) = { Y : ( Y - X ) T ( Y - X ) I h2 } ( 37 ) and k is the number of observations falling within S , , ( X ) and , therefore , the number in the sum . Equation ( 35 ) provides us with an excellent interpretation of the gradient estimation process . The last term in ( 35 ) is the sample mean shift MhtX ) E t x Eqh ( x ) txi - x > ( 38 ) i of the observations in the small region S , ( X ) about X . Clearly , if the gradient or slope is zero , corresponding to a uniform density over the region S , , ( X ) , the average mean shift would be zero due to the symmetry of the observations near X . However , with a nonzero density gradient pointing in the direction of most rapid increase of the probability density function , on the average more observations should fall along its direction than elsewhere in S , ( X ) . Correspondingly , the average mean shift should point in that direction and have a length proportional to the magnitude of the gradient . B . Mean - Shift Normalized Gradient Estimates Examining the proportionality constant in ( 35 ) , we see that it contains a term identical to the probability density estimate using a uniform kernel function over the region FUKUNAGA AND HOSTJiTLER : GRADIENT OF DENWI’Y FUNCTION 35 By taking this to the left side of ( 35 ) and using the properties of the function In y , ‘we see that the mean shift can be used as an estimate of the normalized gradient VAX ) ~ = V , In p ( X ) P ( X ) $ , ln pN ( x ) = F Mb ( X ) . ( 40 ) This mean - shift estimate of the normalized gradient has a pleasingly simple and easily calculated expression ( 41 ) . It can also be given the same intuitive interpretation that was just given in the previous section for the gradient estimate . The normalized gradient can be used in most applications in place of the regular gradient , and , as will be seen in Section IV , it has desirable properties for pattern recognition applications . This mean - shift estimate can easily be generalized to a k - nearest - neighbor approach to normalized gradient esti - mation . Letting h be replaced by the value of dk ( X ) , the distance to the k - nearest - neighbor of X , and S , , ( X ) be replaced by S , , ( X ) = { Y : II Y - X II I dk } ( 42 ) we obtain for the k - nearest - neighbor mean - shift estimate of V , ln PQ , 9 , In pN ( x ) = ( 43 ) Thus we have developed both a kernel and a k - nearest - neighbor approach to normalized gradient estimation . What , if any , limiting properties , such as asymptotic unbiasedness , consistency , and uniform consistency , carry over from the kernel estimates to the k - nearest - neighbor case is still an open research problem . IV . APPLICATIONS In this section we will show how gradient estimates can be applied to pattern recognition problems . A . A Gradient Clustering Algorithm From Fig . 1 , we see that one method of clustering a set of observations into different classes would be to assign each observation to the nearest mode along the direction of the gradient at the observation points . To accomplish this , one could move each observation a small step in the direction of the gradient and iteratively repeat the process on the transformed observations until tight clusters result near the modes . Another approach would be to shift each observation by some amount proportional to the gradient at the observation point . This transformation approach is the one we will investigate in this section since it is in - Xl x2 x3 x4 ‘5 ‘6 Fig . 1 . Gradient mode clustering . x7 tuitively appealing and will be shown to have good physical motivation behind its application . Letting x0 E x . j J’ j = 1 , 2 ; * ~ , iv 04 we will transform each observation recursively according to the clustering algorithm xj + l = Xj’ + aV , In p ( Xj’ ) . ( 45 ) where a is an appropriately chosen positive constant to guarantee convergence . This is the n - dimensional analog of the linear iteration technique for stepping into the roots of the equation v . dw = 0 ( 46 ) and equivalently the modes of the mixture density p ( X ) . Although local minima are also roots of ( 46 ) , it is easy to see that the algorithm ( 45 ) will move observations away from these points since the gradients point away from them . Thus after each iteration each observation will have moved closer to its parent mode or cluster center . The use of the normalized gradient V , In p ( X ) = V , p ( X ) / p ( X ) in place of V , . . ( X ) in ( 45 ) can be justified from the following four points of view . 1 ) The first is that in the tails of the density and near local minima where p ( X ) is relatively small it will be true that V , In p ( X ) = V , p ( X ) / p ( X ) > V , p ( X ) . Thus the cor - responding step size for the same gradient will be greater than that near a mode . This will allow observations far from the mode or near a local minimum to move towards the mode faster than using V , p ( X ) alone . 2 ) The second reason for using the normalized gradient can be seen if a Gaussian density with mean M and identity covariance matrix is considered p ( X ) = ( 27 ~ ) ~ “‘ ~ exp [ - 4 ( X - M ) T ( X - M ) ] . ( 47 ) Then if a is taken to be one , ( 45 ) becomes Xi1 = Xj + V , In p ( Xj ) = Xj - ( Xj - M ) = M . ( 48 ) Thus for this Gaussian density the correct choice of a allows the clusters to condense to single points after one iteration . This fact gives support for its use in the general mixture density problem in which the individual mixture component densities often look Gaussian . 36 IEEl TRANSACTCONS ON INFORMATION THEORY , JANUARY 1975 3 ) The third reason for using the normalized gradient is that we can show convergence in a mean - square sense . In other words , if we make the transformation of random variables Y = X + aV , lnP ( X ) ( 49 ) then by selecting a proper a the covariance of our samples will get smaller , E { Y - My ) T ( Y - My ) } < E ( ( X - MJT ( X - Mx ) } ( 50 ) representing a tightening up of the clusters , where My = E { Y } and M , = E ( X ) . ( 51 ) Letting 2 = V , lnp ( X ) ( 52 ) and Mz = E { Z } ( 53 ) we obtain from ( 49 ) , E { P’ - MdT ( Y - MY ) } = E { ( X - MX ) T ( X - Mx ) } + 2aE { ( X - MX ) T ( Z - MJ } f a2E { ( Z - Mz ) T ( Z - Mz ) } . ( 54 ) Now the ith component of Mz is , from ( 52 ) , ( E { ZI ) i e ( S , . F P ( Y ) dY ) i = S , . c p ( Y ) dY i = s [ p ( Y ) I ; = - , ] dY = 0 . ( 55 ) p - t The last equality holds when the density function satisfies ( 15 ) . Thus the second term of ( 54 ) becomes , upon using ( 55 ) and integrating by parts , E { tX - MxjTV - Mz ) ) = E ( XT21 = l $ l S , . Yi g P ( Y ) dY i = f1 ( s p - 1 CYiP ( Y ) I ~ = “ - mI dY - 1 P ( Y ) dY ] R” = - n where we have assumed ( 56 ) lim Xp ( X ) = 0 . Ilxll + ~ ( 57 ) Using ( 55 ) and ( 56 ) in ( 54 ) , we obtain E { tY - MdTtY - MY ) ) = E { ( X - MX ) T ( X - M , ) ) - 2an + a2E { ZTZ } . ( 58 ) Thus for convergence in a mean - square sense all that is required is that a be chosen small enough ; in particular , from ( 58 ) , 2n O < a < - - - - - . E { Z’Z } ( 59 ) The optimum a , optimum in the sense of decreasing ( 54 ) as much as possible , is obtained by differentiating ( 58 ) with respect to a and noting that E { ZTZ } is greater than zero to obtain a opt = & * This analysis gives us some insight into the choice of the parameter a . If a is too small , then the observations move only slightly towards the modes , but the variance decreases . If a is increased to the optimum value , then the step size is just right , and the variance decreases as much as possible . As a is made larger still , we begin to slightly overshoot the mode , but the variance still has decreased since it only involves distances from the mode . Finally , if a is made too large , we greatly overshoot the mode with the resulting variance being larger than that before the iteration . Thus one should be conservative in the choice of a so that although more iterations may be required to achieve tight clusters , at least the algorithm will not diverge . 4 ) The fourth reason for using the normalized gradient is that it can be estimated directly using the mean - shift estimate ( 41 ) . When a Gaussian kernel function is used , the normalized gradient cannot be estimated directly , and we have to estimate both V , p ( X ) and P ( X ) separately and take their ratio . B . Clustering Applications As an example of gradient estimation in clustering , the algorithm of ( 45 ) was implemented on a computer using both the Gaussian kernel function approach and the sample mean - shift estimate ( 41 ) for V , In p ( X ) . The data for these experiments were computer - generated bivariate - normal random vectors . Sixty observations were obtained from each of three classes , each having identity covariance matrix and the following different means : Ml = 8 M2 = [ I ( 61 ) These distributions are slightly overlapping , the original data is shown in Fig . 2 . By recursively applying the cluster - ing algorithm ( 45 ) using gradient estimates , the data are transformed into three clusters . Fig . 3 shows the resulting transformed data for the Gaussian kernel estimate algorithm with a = 0 . 5 and h = 0 . 8 , and for the mean - shift estimate algorithm with a = 0 . 75 and h = 1 . 5 . Fig . 4 shows the resulting class assignments using the different algorithms . These results show that gradient estimation is indeed applicable to clustering problems . The choice of the parameter h seems to depend upon the size of the clusters for which one is searching . This is due to the fact that the density estimate can be interpreted as approximating the convolution integral of p ( X ) and h - “k ( h - ‘X ) , and thus the kernel function is acting as a smoothing titer on the density , see Fig . 5 . Thus h determines the amount of smoothing of the density and correspondingly the elimination of modes that are too narrow or too close FLJKUNAGA AND HOSTETLER : GRADIENT OF DENSITY FUNCTION 0 0 0 0 O86k o o”o 0 g oo ~ ooo 0 0 3 ““0 * 00 00 0 0 0 00 03 03 00 CD 0 0 00 0 0 0 0 0 000 0 000 0 “ab” 0 0 0 0 0 808 0 0 00 0 0 0 00 O 0 0 0 0 O “0 00 0 00 0 0” 0 0 ( ( D 00 0 Fig . 2 . Original data set . 0 SEVEN GAUSSIAN ESTIMATE ITERATIONS 0 FIVE MEAN - SHIFT ESTIMATE ITERATIONS 0 0 0 0 0 0 0 00 0 Fig . 3 Data set after clustering iterations . 0 0 0 0 0 o” 0 0 0 0 00 00 0 0 0 0 “0 a , 800 00 00 ? Qbm 0 O 0 00 co 03 00 0 03 0 00 0 a n n 00 a n 0” n aaa arm n a 0 a crraa a 0 0 0 0 0 a aan aaa an O0 0 0 a a no na 0 a a a n a oa 2 n a 0 % omoi a an n dlQ oocf , o n a a “ , O $ 3 : n an a m 0 b ASSIGNED D BY MEAN - SHIFT ALGORITHM AND ASSIGNED 0 BY GAUSSIAN ALGORITHM Fig . 4 . Cluster assignments . 37 hw * Jyxl hbX ) ORIGINAL DENSITY KERNEL FUNCTlO : SMOOTHED DENSITY Fig . 5 Kernel smoothing action . to other modes . See [ 4 ] , [ IO ] , and [ 11 ] for discussions of the problem of the choice of h . The choice of a that seemed to work well in these examples was h2 a = - . n + 2 ( 62 ) Substituting ( 62 ) and ( 41 ) into ( 45 ) gives the mean - shift clustering algorithm xj + l = ! c xi’ . k XG E sh ( x / ) This algorithm transforms each observation to the sample mean of the observations within the region S , , around it . Thus , if the entire data set is divided into convex subsets that are greater than distance h apart , the observations will always remain within their respective sets or clusters and cannot diverge . This is due to the fact that ( 63 ) is always a convex combination of members from the same convex set , therefore , it must also lie inside the set . Also , as soon as all the observations in such a set lie within a distance h of one another , the next iteration will transform them all to a common point , their sample mean . C . Applications to Data Filtering This approach can also be used as a data filter to reduce the effect of noise in determining the intrinsic dimensionality of a data set , The intrinsic dimensionality of a data set is defined ( see [ 12 ] ) , to be the minimum number no of param - eters required to account for the observed properties of the data . The geometric interpretation is that the entire data set lies on a topological hypersurface of no dimensions . Fig . 6 shows a two - dimensional distribution that has intrinsic dimensionality of one . The Karhunen - Loeve dominant eigenvector analysis ( see [ 12 ] ) used to find the number of principal axes of data variance of this distribu - tion results in the two dominant axes shown in the figure . This suggests a dimensionality higher than the intrinsic value . This effect is avoided by using small local regions as in Fig . 7 . Then the Karhunen - Lo & e analysis of these subsets indicates dimensionalities close to the intrinsic value . This process experiences difficulty if there is noise present in the data . Thus if our observation space has n > no 38 IEEE TRANSACTIONS ON INFORMATION THEORY , JANUARY 1975 4 Q 2 PRINCIPAL AXES ’ L - - - r ? UNIFORM DENSITY ALONG CURVE / - 1 7 % I * x , Fig . 6 . Intrinsic dimensionality . A x2 LOCALLY ONE PRINCIPAL AXIS fi - XI Fig . 7 . Local data regions . Fig . 8 . Noisy data set . Fig . 9 . Contracted data set . dimensions , all of which contain noise in which the variance is of the order of the size of the local regions , then this process yields the value n as the intrinsic dimensionality . This effect could be offset by taking larger regions , but these regions then might include several surface convolu - tions , again resulting in an overestimate of the intrinsrc dimensionality . To reduce this effect , one would like to eliminate the noise in the minor dimensions , leaving only the dominant data surface . This can be achieved by using a few iterations of the mode clustering algorithm ( 45 ) as a data filter to contract the data onto its intrinsic data surface while still retaining the dominant properties of the data structure . As an example , 400 observations were generated in the form ( x1 , x2 ) = ( r cos 6 , r sin 6 ) ( 64 ) where r and 8 were uniformly distributed over the regions 2 . 25 I r < 2 . 75 018571 ( 65 ) respectively . This noisy data set can be considered to have intrinsic dimensionality one , with the intrinsic data surface being a circle of radius r equal to 2 . 5 . Fig . 8 shows the original noisy data set . By using two iterations of the transformation algorithm ( 45 ) with h = 0 . 6 and a = h2 / 3 = 0 . 12 , we see that the noise has been effectively removed leaving just the intrinsic data surface . This trans - formed data set is shown in Fig . 9 . The results show that gradient estimation can be used to effectively eliminate the noise from a data surface . V . SUMMARY By building upon previous results in nonparametric density estimation , we have been able to obtajn a general class of kernel density gradient estimates . Conditions on the kernel function were derived to guarantee asymptotic unbiasedness , consistency , and uniform consistency of the estimate . By generalizing the results for a Gaussian kernel function , we developed a sample mean - shift estimate of the normalized gradient and extended it to a k - nearest - neighbor approach . Applications of these estimates to the pattern recognition problems of clustering and intrinsic dimensionality deter - mination were presented with examples . Like most non - parametric sample - based techniques , the direct application of the algorithm may be costly in terms of computer time and storage , but still the underlying philosophy of density gradient estimation in pattern recognition systems is worth investigating as a means of furthering our understanding of the pattern recognition process . APPENDIX A . Asymptotic Unbiasedness In this section we will show thal the gradient estimate is asymptotically unbiased for the density function that satisfies ( 15 ) ; that is , ( 66 ) Taking the expectation of vxp ( X ) of ( 9 ) , J ! ? { ? & , , ( X ) } = ~ { h - “v , k ( h - l ( x - xi ) ) } = h - ” s V & ( h - l ( X - Y ) ) p ( Y ) dY . ( 67 ) R " FUKUNAGA AND HOSTETLER : GRADIENT OF DENSITY FUNCTION 39 Now looking at the ith component of this expectation and integrating by parts , - % ( % PNi ~ ) ) d = - h - ” s k ( h - ‘ ( X - Y ) ) p ( Y ) + * dY R” - i y * = - CO + h - ” s R” k ( h - l ( X - Y ) ) $ p ( Y ) dY . 1 WI Since k ( Y ) is bounded , from ( 2 ) , and p ( X ) is a probability density function with ( 15 ) satisfied , the first term of ( 68 ) is zero as , ; rnm jk ( h - l ( X - Y ) ) p ( Y ) j I sup [ k ( Y ) ] lim p ( Y ) = 0 . i’ YER” 1yi1 - m ( 6 % Under the conditions ( 2 ) - ( 6 ) , we can apply ( 14 ) to obtain the second term lim h - ” N - rCO s R” k ( h - ‘ ( X - Y ) ) $ p ( Y ) dY 1 = $ P ( X ) i s R” k ( Y ) dY = $ p ( X ) ( 70 ) 1 where the last equality follows from condition ( 5 ) on k ( X ) . Asymptotic unbiasedness ( 16 ) now follows by substituting ( 69 ) and ( 70 ) in the limit of ( 68 ) . B . Consistency In this section we will show that the gradient estimate is consistent in a mean - square sense ; that is , E { lt % , pN ( X ) - v , p ( x > I12 ) = E { ll ~ x , pN ( X ) - E @ dd ~ ) ~ l12 ~ + llE { ~ xPN ( x ) ) - VXP ( ~ ) II ~ ( 71 ) goes to zero as N approaches infinity . Since the estimate is asymptotically unbiased for a density function that satisfies ( 15 ) , lim jlE { ~ x , pN ( X ) } - ~ x , P ( ~ ) ~ ~ ~ = O * ( 72 ) N - CO Substituting ( 9 ) , the first term of ( 71 ) becomes h - ‘ $ k ( h - l ( X - y ) ) 2 i II - E2 h - “ $ k ( h - l ( X - Y ) ) . i II From ( 16 ) , we have h - ” & k ( h - l ( X - Y ) ) ( 74 ) I Now E h - “ $ , k ( h - l ( X - Y ) ) 2 II h - 2nin [ & k ( h - ? Z ) ] 2p ( X - Z ) dZ k ( h - ‘2 ) 2 p ( X - Z ) dZ 1 . ( 75 ) ( 76 ) Using conditions ( 20 ) - ( 22 ) , we see that [ k , ‘ ( Y ) 12 satisfies the conditions needed for ( 14 ) to hold . Thus lim h - ” a k ( h - ‘Z ) 2 p ( X - Z ) dZ N - CO a ( h - ‘zi ) 1 = P ( X ) s k’O’ ) 12 dY ( 77 ) which is finite . Substituting ( 77 ) , ( 76 ) , an : ( 74 ) into ( 73 ) and using condition ( 19 ) on h ( N ) , we obtain lia EIll ~ x ~ ~ ( X ) - V , P ( - JOII ~ ~ N - 02 = ig [ ik Wh” + 2Y - 1p ( x > 1 k’ ( Y ) 12 dY R” - lim N - ’ $ p ( X ) = 0 . 1 ( 78 ) N + CO i C . Uniform Consistency In this section we will show that the gradient estimate is uniformly consistent in probability ; that is , lim pr { , “wn II ~ xPN ( x ) - vd ( x ) ll ’ & ) = 0 . ( 79 N - PLO To show this , we notice that the gradient estimate is the convolution of the sample function and the function h - “V , k ( h - lx ) . Applying the properties of Fourier transforms , we see that the Fourier transform of qxpN ( X ) is the product of the Fourier transforms of the sample function and that of h - “V , k ( h - IX ) . Using the inversion relationship , we see that f & ( x ) = ( 2 $ - l s exp ( - jWTX > @ N ( W ) [ - jWK ( hW ) ] dW R” 030 ) where W is an n - dimensional vector , @ N ( w ) = $ & ‘exp ( , iWT & ) is the sample characteristic function , ( 81 ) K ( W ) = s exp ( jWTX ) k ( X ) dX ( 82 ) R” is the characteristic function of k ( X ) , and - jWK ( h W ) is the characteristic function of h - “V , k ( h - lx ) as in ( 28 ) . Therefore , E’S ; ! II ~ ~ PN ( X ) - E { ~ x ~ ~ ( X ) III 1 5 ( 2 $ - l s 11 WK ( hW ) IIE { I @ N ( W ) - E { @ NW ) III dW R” ( 83 ) I ( 27 $ - l s 11 WK ( h W ) jl var { @ # ‘ ) ) “2 dW ( 84 ) R” = ( 2 $ 1 s II WK ( hW ) II [ N - l var { exp ( jWTX ) } ] 1’2 dW R” ( 85 ) I ( 27 $ - l s II WK ( hW ) IIN - 1’2 dW ( 86 ) R” = ( 2 # / 2hN + ‘ ) - 1 s II - J’WKW ) II dW . ( 87 ) R” 40 IEEE TRANSACTIONS ON INFORMATION THEORY , VOL . IT - 21 , NO . 1 , JANUARY 1975 Since - j WK ( W ) is absolutely integrable 1h - n EIs ; z IIhN ( x ) - E { ~ xPN ( ~ ) ) II } = 0 . 03f - 9 N - CO We can modify the asymptotically unbiased argument , taking into consideration the uniform continuity of V , p ( X ) , to obtain easily lh [ sup IIE { ~ x ~ N ( X ) ) - V , ~ ( ~ > ll1 = 0 . 0 - W N - co R” Now , applying the triangle inequality , we have s ; F IlvxPN ( x > - v . dx ) II s “ , “R II ~ x : PN ( ~ ) - ‘ % ? c , pN ( X ) ) II + y IIE @ , P ~ ( x ) I - V , P ( - QII . ( 90 ) Therefore , using ( 88 ) and ( 89 ) in the limit of ( go ) , we obtain li , ” , “b ; ; II ~ xPN ( x ) - vxp ( x > II 1 = o ( 91 ) + which implies by Markov’s inequality lim Pr { s ; ; IIVxpN ( X ) - V , p ( X ) I / > E } = 0 , for all E > 0 . N - + KJ ( 92 ) ltl % ERENCEs [ l ] E . Fix and J . L . Hodges , “Discriminatory anilysis , nonparametric discrimination , ” USAF Sch . Aviation Medicine , Randolph Field , Tex . Proj . 21 - 49 - 004 , Rep . 4 , Contr . AF - 41 - ( 128 ) - 31 , Feb . 1951 . [ 2 ] M . Rosenblatt , “Remarks on some nonparametric estimates of a density function , ” Ann . Math . Statist . , vol . 27 , pp . 832 - 837 , 1956 . [ 3 ] E . Parzen , “On estimation of a probability density function and mode , ” Ann . Math . Statist . , yol . 33 , pp . 1065 - 1061 , 1962 . [ 4 ] T . Cacoullos . “Estimation of a multivariate densltv . ” Ann . Inst . 151 t61 [ 71 181 [ 91 [ lOI [ Ill 1121 katist . Math : , vol . 18 , pp . 179 - 189 , 1966 . - ’ E . A . Nadaraya , . “On nonparametric estimates of density func - tions and regressIon curves , ” Theory Prob . Appf . ( USSR ) , vol . 10 , pp . 186 - 190 , 1965 . J . Van Ryzm , “On strong consistency of density estimates , ” Ann . Math . Statist , vol . 40 , pp . 1765 - 1772 , 1969 . P . K . Bhattacharyya , “Estimation of a probability density function and its derivatives , ” Sa + hya : Znd . J . Stat . , vol . 29 , pp . 373 - 382 , 1967 . E . F . Schuster , “Estimation of a probability density function and its derivatives , ” Ann . Math . Statist . , vol . 40 , pp . 1187 - l 195 , 1969 . D . 0 . Loftsgaarden and C . P . Quesenberry , “A nonparametric estimate of a multivariate density function , ” Ann . Math . Statist . , vol . 36 , pp . 1049 - 1051 , 1965 . V . A . Epanechnikov , “Nonparametric estimation of a multi - variate probability density , ” Theory Prob . Appl . ( USSR ) , vol . 14 , pp . 153 - 158 , 1969 . M . Woodroofe , “On choosing a delta - sequence , ” Ann . Math . Statist . , vol . 41 , pp . 1665 - 1671 , 1970 . K . Fukunaga , Introduction to Statistical Pattern Recognition . New York : Academic , 1972 , ch . 10 . A Finite - M e m ory Deterministic Algorithm for the Symmetric Hypothesis Testing Problem B . CHANDRASEKARAN , MEMBER , IEEE , AMD CHUN CHOON LAM Abstract - A class of irreducible deterministic finite - memory algo - where , without loss of generality , 3 < p . < 1 . We further ritbms for the symmetric hypothesis testing problem is studied . It is shown how members of this class can be constructed to give a steady - assume that the hypotheses have equal prior probabilities . state probability of error that decreases asymptotically faster in the Let the data be summarized by a 2m - valued statistic T number of states than the best previously known deterministic algorithm . that is updated according to the rule T , = fK - IA , ) , T , E { 1 , 2 ; * * , 2m } INTRODUCTION where T , is the value of T at time iz . Let the decision d , , ET X1 , X2 , * * * L be a sequence of independent identically taken at time n be distributed Bernoulli random variables with possible values H and T such that Pr ( Xi = H ) = p . Consider the 4 , = CCJ , 4 E { & nHll . following symmetric hypotheses : For givenf and d the probability of error is defined to be & : P = PO H , : p = q . E 1 - p . Manuscript received September 28 , 1973 ; revised June 27 , 1974 . This work was supported by AFOSR Grant 72 - 2351 . B . Chandrasekaran is with the Department of Computer and In - formation Science , Ohio State University , Columbus , Ohio 43210 . C . Lam is with the Division of Statistics , Ohio State University , Columbus , Ohio 43210 . where ei = 0 or 1 depending on whether di is the correct decision or not . Hellman and Cover [ l ] have shown that a lower bound for P , is P , * = [ I + ( $ m - ‘ ] - ‘ , ( 1 )