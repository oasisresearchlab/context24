SIRL : Similarity - based Implicit Representation Learning Andreea Bobu âˆ— abobu @ berkeley . edu University of California , Berkeley United States of America Yi Liu âˆ— yiliu77 @ berkeley . edu University of California , Berkeley United States of America Rohin Shah rohinmshah @ deepmind . com DeepMind Research United Kingdom Daniel S . Brown dsbrown @ berkeley . edu University of California , Berkeley United States of America Anca D . Dragan anca @ berkeley . edu University of California , Berkeley United States of America É¸ Similarity query Preference query H 1 Preference query H 2 Preference query H 3 Optimize ğ‘… ! ! Optimize ğ‘… ! " Optimize ğ‘… ! # ğ‘… ! ! ğ‘… ! " ğ‘… ! # Figure 1 : Our goal is to learn representations for robot behavior that capture what is salient to people , and , thus , support generalizable pref - erence learning with low sample complexity . We propose to extract this representation by asking people trajectory similarity queries ( left ) , where they judge which two out of three trajectories are most similar to each other . We then use the representation to learn reward functions corresponding to different peopleâ€™s preferences on different tasks ( right ) . ABSTRACT When robots learn reward functions using high capacity models that take raw state directly as input , they need to both learn a representation for what matters in the task â€” the task â€œfeatures " â€” as well as how to combine these features into a single objective . If they try to do both at once from input designed to teach the full reward function , it is easy to end up with a representation that contains spurious correlations in the data , which fails to generalize to new settings . Instead , our ultimate goal is to enable robots to identify and isolate the causal features that people actually care about and use when they represent states and behavior . Our idea is that we can tune into this representation by asking users what âˆ— Both authors contributed equally to this research . This research is supported by the Office of Naval Research ( ONR ) Young Investigator Award , the NSF National Robotics Initiative , NSF Career , the Weill Neurohub , and the Apple AI / ML Fellowship . We thank Sid Reddy and Andrea Bajcsy for their feedback . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden Â© 2023 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9964 - 7 / 23 / 03 . https : / / doi . org / 10 . 1145 / 3568162 . 3576989 behaviors they consider similar : behaviors will be similar if the features that matter are similar , even if low - level behavior is dif - ferent ; conversely , behaviors will be different if even one of the features that matter differs . This , in turn , is what enables the robot to disambiguate between what needs to go into the representation versus what is spurious , as well as what aspects of behavior can be compressed together versus not . The notion of learning representa - tions based on similarity has a nice parallel in contrastive learning , a self - supervised representation learning technique that maps vi - sually similar data points to similar embeddings , where similarity is defined by a designer through data augmentation heuristics . By contrast , in order to learn the representations that people use , so we can learn their preferences and objectives , we use their definition of similarity . In simulation as well as in a user study , we show that learning through such similarity queries leads to representations that , while far from perfect , are indeed more generalizable than self - supervised and task - input alternatives . ACM Reference Format : Andreea Bobu , Yi Liu , Rohin Shah , Daniel S . Brown , and Anca D . Dragan . 2023 . SIRL : Similarity - basedImplicitRepresentationLearning . In Proceedings of the 2023 ACM / IEEE International Conference on Human - Robot Interaction ( HRI â€™23 ) , March 13 â€“ 16 , 2023 , Stockholm , Sweden . ACM , New York , NY , USA , 12 pages . https : / / doi . org / 10 . 1145 / 3568162 . 3576989 a r X i v : 2301 . 00810v3 [ c s . R O ] 17 M a r 2023 HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD 1 INTRODUCTION Imagine waking up in the morning and your home robot assistant wants to place a steaming mug of fresh coffee on the table exactly where it knows you will sit . Depending on the context , you will have a different preference for how the robot should be doing its task . Some days it carries your favorite mug close to the table to prevent it from breaking in the case of a slip ( so that it will remain your favorite mug ) ; other days the steam from your delicious meal is difficult to handle for the robotâ€™s perception , so youâ€™d want it to keep a large clearance from the table to avoid collisions . Similarly , some days you want the robot to keep your mug away from your laptop to avoid spilling on it ; other days the mug only has an espresso shot so you want the robot keep it close to the laptop to prevent clutter and leave the rest of the table open for you . Therewardfunctiontherobotshouldoptimizechangesâ€”whether due to variations in the task , having different users , or , as in the examples above , different contexts that are not always part of the robot state ( e . g . holding the userâ€™s favorite mug and not just a regu - lar mug ) . However , the representation on top of which the reward is built , i . e the features that are important ( like the distance from the table , being above the laptop , etc . ) , are shared . If the robot learns this representation correctly , it can use it to obtain the right reward function , even if the task , user , and context changes . Meta - learning and multi - task learning methods [ 30 , 46 , 56 ] learn the representa - tion from user input meant to teach the full reward , like preference queries or demonstrations . By contrast , we propose that if learning generalizable representations is the goal , then we should ask the user for input that is specifically meant to teach the representation itself , rather than input meant to teach the full reward and hoping to extract a good representation along the way . But asking people to teach robots representations , rather than tasks , is not so easy . What are the features that they care about ? While some techniques advocate for people enabling users to teach each feature separately [ 11 ] , people may not always be able to explicate their representation and break it down into concepts that are individually teachable . In this work , our idea is that we can implicitly tune into the representations people use by asking them to do a proxy task of evaluating similarity of behaviors . Behaviors will be similar if the features that matter are similar , even if low - level behavior is different ; conversely , behaviors will be different if even one of the features that matter differs . This , in turn , should enable the robot to arrive at the features that matter â€” we want robots that can disambiguate between what needs to go into the representation versus what is spurious , as well as what aspects of behavior can be compressed together into a feature embedding versus kept separate . We thus introduce a novel type of human input to help the robot extract the personâ€™s representation : trajectory similarity queries . A trajectory similarity query is a triplet of trajectories that the person answers by picking the two more similar trajectories . In Figure 1 ( left ) , the person chooses the two trajectories that are close to the table and far from the laptop , even though visually they look dissimilar . This results in an ( anchor , positive , negative ) triplet that can be used for training a feature representation . We call this process Similarity - based Implicit Representation Learning ( SIRL ) . Our method has a parallel in self - supervised learning work , espe - cially contrastive learning , where the goal is to learn a good visual representation by training from ( anchor , positive , negative ) triplets generated via data augmentation techniques [ 19 ] . However , this notion of similarity is purely visual , driven by manually designed heuristics for data augmentation , and is not necessarily reflective of what users would consider similar . For instance , two images might be labeled as visually different , when in fact their difference is only with respect to some low - level aspects that are not really relevant to the distribution of tasks people care about . This would result in representations that contain too many distractor features that are not present in the humanâ€™s representation . Our method uses similarity too , but we defer to the userâ€™s judgement of similarity , with the goal of reconstructing the userâ€™s representation . Of course , our method is not the full answer to learning causally aligned representations . But our experiments suggest that it out - performs methods that are self - supervised , or that learn from input meant to teach the full tasks . In simulation , where we know the causal features , we show that SIRL learns representations better aligned with them , which in turn leads to learning multiple more generalizable reward functions downstream ( Figure 1 ) . We also present a user study where we crowdsource similarity queries from different people to learn a shared SIRL representation that better re - covers each of their individual preferences . While the study results do show a significant effect , the effect size is much lower than in simulation . This is attributable in part to the interface difficulty of analyzing the robot trajectories , which means more work is needed to determine the best interfaces that enable users to accurately answer similarity queries . Moreover , some users reported strug - gling to trade off the different features , which means that similarity queries might not be entirely preference - agnostic . Nonetheless , our results underscore that there are gains by explicitly aligning robot and human representations , rather than hoping it will happen as a byproduct of learning rewards from standard queries . 2 RELATED WORK Learning from Human Input . Human - in - the - loop learning is a well - established paradigm where the robot uses human input to infer a policy or reward function capturing the desired behavior . In imitation learning , the robot learns a policy that essentially copies human demonstrations [ 47 ] , a strategy that typically doesnâ€™t generalize well outside the training regime [ 40 ] . Meanwhile , inverse reinforcement learning ( IRL ) uses the demonstrations to extract a reward function capturing why a specific behavior is desirable , thus better generalizing to unseen scenarios [ 1 ] . Recent research goes beyond demonstrations , utilizing other types of human input for reward learning , such as corrections [ 6 ] , comparisons [ 55 ] , or rankings [ 14 ] . Unless explicitly designed for , these methods learn a latent representation implicitly from the respective human input . We seek to instead explicitly learn a preference - agnostic latent space that can be used for downstream tasks like reward learning . We focus on learning models of human reward functions via pairwise preference queries [ 55 ] , but we believe the latent space we learn can be useful for learning from any of the above types of feedback . Representation and Similarity Learning . Common represen - tation learning approaches are unsupervised [ 18 , 20 , 32 ] or self - supervised [ 4 , 27 , 39 , 48 ] , but because they are purposefully de - signed to bypass human supervision , the learned embedding does SIRL : Similarity - based Implicit Representation Learning HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden not necessarily correspond to features the person cares about . Prior work leverages task labels [ 17 ] or trajectory rankings [ 13 ] to learn latent spaces that help identify specific goals or preferences . By contrast , we focus on learning task - agnositic measures of feature similarity that are useful for learning multiple preferences . Some work looks at having people interactively select features from a pre - defined set [ 15 , 16 , 42 ] or teach task - agnostic features sequen - tially via kinesthetic feature demonstrations [ 10 ] or active learning techniques [ 9 , 31 , 37 ] . We instead focus on fully learning a lower - dimensional feature representation all - at - once , rather than one at a time . Furthermore , rather than relying on the human to provide physical demonstrations for learning a good feature space [ 10 , 11 ] , we propose a more accessible and general form of human feedback : showing the user triplets of trajectories and simply asking them to label which two trajectories are the most similar . Triplet losses have been widely used to learn similarity models that capture how humans perceive objects [ 2 , 3 , 25 , 44 , 54 ] ; however , to the best of our knowledge , we are the first to use a triplet loss to learn a general , task - agnostic similarity model of how humans perceive trajectories . Meta - and Multi - Task Reward Function Learning . To learn multiple models of human reward functions , prior work has pro - posed clustering demonstrations and learning a different reward function for each cluster [ 5 , 21 , 26 ] ; however , these methods require a large number of demonstrations and do not adapt to new reward functions . Meta - learning [ 29 ] seeks to learn a reward function ini - tialization that enables fast fine - tuning at test time [ 33 , 51 , 57 , 58 ] . Multi - task reward learning approaches pretrain a reward function on multiple human intents and then fine - tune the reward function at test time [ 30 , 46 ] . This has been shown to be more stable and scal - able than meta - learning approaches [ 43 ] , but still requires curating a large set of training environments . By contrast , we do not assume any knowledge of the test - time task distribution a priori and do not require access to a population of different reward functions dur - ing training . Rather , we focus on learning a task - agnostic feature representation that can be utilized for down - stream reward learn - ing tasks . In particular , we test our learned representation on the down - stream task of learning models of human reward functions via pairwise preference queries over trajectories [ 8 , 41 , 50 , 55 ] . 3 METHOD We present our method for learning preference - agnostic represen - tations from trajectory similarity queries . Our intuition is that if a human judges two behaviors to be similar , then their representa - tions should also be similar . Since directly asking if two trajectories are similar is difficult without an explicit threshold , we instead present the human with a triplet of trajectories and ask them to pick the two most similar ( or , equivalently , the most dissimilar one ) . We use the humanâ€™s answers to train the representation such that similar trajectories have embeddings that are close and dissimilar trajectories map to embeddings far apart . The robot then uses this latent space as a shared representation for downstream preference learning tasks with multiple people , each with different preferences . 3 . 1 Preliminaries We define a trajectory ğœ‰ as a sequence of states , and denote the space of all possible trajectories by Î . The humanâ€™s preference over trajectories is given by a reward function ğ‘… : Î â†¦â†’ R that is unobserved by the robot and must be learned from human inter - action . The robot reasons over a parameterized approximation of the reward function ğ‘… ğœƒ , where ğœƒ represents the parameters of a neural network . To learn ğœƒ , the robot collects human preference labels over trajectories [ 22 , 55 ] and seeks to find parameters ğœƒ that maximize the likelihood of the human input . The robot can then use the learned reward function to score trajectories during motion planning in order to align its behavior with a particular humanâ€™s preferences . We focus on explicitly using human input to first learn a good representation and then use that representation for down - stream reward learning , rather than using reward - specific human input ( e . g . , preferences or demonstrations ) to implicitly learn the representation at the same time as the reward function . 3 . 2 Training the Feature Representation via Trajectory Similarity Queries We seek to train a latent space that is useful for multiple down - stream preference learning tasks . To do this , we propose learning a preference - agnostic model of human similarity . One way to learn such a model would be to ask users to judge whether two trajecto - ries are similar or not ; however , humans are better at giving relative rather than binary or quantitative assessments of similarity [ 35 , 53 ] . Thus , rather than asking users to use some internal threshold or scoring mechanism to quantitatively measure similarity , we instead focus on qualitative trajectory similarity queries . We present the user with a visualization of three trajectories and ask them to pick the two most similar ones ( equivalently the most dissimilar one ) . The humanâ€™s queries form a data set D ğ‘ ğ‘–ğ‘š = { ( ğœ‰ ğ‘–ğ‘ƒ 1 , ğœ‰ ğ‘–ğ‘ƒ 2 , ğœ‰ ğ‘–ğ‘ ) } , where ğœ‰ ğ‘–ğ‘ƒ 1 and ğœ‰ ğ‘–ğ‘ƒ 2 are the trajectories that are most similar and ğœ‰ ğ‘–ğ‘ is the trajectory most dissimilar to the other two . We can interpret similarity ( or dissimilarity ) as a distance func - tion , so we define the distance between two trajectories as the ğ¿ 2 feature distance : ğ‘‘ ( ğœ‰ 1 , ğœ‰ 2 ) = âˆ¥ ğœ™ ( ğœ‰ 1 ) âˆ’ ğœ™ ( ğœ‰ 2 ) âˆ¥ 22 . Given a dataset of trajectory similarity queries D ğ‘ ğ‘–ğ‘š , we use the triplet loss [ 7 ] : L ğ‘¡ğ‘Ÿğ‘–ğ‘ ( ğœ‰ ğ´ , ğœ‰ ğ‘ƒ , ğœ‰ ğ‘ ) = max ( ğ‘‘ ( ğœ‰ ğ´ , ğœ‰ ğ‘ƒ ) âˆ’ ğ‘‘ ( ğœ‰ ğ´ , ğœ‰ ğ‘ ) + ğ›¼ , 0 ) , ( 1 ) a form of contrastive learning where ğœ‰ ğ´ is the anchor , ğœ‰ ğ‘ƒ is the positive example , ğœ‰ ğ‘ is the negative example , and ğ›¼ â‰¥ 0 is a margin between positive and negative pairs . However , because our queries do not contain an explicit anchor , our final loss is as follows : L ğ‘ ğ‘–ğ‘š ( ğœ™ ) = | D ğ‘ ğ‘–ğ‘š | âˆ‘ï¸ ğ‘– = 1 L ğ‘¡ğ‘Ÿğ‘–ğ‘ ( ğœ‰ ğ‘–ğ‘ƒ 1 , ğœ‰ ğ‘–ğ‘ƒ 2 , ğœ‰ ğ‘–ğ‘ ) + L ğ‘¡ğ‘Ÿğ‘–ğ‘ ( ğœ‰ ğ‘–ğ‘ƒ 2 , ğœ‰ ğ‘–ğ‘ƒ 1 , ğœ‰ ğ‘–ğ‘ ) . ( 2 ) We train a similarity embedding function ğœ™ : Î â†¦â†’ R ğ‘‘ that mini - mizes the above similarity loss , where ğ‘‘ is the representation di - mensionality . The intuition is that optimizing this loss should push together the embeddings of similar trajectories and push apart the embeddings of dissimilar trajectories . Before training the repre - sentation with the loss in Eq . ( 2 ) , we may also pre - train it using unsupervised learning [ 36 ] , which we experiment with in Sec . 4 . 3 . 3 Using SIRL for Reward Learning Given a learned embedding ğœ™ , we can use it for learning models of specific user preferences . While we focus on learning from pairwise HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD preferences , we note that ğœ™ can in principle be used in downstream tasks that learn from many types of human feedback [ 34 ] . When learning a reward function from human preferences , we show the human two trajectories , ğœ‰ ğ´ and ğœ‰ ğµ , and then ask which of these two the human prefers . We collect a data set of such preferences D ğ‘ğ‘Ÿğ‘’ğ‘“ = { ( ğœ‰ ğ‘–ğ´ , ğœ‰ ğ‘–ğµ , â„“ ğ‘– ) } where â„“ ğ‘– = 1 if ğœ‰ ğ‘–ğ´ is preferred to ğœ‰ ğ‘–ğµ , denoted ğœ‰ ğ‘–ğ´ â‰» ğœ‰ ğ‘–ğµ , and â„“ ğ‘– = 0 otherwise . We interpret the humanâ€™s prefer - ences through the lens of the Bradley - Terry preference model [ 12 ] : ğ‘ƒ ğœƒ ( ğœ‰ ğ´ â‰» ğœ‰ ğµ ) = ğ‘’ ğ‘… ğœƒ ( ğœ™ ( ğœ‰ ğ´ ) ) ğ‘’ ğ‘… ğœƒ ( ğœ™ ( ğœ‰ ğ´ ) ) + ğ‘’ ğ‘… ğœƒ ( ğœ™ ( ğœ‰ ğµ ) ) . ( 3 ) We learn the reward function with a simple cross - entropy loss : L ğ‘ğ‘Ÿğ‘’ğ‘“ ( ğœƒ ) = âˆ’ | D ğ‘ğ‘Ÿğ‘’ğ‘“ | âˆ‘ï¸ ğ‘– = 0 â„“ ğ‘– log ğ‘ƒ ğœƒ ( ğœ‰ ğ‘–ğ´ â‰» ğœ‰ ğ‘–ğµ ) + ( 1 âˆ’ â„“ ğ‘– ) log ğ‘ƒ ğœƒ ( ğœ‰ ğ‘–ğµ â‰» ğœ‰ ğ‘–ğ´ ) . ( 4 ) 3 . 4 Adapting to Different User Preferences We want robots that can adapt to changes to an individual userâ€™s preferences depending on the context as well as quickly adapt to new usersâ€™ preferences . Rather than learn each preference inde - pendently by collecting a new set of human data and training a completely new reward function ğ‘… ğœƒ , we study whether we can leverage the latent space learned by SIRL to perform more accurate and sample - efficient multi - preference learning . When learning a new userâ€™s preference model , ğ‘… ğœƒ the robot can use ğœ™ to more quickly learn the reward function ğ‘… ğœƒ ( ğœ™ ( ğœ‰ ) ) . Our main idea is that because this shared latent representation ğœ™ is trained via preference - agnostic similarity queries , it is more transferable than using a multi - task or meta - learning approach , where the pre - trained network is trained using multiple , specific task objectives . Furthermore , because SIRL uses human input to train ğœ™ , we hypothesize that the learned fea - ture space will be better suited for learning human reward functions than a latent space learned via unsupervised training . 4 EXPERIMENTS IN SIMULATION We first investigate the quality of SIRL - trained representations and their benefits for preference learning using simulated human input in two environments with ground truth rewards and features . 4 . 1 Environments GridRobot ( Figure 2a ) is a 5 - by - 5 gridworld with two obstacles and a laptop ( the blue , green , and black boxes ) . Trajectories are sequences of 9 states with the start and end in opposite corners . The 19 - dimensional input consists of the ğ‘¥ and ğ‘¦ coordinates of each state and a discretized angle in { âˆ’ 90 â—¦ , âˆ’ 60 â—¦ , âˆ’ 30 â—¦ , 0 â—¦ , 30 â—¦ , 60 â—¦ , 90 â—¦ } at the end state . The simulated human answers queries based on 4 features ğœ™ âˆ— in this world : Euclidean distances to each object , and the absolute value of the angle orientation . JacoRobot ( Figure 2b ) is a pybullet [ 24 ] simulated environment with a 7 - DoF Jaco robot arm on a tabletop , with a human and laptop in the environment . Trajectories are length 21 , and each state consists of 97 dimensions : the ğ‘¥ğ‘¦ğ‘§ positions of all robot joints and objects , and their rotation matrices . This results in a 2037 - dimensional input space , much larger than for GridRobot . The 4 features of interest ğœ™ âˆ— for the simulated human are : a ) table â€” ( a ) GridRobot environment . Preprogrammed Views Query Answers Trajectory Replay Query ID ğœ‰ ! ğœ‰ " ğœ‰ # ( b ) JacoRobot environment and user study interface . Figure 2 : Visualization of the experimental environments . distance of the robotâ€™s End - Effector ( EE ) to the table ; b ) upright â€” EE orientation relative to upright , to consider whether objects are carried upright ; c ) laptop â€” ğ‘¥ğ‘¦ - plane distance of the EE to a laptop , to consider whether the EE passes over the laptop at any height ; d ) proxemics [ 45 ] â€” proxemic ğ‘¥ğ‘¦ - plane distance of the EE to the human , where the EE is considered closer to the human when moving in front of the human that to their side . In GridRobot the state space is discretized , so the trajectory space Î can be enumerated ; however , the JacoRobot state space is contin - uous , so we construct Î by smoothly perturbing the shortest path trajectories from 10 , 000 randomly sampled start - goal pairs ( see App . A . 1 ) . We generate similarity and preference queries by randomly sampling from Î . The simulated human answers similarity queries by computing the 4 feature values for each of the three trajecto - ries and choosing the two that were closest in the feature space . For preference queries , the simulated human computes the ground truth reward and samples the trajectory with the higher reward . The space of true reward functions ( used to simulate preference labels ) is defined as linear combinations of the 4 features described above . The robot is not given access to the ground - truth features nor the ground - truth reward function but must learn them from similarity and preference labels over raw trajectory observations . 4 . 2 Qualitative Examples In Figure 3 we show similar and dissimilar trajectories learned by SIRL in a simplified GridRobot environment with only the laptop and the joint angle . Top : the given trajectory stays far from the laptop and holds the cup on its side ; SIRL learns that trajectories that share those features are similar , despite being dissimilar in SIRL : Similarity - based Implicit Representation Learning HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden Figure 3 : SIRL picks the two most and least similar trajectories to a query trajectory . Top : trajectories are similar in features despite being dissimilar in states . Bottom : trajectories are dissimilar in fea - tures despite being close in states . the state - space . Bottom : the trajectory stays close to the laptop and holds the cup at an angle ; SIRL learns that trajectories that hold the cup upright and stay far from the laptop are dissimilar , despite being similar in the state - space ( going up and then right ) . 4 . 3 Experimental Setup Manipulated Variables . We test the importance of user input that is designed to teach the representation by comparing SIRL with multi - task learning techniques from generic preference queries , and unsupervised representation learning . We have 4 baselines : a ) VAE , which learns a representation with a variational reconstruc - tion loss [ 36 ] ; b ) MultiPref , a multi - task baseline [ 30 ] , where we learn the representation ğœ™ implicitly by training multiple reward functions ( each with shared initial layers ) via preference learning ; c ) SinglePref , a hypothetical method that learns from an ideal user who weighs all features equally ; d ) Random , a randomly initial - ized embedding , which does not benefit from human data but is also immune from any spurious correlations that might be learned from biased data . For MultiPref , we trained versions with 10 and 50 simulated human preference rewards for good coverage of the reward space . All embeddings have the same network size : for GridRobot we used MLPs with 2 layers , 128 units each , mapping to 6 output neurons , while for JacoRobot we used 1024 units to handle the larger input space ( see App . A . 2 ) . For a fair compar - ison , we gave SIRL , SinglePref , and MultiPref equal amounts of human data for pre - training : ğ‘ similarity queries for SIRL , and ğ‘ preference queries ( used for a single human for SinglePref or equally distributed amongst humans for MultiPref ) . We also per - formed ablations with and without VAE pre - training and found that SinglePref and MultiPref are better without VAE ( see App . A . 3 ) . Dependent Measures . To test the quality of the learned repre - sentations , we use two metrics : Feature Prediction Error ( FPE ) and Test Preference Accuracy ( TPA ) . The FPE metric is inspired by prior work that argues that good representations are linearly separa - ble [ 23 , 38 , 49 ] . Our goal is to measure whether the embeddings contain the necessary information to recover the 4 ground - truth features in each environment . We generate data sets of sampled trajectories labeled with their ground truth ( normalized ) feature vector D ğ¹ğ‘ƒğ¸ = { ğœ‰ , ğœ™ âˆ— } . We freeze each embedding and add a linear regression layer on top to predict the feature vector for a given trajectory . We split D ğ¹ğ‘ƒğ¸ into 80 % training and 20 % test pairs , and FPE is the mean squared error ( MSE ) on the test set between the predicted feature vector and the ground truth feature vector . For the human query methods , we report FPE with increasing number of representation training queries ğ‘ . For TPA , we test whether good representations necessarily lead to good learning of general preferences . We use the trained em - beddings as the base for 20 randomly selected test preference re - wards . For each ğ‘… ğœƒ ğ‘– , we generate a set of labeled preference queries D ğœƒ ğ‘– ğ‘ğ‘Ÿğ‘’ğ‘“ = { ğœ‰ ğ´ , ğœ‰ ğµ , ğ‘™ } , which we split into 80 % for training and 20 % for test . We train each reward model with ğ‘€ preference queries per test reward , and we vary ğ‘€ . All preference networks have the same architecture : we take the embedding ğœ™ pre - trained with the respective method , and add new fully connected layers to learn a reward function from trajectory preference labels . For GridRobot we used MLPs with 2 layers of 128 units , and for JacoRobot we used 1024 units . We found that all methods apart from SIRL worked better with unfrozen embeddings ( App . A . 3 ) . We report TPA as the preference accuracy for the learned reward models on the test preference set , averaged across the test human preferences . Hypotheses . We test two hypotheses : H1 . Using similarity queries specifically designed to teach the representation ( SIRL ) leads to representations more predictive of the true features ( lower FPE ) than unsupervised ( VAE ) , implicit ( MultiPref , SinglePref ) , or random representations . H2 . The SIRL representations result in more generalizable re - ward learning ( higher TPA ) than unsupervised ( VAE ) , implicit ( Mul - tiPref , SinglePref ) , or random representations . 4 . 4 Results In Figure 4 we show the FPE score for both environments with varying representation queries ğ‘ from 100 to 1000 . For GridRobot , both versions of SIRL ( with or without VAE pre - training ) perform similarly and outperform all baselines , supporting H1 . When pre - training with preference queries , MultiPref with 10 humans per - forms better than SinglePref or MultiPref with 50 humans : Sin - glePref may be overfitting to the one human preference it has seen , while when MultiPref has to split its data budget among 50 humans it ends up learning a worse representation than Random . There is a balance to be struck between the diversity in human training rewards covered and the amount of pre - training data each reward gets , a trade - off which SIRL avoids because similarity queries are agnostic to the particular human reward . For the more complex JacoRobot , both versions of SIRL outperform all baselines , in line with H1 , although SIRL without VAE scores better than with it . In Figure 5 we present the TPA score for both environments with a varying amount of test preference queries ğ‘€ from 10 to 190 , and ğ‘ = 100 , 500 , and 1000 . For GridRobot , each respective method performs comparably with different ğ‘ s , suggesting that this is a simple enough environment that low amounts of representation data are sufficient . For JacoRobot , this is not the case : with just 100 queries , SIRL with VAE pre - training performs like VAE , SIRL without pre - training has random performance ( since itâ€™s frozen ) , and the preference baselines all perform close to Random , as if they HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD 100 200 300 400 500 600 700 800 900 1000 Number of Representation Queries 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 F PE GridRobot Feature Prediction Error ( FPE ) SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref10H Random MultiPref50H VAE 100 200 300 400 500 600 700 800 900 1000 Number of Representation Queries 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 F PE JacoRobot Feature Prediction Error ( FPE ) SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref10H Random MultiPref50H VAE Figure 4 : FPE for the GridRobot ( left ) and JacoRobot ( right ) environments with simulated human data . With enough data , SIRL learns repre - sentations more predictive of the true features ğœ™ âˆ— in both simple and complex environments . werenâ€™t trained with queries at all . For larger ğ‘ , both versions of SIRL start performing better than the baselines , suggesting that with enough data a good representation can be learned . Focusing on ğ‘ = 1000 , our results support H2 : both SIRLs out - perform all baselines in both environments , although for JacoRobot SIRL without VAE is better than with VAE . In the GridRobot en - vironment VAE pre - training helps SIRL . However , while VAE per - forms comparably to other baselines in GridRobot , it severely un - derperforms in JacoRobot . This suggests that the reconstruction loss struggles to recover a helpful starting representation when the input space is higher dimensional and correlated . As a result , using the VAE pre - training to warmstart SIRL hinders performance when compared to starting from a blank slate . When comparing the preference - based baselines , in GridRobot they all perform similarly apart from MultiPref with 50 humans . In JacoRobot we see a trend that more preference humans does not necessarily result in better performance . This confirms our observation from Figure 4 that deciding on an appropriate number of human preferences to use for multi - task pretraining is challenging , a problem that SIRL bypasses . Summary . With enough representation data , SIRL learns represen - tations more predictive of the true features ( H1 ) , leading to learning more generalizable rewards ( H2 ) . This does not necessarily mean that SIRL representations are perfectly aligned with causal features â€” they are just better aligned , so the learned rewards are also better . When VAE pre - training recovers sensible starting representations it further reduces the amount of human data SIRL needs , otherwise it hurts performance . Lastly , surprisingly , Random is often better than pre - training with preference queries : preference - based meth - ods may learn features that correlate with the training data but are not necessarily causal , and an incorrectly biased representation is worse for learning downstream rewards than starting from scratch . 5 USER STUDY We now present a user study with novice users that provide simi - larity queries via an interface for the JacoRobot environment . 5 . 1 Experiment Design We ran a user study in the JacoRobot environment , modified for only two features : table and laptop ( we removed the humanoid in the environment ) . We designed an interface where people can click and drag to change the view , and press buttons to replay trajectories and record their query answer ( Figure 2b ) . We chose to display the Euclidean path of each trajectory in the query traces , as we found that to help users more easily compare trajectories to one another . The study has two phases : collecting similarity queries and col - lecting preference queries . In the first phase , we introduce the user to the interface and describe the two features of interest . Because similarity queries are preference - agnostic , we describe examples of possible preferences akin to the ones in Sec . 1 , but we do not bias the participant towards any specific preference yet . Each participant practices answering a set of pre - selected , unrecorded similarity queries , and then answers 100 recorded similarity queries . In the second phase , we describe a scenario in the environment that has a specific preference associated with it ( e . g . â€œThereâ€™s smoke in the kitchen , so the robot should stay high from the tableâ€ or â€œThere is smoke in the kitchen and the robotâ€™s mug is empty , so you want to stay far from the table and close to the laptop . â€ ) and assign different preference scenarios to each participant . Each person practices un - recorded preference queries , then answers 100 preference queries . Participants . We recruited 10 users ( 3 female , 6 male , 1 non - binary , aged 20 - 28 ) from the campus community to give queries . Most users had technical background , so we caution that our results will speak to SIRLâ€™s usability with this population rather than more generally . Manipulated Variables . Guided by the results in Figure 5 , we compare our best performing method , SIRL without VAE , to Ran - dom , the best performing realistic baseline . For SIRL we collect 100 similarity queries from each participant and train a shared representation using all of their data . Dependent Measures . We present the same two metrics from Sec . 4 , FPE and TPA . For TPA , we collect 100 preference queries for each userâ€™s unique preference , we use 70 % for training individual reward networks which we evaluate on the remaining 30 % queries ( Real ) . We compute TPA with cross - validation on 50 splits . To demonstrate how well SIRL works for new people who donâ€™t contribute to learn - ing the similarity embedding , we also train SIRL on the similarity queries of 9 of the users and compute TPA on the held - out userâ€™s preference data ( Held - out ) , for each user , respectively . Lastly , be - cause real data tends to be noisy , we also compute TPA with 70 SIRL : Similarity - based Implicit Representation Learning HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden 10 30 50 70 90 110 130 150 170 190 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot with N = 100 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE 10 30 50 70 90 110 130 150 170 190 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot with N = 100 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE 10 30 50 70 90 110 130 150 170 190 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 T P A GridRobot with N = 500 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE 10 30 50 70 90 110 130 150 170 190 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 T P A JacoRobot with N = 500 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot with N = 1000 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot with N = 1000 SIRL + VAE ( Ours ) SinglePref SIRL ( Ours ) MultiPref 10H Random MultiPref 50H VAE Figure 5 : TPA for GridRobot ( top ) and JacoRobot ( bottom ) with simulated human data . With enough data , SIRL recovers more generalizable rewards than unsupervised , preference - trained , or random representations . simulated preference queries for 10 different rewards , which we also evaluate on a simulated test set ( Simulated ) . Hypotheses . Our hypotheses for the study are : H3 . Similarity queries ( SIRL ) recover more salient features than a random representation ( lower FPE ) , even with novice user data . H4 . The SIRL representation results in more generalizable re - ward learning ( higher TPA ) , even with novice similarity queries . 5 . 2 Analysis Figure 6 summarizes the results . On the left , SIRL recovers a repre - sentation twice as predictive of the true features , supporting H3 . A 2 - sided t - test ( p < . 0001 ) confirms this . This suggests SIRL can recover aspects of peopleâ€™s feature representation even with noisy similarity queries from novice users . On the right ( Real ) , SIRL recov - ers more generalizable rewards on average than Random , providing evidence for H4 . Furthermore , using the SIRL representation on a novel user ( Held - out ) also performs better than Random , and the result appears almost indistinguishable from Real . This sug - gests that similarity queries can be effectively crowd - sourced and the resulting representation works well for novel user preferences . Lastly , training with simulated preference queries slightly improves performance for both methods , suggesting that noise in the human preference data can be substantial . Three ANOVAs with method as a factor find a significant main effect ( F ( 1 , 18 ) = 6 . 0175 , p = . 0246 , F ( 1 , 18 ) = 4 . 7547 , p = . 0427 , and F ( 1 , 18 ) = 16 . 1068 , p < . 001 , respec - tively ) . For each of the 3 cases , we also separated the 6 humans that were assigned preferences pertaining to both features ( e . g . â€œThere is smoke in the kitchen and the robotâ€™s mug is empty , so stay far from the table and close to the laptop . â€ ) . SIRL performance is slightly better when using preference data from this subset of users , hinting that perhaps the learned representation entangled the two features . HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD All Humans 0 . 00 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 F PE SIRL ( Ours ) Random All Humans 6 Humans All Humans 6 Humans All Humans 6 Humans 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 TP A Real Held - out Simulated Figure6 : Studyvaluesfor FPE , and TPA withrealandsimulatedpreferences . Evenwithnovicesimilarityqueries , SIRLrecoversrepresentations both more predictive of the true features and more useful for learning different user rewards than the baseline . Overall , the quantitative results support H3 and H4 , providing evidence that SIRL can recover more human - aligned representa - tions . Subjectively , some users found the 2D interface deceiving at times , as they would judge trajectory similarity differently based on the viewpoint . This is a natural artifact of visualizing a 3D world in 2D , but future work should investigate better interfaces . Some users reported struggling to trade off the two features when comparing trajectories . This is in part due to the fact that we almost â€œengi - neeredâ€ their internal representation , so a more in - the - wild study could determine whether similarity queries are indeed preference - agnostic . Lastly , some queries were easier than others : usersâ€™ time - to - answer varied across triplets suggesting that future work could use it as a confidence metric for how much to trust their answer . 6 DISCUSSION AND LIMITATIONS In this work , our goal was to tackle the problem of learning good representations that capture ( and disentangle ) the features that matter , while excluding spurious features . If we had such a repre - sentation , then learning rewards that capture different preferences and tasks on top of it would lead to generalizable models that reli - ably incentivize the right behavior across different situations , rather than picking up on correlates and being unable to distinguish good from bad behaviors on new data . Our idea was to implicitly tap into this representation by asking people what they find similar versus not , because two behaviors will be similar if and only if their representations are similar . We introduced a novel human input type , trajectory similarity queries , and tested that it leads to better representations than those learned through self - supervision or via multi - task learning : it enables learning rewards from the same training data that better rank behaviors on test data . That said , we need to be explicit that this is not the be - all end - all solution to our goal above . The representations learned , as we see in simulation , are not perfectly aligned with causal features â€” they are just better aligned . The learned rewards are not perfect â€” they are just better than alternatives . Similarity queries do not solve the problem fully , potentially because they suffer from the same issue preference queries do : when multiple important features change , it becomes harder to make a judgement call on what is more similar . The advantage that we see from similarity queries , though , is that rewards for particular tasks might ignore or down - weigh certain features that matter in other tasks , while similarity queries are task - agnostic and implicitly capture the distribution of tasks in the humanâ€™s head . Rather than having to specify a task distribution for multi - task learning , with similarity queries we are ( implicitly ) ask - ing the user to leverage their more general - purpose representation of the world . But thinking about how to overcome the challenge that multiple changing features make these queries harder to an - swer opens the door to exciting ideas for future work . For instance , what if we iteratively built the space , and based similarity queries on some current estimate of what are the important features ; over time , as the representation becomes more aligned with the humanâ€™s , the queries would get better at honing in on specific features . Another obvious limitation is that we did not do an in - the - wild study . In theory , similarity queries should be used when people already have a robot they are familiar with and , thus , have a distri - bution of tasks they care about in their everyday contexts , but in our study we needed to explain to users these contexts and what might be important . In doing so , we almost â€œengineeredâ€ their inter - nal representation . As robots become more prevalent , a follow - up study where users are given much less structure and allowed to actually tap into their unaltered representation might be possible . In a sense , with SIRL we build a foundation model , and this some - times requires hundreds of queries to learn a good representation . While we donâ€™t think having this much data when pre - training is unreasonable , especially since it leads to significant desirable performance improvement over baselines , sample - complexity is crucial to address as we scale to more complex robotic tasks . Be - cause similarity queries are task agnostic , we can crowd - source the queries from multiple people ( as we did in the user study ) and rely on this economy at scale to alleviate user burden . Future work could also look at active querying methodologies to ask the person for more informative similarity queries and reduce data amounts . A further avenue of work is extending this beyond reward learn - ing , using SIRL representations directly for policy learning or learn - ing exploration functions . We also emphasize that similarity queries are not a replacement for self - supervised learning ; instead , we view them as complementary â€” self - supervised learning might be able to leverage expert - designed heuristics to eliminate many of the spuri - ous features , while SIRL might serve to fine - tune the representation . How to properly combined the two remains an open question . Overall , similarity queries are a step towards recovering human - aligned representations . They improve upon the state of the art , and can benefit from further exploration in how to combine them with other inputs and self - supervision , and how to make them easier through better interfaces and query selection algorithms . SIRL : Similarity - based Implicit Representation Learning HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden REFERENCES [ 1 ] Pieter Abbeel and Andrew Y Ng . 2004 . Apprenticeship learning via inverse reinforcement learning . In Machine Learning ( ICML ) , International Conference on . ACM . [ 2 ] Sameer Agarwal , Josh Wills , Lawrence Cayton , Gert Lanckriet , David Kriegman , and Serge Belongie . 2007 . Generalized non - metric multidimensional scaling . In Artificial Intelligence and Statistics . PMLR , 11 â€“ 18 . [ 3 ] Ehsan Amid , Aristides Gionis , and Antti Ukkonen . 2015 . A Kernel - Learning Approach to Semi - supervised Clustering with Relative Distance Comparisons , Vol . 9284 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 23528 - 8 _ 14 [ 4 ] YusufAytar , TobiasPfaff , DavidBudden , TomLePaine , ZiyuWang , andNandode Freitas . 2018 . Playing Hard Exploration Games by Watching YouTube . In Pro - ceedings of the 32nd International Conference on Neural Information Processing Systems ( MontrÃ©al , Canada ) ( NIPSâ€™18 ) . Curran Associates Inc . , Red Hook , NY , USA , 2935 â€“ 2945 . [ 5 ] Monica Babes , Vukosi N Marivate , Kaushik Subramanian , and Michael L Littman . 2011 . Apprenticeship learning about multiple intentions . In ICML . [ 6 ] Andrea Bajcsy , Dylan P . Losey , Marcia K . Oâ€™Malley , and Anca D . Dragan . 2017 . Learning Robot Objectives from Physical Human Interaction . In Proceedings of the 1st Annual Conference on Robot Learning ( Proceedings of Machine Learning Research , Vol . 78 ) , Sergey Levine , Vincent Vanhoucke , and Ken Goldberg ( Eds . ) . PMLR , 217 â€“ 226 . http : / / proceedings . mlr . press / v78 / bajcsy17a . html [ 7 ] Vassileios Balntas , Edgar Riba , Daniel Ponsa , and Krystian Mikolajczyk . 2016 . Learning local feature descriptors with triplets and shallow convolutional neural networks . 119 . 1 â€“ 119 . 11 . https : / / doi . org / 10 . 5244 / C . 30 . 119 [ 8 ] Erdem Biyik and Dorsa Sadigh . 2018 . Batch active preference - based learning of reward functions . In Conference on robot learning . PMLR , 519 â€“ 528 . [ 9 ] Andreea Bobu , Chris Paxton , Wei Yang , Balakumar Sundaralingam , Yu - Wei Chao , Maya Cakmak , and Dieter Fox . 2022 . Learning Perceptual Concepts by Bootstrapping From Human Queries . IEEE Robotics Autom . Lett . 7 , 4 ( 2022 ) , 11260 â€“ 11267 . https : / / doi . org / 10 . 1109 / LRA . 2022 . 3196164 [ 10 ] Andreea Bobu , Marius Wiggert , Claire Tomlin , and Anca D . Dragan . 0 . Inducing Structure in Reward Learning by Learning Features . The International Jour - nal of Robotics Research 0 , 0 ( 0 ) , 02783649221078031 . https : / / doi . org / 10 . 1177 / 02783649221078031 arXiv : https : / / doi . org / 10 . 1177 / 02783649221078031 [ 11 ] AndreeaBobu , MariusWiggert , ClaireTomlin , andAncaD . Dragan . 2021 . Feature Expansive Reward Learning : Rethinking Human Input . In Proceedings of the 2021 ACM / IEEE International Conference on Human - Robot Interaction ( Boulder , CO , USA ) ( HRI â€™21 ) . Association for Computing Machinery , New York , NY , USA , 216 â€“ 224 . https : / / doi . org / 10 . 1145 / 3434073 . 3444667 [ 12 ] Ralph Allan Bradley and Milton E Terry . 1952 . Rank analysis of incomplete block designs : I . Themethodofpairedcomparisons . Biometrika 39 , 3 / 4 ( 1952 ) , 324 â€“ 345 . [ 13 ] Daniel Brown , Russell Coleman , Ravi Srinivasan , and Scott Niekum . 2020 . Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences . In Proceedings of the 37th International Conference on Machine Learning ( Proceedings of Machine Learning Research , Vol . 119 ) , Hal DaumÃ© III and Aarti Singh ( Eds . ) . PMLR , 1165 â€“ 1177 . http : / / proceedings . mlr . press / v119 / brown20a . html [ 14 ] DanielBrown , WonjoonGoo , PrabhatNagarajan , andScottNiekum . 2019 . Extrap - olating beyond suboptimal demonstrations via inverse reinforcement learning from observations . In International Conference on Machine Learning . PMLR , 783 â€“ 792 . [ 15 ] Kalesha Bullard , Sonia Chernova , and Andrea Lockerd Thomaz . 2018 . Human - Driven Feature Selection for a Robotic Agent Learning Classification Tasks from Demonstration . In 2018 IEEEInternational Conferenceon Roboticsand Automation , ICRA 2018 , Brisbane , Australia , May 21 - 25 , 2018 . IEEE , 6923 â€“ 6930 . https : / / doi . org / 10 . 1109 / ICRA . 2018 . 8461012 [ 16 ] Maya Cakmak and Andrea Lockerd Thomaz . 2012 . Designing robot learners that ask good questions . In International Conference on Human - Robot Interaction , HRIâ€™12 , Boston , MA , USA - March 05 - 08 , 2012 , Holly A . Yanco , Aaron Steinfeld , Vanessa Evers , and Odest Chadwicke Jenkins ( Eds . ) . ACM , 17 â€“ 24 . https : / / doi . org / 10 . 1145 / 2157689 . 2157693 [ 17 ] AnnieS . Chen , SurajNair , andChelseaFinn . 2021 . LearningGeneralizableRobotic Reward Functions from " In - The - Wild " Human Videos . CoRR abs / 2103 . 16817 ( 2021 ) . arXiv : 2103 . 16817 https : / / arxiv . org / abs / 2103 . 16817 [ 18 ] Ricky T . Q . Chen , Xuechen Li , Roger B Grosse , and David K Duvenaud . 2018 . Isolating Sources of Disentanglement in Variational Autoencoders . In Advances in Neural Information Processing Systems , S . Bengio , H . Wallach , H . Larochelle , K . Grauman , N . Cesa - Bianchi , and R . Garnett ( Eds . ) , Vol . 31 . Curran Associates , Inc . [ 19 ] Ting Chen , Simon Kornblith , Mohammad Norouzi , and Geoffrey Hinton . 2020 . A simple framework for contrastive learning of visual representations . In Interna - tional conference on machine learning . PMLR , 1597 â€“ 1607 . [ 20 ] Xi Chen , Yan Duan , Rein Houthooft , John Schulman , Ilya Sutskever , and Pieter Abbeel . 2016 . InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets . In Proceedings of the 30th International Conference on Neural Information Processing Systems ( Barcelona , Spain ) ( NIPSâ€™16 ) . Curran Associates Inc . , Red Hook , NY , USA , 2180 â€“ 2188 . [ 21 ] Jaedeug Choi and Kee - Eung Kim . 2012 . Nonparametric Bayesian inverse rein - forcementlearningformultiplerewardfunctions . AdvancesinNeuralInformation Processing Systems 25 ( 2012 ) . [ 22 ] Paul F Christiano , Jan Leike , Tom Brown , Miljan Martic , Shane Legg , and Dario Amodei . 2017 . Deep Reinforcement Learning from Human Preferences . In Advances in Neural Information Processing Systems , I . Guyon , U . V . Luxburg , S . Bengio , H . Wallach , R . Fergus , S . Vishwanathan , and R . Garnett ( Eds . ) , Vol . 30 . Curran Associates , Inc . https : / / proceedings . neurips . cc / paper / 2017 / file / d5e2c0adad503c91f91df240d0cd4e49 - Paper . pdf [ 23 ] Adam Coates and A . Ng . 2012 . Learning Feature Representations with K - Means . In Neural Networks : Tricks of the Trade . [ 24 ] ErwinCoumansandYunfeiBai . 2016 â€“ 2019 . PyBullet , aPythonmoduleforphysics simulation for games , robotics and machine learning . http : / / pybullet . org . [ 25 ] CagatayDemiralp , MichaelBernstein , andJeffreyHeer . 2014 . LearningPerceptual KernelsforVisualizationDesign . IEEETransactionsonVisualizationandComputer Graphics 20 . https : / / doi . org / 10 . 1109 / TVCG . 2014 . 2346978 [ 26 ] Christos Dimitrakakis and Constantin A Rothkopf . 2011 . Bayesian multitask inverse reinforcement learning . In European workshop on reinforcement learning . Springer , 273 â€“ 284 . [ 27 ] Carl Doersch , Abhinav Kumar Gupta , and Alexei A . Efros . 2015 . Unsupervised Visual Representation Learning by Context Prediction . 2015 IEEE International Conference on Computer Vision ( ICCV ) ( 2015 ) , 1422 â€“ 1430 . [ 28 ] A . D . Dragan , K . Muelling , J . AndrewBagnell , andS . S . Srinivasa . 2015 . Movement primitives via optimization . In 2015 IEEE International Conference on Robotics and Automation ( ICRA ) . 2339 â€“ 2346 . https : / / doi . org / 10 . 1109 / ICRA . 2015 . 7139510 [ 29 ] Chelsea Finn , Pieter Abbeel , and Sergey Levine . 2017 . Model - Agnostic Meta - Learning for Fast Adaptation of Deep Networks . In Proceedings of the 34th Inter - national Conference on Machine Learning - Volume 70 ( Sydney , NSW , Australia ) ( ICMLâ€™17 ) . JMLR . org , 1126 â€“ 1135 . [ 30 ] Adam Gleave and Oliver Habryka . 2018 . Multi - task maximum entropy inverse reinforcement learning . arXiv preprint arXiv : 1805 . 08882 ( 2018 ) . [ 31 ] Bradley Hayes and Brian Scassellati . 2014 . Discovering task constraints through observation and active learning . In 2014 IEEE / RSJ International Conference on Intelligent Robots and Systems , Chicago , IL , USA , September 14 - 18 , 2014 . IEEE , 4442 â€“ 4449 . https : / / doi . org / 10 . 1109 / IROS . 2014 . 6943191 [ 32 ] Irina Higgins , LoÃ¯c Matthey , Arka Pal , Christopher P . Burgess , Xavier Glorot , Matthew M . Botvinick , Shakir Mohamed , and Alexander Lerchner . 2017 . beta - VAE : Learning Basic Visual Concepts with a Constrained Variational Framework . In ICLR . [ 33 ] Chao Huang , Wenhao Luo , and Rui Liu . 2021 . Meta Preference Learning for Fast User Adaptation in Human - Supervisory Multi - Robot Deployments . In 2021 IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) . IEEE , 5851 â€“ 5856 . [ 34 ] Hong Jun Jeon , Smitha Milli , and Anca Dragan . 2020 . Reward - rational ( implicit ) choice : Aunifyingformalismforrewardlearning . AdvancesinNeuralInformation Processing Systems 33 ( 2020 ) , 4415 â€“ 4426 . [ 35 ] Maurice George Kendall . 1948 . Rank correlation methods . ( 1948 ) . [ 36 ] Diederik P . Kingma and Max Welling . 2014 . Auto - Encoding Variational Bayes . In 2nd International Conference on Learning Representations , ICLR 2014 , Banff , AB , Canada , April 14 - 16 , 2014 , Conference Track Proceedings , Yoshua Bengio and Yann LeCun ( Eds . ) . http : / / arxiv . org / abs / 1312 . 6114 [ 37 ] Johannes Kulick , Marc Toussaint , Tobias Lang , and Manuel Lopes . 2013 . Active Learning for Teaching a Robot Grounded Relational Symbols . In IJCAI 2013 , Proceedings of the 23rd International Joint Conference on Artificial Intelligence , Beijing , China , August 3 - 9 , 2013 , Francesca Rossi ( Ed . ) . IJCAI / AAAI , 1451 â€“ 1457 . http : / / www . aaai . org / ocs / index . php / IJCAI / IJCAI13 / paper / view / 6706 [ 38 ] Cheng - I Lai . 2019 . Contrastive Predictive Coding Based Feature for Automatic Speaker Verification . arXiv preprint arXiv : 1904 . 01575 ( 2019 ) . [ 39 ] Michael Laskin , Aravind Srinivas , and Pieter Abbeel . 2020 . CURL : Contrastive Unsupervised Representations for Reinforcement Learning . In Proceedings of the 37th International Conference on Machine Learning ( Proceedings of Machine Learning Research , Vol . 119 ) , Hal DaumÃ© III and Aarti Singh ( Eds . ) . PMLR , 5639 â€“ 5650 . https : / / proceedings . mlr . press / v119 / laskin20a . html [ 40 ] Sergey Levine , Aviral Kumar , George Tucker , and Justin Fu . 2020 . Offline rein - forcement learning : Tutorial , review , and perspectives on open problems . arXiv preprint arXiv : 2005 . 01643 ( 2020 ) . [ 41 ] Kejun Li , Maegan Tucker , Erdem BÄ±yÄ±k , Ellen Novoseller , Joel W Burdick , Yanan Sui , Dorsa Sadigh , Yisong Yue , and Aaron D Ames . 2021 . Roial : Region of interest active learning for characterizing exoskeleton gait preference landscapes . In 2021 IEEE International Conference on Robotics and Automation ( ICRA ) . IEEE , 3212 â€“ 3218 . [ 42 ] Hoai Luu - Duc and Jun Miura . 2019 . An Incremental Feature Set Refinement in a Programming by Demonstration Scenario . In 4th IEEE International Conference on Advanced Robotics and Mechatronics , ICARM 2019 , Toyonaka , Japan , July 3 - 5 , 2019 . IEEE , 372 â€“ 377 . https : / / doi . org / 10 . 1109 / ICARM . 2019 . 8833723 [ 43 ] Zhao Mandi , Pieter Abbeel , and Stephen James . 2022 . On the Effectiveness of Fine - tuningVersusMeta - reinforcementLearning . arXivpreprintarXiv : 2206 . 03271 ( 2022 ) . HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD [ 44 ] Brian McFee , Gert Lanckriet , and Tony Jebara . 2011 . Learning Multi - modal Similarity . Journal of machine learning research 12 , 2 ( 2011 ) . [ 45 ] Jonathan Mumm and Bilge Mutlu . 2011 . Human - robot proxemics : physical and psychological distancing in human - robot interaction . In Proceedings of the 6th international conference on Human - robot interaction . 331 â€“ 338 . [ 46 ] Kentaro Nishi and Masamichi Shimosaka . 2020 . Fine - grained driving behavior prediction via context - aware multi - task inverse reinforcement learning . In 2020 IEEE International Conference on Robotics and Automation ( ICRA ) . IEEE , 2281 â€“ 2287 . [ 47 ] TakayukiOsa , JoniPajarinen , GerhardNeumann , JAndrewBagnell , PieterAbbeel , Jan Peters , et al . 2018 . An Algorithmic Perspective on Imitation Learning . Foun - dations and Trends in Robotics 7 , 1 - 2 ( 2018 ) , 1 â€“ 179 . [ 48 ] Deepak Pathak , Parsa Mahmoudieh , Guanghao Luo , Pulkit Agrawal , Dian Chen , Fred Shentu , Evan Shelhamer , Jitendra Malik , Alexei A . Efros , and Trevor Darrell . 2018 . Zero - Shot Visual Imitation . In 2018 IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops ( CVPRW ) . 2131 â€“ 21313 . https : / / doi . org / 10 . 1109 / CVPRW . 2018 . 00278 [ 49 ] C . J . Reed , X . Yue , A . Nrusimha , S . Ebrahimi , V . Vijaykumar , R . Mao , B . Li , S . Zhang , D . Guillory , S . Metzger , K . Keutzer , and T . Darrell . 2022 . Self - Supervised Pretraining Improves Self - Supervised Pretraining . In 2022 IEEE / CVF Winter Con - ference on Applications of Computer Vision ( WACV ) . IEEE Computer Society , Los Alamitos , CA , USA , 1050 â€“ 1060 . https : / / doi . org / 10 . 1109 / WACV51458 . 2022 . 00112 [ 50 ] Dorsa Sadigh , Anca D Dragan , Shankar Sastry , and Sanjit A Seshia . 2017 . Active preference - based learning of reward functions . In Robotics : Science and systems . [ 51 ] Seyed Kamyar Seyed Ghasemipour , Shixiang Shane Gu , and Richard Zemel . 2019 . Smile : Scalable meta inverse reinforcement learning through context - conditional policies . Advances in Neural Information Processing Systems 32 ( 2019 ) . [ 52 ] Arjun Sripathy , Andreea Bobu , Zhongyu Li , Koushil Sreenath , Daniel S . Brown , and Anca D . Dragan . 2022 . Teaching Robots to Span the Space of Functional Expressive Motion . https : / / doi . org / 10 . 48550 / ARXIV . 2203 . 02091 [ 53 ] Neil Stewart , Gordon DA Brown , and Nick Chater . 2005 . Absolute identification by relative judgment . Psychological review 112 , 4 ( 2005 ) , 881 . [ 54 ] Omer Tamuz , Ce Liu , Serge Belongie , Ohad Shamir , and Adam Tauman Kalai . 2011 . Adaptively learning the crowd kernel . arXiv preprint arXiv : 1105 . 1033 ( 2011 ) . [ 55 ] Christian Wirth , Riad Akrour , Gerhard Neumann , Johannes FÃ¼rnkranz , et al . 2017 . A survey of preference - based reinforcement learning methods . Journal of Machine Learning Research 18 , 136 ( 2017 ) , 1 â€“ 46 . [ 56 ] Kelvin Xu , Ellis Ratner , Anca Dragan , Sergey Levine , and Chelsea Finn . 2019 . Learning a Prior over Intent via Meta - Inverse Reinforcement Learning . In Pro - ceedings of the 36th International Conference on Machine Learning ( Proceedings of Machine Learning Research , Vol . 97 ) , Kamalika Chaudhuri and Ruslan Salakhutdi - nov ( Eds . ) . PMLR , 6952 â€“ 6962 . https : / / proceedings . mlr . press / v97 / xu19d . html [ 57 ] Kelvin Xu , Ellis Ratner , Anca Dragan , Sergey Levine , and Chelsea Finn . 2019 . Learning a prior over intent via meta - inverse reinforcement learning . In Interna - tional Conference on Machine Learning . PMLR , 6952 â€“ 6962 . [ 58 ] Lantao Yu , Tianhe Yu , Chelsea Finn , and Stefano Ermon . 2019 . Meta - inverse reinforcement learning with probabilistic context variables . Advances in Neural Information Processing Systems 32 ( 2019 ) . SIRL : Similarity - based Implicit Representation Learning HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden A APPENDIX A . 1 Trajectory generation In GridRobot the state space is discretized , so the trajectory space Î can be enumerated ; however , the JacoRobot state space is continu - ous , so we need to construct Î by sampling the infinite - dimensional trajectory space . We randomly sample 10 , 000 start - goal pairs and compute the shortest path in the robotâ€™s configuration space for each of them , ğœ‰ ğ‘†ğº . Each trajectory has a horizon length ğ» and consists of ğ‘› - dimensional states . We then apply random torque de - formations ğ‘¢ to each trajectory to obtain a deformed trajectory ğœ‰ ğ‘†ğºğ· . In particular , we randomly select up to 3 states along the trajectory , and then deform each of the selected states with a different random torque ğ‘¢ . To deform a trajectory in the direction of ğ‘¢ we follow : ğœ‰ ğ‘†ğºğ· = ğœ‰ ğ‘†ğº + ğœ‡ğ´ âˆ’ 1 Ëœ ğ‘¢ , ( 5 ) where ğ´ âˆˆ R ğ‘› ( ğ» + 1 ) Ã— ğ‘› ( ğ» + 1 ) defines a norm on the Hilbert space of trajectories and dictates the deformation shape [ 28 ] , ğœ‡ > 0 scales the magnitude of the deformation , and Ëœ ğ‘¢ âˆˆ R ğ‘› ( ğ» + 1 ) is ğ‘¢ at indices ğ‘›ğ‘¡ through ğ‘› ( ğ‘¡ + 1 ) and 0 otherwise ( Ëœ ğ‘¢ is 0 outside of the chosen deformation state index ) . For each deformation , we randomly gen - erated ğœ‡ and the index of the state the deformation is applied to . For smooth deformations , we used a norm ğ´ based on acceleration , but other norm choices are possible as well ( see Dragan et al . [ 28 ] for more details ) . We took inspiration for this deformation strategy from Bajcsy et al . [ 6 ] . A . 2 Training details We present architecture and optimization details that can assist in reproducing our training setup . A . 2 . 1 Feature networks . All embeddings have the same network size : for GridRobot we used MLPs with 2 hidden layers , 128 units each , mapping to 6 output neurons , while for JacoRobot we used 1024 units to handle the larger input space . For both environments , we used ReLU non - linearities after every linear layer . We trained the VAE network with a standard variational recon - struction loss [ 36 ] also including a KL - divergence - based regular - ization term ( to make the latent space regular ) . The regularization part of the loss had a weight of ğœ† = 0 . 01 . For both environments , we optimized the loss function using Adam for 2000 epochs with an exponentially decaying learning rate of 0 . 01 ( decay rate 0 . 99999 ) and a batch - size of 32 . SinglePref and MultiPref with 10 and 50 humans are trained using the standard preference loss in Eq . ( 4 ) . Christiano et al . [ 22 ] ensured that the rewards predicted by the preference network remain small by normalizing them on the fly . We instead add an ğ‘™ 2 regulatization term on the predicted reward to the preference loss , with a weight of 10 for GridRobot and 1 for JacoRobot . All three methods optimize this final loss in the same way : for GridRobot , we use Adam for 5000 epochs with a learning rate of 0 . 01 and batch - size 32 , while for JacoRobot we found a lower learning rate of 0 . 001 to result in more stable training . Lastly , for SIRL we had the option to first pre - train with the above VAE loss . Training with the similarity objective in Eq . ( 2 ) happens disjointly , after pre - training . For both GridRobot and JacoRobot , we optimized this loss function using Adam for 3000 epochs with an exponentially decaying learning rate of 0 . 004 ( decay rate 0 . 99999 ) and batch - size 64 . We note that our current architectures assume fixed - length tra - jectories but one could adopt an LSTM - based architecture for tra - jectories of varying length [ 52 ] . A . 2 . 2 Preference networks . For evaluating TPA , we used preference networks on top of the embeddings for the respective methods we evaluate . For GridRobot we used MLPs with 2 hidden layers of 128 units , and for JacoRobot we used 1024 units for larger capacity . For both environments , we used ReLU non - linearities after every linear layer . We added the same ğ‘™ 2 regularization to the loss in Eq . ( 4 ) as before , with weight 10 for GridRobot and 1 for JacoRobot . For GridRobot , we optimized our final loss function using Adam for 500 epochs with a learning rate of 0 . 001 and a batch - size of 64 . For JacoRobot , we increased the number of epochs to 1000 . A . 3 Ablations Figure 5 illustrates results with frozen SIRL , and unfrozen baselines without VAE pre - training , as these were the best configurations we found for each method . In this section , we show the complete abla - tion we performed to decide which methods benefit from frozen or unfrozen embeddings , or VAE pre - training . Figure 7 showcases the result of this ablation on both GridRobot and JacoRobot . Overall , we see that SIRL does better when the learned representation is frozen , while all the other methods do better when the representa - tions is unfrozen . SinglePref and the MultiPref baselines perform better without VAE pre - training , while SIRL sometimes benefits from pre - training in simple environments like GridRobot . HRI â€™23 , March 13 â€“ 16 , 2023 , Stockholm , Sweden TBD 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot Test Preference Accuracy ( TPA ) with N = 1000 SIRL ( frozen ) SIRL ( unfrozen ) SIRL + VAE ( frozen ) SIRL + VAE ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot Test Preference Accuracy ( TPA ) with N = 1000 SIRL ( frozen ) SIRL ( unfrozen ) SIRL + VAE ( frozen ) SIRL + VAE ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot Test Preference Accuracy ( TPA ) with N = 1000 SinglePref ( frozen ) SinglePref ( unfrozen ) SinglePref + VAE ( frozen ) SinglePref + VAE ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot Test Preference Accuracy ( TPA ) with N = 1000 SinglePref ( frozen ) SinglePref ( unfrozen ) SinglePref + VAE ( frozen ) SinglePref + VAE ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot Test Preference Accuracy ( TPA ) with N = 1000 MultiPref10H ( frozen ) MultiPref10H ( unfrozen ) MultiPref + VAE10H ( frozen ) MultiPref + VAE10H ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot Test Preference Accuracy ( TPA ) with N = 1000 MultiPref10H ( frozen ) MultiPref10H ( unfrozen ) MultiPref + VAE10H ( frozen ) MultiPref + VAE10H ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot Test Preference Accuracy ( TPA ) with N = 1000 MultiPref50H ( frozen ) MultiPref50H ( unfrozen ) MultiPref + VAE50H ( frozen ) MultiPref + VAE50H ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot Test Preference Accuracy ( TPA ) with N = 1000 MultiPref50H ( frozen ) MultiPref50H ( unfrozen ) MultiPref + VAE50H ( frozen ) MultiPref + VAE50H ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A GridRobot Test Preference Accuracy ( TPA ) with N = 1000 Random ( frozen ) Random ( unfrozen ) VAE ( frozen ) VAE ( unfrozen ) 10 30 50 70 90 110 130 150 170 190 Number of Preference Queries 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 TP A JacoRobot Test Preference Accuracy ( TPA ) with N = 1000 Random ( frozen ) Random ( unfrozen ) VAE ( frozen ) VAE ( unfrozen ) Figure 7 : Ablation results for GridRobot ( left ) and JacoRobot ( right ) . Overall , SIRL does better when the learned representation is frozen , while all other method do better when the representations is unfrozen . SinglePref and the MultiPref baselines perform better without VAE pre - training , while SIRL sometimes benefits from pre - training in simple environments like GridRobot .