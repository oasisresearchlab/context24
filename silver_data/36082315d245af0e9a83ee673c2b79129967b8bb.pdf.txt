Vol . : ( 0123456789 ) 1 3 https : / / doi . org / 10 . 1007 / s11121 - 022 - 01407 - y How Consistently Do 13 Clearinghouses Identify Social and Behavioral Development Programs as “Evidence‑Based” ? Jingwen Zheng 1 · Mansi Wadhwa 1 · Thomas D . Cook 1 , 2 Accepted : 1 July 2022 © Society for Prevention Research 2022 Abstract Clearinghouses develop scientific criteria that they then use to vet existing research studies on a program to reach a verdict about how evidence - based it is . This verdict is then recorded on a website in hopes that stakeholders in science , public policy , the media , and even the general public , will consult it . This paper ( 1 ) compares the causal design and analysis preferences of 13 clearinghouses that assess the effectiveness of social and behavioral development programs , ( 2 ) estimates how con - sistently these clearinghouses rank the same program , and then ( 3 ) uses case studies to probe why their conclusions differ . Most clearinghouses place their highest value on randomized control trials , but they differ in how they treat program imple - mentation , quasi - experiments , and whether their highest program ratings require effects of a given size that independently replicate or that temporally persist . Of the 2525 social and behavioral development programs sampled over clearinghouses , 82 % ( n = 2069 ) were rated by a single clearinghouse . Of the 297 programs rated by two clearinghouses , agreement about program effectiveness was obtained for about 55 % ( n = 164 ) , but the clearinghouses agreed much more on program inef - fectiveness than effectiveness . Most of the inconsistency is due to clearinghouses’ differences in requiring independently replicated and / or temporally sustained effects . Without scientific consensus about matters like these , “evidence - based” will remain more of an aspiration than achievement in the social and behavioral sciences . Keywords Evidence - based · Clearinghouse · Social and behavioral development programs Introduction The concept “evidence - based” assumes that high quality scientific studies can validly identify interventions with positive results , and that once these results are disseminated , they will lead to improved social and behavioral develop - ment outcomes nationally . Clearinghouses ( CHs ) play an important role in this process by specifying preferred sci - entific criteria for identifying causal impacts , by searching out studies that meet these standards , by evaluating each study to assess program effectiveness , and by synthesizing the results over studies to reach a conclusion about how evi - dence - based a program is — is it effective , promising , not promising , or even ineffective or harmful ? CHs then display their judgments on a web portal that researchers , policymak - ers , journalists , and the general public can access . This paper examines the reliability of “evidence - based” judgments over CHs seeking to prevent and ameliorate social and behavioral problems . Evidence - based has two main meanings , though the literature doubtless contains many others . One meaning refers to the use of explicit scientific standards to warrant claims of impact , without regard to the direction of this impact . In ordinary language usage , however , an evidence - based program connotes only positive impact . As a result , a program impact estimate can be judged evidence - based because certain scientific procedures were used , and also as not evidence - based because no positive impact was observed despite approved scientific procedures being used . These two meanings are almost always clear in context , and henceforth we rely on context to clarify whether evidence - based refers to applying scientific standards or claiming positive effects . * Thomas D . Cook t - cook @ northwestern . edu Jingwen Zheng jzheng @ gwu . edu Mansi Wadhwa mansi _ wadhwa @ gwu . edu 1 George Washington University , Washington DC , USA 2 Northwestern University , Evanston , USA / Published online : 30 August 2022 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Previous literature has discussed the contexts and insti - tutional factors in which CHs operate ( e . g . , their funding source , purpose , scope , dissemination , and marketing strate - gies ) ; and it also compares the different criteria CHs have adopted for including programs and studies and for evaluat - ing them , considering issues of research design , sample size and outcome measures among others ( Burkhardt et al . , 2015 ; Fagan & Buchanan , 2016 ; Gies et al . , 2020 ; Mayo - Wilson et al . , 2021 ; Means et al . , 2015 ; Westbrook et al . , 2017 ; Zack et al . , 2019 ) . While Burkhardt et al . ( 2015 ) , Means et al . ( 2015 ) , Fagan and Buchanan ( 2016 ) , and Mayo - Wilson et al . ( 2021 ) have examined CHs dealing with social and behav - ioral development , our paper is different because we offer a description of each CH’s criteria for justifying the causal conclusions it draws , estimate how correspondent are CH evaluations of the same program over the 13 CHs , and probe the main reasons that might explain the differences in pro - gram recommendations that we obtain . The first section of this paper describes the 13 CHs , with special emphasis on the causal designs and analyses they are willing to accept . The second section estimates how compa - rable CH ratings of the same program are . Estimating such replication is inherently perilous because ( 1 ) no scientific consensus yet exists about the desirability or necessity of some research procedures that affect causal estimates , ( 2 ) random errors influence estimates even when the same researchers simultaneously evaluate two identical versions of the same program using the same random assignment design and the same outcome , and ( 3 ) within - and between - study differences always exist in the populations , settings , times and in the intervention and outcome variants that are built into single studies . Most researchers know the perils but can - not predict how much inconsistency we will find over CHs , making the estimates we present novel for them as well as for every other stakeholder group in research on social and behavioral development . The third and final section of this paper employs case studies of social and behavioral development programs to explore four explanations for the inconsistency we describe in evidence - based ratings of the same program . Describing Clearinghouses We used three reviews of repositories of studies of individual interventions ( CNCS Evidence Exchange , 2016 ; Neuhoff et al . , 2015 ; Pew - McArthur , 2015 ) , an online search , and sug - gestions from other researchers to help identify 43 CHs in the USA and UK . From this list , we selected CHs with their own standards of evidence that are not borrowed from another CH , whose websites publish the standards of evidence used and the final evaluation reports on individual programs , and that explicitly deal with social and behavioral development programs . 30 CHs were excluded for these reasons ( See Table 6 in Online Appendix for details ) . A list of 13 CHs was finalized in May 2019 , and from June 2019 to August 2020 two reviewers independently collected data from CH webpages . Table 1 lists each of the 13 CHs , the acronyms we use to describe them , and some of their characteristics . The CHs vary in the objects they rate . Five give effective - ness ratings to programs , while the remaining CHs rate some combination of programs , policies , or practices . All 13 CHs review and assess programs , but two of them — SFER and PNRC — do not report summary ratings of each program on their webpage . Of the other 11 , few report how each study is rated by its own research quality standards even though these single studies have to have been vetted to contribute to the reported overall program effectiveness rating . The CHs also differ in their program and study inclusion criteria . Most CHs publish requirements for program inclusion and six CHs explicitly describe their program inclusion process . For exam - ple , CEBC uses a combination of expert advice and exten - sive literature search to find programs ; NGC adds programs that staff want to see included and also programs that CS had earlier assessed ; while EE reviews only interventions that it funded itself . The inclusion of studies for examining the effec - tiveness of a program depends on research design , publication date , sample size , outcome measures and target populations . Causal Identification and Estimation Since CHs test whether interventions affect a designated out - come , they should specify causal design and analysis pref - erences . Table 2 outlines the causal designs listed across the 13 CHs . Each CH was rated zero if it does not men - tion a particular causal design type ; one if a causal design type is mentioned but its meaning , measurement , or qual - ity implementation are not discussed ; two if the design is described in detail but no specific standards are provided to assess quality implementation ; and three if the CH details design - specific standards for causal inference and quality implementation . Evident from the table is that ( 1 ) most CHs invoke RCT as their preferred design ; ( 2 ) they treat issues related to its implementation and analysis more comprehen - sively than for quasi - experimental designs ( QEDs ) ; ( 3 ) all of them discuss a generic QED design characterized by non - randomly assigned treatment and comparison groups ; but ( 4 ) few CHs deal with specific QED types and then only the generic regression discontinuity ( RD ) and interrupted time series ( ITS ) designs . 1344 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Table 2 also shows that CHs vary in how they assess RCT study quality . Nine CHs have explicit guidelines for dealing with attrition . Three of them — CS , HomVEE , and SFER — follow the WWC guidelines on attrition wherein studies showing a tolerable level of expected bias ( 0 . 05 standard deviations or less ) , resulting from a combination of overall and differential attrition , can attain the highest study quality rating . Studies with higher attrition can still attain the second highest quality rating if they fulfill the requirement of baseline equivalence . Other CHs handle attrition differently , however . CMFR and SPTW accept overall attrition if it is under 20 % . BP requires statistical examination of baseline differences in outcomes and socio - demographic characteristics , and HomVEE requires base - line equivalence for demographic variables and tested out - comes , or inclusion of controls for these measures . Thus , while all CHs assign superior value to well - implemented RCTs , they differ in how they test whether an RCT is implemented well enough to deserve the website’s highest ranking for study quality . CHs vary even more in how they treat QEDs . All consider a generic class of QED where selection bias arises from treatment groups being non - randomly allocated , thus mak - ing it imperative to justify how the choice of comparison Table 1 Some differences between clearinghouses “Intervention” can be a program , policy , or practice . “Policy” refers to legal - administrative mandates from governments at different levels ; “pro - gram” refers to stand - alone programs and procedures often funded within programs ; and “practice” refers to activities designed primarily to improve policy or program implementation Name Primary focus field ( s ) Target population Type of funding Which objects are rated ? California Evidence Based Clearinghouse for Child Social and economic welfare ( CEBC ) Socio - behavioral develop - ment , health , socio - eco - nomic welfare , education Children and family Public Programs Promising Practices Network ( PPN ) Socio - behavioral develop - ment , health , education , socio - economic welfare Children and family Nonprofit Programs Social Programs That Work ( SPTW ) Socio - behavioral develop - ment , education , health , socio - economic welfare , labor All Nonprofit Programs Clearinghouse for Military Fam - ily Readiness - Continuum of Evidence ( CMFR ) Socio - behavioral develop - ment , health , education , labor Military family Public Programs National Gang Center ( NGC ) - Strategic Planning Tool Socio - behavioral develop - ment Youth ( under the age of 21 ) Public Programs Blueprints for Healthy Youth Development ( BP ) Socio - behavioral develop - ment , education , health Youth Nonprofit Programs ; study Home Visiting Evidence of Effectiveness ( HomVEE ) Socio - behavioral develop - ment , health , education , socio - economic welfare Families with pregnant women and children from birth to kindergarten entry Public Programs ; study Teen Pregnancy Prevention Evidence Review ( TPP ) Socio - behavioral develop - ment , health Youth Public Programs ; Study CNCS Evidence Exchange ( EE ) Socio - behavioral devel - opment , health , labor , socio - economic welfare , education All Public Programs and interventions Promise Neighborhoods Research Consortium ( PNRC ) Socio - behavioral develop - ment , education , health , socio - economic welfare , labor Youth Public Programs , policies and practices CrimeSolutions . org ( CS ) Socio - behavioral develop - ment All Public Programs and practices ; study What Works Clearinghouse ( WWC ) Socio - behavioral develop - ment , education Students Public Programs , policies , practices ; study Strengthening Families Evi - dence Review ( SFER ) Socio - behavioral develop - ment , socio - economic welfare , labor Children and family Public Study 1345 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 groups , covariates , and data analysis strategy might plausi - bly eliminate selection bias . Only HomVEE and CMFR have distinct standards for appraising the quality of such generic QEDs . HomVEE has separate standards for matched com - parison and single case designs ; and CMFR includes both case - matched and statistically controlled comparison groups . The maximum rating that can be assigned to generic QEDs is almost always lower than for RCTs . CS is one exception , though , since a generic QED can attain the highest qual - ity rating if the comparison group shows baseline equiva - lence ; CMFR is another exception since its highest qual - ity rating is possible if a QED meets the CH’s own QED standards . Generally , though , comprehensive discussion of QED implementation standards is rare . As for more specific QED designs , only WWC , CS , and PNRC explicitly mention ITS . WWC requires ITS studies to establish baseline equiva - lence for multiple time periods before treatment ; CS assigns a favorable score to ITS studies if they include a time series comparison group ; and PNRC provides a general guideline favoring ITS studies if they have sizable effects and long but not necessarily over - lapping baseline time periods . As for RDD , only four CHs explicitly mention it . WWC and Hom - VEE provide a detailed description of its implementation and analysis standards , allowing RDD studies to attain its most favorable rating if stringent evidence is available that a given application meets its assumptions . While all CHs consider a generic difference - in - difference model to which they generally assign lower value than to RCTs , they rarely detail better and worse QED variants . Notably , WWC is exceptional both in its extensive and nuanced treatment of most of the causal designs listed in Table 2 , including subtle methodological issues that researchers encounter while evaluating programs . Thus , it is particularly detailed about how to deal with treatment - related attrition , random assignment at the cluster level , missing outcome / baseline data , calculation of complier average causal effects , implementation standards for RCT and RDD , plus how to assess the quality of ITS and other common QED approaches . Ancillary Causal Conditions In any single study or review , there are many factors that can condition conclusions about effectiveness . Seven CHs invoke a replication standard . CEBC and HomVEE con - sider replication demonstrated if similar results are achieved in two non - overlapping samples ; while PNRC , SPTW and EE emphasize multi - site studies and effects that replicate across locations . To award a program its highest rating , BP requires at least one independent replication conducted by researchers who have no financial stake in the program and who are independent of the program developers and Table 2 The extent to which specific research designs are elaborated in a given clearinghouse 0 = the CH’s standards do not mention the specific causal design type . 1 = a causal design type under analysis is only briefly mentioned without specific elaboration on its mean - ing , measurement or quality implementation in the CH’s standards . 2 = a causal design type is described in detail , but no design - specific standards are provided for quality implementation . 3 = the CH details design - specific standards for both causal inference and quality implementation a SFER has separate QED standards but they are not as detailed . It also did not identify any RDD studies during the process of their review and hence , did not discuss guidelines for the same Clearinghouse RCT Undifferenti - ated QED RDD ITS CrimeSolutions . org ( CS ) 2 2 0 1 Blueprints for Healthy Youth Development ( BP ) 2 1 0 0 California Evidence Based Clearinghouse for Child Welfare ( CEBC ) 2 1 0 0 Home Visiting Evidence of Effectiveness ( HomVEE ) 3 3 3 0 Teen Pregnancy Prevention Evidence Review ( TPP ) 2 2 0 0 Promising Practices Network ( PPN ) 1 1 0 0 Strengthening Families Evidence Review ( SFER ) a 3 2 1 0 Social Programs That Work ( SPTW ) 3 1 0 0 Clearinghouse for Military Family Readiness ( CMFR ) 3 3 0 0 Promise Neighborhoods Research Consortium ( PNRC ) 2 1 1 1 CNCS Evidence Exchange ( EE ) 2 2 0 0 National Gang Center ( NGC ) - Strategic Planning Tool 2 1 0 0 What Works Clearinghouse ( WWC ) 3 2 3 2 1346 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 implementers . CMFR requires replications to entail only minor adaptations of program modalities and to involve no major changes in the populations studied . CHs also vary in the minimal post - intervention follow - up period within which effects must be demonstrated . Six CHs specify such a period , and four require it to attain high ratings . The most frequent follow - up period is 12 months or more after the intervention has ceased , but a period between 6 and 12 months is some - times accepted , albeit with a lower study quality rating . CHs also differ in how they deal with the size , statistical signifi - cance , and direction of effects . Some websites report only on programs with positive effects ( BP , PPN , and SPTW ) , while others publish details on all the programs considered , including those with null or negative effects . Some CHs ( e . g . , WWC , CMFR , CEBC ) distinguish between ineffec - tive programs versus those with indeterminate effects . And while all CHs invoke the statistical significance of causal findings , some also require minimal effect sizes ( e . g . , PPN requires a difference of at least 0 . 25 SDs ) . The estimate - conditioning factors are not emphasized among traditional causal identification and estimation fac - tors even though they can contribute to evidence - based sum - mary judgments . For example , one CH might claim that the very same numerical causal estimate is evidence - based because of a single statistically significant impact . Another might not , because the very same effect size is too small or fails to persist for a year after treatment withdrawal or is not independently and multiply replicated . Such ancillary hurdles for claiming program success differ by CHs and can be consequential for verdicts about how evidence - based a program is . The Joint Operation of Specific Standards To understand how the factors above jointly operate , we use five CHs that were selected for their heterogeneity , espe - cially in their schemes to rate programs , to rate studies , and in their criteria for achieving the highest rating a CH allows for . Table 3 lays out the details . One thing to be seen is that WWC and TPP analyze and report program ratings for each outcome domain separately . The other CHs — CS , BP , and CEBC — assign program ratings combining over one or more separate outcomes . CHs use different terminology to describe programs labeled as “evidence - based” . While CS invokes “effective programs” , WWC refers to “positive effects” , BP to “model” or “model plus” programs , TPP to programs with “positive impacts” , and CEBC of programs that are “well supported by research evidence . ” The evi - dence package each CH reviews depends on individual study results . But CHs differ in how study results are reported . CEBC provides only a summary of each study without a detailed appraisal of its results . In contrast , CS rates each study as Class 1 through 5 depending on the quality of its conceptual framework , experimental or quasi - experimental design , outcome evidence , and program fidelity ; BP desig - nates each study as “certifiable” or not , depending on how well it meets pre - specified criteria that primarily relate to causal identification and estimation ; WWC designates every single study as “meeting WWC evidence standards without reservations , ” meeting them “with reservations , ” or “not meeting” them ; and TPP assigns a high , moderate , or low rating based on each study’s design , attrition levels , baseline equivalence , and confounding issues . To achieve its highest effectiveness rating , CS requires at least one well - designed Class 1 study that identifies signifi - cant , positive effects on justice - related outcomes . Addition - ally , it requires evidence that the program has been imple - mented with fidelity , there is a follow - up period of more than 1 year from program end , and the authors of different studies are independent of each other . WWC’s highest rating depends on having at least two studies with statistically sig - nificant impacts on specified outcomes , one of which must meet WWC’s standards without reservations . There are no requirements about the effect size , follow - up period , or inde - pendence of evaluators . For its Model Plus rating , BP stipu - lates a minimum of two studies with statistically significant , positive results that meet its implementation and analysis requirements , one of which must be an RCT . In addition , the effect must be demonstrated over 12 months and the researchers conducting different study evaluations must be independent of the program developer . TPP needs only one highly or moderately rated study with positive results , and nothing is required about either evaluator independence or follow - up period . CEBC’s highest rating requires positive results from at least two well - implemented RCT studies , at least one of which has to show persisting effects for at least 12 months after treatment removal . It is obvious , therefore , that CHs vary in the number and stringency of the stand - ards they use for assigning their highest rating to a given program . How Consistent are Clearinghouse Ratings of the Same Program ? Methodology To estimate consistency in CH evidence - based summary judgments of program effectiveness , we first identified all the social and behavioral development interventions these 13 CHs rated . Table 7 ( Online Appendix ) lists the criteria used to compile this list of programs . We used two review - ers for generating the list and identifying which programs were rated by more than one CH . A total of 2525 unique 1347 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 T a b l e 3 D i v e r s i t y i n s o m e s t udy a nd p r og r a m a tt r i bu t e s : c o m p a r i s on o f C S , WW C , B P , T PP a nd C E BC C S WW C B P T PP C EB C O b j ec t o f R a t i n g I nd i v i du a l S t ud i e s , P r og r a m s a nd P r ac ti ce s I nd i v i du a l S t ud i e s a nd O u t c o m e - s p ec i fi c r a ti ng s o f i n t e r v e n ti on s I nd i v i du a l S t ud i e s a nd N a m e d P r og r a m s I nd i v i du a l S t ud i e s a nd P r o - g r a m s P r og r a m s , bu t no t i nd i v i du a l s t ud i e s O v er a ll R a t i n g s c h e m e A p r og r a m i s r a t e d a s : E ff ec ti v e , P r o m i s i ng , N o e ff ec t s , I n c on c l u s i v e e v i d e n ce , o r I n s u ffi c i e n t e v i d e n ce A s t udy ca n b e g i v e n a c l a ss o f 1 , 2 , 3 , 4 , a nd 5 , w it h c l a ss 1 s t ud i e s b e i n g t h e h i gh e s t r a ti ng I n t e r v e n ti on e ff ec t s a r e r a t e d : P o s iti v e ; P o t e n ti a ll y po s iti v e ; M i x e d ; P o t e n ti a ll y n e g a ti v e ; N e g a ti v e ; o r N o d i s ce r n i b l e e ff ec t s , a nd t h e e x t e n t o f e v i d e n ce f o r a c on c l u s i on ca n b e M e d i u m t o L a r g e o r S m a ll A s t udy ca n : M ee t WW C E v i d e n ce S t a nd - a r d s W it hou t R e s e r v a ti on s ; M ee t WW C S t a nd a r d s W it h R e s e r v a ti on s ; o r F a il t o m ee t WW C s t a nd a r d s A p r og r a m i s r a t e d a s : M od e l P l u s , M od e l , P r o m i s - i ng , o r N on - ce r ti fi e d s i n ce t h e e v i d e n ce i s I n s u ffi c i e n t o r I n c on c l u s i v e , o r t h e p r og r a m i s I n e ff ec ti v e o r H a r m f u l A s t udy ca n b e ce r ti fi e d a s a d e qu a t e o r no t t o e n t e r i n t o a r e v i e w A p r og r a m i s r a t e d a s h a v i ng : P o s iti v e i m p ac t s ; M i x e d i m p ac t s ; I nd e t e r m i n a t e i m p ac t s ; N e g a ti v e i m p ac t s A s t udy ca n b e g i v e n a h i gh , m od e r a t e , o r l o w s t udy r a ti ng A p r og r a m i s r a t e d a s : W e ll - s uppo r t e d by r e s ea r c h e v i - d e n ce ; S uppo r t e d by r e s ea r c h e v i d e n ce ; P r o m i s i ng r e s ea r c h e v i d e n ce ; E v i d e n ce f a il s t o d e m on s t r a t e e ff ec t ; N o t r a t a b l e S t ud y Q u a li t y R a t i n g s c h e m e S t udy qu a lit y i s a ss e ss e d b a s e d on c on ce p t u a l fr a m e w o r k , d e s i gn qu a lit y , ou t c o m e e v i d e n ce a nd p r og r a m fi d e lit y . D e s i gn qu a lit y i s f u r t h e r a ss e ss e d b a s e d on r e s ea r c h d e s i gn ( w it h h i gh e s t s c o r e on l y f o r w e ll - d e s i gn e d RC T ) , s a m p l e s i ze , s t a ti s ti ca l a d j u s t m e n t , i n t e r n a l v a li d it y , i n s t r u m e n - t a ti on , f o ll o w - up p e r i od , a nd d i s p l ace m e n t H i gh e s t po s iti v e s t udy r a ti ng : S t ud i e s a r e v e r y r i go r ou s a nd w e ll - d e s i gn e d a nd fi nd s i g - n i fi ca n t , po s iti v e e ff ec t s on j u s ti ce - r e l a t e d ou t c o m e s H i gh e s t po s iti v e s t udy r a ti ng : I nd i v i du a l - l e v e l RC T w it h l o w a tt r iti on , C l u s t e r - RC T s w it h no r i s k o f b i a s R DD s s a ti s f y i ng ce r t a i n c ond iti on s S ec ond s t udy r a ti ng l e v e l : RC T s w it h h i gh a tt r iti on a nd Q E D s t h a t s a ti s f y b a s e - li n e e qu i v a l e n ce a nd o t h e r c r it e r i a S t udy qu a lit y i s a ss e ss e d b a s e d on : T r ea t m e n t a ss i gn - m e n t p r o ce du r e , a tt r iti on l e v e l s , d e g r ee o f b a s e li n e e qu i v a l e n ce , u s e o f a pp r o - p r i a t e s t a ti s ti ca l m e t hod , e s ti m a t e s a r e I TT , n a t u r e o f ou t c o m e s , s a m p l e s i ze , r e li - a b l e a nd v a li d m ea s u r e s H i gh s t udy r a ti ng : R a ndo m o r f un c ti on a ll y r a ndo m a ss i gn m e n t w it h l o w a tt r iti on , b a s e li n e e qu i v a l e n ce , a n a l y s i s b a s e d on o r i g i n a l a ss i gn m e n t t o r e s ea r c h g r oup s , a t l ea s t t w o s ub j ec t s o r g r oup s i n eac h r e s ea r c h g r oup a nd no s y s t e m a ti c d i ff e r e n ce s i n d a t a c o ll ec ti on m e t hod s T h e r a ti ng s f o r i nd i v i du a l s t ud i e s no t r e po r t e d on it s w e b s it e , bu t c r it e r i a s t a ff u s e i n c l ud e : s t udy d e s i gn ( on l y RC T s o r Q E D s ) , n a t u r e o f c on t r o l / c o m p a r i s on g r oup , r e li a b ilit y a nd v a li d it y o f a t l ea s t on e ou t c o m e , a ll i n t e r v e n ti on c o m pon e n t s a r e a n a l y ze d t og e t h e r C r i t er i a f o r t h e H i g h e s t P r og r a m E ff ec t i v e n e ss R a t i n g N u m b er a nd T y p e o f S t ud i e s M i n i m u m on e s t udy w it h c l a ss 1 s t udy r a ti n g M i n i m u m t w o s t ud i e s ; a t l ea s t on e m u s t b e r a t e d ‘ M ee t s WW C E v i d e n ce S t a nd a r d s W it hou t R e s e r v a ti on s ’ M i n i m u m t w o h i gh - qu a lit y RC T s o r on e h i gh qu a lit y RC T a nd on e h i gh qu a lit y Q E D M i n i m u m on e s t udy w it h h i gh o r m od e r a t e s t udy r a ti ng M i n i m u m t w o h i gh - qu a lit y RC T s w it h non - ov e r l a pp i ng s a m p l e s D e m o n s t r a t i o n o f E ff ec t P o s iti v e e ff ec t on s p e c i fi e d ou t c o m e s P o s iti v e e ff ec t on s p ec i fi e d ou t c o m e s P o s iti v e e ff ec t on s p ec i fi e d ou t c o m e s P o s iti v e e ff ec t s on s p ec i fi e d ou t c o m e s P o s iti v e e ff ec t s s ho w n S t a t i s t i c a l S i g n i fi c a n ce o f E ff ec t S t a ti s ti ca ll y s i gn i fi ca n t e ff ec t r e qu i r e d S t a ti s ti ca ll y s i gn i fi ca n t e ff ec t r e qu i r e d S t a ti s ti ca ll y s i gn i fi ca n t e ff ec t r e qu i r e d S t a ti s ti ca ll y s i gn i fi ca n t e ff ec t r e qu i r e d N o t s p ec i fi e d 1348 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 T a b l e 3 ( c on ti nu e d ) C S WW C B P T PP C EB C I a t r og e n i c E ff ec t s N o e v i d e n ce o f i n e ff e c ti v e e ff ec t s ca n b e f oun d i n a ny s t ud i e s e v a l u a t e d f o r t h e p r og r a m r a ti ng N o s t ud i e s ca n s ho w s t a ti s ti - ca ll y s i gn i fi ca n t n e g a ti v e , o r s ub s t a n ti v e l y i m po r t a n t , n e g a ti v e e ff ec t s N o e v i d e n ce o f i a t r og e n i c e ff ec t s m u s t b e f ound i n a ny ce r ti fi e d s t udy i n a n a r ea ce n t r a l t o B P m i ss i on N o e v i d e n ce o f a ny a dv e r s e , s t a ti s ti ca ll y s i gn i fi ca n t i m p ac t s on a ny ou t c o m e s N o i a t r og e n i c o r l e g a ll y c o m - p r o m i s i ng e ff ec t s a r e a ll o w e d I nd e p e nd e n ce o f E va l u a t o r I nd e p e nd e n ce o f t h e a u t ho r s f o r e v a l u a ti v e s t udy N o t s p ec i fi e d A t l ea s t on e s t udy m u s t b e by a r e s ea r c h e r i nd e p e nd e n t o f t h e d e v e l op e r a nd o t h e r r e s ea r c h e r s N o t s p ec i fi e d N o t s p ec i fi e d F o ll o w - up P er i o d F o ll o w - up m o r e t h a n 1 y ea r fr o m p r og r a m e nd N o t s p ec i fi e d S u s t a i n e d e ff ec t s s ho w n a t l ea s t on ce 12 m on t h s po s t t r ea t m e n t N o t s p ec i fi e d O n e RC T m u s t s ho w s u s t a i n e d e ff ec t s a t l ea s t 12 m on t h s po s t t r ea t m e n t O t h er F a c t o r s T h e p r og r a m m u s t b e i m p l e - m e n t e d w it h fi d e lit y D o c u m e n t a ti on o f t h e p r og r a m m u s t b e a v a il a b l e on li n e o r i n m a t e r i a l f o r m D o c u m e n t a ti on on p r og r a m c o m pon e n t s a nd a d m i n i s t r a - ti on H o w 2 nd H i g h e s t E ff ec t i v e n e ss R a t i n g D i ff er s f r o m H i g h e s t R a t i n g C r i t er i a A b ov e ? R e qu i r e s a t l ea s t on e c l a ss 2 s t udy s ho w i ng s o m e e v i d e n ce t o i nd i ca t e t h e y ac h i e v e t h e i r i n t e nd e d ou t c o m e s R e qu i r e s on e s t udy i n s t ea d o f t w o ; s t a ti s ti ca ll y s i gn i fi ca n t o r s ub s t a n ti v e l y i m po r t a n t ( > 0 . 25 S D ) po s iti v e e ff ec t s ho w n ; a ny s t ud i e s s ho w i ng “ i nd e t e r m i n a t e e ff ec t s ” m u s t b e < = i n nu m b e r t o t ho s e s ho w i ng po s iti v e e ff ec t s S t a nd a r d s a r e t h e s a m e a s f o r t h e h i gh e s t r a ti ng e x ce p t no r e qu i r e m e n t t h a t on e s t udy i s c ondu c t e d by a n i nd e p e nd e n t r e s ea r c h e r E v i d e n ce o f a m i x o f f a vo r a - b l e , nu ll , a nd / o r a dv e r s e i m p ac t s ac r o ss on e o r m o r e ou t c o m e m ea s u r e s , a n a l y ti c s a m p l e s ( f u ll s a m p l e o r s ubg r oup s ) , a nd / o r s t ud i e s R e qu i r e s on e RC T i n s t ea d o f t w o ; a nd a s ho r t e r f o ll o w - up p e r i od o f 6 m on t h s po s t t r ea t m e n t 1349 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 programs were identified . Three issues complicated the analysis . First , the same core program name can refer to somewhat different components or variants , likely due to developers modifying their programs with the passage of time . We classified each variant as unique if another name was used , even the program details were essentially the same except for some marginally modified details . Second , the same name could be used for different programs in dif - ferent CHs . To examine this requires in - depth examination of all 2525 programs , but we did not do this in light of the proprietary interest program developers have in linking their intervention to a specific brand name as opposed to appropriating an already known program name . To estimate consistency , the summary ratings each CH provides were re - scaled onto a 3 - point scale ( see Table 8 in Online Appendix for details ) . Any judgment that a program is acceptably effective is scored as 1 ( = Evidence - based ) . For some CHs , this means collapsing different levels of certainty about effectiveness , as with BP’s Model and Model Plus judgments and the CBEC judgments of Well - supported by Research Evidence and Supported by Research Evidence . We score as 2 ( = Promising ) all ratings where some results are judged to be likely justified , but plausible doubt still exists . Most CHs invoke their own Promising category , and we use that . Finally , we score as 3 ( = Not Evidence - based ) all evidence - based ratings indicating inconclusive or negative effects . When CHs agree on a 1 , 2 or 3 for a given program , consistency is clear . Inconsistency is most clear when the CH ratings are 1 and 3 ( evidence - based versus not evidence - based ) . Differences of 1 and 2 indicate two judgments that are positive but vary in how certain the evidence is . CH ratings of 2 and 3 point to a program that is designated as promising in one CH but as having inconclusive or negative effects in another . To estimate consistency , we use CH rat - ing pairs . When a program is rated by only two CHs , only one rating pair is possible ; but when four CH ratings are available , six ( partially dependent ) rating pairs are possible . We analyze the percentage of such pairs that are ( a ) fully consistent — either 1 , 2 or 3 — but analyzing each of these possibilities separately ; ( b ) moderately consistent , since the scores are 1 and 2 ; ( c ) moderately inconsistent , with scores of 2 and 3 ; and ( d ) fully inconsistent with scores of 1 and 3 . Results Of the 2525 social and behavioral development programs under investigation , 81 . 9 % were rated by a single CH , 11 . 8 % by two , 3 . 7 % by three , and the remaining 2 . 6 % by four or more . Thus , only 18 . 1 % of all programs have at least one replicated CH judgment about how evidence - based they are . Examining their names strongly suggests that these replicated program are among the best - known , and so interested stakeholders seeking to cross - check CH verdicts will most likely find multiply replicated informa - tion for programs that are older and better - known . Full Consistency in Program Ratings Consistency estimates for programs rated more than once are in Table 4 . When two CHs evaluate a program , full consistency over scores of 1 , 2 or 3 is 55 . 2 % over all rat - ing pairs , decreasing to 43 . 7 % for programs rated by three CHs , 37 . 7 % for four , and 36 . 7 % for five . However , such agreements are most common for programs that are not evidence - based and are rarest for those that are evidence - based . Thus , when two CHs rate the same program , 42 . 8 % of the programs achieve dual ratings of 3 , but only 4 . 0 % of them achieve the dual ratings of 1 that indicate CH agree - ment that a program is evidence - based according to its pre - ferred criteria of acceptable evidence ( see Fig . 1 in Online Appendix ) . Moderate Consistency in Program Ratings Moderately consistent rating pairs are where at least one CH fully endorses a program ( score of 1 ) but others consider it to be only promising ( score of 2 ) . However , none score it as a 3 . The fraction of such pairs increases with the number of CHs rating a program . Among programs rated by two CHs , 6 . 7 % of program ratings fall into this category , followed by 12 . 5 % for three , 12 . 3 % for four and 33 . 3 % for five . If we relax the criterion for consistent evidence - based judg - ments such that each CH scores the same program either as 1 or as a mix of 1’s and 2’s , then Table 5 results . When two CHs rate a program , 19 . 2 % of the programs are rated as evidence - based by this criterion . When three do so , about 17 . 2 % are rated as evidence - based ; and when four do so , the corresponding percentage is 10 . 9 % . So , consensus about the same program meeting this relaxed standard of consistency is around 19 . 2 % versus 4 . 0 % for all CHs rating a program as fully evidence - based . Moderate and Full Inconsistency in Program Ratings If we turn to less consistent patterns of program effectiveness , moderate inconsistency is for CH ratings of 2 and 3 only . About one - third of all rating pairs show this pattern over from two to four CHs – see Table 4 . Maximal disagreement is when one CH issues a 1 rating , but another a 3 . Table 4 shows that 1350 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 such extreme disagreement is not common , with the percent - age of such pairs varying between 9 . 8 % and 15 . 6 % depending on the number of CHs . Case Studies of Clearinghouse Agreement and Disagreement when Rating the Same Program Using the CH descriptions and consistency estimates just presented , we probed ( rather than formally tested ) four hypotheses about why CHs differ in rating the same program . They ( 1 ) vary in the studies included , ( 2 ) examine different versions of the same - named program , ( 3 ) analyze different outcomes for the same program , or ( 4 ) use different methodo - logical standards to warrant judgments about how evidence - based a study or program is . These explanations are not mutually exclusive , of course , and we examine them over four program cases that were chosen because they vary in the level of agreement that CHs have towards their program effective - ness and because they involve better - known programs that CH users are most likely to seek information about . For this section , two reviewers independently reviewed , collected , and coded data from program assessment reports published on CH websites . Where different variants of the same pro - gram were encountered , we examined each of them sepa - rately , albeit under the same generic program label . Table 4 Consistency in ratings when two or more clearinghouses rate the same program Programs rated by 6 , 7 , 8 and 9 CHs are not included as there are only such 8 social and behavioral devel - opment programs Percentage of rating pairs in social and behavioral development Programs rated by 2 CHs Full Consistency ( 1 & 1 or 2 & 2 or 3 & 3 ) 55 . 2 Moderate Consistency ( 1 & 2 ) 6 . 7 Moderate Inconsistency ( 2 & 3 ) 28 . 3 Full Inconsistency ( 1 & 3 ) 9 . 8 Total Number of Comparable Rating Pairs 297 Programs rated by 3 CHs Full Consistency ( 1 & 1 or 2 & 2 or 3 & 3 ) 43 . 7 Moderate Consistency ( 1 & 2 ) 12 . 5 Moderate Inconsistency ( 2 & 3 ) 33 . 3 Full Inconsistency ( 1 & 3 ) 10 . 4 Total Number of Comparable Rating Pairs 279 Programs rated by 4 CHs Full Consistency ( 1 & 1 or 2 & 2 or 3 & 3 ) 37 . 7 Moderate Consistency ( 1 & 2 ) 12 . 3 Moderate Inconsistency ( 2 & 3 ) 34 . 4 Full Inconsistency ( 1 & 3 ) 15 . 6 Total Number of Comparable Rating Pairs 276 Programs rated by 5 CHs Full Consistency ( 1 & 1 or 2 & 2 or 3 & 3 ) 36 . 7 Moderate Consistency ( 1 & 2 ) 33 . 3 Moderate Inconsistency ( 2 & 3 ) 15 . 0 Full Inconsistency ( 1 & 3 ) 15 . 0 Total Number of Comparable Rating Pairs 120 Table 5 Percentage of evidence - based and not evidence - based pro - grams when rated by two or more clearinghouses Percentage Programs rated by 2 CHs Both CHs rate as Not Evidence - based 80 . 8 Both CHs rate as Evidence - based 19 . 2 Programs rated by 3 CHs All 3 CHs rate as Not Evidence - based 17 . 2 1 CH out of 3 rates as Evidence - based 26 . 9 2 CHs out of 3 rate as Evidence - based 38 . 7 All 3 CHs rate as Evidence - based 17 . 2 Programs rated by 4 CHs All 4 CHs rate as Not Evidence - based 2 . 2 1 CH out of 4 rates as Evidence - based 17 . 4 2 CHs out of 4 rate as Evidence - based 39 . 1 3 CHs out of 4 rate as Evidence - based 30 . 4 All 4 CHs rate as Evidence - based 10 . 9 1351 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Big Brothers Big Sisters ( BBBS ) BBBS is a community - based mentoring program that seeks to promote positive development by matching economically disadvantaged children and youth to a vol - unteer adult . Five CHs rate the parent program BBBS ; two assess the BBBS School - Based Mentoring variant ; and two evaluate the Community - based Mentoring vari - ant . There is a high level of agreement when multiple CHs rate the same variant , but the variants differ in how evidence - based they are . Policymakers need to distin - guish them carefully . Of the five CH ratings of the parent program BBBS , three rate it as evidence - based ( PPN , NGC and PNRC ) , and two as promising ( SPTW and BP ) , indicating mod - est agreement . The CHs modestly differ in which studies they include in their review . SPTW uses one study , PPN two , NGC three , PNRC four , and BP certifies two studies but declares five others to be uncertifiable since they fail to meet methodological standards . One study is used by four CHs ( Tierney et al . , 1995 ) , and another by three of them ( Grossman & Tierney , 1998 ) . PPN and NGC give BBBS the highest rating because they require at least one study that meets their optimal rigor , but BP’s promising rating reflects the fact that , across all studies examined , the results turn out to be “very small and lack methodo - logical rigor” ( BP , n . d . ) . PNRC doesn’t justify its rec - ommendation , but from its evaluation criteria , we can infer that sufficient evidence of positive effects should exist from the four studies PNRC has reviewed for this program . SPTW’s promising rating is based on a single study that cannot receive the highest rating because it relies on self - reported outcome measures and has a short follow - up period . Nonetheless , the core BBBS program is evaluated more frequently , and it is universally assessed as either worth endorsing or promising . As for program variants , CS endorses BBBS Community - Based Mentoring . But CMFR finds it to be only promising since its top rating requires an independent replication and effects sustained either 2 years after the start of treatment or for at least 1 year after program completion . None is the case . Thus , different CH standards clearly affect final judg - ments of program merit in the space between full endorse - ment and promising verdict . Two CHs evaluate the BBBS School - based Mentoring variant , and both agree that it should not be endorsed or considered promising . CS reviews a single study and concludes that the evidence is insufficient for a rating . BP rates the same study as inconclusive but provides no detailed justification . In sum , there is considerable but not perfect CH agreement about how evidence - based the basic BBBS program is , but its scale - up to other contexts is judged as more problematic . The Incredible Years ( IY ) IY seeks to reduce child and adolescent behavioral problems while also improving their socio - emotional and academic competencies . Three main interventions exist under the generic IY umbrella , each targeting the separate training of students , parents , or teachers . Conscientious consumers of CH information need to carefully distinguish among them . Again , there is moderate agreement among CHs about IY’s effectiveness – all recommendation scores are of 1 or 2 , though CHs differ in the studies used and in judgements about individual study quality . Five CHs evaluate IY and vary considerably in how many studies are reviewed . PPN includes more than 40 , CEBC 10 , PNRC seven , NGC five , and WWC three . CEBC fully endorses IY as a generic program , hardly distinguishing between the three unique interventions . PPN and NGC discuss the evidence about each IY component and then endorse the program as a whole though PPN notes that long - term effects could not be examined since some studies used waitlist controls . PNRC too describes the operation of each unique IY component and recommends the program over - all given evidence of positive effects and readily available implementation resources . WWC rates it as only promising since its full endorsement requires two studies showing posi - tive results but only one study meets WWC’s specification of acceptable methodological rigor . Other CHs consider only the individual IY components , not the umbrella program . CS endorses IY Child Train - ing , while BP and CMFR rate it promising . BP uses two studies , CS three , and CMFR five . Only one study is com - mon to all ( Webster - Stratton et al . , 2004 ) , and another is shared by BP and CMFR only ( Webster - Stratton & Hamond , 1997 ) . BP rates this component as promising based on the absence of demonstrably sustained effects 12 months after treatment , while CMFR worries about the absence of repli - cation by independent researchers . Thus , the rating differ - ences are largely due to CH variation in evidence standards . The same three CHs also rate the IY Teacher Classroom Management Program , all finding it to be promising but for different reasons . BP finds multiple limitations in the stud - ies , including small sample sizes , absence of implementa - tion fidelity measures , and lack of sustained effects . CMFR notes that the results were stronger when the intervention was implemented in conjunction with other IY interventions and when children had less severe behavioral problems . The IY Parent Training component was rated by CMFR , BP and EE , receiving three different levels of endorsement . After reviewing 10 studies , CMFR gave this component its high - est rating . After examining 42 studies , but only one of them certifiable , BP found it to be promising due to the absence of an effect sustained 1 year after treatment . EE rated a specific 1352 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 variant of this component for Hmong and African American families , but it did not endorse this because no statistically significant effects were claimed in the evaluation reports . Obviously , judgment about IY depends on which of its program versions is under analysis , but the disagreements between CHs are not large . Community Advocacy Project ( CAP ) CAP is a community - based advocacy intervention that seeks to aid the survivors of intimate partner abuse . The program exemplifies modest disagreement . CEBC fully endorses it , CS rates it as promising , and CMFR judges its effectiveness to be unclear . This divergence arises because of CH dif - ferences in the studies and evidence standards used rather than from evaluating different outcomes or program vari - ants . Both CEBC and CMFR review four studies and CS three , with CEBC rejecting six others that failed to meet its inclusion standards . However , the studies each CH includes overlap little . Between CEBC and CMFR , there is only one common study ( Sullivan & Bybee , 1999 ) ; between CS and CMFR there is also only one ( Tan et al . , 1995 ) ; and between CEBC and CS there are two ( Allen et al . , 2004 ; Bybee & Sullivan , 2002 ) . CS gives CAP a promising rating since the evidence base it analyzes contains at least one high - quality RCT and “par - ticipants showed a statistically significant increase in assess - ing community resources , engaging activities to meet their needs , in quality of life , and a reduction in re - abuse” ( CS , 2011 ) . CEBC does not explain its ratings but publishes its review of all ten studies originally considered . From examin - ing them , we conjecture that it gave CAP its second highest rating – “supported by research evidence” – because there was one RCT with a sustained effect of at least 6 months but replicated findings from two RCTs with non - overlapping samples were not demonstrated . CMFR rates the program as not - evidence - based because , while there are some posi - tive results , there is no independent replication and some “contradictory findings across studies” ( CMFR , n . d . ) . The variation in evidence - based claims seems mostly due to the studies examined and standards of evidence applied to indi - vidual studies . Triple P : Positive Parenting Program Triple P is a system of program variants promoting better parenting with children under 8 . Many family services are involved . Mostly , they vary in intensity and mode of deliv - ery . Across all 13 CHs , we discovered ratings of 28 differ - ent program variants . Disentangling them and the different methods used to rate them is challenging ; we suspect that few CH users will be able to locate all these variants and the studies thereof . Four CHs evaluate the original Triple P program . Their verdicts differ considerably . CS and PNRC rate it as evi - dence - based ; PPN gives it a promising rating ; and BP with - holds even a promising rating . While CS and PPN invoke the same single study ( Prinz et al . , 2009 ) claiming positive effects on child maltreatment , injuries , and out - of - home placements , PPN rates the program lower because only 9 of the 18 counties studied were randomly assigned and , in the other counties , baseline equivalence for the main study outcomes could not be ascertained . PNRC reviews six addi - tional studies of the original program and concludes from the pattern of results that the program is evidence - based . But BP evaluates much of the same evidence as inconclusive and withholds even a promising rating . The second most prominently rated version of Triple P is its “system , ” basically the program variants taken as a whole . By our standards , CEBC achieves the highest rating based on three studies , though the website itself gives only its second - highest rating , since a superior rating requiring replicated findings from two RCTs with non - overlapping samples . Both CMFR and BP reach a promising verdict based either on a single study ( CMFR ) or two ( BP ) . CMFR downgrades the system for its lack of replicated findings , while for BP there are not two high - quality studies and uncertainty exists about whether the effects are sustained long term , whether reporter bias has affected some outcome variables , and whether there is spillover of effects across treatment groups . If we understand Triple P as the sum of all its variants , a promising rating is consistently achieved despite the original Triple P receiving ratings that are maxi - mally inconsistent . Individual Triple P variants have also been examined , and a mixed picture results from them too . HomVEE rates only the variants suitable for home visiting , but precludes using Prinz et al . ( 2009 ) due to methodological reasons , result - ing in a non - endorsement result of Triple P Home Visiting . CMFR and CEBC rate each of the five different levels of Triple P - Positive Parenting . For the first three levels , CMFR concludes that the absence of sustained effects of at least 6 months post - intervention makes the program ineffective ; but Levels 4 and 5 of the program were rated as promising . CEBC rates specific interventions within each level rather than each level as a whole . It assesses the Discussion Group intervention in Level 3 as promising , but the Primary Care intervention as not promising . CEBC examines one interven - tion within Level 4 , fully endorsing it thanks to consistent positive results across two RCTs with non - overlapping sam - ples . Overviewing all these verdicts , we see that the original program is evaluated inconsistently over CHs , that the Triple P system is consistently evaluated as promising , and that the individual Triple P variants receive diverse endorsements as a function of the variant assessed , the standards of evidence 1353 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 applied , the set of studies examined , and how a single study , Prinz et al . ( 2009 ) , is evaluated . While each of the four explanations plays some role in accounting for all this variation , the primary explanations concern the studies examined and the design features needed to achieve a high evidence - based rating . Especially impor - tant are features like how much , and what kind of , replicated evidence is needed , and the period over which an effect must be manifest . A much higher hurdle is entailed when a high rating depends on two RCTs with positive findings of a given size that are observed 12 months after intervention in studies conducted by independent research team than when a single RCT or QED with a single statistically significant finding is needed . CH variation in the studies examined also helps explain why CHs differ in program verdicts , due to when a program review was begun or last updated , the funds avail - able for a thorough literature search , and decisions about which single studies meet standards of methodological adequacy . Of lesser importance are CH differences in the outcomes analyzed and which variant of the same - named program is examined . While deliberate variants of the same program are commonly found in CHs , stakeholders will not be led astray if they carefully distinguish among variants . Discussion This paper ( 1 ) describes some sources of variation , espe - cially casual design preferences , between 13 CHs that evalu - ate social and behavioral development programs , ( 2 ) esti - mates how reliably they evaluate the same program , and ( 3 ) uses four programs to explore four explanations for why CHs vary in concluding that a program is evidence - based or not . These tasks test the construct validity of “evidence - based” in the context of CHs , an especially relevant and important context because CHs claim to identify programs that suc - cessfully prevent or ameliorate social and behavioral prob - lems and then to disseminate such evidence broadly . This review uncovered important CH disagreements in operation - ally defining evidence - based , though RCTs were universally preferred over QEDs . We also uncovered important differ - ences in CH conclusions about how evidence - based most individual programs are . Together , these two findings ques - tion what “evidence - based” means with respect to programs targeting social and behavioral problems . Of all the programs sampled over CHs , as many as 81 . 9 % were rated by a single CH . This is important because anyone interested in most programs has no choice about the CHs to consult and has no chance to diversify the assumptions / standards buttressing each CH’s local theory of what con - stitute effectiveness . Despite discovering 13 CHs covering social and behavioral development , it is today the case that about four - fifths of programs are evaluated by a single CH . For the 11 . 8 % of programs rated by two CHs , 55 . 2 % agreed in their program recommendations . This may seem small or large to different stakeholders , but it is appreci - ably more than in education ( Wadhwa , Zheng and Cook , revise & resubmit ) . It is striking , however , that most of these agreements were to the effect that a program is not evidence - based ; fewest agreements ( about 4 . 0 % ) were about whether a program is clearly evidence - based . So , stakeholders requir - ing the highest level of certainty about which programs to adopt or promote will be largely disappointed , though they will know better which programs to avoid . Consistency over CHs in their evidence - based judgments increases to about 19 . 2 % if we relax the definition of consistency to include scores of only 1 and both 1 and 2 . We do not know how much consistency various stakeholders are willing to toler - ate ; but we surmise that the level obtained here was less than even seasoned researchers expected who are finely attuned to differences in evidence standards and to how variably they are implemented in research practice . The inconsistency we obtained is troubling , since evi - dence - based language is now widely used in scientific , policy and media circles to designate programs with robust positive effects . The evidence adduced here suggests that such language may reflect more of a shared aspiration for trustworthy evidence than a shared understanding of what makes evidence trustworthy ; public rhetoric may be more at play than scientific consensus . Researchers , policymakers , program managers , journalists , and the general public might do well to treat the concept of “evidence - based program” with greater care and perhaps even some skepticism . The major obstacle to achieving greater consistency among CHs seems to lie in the different standards of evi - dence each has adopted for assigning program ratings of promising or higher . Among these standards , traditional issues of causal identification and estimation seem to be less important than ancillary design features like the presence and nature of independent replication , the required size of effects , and the post - treatment period over which effective - ness should be demonstrated . No professional consensus currently exists on the extent to which such issues are needed for assigning evidence - based ratings , making disagreements about them entirely legitimate and worth serious future dis - cussion as the source of so much CH variation in the strin - gency of the results required for concluding that a program is evidence - based . Appendix 1 . CHs ( TPP and WWC ) assign different program ratings for different outcome domains . In the second section of the paper on the consistency analysis , we take the high - est program rating across all outcome domains . 1354 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Table 6 The criteria that led to excluding 30 clearinghouses ( CHs ) from this paper Criteria for exclusion CHs excluded Named clearinghouses that do not conduct / publish effectiveness rat - ings but are repositories for other things or other social science - related purposes California Healthy Kids Resource Center , Center on Knowledge Translation for Disability and Rehabili - tation Research , My Brother ' s Keeper , National Quality Measures Clearinghouse , National Guideline Clearinghouse , Washington State Institute of Public Policy ( WSIPP ) , Out - of - School Time Program Database , United States Interagency Council on Homelessness Solutions Database Clearinghouses that cannot be accessed on the web due to dis - continuation , suspension , or other reasons National Registry of Evidence - based Programs and Practices ( NREPP ) SAMHSA , Social Work Policy Institute , Evidence - based Practices for Substance Abuse , FindYouthInfo . gov , Self - Sufficiency Research Clearinghouse , What Works / Lifecourse Interventions to Nurture Kids Successfully ( LINKS ) Clearinghouses that do not carry out their own rating but publish the ratings and program effectiveness reports from ( or linked to ) other , more prominent clearinghouses and those that use the standards of evidence of another clearinghouse Office of Juvenile Justice and Delinquency Prevention ( OJJDP ) Model Programs Guide , What Works in Reentry Clearinghouse , Youth . gov , Best Practices Registry for Suicide Prevention , Campbell Col - laboration Clearinghouses that do not explicitly deal with social and behavioral development HIV / AIDS Prevention Research Synthesis Project , Cochrane Collaboration , What Works for Health Wisconsin , Conduent Promising Practices Database , Clearinghouse for Labor Evaluation and Research ( CLEAR ) , Best Evidence Encyclopedia ( BEE ) , National Dropout Prevention Center ( NDPC ) , Collaborative for Academic , Social , and Emotional Learning Guide ( CASEL ) , Research - Tested Intervention Programs ( now Evidence - Based Cancer Control Programs ) , Employment Strate - gies for Low Income Adults Evidence Review ( ESER ) , The Community Guide Table 7 Criteria we used to select the social and behavioral development programs from all programs that a CH evaluates * This represents the number of social and behavioral development programs that have been examined by each CH based on our selection crite - ria . There are overlaps of programs examined by different CHs CHs Number of programs * The type of programs included as social and behavioral development CEBC 264 Programs that deal with the following topic areas : anger management , domestic violence , substance abuse , behavior management including parent training , engagement and parent partnering programs , prevention and early intervention PPN 54 Programs that deal with the following topic areas : behavior problems , child abuse and neglect , juvenile justice , substance use and dependence , teen sex / pregnancy , violent behaviors SPTW 18 Programs that deal with the following topic areas : unplanned pregnancy prevention , crime / violence prevention , suicide prevention , substance abuse prevention / treatment CMFR 659 Programs that deal with the following topic areas : alcohol / drugs / tobacco , anger , antisocial behavior , anxiety , behavioral problems , bullying , child abuse , early intervention , parenting , safety , intimate partner violence , sexual assault , school culture , suicide , foster care NGC 83 All the programs examined are within the field of social and behavioral development BP 915 All programs with social and behavioral development outcomes specified and introduced in their program sum - maries , excluding those that are still under review HomVEE 52 Programs that deal with the following topic areas : positive parenting practices , reductions in child maltreatment , reductions in juvenile delinquency , family violence and crime TPP 40 Programs that deal with the following topic areas : abstinence - based , comprehensive sexuality education , and youth development EE 53 From the list of programs with a focus on youth development , only the programs that had ‘Impact’ and ‘Out - comes’ studies were selected by using the ‘Program Evaluation’ filter in the search section . Out of these , programs for which no ‘Level of Evidence’ was given were removed as were any duplicates . In making these selections , we made use of the definitions for these terms provided in the CNCS Evidence Exchange Metadata Glossary CS 943 All the programs examined are within the field of social and behavioral development WWC 122 Programs that deal with the following topic areas : behavior , children and youth with disabilities , path to gradua - tion PNRC 14 Programs with general access or limited access to community members SFER 21 Programs for low - income couples and low - income fathers that have been examined by impact studies 1355 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Table 8 Common rating scales we created to deal with the heterogeneity across CHs’ own program rating scales Program rating scales by CHs Common rating scales by the authors CEBC Well - supported by research evidence 1 Supported by research evidence 1 Promising research evidence 2 Evidence fails to demonstrate effect 3 Concerning practice 3 Not able to be rated on the CEBC scientific rating scale 3 PPN Proven 1 Promising 2 Proven / Promising 2 SPTW Top tier 1 Near top tier 1 Suggestive Tier 2 CMFR Effective RCT 1 Effective Quasi 1 Promising 2 Unclear + 3 Unclear 0 3 Unclear - 3 Ineffective 3 NGC Effective 1 Promising 2 BP Model plus 1 Model 1 Promising 2 Inconclusive evidence 3 Insufficient evidence 3 Not dissemination ready 3 Harmful 3 Ineffective 3 EE Strong 1 Moderate 1 Preliminary 2 Pre - preliminary 3 HomVEE Meets HHS criteria 1 Does not meet HHS criteria 3 Not eligible for review 3 CS Effective 1 Promising 2 Inconclusive evidence 3 No effects 3 Insufficient evidence 3 1356 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 Table 8 ( continued ) Program rating scales by CHs Common rating scales by the authors TPP 1 Uniformly positive impacts on outcomes in this domain from 1 study 1 A mix of positive , null , and / or adverse impacts on outcomes in this domain from 1 study 2 A mix of positive , null , and / or adverse impacts on outcomes in this domain across 2 to 4 studies 2 A mix of positive , null , and / or adverse impacts on outcomes in this domain across 5 or more studies 2 Uniformly null impacts on outcomes in this domain from 1 study 3 Uniformly null impacts on outcomes in this domain across 2 to 4 studies 3 Uniformly adverse impacts on outcomes in this domain from 1 study 3 Uniformly null impacts on outcomes in this domain across 5 or more studies 3 WWC 1 Positive 1 Potentially positive 2 Mixed effects 2 Potentially negative 3 Negative 3 No discernible effects 3 No studies of the intervention found that met WWC criteria 3 PNRC 2 Recommended 1 SFER 3 At least one study with High / Moderate quality rating that reports positive program effects on at least one outcome studied AND no evidence on negative effects 1 All the other scenarios 3 Unit : Percentage of Rating Pairs 0 . 00 5 . 00 10 . 00 15 . 00 20 . 00 25 . 00 30 . 00 35 . 00 40 . 00 45 . 00 Full Consistency and Evidence - Based ( 1 & 1 ) Full Consistency and Promising ( 2 & 2 ) Full Consistency and Not Evidence - Based ( 3 & 3 ) Moderate Consistency ( 1 & 2 ) Moderate Inconsistency ( 2 & 3 ) Full Inconsistency ( 1 & 3 ) Fig . 1 Correspondence in rating for social and behavioral development programs rated by 2 CHs 1357 Prevention Science ( 2022 ) 23 : 1343 – 1358 1 3 2 . PNRC lists recommended programs on its website , but it doesn’t report program ratings . We take all recom - mended programs as evidence based ( a generalized rat - ing of 1 ) . 3 . SFER only rates the quality of studies ( that examine a program ) as High , Moderate , Low , or Unrated but it doesn’t provide an overall program rating . Readers must infer judgment of program effectiveness from the study quality ratings and the effects reported . Like any reader , we have to infer program effectiveness by relating study quality rating to impacts identified . Funding This work was supported by NSF Grant 176458 . Declarations Conflict of interest The authors declare that they have no conflict of interest . References Allen , N . E . , Bybee , D . I . , & Sullivan , C . M . ( 2004 ) . Battered women’s multitude of needs : Evidence supporting the need for comprehen - sive advocacy . Violence Against Women , 10 , 1015 – 1035 . Blueprints for Healthy Youth Development . ( n . d . ) . Big Brothers Big Sisters of America . https : / / www . bluep rints progr ams . org / progr ams / 89999 99 / big - broth ers - big - siste rs - of - ameri ca / Burkhardt , J . T . , Schröter , D . C . , Magura , S . , Means , S . N . , & Coryn , C . L . ( 2015 ) . An overview of evidence - based program registers ( EBPRs ) for behavioral health . Evaluation and Program Plan - ning , 48 , 92 – 99 . Bybee , D . I . , & Sullivan , C . M . ( 2002 ) . The process through which an advocacy intervention resulted in positive change for battered women over time . American Journal of Community Psychology , 30 , 103 – 132 . Clearinghouse for Military Family Readiness . ( n . d . ) . Community Advocacy Project ( CAP ) . https : / / www . conti nuum . milit aryfa milies . psu . edu / progr am / fact _ sheet _ 670 CN CS Evidence Exchange . ( 2016 ) . Clearinghouses and evidence reviews for social benefit programs . Retrieved March 10 , 2019 , from https : / / www . natio nalse rvice . gov / sites / defau lt / files / docum ents / Clear ingho uses % 20and % 20Evi dence % 20Rev iews . pdf CrimeSolutions . org . ( 2011 , June 15 ) . Program Profile : Community Advocacy Project . https : / / crime solut ions . ojp . gov / progr amdet ails ? id = 173 # rp Fagan , A . A . , & Buchanan , M . ( 2016 ) . What works in crime preven - tion ? Comparison and critical review of three crime prevention registries . Criminology & Public Policy , 15 , 617 – 649 . Gies , S . V . , Healy , E . , & Stephenson , R . ( 2020 ) . The evidence of effectiveness : Beyond the methodological standards . Justice Evaluation Journal , 3 , 155 – 177 . Grossman , J . B . , & Tierney , J . P . ( 1998 ) . Does mentoring work ? An impact study of the Big Brothers Big Sisters program . Evalua - tion Review , 22 , 403 – 426 . Mayo - Wilson , E . , Grant , S . , & Supplee , L . H . ( 2021 ) . Clearinghouse standards of evidence on the transparency , openness , and reproduc - ibility of intervention evaluations . Prevention Science , 1 – 13 . Means , S . N . , Magura , S . , Burkhardt , J . T . , Schröter , D . , & Coryn , C . ( 2015 ) . Comparing rating paradigms for evidence - based program registers in behavioral health : Evidentiary criteria and implica - tions for assessing programs . Evaluation and Program Planning , 48 , 100 – 116 . Neuhoff , A . , Axworthy , S . , Glazer , S . , & Berfond , D . ( 2015 ) . The what works marketplace : Helping leaders use evidence to make smarter choices . Bridgespan . Pew - McArthur Results First Initiative . ( 2015 ) . Results first clearing - house database user guide . The PEW Charitable Trusts . Prinz , R . J . , Sanders , M . R . , Shapiro , C . J . , Whitaker , D . J . , & Lutzker , J . R . ( 2009 ) . Population - based prevention of child maltreatment : The US Triple P system population trial . Prevention Science , 10 , 1 – 12 . Sullivan , C . M . , & Bybee , D . I . ( 1999 ) . Reducing violence using com - munity - based advocacy for women with abusive partners . Journal of Consulting and Clinical Psychology , 67 , 43 – 53 . Tan , C . , Basta , J . , Sullivan , C . M . , & Davidson , W . S . , II . ( 1995 ) . The role of social support in the lives of women exiting domestic vio - lence shelters : An experimental study . Journal of Interpersonal Violence , 10 , 437 – 451 . Tierney , J . P . , Grossman , J . B . , & Resch , N . L . ( 1995 ) . Making a differ - ence : An impact study of Big Brothers / Big Sisters . Public / Private Ventures . Webster - Stratton , C . , & Hammond , M . ( 1997 ) . Treating children with early - onset conduct problems : A comparison of child and parent training interventions . Journal of Consulting and Clinical Psychol - ogy , 65 , 93 . Webster - Stratton , C . , Reid , M . J . , & Hammond , M . ( 2004 ) . Treating children with early - onset conduct problems : Intervention out - comes for parent , child , and teacher training . Journal of Clinical Child & Adolescent Psychology , 33 , 105 – 124 . Westbrook , T . P . R . , Avellar , S . A . , & Seftor , N . ( 2017 ) . Reviewing the reviews : Examining similarities and differences between federally funded evidence reviews . Evaluation Review , 41 , 183 – 211 . Zack , M . K . , Karre , J . K . , Olson , J . , & Perkins , D . F . ( 2019 ) . Similari - ties and differences in program registers : A case study . Evaluation and Program Planning , 76 , 101676 . Publisher ' s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations . Springer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the author ( s ) or other rightsholder ( s ) ; author self - archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law . 1358 Prevention Science ( 2022 ) 23 : 1343 – 1358