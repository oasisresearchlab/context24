Adaptively Learning the Crowd Kernel Omer Tamuz omertamuz @ weizmann . ac . il Microsoft Research New England and Weizmann Institute of Science Ce Liu celiu @ microsoft . com Microsoft Research New England Serge Belongie sjb @ cs . ucsd . edu UC San Diego Ohad Shamir ohadsh @ microsoft . com Adam Tauman Kalai adum @ microsoft . com Microsoft Research New England Abstract We introduce an algorithm that , given n ob - jects , learns a similarity matrix over all n 2 pairs , from crowdsourced data alone . The al - gorithm samples responses to adaptively cho - sen triplet - based relative - similarity queries . Each query has the form “is object a more similar to b or to c ? ” and is chosen to be maximally informative given the preceding responses . The output is an embedding of the objects into Euclidean space ( like MDS ) ; we refer to this as the “crowd kernel . ” SVMs reveal that the crowd kernel captures promi - nent and subtle features across a number of domains , such as “is striped” among neckties and “vowel vs . consonant” among letters . 1 . Introduction Essential to the success of machine learning on a new domain is determining a good “similarity function” be - tween objects ( or alternatively deﬁning good object “features” ) . With such a “kernel , ” one can perform a number of interesting tasks , e . g . binary classiﬁca - tion using Support Vector Machines , clustering , inter - active database search , or any of a number of other oﬀ - the - shelf kernelized applications . Since this step of determining a kernel is most often the step that is still not routinized , eﬀective systems for achieving this step Appearing in Proceedings of the 28 th International Con - ference on Machine Learning , Bellevue , WA , USA , 2011 . Copyright 2011 by the author ( s ) / owner ( s ) . are desirable as they hold the potential for completely removing the machine learning researcher from “the loop . ” Such systems could allow practitioners with no machine learning expertise to employ learning on their domain . In many domains , people have a good sense of what similarity is , and in these cases the similarity function may be determined based upon crowdsourced human responses alone . The problem of capturing and extrapolating a human notion of perceptual similarity has received increasing attention in recent years including areas such as vi - sion ( Agarwal et al . , 2007 ) , audition ( McFee & Lanck - riet , 2009 ) , information retrieval ( Schultz & Joachims , 2003 ) and a variety of others represented in the UCI Datasets ( Xing et al . , 2003 ; Huang et al . , 2010 ) . Con - cretely , the goal of these approaches is to estimate a similarity matrix K over all pairs of n objects given a ( potentially exhaustive ) subset of human perceptual measurements on tuples of objects . In some cases the set of human measurements represents ‘side infor - mation’ to computed descriptors ( MFCC , SIFT , etc . ) , while in other cases – the present work included – one proceeds exclusively with human reported data . When K is a positive semideﬁnite matrix induced purely from distributed human measurements , we refer to it as the crowd kernel for the set of objects . Given such a Kernel , one can exploit it for a vari - ety of purposes including exploratory data analysis or embedding visualization ( as in Multidimensional Scal - ing ) and relevance - feedback based interactive search . As discussed in the above works and ( Kendall & Gib - bons , 1990 ) , using a triplet based representation of rel - ative similarity , in which a subject is asked “is object a r X i v : 1105 . 1033v2 [ c s . L G ] 25 J un 2011 Adaptively Learning the Crowd Kernel Figure 1 . A sample top - level of a similarity search system that enables a user to search for objects by similarity . In this case , since the user clicked on the middle - left tile , she will “zoom - in” and be presented with similar tiles . a more similar to b or to c , ” has a number of desir - able properties over the classical approach employed in Multi - Dimensional Scaling ( MDS ) , i . e . , asking for a numerical estimate of “how similar is object a to b . ” These advantages include reducing fatigue on human subjects and alleviating the need to reconcile individu - als’ scales of similarity . The obvious drawback with the triplet based method , however , is the potential O ( n 3 ) complexity . It is therefore expedient to seek methods of obtaining high quality approximations of K from as small a subset of human measurements as possible . Accordingly , the primary contribution of this paper is an eﬃcient method for estimating K via an informa - tion theoretic adaptive sampling approach . At the heart of our approach is a new scale - invariant Kernel approximation model . The choice of model is shown to be crucial in terms of the adaptive triples that are produced , and the new model produces eﬀec - tive triples to label . Although this model is noncon - vex , we prove that it can be optimized under certain assumptions . We construct an end - to - end system for interactive vi - sual search and browsing using our Kernel acquisition algorithm . The input to this system is a set of im - ages of objects , such as products available in an online store . The system automatically crowdsources 1 the 1 Crowdsourcing was done on Amazon’s Mechanical Turk , http : / / mturk . com . kernel acquisition and then uses this kernel to produce a visual interface for searching or browsing the set of products . Figure 1 shows this interface for a dataset of 433 ﬂoor tiles available at amazon . com . 1 . 1 . Human kernels versus machine kernels The bulk of work in Machine Learning focuses on “Ma - chine Kernels” that are computed by computer from the raw data ( e . g . , pixels ) themselves . Additional work employs human experiments to try to learn kernels based upon machine features , i . e . , to approximate the human similarity assessments based upon features that can be derived by machine . In contrast , when a ker - nel is learned from human subjects alone ( whether it be data from an individual or a crowd ) one requires no machine features whatsoever . To the computer , the objects are recognized by ID’s only – the images themselves are hidden from our system and are only presented to humans . The primary advantage of machine kernels is that they can generalize immediately to new data , whereas each additional object needs to be added to our system , for a cost of approximately $ 0 . 15 . 2 On the other hand , working with a human kernel has two primary advan - tages . First , it does not require any domain expertise . While for any particular domain , such as music or im - ages of faces , cars , or sofas , decades of research may have provided high - quality features , one does not have to ﬁnd , implement , and tune these sophisticated fea - ture detectors . Second , human kernels contain features that are sim - ply not available with state - of - the - art feature detec - tors , because of knowledge and experience that hu - mans possess . For example , from images of celebri - ties , human similarity may be partly based on whether the two celebrities are both from the same profession , such as politicians , actors , and so forth . Until the long - standing goal of bridging the semantic gap is achieved , humans will be far better than machines at interpret - ing certain features , such as “does a couch look com - fortable , ” “can a shoe be worn to an informal occa - sion , ” or “is a joke funny . ” We give a simple demonstration of external knowledge through experiments on 26 images of the lower - case Roman alphabet . Here , the learned Kernel is shown to capture features such as “is a letter a vowel versus consonant , ” which uses external knowledge beyond the pixels . Note that this experiment is interesting in itself 2 This price was empirically observed to yield “good per - formance” across a number of domains . See the experimen - tal results section for evaluation criteria . Adaptively Learning the Crowd Kernel because it is not at ﬁrst clear if people can meaning - fully answer the question : “is the letter e more similar to i or p . ” Our experiments show statistically sig - niﬁcant consistency with 58 % 3 ( ± 2 % , with 95 % conﬁ - dence ) agreement between users on a random triple of letters . ( For random image triples from an online tie store , 68 % agreement is observed , and 65 % is observed for ﬂoor tile images ) . 2 . Beneﬁts of adaptation We ﬁrst give high - level intuition for why adaptively choosing triples may yield better kernel approxima - tions than randomly chosen triples . Consider n objects organized in a rooted tree with (cid:96) (cid:28) n leaves , inspired by , say , phylogenic trees involving animal species . 4 Say the similarity between objects is decreasing in their distance in the tree graph and , furthermore , that ob - jects are drawn uniformly at random from the classes represented by the leaves of the tree . Ignoring the de - tails of how one would identify that two objects are in the same leaf or subtree , it is clear that a nonadaptive method would have to ask Ω ( n(cid:96) ) questions to deter - mine the leaves to which n objects belong ( or at least to determine which objects are in the same leaves ) , be - cause an expected Ω ( (cid:96) ) queries are required per object until just a second object is chosen from the same leaf . On the other hand , in an ideal setting , an adaptive ap - proach might determine such matters using O ( n log (cid:96) ) queries in a balanced binary tree , proceeding from the root down , assuming a constant number of compar - isons can determine to which subtree of a node an object belongs , hence an exponential savings . 3 . Related work As discussed above , much of the work in machine learning on learning kernels employs ‘side informa - tion’ in the form of features about objects . Schultz & Joachims ( 2003 ) highlight the fact that triple - based information may also be gathered by web search click data . Agarwal et al . ( 2007 ) is probably the most sim - ilar work , in which they learn a kernel matrix from triples of similarity comparisons , as we do . However , the triples they consider are randomly ( nonadaptively ) chosen . Their particular ﬁtting algorithm diﬀers in 3 While this fraction of agreement seems small , it corre - sponds to about 25 % “noise , ” e . g . , if 75 % of people would say that a is more like b then c , then two random people would agree with probability 0 . 75 2 = 0 . 56 . 4 This example is based upon a tree metric rather than a Euclidean one . However , note that any tree with (cid:96) leaves can be embedded in (cid:96) - dimensional Euclidean space , where squared Euclidean distance equals tree distance . that it is based on a max - margin approach , which is more common in the kernel learning literature . There is a wealth of work in active learning for classiﬁ - cation , where a learner selects examples from a pool of unlabeled examples to label . A number of approaches have been employed , and our work is in the same spirit as those that employ probabilistic models and information - theoretic measures to maximize informa - tion . Other work often labels examples based on those that are closest to the margin or closest to 50 % prob - ability of being positive or negative . To see why this latter approach may be problematic in our setting , one could imagine a set of triples where we have accurately learned that the answer is 50 / 50 , e . g . , as may be the case if a , b , and c bear no relation to each other or if they are identical . One may not want to focus on such triples . 4 . Preliminaries The set of n objects is denoted by [ n ] = { 1 , 2 , . . . , n } . For a , b , c ∈ [ n ] , a comparison or triple is of the form , “is a more similar to b or to c . ” We refer to a as the head of the triple . We write p abc for the probability that a random crowd member rates a as more similar to b , so p abc + p acb = 1 . The n objects are assumed to have d - dimensional Euclidean representation , and hence the data can be viewed as a matrix M ∈ R n × d , where M a denotes the row corresponding to a , and the similarity matrix K ∈ R n × n is deﬁned by K ab = M a · M b , or equivalently K = MM T . The goal is to learn M or , equivalently , learn K . ( It is easy to go back and forth between positive semideﬁnite ( PSD ) K and M , though M is only unique up to change of basis . ) Also equivalent is the representation in terms of distances , d 2 ( a , b ) = K aa − 2 K ab + K bb . In our setting , an MDS algorithm takes as input m comparisons ( a 1 b 1 c 1 , y 1 ) . . . ( a m b m c m , y m ) on n items , where y i ∈ { 0 , 1 } indicates whether a i is more like b i than c i . Unless explicitly stated , we will often omit y i and assume that the b i and c i have been permuted , if necessary , so that a i was rated as more similar to b i than c i . The MDS algorithm outputs an embed - ding M ∈ R n × d for some d ≥ 1 . A probabilistic MDS model predicts ˆ p abc based on M a , M b , and M c . The empirical log - loss of a model that predicts ˆ p a i b i c i is 1 / m (cid:80) i log 1 / ˆ p a i b i c i . Our probabilistic MDS model at - tempts to minimize empirical log loss subject to some regularization constraint . We choose a probabilistic model due to its suitability for use in combination with our information - gain criteria for selecting adap - tive triples , and also due to the fact that the same triple may elicit diﬀerent answers from diﬀerent peo - Adaptively Learning the Crowd Kernel ple ( or the same person on diﬀerent occasions ) . An active MDS algorithm chooses each triple , a i b i c i , adaptively based upon previous labels ( a 1 b 1 c 1 , y 1 ) , . . . , ( a i − 1 b i − 1 c i − 1 , y i − 1 ) . We denote by M T the transpose of matrix M . For compact convex set W , let Π W ( K ) = arg min T ∈ W (cid:80) ij ( K ij − T ij ) 2 be the closest matrix in W to K . Also deﬁne the set of symmetric unit - length PSD matrices , B = { K (cid:23) 0 | K 11 = K 22 = . . . = K nn = 1 } . Projection to the closest element of B is a quadratic program which can be solved via a number of exist - ing techniques ( Srebro & Shraibman , 2005 ; Lee et al . , 2010 ) . 5 . Our algorithm Our algorithm proceeds in phases . In the ﬁrst phase , it queries a certain number of random triples compar - ing each object a ∈ [ n ] to random pairs of distinct b , c . ( Note that we never present a triple where a = b or a = c except for quality control purposes . ) Sub - sequently , it ﬁts the results to a matrix M ∈ R n × d ( equivalently , ﬁts K (cid:23) 0 ) using the probabilistic rela - tive similarity model described below . Then it uses our adaptive selection algorithm to select further random triples . This iterates : in each phase all previous data is reﬁt to the relative model , and then the adaptive selection algorithm generates more triples . • For each item a ∈ [ n ] , crowdsource labels for R random triples with head a . • For t = 1 , 2 , . . . , T : – Fit K t to the labeled data gathered thus far , using the method described in Section 5 . 1 ( with d dimensions ) . – For each a ∈ [ n ] , crowdsource a label for the maximally informative triple with head a , us - ing the method described in Section 6 . Typical parameter values which worked quickly and well across a number of medium - sized data sets of ( hundreds of objects ) were R = 10 , T = 25 , and d = 3 . These settings were also used to generate Figure 2 . We ﬁrst describe the probabilistic MDS model and then the adaptive selection procedure . Further details are given in Section 7 . 5 . 1 . Relative similarity model The relative similarity model is motivated by the scale - invariance observed in many perceptual systems ( see , e . g . , Chater & Brown , 1999 ) . Let δ ab = (cid:107) M a − M b (cid:107) 2 = K aa + K bb − 2 K ab . A simple scale - invariant proposal takes ˆ p abc = δ ac δ ab + δ ac . Such a model must also be reg - ularized or else it would have Θ ( n 2 ) degrees of free - dom . One may regularize by the rank of K or by setting K ii = 1 . Due to the scale - invariance of the model , however , this latter constraint does not have reduced complexity . In particular , note that halving or doubling the matrix M doesn’t change any proba - bilities . Hence , descent algorithms may lead to very small , large , or numerically unstable solutions . To ad - dress this , we modify the model as follows , for distinct a , b , c : ˆ p abc = µ + δ ac 2 µ + δ ab + δ ac and K ii = 1 , ( 1 ) for some parameter µ > 0 . Alternatively , this change may be viewed as an additional assumption imposed on the previous model – we suppose each object pos - sesses a minimal amount of “uniqueness , ” µ > 0 , such that K = µI + T , where T (cid:23) 0 . We ﬁt the model by local optimization performed directly on M ( with ran - dom initialization ) , and high - quality adaptive triples are produced even for low dimensions . 5 Here µ serves a purpose similar to a margin constraint . There are two interesting points to make about our choice of model . First , the loss is not convex in K , so there is a concern that local optimization may be susceptible to local minima . In Section 6 . 1 , we state a theorem which explains why this does not seem to be a signiﬁcant problem . Second , in Section 6 . 2 , we discuss a simple convex alternative based on logistic regression , and we explain why this model , in combi - nation with our adaptive selection criterion , gives rise to poor adaptively - selected triples . 6 . Adaptive selection algorithm The idea is to capture the uncertainty about the lo - cation of an object through a probability distribution over points in R d , and then to ask the question that maximizes information gain . Given a set of previous comparisons of n objects , we generate , for each object a = 1 , 2 , . . . , n , a new triple to compare a to , as fol - lows . First , we embed the objects into R d as described above , using the available comparisons . Initially , we use a seed of randomly selected triples for this pur - pose . Later , we use all available comparisons - the initial random ones and those acquired adaptively . 5 For high - dimensional problems , we perform a gradient projection descent on K . In particular , starting with K 0 = I , we compute K t + 1 = Π B ( K t − η ∇L ( K ) ) for step - size η ( see Preliminaries for the deﬁnition of Π B ) . Adaptively Learning the Crowd Kernel Now , say the crowd has previously rated a as more similar to b i than c i , for i = 1 , 2 , . . . , j − 1 , and we want to generate the j th query , ( a , b j , c j ) ( this is a slight abuse of notation because we don’t know which of b j or c j will be rated as closer to a ) . These observations imply a posterior distribution of τ ( x ) ∝ π ( x ) (cid:81) i ˆ p xb i c i over x ∈ R d , where x is the embedding of a , and π ( x ) is a prior distribution , to be described shortly . Given any candidate query for objects in the database b and c , the model predicts that the crowd will rate a as more similar to b than c with probability p ∝ (cid:82) x δ ( x , c ) δ ( x , b ) + δ ( x , c ) τ ( x ) dx . If it rates a more simi - lar to b than c then x has a posterior distribution of τ b ( x ) ∝ τ ( x ) δ ( x , c ) δ ( x , b ) + δ ( x , c ) , and τ c ( x ) ( of similar form ) otherwise . The information gain of this query is de - ﬁned to be H ( τ ) − pH ( τ b ) − ( 1 − p ) H ( τ a ) , where H ( · ) is the entropy of a distribution . This is equal to the mutual information between the crowd’s selection and x . The algorithm greedily selects a query , among all pairs b , c (cid:54) = a , which maximizes information gain . This can be somewhat computationally intensive ( seconds per object in our datasets ) , so for eﬃciency we take the best pair from a sample of random pairs . It remains to explain how we generate the prior π . We take π to be the uniform distribution over the set of points in M . Hence , the process can be viewed as follows . For the purpose of generating a new triple , we pretend the coordinates of all other objects are perfectly known , and we pretend that the object in question , a , is a uniformly random one of these other objects . The chosen pair is designed to maximize the information we receive about which object it is , given the observations we already have about a . The hope is that , for suﬃciently large data sets , such a data - driven prior is a reasonable approximation to the actual dis - tribution over data . Another natural alternative prior would be a multinormal distribution ﬁt to M . 6 . 1 . Optimization guarantee The relative similarity model is appealing in that it ﬁts the data well , suggests good triples , and also repre - sents interesting features on the data . Unfortunately , the model itself is not convex . We now give some justiﬁcation for why gradient descent should not get trapped in local minima . As is sometimes the case in learning , it is easier to analyze an online version of the algorithm , i . e . , a stochastic gradient descent . Here , we suppose that the sequence of triples is pre - sented in order : the learner predicts K t + 1 based on ( a 1 , b 1 , c 1 , y 1 ) , . . . , ( a t , b t , c t , y t ) . The loss on iteration t is (cid:96) t ( K t ) = log 1 / p where p is the probability that the relative model with K t assigned to the correct out - come . We state the following theorem about stochastic gra - dient descent , where K 0 ∈ B is arbitrary and K t + 1 = Π B ( K t − η ∇ (cid:96) t ( K t ) ) . Theorem 1 . Let a t , b t , c t ∈ [ n ] be arbitrary , for t = 1 , 2 , . . . . Suppose there is a matrix K ∗ ∈ B such that Pr [ y t = 1 ] = µ + 2 − 2 K ∗ ac 2 µ + 4 − 2 K ∗ ab − 2 K ∗ ac . For any (cid:15) > 0 , there exists an T 0 such that for any T > T 0 and η = 1 / √ T , E (cid:34) 1 T T (cid:88) t = 1 (cid:96) t ( K t ) − (cid:96) t ( K ∗ ) (cid:35) ≤ (cid:15) . We prove this theorem in Appendix A . 6 . 2 . The logistic model : A convex alternative As a small digression , we explain why the choice of probabilistic model is especially important for adap - tive learning . To this end , consider the following logis - tic model . This model is a natural hybrid of logistic regression and MDS . ˆ p abc = e K ab e K ab + e K ac = 1 1 + e K ac − K ab . ( 2 ) Note that log 1 + e K ac − K ab is a convex function of K ∈ R n × n . Hence , the problem of minimizing its empirical log loss over a convex set is a convex op - timization problem . Experiments indicate that the logistic model ﬁts data well and reproduces interesting features , such as vowel / consonant or stripedness . However , empirically it performs poorly in terms of deciding which triples to ask . Note that the logistic model ( and any generalized linear model ) will select extreme comparisons , where the inner products are as large as possible . To give an intuitive understanding , suppose that hair length was a single feature and one wanted to determine whether a person has hair length x or x + 1 . The logistic model would compare that person to a bald person ( x = 0 ) and a person with hair length 2 x + 1 , while the relative model would ideally compare him to people with hair lengths x and x + 1 . 7 . System parameters & quality control Experiments were performed using Amazon’s Mechan - ical Turk web service , where we deﬁned ‘Human Intel - ligence Tasks’ to be performed by one or more users . Each task consists of 50 comparisons and the inter - face is optimized to be performed with 50 mouse clicks ( and no scrolling ) . The mean completion time was Adaptively Learning the Crowd Kernel approximately 2 minutes . This payment was deter - mined based upon worker feedback . Initial experi - ments revealed a high percentage of seemingly random responses , but after closer inspection the vast major - ity of these poor results came from a small number of individuals . To improve quality control , we im - posed a limit on the maximum number of tasks a sin - gle user could perform on any one day , we selected users who had completed at least 48 tasks with a 95 % approval rate , and each task included 20 % triples for which there was tremendous agreement between users . These “gold standard” triples were also automatically generated and proved to be an eﬀective manner to rec - ognize and signiﬁcantly reduce cheating . The system is implemented using Python , Matlab , and C , and runs completely automatically in Windows or Unix . 7 . 1 . Question phrasing and crowd alignment One interesting issue is how to frame similarity ques - tions . On the one hand , it seems purest in form to give the users carte blanche and ask only , “is a more similar to b than c . ” On the other hand , in feedback users complained about these tasks and often asked what we meant by similarity . Moreover , diﬀerent users will inevitably weigh diﬀerent features diﬀerently when performing comparisons . For example , consider a comparisons of face images , where a is a white male , b is a black male , and c is a white female . Some users will consider gender more important in determining skin color , and others may feel the opposite is true . Others may feel that the question is impossible to answer . Consider phrasing the question as follows , “At a distance , who would you be more likely to mistake for a : b or c ? ” For any two people , there is presumably some distance at which one might be mistaken for the other , so the question may seem more possible to answer for some people . Second , users may more often agree that skin color is more important than gender , because both are eas - ily identiﬁed close up by skin color may be identiﬁable even at a great distance . While we haven’t done exper - iments to determine the importance of question phras - ing , anecdotal evidence suggests that users enjoy the tasks more when more speciﬁc deﬁnitions of similarity are given . Two natural goals of question phrasing might be : ( 1 ) to align users in their ranking of the importance of diﬀerent features and ( 2 ) to align user similarity no - tions with the goals of the task at hand . For example , if the task is to ﬁnd a certain person , the question , “which two people are most likely to be ( genealogi - cally ) related to one another , ” may be poor because 10 20 30 3 . 5 4 4 . 5 5 triples per object ( training ) l og o f r an k i n po s t e r i o r 20 Random Questions 10 20 30 3 . 5 4 4 . 5 5 triples per object ( training ) 20 Adaptive Questions adaptive triples random triples adaptive triples random triples Figure 2 . The 20Q plots comparing training based on adaptively selected triples to randomly selected training triples . The left plot shows the mean predicted log - ranks of randomly chosen objects after 20 randomly chosen ques - tions . The right plot shows the mean predicted log - ranks of randomly chosen objects after 20 adaptive queries . Plots were generated using the mixed dataset consisting of n = 225 objects , with 10 initial random triples per object . In both plots , the performance using 22 = ( 10 random ) + ( 12 adaptively chosen ) triples was matched using all 35 random triples . Hence , approximately 60 % more random triples were required to match this particular performance level of the adaptive algorithm . users may overlook features such as gender and age . In our experiments on neckties , for example , the task was titled “Which ties are most similar ? ” and the complete instructions were : “Someone went shopping at a tie store and wanted to buy the item on top , but it was not available . Click on item ( a ) or ( b ) below that would be the best substitute . ” 8 . Experiments and Applications We experiment on four datasets : ( 1 ) twenty - six im - ages of the lowercase roman alphabet ( Calibri font ) ( 2 ) 223 country ﬂag images from ﬂagpedia . net , ( 3 ) 433 ﬂoor tile images from Amazon . com , and ( 4 ) 300 product images from an online tie store also hosted at Amazon . com . We also consider a hand - selected “mixed” dataset consisting of 225 images : 75 ties , 75 tiles , and 75 ﬂags . Surprisingly , it seems that for these datasets about 30 - 40 triples per object suﬃce to learn the Crowd Kernel well , according to the 20Q metric that we describe below . . Figure 2 shows the results on the mixed dataset , comparing the 20Q metric trained on random vs . adaptive triples . For both adaptive and random questions , for certain performance levels , one requires about 60 % more random queries than adap - tive queries . Given very little data or a lot of data , one Adaptively Learning the Crowd Kernel Figure 3 . Six objects in the mixed dataset along with the adaptive pairs to which that object was compared , below , and user selections in red . The ﬁrst pair below each large object was chosen adaptively based upon the results of ten random comparisons . Then , proceeding down , the pairs were chosen using the ten random comparisons plus the results of the earlier comparisons above . Figure 4 . Nearest - neighbors for some neckties from the tie - store dataset . Nearest neighbors are displayed from left to right . Note that neck ties were never confused for bow ties , tie clips or scarves . does not expect the adaptive algorithm to perform sig - niﬁcantly better . Figure 3 shows the adaptive triples selected on an il - lustrative dataset composed of a mixture of ﬂags , ties and tiles . For ease of implementation , we assume all users are identical . This is a natural starting point , especially given that our main focus is on active learning . 8 . 1 . 20 Questions Metric Since one application of such systems is search , i . e . , searching for an item that a user knows what it looks like ( we assume that the user can answer queries as if she even knows what the store image looks like ) , it is natural to ask how well we have “honed in” on Figure 5 . Examples of nearest neighbors for ﬂoor tiles . Figure 6 . The ﬂag images displayed according to their pro - jection on the top two principal components of a PCA . The principal component is the horizontal axis . Figure 7 . For fun : the faces of 186 colleagues displayed according to their projection on the top two principal com - ponents . The principal component is the horizontal axis . Adaptively Learning the Crowd Kernel Dataset Feature LOO error rate Tiles Ornate 4 . 1 % Ties Bow tie vs . neck tie 0 . 0 % Ties Multicolor vs . plain 0 . 5 % Flags Striped 0 . 0 % Letters Vowel 4 . 0 % Letters Short / tall 5 . 3 % Figure 8 . Empirical results of an SVM using the crowd ker - nel , based on leave - one - out cross validation . Note that in many cases we hand - selected “easy” subsets of objects to label . For example , when judging whether a ﬂag is striped or not , we removed ﬂags which might be interpreted ei - ther way . The selection was based on how unambiguous the objects were , with respect to the desired label , and not related to the target kernel . the desired object after a certain number of questions . For the 20 Questions ( 20Q ) metric , two independent parts of the system are employed , an evaluator and a guesser . First , the evaluator randomly and secretly se - lects an object in the database , x . The guesser is aware of the database but not of which item x has been se - lected . The guesser is allowed to query 20 triples ( as in the game “20 Questions” ) with head x , after which it produces a ranking of items in the database , from most to least likely . Then the evaluator reveals the identity of x and hence its position in the ordered ranking , as well . The metric is the average log of the position of the random target item in this list . The log reﬂects the idea that the position of lower - ranked objects is less import – it weights moving an object from position 2 to 4 as important as moving an object from position 20 to 40 . This metric is meant to roughly capture performance , but of course in a real system users may not have the patience to click on twenty pairs of im - ages and may prefer to have fewer clicks but use larger comparison sets . ( Our GUI has the user select one of 8 or 9 images , which could potentially convey the same information as 3 binary choices . ) Now , the ques - tions that the guesser asks could be random questions , which we refer to as the 20 Random Questions metric , or adaptively chosen , for the 20 Adaptive Questions metric . In the latter case , the guesser uses the same maximum information - gain criterion as in the adap - tive triple generation algorithm , relative to whichever model was learned ( based on random or adaptively se - lected training triples ) . 8 . 2 . Using the Kernel for Classiﬁcation The learned Kernels may be used in a linear classiﬁer such as a support vector machine . This helps elucidate which features have been used by humans in labeling the data . In the experiments below , an unambigu - ous subset of the images were labeled with binary ± classes . For example , we omitted the letter y in label - ing vowels and consonants ( y was in fact classiﬁed as a consonant , and c was misclassiﬁed as a vowel ) , and we selected only completely striped or unstriped ﬂags for ﬂag stripe classiﬁcation . The SVM - Light ( Joachims , 1998 ) package was used with default parameters and its leave - one - out ( LOO ) classiﬁcation results are re - ported in Figure 8 . 8 . 3 . Visual Search We provide a GUI visual search tool , exempliﬁed in Figure 1 . Given n images , their embedding into R d and the related probabilistic model for triples , we would like to help a user ﬁnd either a particular object she has in mind , or a similar one . We do this by playing “20 Questions” with 8 - or 9 - tuple queries , generated by an information - gain adaptive selection algorithm very similar to the one described in Section 6 . Acknowledgments . We thank Sham Kakade and Varun Kanade for helpful discussions . Serge Belongie’s research is partly funded by ONR MURI Grant N00014 - 08 - 1 - 0638 and NSF Grant AGS - 0941760 . A . Proof of Theorem 1 A . 1 . Analysis Before we present the proof of Theorem 1 , we intro - duce a natural generalization which will be a conve - nient abstraction . We call this relative regression . A . 2 . Relative regression Consider the following online relative regres - sion model . There is a sequence of exam - ples ( x 1 , x (cid:48) 1 , y 1 ) , ( x 2 , x (cid:48) 2 , y 2 ) , . . . , ( x T , x (cid:48) T , y T ) ∈ X × X × { 0 , 1 } , for some set X ⊆ R d . For w ∈ R d , deﬁne the relative linear model with w to be , p t ( w ) = w · x t w · x t + w · x (cid:48) t . The sequence x 1 , x (cid:48) 1 , . . . , x T , x (cid:48) T is chosen arbitrary ( or even adversarially ) in advance ; afterwards it is as - sumed that there is some w ∗ ∈ R d such that , Pr [ y t = 1 ] = p t ( w ∗ ) and that the diﬀerent y t ’s are indepen - dent . It is further assumed that w ∗ belongs to some convex compact set W ⊂ R d and that w · x is positive and bounded over w ∈ W , x ∈ X . Without loss of generality , by scaling we can require w · x ∈ [ 1 , β ] for some β > 0 and every w ∈ W , x ∈ X . On the t th period , the algorithm outputs w t ∈ W , then Adaptively Learning the Crowd Kernel observes x t , x (cid:48) t , y t , and ﬁnally incurs loss (cid:96) t ( w t ) where (cid:96) t ( w ) = log 1 / p t ( w ) if y t = 1 and (cid:96) t ( w ) = log 1 / ( 1 − p t ( w ) ) if y t = 0 . The goal of the algorithm is to incur total loss not much larger than (cid:80) t (cid:96) t ( w ∗ ) , the best choice had we known w ∗ in advance . We note that an analogous ( and slightly simpler ver - sion of ) the following lemma can be proven for squared loss . Lemma 1 . Let X , W ⊆ R d and suppose that W is compact and convex and ∃ α > 0 such that for all x ∈ X , w ∈ W : (cid:107) x (cid:107) , (cid:107) w (cid:107) ≤ 1 , and w · x ≥ α . The for any η > 0 and any w 0 ∈ W and w t + 1 = Π W ( w t − η ∇ (cid:96) t ( w t ) ) , 1 T E (cid:34) T (cid:88) t = 1 (cid:96) t ( w t ) − (cid:96) t ( w ∗ ) (cid:35) ≤ η α 2 + 2 Tηα . In particular , for η = (cid:112) 2 α / T , this gives a bound on the right - hand side of (cid:113) 8 Tα 3 . Proof . Following the analysis of Zinkevich ( Zinkevich , 2003 ) we consider the potential equal to the squared distance ( w t − w ∗ ) 2 and argue that it decreases when - ever we have substantial error . Let ∇ t = ∇ (cid:96) t ( w t ) ∈ R d , which is , ∇ t = x t + x (cid:48) t w t · x t + w t · x (cid:48) t − y t x t w t · x t − ( 1 − y t ) x (cid:48) t w t · x (cid:48) t . By the triangle inequality (cid:107)∇ t (cid:107) ≤ G for G = 2 α . Now , as Zinkevich points out , due to convexity of W , ( w − Π W ( v ) ) 2 ≤ ( w − v ) 2 for any v ∈ R d and w ∈ W . Hence , ( w ∗ − w t + 1 ) 2 ≤ ( w ∗ − w t + η ∇ t ) 2 . Thus the decrease in potential , call it ∆ t = ( w ∗ − w t ) 2 − ( w ∗ − w t + 1 ) 2 , is at least : ∆ t ≥ ( w ∗ − w t ) 2 − ( w ∗ − w t + η ∇ t ) 2 = 2 η ∇ t · ( w t − w ∗ ) − η 2 ∇ 2 t . Next , we consider the quantity , E [ ∆ t · w ∗ ] , where the expectation is taken over the random y t ( ﬁxing y 1 , y 2 , . . . , y t − 1 ) . By expansion , the expectations is : w ∗ · x t + w ∗ · x (cid:48) t w · x t + w · x (cid:48) t − p t ( w ∗ ) w ∗ · x t w t · x t − ( 1 − p t ( w ∗ ) ) w ∗ · x (cid:48) t w t · x (cid:48) t . After simple algebraic manipulation , which is diﬃcult to show in two - column format , we have , E [ ∆ t · w ∗ ] = − Z t ( p t ( w ∗ ) − p t ( w t ) ) 2 where Z t = ( w t · x t + w t · x (cid:48) t ) ( w ∗ · x t + w ∗ · x (cid:48) t ) ( w t · x t ) ( w t · x (cid:48) t ) . Also note that ∆ t · w t = 0 regardless of y t . Hence , E [ ∆ t · w t ] = 0 . Combining these with the fact that we have shown that ∆ t ≥ 2 η ∇ t · ( w t − w ∗ ) − η 2 G 2 , gives , E [ ∆ t ] ≥ 2 ηZ t ( p t ( w ∗ ) − p t ( w t ) ) 2 − η 2 G 2 ≥ 2 η w ∗ · x t + w ∗ · x (cid:48) t w t · x t + w t · x (cid:48) t ( p t ( w t ) − p t ( w ∗ ) ) 2 p t ( w t ) ( 1 − p t ( w t ) ) − η 2 G ≥ 2 ηα ( p t ( w t ) − p t ( w ∗ ) ) 2 p t ( w t ) ( 1 − p t ( w t ) ) − η 2 G . In the last line we have used the fact that w · x ∈ [ α , 1 ] for w ∈ W , x ∈ X . Now , by Lemma 2 which follows this proof , (cid:96) t ( w t ) − (cid:96) t ( w ∗ ) ≤ ( p t ( w t ) − p t ( w ∗ ) ) 2 p t ( w t ) ( 1 − p t ( w t ) ) . Combining the previous two displayed equations gives , E [ ∆ t ] ≥ 2 ηα ( (cid:96) t ( w t ) − (cid:96) t ( w ∗ ) ) − η 2 G . Finally , since the potential ( w t − w ∗ ) 2 > 0 , we have (cid:80) ∆ t ≤ ( w 0 − w ∗ ) 2 ≤ 4 . Hence , (cid:88) t (cid:96) t ( w t ) − (cid:96) t ( w ∗ ) ≤ Tη 2 G + 4 2 ηα . Substituting G = 2 / α gives the lemma . Lemma 2 . Let p + q = 1 and p ∗ + q ∗ = 1 for p , p ∗ ∈ [ 0 , 1 ] . Then , p ∗ log p ∗ p + q ∗ log q ∗ q ≤ ( p − p ∗ ) 2 pq . Proof . By concavity of log , Jensen’s inequality implies , p ∗ log p ∗ p + q ∗ log q ∗ q ≤ log ( p ∗ ) 2 p + ( q ∗ ) 2 q . Simple algebraic manipulation shows that , ( p ∗ ) 2 p + ( q ∗ ) 2 q = 1 + ( p − p ∗ ) 2 pq . Finally , the fact that log 1 + x ≤ x completes the lemma . A . 3 . Proof To prove theorem 1 , map matrix S ∈ R n × n to a vector w ( S ) ∈ R 1 + n 2 consisting of the constant µ + 2 in the ﬁrst coordinate followed by the n 2 entries of S . Taken over the set of symmetric S (cid:23) 0 such that S ii = 1 , Adaptively Learning the Crowd Kernel the vectors w ( S ) for a compact convex set of radius (cid:112) n 2 + ( 2 + µ ) 2 . Also , p a t b t c t = µ + 2 − K ac − K ca 2 µ + 4 − K ab − K ba − K ac − K ca , is our relative regression model for w = w ( S ) , x ∈ R 1 + n 2 being the vector with a 1 is the ﬁrst position and - 1’s in the positions corresponding to the ac and ca entries of S ( and zero elsewhere ) , and x (cid:48) having a 1 in the ﬁrst position and - 1’s in the positions cor - responding to ab and ba ( and zero elsewhere ) . The inner product of w ( S ) and x is µ + δ ab and hence is bounded . To apply Lemma 1 , one must scale w ( S ) and x , x (cid:48) down . However , it is clear that for any (cid:15) and T suﬃciently large , setting η = 1 / √ T in Lemma 1 gives the necessary bound . References Agarwal , Sameer , Wills , Josh , Cayton , Lawrence , Lanckriet , Gert , Kriegman , David , and Belongie , Serge . Generalized non - metric multidimensional scaling . In AISTATS , San Juan , Puerto Rico , 2007 . Chater , N and Brown , G D . Scale - invariance as a uni - fying psychological principle . Cognition , 69 ( 3 ) : B17 – 24 , 1999 . Huang , Kaizhu , Ying , Yiming , and Campbell , Colin . Generalized sparse metric learning with relative comparisons . Knowledge and Information Systems , pp . 1 – 21 , 2010 . ISSN 0219 - 1377 . 10 . 1007 / s10115 - 010 - 0313 - 0 . Joachims , Thorsten . Making large - scale svm learning practical . LS8 - Report 24 , Universit¨at Dortmund , LS VIII - Report , 1998 . Kendall , Maurice and Gibbons , Jean D . Rank Corre - lation Methods . A Charles Griﬃn Title , 5 edition , September 1990 . Lee , Jason , Recht , Ben , Salakhutdinov , Ruslan , Sre - bro , Nathan , and Tropp , Joel . Practical large - scale optimization for max - norm regularization . Advances in Neural Information Processing Systems 23 , pp . 1297 – 1305 , 2010 . McFee , B . and Lanckriet , G . R . G . Heterogeneous embedding for subjective artist similarity . In Tenth International Symposium for Music Information Re - trieval ( ISMIR2009 ) ) , October 2009 . Schultz , Matthew and Joachims , Thorsten . Learn - ing a distance metric from relative comparisons . In Advances in Neural Information Processing Systems ( NIPS ) . MIT Press , 2003 . Srebro , Nathan and Shraibman , Adi . Rank , trace - norm and max - norm . In COLT , pp . 545 – 560 , 2005 . Xing , Eric P . , Ng , Andrew Y . , Jordan , Michael I . , and Russell , Stuart . Distance metric learning , with application to clustering with side - information . In Advances in Neural Information Processing Systems 15 , pp . 505 – 512 . MIT Press , 2003 . Zinkevich , Martin . Online convex programming and generalized inﬁnitesimal gradient ascent . In Fawcett , Tom and Mishra , Nina ( eds . ) , ICML , pp . 928 – 936 . AAAI Press , 2003 . ISBN 1 - 57735 - 189 - 4 .