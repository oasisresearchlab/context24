PubGraph : A Large Scale Scientiﬁc Temporal Knowledge Graph Kian Ahrabian , Xinwei Du , Richard Delwin Myloth , Arun Baalaaji Sankar Ananthan , Jay Pujara University of Southern California , Information Sciences Institute { ahrabian , xinweidu , myloth , arunbaal , jpujara } @ usc . edu Abstract Research publications are the primary vehicle for sharing sci - entiﬁc progress in the form of new discoveries , methods , tech - niques , and insights . Publications have been studied from the perspectives of both content analysis and bibliometric struc - ture , but a barrier to more comprehensive studies of scien - tiﬁc research is a lack of publicly accessible large - scale data and resources . In this paper , we present PubGraph , a new resource for studying scientiﬁc progress that takes the form of a large - scale temporal knowledge graph ( KG ) . It contains more than 432M nodes and 15 . 49B edges mapped to the pop - ular Wikidata ontology . We extract three KGs with varying sizes from PubGraph to allow experimentation at different scales . Using these KGs , we introduce a new link predic - tion benchmark for transductive and inductive settings with temporally - aligned training , validation , and testing partitions . Moreover , we develop two new inductive learning methods better suited to PubGraph , operating on unseen nodes without explicit features , scaling to large KGs , and outperforming ex - isting models . Our results demonstrate that structural features of past citations are sufﬁcient to produce high - quality pre - dictions about new publications . We also identify new chal - lenges for KG models , including an adversarial community - based link prediction setting , zero - shot inductive learning , and large - scale learning . 1 Introduction Scientiﬁc progress takes many forms , ranging from discov - ering new species to repurposing extant models for novel tasks . Innovation in science has been studied from a variety of perspectives , including the combination of scholarly do - mains ( Hofstra et al . 2020 ; Uzzi et al . 2013 ) , sociological factors ( De Vaan , Stark , and Vedres 2015 ) , and analogical reasoning ( Hope et al . 2017 ; Kang et al . 2022 ) . However , many studies of this phenomenon have been limited due to the difﬁculty in ﬁnding and using large - scale data for the do - main . In this paper , we address this obstacle by introducing PubGraph , a knowledge graph ( KG ) with new resources and benchmarks , enabling the study of scientiﬁc research at scale using structural patterns in citation and collaboration networks . PubGraph provides a unique opportunity to com - pare multi - relational KG models in both transductive and in - ductive settings at a billion - edge scale . PubGraph is a large - scale multi - relational temporal KG built using the OpenAlex catalog ( Priem , Piwowar , and Orr 2022 ) and the Wikidata ( Vrandeˇci´c and Kr¨otzsch 2014 ) on - tology . It consists of more than 432M entities , including au - thors , institutions , publication venues , and papers , and more than 15 . 49B relationships for those entities . PubGraph also captures temporal information , allowing the study of scien - tiﬁc works’ dynamics . In this paper , we demonstrate that ci - tation structure in PubGraph can be used , in concert with KG link prediction models , to predict future citation links between research papers with high accuracy . Citations have been crucial in studying publications and their impact ( Price 1965 ) . Prior works have studied tasks on citations such as intent classiﬁcation ( Jurgens et al . 2018 ; Cohan et al . 2019 ; Gururangan et al . 2020 ) , recommenda - tion ( Bhagavatula et al . 2018 ; F ¨ arber and Sampath 2020 ) , and prediction ( Cohan et al . 2020 ; Ostendorff et al . 2022 ) . Many such studies use the publication’s content , but content information may be difﬁcult to obtain and parse . Alterna - tively , citation structure has been shown ( Liu et al . 2019 ; Dietz , Bickel , and Scheffer 2007 ) to provide a strong sig - nal for predictive tasks . New large - scale graphs ( Hu et al . 2021 ) for tasks such as link prediction and node classiﬁca - tion support using citation structure but are limited in their relational information . Our work builds on these prior works with new resources and benchmarks that are appropriate for a variety of tasks , emphasizes the multi - relational structure via a knowledge graph , and includes billions of edges . One of the essential requirements for understanding sci - entiﬁc progress is to predict meaningful connections be - tween ideas and discoveries . The main challenge of satis - fying this requirement is to deal with new publications re - quiring inference over unseen samples , i . e . , inductive learn - ing . In this work , we explore this problem from a purely structural standpoint , framing the research question as a link prediction task for KGs . The goal is to predict a citation relationship between two research papers using informa - tion about authors , institutions , venues , time , and other ci - tations . This approach provides insight into the utility of the relational and structural collaborative information available among scholarly entities , which is complementary to indi - vidual content - based features . While KGs capture the structure necessary for this predic - tive task , many models do not address this use case . Specif - ically , traditional KG - based evaluations ( Trouillon et al . 2016 ; Sun et al . 2019 ) are in an interpolated , transduc - a r X i v : 2302 . 02231v1 [ c s . A I ] 4 F e b 2023 Figure 1 : Overview of the training and evaluation scheme . Intra - period current links ( black ) are used for training in all exper - iment settings . Intra - period future links ( red ) are used for evaluation in both validation and testing phases in all experiment settings . Exo - period links ( dotted blue ) are used in the training phase in transductive settings ; however , in inductive settings , these links are only used as auxiliary links during the evaluation phase . Auxiliary links establish connections between seen training nodes and unseen evaluation nodes . tive setting where all entities ( e . g . , papers and authors ) are known , and a portion of their edges are missing . A bet - ter evaluation setting for studying innovation is the extrap - olated , inductive setting . Extrapolated prediction requires train and test sets to be partitioned by a temporal thresh - old , so model predictions are for a future time epoch . In - ductive models are capable of making predictions for previ - ously unseen papers and authors . We create benchmarks for KG models in the extrapolated setting with transductive and inductive variations to support evaluating models that pre - dict future relationships between new publications . As part of our benchmarks , we introduce an efﬁcient , inductive , and featureless hybrid framework for large - scale KGs , outper - forming existing models . The contributions of this work are summarized as follows : 1 . Creating of PubGraph , a billion - scale , multi - relational temporal KG using the Wikidata ontology 2 . Introducing a large - scale extrapolated link prediction benchmarks for KG models in both transductive and in - ductive settings 3 . Developing novel extensions for inductive graph models that improve performance and scalability 4 . Identifying adversarial benchmark variants posing chal - lenging new environments for KG research 5 . Analyzing state - of - the - art static and temporal KG mod - els for multi - relational citation link prediction 2 Related Works Knowledge Graphs : The research interest in KGs has in - creased in recent years . These structured repositories store information in the form of triplets representing a relation between two entities . One of the most prominent tasks on KGs is link prediction which aims to complete a given inherently incomplete KG ( Bordes et al . 2013 ) . However , they have also been applied to various disciplines such as recommender systems ( Wang et al . 2019d , b , a ) , ques - tion answering ( Zhang et al . 2018a ; Narasimhan , Lazeb - nik , and Schwing 2018 ; Zhang et al . 2018b ) , and text - based games ( Adhikari et al . 2020 ; Ammanabrolu and Hausknecht 2020 ) to name a few . Knowledge Graph Embeddings : These embedding models aim to learn representations for entities and relations in a given KG . Most of these models follow the encoder - decoder framework for learning representations . The en - coder generates representations for entities and the relation , and the decoder produces a score given the generated repre - sentations . Traditional models ( Bordes et al . 2013 ; Trouillon et al . 2016 ; Sun et al . 2019 ) use simple shallow embeddings as the encoder and a scoring function as the decoder . More recent models ( Schlichtkrull et al . 2018 ; Dettmers et al . 2018 ) replace shallow embeddings with Graph Neural Networks ( GNN ) as their encoders . These complex encoders enable the models to gather information over the multi - hop neighborhoods around the entities , increasing the reasoning power of the models . However , the main downside of GNNs is their demanding computational cost . This downside espe - cially becomes critical for representation learning at scale . As a result , almost all the past link prediction studies on large - scale KGs rely on traditional models ( Hu et al . 2021 ) . Large Scale Datasets : Closely related to our work is the MAG240M KG ( Hu et al . 2021 ) . However , there are three signiﬁcant differences between these datasets . First , we rely on OpenAlex instead of the Microsoft Academic Graph ( MAG ) ( Wang et al . 2020 ) to extract our KG . This choice is mainly due to the discontinuation of MAG . Sec - Dataset # Nodes # Edges # Relations ogbl - citation2 ( Hu et al . 2020 ) 2 , 927 , 963 30 , 561 , 187 1 Freebase ( Bollacker et al . 2008 ) 86 , 054 , 151 338 , 586 , 276 14 , 824 WikiKG90Mv2 ( Hu et al . 2021 ) 91 , 230 , 610 601 , 062 , 811 1 , 315 PubGraph 432 , 172 , 002 15 , 499 , 528 , 916 51 PG - 1M 3 , 378 , 202 22 , 442 , 976 4 PG - 10M 25 , 312 , 490 315 , 225 , 337 4 PG - Full 184 , 126 , 885 2 , 201 , 239 , 147 4 Table 1 : Statistics of extracted datasets compared to the ex - isting large - scale datasets . The PG - Full dataset has more than 2x nodes and 3 . 6x edges compared to the largest ex - isting dataset . ond , we add publication - venue relations as we believe they potentially introduce a robust bias to the KG throughout the learning process . Finally , in contrast to MAG240M , we only rely on the structural information rather than the contents of the publications . It is interesting to note that the nature of afﬁliations is dynamic and changes through time , making it impossible to capture in a static structure . Consequently , the imposed bias through author - institution relations could potentially lead to degraded performance in static settings . 3 Building PubGraph To create PubGraph , we rely on the OpenAlex cata - log ( Priem , Piwowar , and Orr 2022 ) . OpenAlex is an open - source catalog of scholarly entities that provides meta - data for publications , authors , venues , concepts , and insti - tutions . This work focuses on the connections between pub - lications , authors , venues , and institutions . However , con - cepts and their links to publications are also used in our analysis as supplementary information . Throughout both the transformation and extraction steps , we use the KGTK toolkit ( Ilievski et al . 2020 ) to run our queries . We release PubGraph under a CC - BY - SA license at TBD . 3 . 1 Mapping to Wikidata To transform the OpenAlex dump ( taken on May 15th , 2022 ) into PubGraph , we follow the well - known and well - studied Wikidata ontology ( Vrandeˇci´c and Kr¨otzsch 2014 ) . Specif - ically , we create a mapping between metadata information from the OpenAlex dump to Wikidata properties . Using Wikidata enables broader adoption of the KG and clear se - mantics for entities and relationships . We further discuss the mapping process in Appendix E . 3 . 2 Sampling Methodology The full PubGraph KG contains a vast amount of infor - mation in the form of literal values and sparse properties that are not easily usable by many KG models . We extract subsets of PubGraph , designated as PG , to create easier - to - use datasets for KG models . To extract PGs from the trans - formed data , we ﬁrst remove all the publications that have no citations and do not cite any other papers to get PG - Full . Metric PG - 1M PG - 10M PG - Full Mutual Citations ( % ) 0 . 03 0 . 04 0 . 06 Authorship Completeness ( % ) 99 . 97 99 . 97 99 . 92 Venue Completeness ( % ) 92 . 37 90 . 25 75 . 34 Institution Completeness ( % ) 81 . 45 71 . 21 45 . 77 Table 2 : Validity and completeness metrics of sampled KGs . Since these nodes are disconnected from other publications , this step mitigates the sparsity problem and reduces the KG size by a large margin . Given the enormous size of the PG - Full , we create two small and medium - sized sub - KGs to allow future stud - ies at different scales . To this end , we use snowball sam - pling ( Goodman 1961 ) to extract PG - 1M and PG - 10M with 1M and 10M publication nodes , respectively . After sam - pling , we remove any publication without a publication date . Next , we extract all the publication - publication ( P2860 ) , publication - author ( P50 ) , publication - venue ( P1433 ) , and author - institution ( P1416 ) links from the sampled data . We ensure to include all the available author , venue , and insti - tution links from the sampled publications in the datasets . Finally , we split all the datasets temporally , using all the publications before 2017 for training , 2017 up until 2020 for validation , and 2020 onward for testing . The details of the splits are provided in Appendix A . Figure 2 illustrates an overview of the sampled KG . To create the temporal datasets , we timestamp the links based on the publication date of their respective publication . For example , the timestamp of a citation link is the publi - cation date of the citing paper . For each author - institution link , we use the article’s publication date , which indicates the author’s afﬁliation as the timestamp . Table 1 showcases the statistics of the PubGraph and the sampled KGs along with a comparison to existing large - scale datasets in the lit - erature . 3 . 3 Data Quality To evaluate the quality of the extracted datasets , we check the validity and completeness of our KGs . For validity , we look for potential mutual citations , cases where two papers reference each other , violating strict temporal order . This artifact may appear when articles have several revisions , Figure 2 : Overview of the sampled PG - Full KG . but OpenAlex only reports the earliest publication date . For completeness , we calculate publication - author , publication - venue , and author - institution relations completeness . Table 2 showcases these metrics on the extracted KGs . As evi - dent from the metrics , all the datasets exhibit an extremely low mutual citations percentage which is evidence of their quality . Moreover , the small and medium - sized KGs exhibit higher completeness metrics which we attribute to the forced inclusion of all authors , venues , and institutions links . 4 Methodology We design our experimental setup to accommodate dif - ferent levels of information availability . In the ﬁrst set - ting , we study our research question through the common information - rich graph completion environment , i . e . , static transductive , where each entity is seen many times in the training phase . We then add additional temporal information to study the effects of dynamic behaviors in the graph , i . e . , temporal transductive . Finally , we investigate our problem in an information - deprived environment , i . e . , inductive , to study adaptability and generalizability to unseen nodes . 4 . 1 Static Transductive The static transductive setting applies when all entities are included in the training set , and edges in the test set are miss - ing at random . For this setting , we choose ComplEx and Ro - tatE as our experimental models . This choice is mainly in - ﬂuenced by their superior expressive power ( Sun et al . 2019 ) and availability in the DGL - KE toolkit ( Zheng et al . 2020 ) , which we use throughout these experiments . Given a triplet ( s , r , o ) , ComplEx and RotatE produce scores as follows : f ComplEx = Re ( (cid:104) E e ( s ) , E r ( r ) , E e ( o ) (cid:105) ) ( 1 ) f RotatE = − (cid:107) E e ( s ) ◦ E r ( r ) − E e ( o ) (cid:107) 2 ( 2 ) where (cid:104) . (cid:105) is the generalized dot product , ◦ is the Hadamard product , E e is the entity embedding , and E r is the relation embedding . During the training phase , we use the self - adversarial neg - ative sampling and the log - sigmoid loss : p ( s (cid:48) , r , o (cid:48) ) = exp αf ( s (cid:48) , r , o (cid:48) ) (cid:80) s (cid:48)(cid:48) , o (cid:48)(cid:48) exp αf ( s (cid:48)(cid:48) , r , o (cid:48)(cid:48) ) ( 3 ) L = − log σ ( γ − f ( s , r , o ) ) ( 4 ) − (cid:88) s (cid:48) , o (cid:48) p ( s (cid:48) , r , o (cid:48) ) log σ ( f ( s (cid:48) , r , o (cid:48) ) − γ ) 4 . 2 Temporal Transductive The temporal transductive setting applies when all entities are present during training , temporal information is included for edges , and missing edges for evaluation only occur af - ter a temporal threshold . We use diachronic embeddings , which are temporal extensions of shallow embeddings . Di - achronic embeddings consist of a static part , i . e . , shallow embedding , and a dynamic part , i . e . , high - dimensional sinu - soid , which allows it to produce time - conditioned embed - dings . Consequently , they support extrapolation evaluation settings in which we are required to have embeddings for unseen timestamps . Given an entity e and timestamp t , the diachronic embedding is calculated as D e ( e , t ) = E e ( e ) ⊕ [ E A ( e ) sin ( E F ( e ) t + E φ ( e ) ) ] ( 5 ) where ⊕ is the concatenation operator , E e , E A , E F , and E φ are all shallow embeddings and E A , E F , and E φ re - spectively model the amplitude , frequency , and phase of the high - dimensional sinusoid . Moreover , the dimension of the dynamic part is controlled through the hyper - parameter ψ in proportion to the embedding dimension . For our experiments , we extend the existing TransE ( DE - TransE ) and DistMult ( DE - DistMult ) implementation of DGL - KE to support diachronic embeddings . Given a quadruple ( s , r , o , t ) , DE - TransE and DE - DistMult produce a score as follows : f DE - TransE = − (cid:107) D e ( s , t ) + E r ( r ) − D e ( o , t ) (cid:107) ( 6 ) f DE - DistMult = (cid:104) D e ( s , t ) , E r ( r ) , D e ( o , t ) (cid:105) ( 7 ) Moreover , we apply dropout to the produced scores to regu - larize the model . During the training phase , following the training environ - ment described in the original paper , we minimize the cross - entropy loss : L = − ( (cid:88) ( s , r , o , t ) ∈G exp f ( s , r , o , t ) (cid:80) o (cid:48) exp f ( s , r , o (cid:48) , t ) ( 8 ) + (cid:88) ( s , r , o , t ) ∈G exp f ( s , r , o , t ) (cid:80) s (cid:48) exp f ( s (cid:48) , r , o , t ) ) 4 . 3 Inductive One of the most important aspects of a learning algorithm is its ability to generalize to unseen samples . This aspect of machine learning on graphs is addressed through the in - ductive setting . The inductive setting applies when both en - tities and edges are missing after a temporal threshold . In general , inductive learning algorithms rely on node features and multi - hop node neighborhoods to generate embeddings for unseen nodes . Moreover , these algorithms use the neural message - passing algorithm to aggregate information from multi - hop node neighborhoods . Given a graph G = ( V , E ) with h 0 u = x u ( node fea - tures ) and h 0 ( u , v ) = y ( u , v ) ( edge features ) , the general - ized message - passing scheme at the k - th layer is deﬁned as ( Battaglia et al . 2018 ) : h k ( u , v ) = f edge ( h k − 1 ( u , v ) , h k − 1 u , h k − 1 v , h k − 1 G ) ( 9 ) m N u = f agg ( { h k ( u , v ) } ∀ v ∈N u ) ( 10 ) h ku = f node ( h k − 1 u , m N u , h k − 1 G ) ( 11 ) h k G = f graph ( h k − 1 G , { h ku } ∀ u ∈V , { h k ( u , v ) } ∀ ( u , v ) ∈E ) ( 12 ) where N u is the neighborhood around node u , h k ( u , v ) is the edge representation , h ku is the node representation , and h k G is the graph representation . One limitation of this formulation is the need for features for unseen nodes , which are not part of PubGraph and may Figure 3 : Overview of the hybrid training and evaluation framework used for GraphSAGE - H and RGCN - H . generally be unavailable . To address these limitations , we propose two extensions to existing inductive methods to al - low them to work without node and edge features by leverag - ing network structure . Our alternative generalized message - passing scheme , replaces Equation 11 with h ku = f node ( m N u , h k − 1 G ) . ( 13 ) This approach is equivalent to removing self - loops during the aggregation step . The main idea for this proposal is to build the representation of each node solely based on its multi - hop neighborhood . This simple change allows us to quickly generate representations for unseen nodes only us - ing their links to training nodes , i . e . , auxiliary links . In this scheme , the model is forced to distribute the presentation of each node among its neighbors . Hence , each node’s em - bedding holds a soft aggregation of its neighborhood , poten - tially leading to more robust learned representations . Con - ceptually , these distributed representations have similarities to the NodePiece’s anchor nodes ( Galkin et al . 2021 ) as we could use them both to reconstruct the representation of neighboring nodes . To study the effectiveness of this ap - proach , we transform two popular inductive learning frame - works using the proposed aggregation scheme . We train all models by minimizing the cross - entropy loss ( Equation 8 ) . GraphSAGE - E GraphSAGE’s original node representa - tion function is deﬁned as h ku = σ ( W k [ h k − 1 u ⊕ m N u ] ) ( 14 ) where σ is a non - linear function . Following the above - mentioned aggregation scheme , we change this function to h ku = σ ( W k m N u ) . ( 15 ) RGCN - E Similar to GraphSAGE , we modify RGCN’s node representation function from h ku = σ ( (cid:88) r ∈R (cid:88) v ∈N u 1 c u , r W k − 1 r h k − 1 v + W k − 1 0 h k − 1 u ) ( 16 ) where R is the relations set and c u , r = | N ru | , to h ku = σ ( (cid:88) r ∈R (cid:88) v ∈N u 1 c u , r W k − 1 r h k − 1 v ) . ( 17 ) For both GraphSAGE - E and RGCN - E , given two publica - tions ( u , v ) and a K - layer model , we employ a dot product function to produce a score as follows : f = (cid:104) h Ku , h Kv (cid:105) . ( 18 ) Although our training setup for GraphSAGE - E and RGCN - E is a special case of the oDistMult - ERAvg algo - rithm ( Albooyeh , Goel , and Kazemi 2020 ) , we employ a more general and complex neighborhood encoder to in - crease the reasoning power of our model . In contrast to the oDistMult and NodePiece original evaluation setup , we evaluate the unseen - unseen ( Xu et al . 2020 ) rather than the unseen - seen links . GraphSAGE - H and RGCN - H Apart from the end - to - end models , we introduce a hybrid two - stage training scheme . In this scheme , we ﬁrst pre - train our model us - ing static transductive models to obtain node embeddings for training KG . Then , we freeze these embeddings and use them as feature vectors to train our message - passing model using the above - mentioned aggregation method . This combination allows us to have the beneﬁts of both shallow models , i . e . , speed , and message - passing - based models , i . e . , more reasoning power . Figure 3 illustrates an overview of the proposed hybrid scheme . Given the multiplicative nature of our scoring function as deﬁned in Equation 18 , we use the embeddings from the best ComplEx model to avoid any mismatches with the learned representation space . GraphSAGE - D and RGCN - D We compare our models with two simple baselines , which use relational node degrees as feature vectors ( Hamilton , Ying , and Leskovec 2017 ; Al - booyeh , Goel , and Kazemi 2020 ) . In these models , for each node , a feature vector of size 2 | R | is extracted where the i - th and ( | R | + i ) - th elements are , respectively , the incoming and outgoing node degrees with relation type r i ∈ R . ComplEx As our ﬁnal model , we include the results of the ComplEx model . Since ComplEx is an inherently transduc - tive model , for unseen nodes we assign randomly initialized embeddings taken from U ( − µ , µ ) where µ = ( 2 + γ ) / d ( Sun et al . 2019 ) with d being the embedding dimension . This experiment acts as both a sanity check and a pseudo - random baseline for the rest of our experiments . 5 Evaluation In this section , we evaluate state - of - the - art KG link predic - tion models across static transductive , temporal transduc - tive , and temporal inductive settings . Figure 1 illustrates our training and evaluation settings . We further evaluate the im - pact of different feature classes with an ablation study and introduce a more challenging adversarial negative sampling setting . Dataset Model MRR Hits @ 1 Hits @ 10 Hits @ 50 PG - 1M ComplEx 0 . 705 0 . 579 0 . 916 0 . 969 RotatE 0 . 725 0 . 609 0 . 920 0 . 979 DE - TransE 0 . 611 0 . 466 0 . 875 0 . 961 DE - DistMult 0 . 589 0 . 456 0 . 836 0 . 947 Table 3 : Link prediction results for static and temporal trans - ductive settings on the PG - 1M dataset . In our evaluations , we compare each positive sample with 1000 negative samples to calculate ﬁltered Mean Recipro - cal Rank ( MRR ) , Hits @ 10 , and Hits @ 50 on the tail entities . Apart from the static transductive setting , we report results on the PG - 1M dataset due to computing resource limitations . Finally , we limit the training time on PG - 1M , PG - 10M , and PG - Full to 12 hours , two days , and ﬁve days respectively . All the experiments are conducted using PyTorch ( Paszke et al . 2019 ) and DGL ( Wang et al . 2019c ) implementations on ma - chines with at least 64 CPU cores , 750GB of RAM , and ei - ther 2 Nvidia V100 or 4 Nvidia RTX 2080 Ti GPUs . Ap - pendix B provides the details on the hyperparameter tuning procedure and the hyperparameters of the best - performing models . Implementations are available under a CC - BY - SA license at TBD . 5 . 1 Transductive We compare the static transductive models Com - plEx ( Trouillon et al . 2016 ) and RotatE ( Sun et al . 2019 ) with temporal models that use diachronic repre - sentations ( Goel et al . 2020 ) , speciﬁcally DE - TransE and DE - DistMult . Table 3 presents the results of our experiments with static and temporal transductive models on the PG - 1M dataset . These results show that RotatE performs best among all models , which we attribute to its superior expressivity . How - ever , counterintuitively , the temporal models fall far behind the static models . We believe this is due to their inability to generalize to unseen timestamps , which makes them un - ﬁt for this setting . Table 4 presents the results of our ex - periments with static transductive models on PG - 10M and PG - Full datasets . RotatE and ComplEx record the best per - formance on PG - 1M and PG - Full , respectively . This perfor - mance boost for ComplEx is mainly due to its lower com - putational cost , which results in more training steps in the previously set training time , i . e . , 120M vs . 60M in 5 days . As for the improved performance compared to the PG - 1M dataset , we believe this is due to 1 ) more training time for the model and 2 ) having a less incomplete set of references for all the nodes , which leads to better - learned representa - tions . 5 . 2 Inductive We evaluate the inductive setting using GraphSAGE ( Hamil - ton , Ying , and Leskovec 2017 ) and RGCN ( Schlichtkrull et al . 2018 ) . Additionally , we adapt these models to an embedding - based inductive setting rather than a feature - based one . We attempted experiments with additional state - Dataset Model MRR Hits @ 1 Hits @ 10 Hits @ 50 PG - 10M ComplEx 0 . 677 0 . 562 0 . 872 0 . 945 RotatE 0 . 750 0 . 656 0 . 910 0 . 974 PG - Full ComplEx 0 . 821 0 . 760 0 . 919 0 . 957 RotatE 0 . 801 0 . 731 0 . 921 0 . 973 Table 4 : Link prediction results for the static transductive setting on PG - 10M and PG - Full datasets . Model MRR Hits @ 1 Hits @ 10 Hits @ 50 ComplEx 0 . 007 0 . 000 0 . 008 0 . 071 GraphSAGE - D 0 . 149 0 . 062 0 . 324 0 . 664 RGCN - D 0 . 166 0 . 071 0 . 364 0 . 709 GraphSAGE - E 0 . 779 0 . 697 0 . 917 0 . 968 RGCN - E 0 . 706 0 . 609 0 . 872 0 . 939 GraphSAGE - H 0 . 784 0 . 697 0 . 928 0 . 973 RGCN - H 0 . 746 0 . 645 0 . 919 0 . 977 Table 5 : Link prediction results for the inductive setting . of - the - art inductive models oDistMult ( Albooyeh , Goel , and Kazemi 2020 ) and NodePiece ( Galkin et al . 2021 ) using the original implementations provided by the authors . Unfortu - nately , oDistMult required more than 750GB RAM , and the Nodepiece OOS - LP setting required over 1000 hours for to - kenization processing for our PG - 1M dataset . Table 5 presents the results of our experiments in the inductive setting . The results show that the hybrid variations outperform all of their counterparts . Moreover , GraphSAGE - E and RGCN - E outperform their relational - degree - based counterparts , making them straightforward substitutes for end - to - end embedding - based inductive learn - ing settings . These ﬁndings are exciting as they provide scal - able alternatives to the current state - of - the - art models . Sur - prisingly , both end - to - end and hybrid GraphSAGE varia - tions outperform their RGCN counterparts . A potential ex - planatory factor may be the low number of relation types ( 4 ) present in the PG - 1M sample . 5 . 3 Ablation Study To study the inﬂuence of each node type on the model’s per - formance , we run an ablation study on the PG - 1M dataset . To this end , we ﬁrst create all the valid combinations of the node types . The criteria for a combination to be valid are 1 ) publications must be included in the training as our task is to predict links between publications and 2 ) institution nodes cannot be present without author nodes ; otherwise , because of the network topology ( Figure 1 ) institution nodes will be disjoint . Then , we benchmark our best - performing static transductive model , i . e . , RotatE , on these alternative datasets . Discussion Table 6 provides the results of our experi - ments . As evident from the results , adding each node type improves the model’s performance . As expected , publica - Variation MRR Hits @ 1 Hits @ 10 Hits @ 50 Full 0 . 725 0 . 609 0 . 920 0 . 992 - V 0 . 713 0 . 598 0 . 909 0 . 969 - I 0 . 722 0 . 606 0 . 919 0 . 978 - V - I 0 . 715 0 . 600 0 . 909 0 . 970 - A - I 0 . 536 0 . 394 0 . 802 0 . 922 - V - A - I 0 . 518 0 . 380 0 . 780 0 . 900 Table 6 : Ablation study on the PG - 1M dataset . Legend : I : Institutions , A : Authors , V : Venues . tions are the most inﬂuential nodes and achieve relatively high performance in a citation - only setting . The addition of both venues and institutions only has a marginal , nonethe - less positive , effect on the model’s performance . Finally , when paired with publications , authors provide a relative 38 % MRR improvement over the standalone publications setting . We attribute this boost of performance to two fac - tors : 1 ) addition of authors creates more pathways in the graph for the information to ﬂow 2 ) author representations act as dense summaries of more relevant publication nodes for link prediction . 5 . 4 Negative Sampling Throughout our experiments , we employed the strategy of sampling 1000 negative samples for each positive sample during the evaluation phase . However , this strategy is prone to exhibiting inﬂated performance due to having no control over the difﬁculty of the sampled nodes . For example , we might end up with candidate nodes with different types than the target node , e . g . , publication vs . venue . In most cases , assuming the model is well - trained , and the data is valid , the model will have an easier job assigning lower scores to these nodes . Moreover , calculating the evaluation metrics on the complete set of samples becomes increasingly more expen - sive as the size of the KG grows . Hence , we propose three alternative strategies for negative sampling during the evalu - ation phase . These strategies aim to ﬁnd an efﬁcient method that could be used as a proxy for complete metric calcula - tions . Our proposed strategies are as follows : 1 . Entity Type : This is the most straightforward strategy in which we only sample candidate nodes with the same type as the target node . For example , in our case , we only sample from the publications . 2 . Time Constrained : Building upon our ﬁrst strategy , we further add the constraint of only sampling candidate nodes from the nodes within the evaluation period . In - tuitively , these unseen ( inductive ) or less seen ( transduc - tive ) nodes will pose more problems for the model during the evaluation phase . 3 . Community : This strategy relies on the auxiliary out - puts , i . e . , communities , that we generate from the Pub - Graph as described in Appendix C . Given a target node , we sample candidate nodes only from its community . We Variation # NegativeSamples MRR Hits @ 1 Hits @ 10 Time ( Seconds ) Random 1000 0 . 723 0 . 608 0 . 918 588 ( CPU ) Entity Type 1000 0 . 560 0 . 418 0 . 826 655 ( CPU ) Time Constrained 1000 0 . 577 0 . 449 0 . 817 601 ( CPU ) Community 1000 0 . 076 0 . 023 0 . 167 1008 ( CPU ) Full ∼ 3 . 38M 0 . 015 0 . 000 0 . 036 81987 ( GPU ) Table 7 : Negative sampling results on the PG - 1M datasets . hypothesize that these nodes pose the most difﬁculty for the model during the evaluation phase . Discussion Table 7 presents the results of our experiments with the aforementioned negative sampling strategies . The reported times are for one evaluation run over the complete testing set of the PG - 1M dataset ( ∼ 147K samples ) . As evi - dent from these results , the community - based method is the best proxy to the full metrics calculation while still being signiﬁcantly time efﬁcient . Even if we factor in the 11 . 5 hours ( 41400 seconds ) that it takes to learn communities for all the 91M publications , the difference in computation time becomes much more signiﬁcant when we have to repeat the evaluation process over and over again , e . g . , for validation , ﬁne - tuning , etc . Moreover , the full metrics are calculated on a GPU which is far more efﬁcient than the calculations on the CPU . It is important to note that the community - based method is helpful in evaluation settings where the ground truth is known ; however , in settings where the ground truth is unknown , e . g . , a deployed model , there is no workaround to complete ranking computations as we have to consider all the entities regardless . Appendix D provides more analysis of the proposed approaches . 6 Conclusion and Future Work We extracted a novel large - scale KG from the OpenAlex catalog with more than 15 . 49 billion edges and 432 mil - lion nodes to study the innovation phenomena . Moreover , we created small and medium - sized variations of the full KG to enable future studies at different scales . Although we focused on the extrapolated setting in this work , these KGs could be easily adapted in studies with interpolated settings . Furthermore , given the suitability of PubGraph for both transductive and inductive learning environments , we conducted numerous experiments on many popular static transductive , temporal transductive , and inductive models . We also presented a hybrid inductive learning framework designed for large - scale KGs and adapted the existing mod - els to the featureless setting . Based on the results of our experiments , the introduced hybrid framework outperforms its counterparts across all models . This result is especially signiﬁcant when working with large - scale graphs , as it pro - vides fast training while maintaining high performance . Fi - nally , we presented various analyses to understand our ex - periments’ results better . Our analyses showcased the cost issues in the evaluation phase . Moreover , our experiments shed light on the shortcomings of the temporal transductive models in extrapolated settings . We also showcased the scal - ing issues of current state - of - the - art models . As for future directions , apart from the challenges and problems arising from the aforementioned shortcomings , ﬁnding “surprising” and “should - have” citations is an exciting research direc - tion . References Adhikari , A . ; Yuan , X . ; Cˆot´e , M . - A . ; Zelinka , M . ; Rondeau , M . - A . ; Laroche , R . ; Poupart , P . ; Tang , J . ; Trischler , A . ; and Hamilton , W . 2020 . Learning dynamic belief graphs to gen - eralize on text - based games . Advances in Neural Informa - tion Processing Systems , 33 : 3045 – 3057 . Albooyeh , M . ; Goel , R . ; and Kazemi , S . M . 2020 . Out - of - sample representation learning for knowledge graphs . In Findings of the Association for Computational Linguistics : EMNLP 2020 , 2657 – 2666 . Ammanabrolu , P . ; and Hausknecht , M . 2020 . Graph con - strained reinforcement learning for natural language action spaces . arXiv preprint arXiv : 2001 . 08837 . Battaglia , P . W . ; Hamrick , J . B . ; Bapst , V . ; Sanchez - Gonzalez , A . ; Zambaldi , V . ; Malinowski , M . ; Tacchetti , A . ; Raposo , D . ; Santoro , A . ; Faulkner , R . ; et al . 2018 . Relational inductive biases , deep learning , and graph networks . arXiv preprint arXiv : 1806 . 01261 . Bhagavatula , C . ; Feldman , S . ; Power , R . ; and Ammar , W . 2018 . Content - based citation recommendation . arXiv preprint arXiv : 1802 . 08301 . Bollacker , K . ; Evans , C . ; Paritosh , P . ; Sturge , T . ; and Taylor , J . 2008 . Freebase : a collaboratively created graph database for structuring human knowledge . In Proceedings of the 2008 ACM SIGMOD international conference on Manage - ment of data , 1247 – 1250 . Bordes , A . ; Usunier , N . ; Garcia - Duran , A . ; Weston , J . ; and Yakhnenko , O . 2013 . Translating embeddings for modeling multi - relational data . Advances in neural information pro - cessing systems , 26 . Cohan , A . ; Ammar , W . ; Van Zuylen , M . ; and Cady , F . 2019 . Structural scaffolds for citation intent classiﬁcation in scien - tiﬁc publications . arXiv preprint arXiv : 1904 . 01608 . Cohan , A . ; Feldman , S . ; Beltagy , I . ; Downey , D . ; and Weld , D . S . 2020 . Specter : Document - level representation learn - ing using citation - informed transformers . arXiv preprint arXiv : 2004 . 07180 . De Vaan , M . ; Stark , D . ; and Vedres , B . 2015 . Game changer : The topology of creativity . American Journal of Sociology , 120 ( 4 ) : 1144 – 1194 . Dettmers , T . ; Minervini , P . ; Stenetorp , P . ; and Riedel , S . 2018 . Convolutional 2d knowledge graph embeddings . In Proceedings of the AAAI conference on artiﬁcial intelli - gence , volume 32 . Dietz , L . ; Bickel , S . ; and Scheffer , T . 2007 . Unsupervised prediction of citation inﬂuences . In Proceedings of the 24th international conference on Machine learning , 233 – 240 . F¨arber , M . ; and Sampath , A . 2020 . Hybridcite : A hybrid model for context - aware citation recommendation . In Pro - ceedings of the ACM / IEEE Joint Conference on Digital Li - braries in 2020 , 117 – 126 . Galkin , M . ; Wu , J . ; Denis , E . ; and Hamilton , W . L . 2021 . Nodepiece : Compositional and parameter - efﬁcient repre - sentations of large knowledge graphs . arXiv preprint arXiv : 2106 . 12144 . Goel , R . ; Kazemi , S . M . ; Brubaker , M . ; and Poupart , P . 2020 . Diachronic embedding for temporal knowledge graph completion . In Proceedings of the AAAI Conference on Ar - tiﬁcial Intelligence , volume 34 , 3988 – 3995 . Goodman , L . A . 1961 . Snowball sampling . The annals of mathematical statistics , 148 – 170 . Gururangan , S . ; Marasovi´c , A . ; Swayamdipta , S . ; Lo , K . ; Beltagy , I . ; Downey , D . ; and Smith , N . A . 2020 . Don’t stop pretraining : adapt language models to domains and tasks . arXiv preprint arXiv : 2004 . 10964 . Hamilton , W . ; Ying , Z . ; and Leskovec , J . 2017 . Inductive representation learning on large graphs . Advances in neural information processing systems , 30 . Hofstra , B . ; Kulkarni , V . V . ; Munoz - Najar Galvez , S . ; He , B . ; Jurafsky , D . ; and McFarland , D . A . 2020 . The diversity – innovation paradox in science . Proceedings of the National Academy of Sciences , 117 ( 17 ) : 9284 – 9291 . Hope , T . ; Chan , J . ; Kittur , A . ; and Shahaf , D . 2017 . Ac - celerating innovation through analogy mining . In Proceed - ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 235 – 243 . Hu , W . ; Fey , M . ; Ren , H . ; Nakata , M . ; Dong , Y . ; and Leskovec , J . 2021 . Ogb - lsc : A large - scale challenge for ma - chine learning on graphs . arXiv preprint arXiv : 2103 . 09430 . Hu , W . ; Fey , M . ; Zitnik , M . ; Dong , Y . ; Ren , H . ; Liu , B . ; Catasta , M . ; and Leskovec , J . 2020 . Open graph benchmark : Datasets for machine learning on graphs . Advances in neural information processing systems , 33 : 22118 – 22133 . Ilievski , F . ; Garijo , D . ; Chalupsky , H . ; Divvala , N . T . ; Yao , Y . ; Rogers , C . ; Li , R . ; Liu , J . ; Singh , A . ; Schwabe , D . ; et al . 2020 . KGTK : a toolkit for large knowledge graph manipu - lation and analysis . In International Semantic Web Confer - ence , 278 – 293 . Springer . Jurgens , D . ; Kumar , S . ; Hoover , R . ; McFarland , D . ; and Ju - rafsky , D . 2018 . Measuring the evolution of a scientiﬁc ﬁeld through citation frames . Transactions of the Association for Computational Linguistics , 6 : 391 – 406 . Kang , H . B . ; Qian , X . ; Hope , T . ; Shahaf , D . ; Chan , J . ; and Kittur , A . 2022 . Augmenting scientiﬁc creativity with an analogical search engine . ACM Transactions on Computer - Human Interaction . Liu , H . ; Kou , H . ; Yan , C . ; and Qi , L . 2019 . Link predic - tion in paper citation network to construct paper correlation graph . EURASIP Journal on Wireless Communications and Networking , 2019 ( 1 ) : 1 – 12 . Narasimhan , M . ; Lazebnik , S . ; and Schwing , A . 2018 . Out of the box : Reasoning with graph convolution nets for fac - tual visual question answering . Advances in neural informa - tion processing systems , 31 . Ostendorff , M . ; Rethmeier , N . ; Augenstein , I . ; Gipp , B . ; and Rehm , G . 2022 . Neighborhood contrastive learning for sci - entiﬁc document representations with citation embeddings . arXiv preprint arXiv : 2202 . 06671 . Paszke , A . ; Gross , S . ; Massa , F . ; Lerer , A . ; Bradbury , J . ; Chanan , G . ; Killeen , T . ; Lin , Z . ; Gimelshein , N . ; Antiga , L . ; et al . 2019 . Pytorch : An imperative style , high - performance deep learning library . Advances in neural information pro - cessing systems , 32 . Price , D . J . D . S . 1965 . Networks of scientiﬁc papers : The pattern of bibliographic references indicates the nature of the scientiﬁc research front . Science , 149 ( 3683 ) : 510 – 515 . Priem , J . ; Piwowar , H . ; and Orr , R . 2022 . OpenAlex : A fully - open index of scholarly works , authors , venues , insti - tutions , and concepts . arXiv preprint arXiv : 2205 . 01833 . Schlichtkrull , M . ; Kipf , T . N . ; Bloem , P . ; Berg , R . v . d . ; Titov , I . ; and Welling , M . 2018 . Modeling relational data with graph convolutional networks . In European semantic web conference , 593 – 607 . Springer . Sun , Z . ; Deng , Z . - H . ; Nie , J . - Y . ; and Tang , J . 2019 . Rotate : Knowledge graph embedding by relational rotation in com - plex space . arXiv preprint arXiv : 1902 . 10197 . Traag , V . A . ; Waltman , L . ; and Van Eck , N . J . 2019 . From Louvain to Leiden : guaranteeing well - connected communi - ties . Scientiﬁc reports , 9 ( 1 ) : 1 – 12 . Trouillon , T . ; Welbl , J . ; Riedel , S . ; Gaussier , ´ E . ; and Bouchard , G . 2016 . Complex embeddings for simple link prediction . In International conference on machine learn - ing , 2071 – 2080 . PMLR . Uzzi , B . ; Mukherjee , S . ; Stringer , M . ; and Jones , B . 2013 . Atypical combinations and scientiﬁc impact . Science , 342 ( 6157 ) : 468 – 472 . Vrande ˇ ci ´ c , D . ; and Kr ¨ otzsch , M . 2014 . Wikidata : a free collaborative knowledgebase . Communications of the ACM , 57 ( 10 ) : 78 – 85 . Wang , H . ; Zhang , F . ; Zhang , M . ; Leskovec , J . ; Zhao , M . ; Li , W . ; and Wang , Z . 2019a . Knowledge - aware graph neural networks with label smoothness regularization for recom - mender systems . In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 968 – 977 . Wang , H . ; Zhao , M . ; Xie , X . ; Li , W . ; and Guo , M . 2019b . Knowledge graph convolutional networks for recommender systems . In The world wide web conference , 3307 – 3313 . Wang , K . ; Shen , Z . ; Huang , C . ; Wu , C . - H . ; Dong , Y . ; and Kanakia , A . 2020 . Microsoft academic graph : When experts are not enough . Quantitative Science Studies , 1 ( 1 ) : 396 – 413 . Wang , M . ; Zheng , D . ; Ye , Z . ; Gan , Q . ; Li , M . ; Song , X . ; Zhou , J . ; Ma , C . ; Yu , L . ; Gai , Y . ; Xiao , T . ; He , T . ; Karypis , G . ; Li , J . ; and Zhang , Z . 2019c . Deep Graph Library : A Graph - Centric , Highly - Performant Package for Graph Neu - ral Networks . arXiv preprint arXiv : 1909 . 01315 . Wang , X . ; He , X . ; Cao , Y . ; Liu , M . ; and Chua , T . - S . 2019d . Kgat : Knowledge graph attention network for recommen - dation . In Proceedings of the 25th ACM SIGKDD interna - tional conference on knowledge discovery & data mining , 950 – 958 . Xu , D . ; Ruan , C . ; Korpeoglu , E . ; Kumar , S . ; and Achan , K . 2020 . Inductive representation learning on temporal graphs . arXiv preprint arXiv : 2002 . 07962 . Zhang , Y . ; Dai , H . ; Kozareva , Z . ; Smola , A . J . ; and Song , L . 2018a . Variational reasoning for question answering with knowledge graph . In Thirty - second AAAI conference on ar - tiﬁcial intelligence . Zhang , Y . ; Dai , H . ; Toraman , K . ; and Song , L . 2018b . Kgˆ 2 : Learning to reason science exam questions with contextual knowledge graph embeddings . arXiv preprint arXiv : 1805 . 12393 . Zheng , D . ; Song , X . ; Ma , C . ; Tan , Z . ; Ye , Z . ; Dong , J . ; Xiong , H . ; Zhang , Z . ; and Karypis , G . 2020 . Dgl - ke : Train - ing knowledge graph embeddings at scale . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , 739 – 748 . A Dataset Table 8 presents the statistics on the extracted splits of each dataset . During the ﬁnal experiment on the test set , we add all the edges in the validation period to the training set , align - ing the testing and validation experiment settings . Dataset PG - 1M PG - 10M PG - Full # Training ( Validation ) 18 , 201 , 417 269 , 061 , 163 1 , 881 , 118 , 345 # Training ( Testing ) 20 , 541 , 357 305 , 902 , 166 2 , 174 , 849 , 724 # Validation 265 , 196 3 , 160 , 831 28 , 179 , 279 # Test 147 , 106 2 , 396 , 547 26 , 389 , 424 Table 8 : Statistics of datasets splits . B Hyperparameters B . 1 Static Transductive We tune the hyper - parameters of both models us - ing the following set of values : embedding dimen - sions ∈ { 50 , 100 , 200 , 400 } , learning rate ∈ { 0 . 003 , 0 . 01 , 0 . 03 , 0 . 1 , 0 . 3 } , number of negative samples ∈ { 128 , 256 , 512 , 1024 , 2048 } , regularization coefﬁcient ∈ { 0 . 0 , 1e - 9 , 1e - 8 , 1e - 7 , 1e - 6 , 1e - 5 } , α ∈ { 0 . 25 , 0 . 5 , 1 , 2 } , and γ ∈ { 0 . 0 , 6 , 12 , 18 , 24 } . Due to the high computational cost of running sweeps on larger KGs , we only tune the pa - rameters on the smaller PG - 1M dataset and reuse the same hyper - parameters for PG - 10M and PG - Full . Hyperparameter ComplEx RotatE embedding dimension 200 50 learning rate 0 . 3 0 . 1 number of negative samples 512 64 regularization coefﬁcient 1e - 6 1e - 6 α 0 . 25 1 γ - 6 Table 9 : Best hyperparameters for the DE - X models . B . 2 Temporal Transductive In addition to the original training environment , we experimented with adversarial negative sampling . The hyper - parameters of both models are tuned us - ing the following set of values : embedding dimension ∈ { 50 , 100 , 200 } , ψ ∈ { 0 . 08 , 0 . 16 , 0 . 32 , 0 . 64 } , learning rate ∈ { 0 . 003 , 0 . 01 , 0 . 03 , 0 . 1 , 0 . 3 } , number of nega - tive samples ∈ { 128 , 256 , 512 , 1024 } , regularization coefﬁcient ∈ { 0 . 0 , 1e - 9 , 1e - 8 , 1e - 7 , 1e - 6 , 1e - 5 } , dropout ∈ { 0 . 0 , 0 . 1 , 0 . 2 } , α ∈ { 0 . 25 , 0 . 5 , 1 , 2 } , and γ ∈ { 0 . 0 , 6 , 12 , 18 , 24 } . Hyperparameter TransE DistMult embedding dimension 100 100 ψ 0 . 08 0 . 08 learning rate 0 . 1 0 . 1 number of negative samples 512 512 regularization coefﬁcient 0 . 0 1e - 6 dropout 0 . 0 0 . 1 α 0 . 25 - γ 6 - Table 10 : Best hyperparameters for the DE - X models . B . 3 Inductive We only use 1 - layer models throughout our experiments as running 2 - layer models proved to be infeasible memory - wise on our GPUs . GraphSAGE - X : We tune the variations using the follow - ing set of values : number of negative samples ∈ { 5 , 25 , 50 } , learning rate ∈ { 0 . 001 , 0 . 003 , 0 . 01 , 0 . 03 , 0 . 1 } , fanouts ∈ { 5 , 15 , 25 } , model dimension ∈ { 100 , 200 , 400 } , dropout ∈ { 0 . 0 , 0 . 1 , 0 . 2 } , aggregator type ∈ { mean , pool } , and nor - malization ∈ { None , Layer } . Hyperparameter - D - H - E number of negative samples 25 50 50 learning rate 0 . 03 0 . 03 0 . 03 fanouts 15 50 25 model dimension 100 400 400 dropout 0 . 0 0 . 1 0 . 1 aggregator type mean mean mean normalization Layer Layer Layer Table 11 : Best hyperparameters for the GraphSAGE - X mod - els . RGCN - X : We tune the hyper - parameters of all the vari - ations using the following set of values : number of neg - ative samples ∈ { 5 , 25 , 50 } , learning rate ∈ { 0 . 001 , 0 . 003 , 0 . 01 , 0 . 03 , 0 . 1 } , fanouts ∈ { 5 , 15 , 25 } , model dimen - sion ∈ { 100 , 200 , 400 } , number of bases ∈ { 8 , 16 , 32 } , dropout ∈ { 0 . 0 , 0 . 1 , 0 . 2 } , and normalization ∈ { None , Layer } . Hyperparameter - D - H - E number of negative samples 25 25 50 learning rate 0 . 01 0 . 003 0 . 03 fanouts 25 15 25 model dimension 100 400 200 number of bases 8 8 8 dropout 0 . 0 0 . 0 0 . 0 normalization Layer Layer Layer Table 12 : Best hyperparameters for the RGCN - X models . C Community Detection As an auxiliary output , we use the Leiden algorithm ( Traag , Waltman , and Van Eck 2019 ) to detect communities in Pub - Graph . To this end , we ﬁrst extract the full citation network from all the publication - publication links . Then , we use the Leiden algorithm to detect the communities in the citation network . Throughout our experiments , we use the leidenalg li - brary 1 . We modify this implementation to restrict the maxi - mum number of generated communities . More speciﬁcally , to restrict the number of communities to N , we implement the following procedure : 1 . Create N communities . 2 . Randomly assign each node to an arbitrary community . 3 . Execute the Leiden algorithm without allowing it to form new communities . Moreover , we restrict the number of papers in a com - munity to a max value to avoid creating 1 ) large unfo - cused communities and 2 ) small insigniﬁcant communities . Then , we experiment with four quality functions : Modu - lar , RBER ( Reichardt and Bornholdt’s Potts model with an Erd ˝ os - R ´ enyi null model ) , Signiﬁcance , and Surprise . Our experiments show that the Signiﬁcance quality function pro - duces the highest quality communities . Since OpenAlex concepts have a hierarchical structure , for each community , we extract the ancestral graph of the concepts connected to the publications and count the num - ber of children for each root concept . Then , we select the largest root concept for each community and calculate the percentage of the papers that are children of that root con - cept . This metric is then used to determine the communities’ quality when tuning the algorithm’s parameters . We tune the Leiden algorithm on the extracted citation network with the following parameters : maximum papers per community ∈ { 300 , 000 , 500 , 000 } , number of com - munities ∈ { 3000 , 4000 , 5000 , 6000 } . Based on our exper - iments , the highest quality communities are produced by the following parameters : maximum papers per community = 300 , 000 , number of communities = 3000 . Figure 4 illus - trates our results on different number of communities . D Negative Sampling We further analyze the effect of the number of negative samples on the model’s performance . Figure 5 presents the result of our experiments with varying numbers of nega - tive samples on all the introduced strategies . As expected , the model’s performance rapidly drops with the increase of negative samples . Moreover , the community - based neg - ative sampling results act as an excellent proxy at 5k neg - ative samples and seem to be converging to the full vari - ation around 10k negative samples . This ﬁnding is further evidence of the effectiveness of this method . E WikiData Ontology Table 13 presents the mapping from OpenAlex metadata to WikiData properties , which we created in the transforma - 1 https : / / github . com / vtraag / leidenalg Figure 4 : Analysis of the effect of the number of communi - ties on the quality of communities . Figure 5 : Analysis of the effect of negative samples count on the model’s performance measured by MRR . tion phase . These mappings are selected such that they best describe the metadata ﬁeld . For those metadata with no suit - able parallel property , we create artiﬁcial ones to keep the KG as complete as possible . We explain certain ﬁelds that might come across as ambiguous as follows : 1 . Instance of → P31 : “Instance of” indicates the class to which a particular entity belongs . The entities are Work , WikiData Properties OpenAlexMetadata WikiDataProperty OpenAlexMetadata WikiDataProperty OpenAlexMetadata WikiDataProperty OpenAlex Id P10283 ( OpenAlex ID ) Type P2308 ( class ) Country Code P299 ( ISO 3166 - 1 numeric code ) MAG P6366 ( Microsoft Academic ID ) Updated Date P5017 ( last update ) City P131 ( located in the administrative territorial entity ) Orcid P496 ( ORCID ID ) Year P585 ( point in time ) Geo Region P276 ( location ) Grid Id P2427 ( GRID ID ) Created Date P571 ( inception ) Country P17 ( country ) Scopus Id P1153 ( Scopus author ID ) Date of Publication P577 ( publication date ) Published in P1433 ( published in ) ROR P6782 ( ROR ID ) Author P50 ( author ) Latitude & Longitude P625 ( coordinate location ) Wikidata Id P1687 ( Wikidata property ) Title P1476 ( title ) Description P10358 ( original catalog description ) Geonames Id P1566 ( GeoNames ID ) Publisher P123 ( publisher ) Ancestor P1038 ( relative ) DOI P356 ( DOI ) Related to P1659 ( related properties ) Level / Position P1352 ( ranking ) ISSN P236 ( ISSN ) Works Count P3740 ( number of works ) Twitter Handle P2002 ( Twitter username ) ISSNL P7363 ( ISSN - L ) Volume P478 ( volume ) MeSH Qualiﬁer Id P9341 ( MeSH qualiﬁer ID ) PMID P698 ( PubMed ID ) Issue P433 ( issue ) MeSH Descriptor Id P486 ( MeSH descriptor ID ) PMCID P932 ( PMCID ) Homepage URL P856 ( ofﬁcial website ) Score P4271 ( number of points / goals / set scored ) UMLS CUI P2892 ( UMLS CUI ) OA status P6954 ( online access status ) Relationship P2309 ( relation ) Instance of P31 ( instance of ) Afﬁliation P1416 ( Afﬁliation ) Concept P921 ( main subject ) Wikipedia P5178 ( Wikipedia Library partner ID ) Cites work P2860 ( Cites work ) Display Name P2561 ( name ) Artiﬁcial Properties OpenAlexMetadata ArtiﬁcialProperty OpenAlexMetadata ArtiﬁcialProperty OpenAlexMetadata ArtiﬁcialProperty Cited by count P cited by count Is retracted P is retracted UMLS AUI P umls aui Table 13 : OpenAlex metadata information mapping to WikiData properties . Human , Concept , Institution , and Venue . 2 . Type → P2308 : “Type” indicates the type of work , e . g . , journal article , book , etc . ( uses crossref’s type ) , and the type of institution , e . g . , education , healthcare , etc . ( con - trolled by ROR type ) 3 . Related to → P1659 : “Related to” indicates a related con - cept or an afﬁliated institution . 4 . Ancestor → P1038 : “Ancestor” is used to indicate the ancestors of a concept in the concept hierarchical tree . 5 . Level / Position → P1352 : “Level / Position” indicates the concept level in the concept tree . Moreover , it is also used to indicate an author’s position in a publication . 6 . Score → P4271 : “Score” indicates how much two con - cepts are related . It is further used to indicate how good of a match a concept is to a publication . F Surprising Citations We further analyze the result of our ComplEx model by running a few case studies on the “should - have” , i . e . , high scores for negative samples , and the “surprising , ” i . e . , low Query paper Negative paper Relative Score BiPO 4 - Derived 2D Nanosheets for Efﬁcient Electrocatalytic Reduction of CO2 to Liquid Fuel Carbon - Based Metal - Free Catalysts for Electrocatalytic Reduction of Nitrogen for Synthesis of Ammonia at Ambient Conditions 2 . 927 High - Performance Fullerene Free Polymer Solar Cells Based on New Thiazole - Functionalized Benzo [ 1 , 2 - b : 4 , 5 - b ] dithiophene D - A Copolymer Donors Chlorine substituted 2D - conjugated polymer for high - performance polymer solar cells with 13 . 1 % efﬁciency via toluene processing 1 . 934 Cationic vs . non - cationic polymeric vectors for nucleic acid delivery The promises and pitfalls of RNA - interference - based therapeutics 1 . 109 Force - Induced Turn - On Persistent Room - Temperature Phosphorescence in Purely Organic Luminogen How the Molecular Packing Affects the Room Temperature Phosphorescence in Pure Organi Compounds : Ingenious Molecular Design , Detailed Crystal Analysis , and Rational Theoretical Calculations 2 . 605 Structures and Spin States of Iron ( II ) Complexes of Isomeric 2 , 6 - Di ( 1 , 2 , 3 - triazolyl ) pyridine Structure function relationships in molecular spin - crossover complexes 1 . 408 Table 14 : Examples of “should have” citations . Query paper Positive paper Rank Explanation Morphological evolution of self - assembled PS - g - PA6 graft copolymer via in situ polymerization Analytical characterization of color changing waste polystyrene plastic 527 Positive paper only has 2 citations and 4 references All You Need Is Sleep : The Effects of Sleep Apnea and Treatment Beneﬁts in the Heart Failure Patient Supplemental dataset on the inﬂuence of cardiac resynchronization therapy in pacing - induced cardiomyopathy and concomitant central sleep Apnea 501 Positive paper only has 1 citation and 2 references In - Situ Growth of MOF for Energy Conversion and Storage Devices Waste tyre pyrolysis – Impact of the process and its products on the environment 622 Difﬁcult to ﬁnd the similarity based on the titles Normal Force - Induced Highly Efﬁcient Mechanical Sterilization of GaN Nanopillars Multidrug - Resistant Bacterial Infections in U . S . Hospitalized Patients , 2012 – 2017 812 Difﬁcult to ﬁnd the similarity based on the titles Applications of Machine Learning in Bone and Mineral Research Comparison of Machine Learning Models to Predict Risk of Falling in Osteoporosis Elderly 933 Bad performance of ComplEx Table 15 : Examples of “surprising” citations . scores for positive samples , citations . For each query paper and positive candidate pair , we randomly select 1000 nega - tive papers and rank the results based on the scores generated by the model . Table 14 showcases some examples of “should - have” ci - tations where none of the positive papers are ranked ﬁrst and presents the negative papers that are ranked ﬁrst . The relative score is calculated as the ratio of the negative pa - per’s score to the positive paper’s . Table 15 presents the ex - amples of “surprising” citations where the positive papers are ranked lower than 500 . In some cases , the positive pa - per only has a few citations and references , which leads to being under - trained . In other cases , it isn’t easy to ﬁnd the similarity even provided with the title . Finally , in some sam - ples , even though the titles show a high degree of similarity , ComplEx performs poorly .