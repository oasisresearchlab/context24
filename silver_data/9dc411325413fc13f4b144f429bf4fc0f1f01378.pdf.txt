Understanding the Dataset Practitioners Behind Large Language Model Development Crystal Qian ∗ cjqian @ google . com Google Research New York City , NY , USA Emily Reif ∗ ereif @ google . com Google Research Seattle , WA , USA Minsuk Kahng kahng @ google . com Google Research Atlanta , GA , USA ABSTRACT As large language models ( LLMs ) become more advanced and im - pactful , it is increasingly important to scrutinize the data that they rely upon and produce . What is it to be a dataset practitioner doing this work ? We approach this in two parts : first , we define the role of " dataset practitioner " by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google . Then , we conduct semi - structured interviews with a cross - section of these practitioners ( N = 10 ) . We find that data quality is the top priority . To evaluate data quality , practitioners either rely on their own intuition or write custom evaluation logic . There is a lack of consensus across practitioners on what quality is and how to evaluate it . We discuss potential reasons for this phenomenon and opportunities for alignment . 1 INTRODUCTION As the state - of - the - art for large language models ( LLMs ) advances [ 35 ] , the field of relevant data analysis is rapidly evolving . Because the data used and produced by LLMs is largely unstructured , tradi - tional statistical analyses are insufficient for rigorous evaluation [ 9 , 38 , 41 ] . Furthermore , as applications of these LLMs become more widely adopted and impactful [ 1 , 6 ] , there’s a deeper need to qualitatively understand these datasets ; for instance , to mitigate sociological biases , ensure safe outputs , and minimize harm . We aim to identify the needs and challenges of those who want to understand unstructured , text - based datasets for LLM development : a group that we define as dataset practitioners . To develop this definition , we perform a retrospective analysis within Google . We then conduct semi - structured interviews with a cross - section of these practitioners ( N = 10 ) to better understand their workflows , tools , and challenges . We find that practitioners prioritize data quality ; however , there is no consensus on what constitutes " high quality " data . Despite HCI / VIS researchers’ active efforts to deliver relevant sensemaking methods and tools , data practitioners in aggregate do not appear to be adopting these solutions , instead relying either on cursory visual inspection of spreadsheets or custom analyses logic in notebooks to understand their data . There is demand for frameworks , consensus , and tooling in this space that is not being met . We discuss hypothe - ses for this observed phenomenon , and conclude with opportunities for further research and alignment . ∗ Both authors contributed equally to this research . 2 RELATED WORK 2 . 1 Techniques and tools To better understand unstructured text outputs of LLMs , there is work in developing new explainability algorithms and techniques [ 14 , 15 , 28 ] . These techniques can be packaged in frameworks and tools [ 2 , 24 ] that practitioners can use to explore data with varying degrees of effort . Low - effort exploration involves cursory , surface - level viewing of data , with minimal interaction or manipulation . These interactions typically occur in tools used to view tabular data , such as Google Sheets or Microsoft Excel . On the other end , high - effort exploration may involve writing code to perform cus - tom analyses . This is commonly done in Python scripts or note - books such as Google Colab or Jupyter [ 11 , 26 , 27 , 39 ] . We define intermediate - effort exploration as involving more effort than open - ing up a spreadsheet , but less effort than writing custom code . This category might involve actions such as interacting with a user in - terface or running existing scripts . Using more interactive features in Google Sheets , such as developing formulas or annotating data , would fall in this category . There are a trove of curated tools for data understanding and model interpretability at this level of ex - ploration , including the Language Interpretability Tool [ 40 ] and What - If Tool [ 44 ] , Model Tracker [ 3 ] , ChainForge [ 5 ] , RuleMatrix [ 7 ] , and AI Fairness 360 [ 33 ] . 2 . 2 User studies User studies on practitioner - tooling interaction have identified cognitive biases in the workflow such as selective exploration of data , over - trust and misuse of interpretability tools , and gaps in understanding regarding data visualizations [ 16 , 25 , 32 ] . 2 . 3 Data priorities Datasets relevant to LLM development have become increasingly composed of smaller , curated subsets that target address specific concerns , such as safety and fairness [ 31 ] . The focus is increasingly on data quality [ 38 ] rather than quantity [ 17 ] ; though quantifying the criteria for quality is an open problem [ 16 , 23 ] . 2 . 4 Analyzing analyzers Previous work has classified those who work with data as data workers or data scientists [ 13 , 20 , 22 , 34 , 43 ] . Data worker satisfies the breadth that aim to capture , as our target population spans more job titles than data scientist . Data scientist is too narrow for our population . It does not encompass the specific challenges in - troduced by the new LLM - centered data regime , such as a rising need for qualitative evaluation methods or the broader range of job responsibilities within this role . These broader responsibilities 1 a r X i v : 2402 . 16611v1 [ c s . C L ] 21 F e b 2024 Crystal Qian , Emily Reif , and Minsuk Kahng might include , for example , creating new architecture to interpret data , or developing adjudication methods for human - labelled data . 3 RETROSPECTIVE ANALYSIS To identify the data practitioner , we conducted a retrospective analysis of teams working on developing LLMs within Google . Google’s organizational structure is uniquely positioned to sup - port a broad survey of the landscape as the technology stack is vertically - integrated [ 21 ] ; that is , the relevant tooling , infrastruc - ture , modeling , evaluation , and research are primary developed in - house . Due to the technology stack’s vertical integration , we were able to mitigate selection bias in our sample by maximizing exposure to a diverse range of projects . Using company - internal organizational charts and employee di - rectories , we identified projects associated with the development of Google’s core LLMs . We also conducted a meta - review of company - internal user studies around evaluating tools for data exploration . Applying a grounded theory methodology [ 12 ] , we inductively ap - plied a relational content analysis and synthesized common themes to develop a framework around data practitioning . 3 . 1 Defining the dataset practitioner Our dataset practitioner seeks to understand unstructured , text - based data for the purpose of developing large language models . The day - to - day work of the practitioner can cover a broad range of tasks traditionally defined in roles such as software developer , ma - chine learning engineer , data scientist , research scientist , product manager , and product counsel . The practitioner may prioritize these responsibilities concurrently , or switch gears along the model devel - opment lifecycle . They may do any of the following representative tasks : • Curating a new dataset from scratch • Creating a new benchmark dataset • Cleaning a dataset by removing or fixing bad examples • Analyzing a dataset ( user feedback , comments , etc ) to find trends in it • Understanding what bias issues might exist in the dataset • Making a go / no go decision on whether to use a dataset to train a model • Debugging a specific model error by finding relevant data • Finding ways to improve models , try different datasets , and compare model results • Identifying key metrics to define “quality” for a specific use case Next , we given examples of datasets that they may explore . The term " dataset " can traditionally imply a static and well - curated set of data ; we expand this notion to include any set of text examples , which may come from a variety of provenances ( e . g . scraped , syn - thetically generated , curated by experts ) . They fall in to two broad categories : ( 1 ) Training datasets • Pre - training data : LLMs are pre - trained on huge amounts of data from webscrapes , books , and other giant corpora . The curation of these datasets is hugely impactful on the model’s performance [ 30 ] . • Supervised Fine - Tuning ( SFT ) and Reinforcement Learning from Human Feedback ( RLHF ) data : These datasets are significantly smaller and more spe - cialized than pre - training data , and are used to refine the LLM from an open - ended generation model to a specific use case - most notably , the chatbot interface that many productionized LLMs employ . LLMs can be fine - tuned for other specific products and use cases . ( 2 ) Datasets involved in model evaluation • Benchmark evaluation data : Benchmark datasets are created to test specific functionalities or behaviors of the model . One notable category of these are safety benchmarks , which test the model’s ability to adhere to company policies and safety standards on concepts such as toxicity , hallucination , etc . • Model outputs : Model outputs can be evaluated out - side of the context of a specific benchmark . Side - by - sideanalysisofmodel outputsmaybeconductedagainst golden sets or outputs from a baseline model . • Conversational data : User interactions with LLM - based chatbots can be used to evaluate LLMs in the wild . 4 QUALITATIVE STUDY 4 . 1 Participants Using our updated definition , we recruited ten dataset practitioners ( N = 10 ) across different domains of the model development life - cycle [ 13 , 24 ] ; broadly : tooling , modeling , and evaluation . These participants and their current focus areas are listed in Table 1 . We validated our observation from Section 3 . 1 that the domains of their work are fluid ; participants who identified in one domain during our recruiting cycle demonstrated experience in many adjacent areas within the interview . For example , a practitioner formerly focused on modeling shifted priorities to safety and fairness evalu - ation as their models became more scrutinized and regulated , and two tool - builders reported being driven to build tooling to address their own unmet needs in modeling . Domain Participant ID Focus Area T1 Tools for data annotation T2 Tools for data curation T3 Tools for data understanding Tooling T4 Pipeline infrastructure M1 Data curation . M2 Model architecture Modeling M3 Model refinement R1 Robustness and abuse R2 Unsafe and sensitive content Evaluation R3 Annotator ethnography Table 1 : Study participants and their current focus area , grouped by domain . 2 Understanding the Dataset Practitioners Behind Large Language Model Development Table 2 : This matrix categorizes our findings ( inspired by Kandel et al . [ 24 ] ) . An ‘x’ in the cell indicates that a participant mentioned this specific topic in their interview . Topics are grouped by Processes , Tools , and Challenges , and participant are grouped by their domain from Table 1 . All participants mentioned interacting with spreadsheets and cited data quality as a challenge in their work . 4 . 2 Interviews Following recruitment and an informed consent process , we con - ducted semi - structured , one - on - one interviews with participants over video conferencing . Each 30 - minute interview covered three broad topics : ( 1 ) Understanding the use case : Background , use case , product impact , research questions ( 2 ) Tools and techniques : Awareness and usage of existing tools and pipelines , decision making , advantagesandlimitations , statisticaland visualinterpretabil - ity methods ( 3 ) User challenges : Bottlenecks , unaddressed concerns We synthesized our findings through a thematic analysis [ 8 ] . Each interview was de - identified , transcribed , broken into excerpts , andcoded . Theinterviewsubject matterandcodebookwereadopted from previous informal contextual inquiries and interview proto - cols from adjacent research studies [ 25 , 29 ] . Thematic elements , behaviors , and representative quotes in this paper are saturated [ 4 , 19 ] , with a code repeated in at least three distinct transcriptions . 5 RESULTS 5 . 1 Assessing data quality is the biggest challenge . There has been a recent shift in the literature from prioritizing benchmarks and abstract metrics [ 42 ] to prioritizing data quality [ 38 ] . We observed this phenomenon in our sample as well ; data quality—defining , finding , and identifying high - quality data—was unanimously the biggest user challenge and priority across all use cases ( Table 2 , Challenges ) . Data , historically , has been around volume rather than quality . . we’ve had this big paradigm shift . — T2 “Quality is the big obstacle . . . [ You need ] a lot of high - quality data . . . there’s no shortcut . ” —E1 5 . 2 However , practitioners rely largely on their own intuition to validate this data quality . Despite this focus on high - quality data , all participants mentioned understanding their datasets by viewing it in tabular form in spread - sheets ( Section 2 . 1 , low - effort exploration ) ; that is , they’re looking at a handful of examples . “I’ll read the first 10 examples , and then maybe some in the middle . ” —E1 3 Crystal Qian , Emily Reif , and Minsuk Kahng “I eyeball data . . It’s all my own intuition and kind of individually spot checking examples . ” —M2 Participants cited efficiency , customization , a short learning curve , and ease - of - sharing as reasons why they rely on spread - sheets ( Table 2 , Challenges ) . 1 5 . 3 Or , practitioners will run custom analyses . Participants were not engaging in low - effort exploration for lack of technical know - how ; 7 of the 9 participants mentioned writing custom code in Python notebooks to explore their data ( Section 2 . 1 , high - effort exploration ) , and in one instance even to train production models . Participants liked the ease of sharing and customizing these notebooks , and cited reliability , setup , code management as pain points of these notebooks ( Table 2 , Challenges ) , validating results from other studies on Python notebook usage [ 11 , 26 , 27 , 39 ] . 5 . 4 Practitioners recognize the confirmation biases in their exploration practices . The majority— if not all— of the data exploration is being done between visual inspection in spreadsheets and custom logic in Python notebooks , These two practices have been observed to perpetuate implicit cognitive bias in other settings [ 10 , 18 , 23 , 37 ] . Indeed , our participants admit to this confirmation bias in their practices : ““I look at data . . . eyeballing that things make sense . ” —M2 Model developers reported that they did not look at training data unless their model outputs were surprising . “When the data is passed to the modeling side , we as - sume that the data team has fixed everything . Unless we train and it doesn’t look right , then we’ll [ look at the data ] and give the data team that feedback . ” —M3 5 . 5 There is missing alignment on intermediate - effort exploration patterns . Practitioners across different domains have converged upon a low - effort exploration pattern : visually scanning data in Google Sheets . They’ve also converged upon a high - effort exploration pattern : writing custom Python code in Colab notebooks . However , there were no tools and processes within our description of ( Section 2 . 1 , intermediate - effort exploration ) that achieved inter - practitioner alignment . The few practitioners who performed intermediate - effort exploration did so using different methods , such as running a binary for calculating safety and toxicity thresholds , kicking off a pipeline to automatically classify their data , and using a UI to visualize embeddings . “Everyone is using a different thing , and getting ev - eryone on the same page is really difficult . ” —M1 This lack of alignment poses an organizational challenge ; as mentioned in Section 2 . 3 , LLM training datasets are increasingly 1 Interestingly and consistent with similar user studies , our participants empha - sized that their reliance on visual inspection of spreadsheets were their own behaviors and not best practices . They suggested that other practitioners likely used more so - phisticated tooling [ 36 ] . composed of smaller datsets to leverage the expertise of specific subteams . “With the new generative data— Many people are contributing with many different lenses . In practice , these [ subsets ] get built by random teams , they get added and nobody really reviews it because you can’t . ” —T4 6 DISCUSSION The reason why practitioners have not aligned on an intermediate - level exploration pattern is not obvious . Practitioners across all domains recognize that there is a gap in the workflow : “Not having an easy - to - use - tool is a major bottle - neck . . . Every time [ that I make changes to data ] , I have to write a custom colab to ingest the new fields . ” —M2 “There are no helpful tools from a qualitative re - searcher’s perspective . I jump between spreadsheets , a CSV file and a colab . . . The long story short is that we haven’t really found a very useful tool for this . ” —E3 “Right now , if you want to curate high - quality data , you go through [ each point ] manually as an expert , which is not scalable on the scale of thousands of examples . ” —T2 They are aware of and have tried the existing tools in this space . They are aligned on the properties that they want out of this tool ( Table 2 , Challenges ) , and these requests are being communicated to tooling teams : “The kinds of requests we tend to get nowadays are about larger - scale dataset management , like mix - ture building . When you have a big selection , going through and reviewing 10 , 000 rows is not what you want to do . . . That is much more amenable to sum - mary review . ” —T1 In response , tooling teams are evaluating and building tools to address these requests [ 3 , 40 , 44 ] . So , why is there a lack of alignment ? We discuss hypotheses posed by two different domains of practitioners . 6 . 1 The toolmakers’ hypothesis : the world is new . When tool developers ( T1 - T4 ) described exploration workflows , they explained that there was a lack of alignment because the field is still taking shape : “The pace is very frenetic right now . . tools are fast - changing . . . ” —T1 “There’s been a big step function in the NLP world . . it just takes a while to figure out what tools people need and what all use cases . ” —T2 Two observations from our interviews may support this claim . First , practitioners are using spreadsheets . Perhaps in the absence of a ground truth for unstructured data , practitioners prefer to rely on their own intuition . Similarly , without a definitive framework 4 Understanding the Dataset Practitioners Behind Large Language Model Development for qualitative data exploration , practitioners are sticking to the tools they know . Adopting new practices takes effort ( see Table 2 , Challenges > Learning curve ) , and spreadsheets have been tried - and - true from the previous state - of - the - art when visually spot - checking data and conducting statistical analyses were sufficient . Second , our participants described a landscape where there was a lack of alignment across multiple topics such as objectives , metrics , and benchmarks . This is validated in the literature [ 16 , 18 ] , and suggest that the field and its principles are still emerging . • Data quality : “There’s so many competing definitions of prompt quality . . . it’s a research north star that happens to be a major product priority . How can we improve this extremely important data set ? ’ —T1 , on LLM prompts “The quality of data is subjective ; a lot of people disagree . . . one person thinks it’s really high - quality data , but there’s no objective . ” —M1 , on training data “There’s not a framework for evaluating [ training data ] . . in a perfect world , there is well - articulated behavior ( tone , subject matter , objective results ) . . ” —T3 , on evaluation data • Metrics : “ [ Consider ] search rankings . . . what makes for a goodbenchmark , howdowecometoanagreement ? ” —M1 “If you’re doing simple classification , it’s easy to measure accuracy or precision or recall . But with these generative models , evaluation is very subjec - tive . Even the output of the model is subjective , so then , what’s going into the model - it’s really hard to say , is this better or worse ? ” —E1 • Safety : “Think about safety data curation . . people can’t agree on criteria , let alone apply that criteria at scale . ” —T2 • Communication : “What [ data practitioners are ] actually doing and what they communicate that they need are two very different things . What are they actually trying to do ? ” —T2 This lack of alignment is a bigger problem now that teams are collaborating more closely to curate training datasets . Even if one team in the development pipeline identifies their quality evaluation parameters , there needs to be further agreement at the inter - team level . 6 . 2 The model developers’ hypothesis : there’s no tool that works for my use case . Practitioners in the modeling and evaluation domains speculated that alignment was unlikely due to the custom needs and require - ments across use cases ( Section 3 . 1 ) . “I think why [ a spreadsheet is ] so universal is that it’s so basic . . you can customize it yourself to give this affordance that other tools may not give you . . it’s simple . ” —E1 “We have tried so many [ tools ] . Why I think these tools are limiting is because they offer you exploration on only one aspect of [ the data ] . . . For me , they’re too specific . ” —M2 Interestingly , whenaskedaboutthe custom requirements specific to their use cases , practitioners listed many similar requirements , including : • Summarizing salient features of a dataset and identifying the corresponding data slices ( 6 participants ) • Ensuring safety of outputs / respecting toxicity thresholds ( 4 participants ) • Evaluating numeric distributions on text / token length ( 3 participants ) It is likely the case that both hypotheses are true to some extent . There may be select opportunities for alignment as the field ma - tures , and there are likely other problems that will require custom solutions . For example , there are specific tools being developed to address challenges that persist across datasets , such as safety and toxicity classification [ 7 ] . 7 FUTURE WORK AND CONCLUSIONS In this study , we aimed to identify the needs of those who are exploring unstructured , text - based datasets for the purpose of de - veloping LLMs . To define this population of dataset practitioners , we conducted a retrospective analysis on teams working on LLM development . We then interviewed a broad cross - section of these practitioners to better understand their use cases and challenges . Through our retrospective analysis , we find that the dataset practitioner takes on a fluid role that is not well - defined in cur - rent literature on data workers . We hope that our contribution of defining this population and their use cases will enable the HCI community to better assess and support their needs . In our interviews , we found that data quality is unanimously the top priority , but quality is subjective . Further research should explore what data quality means in different contexts , and how the same data can be high - quality or low - quality depending on the situation and perspective . Clarifying subjectivity across conceptual frameworks , evaluations , and workflows in this space remains a top priority . Two primary data exploration patterns are identified : visually inspecting data in spreadsheets , which is not scalable , and writing custom analyses in Python notebooks , which is both high - effort and prone to confirmation bias . The community has not agreed upon alternative best - practices to explore this data , perhaps due to the newness of the space or the custom needs of the practitioners . There exist opportunities to better understand the landscape around both schools of thought : to formalize the lines of thinking in the shifting landscape , and to develop specific , flexible tooling solutions for custom analysis . “There’sa fundamentalchickenandeggproblem . . . there’s notoolingsopeopledon’tusetoolingsotooling doesn’t develop . ” —T2 5 Crystal Qian , Emily Reif , and Minsuk Kahng ACKNOWLEDGMENTS The authors wish to thank our colleagues at Google’s People + AI Research Team for helpful feedback and discussions , especially Michael Terry , Carrie Cai , and Michael Xieyang Liu REFERENCES [ 1 ] MalakAbdullah , AliaMadain , andYaserJararweh . 2022 . ChatGPT : Fundamentals , Applications and Social Impacts . In 2022 Ninth International Conference on Social Networks Analysis , Management and Security ( SNAMS ) . 1 – 8 . https : / / doi . org / 10 . 1109 / SNAMS58071 . 2022 . 10062688 [ 2 ] Namita Agarwal and Saikat Das . 2020 . Interpretable Machine Learning Tools : A Survey . In 2020 IEEE Symposium Series on Computational Intelligence ( SSCI ) . IEEE , Canberra , Australia , 1528 – 1534 . https : / / doi . org / 10 . 1109 / SSCI47803 . 2020 . 9308260 [ 3 ] Saleema Amershi , Max Chickering , Steven M . Drucker , Bongshin Lee , Patrice Simard , and Jina Suh . 2015 . ModelTracker : Redesigning Performance Analysis ToolsforMachineLearning . In Proceedingsofthe33rdAnnualACMConferenceon Human Factors in Computing Systems ( CHI ’15 ) . Association for Computing Ma - chinery , New York , NY , USA , 337 – 346 . https : / / doi . org / 10 . 1145 / 2702123 . 2702509 [ 4 ] Hikari Ando , Rosanna Cousins , and Carolyn Young . 2014 . Achieving Satu - ration in Thematic Analysis : Development and Refinement of a Codebook , . Comprehensive Psychology 3 ( 2014 ) , 03 . CP . 3 . 4 . https : / / doi . org / 10 . 2466 / 03 . CP . 3 . 4 arXiv : https : / / doi . org / 10 . 2466 / 03 . CP . 3 . 4 [ 5 ] IanArawjo , ChelseSwoopes , PriyanVaithilingam , MartinWattenberg , andElena Glassman . 2023 . ChainForge : A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing . arXiv : cs . HC / 2309 . 09128 [ 6 ] Maria Teresa Baldassarre , Danilo Caivano , Berenice Fernandez Nieto , Domenico Gigante , and Azzurra Ragone . 2023 . The Social Impact of Generative AI : An Analysis on ChatGPT . In Proceedings of the 2023 ACM Conference on Information Technology for Social Good ( GoodIT ’23 ) . Association for Computing Machinery , New York , NY , USA , 363 – 373 . https : / / doi . org / 10 . 1145 / 3582515 . 3609555 [ 7 ] Rachel K . E . Bellamy , Kuntal Dey , Michael Hind , Samuel C . Hoffman , Stephanie Houde , Kalapriya Kannan , Pranay Lohia , Jacquelyn Martino , Sameep Mehta , Aleksandra Mojsilovic , Seema Nagar , Karthikeyan Natesan Ramamurthy , John T . Richards , Diptikalyan Saha , Prasanna Sattigeri , Moninder Singh , Kush R . Varsh - ney , and Yunfeng Zhang . 2018 . AI Fairness 360 : An Extensible Toolkit for Detecting , Understanding , and Mitigating Unwanted Algorithmic Bias . CoRR abs / 1810 . 01943 ( 2018 ) . arXiv : 1810 . 01943 http : / / arxiv . org / abs / 1810 . 01943 [ 8 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative research in psychology 3 , 2 ( 2006 ) , 77 – 101 . [ 9 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language Models are Few - Shot Learners . In Advances in Neural Information Processing Systems , H . Larochelle , M . Ran - zato , R . Hadsell , M . F . Balcan , and H . Lin ( Eds . ) , Vol . 33 . Curran Associates , Inc . , 1877 – 1901 . https : / / proceedings . neurips . cc / paper _ files / paper / 2020 / file / 1457c0d6bfcb4967418bfb8ac142f64a - Paper . pdf [ 10 ] AylinCaliskan , JoannaJBryson , andArvindNarayanan . 2017 . Semanticsderived automatically from language corpora contain human - like biases . Science 356 , 6334 ( 2017 ) , 183 – 186 . [ 11 ] Souti Chattopadhyay , Ishita Prasad , Austin Z . Henley , Anita Sarma , and Titus Barik . 2020 . What’s Wrong with Computational Notebooks ? Pain Points , Needs , and Design Opportunities . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3313831 . 3376729 [ 12 ] JulietMCorbinandAnselmStrauss . 1990 . Groundedtheoryresearch : Procedures , canons , and evaluative criteria . Qualitative sociology 13 , 1 ( 1990 ) , 3 – 21 . [ 13 ] Anamaria Crisan , Brittany Fiore - Gartland , and Melanie Tory . 2021 . Passing the Data Baton : A Retrospective Analysis on Data Science Work and Workers . IEEE Transactions on Visualization and Computer Graphics 27 , 2 ( 2021 ) , 1860 – 1870 . https : / / doi . org / 10 . 1109 / TVCG . 2020 . 3030340 [ 14 ] Marina Danilevsky , Kun Qian , Ranit Aharonov , Yannis Katsis , Ban Kawas , and PrithvirajSen . 2020 . ASurveyoftheStateofExplainableAIforNaturalLanguage Processing . CoRR abs / 2010 . 00711 ( 2020 ) . arXiv : 2010 . 00711 https : / / arxiv . org / abs / 2010 . 00711 [ 15 ] Arun Das and Paul Rad . 2020 . Opportunities and Challenges in Explainable Arti - ficial Intelligence ( XAI ) : A Survey . CoRR abs / 2006 . 11371 ( 2020 ) . arXiv : 2006 . 11371 https : / / arxiv . org / abs / 2006 . 11371 [ 16 ] Finale Doshi - Velez and Been Kim . 2017 . Towards A Rigorous Science of Inter - pretable Machine Learning . arXiv : stat . ML / 1702 . 08608 [ 17 ] Hugh Durrant - Whyte . 2015 . Data , Knowledge and Discovery : Machine Learning meets Natural Science . In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ’15 ) . Association for Computing Machinery , New York , NY , USA , 7 . https : / / doi . org / 10 . 1145 / 2783258 . 2785467 [ 18 ] Leilani H Gilpin , David Bau , Ben Z Yuan , Ayesha Bajwa , Michael Specter , and Lalana Kagal . 2018 . Explaining explanations : An overview of interpretability of machine learning . In 2018 IEEE 5th International Conference on data science and advanced analytics ( DSAA ) . IEEE , IEEE , Turin , Italy , 80 – 89 . [ 19 ] Greg Guest , Arwen Bunce , and Laura Johnson . 2006 . How Many Inter - views Are Enough ? : An Experiment with Data Saturation and Variability . Field Methods 18 , 1 ( 2006 ) , 59 – 82 . https : / / doi . org / 10 . 1177 / 1525822X05279903 arXiv : https : / / doi . org / 10 . 1177 / 1525822X05279903 [ 20 ] Lei Han , Tianwa Chen , Gianluca Demartini , Marta Indulska , and Shazia Sadiq . 2020 . On Understanding Data Worker Interaction Behaviors . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’20 ) . Association for Computing Machinery , New York , NY , USA , 269 – 278 . https : / / doi . org / 10 . 1145 / 3397271 . 3401059 [ 21 ] Kathryn Rudie Harrigan . 1985 . Vertical integration and corporate strategy . Academy of Management journal 28 , 2 ( 1985 ) , 397 – 425 . [ 22 ] Harlan Harris , Sean Murphy , and Marck Vaisman . 2013 . Analyzing the analyzers : An introspective survey of data scientists and their work . " O’Reilly Media , Inc . " . [ 23 ] Bernease Herman . 2019 . The Promise and Peril of Human Evaluation for Model Interpretability . arXiv : cs . AI / 1711 . 07414 [ 24 ] Sean Kandel , Andreas Paepcke , Joseph M . Hellerstein , and Jeffrey Heer . 2012 . Enterprise Data Analysis and Visualization : An Interview Study . IEEE Transac - tions on Visualization and Computer Graphics 18 , 12 ( 2012 ) , 2917 – 2926 . https : / / doi . org / 10 . 1109 / TVCG . 2012 . 219 [ 25 ] Harmanpreet Kaur , Harsha Nori , Samuel Jenkins , Rich Caruana , Hanna Wallach , and Jennifer Wortman Vaughan . 2020 . Interpreting Interpretability : Under - standing Data Scientists’ Use of Interpretability Tools for Machine Learning . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3313831 . 3376219 [ 26 ] Mary Beth Kery , Bonnie E . John , Patrick O’Flaherty , Amber Horvath , and Brad A . Myers . 2019 . Towards Effective Foraging by Data Scientists to Find Past Analysis Choices . In Proceedingsofthe2019CHIConferenceonHumanFactorsinComputing Systems ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300322 [ 27 ] Mary Beth Kery , Marissa Radensky , Mahima Arya , Bonnie E . John , and Brad A . Myers . 2018 . The Story in the Notebook : Exploratory Data Science using a Literate Programming Tool . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 11 . https : / / doi . org / 10 . 1145 / 3173574 . 3173748 [ 28 ] Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda Viegas , and Rory sayres . 2018 . Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV ) . In Proceedings of the 35th International Conference on Machine Learning ( Proceedings of Machine Learning Research ) , Jennifer Dy and Andreas Krause ( Eds . ) , Vol . 80 . PMLR , 2668 – 2677 . https : / / proceedings . mlr . press / v80 / kim18d . html [ 29 ] CatherineLi , TalieMassachi , JordanEschler , andJeffHuang . 2023 . Understanding the Needs of Enterprise Users in Collaborative Python Notebooks : This paper examines enterprise user needs in collaborative Python notebooks through a dyadic interview study . In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI EA ’23 ) . Association for Computing Machinery , New York , NY , USA , Article 402 , 7 pages . https : / / doi . org / 10 . 1145 / 3544549 . 3573843 [ 30 ] Shayne Longpre , Gregory Yauney , Emily Reif , Katherine Lee , Adam Roberts , BarretZoph , DennyZhou , JasonWei , KevinRobinson , DavidMimno , andDaphne Ippolito . 2023 . A Pretrainer’s Guide to Training Data : Measuring the Effects of Data Age , Domain Coverage , Quality , & Toxicity . arXiv : cs . CL / 2305 . 13169 [ 31 ] Ninareh Mehrabi , Fred Morstatter , Nripsuta Saxena , Kristina Lerman , and Aram Galstyan . 2021 . ASurveyonBiasandFairnessinMachineLearning . ACMComput . Surv . 54 , 6 , Article 115 ( jul 2021 ) , 35 pages . https : / / doi . org / 10 . 1145 / 3457607 [ 32 ] Tim Miller . 2019 . Explanation in artificial intelligence : Insights from the social sciences . Artificial Intelligence 267 ( 2019 ) , 1 – 38 . https : / / doi . org / 10 . 1016 / j . artint . 2018 . 07 . 007 [ 33 ] Yao Ming , Huamin Qu , and Enrico Bertini . 2019 . RuleMatrix : Visualizing and Understanding Classifiers with Rules . IEEE Transactions on Visualization and Computer Graphics 25 , 1 ( 2019 ) , 342 – 352 . https : / / doi . org / 10 . 1109 / TVCG . 2018 . 2864812 [ 34 ] Michael Muller , Ingrid Lange , Dakuo Wang , David Piorkowski , Jason Tsay , Q . Vera Liao , Casey Dugan , and Thomas Erickson . 2019 . How Data Science Workers Work with Data : Discovery , Capture , Curation , Design , Creation . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 15 . https : / / doi . org / 10 . 1145 / 3290605 . 3300356 [ 35 ] ROpenAI . 2023 . Gpt - 4technicalreport . arxiv2303 . 08774 . ViewinArticle 2 ( 2023 ) , 13 . 6 Understanding the Dataset Practitioners Behind Large Language Model Development [ 36 ] James W . Pennebaker . 2011 . The secret life of pronouns . New Scientist 211 , 2828 ( 2011 ) , 42 – 45 . https : / / doi . org / 10 . 1016 / S0262 - 4079 ( 11 ) 62167 - 2 [ 37 ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis . Proceedings of international conference on intelligence analysis 5 ( 2005 ) , 2 – 4 . [ 38 ] Nithya Sambasivan , Shivani Kapania , Hannah Highfill , Diana Akrong , Praveen Paritosh , and Lora M Aroyo . 2021 . “Everyone wants to do the model work , not the data work” : Data Cascades in High - Stakes AI . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems ( CHI ’21 ) . Association for Computing Machinery , New York , NY , USA , Article 39 , 15 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445518 [ 39 ] Aurélien Tabard , Wendy E . Mackay , and Evelyn Eastmond . 2008 . From indi - vidual to collaborative : the evolution of prism , a hybrid laboratory notebook . In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work ( CSCW ’08 ) . Association for Computing Machinery , New York , NY , USA , 569 – 578 . https : / / doi . org / 10 . 1145 / 1460563 . 1460653 [ 40 ] Ian Tenney , James Wexler , Jasmijn Bastings , Tolga Bolukbasi , Andy Coenen , Se - bastianGehrmann , EllenJiang , MahimaPushkarna , CareyRadebaugh , EmilyReif , and Ann Yuan . 2020 . The Language Interpretability Tool : Extensible , Interactive Visualizations and Analysis for NLP Models . arXiv : cs . CL / 2008 . 05122 [ 41 ] Krzysztof Wach , Cong Doanh Duong , Joanna Ejdys , R¯uta Kazlauskait˙e , Pawel Korzynski , Grzegorz Mazurek , Joanna Paliszkiewicz , and Ewa Ziemba . 2023 . The dark side of generative artificial intelligence : A critical analysis of controversies andrisksofChatGPT . EntrepreneurialBusinessandEconomicsReview 11 , 2 ( 2023 ) , 7 – 30 . [ 42 ] Kiri Wagstaff . 2012 . Machine Learning that Matters . arXiv : cs . LG / 1206 . 4656 [ 43 ] Dakuo Wang , Justin D . Weisz , Michael Muller , Parikshit Ram , Werner Geyer , CaseyDugan , YlaTausczik , HorstSamulowitz , andAlexanderGray . 2019 . Human - AI Collaboration in Data Science : Exploring Data Scientists’ Perceptions of Automated AI . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 211 ( nov 2019 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3359313 [ 44 ] James Wexler , Mahima Pushkarna , Tolga Bolukbasi , Martin Wattenberg , Fer - nanda Viégas , and Jimbo Wilson . 2020 . The What - If Tool : Interactive Probing of Machine Learning Models . IEEE Transactions on Visualization and Computer Graphics 26 , 1 ( 2020 ) , 56 – 65 . https : / / doi . org / 10 . 1109 / TVCG . 2019 . 2934619 7