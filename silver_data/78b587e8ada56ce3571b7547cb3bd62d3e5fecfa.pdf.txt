Toward End - to - End MLOps Tools Map : A Preliminary Study based on a Multivocal Literature Review Sergio Moreschini a , Gilberto Recupito b , a , Valentina Lenarduzzi c , Fabio Palomba b , David H¨astbacka a , Davide Taibi c , a a Tampere University , Finland b Software Engineering ( SeSa ) Lab , Department of Computer Science , University of Salerno , Italy c University of Oulu , Finland Abstract MLOps tools enable continuous development of machine learning , following the DevOps process . Diﬀerent MLOps tools have been presented on the market , however , such a number of tools often create confusion on the most appropriate tool to be used in each DevOps phase . To overcome this issue , we conducted a multivocal literature review mapping 84 MLOps tools identiﬁed from 254 Primary Studies , on the DevOps phases , highlighting their purpose , and possible incompatibilities . The result of this work will be helpful to both practitioners and researchers , as a starting point for future investigations on MLOps tools , pipelines , and processes . Keywords : MLOps , AIOps , DevOps , Artiﬁcial Intelligence , Machine Learning 1 . Introduction MLOps ( Machine Learning Operations ) is a practice that combines the best practices of software engineering and data science to manage the end - to - end lifecycle of ma - chine learning models , integrating them into traditional software . MLOps involves the use of tools and techniques to automate the building , testing , deployment , and moni - toring of machine learning models . The goal of MLOps is to improve the speed , quality , and reliability of machine learning models while reducing the risk of errors . The use of MLOps tools is becoming increasingly im - portant as the number of machine learning models being deployed in production environments continues to grow . These tools help data scientists and engineers to stream - line the process of building , testing , and deploying ma - chine learning models , and make it easier to monitor and maintain models in production . This allows organizations to be more agile in their use of machine learning , and to quickly adapt to changing business requirements and feed - back from users . Diﬀerent MLOps tools have been introduced in the market [ 1 ] , and a large number of practitioner’s confer - ence started to introduce MLOps and related tools . However , the very large number of MLOps tools , might introduce issues to organizations that need to select a sub - set of them to create their pipelines . Moreover , not all the Email addresses : sergio . moreschini @ tuni . fi ( Sergio Moreschini ) , grecupito @ unisa . it ( Gilberto Recupito ) , valentina . lenarduzzi @ oulu . fi ( Valentina Lenarduzzi ) , fpalomba @ unisa . it ( Fabio Palomba ) , david . hastbacka @ tuni . fi ( David H¨astbacka ) , davide . taibi @ oulu . fi ( Davide Taibi ) tools can be combined together , introducing other prob - lems during the adoption of them . In our previous work [ 1 ] , we proposed a multivocal lit - erature review to investigate the characteristics of MLOps tools mentioned by the ﬁrst 102 hits of Google Search , and by Google Scholar . However , we pointed out our previous work had several limitations , including a limited scope of the search , and the lack of snowballing on the selected sources . The ultimate goal of the study is to provide researchers and practitioners with some preliminary insights into the tools available in the context of an end - to - end machine learning pipeline . On the one hand , the results of the study might be beneﬁcial for researchers interested in under - standing the current support provided to machine learn - ing engineers in the wild . On the other hand , our ﬁndings might pose the basis for further studies aiming at elicit - ing the practitioner’s perspective on where they would use the available tools in practice and how they would create an ideal pipeline to more eﬀectively support end - to - end machine learning . Therefore , we conduct an internal diﬀerentiated repli - cation [ 2 ] of our previous work , overcoming the limitations , and extending it towards the identiﬁcation of the diﬀerent DevOps phases where each tool can be applied , identify - ing both compatibilities and incompatibilities . The main contribution of this work is a Graphical DevOps map of MLOps tools , useful to practitioners and researchers to easily identify useful tools for each DevOps phase . The remainder of this paper is structured as follows . Section 2 introduced the background on DevOps and MLOps . Section 3 describe the replicated study [ 1 ] and highlights the diﬀerences with our work . Section 4 presents Preprint submitted to Journal of Systems and Software April 7 , 2023 a r X i v : 2304 . 03254v1 [ c s . S E ] 6 A p r 2023 the research questions targeted by our study and the meth - ods adopted to address them . The results are presented in Section 5 and further elaborated in Section 6 . Section 7 identiﬁes and reports the threats to the validity of our study . Section 8 discusses the related work in the ﬁeld of MLOps , highlighting how our work advances the state of the art . Finally , Section 9 draws conclusions and highlights future works . 2 . Background DevOps is a set of practices and tools that aim to im - prove collaboration and communication between develop - ment and operations teams in order to accelerate the soft - ware delivery process . The goal of DevOps is to increase the speed and quality of software releases while also reduc - ing the risk of failures and downtime [ 3 ] . The DevOps process typically includes the following phases : • Plan . All the activities conducted before Code de - velopment . Identiﬁcation of the business require - ment and collection of end - user feedback . The goal is to create a product roadmap for future development . • Code . All the activities related strictly to the Code development phase . The tools used at this stage in - clude plugins to aid the development process . • Build . Once a coding task has been completed , a developer commits the code to a shared code repos - itory to produce a build . • Test . The build is deployed in a test environment to undergo several types of testing . Such tests in - clude acceptance tests , security tests , integration tests , performance tests , etc . • Release . After a build has successfully passed all the automated and manual tests , it is ready for de - ployment in the production environment and the op - erations team can schedule the releases . • Deploy . The build is ready and it is released into production . A set of tools are used to automate the release process . • Operate . At this stage the release is out and al - ready used by customers . The operations team is responsible for server conﬁguring and provisioning . • Monitor . The whole pipeline is monitored based on the data acquired at the Operate stage . Machine Learning Operations ( MLOps ) is a set of prac - tices and tools that aim to improve the collaboration and communication between data scientists and IT / De - vOps teams when building , testing , and deploying machine learning models . It is designed to help organizations man - age the complexity of deploying machine learning models in production environments and ensure that the models are reliable , scalable , and maintainable . One key diﬀerence between MLOps and traditional De - vOps is that MLOps focuses speciﬁcally on the needs of machine learning models , which can be very diﬀerent from traditional software . For example , machine learning mod - els require much more data to be trained and are often more computationally intensive than traditional software . Additionally , machine learning models are also subject to diﬀerent types of errors , such as overﬁtting and bias , which can be diﬃcult to detect and ﬁx . Another key diﬀerence is that MLOps often involves more complex , data - intensive workﬂows . Data scientists may need to work with large amounts of data , perform feature engineering , and experiment with diﬀerent model architectures , all of which can be time - consuming and resource - intensive . MLOps helps bridge the gap between data scientists and IT / DevOps teams by automating many of the tasks involved in building , testing , and deploy - ing machine learning models , such as data preprocessing , model training , and model serving . MLOps also helps with model governance and man - agement . Machine learning models need to be deployed , monitored , and maintained , and MLOps provides the nec - essary infrastructure and tooling to automate these tasks . It also helps keep track of the models’ versions , and per - formance metrics , and provides transparency of the data used to build the models . In summary , MLOps is a set of practices and tools that help organizations manage the complexity of deploy - ing machine learning models in production environments . It focuses on the speciﬁc needs of machine learning mod - els and helps bridge the gap between data scientists and IT / DevOps teams by automating many of the tasks in - volved in building , testing , and deploying machine learning models . It also helps with model governance and manage - ment , providing the necessary infrastructure and tooling to automate these tasks . A graphical representation of the MLOps process , pro - posed by Moreschini et al . [ 4 ] is illustrated in ( Figure 1 ) . 5 ML - OPS r e l e a s e d e p l o y o p e r a t e m o n i t o r d e b u g M L d e b u g M L o p t i m i z a t i o n t e s t b u i l d v a l i d a t i o n M L v a l i d a t i o n M L c o d e c o d e p l a n : p l a t f o r m s , c o m p o n e n t s , a r c h i t e c t u r e M L p l a n : d a t a , a l g o r i t h m Figure 1 : MLOps inﬁnite loop [ 4 ] 2 Table 1 : Studies Comparison Recupito et al . [ 1 ] Our Study RQs - List of MLOps tools - Mapping of MLOps tools into the MLOps phases - Main features of MLOps tools Search strings We added more search terms to the search string including ”ML ops” , and ”ML - ops” Bibliographic references Google Scholar Google Scholar , ACM digital Library , IEEEX - plore Digital Library , Science Direct , Scopus , and Springer link Gray literature search engines Google Google , Twitter , Reddit , and Medium Snowballing Not performed Performed Inclusion criteria ( PS ) - The study discusses components of minimal end - to - end MLOps workﬂow ( s ) - PS mentioning tools that can be used to create or compose MLOps pipelines - The study discusses the practice of MLOps or ML - based applications - The study refers to the implementation of MLOps tool ( s ) - The study discusses experience , opinions , or practices on MLOps pipeline ( s ) Exclusion criteria ( PS ) - The study does not oﬀer details concerning the design or implementation of MLOps tool ( s ) - Research Plans , roadmaps , vision papers , Mas - ter Thesis , Books - The study solely oﬀers the design of a speciﬁc component of ML pipeline ( s ) - Less than 750 followers for the author ( only for medium - gray literature ) - The study does not oﬀer or refer to details con - cerning ML automation - The study refers to commercial platform ( s ) that oﬀer ( s ) MLOps applications to sell their services for development and deployment Inclusion criteria ( Tools ) Data extraction to model monitoring tools End - to - end MLOps tools , including all the phases of the MLOps process ( Figure 1 ) Exclusion Criteria ( tools ) 1 ) The MLOps tool does not provide the docu - mentation or does not list characteristics - Not essentials ( Traning Orchestration , Explain - able AI , FeatureStore ) - The MLOps tool is mentioned less than ﬁve times by the selected studies - Tools breaking the automatization ( data gen - eration , data labelling , data monitoring ) - Addons - Not reliable ( ex . not enough stars ) Tools classiﬁcation - main MLOps characteristics - MLOps pipeline steps covered by each tool - data management features related to the ML models - Compatibilities and incompatibilities between tools - ML model management features - Possible combinations of tools for MLOps pipelines Selected primary studies 60 ( 6 white and 54 gray ) 254 ( 51 white and 203 gray ) MLOps Tools 13 84 3 . The Replicated Study In this Section , we summarize the replicated study [ 1 ] and the reasons why we replicated it , highlighting the dif - ferences with our work . We decided to replicate the study [ 1 ] , since – as far as we know – this is the only secondary study that identiﬁed a set of MLOps tools and their characteristics . The replication is performed following the guidelines proposed by Carver et al . [ 2 ] . Recupito et al . [ 1 ] performed a Multivocal Literature Review investigating the MLOps tools that can be adopted in a ML pipeline from the data extraction to the model monitoring phases . They considered gray and white lit - erature sources . They provided the tools list and their main distinctive features in general features , focusing on 1 ) main MLOps characteristics , 2 ) data management fea - tures related to the ML models , and 3 ) ML model man - agement features . Moreover , they classiﬁed the tools into cloud - based ML platforms , orchestration platforms , or TensorFlow extended . They extracted white literature from Google Scholar and the gray literature considering the ﬁrst 102 hits of Google Search . From these 102 hits , they retrieved 96 websites including blog posts and de - velopers’ websites , 9 GitHub repositories , and 5 YouTube videos . After applying inclusion and exclusion criteria , they included a total of 60 sources , of which 42 were web - sites / blogs , 7 GitHub Projects , 5 YouTube videos , and 6 peer - reviewed sources from Google Scholar . As a result , they extracted 13 tools from the 60 selected sources . 3 . 1 . Diﬀerences with this study Diﬀerently from our original study [ 1 ] , we considered tools that can be adopted in all the steps of the MLOps pipeline , we extended the search strings and adopted six bibliographic sources for the white literature instead of one , and three search engines for the gray literature instead of one . 3 Moreover , we performed the snowballing process , and we relaxed inclusion and exclusion criteria to obtain as many relevant papers as possible . As a result , we included 254 sources from which to extract tools in comparison to 60 considered by [ 1 ] , and obtained a list of 84 tools instead of 13 . Besides the larger amount of tools we classiﬁed , the most important result lies in the diﬀerent focus of the repli - cated study . While in our previous work [ 1 ] we identiﬁed the main features of the tools , we focused on the MLOps phases where the tools can be used , and on their compati - bilities and incompatibilities when creating tools pipelines . The complete comparison between this work and the replicated one [ 1 ] is reported in Table 1 . 4 . Study Design The goal of our study is to map all the MLOps tools with the purpose of outlining a fully - supported MLOps pipeline . Therefore , in order to investigate the aforemen - tioned goal , we formulated the following Research Ques - tion ( RQ ) : RQ To what extent do tools support the diﬀerent phases of MLOps ? where we aim at understanding which tools can be used in the diﬀerent phases of the MLOps process . To answer such RQ in an objective , systematic , and reproducible manner , we adopted the Systematic Multivo - cal Literature Review ( MLR ) methodology following the guidelines proposed by Garousi et al . [ 5 ] . The MLR process includes both peer - reviewed as well as gray literature and the diﬀerent perspectives between practitioners and academic researchers are taken into ac - count in the results . MLR classiﬁes contributions as aca - demic literature in the case of peer - reviewed papers and as gray literature in other types of content like blog posts , white papers , podcasts , etc . The MLR process is composed of four steps , as depicted in Figure 2 : • Selection of primary studies from the gray and white literature ; • Quality Assessment of the selected gray literature ; • Data extraction of the information needed to answer our RQ ; – Data Extraction from the primary studies – Data Extraction from the tools’ websites • Tools Selection • Data synthesis of the extracted results Data Extraction Data Synthesis And Interpretation Answer to RQs Quality Assessment Selection of Primary studies Inclusion / Exclusion Criteria White Literature Gray Literature Snowballing Figure 2 : Overview of the followed MLR process 4 . 1 . Selection of Primary Studies The ﬁrst step for selecting the Primary Studies ( PS ) is the search string identiﬁcation that will be adopted in the academic bibliographic sources and the gray literature source engines . To obtain a high recall and include as many papers as possible , we extended the search string used by Recupito et al . [ 1 ] : ( “ml - ops” OR “ml ops” OR mlops OR “machine learning ops” OR “machine learning operations” OR dataops ) AND ( tool OR application ) AND ( lifecycle OR pipeline OR platform OR workﬂow ) The search terms were applied to all the ﬁelds ( i . e . title , abstract , and keywords ) to include as many works as possible . We adopted the same search terms for retrieving gray literature from online sources and for white literature from academic bibliographic sources . Peer - reviewed literature search . We considered the papers indexed by six bibliographic sources : • ACM digital Library 1 • IEEEXplore Digital Library 2 • Science Direct 3 1 https : / / dl . acm . org 2 https : / / ieeexplore . ieee . org 3 https : / / www . sciencedirect . com 4 • Scopus 4 • Google Scholar 5 • Springer link 6 Both searches ( gray and white literature ) were conducted in November 2022 . Gray literature search . We performed the search using four search engines : • Google Search 7 • Twitter 8 • Reddit 9 • Medium 10 The search results consisted of books , blog posts , fo - rums , websites , videos , white - paper , frameworks , and pod - casts . Snowballing . Snowballing refers to using the refer - ence list of a paper or the citations to the paper to iden - tify additional primary studies [ 6 ] . We applied backward snowballing to the academic literature to identify relevant primary studies from the references of the selected sources . Moreover , we applied backward - snowballing for the gray literature following outgoing links of each selected source . Application of inclusion and exclusion criteria . Based on guidelines for Systematic Literature Reviews [ 7 ] , we deﬁned inclusion and exclusion criteria ( Table 2 ) . We considered less restrictive inclusion criteria to enable the inclusion of a more comprehensive tools set . Before applying the inclusion and exclusion criteria , we tested their applicability [ 8 ] on a subset of 25 PSs ran - domly selected from the retrieved ones . The ﬁnal set of inclusion and exclusion criteria is summarized in Table 2 . In order to screen each paper , we assigned two re - searchers to independently review them . To ensure fair - ness , we mixed up the assignments and made sure each researcher had a similar number of papers to review with other members of the team . In case of disagreement , a third author was brought in to reach a consensus . This was done to improve the reliability of our study . Fi - nally , to evaluate the inter - rater agreement before involv - ing the third author , we calculated Cohen’s kappa coef - ﬁcient . These results are documented in the replication package 11 . 4 https : / / www . scopus . com 5 https : / / scholar . google . com 6 https : / / link . springer . com / 7 https : / / www . google . com / 8 https : / / twitter . com / 9 https : / / www . reddit . com / 10 https : / / medium . com 4 . 2 . Quality Assessment of the Gray Literature Diﬀerently than peer - reviewed literature , gray litera - ture does not go through a formal review process , and therefore its quality is less controlled . To evaluate the cred - ibility and quality of the selected gray literature sources and to decide whether to include a gray literature source or not , we followed the recommendation of Garousi et al . guidelines [ 5 ] , considering the authority of the producer , the applied methodology , objectivity , date , novelty , and impact . The ﬁrst two authors assessed each source using the aforementioned criteria , with a binary or three - point Lik - ert scale , depending on the criteria itself . In case of dis - agreement , we discussed the evaluation with the third au - thor that helped to provide the ﬁnal assessment . 4 . 3 . Data Extraction As our goal is to characterize information from MLOps tools , we need to get these pieces of information directly from the tools’ websites . Therefore , the data extraction process is composed of two steps : ( PE ) Extraction of the list of tools from the primary stud - ies ( PSs ) that satisﬁed the quality assessment crite - ria . ( TE ) Extraction of the information from the tools list . In this case , we extracted the information directly from the oﬃcial website portals . Based on the RQ , we extracted the information in a review spreadsheet . The data extraction form , together with the mapping of the information needed to answer the RQ , is summarized in Table 3 . The data extraction process was conducted following the guidelines for qualitative analysis proposed by Wohlin et al . [ 9 ] . Two researchers extracted all the information . In case of disagreement , we discussed the results involving a third author . The discussion was conducted until the disagreement was solved . 4 . 4 . Tool Selection From the list of tools extracted in the previous step , we need to identify the ﬁnal set to answer our RQ . There - fore , we applied a similar process of the one adopted in the papers selection phase ( Section 4 . 1 ) , ﬁltering the tools based on a set of inclusion and exclusion criteria . Before applying the inclusion and exclusion criteria , we tested their applicability [ 8 ] on a subset of 10 % of tools randomly selected from the retrieved ones . The ﬁnal set of inclusion and exclusion criteria is reported in Table 2 . As well as for the PS selection , the selection of the tools was performed by two researchers , that independently re - viewed them . Also in this case , in case of disagreement , a third author was involved to reach a consensus . The Cohen’s kappa coeﬃcient has been calculated also in this case , to conﬁrm the inter - rater agreement . These results are documented in the replication package 11 . 5 Table 2 : Inclusion / Exclusion Criteria for Selection Criteria Inclusion PS mentioning tools that can be used to create or compose MLOps pipelines Exclusion - Not in English - Duplicated ( post summarizing other website ) - Out of topic ( using the terms for other purposes ) - Non peer - reviewed papers - Research Plans , Roadmaps , Vision Papers , Master Thesis - Less than 750 followers for the author ( only for medium - gray literature ) * Tools Selection Criteria Inclusion Commercial and Open Source Tools Exclusion - Not downloadable - No website available / reachable - Not in English - Duplicated - Hardware - Not Reliable ( ex . Not enough Stars in GitHub ) - Not essentials ( ex . Training Orchestration , Explainable AI , FeatureStore ) - Addons - Tools breaking the automatization ( Data Generation , Data Labeling ) * we set up this threshold because some account have been opened only to promote tools or authors did not gain enough followers due to their previous publications Table 3 : Data Extraction Info Description Step Tool Name Name of the tool PE Tool Url Phase Main DevOps phase covered by the tool ( Plan , Code , Build , Test , Release , Deploy , Operate , Monitor ) TE Purpose Main purpose of the tool ( e . g . Continuous Training ) Alternatives Main competitors of the tool 4 . 5 . Conducting the review From the Search process , we retrieved a total of 783 unique PS ( after the exclusion of 137 duplicated ) : 497 PS from the gray literature and 287 PS from the white literature . The snowballing process enabled to include 30 more sources ( 26 from white literature and 4 from gray literature ) . After the application of inclusion and exclusion criteria , we selected 254 PS ( 51 from the white literature and 203 from the gray literature ) . The application of the inclusion criteria performed by the author had an almost perfect agreement ( Cohen’s kappa = 0 . 870 ) . The application of the quality assessment process to the gray literature PS , resulted in the exclusion of 1 PS . As a result , we included a total of 254 PS , of which 203 PS from the gray literature and 51 PS from the white literature . From the data extraction process on the 254 selected PS we obtained 84 MLOps Tools . The application of the inclusion and exclusion crite - ria resulted in an almost perfect agreement ( The Cohen’s Kappa coeﬃcient = 0 . 812 ) . As a result , we obtained a ﬁnal set of 84 as reported in Table ? ? . 4 . 6 . Veriﬁability and Replicability To allow our study to be replicated , we have published the complete raw data in the replication package . 11 . 5 . Analysis of the Results We extracted characteristics of the identiﬁed tools and their alternatives and grouped them into the following sub - categories based on the MLOps phases covered by each tool . We identiﬁed 5 global categories : 1 . Continuous Development ( CD ) : based on the DevOps classic deﬁnition , CD includes those tools which create a single software package or a service . In order to extend this to MLOps we also include those tools which create a system that automatically deploys another service [ 10 ] . 2 . Continuous Integration ( CI ) : based on the De - vOps classic deﬁnition , CI includes the tools used for testing and validating code and components . In order to extend this to MLOps we also include tools for testing and validating data and models [ 10 ] . 3 . Continuous Testing ( CT ) : tools used to automat - ically retrain and serve the models [ 10 ] . 4 . End - to - End Full Stack MLOps tools : includes tools that can be used to create a full pipeline sin - gularly . 5 . Operations ( OPS ) : based on the DevOps classic deﬁnition , OPS includes those tools which manage the phases of Release , Deploy , Operate , and Monitor . 11 https : / / figshare . com / s / 0b62981a4ed90a93099f 6 Plan ML Build Test Release Deploy Operate Monitor Code Continuos deployment ML Lifecycle Applications Events Infrastructure Production model End - to - End Full - stack MLOps tools Browser Model Arts Version Control ML Model OPS Infrastructure Provisioning Deployment and serving Containers Orchestration tools Pipeline Design Continuos Integration Continuous Training CD CI / CD CT / CI / CD Data management Conﬁguration Management Figure 3 : Identiﬁed tools and their subcategories to produce MLOps Pipelines Then , we reﬁned our classiﬁcation considering the phases described in Section 2 . Plan . We identiﬁed 2 subcategories : 1 . Data Management : tools responsible to manage the data used to train the ML models . 2 . Pipeline Design : tools used to create pipelines . These can include dependency resolution , workﬂow management , and visualization . Build . As subcategory , we identiﬁed Version Control : tools responsible for tracking and managing changes to code Test . We identiﬁed 2 subcategories : 1 . Code : code - based test tools . 2 . ML Model : tools used for testing the ML models and the data used to train it . Release / Deploy . We identiﬁed 3 subcategories : 1 . Conﬁguration Management : tools used to perform conﬁguration management i . e , establishing consis - tency of a product’s attributes throughout its life . 2 . Deployment and Serving : tools used to automate the process of deploying machine learning models into production . The process automates many sub - tasks including model serving , a technique for integrating an ML model into a software system . 3 . Infrastructure Provisioning : tools used for the pro - cess of provisioning or creating infrastructure re - sources . Operate . We identiﬁed 2 subcategories : 1 . Containers : tools used to create containers . A con - tainer is deﬁned as a “standard unit of software that packages up code and all its dependencies so the ap - plication runs quickly and reliably from one comput - ing environment to another” [ 11 ] . 2 . Orchestration Tools : tools used to automate the pro - cess of coordination of containers . This includes start and stop , schedule and execution of tasks , and recovery processes . Monitor . We identiﬁed 5 subcategories : 1 . Applications : tools used to examine and investigate performance data of created applications . 2 . Events : tools used for event monitoring and provid - ing alerting functionalities . 3 . Infrastructure : tools used to monitor infrastructure nodes . 4 . ML Lifecycle : tools used for experiment tracking , dataset versioning , and model management of ML models . 5 . Production Model : tools used to monitor the de - ployed ML model in order to avoid models breaking or degrading in production . 7 Events 2 , 4 % Prod . Models 2 , 4 % ML Lifecycle 11 , 9 % Orchestration8 , 3 % Deployment7 , 1 % Version Control 6 , 0 % Containers2 , 4 % End - to - End 23 , 8 % CT & CI & CD 10 , 7 % OPS4 , 8 % Data Man . 6 , 0 % Pipeline Design4 , 8 % Figure 4 : Selected Tools by Subcategory Monitor20 , 2 % Operate10 , 7 % Release / Deploy 10 , 7 % Test2 , 4 % Build6 , 0 % Global39 , 3 % Plan10 , 7 % Figure 5 : Selected Tools by Phase The ﬁnal result of the categorization of the diﬀerent MLOps tools is illustrated in Figure 3 . This resulted in 84 diﬀerent tools being divided into 20 diﬀerent categories . The amount of diﬀerent tools for each subcategory is re - ported in Figure 4 and Table 4 where the tools for CT , CI , and CD have been grouped . Figure 4 shows the distribution of tools based on the phase of the MLOps pipeline they support . Also , in this case , CT , CI , and CD have been grouped . Table 4 : Tools and Categories Category Subcategory # tools Global - CT & CI & CD 9 - End - to - End Full Stack MLOps tools 20 - OPS 4 Plan - Data Management 5 - Pipeline Design 4 Build Version Control 5 Test - Browser 1 - ML Model 1 Release & - Deployment and Serving 6 Deploy - Infrastructure Provisioning 1 - Conﬁguration Management 2 Operate - Containers 2 - Orchestration tools 7 Monitor - Applications 2 - Events 2 - Infrastructures 1 - ML Lifecycle 10 - Production Models 2 6 . Discussion and Implications The results of the study provide a number of impli - cations and actionable points that are worth discussing further . On the existence of MLOps tools . First and foremost , our ﬁndings revealed the existence of 84 MLOps tools . Such a large amount of instru - ments were mostly made available by tool vendors and practitioners—as indicated by the number of tools iden - tiﬁed within the grey literature review analysis . On the one hand , this highlights the ever - increasing attention that practitioners have with respect to MLOps activi - ties , further motivating the research around the matter and the need for a comprehensive analysis of how to combine those tools , as preliminarily done within the scope of our work . On the other hand , we point out a noticeable lack of instruments proposed or empirically investigated by our research community . In particu - lar , both the comprehensive list of MLOps currently available and the mapping onto the MLOps pipeline , which are the key outcomes of our work , might serve as a basis for further investigations into various angles of the problem , like the actionability of these tools in practice , the information needs required by practition - ers to eﬀectively use those tools , or the most eﬀective strategies to combine multiple tools . Implication # 1 . Our study may therefore represent the ﬁrst step toward increased awareness of the impact that the empirical software engineering research ﬁeld may have on the development of new MLOps solutions . On the compatibility of MLOps tools . The “End - to - End MLOps tools” usually provide a pre - deﬁned tool pipeline . However , in some cases , it is pos - sible to replace their tools with alternative ones . Such tools need to cover all of the phases of the MLOps pipeline . For an optimal pipeline generation , not in - volving an “End - to - End MLOps tools” it is therefore needed the use a tool for each one of the phases in the “Dev” phase and at least one tool for the “Ops” phases [ 12 ] . The reason behind it is that while including multi - ple tools for the “Ops” phases might produce a valuable and improved output , the inclusion of multiple tools from the same phase of the “Dev” creates redundancy and might break the pipeline automation . According to our mapping exercise , the tools that emerged from the systematic study seem to be compatible , meaning that they oﬀer orthogonal pieces of information throughout the MLOps pipeline . Implication # 2 . We believe that this result might en - able several future investigations on the matter : ques - tions concerned with the information ﬂows required to 8 sequentially use tools or the redundant / contrasting in - formation that multiple tools might provide to practi - tioners are indeed still open and neglected by our re - search community . On the creation of a taxonomy . Last but not least , the mapping exercise conducted in our work may have implications for tool vendors and practitioners . The former might ﬁnd a comprehensive overview of the state of the art / practice which might potentially serve as a basis for developing novel instru - ments covering the steps of the MLOps pipeline that currently suﬀer from a lower representation . In addi - tion , the overview might also enable further considera - tions , like an informed analysis of the features provided by those tools , which in turn can lead tool vendors to improve the existing instruments and / or design addi - tional support systems . Implication # 3 . Practitioners can use our taxonomy to tune their own MLOps pipelines by uncovering al - ternative tools they were unaware of or experimenting with combinations of tools that might help them cover the entire MLOps pipeline . 7 . Threats to Validity The results of our study may be aﬀected by various sources of bias or error , including inaccuracies in data ex - traction , limitations in the scope of the literature review , subjectivity in the deﬁnition and application of inclusion and exclusion criteria . In this section , we address these potential threats by outlining the strategies we employed to mitigate them , as per the guidelines outlined in [ 9 ] . Construct validity . Construct validity concerns the extent to which the study’s object of investigation accu - rately reﬂects the theory behind the study , according to a reference . The research questions and classiﬁcation schema used in the study may be subject to this type of threat . To min - imize this risk , the authors independently reviewed and then discussed the research questions . As for the classiﬁca - tion schema adopted , we selected a standard classiﬁcation of DevOps . Internal Validity . The source selection approach adopted in this work is described in Section 4 . 1 . In order enable the replicability of our work , we carefully identi - ﬁed and reported bibliographic sources adopted to identify the peer - review literature , search engines , adopted for the gray literature , search strings as well as inclusion and ex - clusion criteria . Possible issues in the selection process are related to the selection of search terms that could have lead to a non complete set of results . To mitigate this risk , we broadened the search string adopted by Re - cupito et al . including possible synonyms for the term MLOps . To overcome the limitation of the search engines , we queried the academic literature from six bibliographic sources , while we included the gray literature from Google , Medium Search , Twitter Search and Reddit Search . Addi - tionally , we applied a snowballing process to include all the possible sources . The application of inclusion and exclu - sion can be aﬀected by researchers’ opinion and experience . To mitigate this threat , all the sources were evaluated by at least two authors independently . Moreover , to evalu - ate the quality of the inter - rater agreement we calculated the Cohen’s Kappa coeﬃcient , obtaining an almost per - fect agreement . We believe that such high agreement is due to the easiness of the application of the inclusion and exclusion criteria , and in particular it was easy to exclude papers not containing any reference to any tool . Conclusion validity . Conclusion validity is related to the reliability of the conclusions drawn from the re - sults [ 9 ] . To ensure the reliability of our treatments , we did not deﬁne the terminology ourselves , but we based our classiﬁcation on the existing DevOps and MLOps steps . Moreover , all primary sources were reviewed by at least two authors to mitigate bias in data extraction and each disagreement was resolved by consensus , involving a third author . External Validity . External validity is related to the generalizability of the results of our multivocal literature review . In this study , we map the literature on MLOps Tools , considering both the academic and the gray litera - ture . However , we cannot claim to have screened all the possible literature , since some documents might have not been properly indexed , or possibly copyrighted or , even not freely available . 8 . Related Work Several studies highlight the current state of tools that assist the ML pipeline . On the basis of feature analysis , Idowu et al . [ 13 ] give a comparison of 30 ML asset man - agement solutions . The ﬁndings demonstrate that ML - enabled systems with tracking , exploring , and retrieving activities for experiment reproducibility are supported by the state of practice and the state of research . By evalu - ating the 26 MLOps tools that may be supported by the ML Pipeline and examining their potential support , Ruf et al . [ 14 ] show the potential of MLOps . The ﬁndings show that none of the MLOps solutions are fully auto - mated enough to support the MLOps workﬂow , and that several products have capabilities that overlap and oﬀer the same support . Kolltveit et al . [ 15 ] conducted an SLR to investigate the actual tools that assist in model opera - tionalization , with the purpose of identifying feature gaps . The review’s ﬁndings showed that managing the edge de - ployment of ML models and implementing features like dynamic model switching and continuous model monitor - ing present real challenges for MLOps . Testi et al . [ 16 ] conducted a review to establish an MLOps taxonomy that outlines the approaches and processes used to establish an ML pipeline . The authors give a summary of the advan - tages and disadvantages of employing a selected tool in a 9 speciﬁc stage of the ML Pipeline based on the collection of tools they have uncovered . Finally , Recupito et al . [ 1 ] pre - sented a multivocal literature review of the several MLOps tools and features present in the state of the art and the state of the practice . After extracting a set of 13 MLOps tools , a feature analysis and comparison of aspects that involve the whole machine learning pipeline , aspects that involve the model management and aspects that involve data management . The most of reviews examined in the state - of - the - art emphasize the need to bridge the gap between research and practice . The results of this research take into ac - count 84 tools that practitioners employ to automate and maintain ML systems . Additionally , we expanded the re - search to the current state of MLOps tools by examining their compatibilities , giving practitioners the opportunity to select the ideal set of solutions for employing MLOps in ML systems . 9 . Conclusion In this work , we performed a Multivocal Literature Re - view to classify the MLOps tools in the DevOps process , and to identify possible incompatibilities among tools . We extracted 84 tools from 254 primary studies ( 203 PS from the gray literature and 51 PS from the white literature ) . We ﬁnally proposed a graphical representation for the MLOps tools into the DevOps process , that will be useful to researchers and practitioners to have a quick overview on the existing MLOps tools . It is interesting to note that the vast majority of end - to - end MLOps plat - forms do not commonly enable to integrate other tools , while no incompatibilities were found among other tools . As a result , it is possible to combine all the tools from diﬀerent DevOps stages into a seamless pipeline . We are planning to extend this work conducting an industrial survey to investigate the most common pipelines adopted in industry and the perceived beneﬁts , issues , and usefulness of each tool . Acknowledgments This work is partially funded by the IndustryX project ( Business Finland ) , the MuFAno project from the Academy of Finland ( grant n . 349488 ) , the EME - LIOT project ( Italy , MUR - PRIN 2020 program , contract 2020W3A5FY ) and the Swiss National Science Foundation through SNF Projects No . PZ00P2 186090 . CRediT authorship contribution statement Sergio Moreschini : Conceptualization , Investiga - tion , Data Curation , Writing - Original Draft , Writing - Review & Editing , Visualization . Gilberto Recupito : Investigation , Data Curation , Writing - Review & Editing . Valentina Lenarduzzi : Writing - Original Draft , Writing - Review & Editing , Visualization . Fabio Palomba : Supervision Reviewing , Editing . David H¨astbacka : Conceptualization , Methodology , Supervision Reviewing , Editing , Funding Acquisition . Davide Taibi : Conceptualization , Methodology , Su - pervision Reviewing , Editing , Funding Acquisition . References [ 1 ] G . Recupito , F . Pecorelli , G . Catolino , S . Moreschini , D . Di Nucci , F . Palomba , D . A . Tamburri , A multivocal litera - ture review of mlops tools and features , Euromicro Conference on Software Engineering and Advanced Applications ( 2022 ) . [ 2 ] J . Carver , N . Juristo , M . T . Baldassarre , S . Vegas , Replica - tions of software engineering experiments , Empirical Software Engineering ( 2014 ) 267 – 276 . [ 3 ] L . Bass , I . Weber , L . Zhu , DevOps : A software architect’s per - spective , Addison - Wesley Professional , 2015 . [ 4 ] S . Moreschini , F . Lomio , D . H¨astbacka , D . Taibi , Mlops for evolvable ai intensive software systems , in : 2022 IEEE Interna - tional Conference on Software Analysis , Evolution and Reengi - neering ( SANER ) , pp . 1293 – 1294 . [ 5 ] V . Garousi , M . Felderer , M . V . M¨antyl¨a , Guidelines for includ - ing grey literature and conducting multivocal literature reviews in software engineering , Information and Software Technology 106 ( 2019 ) 101 – 121 . [ 6 ] C . Wohlin , Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering , in : Inter - national Conference on Evaluation and Assessment in Software Engineering , Ease ’14 , pp . 1 – 10 . [ 7 ] B . Kitchenham , S . Charters , Guidelines for performing System - atic Literature Reviews in Software Engineering , Technical Re - port Ebse 2007 - 001 , Keele University , 2007 . [ 8 ] B . Kitchenham , P . Brereton , A systematic review of systematic review process research in software engineering , Information & Software Technology 55 ( 2013 ) 2049 – 2075 . [ 9 ] C . Wohlin , P . Runeson , M . H¨ost , M . C . Ohlsson , B . Regnell , Experimentation in Software Engineering . , 2012 . [ 10 ] G . A . Center , Mlops : Continuous delivery and automation pipelines in machine learning , https : / / cloud . google . com / architecture / mlops - continuous - delivery - and - automation - pipelines - in - machine - learning # devops _ versus _ mlops , 2022 . [ 11 ] Docker , What is a container ? , https : / / www . docker . com / resources / what - container / , 2022 . [ 12 ] A . Felix , How to monitor jenkins metrics using prometheus & grafana ? , https : / / medium . com / @ AnnFelix / how - to - monitor - jenkins - metrics - using - prometheus - grafana - 152a98d6c7a6 , 2022 . [ 13 ] S . Idowu , D . Str¨uber , T . Berger , Asset management in ma - chine learning : State - of - research and state - of - practice , ACM Comput . Surv . 55 ( 2022 ) . [ 14 ] P . Ruf , M . Madan , C . Reich , D . Ould - Abdeslam , Demystifying mlops and presenting a recipe for the selection of open - source tools , Applied Sciences 11 ( 2021 ) . [ 15 ] A . B . Kolltveit , J . Li , Operationalizing machine learning models - a systematic literature review , in : 2022 IEEE / ACM 1st In - ternational Workshop on Software Engineering for Responsible Artiﬁcial Intelligence ( SE4RAI ) , pp . 1 – 8 . [ 16 ] M . Testi , M . Ballabio , E . Frontoni , G . Iannello , S . Moccia , P . Soda , G . Vessio , Mlops : A taxonomy and a methodology , IEEE Access 10 ( 2022 ) 63606 – 63618 . 10