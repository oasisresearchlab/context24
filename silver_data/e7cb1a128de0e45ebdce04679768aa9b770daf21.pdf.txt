112 ACM ECHT CONFERENCE Hypertext paradigm in the field of information retrieval : a neural approach . LELU Main ENST / Economic et Management 46 rue Barrault 75013 PARIS , FMNCE FRANCOIS Claire INSTITUTE L ‘INFOMTION SCIENTIFIQUE ET TECHNIQUE ( INISWCNR $ ) Dkpartement Recherche et Produits Nouveaux 2 allke du Pare de Brabois 54514 VANDOEWRE LESNANCY CEDLX FMNCE Abstract Application of the hypertext paradigm to information retrieval requires 1 ) an automatic generation of hypertext links , 2 ) a compact graphical representation of the data . After a brief review of the family of neural algorithms required for deriving a compact and relevant representation of a documentary database , as well as links between synthetic “topics” and documents , we present a user interfaee basedon these grounds . This representation is two - step : 1 ) a global topics map , 2 ) local topic axes , ranking both terms and documents according to the values of their “centrality index” . A prototype , running in a Macintosh environment and implementing a basic version of this browser , is then described and commented . Keywords Hypertext , information retrieval , graphic user interface , neural networks , cluster analysis . 1 Introduction : Permission to copy without fee all or part of this material is granted provided that copies are not made or distributed for direct commercial advantage , the ACM copyright no - tice and the title of the publication and its date appear , and notice is given that copying is by permission of the Association for Computing Machinery . To copy otherwise , or to republish , requires a fee and / or specific permission . ( Q1992 ACM O - 89791 - 547 - X / 92 / 0011 / 0112 / $ 1 . 50 What representation of a documentary database are we aiming at ? First , we will sketch the two major paradigms for designing user interfaces in our information retrieval field : - With the emery - amwer rnradigm , a dialog is supposed to take place between a user and a machine . A major research trend has consisted in providing the systemwith a representation of the user . But the essential point is : how can the user have a relevant representation of what & the system , and of what is ~ the system ? Without it , he cannot adapt his action to his interlocutor . Considered ftom this point of view , most of the proposed systems do not take the issueinto account ( DACHELET 90 ) . - An important research line in reeent years is related to the navigational miradimn ( DANIEL - VATONNE 90 ) : the metaphor is that of navigation in the database , with powerful orientation tools at hand - global and local maps , windowing system , synthetic dashboard , . . The concept of hypertext is clearly the solution to this problem of browsing through a documentary database ( LUCARELLA 90 ) . The problem then turns into creating hypertext links in a real - size database with thousands documents and terms . As direct editing is clearly impossible , we must set up an automatic process for deriving an overview , aswell asled views of the system . In our basic philosophy of hyperte ~ we do agree with several statements made by PINTADO and MILANO , NOVEMBER 30 - DECEMBER 4 , 1992 113 TSICHRITZIS ( 1990 ) : - The first req & ement for effective navigation is a notion of locality . - Navigation must be based on an afinity measure between objects . - The user must be provided with a 2D mapping in which hypergraph links do not appear explicitly ( a set of nodes and edges fail to convey navigation proximity ) . Our design options ditTer on several other points : - Taking into account the problems of full - scale information retrieval systemsled us to design a more compact hypertext representation , with two levels instead of one : 1 ) A global 2D map displaying clusters of objects ( “topics” ) , and not basic objects . 2 ) Local ID “maps” , i . e . axes ranking both objects and descriptors according to their salience order in the topic . These axes appear asterm or document list windows . Let us translate these requirements into data analysis terms , and let us stressthe type of representation we consider most adapted to the navigational paradigm from this point of view : - Our set of documents described by terms can be considered as a huge data tnatri ~ with many more zeroes than ones . Data - analysis people are familiar with the so - called “block - seriation” methods ( MARCOTORC ! HTNO 91 ) , for optimally grouping both documents and terms so as to isolate high density blocks in this matrix . - What we propose is a soft version of this all - or - nothing procedure : the purpose is to rank the rows and columns of the matrix so as to make document and term poles appear . Each pole corresponds to a local subset of highly inter - related rows and columns . In this way , the resequenced “matrix displays a higher proportion of ones in the comer corresponding to these row and column poles . It is important to notice that several resequencings‘canbe done on the same data se $ each one showing a different local density fwture . - For each overlapping subset , the resulting graphical representation is a pair of axes , ranking respectively the documents and the terms from the most typical , or central , to the least typical ( see figure 4 ) . This ranking is not arbitrary , and saggests an interpretatio ~ at least to an expert in the application field . This extraction of meaning is in no way a miracle : the type of concept isolated here is not a great universal of human thought . The method visualizes a set of association patterns between terms and documents in a particular database , each of these elements being characterized by centrality indices . The resuh is a number of fuzzy , overlapping subsets of documents and terms , or “topics” , usefal to users browsing through ~ database ; in data anrdysis terms , it is a mix of cluster analysis and factor analysis , where the factors are ~ , and obliaue . Notice that this representation generalizes the concept of “clumps” , introduced twenty five years ago in the field of itiormation retrieval ( SPARK - JONES , JACKSON 67 ) ; these authors made the point that cluster overlap is an essential feature for embedding context effects in the linguistic domain : one term or document may belong to severaf clusters , and then each cluster stresses a distinct context in which the meaning differs . Moreover , a global 2D mapping is an essential orientation tool to an end - user : as each topic is characterized with a weighted term profile , there are lots of techniques for visualizing it as a point in a global map , such as principal tomponent analysis , non - linear mapping , etc . This process results in a synthetic map displaying the relative positions of the topics’ labels ( seefigure 3 ) . How can we derive this representation from the data ? We have developedtwo neural models specific to our documentation problem ( LELU 90 , LELU 91a ) : . me “fi ~ k - m - ” is a vti ~ t of the well - known k - means clustering algorithm : it derives half - axes , or “axoids” maximizing a global inter - axes inertia criterion , instead of deriving cluster centrokls maximizing the inter - class inertia . One can sort the cluster’s describers and documents along one of these half - axes as well as project the other terms and documents onto it . In this way , one can derive a “fhzzy interpretation” of the resulting axes , though the method is a strict clustering technique . This method is fwt and can handle very large amounts of data . It is formally related to neural models with unsupervised “winner takes all” learning ( LELU 88 ) . - In contrast with the previous meth @ the “local 114 ACM ECHT CONFERENCE component analysis” achieves an absolute optimization process , leading to better results : given a value of a “coarseness” parameter , one can define the value of a partial inertia inde ~ or density measure , for each axis passing through the origin of the data space . In this way , a “density landscape” is implicitly drawn , whose peaks identify the salient topics in the database . How to reach these peaks ? Our main approach has been to develop a neural algorithm with a hill - climbing learning law : starting from a random initial value , each “synaptic weights vector” eonvergesto a local maximw i . e . a topic , after a number of passes over the data . Heuristics make it possible to avoid missing any important peak . 2 Description of our neural models The “axial k - means” algorithm : The popular “K - Means” clustering algorithm ( MAC QUEEN 67 ) can be interpreted as a very simple neural model : - K neurons are initialized at random ; their weight vectors m ( k ) are meant to represent the gravity center of each cluster in the space of the I descriptors . - the distances between each data protlIe x and the K weight profiles m ( k ) are computed and compared , in order to determine the closest weight profile . This is equivalent to looking for the maximal dot produet q between the tsvo “augmented” veetors ( lln @ ) 112 / 2MO ( 1 ~ 2 & ‘2rn ~ J ~ ] : ad 7 - this “Winner Takes ~ 1” prhure determines the only weight ve ~ or to be ‘updated ; position of the gravity center of the cluster aeeunmlated t - 1 elements , is computed : ret ( k ) = mt . l ( k ) + ( l / t ) ( x - mt . l ( k ) ) the new k having This formula can be interpreted as a simple learning rule . After a few passes over the data set , the weight veetors eonverge and the cluster contents become stable . This algorithm served for deriving our “axial K - Means” model ( LELU 89 ) ; the only difference is that the output q is the dot produet < x , m ( k ) > in the original descriptor space , not in the “augmented” space . In this casethe objective function is : Maxz X < ~ m ( k ) > 2 k = l , K xechter k instead of : Minx X d2 ( x , m ( k ) ) k = l , K xdu $ ter k This means : - clusters are separated by a tesselation of hyperpkmes passing through the origin O , - each cluster is represented by a half - axis starting from O ; one em sort the cluster’s describers and described objects along this half - axis as well as project the other describers and described objects onto it . In this way , one can derive a “fuzzy interpretation” of the resulting axes , though the method usedis a strict clustering technique . Our simulations on real size documentary data show that these hvo consequences are in no way a handicap when analyzing “pick - any” data - on the contrary , they proved advantageous . “Pick - any” is the name psychologists gave to the situation where a person is asked to deseribe an objeet by a few items chosen in a very long list of elements ( say 1000 to 10 000 ) ; in a machine environment a similar type of problem is encountered during analysis of textual da @ task allocation problems in production systems , consumer preferences . . . The solution lies in giving importance to angles , rather than to distances , i . e . in normalizing the data vectors ; the dot produet turns then into the “cosine distance” similarity index ( SALTON 83 ) . As the preseneeof an item does not necessarily involve the absenee of others , the analysis must display homogeneous groups in which objeets appear together with their features ( “dimensions” of their representation vector space ) . However , as demonstrated for the K - Means , this algorithm is suboptimal and converges towards heal optima . The results depend on the initial conditions . In order to determine stable and reliable , clusters , one way of doing is to let the algorithm eonverge under a number of these conditions , and then determine the common core of clusters common to all the experiences - the “strong patterns” as stated in ( IXDAY 79 ) . An alternative is to find a procedure eonverging towards an absolute optimum ; this introduces our next model . The “local component analysis” algorithm ( LCA ) : When clustering , it is necessary to detect areas of relatively high density in the data cloud . The MILANO , NOVEMBER 30 - DECEMBER 4 . 1992 115 “structuring function” concept ( EMFTOZ 81 ) is more precise than the “density” one . It enablesone to define in any point of the data space a scalar representative of this density . In addition a structuring function depends on a “coarseness” - or “level of perception” ) - parameter varying between two bounds : at one end , one finds as many clusters as points to be clustered ; at the other end , one finds a single cluster . Our model for an individual neuron is asfollows : - transfer function : q’t = @ { qt ( 1 - cot @ O WwO ) } where 00 is the parameter of the structuring fiumt . ion ( O < Oo < n ) , ; xt is the t - ieth data vector and mt is the weight vector at “time” t ; qt = < m ~ xt > ; f + : X - > ! R : P ( X ) = X ifx > o = O otherwise The derivation of q’ from q is shown in Figure I ; it is a kind of “thresholding” based upon the intersection with a cone characterized as folllows : - apex : O ; - axis : mt ; - angle : 00 - learning rule : mt + l = mt + ~ q’t ( xt - qtx ~ ( l + cotgf30tg ( mt , ) xt ) ) where a is a small positive constant This “gradient - ascent” rule follows from the maximization of the objective fimction : Q = x ? l’tz t = l , T The set of all local maxima constitutes an absolute QPI @ M . L given a v ~ ue of the p ~ ameter eo . when one disposes of at least as many neurcms as maxima , and if one supposesthat each neuron must “climb” up a distinct maximum , then the model provides an “axial” clustering procedure which is optimal in an absolute sense . Such a procedure takes us one stepfurther than our previous “axial K - Means” . The delicate question in this approach is : not to “forget” any local maximum . The problem is not too difficult to resolve with a small data set - one has just to chooseas many neurons as data vectors , and then initialize the weight vectors with the data . However the problem is more comp ~ icakd with large data sets . We are currently testing different strategies . For example , starting with 50 neurons and 60 = 90° , we made about a hundred passesover a documentary data table of 39OO * 500 and converged towards a partial set of a dozen half - axes , easier to interpret than in our previous tests ; then we carried out another analysis with the subsetof documentary elements whose q’ was O , and fisionned the new neurons with the previous ones , and so on . . . [ GEORGEL 92 ) . P - - - - - - - - - - - - - - - - r - - - - - - - - - - - - - - 9’ # l . 1 - . . - . - - - . - ” - - - - i Figure 1 : derivation of the “truncated projection” q’ We have proved that there are as many maxima as dtierent & ta vectors when ( 3 . - > O , and a single when 00 - > 180° ( last principal component ) . 3 Design principles for a “neural browser” . In this section , we will present our ideal view databasebrowser built on the previous concepts . Global map : one of a The user is provided with a global map . Each topic is visualized as a circle with variable diameter ; its surface accounts for the importance of the topic , proportionally to the number of documents in the cluster ( axial k - means ) or to the partial inertia index ( local component analysis ) . A label is attached to each topic . This label is generally the most salient term in the topic’s li ~ but the advice of an expe ~ is needed for validation . If the term is inadequate , the expert may choose another term in the list , or may suggest a new term 116 ACM ECHT CONFERENCE more appropriate as regard to the documents’ descriptions and to the term list . How to use the global map ? To each document is attached a set of values : its projections onto the thematic axes . Thus any document subset may be characterized by the projections of its centroid : in this way it is possible to visualize in the global map the position of the documents corresponding to a term , a boolean expression of terms , or an author , a source title , an tilliation , etc . The selection of a term , author , . . . may take place in a window displaying , on user’s demand , the exhaustive list of terms , authors , . . . . or enabling direct typing . Each selection ( “double - clicking” ) highlights one or severhl topics in the global map : the larger the projection , the darker the topic’s icon . On user’s request , this process emphasizes the relevant contexts for a given term , boolean expression , author , sourcetitle , rdliliation , etc . Local topics . When the user selects ( “clicks” ) a given topic , optional windows then display ranked lists of documents , terms , authors , source titles , affiliations , etc , attached to this topic . Selecting again an element of these lists yields further information about this element : - detailed referencesof a document , - for a term , an author , etc : number of indexed , documents , ’ creation dates of the first and last indexed documents , etc . Navigation process . The user initializes the process with a query , which may highlight - or not - a few elements of the global dashboard ; or he may directly explore a topic : if some term or reference in the topic seems particularly relevant to him , he may select it for further information , and consider the other contexts highlighted by this selection in the global map . If one of these contexts looks more relevant to him he would explore it , and so on recursively . This processallows the user to : visualize the documents attached to a given term , or closely related to a given documen ~ in a relevant context ( and not in the whole database , as zooming or query expansion techniques do ( 131ENNER90 ) ) . Note that some of these documents may have no common term with the initial query document ! The query expansion is contextual , and this property is particularly useful for terms with multiple meanings . - determine the authors close to a given author in a given context , - discover which laboratories are involved in a given researchtopic , - be aware of which term variants and alternatives have been used by the indexers of the databasefor a given concept or research line . Instead of building this whole interface in our sophisticated workstation environment we chose a more progressive and pragmatic approach : - We decided to implement in a first phase a basic interface version , applied to limited collections of documents and running in a Macintosh Hypercard @ environment , so as to circulate stacks easily among potential end - users , and collect feed - back advice on an extensive scale . End - users then evaluate the product in their daily working environment , and browse through data in their own expertise field . - In a later step , we will take into account qualitative suggestions ( e . g . additional requirements ) , and quantitative improvements ( e . g . external modules in order to get rid of Hypercard implementation constraints - such as : no more than 25 documents per topic card - , or improve responsetime ) . 4 Basic version for a standard Macintosh @ environment . The NEURODOC project We have designed a prototype user interface in the framework of the INIST project “NEURODOC : new documentation profiles” . It associates in the samepackage a navigational graphical interface and documentary data , providing the end - user with a self - sufficient hypertext browser . In practice , this prototype is aimed at perfecting the so - called “documentation profile” concept ( homogeneoussubsetsof Pascal or Francis databases marketed by INIST ) , resulting in a Hypercard stack . MILANO , NOVEMBER 30 - DECEMBER 4 , 1992 117 “ & & j ) Intellectual lea $ l ? ~ 5 [ TOPICS MAP Softwaxe ( 25 ) T Expext sydem owledge ( 25 ) Iangua ~ @ ~ g 25 ) MamoYy ( 25 ) ‘x’i’i’iav * i : R @ 2’ ) Int ~ el ~ ~ @ ~ $ 22 ) Theory ( 16 ) Ihwvledgebase 25 ) Nmvexbal intelligence & i gical pxogramming ( 1 ) Neural network ( 25 ) ( 25 ) Figure 3 : Example of a global topics map , summarizing 529 documents in the field of Cognitive Sciences . Numbers in brackets indicate the size of the documents non - overlapping clusters ( up to 25 documents , due to implementation constraints of the prototype ) . EECI I Keyt . uords list 6 . 4Z Language Q 3 . 67 Sewantics 3 . 66 Sentencecomprehension ! # J 2 . 71 Languagedevelopment * ! 2 . 65 Linguistic production ~ : * 2 . 59 Syntex : : . : . : . ~ $ ; ~ 2 . 58 Child : : : : * : w . ? : ; 2 . 32 Reading : : . % v , . ! , 2 . 19 Nerrative ; % : : : : : : : q ; : : : 2 - 07 Time : . . . , . . : : ; : : : : 2 . 02 Languagedisorder ‘o Documents list 0 . 79 Cognitive skills associatedwith the onset ~ of multi word utterances 0 . 79 Lenguegeendcognition in two children : : : : : : : ; ~ with Williams syndrome : * Z # # # 0 . 66 A piecedquilt : a critical discussion of ! ! ! % ! StephenSchiffer”s Remnantsof Meaning ~ . y : : ~ . ~ 0 . 60 Onthe threshold of the story realm : . : + : . ~ : ) j : ? : : $ . - . semanticversus pragmatic u3eof connective i n U : : : : : : ; f : ; : narratives : { . : : : . : . : : : : y : ; 0 . 55 Origins of Georgel ( ell ~ ’s constructivism @ in the work of Korzubski andMoreno WI f \ Other topics : OQ I Topics map I Figure 4 : Example of a topic card , displaying the keywords window and the documents window . The akcimal numbers indicate the coordinates of keywords and documents along the topic axis . 118 ACM ECHT CONFERENCE Using this hypertext generator allowed us to avoid heavy developments , while guaranteeing the basic ergonomic quality of the product . Moreover , its commercial or non - commercial distribution will be helped by using the standard configurations of the Hypercard software as provided with Macintosh computers . This project resulted in the design of a production environment for semi - automatic transformation of subsets of INIST’S databases into hyperdocuments . Being evolut . ive and extensible , it makes it possible to develop other product lines . Description of the prototype . The NEURODOC “engine” module is designed as a strict production tool . It derives a set of topics out of a documentation corpus , as described in section 2 above . The topics consist in a special type of clusters : . these clusters are overlapping , as a document or a keyword may belong to several clusters at once . - the elements ( documents and keywords ) in each cluster are ranked according to their degree of likeness relative to the “ideal - type” of their class . This module rqns both in a MS DOS environment , or in a Unix environment when big data collections have to be processed . The Neurodoc browser module is at the end - tiser’s disposal ; it consists in a HyperCard stack . The upper card is the global map ( see figure 3 ) showing the relative positions of the topics . This map is built through a Principal Component Analysis of the topics’ profiles in the keywords vector space ( layout problems are solved with a standard business graphic software ) . Each topic is representedas an axis along which documents and keywords are grouped and ranked . By selecting a topic , the user accessesits content . A topic consists of the lists of keywords and documents with which it is associated . Keywords and documents are ranked according to the value of their projections on the topic axis . Ranked lists of keywords and documents are simultaneously displayed on the screen ( seefigure 4 ) . Clicking a document title allows one to accessthe complete reference of the document . hi a similar way , each keyword leads to the documents it indexes in the topic . These navigation capabilities of our prototype are summedup in figure 2 : 1 n ~ Global topics ~ Documents’ map titles of a topic I I u 1 W ‘ - - ’”” - - - - - - 1 q - llD ~ ~ ~ I topic Fig 2 : Main navigational paths in the hyperdocument . In addition , clicking a document title or a keyword enables one to locate the topics in which this item may be important , if its projection is greater than a parametrized threshold . These topic labels , if any , are underlined in the global map . Therefore , documents and keywords related to several thematic domains allow direct browsing among thesetopics . Diagrams 3 and 4 show an example of hyperdocument derived from a profile actually marketed by INIST , consisting of 529 document references in the domain “Cogrdtive sciences” domain . The analysis was based on the English indexing vocabulary . The stack is made of three dit3erent types of cards : - a global map of the topics ( figure 3 ) ; - topic cards displaying the sorted lists of keywords and document titles associated with each to ~ ic , as well as the number of associated documents ( figure 4 ) ; - the documents , with their title , authors’names and aililiations , source , keywords and abstract . Hyperdocument building process The whole process of building an open range of “elaborated documents” , starting from raw data , is presentedin figure 5 . In order to build a hyperdocument based on a MILANO , NOVEMBER 30 - DECEMBER 4 , 1992 119 documentation corpus , we defined two requirements - processing has to be asautomatic aspossible ; - a maximum adaptability is reqniredj in orfier to conveniently analyze the raw data and make the building processevolve easily , Queries to the PASCAL and FRANCIS databases Conversion of the documents from initial format to SGML . Building inverted ffles ~ + # . k“ Linguistic processing Hyperdocuments , Typesetting , Desktop publishing . I ! Fig 5 : Building an open range of “elaborated documents” . The dashed lines show the data flow . As pointed out in ( DANIEL - VATONNB 9’0 ) , a hypertext structure may be considered as a network which takes into account the logical structure of documents . To describe any entity at any level of our hyperdocument building process , we based our work on a standard for document description : the SGML standard ( Standard Generalized Mark - up Language ) . This standard makes it possible to describe in an exhaustive way the logical structure of documents from different origins . A set of homogeneous tools has been defined for processing a specific type of documents based on this standard . These tools are organized in the “Ilib” Unix library of itiormation retrievat software fimctions ( DUCLOY et al i99 1 ) . The first step for processing a documentation corpus consists in converting the raw documents into a format consistent with the SGML standard . The documents are then stored in a hierarchkrd file system providing a direct access to the data . The documentsare complementedwith indexes in a format alsoin accordancewith the SGML standard , An index of French and English keywords is usually set up . These elements constitute the documentation corpus , ready for statistical processing , Keyword indexes implicitly define the virtual sparse matrix ( documents * descriptors ) implemented as a list of lists . The axial K - means algorithm is then applied to this matrix . The resulting files are then used to build an intermediary file describing the topics issued from the matrix . Its structure is defined in accordance with the SGML standard . It is then turned into a hypertext or a printed document . Most of the hyperdocument building process , starting from the set of raw documents , is done on a Unix machine . The target machine ( a Macintosh computer ) is oidy used to turn the intermediary file into a hyperdocument . By using the SGML standard , we can build an evolutive and extensible working environment . We can easily modfi the statistkxd processing module , We can also include a linguistic processing module in order to derive document indexing from fidl - text titles or abstracts . Moreover , using a standardized intermediary format allows us to easily derive a hyperdocument for other hypertext software . In this way , we have implemented the first prototype of a hyperdocument browser using the FOLIO @ software in a PC environment . A semi - automatic process In the building of a hyperdocument , three stages need human expertise : analysis of the documentation corpus , determination of the clustering parameters , and validation or renaming of 120 ACM ECHT CONFERENCE the topics’ names . As shown by APPEL ( 1991 ) , elimination of both very high frequency and very low frequency terms improve the relevance of the topics ; hence both upper and lower bounds are to be fixed . The expert also has to fix the parameter needed for the axial K - means analysis ( maximum number of expected topics ) . Other implementation - dependant parameters are needed : - the thresholds defining the projection values above which a document or descriptor is included in a fuzzy cluster ; - the limit size of a topic . In order to help in setting these parameters , different hyperdocument assessments based on structurally identical documentation profiles are currently under selection . This study will make the automation of the process more complete . A simple heuristic has been adopted for naming the cliiXerent topics issued from the process . The first descriptor in the list of keywords in descending relevance order is proposed as a name for the corresponding topic . Our tests showed that 50 % of the names obtained this way were relevant . The others have to be modified by the expert . An expert’s version of the browser incorporates a “Topic name” button in each topic card . Users’ comments . Though no systematic evaluation has been carried out yet , this interface prototype has been tested by potential userswho gave the following comments : - The fmtures of the documentation corpus have to be extensively described , including , for instance , the query expression used to derive the corpus horn the data base , the document types , and the publication dates of the papers . - The reading of the topics map requires a specific help application . It seems that the way of getting answers to the user’s queries suggestedin Section 3 could improve the ergonomic qualities of the browser fkom this point of view , - Researchers also used this prototype to list the names of their colleagues in their research field . It seems necessary to allow for computing and displaying the main authors’ names and affiliations in each topic . Of course , this has to be implemented without altering the mvigation facilities of the browser . CONCLUSION A novel prototype of “neural browser” for information retrieval has been presented . It relies on two unsupervised neural models for extracting relevant fuzzy clusters from the data , in other words for iden @ ing “data poles” and their related environment . CONKLIN ( 87 ) has presented three prominent components in a hypertext system : - a textual database , - a semantic net which comects the text components , - and tools for creating this combination of text and semantic net , and for browsing through it . Our SGML description of the topics issued fiom”the neural processing embeds the semantic net . The prototype interface results in three kinds of Hypercard “cards” : the global map , the description of a topic , and the description of a document . Links ( anchored in “card button” and “card field” ) are associatedto these nodes and allow for the browsing process . The software environment developed in the framework of our NEURODOC project makes the evolution of the prototype quite easy : the statistical model may be changed ( e . g . turning from axial k - means to local component analysis ) , or the interface features may evolve into the direction sketched in Section 3 . This flexibility makes it possible to take into account periodic evaluations from end - users . ACKNOWLEDGEMENT This research was supported by the French Ministry of Science and Technology ( MRT / DIST / SERICS ) , asa part of the “Interfaces intelligent” program . Macintosh @ and HyperCard @ are trademarks of Apple Computer Inc . Folio @ is a trademark of Folio Corp . MS DOS @ is a trademark of Microsoft Inc . MILANO , NOVEMBER 30 - DECEMBER 4 , 1992 121 REFERENCES APPEL N . 1991 - “Transformation et a . nalyse de profils docmnentaires en hyperdocmnents . ” Internal report , INIST - CNRS / DPIC - INPL . BIENNER F . , GUIVARCH M . , PINON J . M . 1990 - “Browsing in Hyperdocuments with the Assistance of a Neural Network” - proc . of ECHT’90 , N . Streitz , A , Rizk , A . An & & eds . , Cambridge University Press , pp . 228 - 297 CONKLIN J . 1987 - “Hypertext : an introduction and survey” , Computer , VO1 . 20 , no . 9 , pp . 17 - 41 DACHELET R . 1990 - “Etat de l’art de la recherche en informatique docurnentaire : la reprdsentaticmdes documents et l’accds ~ l’information” - Rewarch paper N“1201 , INR14 Rocquencourt 1990 DANIEL - VATONNE MC . 1990 - “Hypertextes : des principes communs et des variations” , Technique et Science Informatique , special issue : Les hypertextes , vol 9 , no . 6 , pp . 475 - 492 , AFCET , Paris DIDAY E . et CO1l . - “Optimisation en classification automatique” - INRIA , Rocquencourt , 1979 DUCLOY J . , CHARPENTIER P . , FRANCOIS C . , GRIVEL L . 1991 - “Une boite h outils pc ~ ur Ie traitement de l % formation Scientifique et Technique . ” Genie Iogiciel et ~ st & nes experts , N“ 25 , pp 80 - 90 , paris , 1991 . E ~ OZ H . , HASHOM A . , TERRENOIRE M . - “Fonctions structurantes et classification automatique” - proceedings of the 3rd conference Reconnaissance des formes et intelligence art @ cielle - J . P . Haton cd . , AFCET , paris , 1981 , pp . 449 - 457 FUTURA R , PLAISANT C . , SCHNEIDERMAN B . 1989 - “A spectrum of automatic hypertext constructions” , Hypermedia vol . 1 , N“2 , pp . 179 - 195 Georgel A . 1992 - “Classification statistique et rc $ seauxde neurones formels pour la representation des banques de donn4es documentaires” - Ph . D . dissertation , Universit4 Paris 7 LELU . A . 1989 - “A neural model for highly dimensional data analysis” - Data Analysis , Learning Symbolic and Numeric Knowledge - E . Diday Ed . , Nova Science Publishers , New York , 1989 A . LELU , A . GEORGEL1990 - “Neural models for orthogonal and oblique factor analyses : Towards dynamic data analysis of large sets of highly multidimensional objects” - Proc . of INNC90 ( Paris ) , pp . 829 - 832 , Kluwer , Dordrecht LELU A . 1991a - “Automatic generation of hypertext links in information retrieval systems : a stochastic and an incremental algorithm” - ACMNIGIR’91 , special issue of the SIGIR Forum , A . Bookstein cd . , ACM Press , New York , 1991 LELU A . 1991b - “From data anrdysis to neural networks : new prospects for efficient browsing through databases” - Journal ofInformation Science , vol . 17 , 1991 , pp . 1 - 12 , Elsevier cd . , London LUCARELLA D . 1990 - “A model for hypertext - based information retrieval” - proc . of ECHT’90 , N . Streitz , A . Rizk , A . Andr4 eds . , Cambridge University Press , pp . 82 - 94 MAC QUEEN - “Some Methods for Classification and Analysis of Multivariate Observations” - Proc . 5th Berkeley Symp . Math . Stat . Proba . , 1967 MARCOTORCHINO F . 1991 - “Seriation problems : an overview” , Applied Stochastic Models and Data Analysis , VOL7 , 2 , pp . 139 - 152 NUGENT WR 1992 - “Library of Congress enters Cyberspace : the National Demonstration Laboratory” - Information Retrieval and Libra ~ Automation , VO1 . 27 , NOl1 , pp . 1 - 11 PINTADO X . , T $ ICHRITZIS D . 1990 - “SaTellite : Hypermedia Navigation by Atlinity” - proc . of ECHT’90 , pp . 274 - 287 , N . Streitz , A . Rizk , A . AMM eds . , Cambridge University Press SALTON G . 1983 - Introduction to Modern Information Retrieval - Mac Graw Hill , New - York , 1983 SPARK - JONES K . , JACKSON D . M . 1967 - “Current approaches to classification and chunp - finding at the Cambridge Language Research Unit” - Computer Journal vol . 10 , 1967 , pp . 29 - 37 STANDARD 1S0 88791986 - Information processing - Text and office systems - Standard Generalized Markup Language ( SGML ) , 155p .