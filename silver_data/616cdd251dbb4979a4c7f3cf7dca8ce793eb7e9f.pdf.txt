Designing with Language : Wireframing UI Design Intent with Generative Large Language Models SIDONG FENG , Monash University , Australia MINGYUE YUAN , University of New South Wales , Australia JIESHAN CHEN , CSIRO‚Äôs Data61 , Australia ZHENCHANG XING , CSIRO‚Äôs Data61 & Australian National University , Australia CHUNYANG CHEN , Monash University , Australia Fig . 1 . The overview of WireGen to streamline UI design process with automated mid - fidelity wireframe generation . We leverage the power of the generative Large Language Models ( LLMs ) , fine - tuning it with thousands of UI data to endow its understanding to UI design knowledge . With this fine - tuned LLMs , designers can simply describe their design intent , and WireGen will generate creative mid - fidelity wireframes with ease . Wireframing is a critical step in the UI design process . Mid - fidelity wireframes offer more impactful and engaging visuals compared to low - fidelity versions . However , their creation can be time - consuming and labor - intensive , requiring the addition of actual content and semantic icons . In this paper , we introduce a novel solution WireGen , to automatically generate mid - fidelity wireframes with just a brief design intent description using the generative Large Language Models ( LLMs ) . Our experiments demonstrate the effectiveness of WireGen in producing 77 . 5 % significantly better wireframes , outperforming two widely - used in - context learning baselines . A user study with 5 designers further validates its real - world usefulness , highlighting its potential value to enhance UI design process . CCS Concepts : ‚Ä¢ Human - centered computing ‚Üí Human computer interaction ( HCI ) . Additional Key Words and Phrases : UI wireframe , large language model , fine - tune 1 INTRODUCTION User Interface ( UI ) plays a crucial role in today‚Äôs desktop software , mobile applications , and online websites . It serves as a visual bridge between a software application and end - users through which they can interact with each other [ 30 , 33 ] . Good UI designs are essential to the success of a software application and can gain loyalty from the software users [ 19 ] . However , designing a good UI can be challenging , even for experienced designers . On the one hand , designers need to strive for creative ideas and follow many design rules and principles , such as fluent interactivity , universal usability , clear readability , aesthetic appearance , and consistent styles [ 24 , 35 ] . On the other hand , designers need to rapidly sketch the designs to validate the prototypes , solicit higher - level user feedback , and figure out flaws in the early stage [ 18 ] . For this purpose , one common and effective way is wireframing the design intent . 1 a r X i v : 2312 . 07755v1 [ c s . H C ] 12 D ec 2023 Feng et al . UI wireframe is a basic representation of design intent with a rough sketch , intentionally devoid of colors , graphics , and stylized fonts ( see Fig . 2 ) . This is a quick and cheap way to provide a clear overview of structure , functionality , layout , information flow , and possible user behavior when interacting with the app [ 49 ] . Typically , designers start from low - fidelity wireframes , the initial visual representation of their ideas with simple layouts and placeholder elements . Landay [ 48 ] introduces the first interactive tool SILK for low - fidelity UI wireframing . Huang et al . [ 42 ] propose Swire , a system that leverages a deep - learning model to search for similar UI screens from a low - fidelity wireframe , to help designers gain inspiration . Many studies [ 15 , 25 , 51 , 63 , 68 , 69 ] have attempted to develop intelligence tools to support low - fidelity wireframes . However , low - fidelity wireframes can sometimes be too rudimentary to provide an accurate representation of the final product and may need to be modified to account for real - world constraints [ 76 ] . For example , if an original low - fidelity wireframe envisions images and text arranged side by side , but the actual image is too large or the text is too lengthy , it would then need to be altered to a top - to - bottom layout . To address this , designers refine low - fidelity wireframes into mid - fidelity wireframes by incorporating more details , such as relevant written content , interactive semantic icons , and other elements . With higher levels of detail , mid - fidelity wireframes allow for more authentic and complex interactions to be explored and can lead to a better final UI , but also require more time and effort to create . None of the previous works have focused on streamlining the design process for mid - fidelity wireframes . In this study , we introduce WireGen , a novel solution to expedite the UI design process by generating mid - fidelity wireframes from high - level design intent descriptions . To achieve this , we utilize the stunning generative Large Language Models ( LLMs ) that inherit billions of web resources , such as web DOMs , semantic relationship , etc . Since the LLMs are not specifically designed to generate UI wireframes , we fine - tune the LLMs with thousands of UI screens and their corresponding view hierarchies from Rico [ 25 ] to help recognize the patterns of the UIs . With this fine - tuned LLMs , we prompt it in the same way to create innovative wireframes for the test design intent descriptions . However , UI wireframe design is not a straightforward task , it is subject to design rules , guidelines , and knowledge to match the psychology of human perception , which machines may not be aware of . To ensure the generated wireframes meet these standards , we further employ post - processing methods to transform raw generations to beautiful UI wireframes , including adding semantic icons , refining text typography , and adhering to UI guidelines . To evaluate the performance of our LLMs WireGen , we first conduct experiments under the specific UI textual descriptions from Screen2Words [ 75 ] . The results show that our fine - tuned WireGen achieves the best performance ( 77 . 5 % significantly better generations ) compared with two widely - used large language in - context learning baselines . Our WireGen also generates on average 84 . 5 % significantly better UI wireframes , which outperforms three ablation models . As there are many ways to describe a design intent in different words , we further conduct a user study with five professional designers to gain insight into the usefulness of our WireGen . The study reveals that designers respond positively to the mid - fidelity wireframes generated by our tool and provide valuable feedback . Lastly , we discuss the limitations and the implications of WireGen . Altogether , our paper makes the following contributions : ‚Ä¢ To the best of our knowledge , this is the first study that investigates the collaboration between humans and AI for creating mid - fidelity wireframes . ‚Ä¢ We present WireGen , that harnesses the power of Large Language Models ( LLMs ) to generate mid - fidelity wireframes from a simple description of the design intention . ‚Ä¢ The experiments and user study demonstrate the effectiveness and usefulness of WireGen in aiding designers with UI design . 2 Wireframing UI Design Intent with Generative Large Language Models Fig . 2 . Examples of a low - fidelity wireframe , mid - fidelity UI wireframe , and final UI . 2 RELATED WORK We aim to facilitate the design of mobile mid - fidelity UI wireframes by simply prompting the design descriptions using Large Language Models ( LLMs ) . To this end , we review the related work in three main areas : 1 ) mobile UI wireframe , 2 ) designing with natural language , and 3 ) prompting Large Language Models . 2 . 1 Mobile UI Wireframe Wireframing plays a pivotal role in many types of creative activities because of its highly visual nature and its flexibility for creation and manipulation : designers can create and imagine any kind of visual content , and continuously revise it without being constrained by unnecessary details [ 36 , 49 , 79 ] . Subsequently , mobile UI wireframing is well - suited for the early stage of the design process to expand novel ideas , visualize abstract concepts , improve user experience , and rapidly compare alternatives [ 54 , 77 ] . SILK [ 48 ] is the first system that allows designers to create interactive UI prototypes by wireframing . With the essential of UI wireframe as a profession in the UI design process , many academic research [ 44 , 57 ] and commercial graphic tools including Adobe PhotoShop [ 2 ] , Sketch [ 10 ] , and Figma [ 6 ] are developed to allow designers to sketch the UI wireframes at multiple detail levels from low - fidelity to mid - fidelity and final UI ( as shown in Fig . 2 ) . Low - fidelity wireframes serve as the design‚Äôs starting point that tends to be fairly rough , created without any sense of scale , grid , or pixel - accuracy to represent a basic visual of the UI . Mid - fidelity wireframes further boast pixel - specific layouts and make the UIs realistic by including semantic icons and relevant written content , that allows for exploring and documenting complex concepts such as menu systems or interactive buttons . Recent advances in AI have explored human - AI co - creation research for UI wireframes [ 71 ] . In these systems , AI serves as humans‚Äô collaborators , that make recommendations based on designers‚Äô goals and intentions . For example , Swire [ 42 ] proposes a deep - learning model to input a low - fidelity UI wireframe and retrieve relevant UI examples from 3 Feng et al . large - scale datasets to help designers gain inspiration effectively . Lately , researchers conduct data - driven research to support more advanced design inspiration search [ 15 , 25 , 29 , 51 , 63 , 68 , 69 ] or layout generation [ 13 , 39 , 52 ] in complement to designers‚Äô agency and creativity . These works , however , still require the designers to expend time and effort designing and sketching a possible low - fidelity UI wireframe , then adding adequately relevant content to demonstrate mid - fidelity UI wireframes . Our work aims to simplify the process further : given a design intent description , we automatically generate mid - fidelity UI wireframes for designers to broaden their horizons and get inspiration . 2 . 2 Designing with Natural Language To identify business requirements and collaborate ideas for UI designs , instigating conversations with the stakeholders and users is crucial . As a subsequent effort , many studies are devoted to bridging natural language and UI screens . For example , Screen2Vec [ 53 ] uses a self - supervised approach to learn the representation of a UI screen from multi - modal information such as the textual content , visual design , hierarchy layout patterns , and app meta - data . Similarly , Screen2Words [ 75 ] proposes a screen summarization approach to describe the functionalities of the UI screenshots . Some research specifically generates semantically alt - text labels for UI elements [ 16 , 17 , 20 , 27 , 31 , 32 , 32 , 34 , 78 ] . These studies attempt to learn the latent representation of UIs to generate natural language understanding . On the contrary , our work focuses on interpreting natural language descriptions into UIs . The interest in involving natural language as a form of interaction for graphic designs has recently found success in text - to - image generative models . Variational autoencoder ( VAE ) [ 47 ] and generative adversarial network ( GAN ) [ 37 ] are frequently used in generating graphic designs . For example , Aoki et al . [ 12 ] introduce a GAN - based model EmoBalloon to generate emotional speech balloons in the chat UI . Recently , advances in LLMs [ 14 ] and diffusion models [ 41 ] have introduced methods that are remarkable at generating images based on text prompts . While state - of - the - art text - to - image works such as DALLE [ 65 ] , Stable Diffusion [ 67 ] , and MidJourney [ 8 ] show amazing performance in creating aesthetic designs , they still have risks in generating realistic designs [ 4 , 65 ] . First , they are great at drawing but horrible at spelling words , e . g . , prompting with ‚Äúan image with Twitter text‚Äù may generate ‚ÄúTtiter‚Äù , ‚ÄúTw . uTe : ‚Äù , or mostly unrecognizable text . Second , coherence in designs is often missing while human creations would never lack , e . g . , shape incoherences , lack of components composability , etc . These potential flaws make it difficult to apply to create UI wireframes with the finest details as demonstrated in Fig . 8 . As a substitution , Huang et al . [ 43 ] propose a UI Generator , that uses a deep - learning model to generate coordinates of UI elements from textual description . However , those coordinates can only be interpreted into the layout of UI , i . e . , a simple low - fidelity UI wireframe , which still requires designers to mentally imagine the actual contents in their heads . Our work expands their research by prompting generative LLMs , comprising billions of web layout and content understandings , to generate a UI - specific language , which can be parsed into a mid - fidelity wireframe , including similar - to - real content , semantic functionalities , etc . 2 . 3 Prompting Large Language Models Deep learning has introduced more opportunities for natural language understanding . The development of transformer - based deep neural language models such as BERT [ 26 ] has shown its potential value in many applications . For example , the aforementioned UI Generator [ 43 ] leverages a BERT model to embed text description to generate a sequence of UI elements‚Äô coordinates . Recent advancements in large language models ( LLMs ) , such as GPT [ 14 ] , LLaMA [ 72 ] , PaLM [ 22 ] , RoBERTa [ 56 ] , have led to boost performances on zero - shot , few - shot , and fine - tuned with handcrafted prompts compared to prior deep learning methods . Zero - shot prompts directly describe what ought to happen in a task , and few - shot prompts show the LLMs what pattern to follow by feeding it examples of desired inputs and outputs . While 4 Wireframing UI Design Intent with Generative Large Language Models Fig . 3 . The overview of WireGen that contains three phases : ( i ) Data Preparation phase ( Section 3 . 1 ) that collects dataset of UI wireframes and their associated high - level descriptions . ( ii ) Model Fine - tuning phase ( Section 3 . 2 ) that implements the the best practices of modern strategies to fine - tune the LLMs . ( iii ) Output Beautification phase ( Section 3 . 3 ) that transforms the raw output into visually appealing UI wireframes . zero - shot or few - shot prompts allow to prototype common tasks , their inherent limitations ( e . g . , lack of domain - specific understanding , limited input prompt length ) make them less capable of prototyping specific applications [ 55 , 60 , 81 ] . Therefore , LLMs fine - tuning , encoding lexical , syntactic , and semantic regularities of the domain - specific language , is then used to master specific task capabilities . For example , Codex [ 21 ] is a fine - tuned version of GPT - 3 , that can help developers with code generation . Many studies have applied fine - tuning to support different domain - specific tasks , such as InstructGPT [ 62 ] , FLAN - T5 [ 23 ] , math word solving [ 84 ] , etc . In this same line of research , we fine - tune the LLMs to support the domain - specific task of UI wireframing , providing insight into how natural language interaction can help designers . 3 APPROACH Given a natural language description of the design intent for a UI , we harness the power of generative LLMs to automatically create mid - fidelity wireframes to greatly help designers gain design inspiration and create prototypes for different use cases and contexts . The overview of our approach is shown in Fig . 3 , which is divided into three main phases : ( i ) Data Preparation phase that collects dataset of UI wireframes and their associated high - level descriptions ; ( ii ) Model Fine - tuning phase that implements the the best practices of modern strategies to fine - tune the LLMs to recognize the UI patterns ; ( iii ) Output Beautification phase that transforms the raw output into visually appealing UI wireframes that are interactive and consistent with design guidelines . 3 . 1 Prepare Dataset of UI Wireframes and Descriptions 3 . 1 . 1 Mid - fidelity Wireframes and Descriptions . While there are many low - fidelity wireframe datasets available [ 15 , 25 , 51 , 68 , 69 ] , none of the prior work studies on mid - fidelity wireframes . This is because mid - fidelity wireframes go beyond the shape , placeholders , and ‚Äúlorem ipsum‚Äù text of low - fidelity wireframes to include real content , semantic icons , interactive elements , etc . However , manual labeling of wireframes can be prohibitively expensive . To that end , 5 Feng et al . we propose a novel approach to automatically collect a set of representations of mid - fidelity wireframes . As shown in Fig . 2 , mid - fidelity wireframes are typically a simplified and monochromatic representation of the final UI . With this in mind , we propose to gather representations of mid - fidelity wireframes from existing UI screen datasets . We utilize one of the largest open - sourced UI datasets , Rico [ 25 ] . The Rico dataset contains 66k unique UI screens from more than 9 . 7k Android apps across 27 diverse app categories . The dataset provides a screenshot image and a view hierarchy of UI objects . Each object has a set of properties , including its resource id , type ( e . g . , Button , Image , Text , etc . ) , bounding box location on the screen , textual content ( if any ) , and various other properties such as clickability , scrollability , etc . These screens can serve as our mid - fidelity wireframes . Togatherthedescriptionsofthesescreens , weutilizea comprehensivescreensummarizationdataset Screen2Words [ 75 ] , that captures the complex information of UIs into concise language descriptions . The summarization is done by 85 professional labelers through a rigorous labeling process and guidelines , resulting in high consistency in both linguistic coherence and on - screen focus area among the labelers . As a result , we construct a dataset of pairs of UI screens and their corresponding textual descriptions . 3 . 1 . 2 UI - specific Language . One challenge with using LLMs is that they can only process text input 1 , while UI screens are multimodal , containing text , semantic icons , structural information , etc . To help LLMs inherently understand UI screens , we aim to convert the UI screens into domain - specific language that LLMs can understand , which is known as prompt engineering [ 66 ] . A well - designed prompt helps the LLMs elicit specific knowledge and abstractions needed to complete the task . Since the training samples of LLMs are typically scraped from the raw web page data , e . g . , GPT was trained on 410 billion tokens from the Common Crawl web corpus , we use HTML syntax as the domain - specific language to convert the UI screens into text . The closer the prompt is semantically similar to the LLMs‚Äô training samples , the better the inference will be . To translate the UI screen with view hierarchy into HTML syntax , we need to preserve the properties and structural relationship of UI elements . While the view hierarchy resembles a DOM tree in HTML , e . g . , it starts with a root view and contains UI elements descending in the tree , there are two fundamental limitations . First , the native classes in the view hierarchy don‚Äôt always match HTML tags , for instance , a < RadioButton > in the hierarchy corresponds to a combination of < input type = " radio " > and < label > . Second , including all properties of UI elements will result in excessively long HTML text that may exceed the maximum input token length of LLMs . Therefore , we adopt a similar approach to previous work [ 28 , 74 ] to convert the view hierarchy of the UI screen into HTML syntax with similar functionality . An example is shown in Fig . 4 . In detail , we first adopt a depth - first search traversal algorithm to iterate through each node starting from the root of the view hierarchy . During the iteration , we convert each node into HTML code and style sheet , based on a selected subset of properties from the view hierarchy . ‚Ä¢ resource _ id : describes the unique resource id of the element , depicting the referenced resource . ‚Ä¢ class : describes the native UI element type such as TextView and Button . ‚Ä¢ text : describes the text of the element ( if any ) . ‚Ä¢ content _ desc : conveys the content descriptions of the visual element such as ImageView and VideoView . ‚Ä¢ bounds : describes the positional information of the element such as top , left , width , and height . We develop a heuristic approach based on Table 1 to match the classes in the view hierarchy to HTML tags with equivalent functions . For instance , the < TextView > is mapped to the < p > tag ; buttons are mapped to the < button > tag ; 1 Since OpenAI did not release its multimodal API ( GPT - 4 ) before submission , it was difficult for us to measure the capability of LLMs for UI screen understanding of complex structures , semantic icons , etc . 6 Wireframing UI Design Intent with Generative Large Language Models Fig . 4 . Example of converting the UI screen to HTML code and style sheet . Table 1 . The class conversion between view hierarchy and HTML syntax . CLASS FUNCTIONALITY HTML CODE TextView display text content < p class = ùëñùëë > ùë°ùëíùë•ùë° < / p > Button , ToggleButton click to new events < button class = ùëñùëë > ùë°ùëíùë•ùë° < / button > ImageView , ImageButton display image < img class = ùëñùëë alt = ùëêùëúùëõùë°ùëíùëõùë° / > EditText input text < input class = ùëñùëë placeholder = ùë°ùëíùë•ùë° type = ‚Äútext‚Äù > CheckBox , Switch ( de ) select an option < input class = ùëñùëë type = ‚Äúcheckbox‚Äù > < label for = ùëñùëë > ùë°ùëíùë•ùë° < / label > RadioButton choose only one option < input class = ùëñùëë type = ‚Äúradio‚Äù > < label for = ùëñùëë > ùë°ùëíùë•ùë° < / label > DatePicker select a date < input class = ùëñùëë type = ‚Äúdate‚Äù value = ùë°ùëíùë•ùë° > Spinner select an option from drop - down menu < select class = ùëñùëë type = ‚Äúradio‚Äù > < / select > < label for = ùëñùëë > ùë°ùëíùë•ùë° < / label > VideoView display video < video class = ùëñùëë alt = ùëêùëúùëõùë°ùëíùëõùë° > < / video > Other less commonly - used classes < div class = ùëñùëë > < / div > and image elements to the < img > tag . Unlike the classes in the view hierarchy , HTML uses the combination of < input > and < label > for input - related elements ( e . g . , < EditText > , < CheckBox > , < RadioButton > , etc . ) . The < input > represents the specific class and < label > represents the text property . Note that we focus on the most commonly - used classes for simplicity , and the rest of the classes , including containers such as < LinearLayout > , are mapped to the < div > tag . Next , we insert properties into the HTML code following standard syntax . For instance , the unique identifier class for the objects in the HTML code is set using the resource _ id . Text properties are placed between the opening and closing HTML tags . Since the < EditText > usually depicts text in the placeholder , we replace it accordingly in Table 1 . For image - related objects , the alt - text is conveyed using the content _ desc property . To describe the style and layout of the UI , we generate a style sheet in addition to the HTML code . To precisely generate the layout , we use the bounds property in the view hierarchy to encode the absolute position of each atomic element , including the top , left , width , and height . An example of style sheet is shown in Fig . 4 . We avoid using relative 7 Feng et al . positioning , such as inline or margin , as it could limit the scope of adjacent elements [ 1 ] . Note that we also add the overall width and height of the UI screen in the style sheet . 3 . 2 Fine - tune Large Language Models with Best Practice Since the LLMs are not specifically designed to understand the UI design patterns , we fine - tune the LLMs with the natural language description as the input and the UI wireframe in HTML syntax as the output ( the same learning objective as the pre - trained model ) . The implementation of fine - tuning process is not a trivial task [ 50 ] , which can significantly influence the performance of LLMs . To that end , we summarize three highly sensitive aspects in fine - tuning LLMs , denoting training data selection , model selection , and hyperparameter configuration , and detail our implementation informed by the best practices . 3 . 2 . 1 Training data selection . When fine - tuning LLMs , the size of the training dataset plays a non - negligible role [ 59 ] . On the one hand , the data size needs to be sufficient , diverse , and representative to enable the LLMs to learn the characteristics of the specific task effectively . On the other hand , training on an excessive dataset can be time - consuming and costly 2 . To strike a balance between the LLMs‚Äô capability and the training cost , we attempt to select 1 , 000 samples from the dataset , suggested by the previous work [ 83 ] . Regarding the sample selection , a simple random selection cannot ensure the LLMs‚Äô generalizability and diversity , as the UIs in the same app may convert to very similar HTML syntax . To avoid this data leakage problem [ 46 ] , we select the screens in the dataset by the app . We also ensure the representation of app categories and screen summaries are diverse in the training dataset . In total , we collect 1 , 000 samples from 191 apps , covering 27 app categories , summarised by an average of 7 . 1 words . 3 . 2 . 2 Model selection . There are numerous emerging LLMs that have exhibit promising performance in natural language understanding and logical reasoning , such as PaLM [ 22 ] , RoBERTa [ 56 ] , T5 [ 64 ] , etc . In this work , we adopt the recent state - of - the - art LLM , GPT [ 14 ] from OpenAI with 175 billion parameters pre - trained on a massive dataset . It is based on the transformer model [ 73 ] including masked multi - self attention , normalization layers , and feed - forward layers ( see in Fig . 3 ) . GPT offers sets of models to support different levels of tasks , including Curie , Babbage , Ada , etc . For our study , we choose the model Turbo ( gpt - 3 . 5 - turbo ) which is well suited to our task due to two reasons . First , Turbo is the most advanced model and can perform tasks with less instruction compared to other models . It is especially ideal for tasks of creative content generation and extensive understanding of the content . Second , Turbo excels in solving logic problems and understanding the intent of code ( e . g . , HTML ) , and has been fine - tuned for programming applications like Codex . 3 . 2 . 3 Hyperparameter configuration . In addition to the choice of model , fine - tuning performance can also be improved through the customization of hyperparameters . According to the previous fine - tuning practices of LLMs [ 7 , 14 ] that shows promising performance across a range of use cases , we apply a learning rate of 0 . 1 and a batch size of 256 . We train the model for 4 epochs to enable the model to recognize the input prompt syntactic and usage patterns of the fine - tuning data . Furthermore , there are three additional hyperparameters that have been optimized for the LLMs , specifically GPT model : ‚Ä¢ Temperature : determines how much randomness is in the generation , regarding new content creation . At a temperature of 0 . 0 , the model will always produce the same fixed response to an input text , regardless of the 2 The pricing set by OpenAI https : / / openai . com / api / pricing / 8 Wireframing UI Design Intent with Generative Large Language Models Fig . 5 . An illustration of post - processing raw generation into better UI wirframes , adding semantic icons , refining text typography , and adhering to UI guidelines . number of generations . Raising the temperature value ( with a maximum of 1 . 0 ) enables more creative output from the pre - trained resources , but also increases the risk of generating irrelevant output . Based on previous studies [ 14 , 83 ] , we set the value to 0 . 65 to balance robustness and creativity . ‚Ä¢ Maximum _ length : sets the upper limit of the number of generated tokens . The default value is 256 , but generating HTML syntax often requires more . Therefore , we set the value to the maximum of 4 , 096 tokens to accommodate the requirement . ‚Ä¢ Stop _ sequence : when the generation of tokens stops . To control the endpoint of the generation in our task of HTML syntax generation , As we aim to generate HTML syntax , we set the delimiter value to the HTML closing tag , < / html > . 3 . 3 Transform Raw Generation into Beautiful UI Wireframe After fine - tuning process , we prompt the LLMs with a simple description of design intent to generate a HTML syntax of mid - fidelity wireframes . However , programming languages like HTML and CSS are not easy for designers to understand . We wish to hide the programming hardships under - the - hood to reduce the designer‚Äôs burden . Besides , the LLMs may not fully capture the intricacies of the design knowledge behind the UI designs . To this goal , we propose post - processing methods to transform the raw HTML syntax generation into intuitive , fluent , and interactive UI wireframes . An example of the post - processing methods is shown in Fig . 5 . 3 . 3 . 1 Semantic Icons . Different from the buttons with text that explicitly shows the functionalities , the icons fail to expose the textual semantics . To address this , we add the semantic icons in the wireframe by leveraging the alt text in the HTML syntax . First , we carry out a small study to understand the icons and their associated text semantics . Based on the Rico dataset , we extract a large number ( 73 , 449 ) of icons and randomly select 4 , 000 ( 5 % ) as our experimental set . To identify the set of frequently occurred icon semantics in the wireframe , we perform an iterative open coding of the experimental set using the existing expert lexicon of categories in books and websites such as Google‚Äôs Material icon set [ 38 ] and IBM‚Äôs Design Language of Iconography [ 45 ] . Two researchers from our team independently code the 9 Feng et al . Table 2 . The 10 most common icon semantics identified through an iterative open coding of Rico . ICON ASSOCIATED SEMANTICS EXAMPLES return , back , navigate up , previous , backwards , arrow back menu , navigation drawer , list , card , dashboard settings , toolbox , gear , preferences , options more , more options , dots , three , overflow information , info , help , support , question , ask , faq person , user , avatar , account , customer , profile close , quit , logout , exit , switch - off search , investigate , search - engine , magnifier , find , glass share , share button , forward , social media favourite , like , heart , upvote categories of these icons , recording any part of the initial vocabulary . Note that both researchers have design experience in UI wireframe design . After the initial coding , the researchers meet and discuss the discrepancies and the set of new semantic categories until a consensus is reached . A category of top 10 icon semantics can be seen in Table 2 . For example , the ‚Üê icon usually expresses the semantic of ‚Äúreturn‚Äù and the √ó icon usually expresses the semantic of ‚Äúclose‚Äù . Based on the icon semantic category , we prompt the LLMs to convert the alt - text description in the HTML syntax into the corresponding icons . In detail , we first provide the LLMs with the context of our icon semantic category with the prompt , i . e . , ‚ÄúHere is an icon semantic category : first icon can be assigned an alternative description of return , back , navigate up , previous , . . . ‚Äù . Next , we use a prompt to instruct the LLMs to identify each icon image with its alt - text attribute in the HTML syntax . For instance , in Fig . 5 - A , to generate the icon for the alt - text of ‚Äúmore options‚Äù at the top right corner , we prompt the LLMs by ‚ÄúPlease indicate the icon number if there is a corresponding icon for the alternative description of ‚Äúmore options‚Äù . If there is no related icon , please respond with no . ‚Äù In this case , the LLMs returns the ‚Äúfourth‚Äù icon in the category , which corresponds to the ‚Äúthree - dots‚Äù icon as shown in the Fig . 5 - A . Note that we do not perform LLMs fine - tuning for this task , as the icon identification is relatively straightforward , either select a single icon from the category or indicate no relevant icons . 3 . 3 . 2 Text Typography . The typography of text layout is defined as the process of overlaying texts onto the text blocks in the UI wireframe . However , the process poses potential challenges [ 11 , 80 ] on text wrapping , text alignment , font size , etc . We define text wrapping ùëÜ = { ùë† 1 , ùë† 2 , . . . , ùë† ùëõ } consists of 2 | ùë† | ‚àí 1 possible wrapping ways on string ùë† ; text alignment ùê¥ = { ‚Äúleft‚Äù , ‚Äúcenter‚Äù , ‚Äúright‚Äù } consists of the alignment ways of text in block ; ùêπ = { ‚Äúsmall‚Äù , ‚Äúmedium‚Äù , ‚Äúlarge‚Äù } to demonstrate the importance of the text and each font has a number of corresponding sizes ( i . e . , ‚Äúnormal‚Äù ‚àà { 10 , 11 , . . . } ) ; ùë§ and ‚Ñé denotes the width and height of the text block . To ensure the text typography aligns with human design principles , we formulate it as an optimization problem that minimizes the waste of spare block space and the mismatch of information importance in perception and semantics . ùëúùëùùë°ùëñùëöùëñùëßùëí ( s | w , h ) = ùëöùëéùë• ( ùëÜ ùëñ ‚àó ùêπ ùëó ùë§ ‚àó ‚Ñé ) ( 1 ) In detail , we first identify the text font by the class in the HTML syntax , that ùêπ corresponds to the title , normal text , and subtitle for ‚Äúsmall‚Äù , ‚Äúmedium‚Äù , ‚Äúlarge‚Äù , respectively . Then , we calculate the area of each combination of text wrapping 10 Wireframing UI Design Intent with Generative Large Language Models ùëÜ ùëñ and font size ùêπ ùëó . By minimizing the waste of empty space , we calculate the ratio occupied by the text on the text block and determine the combination with the largest ratio as the optimal text typography . Finally , we use optimized wrapping text to imply the text alignment ( ùëÜ ùëñ ‚áí ùê¥ ) , i . e . text is center - aligned if it is a single line , otherwise , left - aligned . For example , in Fig . 5 - B , the text is generated as the class of ‚Äútitle‚Äù , so we highlight the text with ‚Äúlarge‚Äù font . In Fig . 5 - C , the ‚Äúsubtitle‚Äù text is post - processed to ‚Äúsmall‚Äù font . 3 . 3 . 3 UI Guidelines . UI design is not a simple recipe , it is subject to design rules and guidelines to match the psychology of human perception , that machine may not aware of . To investigate the presence of design flaws in LLMs‚Äô generated UI wireframes , we conduct a small - scale study on randomly 100 generated UI wireframes . Note that this study just aims to provide an initial analysis of possible UI design flaws in the generation of LLMs , and a more comprehensive study would be needed to deeply understand it . Following the Card Sorting [ 70 ] method , we summarise three potential flaws based on existing UI / UX design books and websites such as Design Pattern Gallery [ 61 ] . To enhance the wireframes , we apply tailored heuristics that incorporate human design knowledge : a ) Occlusion : the textual information or element is occluded by other elements . This might be caused by the improper generation of the element‚Äôs width and height . To resolve this , we add a small margin between them without affecting other elements , as shown in Fig . 5 - D . b ) Duplication : the elements are similar in many aspects of properties , such as size , class , position , text , etc . The possible reason could be that LLMs lack a very long memory [ 82 ] . To resolve this , we compare the properties of each element and empirically set a threshold to determine if they are similar to be removed , for example , the redundant and overlapped buttons in Fig . 5 - E . c ) Out - of - bound : the size of the element exceeds the bounds of the UI . It usually occurs with text elements that require relatively wide borders . To resolve this , we trim the size of the element , as shown in Fig . 5 - F . 4 EVALUATION In this section , we conduct experiments to demonstrate the effectiveness of our fine - tuned LLMs in generating mid - fidelity UI wireframes . 4 . 1 Research Questions RQ1 : ( Performance of Fine - tuning ) Does the optimization of fine - tuning correlate with better UI wireframe generations ? For RQ1 , we evaluate the effectiveness of fine - tuning the LLMs , compared with widely - used in - context learning baseline approaches ( e . g . , zero - shot and few - shot ) . RQ2 : ( Performance of Variations ) Do the variations of the LLMs result in better UI wireframes ? For RQ2 , we present the comparison among the variations of GPT models ( e . g . , Curie , Babbage , Ada ) to demonstrate the performance of using Turbo as the model . 4 . 2 Testing Data We collect the data as discussed in Section 3 . 1 as our experimental dataset . Since we leverage 1 , 000 sample data to fine - tune the LLMs , we first remove these data to avoid potential bias . To evaluate the LLMs‚Äô generalizability and diversity , we randomly select 2 apps from each category . In total , we collect 100 UI textual descriptions as the input prompts to generate UI wireframes . Note that we do not use the corresponding UI screens as the ground - truth because the generative LLMs may create reasonable UI wireframes but deviate from the ground - truth . 11 Feng et al . Fig . 6 . Examples of prompting UI wireframe generations between zero - shot , few - shot , and our fine - tuned LLMs . 4 . 3 Annotation Methodology We recruit two people who had backgrounds in UI / UX design and art practice to rate the UI wireframe designs generated by our LLMs . At the beginning of the experiment , we first give them an introduction to our study and ask them to read UI / UX design books for 30 minutes to deepen the design principles . Given the natural language descriptions and a grid of UI wireframe designs , annotators are then asked to independently rate which designs in the grid are either significantly better generations or significantly worse generations . Note that we randomly shuffle the designs in the grid , so that the annotators do not know which design is generated from our model or baselines . All annotators are compensated $ 20 / hour for however long it takes them to complete the task . 4 . 4 RQ1 : Performance of Fine - tuning 4 . 4 . 1 Baselines . To demonstrate the advantage of fine - tuning to master the domain - specific task of UI wireframe generation , we compare it with two widely - used in - context learning methods as baselines , including zero - shot learning and few - shot learning . Note that all the experiment settings ( e . g . , model , hyperparameter , etc . ) are the same . Zero - shot learning : It predicts the results without any training samples . The general idea behind zero - shot learning is the LLMs train on a wide collection of different databases and workloads and can thus generalize to a completely new task and workload without the need to be trained particularly on that task . Few - shot learning : It refers to giving a few demonstrations of the task as conditioning to allow the LLMs to predict the results of new tasks . Typically , the demonstration has a prompt and a desired result ( e . g . , a login UI wireframe - > [ UI wireframe ] ) , and few - shot works by giving ùêæ examples of prompt and result , and then one final prompt , with the 12 Wireframing UI Design Intent with Generative Large Language Models Fig . 7 . Examples of the comparison of different models , including Ada , Babbage , Curie , and our model WireGen . LLMs expected to predict the result . We set ùêæ in the range of 1 to 2 , as this is how many examples can fit in the LLMs‚Äô maximum input tokens ( 4 , 096 ) . 4 . 4 . 2 Results . From the annotations we collected , the generations from our WireGen receive much better ratings than that of other baselines , e . g . , 85 . 5 % of our generation is rated as significantly better on average . In contrast , the generations receive an average of 51 . 5 % and 44 . 5 % significantly worse ratings for zero - shot and few - shot , respectively . As the ratings come from the subjective nature of the annotators , we further check the agreement between them by calculating Cohen‚Äôs kappa [ 58 ] . We observe two annotators share an inter - rater reliability of 0 . 32 , indicating a high agreement on scoring across generations . Fig . 6 shows some UI wireframe generations from the baselines and our WireGen . We can see that zero - shot can generate some relevant elements due to its strong in - context learning ability . For instance , it generates ‚Äúdeparture‚Äù , ‚Äúarrival‚Äù , ‚Äúpassengers‚Äù for a flight page and different music genres for a music page . However , without any fine - tuning and example prompts , zero - shot does not consider the elements‚Äô properties ( e . g . , size , alt - text , etc . ) or the overall design of the layout . We can also observe that the few - shot is a double - blade . Providing a small number of samples in the prompts can help the LLMs grasp the task better , leading to improved elements and layouts in the UI wireframes , which address the issues of zero - shot . However , the limited number of samples may not be enough for the LLMs to generalize to the complex UI wireframing task , leading to some subpar layouts as shown in Fig . 6 . Our WireGen overcomes these limitations by the optimization of fine - tuning over thousands of UI samples , allowing the LLMs to inherit its in - context ability and master UI - specific understanding , resulting in better UI wireframe generations . 13 Feng et al . 4 . 5 RQ2 : Performance of Variations 4 . 5 . 1 Baselines . As our implementation of LLMs GPT offers different models to support different levels of tasks , we set up the other models , including Curie , Babbage , and Ada , as the baselines to compare with our model . To guarantee the fairness of the experiment , we fine - tune these models on the training dataset using the same training settings as discussed in Section 3 . 2 . 4 . 5 . 2 Results . As expected , our model Turbo performs more significantly better UI wireframes compared with other baseline models , e . g . , 76 . 5 % vs 11 . 5 % , 10 . 5 % , 1 . 5 % for Curie , Babbage , and Ada , respectively . Both Curie and Babbage show the tendency to generate average UI wireframes ( 78 % and 82 . 5 % ) , while Ada underperforms with 81 . 5 % of UI wireframes rated to be significantly worse . We report the inter - rater reliability score of 0 . 28 in Cohen‚Äôs kappa , which represents a fair agreement , which we think is valid considering the highly subjective nature of the task ( picking ‚Äúbetter‚Äù or ‚Äúworse‚Äù UI wireframe designs ) . Fig . 7 shows some examples generated by our model ( Turbo ) and the baselines ( Curie , Babbage , and Ada ) . Our model , Turbo , outperforms the other models in terms of generating detailed and granular UI wireframes . As seen in the examples , Ada generates the simplest and fewest elements , while Babbage and Curie generate some relevant information but are not as detailed as Turbo . For instance , our model is able to generate precise subtitles for each title , like ‚ÄúAppearance‚Äù with a subtitle of ‚ÄúChange theme colors , text size and color , and more‚Äù . This is unsurprising , given the fact that Turbo is the most powerful model , while the other baseline models are simplified versions that are trained on a smaller corpus of data with limited knowledge . We believe the performance could be further boosted with the more advanced LLMs in the future . 5 USER STUDY In language , there are many ways to say the same thing in different words . We conduct a user study to gain insights into the usefulness of our model in assisting designers with UI wireframe generation with real - world descriptions . 5 . 1 Participants We invite five UI / UX designers to participate in our experiments by word - of - mouth and through online advertisements on social media and design communities . ‚Ä¢ P1 : Visual designer , 6 years of working experience , now working at a large multinational company , responsible for international app design and design innovation . ‚Ä¢ P2 : Interaction designer / product designer from a medium - sized company , with 4 years of working experience . Her work mainly focuses on smart devices . ‚Ä¢ P3 : Designer from a large IT company for 2 years with a focus on mobile advertising user experience design . ‚Ä¢ P4 : Visual designer from a well - known UI / UX design sharing community . He has 4 years of visual design experience . ‚Ä¢ P5 : Interaction designer / researcher from a medium - sized company , with 3 years of working experience . Her focus is on researching UI design intelligence . 5 . 2 Procedure The user studies were conducted online via the Zoom platform . Each participant took about one hour to complete . Each study session began with a short interview in which the participants were asked about their professional background 14 Wireframing UI Design Intent with Generative Large Language Models and their experience in designing UI / UX examples . We then gave them an introduction to our study and also an example to try . After the introduction , we asked participants to use WireGen in two tasks . First , they were given a common wireframe design prompt ( i . e . , a login page ) and were allowed to edit the description prompt to generate a practical mid - fidelity UI wireframe . This task allowed us to observe how participants designed the UI wireframes and which of the descriptions they retained or modified . After the first task , participants started the second task after a 5 - minute break . For the second task , we asked participants to freely use the tool , bringing in their own design scenarios . Participants were allowed to use wireframe sharing tools ( e . g . , Moqups [ 9 ] , Dribbble [ 5 ] , etc . ) and intelligence tools ( e . g . , layout generation tools such as Layout - Transformer [ 39 ] , text - to - image generation tools such as DALLE [ 65 ] , Stable Diffusion [ 67 ] , MidJourney [ 8 ] , etc . ) . We did not restrict how and when they used our tool for generations . This task allowed us to observe how participants used our tool during the real - world UI wireframing process and a rough comparison to other tools . In both tasks , we asked participants to share the screen of the interface . Lastly , we conducted a short survey containing three questions asking participants to rate our tool WireGen : 1 ) the effectiveness of the UI wireframe generations for inspiration ; 2 ) the generated UI wireframes are related to the descriptions ; 3 ) the diversity of the generations . Each question was rated on a 5 - point likert scale ( 1 : strongly disagree and 5 : strongly agree ) . At the end of the session , we conducted a short interview to collect open - ended feedback , such as how they used the WireGen , how they modify the wireframe generated by the tool , how they would adopt WireGen to their practice , and how they expected to improve . 5 . 3 Results Overall , participants appreciated the usefulness of our WireGen , which generates mid - fidelity wireframes by simply describing them in words to facilitate the UI design process . In addition , the statistical analysis revealed a high level of satisfaction for all metrics . We presented the results in detail below . 5 . 3 . 1 Metric performance . Most of the participants ( 4 . 4 / 5 . 0 ) agreed the wireframes can be effectively generated by our tool for inspiration , echoing the participants‚Äô behaviors in the first task to design a login page . They edited and modified an average of 2 . 4 times to the prompt to get satisfactory UI wireframes in the first task . P3 mentioned : ‚ÄúLogin pages usually have the general functionality of ‚ÄòUsername‚Äô and ‚ÄòPassword‚Äô fields , which is effectively generated without any prompt modification . Looking back at my previous design experience , an indispensable functionality of a login page is the third - party authentication service , such as Google . So , I added an additional description to the prompt ( a login page with Google authentication ) . I found the generations very effective and interesting , that some designs use buttons with textual descriptions ( e . g . , ‚ÄòSign in with Google‚Äô ) , and some use icon buttons , depending on the layout of the UI design . ‚Äù Most of the participants ( 4 . 0 / 5 . 0 ) agreed the wireframes generated by WireGen are relevant to the description prompts . For instance , P1 mentioned : ‚ÄúI can see some wireframe design patterns related to my description . For example , when I prompted the login page , the generated wireframe designs ( mostly ) consist of ‚Äòlogin‚Äô and ‚Äòregister‚Äô buttons‚Äù . P2 described a particular use case of using our tool : ‚ÄúI was working on creating a search page for a job searching app , so I prompted the description in the tool to gain inspiration ( Fig . 8 ) . I was amazed at how the UIs were generated . It closely mimics the real UI designs , such as a job , category , location , salary , etc . ‚Äù Most of the participants ( 3 . 6 / 5 . 0 ) admit that our tool can provide diverse UI wireframes . P3 mentioned the diverse UI element designs of using buttons and icons to support third - party authentications . Two participants noted the diversity of the UI layout and P1 provided an example : ‚ÄúIn my last job of designing a food app , I needed to decide on the display 15 Feng et al . Fig . 8 . Examples of three design prompts in the user study . We compare the our generations with the layout generation model ( LayoutTransformer ) and three text - to - image models ( DALLE , Stable Diffusion , MidJourney ) . The wireframes generated by Layout - Transformer are too abstract for understanding . Meanwhile , the wireframes generated by the text - to - image models often contained irrational text and design inconsistencies , which could hinder the process of gaining inspiration . In contrast , our model WireGen generates better wireframes . style of the recipes . I prompted the ‚Äòdisplay food recipes‚Äô in the tool ( Fig . 8 ) . Checking through the generations , I discovered two display styles , grid , and list , that match real UIs and human perception . ‚Äù Besides , two participants appreciated the diversity in functionality . P1 explained : ‚ÄúI especially noticed the different functionalities between the wireframe generations over several prompts . It typically varies in menu functionality . On the one hand , I found the menu interaction in the top left 16 Wireframing UI Design Intent with Generative Large Language Models corner is displayed as a three - line icon to save more design space . On the other hand , I found the interaction of the menu at the button navigation bar to show the functionality more explicitly . ‚Äù 5 . 3 . 2 Comparison performance . All participants agreed that WireGen has significant advantages over other tools . First , the existing wireframe sharing tools are not able to tailor designs to the needs , while WireGen allows for the addition of descriptive prompts to generate more specific UI wireframes . For example , P2 describes a practical use case when he wants to add a message functionality to the job searching page : ‚ÄúWhen I searched ‚Äòjob searching‚Äô or ‚Äòmessage function‚Äô in Dribbble , there are many different kinds of returning results . I had to manually filter out irrelevant ones , which takes time . But when I zoomed in on a more specific query like ‚Äòa job searching UI with message functionality‚Äô , no customized results were retrieved . In contrast , with WireGen , it solves my need to generate more customized designs and I could add more descriptions to flesh out the designs . ‚Äù Second , while there are many tools aimed at helping designers gain inspiration on UI wireframes , they are either low - fidelity ( such as layout generation ) or impractical ( such as text - to - image generation ) . As P5 stated : ‚ÄúI attempted to use the recent state - of - the - art image generation techniques , such as DALLE , Stable Diffusion , and MidJourney , to create UI wireframe designs . The results were not useful , due to 1 ) unreasonable text and 2 ) design incoherence like distorted shapes , as shown in Fig . 8 . ‚Äù P5 also highlighted the limitations of layout generation tools , explaining that : ‚ÄúThe generated layouts are too abstract to understand in Fig . 8 . The designers still have to do a lot of work mentally to imagine the contents in their heads . And the actual contents may impact the overall design . For example , a layout originally intended for image and text side - by - side may change to a top - bottom layout if the actual image is too large or the text too long . ‚Äù In comparison , the generated wireframes by WireGen were perceived as a gateway to gather ideas of actual details of the content , presented in the context of the designer‚Äôs concept . 5 . 3 . 3 Area to improve . The participants responded positively to WireGen and gave several directions to improve our tool . First , they wondered if we could develop our tool as a plugin to integrate with popular wireframe sketching software , such as Adobe PhotoShop [ 2 ] , Sketch [ 10 ] , so that they could modify the wireframe and serve with more powerful editing capabilities . Second , they suggested the addition of representative images in the wireframes , rather than relying solely on alt - text to convey the image content . This could lead to the creation of a high - fidelity wireframe . We believe that it is not difficult to retrieve the representative images once we have a large image resource database . In the future , we will continue to improve our tool for better performance in generating better wireframes . 6 DISCUSSION In Section 4 and Section 5 , we evaluate and examine WireGen ‚Äôs effectiveness and usefulness for automatically generating mid - fidelity UI wireframes from natural language descriptions . We see several opportunities to improve the performance of our LLMs . For example , we see that training on a large dataset can deliver better results . However , due to budget constraints , we only use 1 , 000 data for training . Once we have enough budget , we can improve the performance of the LLMs by incorporating more data . The LLMs are obviously not omnipotent , and still , fail to provide ready - to - use outputs in many cases . To enhance the raw wireframe generations , we implement several post - processing methods ( Section 3 . 3 ) based on a small - scale study , including incorporating semantic icons , adjusting text typography , and following UI guidelines . However , this study was limited in scope and merely aimed to provide a basic analysis of the potential UI design flaws produced by GPT - 3 . Further research is needed to fully understand the design limitations of LLMs and to develop more effective solutions . 17 Feng et al . Our work focuses on using UI screens and their corresponding view hierarchy information to construct UI - specific prompts . However , UI screens have various other modalities , including app information , interaction context , etc , which are left unused in our study . In future work , we aim to enhance the prompts by incorporating representations of multiple modalities , thereby improving performance . Our work focuses on generating UI wireframes from a single description . However , UI design often involves multiple iterations and accumulations of ideas . To streamline this process , we aim to integrate chat interactions in the future . This could be achieved through inspired by ChatGPT [ 3 ] . For example , a designer could interact with the chatbot to design a login page with Google authentication service : ‚Äú [ Designer ] : I want a UI wireframe design of a login page . ‚Äù ; [ Bot ] : < UI Wireframe > ; [ Designer ] : I want to add a Google authentication . " ; [ Bot ] : < UI Wireframe + Google > " , simulating a human - like design process through dialogue . In the future , we aim to enhance the interactivity of our design process , making it more conversational in nature . A straightforward extension of our work would be the retrieval of UI design examples . Numerous studies [ 15 , 40 , 42 ] have demonstrated the usefulness of low - fidelity wireframes to retrieve existing UI design examples from datasets , thereby facilitating UI design . Our mid - fidelity wireframes could further enhance this retrieval process by incorporating semantic information such as text and icon semantics . To achieve this , we could employ the mature Screen2Vec method [ 53 ] to encode visual and textual information into an embedding vector . By comparing the embedding vectors between the generated UI wireframe and the designs in existing UI datasets , we could retrieve semantically similar UI designs . This would help designers broaden their perspectives and gain inspiration . Future work could leverage our work into these potential applications to reduce the time and resources required for UI design , and ultimately improve the user experience . 7 CONCLUSION This paper introduces WireGen , a new method for exploring human - AI collaboration in mid - fidelity UI wireframe creation . Inspired by the success of generative Large Language Models in creating innovative ideas , we purpose to inherit LLMs‚Äô in - context ability from billions of web resources to create mid - fidelity UI wireframes . Since LLMs are not designed for UI understanding and creation , we fine - tune it with thousands of UI data , including screens and view hierarchies , to help the LLMs master UI design knowledge . With the fine - tuned LLMs , we prompt a simple description of design intent to generate a raw wireframe and apply several post - processing methods to make it more intuitive , engaging , and interactive . We set up experiments to demonstrate the performance of WireGen in generating mid - fidelity UI wireframes , significantly outperforming the widely - used baselines and ablated models . Additionally , we carry out a user study to confirm the practical usefulness of WireGen , demonstrating the potential to facilitate the process of UI design . REFERENCES [ 1 ] 2023 . Absolute , Relative , Fixed Positioning : How Do They Differ ? https : / / css - tricks . com / absolute - relative - fixed - positioining - how - do - they - differ / . [ 2 ] 2023 . Adobe Photoshop | Photo and design software . https : / / www . adobe . com / products / photoshop . html . [ 3 ] 2023 . ChatGPT : Optimizing Language Models for Dialogue . https : / / openai . com / blog / chatgpt / . [ 4 ] 2023 . DALLE 2 , Explained : The Promise and Limitations of a Revolutionary AI . https : / / towardsdatascience . com / dall - e - 2 - explained - the - promise - and - limitations - of - a - revolutionary - ai - 3faf691be220 . [ 5 ] 2023 . Dribbble . https : / / dribbble . com / . [ 6 ] 2023 . Figma : the collaborative interface design tool . https : / / www . figma . com / . [ 7 ] 2023 . Fine - tuning from OpenAI . https : / / platform . openai . com / docs / guides / fine - tuning . [ 8 ] 2023 . Midjourney . https : / / www . midjourney . com . 18 Wireframing UI Design Intent with Generative Large Language Models [ 9 ] 2023 . Online Mockup , Wireframe & UI Prototyping Tool Moqups . https : / / moqups . com / . [ 10 ] 2023 . Sketch : Design , collaborate , prototype and handoff . https : / / www . sketch . com / . [ 11 ] Tom√°s Alves , Daniel Nunes , Daniel Gon√ßalves , Joana Henriques - Calado , and Sandra Gama . 2022 . Towards conscientiousness - based graphical user interface design guidelines . Personal and Ubiquitous Computing ( 2022 ) , 1 ‚Äì 14 . [ 12 ] Toshiki Aoki , Rintaro Chujo , Katsufumi Matsui , Saemi Choi , and Ari Hautasaari . 2022 . EmoBalloon - Conveying Emotional Arousal in Text Chats with Speech Balloons . In CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 16 . [ 13 ] Diego Martin Arroyo , Janis Postels , and Federico Tombari . 2021 . Variational transformer networks for layout generation . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 13642 ‚Äì 13652 . [ 14 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . Language models are few - shot learners . Advances in neural information processing systems 33 ( 2020 ) , 1877 ‚Äì 1901 . [ 15 ] Sara Bunian , Kai Li , Chaima Jemmali , Casper Harteveld , Yun Fu , and Magy Seif Seif El - Nasr . 2021 . Vins : Visual search for mobile user interface design . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 14 . [ 16 ] Chunyang Chen , Sidong Feng , Zhengyang Liu , Zhenchang Xing , and Shengdong Zhao . 2020 . From lost to found : Discover missing ui design semantics through recovering missing tags . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW2 ( 2020 ) , 1 ‚Äì 22 . [ 17 ] Chunyang Chen , Sidong Feng , Zhenchang Xing , Linda Liu , Shengdong Zhao , and Jinshui Wang . 2019 . Gallery dc : Design search and knowledge discovery through auto - created gui component gallery . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 ‚Äì 22 . [ 18 ] Jieshan Chen , Chunyang Chen , Zhenchang Xing , Xin Xia , Liming Zhu , John Grundy , and Jinshui Wang . 2020 . Wireframe - based UI design search through image autoencoder . ACM Transactions on Software Engineering and Methodology ( TOSEM ) 29 , 3 ( 2020 ) , 1 ‚Äì 31 . [ 19 ] Jieshan Chen , Jiamou Sun , Sidong Feng , Zhenchang Xing , Qinghua Lu , Xiwei Xu , and Chunyang Chen . 2023 . Unveiling the Tricks : Automated Detection of Dark Patterns in Mobile Applications . In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology . 1 ‚Äì 20 . [ 20 ] Jieshan Chen , Amanda Swearngin , Jason Wu , Titus Barik , Jeffrey Nichols , and Xiaoyi Zhang . 2022 . Towards Complete Icon Labeling in Mobile Applications . In CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 14 . [ 21 ] Mark Chen , Jerry Tworek , Heewoo Jun , Qiming Yuan , Henrique Ponde de Oliveira Pinto , Jared Kaplan , Harri Edwards , Yuri Burda , Nicholas Joseph , Greg Brockman , et al . 2021 . Evaluating large language models trained on code . arXiv preprint arXiv : 2107 . 03374 ( 2021 ) . [ 22 ] Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Sebastian Gehrmann , et al . 2022 . Palm : Scaling language modeling with pathways . arXiv preprint arXiv : 2204 . 02311 ( 2022 ) . [ 23 ] Hyung Won Chung , Le Hou , Shayne Longpre , Barret Zoph , Yi Tay , William Fedus , Eric Li , Xuezhi Wang , Mostafa Dehghani , Siddhartha Brahma , et al . 2022 . Scaling instruction - finetuned language models . arXiv preprint arXiv : 2210 . 11416 ( 2022 ) . [ 24 ] Ian G Clifton . 2015 . Android User Interface Design : Implementing Material Design for Developers . Addison - Wesley Professional . [ 25 ] Biplab Deka , Zifeng Huang , Chad Franzen , Joshua Hibschman , Daniel Afergan , Yang Li , Jeffrey Nichols , and Ranjitha Kumar . 2017 . Rico : A mobile app dataset for building data - driven design applications . In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology . 845 ‚Äì 854 . [ 26 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . Bert : Pre - training of deep bidirectional transformers for language understanding . arXiv preprint arXiv : 1810 . 04805 ( 2018 ) . [ 27 ] Sidong Feng and Chunyang Chen . 2022 . GIFdroid : an automated light - weight tool for replaying visual bug reports . In Proceedings of the ACM / IEEE 44th International Conference on Software Engineering : Companion Proceedings . 95 ‚Äì 99 . [ 28 ] Sidong Feng and Chunyang Chen . 2023 . Prompting Is All Your Need : Automated Android Bug Replay with Large Language Models . arXiv preprint arXiv : 2306 . 01987 ( 2023 ) . [ 29 ] Sidong Feng , Chunyang Chen , and Zhenchang Xing . 2022 . Gallery DC : Auto - created GUI component gallery for design search and knowledge discovery . In Proceedings of the ACM / IEEE 44th International Conference on Software Engineering : Companion Proceedings . 80 ‚Äì 84 . [ 30 ] Sidong Feng , Chunyang Chen , and Zhenchang Xing . 2023 . Video2Action : Reducing Human Interactions in Action Annotation of App Tutorial Videos . In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology . 1 ‚Äì 15 . [ 31 ] Sidong Feng , Minmin Jiang , Tingting Zhou , Yankun Zhen , and Chunyang Chen . 2022 . Auto - Icon + : An Automated End - to - End Code Generation Tool for Icon Designs in UI Development . ACM Transactions on Interactive Intelligent Systems 12 , 4 ( 2022 ) , 1 ‚Äì 26 . [ 32 ] Sidong Feng , Suyu Ma , Jinzhong Yu , Chunyang Chen , TingTing Zhou , and Yankun Zhen . 2021 . Auto - icon : An automated code generation tool for icon designs assisting in ui development . In 26th International Conference on Intelligent User Interfaces . 59 ‚Äì 69 . [ 33 ] Sidong Feng , Mulong Xie , and Chunyang Chen . 2022 . Efficiency Matters : Speeding Up Automated Testing with GUI Rendering Inference . arXiv preprint arXiv : 2212 . 05203 ( 2022 ) . [ 34 ] Sidong Feng , Mulong Xie , Yinxing Xue , and Chunyang Chen . 2023 . Read It , Don‚Äôt Watch It : Captioning Bug Recordings Automatically . arXiv preprint arXiv : 2302 . 00886 ( 2023 ) . [ 35 ] Wilbert O Galitz . 2007 . The essential guide to user interface design : an introduction to GUI design principles and techniques . John Wiley & Sons . [ 36 ] Gabriela Goldschmidt . 1991 . The dialectics of sketching . Creativity research journal 4 , 2 ( 1991 ) , 123 ‚Äì 143 . [ 37 ] IanGoodfellow , JeanPouget - Abadie , MehdiMirza , BingXu , DavidWarde - Farley , SherjilOzair , AaronCourville , andYoshuaBengio . 2020 . Generative adversarial networks . Commun . ACM 63 , 11 ( 2020 ) , 139 ‚Äì 144 . [ 38 ] Google . 2023 . Material Icons . https : / / material . io / resources / icons / . 19 Feng et al . [ 39 ] Kamal Gupta , Justin Lazarow , Alessandro Achille , Larry S Davis , Vijay Mahadevan , and Abhinav Shrivastava . 2021 . Layouttransformer : Layout generation and completion with self - attention . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 1004 ‚Äì 1014 . [ 40 ] Scarlett R Herring , Chia - Chen Chang , Jesse Krantzler , and Brian P Bailey . 2009 . Getting inspired ! Understanding how and why examples are used in creative design practice . In Proceedings of the SIGCHI conference on human factors in computing systems . 87 ‚Äì 96 . [ 41 ] Jonathan Ho , Ajay Jain , and Pieter Abbeel . 2020 . Denoising diffusion probabilistic models . Advances in Neural Information Processing Systems 33 ( 2020 ) , 6840 ‚Äì 6851 . [ 42 ] Forrest Huang , John F Canny , and Jeffrey Nichols . 2019 . Swire : Sketch - based user interface retrieval . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 10 . [ 43 ] Forrest Huang , Gang Li , Xin Zhou , John F Canny , and Yang Li . 2021 . Creating User Interface Mock - ups from High - Level Text Descriptions with Deep - Learning Models . arXiv preprint arXiv : 2110 . 07775 ( 2021 ) . [ 44 ] Forrest Huang , Eldon Schoop , David Ha , Jeffrey Nichols , and John Canny . 2021 . Sketch - Based Creativity Support Tools Using Deep Learning . In Artificial Intelligence for Human Computer Interaction : A Modern Approach . Springer , 379 ‚Äì 415 . [ 45 ] IBM . 2023 . Design Language . https : / / www . ibm . com / design / language / iconography / ui - icons / library . [ 46 ] Shachar Kaufman , Saharon Rosset , Claudia Perlich , and Ori Stitelman . 2012 . Leakage in data mining : Formulation , detection , and avoidance . ACM Transactions on Knowledge Discovery from Data ( TKDD ) 6 , 4 ( 2012 ) , 1 ‚Äì 21 . [ 47 ] Diederik P Kingma and Max Welling . 2013 . Auto - encoding variational bayes . arXiv preprint arXiv : 1312 . 6114 ( 2013 ) . [ 48 ] James A Landay and Brad A Myers . 2001 . Sketching interfaces : Toward more human interface design . Computer 34 , 3 ( 2001 ) , 56 ‚Äì 64 . [ 49 ] Bryan Lawson . 2006 . How designers think . Routledge . [ 50 ] Yann LeCun , Yoshua Bengio , and Geoffrey Hinton . 2015 . Deep learning . nature 521 , 7553 ( 2015 ) , 436 ‚Äì 444 . [ 51 ] Luis A Leiva , Asutosh Hota , and Antti Oulasvirta . 2020 . Enrico : A dataset for topic modeling of mobile UI designs . In 22nd International Conference on Human - Computer Interaction with Mobile Devices and Services . 1 ‚Äì 4 . [ 52 ] JiananLi , JimeiYang , AaronHertzmann , JianmingZhang , andTingfaXu . 2019 . Layoutgan : Generatinggraphiclayoutswithwireframediscriminators . arXiv preprint arXiv : 1901 . 06767 ( 2019 ) . [ 53 ] Toby Jia - Jun Li , Lindsay Popowski , Tom Mitchell , and Brad A Myers . 2021 . Screen2vec : Semantic embedding of gui screens and gui components . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 15 . [ 54 ] James Lin , Mark W Newman , Jason I Hong , and James A Landay . 2000 . DENIM : Finding a tighter fit between tools and practice for web site design . In Proceedings of the SIGCHI conference on Human factors in computing systems . 510 ‚Äì 517 . [ 55 ] XiaoLiu , YananZheng , Zhengxiao Du , MingDing , YujieQian , ZhilinYang , andJie Tang . 2021 . GPT understands , too . arXivpreprintarXiv : 2103 . 10385 ( 2021 ) . [ 56 ] Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019 . Roberta : A robustly optimized bert pretraining approach . arXiv preprint arXiv : 1907 . 11692 ( 2019 ) . [ 57 ] Yuwen Lu , Chengzhi Zhang , Iris Zhang , and Toby Jia - Jun Li . 2022 . Bridging the Gap Between UX Practitioners‚Äô Work Practices and AI - Enabled Design Support Tools . In CHI Conference on Human Factors in Computing Systems Extended Abstracts . 1 ‚Äì 7 . [ 58 ] Mary L McHugh . 2012 . Interrater reliability : the kappa statistic . Biochemia medica 22 , 3 ( 2012 ) , 276 ‚Äì 282 . [ 59 ] Houman Mehrafarin , Sara Rajaee , and Mohammad Taher Pilehvar . 2022 . On the Importance of Data Size in Probing Fine - tuned Models . arXiv preprint arXiv : 2203 . 09627 ( 2022 ) . [ 60 ] Milad Moradi , Kathrin Blagec , Florian Haberl , and Matthias Samwald . 2021 . GPT - 3 models are poor few - shot learners in the biomedical domain . arXiv preprint arXiv : 2109 . 02555 ( 2021 ) . [ 61 ] Theresa Neil . 2014 . Mobile design pattern gallery : UI patterns for smartphone apps . " O‚ÄôReilly Media , Inc . " . [ 62 ] Long Ouyang , Jeff Wu , Xu Jiang , Diogo Almeida , Carroll L Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , et al . 2022 . Training language models to follow instructions with human feedback . arXiv preprint arXiv : 2203 . 02155 ( 2022 ) . [ 63 ] Vinoth Pandian Sermuga Pandian , Sarah Suleri , and Matthias Jarke . 2020 . Syn : Synthetic Dataset for Training UI Element Detector From Lo - Fi Sketches . In Proceedings of the 25th International Conference on Intelligent User Interfaces Companion . 79 ‚Äì 80 . [ 64 ] Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2020 . Exploring the limits of transfer learning with a unified text - to - text transformer . The Journal of Machine Learning Research 21 , 1 ( 2020 ) , 5485 ‚Äì 5551 . [ 65 ] Aditya Ramesh , Prafulla Dhariwal , Alex Nichol , Casey Chu , and Mark Chen . 2022 . Hierarchical text - conditional image generation with clip latents . arXiv preprint arXiv : 2204 . 06125 ( 2022 ) . [ 66 ] Laria Reynolds and Kyle McDonell . 2021 . Prompt programming for large language models : Beyond the few - shot paradigm . In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 7 . [ 67 ] Robin Rombach , Andreas Blattmann , Dominik Lorenz , Patrick Esser , and Bj√∂rn Ommer . 2022 . High - resolution image synthesis with latent diffusion models . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 10684 ‚Äì 10695 . [ 68 ] VinothPandianSermugaPandian , AbdullahShams , SarahSuleri , andProfDrMatthiasJarke . 2022 . LoFiSketch : ALargeScaleDatasetofSmartphone Low Fidelity Sketches . In CHI Conference on Human Factors in Computing Systems Extended Abstracts . 1 ‚Äì 5 . [ 69 ] Vinoth Pandian Sermuga Pandian , Sarah Suleri , and Prof Dr Matthias Jarke . 2021 . UISketch : a large - scale dataset of UI element sketches . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 14 . [ 70 ] Donna Spencer . 2009 . Card sorting : Designing usable categories . Rosenfeld Media . 20 Wireframing UI Design Intent with Generative Large Language Models [ 71 ] Hariharan Subramonyam , Colleen Seifert , and Eytan Adar . 2021 . ProtoAI : Model - Informed Prototyping for AI - Powered Interfaces . In 26th International Conference on Intelligent User Interfaces . 48 ‚Äì 58 . [ 72 ] Hugo Touvron , Louis Martin , Kevin Stone , Peter Albert , Amjad Almahairi , Yasmine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhosale , et al . 2023 . Llama 2 : Open foundation and fine - tuned chat models . arXiv preprint arXiv : 2307 . 09288 ( 2023 ) . [ 73 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , ≈Åukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . Advances in neural information processing systems 30 ( 2017 ) . [ 74 ] Bryan Wang , Gang Li , and Yang Li . 2022 . Enabling Conversational Interaction with Mobile UI using Large Language Models . arXiv preprint arXiv : 2209 . 08655 ( 2022 ) . [ 75 ] Bryan Wang , Gang Li , Xin Zhou , Zhourong Chen , Tovi Grossman , and Yang Li . 2021 . Screen2words : Automatic mobile UI summarization with multimodal learning . In The 34th Annual ACM Symposium on User Interface Software and Technology . 498 ‚Äì 510 . [ 76 ] Christoph Wimmer , Alex Untertrifaller , and Thomas Grechenig . 2020 . SketchingInterfaces : A Tool for Automatically Generating High - Fidelity User Interface Mockups from Hand - Drawn Sketches . In Proceedings of the 32nd Australian Conference on Human - Computer Interaction . 538 ‚Äì 545 . [ 77 ] Ziming Wu , Qianyao Xu , Yiding Liu , Zhenhui Peng , Yingqing Xu , and Xiaojuan Ma . 2021 . Exploring Designers‚Äô Practice of Online Example Management for Supporting Mobile UI Design . In Proceedings of the 23rd International Conference on Mobile Human - Computer Interaction . 1 ‚Äì 12 . [ 78 ] Mulong Xie , Sidong Feng , Zhenchang Xing , Jieshan Chen , and Chunyang Chen . 2020 . UIED : a hybrid tool for GUI element detection . In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 1655 ‚Äì 1659 . [ 79 ] Mulong Xie , Zhenchang Xing , Sidong Feng , Xiwei Xu , Liming Zhu , and Chunyang Chen . 2022 . Psychologically - inspired , unsupervised inference of perceptual groups of GUI widgets from GUI images . In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 332 ‚Äì 343 . [ 80 ] XuyongYang , TaoMei , Ying - QingXu , YongRui , andShipengLi . 2016 . Automaticgenerationofvisual - textualpresentationlayout . ACMTransactions on Multimedia Computing , Communications , and Applications ( TOMM ) 12 , 2 ( 2016 ) , 1 ‚Äì 22 . [ 81 ] Zhengyuan Yang , Zhe Gan , Jianfeng Wang , Xiaowei Hu , Yumao Lu , Zicheng Liu , and Lijuan Wang . 2022 . An empirical study of gpt - 3 for few - shot knowledge - based vqa . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol . 36 . 3081 ‚Äì 3089 . [ 82 ] Min Zhang and Juntao Li . 2021 . A commentary of GPT - 3 in MIT Technology Review 2021 . Fundamental Research 1 , 6 ( 2021 ) , 831 ‚Äì 833 . [ 83 ] Mingyu Zong and Bhaskar Krishnamachari . 2022 . a survey on GPT - 3 . arXiv preprint arXiv : 2212 . 00857 ( 2022 ) . [ 84 ] Mingyu Zong and Bhaskar Krishnamachari . 2022 . Solving math word problems concerning systems of equations with gpt - 3 . In Proceedings of the Thirteenth AAAI Symposium on Educational Advances in Artificial Intelligence . 21