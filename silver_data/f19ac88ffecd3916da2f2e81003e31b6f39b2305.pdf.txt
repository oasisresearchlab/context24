Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) [ DOI : 10 . 2197 / ipsjjip . 24 . 540 ] Regular Paper Citation Block Determination Using Textual Coherence D ain K aplan 1 , † 1 , a ) T akenobu T okunaga 2 , b ) S imone T eufel 1 , c ) Received : May 7 , 2015 , Accepted : January 12 , 2016 Abstract : Detecting the boundaries of citations in the running text of research papers is an important task for re - search paper summarisation , idea attribution , sentiment analysis , and other citation - based analysis research . Recently , detecting non - explicit citing sentences has garnered some attention , but can still be seen as in its infancy . We deﬁne this task as citation block determination ( CBD ) . In this paper we propose and investigate the e ﬀ ects of various types of textual coherence on CBD , positing that it is a crucial aspect of identifying citation blocks , as it is fundamental to the composition of citations themselves . We demonstrate promising results , with our method outperforming previous state - of - the - art on F 1 by a large margin , with an improvement in both precision and recall , and further provide an in - depth error analysis and discussion of why this is the case . Keywords : citation block determination , citation analysis , citations , research paper summarisation , textual coherence , natural language processing , information extraction 1 . Introduction There is a wealth of research from over the decades focusing on citations and citation analysis in various forms ; this includes citation network analysis , like indexes [ 16 ] , [ 18 ] , bibliographic coupling [ 29 ] , co - citation [ 56 ] , citation counts [ 68 ] , and the h - index [ 24 ] , analysis of citation role / function [ 60 ] , [ 67 ] , analysis of sociological aspects [ 70 ] , domain summarisation [ 15 ] , [ 17 ] , [ 44 ] , [ 50 ] , [ 52 ] , paper summarisation [ 27 ] , [ 51 ] , and sentiment analy - sis [ 1 ] , [ 43 ] . We see a progression from manual techniques to automatic , and from simple network metrics to increasingly deeper seman - tic analysis . One hurdle to overcome in this progression is the adequate detection of the span of a citation , i . e . , a citation block , which may encompass multiple sentences ( see Fig . 1 ) . Previous work has mostly used either the explicit citing sentence only ( the citation block’s anchor sentence , e . g . , sentence ( 0 ) in Fig . 1 ) [ 50 ] , a k - word window [ 10 ] , [ 12 ] , [ 43 ] around the citation anchor ( “Si - bun 1990” in Fig . 1 ) , or the presence of simple cue - phrases [ 45 ] as a substitute for knowing the actual boundaries , due to the dif - ﬁculty of this task . A recent study [ 2 ] shows that less than 25 % of negative senti - ment , and half of positive , are present in the citation block’s an - chor sentence , and other studies [ 27 ] , [ 58 ] have suggested that up to half of all citation content is beyond the anchor sentence . The detection of citation blocks ( e . g . , sentences { ( 0 ) , ( 1 ) , ( 2 ) } in Fig . 1 ) for incorporation in research further down stream is therefore all 1 University of Cambridge , Cambridge CB3 – 0FD , U . K . 2 Tokyo Institute of Technology , Meguro , Tokyo 152 – 8550 , Japan † 1 Presently with Tokyo Institute of Technology a ) dain @ cl . cs . titech . ac . jp b ) take @ cl . cs . titech . ac . jp c ) simone . teufel @ cl . cam . ac . uk Fig . 1 An example multi - sentence citation block with following non - citing sentence . the more pertinent . Past studies [ 13 ] , [ 53 ] have , however , pointed out the di ﬃ - culty in identifying citation blocks , with one di ﬃ culty given be - ing manual procurement of “rules” for matching additional citing sentences . However , other options are available for overcoming the di ﬃ culties in detection of citation blocks . Namely , there is at least one feature of citations that we can exploit to this end : citations are objective - driven , i . e . , they are “items introduced [ into the discourse ] for the purpose of saying something about them” * 1 . Since they are a phenomenon of dis - course , brought into the ﬂow of text by the author to fulﬁll some function before moving on , it follows that they should be cohesive as a whole . There are theories for describing the cohesiveness of text — textual coherence [ 22 ] , [ 25 ] — which explain how text joins to - gether to form a uniﬁed whole , in terms of structural relations , and in terms of meaning . * 1 Reference [ 22 ] refers to these as Citation Forms . c (cid:2) 2016 Information Processing Society of Japan 540 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) It follows that proper exploitation of textual coherence related to citations may yield good results in detecting citation blocks . In this paper we propose and evaluate our novel method of applying various features representing di ﬀ erent aspects of tex - tual coherence , both individually and in combination , to see how they contribute to determining citation boundaries on an existing citation corpus [ 2 ] , the best combination achieving an F 1 score ≈ 10 % above the baseline . The corpus , which we have cleaned up and converted to XML . To our knowledge , our work is the ﬁrst to exploit the idea that citations are a function of discourse for determining their boundaries . The rest of the paper is as follows . We next propose and deﬁne the citation block determination ( CBD ) task ( Section 2 . 1 ) , mov - ing on to explaining textual coherence as it relates to this task ( Section 2 . 2 ) ; we then describe our method utilising textual co - herence features for CBD in Section 3 , including elaborating on the di ﬀ erent textual coherence feature sets we create features for and subsequently models from ( Section 4 . 1 ) . This is followed by two experiments ( Sections 4 . 2 and 4 . 3 ) , including an in - depth er - ror analysis and discussion of the results ( 4 . 4 ) . Finally , we men - tion related work ( Section 5 ) prior to concluding and outlining future work ( Section 6 ) . 2 . Deﬁnitions Below we deﬁne the task of citation block determination , and brieﬂy explain textual coherence , which is the foundation upon which our motivations and work are based . 2 . 1 Citation Block Determination ( CBD ) Here we propose and deﬁne the task of citation block deter - mination ( CBD ) , along with related terms and concepts . Figure 1 shows a multi - sentence citation block ; please refer to this ﬁgure for the following section . ( Note that the target anchor within each example is underlined in ﬁgures for the remainder of the work . ) A citation anchor ( anchor ) is the span of text that marks the explicit entry of a citation into the discourse ( “Sibun 1990” in Fig . 1 ) ; similarly , the citation anchor sentence S A is the sen - tence that contains this anchor ( sentence ( 0 ) in Fig . 1 ) . A citation block ( CB , block ) is the set of citing sentences S Cit surrounding the anchor that continue to describe the work referenced by the entry of the anchor ( sentences { ( 0 ) , ( 1 ) , ( 2 ) } in Fig . 1 ) ; this forms a “block” around the citation anchor . We deﬁne the block as always beginning with S A , having optional additional sentences that fol - low * 2 . Also note that in Fig . 1 , sentence ( 3 ) is not part of the citation block for the anchor “Sibun 1990” . CBD is the task of determining the citation block for an an - chor , i . e . , the set of sentences S Cit continuing on from an anchor sentence S A that continue to cite the work referenced by the an - chor . In CBD there is only a very locally scoped possibility of reup - * 2 There are marginal cases in which a citing sentence precedes S A , which are usually the result of coreference ( e . g . , using a pronoun such as “this” ) tying two statements together ; in our corpus these can be considered out - liers , at less than 1 / 4 of one percent ( i . e . , 0 . 24 % ) . We do not consider such marginal cases in our deﬁnition . take , i . e . , of having a citation block that is noncontiguous . Rules and etiquette of proper citation dictate that one should explicitly mark the discourse as such ; implicit reuptake , the idea of con - tinuing to cite a work later on ( or in fact anywhere ) in the citing work without marking the text as a citation , is a slippery - slope , as not only does it violate the rules of citation , but the author’s intent becomes under - deﬁned ( if he / she indeed intended to cite would he / she not have explicitly cited the work again ? ) . Reason - ing about the implied but unmarked intent of the author further complicates the task , so non - local implicit reuptake is excluded from the task deﬁnition . There are marginal cases in which for brevity authors deﬁne an acronym ( e . g . , “W & W” for “Wyndham and Wells” ) for use later in the text ; this , however , is in e ﬀ ect redeﬁning the citation anchor and is therefore in fact an explicit citation . These kind of citations are common in self - citations , when authors extend their own work and therefore heavily cite it . Heavily self - citing papers tend to follow di ﬀ erent patterns of citation as the whole paper may more or less be an extended citation ; in these papers it is often di ﬃ cult for even the reader to distinguish the current work from previous due to this ambiguity . Self - citations are beyond the scope of this work . 2 . 2 Textual Coherence Coherence of text concerns the question of how uniﬁed the constituents of a text are with one another structurally , either in terms of composition , meaning , or both . Textual coherence can be broadly divided into two groups , relational coherence and entity coherence ( which further has two sub - groups , lexical and grammatical ) * 3 . Abbreviations for categories used in Table 1 are given in parentheses . Relational coherence ( REL ) is concerned with how blocks of text are built up from small units into bigger ones , with an edge having a semantic role / description linking them . Examples in - clude the work of Ref . [ 25 ] , RST [ 61 ] and DST [ 38 ] * 4 . Relational coherence also includes aspects of the texture notion of conjunc - tions for bridging ties between sentences , discussed in Ref . [ 22 ] . The Penn Discourse TreeBank ( PDTB ) [ 49 ] is also a good re - source for this , and used in this work . Entity coherence is concerned not with a relational hierarchi - cal structure of the text , but instead with a meaning - structure that looks at mentions of entities and how they relate , such as in Cen - tering theory [ 20 ] , [ 66 ] . Entity - coherence can be split into two subgroups : lexical and grammatical . EntityGrids [ 3 ] , [ 34 ] is an example that spans both subgroups . Lexical coherence ( LEX ) is concerned with the formation of chains from repetition / coocurrence of the same and similar lex - ical items in a text . TextTiling [ 23 ] is one such example that utilises lexical coherence for segmenting text consecutively into “’tiles” or topics . Grammatical coherence ( GRM ) has three cohesive relations : reference ( REF ) , substitution , and ellipsis ; of these , the most common is reference , i . e . , anaphora . One prevalent type of anaphora is coreference , which deals with di ﬀ erent mentions re - * 3 For a good overview of the two , see Ref . [ 32 ] . * 4 For a good overview , see Ref . [ 4 ] . c (cid:2) 2016 Information Processing Society of Japan 541 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) Table 1 List of features ; features marked with † are used in the baseline ; (cid:2) marks block - level features , all others are sentence level . CoherenceType FeatureSet Feature Name Value & Example REL L oc S i DistanceFromS A { 1 , 2 , . . . , 6 } , e . g . , 2 S i LocationInPaper { 1 , 2 , . . . , 8 } , e . g . , 4 D is S i ExplicitDisRelTypeAndConnective “ ( REL TYPE / CONN ) ” , e . g . , “ ( I nstantiation / for instance ) ” S i NonExplicitDisRelType “REL” , e . g . , “C ause ” S i toS A NonExplicitDisRelTypePath (cid:2) “REL 1 ⇒ . . . ⇒ REL N ” , e . g . , “I nstantiation ⇒ C ause ” S i toS A ExplicitDisRelTypePath (cid:2) “REL 1 ⇒ . . . ⇒ REL N ” , e . g . , “I nstantiation ⇒ C ause ” S i toS A ExplicitDisRelConnectivePath (cid:2) “CONN 1 ⇒ . . . ⇒ CONN N ” , e . g . , “for instance ⇒ thus” S i toS A DisRelTypePath (cid:2) “REL 1 ⇒ . . . ⇒ REL N ” , e . g . , “I nstantiation ⇒ C ause ” S i ParagraphBreak T or F S i StartsWithSectionHeader † T or F REF C oref S i toS i - 1 HasCoref T or F S i toS A HasCoref T or F S i toS A HasCorefPath (cid:2) T or F S i HasWorkNounAnaphor T or F S i WorkNounAnaphor “WORD” , e . g . , “this work” GRM & LEX C it S i HasAnotherCitation † T or F S i HasFirstAuthorLastName † T or F S i HasFirstAuthorLastNameAndYear † T or F S i HasAcronymFromAnchorSent † T or F S i HasLexicalHook † T or F S i StartsWithConnective † T or F S i HasDeterminer + WorkNoun † T or F S i StartsWith3rdPersonPronoun † T or F LEX E - grid S i + S i - 1 EgridDi ﬀ Set of role ( S , O , X , - ) di ﬀ s , e . g . , { “ - X” , “SX” } S i toS A EgridCoherence (cid:2) Double , e . g . , − 0 . 43 N - grams S i N - grams † Set of { 1 , 2 , 3 } - grams , e . g . , { “their” , “work” , “their work” } PMI S i + S i - 1 PmiSimilarityScore “W 1 → W 2 ” = ( - 1 , 1 ) , e . g . , “number → equation” = . 4 TM S i + S i - 1 TopicsCosine ( 0 , 1 ) , e . g . , 0 . 4 S i + S A TopicsCosine ( 0 , 1 ) , e . g . , 0 . 4 S i + S i - 1 NumMutualTopics { 0 , 1 , . . . } , e . g . , 4 S i + S A NumMutualTopics { 0 , 1 , . . . } , e . g . , 4 S i + S i - 1 MutualTopics { TOPIC 1 , . . . , TOPIC N } , e . g . , { 4 , 123 } S i + S A MutualTopics { TOPIC 1 , . . . , TOPIC N } , e . g . , { 4 , 123 } S i toS A TopicsCosineBlock (cid:2) ( 0 , 1 ) , e . g . , 0 . 4 S i toS A TopicsCosinePath (cid:2) ( 0 , 1 ) , e . g . , 0 . 4 S A is the anchor sentence for the current citation block . S i is the current sentence within the current citation block . ferring to the same entity ; the lexical representations of these ref - erence expressions may di ﬀ er from one mention to another , but their successive mentions in a text produce a coreference - chain that ties those sentences together . 3 . Coherence in Citation Blocks We hypothesise that as citations are objective - driven , they are introduced into discourse by the author to fulﬁll a function and will continue to be discussed until that function is fulﬁlled * 5 . If this is true , it follows that they should be cohesive as a whole , or rather , that there should be a means to deduce which sentences be - long to the citation and which do not . This is further strengthened when we know in general that text is cohesive due to the intent of the author to convey something meaningful [ 25 ] . This , however , * 5 See Ref . [ 22 ] . c (cid:2) 2016 Information Processing Society of Japan 542 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) introduces a di ﬀ erent complexity that must be overcome , namely , to determine what ties the citation block together as opposed to the surrounding ( con ) text . We must then ﬁnd a way to exploit aspects of the coherence of the text in which citations appear . It should be possible , in fact , to exploit a variety of textual coherence features for detecting ci - tation blocks . CBD can be seen as a cascaded set of decisions about whether or not to declare that a citation block ends after each subsequent sentence S i following on from and including the citation anchor sentence S A . We formalise it as a binary classiﬁcation task of sentences continuing on from the citation anchor . We construct classiﬁers using Support Vector Machine ( SVM ) [ 64 ] following previous work , and Conditional Random Fields ( CRF ) [ 33 ] . 3 . 1 Coherence Feature Sets We next propose feature sets for textual coherence categories that we then use to train classiﬁers . A list of all features can be found in Table 1 . Features can be subdivided into sentence - wise features and block - wise features . The former being those that extract information from a sentence ( S i ) only or a pair of sen - tences ( S i + S i − 1 or S i + S A ) ; the latter instead encodes information about all the sentences from the anchor sentence ( S A ) through a sentence ( S i ) , such as overall similarity / coherence , path informa - tion , i . e . , the chain of transitions between sentences for some fea - ture , e . g . , PDTB - arguments or coreference - chains . This will be elaborated below within individual feature set explanations . The labels used in all tables for a given feature set are given in parentheses after the feature set name . Feature names are given in parentheses throughout explanations . 3 . 1 . 1 Relational Coherence Feature Sets We further categorise relational coherence features into two sets : location and discourse . Relational features , beyond the obvious physical structure of the text , often must be extrapo - lated from surface cues . As a result non - whitespace - based ( sec - tions / paragraphs / etc . ) features are more di ﬃ cult to derive e ﬀ ec - tively . Location Features ( L oc ) — Citation usage often varies from section to section within a paper . For example , in the “introduc - tion” , citations tend to appear in groups and end very quickly , whereas in the “related work” section , citations tend to be longer . This kind of location feature has further proven useful in other research such as argumentative zoning ( AZ ) for identifying the di ﬀ erent zoning labels of sentences within an academic text [ 59 ] . Though we do not have section information available , we can approximate the sections where citations appear by splitting the paper into quantiles ( “ S i LocationInPaper ” ) , e . g . , if the paper were broken into 8 quantiles “ S i LocationInPaper ” = 8 for an anchor in the ﬁnal sentence of the paper . Though CRF captures distance from an anchor sentence im - plicitly , for SVM we can directly encode this as the distance in sentences from the anchor sentence ( “ S i DistanceFromS A ” ) , e . g . , 1 for the sentence after the anchor . Discourse Features ( D is ) — Discourse relations ( e . g . , Penn Discourse TreeBank [ 49 ] ) show the relationship between clauses and sentences in terms of transitions , such as C ontrast , C ause , Fig . 2 A citation showing coreference of “STRAND” ⇐ “Its” . C ondition , A lternatives , etc . These transitions can be used to build a tree of the discourse showing the ﬂow of argument from one statement to another , where nodes represent statements and edges the relations between them . Such relations can be explicit , such as the use of the word “because” to mark a causal relation - ship , and implicit , where “because” is not used , but inferred based on how the statements are constructed ; explicit relations therefore both have a surface form , e . g . , “because” , and a relation type , such as C ause ; note that di ﬀ erent surface forms may have the same relation type ; implicit relations only have a type . Discourse relations seem promising for citation blocks be - cause they describe the ﬂow of argument for a paper , in - cluding the areas where citation blocks appear . We can capture the above depictions of explicit and non - explicit re - lation features with “ S i ExplicitDisRelTypeAndConnective ” and “ S i NonExplicitDisRelType ” . We can further capture the entire set of transitions from an an - chor sentence S A to a sentence S i , such as “since ⇒ for instance ⇒ thus” ( mapping to relation types : “A synchronous ⇒ I nstan - tiation ⇒ C ause ” ) , which may allow the classiﬁer to learn which series contain meaningful and relevant transitions for demar - cating citation blocks . “ S i toS A NonExplicitDisRelTypePath ” captures this path information for non - explicit dis - course relations , “ S i toS A ExplicitDisRelTypePath ” and “ S i toS A ExplicitDisRelConnectivePath ” for explicit path infor - mation , and “ S i toS A DisRelTypePath ” for the combination of both non - explicit and explicit in sequential order of occurrence . Finally , if there was a paragraph break , we can emit a Boolean feature as well ( “ S i ParagraphBreak ” ) ; though not always the case , citations often do not cross paragraph boundaries . 3 . 1 . 2 Entity Coherence Feature Sets There is a wealth of literature on various entity and lexical metrics for similarity comparison / relatedness ; we select several of these known for working well in detecting semantic related - ness / coherence , explaining each , including motivation , below . Coreference Features ( C oref ) — It is common to refer to discourse entities using references , such as pronouns or similar nouns ; these tie sentences together that discuss the same topic , and further let the reader know that it is a continuation of the same topic ( s ) already introduced , rather than new ones . For example , take the citation block shown in Fig . 2 : The second sentence uses a pronoun “its” to refer to the “STRAND” system ; with proper knowledge of gender and ani - macy , along with proper resolution rules for addressing distances between initial mention and subsequent references , a coreference classiﬁer can identify that “its” here refers to “STRAND” ( in - stead of another entity in an earlier sentence , or “web” or just the generic “system” mentioned in the copula ) . Coreference features look promising for CBD because they c (cid:2) 2016 Information Processing Society of Japan 543 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) may have the potential to track the appearance and disappear - ance of speciﬁc entities in a text through their mentions ; this is important since when you cite something you also attach it to one or more mentions ( such as in Fig . 2 , the noun “STRAND” ) . As the surface forms may vary from mention to mention ( e . g . , “STRAND” and “Its” ) , simple bag - of - words approaches will not capture these transitions . Previous work , using an algorithmic approach , have utilised coreference information to moderate success to perform the detection of citing sentences [ 28 ] . They noted coverage is - sues of the coreference resolution system as a main shortcom - ing of this approach , from which this feature set will also likely su ﬀ er . We adopt their method as a basis for several coreference features as follows . We can look for coreference links between two sentences S i and S j ( “ S i toS i - 1 HasCoref ” and “ S i toS A HasCoref ” ) , as well as unbroken chains between S i and S A ( “ S i toS A HasCorefPath ” ) . As some phrases are more likely can - didates for citation - related coreference than others , such as “work nouns” as deﬁned by Ref . [ 60 ] , we can also emit binary and tem - plate features when these are encountered in the anaphor po - sition ( “ S i HasWorkNounAnaphor ” and “ S i WorkNounAnaphor ” , re - spectively ) . Citation Features ( C it ) — Citation features exploit speciﬁc knowledge about how citations are realised lexically . Speciﬁcally , citations may mention authors by name , and may continue to use the author’s name in subsequent sentences describing a method or other ﬁndings . Further , the occurrence of another citation is a good indicator that one citation ends and another begins ( though this is not necessarily the case , see Ref . [ 27 ] ) . Utilising citing sentences from other papers citing the same target , in a lateral manner , we can ﬁnd often cited concepts , i . e . , lexical hooks [ 2 ] , that act as indicators , such as a system name “STRAND” , or a method “CRF” ; this allows us to detect a citing sentence even if such a lexical hook was not present in one anchor sentence , as long as it is present in another . As these features target speciﬁc aspects of citations , it is ex - pected that they would perform fairly well ; however , one question is whether they alone will be able to compete in coverage within other coherence feature sets . The features used for this category are adapted from previ - ous work [ 2 ] , and presented in Table 1 . The ﬁrst ﬁve listed in the table under the citation feature set ( C it ) are explicitly bound to citation anchor and anchor sentence phenomena ( exis - tence of citation anchor , author name / year , and so on ) . The ﬁnal three ( “ S i StartsWithConnective ” , “ S i HasDeterminer + WorkNoun ” , and “ S i StartsWith3rdPersonPronoun ” ) are in some respects related to discourse ( D is ) and coreference ( C oref ) feature sets , but are more surface - form , i . e . lexically motivated , as they relate directly to the continuation of citing sentences , and are thus left in this category in line with the baseline . Entity Grid Features ( E - grid ) — Entity grids [ 3 ] , [ 34 ] repre - sent all the grammatical transitions of nouns in a document ( or portion of text ) between four di ﬀ erent grammatical roles : S ub - ject ( S ) , O bject ( O ) , O ther ( X ) , and N one , i . e . , “not present” ( - ) . These provide information on , for example , how likely a subject of a sentence is to transition to an object role in a subsequent Fig . 3 Entity grid for sentences in Fig . 1 . sentence . This seems promising for identifying citing sentences because it may allow the classiﬁer to learn what series of transitions indi - cate citing sentences . Figure 3 shows an example of an entity grid using the sentences from Fig . 1 ; notice that in this case , sentence ( 3 ) , which is not part of the citation block , has no overlapping entities . In this case , unfortunately neither does sentence ( 2 ) . We can emit the role transitions for appearing entities across two sentences ( e . g . , S i - 1 and S i ) to capture these transitions ( “ S i + S i - 1 EgridDi ﬀ ” ) , e . g . , in Fig . 3 , from sentence ( 0 ) to sentence ( 1 ) , “ discourse ” has the transition “ - S” , indicating that it went from not being mentioned in sentence ( 0 ) to appearing as a S ub - ject in sentence ( 1 ) . We can further compute an overall score for a portion of text to estimate its coherence as deﬁned by Ref . [ 3 ] ( “ S i toS A EgridCoherence ” ) . The coherence score P coherence ( T ) for a given text T is given by : P coherence ( T ) ≈ 1 m × n m (cid:2) j = 1 n (cid:2) i = 1 log P role ( r i , j | r ( i − h ) , j . . . r ( i − 1 ) , j ) , ( 1 ) where n is the number of sentences , m is the number of uniquely identiﬁed entities occuring across those sentences , and h is the size of the history for computing compound role transition probabilities ; r represents one of the four possible roles , with P role ( r i | r ( i − h ) ) providing the probability of the transition . N - gram Features ( N - grams ) — N - grams have been employed in a variety of NLP tasks [ 6 ] . N - grams are realised as binary fea - tures of 1 to 3 word grams ( i . e . , N = 3 ) . As N - grams capture word occurrence , a classiﬁer may learn that a word or words are good cues for a citing sentence . However , N - grams are also noisy and of high - dimension , so unlike some of the other lexical coherence feature sets , it is expected that their precision may be lower . Pointwise Mutual Information Features ( PMI ) — PMI [ 11 ] is a measure of how likely two words are to cooccur ; as such if the actual score is less than the expected score negative PMI scores can result . Whereas with N - grams any cooccurrence within a sen - tence must be implicitly learned by the classiﬁer , PMI allows us to precompute coocurrence probabilities between words explicitly ; further , it gives us freedom on how we deﬁne what coocurrence means . Since for CBD we are interested in subsequent sentences fol - lowing on from the anchor sentence , we can deﬁne a cooccurence in the PMI context as words appearing in adjacent sentences ( and not in the same sentence ) . This follows from the intuition that if a c (cid:2) 2016 Information Processing Society of Japan 544 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) certain word appears in one citing sentence , then a known related word appearing in the following sentence is a good indicator of the citation continuing . In order to use PMI scores as features for the classiﬁer , sim - ilar to Refs . [ 41 ] and [ 55 ] , we deﬁne the formula for computing similarity between two sentences S i and S j using PMI as : max sim 1 ( S i , S j ) = (cid:3) w k ∈ S i max w l ∈ S j ( pmi ( w k , w l ) ) × id f ( w k ) (cid:3) w k ∈ S i id f ( w k ) ( 2 ) max sim 2 ( S i , S j ) = (cid:3) w l ∈ S j max w k ∈ S i ( pmi ( w k , w l ) ) × id f ( w l ) (cid:3) w l ∈ S j id f ( w l ) ( 3 ) sim pmi ( S i , S j ) = 1 2 × (cid:4) max sim 1 ( S i , S j ) + max sim 2 ( S i , S j ) (cid:5) , ( 4 ) where , id f ( w ) is the inverse document frequency [ 57 ] of word w in the corpus , and we deﬁne pmi ( w i , w j ) as : pmi ( w i , w j ) = log P ( w i | w j ) P ( w i | ∗ ) × P ( ∗ | w j ) normalisation (cid:6) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:7)(cid:8) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:9) × − log P ( w i | w j ) , ( 5 ) where P here is the probability of w i occurring in the sentence af - ter w j ; we normalise the scores to a range of - 1 ( completely inde - pendent ) to 1 ( completely dependent ) . Note that by our deﬁnition of pmi , the score is asymmetric ( which is not always the case ) , i . e . , pmi ( w i | w j ) (cid:2) pmi ( w j | w i ) , and by extension , sim pmi ( S i , S j ) (cid:2) sim pmi ( S j , S i ) . Breaking the symmetry of pmi attempts to capture the notion that when citing , certain words coming after others is more likely a signal than the other way around . The general in - tuition behind sim pmi is that sentences that are more similar with more uniquely occuring words will be voted as more similar than sentences that do not . We capture the highest scoring word pair between two sen - tences using sim pmi and encode it in “ S i + S i - 1 PmiSimilarityScore ” . Topic Model Features ( TM ) — Topic models * 6 ( TM ) are es - sentially a set of latent groups ( i . e . , “topics” ) of words that rep - resent how often each word appears with another ; each word has a distribution over the set of these latent topics ; two words may belong to the same topic but never cooccur with one another , only occurring with other mutual words . For example , we might learn that “corpus construction” and “corpus creation” are related de - spite not occurring together but instead with a third word , “anno - tation . ” This may be useful for CBD because there may be heavily re - lated words across sentences that despite the vernacular changing , are still discussing the same thing . We compute the cosine simi - larity between the vectors of topic distributions for two sentences with features “ S i + S i - 1 TopicsCosine ” and “ S i + S A TopicsCosine ” , the number of overlapping topics that exceed a threshold * 7 with fea - tures “ S i + S i - 1 NumMutualTopics ” and “ S i + S A NumMutualTopics ” , * 6 For an excellent overview on topic models , see Ref . [ 8 ] . * 7 We set this to 0 . 7 ; as a word has a distribution over all topics , it is impor - tant to eliminate those for which it is not very representative , or we will be comparing topics from two sentences for words that share a common topic , even if only marginally ; to this end we selected 0 . 7 to insure the word is representative of the topic , but not altogether isolate within it , which may happen for higher values approaching 1 . as well as the actual topics with “ S i + S i - 1 MutualTopics ” and “ S i + S A MutualTopics ” . The “ S i toS A TopicsCosineBlock ” feature computes the cosine from a sentence S i pairwise with all pre - ceding sentences within the citation block , e . g . , for the 3rd sen - tence following an anchor sentence , it would compute ( 3rd , 2nd ) , ( 3rd , 1st ) , ( 3rd , Anchor ) ; “ S i toS A TopicsCosinePath ” computes the cosine pairwise from sentence S i up to S A , e . g . , ( 3rd , 2nd ) , ( 2nd , 1st ) , ( 1st , Anchor ) . “ S i toS A TopicsCosineBlock ” estimates how much the topic has shifted since the anchor sentence , while “ S i toS A TopicsCosinePath ” how continuously the topics have over - lapped from the anchor sentence to sentence S i . 4 . CBD Experiments We perform two experiments as follows ; experiment 1 ( Sec - tion 4 . 2 ) assesses the performance of di ﬀ erent single coherence feature sets as described in Section 3 . 1 ; from this , experiment 2 ( Section 4 . 3 ) assesses the most promising combinations of these feature sets . The section for each experiment contains an in - depth analysis of ﬁndings ; we follow the experiments with a uniﬁed dis - cussion and further error analysis in Section 4 . 4 . Following the precedence of previous research [ 2 ] upon which the baseline ( see below ) is adapted , we begin by building models using SVM [ 64 ] . We can think of this approach as sentence - wise classiﬁcation , since each sentence is analysed one at a time in re - lation to being part of a given citation block . However , as the def - inition of citation blocks reveals ( Section 2 . 1 ) that the identiﬁca - tion of citations is heavily dependent on the previous sentence for context , incorporation of previous / next information seems likely to be important for identifying subsequently citing sentences . In a sentence - wise classiﬁcation scheme like with SVM , this kind of information can be encoded using S i − 1 type features ( where S i represents features for a sentence being classiﬁed ) , but does not ultimately take into account whether the previous sentence was deemed to be part of the citation or not . We can , however , di - rectly model the decision of previous citing sentences ; to do this , we propose the use of a CRF [ 33 ] model for this , which can be expected to perform better than SVM . 4 . 1 Experimental Setup Here we describe the tools and libraries used in our experi - ments , as well as corpus composition , scoring , and baselines . 4 . 1 . 1 Tools and Libraries The following tools / libraries are used : • Topic models We use the MALLET [ 40 ] toolkit , which im - plements topic modeling using LDA [ 9 ] . • CRF We use the FACTORIE library [ 39 ] to build the linear - chain CRF . • SVM We use the WEKA library [ 21 ] for training SVM clas - siﬁers . • Coreference Resolution After performing an adhoc as - sessment * 8 of a number of coreference systems for CBD , namely , BART ( versions 1 and 2 ) [ 65 ] , LBJ [ 5 ] , and IMS [ 7 ] , we selected IMS as it performed the best . The IMS system scored between 61 . 24 and 74 . 33 ( CoNLL and mention de - * 8 We do not have coreference annotations for our corpus , so this assess - ment is an informal one . c (cid:2) 2016 Information Processing Society of Japan 545 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) tection evaluations , respectively ) on the CoNLL 2012 shared task test set ( which is composed of newswire and broadcast news data ) ; it was the best scoring , publically available sys - tem from the 2012 CoNLL Shared Task . Coreference has and continues to be predominately focused on the newswire domain . However , there has been recent in - terest in extending its use to academic texts ; Ref . [ 54 ] reports that the IMS system as trained with newswire data , when ap - plied to the computational linguistics domain , the same do - main as in our experiments , scores 40 . 30 ( over 33 % drop ) for the CoNLL evaluation ; augmenting the original newswire data with a small set of coreferentially annotated academic texts improves performance to 47 . 44 . This is the best ( and only ) known work automatically identifying coreferences in academic texts ; we use their augmented data when training the model for the IMS system used in our experiments . • Discourse Parsing For this we use the PDTB Parser [ 36 ] . It has been trained on the Penn Discourse TreeBank ( PDTB ) [ 49 ] , which is composed of newswire articles ( sim - ilarly to coreference corpora ) * 9 . Evaluation of discourse parsing is more complicated than coreference , as there are many parts involved in discourse structure that can be com - pared in di ﬀ erent ways * 10 . However , most pertinent to our research , the accuracy of the PDTB parser for identifying connectives ( e . g . , “while” ) that are actually serving as dis - course connectives is 96 . 02 . For classifying the relation - types between two arguments ( i . e . , spans of text ) , the parser scored 81 . 19 / 80 . 04 / 80 . 61 ( P / R / F 1 ) for explicit relations , and 24 . 54 / 26 . 45 / 25 . 46 for implicit . However , the authors report that human agreement is only 84 % , so the system may be only a few points shy of the upperbound for explicit rela - tions in the trained domain . There are as of yet no known published studies on using the parser on academic texts . 4 . 1 . 2 Corpus We extend the corpus originally presented in Ref . [ 1 ] as fol - lows . The corpus has been converted to XML , various conver - sion artifacts from the PDF - to - text process have been remedied , and some formatting restored , in addition to abstracts and publi - cation years added as meta data . See end of paper for download details . The original corpus had no distinction of individual citation blocks and allowed for reuptake anywhere in the running text if certain salient words were present , such as the name of a method ( e . g . , “CRF” ) , irrespective of its appearance in a table header , etc . As our deﬁnition disallows non - local reuptake these instances have been removed . The corpus is a collection of 1 , 034 papers citing a total of 20 cited papers , averaging 51 citing papers per cited paper . For each of the 20 cited works , only the citations citing that work are anno - tated . Note that in the corpus , roughly two - thirds of citations are single sentence citations ( 1 , 198 of 1 , 651 ) , making distinguishing * 9 We provide a discussion ( Section 4 . 4 ) with examples of where the dis - course parser performed well and poorly in our domain . * 10 In the PDTB , discourse relations are composed of a relation - type and two arguments , arg1 and arg2 , which have the given relation between them ; explicit relations have a connective serving as the indicator whereas im - plicit relations do not ; see Ref . [ 36 ] for more details . Fig . 4 How exact match is computed , visually . these from multi - sentence citations crucial to a model’s success . There are 738 non - anchor citing sentences in the corpus . 4 . 1 . 3 Scoring To score the performance of a model , we compute the preci - sion , recall , and F 1 scores , as well as tally the number of true positives ( TPs ) and false positives ( FPs ) , all sentence - wise , i . e . counted per non - anchor citing sentence , for a normalised range of 6 sentences * 11 from each block anchor sentence ; note that ci - tation blocks of size 1 ( single sentence citations ) only introduce the possibility for FPs , as there are no TPs present within the fol - lowing 6 - sentence window . We further add a column to the results that shows the pro - portion of exact matches for citation blocks , i . e . , the number of citation blocks which a model predicted without any error ( see Fig . 4 ) ; this is in e ﬀ ect accuracy at the block - level * 12 ; for exam - ple , for blocks of one sentence ( anchor sentence only ) , models that did not output any FPs would score 1 ( YES ) ; similarly , for blocks of 4 citing sentences , models outputing any FPs or FNs would result in 0 ( NO ) . Note that as the ratio of single sentence to multi - sentence citation blocks is 1 , 198 / 1 , 651 ( i . e . , 0 . 726 ) a model that never detects any non - anchor citing sentences would achieve 0 . 726 for exact match , but as it ﬁnds no TPs for non - anchor sen - tences , which is indeed what we are interested in ﬁnding , 0 for recall . 10 - fold cross - validation is used for evaluating all models in this work ; as the corpus is a collection of 1 , 034 citing papers grouped by 20 cited ( target ) papers , this equates to 10 folds of 18 - 2 ( train - test ) pairs , averaging 931 - 103 ( train - test ) citing papers per fold . By splitting data for training / testing in this manner , note that the clusters in each fold used for testing contain citation blocks for cited papers entirely unseen during training ( Fig . 5 illustrates this premise ) . Scores are computed once on the aggregate set of all test instances , i . e . sentences ( collected from all folds ) , as is typical for computing per - instance scores ( micro - averages ) . 4 . 1 . 4 Baselines We create a pseudo - random method , implemented by approx - imating citation block length ( in sentences ) , drawing random numbers from their distribution within the corpus to determine the length of a citation block . The features for the baseline are adapted from the system de - scribed in Ref . [ 1 ] , designed for the joint task of detecting senti - * 11 A range of 6 sentences was selected based on the distribution of block length , insuring 90 % of citation content was preserved . * 12 Note that all other metrics shown in the tables are sentence - level . c (cid:2) 2016 Information Processing Society of Japan 546 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) Fig . 5 Cross - validation , shown visually with only 3 folds for simplicity . Table 2 Experiment 1 Results : Performance of various stand - alone textual coherence feature sets . Features P R F 1 TP FP Exact Random . 125 . 243 . 165 183 1277 . 168 S V M Baseline . 553 . 256 . 350 189 153 . 715 N - grams . 402 . 119 . 184 88 131 . 707 C it . 543 . 267 . 358 197 166 . 705 L oc . 000 . 000 . 000 0 3 . 724 D is . 259 . 030 . 053 22 63 . 711 C oref . 363 . 039 . 071 29 51 . 709 E - grid . 179 . 088 . 118 65 298 . 618 PMI . 318 . 009 . 018 7 15 . 723 TM . 289 . 015 . 028 11 27 . 717 CR F Baseline . 584 . 435 . 498 321 229 . 710 N - grams . 563 . 337 . 422 249 193 . 709 C it . 720 . 320 . 443 236 92 . 726 L oc . 734 . 243 . 365 179 65 . 726 D is . 500 . 351 . 412 259 259 . 688 C oref . 737 . 247 . 370 182 65 . 727 E - grid . 740 . 224 . 343 165 58 . 726 PMI . 666 . 270 . 384 199 100 . 719 TM . 714 . 136 . 228 100 40 . 723 ment in citing sentences ; as evaluation methods di ﬀ er * 13 , we ver - iﬁed equivalent performance between implementations * 14 . Fea - tures in Table 1 marked with † are features used by this baseline . As the original work upon which this baseline was adapted used an SVM classiﬁer , we show numbers for SVM in the experimen - tal results for comparison . Note that the baseline is essentially composed of both citation - speciﬁc features ( C it ) and N - gram fea - tures ( N - grams ) , as deﬁned above in Section 3 . 1 . 4 . 2 Experiment 1 : Individual Coherence Feature Sets We ﬁrst train models using individual coherence feature sets with both SVM and CRF ; the results are shown in Table 2 . We can ﬁrst observe that as expected , performance improves with CRF over SVM using the same features , the baseline’s F 1 score improving over 0 . 14 points ( 40 % lift ) by this alone . This trend can continue to be observed for the other coherence feature sets * 13 The work did not discriminate between separate anchors for the same target paper , and treated many nominal phrases occurring throughout a text as implicit reuptake , such as the occurrence of the phrase “BLEU” when the target was “ [ 47 ] ” , which introduces the BLEU score ; our deﬁ - nition for CBD is much stricter , disallowing this kind of interpretation . * 14 Reference [ 1 ] reported an F 1 score of 0 . 513 , and our implementation , us - ing the same data and following the same task and evaluation as deﬁned by him , scored 0 . 517 . as well . As a CRF models previous sentence decisions directly ( i . e . , whether the sentence was deemed a citation or not ) , in addi - tion to previous sentence features , this is reasonable ; it means that information about a previous sentence is useful in determining if a citation continues or terminates . It is interesting to note that the coherence feature sets perform so poorly with SVM . This is likely for the same reason that they work well with CRF ; training with examples sentence - wise does not capture su ﬃ cient context , even with previous and next features , as they do not capture the decision of previous sentence . The remainder of this section will focus on the results for the CRF models . Notice that the pseudo - random method does not perform well , indicating that sentences are not randomly distributed but follow some rules that dictate their occurrence . Due to having high precision with moderate recall , the citation ( C it ) features achieved the highest F 1 score of single coherence feature sets . ( Note that while the baseline here obtained the high - est F 1 score , it is actually composed of both N - grams and C it feature sets , so the comparison is not a fair one ; we list it in the table only so its performance may be referenced . ) Second to this is N - grams , followed closely by D is . Investi - gating the overlap in the TPs ( true positives ) of each , however , we ﬁnd that they are not identifying entirely the same citing sen - tences . Speciﬁcally , D is identiﬁes 99 TPs that N - grams does not , and conversely , N - grams identiﬁes 89 that D is does not . Further , D is identiﬁes 100 TPs that C it does not . In fact , D is identiﬁes 54 TPs that no other feature set detected at all , the highest of all feature sets ; this is reasonable , as D is captures general transitions in the ﬂow of the text , i . e . , all * 15 discourse transitions within the doc - ument ; what this means is that there is not necessarily a special set of transitions that is only found around citation anchors ; this is also corroborated by D is ’s lower precision and higher number of FPs ( false positives ) . Sorting through these FPs , we discover that about a third ( 87 ) contain references to “we” or “our” , and 40 contain another cita - tion anchor ( 17 of which overlap with the above mentioned ﬁrst person pronoun FPs ) . Though not all sentences with ﬁrst person pronouns are guaranteed to be non - citing sentences , features that capture these two aspects ( ﬁrst person pronouns and presence of another citation anchor ) should drastically improve precision for D is . PMI has over a hundred TPs that TM was not able to iden - tify ; though with proper modiﬁcation of topic model parameters , such as number of topics , it may be possible to boost TM perfor - mance , the current shortcoming intuitively makes sense , as topic models are a kind of abstraction , or smoothing of PMI . Retaining the lexical information that PMI utilises prevents loss of salient information as we see with TM . The coreference ( C oref ) feature set unfortunately su ﬀ ered from recall , likely because the underlying coreference resolver was unable to ﬁnd many of the existing coreference chains present in the text ; this is a result of not having much corefer - ence training data for research papers . The entity - grid ( E - grid ) , * 15 Limited to , of course , the transitions that the discourse resolver can iden - tify . c (cid:2) 2016 Information Processing Society of Japan 547 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) Table 3 Experiment 2 Results : Performance using CRF of various combinations of textual coherence feature sets with the baseline’s citation coherence feature set . Features P R F 1 TP FP Exact Random . 125 . 243 . 165 183 1277 . 168 Baseline ( i . e . C it + N - grams ) . 584 . 435 . 498 321 229 . 710 C it . 720 . 320 . 443 236 92 . 726 2 - s e t C it + L oc . 721 . 382 . 500 282 109 . 732 C it + D is . 674 . 398 . 501 294 142 . 724 C it + C oref . 721 . 354 . 475 261 101 . 727 C it + E - grid . 757 . 321 . 451 237 76 . 730 C it + PMI . 668 . 455 . 541 336 167 . 733 C it + TM . 668 . 358 . 466 264 131 . 721 3 - s e t C it + L oc + D is . 636 . 405 . 495 299 171 . 719 C it + L oc + C oref . 675 . 431 . 526 318 153 . 729 C it + L oc + E - grid . 690 . 367 . 479 271 122 . 723 C it + L oc + PMI . 659 . 481 . 556 355 184 . 735 C it + L oc + TM . 673 . 362 . 470 267 130 . 726 which in our implementation only uses lexical forms of entities to determine them , also su ﬀ ers from this same problem ; incorpora - tion of references would likely improve its recall as well . As the baseline , which combined N - grams with C it features , achieved the highest F 1 score , we next perform an experiment combining the citation features with di ﬀ erent coherence feature sets to see how it impacts performance . As SVM did not perform well , we show only CRF results in the subsequent experiment . 4 . 3 Experiment 2 : Combined Coherence Feature Sets Here we investigate the interplay of coherence feature sets by building models with di ﬀ erent combinations ; results are shown in Table 3 . Since the C it feature set performed best in experiment 1 , we use it as a base for 2 - set combinations * 16 . As will be ex - plained below , we use C it + L oc as a base for 3 - set combinations . Without exception all combinations improve F 1 score , with the C it + L oc + PMI combination yielding the highest results , ≈ . 5 points ( 10 % ) improvement over the baseline . C it + * combina - tions all identiﬁed from 50 to 80 + TPs that the baseline did not identify ( though , conversely , the baseline also identiﬁed 80 + that coherence feature sets did not ) ; of those unique to coherence fea - ture sets , many overlapped across other coherence feature sets . As can be seen by looking at the results for C it + L oc in Ta - ble 3 , simply classifying where in the document the citations ap - pear boosts recall by 0 . 06 points without harming precision ; this shows the importance of citation style by where in a paper a cita - tion appears . Further C it + L oc has 52 TPs not identiﬁed by D is , indicating that indeed paper section location plays a key role in detection of citation blocks . C it + L oc and C it + D is both have similar F 1 to the baseline , but with slightly lower recall while obtaining higher precision . Here again C it + D is manages 84 TPs not found by the baseline , and 64 not found by C it + L oc , showing the importance of discourse structure even in tandem with C it features . C it + PMI and C it + TM perform similarly with respect to pre - cision , but C it + PMI obtains markedly higher recall ; this is for the same reason as with the single feature set experiment from Section 4 . 2 ; however , di ﬀ erent from the single feature set exper - * 16 Inclusion of all baseline features decreased performance across the board for all combinations ; this is likely due to the poor precision of N - grams . iment , C it + PMI and C it + TM di ﬀ er much more in overlapping TPs and FPs , indicating interesting interplay at work . As C it + L oc only boosted recall without harming precision , we use it as a base for the 3 - set combination models , where C it + L oc + PMI scores the highest F 1 . Unfortunately , without a feature or features to discriminate against ﬁrst person pronoun sentences that are not citing sentences , any combination with D is seems to su ﬀ er from a high number of FPs and subsequently lower precision . We experimented with 4 - set combinations and more , but as each feature set brings with it its own set of FPs not present in other sets , the interplay is such that precision continues to drop as more are combined . 4 . 4 Discussion The combination of feature sets shows improvement over in - dividual feature sets , including up to a 10 % lift over the base - line in F 1 when using CRF , and ≈ 60 % improvement over the original baseline using SVM ; in particular , we see that D is - based models have a large set of unique TPs that they alone captured , showing the promise of coherence - based methods . Unfortu - nately , the richer coherence feature sets are not exhaustive , in - cluding , for example , the shortcoming of coreference - chain de - tection which limits coreference features , and , subsequently , any more advanced version of the entity - grid . For all models , with only one exception * 17 , more than half of all FPs were triggered by single - sentence citations ( i . e . , citation anchor sentence only ) , speciﬁcally for the sentence immediately following the anchor sentence . These FPs can be categorised into four types : ( 1 ) key terms such as method names from several citation an - chors in the same sentence get conﬂated and these key terms for other anchors are matched in subsequent sentences ( Fig . 6 ) ; ( 2 ) the author is discussing various similar research and as a result very similar terminology is used for all sentences ( Fig . 7 ) ; ( 3 ) a citation is used to further an author’s claim about a topic and appears mid - discourse about that topic ( Fig . 8 ) ; * 17 Only E - grid did not misﬁre on the sentence following single - sentence citations . c (cid:2) 2016 Information Processing Society of Japan 548 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) Fig . 6 Type ( 1 ) FP : Conﬂating unrelated anchor sentence terms . Fig . 7 Type ( 2 ) FP : Similar terminology used . Fig . 8 Type ( 3 ) FP : Mis - classiﬁed sentence after anchor . Fig . 9 Type ( 4 ) FP : Mis - classiﬁed sentence after anchor . ( 4 ) without a wider view of context it is di ﬃ cult to say if a sen - tence is in fact a citing sentence or not ( Fig . 9 ) . Type ( 1 ) suggests features with sub - sentential awareness are needed ( the terms in bold show the terms acting as distractors ) ; more than half ( 56 % ) of anchor sentences contain distractor an - chors , making distinguishing between them an important task for future work . Type ( 2 ) may be the most di ﬃ cult group of FPs to address , as a deep understanding of the discourse is required to untwine these . However , types ( 3 ) and ( 4 ) are the most intriguing ; an exam - ple of each is given in Fig . 8 and Fig . 9 , respectively , where each shows an anchor - sentence only citation block that had its follow - ing sentence misclassiﬁed as a citing sentence . However , they di ﬀ er in the knowledge necessary to dintinguish the following sentence . For type ( 3 ) , it is clear that the following sentence is not a citing sentence , though di ﬃ cult to express in terms of lexically - motivated features ( one idea may be to use the length in number of citation anchors the anchor appears in to discriminate these ) . For type ( 4 ) , “the approach” ( shown in bold ) in fact refers to an approach introduced several sentences prior to the anchor , but due to the ambiguitiy of phrases like “the approach” it is di ﬃ cult Fig . 10 Example of coreference - chains . Fig . 11 Example of unfound coreference “representing words” ⇐ “these representations” . to tell what its antecedent is without seeing this larger context . Moving on to an analysis of false - negatives ( FNs ) for coreference - based models , over a fourth of FNs contained pro - nouns that the coreference - system failed to identify , with roughly a third going to each of the three pronouns “their” , “they” , and “it” ( for an example see sentence ( 4 ) of Fig . 10 ) . Phrases contain - ing common determiners for indicating coreference ( i . e . , “both” , “such” , “this” , “these” , “those” ) measured almost half of all FNs , and phrases containing “the” plus the headwords of these phrases contained another ﬁfth ; if we further match against these head - words without determiners we capture another fourth . Though some overlap exists between these groups , and indeed not all of these are guaranteed to be coreferences related to the anchor , we are left with only a tenth of FNs not falling into any of these pre - vious groupings ; in addition , this latter group contains in many cases associative / bridging relations [ 37 ] , [ 48 ] between phrases ( e . g . , sentences ( 1 ) and ( 2 ) of Fig . 10 have an example of this with “linguistic knowledge” ⇐ “Grammatical patterns” ) . This breakdown shows the overwhelming prevalence of missed coref - erences among FNs . Though resolution of associative / bridging relations is beyond current state - of - the - art NLP techniques , many of the other cases , which are the majority , seem more promising . For example , we see many coreferences such as “term extraction systems” ⇐ “such systems” ⇐ “term extraction systems” from Fig . 10 that are not overly complicated ( though not identiﬁed by the coreference system ) . Slightly more complicated examples such as “manual ﬁltering” ⇐ “The linguistic ﬁlters” ( Fig . 10 ) and “representing words” ⇐ “these representations” ( Fig . 11 ) contain rephrasing but similar headwords ( also not identiﬁed by the coreference sys - tem , though it did identify “The linguistic ﬁlters used in typical c (cid:2) 2016 Information Processing Society of Japan 549 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) Fig . 12 Positive discourse example of “however” . Fig . 13 Negative discourse example of “however” . term extraction systems” ⇐ “They” ) . As mentioned in Section 4 . 2 , the underwhelming performance of discourse relations can be attributed to the more general na - ture of the ﬂow of discourse . Of 57 connective expressions ( e . g . , “however” ) identiﬁed by the discourse parser , all but 1 * 18 con - tained more negatives than positives , and almost all by a sub - stantial margin . This is the result of distractor anchors , as can be seen by comparing Fig . 12 and Fig . 13 ; notice that in Fig . 12 , the “however” indicates a concession from the previous ( anchor ) sentence , whereas in Fig . 13 the “however” is in relation to the previous sentence , which has introduced a new anchor ( distrac - tor ) and so is not providing a generalisation about several pre - viously mentioned works . The block - level features tracing the transitions from the anchor sentence attempted to remedy these kinds of scenarios , but proved insu ﬃ cient ; a richer awareness of the topics of each sentence , such as through coreference - chains , may be needed here . As a large portion of citing sentences were still not captured by any model ( i . e . , FNs ) , we ran a subsequent experiment in an attempt at distinguishing only between single and multiple sen - tence citation blocks , but as this is essentially only a slightly sim - pler problem than the existing one , none of the current features were adequate and did not perform much better in this exper - iment ; this indicates that identiﬁcation of these single - sentence citation blocks is the most di ﬃ cult part of this task , and should therefore be a focus in future research . 5 . Related Work As far as we know , ours is the only work that exploits cita - tions being a function of discourse to determine their boundaries . There is , however , previous work on ﬁnding citation - related sen - tences as well as non - anchor citing sentences in the running text of research papers . References [ 44 ] , [ 45 ] present a similar task of ﬁnding “related sentences” to a citation anchor ; they use a set of 90 cue - phrases extracted from a set of 100 citation blocks with a simple - matching * 18 Even this one is likely coincidental , as it ( “meanwhile” ) had only a single positive example and no negative examples in the data . algorithm that considers a sentence as a citing one if it is within the same paragraph as the anchor and contains one of the cue - phrases . However , as their task is more general , i . e . , they are looking simply for related content for the sake of creating a re - view article , and not strictly citing sentences , they have many cue - phrases that target sentences describing the paper’s own work , e . g . , “in our work” , “our analysis was” , etc . They reported very high results on their test corpus of 50 citation blocks . We ran a similar experiment using their method and set of cue - phrases on the much larger corpus used in our experiments , but due to the di ﬀ erences in task deﬁnition , along with coverage issues of the list of cue - phrases , it resulted in low numbers ( P / R / F 1 of . 084 / . 407 / . 140 ) . Reference [ 1 ] , from which the baseline was adapted , presents a method for ﬁnding the sentiment of citing sentences within a cit - ing paper in tandem with identifying citing sentences . It uses an SVM classiﬁer . As it has no concrete deﬁnition of what a citation is , and based on its task deﬁnition , seems to include citation re - lated content as well . As deﬁnitions and tasks di ﬀ er , it is di ﬃ cult to make a direct comparison . Reference [ 27 ] present an algorithmic approach to identifying citation blocks using coreference - chains . They report similar coverage issues related to coreference systems and cross - domain adaptation . Reference [ 51 ] use an MRF [ 31 ] model for ﬁnding citing sen - tences by building a model for each cited paper , and using that to ﬁnd potential citing sentences in citing papers ; there is no con - crete deﬁnition of what a citing sentence is , but similar to Ref . [ 1 ] it allows for implicit reuptake anywhere in the document . They are interested in building summaries , such as with Ref . [ 45 ] , and is shown by their use of the F 3 score for evaluation , so ﬁnding related content for maximising recall seems to be a priority . Our work has the advantage that it is generalised , i . e . , a single trained model is used for evaluation of all citing / cited work pairs . 6 . Conclusion and Future Work In this paper we demonstrated that citations , as phenomena of discourse , follow rules of coherence and can be at least par - tially captured using general textual coherence features . Further strengthening this argument , the random method did not perform well ( which is not always the case ) , indicating that citations may not follow a simple distribution . Our results also showed that richer coherence feature sets ( in particular , D is , PMI , and L oc ) outperformed simple lexical co - occurrence ( i . e . , N - grams ) fea - tures , as well as improving C it performance when combined , successfully identifying many TPs that the baseline did not . D is above all others identiﬁed a large set of TPs that no other feature set was able to identify . Our results reveal that the use of CRF over SVM improves performance using the same set of features , indicating its more natural ﬁt to the CBD task . Finally , through an extended set of citation - speciﬁc features , combined with other coherence fea - tures , we achieved higher performance , upwards of 10 % improve - ment , over the baseline based on previous work ( and upwards of 60 % improvement over the original baseline using SVM ) . However , results indicate ample room for improvement in c (cid:2) 2016 Information Processing Society of Japan 550 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) CBD . In particular , the location ( L oc in tables ) feature , a proxy for representing the section of a paper in which a citation appears , demonstrated usefulness by increasing F 1 ( through raising recall ) when combined with other feature sets ; this has further proven useful in other research such as argumentative zoning ( AZ ) for identifying the zoning labels of sentences within a text [ 59 ] . Aug - menting the corpus to properly include section information is therefore one promising direction . Segmenting citations by cita - tion function may also provide a useful dimension for identifying di ﬀ erences across citation features and citation styles [ 60 ] . In addition , as mentioned in Section 4 . 4 , only the entity - grid model was able to properly eliminate non - citing sentences for single - sentence citations ( i . e . , it had no FPs for the sentence fol - lowing the anchor sentence when there was no citing sentence present ) ; improving recall for this method , as well as incorporat - ing proper coreference into entities is a promising area to explore ; to this end , having coreference data for academic texts is a nec - essary ﬁrst step . The discourse ( D is ) feature set had many FPs that were the result of unhandled ﬁrst person pronouns ; augment - ing this feature set in a way to identify these would likely greatly improve precision for this feature set . Lastly , working on detection of single - sentence citations vs . multi - sentence citations is crucial to reducing FPs in all proposed models . Resources Resources used in this work , such as the modiﬁed corpus , are available for download at http : / / www . cl . cs . titech . ac . jp / ˜dain / cbd . Acknowledgments This work has been funded in part by the Microsoft 2008 WEBSCALE grant , as well as by the Intelligence Advanced Research Projects Activity ( IARPA ) via Department of Interior National Business Center ( DoI / NBC ) contract number D11PC20153 . The U . S . Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstand - ing any copyright annotation thereon . Disclaimer : The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the o ﬃ cial policies or endorsements , either expressed or implied , of IARPA , DoI / NBC , or the U . S . Government . References [ 1 ] Athar , A . : Sentiment analysis of citations using sentence structure - based features , Proc . 49th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies — Student Session , HLT - SS ’11 , Stroudsburg , PA , USA , pp . 81 – 87 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 2000976 . 2000991 (cid:10) ( 2011 ) . [ 2 ] Athar , A . and Teufel , S . : Detection of Implicit Citations for Sen - timent Detection , Proc . Workshop on Detecting Structure in Schol - arly Discourse , Jeju Island , Korea , pp . 18 – 26 , Association for Compu - tational Linguistics ( online ) , available from (cid:9) http : / / www . aclweb . org / anthology / W12 - 4303 (cid:10) ( 2012 ) . [ 3 ] Barzilay , R . and Lapata , M . : Modeling Local Coherence : An Entity - based Approach , Proc . 43rd Annual Meeting on Association for Com - putational Linguistics , ACL ’05 , Stroudsburg , PA , USA , pp . 141 – 148 , Association for Computational Linguistics ( online ) , DOI : 10 . 3115 / 1219840 . 1219858 ( 2005 ) . [ 4 ] Bateman , J . and Rondhuis , K . J . : Coherence relations : Towards a gen - eral speciﬁcation , Discourse Processes , Vol . 24 , pp . 3 – 49 ( 1997 ) . [ 5 ] Bengtson , E . and Roth , D . : Understanding the Value of Features for Coreference Resolution , Proc . Conference on Empirical Methods in Natural Language Processing , EMNLP ’08 , Stroudsburg , PA , USA , pp . 294 – 303 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1613715 . 1613756 (cid:10) ( 2008 ) . [ 6 ] Bergsma , S . , Pitler , E . and Lin , D . : Creating Robust Supervised Clas - siﬁers via Web - Scale N - Gram Data , Proc . 48th Annual Meeting of the Association for Computational Linguistics , ACL ’10 , pp . 865 – 874 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / aclweb . org / anthology / P10 - 1089 (cid:10) ( 2010 ) . [ 7 ] Bj¨orkelund , A . and Farkas , R . : Data - driven Multilingual Coreference Resolution using Resolver Stacking , Joint Conference on EMNLP and CoNLL — Shared Task , EMNLP - CoNLL ’12 , Jeju Island , Korea , pp . 49 – 55 , Association for Computational Linguistics ( online ) , avail - able from (cid:9) http : / / www . aclweb . org / anthology / W12 - 4503 (cid:10) ( 2012 ) . [ 8 ] Blei , D . M . : Probabilistic Topic Models , Comm . ACM , Vol . 55 , No . 4 , pp . 77 – 84 ( online ) , DOI : 10 . 1145 / 2133806 . 2133826 ( 2012 ) . [ 9 ] Blei , D . M . , Ng , A . Y . and Jordan , M . I . : Latent Dirichlet Alloca - tion , The Journal of Machine Learning Research , Vol . 3 , pp . 993 – 1022 ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 944919 . 944937 (cid:10) ( 2003 ) . [ 10 ] Bradshaw , S . : Reference Directed Indexing : Redeeming Relevance for Subject Search in Citation Indexes , Proc . 7th European Conference on Research and Advanced Technology for Digital Libraries , ECDL ’03 , pp . 499 – 510 ( online ) , DOI : 10 . 1007 / b11967 ( 2003 ) . [ 11 ] Church , K . W . and Hanks , P . : Word Association Norms , Mutual Information , and Lexicography , Computational Linguistics , Vol . 16 , No . 1 , pp . 22 – 29 ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 89086 . 89095 (cid:10) ( 1990 ) . [ 12 ] Connor , J . O . : Citing statements : Computer recognition and use to im - prove retrieval , Information Processing & Management , Vol . 18 , No . 3 , pp . 125 – 131 ( online ) , DOI : http : / / dx . doi . org / 10 . 1016 / 0306 - 4573 ( 82 ) 90036 - X ( 1982 ) . [ 13 ] Connor , J . O . : Biomedical citing statements : Computer recognition and use to aid full - text retrieval , Information Processing & Manage - ment , Vol . 19 , No . 6 , pp . 361 – 368 ( online ) , DOI : http : / / dx . doi . org / 10 . 1016 / 0306 - 4573 ( 83 ) 90053 - 5 ( 1983 ) . [ 14 ] Deane , P . : A Nonparametric Method for Extraction of Candidate Phrasal Terms , Proc . 43rd Annual Meeting on Association for Com - putational Linguistics , ACL ’05 , Stroudsburg , PA , USA , pp . 605 – 613 , Association for Computational Linguistics ( online ) , DOI : 10 . 3115 / 1219840 . 1219915 ( 2005 ) . [ 15 ] Elkiss , A . , Shen , S . , Fader , A . , States , D . and Radev , D . : Blind men and elephants : what do citation summaries tell us about a research article , Journal of the American Society for Information Science and Technology , Vol . 59 ( 2008 ) . [ 16 ] Garﬁeld , E . : Citation Indexes for Science : A New Dimension in Doc - umentation through Association of Ideas , Science , Vol . 122 , No . 3159 , pp . 108 – 111 ( online ) , DOI : 10 . 1126 / science . 122 . 3159 . 108 ( 1955 ) . [ 17 ] Garﬁeld , E . , Sher , I . H . and Torpie , R . J . : The use of citation data in writing the history of science , Institute for Scientiﬁc Information , Philadelphia , Pennsylvania ( 1964 ) . [ 18 ] Giles , C . L . , Bollacker , K . D . and Lawrence , S . : Citeseer : An auto - matic citation indexing system , International Conference on Digital Libraries , pp . 89 – 98 , ACM Press ( 1998 ) . [ 19 ] Gimpel , K . and Smith , N . A . : Rich Source - side Context for Statis - tical Machine Translation , Proc . 3rd Workshop on Statistical Ma - chine Translation , StatMT ’08 , Stroudsburg , PA , USA , pp . 9 – 17 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1626394 . 1626396 (cid:10) ( 2008 ) . [ 20 ] Grosz , B . J . , Weinstein , S . and Joshi , A . K . : Centering : A Framework for Modeling the Local Coherence of Discourse , Computational Lin - guistics , Vol . 21 , pp . 203 – 225 ( 1995 ) . [ 21 ] Hall , M . , Frank , E . , Holmes , G . , Pfahringer , B . , Reutemann , P . and Witten , I . H . : The WEKA Data Mining Software : An Update , SIGKDD Explorations Newsletter , Vol . 11 , No . 1 , pp . 10 – 18 ( online ) , DOI : 10 . 1145 / 1656274 . 1656278 ( 2009 ) . [ 22 ] Halliday , M . A . K . and Hasan , R . : Cohesion in English ( English Lan - guage ) , Longman Pub Group ( 1976 ) . [ 23 ] Hearst , M . A . : TextTiling : A Quantitative Approach to Discourse Seg - mentation , Technical Report ( 1993 ) . [ 24 ] Hirsch , J . E . : An index to quantify an individual’s scientiﬁc re - search output , Proc . National Academy of Sciences , Vol . 102 , No . 46 , pp . 16569 – 16572 ( online ) , available from (cid:9) http : / / biblioinserm . inist . fr / IMG / pdf / FacteurH . pdf (cid:10) ( 2005 ) . [ 25 ] Hobbs , J . R . : Coherence and Coreference , Technical Report 168 , AI Center , SRI International , 333 Ravenswood Ave . , Menlo Park , CA 94025 ( 1978 ) . [ 26 ] Huang , X . : Planning Argumentative Texts , Proc . 1994 International Conference on Computational Linguistics , COLING ’94 , pp . 329 – 333 ( online ) , available from (cid:9) http : / / dblp . uni - trier . de / db / conf / coling / coling1994 . html # Huang94 (cid:10) ( 1994 ) . c (cid:2) 2016 Information Processing Society of Japan 551 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) [ 27 ] Kaplan , D . , Iida , R . and Tokunaga , T . : Automatic extraction of ci - tation contexts for research paper summarization : A coreference - chain based approach , Proc . 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries , NLPIR4DL ’09 , pp . 88 – 95 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1699750 . 1699764 (cid:10) ( 2009 ) . [ 28 ] Kaplan , D . and Tokunaga , T . : Sighting Citation Sites : A Collective - Intelligence Approach for Automatic Summarization of Research Pa - pers using C - Sites , Proc . 3rd Asian Semantic Web Conference — Workshops , ASWC ’08 ( 2008 ) . [ 29 ] Kessler , M . M . : Bibliographic coupling between scientiﬁc papers , American Documentation , Vol . 14 , pp . 10 – 25 ( 1963 ) . [ 30 ] Kim , S . - M . , Pantel , P . , Chklovski , T . and Pennacchiotti , M . : Au - tomatically Assessing Review Helpfulness , Proc . 2006 Conference on Empirical Methods in Natural Language Processing , EMNLP ’06 , Stroudsburg , PA , USA , pp . 423 – 430 , Association for Computa - tional Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1610075 . 1610135 (cid:10) ( 2006 ) . [ 31 ] Kindermann , R . and Snell , J . L . : Markov Random Fields and Their Applications , AMS ( 1980 ) . [ 32 ] Knott , A . , Oberlander , J . , O’Donnell , M . , Mellish , C . and Mellish , E . M . O . C . : Beyond Elaboration : The Interaction of Relations and Fo - cus in Coherent Text , Text Representation : Linguistic and Psycholin - guistic Aspects , pp . 181 – 196 , John Benjamins ( 2000 ) . [ 33 ] La ﬀ erty , J . , McCallum , A . and Pereira , F . : Conditional random ﬁelds : Probabilistic models for segmenting and labeling sequence data , Proc . 18th International Conference on Machine Learning , ICML ’01 , pp . 282 – 289 , Morgan Kaufmann ( 2001 ) . [ 34 ] Lapata , M . and Barzilay , R . : Automatic Evaluation of Text Coherence : Models and Representations , Proc . 19th International Joint Confer - ence on Artiﬁcial Intelligence , IJCAI ’05 , San Francisco , CA , USA , pp . 1085 – 1090 , Morgan Kaufmann Publishers Inc . ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1642293 . 1642467 (cid:10) ( 2005 ) . [ 35 ] Lauriston , A . : Criteria for Measuring Term Recognition , Proc . 7th Conference on European Chapter of the Association for Computa - tional Linguistics , EACL ’95 , San Francisco , CA , USA , pp . 17 – 22 , Morgan Kaufmann Publishers Inc . ( online ) , DOI : 10 . 3115 / 976973 . 976977 ( 1995 ) . [ 36 ] Lin , Z . , Ng , H . T . and Kan , M . - Y . : A PDTB - Styled End - to - End Discourse Parser , CoRR ( online ) , available from (cid:9) http : / / dblp . uni - trier . de / db / journals / corr / corr1011 . html # abs - 1011 - 0835 (cid:10) ( 2010 ) . [ 37 ] L¨obner , S . : Deﬁnite associative anaphora ( 1998 ) . [ 38 ] Marcu , D . : The Rhetorical Parsing of Unrestricted Texts : A Surface - Based Approach , Computational Linguistics , Vol . 26 , No . 3 , pp . 395 – 448 ( 2000 ) . [ 39 ] McCallum , A . , Schultz , K . and Singh , S . : FACTORIE : Probabilistic Programming via Imperatively Deﬁned Factor Graphs , Neural Infor - mation Processing Systems , NIPS ’09 ( 2009 ) . [ 40 ] McCallum , A . K . : MALLET : A Machine Learning for Language Toolkit ( 2002 ) , available from (cid:9) http : / / mallet . cs . umass . edu (cid:10) . [ 41 ] Mihalcea , R . , Corley , C . and Strapparava , C . : Corpus - based and Knowledge - based Measures of Text Semantic Similarity , Proc . 21st National Conference on Artiﬁcial Intelligence , AAAI ’06 , pp . 775 – 780 , AAAI Press ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1597538 . 1597662 (cid:10) ( 2006 ) . [ 42 ] Mohammad , S . , Dorr , B . and Hirst , G . : Computing Word - pair Antonymy , Proc . Conference on Empirical Methods in Natural Lan - guage Processing , EMNLP ’08 , Stroudsburg , PA , USA , pp . 982 – 991 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1613715 . 1613843 (cid:10) ( 2008 ) . [ 43 ] Nakov , P . I . , Schwartz , A . S . and Hearst , M . A . : Citances : Citation Sentences for Semantic Analysis of Bioscience Text , Proc . SIGIR’04 workshop on Search and Discovery in Bioinformatics ( 2004 ) . [ 44 ] Nanba , H . , Kando , N . and Okumura , M . : Classiﬁcation of Research Papers using Citation Links and Citation Types : Towards Automatic Review Article Generation , Proc . 11th SIG / CR Workshop , pp . 117 – 134 ( 2000 ) . [ 45 ] Nanba , H . and Okumura , M . : Towards multi - paper summarization us - ing reference information , Proc . 13th International Joint Conference on Machine Learning , IJCAI ’99 , pp . 926 – 931 ( 1999 ) . [ 46 ] Pantel , P . : Inducing Ontological Co - occurrence Vectors , Proc . 43rd Annual Meeting on Association for Computational Linguistics , ACL ’05 , Stroudsburg , PA , USA , pp . 125 – 132 , Association for Computa - tional Linguistics ( online ) , DOI : 10 . 3115 / 1219840 . 1219856 ( 2005 ) . [ 47 ] Papineni , K . , Roukos , S . , Ward , T . and jing Zhu , W . : BLEU : A Method for Automatic Evaluation of Machine Translation , pp . 311 – 318 ( 2002 ) . [ 48 ] Poesio , M . and Vieira , R . : A Corpus - based Investigation of Deﬁnite Description Use , Computational Linguistics , Vol . 24 , No . 2 , pp . 183 – 216 ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 972732 . 972733 (cid:10) ( 1998 ) . [ 49 ] Prasad , R . , Dinesh , N . , Lee , A . , Miltsakaki , E . , Robaldo , L . , Joshi , A . and Webber , B . : The Penn Discourse TreeBank 2 . 0 , Proc . 6th Interna - tional Conference on Language Resources and Evaluation , LREC ’08 ( 2008 ) . [ 50 ] Qazvinian , V . and Radev , D . R . : Scientiﬁc Paper Summarization Using Citation Summary Networks ( 2008 ) . [ 51 ] Qazvinian , V . and Radev , D . R . : Identifying non - explicit citing sen - tences for citation - based summarization , Proc . 48th Annual Meet - ing of the Association for Computational Linguistics , ACL ’10 , Stroudsburg , PA , USA , pp . 555 – 564 , Association for Computa - tional Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1858681 . 1858738 (cid:10) ( 2010 ) . [ 52 ] Radev , D . R . , Hovy , E . and McKeown , K . : Introduction to the Special Issue on Summarization ( 2002 ) . [ 53 ] Ritchie , A . , Teufel , S . and Robertson , S . : How to ﬁnd bet - ter index terms through citations , Proc . Workshop on How Can Computational Linguistics Improve Information Retrieval ? , CLIIR ’06 , Stroudsburg , PA , USA , pp . 25 – 32 , Association for Computa - tional Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1629808 . 1629813 (cid:10) ( 2006 ) . [ 54 ] R¨osiger , I . and Teufel , S . : Resolving Coreferent and Associative Noun Phrases in Scientiﬁc Text , Proc . Student Research Workshop at the 14th Conference of the European Chapter of the Associa - tion for Computational Linguistics , Gothenburg , Sweden , pp . 45 – 55 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / www . aclweb . org / anthology / E14 - 3006 (cid:10) ( 2014 ) . [ 55 ] Shrestha , P . : Corpus - Based methods for Short Text Similarity , Re - contre des Etudiants Chercheurs en Informatique pour le Traitement automatique des Langues , Vol . 2 , No . 1 , p . 297 ( online ) , DOI : hal - 00609909 ( 2011 ) . [ 56 ] Small , H . : Co - citation in the scientiﬁc literature : A new measure of the relationship between two documents , JASIS , Vol . 24 , pp . 265 – 269 ( 1973 ) . [ 57 ] Sp¨arck Jones , K . : A statistical interpretation of term speciﬁcity and its application in retrieval , Journal of Documentation , Vol . 28 , pp . 11 – 21 ( 1972 ) . [ 58 ] Teufel , S . , Carletta , J . and Moens , M . : An Annotation Scheme for Discourse - Level Argumentation in Research Articles , Proc . Eighth Meeting of the European Chapter of the Association for Computa - tional Linguistics , EACL ’99 , pp . 58 – 65 ( 1999 ) . [ 59 ] Teufel , S . , Siddharthan , A . and Batchelor , C . : Towards discipline - independent argumentative zoning : Evidence from chemistry and computational linguistics , Proc . 2009 Conference on Empirical Meth - ods in Natural Language Processing , EMNLP ’09 , pp . 1493 – 1502 ( 2009 ) . [ 60 ] Teufel , S . , Siddharthan , A . and Tidhar , D . : Automatic classiﬁcation of citation function , Proc . 2006 Conference on Empirical Methods in Natural Language Processing , EMNLP ’06 ( 2006 ) . [ 61 ] Thompson , S . A . and Mann , W . C . : Rhetorical Structure Theory : A framework for the analysis of texts , IPrA Papers in Pragmatics , Vol . 1 , No . 1 , pp . 79 – 105 ( 1987 ) . [ 62 ] Tomokiyo , T . and Hurst , M . : A Language Model Approach to Keyphrase Extraction , Proc . ACL 2003 Workshop on Multiword Ex - pressions : Analysis , Acquisition and Treatment , MWE ’03 , Strouds - burg , PA , USA , pp . 33 – 40 , Association for Computational Linguistics ( online ) , DOI : 10 . 3115 / 1119282 . 1119287 ( 2003 ) . [ 63 ] Van Gael , J . , Vlachos , A . and Ghahramani , Z . : The Inﬁnite HMM for Unsupervised PoS Tagging , Proc . 2009 Conference on Empirical Methods in Natural Language Processing , EMNLP ’09 , Stroudsburg , PA , USA , pp . 678 – 687 , Association for Computa - tional Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1699571 . 1699601 (cid:10) ( 2009 ) . [ 64 ] Vapnik , V . N . : Statistical Learning Theory , Adaptive and Learning Systems for Signal Processing Communications , and Control , John Wiley & Sons ( 1998 ) . [ 65 ] Versley , Y . , Ponzetto , S . P . , Poesio , M . , Eidelman , V . , Jern , A . , Smith , J . , Yang , X . and Moschitti , A . : BART : A Modular Toolkit for Coreference Resolution , Proc . 46th Annual Meeting of the As - sociation for Computational Linguistics on Human Language Tech - nologies : Demo Session , ACL ’08 , Stroudsburg , PA , USA , pp . 9 – 12 , Association for Computational Linguistics ( online ) , available from (cid:9) http : / / dl . acm . org / citation . cfm ? id = 1564144 . 1564147 (cid:10) ( 2008 ) . [ 66 ] Walker , M . A . , Joshi , A . K . and Prince , E . ( eds . ) : Centering Theory in Discourse , Oxford University Press , Oxford ( 1997 ) . [ 67 ] Weinstock , M . : Citation Indexes , Encyclopedia of Library and Infor - mation Science , Vol . 5 , pp . 16 – 41 ( 1971 ) . [ 68 ] White , H . D . : Citation analysis and discourse analysis revisited , Ap - plied Linguistics , Vol . 25 , No . 1 , pp . 89 – 116 ( 2004 ) . [ 69 ] Zhang , Y . , Wu , K . , Gao , J . and Vines , P . : Automatic Acquisi - tion of Chinese – English Parallel Corpus from the Web , Lalmas , M . , MacFarlane , A . , R¨uger , S . , Tombros , A . , Tsikrika , T . and Yavlinsky , c (cid:2) 2016 Information Processing Society of Japan 552 Journal of Information Processing Vol . 24 No . 3 540 – 553 ( May 2016 ) A . ( eds . ) , Advances in Information Retrieval , Lecture Notes in Com - puter Science , Vol . 3936 , pp . 420 – 431 , Springer Berlin Heidelberg ( 2006 ) . [ 70 ] Ziman , J . M . : Information , Communication , Knowledge , Nature , Vol . 224 , pp . 318 – 324 ( 1969 ) . Dain Kaplan received his Masters and Ph . D . degrees from Tokyo Institute of Technology in 2009 and 2016 , respec - tively ; he worked as a Research Associate at the Computer Laboratory , University of Cambridge from 2012 to 2013 . His in - terests include applied natural language processing ( NLP ) and machine learning ( ML ) , particularly with big data . He enjoys coding in Scala . Takenobu Tokunaga is a Professor at the Department of Computer Science , Tokyo Institute of Technology . He received his B . S . , M . Sc . and Ph . D . degrees from Tokyo Institute of Technology in 1983 , 1985 and 1991 , respectively . His current interests include natural language pro - cessing , in particular , building and man - aging language resources , applications of language technologies to intelligent information access and education , and dialogue sys - tems . Simone Teufel is a Reader in Language and Information at the University of Cambridge . She received her Masters ( CS , Stuttgart University ) in 1994 and her Ph . D . ( Cognitive Science , University Edinburgh ) in 2000 . Her research inter - ests lie in discourse - based summarisation and in information access to scientiﬁc ar - ticles . c (cid:2) 2016 Information Processing Society of Japan 553