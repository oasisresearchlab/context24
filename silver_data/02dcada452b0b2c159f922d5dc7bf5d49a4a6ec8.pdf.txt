Purposeful AI Tiany Li Purdue University West Lafayette , Indiana , USA li4251 @ purdue . edu Francisco Iacobelli Northwestern University Evanston , Illinois , USA Northeastern Illinois University Chicago , Illinois , USA f - iacobelli @ neiu . edu ABSTRACT The potential of AI to improve people’s lives is unlimited . How - ever , designing AI - infused systems to realize this potential for all is challenging , and it is particularly challenging within under - served minority communities . Revisiting algorithms and data sources to eliminate biases in AI is an important step toward expanding its benefits . However , this leads to the realization that some applica - tions , although unbiased , may simply not be what marginalized communities need . Marginalized communities live in a unique so - cioeconomic and cultural context and have difficulties taking ad - vantage of traditional AI - powered systems due to language , digital literacy , or other barriers . Additionally , the systems that currently exist usually fail to address the unique challenges in these commu - nities . Therefore , designers and researchers in AI must understand that a fundamental dimension of fairness and ethics is to empower and lift the lives of under - served populations through purposeful human - AI research . This research not only recognizes the unique intersectionality and needs present in each community , but also treats them as the primary audience by overcoming traditional barriers between researchers and underserved communities . This SIG proposal aims to initiate a multidisciplinary discussion around the design of AI systems that are purposefully targeted to marginalized populations . Within this discussion , our objective is to better understand how to conduct research to support and serve these communities – particularly if we are not members of such communities ; and challenge the effectiveness of techniques like user - centered design in the context of AI - infused systems when the designer is not a member of the user community . In addition , we will delve into ways of translating research into practical applications to create a positive impact within these communities and narrow the disparities that exist in the optimal utilization of AI to improve lives for different communities . CCS CONCEPTS • Human - centered computing ; • Computing methodologies → Artificial intelligence ; Natural language processing ; Dis - tributedartificialintelligence ; Philosophical / theoreticalfoun - dations of artificial intelligence ; This work is licensed under a Creative Commons Attribution - NonCommercial International 4 . 0 License . CSCW ’23 Companion , October 14 – 18 , 2023 , Minneapolis , MN , USA © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 979 - 8 - 4007 - 0129 - 0 / 23 / 10 . https : / / doi . org / 10 . 1145 / 3584931 . 3606954 KEYWORDS ethics , artificial intelligence , applications , communities , fairness , equity ACM Reference Format : Tiany Li and Francisco Iacobelli . 2023 . Purposeful AI . In Computer Supported Cooperative Work and Social Computing ( CSCW ’23 Companion ) , October 14 – 18 , 2023 , Minneapolis , MN , USA . ACM , New York , NY , USA , 3 pages . https : / / doi . org / 10 . 1145 / 3584931 . 3606954 1 INTRODUCTION The rapid advance in artificial intelligence is bringing about trans - formative , and sometimes disruptive changes across various do - mainsandindustries . GPS - basednavigation , facerecognition , health monitoring and physical training systems , and the recent surge of interest in large language models are just a few examples that are revolutionizing how people navigate through their daily routines . Many AI solutions are powered by the collective information of large communities . For example , recommender algorithms are ca - pable of learning user preferences and interpreting user intent [ 10 ] by analyzing patterns of behavior of many users . This allows indi - viduals to optimize their access , filter , and utilization of relevant information . Artificial intelligence and machine learning are also in - creasingly utilized to enhance learning and education [ 4 ] . Student’s data is computationally analyzed to better understand the learning progress of each individual and provide customized instructional support with AI - assisted evaluation and conversational teaching assistants and learning partners . To improve the performance of AI models , many of these AI systems take into account user feedback through the user interfaces , such as up - voting or down - voting AI - generated predictions , clicking on or dismissing the recommended content , or explicitly submitting feedback to the developers [ 10 ] . All of these advances have transformed people’s lifestyles and have augmented their ability to sustain them . This transformation has come , to some extent , to the detriment of some minority populations [ 2 ] . These are minorities that have been historically discriminated against by their race , culture , or other aspects of their social context , implicitly or explicitly , at dif - ferent stages of the data and machine learning pipelines , and the design and development processes of AI - infused systems [ 1 ] . For ex - ample , biases in data can propagate to the AI models for healthcare systems [ 16 ] , policing [ 3 ] , and university admissions [ 6 ] , to name a few . The response to these shortcomings has sparked an interest in the concept of fairness as applied to artificial intelligence [ 15 ] . This concept deals with the prevention and mitigation of biases in data and machine learning models , and pre - assessing potential harms that can be caused by such systems [ 13 ] . 563 CSCW ’23 Companion , October 14 – 18 , 2023 , Minneapolis , MN , USA Tiany Li and Francisco Iacobelli Despite the ongoing endeavor to enhance fairness and ethics in AI systems , the above - mentioned advances do not necessarily result in evident improvements in the lives of disadvantaged com - munities . For example , automatic speech recognition software fails speakers of African American Vernacular English , due to a lack of cultural context and the ability to recognize prosody and gram - matical structures present in the language [ 9 ] . Moreover , although some individuals adapt their speech to optimize the performance of the Automatic Speech Recognition ( ASR ) systems , they feel frus - trated and angry when using the technology [ 12 ] . Another example is that of communities of low - literacy individuals . They lack the fundamental skills to utilize the AI technologies described above optimally . They lack the reading and writing skills to use many text or voice - based interfaces to their benefit . Their lack of the ability to decode and spell languages , and to articulate their thoughts , leads to a poor experience with both written and voice - based search en - gines [ 8 ] . In addition , insufficient digital literacy also poses hurdles for individuals to comprehend the advantages and appropriate uti - lization of intelligent systems [ 11 ] . Consequently , these individuals are often susceptible to scams , frustration , and mistrust of technol - ogy to the point that they may even discontinue the use of such interfaces altogether [ 19 ] . In sum , the communities described in these examples are poised to fail at utilizing most of these applica - tions , since they are language - based , require high levels of literacy , and / or assume certain social or cultural norms that do not apply to them . These shortcomings suggest that to improve the lives of disadvan - taged communities with AI , the technology must be purposefully aimed at these communities . That is an AI designed from the ground up for these communities . The SIG proposed here aims to make this discussion more salient among the community of researchers in AI and human - centered design . Rather than mitigating existing biases and protecting certain communities as the vulnerable , this SIG revisits the inherent sociotechnical power dynamics between AI system designers and disadvantaged communities , to explore new ways to design AI - infused systems for diverse populations and serve community - specific needs . 2 THE UNIQUE CHALLENGES OF PURPOSEFULLY DESIGNING AI FOR DISADVANTAGED COMMUNITIES Understanding disadvantaged communities and designing technol - ogy for them is far from trivial . Although this has been the intent of participatory design [ 17 ] , research and the design of technology for disadvantaged communities are largely carried out from the perspective of privileged researchers and it “neglects the challenges associated with envisioning equitable design solutions among un - derserved populations” [ 7 ] . In part , this happens due to the narrow lens with which problems are examined . Thus , researchers in eq - uitable AI have called to diversify the disciplines involved in AI research [ 15 ] , and in particular , it is essential to include a multidis - ciplinary team whose individuals are deeply invested in making technology more fair and equitable [ 18 ] . In addition to diversifying teams , the power relationships be - tween those that have been historically marginalized and those with privilege – in this case , the researchers or designers of tech - nology cannot be ignored [ 7 ] . Overcoming this relationship may require becoming more than a researcher in a particular community . It may require becoming a trusted partner and advocate . For ex - ample , Xu et al . examined the role of inclusive survey recruitment methods in reaching marginalized voices [ 20 ] . There is a need for more research efforts like these to understand and analyze the inter - sectionalities present in different marginalized communities , and develop responsible and effective research methods accordingly . Inadditiontotheabove - mentioned structuralprecarity , researchers also face institutional legitimacy when conducting research with un - derprivileged populations [ 5 ] . Because disadvantaged communities often experience unique vulnerabilities , the Institutional Review Boards ( IRB ) are especially strict and request precisely how the interventions will be conducted . However , determining the words , language , gestures , and other outputs generated by an AI and to be shown to the participants is often at odds with the vision of an au - tonomous and non - deterministic system . Moreover , when targeting communities with additional medical conditions , the IRB review process can become even more complex , involving multiple steps such as obtaining medical records and collaborating with medical schools . The few challengesoutlined here require timeand effort that goes beyond that of traditional AI research . First , gathering , interacting , and agreeing with an interdisciplinary team can be challenging and time - consuming . It may not only involve looking for those people , but also mentoring minority students . Second , becoming a trusted partner of the community takes time and an investment in social capital . In many ways , it can be a life choice . Lastly , dealing with institutional barriers can be frustrating and lead to unexpected delays in research . Just like humans need to receive education and training to spe - cialize their skills to interact with these populations , AI algorithms , and datasets need to be prepared from the ground up to intention - ally serve members of these communities . This includes intelligent user interfaces that are culturally appropriate for specific groups , language models specifically designed to understand minority pop - ulations , recommender systems that are less sensationalistic and more considerate and useful towards vulnerable groups , as well as new applications that are uniquely aimed at the needs of specific marginalized communities . 3 RESEARCHER REFLEXIVITY IN PURPOSEFUL AI The design of intelligent user interfaces is especially critical as it is the frontier communication channel between the end users and the AI system . It is important to recognize that the design process is not isolated from a particular social context . Most commonly the con - text of the researchers plays an important role in the methods [ 20 ] and lens [ 14 ] through which observations and interpretations are made . However , with a lack of diversity in researchers and research methods , most of these interfaces have not been designed with diverse populations and social contexts in mind , not to mention purposefully designed for a specific community . While the sub - field of equitable AI has been striving to address some of the biases and unintended consequences of AI algorithms 564 Purposeful AI CSCW ’23 Companion , October 14 – 18 , 2023 , Minneapolis , MN , USA on marginalized populations , there is a need for broader discussions on AI research that is tailored to specific sociocultural contexts and aims to solve unique problems within the intersection of those contexts . The SIG intends to foster a discussion around the purposeful design of AI for marginalized populations . That is researchers , schol - ars , industry practitioners , and designers , who scrutinize the needs of a particular marginalized community and , through community - oriented research and design , produce theoretical models , method - ologies , and intelligent user interfaces that are culturally appropri - ate and cater to the unique needs and intersectional contexts of a specific disadvantaged community . In order to comprehensively address the complex questions at hand , it is imperative to engage a diverse and multidisciplinary group of experts in a holistic exploration of the topic regarding its theoretical underpinnings , human behavior and cognition , techni - cal aspects , and practical implications . The main objective of this SIG is to generate a set of thought - provoking questions that propel the discussion forward , thereby promoting a fruitful exchange of ideas that culminates in the collaborative development of a com - prehensive design and evaluation framework for purposeful AI . We prepared questions along five dimensions that help us initiate the discussion : ( a ) Power : How can researchers from a socially empowered culture understand the problems of and accurately identify solutions for marginalized communities ? ( b ) Trust : What are the limitations of user – centered and participatory design when the users do not trust the designers ? ( c ) Community Impact : How can we translate research into a positive impact for these communities ? ( d ) Sustainability : How can we create a pipeline of diverse researchers to continue working on these problems ? ( e ) Validity : How do we operationalize guidelines and best practices to ensure well - intentioned research can be responsibly conducted and bring culturally relevant outcomes to the target community ? REFERENCES [ 1 ] Abeba Birhane , Elayne Ruane , Thomas Laurent , Matthew S . Brown , Johnathan Flowers , Anthony Ventresque , and Christopher L . Dancy . 2022 . The Forgotten Margins of AI Ethics . 2022 ACM Conference on Fairness , Accountability , and Transparency , 948 – 958 . https : / / doi . org / 10 . 1145 / 3531146 . 3533157 [ 2 ] Joy Buolamwini and Timnit Gebru . 2018 . Gender shades : Intersectional accu - racy disparities in commercial gender classification . In Conference on fairness , accountability and transparency . PMLR , 77 – 91 . [ 3 ] Adriane Chapman , Philip Grylls , Pamela Ugwudike , David Gammack , and Jacqui Ayling . 2022 . A Data - driven analysis of the interplay between Criminological theory and predictive policing algorithms . 2022 ACM Conference on Fairness , Ac - countability , and Transparency , 36 – 45 . https : / / doi . org / 10 . 1145 / 3531146 . 3533071 [ 4 ] Xieling Chen , Di Zou , Haoran Xie , Gary Cheng , and Caixia Liu . 2022 . Two Decades of Artificial Intelligence in Education : Contributors , Collaborations , Research Topics , Challenges , and Future Directions . Educational Technology & Society 25 ( 2022 ) , 28 – 47 . Issue 1 . https : / / www . jstor . org / stable / 48647028 [ 5 ] Rachel Ellis . 0 . What Do We Mean By a “Hard - to - reach” Population ? Le - gitimacy Versus Precarity as Barriers to Access . Sociological Methods & Re - search 0 , 0 ( 0 ) , 0049124121995536 . https : / / doi . org / 10 . 1177 / 0049124121995536 arXiv : https : / / doi . org / 10 . 1177 / 0049124121995536 [ 6 ] Juan E . Gilbert . 2006 . Applications quest . Commun . ACM 49 ( 3 2006 ) , 99 – 104 . Issue 3 . https : / / doi . org / 10 . 1145 / 1118178 . 1118183 [ 7 ] Christina Harrington , Sheena Erete , and Anne Marie Piper . 2019 . Deconstructing Community - Based Collaborative Design . Proceedings of the ACM on Human - Computer Interaction 3 ( 11 2019 ) , 1 – 25 . Issue CSCW . https : / / doi . org / 10 . 1145 / 3359318 [ 8 ] Francisco Iacobelli , Ginger Dragon , Giselle Mazur , and Judith Guitelman . [ n . d . ] . Web - Based Information Seeking Behaviors of Low - Literacy Hispanic Survivors of Breast Cancer : Observational Pilot Study . JMIR Formative Resesarch ( [ n . d . ] ) . [ 9 ] Allison Koenecke , Andrew Nam , Emily Lake , Joe Nudell , Minnie Quartey , Zion Mengesha , Connor Toups , John R . Rickford , Dan Jurafsky , and Sharad Goel . 2020 . Racial disparities in automated speech recognition . Proceedings of the National Academy of Sciences 117 , 14 ( 2020 ) , 7684 – 7689 . https : / / doi . org / 10 . 1073 / pnas . 1915768117 arXiv : https : / / www . pnas . org / doi / pdf / 10 . 1073 / pnas . 1915768117 [ 10 ] Zhaoyang Liu , Haokun Chen , Fei Sun , Xu Xie , Jinyang Gao , Bolin Ding , and Yanyan Shen . 2021 . Intent Preference Decoupling for User Representation on Online Recommender System . In Proceedings of the Twenty - Ninth International JointConferenceonArtificialIntelligence ( Yokohama , Yokohama , Japan ) ( IJCAI’20 ) . Article 357 , 8 pages . [ 11 ] Duri Long and Brian Magerko . 2020 . What is AI Literacy ? Competencies and Design Considerations . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 16 . https : / / doi . org / 10 . 1145 / 3313831 . 3376727 [ 12 ] Zion Mengesha , Courtney Heldreth , Michal Lahav , Juliana Sublewski , and Elyse Tuennerman . 2021 . “I don’t Think These Devices are Very Culturally Sensi - tive . ”—Impact of Automated Speech Recognition Errors on African Americans . Frontiers in Artificial Intelligence 4 ( 11 2021 ) . https : / / doi . org / 10 . 3389 / frai . 2021 . 725911 [ 13 ] Jacob Metcalf , Emanuel Moss , Elizabeth Anne Watkins , Ranjit Singh , and Madeleine Clare Elish . 2021 . Algorithmic Impact Assessments and Account - ability : The Co - Construction of Impacts . In Proceedings of the 2021 ACM Con - ference on Fairness , Accountability , and Transparency ( Virtual Event , Canada ) ( FAccT ’21 ) . Association for Computing Machinery , New York , NY , USA , 735 – 746 . https : / / doi . org / 10 . 1145 / 3442188 . 3445935 [ 14 ] Lizzi Milligan . 2016 . Insider - outsider - inbetweener ? Researcher positioning , par - ticipativemethodsandcross - culturaleducationalresearch . Compare : AJournalof Comparative and International Education 46 , 2 ( 2016 ) , 235 – 250 . https : / / doi . org / 10 . 1080 / 03057925 . 2014 . 928510 arXiv : https : / / doi . org / 10 . 1080 / 03057925 . 2014 . 928510 [ 15 ] Deirdre K . Mulligan , Joshua A . Kroll , Nitin Kohli , and Richmond Y . Wong . 2019 . This Thing Called Fairness . Proceedings of the ACM on Human - Computer Interac - tion 3 ( 11 2019 ) , 1 – 36 . Issue CSCW . https : / / doi . org / 10 . 1145 / 3359221 [ 16 ] Ziad Obermeyer , Brian Powers , Christine Vogeli , and Sendhil Mullainathan . 2019 . Dissecting racial bias in an algorithm used to manage the health of populations . Science 366 ( 102019 ) , 447 – 453 . Issue6464 . https : / / doi . org / 10 . 1126 / science . aax2342 [ 17 ] Amalia G . Sabiescu , Salomão David , Izak van Zyl , and Lorenzo Cantoni . 2014 . Emerging spaces in community - based participatory design . Proceedings of the 13th Participatory Design Conference on Research Papers - PDC ’14 , 1 – 10 . https : / / doi . org / 10 . 1145 / 2661435 . 2661446 [ 18 ] Jamila Smith - Loud , Andrew Smart , Darlene Neal , Amber Ebinama , Eric Corbett , Paul Nicholas , Qazi Rashid , Anne Peckham , Sarah Murphy - Gray , Nicole Morris , Elisha Smith Arrillaga , Nicole - Marie Cotton , Emnet Almedom , Olivia Araiza , Eliza McCullough , Abbie Langston , and Christopher Nellum . 2023 . The Equitable AI Research Roundtable ( EARR ) : Towards Community - Based Decision Making in Responsible AI Development . ( 3 2023 ) . [ 19 ] Jessica Vitak , Yuting Liao , Mega Subramaniam , and Priya Kumar . 2018 . ’I Knew It Was Too Good to Be True " . Proceedings of the ACM on Human - Computer Interaction 2 ( 11 2018 ) , 1 – 25 . Issue CSCW . https : / / doi . org / 10 . 1145 / 3274445 [ 20 ] Xuecong Xu , Xiang Yan , and Tawanna R . Dillahunt . 2019 . Reaching Hard - To - Reach Populations : An Analysis of Survey Recruitment Methods . In Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing ( Austin , TX , USA ) ( CSCW ’19 ) . Association for Computing Ma - chinery , New York , NY , USA , 428 – 432 . https : / / doi . org / 10 . 1145 / 3311957 . 3359447 ACKNOWLEDGMENTS We would like to thank the support of the Center for Advancing Safety of Machine Intelligence ( CASMI ) at Northwestern University , and UL Research Institutes . 565