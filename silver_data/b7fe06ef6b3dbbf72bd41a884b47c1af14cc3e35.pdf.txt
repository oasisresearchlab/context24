The Costs of Competition in Distributing Scarce Research Funds Gerald Schweiger 1 , * , Adrian Barnett 2 , Peter van den Besselaar 3 , Lutz Bornmann 4 , Andreas De Block 5 , John P . A . Ioannidis 6 , Ulf Sandström 7 , and Stijn Conix 8 1 Institute of Software Technology , Graz University of Technology , Graz , Austria 2 Australian Centre for Health Services Innovation and Centre for Healthcare Transformation , School of Public Health and Social Work , Faculty of Health , Queensland University of Technology , Kelvin Grove , Australia 3 Department of Organization Sciences and Network Institute , VU University Amsterdam , Netherlands , and DZHW - German Centre for Higher Education Research and Science Studies GmbH , Berlin , Germany 4 Science Policy and Strategy Department , Administrative Headquarters of the Max Planck Society , Munich , Germany 5 Center for Logic and Philosophy of Science ( CLPS ) , Institute of Philosophy , KU Leuven , Leuven , Belgium 6 Departments of Medicine and of Epidemiology and Population Health and Meta - Research Innovation Center at Stanford ( METRICS ) , Stanford University , Stanford , USA 7 Department of Industrial Economics and Management , KTH Royal Institute of Technology , Stockholm , Sweden 8 Institut supérieur de philosophie , Université catholique de Louvain , Louvain - La - Neuve , Belgium * Corresponding author : schweiger . gerald @ gmail . com Abstract Research funding systems are not isolated systems – they are embedded in a larger scientific system with an enormous influence on the system . This paper aims to analyze the allocation of competitive research funding from different perspectives : How reliable are decision processes for funding ? What are the economic costs of com - petitive funding ? How does competition for funds affect doing risky research ? How do competitive funding environments affect scientists themselves , and which ethical issues must be considered ? We attempt to identify gaps in our knowledge of research funding systems ; we propose recommendations for policymakers and funding agen - cies , including empirical experiments of decision processes and the collection of data on these processes . With our recommendations we hope to contribute to developing improved ways of organizing research funding . 1 a r X i v : 2403 . 16934v1 [ ec on . GN ] 2 5 M a r 2024 1 . Introduction Scientific research is expensive . On average , OECD countries allocate 2 . 7 % of their GDP to scientific research , with some countries spending up to 5 % [ 1 ] . Given this substan - tial social commitment to the funding of science and the profound impact of science on society , it is imperative that funding is allocated efficiently [ 2 ] . Society expects only re - search that offers the greatest social benefits in terms of economy , health , culture , etc . , to receive funding , although there is considerable disagreement with regard to defining these “greatest social benefits” and determining the criteria for allocation in a democratic society [ 3 ] . Despite massive investments in science worldwide , rigorous scholarly invest - igations into science funding had long been very limited , and studies that evaluate and compare alternative funding systems ( besides mere descriptive approaches ) have been scarce . Over the past decade , however , a growing field has emerged that is starting to address this gap [ 4 ] – [ 7 ] . Determining the best ways to distribute research funding re - mains challenging , arising primarily from the inherent complexity of the scientific system ( and the funding system ) , that is characterized by nonlinearity and multidimensionality [ 8 ] , [ 9 ] . Both characteristics make it challenging to identify and test causal connections . Performing experiments related to science funding is likely to be resource - intensive and require extended time frames , making it challenging to obtain experimental data . The lack of consensus on the precise aims of research funding compounds this complex - ity . While many policymakers and scientists may argue that scientific “excellence” should be funded , “excellence” is an ambiguous term , and its use has been viewed increasingly with suspicion [ 10 ] , [ 11 ] . Some view its vagueness as beneficial , fostering collaboration among diverse actors with distinct perceptions of “excellence” [ 12 ] . Others interpret “ex - cellence” as counter - productive by potentially playing a purely rhetorical role without any valid and operational meaning necessary to shape and guide research funding [ 13 ] . Even if funding systems use “excellence” as a general criterion , its precise meaning must be defined in relation to the many different aims funding instruments may have : contribut - ing to a specific focus area ; societal and economic impact ; career support to particular categories of researchers to become independent ( e . g . , early - career researchers , women , underrepresented minorities ) ; fostering internationalization ; supporting interdisciplinar - ity , etc . It is crucial to be specific about the various aims of science funding and the criteria that follow , and it is as crucial to be specific on how to measure these criteria so as to avoid noise and bias [ 14 ] . In the absence of a clear definition , “excellence” is often operationalized using bibliometrics such as citation counts , without taking into account that these cannot cover all aspects of research quality and impact [ 15 ] , [ 16 ] . While experts in the field of bibliometrics emphasize that citation data can capture only one aspect of quality - research impact ( and even this may only be captured imperfectly ) [ 17 ] - citizen ( or lay ) bibliometricians frequently use bibliometrics as a surrogate for all dimensions of research quality [ 18 ] . The issue of “excellence” indicates that not only an empirical but 2 also a conceptual analysis of the allocation of research funds is crucial to optimize the system . This paper aims to contribute to this goal by analyzing research funding from eco - nomic , epistemic , social , and ethical perspectives . We also discuss open research ques - tions that should be empirically answered . This paper refrains from delving into high - level decision - making processes , including fundamental decisions such as the shares of basic and applied research or the distribution of funds over the various disciplines . Although these high - level science policy decisions are important , they require different considerations . 2 . Competitive funding At a high level , one can distinguish between two types of distribution systems : Either scientists actively compete for research funds ( e . g . , by writing research proposals ) or they receive funds without actively doing so ( e . g . , through direct block funding of universities ) [ 19 ] . Central to the development of competition in the academia was the move towards New Public Management , which refers to various reforms aimed at increasing the effi - ciency and performance of public organizations [ 20 ] , [ 21 ] . Academic competition occurs at different levels ( individual , institutional , national ) and among various actors [ 21 ] , [ 22 ] . Comparing countries reveals a significant variation in the balance between institutional block funding and competitive project funding , influencing the level and nature of compet - ition for research funding [ 19 ] . Block funding also flows from national research evaluation systems , further fueling competition . Also , block funding can be distributed competit - ively within the universities [ 23 ] . Even within competitive grant allocation , many different instruments exist in which the costs and benefits balance may also be different [ 24 ] . The balance between these various forms of funding may be decisive for the relation between the costs and benefits of competitive research funding . 2 . 1 Decision - making processes Grant peer review operates through two primary models : independent reviews followed by panel discussions and standing panels with dedicated members [ 25 ] , [ 26 ] . Reviewers are asked to follow predefined criteria set by funding agencies , adapted to submission types [ 27 ] . Panel peer reviews may include meetings for discussion and consensus - building on borderline cases . After completing the review process , the funding agency processes eval - uations and recommendations for the final decision on acceptance , revision , or rejection . In the ex - ante evaluation of project ideas , a significant irrational element exists inherent in the heavy reliance on a small group of experts to screen , rank , and select among poten - tially promising research projects . This is because peers , in principle , cannot fully discern the true potential behind written proposals . The limitation lies in the subjective nature of evaluation , where experts may not always grasp a project’s full scope or potential solely 3 from its written description [ 28 ] . Previous studies have indicated that the grant review process has several weaknesses related to issues such as conflicting interests [ 29 ] , [ 30 ] , promoting conformity [ 31 ] , promoting conservatism [ 32 ] , [ 33 ] , early - career blindness [ 34 ] , and reviewers favoring applicants from their field [ 35 ] , [ 36 ] . Furthermore , studies show that reviewers who are applicants themselves are more successful than applicants with better bibliometric performance [ 37 ] , [ 38 ] . Most of the identified weaknesses address three fundamental aspects : the reliability , fairness , and predictive validity of peer review . Reliability is usually examined on the basis of agreement between reviewers . Some studies show no agreement when comparing scores between reviewers of the same grant proposal , and authors highlight the subjectivity of reviewers’ assessments , concluding that the peer review process is arbitrary [ 39 ] . Other studies show very low [ 40 ] , [ 41 ] to low agreement [ 42 ] . Since access to rejected proposals is frequently restricted , some of these studies are statistically flawed as they only analyze accepted proposals [ 42 ] . Findings on the reliability of peer review in paper evaluation for journals confirm the results for grant peer review [ 43 ] . Training peer reviewers for journals and grant reviews leads to little or no improvement in the quality of peer review [ 44 ] . The reliability of panel decisions is usually examined based on agreement between independent panels . The presence of chance in panel decisions was confirmed in quantitative [ 45 ] , [ 46 ] and qualitative studies [ 47 ] – [ 49 ] . Results concerning the predictive validity of grant peer review show no [ 5 ] , [ 50 ] – [ 54 ] or a weak relationship [ 55 ] – [ 57 ] between ranking percentile after peer review and scholarly success measured by bibliometric indicators . In one study , researchers with awarded projects and researchers with rejected projects in the biological and social sciences were compared based on bibliometric indicators [ 58 ] . If only the best rejected applicants ( same number as awarded ) are considered in both fields , they score better than the awardees on citation impact . A review suggests that peer review - based decision processes may be able to separate good and flawed proposals , but discrimination amongst the top tier proposals or applicants may be more difficult [ 59 ] . The best rejected proposals score on average as high on bibliometric indicators as the accepted proposals [ 60 ] . If applications are not only assessed based on universal criteria ( the quality of re - search ) following the ethos of science [ 10 ] , but on personal criteria ( such as gender or nationality ) , the fairness ( and validity ) of the peer review process is compromised . Many personal criteria have been investigated in the context of peer review processes , with most studies examining applicants’ gender . The historical perspective suggests that gender bias in review scores and grant decisions has declined considerably over the last decades . Re - cent reviews found no significant differences in grant success between women and men after controlling for performance and other relevant covariates , but women received smal - ler awards and fewer awards after re - applying [ 61 ] , [ 62 ] . It should be noted here that non - merit criteria can be included in application evaluation criteria , as grants may , for 4 example , aim to support the academic careers of key groups , such as grants for early - career researchers or for women in fields where women are underrepresented . 3 . Costs of competition 3 . 1 Economic costs of competition The economic costs of competitive funding systems include the costs of applicants needing time to write proposals , decision - making processes ( reviews and panels ) , and administrat - ive costs . A study in Australia estimated that 85 % of the costs are incurred by the applic - ants , 10 % by the decision - making processes , and the remaining 5 % by the administration [ 46 ] . Funding schemes will have a net financial gain of zero if the costs for these three categories are equal to the amount of funds awarded [ 63 ] ; this is most likely for schemes with many applicants and low success rates . Depending on the total available funding , the funding rate , and the time that the applicants invest in the proposals , the point of zero net gain is crossed at funding rates that are not uncommon in current funding systems [ 63 ] , [ 64 ] . Studies in various disciplines show that writing a single proposal takes about 25 to 50 days [ 65 ] – [ 67 ] . With average acceptance rates between 10 to 25 % , that is 100 to 500 person - days of effort per funded project . In interpreting reported studies on time investment , it should be considered that they are primarily based on self - reporting , and self - reporting of time use may be inaccurate in many situations [ 68 ] . An evaluation of Europe’s H2020 programs found that between 30 and 50 % of the funding from Hori - zon 2020 is spent on grant writing [ 69 ] . A study of federally funded research projects in the U . S . estimated that principal investigators spend on average about 45 % of their time on administrative activities related to applying for and managing projects rather than conducting active research [ 70 ] . A time allocation study of academics in the US estim - ated that they dedicate an average of 4 . 6 hours per week to grant writing , with already tenured academics spending less time [ 71 ] . There are limited empirical results on the time reviewers spend reviewing applica - tions . An evaluation of the UK Research Council estimated that reviewers spent about 192 person - years reviewing applications [ 72 ] . Feedback from reviewers does not seem to benefit the quality of the proposed research [ 67 ] : A study found that resubmitted pro - posals had a lower probability of winning funding than proposals submitted for the first time [ 67 ] . Success rates in private funding schemes are often even lower than for academic grants . A funding program for women in STEM has a success rate of less than 1 % ; only 6 out of 650 applicants were awarded funding [ 64 ] . If applicants spend more than 7 days to complete their application , the point of net zero gain is crossed . In a private research call in computer science , 2 out of 1090 applications were funded [ 73 ] . With a total prize of 5 $ 120 , 000 , net zero gain is crossed if each application costs just $ 110 . Writing funding applications may also have positive indirect effects . Scientists can generate , refine , and share their ideas , regardless of whether or not they ultimately receive the funding [ 74 ] . Furthermore , even if the overall costs of the competitive funding are equal to the amount funded , at the individual level , there is a financial benefit for those who win the grants at the expense of those who do not . And the research done could theoretically be better than without the competition . Since sharing and refining research ideas is not only restricted to environments with competitive funding systems , the size of the indirect effect is not clear . One could imagine that scientists are more willing to share ideas , data , and results in an environment with less competition . 3 . 2 Epistemic costs of competition Research funding has multiple epistemic aims , including promoting groundbreaking and high - risk research [ 75 ] , [ 76 ] , supporting incremental progress within established paradigms [ 77 ] , and translating theoretical insights into practical applications . The impact of peer review and competitive funding on achieving these different objectives will vary , and an ideal funding strategy should tailor its distribution methods to the specific epistemic goals of choice . The plurality of epistemic choices makes it difficult to assess the epistemic costs of competitive funding . In this section , we discuss those epistemic costs that are most obvious and have been discussed in the literature . Most prominent in the literature is the supposed relationship between competitive funding and the lack of high - risk , high - impact research . Empirical studies indicate that major scientific breakthroughs are infrequent [ 78 ] and the distribution of research out - comes is positively skewed , with highly productive outcomes being rarity [ 79 ] . While such outcomes would likely be rare under any funding method , it has been argued that peer review of grant applications deters researchers from proposing high - risk research . On the one hand , writing and reviewing applications reduce the time for such research , which is probably more time - intensive than conventional research . On the other hand , a study of the European Research Council’s funding decisions showed a bias against applicants with histories of high - risk research , favoring those with more conventional profiles [ 80 ] . The main bias against applicants with very high - risk and , thus , controversial profiles occurs in the first stage of selection when panel members review applications based on a brief description of the proposed research and the applicant’s CV . Other studies draw similar conclusions that high - risk research is disadvantaged in competitive funding systems [ 81 ] – [ 83 ] . However , more empirical studies are required to confirm and explain this finding . Researchers might adjust their behavior in response to the perceived bias against high - risk proposals . A theoretical study suggested that the predominance of conservative submissions is larger in grant proposals than in paper submissions . This is because grant proposals need some preliminary promising data to support the hypotheses , which might 6 lead peer reviewers to favor safer ideas [ 84 ] . A study in life sciences found that programs that reward long - term success lead to higher levels of breakthrough innovation arising from risky research , compared with short grant cycles and predefined deliverables [ 85 ] . Ideas for high - risk research need time , and their implementation requires time and a dependable environment . The competitive nature of a system might affect not just the proportion of risky research but also researchers’ performance , as indicated by bibliometric indices . A study analyzed the influence of competitive research funding on efficiency in 17 countries , defined as the change in funding compared with the change in highly cited publications [ 19 ] . The data show a negative correlation of 0 . 3 between efficiency and the degree of competitive funding . In another study , data from eight countries were analyzed to assess the impact of competitive research funding on productivity as measured by the number of publications [ 23 ] . Countries with high competition ( e . g . , the UK ) are efficient but have not been able to improve efficiency ; other countries with less competition ( e . g . , Denmark ) are either almost as efficient or ( e . g . , Sweden ) have been able to improve their efficiency despite relatively low competition . A study comparing competitive and institutional block funding within Japan reveals mixed results concerning the novelty of outputs measured using bibliometric methods . Competitive funding appears to be associated with higher novelty compared with papers funded by block funding . However , these findings are contingent upon the status of the researchers . Specifically , high - status , senior , and male researchers tend to produce more novel outputs under competitive funding schemes . Conversely , lower - status , early - career , and female researchers had a negative relationship with novelty . These nuanced results underscore the complex interplay between funding mechanisms and researcher characteristics in shaping research outputs [ 86 ] . Note , however , that the latter three studies are not designed to enable the discovery of causal relationships . Further research is needed to uncover the underlying mechanisms . If the conservatism of peer reviewers contributes to competitive funding’s bias against highly novel , risky research , then such funding might be more appropriate for funding incremental scientific progress . Yet , even here , excessive competition may incur epistemic costs , particularly in research communities tackling complex topics . A simulation study coupled with historical data shows that such research communities tackling complex topics ( e . g . , fundamental problems in physics ) are at risk of holding on to popular , but incorrect paradigms due to low self - correction capabilities [ 87 ] , [ 88 ] . Science funding systems should facilitate the quick abandonment of fruitless ideas and minimize investment in scientific dead ends . However , this may not happen since low - yield and fruitless ideas are still defended by communities of scientists and organizations who have made careers based on them [ 89 ] . Conversely , successful groups should be protected from collapsing due to funding gaps . Do the top - cited scientists receive a steady stream of funding ? A study analyzing the association between federal biomedical funding and high citation impact among scientists 7 found that while funded top - cited scientists attracted more citations than non - funded , only a small minority had funding at the time of the survey [ 90 ] . A study in the UK in the field of health research came to similar conclusions . The majority of the UK’s most influential health scientists do not receive funding from the country’s top three public and charity funders [ 91 ] . This research must also be conducted for other disciplines and countries . If only a limited proportion of top - cited researchers currently receive funding from public funders , this must be considered in thoroughly evaluating existing funding systems . Competitive funding also incurs epistemic costs that probably affect all epistemic aims of science . For instance , the economic costs discussed above have a direct epistemic counterpart : the time spent by researchers on writing and reviewing proposals consumes a portion of the total research budget . Economically more efficient ways of distributing the funding would translate to additional epistemic payoff . Even if funded projects in a competitive system perform better than rejected ones , it must be demonstrated that this outweighs the resources that could be allocated to additional research in alternative systems . Another epistemic cost of competitive funding relates to biases , particularly cronyism , in competitive funding . Diversity in background assumptions , perspectives , and research interests is vital for research communities to remain critical and self - correcting [ 92 ] . If competitive funding leads to a homogenization of viewpoints , whether through biases or conservatism , research communities dependent on such funding are more susceptible to confirmation bias and blind spots that would more likely be corrected in more diverse communities . 3 . 3 Social and ethical costs of competition Some of the epistemic and economic costs discussed above also come with distinct social and ethical costs for the society as a whole or certain groups within the society . Inefficient allocation of societal resources comes at the expense of opportunities , as the probably wasted resources could be spent on direct improvements of well - being such as medical care , poverty relief , and other pressing societal problems . Competitive funding processes create distinct groups of winners and losers . The Matthew effect leads to the manifestation and reinforcement of winning and losing in the processes . Most funding schemes have low success rates under 25 % , and in many cases even under 15 % [ 93 ] , so failure is the norm . While the fortunate winners can continue their scientific careers , the losers can face big decisions , including changing their research plans , relocating to another city or country , or even deciding to quit research [ 94 ] . The social costs can extend beyond the scientist to their family [ 67 ] , as bluntly described by one researcher : “My family hates my profession . Not just my partner and children but my parents and siblings . The insecurity despite the crushing hours is a soul destroying combination that is not sustainable . ” [ 95 ] . Failing to win funding has been compared with the grief of bereavement [ 96 ] . 8 The pain of failure is often compounded by regret as researchers reflect on the op - portunity costs of the time spent writing failed applications that could have been spent on actual research . Whereas we know from journal peer review that many rejected ma - nuscripts are submitted elsewhere ( the time spent on writing the manuscript was not wasted ) , this does not appear to be the case with rejected proposals : The results of a previous study in medical research show that resubmissions take only 25 % less time than submitting a new application [ 67 ] ; the result of a study in engineering showed no differ - ences between the time needed to write new applications compared with resubmissions [ 65 ] . An estimated 90 % of researchers perceive that they spend too much time prepar - ing competitive research proposals , and only 10 % of researchers believe that the current competitive funding system positively affects the quality of research [ 65 ] . The immense pressure to win funding can negatively impact researchers’ mental health and work - life balance [ 97 ] , [ 98 ] . Some researchers even choose to have fewer children to remain competitive [ 99 ] . For those with children , there is an inevitable disruption to their career , particularly for female researchers who are often the primary caregivers for children and relatives [ 100 ] . This means women often have less time to apply for funding , which may partly explain the common male - female funding gap [ 101 ] . Many funders have recognized the impact of caring for children on researchers’ careers and allow applicants to explain how any disruption has impacted their productivity , aiming to level the playing field . However , this may not always work as women have reported not wanting to document their disruption for fear of appearing “weak” [ 102 ] , [ 103 ] . It is unclear how reviewers actually respond to such disclosures and whether disclosures level the playing field or make inequities more prominent . Furthermore , hyper - competitive and pressurized funding systems can reduce collegiality , sow distrust in the community , and create feelings of alienation [ 104 ] , [ 105 ] . Although these criticisms of competition have credibility , they do not mean that less competitive funding and more block funding would solve these problems . Competition is inherent in science ( besides cooperation as the other side of the coin ) . It is not restricted to grants but is also true of academic positions , promotion to higher positions , etc . Even if all funding were awarded as block funding , there would still be not enough research funding to create a sufficient research environment for every staff member . Decisions about scarce resources would become bureaucratic ( and potentially more biased ) , and the competition would shift to colleagues from the same organization vying for a larger slice of the block funding . It is doubtful that this would improve the quality of research and reduce social costs . There is also a moral dimension to many of the problems listed above : discriminating based on gender , age , and race is morally wrong , as is wasting societal funds through their inefficient allocation . In addition to these moral problems , competitive grant funding may also incentivize direct violations of core values of research integrity such as honesty , ac - 9 countability , fairness , impartiality , and responsibility [ 106 ] . This seems plausible because success in competitive funding carries substantial weight in tenure , hiring , and promotion decisions [ 107 ] . In combination with low success rates and researchers’ reliance on grant money for research , applicants face strong incentives to cut corners in the application process that might improve their chances [ 106 ] . For example , competitive grant funding might incentivize practices such as salami - slicing and other publication and authorship - related questionable research practices ( QRPs ) , as a researcher’s track record is often an important factor in evaluating grant proposals . These incentives are further exacerbated by funders prioritizing ex - ante grant peer review over post hoc project evaluation , mean - ing that many violations will go unnoticed [ 108 ] . Indeed , one study found evidence for double dipping [ 109 ] , and a recent survey found that a significant portion of respond - ents admitted to frequently engaging in various QRPs , including improper use of funds , overstating confidence in the research proposal , double dipping , and selective citing [ 109 ] , [ 110 ] . In the same survey , reviewers and panelists also indicated that they have frequently engaged in funding - related QRPs , including inadequate preparation for panel meetings or reviewing proposals of close colleagues and friends . There are other potential moral problems with competitive funding , but research is mostly lacking . For example , various authors hypothesize that extreme competition en - courages gaming , commonly known as “grantsmanship” , such as writing the application you think will get funded rather than the work you think is most important [ 104 ] , [ 105 ] . Other potential harms include actively avoiding collaborations with peers so as to have an appropriate expert to review your applications [ 111 ] . Some researchers do not trust fund - ing peer review systems and are concerned that unscrupulous reviewers will steal their ideas [ 106 ] . Occasionally , researchers move beyond gaming and into outright dishonesty [ 104 ] , [ 106 ] , and the desire to win funding has motivated some researchers to commit fraud by faking data [ 112 ] , [ 113 ] . The perceived excessive costs of writing lengthy applications could drive researchers to use large language models to write parts of their applications , with ethical consequences [ 114 ] . More research is needed on the moral problems related to competitive research funding , as almost all evidence is drawn from surveys using convenience samples . In addition , while some researchers may engage in QRPs intentionally to bolster their funding prospects , these QRPs are probably also ingrained in a research culture that junior researchers observe and imitate . In that sense , a targeted attempt to change the culture of research integrity within the funding context could address these moral problems [ 108 ] . However , institutions may be hesitant to promote such changes , as they tend to benefit from success in funding competitions . In addition , many funding - related QRPs are intricately linked to peer review and competitive grant distribution , making it hard to eliminate them without changing the distribution model [ 106 ] , [ 110 ] . 10 4 . Road to progress In the previous section , we have outlined several prevalent costs entailed by competitive funding allocations . This section introduces a series of open research inquiries crucial for optimizing the allocation processes . Some issues can be assessed through measurable means , such as controlled experiments ( e . g . , assessing the reliability of peer review and panel decisions ) or derived from data ( e . g . , evaluating application writing costs ) . However , some challenges can only be minimally measured , and controlled experiments are imprac - tical or impossible due to system complexity or long - term effects . In such situations , researchers have frequently conducted studies examining the funding ecology instead of experimentally investigating individual funding schemes or organizations [ 19 ] , [ 23 ] . To move forward , a combination of studies using different designs is needed : ecological stud - ies , possibly combined with simulation models with hypothetical causal relations , causal analysis using experiments including natural experiments , and causal analysis on cross - sectional data [ 115 ] . Together , these can be the basis for policy decisions . The open research topics that we identified are as follows . The foundation : Research on research . The need to apply the scientific process to the processes of science has been acknowledged for over a century , with a 1910 article stating , “we are at present almost wantonly ignorant and careless regarding the conditions which favor or hinder scientific work " [ 116 ] . A paper published in 1976 concluded that “because the very nature of research on research [ . . . ] requires long periods , we recommend that independent , highly competent groups be established with ample , long term support to conduct and support retrospective and prospective research on the nature of scientific discovery " [ 117 ] . But since that 1976 statement , there has been substantial growth in research groups on the science of science , specialized conferences , and journals , learned societies , and specific funding schemes , e . g . , at the National Science Foundation and as part of the European framework programs . In addition , there are also many meta - researchers who do this in addition to their jobs in other ( often medical ) fields . Some of these researchers are organized in the Research on Research Institute ( RoRI ) – a nonprofit community interest company founded in 2019 . The company accelerates “transformative research on research systems , cultures and decision - making” We can think of no other industry that spends so little on evidence - based quality control and process improvement . This lack of re - investment means that funding systems have rarely been tested and have survived for decades without change . Their longevity has put them on a pedestal , and challenging them can be treated like heresy [ 118 ] . First : Data . A systematic review of funding systems concluded that research funders should „build in before and after comparisons ; strive to make data available for analysis ; openly publish studies of their processes and work together on comparative analysis“ [ 119 ] . Non - sensitive data that does not raise privacy or data protection concerns should be ac - cessible to the public without restrictions . This includes the number of submitted grants , 11 funding rates , and data on the difference between the requested and allotted budget . Data on the applicant pool is also crucial . Examining potential gender bias in funding decisions necessitates knowledge of the expected number of male and female applicants . If the number of female researchers eligible to apply is limited , a correspondingly low number of positive funding decisions can be expected . However , for most research ques - tions , it is crucial that these data can be linked at the researcher level so that one can control for the relevant covariates when analyzing the reliability and possible bias of grant evaluation . Some research questions require more detailed data on the applications and , in some cases , the application text itself . Receiving the data for accepted and rejected projects is crucial for assessing the evaluation process . This information can be used to analyze whether the rejected applications were accepted elsewhere . We would need the application data when analyzing whether specific technical terms or terms , such as “innovative” , “ground - breaking” , or “novel” , occur more frequently in successful applications . Funders could consent to applicants’ data being used for research purposes when they submit their applications . It is also conceivable that some data must be made available to submit to certain calls . Researchers who provide a protocol and get ethical clearance from their institute could—with a non - disclosure agreement and an adequate data management plan—be given access to these microdata . Another option is to use the OpenSafely approach for research data access , where researchers submit their analytical code and , without accessing the original data , receive summary results [ 120 ] . A general problem with data from organizations such as funding agencies is that the data can lead to criticism that strikes at the core of their practice . This can lead to defensive behavior ; therefore , clear policy guidelines are needed . Second : Reliability and predictive validity of funding decisions . Whereas many studies have been conducted that investigate the reliability of journal peer review processes , we need more results on the reliability of funding decisions . The few studies on the reliability of peer reviews and panel decisions have examined research tenders in basic research . Future research should try to broaden this perspective : It should be investigated whether the reliability of decision - making processes differs across disciplines and how reli - able the processes are in applied and especially interdisciplinary research . Since it is easier to define in basic science which people are suitable for reviews and panels , the reliability of the process may be given to a certain extent . In applied and interdisciplinary research , the selection of reviewers is often not so easy due to the broad and unspecific disciplinary nature of the projects . The peer review panel could be further extended in applied re - search by including experts from outside the science sector . Beyond scientific excellence , evaluation criteria frequently encompass additional factors such as market potential and a nation’s contribution to technological leadership . Although these considerations are justified from social and political standpoints , they may lead to additional uncertainty in the decision - making process , especially if one does not have a clear operationalization 12 and measurement strategy for such criteria . Without these , more criteria may only lead to more noise and bias in grant selection . Regarding the methods used to investigate judgments and decisions , we recommend that the reliability of processes should be evaluated in controlled experiments [ 121 ] and with methods allowing for causal analysis with cross - sectional data such as matched pairs , regression discontinuity analysis , and differences in differences . Additional studies on funding processes are also necessary for predictive validity . Most past studies used bibliometric indicators to investigate the judgments of peers and funders . The strong focus on bibliometrics is particularly inadequate for analyzing processes for funding applied research . In addition , today , funders of research ( the gov - ernment ) expect ( measurable ) impact of ( basic and applied ) research on sectors of society beyond the science sector . This impact cannot be measured with bibliometric indicat - ors . Societal impact may be measured by alternative indicators . The citation of scientific papers in patents , e . g . , is an established indicator for measuring the economic impact of research . Another recent example of data measuring research impact beyond science is data from the Overton database [ 122 ] . The data can be used to measure the impact of ( funded ) research on the policy sector . The Overton database includes documents from the policy sector and the cited scientific literature in these documents . Suppose one is interested in the contribution of research to solving societal challenges . In this case , it is possible to measure the contribution of ( funded ) papers to the United Nations Sustainable Development Goals ( SDGs ) [ 123 ] . The SDGs address the biggest problems facing the world today ( e . g . , climate change or poverty ) . Various approaches have been developed to match published research to at least one SDG . One should never - theless beware that we know far more about the limitations , caveats , and gaming poten - tial of traditional bibliometrics than those of alternative indicators . Alternative indicators may be as gameable or even more gameable than traditional bibliometrics . Before we use alternatives to bibliometrics , it is necessary to know the strengths and weaknesses of the metrics . Third : Alternative evaluation systems . If peer review were a drug , it wouldn’t be allowed on the market because it has not been rigorously tested [ 124 ] . As scientists , we should treat peer review like a drug and compare it with alternatives that might be more efficacious , cheaper , or with fewer side effects . Like the human body , the world of academia is complex and may react unexpectedly . For example , it surprised many people that application numbers decreased when the U . S . National Science Foundation removed funding deadlines [ 125 ] . Besides peer review , bibliometrics is the most commonly used method for science evaluation . Although the method has broad acceptance in science [ 126 ] , many criticisms have been published [ 127 ] . Using evaluative bibliometrics may be improved if the in - dicators are theory - based , valid , adequately operationalized , standardized , and properly field - adjusted . Just as medical operations should only be done by experienced physicians , 13 evaluative bibliometrics should only be done by experienced , professional bibliometricians . Advanced indicators include indicators that capture gaming of the system and additional indicators of good research practices ( e . g . , data and code sharing , protocol registrations ) [ 15 ] , [ 128 ] . Bibliometric data could be extended to develop merit indicators , such as indic - ators for independence [ 16 ] , or a score for the research topics covered by a researcher [ 129 ] . Several new indicators and indicator variants have been developed in the field of bibli - ometrics that have been established for many years . Despite various efforts to improve evaluative bibliographics , there are several movements today against the ( extensive ) use of metrics ( in national evaluation systems ) such as the EU Agreement to Reform Research Assessment [ 130 ] . An overview of these movements can be found in [ 131 ] . Movements are characterized by a preference for the peer review process and responsible use of met - rics . Since both methods of evaluating research have their own strengths and weaknesses , a clear decision about one solution cannot be made in all evaluation situations . It is probably the best solution to design a specific evaluation framework for each task that combines both methods in most cases [ 131 ] , [ 132 ] . Since most completed evaluations do not investigate the success or failure of funding decisions - except the relatively small num - ber of studies that focus on the predictive validity of grant evaluation ( see section 2 . 1 ) - the effect of evaluations on science success remains unclear since the advent of these evaluations . Since many of the economic costs in evaluations are time investments ( from various stakeholders , such as applicants and reviewers ) , investigations of peer review pro - cesses should especially focus on this issue . As bibliometric support may be a decisive element in reducing time investments , research is needed to understand how bibliometrics can play an optimal role in in grant decision - making [ 133 ] . This should go hand in hand with selecting appropriate indicators and ways to improve their quality . Using citations from peers worldwide as an ex - post evaluation procedure could offer an alternative to traditional ex - ante evaluation methods . While it would require time to accrue citations , endorsements from the relevant research community would add cred - ibility to the decisions . Rather than relying on a small and often opaque group with unclear selection criteria , researchers would be assessed by the broader scientific com - munity through their citation counts . This approach would democratize the evaluation process , ensuring that the direction of science is not dictated solely by a selected few . Ul - timately , entrusting evaluation to the scientific community could be the most efficient and democratic solution , fostering transparency and inclusivity in the assessment of research projects [ 15 ] . Fourth : Alternative distribution systems . In many current systems , a large part of applications are investigator - led , meaning the investigator proposes what to study . Other types of funding are thematic grants , where themes are defined by the funding organizations , and commissioned research , where research question ( s ) are formulated by organizations or companies and investigators write applications explaining why they are qualified to answer the question ( s ) . The potential 14 benefit of thematic grants and commissioned research provides a more direct return to society than investigator - led , especially if the commissioning process is extensive and done in partnership with end users [ 134 ] . These alternatives should never consume 100 % of a funder’s budget , as investigator - led research is still a vitally important avenue for discovery . The ideal distribution of the budget over the different types of grants to spend on the different types of commissioned research is another unknown . Commissioned research could fail if the process of selecting the topics is flawed and biased and leads to poor , conflicted , and / or dead - end investments . A problem with most current funding systems may be that even when research programs are not entirely com - missioned , there is a strong hidden element of the commission behind them . For example , requests for applications may create boundaries around what ideas and topics are desir - able , even if they seem to be relatively open and non - specific . Modified lotteries are an alternative system and one that has recently been trialed by funders in several countries , e . g . , New Zealand , Switzerland , Germany , and the UK [ 135 ] – [ 138 ] . In a modified lottery , the applications are assessed by peer review . However , they are not ranked linearly , with a funding line splitting applications into funded and not funded . Instead , peer review is used to assess applications as fundable or not , with the winners drawn from those assessed as fundable . In a variation , reviewers can also select “excellent” proposals that should be funded without the risk of lottery . When peer review has been performed on some or all applications , another decision is whether to have weighting in the lottery ( e . g . , to give more tickets to applications with more favorable peer assessments ) [ 4 ] . Early results from The British Academy ( the UK’s national academy for the humanities and the social sciences ) suggests that the diversity of applicants increased after the modified lottery system was introduced , possibly because applicants outside the mainstream perceive the lottery system as fairer and so are more willing to apply [ 139 ] . The total cost of putting these applications together ( and of reviewing them ) is likely to depend on how strong a role peer review plays in the modified lottery , and how extensive proposals are reviewed . If peer review only consists in a minimal formal check for eligibility , the costs for reviewing and creating applications will be much lower than if traditional peer review is used . An alternative distribution system that significantly reduces bureaucracy is to give every scientist a small amount of base funding [ 72 ] . However , the problem then becomes how to define eligible scientists . For example , universities could switch the job description from technical staff to scientists to get more funding . This could be policed , but that would likely involve every scientist describing their work , which would then need to be peer reviewed , and hence the bureaucracy savings could be lost . A related system of base funding is to give money to each scientist but then force them to give away half [ 7 ] . This uses the wisdom of the crowds to allocate more funding to those scientists who are respected by their peers . However , it could be prone to bullying as senior scientists may strong - arm junior scientists into promising them funding , with 15 consequences if the promises are unfulfilled . However , we can only speculate on potential problems and benefits without properly testing the system . A pilot investigation in Australia asked a large number of scientists to name up to ten scientists in the country who they thought would be deserving of funding [ 140 ] . The study showed that this democratic voting process reduced time requirements compared with a traditional grant review system . However , there was some bias in favor of naming scientists from the same institution , and some caveats were noted by participants regarding the potential for vote rigging , lobbying , and turning science into a popularity contest . In conclusion , we do not know what the best distribution system is . A systematic review suggests that greater funding dispersal is likely to be beneficial [ 141 ] . The best way to implement such greater dispersal and achieve maximum benefits remains unclear and needs careful evaluation . This uncertainty provides a solid basis for experimental tests . Should fellowships come with additional funding , e . g . , PhD stipends ? What mix of the funding budget should be spent on early - career versus senior researchers ? What mix of the funding budget should be spent on a project versus people funding ? However , some of these questions could be answered using randomized trials , requiring researchers to consent to participating in experiments , which some will be unwilling to do . Fifth : Economic costs . The cost of competition ( including positive external effects ) as a function of the acceptance rate should be analyzed for different disciplines and along the continuum of applied and basic research . Therefore , we need data on ( i ) the time needed to prepare applications , ( ii ) possible positive indirect effects of proposal writing , ( iii ) the costs of decision - making processes including peer review and panel decisions , ( iv ) administrative costs of funding agencies and other organizations such as those involved in tendering for funding , ( v ) resources spent on training researchers to improve their grant proposal writing skills , including workshops , courses , and mentoring programs , and ( vi ) acceptance rates of the respective funding programs . When studying these economic costs , some specific issues should be considered . For example , it is essential to distinguish between the different evaluation and decision - making forms , as time investments may differ radically . To address the issue with self - reporting of time , studies are needed that analyze the discrepancies between the self - reported time estimates and a logged version of these data [ 142 ] . Another issue is that one should distinguish between the skill levels of applicants . Frequent applicants may become more efficient ( and probably more successful ) over time , indicating a learning effect . If this is the case and research should be done to find out , a division of labor may reduce the costs for grant writing : Not everyone has to be good at writing grants . Research is also needed to record who is actually doing the grant writing , e . g . , it is possible that in many systems , much of the effort is contributed by managerial staff or professional grant writers rather than scientists . If so , this might mean that a large part of the devoted effort does not affect research resources , but it nevertheless entails costs . 16 Sixth : Epistemological costs . In section 3 . 2 , we distinguished between different aims of funding systems , including funding risky research , funding normal science , and funding proliferation and variety . Funding one of these goals is likely at the expense of the others and , therefore , a balance is needed . The optimization depends partly on understanding whether a relation exists between the evaluation and selection process and the level at which the specific aim is realized . Research on this has been scarce up to now . The relation should be studied at the level of the review and decision - making process of individual applications , but also at the level of the funding organization and its instruments . Only with hindsight can one assess whether disruptive projects have been funded [ 143 ] , whether there has been enough variety in the funded research , and whether dead ends have been abandoned in time . Some research has suggested the conservative effects of competition and peer review as selection mechanisms , but that does not preclude that the funding instrument contributed meaningfully to one or more of the mentioned goals - as that cannot be evaluated at the individual project level . So the issue is to evaluate the outcomes of the funding instrument and funding organizations on the system level : Has one funding organization proved to be better at funding breakthroughs than another organization , and why ? Do differences relate to how a funding organization defines eval - uation criteria and organizes selection processes ? The same holds for other epistemic aims , such as stimulating variety and selecting promising developments . Discussing these questions regarding the costs of competition also leads to the macro - ( country ) level . Are countries with a higher share of competitive funding less good at simultaneously support - ing risky research , normal science , variety , and stimulating the promising parts of this variety ? One may speculate that ( too much ) competition clearly entails epistemic costs . Seventh : Social and ethical costs . Our current understanding of the social and ethical costs associated with competitive grant funding has largely been derived from de - scriptive surveys and interviews . These methods remain valuable and could be used to obtain a more detailed insight into problems identified by existing research , such as the extent to which researchers return funds to funders or rapidly spend it before the grant expires , the extent to which funders check for negative and positive conflicts of interest , and the extent to which reviewers account for questionable and responsible research prac - tices ( e . g . , preregistration ) in evaluating grants . However , there is little research on the social costs incurred by individuals who miss out on grant competitions . For example , further exploration into whether the lack of funding for an early career initiates a negative spiral , potentially reducing mental health , motivation , and long - term productivity , could be interesting given how expensive and scarce research positions are . However , most importantly , we should move beyond descriptive research and delve into the root causes of the social and ethical costs of science funding . Designing effective inter - ventions without a clear understanding of the underlying causal structure is challenging . Researchers should aim to formulate causal hypotheses , assess their amenability to inter - 17 ventions , and test these hypotheses empirically through experiments in collaboration with funding organizations . Thus , as with many of the aspects of research funding which we discuss here , we believe that conducting more causal studies is paramount in uncovering actionable insights that can inform the policy and mechanism of science funding . Another notable gap in our current knowledge lies in understanding the social and eth - ical costs associated with changes to the current system or alternative funding systems . It is particularly difficult to capture the complexity of these systems with simulations , and predicting outcomes is challenging . While single experiments have limitations ( so - cial and ethical costs may only arise when a funding method is broadly implemented ) , they remain an indispensable tool . Therefore , we call for increased experimentation with alternative funding models . Such studies should extend beyond assessing epistemic and financial implications ; researchers should also investigate the impact on the well - being and questionable research practices of those involved , providing a comprehensive understand - ing of the broader consequences of alternative funding structures . For example , it may well be that funding groups rather than individuals positively impact the responsible re - search practices of researchers [ 144 ] . Using experiments , such hypotheses could be tested simultaneously with the epistemic benefits of such a system . Table 1 summarises our recommendations for a better understanding of the impact of competition on allocating scarce research funds . Table 1 : Road to progress Expected Outcome Recommendations Facilitates research on funding processes to investigate existing systems and examine the impact of system changes . Enabling transparency and accountability . • Funders should make anonymized data on rejected proposals available . • Use OpenSafelyapproach for sensitive data . Make non - sensitive application and decision data publicly available for analysis , and facilitate responsible use of sensitive data . Data Collection and Accessibility Improved understanding and refinement of decision - making processes in funding allocation . Demonstrates to the research community that processes are being interrogated . • Investigate differences in reliability across disciplines and in interdisciplinary research . • Use bibliometric data to build more meaningful , valid , and reliable indicators . • Use measures beyond bibliometrics : altmetrics , Overton data , connections to SDG , etc . Conduct controlled experiments to test the reliability , predictive validity , and fairness of funding decisions . Decision Making Processes A more accurate picture of the benefits of funding research which would be politically useful for science . Better data on what projects most often fail to complete the proposed work which could then inform funding calls and criteria . • Random audits that evaluate a small number of completed projects in great detail vs bibliometric outcomes for all completedprojects that likely miss important benefits . • Field experiments with new types of reviewing and selecting processes . • Examine the time delay between funding and benefits . Experimentally test and compare different evaluation systems for competitive funding . Alternative Evaluation Systems Potentially reduces biases and administrative burdens , promoting fairness and diversity in funded research . • Experimentally test various project - parameters such as duration or proportion of budget used on people funding . Experimentally test funding models such as lotteries or base funding , or targeted funding based on peer nominations . Alternative Distribution Systems A detailed view on what is invested in competitive funding . • Investigate the division of labour in grant writing . • Investigate the costs of administration , decision - making processes , and positive effects of proposal writing . • Develop models that take into account all costs and potential positive effects of grant writing . Investigate the economic costs of competitive funding beyond the total costs for applicants . Economic Costs A heterogeneous view on science funding where different epistemic aims are pursued using different funding methods . • Map the relations between the various epistemic aims of science and different methods of funding science . • Develop models that can be used to design and compare different funding ecologies . Understand the relations between the various aims of science and different methods of funding . Epistemic Costs Creates a more supportive and ethical research environment , encouraging responsible research practices . • Map out the potential social and ethical costs of alternative funding methods . • Randomized controlled trial of questionable research practices in competitive funding . Go beyond survey studies with convenience samples to map and diminish the social and ethical costs of competitive funding and other funding methods . Social and Ethical Costs 18 Acknowledgments We thank Stephan Pühringer from Johannes Kepler University Linz and Ulf Heyman from Uppsala University for fruitful discussions . The work of Stijn Conix for this paper was funded by the Fonds de la Recherche Scientifique—FNRS under grant no . T . 0177 . 21 . References [ 1 ] OECD , Main science and technology indicators . 2023 . doi : https : / / doi . org / 10 . 1787 / 1cdcb031 - en . [ 2 ] J . P . A . Ioannidis , ‘Fund people not projects , ’ Nature , 2011 . doi : https : / / doi . org / 10 . 1038 / 477529a . [ 3 ] P . Kitcher , Science , truth , and democracy . Oxford University Press , 2001 . [ 4 ] J . Shaw , ‘Peer review in funding - by - lottery : A systematic overview and expansion , ’ Research Evaluation , 2023 . doi : 10 . 1093 / reseval / rvac022 . [ 5 ] F . C . Fang , A . Bowen & A . Casadevall , ‘NIH peer review percentile scores are poorly predictive of grant productivity , ’ Elife , 2016 . doi : 10 . 7554 / eLife . 13323 . [ 6 ] J . Shaw , ‘Bias , lotteries , and affirmative action in science funding policy , ’ The British Journal for the Philosophy of Science , 2024 . doi : https : / / doi . org / 10 . 1086 / 730218 . [ 7 ] J . Bollen , D . Crandall , D . Junk , Y . Ding & K . Börner , ‘An efficient system to fund science : From proposal review to peer - to - peer distributions , ’ Scientometrics , 2017 . doi : 10 . 1007 / s11192 - 016 - 2110 - 3 . [ 8 ] B . Latour , Science in action : How to follow scientists and engineers through society . Milton Keynes : Open University Press , 1987 . [ 9 ] F . Shi , J . G . Foster & J . A . Evans , ‘Weaving the fabric of science : Dynamic network models of science’s unfolding structure , ’ Social Networks , 2015 . doi : 10 . 1016 / j . socnet . 2015 . 02 . 006 . [ 10 ] R . K . Merton , The sociology of science : Theoretical and empirical investigations . University of Chicago press , 1973 . [ 11 ] S . Moore , C . Neylon , M . Paul Eve , D . Paul O’Donnell & D . Pattinson , ‘“Excellence R Us” : University research and the fetishisation of excellence , ’ Palgrave Commu - nications , 2017 . doi : https : / / doi . org / 10 . 1057 / palcomms . 2016 . 105 . [ 12 ] T . Hellström , ‘Homing in on excellence : Dimensions of appraisal in Center of Excel - lence program evaluations , ’ Evaluation , 2011 . doi : 10 . 1177 / 1356389011400891 . [ 13 ] P . O’Connor , E . M . López , C . O’ Hagan et al . , ‘Micro - political practices in higher education : A challenge to excellence as a rationalising myth ? ’ Critical Studies in Education , 2020 . doi : 10 . 1080 / 17508487 . 2017 . 1381629 . 19 [ 14 ] D . Kahneman , O . Sibony & C . R . Sunstein , Noise : a flaw in human judgment . Hachette UK , 2021 . [ 15 ] J . P . Ioannidis & Z . Maniadis , ‘In defense of quantitative metrics in researcher assessments , ’ Plos Biology , 2023 . doi : 10 . 1371 / journal . pbio . 3002408 . [ 16 ] P . Van den Besselaar & U . Sandström , ‘Measuring researcher independence using bibliometric data : A proposal for a new performance indicator , ’ PloS one , 2019 . doi : 10 . 1371 / journal . pone . 0202712 . [ 17 ] D . Hicks , P . Wouters , L . Waltman , S . De Rijcke & I . Rafols , ‘Bibliometrics : The Leiden Manifesto for research metrics , ’ Nature , 2015 . doi : 10 . 1038 / 520429a . [ 18 ] L . Leydesdorff , P . Wouters & L . Bornmann , ‘Professional and citizen bibliometrics : Complementarities and ambivalences in the development and use of indicators—a state - of - the - art report , ’ Scientometrics , 2016 . doi : 10 . 1007 / s11192 - 016 - 2150 - 8 . [ 19 ] U . Sandström & P . Van den Besselaar , ‘Funding , evaluation , and the performance of national research systems , ’ Journal of Informetrics , 2018 . doi : 10 . 1016 / j . joi . 2018 . 01 . 007 . [ 20 ] B . Broucker & K . De Wit , ‘New public management in higher education , ’ in The Palgrave international handbook of higher education policy and governance , Springer , 2015 . [ 21 ] G . Krücken , ‘Multiple competitions in higher education : A conceptual approach , ’ Innovation , 2021 . doi : 10 . 1080 / 14479338 . 2019 . 1684652 . [ 22 ] C . Musselin , ‘New forms of competition in higher education , ’ Socio - Economic Re - view , 2018 . doi : 10 . 1093 / ser / mwy033 . [ 23 ] O . Auranen & M . Nieminen , ‘University research funding and publication per - formance—An international comparison , ’ Research policy , 2010 . doi : 10 . 1016 / j . respol . 2010 . 03 . 003 . [ 24 ] M . Thelwall , K . Kousha , M . Abdoli et al . , ‘Is research funding always beneficial ? A cross - disciplinary analysis of U . K . research 2014 – 20 , ’ Quantitative Science Studies , 2023 . doi : 10 . 1162 / qss _ a _ 00254 . [ 25 ] I . I . Mitroff & D . E . Chubin , ‘Peer review at the NSF : A dialectical policy analysis , ’ Social Studies of Science , 1979 . doi : 10 . 1177 / 030631277900900203 . [ 26 ] J . McCullough , ‘First comprehensive survey of NSF applicants focuses on their concerns about proposal review , ’ Science , Technology , & Human Values , 1989 . doi : 10 . 1177 / 016224398901400107 . [ 27 ] L . Bornmann , ‘Scientific peer review , ’ Annual review of information science and technology , 2011 . doi : 10 . 1002 / aris . 2011 . 1440450112 . [ 28 ] L . Roumbanis , ‘The oracles of science : On grant peer review and competitive fund - ing , ’ Social Science Information , 2021 . doi : 10 . 1177 / 05390184211019241 . 20 [ 29 ] U . Sandström & M . Hällsten , ‘Persistent nepotism in peer - review , ’ Scientometrics , 2008 . doi : 10 . 1007 / s11192 - 008 - 0211 - 3 . [ 30 ] G . Breen , ‘Nepotism and sexism in peer - review , ’ Nature , 1997 . doi : 10 . 1038 / 38594 . [ 31 ] V . Bakanic , C . McPhail & R . J . Simon , ‘The manuscript review and decision - making process , ’ American Sociological Review , 1987 . doi : 10 . 2307 / 2095599 . [ 32 ] T . Luukkonen , ‘Conservatism and risk - taking in peer review : Emerging ERC prac - tices , ’ Research evaluation , 2012 . doi : 10 . 1093 / reseval / rvs001 . [ 33 ] B . Alberts , M . W . Kirschner , S . Tilghman & H . Varmus , ‘Rescuing US biomedical research from its systemic flaws , ’ Proceedings of the National Academy of Sciences , 2014 . doi : 10 . 1073 / pnas . 1404402111 . [ 34 ] H . W . Marsh , L . Bornmann , R . Mutz , H . - D . Daniel & A . O’Mara , ‘Gender effects in the peer reviews of grant proposals : A comprehensive meta - analysis comparing traditional and multilevel approaches , ’ Review of Educational Research , 2009 . doi : 10 . 3102 / 0034654309334143 . [ 35 ] G . D . L . Travis & H . M . Collins , ‘New light on old boys : Cognitive and institutional particularism in the peer review system , ’ Science , Technology , & Human Values , 1991 . doi : 10 . 1177 / 016224399101600303 . [ 36 ] Q . Wang & U . Sandström , ‘Defining the role of cognitive distance in the peer review process with an explorative study of a grant scheme in infection biology , ’ Research Evaluation , 2015 . doi : 10 . 1093 / reseval / rvv009 . [ 37 ] P . Van den Besselaar , ‘Selection committee membership : Service or self - service , ’ Journal of Informetrics , 2012 . doi : 10 . 1016 / j . joi . 2012 . 05 . 003 . [ 38 ] P . A . Abrams , ‘The predictive ability of peer review of grant proposals : The case of ecology and the US National Science Foundation , ’ Social Studies of Science , 1991 . doi : 10 . 1177 / 030631291021001006 . [ 39 ] E . L . Pier , M . Brauer , A . Filut et al . , ‘Low agreement among reviewers evaluating the same NIH grant applications , ’ Proceedings of the National Academy of Sciences , 2018 . doi : 10 . 1073 / pnas . 1714379115 . [ 40 ] N . E . Mayo , J . Brophy , M . S . Goldberg et al . , ‘Peering at peer review revealed high degree of chance associated with funding of grant applications , ’ Journal of clinical epidemiology , 2006 . doi : 10 . 1016 / j . jclinepi . 2005 . 12 . 007 . [ 41 ] R . Mutz , L . Bornmann & H . - D . Daniel , ‘Heterogeneity of inter - rater reliabilities of grant peer reviews and its determinants : A general estimating equations approach , ’ PLoS One , 2012 . doi : 10 . 1371 / journal . pone . 0048509 . 21 [ 42 ] E . A . Erosheva , P . Martinková & C . J . Lee , ‘When zero may not be zero : A cau - tionary note on the use of inter - rater reliability in evaluating grant peer review , ’ Journal of the Royal Statistical Society Series A : Statistics in Society , 2021 . doi : 10 . 1111 / rssa . 12681 . [ 43 ] L . Bornmann , R . Mutz & H . - D . Daniel , ‘A reliability - generalization study of journal peer reviews : A multilevel meta - analysis of inter - rater reliability and its determin - ants , ’ PloS one , 2010 . doi : 10 . 1371 / journal . pone . 0014331 . [ 44 ] J . - O . Hesselberg , T . K . Dalsbø , H . Stromme , I . Svege & A . Fretheim , ‘Reviewer training for improving grant and journal peer review , ’ Cochrane Database of Sys - tematic Reviews , 2023 . doi : https : / / doi . org / 10 . 1002 / 14651858 . MR000056 . pub2 . [ 45 ] S . Cole , J . R . Cole & G . A . Simon , ‘Chance and consensus in peer review , ’ Science ( New York , N . Y . ) , 1981 . doi : 10 . 1126 / science . 7302566 . [ 46 ] N . Graves , A . G . Barnett & P . Clarke , ‘Funding grant proposals for scientific research : Retrospective analysis of scores by members of grant review panel , ’ BMJ ( Clinical research ed . ) , 2011 . doi : 10 . 1136 / bmj . d4797 . [ 47 ] M . Lamont , How professors think : Inside the curious world of academic judgment . Harvard University Press , 2009 . [ 48 ] L . Roumbanis , ‘Academic judgments under uncertainty : A study of collective an - choring effects in Swedish Research Council panel groups , ’ Social studies of science , 2017 . doi : 10 . 1177 / 0306312716659789 . [ 49 ] J . Coveney , D . L . Herbert , K . Hill , K . E . Mow , N . Graves & A . Barnett , ‘‘Are you siding with a personality or the grant proposal ? ’ : Observations on how peer review panels function , ’ Research Integrity and Peer Review , 2017 . doi : 10 . 1186 / s41073 - 017 - 0043 - x . [ 50 ] N . Danthi , C . O . Wu , P . Shi & M . Lauer , ‘Percentile ranking and citation impact of a large cohort of National Heart , Lung , and Blood Institute – funded cardiovascular R01 grants , ’ Circulation research , 2014 . doi : 10 . 1161 / CIRCRESAHA . 114 . 302656 . [ 51 ] N . S . Danthi , C . O . Wu , D . M . DiMichele , W . K . Hoots & M . S . Lauer , ‘Cita - tion impact of NHLBI R01 grants funded through the American Recovery and Reinvestment Act as compared to R01 grants funded through a standard payline , ’ Circulation research , 2015 . doi : 10 . 1161 / CIRCRESAHA . 116 . 305894 . [ 52 ] P . Van den Besselaar & U . Sandström , ‘Early career grants , performance , and careers : A study on predictive validity of grant decisions , ’ Journal of Informetrics , 2015 . doi : 10 . 1016 / j . joi . 2015 . 07 . 011 . 22 [ 53 ] J . Doyle , K . Quinn , Y . Bodenstein , C . Wu , N . Danthi & M . Lauer , ‘Association of percentile ranking with citation impact and productivity in a large cohort of de novo NIMH - funded R01 grants , ’ Molecular psychiatry , 2015 . doi : 10 . 1038 / mp . 2015 . 71 . [ 54 ] J . R . Kaltman , F . J . Evans , N . S . Danthi , C . O . Wu , D . M . DiMichele & M . S . Lauer , ‘Prior publication productivity , grant percentile ranking , and topic - normalized citation impact of NHLBI cardiovascular R01 grants , ’ Circulation research , 2014 . doi : 10 . 1161 / CIRCRESAHA . 115 . 304766 . [ 55 ] S . A . Gallo , A . S . Carpenter , D . Irwin et al . , ‘The validation of peer review through research impact measures and the implications for funding strategies , ’ PLoS One , 2014 . doi : 10 . 1371 / journal . pone . 0106474 . [ 56 ] M . S . Lauer , N . S . Danthi , J . Kaltman & C . Wu , ‘Predicting productivity re - turns on investment : Thirty years of peer review , grant funding , and publication of highly cited papers at the National Heart , Lung , and Blood Institute , ’ Circulation research , 2015 . doi : 10 . 1161 / CIRCRESAHA . 115 . 306830 . [ 57 ] M . Reinhart , ‘Peer review of grant applications in biology and medicine . Reliability , fairness , and validity , ’ Scientometrics , 2009 . doi : 10 . 1007 / s11192 - 008 - 2220 - 7 . [ 58 ] L . Bornmann , L . Leydesdorff & P . Van den Besselaar , ‘A meta - evaluation of sci - entific research proposals : Different ways of comparing rejected to awarded applic - ations , ’ Journal of Informetrics , 2010 . doi : 10 . 1016 / j . joi . 2009 . 10 . 004 . [ 59 ] S . A . Gallo & S . R . Glisson , ‘External tests of peer review validity via impact measures , ’ Frontiers in Research Metrics and Analytics , 2018 . doi : 10 . 3389 / frma . 2018 . 00022 . [ 60 ] W . Thorngate , N . Faregh , M . Maclaren & Y . Greo , ‘Mining the archives : Analyses of CIHR research grant adjudications , ’ Tech . Rep . , 2002 . [ 61 ] K . B . Schmaling & S . A . Gallo , ‘Gender differences in peer reviewed grant applic - ations , awards , and amounts : A systematic review and meta - analysis , ’ Research Integrity and Peer Review , 2023 . doi : 10 . 1186 / s41073 - 023 - 00127 - 3 . [ 62 ] S . Kahn , S . J . Ceci & W . M . Williams , ‘Is there gender bias in grant success ? An extended meta - analysis , ’ An Extended Meta - Analysis ( march 31 , 2023 ) . Boston University Questrom School of Business Research Paper , 2023 . [ 63 ] M . Dresler , E . Buddeberg , U . Endesfelder et al . , ‘Effective or predatory funding ? Evaluating the hidden costs of grant applications , ’ Immunology & Cell Biology , 2023 . doi : 10 . 1111 / imcb . 12592 . [ 64 ] A . Barnett , Funding schemes that cost as much as they reward , 2021 . [ 65 ] G . Schweiger , ‘Can’t We Do Better ? A cost - benefit analysis of proposal writing in a competitive funding environment , ’ Plos one , 2023 . doi : 10 . 1371 / journal . pone . 0282320 . 23 [ 66 ] T . von Hippel & C . von Hippel , ‘To apply or not to apply : A survey analysis of grant writing costs and benefits , ’ PloS one , 2015 . doi : 10 . 1371 / journal . pone . 0118494 . [ 67 ] D . L . Herbert , A . G . Barnett , P . Clarke & N . Graves , ‘On the time spent preparing grant proposals : An observational study of Australian researchers , ’ BMJ open , 2013 . doi : 10 . 1136 / bmjopen - 2013 - 002800 . [ 68 ] A . A . Stone , C . A . Bachrach , J . B . Jobe , H . S . Kurtzman & V . S . Cain , The science of self - report : Implications for research and practice . Psychology Press , 1999 . [ 69 ] European - University - Association , ‘EFSI and horizon 2020 : Efficiency and oppor - tunity cost AN EUA REVIEW , ’ Tech . Rep . , 2017 . [ 70 ] S . Schneider , ‘Results of the 2018 FDP faculty workload survey . Technical report , the federal demonstration partnership , ’ Tech . Rep . , 2020 . [ 71 ] A . N . Link , C . A . Swann & B . Bozeman , ‘A time allocation study of university faculty , ’ Economics of Education Review , 2008 . doi : 10 . 1016 / j . econedurev . 2007 . 04 . 002 . [ 72 ] K . Vaesen & J . Katzav , ‘How much would each researcher receive if competitive government research funding were distributed equally among researchers ? ’ PLOS ONE , L . A . N . Amaral , Ed . , 2017 . doi : 10 . 1371 / journal . pone . 0183967 . [ 73 ] Mozilla - Foundation , Announcing the 2017 mozilla fellows for science ! 2017 . [ 74 ] K . Myers , ‘The potential benefits of costly applications in grant contests , ’ SSRN Electronic Journal , 2022 . doi : 10 . 2139 / ssrn . 4154820 . [ 75 ] J . Wang , R . Veugelers & P . Stephan , ‘Bias against novelty in science : A cautionary tale for users of bibliometric indicators , ’ Research Policy , 2017 . doi : 10 . 1016 / j . respol . 2017 . 06 . 006 . [ 76 ] L . Wu , D . Wang & J . A . Evans , ‘Large teams develop and small teams disrupt science and technology , ’ Nature , 2019 . doi : 10 . 1038 / s41586 - 019 - 0941 - 9 . [ 77 ] K . Thomas , ‘The structure of scientific revolutions , ’ International Encyclopedia of Unified Science , 1962 . [ 78 ] D . Wang & A . - L . Barabási , The science of science . Cambridge University Press , 2021 . [ 79 ] F . Radicchi , S . Fortunato & C . Castellano , ‘Universality of citation distributions : Toward an objective measure of scientific impact , ’ Proceedings of the National Academy of Sciences , 2008 . doi : 10 . 1073 / pnas . 0806977105 . [ 80 ] R . Veugelers , J . Wang & P . Stephan , ‘Do funding agencies select and enable risky research : Evidence from ERC using novelty as a proxy of risk taking , ’ National Bureau of Economic Research , Tech . Rep . , 2022 . [ 81 ] M . Lanoë , ‘The evaluation of competitive research funding : . an application to French programs , ’ Doctoral dissertation , Université de Bordeaux , 2018 . 24 [ 82 ] K . J . Boudreau , E . C . Guinan , K . R . Lakhani & C . Riedl , ‘Looking across and looking beyond the knowledge frontier : Intellectual distance , novelty , and resource allocation in science , ’ Management science , 2016 . doi : 10 . 1287 / mnsc . 2015 . 2285 . [ 83 ] C . S . Wagner & J . Alexander , ‘Evaluating transformative research programmes : A case study of the NSF Small Grants for Exploratory Research programme , ’ Research Evaluation , 2013 . doi : 10 . 1093 / reseval / rvt006 . [ 84 ] K . Gross & C . T . Bergstrom , ‘Why ex post peer review encourages high - risk re - search while ex ante review discourages it , ’ Proceedings of the National Academy of Sciences , 2021 . doi : 10 . 1073 / pnas . 2111615118 . [ 85 ] P . Azoulay , J . S . Graff Zivin & G . Manso , ‘Incentives and creativity : Evidence from the academic life sciences , ’ The RAND Journal of Economics , 2011 . doi : 10 . 1111 / j . 1756 - 2171 . 2011 . 00140 . x . [ 86 ] J . Wang , Y . - N . Lee & J . P . Walsh , ‘Funding model and creativity in science : Competitive versus block funding and status contingency effects , ’ Research Policy , 2018 . doi : 10 . 1016 / j . respol . 2018 . 03 . 014 . [ 87 ] H . Fang , ‘Peer review and over - competitive research funding fostering mainstream opinion to monopoly , ’ Scientometrics , 2011 . doi : 10 . 1007 / s11192 - 010 - 0323 - 4 . [ 88 ] X . Z . Liu & H . Fang , ‘Peer review and over - competitive research funding fostering mainstream opinion to monopoly . Part II , ’ Scientometrics , 2012 . doi : 10 . 1007 / s11192 - 011 - 0526 - 3 . [ 89 ] M . J . Joyner , N . Paneth & J . P . Ioannidis , ‘What happens when underperforming big ideas in research become entrenched ? ’ JAMA : the journal of the American Medical Association , 2016 . doi : 10 . 1001 / jama . 2016 . 11076 . [ 90 ] J . P . Ioannidis , I . Hozo & B . Djulbegovic , ‘Federal funding and citation metrics of US biomedical researchers , 1996 to 2022 , ’ JAMA Network Open , 2022 . doi : 10 . 1001 / jamanetworkopen . 2022 . 45590 . [ 91 ] C . Stavropoulou , M . Somai & J . P . Ioannidis , ‘Most UK scientists who publish extremely highly - cited papers do not secure funding from major public and charity funders : A descriptive analysis , ’ PLoS One , 2019 . doi : 10 . 1371 / journal . pone . 0211460 . [ 92 ] H . E . Longino , Science as social knowledge : Values and objectivity in scientific inquiry . Princeton university press , 1990 . [ 93 ] B . Crew , ‘Here’s how to deal with failure , say senior scientists , ’ Nature , 2019 . [ 94 ] G . Conroy , ‘Here’s why so many young researchers want to quit – in five graphs , ’ Nature , 2020 . 25 [ 95 ] D . L . Herbert , J . Coveney , P . Clarke , N . Graves & A . G . Barnett , ‘The impact of funding deadlines on personal workloads , stress and family relationships : A qual - itative study of Australian researchers , ’ BMJ open , 2014 . doi : 10 . 1136 / bmjopen - 2013 - 004462 . [ 96 ] E . Borgstrom , A . Driessen , M . Krawczyk , E . Kirby , J . MacArtney & K . Almack , ‘Grieving academic grant rejections : Examining funding failure and experiences of loss , ’ The Sociological Review , 2023 . doi : 10 . 1177 / 00380261231207196 . [ 97 ] S . Hall , A mental - health crisis is gripping science — toxic research culture is to blame , 2023 . doi : 10 . 1038 / d41586 - 023 - 01708 - 4 . [ 98 ] C . Woolston , ‘Postdoc survey reveals disenchantment with working life , ’ Nature , 2020 . doi : 10 . 1038 / d41586 - 020 - 03191 - 7 . [ 99 ] E . H . Ecklund & A . E . Lincoln , ‘Scientists want more children , ’ PLoS ONE , M . Perc , Ed . , 2011 . doi : 10 . 1371 / journal . pone . 0022590 . [ 100 ] E . A . Cech & M . Blair - Loy , ‘The changing career trajectories of new parents in STEM , ’ Proceedings of the National Academy of Sciences , 2019 . doi : 10 . 1073 / pnas . 1810862116 . [ 101 ] I . Kingsley , E . Slavich , L . Harvey - Smith , E . L . Johnston & L . A . Williams , ‘Gender differences in Australian research grant awards , applications , amounts , and work - force participation , ’ 2023 . doi : 10 . 31219 / osf . io / cpvqk . [ 102 ] A . Barnett , K . Page , C . Dyer & S . Cramb , ‘Meta - research : Justifying career dis - ruption in funding applications , a survey of Australian researchers , ’ eLife , 2022 . doi : 10 . 7554 / elife . 76123 . [ 103 ] C . M . Pribbenow , J . Sheridan , J . Winchell , D . Benting , J . Handelsman & M . Carnes , ‘The tenure process and extending the tenure clock : The experience of faculty at one university , ’ Higher Education Policy , 2010 . doi : 10 . 1057 / hep . 2009 . 18 . [ 104 ] M . S . Anderson , E . A . Ronning , R . D . Vries & B . C . Martinson , ‘The perverse ef - fects of competition on scientists’ work and relationships , ’ Science and Engineering Ethics , 2007 . doi : 10 . 1007 / s11948 - 007 - 9042 - 5 . [ 105 ] D . H . Osmond , ‘Malices wonderland : Research funding and peer review , ’ Journal of Neurobiology , 1983 . doi : 10 . 1002 / neu . 480140202 . [ 106 ] S . Conix , A . D . Block & K . Vaesen , ‘Grant writing and grant peer review as ques - tionable research practices , ’ F1000Research , 2021 . doi : 10 . 12688 / f1000research . 73893 . 2 . [ 107 ] L . A . Schimanski & J . P . Alperin , ‘The evaluation of scholarship in academic promotion and tenure processes : Past , present , and future , ’ F1000Research , 2018 . doi : 10 . 12688 / f1000research . 16493 . 1 . 26 [ 108 ] G . Gopalakrishna , G . Ter Riet , G . Vink , I . Stoop , J . M . Wicherts & L . M . Bouter , ‘Prevalence of questionable research practices , research misconduct and their po - tential explanatory factors : A survey among academic researchers in The Nether - lands , ’ PloS one , 2022 . doi : 10 . 1371 / journal . pone . 0263023 . [ 109 ] H . R . Garner , L . J . McIver & M . B . Waitzkin , ‘Same work , twice the money ? ’ Nature , 2013 . doi : 10 . 1038 / 493599a . [ 110 ] S . Conix , S . De Peuter , A . D . Block & K . Vaesen , ‘Questionable research practices in competitive grant funding : A survey , ’ Plos one , 2023 . doi : 10 . 1371 / journal . pone . 0293310 . [ 111 ] A . Barnett , P . Clarke & N . Graves , ‘Survey of NHMRC applicants , ’ 2023 . doi : 10 . 17605 / OSF . IO / 9KMTG . [ 112 ] Crime and Corruption Commission Queensland , ‘Australia’s first criminal prosec - ution for research fraud : A case study from the University of Queensland , ’ Tech . Rep . , 2017 . [ 113 ] The Office of Research Integrity , ‘Case summary : Brand , Toni M . , ’ Tech . Rep . , 2022 . [ 114 ] J . M . Parrilla , ‘ChatGPT use shows that the grant - application system is broken , ’ Nature , 2023 . doi : 10 . 1038 / d41586 - 023 - 03238 - 5 . [ 115 ] N . Huntington - Klein , The effect : An introduction to research design and causality . CRC Press , 2021 . [ 116 ] J . M . Cattell & J . Cattell , American men of science : A biographical directory . Bowker , 1910 . [ 117 ] J . H . Comroe Jr & R . D . Dripps , ‘Scientific basis for the support of biomedical science , ’ Science ( New York , N . Y . ) , 1976 . doi : 10 . 1126 / science . 769161 . [ 118 ] A . G . Barnett , ‘Funding by lottery : Political problems and research opportunities , ’ mBio , 2016 . doi : 10 . 1128 / mbio . 01369 - 16 . [ 119 ] S . Guthrie , I . Ghiga & S . Wooding , ‘What do we know about grant peer review in the health sciences ? ’ F1000Research , 2017 . doi : 10 . 12688 / f1000research . 11917 . 1 . [ 120 ] OpenSAFELY , 2024 . [ 121 ] L . Cruz - Castro & L . Sanz - Menéndez , ‘Gender bias in funding evaluation : A ran - domized experiment , ’ Quantitative Science Studies , 2023 . doi : 10 . 1162 / qss _ a _ 00263 . [ 122 ] M . Szomszor & E . Adie , ‘Overton : A bibliometric database of policy document citations , ’ Quantitative science studies , 2022 . doi : 10 . 1162 / qss _ a _ 00204 . 27 [ 123 ] T . Ciarli , A . AlDoh , S . Arora et al . , ‘Changing directions : Steering science , tech - nology and innovation towards the sustainable development goals , ’ Tech . Rep . , 2022 . [ 124 ] R . Smith , ‘Classical peer review : An empty gun , ’ Breast Cancer Research , 2010 . doi : 10 . 1186 / bcr2742 . [ 125 ] Science , No pressure : NSF test finds eliminating deadlines halves number of grant proposals , 2016 . [ 126 ] I . Reymert , ‘Bibliometrics in Academic Recruitment : A Screening Tool Rather than a Game Changer , ’ Minerva , 2021 . doi : 10 . 1007 / s11024 - 020 - 09419 - 0 . [ 127 ] M . H . MacRoberts & B . R . MacRoberts , ‘Problems of citation analysis : A study of uncited and seldom - cited influences , ’ Journal of the American Society for In - formation Science and Technology , 2010 . doi : 10 . 1002 / asi . 21228 . [ 128 ] J . P . Ioannidis & Z . Maniadis , ‘Quantitative research assessment : Using metrics against gamed metrics , ’ Internal and Emergency Medicine , 2023 . doi : 10 . 1007 / s11739 - 023 - 03447 - w . [ 129 ] C . Mom , T . Möller & P . Van den Besselaar , ‘Determinants of cognitive mobility , ’ in Proceedings of ISSI 2023 : 19th international conference of the International Society of Scientometrics and Informetrics , 2023 . [ 130 ] CoARA , Coalition for advancing research assessment , 2023 . [ 131 ] A . Rushforth & S . de Rijcke , ‘Practicing responsible research assessment : Qual - itative study of faculty hiring , promotion , and tenure assessments in the united states , ’ 2023 . doi : 10 . 31235 / osf . io / 2d7ax . [ 132 ] L . Bornmann & J . N . Marewski , ‘Heuristics as conceptual lens for understanding and studying the usage of bibliometrics in research evaluation , ’ Scientometrics , 2019 . doi : 10 . 1007 / s11192 - 019 - 03018 - x . [ 133 ] P . Van den Besselaar & U . Sandström , ‘Bibliometrically disciplined peer review : On using indicators in research evaluation , ’ Scholarly assessment reports , 2020 . doi : 10 . 29024 / sar . 16 . [ 134 ] I . Chalmers , M . B . Bracken , B . Djulbegovic et al . , ‘How to increase value and reduce waste when research priorities are set , ’ The Lancet , 2014 . doi : 10 . 1016 / s0140 - 6736 ( 13 ) 62229 - 1 . [ 135 ] F . Luebber , S . Krach , M . Martinez Mateo et al . , ‘Rethink funding by putting the lottery first , ’ Nature Human Behaviour , 2023 . doi : 10 . 1038 / s41562 - 023 - 01649 - y . [ 136 ] T . Stafford , I . Rombach , D . Hind et al . , ‘Where next for partial randomisation of research funding ? The feasibility of RCTs and alternatives , ’ Wellcome Open Research , 2023 . doi : 10 . 12688 / wellcomeopenres . 19565 . 1 . 28 [ 137 ] S . De Peuter & S . Conix , ‘The modified lottery : Formalizing the intrinsic ran - domness of research funding , ’ Accountability in Research , 2022 . doi : 10 . 1080 / 08989621 . 2021 . 1927727 . [ 138 ] M . Liu , V . Choy , P . Clarke , A . Barnett , T . Blakely & L . Pomeroy , ‘The acceptabil - ity of using a lottery to allocate research funding : A survey of applicants , ’ Research integrity and peer review , 2020 . doi : 10 . 1186 / s41073 - 019 - 0089 - z . [ 139 ] The - british - academy , Promising’ results from first year of innovative grant award - ing trial show greater diversity of awardees and institutions given funding , 2023 . [ 140 ] A . G . Barnett , P . Clarke , C . Vaquette & N . Graves , ‘Using democracy to award research funding : An observational study , ’ Research Integrity and Peer Review , 2017 . doi : 10 . 1186 / s41073 - 017 - 0040 - 0 . [ 141 ] K . Aagaard , A . Kladakis & M . W . Nielsen , ‘Concentration or dispersal of research funding ? ’ Quantitative Science Studies , 2020 . doi : 10 . 1162 / qss _ a _ 00002 . [ 142 ] D . A . Parry , B . I . Davidson , C . J . Sewall , J . T . Fisher , H . Mieczkowski & D . S . Quintana , ‘A systematic review and meta - analysis of discrepancies between logged and self - reported digital media use , ’ Nature Human Behaviour , 2021 . doi : 10 . 1038 / s41562 - 021 - 01117 - 5 . [ 143 ] A . Tatsioni , E . Vavva & J . P . Ioannidis , ‘Sources of funding for Nobel Prize - winning work : Public or private ? ’ The FASEB journal , 2010 . doi : 10 . 1096 / fj . 09 - 148239 . [ 144 ] L . Tiokhin , K . Panchanathan , P . E . Smaldino & D . Lakens , ‘Shifting the level of selection in science , ’ Perspectives on Psychological Science , 2021 . doi : 10 . 1177 / 17456916231182568 . 29