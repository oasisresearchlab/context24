Manuscript submitted to eLife TRex , a fast multi - animal tracking 1 system with markerless 2 identiﬁcation , and 2D estimation of 3 posture and visual ﬁelds 4 Tristan Walter 1 , 2 , 3 * and Iain D Couzin 1 , 2 , 3 * 5 * For correspondence : twalter @ ab . mpg . de ( TW ) ; icouzin @ ab . mpg . de ( IDC ) 1 Max Planck Institute of Animal Behavior , Germany ; 2 Centre for the Advanced Study of 6 Collective Behaviour , University of Konstanz , Germany ; 3 Department of Biology , 7 University of Konstanz , Germany 8 9 Abstract Automated visual tracking of animals is rapidly becoming an indispensable tool for 10 the study of behavior . It oﬀers a quantitative methodology by which organisms’ sensing and 11 decision - making can be studied in a wide range of ecological contexts . Despite this , existing 12 solutions tend to be challenging to deploy in practice , especially when considering long and / or 13 high - resolution video streams . Here , we present TRex , a fast and easy - to - use solution for tracking 14 a large number of individuals simultaneously with real - time ( 60Hz ) tracking performance for up 15 to approximately 256 individuals and estimates 2D body postures and visual ﬁelds , both in open 16 and closed - loop contexts . Additionally , TRex oﬀers highly - accurate , deep - learning - based visual 17 identiﬁcation of up to approximately 100 unmarked individuals , where it is between 2 . 5 - 46 . 7 18 times faster , and requires 2 - 10 times less memory , than comparable software ( with relative 19 performance increasing for more organisms and longer videos ) and provides interactive 20 data - exploration within an intuitive , platform - independent graphical user interface . 21 22 Introduction 23 Tracking multiple moving animals ( and multiple objects , generally ) is important in various ﬁelds of 24 research such as behavioral studies , ecophysiology , biomechanics , and neuroscience ( Dell et al . 25 ( 2014 ) ) . Many tracking algorithms have been proposed in recent years ( Ohayon et al . ( 2013 ) , Fuku - 26 naga et al . ( 2015 ) , Burgos - Artizzu et al . ( 2012 ) , Rasch et al . ( 2016 ) ) , often limited to / only tested with 27 a particular organism ( Hewitt et al . ( 2018 ) , Branson et al . ( 2009 ) ) or type of organism ( e . g . pro - 28 tists , Pennekamp et al . ( 2015 ) ; ﬂy larvae and worms , Risse et al . ( 2017 ) ) . Relatively few have been 29 tested with a range of organisms and scenarios ( Pérez - Escudero et al . ( 2014 ) , Sridhar et al . ( 2019 ) , 30 Rodriguez et al . ( 2018 ) ) . Furthermore , many existing tools only have a specialized set of features , 31 struggle with very long or high - resolution ( ≥ 4K ) videos , or simply take too long to yield results . Exist - 32 ing fast algorithms are often severely limited with respect to the number of individuals that can be 33 tracked simultaneously ; for example xyTracker ( Rasch et al . ( 2016 ) ) allows for real - time tracking at 34 40Hz while accurately maintaining identities , and thus is suitable for closed - loop experimentation 35 ( experiments where stimulus presentation can depend on the real - time behaviors of the individ - 36 uals , e . g . Bath et al . ( 2014 ) , Brembs and Heisenberg ( 2000 ) , Bianco and Engert ( 2015 ) ) , but has a 37 limit of being able to track only 5 individuals simultaneously . ToxTrac ( Rodriguez et al . ( 2018 ) ) , a 38 software comparable to xyTracker in it’s set of features , is limited to 20 individuals and relatively 39 1 of 69 Manuscript submitted to eLife low frame - rates ( ≤ 25fps ) . Others , while implementing a wide range of features and oﬀering high - 40 performance tracking , are costly and thus limited in access ( Noldus et al . ( 2001 ) ) . Perhaps with 41 the exception of proprietary software , one major problem at present is the severe fragmentation 42 of features across the various software solutions . For example , experimentalists must typically 43 construct work - ﬂows from many individual tools : One tool might be responsible for estimating 44 the animal’s positions , another for estimating their posture , another one for reconstructing visual 45 ﬁelds ( which in turn probably also estimates animal posture , but does not export it in any way ) 46 and one for keeping identities – correcting results of other tools post - hoc . It can take a very long 47 time to make them all work eﬀectively together , adding what is often considerable overhead to 48 behavioral studies . 49 TRex , the software released with this publication ( available at trex . run under an Open - Source 50 license ) , has been designed to address these problems , and thus to provide a powerful , fast and 51 easy to use tool that will be of use in a wide range of behavioral studies . It allows users to track 52 moving objects / animals , as long as there is a way to separate them from the background ( e . g . 53 static backgrounds , custom masks , as discussed below ) . In addition to the positions of individuals , 54 our software provides other per - individual metrics such as body shape and , if applicable , head - 55 / tail - position . This is achieved using a basic posture analysis , which works out of the box for most 56 organisms , and , if required , can be easily adapted for others . Posture information , which includes 57 the body center - line , can be useful for detecting e . g . courtship displays and other behaviors that 58 might not otherwise be obvious from mere positional data . Additionally , with the visual sense 59 often being one of the most important modalities to consider in behavioral research , we include 60 the capability for users to obtain a computational reconstruction of the visual ﬁelds of all individuals 61 ( Strandburg - Peshkin et al . 2013 , Rosenthal et al . 2015 ) . This not only reveals which individuals are 62 visible from an individual’s point - of - view , as well as the distance to them , but also which parts of 63 others’ bodies are visible . 64 Included in the software package is a task - speciﬁc tool , TGrabs , that is employed to pre - process 65 existing video ﬁles and which allows users to record directly from cameras capable of live - streaming 66 to a computer ( with extensible support from generic webcams to high - end machine vision cam - 67 eras ) . It supports most of the above - mentioned tracking features ( positions , posture , visual ﬁeld ) 68 and provides access to results immediately while continuing to record / process . This not only saves 69 time , since tracking results are available immediately after the trial , but makes closed - loop support 70 possible for large groups of individuals ( ≤ 128 individuals ) . TRex and TGrabs are written in C + + but , 71 as part of our closed - loop support , we are providing a Python - based general scripting interface 72 which can be fully customized by the user without the need to recompile or relaunch . This inter - 73 face allows for compatibility with external programs ( e . g . for closed - loop stimulus - presentation ) 74 and other custom extensions . 75 The fast tracking described above employs information about the kinematics of each organ - 76 ism in order to try to maintain their identities . This is very fast and useful in many scenarios , e . g . 77 where general assessments about group properties ( group centroid , alignment of individuals , den - 78 sity , etc . ) are to be made . However , when making conclusions about individuals instead , main - 79 taining identities perfectly throughout the video is a critical requirement . Every tracking method 80 inevitably makes mistakes , which , for small groups of two or three individuals or short videos , can 81 be corrected manually – at the expense of spending much more time on analysis , which rapidly 82 becomes prohibitive as the number of individuals to be tracked increases . To make matters worse , 83 when multiple individuals stay out of view of the camera for too long ( such as if individuals move 84 out of frame , under a shelter , or occlude one another ) there is no way to know who is whom 85 once they re - emerge . With no baseline truth available ( e . g . using physical tags as in Alarcón - Nieto 86 et al . ( 2018 ) , Nagy et al . ( 2013 ) ; or marker - less methods as in Pérez - Escudero et al . ( 2014 ) , Romero - 87 Ferrero et al . ( 2019 ) , Rasch et al . ( 2016 ) ) , these mistakes can not be corrected and accumulate over 88 time , until eventually all identities are fully shuﬄed . To solve this problem ( and without the need 89 to mark , or add physical tags to individuals ) , TRex can , at the cost of spending more time on analy - 90 2 of 69 Manuscript submitted to eLife sis ( and thus not during live - tracking ) , automatically learn the identity of up to approximately 100 91 unmarked individuals based on their visual appearance . This machine - learning based approach , 92 herein termed visual identiﬁcation , provides an independent source of information on the identity 93 of individuals , which is used to detect and correct potential tracking mistakes without the need for 94 human supervision . 95 In this paper , we will describe the most important functions of the software in a typical order 96 of execution . This is followed by an evaluation of these functions in terms of speed and reliability 97 using a wide range of experimental systems , including termites , fruit ﬂies , locusts and multiple 98 species of schooling ﬁsh ( although we stress that our software is not limited to such species ) . 99 Speciﬁcally regarding the visual identiﬁcation of unmarked individuals in groups , idtracker . ai 100 is currently state - of - the - art , yielding high - accuracy ( > 99 % in most cases ) in maintaining consistent 101 identity assignments across entire videos ( Romero - Ferrero et al . ( 2019 ) ) . Similarly to TRex , this is 102 achieved by training an artiﬁcial neural network to visually diﬀerentiate between individuals , and 103 using identity predictions from this network to avoid / correct tracking mistakes . Both approaches 104 work without human supervision , and are limited to approximately 100 individuals . Given that 105 idtracker . ai is the only currently available tool with visual identiﬁcation for such large groups of 106 individuals , and also because of the quality of results , we will use it as a benchmark for our visual 107 identiﬁcation system . Results will be compared in terms of both accuracy and computation speed , 108 showing TRex ’ ability to achieve the same high level of accuracy but typically at far higher speeds , 109 and with a much reduced memory requirement . 110 TRex is platform - independent and runs on all major operating systems ( Linux , Windows , macOS ) 111 and oﬀers complete batch processing support , allowing users to eﬃciently process entire sets 112 of videos without requiring human intervention . All parameters can be accessed either through 113 settings ﬁles , from within the graphical user interface ( or GUI ) , or using the command - line . The 114 user interface supports oﬀ - site access using a built - in web - server ( although it is recommended to 115 only use this from within a secure VPN environment ) . Available parameters are explained in the 116 documentation directly as part of the GUI and on an external website ( see below ) . Results can be 117 exported to independent data - containers ( NPZ or CSV ) for further analyses in software of the user’s 118 choosing . We will not go into detail regarding the many GUI functions since albeit being of great 119 utility to the researcher , they are only the means to easily apply the features presented herein . 120 Some examples will be given in the main text and appendix , but a comprehensive collection of all 121 of them , as well as detailed documentation , is available in the up - to - date online - documentation 122 which can be found at trex . run / docs . 123 Methods 124 In the following sections we will explore some of the methods implemented in TRex and TGrabs , 125 as well as their most important features in a typical order of operations ( see Figure 2 for a ﬂow 126 diagram ) , starting out with a raw video . We will then describe how trajectories are obtained and end 127 with the most technically involved features . The workﬂow for using our software is straightforward 128 and can be summarized in four stages : 129 1 . Segmentation in TGrabs . When recording a video or converting a previously recorded ﬁle ( e . g . 130 MP4 , . AVI , etc . ) , it is segmented into background and foreground - objects ( blobs ) . Results are 131 saved to a custom , non - proprietary video format ( PV ) ( Figure 1a ) . 132 2 . Tracking the video , either directly in TGrabs , or after preprocessing in TRex with access to 133 customizable visualizations and the ability to change tracking parameters on - the - ﬂy . Here , we 134 will describe two types of data available within TRex , 2D posture - and visual - ﬁeld estimation , 135 as well as real - time applications of such data ( Figure 1b ) . 136 3 . We then go into detail on how to train a neural network to perform visual identiﬁcation of indi - 137 viduals , used in the process of automatic identity correction ( Figure 1c ) . This step may not 138 be necessary in many cases , but it is the only way to guarantee consistent identities through - 139 3 of 69 Manuscript submitted to eLife out the video . It is also the most time - consuming step , as well as the only one involving ma - 140 chine learning . All previously collected posture - and other tracking - related data are utilized 141 in this step , placing it late in a typical workﬂow . 142 4 . Data visualization is a critical component of any research project , especially for unfamiliar 143 datasets , but manually crafting one for every new experiment can be very time - consuming . 144 Thus , TRex oﬀers a universal , highly customizable , way to make all collected data available 145 for interactive exploration ( Figure 1d ) – allowing users to change many display options and 146 recording video clips for external playback . Tracking parameters can be adjusted on the ﬂy 147 ( many with visual feedback ) – important e . g . when preparing a closed - loop feedback with a 148 new species or setup . 149 TGrabs TGrabs + TRex TGrabs ( a ) ( b ) TRex ( c ) ( d ) Segmentation ( real - time ) - lighting correction - undistortion - cropping - background subtraction - custom segmentation masks Tracking ( real - time ) - position - related data - posture - related data - visual ﬁelds - live tracking & customizable closed - loop feedback Automatic identity correction ( optional ) - up to 100 individuals - supports area - of - interest - hours of video in ~ 1 - 2x real - time - largely idependent of organism Exploration interactively explore . . . - all tracking - data - visual ﬁeld networks - individualized live - heatmaps export . . . - showcase videos with - showcase videos with in - video camera movement - normalized , cropped images of individuals and much more . . . Figure 1 . Videos are typically processed in four main stages , illustrated here each with a list of prominent features . Some of them are accessible from both TRex and TGrabs , while others are software speciﬁc ( as shown at the very top ) . ( a ) The video is either recorded directly with our software ( TGrabs ) , or converted from a pre - recorded video ﬁle . Live - tracking enables users to perform closed - loop experiments , for which a virtual testing environment is provided . ( b ) Videos can be tracked and parameters adjusted with visual feedback . Various exploration and data presentation features are provided and customized data streams can be exported for use in external software . ( c ) After successful tracking , automatic visual identiﬁcation can , optionally , be used to reﬁne results . An artiﬁcial neural network is trained to recognize individuals , helping to automatically correct potential tracking mistakes . In the last stage , many graphical tools are available to users of TRex , a selection of which is listed in ( d ) . Segmentation 150 When an image is ﬁrst received from a camera ( or a video ﬁle ) , the objects of interest potentially 151 present in the frame must be detected and cropped out . Several technologies are available to sep - 152 arate the foreground from the background . Various machine learning algorithms are frequently 153 used to great eﬀect , even for the most complex environments ( Hughey et al . 2018 , Robie et al . 154 2017 , Francisco et al . 2019 ) . These more advanced approaches are typically beneﬁcial for the anal - 155 ysis of ﬁeld - data or organisms that are very hard to see in video ( e . g . very transparent or low 156 contrast objects / animals in the scene ) . However , for most laboratory experiments , simpler ( and 157 also much faster ) , classical image - processing methods yield satisfactory results . More advanced 158 techniques like luminance equalization , which e . g . is useful when lighting varies between images , 159 image undistortion , and brightness / contrast adjustments are available in TGrabs – but are costlier 160 4 of 69 Manuscript submitted to eLife in terms of processing time . Thus , while it is certainly possible to segment objects from the back - 161 ground using external , e . g . deep - learning based , tools , we provide as a generically - useful capability 162 background - subtraction . This can be used immediately in experiments where there are relatively 163 static backgrounds . These are generated automatically by uniformly sampling images from the 164 source video ( s ) – diﬀerent modes are available ( min / max , mode and mean ) for the user to choose 165 from . Importantly , since many behavioral studies rely on ≥ 4K resolution videos , we heavily utilize 166 the GPU ( if available ) to speed up most of the image - processing , allowing TRex to scale well with 167 increasing image resolution . 168 TRex allows users to track anything as long as either ( i ) the background is relatively static while 169 the objects move at least occasionally , ( ii ) the objects / animals of interest have enough contrast 170 to the background or ( iii ) the user provides an additional binary mask per frame which is used to 171 separate the objects to be tracked from the background , the typical means of doing this being by 172 deep - learning based segmentation ( e . g . Caelles et al . 2017 ) . These masks are expected to be in a 173 video - format themselves and correspond 1 : 1 in length and dimensions to the video that is to be 174 analyzed . They are expected to be binary , marking individuals in white and background in black . 175 Of course , these binary videos could be tracked on their own , but would not retain grey - scale 176 information . There are a lot of possible applications where this could be useful ; but generally , 177 whenever individuals are really hard to detect visually and need to be recognized by a diﬀerent 178 software ( e . g . a machine - learning - based detection like Maninis et al . 2018 ) . Individual frames can 179 then be connected using our software as a second step . 180 The detected objects are saved to a custom non - proprietary compressed ﬁle format ( Prepro - 181 cessed Video or PV , see appendix The PV ﬁle format ) , that stores only the most essential information 182 from the original video stream : the objects and their pixel positions and values . This format is opti - 183 mized for quick random index access by the tracking software and stores other meta - information 184 ( like frame timings ) utilized during playback or analysis . When recording videos directly from a 185 camera , they can also be streamed to an additional and independent MP4 container format ( plus 186 information establishing the mapping between PV and MP4 video frames ) . 187 Tracking 188 Once animals ( or , more generally , termed " objects " henceforth ) have been successfully segmented 189 from the background , we can either use the live - tracking feature in TGrabs or open a preprocessed 190 ﬁle in TRex , to generate the trajectories of these objects . This process uses information regarding 191 an object’s movement ( i . e . its kinematics ) to follow it across frames , estimating future positions 192 based on previous velocity and angular speed . It will be referred to as " tracking " in the following 193 text , and is a required step in all workﬂows . 194 Note that this approach alone is very fast , but , as will be shown , is subject to error with respect 195 to maintaining individual identities . If that is required , there is a further step , outlined in Auto - 196 matic Visual Identiﬁcation Based on Machine Learning below , which can be applied at the cost of 197 processing speed . First , however , we will discuss the general basis of tracking , which is common 198 to approaches that do , and do not , require identities to be maintained with high - ﬁdelity . Tracking 199 can occur for two distinct categories , which are handled slightly diﬀerently by our software : 200 1 . there is a known number of objects 201 2 . there is an unknown number of objects 202 The ﬁrst case assumes that the number of tracked objects in a frame cannot exceed a certain 203 expected number of objects ( detected automatically or set by the user ) . This allows the algorithm 204 to make stronger assumptions , for example regarding noise , where otherwise " valid " objects ( con - 205 forming to size expectations ) are ignored due to their positioning in the scene ( e . g . too far away 206 from previously lost individuals ) . In the second case , new objects may be generated until all viable 207 objects in a frame are assigned . While being more susceptible to noise , this is useful for tracking 208 5 of 69 Manuscript submitted to eLife Segmentation / Closed - loop Tracking / Visual Identiﬁcation / Exploration Exports File Camera TGrabs tracking data ( . npz / . csv ) raw video ( . mp4 ) TRex preproc . video ( . pv ) quick load ( . results ) live - tracking and closed - loop Figure 2 . An overview of the interconnection between TRex , TGrabs and their data in - and output formats , with titles on the left corresponding to the stages in Figure 1 . Starting at the top of the ﬁgure , video is either streamed to TGrabs from a ﬁle or directly from a compatible camera . At this stage , preprocessed data are saved to a . pv ﬁle which can be read by TRex later on . Thanks to its integration with parts of the TRex code , TGrabs can also perform online tracking for limited numbers of individuals , and save results to a . results ﬁle ( that can be opened by TRex ) along with individual tracking data saved to numpy data - containers ( . npz ) , which can be used for analysis in third - party applications . If required , videos recorded directly using TGrabs can also be streamed to a . mp4 video ﬁle which can be viewed in commonly available video players like VLC . a large number of objects , where counting objects may not be possible , or where there is a highly 209 variable number of objects to be tracked . 210 For a given video , our algorithm processes every frame sequentially , extending existing trajec - 211 tories ( if possible ) for each of the objects found in the current frame . Every object can only be 212 assigned to one trajectory , but some objects may not be assigned to any trajectory ( e . g . in case the 213 number of objects exceeds the allowed number of individuals ) and some trajectories might not 214 be assigned to any object ( e . g . while objects are out of view ) . To estimate object identities across 215 frames we use an approach akin to the popular Kalman ﬁlter ( Kalman , 1960 ) which makes pre - 216 dictions based on multiple noisy data streams ( here , positional history and posture information ) . 217 In the initial frame , objects are simply assigned from top - left to bottom - right . In all other frames , 218 assignments are made based on probabilities ( see appendix Matching an object to an object in the 219 next frame ) calculated for every combination of object and trajectory . These probabilities repre - 220 sent the degree to which the program believes that " it makes sense " to extend an existing trajectory 221 with an object in the current frame , given its position and speed . Our tracking algorithm only con - 222 siders assignments with probabilities larger than a certain threshold , generally constrained to a 223 certain proximity around an object assigned in the previous frame . 224 Matching a set of objects in one frame with a set of objects in the next frame is representative 225 of a typical assignment problem , which can be solved in polynomial time ( e . g . using the Hungar - 226 ian method Kuhn 1955 ) . However , we found that , in practice , the computational complexity of 227 the Hungarian method can constrain analysis speed to such a degree that we decided to imple - 228 ment a custom algorithm , which we term tree - based matching , which has a better average - case 229 performance ( see evaluation ) , even while having a comparatively bad worst - case complexity . Our 230 algorithm constructs a tree of all possible object / trajectory combinations in the frame and tries to 231 ﬁnd a compatible ( such that no objects / trajectories are assigned twice ) set of choices , maximizing 232 the sum of probabilities amongst these choices ( described in detail in the appendix Matching an 233 6 of 69 Manuscript submitted to eLife object to an object in the next frame ) . Problematic are situations where a large number of objects 234 are in close proximity of one another , since then the number of possible sets of choices grows ex - 235 ponentially . These situations are avoided by using a mixed approach : tree - based matching is used 236 most of the time , but as soon as the combinatorical complexity of a certain situation becomes too 237 great , our software falls back on using the Hungarian method . If videos are known to be prob - 238 lematic throughout ( e . g . with > 100 individuals consistently very close to each other ) , the user may 239 choose to use an approximate method instead ( described in the appendix section 4 ) , which simply 240 iterates through all objects and assigns each to the trajectory for which it has the highest proba - 241 bility and subsequently does not consider whether another object has an even higher probability 242 for that trajectory . While the approximate method scales better with an increasing number of in - 243 dividuals , it is " wrong " ( seeing as it does not consider all possible combinations ) – which is why it is 244 not recommended unless strictly necessary . However , since it does not consider all combinations , 245 making it more sensitive to parameter choice , it scales better for very large numbers of objects 246 and produces results good enough for it to be useful in very large groups ( see appendix Table A2 ) . 247 Situations where objects / individuals are touching , partly overlapping , or even completely over - 248 lapping , is an issue that all tracking solutions have to deal with in some way . The ﬁrst problem is the 249 detection of such an overlap / crossing , the second is its resolution . idtracker . ai , for example , deals 250 only with the ﬁrst problem : It trains a neural network to detect crossings and essentially ignores 251 the involved individuals until the problem is resolved by movement of the individuals themselves . 252 However , using such an image - based approach can never be fully independent of the species or 253 even video ( it has to be retrained for each speciﬁc experiment ) while also being time - costly to use . 254 In some cases the size of objects might indicate that they contain multiple overlapping objects , 255 while other cases might not allow for such an easy distinction – e . g . when sexually dimorphic ani - 256 mals ( or multiple species ) are present at the same time . We propose a method , similar to xyTracker 257 in that it uses the object’s movement history to detect overlaps . If there are fewer objects in a re - 258 gion than would be expected by looking at previous frames , an attempt is made to split the biggest 259 ones in that area . The size of that area is estimated using the maximal speed objects are allowed to 260 travel per frame ( parameter , see documentation track _ max _ speed ) . This , of course , requires rela - 261 tively good predictions or , alternatively , high frame - rates relative to the object’s movement speeds 262 ( which are likely necessary anyway to observe behavior at the appropriate time - scales ) . 263 By default , objects suspected to contain overlapping individuals are split by thresholding their 264 background - diﬀerence image ( see appendix section 11 ) , continuously increasing the threshold un - 265 til the expected number ( or more ) similarly sized objects are found . Greyscale values and , more 266 generally , the shading of three - dimensional objects and animals often produces a natural gradi - 267 ent ( see for example Figure 4 ) making this process surprisingly eﬀective for many of the species 268 we tested with . Even when there is almost no visible gradient and thresholding produces holes in - 269 side objects , objects are still successfully separated with this approach . Missing pixels from inside 270 the objects can even be regenerated afterwards . The algorithm fails , however , if the remaining 271 objects are too small or are too diﬀerent in size , in which case the overlapping objects will not be 272 assigned to any trajectory until all involved objects are found again separately in a later frame . 273 After an object is assigned to a speciﬁc trajectory , two kinds of data ( posture and visual - ﬁelds ) 274 are calculated and made available to the user , which will each be described in one of the follow - 275 ing subsections . In the last subsection , we outline how these can be utilized in real - time tracking 276 situations . 277 Posture Analysis 278 Groups of animals are often modeled as systems of simple particles ( Inada and Kawachi 2002 , Cav - 279 agna et al . 2010 , Pérez - Escudero and de Polavieja 2011 ) , a reasonable simpliﬁcation which helps 280 to formalize / predict behavior . However , intricate behaviors , like courtship displays , can only be 281 fully observed once the body shape and orientation are considered ( e . g . using tools such as Deep - 282 PoseKit , Graving et al . 2019 , LEAP Pereira et al . ( 2019 ) / SLEAP Pereira et al . ( 2020 ) , and DeepLabCut , 283 7 of 69 Manuscript submitted to eLife Mathis et al . 2018 ) . TRex does not track individual body parts apart from the head and tail ( where 284 applicable ) , but even the included simple and fast 2D posture estimator already allows for deduc - 285 tions to be made about how an animal is positioned in space , bent and oriented – crucial e . g . when 286 trying to estimate the position of eyes / antennae as part of an analysis , where this is required ( e . g . 287 Strandburg - Peshkin et al . 2013 , Rosenthal et al . 2015 ) . 288 In TRex , the 2D posture of an animal consists of ( i ) an outline around the outer edge of a blob , 289 ( ii ) a center - line ( or midline for short ) that curves with the body and ( iii ) positions on the outline that 290 represent the front and rear of the animal ( typically head and tail ) . Our only assumptions here are 291 that the animal is bilateral with a mirror - axis through its center and that it has a beginning and an 292 end , and that the camera - view is roughly perpendicular to this axis . This is true for most animals , 293 but may not hold e . g . for jellyﬁsh ( with radial symmetry ) or animals with diﬀerent symmetries ( e . g . 294 radiolaria ( protozoa ) with spherical symmetry ) . Still , as long as the animal is not exactly circular 295 from the perspective of the camera , the midline will follow its longest axis and a posture can be 296 estimated successfully . The algorithm implemented in our software is run for every ( cropped out ) 297 image of an individual and processes it as follows : 298 i . A tree - based approach follows edge pixels around an object in a clock - wise manner . Drawing 299 the line around pixels , as implemented here , instead of through their centers , as done in compa - 300 rable approaches , helps with very small objects ( e . g . one single pixel would still be represented as 301 a valid outline , instead of a single point ) . 302 ii . The pointiest end of the outline is assumed , by default , to be either the tail or the head 303 ( based on curvature and area between the outline points in question ) . Assignment of head vs . tail 304 can be set by the user , seeing as some animals might have " pointier " heads than tails ( e . g . termite 305 workers , one of the examples we employ ) . Posture data coming directly from an image can be very 306 noisy , which is why the program oﬀers options to simplify outline shapes using an Elliptical Fourier 307 Transform ( EFT , see Iwata et al . 2015 , Kuhl and Giardina 1982 ) or smoothing via a simple weighted 308 average across points of the curve ( inspired by common subdivision techniques , see Warren and 309 Weimer 2001 ) . The EFT allows for the user to set the desired level of approximation detail ( via the 310 number of elliptic fourier descriptors , EFDs ) and thus make it " rounder " and less jittery . Using an 311 EFT with just two descriptors is equivalent to ﬁtting an ellipse to the animal’s shape ( as , for example , 312 xyTracker does ) , which is the simplest supported representation of an animal’s body . 313 iii . The reference - point chosen in ( ii ) marks the start for the midline - algorithm . It walks both left 314 and right from this point , always trying to move approximately the same distance on the outline 315 ( with limited wiggle - room ) , while at the same time minimizing the distance from the left to the right 316 point . This works well for most shapes and also automatically yields distances between a midline 317 point and its corresponding two points on the outline , estimating thickness of this object’s body at 318 this point . 319 Compared to the tracking itself , posture estimation is a time - consuming process and can be 320 disabled . It is , however , required to estimate – and subsequently normalize – an animal’s orienta - 321 tion in space ( e . g . required later in Automatic Visual Identiﬁcation Based on Machine Learning ) , or 322 to reconstruct their visual ﬁeld as described in the following sub - section . 323 Reconstructing 2D Visual Fields 324 Visual input is an important modality for many species ( e . g . ﬁsh Strandburg - Peshkin et al . 2013 , 325 Bilotta and Saszik 2001 and humans Colavita 1974 ) . Due to its importance in widely used model or - 326 ganisms like zebraﬁsh ( Danio rerio ) , we decided to include the capability to conduct a 2 - dimensional 327 reconstruction of each individual’s visual ﬁeld as part of the software . The requirements for this 328 are successful posture estimation and that individuals are viewed from above , as is usually the 329 case in laboratory studies . 330 The algorithm makes use of the fact that outlines have already been calculated during posture 331 estimation . Eye positions are estimated to be evenly distanced from the " snout " and will be spaced 332 apart depending on the thickness of the body at that point ( the distance is based on a ratio , relative 333 8 of 69 Manuscript submitted to eLife Figure 3 . Visual ﬁeld estimate of the individual in the center ( zoomed in , the individuals are approximately 2 - 3cm long , Video 15 ) . Right ( blue ) and left ( orange ) ﬁelds of view intersect in the binocular region ( pink ) . Most individuals can be seen directly by the focal individual ( 1 , green ) , which has a wide ﬁeld of view of 260 ◦ per eye . Individual 3 on the top - left is not detected by the focal individual directly and not part of its ﬁrst - order visual ﬁeld . However , second - order intersections ( visualized by grey lines here ) are also saved and accessible through a separate layer in the exported data . to body - size , which can be adjusted by the user ) . Eye orientation is also adjustable , which inﬂu - 334 ences the size of the stereoscopic part of the visual ﬁeld . We then use ray - casting to intersect rays 335 from each of the eyes with all other individuals as well as the focal individual itself ( self - occlusion ) . 336 Individuals not detected in the current frame are approximated using the last available posture . 337 Data are organized as a multi - layered 1D - image of ﬁxed size for each frame , with each image prep - 338 resenting angles from −180 ◦ to 180 ◦ for the given frame . Simulating a limited ﬁeld - of - view would 339 thus be as simple as cropping parts of these images oﬀ the left and right sides . The diﬀerent layers 340 per pixel encode : 341 1 . identity of the occluder 342 2 . distance to the occluder 343 3 . body - part that was hit ( distance from the head on the outline in percent ) 344 While the individuals viewed from above on a computer screen look 2 - dimensional , one major 345 disadvantage of any 2D approach is , of course , that it is merely a projection of the 3D scene . Any 346 visual ﬁeld estimator has to assume that , from an individual’s perspective , other individuals act 347 as an occluder in all instances ( see Figure 3 ) . This may only be partly true in the real world , de - 348 pending on the experimental design , as other individuals may be able to move slightly below , or 349 above , the focal individuals line - of - sight , revealing otherwise occluded conspeciﬁcs behind them . 350 We therefore support multiple occlusion - layers , allowing second - order and 𝑁 th - order occlusions 351 to be calculated for each individual . 352 Realtime Tracking Option for Closed - Loop Experiments 353 Live tracking is supported , as an option to the user , during the recording , or conversion , of a video 354 in TGrabs . When closed - loop feedback is enabled , TGrabs focusses on maintaining stable recording 355 frame - rates and may not track recorded frames if tracking takes too long . This is done to ensure 356 9 of 69 Manuscript submitted to eLife that the recorded ﬁle can later be tracked again in full / with higher accuracy ( thus no information is 357 lost ) if required , and to help the closed - loop feedback to stay synchronized with real - world events . 358 During development we worked with a mid - range gaming computer and Basler cameras at 90 fps 359 and 2048 2 px resolution , where drawbacks did not occur . Running the program on computers with 360 outdated or very low - end hardware may , however , aﬀect frame - rates . 361 TRex loads a prepared Python script , handing down an array of data per individual in every 362 frame . Which data ﬁelds are being generated and sent to the script is selected by the script . Avail - 363 able ﬁelds are : 364 • Position 365 • Midline information 366 • Visual ﬁeld 367 If the script ( or the tracking itself ) takes too long to execute each frame , more frames are 368 dropped until a stable frame rate can be achieved . This scales well for all computer - systems , but 369 may , with signiﬁcantly decreasing frame - rates ( such as if run on very outdated hardware ) , result in 370 very fragmented data and worse identity assignment . When the program terminates , the tracked 371 individual’s data are exported - along with a results ﬁle that can be loaded by the tracker at a 372 later time . 373 In order to make this interface easy to use for prototyping and to debug experiments , the script 374 may be changed during its run - time and will be reloaded if necessary . Errors in the Python code 375 lead to a temporary pause of the closed - loop part of the program ( not the recording ) until all errors 376 have been ﬁxed . 377 Additionally , thanks to Python being a fully - featured scripting language , it is also possible to 378 call and send information to other programs during real - time tracking . Communication with other 379 external programs may be necessary whenever easy - to - use Python interfaces are not available for 380 e . g . hardware being used by the experimenter . 381 Automatic Visual Identiﬁcation Based on Machine Learning 382 Tracking , when it is only based on individual’s positional history , can be very accurate under good 383 circumstances and is currently the fastest way to analyse video recordings or to perform closed - 384 loop experiments . However , such tracking methods simply do not have access to enough informa - 385 tion to allow them to ensure identities are maintained for the duration of most entire trials – small 386 mistakes can and will happen . There are cases , e . g . when studying polarity ( only based on short 387 trajectory segments ) , or other general group - level assessments , where this is acceptable and iden - 388 tities do not have to be maintained perfectly . However , consistent identities are required in many 389 individual - level assessments , and with no baseline truth available to correct mistakes , errors start 390 accumulating until eventually all identities are fully shuﬄed . Even a hypothetical , perfect tracking 391 algorithm will not be able to yield correct results in all situations as multiple individuals might go 392 out of view at the same time ( e . g . hiding under cover or just occluded by other animals ) . There is 393 no way to tell who is whom , once they re - emerge . 394 The only way to solve this problem is by providing an independent source of information from 395 which to infer identity of individuals , which is of course a principle we make use of all the time 396 in our everyday lives : Facial identiﬁcation of con - speciﬁcs is something that comes easily to most 397 of us , to an extent where we sometimes recognize face - like features where there aren’t any . Our 398 natural tendency to ﬁnd patterns enables us to train experts on recognizing diﬀerences between 399 animals , even when they belong to a completely diﬀerent taxonomic order . Tracking individuals is 400 a demanding task , especially with large numbers of moving animals ( Liu et al . 2009 shows humans 401 to be eﬀective for up to 4 objects ) . Human observers are able to solve simple memory recall tasks 402 for 39 objects at only 92 % correct ( see Humphrey and Khan 1992 ) , where the presented objects 403 do not even have to be identiﬁed individually ( just classiﬁed as old / new ) and contain more inher - 404 ent variation than most con - speciﬁc animals would . Even with this being true , human observers 405 10 of 69 Manuscript submitted to eLife are still the most eﬃcient solution in some cases ( e . g . for long - lived animals in complex habitats ) . 406 Enhancing visual inter - individual diﬀerences by attaching physical tags is an eﬀective way to make 407 the task easier and more straight - forward to automate . RFID tags are great at maintaining iden - 408 tities and useful in many situations , but are also limited since individuals have to be in very close 409 proximity to a sensor in order to be detected ( Bonter and Bridge , 2011 ) . Attaching QR codes to 410 animals allows for a very large number of individuals to be uniquely identiﬁed at the same time 411 ( Mersch et al . 2013 , Crall et al . 2015 theoretically up to 7000 ) – and over greater distance than RFID 412 tags . Generating codes can also be automated , generating tags with optimal visual inter - marker 413 distances ( Garrido - Jurado et al . , 2016 ) , making it feasible to identify a large number of individuals 414 with minimal tracking mistakes . 415 While physical tagging is often an eﬀective method by which to identify individuals , it requires 416 animals to be caught and manipulated , which can be diﬃcult ( Mersch et al . , 2013 ) and is subject to 417 the physical limitations of the respective system . Tags have to be large enough so a program can 418 recognize it in a video stream . Even worse , especially with increased relative tag - size , the animal’s 419 behavior may be aﬀected by the presence of the tag , and there might be no way for experimenters 420 to necessarily know that it did . In addition , for some animals , like ﬁsh and termites , attachment of 421 tags that are eﬀective for discriminating among a large number of individuals can be problematic 422 or impossible . 423 Recognizing such issues , ( Pérez - Escudero et al . , 2014 ) ﬁrst proposed an algorithm termed idtracker , 424 generalizing the process of pattern recognition for a range of diﬀerent species . Training an expert 425 program to tell individuals apart , by detecting slight diﬀerences in patterning on their bodies , al - 426 lows the correction of identities without any human involvement . Even while being limited to about 427 15 individuals per group , this was a very promising approach . It became much improved upon only 428 a few years later by the same group in their software idtracker . ai ( Romero - Ferrero et al . , 2019 ) , 429 implementing a paradigm shift from explicit , hard - coded , color - diﬀerence detection to using more 430 general machine learning methods instead – increasing the supported group size by an order of 431 magnitude . 432 We employ a method for visual identiﬁcation in TRex that is similar to the one used in idtracker . ai , 433 where a neural network is trained to visually recognize individuals and is used to correct tracking 434 mistakes automatically , without human intervention – the network layout ( see Figure 1c ) is almost 435 the same as well ( diﬀering only by the addition of a pre - processing layer and using 2D - instead 436 of 1D - dropout layers ) . However , in TRex , processing speed and chances of success are improved 437 ( the former being greatly improved ) by ( i ) minimizing the variance landscape of the problem and 438 ( ii ) exploring the landscape to our best ability , optimally covering all poses and lighting - conditions 439 an individual can be in , as well as ( iii ) shortening the training duration by signiﬁcantly altering the 440 training process – e . g . choosing new samples more adaptively and using diﬀerent stopping - criteria 441 ( accuracy , as well as speed , are part of the later evaluation ) . 442 While Tracking already tries to ( within each trajectory ) consistently follow the same individual , 443 there is no way to ensure / check the validity of this process without providing independent identity 444 information . Generating this source of information , based on the visual appearance of individu - 445 als , is what the algorithm for visual identiﬁcation , described in the following subsections , aims to 446 achieve . Re - stated simply , the goal of using automatic visual identiﬁcation is to obtain reliable pre - 447 dictions of the identities of all ( or most ) objects in each frame . Assuming these predictions are of 448 suﬃcient quality , they can be used to detect and correct potential mistakes made during Tracking 449 by looking for identity switches within trajectories . Ensuring that predicted identities within trajec - 450 tories are consistent , by proxy , also ensures that each trajectory is consistently associated with a 451 single , real individual . In the following , before describing the four stages of that algorithm , we will 452 point out key aspects of how tracking / image data are processed and how we addressed the points 453 ( i ) - ( iii ) above and especially highlight the features that ultimately improved performance compared 454 to other solutions . 455 11 of 69 Manuscript submitted to eLife Preparing Tracking - Data 456 Visual identiﬁcation starts out only with the trajectories that the Tracking provides . Tracking , on its 457 own , is already an improvement over other solutions , especially since ( unlike e . g . idtracker . ai ) 458 TRex makes an eﬀort to separate overlapping objects ( see the Algorithm for splitting touching indi - 459 viduals ) and thus is able to keep track of individuals for longer ( see appendix Figure A2 ) . Here , we 460 – quite conservatively – assume that , after every problematic situation ( deﬁned in the list below ) , 461 the assignments made by our tracking algorithm are wrong . Whenever a problematic situation is 462 encountered as part of a trajectory , we split the trajectory at that point . This way , all trajectories 463 of all individuals in a video become an assortment of trajectory snippets ( termed " segments " from 464 here on ) , which are clear of problematic situations , and for each of which the goal is to ﬁnd the 465 correct identity ( " correct " meaning that identities are consistently assigned to the same real indi - 466 vidual throughout the video ) . Situations are considered " problematic " , and cause the trajectory to 467 be split , when : 468 • The individual has been lost for at least one frame . For example when individuals are 469 moving unexpectedly fast , are occluded by other individuals / the environment , or simply not 470 present anymore ( e . g . eaten ) . 471 • Uncertainty of assignment was too high ( > 50 % ) e . g . due to very high movement speeds 472 or extreme variation in size between frames . With simpler tracking tasks in mind , these seg - 473 ments are kept as connected tracks , but regarded as separate ones here . 474 • Timestamps suggest skipped frames . Missing frames in the video may cause wrong as - 475 signments and are thus treated as if the individuals have been lost . This distinction can only 476 be made if accurate frame timings are available ( when recording using TGrabs or provided 477 alongside the video ﬁles in separate npz ﬁles ) . 478 Unless one of the above conditions becomes true , a segment is assumed to be consecutive 479 and connected ; that is , throughout the whole segment , no mistakes have been made that lead to 480 identities being switched . Frames where all individuals are currently within one such segment at 481 the same time will henceforth be termed global segments . 482 Since we know that there are no problematic situations inside each per - individual segment , and 483 thus also not across individuals within the range of a global segment , we can choose any global 484 segment as a basis for an initial , arbitrary assignment of identities to trajectories . One of the most 485 important steps of the identiﬁcation algorithm then becomes deciding which global segment is 486 the best starting point for the training . If a mistake is made here , consecutive predictions for other 487 segments will fail and / or produce unreliable results in general . 488 Only a limited set of global segments is kept – striking a balance between respecting user - given 489 constraints and capturing as much of the variance as possible . In many of the videos used for 490 evaluation , we found that only few segments had to be considered – however , computation time 491 is ultimately bounded by reducing the number of qualifying segments . While this is true , it is also 492 beneﬁcial to avoid auto - correlation by incorporating samples from all sections of the video instead 493 of only sourcing them from a small portion – to help achieve a balance , global segments are binned 494 by their middle frame into four bins ( each quarter of the video being a bin ) and then reducing the 495 number of segments inside each bin . With that goal in mind , we sort the segments within bins by 496 their " quality " – a combination of two factors : 497 1 . To capture as much as possible the variation due to an individual’s own movement , as well as 498 within the background that it moves across , a " good " segment should be a segment where 499 all individuals move as much as possible and also travel as large a distance as possible . Thus , 500 we derive a per - individual spatial coverage descriptor for the given segment by dissecting the 501 arena ( virtually ) into a grid of equally sized , rectangular " cells " ( depending on the aspect ratio 502 of the video ) . Each time an individual’s center - point moves from one cell to the next , a counter 503 is incremented for that individual . To avoid situations where , for example , all individuals but 504 12 of 69 Manuscript submitted to eLife ( a ) No normalization . ( b ) Using the main body - axis ( moments ) . ( c ) Using posture information . Figure 4 . Comparison of diﬀerent normalization methods . Images all stem from the same video and belong to the same identity . The video has previously been automatically corrected using the visual identiﬁcation . Each object visible here consists of 𝑁 images 𝑀 𝑖 , 𝑖 ∈ [ 0 , 𝑁 ] that have been accumulated into a single image using min 𝑖 ∈ [ 0 , 𝑁 ] 𝑀 𝑖 , with min being the element - wise minimum across images . The columns represent same samples from the same frames , but normalized in three diﬀerent ways : In ( a ) , images have not been normalized at all . Images in ( b ) have been normalized by aligning the objects along their main axis ( calculated using image - moments ) , which only gives the axis within 0 to 180 degrees . In ( c ) , all images have been aligned using posture information generated during the tracking process . As the images become more and more recognizable to us from left to right , the same applies to a network trying to tell identities apart : Reducing noise in the data speeds up the learning process . one are moving , we only use the lowest per - individual spatial coverage value to represent a 505 given segment . 506 2 . It is beneﬁcial to have more examples for the network to learn from . Thus , as a second sorting 507 criterion , we use the average number of samples per individual . 508 After being sorted according to these two metrics , the list of segments per bin is reduced , ac - 509 cording to a user - deﬁned variable ( 4 by default ) , leaving only the most viable options per quarter 510 of video . 511 The number of visited cells may , at ﬁrst , appear to be essentially equivalent to a spatially nor - 512 malized distance travelled ( as used in idtracker . ai ) . In edge cases , where individuals never stop 513 or always stop , both metrics can be very similar . However , one can imagine an individual continu - 514 ously moving around in the same corner of the arena , which would be counted as an equally good 515 segment for that individual as if it had traversed the whole arena ( and thus capturing all variable en - 516 vironmental factors ) . In most cases , using highly restricted movement for training is problematic , 517 and worse than using a shorter segment of the individual moving diagonally through the entire 518 space , since the latter captures more of the variation within background , lighting conditions and 519 the animals movement in the process . 520 Minimizing the Variance Landscape by Normalizing Samples 521 A big strength of machine learning approaches is their resistance to noise in the data . Generally , 522 any machine learning method will likely still converge - even with noisy data . Eliminating unnec - 523 essary noise and degrees of freedom in the dataset , however , will typically help the network to 524 converge much more quickly : Tasks that are easier to solve will of course also be solved more ac - 525 curately within similar or smaller timescales . This is due to the optimizer not having to consider 526 various parts of the possible parameter - space during training , or , put diﬀerently , shrinking the 527 overall parameter - space to the smallest possible size without losing important information . The 528 simplest such optimization included in most tracking and visual identiﬁcation approaches is to seg - 529 ment out the objects and centering the individuals in the cropped out images . This means that ( i ) 530 the network does not have to consider the whole image , ( ii ) needs only to consider one individual 531 at a time and ( iii ) the corners of the image can most likely be neglected . 532 Further improving on this , approaches like idtracker . ai align all objects along their most - 533 elongated axis , essentially removing global orientation as a degree of freedom . The orientation of 534 an arbitrary object can be calculated e . g . using an approach often referred to as image - moments 535 13 of 69 Manuscript submitted to eLife ( Hu , 1962 ) , yielding an angle within [ 0 − 180 ] ◦ . Of course , this means that 536 1 . circular objects have a random ( noisy ) orientation 537 2 . elongated objects ( e . g . ﬁsh ) can be either head - ﬁrst or ﬂipped by 180 ◦ and there is no way to 538 discriminate between those two cases ( see second row , Figure 4 ) 539 3 . a C - shaped body deformation , for example , results in a slightly bent axis , meaning that the 540 head will not be in exactly the same position as with a straight posture of the animal . 541 Each of these issues adds to the things the network has to learn to account for , widening the 542 parameter - space to be searched and increasing computation time . However , barring the ﬁrst point , 543 each problem can be tackled using the already available posture information . Knowing head and 544 tail positions and points along the individual’s center - line , the individual’s heads can be locked 545 roughly into a single position . This leaves room only for their rear end to move , reducing variation 546 in the data to a minimum ( see Figure 4 ) . In addition to faster convergence , this also results in better 547 generalization right from the start and even with a smaller number of samples per individual ( see 548 Figure 7 ) . 549 Guiding the Training Process 550 Per batch , the stochastic gradient descent is directed by the local accuracy ( a fraction of correct / total 551 predictions ) , which is a simple and commonly used metric that has no prior knowledge of where 552 the samples within a batch come from . This has the desirable consequence that no knowledge 553 about the temporal arrangement of images is necessary in order to train and , more importantly , 554 to apply the network later on . 555 In order to achieve accurate results quickly across batches , while at the same time making it pos - 556 sible to indicate to the user potentially problematic sequences within the video , we devised a metric 557 that can be used to estimate local as well as global training quality : We term this uniqueness and 558 it combines information about objects within a frame , following the principle of non - duplication ; 559 images of individuals within the same frame are required to be assigned diﬀerent identities by the 560 networks predictions . 561 The program generates image data for evenly spaced frames across the entire video . All images 580 of tracked individuals within the selected frames are , after every epoch of the training , passed on 581 to the network . It returns a vector of probabilities 𝑝 𝑖𝑗 for each image 𝑖 to be identity 𝑗 ∈ [ 0 , 𝑁 ] , with 582 𝑁 being the number of individuals . Based on these probabilities , uniqueness can be calculated as 583 in Box 1 , evenly covering the entire video . The magnitude of this probability vector per image is 584 taken into account , rewarding strong predictions of max 𝑗 { 𝑝 𝑖𝑗 } = 1 and punishing weak predictions 585 of max 𝑗 { 𝑝 𝑖𝑗 } < 1 . 586 Uniqueness is not integrated as part of the loss function , but it is used as a global gradient 587 before and after each training unit in order to detect global improvements . Based on the average 588 uniqueness calculated before and after a training unit , we can determine whether to stop the train - 589 ing , or whether training on the current segment made our results worse ( faulty data ) . If uniqueness 590 is consistently high throughout the video , then training has been successful and we may terminate 591 early . Otherwise , valleys in the uniqueness curve indicate bad generalization and thus currently 592 missing information regarding some of the individuals . In order to detect problematic sections of 593 the video we search for values below 1 − 0 . 5 𝑁 , meaning that the section potentially contains new 594 information we should be adding to our training data . Using accuracy per - batch and then using 595 uniqueness to determine global progress , we get the best of both worlds : A context - free prediction 596 method that is trained on global segments that are strategically selected by utilizing local context 597 information . 598 The closest example of such a procedure in idtracker . ai is the termination criterion after 599 protocol 1 , which states that individual segments have to be consistent and certain enough in all 600 global segments in order to stop iterating . While this seems to be similar at ﬁrst , the way accu - 601 racy is calculated and the terminology here are quite diﬀerent : ( i ) Every metric in idtracker . ai ’s 602 14 of 69 Manuscript submitted to eLife Box 1 . Calculating uniqueness for a frame 562563 Data : frame 𝑥 564 Result : Uniqueness score for frame 𝑥 565 uids = map { } 566 ̂𝑝 ( 𝑖 | 𝑏 ) is the probability of blob 𝑏 to be identity 𝑖 567 𝑓 ( 𝑥 ) returns a list of the tracked objects in frame 𝑥 568 𝐸 ( 𝑣 ) = ( 1 + exp ( − 𝜋 ) ) ∕ ( 1 + exp ( − 𝜋𝑣 ) ) is a shift of roughly + 0 . 5 and non - linear scaling of values 0 ≤ 𝑣 ≤ 1 569 570 571 foreach object 𝑏 ∈ 𝑓 ( 𝑥 ) do 572 maxid = arg max 𝑖 ̂𝑝 ( 𝑖 | 𝑏 ) with 𝑖 ∈ identities 573 if maxid ∈ uids then 574 uids [ maxid ] = max ( uids [ maxid ] , ̂𝑝 ( maxid , 𝑏 ) ) else 575 uids [ maxid ] = ̂𝑝 ( maxid , 𝑏 ) end 576 end 577 return | uids | −1 | 𝑓 ( 𝑥 ) | ∗ 𝐸 ( | uids | −1 ( ∑ 𝑖 ∈uids uids [ 𝑖 ] ) ) 578 Algorithm 1 : The algorithm used to calculate the uniqueness score for an individual frame . Probabilities ̂𝑝 ( 𝑖 | 𝑏 ) are predictions by the pre - trained network . During the accumulation these predictions will gradually improve proportional to the global training quality . Mul - tiplying the unique percentage | uids | −1 | 𝑓 ( 𝑥 ) | by the ( scaled ) mean probability deals with cases of low accuracy , where individuals switch every frame ( but uniquely ) . 579 ﬁnal assessment after protocol 1 is calculated at segment - level , not utilizing per - frame information . 603 Uniqueness works per - frame , not per segment , and considers individual frames to be entirely inde - 604 pendent from each other . It can be considered a much stronger constraint set upon the network’s 605 predictive ability , seeing as it basically counts the number of times mistakes are estimated to have 606 happened within single frames . Averaging only happens afterwards . ( ii ) The terminology of iden - 607 tities being unique is only used in idtracker . ai once after procotol 1 and essentially as a binary 608 value , not recognizing its potential as a descendable gradient . Images are simply added until a 609 certain percentage of images has been reached , at which point accumulation is terminated . ( iii ) 610 Testing uniqueness is much faster than testing network accuracy across segments , seeing as the 611 same images are tested over and over again ( meaning they can be cached ) and the testing dataset 612 can be much smaller due to its locality . Uniqueness thus provides a stronger gradient estimation , 613 while at the same time being more local ( meaning it can be used independently of whether images 614 are part of global segments ) , as well as more manageable in terms of speed and memory size . 615 In the next four sections , we describe the training phases of our algorithm ( 1 - 3 ) , and how the 616 successfully trained network can be used to automatically correct trajectories based on its predic - 617 tions ( 4 ) . 618 1 . The Initial Training Unit 619 All global segments are considered and sorted by the criteria listed below in 2 . Accumulation of 620 Additional Segments and Stopping - Criteria . The best suitable segment from the beginning of that 621 set of segments is used as the initial dataset for the network . Images are split into a training and 622 a validation set ( 4 : 1 ratio ) . Eﬀorts are made to equalize the sample sizes per class / identity before - 623 hand , but there has to always be a trade - oﬀ between similar sample sizes ( encouraging unbiased 624 priors ) and having as many samples as possible available for the network to learn from . Thus , in or - 625 15 of 69 Manuscript submitted to eLife der to alleviate some of the severity of dealing with imbalanced datasets , the performance during 626 training iterations is evaluated using a categorical focal loss function ( Lin et al . , 2020 ) . Focal loss 627 down - weighs classes that are already reliably predicted by the network and in turn emphasizes ne - 628 glected classes . An Adam optimizer ( Kingma and Ba , 2015 ) is used to traverse the loss landscape 629 towards the global ( or to at least a local ) minimum . 630 The network layout used for the classiﬁcation in TRex ( see Figure 1c ) is a typical Convolutional 631 Neural Network ( CNN ) . The concepts of " convolutional " and " downsampling " layers , as well as the 632 back - propagation used during training , are not new . They were introduced in Fukushima ( 1988 ) , 633 inspired originally by the work of Hubel and Wiesel on cats and rhesus monkeys ( Hubel and Wiesel 634 1959 , Hubel and Wiesel 1963 , Wiesel and Hubel 1966 ) , describing receptive ﬁelds and their hierar - 635 chical structure in the visual cortex . Soon afterward , in LeCun et al . ( 1989 ) , CNNs , in combination 636 with back - propagation , were already successfully used to recognize handwritten ZIP codes – for 637 the ﬁrst time , the learning process was fully automated . A critical step towards making their appli - 638 cation practical , and the reason they are popular today . 639 The network architecture used in our software is similar to the identiﬁcation module of the net - 640 work in Romero - Ferrero et al . ( 2019 ) , and is , as in most typical CNNs , ( reverse - ) pyramid - like . How - 641 ever , key diﬀerences between TRex ’ and idtracker . ai ’s procedure lie with the way that training 642 data is prepared ( see previous sections ) and how further segments are accumulated / evaluated 643 ( see next section ) . Furthermore , contrary to idtracker . ai ’s approach , images in TRex are aug - 644 mented ( during training ) before being passed on to the network . While this augmentation is rela - 645 tively simple ( random shift of the image in x - direction ) , it can help to account for positional noise 646 introduced by e . g . the posture estimation or the video itself when the network is used for pre - 647 dictions later on ( Perez and Wang , 2017 ) . We do not ﬂip the image in this step , or rotate it , since 648 this would defeat the purpose of using orientation normalization in the ﬁrst place ( as in Minimiz - 649 ing the Variance Landscape by Normalizing Samples , see Figure 4 ) . Here , in fact , normalization of 650 object orientation ( during training and predictions ) could be seen as a superior alternative to data 651 augmentation . 652 The input data for TRex ’ network is a single , cropped grayscale image of an individual ( see Fig - 653 ure 1c ) . This image is ﬁrst passed through a " lambda " layer ( blue ) that normalizes the pixel values , 654 dividing them by half the value limit of 255∕2 = 127 . 5 and subtracting 1 – this moves them into the 655 range of [ −1 , 1 ] . From then on , sections are a combination of convolutional layers ( kernel sizes of 656 16 , 64 and 100 pixels ) , each followed by a 2D ( 2x2 ) max - pooling and a 2D spatial dropout layer 657 ( with a rate of 0 . 25 ) . Within each of these blocks the input data is reduced further , focussing it 658 down to information that is deemed important . Towards the end , the data are ﬂattened and ﬂow 659 into a densely connected layer ( 100 units ) with exactly as many outputs as the number of classes . 660 The output is a vector with values between 0 and 1 for all elements of the vector , which , due to 661 softmax - activation , sum to 1 . 662 Training commences by performing a stochastic gradient descent ( using the Adam optimizer , 663 see Kingma and Ba 2015 ) , which iteratively minimizes the error between network predictions and 664 previously known associations of images with identities – the original assignments within the initial 665 frame segment . The optimizer’s behavior in the last ﬁve epochs is continuously observed and 666 training is terminated immediately if one of the following criteria is met : 667 • the maximum number of iterations is reached ( 150 by default , but can be set by the user ) 668 • a plateau is achieved at a high per - class accuracy 669 • overﬁtting / overly optimizing for the training data at the loss of generality 670 • no further improvements can be made ( due to the accuracy within the current training data 671 already being 1 ) 672 The initial training unit is also by far the most important as it determines the predicted identities 673 within further segments that are to be added . It is thus less risky to overﬁt than it is important 674 to get high - quality training results , and the algorithm has to be relatively conservative regarding 675 16 of 69 Manuscript submitted to eLife termination criteria . Later iterations , however , are only meant to extend an already existing dataset 676 and thus ( with computation speed in mind ) allow for additional termination criteria to be added : 677 • plateauing at / circling around a certain val _ loss level 678 • plateauing around a certain uniqueness level 679 2 . Accumulation of Additional Segments and Stopping - Criteria 680 If necessary , initial training results can be improved by adding more samples to the active dataset . 681 This could be done manually by the user , always trying to select the most promising segment next , 682 but requiring such manual work is not acceptable for high - throughput processing . Instead , in order 683 to translate this idea into features that can be calculated automatically , the following set of metrics 684 is re - generated per ( yet inactive ) segment after each successful step : 685 1 . Average uniqueness index ( rounded to an integer percentage in 5 % steps ) 686 2 . Minimal distance to regions that have previously been trained on ( rounded to the next power 687 of two ) , larger is better as it potentially includes samples more diﬀerent from the already 688 known ones 689 3 . Minimum cells visited per individual ( larger is better for the same reason as 2 ) 690 4 . Minimum average samples per individual ( larger is better ) 691 5 . Whether its image data has already been generated before ( mostly for saving memory ) 692 6 . The uniqueness value is smaller than 𝑈 2 𝑝𝑟𝑒𝑣 after 5 steps , with 𝑈 𝑝𝑟𝑒𝑣 being the best uniqueness 693 value previous to the current accumulation step 694 With the help of these values , the segment list is sorted and the best segment selected to be 695 considered next . Adding a segment to a set of already active samples requires us to correct the 696 identities inside it , potentially switching temporary identities to represent the same real identities 697 as in our previous data . This is done by predicting identities for the new samples using the network 698 that has been trained on the old samples . Making mistakes here can lead to signiﬁcant subsequent 699 problems , so merely plausible segments will be added - meaning only those samples are accepted 700 for which the predicted IDs are unique within each unobstructed sequence of frames for every 701 temporary identity . If multiple temporary individuals are predicted to be the same real identity , 702 the segment is saved for later and the search continues . 703 If multiple additional segments are found , the program tries to actively improve local unique - 704 ness valleys by adding samples ﬁrst from regions with comparatively low accuracy predictions . See - 705 ing as low accuracy regions will also most likely fail to predict unique identities , it is important to 706 emphasize here that this is generally not a problem for the algorithm : Failed segments are sim - 707 ply ignored and can be inserted back into the queue later . Smoothing the curve also makes sure 708 to prefer regions close to valleys , making the algorithm follow the valley walls upwards in both 709 directions . 710 Finishing a training unit does not necessarily mean that it was successful . Only the network 711 states improving upon results from previous units are considered and saved . Any training result - 712 except the initial one - may be rejected after training in case the uniqueness score has not improved 713 globally , or at least remained within 99 % of the previous best value . This ensures stability of the 714 process , even with tracking errors present ( which can be corrected for later on , see next section ) . 715 If a segment is rejected , the network is restored to the best recorded state . 716 Each new segment is always combined with regularly sampled data from previous steps , en - 717 suring that identities don’t switch back and forth between steps due to uncertain predictions . If 718 switching did occur , then the uniqueness and accuracy values can never reach high value regimes 719 – leading to the training unit being discarded as a result . The contribution of each previously added 720 segment 𝑅 is limited to ⌈ | 𝑅 𝑆 | ∕ ( samples _ max ∗ | 𝑅 | ∕ 𝑁 ) ⌉ samples , with 𝑁 as the total number of frames 721 in global segments for this individual and samples _ max a constant that is calculated using image size 722 and memory constraints ( or 1GB by default ) . 𝑅 𝑆 is the actual usable number of images in segment 723 17 of 69 Manuscript submitted to eLife 𝑅 . This limitation is an attempt to not bias the priors of the network by sub - sampling segments 724 according to their contribution to the total number of frames in global segments . 725 Training is considered to be successful globally , as soon as either ( i ) accumulative individual 726 gaps between sampled regions is less than 25 % of the video length for all individuals , or ( ii ) unique - 727 ness has reached a value higher than 728 1 − 0 . 5 𝑁 id , ( 1 ) so that almost all detected identities are present exactly once per frame . Otherwise , training will 729 be continued as described above with additional segments – each time extending the percentage 730 of images seen by the network further . 731 Training accuracy / consistency could potentially be further improved by letting the program add 732 an arbitrary amount of segments , however we found this not to be necessary in any of our test - 733 cases . Users are allowed to set a custom limit if required in their speciﬁc cases . 734 3 . The Final Training Unit 735 After the accumulation phase , one last training step is performed . In previous steps , validation data 736 has been kept strictly separate from the training set to get a better gauge on how generalizable the 737 results are to unseen parts of the video . Now this is no longer an objective – so why not use all of 738 the data available ? The entire dataset is simply merged and sub - sampled again , according to the 739 memory strategy used . Network training is started , with a maximum of max { 3 ; max _ epochs ∗ 0 . 25 } 740 iterations ( max _ epochs is 150 by default ) . During this training , the same stopping - criteria apply as 741 during the initial step . 742 Of course , typically , the answer to the above question of why not to do this would be " over - 743 ﬁtting " . However , there is still a way to detect overﬁtting if it occurs : Only improvements upon 744 previous steps – in terms of uniqueness – are saved , making this operation relatively safe . In fact , 745 for some videos this is the step where most progress is made ( e . g . Video 9 ) . The reason being that 746 this is the ﬁrst time when all of the training data from all segments is considered at once ( instead of 747 mostly the current segment plus fewer samples from previously accepted segments ) , and samples 748 from all parts of the video has an equal likelihood of being used in training after possible reduction 749 due to memory - constraints . 750 4 . Assigning Identities Based on Network Predictions 751 After the network has been successfully trained , all parts of the video which were not part of the 752 training are packaged together and the network calculates predictive probabilities for each image 753 of each individual to be any of the available identities . The vectors returned by the network are 754 then averaged per consecutive segment per individual . The average probability vectors for all over - 755 lapping segments are weighed against each other – usually forcing assignment to the most likely 756 identity ( ID ) for each segment , given that no other segments have similar probabilities . When re - 757 ferring to segments here , meant is simply a number of consecutive frames of one individual that 758 the tracker is fairly sure does not contain any mix - ups . We implemented a way to detect tracking 759 mistakes , which is mentioned later . 760 If an assignment is ambiguous , meaning that multiple segments 𝑆 𝑗 … 𝑀 overlapping in time have 761 the same maximum probability index arg max 𝑖 ∈ [ 0 , 𝑁 ] { 𝑃 ( 𝑖 | 𝑆 𝑗 ) } ( for the segment to belong to a certain 762 identity 𝑖 ) , a decision has to be made . Assignments are deferred if the ratio 763 𝑅 max = max { 𝑃 ( 𝑖 | 𝑆 𝑗 ) 𝑃 ( 𝑖 | 𝑆 𝑘 ) , ∀ 𝑆 𝑗 ≠ 𝑘 ∈ overlapping segments } between any two maximal probabilities is larger than 0 . 6 for said 𝑖 ( 𝑅 max is inverted if it is greater 764 than 1 ) . In such a case , we rely on the general purpose tracking algorithm to pick a sensible op - 765 tion – other identities might even be successfully assigned ( using network predictions ) in following 766 frames , which is a complexity we do not have to deal with here . In case all ratios are below 0 . 6 , 767 18 of 69 Manuscript submitted to eLife when the best choices per identity are not too ambiguous , the following steps are performed to 768 resolve remaining conﬂicts : 769 1 . count the number of samples 𝑁 𝑚𝑒 in the current segment , and the number of samples 𝑁 ℎ𝑒 in 770 the other segment that this segment is compared to 771 2 . calculate average probability vectors 𝑃 𝑚𝑒 and 𝑃 ℎ𝑒 772 3 . if 𝑆 ( 𝑃 𝑚𝑒 , 𝑁 𝑚𝑒 ) ≥ 𝑆 ( 𝑃 ℎ𝑒 , 𝑁 ℎ𝑒 ) , then assign the current segment with the ID in question . Otherwise 773 assign the ID to the other segment . Where : 774 norm ( 𝑥 ) = 𝑥 𝑁 𝑚𝑒 + 𝑁 ℎ𝑒 , sig ( 𝑥 ) = ( 1 + 𝑒 2 𝜋 ( 0 . 5− 𝑥 ) ) −1 𝑆 ( 𝑝 , 𝑥 ) = sig ( 𝑝 ) + sig ( norm ( 𝑥 ) ) . ( 2 ) This procedure prefers segments with larger numbers of samples over segments with fewer 775 samples , ensuring that identities are not switched around randomly whenever a short segment 776 ( e . g . of noisy data ) is predicted to be the given identity for a few frames – at least as long as a 777 better alternative is available . The non - linearity in 𝑆 ( 𝑝 , 𝑥 ) exaggerates diﬀerences between lower 778 values and dampens diﬀerences between higher values : For example , the quality of a segment 779 with 4000 samples is barely diﬀerent from a segment with 5000 samples ; however , there is likely to 780 be a signiﬁcant quality diﬀerence between segments with 10 and 100 samples . 781 In case something goes wrong during the tracking , e . g . an individual is switched with another 782 individual without the program knowing that it might have happened , the training might still be 783 successful ( for example if that particular segment has not been used for training ) . In such cases , 784 the program tries to correct for identity switches mid - segment by calculating a running - window 785 median identity throughout the whole segment . If the identity switches for a signiﬁcant length of 786 time , before identities are assigned to segments , the segment is split up at the point of the ﬁrst 787 change within the window and the two parts are handled as separate segments from then on . 788 Results 789 Below we assess TRex ’ performance regarding three properties that are most important when us - 790 ing it ( or in fact any tracking software ) in practice : ( i ) The time it takes to perform tracking ( ii ) the 791 time it takes to perform automatic identity correction and ( iii ) the peak memory consumption when 792 correcting identities ( since this is where memory consumption is maximal ) , as well as ( iv ) the accu - 793 racy of the produced trajectories after visual identiﬁcation . While accuracy is an important metric 794 and speciﬁc to identiﬁcation tasks , time and memory are typically of considerable practical im - 795 portance for all tasks . For example , tracking - speed may be the diﬀerence between only being 796 able to run a few trials or producing more reliable results with a much larger number of trials . 797 In addition , tracking speed can make a major diﬀerence as the number of individuals increases . 798 Furthermore , memory constraints can be extremely prohibitive making tracking over long video 799 sequences and / or for a large number of individuals extremely time - consuming , or impossible , for 800 the user . 801 In all of our tests we used a relatively modest computer system , which could be described as a 802 mid - range consumer or gaming PC : 803 • Intel Core i9 - 7900X CPU 804 • NVIDIA Geforce 1080 Ti 805 • 64GB RAM 806 • NVMe PCIe x4 hard - drive 807 • Debian bullseye ( debian . org ) 808 As can be seen in the following sections ( memory consumption , processing speeds , etc . ) using 809 a high - end system is not necessary to run TRex and , anecdotally , we did not observe noticeable 810 improvements when using a solid state drive versus a normal hard drive . A video card ( presently 811 19 of 69 Manuscript submitted to eLife Table 1 . A list of the videos used in this paper as part of the evaluation of TRex , along with the species of animals in the videos and their common names , as well as other video - speciﬁc properties . Videos are given an incremental ID , to make references more eﬃcient in the following text , which are sorted by the number of individuals in the video . Individual quantities are given accurately , except for the videos with more than 100 where the exact number may be slightly more or less . These videos have been analysed using TRex ’ dynamic analysis mode that supports unknown quantities of animals . ID species common # ind . fps ( Hz ) duration size ( px 2 ) 0 Leucaspius delineatus sunbleak 1024 40 8min20s 3866 × 4048 1 Leucaspius delineatus sunbleak 512 50 6min40s 3866 × 4140 2 Leucaspius delineatus sunbleak 512 60 5min59s 3866 × 4048 3 Leucaspius delineatus sunbleak 256 50 6min40s 3866 × 4140 4 Leucaspius delineatus sunbleak 256 60 5min59s 3866 × 4048 5 Leucaspius delineatus sunbleak 128 60 6min 3866 × 4048 6 Leucaspius delineatus sunbleak 128 60 5min59s 3866 × 4048 7 Danio rerio zebraﬁsh 100 32 1min 3584 × 3500 8 Drosophila melanogaster fruit - ﬂy 59 51 10min 2306 × 2306 9 Schistocerca gregaria locust 15 25 1h0min 1880 × 1881 10 Constrictotermes cyphergaster termite 10 100 10min5s 1920 × 1080 11 Danio rerio zebraﬁsh 10 32 10min10s 3712 × 3712 12 Danio rerio zebraﬁsh 10 32 10min3s 3712 × 3712 13 Danio rerio zebraﬁsh 10 32 10min3s 3712 × 3712 14 Poecilia reticulata guppy 8 30 3h15min22s 3008 × 3008 15 Poecilia reticulata guppy 8 25 1h12min 3008 × 3008 16 Poecilia reticulata guppy 8 35 3h18min13s 3008 × 3008 17 Poecilia reticulata guppy 1 140 1h9min32s 1312 × 1312 Table 1 – source data 1 . Videos 7 and 8 , as well as 13 - 11 , are available as part of the original idtracker paper ( Pérez - Escudero et al . ( 2014 ) ) . Many of the videos are part of yet unpublished data : Guppy videos have been recorded by A . Albi , videos with sunbleak ( Leucaspius delineatus ) have been recorded by D . Bath . The termite video has been kindly provided by H . Hugo and the locust video by F . Oberhauser . Due to the size of some of these videos ( > 150GB per video ) , they have to be made available upon speciﬁc request . Raw versions of these videos ( some trimmed ) , as well as full preprocessed versions , are available as part of videos . trex . run . an NVIDIA card due to the requirements of TensorFlow ) is recommended for tasks involving visual 812 identiﬁcation as such computations will take much longer without it – however , it is not required . 813 We decided to employ this system due to having a relatively cheap , compatible graphics card , as 814 well as to ensure that we have an easy way to produce direct comparisons with idtracker . ai 815 – which according to their website requires large amounts of RAM ( 32 - 128GB , idtrackerai online 816 documentation , accessed 05 / 21 / 2020 ) and a fast solid - state drive . 817 Table 1 shows the entire set of videos used in this paper , which have been obtained from mul - 818 tiple sources ( credited under the table ) and span a wide range of diﬀerent organisms , demonstrat - 819 ing TRex ’ ability to track anything as long as it moves occasionally . Videos involving a large number 820 ( > 100 ) of individuals are all the same species of ﬁsh since these were the only organisms we had 821 available in such quantities . However , this is not to say that only ﬁsh could be tracked eﬃciently 822 in these quantities . We used the full dataset with up to 1024 individuals in one video ( Video 0 ) to 823 evaluate raw tracking speed without visual identiﬁcation and identity corrections ( next sub - section ) . 824 However , since such numbers of individuals exceed the capacity of the neural network used for 825 automatic identity corrections ( compare also Romero - Ferrero et al . ( 2019 ) who used a similar net - 826 work ) , we only used a subset of these videos ( videos 7 through 16 ) to look speciﬁcally into the 827 quality of our visual identiﬁcation in terms of keeping identities and its memory consumption . 828 20 of 69 Manuscript submitted to eLife Tracking : Speed and Accuracy 829 In evaluating the Tracking portion of TRex , the main focus lies with processing speed , while accu - 830 racy in terms of keeping identities is of secondary importance . Tracking is required in all other 831 parts of the software , making it an attractive target for extensive optimization . Especially with re - 832 gards to closed - loop , and live - tracking situations , there may be no room even to lose a millisecond 833 between frames and thus risk dropping frames . We therefore designed TRex to support the simul - 834 taneous tracking of many ( ≥ 256 ) individuals quickly and achieve reasonable accuracy for up to 100 835 individuals – which are the two suppositions we will investigate in the following . 836 Trials were run without posture / visual - ﬁeld estimation enabled , where tracking generally , and 837 consistently , reaches speeds faster than real - time ( processing times of 1 . 5 - 40 % of the video dura - 838 tion , 25 - 100Hz ) even for a relatively large number of individuals ( 77 - 94 . 77 % for up to 256 individ - 839 uals , see Table A1 ) . Videos with more individuals ( > 500 ) were still tracked within reasonable time 840 of 235 % to 358 % of the video duration . As would be expected from these results , we found that 841 combining tracking and recording in a single step generally leads to higher processing speeds . The 842 only situation where this was not the case was a video with 1024 individuals , which suggests that 843 live - tracking ( in TGrabs ) handles cases with many individuals slightly worse than oﬄine tracking ( in 844 TRex ) . Otherwise , 5 % to 35 % shorter total processing times were measured ( 14 . 55 % on average , 845 see Table A4 ) , compared to running TGrabs separately and then tracking in TRex . These percent - 846 age diﬀerences , in most cases , reﬂect the ratio between the video duration and the time it takes to 847 track it , suggesting that most time is spent – by far – on the conversion of videos . This additional 848 cost can be avoided in practice when using TGrabs to record videos , by directly writing to a custom 849 format recognized by TRex , and / or using its live - tracking ability to export tracking data immediately 850 after the recording is stopped . 851 We also investigated trials that were run with posture estimation enabled and we found that 852 real - time speed could be achieved for videos with ≤ 128 individuals ( see column " tracking " in Ta - 853 ble A4 ) . Tracking speed , when posture estimation is enabled , depends more strongly on the size 854 of individuals in the image . 855 Generally , tracking software becomes slower as the number of individuals to be tracked in - 856 creases , as a result of an exponentially growing number of combinations to consider during match - 857 ing . Comparing our mixed approach ( see Tracking ) to purely using the Hungarian method ( also 858 known as the Kuhn – Munkres algorithm ) shows that , while both perform similarly for few individu - 859 als , the Hungarian method is easily outperformed by our algorithm for larger groups of individuals 860 ( as can be seen in Figure A3 ) . This might be due to custom optimizations regarding local cliques of 861 individuals , whereby we ignore objects that are too far away , and also as a result of our optimized 862 pre - sorting . The Hungarian method has the advantage of not leading to combinatorical explosions 863 in some situations – and thus has a lower maximum complexity while proving to be less optimal in 864 the average case . For further details , see the appendix : Matching an object to an object in the next 865 frame . 866 In addition to speed , we also tested the accuracy of our tracking method , with regards to 867 the consistency of identity assignments , comparing its results to the manually reviewed data ( the 868 methodology of which is described in the next section ) . In order to avoid counting follow - up errors 869 as " new " errors , we divided each trajectory in the uncorrected data into " uninterrupted " segments 870 of frames , instead of simply comparing whole trajectories . A segment is interrupted when an in - 871 dividual is lost ( for any of the reasons given in Preparing Tracking - Data ) and starts again when it 872 is reassigned to another object later on . We term these ( re - ) assignments decisions here . Each seg - 873 ment of every individual can be uniquely assigned to a similar / identical segment in the baseline 874 data and its identity . Following one trajectory in the uncorrected data , we can detect these wrong 875 decisions by checking whether the baseline identity associated with one segment of that trajectory 876 changes in the next . We found that roughly 80 % of such decisions made by the tree - based match - 877 ing were correct , even with relatively high numbers of individuals ( 100 ) . For trajectories where 878 21 of 69 Manuscript submitted to eLife no manually reviewed data were available , we used automatically corrected trajectories as a base 879 for our comparison – we evaluate the accuracy of these automatically corrected trajectories in the 880 following section . Even though we did not investigate accuracy in situations with more than 100 881 individuals , we suspect similar results since the property with the strongest inﬂuence on tracking 882 accuracy – individual density – is limited physically and most of the investigated species school 883 tightly in either case . 884 Visual Identiﬁcation : Accuracy 885 Since the goal of using visual identiﬁcation is to generate consistent identity assignments , we eval - 886 uated the accuracy of our method in this regard . As a benchmark , we compare it to manually 887 reviewed datasets as well as results from idtracker . ai for the same set of videos ( where possi - 888 ble ) . In order to validate trajectories exported by either software , we manually reviewed multiple 889 videos with the help from a tool within TRex that allows to view each crossing and correct possi - 890 ble mistakes in - place . Assignments were deemed incorrect , and subsequently corrected by the 891 reviewer , if the centroid of a given individual was not contained within the object it was assigned 892 to ( e . g . the individual was not part of the correct object ) . Double assignments per object are im - 893 possible due to the nature of the tracking method . Individuals were also forcibly assigned to the 894 correct objects in case they were visible but not detected by the tracking algorithm . After manual 895 corrections had been applied , " clean " trajectories were exported – providing a per - frame baseline 896 truth for the respective videos . A complete table of reviewed videos , and the percentage of re - 897 viewed frames per video , can be found in Table 3 . For longer videos ( > 1h ) we relied entirely on 898 a comparison between results from idtracker . ai and TRex . Their paper ( Romero - Ferrero et al . 899 ( 2019 ) ) suggests a very high accuracy of over 99 . 9 % correctly identiﬁed individual images for most 900 videos , which should suﬃce for most relevant applications and provide a good baseline truth . As 901 long as both tools produce suﬃciently similar trajectories , we therefore know they have found the 902 correct solution . 903 Table 2 . Evaluating comparability of the automatic visual identiﬁcation between idtracker . ai and TRex . Columns show various video properties , as well as the associated uniqueness score ( see Box 1 ) and a similarity metric . Similarity ( % similar individuals ) is calculated based on comparing the positions for each identity exported by both tools , choosing the closest matches overall and counting the ones that are diﬀerently assigned per frame . An individual is classiﬁed as " wrong " in that frame , if the euclidean distance between the matched solutions from idtracker . ai and TRex exceeds 1 % of the video width . The column " % similar individuals " shows percentage values , where a value of 99 % would indicate that , on average , 1 % of the individuals are assigned diﬀerently . To demonstrate how uniqueness corresponds to the quality of results , the last column shows the average uniqueness achieved across trials . video # ind . N TRex % similar individuals ⌀ ﬁnal uniqueness 7 100 5 99 . 9522 ± 0 . 2536 0 . 9758 ± 0 . 0018 8 59 5 99 . 7249 ± 0 . 5586 0 . 9356 ± 0 . 0358 13 10 5 99 . 9907 ± 0 . 3668 0 . 9812 ± 0 . 0013 12 10 5 99 . 9565 ± 0 . 8381 0 . 9698 ± 0 . 0024 11 10 5 99 . 9218 ± 1 . 116 0 . 9461 ± 0 . 0039 14 8 5 98 . 8185 ± 5 . 8095 0 . 9192 ± 0 . 0077 15 8 5 99 . 241 ± 4 . 2876 0 . 9576 ± 0 . 0023 16 8 5 99 . 8063 ± 1 . 9556 0 . 9481 ± 0 . 0025 A direct comparison between TRex and idtracker . ai was not possible for videos 9 and 10 904 , where idtracker . ai frequently exceeded hardware memory - limits and caused the application 905 to be terminated , or did not produce usable results within multiple days of run - time . However , 906 we were able to successfully analyse these videos with TRex and evaluate its performance by com - 907 paring to manually reviewed trajectories ( see below in Visual Identiﬁcation : Accuracy ) . Due to the 908 22 of 69 Manuscript submitted to eLife Table 3 . Results of the human validation for a subset of videos . Validation was performed by going through all problematic situations ( e . g . individuals lost ) and correcting mistakes manually , creating a fully corrected dataset for the given videos . This dataset may still have missing frames for some individuals , if they could not be detected in certain frames ( as indicated by " of that interpolated " ) . This was usually a very low percentage of all frames , except for Video 9 , where individuals tended to rest on top of each other – and were thus not tracked – for extended periods of time . This baseline dataset was compared to all other results obtained using the automatic visual identiﬁcation by TRex ( 𝑁 = 5 ) and idtracker . ai ( 𝑁 = 3 ) to estimate correctness . We were not able to track videos 9 and 10 with idtracker . ai , which is why correctness values are not available . video metrics review stats % correct video # ind . reviewed ( % ) of that interpolated ( % ) TRex idtracker . ai 7 100 100 . 0 0 . 23 99 . 97 ± 0 . 013 98 . 95 ± 0 . 146 8 59 100 . 0 0 . 15 99 . 68 ± 0 . 533 99 . 94 ± 0 . 0 9 15 22 . 2 8 . 44 95 . 12 ± 6 . 077 N / A 10 10 100 . 0 1 . 21 99 . 7 ± 0 . 088 N / A 13 10 100 . 0 0 . 27 99 . 98 ± 0 . 0 99 . 96 ± 0 . 0 12 10 100 . 0 0 . 59 99 . 94 ± 0 . 006 99 . 63 ± 0 . 0 11 10 100 . 0 0 . 5 99 . 89 ± 0 . 009 99 . 34 ± 0 . 002 stochastic nature of machine learning , and thus the inherent possibility of obtaining diﬀerent re - 909 sults in each run , as well as other potential factors inﬂuencing processing time and memory con - 910 sumption , both TRex and idtracker . ai have been executed repeatedly ( 5x TRex , 3x idtracker . ai ) . 911 The trajectories exported by both idtracker . ai and TRex were very similar throughout ( see 912 Table 2 ) . While occasional disagreements happened , similarity scores were higher than 99 % in 913 all cases ( i . e . less than 1 % of individuals have been diﬀerently assigned in each frame on average ) . 914 Most diﬃculties that did occur were , after manual review , attributable to situations where multiple 915 individuals cross over excessively within a short time - span . In each case that has been manually 916 reviewed , identities switched back to the correct individuals – even after temporary disagreement . 917 We found that both solutions occasionally experienced these same problems , which often occur 918 when individuals repeatedly come in and out of view in quick succession ( e . g . overlapping with 919 other individuals ) . Disagreements were expected for videos with many such situations due to the 920 way both algorithms deal diﬀerently with them : idtracker . ai assigns identities only based on the 921 network output . In many cases , individuals continue to partly overlap even while already being 922 tracked , which results in visual artifacts and can lead to unstable predictions by the network and 923 causing idtracker . ai ’s approach to fail . Comparing results from both idtracker . ai and TRex to 924 manually reviewed data ( see Table 3 ) shows that both solutions consistently provide high accuracy 925 results of above 99 . 5 % for most videos , but that TRex is slightly improved in all cases while also 926 having a better overall frame coverage per individual ( 99 . 65 % versus idtracker . ai ’s 97 . 93 % , where 927 100 % would mean that all individuals are tracked in every frame ; not shown ) . This suggests that the 928 splitting algorithm ( see appendix , Algorithm for splitting touching individuals ) is working to TRex ’ 929 advantage here . 930 Additionally , while TRex could successfully track individuals in all videos without tags , we were 931 interested to see the eﬀect of tags ( in this case QR tags attached to locusts , see Figure 5a ) on 932 network training . In Figure 5 we visualise diﬀerences in network activation , depending on the 933 visual features available for the network to learn from , which are diﬀerent between species ( or 934 due to physically added tags , as mentioned above ) . The " hot " regions indicate larger between - 935 class diﬀerences for that speciﬁc pixel ( values are the result of activation in the last convolutional 936 layer of the trained network , see ﬁgure legend ) . Diﬀerences are computed separately within each 937 group and are not directly comparable between trials / species in value . However , the distribution 938 of values – reﬂecting the network’s reactivity to speciﬁc parts of the image – is . Results show that 939 the most apparent diﬀerences are found for the stationary parts of the body ( not in absolute terms , 940 23 of 69 Manuscript submitted to eLife Figure 5 . Activation diﬀerences for images of randomly selected individuals from four videos , next to a median image of the respective individual – which hides thin extremities , such as legs in ( a ) and ( c ) . The captions in ( a - d ) detail the species per group and number of samples per individual . Colors represent the relative activation diﬀerences , with hotter colors suggesting bigger magnitudes , which are computed by performing a forward - pass through the network up to the last convolutional layer ( using keract ) . The outputs for each identity are averaged and stretched back to the original image size by cropping and scaling according to the network architecture . Diﬀerences shown here are calculated per cluster of pixels corresponding to each ﬁlter , comparing average activations for images from the individual’s class to activations for images from other classes . ( a ) Locusts from Video 9 with 15 tagged individuals ( N : 5101 , 7942 , 9974 ) – the only video with physical tags . The network activates more strongly in regions close to the tag , as well as the bottom right corner . ( b ) Guppies from Video 15 ( N : 46378 , 34733 , 34745 ) . Activations are less focussed and less consistent across individuals . ( c ) Flies from Video 8 ( N : 993 , 1986 , 993 ) . Activations are not similar between individuals and show various " hotspots " across the entire body . ( d ) Termites from Video 10 ( N : 27097 , 31135 , 22746 ) . Here , the connections between body - segments show strong activations – in contrast to very weak ones in other parts of the body . but following normalization , as shown in Figure 4c ) , which makes sense seeing as this part ( i ) is the 941 easiest to learn due to it being in exactly the same position every time , ( ii ) larger individuals stretch 942 further into the corners of a cropped image , making the bottom right of each image a source of 943 valuable information ( especially in Figure 5a / Figure 5b ) and ( iii ) details that often occur in the head - 944 region ( like distance between the eyes ) which can also play a role here . " Hot " regions in the bottom 945 right corner of the activation images ( e . g . in Figure 5d ) suggest that also pixels are reacted to which 946 are explicitly not part of the individual itself but of other individuals – likely this corresponds to the 947 network making use of size / shape diﬀerences between them . 948 As would be expected , distinct patterns can be recognized in the resulting activations after 949 training as soon as physical tags are attached to individuals ( as in Figure 5a ) . While other parts 950 of the image are still heavily activated ( probably to beneﬁt from size / shape diﬀerences between 951 individuals ) , tags are always at least a large part of where activations concentrate . The network 952 seemingly makes use of the additional information provided by the experimenter , where that has 953 occurred . This suggests that , while deﬁnitely not being necessary , adding tags probably does not 954 24 of 69 Manuscript submitted to eLife Table 4 . Both TRex and idtracker . ai analysed the same set of videos , while continuously logging their memory consumption using an external tool . Rows have been sorted by video _ length ∗ # individuals , which seems to be a good predictor for the memory consumption of both solutions . idtracker . ai has mixed mean values , which , at low individual densities are similar to TRex ’ results . Mean values can be misleading here , since more time spent in low - memory states skews results . The maximum , however , is more reliable since it marks the memory that is necessary to run the system . Here , idtracker . ai clocks in at signiﬁcantly higher values ( almost always more than double ) than TRex . video # ind . length max . consec . TRex memory ( GB ) idtracker . ai memory ( GB ) 12 10 10min 26 . 03s ⌀ 4 . 88 ± 0 . 23 , max6 . 31 ⌀ 8 . 23 ± 0 . 99 , max28 . 85 13 10 10min 36 . 94s ⌀ 4 . 27 ± 0 . 12 , max4 . 79 ⌀ 7 . 83 ± 1 . 05 , max29 . 43 11 10 10min 28 . 75s ⌀ 4 . 37 ± 0 . 32 , max5 . 49 ⌀ 6 . 53 ± 4 . 29 , max29 . 32 7 100 1min 5 . 97s ⌀ 9 . 4 ± 0 . 47 , max13 . 45 ⌀ 15 . 27 ± 1 . 05 , max24 . 39 15 8 72min 79 . 4s ⌀ 5 . 6 ± 0 . 22 , max8 . 41 ⌀ 35 . 2 ± 4 . 51 , max91 . 26 10 10 10min 1 . 91s ⌀ 6 . 94 ± 0 . 27 , max10 . 71 N / A 9 15 60min 7 . 64s ⌀ 13 . 81 ± 0 . 53 , max16 . 99 N / A 8 59 10min 102 . 35s ⌀ 12 . 4 ± 0 . 56 , max17 . 41 ⌀ 35 . 3 ± 0 . 92 , max50 . 26 14 8 195min 145 . 77s ⌀ 12 . 44 ± 0 . 8 , max21 . 99 ⌀ 35 . 08 ± 4 . 08 , max98 . 04 16 8 198min 322 . 57s ⌀ 16 . 15 ± 1 . 6 , max28 . 62 ⌀ 49 . 24 ± 8 . 21 , max115 . 37 worsen , and likely may even improve , training accuracy , for diﬃcult cases allowing networks to 955 exploit any source of inter - individual variation . 956 Visual Identiﬁcation : Memory Consumption 957 In order to generate comparable results between both tested software solutions , the same exter - 958 nal script has been used to measure shared , private and swap memory of idtracker . ai and TRex , 959 respectively . There are a number of ways with which to determine the memory usage of a process . 960 For automation purposes we decided to use a tool called syrupy , which can start and save informa - 961 tion about a speciﬁed command automatically . We modiﬁed it slightly , so we could obtain more 962 accurate measurements for Swap , Shared and Private separately , using ps _ mem . 963 As expected , diﬀerences in memory consumption are especially prominent for long videos ( 4 - 7x 964 lower maximum memory ) , and for videos with many individuals ( 2 - 3x lower ) . Since we already ex - 965 perienced signiﬁcant problems tracking a long video ( > 3h ) of only 8 individuals with idtracker . ai , 966 we did not attempt to further study its behavior in long videos with many individuals . However , we 967 would expect idtracker . ai ’s memory usage to increase even more rapidly than is visible in Figure 6 968 since it retains a lot of image data ( segmentation / pixels ) in memory and we already had to " allow " 969 it to relay to hard - disk in our eﬀorts to make it work for Videos 8 , 14 and 16 ( which slows down 970 analysis ) . The maximum memory consumption across all trials was on average 5 . 01 ± 2 . 54 times 971 higher in idtracker . ai , ranging from 1 . 81 to 10 . 85 times the maximum memory consumption of 972 TRex for the same video . 973 Overall memory consumption for TRex also contains posture data , which contributes a lot to 974 RAM usage . Especially with longer videos , disabling posture can lower the hardware needs for run - 975 ning our software . If posture is to be retained , the user can still ( more slightly ) reduce memory 976 requirements by changing the outline re - sampling scale ( 1 by default ) , which adjusts the outline 977 resolution between sub - and super - pixel accuracy . While analysis will be faster – and memory con - 978 sumption lower – when posture is disabled ( only limited by the matching algorithm , see Figure A3 ) , 979 users of the visual identiﬁcation might experience a decrease in training accuracy or speed ( see 980 Figure 7 ) . 981 25 of 69 Manuscript submitted to eLife 100 500 1000 1500 individual video minutes ( N * min ) 0 10 20 30 40 50 60 70 80 90 100 110 120 m a x i m u m m e m o r y ( G B ) TRex idtracker . ai Figure 6 . The maximum memory by TRex and idtracker . ai when tracking videos from a subset of all videos ( the same videos as in Table 2 ) . Results are plotted as a function of video length ( min ) multiplied by the number of individuals . We have to emphasize here that , for the videos in the upper length regions of multiple hours ( 16 , 14 ) , we had to set idtracker . ai to store segmentation information on disk – as opposed to in RAM . This uses less memory , but is also slower . For the video with ﬂies we tried out both and also settled for on - disk , since otherwise the system ran out of memory . Even then , the curve still accelerates much faster for idtracker . ai , ultimately leading to problems with most computer systems . To minimize the impact that hardware compatibility has on research , we implemented switches limiting memory usage while always trying to maximize performance given the available data . TRex can be used on modern laptops and normal consumer hardware at slightly lower speeds , but without any fatal issues . Visual Identiﬁcation : Processing Time 982 Automatically correcting the trajectories ( to produce consistent identity assignments ) means that 983 additional time is spent on the training and application of a network , speciﬁcally for the video 984 in question . Visual identiﬁcation builds on some of the other methods described in this paper 985 ( tracking and posture estimation ) , naturally making it by far the most complex and time - consuming 986 process in TRex – we thus evaluated how much time is spent on the entire sequence of all required 987 processes . For each run of TRex and idtracker . ai , we saved precise timing information from start 988 to ﬁnish . Since idtracker . ai reads videos directly and preprocesses them again each run , we used 989 the same starting conditions with our software for a direct comparison : 990 A trial starts by converting / preprocessing a video in TGrabs and then immediately opening it in 991 TRex , where automatic identity corrections were applied . TRex terminated automatically after sat - 992 isfying a correctness criterion ( high uniqueness value ) according to equation ( 1 ) . It then exported 993 trajectories , as well as validation data ( similar to idtracker . ai ) , concluding the trial . The sum of 994 time spent within TGrabs and TRex gives the total amount of time for that trial . For the purpose of 995 this test it would not have been fair to compare only TRex processing times to idtracker . ai , but 996 it is important to emphasize that conversion could be skipped entirely by using TGrabs to record 997 videos directly from a camera instead of opening an existing video ﬁle . 998 In Table 5 we can see that video length and processing times did not correlate directly . In - 999 deed , conversion times eventually overtook processing times with increasing video - length if the 1000 number of individuals remained the same . Furthermore , the time it took to track and correct a 1001 video was shorter when the initial segment ( column " sample " in the table ) was longer ( and as such 1002 likely of higher quality / capturing more visual intra - individual variation ) . Conversion times corre - 1003 lated strongly with the total video - length ( in frames ) and not the number of individuals , suggesting 1004 conversion was only constrained by video - decoding / reading speeds and not by ( pre - ) processing . 1005 26 of 69 Manuscript submitted to eLife 0 200 400 600 800 1000 1200 # samples per individual 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 v a li d a t i o n a cc u r a c y posture ( e . g . TRex ) moments ( e . g . idtracker . ai ) none Figure 7 . Convergence behavior of the network training for three diﬀerent normalization methods . This shows the maximum achievable validation accuracy after 100 epochs for 100 individuals ( Video 7 ) , when sub - sampling the number of examples per individual . Tests were performed using a manually corrected training dataset to generate the images in three diﬀerent ways , using the same , independent script ( see Figure 4 ) : Using no normalization ( blue ) , using normalization based on image moments ( green , similar to idtracker . ai ) , and using posture information ( red , as in TRex ) . Higher numbers of samples per individual result in higher maximum accuracy overall , but – unlike the other methods – posture - normalized runs already reach an accuracy above the 90 % mark for ≥ 75 samples . This property can help signiﬁcantly in situations with more crossings , when longer global segments are harder to ﬁnd . Compared to idtracker . ai , TRex ( conversion + visual identiﬁcation ) shows both considerably 1006 lower computation times ( 2 . 57× to 46 . 74× faster for the same video ) , as well as lower variance in 1007 the timings ( 79 % lower for the same video on average ) . 1008 Conclusions 1009 TRex provides a comprehensive , powerful and easy to use software solution for tracking animals . 1010 Its tracking accuracy is at the state - of - the - art while typically being 2 . 57× to 46 . 74× faster than com - 1011 parable software and having lower hardware requirements – speciﬁcally RAM . In addition to visual 1012 identiﬁcation and tracking , it provides a rich assortment of additional data , including body pos - 1013 ture , visual ﬁelds , and other kinematic as well as group - related information ( such as derivatives of 1014 position , border and mean neighbor distance , group compactness , etc . ) ; even in live - tracking and 1015 closed - loop situations . 1016 Raw tracking speeds ( without visual identiﬁcation ) still achieved roughly 80 % accuracy per deci - 1017 sion ( as compared to > 99 % with visual identiﬁcation ) . We have found that real - time performance 1018 can be achieved , even on relatively modest hardware , for all numbers of individuals ≤ 256 with - 1019 out posture estimation ( ≤ 128 with posture estimation ) . More than 256 individuals can be tracked 1020 as well , remarkably still delivering frame - rates at about 10 - 25 frames per second using the same 1021 settings . 1022 TRex is a versatile and fast program , which we have designed to enable researches to study an - 1023 imals ( and other mobile objects ) in a wide range of situations . It maintains identities of up to 100 1024 un - tagged individuals and produces corrected tracks , along with posture estimation and other fea - 1025 tures . Even videos that can not be tracked by other solutions , such as videos with over 500 animals , 1026 can now be tracked within the same day of recording . Not only does the increased processing - 1027 speeds beneﬁt researchers , but the contributions we provide to data exploration should not be 1028 27 of 69 Manuscript submitted to eLife Table 5 . Evaluating time - cost for automatic identity correction – comparing to results from idtracker . ai . Timings consist of preprocessing time in TGrabs plus network training in TRex , which are shown separately as well as combined ( ours ( min ) , 𝑁 = 5 ) . The time it takes to analyse videos strongly depends on the number of individuals and how many usable samples per individual the initial segment provides . The length of the video factors in as well , as does the stochasticity of the gradient descent ( training ) . idtracker . ai timings ( 𝑁 = 3 ) contain the whole tracking and training process from start to ﬁnish , using its terminal _ mode ( v3 ) . Parameters have been manually adjusted per video and setting , to the best of our abilities , spending at most one hour per conﬁguration . For videos 16 and 14 we had to set idtracker . ai to storing segmentation information on disk ( as compared to in RAM ) to prevent the program from being terminated for running out of memory . video # ind . length sample TGrabs ( min ) TRex ( min ) ours ( min ) idtracker . ai ( min ) 7 100 1min 1 . 61s 2 . 03 ± 0 . 02 74 . 62 ± 6 . 75 76 . 65 392 . 22 ± 119 . 43 8 59 10min 19 . 46s 9 . 28 ± 0 . 08 96 . 7 ± 4 . 45 105 . 98 4953 . 82 ± 115 . 92 9 15 60min 33 . 81s 13 . 17 ± 0 . 12 101 . 5 ± 1 . 85 114 . 67 N / A 11 10 10min 12 . 31s 8 . 8 ± 0 . 12 21 . 42 ± 2 . 45 30 . 22 127 . 43 ± 57 . 02 12 10 10min 10 . 0s 8 . 65 ± 0 . 07 23 . 37 ± 3 . 83 32 . 02 82 . 28 ± 3 . 83 13 10 10min 36 . 91s 8 . 65 ± 0 . 07 12 . 47 ± 1 . 27 21 . 12 79 . 42 ± 4 . 52 10 10 10min 16 . 22s 4 . 43 ± 0 . 05 35 . 05 ± 1 . 45 39 . 48 N / A 14 8 195min 67 . 97s 109 . 97 ± 2 . 05 70 . 48 ± 3 . 67 180 . 45 707 . 0 ± 27 . 55 15 8 72min 79 . 36s 32 . 1 ± 0 . 42 30 . 77 ± 6 . 28 62 . 87 291 . 42 ± 16 . 83 16 8 198min 134 . 07s 133 . 1 ± 2 . 28 68 . 85 ± 13 . 12 201 . 95 1493 . 83 ± 27 . 75 underestimated as well – merely making data more easily accessible right out - of - the - box , such as 1029 visual ﬁelds and live - heatmaps , has the potential to reveal features of group - and individual be - 1030 haviour which have not been visible before . TRex makes information on multiple timescales of 1031 events available simultaneously , and sometimes this is the only way to detect interesting proper - 1032 ties ( e . g . trail formation in termites ) . 1033 Future extensions 1034 Since the software is already actively used within the Max Planck Institute of Animal Behavior , re - 1035 ported issues have been taken into consideration during development . However , certain theoreti - 1036 cal , as well as practically observed , limitations remain : 1037 • Posture : While almost all shapes can be detected correctly ( by adjusting parameters ) , some 1038 shapes – especially round shapes – are hard to interpret in terms of " tail " or " head " . This 1039 means that only the other image alignment method ( moments ) can be used . However , it 1040 does introduce some limitations e . g . calculating visual ﬁelds is impossible . 1041 • Tracking : Predictions , if the wrong direction is assumed , might go really far away from where 1042 the object is . Objects are then " lost " for a ﬁxed amount of time ( parameter ) . This can be 1043 " ﬁxed " by shortening this time - period , though this leads to diﬀerent problems when the soft - 1044 ware does not wait long enough for individuals to reappear . 1045 • General : Barely visible individuals have to be tracked with the help of deep learning ( e . g . 1046 using Caelles et al . ( 2017 ) ) and a custom - made mask per video frame , prepared in an external 1047 program of the users choosing 1048 • Visual identiﬁcation : All individuals have to be visible and separate at the same time , at least 1049 once , for identiﬁcation to work at all . Visual identiﬁcation , e . g . with very high densities of 1050 individuals , can thus be very diﬃcult . This is a hard restriction to any software since ﬁnd - 1051 ing consecutive global segments is the underlying principle for the successful recognition of 1052 individuals . 1053 We will continue updating the software , increasingly addressing the above issues ( and likely 1054 others ) , as well as potentially adding new features . During development we noticed a couple of 1055 28 of 69 Manuscript submitted to eLife areas where improvements could be made , both theoretical and practical in nature . Speciﬁcally , 1056 incremental improvements in analysis speed could be made regarding visual identiﬁcation by us - 1057 ing the trained network more sporadically – e . g . it is not necessary to predict every image of very 1058 long consecutive segments , since , even with fewer samples , prediction values are likely to converge 1059 to a certain value early on . A likely more potent change would be an improved " uniqueness " algo - 1060 rithm , which , during the accumulation phase , is better at predicting which consecutive segment 1061 will improve training results the most . This could be done , for example , by taking into account the 1062 variation between images of the same individual . Other planned extensions include : 1063 • ( Feature ) : We want to have a more general interface available to users , so they can create 1064 their own plugins . Working with the data in live - mode , while applying their own ﬁlters . As 1065 well as speciﬁcally being able to write a plugin that can detect diﬀerent species / annotate 1066 them in the video . 1067 • ( Crossing solver ) : Additional method optimized for splitting overlapping , solid - color objects . 1068 The current method , simply using a threshold , is eﬀective for many species but often pro - 1069 duces large holes when splitting objects consisting of largely the same color . 1070 To obtain the most up - to - date version of TRex , please download it at trex . run or update your 1071 existing installation according to our instructions listed on trex . run / docs / install . html . 1072 Software and Licenses 1073 TRex is published under the GNU GPLv3 license ( see here for permissions granted by GPLv3 ) . All of 1074 the code has been written by the ﬁrst author of this paper ( a few individual lines of code from other 1075 sources have been marked inside the code ) . While none of these libraries are distributed alongside 1076 TRex ( they have to be provided separately ) , the following libraries are used : OpenCV ( opencv . org ) 1077 is a core library , used for all kinds of image manipulation . GLFW ( glfw . org ) helps with opening appli - 1078 cation windows and maintaining graphics contexts , while DearImGui ( github . com / ocornut / imgui ) 1079 helps with some more abstractions regarding graphics . pybind11 ( Jakob et al . ( 2017 ) ) for Python 1080 integration within a C + + environment . miniLZO ( oberhumer . com / opensource / lzo ) is used for com - 1081 pression of PV frames . Optional bindings are available to FFMPEG ( ﬀmpeg . org ) and libpng libraries , 1082 if available . ( optional ) GNU Libmicrohttpd ( gnu . org / software / libmicrohttpd ) , if available , can be 1083 used for an HTTP interface of the software , but is non - essential . 1084 Acknowledgments 1085 We thank A . Albi , F . Nowak , H . Hugo , D . E . Bath , F . Oberhauser , H . Naik , J . Graving , I . Etheredge , for 1086 helping with their insights , by providing videos , for comments on the manuscript , testing the soft - 1087 ware and for frequent coﬀee breaks during development . The development of this software would 1088 not have been possible without them . IDC acknowledges support from the NSF ( IOS - 1355061 ) , the 1089 Oﬃce of Naval Research grant ( ONR , N00014 - 19 - 1 - 2556 ) , the Struktur - und Innovationsfunds für 1090 die Forschung of the State of Baden - Württemberg , the Deutsche Forschungsgemeinschaft ( DFG , 1091 German Research Foundation ) under Germany’s Excellence Strategy – EXC 2117 - 422037984 , and 1092 the Max Planck Society . 1093 References 1094 AbuBaker A , Qahwaji R , Ipson S , Saleh M . One Scan Connected Component Labeling Technique . In : 1095 2007 IEEE International Conference on Signal Processing and Communications ; 2007 . p . 1283 – 1286 . doi : 1096 https : / / doi . org / 10 . 1109 / ICSPC . 2007 . 4728561 . 1097 Alarcón - Nieto G , Graving JM , Klarevas - Irby JA , Maldonado - Chaparro AA , Mueller I , Farine DR . An automated 1098 barcode tracking system for behavioural studies in birds . Methods in Ecology and Evolution . 2018 ; 9 ( 6 ) : 1536 – 1099 1547 . doi : https : / / doi . org / 10 . 1111 / 2041 - 210X . 13005 . 1100 29 of 69 Manuscript submitted to eLife Bath DE , Stowers JR , Hörmann D , Poehlmann A , Dickson BJ , Straw AD . FlyMAD : rapid thermogenetic 1101 control of neuronal activity in freely walking Drosophila . Nature Methods . 2014 ; 11 ( 7 ) : 756 – 762 . doi : 1102 https : / / doi . org / 10 . 1038 / nmeth . 2973 . 1103 Bertsekas DP . A new algorithm for the assignment problem . Mathematical Programming . 1981 ; 21 ( 1 ) : 152 – 171 . 1104 doi : https : / / doi . org / 10 . 1007 / BF01584237 . 1105 Bianco IH , Engert F . Visuomotor transformations underlying hunting behavior in zebraﬁsh . Current Biology . 1106 2015 ; 25 ( 7 ) : 831 – 846 . doi : https : / / doi . org / 10 . 1016 / j . cub . 2015 . 01 . 042 . 1107 Bilotta J , Saszik S . The zebraﬁsh as a model visual system . International Journal of Developmental Neuro - 1108 science . 2001 ; 19 ( 7 ) : 621 – 629 . doi : https : / / doi . org / 10 . 1016 / S0736 - 5748 ( 01 ) 00050 - 8 . 1109 Bonter DN , Bridge ES . Applications of radio frequency identiﬁcation ( RFID ) in ornithological research : a review . 1110 Journal of Field Ornithology . 2011 ; 82 ( 1 ) : 1 – 10 . doi : https : / / doi . org / 10 . 1111 / j . 1557 - 9263 . 2010 . 00302 . x . 1111 Branson K , Robie AA , Bender J , Perona P , Dickinson MH . High - throughput ethomics in large groups of 1112 Drosophila . Nature Methods . 2009 ; 6 ( 6 ) : 451 – 457 . doi : https : / / doi . org / 10 . 1038 / nmeth . 1328 . 1113 Brembs B , Heisenberg M . The operant and the classical in conditioned orientation of Drosophila melanogaster 1114 at the ﬂight simulator . Learning & Memory . 2000 ; 7 ( 2 ) : 104 – 115 . doi : 10 . 1101 / lm . 7 . 2 . 104 . 1115 Burgos - Artizzu XP , Dollár P , Lin D , Anderson DJ , Perona P . Social behavior recognition in continuous 1116 video . In : 2012 IEEE Conference on Computer Vision and Pattern Recognition IEEE ; 2012 . p . 1322 – 1329 . doi : 1117 https : / / doi . org / 10 . 1109 / CVPR . 2012 . 6247817 . 1118 Caelles S , Maninis K , Pont - Tuset J , Leal - Taixé L , Cremers D , Van Gool L . One - Shot Video Object Segmenta - 1119 tion . In : 2017 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) ; 2017 . p . 5320 – 5329 . doi : 1120 https : / / doi . org / 10 . 1109 / CVPR . 2017 . 565 . 1121 Cavagna A , Cimarelli A , Giardina I , Parisi G , Santagati R , Stefanini F , Tavarone R . From empirical data to inter - 1122 individual interactions : unveiling the rules of collective animal behavior . Mathematical Models and Methods 1123 in Applied Sciences . 2010 ; 20 ( supp01 ) : 1491 – 1510 . doi : https : / / doi . org / 10 . 1142 / S0218202510004660 . 1124 Chang F , Chen C . A Component - Labeling Algorithm Using Contour Tracing Technique . In : 2013 12th In - 1125 ternational Conference on Document Analysis and Recognition , vol . 3 Los Alamitos , CA , USA : IEEE Computer 1126 Society ; 2003 . p . 741 . https : / / doi . ieeecomputersociety . org / 10 . 1109 / ICDAR . 2003 . 1227760 , doi : 10 . 1109 / IC - 1127 DAR . 2003 . 1227760 . 1128 Clausen J , Branch and bound algorithms - principles and examples . University of Copenhagen ; 1999 . [ Online ; 1129 accessed 22 - Oct - 2020 ] . http : / / www2 . imm . dtu . dk / courses / 04232 / TSPtext . pdf . 1130 Colavita FB . Human sensory dominance . Perception & Psychophysics . 1974 ; 16 ( 2 ) : 409 – 412 . doi : 1131 https : / / doi . org / 10 . 3758 / BF03203962 . 1132 Crall JD , Gravish N , Mountcastle AM , Combes SA . BEEtag : a low - cost , image - based track - 1133 ing system for the study of animal behavior and locomotion . PloS One . 2015 ; 10 ( 9 ) . doi : 1134 https : / / doi . org / 10 . 1371 / journal . pone . 0136487 . 1135 Dell AI , Bender JA , Branson K , Couzin ID , de Polavieja GG , Noldus LP , Pérez - Escudero A , Perona P , Straw AD , 1136 Wikelski M , et al . Automated image - based tracking and its application in ecology . Trends in Ecology & Evolu - 1137 tion . 2014 ; 29 ( 7 ) : 417 – 428 . doi : https : / / doi . org / 10 . 1016 / j . tree . 2014 . 05 . 004 . 1138 Francisco FA , Nührenberg P , Jordan AL . A low - cost , open - source framework for tracking and behavioural 1139 analysis of animals in aquatic ecosystems . bioRxiv . 2019 ; p . 571232 . doi : https : / / doi . org / 10 . 1101 / 571232 . 1140 Fredman ML , Tarjan RE . Fibonacci heaps and their uses in improved network optimization algorithms . Journal 1141 of the ACM ( JACM ) . 1987 ; 34 ( 3 ) : 596 – 615 . doi : https : / / doi . org / 10 . 1145 / 28869 . 28874 . 1142 Fukunaga T , Kubota S , Oda S , Iwasaki W . GroupTracker : video tracking system for multiple an - 1143 imals under severe occlusion . Computational Biology and Chemistry . 2015 ; 57 : 39 – 45 . doi : 1144 https : / / doi . org / 10 . 1016 / j . compbiolchem . 2015 . 02 . 006 . 1145 Fukushima K . Neocognitron : A hierarchical neural network capable of visual pattern recognition . Neural 1146 Networks . 1988 ; 1 ( 2 ) : 119 – 130 . doi : https : / / doi . org / 10 . 1016 / 0893 - 6080 ( 88 ) 90014 - 7 . 1147 30 of 69 Manuscript submitted to eLife Garrido - Jurado S , Muñoz - Salinas R , Madrid - Cuevas FJ , Medina - Carnicer R . Generation of ﬁducial marker 1148 dictionaries using mixed integer linear programming . Pattern Recognition . 2016 ; 51 : 481 – 491 . doi : 1149 https : / / doi . org / 10 . 1016 / j . patcog . 2015 . 09 . 023 . 1150 Glorot X , Bengio Y . Understanding the diﬃculty of training deep feedforward neural networks . In : Teh YW , 1151 Titterington M , editors . Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statis - 1152 tics , vol . 9 of Proceedings of Machine Learning Research Chia Laguna Resort , Sardinia , Italy : PMLR ; 2010 . p . 1153 249 – 256 . http : / / proceedings . mlr . press / v9 / glorot10a . html . 1154 Graving JM , Chae D , Naik H , Li L , Koger B , Costelloe BR , Couzin ID . DeepPoseKit , a software toolkit 1155 for fast and robust animal pose estimation using deep learning . eLife . 2019 ; 8 : e47994 . doi : 1156 https : / / doi . org / 10 . 7554 / eLife . 47994 . 1157 He L , Chao Y , Suzuki K , Wu K . Fast connected - component labeling . Pattern recognition . 2009 ; 42 ( 9 ) : 1977 – 1987 . 1158 doi : https : / / doi . org / 10 . 1016 / j . patcog . 2008 . 10 . 013 . 1159 Hewitt BM , Yap MH , Hodson - Tole EF , Kennerley AJ , Sharp PS , Grant RA . A novel automated rodent tracker 1160 ( ART ) , demonstrated in a mouse model of amyotrophic lateral sclerosis . Journal of neuroscience methods . 1161 2018 ; 300 : 147 – 156 . doi : https : / / doi . org / 10 . 1016 / j . jneumeth . 2017 . 04 . 006 . 1162 Hu MK . Visual pattern recognition by moment invariants . IRE Transactions on Information Theory . 1962 ; 1163 8 ( 2 ) : 179 – 187 . doi : https : / / doi . org / 10 . 1109 / TIT . 1962 . 1057692 . 1164 Hubel DH , Wiesel TN . Receptive ﬁelds of single neurones in the cat’s striate cortex . The Journal of Physiology . 1165 1959 ; 148 ( 3 ) : 574 . doi : 10 . 1113 / jphysiol . 1959 . sp006308 . 1166 Hubel DH , Wiesel TN . Receptive ﬁelds of cells in striate cortex of very young , visually inexperienced kittens . 1167 Journal of Neurophysiology . 1963 ; 26 ( 6 ) : 994 – 1002 . doi : https : / / doi . org / 10 . 1152 / jn . 1963 . 26 . 6 . 994 . 1168 Hughey LF , Hein AM , Strandburg - Peshkin A , Jensen FH . Challenges and solutions for studying collec - 1169 tive animal behaviour in the wild . Philosophical Transactions of the Royal Society B : Biological Sci - 1170 ences . 2018 ; 373 ( 1746 ) : 20170005 . https : / / royalsocietypublishing . org / doi / abs / 10 . 1098 / rstb . 2017 . 0005 , doi : 1171 10 . 1098 / rstb . 2017 . 0005 . 1172 Humphrey GK , Khan SC . Recognizing novel views of three - dimensional objects . Canadian Journal of Psychol - 1173 ogy / Revue canadienne de psychologie . 1992 ; 46 ( 2 ) : 170 . doi : https : / / doi . org / 10 . 1037 / h0084320 . 1174 Inada Y , Kawachi K . Order and Flexibility in the Motion of Fish Schools . Journal of Theoretical Biol - 1175 ogy . 2002 ; 214 ( 3 ) : 371 – 387 . http : / / www . sciencedirect . com / science / article / pii / S002251930192449X , doi : 1176 https : / / doi . org / 10 . 1006 / jtbi . 2001 . 2449 . 1177 Iwata H , Ebana K , Uga Y , Hayashi T . Genomic prediction of biological shape : elliptic fourier analysis and kernel 1178 partial least squares ( PLS ) regression applied to grain shape prediction in rice ( Oryza sativa L . ) . PloS One . 1179 2015 ; 10 ( 3 ) . doi : https : / / doi . org / 10 . 1371 / journal . pone . 0120610 . 1180 Jakob W , Rhinelander J , Moldovan D , pybind11 – Seamless operability between C + + 11 and Python . Wenzel 1181 Jakob ; 2017 . [ Online ; accessed 22 - Oct - 2020 ] . https : / / github . com / pybind / pybind11 . 1182 Kalman RE . A New Approach to Linear Filtering and Prediction Problems . Journal of Basic Engineering . 1960 1183 03 ; 82 ( 1 ) : 35 – 45 . https : / / doi . org / 10 . 1115 / 1 . 3662552 , doi : 10 . 1115 / 1 . 3662552 . 1184 Kingma DP , Ba J . Adam : A Method for Stochastic Optimization . In : Bengio Y , LeCun Y , editors . 3rd Interna - 1185 tional Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track 1186 Proceedings ; 2015 . http : / / arxiv . org / abs / 1412 . 6980 , arXiv : 1412 . 6980 . 1187 Kuhl FP , Giardina CR . Elliptic Fourier features of a closed contour . Computer Graphics and Image Processing . 1188 1982 ; 18 ( 3 ) : 236 – 258 . doi : https : / / doi . org / 10 . 1016 / 0146 - 664X ( 82 ) 90034 - X . 1189 Kuhn HW . The Hungarian method for the assignment problem . Naval Research Logistics Quarterly . 1955 ; 1190 2 ( 1 - 2 ) : 83 – 97 . doi : https : / / doi . org / 10 . 1002 / nav . 3800020109 . 1191 Land AH , Doig AG . In : Jünger M , Liebling TM , Naddef D , Nemhauser GL , Pulleyblank WR , Reinelt G , Rinaldi 1192 G , Wolsey LA , editors . An Automatic Method for Solving Discrete Programming Problems Berlin , Heidel - 1193 berg : Springer Berlin Heidelberg ; 2010 . p . 105 – 132 . https : / / doi . org / 10 . 1007 / 978 - 3 - 540 - 68279 - 0 _ 5 , doi : 1194 10 . 1007 / 978 - 3 - 540 - 68279 - 0 _ 5 . 1195 31 of 69 Manuscript submitted to eLife LeCun Y , Boser B , Denker JS , Henderson D , Howard RE , Hubbard W , Jackel LD . Backpropaga - 1196 tion applied to handwritten zip code recognition . Neural Computation . 1989 ; 1 ( 4 ) : 541 – 551 . doi : 1197 https : / / doi . org / 10 . 1162 / neco . 1989 . 1 . 4 . 541 . 1198 Lin T , Goyal P , Girshick R , He K , Dollár P . Focal Loss for Dense Object Detection . IEEE Transactions on Pattern 1199 Analysis and Machine Intelligence . 2020 ; 42 ( 2 ) : 318 – 327 . doi : https : / / doi . org / 10 . 1109 / TPAMI . 2018 . 2858826 . 1200 Little JD , Murty KG , Sweeney DW , Karel C . An algorithm for the traveling salesman problem . Operations 1201 Research . 1963 ; 11 ( 6 ) : 972 – 989 . doi : https : / / doi . org / 10 . 1287 / opre . 11 . 6 . 972 . 1202 Liu T , Chen W , Xuan Y , Fu X . The eﬀect of object features on multiple object tracking and identiﬁcation . In : 1203 International Conference on Engineering Psychology and Cognitive Ergonomics Springer ; 2009 . p . 206 – 212 . doi : 1204 https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 02728 - 4 _ 22 . 1205 Maninis KK , Caelles S , Chen Y , Pont - Tuset J , Leal - Taixé L , Cremers D , Van Gool L . Video Object Segmentation 1206 Without Temporal Information . IEEE Transactions on Pattern Analysis and Machine Intelligence ( TPAMI ) . 1207 2018 ; doi : https : / / doi . org / 10 . 1109 / TPAMI . 2018 . 2838670 . 1208 Mathis A , Mamidanna P , Cury KM , Abe T , Murthy VN , Mathis MW , Bethge M . DeepLabCut : markerless pose 1209 estimation of user - deﬁned body parts with deep learning . Nature Neuroscience . 2018 ; 21 ( 9 ) : 1281 – 1289 . doi : 1210 https : / / doi . org / 10 . 1038 / s41593 - 018 - 0209 - y . 1211 Mersch DP , Crespi A , Keller L . Tracking individuals shows spatial ﬁdelity is a key regulator of ant social organi - 1212 zation . Science . 2013 ; 340 ( 6136 ) : 1090 – 1093 . doi : 10 . 1126 / science . 1234316 . 1213 Munkres J . Algorithms for the assignment and transportation problems . Journal of the Society for Industrial 1214 and Applied Mathematics . 1957 ; 5 ( 1 ) : 32 – 38 . doi : https : / / doi . org / 10 . 1137 / 0105003 . 1215 Nagy M , Vásárhelyi G , Pettit B , Roberts - Mariani I , Vicsek T , Biro D . Context - dependent hierarchies 1216 in pigeons . Proceedings of the National Academy of Sciences . 2013 ; 110 ( 32 ) : 13049 – 13054 . doi : 1217 https : / / doi . org / 10 . 1073 / pnas . 1305552110 . 1218 Noldus LP , Spink AJ , Tegelenbosch RA . EthoVision : a versatile video tracking system for automation of be - 1219 havioral experiments . Behavior Research Methods , Instruments , & Computers . 2001 ; 33 ( 3 ) : 398 – 414 . doi : 1220 https : / / doi . org / 10 . 3758 / BF03195394 . 1221 Ohayon S , Avni O , Taylor AL , Perona P , Egnor SR . Automated multi - day tracking of marked mice 1222 for the analysis of social behaviour . Journal of Neuroscience Methods . 2013 ; 219 ( 1 ) : 10 – 19 . doi : 1223 https : / / doi . org / 10 . 1016 / j . jneumeth . 2013 . 05 . 013 . 1224 Pennekamp F , Schtickzelle N , Petchey OL . BEMOVI , software for extracting behavior and morphology 1225 from videos , illustrated with analyses of microbes . Ecology and Evolution . 2015 ; 5 ( 13 ) : 2584 – 2595 . doi : 1226 https : / / doi . org / 10 . 1002 / ece3 . 1529 . 1227 Pereira TD , Aldarondo DE , Willmore L , Kislin M , Wang SSH , Murthy M , Shaevitz JW . Fast animal pose estimation 1228 using deep neural networks . Nature Methods . 2019 ; 16 ( 1 ) : 117 – 125 . doi : https : / / doi . org / 10 . 1038 / s41592 - 1229 018 - 0234 - 5 . 1230 Pereira TD , Tabris N , Li J , Ravindranath S , Papadoyannis ES , Wang ZY , Turner DM , McKenzie - Smith G , Kocher 1231 SD , Falkner AL , Shaevitz JW , Murthy M . SLEAP : Multi - animal pose tracking . bioRxiv . 2020 ; https : / / www . biorxiv . 1232 org / content / early / 2020 / 09 / 02 / 2020 . 08 . 31 . 276246 , doi : 10 . 1101 / 2020 . 08 . 31 . 276246 . 1233 Perez L , Wang J . The Eﬀectiveness of Data Augmentation in Image Classiﬁcation using Deep Learning . CoRR . 1234 2017 ; abs / 1712 . 04621 . http : / / arxiv . org / abs / 1712 . 04621 . 1235 Pérez - Escudero A , de Polavieja G . Collective animal behavior from Bayesian estimation and probability match - 1236 ing . Nature Precedings . 2011 ; p . 1 – 1 . doi : https : / / doi . org / 10 . 1038 / npre . 2011 . 5939 . 2 . 1237 Pesant G , Quimper CG , Zanarini A . Counting - based search : Branching heuristics for constraint 1238 satisfaction problems . Journal of Artiﬁcial Intelligence Research . 2012 ; 43 : 173 – 210 . doi : 1239 https : / / doi . org / 10 . 1613 / jair . 3463 . 1240 Pérez - Escudero A , Vicente - Page J , Hinz RC , Arganda S , de Polavieja GG . idTracker : tracking individuals 1241 in a group by automatic identiﬁcation of unmarked animals . Nature Methods . 2014 ; 11 ( 7 ) : 743 . doi : 1242 https : / / doi . org / 10 . 1038 / nmeth . 2994 . 1243 32 of 69 Manuscript submitted to eLife Ramshaw L , Tarjan RE . A Weight - Scaling Algorithm for Min - Cost Imperfect Matchings in Bipartite Graphs . 1244 In : 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science ; 2012 . p . 581 – 590 . doi : 1245 https : / / doi . org / 10 . 1109 / FOCS . 2012 . 9 . 1246 Ramshaw L , Tarjan RE , On Minimum - Cost Assignments in Unbalanced Bipartite Graphs . HP Labs , Palo Alto , CA , 1247 USA ; 2012 . https : / / www . hpl . hp . com / techreports / 2012 / HPL - 2012 - 40 . pdf , Technical Report , HPL - 2012 - 40R1 , 1248 [ Online ; Accessed 22 - Oct - 2020 ] . 1249 Rasch MJ , Shi A , Ji Z . Closing the loop : tracking and perturbing behaviour of individuals in a group in real - time . 1250 bioRxiv . 2016 ; p . 071308 . doi : https : / / doi . org / 10 . 1101 / 071308 . 1251 Risse B , Berh D , Otto N , Klämbt C , Jiang X . FIMTrack : An open source tracking and locomotion 1252 analysis software for small animals . PLoS Computational Biology . 2017 ; 13 ( 5 ) : e1005530 . doi : 1253 https : / / doi . org / 10 . 1371 / journal . pcbi . 1005530 . 1254 RobieAA , SeagravesKM , EgnorSR , BransonK . Machinevisionmethodsforanalyzingsocialinteractions . Journal 1255 of Experimental Biology . 2017 ; 220 ( 1 ) : 25 – 34 . doi : https : / / doi . org / 10 . 1242 / jeb . 142281 . 1256 Rodriguez A , Zhang H , Klaminder J , Brodin T , Andersson PL , Andersson M . ToxTrac : a fast and ro - 1257 bust software for tracking organisms . Methods in Ecology and Evolution . 2018 ; 9 ( 3 ) : 460 – 464 . doi : 1258 https : / / doi . org / 10 . 1111 / 2041 - 210X . 12874 . 1259 Romero - Ferrero F , Bergomi MG , Hinz RC , Heras FJ , de Polavieja GG . idtracker . ai : tracking all indi - 1260 viduals in small or large collectives of unmarked animals . Nature Methods . 2019 ; 16 ( 2 ) : 179 . doi : 1261 https : / / doi . org / 10 . 1038 / s41592 - 018 - 0295 - 5 . 1262 Rosenthal SB , Twomey CR , Hartnett AT , Wu HS , Couzin ID . Revealing the hidden networks of interaction in mo - 1263 bile animal groups allows prediction of complex behavioral contagion . Proceedings of the National Academy 1264 of Sciences . 2015 ; 112 ( 15 ) : 4690 – 4695 . doi : https : / / doi . org / 10 . 1073 / pnas . 1420068112 . 1265 Sridhar VH , Roche DG , Gingins S . Tracktor : Image - based automated tracking of animal movement and 1266 behaviour . Methods in Ecology and Evolution . 2019 ; 10 ( 6 ) : 815 – 820 . doi : https : / / doi . org / 10 . 1111 / 2041 - 1267 210X . 13166 . 1268 Strandburg - Peshkin A , Twomey CR , Bode NW , Kao AB , Katz Y , Ioannou CC , Rosenthal SB , Torney CJ , Wu HS , 1269 Levin SA , et al . Visual sensory networks and eﬀective information transfer in animal groups . Current Biology . 1270 2013 ; 23 ( 17 ) : R709 – R711 . doi : https : / / doi . org / 10 . 1016 / j . cub . 2013 . 07 . 059 . 1271 Suzuki K , Horiba I , Sugie N . Linear - time connected - component labeling based on sequential local opera - 1272 tions . Computer Vision and Image Understanding . 2003 ; 89 ( 1 ) : 1 – 23 . doi : https : / / doi . org / 10 . 1016 / S1077 - 1273 3142 ( 02 ) 00030 - 9 . 1274 Thomas DJ , Matching Problems with Additional Resource Constraints . Universität Trier ; 2016 . https : / / doi . org / 1275 10 . 25353 / ubtr - xxxx - 7644 - a670 / , doi : 10 . 25353 / ubtr - xxxx - 7644 - a670 / , Doctoral Thesis . 1276 Warren J , Weimer H . Subdivision Methods for Geometric Design : A Constructive Approach . 1st ed . San Fran - 1277 cisco , CA , USA : Morgan Kaufmann Publishers Inc . ; 2001 . ISBN : 1558604464 . 1278 Weixiong Z , Branch - and - Bound Search Algorithms and Their Computational Complexity . University of South - 1279 ern California / Marina Del Rey Information Sciences Institute ; 1996 . https : / / apps . dtic . mil / sti / citations / 1280 ADA314598 , Technical Report , ISI / RR - 96 - 443 , [ Online ; Accessed 22 - Oct - 2020 ] . 1281 Wiesel TN , Hubel DH . Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey . 1282 Journal of Neurophysiology . 1966 ; 29 ( 6 ) : 1115 – 1156 . doi : https : / / doi . org / 10 . 1152 / jn . 1966 . 29 . 6 . 1115 . 1283 Williams L . Casting curved shadows on curved surfaces . In : Proceedings of the 5th Annual Conference on Com - 1284 puter Graphics and Interactive Techniques ; 1978 . p . 270 – 274 . doi : https : / / doi . org / 10 . 1145 / 800248 . 807402 . 1285 33 of 69 Manuscript submitted to eLife Appendix 1 1286 Interface and installation requirements 1287 While all options are available from the command - line and a screen is not required , TRex oﬀers a rich , yet straight - foward to use , interface to local as well as remote users . Accom - panied by the integrated documentation for all parameters , each stating purpose , type and value ranges , as well as a comprehensive online documentation , the learning curve for new users is usually quite steep . Especially to the beneﬁt of new users , we evaluated the pa - rameter space on a wide dataset ( ﬁsh , termites , locusts ) and determined which parameters work best in most use - cases to set their default values ( evaluation not part of this paper ) . 1288 1289 1290 1291 1292 1293 1294 Compiled , read - to - use binaries are available for all major operating systems ( Windows , Linux , MacOS ) . However , it should be possible to compile the software yourself for any Unix - or Windows - based system ( ≥ 8 ) , possibly with minor adjustments . Tested setups include : 1295 1296 1297 • Windows , Linux , MacOS 1298 • A computer with ≥ 16GB RAM is recommended 1299 • OpenCV a libraries ≥ v3 . 3 1300 • Python libraries ≥ v3 . 6 , as well as additional packages such as : 1301 • Keras ≈ v2 . 2 with one of the following backends installed 1302 – Tensorﬂow < v2 b ( either CPU - based , or GPU - based ) 1303 – Theano c 1304 • GPU - based recognition requires an NVIDIA graphics - card and drivers ( see Tensorﬂow documentation ) 1305 1306 For detailed download / installation instructions and up - to - date requirements , please re - fer to the documentation at trex . run / install . 1307 1308 The interface is structured into groups ( see Figure A1 ) , categorized by the typical use - case : 1309 1310 1 . The main menu , containing options for loading / saving , options for the timeline and reanalysis of parts of the video 1311 1312 2 . Timeline and current video playback information 1313 3 . Information about the selected individual 1314 4 . Display options and an interactive " omni - box " for viewing and changing parameters 1315 5 . General status information about TRex and the Python integration 1316 Workﬂow 1317 TRex can be opened in one of two ways : ( i ) Simply starting the application ( e . g . using the operating systems’ ﬁle - browser ) , ( ii ) using the command - line . If the user simply opens the application , a ﬁle opening dialog displays a list of compatible ﬁles as well as information on a selected ﬁles content . Certain startup parameters can be adjusted from within the graph - ical user - interface , before conﬁrming and loading up the ﬁle ( see Figure A3 ) . Users with more command - line experience , or the intent of running TRex in batch - mode , can append necessary parameter values without adding them to a settings ﬁle . 1318 1319 1320 1321 1322 1323 1324 To acquire video - ﬁles that can be opened using TRex , one needs to ﬁrst run TGrabs in one way or another . It is possible to use a webcam ( generic USB camera ) for recording , but TGrabs can also be compiled with Basler Pylon5 support d . TGrabs can also convert existing videos and write to a more suitable format for TRex to interact with ( a static background with moving objects clearly separated in front of it ) . It can be started just like TRex , although 34 of 69 Manuscript submitted to eLife most options are either set via the command - line , or a web - interface . TGrabs can perform basic tracking tasks on the ﬂy , oﬀering closed - loop support as well . 1325 1326 1327 1328 1329 1330 1331 For automatic visual recognition , one might need to adjust some parameters . Mostly , these adjustments consist of changing the following parameters : 1332 1333 • blob _ size _ ranges : Setting one ( or multiple ) size thresholds for individuals , by giving lower and upper limit value pairs . 1334 1335 • track _ max _ individuals : Sets the number of individuals expected in a trial . This num - ber needs to be known for recognition tasks ( and will be guessed if not provided ) , but can be set to 0 for unknown numbers of individuals . 1336 1337 1338 • track _ max _ speed : Sets the maximum speed ( cm / s ) that individuals are expected to travel at . This is inﬂuenced by meta information provided to TGrabs by the user ( e . g . the width of the tank ) , as well as frame timings . 1339 1340 1341 • track _ threshold : Even TRex can threshold images of individuals , so it is beneﬁcial to not threshold away too many pixels during conversion / recording and do ﬁner - grade adjustments in the tracker itself . 1342 1343 1344 • outline _ resample : A factor that is > 0 , by which the number of points in the outline is essentially " divided " . Smaller resample rates lead to more points on the outline ( good for very small shapes ) . 1345 1346 1347 Training can be started once the user is satisﬁed with the basic tracking results . Consec - utive segments are highlighted in the time - line and suggest better or worse tracking , based on their quantity and length . Problematic segments of the video are highlighted using yel - low bars in that same time - line , giving another hint to the user as to the tracking quality . To start the training , the user just clicks on " train network " in the main menu – triggering the accumulation process immediately . After training , the user can click on " auto correct " in the menu and let TRex correct the tracks automatically ( this will re - track the video ) . The entire process can be automated by adding the " auto _ train " parameter to the command - line , or selecting it in the interface . 1348 1349 1350 1351 1352 1353 1354 1355 1356 Output 1357 Once ﬁnished , the user may export the data in the desired format . Which parts of the data are exported is up to the user as well . By default , almost all the data is exported and saved in NPZ ﬁles in the output folder . 1358 1359 1360 Output folders are structured in this way : 1361 • output folder : 1362 – Settings ﬁles 1363 – Training weights 1364 – Saved program states 1365 – data folder : 1366 * Statistics 1367 * All exported NPZ ﬁles ( named [ video _ name ] _ fish [ number ] . npz – the preﬁx " ﬁsh " can be changed ) . 1368 1369 * . . . 1370 – frames folder ( contains video clips recorded in the GUI , e . g . for presentations ) : 1371 * [ video name ] folder 1372 · clip [ index ] . avi 1373 · . . . 1374 * . . . 1375 35 of 69 Manuscript submitted to eLife At any point in time ( except during training ) , the user can save the current program state and return to it at a later time ( e . g . after a computer restart ) . 1376 1377 Export options 1378 After individuals have been assigned by the matching algorithm , various metrics are calcu - lated ( depending on settings ) : 1379 1380 • Angle : The angle of an individual can be calculated without any context using image moments ( Hu ( 1962 ) ) . However , this angle is only reliable within 0 to 180 degrees – not the full 360 . Within these 180 degrees it is probably more accurate than is movement direction . 1381 1382 1383 1384 • Position : Centroid information on the current , as well as the previous position of the individual are maintained . Based on previous positions , velocity as well as acceleration are calculated . This process is based on information sourced from the respective video ﬁle or camera on the time passed between frames . The centroid of an individual is calculated based on the mass center of the pixels that the object comprises . Angles calculated in the previous steps are corrected ( ﬂipped by 180 degrees ) if the angle diﬀerence between movement direction and angle + 180 degrees is smaller than with the raw angle . 1385 1386 1387 1388 1389 1390 1391 1392 • Posture : A large part of the computational complexity comes from calculating the posture of individuals . While this process is relatively fast in TRex , it is still the main factor ( except with many individuals , where the matching process takes longest ) . We dedicated a subsection to it below . 1393 1394 1395 1396 • Visual Field : Based on posture , rays can be cast to detect which animal is visible from the position of another individual . We also dedicated a subsection to visual ﬁeld fur - ther down . 1397 1398 1399 • Other features can be computed , such as inter - individual distances or distance to the tank border . These are optional and will only be computed if necessary when exporting the data . A ( non - comprehensive ) list of metrics that can be exported follows : 1400 1401 1402 – Time : The time of the current frame ( relative to the start of the video ) in seconds . 1403 – Frame : Index of the frame in the PV video ﬁle . 1404 – Individual components of position its derivatives ( as well as their magnitudes , e . g . speed ) 1405 1406 – Midline oﬀset : The center - line , e . g . of a beating ﬁsh - tail , is normalized to be roughly parallel to the x - axis ( from its head to a user - deﬁned percentage of a body ) . The y - oﬀset of its last point is exported as a " midline oﬀset " . This is useful , e . g . to detect burst - and - glide events . 1407 1408 1409 1410 – Midline variance : Variance in midline oﬀset , e . g . for detection of irregular pos - tures or increased activity . 1411 1412 – Border distance 1413 – Average neighbour distance : Could be used to detect individuals who prefer to be located far away from the others or are avoided by them . 1414 1415 Additionally , tracks of individuals can be exported as a series of cropped - out images – a very useful tool if they are to be used with an external posture estimator or tag - recognition . This series of images can be either every single image , or the median of multiple images ( the time - series is down - sampled ) . 1416 1417 1418 1419 a opencv . org b tensorﬂow . org c deeplearning . net d The baslerweb . com Pylon SDK is required to be installed to support Basler USB cameras . 36 of 69 Manuscript submitted to eLife posture preview consecutive segments for the selected individual average recognition for current segment interactive settings box the best global segments are marked with colors selected in d ividual ( 3 ) timeline ( 2 ) display options ( 4 ) visual ﬁeld rays ( 1 ) main menu ( 5 ) status and progress info recognition info for global segments probability in this frame for ID 1 to be ﬁsh1 documentation for hovered parameter Appendix 1 Figure A1 . An overview of TRex ’ the main interface , which is part of the documentation at trex . run / docs . Interface elements are sorted into categories in the four corners of the screen ( labelled here in black ) . The omni - box on the bottom left corner allows users to change parameters on - the - ﬂy , helped by a live auto - completion and documentation for all settings . Only some of the many available features are displayed here . Generally , interface elements can be toggled on or oﬀ using the bottom - left display options or moved out of the way with the cursor . Users can customize the tinting of objects ( e . g . sourcing it from their speed ) to generate interesting eﬀect and can be recorded for use in presentations . Additionally , all exportable metrics ( such as border - distance , size , x / y , etc . ) can also be shown as an animated graph for a number of selected objects . Keyboard shortcuts are available for select features such as loading , saving , and terminating the program . Remote access is supported and oﬀers the same graphical user interface , e . g . in case the software is executed without an application window ( for batch processing purposes ) . 37 of 69 Manuscript submitted to eLife Appendix 1 Figure A2 . Using the interactive heatmap generator within TRex , the foraging trail formation of Constrictotermes cyphergaster ( termites ) can be visualized during analysis , as well as other potentially interesting metrics ( based on posture - as well basic positional data ) . This is generalizable to all output data ﬁelds available in TRex , e . g . also making it possible to visualize " time " as a heatmap and showing where individuals were more likely to be located during the beginning or towards end of the video . Video : H . Hugo 38 of 69 Manuscript submitted to eLife Appendix 1 Figure A3 . The ﬁle opening dialog . On the left is a list of compatible ﬁles in the current folder . The center column shows meta - information provided by the video ﬁle , including its frame - rate and resolution – or some of the settings used during conversion and the timestamp of conversion . The column on the right provides an easy interface for adjusting the most important parameters before starting up the software . Most parameters can be changed later on from within TRex as well . 39 of 69 Manuscript submitted to eLife Appendix 2 1420 From video frame to blobs 1421 Video frames can originate either from a camera , or from a pre - recorded video ﬁle saved on disk . TGrabs treats both sources equally , the only exception being some minor details and that pre - recorded videos have a well - deﬁned end ( which only has an impact on MP4 encoding ) . Multiple formats are supported , but the full list of supported codecs depends on the speciﬁc system and OpenCV version installed . TGrabs saves images in RAW quality , but does not store complete images . Merely the objects of interest , deﬁned by common tracking parameters such as size , will actually be written to a ﬁle . Since TGrabs is mostly meant for use with stable backgrounds ( except when contrast is good or a video - mask is provided ) , the rest of the area can be approximated by a static background image generated in the beginning of the process ( or previously ) . 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 Generally , every image goes through a number of steps before it can be tracked in TRex : 1432 1 . Images are decoded by either ( i ) a camera driver , or ( ii ) OpenCV . They consist of an array of values between 0 and 255 ( grayscale ) . Color images will be converted to grayscale images ( color channel or " hue " can be chosen ) . 1433 1434 1435 2 . Timing information is saved and images are appended to a queue of images to be processed 1436 1437 3 . All operations from now on are performed on the GPU if available . Once images are in the queue , they are picked one - by - one by the processing thread , which performs operations on them based on user - deﬁned parameters : 1438 1439 1440 • Cropping 1441 • Inverting 1442 • Contrast / brightness and lighting corrections 1443 • Undistortion ( see OpenCV Tutorial ) 1444 4 . ( optional ) Background subtraction ( 𝑑 ( 𝑥 ) = 𝑏 ( 𝑥 ) − 𝑓 ( 𝑥 ) , with 𝑓 being the image and 𝑏 the background image ) , leaving a diﬀerence image containing only the objects . This can be an absolute diﬀerence | 𝑏 ( 𝑥 ) − 𝑓 ( 𝑥 ) | or a signed one , which has diﬀerent eﬀects on the following step . Otherwise 𝑑 ( 𝑥 ) = 𝑓 ( 𝑥 ) 1445 1446 1447 1448 5 . Thresholding to obtain a binary image , with all pixels either being 1 or 0 : 𝑡 ( 𝑥 ) = ⎧⎪⎨⎪ ⎩ 0 𝑑 ( 𝑥 ) < 𝑇 1 𝑑 ( 𝑥 ) ≥ 𝑇 where 0 ≤ 𝑇 ≤ 255 is the threshold constant . 1449 1450 1451 1452 1453 6 . Options are available for further adjustment of the binary image : Dilation , Erosion and Closing are used to close gaps in the shapes , which are ﬁlled up by successive dilation and erosion operations ( see Figure A1 ) . If there is an imbalance of dilation and erosion commands , noise can be removed or shapes made more inclusive . 1454 1455 1456 1457 7 . The original image is multiplied by the thresholded image , obtaining a masked grayscale image : 𝑡 ( 𝑥 ) ⋅ 𝑓 ( 𝑥 ) , where ⋅ is the element - wise multiplication operator . 1458 1459 At this point , the masked image is returned to the CPU , where connected components ( objects ) are detected . A connected component is a number of adjacent pixels with color values greater than zero . Algorithms for connected - component labeling either use a 4 - neighborhood or an 8 - neighborhood , which considers diagonal neighbors to be adjacent as well . Many such algorithms are available ( AbuBaker et al . ( 2007 ) , Chang and Chen ( 2003 ) , 40 of 69 Manuscript submitted to eLife ( a ) The structure element . ( b ) The original image . ( c ) Image modiﬁed by the structure element . Appendix 2 Figure A1 . Example of morphological operations on images : " Erosion " . Blue pixels denote on - pixels with color values greater than zero , white pixels are " oﬀ - pixels " with a value equal to zero . A mask is moved across the original image , with its center ( dot ) being the focal pixel . A focal pixel is retained if all of the on - pixels within the structure element / mask are on top of on - pixels in the original image . Otherwise the focal pixel is set to 0 . The type of operation performed is entirely determined by the structure element . and many others ) , even capable of real - time speeds ( Suzuki et al . ( 2003 ) , He et al . ( 2009 ) ) . However , since we want to use a compressed representation throughout our solution , as well as transfer over valuable information to integrate it with posture analysis , we needed to implement our own ( see Connected components algorithm ) . 1460 1461 1462 1463 1464 1465 1466 1467 1468 MP4 encoding has some special properties , since its speed is mainly determined by the external encoding software . Encoding at high - speed frame - rates can be challenging , since we are also encoding to a PV - ﬁle simultaneously . Videos are encoded in a separate thread , without muxing , and will be remuxed after the recording is stopped . For very high frame - rates or resolutions , it may be necessary to limit the duration of videos since all of the images have to be kept in RAM until they have been encoded . RAW images in RAM can take up a lot of space ( 1024 ∗ 1024 ∗ 1000 = 1 , 048 , 576 , 000 bytes for 1000 images quite low in resolution ) . If there a recording length is deﬁned prior to starting the program , or a video is converted to PV and streamed to MP4 at the same time ( though it is unclear why that would be necessary ) , TGrabs is able to automatically determine which frame - rate can be maintained reliably and without ﬁlling the memory . 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 41 of 69 Manuscript submitted to eLife Appendix 3 1480 Connected components algorithm 1481 Pixels are not represented individually in TRex . Instead , they are saved as connected hori - zontal line segments . For each of these lines , only y - as well as start - and end - position are saved ( 𝑦 , 𝑥 0 and 𝑥 1 ) . This representation is especially suited for objects stretching out along the x - axis , but of course its worst - case is a straight , vertical line – in which case space re - quirements are 𝑂 ( 2 ∗ 𝑁 ) for 𝑁 pixels . Especially for big objects , however , only a fraction of coordinates has to be kept in memory ( with a space requirement of 𝑂 ( 2 ∗ 𝐻 ) instead of 𝑂 ( 𝑊 ∗ 𝐻 ) , with 𝑊 , 𝐻 being width and height of the object ) . 1482 1483 1484 1485 1486 1487 1488 Extracting these connected horizontal line segments from an image can be parallelized easily by cutting the image into full - width pieces and running the following algorithm re - peatedly for each row : 1489 1490 1491 1 . From 0 to 𝑊 , iterate all pixels . Always maintain the previous value ( binary ) , as well as the current value . We start out with our previous value of 𝑝 = 0 ( the border is considered not to be an object ) . 1492 1493 1494 2 . Now repeat for every pixel 𝑝 𝑖 in the current row : 1495 ( a ) If 𝑝 is 1 and 𝑝 𝑖 is 0 , set 𝑝 ∶ = 0 and save the position as the end of a line segment 𝑥 1 = 𝑖 − 1 . 1496 1497 ( b ) If 𝑝 is 0 and 𝑝 𝑖 is 1 , we did not have a previous line segment and a new one starts . We save it as our current line segment with 𝑥 0 and 𝑦 equal to the current row . Set 𝑝 ∶ = 1 . 1498 1499 1500 3 . After each row , if we have a valid current line , we save it in our array of lines . If 𝑝 = 1 was set , and the line segment ended at the border 𝑊 of the image , we ﬁrst set its end position to 𝑥 1 ∶ = 𝑊 − 1 . 1501 1502 1503 We keep the array of extracted lines sorted by their y - coordinate , as well as their x - coordinates in the order we encountered them . To extract connected components , we now just need to walk through all extracted rows and detect changes in the y - coordinate . The only information needed are the current row and the previous row , as well as a list of active preliminary " blobs " ( or connected components ) . A blob is simply a collection of ordered horizontal line segments belonging to a single connected component . These blobs are pre - liminary until the whole image has been processed , since they might be merged into a single blob further down despite currently being separate ( see Figure A1 ) . 1504 1505 1506 1507 1508 1509 1510 1511 " Rows " are an array of horizontal lines with the same y - coordinate , ordered by their x - coordinates ( increasing ) . The following algorithm only considers pairs of previous row 𝑅 𝑖 −1 and current row 𝑅 𝑖 . We start by inserting all separate horizontal line segments of the very ﬁrst row into the pool of active blobs , each assigned their own blob . Lines within row 𝑅 𝑖 are 𝐿 𝑖 , 𝑗 . Coordinates of 𝐿 𝑖 , 𝑗 will be denoted as 𝑥 0 ( 𝑖 , 𝑗 ) , 𝑥 1 ( 𝑖 , 𝑗 ) and 𝑦 ( 𝑖 , 𝑗 ) . Our current index in row 𝑅 𝑖 −1 is 𝑗 and our index in row 𝑅 𝑖 is 𝑘 . We initialize 𝑗 ∶ = 0 , 𝑘 ∶ = 1 . Now for each pair of rows , three diﬀerent actions may be required depending on the case at hand . All three actions are hierarchically ordered and mutually exclusive ( like a typical if / else structure would be ) , meaning that case 0 - 2 can be true at the same time while no other combination can be simultaneously true : 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1 . Case 0 , 1 and 2 : We have to create a new blob . This is the case if ( 0 ) the line in 𝑅 𝑖 ends before the line in 𝑅 𝑖 −1 starts ( 𝑥 1 ( 𝑖 , 𝑘 ) + 1 < 𝑥 0 ( 𝑖 , 𝑗 ) ) , or ( 1 ) y - coordinates of 𝑅 𝑖 and 42 of 69 Manuscript submitted to eLife 𝑅 𝑖 −1 are farther apart than 1 ( 𝑦 ( 𝑖 − 1 , 𝑗 ) > 𝑦 ( 𝑖 , 𝑘 ) + 1 ) , or ( 2 ) there are no lines left in 𝑅 𝑖 −1 to match the current line in 𝑅 𝑖 to ( 𝑗 ≥ | 𝑅 𝑖 −1 | ) . 𝐿 𝑖 , 𝑘 is assigned with a new blob . 1522 1523 1524 1525 2 . Case 3 : Segment in the previous row ends before the segment in the current row starts . If 𝑥 0 ( 𝑖 , 𝑘 ) > 𝑥 1 ( 𝑖 − 1 , 𝑗 ) + 1 , then we just have to 𝑗 ∶ = 𝑗 + 1 . 1526 1527 3 . Case 4 : Segment in the previous row and segment in the current row intersect in x - coordinates . If 𝐿 𝑖 , 𝑘 is no yet assigned with a blob , assign it with the one from 𝐿 𝑖 −1 , 𝑗 . Otherwise , both blobs have to be merged . This is done in a sub - routine , which guar - antees that lines within blobs stay properly sorted during merging . This means that ( i ) y - coordinates increase or stay the same and ( ii ) x - coordinates increase monotonically . Afterwards , we increase either 𝑘 or 𝑗 based on which one associated line ends earlier : If 𝑥 1 ( 𝑖 , 𝑘 ) ≤ 𝑥 1 ( 𝑖 − 1 , 𝑗 ) , then we increase 𝑘 ∶ = 𝑘 + 1 ; otherwise 𝑗 ∶ = 𝑗 + 1 . 1528 1529 1530 1531 1532 1533 1534 After the previous algorithm has been executed on a pair of 𝑅 𝑖 −1 and 𝑅 𝑖 , we increase 𝑖 by one 𝑖 ∶ = 𝑖 + 1 . This process is continued until 𝑖 = 𝐻 , at which point all connected components are contained within the active blob array . 1535 1536 1537 Retaining information about pixel values adds slightly more complexity to the algorithm , but is straight - forward to implement . In TRex , horizontal line segments comprise 𝑦 , 𝑥 0 and 𝑥 1 values plus an additional pointer . It points to the start of a line within array of all pixels ( or an image matrix ) , adding only little computational complexity overall . 1538 1539 1540 1541 Based on the horizontal line segments and their order , posture analysis can be sped up when properly integrated . Another advantage is that detection of connected components within arrays of horizontal line segments is supported due to the way the algorithm func - tions – we can just get rid of the extraction phase . 1542 1543 1544 1545 Appendix 3 Figure A1 . An example array of pixels , or image , to be processed by the connected components algorithm . This ﬁgure should be read from top to bottom , just as the connected components algorithm would do . When this image is analysed , the red and blue objects will temporarily stay separate within diﬀerent " blobs " . When the green pixels are reached , both objects are combined into one identity . 43 of 69 Manuscript submitted to eLife Appendix 4 1546 Matching an object to an object in the next frame 1547 Terminology 1548 A graph is a mathematical structure commonly used in many ﬁelds of research , such as computer science , biology and linguistics . Graphs are made up of vertices , which in turn are connected by edges . Below we deﬁne relevant terms that we are going to use in the following section : 1549 1550 1551 1552 • Directed graph : Edges have a direction assigned to them 1553 • Weighted edges : Edges have a weight ( or cost ) assigned to them 1554 • Adjacent nodes : Nodes which are connected immediately by an edge 1555 • Path : A path is a sequence of edges , where each edges starting vertex is the end vertex of the previous edge 1556 1557 • Acyclic graph : The graph contains no path in which the same vertex appears more than once 1558 1559 • Connected graph : There are no vertices without edges , there is a path from any vertex to any other vertex in the graph 1560 1561 • Bipartite graph : Vertices can be sorted into two distinct groups , without an edge from any vertex to elements of its own group – only to the other group 1562 1563 • Tree : A tree is a connected , undirected , acyclic graph , in which any two vertices are only connected by exactly one path 1564 1565 • Rooted , directed out - tree : A tree where one vertex has been deﬁned to be the root and directed edges , with all edges ﬂowing away from the root 1566 1567 • Visited vertex : A vertex that is already part of the current path 1568 • Leaf : A vertex which has only one edge arriving , but none going out ( in a tree this are the bottom - most vertices ) 1569 1570 • Depth - ﬁrst / breadth - ﬁrst and best - ﬁrst search : Diﬀerent strategies to pick the next ver - tex to explore for a set of paths with traversable edges . Depth - ﬁrst prefers to ﬁrst go deeper inside a graph / tree , before going on to explore other edges of the same ver - tex . Breadth - ﬁrst is the opposite of depth - search . Best - ﬁrst search uses strategies to explore the most promising path ﬁrst . 1571 1572 1573 1574 1575 Background 1576 The transportation problem is one of the fundamental problems in computer science . It solves the problem of transporting a ﬁnite number of goods to a ﬁnite number of factories , where each possible transport route is associated with a cost ( or weight ) . Every factory has a demand for goods and every good has a limited supply . The sum of this cost has to be minimized ( or beneﬁts maximized ) , while remaining within the constraints given by supply and demand . In the special case where demand by each factory and supply for each good are exactly equal to 1 , this problem reduces to the assignment problem . 1577 1578 1579 1580 1581 1582 1583 The assignment problem can be further separated into two distinct cases : the balanced and the unbalanced assignment problem . In the balanced case , net - supply and demand are the same – meaning that the number of factories matches exactly the number of suppliers . While the balanced case can be solved slightly more eﬃciently , most practical problems are usually unbalanced ( Ramshaw and Tarjan ( 2012 ) ) . Thankfully , unbalanced assignments can be reduced to balanced assignments , for example using graph - duplication methods or by adding nodes ( Ramshaw and Tarjan ( 2012 ) , Ramshaw and Tarjan ( 2012 ) ) . This makes the widely used Hungarian method ( Kuhn ( 1955 ) ; Munkres ( 1957 ) ) a viable solution to both , 44 of 69 Manuscript submitted to eLife with a computational complexity of 𝑂 ( 𝑛 3 ) . It can be further improved using Fibonacci heaps ( not implemented in TRex ) , resulting in 𝑂 ( 𝑚𝑠 + 𝑠 2 log 𝑛 ) time - complexity ( Fredman and Tarjan ( 1987 ) ) , with 𝑚 being the number of possible connections / edges , 𝑠 ≤ 𝑛 the number of facto - ries to be supplied and 𝑛 the number of factories . Re - balancing , by adding nodes or other structures , also adds computational cost – especially when 𝑠 ≪ 𝑛 ( Ramshaw and Tarjan ( 2012 ) ) . 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 Adaptation for our matching problem 1598 Assigning individuals to objects in the frame is , in the worst case , exactly that : an unbal - anced assignment problem – potentially with 𝑟 ≠ 𝑠 . During development , we found that we can achieve better average - complexity by combining an approach commonly used to solve NP - hard problems . This is a class problems for which it is ( probably ) not possible to ﬁnd a polynomial - time solution . In order to motivate our usage of a less stable algorithm than e . g . the Hungarian method , let us ﬁrst introduce a more general algorithm , following along with remarks for adapting it to our special case . The next subsection concludes with considera - tions regarding its complexity in comparison to the more stable Hungarian method . 1599 1600 1601 1602 1603 1604 1605 1606 Branch & Bound ( or BnB , Land and Doig ( 2010 ) , formalized in Little et al . ( 1963 ) ) is a very general approach to traversing the large search spaces of NP - hard problems , traditionally represented by a tree . Branching and bounding gives optimal solutions by traversing the entire search space if necessary , but stopping along the way to evaluate its options , always trying to choose better branches of the tree to explore next or skip unnecessary ones . BnB always consists of three main ingredients : 1607 1608 1609 1610 1611 1612 1 . Branching : The division of our problem into smaller , partial problems 1613 2 . Bounding : Estimate the upper / lower limits of the probability / cost gain to be expected by traversing a given edge 1614 1615 3 . Selection : Determining the next node to be processed 1616 Finding good strategies is essential and can have a big impact on overall computation time . Strategies can only be worked out with insight into the speciﬁc problem , but bound - ing is generally the dominating factor here – in that choosing good selection and branching techniques cannot make up for a bad bounding function ( Clausen ( 1999 ) ) . A bounding func - tion estimates an upper ( or lower ) limit for the quality of results that can be achieved within a given sub - problem ( current branch of the tree ) . 1617 1618 1619 1620 1621 1622 The " problem " is the entire assignment problem located at the root node of the tree . The further down we go in the tree , the smaller the partial problems become until we reach a leaf . Any graph can be represented as a tree by duplicating nodes when necessary ( Weixiong ( 1996 ) , " Graph vs . tree " ) . So even if the bipartite assignment graph ( an example sketched in Figure A1 ) is a more " traditional " representation of the assignment problem , we can trans - late it into a rooted , directed out - tree 𝑇 = ( 𝑈 , 𝑉 , 𝐸 , 𝐹 ) with weighted edges . Here , 𝑈 are individuals and 𝑉 are objects in the current frame that are potentially assigned to identities in 𝑈 . 𝐸 are edges mapping from 𝑈 → 𝑉 , while 𝐹 ∶ 𝑉 → 𝑈 . It is quite visible from Fig - ure A1 , that the representation as a tree ( b ) is much more verbose than a bipartite graph ( a ) . However , its structure is very simple : 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 Looking at the tree in Figure A1 ( b ) , individuals ( blue ) are found along the y - axis / deeper into the tree while objects in the frame ( orange ) are listed along on the x - axis . This includes a " null " case per individual , representing the possibility that it is not assigned to any object – ensuring that every individual has at least one edge . 1633 1634 1635 1636 Tree is never generated in its entirety ( except in extreme cases ) , but it represents all pos - sible combinations of individuals and objects . Overall , the set 𝑄 of every complete and valid path from top to bottom would be exactly the same as the set of every valid permutation 45 of 69 Manuscript submitted to eLife of pairings between objects ( plus null ) and individuals . Edge weights in 𝐸 are equal to the probability 𝑃 𝑖 ( 𝑡 , 𝜏 𝑖 | 𝐵 𝑗 ) ( see equation 7 ) , abbreviated to 𝑃 𝑖 ( 𝐵 𝑗 ) here since we are only ever looking at one time - step . 𝐵 𝑗 is an object and 𝑖 is an individual , so we can rewrite it in the current context as 𝑃 𝑢 ( 𝑣 ) , with 𝑢 ∈ 𝑈 ; 𝑣 ∈ 𝑉 . 1637 1638 1639 1640 1641 1642 1643 We are maximizing the objective function 1644 𝑜 ( 𝜌 ) = ∑ 𝑢𝑣 ∈ 𝜌 𝑃 𝑢 ( 𝑣 ) , 1645 1646 1647 1648 where 𝜌 ∈ 𝑄 is an element of all valid paths within 𝑇 . 1649 The simplest approach would be to traverse every edge in the graph and accumulate a sum of probabilities along each path , guaranteeing to ﬁnd the optimal solution eventually . Since the number of possible combinations | 𝑈 | | 𝐸 | grows rapidly with the number of edges , this is not realistic – even with few individuals . Thus , at least the typical number of visited edges has to be minimized . While we do not know the exact solution to our problem before traversing the graph , we can make very good guesses . For example , we may order nodes in such a way that branching ( visiting a node leads to > 1 new edges to be visited ) is reduced in most cases . To do that , we ﬁrst need to calculate the degree of each individual . The degree 𝐶 𝑢 of individual 𝑢 , which is exactly equivalent to the maximum number of edges going out from that individual , we deﬁne as 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 𝐶 𝑢 ∈ ℕ ∶ = ∑ 𝑢 ∈ 𝑈 ⎧⎪⎨⎪⎩ 1 if 𝑃 𝑢 ( 𝑣 ) > 𝑃 min 0 otherwise . 1660 1661 1662 1663 The maximally probable edge per individual also has to be computed beforehand , de - ﬁned as 1664 1665 𝑃 𝑢 = max 𝑣 ∈ 𝑉 { 𝑃 𝑢 ( 𝑣 ) } . 1666 1667 1668 1669 Nodes are sorted ﬁrst by their degree ( ascending ) and secondly by 𝑃 𝑢 ( descending ) . We call this ordered set 𝑆 . Sorting by degree ensures that the nodes with the fewest outgoing edges are visited ﬁrst , causing severe branching to only happen in the lower regions of the tree . This is preferable , because a new branch in the bottom layer merely results in a few more options . If this happens at the top , the tree is essentially duplicated 𝐶 𝑢 times – in one step drastically increasing the overall number of items to be kept in memory . This process is , ﬁttingly , called node sorting ( Weixiong ( 1996 ) ) . Sorting by 𝑃 𝑢 is only applied whenever nodes of the same degree have to be considered . 1670 1671 1672 1673 1674 1675 1676 1677 We always follow the most promising paths ﬁrst ( the one with the highest accumulated probability ) , which is called " best - ﬁrst search " ( BFS ) – our selection strategy for ( 1 . ) in 4 . BFS is implemented using a queue maintaining the list of all currently expanded nodes . 1678 1679 1680 Regarding ( 2 . ) in 4 , we utilize 𝑃 𝑢 as an approximation for the upper bound to the achiev - able probability in each vertex . For each layer with vertices of 𝑈 , we calculate an accumula - tive sum upper _ limit ( 𝑖 ) = ∑ 𝑗 > 𝑖 ∈ 𝑈 𝑃 𝑗 , with 𝑗 , 𝑖 being indices into our ordered set 𝑆 of individuals and 𝑖 being the current depth in the graph ( only counting vertices of 𝑈 ) . This hierarchical upper limit for the expected value does not consider whether the respective edges are still viable , so they could have been eliminated already by assigning the object of 𝑉 to another vertex of 𝑈 above the current one . Any edge with 𝑃 current + upper _ limit ( 𝑖 ) < 𝑃 best is skipped since it can not improve upon our previous best value 𝑃 best . If we do ﬁnd an edge with a better value , we replace 𝑃 best with the new value and continue . 1681 1682 1683 1684 1685 1686 1687 1688 1689 As an example , let us traverse the tree in Figure A1b : 1690 46 of 69 Manuscript submitted to eLife • We ﬁrst calculate 𝑃 𝑢 for every 𝑢 ∈ 𝑈 ( 𝑃 0 = 0 . 85 ; 𝑃 2 = 0 . 9 ; 𝑃 1 = 0 . 75 ) , as well as the hierarchical probability table upper _ limit ( 𝑖 ) for each index 0 ≤ 𝑖 < 𝑁 ( 0 . 9 + 0 . 75 ; 0 . 75 ; 0 ) . 𝑃 best ∶ = 0 . 1691 1692 1693 • Individual 0 ( the root ) is expanded , which has one edge with probability 0 . 85 + upper _ limit ( 0 ) ≥ 𝑃 best to object 3 ( plus the null case ) and is the only node with a degree of 1 . We know that our now expanded node is the best , since it has the largest probability due to sorting , plus also is the deepest . In fact , this is true for all expanded nodes exactly in the order they are expanded ( depth - ﬁrst search = = best - ﬁrst search for our case ) . We set 𝑃 best ∶ = 0 . 85 . The edge to NIL is added to our queue . 1694 1695 1696 1697 1698 1699 • Objects in 𝑉 are only virtual and always have zero - probability connections to the next individual in an ordered set ( 𝑓 ∈ 𝐹 ) , so they do not add to the overall probability sum . We skip to the next node . 1700 1701 1702 • Individual 2 branches oﬀ into one or two diﬀerent edges , depending on which edges have been chosen previously . 1703 1704 • We ﬁrst explore the edge towards object 4 with a probability of 0 . 9 + upper _ limit ( 1 ) = 1 . 65 ≥ 𝑃 best and add it to 𝑃 best . 1705 1706 • Only one possibility is left and we arrive at a leaf with an accumulated probability of 0 . 85 + 0 . 9 + 0 = 1 . 75 . 1707 1708 • We now perform backtracking , meaning we look at every expanded node in our queue , each time observing 𝑃 𝑢 + upper _ limit ( 𝑖 ) . 1709 1710 – NIL ( from node 2 ) would be added to the front of our queue , however its proba - bility 0 . 85 + 0 + upper _ limit ( 1 ) = 1 . 6 < 1 . 75 = 𝑃 best , so it is discarded . 1711 1712 – NIL ( from node 0 ) would be added now , but its probability of 0 + upper _ limit ( 0 ) = 1 . 65 < 𝑃 best , so it is also discarded . 1713 1714 We can see that with increased depth , we have to keep track of more and more pos - sibilities . Since our nodes and edges are pre - sorted , our path through the tree is optimal after exactly 𝑁 = | 𝑈 | node expansions ( not counting 𝑣 ∈ 𝑉 expansions since they are only " virtual " ) . 1715 1716 1717 1718 Complexity 1719 Utilizing these techniques , we can achieve very good average - case complexity . Of course having a good worst - case complexity is important ( such as the Hungarian method ) , but the impact of a good average - case complexity can be signiﬁcant as well . This is illustrated nicely by the timings measured in Table A3 , where our method consistently surpasses the Hungar - ian method in terms of performance – especially for very large groups of animals – despite having worse worst - case complexity . Usually , even in situations with over 1000 individuals present , the average number of leaves visited was approximately 1 . 112 ( see Table A5 ) and each visit was a global improvement ( not shown ) . The number of nodes visited per frame were around 2844 to 19 , 804 , 880 in the same video , which , given the maximal number of pos - sible combinations 𝑁 𝑀 for 𝑀 edges and 𝑁 individuals ( Thomas ( 2016 ) ) , is quite moderate . Especially considering the number of calculations that the Hungarian method has to per - form in every step , which , according to its complexity , will be in the range of 𝑁 3 ≈ 1e9 for 𝑁 = 1024 individuals . 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 The average complexity of a solution using best - ﬁrst - search BnB is given by Weixiong ( 1996 ) . It depends on the probability of encountering a " zero - cost edge " 𝑝 0 , as well as the mean branching factor 𝑏 of the tree : 1733 1734 1735 1 . Θ ( 𝛽 𝑁 ) when 𝑏𝑝 0 < 1 , with 𝛽 ≤ 𝑏 and 𝑁 is the depth of the tree 1736 2 . Θ ( 𝑁 2 ) when 𝑏𝑝 0 = 1 1737 47 of 69 Manuscript submitted to eLife 0 . 85 0 . 75 0 . 66 0 . 3 2 0 . 9 0 1 2 3 4 ( a ) 0 . 85 0 0 . 9 0 0 0 0 0 . 32 0 0 0 0 . 9 0 0 . 75 0 0 . 66 0 0 . 32 0 0 0 0 . 75 0 . 32 0 0 3 2 4 1 NIL NIL 1 4 NIL NIL 2 4 1 3 NIL 3 1 4 NIL NIL 1 3 4 NIL ( b ) Appendix 4 Figure A1 . A bipartite graph ( a ) and its equivalent tree - representation ( b ) . It is bipartite since nodes can be sorted into two disjoint and independent sets ( { 0 , 1 , 2 } and { 3 , 4 } ) , where no nodes have edges to other nodes within the same set . ( a ) is a straight - forward way of depicting an assignment problem , with the identities on the left side and objects being assigned to the identities on the right side . Edge weights are , in TRex and this example , probabilities for a given identity to be the object in question . This graph is also an example for an unbalanced assignment problem , since there are fewer objects ( orange ) available than individuals ( blue ) . The optimal solution in this case , using weight - maximization , is to assign 0 → 3 ; 2 → 4 and leave 1 unassigned . Invalid edges have been pruned from the tree in ( b ) , enforcing the rule that objects can only appear once in each path . The optimal assignments have been highlighted in red . 3 . Θ ( 𝑁 ) when 𝑏𝑝 0 > 1 ⇔ 𝑏 > 1∕ 𝑝 0 1738 as 𝑁 → ∞ . 1739 In our case the depth of the tree is exactly the number of individuals 𝑁 , which we have already substituted here . This is the number of nodes that have to be visited in the best case . A " zero - cost edge " is an edge that does not add any cost to the current path . We are maximizing ( not minimizing ) so in our case this would be " an edge with a probability of 1 " . While reaching exactly 1 is improbable , it is ( in our case ) equivalent to " having only one viable edge arriving at an object " . 𝑝 0 depends very much on the settings , speciﬁcally the maximum movement speed allowed , and behavior of individuals , which is why in sce - narios with > 100 individuals the maximum speed should always be adjusted ﬁrst . To put it another way : If there are only few branching options available for the algorithm to explore per individual , which seems to be the case even in large groups , we can assume our graph to have a probability 𝑝 0 within 0 ≪ 𝑝 0 ≤ 1 . The mean branching factor 𝑏 is given by the mean number of edges arriving at an object ( not an individual ) . Averaging at around 𝑏 ≈ 𝑘 + 1 , with 𝑘 ≥ 1 being the average number of assignable blobs per individual ( roughly 1 . 005 in Video 0 ) and 1 the null - case , we can assume 𝑏𝑝 0 to be > 1 on average . An average complexity of 𝑂 ( 𝑁 2 ) , as long as 𝑏 > 1∕ 𝑝 0 , is even better than the complexity of the Hungarian method ( which is also 𝑂 ( 𝑁 3 ) in the average - case , Bertsekas ( 1981 ) ) , giving a possible explanation for the good results achieved using tree - based matching in TRex on average ( Table A3 ) . 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 Further optimizations could be implemented , e . g . using impact - based heuristics ( as an example of dynamic variable ordering ) instead of the static and coarse maximum probabil - ity estimate used here . Such heuristics ﬁrst choose the vertex " triggering the largest search space reduction " ( Pesant et al . ( 2012 ) ) . In our case , assigning an individual ﬁrst if , for exam - ple , it has edges to many objects that each only one other individual is connected to . 1757 1758 1759 1760 1761 48 of 69 Manuscript submitted to eLife Appendix 4 Table A1 . Showing quantiles for frame timings for videos of the speed dataset ( without posture enabled ) . Videos 15 , 16 and 14 each contain a short sequence of taking out the ﬁsh , causing a lot of big objects and noise in the frame . This leads to relatively high spikes in these segments of the video , resulting in high peak processing timings here . Generally , processing time is inﬂuenced by a lot of factors involving not only TRex , but also the operating system as well as other programs . While we did try to control for these , there is no way to make sure . However , having sporadic spikes in the timings per frame does not signiﬁcantly inﬂuence overall processing time , since it can be compensated for by later frames . We can see that videos of all quantities ≤ 256 individuals can be processed faster than they could be recorded . Videos that can not be processed faster than real - time are underlaid in gray . video characteristics ms / frame ( processing ) processing time video # ind . ms / frame 5 % mean 95 % max > real - time % video length 0 1024 25 . 0 46 . 93 62 . 96 119 . 54 849 . 16 100 . 0 % 358 . 12 1 512 20 . 0 19 . 09 29 . 26 88 . 57 913 . 52 92 . 11 % 259 . 92 2 512 16 . 67 17 . 51 26 . 53 36 . 72 442 . 12 97 . 26 % 235 . 39 3 256 20 . 0 8 . 35 11 . 28 13 . 25 402 . 54 1 . 03 % 77 . 18 4 256 16 . 67 8 . 04 11 . 62 13 . 48 394 . 75 1 . 13 % 94 . 77 5 128 16 . 67 3 . 54 5 . 14 5 . 97 367 . 92 0 . 41 % 40 . 1 6 128 16 . 67 3 . 91 5 . 64 6 . 89 381 . 51 0 . 51 % 44 . 38 7 100 31 . 25 2 . 5 3 . 57 5 . 19 316 . 75 0 . 1 % 28 . 35 8 59 19 . 61 1 . 43 2 . 29 3 . 93 2108 . 77 0 . 19 % 16 . 33 9 15 40 . 0 0 . 4 0 . 52 1 . 67 4688 . 5 0 . 01 % 2 . 96 10 10 10 . 0 0 . 28 0 . 33 0 . 57 283 . 7 0 . 07 % 8 . 08 11 10 31 . 25 0 . 21 0 . 25 0 . 65 233 . 7 0 . 01 % 3 . 48 12 10 31 . 25 0 . 23 0 . 27 0 . 75 225 . 63 0 . 02 % 2 . 82 13 10 31 . 25 0 . 22 0 . 25 0 . 54 237 . 32 0 . 02 % 2 . 64 14 8 33 . 33 0 . 24 0 . 29 0 . 66 172 . 8 0 . 02 % 1 . 8 15 8 40 . 0 0 . 22 0 . 26 0 . 88 244 . 88 0 . 01 % 1 . 5 16 8 28 . 57 0 . 18 0 . 21 0 . 51 1667 . 14 0 . 02 % 1 . 38 17 1 7 . 14 0 . 03 0 . 04 0 . 06 220 . 81 0 . 01 % 1 . 56 49 of 69 Manuscript submitted to eLife [ 0 - 80s ) [ 0 . 0 - 0 . 26 % ) [ 80 - 160s ) [ 0 . 26 - 0 . 52 % ) [ 160 - 320s ) [ 0 . 52 - 1 . 05 % ) [ 320 - 640s ) [ 1 . 05 - 2 . 1 % ) [ 640 - 1515s ) [ 2 . 1 - 4 . 98 % ) segment length ( s ) % of frames / segment 0 20 40 60 80 % o f d a t a i n s i d e b i n id : 40963 trex : 39075 id : 316 trex : 341 id : 108 trex : 140 id : 15 trex : 83 trex : 13 TRex idtracker . ai Appendix 4 Figure A2 . The same set of videos as in Table 5 pooled together , we evaluate the eﬃciency of our crossings solver . Consecutive frame segments are sequences of frames without gaps , for example due to crossings or visibility issues . We ﬁnd these consecutive frame segments in data exported by TRex , and compare the distribution of segment - lengths to idtracker . ai ’s results ( as a reference for an algorithm without a way to resolve crossings ) . In idtracker . ai ’s case , we segmented the non - interpolated tracks by missing frames , assuming tracks to be correct in between . The Y - axis shows the percentage of ∑ 𝑘 ∈ [ 1 , 𝑉 ] video _ length 𝑘 ∗ # individuals 𝑘 in 𝑉 videos that one column makes up for – the overall coverage for TRex was 98 % , while idtracker . ai was slightly worse with 95 . 17 % . Overall , the data distribution suggests that , probably due to it attempting to resolve crossings , TRex seems to produce longer consecutive segments . Appendix 4 Table A2 . A quality assessment of assignment decisions made by the general purpose tracking system without the aid of visual recognition – comparing results of two accurate tracking algorithms with the assignments made by an approximate method . Here , decisions are reassignments of an individual after it has been lost , or the tracker was too " unsure " about an assignment . Decisions can be either correct or wrong , which is determined by comparing to reference data generated using automatic visual recognition : Every segment of frames between decisions is associated with a corresponding " baseline - truth " identity from the reference data . If this association changes after a decision , then that decision is counted as wrong . Analysing a decision may fail if no good match can be found in the reference data ( which is not interpolated ) . Failed decisions are ignored . Comparative values for the Hungarian algorithm ( Kuhn ( 1955 ) ) are always exactly the same as for our tree - based algorithm , and are therefore not listed separately . Left - aligned total , excluded and wrong counts in each column are results achieved by an accurate algorithm , numbers to their right are the corresponding results using an approximate method . video # ind . length total excluded wrong 7 100 1min 717 755 22 22 45 ( 6 . 47 % ) 65 ( 8 . 87 % ) 8 59 10min 279 312 146 100 55 ( 41 . 35 % ) 32 ( 15 . 09 % ) 9 15 1h0min 838 972 70 111 100 ( 13 . 02 % ) 240 ( 27 . 87 % ) 13 10 10min3s 331 337 22 22 36 ( 11 . 65 % ) 54 ( 17 . 14 % ) 12 10 10min3s 382 404 42 43 83 ( 24 . 41 % ) 130 ( 36 . 01 % ) 11 10 10min10s 1067 1085 50 52 73 ( 7 . 18 % ) 92 ( 8 . 91 % ) 14 8 3h15min22s 7424 7644 1428 1481 1174 ( 19 . 58 % ) 1481 ( 24 . 03 % ) 15 8 1h12min 3538 3714 427 517 651 ( 20 . 93 % ) 962 ( 30 . 09 % ) 16 8 3h18min13s 2376 3305 136 206 594 ( 26 . 52 % ) 1318 ( 42 . 53 % ) sum 16952 16754 −2343 −2554 2811 ( 19 . 24 % ) 4374 ( 27 . 38 % ) 50 of 69 Manuscript submitted to eLife 200 300 400 500 600 700 800 900 # individuals 0 100 200 300 400 m s / f r a m e 17445741744574 4159041590 97439743 3167531675 43624427 1564015575 N tree N hun . tree hungarian Appendix 4 Figure A3 . Mean values of processing - times and 5 % / 95 % percentiles for video frames of all videos in the speed dataset ( Table 1 ) , comparing two diﬀerent matching algorithms . Parameters were kept identical , except for the matching mode , and posture was turned oﬀ to eliminate its eﬀects on performance . Our tree - based algorithm is shown in green and the Hungarian method in red . Grey numbers above the graphs show the number of samples within each bin , per method . Diﬀerences between the algorithms increase very quickly , proportional to the number of individuals . Especially the Hungarian method quickly becomes very computationally intensive , while our tree - based algorithm shows a much shallower curve . Some frames could not be solved in reasonable time by the tree - based algorithm alone , at which point it falls back to the Hungarian algorithm . Data - points belonging to these frames ( 𝑁 = 79 ) have been excluded from the results for both algorithms . One main advantage of the Hungarian method is that , with its bounded worst - case complexity ( see Matching an object to an object in the next frame ) , no such combinatorical explosions can happen . However , even given this advantage the Hungarian method still leads to signiﬁcantly lower processing speed overall ( see also appendix Table A3 ) . 51 of 69 Manuscript submitted to eLife Appendix 4 Table A3 . Comparing computation speeds of the tree - based tracking algorithm with the widely established Hungarian algorithm Kuhn ( 1955 ) , as well as an approximate version optimized for large quantities of individuals . Posture estimation has been disabled , focusing purely on the assignment problem in our timing measurements . The tree - based algorithm is programmed to fall back on the Hungarian method whenever the current problem " explodes " computationally – these frames were excluded . Listed are relevant video metrics on the left and mean computation speeds on the right side for three diﬀerent algorithms : ( 1 ) The tree - based and ( 2 ) the approximate algorithm presented in this paper , and ( 3 ) the Hungarian algorithm . Speeds listed here are percentages of real - time ( the videos’ fps ) , demonstrating usability in closed - loop applications and overall performance . Results show that increasing the number of individuals both increases the time - cost , as well as producing much larger relative standard deviation values . ( 1 ) is almost always fast than ( 3 ) , while becoming slower than ( 2 ) with increasing individual numbers . In our implementation , all algorithms produce faster than real - time speeds with 256 or fewer individuals ( see also appendix Table A1 ) , with ( 1 ) and ( 2 ) even getting close for 512 individuals . video metrics % real - time video # ind . fps ( Hz ) size ( px 2 ) tree approximate hungarian 0 1024 40 3866 × 4048 35 . 49 ± 65 . 94 38 . 69 ± 65 . 39 12 . 05 ± 18 . 72 1 512 50 3866 × 4140 51 . 18 ± 180 . 08 75 . 02 ± 193 . 0 28 . 92 ± 29 . 12 2 512 60 3866 × 4048 59 . 66 ± 121 . 4 65 . 58 ± 175 . 51 23 . 18 ± 26 . 83 3 256 50 3866 × 4140 174 . 02 ± 793 . 12 190 . 62 ± 743 . 54 127 . 86 ± 9841 . 21 4 256 60 3866 × 4048 140 . 73 ± 988 . 15 155 . 9 ± 760 . 05 108 . 48 ± 2501 . 06 5 128 60 3866 × 4048 318 . 6 ± 347 . 8 353 . 58 ± 291 . 63 312 . 05 ± 337 . 71 6 128 60 3866 × 4048 286 . 13 ± 330 . 08 314 . 91 ± 303 . 53 232 . 33 ± 395 . 21 7 100 32 3584 × 3500 572 . 46 ± 98 . 21 611 . 5 ± 96 . 46 637 . 87 ± 97 . 03 8 59 51 2306 × 2306 744 . 98 ± 264 . 43 839 . 45 ± 257 . 56 864 . 01 ± 223 . 47 9 15 25 1880 × 1881 4626 . 84 ± 424 . 8 4585 . 08 ± 378 . 64 4508 . 08 ± 404 . 56 10 10 100 1920 × 1080 2370 . 35 ± 303 . 94 2408 . 27 ± 297 . 83 2362 . 42 ± 296 . 99 11 10 32 3712 × 3712 6489 . 12 ± 322 . 59 6571 . 28 ± 306 . 34 6472 . 0 ± 322 . 03 12 10 32 3712 × 3712 6011 . 59 ± 318 . 12 6106 . 12 ± 305 . 96 5549 . 25 ± 318 . 21 13 10 32 3712 × 3712 6717 . 12 ± 325 . 37 6980 . 12 ± 316 . 59 6726 . 46 ± 316 . 87 14 8 30 3008 × 3008 8752 . 2 ± 2141 . 03 8814 . 63 ± 2101 . 4 8630 . 73 ± 2177 . 16 15 8 25 3008 × 3008 9786 . 68 ± 1438 . 08 10118 . 04 ± 1380 . 2 9593 . 44 ± 1439 . 28 16 8 35 3008 × 3008 9861 . 42 ± 1424 . 91 10268 . 82 ± 1339 . 8 9680 . 68 ± 1387 . 14 17 1 140 1312 × 1312 15323 . 05 ± 637 . 17 15250 . 39 ± 639 . 2 15680 . 93 ± 640 . 99 52 of 69 Manuscript submitted to eLife Appendix 4 Table A4 . Comparing the time - cost for tracking and converting videos in two steps with doing both of those tasks at the same time . The columns prepare and tracking show timings for the tasks when executed separately , while live shows the time when both of them are performed at the same time using the live - tracking feature of TGrabs . The column win shows the time " won " by combining tracking and preprocessing as the percentage ( prepare + tracking − live ) ∕ ( prepare + tracking ) . The process is more complicated than simply adding up timings of the tasks . Memory and the interplay of work - loads have a huge eﬀect here . Posture is enabled in all variants . video metrics minutes video # ind . length fps ( Hz ) prepare tracking live win ( % ) 0 1024 8 . 33min 40 10 . 96 ± 0 . 3 41 . 11 ± 0 . 34 65 . 72 ± 1 . 35 −26 . 23 1 512 6 . 67min 50 11 . 09 ± 0 . 24 24 . 43 ± 0 . 2 33 . 67 ± 0 . 58 5 . 24 2 512 5 . 98min 60 11 . 72 ± 0 . 2 20 . 86 ± 0 . 47 31 . 1 ± 0 . 62 4 . 55 3 256 6 . 67min 50 11 . 09 ± 0 . 21 7 . 99 ± 0 . 17 12 . 35 ± 0 . 17 35 . 26 4 256 5 . 98min 60 11 . 76 ± 0 . 26 9 . 04 ± 0 . 26 15 . 08 ± 0 . 13 27 . 46 6 128 5 . 98min 60 11 . 77 ± 0 . 29 4 . 74 ± 0 . 13 12 . 13 ± 0 . 32 26 . 49 5 128 6 . 0min 60 11 . 74 ± 0 . 26 4 . 54 ± 0 . 1 12 . 08 ± 0 . 25 25 . 79 7 100 1 . 0min 32 1 . 92 ± 0 . 02 0 . 47 ± 0 . 01 2 . 03 ± 0 . 02 14 . 88 8 59 10 . 0min 51 6 . 11 ± 0 . 07 7 . 68 ± 0 . 12 9 . 28 ± 0 . 08 32 . 7 9 15 60 . 0min 25 12 . 59 ± 0 . 18 5 . 32 ± 0 . 07 13 . 17 ± 0 . 12 26 . 47 11 10 10 . 17min 32 8 . 58 ± 0 . 04 0 . 74 ± 0 . 01 8 . 8 ± 0 . 12 5 . 66 12 10 10 . 05min 32 8 . 68 ± 0 . 04 0 . 75 ± 0 . 01 8 . 65 ± 0 . 07 8 . 3 13 10 10 . 05min 32 8 . 67 ± 0 . 03 0 . 71 ± 0 . 01 8 . 65 ± 0 . 07 7 . 76 10 10 10 . 08min 100 4 . 17 ± 0 . 06 2 . 02 ± 0 . 02 4 . 43 ± 0 . 05 28 . 3 14 8 195 . 37min 30 110 . 51 ± 2 . 32 8 . 99 ± 0 . 22 109 . 97 ± 2 . 05 7 . 98 15 8 72 . 0min 25 31 . 84 ± 0 . 53 3 . 26 ± 0 . 07 32 . 1 ± 0 . 42 8 . 55 16 8 198 . 22min 35 133 . 45 ± 2 . 22 11 . 38 ± 0 . 28 133 . 1 ± 2 . 28 8 . 1 mean 14 . 55 % 53 of 69 Manuscript submitted to eLife Appendix 4 Table A5 . Statistics for running the tree - based matching algorithm with the videos of the speed dataset . We achieve low leaf and node visits across the board – this is especially interesting in videos with high numbers of individuals . High values for ’ # nodes visited’ are only impactful if they make up a large portion of the assignments . These are the result of too many choices for assignments – the weak point of the tree - based algorithm – and lead to combinatorical " explosions " ( the method will take a really long time to ﬁnish ) . If such an event is detected , TRex automatically switches to a more computationally bounded algorithm like the Hungarian method . video characteristics matching stats video # ind . # nodes visited ( 5 , 50 , 95 , 100 % ) # leafs visited # improvements 0 1024 [ 1535 ; 2858 ; 83243 ; 18576918 ] 1 . 113 ± 0 . 37 1 . 113 1 512 [ 1060 ; 8156 ; 999137 ; 19811558 ] 1 . 247 ± 0 . 61 1 . 247 2 512 [ 989 ; 2209 ; 56061 ; 8692547 ] 1 . 159 ± 0 . 47 1 . 159 3 256 [ 452 ; 479 ; 969 ; 205761 ] 1 . 064 ± 0 . 29 1 . 064 4 256 [ 475 ; 496 ; 584 ; 608994 ] 1 . 028 ± 0 . 18 1 . 028 5 128 [ 233 ; 245 ; 258 ; 7149 ] 1 . 012 ± 0 . 12 1 . 012 6 128 [ 237 ; 259 ; 510 ; 681702 ] 1 . 046 ± 0 . 25 1 . 046 7 100 [ 195 ; 199 ; 199 ; 13585 ] 1 . 014 ± 0 . 14 1 . 014 8 59 [ 117 ; 117 ; 117 ; 16430 ] 1 . 014 ± 0 . 2 1 . 014 9 15 [ 24 ; 29 ; 29 ; 635 ] 1 . 027 ± 0 . 22 1 . 027 10 10 [ 17 ; 19 ; 19 ; 56 ] 1 . 001 ± 0 . 02 1 . 001 11 10 [ 19 ; 19 ; 19 ; 129 ] 1 . 006 ± 0 . 1 1 . 006 12 10 [ 19 ; 19 ; 19 ; 1060 ] 1 . 023 ± 0 . 23 1 . 023 13 10 [ 19 ; 19 ; 19 ; 106 ] 1 . 001 ± 0 . 04 1 . 001 14 8 [ 11 ; 15 ; 15 ; 893 ] 1 . 003 ± 0 . 08 1 . 003 15 8 [ 13 ; 15 ; 15 ; 597 ] 1 . 024 ± 0 . 23 1 . 024 16 8 [ 15 ; 15 ; 15 ; 2151 ] 1 . 009 ± 0 . 17 1 . 009 17 1 [ 1 ; 1 ; 1 ; 1 ] 1 . 0 ± 0 . 02 1 . 0 54 of 69 Manuscript submitted to eLife Appendix 5 1762 Posture 1763 Estimating an animals orientation and body pose in space is a diverse topic , where angle and pose can mean many diﬀerent things . We are not estimating the individual positions of many legs and antennae in TRex , we simply want to know where the front - and the back - end of the animal are . Ultimately , the goal here is to be able to align animals using an arbitrary axis with their head extending in one direction and their tail roughly in the opposite direction . In order to achieve this , we are required to follow a series of steps to acquire all the necessary information : 1764 1765 1766 1767 1768 1769 1770 1 . Locate objects in the image 1771 2 . Detect the edge of objects 1772 3 . Find an ordered set of points ( the outline ) , which in sequence approximate the outer edge of an object in the scene . This is done for each object ( as well as for holes ) . 1773 1774 4 . Calculate a center - line based on local curvature of the outline . 1775 5 . Calculate head and tail positions . 1776 The ﬁrst point is a given at this point ( see Connected components algorithm ) . We can utilize the format in which connected components are computed in TRex ( an ordered array of horizontal line segments ) , which reduces redundancy by avoiding to look at every individ - ual pixel . These line segments also contain information about edges since every start and end has to be an edge - pixel , too . 1777 1778 1779 1780 1781 Even though we already have a list of edge - pixels , retrieving an ordered set of points is crucial and requires much more eﬀort . Without information about a pixels connectivity , we can not diﬀerentiate between inner and outer shapes ( holes vs . outlines ) and we can not calculate local curvature . 1782 1783 1784 1785 Connecting pixels to form an outline 1786 We implemented an algorithm based on horizontal line segments , which only ever retains three consecutive rows of pixels ( 𝑝 previous , 𝑐 current and 𝑛 next ) . These horizontal line segments always stem from a " blob " ( or connected component ) . Rows contain ( i ) their y - value in pixels , ( ii ) 𝑥 0 , 𝑥 1 values describing the ﬁrst and last " on " - pixel that has been found in it , ( iii ) a set of detected border pixels ( identiﬁed by their x - coordinate ) . A row is valid , whenever the 𝑦 coordinate is not −1 – all three rows are initialized to an invalid 𝑦 = −1 . 𝑙 ′ is the previous row . Using 𝑝 , 𝑐𝑜𝑟𝑛 as a function 𝑐 ( 𝑥 ) returns 1 for on - pixels at that x - coordinate , and 0 for oﬀ - pixels . 1787 1788 1789 1790 1791 1792 1793 1794 For each line 𝑙 in the sorted list of horizontal line segments , we detect border pixels : 1795 1 . subtract the blobs position ( minimum of all 𝑙 𝑥 0 and 𝑙 𝑦 separately ) from 𝑙 1796 2 . if 𝑛 𝑦 ≠ 𝑙 𝑦 , a row has ended and a new one starts : call ﬁnalize else if 𝑙 𝑥 0 − 𝑙 ′ 𝑥 1 ≥ 1 ∧ 𝑙 𝑥 0 ≥ 𝑐 𝑥 0 , we either skipped a few pixels in 𝑛 or 𝑙 starts before 𝑐 even had valid pixels . This means that all pixels 𝑥 between max { 𝑙 ′ 𝑥 1 + 1 ; 𝑐 𝑥 0 } ≤ 𝑥 < min { 𝑙 𝑥 0 ; 𝑐 𝑥 1 + 1 } are border pixels in 𝑐 . 1797 1798 1799 1800 3 . if 𝑙 𝑥 1 < 𝑐 𝑥 0 , or 𝑐 is invalid , then line 𝑙 ends before the previous row ( 𝑐 ) even has any " on " - pixels . All pixels 𝑥 between 𝑙 𝑥 0 ≤ 𝑥 ≤ 𝑙 𝑥 1 are border pixels in 𝑛 . else 1801 1802 1803 ( a ) 𝑠 ∶ = 𝑙 𝑥 0 1804 ( b ) if 𝑠 < 𝑐 𝑥 0 , then lines are overlapping in 𝑐 and 𝑛 ( line 𝑙 ) . We can ﬁll 𝑛 up with border while 𝑥 < 𝑐 𝑥 0 and 𝑥 ≤ 𝑙 𝑥 1 . Set 𝑠 ∶ = min { 𝑐 𝑥 0 − 1 ; 𝑙 𝑥 1 } . 55 of 69 Manuscript submitted to eLife else if 𝑠 = 0 or 𝑠 > 0 ∧ 𝑛 ( 𝑠 − 1 ) = 0 , then 𝑙 starts at the image border ( which is an automatic border pixel ) or there is a gap before 𝑙 . Set 𝑠 ∶ = 𝑠 + 1 . 1805 1806 1807 1808 ( c ) All pixels at 𝑥 - coordinates 𝑠 ≤ 𝑥 ≤ 𝑙 𝑥 1 are border in 𝑛 , if they are either ( i ) beyond 𝑐 ’s bounds ( 𝑥 ≥ 𝑐 𝑥 1 ) , or ( ii ) 𝑐 ( 𝑥 ) = 0 . 1809 1810 4 . Set 𝑛 𝑥 1 ∶ = 𝑙 𝑥 1 . 1811 After iterating through all lines , we need two additional calls to finalize to populate the lines currently in 𝑐 and 𝑛 through . 1812 1813 A graph is updated each time a row is ﬁnalized . This graph stores all border " nodes " , as well as all a maximum of two edges per node ( since this is the maximum number of neigh - bors for a line vertex ) . More on that below . The following procedure ( finalize ) prepares a row ( 𝑐 ) to be integrated into the graph , using two parameters : A triplet of rows ( 𝑝 , 𝑐 , 𝑛 ) and the ﬁrst line 𝑙 , which started the new row to be added . 1814 1815 1816 1817 1818 1 . if 𝑛 is invalid , continue to the next operation . else if 𝑙 𝑦 > 𝑛 𝑦 + 1 , then we skipped at least one row between 𝑛 and the new row – making all on - pixels in 𝑛 border pixels . else we have consecutive rows where 𝑙 𝑦 = 𝑛 𝑦 + 1 . All on - pixels 𝑥 in 𝑛 between 𝑛 𝑥 0 ≤ 𝑥 ≤ 𝑙 𝑥 0 − 1 are border pixels . 1819 1820 1821 1822 1823 2 . Now the current row ( 𝑐 ) is certainly ﬁnished , as it will in the following become the previous row ( 𝑝 ) , which is read - only at that point . We can add every border - pixel of 𝑐 to our graph ( see below ) . 1824 1825 1826 3 . It then discards 𝑝 and moves 𝑐 → 𝑝 and 𝑛 → 𝑐 , as well as reading a new row to assign to 𝑛 , setting 𝑛 𝑥 0 = 𝑙 𝑥 0 , 𝑛 𝑥 1 = 𝑙 𝑥 1 , 𝑛 𝑦 = 𝑙 𝑦 . 1827 1828 The graph consists of nodes ( border pixels ) , indexed by their x and y coordinates ( in - tegers ) and containing a list of all on - pixels around them ( 8 - neighbourhood with top - left , top , left , bottom - left , etc . ) . This information is available when finalize is called , since the middle row ( 𝑐 ) is fully deﬁned at that point ( its entire neighbourhood has been cached ) . 1829 1830 1831 1832 After all rows have been processed , an additional step is needed to connect all nodes and produce a connected , clockwise ordered outline . We already marked all pixels that have at least one border . We can also already mark TOP , RIGHT , BOTTOM and LEFT borders per node if no neighbouring pixel is present in that direction , since these major directions will deﬁnitely get a " line " in the end . So all we have left to do now , is check the diagonals . The points that will be returned , are located half - way along the outer edges of pixels . In the end , each pixel can potentially have four border lines ( if it is a singular pixel without connections to other pixels , see yellow " hole " in Figure A1b ) . The half - edge - points for each node are generated as follows : 1833 1834 1835 1836 1837 1838 1839 1840 1841 1 . A nodes list of border pixels is a sparse , ordered list of directions ( top , top - right , . . . , top - left ) . Each major direction of these ( TOP , RIGHT , BOTTOM , LEFT ) , if present , check the face of their square to the left of them ( own direction - 1 , or - 45° ) . For example , TOP would check top - left . 1842 1843 1844 1845 2 . if the checked neighbour is on , we add an edge between our face ( e . g . TOP ) and its 90° rotated face ( e . g . own direction + 2 = RIGHT ) . else check the face an additional 45° to the left ( e . g . LEFT ) . 1846 1847 1848 ( a ) if it there is an on - pixel attached to this face , add an edge between the two faces ( of the focal and its left pixel ) in the same direction ( e . g . TOP → TOP ) . 1849 1850 ( b ) else we do not seem to have a neighbour to either side , so this must be a corner pixel . Add an edge from the focal face ( e . g . TOP ) to the side 90° to the left of itself ( e . g . LEFT ) . 1851 1852 1853 56 of 69 Manuscript submitted to eLife Each time an edge is added , more and more of the half - edges are becoming fully - connected ( meaning they have two of the allowed two edges ) . To generate the ﬁnal result , all we have to do is to start somewhere in the graph and walk strictly in clockwise direction . " Walking " is done using a queue and edges are followed using depth - ﬁrst search ( see Matching an ob - ject to an object in the next frame ) : Each time a node is visited , all its yet unexplored edges are added to the front of the queue ( in clockwise order ) . Already visited edges are marked ( or pruned ) and will not be traversed again – their ﬂoating - point positions ( somewhere on an edge of its parent pixel ) are added to an array . 1854 1855 1856 1857 1858 1859 1860 1861 After a path ended , meaning that no more edges can be reached from our current node , the collected ﬂoating - point positions are pushed to another array and a diﬀerent , yet unvis - ited , starting node is sought . This way , we can accumulate all available outlines in a given image one - by - one – including holes . 1862 1863 1864 1865 These outlines will usually be further processed using an Elliptical Fourier Transform ( or EFT , Kuhl and Giardina ( 1982 ) ) , as mentioned in the main - text . Outlines can also be smoothed using a weighted average of the 𝑁 points around a given point , or resampled to either reduce or ( virtually ) increase resolution . 1866 1867 1868 1869 Finding the tail 1870 Given an ordered outline , curvature can be calculated locally ( per index 𝑖 ) : 1871 𝐶 ( 𝑖 ) = 4 ∗ triangle _ area ( 𝑝 𝑖 − 𝑟 , 𝑝 𝑖 , 𝑝 𝑖 + 𝑟 ) ∕ ( ‖‖ 𝑝 𝑖 − 𝑝 𝑖 − 𝑟 ‖‖ ∗ ‖‖ 𝑝 𝑖 − 𝑝 𝑖 + 𝑟 ‖‖ ∗ ‖‖ 𝑝 𝑖 − 𝑟 − 𝑝 𝑖 + 𝑟 ‖‖ ) 1872 1873 1874 1875 where 1 ≤ 𝑟 ∈ ℕ is a parameter , which eﬀectively leads to more smoothing when in - creased . Triangle area can be calculated as follows : 1876 1877 triangle _ area ( 𝐚 , 𝐛 , 𝐜 ) = ( 𝐛 𝑥 − 𝐚 𝑥 ) ( 𝐜 𝑦 − 𝐚 𝑦 ) − ( 𝐛 𝑦 − 𝐚 𝑦 ) ( 𝐜 𝑥 − 𝐚 𝑥 ) . 1878 1879 1880 1881 To ﬁnd the " tail " , or the pointy end of the shape , we employ a method closely related to scipys ﬁnd _ peaks function : We ﬁnd local maxima using discrete curve diﬀerentiation and then generate a hierarchy of these extrema . The only major diﬀerence to normal diﬀeren - tiation is that we assume periodicity to achieve our results – values wrap around in both directions , since we are dealing with an outline here . We then ﬁnd the peak with the largest integral , meaning we detect both very wide and very high peaks ( just not very slim ones ) . The center of this peak is the " tail " . 1882 1883 1884 1885 1886 1887 1888 To ﬁnd the head as well , we now have to search for the peak that has the largest ( index - ) distance to the tail - peak . This is a periodic distance , too , meaning that 𝑁 is one of the closest neighbours of 0 . 1889 1890 1891 The entire outline array is then rotated , so that the head is always the ﬁrst point in it . Both indexes are saved . 1892 1893 Calculating the center - line 1894 A center - line , for a given outline , can be calculated by starting out at the head and walking in both directions from there – always trying to ﬁnd a pair of points with minimal distance to each other on both sides . Two indices are used : 𝑙 , 𝑟 for left and right . We also allow some " wiggle - room " for the algorithm to ﬁnd the best - matching points on each side . This is limited by a maximum oﬀset of 𝜔 points which is set to 0 . 025 ∗ 𝑁 by default , where 𝑁 is the number of points in the outline . 𝐟 ( 𝑖 ) gives the point on in outline at position 𝑖 . 1895 1896 1897 1898 1899 1900 Starting from 𝑙 ∶ = −1 , 𝑟 ∶ = 1 we continue while 𝑟 < 𝑙 + 𝑁 : 1901 1 . Find 𝑚 ∶ = argmin 𝑖 { ‖ 𝐟 ( 𝑟 + 𝑖 ) − 𝐟 ( 𝑙 ) ‖ ; ∀ 𝑖 ≤ 𝜔 ∧ 𝑟 + 𝑖 < 𝑁 } . If no valid 𝑚 can be found , abort . Otherwise set 𝑟 ∶ = 𝑚 . 1902 1903 57 of 69 Manuscript submitted to eLife Appendix 5 Figure A1 . The original image is displayed on the left . Each square represents one pixel . The processed image on the right is overlaid with lines of diﬀerent colors , each representing one connected component detected by our outline estimation algorithm . Dots in the centers of pixels are per - pixel - identities returned by OpenCVs findContours function ( for reference ) coded in the same colors as ours . Contours calculated by OpenCVs algorithm can not be used to estimate the one - pixel - wide " tail " of the 9 - like shape seen here , since it becomes a 1D line without sub - pixel accuracy . Our algorithm also detects diagonal lines of pixels , which would otherwise be an aliased line when scaled up . 2 . Find 𝑘 ∶ = argmin 𝑖 { ‖ 𝐟 ( 𝑙 − 𝑖 + 𝑁 ) − 𝐟 ( 𝑟 ) ‖ ; ∀ 𝑖 ≤ 𝜔 ∧ 𝑙 − 𝑖 ≤ − 𝑁 } . If no valid 𝑘 can be found , abort . Otherwise set 𝑙 ∶ = 𝑘 . 1904 1905 3 . Our segment now consists of points 𝐟 ( 𝑚 ) and 𝐟 ( 𝑘 ) , with a center vector of ( 𝐟 ( 𝑘 ) − 𝐟 ( 𝑚 ) ) ∗ 0 . 5 + 𝐟 ( 𝑚 ) . Push it to the center - line array . We can also calculate the width of the body at that point using ‖ 𝐟 ( 𝑘 ) − 𝐟 ( 𝑚 ) ‖ . 1906 1907 1908 4 . Set 𝑙 ∶ = 𝑙 − 1 . 1909 5 . Set 𝑟 ∶ = 𝑟 + 1 . 1910 Head and tail positions can be switched now , e . g . for animals where the wider part is the head . We may also want to start at the slimmest peak ﬁrst , which ever that is , since there we have not as much space for ﬂoating - point errors regarding where exactly the peak was . These options depend on the speciﬁc settings used in each video . 1911 1912 1913 1914 The angle of the center - line is calculated using atan2 for a vector between the ﬁrst point and one point at an oﬀset from it . The speciﬁc oﬀset is determined by a midline stiffness parameter , which oﬀers some additional stability – despite e . g . potentially noisy peak de - tection . 1915 1916 1917 1918 58 of 69 Manuscript submitted to eLife Appendix 6 1919 Visual ﬁeld estimation 1920 Visual ﬁelds are calculated by casting rays and intersecting them with other individuals and the focal individual ( for self - occlusion ) . An example of this can be seen in Figure 3 . The following procedure requires posture for all individuals in a frame . In case an individual does not have a valid posture in the given frame , its most recent posture and position are used as an approximation . The ﬁeld is internally represented as a discretized vector of multi - dimensional pixel values . Depending on the resolution parameter ( 𝐹 res ) , which sets the number of pixels , each index in the array represents step - sizes of ( 𝐹 max − 𝐹 min ) ∕ 𝐹 res radians . The 𝐹 values are constants setting the minimum and maximum ﬁeld of view ( −130 ◦ to 130 ◦ by default , which gives a range of 260 ◦ ) . Each pixel consists of multiple data - streams : The distance to the other individual , the identity of the other individual and the body - part that the ray intersected with . 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 Eyes are simulated to be located on the outline of the focal individual , near the head . The distance to the head can be set by the user as a percentage of midline - length . To ﬁnd the exact eye position , the program calculates intersections between vectors going left / right from that midline point , perpendicular to the midline , and the individual’s outline . In order to be able to simulate diﬀerent types of binocular and monocular sight , a parameter for eye separation 𝐸 sep ( radians ) controls the oﬀset from the head angle 𝐻 𝛼 per eye . Left and right eye are looking in directions 𝐻 𝛼 − 𝐸 sep and 𝐻 𝛼 + 𝐸 sep , respectively . 1932 1933 1934 1935 1936 1937 1938 We iterate through all available postures in a given frame and use a procedure which is very similar to depth - maps ( Williams ( 1978 ) ) in e . g . OpenGL . In the case of 2D visual ﬁelds , this depth - map is 1D . Each pixel holds a ﬂoating - point value ( initialized to ∞ ) which is con - tinuously compared to new samples for the same position – if the new sample is closer to the " camera " than the reference value , the reference value is replaced . This way , after all samples have been evaluated , we generate a map of the objects closest to the " camera " ( in this case the eye of the focal individual ) . For that to work we also have to keep the identity in each of these discrete slots maintained . So each time a depth value is replaced , the same goes for all the other data - streams ( such as identity and head - position ) . When an existing value is replaced , values in deeper layers of occlusion are pushed downwards alongside the old value for the ﬁrst layer . 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 Position of the intersecting object’s top - left corner is located at ̂𝑃 . Let 𝐸 𝑒 be the position of each eye , relative to ̂𝑃 . For each point 𝑃 𝑗 ( coordinates relative to ̂𝑃 ) of the outline , check the distance between 𝐸 𝑒 and the outline segments ( 𝑃 𝑗 − 𝑃 𝑗 −1 ) . For each eye 𝐸 𝑒 : 1950 1951 1952 1 . Project angles ranging from [ atan2 ( 𝑃 𝑗 −1 + 𝐸 𝑒 ) , atan2 ( 𝑃 𝑗 + 𝐸 𝑒 ) ] , where 𝛼 𝑒 is the eye orien - tation , using : Γ 𝑒 ( 𝛽 ) = angle _ normalize ( 𝛽 − 𝛼 𝑒 − 𝐹 min ) ∕ ( 𝐹 max − 𝐹 min ) ∗ 𝐹 res angle _ normalize ( 𝛽 ) normalizes beta to be between [ − 𝜋 , 𝜋 ] . 1953 1954 1955 1956 1957 1958 2 . if either max ( 𝑅 ) or min ( 𝑅 ) is inside the visual ﬁeld ( 0 ≤ Γ 𝑒 ( 𝛽 ) ≤ 1 ) : 1959 ( a ) We call the ﬁrst angle satisfying the condition 𝛽 . 1960 ( b ) Then the search range becomes 𝑅 ∶ = [ ⌊ max { 𝛽 − 0 . 5 ; 0 } ⌉ , ⌊ 𝛽 + 0 . 5 ⌉ ] , where the ele - ments in 𝑅 are integers . 1961 1962 ( c ) Let 𝛿 𝑗 , 𝑒 = ‖ ‖‖ 𝑃 𝑗 −1 − 𝐸 𝑒 ‖ ‖‖ , the distance between outline point at 𝑗 − 1 and the eye ( interpolation could be done here ) . 1963 1964 ( d ) Let index 𝑘 ∈ ℕ , 𝑘 ∈ 𝑅 be our index into the ﬁrst layer of the depth - map depth 0 : 1965 59 of 69 Manuscript submitted to eLife ( e ) if depth 0 ( 𝑘 ) > 𝛿 𝑗 , 𝑒 : Calculate all properties 𝐷 0 ( 𝑘 ) ∶ = { head _ distance , ⋯ ∈ data _ streams } 𝑇 , and push values at 𝑘 in layer 0 to layer 1 . 1966 1967 ( f ) otherwise , if depth 1 ( 𝑘 ) > 𝛿 𝑗 , 𝑒 , calculate properties for layer 1 instead and move data from layer 1 further down , etc . 1968 1969 The data - streams are calculated individually with the following equations : 1970 • Distance : Given already in depth 𝑖 ( 𝑘 ) . In practice , values are cut oﬀ at the maximum distance ( size of the video squared ) and normalized to [ 0 , 255 ] . 1971 1972 • Identity : Is assigned alongside depth 𝑖 ( 𝑘 ) for each element that successfully replacing another in the map . 1973 1974 • Body - part : Let 𝑇 𝑖 = tail index , 𝐿 𝑙 ∕ 𝑟 = number of points in left / right side of the outline ( given by tail - and head - indexes ) : 1975 1976 1 . if 𝑖 > 𝑇 𝑖 : head _ distance = 1 − | 𝑖 − 𝑇 𝑖 | ∕ 𝐿 𝑙 1977 2 . else : head _ distance = 1 − | 𝑖 − 𝑇 𝑖 | ∕ 𝐿 𝑟 1978 60 of 69 Manuscript submitted to eLife Appendix 7 1979 The PV ﬁle format 1980 Since we are using a custom ﬁle format to save videos recorded using TGrabs ( MP4 video can be saved alongside PV for a limited time or frame - rate ) , the following is a short overview of PV6 contents and structure . This description is purely technical and concise . It is mainly intended for users who wish to implement a loader for the ﬁle format ( e . g . in Python ) or are curious . 1981 1982 1983 1984 1985 Structure 1986 Generally , the ﬁle is built as a header ( containing meta information on the video ) followed by a long data section and an index table plus a settings string at the end . The header at the start of the ﬁle can be read as follows : 1987 1988 1989 1 . version ( string ) : " PV6 " 1990 2 . channels ( uint8 ) : Hard - coded to 1 1991 3 . width & height ( uint16 ) : Video size 1992 4 . crop oﬀsets ( 4x uint16 ) : Oﬀsets from original image 1993 5 . size of HorizontalLine struct ( uchar ) 1994 6 . # frames ( uint32 ) 1995 7 . index oﬀset ( uint64 ) : Byte oﬀset pointing to the index table for 1996 8 . timestamp ( uint64 ) : time since 1970 in microseconds of recording ( or conversion time if unavailable ) 1997 1998 9 . empty string 1999 10 . background image ( byte * ) : An array of uint8 values of size width * height * channels . 2000 11 . mask image size ( uint64 ) : 0 if no mask image was used , otherwise size in bytes fol - lowed by a byte * array of that size 2001 2002 Followed by the data section , where information is saved per frame . This information can either be in a zip - compressed format , or raw ( determined by size ) , see below : 2003 2004 1 . compression ﬂag ( uint8 ) : 1 if compression was used , 0 otherwise 2005 2 . if compressed : 2006 ( a ) original size ( uint32 ) 2007 ( b ) compressed size ( uint32 ) 2008 ( c ) lzo1x compressed data ( byte * ) in the format of the uncompressed variant ( below ) 2009 3 . if uncompressed : 2010 ( a ) timestamp since start time in header ( uint32 ) 2011 ( b ) number of images in frame ( uint16 ) 2012 ( c ) for each image in frame : 2013 i . number of HorizontalLines ( uint16 ) 2014 ii . data of HorizontalLine ( byte * ) 2015 iii . pixel data for each pixel in the previous array ( byte * ) 2016 Files are concluded by the index table , which gives a byte oﬀset for each video frame in the ﬁle , and a settings string . This index is used for quick frame skipping in TRex as well as random access . It consists of exactly one uint64 index per video frame ( as determined by the number of video frames read earlier ) . After that map ends , a string follows , which contains a JSON style string of all metadata associated by the user ( or program ) with the video ( such as species or size of the tank ) . 2017 2018 2019 2020 2021 2022 61 of 69 Manuscript submitted to eLife Appendix 8 2023 Automatic visual recognition 2024 Network layout and training procedure 2025 Network layout is sketched in Figure 1c . Using version 2 . 2 . 4 of Keras a , weights of densely con - nected layers as well as convolutional layers are initialized using Xavier - initialization ( Glorot and Bengio ( 2010 ) ) . Biases are used and initialized to 0 . The default image size in TRex is 80×80 , but can be changed to any size in order to retain more detail or improve computation speed . 2026 2027 2028 2029 2030 During training , we use the Adam optimizer ( Kingma and Ba ( 2015 ) ) to traverse the loss landscape , which is generated by categorical focal loss . Categorical focal loss is an adapta - tion of the original binary focal loss ( Lin et al . ( 2020 ) ) for multiple classes : 2031 2032 2033 cFL ( 𝑗 ) = 𝑁 ∑ 𝑐 = 1 − 𝛼 ( 1 − 𝐏 𝑗𝑐 ) 𝛾 𝐕 𝑗𝑐 log ( 𝐏 𝑗𝑐 ) , 2034 2035 2036 2037 where 𝐏 𝑗𝑐 is the prediction vector component returned by the network for class 𝑐 in image 𝑗 . 𝐕 is a set of validation images , which remains the same throughout the training process . It comprises 25 % of the images available per individual . Images are marked globally when becoming part of the validation dataset and are not used for training in the current or any of the following steps . 2038 2039 2040 2041 2042 After each epoch , predictions are generated by performing a forward - pass through the network layers . Returned are the softmax - activations 𝐏 𝑗𝑐 of the last layer for each image 𝑗 in the validation dataset . Simply calculating the mean of 2043 2044 2045 𝐴 = 1 𝑀 ∑ 𝑗 ∈ [ 0 , 𝑀 ] ⎧⎪⎨⎪⎩ 1 if 𝐏 𝑗 = 𝐕 𝑗 0 otherwise , 2046 2047 2048 2049 gives the mean accuracy of the network . 𝑀 is the number of images in the validation dataset , where 𝐕 𝑗 are the expected probability vectors per image 𝑗 . However , much more informative is the per - class ( per - identity ) accuracy of the network among the set of images 𝑖 belonging to class 𝑐 , which is 2050 2051 2052 2053 𝐼 𝑐 = { 𝑗 ; where 𝐕 𝑗𝑐 = 1 , 𝑗 ∈ [ 0 , 𝑀 ] } , 2054 2055 2056 2057 given that all vectors in 𝑉 are one - hot vectors – meaning the vector has length 𝑁 with 𝐕 𝑗𝜙 = 0 ∀ 𝜙 ≠ 𝑐 and 𝐕 𝑗𝑐 = 1 . 2058 2059 𝐴 𝑐 = 1 | 𝐼 𝑐 | ∑ 𝑗 ∈ 𝐼 𝑐 ⎧⎪⎨⎪⎩ 1 if 𝐏 𝑗 = 𝐕 𝑗 0 otherwise 2060 2061 2062 2063 Another constant , across training units – not just across epochs , is the set of images used to calculate mean uniqueness ̄𝑈 ( see Box 1 , as well as Guiding the Training Process ) . Values generated in each epoch 𝑡 of every training unit are kept in memory and used to calculate their derivative ̄𝑈 ′ ( 𝑡 ) . 2064 2065 2066 2067 Stopping - criteria 2068 A training unit can be interrupted if one of the following conditions becomes true : 2069 62 of 69 Manuscript submitted to eLife 1 . Training commenced for at least 𝑡 = 5 epochs , but uniqueness value ̄𝑈 was never above 2070 2071 ̄𝑈 2best > ̄𝑈 ( 𝑡 ) ∀ 𝑡 2072 2073 2074 2075 where ̄𝑈 best is the best mean uniqueness currently achieved by any training unit ( ini - tialized with zero ) . This prevents to train on faulty segments after a ﬁrst successful epoch . 2076 2077 2078 2 . The worst accuracy value per class has been " good enough " in the last three epochs : min 𝑐 ∈ [ 0 , 𝑁 ] { 𝐴 𝑐 } ≥ 0 . 97 2079 2080 2081 2082 3 . The global uniqueness value has been plateauing for more than 10 epochs . ∑ 𝑘 ∈ [ 𝑡 −10 , 𝑡 ] ̄𝑈 ′ ( 𝑘 ) ≤ 0 . 01 2083 2084 2085 2086 4 . Overﬁtting : Change in loss is very low on average after more than 5 epochs . Mean loss is calculated as follows : 2087 2088 cFL 𝜇 ( 𝑡 ) = 1 5 ∑ 𝑘 ∈ [ 𝑡 −6 , 𝑡 −1 ] cFL ( 𝑘 ) 2089 2090 2091 2092 Now if the diﬀerence between the current loss and the previous loss is below a thresh - old : 2093 2094 𝜆 ( 𝑡 ) = ⌊ ln ( cFL ( 𝑡 ) ) ⌉ − 1 1 5 ∑ 𝑘 ∈ [ 𝑡 −5 , 𝑡 ] max { 𝜖 ; | | | cFL ( 𝑘 ) − cFL 𝜇 ( 𝑘 ) | | | } < 0 . 05 ∗ 10 𝜆 ( 𝑡 ) 2095 2096 2097 2098 2099 2100 2101 5 . Maximum number of epochs has been reached . User - deﬁned option limiting the amount of time that training can take per unit . By default this limit is set to 150 epochs . 2102 2103 6 . Loss is zero . No further improvements are possible within the current training unit , so we terminate and continue with the next . 2104 2105 A high per - class accuracy over multiple consecutive epochs is usually an indication that everything that can be learned from the given data has already been learned . No further improvements should be expected from this point , unless the training data is extended by adding samples from a diﬀerent part of the video . The same applies to scenarios with consistently zero or very low change in loss . Even if improvements are still possible , they are more likely to happen during the ﬁnal ( overﬁtting ) step where all of the data is combined . 2106 2107 2108 2109 2110 2111 a See keras . io documentation for default arguments 63 of 69 Manuscript submitted to eLife Appendix 9 2112 Data used in this paper and reproducibility 2113 All of the data , as well as the ﬁgures and tables showing the data , have been generated automatically . We provide the scripts that have been used , as well as the videos if requested . " Data " refers to converted video - ﬁles , as well as log - and NPZ - ﬁles . Analysis has been done in Python notebooks , using mostly matplotlib and pandas , as well as numpy to load the data . Since TRex and TGrabs , as well as idtracker . ai have been run on a Linux system , we were able to run everything from two separate bash ﬁles : 2114 2115 2116 2117 2118 2119 1 . run . bash 2120 2 . run _ idtracker . bash 2121 Where ( 1 ) encompasses all trials run using TRex and TGrabs , both for the speed - and recognition - datasets . ( 2 ) runs idtracker . ai in its own dedicated Python environment , using only the recognition - dataset . The parameters we picked for idtracker . ai vary between videos and are hand - crafted , saved in individual . json ﬁles ( see Table A1 for a list of settings used ) . We ran multiple trials for each combination of tools and data with 𝑁 = 5 where necessary : 2122 2123 2124 2125 2126 2127 • 3x TGrabs [ speed - dataset ] 2128 • 5x TRex + recognition [ recognition - dataset ] 2129 • 3x idtracker . ai [ recognition - dataset ] 2130 • TRex without recognition enabled [ speed - dataset ] : 2131 – 3x for testing the tree - based , approximate and Hungarian methods ( Tracking ) , without posture enabled – testing raw speeds ( see Table A3 ) 2132 2133 – 3x testing accuracy of basic tracking ( see Table A2 ) , with posture enabled 2134 A Python script used for Figure 7 , which is run only once . It generates a series of results for the same video ( Video 7 with 100 individuals ) with diﬀerent sample - sizes . It uses a single set of training samples and then – after equalizing the numbers of images per individual – generates multiple virtual subsets with fewer images . They span 15 diﬀerent sample - sizes per individual , saving a history of accuracies for each run . We repeated the same procedure with for the diﬀerent normalization methods ( no normalization , moments and posture ) , each repeated ﬁve times . 2135 2136 2137 2138 2139 2140 2141 As described in the main text , we recorded memory usage with an external tool ( syrupy ) and used it to measure both software solutions . This tool saves a log - ﬁle for each run , which is appropriately renamed and stored alongside the other ﬁles of that trial . 2142 2143 2144 All runs of TRex are preceded by running a series of TGrabs commands ﬁrst , in order to convert the videos in the datasets . We chose to keep these trials separately and load whenever possible , to avoid data - duplication . Since subsequent results of TGrabs are always identical ( with the exception of timings ) , we only keep one version of the PV ﬁles ( The PV ﬁle format ) as well as only one version of the results ﬁles generated using live - tracking . However , multiple runs of TGrabs were recorded in the form of log - ﬁles to get a measure of variance between runs in terms of speed and memory . 2145 2146 2147 2148 2149 2150 2151 Human validation 2152 To ensure that results from the automatic evaluation ( in Visual Identiﬁcation : Accuracy ) are plausible , we manually reviewed part of the data . Speciﬁcally , the table in 3 shows an overview of the individual events reviewed and percentages of wrongly assigned frames . 64 of 69 Manuscript submitted to eLife Appendix 9 Table A1 . Settings used for idtracker . ai trials , as saved inside the . json ﬁles used for tracking . The minimum intensity was always set to 0 and background subtraction was always enabled . An ROI is an area of interest in the form of an array of 2D vectors , typically a convex polygon containing the area of the tank ( e . g . for ﬁsh or locusts ) . Since this format is quite lengthy , we only indicate here whether we limited the area of interest or not . video length ( # frames ) nblobs area max . intensity roi 7 1921 100 [ 165 , 1500 ] 170 Yes 8 30626 59 [ 100 , 2500 ] 160 Yes 11 19539 10 [ 200 , 1500 ] 10 Yes 13 19317 10 [ 200 , 1500 ] 10 Yes 12 19309 10 [ 200 , 1500 ] 10 Yes 9 90001 8 [ 190 , 4000 ] 147 Yes 16 416259 8 [ 200 , 2500 ] 50 No 14 351677 8 [ 200 , 2500 ] 50 No 15 108000 8 [ 250 , 2500 ] 10 No Due to the length of videos and the numbers of individuals inside the videos we did not re - view all videos in their entirety , as shown in the table . Using the reviewing tools integrated in TRex , we focused on crossings that were automatically detected . These tools allow the user to jump directly to points in the video that it deems problematic . Detecting problematic situations is equivalent to detecting the end of individual segments ( see Automatic Visual Identiﬁcation Based on Machine Learning ) . While iterating through these situations , we corrected individuals that have been assigned to the wrong object , generating a clean and corrected baseline dataset . We assumed that an assignment is correct , as long as the indi - vidual is at least part of the object that the identity has been assigned to . Misassignments were typically ﬁxed after a few frames . Identities always returned to the correct individuals afterward ( thus not causing a chain of follow - up errors ) . 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 Comparison between trajectories from diﬀerent softwares , or multiple runs of the same software 2167 2168 In our tests , the same individuals may have been given diﬀerent IDs ( or " names " ) by each software ( and in each run of each software for the same video ) , so , as a ﬁrst step in every test where this was relevant , we had to determine the optimal pairing between identities of two datasets we wished to compare . This was done using a square distance matrix contain - ing overall euclidean distances between identities is calculated by summing their per - frame distances . Optimally this number would be zero for one and greater than zero for every other pairing , but temporary tracking mistakes and diﬀerences in the calculation of cen - troids may introduce noise . Thus , we solved the matching problem ( see Matching an object to an object in the next frame ) for identities between each two datasets and paired individ - uals with the smallest accumulative distance between them . This was done for all results presented , where a direct comparison between two datasets was required . 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 65 of 69 Manuscript submitted to eLife Appendix 10 2180 Matching probabilities 2181 One of the most important steps , when matching objects in one frame with objects in the next frame , is to calculate a numerical landscape that can then be traversed by a maximiza - tion algorithm to ﬁnd the optimal combination . This landscape , which can be expressed as an 𝑚 × 𝑛 matrix 𝐏 ( 𝑡 ) , contains the probability values between [ 0 , 1 ] for each assignment between individuals 𝑖 and objects 𝐵 𝑗 . 2182 2183 2184 2185 2186 Below are deﬁnitions used in the following text : 2187 • 𝑇 Δ is the typical time between frames ( s ) , which depends on the video 2188 • 𝜏 𝑖 < 𝑡 is most recent frame assigned to individual 𝑖 previous to the current frame 𝑡 2189 • 𝑃 min is the minimally allowed probability for the matching algorithm , underneath which the probabilities are assumed to be zero ( and respective combination of object and individual is ignored ) . This value is set to 0 . 1 by default . 2190 2191 2192 • 𝐹 ( 𝑡 ∈ ℝ ) → ℕ is the frame number associated with the time 𝑡 ( s ) 2193 •  ( 𝑓 ∈ ℕ ) → ℝ is the time in seconds of frame 𝑓 , with 𝐹 (  ( 𝑓 ) ) = 𝑓 2194 • 𝐱 indicates that 𝑥 is a vector 2195 • 𝐔 ( 𝐱 ) = 𝐱 ∕ ‖ 𝐱 ‖ 2196 Some values necessary for the following calculations are independent of the objects in the current frame and merely depend on data from previous frames . They can be re - used per frame and individual in the spirit of dynamic programming , reducing computational complexity in later steps : 2197 2198 2199 2200 𝐯 𝑖 ( 𝑡 ) = 𝐩 ′ 𝑖 ( 𝑡 ) = 𝛿 𝛿𝑡 𝐩 𝑖 ( 𝑡 ) ̂ 𝐯 𝑖 ( 𝑡 ) = 𝐯 𝑖 ( 𝑡 ) ∗ ⎧⎪⎨⎪⎩ 1 if ‖‖ 𝐯 𝑖 ( 𝑡 ) ‖‖ ≤ 𝐷 max 𝐷 max ∕ ‖‖ 𝐯 𝑖 ( 𝑡 ) ‖‖ otherwise 𝐚 𝑖 ( 𝑡 ) = 𝛿 𝛿𝑡 ̂ 𝐯 𝑖 ( 𝑡 ) 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 Velocity 𝐯 𝑖 ( 𝑡 ) and acceleration 𝐚 𝑖 ( 𝑡 ) are simply the ﬁrst and second derivatives of the indi - viduals position at time 𝑡 . ̂ 𝐯 𝑖 ( 𝑡 ) is almost the same as the raw velocity , but its length is limited to the maximally allowed travel distance per second ( 𝐷 max , parameter track _ max _ speed ) . 2211 2212 2213 These are then further processed , combining and smoothing across values of multiple previous frames ( the last 5 valid ones ) . Here , ̄ 𝑓 ( 𝑥 ) indicates that the resulting value uses data from multiple frames . 2214 2215 2216 ̄𝑠 𝑖 ( 𝑡 ) = median 𝑘 ∈ [ 𝐹 ( 𝜏 ) −5 , 𝐹 ( 𝑡 ) ] ‖‖ ̂ 𝐯 𝑖 (  ( 𝑘 ) ) ‖‖ 2217 2218 2219 2220 is the speed at which the individual has travelled at recently . The mean direction of movement is expressed as 2221 2222 ̄ 𝐝 𝑖 ( 𝑡 ) = 1 𝐹 ( 𝑡 ) − 𝐹 ( 𝜏 ) + 5 ∑ 𝑘 ∈ [ 𝐹 ( 𝜏 ) −5 , 𝐹 ( 𝑡 ) ] ̂ 𝐯 𝑖 (  ( 𝑘 ) ) 2223 2224 2225 2226 with the corresponding direction of acceleration 2227 ̄ 𝐚 𝑖 ( 𝑡 ) = 𝐔 ( 1 𝐹 ( 𝑡 ) − 𝐹 ( 𝜏 ) + 5 ∑ 𝑘 ∈ [ 𝐹 ( 𝜏 ) −5 , 𝐹 ( 𝑡 ) ] 𝐚 𝑖 (  ( 𝑘 ) ) ) . 2228 2229 2230 2231 66 of 69 Manuscript submitted to eLife The predicted position for individual 𝑖 at time 𝑡 is calculated as follows : 2232 ̇ 𝐩 𝑖 ( 𝑡 ) = 𝑠 𝑖 ( 𝑡 ) ∑ 𝑘 ∈ [ 𝐹 ( 𝜏 𝑖 ) , 𝐹 ( 𝑡 ) −1 ] 𝑤 ( 𝑘 ) ( ̄ 𝐝 𝑖 ( 𝑡 ) +  ′ ( 𝑘 ) ∗ ̄ 𝐚 𝑖 ( 𝑡 ) ) , 2233 2234 2235 2236 with weights for each considered time - step of 2237 𝑤 ( 𝑓 ) = 1 + 𝜆 4 1 + 𝜆 4 max { 1 , 𝑓 − 𝐹 ( 𝜏 𝑖 ) + 1 } , 2238 2239 2240 2241 where 𝜆 ∈ [ 0 , 1 ] is a decay rate ( parameter track _ speed _ decay ) at which the impact of previous positions on the predicted position decreases with distance in time . With its value approaching 1 , the resulting curve becomes steeper - giving less weight previous positions the farther away they are from the focal frame . 2242 2243 2244 2245 In order to locate an individual 𝑖 in the current frame 𝐹 ( 𝑡 ) , a probability is calculated for each object 𝐵 𝑗 found in the current frame resulting in the matrix : 2246 2247 𝐏 ( 𝑡 ) = ⎡⎢⎢ ⎢⎣ 𝑃 0 ( 𝑡 | 𝐵 0 ) … 𝑃 𝑛 ( 𝑡 | 𝐵 0 ) ⋮ ⋱ ⋮ 𝑃 0 ( 𝑡 | 𝐵 𝑚 ) … 𝑃 𝑛 ( 𝑡 | 𝐵 𝑚 ) ⎤⎥⎥ ⎥⎦ . ( 3 ) 2248 2249 2250 2251 Probabilities 𝑃 𝑖 ( 𝑡 | 𝐵 𝑗 ) for all potential connections between blobs 𝐵 𝑗 and identities 𝑖 at time 𝑡 are calculated by ﬁrst predicting the expected position ̇ 𝐩 𝑖 ( 𝑡 ) for each individual in the current frame 𝐹 ( 𝑡 ) . This allows the program to focus on a small region of where the individual is expected to be located , instead of having to search the whole arena each time . 2252 2253 2254 2255 Based on the individual’s recent speed ̄𝑠 𝑖 ( 𝑡 ) , direction ̄ 𝐝 𝑖 ( 𝑡 ) , acceleration ̄ 𝐚 𝑖 ( 𝑡 ) and angular momentum ̄𝛼 ′ 𝑖 ( 𝑡 ) , the individual’s projected position ̇ 𝐩 𝑖 ( 𝑡 ) is usually not far away from its last seen location for small time - steps . Only when Δ 𝑡 increases , if the individual has been lost for more than one frame or frame - rates are low , does it really play a role . 2256 2257 2258 2259 The actual probability values in 𝐏 ( 𝑡 ) are then calculated by combining three metrics - each describing diﬀerent aspects of potential concatenation of object 𝑏 at time 𝑡 to the already existing track for individual 𝑖 : 2260 2261 2262 The time metric 𝑇 𝑖 ( 𝑡 ) , which does not depend on the blob the individual is trying to be matched to . It merely reﬂects the recency of the individuals last occurence in a way that recently seen individual will always be preferred over individuals that have been lost for longer . 𝐹 min = min { 1 𝑇 Δ , 5 } 𝑅 𝑖 ( 𝑡 ) = ‖‖‖ ‖ {  ( 𝑘 ) | 𝐹 ( 𝑡 ) − 𝑇 −1Δ ≤ 𝑘 ≤ 𝑡 ∧  ( 𝑘 ) −  ( 𝑘 − 1 ) ≤ 𝑇 max } ‖‖‖ ‖ 𝑇 𝑖 ( 𝑡 ) = ( 1 − min { 1 , max { 0 , 𝜏 𝑖 − 𝑡 − 𝑇 Δ } 𝑇 max } ) ∗ ⎧⎪⎨⎪⎩ min { 1 , 𝑅 𝑖 ( 𝜏 𝑖 ) −1 𝐹 min + 𝑃 min } 𝐹 ( 𝜏 𝑖 ) ≥ 𝐹 ( 𝑡 0 ) + 𝐹 min 1 otherwise ( 4 ) 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 𝑆 𝑖 ( 𝑡 | 𝐵 𝑗 ) is the speed that it would take to travel from the individuals position to the blobs position in the given time ( which might be longer than one frame ) , inverted and normalized to a value between 0 and 1 . 𝑆 𝑖 ( 𝑡 | 𝐵 𝑗 ) = ⎛ ⎜⎜⎜⎝ 1 + ‖ ‖‖‖ ( 𝐩 𝐵 𝑗 ( 𝑡 ) − ̇ 𝐩 𝑖 ( 𝑡 ) ) ∕ ( 𝜏 𝑖 − 𝑡 ) ‖ ‖‖‖ 𝐷 max ⎞ ⎟⎟⎟⎠ −2 ( 5 ) 2277 2278 2279 2280 2281 2282 67 of 69 Manuscript submitted to eLife And the angular diﬀerence metric 𝐴 𝑖 ( 𝑡 , 𝜏 𝑖 | 𝐵 𝑗 ) , describing how close in angle the resulting vector of connecting blob and individual to a track would be to the previous direction vector : 𝐚 = ̇ 𝐩 𝑖 ( 𝑡 ) − 𝐩 𝑖 ( 𝜏 𝑖 ) 𝐛 = 𝐩 𝐵 𝑗 ( 𝑡 ) − 𝐩 𝑖 ( 𝜏 𝑖 ) 2283 2284 2285 2286 2287 2288 2289 2290 𝐴 𝑖 ( 𝑡 , 𝜏 𝑖 | 𝐵 𝑗 ) = ⎧⎪⎨⎪⎩ 1 − 1 𝜋 | atan2 { ‖ 𝐚 × 𝐛 ‖ , 𝐚 ⋅ 𝐛 } | if ‖ 𝐚 ‖ > 1 ∧ ‖ 𝐛 ‖ > 1 1 otherwise ( 6 ) 2291 2292 2293 2294 The conditional ensures that the individual travelled a long enough distance , as the atan2 function used to determine angular diﬀerence here lacks numerical precision for very small magnitudes . This is , however , an unproblematic case in this situation as the positions are in pixel - coordinates and anything below a movement of one pixel is likely to be due to noise anyway . 2295 2296 2297 2298 2299 Combining ( 4 ) , ( 5 ) and ( 6 ) into a weighted probability product yields : 𝑃 𝑖 ( 𝑡 , 𝜏 𝑖 | 𝐵 𝑗 ) = 𝑆 𝑖 ( 𝑡 | 𝐵 𝑗 ) ∗ ( 1 − 𝜔 1 ( 1 + 𝐴 𝑖 ( 𝑡 , 𝜏 𝑖 | 𝐵 𝑗 ) ) ) ∗ ( 1 − 𝜔 2 ( 1 + 𝑇 𝑖 ( 𝑡 , 𝜏 𝑖 ) ) ) ( 7 ) 2300 2301 2302 2303 Results from equation ( 7 ) can now easily be used in a matching algorithm , in order to determine the best combination of objects and individuals as in Matching an object to an object in the next frame . 𝜔 1 is usually set to 0 . 1 , 𝜔 2 is set to 0 . 25 by default . 2304 2305 2306 68 of 69 Manuscript submitted to eLife Appendix 11 2307 Algorithm for splitting touching individuals 2308 Data : image of a blob , 𝑁 𝑒 number of expected blobs 2309 Result : 𝑁 ≥ 𝑁 𝑒 smaller image - segments , or error 2310 threshold = 𝑇 𝑐 ; 2311 while threshold < 255 do 2312 blobs = apply _ threshold ( image , threshold ) ; 2313 if ‖ blobs ‖ = 0 then 2314 break ; 2315 end 2316 if ‖ blobs ‖ ≥ 𝑁 𝑒 then 2317 sort blobs by size in decreasing fashion ; 2318 loop through all blobs 𝑖 up to 𝑖 ≤ 𝑁 𝑒 and detect whether the size - ratio between them is roughly even . until then , we keep iterating . ; 2319 2320 if min { ratio 𝑖 ∀ 𝑖 ∈ [ 0 , 𝑁 𝑒 ] } < 0 . 3 then 2321 threshold = threshold + 1 ; 2322 continue ; 2323 else 2324 return blobs ; 2325 end 2326 else 2327 threshold = threshold + 1 ; 2328 end 2329 end 2330 return fail ; 2331 Algorithm 2 : The algorithm used whenever two individuals touch , which is detected by a history - based method . This history - based method also provides 𝑁 𝑒 , the number of expected objects within the current ( big ) object . 𝑇 𝑒 is the starting threshold constant parameter , as set by the user . 2332 69 of 69