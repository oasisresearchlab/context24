1 Computer says ‘no’ : Exploring systemic hiring bias in ChatGPT using an audit approach * Louis Lippens i Abstract Large language models offer significant potential for optimising professional activities , such as streamlining personnel selection procedures . However , concerns exist about these models perpetuating systemic biases embedded into their pre - training data . This study explores whether ChatGPT , a chatbot producing human - like responses to language tasks , displays ethnic or gender bias in job applicant screening . Using a correspondence audit approach , I simulated a CV screening task in which I instructed the chatbot to rate fictitious applicant profiles only differing in names , signalling ethnic and gender identity . Comparing ratings of Arab , Asian , Black American , Central African , Dutch , Eastern European , Hispanic , Turkish , and White American male and female applicants , I show that ethnic and gender identity influence ChatGPT’s evaluations . The ethnic bias appears to arise partly from the prompts’ language and partly from ethnic identity cues in applicants’ names . Although ChatGPT produces no overall gender bias , I find some evidence for a gender – ethnicity interaction effect . These findings underscore the importance of addressing systemic bias in language model - driven applications to ensure equitable treatment across demographic groups . Practitioners aspiring to adopt these tools should practice caution , given the adverse impact they can produce , especially when using them for selection decisions involving humans . Keywords : large language models , ChatGPT , systemic discrimination , adverse impact , correspondence audit JEL Classification : J15 , J16 , J71 , C93 * Acknowledgements . I thank Stijn Baert , Eva Derous , and Pieter - Paul Verhaeghe for their continued support during my PhD trajectory . I also acknowledge the helpful comments of Eva Zschirnt , Sorana Toma , Brecht Neyt , Bart Defloor , and Patrick Button on an earlier draft of this paper . i Corresponding author . Ghent University , Sint - Pietersplein 6 , 9000 Ghent , Belgium , and Vrije Universiteit Brussel . Louis . Lippens @ UGent . be . ORCID : 0000 - 0001 - 7840 - 2753 . 2 1 . Introduction The ( anticipated ) public release of several large language models ( LLMs ) , such as OpenAI’s Generative Pre - trained Transformer ( GPT ) or Google’s Pathways Language Model ( PaLM ) , recently sparked scholarly and public debate on their implications ( Nature , 2023 ; Teubner et al . , 2023 ; The Economist , 2023a , 2023b ; Thorp , 2023 ) . LLMs are generative Artificial Intelligence ( AI ) algorithms trained on large corpora of text data that can generate human - like responses to language tasks ( Teubner et al . , 2023 ) . These algorithms have the potential to impact labour productivity significantly , transforming how we work by automating a myriad of tasks ( Brynjolfsson et al . , 2023 ; Eloundou et al . , 2023 ; Felten et al . , 2023 ; Noy and Zhang , 2023 ; Teubner et al . , 2023 ) . For instance , LLMs can synthesise unstructured business data or make tax calculations based on user input and local tax policy documentation . LLMs could even replace substantial parts of human jobs , such as problem resolution in customer support , computer code debugging , or job applicant screening . Nevertheless , as these models continue to advance and become more deeply integrated into professional activities , it is vital to ensure that we use them responsibly and ethically , particularly when it comes to decision - making that directly impacts humans . Recent research suggests that AI technologies , including LLMs , can assist with human resources ( HR ) practices such as hiring and selection ( Vrontis et al . , 2021 ) . These technologies may help practitioners in ( i ) automating hiring decision - making , ( ii ) increasing the accuracy of decisions , ( iii ) reducing the time required to make them , or ( iv ) improving the objectivity of decisions by bypassing human prejudice ( Cooke et al . , 2019 ; van Esch et al . , 2019 ) . The idea of automating part of the personnel selection process is not new ; firms already use algorithms for hiring automation ( Noble et al . , 2021 ) . Because LLMs are ideally suited to perform text processing and analysis , it is conceivable they are used integrally or as part of a broader AI tool for applicant profile screening purposes . Recently , Pisanelli ( 2022 ) examined the success of automated resume screening through a simple language model in a field setting . They found that automated screening diminished the gender interview invitation gap vis - à - vis human recruiters ’ manual CV screening by almost two - thirds , providing evidence that algorithms may indeed counter human - caused prejudice . However , instead of reducing bias , LLMs might reproduce the systemic , historical biases 3 embedded in their pre - training data ( Caliskan et al . , 2017 ; Peres et al . , 2023 ; Rich & Gureckis , 2019 ; Schramowski et al . , 2022 ) . Examples include amplifying prejudice in human - created text or extrapolating the underrepresentation of vulnerable groups from the underlying data . Systemic discrimination induced by LLMs can further the negative cumulative impact of existing bias across domains and time , increasing group - based disparities ( Bohren et al . , 2022 ) . A recent example of research examining algorithmic racial discrimination is the study of Arnold et al . ( 2021 ) . They show that Black defendants received a considerably lower pretrial release rate than White defendants from an advanced AI algorithm despite identical pretrial misconduct potential . These observations raise concerns regarding the fairness and objectivity of AI - assisted selection activities and call for a deeper examination of these biases ( Tambe et al . , 2019 ) . I focus on ChatGPT , a chatbot actuated by OpenAI’s GPT - series models , and its ability to perform a curriculum vitae ( CV ) screening task without exhibiting bias in its responses . There is no public information about the specific data the GPT - series models are pre - trained on besides that they rely on a comprehensive data corpus encompassing websites , fora , job boards , and other ( online ) content ( Brown et al . , 2020 ; OpenAI , 2023a ; Teubner et al . , 2023 ) . Their text responses are fine - tuned based on specific task prompts and feedback provided by the user . ChatGPT might perpetuate and reinforce biases about specific demographic groups it has learned from its training data’s patterns , language , and concepts , resulting in discriminatory responses to the CV screening task . Hate speech in online fora ( Bliuc et al . , 2018 ; Castaño - Pulgarín et al . , 2021 ) or harmful stereotypes about minority groups in pre - existing job advertisements ( Koçak et al . , 2022 ; Wille and Derous , 2017 ) , for example , can taint the models’ training processes . The GPT - 3 model instance , in particular , has demonstrated anti - Muslim bias in word association tasks before , consistently linking Muslims with violence and terrorism ( Abid et al . , 2021 ) . The social sciences have a broad tradition of examining discrimination in hiring using correspondence audit studies with CVs ( Bartkoski et al . , 2018 ; Heath & Di Stasio , 2019 ; Lippens et al . , 2023b ; Quillian et al . , 2017 , 2019 ; Quillian & Lee , 2023 ; Quillian & Midtbøen , 2021 ; Thijssen et al . , 2021 ; Zschirnt & Ruedin , 2016 ) . The correspondence audit method allows for a causal estimation of discrimination ( Gaddis , 2018 ) . In labour market research , this approach compares callback or invitation rates from recruiters or employers to fictitious 4 job applicants who possess similar qualifications but differ in ascriptive characteristics , such as race or gender . For instance , to examine racial hiring discrimination , a common strategy involves submitting quasi - identical CVs of fictitious applicants to actual vacancies with differences in names indicating racial background . The most recent meta - analytic estimates , comprising worldwide experimental data on hiring discrimination , reveal that candidates signalling ethnic or racial minority group membership receive , on average , 29 % fewer positive responses from recruiters than their majority counterparts ( Lippens et al . , 2023b ) . Across the examined subgroups , positive response penalties are as high as 41 % ( for Arabs ) or as low as 8 % ( for Hispanics ) , averaged over correspondence audit studies . The current CV screening experiment with ChatGPT zooms in on ethnic and gender identity , given their status as the most researched and easily operationalisable discrimination grounds using name variations ( Lippens et al . , 2023b ) . Specifically , I consider the hiring chances of fictitious male and female Arab , Asian , Black American , Central African , Dutch , Eastern European , Hispanic , Turkish , and White American applicants . The task comprised prompting ChatGPT to assess the suitability of these fictitious job candidates , only differing by name signalling ethnic and gender identity , for a given vacancy using the information provided in their CVs . Aside from the name at the top of the CV and minor differences across CVs and vacancies to ensure a fitting vacancy – CV match , all input remained the same . Subsequently , I asked ChatGPT to output ratings for each candidate . I regarded differences in ratings as evidence of bias and , thus , discrimination . This study has three main contributions . First , transposing the correspondence audit method from the field experiment literature into a valid method for detecting bias or discrimination in LLMs bridges the gap between social science and AI research . Bias in LLMs has often been measured through word association tasks ( e . g . Abid et al . , 2021 ; Liu et al . , 2022 ) . For example , by prompting an LLM with the phrase ‘a democrat is [ male / female ] ’ and asking the LLM to fill in the gender . The current experiment applies a different but well - established method of audit research for measuring hiring discrimination through name association to bias in AI applications . Second , the approach is methodologically valuable . Potential bias is experimentally exposed in a real - world selection task using a broad range of ethnically identifiable and gendered names linked to actual CVs and vacancies instead of using a synthetic word association task . At the same time , the experiment benefits from the 5 scalability and automatability LLMs offer , which is a clear advantage over audit studies with human recruiters in the field . Specifically , similar candidate profiles could be repeatedly presented to the model in succession without worrying about spillover ( e . g . ‘Will presenting one candidate affect the bias against another candidate ? ’ ) or detection ( ‘Will it become clear that I am running an experiment ? ’ ) effects . Third , it significantly adds to mapping out bias in AI algorithms and quantifying how discrimination by language models can sustain pre - existing differences based on personal identity . 2 . Methods I submitted ChatGPT to a simulated CV screening task . To this end , I ( i ) collected text data from existing job vacancies and CV templates , ( ii ) created candidate profiles by supplementing these templates with sets of validated names specifically for experimental studies on ethnic and gender identity , and ( iii ) instructed the chatbot to assess these profiles , only differing by name , based on the requirements in the vacancies in combination with the information presented in the CVs . Given the experimental setup , the data were analysed using standard regression techniques . Below , I describe the data - gathering process , the experimental setup , and the estimation strategy to identify potential bias in CV screening by ChatGPT . 2 . 1 . Text data 2 . 1 . 1 Vacancies The website of the Flemish public employment agency VDAB in the Dutch - speaking part of Belgium ( https : / / www . vdab . be ) was the starting point for retrieving vacancy text data . A total of 1 , 920 vacancies , balanced by occupation and experience level , were selected for the experiment . I chose 23 occupations across different industries to obtain a representative set of vacancies requiring different skills and professional experience . These occupations ranged from clerical service sector employees ( e . g . administrative assistant , HR officer ) to IT 6 personnel ( e . g . IT analyst , IT project leader ) and logistics workers ( e . g . industrial logistics planner , trucker - trailer driver ) . The experience level took on three distinct values : no experience ( N = 690 ; 35 . 94 % ) , at least two years of experience ( N = 690 ; 35 . 94 % ) , and at least five years of experience ( N = 540 ; 28 . 13 % ) . As a general rule , I sampled 30 recent vacancies per occupation and experience level to obtain some critical mass , large enough to conduct analyses that assume normally distributed data . Five occupations requiring at least five years of experience ( e . g . seller of clothing accessories ) appeared too infrequent in the vacancy set and were considered non - representative ; these occupations were consequently excluded from the experiment . Table A1 in the appendix provides an overview of counts by occupation and experience level . Examples of the extracted vacancy text can be retrieved from Table A2 . The vacancies further varied regarding job type , shift system , work hours , language requirements , and location . The most prevalent job types were permanent positions ( N = 1 , 501 ; 78 . 18 % ) followed by interim or temporary positions ( N = 382 ; 19 . 90 % ) ; other job types encompassed independent activities , student positions , and flex jobs . The most common shift system was day work ( N = 1 , 823 ; 94 . 95 % ) , with other shift systems including two - and three - shift systems , night work , interrupted service , and continuous systems . Contract work hours were mainly full - time ( N = 1 , 860 ; 96 . 88 % ) ; part - time vacancies constituted only a tiny proportion ( N = 60 ; 3 . 12 % ) . The most frequently mentioned languages in the vacancies were Dutch ( N = 1 , 808 ; 94 , 17 % ) , French ( N = 783 ; 40 . 78 % ) , and English ( N = 760 ; 39 . 58 % ) , with varying levels of desired proficiency ( i . e . from ‘not at all’ to ‘very good’ ) . Count statistics of the various job and vacancy characteristics can be accessed from Tables A3 , A4 , and A5 in the appendix . 2 . 1 . 2 . CVs Each vacancy was paired with 18 CV profiles of fictitious job candidates possessing the educational background and professional experience required to perform the job adequately . Table A6 in the appendix shows representative examples of CV text . Like the vacancies , the CV text template was sourced from the Flemish public employment services website . The CV text contained standard information typically found in a resume , such as a residential address , e - mail address , phone number , birth date , nationality ( i . e . always 7 Belgian ) , vehicle ownership and driving ability , bachelor - level degree , general personal characteristics , and language and computer skills . This information remained identical across the 34 , 560 vacancy – CV combinations to keep the variation between CVs to a minimum and to isolate the effect of ethnic and gender identity . Between vacancies , the CVs varied by the specialisation and graduation year of the bachelor’s degree and the type and duration of work experience to guarantee a suitable vacancy – CV match . Within vacancies , the CVs only varied in ethnic and gender identity , with all other CV and application details identical . These characteristics were added using candidate names randomly matched to CVs based on assigned ethnic and gender identity . The distinct names signalled diverse ethnic identities , i . e . Arab , Asian , Black American , Central African , Dutch , Eastern European , Hispanic , Turkish , and White American , each accounting for 11 . 11 % of the candidate profiles , and two genders , i . e . female and male , each accounting for 50 % of the profiles . Here , Dutch refers to the language , i . e . spoken in Belgium and The Netherlands , amongst other countries , rather than the country of origin , i . e . The Netherlands . Names were drawn from five sources . The first series of Asian male , Black American , Hispanic , and White American names were acquired from the recently published dataset of Crabtree et al . ( 2023 ) . They compiled an extensive set of validated names for use in name experiments based on surveys conducted in the United States , accounting for confounding factors beyond intended race , such as socioeconomic status . Additional Hispanic female first names were taken from the name categorisation test of Gaddis ( 2017 ) to increase variety vis - à - vis the single Hispanic female first name in Crabtree et al . ( 2023 ) . Sets of validated Arab full names and Asian and White American female first names were sourced from Baert et al . ( 2022 ) , who conducted a similar name experiment . The latter set of names was again used to increase variety due to the limited number of Asian and White American female first names in Crabtree et al . ( 2023 ) . Dutch names were obtained from Lippens et al . ’s ( 2023a ) correspondence audit study and Martiniello and Verhaeghe’s ( 2022 ) name perception experiment . Finally , Central African , Eastern European , Turkish , and additional Arab names were sourced from Martiniello and Verhaeghe ( 2022 ) . 8 2 . 2 . Experiment 2 . 2 . 1 . Prompts The experiment consisted of sending prompts and receiving and logging model output . I repeated this process until all vacancy – CV combinations were presented to ChatGPT . The prompts in the experiment included instructions , the vacancy text , and the CV profile text . The only differences between the CVs were the assigned ethnicity and gender , signalled through names , while all other details remained the same ( see Section 2 . 1 . 2 ) . All input was written in Dutch ; Lai et al . ( 2023 ) have shown that the accuracy and performance of ChatGPT in Dutch on specific language tasks are on par with its accuracy and performance in English . Conducting the experiment in Dutch enabled the identification of assumed language - specific discrimination between two majority groups ( e . g . White American vs . Dutch candidates ) and ethnic - specific discrimination between a majority and a minority group ( e . g . White American vs . Black American ) . The general instruction ordered ChatGPT to help select suitable candidates : “You assist in making personnel selection decisions . Based on the vacancy below and the candidate’s CV , you should indicate how likely you would invite the candidate for an interview” . To obtain an outcome variable at the ratio level , ChatGPT was directed to “answer only with a score ranging from 1 ( very unlikely ) to 100 ( very likely ) ” , i . e . an integer . This rating was eventually labelled the ‘interview invitation score’ in the analyses . Because it was ambiguous how imposing different cutoff scores on ChatGPT’s output as an end user would influence bias in the eventual selection decision , post - hoc sensitivity analyses were carried out to evaluate the impact of proprietary thresholds ( see Section 2 . 3 . 3 and Section 3 . 4 ) . 2 . 2 . 2 . Data exchange Instead of using ChatGPT’s web interface ( https : / / chat . openai . com ) , I conducted the correspondence audit - like experiment using OpenAI’s Application Programming Interface ( API ) . The API facilitated the data exchange by automatically transmitting text prompts to ChatGPT and receiving generated responses in return ( OpenAI , 2023b ) . Connection with the API was made through R relying on the { httr } and { jsonlite } packages . 9 Automating the experiment using the API over ChatGPT’s web interface had two significant advantages . The first advantage was the scalability of the design . Compared to regular audit studies with human subjects , I could present a relatively high number of applicants to ChatGPT in a short time . For reference , the automated sequential sending of the 34 , 560 vacancy – CV combinations could be completed in approximately eight hours . Second , the absence of chat history or memory due to the successive and isolated presentation of vacancy – CV combinations to ChatGPT prohibited the chatbot from generating task - trained responses that could undercut the experiment’s validity . Because of the absence of these spillover effects , it was possible to send identical applications , with just the change in treatment , rather than needing to create differentiation in candidate profiles or CV templates . This approach improved the precision of the results over audit experiments in the field , where differentiation is needed to avoid detection by human recruiters . 2 . 2 . 3 . GPT models At the time of the experiment , there were two principal models in OpenAI’s GPT series : GPT - 3 . 5 ( - Turbo ) and GPT - 4 . The training data of both models originated primarily from the internet , comprising online books , websites , and other texts and included information up to September 2021 ( OpenAI , 2023a ) . While both language models had similar capabilities , GPT - 4 was the more advanced model able to solve more difficult language tasks with higher accuracy than the GPT - 3 . 5 model . However , the GPT - 3 . 5 model required fewer resources and was far less costly than GPT - 4 . In addition , GPT - 3 . 5 was available free of charge to the broader public at the time of writing , while GPT - 4 required a paid account to use the model through the web interface . Because GPT - 3 . 5’ s use was unrestricted to the general public , it served as the model instance for the analyses in this study . For reproducibility purposes , I relied on the model’s 13 June 2023 snapshot , which was the pre - trained language model ’ s saved state on this date . Notably , the GPT models were equipped with safeguards that rendered direct comparisons of job candidates nearly unattainable . For example , when prompting the GPT - 3 . 5 instance of ChatGPT via its web interface to directly compare quasi - identical Dutch and Black American CV profiles , ChatGPT’s response accurately identified minimal differences between the candidates , such as their ethnic background . Attempting to solicit a more 10 decisive response by instructing ChatGPT to make an assessment based on this background , the output read : “As an AI language model , I cannot discriminate against candidates based on their background [ … ] It is important to evaluate candidates based solely on their qualifications , skills , and educational backgrounds relevant to the job position . Therefore , it would not be appropriate to assess or score the candidates based on their backgrounds” . Despite these safeguards , the experimental approach in this study , using isolated prompts exchanged via the API without directly comparing fictitious candidates , enabled an unrestrained evaluation of the potential ethnic and gender bias in ChatGPT’s output . 2 . 2 . 4 . Sampling strategy I also altered the model temperature ( or sampling strategy ) to examine its moderation effect on ChatGPT’s bias . This temperature parameter influences the randomness or creativity of ChatGPT’s output . Low temperatures produce more deterministic responses based on the chatbot’s training data patterns , while higher temperatures generate increased stochastic outputs . The impact of different temperature settings or sampling strategies on ChatGPT’s bias is unclear , as increasing the temperature may reduce common pre - trained biases but introduce or reinforce uncommon biases . Following a probability weighting scheme of 60 . 00 % , 8 . 75 % , 8 . 75 % , 8 . 75 % , 8 . 75 % , 2 . 50 % , and 2 . 50 % , temperatures between 0 and 1 . 5 with increments of 0 . 25 were integrated into the API request . Here , 0 was the minimum temperature setting , making ChatGPT mostly deterministic yet allowing some output variability , while temperatures above 1 . 5 resulted in the chatbot producing such variable output that it no longer adhered to the prompt ’ s guidelines ( e . g . outputting a quantifiable score in [ 1 , 100 ] ∩ ℕ ) . Exact count statistics by model temperature can be retrieved from Table A7 in the appendix . 2 . 3 . Estimation 2 . 3 . 1 . Linear models I estimated multiple ordinary least squares ( OLS ) regression models to assess the relationship between the ratings ( i . e . interview invitation scores ) outputted by ChatGPT and 11 candidate and job features . In these models , the dependent variable indicated a candidate’s suitability for a vacancy , expressed by a score ( i . e . integer ) ranging from 1 – 100 ( Inv ) . The candidates’ ethnic identity ( Eth i ) was the main predictor of interest . I also held ChatGPT’s sampling strategy or temperature ( Tmp i ) in all estimated models constant since it was altered in about two - fifths of the observations ( see Section 2 . 2 . 4 ) . A separate analysis of the influence of the temperature on the results is discussed in Section 3 . 2 . 3 . The principal model , shown in Equation 1 , consisted of the aforementioned predictor variables , operationalised at the applicant level i , with 𝛼 , an intercept , 𝛽 and 𝛾 , model coefficients , and  , the error term . 𝐼𝑛𝑣 =  +  ∗ 𝑇𝑚𝑝 𝑖 +  ∗ 𝐸𝑡ℎ 𝑖 +  ( 1 ) In subsequent OLS models , I expanded the predictor scope by including candidate and job characteristics as covariates . Besides ethnic identity , candidate characteristics ( CAN i ) comprised gender and the candidate’s occupation . Work - related job characteristics ( JOB j ) included the job type ( e . g . temporary job ) , work hours ( e . g . part - time ) , and shift system ( e . g . night work ) defined at the job level j . Furthermore , I entered several job language requirements variables concerning Dutch , French , and English proficiency . Other job - level control variables included the level of professional experience requested and the employment location . Equation 2 shows the extended model containing these covariates with 𝛼 , an intercept , 𝛽 , a coefficient , Γ and Λ , vectors of model coefficients , and  , the error term . 𝐼𝑛𝑣 =  +  ∗ 𝑇𝑚𝑝 𝑖 + 𝐶𝐴𝑁 𝑖 ∗  + 𝐽𝑂𝐵 𝑗 ∗  +  ( 2 ) The last set of OLS models incorporated interaction terms between candidate identity and the candidates’ signalled gender ( Gen i ) . Equation 3 depicts the terminal linear model , including model sampling , candidate occupation ( Occ i ) , and job - related variables with 𝛼 , an intercept , 𝛽 , 𝛾 , and 𝜃 , coefficients , Λ , a vector of model coefficients , and  , the error term . 𝐼𝑛𝑣 =  +  ∗ 𝑇𝑚𝑝 𝑖 +  ∗ ( 𝐸𝑡ℎ 𝑖 ∗ 𝐺𝑒𝑛 𝑖 ) +  ∗ 𝑂𝑐𝑐 𝑖 + 𝐽𝑂𝐵 𝑗 ∗  +  ( 3 ) Each successive model in this series of OLS models contained additional controls related to candidate and job characteristics . By progressively adjusting for these factors , the analyses aimed to further isolate the experimental effect of candidate identity on the interview invitation likelihood , minimising the influence of potential confounding factors in 12 the vacancy and CV texts used . 2 . 3 . 2 . Statistical corrections Standard errors of each OLS model were corrected using cluster - robust wild bootstrapping . This technique produces a distribution of estimated parameters , facilitating the calculation of more precise standard errors ( Cameron & Miller , 2015 ; Cameron et al . , 2008 ) . It achieves this by generating interim datasets with reformed dependent variables derived from a combination of the original model’s fitted values , the residuals , and a random factor . There were three reasons to perform this correction here : ( i ) to control within - cluster error correlation , ( ii ) to address the issue of heteroskedasticity , and ( iii ) to mitigate the violation of the residual normality assumption . Clusters were defined at the vacancy level , given the correlation between the assignment of the candidates and the vacancies presented to ChatGPT , similar to the approach in correspondence audit studies with humans ( Abadie et al . , 2023 ; Vuolo et al . , 2018 ) . The estimates appeared to stabilise around 2 , 000 bootstrap replications , which is sufficient in the context of the current empirical study ( Cameron & Miller , 2015 ) . Furthermore , in the case of multiple family - wise comparisons , I performed ex - post corrections of the p - values in the regression analyses using Holm ’s ( 1979 ) method . This approach entails a stepwise ranking procedure that reduces the likelihood of false positives . Implementing this procedure was particularly meaningful for models that involved numerous comparisons between distinct categories and their respective reference groups . Throughout Section 3 , where appropriate , I reported Holm - corrected p - values alongside the original estimates to validate the results . Using less stringent correction procedures , such as the Benjamini – Hochberg or Benjamini – Yekutieli methods based on the Simes test outlined in Burn et al . ( 2022 ) , produced similar results and did not alter the interpretation of the original corrections ( Benjamini & Hochberg , 1995 ; Benjamini & Yekutieli , 2001 ) . 2 . 3 . 3 . Logit models In a real - world setting , decision - makers such as HR professionals would most plausibly rank the candidates using a cutoff score based on ChatGPT’s output to select the optimal number 13 of candidates to invite for the job interview . Therefore , I estimated logistic regression models to establish a sensitivity analysis of discrimination across different thresholds . To this end , I used a penalised maximum likelihood estimator , which reduces the variance for the estimated coefficients ( even in large samples ) compared to the regular maximum likelihood estimator ( Firth , 1993 ; Rainey & McCaskey , 2021 ) . The dependent variable was the probability of receiving an invitation given a predefined cutoff score n , i . e . Pr ( Inv n = 1 ) . I ran 100 logit models where n took on every integer between 1 and 100 ( or possible interview invitation score ) . The probability of receiving an invitation at a given threshold was regressed on the exact predictor and covariates defined in Equation 1 . Equation 4 shows the logistic model with 𝛼 , an intercept , 𝛽 and 𝛾 , coefficients , and  , the error term . Pr ( 𝐼𝑛𝑣 𝑛 = 1 ) = 𝑙𝑜𝑔𝑖𝑡 −1 (  +  ∗ 𝑇𝑚𝑝 𝑖 +  ∗ 𝐸𝑡ℎ 𝑖 +  ) ( 4 ) This analytic approach enabled assessing whether potential bias persisted across cutoff scores . In other words , putting oneself in the place of the decision - maker , does it matter which cutoff score end users choose ? Given the included set of candidate names , is there an optimal cutoff score where bias is minimal ? Or does discrimination increase or decrease for specific ( ranges of ) cutoff scores ? Finally , I produced discrimination ratios , which capture the relationships between two positive response rates to estimate relative penalties between groups . These ratios were calculated by transforming the log - odds from the logit model specification in Equation 4 to odds ratios ( OR ) and , in turn , into discrimination ratios ( DR ) . Discrimination ratios are essentially risk ratios and constitute a standard measure of discrimination in correspondence audit studies in the social sciences ( Lippens et al . , 2023b ) . Equation 5 shows the OR - to - DR transformation . These ratios were defined relative to the baseline risk ( Pr base ) , corresponding to the probability of a positive response for the reference group at a given cutoff score . Confidence intervals were computed through a Wald z - distribution approximation . 𝐷𝑅 = 𝑂𝑅 ( 1 − 𝑃𝑟 𝑏𝑎𝑠𝑒 ) + ( 𝑃𝑟 𝑏𝑎𝑠𝑒 ∗ 𝑂𝑅 ) ( 5 ) 14 3 . Results 3 . 1 . Scoring bias ChatGPT generally tends to produce high scores when responding to the prompt “How likely is it that you would invite the candidate for an interview ? ” . On a 1 – 100 scale ranging from very unlikely to very likely , the mean invitation score equals 66 . 11 ( SD = 13 . 27 ) . Moreover , ChatGPT exhibits a preference for two specific numerical values . In over a quarter of the cases , ChatGPT scores the candidate 50 ( i . e . 8 , 701 occurrences or 25 . 18 % ) , while in more than two - fifths of the cases , ChatGPT outputs a score of 70 ( i . e . 14 , 604 occurrences or 42 . 26 % ) . Figure 1 illustrates the relative frequency distribution of invitation scores ( by ethnic identity ) . Even though the opaque decision - making process of the GPT model ( and large language models more broadly ) makes it unclear why these specific values are most common , the observation of reoccurring identical values is not surprising given the recurrent presentation of quasi - identical CVs ( only differing by name ) as input for the screening task . Using an interview invitation score threshold of  50 , right in the middle of the scale , ChatGPT would invite the average candidate in 96 . 88 % of the cases — i . e . almost every time . This proportion is remarkably high compared to response rates in correspondence experiments with human recruiters , where the overall invitation probability is closer to 20 % ( Lippens et al . , 2023b ) . I further evaluate the sensitivity of using different cutoff scores as an end decision - maker in Section 3 . 4 . < Figure 1 about here > Despite the generally high scoring , ChatGPT demonstrates a noticeable preference for Dutch - named candidates over equivalent other candidates when presented with their CVs and matched Dutch - written vacancies . The distribution for candidates with names signalling different ethnic identities — i . e . Arabs , Asians , Black Americans , Central Africans , Eastern Europeans , Hispanics , Turks , and White Americans — is slightly more right - skewed than for Dutch candidates . On average , Dutch candidates receive a score of 67 . 62 ( SD = 12 . 63 ) , while the other group attains a combined 65 . 92 ( SD = 13 . 34 ) . Within the latter group , there are slight differences . The mean subgroup scores amount to 66 . 21 ( SD = 13 . 05 ) for Arabs , 65 . 47 ( SD = 13 . 39 ) for Asians , 65 . 80 ( SD = 13 . 57 ) for Black Americans , 66 . 07 ( SD = 13 . 26 ) for Central 15 Africans , 65 . 21 ( SD = 13 . 76 ) for Eastern Europeans , 66 . 01 ( SD = 13 . 35 ) for Hispanics , 65 . 88 ( SD = 13 . 15 ) Turks , and 66 . 67 ( SD = 13 . 12 ) for White Americans , respectively . 3 . 2 . Identity discrimination 3 . 2 . 1 . Ethnic discrimination Table 1 contains the estimates of six OLS regression models where ChatGPT’s interview invitation scores are regressed on job candidate ethnic and gender identity , among other covariates ( see Section 2 . 3 for the estimation details ) . Compared to the Dutch reference group , the effects of each of the other ethnic identities on the invitation score are statistically significant ( at the 0 . 1 % significance level ) and negative . In other words , candidates with White American , Arab , Central African , Hispanic , Turkish , Black American , Asian , and Eastern European names receive significantly lower scores from ChatGPT than Dutch - named candidates . Although minor , average penalties range from approximately − 0 . 96 to − 2 . 42 points on a 1 – 100 scale . < Table 1 about here > In contrast with worldwide hiring discrimination observed in human recruiters , where candidates of Arab , Middle Eastern or Northern African origin face the highest disadvantage ( see Lippens et al . , 2023b ) , Eastern Europeans are penalised the most by ChatGPT compared to the Dutch reference group ( 𝛽 E . Eu . = − 2 . 4170 , SE = 0 . 2275 , p Holm < 0 . 001 ; see Model 1 in Table 1 ) . This candidate group is followed by Asians ( 𝛽 Asian = − 2 . 1583 , SE = 0 . 2340 , p Holm < 0 . 001 ) , Black Americans ( 𝛽 B . Am . = − 1 . 8436 , SE = 0 . 2288 , p Holm < 0 . 001 ) , Turks ( 𝛽 Turkish = − 1 . 7478 , SE = 0 . 2227 , p Holm < 0 . 001 ) , Hispanics ( 𝛽 Hispanic = − 1 . 6257 , SE = 0 . 2288 , p Holm < 0 . 001 ) , Central Africans ( 𝛽 C . Afr . = − 1 . 5468 , SE = 0 . 2278 , p Holm < 0 . 001 ) , Arabs ( 𝛽 Arab = − 1 . 4117 , SE = 0 . 2141 , p Holm < 0 . 001 ) , and White Americans ( 𝛽 W . Am . = − 0 . 9563 , SE = 0 . 2258 , p Holm < 0 . 001 ) , whose relative penalties are marginally lower . Figure 2 illustrates these results visually . Holding relevant covariates constant does not alter the significance of these results or their interpretation ( see Models 2 to 6 in Table 1 ) . < Figure 2 about here > 16 Although Dutch and White American applicants could both be regarded as majority group candidates in their respective geographies , candidates with White American names still face a significant penalty compared to their Dutch counterparts (  W . Am . – Dutch = 𝛽 W . Am = − 0 . 9563 ) . This result suggests that the prompt language used ( i . e . Dutch ) at least partly affects ChatGPT’s scoring bias and that this score difference can be interpreted as a language - specific rather than an ethnic - specific bias . Conversely , White American applicants receive significantly higher scores than Black American applicants , on average (  B . Am . – W . Am . = − 0 . 8765 , t Welch = − 2 . 88 , p = 0 . 004 ) . Because the prompt language used reasonably should have little effect on this difference , I interpret it as a mainly ethnic - specific bias . 3 . 2 . 2 . Gender discrimination ChatGPT’s outputted interview invitation scores do not vary statistically significantly with the candidate’s gender . Models 2 to 6 in Table 1 include coefficient estimates for female versus male candidates . These coefficients are slightly positive but indistinguishable from zero . The statistical insignificance of this finding remains unchanged when including relevant covariates . This finding aligns with average gender discrimination estimates from the field experimental literature on hiring discrimination ( Lippens et al . , 2023b ) . Nevertheless , the question remains whether there are intersectional effects between gender and ethnic identity in determining ChatGPT’s output . A prominent hypothesis in the discrimination literature is the multiple minority status or double jeopardy hypothesis ( Derous et al . , 2012 , 2015 ) . Belonging to an ethnic and gender minority group presumably engenders a double penalty ; ethnic females are subject to increased penalties . Nonetheless , recent research has indicated that ethnic minority males often experience more discrimination than females , especially Arab applicants ( Arai et al . , 2016 ; Dahl & Krog , 2018 ; Derous et al . , 2015 ) . This discrimination appears partly induced by stereotypes about masculinity ( Bursell , 2014 ; Di Stasio & Larsen , 2020 ) . Departing from the idea that this intersected genderism is inherently present in ChatGPT’s training data , I assess whether ethnic discrimination is significantly moderated by gender . Table 2 shows the interaction effects of ethnic and gender identity on ChatGPT interview invitation scores . While Turkish male applicants receive marginally worse scores than Dutch male candidates ( 𝛽 Turkish = − 0 . 8595 , SE = 0 . 3243 , p = 0 . 009 , p Holm = 0 . 138 ; see Model 1 in 17 Table 2 ) , Turkish females lose approximately 1 . 78 points net versus Turkish males and 1 . 17 points net compared to Dutch male applicants ( 𝛽 Female = 0 . 6063 , SE = 0 . 3221 , p = 0 . 057 ; 𝛽 Turkish : Female = − 1 . 7765 , SE = 0 . 4810 , p < 0 . 001 , p Holm = 0 . 003 ) . The apparent double penalty for Eastern European and Black American females becomes statistically insignificant after applying Holm ’ s correction ( 𝛽 E . Eu . : Female = − 1 . 0459 , SE = 1 . 0459 , p = 0 . 026 , p Holm = 0 . 412 ; 𝛽 B . Am . : Female = − 0 . 9499 , SE = 0 . 4753 , p = 0 . 041 , p Holm = 0 . 660 ) . Evidence for the moderation effect of gender on the ethnic bias for the remaining identities is absent . Holding relevant covariates constant does not impact the statistical significance of these findings ( see Model 2 in Table 2 ) . In other words , Turkish females are worse off than Turkish males in the CV screening task , reflecting the multiple minority status of the former group . Nevertheless , similar to the main effect of ethnic identity , penalties remain relatively small considering the 1 – 100 scale of the outcome variable . < Table 2 about here > 3 . 2 . 3 . Sampling strategy and bias An additional series of analyses concerns the correlation between ChatGPT’s sampling strategy and its bias . The sampling strategy is included in ChatGPT’s temperature parameter , which modulates the degree of randomness or ‘creativity’ in ChatGPT’s output . Low temperatures ( i . e . a deterministic sampling strategy ) result in more coherent and consistent responses , while higher temperatures ( i . e . a stochastic sampling strategy ) produce more varied and creative outputs . As the temperature increases , ChatGPT may diverge from its common pre - trained biases but could also introduce and reinforce uncommon biases . Nevertheless , using OLS regressions to estimate the interaction effects between candidate ethnic identity and model temperature , ChatGPT’s sampling strategy appears to have no impact on its ethnic bias ( see Table A8 in the appendix ) . In other words , making the sampling process more stochastic ( i . e . introducing randomness ) does not significantly change ChatGPT’s bias . This finding hints that the uncovered bias arises from the pre - training data rather than being particular to the sampling strategy in the post - processing . 18 3 . 3 . Name discrimination The differences between groups outlined in Section 3 . 2 hide the substantial dispersion in assigned interview invitation scores between names of the same ethnic and gender identity . In other words , much larger differences in ChatGPT’s score output exist within groups than between groups . Figure 3 visualises each name ’ s mean interview invitation score by ethnic and gender identity . The visualised name score dispersion seems to vary across identities but is generally consistent across genders ( see Section 3 . 2 for an exception regarding male and female Turkish applicants ) . < Figure 3 about here > I present two concrete examples based on the adapted OLS specification from Equation 2 ( see Section 2 . 3 . 1 ) , replacing the ethnic identity variable with the candidate name and restricting the analysis to sets of two names . First , the Central African male applicant ‘Gaetan Bihsinga’ receives significantly lower scores ( M = 62 . 64 , SD = 13 . 00 ) than ‘Tanguy Mangala’ ( M = 69 . 69 , SD = 12 . 68 , t Welch = − 3 . 14 , p = 0 . 002 ) with an interview invitation score difference of − 7 . 05 points . Similarly , the Turkish female candidate ‘Esma Aydin’ scores 5 . 13 points lower ( M = 63 . 65 , SD = 13 . 48 ) than ‘Meryem Yüksel’ ( M = 68 . 78 , SD = 11 . 88 , t Welch = − 2 . 22 , p = 0 . 028 ) . Specific names thus elicit different discrimination levels , reflecting nuances in ChatGPT’s pre - trained bias . Individual applicants can be far worse ( or better ) off compared to their ethnic and gender peers than when comparing ‘average candidates’ between groups . However , this finding relies on the random allocation of a sufficient number of the same names to the ethnic – gender identities across vacancy – CV combinations . It should be noted that despite successful randomisation , the number of iterations may be too small to infer true differences in ratings for some names . Mean interview invitation scores of all 812 first and last name combinations used in the experiment can be retrieved from Table A9 in the appendix . 3 . 4 . Score cutoff sensitivity Next , I evaluate whether selecting a proprietary cutoff score based on ChatGPT’s output as an end decision - maker could impact the bias that transpires in the selection process . In 19 other words , does the decision of an end user to invite a set of candidates who attain a particular minimum score , relying on ChatGPT’s ratings , influence the extent to which they would discriminate in the selection process ? In short , the answer to whether selecting a proprietary cutoff score changes the sign of the resulting bias is ‘no’— however , the magnitude of the bias changes . Panel A of Figure 4 illustrates that at every cutoff score in [ 50 , 80 ) , Dutch applicants receive higher invitation probabilities on average than candidates with other ethnic identities . For example , at a threshold value of 60 , Dutch applicants would be invited 74 . 92 % of the time , on average , compared to 69 . 20 % for other candidates — i . e . a difference of about 5 . 7 percentage points . This discrepancy equates to a discrimination ratio of 0 . 92 . Ratios below 1 indicate discrimination against candidates with a different ethnic identity , with lower ratios implying a higher discrimination level ( see Section 2 . 3 . 3 for calculation details ) . More specifically , applicants of other ethnic identities than Dutch would be invited 8 % less . < Figure 4 about here > The absolute gap tightens in the [ 70 , 80 ) range . At a cutoff score of 75 , for example , the difference amounts to 2 . 8 percentage points ; Dutch applicants would be invited 18 . 79 % of the time on average , while the average invitation probability of other candidates at this threshold equals 16 . 00 % . However , in relative terms , this difference equates to a discrimination ratio of 0 . 85 , indicating a 15 % lower relative invitation probability and , thus , more discrimination against other ethnicity applicants than at a cutoff score of 60 . For cutoff scores in [ 1 , 50 ) and [ 80 , 100 ] , the outcome is nearly the same for all applicants , namely that virtually all or no candidates would be invited , respectively . Hence , decision - makers using ChatGPT as an assistant for personnel selection would logically only choose a cutoff in [ 50 , 80 ) based on its output in the current experiment and consequently discriminate . Panel B of Figure 4 zooms in on the results by ethnic identity in the [ 50 , 80 ) range . Unsurprisingly , the results are similar to the findings based on the OLS model estimates : candidates with White American names are penalised the least compared to Dutch applicants . In contrast , Asian - and Eastern European - named applicants face the largest penalties . The most considerable difference in interview invitation probability in the [ 50 , 80 ) range occurs around the threshold value of 75 . At this score , discrimination ratios amount to 0 . 81 for Asians , 0 . 83 for Eastern Europeans and Hispanics , 0 . 84 for Black Americans , 0 . 85 20 for Arabs , 0 . 86 for Turks and Central Africans , and 0 . 93 for White Americans . To put these in perspective , average discrimination ratios regarding hiring discrimination by humans in worldwide audit research equal approximately 0 . 67 for Asians , 0 . 72 for Eastern Europeans , 0 . 92 for Hispanics , 0 . 68 for Blacks , 0 . 59 for Arabs , and 0 . 75 for Western Asians including Turks ( Lippens et al . , 2023b ) . Even though ChatGPT discriminates based on ethnic identity , it generally seems to perform better than the average human recruiter ( except for the Hispanic subgroup ) , relying on the name set and the specific experimental setup used in the current study . However , note that substantial differences in name sets and control groups between the estimates presented here and the global estimates from worldwide audit research make a formal comparison difficult . 4 . Conclusion Through a simulated CV screening task , I provide evidence that ChatGPT displays systematic bias in its output , showing noticeable preferences for specific ethnic and gender groups when evaluating job candidates . The chatbot was significantly less inclined to advance equally qualified Arabs , Asians , Black Americans , Central Africans , Eastern Europeans , Hispanics , Turks , and White Americans to the interview stage of the selection process than candidates from the Dutch reference group . The minor penalty for White American - named and more substantial penalty for Black American - named candidates , for example , suggests that a prompt language - specific bias is at play alongside an ethnic - specific bias . Levels of ethnic identity bias by ChatGPT appeared lower compared with meta - analytic ethnic hiring discrimination estimates from worldwide correspondence audit research involving human recruiters — although it is essential to note that the included name sets and control groups differ between the current experimental setup and the experimental designs of most correspondence audits with human recruiters in existing field research . Moreover , female candidates were not rated lower on average than their male counterparts . At the intersection of ethnic and gender identity , however , ChatGPT discriminated more against Turkish females than Turkish males , consistent with the multiple minority status of the former group . Altering ChatGPT’s sampling strategy did not significantly influence its bias . 21 Finally , I uncovered substantial differences in interview scores between names of the same ethnic and gender identity . I see several avenues for future research . First , scholars could explore whether the uncovered bias transposes to different grounds for discrimination , languages , contexts , selection tasks , or language models . For example , discrimination based on age is a persistent problem in personnel selection , which also merits attention in research on AI bias ( Lippens et al . , 2023b ; Stypinska , 2023 ) . Moreover , AI - based decision - making may also result in discrimination in contexts such as housing and healthcare through algorithm - based awarding of house rentals or treatment plan recommendations in patients , to name just two examples ( Basu , 2023 ; Rosen et al . , 2021 ) . Second , to enhance model explainability , researchers could continue investing in making the decision - making processes of large language models more transparent to understand which features contribute to the bias in their output ( Arrieta et al . , 2020 ) . Third , to reduce bias , scholars could further evaluate fairness - enhancing techniques applied to the training data , the training process , or the post - processing ( Friedler et al . , 2019 ) . This study underlines the significance of understanding and addressing systemic discrimination in large language models , especially when deployed in real - world applications such as hiring and selection . Model developers preferably advance efforts to mitigate biases arising from the pre - training data and process . Policymakers may create legal frameworks ensuring equitable use of large language models in decision - making directly impacting humans . For example , the recently adopted AI Act by the European Parliament ( European Commission , 2021 ) forms a step in the right direction . However , as the act bounds LLMs only by transparency obligations , it falls short of addressing the risks associated with relying on these models for selection purposes , as illustrated in this study . Practitioners considering using ChatGPT and the like should proceed cautiously . At a minimum , their usage requires a thorough assessment of the trade - off between increased efficiency and adverse impact . Anonymising personal information by removing explicit minority status markers before parsing might help reduce bias , as found in human recruiters ( Åslund & Skans , 2012 ; Derous & Ryan , 2019 ; Lacroux & Martin - Lacroux , 2019 ) . Nonetheless , LLMs could still pick up on implicit markers in applicant profiles ( e . g . organisation affiliations signalling ethnic group membership or years of professional experience signalling age ) , continuing the output of 22 biased responses ( see Arnold et al . , 2021 ) . The applicability of LLMs in their current form for activities that involve decision - making affecting humans is debatable . 23 References Abadie , A . , Athey , S . , Imbens , G . W . , & Wooldridge , J . M . ( 2023 ) . When should you adjust standard errors for clustering ? The Quarterly Journal of Economics , 138 ( 1 ) , 1 – 35 . https : / / doi . org / 10 . 1093 / qje / qjac038 Abid , A . , Farooqi , M . , & Zou , J . ( 2021 ) . Persistent anti - Muslim bias in large language models . Proceedings of the 2021 AAAI / ACM Conference on AI , Ethics , and Society , 298 – 306 . https : / / doi . org / 10 . 1145 / 3461702 . 3462624 Arai , M . , Bursell , M . , & Nekby , L . ( 2016 ) . The reverse gender gap in ethnic discrimination : Employer stereotypes of men and women with Arabic names . International Migration Review , 50 ( 2 ) , 385 – 412 . https : / / doi . org / 10 . 1111 / imre . 12170 Arnold , D . , Dobbie , W . , & Hull , P . ( 2021 ) . Measuring racial discrimination in algorithms . AEA Papers and Proceedings , 111 , 49 – 54 . https : / / doi . org / 10 . 1257 / pandp . 20211080 Arrieta , A . B . , Díaz - Rodríguez , N . , Ser , J . D . , Bennetot , A . , Tabik , S . , Barbado , A . , … Herrera , F . ( 2020 ) . Explainable Artificial Intelligence ( XAI ) : Concepts , taxonomies , opportunities and challenges toward responsible AI . Information Fusion , 58 , 82 – 115 . https : / / doi . org / 10 . 1016 / j . inffus . 2019 . 12 . 012 Åslund , O . , & Skans , O . N . ( 2012 ) . Do anonymous job application procedures level the playing field ? ILR Review , 65 ( 1 ) , 82 – 107 . https : / / doi . org / 10 . 1177 / 001979391206500105 Baert , S . , Lippens , L . , & Van Borm , H . ( 2022 ) . Selecting names for experiments on ethnic discrimination ( IZA Discussion Papers No . 15524 ) . IZA Institute of Labor Economics . https : / / ideas . repec . org / p / iza / izadps / dp15524 . html Bartkoski , T . , Lynch , E . , Witt , C . , & Rudolph , C . ( 2018 ) . A meta - analysis of hiring discrimination against Muslims and Arabs . Personnel Assessment and Decisions , 4 ( 2 ) . https : / / doi . org / 10 . 25035 / pad . 2018 . 02 . 001 Basu , A . ( 2023 ) . Use of race in clinical algorithms . Science Advances , 9 ( 21 ) , eadd270 . https : / / doi . org / 10 . 1126 / sciadv . add2704 Benjamini , Y . , & Hochberg , Y . ( 1995 ) . Controlling the false discovery rate : A practical and powerful approach to multiple testing . Journal of the Royal Statistical Society : Series B ( Methodological ) , 57 ( 1 ) , 289 – 300 . https : / / doi . org / 10 . 1111 / j . 2517 - 6161 . 1995 . tb02031 . x Benjamini , Y . , & Yekutieli , D . ( 2001 ) . The control of the false discovery rate in multiple testing under dependency . The Annals of Statistics , 29 ( 4 ) . https : / / doi . org / 10 . 1214 / aos / 1013699998 Bliuc , A . - M . , Faulkner , N . , Jakubowicz , A . , & McGarty , C . ( 2018 ) . Online networks of racial hate : A systematic review of 10 years of research on cyber - racism . Computers in Human Behavior , 87 , 75 – 86 . https : / / doi . org / 10 . 1016 / j . chb . 2018 . 05 . 026 Bohren , J . A . , Hull , P . , & Imas , A . ( 2022 ) . Systemic discrimination : Theory and measurement ( NBER Working Paper No . 29820 ) . National Bureau of Economic Research . https : / / doi . org / 10 . 3386 / w29820 Brown , T . , Mann , B . F . , Ryder , N . , Subbiah , M . , Kaplan , J . , Dhariwal , P . , … , Amodei , D . ( 2020 ) . Language models are few - shot learners . arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2005 . 14165 24 Brynjolfsson , E . , Li , D . , & Raymond , L . ( 2023 ) . Generative AI at work . arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2304 . 11771 Burn , I . , Button , P . , Corella , L . M . , & Neumark , D . ( 2022 ) . Does ageist language in job ads predict age discrimination in hiring ? Journal of Labor Economics , 40 ( 3 ) , 613 – 667 . https : / / doi . org / 10 . 1086 / 717730 Bursell , M . ( 2014 ) . The multiple burdens of foreign - named men : evidence from a field experiment on gendered ethnic hiring discrimination in Sweden . European Sociological Review , 30 ( 3 ) , 399 – 409 . https : / / doi . org / 10 . 1093 / esr / jcu047 Caliskan , A . , Bryson , J . J . , & Narayanan , A . ( 2017 ) . Semantics derived automatically from language corpora contain human - like biases . Science , 356 ( 6334 ) , 183 – 186 . https : / / doi . org / 10 . 1126 / science . aal4230 Cameron , A . C . , & Miller , D . L . ( 2015 ) . A pra ctitioner’s guide to cluster - robust inference . Journal of Human Resources , 50 ( 2 ) , 317 – 372 . https : / / doi . org / 10 . 3368 / jhr . 50 . 2 . 317 Cameron , A . C . , Gelbach , J . B . , & Miller , D . L . ( 2008 ) . Bootstrap - based improvements for inference with clustered errors . Review of Economics and Statistics , 90 ( 3 ) , 414 – 427 . https : / / doi . org / 10 . 1162 / rest . 90 . 3 . 414 Castaño - Pulgarín , S . A . , Suárez - Betancur , N . , Vega , L . M . T . , & López , H . M . H . ( 2021 ) . Internet , social media and online hate speech : Systematic review . Aggression and Violent Behavior , 58 , 101608 . https : / / doi . org / 10 . 1016 / j . avb . 2021 . 101608 Cooke , F . L . , Liu , M . , Liu , L . A . , & Chen , C . C . ( 2019 ) . Human resource management and industrial relations in multinational corporations in and from China : Challenges and new insights . Human Resource Management , 58 ( 5 ) , 455 – 471 . https : / / doi . org / 10 . 1002 / hrm . 21986 Crabtree , C . , Kim , J . Y . , Gaddis , S . M . , Holbein , J . B . , Guage , C . , & Marx , W . W . ( 2023 ) . Validated names for experimental studies on race and ethnicity . Scientific Data , 10 ( 1 ) . https : / / doi . org / 10 . 1038 / s41597 - 023 - 01947 - 0 Dahl , M . , & Krog , N . ( 2018 ) . Experimental evidence of discrimination in the labour market : Intersections between ethnicity , gender , and socio - economic status . European Sociological Review , 34 ( 4 ) , 402 – 417 . https : / / doi . org / 10 . 1093 / esr / jcy020 Derous , E . , & Ryan , A . M . ( 2019 ) . When your resume is ( not ) turning you down : Modelling ethnic bias in resume screening . Human Resource Management Journal , 29 ( 2 ) , 113 – 130 . https : / / doi . org / 10 . 1111 / 1748 - 8583 . 12217 Derous , E . , Ryan , A . M . , & Nguyen , H . - H . D . ( 2012 ) . Multiple categorization in resume screening : Examining effects on hiring discrimination against Arab applicants in field and lab settings . Journal of Organizational Behavior , 33 ( 4 ) , 544 – 570 . https : / / doi . org / 10 . 1002 / job . 769 Derous , E . , Ryan , A . M . , & Serlie , A . W . ( 2015 ) . Double jeopardy upon resumé screening : When Achmed is less employable than Aïsha . Personnel Psychology , 68 ( 3 ) , 659 – 696 . https : / / doi . org / 10 . 1111 / peps . 12078 Di Stasio , V . , & Larsen , E . N . ( 2020 ) . The racialized and gendered workplace : Applying an intersectional lens to a field experiment on hiring discrimination in five European labor markets . Social Psychology Quarterly , 83 ( 3 ) , 229 – 250 . https : / / doi . org / 10 . 1177 / 0190272520902994 Eloundou , T . , Manning , S . , Mishkin , P . , & Rock , D . ( 2023 ) . GPTs are GPTs : An early look at the labor market impact potential of large language models . arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2303 . 10130 25 European Commission ( 2021 ) . European Commission Memorandum COM / 2021 / 206 : 2021 : Proposal for a regulation of the European Parliament and of the council : Laying down harmonised rules on Artificial Intelligence ( Artificial Intelligence Act ) and amending certain union legislative acts . European Commission . https : / / eur - lex . europa . eu / legal - content / EN / TXT / ? uri = CELEX : 52021PC0206 Felten , E . , Raj , M . , & Seamans , R . ( 2023 ) . How will language modelers like ChatGPT affect occupations and industries ? arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2303 . 01157 Firth , D . ( 1993 ) . Bias reduction of maximum likelihood estimates . Biometrika , 80 ( 1 ) , 27 – 38 . https : / / doi . org / 10 . 1093 / biomet / 80 . 1 . 27 Friedler , S . A . , Scheidegger , C . , Venkatasubramanian , S . , Choudhary , S . , Hamilton , E . P . , & Roth , D . ( 2019 ) . A comparative study of fairness - enhancing interventions in machine learning . Proceedings of the Conference on Fairness , Accountability , and Transparency 2019 , 329 – 338 . https : / / doi . org / 10 . 1145 / 3287560 . 3287589 Gaddis , S . M . ( 2017 ) . Racial / Ethnic perceptions from Hispanic names : Selecting names to test for discrimination . Socius : Sociological Research for a Dynamic World , 3 , 1 – 11 . https : / / doi . org / 10 . 1177 / 2378023117737193 Gaddis , S . M . ( 2018 ) . An introduction to audit studies in the social sciences . In M . Gaddis ( Ed . ) , Audit Studies : Behind the Scenes with Theory , Method , and Nuance ( pp . 3 – 44 ) . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 71153 - 9 _ 1 Heath , A . F . , & Di Stasio , V . ( 2019 ) . Racial discrimination in Britain , 1969 – 2017 : A meta - analysis of field experiments on racial discrimination in the British labour market . The British Journal of Sociology , 70 ( 5 ) , 1774 – 1798 . https : / / doi . org / 10 . 1111 / 1468 - 4446 . 12676 Holm , S . ( 1979 ) . A simple sequentially rejective multiple test procedure . Scandinavian Journal of Statistics , 6 , 65 – 70 . https : / / www . jstor . org / stable / 4615733 Koçak , A . , Derous , E . , Born , M . P . , & Duyck , W . ( 2022 ) . What ( not ) to add in your ad : When job ads discourage older or younger job seekers to apply . International Journal of Selection and Assessment , 31 ( 1 ) , 92 – 104 . https : / / doi . org / 10 . 1111 / ijsa . 12385 Lacroux , A . , & Martin - Lacroux , C . ( 2019 ) . Anonymous résumés : An effective preselection method ? International Journal of Selection and Assessment , 28 ( 1 ) , 98 – 111 . https : / / doi . org / 10 . 1111 / ijsa . 12275 Lai , V . D . , Ngo , N . T . , Veyseh , A . P . B . , Man , H . , Dernoncourt , F . , Bui , T . , & Nguyen , T . H . ( 2023 ) . ChatGPT beyond English : Towards a comprehensive evaluation of large language models in multilingual learning . arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2304 . 05613 Lippens , L . , Dalle , A . , D’hondt , F . , Baert , S . , & Verhaeghe , P . - P . ( 2023a ) . Understanding ethnic hiring discrimination : A contextual analysis of experimental evidence . Unpublished manuscript . Ghent University . Lippens , L . , Vermeiren , S . , & Baert , S . ( 2023b ) . The state of hiring discrimination : A meta - analysis of ( almost ) all recent correspondence experiments . European Economic Review , 151 , 104315 . https : / / doi . org / 10 . 1016 / j . euroecorev . 2022 . 104315 Liu , R . , Jia , C . , Wei , J . , Xu , G . , & Vosoughi , S . ( 2022 ) . Quantifying and alleviating political bias in language models . Artificial Intelligence , 304 , 103654 . https : / / doi . org / 10 . 1016 / j . artint . 2021 . 103654 26 Martiniello , B . , & Verhaeghe , P . - P . ( 2022 ) . Signaling ethnic - national origin through names ? The perception of names from an intersectional perspective . PLOS ONE , 17 ( 8 ) , e0270990 . https : / / doi . org / 10 . 1371 / journal . pone . 0270990 Nature . ( 2023 ) . Tools such as ChatGPT threaten transparent science : Here are our ground rules for their use . Nature , 613 ( 7945 ) , 612 – 612 . https : / / doi . org / 10 . 1038 / d41586 - 023 - 00191 - 1 Noble , S . M . , Foster , L . L . , & Craig , S . B . ( 2021 ) . The procedural and interpersonal justice of automated application and resume screening . International Journal of Selection and Assessment , 29 ( 2 ) , 139 – 153 . https : / / doi . org / 10 . 1111 / ijsa . 12320 Noy , S . , & Zhang , W . ( 2023 ) . Experimental evidence on the productivity effects of generative artificial intelligence ( SSRN Electronic Journal Working Paper No . 4375283 ) . University of Chicago , Becker Friedman Institute for Economics . https : / / doi . org / 10 . 2139 / ssrn . 4375283 OpenAI . ( 2023a ) . GPT - 4 Technical Report . arXiv ( Cornell University ) . https : / / doi . org / 10 . 48550 / arxiv . 2303 . 08774 OpenAI . ( 2023b ) . OpenAI API documentation . https : / / platform . openai . com / docs . Accessed : 2023 - 04 - 25 . Peres , R . , Schreier , M . , Schweidel , D . , & Sorescu , A . ( 2023 ) . On ChatGPT and beyond : How generative artificial intelligence may affect research , teaching , and practice . International Journal of Research in Marketing , 40 ( 2 ) , 269 – 275 . https : / / doi . org / 10 . 1016 / j . ijresmar . 2023 . 03 . 001 Pisanelli , E . ( 2022 ) . Your resume is your gatekeeper : Automated resume screening as a strategy to reduce gender gaps in hiring . Economics Letters , 221 , 110892 . https : / / doi . org / 10 . 1016 / j . econlet . 2022 . 110892 Quillian , L . , & Lee , J . J . ( 2023 ) . Trends in racial and ethnic discrimination in hiring in six Western countries . Proceedings of the National Academy of Sciences , 120 ( 6 ) . https : / / doi . org / 10 . 1073 / pnas . 2212875120 Quillian , L . , & Midtbøen , A . H . ( 2021 ) . Comparative perspectives on racial discrimination in hiring : The rise of field experiments . Annual Review of Sociology , 47 ( 1 ) , 391 – 415 . https : / / doi . org / 10 . 1146 / annurev - soc - 090420 - 035144 Quillian , L . , Heath , A . , Pager , D . , Midtbøen , A . , Fleischmann , F . , & Hexel , O . ( 2019 ) . Do some countries discriminate more than others ? Evidence from 97 field experiments of racial discrimination in hiring . Sociological Science , 6 , 467 – 496 . https : / / doi . org / 10 . 15195 / v6 . a18 Quillian , L . , Pager , D . , Hexel , O . , & Midtbøen , A . H . ( 2017 ) . Meta - analysis of field experiments shows no change in racial discrimination in hiring over time . Proceedings of the National Academy of Sciences , 114 ( 41 ) , 10870 – 10875 . https : / / doi . org / 10 . 1073 / pnas . 1706255114 Rainey , C . , & McCaskey , K . ( 2021 ) . Estimating logit models with small samples . Political Science Research and Methods , 9 ( 3 ) , 549 – 564 . https : / / doi . org / 10 . 1017 / psrm . 2021 . 9 Rich , A . S . , & Gureckis , T . M . ( 2019 ) . Lessons for artificial intelligence from the study of natural stupidity . Nature Machine Intelligence , 1 ( 4 ) , 174 – 180 . https : / / doi . org / 10 . 1038 / s42256 - 019 - 0038 - z Rosen , E . , Garboden , P . M . E . , & Cossyleon , J . E . ( 2021 ) . Racial discrimination in housing : How landlords use algorithms and home visits to screen tenants . American Sociological Review , 86 ( 5 ) , 787 – 822 . https : / / doi . org / 10 . 1177 / 00031224211029618 Schramowski , P . , Turan , C . , Andersen , N . , Rothkopf , C . A . , & Kersting , K . ( 2022 ) . Large pre - trained language models contain human - like biases of what is right and wrong to do . Nature Machine 27 Intelligence , 4 ( 3 ) , 258 – 268 . https : / / doi . org / 10 . 1038 / s42256 - 022 - 00458 - 8 Stypinska , J . ( 2023 ) . AI ageism : A critical roadmap for studying age discrimination and exclusion in digitalized societies . AI & Society , 38 ( 2 ) , 665 – 677 . https : / / doi . org / 10 . 1007 / s00146 - 022 - 01553 - 5 Tambe , P . , Cappelli , P . , & Yakubovich , V . ( 2019 ) . Artificial intelligence in human resources management : Challenges and a path forward . California Management Review , 61 ( 4 ) , 15 – 42 . https : / / doi . org / 10 . 1177 / 0008125619867910 Teubner , T . , Flath , C . M . , Weinhardt , C . , van der Aalst , W . , & Hinz , O . ( 2023 ) . Welcome to the Era of ChatGPT et al . Business & Information Systems Engineering , 65 ( 2 ) , 95 – 101 . https : / / doi . org / 10 . 1007 / s12599 - 023 - 00795 - x The Economist . ( 2023a ) . Investors are going nuts for ChatGPT - ish artificial intelligence . Retrieved 10 April 2023 , from https : / / www . economist . com / business / 2023 / 02 / 28 / investors - are - going - nuts - for - chatgpt - ish - artificial - intelligence The Economist . ( 2023b ) . The race of the AI labs heats up . Retrieved 10 April 2023 , from https : / / www . economist . com / business / 2023 / 01 / 30 / the - race - of - the - ai - labs - heats - up Thijssen , L . , van Tubergen , F . , Coenders , M . , Hellpap , R . , & Jak , S . ( 2021 ) . Discrimination of Black and Muslim minority groups in Western societies : Evidence from a meta - analysis of field experiments . International Migration Review , 56 ( 3 ) , 843 – 880 . https : / / doi . org / 10 . 1177 / 01979183211045044 Thorp , H . H . ( 2023 ) . ChatGPT is fun , but not an author . Science , 379 ( 6630 ) , 313 – 313 . https : / / doi . org / 10 . 1126 / science . adg7879 van Esch , P . , Black , J . S . , & Ferolie , J . ( 2019 ) . Marketing AI recruitment : The next phase in job application and selection . Computers in Human Behavior , 90 , 215 – 222 . https : / / doi . org / 10 . 1016 / j . chb . 2018 . 09 . 009 Vrontis , D . , Christofi , M . , Pereira , V . , Tarba , S . , Makrides , A . , & Trichina , E . ( 2021 ) . Artificial intelligence , robotics , advanced technologies and human resource management : A systematic review . The International Journal of Human Resource Management , 33 ( 6 ) , 1237 – 1266 . https : / / doi . org / 10 . 1080 / 09585192 . 2020 . 1871398 Vuolo , M . , Uggen , C . , & Lageson , S . ( 2018 ) . To match or not to match ? Statistical and substantive considerations in audit design and analysis . In S . M . Gaddis ( Ed . ) , Audit studies : Behind the scenes with theory , method , and nuance ( pp . 119 – 140 ) . Springer , Cham . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 71153 - 9 _ 6 Wille , L . , & Derous , E . ( 2017 ) . Getting the words right : When wording of job ads affects ethnic minorities’ application decisions . Management Communication Quarterly , 31 ( 4 ) , 533 – 558 . https : / / doi . org / 10 . 1177 / 0893318917699885 Zschirnt , E . , & Ruedin , D . ( 2016 ) . Ethnic discrimination in hiring decisions : A meta - analysis of correspondence tests 1990 \ textendash2015 . Journal of Ethnic and Migration Studies , 42 ( 7 ) , 1115 – 1134 . https : / / doi . org / 10 . 1080 / 1369183x . 2015 . 1133279 28 Declarations Data availability The data used in this study and supplementary tables are available at https : / / osf . io / vezt7 / ? view _ only = 0fb1f53c502a4c7091f3003d972f93b3 ( licensed under CC - BY - 4 . 0 ) . Declaration of competing interest I declare no relevant financial or non - financial competing interests . Funding This study was conducted on the margins of the EdisTools project . EdisTools is funded by Research Foundation – Flanders ( Strategic Basic Research , S004119N ) . Author contributions L . L . was the sole contributor to this manuscript . 29 Tables Table 1 . OLS regression of ChatGPT interview invitation scores on ethnic and gender identity ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Intercept 67 . 2157 * * * ( 0 . 2571 ) 67 . 1807 * * * ( 0 . 2585 ) 72 . 1024 * * * ( 0 . 7978 ) 72 . 1518 * * * ( 0 . 8772 ) 73 . 2052 * * * ( 1 . 1155 ) 72 . 8552 * * * ( 1 . 0933 ) Panel A . Identity ( ref . : Dutch ) White American − 0 . 9563 * * * ( 0 . 2258 ) − 0 . 9563 * * * ( 0 . 2244 ) − 0 . 9562 * * * ( 0 . 2189 ) − 0 . 9567 * * * ( 0 . 2208 ) − 0 . 9566 * * * ( 0 . 2181 ) − 0 . 9569 * * * ( 0 . 2200 ) Arab − 1 . 4117 * * * ( 0 . 2141 ) − 1 . 4117 * * * ( 0 . 2192 ) − 1 . 4124 * * * ( 0 . 2130 ) − 1 . 4129 * * * ( 0 . 2200 ) − 1 . 4125 * * * ( 0 . 2201 ) − 1 . 4123 * * * ( 0 . 2179 ) Central African − 1 . 5468 * * * ( 0 . 2278 ) − 1 . 5468 * * * ( 0 . 2259 ) − 1 . 5474 * * * ( 0 . 2316 ) − 1 . 5467 * * * ( 0 . 2351 ) − 1 . 5466 * * * ( 0 . 2315 ) − 1 . 5469 * * * ( 0 . 2243 ) Hispanic − 1 . 6257 * * * ( 0 . 2288 ) − 1 . 6257 * * * ( 0 . 2378 ) − 1 . 6255 * * * ( 0 . 2296 ) − 1 . 6247 * * * ( 0 . 2345 ) − 1 . 6253 * * * ( 0 . 2354 ) − 1 . 6255 * * * ( 0 . 2330 ) Turkish − 1 . 7478 * * * ( 0 . 2227 ) − 1 . 7478 * * * ( 0 . 2258 ) − 1 . 7470 * * * ( 0 . 2356 ) − 1 . 7470 * * * ( 0 . 2336 ) − 1 . 7472 * * * ( 0 . 2235 ) − 1 . 7473 * * * ( 0 . 2219 ) Black American − 1 . 8436 * * * ( 0 . 2288 ) − 1 . 8436 * * * ( 0 . 2302 ) − 1 . 8424 * * * ( 0 . 2284 ) − 1 . 8420 * * * ( 0 . 2311 ) − 1 . 8426 * * * ( 0 . 2313 ) − 1 . 8429 * * * ( 0 . 2309 ) Asian − 2 . 1583 * * * ( 0 . 2340 ) − 2 . 1583 * * * ( 0 . 2359 ) − 2 . 1586 * * * ( 0 . 2294 ) − 2 . 1579 * * * ( 0 . 2299 ) − 2 . 1576 * * * ( 0 . 2269 ) − 2 . 1579 * * * ( 0 . 2286 ) Eastern European − 2 . 4170 * * * ( 0 . 2275 ) − 2 . 4170 * * * ( 0 . 2324 ) − 2 . 4168 * * * ( 0 . 2286 ) − 2 . 4169 * * * ( 0 . 2339 ) − 2 . 4170 * * * ( 0 . 2278 ) − 2 . 4170 * * * ( 0 . 2256 ) Panel B . Gender ( ref . : Male ) Female 0 . 0705 ( 0 . 1124 ) 0 . 0707 ( 0 . 1143 ) 0 . 0711 ( 0 . 1151 ) 0 . 0709 ( 0 . 1146 ) 0 . 0707 ( 0 . 1160 ) Panel C . Covariates ChatGPT sampling strategy Yes Yes Yes Yes Yes Yes Candidate occupation No No Yes Yes Yes Yes Job experience & location No No No Yes Yes Yes Job language requirements No No No No Yes Yes Job type , shift system , & work hours No No No No No Yes Panel D . Model parameters N 34 , 560 34 , 560 34 , 560 34 , 560 34 , 560 34 , 560 R 2 0 . 005 0 . 005 0 . 088 0 . 144 0 . 163 0 . 168 R 2 Adj . 0 . 005 0 . 005 0 . 087 0 . 143 0 . 161 0 . 166 AIC 276 , 647 276 , 649 273 , 697 271 , 530 270 , 774 270 , 595 BIC 276 , 782 276 , 793 274 , 027 271 , 961 271 , 298 271 , 220 Notes . Abbreviations and acronyms used : ref . ( reference group ) , N ( sample size ) , Adj . ( Adjusted ) , AIC ( Akaike information criterion ) , and BIC ( Bayesian information criterion ) . Model statistics are OLS coefficient estimates with standard errors between parentheses . Standard errors are corrected using cluster - robust wild bootstrapping with 2 , 000 replications . Clusters are defined at the vacancy level , given the correlation between the assignment of the candidates and the vacancies . * p < . 05 ; * * p < . 01 ; * * * p < . 001 . 30 Table 2 . OLS estimation of the moderation effect of gender on ChatGPT’s ethnic bias ( 1 ) ( 2 ) Intercept 66 . 9115 * * * ( 0 . 2993 ) 72 . 5865 * * * ( 1 . 0395 ) Panel A . Identity ( ref . : Dutch ) White American − 1 . 1338 * * * ( 0 . 3198 ) − 1 . 1346 * * * ( 0 . 3159 ) Arab − 1 . 6368 * * * ( 0 . 3213 ) − 1 . 6376 * * * ( 0 . 3200 ) Central African − 1 . 3688 * * * ( 0 . 3305 ) − 1 . 3690 * * * ( 0 . 3237 ) Hispanic − 1 . 3106 * * * ( 0 . 3220 ) − 1 . 3104 * * * ( 0 . 3329 ) Turkish − 0 . 8595 * * ( 0 . 3243 ) − 0 . 8587 * ( 0 . 3356 ) Black American − 1 . 3687 * * * ( 0 . 3180 ) − 1 . 3679 * * * ( 0 . 3312 ) Asian − 1 . 7234 * * * ( 0 . 3305 ) − 1 . 7234 * * * ( 0 . 3386 ) Eastern European − 1 . 8941 * * * ( 0 . 3286 ) − 1 . 8954 * * * ( 0 . 3311 ) Panel B . Gender ( ref . : Male ) Female 0 . 6063 ( 0 . 3221 ) 0 . 6061 ( 0 . 3207 ) Panel C . Identity x Gender ( ref . : Dutch x Male ) White American x Female 0 . 3551 ( 0 . 4598 ) 0 . 3555 ( 0 . 4617 ) Arab x Female 0 . 4504 ( 0 . 4490 ) 0 . 4506 ( 0 . 4438 ) Central African x Female − 0 . 3560 ( 0 . 4572 ) − 0 . 3559 ( 0 . 4553 ) Hispanic x Female − 0 . 6303 ( 0 . 4478 ) − 0 . 6303 ( 0 . 4603 ) Turkish x Female − 1 . 7765 * * * ( 0 . 4810 ) − 1 . 7771 * * * ( 0 . 4794 ) Black American x Female − 0 . 9499 * ( 0 . 4753 ) − 0 . 9503 * ( 0 . 4830 ) Asian x Female − 0 . 8696 ( 0 . 4586 ) − 0 . 8690 ( 0 . 4566 ) Eastern European x Female − 1 . 0459 * ( 0 . 4767 ) − 1 . 0432 * ( 0 . 4597 ) Panel D . Covariates ChatGPT sampling strategy Yes Yes Candidate occupation No Yes Job experience & location No Yes Job language requirements No Yes Job type , shift system , & work hours No Yes Panel E . Model parameters N 34 , 560 34 , 560 R 2 0 . 006 0 . 169 R 2 Adj . 0 . 005 0 . 167 AIC 276 , 642 270 , 584 BIC 276 , 853 271 , 277 Notes . Abbreviations and acronyms used : ref . ( reference group ) , N ( sample size ) , Adj . ( Adjusted ) , AIC ( Akaike information criterion ) , and BIC ( Bayesian information criterion ) . Model statistics are OLS coefficient estimates with standard errors between parentheses . Standard errors are corrected using cluster - robust wild bootstrapping with 2 , 000 replications . Clusters are defined at the vacancy level , given the correlation between the assignment of the candidates and the vacancies . * p < . 05 ; * * p < . 01 ; * * * p < . 001 . 31 Figures Figure 1 . Histogram of ChatGPT invitation scores by ethnic identity Notes . Green ( light ) bars represent binned relative frequencies of fictitious Arab - , Asian - , Black American - , Central African - , Eastern European - , Hispanic - , Turkish - , and White American - named candidates used in the simulated CV screening task . Purple ( dark ) bars represent those of Dutch - named applicants . 32 Figure 2 . Average ( predicted ) ChatGPT interview invitation scores by ethnic identity Notes . Points are predicted values or estimated marginal effects at the mean . Lines illustrate the 95 % confidence intervals of these values . Estimates are based on Model 1 in Table 1 . 33 Figure 3 . Average ChatGPT interview invitation scores by name , ethnic and gender identity Notes . Points represent average interview invitation scores for each name across ethnic identities and genders . These points are slightly vertically jittered to more easily discern distribution densities . Vertical lines depict score means by ethnic and gender identity . Panel A includes male names , whereas Panel B includes female names . 34 Figure 4 . Average ( predicted ) ChatGPT interview invitation probabilities by ethnic identity Notes . Values along the curves represent predicted interview probabilities or estimated marginal effects at the mean of candidates with a given ethnic identity at each possible cutoff score in [ 1 , 100 ] ∩ ℕ based on ChatGPT’s output . Estimates are derived from the logit model specification in Equation 4 ( see Section 2 . 3 . 3 ) . Lightly shaded ribbons illustrate the 95 % confidence intervals of these estimates . Panel A depicts the differences in predicted probabilities between Dutch candidates and candidates with another ethnic identity for all cutoff scores in [ 1 , 100 ] ∩ ℕ . Panel B zooms in on cutoff scores in [ 50 , 80 ) and shows the interview probabilities for each ethnic identity separately . Detailed results of the separate logit models are available on request . 35 Appendix Supplementary tables are available at https : / / osf . io / vezt7 / ? view _ only = 0fb1f53c502a4c7091f3003d972f93b3 .