Generate FAIR Literature Surveys with Scholarly Knowledge Graphs Allard Oelen L3S Research Center , Leibniz University Hannover , Germany oelen @ l3s . de Mohamad Yaser Jaradeh L3S Research Center , Leibniz University Hannover , Germany jaradeh @ l3s . de Markus Stocker TIB Leibniz Information Centre for Science and Technology , Germany markus . stocker @ tib . eu Sören Auer TIB Leibniz Information Centre for Science and Technology & L3S Research Center , Germany auer @ tib . eu ABSTRACT Reviewing scientific literature is a cumbersome , time consuming but crucial activity in research . Leveraging a scholarly knowledge graph , we present a methodology and a system for comparing scholarly literature , in particular research contributions describing the addressed problem , utilized materials , employed methods and yielded results . The system can be used by researchers to quickly get familiar with existing work in a specific research domain ( e . g . , a concrete research question or hypothesis ) . Additionally , it can be used to publish literature surveys following the FAIR Data Princi - ples . The methodology to create a research contribution comparison consists of multiple tasks , specifically : ( a ) finding similar contri - butions , ( b ) aligning contribution descriptions , ( c ) visualizing and finally ( d ) publishing the comparison . The methodology is imple - mented within the Open Research Knowledge Graph ( ORKG ) , a scholarly infrastructure that enables researchers to collaboratively describe , find and compare research contributions . We evaluate the implementation using data extracted from published review articles . The evaluation also addresses the FAIRness of comparisons published with the ORKG . KEYWORDS Scholarly Knowledge Comparison ; Scholarly Information Systems ; Comparison User Interface ; Digital Libraries ; Scholarly Communi - cation ; FAIR Data Principles 1 INTRODUCTION When conducting scientific research , reviewing the existing liter - ature is an essential activity [ 33 ] . Familiarity with the state - of - the - art is required to effectively contribute to advancing it and do relevant research . Mainly because published scholarly knowledge is unstructured [ 17 ] , it is currently very tedious to review existing literature . Relevant literature has to be found among hundreds and increasingly thousands of PDF articles . This activity is supported by library catalogs and online search engines , such as Scopus or Google Scholar [ 18 ] . Because the search is keyword based , typ - ically large numbers of articles are returned by search engines . Researchers have to manually identify the relevant papers . Having identified the relevant papers , the relevant pieces of information need to be extracted in order to obtain an overview of the literature . Overall , these are manual and time consuming steps . We argue that a key issue is that the scholarly knowledge communicated in the literature does not meet the FAIR Data Principles [ 40 ] . While PDF articles can be found and accessed ( assuming Open Access or an institutional subscription ) , the scholarly literature is insufficiently interoperable and reusable , especially for machines . For units more granular than the PDF article , such as a specific result , findability and accessibility score low even for humans . We present a methodology and its implementation integrated into the Open Research Knowledge Graph ( ORKG ) [ 15 ] that can be used to generate and publish literature surveys in form of machine actionable , comparable descriptions of research contributions . Ma - chine actionability of research contributions relates to the ability of machines to access and interpret the contribution data . The benefits for researchers of such an infrastructure are ( at least ) two - fold . Firstly , it supports researchers in creating state - of - the - art overviews for specific research problems efficiently . Secondly , it supports researchers in publishing literature surveys that adhere to the FAIR principles , thus contributing substantially to reuse of state - of - the - art overviews and therein contained information , for both humans and machines . Literature reviews are articles that focus on analysing existing literature . Among other things , reviews can be used to gain under - standing about a research problem or to identify further research directions [ 8 , 29 ] . Reviews can be used by authors to quickly ob - tain an overview of either emerging or mature research topics [ 36 ] . Review papers are important for research fields to develop . When review papers are lacking , the development of a research field is weakened [ 38 ] . Compiling literature review papers is a complicated task [ 39 ] and is often more time consuming than performing origi - nal research [ 38 ] . The structure of such articles often consists of tables that compare published research contributions . Although in the literature the terms “literature review” and “literature sur - vey” are sometimes used interchangeably , we make the following distinction . We refer to the tables in review articles as literature surveys . Together with a ( textual ) analysis and explanation , they form the literature review . The state - of - the - art ( SoTA ) analysis is a special kind of literature review with the objective of comparing the latest and most relevant papers in a specific domain . We implement the presented methodology in the ORKG . The ORKG is a scholarly infrastructure designed to acquire , publish and process structured scholarly knowledge published in the lit - erature [ 14 ] . ORKG is part of a larger research agenda aiming a r X i v : 2006 . 01747v1 [ c s . D L ] 2 J un 2020 at machine actionable scholarly knowledge that understands the ability to more efficiently compare literature as a key feature . We tackle the following research questions : ( 1 ) How to generate literature surveys using scholarly knowl - edge graphs ? ( 2 ) How to ensure that published literature surveys comply with the FAIR principles ? ( 3 ) How to effectively specify and visualize literature surveys in a user interface ? In support of the first research question , we present a methodol - ogy that describes the steps required to generate literature surveys . In support of the second research question , we describe how the FAIRness of the published literature review is ensured . Finally , in support of the third research question , we demonstrate how the methodology is implemented within the ORKG . The paper is structured as follows . Section 2 motivates the work . Section 3 reviews related work . Section 4 presents the system de - sign , the underlying methodology and its implementation . Section 5 explains how the knowledge graph is populated with data . Section 6 presents the evaluation of the system , specifically system FAIRness and performance . Finally , Section 7 discusses the presented and future work . 2 USE CASES We motivate our work by means of two use cases that underscore the usefulness of a literature survey generation system . In the first use case , a researcher wants to obtain an overview on state - of - the - art research addressing a specific problem . The second use case describes how a researcher can publish a FAIR compliant literature review with the ORKG . Familiarize with the state - of - the - art . A state - of - the - art ( SoTA ) analysis reviews new and emerging research . They are useful for multiple reasons . Firstly , they provide a broad overview of a re - search problem and support understanding . Secondly , they juxta - pose different approaches for a problem . Thirdly , they can support claims on why certain research is relevant by giving an overview of the breadth of research addressing a problem . The proposed approach enables automated generation of surveys to quickly ob - tain an overview of state - of - the - art research as well as sharing of surveys for others to reuse . Publishing of literature reviews . Literature reviews typically con - sist of multiple ( survey ) tables in which different approaches from original papers are compared based on a set of properties . These tables can be seen as the main contribution and most informative part of the review paper , since the tables juxtapose and compare existing work . Comparison tables are published in review papers as static content in PDF documents . This presentational format is generated from datasets that typically contain more ( structured ) information than what is presented in the published table . However , the additional information is not published . It is “dark data” which is not stored or indexed and likely lost over time [ 12 ] . Furthermore , published tables are not machine actionable . Their overall low FAIRness hinders reusability of the published content . With the presented service , it is possible to publish a literature survey with high FAIRness , i . e . that is compliant with the FAIR principles to a high degree . Section 3 discusses this aspect in more details . Summary of weaknesses of the current approach to literature review . The weaknesses of the current approach to literature review can be summarized as follows : • Static – reviews are static , since once published as PDF they are rarely updated and there are no possibilities or incen - tives for creating new or updated reviews for considerable time . • Lack of machine assistance – machine assistance is hardly possible , since the PDF representation of reviews is only human readable and relevant raw data is mostly not pub - lished along with the review . • Delay – reviews are produced and published with signifi - cant delay ( often years ) after original research work was done . • Coverage – due to the amount of work required , reviews are often only performed for relatively popular research topics and are stale or missing for less popular topics . • Lacking collaboration – collaboration on reviews is not pos - sible and reviews currently represent only the viewpoint of the few authors not the community . • Missing overarching systematic semantic representation – the overlap between different reviews and related work sections in individual original research papers is not ex - plicit and cannot be exploited . We deem that these weaknesses of the current approach to schol - arly literature review and synthesis significantly hinder scientific progress . 3 RELATED WORK The task of comparing research contributions can be reviewed in light of the more general task of comparing resources ( or entities ) in a knowledge graph . While this is a well - known task in multi - ple domains ( for instance in e - commerce systems [ 42 ] ) , not much work has focused on comparison in knowledge graphs , specifically . One of the few works with this focus is by Petrova et al . [ 28 ] who created a framework for comparing entities in RDF graphs using SPARQL queries . In order to compare contributions , they first have to be found . Finding is an information retrieval problem . As a well - known technique , TF - IDF [ 21 ] can be used for this task . More sophisticated techniques can be used to determine the structural similarity between graphs ( e . g . , [ 20 ] ) and matching semantically similar predicates . This relates to dataset interlinking [ 1 ] or more generally ontology alignment [ 34 ] . For property alignment , tech - niques of interest include edit distance ( e . g . , Jaro - Winkler [ 41 ] or Levenshtein [ 19 ] ) and vector distance . Gromann and Declerck [ 10 ] found that fastText [ 4 ] performs best for ontology alignment . In light of the FAIR Data Principles [ 40 ] , scholarly data should be Findable , Accessible , Interoperable and Reusable both for humans and machines . Due to the publication format , literature survey tables published in scholarly articles weakly adhere to the FAIR guidelines , particularly so for machines . Scholarly data should be considered first - class objects [ 35 ] , including data used to create literature surveys . Rodríguez - Iglesias et al . [ 30 ] describe the diffi - culties of making data FAIR within the plant sciences . They argue 2 that it is more complicated than reformatting data . On the other hand they suggest that most FAIR principles can be implemented relatively easily by using off - the - shelf technologies . Boeckhout et al . [ 3 ] argue that the FAIR principles alone are not sufficient to lead to responsible data sharing . More applied principles are needed to ensure better scholarly data . This claim is supported by the findings of Mons et al . [ 22 ] who suggest that there are very diverse interpretations of the guidelines . In their work , they try to clarify what is FAIR and what is not . An efficient literature comparison relies on scholarly knowl - edge being represented in a structured way . There is substantial related work on representing scholarly knowledge in structured form [ 31 ] . Building on the work of numerous philosophers of sci - ence , Hars [ 11 ] proposed a comprehensive scientific knowledge model that includes concepts such as theory , methodology and statement . More recently , ontologies were engineered to describe different aspects of the scholarly communication process . Semantic Publishing and Referencing ( SPAR ) 1 is a collection of ontologies that can be used to describe scholarly publishing and referencing of documents [ 5 , 9 , 26 , 27 ] . Ruiz Iniesta and Corcho [ 31 ] reviewed the state - of - the - art ontologies to describe scholarly articles . Sateli and Witte [ 32 ] use some of these scholarly ontologies to add se - mantic representations of scholarly articles to the Linked Open Data cloud . A literature survey comparing scholarly ontologies is available via the ORKG . 2 Most of these ontologies are designed to capture metadata about and structure of scholarly articles , not the content communicated in articles . Another literature survey is created to compare approaches for semantically representing scholarly communication . 3 An initial attempt for semantifying review articles was done in [ 7 ] . The work comprises a relatively rigid ontology for describ - ing contributions ( mainly centered around research problems , ap - proaches , implementations and evaluations ) and a prototypical implementation using Semantic MediaWiki . We relax this con - straint , since we are not limited by a rigid ontology schema but rather allow arbitrary domain - specific semantic structures for re - search contributions . The work by Vahdati et al . [ 37 ] focuses on semantic article representations for generating literature overviews . Their method is to use crowdsourcing to generate the overviews . Kohl et al . [ 16 ] present CADIMA , a system that supports systematic literature reviews . The tool supports the formal process of perform - ing a literature review but does , for example , not publish data in machine actionable form for reuse . 4 SYSTEM DESIGN We now present the system design of the literature comparison service . It consists of a methodology that describes how to perform a comparison of research contributions . An early version of this methodology has been presented at the 3rd SciKnow workshop [ 24 ] . The methodology consists of five steps : 1 ) finding comparison candidates , 2 ) selecting related statements , 3 ) aligning contribution descriptions , 4 ) visualizing comparisons and 5 ) publishing FAIR comparisons . The methodology is depicted in Figure 1 . First , we 1 http : / / purl . org / spar / { cito , c4o , fabio , biro , pro , pso , pwo , doco , deo } 2 https : / / www . orkg . org / orkg / comparison / R8342 3 https : / / www . orkg . org / orkg / comparison / R8364 Select comparison candidates Select related statements Align contribution descriptions Publish comparison Visualize comparison • Find similar candidates to compare • Manually select contributions to compare • Select all statements from the comparison contributions • Do this until a predeﬁned depth has been reached • Align contributions properties that are the same ( i . e . , same ID ) • Use word embeddings to align properties that are similar • Hide properties that are not shared among contributions • Let users customize the comparison • Publish a FAIR comparison including relevant metadata • Ensure the persistency of the ( meta ) data Figure 1 : Research contribution comparison methodology . discuss the data structure of the ORKG , which forms the foundation of the comparison . Then , each step of the methodology is described in more detail . Finally , we discuss the implementation . 4 . 1 ORKG ontology In ORKG , each paper is typed as paper class . A paper consists of at least one research contribution , which addresses at least one research problem . Research contributions consist of contribution data that de - scribe the contribution . For instance , a paper in Computer Science might have descriptions for materials , methods , implementation and results as contribution data . These predefined core concepts can be easily extended with domain specific research problems , methods , etc . in ORKG curation using crowdsourcing or other cu - ration approaches . The underlying data structure uses the notion of statements . Statements are triples that consist of a subject , a predicate ( also called a property ) and an object . The granularity of a comparison is at the research contribution , meaning that con - tributions are compared rather than papers . For simplicity , we use the terms “paper comparison” and “contribution comparison” interchangeably . Because a comparison happens on contribution level , it is possible to compare specific elements of a paper instead of the complete paper . The benefit of this is that a comparison does not contain data from irrelevant contributions . The ORKG OWL ontology is available online . 4 4 . 2 Select comparison candidates To perform a comparison , a starting contribution is needed . This contribution is called main contribution and is always manually selected by a user . The main contribution is compared against other comparison contributions . There are two different approaches for selecting the comparison contributions . The first approach automatically selects comparison contributions based on similarity . The second approach lets users manually select contributions . 4 . 2 . 1 Findsimilar contributions . Comparingcontributionsmakes only sense when contributions can sensibly be compared . For ex - ample , it does not make ( much ) sense to compare a biology paper to a history paper . We thus argue that it makes only sense to compare contributions that are similar . More specifically , contributions that share the same ( or a similar set of ) properties are good compari - son candidates . For instance , a paper about question answering has the property orkg : disambiguationTask 5 and another paper is 4 https : / / gitlab . com / TIBHannover / orkg / orkg - ontology 5 orkg : denotes the ontology of the ORKG system described in Section 4 . 1 3 Figure 2 : Implementation of the first step of the methodol - ogy : the selection of comparison candidates . Showing both the similarity - based and the manual selection approaches . Figure 3 : Box showing the manually selected contributions . using the same property to describe what disambiguation tasks are performed . Since they share the same property it makes them likely candidates for comparison . Finding similar contributions is therefore based on finding contributions that share the same or similar informative description properties . To achieve this , each comparison contribution is converted into a string by concatenat - ing all properties of the contribution . TF - IDF [ 21 ] is used to query these strings with the string of the main contribution as query . The search returns the most similar contributions by weighting the most informative properties higher due to TF - IDF . The top - k contributions are selected and form a set of contributions that are used in the next step . Figure 2 displays how the similar contribution selection is im - plemented . As depicted , three similar contributions are suggested to the user ( with the corresponding similarity percentage being displayed next to paper title ) . These suggested contributions can be directly compared . 4 . 2 . 2 Manual selection . There are scenarios where comparison based on similarity computation is not suitable or desired . For example , a researcher wants to compare a specific set of implemen - tations to see which performs best . Therefore , the manual selection method is implemented in a similar fashion to an e - commerce shop - ping cart . When the “Add to comparison” checkbox is checked , a box appears listing the selected contributions ( Figure 3 ) . 4 . 3 Select related statements This step selects the statements from the graph related to the set of contributions selected in the previous step . Statements are selected transitively to match contributions in subject or object position . This search is performed until a predefined maximum transitive depth δ has been reached . The intuition is that the deeper a property is nested the less likely is its relevance for the comparison . The process of selecting statements is repeated until depth δ = 5 is reached . This number is chosen empirically to include statements that are not directly related to the contribution , but to exclude statements that are less relevant because they are nested too deep . 4 . 4 Align contribution descriptions As described in the first step , comparisons are built using shared or similar properties of contributions . In case the same property has been used between contributions , these properties are grouped and form one comparison row . However , often different properties are used to describe the same concept . This occurs for various reasons . The most obvious reason is when two different ontologies are used to describe the same property . For example , for describing the population of a city , DBpedia uses dbo : populationTotal while WikiData uses WikiData : population ( actually the property identifier is P1082 ; for the purpose here we use the label ) . When comparing contributions , these properties should be considered as equivalent . Especially for community - created knowledge graphs , differently identified properties likely exist that are , in fact , equivalent . To overcome this problem , we use pre - trained fastText [ 4 ] word embeddings to determine the similarity of properties . If the sim - ilarity is higher than a predetermined threshold τ , the properties are considered equivalent and are grouped . This happens when the similarity threshold τ ≥ 0 . 9 ( also empirically determined ) . In the end , each group of properties will be visualized as one row in the comparison table . The result of this step is a list of statements for each contribution , where similar properties are grouped . Based on this similarity matrix γ is generated γ p i = (cid:104) cos ( −→ p i , −→ p j ) (cid:105) ( 1 ) with cos ( . ) as the cosine similarity of vector embeddings for property pairs ( p i , p j ) ∈ P , whereby P is the set of all contributions . Furthermore , we create a mask matrix Φ that selects properties of contributions c i ∈ C , whereby C is the set of contributions to be compared . Formally , Φ i , j = (cid:40) 1 if p j ∈ c i 0 otherwise ( 2 ) Next , for each selected property p we create the matrix φ that slices Φ to include only similar properties . Formally , φ i , j = ( Φ i , j ) c i ∈C p j ∈ sim ( p ) ( 3 ) where sim ( p ) is the set of properties with similarity values γ [ p ] ≥ τ with property p . Finally , φ is used to efficiently compute the com - mon set of properties [ 14 ] . This process is displayed in Algorithm 1 . 4 Algorithm 1 Align contribution descriptions 1 : procedure AlignProperties ( properties , threshold ) 2 : for each property p 1 ∈ properties do 3 : for each property p 2 ∈ properties do 4 : similarity ← cos ( Embb ( p 1 ) , Embb ( p 2 ) ) 5 : if similarity > threshold then 6 : similarProps ← similarProps ∪ { p 1 , p 2 } return similarProps 4 . 5 Visualize comparison The next step of the workflow is to visualize the comparison and present the data in a human understandable format . Tabular format is often appropriate for visualizing comparisons since tables provide a good overview of data . Another aspect of the visualization is determining which properties should be displayed and which ones should be hidden . A property is displayed when it is shared among a predetermined amount α of contributions , where α mainly depends on comparison use and can be determined based on the total amount of contributions in the comparison . By default , only properties that are common to at least two contributions ( α ≥ 2 ) are displayed . Another aspect of comparison visualization is the possibility to customize the resulting table . This is needed because of the similarity - based matching of properties and the use of predeter - mined thresholds . For example , users should be able to enable or disable properties . They should also get feedback on property provenance ( i . e . , the property’s path in the graph ) . Ultimately , this contributes to a better user experience , with the possibility to manually correct mistakes made by the system . Figure 4 displays a comparison for research contributions related to visualization tools published in the literature . In this example , four properties are displayed . Literals are displayed as plain text while resources are displayed as links . When a resource link is selected , a popup is displayed showing the statements related to this resource . The UI implements some additional features that are particularly useful to compare research contributions . Customization . Userscancustomizecomparisonsincludingtrans - posing the table as well as hiding and rearranging the properties . Especially the option to hide properties is helpful when contribu - tions with many statements are compared . Only properties consid - ered relevant to the user can be selected to display . Customizing the comparison table can be useful before exporting or sharing the comparison . Sharing and persistence . Comparisons can be shared using a per - sistent link . Especially when sharing the comparison for research purposes , it is important to refer to the original comparison . Since contribution descriptions may change over time comparisons may also change . To support persistency , the whole state of the compar - ison is stored in a document - oriented database and retrieved when the permalink is invoked . Export . It is possible to export comparisons in different output formats such as PDF , CSV , RDF and L A TEX . The L A TEX export is useful for direct integration in research papers . Together with the L A TEX table , a BibTeX file containing the bibliographic information of the papers used in the comparison is also generated . Also , a Figure 4 : Comparison of research contributions related to visualization tools . persistent link referring back to the comparison in ORKG is showed as table footnote . 4 . 6 Publish comparison Visualized and customized comparison tables can be stored . Storing tables is part of the publishing process and therefore only needed when a generated table is going to be used in a paper . In order to regenerate the table the whole state of the comparison should be saved . The knowledge graph from which the comparison was generated changes over time and thus storing just the URIs of the respective papers would not suffice . While saving a comparison , the user can provide additional metadata to ensure findability , an aspect of the FAIR principles . Metadata include a comparison title , which would normally consist of a one sentence description of the comparison . Additionally , a longer textual description can be provided . This metadata is extended with machine generated data , such as the creation date and the creator of the comparison . The metadata is stored in the knowledge graph to support easy access and interoperability . In Figure 5 , the structure of the metadata is displayed using the Dublin Core Metadata Terms 6 . The compari - son data itself is stored in a document - oriented database . An RDF export of both the metadata and the comparison data can be gen - erated . The comparison data is modeled with the RDF Data Cube Vocabulary 7 . A unique identifier is attached when the comparison is saved . This ID is used when the comparison is shared or when it is referenced in a paper . The literature comparison can also be per - formed without publishing . Although the workflow and the steps to create a comparison stay the same , the goal is different . Instead of creating a comparison that will be published and referenced in a paper , the comparison will be used by the researcher herself . 4 . 7 Technical details The user interface of the comparison feature is seamlessly inte - grated with the ORKG front end , which is written in JavaScript and is publicly available 8 . The back end of the comparison feature is a service separate from the ORKG back end written in Python and also available Open Source 9 . The comparison back end is respon - sible for step two and three of the comparison methodology . The 6 https : / / dublincore . org / specifications / dublin - core / dcmi - terms 7 https : / / www . w3 . org / TR / vocab - data - cube 8 https : / / gitlab . com / TIBHannover / orkg / orkg - frontend 9 https : / / gitlab . com / TIBHannover / orkg / orkg - similarity 5 dcterms : description dcterms : date dcterms : creator : hasUrl dcterms : license Comparison String Date User String String Figure 5 : The graph structure of the metadata for a pub - lished comparison . The dcterms : prefix denotes the Dublin Core Metadata Terms ontology . input in step two is the set of contribution IDs . The API selects the related statements and aligns the properties and returns the data needed to visualize the comparison . This data includes the list of papers , list of all properties and the values per property . 5 DATA COLLECTION In order to generate useful literature reviews it is crucial for the knowledge graph to contain sufficient and relevant papers . Popu - lating the knowledge graph with high quality paper descriptions it not straightforward . Structured descriptions of papers should be created in such a way that it is possible to compare papers based on shared properties . Both published papers and papers that will be published in the future should be added to the ORKG , retrospec - tively or prospectively . Although a comprehensive description on how to populate the ORKG is out - of - scope here , we now briefly describe how we envision populating the ORKG in a manner that would facilitate comparing contributions . Prospectively , authors can become part of generating structured descriptions of their papers . This should be done in a crowdsourced manner and can become part of the paper submission process . In - put templates that collect relevant properties can be used to ensure structured and comparable paper descriptions . Retrospectively , automated ( machine learning ) methods can be helpful ensure scal - ability of the process of adding a paper . 5 . 1 Leverage legacy review paper tables To populate the ORKG with comparable paper descriptions , we leverage the data published in review papers . Review papers consist of high quality , curated and often structured data that is collected from a set of papers that address the same ( or a similar ) research problem . Hence , using reviews to populate a scholarly knowledge graph is a relatively straightforward approach to obtain high quality structured paper descriptions . We now present a methodology to convert survey paper data into a knowledge graph structure . The steps are as follows : ( 1 ) Survey paper selection . The first step is the selection of survey papers that are suitable for building a knowledge graph . Firstly , the survey should compare peer - reviewed scientific articles . For instance , a comparison of different systems without a reference to peer - reviewed work is not suitable for the scholarly knowledge graph . Secondly , the review should compare the papers’ content in a structured way and should not merely list work in a field . Especially reviews that present their results and literature compar - isons in tabular format are suitable . The result of this step is a list of papers that will be added to the ORKG . ( 2 ) Table selection . Given the selected survey papers , tables have to be selected . Some surveys contain only one table while in others multiple tables are presented . In some cases a collection of tables can be joined into one larger table . ( 3 ) Data modeling . Given the selected tables , asuitable graph structure has to be determined . The data structure has to be modeled . For instance , when implemented systems are compared , asuitablestructurecouldbe : [ has implementation ] - > System name . The referenced system can be described with a list of properties to be compared . Additionally , a research problem has to be defined , which is typically the same for all papers that are part of the table . ( 4 ) Metadata collection . Next , the metadata for the papers that are referenced in the survey table is collected . In case a referenced paper has a DOI 10 , the metadata can be auto - matically retrieved via a lookup service ( e . g . Crossref 11 ) . Otherwise , at least the title , authors and publication date have to be collected . ( 5 ) Data ingestion . Finally , the paper data is ingestion into the knowledge graph . The paper data consists of both the paper’s metadata and the extracted data from the compari - son table . This does not result in a single description of the survey paper . Each paper referenced in the survey table is ingested individually . In order to speed up the process of adding papers , we developed a Python package 12 that has a function to add a paper to the knowledge graph . This methodology has been used to populate the ORKG with com - parable paper data . The data is used to evaluate the presented literature review tool . The imported paper data is not only useful for the evaluation , but does also provide significant value to the ORKG itself . In total , four review papers were selected for importing into the ORKG . The Python script for importing the table data is available online . 13 From those papers , 12 different tables were imported . Together , 169 papers were reviewed in those four survey papers . This resulted in a total amount of 3750 statements being added to the knowledge graph . Table 1 lists the imported review papers and tables . The survey papers address different research problems . Figure 6 depicts an excerpt of the resulting graph for one particular paper . A set of comparison tables made with the imported data is available online . 14 This list includes some alternative comparison tables that were generated with the same data . 6 EVALUATION In this section , we present an evaluation of multiple aspects of the presented comparison methodology and implementation . Firstly , we evaluate information representation . Then , we evaluate the 10 Digital Object Identifier 11 https : / / www . crossref . org 12 https : / / gitlab . com / TIBHannover / orkg / orkg - pypi 13 https : / / gitlab . com / TIBHannover / orkg / orkg - papers 14 https : / / orkg . org / orkg / featured - comparisons 6 Table 1 : List of imported survey tables in the ORKG . The paper and table reference can be used to identify the original table . Paper reference Table reference Research problem Papers ORKG representation Information loss Bikakis and Sellis [ 2 ] Table 1 Generic visualizations 11 https : / / orkg . org / orkg / c / pdLJDk No Bikakis and Sellis [ 2 ] Table 2 Graph visualizations 21 https : / / orkg . org / orkg / c / Rx476Z No Diefenbach et al . [ 6 ] Table 2 Question answering evaluations 33 https : / / orkg . org / orkg / c / gaVisD No Diefenbach et al . [ 6 ] Table 3 , 4 , 5 , 6 Question answering systems 26 https : / / orkg . org / orkg / c / IuEWl2 No Hussain and Asghar [ 13 ] Table 4 Author name disambiguation 5 https : / / orkg . org / orkg / c / vDxKdr No Hussain and Asghar [ 13 ] Table 5 Author name disambiguation 6 https : / / orkg . org / orkg / c / XXg8Wg No Hussain and Asghar [ 13 ] Table 6 Author name disambiguation 9 https : / / orkg . org / orkg / c / 9rOwPV No Hussain and Asghar [ 13 ] Table 7 Author name disambiguation 6 https : / / orkg . org / orkg / c / mB7kIK No Naidu et al . [ 23 ] Table 4 Text summarization 52 https : / / orkg . org / orkg / c / OUqYB9 No has author has author has publication year has contribution Template - based question answering over RDF data ChristinaUnger . . . 2012 has implementation has evaluation Contribution 1 phrase mapping task disambiguation task TBSL Stringsimilarity Local disambiguation has question amount f - measure Evaluation 1 0 . 42 50 Figure 6 : Partial graph structure of an imported paper . Or - ange colored resources indicate potentially interesting val - ues for a paper comparison . FAIRness of published reviews . Finally , we present a performance evaluation that tests the scalability . 6 . 1 Information representation This part of the evaluation focuses on the aspect of information representation . We use the data from the imported review papers , as described in Section 5 . In order to build and publish useful and correct literature reviews , at a minimum our service should display the same information that was originally presented in the review tables . This means that there should not be information loss when review tables are published using our service . If there is no information loss , it means our service can be used as an alternative to the current way of publishing review tables . Apart from generating the same table , the added value comes from the ability to aggregate new ( tabular ) views using the same data as well as the increased FAIRness of the data published via our service . For each of the imported review tables , listed in Table 1 , we can evaluate whether the same table can be generated with our service . For this , we have compared the table from the review paper to the table generated by the ORKG comparison service . A collection of 169 paper with 9 distinct literature views / tables are part of this evaluation . These tables can be viewed online , the links are listed in the “ORKG representation” column . The results of this evaluation are displayed in the same table , in column “Information loss” . As the results show , using our service it is possible to recreate the same tabular views as originally published in the review papers . 6 . 2 FAIR data evaluation As described before , with the presented service it is possible to publish a generated comparison that adheres with the FAIR princi - ples . Because the service leverages a knowledge graph to generate and save comparisons , complying with the FAIR principles is more obvious for the ORKG comparison service than for tables in pub - lished PDF articles . In order to evaluate the FAIRness of a published comparison , we evaluate each of the four FAIR principles in de - tail . Wilkinson et al . [ 40 ] described each principle by assigning sub - principles . 15 We discuss the relevant sub - principles and ex - plain how they are met . We use the term ( meta ) data to refer to both the actual comparison data ( i . e . , the data that is used to cre - ate the comparison table ) and the associated metadata ( e . g . , the title , description and creator of a comparison ) . Table 2 presents an overview for the evaluation of the FAIR principles . Findable . To make data findable for both humans and machines ( i . e . , agents ) , a unique and persistent identifier should be attached to the data ( F1 ) . Additionally , metadata should describe the data ( F2 ) . In the metadata , the unique identifier of the data should be mentioned ( F3 ) . Also a search interface should be available to find the data ( F4 ) . To ensure the findability of comparisons , users can title and describe them . Furthermore , machine generated metadata is attached to a comparison ( e . g . , the number of papers and the creation date ) . A unique identifier is generated and attached to the data and included in the metadata . Finally , the ORKG search interface allows users to search the whole graph and has a dedicated filter to specifically find comparisons . Additionally , comparisons can be indexed and found by third - party search engines ( such as Google or Bing ) . Accessible . Having found data , agents need to know how to access it . This principle is primarily about using accessible stan - dardised communication protocols ( A1 ) . Additionally , metadata should be available even when the data is not ( A2 ) . The metadata is 15 For a more detailed definition of the FAIR principles , see : https : / / go - fair . org / fair - principles 7 part of the knowledge graph , which can be accessed via the HTTP protocol . The data can be accessed without authentication . To support A2 , the metadata and the actual comparison data are stored separately . Therefore , it is possible to access only metadata when the original data is not available anymore ( for example when data is retracted by the author ) . Interoperable . To ensure the interoperability of data , it should use a formal language for knowledge representation ( I1 ) and should use vocabularies that are FAIR ( I2 ) . Finally , references or links to other ( meta ) data should be made ( I3 ) . As argued before , thanks to highly structured data and the integration of shared vocabularies , interoperability is an inherent feature of knowledge graphs . Data is ( partially ) described using the ORKG core ontology and other ontologies we use to canonicalize the representation of relevant information content types . Links to other data are present in the knowledge graph . For example , if a comparison uses the “Web” resource to specify the domain of an application , this resource is generic , can be shared among paper descriptions and comparisons , and can be described in more detail , independently of a particular comparison . Reusable . Finally , data should be reuseable . This can be accom - plished by adding relevant ( meta ) data ( R1 ) . Required are an ac - cessible data license ( R1 . 1 ) and detailed provenance ( R1 . 2 ) data . Finally , ( meta ) data should use community standards to describe data ( R1 . 3 ) . It is possible to add additional metadata to a comparison , e . g . metadata about the scope of the comparison , which could be a reference to the paper in which the comparison is being used . The metadata is complemented with the metadata that is already part of the Findability principle , e . g . provenance data about the creator of the comparison . The data license of the graph data is CC BY - SA 16 ( Attribution - ShareAlike ) , which allows reuse of the data . There is currently no community standard to describe the comparison data . However , standard ontologies are used to describe metadata ( e . g . , Dublin Core ) . The evaluation of the FAIR principles shows that comparisons pub - lished with our service rank high in FAIRness , which can be even further increased with some effort from users . Users are mainly responsible for adding the correct information to the comparison and reuse vocabularies . Otherwise , findability , accessibility and to some extent also interoperability are largely handled by the service . 6 . 3 Performance evaluation In order to evaluate the performance of the overall comparison , we compared the implemented ORKG approach to a naive approach for comparing multiple resources . The naive approach compares each property against all other properties to perform the property alignment . Table 3 shows the time needed to generate comparisons , for both the naive and the ORKG approach . In total , eight papers are compared with on average ten properties per paper . In the naive approach , the “Align contribution descriptions” step is not scaling well , since each property is compared against all others . If multiple contributions are selected , the number of property simi - larity checks grows exponentially . Table 3 shows that the ORKG approach outperforms the naive approach . The total number of 16 https : / / creativecommons . org / licenses / by - sa / 2 . 0 Table 2 : Overview of FAIR principles compliance . Principle Level Explanation Findable F1 3 Unique IDs exist , DOI assignment for future work F2 2 Machine and user generated metadata is attached F3 1 Properties used to link data to metadata F4 1 Comparisons are findable via a search interface Accessible A1 2 Data is accessed over HTTP ( via REST or a user in - terface ) , requires user effort to integrate the ORKG API specification A1 . 1 1 The protocol is free and widely used A1 . 2 1 No authentication is required to access the data A2 1 Metadata is stored in a persistent way and available without the data itself Interoperable I1 1 RDF ( with type assertions ) and CSV export of com - parisons I2 2 Reuse of ontologies where possible ( ORKG core , Dublin core , RDF Data Cube Vocabulary ) . User re - sponsible for other ontology reuse . I3 3 For comparisons , the compared paper metadata is linked . Morereferencesareneededandcanbecreated by users . Reusable R1 1 Machine and user generated metadata is created while publishing R1 . 1 1 CC - BY SA license R1 . 2 1 If a registered user publishes a comparison , the user is associated with the published data R1 . 3 2 Users can describe contributions using domain - relevant ontologies 1 = Yes ; 2 = Yes , requires user effort ; 3 = Partially / future work Table 3 : Time ( in seconds ) to perform comparisons with 2 - 8 contributions using the naive and ORKG approaches . Number of compared research contributions 2 3 4 5 6 7 8 Naive 0 . 00026 0 . 1714 0 . 763 4 . 99 112 . 74 1772 . 8 14421 ORKG 0 . 0035 0 . 0013 0 . 01158 0 . 02 0 . 0206 0 . 0189 0 . 0204 papers used for the evaluation is limited to eight because the naive approach does not scale to larger sets . 7 DISCUSSION & FUTURE WORK One of the aims of the contribution comparison functionality is to support literature reviews and make this activity less cumbersome and time consuming for researchers . To live up to this aim , more structured contribution descriptions are needed . Existing scholarly knowledge graph initiatives focus primarily on scholarly metadata , while with ORKG we focus on making the actual research contribu - tions machine readable . Currently , the ORKG does not yet contain sufficient contribution descriptions in order for the comparison functionally to be practically useful for researchers . Furthermore , for an evaluation of the effectiveness of certain components of the methodology ( such as finding related papers or aligning similar 8 properties ) , more contribution data is needed . Publishing surveys does not rely on data quantity and is therefore evaluated more extensively in this work . The performance evaluation results in - dicate that the comparison feature performs well . This means the technical infrastructure is in place for the literature survey service . In the evaluation , we focused on the aspects of the system that are necessary for researchers to use the system in practice . The information representation evaluation is a straightforward evalu - ation to see if existing survey tables can be regenerated with the ORKG . This is a minimal requirement for researchers when using the system , since they should at least be able to recreate tables . This evaluation does not give insight to the usefulness and usabil - ity of the system , but still provides an indication that the service can be successfully used to publish literature surveys . One of the reasons for using the service is that also “dark data” in comparisons is published ( as discussed in Section 2 ) . Another interesting aspect of the service is that published litera - ture surveys rank high in FAIRness . Therefore , the second part of the evaluation focuses on how the FAIR principles are met . Merely publishing data as RDF is not sufficient to fully meet the FAIR princi - ples . Hence , we conducted a more detailed evaluation that describes how the service complies with each sub - principle . Since FAIR is not a standard , the principles are permissive and not prescriptive [ 22 ] . No technical requirements are specified . Both the implementation and evaluation of the guidelines are therefore subject to interpreta - tion . With respect to data interoperability and reusability , certain aspects of the service can be improved . For example , to improve interoperability , the contribution data should be reusing existing vocabularies where possible . Additionally , although most of FAIR - ification is done by the system , the researcher is responsible for adding correct and relevant metadata while publishing a survey . As indicated earlier , the usefulness of the presented tool depends on the number of papers present in the knowledge graph . Therefore , future work will focus on data collection , both in a crowdsourced and automated manner . We plan on extending the methodology presented in Section 5 with automated extraction of data and tables from literature review papers . With the extracted review data , the knowledge graph can be extended more quickly than the previously presented manual method . It could form the basis of a high quality scholarly knowledge graph that contains relevant and FAIR survey table data . Furthermore , in the future we will assign ( DataCite ) DOIs to published surveys . They will serve as a persistent identifier for the survey data [ 25 ] . 8 CONCLUSION Reviewing existing literature is an important but cumbersome and time consuming activity . To address this problem , we presented a methodology and service that can be used to generate literature surveys from a scholarly knowledge graph . This service can be used by researchers in order to get familiar with existing literature . Addi - tionally , the tool can be used to publish literature surveys in a way that they largely adhere to the FAIR data principles . The presented methodology addresses multiple aspects , including finding suitable contributions , aligning contribution descriptions , visualization and publishing . The methodology is implemented within the Open Re - search Knowledge Graph ( ORKG ) . Since the comparison relies on structured scholarly knowledge , we discussed how to populate the ORKG with relevant data . This is done by extracting tabular survey data from existing literature reviews . In order to evaluate whether the proposed service can be used to publish literature surveys , the original survey table representations were compared with the ones generated by our service . As the results indicate , it is possible to use the service as an addition or potentially even replacement of the current publishing approach , since the same tables can be generated . The evaluation also showed how the published literature surveys largely adhere to the FAIR data principles . This is crucial for data reusability and machine actionability . To conclude , the proposed literature comparison service addresses multiple weaknesses of the current survey publishing approach and can be used by researchers to generate , publish and reuse literature surveys . ACKNOWLEDGMENTS This work was co - funded by the European Research Council for the project ScienceGRAPH ( Grant agreement ID : 819536 ) and the TIB Leibniz Information Centre for Science and Technology . We want to thank Kheir Eddine Farfar for his contributions to this work . REFERENCES [ 1 ] SamurAraujo , JanHidders , DanielSchwabe , andArjenP . DeVries . 2011 . SERIMI - Resource description similarity , RDF instance matching and interlinking . CEUR Workshop Proceedings 814 ( 2011 ) , 246 – 247 . [ 2 ] Nikos Bikakis and Timos Sellis . 2016 . Exploration and visualization in the web of big linked data : A survey of the state of the art . CEUR Workshop Proceedings 1558 ( 2016 ) . [ 3 ] Martin Boeckhout , Gerhard A . Zielhuis , and Annelien L . Bredenoord . 2018 . The FAIR guiding principles for data stewardship : Fair enough ? European Journal of HumanGenetics 26 , 7 ( 2018 ) , 931 – 936 . https : / / doi . org / 10 . 1038 / s41431 - 018 - 0160 - 0 [ 4 ] Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov . 2017 . En - riching Word Vectors with Subword Information . Transactions of the Association for Computational Linguistics 5 ( 2017 ) , 135 – 146 . https : / / doi . org / 10 . 1162 / tacl _ a _ 00051 [ 5 ] Alexandru Constantin , Silvio Peroni , Steve Pettifer , David Shotton , and Fabio Vitali . 2016 . The Document Components Ontology ( DoCO ) . Semantic Web 7 , 2 ( 2016 ) , 167 – 181 . https : / / doi . org / 10 . 3233 / SW - 150177 [ 6 ] Dennis Diefenbach , Vanessa Lopez , Kamal Singh , and Pierre Maret . 2018 . Core techniques of question answering systems over knowledge bases : a survey . Knowledge and Information Systems 55 , 3 ( 2018 ) , 529 – 569 . https : / / doi . org / 10 . 1007 / s10115 - 017 - 1100 - y [ 7 ] Said Fathalla , Sahar Vahdati , Sören Auer , and Christoph Lange . 2017 . Towards a Knowledge Graph Representing Research Findings by Semantifying Survey Articles . In International Conference on Theory and Practice of Digital Libraries . Springer , 315 – 327 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 67008 - 9 _ 25 [ 8 ] MeredithD . GallandWalterR . Borg . 1996 . EducationalResearch : Anintroduction ( sixth edition ) . White Plains , NY : Longman Publishers USA ( 1996 ) . [ 9 ] Aldo Gangemi , Silvio Peroni , David Shotton , and Fabio Vitali . 2017 . The Publishing Workflow Ontology ( PWO ) . Semantic Web 8 , 5 ( 2017 ) , 703 – 718 . https : / / doi . org / 10 . 3233 / SW - 160230 [ 10 ] Dagmar Gromann and Thierry Declerck . 2019 . Comparing pretrained multi - lingual word embeddings on an ontology alignment task . LREC 2018 - 11th International Conference on Language Resources and Evaluation ( 2019 ) , 230 – 236 . [ 11 ] Alexander Hars . 2001 . Designing Scientific Knowledge Infrastructures : The Contribution of Epistemology . Information Systems Frontiers 3 , 1 ( 2001 ) , 63 – 73 . https : / / doi . org / 10 . 1023 / A : 1011401704862 [ 12 ] Patrick B . Heidorn . 2008 . Shedding light on the dark data in the long tail of science . Library Trends 57 , 2 ( 2008 ) , 280 – 299 . https : / / doi . org / 10 . 1353 / lib . 0 . 0036 [ 13 ] Ijaz Hussain and Sohail Asghar . 2017 . A survey of author name disambiguation techniques : 2010âĂŞ2016 . The Knowledge Engineering Review 32 ( 2017 ) , 1 – 24 . https : / / doi . org / 10 . 1017 / s0269888917000182 [ 14 ] Mohamad Yaser Jaradeh , Allard Oelen , Manuel Prinz , Jennifer D’Souza , Gábor Kismihók , Markus Stocker , and Sören Auer . 2019 . Open Research Knowledge Graph : Next Generation Infrastructure for Semantic Scholarly Knowledge . In In Proceedings of the 10th International Conference on Knowledge Capture ( K - CAP ’19 ) . ACM . https : / / doi . org / 10 . 1145 / 3360901 . 3364435 [ 15 ] Mohamad Yaser Jaradeh , Allard Oelen , Manuel Prinz , Markus Stocker , and Sören Auer . 2019 . Open Research Knowledge Graph : A System Walkthrough . In 9 International Conference on Theory and Practice of Digital Libraries . Springer , 348 – 351 . https : / / doi . org / 10 . 1007 / 978 - 3 - 030 - 30760 - 8 _ 31 [ 16 ] Christian Kohl , Emma J . McIntosh , Stefan Unger , Neal R . Haddaway , Steffen Kecke , Joachim Schiemann , and Ralf Wilhelm . 2018 . Online tools supporting the conduct and reporting of systematic reviews and systematic maps : A case study on CADIMA and review of existing tools . Environmental Evidence 7 , 1 ( 2018 ) , 1 – 17 . https : / / doi . org / 10 . 1186 / s13750 - 018 - 0115 - 5 [ 17 ] Tobias Kuhn , Christine Chichester , Michael Krauthammer , NÃžria Queralt - rosinach , Ruben Verborgh , and George Giannakopoulos . 2016 . Decentralized provenance - aware publishing with nanopublications . PeerJ Computer Science ( 2016 ) , 1 – 29 . https : / / doi . org / 10 . 7717 / peerj - cs . 78 [ 18 ] Elaine M . Lasda Bergman . 2012 . Finding Citations to Social Work Literature : The Relative Benefits of Using Web of Science , Scopus , or Google Scholar . Journal of Academic Librarianship 38 , 6 ( 2012 ) , 370 – 379 . https : / / doi . org / 10 . 1016 / j . acalib . 2012 . 08 . 002 [ 19 ] Vladimir I Levenshtein . 1966 . Binary codes capable of correcting deletions , insertions , and reversals . In Soviet physics doklady , Vol . 10 . 707 – 710 . [ 20 ] PierreMaillot , CarlosBobed , PierreMaillot , CarlosBobed , PierreMaillot , andCar - losBobed . 2019 . MeasuringstructuralsimilaritybetweenRDFgraphs . Proceedings of the 33rd Annual ACM Symposium on Applied Computing ( 2019 ) , 1960 – 1967 . [ 21 ] Carme Pinya Medina and Maria Rosa Rosselló Ramon . 2015 . Using TF - IDF to Determine Word Relevance in Document Queries Juan . New Educational Review 42 , 4 ( 2015 ) , 40 – 51 . https : / / doi . org / 10 . 15804 / tner . 2015 . 42 . 4 . 03 [ 22 ] Barend Mons , Cameron Neylon , Jan Velterop , Michel Dumontier , Luiz Olavo Bonino Da Silva Santos , and Mark D . Wilkinson . 2017 . Cloudy , in - creasingly FAIR ; Revisiting the FAIR Data guiding principles for the Euro - pean Open Science Cloud . Information Services and Use 37 , 1 ( 2017 ) , 49 – 56 . https : / / doi . org / 10 . 3233 / ISU - 170824 [ 23 ] Reddy Naidu , Santosh Kumar Bharti , Korra Sathya Babu , and Ramesh Kumar Mohapatra . 2018 . Text Summarization with Automatic Keyword Extraction in Telugu e - Newspapers . In Smart Innovation , Systems and Technologies . Vol . 77 . 555 – 564 . https : / / doi . org / 10 . 1007 / 978 - 981 - 10 - 5544 - 7 _ 54 [ 24 ] Allard Oelen , Mohamad Yaser Jaradeh , Kheir Eddine Farfar , Markus Stocker , and SÃűrenAuer . 2019 . ComparingResearchContributionsinaScholarlyKnowledge Graph . In Proceedings of the Third International Workshop on Capturing Scientific Knowledge ( SciKnow19 ) . 21 – 26 . [ 25 ] Norman Paskin . 2010 . Digital object identifier ( DOI® ) system . Encyclopedia of library and information sciences 3 ( 2010 ) , 1586 – 1592 . https : / / doi . org / 10 . 1081 / E - ELIS3 - 120044418 [ 26 ] SilvioPeroniandDavidShotton . 2012 . FaBiOandCiTO : Ontologiesfordescribing bibliographic resources and citations . Journal of Web Semantics 17 ( 2012 ) , 33 – 43 . https : / / doi . org / 10 . 1016 / j . websem . 2012 . 08 . 001 [ 27 ] Silvio Peroni and David Shotton . 2018 . The SPAR ontologies . In Interna - tional Semantic Web Conference . Springer , 119 – 136 . https : / / doi . org / 10 . 1007 / 978 - 3 - 030 - 00668 - 6 _ 8 [ 28 ] Alina Petrova , Evgeny Sherkhonov , Bernardo Cuenca Grau , and Ian Horrocks . 2017 . Entity Comparison in RDF Graphs . In International Semantic Web Confer - ence . 526 – 541 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 68288 - 4 _ 31 [ 29 ] Justus J . Randolph . 2009 . A guide to writing the dissertation literature review . Practical Assessment , Research and Evaluation 14 , 13 ( 2009 ) . https : / / doi . org / 10 . 7275 / b0az - 8t74 [ 30 ] Alejandro Rodríguez - Iglesias , Alejandro Rodríguez - González , Alistair G . Irvine , Ane Sesma , Martin Urban , Kim E . Hammond - Kosack , and Mark D . Wilkinson . 2016 . Publishing FAIR data : An exemplar methodology utilizing PHI - base . Frontiers in Plant Science 7 ( 2016 ) . https : / / doi . org / 10 . 3389 / fpls . 2016 . 00641 [ 31 ] Almudena Ruiz Iniesta and Oscar Corcho . 2014 . A review of ontologies for describing scholarly and scientific documents . In 4 th Workshop on Semantic Publishing ( SePublica ) ( CEUR Workshop Proceedings ) . [ 32 ] BaharSateliandRenéWitte . 2015 . Semanticrepresentationofscientificliterature : bringing claims , contributions and named entities onto the Linked Open Data cloud . PeerJ Computer Science 1 ( 2015 ) , e37 . https : / / doi . org / 10 . 7717 / peerj - cs . 37 [ 33 ] Stefan Seuring , Martin Müller , Magnus Westhaus , and Romy Morana . 2005 . Conducting a Literature Review — The Example of Sustainability in Supply Chains . In Research Methodologies in Supply Chain Management . Physica - Verlag HD , Heidelberg , 91 – 106 . https : / / doi . org / 10 . 1007 / 3 - 7908 - 1636 - 1 _ 7 [ 34 ] Pavel Shvaiko and Jérôme Euzenat . 2013 . Ontology matching : State of the art and future challenges . IEEE Transactions on Knowledge and Data Engineering 25 , 1 ( 2013 ) , 158 – 176 . https : / / doi . org / 10 . 1109 / TKDE . 2011 . 253 [ 35 ] Joan Starr , Eleni Castro , MercÃĺ Crosas , Michel Dumontier , Robert R . Downs , Ruth Duerr , Laurel L . Haak , Melissa Haendel , Ivan Herman , Simon Hodson , Joe Hourclé , John Ernest Kratz , Jennifer Lin , Lars Holm Nielsen , Amy Nurnberger , Stefan Proell , Andreas Rauber , Simone Sacchi , Arthur Smith , Mike Taylor , and Tim Clark . 2015 . Achieving human and machine accessibility of cited data in scholarly publications . PeerJ Computer Science 2015 , 5 ( 2015 ) , 1 – 22 . https : / / doi . org / 10 . 7717 / peerj - cs . 1 [ 36 ] Richard J . Torraco . 2005 . Writing Integrative Literature Reviews : Guidelines and Examples . Human Resource Development Review 4 , 3 ( 2005 ) , 356 – 367 . https : / / doi . org / 10 . 1177 / 1534484305278283 [ 37 ] Sahar Vahdati , Said Fathalla , Sören Auer , Christoph Lange , and Maria - Esther Vidal . 2019 . Semantic Representation of Scientific Publications . In International Conference on Theory and Practice of Digital Libraries . 375 – 379 . https : / / doi . org / 10 . 1007 / 978 - 3 - 030 - 30760 - 8 _ 37 [ 38 ] Jane Webster and Richard T . Watson . 2002 . Analyzing the Past to Prepare for the Future : Writing a Literature Review . MIS Quarterly 26 , 2 ( 2002 ) , xiii – xxiii . [ 39 ] BertVanWeeandDavidBanister . 2016 . HowtoWriteaLiteratureReviewPaper ? Transport Reviews 36 , 2 ( 2016 ) , 278 – 288 . https : / / doi . org / 10 . 1080 / 01441647 . 2015 . 1065456 [ 40 ] Mark D . Wilkinson , Michel Dumontier , IJsbrand Jan Aalbersberg , Gabrielle Ap - pleton , MylesAxton , ArieBaak , NiklasBlomberg , JanWillemBoiten , LuizBonino da Silva Santos , Philip E . Bourne , Jildau Bouwman , Anthony J . Brookes , Tim Clark , MercÃĺ Crosas , Ingrid Dillo , Olivier Dumon , Scott Edmunds , Chris T . Evelo , Richard Finkers , Alejandra Gonzalez - Beltran , Alasdair J . G . Gray , Paul Groth , Carole Goble , Jeffrey S . Grethe , Jaap Heringa , Peter A . C . t Hoen , Rob Hooft , Tobias Kuhn , Ruben Kok , Joost Kok , Scott J . Lusher , Maryann E . Mar - tone , Albert Mons , Abel L . Packer , Bengt Persson , Philippe Rocca - Serra , Marco Roos , Rene van Schaik , Susanna Assunta Sansone , Erik Schultes , Thierry Sen - gstag , Ted Slater , George Strawn , Morris A . Swertz , Mark Thompson , Johan Van Der Lei , Erik Van Mulligen , Jan Velterop , Andra Waagmeester , Peter Wittenburg , Katherine Wolstencroft , Jun Zhao , and Barend Mons . 2016 . Comment : The FAIR Guiding Principles for scientific data management and stewardship . Scientific Data 3 ( 2016 ) , 1 – 9 . https : / / doi . org / 10 . 1038 / sdata . 2016 . 18 [ 41 ] William E . Winkler . 1990 . String Comparator Metrics and Enhanced Decision Rules in the Fellegi - Sunter Model of Record Linkage . Proceedings of the Section on Survey Research , American Statistical Association ( 1990 ) , 354 – 359 . https : / / doi . org / 10 . 1007 / 978 - 1 - 4612 - 2856 - 1 _ 101 [ 42 ] Paweł Ziemba , Jarosław Jankowski , and Jarosław Wątróbski . 2017 . Online comparison system with certain and uncertain criteria based on multi - criteria decisionanalysismethod . In InternationalConferenceonComputationalCollective Intelligence . Springer , 579 – 589 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 67077 - 5 _ 56 10