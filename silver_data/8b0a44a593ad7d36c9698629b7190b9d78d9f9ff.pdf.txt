1 The effects of algorithmic flagging on fairness : quasi - experimental evidence from Wikipedia NATHAN TEBLUNTHUIS , University of Washington and Wikimedia Foundation BENJAMIN MAKO HILL , University of Washington AARON HALFAKER , Wikimedia Foundation Online community moderators often rely on social signals like whether or not a user has an account or a profile page as clues that users are likely to cause problems . Reliance on these clues may lead to “over - profiling” bias when moderators focus on these signals but overlook misbehavior by others . We propose that algorithmic flagging systems deployed to improve efficiency of moderation work can also make moderation actions more fair to these users by reducing reliance on social signals and making norm violations by everyone else more visible . We analyze moderator behavior in Wikipedia as mediated by a system called RCFilters that displays social signals and algorithmic flags and to estimate the causal effect of being flagged on moderator actions . We show that algorithmically flagged edits are reverted more often , especially edits by established editors with positive social signals , and that flagging decreases the likelihood that moderation actions will be undone . Our results suggest that algorithmic flagging systems can lead to increased fairness but that the relationship is complex and contingent . CCS Concepts : • Human - centered computing → Collaborative and social computing theory , concepts and paradigms ; Social content sharing ; Computer supported cooperative work ; Additional Key Words and Phrases : sociotechnical systems ; moderation ; AI ; machine learning ; causal inference ; peer production ; Wikipedia ; online communities ; community norms ; fairness ; ACM Reference format : Nathan TeBlunthuis , Benjamin Mako Hill , and Aaron Halfaker . 2020 . The effects of algorithmic flagging on fairness : quasi - experimental evidence from Wikipedia . 1 , 1 , Article 1 ( January 2020 ) , 23 pages . DOI : 1 INTRODUCTION Online community moderators are responsible for reviewing torrents of user generated content for spam , vandalism , attacks , and other violations of community norms and rules . In many large online communities , a small number of moderators—often volunteers—will be responsible for reviewing thousands or millions of actions and taking steps to stop and mitigate problematic behavior [ 24 ] . To help focus their attention within this deluge , moderators typically rely on social signals [ 19 ] that indicate that a user’s contributions are made in good faith and of high quality [ 46 ] . Common signals include visible reputation , experience , and registration status [ 9 , 46 ] . For example , because new users are often more likely to engage in bad behavior , moderators might scrutinize contributions from newcomers more closely [ 46 , 64 ] . However , directing limited moderation attention based on social signals can introduce unfairness through “over - profiling” that occurs when moderators focus This work is licensed under a Creative Commons Attribution 4 . 0 International License © 2020 Copyright held by the owner / author ( s ) . XXXX - XXXX / 2020 / 1 - ART1 $ DOI : , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . a r X i v : 2006 . 03121v1 [ c s . C Y ] 4 J un 2020 1 : 2 TeBlunthuis et al . their attention on users with signals associated with bad behavior while ignoring others engaged in similar or worse behavior [ 18 ] . For this reason , and because relying on social signals can still place enormous demands on limited moderator resources , online communities are increasingly adopting algorithmic flagging systems to direct moderators toward problematic actions [ 13 , 27 ] . Although the consequences are very different , these systems share salient commonalities with algorithmic flagging systems used in employment , college admissions , and criminal justice . All of these systems use predictions of whether an outcome will occur to flag certain individuals as more or less likely sources of problems and leave final decisions to a human judge . The use of these systems when people’s lives are at stake has rightfully attracted critique on the basis of how algorithms engage in misrepresentation and discrimination [ 4 , 11 , 61 ] . On the other hand , advocates of algorithmic prediction in criminal justice argue that algorithms— even those that are measurably biased in their outcomes—might still be less discriminatory than decisions made by biased human judges alone [ 44 , 72 ] . Can algorithmic flagging systems reduce reliance on social signals and lead to more fair outcomes ? We seek to answer this question through a field evaluation of an algorithmic flagging system called RCFilters that was deployed on 23 different Wikipedia language editions from January 2019 to March 2020 . RCFilters flags contributions identified as likely to be damaging by the ORES machine learning system [ 27 ] . These flags are shown alongside existing social signals of quality . We take advantage of a set of arbitrary thresholds built into RCFilters to conduct a quasi - experimental analysis that estimates the causal effect of algorithmic flagging on moderation decisions and that seeks to measure whether algorithmic flags lead to better or worse outcomes for users who are likely to be over - scrutinized ex ante . Our results suggest that algorithmic flagging can lead to more fair outcomes but that this effect may depend on specifics of the social signals in question . Our paper makes several contributions . First , our work answers calls to analyze the impacts of algorithms in situ [ 69 , 72 , 79 ] by offering an empirical evaluation of an algorithmic flagging system in an important social computing context . Second , our analysis contributes to an ongoing debate over when and how algorithms might lead to more or less fair outcomes for individuals subject to profiling by human decision makers . Third , our work offers a methodological contribution by presenting a novel quasi - experimental approach that can act as a template for future non - interventionist studies of causal effects of algorithmic decision support systems . Finally , our work contributes to social computing system design by suggesting improvements to algorithmic flagging and filtering systems . 2 BACKGROUND 2 . 1 Moderation in Online Communities Contemporary online communities are flooded with harassment , spam , misinformation , disinfor - mation , and hate . Users of social media systems frequently and flagrantly violate community and platforms rules , various laws , and norms of decency and decorum . Even users acting in good faith can do damage by taking conversations off - topic , undermining the stated purpose of communities , and lowering the quality of discourse or the knowledge goods being produced . Protecting online communities from unwanted activity are content moderators—many of them volunteers—that Gillespie [ 24 ] has described as “custodians of the Internet . ” Moderation work typically involves three tasks : reviewing content or activity , mitigating damage caused by problematic behavior , and sanctioning users in various ways [ 24 , 38 , 41 , 68 ] . Grimmelmann [ 25 ] defines moderation as “governance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse . ” Discussions of content moderation often focus on individuals occupying formal roles as moderators with special rights and responsibilities . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 3 For example , many of the moderators in Gillespie’s [ 24 ] account are professional moderators working for major platforms like like Facebook and Twitter . Many moderators , and nearly all in platforms like Reddit and Discord [ 38 , 41 , 54 ] , work as volunteers but occupy similar positions of formal authority and responsibility . That said , the work of moderation is often distributed across regular community members [ 42 , 50 ] . In Wikipedia , for example , the bulk of moderation activity as defined by Grimmelmann occurs as normal users review , vet , and undo the work of others to mitigate damage and sanction users they believe have behaved badly [ 63 ] . 2 . 1 . 1 Sanctions . Sanctioning involves enforcing norms in ways that attempt to discourage future non - normative misbehavior . It is a core part of moderation work because it encourages compliance with norms by communicating that rules will be enforced [ 36 , 71 ] . Although it also serves to mitigate damage , removing content is a common form of sanctioning because it communicates that an action was inappropriate [ 63 ] . Halfaker et al . [ 29 ] shows that removing content is an effective sanction and results in higher quality subsequent contributions by the reverted contributor in Wikipedia . Similarly , Srinivasan et al . [ 71 ] found that people whose comments were removed from Reddit were less likely to violate norms in the future . Although the goal of most sanctioning is to steer participants toward more productive types of behavior , the effect is often simply to deter participation . This can be particularly problematic with well - meaning newcomers who often violate norms because they have not yet learned the ropes [ 1 , 26 , 29 ] . Sanctioned newcomers are less likely to continue participating , especially in the absence of clear explanations from moderators [ 26 , 36 , 43 , 64 , 73 ] . On Wikipedia and similar communities , high rates of sanctioning can help explain declines in participation and may be an obstacle to building a community that includes diverse participants [ 26 , 48 , 73 ] . 2 . 1 . 2 Meta - norms . No moderation system is perfect , and moderators inevitably make mistakes and apply sanctions in ways that are arbitrary and unfair . This is particularly difficult to avoid in distributed moderation models used on sites like Slashdot or Wikipedia where moderation carried out by large and diverse groups of untrained and loosely coordinated users . Sanctions can be particularly demotivating to newcomers when contributors feel that sanctions are unfair and incorrect [ 24 , 36 , 71 ] . As a result , steps that make sanctions more fair might ameliorate the negative effects of moderator sanctions on community growth . One way to improve fairness and accountability in moderation is through governance structures that enforce accountability [ 20 ] . Toward this end , Slashdot famously created tools for “meta - moderation” that allowed all users to evaluate the decisions of moderators [ 50 ] . Users whose moderation decisions were controversial or at odds with the opinions of other Slashdot members would be not given moderation privileges again . Although formal systems for meta - moderation remain rare , there exist many common behaviors that serve a similar social function by taking action against controversial sanctions [ 16 ] . Of particular relevance are “meta - norms” which prescribe when and how one should issue sanctions against violations of first - order norms [ 33 ] . Reagle [ 65 ] documents the formalization of meta - norms on Wikipedia and Piskorski and Gorbatˆai [ 63 ] show how Wikipedia users engage in meta - norm maintenance by undoing sanctions in ways that effectively sanction the originally sanctioning user . 2 . 1 . 3 Flagging and Algorithmic Triage . Moderators can face incredible challenges in scaling their work to handle what can become an enormous mass of content and user activity in large online communities [ 24 , 41 , 67 , 68 ] . In interviews conducted by Kiene et al . [ 41 ] , small teams of volunteer moderators tasked with maintaining order in large communities described their work managing tends of thousands of users engaging simultaneously as akin to “running a small city . ” , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 4 TeBlunthuis et al . Some platforms deal with scale by employing more paid moderators . However , the work involved can be exploitative , difficult , traumatizing , and expensive [ 66 ] . Volunteer moderator teams may also attempt to recruit additional volunteers to deal with growth but frequently find it difficult to identify , train , and integrate new members [ 42 ] . On average , volunteer leadership teams become less likely to add new members as their communities grow [ 70 ] . For these reasons and others , it is often impossible for communities to scale moderation resources such that human moderators can review all activity . As a result , many moderation systems implement flagging so a wider group of users can report content for review by moderators [ 25 ] . If users reliably flag problematic behavior , flagging can mitigate issues of scale because moderators can use flags to focus their attention on behavior that is likely problematic . Of course , flagging is far from a perfect solution . From the perspective of a flagged user , flagging can seem arbitrary and opaque [ 16 ] . From a moderator perspective , flagging is flawed because disgruntled users can coordinate to use flagging systems to overwhelm moderators and target opposing viewpoints [ 16 ] . Finally , in that traditional flagging systems continue to rely on volunteer labor , they often fail to fully address issues of scale leaving many bad actions unflagged , unreviewed , and unsanctioned . To address this final limitation , communities have turned to algorithmic flagging systems that use computer programs to automatically mark content for review by human moderators [ 40 , 41 , 67 ] . Although some of these systems rely on keywords , regular expressions , or heuristics , the more advanced and flexible versions of these systems use predictions from machine learning models . These systems are seen as promising answers to the problem of moderation at scale because they can easily be used to review an enormous volume of behavior , they may be less vulnerable to strategic flagging , and they may be more reliable than human reviewers . Algorithmic flagging systems can be thought of as human - in - the - loop versions of similar com - putational systems that engage in full automating moderation activity . For example , many digital platforms use the PhotoDNA system to automatically identify and remove child pornography [ 24 ] . Similarly , Wikipedia’s ClueBot NG uses a machine learning predictor to automatically remove van - dalism [ 22 ] . Although they play a critical role in reducing moderation workloads , fully automated systems are uncertain enough in most of their assessments that they are typically only considered useful in defending against the most clear - cut examples of misbehavior [ 24 ] . Some machine learning systems designed to classify bad behavior are used as a form of algorithmic triage . The most egregious examples of bad behavior might be dealt with automatically by an automatic systems while many other possible or likely norm - violations are flagged for review and action by human moderators . For example , Reddit allows moderators to define a system of rules based on regular expressions to automatically remove or flag content for further review [ 37 ] . Algorithmic flagging systems based on machine learning occupy the vanguard of online activity regulation and numerous examples have been described in recent scholarship . Chandrasekharan et al . [ 13 ] describes a system for Reddit communities to share information and collaborate on automatic flagging that accounts for differences between rules of different communities . Wulczyn et al . [ 77 ] presents a system for classifying harassing behavior on Wikipedia . Finally , Halfaker et al . [ 27 ] developed the Objective Revision Evaluation Service ( ORES ) system to predict quality of contributions and content on Wikipedia . 2 . 2 Will algorithmic flagging decrease discrimination of over - profiled users ? One of the most important debates in contemporary technology policy is the degree to which the introduction of algorithms into socially consequential decision - making leads to more or less fair outcomes [ 14 , 44 , 61 , 69 ] . Much of this debate focuses on arguments about whether algorithms will amplify or entrench discrimination . Discrimination is deferential treatment of individuals , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 5 based on membership in a group . Economists of discrimination distinguish between taste - based and statistical discrimination [ 6 , 7 , 62 ] . Taste - based discrimination is driven by preferences for members of one group and includes both ideologically - driven racism and implicit bias . Statistical discrimination occurs when social signals—visible and socially salient characteristics , such as group memberships—are instrumental in driving decisions . Statistical discrimination can also lead to unequal outcomes for certain groups . Although most discussion of statistical discrimination focus on high - stakes contexts like banking , labor markets , and criminal justice , moderation in online communities is also ripe for statistical discrimination . For example , Wikipedia’s Missing Manual advises would - be vandal fighters on Wikipedia to “consider the source” when “estimating the likelihood that an edit is vandalism” [ 9 ] . Because newcomers are more likely to violate rules , moderators may rely on social signals associated with being new to find bad behavior or to decide if an ambiguous contribution was made in bad faith . Social signals of newness in online communities include formal reputation systems like karma on Reddit and Slashdot , badges on StackExchange , or many other more subtle signals [ 25 , 49 , 57 ] In any case , increased scrutiny and skepticism can translate into an an increased likelihood of sanction , simply for being new . Statistical discrimination emerges because moderators are more likely to scrutinize and sanction new contributors who have legitimate reasons for contributing . Ethical philosophers have objected to the way social signals are used in online moderation activity . Dutch philosopher Paul de Laat adopts the concept of “profiling” from legal scholar Frederick Schauer to argue against the use—and even the public display of—social signals like registration status and experience levels in the user interfaces used for moderation . de Laat objects to the display of these signals because they are prone to “over - use” [ 17 , 18 ] . It is important to note that discriminating by attributes like newness does not raise the same legal or constitutional concerns as discrimination against protected classes such as race or religion . Online communities establish their own norms and may choose to protect or target certain attributes based on a specific community’s values . For example , while discussing Wikipedia , de Laat argues that this type of “over - use” is unethical , immoral and inconsistent with the community’s founding principles of transparency and equality . Drawing on de Laat , we refer to individuals with social signals that elicit undue scrutiny as “over - profiled . ” Although an important debate continues over the use of algorithmic predictions in domains like criminal sentencing , proponents of algorithms argue that they could reduce discrimination and inequality [ 44 , 72 ] . Algorithms can reproduce statistical discrimination , but they might be less biased than the alternative : human decisions that would presumably rely heavily , if perhaps subconsciously , on salient social signals like race . Critics suggest that algorithms simply obscure this discrimination behind complex mathematical models that are difficult to understand , interrogate , or challenge . Although this debate is difficult to resolve in the case of criminal justice , algorithmic flagging in online community moderation provides a setting with lower stakes and more detailed data . Although the social signals and contexts are substantially different , similar social and psychological processes may be in play . If we apply arguments proposing that algorithms can reduce discrimination to community moderation , we would conclude that algorithmic triage systems would reduce the impact of discrimination among over - profiled individuals by making misbehavior by all kinds of users visible to community moderators . If algorithmic flagging reduces over - profiling bias then it will have a smaller effect on over - profiled users than on others . If algorithms simply reproduce discrimination , we would find no such difference . This leads us to our first research question : [ RQ1 ] How will flagging an action change the likelihood an action is sanctioned for over - profiled editors compared to others ? , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 6 TeBlunthuis et al . 2 . 3 Will algorithmic flagging increase fairness ? A system might discriminate by sanctioning one group more than others but still be justifiable if all sanctions were fair . But what does it means for sanctioning to be fair ? The subject of fairness in algorithmic systems is a major subject of debate in computing and AI . There are many different approaches to conceptualizing fairness and no algorithmic predictor can satisfy them all [ 4 , 12 , 45 , 58 , 75 , 78 ] . While such approaches focus on discrimination built into machine learning programs , we seek a concept of fairness that reflects the standards of relevant communities of practice . We find one in the concept of “meta - norms” from social psychology and James Coleman’s sociological conception of norm maintenance . Drawing from these sources , we define unfair sanctions as those that a community is unwilling to let stand—i . e . , sanctions that are themselves the subject of sanction [ 15 , 33 , 63 ] . For example , a norm in Wikipedia governs right and wrong ways of editing wiki pages . Sanctions of first - order norm violations are governed by meta - norms about what sorts of contributions merit sanction . Following Piskorski and Gorbatˆai [ 63 ] , we describe a sanction as controversial —i . e . , in likely violation of a meta - norm—if it in turn is sanctioned by a third community member . Relying on this definition of fairness , our second research question asks how algorithmic flagging shapes the fairness of sanctioning in terms of such sanctions for meta - norm violations : [ RQ2 ] How will flagging an action change the chances it receives a controversial sanction ? Influential theoretical frameworks in social computing seem to predict competing answers to this second question . First , dual process models of behavioral economics suggest that people will tend to rely on “salient signals” for rapid decision making in conditions of uncertainty and imperfect information [ 8 , 44 , 74 ] . When human moderators choose behavior to review or sanction using using social signals associated with over - profiled users these attributes serve as salient signals but remain far from perfect signals of quality . Algorithmic flags provide an additional salient signal but are also far from perfect [ 27 ] . Indeed , algorithmic flagging systems are typically designed to minimize the risk of missing bad behavior by surfacing large numbers of false positives ( i . e . , non - problematic behavior ) and relying on human moderators to make final decisions . Of course , if human moderators use algorithmic flags as salient signals , they may reproduce algorithms’ false predictions . In this case , controversial sanctions will increase . A second perspective suggests that algorithmic flags can increase fairness . Many online com - munities have formalized rules , norms , and meta - norms and act as highly institutionalized and rationalized organizations [ 10 , 63 , 76 ] . Kreiss et al . [ 47 ] argue that increasing formalization and rationalization in online communities can lead to more fair outcomes . Through this lens , an al - gorithmic flagging system can reflect a shift away from idiosyncratic individual decision - making and toward standardization , rationalization , and governance that is more in - line with community meta - norms . In this way , an algorithmic tool can be a “carrier of formal rationality” [ 52 ] that can decrease the volume of controversial sanctions . Finally , we seek to combine our two previous research questions to ask whether algorithmic flagging systems will be more or less fair in their effects on the sanctioning of over - profiled users relative to others . We ask : [ RQ3 ] Within the set of sanctioned actions , how will the effect of flagging an action on controversial sanctions depend on whether contributors are over - profiled ? Once again , influential theoretical frameworks in social computing research seem to point in opposite directions . Under dual - process psychological models , both social signals and algorithmic flags might kinds of signals might cue moderators to issue sanctions and might substitute for one another . In this case , we would hypothesize that flagging would have a more positive effect on controversial sanctions among under - profiled contributors , who had previously been relatively ignored , than it does among the over - profiled individuals , who were always scrutinized . On , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 7 ORES Flags User profile link Unregistered editor Fig . 1 . Screenshot of Wikipedia edit metadata on Special : RecentChanges with RCFilters enabled . Highlighted edits with a colored circle to the left side of other metadata are flagged by ORES . Different circle and highlight colors ( white , yellow , orange , and red in the figure ) correspond to different levels of confidence that the edit is damaging . Users can configure which colors are shown . Visible social signals include registration status ( i . e . whether a user name or an IP address is shown ) and whether an editor’s user page and user talk page exist . RCFilters does not specifically flag edits by new accounts , but does support filtering changes by newcomers . the other hand , if the larger effect of algorithmic flagging is helping moderators comply with meta - norms , it simply will not matter whether contributors are over - profiled . 3 EMPIRICAL SETTING We seek to answer our three research questions through a field evaluation of an algorithmic flagging system called RCFilters that was deployed on 23 different Wikipedia language editions between January 2019 and March 2020 . RCFilters stands for “Recent Changes filters . ” The term “Recent Changes” refers to a page on Wikipedia that allows viewers to see the the most recent changes made to the site . 1 As shown in Figure 1 , RCFilters adds a set of flags represented as colored dots on the left side of the list of recent contributions . Social signals are also visible including registration status and whether a user has created a profile page . Although dense with information about recent edits and hyperlinks , the page is immediately understandable to Wikipedia moderators . When deployed , the RCFilters interface appears both on “Recent Changes” as well as on “watchlists”—a special version of “Recent Changes” that shows only edits to the subset of pages that a user has elected to follow . RCFilters must be enabled by each user on their Wikipedia user preferences page . Algorithmic flagging in the RCFilters system is powered by the ORES edit quality models trained to predict whether edits are labeled “damaging” or “not damaging . ” The models are gradient boosted decision trees trained on a mixture of human labeled Wikipedia edits and edits made by established editors that are assumed to be “not damaging . ” It is important to note that ORES models do not merely reproduce profiling patterns typical of moderation on Wikipedia . The interface for labeling training data obscures social signals from the volunteer Wikipedians doing labeling work and its models are predictive of damage from users that are not anonymous or newcomers . More information on the design and implementation of ORES can be found in Halfaker et al . [ 27 ] . 4 METHODS Our analysis is based around a regression discontinuity design ( RDD ) that seeks to estimate causal effects of flagging by RCFilters on moderator behavior in Wikipedia [ 34 , 35 , 51 ] . Common in empirical economics , RDDs are quasi - experimental in that they resemble a randomized control trial for data points in the neighborhood of an arbitrary cutoff [ 35 , 51 ] . RDDs model how an outcome depends on this cutoff and a continuous “forcing variable . ” The idea behind an RDD 1 For example , the Recent Changes page for English Wikipedia is available here : https : / / en . wikipedia . org / wiki / Special : RecentChanges , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 8 TeBlunthuis et al . is that observations immediately below and above the cutoff will be equal in expectation after adjusting for any underlying ( i . e . , “secular” ) trend . For example , RDDs used in econometrics might estimate the effect of passing a test by comparing the outcomes of people who barely passed and failed . One benefit of an RDD over a field experiments based on A / B tests is that it can provide ecological validity and support causal claims without subjecting users to intervention without consent [ 5 , 39 ] . Although they are still rare in social computing research , RDDs have been used in recent publications in social computing [ 32 , 60 ] . Our forcing variable is scores from the ORES machine learning system and our cut - off variables are a set of arbitrarily chosen operating points used by RCFilters . Our outcomes are constructed by creating two variables that indicate whether a revision’s author is over - profiled as well as variables that indicates whether each revision was reverted or subject to a controversial revert . We discuss each in turn before introducing our analytic approach . 4 . 1 Data and Measures We build our dataset from two publicly available tables of Wikimedia history published by the Wikimedia Foundation ( WMF ) . 2 Although Wikipedia is published and collaborated on in many languages , the vast majority of knowledge about collaboration on Wikipedia is derived from studies of English Wikipedia [ 30 , 31 ] . To support generalizability , we analyze data from 23 language editions of Wikipedia where edit quality flags are displayed in the RCFilters interface . To ensure that we have variation in our outcomes , we exclude wikis with less than 3 edits above and below each threshold ( see 4 . 1 . 1 ) from each sub - analysis . For all of our analyses , our unit of analysis is the revision . Revisions correspond to a single edit to a page by a participant on Wikipedia . Since we care about how algorithmic flagging and social signals are used by human moderators , we exclude revisions by bots . Following guidance for RDDs [ 51 ] , we include only revisions very near to RCFilters thresholds , with ORES scores within 0 . 03 of the thresholds . To manage the total size of our dataset , we analyze a sample that we construct by stratifying along a number of dimensions : Wikipedia language edition ; user registration status ( 4 . 1 . 4 ) ; whether the editor has a user page or not ( 4 . 1 . 4 ) ; whether an edit was reverted in 2 hours , 48 hours , or 30 days ; whether the edit was flagged by RCFilters ( 4 . 1 . 1 ) ; and whether the revert was controversial ( 4 . 1 . 3 ) . We then sample 5000 edits from within unique combinations of the variables . If there are less than 5000 edits in a given strata , we include all of them . We adjust for this stratification using sample weights throughout our analysis . Because RCFilters was introduced to different wikis at different times , we sample edits during the period immediately following the introduction of ORES but weight our sample according to the number of edits to each wiki over the entire study period . The number of observations sampled at each threshold and from each Wiki for each model are available in the supplementary material . 4 . 1 . 1 ORES scores and RCfilter thresholds . The continuous forcing variable used in our RDD analysis is a score from the ORES algorithm described in 3 . Scores range from 0 to 1 and reflect the predicted probability that a revision is damaging . Because the ORES system has been under continual development over time , we obtain ORES scores created at the times revisions were made from a log maintained by the WMF . The treatments in our analysis are whether edits to Wikipedia are flagged by RCFilters . These flags are applied if , and only if , a score from ORES exceeds a threshold . This use of thresholds at arbitrary operating points is a feature of most algorithmic flagging systems . The intuition behind our RDD is that—after adjusting for small differences in quality associated with marginally higher or lower scores—edits with ORES scores immediately 2 https : / / wikitech . wikimedia . org / wiki / Analytics / Data Lake / Edits / Mediawiki history ; https : / / dumps . wikimedia . org / other / mediawiki history / readme . html , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 9 above and below an arbitrary threshold will be similarly likely to receive both first - order and controversial sanctions . As a result , any discontinuous change in reverts at a one of the threshold’s used by RCFilters can be attributed to the flag . RCFilters uses multiple thresholds corresponding to green , yellow , orange , and red flags . By default only orange and red flags are shown , but users can configure which colors to display . Green flags and filters are to help Wikipedia editors find good edits . Our analysis considers only red , orange , and yellow flags which correspond to thresholds making different trade - offs between precision ( the proportion of flagged edits that are truly damaging ) and recall ( the proportion of truly damaging edits that are flagged ) . The red flag is labeled “very likely damaging and corresponds to a high precision threshold . Orange flags corresponds to a “likely damaging” label with greater recall but less precision . Edits with a yellow flag are “maybe damaging” with a high recall but lower precision . RCFilters’s thresholds are truly arbitrary and have changed over time and across language editions in response to shifts in the precision and recall of ORES models and in response to community feedback . We were able to collect data on thresholds over time , fully trained ORES models , code to run the models on our servers , and the precise time that changes are deployed in the WMF server admin log . We combined these data to identify the thresholds that were active for each revision in our dataset . 4 . 1 . 2 Sanctions . Our outcome variable for answering RQ1 must capture sanctioning in Wikipedia . Following a large body of other social computing research , we measure sanctions as identity reverts [ e . g . , 26 , 29 , 63 , 73 ] . Identity reverts occur when a user undoes another user’s edit by restoring a page to an earlier state and are measured by comparing hashes of page revisions [ 29 ] . That said , identity reverts are an imperfect measure of sanctioning . A type of vandalism called “blanking” removes all content on a page and therefore might be measured as identity reverting all prior edits to the page . It is also possible for an individual to “self - revert” by undoing their own edit . To help mitigate these issues , we only label revisions as reverted if they were undone within 48 hours and were not undone by self - reverts . We label revisions as not reverted otherwise . 4 . 1 . 3 Controversial sanctions . Our outcome variable for answering RQ2 and RQ3 measures controversial sanctions . We follow Piskorski and Gorbatˆai [ 63 ] by measuring controversial sanctions as identity reverts that are subsequently reverted by a third party . Specifically , we label a sanction as controversial if the sanction is undone by a third editor who was not the original editor or the reverting editor . Such interactions likely correspond to cases in which a third party observes the initial revert , disagrees with the initial sanction , and then acts to reverse the sanction . 4 . 1 . 4 Social signals . Answering our RQ1 and RQ3 requires that we identify under - and over - profiled individuals in our empirical setting . Drawing from research and documentation for Wikipedia moderators , we identify two such measures shown in the RCFilters interface shown in Figure 1 . Our first measures is whether an editor was logged into an account . Unregistered editors act on Wikipedia without logging in and Registered contributors are those that edit with accounts . Because they are identified by their IP address rather a chosen username , unregistered editors are also referred to as “IP editors” or “anons . ” Unregistered editors are associated with misbehavior and have long had a controversial status on Wikipedia [ 56 ] . Geiger and Ribes describe how tools for moderators highlight unregistered editors [ 23 ] . de Laat argues that unregistered users on Wikipedia are over - profiled in that they are at higher risk to have their contributions rejected unfairly [ 17 , 18 ] . Second , the RCFilters interface indicates whether the editor has created a User page . User pages are Wikipedia’s version of profile pages . Not having a User page is a strong social signal of newness because most committed users will create a User page early into their experience in Wikipedia , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 10 TeBlunthuis et al . [ 3 ] . The presence or absence of pages in Wikipedia is indicated with a subtle user interface clue : links to pages that do not exist are rendered in red while links to pages that exist are blue . For example , Figure 1 shows the user “Mashlova” whose name is shown in red and would be identified as a newcomer . de Laat cites the absence of a User page as a second example of an indicator of vandalism that will result in over - profiling [ 18 ] . We measure whether a user’s User page exists at the time of a given contribution by matching the titles of User pages against the editor’s user name and checking if the creation of the User page was prior to the edit in question . 5 ANALYTIC PLAN Our analysis consists of 9 Bayesian logistic regression models in two parallel analyses . The first analysis treats our dichotomous measure of whether edits are reverted as an outcome . This begins with an “adoption check” ( 6 ) that describes the causal effects of flagging on reverts in general . The adoption check is prerequisite to answering our research questions . The rest of the first analysis ( 7 . 1 ) answers RQ1 by comparing the effect of RCFilters on edits by over - profiled users to its effect on other editors . Our second analysis is very similar but uses controversial reverts as the outcome , and analyzes only reverted edits to model the probability a revert is controversial . It begins by answering RQ2 ( 7 . 2 ) in an analysis similar to the adoption check but with controversial sanctions as an outcome and with a dataset limited to over - profiled users . The rest of the second analysis ( 7 . 2 ) answers RQ3 and is similar to RQ1 but with controversial reverts as the outcome in place of reverts . Following Litschig and Morrison [ 53 ] ’s use of RDD models with multiple discontinuities , our models incorporate all three RCFilters thresholds . Our goal is to estimate τ j which is the causal effect of being flagged at level j , where j ∈ { 1 , 2 , 3 } corresponding to labels of “maybe damaging” , “likely damaging” and “very likely damaging . ” For each cutoff on each wiki , we select revisions whose ORES scores is within a + / − 0 . 03 window of the cutoff . Following established approaches to RDD , we fit “kink” models that allow for a change in slope at the discontinuity [ 51 , 53 ] . We use Bayesian inference to estimate our models for two reasons . First , virtually all edits above the “very damaging” level are reverted in some of the wikis we analyze . The presence of near - perfect “separation” creates estimation problems for classical numerical approaches [ 2 ] . Preferred solutions to this problem in non - Bayesian frameworks include penalized likelihood methods that introduce bias . Our Bayesian approach uses weakly - informative priors that are conservative but avoid the problem of separation as a result . The second reason we use Bayesian inference is that it makes it easy to compare estimates across models . Prior work at CSCW by Gan et al . [ 21 ] uses a similar rationale for adopting Bayesian logistic regression . In Bayesian analysis , fitted models take the form of posterior distributions constituting a probability distribution of model coefficients conditional on our model , data , and priors . We consider a hypothesis supported if it is consistent with at least 95 % of posterior draws . In other words , we accept a given hypothesis if our parameter estimate has the predicted sign and the 95 % credible interval does not contain zero . This is the Bayesian analog to testing a hypothesis with α = 0 . 05 . We fit our models using the rstanarm package ( version 2 . 19 . 3 ) and the default priors which are provided for reference in the supplementary material . 6 ADOPTION CHECK Before presenting results from hypothesis tests associated with our research questions , we first establish that RCFilters was adopted by Wikipedia moderators and that it had an effect on sanc - tioning behavior . This establishes a baseline necessary to answer RQ1 about the differential effects of RCFilters between and over - profiled users and others . This is important because null effects in RQ1 might simply reflect that the system was not used . A successful adoption check rules out this possibility and sets up a credible null hypothesis test for RQ1 . To demonstrate that RCFilters , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 11 Maybe damaging Likely damaging Very likely damaging - 0 . 03 0 . 00 0 . 03 - 0 . 03 0 . 00 0 . 03 - 0 . 03 0 . 00 0 . 03 0 . 80 0 . 85 0 . 90 0 . 95 0 . 20 0 . 30 0 . 40 0 . 50 0 . 05 0 . 10 0 . 15 0 . 20 Distance from threshold P r o b . r e v e r t e d Prob reverted , all editors Fig . 2 . Marginal effects plot showing model predicted relationship between ORES score and the probability that an edit will be reverted around the cutoffs for all contributors with 95 % credible intervals flags are being used by Wikipedia moderators , we test the hypothesis that flagging increases the probability that an edit is reverted . If Wikipedia moderators are using flags in RCFilters to review potentially damaging edits , our estimates for τ j —as described in 5—should be positive . We find strong evidence that RCFilters was adopted and impacted sanctioning . This evidence is visualized in Figure 2 , a marginal effects plot that visualizes our models’ predicted likelihood of reverts across different ORES scores in the neighborhood of the thresholds . In each such plot , the x - axis shows the distance from the threshold such that discontinuities at 0 represent the effect of being flagged . The plots show modeled values for the English language edition of Wikipedia but are representative of relationships across all wikis . 3 Figure 2 shows discontinuous increases in the likelihood of reversion at the “maybe damaging” , and “likely damaging” thresholds in the left and center panels . We find the greatest effect at the “maybe damaging” threshold ( τ 1 = 1 . 23 [ 1 . 19 ; 1 . 28 ] ) . 4 We do not see a discontinuous increase at the “very likely damaging” threshold shown in the right - most panel ( τ 3 = − 0 . 01 , [ − 0 . 1 ; 0 . 09 ] ) . The impacts of the “maybe damaging” and “likely damaging” flags on the likelihood of sanctioning are enormous . Figure 2 shows that likelihood of a revert for an edit just below the “maybe damaging” threshold is between 5 . 5 % and 5 . 8 % indicating that reverts of unflagged edits are relatively rare . Being flagged with the “maybe damaging” flag causes a dramatic increase in the reversion probability to between 16 . 8 % and 17 . 7 % for edits just above the threshold . The effect of algorithmic flags at the “likely damaging” level is even more stark . We estimate that edits just below the “likely damaging” threshold are likely to be reverted between 24 . 3 % and 25 . 8 % of the time while otherwise similar edits just above the threshold are reverted between 46 . 1 % and 48 . 7 % of the time . We believe that we do not observe any increase in the likelihood of sanctioning at the “very likely damaging” level because actions flagged as “very likely damaging” are also flagged as “likely damaging” in the RCFilters’ default configuration . As a result , the marginal impact of being flagged as “very likely damaging” on visibility is likely very small . Moreover , edits flagged as “very likely damaging” are often so egregious that they will be reverted by bots before a human moderator can review them . 3 Because intercepts are the only part of our model that depend on Wikis , slopes and the discontinuities caused by algorithmic flagging represent our inference over all our data . 4 All τ parameter estimates are reported as log - odds ratios . The bracket notation indicates the 95 % credible interval . In other words , the most likely value of the parameter is 1 . 23 , but there is a 95 % probability that the parameter lies in the interval [ 1 . 19 ; 1 . 28 ] . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 12 TeBlunthuis et al . Unregistered No User page 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 - 1 0 1 Marginal Posterior Threshold Maybe damaging Likely damaging Fig . 3 . Results for RQ1 showing point estimates and credible intervals for differences in the causal effect of flagging on sanctioning between over - profiled contributors and others . A value greater than 0 indicates that our estimates of the effect for under - profiled contributors is greater than that for over - profiled contributors . 7 RESULTS 7 . 1 RQ1 : Effect of flagging on sanctioning Our first research question ( RQ1 ) , seeks to understand how the increase in sanctioning caused by flagging affects discrimination against over - profiled users . If algorithmic flagging reduces over - profiling , as some computer scientists have argued [ 44 ] , the effect of flagging will be more scrutiny on users who are more likely to be given a pass . If algorithms simply reproduce discrimination , we will find no difference . Results for hypothesis tests answering this question are shown in Figure 3 which visualizes the point estimates and credible intervals for differences in the causal effects of flagging on reverts between unregistered and registered contributors and between contributors with and without User pages . Values greater than 0 indicate that our estimated effect for the other users is greater than that for the over - profiled group . In support of the idea that algorithmic flagging can reduce over - profiling bias , we find that the effect of flagging on reverts of registered editors is greater than the effect for unregistered editors at the “maybe damaging” threshold ( τ Unreg 1 − τ Reg 1 = 0 . 8 [ 0 . 71 ; 0 . 89 ] ) and at the “likely damaging” threshold ( τ Unreg 2 − τ Reg 2 = 0 . 78 [ 0 . 58 ; 0 . 97 ] ) . For an action by an unregistered contributor near to the “maybe damaging” threshold , being flagged increases the odds of being reverted by a factor of between 1 . 45 and 1 . 6 times . This is significantly greater than the increase of 3 . 16 and 3 . 68 for registered contributors . Figure 4 lets us interpret our models in terms of the probability of revert for actions on English Wikipedia . These plots make it possible to visually compare the effects of being flagged between over - profiled and under - profiled editors at a given threshold because the y - axes in each row span an identical range . The top - left panel shows how our models’ linear predictions of how the probability of sanctioning for unregistered contributors at the “maybe damaging” threshold jumps between 4 . 8 and 6 . 7 percentage points , from 13 . 5 % to 19 . 2 % on average . For registered editors , shown in the top - right of Figure 4 , we estimate a jump of between 9 . 1 and 10 . 3 percentage points , from 4 . 6 % to 14 . 3 % on average . This is between 3 . 3 and 4 . 6 percentage points greater than the jump for unregistered editors . For unflagged edits that ORES scores near the “maybe damaging” threshold , an unflagged unregistered contributor has about the same odds of being sanctioned as a flagged registered contributor . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 13 The bottom row of Figure 4 shows that the change in sanctioning probability at the “likely damaging” threshold is between 9 . 5 and 15 . 2 percentage points greater for registered editors than for unregistered editors . For unregistered contributors , shown in the bottom - left of Figure 4 , being flagged as “likely damaging” increases the probability of reverting between 15 and 18 . 6 percentage points , from 33 . 5 % to 50 . 2 % on average . But for registered editors , shown in the bottom - right of Figure 4 , we detect an even bigger jump of between 23 . 7 and 34 . 6 percentage points , from 15 . 5 % to 44 . 5 % on average . For actions that ORES scores near the “likely damaging” threshold , unflagged actions by unregistered editors are far more likely to be reverted . Once flagged , actions by registered and unregistered editors are reverted at relatively similar rates . These results provide strong evidence of flagging leveling the playing field between registered and unregistered contributors . Our results suggest that actions by unregistered contributors that fall just above the cutoffs are much more likely to be reverted due to RCFilters—but the gap between actions by registered and unregistered contributors is much smaller when RCFilters has flagged an edit . In this way , our analysis suggests algorithmic flagging can reduce over - profiling bias . Surprisingly , our results for our second measure of over - profiling in Wikipedia suggest a dynamic that is opposite in sign to the differences we observe between registered and unregistered users at the “maybe bad” threshold ( τ NoUP1 − τ UP1 = − 0 . 68 [ − 0 . 95 ; − 0 . 41 ] ) . At the “likely bad” threshold ( τ NoUP2 − τ UP2 = − 0 . 05 [ − 1 . 61 ; 1 . 39 ] ) we do not detect a difference in effect size between contributors with and without User pages . At the “maybe damaging” threshold , we find that flagging increases the odds that an editor without a User page is reverted between 3 . 47 and 4 . 06 times . This is significantly more than the increase of between 1 . 47 and 2 . 46 times for registered contributors . As above , we interpret these odds ratios using marginal effects plots , this time shown in Figure 5 . The top - left plot in the figure shows our models’ linear predictions of the probability of reverting for contributors without User pages near to the “maybe damaging” threshold . For these editors , being flagged as “maybe damaging” increases the chances of sanctioning by 11 . 4 and 13 . 8 percentage points , from 5 . 6 % to 18 . 1 % on average . In the top - right of Figure 5 , we see a jump of between 2 . 2 Unregistered - 0 . 03 0 . 00 0 . 03 0 . 10 0 . 15 0 . 20 0 . 25 Registered - 0 . 03 0 . 00 0 . 03 0 . 05 0 . 10 0 . 15 0 . 20 Unregistered - 0 . 03 0 . 00 0 . 03 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 Registered - 0 . 03 0 . 00 0 . 03 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 Distance from threshold P r o b . R e v e r t e d Threshold Maybe damaging Likely damaging Fig . 4 . Results for RQ1 comparing unregistered and registered contributors are displayed in a marginal effects plot showing model predicted relationship between ORES score and reverts around the thresholds that trigger flags . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 14 TeBlunthuis et al . and 4 . 8 percentage points , from 4 % to 7 . 4 % on average for editors that have created User pages . This is between 9 . 7 and 8 . 4 percentage points less than the jump for contributors without User pages . 7 . 2 RQ2 : Effect of flagging on controversial sanctioning Consistent with the idea that algorithmic flagging can support fairness , we find that having an ORES score cross the “maybe damaging” or “likely damaging” threshold decreases the chances that a revert will be controversial for unregistered editors . These results are visualized in Figure 7a . The overall effect hides some variation between the magnitude of the effects across our two thresholds . We have less confidence in the effect at the “maybe damaging” threshold because our 95 % credible interval includes zero ( τ Unreg1 = − 0 . 07 ; CI = [ − 0 . 16 ; 0 . 02 ] ) . We estimate that being flagged at the “maybe damaging” level results in change in the odds that a sanction is controversial by a factor between 0 . 85 and 1 . 02 . Figure 7b shows the modeled relationship between ORES scores and the probability of a controversial sanction in the neighborhood of the thresholds for English Wikipedia . On the left plot we see that being flagged changes unregistered contributor’s likelihood of a controversial revert from a possible increase of 0 . 38 percentage points to a possible decrease of 0 . 69 percentage points , a change from 3 . 74 % to 3 . 5 % on average . We have more confidence in the effect at the “likely damaging” threshold ( τ Unreg2 = − 0 . 09 ; CI = [ − 0 . 16 ; − 0 . 03 ] ) , where the odds that a revert is controversial are between 0 . 85 and 0 . 97 times smaller . On the right side of 7b we that being flagged decreases the probability that a sanction to an action by an unregistered editor is controversial by between 0 . 01 and 0 . 55 percentage points , a change from from 3 . 08 % to 2 . 81 % on average . By summing our posteriors for both threshold parameters , we find algorithmic flagging has a negative effect across both thresholds overall ( τ Unreg1 + τ Unreg2 = − 0 . 16 ; CI = [ − 0 . 27 ; − 0 . 05 ] ) . However , we did not detect an effect of flagging at the “maybe damaging” level when the reverted editor lacks a User page ( τ NoUP1 = 0 . 01 ; CI = [ − 0 . 07 ; 0 . 08 ] ) or at the “likely damaging” level ( τ NoUP1 = − 0 . 01 ; CI = [ − 0 . 15 ; 0 . 14 ] ) . We address the inconsistencies between our results for unregistered editors and editors without User pages in our discussion ( 9 ) . No User page - 0 . 03 0 . 00 0 . 03 0 . 05 0 . 10 0 . 15 0 . 20 Profile page - 0 . 03 0 . 00 0 . 03 0 . 00 0 . 05 0 . 10 0 . 15 No User page - 0 . 03 0 . 00 0 . 03 0 . 2 0 . 4 0 . 6 Profile page - 0 . 03 0 . 00 0 . 03 0 . 0 0 . 2 0 . 4 0 . 6 Distance from threshold P r o b . D a m ag i n g Threshold Maybe damaging Likely damaging Fig . 5 . Results for RQ1 comparing contributors with and without User pages . Each panel shows a marginal effects plot of the modeled relationship between ORES score and reverts around the thresholds that trigger flags . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 15 - 0 . 15 - 0 . 10 - 0 . 05 0 . 00 Marginal Posterior Threshold Maybe damaging Likely damaging ( a ) Parameter estimates and 95 % credible intervals for the effects of flagging on whether reverts are contro - versial for unregistered editors . Maybe damaging Likely damaging - 0 . 03 0 . 00 0 . 03 - 0 . 03 0 . 00 0 . 03 0 . 028 0 . 030 0 . 032 0 . 034 0 . 036 0 . 038 0 . 040 0 . 042 Distance from threshold P r o b . C o n t r o v e r s i a l ( b ) Marginal effects plots for models predicting whether a revert is controversial , for unregistered editors . Fig . 6 . Results for RQ2 : flagging causes a small but detectable decrease in the likelihood that an action by an unregistered contributor receives a controversial sanction . - 0 . 1 0 . 0 0 . 1 Marginal Posterior Threshold Maybe damaging Likely damaging ( a ) Parameter estimates and 95 % credible intervals for effects of flagging on whether reverts are controversial for editors without User pages . Maybe damaging Likely damaging - 0 . 03 0 . 00 0 . 03 - 0 . 03 0 . 00 0 . 03 0 . 030 0 . 032 0 . 034 0 . 036 0 . 038 0 . 040 0 . 042 0 . 044 0 . 046 Distance from threshold P r o b . C o n t r o v e r s i a l ( b ) Marginal effects plots for models predicting whether a revert is controversial , for contributors without User pages . Fig . 7 . Results for RQ2 comparing contributors with User pages to those without show no detectable effect of flagging on controversial sanctioning . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 16 TeBlunthuis et al . User page Registration - 0 . 6 - 0 . 3 0 . 0 0 . 3 0 . 0 0 . 1 0 . 2 Marginal Posterior Threshold Maybe damaging Likely damaging Fig . 8 . Results for RQ3 showing the difference in our parameter estimates between over - profiled editors and others . Lines show 95 % credible intervals . Values greater than 0 would indicate that the effect for under - profiled editors is greater than that for over - profiled editors . 7 . 3 RQ3 : Social signals and effects of flagging on controversial sanctioning In RQ3 we ask if changes in controversial sanctioning caused by flagging depend on whether users are over - profiled . To answer this question , we largely replicate the analysis conducted for RQ1 with the dependent variable used in RQ2 . Results shown in Figure 8 , provide weak evidence that a decrease in controversial sanctioning at the “maybe damaging” threshold may be greater for registered than for unregistered contributors ( τ Reg1 − τ Unreg1 = 0 . 04 [ − 0 . 06 ; 0 . 14 ] ) . The same seems true at the “likely damaging” threshold ( τ NoUP2 − τ UP2 = − 0 . 07 [ − 0 . 05 ; 0 . 2 ] ) . However , our evidence weakly suggests that the effect for contributors with user profiles is greater than those for without ( τ UP1 − τ NoUP1 = 0 . 05 [ − 0 . 08 ; 0 . 17 ] ) , but the opposite seems true at the “likely damaging” threshold ( τ UP2 − τ NoUP2 = − 0 . 26 [ − 0 . 79 ; 0 . 26 ] ) . None of these estimates nor their sums are statistically significant at the 95 % level . 8 THREATS TO VALIDITY Our results are subject to a range of threats to validity that pertain to our ability to make causal claims , rule out alternative explanations , and establish the generalizability of our findings . First , there are several threats to our ability to draw causal inference that are common to RDDs . Formally , RDDs model an outcome Y as a function of a continuous “forcing variable” Z , other covariates , and a cutoff c such that Z > c determines treatment assignment . In principle , treatment assignment conditional on Z is “as good as random” under two assumptions : ( 1 ) that agents have at most limited control over Z > c and ( 2 ) that the relationship between Y and Z is smooth [ 51 ] . While the assumptions required for causal inference are fundamentally unverifiable , we believe that our RDD provides relatively strong evidence of causal relationships between flagging and sanctioning . Our treatment , being flagged in RCFilters , is an ideal candidate for an RDD from the perspective of assumption ( 1 ) because editors are unlikely to have much control over the scores that their edits receive . While attempts to evade sanction by specially crafting edits to evade algorithmic detection are hypothetically possible , the authors of ORES and RCFilters believe they are unrealistic and very unlikely to be wide - spread . Assumption ( 2 ) would be violated if any unobserved treatments affect our outcomes at discrete levels of ORES scores . This is certainly possible because ORES makes scores available via a public API . Indeed , we are aware of bots that automatically revert edits triggered by the “very damaging” threshold on some of the Wikipedia language editions in , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 17 our sample . To mitigate this threat , we exclude reverts by bots and at the “very likely damaging” threshold . Although we identified one anti - vandalism tool—a system called Huggle discussed in 9—that collects ORES damaging scores , it uses ORES scores as one feature in its own algorithmic model and by default presents predictions from this model to users as a list of edits sorted in order of likelihood of vandalism . Given these facts , we believe it is unlikely that Huggle users will drive discontinuities in the relationship between ORES scores and our outcomes . Our study design is also limited in that we cannot present causal evidence of the impact of social signals . Although RCFilters’s algorithmic flags are distributed in a quasi - experimental way , over - profiled status is not . There are a range of possible systematic differences between over - profiled users and others that might be driving our results for RQ1 and RQ3 . For example , if damaging edits by contributors who are unregistered or lack User pages are more difficult for ORES to detect , that might drive our findings of a decrease in over - profiling for RQ1 . Although we believe that this particular threat is unlikely because it would require that over - profiled contributors be systematically more sophisticated than others—something our experience with ORES suggests is unlikely—we cannot rule out either the specific threat or a range of other possibilities . A promising direction for future work might involve experiments or quasi - experiments that are able to jointly vary social signals and algorithmic flagging . Additionally , system designers will likely want to know how overall rates of sanctioning and controversial sanctions change before and after a system like RCFilters is launched . Our analysis cannot answer this question directly . In preliminary work , we attempted to draw a statistical comparison between Wikipedia governance before and after the introduction of ORES scoring but high temporal variation in sanctioning behaviors made this type of aggregate change difficult to measure . Future studies should organize with communities to carry out planned and principled field experiments to study the causal effects of introducing such systems in online communities using the model being pioneered by Matias and Mou [ 55 ] . Finally , a set of largely unanswerable threats involves questions of generalizability across our measures and empirical contexts . While our theories of interactions between algorithmic flags and social signals is general , and although we study RCFilters across 23 distinct communities and cultures , we study a single moderator tool on one platform . We can not claim that our findings generalize beyond the specific pool of communities that we study . We can not claim that our setting is representative of other Wikipedia communities that did not launch RCFilters . Clearly , we also can not claim that our settings is representative of moderation in online communities in general . Like most other empirical studies in social computing , we must sadly leave these questions for further research . 9 DISCUSSION In broadest strokes , our work provides excellent news for advocates of algorithmic flagging in social computing systems . Our work provides some evidence that supports the idea that algorithmic flagging can reduce discrimination in the form of over - profiling bias and that it can increase fairness . Our adoption check ( 6 ) provides strong evidence that RCFilters drives behavior and our answers to RQ1 ( 7 . 1 ) suggests that flagging seems to level the playing field . Flagged actions by unregistered and registered contributors are reverted at similar rates , but unflagged edits of comparable quality by registered editors are reverted relatively infrequently . More good news comes in the form of our answer to RQ2 ( 7 . 2 ) that suggests that flagging is associated with a decrease in controversial sanctions among some over - profiled users and provides evidence that algorithmic flagging systems can help moderators more accurately issue sanctions . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 18 TeBlunthuis et al . When it comes the details however , the picture that emerges from our results is much more contingent and mixed . Our analysis used two different measures of over - profiling in Wikipedia but the pattern of our results diverged substantially between the two . The optimistic story about the effects of algorithmic flagging on over - profiled users only describes our results for unregistered Wikipedia users . Our evidence on over - profiled users without User pages is much weaker and points , in part , in the direction of algorithmic flagging increasing discrimination . Why do these results diverge ? What do these divergent results mean for theory ? One possible explanation is that users without User pages are , quite simply , not particularly over - profiled . Of the two social signals we consider , registration status attracts far more attention from academics and community members in discussions of Wikipedia vandalism [ e . g . , 32 ] . Our analysis for RQ2 , where we did not detect a change in controversial sanctions at the “maybe damaging” threshold for editors without user pages , is consistent with the notion that contributors without user pages may not be over - profiled . However , this can not explain why the effect for editors without profile pages was larger than for editors with them . It is plausible that our mixed results are evidence that algorithmic flags will substitute for some social signals used in profiling while reinforcing others . A better understanding of which signals drive sanctioning misbehavior can help explain if and when algorithmic triage systems can increase fairness . Our results suggest that algorithmic flags can substitute for social signals and reduce discrimina - tion . Our results also suggest that they might also reinforce social signals and make discrimination worse or introduce dnew forms of discrimination through encoded bias . For example , our result might be explained if ORES is somehow biased against contributors without User pages such that their flagged edits are truly less damaging than flagged edits by contributors who do have profile pages . Then flagged edits by both kinds of users might be inspected by moderators at similar rates , but sanctioned differently . Unfortunately , outcomes resulting from myriad factors acting at once are likely deeply contingent on details of sociotechnical arrangements and difficult to know ex ante . Although RQ2 suggests that algorithmic flagging can increase fairness for over - profiled contrib - utors , our null results for RQ3 mean that we could not detect a difference in this effect between over - profiled editors and others . These results are also puzzling . A null effect for RQ3 was sug - gested by a theoretical framework proposing that cues or salient signals such as flagging are less important than meta - norms and useful information when it comes to controversial sanctioning . Yet uncertainty in our models is quite high and parameter values that would be consistent with either a positive or negative average effect remain plausible . This points to methodological limitations in our use of controversial reverts as a measure of fairness . Ultimately , controversial reverts are just too rare—especially for registered contributors with User pages—for us to be confident in our estimates . New approaches to measuring normative and meta - normative compliance in online communities may reflect a promising area for future work . Our work has a number of important implications for designers of algorithmic flagging systems and sociotechnical systems . Scholars of human - computer interaction , science and technology studies , and the law , have all called for analysis of algorithmic fairness to move beyond biases inherent in algorithms to consider the systemic and downstream effects of algorithms in use [ 69 , 72 , 79 ] . We provide one answer to this call by showing one way that designers and managers of online communities might evaluate the way that algorithmic systems may influence community members’ actions . While quality control is an important function in open production communities like Wikipedia , supporting newcomers and encouraging contribution is also essential [ 26 , 59 ] . Past work has shown that increased quality control efforts correspond to a decrease in newcomer engagement and have hypothesized that one mechanism is increased scrutiny of newcomers [ 26 , 73 ] . Similarly , while , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 19 blocking anonymous edits led to a decrease in reverted edits , it also led to a decrease in positive contributions [ 32 ] . While it may be intuitive to think about edits that get sanctioned as obvious vandalism , many of the edits flagged by the “maybe bad” threshold are authored by well - meaning newcomers and anonymous editors [ 26 ] . There’s a potentially high cost to sanctioning these low quality but well intentioned contributions . We believe that our results point to the benefit of tracking changes in the rate of sanctions to sensitive groups of community members in order to assure that such well - meaning contributors aren’t being driven away . There are also lessons to learned from the impressive degree with which RCFilters shapes behavior . Designers should think about whether using thresholds to trigger flagging in moderation interfaces is a fair practice . While thresholds allowed us to explore the effects of flagging on sanctioning behavior , this arbitrary flagging of actions applied by RCFilters brought disproportionate attention to contributions just above the thresholds compared to contributions just below . Our results show that this leads to sanctioning behavior that is disproportionate and , like the thresholds , arbitrary . What types of designs might support quality control support models that scrutinize contributions in proportion to the likelihood that the contributions deserves to be sanctioned ? We see some inspiration in Huggle , a counter - vandalism tool for Wikipedia which sorts actions by the likelihood that they are damaging . 5 Huggle users are encouraged to review the highest likelihood edits first and only move onto lower likelihood edits once those reviews are complete . Such a user experience might increase efficiency and fairness by better concentrating moderator attention wherever it can have the greatest benefits . 10 CONCLUSION As algorithmic flagging becomes more integrated into online community moderation , it is important to understand its effects and consequences on discrimination and fairness . We use a regression discontinuity analysis of the RCFilters used to find and sanction misbehavior by volunteers on Wikipedia to consider how the use of algorithmic flagging and social signals interact . We find that by drawing moderator attention to misbehavior by registered participants , algorithmic flagging can reduce over - profiling . We also find that algorithmic flagging can support fairness by decreasing controversial sanctions of unregistered contributors . On the other hand , our results suggest that the same system may have much less effect , and might even increase discrimination , for other types of over - profiled users . Critics of machine learning trace how algorithms can encode discriminatory patterns in human behavior . Such questions are pertinent to the use of machine predictions in decision making in high - stakes settings like employment , education , and criminal justice . Our work uses data from a lower - stakes context to show that when tools for predictive governance are introduced into a sociotechnical system , their effects may be difficult to anticipate . While our analysis of over - profiling based on registration status supports a rosy account of algorithmic flagging , our analysis of over - profiling based on User pages suggests that the interaction between algorithmic flagging and social signals is more complex and contingent . Our work suggests a need for future work that describes the kinds of social signals that are used in practice and explains how different types of information may be used alongside algorithmic flags . Finally , we present a methodological approach that we hope future studies of algorithmic tools in real - world sociotechnical systems might build upon to establish the causal effects of algorithmic systems without experimental interventions . 5 See discussion in [ 28 ] , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 20 TeBlunthuis et al . REFERENCES [ 1 ] B . Thomas Adler and Luca de Alfaro . 2007 . A Content - Driven Reputation System for the Wikipedia . In Proceedings of the 16th International Conference on World Wide Web ( WWW ’07 ) . Association for Computing Machinery , Banff , Alberta , Canada , 261 – 270 . https : / / doi . org / 10 . 1145 / 1242572 . 1242608 [ 2 ] Paul Allison . 2004 . Convergence Problems in Logistic Regression . In Numerical Issues in Statistical Computing for the Social Scientist . John Wiley & Sons , Ltd , 238 – 252 . https : / / doi . org / 10 . 1002 / 0471475769 . ch10 [ 3 ] Phoebe Ayers , Charles Matthews , and Ben Yates . 2008 . How Wikipedia Works . No Starch Press . [ 4 ] Solon Barocas , Moritz Hardt , and Arvind Narayanan . 2019 . Fairness in Machine Learning . fairmlbook . org . [ 5 ] Solon Barocas and Helen Nissenbaum . 2015 . Big Data’s End Run around Anonymity and Consent . In Privacy , Big Data , and the Public Good : Frameworks for Engagement , Julia I Lane ( Ed . ) . Cambridge University Press , New York , NY . [ 6 ] Gary Stanley Becker . 1957 . The Economics of Discrimination . University of Chicago Press , Chicago . [ 7 ] Marianne Bertrand and Esther Duflo . 2016 . Field Experiments on Discrimination . Technical Report w22014 . National Bureau of Economic Research , Cambridge , MA . https : / / doi . org / 10 . 3386 / w22014 [ 8 ] Pedro Bordalo , Nicola Gennaioli , and Andrei Shleifer . 2012 . Salience Theory of Choice Under Risk . The Quarterly Journal of Economics 127 , 3 ( Aug . 2012 ) , 1243 – 1285 . https : / / doi . org / 10 . 1093 / qje / qjs018 [ 9 ] John Broughton . 2008 . Wikipedia the Missing Manual . Pogue Press / O’Reilly , Beijing ; Sebastopol , CA . [ 10 ] Brian Butler , Elisabeth Joyce , and Jacqueline Pike . 2008 . Don’t Look Now , but We’ve Created a Bureaucracy : The Nature and Roles of Policies and Rules in Wikipedia . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’08 ) . ACM , New York , NY , USA , 1101 – 1110 . https : / / doi . org / 10 . 1145 / 1357054 . 1357227 [ 11 ] Alex Campolo , Madelyn Sanfilippo , Meredith Whittaker , and Kate Crawford . 2017 . AI Now 2017 Report . AI Now Institute at New York University ( 2017 ) . [ 12 ] Ana Caraban , Evangelos Karapanos , Daniel Gonc¸alves , and Pedro Campos . 2019 . 23 Ways to Nudge : A Review of Technology - Mediated Nudging in Human - Computer Interaction . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . Association for Computing Machinery , Glasgow , Scotland Uk , 1 – 15 . https : / / doi . org / 10 . 1145 / 3290605 . 3300733 [ 13 ] Eshwar Chandrasekharan , Chaitrali Gandhi , Matthew Wortley Mustelier , and Eric Gilbert . 2019 . Crossmod : A Cross - Community Learning - Based System to Assist Reddit Moderators . Proc . ACM Hum . - Comput . Interact . CSCW 3 ( Nov . 2019 ) , 174 : 1 – 174 : 30 . https : / / doi . org / 10 . 1145 / 3359276 [ 14 ] Alexandra Chouldechova . 2017 . Fair Prediction with Disparate Impact : A Study of Bias in Recidivism Prediction Instruments . Big Data 5 , 2 ( June 2017 ) , 153 – 163 . https : / / doi . org / 10 . 1089 / big . 2016 . 0047 [ 15 ] James S . Coleman . 1988 . Social Capital in the Creation of Human Capital . Amer . J . Sociology 94 ( 1988 ) , S95 – S120 . [ 16 ] Kate Crawford and Tarleton Gillespie . 2016 . What Is a Flag for ? Social Media Reporting Tools and the Vocabulary of Complaint . New Media & Society 18 , 3 ( March 2016 ) , 410 – 428 . https : / / doi . org / 10 . 1177 / 1461444814543163 [ 17 ] Paul B . de Laat . 2015 . The Use of Software Tools and Autonomous Bots against Vandalism : Eroding Wikipedia’s Moral Order ? Ethics and Information Technology 17 , 3 ( Sept . 2015 ) , 175 – 188 . https : / / doi . org / 10 . 1007 / s10676 - 015 - 9366 - 9 [ 18 ] Paul B . de Laat . 2016 . Profiling Vandalism in Wikipedia : A Schauerian Approach to Justification . Ethics and Information Technology 18 , 2 ( June 2016 ) , 131 – 148 . https : / / doi . org / 10 . 1007 / s10676 - 016 - 9399 - 8 [ 19 ] Judith Donath . 2014 . The Social Machine : Designs for Living Online . [ 20 ] Seth Frey , P . M . Krafft , and Brian C . Keegan . 2019 . ”This Place Does What It Was Built for” : Designing Digital Institutions for Participatory Change . Proc . ACM Hum . - Comput . Interact . 3 , CSCW ( Nov . 2019 ) , 32 : 1 – 32 : 31 . https : / / doi . org / 10 . 1145 / 3359134 arXiv : 1902 . 08728 [ 21 ] Emilia F . Gan , Benjamin Mako Hill , and Sayamindu Dasgupta . 2018 . Gender , Feedback , and Learners’ Decisions to Share Their Creative Computing Projects . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) , 54 : 1 – 54 : 23 . https : / / doi . org / 10 . 1145 / 3274323 [ 22 ] R . Stuart Geiger and Aaron Halfaker . 2013 . When the Levee Breaks : Without Bots , What Happens to Wikipedia’s Quality Control Processes ? . In Proceedings of the 9th International Symposium on Open Collaboration ( OpenSym ’13 ) . ACM , New York , NY , 6 : 1 – 6 : 6 . https : / / doi . org / 10 . 1145 / 2491055 . 2491061 [ 23 ] R . Stuart Geiger and David Ribes . 2010 . The Work of Sustaining Order in Wikipedia : The Banning of a Vandal . In Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work ( CSCW ’10 ) . ACM , New York , NY , 117 – 126 . https : / / doi . org / 10 . 1145 / 1718918 . 1718941 [ 24 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , Content Moderation , and the Hidden Decisions That Shape Social Media . Yale University Press , New Haven . [ 25 ] James Grimmelmann . 2015 . The Virtues of Moderation . Yale Journal of Law and Technology 17 ( 2015 ) , 42 – 109 . [ 26 ] Aaron Halfaker , R . Stuart Geiger , Jonathan T . Morgan , and John Riedl . 2013 . The Rise and Decline of an Open Collaboration System : How Wikipedia’s Reaction to Popularity Is Causing Its Decline . American Behavioral Scientist 57 , 5 ( May 2013 ) , 664 – 688 . https : / / doi . org / 10 . 1177 / 0002764212469365 , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 21 [ 27 ] Aaron Halfaker , R Stuart Geiger , Jonathan T Morgan , Amir Sarabadani , and Adam Wight . 2019 . ORES : Facilitating Re - Mediation of Wikipedia’s Socio - Technical Problems . ( 2019 ) . [ 28 ] Aaron Halfaker , R . Stuart Geiger , and Loren G . Terveen . 2014 . Snuggle : Designing for Efficient Socialization and Ideological Critique . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 311 – 320 . https : / / doi . org / 10 . 1145 / 2556288 . 2557313 [ 29 ] Aaron Halfaker , Aniket Kittur , and John Riedl . 2011 . Don’t Bite the Newbies : How Reverts Affect the Quantity and Quality of Wikipedia Work . In Proceedings of the 7th International Symposium on Wikis and Open Collaboration ( WikiSym ’11 ) . ACM , New York , NY , 163 – 172 . https : / / doi . org / 10 . 1145 / 2038558 . 2038585 [ 30 ] Noriko Hara , Pnina Shachaf , and Khe Foon Hew . 2010 . Cross - Cultural Analysis of the Wikipedia Community . Journal of the American Society for Information Science and Technology 61 , 10 ( 2010 ) , 2097 – 2108 . https : / / doi . org / 10 . 1002 / asi . 21373 [ 31 ] Brent Hecht and Darren Gergle . 2010 . The Tower of Babel Meets Web 2 . 0 : User - Generated Content and Its Applications in a Multilingual Context . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’10 ) . ACM , New York , NY , USA , 291 – 300 . https : / / doi . org / 10 . 1145 / 1753326 . 1753370 [ 32 ] Benjamin Mako Hill and Aaron Shaw . 2020 . The Hidden Costs of Requiring Accounts : Quasi - Experimental Evidence from Peer Production . Communication Research ( 2020 ) , 30 . [ 33 ] Christine Horne . 2001 . The Enforcement of Norms : Group Cohesion and Meta - Norms . Social Psychology Quarterly 64 , 3 ( 2001 ) , 253 – 266 . https : / / doi . org / 10 . 2307 / 3090115 [ 34 ] Guido W . Imbens and Thomas Lemieux . 2008 . Regression Discontinuity Designs : A Guide to Practice . Journal of Econometrics 142 , 2 ( Feb . 2008 ) , 615 – 635 . https : / / doi . org / 10 . 1016 / j . jeconom . 2007 . 05 . 001 [ 35 ] Robin Tepper Jacob , Pei Zhu , Marie - Andr´ee Somers , and Howard Bloom . 2012 . A Practical Guide to Regression Discontinuity . MDRC Working Papers on Research Methodology ( 2012 ) . [ 36 ] Shagun Jhaver , Darren Scott Appling , Eric Gilbert , and Amy Bruckman . 2019 . ”Did You Suspect the Post Would Be Removed ? ” : UnderstandingUserReactionstoContentRemovalsonReddit . ProceedingsoftheACMonHuman - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 192 : 1 – 192 : 33 . https : / / doi . org / 10 . 1145 / 3359294 [ 37 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - Machine Collaboration for Content Regulation : The Case of Reddit Automoderator . ACM Trans . Comput . - Hum . Interact . 26 , 5 ( July 2019 ) , 31 : 1 – 31 : 35 . https : / / doi . org / 10 . 1145 / 3338243 [ 38 ] Jialun ”Aaron” Jiang , Charles Kiene , Skyler Middler , Jed R . Brubaker , and Casey Fiesler . 2019 . Moderation Challenges in Voice - Based Online Communities on Discord . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 23 . https : / / doi . org / 10 . 1145 / 3359157 [ 39 ] Jukka Jouhki , Epp Lauk , Maija Penttinen , Niina Sormanen , and Turo Uskali . 2016 . Facebook’s Emotional Contagion Experiment as a Challenge to Research Ethics . Media and Communication 4 , 4 ( Oct . 2016 ) , 75 – 85 . https : / / doi . org / 10 . 17645 / mac . v4i4 . 579 [ 40 ] Charles Kiene and Benjamin Mako Hill . 2020 . Who Uses Bots ? A Statistical Analysis of Bot Usage in Moderation Teams . In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI EA ’20 ) . Association for Computing Machinery , Honolulu , HI , USA , 1 – 8 . https : / / doi . org / 10 . 1145 / 3334480 . 3382960 [ 41 ] Charles Kiene , Jialun ”Aaron” Jiang , and Benjamin Mako Hill . 2019 . Technological Frames and User Innovation : Exploring Technological Change in Community Moderation Teams . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 44 : 1 – 44 : 23 . https : / / doi . org / 10 . 1145 / 3359146 [ 42 ] Charles Kiene , Andr´es Monroy - Hern´andez , and Benjamin Mako Hill . 2016 . Surviving an “Eternal September” : How an Online Community Managed a Surge of Newcomers . In Proceedings of the 2016 ACM Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , 1152 – 1156 . https : / / doi . org / 10 . 1145 / 2858036 . 2858356 [ 43 ] Sara E . Kiesler , Robert E . Kraut , Paul Resnick , and Aniket Kittur . 2012 . Regulating Behavior in Online Communities . In Building Successful Online Communities : Evidence - Based Social Design , Robert E . Kraut and Paul Resnick ( Eds . ) . The MIT Press . [ 44 ] Jon Kleinberg , Himabindu Lakkaraju , Jure Leskovec , Jens Ludwig , and Sendhil Mullainathan . 2018 . Human Decisions and Machine Predictions . The Quarterly Journal of Economics 133 , 1 ( Feb . 2018 ) , 237 – 293 . https : / / doi . org / 10 . 1093 / qje / qjx032 [ 45 ] Jon Kleinberg , Sendhil Mullainathan , and Manish Raghavan . 2016 . Inherent Trade - Offs in the Fair Determination of Risk Scores . arXiv : 1609 . 05807 [ cs , stat ] ( Sept . 2016 ) . arXiv : 1609 . 05807 [ cs , stat ] [ 46 ] Robert E . Kraut , Paul Resnick , and Sara Kiesler . 2012 . Building Successful Online Communities : Evidence - Based Social Design . MIT Press , Cambridge , MA . [ 47 ] Daniel Kreiss , Megan Finn , and Fred Turner . 2011 . The Limits of Peer Production : Some Reminders from Max Weber for the Network Society . New Media & Society 13 , 2 ( March 2011 ) , 243 – 259 . https : / / doi . org / 10 . 1177 / 1461444810370951 [ 48 ] Shyong ( Tony ) K . Lam , Anuradha Uduwage , Zhenhua Dong , Shilad Sen , David R . Musicant , Loren Terveen , and John Riedl . 2011 . WP : Clubhouse ? : An Exploration of Wikipedia’s Gender Imbalance . In Proceedings of the 7th International Symposium on Wikis and Open Collaboration ( WikiSym ’11 ) . ACM , New York , NY , 1 – 10 . https : / / doi . org / 10 . 1145 / , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . 1 : 22 TeBlunthuis et al . 2038558 . 2038560 [ 49 ] Cliff Lampe . 2012 . The Role of Reputation Systems in Managing Online Communities . In The Reputation Society , Hassan Masum and Mark Tovey ( Eds . ) . The MIT Press . https : / / doi . org / 10 . 7551 / mitpress / 8777 . 003 . 0013 [ 50 ] Cliff Lampe and Paul Resnick . 2004 . Slash ( Dot ) and Burn : Distributed Moderation in a Large Online Conversation Space . In Conference on Human Factors in Computing Systems ( CHI ) . ACM , Vienna , Austria , 543 – 550 . [ 51 ] David S . Lee and Thomas Lemieux . 2010 . Regression Discontinuity Designs in Economics . Journal of Economic Literature 48 , 2 ( 2010 ) , 281 – 355 . https : / / doi . org / 10 . 1257 / jel . 48 . 2 . 281 [ 52 ] Dirk Lindebaum , Mikko Vesa , and Frank den Hond . 2019 . Insights From “The Machine Stops” to Better Understand Rational Assumptions in Algorithmic Decision Making and Its Implications for Organizations . Academy of Management Review 45 , 1 ( May 2019 ) , 247 – 263 . https : / / doi . org / 10 . 5465 / amr . 2018 . 0181 [ 53 ] Stephan Litschig and Kevin M . Morrison . 2013 . The Impact of Intergovernmental Transfers on Education Outcomes and Poverty Reduction . American Economic Journal : Applied Economics 5 , 4 ( Oct . 2013 ) , 206 – 240 . https : / / doi . org / 10 . 1257 / app . 5 . 4 . 206 [ 54 ] J . Nathan Matias . 2016 . Going Dark : Social Factors in Collective Action against Platform Operators in the Reddit Blackout . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , 1138 – 1151 . https : / / doi . org / 10 . 1145 / 2858036 . 2858391 [ 55 ] J . Nathan Matias and Merry Mou . 2018 . Civilservant : Community - Led Experiments in Platform Governance . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , New York , NY , USA , 9 : 1 – 9 : 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3173583 [ 56 ] Nora McDonald , Benjamin Mako Hill , Rachel Greenstadt , and Andrea Forte . 2019 . Privacy , Anonymity , and Perceived Risk in Open Collaboration : A Study of Service Providers . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 . ACM Press , Glasgow , Scotland Uk , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300901 [ 57 ] Arpit Merchant , Daksh Shah , Gurpreet Singh Bhatia , Anurag Ghosh , and Ponnurangam Kumaraguru . 2019 . Signals Matter : Understanding Popularity and Impact of Users on Stack Overflow . In The World Wide Web Conference ( WWW ’19 ) . Association for Computing Machinery , San Francisco , CA , USA , 3086 – 3092 . https : / / doi . org / 10 . 1145 / 3308558 . 3313583 [ 58 ] Shira Mitchell , Eric Potash , and Solon Barocas . 2018 . Prediction - Based Decisions and Fairness : A Catalogue of Choices , Assumptions , and Definitions . arXiv : 1811 . 07867 [ stat ] ( Nov . 2018 ) . arXiv : 1811 . 07867 [ stat ] [ 59 ] Jonathan T . Morgan , Siko Bouterse , Heather Walls , and Sarah Stierch . 2013 . Tea and Sympathy : Crafting Positive New User Experiences on Wikipedia . In Proceedings of the 2013 Conference on Computer Supported Cooperative Work ( CSCW ’13 ) . ACM , New York , NY , USA , 839 – 848 . https : / / doi . org / 10 . 1145 / 2441776 . 2441871 [ 60 ] Sneha Narayan , Nathan TeBlunthuis , Wm Salt Hale , Benjamin Mako Hill , and Aaron Shaw . 2019 . All Talk : How Increasing Interpersonal Communication on Wikis May Not Enhance Productivity . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 101 : 1 – 101 : 19 . https : / / doi . org / 10 . 1145 / 3359203 [ 61 ] Cathy O’Neil . 2018 . Weapons of Math Destruction : How Big Data Increases Inequality and Threatens Democracy . Penguin Books , London . [ 62 ] Edmund S . Phelps . 1972 . The Statistical Theory of Racism and Sexism . The American Economic Review 62 , 4 ( 1972 ) , 659 – 661 . [ 63 ] Miko laj Jan Piskorski and Andreea D . Gorbatˆai . 2017 . Testing Coleman’s Social - Norm Enforcement Mechanism : Evidence from Wikipedia . Amer . J . Sociology 122 , 4 ( 2017 ) , 1183 – 1222 . https : / / doi . org / 10 . 1086 / 689816 [ 64 ] Martin Potthast , Benno Stein , and Robert Gerling . 2008 . Automatic Vandalism Detection in Wikipedia . In Advances in Information Retrieval ( Lecture Notes in Computer Science ) , Craig Macdonald , Iadh Ounis , Vassilis Plachouras , Ian Ruthven , and Ryen W . White ( Eds . ) . Springer , Berlin , Heidelberg , 663 – 668 . https : / / doi . org / 10 . 1007 / 978 - 3 - 540 - 78646 - 7 75 [ 65 ] Joseph M . Reagle . 2010 . “Be Nice” : Wikipedia Norms for Supportive Communication . New Review of Hypermedia and Multimedia 16 , 1 - 2 ( April 2010 ) , 161 – 180 . https : / / doi . org / 10 . 1080 / 13614568 . 2010 . 498528 [ 66 ] Sarah Roberts . 2016 . Commercial Content Moderation : Digital Laborers’ Dirty Work . Media Studies Publications ( Jan . 2016 ) . [ 67 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping Pro and Anti - Social Behavior on Twitch Through Moderation and Example - Setting . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW ’17 ) . ACM , New York , NY , USA , 111 – 125 . https : / / doi . org / 10 . 1145 / 2998181 . 2998277 [ 68 ] JosephSeering , TonyWang , JinaYoon , andGeoffKaufman . 2019 . ModeratorEngagementandCommunityDevelopment in the Age of Algorithms . New Media & Society 21 , 7 ( July 2019 ) , 1417 – 1443 . https : / / doi . org / 10 . 1177 / 1461444818821316 [ 69 ] Andrew D . Selbst , Danah Boyd , Sorelle A . Friedler , Suresh Venkatasubramanian , and Janet Vertesi . 2019 . Fairness and Abstraction in Sociotechnical Systems . In Proceedings of the Conference on Fairness , Accountability , and Transparency ( FAT * ’19 ) . ACM , New York , NY , USA , 59 – 68 . https : / / doi . org / 10 . 1145 / 3287560 . 3287598 , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 . The effects of algorithmic flagging on fairness 1 : 23 [ 70 ] Aaron Shaw and Benjamin Mako Hill . 2014 . Laboratories of Oligarchy ? How the Iron Law Extends to Peer Production . Journal of Communication 64 , 2 ( 2014 ) , 215 – 238 . https : / / doi . org / 10 . 1111 / jcom . 12082 [ 71 ] KumarBhargavSrinivasan , CristianDanescu - Niculescu - Mizil , LillianLee , andChenhaoTan . 2019 . ContentRemovalAs a Moderation Strategy : Compliance and Other Outcomes in the ChangeMyView Community . Proc . ACM Hum . - Comput . Interact . 3 , CSCW ( Nov . 2019 ) , 163 : 1 – 163 : 21 . https : / / doi . org / 10 . 1145 / 3359265 [ 72 ] Megan T . Stevenson . 2017 . Assessing Risk Assessment in Action . SSRN Electronic Journal ( 2017 ) . https : / / doi . org / 10 . 2139 / ssrn . 3016088 [ 73 ] Nathan TeBlunthuis , Aaron Shaw , and Benjamin Mako Hill . 2018 . Revisiting ”The Rise and Decline” in a Population of Peer Production Projects . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , New York , NY , 355 : 1 – 355 : 7 . https : / / doi . org / 10 . 1145 / 3173574 . 3173929 [ 74 ] Amos Tversky and Daniel Kahneman . 1974 . Judgment under Uncertainty : Heuristics and Biases . Science 185 , 4157 ( Sept . 1974 ) , 1124 – 1131 . https : / / doi . org / 10 . 1126 / science . 185 . 4157 . 1124 [ 75 ] Hanna Wallach . 2019 . Big Data , Machine Learning , and the Social Sciences : Fairness , Accountability , and Transparency . Medium ( Jan . 2019 ) . [ 76 ] Max Weber . 1978 . Economy and Society . University of California Press , Berkeley , CA . [ 77 ] Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 . Ex Machina : Personal Attacks Seen at Scale . In Proceedings of the 26th International Conference on World Wide Web - WWW ’17 . ACM Press , Perth , Australia , 1391 – 1399 . https : / / doi . org / 10 . 1145 / 3038912 . 3052591 [ 78 ] Ming Yin , Jennifer Wortman Vaughan , and Hanna Wallach . 2019 . Understanding the Effect of Accuracy on Trust in Machine Learning Models . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 . ACM Press , Glasgow , Scotland Uk , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300509 [ 79 ] HaiyiZhu , BowenYu , AaronHalfaker , andLorenTerveen . 2018 . Value - SensitiveAlgorithmDesign : Method , CaseStudy , and Lessons . Proc . ACM Hum . - Comput . Interact . 2 , CSCW ( Nov . 2018 ) , 194 : 1 – 194 : 23 . https : / / doi . org / 10 . 1145 / 3274463 , Vol . 1 , No . 1 , Article 1 . Publication date : January 2020 .