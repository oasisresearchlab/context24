Dense Prediction on Sequences with Time - Dilated Convolutions for Speech Recognition Tom Sercu Multimodal Algorithms and Engines Group IBM T . J Watson Research Center , USA Tom . Sercu1 @ ibm . com Vaibhava Goel Multimodal Algorithms and Engines Group IBM T . J Watson Research Center , USA vgoel @ us . ibm . com Abstract In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image . Convolutional neural networks achieve good performance on this task , while being computationally efﬁcient . In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames , a task commonly known as framewise classiﬁcation . We show that dense prediction view of framewise classiﬁcation offers several advantages and insights , including computational efﬁciency and the ability to apply batch normalization . When doing dense prediction we pay speciﬁc attention to strided pooling in time and introduce an asymmetric dilated convolution , called time - dilated convolution , that allows for efﬁcient and elegant implementation of pooling in time . We show that by using time - dilated convolutions with a very deep VGG - style CNN with batch normalization , we achieve best published single model accuracy result on the switchboard - 2000 benchmark dataset . 1 Introduction Deep convolutional networks [ 1 ] have seen tremendous sucess both in computer vision [ 2 , 3 , 4 ] and speech recognition [ 5 , 6 , 7 ] over the last years . Many computer vision problems fall into one of two problem types : the ﬁrst is classiﬁcation , where a single label is produced per image , the second dense pixelwise prediction , where a label is produced for each pixel in the image . Examples of dense prediciton are semantic segmentation , depth map prediction , optical ﬂow , surface normal prediction , etc . Efﬁcient convolutional architectures allow to produce a full image sized output rather than predicting the values for each pixel separately from a small patch centered around the pixel . In this paper we argue that we should look at acoustic modeling in speech as a dense prediction task on sequences . This is in contrast to the the usual viewpoint of “framewise classiﬁcation” , indicating the cross - entropy training stage where a context - window is used as input and the network predicts only for the center frame . However , during all other stages , we want the acoustic model to be applied to a sequence , and produce a sequence of predictions . This is the case during sequence training , test time , or in an end - to - end training setting . Similar to convolutional architectures for dense prediction in computer vision , we focus our efforts on convolutional architectures that process an utterance at once and produce a sequence of labels as output , rather than “splicing” up the utterance , i . e . labeling each frame independently from a small window around it . There are four main advantages to convolutional architectures that allow efﬁcient evaluation of full utterance ( without need of splicing ) in this dense prediction viewpoint : • Computational efﬁciency : processing a spliced utterance requires window _ size times more ﬂoating point operations . • Batch normalization can easily be adopted during sequence training ( or end to end training ) , which we will show gives strong improvements ( as outlined in [ 8 ] ) . • The main architectural novelty of this paper is that we can allow for strided pooling in time . In the next two sections , we will adopt a recent technique from dense prediction , named dilated convolutions , for CNN acoustic models to enable strided pooling in time . Experiments and results for this new model are in section 4 . • We will show a unifying viewpoint with Stacked Bottleneck Networks , and discuss the relevance for end - to - end models with convolutional layers in section 5 . 2 Related work : Pooling in CNNs for dense prediction on images Pooling with stride is an essential ingredient of any classiﬁcation CNN , allowing to access more context on higher feature maps , while reducing the spatial resolution before it is absorbed into the fully connected layers . However , for dense pixelwise prediction tasks , it is less straightforward how to deal with downsampling : on the one hand downsampling allows for a “global view” by having large receptive ﬁelds at low resolution , on the other hand we also need detail on a small scale , i . e . we need the high resolution information . To incorporate both global and local information , downsampling pooling has been incorporated in dense prediction networks in several ways . Firstly , many methods involve upsampling lower resolution feature maps , usually combined with some higher resolution feature maps . In [ 9 ] , an image is processed at three different scales with three different CNNs , after which the output feature maps are merged . The Fully Convolutional Networks ( FCNs ) from [ 10 ] use a VGG classiﬁcation network as basis , introducing skip connections to merge hi - res lower layers with upsampled low - res layers from deeper in the network . SegNet [ 11 ] uses a encoder - decoder structure , in which upsampling is done with max - unpooling [ 12 ] , i . e . by remembering the max location of the encoder’s pooling layers . A second way of using CNNs with strided pooling for dense prediction was proposed in [ 3 ] : at every pooling layer with stride s × s , the input is duplicated s × s times , but shifted with offset ( ∆ x , ∆ y ) ∈ [ 0 . . . s − 1 ] × [ 0 . . . s − 1 ] . After the convolutional stages , the output is then interleaved to recover the full resolution . A third way ( which we will use ) is called spatial dilated convolutions , which keeps the feature maps in their original resolution . The idea is to replace the pooling with stride s by pooling with stride 1 , then dilate all convolutions with a factor s , meaning that s − 1 s values get skipped . This was called ﬁlter rarefaction in [ 10 ] , introduced as “d - regularly sparse kernels” in [ 13 ] , and dubbed spatial dilated convolutions in [ 14 ] . It was noted [ 3 , 10 ] that this method is equivalent to shift - and - interleave , though more intuitive . The recent WaveNet work [ 15 ] uses dilated convolutions for a generative model of audio . 3 Time - dilated convolutions Previous work on CNNs for acoustic modeling [ 5 , 6 ] eliminated the possibility of strided pooling in time because of the downsampling effect . Recent work [ 7 , 8 ] shows a signiﬁcant performance boost by using pooling in time during cross - entropy training , however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows . By adapting the notion of dense prediction , we propose to allow pooling in time while maintaining efﬁcient full - utterance processing , by using an asymmetric version of spatial dilated convolution with dilation in the time direction but not in the frequency direction , which we appropriately call time - dilated convolutions . conv3 , 1 pool2 , 2 conv3 , 1 X ( t - 4 , … , t + 3 ) CNN y ( t ) ( a ) Original CNN ( XE ) conv3 , 1 pool2 , 2 conv3 , 1 X ( 1 , … , 10 ) CNN y ( 1 , … , 10 ) ( b ) Sequence : Problem conv3 , 1 pool2 , 1 conv3 , 1 - D2 X ( 1 , … , 10 ) CNN y ( 1 , … , 10 ) ( c ) Solution Figure 1 : Example of simple CNN ( 1 conv , 1 pool , 1 conv layer ) . Pooling with stride 2 is replaced by pooling with stride 1 , while consecutive convolutions are dilated with a factor 2 . 2 The problem with strided pooling in time is that the length of the output sequence is shorter than the length of the input sequence with a factor 2 ( p ) , assuming p pooling layers with stride 2 . For recurrent end - to - end networks typically a factor 4 size reduction is accepted [ 16 , 17 ] which limits the number of pooling layers to 2 , while in the hybrid NN / HMM framework , pooling is not acceptable . Essentially we need a way to do strided pooling in time , while keeping the resolution . We tackle this problem with a 1D version of sparse kernels [ 13 ] , or equivalently spatial dilated convolutions [ 14 ] . Consider the simple toy CNN ( conv3 , pool2 - s2 , conv3 ) in Figure 1 ( a ) , which takes in a context window of 8 frames and produces a single output . Let’s consider applying this CNN to a full utterance of length 10 ( padded to length 16 ) , as in ﬁgure ( b ) . The top row of blue outputs is downsampled with factor 2 because of the strided pooling , so the output sequence length does not match the number of targets ( i . e . input size ) . The solution of this problem is visualized in Figure 1 ( c ) . First , we pool without stride , which preserves the resolution after pooling . However , now our consecutive convolutional layer needs to be modiﬁed ; speciﬁcally the kernel has to skip every other value , in order to ignore the new ( dark blue ) values which came between the values . This is dilation ( or sparsiﬁcation ) of the kernel with a factor 2 in the time direction . Formally a 1 - D discrete convolution ∗ l with dilation l which convolves signal F with kernel k with size r is deﬁned as ( F ∗ l k ) ( p ) = (cid:80) s + lt = p F ( s ) k ( t ) , t ∈ [ − r , r ] . In general , the procedure to change a CNN with time - pooling from the cross - entropy training ( classiﬁcation ) to dense prediction stage for sequence training and testing is as follows . Change pooling layers from stride s to stride 1 , and multiply the dilation factor of all following convolutions with factor s . After this , any convolution coming after p pooling layers with original stride s , will have the dilation factor s p . Fully connected layers are equivalent to , and can be trivially replaced by , convolutional layers with kernel 1 × 1 ( except the ﬁrst convolution which has kernel size matching the output of the conv stack before being ﬂattened for the fully connected layers ) . This dilating procedure is how a VGG classiﬁcation network is adapted for semantic segmentation [ 13 , 14 ] . Using time - dilated convolutions , the feature maps and output can keep the full resolution of the input , while pooling with stride . With pooling , the receptive ﬁeld in time of the CNN can be larger than the same network without pooling . This allows to combine the performance gains of pooling [ 7 ] , while maintaining the computational efﬁciency and ability to apply batch normalization [ 8 ] . 4 Experiments and results We trained a VGG style CNN [ 4 ] in the hybrid NN / HMM setting on the 2000h Switchboard + Fisher dataset . The architecture and training method is similar to our earlier papers [ 7 , 8 ] , and is based on the setup described in [ 21 ] . Our input features are VTLN - warped logmel with ∆ , ∆∆ , the outputs are 32k tied CD states from forced alignment . Table 1 fully speciﬁes the CNN when training on windows Layer Output : fmaps × f × T Input window 3 × 64 × 48 conv 7 × 7 64 × 64 × 42 pool 2 × 1 64 × 32 × 42 conv 3 × 3 64 × 32 × 40 conv 3 × 3 64 × 32 × 38 conv 3 × 3 64 × 32 × 36 pool 2 × 1 64 × 16 × 36 conv 3 × 3 128 × 16 × 34 conv 3 × 3 128 × 16 × 32 conv 3 × 3 128 × 16 × 30 pool 2 × 1 128 × 8 × 30 conv 3 × 3 256 × 8 × 28 conv 3 × 3 256 × 8 × 26 conv 3 × 3 256 × 8 × 24 pool 2 × 2 256 × 4 × 12 conv 3 × 3 512 × 4 × 10 conv 3 × 3 512 × 4 × 8 conv 3 × 3 512 × 4 × 6 pool 2 × 2 512 × 2 × 3 3 × FC 2048 FC 1024 FC 32000 Table 1 : CNN architecture . SWB CH XE ST XE ST Classic 512 CNN [ 18 ] 12 . 6 10 . 4 IBM 2016 RNN + VGG + LSTM [ 19 ] 8 . 6 † 14 . 4 † MSR 2016 ResNet * [ 20 ] 8 . 9 MSR 2016 LACE * [ 20 ] 8 . 6 MSR 2016 BLSTM * [ 20 ] 8 . 7 VGG ( pool , inefﬁcient ) [ 19 ] 10 . 2 9 . 4 16 . 3 16 . 0 VGG ( no pool ) [ 8 ] 10 . 8 9 . 7 17 . 1 16 . 7 VGG - 10 + BN ( no pool ) [ 8 ] 10 . 8 9 . 5 17 . 0 16 . 3 VGG - 13 + BN ( no pool ) 10 . 3 9 . 0 16 . 5 16 . 4 VGG - 13 + BN + pool 9 . 5 8 . 5 15 . 1 15 . 4 VGG - 13 + BN + pool ( uncouple CH acwt ) 14 . 8 15 . 2 Table 2 : Results with small LM ( 4M n - grams ) SWB CH IBM 2015 DNN + RNN + CNN [ 21 ] 8 . 8 † 15 . 3 † IBM 2016 RNN + VGG + LSTM [ 19 ] 7 . 6 † 13 . 7 † MSR 2016 ResNet [ 20 ] 8 . 6 14 . 8 MSR 2016 LACE [ 20 ] 8 . 3 14 . 8 MSR 2016 BLSTM [ 20 ] 8 . 7 16 . 2 VGG + BN + pool 7 . 7 14 . 5 VGG + BN + pool ( uncouple CH acwt ) 14 . 4 Table 3 : Results with big LM ( 36M n - grams ) 3 and predicting the center frame . Corresponding to the observations in [ 8 ] , we do not pad in time , though we do pad in the frequency direction . Training followed the standard two - stage scheme , with ﬁrst 1600M frames of cross - entropy training ( XE ) followed by 310M frames of Sequence Training ( ST ) . XE training was done with SGD with nesterov acceleration , with learning rate decaying from 0 . 03 to 9 e − 4 over 600M frames . We use the data balancing from [ 7 ] with exponent γ = 0 . 8 . We report results on Hub5’00 ( SWB and CH part ) after decoding using the standard small 4M n - gram language model with a 30 . 5k word vocabulary . We saw slight improvement in results when decoding with exponent on the prior γ lower than what is used during training . As mentioned in section 3 , we use batch normalization in our network , where the mean and variance statistics are accumulated over both the feature maps and the frequency direction . The selection of models , decoding prior and acoustic weight happened by decoding on rt02 as heldout set . The result after XE and ST are presented in Tables 2 and 3 . Baseline with * from personal communi - cation with the authors . Baseline with † means system combination . Note that the baselines from [ 20 ] use slightly smaller LMs : 3 . 4M n - grams for small LM ( table 2 ) and 16M n - grams for big LM ( table 3 ) . With n - gram decoding , this result is to our knowledge the best published single model . 5 Relation to other models Stacked bottleneck networks ( SBN ) [ 22 , 23 , 24 ] or hierarchical bottleneck networks [ 25 ] are a inﬂuential acoustic model in hybrid NN / HMM speech recognition . SBNs are typically seen as two consecutive DNNs , each stage separately trained discriminatively with a bottleneck ( small hidden layer ) . The ﬁrst DNN sees the input features , while the second DNN gets the bottleneck features from the ﬁrst DNN as input . Typically , the second DNN gets 5 bottleneck features with stride 5 , i . e . features from position { − 10 , − 5 , 0 , 5 , 10 } relative to the center [ 24 ] . In [ 23 ] , it was pointed out that this SBN is convolutional and one can backpropagate through both stages together . In fact this multi - stage SBN architecture is a special case of a CNN with time - dilated convolution . Speciﬁcally , the DNN is equivalent to a CNN with a large ﬁrst kernel followed by all 1 × 1 kernels . The second DNN is exactly equivalent to a CNN with the ﬁrst kernel having size 5 and dilation factor 5 in the time direction . The layers after the bottleneck in the ﬁrst DNN form an auxilary classiﬁer . This realization prompts a number of directions in which the SBNs can be extended . Firstly , by avoiding the large kernel in the ﬁrst convolutional layer , it is possible to keep time and frequency structure in the internal representations in future layers , enabling increased depth . Secondly , rather than increasing the time - dilation factor to 5 at once , it seems more natural to gradually increase the time - dilation factor throughout the depth of the network . Convolutional networks are also used in end - to - end models for speech recognition . Both the CLDNN architecture [ 17 ] and Deep Speech 2 ( DS2 ) [ 16 ] combine a convolutional network as ﬁrst stage with LSTM and fully connected ( DNN ) output layers . In Wav2Letter [ 26 ] , a competitive end - to - end model is presented which is fully convolutional . Both DS2 and Wav2Letter do a certain amount of downsampling through pooling or striding , which can be accepted when training with a CTC ( or AutoSeg [ 26 ] ) criterion since it doesn’t require the output to be the same length as the input . However , DS2 does report a degradation on English , which they work around using grapheme bigram targets . The time - dilated convolutions we introduced , could improve these end to end models in two ways : either , one could allow the same amount of pooling while keeping a higher resolution , i . e . eliminate the need for the bigram targets . Alternatively , one could keep the same resolution , but expand the receptive ﬁeld by adding more time - dilated convolution layers , which gives access to a broader context in the CNN layers . In conclusion , this work is both relevant to end - to - end models and to hybrid HMM / NN models . 6 Conclusion We drew the parallel between dense prediction in computer vision and framewise sequence labeling , both in the HMM / NN and end - to - end setting . This provided us with the tools ( i . e . time - dilated convolutions ) to adopt pooling in time to CNN acoustic models , while maintaining efﬁcient processing and batch normalization on full utterances . We showed results on Hub5’00 where we brought down the WER from 9 . 4 % in previous work to 8 . 5 % , a 10 % relative improvement . With a big ( 36M N - gram ) language model , we achieve 7 . 7 % WER , the best single model performance reported so far . 4 References [ 1 ] Yann LeCun , Léon Bottou , Yoshua Bengio , and Patrick Haffner , “Gradient - based learning applied to document recog - nition , ” Proceedings of the IEEE , vol . 86 , no . 11 , pp . 2278 – 2324 , 1998 . [ 2 ] Alex Krizhevsky , Ilya Sutskever , and Geoffrey E Hinton , “Imagenet classiﬁcation with deep convolutional neural networks , ” in Advances in neural information processing systems , 2012 , pp . 1097 – 1105 . [ 3 ] Pierre Sermanet , David Eigen , Xiang Zhang , Michaël Mathieu , Rob Fergus , and Yann LeCun , “Overfeat : Integrated recognition , localization and detection using convolutional networks , ” arXiv : 1312 . 6229 , 2013 . [ 4 ] Karen Simonyan and Andrew Zisserman , “Very deep convolutional networks for large - scale image recognition , ” CoRR arXiv : 1409 . 1556 , 2014 . [ 5 ] Ossama Abdel - Hamid , Abdel - rahman Mohamed , Hui Jiang , and Gerald Penn , “Applying convolutional neural networks concepts to hybrid nn - hmm model for speech recognition , ” in Proc . ICASSP , 2012 . [ 6 ] Tara N Sainath , Abdel - rahman Mohamed , Brian Kingsbury , and Bhuvana Ramabhadran , “Deep convolutional neural networks for lvcsr , ” in Proc . ICASSP , 2013 . [ 7 ] Tom Sercu , Christian Puhrsch , Brian Kingsbury , and Yann LeCun , “Very deep multilingual convolutional neural networks for lvcsr , ” Proc . ICASSP , 2016 . [ 8 ] Tom Sercu and Vaibhava Goel , “Advances in very deep convolutional neural networks for lvcsr , ” Proc . Interspeech , 2016 . [ 9 ] Clément Farabet , Camille Couprie , Laurent Najman , and Yann LeCun , “Scene parsing with multiscale feature learning , purity trees , and optimal covers , ” Proc . ICML , 2012 . [ 10 ] Jonathan Long , Evan Shelhamer , and Trevor Darrell , “Fully convolutional networks for semantic segmentation , ” CVPR , 2015 . [ 11 ] Vijay Badrinarayanan , Alex Kendall , and Roberto Cipolla , “Segnet : A deep convolutional encoder - decoder architecture for image segmentation , ” arXiv : 1511 . 00561 , 2015 . [ 12 ] Matthew D Zeiler , Graham W Taylor , and Rob Fergus , “Adaptive deconvolutional networks for mid and high level feature learning , ” in 2011 International Conference on Computer Vision . IEEE , 2011 , pp . 2018 – 2025 . [ 13 ] Hongsheng Li , Rui Zhao , and Xiaogang Wang , “Highly efﬁcient forward and backward propagation of convolutional neural networks for pixelwise classiﬁcation , ” arXiv : 1412 . 4526 , 2014 . [ 14 ] Fisher Yu and Vladlen Koltun , “Multi - scale context aggregation by dilated convolutions , ” proc ICLR , 2016 . [ 15 ] Aaron van den Oord , Sander Dieleman , Heiga Zen , Karen Simonyan , Oriol Vinyals , Alex Graves , Nal Kalchbrenner , Andrew Senior , and Koray Kavukcuoglu , “Wavenet : A generative model for raw audio , ” arXiv : 1609 . 03499 , 2016 . [ 16 ] Dario Amodei , Rishita Anubhai , Eric Battenberg , Carl Case , Jared Casper , Bryan Catanzaro , Jingdong Chen , Mike Chrzanowski , Adam Coates , Greg Diamos , et al . , “Deep speech 2 : End - to - end speech recognition in english and mandarin , ” CoRR arXiv : 1512 . 02595 , 2015 . [ 17 ] Tara N Sainath , Oriol Vinyals , Andrew Senior , and Ha¸sim Sak , “Convolutional , long short - term memory , fully con - nected deep neural networks , ” in proc . ICASSP , 2015 . [ 18 ] Hagen Soltau , George Saon , and Tara N Sainath , “Joint training of convolutional and non - convolutional neural net - works , ” to Proc . ICASSP , 2014 . [ 19 ] George Saon , Tom Sercu , Steven Rennie , and Hong - Kwang J Kuo , “The ibm 2016 english conversational telephone speech recognition system , ” Proc . Interspeech , 2016 . [ 20 ] W Xiong , J Droppo , X Huang , F Seide , M Seltzer , A Stolcke , D Yu , and G Zweig , “The microsoft 2016 conversational speech recognition system , ” arXiv : 1609 . 03528 , 2016 . [ 21 ] George Saon , Hong - Kwang J Kuo , Steven Rennie , and Michael Picheny , “The ibm 2015 english conversational tele - phone speech recognition system , ” Proc . Interspeech , 2015 . [ 22 ] Frantisek Grezl , Martin Karaﬁát , and Lukas Burget , “Investigation into bottle - neck features for meeting speech recog - nition . , ” in Proc . Interspeech , 2009 . [ 23 ] Karel Vesel ` y , Martin Karaﬁát , and František Grézl , “Convolutive bottleneck network features for lvcsr , ” in ASRU , 2011 . [ 24 ] Frantisek Grézl , Martin Karaﬁát , and Karel Vesel ` y , “Adaptation of multilingual stacked bottle - neck neural network structure for new language , ” in Proc . ICASSP , 2014 . [ 25 ] Christian Plahl , Ralf Schlüter , and Hermann Ney , “Hierarchical bottle neck features for lvcsr . , ” in Interspeech , 2010 , pp . 1197 – 1200 . [ 26 ] Ronan Collobert , Christian Puhrsch , and Gabriel Synnaeve , “Wav2letter : an end - to - end convnet - based speech recogni - tion system , ” arXiv : 1609 . 03193 , 2016 . 5