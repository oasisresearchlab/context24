CrowdAnalytics : Support for Redundancy Clustering and Provenance in Crowdsourced Data Analysis Shiry Ginosar Avital Steinitz Abstract —Crowdsourcing can help analysts make sense of large datasets by using paid workers to perform analysis tasks like examining visualizations , generating explanations , and ﬁnding sources that support them . However , explanations generated by multiple workers are often redundant and the quality of explanations can vary heavily depending on the workers who generated them and the sources they used . To utilize crowd work , analysts must be able to quickly make conﬁdence judgments about the explanations that workers produce . We demonstrate two sets of techniques to help analysts manage crowdsourced explanations : ( 1 ) We use workers web searches and browsing histories , as well as explicit highlighting and source - review tasks , to provide analysts with detailed information about where and how each explanation was sourced . ( 2 ) We present crowdsourced and automated techniques to help analysts cluster redundant explanations and identify explanations that occur frequently . To support this process , we use a proxied web browser embedded inside crowdsourced explanation tasks . We expose this information in an explanation - management interface that allows analysts to interactively explore clustered explanations . Index Terms —Crowdsourcing , Social Data Analysis 1 I NTRODUCTION As analysts explore and examine large datasets they often engage in an iterative process of sensemaking involving both visualizations and outside resources [ 7 ] . Generating explanations for trends and outliers in datasets and ﬁnding evidence to support them are key parts of this sensemaking loop and can entail considerable work for the analyst ( ex - amining charts , ideating , ﬁnding and conﬁrming sources , etc . ) . Recent work has shown that using paid crowd workers can provide a scalable way of generating explanations and ﬁnding resources [ 13 ] . However , sets of explanations generated in parallel may contain redundant in - formation and the quality of responses from crowd workers can be highly variable . Moreover , examining and verifying results generated by crowd workers can add additional work for an analyst . Using explanations generated by workers can also be complicated by lack of provenance - information about how explanations were gen - erated and what sources they rely on . For example , it may be unclear if a worker’s explanation derived from a reputable source or is the worker’s own speculation . In order to effectively leverage the crowd’s work , analysts need to quickly make conﬁdence judgments about the explanations and citations that workers produce and decide which ones to pursue . This paper focuses on helping analysts make sense of sets of expla - nations generated by crowd workers . We explore a range of signals that analysts may use to ﬁlter , organize , and make conﬁdence judg - ments about explanations . We then demonstrate techniques to help analysts manage results from the crowd : ( 1 ) We use workers’ web searches and browsing histories , as well as explicit highlighting and source - review tasks , to provide provenance for explanations . Analysts can use this information to make conﬁdence judgments about explanations and decide when and where to explore further . ( 2 ) We demonstrate crowdsourced and automated techniques to help analysts cluster redundant explanations . We propose and evaluate au - tomated and crowd - based approaches for clustering explanations . Our results show that asking workers to cluster explanations directly pro - duces the best clusters . To this end , we use a proxied web browser embedded within crowd - sourced tasks to monitor how workers complete tasks and track the sources that inform their explanations . We expose all of this informa - tion via an explanation - management interface that allows analysts to interactively explore clustered explanations and exposes provenance information . 2 R ELATED W ORK Our work builds on prior work both in social data analysis and crowd - sourcing . Over the past decade , a number of social data analysis systems , in - cluding Sense . us [ 5 ] , Many Eyes [ 11 ] , and Pathﬁnder [ 6 ] have allowed large numbers of users to share , explore , and discuss data visualiza - tions . These research systems , ( as well as commercial social data anal - ysis systems like ata360 . org and the now - defunct Swivel . com ) were premised on the idea that many users operating together can parallelize the process of analyzing datasets . While in practice , motivated users of these tools do explore and discuss extensively [ 12 ] , most visualiza - tions receive little attention and it is difﬁcult for analysts to leverage other users’ effort in a consistent or systematic way . However , evaluations of systems like CommentSpace [ 14 ] have shown that dividing visual analysis tasks into smaller pieces and ex - plicitly dividing the analysis among participants can produce good an - alytic results . We extend this approach , breaking visual analysis into even smaller analysis microtasks and allowing analysts to systemati - cally assign them to crowd workers . These small , granular pieces of work can be incorporated inside analysts’ existing workﬂows . This approach is inspired by systems like Soylent [ 3 ] , which embeds crowd labor into existing word processing workﬂows . Other recent work , has explored the application of distributed col - laborators to sensemaking and analysis tasks . Fisher , et al . [ 4 ] ex - amine how distributed collaborators iteratively build an understanding of information by organizing their work and the work of others . Re - searchers have also explored “instrumenting the crowd” [ 8 ] by aug - menting crowd - based tasks to track workers’ behavior and automati - cally assess the quality of their work . We log worker activity , but use it to help analysts make assessments about explanation provenance . 3 U SEFUL A SPECTS OF E XPLANATIONS When considering explanations for a salient event , an analyst’s key task is to determine whether or not each explanation should be dis - carded , retained , or explored further . To do this , the analyst must determine whether she has conﬁdence in an explanation . An analyst may consider several different aspects of an explanation to make this assessment : Is the explanation text clear , speciﬁc , and plausible ? Some fraction of workers in crowd marketplaces typically satisﬁce - performing the bare minimum amount of work to complete the task - and may gen - erate poorly - constructed , unspeciﬁc , or logically implausible results . By comparison , well - written explanations that appear internally con - sistent can instill greater conﬁdence . Does the explanation cite a reputable source ? If an explanation draws from a source the analyst is familiar with , the analyst can also I t e r a t e ViewResults Analyst Crowd Data SelectCharts Examine Explanations Generate Explanations and Locate Sources Rate ExplanationQuality Check Sources Cluster Explanations Fig . 1 : Our workﬂow for crowdsourcing social data analysis . leverage her knowledge of the source to make conﬁdence judgements about the explanation . Citing a source that an analyst trusts ( for ex - ample a known news organization or reference ) may bolster the ex - planation’s credibility , while an unknown or disreputable source may diminish it . How frequently was an explanation given ? In many cases , multiple workers produce variants of the same explanation . The analyst would wish to consider only a single version rather than reading all of the redundant responses . Additionally , if an explanation is proposed mul - tiple times by different workers , it may indicate that the explanation is more likely a good one . Conversely , a lack of redundant explanations may signal that the cardinality of the space of likely answers is large and the odds that we’ve found the most likely answer is lower [ 10 ] . Does the content of the explanation come from the source or the worker ? When assessing the provenance of an explanation it can be important to understand whether the facts and inferences included in the explanation were produced directly by the worker or were pulled from sources on the web . In our experience , workers who are not do - main experts ( including most workers on Mechanical Turk ) are more adept at pulling good explanations from sources than producing them independently . As a result , explanations that repeat or paraphrase con - tent from a source tend to be more credible than explanations based on facts produced entirely by the worker . As such , providing an indi - cation of whether or not the content is copied or paraphrased directly from the source can aid analysts’ conﬁdence judgements . Is the explanation corroborated by multiple sources ? Multiple ver - sions of an explanation that cite different sources may increase con - ﬁdence even further , since sources can corroborate one another . On the other hand , many copies of the same text appearing in multiple untrusted sources such as blogs may not instill conﬁdence in the au - thenticity of the information content of the text . 4 S UPPORTING THE A NALYST IN H IS T ASK We consider a workﬂow ( Figure 1 ) for crowdsourcing social data anal - ysis similar to [ 13 ] . In this workﬂow , an analyst manually or automat - ically selects views of a dataset that contain trends or other features of interest . The analyst then submits these charts to workers in a paid crowd marketplace who examine and evaluate them in parallel . These workers independently complete analysis microtasks in which they generate candidate explanations for why a trend or an out - lier may have occurred , and provide links to web pages that support their explanation . Each microtask ( Figure 2 ) displays a single chart along with a series of prompts . Using this workﬂow , we can produce large numbers of high - quality explanations for a range of datasets . However , once the explanations are produced it is up to the analyst to examine each one and decide whether or not she is conﬁdent in it and wishes to keep or build on it . This can be time - consuming and may entail considerable effort . Fortunately , analysts can leverage the fact that explanations are of - ten redundant and are frequently supported by known sources on the web to prioritize or triage sets of redundant answers and make conﬁ - dence assessments more quickly . Here we describe crowdsourced and automated methods for extracting provenance and redundancy infor - mation . 4 . 1 Example Deployment In subsequent sections , we consider several approaches for assessing and clustering crowdsourced explanations . To illustrate and test these approaches , we conducted a deployment of our system using work - ers from Amazon Mechanical Turk ( www . mturk . com ) , which we will refer to throughout our discussion . In this deployment , we asked workers to generate explanations for 12 charts drawn from 3 datasets covering a range of public - interest data types ( US employment data for major metropolitan areas , graduation and earning statistics for major universities , and UN food price indices ) . We showed each chart to 10 different workers , for a total of 120 analysis microtasks . 93 workers participated , producing a corpus of 156 explanations . 4 . 2 Collecting Provenance Information In order to make judgements about source reputability , paraphrasing , and corroboration , analysts need more detailed information about the websites workers visit and the speciﬁc source text they use to produce their results . To collect this information , we instrument the analysis microtasks that workers perform so that they provide a record of work - ers’ browsing activity . We also inject custom code into websites that workers visit as part of the task so that workers can highlight speciﬁc portions of source pages that support their explanations ( Figure 2 ) . 4 . 2 . 1 Logging Worker Activity Recording the sites workers visit as they perform microtasks is difﬁ - cult because the same - origin policy [ 1 ] implemented by modern web browsers prevents code from one internet domain from accessing web pages loaded from other domains . As a result , our microtasks cannot monitor activity that occurs in browser windows or tabs that do not originate from our site . We circumvent this restriction by having work - ers browse and search for sources using a custom web browser embed - ded within the analysis microtask ( Figure 2B ) . This custom browser consists of a set of browser controls and an IFrame that loads web pages via our own custom proxy server . Serving and requesting pages via our own server ( Figure 3 ) allows us to log each page a worker vis - its and monitor any web searches they make as they forage for sources and candidate explanations . This proxy infrastructure has a few limitations . For technical and security reasons , we cannot proxy content served using protocols other than HTTP and cannot handle cookies . As a result , we cannot load content from sites that require users to authenticate or log in . Finally , we cannot guarantee that workers perform all of their browsing within our proxied interface and not in another browser window . However , our analysis of log data suggests that most of the sites workers visit are rendered appropriately via the proxy and that workers are active within our browser window for the majority of the time they spend on the task . 4 . 2 . 2 Supporting Fine - Grained Citations Typically , when a worker cites a web page to support an explanation , only a small portion of the page ( a paragraph or even a few sentences ) is directly relevant to their explanation . Page - level citations can make it difﬁcult for analysts or workers in rating microtasks to assess as source , since they may need to examine the entire web page to ﬁnd the relevant text . Using the text of page - level citations to help clus - ter explanations can also be problematic since much of the page text may be unrelated to the explanation and individual pages often provide support for multiple different explanations . We support ﬁner - grained source citations by allowing workers to mark speciﬁc blocks of text within pages as sources . 4 . 3 Detecting Redundancy To help the analyst sift through the metadata created by the crowd we detect redundant explanations and cluster them together . Clustering Fig . 2 : An analysis microtask ( A ) is paired with a proxied web browser embedded inside the HIT ( B ) . The explanation prompts in the interface ( C ) are linked to highlighting tools ( D ) that let workers cite speciﬁc sections of source documents . Web Server Proxy Server EmbeddedBrowser AnalysisTask Internet Worker Log + Source Highlighting Code ( ) 1 2 3 4 5 Fig . 3 : In our instrumented tasks , analysis microtasks are loaded from our web server ( 1 ) . When workers look for evidence using the embed - ded web browser inside the task , page requests are redirected via our proxy server ( 2 ) . The proxy server requests pages from their source ( 3 ) , then logs them and injects custom highlighting code ( 4 ) . Work - ers can then highlight text in embedded browser to have it included directly in their explanations ( 5 ) . redundant explanations can be done with various levels of human in - tervention : in one extreme the clustering is fully automated , in the other clustering is performed manually . In this section we compare methods ranging from one extreme to the other . In a fully automated approach , we use a syntactic text - based similarity metric as input to a clustering algorithm , in particular k - means . In a semi - automated approach , we use a semantic human - generated similarity metric as in - put to the clustering algorithm . We generate this metric by encoding workers’ opinions about pairwise similarities between explanations . In a fully manual approach we explicitly ask human workers to group redundant explanations together . We contrast these three approaches by comparing their performances to a “ground truth” clustering we collected from three experts . 4 . 3 . 1 Automated Clustering The ﬁrst set of methods we discuss use a text - based measure of sim - ilarity between explanations as input to an automated clustering al - gorithm . Although a large number of different text similarity metrics have been developed for comparing and clustering pieces of text the deﬁnition of similarity is often subjective and may vary depending on the task , context , and reader . Here we focus on three variations of syn - tactic metrics that emerge from mining textual data . In essence , we demonstrate how standard text mining algorithms can be used to de - tect redundancies in the explanations . To this end , we represent each explanation as a point in a high dimensional feature space in one of the following ways . Our basic representation constructs a Boolean Bag - of - Words from a textual explanation , as well as their associated URL and selected paragraph . Each explanation in this model is represented by a boolean vector , indexed by all the words in the lexicon . The representation of an explanation e is a vector x , where the w ’th entry of x equals one if and only if the word w occurs at least once in the explanation . This representation is similar to the Vector Space Model but without taking into consideration the frequency in which words appear ( see [ 9 ] ) . Since models based on word frequencies were designed for rep - resenting paragraphs , they seem unﬁtting to our short explanations that contain a couple of sentences at most , and often do not have any word repetitions in them . Using this representation , we compute the similarity between a pair of explanations using Cosine Similarity which naturally compensates for the explanation length [ 9 ] . Since we are using a binomial model , this measure of similarity reduces to a measure of the number of words that co - occur in the two explanations . In order to prevent pairs of explanations from apprearing similar based on the co - occurrence of uninformative words , we perform a manual pre - processing step and remove stop words and all words that appear in the outlier chart . 4 . 3 . 2 Semi - Automated Clustering While automated text similarity methods are scalable and fast , they rely on the assumption that similar explanations will use similar lan - guage . These measures can fail when explanations use different terms to describe the same phenomenon or when the connection between two comments requires outside knowledge . Because detecting these sorts of relationships requires complex processing of natural language and outside context , human workers can make much better similarity assessments . In our semi - automated clustering approach , we use crowd workers to evaluate pairs of explanations and score their similarity . We show workers two explanations at a time and ask them to decide if the two are redundant or not ( Figure 4 ) . Using multiple workers , we collect 5 judgments for every pair of explanations , then average the scores to produce an average similarity score for the pair . We then use these similarity judgments to cluster explanations using k - means . While using worker judgements in lieu of word overlap provides a better measure of explanation similarity , this method also has sev - eral potential drawbacks . Assessing redundancy for all pairs requires (cid:0) n 2 (cid:1) operations and the number of comparisons grows quickly as the number of explanations grows . Comparing only 10 explanations re - quires (cid:0) 10 2 (cid:1) = 45 comparisons and comparing 20 explanations requires (cid:0) 202 (cid:1) = 190 . This number increases even further if scores from multi - ple workers ( in our case , 5 ) are averaged to produce pairwise similarity scores . 4 . 3 . 3 Manual Clustering Finally , we explore a manual clustering approach , in which individ - ual workers examine complete sets of explanations rather than iso - lated pairs and cluster them explicitly . This allows workers to evaluate groups of explanations together and identify larger themes . To simplify the task of specifying clusters , we created a system where workers group comments by color - coding them . In each manual clustering task ( Figure 5 ) , workers see the full set of explanations for a chart and can color code each explanation by clicking in the palette attached to it . When a worker assigns the same color to multiple re - sponses , the responses are pulled together in the list , creating a visual groupings . These groupings can act as intermediate clusters , allowing workers to visually compare similar comments side by side without having to rely as strongly on their working memory . However , be - cause workers must still remember which colors they have assigned to which explanations , this approach may not scale well if the set of explanation is too large to be contained on a single screen . Clustering explanations is a subjective task and the boundaries be - tween clusters can vary depending on subtle interpretations of the ex - planation text . As a result , multiple workers – even well - intentioned and informed ones – may produce different clusterings . On one hand , this makes it practically impossible to pick a clustering by a simple majority vote . On the other hand , it is difﬁcult to combine the clus - terings produced by multiple workers . We observe that if multiple workers cluster explanations in a similar way , there is a high likeli - hood that the clustering is a good one . As a result , we assume that the clusterings that are the most dissimilar from all other clusterings of that set of explanations are likely to be bad , while the clusterings that are the most similar to all the others are likely to be good . To determine which clusterings are the most similar to the oth - ers , we can compute the similarity between all of the clusterings of a chart’s explanations produced by different workers . We then pick the clustering that , on average , is the most similar to all of the other clusterings for that chart , which we term the “central” clustering . In our implementation , we surface only this clustering to the analyst . Fig . 4 : In the semi - automatic method , we show workers pairs of ex - planations for a chart and ask them to decide whether or not the two explanations are redundant . 4 . 3 . 4 Clustering Experiments To compare our three clustering techniques , we conducted an exper - iment in which we used each method ( automated , semi - automated , Fig . 5 : In the manual method , we show workers all explanations for a chart and ask them to create clusters by marking redundant explana - tions with the same color . Similarly - colored explanations are grouped together on - screen , allowing workers to see their clusters in context . manual ) to cluster the 156 explanations produced by workers in our test deployment , and then compared them against expert clusterings . In the semi - automated condition , we created a comparison task for each pair of explanations given for each the 12 charts . We grouped tasks into batches of 20 , and asked ﬁve unique workers to complete each batch . We then averaged the worker scores for each comparison and used k - means clustering to produce a ﬁnal set of clusters . In the manual condition , we asked 10 different workers to cluster the explanations for each of the 12 charts . Because clustering is subjective and no objective “best” clustering exists , we compared the results against manual clusterings generated by three expert users ( the authors ) . There are a variety of different ways to assess the similarity between two clusterings [ 2 ] . We compare clusterings using the F - measure , a similarity metric that is is tolerant to small errors on large clusters , but intolerant to bi - directional impurities . The F - measure similarity for two clusterings is reported on a 0 - 1 scale , where 1 indicates that the clusterings are identical and 0 indicates that they are completely dissimilar . There are many ways of evaluating the similarity between cluster - ings . To elaborate on the difﬁculty in designing a meaningful metric over the space of clusterings we brieﬂy list a few questions we had encountered in our work . Let G be the gold standard clustering , and consider the following cases : 1 . Which should be closer to G , a clustering that reﬁnes G , or a clustering that is reﬁned by G ? 2 . Which should be closer to G , a clustering that agrees with G on the larger clusters but not the smaller ones or vice versa ( a clustering that agrees with G on the smaller clusters but not the larger ones ) ? 3 . Which should be closer to G , a clustering that either reﬁnes or is reﬁned by G , or a clustering that is neither ? The clustering similarity metric that seemed to be the most appro - priate for our needs is the F - measure . We motivate the choice of F , in correlation with the questions posed above , by noting that 1 . it is tolerant to small errors on large clusters Fig . 6 : Average similarity of the various clusterings with the experts clusterings . The “central” worker - experts similarity is almost as good as the experts - experts similarity . 2 . it is intolerant to bi - directional impurities A more thorough discussion of various clustering similarity metrics can be found for example in [ 2 ] . The F measure of a single cluster is the harmonic average of the precision and the recall , and the F measure of an entire clustering is the weighted average of the F measures of all the clusters . Formally , given two clusterings L and R , their F measure is F ( L , R ) = ∑ i | L i | n · max j F ( L i , R j ) where n is the total number of clustered elements , i ranges over the number of clusters in L and j ranges over the clusters in R . The func - tion F ( L i , R j ) is deﬁnes as F ( L i , R j ) = 2 · Recall ( L i , R j ) · Precision ( L i , R j ) Recall ( L i , R j ) + Precision ( L i , R j ) where Recall ( L i , R j ) = Precision ( R j , L i ) = | R j ∩ L i | | R j | 4 . 3 . 5 Results We compare the results of our experiments to the experts clusterings in ( Figure 6 ) . While clustering can be a subjective process , the three experts per - formed quite consistently across most of the charts . Averaged across all charts , the three experts’ clusterings were very similar ( F = 0 . 84 ) and pairwise comparisons between the experts ( E1 - E2 : F = 0 . 84 , E1 - E3 : F = 0 . 85 , E2 - E3 : F = 0 . 83 ) revealed that no one expert was an outlier . While the automated techniques performed as poorly as the average of crowd workers , we ﬁnd that the “central” worker performed almost as similar to one of the experts as the experts were similar to each other . 5 T HE E XPLANATION M ANAGEMENT I NTERFACE Our colleagues in this work have designed and implemented an ex - pressive interface for presenting the mined information to the analyst . Figures 7 and 8 illustrate this interface and the ways in which it ex - poses information about explanation clusters and provenance . 6 D ISCUSSION Here we offer a few observations based on our experience collecting , clustering , and exploring crowdsourced explanations , and offer ideas for future work . Fig . 8 : A closeup of the explanation - management interface , showing two explanation clusters with a single comment visible in each . Choice of clustering algorithms . The clustering task that we inves - tigated in the paper is of a particular interesting size . Clustering ten short and noisy pieces of text is a task too small for most automatic statistical methods . This problem manifested itself in the poor results we achieved using all of our automated methods . Conversely , clustering ten examples is nearing the limits of what hu - mans can comfortably ﬁt in their working memory and perform easily . This results in the observed variation in goodness of the manual clus - terings collected from the crowd . More broadly , this line that straddles the sized of data that are easy for machines versus easy for humans to deal with is an interesting one , and it remains an open question when exactly should one switch from using one method to another . Explanation segmentation . Our current implementation asks work - ers to separate distinct explanations into separate ﬁelds in the explana - tion microtask and allows them to select different source text for each . However , in practice , many workers still give multiple candidate ex - planations as part of a single paragraph or sentence . This can make responses difﬁcult to group , since a single response may contain two or more explanations that belong in disparate clusters . Workers in intermediate segmentation microtasks could break apart compound responses into their constituent explanations , but this intro - duces the potential for information or intent to be lost as workers break apart or alter others’ explanations . This speaks to the broader issue of task granularity when crowd - sourcing creative tasks . Breaking tasks into small , modular compo - nents makes it easier to compose tasks together and process results systematically . Small , straightforward tasks also reduce the potential for worker error , and make it easier to identify and discard poor re - sults . However , small , segmented tasks may actually inhibit contribu - tions from talented or knowledgable workers , since they are not free to explore , author , or contribute outside the constraints of the task and cannot bring their expertise to bear on areas of the problem where it might be beneﬁcial . Crowd composition . Our approach assumes a crowd composed largely of non - expert workers whose responses may be of variable quality - for example , workers recruited in online task markets like Mechanical Turk . With these workers in mind , we designed the analy - sis microtasks to be simple and included little interactivity . We also in - cluded quality - rating and source - checking analysis microtasks to help ﬁlter out low quality results and help analysts identify the most likely explanations . However , more complex analyses or datasets that require speciﬁc domain knowledge may call for the use private crowds . We believe a similar workﬂow could be used to systematically collect and inte - grate ﬁndings from large crowds of trusted workers . In trusted crowds some quality - control mechanisms could be relaxed , reducing the num - ber of post - processing steps and giving workers more freedom to ex - plore . For example , trusted workers could be given the freedom to manipulate the visualization and explore alternate views of the dataset that might inform their explanations . Trusted workers could also self - Fig . 7 : The explanation - management interface . Explanations ( A ) can be clustered and collapsed by chart , topic , and source . Filtering ( B ) and clustering ( C ) controls allow the analyst to hide low - scoring clusters and control how they are nested . Explanations , clusters , and charts , can be dragged to the shoebox ( D ) and annotated for later review . assess the quality of their explanations and sources , reducing the num - ber of steps in the workﬂow while still providing metadata that ana - lysts can use to ﬁlter and reorganize their results . Cross - view clustering . Finally , we currently only cluster redundant explanations that were given for the same outlier . However , know - ing that the same explanation was given for two or more outliers can also be helpful during analysis . Redundant explanations that span views may indicate a connection between the outliers or suggest an underlying phenomenon responsible for broader trends in the dataset . Exposing this sort of cross - view redundancy might help analysts use worker’s speciﬁc observations to make higher - level conclusions about the data . 7 C ONCLUSION In this paper we have explored several techniques that help analysts make sense of the explanations generated by workers in crowdsourced data analysis workﬂows . We show that on small scale tasks the aver - age worker as well as automated NLP tools perform poorly . To over - come the shortage of the manual approach , we present a heuristic for identifying workers who produce high quality clusterings . To over - come the shortage of the automatic approach we propose collecting much more data and validating its quality . Additionally , we begin to identify the provenance of explanations generated by crowd workers , although much remains to be done in this area in future work . Finally , we present an interface that surfaces information to the analyst , that may help her make informed choices about the data she is analyzing . A CKNOWLEDGMENTS Part of this text has been written and edited by Wesley Willett , Ma - neesh Agrawala and Bj ¨ orn Hartmann . The authors wish to thank them for their instruction throughout the work on this project . R EFERENCES [ 1 ] Same Origin Policy - Web Security . [ 2 ] E . Amig ´ o , J . Gonzalo , J . Artiles , and F . Verdejo . A comparison of extrin - sic clustering evaluation metrics based on formal constraints . Inf . Retr . , 12 ( 4 ) : 461 – 486 , Aug . 2009 . [ 3 ] M . Bernstein , G . Little , and R . Miller . Soylent : a word processor with a crowd inside . In Proceedings of the . . . , 2010 . [ 4 ] K . Fisher , S . Counts , and A . Kittur . Distributed Sensemaking : Improving Sensemaking by Leveraging the Efforts of Previous Users . Technical report , Feb . 2012 . [ 5 ] J . Heer and F . Vi´egas . Voyagers and voyeurs : Supporting asynchronous collaborative visualization . Communications of the ACM , 2009 . [ 6 ] K . Luther , S . Counts , K . B . Stecher , A . Hoff , and P . Johns . Pathﬁnder : An Online Collaboration Environment for Citizen Scientists . pages 1 – 10 , Jan . 2009 . [ 7 ] P . Pirolli . Rational analyses of information foraging on the web . Cognitive Science , 29 ( 3 ) : 343 – 373 , 2005 . [ 8 ] J . M . Rzeszotarski and A . Kittur . Instrumenting the crowd : using implicit behavioral measures to predict task performance . In the 24th annual ACM symposium , page 10 . ACM Press , 2011 . [ 9 ] B . Stone and S . Dennis . Comparing Methods for Single Paragraph Simi - larity Analysis . Topics in Cognitive Science , 2011 . [ 10 ] B . Trushkowsky , T . Kraska , M . J . Franklin , and P . Sarkar . Getting It All from the Crowd . arXiv . org , cs . DB , Feb . 2012 . [ 11 ] F . Vi´egas and M . Wattenberg . Manyeyes : a site for visualization at inter - net scale . Visualization and . . . , 2007 . [ 12 ] F . B . Vi´egas , M . Wattenberg , M . McKeon , F . v . Ham , and J . Kriss . Harry Potter and the Meat - Filled Freezer : A Case Study of Spontaneous Usage of Visualization Tools . pages 1 – 10 , Sept . 2007 . [ 13 ] W . Willett , J . Heer , and M . Agrawala . Strategies for crowdsourcing so - cial data analysis . In ACM Human Factors in Computing Systems ( CHI ) , 2012 . [ 14 ] W . Willett , J . Heer , J . Hellerstein , and M . Agrawala . CommentSpace : structured support for collaborative visual analysis . In CHI ’11 : Pro - ceedings of the 2011 annual conference on Human factors in computing systems . ACM Request Permissions , May 2011 .