How Provenance helps Quality Assurance Activities in AI / ML Systems Takao Nakagawa National Institute of Advanced Industrial Science and Technology ( AIST ) Koto - ku , Tokyo , Japan nakagawa - takao @ aist . go . jp Kenichiro Narita National Institute of Advanced Industrial Science and Technology ( AIST ) Koto - ku , Tokyo , Japan narita - kenichirou @ aist . go . jp Kyoung - Sook Kim ∗ National Institute of Advanced Industrial Science and Technology ( AIST ) Koto - ku , Tokyo , Japan ks . kim @ aist . go . jp ABSTRACT Quality assurance is required for the wide use of artificial intel - ligence ( AI ) systems in industry and society , including mission - critical areas such as medical or disaster management domains . However , the quality evaluation methods of machine learning ( ML ) components , especially deep neural networks , have not yet been established . In addition , various metrics are applied by evaluators with different quality requirements and testing environments , from data collection to experimentation to deployment . In this paper , we propose a quality provenance model , AIQPROV , to record who evaluated quality , when from which viewpoint , and how the evalu - ation was used . The AIQPROV model focuses on human activities on how to apply this to the field of quality assurance , where hu - man intervention is required . Moreover , we present an extension of the W3C PROV framework and conduct a database to store the provenance information of the quality assurance lifecycle with 11 use cases to validate our model . CCS CONCEPTS • Computing methodologies → Machine learning ; Artificial intelligence ; • Social and professional topics → Quality as - surance ; • Software and its engineering → Traceability . KEYWORDS machine learning , artificial intelligence , quality assessment , devel - opment lifecycle , testing , provenance ACM Reference Format : Takao Nakagawa , Kenichiro Narita , and Kyoung - Sook Kim . 2022 . How Provenance helps Quality Assurance Activities in AI / ML Systems . In The Second International Conference on AI - ML Systems ( AIMLSystems 2022 ) , Oc - tober 12 – 15 , 2022 , Bangalore , India . ACM , New York , NY , USA , 9 pages . https : / / doi . org / 10 . 1145 / 3564121 . 3564801 ∗ Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India © 2022 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 9847 - 3 / 22 / 10 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3564121 . 3564801 1 INTRODUCTION Industrial use of artificial intelligence ( AI ) systems with machine learning ( ML ) components is spreading and expanding into mission - critical areas such as automated driving and finance . Quality as - surance of AI systems is essential in these areas ; however , AI sys - tems pose several challenges for software engineering compared to traditional software development due to their uncertainty and data - centricity [ 16 ] . In [ 6 ] , three dimensions of artifact type , pro - cess , quality characteristics are specified to characterize AI systems and quality assurance . Some literature also cites challenges due to the complexity of teams , processes , and development environments [ 1 , 3 , 19 ] . During the software development and testing life cycle , trace - ability identifies and tracks changes in artifacts , test methods , and evaluation results . It can also help capture the complex linkages be - tween functions and even the impact of human factors . This paper focuses on the role of traceability in achieving quality assurance for industrial applications , from the following perspectives : • Human in the loop : The non - deterministic behaviors of ML components ( such as datasets , models and / or training programs ) and ethical issues require human quality assess - ment and judgment throughout the process in the AI system development [ 1 , 8 ] . • Diverse project members with different expertise : The project team requires members with diverse expertise , such as data scientists , system developers , and domain experts . Lack of other expertise often leads to process fragmentation among team members [ 5 ] . • Different Frameworks for different expertise : Devel - opment relies on different platforms and software supply chains , such as Integrated development environments ( IDEs ) for developers , scientific notebooks for researchers , and data pipelines for operators [ 19 , 20 ] . Moreover , ML components for AI systems are evaluated from different metrics by different actors in the development phases . In [ 22 ] , the different perspectives of metrics between developers and managers was addressed through surveys . Traceability across phases and ecosystems is fundamental to quality assurance to bet - ter understand the artifacts , processes , and quality characteristics considered by members in each phase . Data provenance is a general - purpose methodology to record and manage the history of how , when , and by whom data and articles have been created , refer - enced , and processed , in the form of graphs , etc . The PROV Model AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . by W3C 1 and the Open Provenance Model 2 are known as standards for the representation of provenance information . In recent , both academia and industry have been considering the provenance information and management into the development of AI systems . PROV - ML [ 21 ] is an example method for documenting the provenance of ML components , particularly for the scientific machine learning pipeline , and is provided as an extension of the PROV model . It records detailed mechanical and computational reference and processing histories on pipelines in the development environments like notebooks . However , existing studies of prove - nance in ML workflow overlook human evaluation and judgment with natural language about quality requirements in the quality evaluation and assurance process , not numerical measures . In this paper , we propose a quality provenance model , AIQPROV , with the following characteristics and contribute to the future development of this research field by confirming the feasibility of the model through a scenario - based evaluation of its usefulness . With the AIQPROV model , the provenance of these quality assurance - related documents and assets is managed and recorded , and how they af - fected each component through the activities of experimenters and developers . These are the contributions of this paper : ( 1 ) We propose a method to record each quality assessment and track the improvement processes based on personnel opinions , judgments , and decisions . ( 2 ) We show an extension of W3C PROV , a provenance standard , to be compatible with other provenance information in the ML workflow . The rest of this paper is organized as follows . Section 2 addresses the relevant studies and tools . Section 3 presents the motivating scenarios and our quality provenance model . In Section 4 , we pro - vide feasibility studies of each scenario that mimics the quality control cycle of an ML component . Finally , Section 5 summarizes this study . 2 RELATED WORKS 2 . 1 Quality Management for AI Systems Quality evaluation methods for ML components have been pro - posed in two main directions . One is to apply conventional software quality techniques to machine learning systems , and the other is to propose new quality techniques for machine learning systems . The ISO / IEC 25000 series , also known as SQuaRE ( System and Software Quality Requirements and Evaluation ) , is an international standard that provides a framework and quality characteristics for the quality assessment of conventional software engineering systems [ 9 ] . Kuwajima et al . analyzed how SQuaRE , a conventional software quality technique , can be adapted to AI systems [ 11 ] . ML components have different characteristics from traditional software and traditional software testing techniques cannot be ap - plied as is ; Dusica et al . discussed the inherent challenges of quality assurance for machine learning - based systems and concluded that adaptation of traditional testing techniques with the support of ML techniques is promising [ 13 ] . Jie et al . also summarized a paper on testing machine learning systems , providing a comprehensive 1 https : / / www . w3 . org / TR / prov - overview / 2 https : / / openprovenance . org / opm / overview and analysis of research results on ML evaluation [ 23 ] . Narita et al . proposed an ML evaluation testbed that applies the FAIR Principles , which describe appropriate methods of data dis - covery , to ML testing methods [ 15 ] On the other hand , several studies have analyzed key challenges related to the means of developing AI systems incorporating ML components and their quality , based on practice and technical ar - ticles on the Web and elsewhere . Felderer et al . summarized the terminological differences and overlaps between AI and software engineering , and discussed eight key challenges for applying the findings of both fields to the quality assurance of AI - based sys - tems [ 6 ] . Amershi et al . conducted an extensive interview and questionnaire survey of teams and personnel involved in the devel - opment of AI systems at Microsoft , noting differences from regular software development and skill sets [ 1 ] . Hutchinson et al . summa - rized the different evaluation perspectives and points that are often overlooked when evaluating AI systems and ML components . The focus here is on a Learner - centric perspective , which is appropri - ate for evaluating machine learning algorithms ( Learner ) , and an application - centric perspective , which focuses on the evaluation of the practical application surrounding the model [ 8 ] . 2 . 2 Provenance Frameworks for AI Systems In general , provenance information is documented with the history of data and things , such as the source or origin , changes , and orga - nizations of authenticity . The PROV ( Provenance ) model proposed by the Provenance Working Group of the W3C ( World Wide Web Consortium ) defines a data model and serialization to support the exchange of historical information on the Web [ 12 ] . In [ 21 ] , Souza et al . have proposed a provenance data repre - sentation ( PLOV - ML ) that conforms to W3C PROV and W3C ML schemas to represent workflows in the ML lifecycle . Ruppprecht et al . proposed and evaluated a new provenance recording system named URSPRUNG to automate provenance recording between data in an increasingly complex data science pipeline [ 17 ] . Samuel et al . have proposed MLProvLab , which allows data scientists and ML practitioners to track , capture , compare , and visualize the prove - nance of machine learning notebooks [ 10 ] . Even though the provenance has been addressed in the traceabil - ity of machine learning pipelines , it is not used for quality assess - ment with human judgment over AI / ML components . For example , ML quality issues can be caused by human error or incomplete reporting of data quality . 2 . 3 Practical Tools for Supporting Reproducibility of the ML Component The execution of machine learning algorithms involves a series of tasks , including preprocessing , feature extraction , model fitting , and validation . There are a number of development support tools for machine learning that divide these tasks and assist development by managing the pipeline . Several industrial company offers prominent development sup - port tools for AI development : Kubeflow 3 Pipelines develop func - tions in containers and connect them to form workflows when 3 https : / / www . kubeflow . org / docs / components / pipelines / introduction / How Provenance helps Quality Assurance Activities in AI / ML Systems AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India developing AI systems [ 2 ] . Apache AirFlow 4 manages the func - tions to be developed and the procedures to be developed as a pipeline [ 7 ] . Denis et . al . proposed TensorFlow Extended ( TFX ) 5 that can be used in production [ 14 ] . However , ML pipelines poses challenges in reproducibility and manageability of data dependencies [ 18 , 19 ] . Provenance is effec - tive in addressing ML pipeline challenges because it helps data scientists better understand complex ML pipelines and facilitates analysis , debugging , and reproduction of ML experiments . pipeline management of ML development tasks is mainstream ML develop - ment , but there is no provenance link between different ML pipeline management tools or human - related activities . There are also several tools that refer to the management of provenance . Specifically , these tools manage versions of models , data , and experiments , such as Pachyderm 6 , DVC 7 and MLflow Tracking 8 . These tools are not stand - alone tools for managing the entire provenance ( in the academic sense ) of ML Pipeline or system development activities , but they may contribute to maintaining high traceability when used in conjunction with more strict provenance management in that they can manage the version of each asset , its reuse , and the influence between versions of assets . 3 PROVENANCE MODEL FOR QUALITY ASSESSMENT OF AI SYSTEMS We first present scenarios to help understand the provenance infor - mation about the quality assurance of AI systems . Next , we describe an extended model of the W3C PROV as provenance records based on the scenarios . 3 . 1 Motivating Scenarios Table 1 shows the use scenarios to extract elements of provenance information for possible discrepancies and search requirements . These scenarios are collected from the guideline in [ 4 ] and the framework introduced in [ 15 ] . Although the scenarios are subjec - tive and cannot cover all use cases of the machine learning quality management and evaluation processes , we can consider them typi - cal use cases in the development life cycle of an industry . As mentioned in the background , developing an AI / ML system requires the involvement of multiple experts in different fields . Therefore , experts with other specialties are required to evaluate and ensure the quality of products from different perspectives . In such an environment , there is a risk that differences in perception of quality among the team members may deteriorate the quality of the final product . For example , if two or more quality measures are in a trade - off relationship , consultation between experts and managers will be necessary for making their decisions . Therefore , we investigated a set of scenarios in which each ex - pert is motivated to carefully examine the provenance of the quality of the deliverable at hand and to ensure that he or she is not inter - fering with other quality characteristics in order to fulfill his or her responsibilities . In this paper , we propose a model for provenance recording for quality management of machine learning components 4 https : / / airflow . apache . org / 5 https : / / www . tensorflow . org / tfx 6 https : / / www . pachyderm . com / 7 https : / / dvc . org / 8 https : / / www . mlflow . org / docs / latest / tracking . html through abstraction of the elements ( entities and activities to be recorded ) that appear in these scenarios . In the evaluation , the provenance data recorded according to the model will be explored on a graph DB to show the feasibility of representative scenarios . 3 . 2 AIQPROV AIQPROV is an extension of W3C PROV , a standard for provenance records , in order to maintain compatibility with other provenance records . It represents a provenance model and records for quality assurance of AI systems to support our use cases described in Table 3 . A group of elements that should be recorded in the quality control lifecycle of machine learning are defined as sub - classes of each element ( Entity , Activity , Agent ) in W3C PROV . Table 2 and 3 lists the elements and their definitions in AIQPROV , respectively . In conjunction with the extraction of elements , we provide a reference association model of AIQPROV entities that shows how each element in AIQPROV relates to each other ( e . g . , " quality as - sessment " activity generates a " quality measurement results " entity ) over a typical quality assurance lifecycle ( target lifecycle ) for devel - opment of AI / ML system . The overview of the reference association model is shown in Figure 1 . In the remainder of this section , we describe in detail the development of the target lifecycle and the intent of the reference association model . First , in parallel with the element extraction , we developed a target lifecycle that we believe is common to many AI / ML system development projects through discussions and surveys of three experts ( two of whom are authors ) with experience in research and development on quality management of AI / ML systems in industrial companies . The target life cycle of AIQPROV is based on a continuous improvement model that includes the processes of planning , evaluation , improvement , and execution . Continuous improvement is practiced at various granularities in the develop - ment of AI systems at the business / management level ( e . g . , PDCA cycle of ISO 9001 and OODA loop ) , at the development process level ( e . g . , Iterative Software Development , MLOps . ) , and in experimental activities by scientists . Since the goal of AIQPROV is to organize the provenance related to ML components’ quality , we will focus on the part of these loops that is specifically related to the provenance of the quality assurance process for ML components ( e . g . , data sets , models ) . Specifically , we assume the following process : ML components are subject to quality control , these components are evaluated from a predetermined perspective , a quality improvement plan is developed based on the evaluation results , and finally the plan is executed . Based on these considerations , the target life cycle was assumed to have an outer cycle ( upper part of the reference association model ) that includes organizing requirements and developing ac - ceptance criteria for the ML component , and an inner cycle ( lower part ) that is responsible for evaluation , improvement , and imple - mentation of the quality control target . In addition , each activity was assumed to be able to be concealed / abstracted into four activ - ities ( QC Planning , Quality Evaluation , Experiment , and Apply ) , indicated in gray box , depending on the required record granularity . ( 1 ) Determine the QC Plan and Quality Viewpoint ( perspec - tives / quality characteristics to be evaluated ) in the outer cycle . AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . Table 1 : Motivating Scenarios Id Title Scenario 1 Which is the quality about the target entity ? Unexpected behavior of the model was observed in the operational environment . We would like to confirm the quality of the model on the verification environment for future correction . 2 Which entity is this qual - ity corresponds to ? I received a dataset from the data management team along with a quality report . We would like to confirm that the quality report is in fact tied to the received dataset . 3 Quality of previous revi - sions of the subject We would like to determine if there has been any degrade in the fairness of the model compared to the past revisions . We should examine changes in quality for each revision of the target model . 4 What are the non - managed entities that affected a certain quality ? Our measurement revealed a bias in classification accuracy . No defects were found in the model training code or model itself . We would like to investigate other factors ( measurement program , test data , and settings at the time of measurement ) that affected the measurement results . 5 Who are the quality re - porters and quality mea - surers ? While viewing a quality report on a trained model , I found a discrepancy between the measurement results and the quality opinion . We would like to identify and speak directly with the evaluator who was involved in the relevant evaluation . 6 Which quality report led to which quality improve - ment activities ? Different teams measure general data quality and security - related quality ( e . g . , checking to see if offensive operational data is mixed in ) for the collected operational data . We would like to see what quality improvement measures have been taken for security - related quality reports . 7 Which program was used to measure a quality ? I had recently seen an alert that a flaw had been found in the fairness measurement program . We would like to confirm the version of the program used for this quality measurement . 8 What quality improve - ment plans have been de - veloped ? As an experimenter , I experimented with model architecture to improve training speed . The training pipeline for production also reflected the experiments in adversarial training . I would like to know where my experiments were reflected in the training pipeline . 9 What plans for quality im - provement were consid - ered ? A PR was submitted for a training pipeline to improve model quality . As a maintainer of the pipeline , I have found that the author of PR employed a model architecture that he is not familiar with . I would like to know why this architecture was adopted . 10 Who determined the qual - ity improvement plan ? A dataset provided by another team contained a adversarial case that is not necessary in our project . The previous version was not a referenceable IRI and the original dataset was not available . We would like to identify an experiment planner to take over the original dataset . 11 What process has been used to improve quality ? After the model was revised , problems related to fairness occurred in the operational environment only in the new version of the model . We need to know the whole picture of the quality control process . Table 2 : Defined Activities Title Definition Measurement An act of measuring the QC Target from a spe - cific Quality Viewpoint . Quality - Aggregation An act of gathering the Measures of a QC Target and compiling them into a Quality entity . Analysis An act of analyzing Quality based on Accep - tance Criteria to write a quality Report . ExperimentDesign An act of creating an Experiment Plan to im - prove the quality based on a Report . Experiment An act of executing the Experiment Plan for the QC Target and obtaining Experiment Result . Decision An act of creating a Manufacturing Plan for the QC Target based on the Experiment Result . Apply An act of executing a Manufacturing Plan to improve the Quality of the QC Target . QC Planning An act of developing an overall QC Plan for the activities related to quality control . Criteria Esta - blishment An act of establishing specific quality accep - tance criteria based on the QC Plan . ( 2 ) QC Target measurement , aggregation of measurement re - sults , and analysis activities are conducted in accordance with Acceptance Criteria . ( 3 ) Experiments for quality improvement are planned based on the Quality Report that summarizes the results . ( 4 ) Planned experiments will be conducted . ( 5 ) Based on the results of the experiments , the processing ( re - vision ) of QC Targets is planned and executed . ( 6 ) The processing plan is executed . The quality improvement and revision of QC Target are achieved . 4 EVALUATION 4 . 1 Implementation To check the feasibility of the proposed model in the previous section , we implemented a prototype library that would realize a provenance record When quality assessments are conducted in a manner that is compliant with our target lifecycle . Our prototype was implemented using the PROV 9 package in Python . In the prototype , the experiment and processing plans are de - scribed in Jupyter Notebook format , treating the notebook before 9 https : / / pypi . org / project / prov / How Provenance helps Quality Assurance Activities in AI / ML Systems AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Apply QC Planning Quality Evaluation Quality QC Target Measurement Measure Quality Aggregation Apply Quality Policy Acceptance Criteria Quality Analysis QC Planning Criteria Establishment QC Plan Quality ViewPoint Measurer MeasurerConﬁguration Quality Report Experiment Experiment Design Experiment Decision Experiment Plan ExperimentalResult Manufacturing Plan hasRole : Evaluator hasRole : Measurer hasRole : Executor hasRole : QC Manager hasRole : Experimentor Agent Entity Activity OUTER CYCLE INNER CYCLE Activities at a higher level of abstraction Figure 1 : A Reference Association Model of QProv Entities Table 3 : Defined Entities Title Definition Measurer An instrument for measuring the Quality based on a specific method , Viewpoint . Configuration Configuration at the time of Measurement . Viewpoint An individual quality viewpoints to be consid - ered during quality control . Measure Measurement results for a given Viewpoint . Quality An overall picture of quality consisting of mul - tiple Measure results . QC Target The asset to be subject to quality control . AcceptanceCriteria Specific quality Acceptance Criteria according to the QC Plan . Report Human readable document of Analysis results . ExperimentPlan An executable experiment plan to improve the target’s Quality . ExperimentResult A result of an experiment . Manufactu - ring Plan An executable and productionized plan to mod - ify and improve the target’s Quality . QC Plan Documentation of quality control processes and plans . execution as the plan and the notebook after execution as the re - sult . In order to clearly distinguish the Notebook before and after execution , we have implemented the functionality which automati - cally commits the result of each notebook execution to the version control system . The IRIs associated with the ( notebook ) elements were designed to be uniquely determined using commit IDs on the version control system ( Here we assume the version control is done with git and GitHub ) . We have created a hypothetical case study of provenance records using a prototype library based on the target life cycle , for the scenario based evaluation . ( 1 ) Two quality measurement activities that take a trained model ( QC Target ) , test data , and evaluate its accuracy in terms of model accuracy and Adversarial Robustness . ( 2 ) Quality analysis activities that aggregate the results of 1 and 2 , add a quality opinion according to the acceptance criteria , and prepare a report . ( 3 ) Experiments on Jupyter Notebook to search for conditions that can improve Adversarial Robustness based on analysis results . ( 4 ) Processing of trained models based on experimental results on Jupyter Notebook . The case study assumes a situation in which models , datasets , and other assets are managed and evaluated on an AI / ML quality eval - uation platform ( Qunomon ) under development at the authors’ organization . In addition , experiments and revisions for quality improvement were recorded as if they were planned and executed on Jupyter Notebook environment . Figure 2 shows the graph of provenance when the provenance is recorded according to the above scenarios . In the following sections , we will conduct a scenario - based evaluation to verify whether the searches required by the motivating scenarios described above can be realized on this graph . 4 . 2 Procedure We will conduct a scenario - based evaluation of the AIQPROV frame - work to confirm that provenance records satisfy each of the require - ments listed in the motivating scenarios and enable reference to the information needed for quality assurance of the AI / ML system . The evaluation procedure is described below . ( 1 ) A Provenance Record of the iterative trial of the assumed life cycle is created by adding to the record shown in Figure 2 in the Implementation section . ( 2 ) The recorded Provenance Record is stored on the graph DB . ( 3 ) The authors confirm that it is possible to write queries that extract the information required by each scenario from the graph DB . AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . Qunomon test data test / 1 / exec / 1 wasGeneratedBy test / 2 / exec / 1 wasGeneratedBy adversarial _ acc _ 1 . 0 acc _ 1 . 0 config config Model Stability Model Correctness used used used used used wasAssociatedWith used used used used used wasAssociatedWith retrained model retraining wasGeneratedBy Result of retraining wasGeneratedBy wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith cctrt : kim Retrained Model retraining wasGeneratedBy wasDerivedFrom plan : retraining wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith model wasAssociatedWith result : exec1 result : exec2 quality / 1 Aggregation wasGeneratedBy wasDerivedFrom wasDerivedFrom Opinion : Adversarial Test Opinion : Accuracy Test Quality Report report _ gen / 1 wasGeneratedBy used used used used used used used Criteria : Adversarial Test test / 1 / criteria _ est / 1 wasGeneratedBy Criteria : Accuracy Test test / 2 / criteria _ est / 1 wasGeneratedBy wasAssociatedWith wasAssociatedWith wasAssociatedWith cctrt : nakagawa Measurement / Analysis Experiment Manufacture cctrt : narita Figure 2 : An Example of Provenance Record for Entire AI / ML Quality Management Lifecycle AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . Qunomon test data test / 1 / exec / 1 wasGeneratedBy test / 2 / exec / 1 wasGeneratedBy adversarial _ acc _ 1 . 0 acc _ 1 . 0 config config Model Stability Model Correctness used used used used used wasAssociatedWith used used used used used wasAssociatedWith retrained model retraining wasGeneratedBy Result of retraining wasGeneratedBy wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith cctrt : kim Retrained Model retraining wasGeneratedBy wasDerivedFrom plan : retraining wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith model wasAssociatedWith result : exec1 result : exec2 quality / 1 Aggregation wasGeneratedBy wasDerivedFrom wasDerivedFrom Opinion : Adversarial Test Opinion : Accuracy Test Quality Report report _ gen / 1 wasGeneratedBy used used used used used used used Criteria : Adversarial Test test / 1 / criteria _ est / 1 wasGeneratedBy Criteria : Accuracy Test test / 2 / criteria _ est / 1 wasGeneratedBy wasAssociatedWith wasAssociatedWith wasAssociatedWith cctrt : nakagawa Measurement / Analysis Experiment Manufacture cctrt : narita Figure 2 : An Example of Provenance Record for Entire AI / ML Quality Management Lifecycle Query 1 : Cypher Query for Scenario 4 . MATCH ( en ) < - - ( ac ) < - [ * ] - ( ql ) WHERE " qprov : Measurement " in ac . ` prov : type ` AND ql . ` prov2neo : identifier ` = " mlc / 1 / quality / 1 " AND en . ` prov : type ` < > " qprov : QCTarget " RETURN distinct en We have used Neo4j 10 as the graph DB to store provenance data during evaluation . Therefore , Cypher was employed as the query language to evaluate the feasibility of each scenario . We have also used prov2neo 11 library to convert and upload provenance infor - mation generated through PROV package into neo4j database . 4 . 3 Result For all scenarios , we were able to confirm that it is possible to write queries that satisfy the requirements . For the sake of space , we show three randomly selected queries as Query 1 , 2 , and 3 . In all cases , it was confirmed that the requirements of the scenar - ios could be investigated with as little as few lines of description . Similar results were obtained for scenarios not shown here . The following describes in detail the results of searching each query against the provenance of Figure 2 . 4 . 3 . 1 Result of Query 1 ( scenario 4 ) . Figure 3 shows the results of executing Query 1 . The figure shows the results as Neo4j’s graph view , with blue representing entities and orange representing activ - ities . For readability , the figure includes not only the ’ en ’ ( entities ) in the query but also the ’ ac ’ ( quality measurement activity ) . Scenario 4 is a scenario in which we consider that the quality doubt is due to the influence of entities other than the quality control target , and we investigate all entities that affected quality . It can be seen that the quality of the trained model is guided by two different measurements , each of which is influenced by the measuring instrument , the setting of the instrument , and the quality perspective , respectively . 10 https : / / neo4j . com / 11 https : / / github . com / DLR - SC / prov2neo Query 2 : Cypher Query for Scenario 9 . MATCH p = ( ex ) < - [ * ] - ( mp ) WHERE mp . ` prov2neo : identifier ` = " qprovrepo : blob / 9b98c7 / example / manu . ipynb " ↩ → AND " qprov : Experiment " in ex . ` prov : type ` AND NONE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN distinct ex Query 3 : Cypher Query for Scenario 11 . MATCH ( qt1 ) < - [ : wasDerivedFrom ] - ( qt2 ) , p = ( ap ) < - - ( qt2 ) , p2 = ( qt1 ) < - [ * ] - ( ap ) WHERE " qprov : QCTarget " in qt1 . ` prov : type ` AND qt2 . ` prov2neo : identifier ` = " mlc / 1 / inv / 4 " AND " qprov : Apply " in ap . ` prov : type ` AND SINGLE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → AND SINGLE ( n IN nodes ( p2 ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN p , p2 Figure 3 : Result of Query 1 ( Graph View ) The details of each node displayed graphically can be viewed in JSON Notation format . Figure 4 shows the details of the entity We have used Neo4j 10 as the graph DB to store provenance data during evaluation . Therefore , Cypher was employed as the query language to evaluate the feasibility of each scenario . We have also used prov2neo 11 library to convert and upload provenance information generated through PROV package into neo4j database . 4 . 3 Result For all scenarios , we were able to confirm that it is possible to write queries that satisfy the requirements . For the sake of space , we show three randomly selected queries as Query 1 , 2 , and 3 . In all cases , it was confirmed that the requirements of the scenar - ios could be investigated with as little as few lines of description . Similar results were obtained for scenarios not shown here . The following describes in detail the results of searching each query against the provenance of Figure 2 . 4 . 3 . 1 Result of Query 1 ( scenario 4 ) . Figure 3 shows the results of executing Query 1 . The figure shows the results as Neo4j’s graph view , with blue representing entities and orange representing activ - ities . For readability , the figure includes not only the ’ en ’ ( entities ) in the query but also the ’ ac ’ ( quality measurement activity ) . Scenario 4 is a scenario in which we consider that the quality doubt is due to the influence of entities other than the quality control target , and we investigate all entities that affected quality . It can be seen that the quality of the trained model is guided by two different measurements , each of which is influenced by the measuring instrument , the setting of the instrument , and the quality perspective , respectively . 10 https : / / neo4j . com / 11 https : / / github . com / DLR - SC / prov2neo AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . Qunomon test data test / 1 / exec / 1 wasGeneratedBy test / 2 / exec / 1 wasGeneratedBy adversarial _ acc _ 1 . 0 acc _ 1 . 0 config config Model Stability Model Correctness used used used used used wasAssociatedWith used used used used used wasAssociatedWith retrained model retraining wasGeneratedBy Result of retraining wasGeneratedBy wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith cctrt : kim Retrained Model retraining wasGeneratedBy wasDerivedFrom plan : retraining wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith model wasAssociatedWith result : exec1 result : exec2 quality / 1 Aggregation wasGeneratedBy wasDerivedFrom wasDerivedFrom Opinion : Adversarial Test Opinion : Accuracy Test Quality Report report _ gen / 1 wasGeneratedBy used used used used used used used Criteria : Adversarial Test test / 1 / criteria _ est / 1 wasGeneratedBy Criteria : Accuracy Test test / 2 / criteria _ est / 1 wasGeneratedBy wasAssociatedWith wasAssociatedWith wasAssociatedWith cctrt : nakagawa Measurement / Analysis Experiment Manufacture cctrt : narita Figure 2 : An Example of Provenance Record for Entire AI / ML Quality Management Lifecycle Query 1 : Cypher Query for Scenario 4 . MATCH ( en ) < - - ( ac ) < - [ * ] - ( ql ) WHERE " qprov : Measurement " in ac . ` prov : type ` AND ql . ` prov2neo : identifier ` = " mlc / 1 / quality / 1 " AND en . ` prov : type ` < > " qprov : QCTarget " RETURN distinct en We have used Neo4j 10 as the graph DB to store provenance data during evaluation . Therefore , Cypher was employed as the query language to evaluate the feasibility of each scenario . We have also used prov2neo 11 library to convert and upload provenance infor - mation generated through PROV package into neo4j database . 4 . 3 Result For all scenarios , we were able to confirm that it is possible to write queries that satisfy the requirements . For the sake of space , we show three randomly selected queries as Query 1 , 2 , and 3 . In all cases , it was confirmed that the requirements of the scenar - ios could be investigated with as little as few lines of description . Similar results were obtained for scenarios not shown here . The following describes in detail the results of searching each query against the provenance of Figure 2 . 4 . 3 . 1 Result of Query 1 ( scenario 4 ) . Figure 3 shows the results of executing Query 1 . The figure shows the results as Neo4j’s graph view , with blue representing entities and orange representing activ - ities . For readability , the figure includes not only the ’ en ’ ( entities ) in the query but also the ’ ac ’ ( quality measurement activity ) . Scenario 4 is a scenario in which we consider that the quality doubt is due to the influence of entities other than the quality control target , and we investigate all entities that affected quality . It can be seen that the quality of the trained model is guided by two different measurements , each of which is influenced by the measuring instrument , the setting of the instrument , and the quality perspective , respectively . 10 https : / / neo4j . com / 11 https : / / github . com / DLR - SC / prov2neo Query 2 : Cypher Query for Scenario 9 . MATCH p = ( ex ) < - [ * ] - ( mp ) WHERE mp . ` prov2neo : identifier ` = " qprovrepo : blob / 9b98c7 / example / manu . ipynb " ↩ → AND " qprov : Experiment " in ex . ` prov : type ` AND NONE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN distinct ex Query 3 : Cypher Query for Scenario 11 . MATCH ( qt1 ) < - [ : wasDerivedFrom ] - ( qt2 ) , p = ( ap ) < - - ( qt2 ) , p2 = ( qt1 ) < - [ * ] - ( ap ) WHERE " qprov : QCTarget " in qt1 . ` prov : type ` AND qt2 . ` prov2neo : identifier ` = " mlc / 1 / inv / 4 " AND " qprov : Apply " in ap . ` prov : type ` AND SINGLE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → AND SINGLE ( n IN nodes ( p2 ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN p , p2 Figure 3 : Result of Query 1 ( Graph View ) The details of each node displayed graphically can be viewed in JSON Notation format . Figure 4 shows the details of the entity AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . Qunomon test data test / 1 / exec / 1 wasGeneratedBy test / 2 / exec / 1 wasGeneratedBy adversarial _ acc _ 1 . 0 acc _ 1 . 0 config config Model Stability Model Correctness used used used used used wasAssociatedWith used used used used used wasAssociatedWith retrained model retraining wasGeneratedBy Result of retraining wasGeneratedBy wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith cctrt : kim Retrained Model retraining wasGeneratedBy wasDerivedFrom plan : retraining wasGeneratedBy used used used wasAssociatedWith wasAssociatedWith model wasAssociatedWith result : exec1 result : exec2 quality / 1 Aggregation wasGeneratedBy wasDerivedFrom wasDerivedFrom Opinion : Adversarial Test Opinion : Accuracy Test Quality Report report _ gen / 1 wasGeneratedBy used used used used used used used Criteria : Adversarial Test test / 1 / criteria _ est / 1 wasGeneratedBy Criteria : Accuracy Test test / 2 / criteria _ est / 1 wasGeneratedBy wasAssociatedWith wasAssociatedWith wasAssociatedWith cctrt : nakagawa Measurement / Analysis Experiment Manufacture cctrt : narita Figure 2 : An Example of Provenance Record for Entire AI / ML Quality Management Lifecycle Query 1 : Cypher Query for Scenario 4 . MATCH ( en ) < - - ( ac ) < - [ * ] - ( ql ) WHERE " qprov : Measurement " in ac . ` prov : type ` AND ql . ` prov2neo : identifier ` = " mlc / 1 / quality / 1 " AND en . ` prov : type ` < > " qprov : QCTarget " RETURN distinct en We have used Neo4j 10 as the graph DB to store provenance data during evaluation . Therefore , Cypher was employed as the query language to evaluate the feasibility of each scenario . We have also used prov2neo 11 library to convert and upload provenance infor - mation generated through PROV package into neo4j database . 4 . 3 Result For all scenarios , we were able to confirm that it is possible to write queries that satisfy the requirements . For the sake of space , we show three randomly selected queries as Query 1 , 2 , and 3 . In all cases , it was confirmed that the requirements of the scenar - ios could be investigated with as little as few lines of description . Similar results were obtained for scenarios not shown here . The following describes in detail the results of searching each query against the provenance of Figure 2 . 4 . 3 . 1 Result of Query 1 ( scenario 4 ) . Figure 3 shows the results of executing Query 1 . The figure shows the results as Neo4j’s graph view , with blue representing entities and orange representing activ - ities . For readability , the figure includes not only the ’ en ’ ( entities ) in the query but also the ’ ac ’ ( quality measurement activity ) . Scenario 4 is a scenario in which we consider that the quality doubt is due to the influence of entities other than the quality control target , and we investigate all entities that affected quality . It can be seen that the quality of the trained model is guided by two different measurements , each of which is influenced by the measuring instrument , the setting of the instrument , and the quality perspective , respectively . 10 https : / / neo4j . com / 11 https : / / github . com / DLR - SC / prov2neo Query 2 : Cypher Query for Scenario 9 . MATCH p = ( ex ) < - [ * ] - ( mp ) WHERE mp . ` prov2neo : identifier ` = " qprovrepo : blob / 9b98c7 / example / manu . ipynb " ↩ → AND " qprov : Experiment " in ex . ` prov : type ` AND NONE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN distinct ex Query 3 : Cypher Query for Scenario 11 . MATCH ( qt1 ) < - [ : wasDerivedFrom ] - ( qt2 ) , p = ( ap ) < - - ( qt2 ) , p2 = ( qt1 ) < - [ * ] - ( ap ) WHERE " qprov : QCTarget " in qt1 . ` prov : type ` AND qt2 . ` prov2neo : identifier ` = " mlc / 1 / inv / 4 " AND " qprov : Apply " in ap . ` prov : type ` AND SINGLE ( n IN nodes ( p ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → AND SINGLE ( n IN nodes ( p2 ) WHERE n . ` prov : type ` = " qprov : QCTarget " ) ↩ → RETURN p , p2 Figure 3 : Result of Query 1 ( Graph View ) The details of each node displayed graphically can be viewed in JSON Notation format . Figure 4 shows the details of the entity Figure 3 : Result of Query 1 ( Graph View ) The details of each node displayed graphically can be viewed in JSON Notation format . Figure 4 shows the details of the entity that affected one of the two measurements in the result of Query1 How Provenance helps Quality Assurance Activities in AI / ML Systems AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Figure 4 : A JSON Notation example for each nodes ( nodes are sampled from result of query 1 ) in JSON Notation format . Element with name prov : type in JSON represents the class of each node on AIQPROV , where the mea - surer , the measurer configuration ( MeasurerConfiguration ) , and the quality viewpoint of the measurement ( QualityViewpoint ) can each be read as elements other than the QC Target . 4 . 3 . 2 Result of Query 2 ( scenario 9 ) . Figure 5 shows the results of running Query 2 . Scenario 9 asks through which experiments the processing ( production level ) for a given QC Target was created . Since only one experiment was conducted for the processing on this provenance record , the result was a single Entity node . In the prototype implementation of AIQPROV , when an experi - ment is run on a notebook , a version of the notebook containing the results of the run is automatically committed on Git , and the commit ID is used to tie the IRI to the entity . The example in Fig - ure 5 assumes that there is a source repository on GitHub that stores the results of the experiment , and that the Experiment De - sign and Experimental Results are stored in the path as shown in the prov2neo : identifier element . 4 . 3 . 3 Result of Query 3 ( scenario 11 ) . Figure 6 shows the results of running query 3 . Scenario 11 is the case where we want to view Figure 5 : Result of Query 2 ( JSON Notation ) Figure 6 : Result of Query 3 ( Graph View , with area annota - tions by the author ) the flow of the process of revising a certain model , and the result is not an element but a path from a specific point to a specific point . The provenance record for the evaluation records the flow of retraining and revising the original model , and thus the result of the execution is an extract of a portion of Figure 2 . The annotations in the figure , such as Evaluation , Analysis , etc . , were added by the authors in a postscript to indicate which part of Figure 2 each element corresponds to . The results show that the process from the top - left entity ( trained model ) as a starting point to the top - right entity ( re - trained model ) can record the flow of evaluation , analysis , experimentation , and processing . Since this figure shows a case of a search on paths , elements that do not appears directly in between the trained and retrained models ( e . g . , Measurers ) are omitted . If this scenario were to be implemented in reality , it is likely that once such a path is viewed , a more in - depth study of the group of elements that influenced each element on the path would be required . AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Nakagawa , et . al . 4 . 4 Discussion The provenance data used in this evaluation is limited and small - scale , based on a hypothetical lifecycle . In practical use , there are likely to be various exceptions and limitations . While the general - ization of the practical survey and model is an issue for the future , we will discuss the exceptions and limitations in practice that can be assumed at this time in this section . Typical limitations in a practical environment include the possi - bility of provenance fragmentation when the platform framework does not support provenance recording and when quality reports are consistently generated manually . This can be avoided by pro - viding an interface that allows manual registration of activities , but depending on how IRIs are specified , there may be problems such as recording the wrong version if it is specified incorrectly . Therefore , in the future , it is desirable to have a system that can verify the validity of activities and entities at the time of provenance recording to some extent . It is also possible that some of the various tasks listed in our target life cycle may not be performed . For example , a case in which independent experiments are not conducted in a small project , and the ML engineer who sees the evaluation results directly rewrites the pipeline code would correspond to this case . In this case , the AIQPROV model does not set any restrictions on the connections between elements , so it is recorded in accordance with the actual situation . If there is a discrepancy between the surveyer’s assumed process and the actual process , the results will not be output correctly at the time of the survey . This can lead to problems such as not being able to determine whether there is a problem with the query , the process , or the provenance record . For practical use , as in Query 3 , it is possible to investigate by obtaining the provenance from a certain point as a path . However , if the provenance path is too complex to overview , it may be nec - essary to detect anomalies in the provenance record or to support automatic summarization of the provenance record . In addition , When conducting a survey on a huge provenance record , there is also a concern that results may contain elements that is linked with editions other than the one being surveyed this time . For this reason , the query written for evaluation excludes patterns in which earlier editions appear in the middle of the path by adding restrictions to the number of times the QC Target appears on the path . Although this constraint cannot exclude cases where the evalua - tion results are affected by older editions without QC Targets , such provenance is difficult to remove mechanically , since it is believed that such provenance may point out problems in the process . One realistic process not covered in this evaluation is nested evaluation , where evaluation takes place during an experiment . This problem can be solved by recording the ( temporary during the experiment ) QC Target as the result of the experiment at the time of recording . In practical use , however , it is easy to imagine that the size of provenance records and query execution time would increase if experiments were conducted under a large number of conditions using mechanical methods such as hyperparameter search . How to handle such a large number of provenance records in a form suitable for investigation is an issue to be addressed in the future . 5 CONCLUSION In this paper , we proposed AIQPROV , a provenance recording method for the quality control process of AI systems , including human decisions and documents . We have also evaluated the useful - ness of AIQPROV using a practical scenario requiring a comestible history and a prototype implementation . In this paper , we did not place many constraints on the model and kept the evaluation scenario - based . Therefore , an attempt was made to ensure a certain level of validity by conducting a literature review from both academic and industrial perspectives , and by having scenarios described by those with R & D experience in industry . As a future work , the generality and validity of the model and the extraction of challenges through practical application are needed . ACKNOWLEDGMENTS This paper is based on results obtained from a project , JPNP20006 , commissioned by the New Energy and Industrial Technology De - velopment Organization ( NEDO ) . REFERENCES [ 1 ] Saleema Amershi , Andrew Begel , Christian Bird , Robert DeLine , Harald Gall , Ece Kamar , Nachiappan Nagappan , Besmira Nushi , and Thomas Zimmermann . 2019 . Software engineering for machine learning : A case study . In 2019 IEEE / ACM 41st International Conference on Software Engineering : Software Engineering in Practice ( ICSE - SEIP ) . IEEE , 291 – 300 . [ 2 ] Ekaba Bisong . 2019 . Kubeflow and Kubeflow Pipelines . Apress , Berkeley , CA , 671 – 685 . https : / / doi . org / 10 . 1007 / 978 - 1 - 4842 - 4470 - 8 _ 46 [ 3 ] Souti Chattopadhyay , Ishita Prasad , Austin Z Henley , Anita Sarma , and Titus Barik . 2020 . What’s wrong with computational notebooks ? Pain points , needs , and design opportunities . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 4 ] Committee for machine learning quality management . 2020 . Machine Learning Quality Management Guideline v1 . 0 . 1 . Technical Report . National Institute of Advanced Industrial Science and Technology ( AIST ) . https : / / https : / / www . cpsec . aist . go . jp / achievements / aiqm / AIQM - Guideline - 1 . 0 . 1 - en . pdf [ 5 ] Sato Danilo and Windheuser Christoph Wider Arif . 2019 . Continuous Delivery for Machine Learning . Retrieved 2022 - 06 - 27 from https : / / martinfowler . com / articles / cd4ml . html [ 6 ] Michael Felderer and Rudolf Ramler . 2021 . Quality Assurance for AI - Based Systems : Overview and Challenges ( Introduction to Interactive Session ) . In Inter - national Conference on Software Quality . Springer , 33 – 42 . [ 7 ] B . P . HarenslakandJ . deRuiter . 2021 . DataPipelineswithApacheAirflow . Manning . https : / / books . google . co . jp / books ? id = 8EwnEAAAQBAJ [ 8 ] Ben Hutchinson , Negar Rostamzadeh , Christina Greer , Katherine Heller , and Vinodkumar Prabhakaran . 2022 . Evaluation Gaps in Machine Learning Practice . In 2022 ACM Conference on Fairness , Accountability , and Transparency ( Seoul , Republic of Korea ) ( FAccT ’22 ) . Association for Computing Machinery , New York , NY , USA , 1859 – 1876 . https : / / doi . org / 10 . 1145 / 3531146 . 3533233 [ 9 ] ISO / IEC 25010 . 2011 . ISO / IEC 25010 : 2011 , Systems and software engineering ? Systems and software Quality Requirements and Evaluation ( SQuaRE ) ? System and software quality models . Standard . ISO . [ 10 ] DominikKerzel , SheebaSamuel , andBirgittaKönig - Ries . 2021 . TowardsTracking Provenance from Machine Learning Notebooks . . In KDIR . 274 – 281 . [ 11 ] Hiroshi Kuwajima and Fuyuki Ishikawa . 2019 . Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems . In IEEE International Symposium on Software Reliability Engineering Workshops , ISSRE Workshops 2019 , Berlin , Germany , October27 - 30 , 2019 , KatinkaWolter , InaSchieferdecker , BarbaraGallina , Michel Cukier , Roberto Natella , Naghmeh Ramezani Ivaki , and Nuno Laranjeiro ( Eds . ) . IEEE , 13 – 18 . https : / / doi . org / 10 . 1109 / ISSREW . 2019 . 00035 [ 12 ] Timothy Lebo , Satya Sahoo , Deborah McGuinness , Khalid Belhajjame , James Cheney , David Corsar , Daniel Garijo , Stian Soiland - Reyes , Stephan Zednik , and Jun Zhao . 2013 . Prov - o : The prov ontology . W3C Recommendation . World Wide Web Consortium ( 2013 ) . https : / / www . w3 . org / TR / prov - o / [ 13 ] Dusica Marijan , Arnaud Gotlieb , and Mohit Kumar Ahuja . 2019 . Challenges of Testing Machine Learning Based Systems . In 2019 IEEE International Conference On Artificial Intelligence Testing ( AITest ) . 101 – 102 . https : / / doi . org / 10 . 1109 / AITest . 2019 . 00010 [ 14 ] Akshay Naresh Modi , Chiu Yuen Koo , Chuan Yu Foo , Clemens Mewald , Denis M . Baylor , Eric Breck , Heng - Tze Cheng , Jarek Wilkiewicz , Levent Koc , Lukasz Lew , MartinA . Zinkevich , MartinWicke , MustafaIspir , NeoklisPolyzotis , NoahFiedel , How Provenance helps Quality Assurance Activities in AI / ML Systems AIMLSystems 2022 , October 12 – 15 , 2022 , Bangalore , India Salem Elie Haykal , Steven Whang , Sudip Roy , Sukriti Ramesh , Vihan Jain , Xin Zhang , and Zakaria Haque . 2017 . TFX : A TensorFlow - Based Production - Scale Machine Learning Platform . In KDD 2017 . [ 15 ] Kenichiro Narita , Michitaka Akita , Kyoung - Sook Kim , Yuta Iwase , Yuichi Watanaka , Takao Nakagawa , and Qiang Zhong . 2021 . Qunomon : A FAIR testbed of quality evaluation for machine learning models . In 2021 28th Asia - Pacific Software Engineering Conference Workshops ( APSEC Workshops ) . 21 – 24 . https : / / doi . org / 10 . 1109 / APSECW53869 . 2021 . 00015 [ 16 ] Ipek Ozkaya . 2020 . What is really different in engineering AI - enabled systems ? IEEE Software 37 , 4 ( 2020 ) , 3 – 6 . [ 17 ] Lukas Rupprecht , James C Davis , Constantine Arnold , Yaniv Gur , and Deepavali Bhagwat . 2020 . Improving reproducibility of data science pipelines through transparent provenance capture . Proceedings of the VLDB Endowment 13 , 12 ( 2020 ) , 3354 – 3368 . [ 18 ] Sheeba Samuel , Frank Löffler , and Birgitta König - Ries . 2020 . Machine learning pipelines : provenance , reproducibility and FAIR data principles . In Provenance and Annotation of Data and Processes . Springer , 226 – 230 . [ 19 ] David Sculley , Gary Holt , Daniel Golovin , Eugene Davydov , Todd Phillips , Diet - mar Ebner , Vinay Chaudhary , Michael Young , Jean - Francois Crespo , and Dan Dennison . 2015 . Hidden technical debt in machine learning systems . Advances in neural information processing systems 28 ( 2015 ) . [ 20 ] Micah J Smith , Carles Sala , James Max Kanter , and Kalyan Veeramachaneni . 2020 . The machine learning bazaar : Harnessing the ml ecosystem for effective system development . In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 785 – 800 . [ 21 ] Renan Souza , Leonardo Azevedo , Vítor Lourenço , Elton Soares , Raphael Thiago , Rafael Brandão , Daniel Civitarese , Emilio Vital Brazil , Marcio Moreno , Patrick Valduriez , Marta Mattoso , Renato Cerqueira , and Marco Netto . 2019 . Prove - nance Data in the Machine Learning Lifecycle in Computational Science and Engineering . In WORKS 2019 - Workflows in Support of Large - Scale Science co - located with SC 2019 - ACM / IEEE International Conference for High Performance Computing , Networking , Storage , and Analysis . ACM , Denver , United States , 10 . https : / / hal - lirmm . ccsd . cnrs . fr / lirmm - 02335500 [ 22 ] Medha Umarji and Carolyn Seaman . 2009 . Gauging Acceptance of Software Metrics : Comparing Perspectives of Managers and Developers . In Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement ( ESEM ’09 ) . IEEE Computer Society , USA , 236 – 247 . https : / / doi . org / 10 . 1109 / ESEM . 2009 . 5315999 [ 23 ] JieMZhang , MarkHarman , LeiMa , andYangLiu . 2020 . Machinelearningtesting : Survey , landscapes and horizons . IEEE Transactions on Software Engineering ( 2020 ) .