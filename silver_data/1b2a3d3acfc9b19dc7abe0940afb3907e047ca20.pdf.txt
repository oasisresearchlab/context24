1 Title : 1 A mixture of generative models strategy helps humans generalize across tasks 2 Authors : 3 Santiago Herce Castañón 1 , * , Pedro Cardoso - Leite 2 , Irene Altarelli 3 , C . Shawn Green 4 , Paul 4 Schrater 5 , Daphne Bavelier 1 5 Affiliations : 6 1 Department of Psychology and Educational Sciences , University of Geneva , Switzerland . 7 2 Department of Behavioural and Cognitive Science , University of Luxembourg , Luxembourg . 8 3 Université de Paris , LaPsyDÉ , CNRS , France . 9 4 Department of Psychology , University of Wisconsin - Madison , USA . 10 5 Department of Computer Science , University of Minnesota , USA . 11 * Correspondence : s . hercecastanon @ gmail . com 12 2 Abstract 13 What role do generative models play in generalization of learning in humans ? Our novel multi - 14 task prediction paradigm—where participants complete four sequence learning tasks , each 15 being a different instance of a common generative family—allows the separate study of within - 16 task learning ( i . e . , finding the solution to each of the tasks ) , and across - task learning ( i . e . , 17 learning a task differently because of past experiences ) . The very first responses participants 18 make in each task are not yet affected by within - task learning and thus reflect their priors . Our 19 results show that these priors change across successive tasks , increasingly resembling the 20 underlying generative family . We conceptualize multi - task learning as arising from a mixture - 21 of - generative - models learning strategy , whereby participants simultaneously entertain multiple 22 candidate models which compete against each other to explain the experienced sequences . This 23 framework predicts specific error patterns , as well as a gating mechanism for learning , both of 24 which are observed in the data . 25 3 Introduction 26 Understanding the mechanisms underlying human learning remain amongst the most 27 important , intriguing and fascinating open challenges for science . Insights about human 28 learning have the potential for large - scale societal impact , for example , by driving progress in 29 artificial learning systems . Humans are exceptional learners . They can learn effectively from 30 very limited experience ( e . g . , Carey & Bartlett , 1978 ; Feldman , 1997 ) . They can generalize 31 their learning beyond the specifics of their experience ( Jern & Kemp , 2013 ; Lake et al . , 2015 ; 32 Xu & Tenenbaum , 2007 ) , such as correctly categorizing previously unseen exemplars of a 33 category ( T . B . Ward , 1994 ) . Interestingly , both the speed with which humans can learn ( i . e . , 34 learning from few examples ) and the ability to generalize ( i . e . , transferring knowledge across 35 contexts ) may indicate that humans approach learning problems in a structured way ( Behrens 36 et al . , 2018 ; Lake et al . , 2017 ) . In essence this would involve making use of pre - existing 37 knowledge about the world ( i . e . , “priors” ) , and then consistently incorporating new pieces of 38 information into a generative model ( i . e . , a structured and generalizable model of how the 39 world has generated the given experiences ) . 40 In a wide variety of fields , including perception and decision - making , an overwhelming 41 amount of evidence shows that humans and other animals rely on internal generative models 42 to make optimal inferences about currently presented information ( Behrens et al . , 2007 ; 43 Kersten et al . , 2004 ; Kilner et al . , 2007 ; Körding et al . , 2004 ; Wolpert et al . , 1995 ) . A major 44 benefit of using a generative model is that such models allow for inferences and predictions to 45 be made about future data . Accordingly , generative models have also been shown to play a 46 critical role in guiding sequential behaviour ( Cleeremans & McClelland , 1991 ; Elman , 1990 ) , 47 where forecasting of future scenarios , planning and preparation of future actions , and the use 48 of counterfactual reasoning are all crucial for success ( Boorman et al . , 2009 ; Daw et al . , 2005 ; 49 Koechlin et al . , 1999 ; G . Ward & Allport , 1997 ) . For instance , after learning to navigate 50 through a maze , internal models of the maze allow individuals to quickly find a short path to a 51 new goal—even when that path had not been previously used ( Tolman & Honzik , 1930 ) . 52 Together , these results highlight the important role that generative models play in guiding 53 4 behaviour . Yet , we know very little about the role generative models play in guiding the 54 learning process itself . We owe this state of affairs , at least partially , to the difficulty of 55 studying the impact of generative models on learning processes . In particular , it is intrinsically 56 the case that those situations where generative models are most useful for guiding learning are 57 also the situations where it is most difficult to study their influence . Below , we provide some 58 of the reasons for this difficulty as well as our approach to circumventing those challenges . 59 Why , then , is it difficult to study the role of generative models in learning ? Quantifying 60 and understanding the role that generative models play during learning has been difficult for 61 several reasons . First , the utility of a generative model in guiding learning is often highest in 62 the earliest phases of learning ( i . e . , when the learner is still “data poor” and must rely on the 63 generative model rather than direct experience ) . Yet , because this phase is often short , it is 64 difficult for researchers to track the underlying beliefs that are guiding participants’ learning 65 during this stage ( e . g . , practically speaking , data cannot be aggregated to effectively reduce 66 measurement noise ) . Second , it is always the case that other cognitive processes may 67 simultaneously compete to control observable behaviours or choices . For instance , learning 68 processes that do not rely on generative models ( e . g . , trial - and - error or model - free learning ) 69 may be equally or more prominent when behaviour reaches a stable state ( i . e . , at asymptotic 70 performance ) , making it difficult to attribute choices to one process over another . Finally , most 71 studies rely on laboratory paradigms where the environment is governed by a single generative 72 process . This is typically the case in temporal sequence learning paradigms , where participants 73 are exposed to sequences of stimuli , often extensively long ones , that are governed by a single 74 generative process . While tasks with a single generative process have been very fruitful in 75 studying how internal generative models guide behaviour ( Cleeremans & McClelland , 1991 ; 76 Knill & Richards , 1996 ; Pouget et al . , 2013 ; Tenenbaum et al . , 2006 ) , it makes it impossible , 77 however , to track ( i ) how experience in that environment shapes learning in other scenarios , or 78 ( ii ) what behaviour looks like during learning in an environment that is possibly controlled by 79 competing generative models . 80 Here we present a novel multi - task paradigm designed to examine the role that 81 generative models play in learning . In this study , participants went through a series of four 82 5 separate temporal sequence prediction tasks . The sequence of stimuli in each of the tasks was 83 governed by a specific generative process selected from amongst a broader common generative 84 family . Across a series of 10 experiments involving over 800 human participants , we asked 85 two fundamental questions . First , when learning a task , are participants using ( or inferring ) a 86 generative family ? To answer this question , we quantified participants’ prior knowledge as 87 they started each new task . Specifically , the first four trials of each task were purposefully 88 designed to reveal participants’ priors uncontaminated by direct experience with the given task . 89 Thus , by analysing these very early trials we are able to show that participants rapidly and 90 consistently ( i . e . , starting from exposure to a single instance of the generative family but 91 continuing through the exposure to following instances ) update their priors to better match the 92 true generative family when approaching each subsequent task . The second fundamental 93 question we ask is , how do people learn within tasks that are situated in a multi - task generative 94 model environment ? We present evidence that participants use a mixture - of - generative - models 95 ( MGM ) learning strategy . In particular , we see that the MGM learning hypothesis ( i ) explains 96 specific structure within the errors participants made , ( ii ) naturally accounts for the large 97 individual variability in behaviour in terms of the prior with which people start the task ( as 98 opposed to relying on ad - hoc assumptions to explain suboptimal behaviour ) , as well as ( iii ) 99 predicts the existence of a gating mechanism for learning which explains why people with 100 inadequate priors fail to learn the task . 101 6 Results 102 Participants ( n = 854 , across 10 experiments , see Methods ) were placed in a multi - task 103 learning environment in which they were asked to perform a series of sequential prediction 104 learning tasks interleaved with production tasks . A common generative family defined the 105 possible generative models that could control the stimulus sequence in any one prediction task 106 ( Fig . 1A ) . All possible generative models within the generative family follow a first order 107 Markov process from which sequences of stimulus locations are sampled . The generative 108 family boils down to two pieces of information required to describe any single instance of a 109 generative model from the generative family ( see Methods for a full description ) : ( i ) all 110 generative models follow one of four dominant transition patterns , i . e . , the pattern describing 111 which location is the most likely next location given any current location ( see Fig . 1A top 112 panel ) , and ( ii ) one dominant transition probability value , 𝝰 , shared across all locations ( i . e . , 113 the probability with which the stimulus will appear in the most likely next location ) . One 114 particular instantiation of this generative family then determines in which of four possible 115 screen locations the stimulus appears on each trial in a given task . 116 7 Fig 1 . The multi - task learning paradigm . ( A ) Across - task learning : In this multi - task 117 learning environment , participants went through a series of sequential prediction tasks . The 118 stimulus’ probabilistic behaviour in each task is determined by a specific generative model— 119 each generative model is a different instance from a common generative family ( top row ; also 120 see methods ) defined by two characteristics : the identity of the dominant transition pattern ( top 121 row , left ; note coloured arrows indicate the most likely stimulus transition from each of the 122 four locations on the screen ) , and the dominant transition probability value , 𝝰 ( i . e . , the 123 probability of the stimulus moving to the most likely next location ; top row , right ) . These two 124 pieces of information are then combined ( second row from top ) to create a particular generative 125 model from which the to - be - predicted sequences of locations are drawn ( e . g . , for prediction 126 task 4 , the generative model uses the purple dominant transition pattern along with a small 127 value of 𝝰 ) . For each generative model , participants first completed a prediction task ( where 128 they observed the stimulus appear in one location and then predicted the next ) and then a 129 production task ( where they were asked to produce a sequence in the absence of any 130 observations ; third row from top ) . Because participants underwent multiple such tasks , learning 131 could occur both within a task as well as across tasks ( bottom row ) . ( B ) Within - task learning : 132 For each prediction task , participants had to guess the next location of “the wolf” . In this , 133 participants were simply told to do their best to “catch the wolf” ( i . e . , they were given no 134 information about the generative models ) . Participants had a three - second window to make 135 each prediction ( clicking one of the four locations ) before the wolf appeared in the next location 136 ( for one second ) . Once the wolf disappeared again , a new prediction could occur ( top panel ) . 137 Critically , the first four choices participants make in each task can only be guided by the 138 background knowledge participants bring to the task , as the wolf does not repeat locations 139 within the those first four trials . Experiencing the wolf transitions in any one task should help 140 participants learn to solve the current task as well as update the background knowledge with 141 which they will approach future tasks ( though applying the exact solution of the current task 142 directly to future tasks can be disastrous for performance ; bottom panel ) . 143 8 Our multi - task learning paradigm , thus , has an advantage over a single - task learning 144 paradigm in that it allows us to assess both within - task learning ( i . e . , where participants 145 increase the accuracy of their predictions within a given task ) and across - task learning ( i . e . , 146 where participants generalize what they learn in one task beyond the context of that one task 147 to influence their behaviour on future tasks ) . For each prediction task , the cartoon face of a 148 wolf appeared successively in one of four locations on a screen following a sequence sampled 149 from a specific generative model ( i . e . , one location per trial ) . Participants had to predict the 150 next location of the wolf ( Fig . 1B ) . For the first few trials of each prediction task , participants 151 had to rely on the background knowledge they brought to the task ( i . e . , the wolf did not reappear 152 in a previously visited location for the first four predictions ; thus , participants could not use 153 previous observations within the task to estimate the most likely next location unless they had 154 intuitive biases about the generative family ) . Then , as they spent more time in each task , 155 participants had the opportunity to learn the task - specific transition probabilities ( i . e . , learn the 156 task - specific generative model ) . Crucially , although the specific generative models that 157 determined the sequences of stimulus locations for each task were drawn from the same 158 generative family , it was nonetheless the case that the exact transition probabilities were very 159 different across tasks ( e . g . , in the first task , the wolf might move mostly to location 4 when in 160 location 1 , while in the second task , the wolf might instead move mostly to location 2 when in 161 location 1 ) . As such , if participants simply reused the previously learned transition probabilities 162 in a new task , they would perform very poorly . However , their experience in a given task could 163 allow participants to change their understanding of how tasks in this environment work—an 164 understanding that might help them learn future tasks . 165 Performance reveals within - task learning while early behaviour reveals across - task learning 166 We first evaluated participants’ performance throughout each of the four tasks by 167 computing the proportion of choices that were made in agreement with the true generative 168 model of that task . Under the true generative model of any one task ( i . e . , the generative model 169 that produced the actual stimulus sequence of the task ) , for each trial there is a unique most 170 likely next stimulus location given the current stimulus location . We note that this metric is a 171 9 better indicator of performance than simple “accuracy” ( i . e . , tallying the times the wolf was 172 caught ) , due to the stochasticity of the generative model ( which means that even if a choice 173 was in agreement with the true generative model , the wolf may not always have been caught 174 and vice versa ) . 175 Examining participants’ performance across the trials of each task reveals , first and 176 foremost , clear evidence of learning . Within each task , participants’ choices across trials , on 177 average , became gradually closer to the performance of an ideal artificial learner agent 178 ( Fig . 2A ) . Second , the extent to which this is true differs from task to task . In tasks with more 179 stochastic stimulus sequences ( i . e . , tasks with lower 𝝰 values , such as task 4 ) , learning was 180 slower , and the learning curves saturated at lower values , as compared to tasks with more 181 deterministic sequences . This difference across tasks appears to reflect task difficulty ( i . e . , 182 performance is “worse” in tasks with more stochasticity ) . We note that this relationship 183 between stochasticity and learning ( i . e . , higher stochasticity requires more trials to learn the 184 optimal behaviour ) would be true even for an ideal artificial learning agent . However , because 185 the tasks differ in difficulty , it suggests that the metric is not ideal for determining whether 186 participants carried over knowledge from one task to the next . 187 As noted above , examining across - task learning is challenging . For instance , revealing 188 across - task learning requires differentiating behaviour that is driven by experiences that 189 occurred prior to the current task from behaviour driven by experiences that occurred during 190 the current task . This distinction motivated our approach to look at the very earliest behaviour , 191 specifically from the first four predictions in each of the tasks , in order to characterize across - 192 task learning . Our focus on the first four trials is not arbitrary but is well founded on specific 193 aspects of our experimental design . 194 Namely , as noted above , the first four choices in each task must be made before a wolf 195 reappears in a previously experienced location for that task and as such , there is no evidence 196 from the current task itself , regarding the probability with which it will transition to any other 197 location . Because of this , the first four choices are a unique window for observing behaviour 198 which is purely driven by the prior background knowledge that the participants brought to the 199 current task and not by evidence acquired from observing transitions within the task . The 200 10 obvious downside to examining such a small number of choices is that estimates related to any 201 one participant will be noisy , and thus , in order to still get reliable estimates of behaviour , we 202 collected and aggregated data from hundreds of participants . 203 Focusing on the first four predictions of each task allows us to ask two interrelated 204 questions . First , what type of background knowledge , or prior , do participants have when they 205 arrive in the experiment ( i . e . , in the very first task ) ? They could arrive with a naïve prior about 206 the sequence of stimuli , or they could come with some structured prior background knowledge . 207 Second , are participant’s experiences through the tasks changing how they approach 208 subsequent tasks ? To answer these questions , we first examined the distribution of choices over 209 locations ( aggregated over participants ) for each of the first four predictions and each of the 210 tasks , and we then quantitatively assessed the observed patterns . 211 To aid in the interpretation of the distribution of early behaviour from participants , we 212 contrasted human behaviour against two reference artificial learning agents : ( i ) a naïve artificial 213 learner and ( ii ) an ideal artificial learner , which are endowed with contrasting extremes of 214 background knowledge in the task ( see Methods ) . The naïve artificial learner has no knowledge 215 of the generative family and instead uses count statistics to infer the transition probabilities . 216 Thus , the first four predictions for the naïve artificial learner , which are made before the wolf 217 revisits a location , are all random ( i . e . , because the agent starts with a flat prior with equal 218 probability of choosing all locations , such an agent can only begin to make informed / non - 219 random choices after a location has been visited at least once ) . At the other extreme of the 220 spectrum is the ideal artificial learner . This artificial learner has perfect knowledge of the 221 generative family and uses the observations to make inferences about which specific generative 222 model controls the current stimulus sequence . While the first prediction must necessarily be a 223 guess for this model , predictions become increasingly informed by the stimulus sequence from 224 the second prediction onwards . 225 As for the first question : did participants come with a structured prior ? Perhaps 226 surprisingly , on the very first task , participants showed choices that were not only non - random , 227 but were , if anything , somewhat aligned with the distribution of choices of the ideal artificial 228 learner ( task 1 in Fig . 2B - C ) . This suggests that participants had some rough intuitions for what 229 11 type of stimulus sequences were possible in the paradigm , or that the generative family we 230 chose happened to broadly align with some default prior that the participants brought to the 231 experiment . Regarding the second question : was there evidence of across - task learning ? 232 Inspecting the distribution of the first four predictions of the subsequent tasks revealed that 233 participants’ background knowledge became increasingly more similar to the one from the 234 ideal artificial learner ( tasks 2 , 3 and 4 in Fig . 2B - C ) . This suggests that during their experience 235 of a task , participants were able to change the background knowledge with which they would 236 approach future tasks . In short , participants’ improvement of their background knowledge 237 revealed across - task learning . 238 To quantitatively analyse the observed patterns , we derived a measure of early 239 behaviour . As noted above , for each of the tasks , we started with the first four choices from 240 participants and the expected distribution of choices from the two extreme agents in each task 241 ( see Methods ) . We then created an axis of background knowledge by using the expectation of 242 choice probabilities from the two learning agents ( the naïve and the ideal artificial learners ) . 243 Projecting participants’ choices onto this axis has a natural interpretation : the highest values 244 on the axis would be attained by choices that have the highest likelihood under the distribution 245 of the ideal artificial learner and lowest likelihood under the distribution of the naïve artificial 246 learner . However , while we have access to the choice probabilities of the artificial learning 247 agents , we only have access to the concrete predictions from each participant . To allow for 248 direct comparison of choices from humans and the artificial learning agents , we constructed a 249 distribution of behaviour from choices of the artificial learning agents by sampling the first 250 four predictions from the distribution of probability over choices for each of the two agents . 251 Thus , for each human participant , we sampled a set of four predictions under each of the 252 artificial learning agents’ distributions , we then projected the background knowledge from 253 humans’ and agents’ choices onto the same axis , which allowed us to visualize comparable 254 distributions of behaviour from participants and from the artificial learning agents ( Fig . 2C , 255 left ) . 256 12 Fig 2 . Across - task learning revealed by early behaviour analysis and not by performance . ( A ) Performance 257 reflects only within - task learning : Participants ( coloured curves , black dots and error bars ) rapidly improved their 258 performance in each task ( increasing the proportion of predictions in agreement with the true generative model of 259 the task ) . Even an ideal artificial learner ( black upper curve ) who knows the generative family , would require a 260 few trials to reach the highest performance level . Task 1 ( blue ) had the highest dominant transition probability 261 value , 𝝰 , ( i . e . , it was the least stochastic task ) and task 4 ( purple ) had the lowest 𝝰 ( i . e . , it was the most stochastic 262 task ) . As expected , both the speed and asymptotic level of learning were lower for tasks with higher stimulus 263 stochasticity , comparing performance across different tasks is thus not indicative of across - task learning . ( B ) Early 264 behaviour reveals across - task learning : the first four predictions of participants have a structured distribution for 265 each of the four tasks ( darker colours indicate higher probability ; the inner white circle indicates the position the 266 wolf actually appeared on that trial after the choice was made ) . As reference points , we computed the distribution 267 of predictions under two different artificial learning agents which represent two extremes on a knowledge scale : 268 a naïve artificial learner and an ideal artificial learner ( to get comparable distributions , one simulation was done 269 for each artificial learner and for each of our participants ) . The naïve artificial learner ( second row , pink ) has no 270 knowledge of the generative family and uses only count statistics to infer the transition probabilities . The ideal 271 artificial learner ( third row , blue ) has full knowledge of the generative family and uses it to infer the generative 272 model of the current task ( e . g . , far left panel , bottom row , the ideal agent , after observing the wolf in the bottom 273 left location on trial 1 , puts zero probability mass on the wolf appearing in the bottom left location on trial 2 , as it 274 knows that such a transition is not possible under the generative family ) . Already in task 1 , participants do not 275 behave like the naïve artificial learner but instead their predictions reveal they arrive with structured intuitions . 276 Furthermore , as participants complete successive tasks , the distribution of their first four predictions increasingly 277 resembles those of the ideal artificial learner , suggesting participants update their background knowledge from 278 one task to another to better match the generative family . ( C ) Background knowledge improves across tasks . To 279 establish that participants update their background knowledge from one task to another , we computed the 280 likelihood ratio of participants’ choices ( in panel B ) relative to the two artificial learners . Distributions represent 281 log - likelihood ratios for human participants and simulated agents rescaled between the extremes of the measure . 282 Participants ' distribution of background knowledge confirmed that already in task 1 , participants’ choices ( dark 283 grey ) deviated strongly from the distribution of the naïve artificial learners ( pink ) and loosely resembled that of 284 the ideal artificial learners ( blue ) . Critically , background knowledge strongly increased across successive tasks 285 both on average ( right side , mean and 95 % confidence intervals ) and within participants , demonstrating that they 286 increasingly matched their background knowledge to the generative family . This improvement occurred 287 independent of the sensory modality , for both visual ( blue ) and auditory ( orange ) versions of the experiment . The 288 apparent discrepancy between overall task performance ( panel A ) and early task behaviour ( panels B and C ) is 289 consistent with the mixture - of - generative - models learning framework introduced in the following section . 290 13 We first confirmed that participants start the experiment with priors that are not naïve 291 but roughly aligned with the ideal artificial learner . Indeed , at the start of the first task , the 292 distribution from humans ( grey ) was far from the distribution of the naïve learner ( pink ) and 293 instead was shifted in the direction of the ideal artificial learner ( blue ) . We used the area under 294 the receiver operator characteristic curve ( AUC , i . e . , a non - parametric statistic ) , along with a 295 permutation test in order to directly compare the statistical difference between distributions . 296 The distribution of background knowledge from participants in the first task was far from that 297 of the naïve artificial learners ( AUC = . 77 , p < . 001 , see Table 2 ) , and was also significantly 298 lower than the distribution of the ideal artificial learners ( although it was shifted in the direction 299 of the ideal artificial learners ; AUC = . 04 , p < . 001 ) . We then confirmed that participants 300 improved their background knowledge through experience in the tasks , becoming increasingly 301 similar to the distribution of the ideal artificial learners ( Fig . 2C , right ) . For this , we compared 302 the background knowledge from participants across all contiguous pairs of tasks ( i . e . , task 1 vs 303 task 2 , task 2 vs task 3 , and task 3 vs task 4 ) . Crucially , the distribution of background 304 knowledge was significantly higher ( toward the ideal artificial learner ) for task 2 compared to 305 task 1 ( AUC = . 75 , p < . 001 ) and task 3 compared to task 2 ( AUC = . 56 , p < . 001 ) , but did not 306 differ statistically between task 3 and task 4 ( AUC = . 509 , p > . 4 ) . 307 Importantly , our results are robust across a diverse set of experimental manipulations . 308 For each and every one of the ten separate experiments that we ran , participants ( i ) arrived in 309 the experiment with a structured prior , and ( ii ) showed an increase in background knowledge 310 as they experienced the tasks of the generative family ( see Table 2 ) . This was the case not only 311 when the tasks were delivered in the visual modality , where simple spatial priors may have 312 existed ( e . g . , left to right , top to bottom visual scanning ) , but also when the tasks utilized an 313 auditory stimulus , where spatial priors are arguably less likely ( i . e . , in 3 of our experiments , 314 participants had to predict , instead of the location of the wolf , which of four syllables would 315 be heard next ) . Together , our results show a strong tendency for participants to approach new 316 learning problems in a structured way , making use of background knowledge acquired prior to 317 the current task , and refining their background knowledge for their future learning experiences . 318 14 Mixture - of - Generative - Models ( MGM ) learning explains within and across task behaviour 319 Yet , while the results above offer compelling evidence that human participants are 320 learning the structure of the generative family across tasks , this result may seem difficult to 321 reconcile with the overall choice performance shown in Fig . 2A . How can participants be 322 moving closer and closer to the optimal prior across tasks , yet be so far away from making 323 optimal choices within given tasks ( e . g . , task 4 where there is the highest stochasticity ) ? We 324 propose that the apparent discrepancy between ( i ) the highly structured early approach to a 325 task , and ( ii ) the decreased performance observed in tasks with higher stochasticity , can be 326 accounted for by assuming participants approach our tasks with a strong model - based 327 approach . Specifically , we propose that participants start each novel task using a mixture - of - 328 generative - models learning approach . In this framework , a pool of candidate generative models 329 ( which increasingly resembles the broad generative family as participants move through the 330 tasks ) is arbitrated against each other with regard to which best explains the stimulus sequence . 331 Below we describe in depth ( i ) details of this framework , ( ii ) how it accounts for the results , 332 and ( iii ) the novel predictions that it makes . 333 Multitask learning is a computationally hard problem . In our study , there is potentially 334 an infinite number of generative models that could have generated each of the task sequences . 335 A sensible strategy in this context is to use a mixture - of - generative - models ( MGM ) learning 336 strategy , which combines elements from mixtures of models for time series ( Fox et al . , 2009 ; 337 Fox & Jordan , 2013 ) as well as the intuitive biases of probabilistic programming ( Tenenbaum 338 et al . , 2006 ; Tenenbaum & Griffiths , 2003 ) . Under MGM learning , participants simultaneously 339 entertain a number of candidate generative models at the start of each task . Then , as the 340 stimulus sequence unfolds , they arbitrate the models against each other . Generative models that 341 make correct predictions gain credit ( i . e . , participants increase their confidence in that model 342 being the one controlling the stimulus ) and thus are more likely to guide a participant’s future 343 choices , while generative models that do not make correct predictions tend to lose credit and 344 fade into the background ( i . e . , no longer influence choices ) . MGM learning is ideally suited to 345 model learning in our study given the paucity of information participants were given to 346 complete the task ( see supplementary materials for a precise description of the prediction and 347 15 production task instructions ) . Indeed , at no point were the participants instructed that a single 348 generative model operated in each task . Here it is important to note that although in the case 349 of the experiment at hand a single generative model operated in each task , if participants had 350 assumed that a single model controlled the stimulus and this was not true ( e . g . , if a task had 351 been controlled by a mixture of several generative models ) it could lead to significant learning 352 failures . Specifically , participants’ internal model in such a scenario , would tend , after a few 353 switches in the generative models , to predict all transitions with equal likelihood . Conversely , 354 an MGM learning strategy would enable the correct inferences to be made as time unfolds . Our 355 use of an MGM learning framework is further motivated by the fact that the early task 356 behaviour observed above is naturally incorporated in such a framework , in addition to a host 357 of other in - task choice behaviours that we examine below . 358 Participants more likely guide their choices using the generative model for which they 359 have higher confidence . Specifically , under the MGM framework , participants’ confidence in 360 each of the models , that is , the probability with which they think each model is controlling the 361 stimulus , is constantly updated with new stimulus observations ( Fig . 3A ) . Participants need to 362 simultaneously ( i ) arbitrate amongst the generative models they entertain in mind , and ( ii ) learn 363 their parameters ( e . g . , estimate the dominant transition probability values for estimating the 364 likelihood of an observation under each of their candidate models ) . Amongst the candidate 365 generative models that participants entertain , one model predicts with a high probability the 366 most likely stimulus on every trial , in agreement with the true generative model ( albeit perhaps 367 with a different dominant transition probability value ) . Henceforth , we call this the “dominant 368 model” , which is characterized by having the dominant pattern that matches the dominant 369 pattern of the true generative model ( note that it is referred to the dominant model regardless 370 of whether participants are actually using it to guide their choices at any given time ) . The 371 dominant model thus predicts with the highest probability each of the dominant transitions , that 372 is , the most likely next locations given any location , but predicts with low probability each of 373 the non - dominant transitions . In other words , because of the stochasticity in the stimulus 374 sequences , as sampled from the true generative model , the stimulus sometimes violates the 375 dominant pattern ( i . e . , the stimulus does not always transition to the most likely next location ) . 376 16 Alternative “non - dominant models” predict the non - dominant transitions with a higher 377 probability than the dominant model , and participants may use those alternative models to 378 guide some of their predictions , which will then result in non - dominant choices . Thus , under 379 the framework , there is an important interplay between ( i ) evidence provided by the stimulus 380 sequence which can be either in favour or against the dominant model , and ( ii ) the credit that 381 participants assigned to the dominant model and which can be inferred by the type of choices 382 they make ( i . e . , choosing the most likely next location or not ) , and which will determine how 383 much participants may update each model’s parameters . 384 The MGM framework can explain both within - and across - task patterns of behaviour . 385 The across - task learning results , interpreted within the MGM framework , suggest that the pool 386 of candidate generative models that participants use is increasingly more similar to the pool of 387 generative models within the generative family . In line with this , only 1 . 3 % ( ± . 07 % SEM 388 across participants ) of choices in the fourth task fell outside the generative family ; the 389 percentage of choices was computed for each participant using all choices and we report the 390 mean and standard error of the mean across participants . Similarly , the drop in within - task 391 learning performance that we observed for tasks with higher stimulus stochasticity has a natural 392 interpretation under MGM learning . Specifically , more stochastic stimulus sequences are 393 compatible with a larger number of generative models , making the arbitration harder and 394 explaining why participants may use alternative models to guide a higher proportion of their 395 choices . Interestingly , typical explanations for the decreased performance for more stochastic 396 sequences require invoking various suboptimal mechanisms ( e . g . , probability matching ) , 397 which often implicitly assume that errors are randomly distributed across trials . Instead , MGM 398 learning makes specific predictions about the structure of errors . Namely , error rates are 399 expected to increase when ( i ) the most recent stimulus transition was non - dominant , as this 400 casts doubt on the dominant generative model , and ( ii ) when the previous choice was non - 401 dominant , because it reveals participants were not using the dominant model to guide their 402 predictions , i . e . , an alternative model had higher credit at the time . 403 To demonstrate that participants choices were well matched to this framework , we 404 analysed the proportion of dominant choices made in groups of trials segregated according to 405 17 the type of previous choice and latest stimulus type ( Fig . 3B ) . As predicted under MGM , a non - 406 dominant stimulus had the effect of reducing the proportion of dominant next choices by ≈ 20 % 407 ± 1 . 3 % SEM compared to a dominant stimulus ( from ≈ 73 % down to ≈ 53 % ) . This reduction 408 was observed regardless of the previous choice type ( after a dominant choice , ≈ 22 . 9 % ± 1 . 3 % 409 SEM ; from ≈ 77 % down to ≈ 54 % ; and after a non - dominant choice , ≈ 23 . 1 % ± 1 . 3 % SEM ; from 410 ≈ 63 % down to ≈ 40 % ) . Similarly , a previous non - dominant choice was followed by ≈ 10 % ± 411 1 . 3 % SEM fewer dominant choices compared to a previous dominant choice ( from ≈ 70 % down 412 to ≈ 59 % ) and this was true regardless of the stimulus type ( after a dominant stimulus , 13 . 7 % ± 413 1 . 2 % SEM ; from ≈ 77 % down to ≈ 63 % ) and after a non - dominant stimulus , ≈ 13 . 9 % ± 1 . 4 % 414 SEM ; from ≈ 54 % down to ≈ 40 % ) . This latter result , namely , the negative effect of a previous 415 non - dominant choice , can be interpreted under MGM as reflecting participants’ decreased 416 confidence in the dominant model , therefore increasing the probability of using an alternative 417 model for guiding the following choice . Together , these results are highly consistent with the 418 MGM view that participants’ confidence in their candidate generative models ( for the most 419 part within the generative family ) , fluctuates along with the evidence , as the stimulus sequence 420 unfolds . 421 Another prediction from MGM is that learning the parameters of each model should be 422 gated by confidence in the model . Confidence in the dominant model decreases when the 423 stimulus violates the expectations from the dominant model , which in turn often leads to a non - 424 dominant choice . As such , a low level of confidence in the dominant model can be inferred by 425 observing a non - dominant choice . However , the influence of confidence on the dominant 426 model is not limited to explaining the structure of errors . In fact , a crucial prediction of the 427 mixture - of - models framework is that feedback , that is the observation of a stimulus in the 428 sequence , needs to be attributed to the different models in proportion to how much each model 429 is deemed responsible for generating the observation . A direct prediction of this process is that 430 learning should be gated by confidence . Specifically , we predict that learning the 𝝰 parameter 431 ( i . e . , the dominant transition probability value ) of the dominant model should be facilitated 432 when confidence on the model is high compared to when it is low . In other words , participants 433 need to learn to expect non - dominant transitions within the dominant model , but they are less 434 18 likely to learn about them when they believe the sequence is being controlled by a different 435 model . 436 We thus tested for the existence of this gated learning mechanism . For this , we ran a 437 logistic regression with five competitive predictors ( where the order of the predictors does not 438 change the estimates of the regression coefficients ) . First , three predictors of the regression 439 allowed us to control for the effects of the previous choice type , and latest stimulus type , as 440 well as a rough estimate of the general effect of time on performance , then two predictors 441 represented the two contrasting states of learning , that is , learning when confidence is high , 442 and learning when confidence is low ( Fig . 3C ) . Indeed , and as expected from the MGM 443 framework , and confirming results from the patterns of errors , we found a negative impact on 444 performance ( i . e . , an increased probability of making a non - dominant choice ) of having made 445 a non - dominant choice in the previous trial ( t ( 598 ) = - 18 . 5 , p < . 001 ) and of observing a non - 446 dominant transition with the latest stimulus ( t ( 598 ) = - 23 . 0 , p < . 001 ) . Furthermore , this 447 analysis showed a positive effect of time on performance ( t ( 598 ) = 7 . 4 , p < . 001 ) , i . e . , on 448 average performance improves over time . 449 19 Fig 3 . Learning via a mixture - of - generative - models . ( A ) Arbitrating amongst a pool of generative models : 450 Under the MGM framework , participants entertain a pool of candidate generative models : the dominant model 451 ( i . e . , the one with the correct dominant pattern , blue ) but also other alternative models ( i . e . , models with different 452 dominant patterns , pink ) . As the stimulus sequence unfolds , stimulus transitions either give evidence in favour of 453 the dominant model ( i . e . , when the participant observes a dominant stimulus transition , blue ) or against it and in 454 favour of some of the alternative models ( i . e . , when the participant observes a non - dominant stimulus transition , 455 pink ) . The observation of new stimulus transitions thus causes fluctuations in participants’ belief about which 456 model controls the stimulus sequence . As confidence in the dominant model increases , so does the probability of 457 participants using that model to guide their choices—and thus the probability with which they will make a 458 dominant prediction ( i . e . , predicting the dominant transition ; blue ) . When evidence for the dominant model 459 decreases ( low confidence ) , alternative models are more likely to guide participants’ choices and thus result in a 460 non - dominant prediction ( pink ) . In short , the “errors” that participants make ( i . e . , when they make a choice to a 461 location other than the one that would be suggested by the dominant transition ) should not be randomly distributed 462 in the sequence . Instead , the probability of certain “errors” should depend upon whether a non - dominant transition 463 was observed ( or a non - dominant choice was made on a previous trial ) . ( B ) Participant errors are structured 464 according to MGM : Participants’ choice behaviour indicated that the proportion of dominant choices in one trial 465 ( green ) depended on the recent history leading up to that trial . As predicted by MGM learning : when the latest 466 stimulus was dominant , participants were more likely to use the dominant model—and thus make a dominant 467 prediction—as compared to when that stimulus was non - dominant . Similarly , when the previous choice was 468 dominant , the next choice was more likely to be dominant as compared to when the previous choice was non - 469 dominant ( i . e . , a choice revealing that participants were not relying on the dominant model at the time ) . ( C ) 470 Evidence for gated learning : We ran a logistic regression analysis , whereby we predicted the trial - by - trial choice 471 type of each participant ( i . e . , dominant or non - dominant , 1 or 0 ) as a function of five predictors . The first predictor , 472 whether the previous choice was non - dominant ( non - dom ch ; 1 or - 1 ) , helped us confirm that performance was 473 lower after having made a previous non - dominant choice . The second predictor , whether the latest stimulus was 474 non - dominant ( non - dom stim ; 1 or - 1 ) , helped us confirm that performance was lower after having observed a 475 non - dominant stimulus . The third predictor , an index of time elapsed ( trial ; z - score of trial number ) helped us 476 capture the general tendency for performance to improve with time . The fourth and fifth predictors are interaction 477 terms between the latest stimulus type ( non - dominant or not , 1 and - 1 ) and time elapsed ( z - score of trial index ) 478 but separately for trials when participants have a predicted low confidence on the dominant model being in control 479 of the stimulus ( fourth predictor , low conf ) and when they have a predicted high confidence ( fifth predictor , high 480 conf ) . These last two predictors provide evidence for gated learning , which predicts that participants learn to deal 481 with ( i . e . , learn to have their performance unaffected by ) non - dominant stimuli mostly when they experienced 482 strong evidence ( high confidence ) of the stimulus being controlled by the dominant model . The reported 483 coefficients have arbitrary units ( but all predictors have close to unit standard error ) , we report the mean and SEM 484 across participants . 485 20 Evidence for gated learning came from the remaining two predictors of the regression , 486 which together probe in what conditions participants learned to deal with non - dominant stimuli . 487 Specifically , non - dominant stimuli provide evidence that favours alternative models and have 488 the effect of increasing the probability of non - dominant choices ( i . e . , participants are more 489 likely to use an alternative model to guide their choices ) . However , under MGM , participants 490 can gradually learn the correct dominant transition probability value , and hence , also the 491 probability values of the non - dominant transitions ; with more accurate estimates of the non - 492 dominant transitions , participants will , on average , be less surprised by non - dominant stimuli 493 ( i . e . , with accurate estimates , it is less likely that non - dominant transitions appear to be too 494 frequent or too rare given the generative model ) . In short , under MGM we expect participants 495 to learn to be less surprised and less affected by non - dominant transitions . Consistent with this , 496 we recovered a positive effect of “learning to deal with non - dominant stimulus transitions” on 497 performance for cases when participants were expected to have high confidence in the 498 dominant model ( fifth predictor , high conf , t ( 598 ) = 3 . 19 , p < . 01 ) . When participants were 499 predicted to have been using an alternative model , the effect was lower than for high confidence 500 ( fifth vs fourth predictors , high conf vs low conf , t ( 598 ) = - 5 . 33 , p < . 001 ) and not different 501 from zero ( fourth predictor , low conf , t ( 598 ) = - 1 . 62 , p = . 1 ) , showing that learning was gated 502 by confidence . These gated learning effects would be otherwise hard to explain , but are easily 503 and naturally explained under MGM learning , providing a final confirmation that indeed 504 participants are engaging in a mixture - of - generative models learning strategy to solve our 505 multi - task experiment . 506 Finally , while the structured pattern of errors provides strong evidence in favour of the 507 MGM framework , it remains possible that simpler forms of learning , like trial - and - error 508 learning could contribute in parallel to control behaviour . Such simpler learning models predict 509 that rewarded choices increase the probability of repeating the choice in the future . We thus 510 sought to assess the impact of reward on subsequent behaviour . However , in our experiment 511 the effect of reward ( i . e . , catching or not catching the wolf ) is confounded with the effect of 512 the previous prediction type ( dominant or not ) and the latest stimulus transition ( dominant or 513 not ) across all trials . To avoid the confound , we focused only on those trials where participants 514 21 made non - dominant choices and the stimulus that followed was also non - dominant . Because 515 in this circumstance the choice could either be rewarded or not ( i . e . , in 50 % of trials where the 516 participant made a non - dominant choice and the wolf moved to a non - dominant location , the 517 wolf would actually be caught and in the other half of trials the wolf would not be caught ) this 518 confound was eliminated . Learning by trial - and - error would predict that rewarding non - 519 dominant choices ( i . e . , when the wolf was caught ) should increase the probability of future 520 non - dominant choices ( and thus decrease the probability of dominant choices ) compared to not 521 rewarding non - dominant choices ( i . e . , when the wolf was not caught ) . Instead , we found that 522 reward on its own had only a minor impact on participants ' predictions . The percentage of 523 dominant predictions was 42 . 3 % ( ± 1 . 1 % SEM ) after a rewarded non - dominant choice and 524 39 . 8 % ( ± 1 . 1 % SEM ) after an unrewarded non - dominant choice ; this small change ( 2 . 5 % ± 1 . 4 525 SEM ) is , if anything , going in the opposite direction of what would be expected from trial - and - 526 error learning . This analysis suggests that the effect of simple trial - and - error learning , in our 527 experiment , is negligible . 528 Discussion 529 How humans and other animals learn a generative model for a single task has been a 530 topic receiving ample attention . Yet , there is limited knowledge about the role that generative 531 models themselves play in guiding future behaviour . Here , we presented a novel experimental 532 paradigm where human participants went through a series of prediction tasks ( i . e . , on each trial 533 they explicitly predicted where a stimulus would appear next ) . Critically , the generative models 534 that controlled the stimulus sequences in each of the prediction tasks were sampled from a 535 common generative family : a space of possible generative models respecting a set of rules . 536 This multi - task prediction learning paradigm allowed us to investigate not only within - task 537 learning but also across - task learning , and thus address a number of key questions about the 538 role that generative models play during learning . 539 Humans have been shown to approach learning problems in perceptual , decision - 540 making and cognitive everyday tasks , equipped with inductive biases . These biases come in 541 the form of prior knowledge and intuitions , which can be both innate and learned , that exploit 542 22 structural properties shared across different tasks , and which result in learning in a particular 543 task to unfold more quickly than would otherwise be expected ( Acuña & Schrater , 2010 ; Braun 544 et al . , 2010 ; Chomsky , 1980 ; Harlow , 1949 ; Lake et al . , 2017 ; Tenenbaum et al . , 2011 ) . The 545 combination of our multi - task prediction learning paradigm with the analysis of the very early 546 behaviour allowed us to determine that participants came to the experiment with good intuitions 547 ( in the form of structured priors ) which they rapidly updated to better match the true generative 548 model of the tasks . Our findings thus suggest that structured priors are a key ingredient for 549 bringing about inductive biases . 550 Our results extend our current knowledge of inductive biases and structure learning in 551 important ways . Structure learning is often conceptualised as a reduction of the dimensionality 552 of the space over which to search for a solution to the new task that causes learning to occur 553 faster ( Braun et al . , 2010 ) and many previous reports suggest that structure learning arises 554 slowly over repeated exposure to ( often several ) different instances of tasks within the task 555 space . For example , Harlow’s research on the formation of learning sets required tens or 556 sometimes hundreds of learning problems ( Harlow , 1949 ) . In contrast , here we observed that 557 exposure to a single task allowed our participants to improve their knowledge of the task space . 558 Second , while previous studies either focused on learning of multiple simple tasks ( Kattner et 559 al . , 2017 ; Schulz et al . , 2020 ) , or learning of a single complex graph - like structure ( Cleeremans 560 & McClelland , 1991 ; Garvert et al . , 2017 ; Schapiro et al . , 2013 ) , we focused on how humans 561 learn multiple complex structures , an issue which had not been looked at until very recently 562 ( Mark et al . , 2020 ; Wu et al . , 2019 ) . Third , studies typically focus on the consequences of what 563 is being transferred , for instance , whether there is an immediate benefit to performance or a 564 change in the rate of learning ( Braun et al . , 2010 ; Kattner et al . , 2017 ) ; our paradigm gives us 565 additional insights into the content that is being transferred , specifically , the pool of candidate 566 models that are used for explaining the task increasingly matches the true pool of possible 567 models within the paradigm . Fourth , previous studies of generalization have primarily focused 568 on how learners search for solutions ( i . e . , a parameter set that is incrementally tuned ) that help 569 them best explain the new task . Instead , we propose that participants faced with complex multi - 570 task environments likely engage in a mixture - of - generative - models ( MGM ) learning strategy , 571 23 whereby they maintain a multitude of competing solutions which are continuously updated and 572 mixed in order to form predictions . 573 Behaviour under an MGM learning framework has all the key characteristics of our 574 human learners . Humans seem to simultaneously engage not only in ( i ) structure learning , i . e . , 575 the discovery of the possible structures at play , meaning both discovering them from scratch 576 as well as discovering that some previously learnt structures are useful in the current context , 577 and in ( ii ) parameter learning , i . e . , the fine tuning of the parameters of any one structure , but 578 also in ( iii ) the arbitration amongst different competing structures in any one task , a process 579 which could be called structure switching . Finally , previous studies that have attempted to 580 capture the variability of choices in learning tasks through the lens of optimality , have done so 581 by casting the problem in terms of balancing the trade - off between ( i ) the benefits of exploiting 582 the best candidate structure thus far and ( ii ) the costs of exploring the possible other structures 583 ( Acuña & Schrater , 2010 ; Gershman , 2018 ) . However , in our tasks , such a trade - off is non - 584 existent because the observations are independent of the choices . Instead , we propose that 585 MGM learning is able to explain variability in choices as a natural consequence of the 586 variability in the priors that participants bring to the experiment . Additional evidence in support 587 for the MGM framework comes from the patterns of gated learning observed from participants’ 588 behaviour which are highly diagnostic of participants’ usage of the strategy . 589 The MGM framework is a fully Bayesian forecast motivated by work in machine 590 learning and artificial intelligence . It is a system with strong inductive biases that nonetheless 591 allow cross - task generalization , and as such , it is more consonant with a probabilistic 592 programming approach ( Tenenbaum et al . , 2006 ; Tenenbaum & Griffiths , 2003 ) than deep 593 learning approaches which do not require inductive biases but instead have much higher 594 requirements of data ( Wang et al . , 2018 ) . A strength of the MGM framework is that it helps 595 solve two problems simultaneously : first , how to allow strong inductive biases while preserving 596 enough flexibility to deal with the idiosyncrasies of the environment , second , how to mimic 597 the fast learning that humans are capable of , while also capturing the individual differences in 598 learning speeds . The framework thus relies on hierarchical generative model families which 599 exploit compositionality ( i . e . , models factor into reusable parts ) — allowing flexible mapping 600 24 between an abstract dynamics space and observation spaces ( i . e . , factoring the dynamics and 601 the observation ) . The framework also relies on the minimum entropy principle : learners have 602 a predisposition to hasten their learning , by having a bias for determinism ( Brand , 1999 ) , at the 603 risk of learning collapse in stochastic domains , where a maximum entropy assumption would 604 not lead to such collapse ( Jaynes , 1982 ) . Preserving flexibility while using strong inductive 605 biases is a serious theoretical and computational challenge , which humans seem extremely 606 good at achieving . We propose that humans rely on MGM as a strategy to solve this challenge . 607 Finally , the MGM framework not only provides a single , theoretically sound , 608 explanation that accounts for all those features of the data without the need to invoke any 609 suboptimal mechanisms on the side of participants , but it also makes important predictions 610 about the behavioural consequences of manipulating participants’ priors that could guide future 611 lines of research . Two specific aspects of participants’ prior are predicted to be of particular 612 importance : ( i ) the size of the model pool ( i . e . , the number of candidate models that participants 613 are arbitrating against one another ) , and ( ii ) the bias for determinism ( i . e . , the amount of 614 stochasticity that participants allow their candidate models to have at the start ) . Regarding the 615 size of the pool of models , there are pros and cons . If participants arrive at the task with a large 616 number of competing models , they are more likely to have a model that is good for solving the 617 task . However , a larger pool of models also means that the space of inferences , and the time 618 taken to find a good model , grows exponentially with the number of candidate models . 619 Regarding the bias for determinism , the more deterministic a model is , the less responsibility 620 it has for explaining transitions that deviate from its predictions , and thus , the less it will be 621 updated to incorporate more stochasticity ( as long as there are other models that explain those 622 transitions ) . This latter point can have disastrous consequences for performance : having too 623 strong a prior for determinism can lead participants to never learn to expect non - dominant 624 transitions within the dominant model , and instead be stuck in a loop where they constantly 625 switch between deterministic models that capture regularities at a short temporal scale . 626 Interestingly , however , the inductive biases we see , including the bias for determinism , reveal 627 a strong desire from our participants to learn quickly . Getting a better understanding of how to 628 influence participants’ inductive biases could help capitalize on participants’ desire for learning 629 25 when facing new learning problems . These insights could benefit fields as diverse as artificial 630 intelligence and educational sciences . 631 Acknowledgements 632 We want to thank Aaron Cochrane and Ornela De Gasperin for useful comments on the 633 manuscript , and Amanda Yung for help with programming the tasks . This work was supported 634 and funded by : a Swiss National Science Foundation grant : 100014 _ 15906 / 1 to DB ; the 635 Luxembourg National Research Fund : ATTRACT / 2016 / ID / 11242114 / DIGILEARN to PCL ; 636 the INTER Mobility / 2017 - 2 / ID / 11765868 / ULALA to PCL and PS . The Office of Naval 637 Research grant N00014 - 17 - 1 - 2049 to CSG . The European Union ' s Horizon 2020 research and 638 innovation program under Marie Sk ł odowska - Curie grant no . 661667 , Learning Determinants 639 to IA . The Office of Naval Research grant MURI GRANT N00014 - 07 - 1 - 0937 to DB . 640 Author contributions 641 PCL , CSG , PS and DB conceptualized the research idea and created the research design ; SHC 642 and PCL programmed the experiments . SHC , PC , and IA collected the data ; SHC , PCL , CSG , 643 PS and DB designed the analyses ; SHC , PCL , and PS analysed the data ; SHC and PS developed 644 the models ; SHC , PCL , CSG , PS and DB interpreted the results ; SHC drafted the manuscript ; 645 SHC , PCL , CSG , PS and DB finalized the manuscript ; SHC and PCL managed the submission 646 process . All authors approved the final manuscript . 647 Competing interests 648 The authors declare no financial or non - financial competing interests . 649 26 Methods 650 Ethics and consent 651 The study was approved by the Ethical Review Panel of the Faculty of Psychology and 652 Educational Sciences of the University of Geneva . All participants were recruited using 653 Amazon’s Mechanical Turk ( MTurk ) platform . Participants were all based in the US , at least 654 18 years old and had an approval rating of at least 95 % . After reading information about the 655 study , participants were asked to consent by clicking on a checkbox . 656 The Catch The Wolf paradigm 657 Catch The Wolf is a multi - task learning paradigm . It presents participants with a series 658 of four different sequential prediction tasks . In each prediction task , participants observed a 659 wolf successively appear in one of four possible screen locations and they were tasked to 660 continuously predict where the wolf will appear next . After each prediction task , participants 661 performed a brief production task , where they were asked to mimic the behaviour of the wolf 662 they just experienced in the latest prediction task by producing a sequence of locations . 663 Crucially , the behaviour of the wolf within a particular task is generated by a specific generative 664 model ( i . e . , a specific location - to - location transition matrix ) . While each of the four tasks can 665 be determined by a different generative model—and hence requires participants to learn each 666 task anew—all generative models stem from a common generative family—knowledge about 667 this generative family could be exploited when confronted with new generative models . 668 The generative model 669 The sequence of wolf locations in a given prediction task is generated as a first Order 670 Markov chain , where the location of the wolf in one trial depends solely and probabilistically 671 on the location from the previous trial . This generative model can be described by a transition 672 probability matrix from one trial’s location to the next trial’s location . However , not all 673 transition probability matrices are valid under the generative family : valid transition matrices 674 need to respect five rules . First , there is one maximum value per row which we call the 675 “dominant transition” , i . e . from any one location there is a unique location which is the most 676 likely next location . Second , all diagonal values are set to zero , i . e . there are no self - transitions , 677 or the wolf never reappears in the same location from where it just disappeared . Third , there is 678 27 a uniform stationary distribution , i . e . on average , all locations are visited by the wolf with the 679 same frequency . Fourth , the value of the dominant transition probabilities is equal across four 680 rows , i . e . there is a single dominant transition probability value to be learnt . Fifth , all non - 681 dominant and non - diagonal transitions have the same probability value , i . e . there is a single 682 non - dominant transition probability value . 683 These five rules strongly constrain the space of possible wolf behaviours . In essence , 684 given that we have four locations for the wolf to appear , this space of valid transition matrices 685 can be characterized by two variables . First , the dominant transitions of valid matrices follow 686 one of six possible patterns ( i . e . , these patterns indicate which is the most likely next location 687 from any one location ) of which we used only four in our studies . Second , valid transition 688 matrices are characterized by the value of the dominant transition probability ( i . e . , the 689 probability with which the wolf will move to the most likely next position ) . Thus , the transition 690 probability matrix controlling the wolf on any one task was obtained by selecting ( i ) a dominant 691 transition pattern and a ( ii ) dominant probability value ( Fig . 1A ) . 692 Prediction tasks 693 Across most experiments the basic paradigm consisted of four prediction tasks that 694 participants completed in succession ( Fig . 1A ) . In each of the four prediction tasks , participants 695 observed a wolf ( a cartoon picture of the head of a wolf ) that successively appeared in either 696 one of four possible screen locations ( Fig . 1B ) . Participants’ goal was to continuously predict 697 ( i . e . , one prediction for each trial ) the wolf’s next location . The wolf’s behaviour can be fully 698 described by a transition probability matrix that governs the probability that the wolf will move 699 from any one location to any other location . To perform well , participants had to learn ( and 700 select ) the most likely destination for each possible current location of the wolf . The sequence 701 of transitions was strictly identical for each participant within an experiment . For each 702 Prediction Task these sequences were generated in such a way that the empirical transition 703 frequencies approximately matched the transition matrix of the true model . 704 The true model ( described in detail in the following section ) can be described as 705 consisting of two parts : ( i ) a permutation matrix ( which unique location is the most likely from 706 each position ) and ( ii ) a dominant transition probability ( i . e . , the probability with which the 707 28 wolf will move onto the most likely location ) . Because participants find it easier to learn more 708 deterministic sequences and to ease participants into the experiment , we implemented a 709 scaffolding strategy whereby the value of the dominant transition probability was gradually 710 decreased across tasks ( but remained constant within a task ) . Thus , for most experiments the 711 dominant transition probability ( 𝝰 ) for prediction task 1 had a value of 1 . 0 , for prediction task 712 2 it had a value of 0 . 95 , for prediction task 3 it had a value of 0 . 85 and for prediction task 4 it 713 had a value of 0 . 75 . Similarly , the number of trials was set for each task in such a way as to 714 keep the task duration as short as possible while allowing enough trials for most participants to 715 achieve learning . 716 Production tasks 717 Immediately after each Prediction Task , participants were asked to imitate the wolf’s 718 stochastic behaviour by sequentially clicking on one of the four possible locations . The exact 719 instructions stated the following : " You ' ve observed how this wolf behaves . Your goal now is 720 to be the wolf . Click on the target locations to make the wolf appear just like the one you ' ve 721 just observed . " They were asked to make 24 clicks creating 23 wolf moves . The production 722 tasks not only helped emphasize the contextual cues that one prediction task was over , but they 723 also provided us with a different perspective to investigate what participants had learnt about 724 the behaviour of the wolf during the prediction task . 725 Experimental manipulations 726 We collected data across 854 participants over a total of ten experiments . All 727 participants within an experiment experienced the exact same sequence of stimuli . However , 728 we manipulated the stimulus sequence across experiments in important ways ( see Table 1 ) . 729 First , in separate experiments , stimuli were presented in the visual and auditory modalities 730 respectively . In the “Catch The Wolf” experiments the stimulus was visual ( the cartoon face of 731 a wolf ; as described above ) and could appear in one of four possible locations . Participants 732 made their predictions ( and productions ) by clicking on either one of the four locations . In the 733 “Catch The Sound” experiments the stimulus was auditory ( brief audio - tracks of the syllables 734 “bi” , ”da” , ”fu” and “go” ) and participants made their predictions ( and productions ) by clicking 735 on circular placeholders with the corresponding text ( i . e . “bi” , ”da” , ”fu” and “go” ) whose 736 29 relative spatial order on the screen varied randomly from trial to trial . Second , different 737 experiments had a different progression of tasks . With each task being described by : ( i ) number 738 of trials , ( ii ) the identity of the dominant pattern and ( iii ) the dominant transition probability 739 value ; ( see Fig . 1A ) . 740 Table 1 . Experiment descriptions . 741 Exp ID Number of Participants Dominant patterns ( Pat ) , dominant transition probability value ( in percentage ) and number of trials ( in parentheses ) Sensory modality of stimulus Task 1 Task 2 Task 3 Task 4 1 173 Pat1 100 % ( 76 ) Pat2 95 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) visual 2 222 Pat1 100 % ( 50 ) Pat2 95 % ( 75 ) Pat3 85 % ( 100 ) Pat4 75 % ( 200 ) auditory 3 64 Pat1 100 % ( 76 ) Pat1 85 % ( 101 ) Pat1 85 % ( 151 ) Pat4 75 % ( 201 ) visual 4 71 Pat1 100 % ( 76 ) Pat2 85 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) visual 5 48 Pat3 100 % ( 76 ) Pat3 95 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) visual 6 44 Pat1 100 % ( 76 ) Pat2 95 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) visual 7 63 Pat3 100 % ( 76 ) Pat3 95 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) auditory 8 56 Pat1 100 % ( 76 ) Pat2 95 % ( 101 ) Pat3 85 % ( 151 ) Pat4 75 % ( 201 ) auditory 9 56 Pat1 100 % ( 76 ) Pat3 95 % ( 101 ) Pat4 75 % ( 201 ) only 3 tasks visual 10 57 Pat1 100 % ( 76 ) Pat4 75 % ( 201 ) Pat3 85 % ( 151 ) only 3 tasks visual Exclusion criteria 742 Recruiting participants on online platforms improves the convenience of collecting data 743 from a larger pool of participants . However , it means we have to pay attention to only include 744 data from participants who are : ( i ) completing the experiment , ( ii ) following the instructions 745 adequately and ( iii ) trying hard to do the tasks . We thus decided to impose the following 746 exclusion criteria to filter out participants that were not actually doing the task . First , we 747 decided to exclude 393 participants who failed to provide a response in at least 85 % of the 748 trials of any one prediction task ( most of these are entries from participants who started the 749 experiment but soon after abandoned it ) . Second , we decided to further exclude the 26 750 participants whose performance ( i . e . , proportion of choices in agreement with the true 751 generative model ) was at or below 25 % ( i . e . , chance level ) at any one prediction task . Third , 752 we excluded a further 124 participants who produced 5 or more consecutive self - transitions 753 ( i . e . , simply clicking the same position over again ) in any one production task . In total , 854 754 participants survived all exclusion criteria . 755 30 Performance progression in relation to the true generative model 756 In order to assess within - task learning , we decided to compute performance in relation 757 to the true generative model : the proportion of choices that match what an agent with full 758 knowledge of the true generative model ( an agent that does not need to learn ) . We then 759 aggregate the proportion of those choices for each participant made within time bins of 25 trials 760 in length . The learning curves shown in Fig . 2A are obtained by showing the mean and standard 761 error of the mean of choice performance across participants for each time bin and for each task . 762 Artificial learning agents 763 As a tool for analysing the quality of participants’ background knowledge and how it 764 changed as participants experienced the different tasks , we created an axis that spanned the 765 space of possible knowledge quality about the generative knowledge , going from zero 766 knowledge of the generative model all the way to perfect knowledge of the generative model . 767 The two extremes of this axis were obtained contrasting the choice probabilities of two artificial 768 learning agents that we describe next : ( i ) a naïve artificial learner and ( ii ) an ideal artificial 769 learner . First , we describe a naïve artificial learner who comes with a flat prior about what 770 could happen in each of the tasks and simply keeps track of the observed transitions so that the 771 next time a location is visited , it chooses the transition with the highest number of observations 772 ( with a random choice amongst any locations that have tied counts ) . Second , we describe an 773 ideal artificial learner who knows the generative family and makes inferences about which 774 specific generative model governs the current stimulus sequence and makes optimal inferences 775 about the most likely next transition . Specifically , the ideal artificial learner starts with a flat 776 prior over the possible transition matrices within the high - level generative model and updates 777 its belief upon every new observation to make inferences about ( i ) the probability of that each 778 of the six possible dominant patterns is governing the stimulus , ( ii ) the value of the dominant 779 transition probability . By marginalizing over these two dimensions , the agent can then select 780 the most likely next location given the observed sequence thus far . 781 31 Quantifying high - level generative model knowledge ( early behaviour analysis ) 782 Priors can quickly change in the face of new evidence . To analyse the type of 783 background knowledge that people came with at the start of any one task , we decided to use 784 the behaviour of the first four trials of each prediction task . The first four trials are special 785 because each of the predictions in those trials is done before the wolf has revisited a location , 786 and thus choices are entirely driven by the prior with which participants arrive at the task and 787 not by the evidence they have acquired in the current task . Note that this need not mean either 788 that the first four locations visited follow the dominant pattern of the task , or that the wolf on 789 the fourth trial did not reappear in a location it had already visited ( because the fourth choice 790 is done before the wolf appears for the fourth time ) . For example , in task 4 of experiments 1 - 791 8 , the first four stimulus transitions are not following the dominant pattern : the first stimulus 792 appears in the top right location , under the dominant pattern it should go to the bottom right , 793 but instead went to the top left location on the second trial . This shows that those first four 794 trials help dissociate the choices that would be correct under the true generative model , from 795 the choices that an ideal learner would make given the evidence that had been observed to that 796 point . 797 To make sense of participants’ early behaviour in the tasks we decided to contrast it 798 against the expected behaviour of the two benchmark learning models described above . 799 Specifically , for any one trial we derived the probability of observing the choice of any one 800 participant given any one of the two models . This allows us to obtain a measure of the relative 801 probability that a participant’s prediction in the i th trial resembles the expected prediction under 802 the ideal artificial learner or under the naïve artificial learner : 803 ( 1 ) However , because not all trials are equally diagnostic of the difference between the two 804 models , we first computed the diagnosticity of each trial by computing , for each of the trials , 805 how divergent the probability distributions of predictions were for the two models . We thus 806 computed the Kullback - Leibler divergence ( KL divergence ) between the two distributions . 807 Because the KL divergence is an asymmetric measure , we computed the divergence using each 808 32 of the distributions as a base and summed the divergences to obtain our diagnosticity measure 809 for each trial : 810 ( 2 ) Finally , we obtain a single measure of each participant’s early behaviour in each task 811 by computing the diagnosticity - weighted average log - likelihood ratio , : 812 ( 3 ) Using this measure , a distribution of behaviour for all participants in each of the 813 prediction tasks can be retrieved . Importantly , the axis over which participants’ individual 814 choices are projected is constructed using the theoretical expectation of choice probabilities 815 under the two benchmark models . This way , the two extremes of the axis correspond to choices 816 that are maximally discriminant of the two models ( which need not be the most typical choices 817 of either model ) . Instead of simply obtaining the distribution of the measure computed 818 from participants’ choices , we decided to compute a similar distribution from choices sampled 819 from the two benchmark models . Thus , for each actual participant , we simulated one 820 instantiation of each of the two benchmark models for each of the trials and constructed a 821 distribution of for each model and each task ( see Fig . 2C , grey ) . More specifically , for 822 each of the first four trials , each of the benchmark models describes a probability distribution 823 over choices . For each participant we decided to take one instantiation of choice ( i . e . , taking a 824 random draw from the probability distribution ) for each trial , under each of the two models . 825 This allowed to create two separate pools of simulated agents ( and their choices ) for which we 826 also computed the measure ( see Fig . 2C , pink and blue ) . 827 In order to test whether the distribution of background knowledge from participants in 828 task 1 differed statistically from the distribution of the naïve artificial learner , we computed the 829 area under the curve ( AUC ) of a receiver operator characteristic score along with a permutations 830 test ( with 1000 reshuffles ) in order to obtain a null distribution of the differences in score and 831 p - value ( i . e . , the proportion of the 1000 reshuffles that had an AUC score as extreme or more 832 extreme than the unshuffled score ) . We used the same statistical procedure to test whether the 833 distribution of background knowledge differed between : ( i ) task 1 and task 2 , ( ii ) task 2 and 834 33 task 3 , and ( iii ) task 3 and task 4 . The results of these tests for each experiment as well as 835 aggregating participants over all experiments , is concentrated in Table 2 . 836 Table 2 . Statistical comparison of the Background Knowledge Distributions . 837 task 1 > naïve task 2 > task 1 task 3 > task 2 task 4 > task 3 Experiment AUC p - val AUC p - val AUC p - val AUC p - val 1 . 832 . 000 . 754 . 000 . 587 . 002 . 509 . 757 2 . 752 . 000 . 687 . 000 . 574 . 001 . 507 . 738 3 . 838 . 000 . 711 . 000 . 586 . 107 . 555 . 182 4 . 869 . 000 . 711 . 000 . 606 . 015 . 493 . 926 5 . 872 . 000 . 844 . 000 . 552 . 253 . 458 . 268 6 . 775 . 000 . 727 . 001 . 557 . 400 . 591 . 126 7 . 565 . 196 . 810 . 000 . 635 . 000 . 492 . 772 8 . 575 . 169 . 911 . 000 . 545 . 317 . 482 . 690 9 . 784 . 000 . 759 . 000 . 500 . 999 . 500 N / A 10 . 817 . 000 . 781 . 000 . 465 . 540 . 500 N / A 11 . 762 . 000 . 800 . 001 . 529 . 733 . 514 . 913 All . 771 . 000 . 752 . 000 . 567 . 000 . 509 . 427 Finding structure in participants’ errors 838 The MGM framework predicts that errors in the tasks would not be random but that 839 they should be structured and predictable given the trial history . We thus decided to look at the 840 proportion of each choice type : ( i ) dominant ( i . e . , the choice is optimal under the true generative 841 model ) , non - dominant ( i . e . , the choice is not the optimal choice under the true generative 842 model , but it could be guided by an alternative model within the generative family ) and ( iii ) 843 self - transition ( i . e . , predicting that the wolf will appear where it just disappeared , guided by a 844 model outside the generative family ) . 845 The proportion of choices of each type was examined conditional on history or recent 846 previous events , in line with predictions of the MGM framework . Specifically , both ( i ) the type 847 of latest stimulus ( i . e . , whether the latest stimulus transition was dominant under the true 848 generative model or whether it was non - dominant ) , and ( ii ) the type of previous choice ( i . e . , 849 whether the previous choice was dominant under the true generative model or not ) , exerted an 850 influence on choices . 851 The rationale behind grouping trials based on the recent history leading to each of them 852 is that the MGM framework makes explicit predictions about how the proportion of current 853 34 choices should change as a function of last stimulus and choice . For instance , when the stimulus 854 is non - dominant under the true generative model , it casts doubt in the mind of a participant 855 about that model being in charge of controlling the stimulus , and instead provides support for 856 one or more of the alternative models participants may be entertaining . This would result in 857 participants increasing the probability that their choice will be guided by an alternative model 858 and thus make a non - dominant choice . Similarly , when the previous choice of a participant is 859 non - dominant , it reflects the fact that participants had low confidence that the dominant model 860 was controlling the stimulus at the time and thus are likely to keep using whatever alternative 861 model they entertain . We decided to focus uniquely on choices of the last task , because both 862 the proportion of non - dominant stimuli and the proportion of non - dominant choices was the 863 greatest of all tasks , thus allowing larger numbers of trials in the different groups . The stimulus 864 sequence in this task was also common for all experiments ( except for experiment 2 ) . 865 Logistic regression to confirm structure and gated learning 866 Looking at the proportion of dominant choices and how it depends on the trial history 867 across all experiments provided strong evidence that both the error structure as well as the 868 patterns of learning aligned well with the predictions from the MGM framework . However , the 869 effects shown in those analyses are achieved by pulling average choice proportions across 870 individuals . As a complementary analysis , we decided to run within participant regressions to 871 predict trial - to - trial choices as a function of the recent history with the aim of confirming the 872 structure in the errors as well as the gated learning mechanism . Thus , for each participant we 873 ran a logistic regression model to predict their choice type ( dominant choices coded as + 1 and 874 non - dominant ones as 0 ) in each trial of the fourth task , as a function of five predictors . The 875 statistical inference was then made on the distribution of the resulting estimates , making the 876 approach equivalent to a “mixed effects model” . 877 The first predictor signalled whether the previous choice was non - dominant or 878 dominant ( coded as + 1 and - 1 , respectively , meaning that negative coefficients reflect a 879 reduction in the probability of making a dominant choice ) . The second predictor signalled 880 whether the latest stimulus was non - dominant or dominant ( coded as + 1 and - 1 , respectively , 881 35 again meaning that negative coefficients reflect a reduction in the probability of making a 882 dominant choice ) . The third predictor signaled the index of the trial in order to get a sense of 883 whether performance was increasing through time ( the index of the trial was z - transformed to 884 have mean of zero and standard deviation of one but kept its ordinal nature ) . The last two 885 predictors allowed us to assess the two facets of the gated learning mechanism by separating 886 the effect of a three - way interaction between the three previous predictors into two simple 887 effects ( based on the previous choice type ) that looked at the interaction between trial index 888 and latest stimulus type . 889 Thus , the two last predictors ( i . e . , fourth and fifth predictors ) both look at how the effect 890 of non - dominant stimuli changes with time ( interaction between time and latest stimulus type ) 891 but they separate the effects by the type of previous choice . The fourth predictor includes the 892 crucial interaction term ( trial index vs stimulus type ) for trials when the previous choice was 893 non - dominant ( and has zeros elsewhere ) , thus reflecting the degree to which participants learn 894 to deal with non - dominant stimulus transitions but limited to trials where the previous choice 895 was non - dominant ( i . e . , when participants were unlikely to have been using the dominant 896 model to guide their choices ) . Finally , the fifth predictor includes also the crucial interaction 897 term ( trial index vs stimulus type ) but this for trials where the previous choice was dominant 898 and zeros elsewhere . Mixture model learning predicts that participants will only update the 899 parameters of their models ( e . g . , to incorporate more stochasticity ) in proportion to the extent 900 that each model is deemed to be responsible for the observation . Thus , we expect for the 901 regression coefficient of the fifth predictor to be greater than the coefficient of the fourth 902 predictor . To obtain reliable estimates in the regression , we made sure that we ran a regression 903 only for participants that had at least six non - dominant choices out of all trials in the fourth 904 task , this left 599 participants . One regression was run for each participant and statistical 905 inferences were made on the distribution of regression coefficients over the participants by 906 comparing the distribution against a null distribution around zero using a single - sample two - 907 tailed t - test for the distribution of each coefficient . 908 36 References 909 Acuña , D . E . , & Schrater , P . ( 2010 ) . Structure learning in human sequential decision - making . 910 PLoS Comput Biol , 6 ( 12 ) , e1001003 . 911 Behrens , T . E . , Muller , T . H . , Whittington , J . C . , Mark , S . , Baram , A . B . , Stachenfeld , K . L . , 912 & Kurth - Nelson , Z . ( 2018 ) . What is a cognitive map ? Organizing knowledge for 913 flexible behavior . Neuron , 100 ( 2 ) , 490 – 509 . 914 Behrens , T . E . , Woolrich , M . W . , Walton , M . E . , & Rushworth , M . F . ( 2007 ) . Learning the 915 value of information in an uncertain world . Nature Neuroscience , 10 ( 9 ) , 1214 – 1221 . 916 Boorman , E . D . , Behrens , T . E . , Woolrich , M . W . , & Rushworth , M . F . ( 2009 ) . How green is 917 the grass on the other side ? Frontopolar cortex and the evidence in favor of alternative 918 courses of action . Neuron , 62 ( 5 ) , 733 – 743 . 919 Brand , M . ( 1999 ) . Pattern discovery via entropy minimization . AISTATS . 920 Braun , D . A . , Mehring , C . , & Wolpert , D . M . ( 2010 ) . Structure learning in action . 921 Behavioural Brain Research , 206 ( 2 ) , 157 – 165 . 922 Carey , S . , & Bartlett , E . ( 1978 ) . Acquiring a single new word . 923 Chomsky , N . ( 1980 ) . Rules and representations . Behavioral and Brain Sciences , 3 ( 1 ) , 1 – 15 . 924 Cleeremans , A . , & McClelland , J . L . ( 1991 ) . Learning the structure of event sequences . 925 Journal of Experimental Psychology : General , 120 ( 3 ) , 235 . 926 Daw , N . D . , Niv , Y . , & Dayan , P . ( 2005 ) . Uncertainty - based competition between prefrontal 927 and dorsolateral striatal systems for behavioral control . Nature Neuroscience , 8 ( 12 ) , 928 1704 – 1711 . 929 Elman , J . L . ( 1990 ) . Finding structure in time . Cognitive Science , 14 ( 2 ) , 179 – 211 . 930 Feldman , J . ( 1997 ) . The structure of perceptual categories . Journal of Mathematical 931 Psychology , 41 ( 2 ) , 145 – 170 . 932 Fox , E . B . , & Jordan , M . I . ( 2013 ) . Mixed membership models for time series . ArXiv Preprint 933 ArXiv : 1309 . 3533 . 934 37 Fox , E . B . , Jordan , M . , Sudderth , E . , & Willsky , A . ( 2009 ) . Sharing features among 935 dynamical systems with beta processes . Advances in Neural Information Processing 936 Systems , 22 , 549 – 557 . 937 Garvert , M . M . , Dolan , R . J . , & Behrens , T . E . ( 2017 ) . A map of abstract relational 938 knowledge in the human hippocampal – entorhinal cortex . Elife , 6 , e17086 . 939 Gershman , S . J . ( 2018 ) . Deconstructing the human algorithms for exploration . Cognition , 940 173 , 34 – 42 . https : / / doi . org / 10 . 1016 / j . cognition . 2017 . 12 . 014 941 Harlow , H . F . ( 1949 ) . The formation of learning sets . Psychological Review , 56 ( 1 ) , 51 . 942 Jaynes , E . T . ( 1982 ) . On the rationale of maximum - entropy methods . Proceedings of the 943 IEEE , 70 ( 9 ) , 939 – 952 . 944 Jern , A . , & Kemp , C . ( 2013 ) . A probabilistic account of exemplar and category generation . 945 Cognitive Psychology , 66 ( 1 ) , 85 – 125 . 946 Kattner , F . , Cochrane , A . , Cox , C . R . , Gorman , T . E . , & Green , C . S . ( 2017 ) . Perceptual 947 learning generalization from sequential perceptual training as a change in learning 948 rate . Current Biology , 27 ( 6 ) , 840 – 846 . 949 Kersten , D . , Mamassian , P . , & Yuille , A . ( 2004 ) . Object perception as Bayesian inference . 950 Annu . Rev . Psychol . , 55 , 271 – 304 . 951 Kilner , J . M . , Friston , K . J . , & Frith , C . D . ( 2007 ) . Predictive coding : An account of the 952 mirror neuron system . Cognitive Processing , 8 ( 3 ) , 159 – 166 . 953 Knill , D . C . , & Richards , W . ( 1996 ) . Perception as Bayesian inference . Cambridge 954 University Press . 955 Koechlin , E . , Basso , G . , Pietrini , P . , Panzer , S . , & Grafman , J . ( 1999 ) . The role of the anterior 956 prefrontal cortex in human cognition . Nature , 399 ( 6732 ) , 148 – 151 . 957 Körding , K . P . , Ku , S . , & Wolpert , D . M . ( 2004 ) . Bayesian integration in force estimation . 958 Journal of Neurophysiology , 92 ( 5 ) , 3161 – 3165 . 959 Lake , B . M . , Salakhutdinov , R . , & Tenenbaum , J . B . ( 2015 ) . Human - level concept learning 960 through probabilistic program induction . Science , 350 ( 6266 ) , 1332 – 1338 . 961 Lake , B . M . , Ullman , T . D . , Tenenbaum , J . B . , & Gershman , S . J . ( 2017 ) . Building machines 962 that learn and think like people . Behavioral and Brain Sciences , 40 . 963 38 Mark , S . , Moran , R . , Parr , T . , Kennerley , S . W . , & Behrens , T . E . ( 2020 ) . Transferring 964 structural knowledge across cognitive maps in humans and models . Nature 965 Communications , 11 ( 1 ) , 1 – 12 . 966 Pouget , A . , Beck , J . M . , Ma , W . J . , & Latham , P . E . ( 2013 ) . Probabilistic brains : Knowns and 967 unknowns . Nature Neuroscience , 16 ( 9 ) , 1170 – 1178 . 968 Schapiro , A . C . , Rogers , T . T . , Cordova , N . I . , Turk - Browne , N . B . , & Botvinick , M . M . 969 ( 2013 ) . Neural representations of events arise from temporal community structure . 970 Nature Neuroscience , 16 ( 4 ) , 486 – 492 . 971 Schulz , E . , Franklin , N . T . , & Gershman , S . J . ( 2020 ) . Finding structure in multi - armed 972 bandits . Cognitive Psychology , 119 , 101261 . 973 Tenenbaum , J . B . , & Griffiths , T . L . ( 2003 ) . Theory - based causal inference . Advances in 974 Neural Information Processing Systems , 43 – 50 . 975 Tenenbaum , J . B . , Griffiths , T . L . , & Kemp , C . ( 2006 ) . Theory - based Bayesian models of 976 inductive learning and reasoning . Trends in Cognitive Sciences , 10 ( 7 ) , 309 – 318 . 977 Tenenbaum , J . B . , Kemp , C . , Griffiths , T . L . , & Goodman , N . D . ( 2011 ) . How to grow a 978 mind : Statistics , structure , and abstraction . Science , 331 ( 6022 ) , 1279 – 1285 . 979 Tolman , E . C . , & Honzik , C . H . ( 1930 ) . Introduction and removal of reward , and maze 980 performance in rats . University of California Publications in Psychology . 981 Wang , J . X . , Kurth - Nelson , Z . , Kumaran , D . , Tirumala , D . , Soyer , H . , Leibo , J . Z . , Hassabis , 982 D . , & Botvinick , M . ( 2018 ) . Prefrontal cortex as a meta - reinforcement learning 983 system . Nature Neuroscience , 21 ( 6 ) , 860 – 868 . 984 Ward , G . , & Allport , A . ( 1997 ) . Planning and problem solving using the five disc Tower of 985 London task . The Quarterly Journal of Experimental Psychology Section A , 50 ( 1 ) , 986 49 – 78 . 987 Ward , T . B . ( 1994 ) . Structured imagination : The role of category structure in exemplar 988 generation . Cognitive Psychology , 27 ( 1 ) , 1 – 40 . 989 Wolpert , D . M . , Ghahramani , Z . , & Jordan , M . I . ( 1995 ) . An internal model for sensorimotor 990 integration . Science , 269 ( 5232 ) , 1880 – 1882 . 991 39 Wu , C . M . , Schulz , E . , & Gershman , S . J . ( 2019 ) . Generalization as diffusion : Human 992 function learning on graphs . BioRxiv , 538934 . 993 Xu , F . , & Tenenbaum , J . B . ( 2007 ) . Word learning as Bayesian inference . Psychological 994 Review , 114 ( 2 ) , 245 . 995 996 997 40 Supplementary materials 998 Prediction and production task instructions . Below , we present screenshots of the 999 instructions given to participants at the start of the experiment . 1000