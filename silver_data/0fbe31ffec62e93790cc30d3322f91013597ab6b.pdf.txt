Evaluation metrics and methodologies for user - centered evaluation of intelligent systems Jean Scholtz a , * , Emile Morse b , Michelle Potts Steves b a Paciﬁc Northwest National Laboratory , P . O . Box 999 , Richland , WA 99352 , USA b National Institute of Standards and Technology , 100 Bureau Drive , Gaithersburg , MD 20899 , USA Available online 17 October 2006 Abstract In the past four years , we have worked with several research programs that were developing intelligent software for use by intelligence analysts . Our involvement in these programs was to develop the metrics and methodologies for assessing the impact on users ; in this case , on intel - ligence analysts . In particular , we focused on metrics to evaluate how much the intelligent sys - tems contribute to the users’ tasks and what the cost is to the user in terms of workload and process deviations . In this paper , we describe the approach used . We started with two types of preliminary investigations – ﬁrst , collecting and analyzing data from analysts working in an instrumented environment for a period of 2 years , and second , developing and conducting for - mative evaluations of research software . The long - term studies informed our ideas about the processes that analysts use and provided potential metrics in an environment without intelli - gent software tools . The formative evaluations helped us to deﬁne sets of application - speciﬁc metrics . Finally , we conducted assessments during and after technology insertions . We describe the metrics and methodologies used in each of these activities , along with the lessons learned . (cid:1) 2006 Elsevier B . V . All rights reserved . Keywords : Evaluation ; Metrics ; Intelligent software systems ; Intelligence analysts 0953 - 5438 / $ - see front matter (cid:1) 2006 Elsevier B . V . All rights reserved . doi : 10 . 1016 / j . intcom . 2006 . 08 . 014 * Corresponding author . Present address : 340 North Slope Way , P . O . Box 70 , Rockaway Beach , OR 97136 , USA . Tel . : + 1 503 355 2792 . E - mail addresses : jean . scholtz @ pnl . gov ( J . Scholtz ) , emile . morse @ nist . gov ( E . Morse ) , msteves @ nist . gov ( M . P . Steves ) . www . elsevier . com / locate / intcom Interacting with Computers 18 ( 2006 ) 1186 – 1214 1 . Introduction The job of an intelligence analyst is to provide ‘‘actionable intelligence . ’’ The analyst , usually in response to a question submitted by a customer , provides infor - mation to aid the customer in making a decision . Often there is not a clear cut ‘‘answer’’ to the question . Rather , the analyst provides as many facts as are avail - able , along with an interpretation of the situation based on her domain expertise . This information is contained in the analytic report , along with the sources used by the analyst and the conﬁdence that the analyst has in the interpretation . Analytic reports of good quality also note what is not known . The task is issued with a sus - pense date ; this is when the report is due . Time is usually of the essence . Tactical analysis deals with much shorter time frames and questions such as , ‘‘What is over the hill ? ’’ Strategic analysis typically has longer time frames and addresses prob - lems such as ‘‘Country - A and Country - B are developing stronger ties with each other despite pressure from the US . What has the relation between these countries looked like in the past ? What might it look like in the future ? How is this aﬀecting US policy regarding Country - B ? How have US actions in Country - C played into this ? ’’ Furthermore , analysts have to make their assessments with incomplete and sometimes even deceptive information . A good analytic product describes what is known as well as what is not . Theoretically , more information would enable intelligence analysts to produce analytic reports with fewer unknowns and higher conﬁdence levels for the interpre - tations they need to provide . And while many technologies such as the Internet , high quality image capture , and sophisticated audio capture have provided tremendous amounts of data , many challenges have arisen in the attempts to exploit these large volumes of data . One challenge is that analysts are not able to examine all relevant data in a timely fashion . In fact , it is extremely diﬃcult for analysts to extract only relevant data , resulting in a situation where they are able to examine only a small portion of relevant information that is available . Additionally , Heuer ( 1999 ) has identiﬁed a number of problems facing information analysts , including cognitive bias and the failure to search for evidence that disconﬁrms a ‘‘favored’’ hypothesis . A number of programs in the intelligence and defense departments in the United States are conducting research in developing intelligent software systems that attempt to address these and similar issues facing the intelligence community . These ‘intelligent’ software systems are designed to facilitate the analyst’s work processes . Examples of the types of software being developed include algorithms that model information analysts and track their behavior in real - time , therefore allowing the possibility of oﬀering the analyst help when large deviations from models are encountered . Hypothesis generation and tracking software analyzes data , determines possible hypotheses based on accumulated evidence in the data and might suggest these hypotheses to analysts if they are not being currently explored . Other systems answer questions , as opposed to queries , that analysts pose . These systems can keep track of the context of the question and use it when responding to follow - up clariﬁ - cation requests . For these types of software systems , the analyst and the software work together to accomplish the analyst’s tasks . J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1187 Evaluation has several important beneﬁts to these programs . First , evaluations help program managers of such projects determine what progress is being made . This helps to obtain the appropriate funding for programs that show progress and to determine alternative directions for programs that are less successful . Sec - ond , evaluations help researchers to see objectively how their software can help end users , and if necessary , help them reﬁne their research goals . Finally , results from evaluations using meaningful metrics can help technology transfer eﬀorts when trying to convince a population of target users to consider adopting a new technology . Given these types of high - level evaluation objectives , e . g . , determining program progress , advancing software researcher understanding of end - user needs , and show - ing system impact , we set out to identify a set of metrics which we could assess , i . e . , obtain measurement data , and provide meaningful insights into these high - level eval - uation objectives . We were interested in developing metrics for overall system perfor - mance , where the ‘‘system’’ is the combination of the strategic intelligence analyst and the software working together . Many earlier evaluations of software in the intel - ligence community concentrated on performance metrics of the software only , i . e . , the accuracy and timing with which the various algorithms operate . While perfor - mance evaluations are essential , they are not suﬃcient to predict if analysts will actu - ally use the software , as there are many other aspects beyond algorithm performance that impact the user’s experience . For example , experience has shown that analysts will not adopt new software if it does not provide additional value to them . There - fore , our task was to develop a set of user - centered metrics that would measure the utility of the software to the analyst and provide feedback on the user experience , thereby providing a more holistic assessment . To that end , we focused on developing user - centric metrics covering the larger user experience rather than performance evaluations for speciﬁc software algorithms . A variety of such user - centric metrics provide insights into the larger picture of the user experience . When too narrow a view is taken , factors that contribute to failure to adopt are often overlooked or missed . Finally , a secondary research objective of ours was to identify which metrics were meaningful for diﬀerent types of software systems , e . g . , those concentrating on user modeling vs . hypothesis generation . ( see Section 1 . 2 for more detail on software functionality categories . ) In the remainder of this paper , we present our work on developing evaluation methods and associated metrics and measures . We provide a brief contextual discus - sion of our evaluation choices pertaining to utility assessments in formative evalua - tions in Section 1 . 1 as well as the application research areas in Section 1 . 2 . In Section 2 , we describe two types of preliminary studies – long - term studies of analysts to help us understand the processes they use and how those processes can be measured , and a series of formative evaluations to help us deﬁne sets of application - speciﬁc metrics . In Section 3 , we describe one technology insertion study as a proof of concept in uti - lizing the new methods , metrics , and measures . In this study , analysts were given software ( i . e . , the inserted technology ) to use in producing an analytic product . We collected a subset of measures based on what we learned in our preliminary stud - ies and compared the process and products produced with the inserted software to 1188 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 the process and products produced in the baseline . In the ﬁnal section , we discuss our conclusions and ideas for future work . 1 . 1 . Evaluation design choices In the early stages of software research and development , it is our view that eﬀort spent on improving the utility of an application provides a greater long - term return than perfecting the overall usability of an application that may not provide increased utility to the end - user . Therefore , for prototype research software , we choose to place an emphasis on assessing utility , e . g . , the value the software application pro - vides to the user , rather than focusing on training , user interface issues , and the like . Having said that , utility is certainly intertwined with usability . Many , such as ( Dumas and Redish , 1999 ; John , 1996 ; Grinstein et al . , 2003 ) have stated that usabil - ity today is multidimensional , encompassing eﬀectiveness , learnability , ﬂexibility , and user attitudes towards the application . So , while we are certainly concerned with the eventual usability of the interface and usability issues that impact our assess - ments of utility of these prototypical applications , we do believe these other aspects of usability need only be ‘‘good enough’’ to identity opportunities to increase an application’s utility and assess the value it is currently providing . We were mindful that the program’s ultimate measure of success was in providing better analytical software environments in which analysts would work . Therefore , these evaluations were designed to be more formative than summative . That is , the evaluations were performed during design and development of the software applications with the intent of informing the design rather than summative or validation evaluations that are performed at the end of development ( Theofanos and Quesenbery , 2005 ) . Since we were interested in assessing utility to the analyst , we were concerned with impacts on both their processes and products . User - centered metrics were employed to reﬂect this perspective . We attempted to identify metrics that would help us assess such questions as : does intelligent software help analysts produce better reports ? Does intelligent software help analysts produce quality reports in less time than it currently takes ? Does such software help analysts have more conﬁdence in the prod - ucts they produce ? 1 . 2 . ‘Intelligent’ software systems types For the program in which the bulk of our work took place , there were ﬁve main research topics for which research teams were developing software . These topics are as follows : • Researchers in Prior and Tacit Knowledge were interested in assisting analysts in improving awareness of existing corporate knowledge that could contribute to the task at - hand . • Research in Hypothesis Generation and Tracking focused on generating a full range of plausible hypotheses and suggesting the ones which had not been explored by the analyst with the objective of reducing bias . J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1189 • Modeling Analysts and Analytic Processes research focused on building represen - tations of each analyst’s preferences , biases , and analytic strategies to support other research areas such as Hypothesis Generation and Tracking eﬀorts . • Massive Data research focused on methods for data organization and retrieval of relevant information using very large - scale data stores . • Human Information Interaction research focused on providing intuitive user envi - ronments for analytic work , e . g . , tools that support appropriate levels of abstrac - tion for information representations for the task at - hand , as well as , providing analysts with software environments that support rather than hinder and distract analysis eﬀorts . Our approach was to identify user - centered metrics for each category that con - tributed to an assessment of programmatic progress as well as metrics that provided feedback to the research teams on the utility of their systems to analysts and direc - tions for improving utility . We then looked at which metrics were useful in more than one of the program’s research categories – these are discussed in Sections 2 and 3 . 2 . Preliminary investigations We used two parallel threads of investigation to explore how we could develop metrics , measures and evaluation methods . In one thread we studied analysts working in the ﬁeld performing intelligence analysis . This study was performed over a 2 - year period during which variables in the analytic environment and study design were adjusted as experience was gained , arriving at a stable state for our subsequent ‘insertion’ studies described in Section 3 . The second type of thread was a series of formative evaluations . The goals of the formative studies were two - fold : ﬁrst , to provide feedback to the software researchers and second , to provide us an opportunity to determine feasible methods for collecting data to instantiate our metrics . We use the term metrics to refer to one or more measures along with human interpretation . Measures are of two types : conceptual and implementation . Conceptual measures can be generic while implementation measures are speciﬁc to a particular software implementation . For example , we might deﬁne an information gain metric as the amount of information that an analyst entered into his knowl - edge base per hour . A conceptual measure could be the number of documents that the analyst processed . Implementing this measure could be diﬀerent depending on the application . We might be able to collect the number of documents saved , print - ed , and copied from . If the application contains a foreign language translator we might count the number of documents the analyst asked to have translated . We might have to ask the analyst to indicate whether or not a document was used in the report . The following sections describe each of the two complementary approaches – long - term studies ( Section 2 . 1 ) and formative evaluations ( Section 2 . 2 ) . Sample results are shown in each section . 1190 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 2 . 1 . Long - term study of working analysts Two types of analysts participated in this phase of the study . A group of contract analysts worked on the project for a total of 7 analyst - years over a 2 . 5 - year period . The size of the group ranged from 2 to 4 at any time . The second group was made up of four people recruited from the Navy Reserves ; they performed intelligence anal - ysis as their reserve assignment . These analysts each worked for one or more 6 - week periods . Since we were interested in knowing how the analysts worked before addi - tional pieces of software were presented in the environment , we provided them with a desktop containing their usual tools – software for word processing , spreadsheet manipulation , presentation graphics , and an Internet browser . They were also pro - vided an instrumentation system , called the Glass Box ( Cowley et al . , 2005 ; Glass Box web site , 2006 ) which was used to help gather baseline measures of the analysts’ activities while completing their tasks . The Glass Box software captures much of the computer activity of the analysts , including keystrokes , queries posed to a number of search engines , copies and pastes between documents , print commands , bookmarks saved , and URLs accessed . The Glass Box also provides capabilities that allowed the analysts to mark documents as relevant and to add annotations to help the evalua - tors understand what they were doing at a given time . The Glass Box software also collects video data of the screen and audio data from the subjects . Tasks that could be accomplished using open source information were designed by senior intelligence analysts . The analysts were asked to generate intelligence prod - ucts and were assigned times when these products were due . We experimented with a variety of conditions . During the ﬁrst year , the tasks required the analysts to produce a product in 10 months , while simultaneously monitoring a topic and producing short reports as needed . During several 6 - week periods , tasks were presented in an overlapping fashion and the due dates ranged from one to ten days after the assign - ment date . 2 . 1 . 1 . Results The results reported here are from two early case studies . In the ﬁrst case study we analyzed data and conducted observation sessions with two senior analysts . In the second case study the participants were two junior analysts . In the ﬁrst case study , we observed the analysts over a two day period . We gave the analysts a short term task ( similar to the ones described earlier ) so that we could observe the entire process of data gathering , analysis , and reporting . They analysts were working in the instrumented Glass Box environment . Tables 1 and 2 below show , respectively , the amount of time the analysts were observed and the time that was captured by the Glass Box . Oﬄine activities that were not captured in the instrumented environment included : • Face to face meetings • Telephone conversations • Reading e - mail ( due to privacy concerns ) J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1191 • Reading documents that had been printed earlier • Reading documents that were not electronic ( conference proceedings , books , etc . ) There was an instance on day 1 when Analyst A forgot to start recording using the instrumented environment ; no activity was recorded for the entire day . Traditionally analysts turned on the recording at the beginning of the day and turned it oﬀ when they left in the evening . Thus , the time captured included coﬀee breaks and lunch periods as well as other oﬄine activities . During our observations analysts continually noted ﬁnding items of interest and using this information to guide further data gathering . Moreover , on the second day of observations , Analyst B arrived at the oﬃce with some notes he had compiled on the train during his morning commute . Both analysts started compiling information they found into a document that eventually became a report . We were able to view the growth of this document over time and were able to trace portions of the document back to the original electronic documents when analysts copied and pasted into their compiled information . We were able to compute the time they spent in the various applications : word process - ing , search engines , and in the Glass Box . In the second case study , we observed two junior analysts over three separate days . These analysts were assigned tasks over a 5 - week period . A number of these tasks overlapped . That is , the analysts would be working on one task and then would be assigned a task that could be due either earlier or later than the task they were currently working on . In some instances , analysts had three or four tasks assigned with various due dates . Table 1 Time captured in observations Day 1 Day 2 Total time Time oﬄine Total time Time oﬄine Analyst A 5 . 77 h 3 . 88 h 5 . 02 h 3 . 22 h 67 % 64 % Analyst B 6 . 05 h 3 . 1 h 7 . 95 h 4 . 12 h 51 % 52 % Table 2 Time captured in instrumented environment Day 1 Day 2 Total time Time oﬄine Total time Time oﬄine Analyst A 6 . 69 h 5 . 00 h 75 % Analyst B 7 . 68 h 4 . 38 h 7 . 32 h 4 . 52 h 57 % 62 % 1192 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 We were able to classify our observations using ﬁve categories : • Handling multiple tasks • Collaboration • Interruptions and oﬄine activity • Data collection • Report generation Analysts did not work on multiple tasks as we had expected . We had anticipated that they might work on one task for a certain period of the day and then work on another related task . However , analysts prioritized their work based on the due dates of the tasks and the estimated time they felt the task would take . Collaboration was informal . Analysts were not oﬃcially collaborating with each other but they were aware of what others were doing and would at times send point - ers to information they came across that they felt might be helpful . Analysts also talked informally with colleagues who were experts in an area that was relevant to a task they were doing . Phone conversations and e - mails were not captured and much of this information was missed . Interruptions and oﬄine activities included the collaborations above . In addition analysts tended to read long documents in printed form and even did oﬄine searches . One analyst went to the Library of Congress and the National Academy Press to do some research . Although analysts made annotations about some of these events , they tended to summarize them at the end of day . For example , an analyst might note that he spent about two hours doing oﬄine reading but we had no idea when that activity occurred or how many segments of time it was broken into . Data collection was accomplished through both oﬄine and online activities as noted above . While we were able to trace information gained through online copies and pastes back to the source document , this was not feasible with oﬄine material . However , for evaluating online search engines , we could trace the amount of infor - mation that analysts collected and used in their ﬁnal reports . Some analysts used a template for diﬀerent types of reports and pasted informa - tion into this template as they were accumulating it . Others used a very rough doc - ument which did eventually turn into their report . 2 . 1 . 2 . Lessons learned in long term studies Our observations and data analysis of the long term studies pointed to some issues with the instrumented environment as well as to some metrics that would be useful in our future work . An initial decision to be as non - invasive as possible was revisited . Our analysis of time usage and observations showed that we were missing a number of events . We asked the analysts to note their oﬄine activity if it exceeded 15 minutes via the Glass Box annotation feature and to enter that annotation immediately upon resuming their online work . After we made this request , we noted an increase in the number of annotations in the Glass Box record and a decline in the number of peri - ods of unexplained inactivity , but the accounting for all time spent never exceeded 85 % and subsequent periods of observation showed analyst’s often forgetting to J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1193 leave a note . We decided to ask the analysts to ﬁll out a basic questionnaire at the end of each day , describing what they had done each day along with successes and problems . We used this debrieﬁng material to supplement annotations and help us understand the process analysts were using . We had originally anticipated the need to assign analysts multiple tasks but as we found they did not actually work on more than one task at a time , this requirement was eliminated . We had initially speculated that analysts’ work is separated into distinct phases . Fig . 1 depicts these phases in a description of intelligence analysis on the home page of the Central Intelligence Agency ( CIA Home Page , 2006 ) . Grinstein et al . ( 2003 ) have hypothesized that analysts spend more time on data collection and report gen - eration than on analysis . Development of intelligent software to help in those areas would , therefore , allow analysts to spend more time on analysis . While this may well be true , the problem is that these phases are not distinct . That is , analysts are always doing analysis . They do analysis while they search , organize , and assemble evidence , and even while they are constructing their reports . We had originally asked analysts to specify what phase of analysis they were in at any given time , but we found they were unable to do this with any consistency . They told us that at any one time they might be doing both data collection and analysis so their reports were of the phase that ‘‘predominated . ’’ While analyzing the phase data , we found that physical tasks tended to dominate . Therefore , phases of data collection and report generation were easier to identify than more cognitive tasks of planning and analysis . Consequently , we eliminated analysis of time spent in phases as a measure . We reformulated our hypotheses to consider that if analysts had to spend less cognitive eﬀort on data col - lection and report generation ( possibly being aided by software ) , they would be able to spend more cognitive eﬀort on the analysis portion of their work . We therefore gave the analysts the NASA TLX ( Hart and Staveland , 1988 ) rating and asked them Fig . 1 . Depiction of the intelligence analysis cycle ( CIA Home Page , accessed 2006 ) . 1194 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 to ﬁll this at the end of every day . This survey asks subjects to rate their perceived levels of workload with respect to 6 scales – temporal , physical , mental , frustration , performance , and eﬀort . Subsequently they indicate which factor was more impor - tant for each of 15 binary comparisons . The latter are used as weighting factors . Col - lecting this information for their baseline work environment allows us to make comparisons with cognitive eﬀort once they start using new tools . As the analysts’ reports diﬀered widely , we provided a template which asked them in addition to their actual product to provide information such as the conﬁdence they had in the report and the number of paths they explored . These were pieces of information we gleaned from conversations with the analysts that we were unable to capture using the instrumented environment . Metrics that we focused on after our initial observations included the following that could be captured or inferred from the Glass Box data : • Number of queries made • Analysis of queries for content to determine the number of analytic paths taken • Number of documents viewed • Relevance ratings of those documents • Number of documents saved , printed , bookmarked , copied • Annotations of collaboration activity • Time spent using each desktop application for each task • Signal to noise measure based on explicit relevance ratings – ratio of ‘good’ doc - uments to total documents viewed • Signal to noise measure based on implicit indicators – ratio of documents { print - ed , saved , copied from , or bookmarked } to total documents viewed • Growth of document over time Additional data that could be calculated from the analytic reports were : • Number of citations in report • Breakdown of citations by electronic reference , oﬄine reference , personal • Number of hypotheses explored • Analyst reported conﬁdence in product 2 . 2 . Formative evaluations While the long - term study of intelligence analysts allowed us to identify user - cen - tered metrics that were appropriate for assessing usability and utility of the standard desktop environment , we also needed to identify metrics speciﬁc for the new software tools that would be introduced . The ﬁrst step in developing software - speciﬁc metrics was to work with the various research teams . We conducted many group brain - storming sessions to discuss possible metrics that would measure the objectives the software developer researchers were trying to achieve . As each research team felt it was ready for some aspect of its software to be evaluated , we worked with them individually to develop a formative evaluation and to select a subset of measures J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1195 from our brainstorming sessions . We used surrogate analysts as participants in con - ducting these evaluations . We did not want to expose the analysts used in collecting these baseline data to the earlier versions of the software as they were going to be participants in our ﬁnal evaluations – insertions of the research software into their environment . Therefore , we recruited Naval reservists who were intelligence analysts when on reserve duty to participate in the formative evaluations . In total , we performed 18 formative evaluations . The majority of these were done in the context of an analytic task . Participants were given the task and asked to use the research software to complete some portion of the analytic task . In some cases , they were asked to generate a report or at least collect the essential information for such a report . We used a number of collection mechanisms . The Glass Box software was run in almost all cases and was used to capture such information as the number of docu - ments read , the queries that participants used to search for information , and the growth of the report document during the experiment . We used questionnaires and interview techniques to provide more qualitative data to help us understand the pro - cesses the participants used and how the research software helped or hindered them in this process . We also worked with the various research teams to help deﬁne the data that would be useful for them to log to help assess how their system was used . 2 . 2 . 1 . Methodologies for formative evaluations There are many types of user - centered evaluation methodologies . We employed two basic methods – heuristic review and user testing . Heuristic review is a common usability evaluation technique that is employed before testing with actual users . In a heuristic review , usability experts walk through the software and evaluate the user interactions according to an agreed upon set of heuristics . This gave the developers feedback prior to having actual users try the software . This allowed us to minimize the user interaction errors and to maximize the analysts’ feedback that is unique to their work . Three projects in the Human - Information Interaction ( HII ) area were sub - jected to a heuristic evaluation using Nielsen’s original 10 heuristics ( Nielsen and Mack , 1994 ) . User testing was the main type of evaluation for our work . The major components of a user - centered evaluation are the subjects , their tasks , and the data collection with which the subjects work . In addition , methods for collecting data from the sub - ject and from the subject’s interaction with the system need to be considered . Again , we used the Glass Box software to facilitate data collection . Twelve projects submitted a single software module for testing and three projects contributed two modules each . For each of these 18 evaluations , Naval reservists served as the subjects . They had from 1 to 19 years of experience with an average of about 5 years . We used the following types of evaluation set ups : 1 . Analysts were asked to generate hypotheses given a number of pieces of informa - tion . They compared their own hypotheses to the ones generated by the software . This method was used only for research software concentrating on hypotheses generation and tracking . 1196 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 2 . Analysts rated the relevance of query results submitted to the software system with user modeling and a baseline system . This method was used only for research software concentrating on user modeling . 3 . Analysts solved textbook problems using research software and in some instances compared it to solutions by hand . These were problems used in analysis classes and as such had deﬁnitive solutions . 4 . Analysts were asked to evaluate the system’s intermediate representation of their knowledge for completeness and correctness . This was used for software with machine reasoning components . 5 . Analysts worked with two versions of the systems – with and without a key feature . The analysts worked with a wide variety of data collections including the Internet , custom structured and unstructured data sets , and text collections such as those dis - tributed by the Center for Non - proliferation Studies ( CNS ) . We collected the same types of data that were analyzed for the long - term obser - vation studies – Glass Box , questionnaires including NASA TLX and analytic prod - ucts . Data were also logged by the systems based on speciﬁcations developed during early brainstorming sessions . These data were not the usual log data that developers create to debug software , but were supposed to provide user interaction data . The Glass Box software could detect a mouse click and identify the window in which it occurred but , without application instrumentation , the target of the click was unknown . We worked with the application teams on deﬁning what to log as well as the schema for their logs . 2 . 2 . 1 . 1 . Sample study results . One formative evaluation was to assess the impact of user modeling on the analyst . The user modeling software tracked the queries the analyst was performing and augmented subsequent queries with this context . We wanted to determine if analysts would ﬁnd more relevant documents with user mod - eling . As the software was in the early stages of development , we devised a labora - tory experiment . A generic user interface was developed for the user to use in submitting queries and in rating the relevance of the documents returned . Behind the scenes , the user’s query was submitted to two search engines ; one with the user modeling component and one without . The user had no knowledge of this and was simply presented with a list of documents and asked to rate the relevance of them . As the software at this point could not analyze and augment queries in real - time , we supplied analysts with an initial task and with 10 scripted queries that were appro - priate to ﬁnding information for that task . Besides the usual demographic data we asked subjects initially about their search strategies when performing analysis . We asked them whether they tended to try to get information about the overall situation ﬁrst or if they were more interested in obtaining detailed information ﬁrst . Of the three subjects participating in the study , one subject said that his strategy was based on knowledge of the topic at the begin - ning . If a lot was known , then he created an outline of useful information ; otherwise he started looking at details before creating an outline . A second subject said he J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1197 develops a search query to encompass the topic of interest and tries to ﬁnd the most relevant results . The third subject said that he attempts to ﬁgure out how a pattern works and then looks for the big picture . Table 3 shows the counts of documents presented by both systems . Note that sub - ject git3 performed only the ﬁrst nine queries of the set and therefore he worked with 90 documents while the other two subjects evaluated 100 documents with each sys - tem . The second row of the table shows the number of documents marked relevant for each subject . Subject git1 was much less likely to rate a document as relevant . All subjects rated more documents as relevant using the user modeling system than using the plain search engine , although the number is the case of subjects git1 and git3 were only slightly higher . The third row of the table shows that many documents were retrieved more than once by both systems . On average two - thirds of the documents returned by the user modeling system were unique , while only half the documents returned by the plain search engine were unique . When the relevant documents are calculated based on the number of unique documents , the document counts for both systems decreased . The user modeling system eliminated more duplicate documents allowing the ana - lysts to see as much as 25 % more documents in two instances . This study was helpful in determining that relevance of documents and signal to noise ratio were useful measures for assessing user modeling tools . We also discov - ered that we could look at strategies of the analysts and overlap in documents rated relevant to determine how well individual strategies were supported . 2 . 2 . 2 . Metrics developed from the formative evaluations We had two goals in conducting formative evaluations . First , we provided feed - back to the research teams about their software . Our reports included : lists of bugs and the severity of the identiﬁed problems , analysts’ comments about the utility of the software for its intended use in the intelligence community , evaluation of the ade - quacy of training materials , and other quantitative and qualitative data . This part of the evaluation was a typical formative usability test . The second goal of the study was to explore metrics that would be appropriate for evaluation of a particular tool or class of tools , where the class was mapped to the program’s research areas . In reviewing the metrics and measures used in our evalu - ations over the past 3 years , some commonalities have surfaced . Table 4 summarizes the measures that were found to be informative for the diﬀerent research areas . Table 3 Counts of documents retrieved User modeling Plain search engine git1 git2 git3 git1 git2 git3 Number of documents presented 100 100 90 100 100 90 Number relevant 16 41 36 11 31 33 Unique documents presented 67 72 54 49 49 45 Number of unique documents marked relevant 10 29 23 9 19 21 1198 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 2 . 2 . 3 . Lessons learned Many other factors came into play in designing each study , such as how would the subjects be trained , would they be observed , would they perform a verbal protocol , the content of questionnaires , the length and timeline of the study , and the nature of any debrieﬁng . Most of these factors are just part of routine study design , but two factors stood out that rise to the level of ‘lessons learned’ – training materials and software logging . The common characteristic is that both must be done by the appli - cation developer and not the evaluation team . The quality of training materials is critical to the success of a user study ; if the subjects can’t ﬁgure out what they need to do and how to do it , the study will fail . The amount of time devoted to training ranged from 30 min to 4 h and this was often broken up into iterations of training , exploration , and testing . Materials ranged from PowerPoint presentations to hardcopy scripts to verbal scripts delivered by the test administrator . Most often a combination of materials was used . The initial drafts of training materials were produced by the researchers since they understood best how the software worked . All materials were beta tested by someone from our team who had not been involved in developing the scripts before they were used with subjects . Table 4 Summary of measures by program research area For all program research areas User conﬁdence rating of report NASA TLX rating Modeling analysts and analytic process • Fraction of total time spent in the search tool • Number of queries made • Number of links expanded • Number of documents read • Relevance ratings for documents Prior and tacit knowledge • Number of redundant documents presented to user • User agreement with software derived ontology ( correctness and completeness ) Hypothesis generation and tracking • Analysts’ ratings of system - generated hypotheses • Number of hypotheses analysts explored in their report • Number of system - generated hypotheses analysts explored Massive data • Amount of documents processed by system / time • Amount of documents processed by analyst / time Human information interaction • Number of documents accessed • Number of documents read / time • Growth rate of analyst’s evidence ﬁle • Number of queries made • Number of hypotheses explored J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1199 It is important to note that the software developer research teams need to have suf - ﬁcient time in their testing schedule to develop and reﬁne these materials . While the concept of logging is familiar to application developers , the user - cen - tered logging that was required for these evaluations required intense negotiations between the evaluation team and the software team . We found that we needed to start to develop the logging speciﬁcation as soon as we knew that an evaluation was being planned . Once the log speciﬁcation was in place , we developed scripts that allowed us to plan for extracting the data for analysis . When the ﬁnal software arrived , we tested the scripts to be sure we could extract the data as planned . 3 . Technology insertions Our ﬁnal evaluations consisted of inserting research software into the analysts’ environment . We use the term ‘‘technology insertion’’ to describe these evaluations . Our goal was to measure how the research software impacted the process and prod - uct of the analysts . That is , we wanted to determine which , if any metrics for the var - ious research areas were correlated with an improvement in process or product . It is important to note that these insertions cannot correctly be labeled ‘‘experiments’’ in the scientiﬁc sense . There are a number of variables that we were unable to control and the sample sizes are too small to draw statistically signiﬁcant conclusions . None - theless , the insertions provide excellent feedback for the software research teams and do provide enough process and product measures to allow us to see if the impacts are positive , neutral , or negative . More importantly these technology insertions were useful to determine which metrics for the various research areas correlated with improvements in the process or products of the analysts . We drew on both types of preliminary studies described in Section 2 . The general design of the insertion studies was informed by our observations of analysts working on a variety of tasks . We used the application - independent measures from the long - term studies of users working in a standard desktop environment and supplemented them with metrics for Human - Information Interaction projects which emerged from our formative studies . The components needed for a successful technology insertion are : • Metrics • Participants • Suﬃciently robust instrumented software • Training , both for the use of the software technology and for the incorporation of the software into the analytic process • Baseline measures for comparison • Analytic tasks • Data for use in the tasks Each of these components is discussed in the following sections . Each technology was handled in the same manner . A technology insertion lasted 2 weeks . During the 1200 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 ﬁrst week , the analysts were given the research software and training to use the soft - ware . The remainder of that week , they were asked to use the software in the context of a task . They were assigned a task that they had previously completed and asked to determine if they could ﬁnd new information on this task by using this software . At the start of the second week , the analysts were assigned a new task developed for the particular technology being inserted and asked to generate a report by the end of the week . The data captured during the second week were used for comparison with one of the weeklong baseline tasks . 3 . 1 . Metrics In conjunction with program managers from the intelligence analysis community , we used results from the formative evaluations to develop a set of program goals and metrics used to assess the progress of the program . The metrics were deﬁned hierar - chically . That is , there were overarching metrics for the program and sets of metrics for the various research areas of this program . For each individual research project , we used the applicable metrics although these metrics may have been computed dif - ferently depending on the actual software system . For example , suppose we wanted to measure the number of hypotheses that the subjects explored during the course of their analysis . For some systems , we may have used a report format that asked the subjects to list these explicitly . For other systems , this information may have been logged directly from the system . In other cases , we might have had to infer this by reasoning about queries that the subject made . It is desirable to use multiple methods to inform metrics . In particular , it is desirable to use checks between qualitative and quantitative measures to provide a more holistic picture of use . The ﬁnal program metrics are shown in Table 5 . The baseline values were comput - ed from early formative evaluations and the long - term studies . We have focused on four major aspects : product quality , number of citations , analyst conﬁdence , and sig - nal to noise ratio . These aspects are not unrelated . A better quality report should result from an analyst’s ability to review more documents which should result in more citations and should increase the analyst’s conﬁdence in his recommendations . Table 5 Overall program metrics • Increase quality of analytic product (cid:1) As assessed by a team of senior analysts according to the following criteria : n Covers the important ground n Avoids irrelevant information n Avoids redundant information n Includes selective information n Is well organized • Increase number of citations by 25 % over baseline • Increase analyst conﬁdence in product ( surveys ) (cid:1) Based on a 5 . 0 scale , increase from 2 . 8 ± . 02 to 4 . 0 based on prior study • Increase by 50 % the signal to noise ratio of information presented to the analyst ( Glass Box data ) over baseline J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1201 A reduction in non - relevant documents should facilitate an analyst’s ability to review more relevant documents in less time . Table 6 contains the measures we collected speciﬁc to the analytic process and product . In addition , for each technology inserted , we collected measures speciﬁc to that technology . There is not a one to one mapping between the measures in Table 4 and the program level metrics in Table 5 . While no one project may be able to make a signiﬁcant impact to the overall program metrics , a number of software tech - nologies working together should be able to show an impact . 3 . 2 . Participants The research program was jumpstarted by having a number of open source ana - lysts produce baseline data on process and products ; they worked in a Glass Box - in - strumented environment . These analysts took part in the long - term studies described in Section 2 . 1 . Two of these analysts – Analyst A and Analyst B – were available to perform the technology insertion studies . For each of the insertion studies , we recruited six Naval Reserve analysts . 3 . 3 . Suﬃciently robust instrumented software We were concerned with both the usability and the robustness of the research soft - ware . Analysts needed to use the software for a 2 - week period . If the software was unreliable and crashed numerous times , it would have been diﬃcult for the analysts to form positive impressions of the software . Moreover , if the usability of the Table 6 Measures collected to compute program metrics Analytical reports • Number of citations in report • Breakdown of citations by electronic reference , oﬄine reference , personal • Number of hypotheses explored • Analyst reported conﬁdence in product Glass box data • Number of queries made • Number of queries for which at least one document was viewed • Breakdown of queries into true queries , other • Analysis of queries into similar paths explored • Number of documents viewed • Number of documents saved , printed , bookmarked , copied • Relevance annotations • Collaboration reports • Application usage • Signal to noise calculation Questionnaires • NASA TLX workload • Qualitative observations 1202 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 software was so poor that analysts were unable to easily locate and use capabilities of the software , it would have been diﬃcult for them to understand how the software could positively impact their work . In theory , there were two steps that researchers were required to take prior to hav - ing their software ‘inserted’ . First , the software was subjected to a formative evalu - ation . This evaluation provided both a good usability assessment and the necessary information for developing logging speciﬁcations for the research software . We gen - erated these logging speciﬁcations based on the capabilities of the software and the metrics that were appropriate for the particular software . Second , the software was turned over to independent contractors for robustness testing , including a logging assessment . However , we were working with distributed research teams – not com - mercial software development shops . The result was that much of the software for the technology insertions had only been seen in part or at a much earlier time by our evaluation team . In some instances , we provided a heuristic review prior to the technology insertion to determine if any changes needed to be made to increase the usability of the software . Robustness testing took between 6 and 8 weeks to complete , depending on the complexity of the software being inserted . Testing included installation , stress testing and usability assessments . Software was always tested along with Glass Box software to determine if there were any conﬂicts . During the testing period , the developers were kept in the loop and periodic releases were delivered to ﬁx software bugs . 3 . 4 . Training It was up to the research team to develop the training materials . In many cases , this eﬀort started with the formative evaluations . These evaluations provided feed - back on the training as well as on the actual software . In some instances , the research team trained the analysts participating in the technology insertion . In other cases , a member of the evaluation team provided the training or the training was provided as a self - contained tutorial . In all cases , a senior analyst who had some exposure to the capabilities of the software spent some time with the analyst participants to discuss eﬀective ways to utilize the software . It was necessary to ensure that after a week of training and practiced use , the ana - lysts felt comfortable using the software . A representative from the evaluation team visited the analysts towards the end of the ﬁrst week to check on the analysts and answer questions as necessary about software use . Evaluation staﬀ was always avail - able during the 2 week insertion period to help with ﬁnding answers to questions or solving technology problems . While the initial training was extremely important , it was also necessary to have materials available for the analysts to refer to during the 2 - week period of use . We also encouraged researchers to work with analysts to determine the analytic use of the product . Questions such as the following were asked : • When in the process will this software be most beneﬁcial ? • What types of tasks beneﬁt most from this software ? J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1203 • What additional steps , if any , will the analysts need to take ( or will have to be done beforehand ) to accommodate information the analyst already has ? 3 . 5 . Baseline comparisons During the summer of 2005 , Analyst A and Analyst B were given six 1 - week tasks . These tasks were in two diﬀerent domains and were designed by senior analysts . During the baseline evaluation period , the analysts were asked to give some explicit feedback : ( 1 ) provide annotations concerning relevant documents and oﬄine activities lasting more than ﬁfteen minutes ; ( 2 ) respond to an online debrieﬁng ques - tionnaire at the end of each day ; and ( 3 ) ﬁll out a workload rating using the NASA TLX each day , ( 4 ) rate the diﬃculty of each completed task using an online form that addressed the attributes described in the next section . The debrieﬁng questions asked the analysts to comment on the work they were doing that day , what portion of the analytic process they were in , if they had set goals for that day , if they met those goals , why or why not , what went well that day , what did not go well . While debrieﬁng the analysts in person would have been preferable , this was not feasible given staﬃng constraints . These questionnaires , the Glass Box data , and the analytic reports were used to compute baseline metrics for the research program . Observational studies were also completed ( Scholtz et al . , 2005 ) . 3 . 6 . Analytic tasks When comparing analytic process and products , there are two choices . We can either use the same tasks and diﬀerent analysts or the same analysts and diﬀerent tasks . The research program was set up to use the same analysts throughout . There - fore , senior analysts were called upon to design two sets of tasks , one for use in the baseline study and a comparable set of tasks for use during technology insertions . Tasks needed to have two attributes : ( 1 ) pairs of tasks needed to have an equivalent level diﬃculty between the baseline and the insertion tasks ; and ( 2 ) insertion tasks need to allow analysts to use the features of the technology being inserted to its full - est . For example , if a tool that was to be inserted claimed to exert its inﬂuence during the collection phase , then we wanted a task that would require the analyst to do a lot of searching . On the other hand , if a tool provided assistance during analysis , we wanted a task that depended heavily on analysis and little on collection . While the area of task diﬃculty is a research area in its own right , we developed a rating questionnaire based on literature ( Greitzer and Allwein , 2005 ; Greitzer , 2004 ; Hewett and Scholtz , 2004 ; Shanteau , 1992 ) and discussions with a number of ana - lysts . Tasks were rated for overall diﬃculty on a 10 point scale . Then analysts were asked to classify the task based on a semantic scale appropriate to each of the fol - lowing 11 characteristics : • Prediction versus assessment • Government organizations versus private organizations 1204 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 • Structured versus unstructured problem • Broad versus in - depth analysis • Report length – short versus long • Level of abstraction required – focus on data versus focus on abstraction • Analyst background in domain – little background versus extensive background • Amount of relevant data – little data versus lots of data • Amount of data already collected – little collection required versus lots of collec - tion required • Reliability of data – low reliability versus high reliability • Potential for deceit – low versus high We gave the rating scale to the senior analysts developing the tasks and to the ana - lysts after they had completed the tasks . In an early study with only eight analysts in the sample , a reasonable degree of consistency existed between the rank ordering of the tasks and the diﬃculty ratings gathered on the questionnaire . For example , cor - relating the average diﬃculty rank of each task with the average overall rating of dif - ﬁculty of that task resulted in a correlation coeﬃcient of 0 . 85 , which for only eight pairs of observations is signiﬁcant at the p < 0 . 05 level . The data also suggest that some aspects may be more important for assessing task diﬃculty than others . These aspects include : • Prediction versus assessment • Level of abstraction required • Broad versus in - depth analysis • Potential for deceit • Amount of collection needed We included the level of background of the analyst but it is debatable whether this should be a variable in task diﬃculty or an analyst variable . The most valuable part of this ratings exercise may have been for the analysts designing the tasks . They used the list of characteristics as guidelines to consider when designing a new task and selecting a comparable baseline task . 3 . 7 . Data for use in the tasks The analysts used open source data from the web . In earlier tasks we found they also used other sources such as the Foreign Broadcast Information Service ( FBIS ) and Lexis Nexis . For the baseline and technology insertions , we asked them to refrain from using subscription databases that were not available readily to research - ers and other subjects participating in the technology insertion process . For one technology insertion , we were not able to use live web data . To develop a closed data set , we retrieved the queries posed by the two long - term analysts and fed those to several search engines . The set of documents retrieved was then reﬁned by purging duplicate URLs . This resulted in a set of around 97 , 000 documents . We ran a second baseline for that insertion , using both long - term analysts and Naval reserv - J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1205 ists . It is important to note that , since the document collection was a static , temporal snapshot , tasks that were developed to support the use of this collection needed to be somewhat time - independent . 3 . 8 . Example of a technology insertion analysis In this section , we present baseline data from Analyst A for one task and then present the analysis for one technology insertion . The particular technology inserted was a user interface that allowed the analyst to perform queries , read documents , and assemble information for analysis in an integrated environment . Queries could be saved and re - executed . Results of queries could be automatically compared to see if there were documents in common . Document abstracts could be scanned in a tool - tip - like pop - up to determine if they were relevant and worth a more detailed review . Analysts were able to extract portions of documents and make notes in an area used for analysis . Information from this section could then be moved into a text editor for producing a report . Since the insertion tool supports both information foraging as well as sense - mak - ing , i . e . collection and analysis , we chose tasks ( Table 7 ) that would exercise both types of user activities . In addition , both the baseline task and the insertion task were constructed to be of comparable diﬃculty . Besides Analyst A , we also had two Naval Reserve analysts who completed the same technology insertion task . We have no corresponding baseline data for these analysts , but used their data to supplement that of Analyst A , thus allowing us to determine the variability in the various types of collected data across several subjects . The following sub - sections describe a subset of the measures that we chose to use to characterize the analytic products ( analysts’ reports ) and analytic processes they used . Section 3 . 8 . 1 presents data and analysis of the analytic reports . Section 3 . 8 . 2 drills down on the issue of how analysts framed their information needs as queries Table 7 Sample tasks Baseline T060403 – Cycle 6 – Political Groups in County A Several political groups all reside in Country A . Where are the major personalities within each group ? Who holds seats within Country A’s government ? Who has control over military assets ? Non - governmental militias ? What factions are most likely to have militant confrontations and what would be the most likely cause ? How does this aﬀect relationships with Country B and Country C ? Technology Insertion Task T090403 – Sanctions against County D How have US sanctions against Country D helped or hindered Country D’s foreign policy ? What countries are becoming dependent on Country D’s resources ? Does this hinder US - Country D relations ? How have recent talks between Country D and several other countries concerning access to Country D’s resource against Country D and including them in the WTO in hopes of deterring Country D from a program being run in the country . Would Country D consider ? 1206 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 and measures of that process . Section 3 . 8 . 3 describes document retrieval and rele - vance ratings measures . The next section reports on our use of the NASA TLX as a cognitive workload measurement instrument . Finally , section 3 . 8 . 5 provides an analysis of how analysts spent their time using the various applications during the study period . Although we computed the frequency of use of many of the features of the inserted software , we do not report these in this paper as we are focusing on general metrics of use , not speciﬁc metrics and features of the inserted software . We believe that only through review of a variety of analyses that we obtain a richer picture of the user experience with the software . 3 . 8 . 1 . Analysis of reports We analyzed the reports both quantitatively and qualitatively . We looked at the number of citations given , the number of hypotheses reported and the conﬁdence ratings given by the analysts . Table 8 gives the quantitative statistics computed for the analytic reports . The Glass Box analyst ( Analyst A ) was more conﬁdent of the report written dur - ing the technology insertion than during the baseline . The other two analysts were also very conﬁdent about the quality of their reports . A senior analyst reviewed the reports from the baseline task and compared the report generated by Analyst A during the technology insertion task . He reported that the baseline and test products of the Analyst A showed few diﬀerences . Both reports answered the task very well , both overall and in detail , with no apparent dif - ferences . It should be noted that the number of references used by Analyst A was small in both cases and reﬂected mainly oﬄine sources of knowledge . It appears that analysis in the political - military area relies much more heavily on previous analyses ( books and expert data integration ) . Therefore , a foraging tool like the inserted software would be unlikely to make a signiﬁcant diﬀerence in the assigned task unless it could either : ( 1 ) ﬁnd a ‘‘golden nugget’’ , i . e . , a very high - value document that was missed by normal keyword searching , or ( 2 ) provide a very rapid way to revisit previous analyses for updates . While in theory both of these could have occurred , such a determination could not be made through comparison of the analyst’s products . Table 8 Report statistics from the Technology Insertion Analyst A Analyst 1 Analyst 2 Baseline Tech insertion Tech insertion Tech insertion CitationsTotal 33 29 20 40 Oﬄine 36 0 0 0 URLs 2 4 20 40 Conﬁdence 6 / 10 8 / 10 8 / 10 9 / 10 Number of hypotheses explored 2 2 1 1 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1207 3 . 8 . 2 . Analysis of queries We looked at the number of queries that the analysts made , including those that were productive – that is , led to documents that were viewed . These statistics are giv - en in Table 9 . Although Analyst A submitted fewer queries using Technology Insertion soft - ware than he did using Internet search engines during the baseline task ( 10 vs . 18 ) , he continued to use his normal tools to a signiﬁcant extent during the Technology Insertion period . In fact , he submitted the same number of queries using ordinary search engines in both the baseline and Technology Insertion periods . The other analysts also supplemented the inserted software by submit - ting queries using a standard search engine . It is interesting to note that inserted software was used during the ﬁrst three days of the task and all the queries made using the standard Internet search engines were done on the last two days while the analysts were writing their reports . When questioned about their use of standard Internet search engines , the analysts said that when they wanted to check a fact , ﬁnd a deﬁnition , or verify spelling , the inserted software slowed them down . Two situations could lead to unproductive queries – those for which no docu - ments were returned and those which returned documents that the analyst did not read . Between 70 % and 100 % of the queries made using the inserted software were productive . This range is similar to the value during the baseline . We analyzed the content of the queries that each of the analysts generated to determine the number of analytic paths that each explored . This analysis was completed using a tool developed by a member of the evaluation team . Each analyst showed evidence of following more than one path to create his report . The ratio of the total time spent on task to the number of queries performed is a useful measure for comparing across technologies . Each query that Analyst A did during the baseline supported his work for about an hour ; in contrast , each query that he did with the inserted software supported over 3 h of work . Table 9 Query statistics from the technology insertion Analyst A Analyst 1 Analyst 2 Baseline Tech insertion Tech insertion Tech insertion Queries made using inserted software 10 a 19 a 31 a Standard search engine queries 18 a 18 11 3 Total 18 28 30 34 Productive queries b 14 ( 78 % ) 7 ( 70 % ) 19 ( 100 % ) 29 ( 93 % ) Paths explored ( from query analysis ) 2 3 2 3 Total time ( hh : mm ) 19 : 19 32 : 37 31 : 49 17 : 03 Minutes per query 64 195 100 33 a Cells used as the base for calculation of Productive queries and time on query . b Productive queries are deﬁned as queries where the analyst views one or more returned links . 1208 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 3 . 8 . 3 . Analysis of documents reviewed When analysts used the Tech Insertion software , most of the documents that they worked with came from queries made within that software , but , as mentioned in the previous section , each analyst also found documents through regular Internet searching . Table 10 shows that between 75 % to 90 % of the total documents viewed during the Tech Insertion periods were found through use of the Tech Insertion tool . The deﬁnition of a bookmark was changed from its browser - only connotation to include dragging a document fragment to the analysis area of the tool . Similarly , in the signal - to - noise calculation , ‘saves’ was redeﬁned to include dragging a whole document to the analysis area . The Glass Box analyst ( Analyst A ) accessed nearly four times more unique doc - uments when using Tech Insertion software than during the baseline period . The other analysts read over one hundred documents using the Tech Insertion Document Viewer . Both measures of signal - to - noise ( S / N ) were lower for Analyst A using Tech Insertion software than during the baseline . The S / N values for Analyst 2 are very high compared with values seen in previous insertions and baselines . This analyst did more queries than the other analysts working the task but she accessed very Table 10 Document statistics Analyst A Analyst 1 Analyst 2 Baseline Tech insertion Tech insertion Tech insertion Total number of documents 48 200 258 122 Number of unique documents – Tech insertion – 160 221 112 Number of unique documents – standard internet search engines 45 40 37 10 Number of documents saved to ﬁle system 0 0 0 0 Number of documents printed 5 1 0 0 Number of documents copied from or number of fragments saved to analysis area 13 17 22 13 Number of bookmarks or number of documents dragged to analysis area 0 11 20 31 Number of documents either saved Œ printed Œ copied Œ bookmarked 15 17 26 32 Signal to noise ( marked novel , relevant , maybe relevant ) / unique documents returned by the inserted software 0 . 16 0 . 02 0 . 15 0 . 47 Signal to noise ( saved Œ printed Œ copied Œ bookmarked ) / unique returned by the inserted software 0 . 33 0 . 11 0 . 12 0 . 29 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1209 few of the returned documents . Apparently she used cues provided in the Tech Inser - tion software interface to ﬁlter the document set prior to selecting documents for reading . This analyst also marked relevance for 100 % of the documents that she read ; she even re - marked documents that she had already seen . This was a rare instance of such complete relevance marking ( Table 11 ) . During the baseline , the Analyst A rated eight documents for relevance and seven of those were either novel , relevant , or maybe relevant . In contrast , while using the Tech Insertion software , the analyst marked 5 such documents out of 75 ratings . During the baseline , 88 % of documents were at least ‘maybe relevant’ , while with the Tech Insertion software , only 7 % fell into this category . During the long - term study , Analyst A developed a pattern of marking only documents that he thought were novel , relevant , or maybe relevant . The fact that he marked relevance for doc - uments during the insertion indicates that the mechanism for marking relevance in the technology made it easier for him to comply with the direction to mark relevance . The other analysts marked 89 % and 46 % , respectively , as at least ‘maybe relevant’ . 3 . 8 . 4 . Workload analysis Analysts were asked to complete NASA TLX rating scales at the end of each day . Fig . 2 shows the NASA TLX scores reported for the baseline task and for the Tech Insertion task . All three analysts showed little frustration using the Tech Insertion software after the ﬁrst day leading us to infer that it was relatively easy for them to use at least some portion of the software . Analyst 1 experienced computer diﬃcul - ties on the third day that precluded his ﬁlling in the daily TLX . He also ﬁnished his report on the fourth day and although he returned on the last day to complete other questionnaires , he did no task work on which to base an estimate of cognitive workload . 3 . 8 . 5 . Analysis of time We also looked at how the analysts spent their time . The Glass Box logs which application is in focus at any time . The data in Fig . 3 show how much time on each day was spent with an application in focus . The Glass Box would be in focus when annotations were being made or when a document was being marked for relevance . Table 11 Relevance of documents Analyst A Analyst 1 Analyst 2 Baseline Tech insertion Tech insertion Tech insertion Novel 3 1 1 1 Relevant 4 3 20 37 Maybe relevant 1 12 16 Not relevant 36 4 57 Redundant 1 9 6 Prior knowledge 25 Total rated 8 75 37 117 1210 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 All three analysts used the Technology Insertion software for the ﬁrst four days . During report generation on the last day , all three analysts used an Internet browser for a small portion of the time but did not use the insertion software . All three ana - lysts used Internet Explorer for some period of time during the week ; Analyst1 spent roughly equivalent times using the insertion software and Internet browser . Accord - ing to the daily questionnaires , Analyst A used web search engines to ﬁnd citations , to check facts , and to do simple searches . Analyst1 used his browser to view many documents found in the insertion software , saying it was not as easy to use as a browser . 3 . 9 . Information obtained We obtained information about the use of intelligent software in the intelligence analysis process despite some problems described in the next section . We observed which features analysts used , and which features they do not use . We observed what analysts were able to do with the inserted tool and when they turned to other software to complete their work . The technology insertions provided quantitative comparison data that was helpful in assessing the progress we made in the program overall . The insertions provided insights about intelligent software in actual use . We caution the reader that our observations and conclusions may not generalize as our sample size was small . However , the information provided to the research teams was valuable in helping them formulate research directions . Obtaining statistically valid data is diﬃcult as our user population has extremely limited time . Fig . 2 . Workload comparisons . J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1211 The technology inserted in the sample case presented in this section was an exam - ple of a Human - Information Interaction ( HII ) project . The measures in Table 4 for the Human - Information Interaction research area were collected during the insertion period . Even though the number of subjects was small , we found that this tool did allow analysts to read through more documents . They submitted fewer queries but worked longer with each of the sets of results that were returned . They marked the relevance of more documents even though the fraction of relevant documents was not necessarily higher . The estimates of signal - to - noise were , in general , higher than in any of the baseline studies ( data not shown ) . 3 . 10 . Lessons learned Technology insertions are diﬃcult for software that is more research than devel - opment . For this program , a number of research projects were utilizing the same user interaction environment and this helped considerably . The logging speciﬁcations were primarily implemented from the user interaction side and the Glass Box ana - lysts were able to incrementally learn new capabilities of the software as new soft - ware was inserted into the same environment . For our surrogate analysts , however , the learning curve increased . This was because we used new surrogate par - ticipants for each insertion due to their availability . The robustness testing actually became more of an integration service . Research - ers tended to develop and debug until the last possible minute . During the latter stag - es of the program , multiple research projects were asked to integrate their work . As a result , the actual integration with ﬁnal versions of the software was only completed at the very last minute and as such , very little testing of the delivered software was Fig . 3 . Time usage by application . 1212 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 conducted prior to insertion . The inserted software , therefore , was not as robust as we would have liked . However , the analysts were very patient and learned to cope with software that tended to crash more frequently than typical production software . Lastly , it was diﬃcult for the analysts to develop a good concept of operations using new software . While we provided 1 week of training and practice prior to the week of actual experimentation , this was not suﬃcient time for analysts to devel - op a good mental model of the software functionality and to explore how to incor - porate that functionality into their normal analytic process . 4 . Conclusions and future work We have been successful in identifying metrics and methodologies that have provided feedback both to the software developer researchers and to the program managers . We have worked with many analysts in the intelligence community in the process of deriving the metrics and methodologies . Their input has been invalu - able . We have been able to develop user - centered metrics that focus on the impact to the analyst , in terms of process and product , of intelligent software technologies . These metrics have been developed through interactions with researchers , intelli - gence analysts , and program management . The metrics have been reﬁned through formative evaluations of research software . We are using the technology insertions to determine the impact of the intelligent software on the analysts’ process and products . Perhaps the most compelling measure of success of a new technology is that the analyst continues to use it after the experiment or study is ﬁnished . We hope that an ‘‘open session’’ will be possible where the Glass Box analysts have access to the set of tools inserted and can use them as desired on their tasks . In the process of developing these evaluation methodologies we have identiﬁed several issues . First , we need to explore more thoroughly the notion of task diﬃculty to ensure that comparison tasks are indeed comparable . This is an open area that needs more research . Secondly , we are also working on a more systematic method of evaluating the products of the analysts . We have obtained data from a number of sources , including quality assessment divisions that will be used for developing our assessment criteria . Finally , our studies have been a combination of within - sub - jects ( long - term studies and the insertion studies using the long - term analysts ) and between - subjects ( insertion studies using Naval reservists ) designs . In both cases the number of subjects was small . In order to validate the metrics that we have iden - tiﬁed , we need to test them with larger numbers of users . For each technology insertion , we focused on analyzing the second week of data – data for the comparable task . We would like to also analyze the ﬁrst week of data to determine if there are insights about learning how to use the new tools . In addition , during the ﬁrst week the analysts were asked to work on a task that they already per - formed to see if they found anything new using the technology . We think that this ﬁrst week of data might provide interesting insights into the evolution of the analytic process due to technology use . J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214 1213 We are involved in the evaluation of other programs aimed at research in intelli - gent software . We are eager to determine how our program - wide metrics apply to these new programs and to determine what project - speciﬁc metrics need to be devel - oped given the objectives of the research programs . We are also investigating ways to obtain a larger population for statistically valid data . Until we are able to conduct evaluations with a larger sample we caution that our results may not generalize to other intelligence analysts . Acknowledgements We thank the other members of the evaluation team , Joseph Konczal , Charles Sheppard , Afzal Godil , and Don Libes . Without their programming and analysis skills , this work would not have been possible . We thank the members of the re - search team for working with us in designing , implementing , and analyzing the experiments . Special thanks to the analysts for all their feedback . References CIA Home Page , 2006 . Accessed from < http : / / www . cia . gov / cia / publications / facttell / intelligence _ cy - cle . html > ( accessed June 19 ) . Cowley , P . , Nowell , L . , Scholtz , J . , 2005 . Glass box : an instrumented infrastructure for supporting human interaction with information . In : Hawaii International Conference on System Science 38 ( HICSS38 ) , January 3 – 6 , Kona , Hawaii . Dumas , J . S . , Redish , J . C . , 1999 . A Practical Guide to Usability Testing . Intellect Publishing Co . , Portland , OR , USA . Glass Box web site , < http : / / glassbox . labworks . org / > ( accessed June 19 ) , 2006 . Password required . Greitzer , F . , Allwein , K . , 2005 . Metrics and measures for intelligence analysis task diﬃculty . Panel presented at Intelligence Analysis 2005 ( IA2005 ) . McLean , VA . Greitzer , F . L . , 2004 . Preliminary thoughts on diﬃculty or complexity of intelligence analysis tasks and methodological implications for Glass Box studies ( Unpublished manuscript ) . Grinstein , G . , Kobsa , A . , Plaisant , C . Shneiderman , B . , Stasko , J . T . , 2003 . Which comes ﬁrst , usability or utility ? In : Proceedings of the IEEE Visualization 2003 Conference . pp 11 – 12 . Hart , S . G . , Staveland , L . E . , 1988 . Development of a NASA - TLX ( task load index ) : results of empirical and theoretical research . In : Hancock , P . , Meshkati , N . ( Eds . ) , Human Mental Workload . Amsterdam , North - Holland , pp . 139 – 183 . Heuer , R . , 1999 . Psychology of Intelligence Analysis . Center for the Study of Intelligence CIA . Hewett , T . , Scholtz , J . , 2004 . Towards a metric for task diﬃculty . Paper presented at the NIMD PI Meeting , November 2004 , Orlando , FL . John , B . E . , 1996 . Evaluating usability evaluation techniques . ACM Computing Surveys 28 ( 4es ) . Nielsen , J . , Mack , R . L . ( Eds . ) , 1994 . Usability Inspection Methods . John Wiley & Sons , New York . Scholtz , J . , Morse , E . , Hewett , T . , In depth observational studies of professional intelligence analysts 2005 . In : Proceedings of the International Conference on Intelligence Analysis , May . MacLean , VA . available at < https : / / analysis . mitre . org / > ( accessed June 19 ) , 2006 . Shanteau , J . , 1992 . Competence in experts : the role of task characteristics . Organizational Behavior and Human Decision Processes 53 , 252 – 266 . Theofanos , M . , Quesenbery , W . , 2005 . Towards the design of eﬀective formative test reports . Journal of Usability Studies 1 , 27 – 45 . 1214 J . Scholtz et al . / Interacting with Computers 18 ( 2006 ) 1186 – 1214