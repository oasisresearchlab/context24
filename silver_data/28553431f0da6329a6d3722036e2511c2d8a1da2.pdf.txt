Published as a conference paper at ICLR 2023 T HE I NFLUENCE OF L EARNING R ULE ON R EPRESEN - TATION D YNAMICS IN W IDE N EURAL N ETWORKS Blake Bordelon & Cengiz Pehlevan School of Engineering and Applied Science Harvard University Cambridge , MA 02138 , USA { blake bordelon , cpehlevan } @ g . harvard . edu A BSTRACT It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations . To gain insight into the relationship be - tween learned features , function approximation , and the learning rule , we analyze infinite - width deep networks trained with gradient descent ( GD ) and biologically - plausible alternatives including feedback alignment ( FA ) , direct feedback align - ment ( DFA ) , and error modulated Hebbian learning ( Hebb ) , as well as gated linear networks ( GLN ) . We show that , for each of these learning rules , the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel ( eNTK ) . In the lazy training limit , this eNTK is static and does not evolve , while in the rich mean - field regime this kernel’s evolution can be deter - mined self - consistently with dynamical mean field theory ( DMFT ) . This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules . In the lazy limit , we find that DFA and Hebb can only learn using the last layer features , while full FA can utilize earlier layers with a scale determined by the initial correlation between feedforward and feedback weight matrices . In the rich regime , DFA and FA utilize a temporally evolving and depth - dependent NTK . Counterintuitively , we find that FA networks trained in the rich regime exhibit more feature learning if initialized with smaller correlation between the forward and backward pass weights . GLNs admit a very simple formula for their lazy limit kernel and preserve conditional Gaussianity of their preactivations under gating functions . Error modulated Hebb rules show very small task - relevant alignment of their kernels and perform most task relevant learning in the last layer . 1 I NTRODUCTION Deep neural networks have now attained state of the art performance across a variety of domains including computer vision and natural language processing ( Goodfellow et al . , 2016 ; LeCun et al . , 2015 ) . Central to the power and transferability of neural networks is their ability to flexibly adapt their layer - wise internal representations to the structure of the data distribution during learning . In this paper , we explore how the learning rule that is used to train a deep network affects its learn - ing dynamics and representations . Our primary motivation for studying different rules is that exact gradient descent ( GD ) training with the back - propagation algorithm is thought to be biologically im - plausible ( Crick , 1989 ) . While many alternatives to standard GD training were proposed ( Whitting - ton & Bogacz , 2019 ) , it is unclear how modifying the learning rule changes the functional inductive bias and the learned representations of the network . Further , understanding the learned represen - tations could potentially offer more insight into which learning rules account for representational changes observed in the brain ( Poort et al . , 2015 ; Kriegeskorte & Wei , 2021 ; Schumacher et al . , 2022 ) . Our current study is a step towards these directions . The alternative learning rules we study are error modulated Hebbian learning ( Hebb ) , Feedback alignment ( FA ) ( Lillicrap et al . , 2016 ) and direct feedback alignment ( DFA ) ( Nøkland , 2016 ) . These rules circumvent one of the biologically implausible features of GD : the weights used in the back - ward pass computation of error signals must be dynamically identical to the weights used on the 1 a r X i v : 2210 . 02157v2 [ s t a t . M L ] 25 M a y 2023 Published as a conference paper at ICLR 2023 forward pass , known as the weight transport problem . Instead , FA and DFA algorithms compute an approximate backward pass with independent weights that are frozen through training . Hebb rule only uses a global error signal . While these learning rules do not perform exact GD , they are still able to evolve their internal representations and eventually fit the training data . Further , experiments have shown that FA and DFA can scale to certain problems such as view - synthesis , recommendation systems , and small scale image problems ( Launay et al . , 2020 ) , but they do not perform as well in convolutional architectures with more complex image datasets ( Bartunov et al . , 2018 ) . However , significant improvements to FA can be achieved if the feedback - weights have partial correlation with the feedforward weights ( Xiao et al . , 2018 ; Moskovitz et al . , 2018 ; Boopathy & Fiete , 2022 ) . We also study gated linear networks ( GLNs ) , which use frozen gating functions for nonlinearity ( Fiat et al . , 2019 ) . Variants of these networks have bio - plausible interpretations in terms of dendritic gates ( Sezener et al . , 2021 ) . Fixed gating can mitigate catastrophic forgetting ( Veness et al . , 2021 ; Budden et al . , 2020 ) and enable efficient transfer and multi - task learning Saxe et al . ( 2022 ) . Here , we explore how the choice of learning rule modifies the representations , functional biases and dynamics of deep networks at the infinite width limit , which allows a precise analytical description of the network dynamics in terms of a collection of evolving kernels . At infinite width , the network can operate in the lazy regime , where the feature embeddings at each layer are constant through time , or the rich / feature - learning regime ( Chizat et al . , 2019 ; Yang & Hu , 2021 ; Bordelon & Pehlevan , 2022 ) . The richness is controlled by a scalar parameter related to the initial scale of the output function . In summary , our novel contributions are the following : 1 . We identify a class of learning rules for which function evolution is described by a dynamical effective Neural Tangent Kernel ( eNTK ) . We provide a dynamical mean field theory ( DMFT ) for these learning rules which can be used to compute this eNTK . We show both theoretically and empirically that convergence to this DMFT occurs at large width N with error O ( N − 1 / 2 ) . 2 . We characterize precisely the inductive biases of infinite width networks in the lazy limit by computing their eNTKs at initialization . We generalize FA to allow partial correlation between the feedback weights and initial feedforward weights and show how this alters the eNTK . 3 . We then study the rich regime so that the features are allowed to adapt during training . In this regime , the eNTK is dynamical and we give a DMFT to compute it . For deep linear networks , the DMFT equations close algebraically , while for nonlinear networks we provide a numerical procedure to solve them . 4 . We compare the learned features and dynamics among these rules , analyzing the effect of rich - ness , initial feedback correlation , and depth . We find that rich training enhances gradient - pseudogradient alignment for both FA and DFA . Counterintuitively , smaller initial feedback cor - relation generates more dramatic feature evolution for FA . The GLN networks have dynamics comparable to GD , while Hebb networks , as expected , do not exhibit task relevant adaptation of feature kernels , but rather evolve according to the input statistics . 1 . 1 R ELATED W ORKS GLNs were introduced by Fiat et al . ( 2019 ) as a simplified model of ReLU networks , allowing the analysis of convergence and generalization in the lazy kernel limit . Veness et al . ( 2021 ) provided a simplified and biologically - plausible learning rule for deep GLNs which was extended by Budden et al . ( 2020 ) and provided an interpretation in terms of dendritic gating Sezener et al . ( 2021 ) . These works demonstrated benefits to continual learning due to the fixed gating . Saxe et al . ( 2022 ) derived exact dynamical equations for a GLN with gates operating at each node and each edge of the network graph . Krishnamurthy et al . ( 2022 ) provided a theory of gating in recurrent networks . Lillicrap et al . ( 2016 ) showed that , in a two layer linear network the forward weights will evolve to align to the frozen feedback weights under the FA dynamics , allowing convergence of the network to a loss minimizer . This result was extended to deep networks by Frenkel et al . ( 2019 ) , who also introduced a variant of FA where only the direction of the target is used . Refinetti et al . ( 2021 ) studied DFA in a two - layer student - teacher online learning setup , showing that the network first undergoes an alignment phase before converging to one the degenerate global minima of the loss . They argued that FA’s worse performance in CNNs is due to the inability of the forward pass gradients to align under the block - Toeplitz connectivity strucuture that arises from enforced weight sharing ( d’Ascoli et al . , 2019 ) . Garg & Vempala ( 2022 ) analyzed matrix factorization with FA , 2 Published as a conference paper at ICLR 2023 proving that , when overparameterized , it converges to a minimizer under standard conditions , albeit more slowly than GD . Cao et al . ( 2020 ) analyzed the kernel and loss dynamics of linear networks trained with learning rules from a space that includes GD , contrastive Hebbian , and predictive coding rules , showing strong dependence of hierarchical representations on learning rule . Recent works have utilized DMFT techniques to analyze typical performance of algorithms trained on high - dimensional random data ( Agoritsas et al . , 2018 ; Mignacco et al . , 2020 ; Celentano et al . , 2021 ; Gerbelot et al . , 2022 ) . In the present work , we do not average over random datasets , but rather over initial random weights and treat data as an input to the theory . Wide NNs have been analyzed at infinite width in both lazy regimes with the NTK ( Jacot et al . , 2018 ; Lee et al . , 2019 ) and rich feature learning regimes ( Mei et al . , 2018 ) . In the feature learning limit , the evolution of kernel order parameters have been obtained with both Tensor Programs framework ( Yang & Hu , 2021 ) and with DMFT ( Bordelon & Pehlevan , 2022 ) . Song et al . ( 2021 ) recently analyzed the lazy infinite width limit of two layer networks trained with FA and weight decay , finding that only one layer effectively contributes to the two - layer NTK . Boopathy & Fiete ( 2022 ) proposed alignment based learning rules for networks at large width in the lazy regime , which performs comparably to GD and outperform standard FA . Their Align - Ada rule corresponds to our ρ - FA with ρ = 1 in lazy large width networks . 2 E FFECTIVE N EURAL T ANGENT K ERNEL FOR A L EARNING R ULE We denote the output of a neural network for input x µ ∈ R D as f µ . For concreteness , in the main text we will focus on scalar targets f µ ∈ R and MLP architectures . Other architectures such as multi - class outputs and CNN architectures with infinite channel count can also be analyzed as we show in the Appendix C . For the moment , we let the function be computed recursively from a collection of weight matrices θ = Vec { W 0 , W 1 , . . . , w L } in terms of preactivation vectors h ℓµ ∈ R N where , f µ = 1 γ 0 N w L · ϕ ( h Lµ ) , h ℓ + 1 µ = 1 √ N W ℓ ϕ ( h ℓµ ) , h 1 µ = 1 √ D W 0 x µ ( 1 ) where nonlinearity ϕ is applied element - wise . The scalar parameter γ 0 controls how rich the network training is : small γ 0 corresponds to lazy learning while large γ 0 generates large changes to the features ( Chizat et al . , 2019 ) . For gated linear networks , we follow Fiat et al . ( 2019 ) and modify the forward pass equations by replacing ϕ ( h ℓµ ) with a multiplicative gating function ˙ ϕ ( m ℓµ ) h ℓµ where gating variables m ℓµ = 1 √ D M ℓ x µ are fixed through training with M ij ∼ N ( 0 , 1 ) . To minimize loss L = (cid:80) µ ℓ ( f µ , y µ ) , we consider learning rules to the parameters θ of the form d dt w L = γ 0 (cid:88) µ ϕ ( h Lµ ( t ) ) ∆ µ , d dt W ℓ = γ 0 √ N (cid:88) µ ∆ µ ˜ g ℓ + 1 µ ϕ ( h ℓµ ) ⊤ , d dt W 0 = γ 0 √ D (cid:88) µ ∆ µ ˜ g 1 µ x ⊤ µ ( 2 ) where the error signal is ∆ µ ( t ) = − ∂ L ∂f µ | f µ ( t ) . The last layer weights w L are always updated with their true gradient . This corresponds to the biologically - plausible and local delta - rule , which merely correlates the error signals ∆ µ and the last layer features ϕ ( h Lµ ) ( Widrow & Hoff , 1960 ) . In interme - diate layers , the pseudo - gradient vectors ˜ g ℓµ are determined by the choice of the learning rule . For concreteness , we provide below the recursive definitions of ˜ g ℓ for our five learning rules of interest . ˜ g ℓµ =    ˙ ϕ ( h ℓµ ) ⊙ (cid:104) 1 √ N W ℓ ( t ) ⊤ ˜ g ℓ + 1 µ (cid:105) , ˜ g Lµ = ˙ ϕ ( h Lµ ) ⊙ w L GD ˙ ϕ ( h ℓµ ) ⊙ (cid:20) 1 √ N (cid:16) ρ W ℓ ( 0 ) + (cid:112) 1 − ρ 2 ˜ W ℓ (cid:17) ⊤ ˜ g ℓ + 1 (cid:21) , ˜ W ℓij ∼ N ( 0 , 1 ) ρ - FA ˙ ϕ ( h ℓµ ) ⊙ ˜ z ℓ , ˜ z ℓi ∼ N ( 0 , 1 ) DFA ˙ ϕ ( m ℓ µ ) ⊙ (cid:104) 1 √ N W ℓ ( t ) ⊤ ˜ g ℓ + 1 µ (cid:105) , ˜ g L = ˙ ϕ ( m ℓ µ ) ⊙ w L ( t ) GLN ∆ µ ( t ) ϕ ( h ℓµ ( t ) ) Hebb ( 3 ) While GD uses the instantaneous feedforward weights on the backward pass , ρ - FA uses the weight matrices which do not evolve throughout training . These weights have correlation ρ with the initial forward pass weights W ℓ ( 0 ) . This choice is motivated by the observation that partial correlation between forward and backward pass weights at initialization can improve training ( Liao et al . , 2016 ; 3 Published as a conference paper at ICLR 2023 Xiao et al . , 2018 ; Moskovitz et al . , 2018 ) , though the cost is partial weight transport at initialization . However , we consider partial correlation at initialization more biologically plausible than the demanding weight transport at each step of training , like in GD . For DFA , the weight vectors ˜ z ℓ are sampled randomly at initialization and do not evolve in time . For GLN , the gating variables m ℓµ are frozen through time but the exact feedforward weights are used in the backward pass . Lastly , we modify the classic Hebb rule ( Hebb , 1949 ) to get ∆ W ℓ ∝ (cid:80) µ ∆ µ ( t ) 2 ϕ ( h ℓ + 1 µ ) ϕ ( h ℓµ ) ⊤ , which weighs each example by its current error . Unlike standard Hebbian updates , this learning rule gives stable dynamics without regularization ( App . G ) . For all rules , the evolution of the function is determined by a time - dependent eNTK K µν which is defined as ∂f µ ∂t = ∂f µ ∂ θ · d θ dt = (cid:88) ν ∆ ν K µν ( t , t ) , K µν ( t , s ) = L (cid:88) ℓ = 0 ˜ G ℓ + 1 µν ( t , s ) Φ ℓµν ( t , s ) ˜ G ℓµν ( t , s ) = 1 N g ℓµ ( t ) · ˜ g ℓν ( s ) , Φ ℓµν ( t , s ) = 1 N ϕ ( h ℓµ ( t ) ) · ϕ ( h ℓν ( s ) ) , ( 4 ) where the base cases ˜ G L + 1 µν ( t , s ) = 1 and Φ 0 µν ( t , s ) = 1 D x µ · x ν are time - invariant . The kernel ˜ G ℓ computes an inner product between the true gradient signals g ℓ µ = γ 0 N ∂f µ ∂ h ℓ µ and the pseudo - gradient ˜ g ℓν which is set by the chosen learning rule . We see that because ˜ G ℓ is not necessarily symmetric , K is also not necessarily symmetric . The matrix ˜ G ℓ quantifies pseudo - gradient / gradient alignment . 3 D YNAMICAL M EAN F IELD T HEORY FOR V ARIOUS L EARNING R ULES For each of these learning rules considered , the infinite width N → ∞ limit of network learning can be described by a dynamical mean field theory ( DMFT ) ( Bordelon & Pehlevan , 2022 ) . At infinite width , the dynamics of the kernels Φ ℓ and ˜ G ℓ become deterministic over random Gaussian initialization of parameters θ . The activity of neurons in each layer become i . i . d . random variables drawn from a distribution defined by these kernels , which themselves are averages over these single - site distributions . Below , we provide DMFT formulas which are valid for all of our learning rules h ℓµ ( t ) = u ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 (cid:2) A ℓ − 1 µν ( t , s ) g ℓν ( s ) + C ℓ − 1 µν ( t , s ) ˜ g ℓν ( s ) + Φ ℓ − 1 µν ( t , s ) ∆ ν ( s ) ˜ g ℓν ( s ) (cid:3) z ℓµ ( t ) = r ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 (cid:104) B ℓµν ( t , s ) + ˜ G ℓ + 1 µν ( t , s ) ∆ ν ( s ) (cid:105) ϕ ( h ℓν ( s ) ) , g ℓµ ( t ) = ˙ ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) { u ℓµ ( t ) } ∼ GP ( 0 , Φ ℓ − 1 ) , Φ ℓµν ( t , s ) = (cid:10) ϕ ( h ℓµ ( t ) ) ϕ ( h ℓν ( s ) ) (cid:11) , A ℓµν ( t , s ) = γ − 1 0 (cid:28) δ δr ℓν ( s ) ϕ ( h ℓµ ( t ) ) (cid:29) { r ℓ µ ( t ) } ∼ GP ( 0 , G ℓ + 1 ) , ˜ G ℓ µν ( t , s ) = (cid:10) g ℓ µ ( t ) ˜ g ℓ ν ( s ) (cid:11) , B ℓ µν ( t , s ) = γ − 1 0 (cid:28) δ δu ℓ + 1 ν ( s ) g ℓ + 1 µ ( t ) (cid:29) ( 5 ) The definitions of ˜ g ℓµ ( t ) depend on the learning rule and are described in Table 1 . The z ℓµ ( t ) is the pre - gradient field defined so that g ℓµ ( t ) = ˙ ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) . The dependence of these DMFT equations on data comes from the base case Φ 0 µν ( t , s ) = 1 D x µ · x ν and error signal ∆ µ = − ∂ L ∂f µ . Rule GD ρ - FA DFA GLN Hebb ˜ g ℓµ ( t ) ˙ ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) ˙ ϕ ( h ℓµ ( t ) ) ˜ z ℓµ ( t ) ˙ ϕ ( h ℓµ ( t ) ) ˜ z ℓ ˙ ϕ ( m ℓµ ) z ℓµ ( t ) ∆ µ ( t ) ϕ ( h ℓµ ( t ) ) Table 1 : The field definitions for each learning rule . For ρ - FA , the field has definition ˜ z ℓ µ ( t ) = ρv ℓµ ( t ) + (cid:112) 1 − ρ 2 ˜ ζ ℓµ ( t ) + γ 0 (cid:82) t 0 ds (cid:80) ν D ℓµν ( t , s ) ϕ ( h ℓν ( s ) ) where { v ℓµ ( t ) , ˜ ζ ℓµ ( t ) } are Gaussian with (cid:10) r ℓµ ( t ) v ℓν ( s ) (cid:11) = ˜ G ℓ + 1 µν ( t , s ) . The ˜ ζ ℓ field is an independent Gaussian with correlation (cid:68) ˜ ζ ℓµ ( t ) ˜ ζ ℓν ( s ) (cid:69) = (cid:10) ˜ g ℓ + 1 µ ( t ) ˜ g ℓ + 1 ν ( s ) (cid:11) = ˜˜ G ℓ + 1 µν ( t , s ) . For DFA , the ˜ z ℓ field is static ˜ z ℓ ∼ N ( 0 , 1 ) . For GLN , we use { m ℓµ } ∼ N ( 0 , K x ) as a gating variable . C ℓ = 0 except for ρ - FA with ρ > 0 . 4 Published as a conference paper at ICLR 2023 We see that , for { GD , ρ - FA , DFA , Hebb } the distribution of h ℓµ ( t ) , z ℓµ ( t ) are Gaussian throughout training only in the lazy γ 0 → 0 limit for general nonlinear activation functions ϕ ( h ) . However , conditional on { m ℓµ } , the { h ℓ , z ℓ } fields are all Gaussian for GLNs . For all algorithms except ρ - FA , C ℓ = 0 . For ρ - FA we have C ℓµα ( t , s ) = γ − 1 0 (cid:68) δ δv ℓν ( s ) ϕ ( h ℓµ ( t ) ) (cid:69) . 0 100 200 300 400 500 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t GDFAGLNHebbDMFT ( a ) Loss Dynamics 0 100 200 300 400 500 t 0 . 2 0 . 3 0 . 4 0 . 5 A ( K , yy ) ( b ) NTK - Target Alignment 0 100 200 300 400 500 t 0 1 2 3 4 T r G ( c ) ˜ G Dynamics 4 2 0 2 4 h 10 2 10 1 10 0 p ( h ) ( d ) Final h Distributions NN E x p t . BP FA GLN Hebb D M F T ( e ) Final Φ Kernels G NN E x p t . BP FA GLN Hebb G D M F T ( f ) Final ˜ G kernels Figure 1 : The DMFT predicts feature dynamics of wide networks trained with gradient descent ( GD ) , feedback alignment ( FA ) with ρ = 0 , gated linear network ( GLN ) , and a error - modulated β = 1 Hebb rule ( Hebb ) in the feature learning regime . ( a ) The loss dynamics in a two layer ( L = 1 , N = 2000 ) network trained with these learning rules at richness γ 0 = 2 . The network is trained on a collection of P = 10 random vectors in D = 50 dimensions . ( b ) The cosine similarity of the eNTK with the targets A ( K , yy ⊤ ) = y ⊤ Ky | K | F | y | 2 reveals increasing alignment for all algorithms . Though FA starts with the lowest alignment , its final NTK task alignment exceeds that of GD . ( c ) The dynamics of the gradient - pseudogradient kernel ˜ G also reveals increasing correlation of g with ˜ g . FA starts with ˜ G = 0 but ˜ G increases to non - zero value . ( d ) The distribution of hidden layer preactivations after training reveals non - Gaussian statistics for both GD and FA , but approximately Gaussian statistics for GLN . ( e ) - ( f ) The final Φ and ˜ G kernels from theory and experiment . As described in prior results on the GD case ( Bordelon & Pehlevan , 2022 ) , the above equations can be solved self - consistently in polynomial ( in train - set size P and training steps T ) time . With an estimate of the dynamical kernels { Φ ℓµν ( t , s ) , ˜ G ℓµν ( t , s ) , G ℓµν ( t , s ) } , one computes the eNTK K µν ( t ) and error dynamics ∆ µ ( t ) . From these objects , we can sample the stochastic processes { h ℓ , z ℓ , ˜ z ℓ } which can then be used to derive new refined estimates of the kernels . This procedure is repeated until convergence . This algorithm can be found in App . A . An example of such a solution is provided in Figure 1 for two layer ReLU networks trained with GD , FA , GLN , and Hebb . We show that our self - consistent DMFT accurately predicts training and kernel dynamics , as well as the density of preactivations { h µ ( t ) } and final kernels { Φ µν , ˜ G µν } for each learning rule . We observe substantial differences in the learned representations ( Figure 1e ) , all predicted by our DMFT . 3 . 1 L AZY OR E ARLY T IME S TATIC - K ERNEL L IMITS When γ 0 → 0 , we see that the fields h ℓµ ( t ) and z ℓµ ( t ) are equal to the Gaussian variables u ℓµ ( 0 ) and r ℓ µ ( 0 ) . In this limit , the eNTK K µν remains static and has the form summarized in Table 2 in terms of the initial feature kernels Φ ℓ and gradient kernels G ℓ . We derive these kernels in Appendix D . The feature P × P matrices Φ ℓ , G ℓ in Table 2 are computed recursively as Φ ℓ = (cid:10) ϕ ( u ) ϕ ( u ) ⊤ (cid:11) u ∼N ( 0 , Φ ℓ − 1 ) , G ℓ = G ℓ + 1 ⊙ (cid:68) ˙ ϕ ( u ) ˙ ϕ ( u ) ⊤ (cid:69) u ∼N ( 0 , Φ ℓ − 1 ) ( 6 ) 5 Published as a conference paper at ICLR 2023 3 2 1 0 1 2 3 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 K ( ) FA L = 4 = 0 . 00 = 0 . 25 = 0 . 50 = 1 . 00 ( a ) ReLU FA varying ρ 3 2 1 0 1 2 3 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 K ( ) FA = 0 . 5 L = 1 L = 2 L = 3 L = 4 ( b ) ReLU FA varying L 3 2 1 0 1 2 3 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 K ( ) GLN L = 1 L = 2 L = 3 L = 4 ( c ) ReLU GLN varying L 10 1 10 2 10 3 N 10 3 10 2 10 1 10 0 | N | 2 / | | 2 N 1 1 2 3 4 ( d ) Φ ℓ convergence 10 1 10 2 10 3 N 10 3 10 2 10 1 10 0 10 1 | G N G | 2 / | G | 2 N 1 G 1 G 2 G 3 G 4 ( e ) G ℓ convergence 10 1 10 2 10 3 N 10 3 10 2 10 1 10 0 10 1 | K N K | 2 / | K | 2 L = 2 , = 0 . 0 L = 2 , = 1 . 0 L = 4 L = 8 ( f ) eNTK convergence Figure 2 : The lazy infinite width limits of the various learning rules can be fully summarized with their initial eNTK . ( a ) The kernels of ρ - aligned ReLU FA and ReLU GLN for inputs separated by angle θ . ( a ) The kernels for varying ρ in ρ - aligned FA . Larger ρ has a sharper peak in the kernel around θ = 0 . The ρ → 0 limit recovers the NNGP kernel Φ L while the ρ → 1 limit gives the back - prop NTK . ( b ) Deeper networks with partial alignment ρ = 0 . 5 . ( c ) ReLU - GLN kernel sharpens with depth . ( d ) - ( e ) The relative error of the infinite width Φ ℓ , G ℓ kernels in a width N ReLU neural network . The late layer Φ ℓ and early layer G ℓ kernels have highest errors since finite size effects accumulate on forward and backward passes respectively . ( f ) Finite width corrections to eNTK are larger for small ρ and large depth L . All square errors go as | K N − K ∞ | 2 ∼ O N ( 1 / N ) . Rule GD ρ - FA DFA GLN Hebb K µν (cid:80) Lℓ = 0 G ℓ + 1 µν Φ ℓµν (cid:80) Lℓ = 0 ρ L − ℓ G ℓ + 1 µν Φ ℓµν Φ Lµν (cid:104)(cid:68) ˙ ϕ ( m µ ) ˙ ϕ ( m ν ) (cid:69)(cid:105) L K xµν Φ Lµν Table 2 : The initial eNTK K µν for each learning rule . The GD kernel is the usual initial NTK of Jacot et al . ( 2018 ) . For ρ - aligned FA , each layer ℓ ’s contribution to the eNTK is suppressed by a factor ρ L − ℓ . For DFA and Hebb , only the last layer feature kernel Φ L contributes to the NTK . For GLN , each layer has an identical contribution . with base cases Φ 0 = K x and G L + 1 = 11 ⊤ . We provide interpretations of this result below . • Backpropagation ( GD ) and ρ = 1 FA recover the usual depth L NTK , with contributions from every layer K µν = (cid:80) ℓ G ℓ + 1 µν Φ ℓµν at initialization . This kernel governs both training dynamics and test predictions in the lazy limit γ 0 → 0 ( Jacot et al . , 2018 ; Lou et al . , 2022 ) . • ρ = 0 FA , DFA and Hebb are equivalent to using the NNGP kernel K µν ∼ Φ Lµν , giving the Bayes posterior mean ( Matthews et al . , 2018 ; Lee et al . , 2018 ; Hron et al . , 2020 ) . In the γ 0 , ρ → 0 limit , only the dynamics of the readout weights w L contribute to the evolution of f µ since error signals cannot successfully propagate backward and gradients cannot align with pseudo - gradients ( App D ) . The standard ρ = 0 FA will be indistinguishable from merely training w L with the delta - rule unless the network is trained in the rich feature learning regime γ 0 > 0 , where ˜ G ℓ can evolve . This effect was also noted in two layer networks by Song et al . ( 2021 ) . • ρ - FA weighs each layer ℓ with scale ρ L − ℓ , since each layer’s pseudo - gradient is only partially correlated with the true gradient , giving recursion ˜ G ℓ = ρ ˜ G ℓ + 1 with base case ˜ G L + 1 = G L + 1 . • GLN’s kernel in lazy limit is determined by the Gaussian gating variables { m ℓµ } ∼ N ( 0 , K x ) . We visualize these kernels for deep ReLU networks and ReLU GLNs for normalized inputs | x | 2 = | x ′ | 2 = D , by plotting the kernel as a function of the angle θ separating two inputs cos ( θ ) = 6 Published as a conference paper at ICLR 2023 1 D x ⊤ x ′ . We find that the kernels develop a sharp discontinuity at the origin θ = 0 , which becomes more exaggerated as ρ and L increase . We further show that the square difference of width N kenels and infinite width kernels go as O ( N − 1 ) . We derive this scaling with a perturbative argument in App . H , which enables analytical prediction of leading order finite size effects ( Figure 7 ) . In the lazy γ 0 → 0 limit , these kernels define the eNTK and the network prediction dynamics . 3 . 2 F EATURE L EARNING E NABLES G RADIENT / P SEUDO - GRADIENT A LIGNMENT AND K ERNEL / T ASK A LIGNMENT In the last section , we saw that , in the γ 0 → 0 limit , all algorithms have frozen preactivations and pregradient features { h ℓµ ( t ) , z ℓµ ( t ) } . A consequence of this fact is that FA and DFA cannot increase their gradient - pseudogradient alignment throughout training in the lazy limit γ 0 = 0 . However , if we increase γ 0 , then the gradient features g ℓµ ( t ) and pseudo - gradients ˜ g ℓµ ( t ) evolve in time and can increase their alignment . In Figure 3 , we show the effect of increasing γ 0 on alignment dynamics in a depth 4 tanh network trained with DFA . In ( b ) , we see that larger γ 0 is associated with high task - alignment of the last layer feature kernel Φ L , which becomes essentially rank one and aligned to yy ⊤ . The asympotic cosine similarity between gradients and pseudogradients also increase with γ 0 . The eNTK also becomes aligned with the task relevant directions ( shown in Figure 3 c ) , like has been observed in GD training ( Baratin et al . , 2021 ; Shan & Bordelon , 2021 ; Geiger et al . , 2021 ; Atanasov et al . , 2022 ) . We see that width N networks have a dynamical eNTK K N ( t ) which deviates from the DMFT eNTK K ∞ ( t ) by O ( 1 / N ) in square loss . DMFT is more predictive for larger γ 0 networks , suggesting a reduction in finite size variability due to task - relevant feature evolution . 0 50 100 150 200 250 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t 0 = 0 . 2 0 = 0 . 5 0 = 1 . 0 0 = 2 . 0 ( a ) DFA Train Loss 0 50 100 150 200 250 t 0 . 4 0 . 6 0 . 8 1 . 0 A ( L , yy ) ( b ) Φ L Alignment 0 50 100 150 200 250 t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 C o rr ( g , g ) ( c ) g ℓ , ˜ g ℓ Correlation K ( T ) E x p t 0 = 0 . 2 K ( T ) D M F T 0 = 0 . 5 0 = 1 . 0 0 = 2 . 0 ( d ) Final NTK Aligns to Task 10 1 10 2 10 3 N 10 3 10 2 10 1 10 0 | K N K | 2 / | K | 2 0 = 0 . 2 0 = 0 . 5 0 = 1 . 0 0 = 2 . 0 N 1 ( e ) Dynamical NTK Convergence Figure 3 : Feature Learning enables alignment for a depth 4 ( L = 3 hidden layers ) tanh network trained with direct feedback alignment ( DFA ) with varying γ 0 . ( a ) Training loss for DFA networks with width N = 4000 with varying richness γ 0 shows that feature learning accelerates training , as predicted by DMFT ( black ) . ( b ) The alignment ( cosine similarity ) of the last layer kernel Φ L with the target function reveals successful task depedent feature learning at large γ 0 . ( c ) The dy - namics of pseudo - grad . / grad . correlation corr ( g , ˜ g ) = 1 LP (cid:80) ℓ , µ g ℓµ ( t ) · ˜ g ℓµ ( t ) | g ℓµ ( t ) | | ˜ g ℓµ ( t ) | averaged over lay - ers ℓ and datapoints µ . Larger γ 0 generates more significant alignment between pseudogradients and gradients . ( d ) The final NTKs as a function of γ 0 reveals increasing clustering of the data points by class . ( e ) The error of the DMFT approximation for K ’s dynamics as a function of N : ⟨ | K N ( t ) − K ∞ ( t ) | 2 ⟩ t ⟨ | K ∞ ( t ) | 2 ⟩ t ∼ O ( N − 1 ) , where the averages are computed over the time interval of training . This error is smaller for larger feature learning strength γ 0 . 7 Published as a conference paper at ICLR 2023 3 . 3 D EEP L INEAR N ETWORK K ERNEL D YNAMICS When γ 0 > 0 the kernels and features in the network evolve according to the DMFT equations . For deep linear networks we can analyze the equations for the kernels in closed form without sampling since the correlation functions close algebraically ( App . E ) . In Figure 4 , we utilize our algebraic DMFT equations to explore ρ - FA dynamics in a depth 4 linear network . Networks with larger ρ train faster , which can be intuited by noting that the initial function time derivative dfdt | t = 0 ∼ (cid:80) Lℓ = 0 ρ L − ℓ ∼ 1 − ρ L + 1 1 − ρ is an increasing function of ρ . We observe higher final gradient pseudogradient alignment in each layer with larger ρ , which is also intuitive from the initial condi - tion ˜ G ℓ ( 0 ) = ρ L − ℓ . However , surprisingly , for large initial correlation ρ , the NTK achieves lower task alignment , despite having larger ˜ G ℓ ( t ) . We show that this is caused by smaller overlap of each layer’s feature kernel H ℓ ( t ) with yy ⊤ . Though this phenomenon is counterintuitive , we gain more insight in the next section by studying an even simpler two layer model . 0 100 200 300 400 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 T r a i n L o ss = 0 . 00 = 0 . 25 = 0 . 50 = 0 . 75 ( a ) ρ - Aligned Loss Dynamics 0 100 200 300 400 0 2 4 6 8 G ( t ) = 1 = 0 . 00 = 0 . 25 = 0 . 50 = 0 . 75 0 100 200 300 400 0 1 2 3 4 5 = 2 0 100 200 300 400 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 = 3 ( b ) Gradient - Pseudogradient Kernel Dynamics 0 100 200 300 400 t 0 . 2 0 . 3 0 . 4 0 . 5 A ( K , yy ) = 0 . 00 = 0 . 25 = 0 . 50 = 0 . 75 ( c ) NTK - Task Alignment 0 100 200 300 400 0 . 6 0 . 8 1 . 0 1 . 2 1 . 4 1 . 6 y H ( t ) y = 1 = 0 . 00 = 0 . 25 = 0 . 50 = 0 . 75 0 100 200 300 400 1 2 3 4 = 2 0 100 200 300 400 0 5 10 15 = 3 ( d ) Feature Kernel Task Overlap Figure 4 : The initial feedback correlation ρ alters alignment dynamics in on the FA dynamics in a depth 4 ( L = 3 hidden layer ) linear network . ( a ) Larger ρ leads to faster initial training since the scale of the eNTK is larger . ( b ) Further , larger ρ leads to larger scales of ˜ G ( t ) = 1 N g ℓ ( t ) · ˜ g ℓ ( t ) . ( c ) However , smaller ρ leads to more alignment of the NTK K ( t ) with the task - relevant subspace , measured with cosine similarity A ( K , yy ⊤ ) . ( d ) The feature kernel H ( t ) overlaps with y reveal that H ℓ ( t ) aligns more significantly in the small ρ networks . 3 . 3 . 1 E XACTLY S OLVEABLE D YNAMICS IN T WO L AYER L INEAR N ETWORK We can provide exact solutions to the infinite width GD and ρ - FA dynamics in the setting of Saxe et al . ( 2013 ) , specifically a two layer linear network trained with whitened data K xµν = δ µν . Unlike Saxe et al . ( 2013 ) ’s result , however , we do not demand small initialization scale ( or equivalently large γ 0 ) , but rather provide the exact solution for all positive γ 0 . We will establish that large initial correlation ρ results in higher gradient / pseudogradient alignment but lower alignment of the hidden feature kernel H ( t ) with the task relevant subspace yy ⊤ . We first note that when K x = I , the GD or FA hidden feature kernel H ( t ) only evolves in the rank - one yy ⊤ subspace . It thus suffices to track the projection of H ( t ) on this rank one subspace , which we call H y ( t ) . In the App . F we derive dynamics for H y for GD and ρ - FA H y ( t ) = (cid:40) ˜ G ( t ) = (cid:112) 1 + γ 20 ( y − ∆ ( t ) ) 2 , d ∆ dt = − (cid:112) 1 + γ 20 ( y − ∆ ( t ) ) 2 ∆ ( t ) GD 2 ˜ G ( t ) + 1 − 2 ρ = 1 + a 2 , da dt = γ 0 y − 12 a 3 − ( 1 + ρ ) a ρ - FA ( 7 ) We illustrate these dynamics in Figure 5 . The fixed points are H y = (cid:112) 1 + γ 20 y 2 for GD and for ρ - FA , H y = 1 + a 2 where a is the smallest positive root of 12 a 3 + ( 1 + ρ ) a = γ 0 y . For both GD and FA , we see that increasing γ 0 results in larger asymptotic values for H y and ˜ G . For ρ - FA the fixed point of a ’s dynamics is a strictly decreasing function of ρ since da dρ < 0 , showing that the final 8 Published as a conference paper at ICLR 2023 value of H y is smaller for larger ρ . On the contrary , we have that the final ˜ G = ρ + 12 a 2 is a strictly increasing function of ρ since ddρ ˜ G = 1 − a 2 32 a 2 + ( 1 + ρ ) > 13 > 0 . Thus , this simple model replicates the phenomenon of increasing ˜ G and decreasing H y as ρ increases . For the Hebb rule with K x = I , the story is different . Instead of aligning H along the rank - one task relevant subpace , the dynamics instead decouple over samples , giving the following P separate equations d dt ∆ µ = − [ H µµ ( t ) + γ 0 ∆ µ ( y µ − ∆ µ ) ] ∆ µ ( t ) , d dtH µµ = 2 γ 0 ∆ µ ( t ) 2 H µµ . ( 8 ) From this perspective , we see that the hidden feature kernel does not align to the task , but rather increases its entries in overall scale as is illustrated in Figure 5 ( b ) . 0 200 400 600 800 1000 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t GDFA = 0 FA = 1 DMFTHebb ( a ) Loss Dynamics 0 200 400 600 800 1000 t 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 H y ( t ) / T r H ( t ) ( b ) Kernel - Task Alignment 10 1 10 0 10 1 10 2 0 10 3 10 2 10 1 10 0 10 1 10 2 10 3 H y 2 / 3020 0 ( c ) Feature Learning vs γ 0 Figure 5 : The feature kernel dynamics and scaling with γ 20 for GD , ρ - FA , and Hebbian rules in an exactly solveable two layer linear network . ( a ) The loss dynamics for all algorithms reveals that ρ = 0 FA and Hebb rules have same early time dynamics and that ρ = 1 FA and GD have same early - time dynamics . However all loss curves become distinct at late times due to different eNTK dynamics . ( b ) The alignment of the kernel to the target function H y ( t ) = 1 | y | 2 y ⊤ Hy / Tr H ( t ) increases significantly for GD , and FA , but not for Hebb , reflecting the task - independence of the learned representation . ( c ) The movement of the feature kernel ∆ H y = lim t →∞ H y ( t ) − H y ( 0 ) as a function of γ 0 for GD , and ρ = 0 , 1 FA . At small feature learning strength , all algorithms have updates on the order of ∆ H y ∼ γ 20 . At large γ 0 , GD has ∆ H y ∼ γ 0 while FA has ∆ H y ∼ γ 2 / 3 0 . The ρ = 1 FA ( green ) has lower ∆ H y than the ρ = 0 FA across all γ 0 . 4 D ISCUSSION We provided an analysis of the training dynamics of a wide range of learning rules at infinite width . This set of rules includes ( but is not limited to ) GD , ρ - FA , DFA , GLN and Hebb as well as many others . We showed that each of these learning rules has an dynamical effective NTK which concentrates over initializations at infinite width . In the lazy γ 0 → 0 regime , it suffices to compute the the initial NTK , while in the rich regime , we provide a dynamical mean field theory to compute the NTK’s dynamics . We showed that , in the rich regime , FA learning rules do indeed align the network’s gradient vectors to their pseudo - gradients and that this alignment improves with γ 0 . We show that initial correlation ρ between forward and backward pass weights alters the inductive bias of FA in both lazy and rich regimes . In the rich regime , larger ρ networks have smaller eNTK evolution . Overall , our study is a step towards understanding learned representations in neural networks , and the quest to reverse - engineer learning rules from observations of evolving neural representations during learning in the brain . Many open problems remain unresolved with the present work . We currently have only implemented our theory in MLPs . An implementation in CNNs could explain some of the observed advantages of partial initial alignment in ρ - FA ( Xiao et al . , 2018 ; Moskovitz et al . , 2018 ; Bartunov et al . , 2018 ; Refinetti et al . , 2021 ) . In addition , our framework is sufficiently flexible to propose and test new learning rules by providing new ˜ g ℓµ ( t ) formulas . Our DMFT gives a recipe to compute their initial kernels , function dynamics and analyze their learned representations . The generalization perfor - mance of these learning rules at varying γ 0 is yet to be explored . Lastly , our DMFT is numerically expensive for large datasets and training intervals , making it difficult to scale up to realistic datsets . Future work could provide theoretical convergence guarantees for our DMFT solver . 9 Published as a conference paper at ICLR 2023 R EFERENCES Elisabeth Agoritsas , Giulio Biroli , Pierfrancesco Urbani , and Francesco Zamponi . Out - of - equilibrium dynamical mean - field equations for the perceptron model . Journal of Physics A : Mathematical and Theoretical , 51 ( 8 ) : 085002 , 2018 . Alexander Atanasov , Blake Bordelon , and Cengiz Pehlevan . Neural networks as kernel learners : The silent alignment effect . In International Conference on Learning Representations , 2022 . URL https : / / openreview . net / forum ? id = 1NvflqAdoom . Aristide Baratin , Thomas George , C´esar Laurent , R Devon Hjelm , Guillaume Lajoie , Pascal Vin - cent , and Simon Lacoste - Julien . Implicit regularization via neural feature alignment . In Arindam Banerjee and Kenji Fukumizu ( eds . ) , Proceedings of The 24th International Conference on Arti - ficial Intelligence and Statistics , volume 130 of Proceedings of Machine Learning Research , pp . 2269 – 2277 . PMLR , 13 – 15 Apr 2021 . URL https : / / proceedings . mlr . press / v130 / baratin21a . html . Sergey Bartunov , Adam Santoro , Blake Richards , Luke Marris , Geoffrey E Hinton , and Timothy Lillicrap . Assessing the scalability of biologically - motivated deep learning algorithms and archi - tectures . Advances in neural information processing systems , 31 , 2018 . Carl M Bender , Steven Orszag , and Steven A Orszag . Advanced mathematical methods for scientists and engineers I : Asymptotic methods and perturbation theory , volume 1 . Springer Science & Business Media , 1999 . Akhilan Boopathy and Ila Fiete . How to train your wide neural network without backprop : An input - weight alignment perspective . In Kamalika Chaudhuri , Stefanie Jegelka , Le Song , Csaba Szepesvari , Gang Niu , and Sivan Sabato ( eds . ) , Proceedings of the 39th International Confer - ence on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp . 2178 – 2205 . PMLR , 17 – 23 Jul 2022 . URL https : / / proceedings . mlr . press / v162 / boopathy22a . html . Blake Bordelon and Cengiz Pehlevan . Self - consistent dynamical field theory of kernel evo - lution in wide neural networks . In Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( eds . ) , Advances in Neural Information Processing Systems , 2022 . URL https : / / openreview . net / forum ? id = sipwrPCrIS . David Budden , Adam Marblestone , Eren Sezener , Tor Lattimore , Gregory Wayne , and Joel Veness . Gaussian gated linear networks . Advances in Neural Information Processing Systems , 33 : 16508 – 16519 , 2020 . Yinan Cao , Christopher Summerfield , and Andrew Saxe . Characterizing emergent representations in a space of candidate learning rules for deep networks . In H . Larochelle , M . Ranzato , R . Hadsell , M . F . Balcan , and H . Lin ( eds . ) , Advances in Neural Information Processing Systems , volume 33 , pp . 8660 – 8670 . Curran Associates , Inc . , 2020 . URL https : / / proceedings . neurips . cc / paper / 2020 / file / 6275d7071d005260ab9d0766d6df1145 - Paper . pdf . Michael Celentano , Chen Cheng , and Andrea Montanari . The high - dimensional asymptotics of first order methods with random data . arXiv preprint arXiv : 2112 . 07572 , 2021 . Lenaic Chizat , Edouard Oyallon , and Francis Bach . On lazy training in differentiable programming . Advances in Neural Information Processing Systems , 32 , 2019 . Francis Crick . The recent excitement about neural networks . Nature , 337 ( 6203 ) : 129 – 132 , 1989 . A Crisanti and H Sompolinsky . Path integral approach to random neural networks . Physical Review E , 98 ( 6 ) : 062120 , 2018 . St´ephane d’Ascoli , Levent Sagun , Giulio Biroli , and Joan Bruna . Finding the needle in the haystack with convolutions : on the benefits of architectural bias . Advances in Neural Information Process - ing Systems , 32 , 2019 . Jonathan Fiat , Eran Malach , and Shai Shalev - Shwartz . Decoupling gating from linearity . arXiv preprint arXiv : 1906 . 05032 , 2019 . 10 Published as a conference paper at ICLR 2023 Charlotte Frenkel , Martin Lefebvre , and David Bol . Learning without feedback : direct random target projection as a feedback - alignment algorithm with layerwise feedforward training . arXiv preprint arXiv : 1909 . 01311 , 10 , 2019 . Shivam Garg and Santosh Vempala . How and when random feedback works : A case study of low - rank matrix factorization . In Gustau Camps - Valls , Francisco J . R . Ruiz , and Isabel Valera ( eds . ) , Proceedings of The 25th International Conference on Artificial Intelligence and Statistics , volume 151 of Proceedings of Machine Learning Research , pp . 4070 – 4108 . PMLR , 28 – 30 Mar 2022 . URL https : / / proceedings . mlr . press / v151 / garg22a . html . Mario Geiger , Leonardo Petrini , and Matthieu Wyart . Landscape and training regimes in deep learning . Physics Reports , 924 : 1 – 18 , 2021 . Cedric Gerbelot , Emanuele Troiani , Francesca Mignacco , Florent Krzakala , and Lenka Zdeborova . Rigorous dynamical mean field theory for stochastic gradient descent methods , 2022 . URL https : / / arxiv . org / abs / 2210 . 06591 . Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . Ian Goodfellow , Yoshua Bengio , Aaron Courville , and Yoshua Bengio . Deep learning , volume 1 . MIT Press , 2016 . Donald O . Hebb . The organization of behavior : A neuropsychological theory . Wiley , New York , June 1949 . ISBN 0 - 8058 - 4300 - 0 . Jiri Hron , Yasaman Bahri , Roman Novak , Jeffrey Pennington , and Jascha Sohl - Dickstein . Exact posterior distributions of wide bayesian neural networks . arXiv preprint arXiv : 2006 . 10541 , 2020 . Arthur Jacot , Franck Gabriel , and Cl ´ ement Hongler . Neural tangent kernel : Convergence and gen - eralization in neural networks . Advances in neural information processing systems , 31 , 2018 . Mehran Kardar . Statistical physics of fields . Cambridge University Press , 2007 . Nikolaus Kriegeskorte and Xue - Xin Wei . Neural tuning and representational geometry . Nature Reviews Neuroscience , 22 ( 11 ) : 703 – 718 , 2021 . Kamesh Krishnamurthy , Tankut Can , and David J Schwab . Theory of gating in recurrent neural networks . Physical Review X , 12 ( 1 ) : 011011 , 2022 . Julien Launay , Iacopo Poli , Franc¸ois Boniface , and Florent Krzakala . Direct feedback alignment scales to modern deep learning tasks and architectures . Advances in neural information processing systems , 33 : 9346 – 9360 , 2020 . Yann LeCun , Yoshua Bengio , and Geoffrey Hinton . Deep learning . nature , 521 ( 7553 ) : 436 – 444 , 2015 . Jaehoon Lee , Jascha Sohl - dickstein , Jeffrey Pennington , Roman Novak , Sam Schoenholz , and Yasaman Bahri . Deep neural networks as gaussian processes . In International Confer - ence on Learning Representations , 2018 . URL https : / / openreview . net / forum ? id = B1EA - M - 0Z . Jaehoon Lee , Lechao Xiao , Samuel Schoenholz , Yasaman Bahri , Roman Novak , Jascha Sohl - Dickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . Advances in neural information processing systems , 32 , 2019 . Jaehoon Lee , Samuel Schoenholz , Jeffrey Pennington , Ben Adlam , Lechao Xiao , Roman Novak , and Jascha Sohl - Dickstein . Finite versus infinite neural networks : an empirical study . Advances in Neural Information Processing Systems , 33 : 15156 – 15172 , 2020 . Qianli Liao , Joel Leibo , and Tomaso Poggio . How important is weight symmetry in backpropaga - tion ? In Proceedings of the AAAI Conference on Artificial Intelligence , volume 30 , 2016 . Timothy P Lillicrap , Daniel Cownden , Douglas B Tweed , and Colin J Akerman . Random synaptic feedback weights support error backpropagation for deep learning . Nature communications , 7 ( 1 ) : 1 – 10 , 2016 . 11 Published as a conference paper at ICLR 2023 Yizhang Lou , Chris E Mingard , and Soufiane Hayou . Feature learning and signal propagation in deep neural networks . In Kamalika Chaudhuri , Stefanie Jegelka , Le Song , Csaba Szepesvari , Gang Niu , and Sivan Sabato ( eds . ) , Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp . 14248 – 14282 . PMLR , 17 – 23 Jul 2022 . URL https : / / proceedings . mlr . press / v162 / lou22a . html . Alessandro Manacorda , Gr´egory Schehr , and Francesco Zamponi . Numerical solution of the dy - namical mean field theory of infinite - dimensional equilibrium liquids . The Journal of chemical physics , 152 ( 16 ) : 164506 , 2020 . Paul Cecil Martin , ED Siggia , and HA Rose . Statistical dynamics of classical systems . Physical Review A , 8 ( 1 ) : 423 , 1973 . Alexander G . D . G . Matthews , Jiri Hron , Mark Rowland , Richard E . Turner , and Zoubin Ghahramani . Gaussian process behaviour in wide deep neural networks . In International Conference on Learn - ing Representations , 2018 . URL https : / / openreview . net / forum ? id = H1 - nGgWC - . Song Mei , Andrea Montanari , and Phan - Minh Nguyen . A mean field view of the landscape of two - layer neural networks . Proceedings of the National Academy of Sciences , 115 ( 33 ) : E7665 – E7671 , 2018 . Francesca Mignacco , Florent Krzakala , Pierfrancesco Urbani , and Lenka Zdeborov´a . Dynamical mean - field theory for stochastic gradient descent in gaussian mixture classification . Advances in Neural Information Processing Systems , 33 : 9540 – 9550 , 2020 . Theodore H Moskovitz , Ashok Litwin - Kumar , and LF Abbott . Feedback alignment in deep convo - lutional networks . arXiv preprint arXiv : 1812 . 06488 , 2018 . Arild Nøkland . Direct feedback alignment provides learning in deep neural networks . Advances in neural information processing systems , 29 , 2016 . Jasper Poort , Adil G Khan , Marius Pachitariu , Abdellatif Nemri , Ivana Orsolic , Julija Krupic , Mar - ius Bauza , Maneesh Sahani , Georg B Keller , Thomas D Mrsic - Flogel , et al . Learning enhances sensory and multiple non - sensory representations in primary visual cortex . Neuron , 86 ( 6 ) : 1478 – 1490 , 2015 . Maria Refinetti , St ´ ephane d’Ascoli , Ruben Ohana , and Sebastian Goldt . Align , then memorise : the dynamics of learning with feedback alignment . In International Conference on Machine Learning , pp . 8925 – 8935 . PMLR , 2021 . Andrew Saxe , Shagun Sodhani , and Sam Jay Lewallen . The neural race reduction : Dynamics of abstraction in gated networks . In International Conference on Machine Learning , pp . 19287 – 19309 . PMLR , 2022 . Andrew M Saxe , James L McClelland , and Surya Ganguli . Exact solutions to the nonlinear dynam - ics of learning in deep linear neural networks . arXiv preprint arXiv : 1312 . 6120 , 2013 . Joseph W Schumacher , Matthew K McCann , Katherine J Maximov , and David Fitzpatrick . Selective enhancement of neural coding in v1 underlies fine - discrimination learning in tree shrew . Current Biology , 32 ( 15 ) : 3245 – 3260 , 2022 . Eren Sezener , Agnieszka Grabska - Barwi´nska , Dimitar Kostadinov , Maxime Beau , Sanjukta Krish - nagopal , David Budden , Marcus Hutter , Joel Veness , Matthew Botvinick , Claudia Clopath , et al . A rapid and efficient learning rule for biological neural circuits . BioRxiv , 2021 . Haozhe Shan and Blake Bordelon . A theory of neural tangent kernel alignment and its influence on training . arXiv e - prints , pp . arXiv – 2105 , 2021 . Ganlin Song , Ruitu Xu , and John Lafferty . Convergence and alignment of gradient descent with ran - dom backpropagation weights . Advances in Neural Information Processing Systems , 34 : 19888 – 19898 , 2021 . 12 Published as a conference paper at ICLR 2023 Joel Veness , Tor Lattimore , David Budden , Avishkar Bhoopchand , Christopher Mattern , Agnieszka Grabska - Barwinska , Eren Sezener , Jianan Wang , Peter Toth , Simon Schmitt , et al . Gated linear networks . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35 , pp . 10015 – 10023 , 2021 . James CR Whittington and Rafal Bogacz . Theories of error back - propagation in the brain . Trends in cognitive sciences , 23 ( 3 ) : 235 – 250 , 2019 . Bernard Widrow and Marcian E Hoff . Adaptive switching circuits . Technical report , Stanford Univ Ca Stanford Electronics Labs , 1960 . Will Xiao , Honglin Chen , Qianli Liao , and Tomaso Poggio . Biologically - plausible learning algo - rithms can scale to large datasets . arXiv preprint arXiv : 1811 . 03567 , 2018 . Greg Yang and Edward J . Hu . Tensor programs iv : Feature learning in infinite - width neural net - works . In Marina Meila and Tong Zhang ( eds . ) , Proceedings of the 38th International Con - ference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp . 11727 – 11737 . PMLR , 18 – 24 Jul 2021 . URL https : / / proceedings . mlr . press / v139 / yang21c . html . 13 Published as a conference paper at ICLR 2023 A PPENDIX A A LGORITHM TO S OLVE N ONLINEAR DMFT E QUATIONS Algorithm 1 : Alternating Monte - Carlo Solution to Saddle Point Equations Data : K x , y , Initial Guesses { Φ ℓ , G ℓ , ˜ G ℓ , ˜˜ G ℓ } Lℓ = 1 , { A ℓ , B ℓ , C ℓ , D ℓ } L − 1 ℓ = 1 , Sample count S , Update Speed β Result : Network predictions through training f µ ( t ) , correlation functions { Φ ℓ , G ℓ , ˜ G ℓ , ˜˜ G ℓ } Lℓ = 1 , response functions { A ℓ , B ℓ , C ℓ , D ℓ } L − 1 ℓ = 1 , 1 Φ 0 = K x ⊗ 11 ⊤ , G L + 1 = 11 ⊤ ; 2 while Kernels Not Converged do 3 From { Φ ℓ , G ℓ } compute K NTK ( t , t ) and solve ddt f µ ( t ) = (cid:80) α ∆ α ( t ) K NTKµα ( t , t ) ; 4 ℓ = 1 ; 5 while ℓ < L + 1 do 6 Draw S samples { u ℓµ , n ( t ) } S n = 1 ∼ GP ( 0 , Φ ℓ − 1 ) , { r ℓµ , n ( t ) , v ℓµ , n ( t ) } S n = 1 ∼ GP (cid:32) 0 , (cid:34) G ℓ + 1 ˜ G ℓ + 1 ˜ G ℓ + 1 ⊤ ˜˜ G ℓ + 1 (cid:35)(cid:33) ; 7 Solve equation 5 for each sample to get { h ℓµ , n ( t ) , z ℓµ , n ( t ) , ˜ g ℓµ , n ( t ) } S n = 1 ; 8 Use learning rule ( Table 1 ) to compute { ˜ g ℓµ , n ( t ) } S n = 1 ; 9 Compute new correlation function { Φ ℓ , G ℓ , ˜ G ℓ , ˜˜ G ℓ } estimates : 10 Φ ℓ , newµν ( t , s ) = 1 S (cid:80) n ∈ [ S ] ϕ ( h ℓµ , n ( t ) ) ϕ ( h ℓν , n ( s ) ) , 11 G ℓ , newµν ( t , s ) = 1 S (cid:80) n ∈ [ S ] g ℓµ , n ( t ) g ℓν , n ( s ) , 12 ˜ G ℓ , newµν ( t , s ) = 1 S (cid:80) n ∈ [ S ] g ℓµ , n ( t ) ˜ g ℓν , n ( s ) , 13 ˜˜ G ℓ , newµν ( t , s ) = 1 S (cid:80) n ∈ [ S ] ˜ g ℓµ , n ( t ) ˜ g ℓν , n ( s ) ; 14 Solve for Jacobians on each sample ∂ϕ ( h ℓn ) ∂ r ℓ ⊤ n , ∂ϕ ( h ℓn ) ∂ v ℓ ⊤ n , ∂ g ℓn ∂ u ℓ ⊤ n , ∂ ˜ g ℓn ∂ u ℓ ⊤ n ; 15 Compute new response functions { A ℓ , B ℓ − 1 , C ℓ , D ℓ − 1 } estimates : 16 A ℓ , new = 1 S (cid:80) n ∈ [ S ] ∂ϕ ( h ℓn ) ∂ r ℓ ⊤ n , B ℓ − 1 , new = 1 S (cid:80) n ∈ [ S ] ∂ g ℓn ∂ u ℓ ⊤ n ; 17 C ℓ , new = 1 S (cid:80) n ∈ [ S ] ∂ϕ ( h ℓn ) ∂ v ℓ ⊤ n , D ℓ − 1 , new = 1 S (cid:80) n ∈ [ S ] ∂ ˜ g ℓn ∂ u ℓ ⊤ n ; 18 ℓ ← ℓ + 1 ; 19 end 20 ℓ = 1 ; 21 while ℓ < L + 1 do 22 Update correlation functions 23 Φ ℓ ← ( 1 − β ) Φ ℓ + β Φ ℓ , new , G ℓ ← ( 1 − β ) G ℓ + β G ℓ , new ; 24 ˜ G ℓ ← ( 1 − β ) ˜ G ℓ + β ˜ G ℓ , new , ˜˜ G ℓ ← ( 1 − β ) ˜˜ G ℓ + β ˜˜ G ℓ , new ; 25 if ℓ < L then 26 Update response functions 27 A ℓ ← ( 1 − β ) A ℓ + β A ℓ , new , B ℓ ← ( 1 − β ) B ℓ + β B ℓ , new 28 C ℓ ← ( 1 − β ) C ℓ + β C ℓ , new , D ℓ ← ( 1 − β ) D ℓ + β D ℓ , new 29 end 30 ℓ ← ℓ + 1 31 end 32 end 33 return { f µ ( t ) } Pµ = 1 , { Φ ℓ , G ℓ , ˜ G ℓ , ˜ ˜ G ℓ } Lℓ = 1 , { A ℓ , B ℓ , C ℓ , D ℓ } L − 1 ℓ = 1 The sample - and - solve procedure we developed and describe below for nonlinear networks is based on numerical recipes used in the dynamical mean field simulations in computational physics Man - acorda et al . ( 2020 ) and is similar to recent work in the GD case Bordelon & Pehlevan ( 2022 ) . The basic principle is to leverage the fact that , conditional on order parameters , we can easily draw 14 Published as a conference paper at ICLR 2023 samples { u ℓµ ( t ) , r ℓµ ( t ) , ζ ℓµ ( t ) , ˜ ζ ℓµ ( t ) } from their appropriate GPs . From these sampled fields , we can identify the kernel order parameters by simple estimation of the appropriate moments . The algorithm is provided in Algorithm 1 . The parameter β controls recency weighting of the samples obtained at each iteration . If β = 1 , then the rank of the kernel estimates is limited to the number of samples S used in a single iteration , but with β < 1 smaller sample sizes S can be used to still obtain accurate results . We used β = 0 . 6 in our deep network experiments . B D ERIVATION OF DMFT E QUATIONS In this section , we derive the DMFT description of infinite network dynamics . The path integral theory we develop is based on the Martin - Siggia - Rose - De Dominicis - Janssen ( MSRDJ ) framework Martin et al . ( 1973 ) . A useful review of this technique applied to random recurent networks can be found here Crisanti & Sompolinsky ( 2018 ) . This framework was recently extended for deep learning with GD in ( Bordelon & Pehlevan , 2022 ) . B . 1 W RITING E VOLUTION E QUATIONS IN F EATURE S PACE First , we will express all of the learning dynamics in terms of preactivation features h ℓµ ( t ) = 1 √ N W ℓ ( t ) ϕ ( h ℓµ ( t ) ) , pre - gradient features z ℓµ ( t ) = 1 √ N W ℓ ( t ) ⊤ g ℓ + 1 and pseudogradient features ˜ g ℓµ ( t ) . Since we would like to understand typical behavior over random initializations of weights θ ( 0 ) = { W 0 ( 0 ) , W 1 ( 0 ) , . . . , w L ( 0 ) } , we want to isolate the dependence of our evolution equations by W ℓ ( 0 ) . We achieve this separation by using our learning dynamics for W ℓ ( t ) W ℓ ( t ) = W ℓ ( 0 ) + γ 0 √ N (cid:90) t 0 ds P (cid:88) µ = 1 ∆ µ ( s ) ˜ g ℓ + 1 µ ( s ) ϕ ( h ℓµ ( s ) ) ⊤ . ( 9 ) The inclusion of the prefactor γ 0 √ N in the weight dynamics ensures that ddt f = O γ 0 , N ( 1 ) and ddt h ℓ = O γ 0 , N ( γ 0 ) at initialization ( Chizat et al . , 2019 ; Bordelon & Pehlevan , 2022 ) . Using the forward and backward pass equations , we find the following evolution equations for our feature vectors h ℓµ ( t ) = χ ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 ∆ ν ( s ) ˜ g ℓ + 1 µ ( s ) Φ ℓ − 1 µν ( t , s ) , χ ℓµ ( t ) = 1 √ N W ℓ ( 0 ) ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) = ξ ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 ∆ ν ( s ) ϕ ( h ℓµ ( s ) ) ˜ G ℓ + 1 µν ( t , s ) , ξ ℓµ ( t ) = 1 √ N W ℓ ( 0 ) ⊤ g ℓ + 1 µ ( t ) , ( 10 ) where we introduced the following feature and gradient / pseudo - gradient kernels Φ ℓµν ( t , s ) = 1 N ϕ ( h ℓ µ ( t ) ) · ϕ ( h ℓ ν ( s ) ) , ˜ G ℓ µν ( t , s ) = 1 N g ℓ µ ( t ) · ˜ g ℓ ν ( s ) . ( 11 ) The particular learning rule defines the definition of the pseudo - gradient ˜ g ℓµ ( t ) . We note that , for all learning rules considered , the pseudogradient ˜ g ℓi , µ ( t ) is a function of the fields { h ℓi , µ ( t ) , z ℓiµ ( t ) , m ℓiµ ( t ) , ζ ℓi , µ ( t ) , ˜ ζ ℓi , µ ( t ) } µ ∈ [ P ] , t ∈ R + , conditional on the value of the kernels { Φ ℓ , ˜ G ℓ } . The additional fields have definitions ζ ℓµ ( t ) = 1 √ N W ℓ ( 0 ) ⊤ ˜ g ℓ + 1 µ ( t ) , ˜ ζ ℓ + 1 µ ( t ) = 1 √ N ˜ W ℓ ⊤ ˜ g ℓ + 1 µ ( t ) ( 12 ) and are specifically required for ρ - FA with ρ > 0 since ˜ g ℓµ ( t ) = ρ ˙ ϕ ( h ℓµ ( t ) ) ⊙ ζ ℓµ ( t ) + (cid:112) 1 − ρ 2 ˙ ϕ ( h ℓ µ ( t ) ) ⊙ ˜ ζ ℓµ ( t ) . The fields m ℓ µ = 1 √ D M ℓ x µ are required for GLNs . All of the necessary fields { h ℓµ ( t ) , z ℓµ ( t ) , ˜ g ℓµ ( t ) } are thus causal functions of the stochastic fields { χ ℓµ ( t ) , ξ ℓµ ( t ) , m ℓµ , ζ ℓµ ( t ) , ˜ ζ ℓµ ( t ) } and the kernels { Φ ℓ , ˜ G ℓ } . It thus suffices to characterize the dis - tribution of these latter objects over random initialization of θ ( 0 ) in the N → ∞ limit , which we study in the next section . 15 Published as a conference paper at ICLR 2023 B . 2 M OMENT G ENERATING F UNCTIONAL We will now attempt to characterize the probability density of the random fields χ ℓ + 1 µ ( t ) = 1 √ N W ℓ ( 0 ) ϕ ( h ℓµ ( t ) ) , ξ ℓµ ( t ) = 1 √ N W ℓ ( 0 ) ⊤ g ℓ + 1 µ ( t ) , m ℓµ = 1 √ D M ℓ x µ ζ ℓµ ( t ) = 1 √ N W ℓ ( 0 ) ⊤ ˜ g ℓ + 1 µ ( t ) , ˜ ζ ℓµ ( t ) = 1 √ N ˜ W ℓ ⊤ ˜ g ℓ + 1 µ ( t ) . ( 13 ) It is readily apparent that the fields m ℓµ are independent of the others and have a Gaussian distribu - tion over random Gaussian M ℓ . These fields , therefore do not can be handled independently from the others , which are statistically coupled through the initial conditions . We will thus characterize the moment generating functional of the remaining fields { χ ℓµ ( t ) , ξ ℓµ ( t ) , ζ ℓµ ( t ) , ˜ ζ ℓµ ( t ) } over random initial condition and random backward pass weights Z [ { j ℓµ ( t ) , k ℓµ ( t ) , n ℓµ ( t ) , p ℓµ ( t ) } ] = E θ ( 0 ) , { ˜ W ℓ } exp (cid:32) P (cid:88) µ = 1 (cid:90) ∞ 0 dt (cid:104) j ℓ µ ( t ) · χ ℓ µ ( t ) + k ℓ µ ( t ) · ξ ℓ µ ( t ) + n ℓ µ ( t ) · ζ ℓ µ ( t ) + p ℓ µ ( t ) · ˜ ζ ℓ µ ( t ) (cid:105)(cid:33) ( 14 ) where χ ℓ , ξ , ζ , ˜ ζ are regarded as functions of θ ( 0 ) , { ˜ W ℓ } . Arbitrary moments of these random variables can be computed by differentiation of Z near zero source . For example , a two - point correlation function can be obtained as (cid:68) χ ℓi , µ ( t ) ζ ℓ ′ i ′ , ν ( s ) (cid:69) = lim j , k , n , p → 0 δ δj ℓi , µ ( t ) δ δn ℓ ′ i ′ ν ( s ) Z [ { j ℓµ ( t ) , k ℓµ ( t ) , n ℓµ ( t ) , p ℓµ ( t ) } ] . ( 15 ) More generally , we let µ = ( i , µ , t ) be a tuple containing the neuron , time , and sample index for an entry of one of these fields so that χ ℓ µ = χ ℓi , µ ( t ) . Further , we let N χ ℓ , N ξ ℓ , N ζ ℓ , N ˜ ζ ℓ be index sets which contain sample and time indices as well as neuron indices N χ = { µ χ 1 , . . . , | µ χ | N χ | } for all of the indices we wish to compute an average over . Then arbitrary moments can be computed with the formula (cid:42)(cid:89) ℓ   (cid:89) µ ∈N χℓ χ ℓ µ (cid:89) ν ∈N ξℓ ξ ℓ ν (cid:89) α ∈N ζℓ ζ ℓ α (cid:89) β ∈N ξℓ ˜ ζ ℓ β   (cid:43) = lim j , k , n , p → 0 (cid:89) ℓ   (cid:89) µ ∈N χℓ δ δj ℓ µ (cid:89) ν ∈N ξℓ δ δk ℓ ν (cid:89) α ∈N ζℓ δ δn ℓ α (cid:89) β ∈N ξℓ δ δp ℓ µ   Z [ { j ℓµ ( t ) , k ℓµ ( t ) , n ℓµ ( t ) , p ℓµ ( t ) } ] . ( 16 ) We now to study this moment generating functional Z in the large width N → ∞ limit . B . 3 P ATH I NTEGRAL F ORMULATION AND I NTEGRATION OVER W EIGHTS To enable the average over the weights , we multiply Z by an integral representation of unity that enforces the relationship between χ ℓ + 1 µ ( t ) , W ℓ ( 0 ) , ϕ ( h ℓµ ( t ) ) 1 = (cid:90) R N d χ ℓ + 1 µ ( t ) δ (cid:18) χ ℓ + 1 µ ( t ) − 1 √ N W ℓ ( 0 ) ϕ ( h ℓµ ( t ) ) (cid:19) = (cid:90) R N (cid:90) R N d χ ℓ + 1 µ ( t ) d ˆ χ ℓ + 1 µ ( t ) ( 2 π ) N exp (cid:18) i ˆ χ ℓ + 1 µ ( t ) · (cid:20) χ ℓ + 1 µ ( t ) − 1 √ N W ℓ ( 0 ) ϕ ( h ℓµ ( t ) ) (cid:21)(cid:19) . ( 17 ) In the second line , we used the Fourier representation of the Dirac - Delta function for each of the N neuron indices δ ( r ) = (cid:82) ∞ −∞ d ˆ r 2 π exp ( i ˆ rr ) . We repeat this procedure for the other fields ξ ℓµ ( t ) , ζ ℓµ ( t ) , ˜ ζ ℓµ ( t ) at each time t and each sample µ . After inserting these delta functions , we find 16 Published as a conference paper at ICLR 2023 the following form of the moment generating functional Z = (cid:90) (cid:89) ℓµt d χ ℓµ ( t ) d ˆ χ ℓµ ( t ) ( 2 π ) N d ξ ℓµ ( t ) d ˆ ξ ℓµ ( t ) ( 2 π ) N d ζ ℓµ ( t ) d ˆ ζ ℓµ ( t ) ( 2 π ) N d ˜ ζ ℓµ ( t ) d ˆ˜ ζ ℓµ ( t ) ( 2 π ) N × exp  (cid:90) ∞ 0 dt (cid:88) ℓ , µ (cid:104) χ ℓµ ( t ) · ( j ℓµ ( t ) + i ˆ χ ℓµ ( t ) ) + ξ ℓµ ( t ) · ( k ℓµ ( t ) + i ˆ ξ ℓµ ( t ) ) (cid:105) × exp  (cid:90) ∞ 0 dt (cid:88) ℓ , µ (cid:104) ζ ℓµ ( t ) · ( n ℓµ ( t ) + i ˆ ζ ℓµ ( t ) ) + ˜ ζ ℓµ ( t ) · ( p ℓµ ( t ) + i ˆ˜ ζ ℓµ ( t ) ) (cid:105) × (cid:89) ℓ E W ℓ ( 0 ) exp (cid:32) − i √ N Tr W ℓ ( 0 ) ⊤ (cid:34)(cid:90) dt (cid:88) µ ˆ χ ℓ + 1 µ ( t ) ϕ ( h ℓµ ( t ) ) ⊤ + g ℓ + 1 µ ( t ) ˆ ξ ℓµ ( t ) ⊤ (cid:35)(cid:33) × exp (cid:32) − i √ N W ℓ ( 0 ) ⊤ (cid:34)(cid:90) dt (cid:88) µ ˜ g ℓ + 1 µ ( t ) ζ ℓµ ( t ) ⊤ (cid:35)(cid:33) × (cid:89) ℓ E ˜ W ℓ exp (cid:32) − i √ N Tr ˜ W ℓ ⊤ (cid:34)(cid:90) dt (cid:88) µ ˜ g ℓ + 1 µ ( t ) ˆ˜ ζ ℓµ ( t ) ⊤ (cid:35)(cid:33) . ( 18 ) We see that we often have simultaneous integrals over time t and sums over samples µ so we will again adopt a shorthand notation for indices µ = ( µ , t ) and define a summmation convention (cid:80) µ a µ b µ = (cid:82) ∞ 0 dt (cid:80) Pµ = 1 a µ ( t ) b µ ( t ) . To perform the averages over weights , we note that for a standard normal variable W ij , that E W ij exp ( iW ij a i b j ) = exp (cid:0) − 12 a 2 i b 2 i (cid:1) . Using this fact for each of the weight matrix averages , we have E W ℓ ( 0 ) exp (cid:32) − i √ N Tr W ℓ ( 0 ) ⊤ (cid:34)(cid:88) µ ˆ χ ℓ + 1 µ ϕ ( h ℓ µ ) ⊤ + g ℓ + 1 µ ˆ ξ ℓ ⊤ µ + ˜ g ℓ + 1 µ ˆ ζ ℓ ⊤ µ (cid:35)(cid:33) = exp   − 1 2 N (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88) µ ˆ χ ℓ + 1 µ ϕ ( h ℓ µ ) ⊤ + g ℓ + 1 µ ˆ ξ ℓ ⊤ µ + ˜ g ℓ + 1 µ ˆ ζ ℓ ⊤ µ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 F   = exp (cid:32) − 1 2 (cid:88) µ , ν (cid:104) ˆ χ ℓ + 1 µ · ˆ χ ℓ + 1 ν Φ ℓ µ , ν + ˆ ξ ℓ µ · ˆ ξ ℓ ν G ℓ + 1 µν + ˆ ζ ℓ µ · ˆ ζ ℓ ν ˜˜ G ℓ + 1 µ , ν + ˆ ξ ℓ µ · ˆ ζ ℓ ν ˜ G ℓ + 1 µ , ν (cid:105)(cid:33) × exp (cid:32) − i (cid:88) µν (cid:2) ˆ χ ℓ + 1 µ · g ℓ + 1 ν A ℓ µν + ˆ χ ℓ + 1 µ · ˜ g ℓ + 1 ν C ℓ µν (cid:3)(cid:33) . ( 19 ) In the above , we introduced a collection of order parameters { Φ , G , ˜ G , ˜˜ G , A , C } , which will corre - spond to correlation and response functions of our DMFT . These are defined as Φ ℓ µ , ν = 1 N ϕ ( h ℓ µ ) · ϕ ( h ℓ ν ) , G ℓ µ , ν = 1 N g ℓ µ · g ℓ ν , ˜ G ℓ µν = 1 N g ℓ µ · ˜ g ℓ ν ˜˜ G ℓ + 1 µ , ν = 1 N ˜ g ℓ µ · ˜ g ℓ ν , iA ℓ µν = 1 N ϕ ( h ℓ µ ) · ˆ ξ ℓ ν , iC ℓ µν = 1 N ϕ ( h ℓ µ ) · ˆ ζ ℓ ν . ( 20 ) We perform a similar average over ˜ W ℓ can be obtained directly E ˜ W ℓ exp (cid:32) − i √ N Tr ˜ W ℓ ⊤ (cid:34)(cid:88) µ ˜ g ℓ + 1 µ ˆ˜ ζ ℓ ⊤ µ (cid:35)(cid:33) = exp (cid:32) − 1 2 (cid:88) µν ˆ˜ ζ ℓ µ · ˆ˜ ζ ℓ ν ˜˜ G ℓ + 1 µν (cid:33) . ( 21 ) Now that we have defined our collection of order parameters , we enforce their definitions with Dirac - Delta functions by multiplying by one . For example , 1 = N (cid:90) d Φ ℓ µν δ (cid:0) N Φ ℓ µν − ϕ ( h ℓ µ ) · ϕ ( h ℓ ν ) (cid:1) = (cid:90) d Φ ℓ µν d ˆΦ ℓ µν 2 πN − 1 exp (cid:16) N ˆΦ ℓ µν Φ ℓ µν − ˆΦ ℓ µν ϕ ( h ℓ µ ) · ϕ ( h ℓ ν ) (cid:17) . ( 22 ) 17 Published as a conference paper at ICLR 2023 We enforce these definitions for all order parameters { Φ ℓ µν , G ℓ µν , ˜ G ℓ µν , ˜˜ G ℓ µν , A ℓ µ , ν , C ℓ µ , ν } . We let the corresponding Fourier duals for each of these order parameters be { ˆΦ ℓ µν , ˆ G ℓ µν , ˆ˜ G ℓ µν , ˆ˜˜ G ℓ µν , − B ℓ µ , ν , − D ℓ µ , ν } . In the next section we show the resulting formula for the moment generating function and take the N → ∞ limit to derive our DMFT equations . B . 4 DMFT A CTION After inserting the Dirac - Delta functions to enforce the definitions of the order pa - rameters , we derive the following moment generating functional in terms of q = { Φ , ˆΦ , G , ˆ G , ˜ G , ˆ˜ G , ˜˜ G , ˆ˜˜ G , A , B , C , D , j , k , n , p } Z = (cid:90) (cid:89) ℓ , µ , ν d Φ ℓ µν d ˆΦ ℓ µν 2 πN − 1 dG ℓ µν d ˆ G ℓ µν 2 πN − 1 d ˜ G ℓ µν d ˆ˜ G ℓ µν 2 πN − 1 d ˜˜ G ℓ µν d ˆ˜˜ G ℓ µν 2 πN − 1 dA ℓ µν dB ℓ µν 2 πN − 1 dC ℓ µν dD ℓ µν 2 πN − 1 exp ( NS [ q ] ) where S [ q ] is the O N ( 1 ) DMFT action which takes the form S [ q ] = (cid:88) ℓ µν (cid:20) Φ ℓ µ , ν ˆΦ ℓ µ , ν + G ℓ µν ˆ G ℓ µν + ˜ G ℓ µν ˆ˜ G ℓ µν + ˜˜ G ℓ µν ˆ˜˜ G ℓ µν − A ℓ µν B ℓ µν − C ℓ µν D ℓ µ ν (cid:21) + 1 N N (cid:88) i = 1 L (cid:88) ℓ = 1 ln Z ℓi [ q ] . ( 23 ) The single - site moment generating functionals ( MGF ) Z ℓi involve only the integrals with sources { j ℓi , k ℓi , n ℓi , p ℓi } for neuron i ∈ [ N ] in layer ℓ . For a given set of order parameters q at zero source , these functionals become identical across all neuron sites i . Concretely , for any ℓ ∈ [ L ] , i ∈ [ N ] , the single site MGF takes the form Z ℓi = (cid:90) (cid:89) µ dχ ℓ µ d ˆ χ ℓ µ 2 π dξ ℓ µ d ˆ ξ ℓ µ 2 π dζ ℓ µ d ˆ ζ ℓ µ 2 π d ˜ ζ ℓ µ d ˆ˜ ζ ℓ µ 2 π ( 24 ) exp (cid:32) − 1 2 (cid:88) µ , ν (cid:104) ˆ χ ℓ + 1 µ ˆ χ ℓ + 1 ν Φ ℓ µ , ν + ˆ ξ ℓ µ ˆ ξ ℓ ν G ℓ + 1 µν + ˆ˜ ζ ℓ µ ˆ˜ ζ ℓ ν ˜˜ G ℓ + 1 µν (cid:105)(cid:33) exp (cid:32) − 1 2 (cid:88) µν (cid:104) ˆ ζ ℓ µ ˆ ζ ℓ ν ˜˜ G ℓ + 1 µ , ν + 2ˆ ξ ℓ µ ˆ ζ ℓ ν ˜ G ℓ + 1 µ , ν (cid:105)(cid:33) exp (cid:32) − (cid:88) µν (cid:20) ˆΦ ℓ µν ϕ ( h ℓ µ ) ϕ ( h ℓ ν ) + ˆ G ℓ µν g ℓ µ g ℓ ν + ˆ˜ G ℓ µν g ℓ µ ˜ g ℓ ν + ˆ˜˜ G ℓ µν ˜ g ℓ µ ˜ g ℓ ν (cid:21)(cid:33) exp (cid:32) − i (cid:88) µν (cid:104) ˆ χ ℓ + 1 µ g ℓ + 1 ν A ℓ µν + ˆ χ ℓ + 1 µ ˜ g ℓ + 1 ν C ℓ µν + ϕ ( h ℓ µ ) ˆ ξ ℓ ν B ℓ µν + ϕ ( h ℓ µ ) ˆ ζ ℓ ν D ℓ µν (cid:105)(cid:33) exp (cid:32)(cid:88) µ (cid:104) χ ℓ µ ( j ℓi , µ + i ˆ χ ℓ µ ) + ξ ℓ µ ( k ℓi , µ + i ˆ ξ ℓ µ ) + ζ ℓ µ ( n ℓi , µ + i ˆ ζ ℓ µ ) + ˜ ζ ℓ µ ( p ℓi , µ + i ˆ˜ ζ ℓ µ ) (cid:105)(cid:33) . As promised , the only terms in Z i which vary over site index i are the sources { j , k , n , p } . To simplify our later saddle point equations , we will abstract the notation for the single site MGF , letting Z ℓi = (cid:90) (cid:89) µ dχ µ d ˆ χ µ 2 π dξ µ d ˆ ξ µ 2 π dζ µ d ˆ ζ µ 2 π d ˜ ζ µ d ˆ˜ ζ µ 2 π exp (cid:16) −H ℓi [ χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ] (cid:17) ( 25 ) where H ℓi is the single site effective Hamiltonian for neuron i and layer ℓ . Note that at zero source , H ℓ i are identical for all i ∈ [ N ] . 18 Published as a conference paper at ICLR 2023 B . 5 S ADDLE P OINT E QUATIONS Letting the full collection of concatenated order parameters q be indexed by b . We now take the N → ∞ limit , using the method of steepest descent Z = (cid:90) (cid:89) b √ Ndq b √ 2 π exp ( NS [ q ] ) ∼ exp ( NS [ q ∗ ] ) , ∇ S [ q ] | q ∗ = 0 , N → ∞ . ( 26 ) We see that the integral over q is exponentially dominated by the saddle point where ∇ S [ q ] = 0 . We thus need to solve these saddle point equations for the q ∗ . To do this , we need to introduce some notation . Let O ( χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ) be an arbitrary function of the single site stochastic processes . We define the ℓ - th layer i - th single site average , denoted by (cid:68) O ( χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ) (cid:69) ℓ , i as (cid:68) O ( χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ) (cid:69) ℓ , i = 1 Z ℓi (cid:90) (cid:89) µ dχ µ d ˆ χ µ 2 π dξ µ d ˆ ξ µ 2 π dζ µ d ˆ ζ µ 2 π d ˜ ζ µ d ˆ˜ ζ µ 2 π exp (cid:16) −H ℓi [ χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ] (cid:17) O ( χ , ˆ χ , ξ , ˆ ξ , ζ , ˆ ζ , ˜ ζ , ˆ˜ ζ ) ( 27 ) which can be interpreted as an average over the Gibbs measure defined by energy H ℓi . With this notation , we now set about computing the saddle point equations which define the primal order parameters { Φ , G , ˜ G , ˜˜ G } . ∂S ∂ ˆΦ ℓ µν = Φ ℓ µν − 1 N N (cid:88) i = 1 (cid:10) ϕ ( h ℓ µ ) ϕ ( h ℓ ν ) (cid:11) ℓ , i = 0 ∂S ∂ ˆ G ℓ µν = G ℓ µν − 1 N N (cid:88) i = 1 (cid:10) g ℓ µ g ℓ ν (cid:11) ℓ , i = 0 ∂S ∂ ˆ˜ G ℓ µν = ˜ G ℓ µν − 1 N N (cid:88) i = 1 (cid:10) g ℓ µ ˜ g ℓ ν (cid:11) ℓ , i = 0 ∂S ∂ ˆ˜˜ G ℓ µν = ˜˜ G ℓ µν − 1 N N (cid:88) i = 1 (cid:10) ˜ g ℓ µ ˜ g ℓ ν (cid:11) ℓ , i = 0 19 Published as a conference paper at ICLR 2023 We further compute the saddle point equations for the dual order parameters ∂S ∂ Φ ℓ µν = ˆΦ ℓ µν − 1 2 N N (cid:88) i = 1 (cid:10) ˆ χ ℓ + 1 µ ˆ χ ℓ + 1 ν (cid:11) ℓ + 1 , i = 0 ∂S ∂G ℓ µν = ˆ G ℓ µν − 1 2 N N (cid:88) i = 1 (cid:68) ˆ ξ ℓ − 1 µ ˆ ξ ℓ − 1 ν (cid:69) ℓ − 1 , i = 0 ∂S ∂ ˜ G ℓ µν = ˆ˜ G ℓ µν − 1 N N (cid:88) i = 1 (cid:68) ˆ ξ ℓ − 1 µ ˆ ζ ℓ − 1 ν (cid:69) ℓ − 1 , i = 0 ∂S ∂ ˜˜ G ℓ µν = ˆ˜˜ G ℓ µν − 1 2 N N (cid:88) i = 1 (cid:68) [ ˆ ζ ℓ − 1 µ ˆ ζ ℓ − 1 ν + ˆ˜ ζ ℓ − 1 µ ˆ˜ ζ ℓ − 1 ν ] (cid:69) ℓ − 1 , i = 0 ∂S ∂A ℓ µν = − B ℓ µν − i N N (cid:88) i = 1 (cid:10) ˆ χ ℓ + 1 µ g ℓ + 1 ν (cid:11) ℓ + 1 , i = 0 ∂S ∂B ℓ µν = − A ℓ µν − i N N (cid:88) i = 1 (cid:68) ϕ ( h ℓ µ ) ˆ ξ ℓ ν (cid:69) ℓ , i = 0 ∂S ∂C ℓ µν = − D ℓ µν − i N N (cid:88) i = 1 (cid:10) ˆ χ ℓ + 1 µ ˜ g ℓ + 1 ν (cid:11) ℓ + 1 , i = 0 ∂S ∂D ℓ µν = − C ℓ µν − i N N (cid:88) i = 1 (cid:68) ϕ ( h ℓ µ ) ˆ ζ ℓ ν (cid:69) ℓ , i = 0 . ( 28 ) The correlation functions involving real variables { h , g , ˜ g } have a straightforward interpetation . However , it is not immediately clear what to do with terms involving the dual fields { ˆ χ , ˆ ξ , ˆ ζ } . As a starting example , let’s consider one of the terms for B ℓ − 1 µν , namely − i (cid:10) ˆ χ ℓ ν g ℓ ν (cid:11) . We make progress by inserting another fictitious source term u ℓ µ and differentiating near zero source − i (cid:10) ˆ χ ℓ ν g ℓ ν (cid:11) i = lim { u µ } → 0 ∂ ∂u ℓ ν (cid:42) g ℓ ν exp (cid:32) − i (cid:88) ν ′ u ν ′ ˆ χ ℓ µ ′ (cid:33)(cid:43) i . ( 29 ) Introducing a vectorization notation u ℓ = Vec { u ℓ µ } µ , ˆ χ ℓ = Vec { ˆ χ ℓ µ } µ and Φ ℓ − 1 = Mat { Φ ℓ − 1 µ , ν } µ , ν , we can perform the internal integrals over ˆ χ ℓ (cid:90) (cid:89) µ d ˆ χ ℓ µ √ 2 π exp (cid:18) − 1 2 ˆ χ ℓ ⊤ Φ ℓ − 1 ˆ χ ℓ + i ˆ χ ℓ · ( χ ℓ − u ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) (cid:19) = exp (cid:18) − 1 2 ( χ ℓ − u ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) [ Φ ℓ − 1 ] − 1 ( χ ℓ − u ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) (cid:19) exp (cid:18) − 1 2 ln det Φ ℓ − 1 (cid:19) . ( 30 ) We thus need to compute a derivative of the above function with respect to u ℓ at u ℓ = 0 , which gives − i (cid:10) ˆ χ ℓ g ℓ ⊤ (cid:11) i = [ Φ ℓ − 1 ] − 1 (cid:10) ( χ ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) g ℓ ⊤ (cid:11) i . ( 31 ) 20 Published as a conference paper at ICLR 2023 From the above reasoning , we can also easily obtain ˆ Φ ℓ − 1 using (cid:10) ˆ χ ℓ ˆ χ ℓ ⊤ (cid:11) = − ∂ 2 ∂ u ℓ ∂ u ℓ ⊤ | u = 0 (cid:10) exp (cid:0) − i u ℓ · ˆ χ ℓ (cid:1)(cid:11) = − (cid:90) d χ ℓ . . . ∂ 2 ∂ u ℓ ∂ u ⊤ | u = 0 × exp (cid:18) − 1 2 ( χ ℓ − u ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) [ Φ ℓ − 1 ] − 1 ( χ ℓ − u ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) − . . . (cid:19) = [ Φ ℓ − 1 ] − 1 − [ Φ ℓ − 1 ] − 1 (cid:10) ( χ ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) ( χ ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ ) ⊤ (cid:11) [ Φ ℓ − 1 ] − 1 . ( 32 ) Performing a similar analysis , we insert source fields r + = (cid:20) r ℓ v ℓ (cid:21) for ˆ ξ ℓ + = (cid:20) ˆ ξ ℓ ˆ ζ ℓ (cid:21) and define G ℓ + 1 + = (cid:34) G ℓ + 1 ˜ G ℓ + 1 ˜ G ℓ + 1 ⊤ ˜˜ G ℓ + 1 (cid:35) , and B ℓ + = (cid:2) B ℓ D ℓ (cid:3) and then we can compute the necessary averages using the same technique − i (cid:68) ϕ ( h ℓ ) ˆ ξ ℓ ⊤ + (cid:69) = ∂ ∂ r ℓ + | r ℓ + = 0 (cid:68) ϕ ( h ℓ ) exp (cid:16) − i r ℓ + · ˆ ξ ℓ + (cid:17)(cid:69) = (cid:10) ϕ ( h ℓ ) ( ξ ℓ + − B ℓ ⊤ + ϕ ( h ℓ ) ) ⊤ (cid:11) (cid:68) ˆ ξ ℓ + ˆ ξ ℓ ⊤ + (cid:69) = − ∂ 2 ∂ r ℓ + ∂ r ℓ ⊤ + | r ℓ + = 0 (cid:68) exp (cid:16) − i r ℓ + · ˆ ξ ℓ + (cid:17)(cid:69) = [ G ℓ + 1 + ] − 1 − [ G ℓ + 1 + ] − 1 (cid:10) ( ξ ℓ + − B ℓ ⊤ + ϕ ( h ℓ ) ) ( ξ ℓ + − B ℓ ⊤ + ϕ ( h ℓ ) ) ⊤ (cid:11) [ G ℓ + 1 + ] − 1 . ( 33 ) We now have formulas for all the necessary averages entirely in terms of the primal fields { χ , ξ , ζ , ˜ ζ } . B . 6 L INEARIZING WITH THE H UBBARD T RICK Now , using the fact that in the N → ∞ limit q concentrates around q ∗ , we will simplify our single site stochastic processes so we can obtain a final formula for { A , B , C , D , ˆΦ , ˆ G , ˆ˜ G , ˆ˜˜ G } . To do so , we utilize the Hubbard - Stratanovich identity exp (cid:18) − σ 2 2 k 2 (cid:19) = (cid:90) du √ 2 πσ 2 exp (cid:18) − 1 2 σ 2 u 2 − iku (cid:19) , ( 34 ) which is merely a consequence of the Fourier transform of the Gaussian distribution . This is often referred to as “linearizing” the action since the a term quadratic in k was replaced with an average of an action which is linear in k . In our setting , we perform this trick on a collection of variables which appear in the quadratic forms of our single site MGFs Z ℓi . For example , for the ˆ χ ℓ + 1 fields , we have exp (cid:32) − 1 2 (cid:88) µν ˆ χ ℓ + 1 µ ˆ χ ℓ + 1 ν Φ ℓ µν (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ ˆ χ ℓ + 1 µ u ℓ + 1 µ (cid:33)(cid:43) { u ℓ + 1 µ } ∼N ( 0 , Φ ℓ ) . ( 35 ) Similarly , we perform a joint decomposition for the { ˆ ξ ℓ , ˆ ζ ℓ } fields which gives exp (cid:32) − 1 2 (cid:88) µν (cid:104) ˆ ξ ℓ µ ˆ ξ ℓ ν G ℓ + 1 µ , ν + 2ˆ ξ ℓ µ ˆ ζ ℓ ν ˜ G ℓ + 1 µν + ˆ ζ ℓ µ ˆ ζ ℓ ν ˜˜ G ℓ + 1 µν (cid:105)(cid:33) ( 36 ) = (cid:42) exp (cid:32) − i (cid:88) µ [ r ℓ µ ˆ ξ ℓ µ + v ℓ µ ˆ ζ ℓ µ ] (cid:33)(cid:43) { r ℓ µ , v ℓ µ } ∼N ( 0 , G ℓ + 1 + ) , G ℓ + 1 + = (cid:34) G ℓ + 1 ˜ G ℓ + 1 ˜ G ℓ + 1 ˜˜ G ℓ + 1 (cid:35) . We thus see that the Gaussian sources { r ℓ µ } µ and { v ℓ µ } µ are mean zero with correlation given by Σ ℓ + 1 . Now that we have linearized the quadratic components involving each of the dual fields 21 Published as a conference paper at ICLR 2023 { ˆ χ , ˆ ξ , ˆ ζ } , we now perform integration over these variables , giving (cid:90) (cid:89) µ d ˆ χ ℓµ 2 π exp (cid:32) i (cid:88) µ ˆ χ ℓ µ (cid:32) χ ℓ µ − u ℓ µ − (cid:88) ν A ℓ − 1 µν g ℓ ν − (cid:88) ν C ℓ − 1 µν ˜ g ℓ ν (cid:33)(cid:33) = (cid:89) µ δ (cid:32) χ ℓ µ − u ℓ µ − (cid:88) ν A ℓ − 1 µν g ℓ ν − (cid:88) ν C ℓ − 1 µν ˜ g ℓ ν (cid:33) (cid:90) (cid:89) µ d ˆ ξ ℓµ 2 π exp (cid:32) i (cid:88) µ ˆ ξ ℓ µ (cid:32) ξ ℓ µ − r ℓ µ − (cid:88) ν B ℓ νµ ϕ ( h ℓ ν ) (cid:33)(cid:33) = (cid:89) µ δ (cid:32) ξ ℓ µ − r ℓ µ − (cid:88) ν B ℓ νµ ϕ ( h ℓ ν ) (cid:33) (cid:90) (cid:89) µ d ˆ ζ ℓµ 2 π exp (cid:32) i (cid:88) µ ˆ ζ ℓ µ (cid:32) ζ ℓ µ − v ℓ µ − (cid:88) ν D ℓ νµ ϕ ( h ℓ ν ) (cid:33)(cid:33) = (cid:89) µ δ (cid:32) ζ ℓ µ − v ℓ µ − (cid:88) ν D ℓ νµ ϕ ( h ℓ ν ) (cid:33) . ( 37 ) This reveals the following set of identities χ ℓ µ = u ℓ µ + (cid:88) ν A ℓ − 1 µν g ℓ ν + (cid:88) ν C ℓ − 1 µν ˜ g ℓ ν ξ ℓ µ = r ℓ µ + (cid:88) ν B ℓ νµ ϕ ( h ℓ ν ) , ζ ℓ µ = v ℓ µ + (cid:88) ν D ℓ − 1 νµ ϕ ( h ℓ ν ) . ( 38 ) Since we know by construction that u ℓ = χ ℓ − A ℓ − 1 g ℓ − C ℓ − 1 ˜ g ℓ is a zero mean Gaussian with covariance Φ ℓ − 1 , we can simplify our expressions for B ℓ − 1 and ˆ Φ ℓ − 1 using Stein’s Lemma B ℓ − 1 = 1 N N (cid:88) i = 1 [ Φ ℓ − 1 ] − 1 (cid:10) u ℓ g ℓ ⊤ (cid:11) i = 1 N N (cid:88) i = 1 (cid:28) ∂ g ℓ ⊤ ∂ u ℓ (cid:29) i D ℓ − 1 = 1 N N (cid:88) i = 1 [ Φ ℓ − 1 ] − 1 (cid:10) u ℓ ˜ g ℓ ⊤ (cid:11) i = 1 N N (cid:88) i = 1 (cid:28) ∂ ˜ g ℓ ⊤ ∂ u ℓ (cid:29) i ( 39 ) ˆ Φ ℓ − 1 = 1 2 [ Φ ℓ − 1 ] − 1 − 1 2 N N (cid:88) i = 1 [ Φ ℓ − 1 ] − 1 (cid:10) u ℓ u ℓ ⊤ (cid:11) i [ Φ ℓ − 1 ] − 1 = 0 . Similarly , using the Gaussianity of r ℓ , ζ ℓ , ˆ ζ ℓ we have A ℓ = 1 N N (cid:88) i = 1 (cid:28) ∂ϕ ( h ℓ ) ∂ r ℓ ⊤ (cid:29) i , C ℓ = 1 N N (cid:88) i = 1 (cid:28) ∂ϕ ( h ℓ ) ∂ v ℓ ⊤ (cid:29) i , ˆ G ℓ + 1 = ˆ˜ G ℓ + 1 = ˆ˜˜ G ℓ + 1 = 0 . B . 7 F INAL DMFT E QUATIONS We now take the limit of zero source j ℓ , k ℓ , n ℓ , p ℓ → 0 . In this limit , all single site averages ⟨⟩ i become identical so we can simplify the expressions for the order parameters . To “symmetrize” the equations we will also make the substitution B → B ⊤ , D → D ⊤ . Next , we also rescale all of the response functions { A ℓ , B ℓ , C ℓ , D ℓ } by γ − 1 0 so that they are O γ 0 ( 1 ) at small γ 0 . This gives us the following set of equations for the order parameters Φ ℓµν ( t , s ) = (cid:10) ϕ ( h ℓµ ( t ) ) ϕ ( h ℓν ( s ) ) (cid:11) , G ℓµν ( t , s ) = (cid:10) g ℓµ ( t ) g ℓν ( s ) (cid:11) , ˜ G ℓµν ( t , s ) = (cid:10) g ℓµ ( t ) ˜ g ℓν ( s ) (cid:11) ˜˜ G ℓµν ( t , s ) = (cid:10) ˜ g ℓµ ( t ) ˜ g ℓν ( s ) (cid:11) , A ℓµν ( t , s ) = γ − 1 0 (cid:42) δϕ ( h ℓµ ( t ) ) δr ℓν ( s ) (cid:43) , C ℓµν ( t , s ) = γ − 1 0 (cid:42) δϕ ( h ℓµ ( t ) ) δv ℓν ( s ) (cid:43) B ℓµν ( t , s ) = γ − 1 0 (cid:42) δg ℓ + 1 µ ( t ) δu ℓ + 1 ν ( s ) (cid:43) , D ℓµν ( t , s ) = γ − 1 0 (cid:42) δ ˜ g ℓ + 1 µ ( t ) δu ℓ + 1 ν ( s ) (cid:43) . 22 Published as a conference paper at ICLR 2023 E x p t . 1 , D M F T . 2 , 3 , ( a ) ρ = 0 Final Kernel E x p t . Tr 1 ( t , s ) D M F T . Tr 2 ( t , s ) Tr 3 ( t , s ) ( b ) ρ = 0 Kernel Dynamics E x p t . 1 , D M F T . 2 , 3 , ( c ) ρ = 1 . 0 Final Kernel E x p t . Tr 1 ( t , s ) D M F T . Tr 2 ( t , s ) Tr 3 ( t , s ) ( d ) ρ = 1 . 0 Kernel Dynamics Figure 6 : Feature kernels Φ ℓ and their dynamics predicted by solving full set of saddle point equa - tions equation 40 for ρ - FA in depth 3 tanh network with γ 0 = 1 . 0 . Solving deep nonlinear ρ - FA requires sampling the full triplet of Gaussian sources { u ℓ , r ℓ , v ℓ } for each layer and computing all four response functions { A ℓ , B ℓ , C ℓ , D ℓ } . DMFT theoretical predictions are compared to a width N = 3000 neural network . For the fields h ℓµ ( t ) , z ℓµ ( t ) , ˜ z ℓµ ( t ) , we have the following equations h ℓµ ( t ) = u ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 [ A ℓ − 1 µν ( t , s ) g ℓν ( s ) + C ℓ − 1 µν ( t , s ) ˜ g ℓµ ( s ) + ∆ ν ( s ) Φ ℓ − 1 µν ( t , s ) ˜ g ℓν ( s ) ] z ℓµ ( t ) = r ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 [ B ℓµν ( t , s ) ϕ ( h ℓν ( s ) ) + ∆ ν ( s ) ˜ G ℓ + 1 µν ( t , s ) ϕ ( h ℓν ( s ) ) ] ˜ g ℓµ ( t ) =    ˙ ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) GD ˙ ϕ ( h ℓµ ( t ) ) (cid:104)(cid:112) 1 − ρ 2 ˜ ζ ℓµ ( t ) + ρv ℓµ ( t ) + ργ 0 (cid:82) t 0 ds (cid:80) Pν = 1 D ℓµν ( t , s ) ϕ ( h ℓν ( s ) ) (cid:105) ρ - FA ˙ ϕ ( h ℓµ ( t ) ) ˜ z ℓ , ˜ z ℓ ∼ N ( 0 , 1 ) DFA ˙ ϕ ( m ℓµ ( t ) ) z ℓµ ( t ) GLN ∆ µ ( t ) ϕ ( h ℓµ ( t ) ) Hebb { u ℓµ ( t ) } ∼ GP ( 0 , Φ ℓ − 1 ) , { r ℓµ ( t ) , v ℓµ ( t ) } ∼ GP ( 0 , G ℓ + 1 + ) , { ˜ ζ ℓµ ( t ) } ∼ GP ( 0 , ˜˜ G ℓ + 1 ) G ℓ + 1 + = (cid:34) G ℓ + 1 ˜ G ℓ + 1 ˜ G ℓ + 1 , ⊤ ˜˜ G ℓ + 1 (cid:35) . ( 40 ) C E XTENSION TO O THER A RCHITECTURES AND O PTIMIZERS In this section , we consider the effect of changing architectural details ( multiple output channels and convolutional structure ) and also optimization choices ( momentum , regularization ) . 23 Published as a conference paper at ICLR 2023 C . 1 M ULTIPLE O UTPUT C LASSES Similar to pre - existing work on the GD case ( Bordelon & Pehlevan , 2022 ) , our new generalized DMFT can be easily extended to C output channels , provided the number of channels C is not simultaneously taken to infinity with network width N . We note that the outputs of the network are now vectors f µ ∈ R C and that each eNTK entry is now a C × C matrix K µν ( t , s ) ∈ R C × C . The relevant true gradient fields are vectors g ℓc , µ = ∂f c , µ ∂ h ℓµ . We construct pseudo - gradients ˜ g ℓc , µ as before using each of our learning rules . The gradient - pseudogradient kernel G ℓµν ∈ R C × C is ˜ G ℓc , c ′ , µν = 1 N g ℓc , µ · g ℓc ′ , ν . The eNTK K µν = (cid:80) ℓ ˜ G ℓ + 1 µν Φ ℓµν can be used to derive the function dynamics ∂ f µ ∂t = (cid:88) ν K µν ∆ µ , ∆ µ = − ∂ L ∂ f µ . ( 41 ) At infinite width N → ∞ , the field dynamics for h ℓµ ( t ) ∈ R , g ℓµ ( t ) ∈ R C satisfy h ℓµ ( t ) = u ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 (cid:2) ˜ g ℓν ( s ) · ∆ ν ( s ) Φ ℓ − 1 µν ( t , s ) + A ℓ − 1 µν ( t , s ) · g ℓν ( s ) + C ℓ − 1 µν · ˜ g ℓν ( s ) (cid:3) z ℓµ ( t ) = r ℓµ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) ν = 1 ϕ ( h ℓν ( s ) ) (cid:104) ˜ G ℓ + 1 µν ( t , s ) ∆ ν ( s ) + B ℓµν ( t , s ) (cid:105) , ( 42 ) where A ℓµν ( t , s ) = γ 0 (cid:28) δϕ ( h ℓµ ( t ) ) δ r ℓν ( s ) (cid:29) ∈ R C , A ℓµν ( t , s ) = γ 0 (cid:28) δϕ ( h ℓµ ( t ) ) δ v ℓν ( s ) (cid:29) ∈ R C , and B µα ( t , s ) = ∂ g ℓ + 1 µ ( t ) ∂u ℓν ( s ) ∈ R C . The feature kernels are the same as before Φ ℓµν ( t , s ) = (cid:10) ϕ ( h ℓµ ( t ) ) ϕ ( h ℓν ( s ) ) (cid:11) while the gradient - pseudogradient kernel is ˜ G ℓµν ( t , s ) = (cid:10) g ℓµ ( t ) ˜ g ℓ ⊤ ν ( s ) (cid:11) ∈ R C × C . The pseudogradient fields ˜ g ℓ are defined analogously for each learning rule as in the single class setting . ˜ g ℓµ ( t ) =   ˙ ϕ ( h ℓµ ( t ) ) z ℓµ ( t ) GD ˙ ϕ ( h ℓµ ( t ) ) (cid:104) ρ v ℓµ ( t ) + (cid:112) 1 − ρ 2 ˜ ζ ℓµ ( t ) + ργ 0 (cid:82) t 0 ds (cid:80) Pν = 1 D ℓν ( t , s ) ϕ ( h ℓν ( s ) ) (cid:105) ρ - FA ˙ ϕ ( h ℓµ ( t ) ) ˜ z ℓ , ˜ z ℓ ∼ N ( 0 , I ) DFA ˙ ϕ ( m ℓµ ( t ) ) z ℓµ ( t ) GLN 1 ∆ µ ( t ) ϕ ( h ℓµ ( t ) ) Hebb ( 43 ) C . 2 CNN The DMFT described for each of these learning rules can also be extended to CNNs with infinitely many channels . Following the work of Bordelon & Pehlevan ( 2022 ) Appendix G on the GD DMFT limit for CNNs , we let W ℓij , a represent the value of the filter at spatial displacement a from the center of the filter , which maps relates activity at channel j of layer ℓ to channel i of layer ℓ + 1 . The fields h ℓµ , i , a satisfy the recursion h ℓ + 1 µ , i , a = 1 √ N N (cid:88) j = 1 (cid:88) b ∈S ℓ W ℓij , b ϕ ( h ℓµ , j , a + b ) , i ∈ [ N ] , ( 44 ) where S ℓ is the spatial receptive field at layer ℓ . For example , a ( 2 k + 1 ) × ( 2 k + 1 ) convolution will have S ℓ = { ( i , j ) ∈ Z 2 : − k ≤ i ≤ k , − k ≤ j ≤ k } . The output function is obtained from the last layer is defined as f µ = 1 γ 0 N (cid:80) N i = 1 (cid:80) a w L i , a ϕ ( h L µ , i , a ) . The true gradient fields have the same definition as before g ℓµ , a = γ 0 N ∂f µ ∂ h ℓµ , a ∈ R N , which as before enjoy the following recursion g ℓµ , a = γ 0 N (cid:88) b ∂f µ ∂ h ℓ + 1 µ , b · ∂ h ℓ + 1 µ , b ∂ h ℓµ , a = ˙ ϕ ( h ℓµ , a ) ⊙   1 √ N N (cid:88) j = 1 (cid:88) b ∈S ℓ W ℓ ⊤ b g ℓ + 1 µ , a − b   . ( 45 ) 24 Published as a conference paper at ICLR 2023 We consider the following learning dynamics for the filters d dt W ℓ b = γ 0 √ N (cid:88) µ , c ∆ µ ˜ g ℓ + 1 µ , c ϕ ( h ℓµ , c + b ) ⊤ ( 46 ) where as before ˜ g ℓ is determined by the learning rule . The relevant kernel order parameters now have spatial indices . For instance the feature kernel at each layer has form Φ ℓµ , ν , ab = 1 N ϕ ( h ℓµ , a ( t ) ) · ϕ ( h ℓν , b ( s ) ) . At the infinite width N → ∞ , the order parameters and field dynamics have the form h ℓµ , a ( t ) = u ℓµ , a ( t ) + γ 0 (cid:90) t 0 ds (cid:88) ν , b , c ∆ ν ( s ) Φ ℓ − 1 µν , a + b , b + c ( t , s ) ˜ g ℓν , c ( s ) ( 47 ) + γ 0 (cid:90) t 0 ds (cid:88) ν , b [ A ℓ − 1 µν , ab ( t , s ) g ℓν , b ( s ) + C ℓ − 1 µν , ab ( t , s ) ˜ g ℓν b ( s ) ] z ℓµ , a ( t ) = r ℓµ , a ( t ) + γ 0 (cid:90) t 0 ds (cid:88) ν , b , c ˜ G ℓ + 1 µν , a − b , c − b ( t , s ) ϕ ( h ℓν , c ( s ) ) + γ 0 (cid:90) t 0 ds (cid:88) ν , b B ℓµν , ab ( t , s ) ϕ ( h ℓν , b ( s ) ) ( 48 ) where correlation and response functions have the usual definitions Φ ℓµα , ab ( t , s ) = (cid:10) ϕ ( h ℓµ a ( t ) ) ϕ ( h ℓα b ( s ) ) (cid:11) , G ℓµα , ab ( t , s ) = (cid:10) g ℓµ a ( t ) g ℓα b ( s ) (cid:11) , ˜ G ℓµν , ab ( t , s ) = (cid:10) g ℓµ a ( t ) ˜ g ℓα b ( s ) (cid:11) A ℓµα , ab ( t , s ) = 1 γ 0 (cid:42) δϕ ( h ℓµ a ( t ) ) δr ℓα b ( s ) (cid:43) , B ℓµα , ab ( t , s ) = 1 γ 0 (cid:42) δg ℓ + 1 µ a ( t ) δu ℓ + 1 α b ( s ) (cid:43) . ( 49 ) C . 3 L2 R EGULARIZATION ( W EIGHT D ECAY ) L2 regularization on the weights W ℓ ( weight decay ) can also be modeled within DMFT . We start by looking at the weight dynamics d dt W ℓ = γ 0 √ N P (cid:88) µ = 1 ∆ µ ˜ g ℓ + 1 µ ϕ ( h ℓµ ) ⊤ − λ W ℓ = ⇒ W ℓ ( t ) = e − λt W ℓ ( 0 ) + γ 0 √ N (cid:90) t 0 ds e − λ ( t − s ) (cid:88) µ ∆ µ ( s ) ˜ g ℓ + 1 µ ( s ) ϕ ( h ℓµ ( s ) ) ⊤ ( 50 ) In the second line we used an integrating factor e λt . We can thus arrive at the following feature dynamics in the DMFT limit h ℓµ ( t ) = e − λt χ ℓµ ( t ) + γ 0 (cid:90) t 0 ds e − λ ( t − s ) (cid:88) ν ∆ ν ( s ) Φ ℓ − 1 µν ( s ) ˜ g ℓν ( s ) z ℓµ ( t ) = e − λt ξ ℓµ ( t ) + γ 0 (cid:90) t 0 ds e − λ ( t − s ) (cid:88) ν ∆ ν ( s ) ˜ G ℓ + 1 µν ( s ) ϕ ( h ℓν ( s ) ) . The ˜ g dynamics are also modified appropriately with factors of e − λt and e − λ ( t − s ) for each of our learning rules . We see that the contribution from the initial conditions χ , ξ are suppressed at late times while the feature learning update which is O ( γ 0 / λ ) in the first layer dominates scale of the final features . C . 4 M OMENTUM Momentum uses a low - pass filtered version of the gradients to update the weights ( Goh , 2017 ) . A continuous time limit of momentum dynamics on the trainable parameters { W ℓ } would give the 25 Published as a conference paper at ICLR 2023 following differential equations ∂ ∂t W ℓ ( t ) = Q ℓ ( t ) τ d dt Q ℓ ( t ) = − Q ℓ + γ 0 √ N (cid:88) µ ∆ µ ( t ) ˜ g ℓ + 1 µ ( t ) ϕ ( h ℓµ ( t ) ) ⊤ . ( 51 ) We write the expression this way so that the small time constant τ → 0 limit corresponds to classic gradient descent . Integration of the Q ℓ ( t ) dynamics gives the following integral expression for W ℓ W ℓ ( t ) = W ℓ ( 0 ) + γ 0 √ Nτ (cid:90) t 0 dt ′ (cid:90) t ′ 0 dt ′′ e − ( t ′ − t ′′ ) / τ (cid:88) µ ∆ µ ( t ′′ ) ˜ g ℓ + 1 µ ( t ′′ ) ϕ ( h ℓµ ( t ′′ ) ) ⊤ . ( 52 ) These weight dynamics give rise to the following field evolution h ℓ + 1 µ ( t ) = χ ℓ + 1 µ ( t ) + γ 0 τ (cid:90) t 0 dt ′ (cid:90) t ′ 0 dt ′′ e − ( t ′ − t ′′ ) / τ (cid:88) ν ∆ ν ( t ′′ ) ˜ g ℓ + 1 ν ( t ′′ ) Φ ℓµν ( t , t ′′ ) z ℓµ ( t ) = ξ ℓµ ( t ) + γ 0 τ (cid:90) t 0 dt ′ (cid:90) t ′ 0 dt ′′ e − ( t ′ − t ′′ ) / τ (cid:88) ν dt ′′ ∆ α ( t ′′ ) ˜ G ℓ + 1 µν ( t , t ′′ ) ϕ ( h ℓν ( t ′′ ) ) . ( 53 ) We see that in the τ → 0 limit , the t ′′ integral is dominated by the contribution at t ′′ ∼ t ′ recov - ering usual gradient descent dynamics . For τ ≫ 0 , we see that the integral accumulates additional contributions from the past values of fields and kernels . D L AZY L IMITS In this section we discuss the lazy γ 0 → 0 limit . In this limit we see that h ℓ ( t ) = u ℓ ( t ) and z ℓ ( t ) = r ℓ ( t ) for all time t . Since the input data gram matrix Φ 0 µν = 1 D x µ · x ν is a constant in time the sources in the first hidden layer u 1 µ are constant in time . Consequently , the first layer feature kernel is constant in time since Φ 1 µν ( t , s ) = (cid:10) ϕ ( h 1 µ ( t ) ) ϕ ( h 1 ν ( s ) ) (cid:11) = (cid:10) ϕ ( u 1 µ ) ϕ ( u 1 ν ) (cid:11) u 1 ∼N ( 0 , Φ 0 ) . ( 54 ) Now , we see that this argument can proceed inductively . Since Φ 1 is time - independent , the second layer fields h 2 = u 2 ∼ N ( 0 , Φ 1 ) are also constant in time , implying Φ 2 is constant in time . This argument is repeated for all layer ℓ ∈ [ L ] . Similarly , we can analyze the backward pass fields z ℓ . Since z L ∼ N ( 0 , G L + 1 ) are constant , then z ℓ are time - independent for all ℓ . It thus suffices to compute the static kernels { Φ ℓ , G ℓ , ˜ G ℓ } at initialization Φ ℓ = (cid:10) ϕ ( u ℓ ) ϕ ( u ℓ ) ⊤ (cid:11) u ℓ ∼N ( 0 , Φ ℓ − 1 ) G ℓ = (cid:68) [ ˙ ϕ ( u ℓ ) ⊙ r ℓ ] [ ˙ ϕ ( u ℓ ) ⊙ r ℓ ] ⊤ (cid:69) u ℓ ∼N ( 0 , Φ ℓ − 1 ) , r ℓ ∼N ( 0 , G ℓ + 1 ) = G ℓ + 1 ⊙ ˙ Φ ℓ , ˙ Φ ℓ = (cid:68) ˙ ϕ ( u ℓ ) ˙ ϕ ( u ℓ ) ⊤ (cid:69) u ℓ ∼N ( 0 , Φ ℓ − 1 ) . ( 55 ) where in the last line we utilized the independence of u ℓ , r ℓ . These above equations give a forward pass recursion for the Φ ℓ kernels and the backward pass recursion for G ℓ . Lastly , depending on the learning rule , we arrive at the following definitions for ˜ G ℓ for ℓ ∈ { 1 , . . . , L } ˜ G ℓ = (cid:68) [ ˙ ϕ ( u ℓ ) ⊙ r ℓ ] ˜ g ℓ ⊤ (cid:69) =    G ℓ + 1 ⊙ ˙ Φ ℓ GD ρ ˜ G ℓ + 1 ⊙ ˙ Φ ℓ ρ - FA 0 DFA , Hebb G ℓ + 1 ⊙ (cid:68) ˙ ϕ ( m ℓ ) ˙ ϕ ( m ℓ ) ⊤ (cid:69) GLN ( 56 ) Using these results for ˜ G , we can compute the initial eNTK K = (cid:80) Lℓ = 0 ˜ G ℓ + 1 ⊙ Φ ℓ which governs prediction dynamics . 26 Published as a conference paper at ICLR 2023 D . 1 L AZY L IMIT P ERFORMANCES ON R EALISTIC T ASKS We note that , while the DMFT equations on P datapoints and T timesteps require O ( P 3 T 3 ) time complexity to solve in the rich regime , the lazy limit gives neural network predictions in O ( P 3 ) time , since the predictor can be obtained by solving a linear system of P equations . The performance of these lazy limit kernels on realistic tasks would match the performances reported by Lee et al . ( 2020 ) . Specifically , GD and ρ = 1 FA would match the test accuracy reported for “infinite width GD” , while ρ = 0 FA , DFA , and Hebbian rules would match “infinite width Bayesian” networks in Figure 1 of Lee et al . ( 2020 ) . E D EEP L INEAR N ETWORKS In deep linear networks , the DMFT equations close without needing any numerical sampling proce - dure , as was shown in prior work on the GD case ( Yang & Hu , 2021 ; Bordelon & Pehlevan , 2022 ) . The key observation is that for all of the following learning rules , the fields { h , g , ˜ g } are linear combinations of the Gaussian sources { u , r , v } , and are thus Gaussian themselves . Concretely , we introduce a vector notation h ℓ = Vec { h ℓµ ( t ) } and g ℓ = Vec { g ℓµ ( t ) } , etc . We have in each layer h ℓ = R h , u u ℓ + R h , r r ℓ + R h , v v ℓ + R h , ˜ ζ ˜ ζ ℓ g ℓ = R g , u u ℓ + R g , r r ℓ + R g , v v ℓ + R g , ˜ ζ ˜ ζ ℓ ˜ g ℓ = R ˜ g , u u ℓ + R ˜ g , r r ℓ + R ˜ g , v v ℓ + R ˜ g , ˜ ζ ˜ ζ ℓ where the matrices R depend on the learning rule and the data . The necessary kernels H ℓ = (cid:10) h ℓ h ℓ ⊤ (cid:11) can thus be closed algebraically since all of the correlation statistics of the sources { u , r , v } have known two - point correlation statistics . E . 1 L INEAR N ETWORK T RAINED WITH GD The R matrices for GD were provided in ( Bordelon & Pehlevan , 2022 ) . We start by noting the following DMFT equations for h ℓ , g ℓ h ℓ = u ℓ + γ 0 ( A ℓ − 1 + H ℓ − 1 ∆ ) g ℓ , g ℓ = r ℓ + γ 0 ( B ℓ + G ℓ + 1 ∆ ) h ℓ ( 57 ) where [ H ℓ − 1 ∆ ] µν , ts = H ℓµν ( t , s ) ∆ ν ( s ) . Isolating the dependence of these equations on u and r , we have (cid:2) I − γ 20 ( A ℓ − 1 + H ℓ − 1 ∆ ) ( B ℓ + G ℓ + 1 ∆ ) (cid:3) h ℓ = u ℓ + γ 20 ( A ℓ − 1 + H ℓ − 1 ∆ ) ( B ℓ + G ℓ + 1 ∆ ) r ℓ (cid:2) I − γ 20 ( B ℓ + G ℓ + 1 ∆ ) ( A ℓ − 1 + H ℓ − 1 ∆ ) (cid:3) g ℓ = r ℓ + γ 20 ( B ℓ + G ℓ + 1 ∆ ) ( A ℓ − 1 + H ℓ − 1 ∆ ) r ℓ . ( 58 ) These equations can easily be closed for H ℓ and G ℓ . E . 2 ρ - A LIGNED F EEDBACK A LIGNMENT In ρ - FA we define the following pseudo - gradient fields ˜ g ℓ = (cid:112) 1 − ρ 2 ˜ ζ ℓ + ρ v ℓ + ργ 0 D ℓ h ℓ ( 59 ) Next , we note that , at initialization , the ˜ G ℓ can be computed recursively ˜ G ℓ = ρ ˜ G ℓ + 1 ( 60 ) We note that ∂∂ r 1 h 1 = 0 which implies A 1 = 0 . Similarly we have ∂∂ r 2 h 2 = 0 . Thus A 2 = 0 . Proceeding inductively , we find A ℓ = 0 . Similarly , we note that ∂ ˜ g L ∂ u L = 0 so D L − 1 = 0 . Inductively , we have D ℓ = 0 for all ℓ . Using these facts , we thus find the following equations h ℓ = u ℓ + γ 0 ( C ℓ − 1 + H ℓ − 1 ∆ ) ˜ g ℓ ( 61 ) g ℓ = r ℓ + γ 0 ( B ℓ + G ℓ + 1 ∆ ) h ℓ ( 62 ) ˜ g ℓ = (cid:112) 1 − ρ 2 ˜ ζ ℓ + ρ v ℓ ( 63 ) 27 Published as a conference paper at ICLR 2023 We can close these equations for H ℓ and ˜ G ℓ H ℓ = H ℓ − 1 + γ 20 (cid:0) C ℓ − 1 + H ℓ − 1 ∆ (cid:1) ˜˜ G ℓ + 1 (cid:0) C ℓ − 1 + H ℓ − 1 ∆ (cid:1) ⊤ ˜ G ℓ = ρ ˜ G ℓ + 1 + γ 20 (cid:0) B ℓ + G ℓ + 1 ∆ (cid:1) (cid:0) C ℓ − 1 + H ℓ − 1 ∆ (cid:1) ˜˜ G ℓ + 1 C ℓ = ρ (cid:0) C ℓ − 1 + H ℓ − 1 ∆ (cid:1) , B ℓ = B ℓ + 1 + G ℓ + 2 ∆ . ( 64 ) The matrices ˜˜ G ℓ = 11 ⊤ are all rank one . Thus it suffices to compute the vectors c ℓ = (cid:0) C ℓ − 1 + H ℓ − 1 ∆ (cid:1) 1 . Further , it suffices to consider d ℓ = ˜ G ℓ 1 / | 1 | 2 . With this formalism we have H ℓ = H ℓ − 1 + γ 20 c ℓ c ℓ ⊤ , d ℓ = ρ d ℓ + 1 + γ 20 ( B ℓ + G ℓ + 1 ∆ ) c ℓ . ( 65 ) The analysis for DFA and Hebb rules is very similar . F E XACTLY SOLVEABLE 2 LAYER LINEAR MODEL F . 1 G RADIENT F LOW Based on the prior results from ( Bordelon & Pehlevan , 2022 ) , the H y = y ⊤ Hy / | y | 2 dynamics for GD are coupled to the dynamics for the error ∆ ( t ) = 1 | y | y · ∆ ( t ) have the form d dtH y ( t ) = 2 γ 20 ( y − ∆ ) ∆ , d dt ∆ = − 2 H y ∆ . ( 66 ) These dynamics have the conservation law ddt H 2 y = γ 20 ddt ( y − ∆ ) 2 . Integrating this conservation law from time 0 to time t , we find H y ( t ) 2 = 1 + γ 20 ( y − ∆ ( t ) ) 2 . We can therefore solve a single ODE for ∆ ( t ) , giving the following simplified dynamics d dt ∆ = − 2 (cid:113) 1 + γ 20 ( y − ∆ ) 2 ∆ , H y = (cid:113) 1 + γ 20 ( y − ∆ ) 2 . ( 67 ) These dynamics interpolate between exponential convergence ( at small γ 0 ) and a logistic conver - gence ( at large γ 0 ) of ∆ ( t ) to zero . Since ∆ → 0 at late time , the final value of the kernel alignment is H y = (cid:112) 1 + γ 20 y 2 . F . 2 ρ - ALIGNED FA For the two layer linear network , the ρ - FA field dynamics are d dth µ ( t ) = γ 0 (cid:88) ν ˜ g ν ( t ) ∆ ν ( s ) K xµν , d dtg µ ( t ) = γ 0 (cid:88) ν ∆ ν ( t ) h ν ( t ) . ( 68 ) FA we have ˜ g µ ( t ) = ˜ g ∼ N ( 0 , 1 ) which is a constant standard normal . We let a µ ( t ) = ⟨ ˜ gh µ ( t ) ⟩ . The dynamics for H µν and a µ are coupled d dtH µν = γ 0 a ν ( t ) (cid:88) ν ∆ ν ( t ) K xµν + γ 0 a µ ( t ) (cid:88) ν ∆ ν ( t ) K xµν d dta µ ( t ) = γ 0 (cid:88) ν ∆ ν K xµν d dt ˜ G ( t ) = γ 0 (cid:88) µ ∆ µ ( t ) a µ ( t ) , d dt ∆ µ ( t ) = − (cid:88) ν [ H µν ( t ) + ˜ G ( t ) K x µν ] ∆ ν ( t ) . ( 69 ) Whitening the dataset K x = I and projecting all dynamics on ˆ y subspace gives the reduced dy - namics d dt H = 2 γ 0 a ∆ , d dt a = γ 0 ∆ , d dt ˜ G = γ 0 ∆ a , d dt ∆ = − [ H + ˜ G ] ∆ . ( 70 ) 28 Published as a conference paper at ICLR 2023 From these dynamics we identify the following set of conservation laws 2 d dt ˜ G = d dta 2 = d dtH = ⇒ 2 ˜ G − 2 ρ = a 2 = H − 1 . ( 71 ) Writing everything in terms of ∆ , a we have d dta = γ 0 ∆ , d dt ∆ = − (cid:20) 3 2 a 2 + ( 1 + ρ ) (cid:21) ∆ = − γ − 1 0 d dt (cid:20) 1 2 a 3 + ( 1 + ρ ) a (cid:21) Integrating both sides of this equation from 0 to t gives ∆ = y − γ − 1 0 (cid:2) 12 a 3 + ( 1 + ρ ) a (cid:3) . Thus , the a dynamics now one dimensional , giving d dta = γ 0 y − 1 2 a 3 − ( 1 + ρ ) a . ( 72 ) When run from initial condition a = 0 , this will converge to the smallest positive root of the cubic equation 12 a 3 + ( 1 + ρ ) a = γ 0 y . This implies that , for small γ 0 we have a ∼ γ 0 y 1 + ρ so that ∆ H = 2∆ ˜ G ∼ γ 20 y 2 ( 1 + ρ ) 2 and so that larger initial alignment ρ leads to smaller changes in the feature kernel and pseudo - gradient alignment kernel . At large γ 0 y , we have that a ∼ ( 2 γ 0 y ) 1 / 3 so that ∆ H = 2∆ ˜ G ∼ ( 2 γ 0 y ) 2 / 3 . F . 3 H EBB For the Hebb rule , ˜ G µ = ⟨ gh µ ⟩ ∆ µ = γ 0 f µ ∆ µ = γ 0 ( y µ − ∆ µ ) ∆ µ . Under the whitening assumption K xµν = δ µν , the dynamics decouples over samples d dtH µ , µ = 2 γ 0 H µµ ∆ 2 µ , d dt ∆ µ = − [ H µµ + γ 0 ( y µ − ∆ µ ) ∆ µ ] ∆ µ . ( 73 ) We see that H µµ strictly increases . The possible fixed points for ∆ µ are ∆ µ = 0 or ∆ µ = 12 (cid:104) y µ ± (cid:113) y 2 µ + γ − 1 0 H µµ (cid:105) . One of these roots shares a sign with y µ and has larger absolute value . The other root has the opposite sign from y µ . From the initial condition ∆ µ = y µ and H µµ = 1 , ∆ µ is initially approaching decreasing in absolute value so that | ∆ µ | ∈ ( 0 , | y µ | ) and will have the same sign as y µ . In this regime ddt | ∆ µ | < 0 . Thus , the system will eventually reach the fixed point at ∆ µ = 0 , rather than increasing in magnitude to the root which shares a sign with y µ or continuing to the root with the opposite sign as y µ . G D ISCUSSION OF M ODIFIED H EBB R ULE We chose to modify the traditional Hebb rule to include a weighing of each example by its instanta - neous error . In this section we discuss this choice and provide a brief discussion of alternatives • Traditional Hebb Learning : ddt W ℓ ∝ (cid:80) µ ϕ ( h ℓ + 1 µ ) ϕ ( h ℓµ ) ⊤ . In the absence of regularization or normalization , this learning rule will continue to update the weights even once the task is fully learned , leading to divergences at infinite time t → ∞ . • Single Power of the Error : ddt W ℓ ∝ (cid:80) µ ∆ µ ϕ ( h ℓ + 1 µ ) ϕ ( h ℓµ ) ⊤ . While this rule may naively appear plausible , it can only learn training points with positive target values y µ in a linear network if γ 0 > 0 . Further this rule only gives Hebbian updates when ∆ µ > 0 . • Two Powers of the Error : ddt W ℓ ∝ (cid:80) µ ∆ 2 µ ϕ ( h ℓ + 1 µ ) ϕ ( h ℓµ ) ⊤ . This was our error modified Hebb rule . We note that the update always has the correct sign for a Hebbian update and the updates stop when the network converges to zero error , preventing divergence of the features at late time . H F INITE S IZE E FFECTS We can reason about the fluctuations of q around the saddle point q ∗ at large but finite N using a Taylor expansion of the DMFT action S around the saddle point . This argument will show that 29 Published as a conference paper at ICLR 2023 at large but finite N , we can treat q as fluctuating over initializations with mean q ∗ and variance O ( N − 1 ) . We will first illustrate the mechanics of this computation of an arbitrary observable with a scalar example before applying this to the DMFT . H . 1 S CALAR E XAMPLE Suppose we have a scalar variable q with a distribution defined by Gibbs measure e − NS [ q ] (cid:82) dqe − NS [ q ] for action S . We consider averaging some arbitrary observable O ( q ) over this distribution ⟨O ( q ) ⟩ = (cid:82) dq exp ( − NS [ q ] ) O ( q ) (cid:82) dq exp ( − NS [ q ] ) . ( 74 ) We Taylor expand S around its saddle point q ∗ giving S [ q ] = S [ q ∗ ] + 12 S ′′ [ q ∗ ] ( q − q ∗ ) 2 + (cid:80) ∞ k = 3 S ( k ) [ q ∗ ] ( q − q ∗ ) k . This gives ⟨ O ( q ) ⟩ = (cid:82) dq exp (cid:0) − N [ 12 S ′′ [ q ∗ ] ( q − q ∗ ) 2 − (cid:80) ∞ k = 3 S ( k ) [ q ∗ ] ( q − q ∗ ) k ] (cid:1) O ( q ) (cid:82) dq exp (cid:0) − N [ 12 S ′′ [ q ∗ ] ( q − q ∗ ) 2 − (cid:80) ∞ k = 3 S ( k ) [ q ∗ ] ( q − q ∗ ) k ] (cid:1) . ( 75 ) The exp ( NS [ q ∗ ] ) terms canceled in both numerator and denominator . We let the variable q − q ∗ = 1 √ N δ . After this change of variable , we have ⟨O ( q ) ⟩ = (cid:82) dδ exp (cid:0) − 12 S ′′ [ q ∗ ] δ 2 − (cid:80) ∞ k = 3 N 1 − k / 2 S ( k ) [ q ∗ ] δ k (cid:1) O ( q ∗ + N − 1 / 2 δ ) (cid:82) dδ exp (cid:0) − 12 S ′′ [ q ∗ ] δ 2 − (cid:80) ∞ k = 3 N 1 − k / 2 S ( k ) [ q ∗ ] δ k (cid:1) . ( 76 ) We note that all the higher order derivatives ( k ≥ 3 ) are suppressed by at least N − 1 / 2 compared to the quadratic term . Letting U = (cid:80) ∞ k = 3 N 1 − k / 2 S ( k ) [ q ∗ ] δ k represent the perturbed potential , we can Taylor expand the exponential around the Gaussian unperturbed potential exp (cid:0) − 12 δ 2 S ′′ [ q ∗ ] (cid:1) . We let ⟨O ( δ ) ⟩ 0 = E q ∼N ( 0 , S ′′ [ q ∗ ] − 1 ) O ( δ ) represent an average over this unperturbed potential ⟨O ( q ) ⟩ = (cid:82) dδ exp (cid:0) − 12 S ′′ [ q ∗ ] δ 2 (cid:1) [ 1 − U + 12 U 2 + . . . ] O ( q ) (cid:82) dδ exp (cid:0) − 12 S ′′ [ q ∗ ] δ 2 (cid:1) [ 1 − U + 12 U 2 + . . . ] ( 77 ) = ⟨O ( q ) ⟩ 0 − ⟨O ( q ) U ⟩ 0 + 12 (cid:10) O ( q ) U 2 (cid:11) 0 + . . . 1 − ⟨ U ⟩ 0 + 12 ⟨ U 2 ⟩ 0 + . . . . ( 78 ) Truncating each series in numerator and denominator at a certain order in 1 / N gives a Pade - Approximant to the full observable average ( Bender et al . , 1999 ) . Alternatively , this can be ex - pressed in terms of a cumulant expansion ( Kardar , 2007 ) ⟨O⟩ = ∞ (cid:88) k = 0 ( − 1 ) k k ! (cid:10) O U k (cid:11) c 0 , ( 79 ) where (cid:10) O U k (cid:11) c 0 are the connected correlations , or alternatively the cumulants . The first two con - nected correlations have the form ⟨O U ⟩ c 0 = ⟨O U ⟩ 0 − ⟨O⟩ ⟨ U ⟩ 0 (cid:10) O U 2 (cid:11) c 0 = (cid:10) O U 2 (cid:11) 0 − 2 ⟨O U ⟩ 0 ⟨ U ⟩ 0 − ⟨O⟩ 0 (cid:10) U 2 (cid:11) 0 + 2 ⟨O⟩ ⟨ U ⟩ 20 . ( 80 ) Using Stein’s lemma , we can now attempt to extract the leading O ( N − 1 ) behavior from each of these terms . First , we will note the following useful identity which follows from Stein’s Lemma (cid:10) O ( q ) δ k (cid:11) = N k / 2 (cid:10) O ( q ) ( q − q ∗ ) k (cid:11) ( 81 ) = N k / 2 − 1 [ S ′′ ] − 1 [ ( k − 1 ) (cid:10) O ( q ) ( q − q ∗ ) k − 2 (cid:11) + (cid:10) O ′ ( q ) ( q − q ∗ ) k − 1 (cid:11) ] = ( k − 1 ) [ S ′′ ] − 1 (cid:10) O ( q ) δ k − 2 (cid:11) + 1 √ N [ S ′′ ] − 1 (cid:10) O ′ ( q ) δ k − 1 (cid:11) . ( 82 ) 30 Published as a conference paper at ICLR 2023 Using these this fact , we can find the first few correlation functions of interest ⟨O ( q ) U ⟩ = 3 N − 1 S ( 3 ) [ S ′′ ] − 2 ⟨O ′ ( q ) ⟩ 0 + 3 N − 1 S ( 4 ) [ S ′′ ] − 2 ⟨O ( q ) ⟩ 0 + O ( N − 2 ) (cid:10) O ( q ) U 2 (cid:11) = 15 N − 1 [ S ( 3 ) ] 2 [ S ′′ ] − 3 ⟨O ( q ) ⟩ + O ( N − 1 ) . ( 83 ) Thus , the leading order Pade - Approximant has the form ⟨O ( q ) ⟩ = ⟨O⟩ 0 − 3 N S ( 3 ) [ S ′′ ] − 2 ⟨O ′ ( q ) ⟩ 0 − 3 N S ( 4 ) [ S ′′ ] − 2 ⟨O ( q ) ⟩ 0 + 152 N [ S ( 3 ) ] 2 [ S ′′ ] − 3 ⟨O ( q ) ⟩ 1 − 3 N S ( 3 ) [ S ′′ ] − 2 − 3 N S ( 4 ) [ S ′′ ] − 2 + 152 N [ S ( 3 ) ] 2 [ S ′′ ] − 3 . ( 84 ) H . 1 . 1 DMFT A CTION E XPANSION The logic of the previous section can be extended to our DMFT . We first redefine the action as its negation S → − S to simplify the argument . Concretely , this action S [ q ] defines a Gibbs measure over the order parameters q which we can use to compute observable averages ⟨O ( q ) ⟩ = (cid:82) exp ( − NS [ q ] ) O ( q ) (cid:82) exp ( − NS [ q ] ) ( 85 ) As before , one can Taylor expand the action around the saddle point q ∗ S [ q ] ∼ S [ q ∗ ] + 1 2 ( q − q ∗ ) ∇ 2 S [ q ∗ ] ( q − q ∗ ) + . . . ( 86 ) As before , the linear term vanishes since ∇ q S [ q ∗ ] = 0 at the saddle point q ∗ . We again change variables to δ = √ N ( q − q ∗ ) and express the average as ⟨O⟩ = (cid:82) d δ exp (cid:0) − 12 δ ⊤ ∇ 2 S δ + U ( δ ) (cid:1) O ( δ ) (cid:82) d δ exp (cid:0) − 12 δ ⊤ ∇ 2 S δ + U ( δ ) (cid:1) = ∞ (cid:88) k = 0 ( − 1 ) k k ! (cid:10) O U k (cid:11) c 0 ( 87 ) where ⟨⟩ 0 denotes a Gaussian average over q ∼ N ( q ∗ , 1 N [ ∇ 2 S ] − 1 ) . H . 2 H ESSIAN C OMPONENTS OF DMFT A CTION To gain insight into the Hessian , we will first restrict our attention to the subset of Hessian entries related to Φ ℓ , ˆ Φ ℓ . We again adopt a multi - index notation µ = ( µ , ν , t , s ) so that Φ ℓ µ = Φ ℓµν ( t , s ) ∂ 2 S ∂ Φ ℓ µ ∂ Φ ℓ ′ µ ′ = 0 ∂ 2 S ∂ Φ ℓ µ ∂ ˆΦ ℓ ′ µ ′ = δ ℓ , ℓ ′ δ µ , µ ′ − δ ℓ ′ , ℓ + 1 ∂ ∂ Φ ℓ µ Φ ℓ + 1 µ ′ ∂ 2 S ∂ ˆΦ ℓ µ ∂ ˆΦ ℓ ′ µ ′ = δ ℓ , ℓ ′ (cid:2)(cid:10) ϕ ( h ℓµ ( t ) ) ϕ ( h ℓν ( s ) ) ϕ ( h ℓµ ′ ( t ′ ) ) ϕ ( h ℓν ′ ( s ′ ) ) (cid:11) − Φ ℓ µ Φ ℓ µ ′ (cid:3) . The first equation follows from the fact that ˆ χ has vanishing moments due to the normalization of the probability distribution induced by Z ℓ . Similarly , for the G , ˆ G kernels we have ∂ 2 S ∂G ℓ µ ∂G ℓ ′ µ ′ = 0 ∂ 2 S ∂G ℓ µ ∂ ˆ G ℓ ′ µ ′ = δ ℓ , ℓ ′ δ µ , µ ′ − δ ℓ ′ , ℓ − 1 ∂ ∂G ℓ µ G ℓ − 1 µ ′ ∂ 2 S ∂ ˆΦ ℓ µ ∂ ˆΦ ℓ ′ µ ′ = δ ℓ , ℓ ′ (cid:2)(cid:10) g ℓµ ( t ) g ℓν ( s ) g ℓµ ′ ( t ′ ) g ℓν ′ ( s ′ ) (cid:11) − G ℓ µ G ℓ µ ′ (cid:3) . Proceeding in a similar manner , we can compute all off - diagonal components such as ∂ 2 S ∂ Φ ∂ ˆ G and ∂ 2 S ∂ ˆΦ ∂ ˆ G . Once all entries are computed , one can seek an inverse of the Hessian to obtain the covariance of the order parameters . 31 Published as a conference paper at ICLR 2023 H . 3 S INGLE S AMPLE N EXT - TO - L EADING O RDER P ERTURBATION T HEORY In order to obtain exact analytical expressions , we will consider L - hidden layer ReLU and linear neural networks in the lazy regime trained on a single sample with K x = | x | 2 D = 1 . To ensure preservation of norm , we will use ϕ ( h ) = √ 2 h Θ ( h ) for ReLU and ϕ ( h ) = h for linear networks . First , we note that in either case , the infinite width saddle point equations give Φ ℓ = (cid:10) ϕ ( h ) 2 (cid:11) h ∼N ( 0 , Φ ℓ − 1 ) = Φ ℓ − 1 , Φ 0 = 1 G ℓ = (cid:68) ˙ ϕ ( h ) 2 z 2 (cid:69) h , z = G ℓ + 1 , G L + 1 = 1 = ⇒ Φ ℓ = 1 , G ℓ = 1 , ∀ ℓ ∈ [ 1 , . . . , L ] . ( 88 ) At large but finite width , the kernels therefore fluctuate around this typical mean value of Φ ℓ = 1 and G ℓ = 1 . We now compute the necessary ingredients to invert the Hessian V ϕ = (cid:10) ϕ ( h ) 4 (cid:11) − Φ 2 = (cid:26) 5 ReLU 2 Linear V g = (cid:68) ˙ ϕ ( h ) 4 z 4 (cid:69) − G 2 = (cid:26) 5 ReLU 2 Linear . ( 89 ) Next , we compute the sensitivity of each layer’s kernel to the previous layer ∂ ∂ Φ ℓ Φ ℓ + 1 = 1 , ∂ ∂G ℓ + 1 G ℓ = 1 . ( 90 ) First , let’s analyze the marginal covariance statistics for Φ = Vec { Φ ℓ } Lℓ = 1 and ˆ Φ = Vec { ˆΦ ℓ } Lℓ = 1 . We note that the DMFT action has Hessian components H Φ = (cid:20) ∇ 2 Φ S ∇ 2 ΦˆΦ S ∇ 2 ˆΦΦ S ∇ 2 ˆΦ S (cid:21) = (cid:20) 0 U U ⊤ V ϕ I (cid:21) , U =   1 − 1 0 . . . 0 0 0 1 − 1 . . . 0 0 . . . . . . . . . . . . . . . . . . 0 0 0 . . . 1 − 1 0 0 0 . . . 0 1   . ( 91 ) We seek a ( physical ) inverse C which has vanishing lower diagonal entry , indicating zero variance in the dual order parameters ˆΦ . This gives us the following linear equations H Φ C = (cid:20) 0 U U ⊤ V ϕ I (cid:21) (cid:20) C 11 C 12 C ⊤ 12 0 (cid:21) = (cid:20) I 0 0 I (cid:21) = ⇒ UC ⊤ 12 = I , U ⊤ C 11 + V ϕ C ⊤ 12 = 0 . ( 92 ) The relevant entry is C 11 = − V ϕ [ U ⊤ ] − 1 U − 1 . This matrix has the form C 11 = − V ϕ   1 0 0 . . . 0 1 1 0 . . . 0 1 1 1 . . . 0 . . . . . . . . . . . . . . . 1 1 1 . . . 1     1 1 1 . . . 1 0 1 1 . . . 1 0 0 1 . . . 1 . . . . . . . . . . . . . . . 0 0 0 . . . 1   = − V ϕ   1 1 1 . . . 1 1 2 2 . . . 2 1 2 3 . . . 3 . . . . . . . . . . . . . . . 1 2 3 . . . L   . ( 93 ) Using the fact that the covariance is the negative of the Hessian inverse multiplied by 1 / N , we have the following covariance structure for { Φ ℓ } Cov ( Φ ℓ , Φ ℓ ′ ) = 1 N V ϕ min { ℓ , ℓ ′ } . ( 94 ) This result can be interpreted as the covariance of Brownian motion . Following an identical argu - ment , we find Cov ( G ℓ , G ℓ ′ ) = 1 N V g min { L + 1 − ℓ , L + 1 − ℓ ′ } . ( 95 ) We verify these scalings against experiments below in Figure 7 . 32 Published as a conference paper at ICLR 2023 0 2 4 6 8 1 0 2 4 6 8 2 Empirical Covariance 0 2 4 6 8 1 0 2 4 6 8 2 DMFT Covariance ( a ) Cross - Layer Φ ℓ Covariance 0 2 4 6 8 1 0 2 4 6 8 2 Empirical G Covariance 0 2 4 6 8 1 0 2 4 6 8 2 DMFT G Covariance ( b ) Cross - Layer G ℓ Covariance 10 1 10 2 10 3 N 10 2 10 1 10 0 V a r DMFT = 1 DMFT = 2 DMFT = 3 DMFT = 4 DMFT = 5 DMFT = 6 ( c ) NLO - DMFT Φ ℓ Variance Scaling 10 1 10 2 10 3 N 10 2 10 1 10 0 10 1 V a r G DMFT = 1 DMFT = 2 DMFT = 3 DMFT = 4 DMFT = 5 DMFT = 6 ( d ) NLO - DMFT G ℓ Variance Scaling Figure 7 : Verification of kernel fluctuations through next - to - leading - order ( NLO ) perturbation the - ory within DMFT formalism . ( a ) The cross layer covariance structure of { Φ ℓ } in a L = 10 hidden layer ReLU MLP . The empirical covariance was estimated by initializing a large number ( 500 ) of random networks and computing their Φ ℓ kernels . We see that variance for Φ ℓ increases as ℓ in - creases . ( b ) The cross - layer covariance structure of { G ℓ } . The variance of G ℓ is larger for smaller ℓ . ( c ) The predicted variance of Φ ℓ for different layer ℓ and widths N . All layers have variance scaling as 1 / N , consistent with NLO perturbation theory . ( d ) The scaling of G ℓ variance . 33