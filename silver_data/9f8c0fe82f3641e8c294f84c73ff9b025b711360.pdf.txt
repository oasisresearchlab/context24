Automated Content Moderation Increases Adherence to Community Guidelines Manoel Horta Ribeiro EPFL Lausanne , Switzerland manoel . hortaribeiro @ epfl . ch Justin Cheng Meta Menlo Park , CA , USA jcheng @ meta . com Robert West EPFL Lausanne , Switzerland robert . west @ epfl . ch ABSTRACT Online social media platforms use automated moderation systems to remove or reduce the visibility of rule - breaking content . While previous work has documented the importance of manual content moderation , the effects of automated content moderation remain largely unknown . Here , in a large study of Facebook comments ( 𝑛 = 412M ) , we used a fuzzy regression discontinuity design to mea - sure the impact of automated content moderation on subsequent rule - breaking behavior ( number of comments hidden / deleted ) and engagement ( number of additional comments posted ) . We found that comment deletion decreased subsequent rule - breaking behav - ior in shorter threads ( 20 or fewer comments ) , even among other participants , suggesting that the intervention prevented conversa - tions from derailing . Further , the effect of deletion on the affected user’s subsequent rule - breaking behavior was longer - lived than its effect on reducing commenting in general , suggesting that users were deterred from rule - breaking but not from commenting . In contrast , hiding ( rather than deleting ) content had small and sta - tistically insignificant effects . Our results suggest that automated content moderation increases adherence to community guidelines . CCS CONCEPTS • Human - centered computing → Empirical studies in collab - orative and social computing . KEYWORDS content moderation , online platforms , community guidelines 1 INTRODUCTION Many online platforms enforce community guidelines using auto - mated content moderation systems that detect and intervene when rule breaking occurs , i . e . , when user behavior violates community guidelines [ 16 , 35 , 48 , 49 ] . These systems prevent harm by remov - ing or reducing the visibility of rule - breaking content [ 17 ] , e . g . , by reducing the number of people who see such content [ 13 ] . However , content removal or visibility reduction may also affect on - platform user behavior [ 30 , 44 ] . Moderation interventions may increase com - pliance with community guidelines , e . g . , as deleted comments may prevent a conversation from derailing [ 54 ] , or , reversely , backfire and increase rule breaking , e . g . , because sanctioned users perceive the decision as unfair [ 7 ] . Understanding the causal effect of automated content modera - tion practices on user behavior is vital for evaluating these systems’ effectiveness and can inform their design and use . However , mea - suring the causal effect of content moderation is difficult because of the ethical and technical challenges in using randomized experi - ments ( e . g . , A / B testing ) to study content moderation practices [ 44 ] . User Comment Score Classifier O u t c o m e Hidden No intervention Deleted Figure 1 : Comments posted on Facebook are scored by clas - sifiers that measure adherence to community standards . When 𝑆 crosses specific thresholds ( 𝑡 hide and 𝑡 delete in the fig - ure ) , different interventions are applied . Though comments around each threshold are similar , they receive different in - terventions , e . g . , a comment with score 𝑡 delete − 𝜖 is hidden , while a comment with score 𝑡 delete + 𝜖 is deleted . Exploiting this fact , this work measures the impact of these interven - tions on user behavior outcomes by studying the discontinu - ities ( 𝛽 ℎ𝑖𝑑𝑒 and 𝛽 delete ) around the thresholds 𝑡 hide and 𝑡 delete . Allowing some users not to be moderated implies not removing con - tent that may harm others , and malicious actors could exploit the randomization of potential experiments to post harmful content . Previous work has extensively documented the role of “man - ual” content moderation in online communities [ 30 , 41 ] , i . e . , where volunteer moderators find and remove content that breaches com - munity guidelines . Some research has sought to estimate the effect of such manual content moderation on online communities , finding that it positively impacts user behavior [ 42 , 44 ] . Nevertheless , these effects may not generalize to automated , platform - level content moderation . Further , research on content moderation has been typ - ically descriptive [ 4 , 6 , 9 , 12 , 14 , 24 ] rather than causal , and the quasi - experimental designs used in previous work are not readily adapted to an automated setting . For example , some approaches that rely on the randomness in time it takes for human moderators to intervene upon rule - breaking content to estimate the effect of content moderation [ 44 ] do not work for automated systems , in which moderation occurs immediately after content is created . Present work . We study the effect of automatically enforcing community guidelines for violence and incitement for Facebook comments on user behavior with a quasi - experimental approach illustrated in Fig . 1 . We examined subsequent rule - breaking behav - ior and commenting activity among users whose comments were moderated ( user - level scenario ) and among users in threads where these comments were posted ( thread - level scenario ) . Analyzing a r X i v : 2210 . 10454v3 [ c s . C Y ] 16 F e b 2023 Horta Ribeiro et al . User C User B User C User B User A Most Relevant All comments Newest ⚠ Your comment goes against our policy ⛔ You have been restricted for 24 hours or if it happens again , your account may be restricted Agree with the decision Disagree with the decision ( a ) No intervention ( b ) Hide comment ( c ) Delete comment + warn user ( P o s t ) ( C o mm en t s ) Figure 2 : Moderation interventions – We depict a hypothetical scenario where User A writes a rule - breaking comment ( in red ) on a post by User C ( white ) that has received a comment by User B , ( grey ) . Depending on the score a comment receives , ( a ) no intervention may be applied , in which case the comment is posted , ( b ) the comment may be hidden , and it will only be visible if a viewer changes the default comment ranking setting to show all comments , or ( c ) the comment may be deleted and the user who posted the comment warned ( an additional sanction may be applied depending on their previous rule - breaking behavior ) . over 412M comments , we measured the effect of two different in - terventions ( hiding and deletion ; see Fig . 2 ) on outcomes capturing commenting activity and rule - breaking behavior ( see Sec . 4 ) . Specif - ically , we estimated the causal effect of content moderation using a fuzzy regression discontinuity design [ 22 ] , an approach that capi - talizes on two design choices commonly used in automated content moderation systems [ 10 , 46 ] . First , on many platforms , machine learning models that predict whether content breaches community guidelines assign a score of 𝑆 to each piece of content , reflecting the likelihood of that content breaking community guidelines . Second , when these assigned scores are sufficiently high , platforms may automatically enforce community guidelines . In other words , if a score exceeds a predetermined threshold 𝑡 and certain other condi - tions are met , content may be hidden or deleted immediately . 1 Our approach allows us to mimic a randomized control trial around the threshold 𝑡 since content with a score right above the threshold ( i . e . , 𝑆 = 𝑡 + 𝜖 ) is similar to content with a score right below ( i . e . , 𝑆 = 𝑡 − 𝜖 ) , but only the former is automatically intervened upon by the content moderation system ( which may , e . g . , delete it ) . Results . Overall , deleting comments reduced rule - breaking be - havior in the thread where the comment was originally posted . Deleting comments also reduced rule - breaking among users whose comments were deleted , i . e . , other comments in the thread or that the user subsequently posted were hidden and deleted less often after the intervention . At the thread level , deleting rule - breaking comments significantly decreased rule - breaking behavior in threads with 20 or fewer comments before the intervention , even among other participants in the thread . This effect was statistically in - significant for threads with more than 20 comments . Deletion at the user level led to a decrease in subsequent rule breaking and posting activity . But while the decrease in rule breaking persisted 1 The regression discontinuity is fuzzy because there is a chance that units ( i . e . , com - ments ) below the threshold may be treated ( i . e . , intervened upon ) and units above it may not be treated ( i . e . , not intervened upon ) . This can happen because other mecha - nisms can trigger or prevent interventions on comments with scores below or above the thresholds ( e . g . , users may manually report comments under the threshold ; other systems may exclude some comments from intervention ) . with time , the decrease in posting activity waned . In other words , users gradually returned to making posts or comments at a rate similar to before their comments were deleted but were less likely to post comments that would subsequently be hidden or deleted . Hiding ( rather than deleting ) content had small and statistically insignificant effects on subsequent user activity and rule - breaking behavior at both the user and thread levels . Implications . Deletions of rule - breaking content by automated content moderation , as currently applied on Facebook , decrease the subsequent creation of content that goes against community guide - lines . Our results suggest two ways that this may happen . First , users whose comments are deleted are less likely to produce subse - quent rule - breaking content . Second , other users are also less likely to create rule - breaking comments in the thread where the content was deleted . Building on previous work that found that “manual” content moderation [ 42 , 44 ] can prevent rule - breaking behavior , here we show that these effects generalize to automated systems responsible for a substantial fraction of moderation interventions carried out by major social networking platforms [ 16 , 35 , 48 , 49 ] . Though our results are limited in that we can only measure the effect of content moderation interventions triggered by classifiers at the thresholds at which they are applied , this study may clarify their present impact on online platforms such as Facebook . And while automated content moderation systems are typically assessed using precision and recall , this work shows how they may also be evaluated in terms of their effects on subsequent user behavior in an observational manner that does not require experimentation . 2 RELATED WORK Our investigation builds on two bodies of work – research on anti - social behavior and content moderation . Anti - social behavior . Anti - social behavior has been present on social media since its early days [ 11 ] . Given its detrimental effect on people’s lives [ 1 , 13 , 51 ] , a vast body of research has characterized it across a variety of platforms , languages , and contexts [ 8 , 47 , 52 ] . Automated Content Moderation Increases Adherence to Community Guidelines One line of work has used the growing capabilities of machine learning models to detect cyberbullying [ 36 ] , hate speech [ 28 ] , trolling [ 31 ] , and online harassment [ 45 ] . Many commonly - used classifiers generate scores that are subsequently used to determine when intervention is appropriate . For example , Google Perspec - tive’s flagship classifier [ 27 ] outputs a “toxicity” score for short texts that reflects “rude , disrespectful or unreasonable comments that are likely to make someone leave a discussion . ” This score has been used to proactively intervene upon potentially rule - breaking content , for instance , on Coral , an open - source commenting plat - form used in over 100 newsrooms , including the Washington Post and Der Spieger [ 10 ] . But while classifiers for detecting undesirable behavior exist , less research focuses on understanding their impact . For instance , previous work has highlighted how such systems may struggle with context and differences in dialects [ 37 , 40 ] . Another line of research relevant to the present work has exam - ined whether anti - social behavior is “contagious , ” i . e . , studying a user’s likelihood to produce trolling or uncivil content after being exposed to similar content . Findings have been mixed : some papers have found that rule - breaking behavior can spread from comment to comment [ 8 , 29 ] , while others have found null results [ 19 , 39 ] . Content moderation . A majority of prior work on content mod - eration has examined how it occurs at the community level , where members of the community enact sanctions ( e . g . , elected “adminis - trators” on Wikipedia [ 4 ] ) , rather than at the platform level , where centralized agents shape moderation decisions [ 41 ] . This prior work has focused on describing content moderation practices and gover - nance systems in online communities either quantitatively [ 6 , 9 , 12 ] or qualitatively [ 4 , 14 , 24 ] . For instance , research characterizing rule - breaking behavior on Reddit found that some norms were universal while others were unique to specific subreddits [ 6 ] . More aligned with the work at hand is research evaluating the effects of content moderation at both the community and platform levels . At the community level , past research has measured the effectiveness of removing content on Reddit [ 44 ] or providing ex - planations for removals [ 26 ] , finding that both reduced subsequent rule - breaking behavior . Research has also explored how proactive moderation tools like chat modes on Twitch [ 42 ] or post approvals on Facebook groups [ 38 ] can prevent anti - social behavior . Yet other work studied the effect of algorithmic flagging on Wikipedia using a sharp regression discontinuity design , finding that the system leads to more fair outcomes on the platform [ 46 ] , although effects are heterogeneous across language editions with different charac - teristics [ 50 ] . At the platform level , previous work on the effects of content moderation has analyzed soft - moderation strategies , e . g . , flagging news as misinformation [ 34 , 43 , 53 ] , and the effect of deplatforming users and communities from mainstream plat - forms [ 5 , 20 , 25 ] . Relationship between prior and present work . Prior work has studied online moderation , often in a descriptive fashion [ 4 , 6 , 9 , 12 , 14 , 24 ] or manual , community - oriented contexts [ 26 , 38 , 42 , 44 ] . Yet less is known about the impact of large - scale platform - level automated online moderation ( a substantial fraction of moderation interventions carried online [ 16 , 35 , 48 , 49 ] ) . Therefore , the results provided here advance the understanding of the effects of content moderation and clarify if and how automated systems deployed in a large online social network impact user behavior . Further , our work can help clarify whether rule - breaking behavior is contagious . Past work has typically relied on lab - based experimental settings to study the effect of rule - breaking or toxic content on subsequent comments [ 8 , 19 , 29 , 39 ] . In contrast , here we examine the effect of removing or hiding rule - breaking content affects other users on a real social media platform , a setup with greater ecological validity . This work is not the first in using quasi - experimental methods to evaluate the impact of moderation interventions ( e . g . , [ 42 , 44 , 46 ] ) . However , approaches used in other work are not readily applicable to our scenario , e . g . , they assume deterministic interventions [ 46 ] or a random interval until an intervention is applied [ 44 ] . Thus , we propose a fuzzy regression discontinuity approach to estimate the effect of automated moderation on user behavior . This approach could be easily adapted to assess how other automated moderation systems affect subsequent user behavior . 3 BACKGROUND Violence and incitement policy . In this paper , we study a clas - sifier and associated interventions used to help in enforcing Face - book’s community standards for violence and incitement . The pol - icy 2 has the following rationale : “We aim to prevent potential offline harm that may be related to content on Facebook . While we understand that people commonly express disdain or disagreement by threatening or calling for violence in non - serious ways , we remove language that incites or facilitates serious violence . ( . . . ) ” Interventions . In this paper , we study two interventions applied to rule - breaking content in the context of enforcing community guidelines . These interventions , illustrated in Fig . 1 , are applied incrementally . Content whose score is greater than the first thresh - old 𝑡 1 is hidden . Then , if the score crosses the second threshold 𝑡 2 , it is immediately deleted , and a warning is sent to the offending user . To other users , there is no indication that a post was made and later deleted . This approach aims to incrementally intervene upon content , acknowledging that some content that is borderline to community standards may remain in the social network with reduced visibility . 3 Scope . The violence and incitement classifier studied here is only one of the ways that Facebook ensures that content follows commu - nity standards for violence and incitement . Other mechanisms also exist to ensure that content on Facebook adheres to these guide - lines , and other community standards ( e . g . , for hate speech ) are also enforced . These are beyond the scope of this paper . 4 MATERIALS AND METHODS We studied the effect of automatically enforcing community guide - lines with two quasi - experiments ( Fig . 3 ) . A post is a piece of content posted on Facebook , a comment is a response to that piece of con - tent , and a thread comprises comments associated with a post . Data . For both quasi - experiments , we used a dataset of public com - ments and posts posted by adult U . S . users in English between June 1st and August 31st , 2022 . This comprised 412 million comments made in 1 . 5 million posts by 1 . 3 million distinct users . All data was 2 https : / / transparency . fb . com / policies / community - standards / violence - incitement / . 3 https : / / transparency . fb . com / features / approach - to - ranking / types - of - content - we - demote Horta Ribeiro et al . Table 1 : Outcomes considered in this study . Outcome Description Interventions in follow - up period The number of interventions that , in the follow - up period , targeted either the comments made by the user ( in the user - level scenario ) or the subsequent com - ments in the thread ( in the thread - level scenario ) . Comments The number of comments made during the follow - up period . In the user - level scenario , we also include posts . de - identified and analyzed in aggregate , and no individual - level data was viewed by the researchers . Thread level . In this first scenario ( Fig . 3a ) , we studied the im - pact of automatic moderation on the thread where comments were intervened upon . For each post in our data , we looked for the first comment 𝑐 0 whose score was in the 5 percentage point range of either of the two thresholds where the “hide” and the “delete” interventions are applied , i . e . , 𝑆 ∈ [ 𝑡 hide − 0 . 05 , 𝑡 hide + 0 . 05 ] or 𝑆 ∈ [ 𝑡 delete − 0 . 05 , 𝑡 delete + 0 . 05 ] ; recall that 𝑆 ∈ [ 0 , 1 ] . ( As described later , we reweight these data points based on their distance from 𝑡 hide or 𝑡 delete . ) If a thread had no comments that met the above criteria , it was excluded from this analysis . For each comment 𝑐 0 selected this way , we considered all comments made before ( in the pre - assignment period ) and after 𝑐 0 in the same thread ( in the follow - up period ) . After computing the outcome measures using data from the follow - up period , we used fuzzy regression discontinuity ( see Sec . 4 ) to determine the effect of hiding or deleting the comment . To study effect heterogeneity , we considered four different setups in this quasi - experiment , varying ( 1 ) whether we included other com - ments from the author of the selected comment 𝑐 0 when calculating the outcomes of interest in the follow - up period ; and ( 2 ) whether we considered threads that had more than 20 comments . We choose 20 as a cutoff point as it induces an 80 / 20 split , i . e . , around 80 % of the threads have less than 20 comments . A 75 / 25 or 85 / 15 split yielded qualitatively similar results . User level . In the second scenario ( Fig . 3b ) , we studied the impact of automatic moderation on the users whose comments were in - tervened upon . For each user 𝑢 in our data , we looked for the first comment in the study period 𝑐 0 whose score was in the 5 percent - age point range of the “hide” and “delete” thresholds . If a user had no comments meeting the above criteria , they were excluded from this analysis . For each user / comment tuple ( 𝑢 0 , 𝑐 0 ) selected this way , we additionally considered all comments the user 𝑢 0 made in the 𝑘 days before ( in the pre - assignment period ) and after posting 𝑐 0 ( in the follow - up period ) . Again , data from the follow - up period was used to calculate outcomes , and a fuzzy regression discontinuity design was used to determine the effect of the interventions . We studied the heterogeneity of the effect of these interventions in two ways . First , we varied the value of 𝑘 , the number of days in the follow - up period ( we considered 𝑘 ∈ { 7 , 14 , 21 , 28 } ) . Second , we separately considered ( 1 ) users who had not violated community . . . Pre - assignment period t User C User B User A User C 0 Follow - up period User A ( a ) Thread - level scenario User A t 0 User A User A User A User A Pre - assignment period Follow - up period * ( b ) User - level scenario Figure 3 : The study approximates a real experiment where comments were intervened upon at random using observa - tional data using a fuzzy regression discontinuity design . We depict the thread - level and user - level scenarios in ( a ) and ( b ) and describe them in Sec . 4 . In ( b ) , the asterisk denotes the setup where users are suspended , and the suspension period is not considered when calculating the outcomes . guidelines recently and only received a warning after having their comment deleted ; and ( 2 ) users who had violated community guide - lines once in the recent past and were thus suspended from posting on Facebook for a day after their comment was deleted . 4 Outcomes . The outcomes considered in this study are shown in Table 1 . One outcome is associated with subsequent rule - breaking behavior ( interventions ) , while one is associated with subsequent activity on the platform ( comments ) . RegressionDiscontinuityDesigns . Regressiondiscontinuity ( RD ) is a quasi - experimental study design that has been widely used in the social sciences since the 1990s [ 22 ] . Here , we provide an overview of the fuzzy regression discontinuity design ( an exten - sion of RD ) , explaining how we use it for the quasi - experiments described in Section 4 and illustrated in Fig . 3 . Fuzzy Regression Discontinuity ( FRD ) . Let each comment 𝑐 be assigned a score 𝑆 𝑐 ∈ [ 0 , 1 ] , and 𝑋 𝑐 be an indicator variable that equals 1 if the content has been intervened upon and 0 otherwise . If 4 https : / / transparency . fb . com / en - gb / enforcement / taking - action / restricting - accounts / Automated Content Moderation Increases Adherence to Community Guidelines the 𝑆 score is beyond a threshold 𝑡 , the probability of that comment getting intervened upon increases sharply : 𝑃 [ 𝑋 𝑐 = 1 | 𝑆 𝑐 ] = (cid:40) 𝑓 1 ( 𝑆 𝑐 ) if 𝑆 𝑐 ≥ 𝑡 𝑓 0 ( 𝑆 𝑐 ) if 𝑆 𝑐 < 𝑡 where 𝑓 1 ( 𝑎 ) > 𝑓 0 ( 𝑎 ) ∀ 𝑎 . ( 1 ) Note that this is a generalization of sharp regression discontinuity designs , where 𝑓 0 ( 𝑆 𝑐 ) = 0 and 𝑓 1 ( 𝑆 𝑐 ) = 1 , i . e . 𝑃 [ 𝑋 𝑐 = 1 | 𝑆 𝑐 ] jumps from 0 to 1 around the threshold . This is more suited to the scenario we are studying since mechanisms other than the classifier may come into play , e . g . , comments may be removed due to user reports when the score is below the threshold ( 𝑆 𝑐 < 𝑡 ) , and other automated systems may prevent comments above the threshold from being removed when the score is above the threshold ( 𝑆 𝑐 ≥ 𝑡 ) . A directed acyclic graph ( DAG ) illustrating the causal relationship between the score , the treatment , and the outcomes we are interested in measuring is shown in Fig . 4a . The treatment 𝑋 is determined by the score 𝑆 given by the classifier and other factors unobserved in the present study ( represented by 𝑈 ) . The key insight of fuzzy regression discontinuity designs is to estimate the effect of the intervention 𝑋 on the outcome 𝑌 , even with unknown confounders 𝑈 , for comments with scores in the interval 𝑆 𝑐 ∈ [ 𝑡 − 𝜖 , 𝑡 + 𝜖 ] , 𝜖 → 0 . We assume that comments that lie right before or right after the threshold are indistinguishable , but those above the threshold are more likely to receive the treatment than those below . Thus , around the threshold , we can consider a new DAG where there is no arrow 𝑈 → 𝑆 , as shown in Fig . 4b . Here , 𝑆 has a causal effect on 𝑌 only through 𝑋 , and thus we can use the same idea behind instrumental variable ( IV ) designs [ 3 ] to study the effect of 𝑋 on 𝑌 . In IV designs , we estimate the Local Average Treatment Effect ( LATE ) , the treatment effect for the subset of the comments that take the treatment ( i . e . , 𝑋 𝑐 = 1 ) if and only if they were “assigned” to the treatment ( i . e . , 𝑆 𝑐 > 𝑡 ) : 𝐿𝐴𝑇𝐸 = 𝐼𝑇𝑇 𝐼𝑇𝑇 𝑑 , ( 2 ) where ITT is the average effect of assigning comments to the treat - ment group ( regardless of them being treated ) , and 𝐼𝑇𝑇 𝑑 is the proportion of subjects treated when assigned to the treated group . S X Y U ( a ) S X Y U ( b ) Figure 4 : Causal Directed Acyclic Graphs ( DAGs ) illustrat - ing the fuzzy regression discontinuity design . 𝑆 is the score attributed to a comment , 𝑋 is an indicator variable repre - senting whether the comment was intervened upon , 𝑈 are unmeasured confounders , and 𝑌 is the outcome of interest . While estimating the effect of 𝑋 on 𝑌 is not possible in ( a ) , around a specific threshold 𝑡 where there is a discontinuity around the probability of treatment ( 𝑃 [ 𝑋 = 1 | 𝑆 ] ) , we can re - move the arrow 𝑈 → 𝑋 [ see ( b ) ] and use the same idea be - hind instrumental variable designs to measure the effect of 𝑋 on 𝑌 ( see main text ) . 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 X : D e l e t e E [ X | S ] ITT d - 5 % - 2 . 5 % t + 2 . 5 % + 5 % Score S 0 . 25 0 . 30 0 . 35 Y : I n t e r v en t i on s E [ Y | S ] ITT Figure 5 : A real example of our fuzzy regression disconti - nuity approach , considering the output of the violence and incitement classifier as the running variable 𝑆 , deletions as the treatment 𝑋 , and the number of interventions in a 7 day follow - up period as the outcome 𝑌 . We estimate the causal effect as the ratio between two discontinuities ( 𝐼𝑇𝑇 / 𝐼𝑇𝑇 𝑑 ) . 𝐼𝑇𝑇 𝑑 ( top figure ) is the discontinuity in the treatment around the threshold 𝑡 ( i . e . , the probability of deletion ) , while 𝐼𝑇𝑇 ( bottom figure ) is the discontinuity in the outcome of inter - est around the same threshold ( i . e . , the number of interven - tions in a 7 - day follow - up period ) . As 𝑆 is only an instrument close to the threshold 𝑡 , we estimate the LATE at the cutoff point ( LATEC ) , rewriting Equation ( 2 ) as : 𝐿𝐴𝑇𝐸𝐶 = 𝐸 [ 𝑌 𝑐 | 𝑆 𝑐 = 𝑡 + 𝜖 ] − 𝐸 [ 𝑌 𝑐 | 𝑆 𝑐 = 𝑡 − 𝜖 ] 𝐸 [ 𝑋 𝑐 | 𝑆 𝑐 = 𝑡 + 𝜖 ] − 𝐸 [ 𝑋 𝑐 | 𝑆 𝑐 = 𝑡 − 𝜖 ] , 𝜖 → 0 . ( 3 ) In practice , we can estimate the LATEC with 2 - stage least squares regression , i . e . , regressing the treatment 𝑋 on the score 𝑆 ( first - stage ) , and then the outcome 𝑌 on the values ˆ 𝑋 predicted on the first - stage ( second - stage ) , see [ 2 ] for details . However , we do not have infinite data , and we cannot consider only comments with 𝑆 𝑐 ∈ [ 𝑡 − 𝜖 , 𝑡 + 𝜖 ] , 𝜖 → 0 . This creates a bias – variance trade - off in the estimation of the LATEC . On the one hand , the wider the range we consider around the threshold 𝑡 , the more the unmeasured confounders can bias our estimator . On the other hand , the narrower the range , the less data we have , and thus the larger the variance of our estimator . A common solution to navigating this trade - off consists of using a local linear regression [ 18 ] , where data points ( here , comments ) receive importance proportional to how far they are from the thresh - old , using a triangular weighting kernel defined as 𝐾 ( 𝑆 ) = 1 | 𝑆 − 𝑡 | < ℎ (cid:16) 1 − 𝑆 − 𝑡 ℎ (cid:17) , ( 4 ) where ℎ is the bandwidth of the kernel that controls the bias – variance trade - off , and 1 | 𝑆 − 𝑡 | < ℎ is an indicator variable that equals 1 if | 𝑆 − 𝑡 | < ℎ and 0 otherwise . We empirically determine the Horta Ribeiro et al . 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 Standardized Effect Interventions in follow - up period Comments O u t c o m e ( a ) Thread level > 20 / Other > 20 / All 20 / Other 20 / All 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 Standardized Effect ( b ) User level : first offender 7 days 14 days 21 days 28 days 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 Standardized Effect ( c ) User level : repeat offender 7 days 14 days 21 days 28 days Figure 6 : We depict the estimated standardized effect of deleting comments at the thread ( a ) and user level ( b and c ) . Error bars represent 95 % CIs . We show that comment deletions can reduce subsequent activity ( as measured by comments ) and rule - breaking behavior ( as measured by interventions in the follow - up period ) across both ( a ) thread - and ( b / c ) user - level scenarios . bandwidth ℎ , choosing the bandwidth that yields the optimal mean squared error ( MSE ) of the LATEC estimator [ 21 ] . Example . Fig . 5 illustrates our fuzzy regression discontinuity de - sign . It uses a random sample of users who did not previously vio - late community guidelines and examines interventions in a 7 - day follow - up period following the first comment of interest ( 𝑐 0 ) . Fig - ure 5 ( top ) shows the percentage of first comments ( 𝑐 0 ) that received the “Delete” treatment ( i . e . 𝐸 [ 𝑋 | 𝑆 ] ; in the 𝑦 - axis ) for different scores received by first comments 𝑐 0 ( in the 𝑥 - axis ) . Figure 5 ( bottom ) de - picts the outcome “Interventions in the follow - up period” ( 𝐸 [ 𝑌 | 𝑆 ] ; in the 𝑦 - axis ) for different scores by first comments 𝑐 0 ( in the 𝑥 - axis ) . Intuitively , the regression discontinuity design estimates the treatment effect of 𝑋 on 𝑌 around the threshold 𝑡 delete by dividing the discontinuity in 𝐸 [ 𝑌 | 𝑆 ] [ corresponding to the numerator in Eq . ( 2 ) and Eq . ( 3 ) ] by the discontinuity in 𝐸 [ 𝑋 | 𝑆 ] [ corresponding to the denominator in Eq . ( 2 ) and Eq . ( 3 ) ] . Robustness checks . To ensure the validity of our regression dis - continuitydesign , weadditionallyconductseveralrobustnesschecks suggested by guides outlining best practices [ 23 , 32 ] . These robust - ness checks can be found in Appendix A . 5 RESULTS Using FRD , we estimated the effect of the “Hide” and “Delete” in - terventions for the thread - and user - level scenarios . We depict the standardized effects associated with our key findings in Fig . 6 and present all the estimated effects in Table 2 . 5 . 1 Thread level Fig . 6 ( a ) shows the standardized effect of deleting comments on the number of comments and interventions in the follow - up period in the thread - level scenario ) . Comment deletion had a significant effect on both the number of subsequent interventions and the number of subsequent comments in threads that had fewer than 20 posts prior to the intervention . When comments from the original commenter were included ( ≤ 20 / All ) , the intervention reduced the number of comments by − 13 . 16 ( 95 % CI : − 21 . 23 , − 5 . 10 ) and the number of subsequent interventions by − 0 . 946 ( 95 % CI : − 1 . 59 , − 0 . 299 ; see non - standardized effects shown in Table 2 ) . To get a sense of the effect size , we calculated the average number of comments and interventions in the follow - up period received by threads right below the intervention threshold , where 𝑆 ∈ [ 𝑡 delete − 0 . 01 , 𝑡 delete ) . Threads in the ≤ 20 / All scenario just before the threshold received 1 . 5 interventions ( 95 % CI : 1 . 3 , 1 . 9 ) and 27 . 7 comments ( 95 % CI : 23 . 2 , 33 . , 2 ) on average , suggesting that these effects were substantial . Still considering the All scenario , for both outcomes , the effect of deletions was neither substantial nor significant for threads that already had more than 20 comments when the delete interven - tion happened [ e . g . , see > 20 / All in Fig . 6 ( a ) ] . Deleting any single comment may have less of an effect in longer threads because par - ticipants are less likely to see such a comment ( e . g . , because such a comment may already have been hidden or because there are at least 19 other comments to see ) . Effects may also have been more difficult to observe because of the smaller sample size—there were fewer threads with more than 20 comments than threads with 20 or fewer comments . Hiding comments , as opposed to deleting , had small and statistically insignificant effects on the number of subse - quent interventions and comments for all setups considered ( see Table 2 ; we discuss this further in Sec . 6 ) . We additionally computed the same outcomes in the follow - up period , not considering comments by the original commenter to understand if the effect was due to changes in the behavior of the original commenter ( i . e . , the individual who had their comment intervened upon ) or other users in the thread ( scenarios ≤ 20 / Other and > 20 / Other in Fig . 6 ( a ) and Table 2 ) . We found that standardized effects remained qualitatively similar , e . g . comments were reduced − 0 . 069 standard deviations ( SDs ) in the ≤ 20 / All setup vs . − 0 . 037 SDs in the ≤ 20 / Other setup , suggesting that the intervention dis - couraged other users from posting rule - breaking comments . Automated Content Moderation Increases Adherence to Community Guidelines 5 . 2 User level For the user - level scenario , we considered both cases where users did and did not have comments deleted previously . We make this distinction as the interventions for these users differ : “first - time offenders” only receive a warning , whereas “repeat offenders” ad - ditionally have their posting privileges suspended for 24 hours . The suspension period for repeat offenders is not considered in the follow - up period as it could explain behavior differences . User level : first - time offenders . Fig . 6 ( b ) shows the effect of dele - tions for first - time offenders . Again , deleting comments had signifi - cant effects on both outcomes . Considering the 7 days following the intervention , deletion decreased the number of comments by 4 . 6 and decreased the number of subsequent interventions by 0 . 12 . To get a sense of the effect size , we calculated outcomes for users right below the intervention threshold , 𝑆 ∈ [ 𝑡 delete − 0 . 01 , 𝑡 delete ) . These users received on average 0 . 23 interventions in the follow - up period ( 95 % CI : 0 . 21 , 0 . 25 ) and made on average 13 comments ( 95 % CI : 12 . 6 , 13 . 4 ) , suggesting that the effects are substantial . Setups that considered larger intervention periods ( 21 and 28 days ) showed that , while the effect on the subsequent number of comments waned with time ( i . e . , effects were smaller for longer follow - up periods ) , the effect on the number of subsequent interventions was largely stable . This indicates that automated content moderation has pos - itive , long - lasting effects on subsequent rule - breaking behavior . Hiding comments had small and statistically insignificant effects on the number of subsequent interventions and comments . User level : repeat offenders . Fig . 6 ( c ) shows the effect of deletions for repeat offenders . For these users , deleting comments yielded decreases in both the number of interventions and comments in the follow - up period . The wider confidence intervals here may be partially explained by the smaller sample , as fewer users had their comments deleted a second time . Nonetheless , for 3 out of the 4 time periods considered ( 7 , 14 , 28 days ) , we again observed significant effects that were similar in magnitude to the effects observed in the “first - time offender” setup . Considering a 28 - day follow - up period , deletions decreased interventions received by repeat offenders by 0 . 28 ( 95 % CI : − 0 . 48 , − 0 . 078 ) vs . 0 . 192 for first - time offenders . This suggests that deletions are also effective for users who have previously broken community guidelines . Hiding comments had small and statistically insignificant effects on the considered outcomes . 6 DISCUSSION AND CONCLUSION Content moderation systems are essential to the functioning of mainstream social networks [ 15 ] and can prevent harm by removing rule - breaking content before anyone sees or interacts with it [ 17 ] . In this work , we studied how these systems may also positively impact on - platform user behavior . Using a fuzzy regression discon - tinuity design [ 22 ] , we found that comment deletion had substantial and statistically significant effects on subsequent rule - breaking be - havior and user activity . At the user level , for “first - time offenders , ” deletions had long - lasting effects on reducing rule breaking , but only temporary effects on posting activity , suggesting that com - ment moderation does not necessarily require making a trade - off between safety and engagement . This result is qualitatively aligned with the findings of Srinivasan et al . [ 44 ] on the r / ChangeMyView community on Reddit and suggests that automated platform - level moderation may yield the same effects as manual community - level moderation . At the thread level , we found that content moderation reduced rule - breaking activity even for other users who were not intervened upon . This result is qualitatively aligned with previous work suggesting that uncivil behavior is contagious [ 8 , 29 ] , further highlighting the importance of proactive content moderation . We also found that hiding comments did not have substantial or significant effects . This may be linked to an important limitation of our work : we were able to measure the effect of content modera - tion only at the thresholds at which they were applied . The hiding intervention may have a stronger effect at a different threshold . Importantly , this study does not necessarily imply that comment hiding is not useful , as hiding comments can still prevent harm by reducing exposure to borderline content and may have other beneficial effects that we did not measure . In that context , future work could also find ways to estimate the effect of moderation across various thresholds . At the same time , the effects of deletion reported here may also be an underestimate . As interventions on Facebook are “cumulative , ” when we study the effect of deletion , we do not compare “deletion” with “no deletion , ” but instead can only compare “deletion” with “hiding . ” Therefore , it could be that the effect of deleting content is even stronger , but that part of the effect is masked by the “hiding” intervention ( which , as previously stated , might itself be impactful if enacted at higher thresholds ) . Last , our study is also limited in that we consider specific interven - tions enacted only upon U . S . - based Facebook users , with effects that could be heterogeneous across other platforms and countries . Despite the aforementioned limitations , we argue that , even in this specific setting , understanding the impact of in - production content moderation systems is of great importance as a first step toward a more holistic understanding of how automated moderation systems impact online platforms such as Facebook . Last , we argue that the methodology discussed and applied in this paper can be used to assess moderation interventions across different scenarios and platforms . While much of the literature on harmful content has focused on developing methods to accurately detect such content , here we provide a way to measure the effects of deploying these systems ( and their associated interventions ) on our information ecosystem . Acknowledgements . We thank the Core Data Science team at Meta for the useful discussions . West’s lab is partly supported by grants from Swiss National Science Foundation ( 200021 _ 185043 ) , Swiss Data Science Center ( P22 _ 08 ) , H2020 ( 952215 ) , Microsoft Swiss Joint Research Center , and Google , and by generous gifts from Facebook , Google , and Microsoft . REFERENCES [ 1 ] Yavuz Akbulut , Yusuf Levent Şahin , and Bahadır Erişti . 2010 . Cyberbullying victimization among Turkish online social utility members . ( 2010 ) . [ 2 ] Joshua D Angrist and Guido W Imbens . 1995 . Two - stage least squares estimation of average causal effects in models with variable treatment intensity . Journal of the American statistical Association ( 1995 ) . [ 3 ] Joshua D Angrist and Jörn - Steffen Pischke . 2008 . Getting a little jumpy : Regres - sion discontinuity designs . In Mostly Harmless Econometrics . [ 4 ] BrianButler , ElisabethJoyce , andJacquelinePike . 2008 . Don’tlooknow , butwe’ve created a bureaucracy : the nature and roles of policies and rules in wikipedia . In Proceedings of the SIGCHI Cnference on Human Factors in Computing Systems . Horta Ribeiro et al . Table 2 : Summary of the effects across all settings . For the thread - level scenario , the setups where we consider only threads with less than 20 comments are marked with ≤ 20 in the “Setup” column ( vs . > 20 for the setup considering more than 20 comments ) , and the setups where the original commenter is not considered are marked as “Other” ( vs . “All” for when they are ) . For the user - level scenario , the “Setup” column shows the number of days considered in the follow - up period . Stars ∗ indicate statistically significant effects , i . e . 𝑝 < 0 . 05 Effect Effect ( Standardized ) n Intervention Scenario Outcome Setup Delete Thread - level Comments ≤ 20 / All - 13 . 16 ( - 21 . 23 , - 5 . 10 ) ∗ - 0 . 069 ( - 0 . 111 , - 0 . 027 ) ∗ 190885 ≤ 20 / Other - 7 . 41 ( - 13 . 39 , - 1 . 42 ) ∗ - 0 . 037 ( - 0 . 067 , - 0 . 007 ) ∗ 200655 > 20 / All 43 . 22 ( - 82 . 98 , 169 . 41 ) 0 . 025 ( - 0 . 047 , 0 . 096 ) 49645 > 20 / Other - 34 . 03 ( - 183 . 04 , 114 . 97 ) - 0 . 019 ( - 0 . 101 , 0 . 063 ) 52241 Interventions ≤ 20 / All - 0 . 946 ( - 1 . 59 , - 0 . 299 ) ∗ - 0 . 058 ( - 0 . 098 , - 0 . 018 ) ∗ 190885 ≤ 20 / Other - 0 . 876 ( - 1 . 53 , - 0 . 218 ) ∗ - 0 . 049 ( - 0 . 085 , - 0 . 012 ) ∗ 200655 > 20 / All 2 . 27 ( - 3 . 02 , 7 . 56 ) 0 . 036 ( - 0 . 048 , 0 . 119 ) 49645 > 20 / Other 1 . 39 ( - 3 . 97 , 6 . 74 ) 0 . 022 ( - 0 . 064 , 0 . 109 ) 52241 User - level ( first offender ) Comments 7 - 4 . 55 ( - 6 . 00 , - 3 . 11 ) ∗ - 0 . 093 ( - 0 . 123 , - 0 . 064 ) ∗ 162149 14 - 5 . 72 ( - 9 . 25 , - 2 . 19 ) ∗ - 0 . 064 ( - 0 . 104 , - 0 . 025 ) ∗ 112793 21 - 3 . 95 ( - 9 . 38 , 1 . 48 ) - 0 . 030 ( - 0 . 072 , 0 . 011 ) 84592 28 - 1 . 99 ( - 10 . 12 , 6 . 14 ) - 0 . 012 ( - 0 . 059 , 0 . 036 ) 60175 Interventions 7 - 0 . 117 ( - 0 . 151 , - 0 . 084 ) ∗ - 0 . 108 ( - 0 . 139 , - 0 . 077 ) ∗ 162149 14 - 0 . 144 ( - 0 . 197 , - 0 . 090 ) ∗ - 0 . 103 ( - 0 . 141 , - 0 . 065 ) ∗ 112793 21 - 0 . 196 ( - 0 . 270 , - 0 . 123 ) ∗ - 0 . 118 ( - 0 . 162 , - 0 . 074 ) ∗ 84592 28 - 0 . 192 ( - 0 . 291 , - 0 . 092 ) ∗ - 0 . 111 ( - 0 . 169 , - 0 . 053 ) ∗ 60175 User - level ( repeat offender ) Comments 7 - 3 . 37 ( - 8 . 35 , 1 . 61 ) - 0 . 060 ( - 0 . 149 , 0 . 029 ) 29825 14 - 6 . 11 ( - 14 . 02 , 1 . 81 ) - 0 . 056 ( - 0 . 129 , 0 . 017 ) 26596 21 - 11 . 07 ( - 31 . 95 , 9 . 81 ) - 0 . 068 ( - 0 . 196 , 0 . 060 ) 21693 28 - 9 . 84 ( - 42 . 03 , 22 . 34 ) - 0 . 045 ( - 0 . 193 , 0 . 103 ) 18468 Interventions 7 - 0 . 107 ( - 0 . 187 , - 0 . 027 ) ∗ - 0 . 122 ( - 0 . 214 , - 0 . 031 ) ∗ 29825 14 - 0 . 155 ( - 0 . 281 , - 0 . 029 ) ∗ - 0 . 115 ( - 0 . 209 , - 0 . 021 ) ∗ 26596 21 - 0 . 166 ( - 0 . 344 , 0 . 012 ) - 0 . 101 ( - 0 . 210 , 0 . 007 ) 21693 28 - 0 . 277 ( - 0 . 483 , - 0 . 072 ) ∗ - 0 . 151 ( - 0 . 263 , - 0 . 039 ) ∗ 18468 Hide Thread - level Comments ≤ 20 / All 0 . 718 ( - 1 . 93 , 3 . 36 ) 0 . 004 ( - 0 . 011 , 0 . 019 ) 868632 ≤ 20 / Other 0 . 928 ( - 1 . 31 , 3 . 17 ) 0 . 005 ( - 0 . 007 , 0 . 018 ) 907871 > 20 / All - 8 . 90 ( - 67 . 40 , 49 . 60 ) - 0 . 003 ( - 0 . 026 , 0 . 019 ) 300716 > 20 / Other - 10 . 94 ( - 68 . 34 , 46 . 45 ) - 0 . 004 ( - 0 . 025 , 0 . 017 ) 314287 Interventions ≤ 20 / All 0 . 023 ( - 0 . 096 , 0 . 142 ) 0 . 003 ( - 0 . 011 , 0 . 017 ) 868632 ≤ 20 / Other 0 . 049 ( - 0 . 065 , 0 . 163 ) 0 . 006 ( - 0 . 008 , 0 . 019 ) 907871 > 20 / All 0 . 285 ( - 0 . 492 , 1 . 06 ) 0 . 011 ( - 0 . 018 , 0 . 040 ) 300716 > 20 / Other 0 . 241 ( - 0 . 492 , 0 . 974 ) 0 . 009 ( - 0 . 018 , 0 . 036 ) 314287 User - level ( first offender ) Comments 7 - 0 . 568 ( - 1 . 41 , 0 . 272 ) - 0 . 010 ( - 0 . 024 , 0 . 005 ) 723278 14 - 1 . 19 ( - 2 . 90 , 0 . 526 ) - 0 . 011 ( - 0 . 027 , 0 . 005 ) 542780 21 - 1 . 98 ( - 4 . 97 , 1 . 00 ) - 0 . 013 ( - 0 . 031 , 0 . 006 ) 422996 28 - 0 . 350 ( - 6 . 11 , 5 . 41 ) - 0 . 002 ( - 0 . 029 , 0 . 026 ) 291712 Interventions 7 0 . 007 ( - 0 . 011 , 0 . 026 ) 0 . 008 ( - 0 . 012 , 0 . 027 ) 723278 14 0 . 005 ( - 0 . 015 , 0 . 025 ) 0 . 004 ( - 0 . 012 , 0 . 020 ) 542780 21 0 . 018 ( - 0 . 015 , 0 . 050 ) 0 . 012 ( - 0 . 010 , 0 . 033 ) 422996 28 0 . 032 ( - 0 . 007 , 0 . 071 ) 0 . 019 ( - 0 . 004 , 0 . 041 ) 291712 User - level ( repeat offender ) Comments 7 0 . 948 ( - 1 . 11 , 3 . 01 ) 0 . 014 ( - 0 . 017 , 0 . 045 ) 126587 14 2 . 68 ( - 1 . 44 , 6 . 79 ) 0 . 021 ( - 0 . 011 , 0 . 053 ) 122545 21 3 . 30 ( - 3 . 64 , 10 . 24 ) 0 . 017 ( - 0 . 019 , 0 . 053 ) 103467 28 3 . 92 ( - 6 . 41 , 14 . 24 ) 0 . 015 ( - 0 . 025 , 0 . 056 ) 82067 Interventions 7 - 0 . 002 ( - 0 . 034 , 0 . 029 ) - 0 . 003 ( - 0 . 039 , 0 . 033 ) 126587 14 0 . 017 ( - 0 . 025 , 0 . 059 ) 0 . 013 ( - 0 . 018 , 0 . 044 ) 122545 21 0 . 013 ( - 0 . 066 , 0 . 092 ) 0 . 008 ( - 0 . 039 , 0 . 054 ) 103467 28 0 . 000 ( - 0 . 113 , 0 . 113 ) 0 . 000 ( - 0 . 059 , 0 . 060 ) 82067 Automated Content Moderation Increases Adherence to Community Guidelines [ 5 ] EshwarChandrasekharan , UmashanthiPavalanathan , AnirudhSrinivasan , Adam Glynn , JacobEisenstein , andEricGilbert . 2017 . YouCan’tStayHere : TheEfficacy of Reddit’s 2015 Ban Examined Through Hate Speech . Proceedings of the ACM on Human - Computer Interaction CSCW1 ( 2017 ) . [ 6 ] Eshwar Chandrasekharan , Mattia Samory , Shagun Jhaver , Hunter Charvat , Amy Bruckman , Cliff Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s hidden rules : An empirical study of Reddit norm violations at micro , meso , and macro scales . Proceedings of the ACM on Human - Computer Interaction CSCW2 ( 2018 ) . [ 7 ] Jonathan Chang and Cristian Danescu - Niculescu - Mizil . 2019 . Trajectories of blocked community members : Redemption , recidivism and departure . In The World Wide Web conference . [ 8 ] Justin Cheng , Michael Bernstein , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2017 . Anyone can become a troll : Causes of trolling behavior in online discussions . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing . [ 9 ] Jithin Cheriyan , Bastin Tony Roy Savarimuthu , and Stephen Cranefield . 2017 . Norm violation in online communities – A study of Stack Overflow comments . In Coordination , Organizations , Institutions , Norms , and Ethics for Governance of Multi - Agent Systems XIII . [ 10 ] Coral . 2022 . Toxic Comments . https : / / bit . ly / 3How7We . Accessed on 2022 - 08 - 26 . [ 11 ] JulianDibbell . 1994 . Arapeincyberspaceorhowanevilclown , aHaitiantrickster spirit , two wizards , and a cast of dozens turned a database into a society . Ann . Surv . Am . L . ( 1994 ) . [ 12 ] Casey Fiesler , Jialun Jiang , Joshua McCann , Kyle Frye , and Jed Brubaker . 2018 . Reddit Rules ! Characterizing an Ecosystem of Governance . In Proceedings of the International AAAI Conference on Web and Social Media . [ 13 ] Katharine Gelber and Luke McNamara . 2016 . Evidencing the harms of hate speech . Social Identities ( 2016 ) . [ 14 ] Sarah A Gilbert . 2020 . “I run the world’s largest historical outreach project and it’s on a cesspool of a website . ” Moderating a Public Scholarship Site on Reddit : A Case Study of r / AskHistorians . Proceedings of the ACM on Human - Computer Interaction CSCW1 ( 2020 ) . [ 15 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , content moderation , and the hidden decisions that shape social media . [ 16 ] Google . 2022 . YouTubeCommunityGuidelines . https : / / bit . ly / 2ZAyMa9 . Accessed on 2022 - 08 - 04 . [ 17 ] James Grimmelmann . 2015 . The virtues of moderation . Yale JL & Tech . ( 2015 ) . [ 18 ] Jinyong Hahn , Petra Todd , and Wilbert Van der Klaauw . 2001 . Identification and estimation of treatment effects with a regression - discontinuity design . Econo - metrica ( 2001 ) . [ 19 ] Soo - Hye Han and LeAnn M Brazeal . 2015 . Playing nice : Modeling civility in online political discussions . Communication Research Reports ( 2015 ) . [ 20 ] Manoel Horta Ribeiro , Shagun Jhaver , Savvas Zannettou , Jeremy Blackburn , Gianluca Stringhini , Emiliano De Cristofaro , and Robert West . 2021 . Do Platform Migrations Compromise Content Moderation ? Evidence from r / The _ Donald and r / Incels . Proceedings of the ACM on Human - Computer Interaction CSCW2 ( 2021 ) . [ 21 ] Guido Imbens and Karthik Kalyanaraman . 2012 . Optimal bandwidth choice for the regression discontinuity estimator . The Review of Economic Studies ( 2012 ) . [ 22 ] Guido W Imbens and Thomas Lemieux . 2008 . Regression discontinuity designs : A guide to practice . Journal of Econometrics ( 2008 ) . [ 23 ] RobinJacob , PeiZhu , Marie - AndréeSomers , andHowardBloom . 2012 . Apractical guide to regression discontinuity . MDRC ( 2012 ) . [ 24 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - machine collaboration for content regulation : The case of Reddit automoderator . ACM Transactions on Computer - Human Interaction ( TOCHI ) ( 2019 ) . [ 25 ] Shagun Jhaver , Christian Boylston , Diyi Yang , and Amy Bruckman . 2021 . Eval - uating the effectiveness of deplatforming as a moderation strategy on Twitter . Proceedings of the ACM on Human - Computer Interaction CSCW2 ( 2021 ) . [ 26 ] Shagun Jhaver , Amy Bruckman , and Eric Gilbert . 2019 . Does transparency in moderation really matter ? User behavior after content removal explanations on reddit . Proceedings of the ACM on Human - Computer Interaction CSCW ( 2019 ) . [ 27 ] Jigsaw . 2022 . Perspective API . https : / / perspectiveapi . com / . Accessed on 2022 - 08 - 26 . [ 28 ] Douwe Kiela , Hamed Firooz , Aravind Mohan , Vedanuj Goswami , Amanpreet Singh , Pratik Ringshia , and Davide Testuggine . 2020 . The hateful memes chal - lenge : Detecting hate speech in multimodal memes . Advances in Neural Informa - tion Processing Systems ( 2020 ) . [ 29 ] Jin Woo Kim , Andrew Guess , Brendan Nyhan , and Jason Reifler . 2021 . The distorting prism of social media : How self - selection and exposure to incivility fuel online comment toxicity . Journal of Communication ( 2021 ) . [ 30 ] Robert E Kraut and Paul Resnick . 2012 . Building successful online communities : Evidence - based social design . [ 31 ] Srijan Kumar , Francesca Spezzano , and VS Subrahmanian . 2014 . Accurately detecting trolls in slashdot zoo via decluttering . In 2014 IEEE / ACM International Conference on Advances in Social Networks Analysis and Mining . [ 32 ] David S Lee and Thomas Lemieux . 2010 . Regression discontinuity designs in economics . Journal of economic literature ( 2010 ) . [ 33 ] Justin McCrary . 2008 . Manipulation of the running variable in the regression discontinuity design : A density test . Journal of Econometrics ( 2008 ) . [ 34 ] Paul Mena . 2020 . Cleaning up social media : The effect of warning labels on likelihood of sharing false news on Facebook . Policy & internet ( 2020 ) . [ 35 ] Meta . 2022 . How enforcement technology works . https : / / bit . ly / 3ZSlmCF . Ac - cessed on 2022 - 08 - 04 . [ 36 ] RahatIbnRafiq , HomaHosseinmardi , RichardHan , QinLv , ShivakantMishra , and Sabrina Arredondo Mattson . 2015 . Careful what you share in six seconds : Detect - ing cyberbullying instances in Vine . In 2015 IEEE / ACM International Conference on Advances in Social Networks Analysis and Mining ( ASONAM ) . [ 37 ] Manoel Horta Ribeiro , Pedro H Calais , Yuri A Santos , Virgílio AF Almeida , and Wagner Meira Jr . 2018 . Characterizing and detecting hateful users on twitter . In Twelfth international AAAI Conference on Web and Social Media . [ 38 ] Manoel Horta Ribeiro , Justin Cheng , and Robert West . 2022 . Post Approvals in Online Communities . In Proceedings of the International AAAI Conference on Web and Social Media . [ 39 ] Leonie Rösner , Stephan Winter , and Nicole C Krämer . 2016 . Dangerous minds ? Effects of uncivil online comments on aggressive cognitions , emotions , and behavior . Computers in Human Behavior ( 2016 ) . [ 40 ] Maarten Sap , Dallas Card , Saadia Gabriel , Yejin Choi , and Noah A Smith . 2019 . The risk of racial bias in hate speech detection . In Proceedings of the 57th annual meeting of the Association for Computational Linguistics . [ 41 ] Joseph Seering . 2020 . Reconsidering community self - moderation : the role of research in supporting community - based models for online content moderation . Proceedings of the ACM on Human - Computer Interaction ( 2020 ) . [ 42 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping pro and anti - socialbehaviorontwitchthroughmoderationandexample - setting . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing . [ 43 ] Haeseung Seo , Aiping Xiong , and Dongwon Lee . 2019 . Trust it or not : Effects of machine - learning warnings in helping individuals mitigate misinformation . In Proceedings of the 10th ACM Conference on Web Science . [ 44 ] Kumar Bhargav Srinivasan , Cristian Danescu - Niculescu - Mizil , Lillian Lee , and Chenhao Tan . 2019 . Content removal as a moderation strategy : Compliance and other outcomes in the changemyview community . Proceedings of the ACM on Human - Computer Interaction CSCW3 ( 2019 ) . [ 45 ] Wessel Stoop , Florian Kunneman , Antal van den Bosch , and Ben Miller . 2019 . Detecting harassment in real - time as conversations develop . In Proceedings of the Third Workshop on Abusive Language Online . [ 46 ] Nathan TeBlunthuis , Benjamin Mako Hill , and Aaron Halfaker . 2021 . Effects of algorithmic flagging on fairness : quasi - experimental evidence from Wikipedia . Proceedings of the ACM on Human - Computer Interaction CSCW1 ( 2021 ) . [ 47 ] Kurt Thomas et al . 2021 . SoK : Hate , harassment , and the changing landscape of online abuse . In 2021 IEEE Symposium on Security and Privacy ( SP ) . [ 48 ] TikTok . 2022 . Communityguidelinesenforcementreport . https : / / bit . ly / 3QVeAb2 . Accessed on 2022 - 08 - 26 . [ 49 ] Twitter . 2022 . Rules enforcement . https : / / bit . ly / 406aIZh . Accessed on 2022 - 08 - 26 . [ 50 ] LeijieWangandHaiyiZhu . 2022 . HowareML - BasedOnlineContentModeration Systems Actually Used ? Studying Community Size , Local Activity , and Disparate Treatment . In 2022ACMConferenceonFairness , Accountability , andTransparency . [ 51 ] David Wiener . 1998 . Negligent publication of statements posted on electronic bulletin boards : Is there any liability left after Zeran . Santa Clara L . Rev . ( 1998 ) . [ 52 ] Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 . Ex machina : Personal attacks seen at scale . In Proceedings of the 26th international conference on World Wide Web . [ 53 ] Savvas Zannettou . 2021 . " I Won the Election ! " : An Empirical Analysis of Soft Moderation Interventions on Twitter . . In Proceedings of the International AAAI Conference on Web and Social Media . [ 54 ] Justine Zhang , Jonathan Chang , Cristian Danescu - Niculescu - Mizil , Lucas Dixon , Yiqing Hua , Dario Taraborelli , and Nithum Thain . 2018 . Conversations Gone Awry : Detecting Early Signs of Conversational Failure . In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) . A ROBUSTNESS CHECKS Visual analysis . As a first sanity check , we visually inspect dis - continuities in the outcome variables around the thresholds ( e . g . , as shown in Fig . 5 ) . We find that we can visualize the discontinu - ities right around the threshold , as expected in a fuzzy regression discontinuity design . In Fig . 7 , we further show the discontinuities for the thread - level scenario ( setup : ≤ 20 / All ) . Manipulation at the cutpoint . One well - established threat to the validity of RD designs is that individuals may have knowledge Horta Ribeiro et al . 14 16 18 Y : C o mm en t s E [ X | S ] Thread level : 20 / All - 5 % - 2 . 5 % t delete + 2 . 5 % + 5 % Score S 0 . 5 0 . 6 0 . 7 Y : I n t e r v en t i on s E [ X | S ] Figure 7 : Example of the discontinuities in the outcome ( top : comments ; bottom : interventions ) we visually inspected to ensure the validity of our approach . - 5 % - 2 . 5 % t hide + 2 . 5 % + 5 % 0 . 02 0 . 04 0 . 06 D en s i t y Check for manipulation at the " hide " cutpoint - 5 % - 2 . 5 % t delete + 2 . 5 % + 5 % 0 . 02 0 . 04 0 . 06 D en s i t y Check for manipulation at the " delete " cutpoint Figure 8 : Density of the running variable ( i . e . , the score 𝑆 ) around the thresholds where content gets deleted ( 𝑡 delete ) and hidden ( 𝑡 hide ) . about the cutpoint and adjust the running variable ( for us , the score 𝑆 ) to fall right before or right above it . For instance , in our scenario , if users knew exactly what score their comment would receive before posting it , they might re - word until they find it below the threshold . While in our case , we do not consider this threat to be credible , we entertain the hypothesis and conduct the standard robustness checks , inspecting the density of scores around the threshold and conducting the McCrary test [ 33 ] to assess whether the discontinuity in the density of the rating variable at the cutpoint equals zero . We plot the density in Fig . 8 , which shows no indication of manipulation around the threshold ( as does the McCrary test , where 𝑝 > 0 . 05 ) . Placebo FRDs . A key assumption of FRD is that around the thresh - old , units are exactly the same except for the fact that those above the threshold have an increased chance of receiving the treatment . As such , it is commonplace ( e . g . , see [ 23 ] ) to repeat the entire FRD analysis considering a variable that the treatment should not im - pact . If comments below and above the threshold are comparable , we should not see significant differences for these placebo FRDs . In our case , we run placebo FRDs considering the same outcome variables calculated in the pre - assignment period ( see Fig . 3 ) , where no intervention occurred . Thus , we should expect no discontinuity 1 % b * + 1 % 0 . 1 0 . 0 0 . 1 S t anda r d i z ed e ff e c t Y : Interventions Scenario : Thread - level Setup : 20 / All 1 % b * + 1 % Y : Interventions Scenario : Thread - level Setup : 20 / All 1 % b * + 1 % Kernel bandwidth 0 . 1 0 . 0 0 . 1 S t anda r d i z ed e ff e c t Y : Interventions Scenario : Thread - level Setup : 20 / All 1 % b * + 1 % Kernel bandwidth Y : Interventions Scenario : Thread - level Setup : 20 / All Figure 9 : We show how the standardized effect ( 𝑦 - axis ) of four of the regression discontinuity designs vary with slight changes to the kernel bandwidth . 𝑏 ∗ corresponds to the MSE - optimal bandwidth for each FRD ( as described in [ 21 ] ) . in the outcomes . Results are reported in Table 3 . We find that effects in the placebo FRDs are small and not statistically significant , i . e . , 𝑝 > 0 . 05 , suggesting that , indeed , comments below and above the threshold are comparable . Varying the bandwidth . Finally , to ensure our findings were ro - bust to slight changes in the kernel bandwidth , we repeated the FRDs with varying bandwidth sizes . In Fig . 9 we show the changes in the standardized effect for four of the FRDs carried ( at both the user level and the thread level ) when varying the kernel bandwidth around the MSE - optimal bandwidth 𝑏 ∗ . Overall , we find that our results are robust to slight changes in the kernel bandwidth . Automated Content Moderation Increases Adherence to Community Guidelines Table 3 : Results for placebo FRDs . This table is exactly the same as Table 2 except that outcomes are calculated in the pre - assignment period and thus should not differ between treatment and control groups ( i . e . , those whose comment fell above or below the intervention thresholds ) . Effect Effect ( Standardized ) n Intervention Scenario Outcome Setup Delete Thread - level Comments ≤ 20 / All - 0 . 125 ( - 0 . 319 , 0 . 068 ) - 0 . 024 ( - 0 . 062 , 0 . 013 ) 190885 ≤ 20 / Other - 0 . 153 ( - 0 . 336 , 0 . 031 ) - 0 . 030 ( - 0 . 065 , 0 . 006 ) 200655 > 20 / All - 11 . 87 ( - 41 . 36 , 17 . 62 ) - 0 . 022 ( - 0 . 076 , 0 . 033 ) 49645 > 20 / Other - 20 . 34 ( - 49 . 02 , 8 . 33 ) - 0 . 046 ( - 0 . 111 , 0 . 019 ) 52241 Interventions ≤ 20 / All 0 . 003 ( - 0 . 011 , 0 . 016 ) 0 . 006 ( - 0 . 022 , 0 . 034 ) 190885 ≤ 20 / Other 0 . 001 ( - 0 . 013 , 0 . 015 ) 0 . 002 ( - 0 . 028 , 0 . 032 ) 200655 > 20 / All 0 . 038 ( - 0 . 429 , 0 . 505 ) 0 . 005 ( - 0 . 061 , 0 . 072 ) 49645 > 20 / Other - 0 . 005 ( - 0 . 450 , 0 . 441 ) - 0 . 001 ( - 0 . 067 , 0 . 065 ) 52241 User - level ( first offender ) Comments 7 - 0 . 186 ( - 1 . 77 , 1 . 39 ) - 0 . 004 ( - 0 . 038 , 0 . 030 ) 162149 14 - 0 . 545 ( - 3 . 38 , 2 . 29 ) - 0 . 007 ( - 0 . 045 , 0 . 031 ) 112793 21 0 . 569 ( - 4 . 11 , 5 . 25 ) 0 . 006 ( - 0 . 042 , 0 . 054 ) 84592 28 3 . 68 ( - 2 . 35 , 9 . 71 ) 0 . 029 ( - 0 . 019 , 0 . 078 ) 60175 Interventions 7 0 . 013 ( - 0 . 008 , 0 . 035 ) 0 . 019 ( - 0 . 012 , 0 . 050 ) 162149 14 0 . 001 ( - 0 . 029 , 0 . 031 ) 0 . 001 ( - 0 . 036 , 0 . 039 ) 112793 21 - 0 . 012 ( - 0 . 052 , 0 . 029 ) - 0 . 013 ( - 0 . 057 , 0 . 031 ) 84592 28 - 0 . 008 ( - 0 . 065 , 0 . 049 ) - 0 . 008 ( - 0 . 063 , 0 . 048 ) 60175 User - level ( repeat offender ) Comments 7 - 0 . 369 ( - 5 . 73 , 4 . 99 ) - 0 . 007 ( - 0 . 108 , 0 . 094 ) 29825 14 - 0 . 906 ( - 7 . 57 , 5 . 76 ) - 0 . 010 ( - 0 . 081 , 0 . 061 ) 26596 21 1 . 94 ( - 12 . 45 , 16 . 33 ) 0 . 015 ( - 0 . 095 , 0 . 124 ) 21693 28 2 . 01 ( - 15 . 87 , 19 . 90 ) 0 . 012 ( - 0 . 097 , 0 . 121 ) 18468 Interventions 7 0 . 048 ( - 0 . 022 , 0 . 118 ) 0 . 057 ( - 0 . 026 , 0 . 139 ) 29825 14 0 . 036 ( - 0 . 069 , 0 . 141 ) 0 . 031 ( - 0 . 061 , 0 . 124 ) 26596 21 0 . 086 ( - 0 . 047 , 0 . 219 ) 0 . 062 ( - 0 . 034 , 0 . 159 ) 21693 28 0 . 021 ( - 0 . 128 , 0 . 170 ) 0 . 013 ( - 0 . 081 , 0 . 108 ) 18468 Hide Thread - level Comments ≤ 20 / All - 0 . 054 ( - 0 . 150 , 0 . 042 ) - 0 . 010 ( - 0 . 029 , 0 . 008 ) 868632 ≤ 20 / Other - 0 . 039 ( - 0 . 140 , 0 . 062 ) - 0 . 007 ( - 0 . 027 , 0 . 012 ) 907871 > 20 / All - 18 . 16 ( - 40 . 09 , 3 . 77 ) - 0 . 018 ( - 0 . 041 , 0 . 004 ) 300716 > 20 / Other - 17 . 14 ( - 38 . 37 , 4 . 08 ) - 0 . 017 ( - 0 . 037 , 0 . 004 ) 314287 Interventions ≤ 20 / All 0 . 001 ( - 0 . 005 , 0 . 006 ) 0 . 002 ( - 0 . 014 , 0 . 019 ) 868632 ≤ 20 / Other 0 . 001 ( - 0 . 005 , 0 . 006 ) 0 . 002 ( - 0 . 014 , 0 . 018 ) 907871 > 20 / All - 0 . 021 ( - 0 . 146 , 0 . 104 ) - 0 . 005 ( - 0 . 033 , 0 . 023 ) 300716 > 20 / Other - 0 . 063 ( - 0 . 163 , 0 . 038 ) - 0 . 014 ( - 0 . 036 , 0 . 008 ) 314287 User - level ( first offender ) Comments 7 - 0 . 501 ( - 1 . 28 , 0 . 279 ) - 0 . 009 ( - 0 . 023 , 0 . 005 ) 723278 14 - 0 . 972 ( - 2 . 37 , 0 . 422 ) - 0 . 011 ( - 0 . 027 , 0 . 005 ) 542780 21 - 0 . 877 ( - 3 . 07 , 1 . 32 ) - 0 . 008 ( - 0 . 026 , 0 . 011 ) 422996 28 1 . 30 ( - 2 . 53 , 5 . 14 ) 0 . 008 ( - 0 . 016 , 0 . 033 ) 291712 Interventions 7 0 . 003 ( - 0 . 008 , 0 . 015 ) 0 . 006 ( - 0 . 014 , 0 . 025 ) 723278 14 - 0 . 009 ( - 0 . 025 , 0 . 008 ) - 0 . 013 ( - 0 . 037 , 0 . 011 ) 542780 21 - 0 . 011 ( - 0 . 033 , 0 . 010 ) - 0 . 014 ( - 0 . 042 , 0 . 013 ) 422996 28 - 0 . 003 ( - 0 . 024 , 0 . 018 ) - 0 . 003 ( - 0 . 026 , 0 . 020 ) 291712 User - level ( repeat offender ) Comments 7 0 . 231 ( - 2 . 11 , 2 . 58 ) 0 . 004 ( - 0 . 033 , 0 . 040 ) 126587 14 0 . 766 ( - 3 . 10 , 4 . 63 ) 0 . 007 ( - 0 . 028 , 0 . 042 ) 122545 21 0 . 214 ( - 5 . 26 , 5 . 69 ) 0 . 001 ( - 0 . 034 , 0 . 037 ) 103467 28 3 . 83 ( - 3 . 82 , 11 . 49 ) 0 . 020 ( - 0 . 020 , 0 . 060 ) 82067 Interventions 7 0 . 003 ( - 0 . 021 , 0 . 026 ) 0 . 004 ( - 0 . 028 , 0 . 036 ) 126587 14 0 . 002 ( - 0 . 035 , 0 . 038 ) 0 . 002 ( - 0 . 035 , 0 . 038 ) 122545 21 0 . 029 ( - 0 . 038 , 0 . 096 ) 0 . 024 ( - 0 . 032 , 0 . 079 ) 103467 28 0 . 009 ( - 0 . 047 , 0 . 065 ) 0 . 007 ( - 0 . 034 , 0 . 047 ) 82067