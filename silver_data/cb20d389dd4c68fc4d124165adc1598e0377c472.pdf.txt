a r X i v : 2203 . 06498v8 [ c s . L G ] 2 J un 2022 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning Jessica Hullman 1 , 4 , Sayash Kapoor 2 , Priyanka Nanayakkara 1 , Andrew Gelman 3 , Arvind Narayanan 2∗ ABSTRACT Arguments that machine learning ( ML ) is facing a reproducibil - ity and replication crisis suggest that some published claims in ML research cannot be taken at face value . These concerns inspire analogies to the replication crisis aﬀecting the social and medical sciences . They also inspire calls for the integration of statistical approaches to causal inference and predictive modeling . A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimen - tal science puts the new concerns in perspective , and helps re - searchers avoid “the worst of both worlds , ” where ML researchers begin borrowing methodologies from explanatory modeling with - out understanding their limitations and vice versa . We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exempliﬁed in psychology versus pre - dictive modeling as exempliﬁed in ML . We identify themes that re - occur in reform discussions , like overreliance on asymptotic the - ory and non - credible beliefs about real - world data generating pro - cesses . We argue that in both ﬁelds , claims from learning are im - plied to generalize outside the speciﬁc environment studied ( e . g . , the input dataset orsubject sample , modeling implementation , etc . ) but are often diﬃcult to refute due to underspeciﬁcation of key parts of the learning pipeline . In particular , errors being acknowl - edged in ML expose cracks in long - held beliefs that optimizing pre - dictive accuracy using huge datasets absolves one from having to consider a true data generating process or formally represent un - certainty in performance claims . We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform . CCS CONCEPTS • Computing methodologies → Learning paradigms ; Super - vised learning . KEYWORDS Machine learning , replication , science reform , generalizability . 1 INTRODUCTION The replication crisis in psychology and the social and medical sci - ences has spread to a general concern about scientiﬁc claims that are based on statistical signiﬁcance . Similar attention has recently been drawn to replication challenges regarding empirical claims in ∗ First author is corresponding author . 1 . Northwestern University . { jhullman @ , priyankan @ u . } northwestern . edu 2 . Princeton University . { sayashk @ , arvindn @ cs . } princeton . edu 3 . Columbia University . gelman @ stat . columbia . edu 4 . Microsoft Research New York City . artiﬁcial intelligence ( AI ) and machine learning ( ML ) . There are di - rect concerns about reproducibility —publishedresults cannot be re - produced using the same software and data due to unavailable tun - ing parameters , random seeds , and other conﬁguration settings or computational infrastructure that are not available to outsiders— replication —where re - implementing described methods does not produce the same results due to unacknowledged dependencies , suchas speciﬁcimplementations , and— generalizabilityor robustness — where methods may work well under certain conditions but fail when applied to new problems or in the world [ 169 ] , where vul - nerability to adversarial manipulations may be costly . For exam - ple , the identiﬁcation of examples by which computer vision mod - els could be tricked into misclassiﬁcation by manipulations not visible to the human eye [ 202 ] has inspired subsequent research proposing a variety of explanations for the apparent brittleness of performance ( e . g . , [ 54 , 79 , 96 ] ) . Terms like “alchemy” [ 123 ] and “graduate student descent” are used to describe how researchers combine optimizations to often opaque parameters to achieve per - formance benchmarks . Model performance evaluations are con - ducted without acknowledging sources of error [ 3 , 99 , 138 ] and can involve data ﬁltering decisions that impact achievable accu - racy [ 33 , 137 , 138 ] . Some amount of replication failure is inevitable : the nature of empirical research is to try out ideas that may work in some set - tings but not others . When claims are published , uncertainty about generalizability is inherent . However , once systemic problems are recognized , corrective actionsshouldbetaken , and claims discounted— especially when they cannot be externally reproduced [ 42 , 85 , 125 ] . One way that authors call attention to concerns in ML research is analogizing them to the replication crisis in psychology [ 19 , 36 , 110 , 115 ] . While psychologists discussed fundamental issues with conventional approaches to inference as early as the 1960s [ 50 , 148 ] , in the last decade critics brought concerns to the forefront , demonstrating how motivated researchers can obtain false posi - tives under various conditions [ 75 , 76 , 192 ] and that many pub - lished conclusions about human behavior in psychology research cannot be replicated [ 67 , 161 ] . These revelations spur hard ques - tions about what are necessary conditions for science , how to re - solve uncertainty about published claims , and how to shift incen - tives . Despite their focus on predictive modeling ( and abundant re - cent successes in terms of performance on benchmarks , e . g . , [ 71 ] and adoption in real - world applications , e . g . , [ 145 ] ) , ﬁelds like AI and ML could learn from psychology’s ongoing attempts to diag - nose sources of non - replicability and reform conventional use and 1 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan reporting of methods in the causally - focused explanatory model - ing paradigm prevalent in psychology . 1 Taking a wider perspec - tive on learning failures is well aligned with the idea of integrative modeling , referring to approaches that combine aspects of both paradigms [ 114 , 115 , 177 , 224 ] . For example , social scientists might use predictionalong with explanation toreduceoverﬁtting tonoisy experiment results , while researchers in ﬁelds like ML can incor - porate explanatory methods to ascertain what a model appears to have learned . Integrative modeling acknowledges how researchers frequently misunderstand the relationship between explanation and prediction , assuming , e . g . , that models that succeed in explaining have greater predictive validity [ 224 ] than those that appear less psychologically plausible ( [ 106 , 191 ] as cited in [ 224 ] ) or that a model that achieves high predictive accuracy won’t deviate much from what a human considers to be a plausible decision rule [ 79 ] . But to avoid integrative modeling leading to “the worst of both worlds , ” researchers will need to understand subtle diﬀerences in ways in which inferences can be limited in each paradigm . To date , connections that authors have drawn between these two reform discussions have been piecemeal , leaving it unclear what lessons , if any , might be gained from putting these domains in conversa - tion . To address this gap , we contribute a detailed comparison of lim - itations of inference in causally - driven explanatory versus predic - tive modeling . Our analysis is synthesized from formal and infor - mal arguments made in hundreds of papers we collected through online search , citation tracing , and our involvement in events and scholarly discussions on replication and reproducibility over mul - tiple years . While we surface issues that aﬀect various areas in psy - chology and ML , we ground our discussion around examples from experimental social psychology , which like ML relies on data re - ﬂecting human behavior and uses controlled comparisons to pro - duce claims , and empirical research in supervised discriminative learning ( i . e . , classiﬁcation ) methods , including deep neural nets ( DNNs ) that encapsulate many recent concerns . Our results highlight where concerns across the two domains can stem from similar types of oversights , including overreliance on theory , underspeciﬁcation of learning goals , non - credible be - liefs about real - world data generating processes , overconﬁdence based in conventional faith in certain procedures ( e . g . , random - ization , test - train splits ) , and tendencies to reason dichotomously about empirical results . In both ﬁelds , claims from learning are im - plied to generalize outside the speciﬁc environment studied ( e . g . , the input dataset orsubject sample , modeling implementation , etc . ) but are often impossible to refute due to undisclosed sources of variance in the learning pipeline . We argue in particular that many of the errors recently discussed in ML expose the cracks in long - held beliefs thatoptimizing predictive accuracy using huge datasets absolves one from having to consider a true data generating pro - cess or formally represent uncertainty in performance claims . At 1 Also known asattribution [ 73 ] , and typically involving estimation of regressionsur - faces and assignment of signiﬁcance to individual predictors . the same time , the goals of ML are inherently oriented toward ad - dressing learning failures , suggesting that lessons about irrepro - ducibility could be resolved through further methodological inno - vation in a way that seems unlikely in social psychology . This as - sumes , however , that ML researchers take concerns seriously and avoid overconﬁdence in attempts to reform . We conclude by dis - cussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role that human inductive biases play in learning and reform . 2 BACKGROUND 2 . 1 Anatomy of a learning process An idealized learning process begins with the formulation of goals ( including scientiﬁc goals suchas understanding whatfactorsinﬂu - ence a particular human behavior , engineering goals such as con - structing a better model for machine translation , or policy goals such as estimating eﬀectiveness among diﬀerent types of patients ) and hypotheses . These are not necessarily statistical “hypothe - ses” ; rather , a hypothesis could be that a certain thinking pattern increases the chances of a behavior , or that a certain technical inno - vation will lead to a better translation system , or that a treatment will be more eﬀective among men than women . Goals and hypothe - ses lead tosteps of data collection and preparation . Researchers specify an observational process to collect information about the latent phenomena of interest from the environment . An observa - tional probe is used to induce explicit observations thought to be sensitive to the target phenomena . For example , psychologists de - sign human subjects experiments using interventions thought to interact with the target phenomena . ML researchers often make use of datasets generated from human produced media and signals of behavior , in the form of digital traces . An observational process becomes a model by making assump - tions about what the observed data represent , namely realizations ofrandomvariables with regularvariation . The observationalmodel is deﬁned by a model representation , i . e . , the model class or functional form that speciﬁes a space of data generating processes ( DGPs , i . e . , ﬁtted functions ) that might have produced the data . This might be a multiple linear regression functional form in so - cial psychology , or a more DNN architecture in ML ( with a speciﬁc conﬁguration of network parameters like arrangement into convo - lutions , activation functions , etc . ) . Because quantifying and search - ing all DGPs implied by probability distributions over the observa - tion space tends to be prohibitively complex , learning pipelines of - ten consider a subset or “small world” of model conﬁgurations [ 27 ] , called the hypothesis space of the learner in ML . Model selection or model - based inference describes how a best ﬁt model that is most consistent with the data is determined . This involves deﬁn - ing an objective or loss function measuring the diﬀerence between the ground - truth observed outcome for an input and the predicted outcome of a parameterized model conﬁguration ( e . g . , squared er - ror ) , as well as an optimization method for searching the space of model conﬁgurations to ﬁnd the ﬁtted function that minimizes loss ( e . g . , gradient descent , adaptive optimization algorithms , ana - lytical solutions like maximum likelihood estimation ( MLE ) , etc . ) . 2 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning An evaluation may follow to validate the usefulness of what is learned relative to alternative modelﬁts or learning pipelines . Eval - uation metrics such as explained variance or log loss can be used to summarize overall usefulness of a ﬁtted function . Evaluation metrics may sometimes be implicit , such as when the usefulness of a ﬁtted model is evaluated relative to one’s hypotheses about the data generating process . The learning process culminates in communication of claims in research papers . By “errors in learn - ing , ” we refer to issues that arise in this larger process in which a researcher speciﬁes and “solves” a learning problem . 2 . 2 Goals of learning in social psychology versus machine learning Social psychology . A primary goal in empirical psychology is to describe the causal underpinnings of human behavior [ 148 , 191 , 224 ] . Researchers identify hypotheses representing predictions about variables that constituteobserved data . Often these constitute“weak theories” [ 149 ] , predicting a directional diﬀerence or association between variables but not the size of the eﬀect . They design obser - vational processes to gather data for testing hypotheses , typically controlled human - subjects experiments that record the thoughts , emotions , orbehavior ofsubjects , underdiﬀerent conditionsthought to interact with the latent phenomena of interest . The approxi - mating functions that researchers learn from these observations ( often low dimensional linear regressions ) are thought to capture key structurein the latent psychological phenomena . Claims about cause and eﬀect hinge on interpreting the parameter values of the ﬁtted function in light of hypotheses and their sampling variation within a statistical testing framework . A function is commonly deemed worthy of interest if it’s p - value is below a false - positive rate deﬁned in the Neyman - Pearson framework , typically 𝛼 = 0 . 05 . Direct claims take the form of statements about novel , statistically signiﬁcant causal attributions , and have been called“stylizedfacts” [ 84 , 113 ] implied by authors to be generally true abouthuman behavior . For example , thinking about old age induces old - like behavior [ 13 ] . Machine learning . A primary goal in supervised ML research is to facilitate the learning of functions which achieve high predic - tive accuracy in tasks like classiﬁcation . Researchers hypothesize procedures or abstractions that may improve the state - of - the - art ( SOTA ) in subareas ( e . g . , natural language processing ( NLP ) , vi - sion ) , which is captured by benchmarks : abstractly deﬁned tasks ( e . g . , image classiﬁcation , machine translation ) instantiated with learning problems consisting of datasets ( input , output pairs ) and an evaluation metric to be used as a scoring function ( e . g . , accu - racy ) [ 138 ] . Standard methods like using a train - test split and cross validation are designed to ensure good predictive performance of a ﬁtted model on unseen data . Claims in empirical research pa - pers typically report performance of a new learner ( i . e . , model ) on benchmarks , compared to baselines representing the prior SOTA . Formal proofs of the statistical properties of new methods are also common . 3 THREATS TO LEARNING IN SOCIAL PSYCHOLOGY AND MACHINE LEARNING We describe threats to valid learning according to whether they in - volve data selection and preparation , model development ( includ - ing choosing a representation and a modelselection and evaluation approach ) , and communication of results in a research paper . 3 . 1 Data collection and preparation Social psychology . High measurement error relative to signal , un - acknowledged ﬂexibility in deﬁning data inputs , underspeciﬁed or non - representative subject samples , and underspeciﬁcation of stim - uli generation , and other “design freedoms” can threaten the validity of conclusions drawn in empirical psychology research . The design of many psychology experiments implies that re - searchers do not grasp the implications of using small samples and noisy measurements to draw inferences about eﬀects that are a pri - ori likely to be small . For example , a thought - to - be pervasive belief is that if an experiment registers a “statistically signiﬁcant” eﬀect on a small sample , then that eﬀect will necessarily remain signiﬁ - cant with a larger sample [ 42 , 192 ] . In reality , with a lower powered study , not only is there a lower probability of ﬁnding a true eﬀect of a given size , but there is a lower probability that an observed eﬀect which passes a signiﬁcance threshold actually reﬂects a true eﬀect that will appear under replication [ 42 ] . Under low power , es - timates of observed eﬀects will tend to reﬂect sampling error that derives from the limited size of the sample relative to a target pop - ulation , and forms of measurement error [ 141 ] , such as random variation due to noise in taking measurements that produces a dif - ference between observed and true values . Studies are “dead on arrival” when standard error due to measurement and sampling variation is large relative to any plausible eﬀect size [ 91 ] . Inherent ﬂexibility in how a researcher speciﬁes an analysis is a diﬀerent type of threat . A “researcher degrees of freedom” or “gar - den of forking paths” metaphor [ 88 , 192 ] suggests that human ten - dencies toward self - serving interpretations of ambiguous evidence ( e . g . , [ 11 , 58 ] as cited in [ 192 ] ) , make researchers likely to draw con - clusions that verify their hypotheses . Given an outcome of interest ( e . g . , self - reported political preference ) , an analyst may bias results toward a preferred conclusion by selecting data transformations and outlier removal processes , or choosing between diﬀerent pre - dictor variables or ways of operationalizing the outcome variable , conditional on seeing the results of these choices , without neces - sarily recognizing they are doing anything improper . More broadly , when a researcher can tweak the design of experiment conditions with feedback through pilot experiments via the design of stim - uli , instructions , and elicitation instruments , they may gravitate toward designs that exaggerate eﬀects in some conditions , result - ing in a form of procedural overﬁtting . Scholarshave pointedtostudyresults notbeing reproduciblebe - cause they use non - representative samples of a target population , such as convenience samples of university students from western educated industrialized rich democratic ( WEIRD ) countries [ 112 ] . As researchers have become more accustomed to the importance of statistical power and representative samples , online recruitment of participants in social psychology [ 183 ] has increased . However , 3 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan it is unclear that sample homogeneity is addressed by online sam - ples [ 46 ] and this trend has led to greater use of self - reported mea - sures [ 183 ] that contribute additional noise . More generally , failure torecognize the implications ofnon - random sampling can leadtoa “big data paradox” of overconﬁdence as sample size increases [ 151 ] . Another fundamental but often overlooked issue concerns how psychologists often leave the target population of their inferences unspeciﬁed [ 93 ] , making it ambiguous what is being learned at all . Machine learning . Standardization of benchmarks and the pro - hibitive cost of amassing large datasets means that researchers of - ten rely on existing datasets [ 108 , 200 ] , typically obtained through crowdsourced annotation and web - scale data ( e . g . , [ 61 , 135 ] ) . Simi - lar to psychology , factors like choosing how to transform data after seeing results , the use of non - representative samples , and underspec - iﬁcation of the population captured in data threaten the validity of claims . More frequently discussed issues include the diﬀerential ef - fects of non - random measurement error on real world outcomes when a model is deployed and the way that a “good” predictive model can perpetuate forms of historical bias like stereotypes . Recent work in ML points to analogous concerns to psychology in recent acknowledgement of ﬂexibility in data transformation , such as in ﬁltering data in ways that simplify a prediction prob - lem ( e . g . , removing translation artifacts in machine translation to improve prediction accuracy [ 137 ] as cited in [ 138 ] ) . Non - representative samples are also a concern , including vio - lations of the assumption that the development distribution from which the training and test data are presumed to be randomly drawn is the same as the deployment distribution from which sam - ples will be drawn in real - world applications [ 14 , 165 , 201 ] . ‘Repre - sentation bias” [ 201 ] involves development data that underrepre - sent some parts of the input space of an ML algorithm , leading to higher error rates for less - represented instances in the input space ( e . g . , [ 41 , 164 , 227 ] ) . Suresh and Guttag [ 201 ] deﬁne this bias as a positive value for a measure of divergence between the probability distribution over the input space and the true distribution , noting that it can occur simply as a result of random sampling from a dis - tribution where some groups are in the minority . Others describe how error in the ( often unreported [ 78 ] ) labeling process used to construct ground truth can lead to overﬁtting [ 38 , 160 ] , as well as how data preparation steps lose information whenever majority - rule is used to construct a ground truth without preserving infor - mation about label distributions ( e . g . , describing variance across annotators ) [ 57 , 98 ] . However , criticism of data practices in ML often focuses on sys - tematic measurement error ( i . e . , bias ) in collected data that threat - ens construct validity : whether the measurement is actually cap - turing the intended concept . “Measurement bias” [ 201 ] has been used to refer to diﬀerential measurement error [ 211 ] , where a mea - surement proxy is generated diﬀerently across groups due to dif - fering granularity or quality of data across groups , or reduction of complex target category ( e . g . , academic success ) to a small num - ber of proxies that favor certain groups over others ( e . g . , [ 134 ] as cited in [ 201 ] ) . Jacobs and Wallach [ 126 ] attribute many mislead - ing claims in the fairness literature in ML to unacknowledged mis - matches between unobservable theoretical constructs in ML appli - cations ( e . g . , risk of recidivism , patient beneﬁt ) and the measure - ment proxies that researchers often tend to assume capture them , and suggest the use of latent variable models to formally specify assumptions . A novel concern about measurement bias in ML relative to psy - chology occurs when biased input data are used to train a model and contribute to undesirable social norms . Data may record his - torical biases [ 201 ] ( e . g . , training a model to recognize success - ful applicants on data where women were admitted less due to bias ) . “Harms of representation” [ 1 , 52 ] refers to how model predic - tions can reinforce potentially harmful stereotypes when trained on data exhibiting bias . For example , returning pictures of only white males on a Google search for CEO reinforces notions that other groups are not as appropriate for CEO positions [ 132 ] . The fact that ML is often intended for prescriptive use in the world , rather than descriptive use as in psychology research helps explain the prevalence of these concerns and the emphasis on systematic measurement error . Finally , data concerns in ML increasingly refer to forms of un - derspeciﬁcation of population details and underacknowledgment of the constructed nature of data , instead taking data as given [ 20 , 78 , 122 , 186 ] . These concerns also imply that real - world harms may result from practices that extract away the subjective judg - ments , biases , and contingent contexts involved in dataset produc - tion [ 165 ] . 3 . 2 Model representation Learning from data requires selecting a model representation , a formal representation that deﬁnes what functions can be learned . Social psychology . Researchers commonly overlook the impor - tance that the small world of model conﬁgurations they explore cap - tures or well approximates the true DGP for valid inference , hold un - realistic views about the separability of large eﬀects in the world , and tend to incorporate prior knowledge into modeling informally rather than explicitly . When modeling a latent psychological phenomenon , often via simple measures of correlation and linear parametric models [ 30 , 31 ] , researchers implicitly assume that there is a true DGP that exactly captures how the target arises as a function of other fac - tors thought to inﬂuence it . Once an observational model is de - ﬁned , inference is conﬁned to the mathematical narratives rep - resented by these functions [ 89 ] . However , the validity of claims made about causal eﬀects by following this process depend upon judicious choices about how to represent structure in the true DGP in the constrained small world model space , which psychology re - searchers often overlook [ 213 ] . A ﬁrst complication arises from the fact that inference is more straightforward when the true DGP is included in the small world of conﬁgurations under consideration [ 26 ] . However , the sorts of human behaviors psychologists tend to target are thought to be conceptualizable but too complicated to specify explicitly , or not even conceptualizable [ 213 ] . Under these conditions , the validity of conventional interpretations of ﬁtted models depends on the ob - servational model faithfully approximating the true DGP [ 89 ] . However , this is not the case when a model is structurally mis - speciﬁed , meaning the ﬁtted models do not adequately capture the true causal structure and / or the functional form of the relation - ships between variables in the true DGP [ 213 ] . For example , if the DGP in a psychology study can be described as a weighted sum of 4 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning Social Psychology Machine Learning Data selection and prepara - tion ◦ High measurement errorrelative tothe size ofeffects being studied [ 15 , 62 , 141 ] ◦ Data transformations decided contingent on ( NHST ) results [ 88 , 192 ] ◦ Non - representative [ 112 , 151 ] or underdeﬁned sam - ples [ 93 ] ; insufﬁcient stimuli sampling [ 92 , 218 , 223 ] ◦ Small samples and noisy measurements ( low power ) leading to biased estimates [ 42 ] ◦ Differential measurement error ( e . g . , across social groups ) [ 41 , 164 , 201 , 227 ] which is not modeled [ 126 , 134 ] ◦ Label errors [ 38 , 160 ] and disagreement [ 57 , 98 ] ◦ Data transformations decided contingent on performance com - parisons [ 33 , 137 ] ◦ Underrepresentation of portions of input space in training data [ 14 , 165 , 201 ] ◦ Input data too huge to understand [ 20 , 165 ] Model represen - tation ◦ Overreliance on models and designs with good as - ymptotic guarantees [ 159 ] ◦ No explicit representation of prior / domain knowl - edge [ 80 , 89 ] ◦ Inappropriate expectations [ 49 , 82 , 223 ] in light of crud factor [ 149 , 162 ] ; belief in many nudging fac - tors with large consistent eﬀects on outcome [ 209 ] ◦ Unacknowledged multiplicity of solutions [ 224 ] ◦ Structural misspeciﬁcation [ 144 , 213 ] ◦ Overreliance on asymptotic ( worst - case ) guarantees [ 65 ] ◦ Underspeciﬁcation of desired inductive biases [ 54 , 124 ] ; failure to prevent shortcut learning [ 79 ] ◦ Inappropriate i . i . d . assumption in light of real - world nonsta - tionarity [ 29 , 198 , 220 ] ◦ Reliance on ﬁne - tuning / foundation models for which hyperpa - rameter tuning is opaque [ 64 , 221 ] ◦ Convergence in architectures around large models [ 20 , 32 , 199 ] Model selection and evaluation ◦ Implicit optimization for statistical signifi - cance [ 85 , 87 , 88 , 97 , 121 , 136 ] ◦ Inference as black box [ 93 , 136 , 217 ] ; Not moti - vating choice of estimator or optimization for particular inference goal [ 24 , 215 ] ◦ Misunderstanding / misusing ideas of statistical signiﬁcance [ 81 , 102 , 120 , 121 , 214 ] ◦ Multiple comparisons problem [ 86 ] ◦ Implicit optimization to beat SOTA [ 114 , 188 ] ◦ Knowledge of how OOD test sets are constructed used to choose representation / method [ 207 ] ◦ Overlooked sensitivity of optimizer performance to hyperpa - rameters [ 36 , 47 , 94 , 168 , 187 ] ; computational budget [ 206 ] ◦ Presence of implementation variation [ 138 ] and tricks [ 6 , 111 ] ◦ Misuse of cross validation [ 25 , 45 , 109 , 116 , 207 ] ◦ Optimism of cross validation [ 72 , 146 ] ◦ Loss metric misalignment [ 119 ] ◦ Not comparing to simpler baselines [ 53 , 188 ] or priors [ 101 ] Communica - tion of claims ◦ Unwarranted speculation about what evidence a p value provides [ 204 ] ◦ Overgenerationalization ( i . e . , beyond studied popu - lation ) [ 60 , 105 , 178 , 194 , 223 ] ◦ Unavailable data and code [ 83 , 117 , 153 ] ◦ Not acknowledging having explored multiple analyses conditioned on data [ 86 , 192 ] ◦ Inaccurate descriptions of what p values mean [ 4 , 28 , 90 , 204 ] ◦ Unwarranted speculation about causes [ 131 , 138 , 140 ] ◦ Implying equivalence of learning problems and human perfor - mance on a task [ 131 , 138 , 140 ] ◦ Lack of dataset documentation [ 20 , 78 , 165 ] ◦ Inaccessible data , code , computational resources [ 99 , 182 , 197 ] ◦ Not reporting implementation conditions / sources of vari - ance [ 140 , 188 ] ◦ Underpowered performance comparisons [ 3 , 36 ] ; ignoring sampling error [ 3 , 138 , 173 ] Table 1 : Overview of learning concerns , roughly ordered to emphasize similarities across social psychology and ML . the set of input variables that are represented in the chosen func - tional form , and all of these predictors are exogenous ( i . e . , com - pletely independent ) , then parameters estimated using ordinary least squares can be interpreted according to convention as infor - mation about the target phenomena ( e . g . , comparing two items that diﬀer by one unit in predictor 𝑥 while being the same in all other predictors will diﬀer in 𝑦 by 𝜃 , on average ) . However , when the true DGP is more complex than the functional form , the choice of which potential confounding variables one measures and in - cludes in the regression equation becomes important . Not includ - ing variables that inﬂuence a regressor and the outcome [ 7 ] or in - cluding variables that could in principle be aﬀected by experimen - tal manipulations ( and hence represent outcome variables them - selves [ 51 ] ) cause the conventional interpretation of the ﬁtted pa - rameter values not to hold . However , researchers seldom acknowl - edge these limitations . Researchers often choose designs based on a preference for sim - plermodels . Perhaps the mostcommonexample is preferring between - subject designs based on their asymptotic properties : as the size of the ( random ) sample increases toward the population size , a between - subjects design provides a simpler procedure for estimat - ing average treatment eﬀects relative to a within - subjects design , which requires estimating carryover eﬀects between treatments ex - perienced by the same individual [ 159 ] . However , high variation between people can lead to poor estimates of average treatment eﬀects if the treatment interacts with background variables associ - ated with diﬀerences in individuals and contexts [ 144 , 159 ] . More generally , psychology researchers have been criticized for estimating eﬀects as if they are constant rather than assuming they will vary across people or contexts [ 82 ] . This can manifest , for ex - ample , as model speciﬁcations that ignore the importance of mod - eling variation in stimuli and other experimental conditions as well as subjects [ 223 ] ( e . g . , a “ﬁxed eﬀect fallacy” [ 49 ] ) . 5 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan Tendencies to overlook important sources of variation in mod - eling are implied by Meehl’s conception of the “crud factor” [ 149 , 162 ] , which emphasizes how causal attribution using constrained model spaces to approximate a highly complex true DGP is fun - damentally challenged by the prevalence of “real and replicable correlations” reﬂecting “true , but complex , multivariate and non - theorized causal relationships” between all variables [ 162 ] . Prob - lems arise when researchers overlook model misspeciﬁcation due to conventional but questionable beliefs about reality . For exam - ple , a tendency toward reporting model ﬁts suggesting that novel yet seemingly trivial “nudging” factors ( e . g . , whether or not some - one is menstruating [ 70 ] or whether there was a recent shark at - tack [ 2 ] ) have large and consistent eﬀects on the same outcomes ( e . g . , voting behavior ) overlooks the fact that if such eﬀects were large , we should expect them to interact in complex ways . Hence , we should expect it to be very diﬃcult to observe stable and repli - cable eﬀects [ 209 ] . In this way , choices of model representation ( i . e . , low dimen - sional linear regressions ) are not fully consistent with prior knowl - edge . Conventional approaches to estimating an eﬀect of interest are also memoryless in the sense that even when prior estimates of an eﬀect of interest are available , e . g . , from past experiments , they are generally not incorporated in the model representation . Combined with incentives to publish surprising results [ 74 , 185 ] and the inﬂated probability of observed eﬀects to be overestimates in small sample size studies ( Section 3 . 1 ) , this can result in pub - lished eﬀects that seem suspiciously big in light of prior domain knowledge . Machine learning . In theory , optimizingfor predictive accuracy does notrequirewell - approximatinga trueDGP . However , researchers’ commonly assume that unseen data are drawn from the same distri - bution as trainingdata and use asymptotics to motivate modelchoice , leadingto unrealisticbeliefsabout thepredictabilityof real world pro - cesses . Threats also arise from failures to explicitly represent a priori human expectations about what predictors are valid for a task , and a convergence on hard - to - analyze models that combine pre - trained representations with domain - speciﬁc data . The biggest point of contrast between representations in su - pervised learning in ML and social psychology is that the former traditionally do not assume that the learning process is “realiz - able” [ 181 ] in the sense that the true DGP is in the set of learnable functions ( or hypothesis space ) , nor even that the ﬁtted function approximates the structure of the true DGP . Instead , the goal of learning can be formulatedas identifying a function with error that can be guaranteed to fall within some bound of the best possible predictor over possible samples [ 210 ] . Choosing a representation ( i . e . , hypothesis space ) in theory means reasoning about the induc - tive biases ( i . e . , properties of the predictors ) it will return in light of prior knowledge , but in practice theoretical guarantees ( e . g . , worst - case bounds ) on convergence or generalization ability have driven representation choices . This can lead , as in psychology , to use of models in cases where asymptotic assumptions don’t apply [ 65 ] . Or , as in the case of much recent DL research , a more empirical , performance - driven approach uses achievable performance as the primary driver of model choice [ 174 ] . One of themostcommonly citeddeﬁciencies attributedtomodel representations in applied ML involves assuming a static relation - ship between the predictor variables and the outcome [ 212 ] , which supports conventions like shuﬄing input data to create training and test sets [ 9 ] . This assumption makes models vulnerable to con - cept drift [ 220 ] ( a . k . a . covariate shift [ 29 ] or distribution or dataset shift [ 198 ] ) , where predictions are inaccurate post - hoc due to non - stationarity in the real - world relationship between the inputs and outputs due to temporal changes ( e . g . , [ 143 ] ) , behavioral reactions ( e . g . , [ 167 ] ) , or other unforeseen dynamics [ 170 ] . Under conven - tional “distribution unawareness , ” it also becomes diﬃcult to dis - tinguish when unexpected errors arise from distribution shift ver - sus ineﬃciencies in the learning pipeline [ 23 ] . Distribution shift can lead to poorly calibrated estimates of the uncertainty of model performance [ 163 ] , similar to how choosing estimators by conven - tion rather than guided by one’s inference goal ( see Section 3 . 3 ) biases uncertainty estimates for eﬀects observed in psych experi - ments . Distribution shift motivates greater focus on how diﬀerent mod - els fare at out - of - distribution ( OOD ) error and their robustness to adversarial manipulation , i . e . , small changes to an input in fea - ture space that dramatically change the predicted output ( e . g . , [ 16 , 44 , 179 , 202 , 207 ] ) . Recent results related to adversarial nonrobust - ness [ 124 ] , underspeciﬁcation [ 54 ] , shortcut learning [ 79 ] , simplic - ity bias [ 190 ] , and competency problems [ 77 ] suggest that beliefs about the true DGP in predictive modeling as in ML are not neces - sarily as distinct from explanatory , attribution - oriented modeling as past comparative accounts ( e . g . , [ 39 ] ) imply . For example , one understanding of concept drift that we can relate to the so - called crud factor in psychology is that the con - cept of interest ( or target task ) in an ML pipeline for discrimina - tive learning often depends on a complex combination of features that are not explicitly represented in the model . Geirhos et al . [ 79 ] use “shortcut learning” to refer to a tendency for ML models to learn simple decision rules ( e . g . , [ 10 , 129 , 147 ] ) that perform well on standard benchmarks . While these features represent “real” cor - relations , the problem is that singular predictive features mined in training data often do not perform as well in more challenging test - ing situations , where a human might naturally expect successful performance torequire combinations of features ( e . g . , derived from several diﬀerent object attributes in object recognition ) . Shortcut learning and related vulnerabilities to adversarial manipulation im - ply not a failure in learning from a modeling standpoint , nor even a failure of a ﬁtted function to generalize [ 79 ] , but a mismatch be - tween a human’s conception of critical , stable properties that pre - dict under the true DGP and those that drive the predictions of the ﬁtted model [ 54 , 79 , 124 ] . A related theory representing a symptom of predictive multi - plicity is underspeciﬁcation [ 54 ] : speciﬁcally , a failure to represent in the learning pipeline which inductive biases are more desirable to constrain learning . Underspeciﬁcation occurs when predictors with equivalent performance on i . i . d . data from the same distri - bution as training degrade non - uniformly in performance when probed along practically relevant dimensions [ 54 ] . Underspeciﬁca - tion is distinct from forms of distribution shift that may give rise to shortcut learning , such as the presence of spurious features in the training data that are not associated with the label in other 6 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning settings . Instead , it captures how a single learning problem speciﬁ - cation can support many near - optimal solutions but which might have diﬀerent properties along some human relevant dimensions like fairness or interpretability [ 180 ] . A common approach to overcoming poor generalization of a modelis tocombine multiplerepresentations . Representation learn - ing—automated , untrained learning of input representations ( i . e . , generic priors ) on huge datasets that capture structure in domains like language or vision—reduces the diﬃculty of achieving high ac - curacy in domains where labeled data is costly [ 22 ] . “Fine - tuning” pretrained “foundation” models [ 32 ] for domain - speciﬁc applica - tions has become standard practice based on the performance that can be achieved over conventional domain - speciﬁc learning pipelines [ 104 , 200 , 221 ] . Though training on highly diverse input data tends to provide foundation models with inductive biases that improve ex - trapolation , a challenge is that ﬁne - tuning performance can be highly sensitive tohow poorly - understoodparameters are set , mak - ing results hard to replicate [ 64 ] . For example , the robustness of a ﬁne - tuned model has been found to vary considerably under small changes to hyperparameters [ 221 ] . Related is a concern that the convergence in deep learning research around large DNN model architectures with minimal task - speciﬁc parameters [ 32 ] doubles down on an approachthatimposesunreasonable environmental [ 20 , 199 ] and research opportunity costs [ 20 ] . More generally , understanding the implications of model selec - tion is complex for DNNs , where classical theory falls short of ex - plaining the generalization performance ( e . g . , [ 17 , 18 , 55 , 127 , 226 ] ) . This has motivated lines of theoretical work that explore diﬀerent explanations of phenomena like “double descent” [ 17 ] , where the generalization performance of a deep model continues to improve even after it has achieved zero loss on ( or perfectly interpolated ) the training data . For example , some analyze the properties of over - parameterized linear regressions [ 55 ] . 3 . 3 Model selection and evaluation Model - based inference involves explicit and implicit choices of ob - jective function , optimization approach , and evaluation metric . Social psychology . Claims made in social psychology research are threatened when researchers treat conventional approaches to model - based inference as a black box for consuming data and out - putting inferences [ 12 , 93 , 136 ] , and by researchers’ implicit use of statistical signiﬁcance alone as a criterion for deciding what to re - port . It is relatively rare for psychology research contributions to in - clude explicit motivation for the estimators and loss functions used in modeling . Such “inference by convention” can produce mislead - ing claims without outright cheating or motivated reasoning simi - lar to how blindly preferring between - subjects designs can . For ex - ample , conventional use of maximum likelihood estimators based on their consistency [ 217 ] may lead researchers to overlook criti - cal assumptions required for these estimators to be well - calibrated ( i . e . , have sampling distributions which are asymptotically normal ) . Analytical approaches tooptimizationbring convenience , butcommonly - used approaches to model ﬁtting and selection tend to be based on pre - experimental guarantees ( i . e . , before data are collected ) , which cannot guarantee thatthey willbeappropriate ( e . g . , well - calibrated ) on a particular dataset [ 24 , 215 ] . A diﬀerent source of misleading claims is the use of statisti - cal signiﬁcance as a coarse objective function . Implicit optimiza - tion for signiﬁcance , in which researchers are essentially searching through a garden of forking paths for speciﬁcations that achieve signiﬁcance as a sort of quasi - optimization approach [ 88 ] , means that conventional interpretations of ﬁtted models and statistical tests on parameter estimates will not hold . For example , the multi - ple comparisons problem , in which researchers neglect to control for data - dependent selection in what they report , alters the statisti - cal properties of estimates and tests [ 86 ] . At the highest level , bias aﬀects the published record when researchers decide whether to report results based on the signiﬁcance levels [ 85 , 185 ] . Using “statistical signiﬁcance” as an implicit objective does not line up with scientiﬁc goals ( e . g . , [ 81 , 102 , 120 , 121 , 214 ] ) . For exam - ple , the use of 𝑝 - values and statistical signiﬁcance in psychology research is describedas fundamentally confusedin thatrejection of straw - man null hypotheses is inappropriately taken as evidence in favor of researchers’ preferred alternatives [ 97 , 121 , 136 ] . In other words , hypothesis testing can sometimes be used as a sort of “truth mill” in psychology [ 85 , 87 ] . Related problems include not acknowledging that as a random variable , 𝑝 can vary considerably even under idealizedreplication [ 34 , 90 , 97 , 154 , 189 ] , such that the diﬀerence between signiﬁcant and not signiﬁcant is not itself signiﬁcant . Researchers also overlook the fact that for 𝑝 to be a valid estimate of the probability of ob - serving an eﬀect as large or larger than that seen , all assumptions about the test and observational process must hold [ 5 , 103 , 171 ] . Machine learning . Claims are often made in ML research with - out acknowledging that they depend critically on choices of hyperpa - rameters , initial conditions , and other conﬁguration details that di - rectlyinﬂuenceperformanceinnon - convex optimization . Researchers also may exploit ﬂexibility in designing performance comparisons in order to achieve superior performance for their contributed approach relative to alternatives [ 114 , 188 ] . In contrast to loss functions in simple regression models , ML models tend to have high dimensional non - convex loss . While this does not necessarily prevent generalization [ 48 ] it makes solutions like saddle points , which can give the illusion of a satisfactory lo - cal minimum , of greater concern [ 56 , 184 ] . Optimizers—algorithms that prescribe how to update parameter values like weights during inference to reduce the value of the objective on the training data— are critical to the accuracy gains seen in recent years . However , to make non - convex optimization tractable requires setting various opaque hyperparameters and initial conditions that inﬂuence how the loss landscape is traversed [ 47 , 94 , 187 ] . For an optimization approach like stochastic gradient descent ( SGD ) , hyperparameters like the learning rate aﬀect how quickly it learns the local optima of a function : too high a rate means the function cannot converge , too low and it may require too long [ 36 ] . Adaptive optimizers ( e . g . , Adagrad , Adam ) allow hyperparameters like learning rate to vary for each training parameter , inducing a new dynamical system with each run and complicating attempts to explain what parts of a pipeline improved performance . Hyperparametertuning is alsoa computationallyexpensive task [ 206 ] , inducing uncertainty about how a solution might diﬀer under a larger computational budget or diﬀerent parameter settings . Some 7 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan recent work ﬁnds that given a ﬁxed computational budget , choos - ing the best optimizer for a task with the default parameters per - forms about as well as choosing any widely - used optimizer and tuning its hyperparameters , questioning claims of state - of - the - art performance of newly introduced optimizers across tasks [ 187 ] . Similarly , suﬃcient hyperparameter optimizationcan mostlyelimi - nate claimedperformance diﬀerences in generative adversarial net - works ( GANs ) [ 142 ] , and better hyperparameter tuning on base - line implementations can eliminate evidence of performance ad - vantages of new learning methods [ 111 , 150 ] . Liao et al . [ 138 ] use the broader term “implementation variation” to refer to how vari - ations in how inference techniques are implemented—including use of speciﬁc software frameworks and libraries , metric scores , and implementation “tricks” [ 6 , 111 ] —can aﬀect their performance in evaluations . A related concern in subareas like reinforcement learning is when researchers overlook sources of inherent stochas - ticity in the training process and evaluation environment [ 133 , 155 , 219 ] . Other inference concerns pertain to the external validity of the functions that are learned : will they predict well on unseen data ? In the absence of a theoretical foundation for understanding DNN performance , exploratory empirical research aims to identify prox - ies for properties like learnability and generalizability ( e . g . , [ 127 , 226 ] ) . Recent results show how counter to classical expectations about overﬁtting , minimizing training error without explicit regu - larization over overparameterized models tends to result in good generalization [ 158 , 195 , 226 ] , driving a new theoretical agenda aimed at disentangling optimization methods and statistical prop - erties of the solutions they ﬁnd . Some emergent properties have been criticized . Related to shortcut learning , SGD has been shown to exhibit “simplicity bias”—a preference for learning simple pre - dictors ﬁrst , resulting in neural nets relying exclusively on the sim - plest features , for example , image color and texture , and remain - ing invariant to complex predictive features , for example , object shape [ 130 , 190 ] . Other concerns with external validity arise when an explicitly chosen objective function is not a good proxy for the metric of in - terest in using the models , called “loss - metric misalignment” and threatening generalization [ 119 ] . For example , cross - entropy loss is often used as a loss function , whereas the evaluation metric of interest is often classiﬁcation error or AUCPR . More generally , re - porting single scalar error measures by convention overlooks im - portant error variation ( e . g . , [ 69 ] ) . Internal and external validity is threatened by leakage—broadly , using information from the test data in training—paralleling the reuse of data for choosing and evaluating a model’s ﬁt in psychol - ogy . Leakage can arise through misuse of cross validation when a single CV procedure is used for model tuning and estimating error at once [ 45 , 109 , 116 ] . Failure to carefully consider which steps involved in model training should be performed on each fold during CV can bias error estimates on test data [ 109 ] , as can con - taminating the procedure with future data in time series applica - tions [ 25 ] . More generally , the use of CV for performance evalua - tion has been shown to lead to overoptimistic results in the pres - ence of dependencies between the training and test set under cer - tain conditions [ 72 , 146 ] . Not unlike how low - power experiments lead to overestimates of eﬀects in psychology , under such depen - dencies , underestimating test error from CV becomes more likely . Other issues occur in performance comparisons of models or algorithms . Similar to data issues in psychology , sampling error can be overlooked , including low power in performance compar - isons [ 43 ] and failure to acknowledge that performance estimates on the standard train - test splits common in benchmark datasets may not hold for randomly created train - test splits [ 99 ] . Finally , implicit optimization for good performance results can also occur in ML . Improving performance on benchmark datasets , which have been thought to have caused most major ML research breakthroughs in the last 50 years [ 66 ] , is how researchers show - case improvement in model performance to get published in top conferences and journals [ 172 , 188 ] across ML subareas ( e . g . , [ 61 , 118 , 216 ] ) . This can create incentives for researchers to implicitly optimize inference around a goal of seeing their new technique rank best in performance in an evaluation , such as selectively re - porting results to highlight the best accuracy achieved ( Section 4 ) , choosing among performance measures conditional on results , or failing to acknowledge how simpler baselines perform relative to a new approach ( e . g . , how well the “language prior , ” the prior distri - bution over labels [ 101 ] , performs in a popular visual question an - swering task ( VQA ) [ 8 ] ) . Attemptstouse OOD data toimprove task performance are not valid when researchers rely on explicit knowl - edge of how the OOD splits were constructed or use the OOD test set for model validation [ 207 ] . 4 COMMUNICATION OF CLAIMS Sources of error can remain unacknowledged due to communica - tion norms that suppress uncertainty and limit reproducibility . Social psychology . The contribution of a social psychology ex - periment can be framed as a stylized fact : a statement presumed gen - erally true and replicable [ 84 , 113 ] about some aspect of the world . Results are used to motivate broad claims [ 60 ] , with deﬁciencies at - tributable to authors failing to acknowledge exploration of multiple analysis paths contingent on the data , and tending to downplay in - herent dependencies and uncertainty when describing results . Because stylized facts derive from the results of experiments in laboratory - like environments , often on non - representative sam - ples [ 84 ] , credible reporting would emphasize the speciﬁc condi - tions studied [ 194 ] . Instead , however , researchers routinely state their ﬁndings in broad terms in articles , referring to how an inter - vention or trait aﬀects “people” or entire groups [ 60 , 105 , 178 ] , not acknowledging potential variation untested by theory and data . Authors can perpetuate 𝑝 - value fallacies when they write about eﬀects as if present orabsent ( e . g . , [ 28 ] ) oroverinterpret alternative hypotheses [ 204 ] . Or they may imply that a lack of signiﬁcance is evidence of an absence of eﬀect [ 4 , 28 ] or that there is a signiﬁcant diﬀerence between signiﬁcant and non - signiﬁcant results [ 90 ] . Finally , while sharing of data and analysis code has increased in psychology in recent years , many authors have not adopted such sharing ( e . g . , [ 205 ] ) . When authors don’t publish data or analysis code they used to arrive at a conclusion , readers cannot as easily identify problems or replicate the work , potentially slowing the rate at which errors that invalidate claims are caught [ 83 , 117 , 153 ] . Machine learning . Communication concerns in ML include ten - dencies to not report trial and error over the modeling pipeline and 8 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning evaluation metrics ( leadingto biasedclaims aboutmodelperformance ) andto downplaydependenciesanduncertaintyaﬀectingperformance . MLresearchers often reportpointestimates ofperformancewith - out quantifying uncertainty [ 3 , 138 , 173 ] or reporting key inputs such as hyperparameter and computational budget settings in non - convex optimization . This can result in performance results for whichthe source ofempirical gains isunclear ormisattributed [ 140 ] . As examples , authors often do not report the number of models trained and the negative results found before the one they high - light is selected [ 3 , 188 ] . Authors may cut corners since computing uncertainty and variance in ML models can incur signiﬁcant com - putational costs , especially for large ML models [ 3 , 36 ] . When not presented along with an estimate of the uncertainty of model per - formance arising from sources of variation like the choice of train - test split [ 99 ] , the computational budget [ 63 ] , the choice of hyper - parameter values , and the random initialization of ML models [ 47 , 142 , 187 ] , point estimates of performance represent the best - case rather than expectedmodelperformance . Worse , researchers some - times apply CV to tune a model then report the best performing model’s error on the training set ( i . e . , the “apparent error” ) as if it were cross - validated error [ 157 ] . As with psychology , researchers may be tempted to speculate about causes without couching them in speculative terms [ 140 ] . Overgeneralization occurs from the loose connection between a task ( e . g . , reading comprehension , image classiﬁcation ) given in colloquialand anthropomorphictermsas whata modelhas learned to do , and a more speciﬁc deﬁnition of the problem [ 138 , 140 ] for which publishable results were achieved . For example , using “read - ing comprehension” to refer to a process is misleading when the model may not have used what a human would call critical infor - mation , like the textit is “comprehending” [ 131 ] ( Section 3 . 3 ) . More broadly , claims about performance are rarely evaluated in the con - text of relevant real - world applications [ 138 ] . ML faces analogous issues to the lack of open data and code in social psychology [ 68 , 99 , 107 , 182 , 197 ] . Details about dataset limitations that can threaten external validity [ 20 , 78 , 165 ] are of - ten unreported in ML literature ( Section 3 . 1 ) , perhaps because new techniques for model creation have historically been valued over documenting datasets [ 186 ] . As in psychology , checking compu - tational reproducibility of results requires making the complete code and data available with published papers [ 40 ] . Recent work at - tempts reproducibility checklists , documentation checklists , com - munity challenges , and workshops [ 78 , 152 , 169 ] . However , while assessing replication in the social sciences is not trivial ( e . g . , [ 196 ] ) , a somewhat unique challenge in ML is that with the creation and widespread use of large ML models requiring signiﬁcant compu - tational resources [ 20 ] , especially in NLP tasks , it becomes impos - sible for many researchers to even attempt replicating certain re - sults . 5 IMPLICATIONS : WHAT CAN WE LEARN FROM THIS COMPARISON ? As researchers move toward integrative modeling , they shouldgrasp common blind spots ; the summary in Table 1 roughly orders issues to emphasize where concerns overlap between the two ﬁelds . We see evidence of diﬀerent ways in which researchers place un - due conﬁdence in particular statistical methods . In ML , the use of a train / test split and cross validation can give the illusion that the inherent inability to know performance on unseen data is manage - able . In social psych , belief in the power of randomized sampling and statistical testing leads researchers to overlook the importance of satisfying other assumptions or modeling other forms of varia - tion , like in sampling stimuli . Motivating choices like model rep - resentation using asymptotic theory without considering its appli - cability to the speciﬁc inference problem is conventional . In both cases , researchers’ trust in methods is undergirded by unrealistic expectations about the predictability of real - world behavior and otherphenomena . Socialpsychologists ignore the “crudfactor” [ 148 ] and improbability that multiple predictors thought to have large eﬀects on the same outcome would not also correlate with one another [ 209 ] . ML researchers seem to embrace the crud factor by recognizing the importance of using many predictors to avoid overﬁtting when the signal from any one predictor is likely to be small [ 55 ] , but have been slow to part with i . i . d . assumptions . Norms around what is publishable in each ﬁeld incentivize re - searchers to hack results to meet implicit objectives such as statis - tical signiﬁcance or better - than - SOTA performance , to the detri - ment of practical signiﬁcance or external validity . Important de - pendencies in the analysis process—from types of data ﬁltering and reuse to unacknowledged computational budgets or unspec - iﬁed populations—are often overlooked , so that results do not gen - eralize as assumed . Overgeneralization and suppression of uncer - tainty via binary statements—about the presence of eﬀects or rank of model performance relative to baselines—are common in report - ing results . 5 . 1 Irrefutable claims On a deeper level , claims researchers are making in both ﬁelds ap - pear to be irrefutable both by design and convention . In social psy - chology , this manifests as papers that set out to conﬁrm hypothe - ses that associations will exist , or be in a certain direction , rather than mechanistic accounts that enable more speciﬁc predictions . When hypotheses provide only weak constraints on researchers’ ability to ﬁnd conﬁrming evidence and there is ﬂexibility in the analysis process ( not to mention incentives to publish positive ev - idence on often counterintuitive eﬀects [ 74 , 185 ] ) , “false positive psychology” [ 192 ] is not a surprising result . Consider how much more diﬃcult , even impossible , it for those who wish to refute , rather than support , a given theory : showing no association , for example , means providing evidence for a point prediction of null eﬀect . At the same time , in the absence of well - motivated stimuli sampling strategies , deﬁned target populations , and attempts to model other sources of contextual variation , assuming that claims made about any particular parameter estimates obtained through analyzing experiment results generalize beyond that particular set of participants , stimuli , etc . is not credible . Turning to ML , many reproducibility failures seem to derive from a similar tolerance for irrefutable contributions , manifesting as a confusion between engineering artifacts and scientiﬁc knowl - edge . Consider a typical supervised ML paper that shows that an innovative algorithm , architecture , or model achieves some accu - racy on a benchmark dataset . Even if we assume the reported ac - curacy is not optimistic for the various reasons discussed above , the researcher has contributed an engineering artifact , a tool that 9 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan the practicing engineer can carry in their toolbox based on its su - perior performance to the state - of - the - art on a particular learning problem . New observations based on additional data cannot refute the performance claim of the given algorithm on the dataset , be - cause the population from which benchmark datasets are drawn are rarely speciﬁed to the detail needed for another sample to be drawn [ 128 ] . Attempts to collect a diﬀerent sample from an im - plied population to refute claims are rare ; when they have been at - tempted , researchers have found that the original claims no longer hold [ 175 ] . Further , when researchers have tried to compare model performance across benchmark datasets , they have found that re - sults on one benchmark rarely generalize to another , and can be fragile [ 59 , 208 ] . At a higher level , analogies between human and artiﬁcial intel - ligence are embedded in AI and ML culture , but are hard to ren - der refutable . Without a priori speciﬁcation of the neurocomputa - tional processing involved in high - level cognition [ 176 ] , whether ML approaches capture critical aspects of human consciousness ( e . g . , [ 100 ] ) or new algorithms intended to instantiate human - like mechanisms ( e . g . , [ 21 ] ) succeed in a human - like way is entirely speculative . The acceptance of non - refutable research claims as research con - tributions , as in social psychology , creates a culture in which other methodological issues amplify the diﬃculty of building generaliz - able knowledge . Hubris from beliefs that big data renders model - ing requirements like uncertainty quantiﬁcation unnecessary [ 35 , 59 , 139 ] , a lack of rigor in evaluation [ 138 , 188 ] , and overreliance on theory [ 95 , 140 ] may leave ML plagued with reproducibility and generalization issues . One potential bright spot lies in wide - spread recognition that the ﬁeld is lacking foundational statistical theory to explain DNN performance . This could naturally encour - age a more cautious and empirical mindset among researchers , but only if pressures to make bold claims from structural incentives that encourage “planting one’s ﬂag” before others do [ 188 ] don’t outweigh the trend toward embracing uncertainty . 5 . 2 Latent expectations versus reality Characterizing the conventions that give rise to irrefutable claims as forms of underspeciﬁcation—meaning that some aspect of the learning problem has not been formalized to an extent that allows it to be solved—might help point researchers toward new methods to address what is missing . In particular , the role of human expec - tations in deﬁning “success” in learning has been implicit , but in - novations are often driven by making these expectations explicit . Colloquially , many ML methods are assumed to free researchers from theorizing how well a ﬁtted function captures critical struc - ture in the true DGP . Yet , many weaknesses being identiﬁed sug - gest that the reality of non i . i . d . test data is pushing ML researchers in “purely” predictive areas toward philosophies underlying ex - planation . Recent deﬁnitions of underspeciﬁcation [ 54 ] , shortcut learning [ 79 ] , adversarial vulnerability [ 124 ] , etc . motivatethe need to impose more constraints on what is learned , and the most natu - ral source is the human who assesses and interprets the results . In social psychology , DGPs are modeled , if only as a symptom of using conventional inference . However , we see a displacement of prior knowledge in designing and interpreting experiments , where a priori expectations about how big an eﬀect could be are often overlooked . There is also a failure to acknowledge how the styles of research the ﬁeld rewards , such as showing that many small interventions can have large eﬀects on a class of outcomes , are in - compatible with common sense expectations of correlated eﬀects . There is little reason to believe that taking steps toward integra - tive modeling will greatly improve practice if researchers fail to actively monitor what new methods help them learn for the issues listed in Table 1 . The worst of both worlds would result if instead they assume that any use of integrative approaches must increase rigor , because “now we do statistical testing , ” “now we do human - subjects experiments , ” or “now we use a test / train split . ” Another bright spot in times of methodological crisis is that when mismatch is recognized , it can lead to new technical inno - vations . Paradoxically , striving to identify a single foolproof solu - tion to a recognized learning problem can drive new techniques to close the gap between expectations and reality . For example , in ML recognizing the “brittleness” of deep learning models in light of human perceptions of learning has led to major improvements to generalization from new approaches to adversarial robustness . ML may have an advantage over psychology in addressing repro - ducibility problems in that a ’cat and mouse’ dynamic character - izes many recent successes , where evaluative work is followed by clever changes to the deﬁnition of a learning problem or pipeline that overcome that weakness . Perhaps the most important ques - tion for modern AI and ML is what , if any , forms of mismatch be - tween human expectations and model behavior cannot be solved through a reframing of the learning problem to ﬁnd beneﬁcial new ”hacks . ” 5 . 3 Epistemological gaps and rhetorical risks Itis natural forﬁelds toamass signals thoughttobeproxies oftrust - worthiness to enable judging work at the time of publication , when how well a claim replicates or generalizes is not known . However , a fundamental challenge in doing this is the need for reformers to recognize the incompleteness of their own knowledge . Consider how irrefutable theories and claims induce greater de - pendence on imperfect ways of validating claims . In social psy - chology , replication is an indirect test for whether eﬀects persist under the same or similar conditions . However , experts do not al - ways agree on what constitutes successful replication [ 166 ] , and intuitions can be proven wrong . For example , under a formal def - inition of a study’s reproduciblity rate , reproducing experimental results does not necessarily indicate a “true” eﬀect , and vice versa for a “false” eﬀect [ 62 ] . In ML , tests are similarly indirect , but the stakes often higher : when an approach fails to perform as expected in the world , researchers may scrutinize the original claims , but at the expense of those aﬀected in deployment . Progress can be hard to judge due to the speed of discovery [ 37 ] and conﬂicting valua - tions of standard approaches like benchmarks ( e . g . , [ 172 , 222 ] ) . There is a need to accurately diagnose the fundamental prob - lems , rather than symptoms only , and avoid the sort of part - for - whole substitution in reforms that drive methodological overconﬁ - dence . As ﬁelds work toward consensus views on errors , uncer - tainty must be embraced . For example , debates over what core problems preregistration addresses point to the challenge of de - termining when a given reform should have privileged status [ 156 , 193 , 203 ] . 10 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning Trustinga method ( whetherit bea statisticalidea suchas Bayesian inference or causal identiﬁcation , or an ML idea such as deep learn - ing or cross validation ) without examining the applied context can mislead researchers by implying that better results can be achieved via a singular universal method of statistical inference [ 93 ] . It can be that more careful researchers tend to use more sophisticated methods , which willshow upas acorrelation between methodolog - ical sophistication and the quality of research—but this can also create an opening for methods to be used as a signal of research quality even when that is not the case . For example , it makes sense for open - science reforms to be supported by researchers who do stronger work ( and there is evidence from betting markets that experts can predict reproducibility with some accuracy [ 67 ] ) and opposed by those whose work has failed to replicate ( for exam - ple , [ 225 ] ) . This would lead to open - science practices themselves being a marker of research quality . On the other hand , honesty and transparency are not enough [ 83 ] : all the openness and preregistra - tion in the world won’t endow replicability to a psychology study with a high ratio of noise to signal , which can happen with exper - iments whose designs focus on procedural issues ( e . g . , randomiza - tion ) , to the detriment of theory and measurement . Open - science practices can be a signal of replicability without that holding in the future . Steps can be taken to reduce rhetorical risks . For example , De - vezer et al . [ 62 ] proposeaccompanying colloquialstatements about reproducibility problems and solutions with formal problem state - ments and results , and provide questions toguide researchers in do - ing so . Greater rigor in reform arguments can mean quicker identi - ﬁcation of logical errors , misintepretations of constructs , or other blind spots in attempts to steer a ﬁeld back on track . 6 ACKNOWLEDGMENTS We thank Jake Hofman , Cyril Zhang , Jean Czerlinski Oretga for comments . Hullman is supported by the NSF ( IIS - 1930642 ) and a Microsoft Faculty Fellowship , Narayanan by the NSF ( IIS - 1763642 ) , and Gelman by the ONR . REFERENCES [ 1 ] Mohsen Abbasi , Sorelle A Friedler , Carlos Scheidegger , and Suresh Venkata - subramanian . 2019 . Fairness in representation : quantifying stereotyping as a representational harm . In Proc . of the 2019 SIAM International Conference on Data Mining . SIAM , 801 – 809 . [ 2 ] Christopher H Achen and Larry M Bartels . 2012 . Blind retrospection : Why sharkattacksarebad for democracy . Center for the Study of DemocraticInstitu - tions , Vanderbilt University . Working Paper ( 2012 ) . [ 3 ] RishabhAgarwal , MaxSchwarzer , PabloSamuelCastro , AaronCCourville , and MarcBellemare . 2021 . Deepreinforcementlearningattheedgeofthestatistical precipice . NeurIPS 34 ( 2021 ) . [ 4 ] Douglas G Altman and J Martin Bland . 1995 . Statistics notes : Absence of evi - dence is not evidence of absence . BMJ 311 , 7003 ( 1995 ) , 485 . [ 5 ] Valentin Amrhein , David Traﬁmow , and Sander Greenland . 2019 . Inferential statistics as descriptive statistics : There is no replication crisis if we don’t ex - pect replication . American Statistician 73 , sup1 ( 2019 ) , 262 – 270 . [ 6 ] Marcin Andrychowicz , Anton Raichuk , Piotr Stańczyk , Manu Orsini , Sertan Girgin , Raphaël Marinier , Leonard Hussenot , Matthieu Geist , Olivier Pietquin , Marcin Michalski , Sylvain Gelly , and Olivier Bachem . 2020 . What matters for on - policy deep actor - critic methods ? A large - scale study . In ICLR . [ 7 ] JoshuaDAngristandJörn - SteﬀenPischke . 2008 . MostlyHarmlessEconometrics . Princeton university press . [ 8 ] Stanislaw Antol , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Ba - tra , CLawrenceZitnick , andDeviParikh . 2015 . Vqa : Visualquestionanswering . In Proc . of ICCV . 2425 – 2433 . [ 9 ] Martin Arjovsky , Léon Bottou , Ishaan Gulrajani , and David Lopez - Paz . 2019 . Invariant risk minimization . arXiv : 1907 . 02893 ( 2019 ) . [ 10 ] DevanshArpit , StanisławJastrzębski , NicolasBallas , DavidKrueger , Emmanuel Bengio , Maxinder S Kanwal , Tegan Maharaj , Asja Fischer , Aaron Courville , Yoshua Bengio , et al . 2017 . A closer look at memorization in deep networks . In International Conference on Machine Learning . PMLR , 233 – 242 . [ 11 ] Linda Babcock and George Loewenstein . 1997 . Explaining bargainingimpasse : Theroleofself - servingbiases . J . ofEconomicPerspectives 11 , 1 ( 1997 ) , 109 – 126 . [ 12 ] David Bakan . 1966 . The test of signiﬁcance in psychological research . Psycho - logical Bulletin 66 , 6 ( 1966 ) , 423 . [ 13 ] John A Bargh , Mark Chen , and Lara Burrows . 1996 . Automaticity of social behavior : Direct eﬀects of trait construct and stereotype activation on action . J . of Personality and Social Psychology 71 , 2 ( 1996 ) , 230 . [ 14 ] Solon Barocasand Andrew DSelbst . 2016 . Big data’sdisparateimpact . Califor - nia Law Review 104 ( 2016 ) , 671 . [ 15 ] Roy F Baumeister , Kathleen D Vohs , and David C Funder . 2007 . Psychology as thescienceofself - reportsandﬁngermovements : Whateverhappenedtoactual behavior ? Perspectives on Psychological Science 2 , 4 ( 2007 ) , 396 – 403 . [ 16 ] Sara Beery , Grant Van Horn , and Pietro Perona . 2018 . Recognition in Terra Incognita . In Proc . of the European Conference on Computer Vision ( ECCV ) . 456 – 473 . [ 17 ] Mikhail Belkin , Daniel Hsu , Siyuan Ma , and Soumik Mandal . 2019 . Reconcil - ingmodernmachine - learningpracticeandtheclassicalbias – variancetrade - oﬀ . Proc . of the National Academy of Sciences 116 , 32 ( 2019 ) , 15849 – 15854 . [ 18 ] Mikhail Belkin , Daniel J Hsu , and Partha Mitra . 2018 . Overﬁtting or per - fect ﬁtting ? risk bounds for classiﬁcation and regressionrules that interpolate . NeurIPS 31 ( 2018 ) . [ 19 ] Samuel J Bell and Onno P Kampman . 2021 . Perspectives on Machine Learning from Psychology’s Reproducibility Crisis . arXiv : 2104 . 08878 ( 2021 ) . [ 20 ] Emily M Bender , Timnit Gebru , Angelina McMillan - Major , and Shmargaret Shmitchell . 2021 . On the dangers of stochastic parrots : Can language models be too big ? . In Proc . of FAccT . 610 – 623 . [ 21 ] Yoshua Bengio . 2017 . The consciousness prior . arXiv : 1709 . 08568 ( 2017 ) . [ 22 ] Yoshua Bengio , Aaron Courville , and Pascal Vincent . 2013 . Representation learning : Areviewand newperspectives . IEEETransactionsonPatternAnalysis and Machine Intelligence 35 , 8 ( 2013 ) , 1798 – 1828 . [ 23 ] DavidBerend , XiaofeiXie , LeiMa , LingjunZhou , YangLiu , ChiXu , andJianjun Zhao . 2020 . Catsarenot ﬁsh : Deeplearningtestingcallsforout - of - distribution awareness . In Proc . of ASE . 1041 – 1052 . [ 24 ] James O Berger and Robert L Wolpert . 1988 . The Likelihood Principle . IMS . [ 25 ] Christoph Bergmeir and José M Benítez . 2012 . On the use of cross - validation for time series predictor evaluation . Information Sciences 191 ( 2012 ) , 192 – 213 . [ 26 ] Jose M . Bernardo and Adrian F . M . Smith . 1994 . Bayesian Theory . Wiley . [ 27 ] RyanBernstein . 2021 . Drawingmapsofmodel spacewithmodularStan . ( 2021 ) . https : / / statmodeling . stat . columbia . edu / 2021 / 11 / 19 / drawing - maps - of - model - space - with - modular - stan / [ 28 ] Lonni Besançon and Pierre Dragicevic . 2019 . The continued prevalence of di - chotomous inferences at CHI . In Extended Abstractsof the 2019 CHI Conference on Human Factors in Computing Systems . 1 – 11 . [ 29 ] Steﬀen Bickel , Michael Brückner , and Tobias Scheﬀer . 2009 . Discriminative learning under covariate shift . J . of Machine Learning Research 10 , 9 ( 2009 ) . [ 30 ] MaríaJBlanca , RafaelAlarcón , andRoserBono . 2018 . Currentpracticesindata analysisproceduresin psychology : What has changed ? Frontiersin Psychology 9 ( 2018 ) , 2558 . [ 31 ] Niall Bolger , Katherine S Zee , Maya Rossignac - Milon , and Ran R Hassin . 2019 . Causal processes in psychology are heterogeneous . J . of Experimental Psychol - ogy : General 148 , 4 ( 2019 ) , 601 . [ 32 ] Rishi Bommasani , Drew A Hudson , Ehsan Adeli , Russ Altman , Simran Arora , SydneyvonArx , MichaelSBernstein , JeannetteBohg , AntoineBosselut , Emma Brunskill , et al . 2021 . On the opportunities and risks of foundation models . arXiv : 2108 . 07258 ( 2021 ) . [ 33 ] Daniel Bone , Matthew S Goodwin , Matthew P Black , Chi - ChunLee , KartikAu - dhkhasi , and Shrikanth Narayanan . 2015 . Applying machine learning to facil - itate autism diagnostics : pitfalls and promises . J . of autism and developmental disorders 45 , 5 ( 2015 ) , 1121 – 1136 . [ 34 ] Dennis D Boos and Leonard A Stefanski . 2011 . P - value precision and repro - ducibility . American Statistician 65 , 4 ( 2011 ) , 213 – 221 . [ 35 ] Xavier Bouthillier , Pierre Delaunay , Mirko Bronzi , Assya Troﬁmov , Brennan Nichyporuk , Justin Szeto , Naz Sepah , Edward Raﬀ , Kanika Madan , Vikram Vo - leti , Samira Ebrahimi Kahou , Vincent Michalski , Dmitriy Serdyuk , Tal Arbel , Chris Pal , Gaël Varoquaux , and Pascal Vincent . 2021 . Accounting for variance in machine learning Bbenchmarks . In Machine Learning and Systems ( MLSys ) . [ 36 ] XavierBouthillierandGaëlVaroquaux . 2020 . Surveyofmachine - learningexper - imental methods at NeurIPS2019 and ICLR2020 . Ph . D . Dissertation . Inria Saclay Ile de France . [ 37 ] Samuel Bowman . 2022 . The Dangers of Underclaiming : Reasons for Caution When Reporting How NLP Systems Fail . In Proc . of the 60th Annual Meeting of the ACL ( Volume 1 : Long Papers ) . 7484 – 7499 . [ 38 ] Samuel R Bowman , Gabor Angeli , Christopher Potts , and Christopher D Man - ning . 2015 . A large annotated corpus for learning natural language inference . arXiv : 1508 . 05326 ( 2015 ) . 11 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan [ 39 ] Leo Breiman . 2001 . Statistical modeling : The two cultures . Statist . Sci . 16 , 3 ( 2001 ) , 199 – 231 . [ 40 ] Jonathan B Buckheit and David L Donoho . 1995 . Wavelab and reproducible research . In Wavelets and Statistics . Springer , 55 – 81 . [ 41 ] Joy Buolamwini and Timnit Gebru . 2018 . Gender shades : Intersectional accu - racy disparities in commercial gender classiﬁcation . In Conference on Fairness , Accountability and Transparency . PMLR , 77 – 91 . [ 42 ] Katherine S Button , John Ioannidis , Claire Mokrysz , Brian A Nosek , Jonathan Flint , Emma SJ Robinson , and Marcus R Munafò . 2013 . Power failure : Why small sample size undermines the reliability of neuroscience . Nature Reviews Neuroscience 14 , 5 ( 2013 ) , 365 – 376 . [ 43 ] Dallas Card , Peter Henderson , Urvashi Khandelwal , Robin Jia , Kyle Ma - howald , and Dan Jurafsky . 2020 . With little power comes great responsibility . arXiv : 2010 . 06595 ( 2020 ) . [ 44 ] Nicholas Carlini and David Wagner . 2017 . Towards evaluating the robustness of neural networks . In 2017IEEESymposium on Security and Privacy ( SP ) . IEEE , 39 – 57 . [ 45 ] Gavin C Cawley and Nicola L C Talbot . 2010 . On over - ﬁtting in model selec - tion and subsequent selection bias in performance evaluation . J . of Machine Learning Research 11 ( 2010 ) , 2079 – 2107 . [ 46 ] Jesse Chandler , Pam Mueller , and Gabriele Paolacci . 2014 . Nonnaïveté among AmazonMechanicalTurkworkers : Consequencesand solutionsforbehavioral researchers . Behavior Research Methods 46 , 1 ( 2014 ) , 112 – 130 . [ 47 ] Dami Choi , Christopher J Shallue , Zachary Nado , Jaehoon Lee , Chris J Maddi - son , andGeorgeEDahl . 2019 . Onempiricalcomparisonsofoptimizersfordeep learning . arXiv : 1910 . 05446 ( 2019 ) . [ 48 ] Anna Choromanska , Mikael Henaﬀ , Michael Mathieu , Gérard Ben Arous , and Yann LeCun . 2015 . The loss surfaces of multilayer networks . In Artiﬁcial Intel - ligence and Statistics . PMLR , 192 – 204 . [ 49 ] Herbert H Clark . 1973 . The language - as - ﬁxed - eﬀect fallacy : A critique of lan - guage statistics in psychological research . J . of Verbal Learning and Verbal Be - havior 12 , 4 ( 1973 ) , 335 – 359 . [ 50 ] Jacob Cohen . 1992 . Statistical power analysis . Current Directions in Psycholog - ical Science 1 , 3 ( 1992 ) , 98 – 101 . [ 51 ] JeremyRCoyle , NimaSHejazi , IvanaMalenica , RachaelVPhillips , Benjamin F Arnold , Andrew Mertens , Jade Benjamin - Chung , Weixin Cai , Sonali Dayal , John M Colford Jr , Alan E Hubbard , and Mark J van der Laan . 2020 . Targeting learning : Robust statistics for reproducible research . arXiv : 2006 . 07333 ( 2020 ) . [ 52 ] Kate Crawford . 2017 . The trouble with bias . ( 2017 ) . https : / / www . youtube . com / watch ? v = fMym _ BKWQzk NIPS 2017 . [ 53 ] Maurizio Ferrari Dacrema , Simone Boglio , Paolo Cremonesi , and Dietmar Jan - nach . 2021 . A troubling analysis of reproducibility and progress in recom - mender systems research . ACM Transactions on Information Systems ( TOIS ) 39 , 2 ( 2021 ) , 1 – 49 . [ 54 ] Alexander D’Amour , Katherine Heller , Dan Moldovan , Ben Adlam , Babak Alipanahi , Alex Beutel , Christina Chen , Jonathan Deaton , Jacob Eisenstein , Matthew D Hoﬀman , et al . 2020 . Underspeciﬁcation presents challenges for credibility in modern machine learning . arXiv : 2011 . 03395 ( 2020 ) . [ 55 ] Yehuda Dar , Vidya Muthukumar , and Richard G Baraniuk . 2021 . A farewell to the bias - variancetradeoﬀ ? an overview of the theory of overparameterized machine learning . arXiv : 2109 . 02355 ( 2021 ) . [ 56 ] Yann N Dauphin , Razvan Pascanu , Caglar Gulcehre , Kyunghyun Cho , Surya Ganguli , and Yoshua Bengio . 2014 . Identifying and attacking the saddle point problem in high - dimensional non - convex optimization . NeurIPS 27 ( 2014 ) . [ 57 ] Aida Mostafazadeh Davani , Mark Díaz , and Vinodkumar Prabhakaran . 2022 . Dealing with disagreements : Looking beyond the majority vote in subjective annotations . Transactions of the ACL 10 ( 2022 ) , 92 – 110 . [ 58 ] Erica Dawson , Thomas Gilovich , and Dennis T Regan . 2002 . Motivated Rea - soning and Performance on the was on Selection Task . Personality and Social Psychology Bulletin 28 , 10 ( 2002 ) , 1379 – 1387 . [ 59 ] Mostafa Dehghani , Yi Tay , Alexey A Gritsenko , Zhe Zhao , Neil Houlsby , Fer - nando Diaz , Donald Metzler , and Oriol Vinyals . 2021 . The benchmark lottery . arXiv : 2107 . 07002 ( 2021 ) . [ 60 ] Jasmine M DeJesus , Maureen A Callanan , GracielaSolis , and Susan A Gelman . 2019 . Genericlanguageinscientiﬁccommunication . Proc . oftheNationalAcad - emy of Sciences 116 , 37 ( 2019 ) , 18370 – 18377 . [ 61 ] Jia Deng , Wei Dong , Richard Socher , Li - JiaLi , Kai Li , and Li Fei - Fei . 2009 . Ima - genet : A large - scale hierarchical image database . In CVPR . Ieee , 248 – 255 . [ 62 ] Berna Devezer , Danielle J Navarro , Joachim Vandekerckhove , and Erkan OzgeBuzbas . 2020 . Thecaseforformalmethodology inscientiﬁcreform . Royal Society Open Science 8 , 3 ( 2020 ) , 200805 . [ 63 ] Jesse Dodge , Suchin Gururangan , Dallas Card , Roy Schwartz , and Noah A Smith . 2019 . Show your work : Improved reporting of experimental results . arXiv : 1909 . 03004 ( 2019 ) . [ 64 ] Jesse Dodge , Gabriel Ilharco , Roy Schwartz , Ali Farhadi , Hannaneh Hajishirzi , and NoahSmith . 2020 . Fine - tuning pretrainedlanguagemodels : Weight initial - izations , data orders , and early stopping . arXiv : 2002 . 06305 ( 2020 ) . [ 65 ] Pedro Domingos . 2012 . A few useful things to know about machine learning . Commun . ACM 55 , 10 ( 2012 ) , 78 – 87 . [ 66 ] DavidDonoho . 2017 . 50yearsofdatascience . J . ofComputationalandGraphical Statistics 26 ( 2017 ) , 745 – 766 . [ 67 ] Anna Dreber , Thomas Pfeiﬀer , Johan Almenberg , Siri Isaksson , Brad Wilson , Yiling Chen , Brian A . Nosek , and Magnus Johannesson . 2015 . Using predic - tion markets to estimate the reproducibility of scientiﬁc research . Proc . of the National Academy of Sciences 112 ( 2015 ) , 15343 – 15347 . [ 68 ] Rotem Dror , Gili Baumer , Marina Bogomolov , and Roi Reichart . 2017 . Replica - bility analysis for natural language processing : Testing signiﬁcance with mul - tiple datasets . Transactions of the ACL 5 ( 2017 ) , 471 – 486 . [ 69 ] Chris Drummond . 2006 . Machine learning as an experimental science ( revis - ited ) . In AAAI Workshop on Evaluation Methods for Machine Learning . 1 – 5 . [ 70 ] KristinaMDurante , AshleyRae , andVladasGriskevicius . 2013 . Theﬂuctuating femalevote : Politics , religion , and the ovulatorycycle . PsychologicalScience 24 , 6 ( 2013 ) , 1007 – 1016 . [ 71 ] Peter Eckersley , Yomna Nasser , et al . 2017 . EFF AI progress measurement project . Retreived from : https : / / eﬀ . org / ai / metrics , accessed on ( 2017 ) , 09 – 09 . [ 72 ] Bradley Efron . 2004 . The estimation of prediction error : Covariance penalties and cross - validation . J . of the American Statistical Association 99 , 467 ( 2004 ) , 619 – 632 . [ 73 ] Bradley Efron . 2020 . Prediction , estimation , and attribution . International Sta - tistical Review 88 ( 2020 ) , S28 – S59 . [ 74 ] Daniele Fanelli . 2010 . “Positive” results increase down the hierarchy of the sciences . PloS one 5 , 4 ( 2010 ) , e10068 . [ 75 ] Gregory Francis . 2012 . The psychology of replication and replication in psy - chology . Perspectives on Psychological Science 7 ( 2012 ) , 585 – 594 . [ 76 ] Gregory Francis . 2012 . Publication bias and the failureof replication in experi - mental psychology . Psychonomic Bulletin and Review 19 ( 2012 ) , 975 – 991 . [ 77 ] Matt Gardner , William Merrill , Jesse Dodge , Matthew E Peters , Alexis Ross , Sameer Singh , and Noah Smith . 2021 . Competency problems : On ﬁnding and removing artifacts in language data . arXiv : 2104 . 08646 ( 2021 ) . [ 78 ] Timnit Gebru , Jamie Morgenstern , Briana Vecchione , Jennifer Wortman Vaughan , Hanna Wallach , HalDauméIII , and KateCrawford . 2021 . Datasheets for datasets . Commun . ACM 64 , 12 ( 2021 ) , 86 – 92 . [ 79 ] Robert Geirhos , Jörn - Henrik Jacobsen , Claudio Michaelis , Richard Zemel , Wieland Brendel , Matthias Bethge , and Felix A Wichmann . 2020 . Shortcut learning in deep neural networks . Nature Machine Intelligence 2 , 11 ( 2020 ) , 665 – 673 . [ 80 ] Andrew Gelman . 2012 . Ethics and statistics : Ethics and the statistical use of prior information . Chance 25 , 4 ( 2012 ) , 52 – 54 . [ 81 ] Andrew Gelman . 2013 . P values and statistical practice . Epidemiology 24 , 1 ( 2013 ) , 69 – 72 . [ 82 ] Andrew Gelman . 2015 . The connection between varyingtreatment eﬀects and the crisis of unreplicable research : A Bayesian perspective . J . of Management 41 , 2 ( 2015 ) , 632 – 643 . [ 83 ] Andrew Gelman . 2017 . Honesty and transparency are not enough . Chance 39 ( 2017 ) , 37 – 39 . Issue 1 . [ 84 ] AndrewGelman . 2018 . Thefailureofnull hypothesissigniﬁcancetesting when studying incremental changes , and what to do about it . Personality and Social Psychology Bulletin 44 , 1 ( 2018 ) , 16 – 23 . [ 85 ] Andrew Gelman and John B . Carlin . 2014 . Beyond power calculations : Assess - ing type S ( sign ) and type M ( magnitude ) errors . Perspectives on Psychological Science 9 , 6 ( 2014 ) , 641 – 651 . [ 86 ] Andrew Gelman and Eric Loken . 2013 . The gardenof forking paths : Why mul - tiple comparisonscan beaproblem , evenwhen thereisno “ﬁshingexpedition” or “p - hacking” and the researchhypothesis was posited ahead of time . Depart - ment of Statistics , Columbia University 348 ( 2013 ) . [ 87 ] Andrew Gelman and Eric Loken . 2014 . Ethics and statistics : The AAA tranche of subprime science . Chance 27 , 1 ( 2014 ) , 51 – 56 . [ 88 ] Andrew Gelman and Eric Loken . 2014 . The statistical crisis in science data - dependent analysis—a “garden of forking paths”—explains why many statisti - cally signiﬁcant comparisons don’t hold up . American Scientist 102 , 6 ( 2014 ) , 460 . [ 89 ] AndrewGelman , DanielSimpson , andMichaelBetancourt . 2017 . Thepriorcan often only be understood in the context of the likelihood . Entropy 19 ( 2017 ) , 555 . [ 90 ] Andrew Gelman and Hal Stern . 2006 . The diﬀerencebetween “signiﬁcant”and “not signiﬁcant” is not itself statistically signiﬁcant . American Statistician 60 , 4 ( 2006 ) , 328 – 331 . [ 91 ] Andrew Gelman and David Weakliem . 2009 . Of beauty , sex and power : Too little attention has been paid to the statistical challenges in estimating small eﬀects . American Scientist 97 , 4 ( 2009 ) , 310 – 316 . [ 92 ] GerdGigerenzer . 2022 . We need to thinkmoreabouthow weconduct research . Behavioral and Brain Sciences 45 ( 2022 ) . [ 93 ] Gerd Gigerenzer and Julian N Marewski . 2015 . Surrogate science : The idol of a universalmethod for scientiﬁc inference . J . of Management 41 , 2 ( 2015 ) , 421 – 440 . 12 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning [ 94 ] Justin Gilmer , Behrooz Ghorbani , Ankush Garg , Sneha Kudugunta , Behnam Neyshabur , David Cardoze , George Dahl , Zack Nado , and Orhan Firat . 2021 . A loss curvature perspective on training instabilities of deep learning models . In ICLR . [ 95 ] TomGoldstein . 2022 . MyrecenttalkattheNSFtownhallfocusedonthehistory of the AI winters , how the ML community became " anti - science , " and whether the rejection of science will causea winter for ML theory . I’ll summarizethese issues below . . . http : / / archive . today / ryryU [ 96 ] Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy . 2014 . Explaining and harnessing adversarialexamples . arXiv : 1412 . 6572 ( 2014 ) . [ 97 ] Steven N Goodman . 1993 . P values , hypothesis tests , and likelihood : implica - tions for epidemiology of aneglected historicaldebate . AmericanJ . of Epidemi - ology 137 , 5 ( 1993 ) , 485 – 496 . [ 98 ] Mitchell L Gordon , Kaitlyn Zhou , Kayur Patel , Tatsunori Hashimoto , and MichaelSBernstein . 2021 . Thedisagreementdeconvolution : Bringingmachine learning performance metrics in line with reality . In Proc . of the 2021 CHI Con - ference on Human Factors in Computing Systems . 1 – 14 . [ 99 ] Kyle Gorman and Steven Bedrick . 2019 . We need to talk about standard splits . In Proc . of ACL . 2786 – 2791 . [ 100 ] Anirudh Goyal and Yoshua Bengio . 2020 . Inductive biasesfor deep learning of higher - level cognition . arXiv : 2011 . 15091 ( 2020 ) . [ 101 ] Yash Goyal , TejasKhot , Douglas Summers - Stay , Dhruv Batra , and DeviParikh . 2017 . Making the v in vqa matter : Elevating the role of image understanding in visual question answering . In Proc . of CVPR . 6904 – 6913 . [ 102 ] Sander Greenland . 2019 . Valid p - values behave exactly as they should : Some misleading criticisms of p - values and their resolution with s - values . American Statistician 73 , sup1 ( 2019 ) , 106 – 114 . [ 103 ] Sander Greenland and Zad Raﬁ . 2019 . To aid scientiﬁc inference , emphasize unconditional descriptions of statistics . arXiv : 1909 . 08583 ( 2019 ) . [ 104 ] SuchinGururangan , AnaMarasović , SwabhaSwayamdipta , KyleLo , IzBeltagy , DougDowney , andNoahASmith . 2020 . Don’tstoppretraining : adaptlanguage models to domains and tasks . arXiv : 2004 . 10964 ( 2020 ) . [ 105 ] Kris D Gutiérrez and BarbaraRogoﬀ . 2003 . Cultural ways of learning : Individ - ual traits or repertoiresof practice . Educational Researcher 32 , 5 ( 2003 ) , 19 – 25 . [ 106 ] Michael R Hagerty and V Srinivasan . 1991 . Comparing the predictive powers of alternative multiple regression models . Psychometrika 56 , 1 ( 1991 ) , 77 – 85 . [ 107 ] Benjamin Haibe - Kains , George Alexandru Adam , Ahmed Hosny , Farnoosh Khodakarami , LeviWaldron , Bo Wang , ChrisMcIntosh , Anna Goldenberg , An - shul Kundaje , CaseyS Greene , et al . 2020 . Transparencyand reproducibilityin artiﬁcial intelligence . Nature 586 , 7829 ( 2020 ) , E14 – E16 . [ 108 ] Alon Halevy , Peter Norvig , and Fernando Pereira . 2009 . The unreasonable ef - fectiveness of data . IEEE intelligent systems 24 , 2 ( 2009 ) , 8 – 12 . [ 109 ] Trevor Hastie , Robert Tibshirani , and Jerome H Friedman . 2009 . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Vol . 2 . Springer . [ 110 ] Will Douglas Heaven . 2020 . AI is wrestling with a replication crisis . MIT Tech - nology Review ( 2020 ) . [ 111 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger . 2018 . Deep reinforcement learning that matters . In Proc . of the AAAI Conference on Artiﬁcial Intelligence , Vol . 32 . [ 112 ] Joseph Henrich , Steven J Heine , and Ara Norenzayan . 2010 . The weirdest peo - ple in the world ? Behavioral and Brain sSiences 33 , 2 - 3 ( 2010 ) , 61 – 83 . [ 113 ] Daniel Hirschman . 2016 . Stylized facts in the social sciences . Sociological Sci - ence 3 ( 2016 ) , 604 – 626 . [ 114 ] Jake M Hofman , Amit Sharma , and Duncan J Watts . 2017 . Prediction and ex - planation in social systems . Science 355 , 6324 ( 2017 ) , 486 – 488 . [ 115 ] Jake M Hofman , Duncan J Watts , Susan Athey , Filiz Garip , Thomas L Grif - ﬁths , JonKleinberg , HelenMargetts , SendhilMullainathan , MatthewJSalganik , Simine Vazire , et al . 2021 . Integrating explanation and prediction in computa - tional social science . Nature 595 , 7866 ( 2021 ) , 181 – 188 . [ 116 ] Mahan Hosseini , Michael Powell , John Collins , Chloe Callahan - Flintoft , William Jones , Howard Bowman , and Brad Wyble . 2020 . I tried a bunch of things : The dangers of unexpected overﬁtting in classiﬁcation of brain data . Neuroscience & Biobehavioral Reviews 119 ( 2020 ) , 456 – 467 . [ 117 ] BobbyLeeHoutkoop , ChrisChambers , MalcolmMacleod , DorothyVMBishop , Thomas E Nichols , and Eric - Jan Wagenmakers . 2018 . Data sharing in psychol - ogy : Asurveyonbarriersandpreconditions . AdvancesinMethodsandPractices in Psychological Science 1 , 1 ( 2018 ) , 70 – 85 . [ 118 ] Weihua Hu , Matthias Fey , Marinka Zitnik , Yuxiao Dong , Hongyu Ren , Bowen Liu , Michele Catasta , and Jure Leskovec . 2021 . Open Graph Benchmark : Datasets for Machine Learning on Graphs . arXiv : 2005 . 00687 ( Feb . 2021 ) . http : / / arxiv . org / abs / 2005 . 00687 arXiv : 2005 . 00687 . [ 119 ] Chen Huang , Shuangfei Zhai , Walter Talbott , Miguel Bautista Martin , Shih - Yu Sun , Carlos Guestrin , and Josh Susskind . 2019 . Addressing the loss - metric mismatchwith adaptivelossalignment . In International ConferenceonMachine Learning . PMLR , 2891 – 2900 . [ 120 ] Raymond Hubbard and MJ Bayarri . 2003 . P values are not error probabilities . Institute of Statistics and Decision Sciences , Working Paper 03 - 26 ( 2003 ) , 27708 – 0251 . [ 121 ] Raymond Hubbard and María Jesús Bayarri . 2003 . Confusion over measures of evidence ( p’s ) versus errors ( 𝛼 ’s ) in classical statistical testing . American Statistician 57 , 3 ( 2003 ) , 171 – 178 . [ 122 ] Ben Hutchinson , Andrew Smart , Alex Hanna , Emily Denton , Christina Greer , Oddur Kjartansson , Parker Barnes , and Margaret Mitchell . 2021 . Towards ac - countability for machine learning datasets : Practices from software engineer - ing and infrastructure . In Proc . of FAccT . 560 – 575 . [ 123 ] Matthew Hutson . 2018 . Has artiﬁcial intelligence become alchemy ? Science ( 2018 ) . [ 124 ] Andrew Ilyas , Shibani Santurkar , Dimitris Tsipras , Logan Engstrom , Brandon Tran , and Aleksander Madry . 2019 . Adversarial examples are not bugs , they are features . NeurIPS 32 ( 2019 ) . [ 125 ] John P . A . Ioannidis . 2008 . Why most discoveredtrue associations are inﬂated . Epidemiology 19 ( 2008 ) , 640 – 648 . [ 126 ] Abigail Z Jacobs and Hanna Wallach . 2021 . Measurementand fairness . In Proc . of FAccT . 375 – 385 . [ 127 ] Yiding Jiang , Behnam Neyshabur , Hossein Mobahi , Dilip Krishnan , and Samy Bengio . 2019 . Fantastic generalization measures and where to ﬁnd them . arXiv : 1912 . 02178 ( 2019 ) . [ 128 ] Eun Seo Jo and Timnit Gebru . 2020 . Lessons from archives : Strategies for col - lecting sociocultural data in machine learning . In Proc . of FAccT . 306 – 316 . [ 129 ] Jason Jo and Yoshua Bengio . 2017 . Measuring the tendency of cnns to learn surface statistical regularities . arXiv : 1711 . 11561 ( 2017 ) . [ 130 ] DimitrisKalimeris , GalKaplun , PreetumNakkiran , BenjaminEdelman , Tristan Yang , Boaz Barak , and Haofeng Zhang . 2019 . SGD on neural networks learns functions of increasing complexity . NeurIPS 32 ( 2019 ) . [ 131 ] Divyansh Kaushik and Zachary C Lipton . 2018 . How much reading does read - ing comprehension require ? a critical investigation of popular benchmarks . arXiv : 1808 . 04926 ( 2018 ) . [ 132 ] MatthewKay , CynthiaMatuszek , andSeanAMunson . 2015 . Unequal represen - tation and gender stereotypes in image search results for occupations . In Proc . of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 3819 – 3828 . [ 133 ] Khimya Khetarpal , ZafaraliAhmed , Andre Cianﬂone , RiashatIslam , and Joelle Pineau . 2018 . RE - EVALUATE : Reproducibility in evaluating reinforcement learningalgorithms . 2ndReproducibilityinMachineLearningWorkshopatICML 2018 ( 2018 ) . [ 134 ] Jon Kleinberg , Jens Ludwig , Sendhil Mullainathan , and Ashesh Rambachan . 2018 . Algorithmic fairness . In AEA Papers and Proceedings , Vol . 108 . 22 – 27 . [ 135 ] Alex Krizhevsky , Geoﬀrey Hinton , et al . 2009 . Learning multiple layers of fea - tures from tiny images . ( 2009 ) . [ 136 ] Michael D Lee and Eric - Jan Wagenmakers . 2014 . Bayesian Cognitive Modeling : A Practical Course . Cambridge University Press . [ 137 ] Thomas Liao , Benjamin Recht , and Ludwig Schmidt . 2020 . In a forward direc - tion : Analyzing distribution shifts in machine translation test sets over time . ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning . [ 138 ] Thomas Liao , Rohan Taori , Inioluwa Deborah Raji , and Ludwig Schmidt . 2021 . Arewelearningyet ? Ametareviewofevaluationfailuresacrossmachinelearn - ing . In Thirty - ﬁfthConferenceonNeuralInformationProcessingSystemsDatasets and Benchmarks Track ( Round 2 ) . [ 139 ] Jimmy Lin , Daniel Campos , Nick Craswell , Bhaskar Mitra , and Emine Yil - maz . 2021 . Signiﬁcant improvements over the state of the art ? A case study of the MS MARCO Document Ranking Leaderboard . ( Feb . 2021 ) . https : / / arxiv . org / abs / 2102 . 12887v1 [ 140 ] Zachary C Lipton and Jacob Steinhardt . 2019 . Researchfor practice : Troubling trends in machine - learning scholarship . Commun . ACM 62 , 6 ( 2019 ) , 45 – 53 . [ 141 ] Eric Loken and Andrew Gelman . 2017 . Measurementerror and the replication crisis . Science 355 , 6325 ( 2017 ) , 584 – 585 . [ 142 ] Mario Lucic , Karol Kurach , Marcin Michalski , Sylvain Gelly , and Olivier Bous - quet . 2018 . Are GANs created equal ? A large - scale study . NeurIPS 31 ( 2018 ) . [ 143 ] Kelvin Luu , Daniel Khashabi , Suchin Gururangan , Karishma Mandyam , and Noah A Smith . 2021 . Timewaits for no one ! Analysis and challenges of tempo - ral misalignment . arXiv : 2111 . 07408 ( 2021 ) . [ 144 ] John G Lynch Jr . 1982 . On the external validity of experiments in consumer research . J . of Consumer Research 9 , 3 ( 1982 ) , 225 – 239 . [ 145 ] Roger Magoulasand SteveSwoyer . 2020 . AI Adoption in the Enterprise . Beijing : O´ Reilly . Recuperado de http : / / www . oreilly . com / data / free / ai . . . . [ 146 ] Momin M Malik . 2020 . A hierarchy of limitations in machine learning . arXiv : 2002 . 05193 ( 2020 ) . [ 147 ] R Thomas McCoy , Ellie Pavlick , and Tal Linzen . 2019 . Right for the wrong reasons : Diagnosing syntactic heuristics in natural language inference . arXiv : 1902 . 01007 ( 2019 ) . [ 148 ] Paul E Meehl . 1967 . Theory - testing in psychology and physics : A methodolog - ical paradox . Philosophy of Science 34 , 2 ( 1967 ) , 103 – 115 . [ 149 ] Paul E Meehl . 1990 . Why summariesof researchon psychological theories are often uninterpretable . Psychological Reports 66 , 1 ( 1990 ) , 195 – 244 . [ 150 ] Gábor Melis , Chris Dyer , and Phil Blunsom . 2017 . On the state of the art of evaluation in neural language models . arXiv : 1707 . 05589 ( 2017 ) . 13 J . Hullman , S . Kapoor , P . Nanayakkara , A . Gelman , and A . Narayanan [ 151 ] Xiao - Li Meng . 2018 . Statistical paradises and paradoxes in big data ( 1 ) : Law of large populations , big data paradox , and the 2016 US presidential election . Annals of Applied Statistics 12 , 2 ( 2018 ) , 685 – 726 . [ 152 ] Margaret Mitchell , Simone Wu , Andrew Zaldivar , Parker Barnes , Lucy Vasser - man , Ben Hutchinson , Elena Spitzer , Inioluwa DeborahRaji , and TimnitGebru . 2019 . Model cards for model reporting . In Proc . of FAccT . 220 – 229 . [ 153 ] Marcus R Munafò , Brian A Nosek , Dorothy VM Bishop , Katherine S Button , ChristopherDChambers , NathaliePercieduSert , UriSimonsohn , Eric - JanWa - genmakers , Jennifer J Ware , and John Ioannidis . 2017 . A manifesto for repro - ducible science . Nature Human Behaviour 1 , 1 ( 2017 ) , 1 – 9 . [ 154 ] DuncanJMurdoch , Yu - LingTsai , andJamesAdcock . 2008 . P - valuesarerandom variables . American Statistician 62 , 3 ( 2008 ) , 242 – 245 . [ 155 ] Prabhat Nagarajan , Garrett Warnell , and Peter Stone . 2018 . Determin - istic implementations for reproducibility in deep reinforcement learning . arXiv : 1809 . 05676 ( 2018 ) . [ 156 ] DanielleNavarro . 2020 . Pathsinstrangespaces : Acomment onpreregistration . ( 2020 ) . [ 157 ] Marcel Neunhoeﬀer and Sebastian Sternberg . 2019 . How cross - validation can go wrong and what to do about it . Political Analysis 27 , 1 ( 2019 ) , 101 – 106 . [ 158 ] Behnam Neyshabur , Ryota Tomioka , and Nathan Srebro . 2014 . In search of the real inductive bias : On the role of implicit regularization in deep learning . arXiv : 1412 . 6614 ( 2014 ) . [ 159 ] Matthew P Normand . 2016 . Less is more : Psychologists can learn more by studying fewer people . Frontiersin Psychology 7 ( 2016 ) , 934 . [ 160 ] Curtis G Northcutt , Anish Athalye , and Jonas Mueller . 2021 . Pervasive label errors in test sets destabilize machine learning benchmarks . arXiv : 2103 . 14749 ( 2021 ) . [ 161 ] Brian A . Nosek et al . 2015 . Estimating the reproducibility of psychological science . Science 349 ( 2015 ) , aac4716 . [ 162 ] Amy Orben and Daniël Lakens . 2020 . Crud ( re ) deﬁned . Advances in Methods and Practices in Psychological Science 3 , 2 ( 2020 ) , 238 – 247 . [ 163 ] Yaniv Ovadia , Emily Fertig , Jie Ren , Zachary Nado , David Sculley , Sebastian Nowozin , JoshuaDillon , BalajiLakshminarayanan , andJasperSnoek . 2019 . Can you trust your model’s uncertainty ? evaluating predictive uncertainty under dataset shift . NeurIPS 32 ( 2019 ) . [ 164 ] JiHoPark , JaminShin , andPascaleFung . 2018 . Reducinggenderbiasinabusive language detection . arXiv : 1808 . 07231 ( 2018 ) . [ 165 ] AmandalynnePaullada , InioluwaDeborahRaji , EmilyMBender , EmilyDenton , and Alex Hanna . 2021 . Dataand its ( dis ) contents : A surveyof datasetdevelop - ment and use in machine learning research . Patterns 2 , 11 ( 2021 ) , 100336 . [ 166 ] Samuel Pawel and Leonhard Held . 2020 . The sceptical Bayes factor for the assessment of replication success . arXiv : 2009 . 01520 ( 2020 ) . [ 167 ] Juan Perdomo , Tijana Zrnic , Celestine Mendler - Dünner , and Moritz Hardt . 2020 . Performativeprediction . In InternationalConferenceonMachineLearning . PMLR , 7599 – 7609 . [ 168 ] David Picard . 2021 . Torch . manual _ seed ( 3407 ) is all you need : On the in - ﬂuence of random seeds in deep learning architectures for computer vision . arXiv : 2109 . 08203 ( 2021 ) . [ 169 ] Joelle Pineau , Philippe Vincent - Lamarre , Koustuv Sinha , Vincent Larivière , Alina Beygelzimer , Florence d’Alché Buc , Emily Fox , and Hugo Larochelle . 2021 . Improving reproducibility in machine learning research : a report from the NeurIPS 2019 reproducibility program . J . of Machine Learning Research 22 ( 2021 ) . [ 170 ] Joaquin Quiñonero - Candela , Masashi Sugiyama , Anton Schwaighofer , and Neil D Lawrence . 2008 . Dataset shift in machine learning . Mit Press . [ 171 ] ZadRaﬁandSanderGreenland . 2020 . Semanticandcognitivetoolstoaidstatis - ticalscience : Replaceconﬁdenceandsigniﬁcancebycompatibilityandsurprise . BMC Medical Research Methodology 20 , 1 ( 2020 ) , 1 – 13 . [ 172 ] Inioluwa Deborah Raji , Emily M Bender , Amandalynne Paullada , Emily Den - ton , and Alex Hanna . 2021 . AI and the everything in the whole wide world benchmark . arXiv : 2111 . 15366 ( 2021 ) . [ 173 ] Sebastian Raschka . 2018 . Model evaluation , model selection , and algorithm selection in machine learning . arXiv : 1811 . 12808 ( 2018 ) . [ 174 ] BenjaminRecht , RebeccaRoelofs , LudwigSchmidt , andVaishaalShankar . 2018 . Do CIFAR - 10 classiﬁersgeneralize to CIFAR - 10 ? arXiv : 1806 . 00451 ( 2018 ) . [ 175 ] B Recht , R Roelofs , L Schmidt , and V Shankar . 2019 . Unbiased look at dataset bias . ICML . [ 176 ] JamesAReggia , GarrettEKatz , andGregoryPDavis . 2020 . Artiﬁcialconscious intelligence . J . of Artiﬁcial Intelligence and Consciousness 7 , 01 ( 2020 ) , 95 – 107 . [ 177 ] RobertaRoccaandTalYarkoni . 2021 . Puttingpsychologytothetest : Rethinking model evaluation through benchmarkingand prediction . Advances in Methods and Practices in Psychological Science 4 , 3 ( 2021 ) , 25152459211026864 . [ 178 ] Barbara Rogoﬀ . 2003 . The Cultural Nature of Human Development . Oxford University Press . [ 179 ] Amir Rosenfeld , RichardZemel , and John K Tsotsos . 2018 . The elephant in the room . arXiv : 1808 . 03305 ( 2018 ) . [ 180 ] Andrew Ross , IsaacLage , and Finale Doshi - Velez . 2017 . The neurallasso : Local linear sparsity for interpretable explanations . In Workshop on Transparent and Interpretable Machine Learning in Safety Critical Environments , 31st Conference on Neural Information Processing Systems , Vol . 4 . [ 181 ] Stuart J Russell and Peter Norvig . 2003 . Artiﬁcial Intelligence : A Modern Ap - proach . Alan R . Apt . [ 182 ] Geir Kjetil Sandve , Anton Nekrutenko , James Taylor , and Eivind Hovig . 2013 . Ten simplerules forreproduciblecomputational research . PLoS Computational Biology 9 , 10 ( 2013 ) , e1003285 . [ 183 ] Kai Sassenbergand Lara Ditrich . 2019 . Research in social psychology changed between 2011 and 2016 : Larger sample sizes , more self - report measures , and moreonline studies . Advances inMethods and Practicesin PsychologicalScience 2 , 2 ( 2019 ) , 107 – 114 . [ 184 ] Andrew M Saxe , James L McClelland , and Surya Ganguli . 2013 . Exact solu - tions to the nonlinear dynamics of learning in deep linear neural networks . arXiv : 1312 . 6120 ( 2013 ) . [ 185 ] JeﬀreyDScargle . 1999 . Publicationbias ( the " ﬁle - drawerproblem " ) inscientiﬁc inference . physics / 9909033 ( 1999 ) . [ 186 ] Morgan Klaus Scheuerman , Alex Hanna , and Emily Denton . 2021 . Do datasets have politics ? Disciplinary values in computer vision dataset development . Proc . of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 – 37 . [ 187 ] Robin M Schmidt , Frank Schneider , and Philipp Hennig . 2021 . Descending through a crowded valley - benchmarking deep learning optimizers . In Interna - tional Conference on Machine Learning . PMLR , 9367 – 9376 . [ 188 ] David Sculley , Jasper Snoek , Alex Wiltschko , and Ali Rahimi . 2018 . Winner’s curse ? On pace , progress , and empirical rigor . ICLR ( 2018 ) . [ 189 ] S Senn . 2001 . Two cheers for P - values ? J . of Epidemiology and Biostatistics 6 , 2 ( 2001 ) , 193 – 204 . [ 190 ] HarshayShah , KaustavTamuly , AditiRaghunathan , PrateekJain , andPraneeth Netrapalli . 2020 . The pitfalls of simplicity bias in neural networks . NeurIPS 33 ( 2020 ) , 9573 – 9585 . [ 191 ] Galit Shmueli . 2010 . To explain orto predict ? Statist . Sci . 25 , 3 ( 2010 ) , 289 – 310 . [ 192 ] Joseph P Simmons , Leif D Nelson , and Uri Simonsohn . 2011 . False - positive psychology : Undisclosed ﬂexibility in data collection and analysis allows pre - senting anything assigniﬁcant . Psychological Science 22 , 11 ( 2011 ) , 1359 – 1366 . [ 193 ] Joseph P Simmons , Leif D Nelson , and Uri Simonsohn . 2021 . Pre - registration is a game changer . But , like random assignment , it is neither necessary nor suﬃcient for credible science . J . of Consumer Psychology 31 , 1 ( 2021 ) , 177 – 180 . [ 194 ] Daniel J Simons , Yuichi Shoda , and D Stephen Lindsay . 2017 . Constraints on generality ( COG ) : A proposed addition to all empirical papers . Perspectives on Psychological Science 12 , 6 ( 2017 ) , 1123 – 1128 . [ 195 ] DanielSoudry , EladHoﬀer , MorShpigelNacson , SuriyaGunasekar , andNathan Srebro . 2018 . The implicit bias of gradient descent on separable data . J . of Machine Learning Research 19 , 1 ( 2018 ) , 2822 – 2878 . [ 196 ] Peter M Steiner , Vivian C Wong , and Kylie Anglin . 2019 . A causal replication framework for designing and assessing replication eﬀorts . Zeitschrift für Psy - chologie ( 2019 ) . [ 197 ] Victoria Stodden and Sheila Miguez . 2014 . Provisioning Reproducible Compu - tational Science . ( 2014 ) . [ 198 ] Amos Storkey . 2009 . When training and test sets are diﬀerent : Characterizing learning transfer . Dataset Shift in Machine Learning 30 ( 2009 ) , 3 – 28 . [ 199 ] Emma Strubell , Ananya Ganesh , and Andrew McCallum . 2020 . Energy and policy considerations for modern deep learning research . In Proc . of the AAAI Conference on Artiﬁcial Intelligence , Vol . 34 . 13693 – 13696 . [ 200 ] Chen Sun , Abhinav Shrivastava , Saurabh Singh , and Abhinav Gupta . 2017 . Re - visitingunreasonableeﬀectivenessofdataindeeplearningera . In Proc . ofICCV . 843 – 852 . [ 201 ] Harini Suresh and John Guttag . 2021 . A frameworkfor understanding sources of harm throughout the machine learning life cycle . In Equity and Access in Algorithms , Mechanisms , and Optimization . 1 – 9 . [ 202 ] Christian Szegedy , Wojciech Zaremba , Ilya Sutskever , Joan Bruna , Dumitru Er - han , Ian Goodfellow , and Rob Fergus . 2013 . Intriguing properties of neural networks . arXiv : 1312 . 6199 ( 2013 ) . [ 203 ] AbaSzollosi , DavidKellen , DanielleNavarro , RichardShiﬀrin , IrisvanRooij , Tr - ishaVanZandt , andChrisDonkin . 2020 . Ispreregistrationworthwhile ? Trends in Cognitive Sciences 24 ( 2020 ) , P94 – 95 . Issue 2 . [ 204 ] Denes Szucs and John Ioannidis . 2017 . When null hypothesis signiﬁcance test - ingisunsuitableforresearch : Areassessment . FrontiersinHumanNeuroscience 11 ( 2017 ) , 390 . [ 205 ] Leho Tedersoo , Rainer Küngas , Ester Oras , Kajar Köster , Helen Eenmaa , Äli Leijen , Margus Pedaste , Marju Raju , Anastasiya Astapova , Heli Lukner , et al . 2021 . Data sharing practices and data availability upon request diﬀer across scientiﬁc disciplines . Scientiﬁc Data 8 , 1 ( 2021 ) , 1 – 11 . [ 206 ] Prabhu Teja Sivaprasad , Florian Mai , Thijs Vogels , Martin Jaggi , and François Fleuret . 2019 . Optimizer benchmarking needs to account for hyperparameter tuning . arXiv e - prints ( 2019 ) , arXiv – 1910 . [ 207 ] Damien Teney , Ehsan Abbasnejad , Kushal Kaﬂe , Robik Shrestha , Christopher Kanan , and Anton Van Den Hengel . 2020 . On the value of out - of - distribution testing : An example of Goodhart’s law . NeurIPS 33 ( 2020 ) , 407 – 417 . 14 The worst of both worlds : A comparative analysis of errors in learning from data in psychology and machine learning [ 208 ] Antonio Torralba and Alexei A Efros . 2011 . Unbiased look at dataset bias . In Proc . of CVPR . IEEE , 1521 – 1528 . [ 209 ] Christopher Tosh , Philip Greengard , Ben Goodrich , Andrew Gelman , Aki Ve - htari , and Daniel Hsu . 2021 . The piranha problem : Large eﬀects swimming in a small pond . arXiv : 2105 . 13445 ( 2021 ) . [ 210 ] Leslie G Valiant . 1984 . A theory of the learnable . Commun . ACM 27 , 11 ( 1984 ) , 1134 – 1142 . [ 211 ] Tyler J VanderWeele and Miguel A Hernán . 2012 . Results on diﬀerential and dependent measurement error of the exposure and the outcome using signed directed acyclicgraphs . AmericanJ . ofEpidemiology 175 , 12 ( 2012 ) , 1303 – 1310 . [ 212 ] Vladimir Vapnik . 1998 . Statistical Learning Theory . Wiley - Interscience . [ 213 ] MatthewJVowels . 2021 . Misspeciﬁcationandunreliableinterpretationsinpsy - chology and social science . Psychological Methods ( 2021 ) . [ 214 ] Eric - JanWagenmakers . 2007 . A practicalsolution to the pervasiveproblemsof p values . PsychonomicBulletin & Review 14 , 5 ( 2007 ) , 779 – 804 . [ 215 ] Eric - Jan Wagenmakers , Maarten Marsman , Tahira Jamil , Alexander Ly , Josine Verhagen , Jonathon Love , Ravi Selker , Quentin F Gronau , Martin Šmíra , Sacha Epskamp , et al . 2018 . Bayesian inference for psychology . Part I : Theoretical advantages and practical ramiﬁcations . Psychonomic Bulletin & Review 25 , 1 ( 2018 ) , 35 – 57 . [ 216 ] Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman . 2018 . GLUE : A Multi - Task Benchmark and Analysis Plat - form for Natural Language Understanding . In Proc . of the 2018 EMNLP Work - shop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP . ACL , Brussels , Belgium , 353 – 355 . https : / / doi . org / 10 . 18653 / v1 / W18 - 5446 [ 217 ] Larry Wasserman . 2004 . Bayesian inference . In All of Statistics . Springer , 175 – 192 . [ 218 ] Gary L Wells and Paul D Windschitl . 1999 . Stimulus sampling and social psy - chological experimentation . Personality and Social Psychology Bulletin 25 , 9 ( 1999 ) , 1115 – 1125 . [ 219 ] Shimon Whiteson , BrianTanner , MatthewE Taylor , andPeterStone . 2011 . Pro - tecting against evaluation overﬁtting in empirical reinforcement learning . In ADPRL . IEEE , 120 – 127 . [ 220 ] GerhardWidmerandMiroslavKubat . 1996 . Learninginthepresenceofconcept drift and hidden contexts . Machine Learning 23 , 1 ( 1996 ) , 69 – 101 . [ 221 ] Mitchell Wortsman , Gabriel Ilharco , Mike Li , Jong Wook Kim , Hannaneh Ha - jishirzi , Ali Farhadi , Hongseok Namkoong , and Ludwig Schmidt . 2021 . Robust ﬁne - tuning of zero - shot models . arXiv : 2109 . 01903 ( 2021 ) . [ 222 ] Chhavi Yadavand Léon Bottou . 2019 . Cold case : The lost mnist digits . NeurIPS 32 ( 2019 ) . [ 223 ] Tal Yarkoni . 2022 . The generalizability crisis . Behavioral and Brain Sciences 45 ( 2022 ) . [ 224 ] Tal Yarkoni and Jacob Westfall . 2017 . Choosing prediction over explanation in psychology : Lessons from machine learning . Perspectives on Psychological Science 12 , 6 ( 2017 ) , 1100 – 1122 . [ 225 ] Ed Yong . 2012 . A failed replication draws a scathing per - sonal attack from a psychology professor . Discover ( 2012 ) . https : / / web . archive . org / web / 20120313012842 / http : / / blogs . discovermagazine . com / notrocketscience / 2012 / 03 / 10 / failed - replication - bargh - psychology - study - doyen / [ 226 ] Chiyuan Zhang , Samy Bengio , Moritz Hardt , Benjamin Recht , and Oriol Vinyals . 2021 . Understanding deep learning ( still ) requires rethinking gener - alization . Commun . ACM 64 , 3 ( 2021 ) , 107 – 115 . [ 227 ] Jieyu Zhao , Tianlu Wang , Mark Yatskar , Vicente Ordonez , and Kai - Wei Chang . 2017 . Menalsolikeshopping : Reducing genderbiasampliﬁcationusingcorpus - level constraints . arXiv : 1707 . 09457 ( 2017 ) . 15