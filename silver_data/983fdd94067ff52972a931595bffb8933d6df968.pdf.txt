Joint Biomedical Entity and Relation Extraction with Knowledge - Enhanced Collective Inference Tuan Lai 1 , Heng Ji 1 , ChengXiang Zhai 1 , Quan Hung Tran 2 1 University of Illinois at Urbana - Champaign 2 Adobe Research { tuanml2 , hengji , czhai } @ illinois . edu qtran @ adobe . com Abstract Compared to the general news domain , infor - mation extraction ( IE ) from biomedical text re - quires much broader domain knowledge . How - ever , many previous IE methods do not utilize any external knowledge during inference . Due to the exponential growth of biomedical pub - lications , models that do not go beyond their ﬁxed set of parameters will likely fall behind . Inspired by how humans look up relevant in - formation to comprehend a scientiﬁc text , we present a novel framework that utilizes exter - nal knowledge for joint entity and relation ex - traction named KECI ( Knowledge - Enhanced Collective Inference ) . Given an input text , KECI ﬁrst constructs an initial span graph rep - resenting its initial understanding of the text . It then uses an entity linker to form a knowledge graph containing relevant background knowl - edge for the the entity mentions in the text . To make the ﬁnal predictions , KECI fuses the ini - tial span graph and the knowledge graph into a more reﬁned graph using an attention mecha - nism . KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representa - tions using graph convolutional networks . Our experimental results show that the framework is highly effective , achieving new state - of - the - art results in two different benchmark datasets : BioRelEx ( binding interaction detection ) and ADE ( adverse drug event extraction ) . For ex - ample , KECI achieves absolute improvements of 4 . 59 % and 4 . 91 % in F1 scores over the state - of - the - art on the BioRelEx entity and relation extraction tasks 1 . 1 Introduction With the accelerating growth of biomedical publi - cations , it has become increasingly challenging to manually keep up with all the latest articles . As 1 The code is publicly available at https : / / github . com / laituan245 / bio _ relex Figure 1 : An example in the BioRelEx dataset . UIM is an abbreviation of “Ubiquitin - Interacting Motif” . Our baseline SciBERT model incorrectly predicts the men - tion as a “DNA” instead of a “Protein Motif” . a result , developing methods for automatic extrac - tion of biomedical entities and their relations has attracted much research attention recently ( Li et al . , 2017 ; Fei et al . , 2020 ; Luo et al . , 2020 ) . Many re - lated tasks and datasets have been introduced , rang - ing from binding interaction detection ( BioRelEx ) ( Khachatrian et al . , 2019 ) to adverse drug event extraction ( ADE ) ( Gurulingappa et al . , 2012 ) . Many recent joint models for entity and relation extraction rely mainly on distributional represen - tations and do not utilize any external knowledge source ( Eberts and Ulges , 2020 ; Ji et al . , 2020 ; Zhao et al . , 2020 ) . However , different from the general news domain , information extraction for the biomedical domain typically requires much broader domain - speciﬁc knowledge . Biomedical documents , either formal ( e . g . , scientiﬁc papers ) or informal ones ( e . g . , clinical notes ) , are written for domain experts . As such , they contain many highly specialized terms , acronyms , and abbrevia - tions . In the BioRelEx dataset , we ﬁnd that about 65 % of the annotated entity mentions are abbre - viations of biological entities , and an example is shown in Figure 1 . These unique characteristics bring great challenges to general - domain systems and even to existing scientiﬁc language models that a r X i v : 2105 . 13456v2 [ c s . C L ] 1 J un 2021 Figure 2 : KECI operates in three main steps : ( 1 ) initial span graph construction ( 2 ) background knowledge graph construction ( 3 ) fusion of these two graphs into a ﬁnal span graph . KECI takes a collective approach to link multiple mentions simultaneously to entities by incorporating global relational information using GCNs . do not use any external knowledge base during inference ( Beltagy et al . , 2019 ; Lee et al . , 2019 ) . For example , even though SciBERT ( Beltagy et al . , 2019 ) was pretrained on 1 . 14M scientiﬁc papers , our baseline SciBERT model still incorrectly pre - dicts the type of the term UIM in Figure 1 to be “DNA” , which should be a “Protein Motif” instead . Since the biomedical literature is expanding at an exponential rate , models that do not go beyond their ﬁxed set of parameters will likely fall behind . In this paper , we introduce KECI ( Knowledge - Enhanced Collective Inference ) , a novel end - to - end framework that utilizes external domain knowledge for joint entity and relation extraction . Inspired by how humans comprehend a complex piece of sci - entiﬁc text , the framework operates in three main steps ( Figure 2 ) . KECI ﬁrst reads the input text and constructs an initial span graph representing its ini - tial understanding of the text . In a span graph , each node represents a ( predicted ) entity mention , and each edge represents a ( predicted ) relation between two entity mentions . KECI then uses an entity linker to form a background knowledge graph con - taining all potentially relevant biomedical entities from an external knowledge base ( KB ) . For each en - tity , we extract its semantic types , its deﬁnition sen - tence , and its relational information from the exter - nal KB . Finally , KECI uses an attention mechanism to fuse the initial span graph and the background knowledge graph into a more reﬁned graph repre - senting the ﬁnal output . Different from previous methods that link mentions to entities based solely on local contexts ( Li et al . , 2020b ) , our framework takes a more collective approach to link multiple semantically related mentions simultaneously by leveraging global topical coherence . Our hypoth - esis is that if multiple mentions co - occur in the same discourse and they are probably semantically related , their reference entities should also be con - nected in the external KB . KECI integrates global relational information into mention and entity rep - resentations using graph convolutional networks ( GCNs ) before linking . The beneﬁt of collective inference can be illus - trated by the example shown in Figure 2 . The entity linker proposes two candidate entities for the men - tion FKBP12 ; one is of semantic type “AA , Peptide , or Protein” and the other is of semantic type “Gene or Genome” . It can be tricky to select the correct candidate as FKBP12 is already tagged with the wrong type in the initial span graph ( i . e . , it is pre - dicted to be a “Chemical” instead of a “Protein” ) . However , because of the structural resemblance between the mention - pair (cid:104) FK506 , FKBP12 (cid:105) and the pair (cid:104) “Organic Chemical” , “AA , Peptide , or Protein” (cid:105) , KECI will link FKBP12 to the entity of semantic type “AA , Peptide , or Protein” . As a result , the ﬁnal predicted type of FKBP12 will also be corrected to “Protein” in the ﬁnal span graph . Our extensive experimental results show that the proposed framework is highly effective , achiev - ing new state - of - the - art biomedical entity and re - lation extraction performance on two benchmark datasets : BioRelEx ( Khachatrian et al . , 2019 ) and ADE ( Gurulingappa et al . , 2012 ) . For example , KECI achieves absolute improvements of 4 . 59 % and 4 . 91 % in F1 scores over the state - of - the - art on the BioRelEx entity and relation extraction tasks . Our analysis also shows that KECI can automati - cally learn to select relevant candidate entities with - out any explicit entity linking supervision during training . Furthermore , because KECI considers text spans as the basic units for prediction , it can extract nested entity mentions . 2 Methods 2 . 1 Overview KECI considers text spans as the basic units for fea - ture extraction and prediction . This design choice allows us to handle nested entity mentions ( Sohrab and Miwa , 2018 ) . Also , joint entity and relation extraction can be naturally formulated as the task of extracting a span graph from an input document ( Luan et al . , 2019 ) . In a span graph , each node represents a ( predicted ) entity mention , and each edge represents a ( predicted ) relation between two entity mentions . Given an input document D , KECI ﬁrst enu - merates all the spans ( up to a certain length ) and embeds them into feature vectors ( Sec . 2 . 2 ) . With these feature vectors , KECI predicts an initial span graph and applies a GCN to integrate initial rela - tional information into each span representation ( Sec . 2 . 3 ) . KECI then uses an entity linker to build a background knowledge graph and applies another GCN to encode each node of the graph ( Sec . 2 . 4 ) . Finally , KECI aligns the nodes of the initial span graph and the background knowledge graph to make the ﬁnal predictions ( Sec . 2 . 5 ) . We train KECI in an end - to - end manner without using any additional entity linking supervision ( Sec . 2 . 6 ) . Overall , the design of KECI is partly inspired by previous research in educational psychology . Students’ background knowledge plays a vital role in guiding their understanding and comprehension of scientiﬁc texts ( Alvermann et al . , 1985 ; Braasch and Goldman , 2010 ) . “Activating” relevant and accurate prior knowledge will aid students’ reading comprehension . 2 . 2 Span Encoder Our model ﬁrst constructs a contextualized rep - resentation for each input token using SciBERT ( Beltagy et al . , 2019 ) . Let X = ( x 1 , . . . , x n ) be the output of the token - level encoder , where n denotes the number of tokens in D . Then , for each span s i whose length is not more than L , we compute its span representation s i ∈ R d as : s i = FFNN g (cid:0)(cid:2) x START ( i ) , x END ( i ) , ˆ x i , φ ( s i ) (cid:3)(cid:1) ( 1 ) where START ( i ) and END ( i ) denote the start and end indices of s i respectively . x START ( i ) and x END ( i ) are the boundary token representations . ˆ x i is an attention - weighted sum of the token representa - tions in the span ( Lee et al . , 2017 ) . φ ( s i ) is a fea - ture vector denoting the span length . FFNN g is a feedforward network with ReLU activations . 2 . 3 Initial Span Graph Construction With the extracted span representations , we predict the type of each span and also the relation between each span pair jointly . Let E denote the set of entity types ( including non - entity ) , and R denote the set of relation types ( including non - relation ) . We ﬁrst classify each span s i : e i = Softmax (cid:0) FFNN e ( s i ) (cid:1) ( 2 ) where FFNN e is a feedforward network mapping from R d → R | E | . We then employ another network to classify the relation of each span pair (cid:104) s i , s j (cid:105) : r ij = Softmax (cid:0) FFNN r (cid:0)(cid:2) s i , s j , s i ◦ s j (cid:3)(cid:1)(cid:1) ( 3 ) where ◦ denotes the element - wise multiplication , FFNN r is a mapping from R 3 × d → R | R | . We will use the notation r ij [ k ] to refer to the predicted probability of s i and s j having the relation k . At this point , one can already obtain a valid out - put for the task from the predicted entity and rela - tion scores . However , these predictions are based solely on the local document context , which can be difﬁcult to understand without any external domain knowledge . Therefore , our framework uses these predictions only to construct an initial span graph that will be reﬁned later based on information ex - tracted from an external knowledge source . To maintain computational efﬁciency , we ﬁrst prune out spans of text that are unlikely to be entity mentions . We only keep up to λn spans with the lowest probability scores of being a non - entity . The value of λ is selected empirically and set to be 0 . 5 . Spans that pass the ﬁlter are represented as nodes in the initial span graph . For every span pair (cid:104) s i , s j (cid:105) , we create | R | directed edges from the node representing s i to the node representing s j . Each edge represents one relation type and is weighted by the corresponding probability score in r ij . Let G s = { V s , E s } denote the initial span graph . We use a bidirectional GCN ( Marcheggiani and Titov , 2017 ; Fu et al . , 2019 ) to recursively update each span representation : (cid:126) h li = (cid:88) s j ∈ V s \ { s i } (cid:88) k ∈ R r ij [ k ] (cid:18) (cid:126) W ( l ) k h lj + (cid:126) b ( l ) k (cid:19) (cid:126) h li = (cid:88) s j ∈ V s \ { s i } (cid:88) k ∈ R r ji [ k ] (cid:18) (cid:126) W ( l ) k h lj + (cid:126) b ( l ) k (cid:19) h l + 1 i = h li + FFNN ( l ) a (cid:32) ReLU (cid:18)(cid:2) (cid:126) h li , (cid:126) h li (cid:3)(cid:19)(cid:33) ( 4 ) where h li is the hidden feature vector of span s i at layer l . We initialize h 0 i to be s i ( Eq . 1 ) . FFNN ( l ) a is a feedforward network whose output dimension is the same as the dimension of h li . After multiple iterations of message passing , each span representation will contain the global relational information of G s . Let h i denote the fea - ture vector at the ﬁnal layer of the GCN . Note that the dimension of h i is the same as the dimension of s i ( i . e . , h i ∈ R d ) . 2 . 4 Background Knowledge Graph Construction In this work , we utilize external knowledge from the Uniﬁed Medical Language System ( UMLS ) ( Bodenreider , 2004 ) . UMLS consists of three main components : Metathesaurus , Semantic Network , and Specialist Lexicon and Lexical Tools . The Metathesaurus provides information about millions of ﬁne - grained biomedical concepts and relations between them . To be consistent with the existing literature on knowledge graphs , we will refer to UMLS concepts as entities . Each entity is anno - tated with one or more higher - level semantic types , such as Anatomical Structure , Cell , or Virus . In addition to relations between entities , there are also semantic relations between semantic types . For example , there is an affects relation from Acquired Abnormality to Physiologic Function . This infor - mation is provided by the Semantic Network . We ﬁrst extract UMLS biomedical entities from the input document D using MetaMap , an entity mapping tool for UMLS ( Aronson and Lang , 2010 ) . We then construct a background knowledge graph ( KG ) from the extracted information . More specif - ically , we ﬁrst create a node for every extracted biomedical entity . The semantic types of each en - tity node are also modeled as type nodes that are linked with associated entity nodes . Finally , we create an edge for every relevant relation found in the Metathesaurus and the Semantic Network . An example KG is in the grey shaded region of Figure 2 . Circles represent entity nodes , and rectangles represent nodes that correspond to semantic types . Note that we simply run MetaMap with the de - fault options and do not tune it . In our experiments , we found that MetaMap typically returns many can - didate entities unrelated to the input text . However , as to be discussed in Section 3 . 4 , we show that KECI can learn to ignore the irrelevant entities . Let G k = { V k , E k } denote the constructed back - ground KG , where V k and E k are the node and edge sets , respectively . We use a set of UMLS em - beddings pretrained by Maldonado et al . ( 2019 ) to initialize the representation of each node in V k . We also use SciBERT to encode the UMLS deﬁnition sentence of each node into a vector and concatenate it to the initial representation . After that , since G k is a heterogeneous relational graph , we use a rela - tional GCN ( Schlichtkrull et al . , 2018 ) to update the representation of each node v i : v l + 1 i = ReLU (cid:32) U ( l ) v li + (cid:88) k ∈ R (cid:88) v j ∈ N ki (cid:18) 1 c i , k U ( l ) k v lj (cid:19)(cid:33) ( 5 ) where v li is the feature vector of v i at layer l . N ki is the set of neighbors of v i under relation k ∈ R . c i , k is a normalization constant and set to be | N ki | . After multiple iterations of message passing are performed , the global relational information of the KG will be integrated into each node’s representa - tion . Let v i denote the feature vector at the ﬁnal layer of the relational GCN . We further project each vector v i to another vector n i using a simple feed - forward network , so that n i has the same dimension as the span representations ( i . e . , n i ∈ R d ) . 2 . 5 Final Span Graph Prediction At this point , we have two graphs : the initial span graph G s = { V s , E s } ( Sec . 2 . 3 ) and the back - ground knowledge graph G k = { V k , E k } ( Sec . 2 . 4 ) . We have also obtained a structure - aware rep - resentation for each node in each graph ( i . e . , h i for Figure 3 : An illustration of the attention mechanism . each span s i ∈ V s and n j for each entity v j ∈ V k ) . The next step is to soft - align the mentions and the candidate entities using an attention mechanism ( Figure 3 ) . Let C ( s i ) denote the set of candidate entities for a span s i ∈ V s . For example , in Figure 2 , the mention FKBP12 has two candidate entities , while FK506 has only one candidate . For each candidate entity v j ∈ C ( s i ) , we calculate a scalar score α ij indicating how relevant v j is to s i : α ij = FFNN c (cid:0)(cid:2) h i , n j (cid:3)(cid:1) ( 6 ) where FFNN c is a feedforward network mapping from R 2 × d → R . Then we compute an additional sentinel vector c i ( Yang and Mitchell , 2017 ; He et al . , 2020 ) and also compute a score α i for it : c i = FFNN s (cid:0) h i (cid:1) α i = FFNN c (cid:0)(cid:2) h i , c i (cid:3)(cid:1) ( 7 ) where FFNN s is another feedforward network map - ping from R d → R d . Intuitively , c i records the information of the local context of s i , and α i mea - sures the importance of such information . After that , we compute a ﬁnal knowledge - aware repre - sentation f i for each span s i as follows : Z = exp ( α i ) + (cid:88) v z ∈ C ( s i ) exp ( α iz ) β i = exp ( α i ) / Z and β ij = exp ( α ij ) / Z f i = β i c i + (cid:88) v j ∈ C ( s i ) β ij n j ( 8 ) The attention mechanism is illustrated in Figure 3 . With the extracted knowledge - aware span repre - sentations , we predict the ﬁnal span graph in a way similar to Eq . 2 and Eq . 3 : (cid:98) e i = Softmax (cid:0) FFNN (cid:98) e ( f i ) (cid:1) (cid:99) r ij = Softmax (cid:0) FFNN (cid:98) r ( (cid:2) f i , f j , f i ◦ f j (cid:3) ) (cid:1) ( 9 ) where FFNN (cid:98) e is a mapping from R d → R | E | , and FFNN (cid:98) r is a mapping from R 3 × d → R | R | . (cid:98) e i is the ﬁnal predicted probability distribution over possi - ble entity types for span s i . (cid:99) r ij is the ﬁnal predicted probability distribution over possible relation types for span pair (cid:104) s i , s j (cid:105) . 2 . 6 Training The total loss is computed as : L total = ( L e 1 + L r 1 ) + 2 ( L e 2 + L r 2 ) ( 10 ) where L e * denotes the cross - entropy loss of span classiﬁcation . L r * denotes the binary cross - entropy loss of relation classiﬁcation . L e 1 and L r 1 are loss terms for the initial span graph prediction ( Eq . 2 and Eq . 3 of Section 2 . 3 ) . L e 2 and L r 2 are loss terms for the ﬁnal span graph prediction ( Eq . 9 of Section 2 . 5 ) . We apply a larger weight score to the loss terms L e 2 and L r 2 . We train the framework using only ground - truth labels of the entity and relation extraction tasks . We do not make use of any entity linking supervision in this work . 3 Experiments and Results 3 . 1 Data and Experiments Setup Datasets and evaluation metrics We evaluate KECI on two benchmark datasets : BioRelEx and ADE . The BioRelEx dataset ( Khachatrian et al . , 2019 ) consists of 2 , 010 sentences from biomedi - cal literature that capture binding interactions be - tween proteins and / or biomolecules . BioRelEx has annotations for 33 types of entities and 3 types of relations for binding interactions . The train - ing , development , and test splits contain 1 , 405 , 201 , and 404 sentences , respectively . The train - ing and development sets are publicly available . The test set is unreleased and can only be evaluated against using CodaLab 2 . For BioRelEx , we report Micro - F1 scores . The ADE dataset ( Gurulingappa et al . , 2012 ) consists of 4 , 272 sentences extracted from medical reports that describe drug - related ad - verse effects . Two entity types ( Adverse - Effect and Drug ) and a single relation type ( Adverse - Effect ) are pre - deﬁned . Similar to previous work ( Eberts 2 https : / / competitions . codalab . org / competitions / 20468 Model Entity ( Micro - F1 ) Relation ( Micro - F1 ) SciIE ( 2018 ) 77 . 90 49 . 60 DYGIEPP + ELMo ( 2020 ) 81 . 10 55 . 60 DYGIEPP + BioELMo ( 2020 ) 82 . 80 54 . 80 SentContextOnly 83 . 98 63 . 90 FlatAttention 84 . 32 64 . 23 KnowBertAttention 85 . 69 65 . 13 Full Model ( KECI ) 87 . 42 66 . 09 Table 1 : Overall results ( % ) on the development set of BioRelEx . Model Entity ( Micro - F1 ) Relation ( Micro - F1 ) SciIE ( 2018 ) 73 . 56 50 . 15 Second Best Model 82 . 76 62 . 18 Full Model ( KECI ) 87 . 35 67 . 09 Table 2 : Overall results ( % ) on the test set of BioRelEx ( from the leaderboard as of January 20th , 2021 ) . and Ulges , 2020 ; Ji et al . , 2020 ) , we conduct 10 - fold cross - validation and report averaged Macro - F1 scores . All the reported results take overlapping entities into consideration . Implementation details We implement KECI using PyTorch ( Paszke et al . , 2019 ) and Hugging - face’s Transformers ( Wolf et al . , 2020 ) . KECI uses SciBERT as the Transformer encoder ( Beltagy et al . , 2019 ) . All details about hyperparameters and reproducibility information are in the appendix . Baselines for comparison In addition to com - paring our method with state - of - the - art methods on the above two datasets , we implement the follow - ing baselines for further comparison and analysis : 1 . SentContextOnly : This baseline does not use any external knowledge . It uses only the local sentence context for prediction . It ex - tracts the ﬁnal output directly from the predic - tions obtained using Eq . 2 and Eq . 3 . 2 . FlatAttention : This baseline does not rely on collective inference . It does not integrate any global relational information into mention and entity representations . Each h i mentioned in Sec . 2 . 3 is set to be s i ( Eq . 1 ) , and each v i mentioned in Sec . 2 . 4 is set to be v 0 i . Then , the prediction of the ﬁnal span graph is the same as described in Sec . 2 . 5 . 3 . KnowBertAttention : This baseline uses the Knowledge Attention and Recontextualization ( KAR ) mechanism of KnowBert ( Peters et al . , 2019 ) , a state - of - the - art knowledge - enhanced Model Entity ( Macro - F1 ) Relation ( Macro - F1 ) Relation - Metric ( 2019 ) 87 . 11 77 . 29 SpERT ( 2020 ) 89 . 28 78 . 84 SPAN Multi - Head ( 2020 ) 90 . 59 80 . 73 SentContextOnly 88 . 13 77 . 23 FlatAttention 89 . 16 78 . 81 KnowBertAttention 90 . 08 79 . 95 Full Model ( KECI ) 90 . 67 81 . 74 Table 3 : Overall results ( % ) on the ADE dataset . Ablation setting Entity ( Micro - F1 ) Relation ( Micro - F1 ) Full Model ( KECI ) 87 . 42 66 . 09 • w / o external knowledge 83 . 98 * 63 . 90 * • w / o collective inference 84 . 32 * 64 . 23 * • w / o the bidirectional GCN 84 . 76 * 64 . 25 * • w / o the relational GCN 85 . 14 * 65 . 32 * • w / o the pretrained UMLS vectors 86 . 25 † 65 . 29 * • w / o the UMLS deﬁnition vectors 86 . 76 † 65 . 45 † Table 4 : Results ( % ) of ablation experiments on the de - velopment set of BioRelEx . We use the symbols * and † to indicate statistical signiﬁcance with 95 % and 90 % conﬁdence levels respectively ( compared to KECI ) . language model . The baseline ﬁrst uses SciB - ERT to construct initial token - level represen - tations . It then uses the KAR mechanism to inject external knowledge from UMLS into the token - level vectors . Finally , it embeds text spans into feature vectors ( Eq . 1 ) and uses the span representations to extract entities and relations in one pass ( similar to Eq . 9 ) . For fair comparison , all the baselines use SciBERT as the Transformer encoder . A major difference between KECI and Know - BertAttention ( Peters et al . , 2019 ) is that KECI explicitly builds and extracts information from a multi - relational graph structure of the candidate en - tity mentions before the knowledge fusion process . In contrast , KnowBertAttention only uses SciBERT to extract features from the candidate entity men - tions . Therefore , KnowBertAttention only takes advantage of the entity - entity co - occurrence infor - mation . On the other hand , KECI integrates more ﬁne - grained global relational information ( e . g . , the binding interactions shown in Figure 2 ) into the mention representations . This difference makes KECI achieve better overall performance , as to be discussed next . 3 . 2 Overall Results Table 1 and Table 2 show the overall results on the development and test sets of BioRelEx , re - spectively . Compared to SentContextOnly , KECI achieves much higher performance . This demon - strates the importance of incorporating external knowledge for biomedical information extraction . KECI also outperforms the baseline FlatAttention by a large margin , which shows the beneﬁt of col - lective inference . In addition , we see that our model performs better than the baseline KnowBertAtten - tion . Finally , at the time of writing , KECI achieves the ﬁrst position on the BioRelEx leaderboard 3 . Table 3 shows the overall results on ADE . KECI again outperforms all the baselines and state - of - the - art models such as SpERT ( Eberts and Ulges , 2020 ) and SPAN Multi - Head ( Ji et al . , 2020 ) . This further conﬁrms the effectiveness of our framework . Overall , the two datasets used in this work focus on two very different subareas of the biomedical domain , and KECI was able to push the state - of - the - art results of both datasets . This indicates that our proposed approach is highly generalizable . 3 . 3 Ablation Study Table 4 shows the results of ablation studies we did on the development set of the BioRelEx benchmark . We compare our full model against several partial variants . The variant [ w / o external knowledge ] is the same as the baseline SentContextOnly , and the variant [ w / o collective inference ] is the same as the baseline FlatAttention ( Section 3 . 1 ) . For the variant [ w / o the bidirectional GCN ] , we simply set each h i mentioned in Section 2 . 3 to be s i . Similarly , for the variant [ w / o the relational GCN ] , we set each v i in Section 2 . 4 to be v 0 i . The last two variants are related to the initialization of each vector v 0 i . We see that all the partial variants perform worse than our full model . This shows that each compo - nent of KECI plays an important role . 3 . 4 Attention Pattern Analysis There is no gold - standard set of correspondences between the entity mentions in the datasets and the UMLS entities . Therefore , we cannot directly evaluate the entity linking performance of KECI . However , for each UMLS semantic type , we com - pute the average attention weight that an entity of that type gets assigned ( Table 5 ) . Overall , we see 3 https : / / competitions . codalab . org / competitions / 20468 that KECI typically pays the most attention to the relevant informative entities while ignoring the ir - relevant ones . 3 . 5 Qualitative Analysis Table 6 shows some examples from the ADE dataset that illustrate how incorporating external knowledge can improve the performance of joint biomedical entity and relation extraction . In the ﬁrst example , initially , there is no edge between the node “bleeding symptoms” and the node “warfarin” , probably because of the distance between their corresponding spans in the original input sentence . However , KECI can link the term “warfarin” to a UMLS entity ( CUI : C0043031 ) , and the deﬁnition in UMLS says that warfarin is a type of anticoagulant that prevents the formation of blood clots . As the initial feature vector of each entity contains the representation of its deﬁnition ( Sec . 2 . 4 ) , KECI can recover the missing edge . In the second example , the initial span graph is predicted to have three entities of type Adverse - Effect , which correspond to three different overlap - ping text spans . Among these three , only “retroperi - toneal ﬁbrosis” can be linked to a UMLS entity . It is also evident from the input sentence that one of these spans is related to “methysergide” . As a result , KECI successfully removes the other two unlinked span nodes to create the ﬁnal span graph . In the third example , probably because of the phrase “due to” , the node “endometriosis” is ini - tially predicted to be of type Drug , and the node “acute abdomen” is predicted to be its Adverse - Effect . However , KECI can link the term “en - dometriosis” to a UMLS entity of semantic type Disease or Syndrome . As a result , the system can correct the term’s type and also predict the right edges for the ﬁnal span graph . Finally , we also examined the errors made by KECI . One major issue is that MetaMap sometimes fails to return any candidate entity from UMLS for an entity mention . We leave the extension of this work to using multiple KBs as future work . 4 Related Work Traditional pipelined methods typically treat entity extraction and relation extraction as two separate tasks ( Zelenko et al . , 2002 ; Zhou et al . , 2005 ; Chan and Roth , 2011 ) . Such approaches ignore the close interaction between named entities and their rela - tion information and typically suffer from the error Datasets Top 3 types with the lowest avg . attention scores Top 3 types with the highest avg . attention scores BioRelEx Diagnostic Procedure ( 0 . 04 ) ; Activity ( 0 . 05 ) ; Plant ( 0 . 05 ) Amino Acid , Peptide , or Protein ( 0 . 32 ) ; Enzyme ( 0 . 32 ) ; Molecular Function ( 0 . 36 ) ADE Intellectual Product ( 0 . 15 ) ; Idea or Concept ( 0 . 19 ) ; Temporal Concept ( 0 . 19 ) Antibiotic ( 0 . 78 ) ; Organic Chemical ( 0 . 79 ) ; Nucleic Acid , Nucleoside , or Nucleotide ( 0 . 87 ) Table 5 : Average attention scores of different UMLS semantic types . Input Sentence Initial Span Graph Final Span Graph # 1 : Despite the low dosage of warfarin , interna - tional normalized ratio ( INR ) was markedly el - evated from 1 . 15 to 11 . 28 for only 4 days , and bleeding symptoms concurrently developed . # 2 : A 25 - year - old woman sought medical attention because of iliocaval manifestations of retroperi - toneal ﬁbrosis while she was taking methysergide . # 3 : TITLE : Acute abdomen due to endometriosis in a premenopausal woman taking tamoxifen . Table 6 : Examples showing how external knowledge improves the quality of extracted span graphs . Edges repre - sent relations of type Adverse - Effect . Only relations with predicted probabilities of at least 0 . 5 are shown . propagation problem . To overcome these limita - tions , many studies have proposed joint models that perform entity extraction and relation extrac - tion simultaneously ( Roth and Yih , 2007 ; Li and Ji , 2014 ; Li et al . , 2017 ; Zheng et al . , 2017 ; Bek - oulis et al . , 2018a , b ; Wadden et al . , 2019 ; Fu et al . , 2019 ; Luan et al . , 2019 ; Zhao et al . , 2020 ; Wang and Lu , 2020 ; Li et al . , 2020b ; Lin et al . , 2020 ) . Particularly , span - based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities . For example , Eberts and Ulges ( 2020 ) propose SpERT , a simple but effective span - based model that utilizes BERT as its core . The recent work of Ji et al . ( 2020 ) also closely follows the overall architecture of SpERT but differs in span - speciﬁc and contextual semantic representations . Despite their impressive perfor - mance , these methods are not designed speciﬁcally for the biomedical domain , and they do not utilize any external knowledge base . To the best of our knowledge , our work is the ﬁrst span - based frame - work that utilizes external knowledge for joint en - tity and relation extraction from biomedical text . Biomedical event extraction is a closely related task that has also received a lot of attention from the research community ( Poon and Vanderwende , 2010 ; Kim et al . , 2013 ; V S S Patchigolla et al . , 2017 ; Rao et al . , 2017 ; Espinosa et al . , 2019 ; Li et al . , 2019 ; Wang et al . , 2020 ; Huang et al . , 2020 ; Ramponi et al . , 2020 ; Yadav et al . , 2020 ) . Sev - eral studies have proposed to incorporate external knowledge from domain - speciﬁc KBs into neural models for biomedical event extraction . For ex - ample , Li et al . ( 2019 ) incorporate entity informa - tion from Gene Ontology into tree - LSTM mod - els . However , their approach does not explicitly use any external relational information . Recently , Huang et al . ( 2020 ) introduce a framework that uses a novel Graph Edge conditioned Attention Network ( GEANet ) to utilize domain knowledge from UMLS . In the framework , a global KG for the entire corpus is ﬁrst constructed , and then a sentence - level KG is created for each individual sentence in the corpus . Our method of KG con - struction is more ﬂexible as we directly create a KG for each input text . Furthermore , the work of Huang et al . ( 2020 ) only deals with event extraction and assumes that gold - standard entity mentions are provided at inference time . Some previous work has focused on integrat - ing external knowledge into neural architectures for other tasks , such as reading comprehension ( Mihaylov and Frank , 2018 ) , question answer - ing ( Pan et al . , 2019 ) , natural language inference ( Sharma et al . , 2019 ) , and conversational modeling ( Parthasarathi and Pineau , 2018 ) . Different from these studies , our work explicitly emphasizes the beneﬁt of collective inference using global rela - tional information . Many previous studies have also used GNNs for various IE tasks ( Nguyen and Grishman , 2018 ; Liu et al . , 2018 ; Subburathinam et al . , 2019 ; Zeng et al . , 2021 ; Zhang and Ji , 2021 ) . Many of these meth - ods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens / spans . However , parsers for spe - cialized biomedical domains are expensive to build . KECI does not rely on such expensive resources . 5 Conclusions and Future Work In this work , we propose a novel span - based frame - work named KECI that utilizes external domain knowledge for joint entity and relation extraction from biomedical text . Experimental results show that KECI is highly effective , achieving new state - of - the - art results on two datasets : BioRelEx and ADE . Theoretically , KECI can take an entire docu - ment as input ; however , the tested datasets are only sentence - level datasets . In the future , we plan to evaluate our framework on more document - level datasets . We also plan to explore a broader range of properties and information that can be extracted from external KBs to facilitate biomedical IE tasks . Finally , we also plan to apply KECI to other infor - mation extraction tasks ( Li et al . , 2020a ; Lai et al . , 2021 ; Wen et al . , 2021 ) . Acknowledgement We thank the three reviewers and the Area Chair for their insightful comments and suggestions . This research is based upon work supported by the Molecule Maker Lab Institute : An AI Research Institutes program supported by NSF under Award No . 2019897 , NSF No . 2034562 , U . S . DARPA KAIROS Program No . FA8750 - 19 - 2 - 1004 , the Ofﬁce of the Director of National Intelligence ( ODNI ) , Intelligence Advanced Research Projects Activity ( IARPA ) , via contract No . FA8650 - 17 - C - 9116 . Any opinions , ﬁndings and conclusions or recommendations expressed in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies , either expressed or implied , of the U . S . Government . The U . S . Gov - ernment is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on . References D . Alvermann , L . Smith , and J . Readence . 1985 . Prior knowledge activation and the comprehension of compatible and incompatible text . Reading Re - search Quarterly , 20 : 420 . A . Aronson and F . Lang . 2010 . An overview of metamap : historical perspective and recent ad - vances . Journal of the American Medical Informat - ics Association : JAMIA , 17 3 : 229 – 36 . Giannis Bekoulis , J . Deleu , Thomas Demeester , and Chris Develder . 2018a . Joint entity recognition and relation extraction as a multi - head selection problem . ArXiv , abs / 1804 . 07847 . Giannis Bekoulis , Johannes Deleu , Thomas Demeester , and Chris Develder . 2018b . Adversarial training for multi - context joint entity and relation extrac - tion . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2830 – 2836 , Brussels , Belgium . Association for Computational Linguistics . Iz Beltagy , Kyle Lo , and Arman Cohan . 2019 . Scib - ert : Pretrained language model for scientiﬁc text . In EMNLP . Abhinav Bhatt and Kaustubh D . Dhole . 2020 . Bench - marking biorelex for entity tagging and relation ex - traction . ArXiv , abs / 2006 . 00533 . O . Bodenreider . 2004 . The uniﬁed medical language system ( umls ) : integrating biomedical terminology . Nucleic acids research , 32 Database issue : D267 – 70 . Jason Braasch and S . Goldman . 2010 . The role of prior knowledge in learning from analogies in sci - ence texts . Discourse Processes , 47 : 447 – 479 . Yee Seng Chan and Dan Roth . 2011 . Exploiting syntactico - semantic structures for relation extrac - tion . In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Hu - man Language Technologies , pages 551 – 560 , Port - land , Oregon , USA . Association for Computational Linguistics . Markus Eberts and A . Ulges . 2020 . Span - based joint entity and relation extraction with transformer pre - training . In European Conference on Artiﬁcial Intel - ligence . Kurt Junshean Espinosa , Makoto Miwa , and Sophia Ananiadou . 2019 . A search - based neural model for biomedical nested and overlapping event detection . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan - guage Processing ( EMNLP - IJCNLP ) , pages 3679 – 3686 , Hong Kong , China . Association for Computa - tional Linguistics . Hao Fei , Yue Zhang , Yafeng Ren , and Donghong Ji . 2020 . A span - graph neural model for overlapping entity relation extraction in biomedical texts . Bioin - formatics . Btaa993 . Tsu - Jui Fu , Peng - Hsuan Li , and Wei - Yun Ma . 2019 . GraphRel : Modeling text as relational graphs for joint entity and relation extraction . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1409 – 1418 , Flo - rence , Italy . Association for Computational Linguis - tics . Harsha Gurulingappa , Abdul Mateen Rajput , Angus Roberts , Juliane Fluck , Martin Hofmann - Apitius , and Luca Toldo . 2012 . Development of a benchmark corpus to support the automatic extraction of drug - related adverse effects from medical case reports . Journal of Biomedical Informatics , 45 ( 5 ) : 885 – 892 . C . Harris , K . J . Millman , S . Walt , Ralf Gommers , P . Virtanen , D . Cournapeau , E . Wieser , J . Taylor , S . Berg , Nathaniel J . Smith , R . Kern , Matti Pi - cus , S . Hoyer , M . Kerkwijk , Matthew Brett , Allan Haldane , Jaime Fern’andez del R’io , Mark Wiebe , P . Peterson , Pierre G’erard - Marchant , K . Sheppard , T . Reddy , W . Weckesser , H . Abbasi , Christoph Gohlke , and T . E . Oliphant . 2020 . Array program - ming with numpy . Nature , 585 7825 : 357 – 362 . Keqing He , Yuanmeng Yan , and Weiran Xu . 2020 . Learning to tag OOV tokens by integrating contex - tual representation and background knowledge . In Proceedings of the 58th Annual Meeting of the As - sociation for Computational Linguistics , pages 619 – 624 , Online . Association for Computational Linguis - tics . Kung - Hsiang Huang , Mu Yang , and Nanyun Peng . 2020 . Biomedical event extraction with hierarchi - cal knowledge graphs . In Findings of the Associa - tion for Computational Linguistics : EMNLP 2020 , pages 1277 – 1285 , Online . Association for Compu - tational Linguistics . Bin Ji , Jie Yu , Shasha Li , Jun Ma , Qingbo Wu , Yusong Tan , and Huijun Liu . 2020 . Span - based joint entity and relation extraction with attention - based span - speciﬁc and contextual semantic representations . In Proceedings of the 28th International Conference on Computational Linguistics , pages 88 – 99 , Barcelona , Spain ( Online ) . International Committee on Compu - tational Linguistics . Hrant Khachatrian , Lilit Nersisyan , Karen Ham - bardzumyan , Tigran Galstyan , Anna Hakobyan , Ar - sen Arakelyan , Andrey Rzhetsky , and Aram Gal - styan . 2019 . BioRelEx 1 . 0 : Biological relation ex - traction benchmark . In Proceedings of the 18th BioNLP Workshop and Shared Task , pages 176 – 190 , Florence , Italy . Association for Computational Lin - guistics . Jin - Dong Kim , Yue Wang , and Yamamoto Yasunori . 2013 . The Genia event extraction shared task , 2013 edition - overview . In Proceedings of the BioNLP Shared Task 2013 Workshop , pages 8 – 15 , Soﬁa , Bul - garia . Association for Computational Linguistics . Tuan Lai , Heng Ji , Trung Bui , Quan Hung Tran , Franck Dernoncourt , and Walter Chang . 2021 . A context - dependent gated module for incorporating symbolic semantics into event coreference resolution . In Pro - ceedings of the 2021 Conference of the North Amer - ican Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3491 – 3499 , Online . Association for Computational Linguistics . Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho So , and Jaewoo Kang . 2019 . BioBERT : a pre - trained biomedical language representation model for biomedical text mining . Bioinformatics , 36 ( 4 ) : 1234 – 1240 . Kenton Lee , Luheng He , Mike Lewis , and Luke Zettle - moyer . 2017 . End - to - end neural coreference reso - lution . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 188 – 197 , Copenhagen , Denmark . Association for Computational Linguistics . Diya Li , Lifu Huang , Heng Ji , and Jiawei Han . 2019 . Biomedical event extraction based on knowledge - driven tree - LSTM . In Proceedings of the 2019 Con - ference of the North American Chapter of the Asso - ciation for Computational Linguistics : Human Lan - guage Technologies , Volume 1 ( Long and Short Pa - pers ) , pages 1421 – 1430 , Minneapolis , Minnesota . Association for Computational Linguistics . Fei Li , Meishan Zhang , G . Fu , and D . Ji . 2017 . A neu - ral joint model for entity and relation extraction from biomedical text . BMC Bioinformatics , 18 . Manling Li , Ying Lin , Tuan Manh Lai , Xiaoman Pan , Haoyang Wen , Sha Li , Zhenhailong Wang , Pengfei Yu , Lifu Huang , Di Lu , Qingyun Wang , Haoran Zhang , Qi Zeng , Chi Han , Zixuan Zhang , Yujia Qin , Xiaodan Hu , Nikolaus Parulian , Daniel Campos , Heng Ji , Brian Chen , Xudong Lin , Alireza Zareian , Amith Ananthram , Emily Allaway , Shih - Fu Chang , Kathleen McKeown , Yixiang Yao , Michael Spec - tor , Mitchell DeHaven , Daniel Napierski , Marjorie Freedman , Pedro Szekely , Haidong Zhu , Ram Neva - tia , Yang Bai , Yifan Wang , Ali Sadeghian , Haodi Ma , and Daisy Zhe Wang . 2020a . GAIA at SM - KBP 2020 - a dockerlized multi - media multi - lingual knowledge extraction , clustering , temporal tracking and hypothesis generation system . In Proceedings of Thirteenth Text Analysis Conference ( TAC 2020 ) . Qi Li and Heng Ji . 2014 . Incremental joint extraction of entity mentions and relations . In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 402 – 412 , Baltimore , Maryland . Association for Computational Linguistics . Zhijing Li , Yuchen Lian , Xiaoyong Ma , Xiangrong Zhang , and Chen Li . 2020b . Bio - semantic relation extraction with attention - based external knowledge reinforcement . BMC Bioinformatics , 21 . Ying Lin , Heng Ji , Fei Huang , and Lingfei Wu . 2020 . A joint neural model for information extraction with global features . In Proceedings of the 58th Annual Meeting of the Association for Computational Lin - guistics , pages 7999 – 8009 , Online . Association for Computational Linguistics . Xiao Liu , Zhunchen Luo , and Heyan Huang . 2018 . Jointly multiple events extraction via attention - based graph information aggregation . In Proceed - ings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1247 – 1256 , Brussels , Belgium . Association for Computational Linguistics . Yi Luan , Luheng He , Mari Ostendorf , and Hannaneh Hajishirzi . 2018 . Multi - task identiﬁcation of enti - ties , relations , and coreference for scientiﬁc knowl - edge graph construction . In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan - guage Processing , pages 3219 – 3232 , Brussels , Bel - gium . Association for Computational Linguistics . Yi Luan , Dave Wadden , Luheng He , Amy Shah , Mari Ostendorf , and Hannaneh Hajishirzi . 2019 . A gen - eral framework for information extraction using dy - namic span graphs . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3036 – 3046 , Minneapolis , Minnesota . Association for Computational Linguistics . Ling Luo , Zhihao Yang , M . Cao , Lei Wang , Y . Zhang , and Hongfei Lin . 2020 . A neural network - based joint learning approach for biomedical entity and re - lation extraction from biomedical literature . Journal of biomedical informatics , page 103384 . R . Maldonado , Meliha Yetisgen , and Sanda M . Harabagiu . 2019 . Adversarial learning of knowl - edge embeddings for the uniﬁed medical language system . AMIA Joint Summits on Translational Sci - ence proceedings . AMIA Joint Summits on Transla - tional Science , 2019 : 543 – 552 . Diego Marcheggiani and Ivan Titov . 2017 . Encoding sentences with graph convolutional networks for se - mantic role labeling . In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan - guage Processing , pages 1506 – 1515 , Copenhagen , Denmark . Association for Computational Linguis - tics . Todor Mihaylov and Anette Frank . 2018 . Knowledge - able reader : Enhancing cloze - style reading compre - hension with external commonsense knowledge . In Proceedings of the 56th Annual Meeting of the As - sociation for Computational Linguistics ( Volume 1 : Long Papers ) , pages 821 – 832 , Melbourne , Australia . Association for Computational Linguistics . T . Nguyen and R . Grishman . 2018 . Graph convo - lutional networks with argument - aware pooling for event detection . In AAAI . Xiaoman Pan , Kai Sun , Dian Yu , Jianshu Chen , Heng Ji , Claire Cardie , and Dong Yu . 2019 . Improving question answering with external knowledge . In Proc . EMNLP2019 Workshop on Machine Reading for Question Answering . Prasanna Parthasarathi and Joelle Pineau . 2018 . Ex - tending neural generative conversational model us - ing external knowledge sources . In Proceedings of the 2018 Conference on Empirical Methods in Nat - ural Language Processing , pages 690 – 695 , Brus - sels , Belgium . Association for Computational Lin - guistics . Adam Paszke , S . Gross , Francisco Massa , A . Lerer , J . Bradbury , G . Chanan , T . Killeen , Z . Lin , N . Gimelshein , L . Antiga , Alban Desmaison , An - dreas Köpf , E . Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , B . Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . Pytorch : An imperative style , high - performance deep learning library . In NeurIPS . Matthew E . Peters , Mark Neumann , Robert Logan , Roy Schwartz , Vidur Joshi , Sameer Singh , and Noah A . Smith . 2019 . Knowledge enhanced contextual word representations . In Proceedings of the 2019 Con - ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer - ence on Natural Language Processing ( EMNLP - IJCNLP ) , pages 43 – 54 , Hong Kong , China . Associ - ation for Computational Linguistics . Hoifung Poon and Lucy Vanderwende . 2010 . Joint inference for knowledge extraction from biomedi - cal literature . In Human Language Technologies : The 2010 Annual Conference of the North Ameri - can Chapter of the Association for Computational Linguistics , pages 813 – 821 , Los Angeles , California . Association for Computational Linguistics . Alan Ramponi , Rob van der Goot , Rosario Lombardo , and Barbara Plank . 2020 . Biomedical event extrac - tion as sequence labeling . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5357 – 5367 , Online . Association for Computational Linguistics . Sudha Rao , Daniel Marcu , Kevin Knight , and Hal Daumé III . 2017 . Biomedical event extraction us - ing Abstract Meaning Representation . In BioNLP 2017 , pages 126 – 135 , Vancouver , Canada , . Associa - tion for Computational Linguistics . Dan Roth and Wen - tau Yih . 2007 . Global inference for entity and relation identiﬁcation via a linear pro - gramming formulation . Introduction to statistical re - lational learning , pages 553 – 580 . M . Schlichtkrull , Thomas Kipf , P . Bloem , R . V . Berg , Ivan Titov , and M . Welling . 2018 . Modeling re - lational data with graph convolutional networks . ArXiv , abs / 1703 . 06103 . Soumya Sharma , Bishal Santra , Abhik Jana , Santosh Tokala , Niloy Ganguly , and Pawan Goyal . 2019 . In - corporating domain knowledge into medical NLI using knowledge graphs . In Proceedings of the 2019 Conference on Empirical Methods in Natu - ral Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6092 – 6097 , Hong Kong , China . Association for Computational Linguistics . Mohammad Golam Sohrab and Makoto Miwa . 2018 . Deep exhaustive model for nested named entity recognition . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process - ing , pages 2843 – 2849 , Brussels , Belgium . Associa - tion for Computational Linguistics . Ananya Subburathinam , Di Lu , Heng Ji , Jonathan May , Shih - Fu Chang , Avirup Sil , and Clare Voss . 2019 . Cross - lingual structure transfer for relation and event extraction . In Proc . 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP2019 ) . T . Tran and Ramakanth Kavuluru . 2019 . Neural met - ric learning for fast end - to - end relation extraction . ArXiv , abs / 1905 . 07458 . Rahul V S S Patchigolla , Sunil Sahu , and Ashish Anand . 2017 . Biomedical event trigger identiﬁ - cation using bidirectional recurrent neural network based models . In BioNLP 2017 , pages 316 – 321 , Vancouver , Canada , . Association for Computational Linguistics . David Wadden , Ulme Wennberg , Yi Luan , and Han - naneh Hajishirzi . 2019 . Entity , relation , and event extraction with contextualized span representations . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan - guage Processing ( EMNLP - IJCNLP ) , pages 5784 – 5789 , Hong Kong , China . Association for Computa - tional Linguistics . Jue Wang and Wei Lu . 2020 . Two are better than one : Joint entity and relation extraction with table - sequence encoders . In Proceedings of the 2020 Con - ference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1706 – 1721 , Online . As - sociation for Computational Linguistics . Xing David Wang , Leon Weber , and Ulf Leser . 2020 . Biomedical event extraction as multi - turn question answering . In Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis , pages 88 – 96 , Online . Association for Com - putational Linguistics . Haoyang Wen , Ying Lin , Tuan Lai , Xiaoman Pan , Sha Li , Xudong Lin , Ben Zhou , Manling Li , Haoyu Wang , Hongming Zhang , Xiaodong Yu , Alexander Dong , Zhenhailong Wang , Yi Fung , Piyush Mishra , Qing Lyu , Dídac Surís , Brian Chen , Susan Windisch Brown , Martha Palmer , Chris Callison - Burch , Carl Vondrick , Jiawei Han , Dan Roth , Shih - Fu Chang , and Heng Ji . 2021 . RESIN : A dockerized schema - guided cross - document cross - lingual cross - media in - formation extraction and event tracking system . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies : Demonstrations , pages 133 – 143 , Online . Associa - tion for Computational Linguistics . Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pier - ric Cistac , Tim Rault , Remi Louf , Morgan Funtow - icz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 . Trans - formers : State - of - the - art natural language process - ing . In Proceedings of the 2020 Conference on Em - pirical Methods in Natural Language Processing : System Demonstrations , pages 38 – 45 , Online . Asso - ciation for Computational Linguistics . Shweta Yadav , Pralay Ramteke , Asif Ekbal , Sriparna Saha , and Pushpak Bhattacharyya . 2020 . Exploring disorder - aware attention for clinical event extraction . 16 ( 1s ) . Bishan Yang and Tom Mitchell . 2017 . Leveraging knowledge bases in LSTMs for improving machine reading . In Proceedings of the 55th Annual Meet - ing of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1436 – 1446 , Van - couver , Canada . Association for Computational Lin - guistics . Dmitry Zelenko , Chinatsu Aone , and Anthony Richardella . 2002 . Kernel methods for relation ex - traction . In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2002 ) , pages 71 – 78 . Association for Com - putational Linguistics . Qi Zeng , Manling Li , Tuan Lai , Heng Ji , Mohit Bansal , and Hanghang Tong . 2021 . GENE : Global event network embedding . In Proceedings of the Fif - teenth Workshop on Graph - Based Methods for Nat - ural Language Processing ( TextGraphs - 15 ) , pages 42 – 53 , Mexico City , Mexico . Association for Com - putational Linguistics . Zixuan Zhang and Heng Ji . 2021 . Abstract Meaning Representation guided graph encoding and decoding for joint information extraction . In Proceedings of the 2021 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies , pages 39 – 49 , On - line . Association for Computational Linguistics . Shan Zhao , Minghao Hu , Zhiping Cai , and Fang Liu . 2020 . Modeling dense cross - modal interactions for joint entity - relation extraction . In Proceedings of the Twenty - Ninth International Joint Conference on Artiﬁcial Intelligence , IJCAI - 20 , pages 4032 – 4038 . International Joint Conferences on Artiﬁcial Intelli - gence Organization . Main track . Suncong Zheng , Feng Wang , Hongyun Bao , Yuexing Hao , Peng Zhou , and Bo Xu . 2017 . Joint extrac - tion of entities and relations based on a novel tag - ging scheme . In Proceedings of the 55th Annual Meeting of the Association for Computational Lin - guistics ( Volume 1 : Long Papers ) , pages 1227 – 1236 , Vancouver , Canada . Association for Computational Linguistics . GuoDong Zhou , Jian Su , Jie Zhang , and Min Zhang . 2005 . Exploring various knowledge in relation ex - traction . In Proceedings of the 43rd Annual Meet - ing of the Association for Computational Linguis - tics ( ACL’05 ) , pages 427 – 434 , Ann Arbor , Michi - gan . Association for Computational Linguistics . A Reproducibility Checklist In this section , we present the reproducibility infor - mation of the paper . We are planning to make the code publicly available after the paper is reviewed . Implementation Dependencies Libraries Py - torch 1 . 6 . 0 ( Paszke et al . , 2019 ) , Transformers 4 . 0 . 0 ( Wolf et al . , 2020 ) , DGL 0 . 5 . 3 4 , Numpy 1 . 19 . 1 ( Harris et al . , 2020 ) , CUDA 10 . 2 . Computing Infrastructure The experiments were conducted on a server with Intel ( R ) Xeon ( R ) Gold 5120 CPU @ 2 . 20GHz and NVIDIA Tesla V100 GPUs . The allocated RAM is 187G . GPU memory is 16G . Datasets The BioRelEx dataset ( Khachatrian et al . , 2019 ) is available at https : / / github . com / YerevaNN / BioRelEx . The ADE dataset ( Gurulin - gappa et al . , 2012 ) can be downloaded by using the script at https : / / github . com / markus - eberts / spert . 4 https : / / www . dgl . ai / Average Runtime Table 7 shows the estimated average run time of our full model . Number of Model Parameters The number of parameters in a full model trained on BioRelEx is about 121 . 0M parameters . The number of pa - rameters in a full model trained on ADE is about 119 . 9M parameters . Hyperparameters of Best - Performing Models The span length limit L is set to be 20 tokens . Note that the choice of L only has some noticeable ef - fects on the training time of KECI during the ﬁrst epoch . KECI with randomly initialized parame - ters may include many non - relevant spans in the initial span graph . However , after a few training iterations , KECI typically can ﬁlter out most non - relevant spans . The pruning parameter λ is set to be 0 . 5 . All of our models use SciBERT as the Transformer encoder ( Beltagy et al . , 2019 ) . We use two different learning rates , one for the lower pretrained Transformer encoder and one for the up - per layers . Table 8 summarizes the hyperparameter conﬁgurations of best - performing models . Expected Validation Performance The main paper has the results on the dev set of BioRelEx . For ADE , as in previous work , we conduct a 10 - fold cross validation . Hyperparameter Tuning Process We experi - mented with the following range of possible values : { 16 , 32 } for batch size , { 2e - 5 , 3e - 5 , 4e - 5 , 5e - 5 } for lower learning rate , { 1e - 4 , 2e - 4 , 5e - 4 } for upper learning rate , and { 50 , 100 } for number of training epochs . For each particular set of hyperparame - ters , we repeat training for 3 times and compute the average performance . Dataset One Training Epoch Evaluation ( Dev Set ) BioRelEx 337 . 51 seconds 35 . 38 seconds ADE 712 . 89 seconds 52 . 39 seconds Table 7 : Estimated average runtime of our full model . Hyperparameters BioRelEx ADE Lower Learning Rate 5e - 05 5e - 05 Upper Learning Rate 2e - 04 1e - 04 Batch Size 32 32 Number Epochs 50 50 Table 8 : Hyperparameters for best - performing models .