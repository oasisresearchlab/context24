Semiautomatic Generation of Glossary Links : A Practical Solution Hermann Kaindl , Stefan Kramer Siemens AG osterreich , Geusaugasse 17 , A - 1030 Wien , Austria cmail : { hermann . kaindl , stefan . kramer } @ siemens . at Papa Samba Niang Diallo Mitterberggasse 23 / 6 , A - 1180 Wien , Austria ABSTRACT Especially through the increasing popularity of the World - Wide Web ( WWW , Web ) , more and more hypertext is cre - ated . A major task in creating hypertext is link generation , and in particular for larger hypertexts , generating the links manually takes a lot of effort . Therefore , at least some sup - port for link generation is highly desirable . We faced an important case of this problem in practice - to make ex - plicit glossary links , i . e . , links within and into a ( technical ) glossary . So , we developed a new and interactive algorithm for the semiautomatic gcncration of glossary links in hypcr - text . In order to be useful in practice , it deals with incxacl matching of text and with names that may consist of scvcral words , which may overlap or encompass each other in the text . Since the hypertext author using this algorithm should have full control over which links to include , our approach relies on user cooperation and interaction . Therefore , the de - sign of our algorithm aims at reducing the cognitive overhead of the hypertext author . While we use this approach success - fully in practice , its performance there is hard to evaluate quantitatively . So , we present a novel experiment design for quantitatively measuring the success of semiautomatic link generation and its application to our new algorithm . Both our qualitative empirical evidence and these quantitative results suggest its usefulness . KEYWORDS : Authoring , hypertext , automatic link gener - ation , glossary links , WWW . BACKGROUND AND INTRODUCTION While hypertext showed its uscfulncss long bcforc the dcvcl - opment of the World - Wide Web ( WWW , Web ) , the increasing popularity of the WWW entails that more and more hyper - text is created . For the creation of hypertext , generating links is a major task to be performed . Especially for larger hy - pertexts , generating the links manually may take too much time and effort , so that at least some support for link gen - eration is highly desirable . When links are generated fully automatically , however , the author responsible for creating some hypertext does not have full control over which links to include . Therefore , we decided to focus on semiautomatic link gen - eration . This approach involves interaction with a human author of the hypertext who is supposed to know which links should be included or not . We assume that this author is responsible for the quality of the links installed and therefore needs full control over which links to include . Compared to manual creation , semiautomatic link generation saves major effort for recognizing link anchors and also some effort for technically installing links , since just clicking “yes” or “no” is both easier and faster than the best possible way of fully installing a link . In practice , an important case of the general link generation problem is to make explicit what we define as glossary links : these are links within and into a glossary . For example , the string “ . . . to be checked by the bank computers . . . . ” contains a source link anchor to the glossary entry “Bank Computer” . In contrast to the slightly more general case of dictionary links , we do not aim at linking all except stop words to some dictionary . Both glossary and dictionary links are the most obvious examples of implicit links [ 111 . In this view , such links are ( implicitly ) available within pure natural - language text . For making them explicit they must be recognized within the text . Since the source of a glossary link is a name of an entry in a glossary , this means that these names must bc recognized in the text . With increasing size of the glossary , this task becomes increasingly difficult for humans . If there always were a unique text string for such a name , the problem would just be ( exact ) matching of text as in usual text processors . However , finding the name of a glossary entry in natural - language text’ by machine involves complexities such as the issues l of names that are noun clusters possibly consisting of sev - eral words , l of noun clusters that overlap or encompass each other in the text , e of different word forms due to morphology , and ‘WC focus on English and German , and our approach may also work for other languages . Rut certain others such as prefix - agglutinative languages ( e . g . . Hehrcw or Arabic ) arc outside or our scope . 3 l of synonyms , acronyms and abbreviations . While generating links has attracted a lot of attention for other link types , generating glossary links was discussed but not seriously addressed . So , no solution is available in practice yet . We present a new algorithm that deals with all the above is - sues to some extent . Since it implements semiautomatic link generation , this algorithm relies on user cooperation and in - teraction . Therefore its design aims at reducing the cognitive overhead of the user . We faced the problem of generating glossary links in the context of requirements engineering . Our system RETH ( Requirements Engineering Through IIypertext ) [ 19 , 201 is a hypermedia application for specifying requirements . Its basic hypertext approach was originally inspired by our pre - vious work as described in [ 22 ] . Among other objectives , hypertext serves as a semiformal representation of the re - quirements in RETH . Explicit links from their statements into a glossary of technical concepts of the domain and links between these glossary entries make the requirements more understandable . Especially in this application , it is impor - tant to show that there is indeed a definition in a glossary available . Otherwise , readers either may be frustrated when not finding glossary entries in spite of several attempts , or they confuse defined terms with common - sense notions . In several real - world projects - e . g . , for ESA ( European Space Agency ) and within Siemens - our approach to semiauto - matic glossary link generation showed its usefulness , and only few glossary links had to be installed manually . The RETH tool has a feature for automatic export into a WWW representation . In order to make this export straight - forward , our approach avoids to generate overlapping or en - compassing source link anchors . Apart from the usefulness of this feature for browsing the requirements from distant places concurrently , it allows viewing RETH as an “authoring tool” for the Web with a link generation facility . Although the project experience using RETH gave us invalu - able qualitative insights , we needed an approach to test our automatic link generation mechanisms also quantitatively . Therefore , we developed a novel experiment design for quan - titatively measuring the success of semiautomatic link gen - eration in terms of recall / precision scores . Its application for our new algorithm shows that our approach to semiautomatic link generation is very useful in the context of requirements engineering through hypertext . We conjec - ture its more general applicability to linking any dictionary of technical concepts , based on the very good result of gen - erating glossary links in a dictionary of artificial intelligence concepts . In summary , we developed a practical approach to semiauto - matic generation of glossary links in hypertext that performs inexact matching of text . It deals successfully with names that may consist of several words , which may overlap or encompass each other in the text . This paper is organized in the following manner . First , we give an overview of previous work on automatic link gen - eration in general . Then we describe our approach to semi - automatic link generation for glossary look - up . In addition , we discuss the issue of quantitatively testing the success of automatic link generation in general , and sketch our specific experiment design . Finally , we present experimental results in terms of recall / precision scores . PREVIOUS WORK Most of the previous work on automatic link generation in hypertext dealt with similarities computed from term fre - quencies for finding associative links . Similarities can be estimated using a simple formula as in a prototype for au - tomatically linking similar network news articles [ 2 ] , or cal - culating the presence or absence of terms in a bit signature [ 51 . While such approaches opt for fast processing , more sophis - ticated estimates of similarities can be computed using the vector - space model of Salton [ 29 ] that represents the “con - tent” of a node as a term vector . Gloor [ 151 clusters nodes into what he calls hyperdrawers according to Salton’s simi - larity measure , and connects the most related hyperdrawers according to the same measure . The SmarText system [ 28 ] also uses such a similarity measure , but allows for more flexibility through parameter adjustments of the similarity computation , and through manipulation of the keyword and the stop - word lists . Chang’s approach [ 7 ] generates links similar to those generated manually by the user . Salton and Allan [ 30 ] improve on the results of only global text com - parison through using global / local context restrictions and vector matching processes . The link generation process de - veloped by Smeaton and Morrissey [ 31 ] uses values from a dynamically computed metric which measures the topolog - ical compactness of the overall hypertext being generated . Allan [ I ] uses information retrieval techniques for automati - cally generating link types . Finding associative links was also addressed by knowledge - based approaches : CONSTRUE [ 181 performs shallow se - mantic analysis , and VISAR [ 9 ] uses a very large common - sense knowledge base ( Cyc ) for semantic analysis of text . These are very ambitious approaches , but their practicabil - ity is yet unclear . Cleary and Bareiss [ 8 ] present a practical approach for automatically generating typed links . Other approaches address different link generation problems , such as to find structure in spatial arrangements of objects . This is addressed in a system named VIKI [ 24 , 25 ] . Coombs [ lo ] uses full - text search as a foundation for automatic linking in the Intermedia system and deals with automatic resolution of coded cross references . DeYoung [ 12 , 131 indicates gen - erating a variety of links , e . g . , isomorphic links between one section in different versions of the same document , but she does not mention the problems involved in finding implicit links in and into a dictionary or glossary . Thistlewaite [ 33 ] classifies links into several types and notes that for refer - ential links the identifier for the appropriate target can be computed as a function of the string that matches the source anchor pattern . While this is necessary for inexact match - ing of dictionary links , he does not provide such a function . Computing a function for the slightly more specific problem 4 of generating glossary links is one of the major achievements of the algorithm that we present below . Only few authors mention the problem of automatically gen - erating dictionary links . A transformation of the Oxford English Dictionary into hypertext raised many related prob - lems involved in automatic link generation in such a dic - tionary , but Raymond and Tompa [ 27 ] do not provide a so - lution to the problem of automatically generating dictionary links . Rcarick l28l discusses computer - assisted linkingbascd on lexical analysis - wt ~ rtl st ~ nznzing tither heuristically or recording the pcrmissiblc suftixcs with each word - but the SmarText system does not include either of these possibilities . DeRose [ 11 J discusses a taxonomy of links , and mentions the complexities involved in automatically finding implicit links for dictionary look - up , such as the issue of different word forms according to morphology . Unfortunately , no solution is provided or indicated . Also the slightly more specific problem of generating glos - sary links was primarily discussed but appears yet unsolved . While Parunak [ 26 ] states that glossary support should be feasible in practical systems , he does not say how this could be achieved . Glushko [ 161 states that links to a glossary pose little risk of disorientation or cognitive overload and con - siders than as nearly as important as explicit intra - document links . However , hc has been disappointed so far about the ap - proachcs lo aulomatic extraction from natural - language text . Furuta et al . [ 141 actually cxperimcnted with automatic gcn - eration of links to glossaries , and some of their findings are included in our algorithm as cited below . However , their approach did not include inexact text matching and they only shortly discuss user interaction to eliminate errors . Automatic generation of glossary and dictionary links is also provided by Microcosm [ 171 . While its current implementa - tion only uses exact matches of text strings , the model would allow more general treatment . ’ In fact , the algorithm that we present below may be usefully applied in the context of systems like Microcosm . It may also be useful for a better recognition of textual source link anchors of generic links in Microcosm , although its authors do not view the use of generic links as automatic link gcncration . In summary , while there exist useful approaches to certain as - pccts of automatic link gcncration , the problem of automati - cally generating glossury links is recognized but yet unsolved by the approaches that we are aware of . GENERATING GLOSSARY LINKS Now let us describe our approach to generating glossary links . These are more precisely links that have as source link an - chors noun clusters and as destination link anchors glossary entries , represented either as whole nodes or as parts of nodes . We assume that every glossary entry describes an object or concept each , and that at least the name of the entry is given . Since in the current Web any two source link anchors must be disjoint in the sense that they do not overlap and that no such anchor encompasses another one , our approach gener - atcs only links with disjoint source link anchors . For discovering potential glossary links , we use the name of the object or concept described in a glossary entry and possibly other user - defined strings denoting , e . g . , synonyms as one or more patterns to be matched in a given text string . In general , the match need not be exact in the sense of having identical strings . Such names are noun clusters that may consist of several words . Since the user of this algorithm should have full control over which links to include WC rely on user cooperation and in - teraction . Whcncver a potential link is found by the system , it is proposed to the user , and only if accepted it is actually generated . When a potential link is not proposed since the system does not yet know , e . g . , a synonym , the user can in - form the system about the corresponding string . The system uses this information then in the same way as it uses the name of a glossary entry . Below we present a pseudocode of our interactive algorithm for generating glossary links according to this approach . This pseudocode should be more or less self - explanatory on what this algorithm does . For really understanding it , however , we think that it is important to figure out why it works in this way . So , the following description primarily focuses on these aspects and design decisions , but it links explicitly to defined labels in the pseudocode . This central section about our approach to gcncrating glossary links is organized in the following manner . First , WC describe our semiautomatic approach when links are lo be proposed lo the user . Then we show how to break noun clusters consisting of several words into single words . After that , we deal with the issues of matching single words “inexactly” . Finally , we describe how additional link information can be provided by the user , e . g . , about synonyms . Links proposed to the user After the user has edited text in a hypertext node , either by typing or copy / pasting , the system tries to find glossary links in this text as described below . Whenever it finds a candidate link , the system instantaneously proposes it to the user , who simply accepts or rejects it . For the purpose of illustration , let us consider an cxamplc scenario of semiautomatic link gcncration according lo our approach . After the user has finished editing the text of the glossary entry of node Card Authorization , the system instan - taneously proposes a link from it to node Bank Computer . Since the user wants to ascertain the link destination , she dis - plays it on the screen . Fig . 1 presents a screen dump of our user interface for this situation . We refer to this screen dump in more detail below . In this example scenario we assume that the user accepts the proposed link by pressing the “Accept” button . ( Rejecting would be possible through pressing the “Skip” button in order to skip this link suggestion . ) As a result , the link from the string “bank computers” in Card Authorization to Bank Com - puter is installed and can therefore be followed immediately . For such a dialogue , it is important to reduce the cognitive overhead ofthc user . Thcrcforc , our algorithm tries lo prcscnt them in a “reasonable” order and to keep the frequency 01 5 The comuuter owned by am that Interfaces wrth the e ATM network and the & & ‘r own cashrer stations . A ! JK & * may actually have rtr own internal network of comuuters to ‘ , Computer & I _ . - p & err accountr . but we are only concerned with the one that talks to the network . Figure 1 : A screen dump illustrating our user interface for semiautomatic link generation . asking the user small . A pseudocode for our top - level al - gorithm that implements this strategy is given by procedure Semi - automaticlink - generation in Fig . 2 . Since we assumed that source link anchors are disjoint , the major problem to be addressed by this ( top - level ) procedure is that for a given list of matches , there may be overlapping and / or encompassing matches . Matching strings in the text may overlap , primarily when they consist of several words each , e . g . , “bank computer” and “computer network” . In addition , one string may encompass another one , e . g . , “bank computer” encompasses “computer” . In such cases , the users decide ( they are to check the links , anyway ) . However , our approach tries to avoid that users must check every single match in order to resolve such con - flicts . So , our algorithm excludes those matches that are encompassed or overlapped by already accepted source link anchors . Since the scenario of semiautomatic link generation proceeds sequentially ( see ( v ) in Fig . 2 ) , the first acceptance of a link proposal makes those links with overlapping or en - compassed source link anchors automatically irrelevant in the sense that they are not proposed ( see ( vi ) ) . Encompassing strings mostly denote the more specific con - cept than encompassed strings , and links to the more specific concepts were found to be the more useful ones ( cf . also [ 141 ) . So , our algorithm proposes the encompassing string as a source link anchor first , e . g . , “bank computer” ( see ( iv ) ) . Only when this one is rejected , it proposes encompassed ones , e . g . , “computer” . This example is illustrated in Fig . 1 , where both Bank Computer and Computer ( and actually also Bank ) are glossary entries ( in the sense that the objects in this approach contain a glossary entry each ) . The rationale for this approach is to reduce the cognitive overhead of the users by keeping the frequency of asking them small . This frequency is kept small , since the encompassed one is more unlikely to be accepted and not even asked about when theencompassing one is already accepted . The question is whether not presenting all potential source link anchors may lead to the creation of wrong links , since the right one is potentially not even proposed . While this may be a serious problem for other link types , glossary links can be determined objectively . ( In [ 33 ] , even the subsuming class of referential links is said to be unambiguous . ) That is , when a link is proposed , it is possible to decide whether it is the right one or not . For making these decisions , the users have sufficient information as they can see both the context of the string in question and the text describing the object that the proposed link is supposed to refer to . See Fig . 1 again , where both the glossary text of Card Authorization is shown ( where the string “bank computers” occurs and is highlighted ) and the glossary text describing Bank Computer . Our approach also tries to present the proposed links from left to right , which is reasonable for English and German ( our scope ) . ( More precisely , this order relates to the positions of the source link anchors in the text string . ) For this reason , the algorithm attempts to sort the matches in ascending order . For overlapping matches , however , an exception from this order is required . For instance , when given a text string “ . . . bank computer network . . . ” and the patterns “bank computer” and “computer network” , overlapping matches occur . From these , the link corresponding to the latter pattern 6 procedure Semiautomaric _ linkqeneration ( T , P ) Input : T : the text consisting of n words UJ ~ , . . . UJ ~ . P : a set of patterns p , each consisting of one or several words Zi , l , . . . , rt , m . Find list of all matches in the text : Breakinto _ single _ urords ( T , P ; M ) ( i ) Determine “dusters” of overlapping or encompassing matches : Determine _ clusters _ of _ matches ( M ; C ) / * Clusters are defined by the transitive closure of overlapping or encompassing * / / * matches . Within these clusters , the conflicts between overlapping and * / / * encompassing matches have to be resolved by the user . C is a list consisting * I / * of lists of matches * I ( ii ) Sort list of clusters : SC + sort the list of clusters C . The clusters are sorted according to the appearance of their matches in the text in ascending order ( from left to right ) . foreach cluster M in SC in the given sequence & J ( iii ) Sort list of all matches : SMt sort the list of all matches M . Formally , this list consists of pairs ( i , j ) , where i is the starting position of the matching part of the text and j is the index of the pattern p , . These pairs are sorted according to i + Length ( p3 ) - 1 ( the end position of the appearance in the text ) in descending order ( from right to left ) . ( iv ) Additionally , pairs with the same values for i + Length ( pJ ) - 1 are sorted according to the length of the pattern p , ( the length of the match in the text ) in descending order ( the longest matches first ) . Select source link anchors from matches : [ v ) foreach element ( ; , j ) in SM in the given sequence & : vi ) if ( ; , j ) is encompassed or overlapped by some match that was previously accepted as source link anchor then discard ( i , j ) & propose the potential source link anchor corresponding to ( ; , j ) to the user if the user accepts then create a hypertext link to a node identified by j & discard ( ; , j ) : nd arocedure Figure 2 : Pseudocode of the procedure for proposing links to the user . is the one more likely to be chosen , since the text refers to a network rather than to a computer ( see ( iii ) ) . Therefore , we defined clusters as the transitive closure of overlapping and encompassing matches ( see ( i ) ) . Our algorithm actually sorts these clusters in ascending order , i . e . , from left to right ( see ( ii ) ) , rather than directly the matches for potential source link anchors . Fortunately , overlapping matches do not occur frequently , and as long as they do not occur , our algorithm keeps the order of presentation of all the matches from left to right . Another special case is given by self - referencing links ( cf . also [ 141 ) : a text string within a node that contains the defi - nition of an object may be matched by a pattern that would suggest it as a source link anchor of a link to the node of this object itself . We did not find such links useful and therefore inhibit proposing such links . This aspect is not shown in the pseudocode , since we assume that the corresponding patterns are not included in the input P to our top - level procedure . While we have developed this approach to proposing links to the user in the context of glossary links , our approach so far is not restricted to glossary links only . It works in general for semiautomatic link generation where the source link anchors are not allowed to overlap or to be encompassed by other source link anchors . For instance , this is the case in the current Web approach based on HTML ( hypertext mark - up language ) . Breaking into single words Our approach deals with source link anchors that are noun clusters , and these may consist of several words in natural lan - guage . Using natural language , there is the issue of finding “inexact” matches due to morphology . Since morphology re - lates to single words , we must break noun clusters consisting of several words into single words . A pseudocode that im - plements this is given by procedure Break - into - single - words 7 procedure BreakintosinRle _ word . ~ ( T , P ; M ) Input : T : the text consisting of n words w ~ , . . . uJ ~ . P : a set of patterns p , each consisting of one or several words Z , , I , . . . , Zi , m . output : M : List of all matches , i . e . , pairs ( ; , j ) where i is the starting position of the matching part of the text and j is the index of the pattern p , . Preprocess the text : ( vii ) T’ + Preprocess _ tezt ( T ) / * Remove all parentheses , M - 04 and ' _ " . * / Determine all matches in the preprocessed text : for i from 1 @ Length ( T’ ) & ( viii ) foreach pattern pJ consisting of z ~ , ~ , . . . , z ~ , ~ & check all possible matches of single and composite words in the text and in the pattern using backtracking . The backtracker employs Inexact _ match _ of _ single _ woTds ( x , w ; inexact _ qmtch ) where z may be zj , kr Concat ( z ~ , k , Zl , k + l ) rConcat ( COncat ( Zj , k , ~ l , k + l ) , ~ 3 , k + 2 ) ) r . . . and w may be wl , Concat ( wl , wl + l ) , Concat ( Concat ( wl , zul + , ) , w1 + * ) , . . . , where I 2 i . if inexact _ match is true for all components of a pattern p , then add ( ; , j ) to the list M of matches in the original text end procedure Figure 3 : Pseudocode of the procedure for breaking into single words . in Fig . 3 . In usual cases like the one illustrated in Fig . 1 , it is actually easy to break a noun cluster into single words , e . g . , “bank computers” is simply broken into “bank” and “computers” . But at this point , we must be more precise about what con - stitutes a word . In our approach , we mean a maximal string delineated by blanks , punctuation characters except hyphens , underscores and parentheses . In English and German , how - ever , words are sometimes separated , concatenated or con - nected with hyphens . In technical texts , underscores are also sometimes used instead of hyphens . Because hyphens ( and underscores ) mostly do not have a real meaning for distinguishing between text strings in natural language , our algorithm preprocesses both the text and the patterns in order to avoid that matches are not found because these characters are not uniformly used in practice ( see ( vii ) in Fig . 3 ) . For instance , “pseudocode” may also occur in the form “pseudo - code” . However , this kind of preprocess - ing should be as simple as possible since it should not incur that wrong matches are found . Unusual pattern strings mean something unusual , so they should not be ignored for match - ing . For instance , any special character except parentheses , hyphens and underscores may carry a special meaning , e . g . , an ampersand . As another example , pronouns like “their 2” have a meaning in the text that should not be ignored during matching . The actual task of breaking noun groups into single words is in general not as trivial as it may seem , because words may be concatenated to composite words or not . For instance , “pseudocode” may also occur in the form “pseudo code” . Since the way words are written may differ in the pattern and in the text , pairwise comparison of words is insufficient . So , our algorithm for determining all matches performs a systematic check of possible matches using a backtracking approach ( see ( viii ) ) . Unfortunately , such an approach may also lead to wrong matches . Such rare cases , however , can be easily corrected by the user . The backtracking procedure finally makes use of the procedure for inexact matching of single words . Inexact match of single words Our procedure for matching single words takes a single word in the text and in the pattern each and checks whether they match ( exactly or in a derived form ) from the point of view of natural language morphology . While the focus is on the much more difficult case of “inexact” matching , the simple task of exact matching is treated as a special case here . The standard approaches to inexact pattern matching ( see , e . g . , [ 3 , 4 ] ) are insufficient because they do not take specifics of natural language into account , such as the position of vari - ations in the pattern . Most heuristic approaches to word stemming take such positions into account , as they incremen - tally remove hypothesized suffixes from words and thereby attempt to reduce words to their stems . However , such ap - proaches are too inexact with respect to natural - language morphology and leave several problems unsolved [ 29 , 301 . The issue of different word forms in natural language could in principle be tackled by a system for natural - languageprocess - ing , more specifically by one dealing with morphology [ 32 ] . ’ However , we found that simple issues of morphology can be 30ne problem for such systems may be that general natural - languagedic - tionaries do not contain all the relevant words of a glossary in some highly specialized domain . Consequently , for each new project linguistic infor - mation would have to be acquired , which is both annoying and potentially impossible . more or less solved with reasonable effort , without solutions that are expensive to build or expensive to buy . Also , a full - fledged morphological component is apparently not required for the relatively simple technical domains we are dealing with . Our procedure for inexactly matching single words is based on linguistically motivated word stemming approaches such as [ 23 ] . These approaches incorporate basic morphological knowledge for the languages they are trying to support ( in our case English and German ) . One particular constraint that could have made things complicated is that we have to deal with morphology without the possibility to acquire linguistic knowledge from the user . However , given the glossary and the assumption that glossary entries are noun clusters , a fairly straightforward solution is sufficient for our purposes . The algorithm checks whether the exact match at the begin - ning of the words is sufficiently high ( its proportion is greater than a parameter 0 ) . This is slightly different from the ap - proach in [ 23 ] , since we require a minimum percentage of a match instead of a minimum length . The restriction of a minimum match is intended to make the procedure stricter than heuristic word stemming as described in [ 29 ] . When the algorithm finds the remainders in a ( language - dependent ) table , it returns a match . We employ suffix tables that contain inflectional suffixes for adjectives and nouns . Table 1 shows an excerpt of the tables that correspond to those in [ 23 ] . These tables contain the essential morphological knowledge used by our algorithm . The entries are in capitals in order to make the approach case - insensitive . For instance , “Comput - ers” matches “computer” since “S” is in this table for English , and “C” is not distinguished from “c” ( see also Fig . 1 ) . As an example for a case where the proportion of the exact match is too small , “fuses” does not match “fuss” in our approach , although “S” and “ES” ( after “E” ) can be found in the table for English . This aspect is more important in German than in English since the suffix table for German contains longer strings . Also independently of this proportion , the language - dependent tables help to avoid some erroneous matches . For instance , “chronic” does not match “chronicle” , since “LE” is not in this table for English . The approach based on suffix tables includes exact matches , since the tables contain the empty string E as well . In our approach , matches must start at the beginning of words in order to avoid certain incorrect matches . For instance , “inflame” does ( correctly ) not match “flame” . We think that this is particularly important in the case of matching source link anchors of glossary links , that consist of noun clusters . As stated above , our algorithm normally does not distinguish capitals from non - capitalized characters . In case of acro - nyms , however , this would lead to systematic errors . For instance , the pattern “IN” ( for intelligent networks ) matches with “in” . Therefore , in the case of acronyms the matching algorithm treats the word in the pattern and the word in the text differently . When a pattern consists solely of capitals , our algorithm treats it as an acronym and requires a stricter form of match . While in general there is a symmetry be - tween the text and the patterns , this treatment of acronyms is an exception . 1 English German E E “S” “EM , “NM , “S” “ES” if after “S” , “X” or “H” “EM” , “EN” , “ER” , “ ~ S” “ERN” Table 1 : An excerpt of our suffix tables . We consider to make further improvements along the fol - lowing lines [ 23 ] : we plan to keep three different tables of inflectional suffixes for adjectives , nouns and for verbs ( for each language ) . These tables can be used to hypothesize the category of the word in question . Considering the category of the words to be matched would make the matching more pre - cise . Additionally , heuristics for recognizing word categories ( such as “nouns mostly begin with capital letters in German” ) could be employed to improve its precision . Finally , we plan to keep lists of exceptions for adjectives , nouns and verbs that do not adhere to the rules underlying the inflectional suffixes in the tables . So , what is the role of natural - language processing in our ap - proach , and in particular in matching single words inexactly ? Our approach deals with morphology only rudimentarily . In addition , our algorithm cannot deal with phrase variants ( e . g . , “network of ATMs” does not match “ATM network” ) or se - mantic disambiguation ( e . g . , what kind of bank is meant with the string “bank” ) . While dealing with phrase variants in general is not at all an easy problem , relatively simple rules that make use of regular expressions could cover a portion of the most frequent cases . Semantic disambiguation would require more subtle means of natural - language processing . Homonyms , however , should be avoided in technical texts , anyway , and we found that our overall approach even helps to uncover them ( see below ) . In general , more elaborate ap - proaches are much more expensive to build or even require special effort for each new domain . In addition , issues of natural language and in particular matching single words are just part of our overall approach . Link information provided by the user While the approach to semiautomatic link generation de - scribed so far works already quite well , it was clear that it would be insufficient in practice because the system does not have information about synonyms , acronyms or abbrevia - tions without being told about such strings that are equivalent to the string of the name . If a thesaurus is available it could be a source of such knowledge , but dealing with these issues fully automatically appears to be still out of reach . Since we also cannot assume the availability of a thesaurus for each domain and since the user knows synonyms , acronyms or abbreviations for the given domain , our approach deals with them semiautomatically in the sense that the user can provide them . Still , the user needs to know what the tool does not know . So , users are instructed that originally the tool can only use the name of the glossary entry as a basis for semiautomatic link generation , and that any information on synonyms , acronyms and abbreviations needs to be provided . In addition , the user 9 can see the result ( the installed links in the text ) immediately after link generation . Whenever a glossary link is missing in the text that should be there , this is an indication that such information needs to be provided . In addition to an extra interface for entering synonyms , acro - nyms and abbreviations , we solved this issue more generally by reusing the interface for manual linkgeneration . When the user installs a link manually , our system acts like an appren - tice and memorizes the link information for use in subsequent link proposals by the system . It assumes that the text string forming the source link anchor denotes a synonym , acronym or an abbreviation . For these , the system applies the same method of inexact matching of text . So , the user just has to tell the system once about a specific synonym , acronym or an abbreviation , and the system deals with it as if this string were the string of the name . In effect , this feature is reminiscent of the use of generic links in Microcosm . Our approach does not cover homonyms , but this seems to be a difficult problem , anyway [ 6 ] . In practice , however , we found that our semiautomatic approach can even be useful for discovering homonyms . Whenever the system proposes a link to the user that is to be rejected , this may indicate a homonym , More generally , it may even indicate an ambigu - ous statement . At least in technical texts ( that we are trying to support ) , such an issue should be resolved , anyway . The point is that our approach to link generation helps to uncover such issues as shown in [ 21 ] . QUANTITATIVELY MEASURING THE SUCCESS OF SEMI - AUTOMATIC LINK GENERATION Although our project experience gave us invaluable quali - tative insights , we needed an approach to test our automatic link generation mechanisms also quantitatively . Bernstein [ 5 ] was the first to mention measuring the performance of a link generation approach via recall ( in this context the proportion of relevant links generated ) and precision ( in this context the proportion of generated links that are relevant ) . In more re - cent work [ 8 , 301 such approaches were developed and used for evaluating fully - automatic link generation . We developed the following experiment design for evaluat - ing semiautomatic link generation . Consider the availability of some idealized user who knows which links are “rele - vant” . When such a user would employ a semiautomatic link - generation algorithm , we could determine how often she would reject links . When the rejection rate is low , this means a high precision score . We could also determine how often such a user would have to create additional links “manually” since they were not proposed by the algorithm . When their proportion of the total number of relevant links is small , this means a high recall score . While such a user would not really need a link - generation facility due to her knowledge , it could still be useful from a practical perspective in saving much effort compared to creating all relevant links manually . This approach focuses on an evaluation of the link genera - tion algorithm rather than of the algorithm plus its concrete user interface . In addition , it avoids the influence on the re - sults from users rejecting relevant and manually generating irrelevant links . 1 03 0 . 8 0 . 7 0 . 6 0 , 5 0 , 4 0 . 3 02 O , l 0 CORTEX ELEFANT PASTEL ATM AI - DICT Figure 4 : Recall / precision scores for several real - world hyperdocuments . The key point for applying this approach successfully is the availability of at least a good approximation of an idealized user . Fortunately , glossary links can be determined objec - tively when sufficient domain knowledge is available . In our experiments , we approximated an idealized user together with domain experts and using significant time and effort . If results are needed for very large documents , we propose to use sampling techniques from statistical quality control . EXPERIMENTAL RESULTS Now let us sketch the results of applying this experiment de - sign to our algorithm for generating glossary links . We used several hyperdocuments from various real - world projects on requirements engineering for this experiment . Most of them are written in English , but one is in German ( ELEFANT ) . In order to examine the generality of our approach , we also used a ( German ) dictionary of concepts in artificial intelli - gence ( AI - DICT ) . It was prepared by the first author several years ago as a linear document , with the cross - reference links printed in bold face . Note , that it was prepared then without any intention to put it into a hypertext system . Fig . 4 shows recall / precision scores for these documents . More precisely , we calculated these scores by counting all link instances in the sense of concrete pairs of one source link anchor ( text string ) and one target link anchor ( glossary entry ) , respectively . In general , both the recall and the preci - sion scores are quite high . Especially the precision scores are very high and for most documents even perfect . This means that only few of the proposed links would have to be rejected by the idealized user . Primarily these are due to homonyms , but as discussed above this is even useful to discover them . Compared to the precision scores , the recall scores are lower , however , and for ELEFANT this score is comparably low . It may seem that the matching algorithm returns too few matches to the overall interactive algorithm . In fact , how - ever , the reason for the lower recall scores is that information about synonyms , acronyms and abbreviations was unavail - able to the algorithm in our experiment . For this reason , in some documents - like in ELEFANT - fewer links are discovered than in others . For the ATM example , e . g . , mem - orizing a single manual link would result in a perfect recall score of 1 . So , this shows the importance of our feature of memorizing information of specific instances provided man - 10 ually , more generally of information about synonyms , acro - nyms and abbreviations provided by the user . Depending on the concrete document , however , even without such link in - formation provided additionally , a useful set of glossary links can be generated by our approach . Comparing these results with those of other approaches and systems is difficult , since most of them generate different kinds of links . The few real approaches that can be used for generating glossary links only perform exact matching of text , and they seem to have no mechanism for handling overlapping or encompassing source link anchors . Assuming that the latter do not arise , however , it is possible to make a comparison by the following argument . The recall scores of our more advanced algorithm can only be better , since link anchors missed by inexact matching are also missed by exact matching . However , there may be source link anchors that are missed by exact matching but not by our approach to inexact matching . That is , our algorithm matches a superset of source link anchors . For the same reason , the precision scores might be worse . When looking again at these scores in our experiment ( Fig . 4 ) this appears to be possible only occasionally . In summary , our approach to semiautomatic link generation is very useful in the context of requirements engineering through hypertext , which is supported by these data . We conjecture its more general applicability to linking any dic - tionary of technical concepts , which is supported by the re - sults on AI - DICT . Since we have very positive experience with English and German documents and also corresponding data from our experiment , it seems that our approach supports generation of glossary links well in both languages . CONCLUSION We designed and implemented a practical approach to semi - automatic generation of glossary links in hypertext , i . e . , links within and into technical glossaries . Our main contributions are l a new and interactive algorithm for generating glossary links that deals with inexact matching of text and with names possibly consisting of several words , which may overlap or encompass each other in the text ; l a novel experiment design for quantitatively measuring the success of semiautomatic link generation . Links generated with our tool can easily be exported into a representation for the current Web . Our approach to dealing with overlapping and encompassing source link anchors is even more general than “just” for glossary links . So , it may be important for other kinds of links in the Web as well . Empirical evidence from both the use of our tool in industrial practice and from an experiment according to our experiment design indicates the usefulness of our approach . We conjec - ture the general applicability to linking in any dictionary of technical concepts , which is supported by experimental data . In summary , we developed a comprehensive and practical solution to semiautomatic generation of glossary links in hy - pertext . Both our qualitative empirical evidence from appli - cations in real - world projects and quantitative experimental results suggest its usefulness . Acknowledgments Helmut Horacek and Alexandra Klein provided us with very useful comments on previous versions of this paper and made several technical suggestions . REFERENCES 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . J . Allan . Automatic hypertext link typing . In Pro - ceedings of the Seventh ACM Conference on Hypertext ( Hypertext’96 ) , Washington , DC , March 1996 . ACM . M . H . Andersen , J . Nielsen , and H . Rasmussen . A similarity - based hypertext browser for reading the UNIX network news . Hypermedia , 1 ( 3 ) : 255 - 265 , 1989 . M . J . Atallah , I ? Jacquet , and W . Szpankowski . Pat - tern matching with mismatches : Probabilistic analysis and a randomized algorithm . In Combinatorial Pattern Matching : Proceedings of the Third Annual Sympo - sium , pages 27 - 40 , Tucson , Arizona , April / May 1992 . R . A . Baeza - Yates and Ch . H . Perleberg . Fast and practi - cal approximate string matching . In Combinatorial Pat - tern Matching : Proceedings of the Third Annual Sym - posium , pages 182 - l 89 , Tucson , Arizona , April / May 1992 . M . Bernstein . An apprentice that discovers hypertext links . In Proceedings of the First European Confer - ence on Hypertext ( ECHT - 90 ) - Hypertext : Concepts , Systems , and Applications , pages 2 12 - 223 , Paris , 1990 . L . A . Carr , W . Hall , and S . Hitchcock . Link services or link agents ? In Proceedings of the Ninth ACM Con - ference on Hypertext and Hypermedia ( Hypertext’98 ) , pages 113 - 122 , Pittsburgh , PA , June 1998 . D . Chang . HieNet : a user - centered approach for au - tomatic link generation . In Proceedings of the Fifth ACM Conference on Hypertext ( Hypertext ‘93 ) , pages 145 - 158 , Seattle , WA , November 1993 . ACM . C . Cleary and R . Bareiss . Practical methods for auto - matically generated typed links . In Proceedings of the Seventh ACM Conference on Hypertext ( Hypertext’96 ) , Washington , DC , March 1996 . ACM . P . Clitherow , D . Riecken , and M . Muller . VISAR : a system for inference and navigation in hypertext . In Proceedings of the Second ACM Conference on Hy - pertext ( Hypertext’89 ) , pages 293 - 304 , Pittsburgh , PA , November 1989 . J . H . Coombs . Hypertext , full text , and automatic link - ing . In Proceedings of the International Conference on Research and Development in Information Retrieval ( SIGIR ‘90 ) , Brussels , Belgium , September 1990 . S . J . DeRose . Expanding the notion of links . In Proceed - ings of the Second ACM Conference on Hypertext ( Hy - pertext’89 ) , pages 249 - 257 , Pittsburgh , PA , November 1989 . ACM . 11 12 . L . DeYoung . Hypertext challenges in the auditing do - main . In Proceedings of the Second ACM Conference on Hypertext ( Hypertext’89 ) , pages 169 - 180 , Pittsburgh , PA , November 1989 . ACM . 13 . L . DeYoung . Linking considered harmful . In Proceed - ings of the First European Conference on Hypertext ( ECHT - 90 ) - Hypertext : Concepts , Systems , andAppli - cations , pages 238 - 249 , Paris , 1990 . 14 . R . Furuta , C . Plaisant , and B . Shneiderman . A spec - trum of automatic hypertext constructions . Hyperme - dia , l ( 2 ) : 179 - 195 , 1989 . 15 . I ? A . Gloor . CYBERMAP : Yet another way of navi - gating in hyperspace . In Proceedings of the Third ACM Conference on Hypertext ( Hypertext ‘91 ) , pages 107 - 121 , San Antonio , TX , December 1991 . 16 . R . J . Glushko . Design issues for multi - document hyper - texts . In Proceedings of the Second ACM Conference on Hypertext ( Hypertext’89 ) , pages 5 I - 60 , Pittsburgh , PA , November 1989 . 17 . W . Hall , H . Davis , and G . Hutchings . Rethinking Hyper - media : The Microcosm Approach . Kluwer Academic Publishers , 1996 . 18 . P . Hayes and J . Pepper . Towards an integrated main - tenance advisor . In Proceedings of the Second ACM Conference on Hypertext ( Hypertext’89 ) , pages 119 - 127 , Pittsburgh , PA , November 1989 . 19 . H . Kaindl . An approach to hypertext - based require - ments specification and its application . In Human - Computer Interaction : Interact ‘95 , pages 354 - 357 , Lillehammer , Norway , June 1995 . IFIP , Chapman & Hall , London , Great Britain . 20 . H . Kaindl . Using hypertext for semiformal representa - tion in requirements engineering practice . The New Re - view of Hypermedia andMultimedia , 2 : 149 - 173 , 1996 . 21 . H . Kaindl and S . Kramer . Extending V & V of knowledge - based systems through semiformal repre - sentation . In Proceedings of Expert Systems ‘97 ( ES - 97 ) , pages 265 - 276 , Cambridge , UK , December 1997 . 22 . H . Kaindl and M . Snaprud . Hypertext and structured object representation : A unifying view . In Proceedings of the Third ACM Conference on Hypertext ( Hyper - text’91 ) , pages 345 - 358 , San Antonio , TX , December 1991 . 23 . A . Klein . Extrahierung und Nutzung morphologi - scher Muster fur die maschinelle Kategorisierung , 1994 . Magisterarbeit , Department of Linguistic Data Process - ing / Computational Linguistics , University of Trier . 24 . C . C . Marshall and F . M . Shipman . Searching for the missing link : Discovering implicit structure in spatial hypertext . In Proceedings of the Fifth ACM Conference on Hypertext ( Hypertext’93 ) , pages l - l 4 , Seattle , WA , November 1993 . ACM . 25 . C . C . Marshall , F . M . Shipman , and J . H . Coombs . VIKI : Spatial hypertext supporting emergent structure . In Pro - ceedings of the Third European Conference on Hyper - text ( ECHT - 94 ) , Edinburgh , UK , September 1994 . 26 . H . V . D . Parunak . Toward industrial strength hyper - media . In E . Berk and J Devlin , editors , Hypertext / Hypermedia Handbook , pages 381 - 395 . McGraw - Hill , New York , NY , 1989 . 27 . D . R . Raymond and F . WM . Tompa . Hypertext and the Oxford English Dictionary . Communications of the ACM , 3 1 ( 7 ) : 87 l - 879 , July 1988 . 28 . T . C . Rearick . Automating the conversion of text into hypertext . In E . Berk and J Devlin , editors , Hypertext / Hypermedia Handbook , pages 113 - l 40 . McGraw - Hill , New York , NY , 1989 . 29 . G . Salton . Automatic Text Processing : The Transfor - mation , Analysis , and Retrieval of Information by Com - puter . Addison - Wesley , Reading , MA , 1989 . 30 . G . Salton and J . Allan . Selective text utilizationand text traversal . In Proceedings of the Fifth ACM Conference on Hypertext ( Hypertext’93 ) , pages 13 l - 144 , Seattle , WA , November 1993 . ACM . 31 . A . F . Smeaton and I ? J . Morrissey . Experiments on the automatic construction of hypertexts from texts . The New Review of Hypermedia and Multimedia , 1123 - 39 , 1995 . 32 . R . Sproat . Morphology and Computation . The MIT Press , Cambridge , MA , 1992 . 33 . I ? Thistlewaite . Automatic construction and manage - ment of large open webs . Information Processing and Management , 1997 . 12