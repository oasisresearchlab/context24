Retrieval - Augmented Layout Transformer for Content - Aware Layout Generation Daichi Horita 1 Naoto Inoue 2 Kotaro Kikuchi 2 Kota Yamaguchi 2 Kiyoharu Aizawa 1 1 The University of Tokyo 2 CyberAgent { horita , aizawa } @ hal . t . u - tokyo . ac . jp { inoue naoto , kikuchi kotaro xa , yamaguchi kota } @ cyberagent . co . jp Abstract Content - aware graphic layout generation aims to auto - matically arrange visual elements along with a given content , such as an e - commerce product image . In this paper , we ar - gue that the current layout generation approaches suffer from the limited training data for the high - dimensional lay - out structure . We show that a simple retrieval augmentation can significantly improve the generation quality . Our model , which is named Retrieval - Augmented Layout Transformer ( RALF ) , retrieves nearest neighbor layout examples based on an input image and feeds these results into an autoregressive generator . Our model can apply retrieval augmentation to various controllable generation tasks and yield high - quality layouts within a unified architecture . Our extensive experi - ments show that RALF successfully generates content - aware layouts in both constrained and unconstrained settings and significantly outperforms the baselines . 1 1 . Introduction Layout is an essential part of graphic design , where the aesthetic appeal relies on the harmonious arrangement and selection of visual elements such as logos and texts . In real - world creative workflows , such as posters [ 13 , 36 ] and magazines [ 20 , 48 ] creation , designers typically work on a given subject ; for example , creating an advertising poster of a specific product . We call layout generation under such conditions content - aware layout generation , where the goal is to generate diverse yet plausible arrangements of element bounding boxes that harmonize with the given background image ( canvas ) . Recent studies [ 52 , 53 ] show that generative models can produce content - aware layouts that respect aes - thetic principles , such as avoiding overlaps [ 13 ] . However , generated layouts often still suffer from artifacts , including misaligned underlay embellishment and text elements . We hypothesize that current approaches based solely on gen - erative models do not scale due to the scarcity of highly structured layout data . Unlike public images on the Web , 1 Our project page is available at https : / / udonda . github . io / RALF / Input image Output layouts Retrieved examples Figure 1 . Retrieval - augmented content - aware layout generation . We retrieve nearest neighbor examples based on the input image and use them as a reference to augment the generation process . curating a large dataset of layered graphic designs is not a viable solution since designers typically create their work in proprietary authoring tools , such as Adobe Illustrator [ 1 ] . Inspired by the fact that designers often refer to existing designs [ 17 ] , we propose a retrieval - augmented generation method to address the challenges in the layout domain . Re - cent literature shows that retrieval augmentation helps in enhancing the generation quality of language models [ 6 , 15 ] and image synthesis [ 5 , 42 ] , thanks to the ability to reference real examples in the limited data domain . We argue that retrieval augmentation plays an important role in mitigating the data scarcity problem in content - aware layout generation . We build R etrieval - A ugmented L ayout Trans F ormer ( RALF ) , which is an autoregressive generator capable of ref - erencing external layout examples . RALF retrieves reference layouts by nearest neighbor search based on the appearance of the input and supplements the generation process ( Fig . 1 ) . Since the input canvas and retrieved layouts have different modalities , we use the cross - attention mechanism to aug - ment the feature input to the generator . Although we build RALF with an autoregressive approach , retrieval augmenta - tion is also effective in other generation approaches such as diffusion models [ 19 ] , which we show in the experiments . We evaluate our RALF on public benchmarks [ 18 , 53 ] and show that RALF outperforms state - of - the - art models in content - aware layout generation . Thanks to the retrieval capability , RALF requires less than half the training data to achieve the same performance as the baseline . We further 1 a r X i v : 2311 . 13602v1 [ c s . C V ] 22 N ov 2023 show that our modular architecture can adapt to control - lable generation tasks that impose various user - specified constraints , which is common in real - world workflow . We summarize our contributions as follows : 1 ) We find that retrieval augmentation effectively addresses the data scarcity problem in content - aware layout generation . 2 ) We propose a Retrieval - Augmented Layout Transformer ( RALF ) designed to integrate retrieval augmentation for layout gen - eration tasks . 3 ) Our extensive evaluations show that our RALF successfully generates high - quality layouts under var - ious scenarios and significantly outperforms baselines . We will make our code publicly available on acceptance . 2 . Related Work 2 . 1 . Content - agnostic Layout Generation Content - agnostic layout generation , which aims at generat - ing layouts without a specific input canvas , has been stud - ied for a long time [ 2 , 33 , 36 , 48 ] . The typical approach involves predicting the arrangement of elements , where each element has a tuple of attributes such as category , po - sition , and size [ 30 ] . Recent approaches employ various types of neural networks - based generative models , such as generative adversarial networks ( GAN ) [ 25 , 30 , 31 ] , varia - tional autoencoders ( VAE ) [ 3 , 21 , 24 ] , autoregressive mod - els [ 14 , 22 ] , non - autoregressive models [ 26 ] , and diffusion models [ 9 , 19 , 28 , 49 ] . Note that the retrieval augmentation discussed in this paper may not be directly applicable to the content - agnostic setup due to the lack of input queries . Several works consider user - specified design constraints such as “a title is above the body” , which are often seen in real - world workflow . Such constraints are studied as controllable generation [ 19 , 22 , 25 , 26 ] , where the model generates a complete layout from a partial or noisy layout . In this paper , we adapt the concept of controllable generation to the content - aware generation . 2 . 2 . Content - aware Layout Generation Content - aware layout generation , relatively less studied compared to the content - agnostic setup , has seen notable progress . ContentGAN [ 52 ] first tackles to incorporate image semantics of input canvases . Subsequently , CGL - GAN [ 53 ] introduces a saliency map to a non - autoregressive decoder [ 8 , 10 , 45 ] for better subject representation . DS - GAN [ 18 ] proposes a CNN - LSTM framework . ICVT [ 7 ] em - ploys a conditional VAE , predicting a category and bounding box autoregressively based on previously predicted elements . RADM [ 29 ] leverages a diffusion model and introduces modules to refine both visual – textual and textual – textual pre - sentations . We note that we cannot compare RADM in our experiments because their text annotations are not available . Current approaches rely solely on generative models and may struggle with capturing sparse data distributions with limited training data . We use retrieval augmentation to miti - gate this issue , and our experiments confirm its significant impact on enhancing content - aware generation . 2 . 3 . Retrieval - Augmented Generation Retrieval augmentation [ 4 – 6 , 15 , 42 ] offers an orthogonal approach to enhance generative models without increasing network parameters or relying heavily on extensive training datasets . Generative models equipped with retrieval aug - mentation stop storing all relevant knowledge in their model parameters and instead use external memory via retrieving relevant information as needed . A common approach in - volves retrieving the k - nearest neighbors ( k - NN ) based on a pre - calculated embedding space as additional input . For example , REALM [ 15 ] introduces a retrieval augmentation into language models that fetch k - NN based on preceding tokens . In image generation , RDM [ 5 ] demonstrates even a relatively compact network can achieve state - of - the - art per - formance by retrieval augmentation . KNN - Diffusion [ 42 ] shows its capacity to generate out - of - distribution images . The unique challenge in content - aware layout generation involves encoding both image and layout modalities , which we address using a cross - attention mechanism . Given that tasks related to graphic design , such as content - aware layout generation , often suffer from data scarcity prob - lems [ 38 ] , we believe that retrieval augmentation is particu - larly beneficial . It provides an efficient training method that leverages existing data more effectively . 3 . Method 3 . 1 . Preliminaries Let X and Y be the sets of canvas images and graphic lay - outs , respectively . We use I ∈ X and L ∈ Y to represent the canvas and layout , respectively . The canvas I ∈ R H × W × 3 and layout L are paired data , where H and W represent the height and width , respectively . We obtain a saliency map S ∈ R H × W × 1 by the off - the - shelf saliency detec - tion method [ 46 , 50 ] from the canvas . We denote the lay - out by L = { l 1 , . . . , l T } = { ( c 1 , b 1 ) , . . . , ( c T , b T ) } , where b ∈ [ 0 , 1 ] 4 indicates the bounding box in normalized coordi - nates , c i ∈ { 1 , . . . , C } indicates an element category of i - th element , and T indicates the number of elements in L . 3 . 2 . Retrieval - Augmented Layout Transformer We approach content - aware layout generation by referenc - ing similar examples and generating layout tokens ˆ Z au - toregressively . Following content - agnostic layout genera - tion works [ 14 , 22 ] , we quantize each value in the bound - ing box of the i - th element b i and obtain representation [ x i , y i , w i , h i ] T ∈ { 1 , . . . B } 4 , where B denotes the number of bins . Here , x , y , w , and h correspond to the tokens for center coordinates , width , and height of the bounding box . 2 (cid:10)(cid:9)(cid:7)(cid:6)(cid:4)(cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) (cid:19)(cid:5)(cid:2)(cid:1)(cid:7)(cid:14)(cid:3)(cid:13)(cid:2)(cid:7)(cid:14)(cid:11)(cid:5)(cid:2) (cid:10)(cid:9)(cid:7)(cid:6)(cid:3) (cid:27)(cid:7)(cid:24)(cid:11)(cid:3)(cid:2)(cid:1)(cid:23) (cid:31)(cid:30)(cid:29)(cid:3) ) (cid:7)(cid:23)(cid:5)(cid:30) # (cid:0)(cid:3)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) 5(cid:3)(cid:2)(cid:3)(cid:8)(cid:7)(cid:14)(cid:3)(cid:0)2(cid:24)(cid:7)(cid:23)(cid:5)(cid:30)(cid:14) 666 ? (cid:8)(cid:3)(cid:0)(cid:11)(cid:1)(cid:14)(cid:3)(cid:0)2(cid:14)(cid:5)9(cid:3)(cid:2)(cid:29) NOG2(cid:19)(cid:5)(cid:2)(cid:29)(cid:14)(cid:8)(cid:7)(cid:11)(cid:2)(cid:14)2(cid:29)(cid:3)(cid:8)(cid:11)(cid:7)(cid:24)(cid:11)D(cid:7)(cid:14)(cid:11)(cid:5)KN(cid:5)C(cid:14)(cid:11)(cid:5)(cid:2)(cid:7)(cid:24)G (cid:19)(cid:5)(cid:2)(cid:29)(cid:14)W(cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) d 2 (cid:14)(cid:5)C2(cid:5)(cid:2)2 ^ ) (cid:5)(cid:6)(cid:5)2e o(cid:3)m(cid:14)2j N(cid:7)G2z(cid:3)(cid:14)(cid:8)(cid:11)(cid:3)x(cid:7)(cid:24)2(cid:7)(cid:30)(cid:6)(cid:9)(cid:3)(cid:2)(cid:14)(cid:7)(cid:14)(cid:11)(cid:5)(cid:2) ) (cid:7)(cid:23)(cid:5)(cid:30) # (cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) z(cid:3)(cid:14)(cid:8)(cid:11)(cid:3)x(cid:3)(cid:0)2(cid:24)(cid:7)(cid:23)(cid:5)(cid:30)(cid:14)(cid:29) „(cid:30)(cid:3)(cid:8)(cid:23) ? (cid:5)(cid:29)(cid:14)(cid:3)‡ (cid:0)(cid:7)(cid:14)(cid:7)O(cid:7)(cid:29)(cid:3) Figure 2 . Overview of Retrieval - Augmented Layout Transformer ( RALF ) . RALF takes a canvas image and a saliency map as input , and then autoregressively generates a layout along with the input image . Our model uses ( a ) retrieval augmentation that incorporates useful examples to better capture the relationship between the image and the layout , and ( b ) constraint serialization , an optional module that encodes user - specified requirements , enabling the generation of layouts that adhere to specific requirements for controllable generation . We represent an overall layout as a flattened 1D sequence Z = ( bos , c 1 , x 1 , y 1 , . . . , w T , h T , eos ) ∈ N 5 T + 2 , where bos and eos are special tokens to denote the start and end of the sequence . We model the joint probability distribution of Z given I and S as a product over a series of conditional distributions using the chain rule : P θ ( Z | I , S ) = 5 T + 2 (cid:89) t = 2 P θ ( Z t | Z < t , I , S ) , ( 1 ) where θ is the parameters of our model . Similarly to au - toregressive language modeling [ 39 ] , the model is trained to maximize the log - likelihood of the next token prediction . Our proposed model consists of four modules : image encoder , retrieval augmentation module , layout decoder , and optional constraint encoder , as illustrated in Fig . 2 . We describe each module below . Image encoder . The image encoder E takes in the input canvas I and the saliency map S , and outputs the feature f I = E ( I , S ) ∈ R H ′ W ′ × d , where H ′ and W ′ represent the down - sampled height and width , and d represents the depth of the feature map . This part is common among content - aware approaches , and we follow the architecture of CGL - GAN [ 53 ] . The encoder builds on a CNN back - bone and a Transformer encoder . The CNN backbone , typ - ically ResNet50 [ 16 ] , uses a multi - scale feature pyramid network [ 32 ] . The Transformer encoder further refines the encoded image feature . Retrieval augmentation module . The augmentation mod - ule transforms the image feature f I into the augmented fea - ture f R . We describe the details in Sec . 3 . 3 . Constraint encoder . Optionally , our model allows control of the layout generation process by additional instruction on desired layout properties such as element types , coordinates , or inter - element relationships . We adopt the Transformer encoder - based model [ 22 ] to encode the instructions into a fixed - dimensional vector f const ∈ R n × d , where n denotes the length of the task - specific sequence . f const is then con - catenated with the augmented feature f R and fed to the layout decoder . Layout decoder . Our model autoregressively generates a layout ˆ Z using a Transformer decoder . Starting from the bos token , our decoder iteratively produces output tokens with cross attention to the side feature sequence f R from the retrieval augmentation module and the optional sequence f const from the constraint encoder . A key distinction be - tween our model and previous approaches is that we flatten all the attributes into a single sequence for full attention dur - ing generation , which is shown effective in content - agnostic layout generation [ 14 , 22 ] . As we discuss in Eq . ( 1 ) , we gen - erate layout tokens one by one in 5 T + 1 steps using attribute - wise attention . In contrast , GAN - based models [ 18 , 53 ] gen - erate in one step , and ICVT [ 7 ] generates in T steps using element - wise attention . 3 . 3 . Retrieval Augmentation We introduce retrieval augmentation to effectively learn the structured layout domain with limited training data . The retrieval augmentation module consists of the following three stages : 1 ) retrieving reference layouts from a database , 2 ) encoding these layouts into a feature representation , and 3 ) 3 fusing all features into the final augmented feature f R . We elaborate on the details of these three stages . Layout retrieval . Given the input canvas I , we retrieve a set of useful layout examples { ˜ L 1 , . . . , ˜ L K } , where K ∈ N . A challenge lies in the absence of joint embedding for image – layout retrieval , unlike the CLIP [ 40 ] embedding for image – text retrieval . We hypothesize that given an image – layout pair ( ˜ I , ˜ L ) , ˜ L is more likely to be useful when ˜ I is similar to I . From a large dataset of image – layout pairs , we retrieve top - K pairs based on image similarity between I and ˜ I , and extract layouts from these pairs . The choice of the image similarity measure influences the generation quality , as we will discuss in Sec . 4 . 7 in detail . We use DreamSim [ 12 ] , which better aligns with human perception of image simi - larity in diverse aspects such as object appearance , viewing angles , camera poses , and overall layout . All samples from the training split serve as the retrieval source for both training and inference , excluding the query sample from the retrieval source during training to prevent ground - truth leakage . Encoding retrieved layouts . Each retrieved layout { ˜ L 1 , . . . , ˜ L K } is encoded into representative features ˜ f L = { ˜ f 1 , . . . , ˜ f K } ∈ R K × d , since each layout has a different number of elements . A layout encoder F embeds each re - trieved layout ˜ L k into the representative feature , denoted as ˜ f k = F ( ˜ L k ) ∈ R d . These extracted features are then concatenated into ˜ f L . Following [ 25 ] , we pre - train F in a self - supervised manner and freeze F thereafter . Feature augmentation . The last step yields the final aug - mented feature f R by concatenating three features : f R = Concatenate ( f I , ˜ f L , f C ) ∈ R ( 2 H ′ W ′ + K ) × d , ( 2 ) where f C is a cross - attended feature between f I and ˜ f L : f C = CrossAttn ( f I , ˜ f L ) ∈ R H ′ W ′ × d . In the cross - attention mechanism , the image feature acts as the query , and the retrieved layout feature serves as both the key and value . This design facilitates an interaction between the input canvas and the reference layouts . We then feed the augmented feature f R into the layout generator . We will validate the design of the augmentation module in Sec . 4 . 7 . 4 . Experiments We evaluate our RALF in the unconstrained generation as well as in a variety of constrained generation tasks . 4 . 1 . Datasets We use two publicly available datasets , CGL [ 53 ] and PKU [ 18 ] , which mainly cover e - commerce posters such as cosmetics and clothing . PKU includes three element categories : logo , text , and underlay , and CGL additionally contains embellishment elements . CGL comprises 60 , 548 annotated posters , i . e . , layouts and corresponding images , and 1 , 000 unannotated canvases , i . e . , images only . PKU contains 9 , 974 annotated posters and 905 unannotated can - vases . To obtain canvas – layout pairs for the training , previ - ous works [ 18 , 53 ] employ image inpainting to remove the visual elements . However , CGL does not provide inpainted posters , and PKU provides inpainted posters with undesir - able artifacts . We inpaint the posters of both CGL and PKU using a state - of - the - art inpainting technique [ 44 ] . The original datasets do not provide validation and test splits for annotated posters . This limitation prevents fair hyper - parameter tuning , adopting evaluation metrics relying on ground - truth annotations , and the quantitative evaluation of constrained generation tasks since we cannot create con - straints from the annotations . To overcome these issues , we create new dataset splits with a train / val / test ratio of roughly 8 : 1 : 1 . For CGL , we allocate 48 , 544 / 6 , 002 / 6 , 002 annotated posters for train / val / test . For PKU , after exclud - ing posters with more than 11 elements and those with ele - ments occupying less than 0 . 1 % of the canvas , we designate 7 , 735 / 1 , 000 / 1 , 000 posters for train / val / test . Both datasets have a maximum of 10 elements . For the evaluations , we use the annotated and unannotated test splits . 4 . 2 . Evaluation Metrics Inspired by the previous works [ 18 , 53 ] , we employ five metrics that evaluate the layout quality both in terms of graphic and content aspects . Graphic metrics . These metrics evaluate the quality of the generated layouts without considering the canvas . FID ( ↓ ) for layout [ 25 , 27 ] has been a primal metric in content - agnostic layout generation , and we adopt this metric in our content - aware scenario . Underlay effectiveness ( Und ↑ ) calculates the proportion of valid underlay elements to the total underlay elements . An underlay element is regarded as valid and scores 1 if it entirely covers a non - underlay element ; otherwise , it scores 0 . Overlay ( Ove ↓ ) represents the average Intersection over Union of all element pairs , excluding underlay elements . Content metrics . These metrics evaluate whether the gener - ated layouts harmonize with the canvas . Occlusion ( Occ ↓ ) computes the average saliency value in the overlapping re - gion between the saliency map S and the layout elements . Readability score ( Rea ↓ ) evaluates the non - flatness of text elements by calculating gradients in the image space along both vertical and horizontal axes within these elements . 4 . 3 . Baseline Methods We compare the following methods in the experiments . CGL - GAN [ 53 ] is a non - autoregressive encoder – decoder model employing a Transformer architecture . The model takes in the empty or layout constraint to the decoder . 4 Method # Params PKU CGL Content Graphic Content Graphic Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Real Data - 0 . 112 0 . 0102 0 . 99 0 . 0009 1 . 58 0 . 125 0 . 0170 0 . 98 0 . 0002 0 . 79 Top - 1 Retrieval - 0 . 212 0 . 0218 0 . 99 0 . 002 1 . 43 0 . 214 0 . 0266 0 . 99 0 . 0005 0 . 93 CGL - GAN [ 53 ] 41M 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 0 . 157 0 . 0237 0 . 29 0 . 161 66 . 75 DS - GAN [ 18 ] 30M 0 . 142 0 . 0169 0 . 63 0 . 027 11 . 80 0 . 141 0 . 0229 0 . 45 0 . 057 41 . 57 ICVT [ 7 ] 50M 0 . 146 0 . 0185 0 . 49 0 . 318 39 . 13 0 . 124 0 . 0205 0 . 42 0 . 310 65 . 34 LayoutDM † [ 19 ] 43M 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 0 . 127 0 . 0192 0 . 82 0 . 020 2 . 36 Autoreg Baseline 41M 0 . 134 0 . 0164 0 . 43 0 . 019 13 . 59 0 . 125 0 . 0190 0 . 92 0 . 011 2 . 89 RALF ( Ours ) 43M 0 . 119 0 . 0128 0 . 92 0 . 008 3 . 45 0 . 125 0 . 0180 0 . 98 0 . 004 1 . 32 Table 1 . Unconstrained generation results on the PKU and CGL test split . Our RALF outperforms the Autoreg Baseline and achieves the best score on almost all metrics . For reference , we show the Real Data and the Top - 1 Retrieval baselines , which do not have a generator . Method PKU unannotated CGL unannotated Content Graphic Content Graphic Occ ↓ Rea ↓ Und ↑ Ove ↓ Occ ↓ Rea ↓ Und ↑ Ove ↓ CGL - GAN 0 . 191 0 . 0312 0 . 32 0 . 069 0 . 481 0 . 0568 0 . 26 0 . 269 DS - GAN 0 . 180 0 . 0301 0 . 52 0 . 026 0 . 435 0 . 0563 0 . 29 0 . 071 ICVT 0 . 189 0 . 0317 0 . 48 0 . 292 0 . 446 0 . 0425 0 . 67 0 . 301 LayoutDM † 0 . 165 0 . 0285 0 . 38 0 . 201 0 . 421 0 . 0506 0 . 49 0 . 069 Autoreg Baseline 0 . 154 0 . 0274 0 . 35 0 . 022 0 . 384 0 . 0427 0 . 76 0 . 058 RALF ( Ours ) 0 . 133 0 . 0231 0 . 87 0 . 018 0 . 336 0 . 0397 0 . 93 0 . 027 Table 2 . Unconstrained generation results on the PKU and CGL unannotated test split . DS - GAN [ 18 ] is a non - autoregressive model using a CNN - LSTM architecture . DS - GAN is only applicable to the un - constrained task because of the internal sorting algorithm . ICVT [ 7 ] is an autoregressive model that combines a Trans - former with a conditional VAE . LayoutDM † [ 19 ] is a discrete state - space diffusion model that can handle many constrained generation tasks . Since the model is originally designed for content - agnostic layout generation , we extend the model to accept an input image . Autoreg Baseline is the one described in Sec . 3 . 2 and is equivalent to our RALF without retrieval augmentation . RALF is our model described in Sec . 3 . Real Data is the ground truth , which can be considered the upper bound . Since we draw the sample from the test split , we calculate the FID score using the validation split . Top - 1 Retrieval is a nearest - neighbor layout without any generator , which can be considered a retrieval - only baseline . 4 . 4 . Implementation Details We re - implement most of the baselines since there are few official implementations publicly available , except for DS - GAN [ 18 ] . In RALF , we retrieve K = 16 nearest neighbor layouts ． Following CGL - GAN [ 53 ] , the height and width size of the input image are set to 350 and 240 , respectively . We generate layouts on three independent trials and report the average of the metrics . We describe the details of training Method Retrieval Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ CGL - GAN 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 CGL - GAN ✓ 0 . 144 0 . 0164 0 . 63 0 . 039 13 . 28 LayoutDM † 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 LayoutDM † ✓ 0 . 123 0 . 0144 0 . 51 0 . 091 10 . 03 Table 3 . Retrieval augmentation for CGL - GAN and LayoutDM † on the PKU test split . and network configuration in the appendix . 4 . 5 . Unconstrained Generation Baseline comparison . Table 1 presents the quantitative results on the annotated test split without user constraints . RALF achieves the best scores , except for the Occ metric of ICVT on CGL . Top - 1 Retrieval , which almost disregards the given content , is unsuitable for the task , as we show deficient performance in content metrics . Table 2 summarizes results on the unannotated test split . RALF achieves the best scores in all the metrics . Compared with Table 1 , all the models exhibit slight performance degra - dation in PKU due to the domain gap problem [ 47 ] between inpainted canvases and clean canvases . We conjecture that the significant performance degradation in CGL comes from non - negligible spatial shifts in subject distributions , which we demonstrate in the appendix . Effectiveness of retrieval augmentation . Tables 1 and 2 demonstrate that retrieval augmentation significantly en - hances the Autoreg Baseline . The only exception is the Occ metric on CGL in Table 1 , where the Autoreg Baseline already closely matches Real Data metrics . Qualitative results . We show the qualitative comparison in Fig . 3 . The results demonstrate that our RALF’s ability to generate well - fitted , non - overlapping , and rational layouts . In contrast , the baseline methods often produce misaligned underlay embellishments and overlapped text elements as 5 ! LayoutDM CGL - GAN DS - GAN AutoregBaseline RALF ( Ours ) PKU Dataset Logo Text Underlay CGL Dataset Embellishment Logo Text Underlay Figure 3 . Visual comparison of unconstrained generation with baselines . Input canvases are selected from the unannotated split . we indicate by red arrows . We also indicate undesirable elements that appear on a salient region by green arrows . Training dataset size . Here , we show that retrieval augmen - tation is effective regardless of the training dataset size in Fig . 4 . Notably , our RALF trained on just 3 , 000 samples outperforms the Autoreg Baseline trained on the full 7 , 734 samples in PKU . Retrieval size K . We show that retrieval augmentation is not highly sensitive to the number of retrieved layouts 1000 3000 5000 7734 # TrainingDataset 0 20 40 60 80 F I D Autoreg Baseline RALF ( Ours ) Figure 4 . FID over the training dataset size ( # TrainingDataset ) , which has up to 7 , 734 samples . 1 2 4 8 16 # Retrieval 0 4 8 12 F I D Autoreg Baseline RALF ( Ours ) Figure 5 . FID over the retrieval size K ( # Retrieval ) . Input image Output 1 Output 2 Retrieved example ( s ) 𝐾 = 16 𝐾 = 1 Figure 6 . Visual comparison of retrieval and generated layouts with different retrieval sizes ( K = 1 and 16 ) . We display the top - 4 examples for K = 16 due to the limited space . The output layouts are generated using different random seeds for variety . Train Test Method Occ ↓ Rea ↓ Und ↑ Ove ↓ CGL PKU Autoreg Baseline 0 . 176 0 . 0276 0 . 84 0 . 037 RALF ( Ours ) 0 . 144 0 . 0249 0 . 96 0 . 023 PKU CGL Autoreg Baseline 0 . 341 0 . 0464 0 . 29 0 . 037 RALF ( Ours ) 0 . 286 0 . 0355 0 . 79 0 . 036 Table 4 . Generation across the unannotated test splits . We train a model on PKU and then test it on CGL , or vice versa . K . As we plot in Fig . 5 , retrieval augmentation significantly enhances the performance even with a single retrieved layout compared to the baseline . The plot indicates FID moderately gets better as we increase the retrieval size K . We examine how different K affects the generated results in Fig . 6 . The result of K = 1 shows that the generated layout is similar to the reference layouts , while the result of K = 16 shows that a variety of layouts are generated . Retrieval augmentation for other generators . While our RALF is an autoregressive generator , we show that retrieval augmentation also benefits other generative mod - els for content - aware layout generation . Here , we adapt 6 C → S + P “ Underlay , Logo , Text , Text” C + S → P “Logo , Text , … , with size” Completion “Logo with size and position” Re0inement “Perturbed layout” Relationship “Logo top on Text” Input Figure 7 . Examples of input con - straints and generated results for each constrained generation task . Quota - tion marks indicate the constraints . Method PKU CGL Content Graphic Content Graphic Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ C → S + P CGL - GAN 0 . 132 0 . 0158 0 . 48 0 . 038 11 . 47 0 . 140 0 . 0213 0 . 65 0 . 047 23 . 93 LayoutDM † 0 . 152 0 . 0201 0 . 46 0 . 172 20 . 56 0 . 127 0 . 0192 0 . 79 0 . 026 3 . 39 Autoreg Baseline 0 . 135 0 . 0167 0 . 43 0 . 028 10 . 48 0 . 124 0 . 0188 0 . 89 0 . 015 1 . 36 RALF ( Ours ) 0 . 124 0 . 0138 0 . 90 0 . 010 2 . 21 0 . 126 0 . 0180 0 . 97 0 . 006 0 . 50 C + S → P CGL - GAN 0 . 129 0 . 0155 0 . 48 0 . 043 9 . 11 0 . 129 0 . 0202 0 . 75 0 . 027 6 . 96 LayoutDM † 0 . 143 0 . 0185 0 . 45 0 . 122 24 . 90 0 . 127 0 . 0190 0 . 82 0 . 021 2 . 18 Autoreg Baseline 0 . 137 0 . 0169 0 . 46 0 . 028 5 . 46 0 . 127 0 . 0191 0 . 88 0 . 013 0 . 47 RALF ( Ours ) 0 . 125 0 . 0138 0 . 87 0 . 010 0 . 62 0 . 128 0 . 0185 0 . 96 0 . 006 0 . 21 Completion CGL - GAN 0 . 150 0 . 0174 0 . 43 0 . 061 25 . 67 0 . 174 0 . 0231 0 . 21 0 . 182 78 . 44 LayoutDM † 0 . 135 0 . 0175 0 . 35 0 . 134 21 . 70 0 . 127 0 . 0192 0 . 76 0 . 020 3 . 19 Autoreg Baseline 0 . 125 0 . 0161 0 . 42 0 . 023 5 . 96 0 . 124 0 . 0185 0 . 91 0 . 011 2 . 33 RALF ( Ours ) 0 . 120 0 . 0140 0 . 88 0 . 012 1 . 58 0 . 126 0 . 0185 0 . 96 0 . 005 1 . 04 Refinement CGL - GAN 0 . 122 0 . 0141 0 . 39 0 . 090 6 . 40 0 . 124 0 . 0182 0 . 86 0 . 024 1 . 20 LayoutDM † 0 . 115 0 . 0121 0 . 57 0 . 008 2 . 86 0 . 127 0 . 0188 0 . 75 0 . 018 1 . 98 Autoreg Baseline 0 . 131 0 . 0171 0 . 41 0 . 026 5 . 89 0 . 126 0 . 0183 0 . 89 0 . 004 0 . 15 RALF ( Ours ) 0 . 113 0 . 0109 0 . 95 0 . 004 0 . 13 0 . 126 0 . 0176 0 . 98 0 . 002 0 . 14 Relationship Autoreg Baseline 0 . 140 0 . 0177 0 . 44 0 . 028 10 . 61 0 . 127 0 . 0189 0 . 88 0 . 015 1 . 28 RALF ( Ours ) 0 . 122 0 . 0141 0 . 85 0 . 009 2 . 23 0 . 126 0 . 0184 0 . 95 0 . 006 0 . 55 Table 5 . Quantitative result of six constrained generation tasks on the PKU and CGL test split . CGL - GAN and LayoutDM † with retrieval augmentation and evaluate the performance . Table 3 summarizes the results . CGL - GAN and LayoutDM † combined with our retrieval augmentation consistently improve many evaluation metrics . We provide additional results in the appendix . Out - of - domain generalization . Table 4 summarizes the results of a cross - evaluation setup where we use different datasets for training and testing . For example , we use the database and training data from CGL and evaluate PKU in the upper half of Table 4 . Remarkably , even in this out - of - domain setting , retrieval augmentation shows notable im - provement and robust generalizability . 4 . 6 . Constrained Generation Following the task setup of content - agnostic generation [ 22 ] , we evaluate several methods in the following constrained tasks in content - aware generation : Category → Size + Position ( C → S + P ) takes in element types and generates the sizes and positions for each element . Category + Size → Position ( C + S → P ) generates element positions based on given element categories and sizes . Completion generates a complete layout using partially placed elements . Refinement corrects cluttered layouts where elements are per - turbed from the ground truth based on a normal distribution with mean 0 and standard deviation 0 . 01 , following [ 41 ] . Relationship is conditioned on both element types and their spatial relationships , determined by the size and position of element pairs . We randomly use 10 % of these relationships in our experiments , following [ 25 ] . Input constraints and generated examples for these tasks are illustrated in Fig . 7 . Baseline comparison . Table 5 summarizes constrained gen - eration results . The results indicate that RALF is effective even for constrained generation tasks . For tasks such as C + S → P and Refinement , RALF shows notable improvement in the FID metric . This suggests that referencing authen - tic examples to understand element relationships enhances position prediction accuracy . Overall , the results highlight RALF’s capability to significantly augment the generative performance over the baseline approach . 4 . 7 . Ablation Study We investigate our design choices in our retrieval augmenta - tion proposed in Sec . 3 . 3 . Layout retrieval . We employ an image feature extractor 7 0 . 013 0 . 014 Readability score 3 . 5 4 . 0 4 . 5 F I D DreamSim LPIPS CLIP Saliency Random Figure 8 . Comparison across different retrieval methods on the PKU test split . We report FID as the representative graphic metric and Readability score as the content metric . to compute the similarity between canvases . We provide a brief overview of possible choices . DreamSim [ 12 ] captures diverse aspects of the similarity simultaneously . LPIPS [ 51 ] focuses on low - level appearance similarity . CLIP [ 40 ] fo - cuses on semantic similarity . Saliency focuses on spatial similarity using the saliency map . We obtain embeddings for similarity computation by down - sampling and flattening S . Random serves as a na ¨ ıve baseline by randomly sampling layouts without focusing on image similarity . We train our RALF with each choice and assess the per - formance . Figure 8 plots FID and Readability score for each retrieval method , and Fig . 9 presents some retrieved exam - ples . DreamSim shows the best balance in the graphic and content metrics . We conjecture that retrieving images based on both semantic and spatial similarity is important . Feature augmentation . We explore the design of our feature augmentation module , as detailed in Table 6 . What types of features to fuse ? RALF combines three fea - tures in Eq . ( 2 ) . We observe that dropping some of the features , as in scenarios ( B ) and ( C ) , leads to a slight dete - rioration of the performance . We try adding features of the top - K retrieved images ˜ f I ∈ R KH ′ W ′ × d that are encoded by the image encoder from the retrieved canvas . However , adding ˜ f I results in decreased performance , as shown in ( D ) . Where to apply ? Our model first applies the Transformer encoder and then retrieval augmentation to the image fea - ture ( A ) . We try another design ( E ) , which places the augmen - tation module before the Transformer encoder , however , this results in worse readability and underlay metrics in exchange for the slight improvement in FID . 5 . Discussion Limitations . We acknowledge two limitations as follows : 1 ) Evaluation of content metrics : The current content metrics assume that well - designed layouts avoid placing elements over salient or cluttered areas . If a counterexample exists , the content metrics may not adequately measure layout quality . Also , the graphic metrics can be easily fooled by a real example , as evidenced by the FID score of the Top - 1 baseline in Table 1 . 2 ) Feature extraction of retrieved layouts : The DreamSim LPIPS CLIP Query Saliency Figure 9 . Qualitative comparison of different retrieval methods . We show the query and the top - 3 retrieved examples for each method as well as saliency maps . Setting Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ A Ours ( Concatenate ( f I , ˜ f L , f C ) ) 0 . 119 0 . 0129 0 . 92 0 . 008 3 . 45 What types of features to fuse ? B Concatenate ( f C , ) 0 . 134 0 . 0144 0 . 92 0 . 008 4 . 67 C Concatenate ( f I , ˜ f L ) 0 . 123 0 . 0133 0 . 91 0 . 007 4 . 08 D Concatenate ( f I , ˜ f L , f C , ˜ f I ) 0 . 141 0 . 0148 0 . 93 0 . 009 8 . 82 Where to apply ? E Before Trans enc 0 . 120 0 . 0138 0 . 72 0 . 009 2 . 34 Table 6 . Ablation study of RALF design on the PKU test split . The top two results are highlighted in bold and underline , respectively . Features include the input canvas feature ( f I ) , retrieved layouts feature ( ˜ f L ) , cross - attended feature ( f C ) , and retrieved images feature ( ˜ f I ) . The full setting of our model ( A ) is described in Eq . ( 2 ) . layout encoder depends on the number of element categories in the dataset . For real - world creative scenarios , extending to an unlimited number of categories , i . e . an open - vocabulary setting [ 11 ] , would be necessary . Future work . We outline two prospective directions to enhance retrieval augmentation for content - aware genera - tion further : 1 ) Ensemble approaches : integrating multiple retrieval results could potentially improve the generation quality . 2 ) Diversifying retrieval modalities : exploring lay - out retrieval using alternative modalities , such as language , could widen the application scope . Yet , generating a whole poster beyond bounding boxes , such as image content , text copies , or styling attributes , remains challenging due to the limited training data for layered graphic designs . Even for such a task , we expect that the retrieval augmentation ap - proach could alleviate the data scarcity problem . Potential societal impacts . As common in any generative models , our RALF may unintentionally produce counterfeit advertisements or magazine layouts , posing risks of decep - tion and dissemination of misleading information . 8 References [ 1 ] Adobe Illustrator CC . https : / / www . adobe . com / products / illustrator . html , Last accessed 17 November , 2023 . 1 [ 2 ] Maneesh Agrawala , Wilmot Li , and Floraine Berthouzoz . De - sign Principles for Visual Communication . Communications of the ACM , 54 ( 4 ) , 2011 . 2 [ 3 ] Diego Martin Arroyo , Janis Postels , and Federico Tombari . Variational Transformer Networks for Layout Generation . In CVPR , 2021 . 2 [ 4 ] Akari Asai , Sewon Min , Zexuan Zhong , and Danqi Chen . ACL 2023 Tutorial : Retrieval - based Language Models and Applications . ACL , 2023 . 2 [ 5 ] Andreas Blattmann , Robin Rombach , Kaan Oktay , and Bj¨orn Ommer . Retrieval - Augmented Diffusion Models . In NeurIPS , 2022 . 1 , 2 , 13 [ 6 ] Sebastian Borgeaud , Arthur Mensch , Jordan Hoffmann , Trevor Cai , Eliza Rutherford , Katie Millican , George van den Driessche , Jean - Baptiste Lespiau , Bogdan Damoc , Aidan Clark , Diego de Las Casas , Aurelia Guy , Jacob Menick , Ro - man Ring , Tom Hennigan , Saffron Huang , Loren Maggiore , Chris Jones , Albin Cassirer , Andy Brock , Michela Paganini , Geoffrey Irving , Oriol Vinyals , Simon Osindero , Karen Si - monyan , Jack W . Rae , Erich Elsen , and Laurent Sifre . Improv - ing language models by retrieving from trillions of tokens . arXiv preprint arXiv : 2112 . 04426 , 2021 . 1 , 2 [ 7 ] Yunning Cao , Ye Ma , Min Zhou , Chuanbin Liu , Hongtao Xie , Tiezheng Ge , and Yuning Jiang . Geometry Aligned Varia - tional Transformer for Image - conditioned Layout Generation . In ACM MM , 2022 . 2 , 3 , 5 , 14 [ 8 ] Nicolas Carion , Francisco Massa , Gabriel Synnaeve , Nicolas Usunier , Alexander Kirillov , and Sergey Zagoruyko . End - to - End Object Detection with Transformers . In ECCV , 2020 . 2 [ 9 ] Shang Chai , Liansheng Zhuang , and Fengying Yan . Lay - outDM : Transformer - based Diffusion Model for Layout Gen - eration . In CVPR , 2023 . 2 [ 10 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . BERT : Pre - training of Deep Bidirectional Trans - formers for Language Understanding . In NAACL , 2019 . 2 [ 11 ] Weixi Feng , Wanrong Zhu , Tsu - jui Fu , Varun Jampani , Ar - jun Akula , Xuehai He , Sugato Basu , Xin Eric Wang , and William Yang Wang . LayoutGPT : Compositional Visual Planning and Generation with Large Language Models . In NeurIPS , 2023 . 8 [ 12 ] Stephanie Fu * , Netanel Tamir * , Shobhita Sundaram * , Lucy Chai , Richard Zhang , Tali Dekel , and Phillip Isola . Dream - Sim : Learning New Dimensions of Human Visual Similarity using Synthetic Data . In NeurIPS , 2023 . 4 , 8 [ 13 ] Shunan Guo , Zhuochen Jin , Fuling Sun , Jingwen Li , Zhaorui Li , Yang Shi , and Nan Cao . Vinci : An Intelligent Graphic Design System for Generating Advertising Posters . In CHI , 2021 . 1 [ 14 ] Kamal Gupta , Alessandro Achille , Justin Lazarow , Larry Davis , Vijay Mahadevan , and Abhinav Shrivastava . Lay - outTransformer : Layout Generation and Completion with Self - attention . In ICCV , 2021 . 2 , 3 [ 15 ] Kelvin Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang . REALM : Retrieval - Augmented Language Model Pre - Training . In ICML , 2020 . 1 , 2 [ 16 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep Residual Learning for Image Recognition . In CVPR , 2016 . 3 [ 17 ] Scarlett R . Herring , Chia - Chen Chang , Jesse Krantzler , and Brian P . Bailey . Getting Inspired ! Understanding How and Why Examples Are Used in Creative Design Practice . In CHI , 2009 . 1 [ 18 ] Hsiao Yuan Hsu , Xiangteng He , Yuxin Peng , Hao Kong , and Qing Zhang . PosterLayout : A New Benchmark and Approach for Content - Aware Visual - Textual Presentation Layout . In CVPR , 2023 . 1 , 2 , 3 , 4 , 5 , 11 , 12 , 14 [ 19 ] Naoto Inoue , Kotaro Kikuchi , Edgar Simo - Serra , Mayu Otani , and Kota Yamaguchi . LayoutDM : Discrete Diffusion Model for Controllable Layout Generation . In CVPR , 2023 . 1 , 2 , 5 , 14 [ 20 ] Ali Jahanian , Jerry Liu , Qian Lin , Daniel Tretter , Eamonn O’Brien - Strain , Seungyon Claire Lee , Nic Lyons , and Jan Allebach . Recommendation System for Automatic Design of Magazine Covers . In IUI , 2013 . 1 [ 21 ] Zhaoyun Jiang , Shizhao Sun , Jihua Zhu , Jian - Guang Lou , and Dongmei Zhang . Coarse - to - fine generative modeling for graphic layouts . In AAAI , 2022 . 2 [ 22 ] Z . Jiang , J . Guo , S . Sun , H . Deng , Z . Wu , V . Mijovic , Z . Yang , J . Lou , and D . Zhang . LayoutFormer + + : Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction . In CVPR , 2023 . 2 , 3 , 7 , 11 [ 23 ] Jeff Johnson , Matthijs Douze , and Herv´e J´egou . Billion - scale similarity search with GPUs . IEEE Transactions on Big Data , 7 ( 3 ) : 535 – 547 , 2019 . 11 [ 24 ] Akash Abdu Jyothi , Thibaut Durand , Jiawei He , Leonid Si - gal , and Greg Mori . LayoutVAE : Stochastic Scene Layout Generation from a Label Set . In CVPR , 2019 . 2 [ 25 ] Kotaro Kikuchi , Edgar Simo - Serra , Mayu Otani , and Kota Ya - maguchi . Constrained Graphic Layout Generation via Latent Optimization . In ACM MM , 2021 . 2 , 4 , 7 , 12 [ 26 ] Xiang Kong , Lu Jiang , Huiwen Chang , Han Zhang , Yuan Hao , Haifeng Gong , and Irfan Essa . BLT : Bidirectional Layout Transformer for Controllable Layout Generation . In ECCV , 2022 . 2 [ 27 ] Hsin - Ying Lee , Weilong Yang , Lu Jiang , Madison Le , Irfan Essa , Haifeng Gong , and Ming - Hsuan Yang . Neural Design Network : Graphic Layout Generation with Constraints . In ECCV , 2019 . 4 [ 28 ] Elad Levi , Eli Brosh , Mykola Mykhailych , and Meir Perez . DLT : Conditioned Layout Generation with Joint Discrete - Continuous Diffusion Layout Transformer . In ICCV , 2023 . 2 [ 29 ] Fengheng Li , An Liu , Wei Feng , Honghe Zhu , Yaoyu Li , Zheng Zhang , Jingjing Lv , Xin Zhu , Junjie Shen , Zhangang Lin , and Jingping Shao . Relation - Aware Diffusion Model for Controllable Poster Layout Generation . In CIKM , 2023 . 2 [ 30 ] Jianan Li , Jimei Yang , Aaron Hertzmann , Jianming Zhang , and Tingfa Xu . LayoutGAN : Generating Graphic Layouts with Wireframe Discriminators . In ICLR , 2019 . 2 9 [ 31 ] Jianan Li , Jimei Yang , Jianming Zhang , Chang Liu , Christina Wang , and Tingfa Xu . Attribute - Conditioned Layout GAN for Automatic Graphic Design . IEEE TVCG , 27 ( 10 ) , 2021 . 2 , 12 [ 32 ] Tsung - Yi Lin , Piotr Doll´ar , Ross Girshick , Kaiming He , Bharath Hariharan , and Serge Belongie . Feature Pyramid Networks for Object Detection . In CVPR , 2017 . 3 , 11 [ 33 ] Simon Lok and Steven Feiner . A Survey of Automated Layout Techniques for Information Presentations . In SmartGraphics , 2001 . 2 [ 34 ] Ilya Loshchilov and Frank Hutter . Fixing Weight Decay Regularization in Adam . In ICLR , 2019 . 11 [ 35 ] Muhammad Ferjad Naeem , Seong Joon Oh , Youngjung Uh , Yunjey Choi , and Jaejun Yoo . Reliable Fidelity and Diversity Metrics for Generative Models . In ICML , 2020 . 13 [ 36 ] Peter O’Donovan , Aseem Agarwala , and Aaron Hertzmann . DesignScape : Design with Interactive Layout Suggestions . In CHI , 2015 . 1 , 2 [ 37 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , An - dreas K¨opf , Edward Z . Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . PyTorch : An Im - perative Style , High - Performance Deep Learning Library . In NeurIPS , 2019 . 11 [ 38 ] Chunyao Qian , Shizhao Sun , Weiwei Cui , Jian - Guang Lou , Haidong Zhang , and Dongmei Zhang . Retrieve - Then - Adapt : Example - based Automatic Generation for Proportion - related Infographics . IEEE TVCG , 27 ( 2 ) , 2021 . 2 [ 39 ] Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , et al . Language models are unsuper - vised multitask learners . OpenAI blog , 1 ( 8 ) : 9 , 2019 . 3 [ 40 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , Gretchen Krueger , and Ilya Sutskever . Learning Transferable Visual Models From Natural Language Supervision . In ICML , 2021 . 4 , 8 [ 41 ] Soliha Rahman , Vinoth Pandian Sermuga Pandian , and Matthias Jarke . RUITE : Refining UI Layout Aesthetics Using Transformer Encoder . In IUI Companion , 2021 . 7 [ 42 ] Shelly Sheynin , Oron Ashual , Adam Polyak , Uriel Singer , Oran Gafni , Eliya Nachmani , and Yaniv Taigman . KNN - Diffusion : Image Generation via Large - Scale Retrieval . In ICLR , 2023 . 1 , 2 [ 43 ] Karen Simonyan and Andrew Zisserman . Very Deep Con - volutional Networks for Large - Scale Image Recognition . In ICLR , 2015 . 13 [ 44 ] Roman Suvorov , Elizaveta Logacheva , Anton Mashikhin , Anastasia Remizova , Arsenii Ashukha , Aleksei Silvestrov , Naejin Kong , Harshith Goka , Kiwoong Park , and Victor Lem - pitsky . Resolution - robust Large Mask Inpainting with Fourier Convolutions . In WACV , 2022 . 4 [ 45 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkor - eit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . Attention Is All You Need . In NeurIPS , 2017 . 2 [ 46 ] Bo Wang , Quan Chen , Min Zhou , Zhiqiang Zhang , Xiaogang Jin , and Kun Gai . Progressive Feature Polishing Network for Salient Object Detection . In AAAI , 2020 . 2 [ 47 ] Chenchen Xu , Min Zhou , Tiezheng Ge , Yuning Jiang , and Weiwei Xu . Unsupervised Domain Adaption With Pixel - Level Discriminator for Image - Aware Layout Generation . In CVPR , 2023 . 5 [ 48 ] Xuyong Yang , Tao Mei , Ying - Qing Xu , Yong Rui , and Shipeng Li . Automatic Generation of Visual - Textual Pre - sentation Layout . ACM TOMM , 12 ( 2 ) , 2016 . 1 , 2 [ 49 ] Junyi Zhang , Jiaqi Guo , Shizhao Sun , Jian - Guang Lou , and Dongmei Zhang . LayoutDiffusion : Improving Graphic Lay - out Generation by Discrete Diffusion Probabilistic Models . In ICCV , 2023 . 2 [ 50 ] Peiying Zhang , Chenhui Li , and Changbo Wang . Smarttext : Learning To Generate Harmonious Textual Layout Over Nat - ural Image . In ICME , 2020 . 2 [ 51 ] Richard Zhang , Phillip Isola , Alexei A Efros , Eli Shechtman , and Oliver Wang . The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , 2018 . 8 [ 52 ] Xinru Zheng , Xiaotian Qiao , Ying Cao , and Rynson W . H . Lau . Content - Aware Generative Modeling of Graphic Design Layouts . ACM TOG , 38 ( 4 ) , 2019 . 1 , 2 [ 53 ] Min Zhou , Chenchen Xu , Ye Ma , Tiezheng Ge , Yuning Jiang , and Weiwei Xu . Composition - aware Graphic Layout GAN for Visual - textual Presentation Designs . In IJCAI , 2022 . 1 , 2 , 3 , 4 , 5 , 13 , 14 10 Appendix Table of contents : • Section A : Code Availability • Section B : Implementation Details • Section C : Dataset Preprocessing • Section D : Additional Results A . Code Availability We will make our code publicly available on acceptance . B . Implementation Details Architecture details . Our RALF consists of four modules : the image encoder , retrieval augmentation , layout decoder , and optional constraint encoder . Table A provides the num - ber of parameters of these modules . Image encoder consists of ResNet - 50 - FPN [ 32 ] and the Transformer encoder . We obtain the saliency map following the approach in DS - GAN [ 18 ] . Retrieval augmentation . We implement the retrieval part using faiss [ 23 ] . The layout encoder for retrieved layouts consists of the Transformer encoder and a feed - forward net - work , which adapts the feature map size of retrieved layouts to the size of the layout decoder . Before training , we pre - train the layout encoder for each dataset and extract features over each training dataset to construct the retrieval database . We note that the parameters of the layout encoder ( 1 . 59M ) are excluded from the total parameters of RALF , since they are set with the retrieval database . Layout decoder . We employ the Transformer decoder . The configurations of the Transformer layers are as follows : 6 layers , 8 attention heads , 256 embedding dimensions , 1 , 024 hidden dimensions , and 0 . 1 dropout rate . The size of bins for the layout tokenizer is set to 128 . In the inference phase , for the relationship task , we use a decoding space restriction mechanism [ 22 ] , which aims to prune the predicted tokens that violate a user - specified constraint . Training details . We implemente RALF in PyTorch [ 37 ] and train for 50 and 70 epochs with AdamW optimizer [ 34 ] for the PKU and CGL datasets , respectively . The training time is about 4 hours and 20 minutes for the PKU dataset and 18 hours for the CGL dataset on a single A100 GPU . We divide the learning rate by 10 after 70 % of the total epoch elapsed . We set the batch size , learning rate , weight decay , and gradient norm to 32 , 10 − 4 , 10 − 4 , and 10 − 1 , respectively . Testing details . We generate layouts on three independent trials and report the average of the metrics . We use top - k sampling for all the models that rely on sampling in logit space . We set k and temperature to 5 and 1 . 0 , respectively . Other baselines . For the training of baseline methods , we follow the original training setting referring to their papers Module # Params Image encoder ( ResNet50 ) 25 . 02 M Image encoder ( Trans Enc ) 4 . 74 M Constraint encoder 4 . 88 M Retrieval augmentation 1 . 59 M Layout decoder 6 . 59 M Total 42 . 82 M Table A . The number of parameters of each module . ( b ) Original poster w / layout ( c ) Original Inpaintingresults ( d ) Inpainting results ( Ours ) ( a ) Originalposter Overview Text region ( zoomed ) Figure A . Comparison of inpainting for the dataset preprocessing . as much as possible . There are some exceptions for a fair comparison . For example , the number of embedding dimen - sions and hidden dimensions in Transformer is adjusted to roughly match the number of parameters for each model . We use ResNet - 50 - FPN as the image encoder for all of our baseline methods . C . Dataset Preprocessing We demonstrate the importance of adequately preprocessing annotated poster images in Fig . A . Layout annotations in existing datasets sometimes exhibit inaccuracies for some underlying factors , including the semi - automatic collection process using object detection models [ 18 ] as shown in ( a ) 11 ( b ) Original poster w / layout ( c ) Original Inpaintingresults ( d ) Inpainting results ( Ours ) ( a ) Originalposter Figure B . Comparison of inpainting for the dataset preprocessing . 0 5 10 15 # Elements per layout 0 500 1000 1500 2000 # S a m p l e s Figure C . Number of elements per layout in the original PKU dataset . A red dashed line indicates the maximum number of elements we use . and ( b ) . The inaccuracy severely harms the image inpainting quality when we fully depend on the annotations , as shown in ( c ) . To cope with the inaccuracy , we slightly dilate the target region for inpainting and get better results with fewer artifacts , as shown in ( d ) . We show more examples in Fig . B . We observe that about 20 % of the original inpainted images in PKU contain significant artifacts . We plot the number of layout elements for each poster in Fig . C . Although we filter out posters with more than 11 layout elements , it only accounts for about 2 % of the original dataset . Test split Unannotated test split Averaged saliency map Averaged saliency map Mean = 0 . 375 Mean = 0 . 458 Figure D . Visual comparison of canvases and saliency maps be - tween the test and unannotated test split of the CGL dataset . Can - vases are randomly selected from each split . The averaged saliency map is produced by computing the spatial average of all saliency maps of each split . Mean represents the spatial average of all saliency maps of each split . D . Additional Results Spatial distribution shift . Figure D shows the visual com - parison of canvases and saliency maps between the test and unannotated test split of CGL . We see that the proportion of space occupied by the saliency map is different according to the different values of Mean . As a result , this difference causes the performance degradation in CGL . Comprehensive quantitative comparison . We additionally adopt five metrics . Graphic metrics . Alignment ( Align ↓ ) [ 25 , 31 ] computes how well the elements are aligned with each other . For de - tailed calculation , please refer to [ 25 , 31 ] . Loose underlay effectiveness ( Und L ↑ ) [ 18 ] also calculates the proportion of the total area of valid underlay elements to the total of un - derlay and non - underlay elements . Note that we define this loose metric as Und L ↑ to distinguish it from the strict under - 12 lay effectiveness Und S ↑ introduced in the main manuscript . Density ( Den ↑ ) and Coverage ( Cov ↑ ) [ 35 ] compute fidelity and diversity aspects of the generated layouts against ground - truth layouts . Please refer to [ 35 ] for more details . Content metrics . Salient consistency ( R shm ↓ ) [ 53 ] com - putes the Euclidean distance between the output logits of the canvases with or without layout regions masked using a pre - trained VGG16 [ 43 ] . Tables B and C present the quantitative result on the annotated test split without user constraints on the PKU and CGL datasets , respectively . RALF notably improves Density and Coverage metrics , indicating that RALF can generate better layouts in terms of both fidelity and diversity . RALF does not achieve the best score regarding R shm and Alignment . However , these metrics may not be very reliable since the best scores for these metrics largely deviate from the scores for Real - Data , unlike other metrics . Retrieval augmentation for baseline method . Table D shows the results of retrieval augmentation for CGL - GAN and LayoutDM † . Even for constrained generation tasks , retrieval augmentation achieves a better quality of generation for other generators on almost all metrics . Impact on changing # Dim in layout decoder . Table E provides the results of RALF and Autoreg Baseline while changing the number of parameters in the layout decoder . We modify the number of features ( # Dim ) and hidden dim to four times the number of # Dim . RALF’s performance peaks when # Dim is 256 . Autoreg Baseline’s performance improves as # Dim increases , but the model with # Dim = 768 still clearly underperforms RALF with # Dim = 256 . Thus , retrieval augmentation enables us to use a relatively compact network for content - aware layout generation . This result aligns with the trend observed in other domains , such as im - age generation [ 5 ] . We conjecture slight performance degra - dation as we increase # Dim over 256 in RALF is caused by overfitting as we watch loss curves for training and valida - tion . Visual comparison on constrained generation . Figures E and F provide the qualitative comparisons of constrained generation for the PKU and CGL datasets , respectively . The results demonstrate that our RALF successfully generates well - fitted , non - overlapping , and rational layouts even in constrained generation tasks . 13 Method PKU Content Graphic Occ ↓ Rea ↓ R shm ↓ Align ↓ Und L ↑ Und S ↑ Ove ↓ Den ↑ Cov ↑ FID ↓ Real - Data 0 . 112 0 . 0102 13 . 94 0 . 00379 0 . 99 0 . 99 0 . 0009 0 . 95 0 . 95 1 . 58 Top1 - Retrieval 0 . 212 0 . 0218 16 . 33 0 . 00371 0 . 99 0 . 99 0 . 002 1 . 07 0 . 97 1 . 43 CGL - GAN [ 53 ] 0 . 138 0 . 0164 14 . 32 0 . 00311 0 . 81 0 . 41 0 . 074 0 . 70 0 . 68 34 . 51 DS - GAN [ 18 ] 0 . 142 0 . 0169 14 . 95 0 . 00347 0 . 89 0 . 63 0 . 027 1 . 10 0 . 82 11 . 80 ICVT [ 7 ] 0 . 146 0 . 0185 13 . 92 0 . 00228 0 . 63 0 . 49 0 . 318 0 . 35 0 . 40 39 . 13 LayoutDM † [ 19 ] 0 . 150 0 . 0192 13 . 06 0 . 00298 0 . 64 0 . 41 0 . 190 0 . 74 0 . 59 27 . 09 Autoreg Baseline 0 . 134 0 . 0164 14 . 43 0 . 00192 0 . 79 0 . 43 0 . 019 1 . 13 0 . 79 13 . 59 RALF ( Ours ) 0 . 119 0 . 0129 14 . 11 0 . 00267 0 . 98 0 . 92 0 . 008 1 . 25 0 . 97 3 . 45 Table B . Unconstrained generation results on the PKU test split . Method CGL Content Graphic Occ ↓ Rea ↓ R shm ↓ Align ↓ Und L ↑ Und S ↑ Ove ↓ Den ↑ Cov ↑ FID ↓ Real - Data 0 . 125 0 . 0170 14 . 33 0 . 00240 0 . 99 0 . 98 0 . 0002 0 . 93 1 . 00 0 . 79 Top1 - Retrieval 0 . 214 0 . 0266 16 . 02 0 . 00254 0 . 99 0 . 99 0 . 0005 1 . 01 0 . 90 0 . 93 CGL - GAN [ 53 ] 0 . 157 0 . 0237 14 . 12 0 . 00320 0 . 67 0 . 29 0 . 161 0 . 31 0 . 28 66 . 75 DS - GAN [ 18 ] 0 . 141 0 . 0229 14 . 85 0 . 00257 0 . 71 0 . 45 0 . 057 0 . 64 0 . 40 41 . 57 ICVT [ 7 ] 0 . 124 0 . 0205 13 . 40 0 . 00319 0 . 55 0 . 42 0 . 310 0 . 16 0 . 22 65 . 34 LayoutDM † [ 19 ] 0 . 127 0 . 0192 14 . 15 0 . 00242 0 . 92 0 . 82 0 . 020 0 . 87 0 . 93 2 . 36 Autoreg Baseline 0 . 125 0 . 0190 14 . 22 0 . 00234 0 . 97 0 . 92 0 . 011 1 . 05 0 . 91 2 . 89 RALF ( Ours ) 0 . 125 0 . 0180 14 . 26 0 . 00236 0 . 99 0 . 98 0 . 004 1 . 09 0 . 96 1 . 32 Table C . Unconstrained generation results on the CGL test split . 14 Task Method Retrieval PKU CGL Content Graphic Content Graphic Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Unconstraint Real Data 0 . 112 0 . 0102 0 . 99 0 . 0009 1 . 58 0 . 125 0 . 0170 0 . 98 0 . 0002 0 . 79 Top - 1 Retrieval 0 . 212 0 . 0218 0 . 99 0 . 002 1 . 43 0 . 214 0 . 0266 0 . 99 0 . 0005 0 . 93 CGL - GAN 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 0 . 157 0 . 0237 0 . 29 0 . 161 66 . 75 CGL - GAN ✓ 0 . 144 0 . 0164 0 . 63 0 . 039 13 . 28 0 . 172 0 . 0245 0 . 42 0 . 157 60 . 67 LayoutDM † 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 0 . 127 0 . 0192 0 . 82 0 . 020 2 . 36 LayoutDM † ✓ 0 . 123 0 . 0144 0 . 51 0 . 091 10 . 03 0 . 126 0 . 0187 0 . 85 0 . 019 1 . 97 C → S + P CGL - GAN 0 . 132 0 . 0158 0 . 48 0 . 038 11 . 47 0 . 140 0 . 0213 0 . 65 0 . 047 23 . 93 CGL - GAN ✓ 0 . 140 0 . 0153 0 . 66 0 . 030 10 . 23 0 . 138 0 . 0202 0 . 82 0 . 021 10 . 01 LayoutDM † 0 . 152 0 . 0201 0 . 46 0 . 172 20 . 50 0 . 127 0 . 0192 0 . 79 0 . 026 3 . 39 LayoutDM † ✓ 0 . 121 0 . 0141 0 . 55 0 . 088 9 . 02 0 . 127 0 . 0189 0 . 81 0 . 026 3 . 36 C + S → P CGL - GAN 0 . 129 0 . 0155 0 . 48 0 . 043 9 . 11 0 . 129 0 . 0202 0 . 75 0 . 027 6 . 96 CGL - GAN ✓ 0 . 146 0 . 0178 0 . 57 0 . 036 7 . 74 0 . 135 0 . 0207 0 . 78 0 . 020 6 . 01 LayoutDM † 0 . 143 0 . 0185 0 . 45 0 . 122 24 . 90 0 . 127 0 . 0190 0 . 82 0 . 021 2 . 18 LayoutDM † ✓ 0 . 123 0 . 0144 0 . 59 0 . 071 10 . 68 0 . 127 0 . 0188 0 . 83 0 . 020 1 . 77 Completion CGL - GAN 0 . 146 0 . 0175 0 . 42 0 . 076 27 . 18 0 . 174 0 . 0231 0 . 21 0 . 182 78 . 44 CGL - GAN ✓ 0 . 146 0 . 0169 0 . 71 0 . 039 12 . 46 0 . 155 0 . 0230 0 . 46 0 . 102 48 . 82 LayoutDM † 0 . 135 0 . 0175 0 . 35 0 . 134 21 . 70 0 . 127 0 . 0192 0 . 76 0 . 020 3 . 19 LayoutDM † ✓ 0 . 120 0 . 0143 0 . 45 0 . 071 12 . 96 0 . 126 0 . 0189 0 . 79 0 . 018 2 . 55 Refinement CGL - GAN 0 . 122 0 . 0141 0 . 39 0 . 090 6 . 40 0 . 124 0 . 0182 0 . 86 0 . 024 1 . 20 CGL - GAN ✓ 0 . 129 0 . 0157 0 . 37 0 . 072 4 . 91 0 . 133 0 . 0195 0 . 87 0 . 022 3 . 06 LayoutDM † 0 . 115 0 . 0121 0 . 57 0 . 008 2 . 86 0 . 127 0 . 0188 0 . 75 0 . 018 1 . 98 LayoutDM † ✓ 0 . 115 0 . 0121 0 . 57 0 . 007 2 . 91 0 . 126 0 . 0186 0 . 76 0 . 019 1 . 79 Table D . Retrieval augmentation for CGL - GAN and LayoutDM † on the PKU and CGL test split for unconstrained and constrained generation . Method # Dim # ParamsDec PKU CGL Content Graphic Content Graphic Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Occ ↓ Rea ↓ Und ↑ Ove ↓ FID ↓ Autoreg Baseline 128 2 . 55M 0 . 146 0 . 0184 0 . 41 0 . 030 18 . 86 0 . 127 0 . 0196 0 . 86 0 . 013 3 . 60 RALF 0 . 123 0 . 0141 0 . 71 0 . 007 4 . 14 0 . 125 0 . 0180 0 . 97 0 . 005 1 . 27 Autoreg Baseline ♢ 256 6 . 59M 0 . 134 0 . 0165 0 . 44 0 . 018 13 . 51 0 . 125 0 . 0190 0 . 92 0 . 011 2 . 90 RALF ♢ 0 . 119 0 . 0129 0 . 92 0 . 008 3 . 45 0 . 125 0 . 0180 0 . 98 0 . 004 1 . 31 Autoreg Baseline 512 19 . 46M 0 . 128 0 . 0150 0 . 57 0 . 011 10 . 85 0 . 122 0 . 0184 0 . 95 0 . 009 2 . 74 RALF 0 . 122 0 . 0131 0 . 94 0 . 010 3 . 61 0 . 128 0 . 0182 0 . 97 0 . 004 1 . 72 Autoreg Baseline 768 38 . 82M 0 . 122 0 . 0150 0 . 70 0 . 012 8 . 46 0 . 124 0 . 0183 0 . 95 0 . 008 2 . 26 RALF 0 . 126 0 . 0131 0 . 93 0 . 008 3 . 19 0 . 131 0 . 0187 0 . 97 0 . 004 1 . 72 Table E . Qualitative result of varying network parameters on unconstrained generation metrics on the PKU and CGL test split . We modify the number of features ( # Dim ) in the input of cross - attention layers and the sequence to the decoder layer . # ParamsDec indicates the number of parameters of the layout decoder . ♢ represents the setting of our experiments in the main manuscript . 15 C + S → P Completion Re / inement C → S + P Relationship Constraint RALF ( Ours ) Output 1 Output 2 Output 3 CGL - GAN ! LayoutDM AutoregBaseline Ground truth Input image Category : Logo , Text , Underlay Category + Size : Logo , … , Underlaywith Size Category + Relation : e . g . Underlay bottom Canvas , … PKU Dataset Logo Text Underlay Figure E . Visual comparison of constrained generation with baselines on the PKU annotated test split . 16 C + S → P Completion Re / inement C → S + P Relationship Constraint RALF ( Ours ) Output 1 Output 2 Output 3 CGL - GAN ! LayoutDM AutoregBaseline Ground truth Input image Category + Size : Logo , … , Underlaywith Size Category + Relation : e . g . Underlay bottom Canvas , … CGL Dataset Embellishment Logo Text Underlay Category : Logo , Text , Underlay Figure F . Visual comparison of constrained generation with baselines on the CGL annotated test split . 17