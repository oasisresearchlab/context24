Retrieval - Augmented Layout Transformer for Content - Aware Layout Generation Daichi Horita 1 Naoto Inoue 2 Kotaro Kikuchi 2 Kota Yamaguchi 2 Kiyoharu Aizawa 1 1 The University of Tokyo 2 CyberAgent { horita , aizawa } @ hal . t . u - tokyo . ac . jp { inoue naoto , kikuchi kotaro xa , yamaguchi kota } @ cyberagent . co . jp Abstract Content - aware graphic layout generation aims to auto - matically arrange visual elements along with a given content , such as an e - commerce product image . In this paper , we ar - gue that the current layout generation approaches suffer from the limited training data for the high - dimensional lay - out structure . We show that a simple retrieval augmentation can significantly improve the generation quality . Our model , which is named Retrieval - Augmented Layout Transformer ( RALF ) , retrieves nearest neighbor layout examples based on an input image and feeds these results into an autoregressive generator . Our model can apply retrieval augmentation to various controllable generation tasks and yield high - quality layouts within a unified architecture . Our extensive experi - ments show that RALF successfully generates content - aware layouts in both constrained and unconstrained settings and significantly outperforms the baselines . 1 1 . Introduction Layout is an essential part of graphic design , where the aesthetic appeal relies on the harmonious arrangement and selection of visual elements such as logos and texts . In real - world creative workflows , such as posters [ 13 , 36 ] and magazines [ 20 , 48 ] creation , designers typically work on a given subject ; for example , creating an advertising poster of a specific product . We call layout generation under such conditions content - aware layout generation , where the goal is to generate diverse yet plausible arrangements of element bounding boxes that harmonize with the given background image ( canvas ) . Recent studies [ 52 , 53 ] show that generative models can produce content - aware layouts that respect aes - thetic principles , such as avoiding overlaps [ 13 ] . However , generated layouts often still suffer from artifacts , including misaligned underlay embellishment and text elements . We hypothesize that current approaches based solely on gen - erative models do not scale due to the scarcity of highly structured layout data . Unlike public images on the Web , 1 Our project page is available at https : / / udonda . github . io / RALF / Input image Output layouts Retrieved examples Figure 1 . Retrieval - augmented content - aware layout generation . We retrieve nearest neighbor examples based on the input image and use them as a reference to augment the generation process . curating a large dataset of layered graphic designs is not a viable solution since designers typically create their work in proprietary authoring tools , such as Adobe Illustrator [ 1 ] . Inspired by the fact that designers often refer to existing designs [ 17 ] , we propose a retrieval - augmented generation method to address the challenges in the layout domain . Re - cent literature shows that retrieval augmentation helps in enhancing the generation quality of language models [ 6 , 15 ] and image synthesis [ 5 , 42 ] , thanks to the ability to reference real examples in the limited data domain . We argue that retrieval augmentation plays an important role in mitigating the data scarcity problem in content - aware layout generation . We build R etrieval - A ugmented L ayout Trans F ormer ( RALF ) , which is an autoregressive generator capable of ref - erencing external layout examples . RALF retrieves reference layouts by nearest neighbor search based on the appearance of the input and supplements the generation process ( Fig . 1 ) . Since the input canvas and retrieved layouts have different modalities , we use the cross - attention mechanism to aug - ment the feature input to the generator . Although we build RALF with an autoregressive approach , retrieval augmenta - tion is also effective in other generation approaches such as diffusion models [ 19 ] , which we show in the experiments . We evaluate our RALF on public benchmarks [ 18 , 53 ] and show that RALF outperforms state - of - the - art models in content - aware layout generation . Thanks to the retrieval capability , RALF requires less than half the training data to achieve the same performance as the baseline . We further 1 a r X i v : 2311 . 13602v1 [ c s . C V ] 22 N ov 2023 show that our modular architecture can adapt to control - lable generation tasks that impose various user - specified constraints , which is common in real - world workflow . We summarize our contributions as follows : 1 ) We find that retrieval augmentation effectively addresses the data scarcity problem in content - aware layout generation . 2 ) We propose a Retrieval - Augmented Layout Transformer ( RALF ) designed to integrate retrieval augmentation for layout gen - eration tasks . 3 ) Our extensive evaluations show that our RALF successfully generates high - quality layouts under var - ious scenarios and significantly outperforms baselines . We will make our code publicly available on acceptance . 2 . Related Work 2 . 1 . Content - agnostic Layout Generation Content - agnostic layout generation , which aims at generat - ing layouts without a specific input canvas , has been stud - ied for a long time [ 2 , 33 , 36 , 48 ] . The typical approach involves predicting the arrangement of elements , where each element has a tuple of attributes such as category , po - sition , and size [ 30 ] . Recent approaches employ various types of neural networks - based generative models , such as generative adversarial networks ( GAN ) [ 25 , 30 , 31 ] , varia - tional autoencoders ( VAE ) [ 3 , 21 , 24 ] , autoregressive mod - els [ 14 , 22 ] , non - autoregressive models [ 26 ] , and diffusion models [ 9 , 19 , 28 , 49 ] . Note that the retrieval augmentation discussed in this paper may not be directly applicable to the content - agnostic setup due to the lack of input queries . Several works consider user - specified design constraints such as ‚Äúa title is above the body‚Äù , which are often seen in real - world workflow . Such constraints are studied as controllable generation [ 19 , 22 , 25 , 26 ] , where the model generates a complete layout from a partial or noisy layout . In this paper , we adapt the concept of controllable generation to the content - aware generation . 2 . 2 . Content - aware Layout Generation Content - aware layout generation , relatively less studied compared to the content - agnostic setup , has seen notable progress . ContentGAN [ 52 ] first tackles to incorporate image semantics of input canvases . Subsequently , CGL - GAN [ 53 ] introduces a saliency map to a non - autoregressive decoder [ 8 , 10 , 45 ] for better subject representation . DS - GAN [ 18 ] proposes a CNN - LSTM framework . ICVT [ 7 ] em - ploys a conditional VAE , predicting a category and bounding box autoregressively based on previously predicted elements . RADM [ 29 ] leverages a diffusion model and introduces modules to refine both visual ‚Äì textual and textual ‚Äì textual pre - sentations . We note that we cannot compare RADM in our experiments because their text annotations are not available . Current approaches rely solely on generative models and may struggle with capturing sparse data distributions with limited training data . We use retrieval augmentation to miti - gate this issue , and our experiments confirm its significant impact on enhancing content - aware generation . 2 . 3 . Retrieval - Augmented Generation Retrieval augmentation [ 4 ‚Äì 6 , 15 , 42 ] offers an orthogonal approach to enhance generative models without increasing network parameters or relying heavily on extensive training datasets . Generative models equipped with retrieval aug - mentation stop storing all relevant knowledge in their model parameters and instead use external memory via retrieving relevant information as needed . A common approach in - volves retrieving the k - nearest neighbors ( k - NN ) based on a pre - calculated embedding space as additional input . For example , REALM [ 15 ] introduces a retrieval augmentation into language models that fetch k - NN based on preceding tokens . In image generation , RDM [ 5 ] demonstrates even a relatively compact network can achieve state - of - the - art per - formance by retrieval augmentation . KNN - Diffusion [ 42 ] shows its capacity to generate out - of - distribution images . The unique challenge in content - aware layout generation involves encoding both image and layout modalities , which we address using a cross - attention mechanism . Given that tasks related to graphic design , such as content - aware layout generation , often suffer from data scarcity prob - lems [ 38 ] , we believe that retrieval augmentation is particu - larly beneficial . It provides an efficient training method that leverages existing data more effectively . 3 . Method 3 . 1 . Preliminaries Let X and Y be the sets of canvas images and graphic lay - outs , respectively . We use I ‚àà X and L ‚àà Y to represent the canvas and layout , respectively . The canvas I ‚àà R H √ó W √ó 3 and layout L are paired data , where H and W represent the height and width , respectively . We obtain a saliency map S ‚àà R H √ó W √ó 1 by the off - the - shelf saliency detec - tion method [ 46 , 50 ] from the canvas . We denote the lay - out by L = { l 1 , . . . , l T } = { ( c 1 , b 1 ) , . . . , ( c T , b T ) } , where b ‚àà [ 0 , 1 ] 4 indicates the bounding box in normalized coordi - nates , c i ‚àà { 1 , . . . , C } indicates an element category of i - th element , and T indicates the number of elements in L . 3 . 2 . Retrieval - Augmented Layout Transformer We approach content - aware layout generation by referenc - ing similar examples and generating layout tokens ÀÜ Z au - toregressively . Following content - agnostic layout genera - tion works [ 14 , 22 ] , we quantize each value in the bound - ing box of the i - th element b i and obtain representation [ x i , y i , w i , h i ] T ‚àà { 1 , . . . B } 4 , where B denotes the number of bins . Here , x , y , w , and h correspond to the tokens for center coordinates , width , and height of the bounding box . 2 (cid:10)(cid:9)(cid:7)(cid:6)(cid:4)(cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) (cid:19)(cid:5)(cid:2)(cid:1)(cid:7)(cid:14)(cid:3)(cid:13)(cid:2)(cid:7)(cid:14)(cid:11)(cid:5)(cid:2) (cid:10)(cid:9)(cid:7)(cid:6)(cid:3) (cid:27)(cid:7)(cid:24)(cid:11)(cid:3)(cid:2)(cid:1)(cid:23) (cid:31)(cid:30)(cid:29)(cid:3) ) (cid:7)(cid:23)(cid:5)(cid:30) # (cid:0)(cid:3)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) 5(cid:3)(cid:2)(cid:3)(cid:8)(cid:7)(cid:14)(cid:3)(cid:0)2(cid:24)(cid:7)(cid:23)(cid:5)(cid:30)(cid:14) 666 ? (cid:8)(cid:3)(cid:0)(cid:11)(cid:1)(cid:14)(cid:3)(cid:0)2(cid:14)(cid:5)9(cid:3)(cid:2)(cid:29) NOG2(cid:19)(cid:5)(cid:2)(cid:29)(cid:14)(cid:8)(cid:7)(cid:11)(cid:2)(cid:14)2(cid:29)(cid:3)(cid:8)(cid:11)(cid:7)(cid:24)(cid:11)D(cid:7)(cid:14)(cid:11)(cid:5)KN(cid:5)C(cid:14)(cid:11)(cid:5)(cid:2)(cid:7)(cid:24)G (cid:19)(cid:5)(cid:2)(cid:29)(cid:14)W(cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) d 2 (cid:14)(cid:5)C2(cid:5)(cid:2)2 ^ ) (cid:5)(cid:6)(cid:5)2e o(cid:3)m(cid:14)2j N(cid:7)G2z(cid:3)(cid:14)(cid:8)(cid:11)(cid:3)x(cid:7)(cid:24)2(cid:7)(cid:30)(cid:6)(cid:9)(cid:3)(cid:2)(cid:14)(cid:7)(cid:14)(cid:11)(cid:5)(cid:2) ) (cid:7)(cid:23)(cid:5)(cid:30) # (cid:3)(cid:2)(cid:1)(cid:5)(cid:0)(cid:3)(cid:8) z(cid:3)(cid:14)(cid:8)(cid:11)(cid:3)x(cid:3)(cid:0)2(cid:24)(cid:7)(cid:23)(cid:5)(cid:30)(cid:14)(cid:29) ‚Äû(cid:30)(cid:3)(cid:8)(cid:23) ? (cid:5)(cid:29)(cid:14)(cid:3)‚Ä° (cid:0)(cid:7)(cid:14)(cid:7)O(cid:7)(cid:29)(cid:3) Figure 2 . Overview of Retrieval - Augmented Layout Transformer ( RALF ) . RALF takes a canvas image and a saliency map as input , and then autoregressively generates a layout along with the input image . Our model uses ( a ) retrieval augmentation that incorporates useful examples to better capture the relationship between the image and the layout , and ( b ) constraint serialization , an optional module that encodes user - specified requirements , enabling the generation of layouts that adhere to specific requirements for controllable generation . We represent an overall layout as a flattened 1D sequence Z = ( bos , c 1 , x 1 , y 1 , . . . , w T , h T , eos ) ‚àà N 5 T + 2 , where bos and eos are special tokens to denote the start and end of the sequence . We model the joint probability distribution of Z given I and S as a product over a series of conditional distributions using the chain rule : P Œ∏ ( Z | I , S ) = 5 T + 2 (cid:89) t = 2 P Œ∏ ( Z t | Z < t , I , S ) , ( 1 ) where Œ∏ is the parameters of our model . Similarly to au - toregressive language modeling [ 39 ] , the model is trained to maximize the log - likelihood of the next token prediction . Our proposed model consists of four modules : image encoder , retrieval augmentation module , layout decoder , and optional constraint encoder , as illustrated in Fig . 2 . We describe each module below . Image encoder . The image encoder E takes in the input canvas I and the saliency map S , and outputs the feature f I = E ( I , S ) ‚àà R H ‚Ä≤ W ‚Ä≤ √ó d , where H ‚Ä≤ and W ‚Ä≤ represent the down - sampled height and width , and d represents the depth of the feature map . This part is common among content - aware approaches , and we follow the architecture of CGL - GAN [ 53 ] . The encoder builds on a CNN back - bone and a Transformer encoder . The CNN backbone , typ - ically ResNet50 [ 16 ] , uses a multi - scale feature pyramid network [ 32 ] . The Transformer encoder further refines the encoded image feature . Retrieval augmentation module . The augmentation mod - ule transforms the image feature f I into the augmented fea - ture f R . We describe the details in Sec . 3 . 3 . Constraint encoder . Optionally , our model allows control of the layout generation process by additional instruction on desired layout properties such as element types , coordinates , or inter - element relationships . We adopt the Transformer encoder - based model [ 22 ] to encode the instructions into a fixed - dimensional vector f const ‚àà R n √ó d , where n denotes the length of the task - specific sequence . f const is then con - catenated with the augmented feature f R and fed to the layout decoder . Layout decoder . Our model autoregressively generates a layout ÀÜ Z using a Transformer decoder . Starting from the bos token , our decoder iteratively produces output tokens with cross attention to the side feature sequence f R from the retrieval augmentation module and the optional sequence f const from the constraint encoder . A key distinction be - tween our model and previous approaches is that we flatten all the attributes into a single sequence for full attention dur - ing generation , which is shown effective in content - agnostic layout generation [ 14 , 22 ] . As we discuss in Eq . ( 1 ) , we gen - erate layout tokens one by one in 5 T + 1 steps using attribute - wise attention . In contrast , GAN - based models [ 18 , 53 ] gen - erate in one step , and ICVT [ 7 ] generates in T steps using element - wise attention . 3 . 3 . Retrieval Augmentation We introduce retrieval augmentation to effectively learn the structured layout domain with limited training data . The retrieval augmentation module consists of the following three stages : 1 ) retrieving reference layouts from a database , 2 ) encoding these layouts into a feature representation , and 3 ) 3 fusing all features into the final augmented feature f R . We elaborate on the details of these three stages . Layout retrieval . Given the input canvas I , we retrieve a set of useful layout examples { Àú L 1 , . . . , Àú L K } , where K ‚àà N . A challenge lies in the absence of joint embedding for image ‚Äì layout retrieval , unlike the CLIP [ 40 ] embedding for image ‚Äì text retrieval . We hypothesize that given an image ‚Äì layout pair ( Àú I , Àú L ) , Àú L is more likely to be useful when Àú I is similar to I . From a large dataset of image ‚Äì layout pairs , we retrieve top - K pairs based on image similarity between I and Àú I , and extract layouts from these pairs . The choice of the image similarity measure influences the generation quality , as we will discuss in Sec . 4 . 7 in detail . We use DreamSim [ 12 ] , which better aligns with human perception of image simi - larity in diverse aspects such as object appearance , viewing angles , camera poses , and overall layout . All samples from the training split serve as the retrieval source for both training and inference , excluding the query sample from the retrieval source during training to prevent ground - truth leakage . Encoding retrieved layouts . Each retrieved layout { Àú L 1 , . . . , Àú L K } is encoded into representative features Àú f L = { Àú f 1 , . . . , Àú f K } ‚àà R K √ó d , since each layout has a different number of elements . A layout encoder F embeds each re - trieved layout Àú L k into the representative feature , denoted as Àú f k = F ( Àú L k ) ‚àà R d . These extracted features are then concatenated into Àú f L . Following [ 25 ] , we pre - train F in a self - supervised manner and freeze F thereafter . Feature augmentation . The last step yields the final aug - mented feature f R by concatenating three features : f R = Concatenate ( f I , Àú f L , f C ) ‚àà R ( 2 H ‚Ä≤ W ‚Ä≤ + K ) √ó d , ( 2 ) where f C is a cross - attended feature between f I and Àú f L : f C = CrossAttn ( f I , Àú f L ) ‚àà R H ‚Ä≤ W ‚Ä≤ √ó d . In the cross - attention mechanism , the image feature acts as the query , and the retrieved layout feature serves as both the key and value . This design facilitates an interaction between the input canvas and the reference layouts . We then feed the augmented feature f R into the layout generator . We will validate the design of the augmentation module in Sec . 4 . 7 . 4 . Experiments We evaluate our RALF in the unconstrained generation as well as in a variety of constrained generation tasks . 4 . 1 . Datasets We use two publicly available datasets , CGL [ 53 ] and PKU [ 18 ] , which mainly cover e - commerce posters such as cosmetics and clothing . PKU includes three element categories : logo , text , and underlay , and CGL additionally contains embellishment elements . CGL comprises 60 , 548 annotated posters , i . e . , layouts and corresponding images , and 1 , 000 unannotated canvases , i . e . , images only . PKU contains 9 , 974 annotated posters and 905 unannotated can - vases . To obtain canvas ‚Äì layout pairs for the training , previ - ous works [ 18 , 53 ] employ image inpainting to remove the visual elements . However , CGL does not provide inpainted posters , and PKU provides inpainted posters with undesir - able artifacts . We inpaint the posters of both CGL and PKU using a state - of - the - art inpainting technique [ 44 ] . The original datasets do not provide validation and test splits for annotated posters . This limitation prevents fair hyper - parameter tuning , adopting evaluation metrics relying on ground - truth annotations , and the quantitative evaluation of constrained generation tasks since we cannot create con - straints from the annotations . To overcome these issues , we create new dataset splits with a train / val / test ratio of roughly 8 : 1 : 1 . For CGL , we allocate 48 , 544 / 6 , 002 / 6 , 002 annotated posters for train / val / test . For PKU , after exclud - ing posters with more than 11 elements and those with ele - ments occupying less than 0 . 1 % of the canvas , we designate 7 , 735 / 1 , 000 / 1 , 000 posters for train / val / test . Both datasets have a maximum of 10 elements . For the evaluations , we use the annotated and unannotated test splits . 4 . 2 . Evaluation Metrics Inspired by the previous works [ 18 , 53 ] , we employ five metrics that evaluate the layout quality both in terms of graphic and content aspects . Graphic metrics . These metrics evaluate the quality of the generated layouts without considering the canvas . FID ( ‚Üì ) for layout [ 25 , 27 ] has been a primal metric in content - agnostic layout generation , and we adopt this metric in our content - aware scenario . Underlay effectiveness ( Und ‚Üë ) calculates the proportion of valid underlay elements to the total underlay elements . An underlay element is regarded as valid and scores 1 if it entirely covers a non - underlay element ; otherwise , it scores 0 . Overlay ( Ove ‚Üì ) represents the average Intersection over Union of all element pairs , excluding underlay elements . Content metrics . These metrics evaluate whether the gener - ated layouts harmonize with the canvas . Occlusion ( Occ ‚Üì ) computes the average saliency value in the overlapping re - gion between the saliency map S and the layout elements . Readability score ( Rea ‚Üì ) evaluates the non - flatness of text elements by calculating gradients in the image space along both vertical and horizontal axes within these elements . 4 . 3 . Baseline Methods We compare the following methods in the experiments . CGL - GAN [ 53 ] is a non - autoregressive encoder ‚Äì decoder model employing a Transformer architecture . The model takes in the empty or layout constraint to the decoder . 4 Method # Params PKU CGL Content Graphic Content Graphic Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Real Data - 0 . 112 0 . 0102 0 . 99 0 . 0009 1 . 58 0 . 125 0 . 0170 0 . 98 0 . 0002 0 . 79 Top - 1 Retrieval - 0 . 212 0 . 0218 0 . 99 0 . 002 1 . 43 0 . 214 0 . 0266 0 . 99 0 . 0005 0 . 93 CGL - GAN [ 53 ] 41M 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 0 . 157 0 . 0237 0 . 29 0 . 161 66 . 75 DS - GAN [ 18 ] 30M 0 . 142 0 . 0169 0 . 63 0 . 027 11 . 80 0 . 141 0 . 0229 0 . 45 0 . 057 41 . 57 ICVT [ 7 ] 50M 0 . 146 0 . 0185 0 . 49 0 . 318 39 . 13 0 . 124 0 . 0205 0 . 42 0 . 310 65 . 34 LayoutDM ‚Ä† [ 19 ] 43M 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 0 . 127 0 . 0192 0 . 82 0 . 020 2 . 36 Autoreg Baseline 41M 0 . 134 0 . 0164 0 . 43 0 . 019 13 . 59 0 . 125 0 . 0190 0 . 92 0 . 011 2 . 89 RALF ( Ours ) 43M 0 . 119 0 . 0128 0 . 92 0 . 008 3 . 45 0 . 125 0 . 0180 0 . 98 0 . 004 1 . 32 Table 1 . Unconstrained generation results on the PKU and CGL test split . Our RALF outperforms the Autoreg Baseline and achieves the best score on almost all metrics . For reference , we show the Real Data and the Top - 1 Retrieval baselines , which do not have a generator . Method PKU unannotated CGL unannotated Content Graphic Content Graphic Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì CGL - GAN 0 . 191 0 . 0312 0 . 32 0 . 069 0 . 481 0 . 0568 0 . 26 0 . 269 DS - GAN 0 . 180 0 . 0301 0 . 52 0 . 026 0 . 435 0 . 0563 0 . 29 0 . 071 ICVT 0 . 189 0 . 0317 0 . 48 0 . 292 0 . 446 0 . 0425 0 . 67 0 . 301 LayoutDM ‚Ä† 0 . 165 0 . 0285 0 . 38 0 . 201 0 . 421 0 . 0506 0 . 49 0 . 069 Autoreg Baseline 0 . 154 0 . 0274 0 . 35 0 . 022 0 . 384 0 . 0427 0 . 76 0 . 058 RALF ( Ours ) 0 . 133 0 . 0231 0 . 87 0 . 018 0 . 336 0 . 0397 0 . 93 0 . 027 Table 2 . Unconstrained generation results on the PKU and CGL unannotated test split . DS - GAN [ 18 ] is a non - autoregressive model using a CNN - LSTM architecture . DS - GAN is only applicable to the un - constrained task because of the internal sorting algorithm . ICVT [ 7 ] is an autoregressive model that combines a Trans - former with a conditional VAE . LayoutDM ‚Ä† [ 19 ] is a discrete state - space diffusion model that can handle many constrained generation tasks . Since the model is originally designed for content - agnostic layout generation , we extend the model to accept an input image . Autoreg Baseline is the one described in Sec . 3 . 2 and is equivalent to our RALF without retrieval augmentation . RALF is our model described in Sec . 3 . Real Data is the ground truth , which can be considered the upper bound . Since we draw the sample from the test split , we calculate the FID score using the validation split . Top - 1 Retrieval is a nearest - neighbor layout without any generator , which can be considered a retrieval - only baseline . 4 . 4 . Implementation Details We re - implement most of the baselines since there are few official implementations publicly available , except for DS - GAN [ 18 ] . In RALF , we retrieve K = 16 nearest neighbor layouts Ôºé Following CGL - GAN [ 53 ] , the height and width size of the input image are set to 350 and 240 , respectively . We generate layouts on three independent trials and report the average of the metrics . We describe the details of training Method Retrieval Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì CGL - GAN 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 CGL - GAN ‚úì 0 . 144 0 . 0164 0 . 63 0 . 039 13 . 28 LayoutDM ‚Ä† 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 LayoutDM ‚Ä† ‚úì 0 . 123 0 . 0144 0 . 51 0 . 091 10 . 03 Table 3 . Retrieval augmentation for CGL - GAN and LayoutDM ‚Ä† on the PKU test split . and network configuration in the appendix . 4 . 5 . Unconstrained Generation Baseline comparison . Table 1 presents the quantitative results on the annotated test split without user constraints . RALF achieves the best scores , except for the Occ metric of ICVT on CGL . Top - 1 Retrieval , which almost disregards the given content , is unsuitable for the task , as we show deficient performance in content metrics . Table 2 summarizes results on the unannotated test split . RALF achieves the best scores in all the metrics . Compared with Table 1 , all the models exhibit slight performance degra - dation in PKU due to the domain gap problem [ 47 ] between inpainted canvases and clean canvases . We conjecture that the significant performance degradation in CGL comes from non - negligible spatial shifts in subject distributions , which we demonstrate in the appendix . Effectiveness of retrieval augmentation . Tables 1 and 2 demonstrate that retrieval augmentation significantly en - hances the Autoreg Baseline . The only exception is the Occ metric on CGL in Table 1 , where the Autoreg Baseline already closely matches Real Data metrics . Qualitative results . We show the qualitative comparison in Fig . 3 . The results demonstrate that our RALF‚Äôs ability to generate well - fitted , non - overlapping , and rational layouts . In contrast , the baseline methods often produce misaligned underlay embellishments and overlapped text elements as 5 ! LayoutDM CGL - GAN DS - GAN AutoregBaseline RALF ( Ours ) PKU Dataset Logo Text Underlay CGL Dataset Embellishment Logo Text Underlay Figure 3 . Visual comparison of unconstrained generation with baselines . Input canvases are selected from the unannotated split . we indicate by red arrows . We also indicate undesirable elements that appear on a salient region by green arrows . Training dataset size . Here , we show that retrieval augmen - tation is effective regardless of the training dataset size in Fig . 4 . Notably , our RALF trained on just 3 , 000 samples outperforms the Autoreg Baseline trained on the full 7 , 734 samples in PKU . Retrieval size K . We show that retrieval augmentation is not highly sensitive to the number of retrieved layouts 1000 3000 5000 7734 # TrainingDataset 0 20 40 60 80 F I D Autoreg Baseline RALF ( Ours ) Figure 4 . FID over the training dataset size ( # TrainingDataset ) , which has up to 7 , 734 samples . 1 2 4 8 16 # Retrieval 0 4 8 12 F I D Autoreg Baseline RALF ( Ours ) Figure 5 . FID over the retrieval size K ( # Retrieval ) . Input image Output 1 Output 2 Retrieved example ( s ) ùêæ = 16 ùêæ = 1 Figure 6 . Visual comparison of retrieval and generated layouts with different retrieval sizes ( K = 1 and 16 ) . We display the top - 4 examples for K = 16 due to the limited space . The output layouts are generated using different random seeds for variety . Train Test Method Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì CGL PKU Autoreg Baseline 0 . 176 0 . 0276 0 . 84 0 . 037 RALF ( Ours ) 0 . 144 0 . 0249 0 . 96 0 . 023 PKU CGL Autoreg Baseline 0 . 341 0 . 0464 0 . 29 0 . 037 RALF ( Ours ) 0 . 286 0 . 0355 0 . 79 0 . 036 Table 4 . Generation across the unannotated test splits . We train a model on PKU and then test it on CGL , or vice versa . K . As we plot in Fig . 5 , retrieval augmentation significantly enhances the performance even with a single retrieved layout compared to the baseline . The plot indicates FID moderately gets better as we increase the retrieval size K . We examine how different K affects the generated results in Fig . 6 . The result of K = 1 shows that the generated layout is similar to the reference layouts , while the result of K = 16 shows that a variety of layouts are generated . Retrieval augmentation for other generators . While our RALF is an autoregressive generator , we show that retrieval augmentation also benefits other generative mod - els for content - aware layout generation . Here , we adapt 6 C ‚Üí S + P ‚Äú Underlay , Logo , Text , Text‚Äù C + S ‚Üí P ‚ÄúLogo , Text , ‚Ä¶ , with size‚Äù Completion ‚ÄúLogo with size and position‚Äù Re0inement ‚ÄúPerturbed layout‚Äù Relationship ‚ÄúLogo top on Text‚Äù Input Figure 7 . Examples of input con - straints and generated results for each constrained generation task . Quota - tion marks indicate the constraints . Method PKU CGL Content Graphic Content Graphic Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì C ‚Üí S + P CGL - GAN 0 . 132 0 . 0158 0 . 48 0 . 038 11 . 47 0 . 140 0 . 0213 0 . 65 0 . 047 23 . 93 LayoutDM ‚Ä† 0 . 152 0 . 0201 0 . 46 0 . 172 20 . 56 0 . 127 0 . 0192 0 . 79 0 . 026 3 . 39 Autoreg Baseline 0 . 135 0 . 0167 0 . 43 0 . 028 10 . 48 0 . 124 0 . 0188 0 . 89 0 . 015 1 . 36 RALF ( Ours ) 0 . 124 0 . 0138 0 . 90 0 . 010 2 . 21 0 . 126 0 . 0180 0 . 97 0 . 006 0 . 50 C + S ‚Üí P CGL - GAN 0 . 129 0 . 0155 0 . 48 0 . 043 9 . 11 0 . 129 0 . 0202 0 . 75 0 . 027 6 . 96 LayoutDM ‚Ä† 0 . 143 0 . 0185 0 . 45 0 . 122 24 . 90 0 . 127 0 . 0190 0 . 82 0 . 021 2 . 18 Autoreg Baseline 0 . 137 0 . 0169 0 . 46 0 . 028 5 . 46 0 . 127 0 . 0191 0 . 88 0 . 013 0 . 47 RALF ( Ours ) 0 . 125 0 . 0138 0 . 87 0 . 010 0 . 62 0 . 128 0 . 0185 0 . 96 0 . 006 0 . 21 Completion CGL - GAN 0 . 150 0 . 0174 0 . 43 0 . 061 25 . 67 0 . 174 0 . 0231 0 . 21 0 . 182 78 . 44 LayoutDM ‚Ä† 0 . 135 0 . 0175 0 . 35 0 . 134 21 . 70 0 . 127 0 . 0192 0 . 76 0 . 020 3 . 19 Autoreg Baseline 0 . 125 0 . 0161 0 . 42 0 . 023 5 . 96 0 . 124 0 . 0185 0 . 91 0 . 011 2 . 33 RALF ( Ours ) 0 . 120 0 . 0140 0 . 88 0 . 012 1 . 58 0 . 126 0 . 0185 0 . 96 0 . 005 1 . 04 Refinement CGL - GAN 0 . 122 0 . 0141 0 . 39 0 . 090 6 . 40 0 . 124 0 . 0182 0 . 86 0 . 024 1 . 20 LayoutDM ‚Ä† 0 . 115 0 . 0121 0 . 57 0 . 008 2 . 86 0 . 127 0 . 0188 0 . 75 0 . 018 1 . 98 Autoreg Baseline 0 . 131 0 . 0171 0 . 41 0 . 026 5 . 89 0 . 126 0 . 0183 0 . 89 0 . 004 0 . 15 RALF ( Ours ) 0 . 113 0 . 0109 0 . 95 0 . 004 0 . 13 0 . 126 0 . 0176 0 . 98 0 . 002 0 . 14 Relationship Autoreg Baseline 0 . 140 0 . 0177 0 . 44 0 . 028 10 . 61 0 . 127 0 . 0189 0 . 88 0 . 015 1 . 28 RALF ( Ours ) 0 . 122 0 . 0141 0 . 85 0 . 009 2 . 23 0 . 126 0 . 0184 0 . 95 0 . 006 0 . 55 Table 5 . Quantitative result of six constrained generation tasks on the PKU and CGL test split . CGL - GAN and LayoutDM ‚Ä† with retrieval augmentation and evaluate the performance . Table 3 summarizes the results . CGL - GAN and LayoutDM ‚Ä† combined with our retrieval augmentation consistently improve many evaluation metrics . We provide additional results in the appendix . Out - of - domain generalization . Table 4 summarizes the results of a cross - evaluation setup where we use different datasets for training and testing . For example , we use the database and training data from CGL and evaluate PKU in the upper half of Table 4 . Remarkably , even in this out - of - domain setting , retrieval augmentation shows notable im - provement and robust generalizability . 4 . 6 . Constrained Generation Following the task setup of content - agnostic generation [ 22 ] , we evaluate several methods in the following constrained tasks in content - aware generation : Category ‚Üí Size + Position ( C ‚Üí S + P ) takes in element types and generates the sizes and positions for each element . Category + Size ‚Üí Position ( C + S ‚Üí P ) generates element positions based on given element categories and sizes . Completion generates a complete layout using partially placed elements . Refinement corrects cluttered layouts where elements are per - turbed from the ground truth based on a normal distribution with mean 0 and standard deviation 0 . 01 , following [ 41 ] . Relationship is conditioned on both element types and their spatial relationships , determined by the size and position of element pairs . We randomly use 10 % of these relationships in our experiments , following [ 25 ] . Input constraints and generated examples for these tasks are illustrated in Fig . 7 . Baseline comparison . Table 5 summarizes constrained gen - eration results . The results indicate that RALF is effective even for constrained generation tasks . For tasks such as C + S ‚Üí P and Refinement , RALF shows notable improvement in the FID metric . This suggests that referencing authen - tic examples to understand element relationships enhances position prediction accuracy . Overall , the results highlight RALF‚Äôs capability to significantly augment the generative performance over the baseline approach . 4 . 7 . Ablation Study We investigate our design choices in our retrieval augmenta - tion proposed in Sec . 3 . 3 . Layout retrieval . We employ an image feature extractor 7 0 . 013 0 . 014 Readability score 3 . 5 4 . 0 4 . 5 F I D DreamSim LPIPS CLIP Saliency Random Figure 8 . Comparison across different retrieval methods on the PKU test split . We report FID as the representative graphic metric and Readability score as the content metric . to compute the similarity between canvases . We provide a brief overview of possible choices . DreamSim [ 12 ] captures diverse aspects of the similarity simultaneously . LPIPS [ 51 ] focuses on low - level appearance similarity . CLIP [ 40 ] fo - cuses on semantic similarity . Saliency focuses on spatial similarity using the saliency map . We obtain embeddings for similarity computation by down - sampling and flattening S . Random serves as a na ¬® ƒ±ve baseline by randomly sampling layouts without focusing on image similarity . We train our RALF with each choice and assess the per - formance . Figure 8 plots FID and Readability score for each retrieval method , and Fig . 9 presents some retrieved exam - ples . DreamSim shows the best balance in the graphic and content metrics . We conjecture that retrieving images based on both semantic and spatial similarity is important . Feature augmentation . We explore the design of our feature augmentation module , as detailed in Table 6 . What types of features to fuse ? RALF combines three fea - tures in Eq . ( 2 ) . We observe that dropping some of the features , as in scenarios ( B ) and ( C ) , leads to a slight dete - rioration of the performance . We try adding features of the top - K retrieved images Àú f I ‚àà R KH ‚Ä≤ W ‚Ä≤ √ó d that are encoded by the image encoder from the retrieved canvas . However , adding Àú f I results in decreased performance , as shown in ( D ) . Where to apply ? Our model first applies the Transformer encoder and then retrieval augmentation to the image fea - ture ( A ) . We try another design ( E ) , which places the augmen - tation module before the Transformer encoder , however , this results in worse readability and underlay metrics in exchange for the slight improvement in FID . 5 . Discussion Limitations . We acknowledge two limitations as follows : 1 ) Evaluation of content metrics : The current content metrics assume that well - designed layouts avoid placing elements over salient or cluttered areas . If a counterexample exists , the content metrics may not adequately measure layout quality . Also , the graphic metrics can be easily fooled by a real example , as evidenced by the FID score of the Top - 1 baseline in Table 1 . 2 ) Feature extraction of retrieved layouts : The DreamSim LPIPS CLIP Query Saliency Figure 9 . Qualitative comparison of different retrieval methods . We show the query and the top - 3 retrieved examples for each method as well as saliency maps . Setting Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì A Ours ( Concatenate ( f I , Àú f L , f C ) ) 0 . 119 0 . 0129 0 . 92 0 . 008 3 . 45 What types of features to fuse ? B Concatenate ( f C , ) 0 . 134 0 . 0144 0 . 92 0 . 008 4 . 67 C Concatenate ( f I , Àú f L ) 0 . 123 0 . 0133 0 . 91 0 . 007 4 . 08 D Concatenate ( f I , Àú f L , f C , Àú f I ) 0 . 141 0 . 0148 0 . 93 0 . 009 8 . 82 Where to apply ? E Before Trans enc 0 . 120 0 . 0138 0 . 72 0 . 009 2 . 34 Table 6 . Ablation study of RALF design on the PKU test split . The top two results are highlighted in bold and underline , respectively . Features include the input canvas feature ( f I ) , retrieved layouts feature ( Àú f L ) , cross - attended feature ( f C ) , and retrieved images feature ( Àú f I ) . The full setting of our model ( A ) is described in Eq . ( 2 ) . layout encoder depends on the number of element categories in the dataset . For real - world creative scenarios , extending to an unlimited number of categories , i . e . an open - vocabulary setting [ 11 ] , would be necessary . Future work . We outline two prospective directions to enhance retrieval augmentation for content - aware genera - tion further : 1 ) Ensemble approaches : integrating multiple retrieval results could potentially improve the generation quality . 2 ) Diversifying retrieval modalities : exploring lay - out retrieval using alternative modalities , such as language , could widen the application scope . Yet , generating a whole poster beyond bounding boxes , such as image content , text copies , or styling attributes , remains challenging due to the limited training data for layered graphic designs . Even for such a task , we expect that the retrieval augmentation ap - proach could alleviate the data scarcity problem . Potential societal impacts . As common in any generative models , our RALF may unintentionally produce counterfeit advertisements or magazine layouts , posing risks of decep - tion and dissemination of misleading information . 8 References [ 1 ] Adobe Illustrator CC . https : / / www . adobe . com / products / illustrator . html , Last accessed 17 November , 2023 . 1 [ 2 ] Maneesh Agrawala , Wilmot Li , and Floraine Berthouzoz . De - sign Principles for Visual Communication . Communications of the ACM , 54 ( 4 ) , 2011 . 2 [ 3 ] Diego Martin Arroyo , Janis Postels , and Federico Tombari . Variational Transformer Networks for Layout Generation . In CVPR , 2021 . 2 [ 4 ] Akari Asai , Sewon Min , Zexuan Zhong , and Danqi Chen . ACL 2023 Tutorial : Retrieval - based Language Models and Applications . ACL , 2023 . 2 [ 5 ] Andreas Blattmann , Robin Rombach , Kaan Oktay , and Bj¬®orn Ommer . Retrieval - Augmented Diffusion Models . In NeurIPS , 2022 . 1 , 2 , 13 [ 6 ] Sebastian Borgeaud , Arthur Mensch , Jordan Hoffmann , Trevor Cai , Eliza Rutherford , Katie Millican , George van den Driessche , Jean - Baptiste Lespiau , Bogdan Damoc , Aidan Clark , Diego de Las Casas , Aurelia Guy , Jacob Menick , Ro - man Ring , Tom Hennigan , Saffron Huang , Loren Maggiore , Chris Jones , Albin Cassirer , Andy Brock , Michela Paganini , Geoffrey Irving , Oriol Vinyals , Simon Osindero , Karen Si - monyan , Jack W . Rae , Erich Elsen , and Laurent Sifre . Improv - ing language models by retrieving from trillions of tokens . arXiv preprint arXiv : 2112 . 04426 , 2021 . 1 , 2 [ 7 ] Yunning Cao , Ye Ma , Min Zhou , Chuanbin Liu , Hongtao Xie , Tiezheng Ge , and Yuning Jiang . Geometry Aligned Varia - tional Transformer for Image - conditioned Layout Generation . In ACM MM , 2022 . 2 , 3 , 5 , 14 [ 8 ] Nicolas Carion , Francisco Massa , Gabriel Synnaeve , Nicolas Usunier , Alexander Kirillov , and Sergey Zagoruyko . End - to - End Object Detection with Transformers . In ECCV , 2020 . 2 [ 9 ] Shang Chai , Liansheng Zhuang , and Fengying Yan . Lay - outDM : Transformer - based Diffusion Model for Layout Gen - eration . In CVPR , 2023 . 2 [ 10 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . BERT : Pre - training of Deep Bidirectional Trans - formers for Language Understanding . In NAACL , 2019 . 2 [ 11 ] Weixi Feng , Wanrong Zhu , Tsu - jui Fu , Varun Jampani , Ar - jun Akula , Xuehai He , Sugato Basu , Xin Eric Wang , and William Yang Wang . LayoutGPT : Compositional Visual Planning and Generation with Large Language Models . In NeurIPS , 2023 . 8 [ 12 ] Stephanie Fu * , Netanel Tamir * , Shobhita Sundaram * , Lucy Chai , Richard Zhang , Tali Dekel , and Phillip Isola . Dream - Sim : Learning New Dimensions of Human Visual Similarity using Synthetic Data . In NeurIPS , 2023 . 4 , 8 [ 13 ] Shunan Guo , Zhuochen Jin , Fuling Sun , Jingwen Li , Zhaorui Li , Yang Shi , and Nan Cao . Vinci : An Intelligent Graphic Design System for Generating Advertising Posters . In CHI , 2021 . 1 [ 14 ] Kamal Gupta , Alessandro Achille , Justin Lazarow , Larry Davis , Vijay Mahadevan , and Abhinav Shrivastava . Lay - outTransformer : Layout Generation and Completion with Self - attention . In ICCV , 2021 . 2 , 3 [ 15 ] Kelvin Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang . REALM : Retrieval - Augmented Language Model Pre - Training . In ICML , 2020 . 1 , 2 [ 16 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep Residual Learning for Image Recognition . In CVPR , 2016 . 3 [ 17 ] Scarlett R . Herring , Chia - Chen Chang , Jesse Krantzler , and Brian P . Bailey . Getting Inspired ! Understanding How and Why Examples Are Used in Creative Design Practice . In CHI , 2009 . 1 [ 18 ] Hsiao Yuan Hsu , Xiangteng He , Yuxin Peng , Hao Kong , and Qing Zhang . PosterLayout : A New Benchmark and Approach for Content - Aware Visual - Textual Presentation Layout . In CVPR , 2023 . 1 , 2 , 3 , 4 , 5 , 11 , 12 , 14 [ 19 ] Naoto Inoue , Kotaro Kikuchi , Edgar Simo - Serra , Mayu Otani , and Kota Yamaguchi . LayoutDM : Discrete Diffusion Model for Controllable Layout Generation . In CVPR , 2023 . 1 , 2 , 5 , 14 [ 20 ] Ali Jahanian , Jerry Liu , Qian Lin , Daniel Tretter , Eamonn O‚ÄôBrien - Strain , Seungyon Claire Lee , Nic Lyons , and Jan Allebach . Recommendation System for Automatic Design of Magazine Covers . In IUI , 2013 . 1 [ 21 ] Zhaoyun Jiang , Shizhao Sun , Jihua Zhu , Jian - Guang Lou , and Dongmei Zhang . Coarse - to - fine generative modeling for graphic layouts . In AAAI , 2022 . 2 [ 22 ] Z . Jiang , J . Guo , S . Sun , H . Deng , Z . Wu , V . Mijovic , Z . Yang , J . Lou , and D . Zhang . LayoutFormer + + : Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction . In CVPR , 2023 . 2 , 3 , 7 , 11 [ 23 ] Jeff Johnson , Matthijs Douze , and Herv¬¥e J¬¥egou . Billion - scale similarity search with GPUs . IEEE Transactions on Big Data , 7 ( 3 ) : 535 ‚Äì 547 , 2019 . 11 [ 24 ] Akash Abdu Jyothi , Thibaut Durand , Jiawei He , Leonid Si - gal , and Greg Mori . LayoutVAE : Stochastic Scene Layout Generation from a Label Set . In CVPR , 2019 . 2 [ 25 ] Kotaro Kikuchi , Edgar Simo - Serra , Mayu Otani , and Kota Ya - maguchi . Constrained Graphic Layout Generation via Latent Optimization . In ACM MM , 2021 . 2 , 4 , 7 , 12 [ 26 ] Xiang Kong , Lu Jiang , Huiwen Chang , Han Zhang , Yuan Hao , Haifeng Gong , and Irfan Essa . BLT : Bidirectional Layout Transformer for Controllable Layout Generation . In ECCV , 2022 . 2 [ 27 ] Hsin - Ying Lee , Weilong Yang , Lu Jiang , Madison Le , Irfan Essa , Haifeng Gong , and Ming - Hsuan Yang . Neural Design Network : Graphic Layout Generation with Constraints . In ECCV , 2019 . 4 [ 28 ] Elad Levi , Eli Brosh , Mykola Mykhailych , and Meir Perez . DLT : Conditioned Layout Generation with Joint Discrete - Continuous Diffusion Layout Transformer . In ICCV , 2023 . 2 [ 29 ] Fengheng Li , An Liu , Wei Feng , Honghe Zhu , Yaoyu Li , Zheng Zhang , Jingjing Lv , Xin Zhu , Junjie Shen , Zhangang Lin , and Jingping Shao . Relation - Aware Diffusion Model for Controllable Poster Layout Generation . In CIKM , 2023 . 2 [ 30 ] Jianan Li , Jimei Yang , Aaron Hertzmann , Jianming Zhang , and Tingfa Xu . LayoutGAN : Generating Graphic Layouts with Wireframe Discriminators . In ICLR , 2019 . 2 9 [ 31 ] Jianan Li , Jimei Yang , Jianming Zhang , Chang Liu , Christina Wang , and Tingfa Xu . Attribute - Conditioned Layout GAN for Automatic Graphic Design . IEEE TVCG , 27 ( 10 ) , 2021 . 2 , 12 [ 32 ] Tsung - Yi Lin , Piotr Doll¬¥ar , Ross Girshick , Kaiming He , Bharath Hariharan , and Serge Belongie . Feature Pyramid Networks for Object Detection . In CVPR , 2017 . 3 , 11 [ 33 ] Simon Lok and Steven Feiner . A Survey of Automated Layout Techniques for Information Presentations . In SmartGraphics , 2001 . 2 [ 34 ] Ilya Loshchilov and Frank Hutter . Fixing Weight Decay Regularization in Adam . In ICLR , 2019 . 11 [ 35 ] Muhammad Ferjad Naeem , Seong Joon Oh , Youngjung Uh , Yunjey Choi , and Jaejun Yoo . Reliable Fidelity and Diversity Metrics for Generative Models . In ICML , 2020 . 13 [ 36 ] Peter O‚ÄôDonovan , Aseem Agarwala , and Aaron Hertzmann . DesignScape : Design with Interactive Layout Suggestions . In CHI , 2015 . 1 , 2 [ 37 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , An - dreas K¬®opf , Edward Z . Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . PyTorch : An Im - perative Style , High - Performance Deep Learning Library . In NeurIPS , 2019 . 11 [ 38 ] Chunyao Qian , Shizhao Sun , Weiwei Cui , Jian - Guang Lou , Haidong Zhang , and Dongmei Zhang . Retrieve - Then - Adapt : Example - based Automatic Generation for Proportion - related Infographics . IEEE TVCG , 27 ( 2 ) , 2021 . 2 [ 39 ] Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , et al . Language models are unsuper - vised multitask learners . OpenAI blog , 1 ( 8 ) : 9 , 2019 . 3 [ 40 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , Gretchen Krueger , and Ilya Sutskever . Learning Transferable Visual Models From Natural Language Supervision . In ICML , 2021 . 4 , 8 [ 41 ] Soliha Rahman , Vinoth Pandian Sermuga Pandian , and Matthias Jarke . RUITE : Refining UI Layout Aesthetics Using Transformer Encoder . In IUI Companion , 2021 . 7 [ 42 ] Shelly Sheynin , Oron Ashual , Adam Polyak , Uriel Singer , Oran Gafni , Eliya Nachmani , and Yaniv Taigman . KNN - Diffusion : Image Generation via Large - Scale Retrieval . In ICLR , 2023 . 1 , 2 [ 43 ] Karen Simonyan and Andrew Zisserman . Very Deep Con - volutional Networks for Large - Scale Image Recognition . In ICLR , 2015 . 13 [ 44 ] Roman Suvorov , Elizaveta Logacheva , Anton Mashikhin , Anastasia Remizova , Arsenii Ashukha , Aleksei Silvestrov , Naejin Kong , Harshith Goka , Kiwoong Park , and Victor Lem - pitsky . Resolution - robust Large Mask Inpainting with Fourier Convolutions . In WACV , 2022 . 4 [ 45 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkor - eit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . Attention Is All You Need . In NeurIPS , 2017 . 2 [ 46 ] Bo Wang , Quan Chen , Min Zhou , Zhiqiang Zhang , Xiaogang Jin , and Kun Gai . Progressive Feature Polishing Network for Salient Object Detection . In AAAI , 2020 . 2 [ 47 ] Chenchen Xu , Min Zhou , Tiezheng Ge , Yuning Jiang , and Weiwei Xu . Unsupervised Domain Adaption With Pixel - Level Discriminator for Image - Aware Layout Generation . In CVPR , 2023 . 5 [ 48 ] Xuyong Yang , Tao Mei , Ying - Qing Xu , Yong Rui , and Shipeng Li . Automatic Generation of Visual - Textual Pre - sentation Layout . ACM TOMM , 12 ( 2 ) , 2016 . 1 , 2 [ 49 ] Junyi Zhang , Jiaqi Guo , Shizhao Sun , Jian - Guang Lou , and Dongmei Zhang . LayoutDiffusion : Improving Graphic Lay - out Generation by Discrete Diffusion Probabilistic Models . In ICCV , 2023 . 2 [ 50 ] Peiying Zhang , Chenhui Li , and Changbo Wang . Smarttext : Learning To Generate Harmonious Textual Layout Over Nat - ural Image . In ICME , 2020 . 2 [ 51 ] Richard Zhang , Phillip Isola , Alexei A Efros , Eli Shechtman , and Oliver Wang . The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , 2018 . 8 [ 52 ] Xinru Zheng , Xiaotian Qiao , Ying Cao , and Rynson W . H . Lau . Content - Aware Generative Modeling of Graphic Design Layouts . ACM TOG , 38 ( 4 ) , 2019 . 1 , 2 [ 53 ] Min Zhou , Chenchen Xu , Ye Ma , Tiezheng Ge , Yuning Jiang , and Weiwei Xu . Composition - aware Graphic Layout GAN for Visual - textual Presentation Designs . In IJCAI , 2022 . 1 , 2 , 3 , 4 , 5 , 13 , 14 10 Appendix Table of contents : ‚Ä¢ Section A : Code Availability ‚Ä¢ Section B : Implementation Details ‚Ä¢ Section C : Dataset Preprocessing ‚Ä¢ Section D : Additional Results A . Code Availability We will make our code publicly available on acceptance . B . Implementation Details Architecture details . Our RALF consists of four modules : the image encoder , retrieval augmentation , layout decoder , and optional constraint encoder . Table A provides the num - ber of parameters of these modules . Image encoder consists of ResNet - 50 - FPN [ 32 ] and the Transformer encoder . We obtain the saliency map following the approach in DS - GAN [ 18 ] . Retrieval augmentation . We implement the retrieval part using faiss [ 23 ] . The layout encoder for retrieved layouts consists of the Transformer encoder and a feed - forward net - work , which adapts the feature map size of retrieved layouts to the size of the layout decoder . Before training , we pre - train the layout encoder for each dataset and extract features over each training dataset to construct the retrieval database . We note that the parameters of the layout encoder ( 1 . 59M ) are excluded from the total parameters of RALF , since they are set with the retrieval database . Layout decoder . We employ the Transformer decoder . The configurations of the Transformer layers are as follows : 6 layers , 8 attention heads , 256 embedding dimensions , 1 , 024 hidden dimensions , and 0 . 1 dropout rate . The size of bins for the layout tokenizer is set to 128 . In the inference phase , for the relationship task , we use a decoding space restriction mechanism [ 22 ] , which aims to prune the predicted tokens that violate a user - specified constraint . Training details . We implemente RALF in PyTorch [ 37 ] and train for 50 and 70 epochs with AdamW optimizer [ 34 ] for the PKU and CGL datasets , respectively . The training time is about 4 hours and 20 minutes for the PKU dataset and 18 hours for the CGL dataset on a single A100 GPU . We divide the learning rate by 10 after 70 % of the total epoch elapsed . We set the batch size , learning rate , weight decay , and gradient norm to 32 , 10 ‚àí 4 , 10 ‚àí 4 , and 10 ‚àí 1 , respectively . Testing details . We generate layouts on three independent trials and report the average of the metrics . We use top - k sampling for all the models that rely on sampling in logit space . We set k and temperature to 5 and 1 . 0 , respectively . Other baselines . For the training of baseline methods , we follow the original training setting referring to their papers Module # Params Image encoder ( ResNet50 ) 25 . 02 M Image encoder ( Trans Enc ) 4 . 74 M Constraint encoder 4 . 88 M Retrieval augmentation 1 . 59 M Layout decoder 6 . 59 M Total 42 . 82 M Table A . The number of parameters of each module . ( b ) Original poster w / layout ( c ) Original Inpaintingresults ( d ) Inpainting results ( Ours ) ( a ) Originalposter Overview Text region ( zoomed ) Figure A . Comparison of inpainting for the dataset preprocessing . as much as possible . There are some exceptions for a fair comparison . For example , the number of embedding dimen - sions and hidden dimensions in Transformer is adjusted to roughly match the number of parameters for each model . We use ResNet - 50 - FPN as the image encoder for all of our baseline methods . C . Dataset Preprocessing We demonstrate the importance of adequately preprocessing annotated poster images in Fig . A . Layout annotations in existing datasets sometimes exhibit inaccuracies for some underlying factors , including the semi - automatic collection process using object detection models [ 18 ] as shown in ( a ) 11 ( b ) Original poster w / layout ( c ) Original Inpaintingresults ( d ) Inpainting results ( Ours ) ( a ) Originalposter Figure B . Comparison of inpainting for the dataset preprocessing . 0 5 10 15 # Elements per layout 0 500 1000 1500 2000 # S a m p l e s Figure C . Number of elements per layout in the original PKU dataset . A red dashed line indicates the maximum number of elements we use . and ( b ) . The inaccuracy severely harms the image inpainting quality when we fully depend on the annotations , as shown in ( c ) . To cope with the inaccuracy , we slightly dilate the target region for inpainting and get better results with fewer artifacts , as shown in ( d ) . We show more examples in Fig . B . We observe that about 20 % of the original inpainted images in PKU contain significant artifacts . We plot the number of layout elements for each poster in Fig . C . Although we filter out posters with more than 11 layout elements , it only accounts for about 2 % of the original dataset . Test split Unannotated test split Averaged saliency map Averaged saliency map Mean = 0 . 375 Mean = 0 . 458 Figure D . Visual comparison of canvases and saliency maps be - tween the test and unannotated test split of the CGL dataset . Can - vases are randomly selected from each split . The averaged saliency map is produced by computing the spatial average of all saliency maps of each split . Mean represents the spatial average of all saliency maps of each split . D . Additional Results Spatial distribution shift . Figure D shows the visual com - parison of canvases and saliency maps between the test and unannotated test split of CGL . We see that the proportion of space occupied by the saliency map is different according to the different values of Mean . As a result , this difference causes the performance degradation in CGL . Comprehensive quantitative comparison . We additionally adopt five metrics . Graphic metrics . Alignment ( Align ‚Üì ) [ 25 , 31 ] computes how well the elements are aligned with each other . For de - tailed calculation , please refer to [ 25 , 31 ] . Loose underlay effectiveness ( Und L ‚Üë ) [ 18 ] also calculates the proportion of the total area of valid underlay elements to the total of un - derlay and non - underlay elements . Note that we define this loose metric as Und L ‚Üë to distinguish it from the strict under - 12 lay effectiveness Und S ‚Üë introduced in the main manuscript . Density ( Den ‚Üë ) and Coverage ( Cov ‚Üë ) [ 35 ] compute fidelity and diversity aspects of the generated layouts against ground - truth layouts . Please refer to [ 35 ] for more details . Content metrics . Salient consistency ( R shm ‚Üì ) [ 53 ] com - putes the Euclidean distance between the output logits of the canvases with or without layout regions masked using a pre - trained VGG16 [ 43 ] . Tables B and C present the quantitative result on the annotated test split without user constraints on the PKU and CGL datasets , respectively . RALF notably improves Density and Coverage metrics , indicating that RALF can generate better layouts in terms of both fidelity and diversity . RALF does not achieve the best score regarding R shm and Alignment . However , these metrics may not be very reliable since the best scores for these metrics largely deviate from the scores for Real - Data , unlike other metrics . Retrieval augmentation for baseline method . Table D shows the results of retrieval augmentation for CGL - GAN and LayoutDM ‚Ä† . Even for constrained generation tasks , retrieval augmentation achieves a better quality of generation for other generators on almost all metrics . Impact on changing # Dim in layout decoder . Table E provides the results of RALF and Autoreg Baseline while changing the number of parameters in the layout decoder . We modify the number of features ( # Dim ) and hidden dim to four times the number of # Dim . RALF‚Äôs performance peaks when # Dim is 256 . Autoreg Baseline‚Äôs performance improves as # Dim increases , but the model with # Dim = 768 still clearly underperforms RALF with # Dim = 256 . Thus , retrieval augmentation enables us to use a relatively compact network for content - aware layout generation . This result aligns with the trend observed in other domains , such as im - age generation [ 5 ] . We conjecture slight performance degra - dation as we increase # Dim over 256 in RALF is caused by overfitting as we watch loss curves for training and valida - tion . Visual comparison on constrained generation . Figures E and F provide the qualitative comparisons of constrained generation for the PKU and CGL datasets , respectively . The results demonstrate that our RALF successfully generates well - fitted , non - overlapping , and rational layouts even in constrained generation tasks . 13 Method PKU Content Graphic Occ ‚Üì Rea ‚Üì R shm ‚Üì Align ‚Üì Und L ‚Üë Und S ‚Üë Ove ‚Üì Den ‚Üë Cov ‚Üë FID ‚Üì Real - Data 0 . 112 0 . 0102 13 . 94 0 . 00379 0 . 99 0 . 99 0 . 0009 0 . 95 0 . 95 1 . 58 Top1 - Retrieval 0 . 212 0 . 0218 16 . 33 0 . 00371 0 . 99 0 . 99 0 . 002 1 . 07 0 . 97 1 . 43 CGL - GAN [ 53 ] 0 . 138 0 . 0164 14 . 32 0 . 00311 0 . 81 0 . 41 0 . 074 0 . 70 0 . 68 34 . 51 DS - GAN [ 18 ] 0 . 142 0 . 0169 14 . 95 0 . 00347 0 . 89 0 . 63 0 . 027 1 . 10 0 . 82 11 . 80 ICVT [ 7 ] 0 . 146 0 . 0185 13 . 92 0 . 00228 0 . 63 0 . 49 0 . 318 0 . 35 0 . 40 39 . 13 LayoutDM ‚Ä† [ 19 ] 0 . 150 0 . 0192 13 . 06 0 . 00298 0 . 64 0 . 41 0 . 190 0 . 74 0 . 59 27 . 09 Autoreg Baseline 0 . 134 0 . 0164 14 . 43 0 . 00192 0 . 79 0 . 43 0 . 019 1 . 13 0 . 79 13 . 59 RALF ( Ours ) 0 . 119 0 . 0129 14 . 11 0 . 00267 0 . 98 0 . 92 0 . 008 1 . 25 0 . 97 3 . 45 Table B . Unconstrained generation results on the PKU test split . Method CGL Content Graphic Occ ‚Üì Rea ‚Üì R shm ‚Üì Align ‚Üì Und L ‚Üë Und S ‚Üë Ove ‚Üì Den ‚Üë Cov ‚Üë FID ‚Üì Real - Data 0 . 125 0 . 0170 14 . 33 0 . 00240 0 . 99 0 . 98 0 . 0002 0 . 93 1 . 00 0 . 79 Top1 - Retrieval 0 . 214 0 . 0266 16 . 02 0 . 00254 0 . 99 0 . 99 0 . 0005 1 . 01 0 . 90 0 . 93 CGL - GAN [ 53 ] 0 . 157 0 . 0237 14 . 12 0 . 00320 0 . 67 0 . 29 0 . 161 0 . 31 0 . 28 66 . 75 DS - GAN [ 18 ] 0 . 141 0 . 0229 14 . 85 0 . 00257 0 . 71 0 . 45 0 . 057 0 . 64 0 . 40 41 . 57 ICVT [ 7 ] 0 . 124 0 . 0205 13 . 40 0 . 00319 0 . 55 0 . 42 0 . 310 0 . 16 0 . 22 65 . 34 LayoutDM ‚Ä† [ 19 ] 0 . 127 0 . 0192 14 . 15 0 . 00242 0 . 92 0 . 82 0 . 020 0 . 87 0 . 93 2 . 36 Autoreg Baseline 0 . 125 0 . 0190 14 . 22 0 . 00234 0 . 97 0 . 92 0 . 011 1 . 05 0 . 91 2 . 89 RALF ( Ours ) 0 . 125 0 . 0180 14 . 26 0 . 00236 0 . 99 0 . 98 0 . 004 1 . 09 0 . 96 1 . 32 Table C . Unconstrained generation results on the CGL test split . 14 Task Method Retrieval PKU CGL Content Graphic Content Graphic Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Unconstraint Real Data 0 . 112 0 . 0102 0 . 99 0 . 0009 1 . 58 0 . 125 0 . 0170 0 . 98 0 . 0002 0 . 79 Top - 1 Retrieval 0 . 212 0 . 0218 0 . 99 0 . 002 1 . 43 0 . 214 0 . 0266 0 . 99 0 . 0005 0 . 93 CGL - GAN 0 . 138 0 . 0164 0 . 41 0 . 074 34 . 51 0 . 157 0 . 0237 0 . 29 0 . 161 66 . 75 CGL - GAN ‚úì 0 . 144 0 . 0164 0 . 63 0 . 039 13 . 28 0 . 172 0 . 0245 0 . 42 0 . 157 60 . 67 LayoutDM ‚Ä† 0 . 150 0 . 0192 0 . 41 0 . 190 27 . 09 0 . 127 0 . 0192 0 . 82 0 . 020 2 . 36 LayoutDM ‚Ä† ‚úì 0 . 123 0 . 0144 0 . 51 0 . 091 10 . 03 0 . 126 0 . 0187 0 . 85 0 . 019 1 . 97 C ‚Üí S + P CGL - GAN 0 . 132 0 . 0158 0 . 48 0 . 038 11 . 47 0 . 140 0 . 0213 0 . 65 0 . 047 23 . 93 CGL - GAN ‚úì 0 . 140 0 . 0153 0 . 66 0 . 030 10 . 23 0 . 138 0 . 0202 0 . 82 0 . 021 10 . 01 LayoutDM ‚Ä† 0 . 152 0 . 0201 0 . 46 0 . 172 20 . 50 0 . 127 0 . 0192 0 . 79 0 . 026 3 . 39 LayoutDM ‚Ä† ‚úì 0 . 121 0 . 0141 0 . 55 0 . 088 9 . 02 0 . 127 0 . 0189 0 . 81 0 . 026 3 . 36 C + S ‚Üí P CGL - GAN 0 . 129 0 . 0155 0 . 48 0 . 043 9 . 11 0 . 129 0 . 0202 0 . 75 0 . 027 6 . 96 CGL - GAN ‚úì 0 . 146 0 . 0178 0 . 57 0 . 036 7 . 74 0 . 135 0 . 0207 0 . 78 0 . 020 6 . 01 LayoutDM ‚Ä† 0 . 143 0 . 0185 0 . 45 0 . 122 24 . 90 0 . 127 0 . 0190 0 . 82 0 . 021 2 . 18 LayoutDM ‚Ä† ‚úì 0 . 123 0 . 0144 0 . 59 0 . 071 10 . 68 0 . 127 0 . 0188 0 . 83 0 . 020 1 . 77 Completion CGL - GAN 0 . 146 0 . 0175 0 . 42 0 . 076 27 . 18 0 . 174 0 . 0231 0 . 21 0 . 182 78 . 44 CGL - GAN ‚úì 0 . 146 0 . 0169 0 . 71 0 . 039 12 . 46 0 . 155 0 . 0230 0 . 46 0 . 102 48 . 82 LayoutDM ‚Ä† 0 . 135 0 . 0175 0 . 35 0 . 134 21 . 70 0 . 127 0 . 0192 0 . 76 0 . 020 3 . 19 LayoutDM ‚Ä† ‚úì 0 . 120 0 . 0143 0 . 45 0 . 071 12 . 96 0 . 126 0 . 0189 0 . 79 0 . 018 2 . 55 Refinement CGL - GAN 0 . 122 0 . 0141 0 . 39 0 . 090 6 . 40 0 . 124 0 . 0182 0 . 86 0 . 024 1 . 20 CGL - GAN ‚úì 0 . 129 0 . 0157 0 . 37 0 . 072 4 . 91 0 . 133 0 . 0195 0 . 87 0 . 022 3 . 06 LayoutDM ‚Ä† 0 . 115 0 . 0121 0 . 57 0 . 008 2 . 86 0 . 127 0 . 0188 0 . 75 0 . 018 1 . 98 LayoutDM ‚Ä† ‚úì 0 . 115 0 . 0121 0 . 57 0 . 007 2 . 91 0 . 126 0 . 0186 0 . 76 0 . 019 1 . 79 Table D . Retrieval augmentation for CGL - GAN and LayoutDM ‚Ä† on the PKU and CGL test split for unconstrained and constrained generation . Method # Dim # ParamsDec PKU CGL Content Graphic Content Graphic Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Occ ‚Üì Rea ‚Üì Und ‚Üë Ove ‚Üì FID ‚Üì Autoreg Baseline 128 2 . 55M 0 . 146 0 . 0184 0 . 41 0 . 030 18 . 86 0 . 127 0 . 0196 0 . 86 0 . 013 3 . 60 RALF 0 . 123 0 . 0141 0 . 71 0 . 007 4 . 14 0 . 125 0 . 0180 0 . 97 0 . 005 1 . 27 Autoreg Baseline ‚ô¢ 256 6 . 59M 0 . 134 0 . 0165 0 . 44 0 . 018 13 . 51 0 . 125 0 . 0190 0 . 92 0 . 011 2 . 90 RALF ‚ô¢ 0 . 119 0 . 0129 0 . 92 0 . 008 3 . 45 0 . 125 0 . 0180 0 . 98 0 . 004 1 . 31 Autoreg Baseline 512 19 . 46M 0 . 128 0 . 0150 0 . 57 0 . 011 10 . 85 0 . 122 0 . 0184 0 . 95 0 . 009 2 . 74 RALF 0 . 122 0 . 0131 0 . 94 0 . 010 3 . 61 0 . 128 0 . 0182 0 . 97 0 . 004 1 . 72 Autoreg Baseline 768 38 . 82M 0 . 122 0 . 0150 0 . 70 0 . 012 8 . 46 0 . 124 0 . 0183 0 . 95 0 . 008 2 . 26 RALF 0 . 126 0 . 0131 0 . 93 0 . 008 3 . 19 0 . 131 0 . 0187 0 . 97 0 . 004 1 . 72 Table E . Qualitative result of varying network parameters on unconstrained generation metrics on the PKU and CGL test split . We modify the number of features ( # Dim ) in the input of cross - attention layers and the sequence to the decoder layer . # ParamsDec indicates the number of parameters of the layout decoder . ‚ô¢ represents the setting of our experiments in the main manuscript . 15 C + S ‚Üí P Completion Re / inement C ‚Üí S + P Relationship Constraint RALF ( Ours ) Output 1 Output 2 Output 3 CGL - GAN ! LayoutDM AutoregBaseline Ground truth Input image Category : Logo , Text , Underlay Category + Size : Logo , ‚Ä¶ , Underlaywith Size Category + Relation : e . g . Underlay bottom Canvas , ‚Ä¶ PKU Dataset Logo Text Underlay Figure E . Visual comparison of constrained generation with baselines on the PKU annotated test split . 16 C + S ‚Üí P Completion Re / inement C ‚Üí S + P Relationship Constraint RALF ( Ours ) Output 1 Output 2 Output 3 CGL - GAN ! LayoutDM AutoregBaseline Ground truth Input image Category + Size : Logo , ‚Ä¶ , Underlaywith Size Category + Relation : e . g . Underlay bottom Canvas , ‚Ä¶ CGL Dataset Embellishment Logo Text Underlay Category : Logo , Text , Underlay Figure F . Visual comparison of constrained generation with baselines on the CGL annotated test split . 17