Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models Hyung - Kwon Ko hyungkwonko @ gmail . com KAIST Republic of Korea Hyeon Jeon hj @ hcil . snu . ac . kr Seoul National University Republic of Korea Gwanmo Park gmpark @ hcil . snu . ac . kr Seoul National University Republic of Korea Dae Hyun Kim dhkim16 @ cs . stanford . edu KAIST Republic of Korea Nam Wook Kim nam . wook . kim @ bc . edu Boston College USA Juho Kim juhokim @ kaist . ac . kr KAIST Republic of Korea Jinwook Seo jseo @ snu . ac . kr Seoul National University Republic of Korea ABSTRACT We introduce a Large Language Model ( LLM ) framework that gener - ates rich and diverse NL datasets using only Vega - Lite specifications as input , thereby streamlining the development of Natural Language Interfaces ( NLIs ) for data visualization . We propose two techniques to synthesize relevant chart semantics accurately and enhance syn - tactic diversity in each NL dataset , respectively : 1 ) a guided discov - ery incorporated into prompting so that LLMs can steer themselves to create varying NL datasets in a self - directed manner ; 2 ) a score - based paraphrasing to augment NL syntax along with four well - defined language axes . We also present a new chart collection of 1 , 981 real - world Vega - Lite specifications that have increased diver - sity and complexity compared to benchmarks , to demonstrate the generalizability of our framework . The experimental results show that our framework accurately extracts chart semantics and gen - erates L1 / L2 captions with 89 . 4 % and 76 . 0 % accuracy , respectively , while generating and paraphrasing utterances and questions with greater diversity than benchmarks . The codes and chart collection are available at https : / / github . com / hyungkwonko / chart - llm . CCS CONCEPTS • Human - centered computing → Visualization ; Natural lan - guage interfaces . KEYWORDS Vega - Lite , natural language datasets , large language models , frame - work , natural language interfaces , data visualization Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY © 2018 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - XXXX - X / 18 / 06 . . . $ 15 . 00 https : / / doi . org / XXXXXXX . XXXXXXX ACM Reference Format : Hyung - Kwon Ko , Hyeon Jeon , Gwanmo Park , Dae Hyun Kim , Nam Wook Kim , Juho Kim , and Jinwook Seo . 2018 . Natural Language Dataset Gen - eration Framework for Visualizations Powered by Large Language Mod - els . In Woodstock ’18 : ACM Symposium on Neural Gaze Detection , June 03 – 05 , 2018 , Woodstock , NY . ACM , New York , NY , USA , 20 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION Recent advancements in Natural Language Processing ( NLP ) tech - niques empowered individuals with limited data analysis and visu - alization expertise to engage in text - based interaction and execute data visualization tasks [ 73 , 81 ] . Because NL interaction methods are recognized as more natural and user - friendly compared to tra - ditional approaches like direct manipulation [ 71 ] , many studies have incorporated Natural Language Interfaces ( NLIs ) into their systems . For example , users can query key data insights within charts using NL sentences [ 76 ] , significantly reducing the need for manual extraction via programming . Furthermore , they can input text to receive automatic recommendations for the most appropriate chart types [ 16 , 58 ] , rather than selecting effective representations manually based on graphical language criteria [ 48 ] . The problem is that numerous publications have consistently highlighted the scarcity of high - quality chart collections and NL datasets [ 10 , 13 , 26 , 46 , 72 , 77 ] to develop NLIs for visualizations . Among many factors that explain the quality of NL datasets , secur - ing syntactic diversity is pivotal [ 19 , 71 , 95 ] to capture the language variations arising from diverse spectrum of user expertise , usage scenario , and personal preferences . However , it is challenging to create NL datasets designed for specific tasks with varying syn - tax through crowdsourcing , as excluding inattentive participants may diminish sample diversity , and overlooking inattention can introduce spurious outcomes alongside heightened statistical noise [ 83 ] . Moreover , to annotate diverse and complex charts , researchers must disassemble the task into discrete subtasks to attain a faithful dataset [ 35 ] . Considering the distinct characteristics of each NL dataset [ 72 ] , they need to devise multiple crowdsourcing initiatives , a r X i v : 2309 . 10245v2 [ c s . H C ] 16 O c t 2023 Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . which entails additional efforts that scale proportionally with the number of dataset types . Similarly , it is noted only a fraction of chart collections ( 17 out of 56 ) is publicly accessible . While most of them exhibit limited diversity ( e . g . , in terms of chart type ) [ 10 ] , it is also a crucial factor in generating a variety of NL datasets , since many of them are derived from existing chart collections [ 31 , 44 , 77 ] . We introduce an LLM framework for generating various NL datasets from Vega - Lite specifications [ 66 ] ( Figure 2 ) . Our frame - work comprises of two novel prompting techniques to synthesize relevant chart semantics accurately and to enhance syntactic di - versity , respectively . First , we leverage guided discovery [ 6 ] so that LLMs can steer themselves to create varying NL datasets in a self - directed manner . In detail , the framework analyzes and inte - grates chart semantics ( e . g . , mark , encoding ) with our scaffolding in accordance with the characteristics of each NL dataset . Also , by answering on key questions , it autonomously concentrates on the chart’s key features or propose high - level decisions . Second , we utilize score - based paraphrasing ( Table 3 ) to increase the syntactic diversity of the generated NL datasets . We define four language axes ( clarity , expertise , formality , subjectivity ) using automatic quali - tative coding [ 20 ] , and the framework augments the generated datasets quantitatively along these axes . We also present a new collection of 1 , 981 Vega - Lite specifications ( Figure 3 ) , which is used to demonstrate the generalizability and viability of our NL generation framework . This collection is the largest set of human - generated charts obtained from GitHub to date . It covers varying levels of complexity from a simple line chart without any interaction ( i . e . , simple charts ) to a chart with four plots where data points are linked with selection interactions ( i . e . , extra complex charts ) ( see the charts highlighted with red stroke in Figure 3 ) . As we focus on amassing a richer set of charts in terms of complexity , more than 86 % of them are in complex and extra complex levels . Compared to the benchmarks , our dataset shows the highest average pairwise edit distance between specifications , which proves that the charts are highly diverse from one another . Moreover , it contains the largest number of charts with composite views , interactions ( e . g . , tooltips , panning & zooming , and linking ) , and diverse chart types ( e . g . , map , grid & matrix , diagram , etc . ) ( Table 5 ) . Through our experiment , we show that using Vega - Lite speci - fications in conjunction with our prompting techniques for LLMs yields NL datasets of high fidelity . Specifically , we have generated L1 / L2 captions [ 44 ] , utterances with different phrasings ( command , query , question ) [ 77 ] , questions with varying types ( lookup / compo - sitional , visual / non - visual , open - ended ) [ 23 , 31 ] . We assessed the accuracies of both the chart semantics and L1 / L2 captions . We also measured the within - and cross - distribution metrics of utterances and questions compared to the benchmark datasets [ 31 , 77 ] . Our ex - periments showed that the accuracy of the analyzed chart semantics and generated L1 / L2 captions is 89 . 4 % and 76 . 0 % , respectively . For the semantic diversity , the generated and paraphrased NL datasets had greater diversity in terms of 4 . 75 out of 6 within - distribution metrics on average . The main contributions of our work are summarized as follows : • Wepresentan LLMframeworktogeneratevarious NLdatasets leveraging guided discovery to synthesize relevant chart se - mantics accurately ; • We introduce a score - based paraphrasing technique to en - hance the syntactic diversity of NL datasets ; • We propose a collection of 1 , 981 real - world Vega - Lite speci - fications that are highly diverse and complex compared to the benchmarks . 2 BACKGROUND AND RELATED WORK In this section , we explain the types of NL datasets that are of particular interest in the context of this work . We also explain the Vega - Lite specification and use of LLMs in synthesizing NL datasets . 2 . 1 NLIs for Data Visualization NLIs for data visualization have garnered significant attention due to their user - friendly nature [ 72 , 78 , 85 ] . These interfaces allow users to focus on their tasks rather than learning how to interact with systems [ 12 ] . A recent survey paper [ 72 ] suggested six high - level topics ( e . g . , visualization recommendation ) to cluster tasks . They also presented a pipeline with seven stages by extending the classical information visualization pipeline [ 7 ] . To address diverse NLI tasks , we considered three types of NL datasets : captions , utterances , and questions . This choice was made based on the analysis of each topic , the number of representative works , and the relevance of NL datasets to their respective tasks . The first NL dataset is chart caption . The captions can help people communicate and grasp insights in the charts easily , also improving the accessibility for readers of the blind and low vision people [ 44 ] . A lot of research delved into this problem leveraging from templates [ 54 ] to deep learning models [ 59 , 62 , 75 ] . Lundgard and Satyanarayan [ 44 ] proposed a four - level classifi - cation of captions where each level contains different semantic con - tent of the same chart : L1 provides elemental and encoded attributes , including chart type and encoding channel ; L2 encompasses statis - tical and relational attributes such as descriptive statistics and cor - relation ; L3 addresses perceptual and cognitive attributes , covering complex trends and patterns ; L4 contains contextual and domain - specific knowledge . Recently , VisText [ 82 ] generated L2 / L3 cap - tions by training ByT5 transformer model [ 91 ] with crowdsourced dataset . Our work shares similarities with VisText in generating captions with varying levels . However , it differs in that we do not rely on crowdsourcing NL datasets or training machine learning models . Instead , our approach solely depends on Vega - Lite speci - fication input and vanilla LLMs . It is worth noting that previous studies in caption generation have predominantly focused on basic chart types , as highlighted in [ 72 ] . In contrast , our work offers a generalizable solution capable of generating captions for complex and diverse charts . The second NL dataset is utterance for chart generation . For many decades , automatically representing graphical information has been one of the important topics in information visualization [ 48 ] . Many NLIs were introduced and adopted to solve multiple stages that are entangled one another for the automatic represen - tation . The most relevant stages are 1 ) utterance interpretation [ 18 , 24 , 34 , 41 , 47 , 58 , 79 , 95 ] and 2 ) mapping utterances to visual NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Figure 1 : Example of Vega - Lite Specification . As previously noted in several works [ 46 , 96 ] , Vega - Lite specification can be regarded to follow a tree structure , with its keys ( i . e . , properties ) connected in a nested structure . elements [ 25 , 29 , 45 , 49 , 55 , 80 , 84 , 87 , 89 ] , and 3 ) human interaction for clarifying ambiguity or suggesting commands [ 19 , 24 , 57 , 58 , 71 ] . Srinivasan et al . [ 77 ] analyzed the characteristics and semantics of NL utterances employed in chart generation . According to their research , NL utterances can be classified into three types based on their structures : commands , which are instructions or systematic requests ; queries , which are concise lists of keywords similar to web search queries ; and questions , which are data - driven inquiries that users wish to visualize . In our work , we generate all three types of utterances , incorporating heightened syntactic diversity for a comprehensive evaluation . The last NL dataset is question . Chart Question Answering is a popular task in both machine learning [ 9 , 42 , 43 , 50 ] and human - computer interaction [ 31 ] communities . This popularity stems from its effectiveness in eliciting insights and aiding in decision - making processes [ 23 ] . Kim et al . [ 31 ] investigated the semantics used in the questions by collecting 629 crowd - sourced questions and provided two or - thogonal dimensions . First axis is lookup or compositional , which is whether to retrieve a single value or using multiple mathematical operations . Second axis is visual or non - visual , which is whether to reference visual chart features or not . These question types are all focusing on retrieving factual short answers . In our work , we target five different types of questions , including the aforemen - tioned types as well as the open - ended question type [ 23 ] , which encourages deeper reflection on the underlying reasons or causes behind specific events or patterns . 2 . 2 Chart Datasets According to Chen et al . ’s recent survey [ 10 ] , chart datasets are typically collected in three formats : bitmap graphics ( e . g . , . png ) , vector graphics ( e . g . , . svg ) , and programs ( e . g . , Vega - Lite specifi - cations [ 66 ] ) . Among the surveyed datasets , the majority ( 48 out of 56 ) consisted of bitmap graphics , followed by vector graphics ( 10 out of 56 ) , while programs were less prevalent , comprising only five instances ( some works included multiple formats ) . Among many program formats , we are especially interested in Vega - Lite ( Figure 1 ) , which is an abstract specification that enables the creation of interactive visualizations using a high - level grammar . It is represented as a nested JSON object , consisting of numerous key - value pairs , which can be also seen as a tree structure [ 46 , 96 ] . Each key defined in the specification is referred to as a property [ 88 ] , serving a distinct role in generating charts . For example , mark property is used to map data to graphical elements ( e . g . , points , lines ) . Vega - Lite provides additional advantages beyond those offered by SVG formats , since it is easy to modify and reuse for creat - ing diverse chart variations [ 21 ] . It provides interactive features like zooming , panning , and brushing , as well as concatenating or faceting multiple plots / views . Furthermore , it support data - driven manipulation , allowing users to dynamically update the data and reflect changes in real time . It can be seamlessly converted to other formats like bitmaps and SVG [ 67 ] , while converting from those formats to program specifications typically requires manual effort or complex external algorithms [ 61 ] . There are two types of Vega - Lite benchmarks : synthetic and real - world datasets . A critical limitation of synthetic datasets lies is their reliance on pre - defined templates and rules , which leads to a high degree of repetition and a limited range of chart types and functionalities ( see Table 4 ) . On the other hand , the real - world dataset reveals significant variation from one spec to another , en - suring a high level of diversity in realistic scenarios . However , they are generally much smaller in size compared to synthetic datasets [ 10 ] . We found three synthetic Vega - Lite datasets . In detail , Poco et al . generated 4 , 318 Vega specifications [ 69 ] using the Compass recommendation engine [ 88 ] . They randomly selected values for a few variables ( e . g . , fonts , font size , legend positions , etc . ) from a curated set of options . These specifications were later converted to Vega - Lite specifications in Data2Vis [ 17 ] . Zhao et al . [ 96 ] followed a similar approach to generate the Chartseer dataset , consisting of 9 , 925 specifications based on Data2Vis , although it is specifically designed for training a deep learning model and may not readily render into charts , making it less suitable for broader research adoption . The nvBench dataset [ 46 ] presented 7 , 274 specifications , representing SQL queries as tree structures and mapping them into Vega - Lite specifications . Therearetworeal - world datasetsthatconsistofhuman - generated specifications . For instance , Kim et al . [ 31 ] curated 52 charts from various web sources , encompassing two chart types ( bar chart and line chart ) . Additionally , the Vega - Lite gallery example dataset [ 68 ] , Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Table 1 : Prompting techniques to generate each NL dataset . Each prompt is designed by choosing the most appropriate techniques considering the characteristics of different NL datasets . Target Technique L1 caption L2 caption Utterance Question Semantic Scaffold O - O - Key question - O O O Syntactic Paraphrasing - - O O the largest publicly available human - generated collection of Vega - Lite data , provides 716 high - quality examples with diverse chart types and interactions . However , due to the challenges associated with data collection , these datasets have a limited quantity of speci - fications compared to synthesized datasets . As a result , researchers often face difficulties in finding a comprehensive set of specifica - tions for their own research purposes . 2 . 3 LLMs and NL Datasets Many past research typically have used crowdsourcing to collect varying types of NL datasets ( e . g . , captions , utterances , questions , etc . ) by asking crowd workers to come up with generation queries using available chart datasets [ 31 , 46 , 77 ] . However , this approach is frequently time - consuming and costly [ 15 , 90 ] , which can ad - versely affect the scalability of datasets . It is prone to issues such as participant laziness and the collection of subpar queries [ 4 ] . To ensure a consistent performance among workers , it is essential to simplify the tasks and making them easy to follow , thereby pre - venting workers from feeling overwhelmed or fatigued during the study , as recommended by Kittur et al . [ 35 ] . With all these efforts , such crowdsourced NL datasets are often fragmented , posing chal - lenges for researchers seeking to apply them to their own tasks . The characteristics of NL queries designed for each task can vary significantly , making a single NL dataset unsuitable for other tasks . This motivates the need for a unified and adaptable framework that can generate NL datasets tailored to any specific NLIs for data visualization research . As LLMs are known to simulate human behavior [ 60 ] and have become more prevalent due to their powerful performance , re - searchers are increasingly using generated NL datasets to train smaller - sized language models for specific tasks [ 53 , 70 , 93 ] . This training strategy is known as ‘teaching via data’ [ 40 ] . Here , LLMs , acting as teacher models , generate synthetic datasets which are then used to train smaller - sized models , referred to as students , designed for specific tasks . This method is adopted to increase the performance of different tasks like knowledge - based question an - swering [ 40 ] , symbolic language generation ( e . g . , SQL query ) [ 94 ] , and semantic parsing [ 65 ] . Our work aligns with this trend , aiming to assist researchers in developing NLIs for data visualization by generating the necessary NL datasets using LLMs . 3 NL GENERATION FRAMEWORK The goal of our framework is to generate high - quality NL datasets using only Vega - Lite specifications and prompt engineering . The framework consists of two main ideas ( Figure 2 ) ; the first focuses on identifying relevant and accurate information through guided - discovery , while the second focuses on increasing syntactic diver - sity using score - based paraphrasing . To generate each type of NL dataset , we design each prompt to be maximally helpful by selecting the most appropriate strategies ( Table 1 ) . 3 . 1 Pre - processing Vega - Lite Specifications Using raw Vega - Lite specifications is not appropriate for prompt - ing , because some of them include the dataset they use within the specification , resulting in excessively file length . Therefore , we save the data as an external files with the most suitable data formats ( e . g . , . csv , . json ) . Subsequently , the location of the saved files is overwritten with their URLs , rather than being embedded in the specification . In our current implementation of the framework , we only support . csv data format . Therefore , we have converted . json files into . csv files . Last , we minify the Vega - Lite specifi - cations by removing all line breaks and indentations to reduce the number of tokens sent through API usage . 3 . 2 Ensuring Accuracy and Relevance Our framework leverages the concept of guided discovery [ 6 ] based on Chain - of - Thought prompting [ 86 ] to harness the maximum rea - soning capability of LLMs . We employ two strategies of guided discovery : providing scaffolds [ 22 ] . and posing key questions [ 14 ] . To analyze and integrate the chart semantics necessary for gener - ating a specific NL dataset , we assist LLMs by offering scaffolds . Additionally , we furnish LLMs with key questions to guide their self - directed progress . This maximizes the use of LLMs’ reasoning abilities , allowing them to make decisions on which aspects to fo - cus on and delve into when creating a particular data . Below , we denote each step where we italicize the relevant phrases , and utilize symbols for ( # S ) scaffolding and ( # K ) key question to make them easily identifiable . To demonstrate how we can ensure relevance and accuracy in generating different types of NL datasets , we have selected three datasets commonly used in NLIs for data visualization research . We made these selections based on their significance in conjunc - tion with related tasks , as indicated in a recent survey paper [ 72 ] : captions ( L1 , L2 ) , utterance ( command , query , question ) , question ( visual - lookup , visual - compositional , nonvisual - lookup , nonvisual - compositional , open - ended ) . The detailed generation process for each NL dataset can differ from one another . Here , we design each step in prompting to be merged or separated when generating different NL datasets so they can best capture each of their charac - teristics . We only explain the high - level descriptions of each , and the detailed and full prompting used for generating each NL dataset is presented in Appendix A . 3 . 2 . 1 L1 Caption . Considering real - world Vega - Lite specifications , we first understand whether the given chart is ( # S ) a composite view ( e . g . , layered , trellis , and multiple views ) , to enable top - down anal - ysis of each chart one by one . The prompt follows a template with three questions to answer : Is it a composite view ? ; If it is , identify its type among layered , trellis , and multiple views ; and determine the number of plots in the chart . Next , it analyzes each chart in - dividually based on the provided scaffold of ( # S ) chart semantics : NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY { " $ schema " : ” . / vega - lite / v3 . json " , “data” : { “Region” : [ . . . ] , “Profit” : [ . . . ] , } " mark " : { " type " : " bar " , " tooltip " : null } , . . . . } } Vega - Lite Specification a data . csv Single - line specification Preprocessing b Chain - of - Thoughts Guided discovery ( by LLM ) Scaffolding e . g . , composite view , chart semantics , fields Key Questions e . g . , What is the most prominent and meaningful feature in the given chart ? Score - based Paraphrasing ( by LLM ) INPUT Generated NL L1 Caption : This is a bar chart that uses data from a superstore . The chart encodes the ' Region ' field on the x - axis and the sum of ' Profit ' on the y - axis . The ' Ship Status ' field is represented by different colors . L2 Caption : The visualization presents the total profit for each region , with the West region having the highest profit of approximately 67860 . 56 and the South region having the lowest profit of approximately 26551 . 72 . Utterance : Create a bar chart showing the sum of profit from different regions . Question : Which region has the longest bar representing the highest total profit ? Rewrite the following sentence as if it were spoken by a person with a given score for language usage Score : 1 - 5 Language Axis : subjectivity , formality , clarity , and expertise Paraphrased NL Utterance : Could you possibly put together a visual , maybe a bar chart , that gives us an idea of how profits are distributed across various regions ? Question : Which area is depicted with the most extended bar symbolizing the maximum overall gain ? c d e f Figure 2 : LLM Framework to Generate Varying NL Datasets for Visualizations . We start by minifying Vega - Lite specifications ( b ) . Subsequently , we employ scaffolding and key questions ( c ) , to generate L1 / L2 captions , utterances , and questions ( d ) . This is followed by score - based paraphrasing ( e ) , allowing us to produce syntactically paraphrased NL datasets ( f ) . Table 2 : Results of automatic qualitative coding [ 20 ] . From previous NL datasets of captions , utterances , and questions , we identified four language axes of syntactic diversity : subjectivity , formality , clarity , and expertise . The top five most frequently occurring codes within each axis are presented along with their respective frequencies in parentheses . Axes Formality Clarity Expertise Subjectivity Directions Colloquial / Standard Implicit / Explicit Non - technical / Technical Subjective / Objective Example codes interrogative form ( 540 ) specificity ( 221 ) economic ( 159 ) descriptive ( 611 ) formal ( 384 ) specific ( 91 ) geographical context ( 125 ) negative connotation ( 58 ) passive voice ( 211 ) ambiguity ( 68 ) financial ( 106 ) subjectivity ( 22 ) analytical ( 195 ) conciseness ( 64 ) business - oriented ( 81 ) negative ( 15 ) command - oriented ( 166 ) abstract ( 63 ) business terminology ( 65 ) third person perspective ( 11 ) Data , Transform , Mark , Chart - Type , Encoding , Style , and Interac - tion , using the information about composite view . Here , we access the backing dataset to provide the ( # S ) fields that are presented in the Vega - Lite specification , along with their synonyms ( i . e . , ( # S ) titles also found in the same Vega - Lite specification ) and their ( # S ) unique values if they are categorical variables . After analyzing all of these semantics , the LLM finally generates the L1 caption by combining them . 3 . 2 . 2 L2 Caption . L2 captions , unlike L1 captions that provide an overall description of the chart , offer the flexibility to selectively focus on specific features that capture the viewer’s interest . To craft informative and insightful captions , we follow a structured approach centered around a key question : ( # K ) What is the most prominent and meaningful feature in the given chart ? Once we iden - tify this feature , we delve deeper by exploring the mathematical operations required to analyze it : ( # K ) What is the mathematical operation ( s ) required to describe the feature ? Subsequently , based on these operations , we generate ( # K ) a series of questions to an - alyze the feature ( e . g . , for the simple line chart with a red border in Figure 3 , the following questions are generated : What was the highest stock price of Google ? ; What was the lowest stock price of Google ? ; What is the difference between the highest and lowest stock prices of Google ? ) . This process allows us to create captions that provide valuable insights into the chart’s content . When an - swering questions , we utilize LangChain [ 8 ] to ensure that our captions contain only factual information in the captions . This step is crucial , as large language models ( LLMs ) have been known to produce hallucinations in response to mathematical problems [ 27 ] . Once each question is answered , the collected information is subse - quently incorporated into the final prompting stage for generating L2 captions . It’s important to note that , unlike previous work [ 82 ] , we do not use any of the L1 captions when generating L2 captions . Instead , this is performed as an independent process . Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Table 3 : Example of score - based paraphrasing with two axes . We used a sample LLM - generated utterance to create the chart shown in Figure 2 . We perform linear interpolation to manipulate formality and expertise scores from 1 to 5 , one at a time to generate 25 paraphrased sentences . While the content ( i . e . , semantics ) of the utterance remains consistent , the tone and voice ( i . e . , syntax ) change linearly in accordance with the provided scores . Sample Utterance : Create a bar chart showing the sum of profit from different regions . Expertise ( Non - technical : 1 , Technical : 5 ) 1 2 3 4 5 1 Hey , can you whip up a bar graph showing how much dough we’ve made from different places ? Hey , canyoumakeabar graph showing the total profit from different re - gions ? Can you put together a bar chart showing the aggregate profit from various geographical ar - eas ? Can you construct a bar chart illustrating the cu - mulative profit derived from distinct regions ? Can you generate a bar chart delineating the summation of fiscal gain from disparate ge - ographical sectors ? F o r m a l i t y ( C o ll o q u i a l : 1 , S t a n d a r d : 5 ) 2 Could you create a bar chart that shows how much money we’ve made from different places ? Could you create a bar chart that shows the to - tal profit from different regions ? Could you create a bar chart that illustrates the aggregate profit from various geographical ar - eas ? Could you create a bar chartthatdelineatesthe cumulative profit de - rived from distinct re - gions ? Could you create a bar chart that represents the summation of fiscal gain from disparate ge - ographical sectors ? 3 Please create a bar chart showing how much money we’ve made from different places . Please create a bar chart showing the total profit from different regions . Please create a bar chart illustrating the aggre - gate profit from various geographical areas . Please create a bar chart delineating the cumula - tive profit derived from distinct regions . Please create a bar chart representing the summation of fiscal gain from disparate geographical sectors . 4 It is requested that you create a bar chart show - ing the money made from different places . It is requested that you create a bar chart show - ing the total profit from different regions . It is requested that you create a bar chart il - lustrating the aggregate profit from various geo - graphical areas . It is requested that you create a bar chart delin - eating the cumulative profit derived from dis - tinct regions . It is requested that you create a bar chart repre - senting the summation of fiscal gain from dis - parate geographical sec - tors . 5 You are required to construct a bar chart demonstrating the mon - etary gain from various locations . You are required to construct a bar chart demonstrating the total profit from different re - gions . You are required to con - struct a bar chart il - lustrating the aggregate profit from various geo - graphical areas . You are required to con - struct a bar chart delin - eating the cumulative profit derived from dis - tinct regions . You are required to con - struct a bar chart repre - senting the summation of fiscal gain from dis - parate geographical sec - tors . 3 . 2 . 3 Utterance . Similar to L1 captions , we begin by analyzing whether the chart is a ( # S ) composite view . We then proceed to generate ( # S ) instructions for each plot independently . This process entails creating a comprehensive set of step - by - step instructions for constructing each plot . To enhance readability and user - friendliness , we ensure that each instruction focuses on a single specific action , aligning with the same semantics used when generating L1 captions . For example , in the case of the simple line chart with a red border shown in Figure 3 , the following instructions are generated : • Data : Use Google’s stock price data ; • Chart - Type : Create a line chart ; • Mark : Use a line mark ; • Encoding : Encode the x - axis with the date field , using a temporal type and a time unit of year , month , date , hours , and minutes , and scale it using UTC ; • Encoding : Encode the y - axis with the price field , using a quantitative type . However , it’s important to note that the generated instructions may sometimes feature overly technical variable names from the chart , which might not align with users’ NL usage patterns . In such cases , we leverage information from synonyms found in the ( # S ) title of the Vega - Lite specification or ( # S ) values from the fields to replace these technical terms , resulting in more user - friendly instructions . Next , we ask a key question to ( # K ) identify primary and sec - ondary information . In this context , we anticipate that LLMs are able to automatically prioritize crucial semantics to paint a compre - hensive picture of the chart , such as chart type or encoding , over additional instructions like style or interaction . This thought is based on Wang et al . ’s observations [ 85 ] , who noted that the typical workflow for creating visualizations often starts with this informa - tion ( e . g . , ’show me the price over time as a line chart’ ) . Once we have all these components ready , we proceed to generate each type of utterance one by one , adhering to ( # S ) specific rules for each type . For commands , we employ the imperative voice . For queries , we NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY use only variables , fields , attributes , mathematical formulas , abbre - viations , and prepositions , while avoiding verbs and articles . For questions , we formulate inquiries in the form of questions . Across all types , we maintain the following rules : express each utterance in a single sentence , utilize only primary information , and keep the language concise and straightforward . 3 . 2 . 4 Question . In general , we conduct chart question answering to facilitate decision - making [ 31 ] . Thus , the process involves an - alyzing charts through a question - answering , which ultimately leads to a conclusion and informs the decision - making . To gener - ate questions , we employ a reverse thought process . This entails first identifying the decisions that can be derived from the charts ( i . e . , ( # K ) What higher - level decision can be made by analyzing this chart ? ) , followed by formulating a possible conclusion that leads to such a decision ( i . e . , ( # K ) What is a possible conclusion that can be reached from this decision ? ) . Finally , we determine what needs to be analyzed ( i . e . , ( # K ) What specific value can be retrieved to reach this conclusion ? What are the mathematical operations to reach the conclusion ? ) . We generate non - visual lookup and compositional questions using the provided values and mathematical operations . To transform these into visual questions , we identify the necessary visual attributes and incorporate them into the generated questions ( i . e . , ( # K ) What visual attributes are required to paraphrase this ques - tion ? ) . Finally , we formulate an open - ended question designed to lead to the same conclusion obtained in the previous step . 3 . 3 Increasing Syntactic Diversity 3 . 3 . 1 Automatic Qualitative Coding . Before increasing the syntac - tic diversity of NL datasets , we need to analyze which meaningful axes of diversity to address . To this end , we collected sample NL sen - tences from existing sources , which consist of 2 , 147 captions , 893 utterances , and 629 questions [ 31 , 44 , 77 ] . Next , following the auto - matic coding process that Hämäläinen et al . have proposed [ 20 ] , we utilized these sample sentences to conduct a thematic analysis using LLMs , generating five different codes for each caption , utterance , and question ( see the prompt in Appendix B ) . We manually checked the generated codes to eliminate irrelevant and erroneous ones , resulting in 15 , 271 valid codes out of 18 , 345 . Then , we retained 2 , 759 unique codes and vectorized them using Sentence - Bert [ 63 ] . After - wards , we applied dimensionality reduction technique to project them into a lower dimensional space using UMAP [ 52 ] , reducing the 100 - dimensional vectors to 5 - dimensional vectors . Next , we employed HDBSCAN [ 51 ] to cluster them into a few classes for detailed investigation . We aggregated clusters into a higher - level cluster , except for the codes that are not clustered through HDB - SCAN , to derive the final themes . We identified a total of six themes , but selected four meaningful axes related to NL syntax – clarity , expertise , formality , subjectivity ( Table 2 ) . Two themes were removed – 1 ) measurement , and 2 ) chart and data analytics – as they are not directly related to the syntax of NL datasets but rather to the semantic properties of charts . Clarity represents a language axis with two opposite meanings—implicit and explicit . Implicit language relies on context , shared knowledge , and non - verbal cues to convey meaning , while explicit language is clear and direct , leaving little room for interpretation or misunder - standing . The expertise axis also has two opposite meanings—non - technical and technical . Technical language includes specialized terminology and jargon , whereas non - technical language is more accessible to a general audience and avoids the use of complex terms . Formality , the third language axis , ranges from colloquial , which is informal and used in everyday conversation , to standard , which follows established rules and conventions . Finally , the sub - jectivity axis encompasses subjective language , which expresses personal opinions , feelings , or judgments , and objective language , which presents facts or information without bias or personal inter - pretation . 3 . 3 . 2 Score - based Paraphrasing . Our paraphrasing technique is inspired by a linear interpolation in the latent space for image generation and manipulation as demonstrated in many system and application papers [ 1 , 3 , 37 , 56 ] . It enables a smooth transition from one expression to another by focusing on creating controllable and meaningful variations of a single sentence . We employ a five - point Likert - scale and assign one of the axes to each . Here , we focus on altering only the sentence’s syntax , while maintaining its meaning . In detail , we provide LLMs with a sentence ( i . e . , Example Sentence ) and an explanation about one of the defined axes ( i . e . , Axis ) and its two directions ( i . e . , Direction - 1 , Direction - 2 ) . We assign a specific value on a Likert scale ranging from one to five , to paraphrase the sentence as if it were spoken by a person using a language with a certain degree indicated by the score . Below is the template prompt we used to paraphrase on a single axis : 1 { Axis } 2 3 Score of 1 indicates a higher tendency to use { Direction - 1 } language and a Score of 5 indicates a higher tendency to use { Direction - 2 } language . Rewrite the following sentence as if it were spoken by a person with a given score for language usage . 4 5 Sentence : { Example Sentence } 6 Score : { Score } This technique can be extended to scenarios involving multiple scores and multiple axes ( refer to an example result with two axes in Table 3 ) . Here is the template prompt for paraphrasing across two language axes : 1 { Axis - 1 } 2 { Axis - 2 } 3 4 Score - A of 1 indicates a higher tendency to use { Direction - 1 - 1 } language and a Score - A of 5 indicates a higher tendency to use { Direction - 1 - 2 } language . 5 Score - B of 1 indicates a higher tendency to use { Direction - 2 - 1 } language and a Score - B of 5 indicates a higher tendency to use { Direction - 2 - 2 } language . 6 Rewrite the following sentence as if it were spoken by a person with a given score for language usage . 7 8 Sentence : { Example Sentence } 9 Score - A : { Score - A } , Score - B : { Score - B } Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Table 4 : Summary of the Vega - Lite dataset construction pro - cess . First we collect all possible cases of URLs including Vega - Lite specifications ( a ) . Next , we have filtered unique URLs that are allowed to redistribute for academic purpose ( b , c ) . Finally we iteratively inspect each specification manu - ally to check whether it is valid and unique , since we want to collect charts with a high level of diversity ( d ) . # of URLs / Vega - Lite specs ( a ) URLs crawled 67 , 789 ( b ) URLs w / o duplicate 18 , 420 ( c ) URLs w / license 7 , 408 ( d ) Specs after manual inspection 1 , 981 4 VEGA - LITE DATASET We have collected a new set of 1 , 981 real - world Vega - Lite specifica - tions . It was used to test our LLM - based NL generation framework on diverse and complex charts . In this section , we present the details of our data collection process . 4 . 1 Dataset Construction 4 . 1 . 1 Search Queries . We utilize the GitHub API 1 to create our Vega - Lite dataset . Due to the API’s limitation of providing up to 1 , 000 results per search query , we employ various techniques , as we elaborate below , to crawl Vega - Lite specifications in a mutually exclusive and exhaustive manner to the best of our abilities . When building search queries , we use the keyword https : / / vega . github . io / schema / vega - lite / [ version ] to indicate the version of the specification that Vega - Lite uses for rendering purposes . We collect versions from v2 to v5 : there are no v1 data to be found . To partition the query into a more fine - grained manner , we use keywords such as . csv and . json to gather specifications with external links . Similarly , we employ keywords like values and datasets to identify ones with internally embedded data . We also leverage additional keywords using the main properties defined in the version 5 Vega - Lite specification 2 . These properties encom - pass essential elements for creating a single plot , including data , transform , mark , and encoding , while there are properties like layer , facet , concat , and repeat , which are specifically rele - vant to visualizing composite views [ 66 ] ( e . g . , layered plots , trellis plots , or multiple views ) . A comprehensive list of the properties we use can be found on the official documentation page 3 . 4 . 1 . 2 Inclusion and Exclusion Criteria . We target files with exten - sion . json , vg . json , . vl . json , . vl , and . vg which denotes Vega - Lite specifications . We also examine HTML and JavaScript files containing Vega - Lite specifications manually to get additional specifications . Throughout the process , we exclude forked reposi - tories to prevent redundancy . We also filter out any data from the benchmark datasets , such as Vega - Lite gallery [ 68 ] . 1 https : / / docs . github . com / en / rest 2 https : / / github . com / vega / schema 3 https : / / vega . github . io / vega - lite / docs 4 . 1 . 3 Post - processing . To obtain a large number of unique sets of Vega - Lite specifications , we follow a step - by - step approach . During the initial stage , a total of 67 , 789 URLs are collected . Despite efforts to ensure a mutually exclusive and comprehensive set of specifi - cations , duplicate URLs are identified and removed , resulting in 18 , 420 unique URLs . Each URL is scrutinized to verify the license of the corresponding repository , ensuring compliance with copyright regulations for academic redistribution . This process yields 7 , 408 URLs . Lastly , we verify their validity using the Vega - Lite editor [ 67 ] . This involves identifying the URLs of the datasets used by each specification and making necessary modifications , ranging from minor adjustments such as closing unclosed brackets to more sig - nificant ones like debugging the entire code , in order to achieve successful rendering . An overview of the post - processing and the number of URLs and specifications obtained at each stage can be found in Table 4 . Our chart collection is publicly accessible via the following link : https : / / hyungkwonko . info / chart - llm - data . 4 . 2 Quantitative Analysis 4 . 2 . 1 Benchmarks . We compare three synthetic and two real - world Vega - Lite datasets [ 17 , 31 , 46 , 68 , 96 ] described in Section 2 . To en - sure a fair comparison , we implement a process to remove exact code duplication within each benchmark . In detail , each specifi - cation is sorted in alphabetical order by the keys and edited to maintain consistent indentation . Next , we convert each file into a hash where files with identical hashes are subsequently removed from the dataset . Following this procedure , the number of specifi - cations in Chartseer dataset decrease from 9 , 917 to 9 , 897 , nvBench decrease from 7 , 241 to 6 , 680 , and the Vega - Lite gallery example dataset decrease from 716 to 709 . 4 . 2 . 2 Quality Metrics . To comprehensively assess the Vega - Lite datasets , we consider three different aspects : quantity , complexity , and diversity . Initially , we count the number of collected specifica - tions to determine the overall quantity of Vega - Lite specifications , as previously done by Luo et al . [ 46 ] . However , we argue that addi - tional metrics are necessary to gauge the quality of the Vega - Lite dataset . This is because some specifications include only manda - tory properties to construct a single plot without any interaction ( e . g . , data , encoding , mark for a simple bar chart ) , while oth - ers contain multiple plots or views linked by varying interactions . Therefore , the number of keys in a specification can highly differ depending on whether it includes properties for data pre - processing ( e . g . , aggregate , calculate , etc . ) , interactivity ( e . g . , bind , se - lect , etc . ) , or composite views ( e . g . , concat , repeat , etc . ) . We can expect the Vega - Lite specification becomes more complex as the number of defined properties increases . Therefore , we propose a new standard to understand the complexity of a Vega - Lite dataset by counting the total number of keys present across all specifica - tions and the average number of keys in a singe specification . To ensure a fair comparison , we only consider keys defined in the ver - sion 5 specification . We also ignore keys associated with internally embedded datasets , such as values and datasets , along with their corresponding keys . In addition to this , we also measure the average depth and branching factor of the JSON structure as they are commonly adopted to evaluate the complexity of a JSON file . NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Simple ( 2 ) Medium ( 5 ) Complex ( 18 ) Extra Complex ( 23 ) Figure 3 : Vega - Lite dataset divided by their complexity levels : simple , medium , complex , extra complex . These 48 charts were selected via stratified sampling and used in our evaluation ( Section 5 ) . The level is divided based on the number of keys each specification contains . The number of keys , which are the criteria for dividing the levels , are set based on the quartiles ( Q1 , Q2 , Q3 ) of Vega - Lite example gallery dataset [ 68 ] . We found no metrics to quantify the diversity of chart dataset [ 10 ] . Therefore , we also propose metrics for gaining insights into the diversity of dataset in terms of both the range of properties within the entire dataset and the variance between individual specifica - tions . Specifically , we count the number of unique keys employed across the entire dataset and calculate the average pairwise edit distance among all possible pairs of specifications . The number of Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Table 5 : Summary statistics of our dataset and benchmark datasets that are publicly available . Two types of datasets are presented : synthetic and real - world datasets . The best statistics within each type are highlighted in bold , while the best statistics across all datasets are also underscored . Type Evaluation Metric / Criteria Synthetic data ( machine - generated ) Real - world data ( human - generated ) Data2Vis [ 17 ] Chartseer [ 96 ] nvBench [ 46 ] Kim et al . [ 31 ] Gallery [ 68 ] Ours Quantity # of specs 4 , 318 9 , 897 6 , 680 52 709 1 , 981 Complexity Total # of keys across specs 101 , 881 147 , 676 98 , 074 769 26 , 469 107 , 802 Average # of keys in a spec 24 15 15 15 37 54 Simple ( key ≤ 16 ) 0 6 , 164 6 , 354 41 186 73 Medium ( key ≤ 24 ) 4 , 318 3 , 733 326 10 170 199 Complex ( key ≤ 41 ) 0 0 0 1 179 733 Extra complex ( key > 41 ) 0 0 0 0 174 976 Average depth of JSON 4 . 00 3 . 00 3 . 48 3 . 13 5 . 01 5 . 19 Average branching factor 1 . 22 1 . 44 1 . 18 1 . 17 1 . 41 1 . 38 Diversity Total # of unique keys 24 12 18 31 275 362 Average pairwise edit distance 122 . 62 75 . 90 48 . 18 129 . 51 1 , 096 . 11 1 , 549 . 48 Composite views 0 0 0 0 136 746 Interaction ( e . g . , zoom , pan ) 0 0 0 0 188 1 , 010 # of chart types 6 6 4 2 10 10 unique keys indicates how many distinct properties that can be de - fined in a Vega - Lite specification are used across the specifications . For example , if a handful of unique keys are used within the dataset , this indicates a restricted recurrence of only a few properties . In turn , it likely signifies a low level of diversity . The average pairwise edit distance provides an overview of the dissimilarity between each pair at the code level . To perform this analysis , we sort the keys alphabetically , replace their corresponding values with empty values , and exclude keys associated with embedded datasets , as mentioned earlier . 4 . 2 . 3 Complexity Levels . We observe that the existing criteria used to establish the complexity levels of chart datasets are somewhat subjective and may not possess broad applicability [ 30 , 44 , 46 ] . Instead , we suggest using the number of keys as a criterion for categorizing the complexity levels of charts , particularly in the context of Vega - Lite specifications . This is because , as explained above , the number of properties increases proportionately to the number of keys in a specification . To establish the standard number of keys , we refer to the Vega - Lite example gallery dataset [ 68 ] and calculate the quartiles ( Q1 , Q2 , Q3 ) based on the distribution of the number of keys . These quartiles , specifically 16 , 24 , and 41 , are utilized as reference points to divide the specifications’ level of complexity . For instance , a specification with a total number of keys less than or equal to 16 is classified as ‘simple’ complexity . Likewise , a specification with a total number of keys greater than 16 and less than or equal to 24 is classified as ‘medium’ complexity ( Figure 3 ) . 4 . 2 . 4 Composite View , Interactivity , and Chart Type Distribution . We choose three additional factors by referring to previous works [ 5 , 39 ] to further assess the quality of the datasets . First , we examine the presence of composite views , which offer diverse perspectives on the same data simultaneously [ 11 ] . Secondly , considering the benefits of collecting Vega - Lite specifications over static bitmap images , we count the number of charts that incorporate interactive techniques such as tooltips , zooming , and brushing . Lastly , we eval - uate the number of charts types based on the taxonomy proposed by Borkin et al . [ 5 ] . 4 . 2 . 5 Results . We present the results in terms of quantity , com - plexity , and diversity , highlighting the superiority of our dataset compared to the benchmarks . Regarding quantity , all three syn - thetic datasets demonstrate a higher number of specifications com - pared to the other three real - world datasets . Among all datasets , Chartseer shows the highest number of specifications ( i . e . , 9 , 897 ) , while our dataset has 1 , 981 specifications which outnumbers the other real - world datasets in terms of quantity . In terms of complexity , our dataset ranks the first in average number of keys in a single specification ( i . e . , 54 ) and the second in total number of keys across specifications ( i . e . , 107 , 802 ) , which is 1 . 4 and 4 . 0 times larger than the largest previous real - world Vega - Lite dataset , respectively . Chartseer presents the highest total number of keys across specifications ( i . e . , 147 , 676 ) with the smallest average number of keys per specification ( i . e . , 15 ) among all datasets . Our dataset includes the highest number of specifications classified as complex ( i . e . , 733 ) and extra complex ( i . e . , 976 ) , while all synthetic datasets do not contain any specifications in the complex and extra complex level . Data2Vis and nvBench demonstrate the largest num - ber of specifications classified as medium ( i . e . , 4 , 318 ) and easy ( i . e . , 6 , 354 ) , respectively . Our dataset also exhibits the highest average depth of JSON structure ( i . e . , 5 . 19 ) , while Chartseer showcases the highest average branching factor ( i . e . , 1 . 44 ) . NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Lastly , with respect to diversity , our dataset demonstrates the largest total number of unique keys and the highest average pair - wise edit distance among all datasets . Furthermore , our dataset includes the largest number of specifications featuring composite views ( i . e . , 1 , 010 ) and interactions ( i . e . , 746 ) , exceeding the Vega - Lite gallery dataset by 1 . 8 and 5 . 3 times , respectively . None of the synthetic datasets or Kim et al . ’s dataset include specifications with composite views and interactions . Both our dataset and the Vega - Lite gallery dataset cover the widest variety of chart types , encompassing ten types : Area , Bar , Circle , Diagram , Distribution , Grid & Matrix , Line , Map , Point , and Trees & Networks . Please refer to Table 5 for detailed results . 5 EXPERIMENTS In this section , we introduce quantitative analysis of our generated NL datsets , lexical analysis on generated utterances , and types of low - level tasks in generated questions . 5 . 1 Experimental Setup Our experiment aims to investigate the effectiveness of our frame - work in generating diverse NL datasets from Vega - Lite specifica - tions , with a focus on accuracy and diversity . To achieve this , we apply tailored metrics to each NL dataset , taking into account their different characteristics . Specifically , we evaluate accuracy for L1 / L2 captions , as they primarily serve the objective conveyance of chart information . We assess the diversity of utterances and questions , as it is important to reflect inclusive language usage among individu - als with different background . The results are presented in Table 6 and Table 7 where each type of NL dataset is classified with capital English letter ( A - G ) . 5 . 1 . 1 Benchmarks . Forutterancesandquestions , weutilizedcrowd - sourced NL datasets gathered in prior studies [ 31 , 77 ] ( F - BM and G - BM ) . In case of utterance dataset , we only used the singleton case , so it was 804 sentences instead of 893 . However , when it comes to captions , we encountered a challenge in finding suitable benchmarks for comparing with different caption levels . Previous research employed bitmap images of charts [ 44 , 82 ] , whereas our approach leverages Vega - Lite specifications . This difference in data format prevented us from making an exact comparison . 5 . 1 . 2 Gold Standard Datasets . Given that benchmarks mostly fo - cus on simple and medium level complexity with confined diversity , we decided to make a gold standard dataset to test the generalizable performance of our framework over diverse and complex charts . We referred to previous works [ 32 , 38 ] that have demonstrated how to create gold standard datasets . We selected 48 Vega - Lite specifications ( Figure 3 ) by stratified sampling , taking into account their complexity level and whether they included interaction or composite views . Subsequently , three visualization experts ( first three authors ) collaborated to develop three guidelines for gen - erating utterances and questions . These guidelines were crafted by referring to relevant suggestions and guidelines from prior re - search [ 31 , 33 , 77 , 85 ] . We began by creating sample utterances and questions for the same chart using the initial drafts , and jointly revised each guideline by reviewing the generated NL datasets . After making consensus about the final guidelines Appendix C , we divided the charts into thirds , with each person tasked with generating NL datasets for their assigned charts . This resulted in 48 utterances ( comprising 16 commands , 16 queries , and 16 questions ) and 80 questions ( including 16 non - visual lookup , 16 non - visual compositional , 16 visual lookup , 16 visual compositional , and 16 open - ended questions ) per each expert . After one expert created NL datasets for the assigned charts , the other two individuals conducted verification to find any issues or errors within these generated NL datasets . In cases where issues or errors were detected , all three experts convened to discuss and reach a consensus on how to ad - dress them . This collaborative effort resulted in the generation of 144 utterances with three different phrasings and 240 questions categorized into five types ( D - Gold and E - Gold ) . 5 . 1 . 3 LLM - generated Datasets . To generate our datasets , we used an official API of GPT4 4 with the gpt - 4 - 0613 model . We set the temperature to 0 . 0 , to solely observe the influence of our paraphras - ing technique on diversity . We used different prompt for generating each dataset and paraphrasing the generated NL datasets ( see Ap - pendix A and Section 3 . 3 ) . To be more specific , we generated all types of chart semantics , captions , utterances , and questions for the 48 sample charts , as well as all types of utterances for 30 charts from the benchmark . This resulted in a total of 432 chart semantics ( A - LLM ) , 48 L1 captions ( B - LLM ) , 48 L2 captions ( C - LLM ) , 144 ut - terances ( D - LLM ) , and 240 questions ( E - LLM ) for the 48 sampled charts , and 90 utterances for the 30 benchmark charts ( F - LLM ) . Since the benchmark [ 31 ] did not include open - ended questions , we generated only four types of questions . This led to a total of 208 questions for the 52 charts ( G - LLM ) . We augmented our NL datasets for utterances and questions using the generated NL datasets ( * - LLM ) and the score - based para - phrasingtechnique , resultinginaugmentedparaphrasedNLdatasets ( * - LLM + P and * - LLM + P2 ) . With four language axes and five Likert - scale values ( 1 - 5 ) , it is possible to generate 20 different versions ( 4 * 5 ) of paraphrased sentences for each original sentence ( i . e . , LLM + P ) . Likewise , in case of two axes , there are six combinations chosen from the four axes . Since there are five Likert - scale options for each axis , this leads to the generation of 150 ( 6 * 5 * 5 ) different para - phrased sentence versions per original sentence ( i . e . , LLM + P2 ) . We meticulously generated all possible paraphrases and selected five distinct sets of NL datasets to mitigate any sampling bias . Thus we calculated metrics and their averages and standard deviations across these five sample sets . When sampling the paraphrased sentences , our goal is to com - pare the syntactic diversity of different NL datasets while aligning the semantic diversity of the two datasets being compared to en - sure a fair comparison . To this end , we adjust the frequency of each chart - NL pair in both datasets . This is necessary because the benchmark data exhibit biases in NL sentence distribution for each chart . For instance , one chart has 30 associated questions , while an - other chart has only one question . We count the frequency of each chart - NL pair and reflect the same frequency when augmenting the datasets . This became an issue when creating G - LLM + P , since one chart has 30 questions , which exceeds the maximum number 4 https : / / platform . openai . com / docs / models / gpt - 4 Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Table 6 : Accuracy of the generated chart semantics and L1 / L2 captions for 48 sample charts ( Figure 3 ) . Although 41 out of the 48 sample charts used in our experiment are complex and extra complex , LLMs were able to capture chart semantics and generate L1 / L2 captions successfully in general . Metadata Accuracy NL Type ( # ) Source Chart / NL # w / Strict criteria w / Lenient criteria A . Chart Semantics ( 9 ) LLM 48 / 432 89 . 4 % 96 . 9 % B . L1 Caption ( 1 ) LLM 48 / 48 76 . 0 % 95 . 8 % C . L2 Caption ( 1 ) LLM 48 / 48 76 . 0 % 87 . 5 % Table 7 : Quantitative comparison of benchmarks and LLM - generated utterances and questions . Two type of metrics were adopted , cross - distribution , which is to compare the two distributions to get the similarity and difference , and within - distribution , which is to compare the diversity within a single distribution . Each NL dataset has come from 4 sources , gold standard or benchmarks , LLM , LLM + P ( paraphrased ) , LLM + P2 ( paraphrased with 2 axes ) . The best metric from all sources are bold , while the best metric in ours ( LLM , LLM + P , LLM + P2 ) are underlines . Metadata Cross - Distribution Within - Distribution NL Type ( # ) Source Chart / NL # FD ( ↓ ) Precision ( ↑ ) Recall ( ↑ ) RC ( ↑ ) Chamfer ( ↑ ) MST ( ↑ ) Span ( ↑ ) Sparsness ( ↑ ) Entropy ( ↑ ) D . Utterance ( 3 ) Gold 48 / 144 · · · 3 . 15 0 . 19 47 . 59 3 . 26 2 . 34 2 . 60 LLM 48 / 144 0 . 58 0 . 81 0 . 31 3 . 31 0 . 19 49 . 81 3 . 41 2 . 48 2 . 54 LLM + P 0 . 45±0 . 01 0 . 81±0 . 01 0 . 66±0 . 01 3 . 65±0 . 36 0 . 16±0 . 01 54 . 58±3 . 63 3 . 52±0 . 21 2 . 70±0 . 22 2 . 09±0 . 51 LLM + P2 0 . 46±0 . 00 0 . 80±0 . 02 0 . 67±0 . 03 3 . 43±0 . 38 0 . 16±0 . 01 52 . 91±3 . 11 3 . 50±0 . 22 2 . 49±0 . 20 2 . 27±0 . 35 E . Question ( 5 ) Gold 48 / 240 · · · 3 . 47 0 . 17 70 . 28 3 . 45 2 . 64 2 . 44 LLM 48 / 240 0 . 35 0 . 84 0 . 56 6 . 20 0 . 09 105 . 16 5 . 92 4 . 40 1 . 68 LLM + P 0 . 35±0 . 00 0 . 76±0 . 02 0 . 64±0 . 03 4 . 17±0 . 20 0 . 13±0 . 01 70 . 00±3 . 47 4 . 28±0 . 30 3 . 13±0 . 11 2 . 51±0 . 10 LLM + P2 0 . 36±0 . 00 0 . 74±0 . 02 0 . 64±0 . 03 4 . 29±0 . 34 0 . 14±0 . 01 77 . 36±5 . 91 4 . 44±0 . 27 3 . 12±0 . 19 2 . 21±0 . 34 F . Utterance ( 3 ) BM [ 77 ] 30 / 804 · · · 10 . 42 0 . 07 177 . 63 10 . 56 8 . 41 2 . 42 LLM 30 / 90 · · · · · · · · - LLM + P 30 / 804 1 . 11±0 . 27 0 . 63±0 . 03 0 . 51±0 . 05 12 . 36±0 . 18 0 . 06±0 . 00 209 . 59±7 . 07 11 . 74±0 . 45 9 . 66±0 . 38 2 . 40±0 . 06 LLM + P2 30 / 804 0 . 87±0 . 56 0 . 58±0 . 04 0 . 43±0 . 08 12 . 24±0 . 18 0 . 06±0 . 00 227 . 70±15 . 66 11 . 86±0 . 37 9 . 79±0 . 19 2 . 45±0 . 07 G . Question ( 4 ) BM [ 31 ] 52 / 629 · · · 8 . 66 0 . 07 202 . 83 11 . 36 6 . 12 1 . 96 LLM 52 / 208 · · · · · · · · - LLM + P 52 / 619 0 . 33±0 . 00 0 . 52±0 . 01 0 . 13±0 . 01 11 . 95±0 . 27 0 . 05±0 . 00 247 . 16±11 . 72 12 . 46±0 . 49 9 . 07±0 . 32 2 . 41±0 . 14 LLM + P2 52 / 629 0 . 33±0 . 00 0 . 50±0 . 01 0 . 18±0 . 01 11 . 61±0 . 18 0 . 06±0 . 00 222 . 39±8 . 21 12 . 69±0 . 24 8 . 73±0 . 25 2 . 39±0 . 06 of paraphrases possible ( limited to 20 ) through our single - axis para - phrasing method . As a result , our overall number of NL datasets reaches 619 . Last , we included open - ended questions in E - LLM + P and E - LLM + P2 , as these questions were available in E - Gold . However , we did not include them in G - LLM + P and G - LLM + P2 in Table 7 to preserve the semantic diversity of the datasets . 5 . 1 . 4 Procedure . We manually grade chart semantics and L1 / L2 captions to compute their accuracy . To enhance the reliability of our scoring , two experts ( the first and second authors ) independently scored them and calculated the average score . Specifically , the chart semantics include whether they contain composite views , the type of composite view , the number of plots , chart type , mark , trans - form , encoding , style , and interaction . We scored whether each of them is correct or not . However , during our evaluation of style , we encountered many cases where multiple width or height values were defined within the Vega - Lite specification . In such cases , we chose to exclude the width and height information from our style evaluation . Moreover , we encountered many cases that were hard to definitively categorize as either correct or incorrect . For instance , situations where nine lines were drawn on the same chart but di - vided into separate layers , resulting in a count of nine plots instead of one . As a result , we adopted two different scoring approaches , consisting of strict and lenient criteria . Strict criteria only considers those that were 100 % accurate . For instance , if a stacked bar chart was categorized as a bar chart , it was deemed incorrect . Conversely , with lenient criteria , we adopted a more flexible approach , con - sidering the aforementioned cases as correct . We extended these criteria to the evaluation of L1 / L2 captions as well as their formal definitions [ 44 ] . As they contain objective information , we applied the same two criteria and reasoning to assess their accuracy . To assess the quality of utterances and questions in compar - ison to both the benchmark and the gold standard dataset , we employ two types of statistical metrics : within - distribution and cross - distribution metrics . The within - distribution metrics are de - signed to calculate the similarity and divergence between a given dataset and another dataset by means of comparison . Examples of such metrics include Frechet distance ( FD ) , precision , and recall . NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Table 8 : Types of low - level tasks in questions Top four ranked in both datasets were identical , while LLM - generated dataset has more questions assigned to these ranks . Low - level Analytical Task Gold # ( % ) LLM # ( % ) Retrieve Value 94 ( 39 . 2 % ) 103 ( 42 . 9 % ) Find Extremum 35 ( 14 . 6 % ) 65 ( 27 . 1 % ) Correlate 31 ( 12 . 9 % ) 41 ( 17 . 1 % ) Compute Derived Value 31 ( 12 . 9 % ) 17 ( 7 . 1 % ) Filter 23 ( 9 . 6 % ) 3 ( 1 . 3 % ) Find Anomalies 14 ( 5 . 8 % ) 0 Characterize Distribution 5 ( 2 . 1 % ) 4 ( 1 . 7 % ) Sort 3 ( 1 . 3 % ) 0 Cluster 1 ( 0 . 4 % ) 0 Determine Range 1 ( 0 . 4 % ) 5 ( 2 . 1 % ) ETC 2 ( 0 . 8 % ) 2 ( 0 . 8 % ) Sum 240 ( 100 % ) 240 ( 100 % ) By utilizing these metrics , we can evaluate how closely a given distribution aligns with the benchmark distribution . These metrics have already been applied in the comparison of human - generated and LLM - generated datasets [ 20 ] . To this end , we vectorize the gold standard , benchmarks , and LLM - generated as well as paraphrased datasets , transforming them into sets of vectors for quantitative comparison . However , we recognize that the aforementioned metrics may not provide a comprehensive measure of the quality of LLM - generated and - paraphrased NL datasets . These metrics mainly focus on the coverage of distribution rather than emphasizing diversity . It is crucial to delve deeper into a single distribution , as duplicate or highly similar data points may be present within it [ 36 , 74 ] . To address this , we incorporate cross - distribution metrics [ 64 ] that allow us to quantify the diversity within a single distribution . These metrics include remote - clique ( average of mean pairwise distances ) , Chamfer distance ( average of minimum pairwise distances ) , MST dispersion ( sum of edge weights of MST ) , span ( Pth percentile distance to centroid ) , sparseness ( mean distance to medoid ) , and entropy ( Shannon - Wiener index for points in a grid partition ) . 5 . 2 Quantitative Results We first report the accuracy of chart semantics and L1 / L2 captions . Under the strict criteria , the accuracy rates for chart semantics , L1 captions , and L2 captions were 89 . 4 % , 76 . 0 % , and 76 . 0 % , respectively . In detail , accuracy under strict criteria reveals that ‘chart - type’ achieved the lowest accuracy at 75 % , while ‘mark’ and ‘interac - tion’ showed the highest accuracy at 96 . 9 % . Under lenient criteria , the accuracy rates for chart semantics , L1 captions , and L2 cap - tions significantly improved to 96 . 9 % , 95 . 8 % , and 87 . 5 % , respectively . Specifically , the lowest accuracy for chart semantics was observed in the ‘number of plots’ ( 88 . 5 % ) , while ‘mark’ and ‘interaction’ main - tained the highest accuracy at 100 % . Additionally , the accuracy of chart type substantially improved to 97 . 9 % . A summary of these results is provided in Table 6 . We next report the diversity of utterance and question . In terms of cross - distribution metrics , LLM + P exhibited the highest quality in terms of precision ( D ) , precision and recall ( F ) , and precision and FD ( G ) . In case of datasets containing five question types ( E ) , the metric results were not consistent . Specifically , LLM + P performed the best in FD , LLM was the best for precision , and LLM + P2 achieved the highest recall . When considering within - distribution metrics , LLM - generated and paraphrased datasets demonstrated greater diversity compared to the gold standard and benchmark datasets . On average , higher diversity was observed in 4 . 75 out of six metrics . For both question and utterance datasets ( E , F ) , paraphrased datasets with two axes demonstrated greater diversity than paraphrased datasets with one axis in four out of six metrics . Conversely , in the other two datasets ( D , G ) , paraphrased datasets with one axis exhibited higher diversity in four out of six metrics . In the utterance dataset ( D ) , paraphrasing increased diversity in four out of six metrics , whereas in the question dataset ( E ) , paraphrasing reduced diversity in four metrics . A summary of the results is presented in Table 7 . 5 . 3 Lexical Analysis in Utterances To gain a deeper understanding of the syntactic diversity in LLM - generated datasets , we conducted a lexical analysis on three NL datasets ( F - BM , F - LLM + P , F - LLM + P2 ) to investigate the types of words used within each dataset . Our pre - processing steps encom - passed sentence tokenization , converting all text to lowercase , re - moval of stopwords , and lemmatization . As evidenced by the quan - titative outcome presented in the previous section , the LLM + P exhibited a notable richness in its lexical diversity . It contained a total of 555 unique words , surpassing the benchmark dataset’s count of 349 unique words . Also , the total word count in the LLM + P , amounting to 7 , 132 words , exceeded that of the benchmark dataset , which consisted of 4 , 480 words . In case of LLM + P2 , it demonstrated an even greater number of unique words , totaling 608 , surpassing both the benchmark and LLM + P datasets in this regard . However , the overall word count in LLM + P2 was lower at 6 , 645 words com - pared to the LLM + P dataset ( 7 , 132 words ) . There were some additional patterns in the use of specific words employed within the LLM - generated datasets . First , the paraphrased dataset introduced a multitude of new action verbs . For instance , when issuing commands , terms such as construct , fabricate , or - ganize , and arrange were employed to create charts ( e . g . , ‘Fabri - cate a line diagram’ ) . In previous work [ 77 ] , there was a tendency among crowd workers to adhere to specific terminology , thus re - searchers have to be careful when providing instructions for col - lecting datasets . Our paraphrasing technique effectively addresses this issue by promoting diverse syntax through the use of various action verbs automatically . Second , the datasets incorporate words that may be adopted by people of specific groups or domains , but not used often by ordinary people , such as domain - specific jargon ( e . g . , provenance , bifurcated , barometric , pecuniary ) . Last , certain words have been adopted to introduce diverse tones and voices of the speaker . These encompass terms of a more personal and informal nature , as well as expressions that convey uncertainty and speculation ( e . g . , maybe , seems , might , quite , sure ) , as well as Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . words that have been included to enhance conversational aspects ( e . g . , possibly , would , could ) . 5 . 4 Types of Low - level Tasks in Questions Based on a taxonomy [ 2 ] comprising ten low - level analytical tasks , we conducted an analysis of the question types present in the gold standard and LLM - generated questions ( E - Gold and E - LLM ) . This analysis aimed to assess the dissimilarities or similarities between these questions . To this end , we associated each low - level analytical task with individual questions within both datasets . Both datasets exhibited a congruent pattern , with identical rank - ings for the top four elements . The task with the highest frequency in both datasets is retrieve value , which is unsurprising , as it con - sists of 40 % of lookup questions in the dataset . Notably , in the LLM - generated dataset , the second most prevalent task is find extremum at 27 . 1 % . This percentage closely aligns with Kim et al . ’s observation [ 31 ] , where they reported a similar prevalence of questions related to extrema at 26 . 7 % . Furthermore , it is worth highlighting that , akin to their research , there is a clear bias towards certain task types , including retrieve value , find extremum , correlate , and compute derived value Table 8 . 6 DISCUSSION 6 . 1 Strengths and Weaknesses of LLM Framework 6 . 1 . 1 The Framework Can Guide Itself via Key Questions . We ob - served several interesting key questions discovered by LLMs that play a vital role in guiding themselves . They were formulated through a meticulous analysis of chart contents . Various decision - making processes were identified , spanning diverse domains such as financial decision - making ( e . g . , assessing whether to invest in a company’s stock ) , public policy planning ( e . g . , formulating policies based on employment trends across different age groups and coun - tries ) , and location - based business strategies ( e . g . , selecting optimal sites for a new shoe factory relative to the distribution of existing facilities ) . These key questions served as the foundation for eliciting subsequent conclusions , retrieving specific values , and deciding mathematical operations for generating interesting questions . 6 . 1 . 2 The Framework Works Robustly on Different Chart Complex - ity . Considering that the 48 sampled charts mostly belong to the categories of complex and extra - complex charts , our observations indicate that the reported accuracy ( Table 6 ) pertaining to chart semantics and L1 / L2 captions does not exhibit a dependence on the complexity levels of the Vega - Lite specifications . This finding un - derscores the robustness of the system . For instance , it successfully generated an accurate L1 caption for a chart comprising two views interconnected through selection interactions ( see Figure 4 - a ) . Sim - ilarly , it effectively generated an L2 caption for a chart containing multiple plots , allowing the selection of a data range in the main bar plot to trigger the highlighting of related data points in other plots ( see Figure 4 - d ) . 6 . 1 . 3 The Framework Depends Highly on Vega - Lite Specifications . We observed that the framework is highly dependent on the Vega - Lite specifications in generating NL datasets . In many cases , this dependency is advantageous as it enables a focus on intricate func - tionalities such as interactions . For a particular chart , determining the presence of interactions was challenging because the selection interaction was indicated solely by the color label of the chart . Nevertheless , the framework successfully captured this ( see Fig - ure 4 - b ) . Similarly , charts lacking titles or descriptions can pose a challenge in comprehending the content of charts . However , it appears that the framework was able to extract additional informa - tion , even utilizing the URL of the data included in the specification ( e . g . , how - did - levels - of - uk - hate - crime - change - during - and - after - covid - 19 / data / f5 . csv ) , enabling the generation of informative and coherent captions ( see Figure 4 - c ) . However , we have identified certain cases where relying solely on Vega - Lite specifications proves disadvantageous . First , in some instances , the generated NL datasets include information that was not visually represented in the chart but was present in the Vega - Lite specifications . For instance , additional categories or values that exceed the specified axis range were presented in the NL dataset . Second , if certain information is not explicitly stated within the Vega - Lite specification , it cannot be incorporated into the NL dataset . For example , when generating trellis plots , the number of plots is determined using the unique count of elements . However , since the number is not explicitly provided in the specification , our framework is unable to predict the exact plot number accurately . Last , any errors present in the Vega - Lite specifications are faithfully represented in the generated NL datasets . For instance , a specifi - cation contained a typo that divided facets into 3 parts but was mistakenly denoted as 4 , our framework predicted the number of plots as 4 instead of the correct 3 . Similarly , the Vega - Lite specifica - tion included code that is non - functional , which is reflected in the generated captions , resulting in inaccuracies . 6 . 1 . 4 The Framework Predicts Only Common Chart Types . Our categorization relied on the chart type taxonomy presented by Borkin et al . [ 5 ] , which led to different categorizations even when the same mark was used . For instance , although the same point mark was employed , it could be interpreted as either a distribution chart ( e . g . , dot array ) or a scatter plot . Furthermore , we conducted an analysis of the charts by considering their detailed sub - types rather than grouping them into larger categories . For instance , a chart featuring a stacked area chart was considered incorrect according to strict criteria if it was predicted as an area chart . However , we observed that in most cases , the LLM framework tended to assign the charts to the most prevalent and common chart types such as scatterplots , area charts , and bar charts , rather than classifying them as distribution charts , stacked area charts , or stacked bar charts ( Figure 4 - c ) . With this reasons , the chart type exhibited the highest accuracy gap between strict and lenient criteria . 6 . 2 Limitations and Future Work 6 . 2 . 1 Enriching Capabilities of the Framework through External Information or Human - in - the - Loop Approach . While Vega - Lite spec - ifications serve as powerful inputs for generating various types of NL datasets , it is inherently challenging to extract information that does not exist within these specifications . Considering that our framework operates in a fully automatic manner , it does not rely on any external resources . However , this limitation can potentially NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY L1 Caption : This visualization consists of two views . The first view is a geoshape chart that shows the geographical distribution of FRV districts . The districts are colored based on the percentage of responses above the 7 . 7 - minute standard . The user can select a quarter and a district to view detailed information . The second view is a line chart that ranks the stations based on the percentage of responses above the standard . The stations are represented by lines and circles , and the user can select a station to highlight it . The chart also includes text marks that label the stations and their rankings . L1 caption : This is a Stacked Area Chart that visualizes the GDP of G7 countries from 1952 to 2017 . The x - axis represents the year , and the y - axis represents the total GDP . Each country is represented by a different color . The areas are semi - transparent , but become opaque when selected from the legend . A tooltip provides additional information when an area is hovered over . L1 caption : This layered scatter plot visualizes data about hate crimes in the UK , specifically focusing on women . The scatter plot uses points to represent the ' value ' of each record , with different shapes indicating different ' models ' . The points are colored and filled based on the ' group ' field . A rule mark is used to indicate the range between ' min ' and ' max ' for each ' groupaction ' . The color scheme used is ' dark2 ' . L2 Caption : The visualization shows the total number of voters in each department of Colombia , with Bogota D . C . having the highest number of voters at 5 , 702 , 805 and Vaupes having the least at 21 , 537 . The average abstention percentage across all departments is approximately 50 . 21 % , indicating that about half of the total voters did not vote . The difference between the total number of voters and the actual voters varies significantly across departments , with Bogota D . C . having the highest difference of 2 , 130 , 107 and Vaupes having the least difference of 13 , 482 . a b c d Figure 4 : Four examples of generated L1 / L2 captions with corresponding charts . We found that our framework can successfully generate captions even on complex charts with varying interactions and multiple views . impact the performance of NL generation , as it aligns with obser - vations in guided discovery , where insufficient prior knowledge can hinder learners from formulating hypotheses , interpreting data effectively , and engaging in systematic experimentation [ 14 ] . To enhance the capabilities of our LLM framework , we suggest accessing external sources of information or adopting a human - in - the - loop approach to guide the process during NL dataset gener - ation . For instance , generating L3 / L4 captions often necessitates access to common or domain - specific knowledge [ 44 ] . In this re - gard , employing tools like ReAct [ 92 ] becomes advantageous , as it facilitates reasoning to assist the model in deducing , tracking , and updating action plans while also handling exceptions . This enables us to proactively retrieve information from the web when required . Similarly , in the context of providing guidance , domain experts can contribute by specifying particular directions they wish to proceed . By providing key questions , they can guide the model and steer its focus accordingly . This collaborative approach between auto - mated NL generation and human expertise offers the potential to address knowledge gaps and improve the quality and relevance of generated NL datasets . 6 . 2 . 2 Augmenting Vega - Lite Specifications . While we have pre - sented the largest amount of Vega - Lite specifications and acknowl - edge their ability as input for generating diverse NL datasets , it is noteworthy that the quantity of Vega - Lite specifications is signifi - cantly smaller compared to bitmap images . This is mainly because collecting Vega - Lite specifications is more challenging when com - pared to other formats . This limitation hinders the effective training or fine - tuning of machine learning models to achieve robust perfor - mance . Consequently , we posit the need for methods to augment Vega - Lite specifications . Various augmentation techniques have been introduced and adopted for bitmap images of charts to in - crease both their quantity [ 28 ] and diversity [ 96 ] . However , to the best of our knowledge , we have not found any pertinent research that addresses the augmentation of Vega - Lite specifications . As part of our future work , we aim to tackle this gap by developing a re - verse engineering technique [ 61 ] specifically designed for Vega - Lite specifications . 7 CONCLUSION In this study , we introduce an LLM framework designed to generate diverse NL datasets aimed at enhancing NLIs for data visualization research . Our framework takes a Vega - Lite specification as only input and employs multiple prompting techniques to accurately generate various NL datasets , including captions , utterances , and questions . We also propose a score - based paraphrasing approach to enhance the syntactic diversity of the generated NL datasets . To demonstrate the robust and generalizable performance of our framework , we present a dataset comprising 1 , 981 Vega - Lite specifi - cations . This dataset surpasses the baselines in terms of complexity and diversity . Our experimental results substantiate that the frame - work excels in accurately generating both L1 and L2 captions , while achieving higher diversity in the generation of utterances and ques - tions compared to the baselines . We hope our framework and chart Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . dataset can advance research in developing diverse NLIs for data visualization . REFERENCES [ 1 ] Rinat Abdrashitov , Fanny Chevalier , and Karan Singh . 2020 . Interactive Explo - ration and Refinement of Facial Expression Using Manifold Learning . In Proceed - ingsofthe33rdAnnualACMSymposiumonUserInterfaceSoftwareandTechnology ( VirtualEvent , USA ) ( UIST’20 ) . AssociationforComputingMachinery , NewYork , NY , USA , 778 – 790 . https : / / doi . org / 10 . 1145 / 3379337 . 3415877 [ 2 ] Robert Amar , James Eagan , and John Stasko . 2005 . Low - level components of analytic activity in information visualization . In IEEE Symposium on Information Visualization , 2005 . INFOVIS 2005 . IEEE , 111 – 117 . [ 3 ] Toshiki Aoki , Rintaro Chujo , Katsufumi Matsui , Saemi Choi , and Ari Hautasaari . 2022 . EmoBalloon - Conveying Emotional Arousal in Text Chats with Speech Balloons . In CHI Conference on Human Factors in Computing Systems . 1 – 16 . [ 4 ] Michael S Bernstein , Greg Little , Robert C Miller , Björn Hartmann , Mark S Ackerman , David R Karger , David Crowell , and Katrina Panovich . 2010 . Soylent : a word processor with a crowd inside . In Proceedings of the 23nd annual ACM symposium on User interface software and technology . 313 – 322 . [ 5 ] MichelleABorkin , AzaleaAVo , ZoyaBylinskii , PhillipIsola , ShashankSunkavalli , Aude Oliva , and Hanspeter Pfister . 2013 . What makes a visualization memorable ? IEEEtransactionsonvisualizationandcomputergraphics 19 , 12 ( 2013 ) , 2306 – 2315 . [ 6 ] Ann L Brown and Joseph C Campione . 1994 . Guided discovery in a community of learners . The MIT Press . [ 7 ] Stuart K Card , Jock Mackinlay , and Ben Shneiderman . 1999 . Readings in informa - tion visualization : using vision to think . Morgan Kaufmann . [ 8 ] Harrison Chase . 2022 . LangChain . https : / / github . com / hwchase17 / langchain [ 9 ] Ritwick Chaudhry , Sumit Shekhar , Utkarsh Gupta , Pranav Maneriker , Prann Bansal , and Ajay Joshi . 2020 . Leaf - qa : Locate , encode & attend for figure question answering . In Proceedings of the IEEE / CVF Winter Conference on Applications of Computer Vision . 3512 – 3521 . [ 10 ] ChenChenandZhichengLiu . 2023 . TheStateoftheArtinCreatingVisualization Corpora for Automated Chart Analysis . Computer Graphics Forum ( 2023 ) . https : / / doi . org / 10 . 1111 / cgf . 14855 [ 11 ] Xi Chen , Wei Zeng , Yanna Lin , Hayder Mahdi Ai - Maneea , Jonathan Roberts , and Remco Chang . 2020 . Composition and configuration patterns in multiple - view visualizations . IEEE Transactions on Visualization and Computer Graphics 27 , 2 ( 2020 ) , 1514 – 1524 . [ 12 ] Kenneth Cox , Rebecca E Grinter , Stacie L Hibino , Lalita Jategaonkar Jagadeesan , and David Mantilla . 2001 . A multi - modal natural language interface to an infor - mation visualization environment . International Journal of Speech Technology 4 ( 2001 ) , 297 – 314 . [ 13 ] Kenny Davila , Srirangaraj Setlur , David Doermann , Bhargava Urala Kota , and Venu Govindaraju . 2020 . Chart mining : A survey of methods for automated chart analysis . IEEE transactions on pattern analysis and machine intelligence 43 , 11 ( 2020 ) , 3799 – 3819 . [ 14 ] Ton De Jong and Wouter R Van Joolingen . 1998 . Scientific discovery learning with computer simulations of conceptual domains . Review of educational research 68 , 2 ( 1998 ) , 179 – 201 . [ 15 ] Biplab Deka , Zifeng Huang , Chad Franzen , Joshua Hibschman , Daniel Afergan , Yang Li , Jeffrey Nichols , and Ranjitha Kumar . 2017 . Rico : A mobile app dataset for building data - driven design applications . In Proceedings of the 30th annual ACM symposium on user interface software and technology . 845 – 854 . [ 16 ] VictorDibia . 2023 . LIDA : AToolforAutomaticGenerationofGrammar - Agnostic Visualizations and Infographics using Large Language Models . arXiv preprint arXiv : 2303 . 02927 ( 2023 ) . [ 17 ] Victor Dibia and Çağatay Demiralp . 2019 . Data2vis : Automatic generation of data visualizations using sequence - to - sequence recurrent neural networks . IEEE computer graphics and applications 39 , 5 ( 2019 ) , 33 – 46 . [ 18 ] Siwei Fu , Kai Xiong , Xiaodong Ge , Siliang Tang , Wei Chen , and Yingcai Wu . 2020 . Quda : natural language queries for visual data analytics . arXiv preprint arXiv : 2005 . 03257 ( 2020 ) . [ 19 ] Tong Gao , Mira Dontcheva , Eytan Adar , Zhicheng Liu , and Karrie G Karahalios . 2015 . Datatone : Managing ambiguity in natural language interfaces for data visualization . In Proceedings of the 28th annual acm symposium on user interface software & technology . 489 – 500 . [ 20 ] Perttu Hämäläinen , Mikke Tavast , and Anton Kunnari . 2023 . Evaluating large language models in generating synthetic hci research data : a case study . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1 – 19 . [ 21 ] Jonathan Harper and Maneesh Agrawala . 2017 . Converting basic D3 charts into reusable style templates . IEEE transactions on visualization and computer graphics 24 , 3 ( 2017 ) , 1274 – 1286 . [ 22 ] CindyEHmelo - Silver , RavitGolanDuncan , andClarkAChinn . 2007 . Scaffolding and achievement in problem - based and inquiry learning : a response to Kirschner , Sweller , and . Educational psychologist 42 , 2 ( 2007 ) , 99 – 107 . [ 23 ] Enamul Hoque , Parsa Kavehzadeh , and Ahmed Masry . 2022 . Chart question answering : State of the art and future directions . In Computer Graphics Forum , Vol . 41 . Wiley Online Library , 555 – 572 . [ 24 ] Enamul Hoque , Vidya Setlur , Melanie Tory , and Isaac Dykeman . 2017 . Applying pragmatics principles for interaction with visual analytics . IEEE transactions on visualization and computer graphics 24 , 1 ( 2017 ) , 309 – 318 . [ 25 ] Kevin Hu , Diana Orghian , and César Hidalgo . 2018 . DIVE : A mixed - initiative system supporting integrated data exploration workflows . In Proceedings of the workshop on human - in - the - loop data analytics . 1 – 7 . [ 26 ] Maeve Hutchinson , Aidan Slingsby , Radu Jianu , and Pranava Madhyastha . 2023 . TowardsVisualisationSpecificationsfromMultilingualNaturalLanguageQueriesusingLargeLanguageModels . In EuroVis 2023 - Posters , Christina Gillmann , Michael Krone , and Simone Lenti ( Eds . ) . The Eurographics Association . https : / / doi . org / 10 . 2312 / evp . 20231072 [ 27 ] Ziwei Ji , Nayeon Lee , Rita Frieske , Tiezheng Yu , Dan Su , Yan Xu , Etsuko Ishii , Ye Jin Bang , Andrea Madotto , and Pascale Fung . 2023 . Survey of hallucination in natural language generation . Comput . Surveys 55 , 12 ( 2023 ) , 1 – 38 . [ 28 ] Daekyoung Jung , Wonjae Kim , Hyunjoo Song , Jeong - in Hwang , Bongshin Lee , Bohyoung Kim , and Jinwook Seo . 2017 . Chartsense : Interactive data extraction from chart images . In Proceedings of the 2017 chi conference on human factors in computing systems . 6706 – 6717 . [ 29 ] Sean Kandel , Ravi Parikh , Andreas Paepcke , Joseph M Hellerstein , and Jeffrey Heer . 2012 . Profiler : Integrated statistical analysis and visualization for data quality assessment . In Proceedings of the International Working Conference on Advanced Visual Interfaces . 547 – 554 . [ 30 ] Shankar Kantharaj , Rixie Tiffany Ko Leong , Xiang Lin , Ahmed Masry , Megh Thakkar , Enamul Hoque , and Shafiq Joty . 2022 . Chart - to - text : A large - scale benchmark for chart summarization . arXiv preprint arXiv : 2203 . 06486 ( 2022 ) . [ 31 ] Dae Hyun Kim , Enamul Hoque , and Maneesh Agrawala . 2020 . Answering ques - tions about charts and generating visual explanations . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 13 . [ 32 ] Dae Hyun Kim , Enamul Hoque , Juho Kim , and Maneesh Agrawala . 2018 . Facil - itating document reading by linking text and tables . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology . 423 – 434 . [ 33 ] Dae Hyun Kim , Vidya Setlur , and Maneesh Agrawala . 2021 . Towards understand - ing how readers integrate charts and captions : A case study with line charts . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 11 . [ 34 ] Robert Kincaid and Graham Pollock . 2017 . Nicky : Toward a virtual assistant for test and measurement instrument recommendations . In 2017 IEEE 11th Interna - tional Conference on Semantic Computing ( ICSC ) . IEEE , 196 – 203 . [ 35 ] Aniket Kittur , Boris Smus , Susheel Khamkar , and Robert E Kraut . 2011 . Crowd - forge : Crowdsourcing complex work . In Proceedings of the 24th annual ACM symposium on User interface software and technology . 43 – 52 . [ 36 ] Mark Klein and Ana Cristina Bicharra Garcia . 2015 . High - speed idea filtering with the bag of lemons . Decision Support Systems 78 ( 2015 ) , 39 – 50 . [ 37 ] Hyung - Kwon Ko , Subin An , Gwanmo Park , Seung Kwon Kim , Daesik Kim , Bo - hyoung Kim , Jaemin Jo , and Jinwook Seo . 2022 . We - toon : A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision . In The 35th Annual ACM Symposium on User Interface Software and Technology . 1 – 14 . [ 38 ] Nicholas Kong , Marti A Hearst , and Maneesh Agrawala . 2014 . Extracting refer - ences between text and charts via crowdsourcing . In Proceedings of the SIGCHI conference on Human Factors in Computing Systems . 31 – 40 . [ 39 ] Yixuan Li , Yusheng Qi , Yang Shi , Qing Chen , Nan Cao , and Siming Chen . 2022 . Diverse interaction recommendation for public users exploring multi - view visu - alization using deep learning . IEEE Transactions on Visualization and Computer Graphics 29 , 1 ( 2022 ) , 95 – 105 . [ 40 ] Zhenyu Li , Sunqi Fan , Yu Gu , Xiuxing Li , Zhichao Duan , Bowen Dong , Ning Liu , and Jianyong Wang . 2023 . FlexKBQA : A Flexible LLM - Powered Framework for Few - Shot Knowledge Base Question Answering . arXiv preprint arXiv : 2308 . 12060 ( 2023 ) . [ 41 ] Can Liu , Yun Han , Ruike Jiang , and Xiaoru Yuan . 2021 . Advisor : Automatic visualization answer for natural - language question on tabular data . In 2021 IEEE 14th Pacific Visualization Symposium ( PacificVis ) . IEEE , 11 – 20 . [ 42 ] Fangyu Liu , Julian Martin Eisenschlos , Francesco Piccinno , Syrine Krichene , ChenxiPang , KentonLee , MandarJoshi , WenhuChen , NigelCollier , andYasemin Altun . 2022 . DePlot : One - shot visual language reasoning by plot - to - table transla - tion . arXiv preprint arXiv : 2212 . 10505 ( 2022 ) . [ 43 ] Fangyu Liu , Francesco Piccinno , Syrine Krichene , Chenxi Pang , Kenton Lee , Mandar Joshi , Yasemin Altun , Nigel Collier , and Julian Martin Eisenschlos . 2022 . Matcha : Enhancing visual language pretraining with math reasoning and chart derendering . arXiv preprint arXiv : 2212 . 09662 ( 2022 ) . [ 44 ] Alan Lundgard and Arvind Satyanarayan . 2021 . Accessible visualization via natural language descriptions : A four - level model of semantic content . IEEE transactions on visualization and computer graphics 28 , 1 ( 2021 ) , 1073 – 1083 . [ 45 ] Yuyu Luo , Xuedi Qin , Nan Tang , and Guoliang Li . 2018 . Deepeye : Towards automatic data visualization . In 2018 IEEE 34th international conference on data NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY engineering ( ICDE ) . IEEE , 101 – 112 . [ 46 ] Yuyu Luo , Nan Tang , Guoliang Li , Chengliang Chai , Wenbo Li , and Xuedi Qin . 2021 . Synthesizing natural language to visualization ( NL2VIS ) benchmarks from NL2SQL benchmarks . In Proceedings of the 2021 International Conference on Management of Data . 1235 – 1247 . [ 47 ] Yuyu Luo , Nan Tang , Guoliang Li , Jiawei Tang , Chengliang Chai , and Xuedi Qin . 2021 . Natural language to visualization by neural machine translation . IEEE Transactions on Visualization and Computer Graphics 28 , 1 ( 2021 ) , 217 – 226 . [ 48 ] Jock Mackinlay . 1986 . Automating the design of graphical presentations of relational information . Acm Transactions On Graphics ( Tog ) 5 , 2 ( 1986 ) , 110 – 141 . [ 49 ] Jock Mackinlay , Pat Hanrahan , and Chris Stolte . 2007 . Show me : Automatic presentation for visual analysis . IEEE transactions on visualization and computer graphics 13 , 6 ( 2007 ) , 1137 – 1144 . [ 50 ] Ahmed Masry , Do Xuan Long , Jia Qing Tan , Shafiq Joty , and Enamul Hoque . 2022 . ChartQA : A benchmark for question answering about charts with visual and logical reasoning . arXiv preprint arXiv : 2203 . 10244 ( 2022 ) . [ 51 ] LelandMcInnes , JohnHealy , andSteveAstels . 2017 . hdbscan : Hierarchicaldensity based clustering . J . Open Source Softw . 2 , 11 ( 2017 ) , 205 . [ 52 ] Leland McInnes , John Healy , and James Melville . 2018 . Umap : Uniform man - ifold approximation and projection for dimension reduction . arXiv preprint arXiv : 1802 . 03426 ( 2018 ) . [ 53 ] YuMeng , JiaxinHuang , YuZhang , andJiaweiHan . 2022 . Generatingtrainingdata with language models : Towards zero - shot language understanding . Advances in Neural Information Processing Systems 35 ( 2022 ) , 462 – 477 . [ 54 ] Valerie S Morash , Yue - Ting Siu , Joshua A Miele , Lucia Hasty , and Steven Landau . 2015 . Guidingnovicewebworkersinmakingimagedescriptionsusingtemplates . ACM Transactions on Accessible Computing ( TACCESS ) 7 , 4 ( 2015 ) , 1 – 21 . [ 55 ] Dominik Moritz , Chenglong Wang , Greg L Nelson , Halden Lin , Adam M Smith , Bill Howe , and Jeffrey Heer . 2018 . Formalizing visualization design knowledge as constraints : Actionable and extensible models in draco . IEEE transactions on visualization and computer graphics 25 , 1 ( 2018 ) , 438 – 448 . [ 56 ] Mohammad Amin Mozaffari , Xinyuan Zhang , Jinghui Cheng , and Jin LC Guo . 2022 . GANSpiration : Balancing Targeted and Serendipitous Inspiration in User Interface Design with Style - Based Generative Adversarial Network . In CHI Con - ference on Human Factors in Computing Systems . 1 – 15 . [ 57 ] Arpit Narechania , Adam Fourney , Bongshin Lee , and Gonzalo Ramos . 2021 . DIY : Assessingthecorrectnessofnaturallanguagetosqlsystems . In 26thInternational Conference on Intelligent User Interfaces . 597 – 607 . [ 58 ] Arpit Narechania , Arjun Srinivasan , and John Stasko . 2020 . NL4DV : A toolkit for generating analytic specifications for data visualization from natural language queries . IEEE Transactions on Visualization and Computer Graphics 27 , 2 ( 2020 ) , 369 – 379 . [ 59 ] Jason Obeid and Enamul Hoque . 2020 . Chart - to - text : Generating natural lan - guage descriptions for charts by adapting the transformer model . arXiv preprint arXiv : 2010 . 09142 ( 2020 ) . [ 60 ] Joon Sung Park , Joseph C O’Brien , Carrie J Cai , Meredith Ringel Morris , Percy Liang , and Michael S Bernstein . 2023 . Generative agents : Interactive simulacra of human behavior . arXiv preprint arXiv : 2304 . 03442 ( 2023 ) . [ 61 ] JorgePocoandJeffreyHeer . 2017 . Reverse - engineeringvisualizations : Recovering visual encodings from chart images . In Computer graphics forum , Vol . 36 . Wiley Online Library , 353 – 363 . [ 62 ] Xin Qian , Eunyee Koh , Fan Du , Sungchul Kim , Joel Chan , Ryan A Rossi , Sana Malik , and Tak Yeon Lee . 2021 . Generating accurate caption units for figure captioning . In Proceedings of the Web Conference 2021 . 2792 – 2804 . [ 63 ] Nils Reimers and Iryna Gurevych . 2019 . Sentence - BERT : Sentence Embeddings using Siamese BERT - Networks . In Proceedings of the 2019 Conference on Em - pirical Methods in Natural Language Processing . Association for Computational Linguistics . https : / / arxiv . org / abs / 1908 . 10084 [ 64 ] Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Christian Von Der Weth , and Brian Y . Lim . 2021 . Directed diversity : Leveraging language embedding dis - tances for collective creativity in crowd ideation . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 35 . [ 65 ] Andy Rosenbaum , Saleh Soltan , Wael Hamza , Amir Saffari , Macro Damonte , and Isabel Groves . 2022 . CLASP : Few - shot cross - lingual data augmentation for semantic parsing . arXiv preprint arXiv : 2210 . 07074 ( 2022 ) . [ 66 ] Arvind Satyanarayan , Dominik Moritz , Kanit Wongsuphasawat , and Jeffrey Heer . 2017 . Vega - Lite : A Grammar of Interactive Graphics . IEEE Transactions on Visualization & Computer Graphics ( Proc . InfoVis ) ( 2017 ) . https : / / doi . org / 10 . 1109 / tvcg . 2016 . 2599030 [ 67 ] Arvind Satyanarayan , Dominik Moritz , Kanit Wongsuphasawat , and Jeffrey Heer . 2023 . Vega Editor . [ 68 ] Arvind Satyanarayan , Dominik Moritz , Kanit Wongsuphasawat , and Jeffrey Heer . 2023 . Vega - Lite gallery . [ 69 ] Arvind Satyanarayan , Ryan Russell , Jane Hoffswell , and Jeffrey Heer . 2015 . Re - active vega : A streaming dataflow architecture for declarative interactive visu - alization . IEEE transactions on visualization and computer graphics 22 , 1 ( 2015 ) , 659 – 668 . [ 70 ] Timo Schick and Hinrich Schütze . 2021 . Generating datasets with pretrained language models . arXiv preprint arXiv : 2104 . 07540 ( 2021 ) . [ 71 ] Vidya Setlur , Sarah E Battersby , Melanie Tory , Rich Gossweiler , and Angel X Chang . 2016 . Eviza : Anaturallanguageinterfaceforvisualanalysis . In Proceedings of the 29th annual symposium on user interface software and technology . 365 – 377 . [ 72 ] Leixian Shen , Enya Shen , Yuyu Luo , Xiaocong Yang , Xuming Hu , Xiongshuai Zhang , ZhiweiTai , andJianminWang . 2022 . Towardsnaturallanguageinterfaces for data visualization : A survey . IEEE transactions on visualization and computer graphics ( 2022 ) . [ 73 ] Leixian Shen , Yizhi Zhang , Haidong Zhang , and Yun Wang . 2023 . Data player : Automatic generation of data videos with narration - animation interplay . arXiv preprint arXiv : 2308 . 04703 ( 2023 ) . [ 74 ] Pao Siangliulue , Joel Chan , Steven P Dow , and Krzysztof Z Gajos . 2016 . Idea - Hound : improving large - scale collaborative ideation with crowd - powered real - time semantic modeling . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology . 609 – 624 . [ 75 ] Andrea Spreafico and Giuseppe Carenini . 2020 . Neural data - driven captioning of time - series line charts . In Proceedings of the International Conference on Advanced Visual Interfaces . 1 – 5 . [ 76 ] Arjun Srinivasan , Steven M Drucker , Alex Endert , and John Stasko . 2018 . Aug - menting visualizations with interactive data facts to facilitate interpretation and communication . IEEE transactions on visualization and computer graphics 25 , 1 ( 2018 ) , 672 – 681 . [ 77 ] Arjun Srinivasan , Nikhila Nyapathy , Bongshin Lee , Steven M Drucker , and John Stasko . 2021 . Collecting and characterizing natural language utterances for specifyingdatavisualizations . In Proceedingsofthe2021CHIConferenceonHuman Factors in Computing Systems . 1 – 10 . [ 78 ] Arjun Srinivasan and John Stasko . 2017 . Natural language interfaces for data analysis with visualization : Considering what has and could be asked . In Pro - ceedings of the Eurographics / IEEE VGTC conference on visualization : Short papers . 55 – 59 . [ 79 ] ArjunSrinivasanandJohnStasko . 2017 . Orko : Facilitatingmultimodalinteraction forvisualexplorationandanalysisofnetworks . IEEEtransactionsonvisualization and computer graphics 24 , 1 ( 2017 ) , 511 – 521 . [ 80 ] Chris Stolte , Diane Tang , and Pat Hanrahan . 2002 . Polaris : A system for query , analysis , and visualization of multidimensional relational databases . IEEE Trans - actions on Visualization and Computer Graphics 8 , 1 ( 2002 ) , 52 – 65 . [ 81 ] Nicole Sultanum and Arjun Srinivasan . 2023 . DataTales : Investigating the use of Large Language Models for Authoring Data - Driven Articles . arXiv preprint arXiv : 2308 . 04076 ( 2023 ) . [ 82 ] Benny J . Tang , Angie Boggust , and Arvind Satyanarayan . 2023 . VisText : A Benchmark for Semantically Rich Chart Captioning . In The Annual Meeting of the Association for Computational Linguistics ( ACL ) . http : / / vis . csail . mit . edu / pubs / vistext [ 83 ] Kyle A Thomas and Scott Clifford . 2017 . Validity and Mechanical Turk : An assessment of exclusion methods and interactive experiments . Computers in Human Behavior 77 ( 2017 ) , 184 – 197 . [ 84 ] Manasi Vartak , Sajjadur Rahman , Samuel Madden , Aditya Parameswaran , and Neoklis Polyzotis . 2015 . Seedb : Efficient data - driven visualization recommenda - tions to support visual analytics . In Proceedings of the VLDB Endowment Interna - tional Conference on Very Large Data Bases , Vol . 8 . NIH Public Access , 2182 . [ 85 ] Yun Wang , Zhitao Hou , Leixian Shen , Tongshuang Wu , Jiaqi Wang , He Huang , HaidongZhang , andDongmeiZhang . 2022 . TowardsNaturalLanguage - BasedVi - sualization Authoring . IEEE Transactions on Visualization and Computer Graphics 29 , 1 ( 2022 ) , 1222 – 1232 . [ 86 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Fei Xia , Ed Chi , QuocVLe , DennyZhou , etal . 2022 . Chain - of - thoughtpromptingelicitsreasoning in large language models . Advances in Neural Information Processing Systems 35 ( 2022 ) , 24824 – 24837 . [ 87 ] Kanit Wongsuphasawat , Dominik Moritz , Anushka Anand , Jock Mackinlay , Bill Howe , andJeffreyHeer . 2015 . Voyager : Exploratoryanalysisviafacetedbrowsing ofvisualizationrecommendations . IEEEtransactionsonvisualizationandcomputer graphics 22 , 1 ( 2015 ) , 649 – 658 . [ 88 ] Kanit Wongsuphasawat , Dominik Moritz , Anushka Anand , Jock Mackinlay , Bill Howe , and Jeffrey Heer . 2016 . Towards a general - purpose query language for visualization recommendation . In Proceedings of the Workshop on Human - In - the - Loop Data Analytics . 1 – 6 . [ 89 ] Kanit Wongsuphasawat , Zening Qu , Dominik Moritz , Riley Chang , Felix Ouk , Anushka Anand , Jock Mackinlay , Bill Howe , and Jeffrey Heer . 2017 . Voyager 2 : Augmenting visual analysis with partial view specifications . In Proceedings of the 2017 chi conference on human factors in computing systems . 2648 – 2659 . [ 90 ] Jason Wu , Siyan Wang , Siman Shen , Yi - Hao Peng , Jeffrey Nichols , and Jeffrey P Bigham . 2023 . WebUI : A Dataset for Enhancing Visual UI Understanding with Web Semantics . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 91 ] Linting Xue , Aditya Barua , Noah Constant , Rami Al - Rfou , Sharan Narang , Mi - hir Kale , Adam Roberts , and Colin Raffel . 2022 . Byt5 : Towards a token - free future with pre - trained byte - to - byte models . Transactions of the Association for Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . Computational Linguistics 10 ( 2022 ) , 291 – 306 . [ 92 ] Shunyu Yao , Jeffrey Zhao , Dian Yu , Nan Du , Izhak Shafran , Karthik Narasimhan , and Yuan Cao . 2023 . ReAct : Synergizing Reasoning and Acting in Language Models . In International Conference on Learning Representations ( ICLR ) . [ 93 ] Jiacheng Ye , Jiahui Gao , Qintong Li , Hang Xu , Jiangtao Feng , Zhiyong Wu , Tao Yu , and Lingpeng Kong . 2022 . Zerogen : Efficient zero - shot learning via dataset generation . arXiv preprint arXiv : 2202 . 07922 ( 2022 ) . [ 94 ] Jiacheng Ye , Chengzu Li , Lingpeng Kong , and Tao Yu . 2023 . Generating Data for SymbolicLanguagewithLargeLanguageModels . arXivpreprintarXiv : 2305 . 13917 ( 2023 ) . [ 95 ] Bowen Yu and Cláudio T Silva . 2019 . FlowSense : A natural language inter - face for visual data exploration within a dataflow system . IEEE transactions on visualization and computer graphics 26 , 1 ( 2019 ) , 1 – 11 . [ 96 ] Jian Zhao , Mingming Fan , and Mi Feng . 2020 . Chartseer : Interactive steering exploratory visual analysis with machine intelligence . IEEE Transactions on Visualization and Computer Graphics 28 , 3 ( 2020 ) , 1500 – 1513 . A PROMPT FOR NL GENERATION PIPELINE In the prompt , certain variables are enclosed within curly brackets . We colored them blue for easy recognition . Here , we provide a de - tailed explanation of each variable and specify its usage in different NL generation prompts : • vl [ ALL ] : Pre - processed Vega - Lite specification converted as one line string ; • ftt _ str [ L1 , L2 , Utterance ] : Information about fields , titles , types , and values ; • prompt [ L2 ] : Questions derived from the guided discovery process ; • info [ L2 ] : Answers for the questions computed through LangChain library [ 8 ] ; • info _ first _ concat [ Utterance ] : A list of primary instruc - tions by analyzing the chart semantics . A . 1 L1 Caption 1 { vl } 2 3 Let ' s generate a level 1 NL description step by step . 4 5 Step 1 . Determine if the visualization contains composite views , such as layered plots , trellis plots , or other types of multiple view displays , and provide a count of the number of plots if any are present . 6 Step 2 . Analyze the semantics of each chart individually , including [ Data ] , [ Transform ] , [ Mark ] , [ Chart - Type ] , [ Encoding ] , [ Style ] , and [ Interaction ] . Refer to this : 7 { ftt _ str } 8 Step 3 . Generate a level 1 NL description using the semantics . It contains elemental and encoded properties of the visualization ( i . e . , the visual components that comprise a graphical representation ' s design and construction ) . 9 10 # # 11 Step 1 . Composite Views : 12 - True / False : 13 - ( If True ) Type : ( layered , trellis , multiple views ) 14 - Number of plots : 15 Step 2 . Chart Semantics : 16 - Data : 17 - Field ( Value ) : 18 - Transform : 19 - Mark : 20 - Chart - Type : 21 - Encoding : 22 - Style : 23 - Interaction ( e . g . , tooltip ) : 24 Step 3 . Level 1 NL Description : A . 2 L2 Caption 1 { vl } 2 3 Let ' s generate question ( s ) step by step . 4 5 Step 1 . What is the most prominent and meaningful feature in the given chart ? 6 Step 2 . What is the mathematical operation ( s ) ( e . g . , max , min , sum , difference , and average ) required to describe the feature ? 7 Step 3 . Generate question ( s ) using the mathematical operation ( s ) required to describe the feature . If there are multiple questions , separate them with semicolon ( ; ) . 8 9 # # 10 Step 1 . Features : 11 Step 2 . Operations : 12 Step 3 . Questions : 1 Refer to this : { ftt _ str } 2 Do not draw any charts to answer the question . 3 4 Question : { prompt } 1 Information : { info } 2 3 { ftt _ str } 4 5 Generate a concise level 2 natural language description of a visualization , with 1 or 2 sentences for each level . 6 The level 2 description should contain statistical and relational properties of the visualization ( e . g . , descriptive statistics , extrema , outliers , correlations , point - wise comparisons ) . 7 8 # # 9 Level 2 NL Description : A . 3 Utterance 1 { vl } 2 3 Step 1 . Determine if the visualization contains composite views , such as layered plots , trellis plots , or other types of multiple view displays , and provide a count of the number of plots if any are present . 4 Step 2 . Provide a list of instructions to create the chart using natural language . 5 - Write instructions for each view and separate with < % > 6 - Separate each instruction by a semicolon ( ; ) NL Dataset Generation Framework for Visualizations Powered by LLM Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY 7 - Divide each instruction to contain only one specific action 8 - Use the following chart semantics to specify instructions : [ Data ] , [ Chart - Type ] , [ Mark ] , [ Encoding ] , [ Transform ] , [ Style ] , [ Interaction ] 9 Step 3 . Given the information about the fields and their synonyms , please replace the field names with their corresponding synonyms . 10 { ftt _ str } 11 12 # # 13 Step 1 . Composite Views : 14 - True / False : 15 - ( If True ) Type : ( layered , trellis , multiple views ) 16 - Number of plots : 17 Step 2 . Instructions : 18 [ View # ] ; [ < Chart Semantic > ] : < Instruction > ; [ < Chart Semantic > ] : < Instruction > ; . . . < % > 19 Step 3 . Instructions : 20 [ View # ] ; [ < Chart Semantic > ] : < Instruction > ; [ < Chart Semantic > ] : < Instruction > ; . . . < % > 1 { inst _ first _ concat } 2 The above are instructions to generate a chart . Let ' s generate combined instructions ( [ Command ] , [ Query ] , [ Question ] ) for each view step by step . 3 4 Step 1 . Identify the primary information in each view . 5 Step 2 . Identify the secondary information in each view . 6 Step 3 . Generate a [ Command ] for each view using only the primary info . Please follow these rules : 7 - Use imperative voice 8 - Write in a single sentence 9 - Use only the primary info 10 - Make it concise and simple 11 Step 4 . Generate a [ Query ] for each view using only the primary info . Please follow these rules : 12 - Refrain from using verbs and articles ( e . g . , a , the ) 13 - Use only variables , fields , attributes , mathematical formulas ( e . g . , sum , avg , mix , max , count , order ) , abbreviations ( e . g . , vs ) , and prepositions ( e . g . , of , by , for , with , over , from , to ) 14 - Write in a single sentence 15 - Use only the primary info 16 - Make it concise and simple 17 Step 5 . Generate a [ Question ] for each view using only the primary info . Please follow these rules : 18 - Ask an inquiry as a question 19 - Write in a single sentence 20 - Use only the primary info 21 - Make it concise and simple 22 23 # # 24 View # < Number > : 25 Step 1 . Primary Information : 26 Step 2 . Secondary Information : 27 Step 3 . Command : 28 Step 4 . Query : 29 Step 5 . Question : A . 4 Question 1 { vl } 2 3 Let ' s generate a lookup question , a compositional question , and an open - ended question for a given Vega - Lite spec step by step . The lookup question requires a single value retrieval . The compositional question requires multiple operations . 4 5 Step 1 . What higher - level decision can be made by analyzing this chart ? 6 Step 2 . What is a possible conclusion that can be reached from this decision ? 7 Step 3 . What specific value can be retrieved to reach this conclusion ? 8 Step 4 . Generate a lookup question using this value , without including any visual attributes such as color , length , size , or position . 9 Step 5 . What visual attributes are required to paraphrase this question ? 10 Step 6 . Paraphrase the generated question using the chart ' s visual attributes . 11 Step 7 . What are the mathematical operations ( e . g . , max , min , sum , difference , and average ) to reach the conclusion in Step 2 ? 12 Step 8 . Generate a compositional question using this value , without including any visual attributes such as color , length , size , or position . 13 Step 9 . What visual attributes are required to paraphrase this question ? 14 Step 10 . Paraphrase the generated question using the chart ' s visual attributes . 15 Step 11 . Generate an open - ended question to reach the conclusion in Step 2 . 16 17 # # 18 Step 1 . Decision : 19 Step 2 . Conclusion : 20 Step 3 . Specific Value : 21 Step 4 . Lookup Question : 22 Step 5 . Visual Attributes : 23 Step 6 . Paraphrased Question : 24 Step 7 . Operations : 25 Step 8 . Compositional Question : 26 Step 9 . Visual Attributes : 27 Step 10 . Paraphrased Question : 28 Step 11 . Open - ended Question : B PROMPT FOR AUTOMATIC QUALITATIVE CODING When extracting codes , we omitted the words ‘language’ and ‘use of’ since they were frequently added to the code . We believe that these additions do not contribute any additional meaning to the thematic analysis . 1 Let ' s perform a thematic analysis in the field of human - computer interaction . Generate characteristics of languages leveraged in the given sentence . The total number is five and each of them is separated by semicolons . Do not add numbering or any explanations . Conference acronym ’XX , June 03 – 05 , 2018 , Woodstock , NY Ko et al . 2 3 Sentence : { sent } 4 5 # # 6 ; ; ; ; C GOLD REFERENCE GUIDELINES C . 1 Utterance • Imagine writing utterances to display a visualization using a system like Excel , Tableau , or Microsoft Power BI ; • Refer to both the dataset and the chart to better understand the context in which the data has been used for the visual - ization and formulate more naturalistic utterances . • Avoid referring to specific instructions to prevent acclimati - zation to the words or phrases in the instruction [ 77 ] ; • Write utterances as singletons , which are basic types of ut - terances , but can be more than one sentence if necessary due to complexity , forming a sequential utterance that provides all necessary information ; • Write utterance for each view . If the chart is has layered plots , and they have different chart types , write utterance with according to the number of different chart types ; • Focus on primary information such as chart type and en - coding rather than secondary information such as style and interaction [ 85 ] . C . 2 Question • Ask one question in one complete sentence ; • Keep questions clear and concise , avoiding overly broad or vague questions by focusing on specific aspects of the chart ; • Formulate questions that can elicit useful insights from the visualization to facilitate visual data analysis and decision - making [ 31 ] .