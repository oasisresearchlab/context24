Crossbar - Aligned & Integer - Only Neural Network Compression for Efficient In - Memory Acceleration Shuo Huai 1 , 2 , Di Liu 2 , Xiangzhong Luo 1 , Hui Chen 1 , Weichen Liu 1 ∗ , and Ravi Subramaniam 3 1 School of Computer Science and Engineering , Nanyang Technological University , Singapore 2 HP - NTU Digital Manufacturing Corporate Lab , Nanyang Technological University , Singapore 3 Innovations and Experiences – Business Personal Systems , HP Inc . , Palo Alto , California , USA ABSTRACT Crossbar - based In - Memory Computing ( IMC ) accelerators preload the entire Deep Neural Network ( DNN ) into crossbars before in - ference . However , devices with limited crossbars cannot infer in - creasingly complex models . IMC - pruning can reduce the usage of crossbars , but current methods need expensive extra hardware for data alignment . Meanwhile , quantization can represent weights of DNNs by integers , but they employ non - integer scaling factors to ensure accuracy , requiring costly multipliers . In this paper , we first propose crossbar - aligned pruning to reduce the usage of crossbars without hardware overhead . Then , we introduce a quantization scheme to avoid multipliers in IMC devices . Finally , we design a learning method to complete above two schemes and cultivate an optimal compact DNN with high accuracy and large sparsity during training . Experiments demonstrate that our framework , compared to state - of - the - art methods , achieves larger sparsity and lower power consumption with higher accuracy . We even improve the accuracy by 0 . 43 % for VGG - 16 with an 88 . 25 % sparsity rate on the Cifar - 10 dataset . Compared to the original model , we reduce computing power and area by 19 . 8x and 18 . 8x , respectively . KEYWORDS In - memory computing , pruning , quantization , neural networks ACM Reference Format : Shuo Huai 1 , 2 , Di Liu 2 , Xiangzhong Luo 1 , Hui Chen 1 , Weichen Liu 1 ∗ , and Ravi Subramaniam 3 . 2023 . Crossbar - Aligned & Integer - Only Neural Net - work Compression for Efficient In - Memory Acceleration . In 28th Asia and South Pacific Design Automation Conference ( ASPDAC ’23 ) , January 16 – 19 , 2023 , Tokyo , Japan . ACM , New York , NY , USA , 6 pages . https : / / doi . org / 10 . 1145 / 3566097 . 3567856 1 INTRODUCTION Deep Neural Networks ( DNNs ) are increasingly complex to gain more accuracy , hindering their efficient deployment on today’s low - power accelerators . DNN algorithms own a high degree of comput - ing parallelism and extensive memory access ; hence in - memory computing ( IMC ) crossbar architectures based on memristor de - vices ( e . g . , Resistive Random Access Memory ) , which have many ∗ Corresponding author : Weichen Liu ( liu @ ntu . edu . sg , shuo001 @ e . ntu . edu . sg ) . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan © 2023 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 9783 - 4 / 23 / 01 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3566097 . 3567856 parallel digital arithmetic units and manage computing in memory to reduce memory access , is an emerging and promising architec - ture to accelerate DNN algorithms [ 18 ] . Nevertheless , the extensive parameters and non - integer computing in state - of - the - art DNN models still prohibit them from being efficiently deployed on IMC devices [ 2 , 4 ] . Thus , DNN models need to be optimized to deploy on IMC devices according to the IMC characteristic . Due to the crossbar’s large reconfigure latency and limited write endurance [ 20 ] , IMC devices preload an entire model into crossbars before inference to reduce the writing ( configuration ) operations , requiring too many crossbars for complex models [ 19 ] . Since com - putations of DNNs are distributedly mapped to crossbars , model pruning becomes a promising technique to reduce the number of used crossbars . Existing pruning methods [ 2 , 11 , 12 ] for IMC devices exploit the inherent property of crossbar architectures to employ fine - grained ( i . e . , crossbar column / row level ) pruning , which trims columns / rows of weights in each crossbar . Although these fine - grained methods can reduce the number of crossbars , they require expensive extra hardware ( e . g . , about 44 % more storage and 10 % more area introduced in crossbar - column pruning ) to align the intra - output of each crossbar [ 2 ] ( see Fig . 1 ( b ) , Fig . 2 ) . The newly introduced unit severely degrades the hardware integration density and reduces the number of crossbars fabricated on IMC devices , making it unable to accomplish the expected hardware savings . We , in practice , should explore crossbar - aligned pruning meth - ods , like kernel - group pruning and crossbar pruning , which remove entire crossbar weights to prevent aligning the intra - output of each crossbar ( see Fig . 1 ( c - d ) ) . Kernel - group pruning is a coarse - grained method that removes a group of kernels in each layer by consider - ing the model - IMC mapping . While crossbar pruning directly trims an entire crossbar’s weights , and it is a medium - grained pruning method . Unfortunately , the coarser the pruning , the larger the ac - curacy drop under the same sparsity [ 14 ] . Thus , previous works [ 2 , 11 , 12 ] cause a large accuracy drop or a low sparsity rate if they are used to perform coarser - grained crossbar - aligned pruning . Meanwhile , itiscomplicatedandexpensivetoimplement floating - point ( FP ) arithmetic units using IMC crossbars [ 4 ] . Then the FP operations in DNNs need extra multipliers , which bring about 7 % power and 9 % area overhead ( see Fig . 2 ) . In order to comprehen - sively realize low - power compact DNN for IMC devices , it is better to only include integer operations in final compressed DNN models . Traditional model quantization schemes [ 8 ] can approximate FP inputs and weights with integers , but they still employ FP scaling factors to ensure accuracy . Although IAO [ 8 ] proposes to use fixed - point multiplication to replace FP scaling factors , the fixed - point multiplier is still needed and introduces some overhead . 234 ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan Shuo Huai , Di Liu , Xiangzhong Luo , Hui Chen , Weichen Liu , and Ravi Subramaniam 5 6 2 3 3 5 2 1 1 4 2 5 2 2 1 3 O u t pu t 4 5 24 37 1 3 5 4 6 8 30 36 1 2 3 3 XB 1 XB 2 XB 3 XB 4 24 37 30 36 S + A ( d ) Crossbar pruning 4 5 1 6 8 3 2 5 2 Input Output ( 4 channels ) 76 40 67 40 57 39 67 51 48 29 45 33 86 56 97 74 1 4 2 5 Kernel 1 3 5 2 1 Kernel 2 5 6 2 3 Kernel 4 2 2 1 3 Kernel 3 8 6 5 4 3 8 1 5 5 2 8 6 2 5 3 8 4 5 6 8 pp 4 5 6 8 Input Matrix One by one ( a ) Executing a convolutional layer on crossbars 4 5 24 37 1 3 5 4 4 5 18 50 5 6 2 2 6 8 52 20 2 2 1 5 6 8 30 36 1 2 3 3 XB 1 XB 2 XB 3 XB 4 24 37 18 50 52 20 30 36 S + A Output ( b ) Crossbar - column pruning 5 6 2 3 3 5 2 1 1 4 2 5 2 2 1 3 4 5 24 18 1 2 2 4 6 8 20 30 2 1 3 1 XB 1 XB 2 XB 3 XB 4 O u t pu t S + A 24 18 20 30 1010XX . . . 0110XX . . . 24 0 18 0 0 20 30 0 ST data aligning ( c ) IMC - aware kernel - group pruning 5 6 2 3 3 5 2 1 1 4 2 5 2 2 1 3 4 5 24 18 1 2 2 4 6 8 52 30 2 1 3 5 XB 1 XB 2 XB 3 XB 4 O u t pu t S + A 24 18 52 30 Figure 1 : ( a ) The mapping scheme of a convolutional layer to crossbars ; ( b - d ) The schematics of executing a convolutional layer with crossbar - column sparsity , kernel - group sparsity and crossbar sparsity , respectively . To address the aforementioned problem , in this paper , we pro - pose a new DNN learning framework to cultivate an optimal com - pact architecture for high accuracy during the training process . Also , it applies crossbar - aligned pruning and integer - only quantization to completely achieve efficient and low - power IMC acceleration . Our main contributions are as summarized : • We introduce a crossbar - aligned pruning approach to reduce crossbar usage without extra processing units for better hard - ware efficiency and greater integration density . Also , it includes both kernel - group pruning and crossbar pruning to form multi - grained pruning for high accuracy and large sparsity . • We apply a simple yet efficient integer - only quantization scheme for IMC architecture by reusing the bit - shift units . The quantiza - tion approach is co - optimized with the pruning strategy during the training process to improve accuracy . • We propose a model learning framework to complete the prun - ing and quantization schemes . And it compacts models by one training process with a global importance rank of weights and a dynamic zero - recovery procedure , widening the exploration space for better architecture and higher accuracy . • We demonstrate the efficiency and effectiveness of our frame - work with extensive experiments . Compared to state - of - the - art methods , our method can achieve a higher sparsity rate and a slighter accuracy drop without extra hardware . Thus , we can reduce computing power and computing area significantly . 2 BACKGROUND & RELATED WORKS In this section , we introduce the preliminaries of in - memory com - puting , IMC - pruning and quantization . The IMC device consists of many crossbars . Memristive cells are located between a wordline ( horizontal ) and a bitline ( vertical ) in each crossbar [ 18 ] . The weights of a DNN model are mapped to memristive cells , and values are represented by the conductance of associated cells . The Digital - to - Analog Converter ( DAC ) converts inputs of each layer to voltage pulses , which are injected into the wordline of the crossbar . According to Kirchoff’s Law , the current generated in each cell , representing the inner product between the voltage and the cell conductance , accumulates along the bitline , and the total current is the dot product result , which is converted to digital values by the Analog - to - Digital Converter ( ADC ) . Matrix multiplication can be performed on crossbars easily , hence the fully - connected layer can be directly mapped to crossbars . Fig . 1 ( a ) illustrates the mapping of convolutional layers on crossbars . Each kernel is unfolded to a vector and mapped into crossbar memristive cells . A large kernel occupies several crossbars , which are connected by peripheral circuits . The input of each layer is reshaped to a series of vectors and sent to the wordline of crossbars one by one . To fit complex DNN models into limited crossbars of the IMC ar - chitecture , pruning methods are proposed to reduce model complex - ity . Current IMC - pruning methods are fine - grained ( i . e . , crossbar column / row pruning ) . Ling et al . [ 11 ] exploit pruning the columns of weights within a crossbar , and then they recombine the weights from different sparse crossbars to reduce the number of used cross - bars . Meng et al . [ 15 ] propose a multi - group Lasso to prune a group of columns of weights in a crossbar . It is still a crossbar - column pruning method . PIM - Prune [ 2 ] proposes to exploit the sparsity in both row and column directions of the weight matrix . And they design a new hardware data path to support their pruning method . These fine - grained methods require specifically designed hard - ware masks [ 2 , 15 ] to align the intra - output data from crossbars . PIM - Prune [ 2 ] proposes a way to produce this mask – Sparsity Ta - ble ( ST ) . As illustrated in Fig . 1 ( b ) , each 1 in ST represents picking a number from the original crossbar output , and 0 means insert - ing a 0 into the final output queue . Thus , each crossbar needs a corresponding mask memory in the ST [ 2 ] . The length of a mask in an ST equals the maximum pruning rate times the number of weight columns in each crossbar . As shown in Fig . 1 ( b ) , the prun - ing rate is 8 / 4 = 2 , and the number of columns in a crossbar is 2 , hence the valid number in a mask is 4 . However , the mask must support a higher pruning rate . The ST size calculated in Fig . 2 uses a maximum pruning rate of 32 , and the number of columns is also 32 , resulting in that for 8 crossbars , the ST is 1KB and introduces about 44 % more memory and 10 % more area into an In - Situ Multiply Accumulate ( IMA ) [ 18 ] . If the crossbar - row pruning is supported , another ST is needed to align the input [ 2 ] , which exacerbates the overhead and is even unaffordable . Especially in practice , different crossbars can configure different pruning rates , resulting in huge resource waste of masks intended for the maximum pruning rate . Meanwhile , quantization is used to remove the FP operations in DNN models . But previous methods [ 8 ] employ FP scaling factors to reduce the accuracy loss , still requiring FP multiplication processors 235 Crossbar - Aligned & Integer - Only Neural Network Compression for Efficient In - Memory Acceleration ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan DRAM Buffer 64 KB 20 . 70 0 . 08300 OR / TILE 3 KB 1 . 68 0 . 00320 MUL - 23 . 72 0 . 02604 12 IMA 63 KB 294 . 84 0 . 17256 Others / TILE - 11 . 47 0 . 03865 ComponentComponent SizeSize Power ( mW ) Power ( mW ) Area ( mm(cid:2) ) Area ( mm(cid:2) ) IR 2 KB 1 . 24 0 . 00210 OR / IMA 256 B 0 . 23 0 . 00077 ST 1 KB 0 . 50 0 . 00126 XB array 128 x 128 2 . 40 0 . 00020 S + A / IMA - 0 . 20 0 . 00024 Others / IMA - 20 . 00 0 . 00981 D A C D A C XBXB S + H S + H D A C D A C XBXB S + H S + H D A C D A C XBXB S + H S + H D A C D A C XBXB S + H S + H RIRORORS + A S + A ADCADCADCADCADCADCADCADCSTST In - Situ Multiply In - Situ Multiply Accumulate ( IMA ) 1 Accumulate ( IMA ) 1 MA 2 IMA 2 MA12IMA12 DRAMDRAMBufBufferferfff OROR ReLUReLU S + A S + A MULMUL TILE 1 TILE 1 PoolingPooling TILE2TILE2 TILE n TILE n C O N T R O LL E R C O N T R O LL E R EX T E RN A L I O I N T E R F EX T E RN A L I O I N T E R F A C E A C E FFFF CHIPCHIP IR - (cid:2) (cid:2)Input Register OR - (cid:2) (cid:2)Output Register S + A - (cid:2) (cid:2)Shift and Add MUL - (cid:2) (cid:2)FP Multiplier Unit XB - (cid:2) (cid:2)Memristor Crossbar ST - (cid:2) (cid:2)Sparsity Table ADC - (cid:2) (cid:2)Analog to Digital DAC - (cid:2) (cid:2)Digital to Analog S + H - (cid:2) (cid:2)Sample and Hold RIR Data Flow : XBXB STST S + A S + A OROR MULMUL 2222 M A I M A T I L E T I L E Figure 2 : Left : IMC architecture from ISAAC [ 18 ] ; Right : Compo - nents and their parameters . Please note ST and MUL are only nec - essary for specific methods . in the IMC architecture . However , as shown in Fig . 2 , a multiplier introduces about 7 % power overhead and 9 % area overhead into each Tile [ 18 ] , significantly reducing energy efficiency and hard - ware integration density while also increasing pipeline cycles and lowering performance . Thus , it is imminent to eliminate FP scaling factors . IAO [ 8 ] , besides training for the optimal scaling factors , proposes a scheme to remove the FP scaling factors by fixed - point multiplication . Nevertheless , it still requires multiplications , and fixed - point multipliers are needed in IMC devices . Some overhead is still introduced to the hardware . 3 METHODOLOGY In this section , we first introduce our crossbar - aligned pruning algorithm , which can achieve large sparsity without hardware over - head by combining kernel - group pruning and crossbar pruning . Then , we demonstrate our integer - only quantization method for IMC architectures to avoid using multipliers . Finally , we present our compact learning framework to extract an optimal DNN archi - tecture during the training process to further improve accuracy . 3 . 1 Crossbar Reduction w / o Hardware Overhead Current IMC - pruning is able to save crossbars , but they require extra hardware to align data . From our analysis in Section 2 , the area of the data alignment unit is about 6 time that of a crossbar array , reducing the integration density and further reducing the number of crossbars on the IMC chip . We cannot fully benefit from these methods in terms of hardware savings . To solve this problem , we need to ensure that the pruned DNN model is data - aligned . The coarse - grained kernel pruning and the medium - grained crossbar pruning can achieve our goals . However , as stated in the previous work [ 14 ] , the coarser the pruning granularity , the larger the ac - curacy drop under the same sparsity . Thus , the accuracy drops of existing fine - grained IMC pruning methods [ 2 , 11 , 12 ] are large when they are employed to perform these data - aligned pruning . To enlarge the sparsity rate and accuracy under data alignment , we first design a new multi - grained pruning method . 3 . 1 . 1 Kernel - group Pruning . Kernel pruning means we directly remove whole kernels from DNNs . Suppose a DNN model has two connected convolutional layers ( denoted as l 1 , l 2 ) , then the number of kernels in layer l 1 is the number of channels in layer l 2 . We use C 2 × K 1 × K 1 × C 1 and C 3 × K 2 × K 2 × C 2 to represent the shape of kernels in these two layers , where C n is the number of channels in layer l n , C n + 1 is the number of kernels in layer l n , and K means the kernel size . From Fig . 1 ( a ) , we can derive that layer l 1 needs (cid:3) C 2 / XB w (cid:4) × (cid:3) ( K 1 × K 1 × C 1 ) / XB h (cid:4) crossbars , where XB w and XB h represent the width and height of a crossbar , respectively . For layer l 2 , it needs (cid:3) C 3 / XB w (cid:4) × (cid:3) ( K 2 × K 2 × C 2 ) / XB h (cid:4) crossbars . Removing kernels indicates reducing the number of kernels C n + 1 in layer l n . To maximize savings of crossbars , we use kernel - group pruning to remove a group of kernels in each layer and enable the number of remaining kernels in each layer to be integer times of XB w . During the pruning process , we first calculate the number of kernels that need to be removed in each layer by the pruning ratio and the importance rank ( see Section 3 . 3 ) . Then we round the number of remaining kernels of each layer to integer times of XB w . Kernel - group pruning has a potential advantage . For a clear explanation , we use C (cid:5) n + 1 to represent the remaining kernels in layer l n . According to the above design , the used crossbars of layer l 1 and layer l 2 are C (cid:5) 2 / XB w × (cid:3) ( K 1 × K 1 × C 1 ) / XB h (cid:4) and C (cid:5) 3 / XB w × (cid:3) ( K 2 × K 2 × C (cid:5) 2 ) / XB h (cid:4) , respectively . We can deduce that if the crossbar is square ( i . e . , XB w = XB h ) or ( K 2 × K 2 × XB w ) | XB h , the by - product of kernel - group pruning in layer l 1 can also guarantee that kernels in l 2 can fully occupy crossbars from the row aspect , resulting in maximum crossbar saving . In other cases , the by - product of kernel - group pruning in layer l 1 can still save some crossbars of layer l 2 from the row - aspect . However , the accuracy drop of coarse - grained pruning is larger than that of medium / fine - grained pruning methods , especially under a large pruning ratio [ 14 ] . To guarantee accuracy , the crossbar saving of kernel - group pruning is limited , indicating that the pruned model can be further compressed . 3 . 1 . 2 Crossbar Pruning . Crossbar pruning means removing the crossbar - block of weights from the model to save crossbars . Cross - bar pruning is a medium - grained unstructured pruning method , and it is only used in accelerators with crossbar architecture . As we remove a whole crossbar at one time , no data aligning unit is needed , either . Meanwhile , since both convolutional and fully - connected layers can be mapped to crossbars , crossbar pruning can compress these two kinds of layers . In contrast , kernel pruning can only compact convolutional layers . According to the mapping scheme ( see Fig . 1 ( a ) ) , we need a mask layer after each convolutional or fully - connected layer for crossbar pruning , and each mask value is corresponding to a crossbar . Mask layers are differentiable to the loss function ; hence mask values are optimized to reduce the loss in the training stage . We remove crossbar - block of weights from the DNN model with zeroized mask values ( see Section 3 . 3 ) , and other mask values are multiplied to their corresponding weights before inference to remove the mask operation in the inference stage . Although crossbar pruning can achieve higher sparsity than kernel - group pruning , the mask layer challenges the training stage . The reason mainly comes from two aspects . First , deeper DNNs are harder to train , as different layers in the DNN tend to learn at different speeds [ 3 ] , the mask layer makes the model training stage more difficult to converge . Second , in our zero - recovery training process ( see Section 3 . 3 ) , more mask values lead to a larger vari - ation of the compressed model architecture during the training , exacerbating the difficulty of convergence . As such , we propose using kernel - group pruning to compress the DNN first to reduce the number of mask values , which lessens the impact of mask layers in the crossbar pruning process for higher accuracy . 236 ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan Shuo Huai , Di Liu , Xiangzhong Luo , Hui Chen , Weichen Liu , and Ravi Subramaniam (cid:53)(cid:88)(cid:79)(cid:77)(cid:79)(cid:84)(cid:71)(cid:82)(cid:3)(cid:83)(cid:85)(cid:74)(cid:75)(cid:82) (cid:82)(cid:3)(cid:90)(cid:78) (cid:41)(cid:85)(cid:84)(cid:92)(cid:20)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:14)(cid:82)(cid:17)(cid:23)(cid:15)(cid:3)(cid:90)(cid:78)(cid:41)(cid:85)(cid:84)(cid:92)(cid:20)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:82)(cid:3)(cid:90)(cid:78) (cid:40)(cid:52)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:14)(cid:82)(cid:17)(cid:23)(cid:15)(cid:3)(cid:90)(cid:78)(cid:3)(cid:40)(cid:52)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:8) (cid:51) (cid:71) (cid:86)(cid:86) (cid:79) (cid:84) (cid:77) (cid:8) (cid:47)(cid:84)(cid:79)(cid:90)(cid:79)(cid:71)(cid:82)(cid:3)(cid:43)(cid:86)(cid:85)(cid:73)(cid:78)(cid:89) (cid:64)(cid:75)(cid:88)(cid:85)(cid:3)(cid:43)(cid:86)(cid:85)(cid:73)(cid:78) (cid:56) (cid:71) (cid:90) (cid:79) (cid:85) (cid:56)(cid:75)(cid:73)(cid:85)(cid:92)(cid:75)(cid:88)(cid:95)(cid:3)(cid:43)(cid:86)(cid:85)(cid:73)(cid:78) (cid:56) (cid:71) (cid:90) (cid:79) (cid:85) (cid:64)(cid:75)(cid:88)(cid:85)(cid:3)(cid:43)(cid:86)(cid:85)(cid:73)(cid:78)(cid:3)(cid:14)(cid:76)(cid:79)(cid:84)(cid:71)(cid:82)(cid:15) (cid:540) (cid:56)(cid:71)(cid:84)(cid:81)(cid:3)(cid:12)(cid:3)(cid:64)(cid:75)(cid:88)(cid:85)(cid:79)(cid:96)(cid:75) (cid:56)(cid:71)(cid:84)(cid:81)(cid:3)(cid:12)(cid:3)(cid:64)(cid:75)(cid:88)(cid:85)(cid:79)(cid:96)(cid:75) (cid:540) (cid:41)(cid:85)(cid:84)(cid:92)(cid:20)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:51)(cid:71)(cid:89)(cid:81)(cid:3)(cid:82)(cid:71)(cid:95)(cid:75)(cid:88) (cid:56) (cid:71) (cid:90) (cid:79) (cid:85) (cid:540) (cid:56) (cid:71) (cid:90) (cid:79) (cid:85) (cid:540) (cid:56) (cid:75) (cid:83) (cid:85) (cid:92) (cid:75) (cid:3) (cid:64) (cid:75) (cid:88) (cid:85) (cid:3) (cid:12) (cid:39) (cid:74)(cid:74) (cid:3) (cid:51) (cid:71) (cid:89) (cid:81) (cid:49) (cid:75) (cid:88) (cid:84) (cid:75) (cid:82) (cid:19) (cid:77) (cid:88) (cid:85) (cid:91) (cid:86) (cid:3) (cid:54) (cid:88) (cid:91)(cid:84) (cid:79) (cid:84) (cid:77) Figure 3 : Changes in the crossbar usage during compact training . Top : Kernel - group Pruning ; Bottom : Crossbar Pruning . 3 . 2 Floating - point Operations Elimination Quantization can approximate FP inputs ( activations ) and weights with n - bits integer . However , existing quantization methods intro - duce non - integer scaling factors to adjust the scope of inputs and weights for better accuracy [ 8 ] . These scaling factors still need multipliers , which bring large power and area overhead , as shown in Section 2 . Especially considering the integration density , the area of the multiplier is up to 11 times the crossbar , hence removing all multipliers from IMC architectures can increase the number of crossbars to support more complex DNN models . X = S X Q X , W = S W ( Q W − Z W ) , B = S B ( Q B − Z B ) ( 1 ) X 1 = W (cid:2) X + B = S X S W Q X (cid:2) ( Q W − Z W ) + S B ( Q B − Z B ) ( 2 ) Q X 1 = S X S W S X 1 Q X (cid:2) ( Q W − Z W ) + S B S X 1 ( Q B − Z B ) ( 3 ) We first symmetrically quantize the inputs , as the voltage in IMC architectures can represent both positive and negative integers . But the crossbar cells’ conductivity can be only positive , so all weights are asymmetrically quantized . Take the convolutional layer as an example , Eq . ( 1 ) depicts the quantization process , where W and B are weights and biases of this layer , S denotes the scaling factor , Q means the corresponding quantized value , and Z is the zero - point . All values , except for Q X , are determined during training and remain constant throughout the inference stage . In the inference stage , the next layer’s input X 1 is calculated by W (cid:2) X + B , as shown in Eq . ( 2 ) ( For Batch Normalization ( BN ) layers , we use the BN fusing [ 8 ] to fuse BN layer into the convolutional layer ( same for mask layers ) ) . The training stage also determines the S X 1 of the next layer ( X 1 = S X 1 Q X 1 ) . But the IMC crossbar can only perform integer arithmetic ( Q X (cid:2) ( Q W k − Z W ) , Q B − Z B ) , so we need to derive the Q X 1 in Eq . ( 3 ) for the next layer’s inference . S X = 2 a 1 , S W = 2 a 2 , S B = 2 a 3 , S X 1 = 2 b 1 Q X 1 = 2 a 1 + a 2 − b 1 Q X (cid:2) ( Q W − Z W ) + 2 a 3 − b 1 ( Q B − Z B ) ( 4 ) Considering that IMC devices support the bit - shift operation , we can approximate scaling factors using the power of 2 . Then , multi - plications of all scaling factors can be calculated with the bit - shift units in crossbars , as illustrated in Eq . ( 4 ) . Thus , the quantization only needs to reuse the S + A unit in crossbars , without overhead . Although FP values are widely represented by a · 2 b ( 1 ≤ | a | < 2 ) in computers according to IEEE Standard 754 [ 9 ] , approximating a = 1 in the IMC architecture leads to a loss of accuracy . Other IMC optimization methods preserve the FP scaling factors for ac - curacy , resulting in significant power and area overhead from the additional multipliers . To reduce the accuracy drop of our quanti - zation , we include it into the learning process ( see Section 3 . 3 ) for co - optimization with the pruning strategy . 3 . 3 Compact DNN Learning framework As indicated above , we are employing “large - accuracy - loss " prun - ing and quantization methods to eliminate all extra hardware and achieve comprehensive low - power IMC acceleration . To further minimize the accuracy loss , besides the multi - grained pruning scheme , we propose a well - designed compact model learning frame - work to complete our pruning and quantization schemes and opti - mize the compact architecture during the training process . Also , our framework combines pruning and quantization together into the training procedure , allowing these two operations to co - optimize for higher accuracy . As illustrated in Algorithm 1 , the proposed framework is mainly divided into two phases : Kernel - group prun - ing ( Line 15 ) and Crossbar pruning ( Line 17 - 18 ) . In order to learn an efficient compact architecture from the complex DNN , we need an explicit learning medium δ to help us figure out which part can be removed from the model during the pruning process . Kernel - group pruning removes whole kernels at a time , hence we can leverage the trainable parameter γ of the BN layer to provide relevant in - structions without any overhead , as γ can indicate the importance of kernels [ 7 ] . For crossbar pruning , we use the designed mask layer to distinguish the importance of crossbars . Next , we introduce the core function – Compact _ Learning ( Line 1 - 12 ) in detail . Fig . 3 shows the crossbar usage of the model during the compact training pro - cess . Note that it is not necessary to map the model to the IMC device in the training and this figure is only for easy description . We first train the model for some epochs , called initial epochs , to let all learning medium ( importance factors ) δ have informa - tive values to reflect kernel / crossbar importance ( Line 2 - 4 ) instead of randomly initialized values ( Line 13 , 17 ) . Specially , we adopt sparse training [ 13 ] ( Line 4 , 7 ) in the whole training process to force these importance factors δ close to zero for further safely removing . Next is our zero - recovery training epochs ( Line 5 - 11 ) . In the zero epoch ( Line 8 - 11 ) , we first sort all importance factors δ according to their absolute values to get the global importance rank of kernels / crossbars in the entire model ( Line 9 ) . During the whole training process , these importance factors are jointly optimized with the network weights , and the network can automatically iden - tify the importance of each kernel / crossbar . Next , we calculate the importance factor threshold for each layer ( Line 10 ) and ensure that the compact model fully occupies each used crossbar . Finally , we zeroize δ li corresponding to unimportant parts with these thresh - olds ( Line 11 ) to temporarily shield these weights for the current zero epoch . After each zero epoch , we extract a compact model . Following each zero epoch , we propose a recovery epoch ( Line 8 ) to allow those zeroized parameters ( i . e . , δ li ) to recover instead of being permanently removed . If any weights that are eliminated in 237 Crossbar - Aligned & Integer - Only Neural Network Compression for Efficient In - Memory Acceleration ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan the previous zero epoch but are potentially essential , the recovery epoch can help them escape from zero and play a crucial part in the subsequent training process and even the inference processes . The recovery epoch gives our method a chance to learn more efficient architecture with higher accuracy . Eq . ( 5 ) indicates the common calculation formula in a convolutional layer with δ and activation function , where L is the index of layer , X L is the input of layer L , W L is the weight between layer L + 1 and L , and σ L is the activation function at layer L . (cid:2) represents the convolution operation . In the recovery epoch , we focus more on updating δ L that are zeroized during the zero epoch . Eq . ( 6 ) shows the gradients of δ L to the final loss function . Since we have zeroized some values in δ L and the most often used activation function is ReLU , the corresponding output of σ L is likewise zero under ReLU . Although ReLU is not differentiable at zero , it is widely accepted that its derivative is also zero . Thus , we can easily derive Eq . ( 7 ) . X L + 1 = σ L ( δ L · ( W L (cid:2) X L ) ) ( 5 ) ∂ loss ∂ δ L = ∂ loss ∂ X L + 1 · σ L (cid:5) · ( W L (cid:2) X L ) ( 6 ) δ L = 0 → σ L (cid:5) = 0 → ∂ loss ∂ δ L = 0 ( 7 ) Until now , we may need to offer these zeroized δ L some small values to break this deadlock . However , most training algorithms employ momentum in the optimizer to speed up convergence , as the momentum accumulates the gradients of the past steps to determine the direction to go , rather than using only the gradient of the current step [ 5 ] . Eq . ( 8 ) - ( 9 ) illustrates the updating rules of weight with momentum , in which lr is the learning rate , z k is the updated value of the last step and m is the accumulation coefficient . Thus , w k are easily updated to w k + 1 by combining current and past gradients . z k + 1 = m · z k + ∂ loss ∂ w k ( 8 ) w k + 1 = w k − lr · z k + 1 ( 9 ) As a result , even though the current gradients of δ L are zero , they can be updated to recover from zero by following the last updating directions . This process is fully automated and efficient , no addi - tional steps are required . These kernels / crossbars zeroized in the previous zero epoch may become significant and we will not zeroize them in the future zero epoch , thus , the architecture of the final compact model is changed . Moreover , the training process tries to minimize loss and improve accuracy , allowing the compact archi - tecture to improve itself for better accuracy . Therefore , our learning framework gradually cultivates an optimal compact architecture through the zero - recovery training process to minimize accuracy loss . Other IMC - pruning methods [ 2 , 11 , 12 ] only proposed pruning schemes but did not design suitable learning processes to optimize the pruning . This is why our accuracy is higher than others . For quantization , the model is quantized to infer in the for - ward propagation stage ( just quantized computing , no changes to weights ) , and the backpropagation is the same as traditional training . Hence , we do not remove weights in the quantization , and we do not need recovery for quantization . We perform the quan - tization process ( Line 6 ) in all training epochs . It first calculates scaling factors by q and converts them to 2 n . Then , it quantizes the model with all scaling factors . Because the quantization method and the pruning method are both related to the training loss function , they can be co - optimized to improve accuracy . After all training epochs , eliminating all weights corresponding to zeroized δ li ( Line Algorithm 1 : Compact DNN Learning Framework Input : model , training data and settings , zero start epoch : s , prune _ ratio : p , xb _ size : x s , quan _ bits : q . Output : the well - trained , pruned and quantized model 1 Function Compact _ Learning ( model , δ ) : 2 for e ← 1 to s − 1 do 3 f orward ( model ) ; # Training 4 update _ with _ sparsity ( model , δ ) ; # Training 5 for e ← s to epoch max do 6 f orward _ with _ quantization ( model , q ) ; # Training 7 update _ with _ sparsity ( model , δ ) ; # Training 8 if e % 2 = = 0 or e = = epoch max then 9 Imp r ← Sort kernels / Crossbars by | δ | ; 10 Find δ threshold δ lth of each layer by Imp r , p , x s ; 11 Zeroize δ li if δ li < δ lth ; # Temporarily Pruning 12 Remove all weights from model with zero δ li ; 13 Initialize model and its parameters randomly ; 14 # Phase 1 – Kernel - group pruning 15 Compact _ Learning ( model , γ ) ; 16 # Phase 2 – Crossbar pruning 17 model _ mk ← Mask ( model ) ; 18 Compact _ Learning ( model _ mk , mask ) ; 12 ) and quantizing , we can construct an efficient , integer - only , and high - accuracy model that suits the IMC architecture well . 4 EXPERIMENTAL EVALUATION 4 . 1 Experiment Setup The IMC architecture we used is a widely - adopted ReRAM - based DNN accelerator – ISAAC [ 18 ] . The crossbar size is 128 × 128 , and each memristor cell can store two bits . We quantize the model to 8 bits and map each weight to 4 memristor cells , then the S + A is used to combine the results from different cells . For components that ISAAC does not provide ( e . g . , ST ) , we use CACTI [ 16 ] at 32nm to model the power and area . We model above design configurations using a modified NeuroSim [ 1 ] simulator with the 32nm CMOS library . And following other works [ 2 , 11 , 12 , 15 ] , the effect of conductance variability is ignored . We compare our method against state - of - the - art methods on representative DNNs : VGG [ 19 ] and ResNet [ 6 ] , and on two dataset CIFAR - 10 [ 10 ] and ImageNet [ 17 ] . 4 . 2 Evaluation and Analysis 4 . 2 . 1 Quantization performance evaluation . Table 1 demonstrates that compared to the baseline ( i . e . , full precision ) , the proposed quantization approach ( denoted as INT - quan ) does not result in a significant reduction of accuracy . This quantization approach can even provide higher accuracy than full precision in some models . Under the same training settings , our technique achieves similar accuracy to the IAO quantization method [ 8 ] , whose scaling factors are not equal to the power of 2 . Thus , our quantization scheme avoids the multiplier without discernible accuracy loss . 4 . 2 . 2 Accuracy evaluation . Table 2 presents that on two dataset and two models , our framework can perform well compared to state - of - the - art IMC - pruning – CSAO [ 12 ] , SPRC [ 15 ] , XBA [ 11 ] and PIM - P [ 2 ] . Please note that all compared methods still used FP scaling factors in their quantization schemes . All lowest accuracy reductions are marked in boldface . For the small dataset like Cifar - 10 , all methods can compress the model to a high sparsity rate with a little accuracy drop . And our method can achieve a higher sparsity rate on the VGG - 16 model with even accuracy increasing 238 ASPDAC ’23 , January 16 – 19 , 2023 , Tokyo , Japan Shuo Huai , Di Liu , Xiangzhong Luo , Hui Chen , Weichen Liu , and Ravi Subramaniam Table 1 : Accuracy comparison of quantization methods . Dataset Network BaselineAcc . Method QuantizedAcc . Acc . Drop Cifar - 10 VGG - 16 93 . 28 IAO [ 8 ] 93 . 14 0 . 14 INT - quan 93 . 39 - 0 . 11 Resnet - 56 93 . 34 IAO [ 8 ] 93 . 44 - 0 . 10 INT - quan 93 . 37 - 0 . 03 ImageNet Resnet - 18 69 . 79 IAO [ 8 ] 69 . 54 0 . 25 INT - quan 69 . 48 0 . 31 Table 2 : Accuracy comparison of final compact models . Network ( Dataset ) Method BaselineAcc . SparsityRate FinalAcc . Acc . Drop VGG - 16 ( C ifar - 10 ) CSAO [ 12 ] 93 . 30 34 . 90 % 93 . 10 0 . 20 Ours 93 . 28 88 . 25 % 93 . 71 - 0 . 43 Resnet - 56 ( C ifar - 10 ) CSAO [ 12 ] 92 . 90 23 . 50 % 92 . 50 0 . 40 SPRC [ 15 ] - 33 . 40 % 92 . 80 - Ours 93 . 34 51 . 36 % 93 . 20 0 . 14 Resnet - 18 ( I mageNet ) XBA [ 11 ] 69 . 31 24 . 89 % 66 . 07 3 . 24 SPRC [ 15 ] 69 . 76 26 . 41 % 67 . 82 1 . 94 PIM - P [ 2 ] 69 . 76 32 . 41 % 68 . 67 1 . 09 Ours 69 . 79 50 . 50 % 68 . 82 0 . 97 ( indicating VGG - 16 over - fits Cifar - 10 much ) . For Resnet - 56 , we can minimize the accuracy drop 1 . For the large dataset like ImageNet , the sparsity rate is lower than that on Cifar - 10 , but our method achieves a large sparsity rate and higher accuracy . 4 . 2 . 3 Power & area evaluation . We use our simulator to evaluate models from different methods to get their power and area con - sumption . Please note that crossbar - column and crossbar - row level pruning methods need the Sparsity Table for data alignment , and FP processors are needed in methods with FP scaling factors . And SPRC [ 15 ] use 4 - bits quantization . As shown in Fig . 4 , the baseline is the original model , and the power and area consumption is normalized to the total consumption of the baseline . Specifically , we calculate the computing power and static power separately . Also , we calcu - lated the computing area ( i . e . , crossbars , multipliers and ST s ) and other area ( i . e . , interconnect and memory ) separately . Our method can achieve the highest power - efficiency and area - efficiency im - provement , especially for the computing parts . Compared to the original model , our method obtains 5 . 4 × total power reduction and 19 . 8 × computing power reduction on average . For the area , we can save 3 . 9 × total area and 18 . 8 × computing area . 5 CONCLUSION In this work , to comprehensively achieve low - power and efficient IMCacceleration , wefirstpresentedamulti - grainedcrossbar - aligned pruning method , including kernel - group pruning and crossbar pruning , for IMC architecture to save crossbars without incurring any hardware overhead . Then , we proposed a simple yet efficient integer - only quantization scheme to avoid using the multipliers in IMC devices . Finally , we designed a novel learning framework to extract an optimal compact model by a dynamic zero - recovery process , and it co - optimizes the pruning and quantization during the training process . The combination of multi - grained pruning , dynamic zero - recovery training , and co - optimization not only elim - inates expensive extra hardware but also achieves high accuracy and large sparsity . The evaluation results on the power and area con - sumption showed that compared to the original model , our method 1 SPRC does not report the baseline accuracy of Resnet - 56 on Cifar - 10 . And We use 4 - bits quantization to model its power and area . Figure 4 : Evaluation of the power and area . could achieve 19 . 8 × computing power reduction on average and save computing area by 18 . 8 × on average . In addition , compared to state - of - the - art methods , our method achieved higher accuracy with a larger sparsity rate , and we even improved accuracy by 0 . 43 % for VGG - 16 with an 88 . 25 % sparsity rate on the Cifar - 10 dataset . ACKNOWLEDGEMENT This study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects ( IAF - ICP ) Funding Initiative , as well as cash and in - kind contribution from the industry partner , HP Inc . , through the HP - NTU Digital Manufacturing Corporate Lab ( I1801E0028 ) . This work is also partially supported by the Ministry of Education , Singapore , under its Academic Research Fund Tier 2 ( MOE2019 - T2 - 1 - 071 ) and Tier 1 ( MOE2019 - T1 - 001 - 072 ) , and par - tially supported by Nanyang Technological University , Singapore , under its NAP ( M4082282 ) . REFERENCES [ 1 ] Pai - YuChenetal . 2018 . NeuroSim : Acircuit - levelmacromodelforbenchmarking neuro - inspired architectures in online learning . TCAD ( 2018 ) . [ 2 ] Chaoqun Chu et al . 2020 . PIM - Prune : fine - grain DCNN pruning for crossbar - based process - in - memory architecture . In 2020 57th DAC . IEEE . [ 3 ] Simon Du et al . 2019 . Gradient descent finds global minima of deep neural networks . In International Conference on Machine Learning . PMLR , 1675 – 1685 . [ 4 ] Sina Sayyah Ensan et al . 2019 . FPCAS : In - memory floating point computations for autonomous systems . In IJCNN . IEEE , 1 – 8 . [ 5 ] Gabriel Goh . 2017 . Why Momentum Really Works . Distill ( 2017 ) . [ 6 ] Kaiming He et al . 2016 . Deep Residual Learning for Image Recognition . In CVPR . [ 7 ] Shuo Huai et al . 2021 . ZeroBN : Learning Compact Neural Networks For Latency - Critical Edge Systems . In 2021 58th DAC . IEEE . [ 8 ] BenoitJacobetal . 2018 . Quantizationandtrainingofneuralnetworksforefficient integer - arithmetic - only inference . In CVPR . [ 9 ] William Kahan . 1996 . IEEE standard 754 for binary floating - point arithmetic . Lecture Notes on the Status of IEEE 94720 - 1776 ( 1996 ) , 11 . [ 10 ] AlexKrizhevskyetal . 2009 . Learningmultiplelayersoffeaturesfromtinyimages . Citeseer . [ 11 ] Ling Liang et al . 2018 . Crossbar - aware neural network pruning . IEEE Access . [ 12 ] Chenchen Liu et al . 2020 . Enabling efficient ReRAM - based neural network computing via crossbar structure adaptive optimization . In ISLPED . [ 13 ] Zhuang Liu et al . 2017 . Learning Efficient Convolutional Networks Through Network Slimming . In ICCV . [ 14 ] Huizi Mao et al . 2017 . Exploring the granularity of sparsity in convolutional neural networks . In CVPR . 13 – 20 . [ 15 ] Jian Meng et al . 2021 . Structured Pruning of RRAM Crossbars for Efficient In - Memory Computing Acceleration of Deep Neural Networks . Trans . Circuits Syst . II Express Briefs ( 2021 ) . [ 16 ] Naveen Muralimanohar et al . 2009 . CACTI 6 . 0 : A tool to model large caches . HP laboratories 27 ( 2009 ) , 28 . [ 17 ] OlgaRussakovskyetal . 2015 . ImageNetLargeScaleVisualRecognitionChallenge . IJCV ( 2015 ) . [ 18 ] Ali Shafiee et al . 2016 . ISAAC : A convolutional neural network accelerator with in - situ analog arithmetic in crossbars . In ACM SIGARCH Comput . Archit . News . [ 19 ] Karen Simonyan et al . 2014 . Very Deep Convolutional Networks for Large - Scale Image Recognition . In CVPR . [ 20 ] Shimeng Yu et al . 2016 . Emerging memory technologies : Recent trends and prospects . IEEE Solid - State Circuits Magazine 8 ( 2016 ) . 239