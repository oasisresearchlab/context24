RESEARCH CONlRlf3UllONS Human Aspects of Computing The Vocabulary Problem Hen y Ledgard Editor in Human - System Communication G . W . FURNAS , T . K . LANDAUER , L . M . GOMEZ , and S . T . DUMAIS ABSTRACT : In almost all computer applications , users must enter correct words for the desired objects or actions . For success without extensive training , or in first - tries for new targets , the system must recognize terms that will be chosen spontaneously . We studied spontaneous word choice for objects in five application - related domains , and found the variability to be surprisingly large . In every case two people favored the same term with probability < 0 . 20 . Simulations show how this fundamental property of language limits the success of various design methodologies for vocabulary - driven interaction . For example , the popular approach in which access is via one designer’s favorite single word will result in SO - 90 percent failure rates in many common situations . An optimal strategy , unlimited aliasing , is derived and shown to be capable of several - fold improvements . 1 . INTRODUCTION Many functions of most large systems depend on users typing in the right words . New or intermittent users often use the wrong words and fail to get the actions or information they want . This is the vocabulary problem . It is a troublesome impediment in computer interac - tions both simple ( file access and command entry ) and complex [ database query and natural language dialog ) . In what follows we report evidence on the extent of the vocabulary problem , and propose both a diagnosis and a cure . The fundamental observation is that people use a surprisingly great variety of words to refer to the same thing . In fact , the data show that no single access word , however well chosen , can be expected to cover more than a small proportion of users’ attempts . Design - ers have almost always underestimated the problem , and , by assigning far too few alternate entries to data - A much longer . highly technical version of some early parts of this work appeared in [ 3 ] . C 1987 ACM OOOl - 0782 / 87 / 1100 - 0964 $ 1 . 50 bases or services , created an unnecessary barrier to ef - fective use . Simulations and direct experimental tests of several alternative solutions show that rich , probabil - istically weighted indexes or alias lists can improve success rates by factors of three to five . 2 . THE VOCABULARY PROBLEM AS VARIABILITY IN WORD USAGE If everyone always agreed on what to call things , the user’s word would be the designer’s word would be the system’s word , and what the user typed or pointed to would be mutually understood . Unfortunately , people often disagree on the words they use for things . To experience the problem , try the exercise in Figure 1 . It is given here in the context of a command access ( “command naming” ) problem , but it could just as well have come from library information retrieval or data - base query . Ask one other colleague to alsio try it and compare your responses . We have asked this of more than a thousand pairs of people , including programmers , computer science stu - dents , and human factors specialists . Less than a dozen pairs have agreed . This means that without tutoring , less than a dozen people out of a thousand could have directly accessed the command , had their partner On a piece of paper write the name you would give to a program that tells about interesting activities occurring in some major metropolitan area ( e . g . , this program would tell you what is interesting Jo do on Friday or Saturday night ) . Make the name 10 characters or less . l’y to think of a name that will be as obvious a : ; possible , one that other people would think of , FIGURE 1 . To Demonstrate How Rarely Two People Will Agree on What to Call Things , Ask One Other Colleague to Try it too and Compare Your Responses 964 Communications of the ACM November 1987 Volume 30 Number 11 named it . ( As a follow - up exercise , list other good names you think of . You will be able to compare your list against one we reveal later . ) In current computer systems , the vocabulary problem is largely ignored . Designers decide on the terms to be used , and , as heavy users , grow to find these terms obvious and natural . Other users are simply required to learn the system’s words . ’ In information retrieval systems , the keywords that are assigned by indexers are often at odds with those tried by searchers . The seriousness of the problem is indicated by the need for professional intermediaries between users and systems , and by disappointingly low average performance ( recall ) rates . The standard conceptualization of the naming prob - lem is to begin with the system’s objects or functions , and ask what word or words should be associated with each : What name for this command ? What keyword for this document ? In Section 3 . 5 , we will argue that this approach is basically misleading , but will explore it first . Throughout we take an empirical approach , col - lecting large amounts of data on actual human language usage , then modeling and evaluating different system strategies . A more detailed exposition of the data col - lection and analysis methods can be found in [ 4 ] . 3 . THE DATA In an attempt to identify some basic principles of broad generality , we collected data on five very different but typical application domains . The words in square brackets are titles for the data sets , and will be used in the tables that follow . ( 1 ) ( 2 ) ( 3 ) ( 4 ) Main verbs applied by 48 typists to describe man - ual text editing operations , for 5 generic operations or 25 combinations of operations and text units [ Editor - 5 , Editor - 251 . The typists saw author - marked corrections and provided brief instructions as if to another typist . Commands for a hypothetical “message decoder” program [ “Decoder” ] , nominated by 100 experi - enced system designers after studying the design and interface . ’ The first content word used in describing a selec - tion of 50 common objects [ “Common objects” ] , given by 337 college students . They were shown short verbal descriptions such as “love , ” “motor - cycle , ” or “Newsweek , ” and asked to give alter - native words or phrases for a person or computer to understand . First - nominated superordinate category for 64 “Swap ‘n Sale” classified ad items [ “Classifieds” ] , given by 30 New Jersey homemakers . ’ Landauer . Galotti . and Hartwell 181 found that the initial learning of an editor by beginners was not much affected by the differences in the “natural - ness” of the small number of terms thev needed to learn . However . learnine a few names is not the major problem in’mastering one * s first computer applca - lion while knowing the right terms may be a very large factor in the effective use of exlensivs systems . For our data sets the probabilities that two people ( e . g . , a user and designer ) coincide on the word they spontaneously apply to a given object is presented in Results Table I . 3 ZData from P . Barnard . Applied Psychology Unit . Cambridge . England . per - 3 We use an unbiased statistical estimator of this quantity . the repeat rafe sonal communicalion . given in [ 7 ] . ( 5 ) Research Contributions First priority keyword nominated for a computer - ized file of 188 recipes . [ “Recipe Keywords” ] , by 8 expert cooks and 16 homemakers . These domains all involved information objects that one might want to access on a computer and were all of modest size ( 5 - 200 objects ) , but in most other respects they differed : in knowledge domain , in elicitation tech - nique , in type of people , in preprocessing ( e . g . , ignoring endings or noncontent words ) . Note also that the first two relate to command access , the third to general knowledge description , and the last two to information retrieval . Conclusions that apply to all of these dispar - ate domains must be based on general rather than idio - syncratic properties , and are therefore likely to hold for many others . In what follows we will use “object” or “item” to refer to any of the entities ( text editor operations , de - coder commands , common objects , advertised items , and cooking recipes ) which we presented to people as stimuli . The words we got back we will call “terms , ” “words , ” “descriptors , ” or “names . ” 3 . 1 Analyses For each domain , we counted how often each word was applied to each object . Samples of the resulting data are shown in Tables Ia and Ib . ( Objects are denoted at the top of each column by abbreviated examples or descrip - tions : the actual ‘objects’ seen by the subjects , as sum - marized above , are given in [ 4 ] . ) For example Table Ia indicates that 22 subjects pro - posed the word “change” as a descriptor for the editing operation indicated by a crossed out word by an author , and referred to in the table as “delete . ” All the tables were very sparse . One reason is that word usage tended to follow Zipf’s distribution [ 141 - a few words are used very frequently , the vast majority only rarely ; more importantly however , most words are applied to only a few objects . Because they estimate the likelihood of users or de - signers giving a word for an object , these tables allow us to simulate the performance of various approaches to word - based access . To see how untutored users’ words would fare against a system , we can simply sam - ple from these tables instead of going back to subjects . 3 . 2 “Armchair” Naming The simplest version of vocabulary - based access is when objects are accessible through a single special term - its “name . ” The most common strategy is for de - signers to install their own personal favorites as object names . We call this the “Armchair” design method . It gives users access only if their word coincides with the designer’s , November 1987 Volume 30 Number 11 Communications of the ACM 965 Research Contributions TABLE 1 . Word - Object Data Results Table I Probability bf two people applyi & the same term to an obieCt Common Recipe Editor - 5 Editor - 25 Decoder Objects Ck & eds & & ords . O7 . 11 . 08 . 12 . 14 38 Clearly people disagree greatly in the labels they use . The probability that two typists will use the same main verb in describing an editing operation is less than one in fourteen ; that two cooks will use the same first key - word for a recipe is less than one in five . The data tell us that the armchair method is highly unsatisfactory . If one person assigns the name of an item , other untutored people will fail to access it on 80 to 90 percent of their attempts . This finding is not only true of all six of our laboratory data sets ; it has also been confirmed several times by research with actual systems ( Furnas [ 4 ] ; Gomez and Lochbaum [ 5 ] ; Good , Whiteside , Wixon , and Jones [ 6 ] ) . One might be tempted to think that domain experts could do a better naming job . In the recipe study , one third of keyword providers were expert cooks . We found , however , that their keywords fared no better than average , either in indexing for other experts or for novices . Similarly the decoder command names were chosen by and for experts , and had very low agreement . So far we have assumed no requirement that access terms be distinct , that is , that each name be assigned to only one object . While in information retrieval con - texts , several documents may share a keyword , for command or file names uniqueness is typically impor - tant : the system must have an unambiguous interpreta - tion for the intended action and objects . Adding such a constraint obviously means that good candidate names for one object often get ruled out , having already been assigned to another object . As a result , untutored peo - ple will be even less likely to hit upon the official name . In our data , requiring uniqueness caused propor - tional decrements of 5 to 60 percent [ typically about 10 percent ) in the already low performances . Constraining names to be multiple word strings . system . atic , “cute . ” or even “mnemonic , ” is certain to result in even less likely first - try success . For example Good , et al . [ 6 ] found that their system’s initial , armchair - named com - mands , were slightly below our predictions , presum - ably reflecting other constraints imposed buy the system design on the name choices . 3 . 3 How Good is the Best Possible Name ? From the standpoint of first - try success for the un - trained , the best possible access term would be the word real users most often apply to an object . This empirically based naming approach has been a popular proposal in human factors circles : terms offered by a representative sample of potential users would be col - lected , and the most frequent term identified . We can identify this “best” name from data tables like Table 1 . For statistical reason ? data can only pro - ‘There is no way to get an unbiased . distribution - free estimate of the “true” value of the maximum - frequency cell . 966 Communications of the ACM November 1987 Volume 30 Number 11 Research Contributions vide estimates of bounds on the success rate of this method . A low estimate can be obtained by using a random half of the data to pick the best words , and the remaining half to test their effectiveness . A high esti - mate can be obtained by using the actual relative fre - quency in the data of the most popular word . Results Table z presents the range of these low and high estimate hit rates for the data sets . Results Table II Probability of someone using the most popular term for an object Recipe Common Class - Key - Editor - 5 Editor - 25 Decoder Objects ifieds words . 15 . 19 . 22 . 26 . 26 . 31 ( low estimate ) . 16 . 22 . 21 . 28 . 34 . 36 ( high estimate ] While there is typically about a 2 to 1 improvement over the straight armchair method , failures still occur 65 - 85 percent of the time . Since no term will be hit upon more often than the most popular term , these numbers represent a true per - formance limit when a single access term is assigned to each object . Simply stated , the data tell us there is no one good uccess tern1 for most objects . The idea of an “obvious , ” “ self - evident , ” or “natural” term is a myth ! Since even the best possible name is not very useful , it follows that there can exist no rules , guidelines or proce - dures for choosing a good name , in the sense of “accessible to the unfamiliar user . ” Any further improvement will require a different strategy . 3 . 4 The Value of a Few Aliases Once the idea of a single access term is rejected , the next logical suggestion is to try several access terms . Library catalogues and indices , for example , usually have at least two ( but seldom more than four ) entry points for each referenced object . Below we present the estimated performance of a system where each object had three access terms known to the system , selected by frequency - weighted random sampling , mimicking the frequencies of a de - sign group’s first three “armchair nominations . ” Results Table III Probability of someone using one of three armchair aliases for an object Common Recipe Editor - 5 Editor - 25 Decoder Objects Classifieds Keywords . 21 . 30 . 20 . 26 . 34 . 45 While the improvement over a single armchair name is considerable , three armchair aliases are no better than a single optimally chosen term . If the three most popular , i . e . , three optimally chosen access terms , are used , the following results are ob - tained . Results Table IV Probability of someone using one of the three most popular aliases for an object Recipe Common Class - Key - Editor - 5 Editor - 25 Decoder Objects ifieds words . 37 . 42 . 38 . 42 . 45 . 58 ( low estimate ) . 38 . 49 . 41 . 48 . 59 . 67 ( high estimate ] This approach looks promising . Unfortunately , the mar - ginal returns for even more aliases diminishes rapidly . Figure z presents an operating characteristic , plotting how many optimally selected aliases are needed to ac - count for a given percentage of the untutored users’ attempts . The figure shows the results for the Common Objects data , which is typical of our data sets . 100 80 20 0 I I I I I I I I I 100 80 60 40 20 0 1 5 10 15 20 Number of words needed per object ( common object data ) FIGURE 2 . Plot of How Many Different Names [ or “aliases” ) Are Needed for Each Object to Account for a Given Percentage of Success ( the exact values are not known because of statistical estimation problems , so ranges are indicated by dashed lines ) November 1987 Volume 30 Number II Communications of the ACM 967 Research Contributions The two curves present the high and low statistical functionality and good design . 5 But , regardless of the estimates respectively . ( Especially for larger numbers of number of commands or objects in a system and what - aliases , practical limits are probably closer to the lower than the upper estimate . ) Even with fifteen aliases , only ever the choice of their “official” names , the designer must make available many , many alternate verbal access 60430 percent of “attempts” will be satisfied . routes to each . Clearly the only hope for untutored vocabulary driven access is to provide many , many alternate entry terms . Thus aliases are , indeed , the answer , but only if used on a much larger scale than usually considered . The tendency to underestimate the need for so many aliases results , we conjecture , from the fact that any one person [ for example , the designer ) usually can think of only a handful of the terms that other people would consider appropriate . For the example exercise in Figure 1 at the beginning of the paper it is rare for one person to come up with more than a half dozen names . Across all people , however , over a hundred command names for that service have been suggested , including : activities , calendar , events , cityevents , whatsup , sparetime , funtime , weekender , nightout , downtown , entertain , outings , diversions , play , fun & games , amusements , guide , goingson , happenings , thingstodo , aroundtown , leisure , weekend , nightlife , abouttown , onthetown . . . 3 . 5 . 1 The Precision Problem . Before we discuss the practicality and estimated performance of this ap - proach , we must face what is called , in information retrieval literature , the “precision” problem . Whenever a given word is applied to more than one object , ambiguity results . For example , in the data of Table Ib , PEAR ( frequency 19 ) is the best guess for “fruit , ” but the user might also mean NECTARINE ( fre - quency 12 ) or even RAISIN ( frequency 11 . The “fruit” example illustrates imprecision arisina from the nener - ality of a term . It may Also arise from polysemy - the same word may mean two or more distinct things . In retrieval based on the term “nut” a system can only guess whether to return items related to h . ardware , seeds or eccentrics . a How serious is the precision problem ? I : n our data the probabilities that two people intended the same referent ( s ) by a given term were : 3 . 5 A Solution is Unlimited Aliasing Our discussion of the “unfamiliar access” problem has followed the spirit of common practice , and been domi - nated by a computer - centered point of view . Beginning with the set of system entities , various schemes for assigning access terms were proposed and evaluated . We believe this approach is misleading because it tends to focus on what the computer brings to the interaction ( the system’s objects ) , yet it is a characteristic of what the human brings , the variability in vocabulary usage , that dominates the problem . Results Table V Probability that two people using a term intend the same object COOlmOO Recipe Editor - 5 Editor - 25 Decoder Objects Classifieds Keywords . 41 . t5 . 73 . 52 62 . 13 If we want the system to have the best chance of “understanding” the user , it is better to conceive of design the other way around ; begin with user’s words , and find system interpretations . We need to know , for every word that users will try in a given task environ - ment , the relative frequencies with which they would be satisfied by various objects . The system would be designed using a table of word usage much like the ones we have collected . When users attempt access with a word , the highest frequency object for that word is the system’s best guess as to the user’s intent . For example in Table l ( b ) , when confronted by a user’s term “fruit , ” the best guess is a PEAR . If we had com - plete data this approach would yield the theoretically optimum performance for getting users and objects together . Here there was considerable variation between data sets . In the recipe data , two uses of a given keyword refer to the same recipe only about an eighth of the time , reflecting the fact that cooking terms , often refer to several recipes . In contrast , two occurrences of a particular name for the hypothetical decoder referred to the same action three quarters of the time , indicating more restricted and precise usage . In terms of the designer’s usual system - oriented standpoint , the proposal here is to allow essentially unlimited numbers of aliases . This is not to say that designers should build systems with unruly thousands of commands or objects in them . The number of dis - tinct objects or commands needed is a matter of system There was a modest but reliable tendency within each data set for less frequent terms to be : more precise [ 12 ] . Often more popular words , while being more likely for a given object , are also more likely for other objects ( e . g . , “change” in Table l ( a ) ) . This clbservation is important since it means unlimited aliasing will not incur a great precision cost . Having more access words per object does not logically imply more objects per word , and empirically the average tendency appears to be the reverse . Moreover , in an interactive situation in which users can refine their selections by a series of ‘In fact , careful system design may itself help the untutorec’s vocabulary access problem . For example . if a system’s intrinsic structure can be designed to suooort it . a factorial task decomwsition can lead to oredictable oatterns of . . official terminology . perhaps allowing users to predict some terminology from others ( “delete word . ” “transpose word . ” “delete sentence . ” = = a “transpose sen - k ? “OZ” ) . 966 Communications of the ACM November 1987 Volume : $ O Number 11 Research Contributions inputs , the higher hit rates afforded by extensive aliasing has been found to be extremely beneficial even when it does carry with it a lower average precision [ 51 . 3 . 52 Dealing with Imprecision : Disambiguation . Our data show that any keyword system capable of provid - ing a high hit rate for unfamiliar users must let them use words of their own choice for objects . But , because of the inherent amgibuity of users’ own words , the sys - tem can never be sure it has correctly inferred the user’s referent ; it can only make good guesses . The cost of an incorrect guess must therefore be considered . While users might be happy with a system’s mere guess about a book to look at , many commands are too conse - quential to be executed on guessed intent ( e . g . , “delete file” ) and so the inherent vagueness in words must be resolved . There are two approaches : either make the user memorize precise system meanings , or have the user and system interact to identify the precise referent . The latter might possibly be done in many ways , including interpretation of multiterm Boolean expressions6 , for - mal query languages or “natural language” understand - ing , although none of these has demonstrated notable success to date . A simpler approach , which our data let us model , is to return a set of choices to the user when - ever there is ambiguity . If we assume that in the re - stricted context of a particular application the user can recognize the correct interpretation on sight , there is probably no faster route to resolution . As we will show below , with unlimited aliases the number of alterna - tives to be returned for user selection will usually be small . Alternatives can be ordered by frequency , the most likely objects first . Nevertheless , correct recogni - tion by the user requires adequate description of the meaning or content of each choice , perhaps including more precise terms , examples , etc . It must be realized that this too is a highly error prone communication process [ Z ] . One question raised by the unlimited alias proposal is whether people will acquire undesirable habits if the system is so tolerant . We think this problem is self limiting . As mentioned , in command execution ( though not necessarily in information retrieval ) spontaneous entries will often require disambiguation procedures . Since such procedures are inherently costly to users , there is incentive for them to move toward more effi - cient language . This suggests using the unlimited alias system only as an index into a help facility . When users invoke it with their own words , the system would pick its best guesses , and present them in a menu . Each guess would be labeled by some “standard” access ‘Note that in terms of a system correctly recognizing untutored users’ terms . Boolean conjunctions multiply the single - term problem addressed in this pa - per . For examtk indexer and retriever might a . ree to call larae thines . “bia” with probabiliiy - 0 . 1 . and circular things , “ & nd . ” with probability % Z . - Then . assuming independence . the Boolean combination , “big and round” will retrieve only 0 . 02 of the desired large . circular things . terminology and be accompanied by a description of the standard referent . Actual execution would always be via the standard “name , ” thereby encouraging the learning of precise terms required to take the disambiguation process out of the loop . 3 . 5 . 3 Performance of Unlimited Aliasing Systems . We wish to know how well unlimited aliasing can work in principle and what can be expected in practice . First , how good would performance be given an infi - nite amount of data ? By definition the system would recognize all user words . However , it might have to make several guesses about the intended object . As - sume the system asked the user to verify one object at a time , and the user was only satisfied by exactly one object in the database . Then , for our data sets , the me - dian number of guesses before success in each disambi - guation dialogue would be approximately as follows ( the number in parentheses is a reminder of how many objects there were in the domain and is useful to help judge the magnitude of the accomplishment ) . Results Table VI Median number of system object guesses for success Guesses Objects in domain Recipe Common Class - Key - Editor - 5 Editor - 25 Decoder Objects ifieds words In real life , we can never have complete data on word - object usage . Incomplete data can have several detri - mental effects , the most important being that the sys - tem might not know a user’s word at all , or only have it listed with wrong objects . This would lead to a com - plete miss , of the sort found very frequently in arm - chair naming methods . The seriousness of this problem varied from domain to domain . In the decoder data , even after 100 subjects were asked about each object , a new person would have had a 28 percent chance of proposing a completely new term . In contrast , after only 24 people had pro - posed keywords for a recipe , there was less than one chance in 50 that the next person would come up with a new term . There is no single number characterizing how overall performance would suffer from incomplete data ; it would depend on how much data was collected and the variability of naming in the given domain . We esti - mated how well a system could do in our domains with half the amount of data we originally collected . At the median number of guesses needed with infinite data , performance was degraded by approximately one third when restricted to half our available data : performance was cut in half for Classifieds using only 8 subjects’ November 1987 Volume 30 Number II Communicarions of the ACM 969 Research Contributions data , cut by a third for the Common Objects using 178 subjects’ data , and cut by only 10 percent for the Editor - 5 data using 24 subjects’ data . 4 . Summary and Discussion We have been studying vocabulary problems in using unfamiliar computer applications . We can summarize the findings in a few major points . First , a single access term ( e . g . , a “name” ) chosen by a single designer will provide very poor access ( 10 - 20 percent hit rates ) . Sec - ond , a single empirically optimized term is much bet - ter : and will work as well as 2 - 3 armchair aliases to - gether . Third , to achieve really good performance , very many aliases are needed . An empirically based , fre - quency weighted “unlimited aliases” system can often produce 50 to 100 percent hit rates in its first three guesses to untutored queries , depending on the domain and the amount of data collected . In the limit , the only barrier is precision , and our data show that frequency data on the possible objects intended by a given term will usually provide good ambiguity resolution . This brings us to the conclusion that reasonable ac - cess can be provided for the untutored , but only if an extensive table of word usage behavior is compiled . Such a solution is technically simple , but potentially tedious . We note that to some extent the collection of multiple aliases will arise in any well done iterative interface design . Good , et al . [ 6 ] provide such an exam - ple . They report that alias collection accounted for the largest share of the dramatic interface performance im - provements resulting from a careful iterative design process with extensive user testing . Iterative design of interfaces is very useful for many reasons , but our analyses suggest the vocabulary prob - lem can be attacked separately . But what can be done about the tedium and cost ? Fortunately there appear to be several attractive alternatives . A “quick and dirty” approach is to get a fair number , say 4 to 8 representa - tive users to supply a fair number , say 3 to 6 , terms apiece for each object . An experiment by Gomez and Lochbaum [ 5 ] with a small interactive search system obtained the predicted four - fold increase in users’ abil - ity to find a desired recipe using this approach . This technique is most likely to be cost - effective for data - bases of moderate size that are expected to remain sta - ble over long periods and be used infrequently by many people . A more familiar approach for textual objects is to extract multiple access terms automatically from the content . The recipe texts contain roughly 80 percent of the keywords selected by users , and in an experi - ment to be reported elsewhere , Gomez , Lochbaum and Landauer found full - text indexing to be roughly equivalent to extensive alias “harvesting” from experts for the recipe database . A third , exciting possibility is to construct unlimited alias indices adaptively , on site , in use . Such adaptive indexing methods are discussed elsewhere [ 3 ] but the idea is simple and has some partial precursors in the literature [ 9 ] [ lo ] . Basically an adaptive indexing sys - tem begins with an index obtained in an ordinary way ( for example , by armchair naming ) . Users try their own words to access desired objects , and , on their first few tries , usually fail . But sometimes they wil ; . eventually find something they want . With user concurrence , the initially unsuccessful words are added to - the system’s usage table . The next time someone seeks the object , the index is more knowledgeable . Adaptive indexes collect the needed data relatively painlessly , from the right users under authentic conditions . They also start to pay off rapidly , since they automaticall : y tend to fo - cus data collection on the most sought - after objects . Trials of such systems show that they produce the kind of large performance improvements predicted in this paper for systems with massive alias lists . Thus we can augment our earlier conclusion to say that not only is drastic improvement needed and possi - ble , but also practicable . In closing , we would like to comment briefly on the research strategy that was employed in this work . Typi - cally , human - computer interaction research has in - volved the comparison of two or more particular sys - tems or features , either by model - based analysis ( e . g . , Card , Moran and Newell ( 11 ) . by a one - time “contest” ( e . g . , Roberts and Moran [ ll ] ) . or by successive iterative test ( e . g . , Good , et al . [ 6 ] ) . This approach is . valuable in increasing the speed and accuracy with which the field can choose good examples to follow . But , because of the complexity of most systems , it is limited in the general - ity of its conclusions . By contrast , the approach exemplified here begins with what we call a failure analysis . In pa : rticular . we look for failures of humans to obtain desired results in interactions with computers . In the present case , we found that people often enter words that fail to be cor - rectly recognized by systems . We then collect experi - mental or observational data , do simulations and model building - whatever is necessary to unders : tand what the human can and will do in the situation , and what is needed on the part of the machine in order that the combination of human and machine can succeed more often . In the present case , we discovered unexpectedly large and pervasive variability in spontaneous term ap - plication , and learned that systems need to recognize a very rich variety of aliases . If the emerging machine requirements can be realized practically , the next step to invention or improved design is straightforward . In the present case , many avenues for interface improve - ments are apparent . It is essential to stress that in this approach inven - tions and design improvements come from basic under - standing of performance problems and hurnan behav - ior , not from analysis or tests of alternative designs . The extreme variability of word selection by humans is a fundamental fact of human behavior found by studying a performance problem . Knowledge of this fact can lead to better system design in a wide range of applications , not just in the choice of one system over a : nother . 970 Communications of the ACM November 1987 Volume 30 Number 71 SUMMARY : Many , many alternative access words are needed for users to get what they want from large and complex systems . REFERENCES 1 . Card . S . , Moran . T . P . . and Newell , A . The Psychology of Hunra ~ ~ - Compufer Interaction . Lawrence Erlbaum Associates . Hillsdale . N . J . . 1983 . 2 . Dumais . ST . , and Landauer . T . K . Describing categories of objects for menu retrieval systems . Brhavior Research Methods . lrrslrunmrts . b Compu ~ us , 16 . 2 ( Apr . 1984 ) . 242 - 248 . 3 . Furnas . G . W . Experience with an adaptive indexing scheme . Hunla ~ ~ Factors in Computer Sysfenls , CHI ‘85 Proceedings . Conference held in San Francisco , CA , April 15 - 18 . 1985 . 131 - 135 . 4 . Furnas , G . W . , Landauer , T . K . . Gomez . L . M . . and Dumais . ST . Statis - tical semantics : Analysis of the potential performance of key - word information systems . Bell System Technical / oumal . 62 . 6 ( Jul . - Aug . 1983 ) . 1753 - 1806 . 5 . Gomez . L . M . . and Lochbaum , CC . People can retrieve more objects with enriched key - word vocabularies . But is there a human per - formance cost ? In B . Shackel ( Ed . ) Human - Computer Inleractm - Interact ‘84 , North - Holland . Amsterdam . 257 - 261 . 6 . Good , M . D . , Whiteside . J . A . . Wixon . D . R . . and Jones . S . J . Building a user - derived interface . Comn ~ un . ACM , 27 , 10 ( Oct . 1984 ) . 1032 - 1043 . 7 . Herdan . G . Type Tokerr Mathematics : A Textbook of Mathenratical Lin - guisfics , S - Gravenhage . Mouton . 1960 . 8 . Landauer . T . K . , Galotti . K . . and Hartwell . S . Natural command names and initial learning : A study of text editing terms . Conrntun . ACM , 26 , 7 ( Jul . 1983 ) . 495 - 503 . 9 . Reisner . P . Construction of a growing thesaurus by conversational interaction in a man - machine system . Proceedings of the American Dmmw ~ fafim Insfifute . 26th Annual Meeting . Chicago , Ill . October 1963 . 10 . Reisner . P . Evaluation of a ‘Growing Thesaurus’ . Research Paper RC - 1662 . August 9 . 1966 . IBM Watson Research Center . Yorktown Heights , N . Y . Research Contributions 11 . Roberts . T . L . . and Moran . T . P . The evaluation of text editors : Meth - odology and empirical results . C ~ vvn ~ utt . ACM , 26 . 4 ( Apr . 1983 ) . 265 - 283 . 12 . Sparck - Jones . K . A Statistical interpretation of term specificity and its application in retrieval . I . LXX . 28 . 1 ( Mar . 1972 ) . 11 - 21 . 13 . Whalen , T . . and Latremouille . S . The effectiveness of a tree - structured index when the existence of information is uncertain . Tcledort Behavioral Research 2 : The Drsip of Vidrotex Trre Itldiccs . Ottawa , Canada : Department of Communications . ( May 1981 ) . pp . 3 - 12 . 14 . Zipf , G . K . Hunra ~ l Behavior wd the Prirmple of Least Effort . AII Irtfnr - durtuw fn Human Emlogy . Addison - Wesley . Reading . Mass . . 1949 . CR Categories and Subject Descriptors : H . l . 2 [ Models and Princi - ples ] : User / Machine Systems - hunfatr factors . huntarl i @ wrmtim pnm’ss - iq ; H . 3 . 1 [ Information Storage and Retrieval ] : Content Analysis and Indexing - frldrxiq Methods . Lirlpisfir Pror ? ssiq . Thrsouruscs General Terms : Human Factors Additional Key Words and Phrases : human - computer communica - tion . human - computer interaction . keyword . repeat rate . vocabulary . Zipf law Received 7 / 85 : revised 4 / 86 : accepted 12 / 86 Authors’ Present Address : G . W . Furnas . T . K . Landauor . L . M . Gomez S . T . Dumais . Bell Communications Research . Inc . . 435 South Street . Morristown . Nl 07960 . Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commer - cial advantage , the ACM copyright notice and the title of the publication and its date appear . and notice is given that copying is by permission of the Association for Computing Machinery . To copy otherwise . or to republish . requires a fee and / or specific permission . 1988 ACM COMPUTER SCIENCE CONFERENCE” FEBRUARY 23 - 25 ATLAN’EA , GEORGIA n Quality Technical Program H Educational Exhibits n CSC Employment Register n National Scholastic Programming Contest n SIGCSE Technical Symposium Attendance & Exhibits Information : November 1987 Volume 30 Number 11 Communications of the ACM 971