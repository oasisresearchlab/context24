Reliability and Learnability of Human Bandit Feedback for Sequence - to - Sequence Reinforcement Learning Julia Kreutzer 1 and Joshua Uyheng 3 ∗ and Stefan Riezler 1 , 2 1 Computational Linguistics & 2 IWR , Heidelberg University , Germany { kreutzer , riezler } @ cl . uni - heidelberg . de 3 Departments of Psychology & Mathematics , Ateneo de Manila University , Philippines juyheng @ ateneo . edu Abstract We present a study on reinforcement learn - ing ( RL ) from human bandit feedback for sequence - to - sequence learning , exem - pliﬁed by the task of bandit neural ma - chine translation ( NMT ) . We investigate the reliability of human bandit feedback , and analyze the inﬂuence of reliability on the learnability of a reward estimator , and the effect of the quality of reward esti - mates on the overall RL task . Our anal - ysis of cardinal ( 5 - point ratings ) and ordi - nal ( pairwise preferences ) feedback shows that their intra - and inter - annotator α - agreement is comparable . Best reliabil - ity is obtained for standardized cardinal feedback , and cardinal feedback is also easiest to learn and generalize from . Fi - nally , improvements of over 1 BLEU can be obtained by integrating a regression - based reward estimator trained on cardinal feedback for 800 translations into RL for NMT . This shows that RL is possible even from small amounts of fairly reliable hu - man feedback , pointing to a great potential for applications at larger scale . 1 Introduction Recent work has received high attention by suc - cessfully scaling reinforcement learning ( RL ) to games with large state - action spaces , achieving human - level ( Mnih et al . , 2015 ) or even super - human performance ( Silver et al . , 2016 ) . This success and the ability of RL to circumvent the data annotation bottleneck in supervised learning has led to renewed interest in RL in sequence - to - sequence learning problems with exponential ∗ The work for this paper was done while the second au - thor was an intern in Heidelberg . output spaces . A typical approach is to com - bine REINFORCE ( Williams , 1992 ) with poli - cies based on deep sequence - to - sequence learn - ing ( Bahdanau et al . , 2015 ) , for example , in ma - chine translation ( Bahdanau et al . , 2017 ) , seman - tic parsing ( Liang et al . , 2017 ) , or summarization ( Paulus et al . , 2017 ) . These RL approaches fo - cus on improving performance in automatic eval - uation by simulating reward signals by evalua - tion metrics such as BLEU , F1 - score , or ROUGE , computed against gold standards . Despite coming from different ﬁelds of application , RL in games and sequence - to - sequence learning share ﬁrstly the existence of a clearly speciﬁed reward func - tion , e . g . , deﬁned by winning or losing a game , or by computing an automatic sequence - level evalu - ation metric . Secondly , both RL applications rely on a sufﬁcient exploration of the action space , e . g . , by evaluating multiple game moves for the same game state , or various sequence predictions for the same input . The goal of this paper is to advance the state - of - the - art of sequence - to - sequence RL , exempli - ﬁed by bandit learning for neural machine trans - lation ( NMT ) . Our aim is to show that successful learning from simulated bandit feedback ( Sokolov et al . , 2016b ; Kreutzer et al . , 2017 ; Nguyen et al . , 2017 ; Lawrence et al . , 2017 ) does in fact carry over to learning from actual human bandit feed - back . The promise of bandit NMT is that human feedback on the quality of translations is easier to obtain in large amounts than human references , thus compensating the weaker nature of the signals by their quantity . However , the human factor en - tails several differences to the above sketched sim - ulation scenarios of RL . Firstly , human rewards are not well - deﬁned functions , but complex and inconsistent signals . For example , in general ev - ery input sentence has a multitude of correct trans - lations , each of which humans may judge differ - a r X i v : 1805 . 10627v3 [ c s . C L ] 13 D ec 2018 ently , depending on many contextual and personal factors . Secondly , exploration of the space of pos - sible translations is restricted in real - world scenar - ios where a user judges one displayed translation , but cannot be expected to rate an alternative trans - lation , let alone large amounts of alternatives . In this paper we will show that despite the fact that human feedback is ambiguous and partial in nature , a catalyst for successful learning from hu - man reinforcements is the reliability of the feed - back signals . The ﬁrst deployment of bandit NMT in an e - commerce translation scenario conjectured lacking reliability of user judgments as the rea - son for disappointing results when learning from 148k user - generated 5 - star ratings for around 70k product title translations ( Kreutzer et al . , 2018 ) . We thus raise the question of how human feed - back can be gathered in the most reliable way , and what effect reliability will have in downstream tasks . In order to answer these questions , we measure intra - and inter - annotator agreement for two feedback tasks for bandit NMT , using car - dinal feedback ( on a 5 - point scale ) and ordinal feedback ( by pairwise preferences ) for 800 trans - lations , conducted by 16 and 14 human raters , respectively . Perhaps surprisingly , while relative feedback is often considered easier for humans to provide ( Thurstone , 1927 ) , our investigation shows that α - reliability ( Krippendorff , 2013 ) for intra - and inter - rater agreement is similar for both tasks , with highest inter - rater reliability for stan - dardized 5 - point ratings . In a next step , we address the issue of machine learnability of human rewards . We use deep learn - ing models to train reward estimators by regres - sion against cardinal feedback , and by ﬁtting a Bradley - Terry model ( Bradley and Terry , 1952 ) to ordinal feedback . Learnability is understood by a slight misuse of the machine learning notion of learnability ( Shalev - Shwartz et al . , 2010 ) as the question how well reward estimates can approx - imate human rewards . Our experiments reveal that rank correlation of reward estimates with TER against human references is higher for regression models trained on standardized cardinal rewards than for Bradley - Terry models trained on pairwise preferences . This emphasizes the inﬂuence of the reliability of human feedback signals on the qual - ity of reward estimates learned from them . Lastly , we investigate machine learnability of the overall NMT task , in the sense of Green et al . ( 2014 ) who posed the question of how well an MT system can be tuned on post - edits . We use an RL approach for tuning , where a crucial difference of our work to previous work on RL from human re - wards ( Knox and Stone , 2009 ; Christiano et al . , 2017 ) is that our RL scenario is not interactive , but rewards are collected in an ofﬂine log . RL then can proceed either by off - policy learning using logged single - shot human rewards directly , or by using es - timated rewards . An expected advantage of esti - mating rewards is to tackle a simpler problem ﬁrst — learning a reward estimator instead of a full RL task for improving NMT — and then to de - ploy unlimited feedback from the reward estimator for off - policy RL . Our results show that signiﬁcant improvements can be achieved by training NMT from both estimated and logged human rewards , with best results for integrating a regression - based reward estimator into RL . This completes the ar - gumentation that high reliability inﬂuences quality of reward estimates , which in turn affects the qual - ity of the overall NMT task . Since the size of our training data is tiny in machine translation propor - tions , this result points towards a great potential for larger - scaler applications of RL from human feedback . 2 Related Work Function approximation to learn a “critic” instead of using rewards directly has been embraced in the RL literature under the name of “actor - critic” methods ( see Konda and Tsitsiklis ( 2000 ) , Sut - ton et al . ( 2000 ) , Kakade ( 2001 ) , Schulman et al . ( 2015 ) , Mnih et al . ( 2016 ) , inter alia ) . In differ - ence to our approach , actor - critic methods learn online while our approach estimates rewards in an ofﬂine fashion . Ofﬂine methods in RL , with and without function approximation , have been pre - sented under the name of “off - policy” or “coun - terfactual” learning ( see Precup et al . ( 2000 ) , Pre - cup et al . ( 2001 ) , Bottou et al . ( 2013 ) , Swami - nathan and Joachims ( 2015a ) , Swaminathan and Joachims ( 2015b ) , Jiang and Li ( 2016 ) , Thomas and Brunskill ( 2016 ) , inter alia ) . Online actor - critic methods have been applied to sequence - to - sequence RL by Bahdanau et al . ( 2017 ) and Nguyen et al . ( 2017 ) . An approach to off - policy RL under deterministic logging has been pre - sented by Lawrence et al . ( 2017 ) . However , all these approaches have been restricted to simulated rewards . RL from human feedback is a growing area . Knox and Stone ( 2009 ) and Christiano et al . ( 2017 ) learn a reward function from human feed - back and use that function to train an RL system . The actor - critic framework has been adapted to interactive RL from human feedback by Pilarski et al . ( 2011 ) and MacGlashan et al . ( 2017 ) . These approaches either update the reward function from human feedback intermittently or perform learn - ing only in rounds where human feedback is pro - vided . A framework that interpolates a human cri - tique objective into RL has been presented by Ju - dah et al . ( 2010 ) . None of these works system - atically investigates the reliability of the feedback and its impact of the down - stream task . Kreutzer et al . ( 2018 ) have presented the ﬁrst application of off - policy RL for learning from noisy human feedback obtained for determinis - tic logs of e - commerce product title translations . While learning from explicit feedback in the form of 5 - star ratings fails , Kreutzer et al . ( 2018 ) pro - pose to leverage implicit feedback embedded in a search task instead . In simulation experiments on the same domain , the methods proposed by Lawrence et al . ( 2017 ) succeeded also for neural models , allowing to pinpoint the lack of reliabil - ity in the human feedback signal as the reason for the underwhelming results when learning from hu - man 5 - star ratings . The goal of showing the effect of highly reliable human bandit feedback in down - stream RL tasks was one of the main motivations for our work . For the task of machine translation , estimat - ing human feedback , i . e . quality ratings , is re - lated to the task of sentence - level quality estima - tion ( sQE ) . However , there are crucial differences between sQE and the reward estimation in our work : sQE usually has more training data , often from more than one machine translation model . Its gold labels are inferred from post - edits , i . e . cor - rections of the machine translation output , while we learn from weaker bandit feedback . Although this would in principle be possible , sQE predic - tions have not ( yet ) been used to directly reinforce predictions of MT systems , mostly because their primary purpose is to predict post - editing effort , i . e . give guidance how to further process a trans - lation . State - of - the - art models for sQE such as ( Martins et al . , 2017 ) and ( Kim et al . , 2017 ) are unsuitable for the direct use in this task since they rely on linguistic input features , stacked architec - Figure 1 : Rating interface for 5 - point ratings . Figure 2 : Rating interface for pairwise ratings . tures or post - edit or word - level supervision . Sim - ilar to approaches for generative adversarial NMT ( Yu et al . , 2017 ; Wu et al . , 2017 ) we prefer a sim - pler convolutional architecture based on word em - beddings for the human reward estimation . 3 Human MT Rating Task 3 . 1 Data We translate a subset of the TED corpus with a general - domain and a domain - adapted NMT model ( see § 6 . 2 for NMT and data ) , post - process the translations ( replacing special charac - ters , restoring capitalization ) and ﬁlter out identi - cal out - of - domain and in - domain translations . In order to compose a homogeneous data set , we ﬁrst select translations with references of length 20 to 40 , then sort the translation pairs by difference in character n - gram F - score ( chrF , β = 3 ) ( Popovi´c , 2015 ) and length , and pick the top 400 translation pairs with the highest difference in chrF but lowest difference in length . This yields translation pairs of similar length , but different quality . 3 . 2 Rating Task The pairs were treated as 800 separate transla - tions for a 5 - point rating task . From the orig - inal 400 translation pairs , 100 pairs ( or 200 in - dividual translations ) were randomly selected for Inter - rater Intra - rater Type α Mean α Stdev . α 5 - point 0 . 2308 0 . 4014 0 . 1907 5 - point norm . 0 . 2820 5 - point norm . part . 0 . 5059 0 . 5527 0 . 0470 5 - point norm . trans . 0 . 3236 0 . 3845 0 . 1545 Pairwise 0 . 2385 0 . 5085 0 . 2096 Pairwise ﬁlt . part . 0 . 3912 0 . 7264 0 . 0533 Pairwise ﬁlt . trans . 0 . 3519 0 . 5718 0 . 2591 Table 1 : Inter - and intra - reliability measured by Krippendorff’s α for 5 - point and pairwise ratings of 1 , 000 translations of which 200 translations are repeated twice . The ﬁltered variants are restricted to either a subset of participants ( part . ) or a subset of translations ( trans . ) . repetition . This produced a total of 1 , 000 indi - vidual translations , with 600 occurring once , and 200 occurring twice . The translations were shuf - ﬂed and separated into ﬁve sections of 200 trans - lations , each with 120 translations from the unre - peated pool , and 80 translations from the repeated pool , ensuring that a single translation does not oc - cur more than once in each section . For a pair - wise task , the same 100 pairs were repeated from the original 400 translation pairs . This produced a total of 500 translation pairs . The translations were also shufﬂed and separated into ﬁve sections of 100 translation pairs , each with 60 translation pairs from the unrepeated pool , and 40 translation pairs from the repeated pool . None of the pairs were repeated within each section . We recruited 14 participants for the pairwise rating task and 16 for the 5 - point rating task . The participants were university students with ﬂuent or native language skills in German and English . The rating interface is shown in Figures 1 and 2 . Rat - ing instructions are given in Appendix A . 1 . Note that no reference translations were presented since the objective is to model a realistic scenario for bandit learning . 1 4 Reliability of Human MT Ratings 4 . 1 Inter - rater and Intra - rater Reliability In the following , we report inter - and intra - rater re - liability of the cardinal and ordinal feedback tasks described in § 3 with respect to Krippendorff’s α 1 The collection of ratings can be downloaded from http : / / www . cl . uni - heidelberg . de / statnlpgroup / humanmt / . ( Krippendorff , 2013 ) evaluated at interval and or - dinal scale , respectively . As shown in Table 1 , measures of inter - rater reliability show small differences between the 5 - point and pairwise task . The inter - rater reliabil - ity in the 5 - point task ( α = 0 . 2308 ) is roughly the same as that of the pairwise task ( α = 0 . 2385 ) . Normalization of ratings per participant ( by stan - dardization to Z - scores ) , however , shows a marked improvement of overall inter - rater reliability for the 5 - point task ( α = 0 . 2820 ) . A one - way analysis of variance taken over inter - rater reli - abilities between pairs of participants suggests statistically signiﬁcant differences across tasks ( F ( 2 , 328 ) = 6 . 399 , p < 0 . 01 ) , however , a post hoc Tukey’s ( Larsen and Marx , 2012 ) honest sig - niﬁcance test attributes statistically signiﬁcant dif - ferences solely between the 5 - point tasks with and without normalization . These scores indicate that the overall agreement between human ratings is roughly the same , regardless of whether partici - pants are being asked to provide cardinal or ordi - nal ratings . Improvement in inter - rater reliability via participant - level normalization suggests that participants may indeed have individual biases to - ward certain regions of the 5 - point scale , which the normalization process corrects . In terms of intra - rater reliability , a better mean was observed among participants in the pair - wise task ( α = 0 . 5085 ) versus the 5 - point task ( α = 0 . 4014 ) . This suggests that , on average , hu - man raters provide more consistent ratings with themselves in comparing between two translations versus rating single translations in isolation . This may be attributed to the fact that seeing multi - ple translations provides raters with more cues with which to make consistent judgments . How - ever , at the current sample size , a Welch two - sample t - test ( Larsen and Marx , 2012 ) between 5 - point and pairwise intra - rater reliabilities shows no signiﬁcant difference between the two tasks ( t ( 26 . 92 ) = 1 . 4362 , p = 0 . 1625 ) . Thus , it re - mains difﬁcult to infer whether one task is deﬁni - tively superior to the other in eliciting more con - sistent responses . Intra - rater reliability is the same for the 5 - point task with and without normaliza - tion , as participants are still compared against themselves . Figure 3 : Improvements in inter - rater reliability using intra - rater consistency ﬁlter . Figure 4 : Improvements in inter - rater reliability using item variance ﬁlter . 4 . 2 Rater and Item Variance The succeeding analysis is based on two assump - tions : ﬁrst , that human raters vary in that they do not provide equally good judgments of translation quality , and second , rating items vary in that some translations may be more difﬁcult to judge than others . This allows to investigate the inﬂuence of rater variance and item variance on inter - rater re - liability by an ablation analysis where low - quality judges and difﬁcult translations are ﬁltered out . Using intra - rater reliability as an index of how well human raters judge translation quality , Fig - ure 3 shows a ﬁltering process whereby human raters with α scores lower than a moving thresh - old are dropped from the analysis . As the relia - bility threshold is increased from 0 to 1 , overall inter - rater reliability is measured . Figure 4 shows a similar ﬁltering process implemented using vari - ance in translation scores . Item variances are nor - malized on a scale from 0 to 1 and subtracted from 1 to produce an item variance threshold . As the threshold increases , overall inter - rater reliability is likewise measured as high - variance items are pro - gressively dropped from the analysis . As the plots demonstrate , inter - rater reliability generally increases with consistency and variance ﬁltering . For consistency ﬁltering , Figure 3 shows how the inter - rater reliability of the 5 - point task experiences greater increases than the pairwise task with lower ﬁltering thresholds , especially in the normalized case . This may be attributed to the fact that more participants in the 5 - point task had low intra - rater reliability . Pairwise tasks , on the other hand , require higher thresholds before large gains are observed in overall inter - rater reliabil - ity . This is because more participants in the pair - wise task had relatively high intra - rater reliability . In the normalized 5 - point task , selecting a thresh - old of 0 . 49 as a cutoff for intra - rater reliability re - tains 8 participants with an inter - rater reliability of 0 . 5059 . For the pairwise task , a threshold of 0 . 66 leaves 5 participants with an inter - rater reliability of 0 . 3912 . The opposite phenomenon is observed in the case of variance ﬁltering . As seen in Figure 4 , the overall inter - rater reliability of the pairwise task quickly overtakes that of the 5 - point task , with and without normalization . This may be at - tributed to how , in the pairwise setup , more items can be a source of disagreement among human judges . Ambiguous cases , that will be discussed in § 4 . 3 , may result in higher item variance . This problem is not as pronounced in the 5 - point task , where judges must simply rate individual transla - tions . It may be surmised that this item variance accounts for why , on average , judges in the pair - wise task demonstrate higher intra - rater reliabil - ity than those in the 5 - point task , yet the overall inter - rater reliability of the pairwise task is lower . By selecting a variance threshold such that at least 70 % of items are retained in the analysis , the im - proved inter - rater reliabilities were 0 . 3236 for the 5 - point task and 0 . 3519 for the pairwise task . 4 . 3 Qualitative Analysis On completion of the rating task , we asked the par - ticipants for a subjective judgment of difﬁculty on a scale from 1 ( very difﬁcult ) to 10 ( very easy ) . On average , the pairwise rating task ( mean 5 . 69 ) was perceived slightly easier than the 5 - point rating task ( mean 4 . 8 ) . They also had to state which as - pects of the tasks they found difﬁcult : The biggest challenge for 5 - point ratings seemed to be the weighing of different error types and the rating of long sentences with very few , but essential errors . For pairwise ratings , difﬁculties lie in distinguish - ing between similar , or similarly bad translations . Both tasks showed difﬁculties with ungrammatical or incomprehensible sources . Comparing items with high and low agreement across raters allows conclusions about objective difﬁculty . We assume that high inter - rater agree - ment indicates an ease of judgment , while difﬁ - culties in judgment are manifested in low agree - ment . A list of examples is given in Appendix A . 2 . For 5 - point ratings , difﬁculties arise with ungram - matical sources and omissions , whereas obvious mistakes in the target , such as over - literal trans - lations , make judgment easier . Preference judg - ments tend to be harder when both translations contain errors and are similar . When there is a tie , the pairwise rating framework does not allow to indicate whether both translations are of high or low quality . Since there is no normalization strat - egy for pairwise ratings , individual biases or rating schemes can hence have a larger negative impact on the inter - rater agreement . 5 Learnability of a Reward Estimator from MT Ratings 5 . 1 Learning a Reward Estimator The numbers of ratings that can be obtained di - rectly from human raters in a reasonable amount of time is tiny compared to the millions of sen - tences used for standard NMT training . By learn - ing a reward estimator on the collection of human ratings , we seek to generalize to unseen transla - tions . The model for this reward estimator should ideally work without time - consuming feature ex - traction so it can be deployed in direct interaction with a learning NMT system , estimating rewards on the ﬂy , and most importantly generalize well so it can guide the NMT towards good local optima . Learning from Cardinal Feedback . The inputs to the reward estimation model are sources x and their translations y . Given cardinal judgments for these inputs , a regression model with parameters ψ is trained to minimize the mean squared error ( MSE ) for a set of n predicted rewards ˆ r and judg - ments r : L MSE ( ψ ) = 1 n n (cid:88) i = 1 ( r ( y i ) − ˆ r ψ ( y i ) ) 2 . In simulation experiments , where all translations can be compared to existing references , r may be computed by sentence - BLEU ( sBLEU ) . For our human 5 - point judgments , we ﬁrst normalize the judgments per rater as described in § 4 , then aver - age the judgments across raters and ﬁnally scale them linearly to the interval [ 0 . 0 , 1 . 0 ] . Learning from Pairwise Preference Feedback . When pairwise preferences are given instead of cardinal judgments , the Bradley - Terry model al - lows us to train an estimator of r . Following Chris - tiano et al . ( 2017 ) , let ˆ P ψ [ y 1 (cid:31) y 2 ] be the proba - bility that any translation y 1 is preferred over any other translation y 2 by the reward estimator : ˆ P ψ [ y 1 (cid:31) y 2 ] = exp ˆ r ψ ( y 1 ) exp ˆ r ψ ( y 1 ) + exp ˆ r ψ ( y 2 ) . Let Q [ y 1 (cid:31) y 2 ] be the probability that translation y 1 is preferred over translation y 2 by a gold stan - dard , e . g . the human raters or in comparison to a reference translation . With this supervision signal we formulate a pairwise ( PW ) training loss for the reward estimation model with parameters ψ : L PW ( ψ ) = − 1 n n (cid:88) i = 1 Q [ y 1i (cid:31) y 2i ] log ˆ P ψ [ y 1i (cid:31) y 2i ] + Q [ y 2i (cid:31) y 1i ] log ˆ P ψ [ y 2i (cid:31) y 1i ] . For simulation experiments — where we lack a genuine supervision for preferences — we com - pute Q comparing the sBLEU scores for both translations , i . e . translation preferences are mod - eled according to their difference in sBLEU : Q [ y 1 (cid:31) y 2 ] = exp sBLEU ( y 1 ) exp sBLEU ( y 1 ) + exp sBLEU ( y 2 ) . When obtaining preference jugdments directly from raters , Q [ y 1 (cid:31) y 2 ] is simply the relative fre - quency of y 1 being preferred over y 2 by a rater . 5 . 2 Experiments Data . The 1 , 000 ratings collected as described in § 3 are leveraged to train regression models and pairwise preference models . In addition , we train models on simulated rewards ( sBLEU ) for a com - parison with arguably “clean” feedback for the Model Feedback ρ MSE Simulated - 0 . 2571 PW Simulated - 0 . 1307 MSE Human - 0 . 2193 PW Human - 0 . 1310 MSE Human ﬁlt . - 0 . 2341 PW Human ﬁlt . - 0 . 1255 Table 2 : Spearman’s rank correlation ρ between estimated rewards and TER for models trained with simulated rewards and human rewards ( also ﬁltered subsets ) . same set of translations . In order to augment this very small collection of ratings , we leverage the available out - of - domain bitext as auxiliary train - ing data . We sample translations for a subset of the out - of - domain sources and store sBLEU scores as rewards , collecting 90k out - of - domain training samples in total ( see Appendix B . 1 for details ) . During training , each mini - batch is sam - pled from the auxiliary data with probability p aux , from the original training data with probability 1 − p aux . Adding this auxiliary data as a regu - larization through multi - task learning prevents the model from overﬁtting to the small set of human ratings . In the experiments p aux was tuned to 0 . 8 . Architecture . We choose the following neural architecture for the reward estimation ( details in Appendix B . 2 ) : Inputs are padded source and target subword embeddings , which are each pro - cessed with a biLSTM ( Hochreiter and Schmid - huber , 1997 ) . Their outputs are concatenated for each time step , further fed to a 1D - convolution with max - over - time pooling and subsequently a leaky ReLU ( Maas et al . , 2013 ) output layer . This architecture can be seen as a biLSTM - enhanced bilingual extension to the convolutional model for sentence classiﬁcation proposed by Kim ( 2014 ) . It has the advantage of not requiring any feature extraction but still models n - gram features on an abstract level . Evaluation Method . The quality of the reward estimation models is tested by measuring Spear - man’s ρ with TER on a held - out test set of 1 , 314 translations following the standard in sQE eval - uations . Hyperparameters are tuned on another 1 , 200 TED translations . Results . Table 2 reports the results of reward es - timators trained on simulated and human rewards . When trained from cardinal rewards , the model of simulated scores performs slightly better than the model of human ratings . This advantage is lost when moving to preference judgments , which might be explained by the fact that the softmax over sBLEUs with respect to a single reference is just not as expressive as the preference proba - bilities obtained from several raters . Filtering by participants ( retaining 8 participants for cardinal rewards and 5 for preference jugdments , see Sec - tion 4 ) improves the correlation further for cardi - nal rewards , but slightly hurts for preference judg - ments . The overall correlation scores are relatively low — especially for the PW models — which we suspect is due to overﬁtting to the small set of training data . From these experiments we con - clude that when it comes to estimating translation quality , cardinal human jugdments are more useful than pairwise preference jugdments . 6 Reinforcement Learning from Direct and Estimated Rewards in MT 6 . 1 NMT Objectives Supervised Learning . Most commonly , NMT models are trained with Maximum Likelihood Es - timation ( MLE ) on a parallel corpus of source and target sequences D = { ( x ( s ) , y ( s ) ) } Ss = 1 : L MLE ( θ ) = S (cid:88) s = 1 log p θ ( y ( s ) | x ( s ) ) . The MLE objective requires reference translations and is agnostic to rewards . In the experiments it is used to train the out - of - domain baseline model as a warm start for reinforcement learning from in - domain rewards . Reinforcement Learning from Estimated or Simulated Direct Rewards . Deploying NMT in a reinforcement learning scenario , the goal is to maximize the expectation of a reward r over all source and target sequences ( Wu et al . , 2016 ) , leading to the following REINFORCE ( Williams , 1992 ) objective : R RL ( θ ) = E p ( x ) p θ ( y | x ) [ r ( y ) ] ( 1 ) ≈ S (cid:88) s = 1 k (cid:88) i = 1 p τ θ ( ˜y ( s ) i | x ( s ) ) r ( ˜y i ) ( 2 ) The reward r can either come from a reward esti - mation model ( estimated reward ) or be computed with respect to a reference in a simulation setting ( simulated direct reward ) . In order to counteract high variance in the gradient updates , the running average of rewards is subtracted from r for learn - ing . In practice , Equation 1 is approximated with k samples from p θ ( y | x ) ( see Equation 2 ) . When k = 1 , this is equivalent to the expected loss minimization in Sokolov et al . ( 2016a , b ) ; Kreutzer et al . ( 2017 ) , where the system interactively learns from online bandit feedback . For k > 1 this is similar to the minimum - risk training for NMT pro - posed in Shen et al . ( 2016 ) . Adding a tempera - ture hyper - parameter τ ∈ ( 0 . 0 , ∞ ] to the softmax over the model output o allows us to control the sharpness of the sampling distribution p τ θ ( y | x ) = softmax ( o / τ ) , i . e . the amount of exploration dur - ing training . With temperature τ < 1 , the model’s entropy decreases and samples closer to the one - best output are drawn . We seek to keep the explo - ration low to prevent the NMT to produce samples that lie far outside the training domain of the re - ward estimator . Off - Policy Learning from Direct Rewards . When rewards can not be obtained for samples from a learning system , but were collected for a static deterministic system ( e . g . in a production environment ) , we are in an off - policy learning sce - nario . The challenge is to improve the MT sys - tem from a log L = { ( x ( h ) , y ( h ) , r ( y ( h ) ) ) } Hh = 1 of rewarded translations . Following Lawrence et al . ( 2017 ) we deﬁne the following off - policy learning ( OPL ) objective to learn from logged rewards : R OPL ( θ ) = 1 H H (cid:88) h = 1 r ( y ( h ) ) ¯ p θ ( y ( h ) | x ( h ) ) , with reweighting over the current mini - batch B : ¯ p θ ( y ( h ) | x ( h ) ) = p θ ( y ( h ) | x ( h ) ) (cid:80) Bb = 1 p θ ( y ( b ) | x ( b ) ) . 2 In contrast to the RL objective , only logged translations are re - inforced , i . e . there is no exploration in learning . 6 . 2 Experiments Data . We use the WMT 2017 data 3 for training a general domain ( here : out - of - domain ) model for 2 Lawrence et al . ( 2017 ) propose reweighting over the whole log , but this is infeasible for NMT . Here B (cid:28) H . 3 Pre - processed data available at http : / / www . statmt . org / wmt17 / translation - task . html . WMT TED Model BLEU METEOR BEER BLEU METEOR BEER WMT 27 . 2 31 . 8 60 . 08 27 . 0 30 . 7 59 . 48 TED 26 . 3 31 . 3 59 . 49 34 . 3 34 . 6 64 . 94 Table 3 : Results on test data for in - and out - of - domain fully - supervised models . Both are trained with MLE , the TED model is obtained by ﬁne - tuning the WMT model on TED data . translations from German to English . The train - ing data contains 5 . 9M sentence pairs , the devel - opment data 2 , 999 sentences ( WMT 2016 test set ) and the test data 3 , 004 sentences . For in - domain data , we choose the translations of TED talks 4 as used in IWSLT evaluation campaigns . The training data contains 153k , the development data 6 , 969 , and the test data 6 , 750 parallel sentences . Architecture . Our NMT model is a standard subword - based encoder - decoder architecture with attention ( Bahdanau et al . , 2015 ) . An encoder Re - current Neural Network ( RNN ) reads in the source sentence and a decoder RNN generates the tar - get sentence conditioned on the encoded source . We implemented RL and OPL objectives in Neu - ral Monkey ( Helcl and Libovick´y , 2017 ) . 5 The NMT has a bidirectional encoder and a single - layer decoder with 1 , 024 GRUs each , and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte - pair merges ( Sennrich et al . , 2016 ) . For model selection we use greedy decoding , for test set evaluation beam search with a beam of width 10 . We sample k = 5 translations for RL models and set the softmax temperature τ = 0 . 5 . Appendix C . 1 reports re - maining hyperparameters . Evaluation Method . Trained models are eval - uated with respect to BLEU ( Papineni et al . , 2002 ) , METEOR ( Denkowski and Lavie , 2011 ) using MULTEVAL ( Clark et al . , 2011 ) and BEER ( Stanojevi´c and Sima’an , 2014 ) to cover a diverse set of automatic measures for translation quality . 6 We test for statistical signiﬁcance with approxi - mate randomization ( Noreen , 1989 ) . 4 Pre - processing and data splits as described in https : / / github . com / rizar / actor - critic - public / tree / master / exp / ted . 5 The code is available in the Neural Monkey fork https : / / github . com / juliakreutzer / bandit - neuralmonkey / tree / acl2018 . 6 Since tendencies of improvement turn out to be consis - tent across metrics , we only discuss BLEU in the text . Model Rewards BLEU METEOR BEER Baseline - - 27 . 0 30 . 7 59 . 48 RL D S 32 . 5 (cid:63) ± 0 . 01 33 . 7 (cid:63) ± 0 . 01 63 . 47 (cid:63) ± 0 . 10 OPL D S 27 . 5 (cid:63) 30 . 9 (cid:63) 59 . 62 (cid:63) RL + MSE E S 28 . 2 (cid:63) ± 0 . 09 31 . 6 (cid:63) ± 0 . 04 60 . 23 (cid:63) ± 0 . 14 RL + PW E S 27 . 8 (cid:63) ± 0 . 01 31 . 2 (cid:63) ± 0 . 01 59 . 83 (cid:63) ± 0 . 04 OPL D H 27 . 5 (cid:63) 30 . 9 (cid:63) 59 . 72 (cid:63) RL + MSE E H 28 . 1 (cid:63) ± 0 . 01 31 . 5 (cid:63) ± 0 . 01 60 . 21 (cid:63) ± 0 . 12 RL + PW E H 27 . 8 (cid:63) ± 0 . 09 31 . 3 (cid:63) ± 0 . 09 59 . 88 (cid:63) ± 0 . 23 RL + MSE E F 28 . 1 (cid:63) ± 0 . 20 31 . 6 (cid:63) ± 0 . 10 60 . 29 (cid:63) ± 0 . 13 Table 4 : Results on TED test data for training with estimated ( E ) and direct ( D ) rewards from simula - tion ( S ) , humans ( H ) and ﬁltered ( F ) human rat - ings . Signiﬁcant ( p ≤ 0 . 05 ) differences to the baseline are marked with (cid:63) . For RL experiments we show three runs with different random seeds , mean and standard deviation in subscript . The out - of - domain model is trained with MLE on WMT . The task is now to improve the gener - alization of this model to the TED domain . Ta - ble 3 compares the out - of - domain baseline with domain - adapted models that were further trained on TED in a fully - supervised manner ( super - vised ﬁne - tuning as introduced by Freitag and Al - Onaizan ( 2016 ) ; Luong and Manning ( 2015 ) ) . The supervised domain - adapted model serves as an up - per bound for domain adaptation with human re - wards : if we had references , we could improve up to 7 BLEU . What if references are not available , but we can obtain rewards for sample translations ? Results for RL from Simulated Rewards . First we simulate “clean” and deterministic rewards by comparing sample translations to references using GLEU ( Wu et al . , 2016 ) for RL , and smoothed sBLEU for estimated rewards and OPL . Table 4 lists the results for this simulation experiment in rows 2 - 5 ( S ) . If unlimited clean feedback was given ( RL with direct simulated rewards ) , im - provements of over 5 BLEU can be achieved . When limiting the amount of feedback to a log of 800 translations , the improvements over the base - line are only marginal ( OPL ) . When replacing the direct reward by the simulated reward estimators from § 5 , i . e . having unlimited amounts of approx - imately clean rewards , however , improvements of 1 . 2 BLEU for MSE estimators ( RL + MSE ) and 0 . 8 BLEU for pairwise estimators ( RL + PW ) are found . This suggests that the reward estimation model helps to tackle the challenge of generaliza - tion over a small set of ratings . Results for RL from Human Rewards . Know - ing what to expect in an ideal setting with non - noisy feedback , we now move to the experiments with human feedback . OPL is trained with the logged normalized , averaged and re - scaled human reward ( see § 5 ) . RL is trained with the direct re - ward provided by the reward estimators trained on human rewards from § 5 . Table 4 shows the re - sults for training with human rewards in rows 6 - 8 : The improvements for OPL are very similar to OPL with simulated rewards , both suffering from overﬁtting . For RL we observe that the MSE - based reward estimator ( RL + MSE ) leads to sig - niﬁcantly higher improvements as a the pairwise reward estimator ( RL + PW ) — the same trend as for simulated ratings . Finally , the improvement of 1 . 1 BLEU over the baseline showcases that we are able to improve NMT with only a small num - ber of human rewards . Learning from estimated ﬁltered 5 - point ratings , does not signiﬁcantly im - prove over these results , since the improvement of the reward estimator is only marginal ( see § 5 ) . 7 Conclusion In this work , we sought to ﬁnd answers to the questions of how cardinal and ordinal feedback differ in terms of reliability , learnability and ef - fectiveness for RL training of NMT , with the goal of improving NMT with human bandit feedback . Our rating study , comparing 5 - point and prefer - ence ratings , showed that their reliability is com - parable , whilst cardinal ratings are easier to learn and to generalize from , and also more suitable for RL in our experiments . Our work reports improvements of NMT lever - aging actual human bandit feedback for RL , leav - ing the safe harbor of simulations . Our experi - ments show that improvements of over 1 BLEU are achievable by learning from a dataset that is tiny in machine translation proportions . Since this type of feedback , in contrast to post - edits and references , is fast and cheap to elicit from non - professionals , our results bear a great potential for future applications on larger scale . Acknowledgments . This work was supported in part by DFG Research Grant RI 2221 / 4 - 1 , and by an internship program of the IWR at Heidelberg University . References Dzmitry Bahdanau , Philemon Brakel , Kelvin Xu , Anirudh Goyal , Ryan Lowe , Joelle Pineau , Aaron Courville , and Yoshua Bengio . 2017 . An actor - critic algorithm for sequence prediction . In Proceedings of the International Conference on Learning Repre - sentations ( ICLR ) . Toulon , France . Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Ben - gio . 2015 . Neural machine translation by jointly learning to align and translate . In Proceedings of the International Conference on Learning Represen - tations ( ICLR ) . San Diego , CA , USA . L´eon Bottou , Jonas Peters , Joaquin Qui˜nonero - Candela , Denis X . Charles , D . Max Chickering , Elon Portugaly , Dipanakar Ray , Patrice Simard , and Ed Snelson . 2013 . Counterfactual reasoning and learning systems : The example of computational ad - vertising . Journal of Machine Learning Research 14 : 3207 – 3260 . Ralph Allan Bradley and Milton E . Terry . 1952 . Rank analysis of incomplete block designs : I . the method of paired comparisons . Biometrika 39 ( 3 - 4 ) : 324 – 345 . Paul F . Christiano , Jan Leike , Tom Brown , Miljan Mar - tic , Shane Legg , and Dario Amodei . 2017 . Deep reinforcement learning from human preferences . In Advances in Neural Information Processing Systems ( NIPS ) . Long Beach , CA , USA . Jonathan H . Clark , Chris Dyer , Alon Lavie , and Noah A . Smith . 2011 . Better hypothesis testing for statistical machine translation : Controlling for op - timizer instability . In Proceedings of the 49th An - nual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) . Portland , OR , USA . Michael Denkowski and Alon Lavie . 2011 . Meteor 1 . 3 : Automatic metric for reliable optimization and evaluation of machine translation systems . In Pro - ceedings of the Sixth Workshop on Statistical Ma - chine Translation ( WMT ) . Edinburgh , Scotland . Markus Freitag and Yaser Al - Onaizan . 2016 . Fast domain adaptation for neural machine translation . CoRR abs / 1612 . 06897 . Spence Green , Sida I . Wang , Jason Chuang , Jeffrey Heer , Sebastian Schuster , and Christopher D . Man - ning . 2014 . Human effort and machine learnabil - ity in computer aided translation . In Proceedings the onference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) . Doha , Qatar . Jindˇrich Helcl and Jindˇrich Libovick´y . 2017 . Neural Monkey : An Open - source Tool for Sequence Learn - ing . The Prague Bulletin of Mathematical Linguis - tics ( 107 ) : 5 – 17 . Sepp Hochreiter and J¨urgen Schmidhuber . 1997 . Long short - term memory . Neural Computation 9 ( 8 ) : 1735 – 1780 . Nan Jiang and Lihong Li . 2016 . Doubly robust off - policy value evaluation for reinforcement learning . In Proceedings of the 33rd International Conference on Machine Learning ( ICML ) . New York , NY , USA . Kshitij Judah , Saikat Roy , Alan Fern , and Thomas G . Dietterich . 2010 . Reinforcement learning via prac - tice and critique advice . In Proceedings of the 24th AAAI Conference on Artiﬁcial Intelligence . Atlanta , GA , USA . Sham Kakade . 2001 . A natural policy gradient . In Advances in Neural Information Processing Systems ( NIPS ) . Vancouver , Canada . Hyun Kim , Jong - Hyeok Lee , and Seung - Hoon Na . 2017 . Predictor - estimator using multilevel task learning with stack propagation for neural quality estimation . In Proceedings of the Second Confer - ence on Machine Translation ( WMT ) . Copenhagen , Denmark . Yoon Kim . 2014 . Convolutional neural networks for sentence classiﬁcation . In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) . Doha , Qatar . Diederik Kingma and Jimmy Ba . 2014 . Adam : A method for stochastic optimization . CoRR abs / 1412 . 6980 . W . Bradley Knox and Peter Stone . 2009 . Interac - tively shaping agents via human reinforcement : The TAMER framework . In Proceedings of the Interna - tional Conference on Knowledge Capture ( K - CAP ) . Redondo Beach , CA , USA . Vijay R . Konda and John N . Tsitsiklis . 2000 . Actor - critic algorithms . In Advances in Neural In - formation Processing Systems ( NIPS ) . Vancouver , Canada . Julia Kreutzer , Shahram Khadivi , Evgeny Matusov , and Stefan Riezler . 2018 . Can neural machine trans - lation be improved with user feedback ? In Pro - ceedings of the 16th Annual Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies - Industry Track ( NAACL - HLT ) . New Orleans , LA , USA . Julia Kreutzer , Artem Sokolov , and Stefan Riezler . 2017 . Bandit structured prediction for neural sequence - to - sequence learning . In Proceedings of the 55th Annual Meeting of the Association for Com - putational Linguistics ( ACL ) . Vancouver , Canada . Klaus Krippendorff . 2013 . Content Analysis . An Intro - duction to Its Methodology . Sage , third edition . Richard Larsen and Morris Marx . 2012 . An Introduc - tion to Mathematical Statistics and Its Applications . Prentice Hall , ﬁfth edition . Carolin Lawrence , Artem Sokolov , and Stefan Riezler . 2017 . Counterfactual learning from bandit feedback under deterministic logging : A case study in statisti - cal machine translation . In Proceedings of the Con - ference on Empirical Methods in Natural Language Processing ( EMNLP ) . Copenhagen , Denmark . Chen Liang , Jonathan Berant , Quoc Le , Kenneth D . Forbus , and Ni Lao . 2017 . Neural symbolic ma - chines : Learning semantic parsers on freebase with weak supervision . In Proceedings of the 55th An - nual Meeting of the Association for Computational Linguistics ( ACL ) . Vancouver , Canada . Minh - Thang Luong and Christopher D . Manning . 2015 . Stanford neural machine translation systems for spoken language domains . In Proceedings of the International Workshop on Spoken Language Trans - lation ( IWSLT ) . Da Nang , Vietnam . Andrew L Maas , Awni Y Hannun , and Andrew Y Ng . 2013 . Rectiﬁer nonlinearities improve neural net - work acoustic models . In ICML Workshop on Deep Learning for Audio , Speech and Language Process - ing . Atlanta , GA , USA . James MacGlashan , Mark K . Ho , Robert Loftin , Bei Peng , Guan Wang , David L . Roberts , Matthew E . Taylor , and Michael L . Littman . 2017 . Interactive learning from policy - dependent human feedback . In Proceedings of the 34th International Conference on Machine Learning ( ICML ) . Sydney , Australia . Andr ´ e Martins , Marcin Junczys - Dowmunt , Fabio Ke - pler , Ram ´ on Astudillo , Chris Hokamp , and Roman Grundkiewicz . 2017 . Pushing the limits of transla - tion quality estimation . Transactions of the Associ - ation for Computational Linguistics ( TACL ) 5 : 205 – 218 . Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean . 2013 . Efﬁcient estimation of word represen - tations in vector space . CoRR abs / 1301 . 3781 . Volodymyr Mnih , Adri ` a Puigdom ` enech Badia , Mehdi Mirza , Alex Graves , Timothy P . Lillicrap , Tim Harley , David Silver , and Koray Kavukcuoglu . 2016 . Asynchronous methods for deep reinforce - ment learning . In Proceedings of the 33rd Inter - national Conference on Machine Learning ( ICML ) . New York , NY , USA . Volodymyr Mnih , Koray Kavukcuoglu , David Silver , Andrei A . Rusu , Joel Veness , Marc G . Bellemare , Alex Graves , Martin Riedmiller , Andreas K . Fidje - land , Georg Ostrovski , Stig Petersen , Charles Beat - tie , Amir Sadik , Ioannis Antonoglou , Helen King , Dharshan Kumaran , Daan Wierstra , Shane Legg , and Demis Hassabis . 2015 . Human - level con - trol through deep reinforcement learning . Nature 518 : 529 – 533 . Khanh Nguyen , Hal Daum´e , and Jordan Boyd - Graber . 2017 . Reinforcement learning for bandit neural ma - chine translation with simulated feedback . In Pro - ceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) . Copen - hagen , Denmark . Eric W . Noreen . 1989 . Computer Intensive Methods for Testing Hypotheses . An Introduction . Wiley . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic eval - uation of machine translation . In Proceedings of the 40th Annual Meeting of the Association for Compu - tational Linguistics ( ACL ) . Philadelphia , PA , USA . Razvan Pascanu , Tomas Mikolov , and Yoshua Bengio . 2013 . On the difﬁculty of training recurrent neural networks . In Proceedings of the 30th International Conference on Machine Learning ( ICML ) . Atlanta , GA , USA . Romain Paulus , Caiming Xiong , and Richard Socher . 2017 . A deep reinforced model for abstractive sum - marization . CoRR abs / 1705 . 04304 . Patrick M . Pilarski , Michael R . Dawson , Thomas De - gris , Farbod Fahimi , Jason P . Carey , and Richard S . Sutton . 2011 . Online human training of a myoelec - tric prosthesis controller via actor - critic reinforce - ment learning . In Proceedings of the IEEE In - ternational Conference on Rehabilitation Robotics . Z ¨ urich , Switzerland . Maja Popovi ´ c . 2015 . chrf : character n - gram f - score for automatic mt evaluation . In Proceedings of the Tenth Workshop on Statistical Machine Translation ( WMT ) . Lisbon , Portugal . Doina Precup , Richard S . Sutton , and Sanjoy Dasgupta . 2001 . Off - policy temporal - difference learning with function approximation . In Proceedings of the Eigh - teenth International Conference on Machine Learn - ing ( ICML ) . Williams College , MA , USA . Doina Precup , Richard S . Sutton , and Satinder P . Singh . 2000 . Eligibility traces for off - policy policy eval - uation . In Proceedings of the Seventeenth Inter - national Conference on Machine Learning ( ICML ) . San Francisco , CA , USA . John Schulman , Sergey Levine , Philipp Moritz , Michael I . Jordan , and Pieter Abbeel . 2015 . Trust region policy optimization . In Proceedings of the 31st International Conferene on Machine Learning ( ICML ) . Lille , France . Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 . Neural machine translation of rare words with subword units . In Proceedings of the 54th Annual Meeting of the Association for Computational Lin - guistics ( ACL ) . Berlin , Germany . Shai Shalev - Shwartz , Ohad Shamir , Nathan Srebro , and Karthik Sridharan . 2010 . Learnability , stabil - ity and uniform convergence . Journal of Machine Learning Research 11 : 2635 – 2670 . Shiqi Shen , Yong Cheng , Zongjun He , Wei He , Hua Wu , Maosong Sun , and Yang Liu . 2016 . Minimum risk training for neural machine translation . In Pro - ceedings of the 54th Annual Meeting of the Associ - ation for Computational Linguistics ( ACL ) . Berlin , Germany . David Silver , Aja Huang , Chris J . Maddison , Arthur Guez , Laurent Sifre , George van den Driessche , Ju - lian Schrittwieser , Ioannis Antonoglou , Veda Pan - neershelvam , Marc Lanctot , Sander Dieleman , Do - minik Grewe , John Nham , Nal Kalchbrenner , Ilya Sutskever , Timothy Lillicrap , Madeleine Leach , Ko - ray Kavukcuoglu , Thore Graepel , and Demis Has - sabis . 2016 . Mastering the game of go with deep neural networks and tree search . Nature 529 : 484 – 489 . Artem Sokolov , Julia Kreutzer , Christopher Lo , and Stefan Riezler . 2016a . Learning structured predic - tors from bandit feedback for interactive NLP . In Proceedings of the 54th Annual Meeting of the Asso - ciation for Computational Linguistics ( ACL ) . Berlin , Germany . Artem Sokolov , Julia Kreutzer , Christopher Lo , and Stefan Riezler . 2016b . Stochastic structured predic - tion under bandit feedback . In Advances in Neural Information Processing Systems ( NIPS ) . Barcelona , Spain . Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov . 2014 . Dropout : A simple way to prevent neural networks from overﬁtting . Journal of Machine Learning Re - search 15 : 1929 – 1958 . Milo ˇ s Stanojevi ´ c and Khalil Sima’an . 2014 . Fit - ting sentence level translation evaluation with many dense features . In Proceedings of the 2014 Con - ference on Empirical Methods in Natural Language Processing ( EMNLP ) . Doha , Qatar . Richard S . Sutton , David McAllester , Satinder Singh , and Yishay Mansour . 2000 . Policy gradient methods for reinforcement learning with function approxima - tion . In Advances in Neural Information Process - ings Systems ( NIPS ) . Vancouver , Canada . Adith Swaminathan and Thorsten Joachims . 2015a . Counterfactual risk minimization : Learning from logged bandit feedback . In International Confer - ence on Machine Learning ( ICML ) . Lille , France . Adith Swaminathan and Thorsten Joachims . 2015b . The self - normalized estimator for counterfactual learning . In Advances in Neural Information Pro - cessing Systems ( NIPS ) . Montreal , Canada . Philip S . Thomas and Emma Brunskill . 2016 . Data - efﬁcient off - policy policy evaluation for reinforce - ment learning . In Proceedings of the 33nd Inter - national Conference on Machine Learning ( ICML ) . New York , NY , USA . Louis Leon Thurstone . 1927 . A law of comparative judgement . Psychological Review 34 : 278 – 286 . Ronald J . Williams . 1992 . Simple statistical gradient - following algorithms for connectionist reinforce - ment learning . Machine Learning 8 : 229 – 256 . Lijun Wu , Yingce Xia , Li Zhao , Fei Tian , Tao Qin , Jianhuang Lai , and Tie - Yan Liu . 2017 . Adversarial neural machine translation . CoRR abs / 1704 . 06933 . Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V . Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , Jeff Klingner , Apurva Shah , Melvin Johnson , Xiaobing Liu , Lukasz Kaiser , Stephan Gouws , Yoshikiyo Kato , Taku Kudo , Hideto Kazawa , Keith Stevens , George Kurian , Nishant Patil , Wei Wang , Cliff Young , Jason Smith , Jason Riesa , Alex Rudnick , Oriol Vinyals , Greg Corrado , Macduff Hughes , and Jeffrey Dean . 2016 . Google’s neural machine translation system : Bridging the gap between human and machine translation . CoRR abs / 1609 . 08144 . Lantao Yu , Weinan Zhang , Jun Wang , and Yong Yu . 2017 . Seqgan : Sequence generative adversarial nets with policy gradient . In Proceedings of the Thirty - First AAAI Conference on Artiﬁcial Intelli - gence ( AAAI ) . San Francisco , CA , USA . Appendix A Rating Task A . 1 Rating Instructions Participants for the 5 - star rating task were given the following instructions : “You will be presented with a German statement and a translation of this statement in English . You must assign a rating from 1 ( Very Bad ) to 5 ( Very Good ) to each trans - lation . ” Participants for the pairwise task were given the following instructions : “You will be presented with a German statement and two translations of this statement in English . You must decide which of the two translations you prefer , or whether you have no preference . ” A . 2 Example Ratings Table 5 lists low - and high - variance items for 5 - star ratings , Table 6 for pairwise ratings . From the annotations in the tables , the reader may get an impression which translations are “easier” to judge than others . B Reward Estimation B . 1 Auxiliary Data for Reward Estimation In order to augment the small collection of 1 , 000 rated translations , we leverage the available out - of - domain bitext as auxiliary training data : 10k source sentences of WMT ( out - of - domain ) are translated by the out - of - domain model . Transla - tions from 9 beam search ranks are compared to their references to compute sBLEU rewards . This auxiliary data hence provides 90k out - of - domain training samples with sBLEU reward . For pair - wise rewards , sBLEU scores for two translations for the same source are compared . Each mini - batch during training is sampled from the auxiliary data with probability p aux , from the original train - ing data with probability 1 − p aux . Adding this auxiliary data as a regularization through multi - task learning prevents the model from overﬁtting to the small set of human ratings . In our experi - ments , p aux = 0 . 8 worked best . B . 2 Reward Estimation Architecture Input source and target sequence are split into the BPE subwords used for NMT training , padded up to a maximum length of 100 tokens , and repre - sented as 500 - dimensional subword embeddings . Subword embeddings are pre - trained on the WMT Source BiLSTM Target BiLSTM s 0 s 1 s 2 s 3 s 4 s 5 s 6 PAD t 0 t 1 t 2 t 3 t 4 t 5 PAD PAD 1D Convolution Max over time Fully connected output layer Figure 5 : Reward estimation architecture : Source and target biLSTM outputs over subword embed - dings are concatenated for each position , followed by a convolutional layer with several ﬁlters ( here one each for sizes 1 to 3 ) , a max - over - time pool - ing and a fully connected output layer with a single leaky ReLU output unit . bitext with word2vec ( Mikolov et al . , 2013 ) , normalized to unit length and held constant dur - ing further training . Additional 10 - dimensional BPE - feature embeddings are appended to the sub - word embeddings , where a binary indicator en - codes whether each subword contains the subword preﬁx marker ” @ @ ” . BPE - preﬁx features are use - ful information for the model since bad transla - tions can arise from “illegal” compositions of sub - word tokens . The embeddings are then fed to a ssource - side and a target - side bidirectional LSTM ( biLSTM ) ( Hochreiter and Schmidhuber , 1997 ) , respectively . The biLSTM outputs are concate - nated for each time step and fed to a 1 - D convolu - tional layer with 50 ﬁlters each for ﬁlter sizes from 2 to 15 . The convolution is followed by max - over - time pooling , producing 700 input features for a fully - connected output layer with leaky ReLU ( Maas et al . , 2013 ) activation function . Dropout ( Srivastava et al . , 2014 ) with p = 0 . 5 is applied before the ﬁnal layer . This architecture , depicted in Figure 5 , can be seen as a biLSTM - enhanced bilingual extension to the convolutional model for sentence classiﬁcation proposed by Kim ( 2014 ) . C NMT C . 1 NMT Hyperparameters The NMT has a bidirectional encoder and a single - layer decoder with 1 , 024 GRUs each , and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte - pair merges ( Sennrich et al . , 2016 ) . Maximum input and out - put sequence length are set to 60 . For the MLE training of the out - of - domain model , we optimize # 1 source Diese k¨onnten Kurierdienste sein , oder Techniker zum Beispiel , nur um sicherzustellen , dass der gemeldete AED sich immer noch an seiner Stelle beﬁndet . target These could be courier services , or technicians like , for example , just to make sure that the abalone aed is still in its place . rating σ = 0 . 46 , ∅ = − 0 . 30 # 2 source Es muss f¨ur mich im Hier und Jetzt stimmig sein , sonst kann ich mein Publikum nicht davon ¨uberzeugen , dass das mein Anliegen ist . target It must be for me here and now , otherwise i cannot convince my audience that my concern is . rating σ = 0 . 46 , ∅ = − 0 . 70 # 3 source Aber wenn Sie biologischen Evolution akzeptieren , bedenken Sie folgendes : ist es nur ¨uber die Vergangenheit , oder geht es auch um die Zukunft ? target But if you accept biological evolution , consider this : Is it just about the past , or is it about the future ? rating σ = 0 . 48 , ∅ = 1 . 12 # 4 source Finden Sie heraus , wie Sie ¨uberleben w¨urden . Die meisten unserer Spieler haben die im Spiel gelernten Gewohnheiten beibehalten . target Find out how you would survive . rating σ = 1 . 31 , ∅ = − 0 . 79 # 5 source Sie k¨onnen das googlen , aber es ist keine Infektion des Rachens sondern der oberen Atemwege und verursacht den Verschluss der Atemwege . target You can googlen , but it’s not an infection of the rag , but the upper respiratory pathway , and it causes respiratory traction . rating σ = 1 . 31 , ∅ = − 0 . 52 # 6 source Nun , es scheint mir , dieses Thema wird , oder sollte wenigstens die interessanteste politische Debatte zum Verfolgen sein ¨uber die n¨achsten paar Jahre . target Well , it seems to me that this issue is going to be , or should be at least the most interesting political debate about the next few years . rating σ = 1 . 25 , ∅ = − 0 . 93 Table 5 : Items with lowest ( top ) and highest ( bottom ) deviation in 5 - star ratings . Mean normalized rating and standard deviation are reported . Problematic parts of source and target are underlined , namely hallucinated or inadequate target words ( # 1 , # 5 , # 6 ) , over - literal translations ( # 2 ) , ungrammatical source ( # 3 , # 6 ) and omissions ( # 4 ) . # 1 source ZudiesemZeitpunkthabenwirmehrzelligeGemeinschaften , GemeinschaftenvonvielenverschiedlichenZellentypen , welchezusammenalseinzelnerOrganismusfungieren . target1 Atthistimewehavemulti - tentcommunities , communitiesofmanydifferentcelltypes , whichacttogetherasindividualorganism . target2 Atthispoint , wehavemulticellularcommunities , communitiesofmanydifferentcelltypes , whichacttogetherasindividualorganism . rating σ = 0 . 0 , ∅ = 1 . 0 # 2 source WirdurchgehendieselbenStufen , welcheMehrzellerorganismendurchgemachthaben – DieAbstraktionunsererMethoden , wiewirDatenfesthalten , pr¨asentieren , verarbeiten . target1 Wepassthesamestepsthathavepassedthroughmulti - cellorganismstoprocesstheabstractionofourmethods , howwerecorddata . target2 Wegothroughthesamestepsthatmulticellularorganismshavegonethrough – theabstractionofourmethodsofholdingdata , representing , processing . rating σ = 0 . 0 , ∅ = 1 . 0 # 3 source Ichhieltmeinen ¨ublichenVortrag , unddanachsahsiemichanundsagte : ”Mhmm . Mhmm . Mhmm . ” target1 Ithoughtmyusualtalk , andthenshelookedatmeandsaid : mhmm . target2 Igavemyusualtalk , andthenshelookedatmeandsaid , ”mhmm . Mhmm . Mhmm . ” rating σ = 0 . 0 , ∅ = 1 . 0 # 4 source SoindiesenPl¨anen , wirhattenungef¨ahr657Pl¨anendiedenMenschenirgendetwaszwischenzweibis59verschiedenenFondsanboten . target1 Sointheseplans , wehadabout657plansthatofferedthepeoplesomethingbetweentwoto59differentfunds . target2 Sointheseplans , wehadabout657plansthatofferedpeopleanythingbetweentwoto59differentfunds . rating σ = 0 . 99 , ∅ = 0 . 14 # 5 source Wirﬁngendannan , ¨uberMusikzusprechen , angefangenvonBach ¨uberBeethoven , Brahms , BrucknerundalldieanderenBs , vonBart´okbishinzuEsa - PekkaSalonen . target1 Wethenbegantotalkaboutmusic , startingfrombachonBeethoven , Brahms , Brucknerandalltheotherbs , fromBart ´ oktoesa - pekkasalons . target2 Westartedtalkingaboutmusicfrombach , Beethoven , Brahms , Brucknerandalltheotherbs , fromBartoktoesa - pekkasalons . rating σ = 0 . 99 , ∅ = − 0 . 14 # 6 source Heinrichmussaufalldieswarten , nichtweilertats¨achlicheinanderesbiologischeAlterhat , nuraufgrunddesZeitpunktesseinerGeburt . target1 Heinrichhastowaitforallofthis , notbecausehe’sactuallyhavinganotherbiologicalage , justbecauseofthetimeofhisbirth . target2 Heinrichmustwaitforallthis , notbecauseheactuallyhasanotherbiologicalage , onlyduetothetimeofhisbirth . rating σ = 0 . 99 , ∅ = − 0 . 14 Table 6 : Items with lowest ( top ) and highest ( bottom ) deviation in pairwise ratings . Preferences of target1 are treated as ” - 1” - ratings , preferences of target2 as ”1” , no preference as ”0” , so that a mean ratings of e . g . - 0 . 14 expresses a slight preference of target1 . Problematic parts of source and targets are underlined , namely hallucinated or inadequate target words ( # 1 , # 2 , # 3 , # 4 ) , incorrect target logic ( # 2 ) , omissions ( # 3 ) , ungrammatical source ( # 4 ) , capitalization ( # 5 ) , over - literal translations ( # 5 , # 6 ) . the parameters with Adam ( α = 10 − 4 , β 1 = 0 . 9 , β 2 = 0 . 999 , (cid:15) = 10 − 8 ) ( Kingma and Ba , 2014 ) . For further in - domain tuning ( supervised , OPL and RL ) , α is reduced to 10 − 5 . To prevent the models from overﬁtting , dropout with probability 0 . 2 ( Srivastava et al . , 2014 ) and l2 - regularization with weight 10 − 8 are applied during training . The gradient is clipped to its norm when its norm ex - ceeds 1 . 0 ( Pascanu et al . , 2013 ) . Early stopping points are determined on the respective develop - ment sets . For model selection we use greedy de - coding , for test set evaluation beam search with a beam of width 10 . For MLE and OPL models , mini - batches of size 60 are used . For the RL mod - els , we reduce the batch size to 20 to ﬁt k = 5 samples for each source into memory . The tem - perature is furthermore set to τ = 0 . 5 . We found that learning rate and temperature were the most critical hyperparameters and tuned both on the de - velopment set .