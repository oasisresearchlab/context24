Ex Machina : Personal Attacks Seen at Scale Ellery Wulczyn ∗ Wikimedia Foundation ellery @ wikimedia . org Nithum Thain ∗ Jigsaw nthain @ google . com Lucas Dixon Jigsaw ldixon @ google . com ABSTRACT The damage personal attacks make to online discourse motivates many platforms to try to curb the phenomenon . However , under - standing the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difﬁcult . The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale . We show an evaluation method for a classiﬁer in terms of the aggregated number of crowd - workers it can approximate . We apply our methodology to English Wikipedia , generating a cor - pus of over 100k high quality human - labeled comments and 63M machine - labeled ones from a classiﬁer that is as good as the ag - gregate of 3 crowd - workers . Using the corpus of machine - labeled scores , our methodology allows us to explore some of the open questions about the nature of online personal attacks . This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users , nor primarily the consequence of allowing anonymous contributions . 1 . INTRODUCTION With the rise of social media platforms , online discussion has become integral to people’s experience of the internet . Unfortu - nately , online discussion is also an avenue for abuse . The 2014 Pew Report highlights that 73 % of adult internet users have seen some - one harassed online , and 40 % have personally experienced it [ 5 ] . Platforms combat this with policies concerning such behavior . For example Wikipedia has a policy of “Do not make personal attacks anywhere in Wikipedia” [ 31 ] and notes that attacks may be removed and the users who wrote them blocked . 1 The challenge of creating effective policies to identify and ap - propriately respond to harassment is compounded by the difﬁculty of studying the phenomena at scale . Typical annotation efforts of abusive language , such as that of Warner and Hirschberg [ 26 ] , in - volve labeling thousands of comments , however platforms often have many orders of magnitude more ; Wikipedia for instance has ∗ Equal contribution . 1 This study uses data from English Wikiedia , which for brevity we will simply refer to as Wikipedia . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Copyright 20XX ACM X - XXXXX - XX - X / XX / XX . . . $ 15 . 00 . 63M English talk page comments . Even using crowd - workers , get - ting human - annotations for a large corpus is prohibitively expen - sive and time consuming . The primary contribution of this paper is a methodology for quan - titative , large - scale , longitudinal analysis of a large corpus of on - line comments . Our analysis is applicable to properties of com - ments that can be labeled by crowd - workers with high levels of inter - annotator agreement . We apply our methodology to personal attacks on Wikipedia , inspired by calls from the community for re - search to understand and reduce the level of toxic discussions [ 29 , 28 ] , and by the clear policy Wikipedia has on personal attacks [ 31 ] . We start by crowdsourcing a small fraction of the corpus , label - ing each comment according to whether it is a personal attack or not . We use this data to train a machine learning classiﬁer , exper - imenting with features and labeling methods . Our results validate those of Nobata et al . [ 15 ] : character - level n - grams result in an im - pressively ﬂexible and performant classiﬁer . Moreover , it also re - veals that using the empirical distribution of human - ratings , rather than the majority vote , produces a signiﬁcantly better classiﬁer . The classiﬁer is then used to annotate the entire corpus of com - ments - acting as a surrogate for crowd - workers . To know how meaningful the automated annotations are , we develop an evalua - tion method for comparing an algorithm to a group of human an - notators . We show that our classiﬁer is as good at generating labels as aggregating the judgments of 3 crowd - workers . To enable inde - pendent replication of the work in this paper , as well as to support further quantitative research , we have made public our corpus of both human and machine annotations as well as the classiﬁer we trained [ 34 ] . We use our classiﬁer’s annotations to perform quantitative anal - ysis over the whole corpus of comments . To ensure that our results accurately reﬂect the real prevalence of personal attacks within dif - ferent sub - groups of comments , we select a threshold that appro - priately balances precision and recall . We also empirically validate that the threshold produces results on subgroups of comments com - mensurate with the results of crowd - workers . This allows us to answer questions that our much smaller sample of crowdsourced annotations alone would struggle to . We illustrate this by showing how to use our method to explore several open questions about the nature of personal attacks on Wikipedia : What is the impact of anonymity ? How do attacks vary with the quantity of a user’s contributions ? Are attacks concentrated among a few highly toxic users ? When do attacks result in a moderator action ? And is there a pattern to the timing of personal attacks ? The rest of the paper proceeds as follows : Sec . 2 discusses re - lated work on the prevalence , impact , and detection of personal attacks and closely related online behaviors . In Sec . 3 we describe our data collection and labeling methodology . Sec . 4 covers our 1 a r X i v : 1610 . 08914v1 [ c s . C L ] 27 O c t 2016 model - building and evaluation approaches . We describe our analy - sis of personal attack in Wikipedia in Sec . 5 . We conclude in Sec . 6 and outline challenges with our method and possible avenues of future work . 2 . RELATED WORK Deﬁnitions , Prevalence and Impact . One of the challenges in studying negative online behavior is the myriad of forms it can take and the lack of a clear , common deﬁnition [ 18 ] . While this study focuses on personal attacks , other studies explore different forms of online behavior including hate speech ( [ 7 ] , [ 13 ] , [ 18 ] , [ 26 ] ) , online harassment ( [ 3 ] , [ 37 ] ) , and cyberbullying ( [ 17 ] , [ 19 ] , [ 24 ] , [ 33 ] ) . Online harassment itself is sometimes further divided into a tax - onomy of forms . A recent Pew Research Center study deﬁnes on - line harassment to include being : called offensive names , purpose - fully embarrassed , stalked , sexually harassed , physically threat - ened , and harassed in a sustained manner [ 5 ] . The Wikimedia Foundation Support and Safety team conducted a similar survey [ 22 ] using a different taxonomy ( see Figure 1 ) . Figure 1 : Forms of harassment experienced on Wikimedia [ 22 ] . This toxic behavior has a demonstrated impact on community health both on and off - line . The Wikimedia Foundation found that 54 % of those who had experienced online harassment expressed decreased participation in the project where they experienced the harassment [ 22 ] . Online hate speech and cyberbullying are also closely connected to suppressing the expression of others [ 20 ] , phys - ical violence [ 27 ] , and suicide [ 4 ] . Automated Detection . There have been a number of recent papers on detecting forms of toxic behavior in online discussions . Much of this work builds on existing machine learning approaches in ﬁelds like sentiment analysis [ 16 ] and spam detection [ 21 ] . On the topic of harassment , the earliest work on machine learning based detec - tion is Yin et al . ’s 2009 paper [ 37 ] which used support vector ma - chines on sentiment and context features extracted from the CAW 2 . 0 dataset [ 6 ] . In [ 20 ] , Sood et al . use the same algorithmic frame - work to detect personal insults using a dataset labeled via Amazon Mechanical Turk from the Yahoo ! Buzz social news site . Dinakar et al . [ 4 ] decompose the issue of cyberbullying by training separate classiﬁers for related to variants that target sexuality , race or intel - ligence in YouTube comments . Building on these works , Cheng et al . [ 3 ] use random forests and logistic regression techniques to predict which users of the comment sections of several news sites would become banned for antisocial behavior . Most recently , No - bata et al . [ 15 ] extract character n - gram , linguistic , syntactic , and distributional semantic features from a very large corpus of Yahoo ! Finance and News comments to detect abusive language . Data Sets . A barrier to further algorithmic progress in the detec - tion of toxic behavior is a dearth of large publicly available datasets [ 18 ] . To our knowledge , the current open datasets are limited to the Internet Argument Corpus [ 25 ] , the CAW 2 . 0 dataset provided by the Fundacion Barcelona Media [ 6 ] , and the " Detecting Insults in Social Commentary " dataset released by Impermium via Kaggle [ 10 ] . In past work , many researchers have relied on creating their own hand - coded datasets ( [ 13 ] , [ 20 ] , [ 26 ] ) , using crowd - sourced or in - house annotators . These approaches limits the size of the labeled corpora due to the expense of labeling examples . A few authors have suggested alternative techniques that could be effective in ob - taining larger scale datasets . In [ 18 ] , Saleem et al . outline some of the limitations of using a small hand - coded dataset and suggest an alternative approach that uses all comments within speciﬁc on - line communities as positive and negative training examples of hate speech . Xiang et al . [ 35 ] use topic modeling approaches along with a small seed set of tweets to produce a training set for detecting of - fensive tweets containing over 650 million entries . Building on the work of [ 37 ] , Moore et al . [ 14 ] use a simple rules based algorithm for the automatic labeling of forum posts on which they wish to do further analysis . 3 . CROWDSOURCING In this section we discuss our approach to identifying personal attacks in a subset of Wikipedia discussion comments via crowd - sourcing . The crowdsourcing process involves : 1 . generating a corpus of Wikipedia discussion comments , 2 . choosing a question for eliciting human judgments , 3 . selecting a subset of the discussion corpus to label , 4 . designing a strategy for eliciting reliable labels . To generate a corpus of discussion comments , we processed the public dump of the full history of English Wikipedia as described in Appendix A . The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004 - 2015 . The question we posed to get human judgments on whether a comment contains a personal attack is shown in Figure 2 . In addi - tion to identifying the presence of an attack , we also try to elicit if the attack has a target or whether the comment quotes a previous attack . We do not , however , make use of this additional information in this study . Before settling on the exact phrasing of the question , we experimented with several variants and chose the one with the highest inter - annotator agreement on a set of 1000 comments . Figure 2 : An example unit rated by our Crowdﬂower annotators . To ensure representativeness , we undertook the standard approach of randomly sampling comments from the full corpus . We will re - fer to this set of comments as the random dataset . Through labeling a random sample , we discovered that the overall prevalence of per - sonal attacks on Wikipedia is around 1 % ( see Section 5 . 1 ) . To allow training of classiﬁers , we need enough examples of per - sonal attacks for the machine to learn from . We increase the num - ber of personal attacks found by also sampling comments made by users who where blocked for violating Wikipedia’s policy on per - sonal attacks [ 31 ] . In particular , we consider the 5 comments made by these users around every block event . We call this the blocked dataset and note that it has a much higher prevalence of attacks ( approximately 17 % ) . 2 Sample Type AnnotatedComments PercentageAttacking Random 37611 0 . 9 % Blocked 78126 16 . 9 % Total 115737 11 . 7 % Table 1 : Summary statistics of labeled data . Each comment was labeled 10 times . Here we deﬁne a comment as an attack if the majority of annotators labeled it as such . We labeled our subset of comments using the Crowdﬂower crowd - sourcing platform . 2 Crowdsourcing as a data collection methodol - ogy is well studied ( [ 23 ] , [ 2 ] ) and has proven an effective way to get datasets to train machine learning of online harassment ( [ 3 ] , [ 20 ] ) and hate speech ( [ 26 ] , [ 13 ] ) . As a ﬁrst step to ensuring data quality , each annotator was re - quired to pass a test of 10 questions . These questions were ran - domly selected from a set that we devised to contain balanced rep - resentation of both attacking and non - attacking comments . An - notators whose accuracy on these test questions fell below a 70 % threshold would be removed from the task . This improved our an - notator quality by excluding the worst ~ 2 % of contributors . Under the Crowdﬂower system , additional test questions are randomly in - terspersed with the genuine crowdsourcing task ( at a rate of 10 % ) in order to maintain response quality throughout the task . In order to get reliable estimates of whether a comment is a per - sonal attack , each comment was labeled by at least 10 different Crowdﬂower annotators . This allows us to aggregate judgments from 10 separate people when constructing a single label for each comment . We chose 10 judgments based on experiments in Sec . 4 . 3 that showed that aggregating more judgments provided little further improvement . Finally , we applied several data cleaning steps to the Crowdﬂower annotations . This included removing annotations where the same worker labeled a comment as both an attack and not an attack and removing comments that most workers ﬂagged as not being English . We evaluated the quality of our crowd - sourcing pipeline by mea - suring inter - annotator agreement [ 11 ] . This technique measures whether a set of “common instructions to different observers of the same set of phenomena , yields the same data within a tolerable mar - gin of error” [ 9 ] . We chose the speciﬁc inter - annotator agreement metric of Krippendorf’s alpha due to our context , where multiple raters rate overlapping but disparate sets of comments ! [ 12 ] . Our data achieves Krippendorf’s alpha scores of 0 . 45 . This result is in - line with results achieved in other crowdsourced studies of toxic behavior in online communities [ 3 ] . 4 . MODEL BUILDING We now use the set of crowdsourced annotations to build a ma - chine learning classiﬁer for identifying personal attacks . We ﬁrst discuss the set of machine learning architectures we explored and then describe our evaluation methodology . 4 . 1 Model Building Methodology We treat the problem of identifying personal attacks as a binary text classiﬁcation problem . We rely purely on features extracted from the comment text instead of including features based on the authors’ past behavior and the discussion context . This makes it easy for Wikipedia editors and administrators , journalists and other 2 https : / / www . crowdﬂower . com / researchers to explore the strengths and weaknesses of the models by simply generating text examples . It also allows the models to be applied beyond the context of Wikipedia . In terms of model architectures , we explored logistic regression ( LR ) , and multi - layer perceptrons ( MLP ) . In future work , we plan to experiment with long short - term memory recurrent neural net - works ( LSTM ) as well . For the LR and MLP models we simply use bag - of - words representations based on either word - or character - level n - grams . Past work in the domain of detecting abusive lan - guage in online discussion comments , showed that simple n - gram features are more powerful than linguistic and syntactic features , hand - engineered lexicons , and word and paragraph embeddings [ 15 ] . In all of the model architectures , we have a ﬁnal softmax layer and use the cross - entropy as our loss function . The cross - entropy function is deﬁned as : H ( y , ˆ y ) = − (cid:88) i y i log ( ˆ y i ) ( 1 ) where ˆ y is our predicted probability distribution over classes , and y is the true distribution . In addition to experimenting with different model architectures , we also experimented with two different ways of synthesizing our 10 human annotations per comment to create training labels . In the traditional classiﬁcation approach , there is only one true class and so the true distribution , y , is represented as a one - hot ( OH ) vector determined by the majority class in the comment’s set of annotations . For the problem of identifying personal attacks , however , one can argue that there is no single true class . Different people may judge the same comment differently . Unsurprisingly , we see this in the annotation data : most comments do not have a unanimous set of judgments , and the fraction of annotators who think a comment is an attack differs across comments . The set of annotations per comment naturally forms an approxi - mate empirical distribution ( ED ) over opinions of whether the com - ment is an attack . A comment considered a personal attack by 7 of 10 annotators can thus be given a true label of [ 0 . 3 , 0 . 7 ] instead of [ 0 , 1 ] . Using ED labels is motivated by the intuition that comments for which 100 % of annotators think it is an attack are probably different in nature from comments where only 60 % of annotators consider it so . Since the majority class is the same in both cases , the OH labels lose the distinction . Hence , in addition to the OH labels , we also trained each architecture using ED labels . Finally , we should note that the interpretation of a model’s scores depends on whether it was trained on ED or OH labels . In the case of a model trained on ED labels , the attack score represents the predicted fraction of annotators who would consider the com - ment an attack . In the case of a model trained on OH labels , the attack score represents the probability that the majority of annota - tors would consider the comment an attack . 4 . 2 Model Building Evaluation As discussed above , we considered three major dimensions in the model design space : 1 . model architecture ( LR , MLP ) 2 . ngram type ( word , char ) 3 . label type ( OH , ED ) In order to evaluate each of the 8 possible modeling strategies we randomly split our set of annotations into train , development and 3 test splits ( in a 3 : 1 : 1 ratio ) . For each model , we performed 15 itera - tions of random search over a grid of relevant hyper - parameters [ 1 ] . 3 During the model tuning process , each run was trained on the train split and evaluated on the development split . Table 2 shows two evaluation metrics for each of the 8 tuned models . The standard 2 - class area under the receiver operating characteristic curve ( AUC ) score is computed between the models’ predicted probability of be - ing an attack and the majority class label in the set of annotations for each comment . To better evaluate the performance of models trained on ED labels , we also include the Spearman rank correlation between the models’ predicted probability of being an attack and the fraction of annotators who considered the comment an attack . ModelType NGramType LabelType AUC Spearman LR Word OH 94 . 62 53 . 16 ED 95 . 55 65 . 2 Char OH 96 . 18 59 . 20 ED 96 . 24 66 . 68 MLP Word OH 95 . 25 56 . 11 ED 96 . 15 66 . 33 Char OH 95 . 90 58 . 77 ED 96 . 59 68 . 17 Table 2 : Evaluation metrics of different model architectures trained on the train split and evaluated on the development split . The hyper - parameters of each architecture were tuned using random - ized search Across all model and label types , we see that character n - grams outperform word n - grams , which is consistent with feature impor - tance analysis in [ 15 ] . 4 We suspect this is due to the higher robust - ness to spelling variations that char - ngrams exhibit , which are very common in online discussions , especially in expletives commonly used in personal attacks . Another consistent pattern is the boost in the performance met - rics for models trained using ED labels . The large boost in Spear - man correlation is somewhat unsurprising because it is a function of the fraction of annotators who consider a comment to be a per - sonal attack . The models trained using OH labels did not receive any supervision on how to estimate this fraction ( they only see ma - jority class ) . The interesting result is that even the AUC scores are consistently higher : this means that using training labels that en - code what fraction of people think a comment is an attack helps in predicting what the majority of annotators think . Our results indi - cate that using ED labels may give a performance boost over the standard OH labels for other machine - learning tasks using multiple crowdsourced labels per training example . 4 . 3 Human Baseline Comparison As mentioned in the introduction , we developed a classiﬁer for detecting personal attacks in order to score the full history of com - ments on Wikipedia in a cost and time effective manner . For our purposes , the model is an approximation of the crowdsourcing pro - 3 For details on the set of hyper - parameters explored , we re - fer the reader to the relevant notebook in our code repos - itory : https : / / github . com / ewulczyn / wiki - detox / blob / master / src / modeling / cv _ ngram _ architectures . ipynb 4 Note we explored word n - grams in the range of 1 - 2 and characters n - grams in the range 1 - 5 . During the hyper - parameters search , we searched over the same range of values for the number of ngram features to include . cess . Hence , we want to be able to answer the question : How good of a surrogate is our model for crowdsourced annotations ? To answer this we will use one group of annotators , call them the prediction - group , to predict what another group of annotators , call them the truth - group , thinks about a comment . We treat the aggre - gated judgments of the truth - group as ground truth labels . We treat the prediction - group as a model : an ensemble of annotators , who pool their judgments to make predictions . Hence , we will refer to the prediction - group as the annotator ensemble . By comparing our machine learning model’s predictive power to the predictive power of the annotator ensemble , we can get an estimate of how good of a surrogate our model is for a ﬁxed size annotator ensemble . We will refer to this method of generating baselines as annotator ensemble baselining . To be more speciﬁc , lets ﬁx the size of the truth - group at n t and the size of the prediction - group at n p . Assume we have collected at least n t + n p annotations per comment in our corpus . Now , for each comment c , we randomly split the full set of annotations for c into two non - overlapping sets , T c for the truth - group and P c for the prediction - group , of sizes n t and n p respectively . We split the set of annotators at the comment level , since the corpus of comments may be so large that not every annotator judges every comment , making a ﬁxed split across all comments impossible in general . We will ag - gregate annotations in T c using the function agg true to get a ground truth label y ( c ) for comment c . The choice of agg true depends on the evaluation metric we want to use . For the AUC metric agg true is the OH aggregation function , whereas for Spearman correlation agg true is the ED aggregation function . We will take the average of the annotations in P c to get a prediction ˆ y ( c ) AE . We apply our machine learning model , to comment c to get prediction ˆ y ( c ) ML . Finally , we compute the evaluation metrics of AUC and Spearman correlation over the entire corpus between y and ˆ y ML and between y and ˆ y AE to compare the machine learning model to the annota - tor ensemble . The comparison of these scores , tells us how good our model is compared to an ensemble of annotators of size n p at predicting labels generated by pooling n t annotations . Note that , for each question , the annotators are randomly split into a prediction - group and a truth - group . As a result , there is some variability in the evaluation metrics stemming from this assignment step . By running the entire process several times , we can estimate this variability and average the evaluation metric results from each run to get a more stable estimate . We applied our annotator ensemble baselining method to a set of 8 , 000 comments from the test split and had each comment la - beled 20 times . We will refer to this special subset of comments as the baseline split . Out of the baseline split , 4000 comments come from our random dataset and 4000 come from our blocked dataset . We ﬁx n t , the number of annotations used to generate labels , at 10 , since this is the number of annotations per comment used in training the model . Table 3 shows AUC scores and Spearman cor - relations for the aggregate prediction of the prediction - group as we vary its size n p from 1 to 10 . The ﬁnal line of the table also reports the values for the best LR model architecture from Table 2 . 5 The reported mean scores and standard errors are the result of running the entire process 25 times . For the annotator ensemble , both the AUC scores and Spearman correlations increase with diminishing returns as the size of the en - semble increases . On both of our metrics , our model outperforms 5 Note that the reported performance of our model is slightly differ - ent in Table 2 than in Table 3 , since in the former table the model is evaluated on the dev split , while in the latter table it is evaluated in the baseline split and ratios of random to blocked comments differ across the two splits . 4 an annotator ensemble of size n p = 3 . Thus , by these two metrics , running our model over the full history of comments in Wikipedia is as good as having each comment labeled by 3 annotators . n p AUC Spearman 1 88 . 54 ( 0 . 42 ) 53 . 58 ( 0 . 79 ) 3 95 . 49 ( 0 . 31 ) 64 . 75 ( 0 . 44 ) 5 97 . 13 ( 0 . 23 ) 68 . 27 ( 0 . 46 ) 7 97 . 81 ( 0 . 15 ) 69 . 86 ( 0 . 60 ) 9 98 . 24 ( 0 . 14 ) 70 . 97 ( 0 . 44 ) 10 98 . 53 ( 0 . 12 ) 71 . 11 ( 0 . 36 ) Model : 97 . 19 ( 0 . 14 ) 66 . 02 ( 0 . 44 ) Table 3 : Mean evaluation metrics ( and standard errors ) on the baseline split , ﬁxing the truth - group size n t at 10 and varying the prediction - group size n p . 5 . ANALYSIS Using the best personal attack classiﬁer from Sec . 4 . 2 , we obtain a full corpus of machine - labeled discussions in Wikipedia . In this section , we use the fully annotated corpus to better understand the prevalence and nature of attacks in Wikipedia . For the following analyses , we focus on comments made in 2015 and exclude admin - istrative comments and comments generated by bots as described in Appendix A . 5 . 1 Choosing a Threshold Given a comment , our classiﬁer outputs a continuous score in the interval [ 0 , 1 ] . To get a discrete label from this score , we pick a threshold t and let comments with a score above t have label 1 , indi - cating an attack . Using discrete labels makes it possible to identify individual comments that are predicted to contain personal attacks and estimate the fraction of attacks within a set of comments . To choose a threshold , we pick the point that strikes a balance be - tween precision and recall on random evaluation data . A key prop - erty of this threshold for the purpose of using machine - generated labels for analysis , is that false positives are offset by false nega - tives . As a result , the fraction of comments that are labeled as at - tacks by the classiﬁer is the same as the fraction of comments that are labeled as attacks by the human annotators . Hence , we refer to this threshold as the equal - error threshold . 6 To see how well this property generalizes to new data , we used the equal - error threshold on the development set and used it to get model - generated labels for the test set . Fig . 3a shows that the es - timated rate of attacks computed using model - generated labels lies within a 95 % conﬁdence interval for the rate of attacks computed from crowd - generated labels . Even though the thresholded model - scores give good estimates of the rate of attacks over a random sample of comments , it is not given that they also give accurate estimates when partitioning com - ments into different groups . To provide empirical evidence that the thresholded scores give accurate estimates when subdividing the dataset into groups that will be important for later analysis , we shows various splits of the dataset in Fig . 5 . 1 . We split on the year the comment was posted , by whether the author was logged - in , by the number of days the author has been active and by whether the comment contains the ngram " thank " , an important feature of the 6 This threshold also maximizes the F1 score , since our precision and recall are monotonic functions of the decision threshold . classiﬁer . For the following analyses , we then use the equal - error threshold over the union of the development and tests sets . At this threshold ( t = 0 . 425 ) , the precision is 0 . 63 , the recall ( e . g true - positive rate ) is 0 . 63 and the false - positive rate is 0 . 0034 . ( a ) ( b ) ( c ) ( d ) ( e ) ( f ) Figure 3 : Bootstrapped conﬁdence intervals for the fraction of at - tacks within the test set broken down by ( b ) discussion namespace , ( c ) year , ( d ) logged in status of the author , ( e ) number of days au - thor made an edit as of 2015 , ( f ) whether the comment contains the word " thank " . Blue intervals come from ground - truth human labels , green intervals come from machine generated labels . 5 . 2 Understanding Attacks In this section , we ask a number of questions about the nature of attackers , attack timing , and moderation on Wikipedia . We investi - gate the answers using our machine - labeled data at the equal - error threshold . What is the impact of anonymity ? Wikipedia users can make edits either under a registered user - name or anonymously . In the latter case , the edits are attributed to the IP address from which they were made . 7 Table 4 shows that , last year , 43 % of editing accounts were anonymous and these con - tributed 9 . 6 % of the comments in our dataset . It has been shown that anonymity provides psycho - social bene - ﬁts to cyberbullies [ 14 ] and can lead to " heightened aggression and inappropriate behavior " [ 36 ] . We compare the prevalence of at - tacks for registered and anonymous users in Table 4 . 8 This shows that the attack prevalence among comments by anonymous users is six times as high as that of registered users . Thus , while anony - mous contributions are much more likely to be an attack , overall they contribute less than half of attacks . This difference of means is signiﬁcant at a p < 0 . 0001 ( t = 63 . 8 ) level . These extreme values of signiﬁcance are not surprising as our algorithm allows us to label data at a population level . How do attacks vary with the quantity of a user’s contributions ? 7 This means that , in principle , there can be multiple users editing under the same IP , and the same user editing under multiple IPs 8 A future analysis might also try to differentiate registered accounts with a long running reputation from so called sock - puppet accounts created by a user to appear as if their contributions are coming from multiple users . 5 Anonymity Number of Accounts Number of Comments AttackPrevalence Anonymous 97 , 742 191 , 460 3 . 1 % Registered 129 , 394 2 , 023 , 559 0 . 5 % Totals 227 , 136 2 , 215 , 019 0 . 8 % Table 4 : Comment statistics by user anonymity ( 2015 ) . Editors on Wikipedia fall along a wide spectrum in terms of their engagement with discussions on the platform . Some comment a few times a year whereas others will comment several times a week . For our purposes , a user’s activity level is the number of comments that they made in 2015 . In Fig . 4a , we show how many comments were made by users with different activity levels . We see that over 60 % of comments are made by users who made over 100 comments over the year . Users who made 5 or fewer comments are only re - sponsible for 15 % of total comments . ( a ) ( b ) Figure 4 : ( a ) . A histogram of the percentage of total comments by user activity level . ( b ) A histogram of the percentage of total attacks by user activity level . The story completely changes when we use these same user seg - ments to understand attacking behavior . Fig . 4b shows the per - centage of total attacks attributable to users with different activity levels . We ﬁnd that almost half of all attacks are made by users with an activity level below 5 . Even controlling for the effect of anonymity that we saw earlier , more than 18 % of attacking com - ments are made by registered users with an activity level below 5 . Users with an activity level of over 100 comments ( almost all of whom are registered ) are responsible for 30 % of attacking com - ments . Thus , users at both low and high levels of contribution are responsible for a signiﬁcant portion of attacks . Are attacks concentrated among a few highly toxic users ? We deﬁne the toxicity level of a user to be the number of attacks written by that user in 2015 . By segmenting users by toxicity level , we are able to uncover whether attacks are diffused among many low toxicity users or concentrated among a few users with high toxicity . Fig . 5a describes the proportion of attacks made by users at dif - ferent levels of toxicity . Fig . 5b provides the total number of users at each toxicity level . By comparing these ﬁgures , we see that al - most 80 % of attacks come from the over 9000 users who have made fewer than 5 attacking comments . However , the 34 users with a tox - icity level of more than 20 are responsible for almost 9 % of attacks . Thus , while the majority of Wikipedia’s attacks are diffused infre - quent attackers , signiﬁcant progress could be made by moderating a relatively small number of frequent attackers . ( a ) ( b ) Figure 5 : ( a ) A histogram of the percentage of total attacks by user toxicity level . ( b ) A histogram of the total number of users at each toxicity level . When do attacks result in moderation ? Moderators and administrators can enforce the policy on per - sonal attacks [ 31 ] by warning or blocking offending users for a period of time . Our analysis takes all attacking comments in the 2015 Wikipedia corpus and asks how many of these lead to a mod - eration event in the following 7 days . We ﬁnd that 7 . 7 % of attacks are followed by a warning and 7 . 0 % of attacks are followed by a block within this period . 11 . 3 % of attacks are followed by either a warning or a block . As discussed in Sec . 5 . 1 , at the equal - error threshold , our algo - rithm has a precision of 0 . 63 . After normalizing by this precision , we ﬁnd that 12 . 2 % of the expected number of true attacks are fol - lowed by a warning , 11 . 1 % are followed by a block and 17 . 9 % are followed by either . Thus , a high proportion of attacking comments remain unmoderated . There are a number of factors that affect the chances of moder - ation , including repeated attacks and having been moderated in the past . Fig . 6 shows us that the chance of being blocked or warned increases with the number of personal attacks a user makes . Figure 6 : Probability of being warned or blocked in 2015 as a func - tion of the number of personal attacks made in 2015 Finally , we see in Fig . 7 that the likelihood of a new attack lead - ing to a block increases with the number of times a user has been blocked in the past . This may be due to heightened scrutiny of pre - viously blocked users . Alternatively , it may be that blocked users make more frequent or more toxic attacks , and are hence more likely to be warned and moderated in the future . Is there a pattern to the timing of attacks ? With machine labeled longitudinal data , we can generate a time - series of attacks in conversations as they occur on each page of Wikipedia . This allows us to ask whether there is a pattern to the 6 Figure 7 : Probability of being blocked again after a new attack as function of the number of times the user has been blocked in the past . timing of attacks in Wikipedia comments . To answer this ques - tion , we segment every comment in our corpus by whether or not it is a personal attack . We then build a neighborhood around each comment consisting of the n comments that occurred on the same page immediately before and after it , excluding the central com - ment . For each central comment , we compute the fraction of at - tacks that occur in this neighborhood and call this the neighboring attack fraction of the central comment . Table 5 shows the average neighboring attack fraction around attacking and non - attacking comments at different values of n . We see that even for small n , there is a signiﬁcant difference ( t = 56 . 2 , p < 0 . 0001 ) in neighboring attack fractions . Indeed , the neigh - boring attack fraction for n = 1 is twenty - two times higher around attacking comments than non - attacking comments . This is a strong indication personal attacks cluster in time on Wikipedia discus - sions . 9 It also suggests that early intervention by moderators could be an effective means of curbing the prevalence of personal attacks . n Attacking Non - Attacking 1 15 . 6 % 0 . 7 % 3 10 . 6 % 0 . 7 % 5 8 . 3 % 0 . 8 % Table 5 : Average neighboring attack fraction around attacking comments and non - attacking comments . 6 . DISCUSSION & CONCLUSION We have introduced a methodology for generating large - scale , longitudinal data on personal attacks in online discussions . Af - ter crowdsourcing the identiﬁcation of personal attacks within a sample of discussion comments , machine learning classiﬁcation is leveraged to scale the identiﬁcation process to the whole corpus . In so doing , we explored methods for aggregating multiple human judgments per comment into training labels , compared different model architectures and text features , and introduced a technique for comparing the performance of machine learning models to hu - man annotators . 9 A follow up analysis could investigate to what extent an initial attack sparks retaliation . We illustrated our methodology by applying it to Wikipedia , gen - erating an open dataset of over 100k high - quality human - labeled comments , 63M machine - labeled comments , and a classiﬁer that approximates the aggregate of 3 crowd - workers . We believe this provides the largest corpus of human - labeled comments support - ing the study of online toxicity to date . By calibrating our classiﬁer’s threshold we can then perform large scale longitudinal analysis of the whole corpus of Wikipe - dia discussions along a wide variety of dimensions . We illustrate this by exploring some open questions about the nature of personal attacks on Wikipedia . This leads to several interesting ﬁndings : while anonymously contributed comments are 6 times more likely to be an attack , they contribute less than half of the attacks . Simi - larly less than half of attacks come from users with little prior par - ticipation ; and perhaps surprisingly , approximately 30 % of attacks come from registered users with over a 100 contributions . These results suggest the problems associated with personal at - tacks do not have an easy solution . However , our study also shows that less than a ﬁfth of personal attacks currently trigger any action for violating Wikipedia’s policy . Moreover , personal attacks clus - ter in time - perhaps because one personal attacks triggers another . If so , early intervention by a moderator could have a disproportion - ately beneﬁcial impact . Moreover , automated classiﬁers may then be a valuable tool , not only for researchers , but also for moderators on Wikipedia . They might be used to help moderators build dash - boards that better visualize the health of Wikipedia conversations , or to develop systems to better triage comments for review . Perhaps the biggest challenge with our methodology is illus - trated by our case study with Wikipedia : we used a relatively small set of annotators , 4053 in total , whom we know little about . While they have reasonable levels of inter - annotator agreement , their in - terpretation of a comment being a personal attack may differ from that of the Wikipedia community . Moreover , the crowdsourced data may also result in other forms of unintended bias . This brings up key questions for our method and more generally for applications of machine learning to analysis of comments : who deﬁnes the truth for the property in question ? How much do clas - siﬁers vary depending on who is asked ? What is the subsequent impact of applying a model with unintended bias to help an online community’s discussion ? The methodology and data sets we have developed also open many other avenues for further work . The corpus of human - labeled comments can be used train more sophisticated machine learning models . It can also be used to analyze Wikipedia with traditional statistical inference , while our corpus of machine - labels can be em - ployed to carry out further analysis that require large scale and lon - gitudinal data . While there are many such questions to analyze , some notable examples include : 1 . What is the impact of personal attacks on a user’s future con - tributions ? 2 . What interventions can reduce the level of personal attacks on a conversation ? 3 . What are the triggers for personal attacks in Wikipedia com - ments ? Finally , we remark that our methodology can easily be applied to different characteristics of comments , not just personal attacks . As mentioned in Sec . 2 , there are many taxonomies by which one can analyze the positive and negative properties of a comment . There are also many other discussion corpora to be considered . Acknowledgements . The authors would like to thank CJ Adams , Dario Taraborelli and Patrick Earley for fruitful feedback and discussions . 7 APPENDIX A . WIKIPEDIA COMMENT CORPUS Here we describe our approach to generating a corpus of discus - sion comments from English Wikipedia . Every Wikipedia page , including articles and user pages , has an accompanying " talk page " that can be used for communicating with other users . Discussion pages pertaining to user pages are said to belong to the user talk namespace , while discussions pertaining to articles belong to the article talk namespace . Although there are 35 talk namespaces in total , we focus on these two throughout the paper since they contain at least an order of magnitude more discussion pages and comments than the others . MediaWiki , the software underlying Wikipedia , does not impose any constraints on editing talk pages . However , an edit on a talk page typically consists of a user adding a comment to a discussion in accordance with a set of formatting conventions . Figure 8 gives an example of a conventionally formatted discussion . Figure 8 : Example of a discussion thread taken from [ 30 ] . Includes the raw MediaWiki markdown or " Wiki text " and the corresponding rendering . One approach to generating a corpus of comments , is to take a current snapshot of all talk pages and parse each page into discus - sions and comments . The downside of this approach is that com - ments with personal attacks are usually quickly removed and that comments on user talk pages are often removed after they have been read to reduce clutter . As a result , no single snapshot of talk pages will contain a representative or complete collection of com - ments made on Wikipedia . We pursue an alternative approach , which involves processing the " revision history " , which represents the history of edits on a page as a sequence of ﬁles . There is a separate ﬁle , called a revi - sion , corresponding to the state of the article after each edit . We can compute a diff between successive revisions of a talk page to see what text was added as a result of each edit . The beneﬁt of this approach is that it captures all content that has been added to a talk page . 10 The downside is that the content added during a talk page edit is not always a full , new comment . The content added can also represent a modiﬁcation of an existing comment . For com - pleteness , we include the text added in these types of edits in our corpus . In practice , we processed the revision history from a public dump of English Wikipedia made available on 2016 - 01 - 13 . To generate the set of diffs from the revision history , we used the existing mwd - iffs [ 8 ] python package along with the standard longest - common - substring diff algorithm . For the purpose of this study we deﬁne a talk page comment as the concatenation of the MediaWiki markup 10 Note that there is a mechanism for removing revisions from the public record , and that personal attacks are a valid reason for doing so [ 32 ] . Since this work is based entirely on publicly available data , comments introduced on deleted revisions are not included in our corpus . added during an edit of a talk page . We also compute a clean , plain - text version of each comment by stripping out any html or Medi - aWiki markup , which we use in the crowd - sourcing task described below . While manually inspecting the data , we found that a large por - tion of comments left on talk pages ( 20 % - 50 % , depending on the namespace ) were clearly administrative in nature and generated us - ing a bot or template . In this study we are interested in comments made by human users in the context of discussions . After using a regular expression to ﬁlter out all messages from these users , we still observed a large number of administrative comments with lit - tle or nor modiﬁcation on many user talk pages . We generated a another set of regular expressions to remove the most commonly occurring comments of this nature . Table 6 gives the number of comments in the user and article talk namespaces after each ﬁlter - ing step . Namespace All No Bot NoBot / Admin User 47 . 3M 36 . 5M 24 . 2M Article 47 . 8M 39 . 2M 39 . 2M Totals 95 . 1M 75 . 7 63 . 4M Table 6 : Summary statistics of comment corpus broken down by namespace . We ﬁrst ﬁlter out all comments from bot accounts and then ﬁlter out messages containing templates used for administra - tive purposes . 7 . REFERENCES [ 1 ] J . Bergstra and Y . Bengio . Random search for hyper - parameter optimization . J . Mach . Learn . Res . , 13 : 281 – 305 , Feb . 2012 . [ 2 ] M . Buhrmester , T . Kwang , and S . D . Gosling . Amazon’s mechanical turk a new source of inexpensive , yet high - quality , data ? Perspectives on psychological science , 6 ( 1 ) : 3 – 5 , 2011 . [ 3 ] J . Cheng , C . Danescu - Niculescu - Mizil , and J . Leskovec . Antisocial behavior in online discussion communities . In ICWSM , 2015 . [ 4 ] K . Dinakar , R . Reichart , and H . Lieberman . Modeling the detection of textual cyberbullying . The Social Mobile Web , 11 : 02 , 2011 . [ 5 ] M . Duggan . Online harassment . Pew Research Center , 2014 . [ 6 ] Fundacion Barcelona Media ( FBM ) . Caw 2 . 0 training datasets , 2009 . http : / / caw2 . barcelonamedia . org / . [ 7 ] I . Gagliardone , D . Gal , T . Alves , and G . Martinez . Countering online hate speech . UNESCO Publishing , 2015 . [ 8 ] A . Halfaker . mwdiffs . https : / / github . com / mediawiki - utilities / python - mwdiffs . [ 9 ] A . F . Hayes and K . Krippendorff . Answering the call for a standard reliability measure for coding data . Communication methods and measures , 1 ( 1 ) : 77 – 89 , 2007 . [ 10 ] Impermium . Detecting insults in social commentary dataset , 2012 . https : / / www . kaggle . com / c / detecting - insults - in - social - commentary . [ 11 ] K . Krippendorff . Content analysis : An introduction to its methodology . Sage , 2004 . [ 12 ] K . Krippendorff . Reliability in content analysis . Human communication research , 30 ( 3 ) : 411 – 433 , 2004 . 8 [ 13 ] I . Kwok and Y . Wang . Locate the hate : Detecting tweets against blacks . In AAAI , 2013 . [ 14 ] M . J . Moore , T . Nakano , A . Enomoto , and T . Suda . Anonymity and roles associated with aggressive posts in an online forum . Computers in Human Behavior , 28 ( 3 ) : 861 – 867 , 2012 . [ 15 ] C . Nobata , J . Tetreault , A . Thomas , Y . Mehdad , and Y . Chang . Abusive language detection in online user content . In WWW , 2016 . [ 16 ] B . Pang and L . Lee . Opinion mining and sentiment analysis . Foundations and trends in information retrieval , 2 ( 1 - 2 ) : 1 – 135 , 2008 . [ 17 ] S . Pieschl , C . Kuhlmann , and T . Porsch . Beware of publicity ! perceived distress of negative cyber incidents and implications for deﬁning cyberbullying . Journal of School Violence , 14 ( 1 ) : 111 – 132 , 2015 . [ 18 ] H . M . Saleem , K . P . Dillon , S . Benesch , and D . Ruths . A web of hate : Tackling hateful speech in online social spaces . In TA - COS , 2016 . [ 19 ] A . Schrock and D . Boyd . Problematic youth interaction online : Solicitation , harassment , and cyberbullying . Computer - Mediated Communication in Personal Relationships , pages 368 – 398 , 2011 . [ 20 ] S . O . Sood , E . F . Churchill , and J . Antin . Automatic identiﬁcation of personal insults on social news sites . Journal of the American Society for Information Science and Technology , 63 ( 2 ) : 270 – 285 , 2012 . [ 21 ] N . Spirin and J . Han . Survey on web spam detection : principles and algorithms . ACM SIGKDD Explorations Newsletter , 13 ( 2 ) : 50 – 64 , 2012 . [ 22 ] Support and Safety Team . Harassment Survey . Wikimedia Foundation , 2015 . https : / / upload . wikimedia . org / wikipedia / commons / 5 / 52 / Harassment _ Survey _ 2015 _ - _ Results _ Report . pdf . [ 23 ] J . R . Tetreault , E . Filatova , and M . Chodorow . Rethinking grammatical error annotation and evaluation with the amazon mechanical turk . In NAACL - HLT , 2010 . [ 24 ] R . S . Tokunaga . Following you home from school : A critical review and synthesis of research on cyberbullying victimization . Computers in human behavior , 26 ( 3 ) : 277 – 287 , 2010 . [ 25 ] M . A . Walker , J . E . F . Tree , P . Anand , R . Abbott , and J . King . A corpus for research on deliberation and debate . In LREC , pages 812 – 817 , 2012 . [ 26 ] W . Warner and J . Hirschberg . Detecting hate speech on the world wide web . In LSM , 2012 . [ 27 ] D . Wiener . Negligent publication of statements posted on electronic bulletin boards : Is there any liability left after zeran . Santa Clara L . Rev . , 39 : 905 , 1998 . [ 28 ] Wikimedia . Harassment consultation 2015 . https : / / meta . wikimedia . org / wiki / Harassment _ consultation _ 2015 . [ 29 ] Wikimedia . Machine - learning tool to reduce toxic talk page interactions . https : / / meta . wikimedia . org / wiki / 2015 _ Community _ Wishlist _ Survey / Bots _ and _ gadgets # Machine - learning _ tool _ to _ reduce _ toxic _ talk _ page _ interactions . [ 30 ] Wikipedia . Help : Talk pages . https : / / www . mediawiki . org / wiki / Help : Talk _ pages . [ 31 ] Wikipedia . Wikipedia : No personal attacks . https : / / en . wikipedia . org / wiki / Wikipedia : No _ personal _ attacks . [ 32 ] Wikipedia . Wikipedia : Revision _ deletion . https : / / en . wikipedia . org / wiki / Wikipedia : Revision _ deletion . [ 33 ] N . E . Willard . Cyberbullying and cyberthreats : Responding to the challenge of online social aggression , threats , and distress . Research Press , 2007 . [ 34 ] E . Wulczyn , N . Thain , and L . Dixon . https : / / figshare . com / articles / Wikipedia _ Detox _ Data / 4054689 . [ 35 ] G . Xiang , B . Fan , L . Wang , J . Hong , and C . Rose . Detecting offensive tweets via topical feature discovery over a large scale twitter corpus . In CIKM , 2012 . [ 36 ] M . L . Ybarra and K . J . Mitchell . Youth engaging in online harassment : Associations with caregiver – child relationships , internet use , and personal characteristics . Journal of adolescence , 27 ( 3 ) : 319 – 336 , 2004 . [ 37 ] D . Yin , Z . Xue , L . Hong , B . D . Davison , A . Kontostathis , and L . Edwards . Detection of harassment on web 2 . 0 . In WWW , 2009 . 9