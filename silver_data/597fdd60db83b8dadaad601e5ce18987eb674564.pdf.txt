Estimating Replicability of Classiﬁer Learning Experiments Remco R . Bouckaert 1 , 2 remco @ cs . waikato . ac . nz , rrb @ xm . co . nz 1 . Computer Science Department , University of Waikato , Hamilton , New Zealand 2 . Xtal Mountain Information Technology , Auckland , New Zealand Abstract Replicability of machine learning experi - ments measures how likely it is that the out - come of one experiment is repeated when per - formed with a diﬀerent randomization of the data . In this paper , we present an estima - tor of replicability of an experiment that is eﬃcient . More precisely , the estimator is un - biased and has lowest variance in the class of estimators formed by a linear combination of outcomes of experiments on a given data set . We gathered empirical data for comparing experiments consisting of diﬀerent sampling schemes and hypothesis tests . Both factors are shown to have an impact on replicability of experiments . The data suggests that sign tests should not be used due to low replica - bility . Ranked sum tests show better perfor - mance , but the combination of a sorted runs sampling scheme with a t - test gives the most desirable performance judged on Type I and II error and replicability . 1 . Introduction Machine learning research on classiﬁers relies to a large extent on experimental observations . It is widely rec - ognized that there are many pitfalls in performing ex - periments [ 3 , 6 , 8 ] . But , so far , most research in this area concentrates on undesirable high levels of Type I error , the situation where the experiment indicates that one classiﬁer outperforms another , while in real - ity it does not . An often overlooked issue with experi - mental research is that the particular randomizations used in the experiment can have a major impact on the outcome of the experiment . This eﬀect can be so large that for some experimental designs only in 2 out of 3 cases repetition of the experiment produces the same outcome [ 2 ] . Appearing in Proceedings of the 21 st International Confer - ence on Machine Learning , Banﬀ , Canada , 2004 . Copyright 2004 by the author . In this paper , we try to get a better insight in this issue of replicability and which factors in an exper - iment inﬂuence replicability . In order to do so , we need a practical deﬁnition of replicability and a way to measure replicability of an experiment . Once this is established we can actually perform experiments on various set - ups . In the following section , we consider a number of experimental designs . We continue in Sec - tion 3 with ways to estimate replicability and perform a theoretical analysis of their performance . Section 4 presents empirical results where we measure replica - bility for the various experimental set - ups . We ﬁnish with some concluding remarks . 2 . Machine learning experiments The problem we want to address is , given two learn - ing algorithms A and B that generate classiﬁers and a small data set D , how to make a decision which of the two algorithms performs best based on classiﬁcation accuracy for the given data set . A general method to make such a decision is to split D into a training set D t and a test set D \ D t . Then , train algorithm A and B on D t and register the classiﬁcation accuracy on the D \ D t . This way , we obtain two classiﬁcation accura - cies P A and P B and the diﬀerence x = P A − P B gives an indication which algorithm performs better . A formal way to make such a decision is to apply a hy - pothesis test . However , such hypothesis test typically requires more than a single outcome x . Unfortunately , for small datasets D , we have to split D repeatedly in training and test sets to obtain multiple outcomes P A , i and P B , i with associated diﬀerences x i = P A , i − P B , i , 1 ≤ i ≤ n obtaining a sample of size n . So , an experiment has two components . Firstly , a sam - pling scheme for obtaining a sample x 1 , . . . , x n , and secondly , a hypothesis test to make a decision based on the sample . There are various ways to obtain sam - ples and to perform hypothesis tests . 2 . 1 . Sampling methods We consider six diﬀerent sampling schemes . Figure 1 . Example illustrating the data used for the various sampling schemes . used when averaging over runs used when averaging over folds used when averaging over sorted runs Fold Sorted Fold Run Run 1 2 3 1 2 3 123 123 3 . 33 6 . 66 6 . 66 5 . 55 10 3 . 33 - 10 1 . 11 - 6 . 660 - 3 . 33 - 3 . 33 2 . 22 3 . 33 - 2 . 22 - 6 . 660 - 10 - 5 . 55 3 . 33 3 . 33 - 3 . 33 1 . 11 10 6 . 66 6 . 66 7 . 77 Resampling : Resampling consist of splitting the data n times in a randomly selected test set D t , i contain - ing a fraction of the data ( typically 10 % to 33 % ) and a training set D \ D t , i . The algorithms A and B learn on the training set and accuracies P A , i and P B , i , 1 ≤ i ≤ n are obtained by classifying instances on the accompanying test set giving n accuracy diﬀerences x i = P A , i − P B , i for the sample . Resampling used to be an accepted way for applying the t - test on the sam - ple x 1 , . . . , x n till it was discredited by Dietterich [ 3 ] due to its extremely high Type I error . Nadeau and Bengio [ 6 ] showed how this problem can be solved by correcting the variance . K - fold cross validation : Cross validation splits the data D into k approximately equal parts D 1 , . . . , D k , and learns on the data D \ D i , 1 ≤ i ≤ k with one part left out . The part D i left out is used as test set , giving n = k accuracy diﬀerences x i = P A , i − P B , i . Dietterich [ 3 ] observed a slightly elevated Type I error for cross validation with a t - test and its replicability is rather low [ 2 ] . Use all data : To obtain more samples , we can repeat k - fold cross validation r times with diﬀerent random splits into folds for each of the runs . This gives us r × k accuracy diﬀerences . Let ˆ x i , j , 1 ≤ i ≤ r , 1 ≤ j ≤ k denote the diﬀerence in accuracy of algorithms A and B in the i th run on the j th fold . Here A and B are trained on the k − 1 remaining folds in the i th run . We obtain a sample of size n = r × k by using all of the accuracy diﬀerences x i , j ( formally by setting x i = ˆ x i mod r , ⌈ i / r ⌉ ) . Average over folds : In averaging over folds , the rec - ommended method for Weka [ 9 ] , we take the result in a repeated cross validation experiment . We obtain one sample value per run by taking the average diﬀer - ence over all results for a single run , x i = P kj = 1 ˆ x i , j / k ( where ˆ x i , j as for the use all data scheme ) . Average over runs : Averaging over folds can be interpreted as an improved way of doing resampling . The natural extension is performing an improved way of k - fold cross validation , and instead of averaging over folds , average over runs . We obtain one sam - ple value per fold deﬁned as the average diﬀerence x i = P ra = 1 ˆ x a , i / r . Both averaging over folds and over runs show a very high Type I error when applying a t - test [ 2 ] . Average over sorted runs : Averaging over runs combines results from diﬀerent runs rather arbitrar - ily . One gets better estimates of a k - fold cross valida - tion experiment by ﬁrst sorting the results for the in - dividual k - fold cross validation experiments and then taking the average . This way , the estimate for the minimum value is calculated from the minimum val - ues in all folds , the one but lowest from the one but lowest results in all folds , etc . Let ˆ x θ ( i , j ) be the j th highest value of accuracy diﬀerence ˆ x i ′ j ′ of run i . Then , the sample consisting of k values is deﬁned by x i = P ra = 1 ˆ x θ ( a , i ) / r . Figure 1 illustrates the diﬀerence between the data used for the sampling schemes . The ﬁgure shows an example of 3x3 fold cross validation outcomes in the box at the left half ( though in practice a 10x10 fold cross validation is more appropriate ) . All the data in the box in Figure 1 is used for the ”use all data” scheme . For resampling , essentially only the ﬁrst col - umn is required when performing a 2 / 3 - 1 / 3 split of training and test data . Cross validation uses only the ﬁrst run , that is , the ﬁrst row of a 3x3 fold cross val - idation outcome . Averaging over folds and runs is es - sentially summing over columns and rows respectively . For getting sorted means , ﬁrst the results have to be sorted over folds , giving the table at the right of Fig - ure 1 . Then the means are obtained by summing over rows . 2 . 2 . Hypothesis tests In our experiment , we want to test the null hypothesis H 0 that A and B perform the same . More formally , we want to test whether the sample x 1 , . . . , x n has zero mean . There are diﬀerent methods to test such hy - pothesis , all of which are based on slightly diﬀerent assumptions . We consider the popular t - test , the sign test and the rank sum test , also known as Wilcoxon’s test . All these tests assume that the outcomes x i in the sample are mutually independent , an assumption that is obviously violated . These hypothesis tests follow a similar procedure . First , we calculate a statistic Z from the sample . Dif - ferent tests have diﬀerent methods of calculating Z ( see below ) . Then , we calculate the probability p ( Z ) that the value Z or less is observed assuming H 0 is true . We choose a signiﬁcance level α and accept H 0 if p ( Z ) is higher than α / 2 but less than 1 − α / 2 . If p ( Z ) < α / 2 , the test indicates B outperforms A and if p ( Z ) > 1 − α / 2 , the test indicates A outperforms B . Paired t - test : The assumption underlying the paired t - test is that the outcomes x i are normally dis - tributed . If this is true , then the mean can be esti - mated using ˆ m = 1 n P ni = 1 x i , the variance using ˆ σ 2 = 1 n − 1 P ni = 1 ( x i − ˆ m ) 2 . With n − 1 degrees of freedom ( df = n − 1 ) we have a statistic Z = ˆ m √ ˆ σ 2 / √ df + 1 , which is distributed according to Students t - distribution P t with df degrees of freedom . The probability that the data x 1 , . . . , x n is observed assuming the null hypoth - esis is true is obtained by ﬁnding P t ( T , df ) . Sign test : The attractiveness of the sign test is that it is simple and makes no assumptions about the under - lying distribution of the sample . Instead , it only looks at the signs of x 1 , . . . , x n and statistic Z is the num - ber of pluses . When accuracies P A , i and P B , i are the same , which occurs quite often when two algorithms perform very similarly , x i = 0 and we count this as half a plus . If the null hypothesis is true , the probability of generating a plus or a minus is 0 . 5 , in other words H 0 : p = 0 . 5 . The probability of observing Z pluses in n comparisons is P ( Z ) = P Zi = 0 (cid:0) ni (cid:1) p i ( 1 − p ) n − i , which with p = 0 . 5 is P ( Z ) = P Zi = 0 (cid:0) ni (cid:1) 12 n . Rank sum test : Like the sign test , the rank sum test makes no assumption about the underlying dis - tribution of outcomes x i . However , the rank sum test does exploit the size of the values of x i , which contains potentially valuable information . The rank sum test sorts the outcomes x i on its absolute value , giving a set of outcomes y 1 , . . . , y n , | y i | ≤ | y i + 1 | ( 1 ≤ i < n ) . When accuracies are the same ( i . e . outcomes for which x i = 0 ) they are removed from the sample , leaving n ′ items . Now , we add the ranks of outcomes that are positive , r = P n ′ i = 1 , y i > 0 i . This statistic has mean m = n ′ ( n ′ + 1 ) 4 and variance σ 2 = n ′ ( n ′ + 1 ) ( n ′ + 2 ) 24 and is approximately normally distributed . So , we use Z = r − mσ , which is normally distributed with mean 0 and variance 1 . 2 . 3 . Quality of experiments There are essentially three methods to judge the qual - ity of an experiment : • The Type I error is the probability that the conclu - sion of an experiment is there is a diﬀerence between algorithms , while in reality there is not . In theory , the Type I error equals the signiﬁcance level chosen for the hypothesis test if none of the assumptions of the test are violated . In practice , the independence assump - tion is often violated resulting in an elevated Type I error . • The Type II error is the probability the conclusion of an experiment is there is no diﬀerence between algo - rithms , while in reality there is . The power is deﬁned as 1 minus the Type II error . The power is not directly controllable like the Type I error is . However , there is a trade - oﬀ between power and Type I error and a higher power can be obtained at the cost of a higher Type I error . The exact relation between the two de - pends on the experimental design . • Replicability of an experiment is a measure of how well the outcome of an experiment can be reproduced . The most desirable experiment has a low Type I error , a high power an high replicability . In the following section we will have a closer look at replicability . 3 . Replicability In [ 2 ] , an ad hoc deﬁnition for replicability was pro - posed as follows . When an experiment is repeated ten times with diﬀerent randomizations of a given data set , the experiment is deemed replicable if its outcome is the same for all ten experiments . If one or more outcomes diﬀer , it is not replicable . An impression of the replicability of an experiment can be obtained by averaging over a large number ( say 1000 ) of data sets . This deﬁnition is useful in highlighting that repli - cability of experiments is indeed an issue in machine learning . However , the disadvantage is that replica - bility measured this way cannot be compared with re - sults for doing the experiment another number than ten times . Also , replicability deﬁned this way would not distinguish between having 1 out of 10 outcomes being diﬀerent and 5 out of 10 outcomes being diﬀer - ent . Further , increasing the number of experiments to say 100 increases the likelihood that one of the experi - ments diﬀer and thus decreases replicability according to the deﬁnition of [ 2 ] . A deﬁnition of replicability that does not suﬀer from these issues is the following . Deﬁnition : Replicability of an experiment is the prob - ability two runs of the experiment on the same data set , with the same pair of algorithms and the same method of sampling the data produces the same out - come . This deﬁnition applies both in the situation where the algorithms perform the same and when one outper - forms another . Note the diﬀerence between Type I error and replicability . When the algorithms perform the same , the Type I error expresses the probability over all data sets that a diﬀerence is found . Replica - bility only expresses that error for one data set . By deﬁning replicability in terms of probabilities , one can compare replicability of diﬀerent experiments with diﬀerent experimental set - ups and number of runs . Furthermore , an experiment that produces 9 same out - comes out of 10 has a higher replicability this way than when it only produces 5 same outcomes out of 10 . Note that replicability always lies between 50 % and 100 % . Normalized replicability is replicability linearly scaled to the range 0 % to 100 % . So , if replicability is r , normalized replicability is 2 ( r − 12 ) . 3 . 1 . A simple estimator The only way to determine the replicability of an ex - periment is to measure it empirically . So , we need an estimator of replicability . A simple approach is to ob - tain pairs of runs of an experiment on a data set D and just interpret those as the outcome of Bernoulli trial with probability r that the outcomes are the same . The outcome e of an experiment on data set D is ’ac - cept’ or ’reject’ . When the outcome is ’accept’ the null hypothesis that the two learning algorithms per - form the same on D is accepted , otherwise they are not . Deﬁnition Let e = e 1 , . . . , e n ( n > 0 and n even ) be the outcomes of n experiments with diﬀerent random - izations on data set D . The estimator ˆ R 1 of replica - bility r is ˆ R 1 ( e ) = P n / 2 i = 1 I ( e 2 i = e 2 i − 1 ) n / 2 where I is the indicator function , which is 1 if its ar - gument is true , and 0 otherwise . We write ˆ R 1 if it is clear from the context what the argument e of ˆ R 1 ( e ) is . Lemma 3 . 1 ˆ R 1 is an unbiased estimator of replicabil - ity r with variance r − r 2 n / 2 . Proof : The bias of ˆ R 1 is E ( ˆ R 1 ) − r . Now , E ( ˆ R 1 ) = E ( P n / 2 i = 1 I ( e 2 i = e 2 i − 1 ) n / 2 ) . Taking the constant 1 n / 2 outside the expectation gives E ( ˆ R 1 ) = 1 n / 2 E ( P n / 2 i = 1 I ( e 2 i = e 2 i − 1 ) ) . Distributing the sum results in E ( ˆ R 1 ) = 1 n / 2 P n / 2 i = 1 E ( I ( e 2 i = e 2 i − 1 ) ) . Now , E ( I ( e 2 i = e 2 i − 1 ) ) = P ( I ( e 2 i = e 2 i − 1 ) ) I ( e 2 i = e 2 i − 1 ) + P ( I ( e 2 i = e 2 i − 1 ) ) I ( e 2 i 6 = e 2 i − 1 ) . Note that P ( I ( e 2 i = e 2 i − 1 ) ) = r and P ( I ( e 2 i 6 = e 2 i − 1 ) ) = 1 − r so we get E ( I ( e 2 i = e 2 i − 1 ) ) = r · 1 + ( 1 − r ) · 0 = r . Substituting in E ( ˆ R 1 ) above gives E ( ˆ R 1 ) = 1 n / 2 P n / 2 i = 1 r = n / 2 n / 2 r = r . So , the bias of ˆ R 1 = E ( ˆ R 1 ) − r = r − r = 0 , which shows that ˆ R 1 is an unbiased estimator of r . The variance of ˆ R 1 is var ( ˆ R 1 ) = E ( ˆ R 21 ) − E ( ˆ R 1 ) 2 = P n / 2 i = 0 P ( i same pairs out of n / 2 ) ( in / 2 ) 2 − E ( ˆ R 1 ) 2 where ˆ R 1 = 1 n / 2 . From the deriva - tion above , we have E ( ˆ R 1 ) 2 = r 2 . Further , ob - serve that P ( i same pairs out of n / 2 ) follows the bi - nomial distribution with probability r . So , we have var ( ˆ R 1 ) = P n / 2 i = 0 r i ( 1 − r ) n / 2 − i (cid:0) n / 2 i (cid:1) ( in / 2 ) 2 − r 2 = 1 ( n / 2 ) 2 P n / 2 i = 0 r i ( 1 − r ) n / 2 − i (cid:0) n / 2 i (cid:1) i 2 − r 2 . Us - ing Lemma A . 1 ( see Appendix ) , P n / 2 i = 0 r i ( 1 − r ) n / 2 − i (cid:0) n / 2 i (cid:1) i 2 = ( n / 2 ) 2 r 2 − r 2 n / 2 + rn / 2 giving var ( ˆ R 1 ) = 1 ( n / 2 ) 2 ( ( n / 2 ) 2 r 2 − r 2 n / 2 + rn / 2 ) − r 2 = r − r 2 n / 2 . 3 . 2 . An advanced estimator The simple estimator ˆ R 1 uses experiment e 1 only to compare with e 2 . Since e 3 is independent of e 1 , one could compare e 1 with e 3 as well . Likewise , the pair ( e 1 , e k ) for any k > 1 could be compared and used in the estimate of replicability . In fact , we can use all pairs of outcomes and estimate replicability as the fraction of pairs with the same outcome . This deﬁnes a new estimator ˆ R 2 . Deﬁnition Let e = e 1 , . . . , e n and n as before , then we deﬁne estimator ˆ R 2 ( e ) of r as ˆ R 2 ( e ) = X 1 ≤ i < j ≤ n I ( e i = e j ) n · ( n − 1 ) / 2 ( 1 ) According to the following lemma , we can actually cal - culate ˆ R 2 directly from counting the number of ac - cepted tests out of the n experiments . So , ˆ R 2 can be calculated eﬃciently in linear time of the number of experiments . Lemma 3 . 2 Let e = e 1 , . . . , e n and n as before and i out of n tests be accepting the null hypothesis , then ˆ R 2 ( e ) = ˆ R 2 ( i , n ) = i · ( i − 1 ) + ( n − i ) · ( n − i − 1 ) n · ( n − 1 ) Proof : The numerator of ˆ R 2 in ( 1 ) is the number of pairs with equal outcomes . If i ( 0 ≤ i ≤ n ) tests accept the null hypothesis and the remaining n − i do not , then (cid:0) i 2 (cid:1) pairs of rejecting pairs and (cid:0) n − i 2 (cid:1) pairs of non re - jecting pairs can be formed . This gives an estimate of replicability as ˆ R 2 ( i , n ) = ( (cid:0) i 2 (cid:1) + (cid:0) n − i 2 (cid:1) ) / ( n ( n − 1 ) / 2 ) = i ( i − 1 ) + ( n − i ) ( n − i − 1 ) n ( n − 1 ) . Now , we will examine the bias and variance of ˆ R 2 . It turns out that ˆ R 2 is an unbiased estimator of replica - bility and its variance can be expressed in closed form . Theorem 3 . 1 ˆ R 2 is an unbiased estimator of replica - bility r with variance 1 n · ( n − 1 ) · ( 2 ( n − 2 ) ( n − 3 ) F ( p , 4 ) + ( 4 − 2 ( n − 3 ) ) ( n − 2 ) F ( p , 3 ) + ( n − 2 ) ( n − 3 ) + 2 ) F ( p , 2 ) − r 2 where F ( p , x ) = p x + ( 1 − p ) x and p = 1 / 2 + 1 / 2 √ 2 r − 1 . The proof that ˆ R 2 is unbiased closely follows that of Lemma 3 . 1 . The proof establishing the variance of ˆ R 2 is rather technical and is omitted here . A full proof is available in the report version of this paper . Unfortunately , the closed form expression for the vari - ance of ˆ R 2 is hard to interpret and compare with that of ˆ R 1 . Figure 2 shows the variance of ˆ R 1 and ˆ R 2 for various values of replicability r and number of experi - ments n . It shows that the variance of ˆ R 2 is equal to that of ˆ R 1 when r = 1 . This is when there is full repli - cability and in this case the variance is zero . However , for other values of r , the variance of ˆ R 2 is always be - low that of ˆ R 1 , indicating that ˆ R 2 is a more eﬃcient estimator of replicability than ˆ R 1 . 3 . 3 . Is there a better estimator ? Is there an unbiased estimator of replicability with lower variance than ˆ R 2 based on experiments e = e 1 , . . . , e n on a single database ? We will consider the class of estimators based on linear functions of I ( e i = e j ) . Deﬁnition : Let e = e 1 , . . . , e n ( n > 0 and n even ) be the outcomes of n experiments with diﬀerent ran - domizations on data set D . Then estimator ˆ R k of r is ˆ R k ( e ) = X 1 ≤ i < j ≤ n k i , j I ( e i = e j ) ( 2 ) Note that ˆ R 1 is in this class with k i , i + 1 = 1 n / 2 for odd i and k i , j = 0 otherwise . Likewise , ˆ R 2 is in this class with k i , j = 1 n ( n − 1 ) / 2 for all 1 ≤ i < j ≤ n . If we demand that ˆ R k is unbiased , we put a restriction on the coeﬃcients k i , j expressed in the following lemma . Figure 2 . Variance of ˆ R 1 ( upper surface ) and ˆ R 2 ( lower surface ) as function of replicability r ∈ ( 0 . 5 . . . 1 . 0 ) and number of tests n ∈ ( 4 . . . 28 ) 4 8 12 16 20 24 28 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 0 0 . 02 0 . 04 0 . 06 0 . 08 0 . 1 0 . 12 0 . 14 Lemma 3 . 3 ˆ R k is an unbiased estimator of replicabil - ity r iﬀ X 1 ≤ i < j ≤ n k i , j = 1 ( 3 ) In the proof , we use the property that if r is the repli - cability of an experiment for a given data set , then , by deﬁnition , r is the probability two experiments pro - duce the same outcome . Now , two experiments use diﬀerent independent randomizations . So , if p is the probability that the outcome of a single experiment is accept , then the replicability is the probability that two outcomes are accept ( p · p ) plus the probability that two outcomes are reject ( ( 1 − p ) · ( 1 − p ) ) . So , r = p · p + ( 1 − p ) · ( 1 − p ) , which can be solved for p giving p = 12 ± 12 √ 2 r − 1 . Proof : For ˆ R k to be an unbiased estimator of replica - bility r , we must have E ( ˆ R k ) = r . Now , E ( ˆ R k ) by def - inition of expectation is P e P ( e ) ˆ R k ( e ) . Using ( 2 ) , this equals P e P ( e ) P 1 ≤ i < j ≤ n k i , j I ( e i = e j ) . Changing the order of sums , we get P 1 ≤ i < j ≤ n P e P ( e ) k i , j I ( e i = e j ) . Note that P e P ( e ) has equal outcomes for e i and e j only with probability p 2 ( both accept ) and ( 1 − p ) 2 ( both rejects ) . So , P e P ( e ) k i , j I ( e i = e j ) = ( p 2 + ( 1 − p ) 2 ) k i , j = rk i , j . Summing over i and j gives P 1 ≤ i < j ≤ n rk i , j = r P 1 ≤ i < j ≤ n k i , j = r where the last equality follows from the condition that the estimator is unbiased . Consequently , P 1 ≤ i < j ≤ n k i , j = 1 . So , ˆ R 1 and ˆ R 2 being unbiased ( Lemma 3 . 1 and The - orem 3 . 1 ) can be proven observing ˆ R 1 and ˆ R 2 are in - stances of ˆ R k and noting that the coeﬃcients k i , j add to 1 . Theorem 3 . 2 var ( ˆ R k ) ≥ var ( ˆ R 2 ) for any unbiased Table 1 . Type I error on Set 1 , power on Set 2 , 3 and 4 and replicability ( in percentages ) for various sampling methods ( 95 % conﬁdence interval in brackets ) . Source 1 Source 2 Source 3 Source 4 Minimum average Test Sampling scheme Type I Power Power Power norm . replicability Rank sum Resampling 14 . 8 ( ± 0 . 4 ) 27 . 9 ( ± 0 . 6 ) 48 . 0 ( ± 0 . 3 ) 95 . 7 ( ± 0 . 3 ) 34 . 0 ( ± 0 . 5 ) test k - fold cv 11 . 0 ( ± 0 . 2 ) 23 . 2 ( ± 0 . 2 ) 45 . 8 ( ± 0 . 7 ) 97 . 5 ( ± 0 . 2 ) 46 . 0 ( ± 0 . 5 ) Use all data 55 . 2 ( ± 0 . 3 ) 71 . 5 ( ± 0 . 2 ) 88 . 0 ( ± 0 . 1 ) 100 . 0 ( ± 0 . 0 ) 61 . 8 ( ± 0 . 6 ) Average over folds 59 . 8 ( ± 0 . 5 ) 78 . 9 ( ± 0 . 4 ) 90 . 2 ( ± 0 . 2 ) 100 . 0 ( ± 0 . 0 ) 56 . 4 ( ± 0 . 7 ) Average over runs 50 . 5 ( ± 0 . 6 ) 68 . 3 ( ± 0 . 1 ) 86 . 1 ( ± 0 . 1 ) 100 . 0 ( ± 0 . 0 ) 57 . 0 ( ± 0 . 9 ) Average sorted runs 4 . 1 ( ± 0 . 1 ) 20 . 2 ( ± 0 . 3 ) 46 . 8 ( ± 0 . 5 ) 99 . 3 ( ± 0 . 1 ) 80 . 6 ( ± 0 . 2 ) Sign test Average sorted runs 5 . 0 ( ± 0 . 5 ) 21 . 2 ( ± 0 . 4 ) 48 . 6 ( ± 0 . 3 ) 99 . 1 ( ± 0 . 1 ) 75 . 2 ( ± 0 . 7 ) T - test Average sorted runs 4 . 5 ( ± 0 . 1 ) 21 . 1 ( ± 0 . 2 ) 51 . 7 ( ± 0 . 5 ) 99 . 6 ( ± 0 . 1 ) 81 . 6 ( ± 0 . 6 ) estimator ˆ R k . Proof : We determine the minimum of var ( ˆ R k ) and show that ˆ R 2 realizes the minimum . By deﬁnition , var ( ˆ R k ) equals E ( ˆ R 2 k ) − E ( ˆ R k ) 2 . Since ˆ R k is un - biased , E ( ˆ R k ) = r so var ( ˆ R k ) = E ( ˆ R 2 k ) − r 2 = P e P ( e ) ˆ R 2 k ( e ) − r 2 . At the minimum , dvar ( ˆ R k ) / dk i , j = 0 for all 1 ≤ i < j ≤ n . Taking derivatives w . r . t . k a , b for any a , b such that ( a , b ) 6 = ( 1 , 2 ) gives dvar ( ˆ R k ) / dk a , b = d P e P ( e ) ˆ R 2 k ( e ) − r 2 / dk a , b which computes as P e P ( e ) 2 ˆ R k ( e ) ( d ˆ R k ( e ) / dk a , b ) . We can write ˆ R k = P 1 ≤ i < j ≤ n , j > 2 k i , j I ( e i = e j ) + k 1 , 2 I ( e 1 = e 2 ) and use ( 3 ) to write k 1 , 2 = 1 − P 1 ≤ i < j ≤ n , j > 2 k i , j , giv - ing ˆ R k = P 1 ≤ i < j ≤ n , j > 2 k i , j I ( e i = e j ) + ( 1 − P 1 ≤ i < j ≤ n , j > 2 k i , j ) I ( e 1 = e 2 ) . So , the term d ˆ R k ( e ) / dk a , b can be written as d P 1 ≤ i < j ≤ n , j > 2 k i , j I ( e i = e j ) + ( 1 − P 1 ≤ i < j ≤ n , j > 2 k i , j ) I ( e 1 = e 2 ) / dk a , b which equals I ( e a = e b ) − I ( e 1 = e 2 ) . So dvar ( ˆ R k ) / dk a , b is P e P ( e ) 2 ˆ R k ( e ) ( I ( e a = e b ) − I ( e 1 = e 2 ) ) . We need to distinguish two cases , namely a ≤ 2 and a > 2 . If a > 2 , dvar ( ˆ R k ) / dk a , b is P e P ( e ) 2 P 1 ≤ i < j ≤ n k i , j I ( e i = e j ) ( I ( e a = e b ) − I ( e 1 = e 2 ) ) reduces to 2 k a , b ( p ( 1 − p ) 3 + p 3 ( 1 − p ) ) − 2 k 1 , 2 ( p ( 1 − p ) 3 + p 3 ( 1 − p ) ) where p = 12 + 12 √ 2 r − 1 as before . For this to equal zero , we have p = 0 or p = 1 coinciding with replicability of r = 1 , or k a , b = k 1 , 2 . Likewise , if a ≤ 2 dvar ( ˆ R k ) / dk a , b reduces to 2 k a , b ( p ( 1 − p ) 2 + p 2 ( 1 − p ) ) − 2 k 1 , 2 ( p ( 1 − p ) 2 + p 2 ( 1 − p ) ) . And again , we have r = 1 or k a , b = k 1 , 2 . So , var ( ˆ R k ) reaches an optimum at k a , b = k 1 , 2 for all a , b , which means all coeﬃcients are equal . And since they sum to 1 , we have k a , b = 1 n ( n − 1 ) / 2 since there are n ( n − 1 ) / 2 coeﬃcients . The optimum is a minimum , as Figure 2 shows . In summary , Theorem 3 . 2 states that ˆ R 2 is indeed an eﬃcient ( i . e . unbiased with lowest variance ) estimator in the class of unbiased estimators ˆ R k . 4 . Empirical results First , we establish which sampling scheme results in acceptable experiments based on Type I error and power . Then , we look at factors that impact replica - bility . To measure Type I error and power , algorithm A ( naive Bayes [ 5 ] as implemented in Weka 3 . 3 [ 9 ] ) and algorithm B ( C4 . 5 [ 7 ] as implemented in Weka with default parameters ) were compared on synthetic data and UCI data sets . The synthetic data sets was generated using four data sources based on four ran - domly generated Bayesian networks ( [ 2 ] for more de - tails ) . The data sets contained 10 binary variables and 50 % class probability . Each of the data sources were used to generate 1000 data sets with 300 instances . Data source 1 has mutually independent variables , so there is no performance diﬀerence between naive Bayes and C4 . 5 , which allows us to measure the Type I error . For sources 2 , 3 and 4 , C4 . 5 outperforms naive Bayes with increasing margin ( on average 2 . 77 % , 5 . 83 % and 11 . 27 % respectively as measured on 10 . 000 instance test sets ) , which allows us to measure the power of tests . The sampling methods mentioned in Section 2 . 1 were performed 10 times with 10 folds and 10 runs at 5 % signiﬁcance level . Table 1 shows the results on the synthetic data with numbers in brackets indicating a 95 % conﬁdence inter - val . The ﬁrst six rows are for the rank sum test . Note that the use all data , average over folds and over runs sampling schemes have a Type I error over 50 % , while a 5 % Type I error is desired . The resampling scheme has an elevated Type I error as has the 10 fold cross validation scheme . Only the sorted runs scheme shows an appropriate level of Type I error . This comes at the Table 2 . UCI data sets . Nr of draws of sorted 10 x 10 fold cv ( α = 5 % , 95 % intervals for ˆ R 2 within ± 3 % ) 10 + 20 + Data set 123456789 01234567890 01234567 Mean norm . ˆ R 2 Sign test 84 . 4 NB vs C45 . . 9 . . . . . 2 . 4 . . 3 . . 27 . . . . . . . 5 . 78 . 2 NB vs NN . . . . . 96 . 9 . . . . 19 . 9 . . . . . . 6 . . . 84 . 6 C45 vs NN . . . 1 . 4 . . . . . . . . . . . . 5 . . . . . . . . 90 . 4 Rank test 90 . 2 NB vs C45 . . . . . . . . . . 4 . . 3 . . . 9 . . . . . . . 7 . 87 . 6 NB vs NN . . . . . . 8 . . . . . . . . . 9 . . . . . . 8 . . . 93 . 2 C45 vs NN . . . 2 . 6 . . . . . . . . . . . . 7 . . . . . . . . 90 . 0 T test 90 . 8 NB vs C45 . . . . . . . . . . 1 . . 2 . . . 9 . . . . . . . 7 . 91 . 0 NB vs NN . . . . . . 9 . . . . . . . . . 9 . . . . . . 7 . . . 93 . 6 C45 vs NN . . . 2 . 6 . . . . . . . . . . . . 6 . . . . . . . 9 88 . 0 cost of decreased power compared to most of the other schemes . Table 1 also shows the minimum of the average repli - cability over Set 1 to 4 . It shows that resampling and 10 - fold cv has a level of replicability which is not ac - ceptable ( below 50 % ) . The schemes based on repeated cross validation do show acceptable replicability . In particular , the sorted runs scheme has a replicability of over 80 % . Results for the sign test and t - test are similar to the results for the rank sum test . Table 1 also shows Type I error and power for sorted runs with sign test and t test . Those ﬁgures are very close to the ones for the rank sum test , taking in ac - count that a slightly higher Type I error should lead to slightly better power . The replicability for sorted runs with the sign test is 75 . 2 % and with the t - test is 81 . 6 % . Compared to the 80 . 6 % for the rank sum test , the sign test performs considerably worse . This can be explained by the lack of exploiting sizes of diﬀerences in the sample by the sign test . The replicability of the t - test is only slightly better . Further experiments were performed using the sorted runs sampling scheme while varying various parame - ters of the experiment , namely • signiﬁcance level ( 1 % , 2 . 5 % , 5 % and 10 % ) , • number of runs ( 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 and 100 ) , • class probability for binary data ( 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 and 0 . 5 ) , • class cardinality ( 2 , 3 and 4 ) , • diﬀerent pairs of algorithms ( out of Naive Bayes , C4 . 5 , nearest neighbor , tree augmented naive Bayes , decision stump and support vector ) . Though space limitations prevent us from presenting the complete set of outcomes here , we can report that the experiments resulted in a Type I error not exceed - ing the signiﬁcance level by more than 1 % with the sorted runs sampling scheme for all three tests con - sidered . Decreasing the class probability increased replicability . The explanation for this behavior can be found in realizing that learners tend to predict the majority class the more this class dominates the data . Increasing the number of runs consistently increased replicability . It appears that the sorted runs sampling scheme results in a sample for which the independence assumption is not heavily violated , so that no correc - tion in variance [ 6 ] or degrees of freedom [ 2 ] is required . Table 2 shows results for 27 data sets from the UCI repository [ 1 ] 1 using the sorted runs sampling scheme with the three diﬀerent types of tests . We compared naive Bayes , C4 . 5 and nearest neighbor ( NB , C4 . 5 and NN respectively in Table 2 ) . Each algorithm was run ten times . The middle three columns show the num - ber of times that the experiment decides that the null hypothesis is acceptable ( so algorithms perform equal on a data set ) as numbered in the footnote 1 . When the null hypothesis is 0 or 10 times accepted only a dot is shown , since both situations indicate perfect repli - cability . The ﬁrst observation is that replicability is an issue for non - synthetic data sets , and thus aﬀects many machine learning researchers . Further , the sign test performs much worse than the other two tests , while the t - test shows marginally higher replicability than the rank sum test . So , not only the sampling method , but also the hypothesis test has an impact on the replicability of the experiment . 1 1 : anneal , 2 : arrhythmia , 3 : audiology , 4 : autos , 5 : balance - scale , 6 : breast - cancer , 7 : credit - rating , 8 : ecoli , 9 : German credit , 10 : glass , 11 : heart - statlog , 12 : hepati - tis , 13 : horse - colic , 14 : Hungarian heart disease , 15 : iono - sphere , 16 : iris , 17 : labor , 18 : lymphography , 19 : pima - diabetes , 20 : primary - tumor , 21 : sonar , 22 : soybean , 23 : vehicle , 24 : vote , 25 : vowel , 26 : Wisconsin breast cancer , and 27 : zoo . 5 . Conclusions We deﬁned replicability of machine learning experi - ments in terms of probability . This has the beneﬁt that it allows for comparison over diﬀerent experimen - tal designs , unlike a previous rather ad hoc deﬁnition [ 2 ] . For example , replicability measured on n repeats of an experiment can be compared with replicability measure on 2 n repeats . Furthermore , threshold eﬀects present in the ad hoc deﬁnition are not present in our deﬁnition . The main theoretical result of this paper is the presen - tation of an estimator for replicability that was shown to be unbiased and which has the lowest variance in its class . Using this estimator , we gathered empirical data to gain new insights in how experimental designs inﬂuence replicability and found that the hypothesis test , the sampling scheme , and the class probability impact replicability . In our experiments , replicabil - ity consistently increased with sampling methods that draw more samples from the same data set . Replica - bility appears to be an issue both with synthetic data sets as well as with UCI data sets . This indicates that machine learning researcher and data analysts should be wary when interpreting experimental results . The main practical outcome of the experiments is that judged on replicability the sorted runs sampling scheme with the widely used t - test showed superior properties compared to the sign test and performed marginally better than the rank sum test . The sorted runs scheme is based on combining accuracy estimates in a way that produces a representative sample of ac - curacy diﬀerences of learning algorithms . Surprisingly , the sorted runs sampling schemes is the only scheme out of a set of popular schemes we considered that also showed acceptable Type I errors and reasonable power for a wide range of parameters using the three hypoth - esis tests considered . Consequently , experiments based on sorted runs sampling schemes do not require vari - ance corrections [ 6 ] or calibration of degrees of freedom [ 2 ] . In summary , based on replicability , Type I error , power and theoretical considerations , we recommend using the sorted runs sampling scheme with a t - test for comparing classiﬁers on a small data set . One would expect that replicability ceases to be an is - sue with larger data sets . In the future , we would like to perform larger scale experiments to get a better in - sight in the relation between replicability , the number of samples taken in an experiment and data set size . This should also give a better insight in the relation between replicability and Type I and II error . In this paper , we considered machine learning experi - ments in which we choose the best of two classiﬁers for a given data set . In practice , more than two classiﬁers are available . Also , machine learning researchers rou - tinely compare algorithms over a large number of data sets . This leads to new replicability issues and mul - tiple comparison problems , issues that require further research . Acknowledgements I would like to thank the Machine Learning Group of Waikato University for stimulating discussions and the anonymous reviewers for their helpful comments . A . Appendix Lemma A . 1 For 0 ≤ p ≤ 1 , and n ≥ 2 a positive integer , P ni = 0 p i ( 1 − p ) n − i ` ni ´ = 1 P ni = 0 p i ( 1 − p ) n − i ` ni ´ i = np P n i = 0 p i ( 1 − p ) n − i ` ni ´ i 2 = n 2 p 2 − np 2 + np Proof : ( sketch ) The ﬁrst equation is the binomial theorem [ 4 ] . The second follows from the observation that the term in sum is zero for i = 0 , so the range of the sum can be changed to 1 ≤ i ≤ n . Using ` ni ´ = ` n − 1 i − 1 ´ ni for i > 0 we can absorb the i at the end of the sum , and taking p outside the term in the summation , we can apply the binomial theorem . The third equation follows from a similar line of reasoning . References [ 1 ] C . L . Blake and C . J . Merz . UCI Repository of machine learning databases . Irvine , CA : University of Califor - nia , 1998 . [ 2 ] R . R . Bouckaert . Choosing between two learning algo - rithms based on calibrated tests . ICML , 51 – 58 , 2003 . [ 3 ] T . G . Dietterich . Approximate Statistical Tests for Comparing Supervised Classiﬁcation Learning Algo - rithms . Neural Computation , 10 ( 7 ) 1895 – 1924 , 1998 . [ 4 ] R . L . Graham , D . E . Knuth and O . Patashnik Concrete mathematics . Addison - Wesley , 1994 . [ 5 ] G . H . John and Pat Langley . Estimating Continuous Distributions in Bayesian Classiﬁers . UAI , 338 – 345 , 1995 . [ 6 ] C . Nadeau and Y . Bengio . Inference for the generaliza - tion error . Advances in Neural Information Processing Systems 12 , MIT Press , 2000 . [ 7 ] R . Quinlan . C4 . 5 : Programs for Machine Learning . Morgan Kaufmann Publishers , San Mateo , CA , 1993 . [ 8 ] S . Salzberg . On Comparing Classiﬁers : Pitfalls to Avoid and a Recommended Approach . Data Mining and Knowledge Discovery 1 : 3 , 317 - 327 , 1997 . [ 9 ] I . H . Witten and E . Frank . Data mining : Practical ma - chine learning tools and techniques with Java imple - mentations . Morgan Kaufmann , San Francisco , 2000 .