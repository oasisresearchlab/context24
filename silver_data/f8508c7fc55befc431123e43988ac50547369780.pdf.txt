a r X i v : 2401 . 10629v1 [ c s . H C ] 19 J a n 2024 A Critical Reflection on the Use of Toxicity Detection Algorithms in Proactive Content Moderation Systems MARK WARNER ‚àó , University College London , United Kingdom ANGELIKA STROHMAYER , Northumbria University , United Kingdom MATTHEW HIGGS , Independent Researcher , United Kingdom LYNNE COVENTRY , Abertay University , United Kingdom Toxicity detection algorithms , originally designed with reactive content moderation in mind , are increasingly being deployed into proactive end - user interventions to moderate content . Through a socio - technical lens and focusing on contexts in which they are applied , weexploretheuseof thesealgorithms inproactive moderation systems . Placinga toxicitydetection algorithm inan imagined virtual mobile keyboard , we critically explore how such algorithms could be used to proactively reduce the sending of toxic content . WepresentÔ¨Åndingsfromdesignworkshopsconducted withfourdistinctstakeholdergroups andÔ¨Åndconcernsaroundhowcontextual complexities may exasperate inequalities around content moderation processes . Whilst only speciÔ¨Åc user groups are likely to directly beneÔ¨Åt from these interventions , we highlight the potential for other groups to misuse them to circumvent detection , validate and gamify hate , and manipulate algorithmic models to exasperate harm . CCSConcepts : ‚Ä¢ Human - centered computing ‚Üí Empiricalstudies inHCI ; Keyboards ; Collaborativeand socialcomputingsystems and tools . Additional Key Words and Phrases : proactive moderation , moderation , hate speech , context , toxicity - detection ; abusability ACM Reference Format : MarkWarner , AngelikaStrohmayer , MatthewHiggs , andLynneCoventry . 0000 . ACriticalReÔ¨Çection on theUseof ToxicityDetection Algorithms in Proactive Content Moderation Systems . 1 , 1 ( January 0000 ) , 21 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION As people‚Äôs communications and interactions move increasingly online , societies have experienced increases in diÔ¨Äer - ent forms of online abuse . In 2019 , it was estimated that 30 - 40 % of people had been exposed to online abuse , and 10 - 20 % had been a direct target of abuse [ 90 ] . Exposure to toxic content can have a signiÔ¨Åcant impact on mental health and wellbeing of victims ( [ 22 , 29 , 42 ] ) , including depression and anxiety , as well as suicidal ideation [ 82 ] . The impact may also be felt by producers of online abuse , such as damage to identity and reputation [ 59 ] . Whilst some online abuse is created by organised groups in coordinated attacks on platforms like Reddit [ 65 , 66 ] , 4Chan [ 60 , 65 , 66 ] , Gab [ 99 ] , and Twitter ( now ‚ÄôX‚Äô , though we continue to use ‚ÄòTwitter‚Äô throughout this paper since the Ô¨Åeldwork was carried out prior to the name change ) [ 17 , 18 ] , other abuse is less organised . Research on regrettable messages posted on Facebook shows people posting ( and then regretting ) messages containing profanities , obscenities , and oÔ¨Äensive statements [ 94 ] . On Authors‚Äôaddresses : Mark Warner , mark . warner @ ucl . ac . uk , UniversityCollegeLondon , 169EustonRoad , London , United Kingdom , NW12AE ; Angelika Strohmayer , Northumbria University , Newcastle Upon Tyne , United Kingdom , angelika . strohmayer @ northumbria . ac . uk ; Matthew Higgs , Independent Researcher , United Kingdom , matthewm @ higgs . ac ; Lynne Coventry , Abertay University , Dundee , United Kingdom , l . coventry @ abertay . ac . uk . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not madeordistributed forproÔ¨Åtorcommercialadvantageandthat copiesbearthisnotice and thefull citation on the Ô¨Årstpage . Copyrightsforcomponents of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciÔ¨Åc permission and / or a fee . Request permissions from permissions @ acm . org . ¬© 0000 Association for Computing Machinery . Manuscript submitted to ACM Manuscript submitted to ACM 1 2 Warner et al . Twitter , similar behaviours have been identiÔ¨Åed , with peopleregretting posts containing direct attacks or criticism [ 79 ] , while other work Ô¨Ånds ordinary people engaging in trolling behaviour under certain conditions [ 19 ] . Several algorithmic approaches for detecting toxic speech have been developed and evaluated ( e . g . , [ 1 , 36 , 54 ] ) with these and similar algorithms used to support online content moderation on social media platforms . This approach to moderation has the beneÔ¨Åt of limiting human exposure to toxic content ( i . e . , to human moderators [ 81 ] ) , yet is fraught with ethical concerns related to fairness , bias , and freedom of speech [ 87 ] . An alternative approach to using these same or similar algorithms involves proactive moderation at the point of message creation to nudge senders into making better choices [ 55 ] . Similar nudging approaches have been explored to help protect users against inappropriate disclosures , through privacy nudges [ 92 , 93 ] , and to help prevent misinformation [ 46 ] . This proactive approach helps to address concerns related to freedom of speech and pushes the responsibility of dealing with the toxic content away from receivers and human moderators , and onto the sender . Moreover , it could allow for previously unmoderated spaces ( e . g . , WhatsApp ) to incorporate moderation despite the closed and secure nature of these platforms . Online social networks have started embedding these systems on their platforms [ 53 , 80 ] and research has started to explore the role of interaction design for delivering reÔ¨Çective prompts [ 51 , 89 ] . Yet despite this work , we still know very little about the broader challenges of embedding a toxicity - detection algorithm ( which we refer to as toxicity algorithms throughoutthe paper ) into socio - technical systems such as proactive moderation systems , and how tothink and engage across disciplines when designing such interventions . The work presented in this paper aims to explore whether and how an interface could proactively dissuade people from sending toxic content online by integrating an algorithmic intervention into a virtual mobile keyboard that could be used across diÔ¨Äerent platforms ( e . g . , instant messaging , and social media ) . We begin to address the aforementioned research gaps by reporting on the Ô¨Åndings from a series of design workshops where participants were asked to de - sign an imagined mobile phone keyboard intervention to respond to toxic content production at the point of writing . These workshops were conducted with four groups ( ùëÅ = 18 ) : ( 1 ) Ô¨Åve early career HCI researchers ( 2 ) academic and non - academic experts in cyberbullying and online hate ( including an expert from a UK - based hate charity ) , ( 3 ) online moderators , and ( 4 ) people who have previously sent toxic content which they later regretted sending . Our Ô¨Åndings highlight the complexity of integrating toxicity algorithms into proactive moderation systems . Prior work [ 14 , 38 ] highlights the diÔ¨Éculty of incorporating contextual factors into content moderation . It has also identiÔ¨Åed speciÔ¨Åc con - textual factors that may impact on moderation decisions [ 48 , 77 ] and how identiÔ¨Åed factors can impact on severity of harm [ 75 ] . We draw on this prior work to help us understand the role of context within a proactive moderation paradigm , and build on this work by contributing a holistic and user - centred understanding of the diÔ¨Äerent contextual factors around the proactive evaluation of toxicity within messages , which range from understanding the relationship between conversation partners , to social histories of oppression and power structures . Expanding on prior work [ 19 ] we explore end - users for which such an intervention may be eÔ¨Äective and why , allowing us to identify who we are designing for . We also build on previous work conducted within the reactive moderation paradigm by providing sup - port for these proactive moderation systems to be circumvented by users to avoid detection [ 15 , 33 , 37 , 47 ] . Our work expands on prior Ô¨Åndings related to system misuse by contributing new understanding into how proactive moderation systems could be used to validate and gamify hate and be used to manipulate the algorithmic models to exasperate harm . Drawing on our Ô¨Åndings and those from prior research , we discuss considerations for designing and developing these moderation systems , suggesting the need for true interdisciplinary work to address the contextual complexities that could result in inequalities , as well as harms through misuse of these systems . Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 3 2 RELATED WORK Proactive moderation systems are user - facing interventions that rely on users responding to prompts within the sys - tem . As such , we Ô¨Årst explore prior research looking at why people post toxic content online . As our research explores the design of systems that embed toxicity detection algorithms , we draw on both Machine Learning ( ML ) and Natural Language Processing ( NLP ) literature , focusing in particular on the role of context , and the embedding of inequality within toxicity detection models . We then explore HCI and design work related to interfaces designed to cause re - Ô¨Çection around the creation of toxic online content . Finally , as our work takes a critical perspective to highlight the potential unintended consequences of these systems , we explore literature on the abuse of algorithmic systems , and how algorithms can be used for multiple purposes ( both good , and bad ) . 2 . 1 Why do people post toxic content online ? Toxic speech 1 can be spread online in a very organised and coordinated manor ( see : [ 17 , 18 , 60 , 65 , 66 , 99 ] ) , with people acting either alone or as part of a group [ 32 ] . Research Ô¨Ånds those with a desire for social recognition , and those with heightened moral arousal towards the target and their behaviour , are more likely to participate in online Ô¨Årestorms [ 50 ] ( i . e . ‚Äúthe sudden discharge of large quantities of messages containing negative word of mouth and complaint behaviour against a person , company , or group in social media networks " [ 71 ] ) . Prior work suggests that people may use the actions of others to help justify their own behaviours . Blackwell and colleagues [ 10 ] explored this idea in a controlled experiment and found that when the target of harassment had committed an oÔ¨Äence , the harassment behaviour was felt to be more justiÔ¨Åed and deserved ( although not more appropriate ) . Phillips [ 72 ] explored the relationship between trolling and mainstream culture , making the case that we not only have a trolling problem , but we also have a culture problem due to the way in which exploitation is viewed as a leisure activity within many societies ; the internet has enabled trolls to organise , to mask their identities , and emotionally disassociate from the negative impact their behaviours have on those targeted . Less organised forms of toxic speech production also exist . In these instances , sending toxic messages may become a source of regret [ 79 , 94 , 95 ] . Wang et al . ‚Äôs [ 94 ] work on regretful messages on Facebook found people experiencing feelings of regret after posting comments containing foul or obscene language , as well as negative and oÔ¨Äensive com - ments . Often they reported these types of messages being posted during periods of anger and upset , suggesting a more hastily and less organised motivation for posting . Similarly , Sleeper et al . [ 79 ] identiÔ¨Åed types of messages that users tend to regret including messages that contain direct attacks and criticism . They also found users experiencing a neg - ative and highly emotional state , prior to sending these messages . This supports research that suggests , alongside the nature of the conversation , a negative mood can act as a primary trigger for trolling , and that ordinary people under certain circumstances can engage in this type of behaviour [ 19 ] . Whilst most work on regretful messages has been explored on social media , less work has focused on private instant messaging environments . The little work that has been done has identiÔ¨Åed people regretting and then deleting messages that the sender deemed inappropriate soon after sending [ 95 ] . This prior research also highlights how regrets can develop due to inappropriate disclosures , leading to privacy and information quality concerns . Proactive interventions have been explored to address these privacy related behaviours , with work focused on the eÔ¨Äectiveness of nudges [ 92 , 93 ] , Ô¨Ånding potential for these types of nudges to discourage unintended disclosures , and to reduce the sharing of misinformation [ 46 ] . 1 Harmfulspeech canincludehateful , oÔ¨Äensive , and abusivelanguage , amongothers . Toourknowledge , thereisno standarddeÔ¨Ånitionthatencapsulates these diÔ¨Äerences [ 23 , 39 , 96 ] . As such , we use the term ‚Äútoxic " to refer collectively to these diÔ¨Äerent forms . Where work has explored a speciÔ¨Åc type of speech , we will use that speciÔ¨Åc term . Manuscript submitted to ACM 4 Warner et al . 2 . 2 Toxicity interventions : from algorithm to interface Looking beyond peoples‚Äô reasons for producing toxic content , researchers in the areas of ML and NLP have developed algorithms for detecting diÔ¨Äerent forms of abusive content such as toxic text ( e . g . , [ 36 ] ) , hate speech ( e . g . , [ 54 ] ) , and cy - berbullying ( e . g . , [ 1 ] ) . Research has identiÔ¨Åed the importance of considering context when evaluating the harmfulness of online messages [ 11 ] . To help improve the accuracy of detection systems , researchers have lookedto incorporate spe - ciÔ¨Åc contextual factors into their models . These include the nature of the broader conversation [ 64 , 70 ] ) , the sender and broad topic [ 74 ] , diÔ¨Äerent dimensions of abuse ( e . g . , direct vs indirect ) [ 96 ] , and sarcasm [ 21 ] . However , these models are still unable to understand complex contextual nuances within speech [ 14 ] , yet are often used within industrial ap - proaches to content moderation where consistency is prioritised over context [ 14 ] . This can lead to miss - classiÔ¨Åcation of content , and the embedding of structural inequalities into content moderation algorithms that impact Black peo - ple [ 40 , 61 ] , trans people [ 26 , 40 ] , people with mental illness [ 33 ] , sex workers , or content producers that relate to nudity [ 3 , 4 ] , and others . Unsurprisingly perhaps , signiÔ¨Åcant ethical concerns have been raised around algorithms used for moderation , including concerns of fairness , discrimination , and freedom of expression [ 55 , 87 ] . Kiritchenko , Nejadgholi and Fraser [ 55 ] highlight several alternative algorithmic approaches to reducing toxic speech , most of which do not involve improving the underlying algorithms , but rather shift how models are used within systems . These include the use of algorithms within proactive moderation systems , to nudge users into making better choices ; importantly , this approach does not have to limit freedom of expression . Similar to this approach , sev - eral platforms have tested toxicity algorithms that use text - based nudges at the point of message creation , intending to reduce instances of abuse [ 53 , 80 ] . For example , a recent evaluation of Twitter‚Äôs implementation of such nudges suggests that , over a short period at least , their intervention is eÔ¨Äective at reducing oÔ¨Äensive Tweets ( 9 % deleted , 22 % revised ) [ 53 ] . Moreover , the reduction in oÔ¨Äensive Tweets due to the prompt intervention also resulted in a reduction in oÔ¨Äensive Tweets produced in reply to the initial Tweets , suggesting a downstream eÔ¨Äect can occur ; a Ô¨Ånding re - Ô¨Çected in Chang et al . ‚Äôs work [ 16 ] , and supported by the Ô¨Åndings from [ 19 ] . When designing interfaces to support proactive moderation , researchers have drawn from the literature on reÔ¨Çective interfaces [ 27 , 51 , 89 ] , and resonate with literature on reÔ¨Çective informatics [ 6 , 7 ] . For instance , Royen et al . , [ 89 ] explored diÔ¨Äerent interface and interac - tion design opportunities such as text - based prompts and time delays to understand how they may reduce instances of online harassment by adolescents . They found a reduction in participants‚Äô intention to engage in online harassment when presented with a prompt . Similarly , Jones [ 51 ] explored the potential of reÔ¨Çective interfaces in a Ô¨Åctitious so - cial networking website and found them helping to encourage users into self - corrective behaviour towards positive behavioural norms . Additional mechanisms may also have the potential to reduce incivility in discourse , supported by language models , with prior work highlighting the potential for humour to reduce incivility online [ 30 ] . More recent research has explored the use of algorithms in supporting moderators in proactively evaluating content , with Schluger et al . [ 76 ] reporting on a tool that supports the proactive moderation of the Wikipedia Talk Page ; which resulted in increased use of soft moderation practices . Other researchers have explored browser plug - ins to prompt users when engaged in messaging on the Reddit platform , with the system providing users with guidance on the potential eÔ¨Äect of their message on the civility of the conversation prior to the message being sent [ 16 ] . 2 . 3 Manipulation and abuse of algorithmic systems Systems that utilise algorithms to inform their operation have the potential to be manipulated or abused by users . Some algorithmic manipulations are complex and require signiÔ¨Åcant knowledge and skill to perform . For example , Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 5 data poisoning involves attacker ( s ) contributing large amounts of data to a machine learning model to change the way the model makes predictions , often causing the model to make bad predictions ( e . g . , causing non - toxic speech to be classiÔ¨Åed as toxic ) . Other forms of manipulation and abuse are less sophisticated . For example , Hastings et al . , [ 41 ] have highlighted the potential for students to manipulate the outcome of AI team formation software , allowing them to manipulate the system into grouping them with their friends . They do this by understanding the features the AI uses to cluster individuals into teams based on data collected through student surveys . Understanding the features of the system , students complete the surveys to Ô¨Åt the model . Similar behaviours have been observed around natural language processing models . For example , manipulating a system that is designed to detect gender from the written text by perturbing input text [ 73 ] , and similarly perturbing toxic text prior to submitting it to a toxicity detection model to trick the system into reporting it to be non - toxic [ 44 ] . These forms of manipulation of input text are often referred to as ‚Äúadversarial examples " [ 85 ] . Of course , manipulations like these are performed for diÔ¨Äering reasons , sometimes with good intentions , and sometimes with bad ‚Äì points we will return to in the Findings . In the case of gender recognition example , users likely manipulate the AI to prevent it from invading their privacy , as they prefer not to reveal this information . However , toxicity manipulation is likely to be used for more nefarious reasons such as to cause harm whilst evading automated moderation . Both technologies and their manipulation techniques can be subjected to what Hovy and Spruit [ 45 ] refer to as " dual - use " which describes a technology that can be used as it was intended to be used ( for good ) , whilst also being used in unintended ways , for bad . In summary , research is being conducted to develop more accurate models to predict toxic speech . We also highlight research into how interfaces could be designed to prompt users into making better choices when communicating . However , there is a paucity of research on the process of integrating toxicity algorithms into socio - technical systems , and the implications that this could have on user communication and potential risks they could expose users to . 3 METHOD This section details the virtual design workshop method that we used as part of our research , describing the design activities developed for probing and trigger discussion , rather than developing speciÔ¨Åc designs . Participant recruitment Our virtual design workshops were conducted with four diÔ¨Äerent communities and were facilitated by either the Ô¨Årst or second author , with author four participating where possible . These communities included : ( 1 ) Ô¨Åve early career HCI researchers living in the UK , all using participatory , co - creative , or user - centred design methodologies to develop gen - eral insights about how online moderation impacts users ; ( 2 ) four expert researchers and practitioners , both academic and non - academic , living across Europe , with a combined experience of 30 years working or researching in the area of cyberbullying , online hate , and other toxic forms of content online to ensure we are building on current best - practice expertise that broadly addresses issues of online toxicity ; ( 3 ) Ô¨Åve online moderators from the UK and US with a com - bined experience of 26 years moderating large online communities in Reddit , Discord , and Twitch to focus the research on the practicalities of what content moderation algorithms must consider ; and ( 4 ) four people living in the UK who have previously sent toxic content which they later regretted sending . This group acted as a key end user of any tool that may be developed from this work . To recruit group 1 ( researchers ) we advertised through internal university mailing lists and online discussion groups . For group 2 ( experts ) the authors drew from their existing network of contacts and also reached out to individuals iden - tiÔ¨Åed during an initial review of the literature . Group 3 ( moderators ) was recruited via a number of online moderator Manuscript submitted to ACM 6 Warner et al . Fig . 1 . Slidesused tosupportworkshop discussions . SlideAprovided anoverviewofthe workshop aims ; slide Bsupported asituation card sorting exercise ; slide C supported an activity to elicit feedback on the benefits and limitations of the system ; slide D supported participants in thinking and discussing the design of an intervention . discussion spaces . The Ô¨Årst author joined these groups and obtained permission from admins to advertise the study . Finally , group 4 participants ( producers ) were recruited via ProliÔ¨Åc . ac participant recruitment platform . We deployed an initial screening survey to identify individuals who met our inclusion criteria of having previously sent harmful content that they later regretted sending . All participants were remunerated for their time ( approximately ¬£10 / ph ) either with a shopping voucher or a charitable donation . Virtual design workshops In total , we conducted four virtual design workshops with each of the four diÔ¨Äerent groups . Each workshop included four to Ô¨Åve participants and lasted approximately two hours . Workshops have previously been used in research to explore related topics ( e . g . , [ 5 ] ‚Äôs work on cyberbullying ) and provide a means of engaging diÔ¨Äerent stakeholder groups in discussions related to a shared goal [ 68 ] . They also allow for a variety of activities to be conducted with partici - pants to facilitate a rich set of insights . We used this method to ( 1 ) explore and critique proactive content moderation interventions and ( 2 ) design potential future proactive content moderation interventions that responded to this initial critique . In this paper , we report on Ô¨Åndings from the former . The workshops were supported by a slide deck that included information about the study , the workshop aims , and guiding questions , as well as interactive activities for participants ( see Figure 1 for a preview of a number of the slides ) . During these activities , participants were invited Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 7 to collaborate using a resource made available to them via a shared PowerPoint slide deck [ 56 ] , and to discuss their thoughts . Ultimately , for this paper , it was the discussions that were prompted by the slides that we are working with . In the Ô¨Årst part of the workshop , we used a situation card approach [ 68 ] and a scale of harm ( Figure 1B ) to facilitate group discussion on the boundaries between diÔ¨Äerent types of content ( e . g . , hateful and hurtful content ) . Each card contained a problematic situation containing toxic speech , and participants were asked to discuss where they felt the situation should sit on the scale . These cards were based on real - world examples that the research team collated . This activity allowed us to explore the porous boundaries of language and legality . We then presented the algorithmic toxicity model to the participants and proposed the idea of integrating the model into a mobile keyboard to help reduce the production of toxic content . We prompted a discussion on the beneÔ¨Åts and limitations of this approach ( Figure 1C ) and recorded participant comments in two boxes so all participants could visualise and collaboratively add to them . The group discussions in this part of the workshop helped to lay the groundwork for nuanced discussion in the second part . In the second part of the workshop , we split participants into two breakout groups . There , we engaged them in a virtual collaborative prototyping activity [ 12 ] in which they were tasked with designing an imagined keyboard intervention to respond to toxic content production at the point of writing . We asked them to discuss their ideas , and to explain and reÔ¨Çect on their choices as they developed their ideas together . We provided them with a basic layout , and virtual drawing tools and ‚Äúprops " that they could use to support their ideas ( Figure 1D ) . Designs and discussions were then also discussed among the whole group , adding depth to the initial discussions in the breakout groups . This study was reviewed bothby [ hidden ] , and by [ hidden ] who funded this research . The main considerations made relate to the potential for participants to discuss issues that might be considered hateful or harmful , and for this to have a negative eÔ¨Äect on participants . To mitigate this , we instructed participants not to discuss personal experiences of abuse . At the end of each workshop session , all participants were sent links to a number of well - respected charities that support people who experience instances of bullying and abuse . Data analysis Each of the workshops produced three transcripts ( one for the main discussion , and one for each of the two breakout groups in the second part of the workshop ) . We used a collaborative constructionist thematic analysis on all tran - scripts , following Braun and Clarke‚Äôs [ 13 ] six phases . Initially , author two inductively coded the Ô¨Årst set of transcripts to develop an initial codebook that was visually represented on a shared online whiteboard . Following this , authors one , two , and three deductively coded the remaining transcripts using the shared whiteboard . This allowed for contin - uous collaborative discussion on how transcript data was being coded . Using a shared online whiteboard , all authors asynchronously developed and reviewed themes . Adding to this asynchronous analysis , all authors discussed theme development and used the online whiteboard to support them in deÔ¨Åning and naming themes before collectively pro - ducing a report . Drawing on Walny et al . ‚Äôs [ 91 ] learning about sketching in software development and Sturdee et al . ‚Äôs [ 83 ] sketching as future inquiry , we augmented our thematic analysis with the sketching of design ideas produced by participants . This allowed us to visualise potential futures , adding depth and insight into our thematic analysis . Our thematic analysis was reÔ¨Çexive and understands that we have constructed our below arguments from the data . As such , we do not present quantitative information on our codes or themes . Positionality Statement : All authors of this paper live and work in the UK . At the time of writing the piece , all re - searchers worked in academia , but across various diÔ¨Äerent areas of research . The team is made up of researchers from Manuscript submitted to ACM 8 Warner et al . across Ô¨Åelds of HCI ( one of us coming at this project from a Computing , another from a Psychology , and a third from a Design perspective ) and Data Science . We have varying perspectives on the topic of online toxicity , stemming from our gendered experiences of participating in online communities as well as our past research which focuses on a vari - ety of topics such as cybersecurity , post - digital safety , understandings of harm reduction among various groups that experience heightened unsafety online , and data sciences . 4 FINDINGS The Ô¨Åndings that we present in this paper relate tothree themes that were developed as part of ouranalysis that focused on the critique of these systems : ( 1 ) the diÔ¨Äerent contextual factors ( which explores diÔ¨Äerent contextual elements that relate to how toxicity is understood ) , and how they can help us design better algorithmic socio - technical systems ; ( 2 ) the diÔ¨Äerent end - user groups involved in the creation of toxic content , and ( 3 ) the ways such algorithmic systems could be abused or circumvented by diÔ¨Äerent user groups . 4 . 1 An exploration of ‚Äòcontextual factors‚Äô Discussions throughout our workshops related to how contextual factors relate to peoples‚Äô understanding of proactive content moderation . These were woven into many of the conversations that our participants had around proactive moderation systems while they were responding to the design prompts . Participants discussed how context might aÔ¨Äect how the system would work , and in turn how this would aÔ¨Äect users . During our analysis , we discussed various diÔ¨Äerent aspects related to severity of toxicity , how toxicity is understood , and how it should be proactively moderated , which relate to diÔ¨Äerent types of contextual factors . Below , we present nine diÔ¨Äerent elements our participants dis - cussed : starting from the technical infrastructure ( speciÔ¨Åc platforms ) , working through socio - technical and ultimately socio - cultural experiences , and then working our way towards socio - political issues such as histories of oppression and wider power structures . Platform culture and aÔ¨Äordances : Participants discussed a diversity of platforms such as Whatsapp , Twitter , Facebook , and Reddit . They pointed to these as having ‚ÄúdiÔ¨Äerent dynamic [ s ] ‚Äù ( producer 4 ) , not least because the purpose and user - base diÔ¨Äer across platforms . Participants pointed speciÔ¨Åcally to a diÔ¨Äerence in political conversations across diÔ¨Äerent platforms , which ‚Äúhave become incredibly tribal [ . . . especially on Twitter , where ] it is very , very diÔ¨Écult to just have a level - headed political conversation‚Äù ( producer 1 ) . Many of the discussions were entwined with the aÔ¨Äordances of platforms . For example , Twitter‚Äôs trending topics feature and recommendation algorithms would make it easier for transient trending topics to be hijacked and used to spread more subtle forms of hate . Moderator 4 described this in relation to a Twitter trend involving a comedian perpetuating transphobia that resulted in people using pictures of the comedian to target LGBTQ users ; they said : ‚Äúit‚Äôs almost impossible to be ahead of those trends [ . . . ] unless you‚Äôre like constantly looking at whatever is trending on Twitter " ( moderator 4 ) . Audience size : Participants described how " a big diÔ¨Äerentiator [ in the impact of toxic content online is ] who‚Äôs seeing it " ( moderator 3 ) . This can be exacerbated by the size of the audience who is likely to see the toxic content , here " a large audience could potentially be persuaded to that viewpoint‚Äù ( moderator 2 ) . Similarly , the type of audience that the toxic content has is important to consider : is the message targeted to an individual or a speciÔ¨Åc group ? These discussions were often had in relation to the type of platform the toxic content was being shared , with diÔ¨Äerences being considered between more private and intimate spaces ( e . g . , chat apps ) , and more public fora ( e . g . , Twitter ) . Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 9 Public vs Private : Further to the size of an audience , participants pointed to a diÔ¨Äerence between potentially toxic content that was available publicly and that which is visible only by individuals ( or even one individual ) in a private setting ( moderator 3 ) . One of the previous producers of toxic content linked this to the reaction that a message might cause : " If you were to send this [ toxic message ] only to this individual , then it‚Äôs sort of personal . But if it‚Äôs on a wider forum [ . . . ] then maybe you are starting towards hate " ( producer 4 ) . Finally , creating content in a public space means that the person is ‚Äúreiterating the message over and over " ( producer 3 ) , increasing its potential impact . In - group conversation : Participants across the workshops made it clear that they believe there is a big diÔ¨Äerence in a conversation and the language that is used between friends , people within a cultural or social in - group ( eg . LGBT + fora ) , a workplace , or culturalcommunity : ‚Äúboundaries will be diÔ¨Äerent in diÔ¨Äerent cultures and so how do you factor in all those diÔ¨Äerent variables to make something that that actually does the job for everybody ? " ( producer 1 ) . One participant made a distinction between friends and strangers : ‚Äúthe impact is not only dependent on the content of the message . If I say ‚Äòbitch‚Äô to my friends , he or she will not be oÔ¨Äended by that . But if I say ‚Äòbitch‚Äô to somebody I don‚Äôt know I think the impact will be much larger‚Äù ( expert 1 ) . Similarly , content moderators discussed the need for freedom to share toxic content to build in - group solidarity through humour , for example in an LGBT + forum . Yet , they also discussed the negative consequences of this approach if the same rule were applied to extremist groups ( e . g . , white supremacists ) . Frequency of messages : How frequently a message is repeated can impact greatly on the harm caused , " even if the single messages aren‚Äôt hateful‚Äù ( moderator 4 ) . This is due to a number of reasons , such as the accumulation of such messages during a pile - on on social media ( expert 4 ) , where there might be ‚Äú100 people sending 3 messages a month to the same person , that volume [ . . . ] makes it much more explicitly , uh , misogynist , hateful‚Äù ( moderator 1 ) . This leads us to the next point , that whether in private or public fora , people may be recruited into the sharing of toxic content . Where moderation was performed proactively based on the content being typed , there were concerns around the algorithms ability to consider prior messages within the broader context of the behaviour . Recruiting others into targeted attacks : A message may not be explicitly toxic , but it may be written in an attempt to recruit others into engaging in a " pile - on " ( expert 4 ) or targeted attack . One participant said that this would increase the toxicity of the message : " I think that‚Äôs far more painful because you‚Äôre not only carrying the hate yourself , you‚Äôre trying to recruit others into your hatred as well " ( producer 1 ) . Further to this recruitment into their toxic way of thinking , another participant explained how such actions can lead to attracting further abuse , turning into a targeted campaign that can snowball out of proportion ‚Äúbecause this stuÔ¨Ä is also then picked up by certain media outlets [ in a ] kind of regurgitation [ of ] the message , [ . . . ] essentially just making online abuse news . " ( expert 4 ) . History of the conversation : Conversations containing toxic language are not sent in vacuums , they have histories - both interpersonally and publicly . Looking towards people who know each other , for example : " Maybe in a direct argument people [ are ] having between each other and somebody just says something to hurt the other person to try and win the argument [ . . . ] particularly with something they might think will deÔ¨Ånitely , deÔ¨Ånitely sort of hurt them " ( producer 1 ) . Looking at more public debates , one participant pointed to a conversation with a history that may not involve the two speciÔ¨Åc individuals currently in conversation , but which is a longstanding public debate : ‚ÄúI‚Äôm thinking of the examples on debates about sex work in the UK . A lot of female politicians are very openly against sex work in a way that might bring in policies that actually make sex workers‚Äô lives harder . And therefore a very toxic kind of debate cycle ensues , so I can see people being scared about these policies coming forward and being very argumentative and almost insulting‚Äù ( expert 4 ) . Manuscript submitted to ACM 10 Warner et al . Social histories of oppression and power structures : Conversations and content shared online are part of our lived expe - rience , both in our oÔ¨Ñine and online developments . In turn , this means they do not exist in socio - political or power vacuums . What bell hooks [ 43 ] describes as ‚Äúthe capitalist imperialist white supremacist patriarchy " continues to shape our post - digital world - including the online content we share and how algorithms are trained to understand them . This means , as one of our participants points out , that Ô¨Çagging sentences like ‚Äúmen are trash‚Äù and ‚Äúwomen are trash‚Äù [ 97 ] in the same way does not acknowledge the relation of these sentences to ‚Äúcenturies of misogyny‚Äù ( expert 4 ) . Even if a message is not outwardly visibly harmful to some , ‚Äúthat message may still be hateful . That message may still be targeted or rooted in misogyny and in being hateful to women " ( moderator 2 ) . While our examples relate to misogyny in particular , these kinds of power structures relate to all axes of oppression such as race , sexuality , and disability . 4 . 1 . 1 Summary : Overall , participants expressed that simply using the textual and conversational context of toxic messages in algorithms is not enough . When we start to use toxicity algorithms in socio - technical systems such as proactive moderation systems , additional contextual factors shape what both interface and algorithm designers must consider . We do not believe that a single design can address each of these factors , but we should be conscious of them as and when systems are developed . We are far from the Ô¨Årst topoint outcontextual factors of content moderation [ 14 , 38 ] , or the Ô¨Årst to show that frequency of messages are an important component of severity of online hate [ 48 ] , or that in - the - world power structures shape online harms [ 77 ] . However , we do identify the presence of these contextual factors within a proactive moderation paradigm , and aggregate and arrange these factors to help add a new kind of interest in this area . 4 . 2 An end - user continuum of intention to send toxic messages Across the workshop activities participants spoke of how the eÔ¨Äectiveness of proactive moderation systems would diÔ¨Äer across diÔ¨Äerent user - groups due to the diÔ¨Äerent reasons people may engage in the sending of toxic content . Here , we present our analysis of how participants expressed who may be using proactive toxicity - reducing interventions , presented across an end - user continuum from those with a high level of intention to commit acts of harm , to those unaware they are harming others through their messages . We start from those users with a low intention to cause harm and increase the intentionality across the six groups . We hope that by presenting these various user groups , future interventions can become more speciÔ¨Åc to use - cases and users . The unaware : On the lower end of the continuum , participants talked about people who may be unaware of the lan - guage they are using when communicating . For instance , people ‚Äúaccidentally using dog whistles‚Äù they ‚Äúgenuinely don‚Äôt know about‚Äù ( moderator 3 ) . Participants discussed the potential for a proactive moderation system to be eÔ¨Äective in highlighting harm to those unaware that they are producing it , and providing a mechanism to supportthem in learning . For example , Producer 4 said : ‚ÄúSome people might not have the intuition to just realise it straight away because probably if they had they would have not written the message in the Ô¨Årst place‚Äù . Those wanting to learn : Participants recognised that " [ the keyboard ] is mostly targeted at people who , like , want to learn‚Äù ( moderator 4 ) . Especially for those unaware of the reason for their messages being considered hateful , participants highlight the importance of the intervention to act as a form of support , oÔ¨Äering not just prompts , but information to educate those to help them avoid engaging in this form of toxic speech production in the future . However , there was a recognition that this was not easy for people . Producer 3 spoke of their own journey , saying : ‚Äúthat‚Äôs something that Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 11 I‚Äôve had to learn , and that‚Äôs something that that takes real dedication and mindset change for an individual , and they have to want to better themselves as a person and develop themselves as a person‚Äù . The emotionally triggered : There was also a recognition amongst participants that people who do not regularly engage in toxic content production may send a toxic message after being emotionally triggered , such as when someone is ‚Äúan - gry‚Äù ( researcher 2 , expert 4 ) or sending something ‚Äúin the spur of the moment‚Äù ( producer 1 ) . Producer 3 again spoke of their own experience : ‚Äúif I was to say something hurtful to someone . It‚Äôs very likely that it‚Äôs almost like a rebound defence mechanism : ‚ÄòSomebody said something to me and therefore I want to say something back‚Äô . It‚Äôs that kind of mentality‚Äù . In these moments when people are emotionally triggered , they may be more susceptible to engaging in toxic speech production . However , there was a concern amongst participants that this behaviour may become a source of regret . Those using hate as a form of emotional arousal : As we move towards the higher end of the continuum , participants discussed people who may engage in these behaviours as ‚Äújust a way of having fun‚Äù ( expert 1 ) . Expert 1 again spoke of their experience in this area , having identiÔ¨Åed how ‚Äúboredom is often a trigger and then being aggressive or joining in bullying or cyberbullying is actually just the way of having fun . Or yeah , being stimulated or being excited " . Those playing to an audience : People may also act or play up to an audience , engaging in toxic speech production as a means of showing oÔ¨Ä . Moderator 4 talked about this explicitly , saying how : " once you add an audience , I think bad actors do weirder shit " . There were concerns that a proactive moderation system such as that built into a keyboard may make this ‚Äúacting up‚Äù behaviour worse , allowing those involved to highlight how harmful their communications are being in a more quantiÔ¨Åed way , which we come back to later in the paper . The determined and organised : Towards the extreme end of the continuum , participants talked about people who may change their behaviour but not their views and beliefs . For instance , researcher 5 said : " you might stop people from [ . . . ] sending these things , but it might not make people believe diÔ¨Äerently " There was a general view that it would be more diÔ¨Écult to change the mindset of more stubborn actors , especially : ‚Äúpolitical activists or [ those ] who are part of a certain community and do this in an organised way ( expert 1 ) . Similarly , participants were concerned about the eÔ¨Äectiveness of any intervention aimed at people involved in organised toxicity campaigns . Expert 4 said : " for people who actively set out to cause harm . [ . . . ] if you‚Äôre going there on your keyboard to just be hurtful or hateful or harmful getting that message [ . . . ] if anything , it might just spur you to do more " - an issue we come back to in the next section . Participants highlighted how those that are determined and organise would be unlikely to consider using interventions at all , and if they did were unlikely to change their behaviour , or may even use it to further their abuse . Summary : We know from prior work that both organised and coordinated groups [ 17 , 18 , 60 , 65 , 66 , 99 ] , as well as everyday people [ 19 ] engage in uncivil discourse , but we lack a more nuanced understanding of diÔ¨Äerent users that may engage in this form of behaviour , and how these groups may engage in proactive forms of moderation . The above outlines a continuum of potential users of a proactive moderation system that presents information from a toxicity algorithm . We feel it is important to state that people are likely to move between various diÔ¨Äerent aspects of the continuum , based on their purpose for engaging with others online - for example , a person may simultaneously want to learn about how to change their behaviour , while also having instances where they are emotionally triggered . We come back to this point in the implications for design . Throughout the continuum , there are several points where participants point towards potential abuses of such systems , which we expand on below . Manuscript submitted to ACM 12 Warner et al . 4 . 3 Abuse and manipulation of applications which embed toxicity models Despite the presumed positive impact of a keyboard that highlights toxicity levels of a message , such systems are rife with potential abuse - and may have a ‚Äúreverse eÔ¨Äect‚Äù ( moderator 2 ) than what was originally intended , for ex - ample , through ‚Äúvalidation‚Äù ( moderator 2 ) , ‚ÄúgamiÔ¨Åcation‚Äù ( moderator 4 ) , and people ‚Äúcircumventing‚Äù ( producer 4 ) or manipulating ( moderator 3 ) the system . All groups talked about the potential abuse of an intervention that presents algorithmically - determined scores or information about a degree of toxicity , similarly Jhaver and Chancellor [ 15 , 47 ] have explored various manipulation and gamiÔ¨Åcation practices among AI - based moderation tools . The ‚Äòabusability‚Äô our participants discussed expand these literatures as they link to the above end - user continuum , as it does not refer to all of these potential users equally . Similarly , it relates to the contextual factors as these may shape how people use the system . Participants highlighted features of the interface which could lead to reverse eÔ¨Äects , in particular : ‚Äúmaking sure you‚Äôre deÔ¨Ånitely not showing the percentages to gamify it‚Äù ( moderator 3 ) or showing any ‚Äúgradients‚Äù ( moderator 4 ) . One design choice discussed was ‚Äúnot giving you value judgements on every message you ever send . It‚Äôs just Ô¨Çagging prob - lems to you when they come up‚Äù ( moderator 4 ) . Ultimately though , this is not only an interface issue - our participants discussed repeatedly in all the workshops that the abusability of such systems directly relates to the toxicity detec - tion algorithms themselves which determine the kinds of features that can be implemented via design interventions , such as interfaces . Below , we outline four ways in which systems these systems could be abused : through validation , gamiÔ¨Åcation , model manipulation , and circumvention . Validation . Some participants pointed to how an intervention that provides toxicity feedback could be used to validate attacks . Moderator 2 , for example , used the insight about toxic content they have moderated to argue that people who want to cause harm online may interpret toxicity scores with the inverse reaction as intended . They may say : ‚ÄúI got validation , I‚Äôm being hateful‚Äù . Another moderator addressed how the design may facilitate an exploration of future hurtful content : ‚Äúnow you‚Äôve drawn attention to , possibly , a stereotype or insult‚Äù ( moderator 3 ) . Gamification . The embedding of toxicity algorithms into a proactive moderation system could also lead to a reverse eÔ¨Äect through gamiÔ¨Åcation . For example , moderator 4 pointed out : ‚ÄúPeople would take the most mundane things and turn [ them ] into [ a ] game " . They also pointed out that people who want to cause harm may interpret a ‚Äòtoxicity score‚Äô ( whether presented in percentages , colour - coding , or any other form of feedback ) as a competition : ‚ÄúWho can be the most hateful with a percentage score ? ‚Äù ( moderator 4 ) ; or as another moderator said to share their progress ‚Äúscreencap it and just say look how , how hateful I am‚Äù ( moderator 1 ) . Again , drawing on the experience of moderating online forums , moderator 4 even pointed out how people who want to cause harm may even ‚Äúrack up‚Äù their ‚Äúnegative internet points‚Äù or as producer 2 adds : " Some people would actually relish having that needle go up to the top . [ . . . ] They‚Äôre looking for that . Uh , almost conÔ¨Årms to them that they‚Äôre accomplishing , what they want to do inside themselves ‚Äù . Model manipulation . Participants spoke about ways in which they themselves and their communities could help to improve a toxicity detection model , making it more accurate by better understanding language and norms within their communities . Moderator 3 who had some high - level understanding of how machine learning models worked , talked about allowing communities to help train a model by highlighting words and sentences that the model should not Ô¨Çag as being hate . She said : I think if at [ . . ] the subreddit level of r / [ anonymised ] [ I ] was able to see a list of all the stuÔ¨Ä that gets Ô¨Çagged and we could say " like , yeah , that‚Äôs not actually hate speech that‚Äôs not hate speech , that‚Äôs not hate speech " . I think that would probably be useful feedback for like the whole model " . However , as part of the same discussion moderator 3 and others highlighted the potential downsides to this type of approach due to the potential for this to become an Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 13 avenue for abuse . Again , moderator 3 said " you would also open it up to bad actors intentionally providing it bad data [ . . ] and communities will just go through and Ô¨Çag everything they don‚Äôt agree as not hateful " . Circumvention . Furthermore , participants pointed to the potential users on the ‚Äòdetermined and organised‚Äô side of the end - user continuum , and how they are likely to Ô¨Ånd ways of circumventing the system . Producer 4 puts this matter - of - factly : " Obviously , apart from the obvious that people who want to cause harm and hurt , they would just circumnavigate it or or they would not have this keyboard " . The experts focused more on the impacts of toxicity detection algorithms if they were implemented in a way that people were forced to use them : ‚ÄúHow far can I push the system ? What kind of combinations of things can I say that maybe go undetected ? Or how can I just ? I don‚Äôt know Ô¨Ånd another way to say something hurtful in a way that just the system doesn‚Äôt recognise it " ( expert 2 ) . Expert 5 expands on this , adding that this behaviour may only arise after a user becomes frustrated with the platform ( which points to a broader group of people on the end - user continuum than just those who are determined ) : " if they start to get frustrated at the fact you know you keep warning them and highlighting things in their message , they might then go to just send a voice note instead‚Äù . 4 . 3 . 1 Summary . We highlight several ways a toxicity detection algorithm ( embedded into a proactive moderation system ) could be abused by end - users and lead to the opposite impact than intended or have the reverse eÔ¨Äect for which it was designed . As with any intervention , such potential negative outcomes need to be considered and ( ideally ) mitigated in design ; and measures should be put in place to allow for changes post - deployment based on user feedback . 5 DISCUSSION Above , we have presented ourÔ¨Åndings systematically in a way that researchers and workers whodesign and implement proactive toxicity moderation tools can make use of them . We presented context factors , the continuum of end - users , and ways in which tools may be abused and / or manipulated . We argue that with this collected knowledge , we will be able to design more nuanced , context - and community - speciÔ¨Åc interventions that take into consideration at least some aspects of abusability ; to work with a harm reduction model . Below , we present three points of discussion before presenting considerations for designing proactive moderation systems that embed toxicity algorithms . 5 . 1 Algorithmic inequality and the importance of context We know from prior work how inequitable many of the existing social media content moderation processes are [ 40 , 61 ] . This inequality results in certain groups experiencing higher instances of content removal and account restriction [ 40 ] . Applying moderation at the point of sending can help to alleviate some of these issues , as it aÔ¨Äords users with increased transparency around what content might be considered toxic or inappropriate prior to sending [ 55 ] . This allows users to curate their content to reduce the risk of it being subjected to unjust content moderation practices . In exploring as - pects of context around these systems , many of the concerns related to context were woven into concerns of inequality . Equality and representation in content moderation systems have been explored by Vaccaro et al . [ 88 ] who identiÔ¨Åed the need for systems to account for local diÔ¨Äerences or norms and behaviours when evaluating text toxicity , supporting our Ô¨Åndings related to contextual consideration around in - group conversations . Whilst proactive moderation helps to address issues of representation and contestability [ 88 ] by making moderation more transparent by being embedded within the user interaction , participants rarely considered this , and instead focused on the injustice felt if the system was to prompt them incorrectly , treating them ( and their communities ) unfairly . While these systems may not explic - itly curtail freedoms of expression by restricting what a person can , or cannot say , there were concerns of algorithmic Manuscript submitted to ACM 14 Warner et al . conformity [ 58 ] occurring that could indirectly curtail these freedoms [ 33 ] . This involves individuals and groups ad - justing their behaviours to conform to what the algorithm considers to be good and that this could be made worse through abusive model manipulation . Highlighting the duality of these systems , algorithmic conformity is a desirable outcome of the system for those committing acts of harm by producing toxic speech , yet wholly undesirable for those marginalised groups whose speech is being inappropriately Ô¨Çagged as toxic as a result of the algorithm being unable to consider the many contextual complexities that we highlight as part of our research . 5 . 2 Negotiate public good with personal regret Integrating a toxicity algorithm into socio - technical proactive moderation systems has some individual beneÔ¨Åts . For instance , it allows users with low intention to commit acts of harm ( e . g . , the unaware ) to become aware of the unin - tentional harm embedded within their message . This would allow them to make a more informed choice and would help to reduce instances of regretful messaging [ 79 , 94 , 95 ] . Moreover , it re - enforces intentionality within the message and limits a person‚Äôs ability to claim ignorance . Yet , this same mechanism also has the potential to exasperate harm where both the sender and receiver are aware that messages are being moderated prior to being sent . Here , the system is likely to generate what Donath refers to as a ‚Äòsignal‚Äô [ 28 ] , signalling a higher intention to cause harm where the sender has been notiÔ¨Åed of the toxicity in their messages and sent it regardless . Our work also highlights user groups ( e . g . , determined and organised users ) for which this moderation approach is unlikely to have any direct eÔ¨Äect , but may have indirect eÔ¨Äects such as changes in community - level norms [ 19 , 78 ] . We have seen some early evidence of this from Twitter‚Äôs embedded intervention where they identiÔ¨Åed fewer toxic tweets being created as a result of their intervention prompt ( direct eÔ¨Äect ) , as well as fewer replies to Tweets that received a prompt ( indirect eÔ¨Äect ) [ 53 ] . Similarly , Cheng [ 19 ] found exposure to trolling behaviours online can act as a primary trolling trigger . Yet what we do not yet know is how platform norms may reshape as a result of these interventions , and whether the knowledge by both the sender and receiver that these interventions are in place , will result in broader changes in behaviour . Of course , there remains some ambiguity between the sender and receiver as to whether the sender‚Äôs message met the algorithmic threshold to cause a toxicity prompt . However , we know from prior work how people develop mental models ( or " folk theories " ) around the functionality and capabilities of algorithms in online social environments [ 24 , 25 ] and change their behaviour as a result [ 52 ] . People may therefore start to loosely predict ( or test ) when a message has met the threshold for being considered toxic . If the intervention is embedded into the platform itself as opposed to being a standalone keyboard ( e . g . , [ 53 , 80 ] ) , a Ô¨Çag that a message is toxic could be embedded within the sent message . Drawing on Erickson and Kellogg‚Äôs concept of socially translucent systems [ 31 ] , this would enhance visibility , awareness , and accountability within the system by reducing ambiguity within the communication to foster community - level norm change and social learning . However , we must also be mindful of the potential for this type of system to act as a signal that could be exploited to exasperate harm . 5 . 3 Circumvention , manipulation , and abuse of proactive moderation systems Drawing on our Ô¨Åndings and those from prior research , it is clear there is potential for proactive moderation systems to act as interventions to supportpeople‚Äôs self - presentation and reduce toxicity . For example , helping to reduce the risk of peopleposting and sending something that they later regret that might have otherwise harmed their online identity [ 79 , 94 , 95 ] , and harmed receivers of that content . However , our work also highlights a number of potential abuse and manipulations of these systems which would be critical to consider when thinking about their design . Supporting prior work in HCI [ 15 , 33 , 37 , 47 ] we Ô¨Ånd concerns related to the circumvention of these moderation systems , through the Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 15 purposeful manipulation of content . NLP researchers refer to these manipulative inputs as adversarial examples . They typically perturbtheinput intothe modelwiththe intention ofcausing themodeltomake an incorrect classiÔ¨Åcation [ 44 , 49 , 57 , 85 ] . Our participants alsoraised concerns related tomodelmanipulation where users are able toprovide feedback to a model in a way that manipulates how it behaves . Prior research has explored this form of model manipulation around Twitter‚Äôs ‚Äútrending " algorithm showing how compromised and fake accounts could work to manipulate this platform feature [ 100 ] . Other forms of manipulation have also been observed in relation to user content ‚ÄòÔ¨Çagging‚Äô where users are able to Ô¨Çag a piece of content ( e . g . , Ô¨Çag a post as abusive ) . Thomas et al . [ 86 ] refer to the manipulation of these features as ‚ÄòfalsiÔ¨Åed abuse Ô¨Çagging‚Äô and can result in a moderation system incorrectly removing content that it considers abusive as a result of false Ô¨Çags that have been sent . Fiore - Silfvast [ 34 ] uses more militaristic language to describe group actions to remove content , describing these behaviours as ‚Äòuser - generated warfare‚Äô . These behaviours have been observed in the world [ 62 ] with activists using Ô¨Çagging functions to close porn performers‚Äô Instagram accounts [ 20 ] , and provide political groups with a means to remove their rival‚Äôs Facebook content and restrict access to their account [ 98 ] . These highlight the extent to which certain groups ( or ‚Äòend - users‚Äô ) will go to misuse systems for their own harmful intentions . Extending this prior work , we identify further ways in which proactive moderation systems could be abused , and in doing so we highlight their potential to exasperate harms . Like many technologies that are designed with good intentions , there often exists the possibility for technologies to be misused to cause harm . Hovy and Spruit [ 45 ] in their position paper on the social impact of NLP systems highlight the need to be mindful of ‚Äòdual use‚Äô of systems and the negative social impact NLP systems could have through their use . Distinct from prior literature , we oÔ¨Äer new insights into how proactive moderation systems could be misused to validate hateful comments and to facilitate harmful forms of gamiÔ¨Åcation . Within an educational context , Andrade et al [ 2 ] highlight the risk of undesired competition as users feel forced to engage in competition with peers . Similar concerns have been highlighted by developers and designers of social VR spaces , concerned that gamiÔ¨Åed or incentive systems may be ‚Äúcorrosive to community " by creating a culture of competition and envy [ 63 ] . Our work resonates with these concerns , whilst also pointing to a form of inversion of the intended goal of proactive moderation systems . We highlight the potential for certain end - users to appropriate these systems , gameifying them to amplify and validate their abuse . 5 . 4 Considerations for designing proactive moderation systems that embed toxicity algorithms Below , we present three considerations for working in HCI - ML collaborations . We understand that some of the con - siderations are more design orientated while others are more targeted towards ML or NLP developers . This was a conscious choice , since the work of these two end - users are deeply intertwined ( both in academia and industry ) , and increasingly must collaborate . Ultimately , we argue for increased , respectful , and genuine collaboration between the two disciplines . 5 . 4 . 1 Presenting the output of the model . Our Ô¨Åndings suggest the need to consider how the output of the deployed ML model is being integrated into the front - end design of the system . Misuse through gamiÔ¨Åcation of a system could be reduced where the output of the ML model is abstracted into a single dimension ( e . g . , toxic or not toxic ) as opposed to providing a more granular output ( e . g . , 70 % toxic ) . Moreover , our work also suggests the need to consider how the output of the model is described to help reduce the risk of the system being used to validate hateful attacks . One ap - proach could be to introduce an element of ambiguity of information into the system [ 35 ] . For instance , rather than describe the output as being ‚Äòtoxic‚Äô , describe it as being ‚Äòinappropriate‚Äô . It is important to highlight here the trade - oÔ¨Ä Manuscript submitted to ACM 16 Warner et al . between transparency and reducing the risk of exasperating harm . While there is generally a push towards increasing transparency and explainability of AI systems more generally ( e . g . , [ 9 ] as well as in AI systems that support mod - eration [ 84 ] , we should also consider how increased transparency impacts risks of misuse . For example , increasing transparency within proactive moderation systems that rely on the same algorithm used for post - publication modera - tion may provide users with a tool to curate and evaluate the toxicity of messages prior to sending , to avoid automated Ô¨Çagging of content by post - publication moderation systems . 5 . 4 . 2 Building feedback into the system . Prior work suggests that where users are able to provide feedback on the decision of an automated moderation system , this can increase trust through an increased sense of user - agency [ 67 ] . Our Ô¨Åndings resonate with this , with our participants ( and their communities ) wanting some involvement in the process of training the model , and in improving the accuracy of the underlying toxicity detection model . This approach could help to address some of the concerns related to contextual understanding , and local language norms . However , we identiÔ¨Åed a concern that any such system could itself be misused as has been previously discussed in relation to ‚Äòuser - generated warfare‚Äô [ 4 , 20 , 34 , 86 ] . This could result in an increase in false positives related to posts of already marginalised users [ 8 , 69 ] . One way of mitigating this type of misuse would be to deploy individual or community - level models that are reÔ¨Åned for individuals or communities . This would allow the user to Ô¨Çag false positives to help the accuracy of the model in a way that is bespoke tothem , as opposedtoone thatimpacts broader model predictions across an entire platform . Yet , this type of approach may act as a double - edged sword . While this type of model customisation ( either at the individual or community level ) may be ‚Äògood‚Äô for some communities ( e . g . , preventing posts discussing abusive behaviour in an abuse supportfora being Ô¨Çagged as toxic ) , as discussed by our participants it could be abused to exasperate harms that manifest in problematic communities ( e . g . , to legitimatise racism in online fringe communities ) . 5 . 4 . 3 Interdisciplinaryworkingfrom modeltoimplementation . Drawing on the literature we outline earlier in the paper , we understand that ML and NLP experts‚Äô assumptions for better hate - detection and reduction systems often relate to having enough data , the right kind of data , or even the right labelling of this data [ 70 , 96 ] . Counter to this , hate - content reduction systems that draw primarily on design and HCI literatures rely on the assumption that what is required is the ‚Äòright‚Äô interface , data - representation , or behaviour change system to do this [ 51 , 89 ] . It should be without saying , that both of these issues should be addressed when implementing new models as part of toxicity - detection algorithms in proactive content moderation systems : they are both a ML and / or NLP and an HCI issue . As such , we argue that it is important to also reÔ¨Çect on the design process of creating interactive systems that turn toxicity algorithms into socio - technical systems . When bringing these two audiences and problems into conversation with one another , and presenting this as a potential system to a variety of potential users of such systems as we have done , we learn that this becomes not only a socio - technical but also a socio - political issue ; where hate - detection and reduction systems are embedded in messy contexts ( as we outline in our Ô¨Ånding on contextual factor ) which relate to people in diÔ¨Äerent ways ( as we expand on in the end - user continuum ) , and one which lays bare multiple misuses of the system which can lead to increased harm in some instances ( as we lay out in our section on the abusability of such systems ) . Considering the contextual complexities highlighted in this work , it is unlikely that any one discipline can resolve the socio - politicaland socio - technical issues highlighted . Therefore , we argue for the need for ML and NLP researchers and Design and HCI researchers to genuinely work together to create systems that are greater than the sum of their parts . When designing any socio - technical system that utilises an algorithm , careful consideration should be made towards how multidisciplinary teams work together to develop solutions . For instance , care should be taken to avoid adding unnecessary constraints to design and development processes . As an example , some machine learning and NLP Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 17 models are developed before the problem space and user needs are fully understood . E . g . , where ML or NLP experts Ô¨Årst develop an algorithm that is then later implemented into systems by designers and software developers . This model of development ( algorithm Ô¨Årst , design second ; or the use of resources such as perspective API to pre - build systems ) can constrain front - end developers and interaction designers by forcing them to develop user interfaces and experiences that conform to both the input and output requirements of the developed algorithmic model . We do not say here that designers are always or entirely constrained by using only existing models . Wizard of Oz testing , Design Fictions , and other design methods allow us to explore potential new design - led avenues for proactice content moderation systems and toxicity detection algorithms . However , when implementing systems in - the - world , we rarely see processes that are truly iterative at both the ML and Design stages . As such , we argue for more genuine interdisciplinary ( or at least iterative multidisciplinary ) working : where HCI and ML / NLP experts work together prior to a model being developed , so considerations can be made as to how models are trained , the type of data used to train them , and approaches for labelling of data that better reÔ¨Çect the end - users that the system is designed to support . 6 LIMITATIONS AND FUTURE WORK While we believe our Ô¨Åndings are useful for researchers working across and at intersections of HCI and and Data Sci - ence , we also understand that the Ô¨Åndings will have limitations . What we present here is an analysis of the experiences and expertise that our participants have shared with us , we do not present neither speciÔ¨Åc design choices nor speciÔ¨Åc designs that participants created during the workshops . While there are concerns about harmful misuse of proactive moderation systems through gamiÔ¨Åcation and validation of hate , it would be important to understand the extent this might occur . As part of a broader project , we have developed a test social media and instant messaging environment to help us understand how users respond to moderation systems that provide granular model feedback ( e . g . , " Your message is 70 % toxic " ) , vs more abstracted feedback ( e . g . , " Your message is toxic " ) ; and how the intervention timing might impact both the eÔ¨Äectiveness of the intervention and the potential for misuse by exploring moderation prompts during typing and prior to sending ( i . e . , when the user presses ‚Äúsend " ) [ under review - CHI24 ] . 7 CONCLUSIONS While changes that aim to reduce toxicity online are often well intended , our research points towards several ways in which they could be misused . We oÔ¨Äer a critique of automated toxicity - detection systems , highlighting their potential for misuse by certain groups , for example through their potential to validate and gamify hateful attacks . We also high - light less explicit harms , including the potential for users to adapt their behaviours to conform to what they believe the algorithm considers to be good , resulting in algorithmic conformity which could indirectly curtail freedoms of speech . As such , we provide considerations for the designing of socio - technical systems that embed toxicity algorithms . Ulti - mately , we point to the need for truly multidisciplinary teams that work in interdisciplinary ways where algorithmic and HCI expertise work together to develop solutions which lessen potential negative impacts and misuse of these sys - tems , while also improving the socio - technically situated positive and harm - reducing impacts that are potentially made possible with automated toxicity - detection algorithms when embedded meaningfully into socio - technical systems . ACKNOWLEDGMENTS This research was supportedby UKRI through REPHRAIN ( EP / V011189 / 1 ) , the UK‚Äôs Research Centre on Privacy , Harm Reduction and Adversarial InÔ¨Çuence Online . Manuscript submitted to ACM 18 Warner et al . REFERENCES [ 1 ] Sweta Agrawal and Amit Awekar . 2018 . Deep learning for detecting cyberbullyingacrossmultiple social media platforms . In European conference on information retrieval . Springer , 141 ‚Äì 153 . [ 2 ] Fernando RH Andrade , Riichiro Mizoguchi , and Seiji Isotani . 2016 . The bright and dark sides of gamiÔ¨Åcation . In International conference on intelligent tutoring systems . Springer , 176 ‚Äì 186 . [ 3 ] Carolina Are . 2021 . The Shadowban Cycle : an autoethnography of pole dancing , nudity and censorship on Instagram . Feminist Media Studies ( 2021 ) , 1 ‚Äì 18 . [ 4 ] Carolina Are . 2022 . An autoethnography of automated powerlessness : lacking platform aÔ¨Äordances in Instagram and TikTok account deletions . Media , Culture & Society ( 2022 ) . [ 5 ] Zahra Ashktorab and Jessica Vitak . 2016 . Designing cyberbullying mitigation and prevention solutions through participatory design with teenagers . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 3895 ‚Äì 3905 . [ 6 ] Eric PS Baumer . 2015 . ReÔ¨Çective informatics : conceptual dimensions for designing technologies of reÔ¨Çection . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 585 ‚Äì 594 . [ 7 ] Eric PS Baumer , Vera Khovanskaya , Mark Matthews , Lindsay Reynolds , Victoria Schwanda Sosik , and Geri Gay . 2014 . Reviewing reÔ¨Çection : on the use of reÔ¨Çection in interactive system design . In Proceedings of the 2014 conference on Designing interactive systems . 93 ‚Äì 102 . [ 8 ] EmilyMBender , TimnitGebru , Angelina McMillan - Major , and ShmargaretShmitchell . 2021 . OntheDangersof StochasticParrots : CanLanguage Models Be Too Big ? . In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency . 610 ‚Äì 623 . [ 9 ] Reuben Binns , Max Van Kleek , Michael Veale , Ulrik Lyngs , Jun Zhao , and Nigel Shadbolt . 2018 . ‚ÄôIt‚Äôs Reducing a Human Being to a Percentage‚Äô Perceptions of Justice in Algorithmic Decisions . In Proceedings of the 2018 Chi conference on human factors in computing systems . 1 ‚Äì 14 . [ 10 ] Lindsay Blackwell , Tianying Chen , Sarita Schoenebeck , and CliÔ¨Ä Lampe . 2018 . When online harassment is perceived as justiÔ¨Åed . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 12 . [ 11 ] Lindsay Blackwell , Mark Handel , Sarah T . Roberts , Amy Bruckman , and Kimberly Voll . 2018 . Understanding ‚ÄúBad Actors‚Äù Online . In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI EA ‚Äô18 ) . Association for Computing Machinery , New York , NY , USA , 1 ‚Äì 7 . https : / / doi . org / 10 . 1145 / 3170427 . 3170610 [ 12 ] Susanne B√∏dker , Christian Dindler , Ole S Iversen , and Rachel C Smith . 2022 . What Are the Activities and Methods of Participatory Design ? In Participatory Design . Springer , 49 ‚Äì 64 . [ 13 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative research in psychology 3 , 2 ( 2006 ) , 77 ‚Äì 101 . [ 14 ] Robyn Caplan . 2018 . Content or context moderation ? ( 2018 ) . [ 15 ] Stevie Chancellor , JessicaAnnette Pater , Trustin Clear , Eric Gilbert , and Munmun De Choudhury . 2016 . # thyghgapp : Instagram content modera - tion and lexical variation in pro - eating disorder communities . In Proceedings of the 19th ACM conference on computer - supported cooperative work & social computing . 1201 ‚Äì 1213 . [ 16 ] Jonathan P Chang , Charlotte Schluger , and Cristian Danescu - Niculescu - Mizil . 2022 . Thread With Caution : Proactively Helping Users Assess and Deescalate Tension in Their Online Discussions . Proceedings of the ACM on Human - Computer Interaction 6 , CSCW2 ( 2022 ) , 1 ‚Äì 37 . [ 17 ] Despoina Chatzakou , Nicolas Kourtellis , Jeremy Blackburn , Emiliano De Cristofaro , Gianluca Stringhini , and Athena Vakali . 2017 . Hate is not binary : Studying abusive behavior of # gamergate on twitter . In Proceedings of the 28th ACM conference on hypertext and social media . 65 ‚Äì 74 . [ 18 ] Despoina Chatzakou , Nicolas Kourtellis , Jeremy Blackburn , Emiliano De Cristofaro , Gianluca Stringhini , and Athena Vakali . 2017 . Mean birds : Detecting aggression and bullying on twitter . In Proceedings of the 2017 ACM on web science conference . 13 ‚Äì 22 . [ 19 ] Justin Cheng , Michael Bernstein , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2017 . Anyone can become a troll : Causes of trolling behavior in online discussions . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing . 1217 ‚Äì 1230 . [ 20 ] Tracy Clark - Flory . 2019 . A Troll‚Äôs Alleged Attempt to Purge Porn Performers from Instagram . https : / / jezebel . com / a - trolls - alleged - attempt - to - purge - porn - performers - from - 1833940198 [ 21 ] DDavidov , OTsur , and A Rappoport . 2010 . Semi - supervisedrecognition of sarcasticsentences in Twitterand Amazon ( pp . 15 ‚Äì 16 ) . Retrieved from Association for Computational Linguistics website : https : / / www . aclweb . org / anthology / W10 - 2914 . pdf ( 2010 ) . [ 22 ] Julia Davidson , Sonia Livingstone , Sam Jenkins , Anna Gekoski , Clare Choak , Tarela Ike , and Kirsty Phillips . 2019 . Adult online hate , harassment and abuse : a rapid evidence assessment . ( 2019 ) . [ 23 ] Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017 . Automated hate speech detection and the problem of oÔ¨Äensive language . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 11 . 512 ‚Äì 515 . [ 24 ] MichaelA DeVito , JeremyBirnholtz , JeÔ¨ÄeryTHancock , MeganFrench , and Sunny Liu . 2018 . How people form folk theories of socialmedia feeds and what it means for how we study self - presentation . In Proceedings of the 2018 CHI conference on human factors in computing systems . 1 ‚Äì 12 . [ 25 ] Michael A DeVito , Darren Gergle , and Jeremy Birnholtz . 2017 . " Algorithms ruin everything " # RIPTwitter , Folk Theories , and Resistance to Algorithmic Change in Social Media . In Proceedings of the 2017 CHI conference on human factors in computing systems . 3163 ‚Äì 3174 . [ 26 ] ThiagoDiasOliva , DennysMarceloAntonialli , andAlessandraGomes . 2021 . Fighting HateSpeech , Silencing DragQueens ? ArtiÔ¨ÅcialIntelligence in Content Moderation and Risks to LGBTQ Voices Online . Sexuality & Culture 25 , 2 ( 2021 ) , 700 ‚Äì 732 . Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 19 [ 27 ] KarthikDinakar , BiragoJones , CatherineHavasi , HenryLieberman , andRosalindPicard . 2012 . Commonsensereasoningfordetection , prevention , and mitigation of cyberbullying . ACM Transactions on Interactive Intelligent Systems ( TiiS ) 2 , 3 ( 2012 ) , 1 ‚Äì 30 . [ 28 ] Judith Donath . 2007 . Signals , cues and meaning . Signals , Truth and Design ( 2007 ) . [ 29 ] MaeveDuggan . 2017 . Online Harassment . ( 2017 ) . [ 30 ] Yomna Elsayed and Andrea B Hollingshead . 2022 . Humor Reduces Online Incivility . Journal of Computer - Mediated Communication 27 , 3 ( 2022 ) , zmac005 . [ 31 ] Thomas Erickson and Wendy A Kellogg . 2000 . Social translucence : an approach to designing systems that support social processes . ACM transactions on computer - human interaction ( TOCHI ) 7 , 1 ( 2000 ) , 59 ‚Äì 83 . [ 32 ] Karmen Erjavec and Melita Poler Kovaƒçiƒç . 2012 . ‚ÄúYou Don‚Äôt Understand , This is a New War ! ‚Äù Analysis of Hate Speech in News Web Sites‚Äô Comments . Mass Communication and Society 15 , 6 ( 2012 ) , 899 ‚Äì 920 . [ 33 ] Jessica L Feuston , Alex S Taylor , and Anne Marie Piper . 2020 . Conformity of eating disorders through content moderation . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( 2020 ) , 1 ‚Äì 28 . [ 34 ] Brittany Fiore - Silfvast . 2012 . User - generated warfare : A case of converging wartime information networks and coproductive regulation on YouTube . International Journal of Communication 6 ( 2012 ) , 24 . [ 35 ] William W Gaver , Jacob Beaver , and Steve Benford . 2003 . Ambiguity as a resource for design . In Proceedings of the SIGCHI conference on Human factors in computing systems . 233 ‚Äì 240 . [ 36 ] Spiros V Georgakopoulos , Sotiris K Tasoulis , Aristidis G Vrahatis , and Vassilis P Plagianakos . 2018 . Convolutional neural networks for toxic comment classiÔ¨Åcation . In Proceedings of the 10th hellenic conference on artiÔ¨Åcial intelligence . 1 ‚Äì 6 . [ 37 ] Ysabel Gerrard . 2018 . Beyond the hashtag : Circumventing content moderation on social media . New Media & Society 20 , 12 ( 2018 ) , 4492 ‚Äì 4511 . [ 38 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , content moderation , and the hidden decisions that shape social media . Yale University Press . [ 39 ] Tommi Gr√∂ndahl , LucaPajola , MikaJuuti , MauroConti , and NAsokan . 2018 . All you need is " love " evading hate speech detection . In Proceedings of the 11th ACM workshop on artiÔ¨Åcial intelligence and security . 2 ‚Äì 12 . [ 40 ] Oliver L Haimson , Daniel Delmonaco , Peipei Nie , and Andrea Wegner . 2021 . Disproportionate removals and diÔ¨Äering content moderation ex - periences for conservative , transgender , and black social media users : Marginalization and moderation gray areas . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 ‚Äì 35 . [ 41 ] Emily M Hastings , Albatool Alamri , Andrew Kuznetsov , Christine Pisarczyk , Karrie Karahalios , Darko Marinov , and Brian P Bailey . 2020 . LIFT : integrating stakeholder voices into algorithmic team formation . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 ‚Äì 13 . [ 42 ] Sameer Hinduja and Justin W Patchin . 2010 . Bullying , cyberbullying , and suicide . Archives of suicide research 14 , 3 ( 2010 ) , 206 ‚Äì 221 . [ 43 ] bell hooks . 2003 . Teaching community : A pedagogy of hope . Vol . 36 . Psychology Press . [ 44 ] Hossein Hosseini , Sreeram Kannan , Baosen Zhang , and Radha Poovendran . 2017 . Deceiving google‚Äôs perspective api built for detecting toxic comments . arXiv preprint arXiv : 1702 . 08138 ( 2017 ) . [ 45 ] Dirk Hovy and Shannon L Spruit . 2016 . The social impact of natural language processing . In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) . 591 ‚Äì 598 . [ 46 ] Farnaz Jahanbakhsh , Amy X Zhang , Adam J Berinsky , Gordon Pennycook , David G Rand , and David R Karger . 2021 . Exploring lightweight interventions at posting time to reduce the sharing of misinformation on social media . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) , 1 ‚Äì 42 . [ 47 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - machine collaboration for content regulation : The case of reddit automoderator . ACM Transactions on Computer - Human Interaction ( TOCHI ) 26 , 5 ( 2019 ) , 1 ‚Äì 35 . [ 48 ] Shagun Jhaver , Larry Chan , and Amy Bruckman . 2018 . The view from the other side : The border between controversial speech and harassment on Kotaku in Action . First Monday 23 , 2 ( Feb . 2018 ) . https : / / doi . org / 10 . 5210 / fm . v23i2 . 8232 [ 49 ] Robin Jia and Percy Liang . 2017 . Adversarial examples for evaluating reading comprehension systems . arXiv preprint arXiv : 1707 . 07328 ( 2017 ) . [ 50 ] MariusJohnen , MarcJungblut , and MarcZiegele . 2018 . Thedigital outcry : What incites participationbehaviorin an online Ô¨Årestorm ? NewMedia & Society 20 , 9 ( 2018 ) , 3140 ‚Äì 3160 . [ 51 ] BiragoJones . 2012 . ReÔ¨Çective interfaces : Assistingteens with stressfulsituations online . Ph . D . Dissertation . MassachusettsInstitute of Technology . [ 52 ] Nadia Karizat , Dan Delmonaco , Motahhare Eslami , and Nazanin Andalibi . 2021 . Algorithmic folk theories and identity : How TikTok users co - produce Knowledge of identity and engage in algorithmic resistance . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 ‚Äì 44 . [ 53 ] MatthewKatsaros , Kathy Yang , andLaurenFratamico . 2022 . Reconsidering tweets : Intervening duringtweet creationdecreasesoÔ¨Äensivecontent . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 16 . 477 ‚Äì 487 . [ 54 ] Lida Ketsbaia , Biju Issac , and Xiaomin Chen . 2020 . Detection of Hate Tweets using Machine Learning and Deep Learning . In 2020 IEEE 19th International Conference on Trust , Security and Privacy in Computing and Communications ( TrustCom ) . IEEE , 751 ‚Äì 758 . [ 55 ] Svetlana Kiritchenko , Isar Nejadgholi , and Kathleen C Fraser . 2021 . Confronting abusive language online : A survey from the ethical and human rights perspective . Journal of ArtiÔ¨Åcial Intelligence Research 71 ( 2021 ) , 431 ‚Äì 478 . Manuscript submitted to ACM 20 Warner et al . [ 56 ] KungJin Lee , Wendy Roldan , TianQiZhu , HarkiranKaurSaluja , Sungmin Na , BritnieChin , Yilin Zeng , JinHaLee , andJasonYip . 2021 . Theshow must go on : A conceptual model of conducting synchronous participatory design with children online . In Proceedings of the 2021 CHI conference on human factors in computing systems . 1 ‚Äì 16 . [ 57 ] Bin Liang , Hongcheng Li , Miaoqiang Su , Pan Bian , Xirong Li , and Wenchang Shi . 2017 . Deep text classiÔ¨Åcation can be fooled . arXiv preprint arXiv : 1704 . 08006 ( 2017 ) . [ 58 ] Yotam Liel and Lior Zalmanson . 2020 . What If an AI Told You That 2 + 2 Is 5 ? Conformity to Algorithmic Recommendations . . In ICIS . [ 59 ] Mary Madden and Aaron Smith . 2010 . Reputation management and social media . Pew Internet & American Life Project ( 2010 ) . [ 60 ] Enrico Mariconti , Guillermo Suarez - Tangil , Jeremy Blackburn , Emiliano De Cristofaro , Nicolas Kourtellis , Ilias Leontiadis , Jordi Luque Serrano , andGianlucaStringhini . 2019 . " YouKnowWhattoDo " ProactiveDetectionofYouTubeVideosTargetedbyCoordinatedHateAttacks . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 ‚Äì 21 . [ 61 ] Brandeis Marshall . 2021 . Algorithmic misogynoir in content moderation practice . Heinrich - B√∂ll - Stiftung European Union ( 2021 ) . [ 62 ] J Matias , Amy Johnson , Whitney Erin Boesel , Brian Keegan , Jaclyn Friedman , and CharlieDeTar . 2015 . Reporting , Reviewing , and Responding to Harassment on Twitter . Available at SSRN 2602018 ( 2015 ) . [ 63 ] JoshuaMcVeigh - Schultz , Anya Kolesnichenko , and Katherine Isbister . 2019 . Shaping pro - socialinteractionin VR : an emergingdesignframework . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 12 . [ 64 ] Stefano Menini , Alessio Palmero Aprosio , and Sara Tonelli . 2021 . Abuse is contextual , what about nlp ? the role of context in abusive language annotation and detection . arXiv preprint arXiv : 2103 . 14916 ( 2021 ) . [ 65 ] Alexandros Mittos , Savvas Zannettou , Jeremy Blackburn , and Emiliano De Cristofaro . 2020 . Analyzing Genetic Testing Discourse on the Web Through the Lens of Twitter , Reddit , and 4chan . ACM Trans . Web 14 , 4 , Article 17 ( aug 2020 ) , 38 pages . https : / / doi . org / 10 . 1145 / 3404994 [ 66 ] Alexandros Mittos , Savvas Zannettou , Jeremy Blackburn , and Emiliano De Cristofaro . 2020 . ‚ÄúAnd We Will Fight for Our Race ! ‚Äù A Measurement Study of Genetic TestingConversationson Reddit and 4chan . In Proceedings of the International AAAIConferenceon Weband Social Media , Vol . 14 . 452 ‚Äì 463 . [ 67 ] Maria D Molina and S Shyam Sundar . 2022 . When AI moderates online content : eÔ¨Äects of human collaboration and interactive transparency on user trust . Journal of Computer - Mediated Communication 27 , 4 ( 2022 ) , zmac010 . [ 68 ] Michael J Muller and Allison Druin . 2012 . Participatory design : The third space in human ‚Äì computer interaction . In The Human ‚Äì Computer Interaction Handbook . CRC Press , 1125 ‚Äì 1153 . [ 69 ] SaÔ¨Åya Umoja Noble . 2018 . Algorithms of oppression . In Algorithms of Oppression . New York University Press . [ 70 ] John Pavlopoulos , JeÔ¨ÄreySorensen , Lucas Dixon , Nithum Thain , and Ion Androutsopoulos . 2020 . Toxicity detection : Does context really matter ? arXiv preprint arXiv : 2006 . 00998 ( 2020 ) . [ 71 ] J√ºrgen PfeÔ¨Äer , Thomas Zorbach , and Kathleen M Carley . 2014 . Understanding online Ô¨Årestorms : Negative word - of - mouth dynamics in social media networks . Journal of Marketing Communications 20 , 1 - 2 ( 2014 ) , 117 ‚Äì 128 . [ 72 ] Whitney Phillips . 2015 . This is why we can‚Äôt have nice things : Mapping the relationship between online trolling and mainstreamculture . Mit Press . [ 73 ] SravanaReddyandKevinKnight . 2016 . Obfuscatinggenderinsocialmediawriting . In ProceedingsoftheFirstWorkshoponNLPandComputational Social Science . 17 ‚Äì 26 . [ 74 ] Yafeng Ren , Yue Zhang , Meishan Zhang , and Donghong Ji . 2016 . Context - sensitive twitter sentiment classiÔ¨Åcation using neural network . In Thirtieth AAAI conference on artiÔ¨Åcial intelligence . [ 75 ] Morgan Klaus Scheuerman , Jialun Aaron Jiang , Casey Fiesler , and Jed R Brubaker . 2021 . A framework of severity for harmful content online . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 ‚Äì 33 . [ 76 ] Charlotte Schluger , Jonathan P Chang , Cristian Danescu - Niculescu - Mizil , and Karen Levy . 2022 . Proactive Moderation of Online Discussions : Existing Practices and the Potential for Algorithmic Support . Proceedings of the ACM on Human - Computer Interaction 6 , CSCW2 ( 2022 ) , 1 ‚Äì 27 . [ 77 ] Sarita Schoenebeck , Oliver L Haimson , and Lisa Nakamura . 2021 . Drawing from justice theories to support targets of online harassment . new media & society 23 , 5 ( 2021 ) , 1278 ‚Äì 1300 . [ 78 ] Joseph Seering , Robert Kraut , and LauraDabbish . 2017 . Shaping pro and anti - socialbehavioron twitch through moderation and example - setting . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing . 111 ‚Äì 125 . [ 79 ] Manya Sleeper , Justin Cranshaw , Patrick Gage Kelley , Blase Ur , Alessandro Acquisti , Lorrie Faith Cranor , and Norman Sadeh . 2013 . " I read my Twitter the next morning and was astonished " a conversational perspective on Twitter regrets . In Proceedings of the SIGCHI conference on human factors in computing systems . 3277 ‚Äì 3286 . [ 80 ] JitendraSoni . 2021 . Tindertweakurgespeopletothinkbeforesendingabuse . https : / / www . techradar . com / uk / news / tinder - will - alert - users - before - they - send - oÔ¨Äensive - messages [ 81 ] MiriahSteiger , TimirJBharucha , SukritVenkatagiri , MartinJ . Riedl , andMatthewLease . 2021 . ThePsychologicalWell - Being ofContentModerators : The Emotional Labor of Commercial Moderation and Avenues for Improving Support . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3411764 . 3445092 [ 82 ] Francesca Stevens , Jason RC Nurse , and Budi Arief . 2021 . Cyber stalking , cyber harassment , and adult mental health : A systematic review . Cyberpsychology , Behavior , and Social Networking 24 , 6 ( 2021 ) , 367 ‚Äì 376 . [ 83 ] Miriam Sturdee and Joseph Lindley . 2019 . Sketching & drawing as future inquiry in HCI . In Proceedings of the Halfway to the Future Symposium 2019 . 1 ‚Äì 10 . Manuscript submitted to ACM A Critical ReÔ¨Çection on the Use of Toxicity Detection Algorithms in Proactive Moderation 21 [ 84 ] Nicolas P Suzor , Sarah Myers West , Andrew Quodling , and Jillian York . 2019 . What do we mean when we talk about transparency ? Toward meaningful transparency in commercial content moderation . International Journal of Communication 13 ( 2019 ) , 18 . [ 85 ] Christian Szegedy , Wojciech Zaremba , Ilya Sutskever , Joan Bruna , Dumitru Erhan , Ian Goodfellow , and Rob Fergus . 2013 . Intriguing properties of neural networks . arXiv preprint arXiv : 1312 . 6199 ( 2013 ) . [ 86 ] Kurt Thomas , Devdatta Akhawe , Michael Bailey , Dan Boneh , Elie Bursztein , Sunny Consolvo , Nicola Dell , Zakir Durumeric , Patrick Gage Kelley , Deepak Kumar , et al . 2021 . Sok : Hate , harassment , and the changing landscape of online abuse . In 2021 IEEE Symposium on Security and Privacy ( SP ) . IEEE , 247 ‚Äì 267 . [ 87 ] Kristen Vaccaro , Christian Sandvig , and Karrie Karahalios . 2020 . " At the End of the Day Facebook Does What It Wants " How Users Experience Contesting Algorithmic Content Moderation . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW2 ( 2020 ) , 1 ‚Äì 22 . [ 88 ] Kristen Vaccaro , Ziang Xiao , Kevin Hamilton , and Karrie Karahalios . 2021 . Contestability For Content Moderation . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 ‚Äì 28 . [ 89 ] Kathleen Van Royen , Karolien Poels , Heidi Vandebosch , and Philippe Adam . 2017 . ‚ÄúThinking before posting ? ‚Äù Reducing cyber harassment on social networking sites through a reÔ¨Çective message . Computers in human behavior 66 ( 2017 ) , 345 ‚Äì 352 . [ 90 ] Bertie Vidgen , Helen Margetts , and Alex Harris . 2019 . How much online abuse is there . Alan Turing Institute ( 2019 ) . [ 91 ] Jagoda Walny , Jonathan Haber , Marian D√∂rk , Jonathan Sillito , and Sheelagh Carpendale . 2011 . Follow that sketch : Lifecycles of diagrams and sketches in software development . In 2011 6th International Workshop on Visualizing Software for Understanding and Analysis ( VISSOFT ) . IEEE , 1 ‚Äì 8 . [ 92 ] YangWang , Pedro GiovanniLeon , AlessandroAcquisti , LorrieFaithCranor , AlainForget , andNormanSadeh . 2014 . AÔ¨Åeldtrialofprivacynudges for facebook . In Proceedings of the SIGCHI conference on human factors in computing systems . 2367 ‚Äì 2376 . [ 93 ] Yang Wang , Pedro Giovanni Leon , Kevin Scott , Xiaoxuan Chen , Alessandro Acquisti , and Lorrie Faith Cranor . 2013 . Privacy nudges for social media : an exploratory Facebook study . In Proceedings of the 22nd international conference on world wide web . 763 ‚Äì 770 . [ 94 ] Yang Wang , Gregory Norcie , Saranga Komanduri , Alessandro Acquisti , Pedro Giovanni Leon , and Lorrie Faith Cranor . 2011 . " I regretted the minute I pressed share " a qualitative study of regrets on Facebook . In Proceedings of the seventh symposium on usable privacy and security . 1 ‚Äì 16 . [ 95 ] Mark Warner , Laura Lascau , Anna L Cox , Duncan P Brumby , and Ann Blandford . 2021 . ‚ÄúOops . . . ‚Äù : Mobile Message Deletion in Conversation Error and Regret Remediation . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3411764 . 3445118 [ 96 ] Zeerak Waseem , Thomas Davidson , Dana Warmsley , and Ingmar Weber . 2017 . Understanding abuse : A typology of abusive language detection subtasks . arXiv preprint arXiv : 1705 . 09899 ( 2017 ) . [ 97 ] LibbyWatson . 2017 . FacebookThinksSaying " MenAreTrash " IsHateSpeech . https : / / gizmodo . com / facebook - thinks - saying - men - are - trash - is - hate - speech - 1795170688 [ 98 ] Oded Yaron . 2012 . Another chapter in the Facebook wars : a right - wing group against a creator from the left ( translated ) . https : / / www . haaretz . co . il / captain / net / 2012 - 06 - 12 / ty - article / 0000017f - e622 - da9b - a1Ô¨Ä - ee6fdf660000 [ 99 ] Savvas Zannettou , Barry Bradlyn , Emiliano De Cristofaro , Haewoon Kwak , Michael Sirivianos , Gianluca Stringini , and Jeremy Blackburn . 2018 . What is gab : A bastion of free speech or an alt - right echo chamber . In Companion Proceedings of the The Web Conference 2018 . 1007 ‚Äì 1014 . [ 100 ] Yubao Zhang , Xin Ruan , Haining Wang , Hui Wang , and Su He . 2016 . Twitter trends manipulation : a Ô¨Årst look inside the security of twitter trending . IEEE Transactions on Information Forensicsand Security 12 , 1 ( 2016 ) , 144 ‚Äì 156 . Manuscript submitted to ACM