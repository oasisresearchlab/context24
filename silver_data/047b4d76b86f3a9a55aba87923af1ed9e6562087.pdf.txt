Large - scale , Parallel Automatic Patent Annotation Milan Agatonovic , Niraj Aswani , Kalina Bontcheva , Hamish Cunningham , Thomas Heitz , Yaoyong Li , Ian Roberts , Valentin Tablan Department of Computer Science University of Shefﬁeld { Initial . Surname } @ dcs . shef . ac . uk ∗ ABSTRACT When researching new product ideas or ﬁling new patents , inventors need to retrieve all relevant pre - existing know - how and / or to exploit and enforce patents in their technologi - cal domain . However , this process is hindered by lack of richer metadata , which if present , would allow more powerful concept - based search to complement the current keyword - based approach . This paper presents our approach to au - tomatic patent enrichment , tested in large - scale , parallel experiments on USPTO and EPO documents . It starts by deﬁning the metadata annotation task and examines its challenges . The text analysis tools are presented next , in - cluding details on automatic annotation of sections , refer - ences and measurements . The key challenges encountered were dealing with ambiguities and errors in the data ; cre - ation and maintenance of large , domain - independent dic - tionaries ; and building an eﬃcient , robust patent analysis pipeline , capable of dealing with terabytes of data . The ac - curacy of automatically created metadata is evaluated aga - inst a human - annotated gold standard , with results of over 90 % on most annotation types . Categories and Subject Descriptors H . 3 . 1 [ Information Storage And Retrieval ] : Content Analysis and Indexing— Linguistic processing ; I . 2 . 7 [ Artiﬁcial Intelligence ] : Natural Language Processing— Text analysis General Terms Experimentation , Measurement , Performance Keywords Patent Enrichment , Information Extraction , Parallel , Large - Scale , GATE ∗ Author names are ordered alphabetically . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . PaIR’08 , October 30 , 2008 , Napa Valley , California , USA . Copyright 2008 ACM 978 - 1 - 60558 - 256 - 6 / 08 / 10 . . . $ 5 . 00 . 1 . INTRODUCTION Patents are an important vehicle for protecting intellec - tual property and this importance is increasing in the cur - rent globalised and knowledge based economy . When re - searching new product ideas or ﬁling new patents , inventors need to retrieve all relevant pre - existing know - how and / or exploit and enforce patents in their technological domain . However , this process is hindered by lack of richer meta - data , which if present , would allow more powerful concept - based search to complement the traditional keyword - based approach . Semantic annotation is the task of attaching metadata tags and / or ontology classes to text segments , as a pre - requisite for knowledge access and retrieval tools and user interfaces . Automatic annotation is carried out by employ - ing Information Extraction ( IE ) [ 4 ] techniques , which recog - nise automatically mentions of a given set of events , entities or relationships . From an algorithmic perspective , IE ap - proaches fall in two broad categories : manually engineered ones ( frequently based on pattern - matching like rules ) ( e . g . , [ 13 ] ) and machine learning ones ( e . g . [ 2 , 11 ] ) . Rule - based approaches are more suitable where a carefully engineered , high precision system is needed and there is no suﬃcient training data for a machine learning approach to be success - ful . From an operational perspective , IE tools can be de - ployed in both fully and semi - automatic applications ( where users can inspect and , if needed , correct the automatically created metadata ) . In general , fully automatic methods are preferred when the volume of data is too large to make hu - man post - annotation feasible , as is the case with patents . In particular , patent processing and search require high re - call methods , capable of operating robustly on large - volumes of data . Previous research on IE has been carried out mostly on smaller datasets from narrower domains , mostly news ar - ticles [ 12 , 2 , 7 ] , with accuracy results exceeding 85 % . The new challenge addressed here is in scaling up these methods to deal with the diversity and volume of patent data , without sacriﬁcing computational performance and accuracy levels . Applications of information extraction to patent annota - tions are quite scarce . [ 9 ] mostly focus on OCR and text classiﬁcation , while discussing only brieﬂy the importance and challenges of identifying references to ﬁgures and claims in patents . In this area they have only carried out a small feasibility study using the Xerox language processing tools , without providing any evaluation ﬁgures or suﬃcient imple - mentational details . More recently , the PatExpert project [ 16 ] has developed some content extraction components , ba - sed on deeper linguistic analysis than the approach proposed here . The advantages of shallow IE methods such as ours are that they are more robust in face of language variability and also scale better in terms of computational eﬃciency . The latter objective is particularly important in our case , as the requirement is to process eﬃciently terabytes of patents . This paper presents our shallow IE approach to automatic patent enrichment , tested in large - scale , parallel processing experiments on USPTO and EPO documents . Section 2 starts by deﬁning the patent annotation task and examines its challenges . The information extraction tools are pre - sented next ( Section 3 ) , including details on automatic an - notation of sections , references and measurements . The key challenges encountered were dealing with ambiguities and errors in the data ; and creation and maintenance of large , domain - independent dictionaries . Section 4 is dedicated to the third , most signiﬁcant challenge , i . e . , building an ef - ﬁcient , robust patent analysis pipeline , capable of dealing with terabytes of data . The accuracy of automatically cre - ated metadata is evaluated against a human - annotated gold standard , with results of over 90 % on most annotation types ( see Section 5 ) . This excellent performance is achievable due to the relatively constrained , legal sub - language used in patents , although we did encounter signiﬁcant diﬀerences in the way American and European patents are phrased and structured and the literature references and measurement units are extremely variable . In the end we summarise our ﬁndings and discuss future work . 2 . THE PATENT ANNOTATION TASK The experiments in this paper are based on two kinds of patents – American ( USPTO ) and European ( EPO ) ones . The reason behind choosing two data sources is because they diﬀer in terms of already provided metadata as XML tags , formatting , quality , and legal language used . The semantic annotation process adds new metadata in the form of XML tags , which are also mapped to a patent - speciﬁc ontology , encoded in OWL [ 8 ] . We chose to support two diﬀerent annotation formats to aid interoperability and also to enable the use of ontology - based semantic query tools such as KIM [ 14 ] and OWLIM [ 10 ] . The automatically added metadata falls into two broad categories : wide and deep annotation types . Wide anno - tations are intended to cover meta - data types that apply to patents in general and do not depend on the speciﬁc subject area of the patent ( as identiﬁed , e . g . , by its IPC code ) . Examples of such meta - data include document sec - tions and references to cited literature , examples , ﬁgures , claims , and other patents . Deep annotations are speciﬁc to one or more subject areas and are of interest to specialised patent searchers . The experiments reported here focus on automatic annotation of measurements , as they are very im - portant for patent professionals , while also being very hard to ﬁnd using keyword search , due to the diverse ways in which they are expressed in language . The beneﬁts from the automatic metadata enrichment process are three - fold . Firstly , IE is capable of dealing with variable language patterns and format irregularities much easier than text - based regular expressions . For example , references to other patents can be very diverse : U . S . Patent 4 , 524 , 128 , Korean laid open utility model application No . 1999 - 007692 . Secondly , once we markup the relevant parts of the patent , the IE tools can also carry out data normal - isation . Again , taking an example from references to ﬁg - ures or similarly claims , expressions such as “Figures 1 - 3” or “Claims 5 - 10”imply references not just to the explicitly men - tioned ﬁgure / claim numbers but also to all those in between . Lastly , by using text mining techniques we are capable to ex - tract a signiﬁcantly wider range of useful information and provide it as additional XML tags in the patent documents . 2 . 1 Section annotations Patent documents are typically quite long , contain mul - tiple required sections , and use highly formalised legal and technical terminology with the notable exception of litera - ture references and measurements . Diﬀerent aspects of the patent application are typically presented in a pre - deﬁned set of sections and subsections ( e . g . prior art , patent claims , technical problem addressed and eﬀect ) . Both USPTO and EPO documents have at least three main parts , the ﬁrst page containing bibliographical data and abstract , the de - scriptions part , and the claims part . Automatic section recognition is based on identifying typ - ical section titles and then partitioning the text automati - cally based on that . Pre - existing section markup is used , if available . For instance , BibliographicData , Abstract and Claims sections tend to be already annotated in patent doc - uments so we use them directly . As we distinguish about 20 diﬀerent types of sections , most of these still need to be detected automatically ( the complete list appears in table 4 ) . 2 . 2 Reference annotations Reference annotations are used for parts of text that re - fer to either objects in the current document ( e . g . ﬁgures , tables , etc . ) or to other documents ( e . g . scientiﬁc papers ) . A reference annotation consists of two parts , a header in - dicating the type of the reference , and one or more identiﬁers which typically consist of a mixture of numbers and letters . For example , in Figure 1 and 2 the header is Figure and the identiﬁers are 1 and 2 . In U . S . Pat . No . 3 , 765 , 999 the header is U . S . Pat . and the identiﬁer is No . 3 , 765 , 999 . Conjunctive phrases mentioning references to two or more objects of the same reference type are tagged initially as one Reference annotation , including the conjunction and all punctuation . For example , Figures 1 and 2 ; Claims 1 - 3 ; Ta - bles 1 to 10 are ﬁrst annotated as one Reference each , of type Figure , Claim and Table respectively . The normalisation step then separates these into their constituency references , also including all implied references ( e . g . , to claim 2 ) . From an IE perspective , some types of references are much simpler to identify than others . For instance , there is sig - niﬁcantly less variability in the way patents refer to ﬁgures , tables , claims , equations , and examples . References to other patents tend to be slightly more challenging , as they often in - clude the inventor names , patent date , or even title , in addi - tion to a simple header and identiﬁer . The hardest of all are references to external sources , such as published papers ( e . g . , Hudson & Hay , Practical Immunology ( Blackwell Scientiﬁc Publications , Oxford , UK , 1980 ) , Chapter 8 ) , which tend to be quite long and typically contain many abbreviations and idiosyncratic formatting . We have also observed signiﬁ - cant diﬀerences between American and European patents in this respect and had to adapt the IE tools to deal with that accordingly . 2 . 3 Measurements annotations Most measurements comprise a scalar value followed by a unit , e . g . 2x10 - 7 Torr . Furthermore , two scalar values with or without unit can be contained in an interval . Sometimes there are also accompanying words , such as “less than” or “between”which are important for professional searchers and are therefore also marked by the IE tools , e . g . , “less than about 0 . 0015 mm” , “2 x 10 5 to 2 x 10 7 cpm / ml” . Lastly , we also deal with relative measurements , such as percentages and ratios . The main challenge in recognising measurements in pate - nts comes from the large number of measurement units in ex - istence ( e . g . , units used in physics patents are very diﬀerent to those used in engineering ones ) . Another challenge is that some units have single letter abbreviations , which introduce ambiguities in many cases and therefore the wider context needs to be considered in order to determine , whether the sequence of numbers followed by a letter is indeed a mea - surement . One frequently encountered example are tem - peratures , e . g . , “1C” where we need to distinguish correct temperature mentions from other cases , such as references to ﬁgures , examples , tables , etc . ( as in “see Figure 1C” ) . 3 . OUR APPROACH 3 . 1 IE tools We have developed our information extraction system us - ing GATE 1 [ 5 ] . GATE , the General Architecture for Text Engineering , is a framework providing support for a vari - ety of language engineering tasks . It includes a vanilla in - formation extraction system , ANNIE , and a large number of plugins for various tasks and applications , such as on - tology support , information retrieval , support for diﬀerent languages , WordNet , machine learning algorithms , and so on . The processing resources we use from ANNIE are as follows : tokeniser , gazetteer and ﬁnite state transduction grammars . The resources communicate via GATE’s anno - tation API , which is a directed graph of arcs bearing arbi - trary feature / value data , and nodes rooting this data into document content ( in this case text ) . The tokeniser splits text into simple tokens , such as num - bers , punctuation , symbols , and words of diﬀerent types ( e . g . with an initial capital , all upper case , etc . ) , adding a “Token” annotation to each . It does not need to be modi - ﬁed for diﬀerent applications or text types . Our application developed its own , patent - speciﬁc gaze - tteers ( list of expressions ) that aid the recognition of mea - surements and references ( see below ) . The lists are compiled into ﬁnite state machines , which can match text tokens . The semantic tagger ( or JAPE transducer ) consists of hand - crafted rules written in the JAPE pattern language [ 6 ] , which describe patterns to be matched and annotations to be created . Patterns can be speciﬁed by describing a speciﬁc text string or annotation ( e . g . those created by the tokeniser , gazetteer , document format analysis , etc . ) . 3 . 2 Building the Gazetteers Rule - based IE systems comprise of a set of grammar rules based on some patterns and clue words . For example to locate a reference to a table , one could use the clue word 1 Gate is freely available for download from http : / / gate . ac . uk / table followed by a number . The idea of using gazetteers is to annotate such clue words in the text with all their inﬂections . One approach is to use a set of hand - annotated examples to derive such lists , however this requires a thorough corpus analysis . This is the approach we used to build gazetteers for locating the references , based on the gold standard corpus ( see Section 5 . 1 ) . The reference gazetteers are rather small in size , 314 elements in total , and contain clue words such as Figure , Table and Example to name a few . They also contain entries such as described in or Patent application no . to help locate literature and patent references . In case of measurements , a database 2 containing more than 30K entries was used to automatically populate a gaz - etteer list . The database also contains transformation rules for transforming one measurement value into another ( e . g . , inches to cm ) . Since a gazetteer is simply a list of entries , the information about transforming rules has been populated in an ontology . These rules will be used for answering semantic queries by transforming values in one measurement unit into the other on the ﬂy . 3 . 3 Creating the Annotation Rules A typical JAPE rule consists of two parts : left hand side ( LHS ) and right hand side ( RHS ) . LHS consists of an anno - tation pattern that should be matched in the text and RHS declares the action that should be taken when the pattern speciﬁed in LHS is found in the document . An example of such a pattern is given below : Rule : FindANumberFollowedByAUnit ( { Number } { MeasurementUnit } ) : match - - > : match . Measurement = { } The pattern , speciﬁed above , will try to locate a sequence of annotations where the ﬁrst annotation is of type Number and the next annotations is of type MeasurementUnit – the latter being created on the basis of the measurement unit gazetteer . If such a pattern is found , the entire sequence is annotated as the Measurement annotation . In total , the ap - plication has over 30 JAPE rules that identify measurement units in the text . These include identiﬁcation of complex equations and intervals of measurements as well . The process of identifying sections and locating various references in the text consists of executing similar rules over the text . Annotations produced by other processing re - sources such as the gazetteers are used in the rules . Similar techniques are used along with the annotations produced by various gazetteer lists to identify annotations of type Reference . Each reference is then classiﬁed into a subtype such as Figure , Formula or Table . Unlike these references where the used keywords are part of the actual reference tag , contextual information is needed for patent and literature references . For example a reference to a lit - erature can have keywords such as described in or according to in their left contexts . Below we give an example of such a pattern : 2 http : / / www . gnu . org / software / units Table 1 : Application pipeline . Phase Gate processing resource 1 Section Finder 2 English Tokeniser 3 Patent - speciﬁc gazetteer 4 Reference Finder 5 Measurements Finder Rule : Patent ( { PatentContext } ( { PatStart } { PatentNumber } ) : match ) : match - with - context - - > : match . Patent = { } Given this pattern , it will match with the string such as described in U . S . Patent 4 , 524 , 128 , where described in , U . S . Patent and 4 , 524 , 128 are annotated as PatentContext , Pat - Start and PatentNumber respectively . However , only the part that matches with ( { PatStart } { PatentNumber } ) is an - notated as the Patent reference . The application has over 30 JAPE rules that identify mea - surement units in the text . This include identiﬁcation of complex equations and intervals of measurements as well . As explained in the previous subsection , the measurement gazetteer is used for identifying measurement units in the text . For example , the following pattern would annotate the text such as 40 - 50mph where 40 and 50 are the two numbers and mph is the measurement unit . Rule : MeasurementInterval ( { Number } { Token . string = = " - " } { Number } { Unit } ) : span - - > : span . Measurement = { type = " interval " } 3 . 4 The Application The application pipeline consists of a number of processing resources that are executed sequentially , where some compo - nents rely on the output of earlier ones . An example of this is the Reference ﬁnder resource that depends on the output of the gazetteer . Table 1 lists the resources in their order of execution in our application . The pipeline is executed on one document at a time . Fig - ure 1 shows an example of a processed document . 4 . LARGE - SCALE EFFICIENCY TRIALS 4 . 1 Experiments One of the most challenging tasks of any IE application is to adapt it to process a large amount of data without com - promising on quality or performance . The main purpose of carrying out these experiments was to develop a highly optimised and equally accurate application that can exploit the hardware at its best . This is achieved by benchmark - ing the individual resources used in the application and by Table 2 : Baseline Experiments . Patent No of KB / Sec Time / Document Type Processes USPTO or EPO USPTO 1 8 . 06 10 . 54s USPTO 4 29 . 95 2 . 84s USPTO 8 53 . 15 1 . 60s EPO 1 6 . 56 4 . 45s EPO 4 27 . 41 1 . 08s EPO 8 47 . 12 0 . 62s identifying those that need optimisation . In this section , we describe how we achieve this . We ﬁrst describe the setup of our experiments , followed by some baseline results and some details on optimisation . Finally , we compare the base - line results with the optimised application results . The main purpose of the application is to consistently process large amount of patent data and produce metadata in the form of patent - speciﬁc annotations ( i . e . sections , references and measurements ) and other linguistic annota - tions ( such as tokens , sentences etc . ) . In order to evaluate the consistency in the application’s performance on a large dataset , experiments were carried out on a corpus consist - ing of 1 . 3 million USPTO ( 108GB ) and 27 thousand EPO ( 780MB ) documents in XML format with few attributes on each markup . The average sizes of USPTO and EPO docu - ments were 85KB and 29 . 21KB respectively . Our experiments were carried out on the IRF’s Large Data Collier ( LDC ) 3 . This is an SGI Altix 4700 system comprising 20 processing nodes each with four 1 . 4GHz Itanium proces - sor cores and 18GB RAM . The nodes are connected using SGI’s high speed NUMAlink interconnect technology , allow - ing the whole cluster to appear as a single shared - memory system with a total of 80 processor cores and 360GB of main memory . Storage is provided via a ﬁbre channel SAN . In this environment , experiments were run with diﬀerent numbers of processes running simultaneously . Table 2 gives details on the baseline experiments . As shown in the table , the application was able to pro - cess 8 . 06KB per second when executed in a single process . In other words , it took a process 10 . 54 seconds to process a single USPTO document with an average size of 85KB . Whereas processing rates of 29 . 95 KB / Sec and 53 . 15 KB / Sec were observed for 4 and 8 parallel processes respectively . Even though the number of processes were increased to 4 and 8 , the processing rate did not increase with the multi - plication of the number of processes . Similar results can be seen for the EPO documents . The time taken to process one EPO document is bit less than the half of the time taken to process one USPTO document . The same is also true for the average sizes of USPTO and EPO documents . This in - dicates that the processing rate is linear as the size of the patent documents grows ( see ﬁgure 2 ) . It was also a part of these experiments to benchmark the individual processing resources and report their data pro - cessing rates and their share in the overall time taken by the entire application . As speciﬁed earlier , the motive was to identify components which needed further optimisation . Having obtained results on 8 parallel processes , the interim changes were instantly applied to remove linguistic compo - 3 http : / / www . ir - facility . org / the irf / semantic - supercomputing Figure 1 : Annotated patent document in GATE GUI . Figure 2 : Baseline Results . nents that were identiﬁed as expensive and did not have signiﬁcant impact on the overall results , such as , ANNIE’s morphological analyser and named entity recogniser . Al - though this resulted in a slightly reduced number of linguis - tic annotations , it did not aﬀect the automatic processing of patent - speciﬁc annotations . As a result , when the same ex - periment was performed with 12 parallel processes , the pro - cessing rate of 110 . 03 KB / Sec was achieved which is slightly less than double the processing rate for 8 parallel processes . Similarly the time taken to process one document decreased from 1 . 60 seconds to 0 . 77 seconds per document . The patent processing application contains several linguis - tic components that depend on the output of the previ - ous stages . One such example is the grammar that iden - tiﬁes measurement units , which requires word boundaries to be already annotated by the tokeniser . The benchmark - ing tool was also used to identify expensive resources , which are needed by subsequent grammars . Such resources were then either optimised or completely removed after refactor - ing the depending grammars . For example , instead of using a morphological analyser , the rules were instead modiﬁed to match all word forms under consideration . Having thus optimised the application , the experiments on all 1 . 3 million documents were repeated to collect new benchmark results . Table 3 shows a comparison between the Figure 3 : Baseline Vs Optimised . Table 3 : Baseline Vs Optimised Application . No of KB / seconds seconds / Doc processes Baseline Optimised Baseline Optimised 1 8 . 06 20 . 7 10 . 54 4 . 11 4 29 . 95 73 . 86 2 . 84 1 . 15 8 53 . 15 144 . 44 1 . 6 0 . 59 12 110 . 03 203 . 76 0 . 77 0 . 42 baseline results and the results obtained for the optimised application . Figure 3 explains these results through graphs . As shown in table 3 , the processing rates for 1 , 4 and 8 parallel processes are almost 2 . 5 times higher , whereas with 12 processes the optimised application performes 1 . 8 times better . This is most likely due to the fact that the entire LDC system is a cluster of nodes where the memory and input / output operations are being shared among the pro - cessor nodes . With 12 processes ( in comparison to the fewer 8 processes ) , this means further segregation of the shared LDC resources . After automatic annotation the 1 . 3 million documents re - quire around 139 GB when stored as stand - oﬀ XML ﬁles plus content text ﬁles . The average ﬁle size is 85 KB before and 113 KB ( standoﬀ and content ) after processing . Finally , to summarise the results , the baseline application took 264 hours ( 11 days ) to process 1 . 3 million USPTO doc - uments at the processing rate of 110 . 03 KB / Sec , whereas the optimised application took 142 hours ( 5 . 92 days ) to process the same number of documents at a processing rate of 203 . 76 KB / Sec . Given that the hardware has 80 processor nodes in total , the overall speed can be improved even further . In order to be able to estimate the number of annotations that the application produces per document , 20 documents ( both from the USPTO and EPO documents ) were obtained at random . These contained 147 section annotations , 604 measurements , 1 , 351 references and 150 , 140 linguistic anno - tations . Based on these results , it would be reasonable to estimate that each document in our corpus contains an av - erage of 105 end - user ( patent speciﬁc ) annotations and 7507 linguistic annotations . 5 . ACCURACY EVALUATION 5 . 1 Gold standard Patent documents typically contain one or more IPC codes indicating the nature of the invention , so we used these codes to select patents relating to particular technology categories . We selected the patents from two very diﬀerent ﬁelds , me - chanical engineering and biomedical technology , to better examine the diversity in the data . We manually annotated some USPTO documents and EPO documents in several iterations , in order to test the anno - tation user interface , reﬁne the annotation deﬁnitions , and evaluate the automatic processing . The evaluation corpus used in this paper consists of 23 USPTO and 28 EPO doc - uments . The reason behind annotating more EPO than USPTO documents is that the latter tend to be more uni - form in terms of formatting , language used , sectioning , etc . We consider that a corpus of 51 documents , 376 , 713 words or 2 , 490 , 666 characters is big enough to test our system . For the section annotations , in addition we also extracted all heading annotations from the 1 . 3 millions documents and checked that the most frequent ones are found by our system . In order to ensure more consistent gold standard data , manual annotation was carried out in two passes . First two annotators would markup each patent documents indepen - dently and then an expert checked the two sets and corrected any disagreements and missed annotations . In total , more than 10 diﬀerent human annotators were involved . Through - out the process we also controlled the inter - annotator agree - ment , which ensures good quality of the human - annotated data . 5 . 2 Results on the Gold Standard Accuracy evaluation is an essential part of the develop - ment of information extraction applications and is carried out by comparing the annotations produced by the auto - matic system against those in the gold standard . The reported results make use of traditional evaluation metrics for information extraction [ 3 ] : precision , recall , and F - measure . Precision measures the number of correctly iden - tiﬁed items as a percentage of the number of items identiﬁed . It measures how many of the items that the system identiﬁed were actually correct , regardless of whether it also failed to retrieve correct items . The higher the precision , the better the system is at ensuring that what is identiﬁed is correct . Recall measures the number of correctly identiﬁed items as a percentage of the total number of correct items measuring Table 4 : Corpus statistics . USPTO contains 23 doc - uments and EPO contains 28 documents . Annotation type USPTO EPO Section . Abstract 23 28 S . BackgroundArt 19 22 S . BestMode 2 5 S . BibliographicData 23 28 S . Bibliography 0 8 S . Claims 23 0 S . CrossReferenceToR . A . 6 1 S . DetailedDescription 11 18 S . DisclosureOfInvention 3 6 S . DrawingDescription 16 20 S . Eﬀects 1 2 S . Examples 17 25 S . PreferredEmbodiment 10 7 S . PriorArt 4 6 S . Sponsorship 2 0 S . SummaryOfTheInvent . 20 18 S . TechnicalField 14 17 S . UsageOfInvention 1 6 Annotations / Doc 8 . 5 8 Reference . Claim 352 2 R . Example 99 264 R . Figure 375 570 R . Formula 79 66 R . Literature 114 488 R . Patent 92 182 R . Table 59 105 Annotations / Doc 51 60 Annotations / 1000 Char 1 . 4 1 M . scalarValue 1998 3409 Measurement . unit 1613 2994 M . interval 432 375 Annotations / Doc 176 242 Annotations / 1000 Char 4 . 9 4 Characters 827 , 294 1 , 663 , 372 per document 35 , 969 59 , 406 how many of the items that should have been identiﬁed ac - tually were identiﬁed . The higher the recall rate , the better the system is at not missing correct items . The F - measure [ 15 ] is often used in conjunction with Precision and Recall , as a weighted average of the two – usually an application requires a balance between Precision and Recall . Overall , the evaluation ﬁgures obtained are above 85 % , which makes them suitable for immediate deployment and use in end - user applications ( see Table 5 ) . The main ex - ception are references to other patents and external publi - cations / literature , where the results are not yet ﬁnal , as the rules are still under development . Concerning the recogni - tion of measurement intervals in the 28 EPO documents , the lower results can be explained by their more versatile forms that are not easy to generalise . In the case of measurements , for comparison’s sake , the highest results obtained in the Matrixware TempRanger ex - periment 4 were 75 . 51 % precision and 88 . 48 % recall , while only identifying temperature expressions . Therefore , our measurement grammars not only achieve higher performance , but it also captures a much wider range of measurements . Table 5 : Evaluation ﬁgures per annotation type on the USPTO 23 documents and EPO 28 documents gold standard for micro - averaged precision , recall and F1 - score . Annotation type USPTO EPO P . R . F1 P . R . F1 S . BackgroundArt 74 74 74 56 68 61 S . DrawingDescr . 75 75 75 84 80 82 Section . Examples 65 65 65 61 56 58 S . SummaryOf . 89 80 84 83 83 83 S . TechnicalField 80 57 67 94 94 94 Reference . Claim 100 100 100 100 100 100 R . Example 97 100 99 100 99 99 R . Figure 99 99 99 99 98 98 R . Formula 99 99 99 100 100 100 R . Literature 69 75 72 70 74 72 R . Patent 76 77 77 72 84 78 R . Table 100 98 99 100 100 100 M . scalarValue 96 93 94 94 92 93 Measurement . unit 95 92 93 94 93 93 M . interval 93 92 93 82 81 82 5 . 3 Comparison against Pre - Existing Markup There is some pre - existing markup for some kinds of ref - erences and sections in the USPTO documents ( there is no such markup in the EPO documents that we have at present ) . This enabled us to also compare our automati - cally created annotations against those already in the data . At present , there is pre - existing markup only for two kinds of references , those to ﬁgures and claims , therefore these are the only ones we can compare on . Table 6 presents the pre - cision , recall and F1 results , when the pre - existing markup is used as a gold standard . It can be seen that most pre - existing references to ﬁgures and claims are identical to those obtained by our grammars , in particular for the claim refer - ence . However , there are quite a few diﬀerent annotations between the two for ﬁgure references . Through manual inspection , we found that ﬁgure refer - ences are more complicated than that for claim references . While most pre - existing annotations are correct for the sim - pler cases , e . g . , “FIG . 1” and “FIG . 5A” , there are many mistakes for more complex references such as “FIGS . 5A to 5 D” ( only“FIGS . 5A”is tagged while 5D is missed ) , “FIGS . 4B , a device” ( tagged wrongly as “FIGS . 4B , a” ) , and “FIG . 4 a” ( only “FIG . 4” is tagged ) . In contrast , our grammars produced the correct annotations in such cases . There are also slight diﬀerences in the way conjunctions are tagged , where pre - existing and automatically produced annotations diﬀer slightly , consequently lowering the accuracy results . To summarise , for reference identiﬁcation it is better to rely entirely on those produced by our system , because on 4 These unpublished results were provided to us in email communication with Matrixware staﬀ . one hand , there are mistakes and inconsistencies in the pre - existing markup , and on the other , the claim and ﬁgure references produced by our system are very reliable ( see the respective precision and recall in Table 5 . Table 6 : Comparing the pre - processing results ( as the key set ) with those in the original markups ( as the response set ) for the two types of reference on the 23 USPTO documents : the micro - average over the two sets of documents for the F - measures ( Pre - cision , Recall , F1 ) for each reference type . Annotation type Precision Recall F1 Reference . Claim 95 97 96 Reference . Figure 91 87 88 We also carried out a similar analysis of the section tags . There the pre - existing markup covers section types such as cross - reference - to - related - applications , summary - of - inve - ntion and detailed - description . We found that the ﬁrst one is quite reliable but the other two are less so . For exam - ple , one detailed - description annotation covers also the sec - tion “BEST MODE FOR CARRYING OUT THE INVEN - TION” , while another one covers not only the correct sec - tion “DETAILED DESCRIPTION OF THE INVENTION” but also the examples section . Yet another example is a summary - of - invention annotation covering the sections for technical ﬁeld , background and summary . While this might not be problematic for some applications , our goal is to pro - vide as detailed and ﬁne grained section identiﬁcation as possible , in order to enable users to search only within the desired parts . Consequently , our automatic approach cur - rently uses only the cross - reference - to - related - applications tags , plus the annotations for the sections containing bibli - ographic data , abstract , and claims . 6 . CONCLUSION This paper presented a large - scale , parallel IE system to automatically annotate USPTO and EPO documents with relevant new metadata , in order to enable richer searches by inventors and companies . This system has been tested on a 1 . 3 million documents corpus ( more than 100 GB ) with 12 parallel processes and achieves in its optimised version a data rate of 200 KB / seconds or less than 6 days of process - ing . Processing is highly parallelisable and the overall time can be reduced further by using more servers . Our system supports not only batch - mode automatic an - notation but can also be used with an interface to check / corr - ect the annotations and to search using the newly generated tags . Notably , ANNIC [ 1 ] a tool present in GATE , allows us to input semantic queries such as Find all length measure - ments . We are in the process of extending the possibilities to queries like Find a measurement greater than 10 cm or Find a measurement in the description section . We already have an ontology of units that is linked to the measurement annotations , which will allow us to normalise all measure - ments in order to make semantic queries a lot more eﬃcient and also independent of the measurement unit used ( e . g . , a query for inches can retrieve patents with units in cm ) . Our system can also be deployed with a web interface that allows a large number of annotators to correct the automatic annotations . This is done in order to create a gold standard that will be used for machine learning and also to allow fur - ther evaluation of the rule - based processing components . As machine learning IE applications can be deployed only when suﬃcient data has been annotated , this has not been possi - ble without ﬁrst developing the rule - based system describe in this paper . Another strand of our future work will be to create an eﬃcient machine learning system to improve further the precision and recall on all annotation types and introduce new ones . 7 . ACKNOWLEDGMENTS This work has been supported by a Matrixware / IRF re - search grant . 8 . REFERENCES [ 1 ] N . Aswani , V . Tablan , K . Bontcheva , and H . Cunningham . Indexing and Querying Linguistic Metadata and Document Content . In Proceedings of Fifth International Conference on Recent Advances in Natural Language Processing ( RANLP2005 ) , Borovets , Bulgaria , 2005 . [ 2 ] D . Bikel , R . Schwartz , and R . Weischedel . An Algorithm that Learns What’s in a Name . Machine Learning , Special Issue on Natural Language Learning , 34 ( 1 - 3 ) , Feb . 1999 . [ 3 ] N . Chinchor . Muc - 4 evaluation metrics . In Proceedings of the Fourth Message Understanding Conference , pages 22 – 29 , 1992 . [ 4 ] H . Cunningham . Information Extraction , Automatic . Encyclopedia of Language and Linguistics , 2nd Edition , pages 665 – 677 , 2005 . [ 5 ] H . Cunningham , D . Maynard , K . Bontcheva , and V . Tablan . GATE : A Framework and Graphical Development Environment for Robust NLP Tools and Applications . In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics ( ACL’02 ) , 2002 . [ 6 ] H . Cunningham , D . Maynard , K . Bontcheva , V . Tablan , and C . Ursu . The GATE User Guide . http : / / gate . ac . uk / , 2002 . [ 7 ] D . Day , P . Robinson , M . Vilain , and A . Yeh . MITRE : Description of the Alembic System Used for MUC - 7 . In Proceedings of the Seventh Message Understanding Conference ( MUC - 7 ) , 1998 . [ 8 ] M . Dean , G . Schreiber , S . Bechhofer , F . van Harmelen , J . Hendler , I . Horrocks , D . L . McGuinness , P . F . Patel - Schneider , and L . A . Stein . OWL web ontology language reference . W3C recommendation , W3C , Feb 2004 . http : / / www . w3 . org / TR / owl - ref / . [ 9 ] D . Hull , S . Ait - Mokhatar , M . Chuat , A . Eisele , E . Gaussier , G . Grefenstette , P . Isabelle , C . Samuelsson , and F . Segond . Language technologies and patent search and classiﬁcation . World Patent Information , 23 : 265 – 268 , 2001 . [ 10 ] A . Kiryakov . OWLIM : balancing between scalable repository and light - weight reasoner . In Proc . of WWW2006 , Edinburgh , Scotland , 2006 . [ 11 ] Y . Li , K . Bontcheva , and H . Cunningham . SVM Based Learning System For Information Extraction . In M . N . J . Winkler and N . Lawerence , editors , Deterministic and Statistical Methods in Machine Learning , LNAI 3635 , pages 319 – 339 . Springer Verlag , 2005 . [ 12 ] D . Maynard , K . Bontcheva , and H . Cunningham . Towards a semantic extraction of Named Entities . In Recent Advances in Natural Language Processing , Bulgaria , 2003 . [ 13 ] D . Maynard , V . Tablan , C . Ursu , H . Cunningham , and Y . Wilks . Named Entity Recognition from Diverse Text Types . In Recent Advances in Natural Language Processing 2001 Conference , pages 257 – 274 , Tzigov Chark , Bulgaria , 2001 . [ 14 ] B . Popov , A . Kiryakov , D . Ognyanoﬀ , D . Manov , and A . Kirilov . KIM – A semantic platform for information extraction and retrieval . Natural Language Engineering , 10 : 375 – 392 , 2004 . [ 15 ] C . van Rijsbergen . Information Retrieval . Butterworths , London , 1979 . [ 16 ] L . Wanner , R . Baeza - Yates , S . Brugmann , J . Codina , B . Diallo , E . Escorsa , M . Giereth , Y . Kompatsiaris , S . Papadopoulos , E . Pianta , G . Piella , I . Puhlmann , G . Rao , M . Rotard , P . Schoester , L . Seraﬁni , and V . Zervaki . Towards Content - oriented Patent Document Processing . World Patent Information , 30 ( 1 ) : 21 – 33 , 2008 .