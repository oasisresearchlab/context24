4 / 25 / 2019 1 Neural Machine Translation by Jointly Learning to Align and Translate KAMRAN ALIPOUR APRIL 2019 D . Bahdanau , K . Cho , Y . Bengio ( ICLR 2015 ) O UTLINE ● PROBLEM : ○ Fixed - length vector representation is a bottleneck ○ Difficult to cope with long sentences , especially when longer than the sentences in the training corpus . 2 Source Target Context Summary Encoder Decoder 4 / 25 / 2019 2 O UTLINE ● Previous work shows performance of a basic encoder – decoder deteriorates rapidly as the length of an input sentence increases . 3 B L E U s c o r e Sentence length O UTLINE ● SOLUTION : ○ An extension which learns to align and translate jointly . ○ Does NOT encode a whole input sentence into a single fixed - length vector . 4 Source Target Context Summary Encoder Decoder 4 / 25 / 2019 3 O UTLINE ● SOLUTION : ○ Encode the input into a sequence of vectors and choose a subset of them adaptively while decoding . 5 Source Target Context Summary Encoder Decoder O UTLINE ● SOLUTION : ○ For each translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated . 6 Source Target Context Summary Encoder Decoder 4 / 25 / 2019 4 O UTLINE ● SOLUTION : ○ The model predicts a target word based on the context vectors associated with source positions and all the previously generated target words . 7 Source Target Context Summary Encoder Decoder O UTLINE ● SOLUTION : ○ The model does not squash all the information of a source sentence , regardless of its length , into a fixed - length vector 8 Source Target Context Summary Encoder Decoder 4 / 25 / 2019 5 O UTLINE ● SOLUTION : ○ State - of - the - art phrase - based system on the task of English - to - French translation . 9 Source Target Context Summary Encoder Decoder P REVIOUS W ORK Phrase - based translation system ● Tuning sub - components separately ( e . g . Koehn et al . 2003 ) Neural machine translation ● Proposed by : Kalchbrenner and Blunsom ( 2013 ) Cho et al . ( 2014 ) Sutskever et al . ( 2014 ) ● Mostly encoder - decoder architectures ● Encoders and decoders for each language ● Encoder - decoder jointly trained 10 4 / 25 / 2019 6 B ACKGROUND 11 NEURAL MACHINE TRANSLATION Cho et al . ( 2014 ) & Sutskever et al . ( 2014 ) Variable - length Fixed - length C Variable - length RNN Encoder - Decoder RNN Encoder RNN Decoder RNN E NCODER - D ECODER 12 Introduced by : Cho et al . ( 2014 ) 4 / 25 / 2019 7 RNN E NCODER - D ECODER 13 Encoder : Sutskever et al . ( 2014 ) : LSTM as RNN E NCODER - D ECODER 14 Decoder : 4 / 25 / 2019 8 RNN E NCODER - D ECODER 15 Encoder - decoder jointly trained to maximize the conditional log - likelihood : RNN E NCODER - D ECODER 16 ● Better captures the linguistic regularities in the phrase table ● Indirectly explains the quantitative improvements in the overall translation performance ● Learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase . 4 / 25 / 2019 9 A LIGN AND T RANSLATE 17 New Architecture Encoder : ○ Bidirectional ( BiRNN , Schuster and Paliwal , 1997 ) ○ Annotation of each word summarizes not only the preceding words , but also the following words . E NCODER : B IDIRECTIONAL RNN FOR A NNOTATING S EQUENCES 18 BiRNN : ● Introduced by Schuster and Paliwal , 1997 ● Also successfully used for speech recognition ( Graves et al . , 2013 ) ● Consists of a forward and a backward RNN 4 / 25 / 2019 10 A LIGN AND T RANSLATE 19 A LIGN AND T RANSLATE 20 Annotations : 4 / 25 / 2019 11 A LIGN AND T RANSLATE 21 Decoder : ○ Searching through source sentence while decoding a translation Alignment Model 22 Alignment model : 4 / 25 / 2019 12 Alignment Model 23 Notes about alignment model : ● Parameterized a as a feedforward neural network ● Trained jointly with all the other components of the proposed model ● Alignment is NOT a latent variable ● Directly computes a soft alignment ● The gradient of the cost function can backpropagate through it A LIGNMENT M ODEL 24 “Expected Annotation” is the probability that the target word is aligned to , or translated from , a source word 4 / 25 / 2019 13 A LIGNMENT M ODEL 25 Attention based decoder ● Decoder decides which parts of sentence to pay attention to . ● Encoder relieved from encoding all the information in a fixed - length vector . E XPERIMENT S ETTINGS 26 Task : English to French translation Bilingual , parallel corpora provided by ACL WMT ’14 Results compared with original RNN Encoder - Decoder by Cho et al . ’14 4 / 25 / 2019 14 E XPERIMENT S ETTINGS 27 Dataset : WMT ’14 Europarl 61M words News commentary 5 . 5M UN 421M Crawled corpora 90M + 272 . 5M Total 850M E XPERIMENT S ETTINGS 28 Selected Data : 384M words Using data selection method by Axelrod et al . 2011 4 / 25 / 2019 15 E XPERIMENT S ETTINGS 29 Training Validation News - test - 2012 + news - test - 2013 Test Set News - test - 2014 from WMT ‘14 E XPERIMENT S ETTINGS 30 Training Models : ● RNN Encoder - Decoder ● RNNsearch Each model trained twice : RNNencdec - 30 RNN Encoder - Decoder 30 words RNNencdec - 50 RNN Encoder - Decoder 50 words RNNsearch - 30 RNNsearch 30 words RNNsearch - 50 RNNsearch 50 words 4 / 25 / 2019 16 E XPERIMENT S ETTINGS 31 ● 1000 hidden units in both models ● A multilayer network with a single maxout hidden layer ( Goodfellow et al . ‘13 ) ● Using a mini - batch stochastic gradient descent ( SGD ) algorithm together with Adadelta ( Zeiler 2012 ) ● Training for each model : 5 days R ESULTS 32 Model All No UNK RNNencdec - 30 13 . 93 24 . 19 RNNsearch - 30 21 . 50 31 . 44 RNNencdec - 50 17 . 82 26 . 71 RNNsearch - 50 26 . 75 34 . 16 RNNsearch - 50 * 28 . 45 36 . 15 Moses 33 . 30 35 . 63 BLEU Scores 4 / 25 / 2019 17 R ESULTS 33 R ESULTS 34 Strength of sof - alignments : ● Understands change of order ● Looks at close words for better translation ● Naturally deals with different lengths of source and target 4 / 25 / 2019 18 Conclusion 35 ● Use of a fixed - length context vector is problematic for translating long sentences ● Novel architecture that models a ( soft - ) search for a set of input words , or their annotations computed by an encoder , when generating each target word ● Better results on long sentences ● All components ( including annotations ) are jointly trained ● Performance comparable to the existing phrase - based statistical machine translation