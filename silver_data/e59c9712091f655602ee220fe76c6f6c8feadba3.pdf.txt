Demonstrating AHA : Boosting Unmodified AI’s Robustness by Proactively Inducing Favorable Human Sensing Conditions Sungjae Cho sungjaecho @ postech . ac . kr POSTECH Pohang , Gyeongbuk , South Korea Jaewoong Jang jaewoong . jang @ postech . ac . kr POSTECH Pohang , Gyeongbuk , South Korea Yoonsu Kim ∗ yoonsu16 @ kaist . ac . kr KAIST Daejeon , South Korea Inseok Hwang i . hwang @ postech . ac . kr POSTECH Pohang , Gyeongbuk , South Korea Figure 1 : An example of how AI - to - Human Actuation complements an AI sensor under perception difficulty : ( A ) two vision sensors suffering an unfavorable condition , i . e . , unable to locate the target person from their FoVs . ( B ) Content push on a proxemic display [ 7 ] triggered as an actuation to re - locate the target person . ( C ) The unfavorable condition has been resolved by the human reaction and interaction , i . e . , relocation of the target human ABSTRACT Imagine a near - future smart home . Home - embedded visual AI sen - sors continuously monitor the resident , inferring her activities and internal states that enable higher - level services . Here , as home - embedded sensors passively monitor a free person , good inferences happen inconsistently . The inferences’ confidence highly depends on how congruent her momentary conditions are to the conditions favored by the AI models , e . g . , front - facing or unobstructed . We envision new strategies of AI - to - Human Actuation ( AHA ) that boost the sensory AI’s robustness by inducing favorable condi - tions from the person with proactive actuations . To demonstrate our concept , in this demo , we show how the inference quality of the AI model changes relative to the person’s conditions and introduce ∗ Yoonsu Kim was an undergraduate student at POSTECH during a major period of this research . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . UbiComp / ISWC ’23 Adjunct , October 08 – 12 , 2023 , Cancun , Quintana Roo , Mexico © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 979 - 8 - 4007 - 0200 - 6 / 23 / 10 . https : / / doi . org / 10 . 1145 / 3594739 . 3610718 possible actuations , used in our full paper experiments , that could drive more favorable conditions for visual AIs . CCS CONCEPTS • Human - centered computing → Empirical studies in inter - action design ; Ambient intelligence ; • Computing method - ologies → Intelligent agents ; Active vision . KEYWORDS Human - AI Interaction ; IoT ; Actuation ACM Reference Format : Sungjae Cho , Jaewoong Jang , Yoonsu Kim , and Inseok Hwang . 2023 . Demon - strating AHA : Boosting Unmodified AI’s Robustness by Proactively Induc - ing Favorable Human Sensing Conditions . In Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing & the 2023 ACM International Symposium on Wearable Computing ( Ubi - Comp / ISWC ’23 Adjunct ) , October 08 – 12 , 2023 , Cancun , Quintana Roo , Mexico . ACM , New York , NY , USA , 4 pages . https : / / doi . org / 10 . 1145 / 3594739 . 3610718 1 INTRODUCTION Imagine your toddler son is playing on the floor . You just see that he is taking something into his mouth but you cannot confirm what that is as he is sitting sideways to you . You hail him ; he turns 187 UbiComp / ISWC ’23 Adjunct , October 08 – 12 , 2023 , Cancun , Quintana Roo , Mexico Cho et al . Figure 2 : Demonstration plan : ( A ) error varying by the facial orientation in the age estimation model [ 2 ] and confidence score varying by the body orientation in the pose estimation model [ 32 ] ( B ) actuation embodiments to show in the demo – a smart speaker and an interactive display that was used in the full paper [ 5 ] around . Now you see a clear view of a leftover cookie in his hand , feeling relaxed . Similarly , a kindergarten kid has just rushed to her grandfather , showing off her masterpiece drawing right in front of his eyes , as usual . The grandfather , farsighted , cannot see the details . “Wow , too close , sweetie . ” he says . “Oops , sorry grandpa . ” she moves it a little back . Like the above situations , our human - to - human perception abil - ities are often challenged by unfavorable conditions , and we often overcome such challenges by actively actuating the target towards a more favorable condition to help our perception , rather than passively waiting until the conditions become favorable . In this light , we proposed a novel strategy AI - to - Human Actu - ation ( AHA ) [ 5 ] which applies proactive actuations on the target human to induce her conditions more favorable to AIs . Through AHA , AI - powered sensors that continuously monitor the target human [ 10 , 11 , 19 , 20 , 23 , 26 , 35 ] can be complemented and be - come robust towards unfavorable conditions that happen during its inference . Figure 1 depicts an example scenario of AHA out of many that we developed . Two vision AI sensors continuously infer the target human’s health conditions , e . g . , respiratory rate , by observing his upper body . However , the target human is out of the FoV of two sensors , making their inference difficult . At this point , a proactive actuation is given , e . g . , a content push from the near proxemic dis - play . It naturally induces him to move towards the display , yielding a view favorable to the vision AI sensor . In this demo , we demonstrate the motivation of AHA and the actuation embodiments for AHA . Specifically , we show that visual AI models have favorable and unfavorable conditions to sense the target human , and introduce a few embodiments activated in smart devices , which were also used in our full paper [ 5 ] , that could handle such unfavorable conditions . 2 RELATED WORKS Broadly , AHA is positioned in the problem space of facilitating human - targeted AI to be more effective in unfavorable real - life sensing conditions . Approaches for robust AI in ML , CV , and HCI literature [ 4 , 8 , 36 ] mostly increased the complexity of the AI model or system , making it harder to run on resource - tight , IoT platforms . Unlike previous approaches , AHA adopts proactive interactions to boost the AI’s robustness . Proactive interactions have been ex - tensively studied in the HCI community , including push notifi - cations [ 12 , 14 , 16 , 24 , 25 , 27 , 28 ] , proactive speakers [ 3 , 17 , 31 ] , home robotics & automation [ 13 , 15 , 18 ] , gamifications [ 6 , 9 , 21 ] , and interactive displays [ 1 , 30 , 33 , 34 ] . These interactions engage humans to react and change the user’s state ( e . g . , start speaking , move towards the device ) . Consequently , the trend of proliferating proactive interactions shows the readiness of AHA , extending the pool of interactions that could be used as actuations . The spectrum of human - AI interaction has been largely discussed in terms of the balance between human control and computer au - tomation [ 22 , 29 ] . Mostly , these models converged in assisting hu - man tasks . Although AHA shares the key stakeholders ( i . e . , humans , AIs , and signals in between ) , AHA assists the AIs to perform better by deploying actuation but remains as transparent as possible to the human . 3 AI - TO - HUMAN ACTUATION Abstractly , three steps are needed to embody and evaluate AHA in a living space . 3 . 1 Perceiving AI’s Unfavorable Conditions Unfavorable conditions that the AI model currently suffers should be first investigated . Recognizing such information hints at select - ing the type and variables of actuations that can help the AI model overcome those conditions . For example , whether the target hu - man face is viewed sideways , and if so at what direction and angle 188 Demonstrating AHA UbiComp / ISWC ’23 Adjunct , October 08 – 12 , 2023 , Cancun , Quintana Roo , Mexico the face is turned ( e . g . , left , 30 ◦ ) , or whether the face is hidden are possible information that could be determined in the case of a face detection model . This information helps in selecting effective actuations for each unfavorable condition , e . g . , actuations triggered from the human’s right , left , or back . 3 . 2 Matching between Unfavorable Conditions and Existing Actuations In a living space , there are numerous installed sensors running an AI model and numerous actuators that are capable of trigger - ing proactive actuations . Here , there should be a method or an algorithm matching these two in an objective to relieve real - time happening AI’s unfavorable conditions with the information from the first step . In our full paper experiment , the matching was done in a Wizard - of - Oz manner with the self - defined criteria to discover such methods . Each criterion was composed of ( 1 ) an unfavorable condition , ( 2 ) sensors that suffer from that condition , ( 3 ) a candidate actuation ( s ) to trigger , and ( 4 ) the expected human reaction . 3 . 3 Evaluation : Performance Improvement and User Acceptance After the actuation is selected and triggered for the AI model , two perspectives should be considered in evaluating the strategy – per - formance improvement and user acceptance . The first perspective is to find out whether the actuation truly supported the AI model to overcome its unfavorable conditions . From this perspective , the improvement in AI inference quality and the time for the quality to surpass a moderate threshold can be considered . The second is to guarantee that the actuated person’s experience is not negatively affected due to the proactivity of actuations . In our full paper , we conducted an experiment with 20 par - ticipants to evaluate AHA . Each participant stayed at our con - structed testbed for 1 hour and five types of actuations were given in a Wizard - of - Oz manner . After the 1 hour session , 30min 1 - on - 1 semi - structured interview followed to examine the user experi - ence of each actuation type as well as the overall experience of the actuation - enabled environment . The experiment was approved by our university IRB . From the experiment , we found that 92 . 1 % actuations improved the AI model performance and such improvements happen mostly within 20 - 30 seconds . Furthermore , the trend of user experience to actuations was positive , perceiving actuation as a near - future smart home action and as a kind of care . For more detailed explanations and results , please refer to our full paper [ 5 ] . 4 DEMONSTRATION PLAN Our demo will be done in two - fold as shown in Figure 2 . We first motivate people that unfavorable conditions can happen in real - time inference , and that favorable conditions exist in the AI model . Then , we introduce a number of actuations , so that people can try them , and experience how the actuation can derive AI’s favorable conditions and how it feels to them when triggered . Specifically , we plan to prepare a display to visualize the real - time inference quality of the visual AI models deployed at the demo venue ( e . g . , age estimation , pose estimation ) where the live video feed from the installed camera is given . With this setup , we guide people to find favorable and unfavorable conditions of the AI model , for example , by changing facial angle towards the camera , varying distance to the camera , or hiding behind obstacles . We also plan to prepare a set of actuators ( e . g . , smart speakers , interactive displays ) that could alleviate the AI models’ unfavorable conditions that attendees have discovered earlier . We will introduce each actuator and trigger an actuation from the actuator , inducing people to respond and freely interact with it . The visitors to our demo should be able to have comprehensive experiences that help them assess the degree of noticeability of actuation unawarely applied to them , the degree of AI performance improvement , and the degree of positive or negative influence along with their natural routine . ACKNOWLEDGMENTS This research was supported by the National Research Foundation of Korea ( NRF ) grant funded by the Korea government ( MSIT ) ( No . 2021R1A2C200386612 ) . This research was also partly supported by the IITP grants by the MSIT , Korea ( IITP - 2020 - 0 - 01778 , RS - 2023 - 00229171 ) . REFERENCES [ 1 ] Florian Alt , Daniel Buschek , David Heuss , and Jörg Müller . 2021 . Orbuculum - Predicting When Users Intend to Leave Large Public Displays . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 1 ( 2021 ) , 1 – 16 . [ 2 ] Wenzhi Cao , Vahid Mirjalili , and Sebastian Raschka . 2020 . Rank consistent ordinalregressionforneuralnetworkswithapplicationtoageestimation . Pattern Recognition Letters 140 ( 2020 ) , 325 – 331 . [ 3 ] Narae Cha , Auk Kim , Cheul Young Park , Soowon Kang , Mingyu Park , Jae - Gil Lee , Sangsu Lee , and Uichin Lee . 2020 . Hello there ! is now a good time to talk ? Opportune moments for proactive interactions with smart speakers . Proceedings oftheACMonInteractive , Mobile , WearableandUbiquitousTechnologies 4 , 3 ( 2020 ) , 1 – 28 . [ 4 ] Yu Cheng , Bo Yang , Bo Wang , Wending Yan , and Robby T Tan . 2019 . Occlusion - aware networks for 3d human pose estimation in video . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 723 – 732 . [ 5 ] Sungjae Cho , Yoonsu Kim , Jaewoong Jang , and Inseok Hwang . 2023 . AI - to - HumanActuation : BoostingUnmodifiedAI’sRobustnessbyProactivelyInducing Favorable Human Sensing Conditions . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 7 , 1 ( 2023 ) , 1 – 32 . [ 6 ] Woohyeok Choi , Jeungmin Oh , Taiwoo Park , Seongjun Kang , Miri Moon , Uichin Lee , Inseok Hwang , Darren Edge , and Junehwa Song . 2016 . Designing interactive multiswimmer exergames : a case study . ACM Transactions on Sensor Networks ( TOSN ) 12 , 3 ( 2016 ) , 1 – 40 . [ 7 ] SaulGreenberg , NicolaiMarquardt , TillBallendat , RobDiaz - Marino , andMiaosen Wang . 2011 . Proxemic interactions : the new ubicomp ? interactions 18 , 1 ( 2011 ) , 42 – 50 . [ 8 ] Jun He , Dongliang Li , Bin Yang , Siming Cao , Bo Sun , and Lejun Yu . 2017 . Multi view facial action unit detection based on CNN and BLSTM - RNN . In 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition ( FG 2017 ) . IEEE , 848 – 853 . [ 9 ] Inseok Hwang , Youngki Lee , Taiwoo Park , and Junehwa Song . 2012 . Toward a mobile platform for pervasive games . In Proceedings of the first ACM international workshop on Mobile gaming . 19 – 24 . [ 10 ] Inseok Hwang , Youngki Lee , Chungkuk Yoo , Chulhong Min , Dongsun Yim , and JohnKim . 2019 . Towardsinterpersonalassistants : next - generationconversational agents . IEEE Pervasive Computing 18 , 2 ( 2019 ) , 21 – 31 . [ 11 ] Inseok Hwang , Chungkuk Yoo , Chanyou Hwang , Dongsun Yim , Youngki Lee , Chulhong Min , John Kim , and Junehwa Song . 2014 . TalkBetter : family - driven mobile intervention care for children with language delay . In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing . 1283 – 1296 . [ 12 ] Hyukjae Jang , Sungwon Peter Choe , Inseok Hwang , Chanyou Hwang , Lama Nachman , and Junehwa Song . 2012 . RubberBand : augmenting teacher’s aware - ness of spatially isolated children on kindergarten field trips . In Proceedings of the 2012 ACM conference on ubiquitous computing . 236 – 239 . 189 UbiComp / ISWC ’23 Adjunct , October 08 – 12 , 2023 , Cancun , Quintana Roo , Mexico Cho et al . [ 13 ] BumsooKang , InseokHwang , JinhoLee , SeungchulLee , TaegyeongLee , Youngjae Chang , and Min Kyung Lee . 2018 . My being to your place , your being to my place : Co - present robotic avatars create illusion of living together . In Proceedings of the 16th Annual International Conference on Mobile Systems , Applications , and Services . 54 – 67 . [ 14 ] Bumsoo Kang , Seungwoo Kang , and Inseok Hwang . 2021 . MomentMeld : AI - augmented Mobile Photographic Memento towards Mutually Stimulatory Inter - generational Interaction . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 16 . [ 15 ] Bumsoo Kang , Seungwoo Kang , and Inseok Hwang . 2023 . AI - driven Family Interaction Over Melded Space and Time . IEEE Pervasive Computing 22 , 1 ( 2023 ) , 85 – 94 . [ 16 ] Bumsoo Kang , Chulhong Min , Wonjung Kim , Inseok Hwang , Chunjong Park , Seungchul Lee , Sung - Ju Lee , and Junehwa Song . 2017 . Zaturi : We put together the 25th hour for you . create a book for your baby . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . 1850 – 1863 . [ 17 ] Auk Kim , Woohyeok Choi , Jungmi Park , Kyeyoon Kim , and Uichin Lee . 2018 . In - terrupting drivers for interactions : Predicting opportune moments for in - vehicle proactive auditory - verbal tasks . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 4 ( 2018 ) , 1 – 28 . [ 18 ] Wonjung Kim , Seungchul Lee , Youngjae Chang , Taegyeong Lee , Inseok Hwang , and Junehwa Song . 2021 . Hivemind : social control - and - use of IoT towards democratization of public spaces . In Proceedings of the 19th Annual International Conference on Mobile Systems , Applications , and Services . 467 – 482 . [ 19 ] Wonjung Kim , Seungchul Lee , Seonghoon Kim , Sungbin Jo , Chungkuk Yoo , In - seokHwang , SeungwooKang , andJunehwaSong . 2020 . DyadicMirror : Everyday Second - person Live - view for Empathetic Reflection upon Parent - child Interac - tion . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 3 ( 2020 ) , 1 – 29 . [ 20 ] Sanna Kuoppamäki , Sylvaine Tuncer , Sara Eriksson , and Donald McMillan . 2021 . Designing Kitchen Technologies for Ageing in Place : A Video Study of Older Adults’CookingatHome . ProceedingsoftheACMonInteractive , Mobile , Wearable and Ubiquitous Technologies 5 , 2 ( 2021 ) , 1 – 19 . [ 21 ] Haechan Lee , Miri Moon , Taiwoo Park , Inseok Hwang , Uichin Lee , and Junehwa Song . 2013 . Dungeons & swimmers : designing an interactive exergame for swimming . In Proceedings of the 2013 ACM conference on Pervasive and Ubiquitous Computing adjunct publication . 287 – 290 . [ 22 ] Jungeun Lee , Sungnam Kim , Minki Cheon , Hyojin Ju , JaeEun Lee , and Inseok Hwang . 2022 . SleepGuru : Personalized Sleep Planning System for Real - life ActionabilityandNegotiability . In Proceedingsofthe35thAnnualACMSymposium on User Interface Software and Technology . 1 – 16 . [ 23 ] Youngki Lee , Younghyun Ju , Chulhong Min , Seungwoo Kang , Inseok Hwang , and Junehwa Song . 2012 . Comon : Cooperative ambience monitoring platform with continuity and benefit awareness . In Proceedings of the 10th international conference on Mobile systems , applications , and services . 43 – 56 . [ 24 ] Chulhong Min , Saumay Pushp , Seungchul Lee , Inseok Hwang , Youngki Lee , Seungwoo Kang , and Junehwa Song . 2014 . Uncovering embarrassing moments in in - situ exposure of incoming mobile messages . In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing : Adjunct Publication . 1045 – 1054 . [ 25 ] VarunMishra , FlorianKünzler , Jan - NiklasKramer , ElgarFleisch , TobiasKowatsch , and David Kotz . 2021 . Detecting receptivity for mHealth interventions in the natural environment . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 2 ( 2021 ) , 1 – 24 . [ 26 ] Fatemeh Noroozi , Ciprian Adrian Corneanu , Dorota Kamińska , Tomasz Sapiński , Sergio Escalera , and Gholamreza Anbarjafari . 2018 . Survey on emotional body gesture recognition . IEEE transactions on affective computing 12 , 2 ( 2018 ) , 505 – 523 . [ 27 ] Martin Pielot , Bruno Cardoso , Kleomenis Katevas , Joan Serrà , Aleksandar Matic , and Nuria Oliver . 2017 . Beyond interruptibility : Predicting opportune moments to engage mobile phone users . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 1 , 3 ( 2017 ) , 1 – 25 . [ 28 ] MeeraRadhakrishnan , DarshanaRathnayake , OngKoonHan , InseokHwang , and Archan Misra . 2020 . ERICA : enabling real - time mistake detection & corrective feedback for free - weights exercises . In Proceedings of the 18th Conference on Embedded Networked Sensor Systems . 558 – 571 . [ 29 ] Ben Shneiderman . 2020 . Human - centered artificial intelligence : Reliable , safe & trustworthy . International Journal of Human – Computer Interaction 36 , 6 ( 2020 ) , 495 – 504 . [ 30 ] Miaosen Wang , Sebastian Boring , and Saul Greenberg . 2012 . Proxemic peddler : a public advertising display that captures and preserves the attention of a passerby . In Proceedings of the 2012 international symposium on pervasive displays . 1 – 6 . [ 31 ] Jing Wei , Tilman Dingler , and Vassilis Kostakos . 2021 . Understanding User Perceptions of Proactive Smart Speakers . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 4 ( 2021 ) , 1 – 28 . [ 32 ] Yuxin Wu , Alexander Kirillov , Francisco Massa , Wan - Yen Lo , and Ross Girshick . 2019 . Detectron2 . https : / / github . com / facebookresearch / detectron2 . [ 33 ] Chungkuk Yoo , Inseok Hwang , Seungwoo Kang , Myung - Chul Kim , Seonghoon Kim , Daeyoung Won , Yu Gu , and Junehwa Song . 2017 . Card - stunt as a service : Empowering a massively packed crowd for instant collective expressiveness . In Proceedings of the 15th Annual International Conference on Mobile Systems , Applications , and Services . 121 – 135 . [ 34 ] Chungkuk Yoo , Inseok Hwang , Eric Rozner , Yu Gu , and Robert F Dickerson . 2016 . Symmetrisense : Enabling near - surface interactivity on glossy surfaces using a single commodity smartphone . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 5126 – 5137 . [ 35 ] Chungkuk Yoo , Seungwoo Kang , Inseok Hwang , Chulhong Min , Seonghoon Kim , Wonjung Kim , and Junehwa Song . 2019 . Mom , I see You Angry at Me ! Designing a Mobile Service for Parent - child Conflicts by In - situ Emotional Empathy . In Proceedings of the 5th ACM Workshop on Mobile Systems for Computational Social Science . 21 – 26 . [ 36 ] Tianshu Zhang , Buzhen Huang , and Yangang Wang . 2020 . Object - occluded human shape and pose estimation from a single color image . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 7376 – 7385 . 190