i Artificial intelligence technologies to support research assessment : A review Supplementary Report about the Responsible Use of Technology - Assisted Research Assessment June 2022 Authors Kayvan Kousha and Mike Thelwall Statistical Cybermetrics and Research Evaluation Group , University of Wolverhampton , UK . ii Table of Contents Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv 1 . Predicting journal article citation counts or quality from article text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . Article title and citation impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . 1 . Article title length and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . 2 . Non - alphanumeric title characters and citation impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1 . 1 . 3 . Other characteristics in titles and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1 . 2 . Article length and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1 . 3 . Abstract length and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1 . 4 . Article readability ( abstract or full text ) and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1 . 5 . Cited references and citation impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1 . 6 . Other article features and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2 . Predicting journal article citation counts from metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2 . 1 . The relationship between the number of authors and citation counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2 . 2 . The relationship between international collaboration and citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2 . 3 . The relationship between institutional collaboration and citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2 . 4 . The relationship between journal impact factors and citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2 . 5 . Author publication and citation records and article citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2 . 6 . Predicting the quality of journal articles based on machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3 . Predicting or assessing journal article quality scores from metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3 . 1 . UK RAE / REF scores and bibliometric indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3 . 2 . Peer review and bibliometrics in other countries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4 . Assessing the accuracy of score predictions for individual documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5 . Availability of relevant public datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5 . 1 . Sources of open citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5 . 2 . Google Scholar citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5 . 3 . Google Books Citations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5 . 4 . Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5 . 5 . OpenCitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5 . 6 . Sources of alternative metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5 . 6 . 1 . Social media indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5 . 6 . 2 . Other Sources of Online Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 6 . Transparency in technology assisted assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6 . 1 . Transparent data sources and processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 iii 6 . 2 . Transparent research indicator calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6 . 3 . Transparent AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6 . 4 . Transparent AI and the REF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 7 . Sources of bias in technology assisted assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 7 . 1 . Algorithmic bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 7 . 2 . Bibliometric bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 7 . 3 . Peer review bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 7 . 4 . Bias in technology assisted assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 8 . Field categorisations for journal articles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 8 . 1 . Journal - based field categorisations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 8 . 2 . Article - based field classification systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 8 . 3 . Dimensions document categorisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 9 . Predicting journal article citation counts or output quality from open review text . . . . . . . . . . . . . . . . 54 9 . 1 . Publons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9 . 2 . Faculty Opinions ( formerly F1000Prime ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 9 . 3 . MDPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 10 . Publishing quality control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 10 . 1 . Tools to recommend journals for manuscripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 10 . 2 . Tools to automate editorial process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 11 . Allocation of research outputs to appropriate reviewers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 11 . 1 . Tools for reviewer selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 12 . Conclusions and recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 iv Executive Summary This document provides literature review for The Responsible Use of Technology - Assisted Research Assessment project that was commissioned by the This study was funded by the four UK higher education funding bodies as part of the Future Research Assessment Programme ( https : / / www . jisc . ac . uk / future - research - assessment - programme ) to assess how technology , in the form of Artificial Intelligence ( AI ) , can help research evaluation in the future , especially for the Research Excellence Framework ( REF ) . Here , AI is essentially software that automates complex tasks . The literature review identifies indicators that associate with higher impact or higher quality research from article text ( e . g . , titles , abstracts , lengths , cited references and readability ) or metadata ( e . g . , the number of authors , international or domestic collaborations , journal impact factors and au thors’ h - index ) . This includes studies that used machine learning techniques to predict citation counts or quality scores for journal articles or conference papers . The literature review also includes evidence about the strength of association between bibliometric indicators and quality score rankings from previous UK Research Assessment Exercises ( RAEs ) and REFs in different subjects and years and similar evidence from other countries ( e . g . , Australia and Italy ) . In support of this , the document also surveys studies that used public datasets of citations , social media indictors or open review texts ( e . g . , Dimensions , OpenCitations , Altmetric . com and Publons ) to help predict the scholarly impact of articles . The results of this part of the literature review were used to inform the experiments using machine learning to predict REF journal article quality scores , as reported in the AI experiments report for this project . The literature review also covers technology to automate editorial processes , to provide quality control for papers and reviewers ’ suggestion s , to match reviewers with articles , and to automatically categorise journal articles into fields . Bias and transparency in technology assisted assessment are also discussed . Recommendations : In addition to the analysis of inputs for the AI system to predict REF journal article scores , as discussed in the main report , the following recommendations are made based on the literature review . 1 . Implement a system to recommend sub - panel members to review outputs . This would likely be based on the ORCIDs of sub - panel members matching their Scopus / Web of Science / Dimensions / etc . profiles , then using text mining to assess the similarity of their outputs with each sub - panel output to be assessed . The text mining might use article titles , abstracts , field classifications and references . 2 . Build for the long - term implementation of quality control systems for academic articles by recommending that preprints of outputs for the next REF are saved in format suitable for text mining . Ideally , this would be in a markup format , such as XML , rather than PDF . This will also help longer - term AI systems for predicting REF journal article scores with article full text processing . At the end of the next REF , a future technology programme could then investigate the potential for full text mining for quality control purposes ( e . g . , checking statistics , plagiarism checks ) . 3 . Build for the long - term exploitation of open peer review by , at the end of the next REF , calling for a review of current progress in the use of AI to exploit open peer review to assess article quality . Whilst open peer review should not be used as an input because it can be too easily exploited , investigations into its properties might shed useful light on aspects of quality identified by reviewers . Research into this is likely to occur over the next few years , and a v review of it near the next REF might provide useful insights for both future AI and future human peer review guidelines for sub - panel members . 4 . In the next REF , collate information on inter - reviewer agreement rates within sub - panels for outputs scored before cross - checking between reviewers . Use this to assess the human level agreement rates ( for all output types ) to use as a benchmark for score prediction AI systems . 5 . In the tender for bibliometrics and AI for the next REF ( if used ) , mention the importance of accurate classification for bibliometric indicators , including for the percentile system currently used . 6 . Warn sub - panel members of the potential for small amounts of bias in the bibliometric data and AI ( if used ) and continue with the anti - bias warnings / training employed in REF2021 . 1 1 . Predicting journal article citation counts or quality from article text Text mining approaches for predicting journal article citation counts or article quality have been applied to ( a ) the article title , abstract and keywords , ( b ) the article full text , or ( c ) the text of reviews of the article . Although not a heavily researched area , some studies have also investigated the readability of articles , often showing that more readable articles ( or abstracts ) tend to be more cited . 1 . 1 . Article title and citation impact Interesting or informative titles may attract the attention of other researchers , making the articles more likely to be read and then cited . Many studies have investigated the relationship between different characteristics of article titles ( e . g . , length , readability , and the presence of punctuation ) and their subsequent citation counts in various subject areas . 1 . 1 . 1 . Article title length and citation counts Investigations of the relationship between citation counts and article title length , measured in words or characters , have generated mixed results for unknown reasons so there is not a simple and universal relationship . The following studies analysed relatively few articles from small sets of journals . Longer titles associate with more citations : One simple study gathered the 25 most cited and 25 least cited articles from three medical journals ( Lancet , BMJ and Journal of Clinical Pathology ; total n = 150 ) published in 2005 , finding that number of title words positively correlates ( rho = 0397 ) with article citations ( Jacques & Sebire , 2010 ) . Similarly , a study of 9 , 031 articles published in 22 medical ( e . g . , Lancet and JAMA ) and multidisciplinary ( e . g . , Science and Nature ) journals in 2005 also found that articles with longer titles either as measured in characters ( rho = 0266 ) or in words ( rho = o244 ) received more citations and this positive association was more common in high impact journals ( 7 of 8 journals ) ( Habibzadeh & Yadollahie , 2010 ) . Insufficient evidence of an association between title length and citations : In contrast to the above , a paper about 302 research articles published in the journal Addictive Behaviors in 2007 found no correlation between the number of title words and citation counts ( Rostami , Mohammadpoorasl , & Hajizadeh , 2014 ) . Similarly , no meaningful association was found between title length and citation counts for 1 , 825 articles published during 1990 to 2002 in five major marketing journals ( Stremersch , Verniers & Verhoef , 2007 ) . An investigation of 2 , 172 open access articles published in six PLoS ( Public Library of Science ) journals in 2007 also found no significant correlation between title length and citations ( Jamali & Nikzad , 2011 ) . Shorter titles associate with more citations : In psychology , a study of 258 articles from 40 journals showed that articles with shorter titles received more citations ( Subotic & Mukherjee , 2014 ) . Similarly , an investigation of 423 research articles published in 2008 from BioMed Central ( BMC ) and Public Library of Science ( PLoS ) journals found a negative association ( albeit very weak , r = - 0 . 104 ) between title length ( number of characters ) and article citations ( Paiva et al . , 2012 ) . All the above studies investigated relatively few journals from a single year . The results - positive , negative or no association between title length and citation counts – do not reveal a simple pattern in terms of the disciplines covered ( e . g . , biomedical studies found both positive and negative associations ) and hence it does not seem possible to generalise them into a general rule for the relationship between title length on article citations . Several large - scale studies have also investigated 2 the relationship between article titles lengths and citation counts . These have different problems in that any differences found could be due to journal or topic style variations . Time differences in associations between title length and citations : A study of 302 , 048 Web of Science economics articles from 1956 – 2012 found that correlations between title length and citation counts were negative between 1956 and 2000 but positive after 2000 ( Guo et al . 2018 ) . This result is partly corroborated by another large report on 140 , 000 highly cited Scopus papers from 2007 - 2013 ( 20 , 000 papers in each year ) . It found that highly cited articles with shorter titles tend to attract more citations ( negative correlation with title length ) ( Letchford , Moat , & Preis , 2015 ) . Supporting the correlation found in the second half of the Guo et al . investigation , a large study of 1 . 3 million articles published in 2012 found a weak positive correlation ( rho = 0 . 142 ) between the number of characters in title and citation counts ( Haustein , Costas , & Larivière , 2015 ) , although no other large - scale study seems to have reported changes over time in the relationship between title length and citations . Thus , it seems that there may be a general trend for articles with longer titles to be slightly more cited overall now , reversing an earlier trend . This interdisciplinary finding could be a second order effect of disciplinary differences in title lengths and citation rates , however . Disciplinary differences in associations between title length and citations : A study of articles published in journals with the highest impact factors during 1996 - 2005 found that articles with longer titles received fewer citations in both Sociology ( n = 2 , 016 ; r = - 0 . 046 ) and Applied Physics ( n = 23 , 676 ; r = - 0 . 089 ) , but the reverse in General Medicine ( n = 6 , 957 ; r = - 0 . 166 ) ( Van Wesel , Wyatt , & ten Haaf , 2014 ) . For Web of science articles in Biology and Biochemistry ( n = 16 , 058 ) and Social Sciences ( n = 15 , 932 ) from 2000 - 2009 , title length associates with more citations in both subject areas according to both regression models and Spearman correlations ( rho = 0 . 021 and 0 . 014 respectively ) but no association was found in Chemistry ( n = 16 , 378 ) ( Didegah & Thelwall , 2013b ) . Journal impact differences in associations between title length and citations : One of the largest studies investigating the association between article length and citations used 4 . 3 million papers in articles published 1995 - 2004 in 1500 large journals , finding that for highly cited journals , shorter titles tend to be more cited , whereas for the remaining journals , longer titles tend to be more cited ( Sienkiewicz & Altmann , 2016 ) . The former result may be due to strict title length restrictions in the most highly cited journals within the highly cited set . Despite all the above studies , it is still not fully clear if ( a ) shorter titles are more common in high impact journals than in other journals or ( b ) if longer ( or shorter ) title lengths have a citation advantage in some fields because they increase readership and therefore are more likely to be subsequently cited . The contradictory findings might be related to other factors related to the studied data sets such as a focus on highly cited articles or journals ( Subotic & Mukherjee , 2014 ; Sienkiewicz & Altmann , 2016 ) , different publication years examined because article title lengths have increased over time ( Lewison & Hartley , 2005 ; Guo et al . 2015 ; Gnewuch & Wohlrabe , 2017 ) , differing document types ( e . g . , see Soler , 2007 ) or other factors , such as changing editorial policies regarding title lengths . 1 . 1 . 2 . Non - alphanumeric title characters and citation impact There is evidence that non - alphanumeric characters in articles can have an impact on citation counts , presumably because they reflect successful article styles , such as asking a question . 3 Colons and hyphens in titles associate with more citations : A colon or a hyphen in an article title may help to make it more readable or may help to express more complex information . These may associate with more citations . For instance , an early study of 150 articles from three medical journals ( Lancet , BMJ and Journal of Clinical Pathology ; total n = 150 ) showed that colons was significantly more frequent in highly cited articles ( Jacques & Sebire , 2010 ) . Colons and hyphens in titles associate with less citations : Two small studies ( n = 423 and n = 2 , 172 ) found that biomedical article titles with colons or hyphens in their titles received fewer citations than titles that did not have these characters ( Paiva et al . , 2012 ; Jamali & Nikzad , 2011 , respectively ) . The difference from the above set is presumably the wider collection of articles analysed . Question marks in titles associate with more citations : A paper about 312 , 879 Web of Science articles in economics published between 1980 and 2015 in 430 journals found non - alphanumeric characters in article titles associated with increased their citation impact , with question marks having the strongest apparent effect : 1 . 64 extra citations ( Gnewuch & Wohlrabe , 2017 ) . Similarly , a recent large - scale study of about 2 million Web of Science journal articles and conference proceedings papers ( 1945 to 2014 ) in Computer Science also found that articles with titles ending with a question mark ( n = 5 , 682 ) received 16 % more citations than articles not asking questions ( n = 957 , 837 ) ( Fiala , Král , & Dostal , 2021 ) . Non - alphanumeric characters in titles associate with more citations : A large - scale study of 5 % of all Web of Science articles from 1999 - 2008 ( n = 642 , 807 ) found that 68 % had at least one out of 29 non - alphanumeric characters in their titles , with hyphens , colons , and commas being the most common . In general , articles with non - alphanumeric characters in their titles had higher field - normalised citation impact than titles with only alphanumeric characters . However , there were disciplinary differences , and this association was positive in Clinical Medicine , negative in Biological Sciences , and no significant association was found in Agriculture and Food Science ( Buter and van Raan 2011 ) . 1 . 1 . 3 . Other characteristics in titles and citation counts Several other studies in different fields have suggested that using humour ( Sagi & Yechiam 2008 ) , unusual words that are rarely used in other titles in the same fields ( Thelwall , 2017c ) , stylistic cues such as metaphor or alliteration ( Keating et al . , 2022 ) and quoted or direct speech ( Pearson , 2021 ) in research article titles can have a significant negative association with citation counts . 1 . 2 . Article length and citation counts A hypothesis tested by several studies is that longer papers tend to attract more citations . The assumption is that longer articles provide more information or analysis about the investigated topic and therefore contain more citeable content . Although most findings have been positive , some prestigious journals require short articles and so the relationship is not universal . All studies reviewed here measured article length using the total number of pages . Nevertheless , page counts may depend on page layouts and printing formats ( e . g . , columns and font size ) , which vary between journals and over time . Although word counts could be a better indicator to measure article length , the article full text is needed , and this could be more difficult for a large - scale analysis and when using PDF version of articles . 4 Longer articles are more cited : An investigation of 13 , 125 articles in immunology and 17 , 083 articles in Surgery found a weak but significant correlation ( rho = 0 . 286 and 0 . 335 respectively ) between page counts and citations per article ( Weale , Bailey , & Lear , 2004 ) . A paper about 26 , 088 articles published in 32 ecology Web of Science indexed journals ( 2009 - 2012 ) found that both article page and reference counts positively correlated with citation counts , suggesting that longer articles attract more citations ( Fox , Paine , & Sauterey , 2016 ) . Similarly , a large - scale study of research articles published in The New England Journal of Medicine ( n = 27 , 305 ) , The Journal of the American Medical Association ( n = 42 , 733 ) and The Lancet ( n = 65 , 525 ) from their formation to 2016 found that the length of articles ( as measured by the number of pages ) published by these journals had increased over time and longer articles tended to receive more citations ( Lyu & Wolfram , 2018 ) . A multidisciplinary study of articles from Sociology ( n = 2 , 016 ) , Applied Physics ( n = 23 , 676 ) and General Medicine ( n = 6 , 957 ) found significant positive correlations between page counts and citation counts ( Van Wesel , Wyatt , & ten Haaf , 2014 ) . Similarly , another multidisciplinary study of Web of Science articles found that the size of the paper ( number of pages ) positively associated with citation counts for articles in Biology & Biochemistry ( n = 44 , 248 ; R 2 = 0 . 666 ) , Chemistry ( n = 97 , 177 ; R 2 = 0 . 355 ) , Mathematics ( n = 20 , 127 ; R 2 = 0 . 0864 ) and Physics ( n = 64 , 614 ; R 2 = 0 . 615 ) ( Vieira & Gomes , 2010 ) . A recent report about 1 , 561 articles published in five economics journals during 2010 - 2014 found a 1 % increase in article page count associated with 0 . 56 % more Google Scholar citations ( Hasan & Breunig , 2021 ) . A positive association , albeit weak ( rho = 0194 ) , between article page counts and citation counts has been also confirmed in a large study of 1 . 3 million articles published in 2012 ( Haustein , Costas , & Larivière , 2015 ) . Other relatively small - scale studies ( see also the summary section ) have also reported significant low to medium positive correlations between article lengths ( all measured by the number of pages ) and citation counts in psychology ( Haslam et al . , 2008 , n = 308 ) , psychiatry ( Hafeez , Jalal , & Khosa , 2019 , n = 545 ) , medicine ( Falagas et al . 2013 , n = 196 ) , management ( Mingers & Xu 2010 , n = 696 ) and social sciences ( Hodge et al . , 2017 , n = 3 , 066 ) . Finally , a random effects meta - analysis study found a moderate positive overall correlation ( r = 0 . 310 ) between article length and citations ( Xie et al , 2019 ) . Longer articles are not more cited : One investigation of Biology and Biochemistry ( n = 16 , 058 ) and Social Sciences ( n = 15 , 932 ) articles from 2000 - 2009 found no significant associations between article page and citation counts ( Didegah & Thelwall , 2013b ) . 1 . 3 . Abstract length and citation counts Abstracts have become near - universal for journal articles over the past half century ( Thelwall & Sud , 2022 ) . They help potential readers to understand the topic and results of an article efficiently before they read the full article . Presumably , informative and reliable abstracts can help relevant research to be quickly identified . Many relevant studies have investigated abstract length , structure or readability . This section focuses on abstract length . Longer abstracts tend to be more cited : One of the early studies of The Lancet articles during 1997 – 1999 found that the median number of words in abstracts was 2 . 5 - 7 higher in articles with the most citations compared with articles with the least citations ( Kostoff , 2007 ) . Such large differences in abstract lengths seem unlikely in the current era of greater journal format standardisation , however . A large multidisciplinary study of one million abstracts from eight subject areas showed that longer abstracts and more sentences in abstracts associated with more citations in all fields , although in Mathematics and Physics abstracts with shorter sentences attracted more citations ( Weinberger , Evans , & Allesina , 2015 ) . Similarly , longer abstracts significantly correlate with more citations in 5 Biology and Biochemistry ( n = 16 , 058 ) and Social Sciences ( n = 15 , 932 ) and Chemistry ( n = 16 , 378 ) , although this association is very weak , rho = 0 . 153 , 0 . 122 and 0 . 148 respectively ( Didegah & Thelwall , 2013b ) . A very large study of 4 . 3 million papers from over 1500 journals also showed that the length of abstracts positively correlated with citation counts in nearly all of the journals ( Sienkiewicz & Altmann , 2016 ) . Positive associations between citation counts and abstract length might be a statistical side - effect of a minority of minor articles having very short abstracts , however . These could be errors ( e . g . , corrections published as articles ) , or short articles or comments with a few summary sentences instead of a detailed abstract . Shorter abstracts tend to be more cited : In contrast to the above studies , another large - scale investigation of 300 , 000 highly cited articles between 1999 and 2008 ( 30 , 000 papers per year ) found that articles with shorter abstracts received more citations , with this pattern being found at the journal level . The authors argued that some high impact journal might limit abstract size ( e . g . , 125 words for the journal Science ) and hence the selection of highly cited papers could influence the result ( Letchford , Preis , & Moat , 2016 ) . Keyword repetition associates with more citations : One study introduced an abstract ratio indicator ( the sum of repetition of keywords in abstract divided by abstract length ) , finding that it statistically correlates with citation counts for 5 , 875 articles in Education ( Sohrabi & Iraj , 2017 , p . 250 ) . Keyword repetition may suggest a narrower focus for a study or an emphasis on a key message . Another study tested five keyword popularity features , finding that keyword popularities can more effectively predict highly cited papers ( n = 746 articles from 46 journals in marketing and MIS ) than can author ( author’s h - index , publications or citations ) and journal ( e . g . , journal impact factor and SCImago ) features ( Hu et al . , 2020 ) . 1 . 4 . Article readability ( abstract or full text ) and citation counts Several studies have investigated whether more readable abstracts associate with higher citation rates , mostly finding the opposite . There are many ways of measuring readability , such as the relative frequency of rare or long words , with no measure being recognised as the best . Articles with more readable abstracts are less cited : Using Flesch Readability scores to assess the readability of abstracts , a study of 264 , 156 articles from five American universities during 2000 – 2009 found that abstracts with more difficult language ( e . g . , with scientific or technical terms ) were cited more than articles with easier abstracts to read , ranging from r = – 0 . 469 to – 0 . 821 ( Gazni , 2011 ) . Abstract readability , as measured by the Flesch reading ease score , was found to significantly correlate , albeit weak ( rho = 0 . 073 ) , with fewer citations in Biology & Biochemistry and no significant relationship was found between the readability of abstracts and citation counts in Social Sciences and Chemistry ( Didegah & Thelwall , 2013b ) . Another study using the full text of 1 , 825 articles from five major marketing journals during 1990 to 2002 also found that Flesch reading ease scores negatively correlated ( r = 0 . 02 ) with citations ( Stremersch , Verniers & Verhoef , 2007 ) . Using the Coh - Metrix analysis tool to assess the difficulty of written text based on computing computational cohesion and coherence metrics , a more recent study of 10 , 000 highly cited and 10 , 000 uncited English language research articles published during 2008 - 2017 across 22 subject areas found that abstracts of highly cited articles contained more complex , difficult and professional terms . The highly cited articles also had more adjectives , adverbs , conjunctions and personal pronouns and longer sentences , making them less readable compared with abstracts of uncited articles ( Hu , Wang , & Deng , 2021 ) . Using the 6 Python package Readability 0 . 3 . 1 , a recently published study of 135 , 502 abstracts from research articles on 12 emerging technologies ( e . g . , artificial intelligence , big data , and virtual reality ) by the end of 2020 found that abstracts had become more difficult to understand over time across all emerging technology topics and articles with more complex abstracts received fewer citations or remained uncited ( Ante , 2022 ) . A large - scale paper about over 4 . 3 million papers from over 1500 journals also found that the text complexity of abstracts positively correlated with citation counts ( Sienkiewicz & Altmann , 2016 ) . Articles with more readable abstracts are more cited : In contrast to the above investigations , a study of 3 , 229 articles published in the journal Economics Letters during 2003 – 2012 found a positive correlation between Flesch Reading Ease Score of abstracts and citation counts , suggesting that clearer writing can potentially enhance the impact of economic research ( Dowling , Hammami , & Zreik , 2018 ) . Economics might be an exception to the general rule in science in this regard . Articles with more readable abstracts are similarly cited : Analysing the full text of articles in Biology ( n = 36 , 400 ) and Psychology ( n = 1 , 797 ) , a study found no practical significant correlation between the linguistic complexity of scientific articles and their citation impact ( Lu et al . , 2019 ) . Articles with structured abstracts are more cited : Structured abstracts ( usually with sub - headings such as background , method , aim or results ) could be easier to read than traditional abstracts ( Hartley & Sydes , 1997 ) . In support of this , a small study found weak but positive correlation ( rho = 0 . 03 ) between structured abstracts and citation counts ( n = 545 ) from six high impact psychiatry journals ( Hafeez , Jalal , & Khosa , 2019 ) . Articles with structured abstracts are less cited : Structured abstracts negatively correlated with citation counts in another study of 757 clinical articles from medical journals ( Lokker et al . , 2008 ) . 1 . 5 . Cited references and citation impact Citing more references in articles may be one of the factors influencing the scholarly impact of articles and make them more visible to researchers using citation tracing in citation databases such as the Web of Science , Scopus and Google Scholar . Longer articles with more content may also tend to have more references and be cited more . To investigate this , many studies have assessed features of cited references and their association with citation metrics . Articles with more references are more cited : A large study of 226 , 166 Web of Science articles from 2004 in Biology & Biochemistry ( n = 44 , 248 ; R 2 = 0 . 917 ) , Chemistry ( n = 97 , 177 ; R 2 = 0 . 858 ) , Mathematics ( n = 20 , 127 ; R 2 = 0 . 799 ) and Physics ( n = 64 , 614 ; R 2 = 0 . 846 ) found that reference counts correlate positively with citation counts ( Vieira & Gomes , 2010 ) . Another more recent study of articles published in seventeen ecological journals between 1997 and 2017 ( n = 50 , 878 ) confirmed that articles with more references are more cited ( Mammola et al . , 2021 ) . An analysis of 757 clinical articles from 105 medical journals published from January to June 2005 , finding that reference counts and other factors ( e . g . , number of authors ; indexing in numerous databases ) associate with increased citations ( Lokker et al . , 2008 ) . Other relatively small - scale studies in Artificial Intelligence ( Xiao & Jiang , 2020 , n = 9 , 458 ) , psychology ( Haslam et al . , 2008 , n = 308 ) , psychiatry ( Hafeez , Jalal , & Khosa , 2019 , n = 545 ) , library and information science ( Yu et al . , 2014 , n = 1 , 025 ) and management ( Antonakis et al . , 2014 , n = 776 ) also reported low to medium correlations between reference counts and citation counts for articles ( see also Table 1 ) . 7 Articles with more recent references are more cited : Using quantile regression and normalisation of all variables , a very large study of 955 , 663 Web of Science research articles published in 2009 found that more references and more recent references in academic articles positively correlate with their citation impact ( Ahlgren , Colliander & Sjögårde , 2018 ) . A report on 1 , 395 articles published in 2000 across five science and one engineering subjects ( n = 230 – 240 in each field ) found that the recency of cited references , as measured by the Price index ( percentage of the references within five years before the publication year of the article ) , had the strongest association with the citation counts , followed by the number of references ( Onodera & Yoshikane , 2015 ) . Articles with more high impact references are more cited : There is substantial evidence that articles with high impact cited references tend to be more cited . For instance , a very large - scale study of 780 , 049 articles from The Science Citation Index Expanded and The Social Sciences Citation Index 2002 - 2003 found a significant correlation between the citation counts of articles and the citation impact of their cited references ( the number of times the article’s references were cited ) , suggesting that an academic article with highly cited references is more likely to become more cited , although there were disciplinary differences ( Boyack & Klavans , 2005 ) . Later studies confirmed that reference impact could be an indicator of future citations . A study of over 1 . 6 million articles from Scopus in 2007 found that both the number and citation impact of the cited references of articles correlated with their citation counts ( Lancho - Barrantes , Guerrero - Bote , & Moya - Anegon , 2010 ) and another paper about 1 , 765 chemical papers calculated the h - index for the cited references of each article , finding a statistically significant correlation with citation counts ( Bornmann , Schier , Marx , & Daniel , 2012 ) . From a related perspective , an investigation of 7 , 749 articles published in 105 journals related to Internet studies found that the authoritativeness of the cited references ( the proportion of highly cited references among total cited references in the topic ) had a significant positive correlation with citation counts ( γ = 0 . 988 , p < 0 . 001 ) ( Peng & Zhu , 2012 ) . For 50 , 162 articles in nanoscience and nanotechnology journals published 2007 - 2009 , the number of cited references and their average citation impact significantly correlated with the article s’ citation counts ( Didegah & Thelwall , 2013a ) . Similar associations were found for articles in Biology & Biochemistry ( n = 16 , 058 ) , Social Sciences ( n = 15 , 932 ) and Chemistry ( n = 16 , 378 ) published during 2000 - 2009 ( Didegah & Thelwall , 2013b ) . Articles with more international references are more cited : There is evidence that the internationality of cited references significantly correlates with citation counts from a study of 50 , 162 articles in nanoscience and nanotechnology from 2007 - 2009 ( Didegah & Thelwall , 2013a ) . 1 . 6 . Other article features and citation counts This section reviews article features that have not been extensively investigated for associations with citation counts . Images in articles : Analysing over 4 . 8 million figures from 650 , 000 PubMed articles , higher - impact articles , as measured by article - level Eigenfactor , had more diagrams per page and a higher proportion of diagrams but a lower proportion of photos ( Lee , West , & Howe , 2017 ) . Review articles are more cited : Review articles tend to be more cited than other research articles , although there are some disciplinary differences ( e . g . , Aksnes , 2006 ; Colebunders , Kenyon , & Rousseau , 2014 ; for a review see Blümel & Schniedermann , 2020 ) . For instance , a very large - scale study of 14 . 2 million records from Science Citation Index Expanded database during 2000 – 2015 across 8 35 science subject areas found that reviews received 1 . 3 to 6 . 7 times more citations than standard research articles , depending on the subject area ( e . g . , much higher in Engineering Electrical Electronic , Chemistry Multidisciplinary , Physics Applied and Materials Science than Oncology , Radiology Nuclear Medicine , Surgery , and Mathematics ( Miranda & Garcia - Carpintero , 2018 ) . 9 Table 1 . Studies associating journal article citation counts with information extracted from article text . Factors Study No . of articles Dataset Subject Association with citation Main result T h e l e n g t h o f a r t i c l e t i t l e a nd c i t a t i o n c o un t s Jacques & Sebire , 2010 150 25 most cited and 25 least cited articles form the Lancet , BMJ J Clin . Pathology in 2005 . Medical Sci . Positive Words in titles significantly correlated with citation counts ( rho = 0 . 397 , p = 0 . 004 ) . Paiva et al . , 2012 423 Articles from the BioMed Central and PLoS journals ( n = 19 ) published in 2008 . Medical Sci . Biomedical Negative Articles with shorter titles had more citations than those with longer titles ( r = - 0 . 104 , p = 0 . 032 ) . The number of title characters was a statistically significant predictor of citation counts ( F = 7 . 581 , p = 0 . 001 ) . Habibzadeh & Yadollahie , 2010 9 , 031 Articles from 22 medical and multidisciplinary journals in 2005 with high , medium , and low journal impact factors . Medical Sci . Biomedical Multidiscip . Positive Articles with longer titles received more citations either measured in character ( rho = 0 . 266 ) or in words ( rho = 0 . 244 ) both at p < 0 . 001 . However , this association was more common in high impact journals ( 7 of 8 ) than other journals ( 2 of 14 ) . Rostami et al . , 2014 302 Articles published in the journal Addictive Behaviors in 2007 . Psychology No association The number of words in the title was not correlated with citation counts . Stremersch et al . , 2007 1 , 825 Articles from five major marketing journals during 1990 to 2002 Marketing No association The number of words in the title was not correlated with citation counts . Jamali & Nikzad , 2011 2 , 172 Articles from six PLoS ( Public Library of Science ) journals in 2007 . Biomedical ( excl . PLoS One , ) No association No significant correlation found between title lengths and citation counts . Guo et al . 2018 302 , 048 Web of Science articles in Economics during 1956 – 2012 . Economics Negative association between 1956 - 2000 but became positive between 2001 – 2012 Correlation between title length and the citation counts was significantly negative between 1970s to 1990s ( Coefficients ranging from - 0 . 00417 to - 0 . 00818 , respectively ) , but becomes positive after 2000s ( 0 . 00596 ) . Letchford et . al . , 2015 140 , 000 Highly cited Scopus articles during 2007 - 2013 . General Negative Highly cited articles with shorter titles receive more citations ( Kendall ' s τ coefficient = − 0 . 042 , p < 0 . 001 ) Van Wesel et al . , 2014 2 , 016 ( Sociology ) 6 , 957 ( Gen . Med ) 23 , 676 ( Physics ) Articles published in journals with the highest impact factor during 1996 - 2005 . Sociology Gen . Med . Physics Negative association in Sociology and Physics , but positive in Gen . Med . Articles with shorter titles received more citations in both Sociology ( r = - 0 . 046 ) and Applied Physics ( r = - 0 . 089 ) , but in Medicine longer titles had more citations ( r = 0 . 166 ) all at p = 0 . 01 . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Social Sci . ) 16 , 378 ( Chem . ) Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry Negative association in Biology & Biochem . and Articles with shorter titles had more citation in Biology & Biochemistry ( rho = 0 . 021 ) and Social Sciences ( rho = 0 . 014 ) and there was no association in Chemistry . 10 Social Sci . , but no association in Chemistry Gnewuch & Wohlrabe , 2017 312 , 879 Articles in economics published between 1980 and 2015 in 430 journals . Economics Negative Shorter article titles associate with increased citation counts ( coefficient = - 0 . 15 ) . Haustein et al . , 2015 1 . 3 million Articles published in 2012 across different fields . - Positive Articles with more characters in title received more citations ( rho = 0 . 142 ) . Sienkiewicz & Altmann , 2016 4 . 3 million Articles from over 1 , 500 journals during 1995 – 2004 . General Negative or positive depending on articles ( highly cited and normal ) Shorter titles of highly cited articles positively correlated with citation counts , whereas for normal papers , longer article titles received more citations . T h e n o n - a l ph a nu m e r i c c h a r a c t e r i n t i t l e a nd c i t a t i o n i m p a c t Jacques & Sebire , 2010 150 25 most cited and 25 least cited articles form the Lancet , BMJ J Clin . Pathology in 2005 . Medical Sci . Positive Colons were significantly more common in highly cited articles compared with least cited articles ( Z = 2 . 3 , P = 0 . 02 ) . Paiva et al . , 2012 423 Articles from the BioMed Central and PLoS journals ( n = 19 ) published in 2008 . Biomedical Positive Article titles with two components separated by a colon or a hyphen had fewer citations compared with titles that did not have these characters ( p = 0 . 004 ) . Jamali & Nikzad , 2011 2 , 172 Articles from six PLoS ( Public Library of Science ) journals in 2007 . Biomedical ( excl . PLoS One , ) Positive Articles with colon in their titles received fewer citations ( median = 9 ) compared to titles without colons ( median = 12 ; p = 0 . 012 ) . Buter and van Raan 2011 642 , 807 About 5 % of Web of Science articles during 1999 - 2008 across different subjects . Multidisciplin ary Positive In general , articles with non - alphanumeric characters in their titles had higher citation impact than titles with only alphanumeric characters , although there were disciplinary differences . Gnewuch & Wohlrabe , 2017 312 , 879 Articles in economics published between 1980 and 2015 in 430 journals . Economics Positive A non - alphanumeric character in article title associates with higher citation impact ( coefficient = 0 . 47 ) . Question marks had the greatest association , increasing the citation count by 1 . 64 ; 0 . 90 for colons . Fiala et al . , 2021 1 , 922 , 652 Articles and conference proceedings papers published during 1945 to 2014 . Computer Science Positive Citation counts per article asking questions ( n = 5682 ) was 16 % higher than for other papers ( n = 957 , 837 ) and this difference was statistically significant at the 0 . 001 level . T h e I m p a c t o f a r t i c l e l e n g t h o n c i t a t i o n s Weale et al . , 2004 13 , 125 ( Immun . ) 17 , 083 ( Surgery ) Articles in immunology and Surgery . Immunology and Surgery Positive A weak but significant correlation ( rho = 0 . 286 and 0 . 335 respectively ) between the number of pages and citations per article in immunology and Surgery . Fox et al . , 2016 26 , 088 Articles published in 32 Ecology Web of Science indexed journals during 2009 - 2012 . Ecology Positive Longer articles in Ecology tended to be cited more ( r = 0 . 147 ) , although this association varied among journals based on manuscript length in author guidelines . Lyu & Wolfram , 2018 27 , 305 ( NEJM ) 42 , 733 ( JAMA ) 65 , 525 ( Lancet ) Articles from three medical journals ( New England Journal of Medicine , JAMA and Lancet ) since their creation up to 2016 . Medical Sci . Positive The length of medical articles in the studied journals had increased over time and , on average , longer articles received more citations than shorter articles in all three journals . 11 Van Wesel et al . , 2014 2 , 016 ( Sociology ) 6 , 957 ( Gen . Med ) 23 , 676 ( Physics ) Articles published in journals with the highest impact factor during 1996 - 2005 . Sociology Gen . Med . Physics Positive Number of article pages associates with citation counts in Sociology ( r = 0 . 122 ) and Physics ( r = 0 . 033 ) , and strongly in General Medicine ( r = 0 . 435 ) , all significant with p < 0 . 01 . Vieira & Gomes , 2010 44 , 248 ( Bio . ) 97 , 177 ( Chem . ) 20 , 127 ( Maths ) 64 , 614 ( Physics ) Web of Science articles in four science fields in 2004 . Bio . Biochem . Chemistry Maths Physics Positive Article length ( page count ) associates with higher citation counts for articles in Bio . & Biochem . ( R 2 = 0 . 666 ) , Chemistry ( R 2 = 0 . 355 ) , Mathematics ( R 2 = 0 . 864 ) and Physics ( R 2 = 0 . 615 ) with citation enhancements of 50 % , 30 % , 62 % and 37 % respectively . Hasan & Breunig , 2021 1 , 561 Articles published in five Economics journals during 2010 - 2014 . Economics Positive A 1 % increase in article size predicts an increase in Google Scholar citations by 0 . 56 % . Haslam et al . , 2008 308 Articles published in top five economics journals between 2010 and 2014 . Psychology Positive Longer articles tended to receive more citations ( overall multiple regression analysis = 0 . 21 at p < 0 . 001 ) . Hafeez et al . , 2019 545 Articles in 2007 from six major psychiatry journals in 2007 . Psychiatry Positive Number of pages of articles significantly correlated with citation counts ( rho = 0 . 15 ) . Falagas et al . 2013 196 Articles from five journals with highest impact factors in General Medicine in 2016 . Medical Sci . Positive Article length ( number of pages ) independently predicted the number of future citations ( r = 0 . 700 ) . Hodge et al . , 2017 3 , 066 Articles from 18 Social Work journals published during 2005 to 2009 . Social Sci . Positive Longer articles tend to receive more citations and every additional page was associated with almost 4 % more citations ( rho = 0 . 09 ) . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Social Sci . ) 16 , 378 ( Chem . ) Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry No association No significant associations found between article length and citations . Xie et al . , 2019 1 , 548 , 088 ( meta - analysis ) A meta - analysis of 18 relevant studies . Different subjects Positive Meta - regression analysis of relevant studies found a moderate , positive correlation between article length and citations ( r = 0 . 310 ) . Mingers & Xu 2010 696 Articles published in six Management journals in 1990 . Management Positive Longer article attracted more citations ( 0 . 277 ) . Haustein et al . , 2015 1 . 3 million Articles published in 2012 across different fields . Multidisciplin ary Positive Article length significantly correlated with citation counts ( rho = 0 . 194 ) . T h e l e n g t h o f a b s t r a c t a nd c i t a t i o n c o un t s Weinberger et al . , 2015 one million Abstracts from articles with abstracts from eight science subjects within 17 years . Science Negative Shorter abstracts associated with decreased citation impact for articles in all fields . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Social Sci . ) 16 , 378 ( Chem . ) Abstracts from Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry Positive Abstract length significantly correlated with increased citations in all fields , but this association found to be weak in Social Science ( rho = 0 . 122 ) in Chemistry ( rs = 0 . 148 ) and in Biology & Biochemistry ( rho = 0 . 153 ) . 12 Letchford et al . , 2016 300 , 000 Abstracts from most highly cited articles between 1999 and 2008 . General Negative Shorter abstracts receive slightly more citations at the journal level . Adding a 5 - letter word decreases the predicted number of citations by 0 . 02 % . Van Wesel et al . , 2014 2 , 016 ( Sociology ) 6 , 957 ( Gen . Med ) 23 , 676 ( Physics ) Abstracts from articles published in journals with the highest impact factor during 1996 - 2005 . Sociology Gen . Med . Physics Positive The length of the abstract ( as measured by numbers of sentences ) , correlated positively with citations in both General & Internal Medicine ( r = 0 . 314 ) and Applied Physics ( 0 . 049 ) , but not in Sociology . Sohrabi & Iraj , 2017 5 , 875 Abstracts from articles in Education subject areas . Education Positive Both abstract ratios ( logistic regression = 5 . 216 ) and weight ( logistic regression = 3 . 58 ) were significant variables in predicting future citations . Hafeez et al . , 2019 545 Abstracts from articles form six high impact psychiatry journals in 2007 . Psychiatry Positive Structured and longer abstracts tended to receive more citations . Both abstract character count ( rho = 0 . 22 ) and word count ( rho = 0 . 17 ) correlated positively with citation counts . Sienkiewicz & Altmann , 2016 4 . 3 million Articles from over 1500 journals during 1995 – 2004 . Multidisciplin ary Positive The number of words in the abstract positively correlated with citation counts in almost all studied journals . A r t i c l e r e a d a b ili t y a nd c i t a t i o n i m p a c t Gazni , 2011 264 , 156 Abstracts from articles from five American universities during 2000 – 2009 . General Negative Abstracts with more difficult language ( as Flesch reading ease score ) tended to attract more citations than abstracts with easier language . The Pearson correlations between citation per paper and Flesch score ranged from – 0 . 469 to – 0 . 821 all significant with p < 0 . 01 . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Social Sci . ) 16 , 378 ( Chem . ) Abstracts of Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry Negative association in Bio . & Biochem . No association in Social Sciences and Chemistry Abstract readability ( Flesch reading ease score ) correlated significantly with decreased citations in Biology & Biochemistry ( rho = − 0 . 073 ) , but no significant relationship was found in Social Sciences and Chemistry . Stremersch et al . , 2007 1 , 825 Full texts of articles from five major marketing journals during 1990 to 2002 . Marketing Negative Readability of articles ( Flesch reading ease score ) negatively associates with citations and more difficult texts tended to attract more citations ( r = - 0 . 02 , p < 0 . 01 ) . Hu et al . , 2021 20 , 000 Abstracts of 10 , 000 highly cited and 10 , 000 uncited articles published during 2008 - 2017 . General Negative Abstracts of highly cited articles tended to have more complex , difficult and professional terms than abstracts of uncited articles and this difference was significant with p < 0 . 01 . Lu et al . , 2019 36 , 400 1 , 797 Full text of articles . Biology Psychology No association No practical significant relationship between linguistic complexity and citation impact . Ante , 2022 135 , 502 Abstracts from articles on 12 emerging technologies subjects by the end of 2020 . Emerging technologies Negative The abstracts of the top 10 % and 1 % of the most frequently cited articles were significantly less readable and zero - cited articles on average were almost always easier to read . Hafeez et al . , 2019 545 Articles form six high impact psychiatry journals in 2007 . Psychiatry Positive Articles with structured abstracts associate with higher citation counts ( rho = 0 . 03 ) . 13 Dowling , 2018 3 , 229 Articles published in the journal Economics Letters during 2003 – 2012 . Economics Positive A positive correlation between Flesch Reading Ease Score of abstracts and citation counts . Sienkiewicz & Altmann , 2016 4 . 3 million Articles from over 1500 journals during 1995 – 2004 . Multidisc . Negative The text complexity of abstracts positively correlated with citation counts . T h e r e l a t i o n s h i p b e t w ee n f e a t u r e s o f c i t e d r e f e r e n c e s a nd c i t a t i o n i m p a c t Hafeez et al . , 2019 545 Articles form six high impact psychiatry journals in 2007 . Psychiatry Positive The number of references significantly corelated with citations ( rho = 0 . 2 ) , Vieira & Gomes , 2010 44 , 248 ( Bio & Biochem . ) 97 , 177 ( Chemistry ) 20 , 127 ( Maths ) 64 , 614 ( Physics ) Articles in four science fields in 2004 . Bio . & Biochem . Chemistry Maths Physics Positive The number of references significantly predicted the citation counts of articles in Bio . & Biochem . ( R 2 = 0 . 917 ) , Chemistry ( R 2 = 0 . 858 ) , Mathematics ( R 2 = 0 . 799 ) and Physics ( R 2 = 0 . 846 ) with citation enhancement 69 % , 60 % , 72 % and 58 % respectively . Ahlgren et al . , 2018 955 , 663 Articles published in 2009 . General Positive Using quantile regression , reference counts and more recent references in academic articles positively correlate with their citation impact . Didegah & Thelwall , 2013a 50 , 162 Articles in nanoscience and nanotechnology journals published between 2007 - 2009 . nanoscience and nanotechnolo gy Positive Number , internationality , and impact of cited references predict increased citations . A one standard deviation increase in these three factors predicted a 19 . 2 % , 17 . 3 % and 35 % increase in the citation counts , respectively . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Social Sci . ) 16 , 378 ( Chem . ) Articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry Positive The number and impact of cited references significantly correlated with article citations in Biology & and Biochemistry ( rho = 0 . 265 and 0 . 416 respectively ) , Social Science ( rho = 0 . 104 and 0 . 302 ) and Chemistry ( rho = 0 . 304 , 0 . 359 ) . Mammola et al . , 2021 50 , 878 Articles published in seventeen ecological journals between 1997 and 2017 . Ecology Positive On average research articles with more references are more cited than articles with less references and this difference is statistically significant . Hafeez et al . , 2019 545 Articles in 2007 from six major psychiatry journals in 2007 . Psychiatry Positive Number of cited references in articles significantly correlated with citation counts ( rho = 0 . 2 ) Haslam et al . , 2008 308 Articles published in top five economics journals between 2010 and 2014 . Psychology Positive Number and recency of cited references significantly correlated with citation counts ( r = 0 . 34 and 0 . 19 , respectively ) . Peng & Zhu , 2012 7 , 749 Articles from 105 journals in Internet studies . Internet studies Positive Authoritativeness of cited references had significant positive correlation with citations ( γ = 0 . 988 , p < 0 . 001 ) . Yu et al . , 2014 1 , 025 Articles published in 20 Library and Information Science journals . Library & Information Science Positive The number of references significantly corelated with citation impact ( rho = 0 . 406 at the 0 . 01 level ) . Boyack & Klavans , 2005 780 , 049 Articles from The Science Citation Index Expanded and The Social Sciences Citation Index during 2002 and 2003 . Multidisciplinary Positive Cited reference impact as measured by number of times the references from a certain paper were cited significantly correlated with citation counts , although there were disciplinary differences . 14 Lancho - Barrantes et al . , 2010 1 . 6 million Articles from in 2007 . 27 Scopus subject categories Positive The averages of the SJR and JIF indicators are strongly correlated with the average number of references to recent papers included in the Scopus database . Bornmann et al . , 2012 1 , 765 Chemical articles . Chemistry Positive h - index for the cited references significantly correlated with citation rates . Onodera & Yoshikane , 2015 1 , 395 Sampled articles published in 2000 ( n = 230 – 240 in each field ) . Five sciences and one engineering subject Positive The Price index ( as measured by percentage of the references within five years before the publication year of the article ) had the strongest association with the citation counts across fields ranging from rho = 0 . 188 in electric and electronic engineering to rho = 0 . 555 in biochemistry and molecular biology , followed by number of cited references ranging from rho = 0 . 254 in condensed matter physics to rho = 0 . 494 in physiology . Lokker et al . , 2008 757 clinical articles from medical journals published from January to June 2005 . Medical Sci . Positive Citation counts of clinical articles were predicted by number of cited references [ regression coefficient 0 . 004 ( 0 . 001 to 0 . 008 ) ] in combinations with other factors . 15 2 . Predicting journal article citation counts from metadata Many statistical studies have attempted to model factors that may associate with higher citation counts or have predicted long term citation counts from metadata rather than article text . For example , it is known that greater numbers of authors and country affiliations associate with more citations in many fields . Other studies have investigated the relationship between journal impact factors and article citation counts . 2 . 1 . The relationship between the number of authors and citation counts It seems reasonable to assume that articles with more authors tend to be better quality due to the greater range of expertise . Larger numbers of authors may also generate more interest for an article , an audience effect ( Wagner , Whetsell , & Mukherjee . , 2019 ) . This section reviews research into whether different types of research collaboration tend to produce more highly cited articles . A positive association has been found in nearly all cases , but there is no agreed formula for the relationship between the two ( e . g . , linear , logarithmic ) . Articles with more authors are more cited ( evidence form high impact journals ) : A study of articles published in eight high impact multidisciplinary ( Nature , Science and Proc . Natl . Acad . Sci . ) , biomedical ( Circulation and Blood ) and science ( J . Am . Chem . Soc . , Phys . Rev . Lett . and Astrophys . J . ) journals during 1995 to 2004 found that co - authored papers tended to attract more citations . For instance , on average , a solo - authored Nature article had 61 citations , whereas Nature articles with 10 authors had 263 citations ( Hsu & Huang , 2011 ) . A similar report about the high impact journals Cell , Science , Nature , New England Journal of Medicine , The Lancet , and JAMA ( n = 164 to 886 ) for the years 1975 , 1985 , and 1995 confirmed that the number of authors and the number of citations to articles positively correlated . For all journals , solo - authored research was the least cited ( Figg et al . , 2006 ) . Articles with more authors are more cited ( evidence from small studies ) : Small studies on single fields have found that more authors associates with higher citation rates in Chemical Engineering ( Peters & van Raan , 1994 , n = 226 ) , Medical Sciences ( Lokker et al . , 2008 , n = 757 ) , Psychology ( Haslam et al . , 2008 , n = 308 ) , Pharmacology and Pharmacy ( Bordons , Aparicio , & Costas , 2013 , n = 1 , 971 and 2 , 858 ) , Ecology ( Leimu & Koricheva , 2005 , n = 214 ) , Library and Information Science ( Sin , 2011 , n = 7 , 489 ) , Computer Science ( Ibanez , Bielza , & Larranaga , 2013 , n = 20 , 000 ) , and management ( Ronda - Pupo , 2017 , n = 36 , 241 ) . Articles with more authors are more cited ( evidence from large studies ) : An early study of all papers from the Science Citation Index in Biomedical Research , Chemistry , and Mathematics in 1980 , 1986 , 1992 , 1996 , and 1998 found that multi - authored papers tended to attract more citations than solo papers ( Glänzel , 2002 ) . Another large - scale study of 19 . 9 million Web of Science articles in science and engineering ( 1955 - 2000 ) , social sciences ( 1956 - 2000 ) , and arts and humanities ( 1975 - 2000 ) found that articles with more co - authors received more citations than articles with individual authors across all broad fields and this citation advantage of co - authored research had increased over time . Although the majority ( 90 % ) of articles in the arts and humanities had single authors , co - authored papers tended to attract more citations and this difference was statistically significant ( Wuchty , Jones , & Uzzi , 2007 ) . A very large - scale study of 32 . 5 million Web of Science publications ( articles , notes , and reviews ) from 1900 – 2011 across the Natural & Medical Sciences and Social Sciences & Humanities found that more authors associated with higher citation impact in all fields . In Natural & Medical Sciences five authors and in Social Sciences & Humanities three authors on average were found to be important to start attracting substantially more citations ( Larivière et al . , 2015 ) . Another large 16 investigation across all 27 Scopus broad subjects from 10 countries with the most journal articles during 2008 - 2012 found that there was a significant increase in the average citation impact of research from single to two authored articles with a subsequent linear rise with additional authorship , giving overall logarithm - like shape ( Thelwall & Maflahi , 2020 ) . A paper about 226 , 166 Web of Science indexed articles in 2004 in Biology & Biochemistry , Chemistry , Mathematics and Physics found that the number of co - authors correlated with citation counts and the citation enhancement varied from 24 % in Physics to 52 % in Mathematics ( Vieira & Gomes , 2010 ) . A recent study of articles about Robotics and Artificial Intelligence ( AI ) ( n = 52 , 175 ) during 2008 - 2017 found that the number of authors and international collaboration have a significant low negative correlation with the waiting time to receive a first citation ( r = - 0 . 12 and - 0 . 068 respectively ) , suggesting that as research collaboration increases , the waiting time for first citations decreases ( Kumari et al . , 2020 ) . Formulae for the relationship between the number of authors and the expected citation counts of publications : Although there is no agreement on the relationship between the number of authors of a paper and its expected citation impact , in general , the expected citation impact of a publication increases with the logarithm of the number of authors ( based on all Scopus - indexed journal articles 2008 - 2012 : Thelwall & Maflahi , 2020 ) . The logarithm shape fits the UK well , for example ( see Figure 10 of : Thelwall & Maflahi , 2020 ) . Articles with more authors are more cited ( evidence from a single country or institution ) : A study of Norwegian scientific production during 1981 - 1996 ( n = 46 , 849 ) in Natural Sciences found that highly cited articles tended to have more authors than normal papers . For instance , the average citation rate for articles with 10 authors was 4 . 5 times higher than for articles with solo articles ( Aksnes , 2003 ) . An investigation of Web of Science articles published in 2013 by Belgium ( n = 26 , 886 ) , Israel ( n = 16 , 618 ) and Iran ( n = 28 , 203 ) found low but significant correlations between the number of authors and citation counts for most broad subjects . This included Chemistry ( ranging from r = 0 . 082 to r = 0 . 105 ) and Clinical & Experimental Medicine ( from r = 0 . 161 to r = 0 . 250 ) , although there were differences between subjects and countries . For instance , in Social Sciences , while no significant correlation was found between co - authorship and citations for articles with Iranian addresses , the strongest association was found in this field for Israeli authors ( r = 0 . 342 ) ( Chi & Glänzel , 2017 ) . At the country level , an investigation of 11 , 196 South African ( Sooryamoorthy , 2009 ) and 15 , 301 Italian publications ( Francescheta & Costantini , 2010 ) also found positive correlations between the number of authors and citations . Similarly , articles with more co - authors published by Harvard University during 2000 - 2009 ( n = 124 , 937 ) had more citations than single authored publications ( Gazni & Didegah , 2011 , R 2 = 0 . 9 ) . A large study across 27 broad subjects from the 10 countries with most journal articles during 2008 - 2012 found that increased collaboration associated with higher citation for all countries and subjects , except for China with a much lower association between academic collaboration rates and citation counts ( Thelwall & Maflahi , 2020 ) . Finally , a study of all Italian scientific production indexed in the Web of Science from 2004 - 2010 ( n = 392 , 257 ) also found significant associations between the number of authors and both citation impact and journal impact factors , with some disciplinary differences in the results ( Abramo & D’Angelo , 2015 ) . Articles with more authors are more cited ( evidence from meta - analyses ) : A recent meta ‑ analysis of 92 relevant articles involving 340 effect sizes found a significant positive correlation , albeit weak , between research collaboration and citation counts ( r = 0 . 146 ) , although this association was higher in Sciences & Biomedical and Social Science fields ( both r = 0 . 167 ) than in other subjects . There was a 17 stronger association between citations and research collaboration for developing countries ( r = 0 . 180 ) than for developed countries ( r = 0 . 112 ) ( Shen et al . , 2021 ) . No evidence of associations between the number of authors and citation counts : A few investigations have not found articles with more authors to be more cited . A study of 568 articles in eight economics journals in 1990 found no significant association between collaboration and citation counts ( Medoff , 2003 ) . Similarly , a study of 1 , 765 chemical articles papers in 2000 ( Bornmann , Schier , Marx , & Daniel , 2012 ) , an investigation of 2 , 792 articles from fourteen Finance journals during 1987 – 1991 ( Avkiran , 1997 ) and an analysis of 50 , 162 articles in nanoscience and nanotechnology ( 2007 - 2009 ) found no significant correlations between the number of authors and citation counts ( Didegah & Thelwall , 2013a ) . Similarly , no association was found in geography and forestry ( n = 213 ) ( Slyder et al . , 2011 ) . Thus , in specific fields , co - authorship may not associate with more highly cited research . The same seems to be true for monographs . No association has been found between co - authorship of monographs ( n = 17 , 737 ) and their citation impacts , suggesting that collaboration indicators should not be used to predict scholarly impact in book - based fields ( Thelwall & Sud , 2014 ) . 2 . 2 . The relationship between international collaboration and citations Many studies have suggested that internationally co - authored papers tend to attract more citations compared with domestic articles . This may be due to wider audiences for the research ( more people knowing the authors : Wagner , Whetsell , & Mukherjee , 2019 ) , more varied expertise , or more funding ( assuming that international collaboration is often triggered by grants ) . Conversely , a higher proportion of international research may be of highly funded types , whilst other international collaborations are more average . This section reviews relevant studies about this topic . Articles with international collaboration are cited more : An early paper about astronomy papers ( n = 2 , 090 , 1980 - 1991 ) found that on average articles with international collaboration tended to attract more citations than articles with national or no collaboration ( Van Raan , 1998 ) . For highly cited European physics articles from 1980 - 1987 , about 41 % had international collaboration ( Glänzel et al . 1995 ) . For 20 , 804 Web of Science articles in Sport Sciences during 2000 – 2001 and 2010 – 2011 , articles with international co - authorship received more citations than articles with domestic co - authorship . Moreover , the relative citation impact of international publications was 1 . 16 and 1 . 29 for the periods 2000 – 2001 and 2010 – 2011 , indicating that papers with international collaboration had higher citation scores than world average ( 1 ) in Sport Sciences ( Wang , Thijs , & Glänzel , 2015 ) . International collaboration also correlated with more citations for Web of Science articles between 2000 - 2009 in Biology and Biochemistry ( n = 16 , 058 ) and Chemistry ( n = 16 , 378 ) and one additional international collaboration associated with an increase the average citation count by 5 . 5 % and 8 . 6 % respectively ( Didegah & Thelwall , 2013b ) . Articles with international authors are cited more ( small - scale evidence ) : Astronomy papers ( n = 2 , 090 , 1980 - 1991 ) with international collaboration attracted more citations than articles with national or no collaborations ( Van Raan , 1998 ) . Articles with international authors are cited more ( large - scale evidence ) : An early study of 400 , 000 articles published between 1977 and 1986 in 28 subjects found that , on average , internationally co - authored papers ( with more than one European Community country ) attracted two times more citations than articles authored by a single country . The study also found that highly cited EU research tends to have multinational co - authorship ( Narin et al . 1991 ) . A large - scale study of 1 . 25 million articles from 1996 - 2012 in eight subject areas found that international collaboration had a significant 18 positive association with citation counts , although varying between fields . For instance , in Ecology more than a quarter of articles with authors from five or more countries were within the 10 % of most cited papers ( Smith et al . , 2014 ) . Another large investigation of 32 . 5 million Web of Science publications in two broad research areas ( Natural and Medical Sciences and Social Sciences and Humanities ) also found that more international collaboration associates with increased citation impact for research both over time and between research areas ( Larivière et al . , 2015 ) . Articles with international authors are cited more ( evidence from one country or institution ) : To assess if collaboration may associate with increased citation impact for UK academic research , an early study used a half million publications ( between 1981 and 1991 ) , finding that international and domestic collaborations associated with increases in citations by 1 . 6 and 0 . 75 , respectively ( Katz & Hicks , 1997 ) . Similarly , a study of 46 , 849 Norwegian publications ( 1981 - 1996 ) in Natural Sciences found that about 63 % of the highly cited papers were co - authored internationally compared with 26 % overall ( Aksnes , 2003 ) . A report into 33 , 524 Scopus articles published in 2000 with at least two different author affiliations from European countries found that the geographical distance between the collaborating countries positively associates with citation counts . The regression analysis suggested that for each kilometre , citation counts increased by 7 % - 9 % ( Nomaler , Frenken , & Heimeriks , 2013 ) . A study of 143 , 221 Finnish publications between 1990 and 2008 also showed a positive association between international co - authorship and citations and that articles produced via international collaboration tended to receive more citations than domestic co - authored research ( Puuska , Muhonen , & Leino , 2014 ) . A study of Web of Science articles from 2003 - 2013 found a negative association between citations and government funding for 35 OECD countries , but international collaboration had a significant and positive association with citation impact ( Leydesdorff , Bornmann , & Wagner , 2019 ) . International collaboration also positively associated with the overall citation impact of both young ( n = 26 ) and old universities ( n = 28 ) ( Khor & Yu , 2016 ) . Not all international collaboration is beneficial : There is evidence that some countries may extract more value from international collaboration than others ( Lancho - Barrantes , Guerrero - Bote , & de Moya - Anegón , 2013 ) and some countries may not benefit from international collaboration in terms of increased citation impact ( Smith et al . , 2014 ) . For example , a study of articles published during 2004 - 2008 in the Nature and Science found that American authors publishing in these journals did not benefit from international collaboration in terms of citation impact ( Rousseau & Ding , 2016 ) . For Biochemistry articles in 2011 ( n = 13 , 578 ) , research collaboration with the U . S . associated with increased scholarly impact for published research , whereas co - authorship with some other countries including India and China associated with reduced impact ( Sud & Thelwall , 2016 ) . No evidence of an association between international collaboration and citations : International collaboration does not correlate with more citations for articles in the Social Sciences ( n = 15 , 932 , 2000 - 2009 ) ( Didegah & Thelwall , 2013b ) . A study of Harvard University publications 2000 - 2009 ( n = 124 , 937 ) also found no significant association between international collaboration and citation counts ( Gazni & Didegah , 2010 ) . 2 . 3 . The relationship between institutional collaboration and citations Articles with more institutional collaborations are cited more : An early study of UK publications ( n = 376 , 000 ) between 1981 and 1991 found that domestic collaborations were associated with 0 . 75 more citations ( Katz & Hicks , 1997 ) . For 124 , 937 publications affiliated with Harvard University 2000 - 2009 19 there was a significant correlation between the number of collaborating institutions and citation counts ( Gazni & Didegah , 2011 , R 2 = 0 . 72 ) . For Pharmacology and Pharmacy articles by Spanish authors ( n = 1 , 971 for 1998 - 2000 ) , articles with authors from different institutional sectors received more citations than articles with authors within the same institution ( Bordons , Aparicio , & Costas , 2013 , 2013 ) . An investigation of 765 , 491 Web of Science articles in Artificial Intelligence 1997 to 2017 found that the type of institutional collaboration has a significant association with citations and there is significant association between citation counts and the number of “Main institutions type” ( top 20 institutions in the field such as MIT , Stanford University , or University of Oxford ) , whereas no significant relationship was found between citations and the number of “Normal institutions type” ( Fan et al . , 2020 ) . A report about nanoscience and nanotechnology journal articles from 2007 - 2009 ( n = 50 , 162 ) also found weak but significant associations between the number of collaborating institutions and citation counts ( Didegah & Thelwall , 2013a ) . A very large investigation of over 32 . 5 million publications ( 1900 – 2011 ) in two research areas ( Natural & Medical Sciences and Social Sciences & Humanities ) also found that Web of Science publications with more author institutional addresses tended to have higher citation counts ( Larivière et al . , 2015 ) . A study of six high impact multi - disciplinary and medical journals ( Cell , Science , Nature , New England Journal of Medicine , The Lancet , and JAMA ) for the years 1975 , 1985 , and 1995 found that the number of citations to articles correlated significantly with the number of institutions ( Figg et al . , 2006 ) . No evidence of an association between institutional collaboration and citations : A study of articles in Biology & Biochemistry ( n = 16 , 058 ) , Chemistry ( n = 16 , 378 ) and Social Sciences ( n = 15 , 932 ) from 2000 - 2009 found no significant association between the number of collaborating institutions and the citation impact of the published research ( Didegah & Thelwall , 2013b ) . In the field of Artificial Intelligence , no significant association was found between the citation impact of research and the number of “Normal institutions type” ( Fan et al . , 2020 ) . 2 . 4 . The relationship between journal impact factors and citations Since the journal impact factor is calculated from the citation rates of the articles in a journal , it is logical to expect articles to be more cited when they are in a journal with a higher journal impact factor . This relationship is not certain , however , since individual highly cited articles may be the cause of a higher journal impact factor and the impact factor calculation only covers a limited range of citations . Nevertheless , there is strong evidence from many studies of different fields that there is a strong general relationship . Journal impact factors associate with higher citation rates for their articles ( evidence from small studies ) : An analysis of 204 articles in Emergency Medicine found that journal impact factors were the strongest predictor of citation counts ( R 2 = 0 . 14 ) ( Callaham , Wears , & Weber , 2002 ) . For 196 articles published in five General & Internal Medicine journals with high impact factors in 2006 , there was a significant medium Spearman correlation ( rho = 0 . 63 ) between the impact factor of the journals and future article citations ( Falagas et al . 2013 ) . Similarly , in geography and forestry ( n = 213 ) articles published in journals with higher impact factors also had more citations ( R 2 = 0 . 28 ) ( Slyder et al . , 2011 ) and an investigation of 131 articles in Environment and Ecology during 2006 – 2007 found that journal impact factors had a significant medium correlation ( r = 0 . 56 ) with citation counts ( Vanclay , 2013 ) . A study of 1 , 371 articles in demography ( 1990 - 1992 ) also found a significant and high association ( r = 0 . 74 ) between journal impact factors and the number of citations to articles after 10 years ( Van Dalen & Henkens , 2005 ) . Using negative binomial regression models , a study of 1 , 586 articles in in 20 biomedicine found the journal impact factor to be the most significant factor ( coefficient = 0 . 11 ) to predict article citation counts ( Bornmann & Daniel , 2006 ) . For clinical systematic reviews and meta - analyses ( n = 1 , 261 ) published in 2008 , journal impact factors could predict more than half ( R 2 = 0 . 592 ) of the variation in their future citations ( Royle et al . , 2013 ) . Journal impact factor associate with more citations ( evidence from medium - large studies ) : A study of 46 , 849 articles by Norwegian scientists 1981 - 1996 in Natural Sciences found that articles published in journals with high impact factors tended to be cited more . About 91 % of the highly cited articles were published in journals with an impact factor above the field average ( Aksnes , 2003 ) . An investigation of immunology ( n = 13 , 125 ) and surgery ( n = 17 , 083 ) found significant high negative associations between the proportion of uncited articles and journal impact factors for both subject areas ( rho = - 0 . 854 and - 0 . 924 respectively ) , suggesting that high impact journals published few uncited articles ( Weale , Bailey , & Lear , 2004 ) . Using machine learning models to predict citation counts , a study of 3 , 788 papers about internal medicine published between 1991 and 1994 found that the journal impact factor was the only content - based and bibliometric feature that ranked highly for all three studied citation thresholds , reporting absolute value of regression coefficients 4 . 04 , 3 . 34 and 3 . 32 for citation thresholds 20 , 50 and 100 , respectively ( Fu & Aliferis , 2010 ) . For articles in Biology & Biochemistry ( n = 44 , 248 ) , Chemistry ( n = 97 , 177 ) , Mathematics ( n = 20 , 127 ) and Physics ( n = 64 , 614 ) , the journal impact factor was the variable with the largest effect on citations ( Vieira & Gomes , 2010 ) . Internet studies articles ( n = 7 , 749 ) published in journals with higher impact factors also had more citations ( γ = 0 . 537 , p < 0 . 001 ) ( Peng & Zhu , 2012 ) . In nanoscience and nanotechnology the journal impact factor was the most significant factor associating with article citation counts ( n = 50 , 162 ) , with a 1 SD increase in the impact factor associating with a 39 % rise in citations to articles ( Didegah & Thelwall , 2013a ) . A follow - up paper about articles in Biology and Biochemistry ( n = 16 , 058 ) , Chemistry ( n = 16 , 378 ) and Social Sciences ( n = 15 , 932 ) published between 2000 - 2009 also found that the journal impact factor significantly correlated ( rho = 0 . 455 , 0 . 459 , 0 . 186 , respectively ) with increased citations to articles in all three areas ( Didegah & Thelwall , 2013b ) . Using the field - normalised average journal impact , a report about Pharmacology & Pharmacy ( n = 1 , 971 and n = 2 , 858 ) also found that articles published in high impact factor journals were likely to attract more citations ( Bordons , Aparicio , & Costas , 2013 ) . A study of 9 , 898 papers from 2000 to 2004 matched with F1000 data found that journal impact factors had the strongest association with citations out of a range of different bibliometric and quality indicators ( judgments of peers ) , discussing differences between qualitative judgments by experts and journal impact factor to predict future citations ( Bornmann & Leydesdorff , 2015 ) . A positive but weak correlation was also found between the journal impact factor and citation counts ( r = 0 . 327 ) in a recent paper about 9 , 823 articles published in 2016 and 2017 from 33 plastic surgery journals ( Asaad et al . , 2020 ) . Finally , a large study of 780 , 049 Web of Science articles in 2002 and 2003 also showed that the journal impact factor had the strongest bibliometric correlation with citations ( r = 0 . 478 ) and a similar positive association was found in 17 out of 24 subject areas ( Boyack & Klavans , 2005 ) . No evidence of an association between journal impact factors and citation : A few studies have found insufficient statistical evidence that articles published in journals with high impact factors tend to have higher citation impacts . These have covered Urology ( Willis et al . , 2011 , n = 200 ) , Ecology ( Leimu & Koricheva , 2005 , n = 214 ) , and Gastroenterology and Hepatology ( Roldan - Valadez & Rios , 2015 ) . 21 2 . 5 . Author publication and citation records and article citations It seems reasonable to hypothesise that authors with a good track record of publishing or attracting citations would be more likely to author future highly cited papers . It is hard to fully assess this with career - level analyses , but there is some evidence in favour of the hypothesis . An a uthor’s h - index associates with citation counts : A report about 1 , 025 articles in library and information science found low but significant correlations between the h - index of the first author and the maximum h - index of all authors with the citation impacts of their future articles ( r = 0 . 175 and 0 . 287 , respectively ) ( Yu et al . 2014 ) . A paper concerning 100 , 000 papers recommended by the China Computer Federation found that the maximum h - index of all authors associated with higher citation counts in computer science subjects ( Qian et al . , 2017 ) . Similarly , a study of 219 articles in Astronomy & Astrophysics ( Wang , Yu , & Yu , 2011 ) published in four journals in 1985 ( Wang et al . , 2012 ) and 1 , 860 papers written by 65 biomedical researchers ( He , 2009 ) both found the h - index to be a significant predictor of citation counts . An investigation of 131 articles in Environment and Ecology ( 2006 – 2007 ) also found a medium correlation ( r = 0 . 42 ) between the maximum author h - index and citation counts . This association might be due to authors with higher h - indexes being likely to publish in high impact journals ( Vanclay , 2013 ) . For 18 , 000 publications by senior researchers from 147 chemistry research groups in the Netherlands during 1991 - 1998 , there was a high significant correlation ( R 2 = 0 . 89 ) between the h - index and the total number of citations for all research groups . There were also associations between both the h - index and normalized citation impact and peer review judgment about the research quality of groups ( van Raan , 2006 ) . A large multidisciplinary Web of Science article study across 22 subjects for 2000 - 2009 found that a unit increase in the h - index associates with a 2 . 3 % increase in citations for all studied subjects , although there were disciplinary differences , with the increase in citation counts being much higher in Mathematics ( 6 . 6 % ) and Economics & Business ( 5 . 1 % ) than in Immunology and Materials Science ( both 0 . 8 % ) ( Didegah , 2014 ) . 2 . 6 . Predicting the quality of journal articles based on machine learning Several teams have used machine learning techniques to predict the long - term citation counts or quality scores of papers . An early attempt used Gradient Boosted Regression Trees to predict citation counts for 27 , 770 papers from arXiv high energy physics theory based on the contents , topics and author collaboration features of papers , finding that the models used were reasonably effective at predicting the future citation counts of papers ( Chen & Zhang , 2015 ) . Using machine learning on citation - based indicators ( e . g . , total citations and average h - index ) and Times Higher Education indicators , an experiment assessed if REF 2014 overall university grade scores could be predicted . For this , 79 and 30 UK universities were divided into training and test sets respectively . The number of Web of Science publications , entry tariff and percentage of students were the most significant predictors university grade point averages ( Balbuena , 2018 ) , but the sample sizes used were very small for machine learning . Most experiments in this section predicted future citation counts for articles from a field or set of journals . Machine learning models have been used to predict the future citations of biomedical research ( 1991 - 1994 ) in six medical journals ( JAMA , Lancet , NEJM , BMJ , American Journal of Medicine , and Annals of Internal Medicine ) . Overall , 3 , 788 documents , 20 , 005 article text features ( article title , abstract , MeSH terms , publication type ) , metadata ( number of authors and institutions , number articles for first and last authors in the previous 10 years , quality of first author’s institution ) and citations ( number of citations for first and last authors , Journal impact factor ) were leveraged . First author citations had the greatest association ( coefficient = 5 . 75 ) with articles reaching a citation 22 threshold 100 , followed by the MeSH topic Smoking : mortality ( 4 . 22 ) , journal impact factor ( 3 . 32 ) and last author citations ( 3 . 02 ) . Overall , the study suggested that it is feasible to predict future citation counts with machine learning techniques to some extent ( Fu & Aliferis , 2010 ) . Other small - scale studies have used machine learning and different article or metadata features to predict citation counts , such as from machine learning conference papers ( Li et al . , 2019 ; Cummings & Nassar , 2020 ) , articles from the selected journals ( e . g . , Wang et al . , 2020 ; Zhao & Feng , 2022 ) or papers on a specific topic ( Xu et al . , 2019 ) . Long term citation counts can be predicted from early citation counts and / or metadata . Deep learning techniques have been used on a dataset of articles published in Nature , Science , The New England Journal of Medicine , Cell and Proceedings of the National Academy of Sciences ( n = 175 , 432 ) to predict long - term counts of articles based on citation counts soon after publication ( Abrishami & Aliakbary , 2019 ) . A similar study used deep learning to predict the 5 - year citation impact of library , information and documentation articles ( n = 49 , 834 ) from Chinese Social Sciences Citation Index ( 2000 to 2013 ) . The study applied multiple features from article text ( document type , article length , title length , funding , month of publication , punctuation in the title ) , journal ( journal impact factor and number of publications in the journal ) , authors ( e . g . , number of authors , productivity , previous citations , h - index and number of organizations ) , references ( e . g . , number and age of references , self - citations and percentage of different document types in references ) and citations ( citations in the first or first two years , number of citing journals in the first or first two years ) , with some positive results ( Ruan et al . , 2020 ) . One study used metadata semantic features from Artificial Intelligence ( AI ) related articles published in 20 journals indexed by China Computer Federation catalogue to predict the future citation impact of papers with deep learning techniques for semantic features extraction in the AI subject ( Ma et al . , 2021 ) . In contrast to above studies , a study used multiple altmetric indicators ( e . g . , Mendeley reader , open peer - review shares , or mentions in Twitter , news or blogs ) in addition to other metadata to predict future citations for a random sample of 12 , 374 articles published in 2015 . Using machine learning models , the study found that Mendeley readership , maximum followers on Twitter , and academic status ( e . g . , student , postdoc , researcher , or professor ) were top parameters to predict the short - term and long - term citations impact of papers ( Akella et al . , 2021 ) . All of the papers in this section used relevant inputs to predict future or long - term citations but because of the different datasets used , it is not possible to generate general conclusions about which methods or inputs are best overall or in particular cases . A very large multi - disciplinary study used 32 different machine learning methods and all Scopus journals published during 2014 to 2020 across 326 Scopus narrow subjects to predict the quality of published research . Citations ( the Normalized Log - transformed Citation Score ) , collaboration ( Number of authors and number of country affiliations ) and article text ( words from the title , abstract , and keywords ) were used as inputs for the machine learning process . The study found that two machine learning methods ( Gradient Boosting Classifier and Random Forest Classifier ) had the highest levels of accuracy compared with other methods ( 46 % and 45 % respectively ) and machine learning can predict the citation - based journal third of articles using the selected features ( Thelwall , 2022b ) . Machine learning methods have also been used to identify the characteristics of highly cited articles . A study extracted features from article text ( title length , number of figures , tables , equations , and characters with no spaces ) , metadata ( number of authors and number of views ) and citation counts from high and low cited papers ( each 100 articles , n = 200 ) published by MDPI in 2017 , finding significant positive associations between citation counts and the number of views , tables , and authors and a negative but significant correlation with title length ( Elgendi , 2019 ) . 23 A study of the association between REF 2014 grade point averages and impact factors ( IF ) in Neuroscience , psychiatry , and psychology found that the proportions of publications ranked 4 * and 3 * can be predicted with 95 % and 98 % accuracy ( Al - Janabi , Lim , & Aquili , 2021 ) . 24 Table 2 . Summary of studies predicting journal article citation counts from metadata Factors Study Number of articles Dataset Subject Association with citations Main result N u m b e r o f a u t h o r s a nd c i t a t i o n i m p a c t Hsu & Huang , 2011 92 , 034 Articles published in eight high impact multidisciplinary , biomedical and science journals during 1995 to 2004 . Multidisc . Biomed . Science Positive Nature solo articles had 61 citations , whereas articles with 10 ( or more ) had 263 ( or 370 ) citations . Figg et al . , 2006 8 , 631 Articles from six high impact multidisciplinary and biomedical journals for the years 1975 , 1985 , and 1995 . Multidisc . Biomed . Positive Articles with more authors had more citations and solo - authored papers had the least citation counts . Glänzel , 2002 All papers from SCI Papers from Science Citation Index in Biomedical Research , Chemistry , and Mathematics in the years 1980 , 1986 , 1992 , 1996 , and 1998 . Biomed . Chem . Math . Positive Multi - authored papers tended to attract more citation than solo author papers in the three selected fields . Wuchty et al . , 2007 19 . 9 million Web of Science articles in science and engineering , social sciences and arts and humanities . Science Social Sci . Humanities Positive Articles with more co - authors had more citations than articles with solo authors across all broad fields . Larivière et al . , 2015 32 . 5 million Web of Science publications ( 1900 – 2011 ) across two broad research areas ( Natural & Medical Sciences and Social Sciences & Humanities ) . Natural & Medical Sci . Social Sci . & Humanities Positive Author counts associated with citation counts in all research fields . Thelwall & Maflahi , 2020 - Ten countries with most journal articles during 2008 - 2012 in 27 broad subjects . Multidisc . Positive A linear association between co - authorship and citation impact . Vieira & Gomes , 2010 226 , 166 Web of science articles in 2004 in Biology & Biochemistry , Chemistry , Mathematics and Physics . Biochem . Chem . Math . Phys . Positive The number of co - authors correlated with citation counts and the citation enhancement varied from 24 % in Physics to 52 % in Mathematics . Kumari et al . , 2020 52 , 175 Articles about Robotics and Artificial Intelligence ( AI ) related subjects during 2008 - 2017 . Robotics and AI Positive Articles with more authors and international collaboration attract citations faster than other articles . Aksnes , 2003 46 , 849 Articles by Norwegian authors 1981 - 1996 . Natural Sciences Positive The average citation rate for articles with 10 authors was 4 . 5 higher than for solo articles . Chi & Glänzel , 2017 Belgium ( 26 , 886 ) Israel ( 16 , 618 ) Iran ( 28 , 203 ) Web of Science articles published in 2013 by Belgium , Israel and Iran . Multidisc . Positive Significant but low correlations between the number of authors and citations for most broad subjects and three countries such as in Chemistry ( ranging from r = 0 . 082 to r = 0 . 105 ) and Clinical & Experimental Medicine ( from r = 0 . 161 to r = 0 . 250 ) . 25 Shen et al . , 2021 Meta - analysis Meta ‑ analysis of 92 relevant articles involving 340 effect sizes . Multidisc . Positive A significant but weak association between collaboration and citation counts ( r = 0 . 146 ) , higher in Sciences & Biomedical and Social Science fields ( both r = 0 . 167 ) and for developing countries ( r = 0 . 180 ) than developed countries ( r = 0 . 112 ) . Medoff , 2003 568 Articles published in eight economics journals in 1990 . Economics No association No significant association between collaboration and citation counts . Bornmann et al . , 2012 1 , 765 Article in Chemistry published in 2000 . Chemistry No association No significant correlation between co - authorship and citation counts . Avkiran , 1997 2 , 792 Articles from fourteen Finance journals during 1987 – 1991 . Finance No association No significant correlation between the number of authors and citation counts . Didegah & Thelwall , 2013a 50 , 162 Articles in nanoscience and nanotechnology in 2007 - 2009 . Nanosci . & nanotech . No association No significant association between the number of authors and citation counts . Slyder et al . , 2011 213 Articles in geography & forestry up to the year 2010 from ten American universities . Geography & forestry No association No relationship between the number of authors and citation counts . I n t e r n a t i o n a l c o ll a b o r a t i o n a nd c i t a t i o n i m p a c t Narin et al . 1991 400 , 000 Articles published between 1977 and 1986 in 28 subjects . Multidisc . Positive Internationally co - authored papers had two times more citations than articles authored by a single country . Smith et al . , 2014 1 . 25 million Articles between 1996 - 2012 in eight subject areas . Chem . , Phys . , Ecol . , Bio . , Gene . Geol . , Math . , Psych . Positive Articles with authors from more countries were cited more . In Ecology more than a quarter of articles with authors from five or more countries were within 10 % of mostly cited papers . Larivière et al . , 2015 32 . 5 million Web of Science publications ( 1900 – 2011 ) across two broad research areas ( Natural & Medical Sciences and Social Sciences & Humanities ) . Natural & Medical Sci . Social Sci . & Humanities Positive More international collaboration can subsequently increase citation impact of research both over time and between research areas . Van Raan , 1998 2 , 090 Articles in Astronomy published during 1980 - 1991 1977 in 28 subjects . Astronomy Positive International collaboration attracted on more citations than articles with national or no collaborations . Wang et al . , 2015 20 , 804 Web of Science articles in Sport Sciences during 2000 – 2001 and 2010 – 2011 . Sport Sciences Positive Relative citation impact of international publications was 1 . 16 and 1 . 29 compared to domestic co - authorship for the periods 2000 – 2001 and 2010 – 201 respectively . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 16 , 378 ( Chem . ) Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Chemistry Positive in Biology Chemistry . One additional found to increase the average citation count by 5 . 5 % and 8 . 6 % in Biology and Biochemistry and Chemistry respectively . Katz & Hicks , 1997 376 , 226 UK publications between 1981 and 1991 . General Positive International collaborations increase citations by 1 . 6 . Aksnes , 2003 46 , 849 Articles by Norwegian authors 1981 - 1996 . Natural Sciences Positive About 63 % of the highly cited papers were co - authored internationally compared 26 % overall . Nomaler et al . , 2013 33 , 524 Scopus articles published in 2000 from the European countries . General Positive Geographical distance between collaborating countries positively associates with citation counts . 26 Puuska et al . , 2014 143 , 221 Finnish publications between 1990 and 2008 in six broad fields ( natural science , medicine , engineering , agriculture , social sciences humanities ) . Six broad fields . Positive Iinternational collaborations tended to receive more citations than domestic co - authored research . Leydesdorff et al . , 2019 - Web of Science articles during 2003 - 2013 . General Positive International collaboration had a significant and positive association with citation impact . Sud & Thelwall , 2016 13 , 578 Biochemistry articles in 2011 . Biochemistry Positive Research collaboration with the U . S . increase citation impact , whereas co - authorship with some other countries may reduce it . Didegah & Thelwall , 2013b 15 , 932 ( Soc . Sci . ) Web of Science indexed articles during 2000 - 2009 . Social Sci . No association No meaningful association between international collaboration and citation rates in Social Sciences . Gazni & Didegah , 2010 124 , 937 Harvard University publications between 2000 - 2009 . General No association No significant relationship between international collaboration and citation counts . I n s t i t u t i o n a l c o ll a b o r a t i o n a nd c i t a t i o n i m p a c t Larivière et al . , 2015 32 . 5 million Web of Science publications ( 1900 – 2011 ) across two broad research areas ( Natural & Medical Sciences and Social Sciences & Humanities ) . Natural & Medical Sci . Social Sci . & Humanities Positive Web of Science publications with more institutional addresses of authors tended to have higher citation impact over time and in the studied research areas . Fan et al . , 2020 765 , 491 Web of Science articles in Artificial Intelligence subject between 1997 to 2017 . Artificial Intelligence Positive S ignificant association between citation counts and number of “Main institutions type” ( top 20 institutions in the field ) . Gazni & Didegah , 2010 124 , 937 Harvard University publications between 2000 - 2009 . General Positive Significant correlation between the number of collaborating institutions and citation counts . Bordons et al . , 2013 1 , 971 articles by Spanish researchers in Pharmacology and Pharmacy during 1998 - 2000 . Pharmacology and Pharmacy Positive Authors from different institutional sectors received more citations than articles with authors within the same institution . Didegah & Thelwall , 2013a 50 , 162 Articles in nanoscience and nanotechnology in 2007 - 2009 . Nanosci . & nanotech . Positive Weak association between the number of collaborating institutions and citation counts . Gazni & Didegah , 2010 124 , 937 Harvard University publications between 2000 - 2009 . General Positive Significant correlation between the number of collaborating institutions in the published research and their citation counts . Figg et al . , 2006 8 , 631 Articles from six high impact multidisciplinary and biomedical journals for the years 1975 , 1985 , and 1995 . Multidisc . Biomed . Positive Articles with more authors had more citations and solo - authored papers had the least citations . Katz & Hicks , 1997 376 , 226 UK publications between 1981 and 1991 General Positive Domestic collaborations increase citations by 0 . 75 . Didegah & Thelwall , 2013b 15 , 932 ( Soc . Sci . ) Web of Science indexed articles during 2000 - 2009 . Social Sci . No association No significant association between the number of collaborating institutions and citation impact of published research . Fan et al . , 2020 765 , 491 Web of Science articles in Artificial Intelligence subject ( 1997 to 2017 ) Artificial Intelligence No association No significant relationship found between citations and the number of “Normal institutions type” . 27 J o u r n a l I m p a c t f a c t o r a nd c i t a t i o n c o un t s Aksnes , 2003 46 , 849 Articles by Norwegian authors 1981 - 1996 . Natural Sciences Positive Articles in journals with high impact factors tended to be cited more . About 91 % of the highly cited articles were published in journals with an impact factor above the field average . Weale et al . , 2004 13 , 125 ( Immun . ) 17 , 083 ( Surgery ) Articles in immunology and Surgery . Immunology and Surgery Negative ( between JIF and zero citation ) High negative associations between the proportion of uncited articles and journal impact factors for both immunology ( rho = - 0 . 854 ) and surgery ( rho = - 0 . 924 ) , suggesting that high impact journals published few uncited articles . Fu & Aliferis , 2010 3 , 788 Articles about internal medicine published between 1991 and 1994 . Medical Sci . Positive Journal impact factor was the only feature that ranked highly for all three studied citation thresholds ( 20 , 50 and 100 ) , reporting absolute value of regression coefficients 4 . 04 , 3 . 34 and 3 . 32 respectively . Peng & Zhu , 2012 7 , 749 Articles from 105 journals in Internet studies . Internet studies Positive Higher impact factor journal articles had more citations ( γ = 0 . 537 , p < 0 . 001 ) . Vieira & Gomes , 2010 44 , 248 ( Bio . ) 97 , 177 ( Chem . ) 20 , 127 ( Maths ) 64 , 614 ( Physics ) Web of Science articles in four science fields in 2004 . Bio . Biochem . Chemistry Maths Physics Positive The journal impact factor was the variable with the largest effect on citation counts across all fields . Didegah & Thelwall , 2013a 50 , 162 Articles in nanoscience and nanotechnology in 2007 - 2009 . Nanosci . & nanotech . Positive The journal impact factor was the most significant contributing factor of citation counts of articles . A 1 SD increase in the impact factor relates to a 39 % rise in citations to articles . Didegah & Thelwall , 2013b 16 , 058 ( Bio . Sci . ) 15 , 932 ( Soc . Sci . ) 16 , 378 ( Chem . ) Web of Science indexed articles during 2000 - 2009 . Biology & Biochem . Social Sci . Chemistry Positive The journal impact factor significantly correlated with increased citations of articles in all three subject areas ( rho = 0 . 455 , 0 . 459 , 0 . 186 , respectively ) . Bordons et al . , 2013 n = 1 , 971 and 2 , 858 Pharmacology and Pharmacy Web of Science articles by Spanish authors during 1998 - 2000 and 2006 - 2008 . Pharmacology and Pharmacy Positive Articles in high impact factor journals are likely to attract more citations . Bornmann & Leydesdorff , 2015 9 , 898 Papers from 2000 to 2004 matched with F1000 data . Positive Journal impact factor had the strongest association with citations among other studied bibliometric and quality indicators ( judgments of peers ) . Asaad et al . , 2020 9 , 823 Articles published in 2016 and 2017 from 33 plastic surgery journals . Plastic surgery Positive A positive moderate correlation between the journal impact factor and citation counts ( r = 0 . 327 ) . Boyack & Klavans , 2005 780 , 049 Articles from The Science Citation Index Expanded and The Social Sciences Citation Index during 2002 and 2003 . Multidisciplinary Positive The journal impact factor has the strongest correlation with citations ( r = 0 . 478 ) and a similar positive association found in 17 out of 24 subject areas . Van Dalen & Henkens , 2005 1 , 371 Articles in demography published during 1990 - 1992 . Demography Positive A significant and high association ( coefficient : 0 . 74 ) between journal impact factors and citations to articles after 10 years . Bornmann & Daniel , 2006 1 , 586 Articles in biomedicine published by postdoctoral researchers . Biomedicine Positive Journal impact factor was the most significant factor ( Coefficient = 0 . 11 ) to predict citation counts of articles . Royle et al . , 2013 1 , 261 Clinical systematic reviews or meta - analysis published in 2008 . Med . Sci . Positive Journal impact factor could predict about more than half ( R 2 = 0 . 592 ) of the variation in citations . 28 Callaham et al . , 2002 204 Articles in Emergency Medicine Emergency Medicine Positive Journal impact factor was the strongest predictor of citations ( R 2 = 0 . 14 ) . Falagas et al . 2013 196 Articles published in five General and Internal Medicine journals with highest impact factor in 2006 . Med . Sci . Positive Significant and medium correlation ( rho = 0 . 63 ) between the impact factor of journals and future article citations . Slyder et al . , 2011 213 Articles in geography & forestry up to the year 2010 from ten American universities . Geography & forestry Positive Articles published in journal with higher impact factors also had more citations ( R 2 = 0 . 28 ) . Vanclay , 2013 131 Articles in Environment and ecology during 2006 – 2007 . Environment and ecology Positive Journal impact factor had the strongest correlation ( r = 0 . 56 ) with citation counts . Willis et al . , 2011 200 Articles published between January and June 2004 from three Urology journals . Urology No association No association between articles published in journals with high impact factors and citations . Leimu & Koricheva , 2005 214 Web of Science indexed articles in Ecology . Ecology No association No relationship between journal impact factor and citation counts . Roldan - Valadez & Rios , 2015 - Web of Science indexed articles from 74 Gastroenterology and Hepatology journals . Gastroent . & Hepatol . No association No association between journal impact factor and citation impact . h - i nd e x a nd c i t a t i o n i m p a c t Didegah , 2014 Large study across 22 fields Multidisciplinary study of Web of Science articles across 22 subjects during 2000 - 2009 . Multidiscip . Positive A unit increase in the h - index predicts a 2 . 3 % increase in article citations for all studied subjects . Qian et al . 2017 100 , 000 Recommended papers by the China Computer Federation . Computer Science Positive The maximum h - index of all authors associated with the higher citation counts in computer science subjects . van Raan , 2006 18 , 000 Papers by senior researchers from 147 chemistry research groups in the Netherlands during 1991 - 1998 . Chemistry Positive High significant correlation ( R 2 = 0 . 89 ) between the h - index and the total number of citations for all research groups . He , 2009 1 , 860 Papers written by 65 biomedical researchers . biomedical Positive h - index is a significant predictor for citation counts . Yu et al . , 2014 1 , 025 Articles published in 20 Library and Information Science journals . Library & Information Science Positive Significant correlation between the h - index of the first author and the maximum h - index of the authors with citation impact of future articles ( r = 0 . 175 and 0 . 287 , respectively ) . Vanclay , 2013 131 Articles in Environment and ecology during 2006 – 2007 . Environment and ecology Positive Significant correlation ( r = 0 . 42 ) between the maximum author h - index and citation counts . Wang et al . , 2011 219 Articles in astronomy and astrophysics . Astronomy Positive h - index was significant predictor for citation counts . 29 3 . Predicting or assessing journal article quality scores from metadata There are few empirical studies about the estimation or prediction of the quality ( however defined ) of individual academic publications using automatic or semi - automatic methods , such as based on metrics . This is due to the lack of published large - scale expert judgements on articles or other research outputs . Nevertheless , some studies have used alternative approaches , such as predicting quality profiles or averages from published summary or aggregate data ( e . g . , institution - UoA departmental quality profiles for previous REFs and RAE 2008 or departmental numerical / star ratings for RAE 1992 / 1996 / 2001 ) . 3 . 1 . UK RAE / REF scores and bibliometric indicators Many investigations of the relationship between departmental citation - based and journal - based bibliometric indicators and departmental average RAE / REF scores or score profiles have found statistically significant positive correlations . None have found a method that is capable of closely predicting the average scores ( rather than rankings ) , however . For REF correlations , it is important to distinguish between those based on total output scores and average output scores . The latter will usually be lower , given that larger institutions tend to get higher scores in the UK . RAE 1992 scores associate with citation counts in Library and Information Science , Anatomy , Genetics and Archaeology : Several studies investigated associations between Research Assessment Exercise ( RAE ) scores and citation metrics . An early study of RAE 1992 outputs from UK Library & Information Science ( LIS ) academics ( n = 217 ) found a significant and strong Spearman correlation between both the numbers of citations received by library and information science departments and the numbers of citations per member of staff and departmental RAE ratings ( rho = 0 . 81 and 0 . 82 , respectively , significant at the p = 0 . 01 level ) . The author concluded that “ the cost and effort of the Research Assessment Exercise may not be justified when a simpler and cheaper alternative , namely a citation counting exercise , could be undertaken” ( Oppenheim , 1995 , p . 18 ) . High significant correlations were also found between RAE 1992 ratings of Library and Information Science departments and total citations , average citations per staff member , and average citations per publication ( Seng & Willett , 1995 ) . A paper about multiple disciplines , including Anatomy , Genetics and Archaeology , also found high statistically significant Spearman correlations between 1992 RAE rankings and the total number of citations by departments ( rho = 0 . 718 , 0 . 794 and 0 . 823 , respectively ) as well as medium - high correlations between RAE ratings and average number of citations per academic of staff ( rho = 0 . 487 , 0 . 680 and 0 . 740 respectively ) ( Oppenheim , 1997 ) . In the field of Business & Management Studies , another study found a significant correlation ( r = 0 . 682 ) between journal ranking scores ( Discipline Contribution Scoring ) and the 1992 RAE rating ( Thomas & Watkins , 1998 ) . The RAE 1996 score associates with average citations in Psychology : One unpublished report about the field of Psychology ( n academics = 747 ) found a very high Spearman correlation ( rho = 0 . 91 ) between the 1996 RAE ratings and mean departmental citations ( Smith & Eysenck , 2002 ) . The RAE 2001 score associates with average citation counts in Archaeology , Music , Psychology and Political Science : A paper about Archaeology ( n academics = 692 ) found high significant correlations between the 2001 RAE ranking score and total staff average citations ( rho = 0 . 85 ) and the total of all staff citations ( rho = 0 . 79 ) ( Norris & Oppenheim , 2003 ) , which was almost the same as result in the 30 same subject area for 1992 RAE ( 0 . 740 and 0 . 823 respectively , see also above for Oppenheim , 1997 ) . The 2001 RAE scores for UK university Music departments highly correlated with departmental total and average citation counts ( rho = 0 . 80 and 0 . 81 respectively ) , although a weaker correlation was found between RAE scores and individual citation counts ( rho = 0 . 46 ) ( Oppenheim & Summers , 2008 ) . In Psychology , a strong association ( rho = 0 . 86 ) was also reported between the 2001 RAE scores and mean departmental citations ( Smith & Eysenck , 2002 ) . A regression study of 4 , 400 submissions to the 2001 RAE Political Science panel found that the mean number of citations to the submitted works were the most significant predictor of the RAE scores for the 69 political science departments ( Partial and standardised coefficients : 0 . 541 and 0 . 340 respectively ) . This study also investigated which document types were more likely to attract citations , finding that journal articles were the most significant publication type in predicting the RAE outcome ( Partial and standardised coefficients : 0 . 585 and 0 . 427 respectively ) compared with authored books ( 0 . 151 and 0 . 223 ) or book chapters ( 0 . 115 and 0 . 250 ) , although the 2001 RAE advice was that authored books would be rated higher than journal articles in the Political Science panel ( Butler & McAllister , 2009 ) . Mixed results for associations between the RAE 2001 scores and citation metrics : A large and multidisciplinary study of RAE 2001 research outputs ( n = 112 , 201 or 55 % of all submissions ) found positive and statistically significant correlations between departmental Web of Science citations and departmental RAE peer review score profiles across most science subjects , but no significant association was found in most social science and humanities subjects . The Spearman correlations between departmental average numbers of citations and RAE 2001 scores were high and significant in most biomedical fields ( 7 out of 8 ) ranging from 0 . 821 in Clinical Laboratory Sciences to 0 . 573 in Hospital - based Clinical Subjects , except for Nursing , where insignificant associations were found . However , in Physical and Engineering subjects , there were more diverse relationships between citation and RAE scores ( significant in 9 out of 13 subjects ) and associations were higher in Chemistry , Earth Sciences , and Physics ( 0 . 789 , 0 . 754 and 0 . 685 , respectively ) , whereas no significant associations were found for Pure Mathematics , Civil Engineering , Electrical & Electronic Engineering and Mechanical Engineering . The correlations were not significant in most Social Science subjects ( e . g . , Politics , Sociology and History ) , except for Business & Management ( 0 . 782 ) , Economics & Econometrics ( 0 . 677 ) and Geography ( 0 . 383 ) ( Mahdi , D ' Este & Neely , 2008 ) . This study suggested that there were some disciplinary differences in the relationships between departmental average peer judgment scores and citations . RAE 2008 rankings associate with citations in Chemistry and Political Science : Butler and McAllister ( 2011 ) used a method that had been previously applied to the RAE 2001 Political Science category ( see above Butler & McAllister , 2009 ) , but this time to predict the RAE 2008 outcome in Chemistry ( n = 44 departments ) and Political Science ( n = 69 departments ) . They again found that citation metrics could be useful indicators for predicting RAE outcomes in Chemistry ( Partial and standardised coefficients : 0 . 491 and 0 . 370 respectively ) and in Political Science ( 0 . 410 and 0 . 273 ) ( Butler & McAllister , 2011 ) . The RAE 2008 rankings associate with Article Influence Score in Sociology : A study used three variables ( quality of journals in the submission , research income per capita and scale of research activity ) , predicting about 83 % of the variance in RAE 2008 outcomes for Sociology . The impact of 31 journals was assessed using an indicator called Article Influence Score 1 ( n = 2 , 366 ) , finding significant associations ( beta coefficients of 0 . 37 ) between the journal quality indicator and the RAE 2008 average scores for sociology ( Kelly & Burrows , 2011 ) . Weak evidence of an association between the RAE 2008 scores and citations : A multi - disciplinary study found varied Spearman correlations between RAE 2008 scores for research groups and citation counts across the selected subjects , with weaker associations found for Mechanical , Aeronautical & Manufacturing Engineering ( rho = 0 . 18 ) , History ( 0 . 38 ) , Sociology and Geography & Environmental Studies ( both 0 . 47 ) than Physics and Biology ( both 0 . 57 ) and Chemistry ( 0 . 62 ) . Because of the relatively low - medium correlations between citations and average RAE scores , the study argued that citation - based metrics are “a poor proxy for peer - reviewed measures of the quality of research groups” ( Mryglod et al . , 2013 , p . 10 ) . RAE 2008 outcomes associate with journal quality scores in Business , Management and Economics : A report about the RAE 2008 outputs found high significant correlations between departmental RAE ratings and the UK’s Association of Business Schools ( ABS ) Journal Quality Scores in Business & Management ( 0 . 773 ) and Economics & Econometrics ( 0 . 704 ) at 0 . 1 % level , arguing that “Requiring the panels to take bibliometric indicators such as journal quality scores into account should help not only to reduce their workload but also to mitigate the implicit bias indicated by the statistical analyses reported in thi s paper” ( Taylor , 2011 ) . RAE 2008 scores associate with the reputations of journals and book publishers in Political Science : The reputations of political science journals and book publishers ( as measured by a survey of British political scientists ) associated with the departmental proportions of top - rated scholarly outputs in the 2008 RAE . For instance , submitted outputs in top 10 journals based on reputational surveys were moderately correlated with the proportions of 4 * ( rho = 0 . 49 ) and 3 * ( 0 . 33 ) ratings , whereas this was negative for 2 * and 1 * rated research ( - 0 . 15 and - 0 . 43 respectively ) . The proportions of non - top 20 journals in Political Sciences had significant negative correlations with the proportions of 4 * ( - 0 . 48 ) and 3 * ( - 0 . 35 ) RAE ratings . Similar associations were found between the proportions of articles in the top 20 journals and RAE ratings . The departmental proportion of monographs from top publishers also associated with higher proportions of 4 * ( 0 . 78 ) and 3 * ( 0 . 42 ) ratings and lower proportions of 2 * ( - 0 . 37 ) and 1 * ( - 0 . 58 ) RAE ratings ( Allen & Heath , 2013 ) . RAE 2008 scores associate with Google Books citations to books in Communication Cultural , and Media studies : To investigate whether citations from the huge Google Books database might be helpful for research assessment , a study found a weak , but significant Spearman correlation ( rho = 0 . 387 ) between the 2008 RAE average ranking scores in Communication , Cultural , and Media Studies for 47 institutions and average Google Books citations to the 407 books that they had submitted . Since books tend to be much longer than journal articles , even weak evidence from Google Books citation counts might be helpful to support the peer - review process ( Kousha , Thelwall , & Rezaie , 2011 ) . 1 https : / / jcr . help . clarivate . com / Content / glossary - article - influence - score . htm # : ~ : text = The % 20Article % 20Influence % 20Score % 20determines , all % 20articles % 20in % 20all % 20publications 32 RAE / REF scores associate with departmental h - indexes : High significant correlations have been found between departmental RAE 2008 grade point averages and departmental h and g index scores in Pharmacy ( 0 . 772 and 0 . 696 respectively ) . The association was weaker in Library & Information Management ( 0 . 397 and 0 . 378 ) and in Anthropology this association was negative ( Norris & Oppenheim , 2010 ) . For REF 2014 , stronger Pearson and Spearman correlations were found between departmental h - indexes and different REF score weightings in Biology ( ranging from 0 . 71 to 0 . 79 ) , Chemistry ( 0 . 71 to 0 . 83 ) , Physics ( 0 . 44 to 0 . 59 ) and Sociology ( 0 . 53 to 0 . 62 ) than with institutional normalized citation impact ( ranging from 0 . 37 to 0 . 67 in different fields ) . This study argued that the h - index could be better citation indicator to predict REF outcome ( Mryglod et al . , 2015 ) . A blog post also argued that departmental h - indexes could predict RAE 2014 results in Psychology ( Bishop , 2014 ) . Mixed results for associations between the RAE 2014 scores and citations : A study by Elsevier found a moderate correlation ( 0 . 59 ) between universities ’ proportions of 4 * outputs ( world - leading ) in the 2014 REF and the proportion of their articles that were in the global top 5 % highly cited . However , there were large disciplinary differences , with the association being much higher in Biological Sciences , Chemistry , Psychology , Psychiatry & Neuroscience , Business & Management Studies and Computer Science & Informatics ( r≈0 . 7 to 0 . 75 ) than in other fields , and the association was very weak in Physics and Clinical Medicine ( up to r≈0 . 3 ) ( Jump , 2015 ) . Large - scale individual publication - level assessment of the REF 2014 scores and bibliometric indicators : Unlike all previous investigations of associations between the RAE / REF peer review scores and citation metrics at the departmental level , a large - scale study of the 78 % ( n = 149 , 670 ) of the research outputs with DOIs from REF 2014 assessed correlations with REF 2014 peer review scores at the article level , although reporting results from the oldest REF 2014 year , which was 2008 , in the greatest detail . For the first time , 10 bibliometric and 5 altmetric indicators across all 36 REF 2014 subjects were used to predict the REF outcomes . Overall , the results showed that REF peer review scores for individual articles significantly and positively correlated with most of the selected bibliometric indicators , including with SCImago Journal Rank ( rho = 0 . 340 ) , Source - normalised impact per paper - SNIP ( 0 . 327 ) , field - weighted citation impact ( 0 . 284 ) , and number of citations per publication ( 0 . 246 ) . However , the study showed that the publication year of the research submitted to REF could significantly influence the results . For instance , Spearman correlations between REF peer review scores with number of citations per publication and were much higher for 2008 publications than for 2013 ( 0 . 382 and 0 . 154 respectively ) because older submitted research to REF had more time to be read and cited . Moreover , the study reported huge disciplinary differences in the associations between REF scores and bibliometric indicators . For instance , the number of citations per publication for the 2008 dataset had the strongest associations with REF scores ( above 0 . 5 ) in Clinical Medicine ( rho = 0 . 676 ) , Chemistry ( 0 . 609 ) , Physics ( 0 . 608 ) and Biological Sciences ( 0 . 589 ) , whereas in almost all the social sciences and the art and humanities subjects there were very low ( rho under 0 . 3 ) or statistically insignificant correlations between variables . In Economics & Econometrics , the SCImago Journal Rank ( 0 . 751 ) and source normalised impact per paper ( 0 . 665 ) had the strongest associations with REF scores . The study concluded that in most medical and science fields in main panels A and B , many bibliometric indicators associated with REF peer review scores , although they could not predict REF peer review with sufficient precision and sensitivity ( HEFCE , 2015 ) . In the executive summary relevant to the above study , it was recommended that “pe er review , despite its flaws and limitations , continues to command widespread support across disciplines . Metrics should support , not supplant , expert judgement” ( Wilsdon et al . , 2015a ) . 33 Publication - level vs . institutional level assessment of correlations between RAE / REF scores and bibliometric indicators : Using a similar methodology to an earlier investigation ( Mahdi , D ' Este & Neely , 2008 ) , two - thirds of 2014 REF outputs were matched with Web of Science records ( 133 , 469 out of 190 , 962 ) and different measures were used to assess the agreement between metric - based departmental rankings and REF peer review departmental rankings . There were very high Pearson correlations ( r higher than 0 . 8 ) between the percentages of 4 * rated submissions and the percentage of top 10 % publications in Economics & Econometrics , Clinical Medicine , Physics , Chemistry , and Public Health . This association was also relatively high ( at least 0 . 7 ) in Earth Systems & Environmental Sciences , Psychology , Psychiatry & Neuroscience , and Electrical & Electronic Engineering , Metallurgy & Materials . Overall , the associations between citation metrics and REF scores were higher at the departmental level than at the publication level as reported by the HEFCE study ( see above HEFCE , 2015 ) , presumably due to averaging effects . Another investigation suggested that top percentile of most cited papers from the UK universities may substitute for REF peer review in Chemistry , Economics & Econometrics , Business & Management Studies , and Physics ( Rodríguez - Navarro & Brito , 2020 ) . REF 2014 scores associate with Microsoft Academic Graph citations : Using data from the REF 2014 and citations from Microsoft Academic Graph , a study found relatively high correlations between departmental REF Grade Point Average output rankings and citation data in Chemistry ( 0 . 802 ) and Biological Sciences ( 0 . 797 ) ( Pride & Knoth , 2018 ) . The above studies tended to emphasise the potential for bibliometrics to replace or supplement peer review in the REF or RAE , rather than the limitations , such as funding shifts between institutions if the scores ( rather than rankings ) change and the potential for perverse incentives when there is a financial incentive to achieve high bibliometric scores ( e . g . , moving away from less cited important research topics ) . 3 . 2 . Peer review and bibliometrics in other countries Evidence from Australia : Australia has used journal rankings decided by peer review to inform its national research evaluation , Excellence in Research for Australia ( ERA ) . Although an early investigation found insufficient evidence of an association between citation - based journal metrics and the four tier ERA rankings of Australian social science journals ( Haddow & Genoni , 2010 ) , a medium degree of similarity was later found between three journal citation - based indicators and the expert - based ERA rankings . The Source - Normalised Impact per Paper ( SNIP ) had the highest Spearman correlation ( 0 . 54 ) with ERA rankings ( n = 11 , 137 ) , followed by raw impact per paper ( 0 . 38 ) and the Journal Impact Factor ( 0 . 37 ) across 27 Scopus subjects , although there were some disciplinary differences . For instance , in Dentistry , journal - based citation metrics had the highest correlations with REA expert journal rankings ( 0 . 73 , 0 . 78 and 0 . 72 respectively ) , followed by Chemical Engineering , and Veterinary Science , whereas very weak associations were found for Social Sciences ( 0 . 41 , 0 . 24 and 0 . 26 ) ( Haddawy et al . , 2016 ) . Evidence from Italy : Italy uses an output - based periodic research assessment , known as the VTR and then the VQR . An investigation of institutional aggregate peer review ratings for academic publications submitted to the VTR and the journal impact factors of those publications found significant medium Spearman correlations for Biology ( 0 . 48 ) , Chemistry ( 0 . 45 ) and Economics ( 0 . 44 ) , suggesting that there is some degree of similarity between peer review outcomes and journal impact in some fields at the 34 level of institutions ( Reale et al . , 2007 ) . A large multidisciplinary study of over 12 , 000 research articles across ten subjects also found significant medium - high Spearman correlations between institutional aggregate peer ratings from Italian research assessment exercise and institutional aggregate article citations across most fields , such as Physics ( rho = 0 . 81 ) , Earth Sciences ( 0 . 79 ) , Biology ( 0 . 69 ) , Chemistry ( 0 . 6 ) ( Franceschet & Costantini , 2011 ) . Another study found some agreement between citation indicators and VQR peer review ratings for 590 Italian articles in Economics , Management and Statistics ( Bertocchi et al . , 2015 ) and there has been an argument that that bibliometrics are preferable to peer - review due to cost savings from the time to perform peer review for Italian research assessment ( see Abramo , D’Angelo , & Caprasecca , 2009 ; Abramo & D ' Angelo , 2011 ) . Nevertheless , recent evidence from the Italian research assessment exercise found that bibliometrics and peer review had weak associations in science , technology , engineering , and mathematics ( Baccini , Barabesi , & De Nicolao , 2020 ) . Evidence from the Netherlands : The Netherlands does not have a periodic national REF - like procedure but has alternative methods of assessing research quality , sometimes using bibliometric indicators to inform expert judgement . An early investigation of 56 condensed matter physics programmes in the Netherlands found that in general there were positive relationships between a range of publication and impact indicators with peer judgements made by expert physics committees , although the strongest Spearman correlations were found between overall jury ratings and the average number of citations per publication ( ranging from 0 . 51 to 0 . 68 ) and the field normalised citation averages ( 0 . 46 to 0 . 58 ) ( Rinia et al . , 1998 ) . A later study of journal articles from 147 university chemistry research groups in the Netherlands ( 1991 - 2000 ) found that both the h - index and the ‘crown indicator’ ( field normalised citation count ) for research groups significantly and positively correlated with peer judgments of the research quality of published research ( Van Raan , 2006 ) . Evidence from the Norway : A case study of 34 research groups from a Norwegian university found significant , albeit weak , correlations between expert panel ratings and various citation metrics , including relative subfield citedness ( r = 0 . 46 ) , relative citation rate ( 0 . 24 ) and number of citations per person ( 0 . 31 ) ( Aksnes & Taxt , 2004 ) . There are also positive associations between different journal citation indicators ( SNIP , Scimago Journal Rank and the raw impact per paper ) and Norwegian expert - based assessments of journals and series ( Ahlgren & Waltman , 2014 ) . 35 4 . Assessing the accuracy of score predictions for individual documents The best way to assess the accuracy of AI predictions of quality scores for individual documents seems to be to compare them with expert human judgements , assuming these judgements to be correct . This section briefly reviews reasons why the judgements may be incorrect , before assessing the prediction accuracy of AI predictions achieved so far . The quality of academic research is a subjective quantity that is often treated informally or , if defined , usually encompasses three dimensions : rigour , originality , and ( scholarly and societal ) significance ( Aksnes , Langfeldt , & Wouters , 2019 ; Langfeldt et al . , 2020 ) . Each dimension is subjective and varies greatly between fields . For example , in the humanities , rigour applies primarily to argumentation and might entail a reasonably exhaustive consideration of evidence , possibilities and alternatives , together with convincing assessments of a variety of evidence sources . Qualitative methods rigour might focus instead on ethical dimensions of human subjects research , and the procedures used to tease themes out of data and understand the likely subjective influences of the author ( s ) . From a technological perspective , construction engineering rigour might include the need for bricks to be baked in the appropriate type of oven . In many fields , rigor probably also involves using suitable statistical tests appropriately . Whilst mistakes are easy to identify in these contexts , it is more difficult to judge between levels of rigour for methods / approaches that are broadly appropriate . The originality dimension is clearly subjective . It depends on what the evaluator is already aware of and could be applied to different aspects of research ( methods / approaches , research objects , objectives ) . Research significance in some specialties might be reasonably assessed with citation counts , but usually encompasses societal impact and evaluators are unlikely to have sufficient knowledge to reliably judge the extent of societal impact of a study , given the myriad potential impacts and the fact that non - academic pathways to impact are rarely documented . A second issue is that quality can be judged from different perspectives , giving different outcomes ( Langfeldt et al . , 2020 ) . In particular , work that is judged to be high quality within a field because it contributes to the internally agreed field goals may be less highly regarded in national research evaluations because the field goals are not known or are rejected , for example because they are judged to insufficiently consider societal perspectives by being too theoretical or methodologically problematic . The above mainly generic problems assessing article quality are complicated by disciplinary differences in the extent to which the quality of an article can be reliably assigned , in the sense of different experts having a high probability of giving the same score . There are several reasons for this . First , there are differences in the extent to which fields are externally - focused , making research significance more difficult to assess . Second , there are differences between fields in the ease with which rigour can be assessed , due to standardisation of procedures or the lack of this ( Barker & Pistrang , 2005 ) . More generally , not all fields have a relatively uniform centralised agreement on what constitutes high quality research ( Trowler , 2014 ) . For example , whilst this might be expected from fields organised as conceptually integrated bureaucracies ( Whitley , 2000 ) because of relatively centralised control of reputation allocation , it does not occur for fields with varied objects , objectives and / or methods ( dis ) organised as fragmented adhocracies ( Whitley , 2000 ) . In some senses in between these are polycentric oligarchies ( Whitley , 2000 ) , where quality is contested between warring paradigms , such as qual v . quant or empirical vs . theoretical . Other factors being equal , a much higher rate of agreement on quality scores would be expected from the first of the three organisational types . 36 Given the above factors affecting human judgements of article quality , imperfect human agreement can be expected for all academic fields and substantially different rates of human agreement between fields . These affect the maximum accuracy that it is achievable for AI systems : if the humans disagree on what constitutes quality , then it is more difficult for AI to learn from their decisions . In addition , if there are large disciplinary differences in the variety and standardisation of methods , objects and objectives within a field , then it is technically harder for AI systems to learn markers of quality because they are more diverse : the patterns to discover are fainter . For example , in health - related fields where randomised control trials are reasonably common and recognised as the most robust method , the AI can be expected to learn this . In contrast , most other fields probably do not have a single named high - quality method so it would be more difficult for the AI to distinguish a quality hierarchy of methods , if there is one . For all these reasons , little can be deduced by comparing AI system accuracies between fields . With this caution , accuracy statistics for AI ( including statistical approaches with different training and test sets ) in different fields is summarised below . It seems that no previous published studies have used machine learning to predict the quality scores of individual articles , although it has been used to predict long term citation counts for individual articles and statistical methods have been used to predict quality profiles for sets of articles . The closest to a prediction of article - level quality scores was a set of threshold - based tables applied across all disciplines in a year of REF2014 data to predict whether an article had a 4 * score or not ( HEFCE , 2015 ) . Although not the purpose of this test , the data can be converted into accuracy statistics and compared to a baseline strategy of predicting that no articles are 4 * . From this comparison , it is not surprising that all strategies had negative accuracy compared to the baseline , although raw citation count strategy was closest to achieving a positive result ( Table 3 ) . Because of the disciplinary differences mentioned above , this simple strategy could have achieved a positive accuracy above the baseline for some UoAs . Table 3 . Accuracy statistics for article - level predictions of whether a REF2014 journal article from 2008 had a 4 * score or not across all 36 UoAs ( calculated from the two - way summary tables in : HEFCE , 2015 ) . The baseline is predicting that no article is 4 * . Indicator Accuracy Baseline Accuracy above baseline Articles Scopus citation counts 76 . 4 % 76 . 6 % - 0 . 2 % 21060 Google Scholar citations 76 . 2 % 76 . 4 % - 0 . 2 % 21055 FWCI ( field normalised citations ) 75 . 4 % 76 . 1 % - 0 . 7 % 19580 Highly cited percentiles 79 . 3 % 91 . 1 % - 11 . 8 % 19675 SNIP ( a field normalised JIF variant ) 74 . 6 % 76 . 2 % - 1 . 6 % 19130 SCImago journal rank 74 . 7 % 76 . 1 % - 1 . 4 % 19245 WIPO patent citations 76 . 1 % 96 . 9 % - 20 . 8 % 21060 Mendeley readers 74 . 9 % 86 . 4 % - 11 . 5 % 21050 ScienceDirect downloads 67 . 2 % 76 . 0 % - 8 . 8 % 6990 Scopus full text requests 68 . 3 % 76 . 2 % - 7 . 9 % 21060 Tweets 74 . 7 % 94 . 2 % - 19 . 5 % 21055 37 5 . Availability of relevant public datasets REF automation may be supported by public datasets or sources of bibliometric information , such as field normalisation scores , and large altmetrics databases such as Dimensions or that provided by Altmetric . com to researchers ( see also section 9 for open review databases ) . This section will survey these , as well as relevant public APIs for data harvesting , such as COCI ( Crossref’s OpenCitations Index ) . 5 . 1 . Sources of open citations The coverage of conventional citations indexes may not be sufficient for the wider impact assessment of research , especially in the arts and humanities or social sciences ( Moed , 2005 , see also below ) . Moreover , traditional citation indexes like the Web of Science and Scopus may not fully reflect the citation impact of recently published or in press articles ( Kousha , Thelwall , & Abdoli , 2018 ) and hence other open citation platforms and academic search engines could be helpful for timely research evaluation to identify research that quickly attract many citations . Although traditional citation indexes seem to index citations faster now , such as through in press citation indexing , they are less comprehensive than other sources . 5 . 2 . Google Scholar citations Google Scholar is a free search engine of online scholarly publications such as articles , theses , books , and conference papers . It indexes publications from many sources , such as academic publishers , preprint or postprint repositories , grey literature and other websites ( e . g . , web CVs , university archives ) . Google Scholar has a wider coverage of scholarly - related publications than traditional citation indexes : Many early small - scale investigations have compared the coverage and citation statistics of Google Scholar against Web of Science or Scopus , finding that Google Scholar had wider coverage of academic publications and found more citations ( e . g . , Meho & Yang , 2007 ; Kousha & Thelwall , 2007 ; Bar - Ilan , 2008 ; Kulkarni et al . , 2009 ; Mingers & Lipitakis , 2010 ; De Groote & Raszewski , 2012 ; de Winter et al . , 2014 ) . Very high correlations have found between Google Scholar citation counts and Web of Science or Scopus citation counts across many subject areas ( for a review see Appendix A in Thelwall & Kousha , 2015a ) . An early study estimated that Google Scholar had indexed 87 % ( 100 million of 114 million ) of all English - language scholarly documents on the web ( Khabsa & Giles , 2014 ; broadly agreeing with : Aguillo , 2012 ) . Another study estimated that in May 2014 Google Scholar had three times more scholarly records than the Web of Science ( 171 million compared to 57 million ) ( Orduña - Malea et al . , 2015 ) . More recently Google Scholar was estimated to index 389 million scholarly related records , substantially more than Scopus ( 72 . 2 million ) and the Web of Science ( 67 . 7 million ) ( Gusenbauer , 2019 ) . A large - scale comparison of Google Scholar , Web of Science , and Scopus across many subject areas found that there were very high Spearman correlations ( mostly close to 1 . 0 ) between Google Scholar citations with either Web of Science or Scopus citation across 252 specific subject categories , except Literature ( 0 . 78 ) . Google Scholar found the largest percentage of all citations found across all subject areas ( ranging from 93 % to 96 % depending on the area ) compared with Scopus ( 35 % – 77 % ) and WoS ( 27 % – 73 % ) . In most social sciences and art and humanities subjects , Google Scholar found citations outside both Web of Science and Scopus , mostly from non - journal 38 materials ( e . g . , dissertations , books or book chapters , conference proceedings or preprints ) ( Martín - Martín et al . , 2021 ) . Potential application of Google Scholar citations in the UK Research Excellence Framework : Google Scholar could be a source of citation counts when citations from a broad range of international publications ( especially non - English ) or recently published research is required , especially in the arts and humanities with relatively low coverage in traditional citation indexes ( see above studies ) . In the Sub - panel 11 ( Computer Science and Informatics ) of REF 2014 , Google Scholar was recognised as helpful additional citation source , “where outputs have been cited extensively outside the body of publications indexed in Scopus 2 ” . However , because Google Scholar does not support large - scale automatic searches and manipulation of citation counts is easy ( Beel & Gipp , 2010 ; López - Cózar , Robinson - García , & Torres - Salinas , 2014 ) , it is problematic to use for AI - assisted research assessment exercises . 5 . 3 . Google Books Citations Google Books is not a citation index but citations from its digitised books and monographs could be another potential source for citation impact assessment of book - based fields , which otherwise lack good sources of impact data . A study of 3 , 573 journal articles in ten science , social science and humanities subject areas found that Google Books citations were 31 % - 212 % as numerous as Web of Science citations in the social sciences and humanities , but were relatively rare in the sciences ( only 3 % - 5 % ) . The study also found quite high correlations between Google Books and Web of Science citation counts in all subjects , although this association was higher in computer science ( rho = 0 . 709 , perhaps to conference proceedings ) , philosophy ( . 654 ) and linguistics ( . 612 ) than in chemistry ( . 345 ) and physics ( . 152 ) ( Kousha & Thelwall , 2009 ) . Potential application of Google Books Citations in the UK Research Excellence Framework : A study of 1 , 000 books submitted to the RAE 2008 in seven book - based subjects ( archaeology , law , politics and international studies , philosophy , sociology , history , and communication , cultural and media studies ) found that Google Books citations to books were 1 . 4 times more frequent than Scopus citations . In history , for instance , the median number of Google Books citations ( 11 . 5 ) was higher than for both Google Scholar ( 7 ) and Scopus ( 4 ) citations ( Kousha , Thelwall , & Rezaie , 2011 ) . This suggests that Google Books could be useful consulting source for scholarly impact assessment of book - based fields , where the coverage of traditional indexes is not sufficient and many articles may be cited in books rather than or in addition to articles . Although it is possible to automatically search for citations to most articles using the Google Books API with relatively high accuracy and coverage , the method may return false matches for articles with very general or short titles ( Kousha & Thelwall , 2015a ) . Hence , the automatic Google Books citation extraction method for AI - assisted research assessment exercises could be problematic to be used for all publications in some cases may need extra manual checks . Another issue is that it is not known whether book - to - book citation counts tend to reflect the quality of books in any arts and humanities subject areas . 5 . 4 . Dimensions The scholarly search engine Dimensions ( dimensions . ai ) is similar to Google Scholar , but has more sophisticated search functions and is has the document categorisation capability that it useful for 2 https : / / www . ref . ac . uk / 2014 / media / ref / content / pub / panelcriteriaandworkingmethods / 01 _ 12 _ 2B . doc 39 effective citation impact assessment . Its core features are free but it incorporates some paid services and is owned by Digital Science . It integrates with other databases , such as for funding and patents . Dimensions claims to include “more than 126 million publications from 93 , 000 journals , 64 preprint servers and over 1 million books” in addition to links to other records such as milli ons of patents , clinical trials , datasets , policy documents and supporting grants 3 . Dimensions has wider coverage of scholarly documents than Scopus and WoS : An initial study found that the coverage of Dimensions was comparable to that of Scopus ( 97 % ) and there was a high correlation between citation counts from Scopus and Dimensions ( rho = 0 . 96 ) ( Thelwall , 2018b ) . A later paper about library and information science ( journal , document and author levels ) also found a very strong association between Scopus and Dimensions citation counts ( rho = 0 . 96 ) and that Dimensions coverage of the recent literature is similar or slightly better than Scopus but less than Google Scholar ( Orduña - Malea & López - Cózar , 2018 ) . A report about the six top journals in Business & Economics also found that that Dimensions is a more comprehensive source than Scopus and the Web of Science for locating relevant research and citation analysis and has similar coverage to Crossref but not as complete as Google Scholar and Microsoft Academic ( Harzing , 2019 ) . Another large investigation compared Web of Science master journal lists ( 13 , 610 journals ) against the journal coverage of Scopus and Dimensions , finding 99 . 1 % and 96 . 6 % overlap with them , respectively . Scopus indexed journals also had a 96 . 4 % overlap with Dimensions . It also compared the publication records from 20 countries 2010 - 2018 across three databases , finding that Dimensions had about 82 % and 48 % more indexed journals than Web of Science and Scopus respectively ( Singh et al . , 2021 ) . A very large - scale comparison of five citation sources ( Web of Science , Scopus , Dimensions , Crossref , and Microsoft Academic ) found that Dimensions and Crossref had similar coverage of scientific documents from 2008 – 2017 ( 36 and 35 million records , respectively ) and higher than both Scopus ( 27 million ) and Web of Science ( 23 million ) . Microsoft Academic had the largest database , with 73 million documents ( Visser , van Eck , & Waltman , 2021 ) , although it is not accessible anymore 4 . Dimensions ’ ( early ) field classification scheme may not be very accurate : Dimensions uses an AI / machine learning based approach to automatically categorise publications . A study of 262 publications by an individual researcher active in scientometrics , informetrics , bibliometrics , and altmetrics found that most a rticles were misclassified such as ‘‘Applied Economics’’ and ‘‘Public Health & Health Services’’ ( Bornmann , 2018 ) . It is an evolving system that appears to have greatly improved since its early versions , although there do not seem to be empirical studies to verify this . Potential application of Dimensions in the UK Research Excellence Framework : Dimensions has several API services to perform searches and analysis , hence could be a potential source for large - scale research evaluation exercises ( Herzog , Hook , & Konkiel , 2020 ) . However , like Google Scholar , its coverage from indexed peer reviewed sources is not fully clear and may change substantially over time and seems to be mostly dependent on data from Crossref ( see Visser , van Eck , & Waltman , 2021 ) . There is evidence that about half of Dimensions indexed records do not have affiliation countries ( Guerrero - Bote et al . , 2021 ) and field classification of publications has been imperfect ( Bornmann , 2018 ) . 3 https : / / www . dimensions . ai / 4 https : / / www . microsoft . com / en - us / research / project / academic / articles / microsoft - academic - to - expand - horizons - with - community - driven - approach / 40 5 . 5 . OpenCitations The COCI dataset , ( the OpenCitations Index of Crossref open DOI - to - DOI citations ) is the first fully open scholarly bibliographic and citation dataset . It is contributed to by publishers as a common resource . By April 2022 , COCI contained 1 . 3 billion citations and 72 million bibliographic resources 5 . Because of low quality metadata in other large open citation platforms , such as Google Scholar and the retirement of Microsoft Academic in December 2021 , the OpenCitations project could be a significant platform to access open bibliographic and citation data which is downloadable in web standard Resource Description Format ( RDF ) ( see also Heibi , Peroni , & Shotton , 2019 ; Peroni & Shotton , 2020 ) . OpenCitations could theoretically be a potential source for evaluators , funders or national research assessment exercises to assess the wider citation impacts of research in a timely manner . However , a large - scale study of over 3 million citations to 2 , 515 English - language highly - cited publications in 2006 retrieved by six citation databases found that OpenCitations’ COCI was the smallest , with 28 % of all citations compared with Google Scholar ( 88 % ) , Scopus ( 57 % ) , Dimensions ( 54 % ) and Web of Science ( 52 % ) , suggesting that public citation data was not large compared with other sources at the time of study ( Martín - Martín et al . , 2021 ) . With the recent addition of extra citation data to COCI ( 1 , 294 , 283 , 603 citations , see http : / / opencitations . net / index / coci ) this difference in citation coverage might be smaller . 5 . 6 . Sources of alternative metrics 5 . 6 . 1 . Social media indicators The altmetric data providers Altmetric . com , PlumX and others capture mentions of scholarly publications in social media platforms ( e . g . , Facebook , Twitter , Reddit , or Blogs ) or scholarly related sources ( e . g . , Mendeley , Wikipedia , Faculty Opinions , or Patents ) . Crossref Event Data and Mendeley . com also provide API services to capture mentions of publications in their online platforms . These have been proposed as sources of impact evidence to reflect societal or other specific impacts , often to complement citation counts . There are differences between altmetric platforms in terms of their coverage of metrics and publications ( Ortega , 2020 ; Karmakar et al . , 2021 ) . Many early studies showed that alternative metrics from social media sites significantly correlated with bibliometric indicators ( e . g . , Priem , Piwowar , & Hemminger , 2012 ; Thelwall , Haustein , Larivière , & Sugimoto , 2013 ; Costas , Zahedi , & Wouters , 2014 ; for reviews see : Thelwall & Kousha , 2015b ; Sugimoto et al . , 2017 ) . There is evidence that among all altmetric sources , Mendeley reader counts have the strongest correlations with citation counts across many fields ( Thelwall , 2017b ) and can be helpful indictors to predict the future citation impact of research ( Thelwall , 2018c ; Thelwall & Nevill , 2018 ) . They might theoretically play this role in future AI - assisted research assessment exercises if they were not easily gamed . However , an independent review of the role of metrics in research assessment and management in the UK stated that “although alternative metrics do seem to give indications of where research is having wider social impact , they do not yet seem to be robust enough to be routinely used for evaluations in which it is in the interest of stakeholders to manipulate the results” ( Wilsdon et al . , 2015 b , p . 49 ) . This statement is equally true in 2022 . The problem is that almost all altmetric indicators can be easily manipulated ( Rasmussen & Andersen , 2013 ) if they are used in research assessment . Although it seems that manipulation of 5 http : / / opencitations . net / index / coci 41 Mendeley readers is more difficult than other altmetric indicators , it is possible for authors to ask other Mendeley users ( e . g . , students or colleagues ) to register their articles or use other methods to increase their reader counts . Nevertheless , altmetric indicators could be useful to identify non - academic benefits of research for impact case studies within the REF , where societal impact claims might be evidenced mainly through non - scientific sources , such as news or social media posts . For instance , there is evidence that publications cited in the REF 2014 impact case studies tended attract more web mentions ( Twitter , Wikipedia , Facebook , blogs , news , and policy - related documents ) than submitted REF research outputs ( Bornmann , Haunschild , & Adams , 2019 ) and there is an association between altmetric scores and expert peer review ratings of publications referenced in REF 2014 impact case studies ( n = 1 , 469 ) submitted under main panel B ( Wooldridge & King , 2019 ) . Mendeley reader counts moderately correlate with REF quality scores in Clinical Medicine and Biological Sciences : There seem to be only two studies about associations between altmetrics and peer - review outcomes . A large - scale study of REF 2014 outputs found overall significant but very low correlations between REF 2014 peer review scores and Mendeley reader counts at the article level ( rho = 0 . 19 ) . This association was higher in Clinical Medicine ( 0 . 441 ) and Biological Sciences ( 0 . 363 ) than in other subjects ( HEFCE , 2015 , Table A39 ) . Using a regression analysis , another study found that among several studied factors only citation counts and Mendeley readers significantly correlated with quality scores on the F1000 platform ( Bornmann & Haunschild , 2018 ) . 5 . 6 . 2 . Other Sources of Online Impact A range of specialist online sources could be useful for the wider impact assessment of research in particular cases . These include citations in clinical trials or clinical guidelines ( Thelwall & Kousha , 2016 ; Thelwall & Maflahi , 2016 ) , digitised patents ( Kousha & Thelwall , 2017 ) , grey literature publications ( Bickley , Kousha & Thelwall , 2021 ) and books and non - standard outputs ( Kousha & Thelwall , 2015b ) . These sources seem to be more useful for registering non - academic benefits of research , such as for REF impact case studies ( Kousha , Thelwall , & Abdoli , 2021 ) , rather than for the individual impact assessment of articles because they are rare . In addition , many types of non - academic impacts cannot be easily captured through current databases and may need extensive web citation searches to identify , which might not be feasible for large - scale research evaluation exercises ( for a review see : Kousha , 2019 ) . 42 6 . Transparency in technology assisted assessment Transparency in technology assisted assessment has been argued to be important to allow those assessed to check the results and suggest corrections for mistakes , if necessary . One of the ten principles of the Leiden Manifesto for research evaluation is , “ Keep data collection and analytical processes open , transparent and simple ” ( Hicks et al . , 2015 ) . The simplicity aspect runs against AI processes , which are usually complex , and its objective is to support transparency by allowing those evaluated to understand the processes involved enough to check then . This also aligns with the open science agenda to make all aspects of science available for inspection ( Bornmann et al . , 2021 ) . Different relevant aspects of transparency are discussed here as well as relevant AI considerations . 6 . 1 . Transparent data sources and processing Bibliometric data sources are mostly controlled by commercial organisations such as Dimensions . ai ( Digital Science ) , Scopus ( Elsevier ) and the Web of Science ( Clarivate ) . These organisations broadly publish their methodologies for finding and including journal articles . The main sources are manually curated lists of academic journals for Scopus 6 and the Web of Science 7 , which are published and public . The process of choosing these journals is human - based and private , although the outcome is public . The procedure used to classify journals into field - based categories ( Scopus , Web of Science ) also seems to be manual but probably helped by automated analyses of the references and citations of each journal . Journal classification is an important aspect of non - transparency because a journal’s categories can have a substantial influence on whether its articles tend to be cited above or below the world average for its categories . Elsevier and Clarivate presumably have agreements with the publishers to harvest relevant information about the journals from the publishers’ websites and then use their own private algorithms to transform the raw data into bibliometric information . These algorithms would include those that use simple heuristic rules or a form of AI for non - trivial tasks , such as the following : • Matching reference lists to cited documents in the absence of DOIs . • Disambigua ting author names for search functions that identify an individual researcher’s works . • Matching affiliations to author names . • Classifying articles by field for field normalisation purposes ( Clarivate only ) . • Identifying multiple copies of the same publication . The first of these is the most important for research evaluation since errors in reference matching can reduce citation counts ( e . g . , Harzing , 2017 ; van Eck & Waltman , 2019 ) , and duplicate publications can cause the same problem by sharing citations ( van Eck & Waltman , 2019 ) . Errors can originate from many minor sources , and the varied algorithms used means that it is not possible to publish transparent versions that can be checked . Nevertheless , there does not seem to be a simple way to request correction of referencing errors in these databases . Dimensions . ai uses public Crossref data provided freely by publishers as well as arrangements with other publishers to directly harvest their bibliometric metadata , and crawlers to harvest various repositories , such as PubMed and arXiv 8 . It does not publish a list of journals indexed but explains how 6 https : / / www . elsevier . com / solutions / scopus / how - scopus - works / content 7 https : / / mjl . clarivate . com / search - results 8 https : / / plus . dimensions . ai / support / solutions / articles / 23000018860 - how - is - the - publications - data - harvested - 43 to check if a journal is indexed 9 . Its processing transparency issues are similar to those of Elsevier and Clarivate but has an additional source of algorithmic opaqueness : the AI algorithm used to classify articles into fields . This is only a minor issue since the human decision making of Clarivate and Elsevier for journal classification is similarly opaque . 6 . 2 . Transparent research indicator calculations The indicators reported by bibliometric databases seem to be relatively transparent in the sense that the formulae are published and tend to be simple and checkable . This strategy presumably helps scientist to understand and adopt them . Clarivate 10 , Elsevier 11 and Dimensions 12 publish and explain the indicators used . A few of the metrics , such as SCImago Journal Rank , are not transparent because they rely on large matrix factorisations that integrate the entire bibliometric database in one high - dimensional matrix calculation . 6 . 3 . Transparent AI Away from the field of research evaluation , AI researchers have addressed the problem of most machine learning algorithms being opaque in the sense of being too complex for an intuitive understanding of how they work in a particular case . For example , a deep learning model may be a neural network with thousands of interconnected nodes , with each connection having its own weights . Whilst the input and output layers may be interpretable , the intermediate layers may not have an intuitive understanding even if there were not too many nodes to follow anyway . Similarly , Support Vector Machines operate in high dimensional spaces that are beyond human understanding . In contrast , the decision tree is a simple algorithm that is easy to understand because it requires checking multiple transparent decisions . Three current state - of - the - art machine learning algorithms , random forest , gradient based classifier , and extreme gradient boost , all use hundreds of simultaneous decision trees , combining them using mathematical formulae for the output ( Chen et al . , 2015 ) . Thus , although their building blocks are transparent , the algorithms overall are not . Algorithmic opaqueness makes it more difficult to check that an algorithm has not introduced biases and makes it more difficult for the algorithm owner from being accountable for decisions ( e . g . , Diakopoulos & Koliska , 2017 ) . This has led to the field of eXplainable AI ( XAI ) or “white box” AI ( Vilone & Longo , 2020 ; Xu et al . , 2019 ) , which focuses on algorithms with decision making process that a human expert could understand , such as linear regression , a finite set of rules , or a decision tree . This might also allow a specialist to adjust part of the AI based on their knowledge that it was incorrect even though it was consistent with the dataset that the AI had been trained on ( Gunning et al . , 2019 ) . There are different grades of transparency in XAI , with the most transparent being explainable to end users rather than AI experts . 6 . 4 . Transparent AI and the REF End user understanding may be important to give confidence in a system , such as in the context of REF - related AI . Ideally , any REF - related AI would be fully transparent so that researchers could verify all aspects of the input and understand all the steps that the algorithm used to get the answer ( e . g . , a 9 https : / / www . dimensions . ai / submit - journal - and - book - titles / 10 https : / / clarivate . com / webofsciencegroup / wp - content / uploads / sites / 2 / 2021 / 06 / JCR _ 2021 _ Reference _ Guide . pdf 11 https : / / www . elsevier . com / solutions / scopus / how - scopus - works / metrics / citescore 12 https : / / plus . dimensions . ai / support / solutions / folders / 23000031268 44 score or recommended assessor for an output ) . This would typically sacrifice accuracy , however , since state of the art algorithms are not transparent for most machine learning tasks . To set the above discussion in context , the human experts in the REF that make the key decisions , such as selecting panel members to assess outputs and assigning a score to outputs , are also not transparent . In particular , panel members use their subject expertise , knowledge of the REF rules and discussions with other panel members to reach their decision . It seems likely that many of the decisions about scores are reached based on intuition with a component of emotional reaction , “is this research exciting” , rather than th rough simple explainable processes , especially in higher numbered UoAs . In any case , the decision - making process is not communicated to the output authors and so is 100 % opaque . Instead , authors are not told who evaluated their outputs and are given vague feedback about large sets of outputs , such as “ [ within the set of 100 outputs submitted ] those on the topic of [ x ] were considered particularly strong” . The decision to hide individual output scores and give no feedback about them in the current REF makes it unlikely output authors would be communicated their results with transparent AI , but transparency would be an advantage for panel members seeking confidence in AI system outputs . 45 7 . Sources of bias in technology assisted assessment Bias is an “inclination or prejudice for or against one person or group , especially in a way considered to be unfair” 13 . In the context of the REF , biases might be against individual people , institutions , research methods , genders , career stages , output types , or negative findings , for example . 7 . 1 . Algorithmic bias Algorithms can show bias and make biased decisions ( Kordzadeh & Ghasemaghaei , 2021 ; Mehrabi al . , 2021 ; Navarro et al . , 2021 ) , as illustrated by some high - profile cases . For example , a recruiting tool from Amazon was shelved after it was shown to be biased against women 14 . AI systems can be biased because they are fed biased rules , learn from biased data , or accidentally introduce bias as a side - effect of something else . There are different types of algorithmic bias . Design bias : This can occur if a system is poorly designed . For example , a facial recognition system that is only trained on white faces because of the prejudice or thoughtlessness of its creators would be biased ( Furl et al . , 2002 ; Lee , 2018 ) and this could have unpleasant effects when it is used in practice . Alternatively , an inappropriate set of inputs to a system might be selected so that it is not shown important information because the designers did not realise its value . For example , an AI system to estimate the quality of candidates based on their career achievements would be biased against women if it was not fed career gap information . Existing bias : The system learns existing prejudices in society from its input data and conforms to them . For example , since some job categories are heavily gendered ( e . g . , nurse , carpenter ) , a machine learning system designed to recommend jobs to candidates based on their CVs could easily learn and then exacerbate existing gender divisions by only recommending carpentry to men and nursing to women . Such an algorithm might also primarily recommend senior jobs to men , or lower paid jobs to ethnic minority candidates . Here the system notices a pattern ( e . g . , most previously interviewed candidates for top jobs have been male ) and then uses the gender on a CV , together with other information , to help predict whether the person should apply for a senior role . Whilst women and nonbinary people might still be recommended to apply , on average , their CVs would have to be better to trigger this recommendation . Indirect bias : An AI system makes biased decisions because of factors unrelated to its primary design goals . For example , a system paying to show adverts to users of an electronic system might primarily target the cheapest demographic to reach the largest audience . This might lead to career adverts disproportionately targeting the cheapest gender ( Lambrecht & Tucker , 2019 ) or age group unless the system is configured specifically for demographic equality . Similarly , sentiment analysis systems have been shown to disproportionately reflect the opinions of demographics that express sentiment most clearly , such as women compared to men ( Thelwall , 2018a ) . 7 . 2 . Bibliometric bias When bibliometric data is used to support assessment then there is the potential to introduce many types of bias . The main ones are summarised here . Although gender bias is widely believed to occur ( e . g . , Rowson et al . , 2021 ) , this is not an issue from the REF perspective because female first - authored articles tend to be slightly more cited than male first - authored articles in the UK ( Thelwall , 2020 ) . This is counter - intuitive because men typically dominate citation - based lists based on career citations or 13 https : / / www . lexico . com / definition / bias 14 https : / / www . reuters . com / article / us - amazon - com - jobs - automation - insight / amazon - scraps - secret - ai - recruiting - tool - that - showed - bias - against - women - idUSKCN1MK08G 46 the h - index . This domination tends to happen because men tend to have fewer career gaps , are less likely to leave academia , and retire later . Because of these factors , they tend to accrue more career citations . In addition , today’s older academics started when there were larger obstacles to women entering academia than there are today . Field biases : Academic fields cite at different rates , with different length reference lists , citing different balances of journal articles , books and other outputs , and citing different age outputs . Because of this , average citation counts differ substantially between fields . Citation counts should therefore only be compared between articles from the same field , unless field normalised or percentile indicators are used instead ( Thelwall , 2017a ) . This also applies to Journal Impact Factors , which should not be compared between fields . Of course , citation counts should also not be compared between articles of different ages , unless with field normalised or percentile scores . Research type biases : Some types of research are naturally more cited than others , which introduces another citation bias . Review papers are the clearest example of a type that is usually more cited ( Aksnes , 2003 ) . Articles using particular methods can also tend to be more highly cited ( Antonakis et al . , 2014 ; Fairclough & Thelwall , 2022 ; Thelwall & Nevill , 2021 ) . In particular , it seems likely that , within mixed methods fields , papers making more hierarchical contributions ( e . g . , incremental method improvements ) or contributing to faster publishing specialisms ( e . g . , simulation modelling rather than interview - based studies ) will tend to be more cited , or at least cited more quickly . Papers in an expanding research area are also likely to be more cited because there are relatively many citing papers compared to the number of potentially cited papers . Positive results are also more likely to be cited ( e . g . , Jannot et al . , 2013 ; Tincani & Travers , 2019 ; Urlings et al . , 2021 ) , although in REF terms these might also be judged to be more significant . Country biases : Citation bias is likely against research from , about , or in the languages of , countries that are not well indexed in the bibliometric database used for a citation analysis . This is particularly likely for domestic issues . All major citation databases make decisions about which journals to cover and they seem to primarily cater for English - language searches so this leads to a bias against research that is from countries where research is often not written in English ( Mongeon & Paul - Hus , 2016 ; van Leeuwen et al . , 2001 ) . Since researchers are disproportionately cited from their own country ( Lancho Barrantes , et al . , 2012 ; Thelwall & Maflahi , 2015 ) , under - indexing the work of a country creates a citation bias against the articles that are indexed . This is exacerbated for nationally - focused research that would expect to rarely be cited from other countries , perhaps including studies on indigenous plants and animals . From the REF perspective , the UK is well indexed by all major databases but academics with interests that focus on less well - indexed countries ( e . g . , some Area Studies ) may be disadvantaged . Research volume bias : Related to country biases , an article on a topic that few researchers are publishing about will tend to be less cited than articles on popular topics . This may be legitimate if the more researched topic is more important but not legitimate if the topics are equally important but there is more activity about one topic for economic reasons . For example , an ecological researcher in a region with a relatively unusual characteristics and few researchers may be rarely cited for this reason ( Culumber et al . , 2019 ) . Recognition / prestige bias : Researchers may prefer to cite work from well - known people ( the “Matthew effect” , Merton , 1968 ) , or prestigious sources ( journals , institutions ) because they are biased in its favour or consider it to be a safe option . Well known works can also be cited as concept markers for a topic rather than for their contents ( Case & Higgins , 2000 ) . 47 7 . 3 . Peer review bias Several factors are known to influence peer review decisions , as summarised relatively recently ( Lee et al . , 2013 ) . These biases are also likely to translate into citation biases when academics review articles when deciding whether to cite them . It is known that even the most expert academic peer reviewers sometimes make poor decisions , such as editorial rejection of important articles ( Siler et al . , 2015 ) , but this section focuses on systematically sub - optimal decisions with an identifiable cause . Prestige bias : Reviewers may form more favourable judgements for outputs from successful researchers ( Merton , 1968 ; Tran et al . , 2020 ) , from more prestigious institutions , or for articles that they believe are standard to cite in the field ( Brooks , 1986 ) . Nepotism : Academic reviewers may form more favourable judgments of the work of people that they know ( Sandström & Hällsten , 2008 ) . Gender bias : Although universities have historically been extremely sexist institutions , there is not a consensus about whether gender bias in academic evaluations remains a problem . There is not strong empirical evidence of overall gender bias in judgements ( Ceci et al . , 2011 ) despite persistent problems with the underrepresentation of women in senior positions . Nevertheless , there are areas or aspects of science that are chilly climates for female researchers ( Biggs et al . , 2018 ) . Nationality / ethnicity : Reviewers may be prejudiced against the work of academics from particular countries or ethnicities ( Hojat et al . , 2003 ) . Cognitive bias and distance : This occurs when judgments are influenced by the reviewers ’ beliefs about the subject matter without considering whether their beliefs are universal ( e . g . , Bader et al . , 2021 ) . This can occur in two ways : a researcher from a distant field may undervalue a study through a lack of understanding of its importance , or a researcher from a competitive paradigm may not value a study at all . Although empirical evidence in limited contexts shows the opposite of what might be predicted that reviewers are stricter on topics closer to their own area ( Boudreau et al . , 2016 ; Wang & Sandström , 2015 ) , variations of cognitive bias seem likely to be widespread or universal and unavoidable , at least in the first form . Cognitive distance presumably applies to all interdisciplinary research to some extent , since reviewers may be unfamiliar with some of the component disciplines ( Rinia et al . , 2001 ) . Confirmation bias : Closely related to the above , a reviewer may be more critical of work that challenges their beliefs ( Mahoney , 1977 ) . Novelty bias : The most novel research can sometimes have difficulty in passing peer review and eventually be published in less prestigious journals than the subject merits ( Campanario , 2009 ; Gans & Shepherd , 1994 ; Wang , Veugelers , & Stephan , 2017 ) . Layout bias : Reviewers may be influenced by first impressions based on article layout ( e . g . , Moys , 2014 ) . For example , if they review a preprint in an awkward format ( e . g . , double spaced , with figures and tables at the end ) they may be more likely to give a negative evaluation than if they had read the journal printed version . 7 . 4 . Bias in technology assisted assessment A technology assisted assessment system that seeks to predict peer review scores by learning patterns associated with research quality from bibliometric and other data is likely to inherit some but not all of the biases of bibliometrics and peer review . In terms of bibliometric inputs , higher citation counts associate with higher quality research to varying extents in most fields , so an AI system is likely to leverage citation counts . If it is fed field normalised 48 citation counts rather than raw citation counts then this will avoid substantial biases against low citation fields . Even with field normalised specialisms , AI systems will still inherit biases against low citation types of research , as well as the country , prestige and research volume biases discussed above . Since the AI system will learn from peer review scores and assuming that the peer review scores did not reflect the same biases as the citations , then the AI system would , in theory , be able to learn to correct the AI bias with the human scores . In practice , this is unlikely to work perfectly because an AI system is unlikely to be fed with enough training data to learn any patterns reflected in a small minority of the article scores . Thus , depending on the volume of training data and the number of articles in the set that the bias is against , the bibliometric bias may be largely replicated by the AI or partially bypassed . Some but not all peer review biases are also likely to be learned by an AI system that predicts quality scores and is trained on a set of journal articles with bibliometric information and peer review scores . This essentially depends on whether the relevant biasing information is fed into the AI system in the learning phase and the variety of the reviewer judgements . In the case of prestige information , if the AI system is not fed author career information , then it could not directly learn a prestige bias from the human reviewer scores and if it is not fed the gender and nationality of the authors then it cannot learn gender and nationality bias directly , even if it is present in the peer review scores . AI is also likely to ignore layout bias , as it would presumably not be fed with layout information . A system could learn cognitive distance bias and confirmation bias if it dominated the peer review scores of relevant articles . For example , if all education reviewers gave low scores to qualitative research because they thought that quantitative research was inherently superior , then the AI system would probably learn to be biased against qualitative research . On the other hand , if the reviewers were evenly split between those that favoured quantitative and those that favoured qualitative research , then the AI system may well not learn a qual / quant bias and be less biased than individual reviewers in this regard . Similarly , if one topic was cognitively distant from all reviewers then the AI might learn to allocate lower scores to that topic . Automatic translation systems can introduce gender biases ( Prates et al . , 2020 ) and so AI systems relying on translation ( e . g . , for articles not written in English and without an English translation ) may introduce gender biases . AI systems processing textual input as part of quality score prediction may generate biases against minority groups through language expression ( Cheuk , 2021 ) . For example , one empirical study has developed AI systems to predict conference review accept / reject decisions from word frequency text analysis of the submitted papers . The factors found most useful by the system were all superficial and indirectly associated with higher quality rather than measures of it : avoiding “quadratic” , few sentences , many difficult words , many pages , and many syllables per word ( Checco et al . , 2021 ) . This approach seems likely to generate a bias against non - native English speakers who may prefer to use more straightforward language . 49 8 . Field categorisations for journal articles The method to automatically categorise journal articles into fields is an important aspect of field normalisation for AI systems predicting peer review . It is also relevant as a potential aid to sub - panel chairs assigning outputs to reviewers . The latter is a substantial time - consuming task that delays the start of the reviewing process . Whilst the most well - known field classifications , from the Web of Science ( WoS ) and Scopus , are based on human classifications of journals , automatic methods are available to classify at the article level , as implemented by Dimensions . ai . WoS also has an automatic method to reclassify articles in multidisciplinary journals for some of its indicators ( Clarivate , 2022 ) . These methods typically exploit the references of articles , the citations to them and / or words or phrases in the full text or title / abstract / keywords . They can generate more accurate field classifications by clustering articles into thematically related sets or classifying them into pre - defined classes ( e . g . , Dimensions ) . These classifications can help with identifying reviewers for articles as well as making field normalisation calculations for citations or other bibliometric indictors ( e . g . , research collaboration ) more accurate . This is one of the principles in The Leiden Manifesto for Research Metrics ( Hicks et al . , 2015 ) . Different Unit of Assessment groupings might also be suggested , in theory . The subject classification of sciences used in citation databases ( e . g . , WoS , Scopus , Dimensions ) can have a substantial impact on the field normalisation of scientometric indicators for research assessment exercises ( Glänzel et al . , 2009 ) . Hence , there have been many studies about alternative classification systems for field normalised impact indicators ( for reviews see Waltman & van Eck , 2019 ; Gläser , Glänzel , & Scharnhorst , 2017 ) . 8 . 1 . Journal - based field categorisations Both Scopus and the Web of Science use field classification based on assigning academic journals to one or more subject categories . Hence , articles are categorised based on their journals’ subject ( s ) rather than article topics . Web of Science uses 254 specific “Subject Categories” 15 in addition to 153 broader “Research Areas” 16 for journal classification and users can search a record by using advanced search commands “ WC = ” and “ SU = ” in these fields respectively . For instance , Web of Science has 7 subjects for chemistry journals : “Chemistry , Analytical” , “Chemistry , Applied” , “Chemistry , Inorganic & Nuclear” , “Chemistry , Medicinal” , “Chemistry , Multidisciplinary” , “Chemistry , Organic” , and “Chemistry , Physical” . These subjects combine to form the “Chemistry” Research Area . Scopus uses 27 broad subjects in its main database and 334 narrow subject codes 17 which can be searched with the SUBJTERMS advanced search command ( e . g . , SUBJTERMS ( 2310 ) for articles assigned to Pollution ) . There are other journal - based schemes such as the Science - Metrix non - overlapping classification with six domains , 22 fields and 176 subfields 18 ( see also Archambault , Beauchesne , & Caruso , 2011 ) or the DOAJ : Directory of Open Access Journals ( https : / / doaj . org / ) with 518 hierarchical subjects ( e . g . , Fine Arts / Architecture / Architectural drawing and design ) . Some researchers have also produced new classifications of science fields . For instance , a two - level classification scheme of science fields and subfields was proposed for research evaluation reasons , comprising 12 broad categories and 60 subfields for the sciences and 3 major fields and 7 narrow subjects of for both the social sciences and the humanities ( Glänzel & Schubert , 2003 ) . The UCSD ( University of California , San Diego ) Map of 15 https : / / images . webofknowledge . com / images / help / WOS / hp _ subject _ category _ terms _ tasca . html 16 https : / / images . webofknowledge . com / images / help / WOS / hp _ research _ areas _ easca . html 17 https : / / service . elsevier . com / app / answers / detail / a _ id / 14882 / supporthub / scopus / ~ / what - are - the - most - frequent - subject - area - categories - and - classifications - used - in / 18 https : / / science - metrix . com / classification / 50 Science classification system categorised Web of Science and Scopus journals into 13 broad fields and 554 subdisciplines ( Börner et al . , 2012 ) . There are conflicting subject classifications for some scientific journals . For instance , the Journal of the American Medical Informatics Association has been classified under the broad and narrow subjects “ Medicine ” and “ Health Informatics ” in Scopus , respectively , whereas Web of Science classified it under multiple Research Areas : “Computer Science” , “Health Care Sciences Services” , “Information Science Library Science” , and “Medical Informatics” . Science - Metrix classified this journal in the broad field “ Information & Communication Technologies ” with sub - field “ Medical Informatics ” . Although the subject categories used in the above databases were mainly developed for information retrieval , they can influence bibliometric results . For instance , there is evidence that the set of journals in the Web of Science “ Information Science & Library S cience” and “ Science and Technology S tudies” subject categories are not suitable for field normalisation in bibliometric evaluations ( Leydesdorff & Bornmann , 2016 ) . A comparison of the journal level classification of publications from the Chinese Science Citation Database and the paper level classification from the Chinese Library Classification for the same dataset showed that about half of the papers could be misclassified using the journal classifications system ( Shu et al . , 2019 ) , confirming that journal - based field categorisation systems could be problematic for research assessment exercises ( Leydesdorff & Bornmann , 2016 ; Sīle et al . , 2021 ; Wang & Waltman , 2016 ; Klavans & Boyack , 2017a ) . Journal - based classification have problems with classifying research published in multidisciplinary journals , fast changes in research areas and making comparative analyses using different databases ( Archambault , Beauchesne , & Caruso , 2011 ; Waltman & van Eck , 2019 ) . For instance , using visualization and natural language processing techniques , bibliometric indicators ( the h - index and the impact factor ) derived from three Web of Science medical subject categories ( Cardiac & cardiovascular systems , Clinical neurology , and Surgery ) may provide invalid results because even within an individual subject area there could be substantial differences in terms of citation practices and impact ( van Eck et al . , 2013 ) . 8 . 2 . Article - based field classification systems Theoretically , article - level classification systems can more accurately reflect scientific fields because it is common for journals to publish articles from more than one discipline , especially if they are in multidisciplinary or other generalist journals . The academic databases PubMed , Eric , Library and Information Science Abstracts , CAB Abstracts and PsycINFO have a list of controlled vocabulary terms attributed to articles ( e . g . , MeSH or ERIC thesaurus ) 19 and there have been attempts to use these human constructed classification schemes to improve field normalised indicators , such as with the EconLit database for economic publications ( van Leeuwen & Calero Medina , 2012 ) or the Chemical Abstracts database ( Neuhaus & Daniel , 2009 ; Bornmann , Marx , & Barth , 2013 ) . Many statistical methods , bespoke algorithms and general machine learning algorithms have been proposed to individually classify the subjects of articles based on references , citations and metadata . The disadvantages of these methods are a lack of transparency compared to journal classification 19 For other examples see https : / / connect . ebsco . com / s / article / Which - EBSCOhost - database - authorities - have - limited - support - via - the - EBSCOhost - API - AuthoritySearch - Method ? language = en _ US 51 systems and a lack of replicability due to changes in citation databases . For example , an early algorithm used cited references to classify articles in multidisciplinary journals ( Glänzel , Schubert , & Czerwon , 1999 ) . Some small - scale studies have shown that automatic classification is possible for some datasets . For example , one system had an accuracy of over 91 % for 680 articles in 10 different topics ( e . g . , biotechnology , technology , fisheries , education , economics ) published in a science journal by a Vietnamese university ( Dien , Loc , & Thai - Nghe , 2019 ) . Larger scale systems are essential for REF - related tasks , however . Several medium scale systems have compared different approaches to classify individual fields . Four supervised machine learning algorithms were trained on article titles and abstracts of journal articles ( n = 66 , 251 ) indexed by the Sociological Abstracts to automatically classify sociology research into a pre - defined set of categories . The popular Gradient Boosting Classifier algorithm was shown to correctly classify over 80 % of the documents ( Eykens et al . , 2019 ) . A follow - up paper about Education , Economics and Sociology also used article titles and abstracts from ERIC , EconLit and Sociological Abstracts databases ( 113 , 909 records ) to automatically classify articles into multiple subject categories . In this more complicated experiment , 46 % of the label combinations were predicted by the Gradient Boosting model ( Eykens , Guns , & Engels , 2021 ) . Several large - scale classification approaches have been developed for the whole of science . A multinomial logistic regression model to classify articles has been built with the help of 11 million papers from more than 4 , 000 journals across all academic subjects . It exploits terms in article titles , and abstracts as well as cited references . The system was subsequently used to classify 25 million articles from Scopus into four pre - defined research levels , showing that it is scalable for large problems ( Boyack et al . , 2014 ) . Another very large - scale study used deep learning applied to characters extracted from article metadata to classify scientific publications into 176 Science - Metrix subfields , and the results were compared with bibliographic coupling , direct citation , and manual - based classifications . The study used 41 million Scopus indexed publications for the experiment and found that the deep learning algorithm , despite its crude inputs , could classify scientific publications with almost the same level of accuracy as the other classification approaches ( Rivest , Vignola - Gagné , & Archambault , 2021 ) . Using a deep attentive neural network ( DANN ) to systematically classify publications into 104 Web of Science subject categories , another experiment was trained on 9 million abstracts from the Web of Science . The best model achieved was “ micro - F1 measure of 0 . 76 with F1 of individual subject categories ranging from 0 . 50 to 0 . 95 ” ( Kandimalla et al . , 2021 ) . A recent sophisticated study used a novel unsupervised machine learning method to cluster individual publications into broad scientific disciplines and subfields . The algorithm harnessed direct citation relations between publications , clustering them into research areas and assigning labels to the research areas by extracting terms from the titles and abstracts of the publications in them . The first level of the resulting clusters included 10 to 20 major fields , the second level 500 to 1 , 000 fields , and the third level 20 , 000 to 25 , 000 very narrow topics ( Waltman & van Eck , 2012 ) . This method has been harnessed to find topics and specialties of pre - defined sizes ( Sjögårde & Ahlgren , 2018 , 2020 ) . Using over 58 million papers , another study extended the direct citation method to create the model for classification of science into 91 , 726 topics that were assigned to 12 broad fields ( Klavans & Boyack , 2017a ) . These unsupervised approaches have the advantage of probably generating finer grained and more accurate clusters of documents compared to systems that use pre - existing classes . Another 52 advantage is flexibility to changes in research topics , but a disadvantage is a lack of transparency and the possibility that some clusters may not be meaningful groupings . Although the accuracy of algorithms targeting the same final classification scheme can easily be compared , it is less straightforward to compare algorithms that produce different classification schemes or that cluster documents into new groups . A novel approach to solve this problem is to treat articles with many references as “gold standards” that probably broadly delineate a research area as a topic - based review ( Klavans & Boyack , 2017b ) . Using this approach , algorithms exploiting direct citations produce better results than algorithms using bibliometric coupling ( two articles citing the same paper ) or co - citation ( two articles cited by the same paper ) and better results than the well - known journal - level classification schemes . The pre - eminence of article - level classification compared to journal level classification has been verified for human classifications too ( Shu et al . , 2019 ) . The main automatic clustering approaches can be tested with free bibliometric clustering and visualisation software ( Aria & Cuccurullo , 2017 ; van Eck & Waltman , 2017 ) . It is not clear yet whether systems based on direct citations ( i . e . , clustering documents based on whether they cite each other ) could be enhanced by systems that also exploit article title and abstract text . 8 . 3 . Dimensions document categorisation Dimensions is discussed separately here because it is part of a major scholarly database . It uses a machine learning algorithm applied to document titles and abstracts to automatically assign subject categories to millions of documents ( e . g . , papers , grants , clinical trials , patents , datasets and policy documents ) . Dimensions provides different schemes for categorising documents , but primarily uses the Fields of Research ( FoR ) categories from the Australian and New Zealand Standard Research Classification . It also incorporates other classification systems , including REF Units of Assessment ( UoAs ) , although some are only available in subscription versions . Only the Fields of Research ( FoR ) and Sustainable Development Goals classification schemes are available in the free version of Dimensions 20 . For instance , Dimensions Analytics , which needs a subscription , provides a 6 - digit level for categorisation of science , such as Informetrics ( 080705 ) under the broader category Library and Information Studies ( 0807 ) . This could be useful when a narrow classification scheme is required for field normalised impact indicators ( see Herzog & Lunn , 2018 ) . The Dimensions automatic field classification was initially not very accurate for library and information science ( Bornmann , 2018 ) , but its algorithm was subsequently strengthened ( Herzog & Lunn , 2018 ) . A small - scale report about a random sample of 1 , 000 articles from different fields compared the classification accuracy of Web of Science , Scopus and Dimensions with three independent human classifiers , finding that Web of Science had the most accurate subject classification followed by Dimensions and Scopus ( Singh et al , 2020 ) . A comparison of the classifications for research articles and letters published in Nature ( 2010 – 2020 ) in the Web of Science ( journal level ) , Dimensions ( machine learning ) and Springer Nature ( author - selected ) , found significant differences between paper - level classifications . Only a quarter ( 27 % ) of the papers had the same fields and 59 % had partially identical subjects in Dimensions and the Springer Nature classification systems , suggesting that there are substantial differences between article - level machine classification and human - based journal approaches to classify science . More than half ( 52 % ) of the publications had identical classifications in the Web of Science and Springer Nature classification schemes and almost a third ( 32 % ) had overlapping subjects ( Zhang et al . , 2022b ) . 20 https : / / app . dimensions . ai / browse / categories / publication / for 53 54 9 . Predicting journal article citation counts or output quality from open review text Text mining can be applied to online reviews . Increasingly many papers have open reviews within publishers’ websites or public review sites ( e . g . , PubPeer ) so this is a promising avenue for the future . It might be useful for monograph evaluation , given that reviews are important and common for monographs in the humanities ( e . g . , published in journals ) but despite this , monograph reviews are usually positive so do not obviously form a useful source of evaluation evidence . For journal articles , open reviews are currently available for a minority of articles from publishers that permit it like MDPI . Moreover , reviews are often difficult to parse for text mining ( e . g . , commented copies of reviewed article PDFs ) . Most open reviews are from reviewers and address pre - final versions of the article , so it is not clear that they provide useful information about the final published article . There are several open review platforms sharing pre - publication reviews ( formal reviews or editorial comments from the publishing journal ) or post - publication comments ( recommendations or feedback by researchers or experts ) for scholarly publications . Publons ( publons . com ) is an open review platform from Clarivate Analytics claim ing to include “over 6 . 9 - million reviews for more than 5 , 000 partnered journals” 21 . Partner journals can share pre - publication reviews publicly on this site , with reviewers and authors deciding what information to reveal ( e . g . , review text , reviewer identities ) . Others can also write post - publication comments on the site and anonymously score articles for significance and rigour ( see : https : / / publons . com / benefits / reviewers / how , e . g . , https : / / publons . com / publon / 353160 / ) . PubPeer ( pubpeer . com ) is another online open platform but focuses on post - publication peer review , where researchers can provide feedback or comments about published research and authors can respond . Reviews in PubPeer have identified mistakes published in leading cell biology journals 22 23 , suggesting that post - publication reviews might be a helpful source for quality assessment of published research . ScienceOpen ( scienceopen . com ) combines publishing and promotion services for journals with a recommendation capability where other researchers or experts can write public reviews and use a five - star score about the “importance” , “validity” , “comprehensibility” , and “completeness” of published research 24 . Peer Community in ( peercommunityin . org ) is a free recommendation platform for preprints . It publishes peer - reviews of preprints in 14 subject areas , including Ecology , Genomics , Animal Science , Evolutionary Biology . The peer review process is managed by 1 , 700 ‘Recommenders’ making editorial decisions about public reviews 25 . Multidisciplinary Digital Publishing Institute ( MDPI ) and several other publishers and journals provide an open peer - review option , where authors can decide to publish their reviews and reviewers can choose to be named or remained anonymous ( for a review see Wolfram et al . , 2020 ) . These provide a collective source of open peer review reports for the journals covered . In contrast to the above , Faculty Opinions ( facultyopinions . com , previously F1000 Prime ) is a paywalled source of post - publication biomedical research reviews wri tten “by over 8 , 000 experts in the Life Sciences and Medicine” . Articles are classified based on contribution type , such as ‘Good for 21 https : / / publons . freshdesk . com / support / solutions / articles / 12000012231 - what - is - publons - and - why - partner - with - us - 22 https : / / phys . org / news / 2013 - 05 - stem - cell . html 23 https : / / www . nature . com / articles / nature . 2013 . 13060 24 https : / / blog . scienceopen . com / 2016 / 05 / peer - review - at - scienceopen - is - surprisingly - simple / 25 https : / / peercommunityin . org / about / 55 Teaching’ , ‘New Finding’ , ‘Technical Advance , ’ or ‘Interesting Hypothesis’ and can be given one ( Good ) , two ( Very Good ) or three ( Exceptional ) stars 26 . The rationale behind investigating these sites is that unbiased and high - quality open reviews by subject experts might provide a further quality control mechanism for research assessment exercises , or inputs for future machine learning exercises . However , the manipulation of post - publication reviews could be problematic for formal assessments of individual academic outputs . Moreover , the current platforms for pre - publication reviews often do not show review reports for rejected articles ( Thelwall , 2022a ) and may have a positive bias . For example , F1000 article recommendations are exclusively positive : ‘Good’ ( 58 . 6 % ) , ‘Very Good’ ( 34 . 6 % ) and ‘Exceptional’ ( 6 . 9 % ) ( Waltman & Costas , 2014 ) . Nevertheless , a study of a sample of PubPeer comments about publications found that two thirds were related to some type of misconduct ( Ortega , 2022 ) , which might be useful to flag to REF reviewers or incorporate within machine learning approaches . The rest of this subsection reviews research into these peer review sites . Often experimental tests of site content or ratings correlate them with citation counts for the articles . Since citation counts associate with ( but do not measure ) research quality in many fields , the absence of research quality evidence , it is reasonable to use citation counts to help investigate the value of other indicators . 9 . 1 . Publons Insufficient evidence of an association between Publons reviews and citations : An early study found weak or no significant associations between bibliometric indicators from Google Scholar and the peer review activity of Publons users ( Ortega , 2017 ) and another study of 45 , 819 articles from Publons also found low or insignificant correlations between bibliometric scores ( e . g . , WoS or Scopus citations ) and Publons metrics ( e . g . , Quality , Significance and Overall Publons score of articles ) ( Ortega , 2019 ) . These suggest that Publons metrics might not be useful indicators of citation impact or , by extension ( because the two correlate in many fields ) , research quality . Articles with more positive Publons post - publication reviews receive more citations : A small - scale paper about four experimental groups of papers from Publons with neutral , negative , positive and both negative and positive post - publication reviews found that papers with positive reviews had significantly more citations ( rho = 0 . 498 , p < 0 . 05 ) while very low or non - significant associations were found between citation counts and other review polarities ( Zong et al . , 2020 ) . Imbalances and problems in Publons content : There has been a criticism about the coverage and contents of peer review in Publons ( Teixeira da Silva & Al - Khatib , 2019 ; Teixeira da Silva , 2020 ) and a 2018 study identified a large imbalance in the coverage of Publons reviews across subjects and journals . For instance , the Publons coverage of Life Sciences ( 40 % ) was twice as much as for the Physical Sciences ( 18 % ) . Most articles reviewed in Publons were from Frontiers Media open access journals ( Ortega , 2019 ) . A large - scale report about a sample of 183 , 743 unique review reports submitted to Publons found that although most of Publons reviews were for legitimate journals ( 96 . 7 % or 177 , 666 for 6 , 403 journals ) , with very few reviews for apparently predatory journals ( 3 . 3 % or 6 , 077 reviews for 1 , 160 journals ) . The share of predatory reviews was higher from sub - Saharan Africa ( 22 % ) , Middle East and North Africa ( 14 % ) and South Asia ( 7 . 0 % ) than from other regions including North America ( 2 . 1 % ) , Latin America and the Caribbean ( 2 . 1 % ) , Europe and Central Asia ( 1 . 9 % ) and East Asia and the Pacific ( 1 . 5 % ) ( Severin et al . , 2021 ) . There have been criticisms of the quality of post - publication reviews ( e . g . , da Silva & Al - Khatib , 2021 , see also above for Severin et al . , 2021 ) , the recognition system for reviewing activities ( Smith , 2016 ) and using Publons to identify potential 26 https : / / facultyopinions . com / wp - content / uploads / 2020 / 07 / Faculty - Opinions _ Reference _ Guide . pdf 56 reviewers ( Jorm , 2021 ) . Nevertheless , Publons pre - publication reviews may help universities to access formal peer review reports to “improve and promote research excellence assessment” ( Wilkinson & Down , 2018 ) . Gender gap in Publons top reviewers : A paper about the gender of Publons top reviewers ( the Top 1 % most active reviewers in a field ) found that male reviewers dominate across all 23 subject areas . Social Sciences , Psychiatry & Psychology and Immunology had the highest proportion of female reviewers ( 25 % , 24 . 5 % , 22 . 3 % ) , whereas in Mathematics , Physics and most engineering subjects this proportion was less than 10 % . On average , the top male Publons reviewers had significantly more reviews than the top female Publons reviewers ( means : 166 vs . 117 respectively ) , although female reviewers tended to write longer reviews ( average number of review words : 380 vs . 333 ) . The study found low but significant Spearman correlations between reviewing activity and the number of publications of the top Publons male ( 0 . 36 ) and female reviewers ( 0 . 44 ) ( Zhang et al . , 2022a ) . In summary , Publons reviews tend to be disproportionately written by male reviewers and disproportionately cover life sciences topics . 9 . 2 . Faculty Opinions ( formerly F1000Prime ) F1000Prime article factor and recommendations associate with citation counts : Several investigations have reported significant associations between F1000Prime ( now Faculty Opinions ) ratings and citation metrics . An initial study of a sample of 1 , 397 F1000Prime selected Genomics and Genetics articles published in 2008 found low but positive Spearman correlations between F1000 article scores and citation counts from Web of Science ( 0 . 303 ) , Scopus ( 0 . 300 ) and Google Scholar ( 0 . 295 ) . Similarly , low significant correlations were found between article evaluator counts and citation counts , although a higher association was found between both F1000 article factors and citation counts ( 0 . 369 ) and Journal Impact Factors ( 0 . 353 ) ( Li & Thelwall , 2012 ) . A later study of 125 articles published in 2008 in Cell Biology or Immunology also found low - medium correlations between F1000 article factors and seven bibliometric metrics , although Percentile in Subject Area and Web of Science citation counts had higher associations with F1000 Prime article factors ( R 2 = 20 % and 18 % respectively ) ( Bornmann & Leydesdorff , 2013 ) . Similarly , a report about a random sample of F1000 medical publications in 2007 ( n = 350 ) and 2008 ( n = 550 ) also found significant positive , albeit low , Spearman correlations between F1000 article factors and citation counts for both 2007 ( rho = 0 . 383 ) and 2008 ( 0 . 300 ) , and citation counts varied between F1000Prime article type classifications , such as “new finding” and “changes clinical practice” ( Mohammadi & Thelwall , 2013 ) . Another later study also confirmed that article type has a significant effect on the difference between F1000Prime article factor scores and citations ( Du , Tang , & Wu , 2016 ) . A large - scale study of all F1000 Prime recommendations about publications found that 2 % of biomedical research had at least one F1000 Prime recommendation ( WoS matched rate = 93 % or 95 , 385 publications ) . The study also reported weak but significant Pearson correlations between both F1000Prime recommendation scores and the number of recommendations and citation counts ( 0 . 24 and 0 . 26 respectively ) and journal citation scores ( 0 . 33 and 0 . 34 ) , suggesting that one reason for the weak associations could be that the majority ( 98 % ) of biomedical research does not have any kind of recommendation in F1000 ( Waltman & Costas , 2014 ) . Using negative regression analysis to estimate factors influencing F1000 recommendation scores , a study of 94 , 641 F1000Prime publications 2000 - 2004 found a significant association between F1000 scores citation counts , the journal impact factor had stronger association with the citation impact of research than F1000 recommendations ( Bornmann , 2015 ) . Other studies also reported low but significant associations between F1000Prime recommendations and citation or reader metrics ( e . g . , 57 Bornmann & Leydesdorff , 2015 , Smith et al . , 2019 ) . Finally , a study of 830 research articles in 2010 published by four high impact journals found that articles recommended in Faculty Opinions ( previously F1000Prime ) on average had more citations ( between 2010 to 2019 ) and this difference was statistically significant for articles published in Nature Genetics , Nature Medicine , and PLoS Biology except for the journal Cell ( Wang & Su , 2021 ) . Insufficient evidence of an association between F1000Prime scores and citation metrics : An early small comparison of 1 , 530 publications from seven major ecological journals in 2005 and 103 publications that were highlighted by F1000 found that publications highlighted by F1000 were cited less frequently than other articles ( Wardle , 2010 ) . Weak associations between F1000 Prime quality assessments of biomedical publications and their research collaboration : A study of 16 , 557 papers published between 1996 and 2012 found very weak associations between F1000Prime member scores ( as an indication of the paper quality ) and collaboration indicators . Using regression models , only 1 % of the variance in the F1000 ratings was explained by the number of authors , number of affiliations , and number of countries , whereas the number of authors alone could explain 7 . 7 % of the citation scores ( Bornmann , 2017 ) . Expert assessment about importance of research associated with F1000Prime ratings : A study of 687 papers associated with the Wellcome Trust published in 2005 found a moderate positive Spearman correlation ( rho = 0 . 445 ) between the importance of papers , as judged by Wellcome Trust reviewers ( each paper was reviewed by two reviewers using a four - point scale ) , and those identified by the Faculty of 1000 experts . The study also found a moderate positive correlation ( 0 . 45 ) between Wellcome Trust reviewer ratings and citation counts to articles three years after the reviews had been conducted . The Journal Impact Factor had the strongest association ( 0 . 625 ) with Wellcome Trust reviewer ratings , suggesting that journal impact might be helpful proxy indicator of research quality ( Allen et al . , 2009 ) . 9 . 3 . MDPI Disciplinary differences in characterises of standard open peer - review : A study of 45 , 385 open standard article reviews for 288 MDPI journals found a large disciplinary differences in review lengths , reviewer anonymity , review outcomes , and the use of attachments . For instance , reviewers in the Physical Sciences are most likely to ask for major revisions and to use attachments in the review process , although they are less likely to disclose their identity . In Life Sciences and Social Sciences fields , reviewers tend to write longer review reports than in the Physical Sciences ( Thelwall , 2022a ) . 58 10 . Publishing quality control Various text mining AI programs have been developed to help with quality control for papers , some of which are routinely used by publishers . Probably the most widely - used are plagiarism detection and biomedical image manipulation software . A related approach is paraphrase detection ( so called “tortured phrases” ) which points to hidden plagiarism , sometimes of whole articles . Software has also been developed to check the plausibility of statistical results in papers by testing if the numbers reported for a test are theoretically capable of having been generated at the level of rounding reported . Whilst problematic articles seem likely to be rare in the REF , the systems represent types of approach that might support future REFs , especially if they become robust and easy to use . 10 . 1 . Tools to recommend journals for manuscripts Identifying relevant journals or conferences is the most significant step to publish research results . Hence , several publishers have developed web services to help authors to find relevant journals for manuscripts mainly based on comparing article texts ( titles , abstracts or keywords ) against previously published articles . Current journal recommendation tools include Springer Nature Journal Suggester 27 , Wiley Journal Finder 28 or IEEE Publication Recommender 29 . EndNote Manuscript Matcher also uses manuscript title , abstract , and references and Web of Science data to suggest related journals for manuscripts 30 . The Journal / Author Name Estimator ( JANE ) is another free service that uses the similarity scores and PubMed data to suggest most relevant journals based on manuscript titles or abstracts 31 . Various types of AI are used in these systems to match the subjects of manuscripts to related journals . For instance , Elsevier’s JournalFinder 32 service “uses smart search technology and field - of - research specific vocabularies ” to match paper to scientific journals , Taylor & Francis Journal Suggester 33 applies “artificial intelligence to match the subjects covered in articles” and Sage Journal Selector 34 utilizes “an advanced AI technology” to recommend journals with similar published articles . Several studies have shown that AI or machine learning can be useful to identify appropriate academic journals or conferences with relatively high accuracy for papers ( e . g . , Wang et al . , 2018 ; Feng et al . , 2019 ; Ghosal et al . , 2019a ; Pradhan & Pal , 2020 ) . For instance , a recent experiment used the XGBoost algorithm and different features ( title , abstract , and keywords ) from 20 , 250 articles from Web of Science indexed computer technology journals , reporting an accuracy of 84 % for academic journal recommendations ( ZhengWei et al . , 2022 ) . Using deep learning techniques , another study developed a journal recommendation system ( Pubmender ) to propose appropriate PubMed journals based upon a rticles’ abstracts . The experiential dataset included abstracts of over 880 , 000 papers from 1 , 130 PubMed Central journals , reporting an accuracy of 87 % . This was claimed to be much higher than Elsevier’s Journal Finder and Springer’s Journal Suggester tools ( Feng et al . , 2019 ) . , The tool GraphConfRec has been developed to recommend relevant computer science conferences based on paper text , co - authorship and citation networks ( Iana & Paulheim , 2021 ) . 27 https : / / journalsuggester . springer . com / 28 https : / / journalfinder . wiley . com / search ? type = match 29 https : / / publication - recommender . ieee . org / home 30 https : / / endnote . com / product - details / manuscript - matcher / 31 https : / / jane . biosemantics . org / 32 https : / / journalfinder . elsevier . com / 33 https : / / authorservices . taylorandfrancis . com / publishing - your - research / choosing - a - journal / journal - suggester / 34 https : / / www . edanz . com / journal - selector 59 The approaches used to recommend journals for manuscripts might be customised to recommend REF or journal / conference reviewers by matching the manuscripts against the publications of potential reviewers . Separate software has been generated for reviewer selection , however , as discussed below . 10 . 2 . Tools to automate editorial process AI and Natural Language Processing can assist editors and publishers in many ways from sending automatic emails to authors or reviewers and checking plagiarism in the manuscripts to statistical test and methods checking ( For reviews see : Price & Flach , 2017 ; Checco et al . , 2021 ; Lin et al . , 2021 ) . One study compared different manuscript management tools ( e . g . , ScholarOne , Editorial Manager , EVISE and Open Journal Systems ) from the authors ’ , reviewers ’ , and editors’ perspectives , reporting tools supporting automatic editorial tasks ( e . g . , sending e - mails , reviewer recommendation , statistical analysis , similarity check , linking references to Crossref or and PubMed ) ( Kim et al . , 2018 ) . A Jisc report on “ Artificial Intelligence , Automation and Peer Review ” has summarised tools that can automate some tasks of editorial management or peer review process such as the following . • Plagiarism detection : Ithenticate 35 detects copied text in a manuscript . A procedure to detect tortured phrases – can identify plagiarism in the form of papers or sections of papers that have been processed by paraphrasing software or that have been translated from English to another language and back again . It identifies meaningless phrases that are non - idiomatic translations of scientific phrases , such as “counterfeit consciousness” from “art ificial Intelligence” ( Cabanac et al . , 2021 ) . • Automated statistical checking : StatReviewer 36 checks manuscripts against standardized reporting guidelines and StatCheck 37 to detect statistical errors in the submitted works ( see Thelwall , 2019 ) . • Multipurpose manuscript evaluation : Other AI - assisted tools assess multiple quality control aspects of manuscripts such as Frontiers AI Review Assistant 38 or UNSILO Manuscript Evaluation 39 ( see also Heaven , 2018 ) . • Article summarisation . Many natural language programs can summarise the contents of academic and other documents . UNSILO is an example . • Manuscript structure checking : Penelope . ai 40 checks if the structure of a manuscript meets a journal ’ s submission guidelines for the title page , abstract , citation style , references , tables and figures and information about other sections of articles ( e . g . , funding , acknowledgements , keywords and data / ethics statements ) . This avoids the need for manual checks by reviewers , publishers or editors . • Reference matching with in - text citations : Recite 41 automatically checks and highlights if citations in the manuscript text match the reference list and vice versa . • Summarising articles : S cholarcy 42 applies AI to summarise articles and to extract tables , figures and references . 35 http : / / www . ithenticate . com 36 http : / / www . statreviewer . com / 37 http : / / statcheck . io / 38 https : / / blog . frontiersin . org / 2020 / 07 / 01 / artificial - intelligence - peer - review - assistant - aira / 39 https : / / unsilo . ai / unsilo - manuscript - evaluation / 40 https : / / www . penelope . ai / faq 41 https : / / reciteworks . com / 42 https : / / www . scholarcy . com / 60 • Article writing : SCIGen 43 uses Natural Language Generation ( NLG ) to automatically generate academic papers for conferences . This has been used for quality control purposes , such as testing whether journals have genuine review procedures . • Methods checking : SciScore 44 generates an automated assessment of articles methods on a scale of 1 - 10 and other reports ( Design Analysis Reporting checklist and the Rigor and Transparency Index ) , assisting reviewers to find key information throughout a paper in a standard format ( see also Menke et al . , 2020 ) . • Review sentiment extraction : PeerJudge 45 uses AI - assisted sentiment detection to estimate the strength of praise and criticism in peer review reports on academic papers that could be useful for editorial management decisions when they want to analyse a large number or review reports . PeerJudge can predict F1000Research reviewer decisions with a moderate degree of accuracy ( Thelwall et al . 2020 ) . 43 https : / / pdos . csail . mit . edu / archive / scigen / 44 https : / / www . sciscore . com / 45 http : / / sentistrength . wlv . ac . uk / PeerJudge . html 61 11 . Allocation of research outputs to appropriate reviewers Editorial systems for publishers suggest possible reviewers for submitted articles , perhaps based on references in the submitted outputs or by matching article keywords to the keywords of registered reviewers . REF subpanel chairs have to start the review process by assigning articles to at least two reviewers . Fully or partly automating this labour - intensive process might improve the overall match between subpanel reviewers and articles and save time . 11 . 1 . Tools for reviewer selection With the constant increase in the number of manuscript submissions to the academic journals and conferences , the editorial and peer review process is becoming more challenging and time - consuming . It was estimated that “ over 15 million hours ” are spent on reviewing rejected papers each year ( American Journal Experts , 2018 ) . For example , 1 . 2 million manuscripts are submitted to 2 , 300 Elsevier journals every year and only 30 % ( about 350 , 000 ) are published ( Tedford , 2015 ) . Due to increasing numbers of submissions and peer review workloads , a report from BioMed Central and Digital Science entitled “ What might peer review look like in 2030 ? ” , recommended to “use technology to support and enhance the peer review process , including finding automated ways to identify inconsistencies that are difficult for reviewers to spot” ( Burley & Moylan , 2017 , p . 3 ) . Several programs have also been developed by commercial publishers to help editors identify suitable reviewers ( see examples below ) , but these are typically not transparent . • Clarivate’s Reviewer Locator 46 automatically suggests reviewers based on data from the Web of Science and Publons peer review databases and connects to the ScholarOne submission management system integrating editorial and peer - review processes . • Reviewer Discovery 47 is a tool from Aries Systems that uses ProQuest author profiles and automatically suggests reviewers based on the Editorial Manager system . • Elsevier’s EVISE 48 uses Reviewer Finder to identify and recommend reviewers based on Scopus data . • Frontiers Coronavirus Reviewer Recommender 49 suggests experts to review COVID - 19 research proposals using “ Frontiers knowledge graph and advanced information extraction and retrieval methods ” . The National Natural Science Foundation of China ( NSFC ) has developed an AI - assisted reviewer recommender for grant applications using natural language processing and an assignment decision support system to help select expert panels . An initial version of the AI system had chosen “at least one member of each of nearly 44 , 000 panels that approved projects” in 2018 , and the accuracy of system was about 80 % ( Cyranoski , 2019 , p . 317 ) , but accuracy improvements are still being made ( Liu et al . , 2022 ) . The system classifies the reviewers and proposals by discipline and uses information from scientific databases ( e . g . , Web of Science ) and referee profiles in NSFC databases about the publication records or research projects of potential reviewers and then uses lexical semantic analysis to compare the extracted information with the grant applications . Different rules were used in the system to avoid conflicts of interests between reviewers and applicants ( e . g . , affiliation , co - authorship , project and tutor - student relationships ) ( Liu et al . , 2016 ) . The Toronto Paper Matching System also automatically suggested reviewer assignments for the NIPS 2010 conference using a topic modelling approach to estimate reviewers’ expertise areas . The system extracts publication records 46 https : / / clarivate . com / webofsciencegroup / solutions / reviewerlocator / 47 https : / / www . ariessys . com / newsletter / june - 2013 / reviewer - discovery - free - trial - available / 48 https : / / www . elsevier . com / connect / reviewers - update / rolling - out - our - new - editorial - system - evise 49 https : / / coronavirus . frontiersin . org / covid - 19 - research - funding - monitor 62 from Google Scholar to generate profiles for reviewers and uses supervised score prediction models to suggest reviewer assignments ( Charlin & Zemel , 2013 ) . Several other studies have also suggested algorithms for the automatic assignment of reviewers to conference papers ( e . g . , Li & Watanabe , 2013 ; Al Mahmud , Hossain , & Ara , 2018 ; Kalmukov , 2020 ) , mostly for AI related conferences . No robust accuracy measures seem to have been generated for these systems yet , however . Presumably the ground truth for such a system would be human editor assignments or ( in conferences that allow this ) reviewer requests to review . The implementation of such systems seems to be practical and beneficial for the REF . Two topics are discussed in more detail below . These are relevant to the REF and have academic research about them . Review decision and comment automation is challenging with limited progress on a few topics so far . Whilst positive correlations between human and automated decisions have been generated , no current system challenges human reviewing yet . Positive correlations between peer review judgements and machine learning do not necessarily mean that further progress is likely soon because an AI system would achieve a positive correlation by rejecting papers with obvious errors , such as very poor grammar , too short , or lacking references . ReviewAdvisor 50 is a natural language processing toolkit designed to help select good manuscripts for a journal and provide feedback to help authors improve their submitted articles . Whilst its performance on the authors’ ASAP - review set of 28 , 119 machine learning conference paper reviews was weak , it provides a starting point and might help reviewers by suggesting comments on paper aspects that they may have overlooked . Another study developed an AI tool using trained neural network and a set of features from papers including word frequencies , readability scores , and formatting measures , finding that automated systems developed unethical biases , such as against grammar and formatting errors , that helped them be more accurate ( Checco et al . , 2021 ) . Similarly , another study found that Natural Language Processing models to generate reviews for scientific papers could make the peer - review task easier and more effective but not to replace it ( Yuan , Liu , & Neubig , 2021 ) . Review decisions from peer review comments : Based on dataset of scientific peer reviews from PeerRead 51 , a deep learning network was used to predict acceptance or rejection of articles from peer review reports and to generate the final meta - review , finding “good consistency between the rec ommended decisions and original decisions” , with 74 % - 86 % accuracy at predicting the binary decision accept / reject , which was better than standard machine learning algorithms and prior bespoke peer review judgement algorithms ( Pradhan et al . , 2021 , p . 237 ) . Another study used 3 , 341 papers from three computing conferences ( 2018 IEEE Wireless Communications and Networking Conference and 2018 and 2019 the International Conference on Learning Representations ) and assessed to what extent AI can predict human peer - review decisions about papers ( acceptance / rejection or average review score ) . Using a Random Forest classifier , the above study used machine learning to predict the acceptance of papers submitted to a top AI conference with 81 % accuracy based on surface features , such as the number of tables in the papers or characters in the title ( Skorikov & Momen , 2020 ) . Similarly , the pReview software package developed for automatically generating summarization , contribution detection , writing quality analysis and potential related works of academic papers to support reviewers ( Bond & Craig ) . There is also evidence that sentiment analysis of review reports could be helpful to predict the final decision ( acceptance or rejection ) of conference 50 https : / / github . com / neulab / ReviewAdvisor 51 https : / / paperswithcode . com / paper / a - dataset - of - peer - reviews - peerread - collection 63 papers ( Wang & Wan , 2018 ; Ghosal et al . , 2019b ; Chakraborty , Goyal & Mukherjee , 2020 ) or review scores of funding programs ( Luo et al . , 2021 ) . 12 . Conclusions and recommendations Many bibliometric studies have shown that a wide range of factors derived from article text ( e . g . , length of articles , titles or abstracts , number or impact of cited references and article readability ) might be related to the scientific impact of research as measured by citation counts ( see Table 1 for summary ) . However , there are disciplinary differences in almost all the results and some findings could be biased towards impactful research . Some of the results also varied over time or between journals . Thus , whilst there are general trends , there are no universal laws . A risk with text mining to predict citation counts is that it is likely to work best by identifying highly cited topics , predicting higher citation counts for all articles on these topics . A successful prediction model for one year might be invalid for the next one due to topic changes , so text mining may need rebuilding each year to identify the new hot topics . Many studies have also found that several factors derived from article metadata ( e . g . , the number of authors , institutional or international collaboration , journal impact or author publication and citedness ) sometimes associate with the citation rates of published papers ( see Table 2 for a summary ) . Nevertheless , there are disciplinary differences in some of the results and other factors such as the selection method for journals ( e . g . , high impact journals ) or publication years might have influenced the results . It seems that the journal impact factor had the strongest association with citation counts , as reported by several studies , although collaboration factors also have strong associations with the citation impact of research . In particular , the expected citation rate of UK authored research increases roughly in line with the logarithm of the number of authors . Several machine learning studies suggested that it is feasible to predict the long - term citation counts or quality scores of papers in some extend . Some studies have used machine learning to estimate future citations for articles , although there is no evidence how accurately AI can be used to predict human judgements of journal article in a large - scale . There is evidence that public datasets of citations ( e . g . , Dimensions and OpenCitations ) can be useful sources to automatically capture wider citations from larger scholarly publications , especially for the recently published articles for the future REF in addition to traditional citation indexes ( e . g . , Scopus and Web of Science ) . Many studies have also shown that altermatic indicators and particularly Mendeley readers can be useful to predict future scholarly impact . However , these sources lack quality control mechanism for the future REF and can potentially be spammed such as by publishing non - peer reviewed publications online to inflate citations to the REF submissions or adding readers or bookmarks via fake accounts or manipulated users . There are also arguments about the accuracy of automatic document classification systems to assign REF outputs to the scientific fields ( e . g . , Dimensions ) . Pre - publication open reviews platforms such as Publons and MDPI could be helpful counselling source for the future REF , when text mining and AI - assisted sentiment detection ( e . g . , PeerJudge ) can automatically be applied to estimate the strength of praise and criticism in formal peer review reports on REF submissions . However , few journals or publishers allow public sharing of pre - publication reviews , many reviews may not have all comments from reviewers in a structured format and currently review reports for rejected manuscripts submitted to journals are not usually available . Post - publishing peer reviews ( e . g . , Faculty Opinions ) can potentially be helpful advisory source when feedback or comments by experts in the fields about published articles could be captured , although post - publication reviews could be manipulated and hence problematic to be used . 64 There are several AI - assisted tools and software that might be consulted to automatically suggest reviewers ( REF subpanel members ) for submitted outputs , enhancing the peer review process by assigning relevant reviewers in different the subject area . However , the accuracy of the developed tools to assign reviewers is not fully known and it is not clear how they may work based on few referee profiles in some REF subjects . Finally , the review of the literature on the responsible use of technology to assist research assessment suggested that most AI and machine learning methods have recently developed and tested and there is evidence that AI adoption continues to grow in many areas ( see The State of AI in 2021 52 ) . Hence , the use technology to assist or even enhance evaluations of individual articles could be possible in the future . The main recommendations are listed in the executive summary . Acknowledgements Thank you to members of the steering group for comments on earlier drafts : Andy Hepburn ( Research England ) , Steven Hill ( Research England ) , Petr Knoth ( Open University ) , Duncan Shermer , ( Research England ) , and Jennifer Stergiou ( University of Northumbria and ARMA Chair ) . This study was funded by Research England , Scottish Funding Council , Higher Education Funding Council for Wales , and Department for the Economy , Northern Ireland as part of the Future Research Assessment Programme ( https : / / www . jisc . ac . uk / future - research - assessment - programme ) . The content is solely the responsibility of the authors and does not necessarily represent the official views of the funders . References Abramo , G . , & D’Angelo , C . A . ( 2015 ) . The relationship between t he number of authors of a publication , its citations and the impact factor of the publishing journal : Evidence from Italy . Journal of Informetrics , 9 ( 4 ) , 746 - 761 . Abramo , G . , & D ' Angelo , C . A . ( 2011 ) . Evaluating research : from informed peer review to bibliometrics . Scientometrics , 87 ( 3 ) , 499 - 514 . Abramo , G . , D’Angelo , C . A . , & Caprasecca , A . ( 2009 ) . Allocative efficiency in public research funding : Can bibliometrics help ? . Research policy , 38 ( 1 ) , 206 - 215 . Abrishami , A . , & Aliakbary , S . ( 2019 ) . Predicting citation counts based on deep neural network learning techniques . Journal of Informetrics , 13 ( 2 ) , 485 - 499 . Aguillo , I . F . ( 2012 ) . Is Google Scholar useful for bibliometrics ? A webometric analysis . Scientometrics , 91 ( 2 ) , 343 - 351 . Ahlgren , P . , & Waltman , L . ( 2014 ) . The correlation between citation - based and expert - based assessments of publication channels : SNIP and SJR vs . Norwegian quality assessments . Journal of Informetrics , 8 ( 4 ) , 985 - 996 . Ahlgren , P . , Colliander , C . , & Sjögårde , P . ( 2018 ) . Exploring the relation between referencing practices and citation impact : A large‐scale study based on Web of Science data . Journal of the Association for Information Science and Technology , 69 ( 5 ) , 728 - 743 . Akella , A . P . , Alhoori , H . , Kondamudi , P . R . , Freeman , C . , & Zhou , H . ( 2021 ) . Early indicators of scientific impact : Predicting citations with altmetrics . Journal of Informetrics , 15 ( 2 ) , 101128 . Aksnes , D . W . ( 2003 ) . Characteristics of highly cited papers . Research evaluation , 12 ( 3 ) , 159 - 170 . Aksnes , D . W . ( 2006 ) . Citation rates and perceptions of scientific contribution . Journal of the American Society for Information Science and Technology , 57 ( 2 ) , 169 - 185 . 52 https : / / www . mckinsey . com / business - functions / quantumblack / our - insights / global - survey - the - state - of - ai - in - 2021 65 Aksnes , D . W . , & Taxt , R . E . ( 2004 ) . Peer reviews and bibliometric indicators : a comparative study at a Norwegian university . Research evaluation , 13 ( 1 ) , 33 - 41 . Aksnes , D . W . , Langfeldt , L . , & Wouters , P . ( 2019 ) . Citations , citation indicators , and research quality : An overview of basic concepts and theories . Sage Open , 9 ( 1 ) , 2158244019829575 . Al Mahmud , T . , Hossain , B . M . , & Ara , D . ( 2018 ) . Automatic Reviewers Assignment to a Research Paper Based on Allied References and Publications Weight . In 2018 4th International Conference on Computing Communication and Automation ( ICCCA ) ( pp . 1 - 5 ) . IEEE . Al - Janabi , S . , Lim , L . W . , & Aquili , L . ( 2021 ) . Development of a tool to accurately predict UK REF funding allocation . Scientometrics , 126 ( 9 ) , 8049 - 8062 . Allen , L . , Jones , C . , Dolby , K . , Lynn , D . , & Walport , M . ( 2009 ) . Looking for landmarks : the role of expert review and bibliometric analysis in evaluating scientific publication outputs . PloS one , 4 ( 6 ) , e5910 . Allen , N . , & Heath , O . ( 2013 ) . Reputations and research quality in British political science : The importance of journal and publisher rankings in the 2008 RAE . The British Journal of Politics and International Relations , 15 ( 1 ) , 147 - 162 . American Journal Experts ( 2018 ) . Peer review : how we found 15 million hours of lost time . https : / / www . aje . com / en / arc / peer - review - process - 15 - million - hours - lost - time / Ante , L . ( 2022 ) . The relationship between readability and scientific impact : Evidence from emerging technology discourses . Journal of Informetrics , 16 ( 1 ) , 101252 . Antonakis , J . , Bastardoz , N . , Liu , Y . , & Schriesheim , C . A . ( 2014 ) . What makes articles highly cited ? . The Leadership Quarterly , 25 ( 1 ) , 152 - 179 . Archambault , É . , Beauchesne , O . H . , & Caruso , J . ( 2011 ) . Towards a multilingual , comprehensive and open scientific journal ontology . In Proceedings of the 13th international conference of the international society for scientometrics and Informetrics ( pp . 66 - 77 ) . South Africa : Durban . Aria , M . , & Cuccurullo , C . ( 2017 ) . bibliometrix : An R - tool for comprehensive science mapping analysis . Journal of Informetrics , 11 ( 4 ) , 959 - 975 . Asaad , M . , Kallarackal , A . P . , Meaike , J . , Rajesh , A . , De Azevedo , R . U . , & Tran , N . V . ( 2020 ) . Citation skew in plastic surgery journals : does the journal impact factor predict individual article citation rate ? . Aesthetic Surgery Journal , 40 ( 10 ) , 1136 - 1142 . Avkiran , N . ( 1997 ) . Scientific collaboration in finance does not lead to better quality research . Scientometrics , 39 ( 2 ) , 173 - 184 . Baccini , A . , Barabesi , L . , & De Nicolao , G . ( 2020 ) . On the agreement between bibliometrics and peer review : Evidence from the Italian research assessment exercises . PloS one , 15 ( 11 ) , e0242520 . Bader , H . , Abdulelah , M . , Maghnam , R . , & Chin , D . ( 2021 ) . Clinical peer review ; A mandatory process with potential inherent bias in desperate need of reform . Journal of Community Hospital Internal Medicine Perspectives , 11 ( 6 ) , 817 - 820 . Balbuena , L . D . ( 2018 ) . The UK Research Excellence Framework and the Matthew effect : Insights from machine learning . Plos one , 13 ( 11 ) , e0207919 . Bar - Ilan , J . ( 2008 ) . Which h - index ? - A comparison of WoS , Scopus and Google Scholar . Scientometrics , 74 ( 2 ) , 257 - 271 . Barker , C . , & Pistrang , N . ( 2005 ) . Quality criteria under methodological pluralism : Implications for conducting and evaluating research . American Journal of Community Psychology , 35 ( 3 ) , 201 - 212 . Beel , J . , & Gipp , B . ( 2010 ) . Academic search engine spam and Google Scholar ' s resilience against it . Journal of Electronic Publishing , 13 ( 3 ) . Bertocchi , G . , Gambardella , A . , Jappelli , T . , Nappi , C . A . , & Peracchi , F . ( 2015 ) . Bibliometric evaluation vs . informed peer review : Evidence from Italy . Research Policy , 44 ( 2 ) , 451 - 466 . Bickley , M . , Kousha , K . , & Thelwall , M . ( 2021 ) . A systematic method for identifying references to academic research in grey literature . 18th International Conference on Scientometrics & Informetrics Proceedings , 121 – 132 . 66 Biggs , J . , Hawley , P . H . , & Biernat , M . ( 2018 ) . The academic conference as a chilly climate for women : Effects of gender representation on experiences of sexism , coping responses , and career intentions . Sex Roles , 78 ( 5 ) , 394 - 408 . Bishop , D . ( 2014 ) . BishopBlog : an alternative to REF2014 ? Blogpost . http : / / deevybee . blogspot . nl / 2013 / 01 / an - alternative - to - ref2014 . html Blümel , C . , & Schniedermann , A . ( 2020 ) . Studying review articles in scientometrics and beyond : a research agenda . Scientometrics , 124 ( 1 ) , 711 - 728 . Bond , J . , & Craig , D . pReview : The Artificially Intelligent Conference Reviewer A Step Toward Computational Criticism . Bordons , M . , Aparicio , J . , & Costas , R . ( 2013 ) . Heterogeneity of collaboration and its relationship with research impact in a biomedical field . Scientometrics , 96 ( 2 ) , 443 - 466 . Börner , K . , Klavans , R . , Patek , M . , Zoss , A . M . , Biberstine , J . R . , Light , R . P . , . . . & Boyack , K . W . ( 2012 ) . Design and update of a classification system : The UCSD map of science . PloS one , 7 ( 7 ) , e39464 . Bornmann , L . ( 2015 ) . Interrater reliability and convergent validity of F 1000 P rime peer review . Journal of the Association for Information Science and Technology , 66 ( 12 ) , 2415 - 2426 . Bornmann , L . ( 2017 ) . Is collaboration among scientists related to the citation impact of papers because their quality increases with collaboration ? An analysis based on data from F1000Prime and normalized citation scores . Journal of the Association for Information Science and Technology , 68 ( 4 ) , 1036 - 1047 . Bornmann , L . ( 2018 ) . Field classification of publications in Dimensions : A first case study testing its reliability and validity . Scientometrics , 117 ( 1 ) , 637 - 640 . Bornmann , L . , & Daniel , H . D . ( 2006 ) . Selecting scientific excellence through committee peer review - a citation analysis of publications previously published to approval or rejection of post - doctoral research fellowship applicants . Scientometrics , 68 ( 3 ) , 427 - 440 . Bornmann , L . , & Haunschild , R . ( 2018 ) . Do altmetrics correlate with the quality of papers ? A large - scale empirical study based on F1000Prime data . PloS one , 13 ( 5 ) , e0197133 . Bornmann , L . , & Leydesdorff , L . ( 2013 ) . The validation of ( advanced ) bibliometric indicators through peer assessments : A comparative study using data from InCites and F1000 . Journal of informetrics , 7 ( 2 ) , 286 - 291 . Bornmann , L . , & Leydesdorff , L . ( 2015 ) . Does quality and content matter for citedness ? A comparison with para - textual factors and over time . Journal of Informetrics , 9 ( 3 ) , 419 - 429 . Bornmann , L . , Guns , R . , Thelwall , M . , & Wolfram , D . ( 2021 ) . Which aspects of the Open Science agenda are most relevant to scientometric research and publishing ? An opinion paper . Quantitative Science Studies , 2 ( 2 ) , 438 - 453 . Bornmann , L . , Haunschild , R . , & Adams , J . ( 2019 ) . Do altmetrics assess societal impact in a comparable way to case studies ? An empirical test of the convergent validity of altmetrics based on data from the UK research excellence framework ( REF ) . Journal of Informetrics , 13 ( 1 ) , 325 - 340 . Bornmann , L . , Marx , W . , & Barth , A . ( 2013 ) . The normalization of citation counts based on classification systems . Publications , 1 ( 2 ) , 78 - 86 . Bornmann , L . , Schier , H . , Marx , W . & Daniel , H - D . ( 2012 ) . What factors determine citation counts of publications in chemistry besides their quality ? Journal of Informetrics , 6 , 11 - 18 . Boudreau , K . J . , Guinan , E . C . , Lakhani , K . R . , & Riedl , C . ( 2016 ) . Looking across and looking beyond the knowledge frontier : Intellectual distance , novelty , and resource allocation in science . Management Science , 62 ( 10 ) , 2765 - 2783 . Boyack , K . W . , & Klavans , R . ( 2005 ) . Predicting the importance of current papers . In Proceedings of the 10th International conference of the International Society for Scientometrics and Informetrics ( Vol . 1 , pp . 335 - 342 ) . Stockholm : Karolinska University Press . Boyack , K . W . , Patek , M . , Ungar , L . H . , Yoon , P . , & Klavans , R . ( 2014 ) . Classification of individual articles from all of science by research level . Journal of Informetrics , 8 ( 1 ) , 1 - 12 . 67 Brooks , T . A . ( 1986 ) . Evidence of complex citer motivations . Journal of the American Society for Information Science , 37 ( 1 ) , 34 - 36 . Burley , R . & Moylan , E . ( 2017 ) . What might Peer Rev look like in 2030 ? A report from BioMed Central and Digital Science . https : / / figshare . com / articles / journal _ contribution / What _ might _ peer _ review _ look _ like _ in _ 2030 _ / 4884878 / 1 Buter , R . K . , & van Raan , A . F . ( 2011 ) . Non - alphanumeric characters in titles of scientific publications : An analysis of their occurrence and correlation with citation impact . Journal of Informetrics , 5 ( 4 ) , 608 - 617 . Butler , L . , & McAllister , I . ( 2009 ) . Metrics or peer review ? Evaluating the 2001 UK research assessment exercise in political science . Political Studies Review , 7 ( 1 ) , 3 - 17 . Butler , L . , & McAllister , I . ( 2011 ) . Evaluating university research performance using metrics . European Political Science , 10 ( 1 ) , 44 - 58 . Cabanac , G . , Labbé , C . , & Magazinov , A . ( 2021 ) . Tortured phrases : A dubious writing style emerging in science . Evidence of critical issues affecting established journals . arXiv preprint arXiv : 2107 . 06751 . Callaham , M . , Wears , R . L . , & Weber , E . ( 2002 ) . Journal prestige , publication bias , and other characteristics associated with citation of published studies in peer - reviewed journals . Jama , 287 ( 21 ) , 2847 - 2850 . Campanario , J . ( 2009 ) . Rejecting and resisting Nobel class discoveries : accounts by Nobel Laureates . Scientometrics , 81 ( 2 ) , 549 - 565 . Case , D . O . , & Higgins , G . M . ( 2000 ) . How can we investigate citation behavior ? A study of reasons for citing literature in communication . Journal of the American Society for Information Science , 51 ( 7 ) , 635 - 645 . Ceci , S . J . , & Williams , W . M . ( 2011 ) . Understanding current causes of women ' s underrepresentation in science . Proceedings of the National Academy of Sciences , 108 ( 8 ) , 3157 - 3162 . Chakraborty , S . , Goyal , P . , & Mukherjee , A . ( 2020 , August ) . Aspect - based sentiment analysis of scientific reviews . In Proceedings of the ACM / IEEE Joint Conference on Digital Libraries in 2020 ( pp . 207 - 216 ) . Charlin , L . , & Zemel , R . ( 2013 ) . The Toronto paper matching system : an automated paper - reviewer assignment system . https : / / openreview . net / pdf ? id = caynafZAnBafx Checco , A . , Bracciale , L . , Loreti , P . , Pinfield , S . , & Bianchi , G . ( 2021 ) . AI - assisted peer review . Humanities and Social Sciences Communications , 8 ( 1 ) , 1 - 11 . Chen , J . , & Zhang , C . ( 2015 ) . Predicting citation counts of papers . In 2015 IEEE 14th International Conference on Cognitive Informatics & Cognitive Computing ( ICCI & CC ) ( pp . 434 - 440 ) . Los Alamitos : IEEE Press . Chen , T . , He , T . , Benesty , M . , Khotilovich , V . , Tang , Y . , Cho , H . , & Chen , K . ( 2015 ) . Xgboost : extreme gradient boosting . R package version 0 . 4 - 2 , 1 ( 4 ) , 1 - 4 . Cheuk , T . ( 2021 ) . Can AI be racist ? Color‐evasiveness in the application of machine learning to science assessments . Science Education , 105 ( 5 ) , 825 - 836 . Chi , P . S . , & Glänzel , W . ( 2017 ) . An empirical investigation of the associations among usage , scientific collaboration and citation impact . Scientometrics , 112 ( 1 ) , 403 - 412 . Clarivate ( 2022 ) . Reclassification of papers in multidisciplinary journals for creation of field baselines . http : / / help . prod - incites . com / inCites2Live / filterValuesGroup / researchAreaSchema / wosDetail / reclassification OfPapers . html Colebunders , R . , Kenyon , C . , & Rousseau , R . ( 2014 ) . Increase in numbers and proportions of review articles in tropical medicine , infectious diseases , and oncology . Journal of the Association for Information Science and Technology , 65 ( 1 ) , 201 – 205 . 68 Costas , R . , Zahedi , Z . , & Wouters , P . ( 2015 ) . Do “altmetrics” correlate with citations ? Extensive comparison of altmetric indicators with citations from a multidisciplinary perspective . Journal of the Association for Information Science and Technology , 66 ( 10 ) , 2003 - 2019 . Culumber , Z . W . , Anaya - Rojas , J . M . , Booker , W . W . , Hooks , A . P . , Lange , E . C . , Pluer , B . , . . . & Travis , J . ( 2019 ) . Widespread biases in ecological and evolutionary studies . Bioscience , 69 ( 8 ) , 631 - 640 . Cummings , D . , & Nassar , M . ( 2020 ) . Structured citation trend prediction using graph neural networks . In ICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) ( pp . 3897 - 3901 ) . IEEE . Cyranoski , D . ( 2019 ) . Artificial intelligence is selecting grant reviewers in China . Nature , 569 ( 7756 ) , 316 - 318 . https : / / www . nature . com / articles / d41586 - 019 - 01517 - 8 da Silva , J . A . T . , & Al - Khatib , A . ( 2021 ) . How do Clarivate Analytics and Publons propose to fortify peer review in the COVID - 19 era ? . Journal of Taibah University Medical Sciences , 16 ( 2 ) , 139 . De Groote , S . L . , & Raszewski , R . ( 2012 ) . Coverage of Google scholar , Scopus , and Web of Science : A case study of the h - index in nursing . Nursing Outlook , 60 ( 6 ) , 391 - 400 . de Winter , J . C . F . , Zadpoor , A . A . , & Dodou , D . ( 2014 ) . The expansion of Google Scholar versus Web of Science : A longitudinal study . Scientometrics , 98 ( 2 ) , 1547 - 1565 . Diakopoulos , N . , & Koliska , M . ( 2017 ) . Algorithmic transparency in the news media . Digital journalism , 5 ( 7 ) , 809 - 828 . Didegah , F . ( 2014 ) . Factors associating with the future citation impact of published articles : A statistical modelling approach . Doctoral dissertation , University of Wolverhampton . https : / / wlv . openrepository . com / handle / 2436 / 322738 Didegah , F . , & Thelwall , M . ( 2013a ) . Determinants of research citation impact in nanoscience and nanotechnology . Journal of the American Society for Information Science & Technology , 64 ( 55 ) , 1055 – 1064 . Didegah , F . , & Thelwall , M . ( 2013b ) . Which factors help authors produce the highest impact research ? Collaboration , journal and document properties . Journal of Informetrics , 7 ( 4 ) , 861 - 873 . Dien , T . T . , Loc , B . H . , & Thai - Nghe , N . ( 2019 ) . Article classification using natural language processing and machine learning . In 2019 International Conference on Advanced Computing and Applications ( ACOMP ) ( pp . 78 - 84 ) . IEEE . Dowling , M . , Hammami , H . , & Zreik , O . ( 2018 ) . Easy to read , easy to cite ? . Economics letters , 173 , 100 - 103 . Du , J . , Tang , X . , & Wu , Y . ( 2016 ) . The effects of research level and article type on the differences between citation metrics and F 1000 recommendations . Journal of the Association for Information Science and Technology , 67 ( 12 ) , 3008 - 3021 . Elgendi , M . ( 2019 ) . Characteristics of a highly cited article : A machine learning perspective . IEEE Access , 7 , 87977 - 87986 . Eykens , J . , Guns , R . , & Engels , T . C . ( 2021 ) . Fine - grained classification of social science journal articles using textual data : A comparison of supervised machine learning approaches . Quantitative Science Studies , 2 ( 1 ) , 89 - 110 . Eykens , J . , Guns , R . , Engels , T . C . , Catalano , G . , Daraio , C . , Gregori , M . , . . . & Ruocco , G . ( 2019 ) . Article Level Classification of Publications in Sociology : An Experimental Assessment of Supervised Machine Learning Approaches . In G . Catalano , C . Daraio , M . Gregori , H . F . Moed , & G . Ruocco ( Eds . ) , 17th International Conference on Scientometrics & Informetrics ( ISSI2019 ) ( vol . 1 , pp . 738 – 743 ) . Fairclough , R . & Thelwall , M . ( 2022 ) . Questionnaires mentioned in academic research 1996 - 2019 : Rapid increase but declining citation impact . Learned Publishing , 35 , 241 - 252 . Falagas , M . E . , Zarkali , A . , Karageorgopoulos , D . E . , Bardakas , V . , & Mavros , M . N . ( 2013 ) . The impact of article length on the number of future citations : a bibliometric analysis of general medicine journals . PLoS One , 8 ( 2 ) , e49476 . 69 Fan , L . , Wang , Y . , Ding , S . , & Qi , B . ( 2020 ) . Productivity trends and citation impact of different institutional collaboratio n patterns at the research units’ level . Scientometrics , 125 ( 2 ) , 1179 - 1196 . Feng , X . , Zhang , H . , Ren , Y . , Shang , P . , Zhu , Y . , Liang , Y . , & Xu , D . ( 2019 ) . The deep learning - based recommender system “Pubmender” for choosing a biomedical publication venue : Development and validation study . Journal of Medical Internet Research , 21 ( 5 ) , 27 – 35 . Fiala , D . , Král , P . , & Dostal , M . ( 2021 ) . Are papers asking questions cited more frequently in Computer Science ? . Computers , 10 ( 8 ) , 96 . Figg , W . D . , Dunn , L . , Liewehr , D . J . , Steinberg , S . M . , Thurman , P . W . , Barrett , J . C . , & Birkinshaw , J . ( 2006 ) . Scientific collaboration results in higher citation rates of published articles . Pharmacotherapy : The Journal of Human Pharmacology and Drug Therapy , 26 ( 6 ) , 759 - 767 . Fox , C . W . , Paine , C . T . , & Sauterey , B . ( 2016 ) . Citations increase with manuscript length , author number , and references cited in ecology journals . Ecology and Evolution , 6 ( 21 ) , 7717 - 7726 . Franceschet , M . , & Costantini , A . ( 2010 ) . The effect of scholar collaboration on impact and quality of academic papers . Journal of Informetrics , 4 ( 4 ) , 540 - 553 . Franceschet , M . , & Costantini , A . ( 2011 ) . The first Italian research assessment exercise : A bibliometric perspective . Journal of Informetrics , 5 ( 2 ) , 275 - 291 . Fu , L . , & Aliferis , C . ( 2010 ) . Using content - based and bibliometric features for machine learning models to predict citation counts in the biomedical literature . Scientometrics , 85 ( 1 ) , 257 - 270 . Furl , N . , Phillips , P . J . , & O ' Toole , A . J . ( 2002 ) . Face recognition algorithms and the other‐race effect : computational mechanisms for a developmental contact hypothesis . Cognitive science , 26 ( 6 ) , 797 - 815 . Gans , J . S . , & Shepherd , G . B . ( 1994 ) . How are the mighty fallen : Rejected classic articles by leading economists . Journal of Economic Perspectives , 8 ( 1 ) , 165 - 179 . Gazni , A . ( 2011 ) . Are the abstracts of high impact articles more readable ? Investigating the evidence from top research institutions in the world . Journal of Information Science , 37 ( 3 ) , 273 - 281 . Gazni , A . , & Didegah , F . ( 2011 ) . Investigating different types of research collaboration and citation impact : a case study of Harvard University ' s publications . Scientometrics , 87 ( 2 ) , 251 - 265 . Ghosal , T . , Raj , A . , Ekbal , A . , Saha , S . , & Bhattacharyya , P . ( 2019a ) . A deep multimodal investigation to determine the appropriateness of scholarly submissions . In 2019 ACM / IEEE Joint Conference on Digital Libraries ( JCDL ) ( pp . 227 - 236 ) . IEEE . Ghosal , T . , Verma , R . , Ekbal , A . , & Bhattacharyya , P . ( 2019b ) . DeepSentiPeer : harnessing sentiment in review texts to recommend peer review decisions . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ( pp . 1120 - 1130 ) . Glänzel , W . ( 2002 ) . Co - authorship patterns and trends in the sciences : a bibliometric study with implications for database indexing and search strategic 1980 – 1998’ . Library Trends , 50 ( 3 ) , 461 – 73 . Glänzel , W . , & Schubert , A . ( 2003 ) . A new classification scheme of science fields and subfields designed for scientometric evaluation purposes . Scientometrics , 56 ( 3 ) , 357 - 367 . Glänzel , W . , Rinia , E . J . , & Brocken , M . G . ( 1995 ) . A bibliometric study of highly cited European physics papers in the 80s . Research Evaluation , 5 ( 2 ) , 113 - 122 . Glänzel , W . , Schubert , A . , & Czerwon , H . J . ( 1999 ) . An item - by - item subject classification of papers published in multidisciplinary and general journals using reference analysis . Scientometrics , 44 ( 3 ) , 427 - 439 . Glänzel , W . , Thijs , B . , Schubert , A . , & Debackere , K . ( 2009 ) . Subfield - specific normalized relative indicators and a new generation of relational charts : Methodological foundations illustrated on the assessment of institutional research performance . Scientometrics , 78 ( 1 ) , 165 - 188 . 70 Gläser , J . , Glänzel , W . , & Scharnhorst , A . ( 2017 ) . Same data — different results ? Towards a comparative approach to the identification of thematic structures in science . Scientometrics , 111 ( 2 ) , 981 - 998 . Gnewuch , M . , & Wohlrabe , K . ( 2017 ) . Title characteristics and citations in economics . Scientometrics , 110 ( 3 ) , 1573 - 1578 . Guerrero - Bote , V . P . , Chinchilla - Rodríguez , Z . , Mendoza , A . , & de Moya - Anegón , F . ( 2021 ) . Comparative analysis of the bibliographic data sources Dimensions and Scopus : an approach at the country and institutional levels . Frontiers in Research Metrics and Analytics , 5 , 19 . 10 . 3389 / frma . 2020 . 593494 Gunning , D . , Stefik , M . , Choi , J . , Miller , T . , Stumpf , S . , & Yang , G . Z . ( 2019 ) . XAI — Explainable artificial intelligence . Science Robotics , 4 ( 37 ) , eaay7120 . Guo , F . , Ma , C . , Shi , Q . , & Zong , Q . ( 2018 ) . Succinct effect or informative effect : The relationship between title length and the number of citations . Scientometrics , 116 ( 3 ) , 1531 - 1539 . Guo , S . , Zhang , G . , Ju , Q . , Chen , Y . , Chen , Q . , & Li , L . ( 2015 ) . The evolution of conceptual diversity in economics titles from 1890 to 2012 . Scientometrics , 102 ( 3 ) , 2073 - 2088 . Gusenbauer , M . ( 2019 ) . Google Scholar to overshadow them all ? Comparing the sizes of 12 academic search engines and bibliographic databases . Scientometrics , 118 ( 1 ) , 177 - 214 . Habibzadeh , F . , & Yadollahie , M . ( 2010 ) . Are shorter article titles more attractive for citations ? Crosssectional study of 22 scientific journals . Croatian Medical Journal , 51 ( 2 ) , 165 - 170 . Haddawy , P . , Hassan , S . U . , Asghar , A . , & Amin , S . ( 2016 ) . A comprehensive examination of the relation of three citation - based journal metrics to expert judgment of journal quality . Journal of Informetrics , 10 ( 1 ) , 162 - 173 . Haddow , G . , & Genoni , P . ( 2010 ) . Citation analysis and peer ranking of Australian social science journals . Scientometrics , 85 ( 2 ) , 471 - 487 . Hafeez , D . M . , Jalal , S . , & Khosa , F . ( 2019 ) . Bibliometric analysis of manuscript characteristics that influence citations : A comparison of six major psychiatry journals . Journal of Psychiatric Research , 108 , 90 - 94 . Hartley , J . , & Sydes , M . ( 1997 ) . Are structured abstracts easier to read than traditional ones ? . Journal of Research in Reading , 20 ( 2 ) , 122 - 136 . Harzing , A . ( 2017 ) . Web of Science : How to be robbed of 10 years of citations in one week ! https : / / harzing . com / blog / 2017 / 02 / web - of - science - to - be - robbed - of - 10 - years - of - citations - in - one - week Harzing , A . W . ( 2019 ) . Two new kids on the block : How do Crossref and Dimensions compare with Google Scholar , Microsoft Academic , Scopus and the Web of Science ? . Scientometrics , 120 ( 1 ) , 341 - 349 . Hasan , S . , & Breunig , R . ( 2021 ) . Article length and citation outcomes . Scientometrics , 126 ( 9 ) , 7583 - 7608 . Haslam , N . , Ban , L . , Kaufmann , L . , Loughnan , S . , Peters , K . , Whelan , J . , & Wilson , S . ( 2008 ) . What makes an article influential ? Predicting impact in social and personality psychology . Scientometrics , 76 ( 1 ) , 169 - 185 . Haustein , S . , Costas , R . , & Larivière , V . ( 2015 ) . Characterizing social media metrics of scholarly papers : The effect of document properties and collaboration patterns . PloS one , 10 ( 3 ) , e0120495 . He , Z . L . ( 2009 ) . International collaboration does not have greater epistemic authority . Journal of the American Society for Information Science and Technology , 60 ( 10 ) , 2151 – 2164 . Heaven , D . ( 2018 ) . The age of AI peer reviews . Nature 563 ( 29 ) , 609 – 610 . https : / / media . nature . com / original / magazine - assets / d41586 - 018 - 07245 - 9 / d41586 - 018 - 07245 - 9 . pdf HEFCE . ( 2015 ) . The Metric Tide : Correlation analysis of REF2014 scores and metrics ( Supplementary Report II to the independent Review of the Role of Metrics in Research Assessment and 71 Management ) . Higher Education Funding Council for England . http : / / www . dcscience . net / 2015 _ metrictideS2 . pdf Heibi , I . , Peroni , S . , & Shotton , D . ( 2019 ) . Software review : COCI , the OpenCitations Index of Crossref open DOI - to - DOI citations . Scientometrics , 121 ( 2 ) , 1213 - 1228 . Herzog , C . , & Lunn , B . K . ( 2018 ) . Response to the letter field classification of publications in dimensions : A first case study testing its reliability and validity . Scientometrics , 117 ( 1 ) , 641 – 645 . Herzog , C . , Hook , D . , & Konkiel , S . ( 2020 ) . Dimensions : Bringing down barriers between scientometricians and data . Quantitative Science Studies , 1 ( 1 ) , 387 - 395 . Hicks , D . , Wouters , P . , Waltman , L . , De Rijcke , S . , & Rafols , I . ( 2015 ) . Bibliometrics : the Leiden Manifesto for research metrics . Nature , 520 ( 7548 ) , 429 - 431 . Hodge , D . R . , Victor , B . G . , Grogan - Kaylor , A . , & Perron , B . E . ( 2017 ) . Disseminating high - impact social work scholarship : A longitudinal examination of 5 - year citation count correlates . Journal of the Society for Social Work and Research , 8 ( 2 ) , 211 - 231 . Hojat , M . , Gonnella , J . S . , & Caelleigh , A . S . ( 2003 ) . Impartial judgment by the “gatekeepers” of science : fallibility and accountability in the peer review process . Advances in Health Sciences Education , 8 ( 1 ) , 75 - 96 . Hsu , J . W . , & Huang , D . W . ( 2011 ) . Correlation between impact and collaboration . Scientometrics , 86 ( 2 ) , 317 - 324 . Hu , H . , Wang , D . , & Deng , S . ( 2021 ) . Analysis of the scientific literature ' s abstract writing style and citations . Online Information Review , 45 ( 7 ) , 1290 - 1305 . Hu , Y . H . , Tai , C . T . , Liu , K . E . , & Cai , C . F . ( 2020 ) . Identification of highly - cited papers using topic - model - based and bibliometric features : The consideration of keyword popularity . Journal of Informetrics , 14 ( 1 ) , 101004 . Iana , A . , & Paulheim , H . ( 2021 , September ) . GraphConfRec : A Graph Neural Network - Based Conference Recommender System . In 2021 ACM / IEEE Joint Conference on Digital Libraries ( JCDL ) ( pp . 90 - 99 ) . IEEE . Ibanez , A . , Bielza , C . , & Larranaga , P . ( 2013 ) . Relationship among research collaboration , number of documents and number of citations : a case study in Spanish computer science production in 2000 – 2009 . Scientometrics , 95 ( 2 ) , 689 - 716 . Jacques , T . S . , & Sebire , N . J . ( 2010 ) . The impact of article titles on citation hits : an analysis of general and specialist medical journals . JRSM short reports , 1 ( 1 ) , 1 - 5 . Jamali , H . R . , & Nikzad , M . ( 2011 ) . Article title type and its relation with the number of downloads and citations . Scientometrics , 88 ( 2 ) , 653 - 661 . Jannot , A . S . , Agoritsas , T . , Gayet - Ageron , A . , & Perneger , T . V . ( 2013 ) . Citation bias favoring statistically significant studies was present in medical research . Journal of clinical epidemiology , 66 ( 3 ) , 296 - 301 . Jorm , A . F . ( 2021 ) . Publons as a source of high volume , poorly targeted reviewer requests : The need for better standards of practice by publishers . Learned Publishing . https : doi . org / 10 . 1002 / leap . 1420 Jump . P . ( 2015 ) . Can the Research Excellence Framework run on metrics ? Times Higher Education . https : / / www . timeshighereducation . com / can - the - research - excellence - framework - ref - run - on - metrics Kalmukov , Y . ( 2020 ) . An algorithm for automatic assignment of reviewers to papers . Scientometrics , 124 ( 3 ) , 1811 - 1850 . Kandimalla , B . , Rohatgi , S . , Wu , J . , et al . ( 2021 ) . Large scale subject category classifcation of scholarly papers with deep attentive neural networks . Frontiers in Research Metrics and Analytics , 5 , 31 . Karmakar , M . , Banshal , S . K . , & Singh , V . K . ( 2021 ) . A large - scale comparison of coverage and mentions captured by the two altmetric aggregators : Altmetric . com and PlumX . Scientometrics , 126 ( 5 ) , 4465 - 4489 . 72 Katz , J . , & Hicks , D . ( 1997 ) . How much is a collaboration worth ? A calibrated bibliometric model . Scientometrics , 40 ( 3 ) , 541 - 554 . Keating , D . M . , Richards , A . S . , Palomares , N . A . , Banas , J . A . , Joyce , N . , & Rains , S . A . ( 2022 ) . Titling practices and their implications in communication research 1970 - 2010 : Cutesy cues carry citation consequences . Communication Research , 49 ( 5 ) , 627 - 648 . Kelly , A . , & Burrows , R . ( 2011 ) . Measuring the value of sociology ? Some notes on performative metricization in the contemporary academy . The Sociological Review , 59 , 130 - 150 . Khabsa , M . , & Giles , C . L . ( 2014 ) . The number of scholarly documents on the public web . PloS one , 9 ( 5 ) , e93949 . Khor , K . A . , & Yu , L . G . ( 2016 ) . Influence of international co - authorship on the research citation impact of young universities . Scientometrics , 107 ( 3 ) , 1095 - 1110 . Kim , S . , Choi , H . , Kim , N . , Chung , E . , & Lee , J . Y . ( 2018 ) . Comparative analysis of manuscript management systems for scholarly publishing . Science Editing , 5 ( 2 ) , 124 - 134 . Klavans , R . , & Boyack , K . W . ( 2017a ) . Research portfolio analysis and topic prominence . Journal of Informetrics , 11 ( 4 ) , 1158 - 1174 . Klavans , R . , & Boyack , K . W . ( 2017b ) . Which type of citation analysis generates the most accurate taxonomy of scientific and technical knowledge ? . Journal of the Association for Information Science and Technology , 68 ( 4 ) , 984 - 998 . Kordzadeh , N . , & Ghasemaghaei , M . ( 2021 ) . Algorithmic bias : review , synthesis , and future research directions . European Journal of Information Systems . https : / / doi . org / 10 . 1080 / 0960085X . 2021 . 1927212 Kostoff , R . ( 2007 ) . The difference between highly and poorly cited medical articles in the journal Lancet . Scientometrics , 72 , 513 - 520 . Kousha , K . ( 2019 ) . Web citation indicators for wider impact assessment of articles . Springer handbook of science and technology indicators , 801 - 818 . Kousha , K . , & Thelwall , M . ( 2007 ) . Google Scholar citations and Google Web / URL citations : A multi - discipline exploratory analysis . Journal of the American Society for Information Science and Technology , 58 ( 7 ) , 1055 - 1065 . Kousha , K . , & Thelwall , M . ( 2009 ) . Google Book Search : Citation analysis for social science and the humanities . Journal of the American Society for Information Science and Technology , 60 ( 8 ) , 1537 - 1549 . Kousha , K . , & Thelwall , M . ( 2015a ) . An automatic method for extracting citations from Google Books . Journal of the Association for Information Science and Technology , 66 ( 2 ) , 309 - 320 . Kousha , K . , & Thelwall , M . ( 2015b ) . Web indicators for research evaluation . Part 3 : books and non standard outputs . Profesional de la Información , 24 ( 6 ) , 724 - 736 . Kousha , K . , & Thelwall , M . ( 2017 ) . Patent citation analysis with Google . Journal of the Association for Information Science and Technology , 68 ( 1 ) , 48 - 61 . Kousha , K . , Thelwall , M . , & Abdoli , M . ( 2018 ) . Can Microsoft Academic assess the early citation impact of in - press articles ? A multi - discipline exploratory analysis . Journal of Informetrics , 12 ( 1 ) , 287 - 298 . Kousha , K . , Thelwall , M . , & Abdoli , M . ( 2021 ) . Which types of online evidence show the nonacademic benefits of research ? Websites cited in UK impact case studies . Quantitative Science Studies , 2 ( 3 ) , 864 - 881 . Kousha , K . , Thelwall , M . , & Rezaie , S . ( 2011 ) . Assessing the citation impact of books : The role of Google Books , Google Scholar , and Scopus . Journal of the American Society for information science and technology , 62 ( 11 ) , 2147 - 2164 . Kulkarni , A . V . , Aziz , B . , Shams , I . , & Busse , J . W . ( 2009 ) . Comparisons of citations in Web of Science , Scopus , and Google Scholar for articles published in general medical journals . JAMA - Journal of the American Medical Association , 302 ( 10 ) , 1092 - 1096 . Kumari , R . , Uddin , A . , Lee , B . H . , & Choi , K . ( 2020 ) . Analyzing the factors influencing the waiting time to first citation and long - term impact of publications . J . Sci . Res . , 9 ( 2 ) , 127 - 135 . 73 Lambrecht , A . , & Tucker , C . ( 2019 ) . Algorithmic bias ? An empirical study of apparent gender - based discrimination in the display of STEM career ads . Management Science , 65 ( 7 ) , 2966 - 2981 . Lancho Barrantes , B . S . , Guerrero Bote , V . P . , Rodríguez , Z . C . , & de Moya Anegón , F . ( 2012 ) . Citation flows in the zones of influence of scientific collaborations . Journal of the American Society for Information Science and Technology , 63 ( 3 ) , 481 - 489 . Lancho - Barrantes , B . S . , Guerrero - Bote , V . P . , & de Moya - Anegón , F . ( 2013 ) . Citation increments between collaborating countries . Scientometrics , 94 ( 3 ) , 817 - 831 . Lancho - Barrantes , B . S . , Guerrero - Bote , V . P . , & Moya - Anegón , F . ( 2010 ) . What lies behind the averages and significance of citation indicators in different disciplines ? . Journal of Information Science , 36 ( 3 ) , 371 - 382 . Langfeldt , L . , Nedeva , M . , Sörlin , S . , & Thomas , D . A . ( 2020 ) . Co - existing notions of research quality : A framework to study context - specific understandings of good research . Minerva , 58 ( 1 ) , 115 - 137 . Larivière , V . , Gingras , Y . , Sugimoto , C . R . , & Tsou , A . ( 2015 ) . Team size matters : Collaboration and scientific impact since 1900 . Journal of the Association for Information Science and Technology , 66 ( 7 ) , 1323 - 1332 . Lee , C . J . , Sugimoto , C . R . , Zhang , G . , & Cronin , B . ( 2013 ) . Bias in peer review . Journal of the American Society for Information Science and Technology , 64 ( 1 ) , 2 - 17 . Lee , N . T . ( 2018 ) . Detecting racial bias in algorithms and machine learning . Journal of Information , Communication and Ethics in Society . 16 ( 3 ) , 252 - 260 . Lee , P . S . , West , J . D . , & Howe , B . ( 2017 ) . Viziometrics : Analyzing visual information in the scientific literature . IEEE Transactions on Big Data , 4 ( 1 ) , 117 - 129 . Leimu , R . , & Koricheva , J . ( 2005 ) . What determines the citation frequency of ecological papers ? Trends in Ecology & Evolution , 20 ( 1 ) , 28 – 32 . Letchford , A . , Moat , H . S . , & Preis , T . ( 2015 ) . The advantage of short paper titles . Royal Society open science , 2 ( 8 ) , 150266 . Letchford , A . , Preis , T . , & Moat , H . S . ( 2016 ) . The advantage of simple paper abstracts . Journal of Informetrics , 10 ( 1 ) , 1 - 8 . Lewison , G . , & Hartley , J . ( 2005 ) . What ' s in a title ? Numbers of words and the presence of colons . Scientometrics , 63 ( 2 ) , 341 - 356 . Leydesdorff , L . , & Bornmann , L . ( 2016 ) . The operational ization of “fields” as WoS subject categories ( WC s ) in evaluative bibliometrics : The cases of “library and information science” and “science & technology studies” . Journal of the Association for Information Science and Technology , 67 ( 3 ) , 707 - 714 . Leydesdorff , L . , Bornmann , L . , & Wagner , C . S . ( 2019 ) . The relative influences of government funding and international collaboration on citation impact . Journal of the Association for Information Science and Technology , 70 ( 2 ) , 198 - 201 . Li , S . , Zhao , W . X . , Yin , E . J . , & Wen , J . R . ( 2019 ) . A neural citation count prediction model based on peer review text . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) ( pp . 4914 - 4924 ) . Li , X . , & Thelwall , M . ( 2012 ) . F1000 , Mendeley and traditional bibliometric indicators . In Proceedings of the 17th international conference on science and technology indicators ( Vol . 2 , pp . 451 - 551 ) . Canada : Montréal . Li , X . , & Watanabe , T . ( 2013 ) . Automatic paper - to - reviewer assignment , based on the matching degree of the reviewers . Procedia Computer Science , 22 , 633 - 642 . Lin , J . , Song , J . , Zhou , Z . , & Shi , X . ( 2021 ) . Automated scholarly paper review : Possibility and challenges . arXiv preprint arXiv : 2111 . 07533 . Liu , O . , Wang , J . , Ma , J . , & Sun , Y . ( 2016 ) . An intelligent decision support approach for reviewer assignment in R & D project selection . Computers in Industry , 76 , 1 - 10 . 74 Liu , X . , Wang , X . , & Zhu , D . ( 2022 ) . Reviewer recommendation method for scientific research proposals : a case for NSFC . Scientometrics , 127 , 3343 - 3366 . Lokker , C . , McKibbon , K . A . , McKinlay , R . J . , Wilczynski , N . L . , & Haynes , R . B . ( 2008 ) . Prediction of citation counts for clinical articles at two years using data available within three weeks of publication : retrospective cohort study . BMJ , 336 ( 7645 ) , 655 - 657 . López - Cózar , E . D . , Robinson - García , N . , & Torres - Salinas , D . ( 2014 ) . The Google Scholar experiment : How to index false papers and manipulate bibliometric indicators . Journal of the Association for Information Science and Technology , 65 ( 3 ) , 446 - 454 . Lu , C . , Bu , Y . , Dong , X . , Wang , J . , Ding , Y . , Larivière , V . , . . . & Zhang , C . ( 2019 ) . Analyzing linguistic complexity and scientific impact . Journal of Informetrics , 13 ( 3 ) , 817 - 829 . Luo , J . , Feliciani , T . , Reinhart , M . , Hartstein , J . , Das , V . , Alabi , O . , & Shankar , K . ( 2021 ) . Analysing sentiments in peer review reports : evidence from two science funding agencies . Quantitative Science Studies , 1 - 52 . Lyu , P . H . , & Wolfram , D . ( 2018 ) . Do longer articles gather more citations ? Article length and scholarly impact among top biomedical journals . Proceedings of the Association for Information Science and Technology , 55 ( 1 ) , 319 - 326 . Ma , A . , Liu , Y . , Xu , X . , & Dong , T . ( 2021 ) . A deep - learning based citation count prediction model with paper metadata semantic features . Scientometrics , 126 ( 8 ) , 6803 - 6823 . Mahdi , S . , D ' Este , P . & Neely , A . ( 2008 ) . Citation counts : Are they good predictors of RAE scores ? London : Advanced Institute of Management Research . https : / / papers . ssrn . com / sol3 / papers . cfm ? abstract _ id = 1154053 Mahoney , M . J . ( 1977 ) . Publication prejudices : An experimental study of confirmatory bias in the peer review system . Cognitive Therapy and Research , 1 ( 2 ) , 161 - 175 . Mammola , S . , Fontaneto , D . , Martínez , A . , & Chichorro , F . ( 2021 ) . Impact of the reference list features on the number of citations . Scientometrics , 126 ( 1 ) , 785 - 799 . Martín - Martín , A . , Thelwall , M . , Orduna - Malea , E . , & Delgado López - Cózar , E . ( 2021 ) . Google Scholar , Microsoft Academic , Scopus , Dimensions , Web of Science , and OpenCitations’ COCI : a multidisciplinary comparison of coverage via citations . Scientometrics , 126 ( 1 ) , 871 - 906 . Medoff , M . H . ( 2003 ) . Collaboration and the quality of economics research . Labour Economics , 10 ( 5 ) , 597 - 608 . Meho , L . I . , & Yang , K . ( 2007 ) . Impact of data sources on citation counts and rankings of LIS faculty : Web of Science versus Scopus and Google Scholar . Journal of the American Society for Information Science and Technology , 58 ( 13 ) , 2105 - 2125 . Mehrabi , N . , Morstatter , F . , Saxena , N . , Lerman , K . , & Galstyan , A . ( 2021 ) . A survey on bias and fairness in machine learning . ACM Computing Surveys ( CSUR ) , 54 ( 6 ) , 1 - 35 . Menke , J . , Roelandse , M . , Ozyurt , B . , Martone , M . , & Bandrowski , A . ( 2020 ) . The rigor and transparency index quality metric for assessing biological and medical science methods . Iscience , 23 ( 11 ) , 101698 . Merton , R . K . ( 1968 ) . The Matthew Effect in Science : The reward and communication systems of science are considered . Science , 159 ( 3810 ) , 56 - 63 . Mingers , J . , & Lipitakis , E . A . E . C . G . ( 2010 ) . Counting the citations : A comparison of Web of Science and Google Scholar in the field of business and management . Scientometrics , 85 ( 2 ) , 613 - 625 . Mingers , J . , & Xu , F . ( 2010 ) . The drivers of citations in management science journals . European Journal of Operational Research , 205 ( 2 ) , 422 - 430 . Miranda , R . , & Garcia - Carpintero , E . ( 2018 ) . Overcitation and overrepresentation of review papers in the most cited papers . Journal of Informetrics , 12 ( 4 ) , 1015 - 1030 . Moed , H . F . ( 2006 ) . Citation analysis in research evaluation ( Vol . 9 ) . Springer Science & Business Media . Mohammadi , E . , & Thelwall , M . ( 2013 ) . Assessing non - standard article impact using F1000 labels . Scientometrics , 97 ( 2 ) , 383 - 395 . 75 Mongeon , P . , & Paul - Hus , A . ( 2016 ) . The journal coverage of Web of Science and Scopus : a comparative analysis . Scientometrics , 106 ( 1 ) , 213 - 228 . Moys , J . L . ( 2014 ) . Typographic layout and first impressions : testing how changes in text layout influence reader ' s judgments of documents . Visible Language , 48 ( 1 ) , 881 . https : / / centaur . reading . ac . uk / 39859 / 1 / Typographic % 20layout % 20ACCEPTED . pdf Mryglod , O . , Kenna , R . , Holovatch , Y . , & Berche , B . ( 2013 ) . Comparison of a citation - based indicator and peer review for absolute and specific measures of research - group excellence . Scientometrics , 97 ( 3 ) , 767 - 777 . Mryglod , O . , Kenna , R . , Holovatch , Y . , & Berche , B . ( 2015 ) . Predicting results of the Research Excellence Framework using departmental h - index . Scientometrics , 102 ( 3 ) , 2165 - 2180 . Narin , F . , Stevens , K . , & Whitlow , E . S . ( 1991 ) . Scientific co - operation in Europe and the citation of multinationally authored papers . Scientometrics , 21 ( 3 ) , 313 - 323 . Navarro , C . L . A . , Damen , J . A . , Takada , T . , Nijman , S . W . , Dhiman , P . , Ma , J . , . . . & Hooft , L . ( 2021 ) . Risk of bias in studies on prediction models developed using supervised machine learning techniques : systematic review . BMJ , 375 . Neuhaus , C . , & Daniel , H . D . ( 2009 ) . A new reference standard for citation analysis in chemistry and related fields based on the sections of Chemical Abstracts . Scientometrics , 78 ( 2 ) , 219 - 229 . Nomaler , Ö . , Frenken , K . , & Heimeriks , G . ( 2013 ) . Do more distant collaborations have more citation impact ? . Journal of Informetrics , 7 ( 4 ) , 966 - 971 . Norris , M . & Oppenheim , C . ( 2003 ) . Citation counts and the Research Assessment Exercise V : Archaeology and the 2001 RAE . Journal of Documentation , 59 ( 6 ) , 709 - 730 . Norris , M . & Oppenheim , C . ( 2010 ) . Peer review and the h - index : Two studies . Journal of Informetrics , 4 ( 3 ) , 221 - 232 . Onodera , N . , & Yoshikane , F . ( 2015 ) . Factors affecting citation rates of research articles . Journal of the Association for Information Science and Technology , 66 ( 4 ) , 739 - 764 . Oppenheim , C . & Summers , M . ( 2008 ) . Citation counts and the Research Assessment Exercise , part VI : Unit of assessment 67 ( Music ) . Information Research , 13 ( 2 ) . http : / / InformationR . net / ir / 13 - 2 / paper342 . html Oppenheim , C . ( 1995 ) . The correlation between citation counts and the 1992 Research Assessment Exercise ratings for British Library and Information Science university departments . Journal of Documentation , 51 ( 1 ) , 18 - 27 . Oppenheim , C . ( 1997 ) . The correlation between citation counts and the 1992 Research Assessment Exercise ratings for British research in Genetics , Anatomy and Archaeology . Journal of Documentation , 53 ( 5 ) , 477 - 487 . Orduña - Malea , E . , & López - Cózar , E . D . ( 2018 ) . Dimensions : re - discovering the ecosystem of scientific information . arXiv preprint arXiv : 1804 . 05365 . Orduña - Malea , E . , Ayllón , J . M . , Martín - Martín , A . , & Delgado Lopez - Cozar , E . ( 2015 ) . Methods for estimating the size of Google Scholar . Scientometrics , 104 ( 3 ) , 931 - 949 . Ortega , J . L . ( 2017 ) . Are peer - review activities related to reviewer bibliometric performance ? A scientometric analysis of Publons . Scientometrics , 112 ( 2 ) , 947 - 962 . Ortega , J . L . ( 2019 ) . Exploratory analysis of Publons metrics and their relationship with bibliometric and altmetric impact . Aslib Journal of Information Management , 71 ( 1 ) , 124 - 136 . Ortega , J . L . ( 2020 ) . Altmetrics data providers : A metaanalysis review of the coverage of metrics and publication . El profesional de la información ( EPI ) , 29 ( 1 ) . http : / / profesionaldelainformacion . com / contenidos / 2020 / ene / ortega . pdf Ortega , J . L . ( 2022 ) . Classification and analysis of PubPeer comments : How a web journal club is used . Journal of the Association for Information Science and Technology . 73 ( 5 ) , 655 - 670 . Paiva , C . E . , Lima , J . P . D . S . N . , & Paiva , B . S . R . ( 2012 ) . Articles with short titles describing the results are cited more often . Clinics , 67 , 509 - 513 . Pearson , W . S . ( 2021 ) . Quoted speech in linguistics research article titles : patterns of use and effects on citations . Scientometrics , 126 ( 4 ) , 3421 - 3442 . 76 Peng , T . Q . , & Zhu , J . J . ( 2012 ) . Where you publish matters most : A multilevel analysis of factors affecting citations of internet studies . Journal of the American Society for Information Science and Technology , 63 ( 9 ) , 1789 - 1803 . Peroni , S . , & Shotton , D . ( 2020 ) . OpenCitations , an infrastructure organization for open scholarship . Quantitative Science Studies , 1 ( 1 ) , 428 - 444 . Peters , H . P . , & van Raan , A . F . ( 1994 ) . On determinants of citation scores : A case study in chemical engineering . Journal of the American Society for Information Science , 45 ( 1 ) , 39 - 49 . Pradhan , T . , & Pal , S . ( 2020 ) . CNAVER : A content and network - based academic venue recommender system . Knowledge - Based Systems , 189 , 105092 . Pradhan , T . , Bhatia , C . , Kumar , P . , & Pal , S . ( 2021 ) . A deep neural architecture based meta - review generation and final decision prediction of a scholarly article . Neurocomputing , 428 , 218 - 238 . Prates , M . O . , Avelar , P . H . , & Lamb , L . C . ( 2020 ) . Assessing gender bias in machine translation : a case study with google translate . Neural Computing and Applications , 32 ( 10 ) , 6363 - 6381 . Price , S . , & Flach , P . A . ( 2017 ) . Computational support for academic peer review : A perspective from artificial intelligence . Communications of the ACM , 60 ( 3 ) , 70 - 79 . Pride , D . , & Knoth , P . ( 2018 ) . Peer review and citation data in predicting university rankings , a large - scale analysis . In International Conference on Theory and Practice of Digital Libraries ( pp . 195 - 207 ) . Springer , Cham . Priem , J . , Piwowar , H . A . , & Hemminger , B . M . ( 2012 ) . Altmetrics in the wild : Using social media to explore scholarly impact . arXiv preprint arXiv : 1203 . 4745 . Puuska , H . M . , Muhonen , R . , & Leino , Y . ( 2014 ) . International and domestic co - publishing and their citation impact in different disciplines . Scientometrics , 98 ( 2 ) , 823 - 839 . Qian , Y . , Rong , W . , Jiang , N . , Tang , J . , & Xiong , Z . ( 2017 ) . Citation regression analysis of computer science publications in different ranking categories and subfields . Scientometrics , 110 ( 3 ) , 1351 - 1374 . Rasmussen , P . G . , & Andersen , J . P . ( 2013 ) . Altmetrics : An alternate perspective on research evaluation . Sciecom info , 9 ( 2 ) . https : / / journals . lub . lu . se / index . php / sciecominfo / article / download / 7292 / 6102 Reale , E . , Barbara , A . , & Costantini , A . ( 2007 ) . Peer review for the evaluation of academic research : Lessons from the Italian experience . Research Evaluation , 16 ( 3 ) , 216 – 228 . Rinia , E . J . , van Leeuwen , T . N . , Van Vuren , H . G . , & Van Raan , A . F . ( 1998 ) . Comparative analysis of a set of bibliometric indicators and central peer review criteria : Evaluation of condensed matter physics in the Netherlands . Research Policy , 27 ( 1 ) , 95 - 107 . Rinia , E . J . , van Leeuwen , T . N . , Van Vuren , H . G . , & Van Raan , A . F . J . ( 2001 ) . Influence of interdisciplinarity on peer - review and bibliometric evaluations in physics research . Research Policy , 30 ( 3 ) , 357 - 361 . Rivest , M . , Vignola - Gagné , E . , & Archambault , É . ( 2021 ) . Article - level classification of scientific publications : A comparison of deep learning , direct citation and bibliographic coupling . PloS one , 16 ( 5 ) , e0251493 . Rodríguez - Navarro , A . , & Brito , R . ( 2020 ) . Like - for - like bibliometric substitutes for peer review : Advantages and limits of indicators calculated from the ep index . Research Evaluation , 29 ( 2 ) , 215 - 230 . Roldan - Valadez , E . , & Rios , C . ( 2015 ) . Alternative bibliometrics from impact factor improved the esteem of a journal in a 2 - year - ahead annual - citation calculation : multivariate analysis of gastroenterology and hepatology journals . European Journal of Gastroenterology & Hepatology , 27 ( 2 ) , 115 - 122 . Ronda - Pupo , G . A . ( 2017 ) . The effect of document types and sizes on the scaling relationship between citations and co - authorship patterns in management journals . Scientometrics , 110 ( 3 ) , 1191 - 1207 . 77 Rostami , F . , Mohammadpoorasl , A . , & Hajizadeh , M . ( 2014 ) . The effect of characteristics of title on citation rates of articles . Scientometrics , 98 ( 3 ) , 2007 - 2010 . Rousseau , R . , & Ding , J . ( 2016 ) . Does international collaboration yield a higher citation potential for US scientists publishing in highly visible interdisciplinary Journals ? . Journal of the Association for Information Science and Technology , 67 ( 4 ) , 1009 - 1013 . Rowson , B . , Duma , S . M . , King , M . R . , Efimov , I . , Saterbak , A . , & Chesler , N . C . ( 2021 ) . Citation diversity statement in BMES journals . Annals of biomedical engineering , 49 ( 3 ) , 947 - 949 . Royle , P . , Kandala , N . B . , Barnard , K . , & Waugh , N . ( 2013 ) . Bibliometrics of systematic reviews : analysis of citation rates and journal impact factors . Systematic reviews , 2 ( 1 ) , 1 - 11 . Ruan , X . , Zhu , Y . , Li , J . , & Cheng , Y . ( 2020 ) . Predicting the citation counts of individual papers via a BP neural network . Journal of Informetrics , 14 ( 3 ) , 101039 . Sagi , I . , & Yechiam , E . ( 2008 ) . Amusing titles in scientific journals and article citation . Journal of Information Science , 34 ( 5 ) , 680 - 687 . Sandström , U . , & Hällsten , M . ( 2008 ) . Persistent nepotism in peer - review . Scientometrics , 74 ( 2 ) , 175 - 189 . Seng , L . B . , & Willett , P . ( 1995 ) . The citedness of publications by United Kingdom library schools . Journal of Information Science , 21 ( 1 ) , 68 - 71 . Severin , A . , Strinzel , M . , Egger , M . , Domingo , M . , & Barros , T . ( 2021 ) . Characteristics of scholars who review for predatory and legitimate journals : linkage study of Cabells Scholarly Analytics and Publons data . BMJ open , 11 ( 7 ) , e050270 . Shen , H . , Xie , J . , Li , J . , & Cheng , Y . ( 2021 ) . The correlation between scientific collaboration and citation count at the paper level : a meta - analysis . Scientometrics , 126 ( 4 ) , 3443 - 3470 . Shu , F . , Julien , C . A . , Zhang , L . , Qiu , J . , Zhang , J . , & Larivière , V . ( 2019 ) . Comparing journal and paper level classifications of science . Journal of Informetrics , 13 ( 1 ) , 202 - 225 . Sienkiewicz , J . , & Altmann , E . G . ( 2016 ) . Impact of lexical and sentiment factors on the popularity of scientific papers . Royal Society Open Science , 3 ( 6 ) , 160140 . Sīle , L . , Guns , R . , Vandermoere , F . , Sivertsen , G . , & E ngels , T . C . ( 2021 ) . Tracing the context in disciplinary classifications : A bibliometric pairwise comparison of five classifications of journals in the social sciences and humanities . Quantitative Science Studies , 2 ( 1 ) , 65 - 88 . Siler , K . , Lee , K . , & Bero , L . ( 2015 ) . Measuring the effectiveness of scientific gatekeeping . Proceedings of the National Academy of Sciences , 112 ( 2 ) , 360 - 365 . https : / / www . pnas . org / doi / pdf / 10 . 1073 / pnas . 1418218112 Sin , S . C . J . ( 2011 ) . International coauthorship and citation impact : A bibliometric study of six LIS journals , 1980 – 2008 . Journal of the American Society for Information Science and Technology , 62 ( 9 ) , 1770 - 1783 . Singh , P . , Piryani , R . , Singh , V . K . , & Pinto , D . ( 2020 ) . Revisiting subject classification in academic databases : a comparison of the classification accuracy of Web of Science , Scopus & Dimensions . Journal of Intelligent & Fuzzy Systems , 39 ( 2 ) , 2471 - 2476 . Singh , V . K . , Singh , P . , Karmakar , M . , Leta , J . , & Mayr , P . ( 2021 ) . The journal coverage of Web of Science , Scopus and Dimensions : A comparative analysis . Scientometrics , 126 ( 6 ) , 5113 - 5142 . Sjögårde , P . , & Ahlgren , P . ( 2018 ) . Granularity of algorithmically constructed publication - level classifications of research publications : Identification of topics . Journal of Informetrics , 12 ( 1 ) , 133 - 152 . Sjögårde , P . , & Ahlgren , P . ( 2020 ) . Granularity of algorithmically constructed publication - level classifications of research publications : Identification of specialties . Quantitative Science Studies , 1 ( 1 ) , 207 - 238 . Skorikov , M . , & Momen , S . ( 2020 ) . Machine learning approach to predicting the acceptance of academic papers . In 2020 IEEE International Conference on Industry 4 . 0 , Artificial Intelligence , and Communications Technology ( IAICT ) ( pp . 113 - 117 ) . IEEE . 78 Slyder , J . B . , Stein , B . R . , Sams , B . S . , Walker , D . M . , Jacob Beale , B . , Feldhaus , J . J . , & Copenheaver , C . A . ( 2011 ) . Citation pattern and lifespan : a comparison of discipline , institution , and individual . Scientometrics , 89 ( 3 ) , 955 - 966 . Smith , D . R . ( 2016 ) . Will Publons popularize the scientific peer - review process ? . BioScience , 66 ( 4 ) , 265 - 266 . Smith , M . J . , Weinberger , C . , Bruna , E . M . , & Allesina , S . ( 2014 ) . The scientific impact of nations : Journal placement and citation performance . PloS one , 9 ( 10 ) , e109195 . Smith , Z . L . , Chiang , A . L . , Bowman , D . , & Wallace , M . B . ( 2019 ) . Longitudinal relationship between social media activity and article citations in the journal Gastrointestinal Endoscopy . Gastrointestinal Endoscopy , 90 ( 1 ) , 77 - 83 . Smith , A . & Eysenck , M . ( 2002 ) . The correlation between RAE ratings and citation counts in psychology . June 2002 . http : / / cogprints . org / 2749 / 1 / citations . pdf Sohrabi , B . , & Iraj , H . ( 2017 ) . The effect of keyword repetition in abstract and keyword frequency per journal in predicting citation counts . Scientometrics , 110 ( 1 ) , 243 - 251 . Soler , V . ( 2007 ) . Writing titles in science : An exploratory study . English for Specific Purposes , 26 ( 1 ) , 90 - 102 . Sooryamoorthy , R . ( 2009 ) . Do types of collaboration change citation ? Collaboration and citation patterns of South African science publications . Scientometrics , 81 ( 1 ) , 177 - 193 . Stremersch , S . , Verniers , I . , & Verhoef , P . C . ( 2007 ) . The quest for citations : Drivers of article impact . Journal of Marketing , 71 ( 3 ) , 171 - 193 . Subotic , S . , & Mukherjee , B . ( 2014 ) . Short and amusing : The relationship between title characteristics , downloads , and citations in psychology articles . Journal of information Science , 40 ( 1 ) , 115 - 124 . Sud , P . , & Thelwall , M . ( 2016 ) . Not all international collaboration is beneficial : The Mendeley readership and citation impact of biochemical research collaboration . Journal of the Association for Information Science and Technology , 67 ( 8 ) , 1849 - 1857 . Sugimoto , C . R . , Work , S . , Larivière , V . , & Haustein , S . ( 2017 ) . Scholarly use of social media and altmetrics : A review of the literature . Journal of the Association for Information Science and Technology , 68 ( 9 ) , 2037 - 2062 . Taylor , J . ( 2011 ) . The assessment of research quality in UK universities : Peer review or metrics ? . British Journal of Management , 22 ( 2 ) , 202 - 217 . Tedford , A . ( 2015 ) . Rolling out our new editorial system : EVISE® : Find out how the new system will help reviewers streamline their workload . Elsevier , https : / / www . elsevier . com / connect / reviewers - update / rolling - out - our - new - editorial - system - evise Teixeira da Silva , J . A . ( 2020 ) . Are negative reviews , predatory reviewers or failed peer review rewarded at Publons . International Orthopaedics , 44 ( 10 ) , 2193 - 2194 . Teixeira da Silva , J . A . , & Al - Khatib , A . ( 2019 ) . The ClarivateTM Analytics acquisition of Publons – an evolution or commodification of peer review ? . Research Ethics , 15 ( 3 - 4 ) , 1 - 11 . Thelwall , M . ( 2017a ) . Three practical field normalised alternative indicator formulae for research evaluation . Journal of Informetrics , 11 ( 1 ) , 128 – 151 . 10 . 1016 / j . joi . 2016 . 12 . 002 Thelwall , M . ( 2017b ) . Are Mendeley reader counts useful impact indicators in all fields ? . Scientometrics , 113 ( 3 ) , 1721 - 1731 . Thelwall , M . ( 2017c ) . Avoiding obscure topics and generalising findings produces higher impact research . Scientometrics , 110 ( 1 ) , 307 - 320 . Thelwall , M . ( 2018a ) . Gender bias in machine learning for sentiment analysis , Online Information Review , 42 ( 3 ) , 343 - 354 . https : / / doi . org / 10 . 1108 / OIR - 05 - 2017 - 0152 Thelwall , M . ( 2018b ) . Dimensions : A competitor to Scopus and the Web of Science ? Journal of Informetrics , 12 ( 2 ) , 430 - 435 . Thelwall , M . ( 2018c ) . Early Mendeley readers correlate with later citation counts . Scientometrics , 115 ( 3 ) , 1231 - 1240 . 79 Thelwall , M . ( 2019 ) . Artificial intelligence , automation and peer review . Bristol : JISC . https : / / repository . jisc . ac . uk / id / eprint / 7614 Thelwall , M . ( 2020 ) . Female citation impact superiority 1996 – 2018 in six out of seven English‐ speaking nations . Journal of the Association for Information Science and Technology , 71 ( 8 ) , 979 - 990 . Thelwall , M . ( 2022a ) Journal and disciplinary variations in academic open peer review anonymity , outcomes , and length . Journal of Librarianship and Information Science . https : / / doi . org / 10 . 1177 / 09610006221079345 Thelwall , M . ( 2022b ) . Can the quality of published academic journal articles be assessed with machine learning ? . Quantitative Science Studies , 3 ( 1 ) , 208 - 226 . Thelwall , M . , & Kousha , K . ( 2015a ) . Web indicators for research evaluation . Part 1 : Citations and links to academic articles from the Web . Profesional de la Información , 24 ( 5 ) , 587 - 606 . Thelwall , M . , & Kousha , K . ( 2015b ) . Web indicators for research evaluation . Part 2 : Social media metrics . Profesional de la Informacion , 24 ( 5 ) , 607 - 620 . Thelwall , M . , & Kousha , K . ( 2016 ) . Are citations from clinical trials evidence of higher impact research , ? An analysis of ClinicalTrials . gov . Scientometrics , 109 ( 2 ) , 1341 - 1351 . Thelwall , M . , & Maflahi , N . ( 2015 ) . Are scholarly articles disproportionately read in their own country ? An analysis of Mendeley readers . Journal of the Association for Information Science and Technology , 66 ( 6 ) , 1124 - 1135 . Thelwall , M . , & Maflahi , N . ( 2016 ) . Guideline references and academic citations as evidence of the clinical value of health research . Journal of the Association for Information Science and Technology , 67 ( 4 ) , 960 - 966 . Thelwall , M . , & Maflahi , N . ( 2020 ) . Academic collaboration rates and citation associations vary substantially between countries and fields . Journal of the Association for Information Science and Technology , 71 ( 8 ) , 968 - 978 . Thelwall , M . , & Nevill , T . ( 2018 ) . Could scientists use Altmetric . com scores to predict longer term citation counts ? . Journal of Informetrics , 12 ( 1 ) , 237 - 248 . Thelwall , M . , & Nevill , T . ( 2021 ) . Is research with qualitative data more prevalent and impactful now ? Interviews , case studies , focus groups and ethnographies . Library & Information Science Research , 43 ( 2 ) , paper 101094 . https : / / doi . org / 10 . 1016 / j . lisr . 2021 . 101094Thelwall , M . , & Sud , P . ( 2014 ) . No citation advantage for monograph - based collaborations ? . Journal of Informetrics , 8 ( 1 ) , 276 - 283 . Thelwall , M . , & Sud , P . ( 2022 ) . Scopus 1900 - 2020 : Growth in articles , abstracts , countries , fields , and journals . Quantitative Science Studies , 3 ( 1 ) , 37 - 50 . Thelwall , M . , Haustein , S . , Larivière , V . , & Sugimoto , C . R . ( 2013 ) . Do altmetrics work ? Twitter and ten other social web services . PloS one , 8 ( 5 ) , e64841 . Thelwall , M . , Papas , E . R . , Nyakoojo , Z . , Allen , L . , & Weigert , V . ( 2020 ) . Automatically detecting open academic review praise and criticism . Online Information Review , 44 ( 5 ) , 1057 - 1076 . Thomas , P . , & Watkins , D . ( 1998 ) . Institutional research rankings via bibliometric analysis and direct peer review : A comparative case study with policy implications . Scientometrics , 41 ( 3 ) , 335 - 355 . Tincani , M . , & Travers , J . ( 2019 ) . Replication research , publication bias , and applied behavior analysis . Perspectives on Behavior Science , 42 ( 1 ) , 59 - 75 . Tran , D . , Valtchanov , A . , Ganapathy , K . , Feng , R . , Slud , E . , Goldblum , M . , & Goldstein , T . ( 2020 ) . An open review of openreview : A critical analysis of the machine learning conference review process . arXiv preprint arXiv : 2010 . 05137 . Trowler , P . ( 2014 ) . Academic Tribes and Territories : the theoretical trajectory . Österreichische Zeitschrift für Geschichtswissenschaften , 25 ( 3 ) , 17 - 26 . Urlings , M . J . , Duyx , B . , Swaen , G . M . , Bouter , L . M . , & Zeegers , M . P . ( 2021 ) . Citation bias and other determinants of citation in biomedical research : findings from six citation networks . Journal of Clinical Epidemiology , 132 , 71 - 78 . 80 van Dalen , H . P . , & Henkens , K . N . ( 2005 ) . Signals in science - On the importance of signaling in gaining attention in science . Scientometrics , 64 ( 2 ) , 209 - 233 . van Eck , N . J . , & Waltman , L . ( 2017 ) . Citation - based clustering of publications using CitNetExplorer and VOSviewer . Scientometrics , 111 ( 2 ) , 1053 - 1070 . van Eck , N . J . , & Waltman , L . ( 2019 ) . Accuracy of citation data in Web of Science and Scopus . arXiv preprint arXiv : 1906 . 07011 . van Eck , N . J . , Waltman , L . , van Raan , A . F . , Klautz , R . J . , & Peul , W . C . ( 2013 ) . Citation analysis may severely underestimate the impact of clinical research as compared to basic research . PloS one , 8 ( 4 ) , e62395 . van Leeuwen , T . N . , & Calero Medina , C . ( 2012 ) . Redefining the field of economics : Improving field normalization for the application of bibliometric techniques in the field of economics . Research Evaluation , 21 ( 1 ) , 61 - 70 . van Leeuwen , T . , Moed , H . , Tijssen , R . , Visser , M . , & Van Raan , A . ( 2001 ) . Language biases in the coverage of the Science Citation Index and its consequences for international comparisons of national research performance . Scientometrics , 51 ( 1 ) , 335 - 346 . Van Raan , A . ( 1998 ) . The influence of international collaboration on the impact of research results : Some simple mathematical considerations concerning the role of self - citations . Scientometrics , 42 ( 3 ) , 423 - 428 . Van Raan , A . F . ( 2006 ) . Comparison of the Hirsch - index with standard bibliometric indicators and with peer judgment for 147 chemistry research groups . Scientometrics , 67 ( 3 ) , 491 - 502 . Van Wesel , M . , Wyatt , S . , & ten Haaf , J . ( 2014 ) . What a difference a colon makes : How superficial factors influence subsequent citation . Scientometrics , 98 ( 3 ) , 1601 - 1615 . Vanclay , J . K . ( 2013 ) . Factors affecting citation rates in environmental science . Journal of Informetrics , 7 ( 2 ) , 265 - 271 . Vieira , E . S . , & Gomes , J . A . N . F . ( 2010 ) . Citation to scientific articles : Its distribution and dependence on the article features . Journal of Informetrics , 4 ( 1 ) , 1 – 13 . Vilone , G . , & Longo , L . ( 2020 ) . Explainable artificial intelligence : a systematic review . arXiv preprint arXiv : 2006 . 00093 . Visser , M . , van Eck , N . J . , & Waltman , L . ( 2021 ) . Large - scale comparison of bibliographic data sources : Scopus , Web of Science , Dimensions , Crossref , and Microsoft Academic . Quantitative Science Studies , 2 ( 1 ) , 20 - 41 . Wagner , C . S . , Whetsell , T . A . , & Mukherjee , S . ( 2019 ) . International research collaboration : Novelty , conventionality , and atypicality in knowledge recombination . Research Policy , 48 ( 5 ) , 1260 - 1270 . Waltman , L . , & Costas , R . ( 2014 ) . F 1000 Recommendations as a potential new data source for research evaluation : A comparison with citations . Journal of the Association for Information Science and Technology , 65 ( 3 ) , 433 - 445 . Waltman , L . , & v an Eck , N . J . ( 2012 ) . A new methodology for constructing a publication‐level classification system of science . Journal of the American Society for Information Science and Technology , 63 ( 12 ) , 2378 - 2392 . Waltman , L . , & van Eck , N . J . ( 2019 ) . Field normalization of scientometric indicators . In Springer handbook of science and technology indicators ( pp . 281 - 300 ) . Springer , Cham . Wang , D . , Liang , Y . , Xu , D . , Feng , X . , & Guan , R . ( 2018 ) . A content - based recommender system for computer science publications . Knowledge - Based Systems , 157 , 1 - 9 . Wang , J . , Veugelers , R . , & Stephan , P . ( 2017 ) . Bias against novelty in science : A cautionary tale for users of bibliometric indicators . Research Policy , 46 ( 8 ) , 1416 - 1436 . Wang , K . , & Wan , X . ( 2018 ) . Sentiment analysis of peer review texts for scholarly papers . In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval ( pp . 175 - 184 ) . Wang , L . , Thijs , B . , & Glänzel , W . ( 2015 ) . Characteristics of international collaboration in sport sciences publications and its influence on citation impact . Scientometrics , 105 ( 2 ) , 843 - 862 . 81 Wang , M . , Jiao , S . , Zhang , J . , Zhang , X . , & Zhu , N . ( 2020 ) . Identification high influential articles by considering the topic characteristics of articles . IEEE Access , 8 , 107887 - 107899 . Wang , M . , Yu , G . , & Yu , D . ( 2011 ) . Mining typical features for highly cited papers . Scientometrics , 87 ( 3 ) , 695 - 706 . Wang , M . , Yu , G . , An , S . , & Yu , D . ( 2012 ) . Discovery of factors influencing citation impact based on a soft fuzzy rough set model . Scientometrics , 93 ( 3 ) , 635 - 644 . Wang , P . , & Su , J . ( 2021 ) . Post - publication expert recommendations in faculty opinions ( F1000Prime ) : Recommended articles and citations . Journal of Informetrics , 15 ( 3 ) , 101174 . Wang , Q . , & Sandström , U . ( 2015 ) . Defining the role of cognitive distance in the peer review process with an explorative study of a grant scheme in infection biology . Research Evaluation , 24 ( 3 ) , 271 - 281 . Wang , Q . , & Waltman , L . ( 2016 ) . Large - scale analysis of the accuracy of the journal classification systems of Web of Science and Scopus . Journal of Informetrics , 10 ( 2 ) , 347 - 364 . Wardle , D . A . ( 2010 ) . Do ' Faculty of 1000 ' ( F1000 ) ratings of ecological publications serve as reasonable predictors of their future impact ? . Ideas in Ecology and Evolution , 3 , 11 - 15 . Weale , A . R . , Bailey , M . , & Lear , P . A . ( 2004 ) . The level of non - citation of articles within a journal as a measure of quality : a comparison to the impact factor . BMC Medical Research methodology , 4 ( 1 ) , 1 - 8 . Weinberger , C . J . , Evans , J . A . , & Allesina , S . ( 2015 ) . Ten simple ( empirical ) rules for writing science . PLOS Computational Biology , 11 ( 4 ) , e1004205 . Whitley , R . ( 2000 ) . The intellectual and social organization of the sciences ( 2 ed ) . Oxford , UK : Oxford University Press on Demand . Wilkinson , J . , & Down , P . ( 2018 ) . Publons : Releasing the untapped power of peer review for universities . Insights , 31 : 20 . https : / / insights . uksg . org / article / 10 . 1629 / uksg . 407 / Willis , D . L . , Bahler , C . D . , Neuberger , M . M . , & Dahm , P . ( 2011 ) . Predictors of citations in the urological literature . BJU international , 107 ( 12 ) , 1876 - 1880 . Wilsdon , J . , et al . ( 2015a ) . Metric Tide ( Executive Summary ) : report of the independent review of the role of metrics in research assessment and management . Technical Report . Higher Education Funding Council for England . https : / / www . ukri . org / wp - content / uploads / 2021 / 12 / RE - 151221 - TheMetricTideFullReportExecSummary . pdf Wilsdon , J . , et al . ( 2015b ) . The Metric Tide : Report of the Independent Review of the Role of Metrics in Research Assessment and Management . DOI : 10 . 13140 / RG . 2 . 1 . 4929 . 1363 . https : / / kar . kent . ac . uk / 81123 / 1 / Metric _ Tide _ main _ report . pdf Wolfram , D . , Wang , P . , Hembree , A . , & Park , H . ( 2020 ) . Open peer review : promoting transparency in open science . Scientometrics , 125 ( 2 ) , 1033 - 1051 . Wooldridge , J . , & King , M . B . ( 2019 ) . Altmetric scores : An early indicator of research impact . Journal of the Association for Information Science and Technology , 70 ( 3 ) , 271 - 282 . Wuchty , S . , Jones , B . F . , & Uzzi , B . ( 2007 ) . The increasing dominance of teams in production of knowledge . Science , 316 ( 5827 ) , 1036 - 1039 . Xiao , L . , & Jiang , W . ( 2020 ) . Correlation between references and citations in artificial intelligence : A preliminary study . Proceedings of the Association for Information Science and Technology , 57 ( 1 ) , e403 . Xie , J . , Gong , K . , Cheng , Y . , & Ke , Q . ( 2019 ) . The correlation between paper length and citations : a meta - analysis . Scientometrics , 118 ( 3 ) , 763 - 786 . Xu , F . , Uszkoreit , H . , Du , Y . , Fan , W . , Zhao , D . , & Zhu , J . ( 2019 ) . Explainable AI : A brief survey on history , research areas , approaches and challenges . In CCF international conference on natural language processing and Chinese computing ( pp . 563 - 574 ) . Berlin , Germany : Springer . Xu , J . , Li , M . , Jiang , J . , Ge , B . , & Cai , M . ( 2019 ) . Early prediction of scientific impact based on multi - bibliographic features and convolutional neural network . IEEE Access , 7 , 92248 - 92258 . 82 Yu , T . , Yu , G . , Li , P . Y . , & Wang , L . ( 2014 ) . Citation impact prediction for scientific papers using stepwise regression analysis . Scientometrics , 101 ( 2 ) , 1233 - 1252 . Yuan , W . , Liu , P . , & Neubig , G . ( 2021 ) . Can we automate scientific reviewing ? arXiv preprint arXiv : 2102 . 00176 . Zhang , L . , Shang , Y . , Huang , Y . , & Sivertsen , G . ( 2022a ) . Gender differences among active reviewers : an investigation based on Publons . Scientometrics , 127 ( 1 ) , 145 – 179 . Zhang , L . , Sun , B . , Shu , F . , & Huang , Y . ( 2022b ) . Comparing paper level classifications across different methods and systems : an investigation of Nature publications . Scientometrics , 1 - 19 . https : / / doi . org / 10 . 1007 / s11192 - 022 - 04352 - 3 Zhao , Q . , & Feng , X . ( 2022 ) . Utilizing citation network structure to predict paper citation counts : A Deep learning approach . Journal of Informetrics , 16 ( 1 ) , 101235 . ZhengWei , H . , JinTao , M . , YanNi , Y . , Jin , H . , & Ye , T . ( 2022 ) . Recommendation method for academic journal submission based on doc2vec and XGBoost . Scientometrics , 127 , 2381 – 2394 . Zong , Q . , Fan , L . , Xie , Y . , & Huang , J . ( 2020 ) . The relationship of polarity of post - publication peer review to citation count : Evidence from Publons . Online Information Review , 44 ( 3 ) , 583 - 602 .