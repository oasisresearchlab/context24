Principles and Tools for Collaborative Entity - Based Intelligence Analysis Eric A . Bier , Stuart K . Card , and John W . Bodnar Abstract —Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives . We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information ( including evidence , schemas , and hypotheses ) . We have modified the Entity Workspace system , described previously , to test such designs . We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team . In both cases , effects on collaboration appear to be positive . Key aspects of the design include an evidence notebook optimized for organizing entities ( rather than text characters ) , information structures that can be collapsed and expanded , visualization of evidence that emphasizes events and documents ( rather than emphasizing the entity graph ) , and a notification system that finds entities of mutual interest to multiple analysts . Long - term tests suggest that this approach can support both top - down and bottom - up styles of analysis . Index Terms —User interfaces , graphical user interfaces ( GUI ) , information search and retrieval , information filtering , information systems applications , miscellaneous , group and organization interfaces , collaborative computing , computer - supported cooperative work , Web - based interaction . Ç 1 I NTRODUCTION C OLLABORATION is an essential component of intelligence analysis and other forms of knowledge work . In intelligence analysis in particular , essential knowledge of evidence and where to find it lives in the minds of other analysts and other experts . As a result , as new software tools for analysts are being designed , it is important to consider the extent to which those tools will support collaboration . In addition , while it is possible to support some forms of collaboration with general - purpose collaboration tools , such as e - mail , instant messaging , and video conferencing , it is becoming clear that there is a need for analytical tools that integrate collaboration more directly into the analytical process . In this paper , we describe a collaborative analytical tool based on the idea that collaboration is made easier if it is organized around entities ( people , places , things , times , etc . ) rather than free - form text . There are several reasons that integrated collaboration is needed . In the first place , stand - alone collaboration tools can interrupt the process of analysis by drawing attention away from analytical tools ; interruptions from knowledge work can reduce productivity . In the second place , the need for collaboration may become apparent as a direct result of the analytical work ; for example , the need for another opinion or additional information may flow directly out of the process of organizing evidence , or evaluating evidence against hypotheses . Finally , the analyst may not know at first which experts are available on a particular topic or which other analysts are working on a similar problem . In this case , the information in the analytic tools themselves may be just the input software tools need to recommend people or information sources to consult next . We are studying software tools that support intelligence analysis and other forms of sensemaking around informa - tion . Our approach to analytical processes is based on a model for all - source analysis , which we term the Think Loop Model [ 7 ] , [ 8 ] , [ 14 ] , [ 17 ] ( see Fig . 1 ) . An extension of this model to better represent the possible information flows in collaborative analysis will be described below . Both are notional models that describe the process often seen in such analysis : External data are filtered and collected into a “shoebox” of saved documents . These are processed to build up nuggets of evidence . Either in the analyst’s head or with some external aid , these nuggets of evidence are organized according to some schematized conceptual structures , such as timelines , maps , or organization charts . Hypotheses are formed and eventually translated into a report or briefing . Taken as a whole , these processes constitute two larger activities : foraging for information [ 15 ] ( see the Foraging Loop in the lower left of Fig . 1 ) and making sense of it ( see the Sensemaking Loop in the upper right of Fig . 1 ) . The process is not linear , but can proceed top - down or bottom - up and with many loops . The systems we have built are intended to support both the information foraging and sensemaking parts of analysis . Our CorpusView document collection tool [ 4 ] is an electronic version of a shoebox and the Entity Workspace evidence notebook [ 3 ] , [ 5 ] , [ 6 ] supports the evidence , schema , and hypothesis stage of the process . Fig . 2 provides an overview of both tools . Analysts often work the process in Fig . 1 by chaining through entities . A name may lead to an organization , 178 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 . E . A . Bier and S . K . Card are with the Palo Alto Research Center , Inc . , Palo Alto , CA 94304 . E - mail : { bier , card } @ parc . com . . J . W . Bodnar is with Science Applications International Corporation , 1710 SAIC Drive , McLean , VA 22102 . E - mail : john . w . bodnar @ saic . com . Manuscript received 13 Feb . 2009 ; revised 16 Apr . 2009 ; accepted 16 July 2009 ; published online 20 Aug . 2009 . Recommended for acceptance by T . Ertl . For information on obtaining reprints of this article , please send e - mail to : tvcg @ computer . org , and reference IEEECS Log Number TVCGSI - 2009 - 02 - 0031 . Digital Object Identifier no . 10 . 1109 / TVCG . 2009 . 104 . 1077 - 2626 / 10 / $ 26 . 00 (cid:2) 2010 IEEE Published by the IEEE Computer Society Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . which , in turn , may lead to a project and another name . Discovering , prioritizing , and keeping track of a growing set of entities is one of the difficulties of analysis . Entity Workspace helps by allowing the analyst to extract entities from text rapidly and to assemble these together into higher level statements . Entities are a natural information unit to use in collaboration , so we have extended our entity - based tools , originally designed to support the individual analyst , to support collaborative situations as well . As a result of this work , we have developed a set of design guidelines for collaborative analysis tools as follows : 1 . Entities . Analysts think of people , places , things , and their relationships . Make it easy to view and manipulate information at the level of entities ( people , places , and things ) and their relationships rather than just text . Create features that reduce the cost of processing entities both for the writer and for the reader of them . 2 . Collapsing Information . Analysts sometimes study a few broad topics and sometimes a single narrow topic . The space available for manipulating informa - tion is extremely limited physically , perceptually , and cognitively . Make it possible to collapse in - formation so that it is still visible , but takes up less space . Make it easy to find and refind all mentions of a topic , even in collapsed information . Support both top - down and bottom - up processes . 3 . Organizing by Stories . Analysts ultimately “tell stor - ies” in their presentations . Provide a way to organize evidence by events and source documents so that the story behind the evidence can be represented . 4 . Quiet Collaborative Entity Recommendations . Too much data on the screen are confusing . “Help” must always be available for the analyst but should only be salient when needed . When notifying analysts of the work of other experts , use subtle visual cues in the analytic tools themselves , so the analysts are not interrupted and see the cues at the time when they are most relevant . 5 . Sharing . Reaching a consensus on an analytical task virtually always requires all the analysts involved to be confident that they have read all the supporting reports or documents . Support the sharing of document collections . Based on these guidelines , we built a set of novel collaboration features into our prototype analytic tools . We tested the first prototype on a group of four intelligence analysts at a three - day formative evaluation at NIST in March 2007 . The results of that evaluation were very positive , particularly for support of collaboration . Based on suggestions from that evaluation , we extended our colla - boration facilities further . The resulting prototype is now being tested in a longer term trial by a group of analysts at SAIC . Early results from that evaluation are promising . In the remainder of the paper , we describe the features of CorpusView and Entity Workspace that support our design guidelines . Then , we describe what we have learned about collaboration tools for analysis from our two evaluations . Then , we discuss our design principles further and speculate on how they may support other forms of collaboration . 2 C OLLABORATION T ECHNOLOGIES 2 . 1 Entities in Notebooks In this section , we describe our extended versions of CorpusView and Entity Workspace , focusing on the five design guidelines for collaboration that we described above . Analysts think of people , places , things , and their relation - ships . Our first guideline relates to the granularity at which information is viewed and manipulated . Consider this sentence : Oluf Zabani is the owner of the Persian Design Carpets shop in Meszi , Ugakostan . If analyst A saw this sentence in a text - based notebook created by analyst B , the work of understanding and using the information in the sentence would fall almost exclu - sively on A . Based on capitalization and other cues , A could BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 179 Fig . 1 . The Think Loop Model of all - source analysis . Fig . 2 . ( a ) CorpusView and ( b ) Entity Workspace . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . guess that Oluf Zabani is the name of a person , that Persian Design Carpets is the name of a shop , and that Meszi is the name of a city or region in Ugakostan ( a fictional country ) . Likewise , if analyst A wanted to make use of the information in this sentence , she wouldn’t get much help . To copy the sentence or one of the entities mentioned , she would have to select the desired sequence of characters and use a copy - and - paste operation . To learn about any known nick names of Oluf Zabani , she would have to search for them or look them up in a database . To find out what other people are connected with him , she might search for articles that mention him one by one and collect other names that appear in those articles . In short , even though she has access to this notebook from analyst B , making use of its information is manual and time - consuming . In our view , the problem with the received notebook is that it has no structuring at the entity level . Now consider the alternate view of the same sentence shown in Fig . 3 . This sentence was constructed in Entity Workspace using our snap - together knowledge technique . An analyst viewing the sentence this way sees the text plus several kinds of additional information . First , entity names are contained in a box , to show clearly where the text of each name begins and ends . Text between entities is also enclosed in a box . Each entity box also contains an icon at the left end that indicates the entity type . In this example , a circle indicates a person and a house shape indicates an organization or place . Color and font weight show how important the author thinks each entity is ; bold red entities are highly rated . Background color shows selection ; the “Oluf Zabani” entity is selected in this example . To make entities easy to import , the system runs an entity extractor over input documents and highlights entities that are found . Entities can then be imported using the entity quick click technique , described previously [ 3 ] . Given our emphasis on entities and entity - based ap - proaches , we should define carefully what we mean by an entity . From the conceptual point of view , an entity is a person , place , organization , address , phone number , docu - ment , or some other thing in the real world . However , from a representational point of view , an entity is a phrase in a natural language , such as “John Smith , ” together with : . Alternate phrases , spellings , nicknames , or aliases , such as “Jack Smith , ” “Smith , John , ” “J . R . Smith , ” or “Little John . ” . An associated type or types , such as “person . ” . A degree of interest rating , which captures how important , interesting , or relevant this entity is to a particular analyst . We use a number , such as “3 out of 3 . ” . A network of links to other entities , such as “John Smith works with Robert Johnson . ” Entity Workspace automatically derives a graph of entity relationships , beginning with the analyst’s notes as shown in Fig . 4 . Entities that are placed together in the same evidence group are connected by an edge . Entities that are together in the same belief statement are connected by a stronger ( more heavily weighted ) edge . Entities that are so closely related that the analyst has specified a coupling relationship between them ( e . g . , an analyst might couple a person’s phone number to the person’s name ) are con - nected by an even stronger edge , and so on . This graph is not ( necessarily ) displayed to the analyst , but is used behind the scenes to enhance the user interface of the notebook . This emphasis on entities in the workspace and their snap - together composition distinguishes Entity Workspace from other knowledge workspaces . nSpace [ 12 ] , [ 18 ] uses entities to organize search results in its TRIST information foraging tool and in its Sandbox workspace , but does not provide a snap - together way to capture entity relationships . Jigsaw [ 16 ] focuses on representing connections between pairs of entities through its graph and matrix organizations , whereas Entity Workspace can represent more complex and loosely specified structures . Analyst Notebook [ 10 ] provides multiple ways to manipulate and display links between entities , where Entity Workspace groups entities and manipulates relationships among higher level entity orga - nizations . All of these systems attempt in their own ways to reduce the cost of manipulating text into organized insight . In addition to providing an enhanced display of note - book material , the snap - together knowledge notebook also differs from standard text editing in the kind of interaction that it affords . In particular , a single click anywhere on an entity selects the entire phrase that represents it ( so drag selection is not needed ) . Selection automatically causes highlighting of all instances of that entity in the notebook and all evidence groups that contain them ( without the need for an explicit search operation ) to make it easy to see all of the contexts in which that entity appears . For example , in Fig . 5 , the analyst has selected the entity “Oluf Zabani” ; Entity Workspace responds by highlighting all of the evidence groups in that notebook that mention that entity and also all instances of that entity and closely related entities , such as Oluf Zabani’s phone number . In addition , an entity can be copied or moved by clicking and dragging , without the need to select it first . The analyst can also select two entities in the notebook , in which case , special highlighting is used to show 180 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 3 . Snap - together knowledge belief statement . Fig . 4 . Automatic graph extraction from note - taking . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . paragraphs that mention both entities . In Entity Workspace , all evidence associated only with the first entity is high - lighted in orange , all evidence associated only with the second entity is highlighted in magenta , and all evidence associated with both entities is highlighted in green . ( We are designing a different technique for use by color - blind users . ) In the case that no evidence directly connects two entities with each other , the analyst can ask for a display of the shortest path between two entities . Fig . 6 shows an example of the shortest path display . Here , the analyst has selected entities representing two people and the system has found a connection between them via a phone number . In Fig . 6 , Entity Workspace dims most of the evidence in order to highlight the evidence that is relevant to the shortest path . Entity - based notebooks can also be used to support a smarter form of search . For example , using Entity Work - space , an analyst can query a document collection by selecting any entity ( or entities ) in the collection and pressing a search button . Not only is this a very quick way to make a query ( without any typing ) but also automatic query expansion is used , so the analyst is more likely to get the desired results . Two kinds of query expansion are used . First , if any aliases , nicknames , or alternate names are available for the entity , these are added to the query . Next , any abbreviations or rearrangements of the name , appropriate to the type of the entity , are also added to the query . Special handling for Asian names also tests for variations of names with and without hyphens and apostrophes . The result is a form of search that often requires no typing at all , while finding more relevant documents than would be found by a short manual query . While the use of entities in notebooks was originally developedtohelpindividualanalystsreviewtheirownnotes , we have found that it can also help analysts more quickly understand and reorganize information authored by some - one else , because it looks enough like an ordinary sentence to be easy to read butis organized into entities so that it is easy to reorganize and interact with at the entity granularity . 2 . 2 Collapsing Information Analysts sometimes study a few broad topics and sometimes a single narrow topic . Our second guideline relates to collapsing material , when desired , so that it takes up less screen space . In Entity Workspace , entities and comments can be combined into sentence - like structures called belief state - ments , like the one shown in Fig . 3 . Belief statements allow analysts to express hypotheses . Belief statements , in turn , can be gathered into paragraph - like structures called evidence groups . Evidence groups , in turn , can be gathered into section - like structures called supergroups . Supergroups can be nested to any level . Any group or supergroup in the resulting hierarchical structure can be collapsed to hide all of its nested contents , or expanded to reveal it again . The ability to collapse and expand information is important for individual analysts because it helps them manage their screen space and helps them focus on those details that are currently of interest while hiding others that would be distracting . We believe that collapsing structures can be important in collaboration as well for similar reasons . In particular , they allow the authoring analyst to focus the attention of others and to organize material better within the confines of screen space . However , collapsing structures do intro - duce the danger that some important material will be missed by the eventual readers of a notebook , if hidden when the notebook is received . To mitigate this concern , our collapsing structures use several semantic zooming [ 2 ] mechanisms to help the reader determine when to expand them again . The first is summarization . As shown in Fig . 7a , the two collapsed BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 181 Fig . 5 . An entity workspace notebook . Fig . 6 . Highlighting the shortest path between two entities . Fig . 7 . Expansion guided by selection highlighting . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . groups ( short wide rectangles ) display a textual summary of everything that they contain . For example , the lower one mentions “Oluf Zabani , ” “FBI Report 02 , ” and other information . This gives the reader information scent [ 13 ] as to what is inside of them . The second mechanism is highlight - guided drill - down . The ideahereistohighlightallgroupsinahierarchythatcontainaselectedentity . For example , in Fig . 7a , the reader has become interested in a person named “Heydar Mosei” and has selected his name . All groups containing this entity ( at any level ) are displayed with an orange background . If a reader clicks the “ + ” sign on the orange supergroup in Fig . 7a , that supergroup expands to reveal four child objects in Fig . 7b . Onlyoneofthesechildobjectsishighlightedinorange ( itselfa collapsed group ) . ThereaderexpandsthatgrouptogetFig . 7c and then reads the group to learn about the entity of interest . We have also found collapsing to be useful when one analyst is copying material from another analyst’s notebook . If many pieces of evidence are to be copied , they can often be collapsed , copied , and then expanded again once they reach the new notebook . This allows an analyst to copy another analyst’s work either in terms of broad categories ( supergroups ) or in terms of specific evidence on a single narrow topic ( evidence groups or belief statements ) . 2 . 3 Organizing by Stories and Documents Analysts ultimately “tell stories” in their presentations . The snap - together structures that an analyst builds in Entity Workspace contain enough information to allow it to construct an entity graph structure “behind the scenes . ” In the graph , two entities are connected by an edge if instances of those entities appear together in a belief statement or evidence group in the workspace ( or if the analyst has indicated that these two entities are so closely related that they should be considered to be coupled ) . Many authors have already written about on - screen presentation of graphs to support their exploration , such as i2’s Analyst’s Notebook [ 10 ] . Indeed , such visualizations can be used side by side with the snap - together notebook system described here ; Fig . 8 shows the BBN KineViz system visualization for an entity graph from an Entity Workspace notebook . Graph visualizations have a number of advantages : each entity appears only once , and good graph visualizations can make it easy to see all of the members of a group at once . However , we decided not to use such visualizations as the primary presentation of evidence in our system for two reasons . In the first place , once the amount of evidence becomes large , such a graph will have many edges , making it hard to see relationships in a presentation that resembles spaghetti . More important from an analyst’s perspective , the notebook presentation style allows the analyst to organize evidence into self - contained story pieces . This is possible in part because any entity can appear any number of times in the notebook so that the entity is always seen in the context of a story line . Each appearance of the entity may relate it to a different set of entities and each instance may be contributed by a different source document . Fig . 9 shows Entity Workspace notes that have been organized temporally in order to display a story that is suggested by the evidence . Consider the sentence “Oluf Zabani is the owner of the Persian Design Carpets shop in Meszi , Ugakostan . ” Some - times the analyst is interested in the shop because : . It is owned by Oluf Zabani and she wants to follow his contacts in other businesses or organizations . . It is owned by Oluf Zabani and she wants to investigate the other shop employees or share - holders . . It is in Meszi , Ugakostan , and she wants to follow its links to Abala , Karjakistan , right across the river . The analyst separates all these story lines in her mind and will be confused if all the links from the “Persian Design Carpets shop” are shown on the same link chart . Being able to organize evidence into a story is important for individual analysts , but even more important for sharing collaborative information . Indeed , we believe that analysts want to share both graph information ( “tell me what you 182 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 8 . BBN KineViz 2 visualizing the entities in a notebook . ( KineViz 2 software courtesy of R . Bobrow , BBN Technologies ) Fig . 9 . Using notes to tell stories . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . know about the social network of this individual” ) and story - or source - oriented information ( “what events lead you to believe this person has purchased explosives ? ” or “what document makes you believe this person works for this company ? ” ) . A notebook style of information makes this second kind of information easy to share . Also , a notebook that allows entity graphs to be included as one kind of “evidence” will allow both kinds of information to be shared in a format that is as close to a “story” as possible . In Entity Workspace , we are working to support both kinds of sharing . 2 . 4 Quiet Collaborative Entity Recommendations Too much data on the screen is confusing . “Help” must always be available for the analyst but should only be salient when needed . The entity graph computed by Entity Workspace from each notebook can become even more valuable once multiple analysts have produced multiple notebooks . In that case , we can compute an entity graph from each notebook . Each of these graphs encapsulates one analyst’s perspective on a set of entities , how important they are , and how they are related to each other . If two analysts are studying related topics , it is likely that entity graphs from those analysts will share common entities . In this case , we can combine information from the two entity graphs to recommend to each analyst which notes from the other analyst are likely to be most valuable . For example , simply finding the common entities between graphs allows Entity Workspace to notify one analyst that another analyst is working on the same entities . In addition , if we compare outgoing links from the common entities , we may discover that a remote notebook mentions relation - ships not yet contained in our notebook . These relationships may represent new facts of interest . We may wish to particularly highlight entities that participate in new facts . Other results can be achieved by combining two ( or more ) graphs into one larger graph ( Fig . 10 ) . This graph represents combined knowledge from multiple notebooks ( and multiple analysts if the notebooks have different authors ) . The degree of interest rating of each entity in the final graph can be computed from the ratings in the initial graphs . The system can be used to flag differences , such as entities that are marked as high interest in notebook A , but have a lower rating in notebook B . Such entities may deserve more attention from the author of B . For example , the analyst who created the leftmost graph of Fig . 10 may discover the new fact that entity C is related to a ( highly rated ) entity D not mentioned in his notebook and may also be interested that entity A is rated more highly in another analyst’s entity graph . Entity Workspace uses a spreading activation technique [ 1 ] to compute a degree of interest rating for all entities in an entity graph , including those that have not been manually rated by the analyst . It gives entities points if they are connected in the graph to highly rated entities and generally higher scores if they are connected to more such entities . The process iterates so that entities may be given a higher rating even if they are several edges away from an entity that was explicitly rated highly . This technique can help an individual analyst decide which entities to focus on next . We anticipate that this technique will be even more effective when used on the composite graph constructed from several notebooks . In designing Entity Workspace , we wanted any notifica - tions about information available in other notebooks to be displayed in a reasonably subtle manner , so that it would be clear but not distracting . We decided to add a flag - shaped icon to the entity representations for each entity for which additional information is available in a remote notebook . Entities highlighted in this way look as is shown in Fig . 11 . In this example , an orange triangle appears near the left end of three entities . When an analyst is working on this section of the notebook , she is probably interested in the entities mentioned there , so she will learn about the availability of additional information about these entities exactly at the moment when that information is most likely to be of interest . In addition to being notifiers , the orange flags are buttons . When the analyst clicks on an orange flag displayed next to a particular entity , the system displays a menu describing additional information available about that entity in remote notebooks and ways that the information can be displayed . For example , the analyst may have the option to see the entire remote notebook or to see a filtered version of it showing only the remote notes relevant to the selected entity . After making a selection , the analyst can view the remote notes and import them into the local notebook using drag - and - drop ( Fig . 12 ) . As shown in the figure , a window onto the remote notebook appears in the local notebook . Drag - and - drop operations can then be used to copy notes into the local notebook at any desired granularity from copying a single entity to copying an entire supergroup in a single operation . BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 183 Fig . 10 . Joining two graphs into one . Fig . 11 . Flag - shaped icons . Fig . 12 . Importing remote notes . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . 2 . 5 Sharing Document Collections Reaching a consensus on an analytical task virtually always requires all the analysts involved to be confident that they have read all the supporting reports or documents . While our research has largely focused on the evidence notebook as a vehicle for collaborative analysis , we have also discovered value in allowing one analyst to share his / her private document collection with team members . Indeed , working groups of analysts often stop quibbling over assessments after a meeting in which they all bring their source documents and pass them around the table so that everyone on the team can read all the available data . Many technologies already exist for sharing document collections . However , we noticed that several capabilities of our CorpusView system were particularly valuable in the context of collaborative analysis . First , each document placed in a collection is given a URL on the local network . This doesn’t immediately make the documents shared , as the collection is password protected , but it makes it possible to name the documents in a persistent way . These URLs are then placed in the Entity Workspace notebooks . As a result , an analyst receiving a notebook ( and a collection password ) from another analyst will be able to follow links in the notebook to the source documents used to make it . Second , CorpusView allows the analyst to organize documents into categories as shown in Fig . 13 . The analyst can define new category schemes as needed . Fig . 13 shows an organization according to the institution that produced each document . When the collection is shared , so are the categories , making it possible for the receiving analyst to learn the categorization technique of the sending analyst . Finally , CorpusView produces its own index of the documents in the collection , making it easy for the receiving analyst to learn about the contents of the collection by performing searches . Because an analyst’s document collec - tion is generally carefully selected ( and the collection of a collaborating analyst is likely on a relevant topic ) , searching on it will often produce more relevant results than searching on the Web or on a more generic library of intelligence community information . In some of our studies , we noticed that the category structure of the shared collection gives analysts an easy way to do Boolean searches without explicitly adding “AND” constructs to the query . For example , if one analyst has organized documents into a category about Country X and a second analyst searches that collection with the query “nuclear , ” any documents that match the query and are located in the Country X collection are probably about both “nuclear” and Country X . Another subtle but more power - ful use of this approach would be to look for sharing of nuclear data among organizations or nations by looking for instances , where a particular Country Y scientist appears in the Country X category of the “Nuclear” library and where a Country X scientist appears in the Country Y category of the same library . Other systems , including TRIST [ 12 ] , use intersecting categories to get this effect in the context of organizing query results . CorpusView does it in the context of a shared collection organized by people . 3 A L ABORATORY T EST ON A NALYSTS 3 . 1 Introduction As part of our participation in a government grant program , we had an opportunity to work with the National Institute of Standards and Technology in March of 2007 to perform a formative evaluation of CorpusView and Entity Workspace using intelligence analysts as experimental subjects . At that time , the orange notification flags and the ability to view and import remote notes had not yet been implemented , but the other capabilities described above were in place . We designed the study to provide information both on the suitability of these tools for individual analysis and for collaborative analysis . Once the results of the evaluation were available , we were surprised to see that collaboration support was among the highest rated features of the system . In this section , we briefly describe the experimental design and those results that appear to be most relevant to support collaborative analysis . 3 . 2 Laboratory Study Design In this analyst - centered evaluation , no ground truth was available . Therefore , we relied in part on analysts’ sub - jective feedback ; although we did collect quantitative logging data . Because of the growing presence of novices in the analyst workforce , we wanted to investigate whether the tool accommodates the interaction and analytical behaviors of both senior and novice analysts . Therefore , we report findings in terms of senior and novice analysts . Four intelligence analysts participated in the evaluation . Two ( A1 and A2 ) were novice analysts with less than two years of experience and two ( A3 and A4 ) were seniors with eight or more years as intelligence analysts . Because of the nature of the tools being studied , the NIST team decided to use innovative analyst - centric metrics developed to measure analysts’ interactions with visualiza - tion tools and with each other while using such tools [ 11 ] . Using these metrics together with classic usability metrics , we logged analysts’ interactions with the tool . A survey contained Likert scale questions using a one - to - seven point range , where one signified worst and seven the best . Open - ended questions gave analysts the opportunity to explain their scalar ratings and to share their thoughts on any aspect of the integrated tool . To learn if analysts could build evidence files , we logged the number of entities and comments created and which 184 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 13 . CorpusView with categories and search results . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . method was used to create them . To learn if analysts could create schemas , we logged the number of snap - together knowledge events performed including operations to create belief statements , evidence groups , supergroups , and coupled entities . We also developed metrics to measure collaboration . To study analysts’ ability to continue each other’s work , we logged the number of times analysts added to , deleted from , or modified notebooks created by other analysts . We asked analysts if Entity Workspace helped them collaborate with colleagues . We asked if seeing each other’s notes in a shared notebook helped analysts to perform better analysis . A senior intelligence analyst ( who was not one of the subjects ) developed two strategic analysis tasks that mimic the types of problems that intelligence analysts must solve in their workplaces . The selected tasks involved answering questions about the probable military strategy , weapons capability , and eco - nomic development of a selected country . Each subject was given a computer workstation with two 20 - inch displays . Training started with an eight - minute video overview of the tool . A self - paced on - screen tutorial explained key concepts and major functionality . A compe - tency test asked analysts to show their ability to perform 23 key functions . Four analysts performed Tasks 1 and 2 the first afternoon and most of the second day . Usability engineers observed analysts as they performed their tasks . The observers took time - stamped notes , without interfering with analysts’ workflow . To evaluate the ability of the tools to support collabora - tion , we asked analysts to trade workstations , notebooks , and tasks twice over the course of the evaluation . This process is illustrated in Fig . 14 . Analysts worked in pairs over three rotations , R1 , R2 , and R3 . Two senior analysts formed one pair of partners ; two novices formed another pair . In R1 , analysts started work on either Task 1 or Task 2 . Within a pair , each partner was given a different task . On the first day , in R2 , after having spent two hours on their R1 tasks , partners swapped workstations , thereby swapping tasks . Analysts then continued work where their partners had left off . At the beginning of the second day , in R3 , analysts returned to their original workstations and original tasks . 3 . 3 Laboratory Study Results Our NIST collaborators produced a hundred - page long report of the evaluation , detailing results from surveys , analystobservation , logging , finalnotebooks , andinterviews . Here , we present just results most relevant to the collabora - tive aspects of the system . Entities . Related to our first design guideline ( entities in notebooks ) , we wanted to understand whether analysts were willing to put in the effort needed to create an entity - based notebook . The four analysts imported a total of 359 entities into their evidence files over the three rotations . Four techniques were provided to add entities : hand typing , copy - and - paste , entity quick click ( a novel technique described previously [ 3 ] ) , and drag - and - drop . Drag - and - drop was rarely used . Each of the remaining techniques contributed over 100 entities to the total . Analysts varied greatly as to which technique was preferred . Because this evaluation did not compare perfor - mance to other approaches , such as text - based notebooks , we do not know how this entity count compares to traditional approaches . However , we did ask subjects to rate the statement “It is easy to copy - and - paste items into the Evidence Panel” ( score : 5 out of 7 ) , and “It is easy to use quick clickto bring items intothe Evidence Panel” ( score : 6 out of7 ) . Likewise , analysts were able to use snap - together knowl - edge features to relate entities to each other . They created 169 belief statements ( entity hypotheses ) , 108 evidence groups , 31 supergroups , and 30 entity couples during the evaluation . The subjects gave a high score to the statement “Snap - together knowledge makes it easier to organize information than other tools or methods I currently use” ( score : 6 out of 7 ) . There was also good survey evidence that Entity Work - space helps analysts find notes . The statement “The Evidence Panel helps me find notes ( e . g . , individual entities , belief statements , groups , lists / supergroups ) that another analyst or I had taken in the past” received a score of 6 . 75 out of 7 . Collapsing information . Related to our second design guideline , collapsing information , this experiment produced less support . In the final notebooks , only one group is collapsed . We did not log collapse / expand actions , so we don’t know how much this capability was used in inter - mediate stages . This feature will also be workstation specific , where an analyst working with multiple large computer monitors will have less need to compress information than the same analysts working on a single laptop screen . Furthermore , given that the evaluation took place over a relatively short period of time and produced a relatively small amount of evidence , there was less need to collapse groups than would be the case for longer notebooks . Organizing by stories . Related to our third design guideline ( organizing by stories and documents ) , several results are relevant . One concern we had was that analysts would be more familiar with entity graph - based systems in which each entity appears only once and wouldn’t see the value in the notebook approach . This does not appear to be a problem . The statement “Having some entity names appear more than once in the Evidence Panel is valuable” scored 6 . 25 out of 7 . Given that it is easy in Entity Workspace to put document links into notes and given the ability to find notes again , we would predict that analysts would be able to find the source documents associated with facts . This is fairly well supported . The statement “Entity Workspace helps me remember which source documents provided BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 185 Fig . 14 . Analysts traded workstations twice . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . evidence of a connection between two entities” received 5 . 75 out of 7 . However , some of our feedback suggests that our software or its documentation may stress organization around documents too much . One analyst commented that she “usually organizes data by topic , not by source . Entity Workspace presents information by source . ” We are looking into ways whereby the Entity Workspace will mimic traditional methods of sourcing such as footnotes and endnotes , which are standard in research communities from historians to physicists . One of the analysts in our study defines sensemaking as “Putting pieces of a puzzle together and creating a story out of it . ” We asked our subjects whether Entity Workspace helped with sensemaking via a series of questions . The response is well summarized by the response to a survey question “Snap - together knowledge facilitates sensemak - ing , ” which scored 6 out of 7 . However , there were suggestions for improvement , including requests for a timeline capability and the ability to represent organiza - tional structures ( e . g . , org . charts ) directly in the notebook . We didn’t test our fourth design guideline ( quiet , collaborative entity recommendations ) directly in this study . Sharing . Related to our fifth design guideline ( sharing document collections ) , we did get some relevant informa - tion . The analysts in the study were all asked to use a document collection that had been prepared by another analyst ( who was not a subject ) and not to use any other sources of documents . This condition allowed us to test their ability to use the search tool , reading tools , and entity quick click information copying that are all available from CorpusView and Entity Workspace . It was interesting that the novice analysts in this study performed no explicit Boolean queries at all , while the expert analysts performed 14 each . We are intrigued by the idea that the pseudo - Boolean queries enabled by sharing collections that have been categorized may introduce the power of Boolean queries to some analysts who would otherwise do without . This also suggests the value of a shared document collection in providing a rapid means for analysts new to the project—especially novice analysts—to acquaint themselves with previous analysis . 4 T ESTING IN THE F IELD ( SAIC E VALUATION ) Our next stage of testing is an operational test at SAIC that is taking place in three trials as follows : First , in a follow - up to the NIST tests , we are duplicating the NIST conditions , tasks , and time constraints with additional analysts to collect more data to compare on how workspace operation , analyst experience , and time constraints affect analysis using the Entity Workspace . This test is being performed with Entity Workspace on a laptop ( with an attached monitor for additional screen space ) instead of a workstation with two large monitors , so we expect screen space management to be an issue from the start . Indeed , we are already seeing heavier use of information collapsing in this trial . Second , we are extending the NIST tests to look at performance on long - term analysis . In this test , we are using the same data set as in the NIST test , but giving the analysts varying time deadlines to perform the task . Early results from this trial have shown additional support for our Sharing design guideline . In particular , the senior SAIC analyst ( who built the test data set on a national WMD program over the course of a year in a previous SAIC / PARC experiment ) was able to find virtually all the documents relevant to biological warfare in the target nation and build a “storyboard” assessment in less than two hours using only the single search query “anthrax . ” While there is obviously a test artificiality involved ( the senior analyst actually was the one to collect the 500 documents in the target nation WMD library ) , it does indicate the power of the system to allow an analyst to rapidly retrieve thought trains from hundreds of documents and organize them quickly for a presentation / assessment . It also highlights the use of implicit Boolean searching that is possible against an analyst library / shoebox on a single topic . The third trial is studying how Entity Workspace can be used from scratch to build a topic library and build a long - term understanding on that topic . The senior SAIC analyst has begun this task and has already changed his methodol - ogy “on the screen” to correspond better with his methodology “in his brain . ” By taking advantage of the supergroups ( with belief statements and entity groups ) feature of the Entity Workspace , he is using that environ - ment directly as a hypothesis testing tool . The resulting methodology lends support to both our Entities guideline and our Collapsing Information guideline . The key to this phase of the SAIC experiment is to build an environment that supports analysis the way analysts actually think . We all learn that good storytelling depends on answering six key questions : Who ? What ? When ? Where ? Why ? and How ? The first four focus on reporting and one can usually answer these questions with nouns ( Fig . 15 ) . Accordingly , a first - level analyst workspace needs to provide an environment in which to link the nouns that answer the Who , What , and Where questions and then provide a timeline capability to link the events ( which again can be denoted by nouns ) to answer the Who question . Software tools can link nouns in many ways and some can even support the When / timeline question . Within such 186 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 15 . Entities compared to hypotheses . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . environments , one can do creditable “reporting”—“just the facts , Ma’am , nothing but the facts”—and build schemas that show the interrelationships of the nouns that answer Who ? What ? Where ? When ? The first versions of the Entity Workspace aimed at providing a workspace for reporting . The current SAIC / PARC experiments focus on meth - odologies to address the How ? and Why ? questions that are the core of analysis . A mere reporting of facts can neither assess capabilities ( how a target nation can be a threat ) nor intent ( why a target nation can be a threat ) . The difference here is that assessing how and why requires a system that can support hypotheses , which are phrases and sentences in which the key words are usually verbs and adverbs . Accordingly , any tool aimed at supporting analytical assess - ments using the scientific method—hypothesis testing— must always support the use of sentences . The Entity Workspace assumes that the analyst will be using some kind of hypothesis testing method ; our test methodology is the method of Alternative Competing Hypotheses ( ACH ) . With this methodology , a hypothesis is a belief state - ment , stating an assertion without any evidence to support it . As such , one can take a task or question and begin with test hypotheses as true / false assertions ( “Target nation A has a WMD Program” ) or assertion lists ( “Biotech compa - nies in target nation A that could grow anthrax are : ” ) . Each hypothesis is inserted into the Entity Workspace as a belief statement and research is begun to test them . As documents are found containing evidence for or against a hypothesis , the evidence statements ( from a single sentence to a paragraph ) are dragged under the belief statement and dropped as part of a growing evidence group . Our testing also shows that analysts also tend to work in two modes : bottom - up ( deductive ) assembly of data into schemas and theories , and top - down ( inductive ) collection of data as evidence to support or refute hypotheses . The PARC / SAIC experiments with multiple analysts have also indicated that junior analysts tend to work bottom - up while senior expert analysts tend to work top - down , meaning that any new workspace needs to support both methods . In bottom - up analysis , the analyst begins with a very simple hypothesis or keyword and linearly builds a theory . This is a data - driven mode that begins on the bottom of the information ladder ( see the Think Loop , Fig . 1 ) and works linearly up the chain from data to theory . In the bottom - up mode , analysts begin with the source documents ( such as the ones in CorpusView ) and : . identify a relevant document , . find data with potential to assemble into a story , . drag - and - drop nuggets from CorpusView into En - tity Workspace , . extract and name key entities , . link entities with data , and . assemble growing data files into a story . Currently , many IT tools focus on bottom - up methods for analysis . This is important because novice analysts ( and even expert analysts working on a problem totally outside their area of expertise ) begin any new task bottom - up . The bottom - up approach is supported in Entity Workspace by snap - together knowledge , which makes it easy to group and rearrangeentitiesandsnippets , asshowninFig . 16 . However , as analysts become more expert , they tend to shift to top - down methods , and SAIC / PARC are building tools to both support current experts and provide a working environment and learning templates to assist novice analyst to make the shift to more accurate and precise top - down thinking . In top - down analysis , the analyst begins with several well thought out hypothesis statements and loops down - ward to data and previous knowledge to test each hypothesis and build the best theory . This is a hypothesis - driven mode that begins on the top of the information ladder ( see the Think Loop , Fig . 1 ) and works nonlinearly down and up the chain as an analyst searches for evidence in data sets to support or refute the initial hypothesis and builds theories based on whether the initial hypotheses are supported or the data trail leads to evidence for alternative hypothesis . Accordingly , theories are constantly being built and tested as new hypotheses for further analysis . In the top - down mode , analysts begin in the Entity Workspaceand follow a nonlinear path to a theory asfollows : 1 . Hypothesis Building . Build a set of alternative hypotheses that explain How ? and Why ? ( see Fig . 17 ) : a . Review everything already known on the topic ( either mentally or by rereading previous assessments and theories ) . b . Build alternative test hypotheses . c . Insert into Entity Workspace as belief statements . d . Pick a hypothesis . Build belief statements for evidence to support that hypothesis . BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 187 Fig . 16 . Grouping entities and snippets into a group . Fig . 17 . Building alternative test hypotheses . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . 2 . Schema Building . Find evidence to support belief statements . Assemble schemas for belief statements . Fig . 18 illustrates this process . a . Search previous data in the analyst’s own collec - tion of documents ( already in CorpusView ) . b . Search for new evidence ( in all available data - bases ) and import into CorpusView and Entity Workspace . The tools should keep track of the source document for each piece of evidence . c . Assemble evidence under belief statements ( in - cluding evidence that refutes a particular belief ) . 3 . Theory Building . Assemble related schemas into theories ( hypothesis belief statements with support - ing / refuting evidence ) . a . Assemble schemas ( evidence groups ) under more global belief statements ( Fig . 19 ) . b . Review growing theories for missing evidence . Redo step 2 to search for new evidence and add additional schemas to growing theory . 4 . Hypothesis Triage . Build opposing theories based on alternative hypotheses and assess for viability . a . Pick an alternative hypothesis . b . Repeat the previous steps . c . Theory gisting . As an evidence group grows to more than seven evidence statements , collapse related groups under belief statements ( Fig . 20 ) and split into more - detailed categories . d . Collapse all discredited hypotheses . ( By main - taining all hypotheses on the Entity Workspace , they are available if subsequent data are added that relate to them ; in that case , they will be highlighted for reevaluation . ) 5 . Presentation . Tell the best story to the customer . a . Reassess , find common threads ( Fig . 21 ) . b . Rearrange evidence by the common threads . c . Build new supergroups for each thread . d . Regroup as a super - supergroup and so on . e . Generate text version of the workspace . Entity Workspace will generate a formatted text version of the workspace that can be pasted into a document . If the Entity Workspace is set up and rearranged by the method described above , each supergroup becomes an outline for a presentation that can be exported as a rough draft for a 188 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 18 . Building schemas in top - down analysis . Fig . 19 . Theory building in top - down analysis . Fig . 20 . Hypothesis triage in top - down analysis . Fig . 21 . Storytelling in top - down analysis . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . Microsoft PowerPoint 1 presentation or a Micro - soft Word 1 assessment . Preliminary tests indicate that this method can be extended to support hypothesis testing on complex issues : a . By grouping evidence under the belief statement in terms of whether it supports or refutes the belief , an analyst can build alternative hypotheses easily in parallel . Also , when working on the evidence supporting the “true” assertion , he can compress the “false” evidence by collapsing and have it available to consider later . The analyst can also easily rearrange the evidence to put the most convincing on the top of the list and the least convincing on the bottom . b . As an evidence group grows to contain more than roughly seven evidence statements from source documents , the analyst can reassess them all , find common threads , rearrange the evidence by the common threads , then add new belief statements to each group that are subhypotheses of the overall hypothesis . Both these methods take advantage of the outline - like hierarchical structures created by our entity - based ap - proach . These methods support both individual analysts and collaboration . As of this writing , one senior analyst and two junior analysts at SAIC have used CorpusView and Entity Work - space over periods ranging from weeks to months . During a February 2009 session in which the researchers closely observed the second junior analyst , several additional challenges to implementing new kinds of tools and visuali - zations were observed . In general , these relate to analysts becoming so accustomed to “workarounds” required by current software that they continue to use the “old - fash - ioned” ways even when given a more logical and simpler alternative . A simple example ( which has been reported by all three SAIC analysts ) is that analysts are so accustomed to manually cutting , pasting , and reformatting evidence , then cutting and pasting document bibliographic data that they often continue to do so even when new drag - and - drop or automatic reformatting tools are available . However , this seemingly minor inconvenience can be a huge time - waster ; the senior SAIC analyst—a highly paid subject matter expert—estimates that he spends about half his analytical time cutting , pasting , and reformatting evidence from one document to another . In some cases , new “automatic” tools are intentionally disabled because the “automatic” function needs to be corrected more than it helps . In particular , we found several tool / workflow mis - matches in experiments with our second junior analyst , who currently is accustomed to making heavy use of text editors , such as Microsoft Word , and occasional use of link analysis tools , such as i2’s Analyst Notebook . Observing this analyst using CorpusView and Entity Workspace , we noted several areas , where familiarity with current tools drives analyst workflow . These lessons can provide guide - lines for an improved next generation workspace that is closer to the “look and feel” of current tools but provides enhanced functions in a familiar framework : . Current text editors provide the ability to highlight entities and important evidence by font changes and / or color highlighting . Creating each entity , comment , and group requires an explicit operation in Entity Workspace’s pull - down menus . A next generation workspace needs to provide font and highlighting methods in familiar formats . . Many analysts are so adept at current cut ( Control - C ) and paste ( Control - V ) conventions that they can cut - and - paste much faster than drag - and - drop . Such data manipulation tools need to be both as effective as current ones and easily learnable for analyst to discard the ones they know . . The incompatibility of virtually all of today’s special tools often requires complex workaround methods to transfer formatted paragraphs from those tools to standard presentation tools like PowerPoint or Microsoft Word . Indeed , we observed our subject copying and pasting out of the original sources when it came time to write the assessment , instead of using material from the workspace . Next generation analytical tools must be directly input - and output - compatible with standard presentation tools . . Analysts sometimes think by “binning” entities ( which is best visualized as outlines and super - groups ) , sometimes by “linking” entities ( best sup - ported by link charts ) , and sometimes by “drill - down” ( to focus on particular pieces of the puzzle ) . The current generation Entity Workspace is designed to support the familiar outline and presentation format , but a next generation tool needs the flexibility to take the same evidence organization and re - present it in other formats ( link charts , for example ) and zoom - in and zoom - out as the analytical tasks encompasses general or very specific issues . . Finally , analysts think in terms of ideas that are whole sentences . Accordingly , they usually cut - and - paste whole sentences—and sometimes entire para - graphs—as evidence groups into the Entity Work - space , often creating multiple new entities de facto in a single operation . Without automatic extraction of new entities for each cut - and - paste , an evidence file could grow enormously without the analyst building an accompanying entity file , especially one with appropriate links . Next generation tools need to automatically build entity - link libraries as fast as the analyst can read and import them . As a result of our evaluation at SAIC , we plan to make several modifications to our Entity Workspace design to increase its appeal to a larger set of analysts . These improvements will make the user experience more text - editor like , with a design that supports close placement of objects , click - to - type , easy highlighting , easy copy - and - paste both into and out of the workspace , integration with link chart visualizations , and automatic entity identification in cut - and - pasted evidence snippets . 5 D ISCUSSION OF THE P RINCIPLES According to the Think Loop Model ( Fig . 1 ) , all - source analysts ( analysts who combine multiple kinds of informa - tion to understand a topic or answer a question ) make sense of information using a process that produces a set of BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 189 Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . intermediate products . The intermediate products include a “shoebox” ( private collection of documents ) , an evidence file , schemas of organized evidence , hypotheses , and eventually a report . After studying the kinds of collaborations that are possible with modern software tools , we propose a revised version of this model that takes collaboration into account . While analysts have long collaborated with each other through phone calls , correspondence , and by sharing final reports , it is rarer that they share their intermediate products . Given our experience , sharing intermediate products is valuable . Our revised model ( Fig . 22 ) includes channels for sharing intermediate products as a central feature . In the model , sharing can happen in at least two ways , a bottom - up and top - down mode : . In the top - down mode , analysts working together closely share unpolished and even unfinished shoe - boxes , evidence files , schemas , and hypotheses . By having partially completed analysis available , the analysts can reach back down to collaborators’ : 1 ) private document collections to find new docu - ments relevant to the task and 2 ) hypotheses ( as belief statements ) and supporting evidence groups to check alternative hypotheses considered by their collaborators or new evidence for or against a particular hypothesis . In this mode , they do so directly with synchronous and asynchronous colla - boration tools . . In the bottom - up mode , analysts working less closely may choose to create polished versions of intermediate products that can be shared in libraries of such products , in analogy to the way that topic information is shared on Wikipedia , etc . This allows novice analysts or analysts new to a task to find previous analyses at the schema ( Wikipedia or encyclopedia ) , or theory ( presentations or published analyses ) level that can be used directly to begin building evidence groups and supergroups based on previous analysts’ work . Entity - based notebooks appear to support sharing of evidence files in several ways . First , they make it easier to pick up entities , particularly highly rated entities in another analyst’s notebook . Second , by comparing entity lists and entity graphs from two notebooks , we are able to recom - mend the information in remote notebooks with ( relatively high ) confidence . Finally , by making it easy to reorganize material at the entity level , select entire entities , search for an entity in the notebook and in the document collection , and find connections between several entities , these note - books make it easier to discover those parts of another analyst’s notebook that are of interest to a first analyst . Based on our experience with the longer term ( field ) evaluation , the ability to collapse information appears to be valuable . By allowing the system to show that information is available without showing all of the details , collapsed groups and supergroups allow an analyst to get more information into a single notebook while still having enough space to work . Highlighting - aided drill down makes it easy to find hidden information once a user selection indicates that it may be valuable . We are confident that analysts want to see evidence both from a storytelling prose - like perspective and from a social network or entity graph perspective . Furthermore , we believe that the most value will come if the two perspectives are tightly integrated . An entity - based approach provides a valuable form of integration in which it will be possible to use insights from one perspective immediately to make inferences in the other . For example , selecting a person entity in either an entity - based notebook or an entity graph visualization can cause the entity to highlight in both views , allowing the analyst to follow paths in both the social network and the notebook narrative to make new inferences . While we have not tested our “noninterrupting” notifica - tions on analysts yet , we are confident that they will be less disruptive than pop - up dialog boxes and other temporal notifications . We are interested to see how much they are used and how they might enable new collaborations . Based on our field evaluation , we have evidence that Web - based shared document collections can be of value to collaborating analysts , both because they allow analysts to take advantage of each other’s information - finding skills and because the organization of the documents into categories supports a kind of pseudo - Boolean search that makes queries more precise with little extra work on the part of the analyst doing the searching . Our field evaluation has also revealed several ways that our current design of an entity - based workspace can be improved to support a wider range of analysts by designing tools with a more familiar “look - and - feel” and providing several ways to visualize the same evidence . Our design principles address only a subset of the design considerations of collaborative visual analytics . Heer and Agrawal [ 9 , Table 1 ] list 24 considerations . Our results can be seen as specific recommendations relative to some of their considerations : They note that different granularities of information allow different kinds of collaboration and information integration ; our method for collapsing and expanding is one way to allow flexible granularity . They note the value of notifying users when new information is found ; our quiet entity - based notifications are one way to do that . Finally , they note the value of shared artifacts for several purposes ; the ability to view the same workspaces and document collections in our approach are important ways to achieve common ground . 6 S UMMARY In this paper , we presented five design guidelines for collaborative intelligence analysis and other forms of collaborativeknowledge work . These fiveguidelines support 190 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS , VOL . 16 , NO . 2 , MARCH / APRIL 2010 Fig . 22 . Proposed collaborative think loop model . Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply . information sharing , particularly sharing of the intermediate products of analysis ( and sensemaking ) , including document collections , evidence , schemas , and hypotheses . The first four guidelines describe ways to take advantage of explicit representation of entities in shared notebooks . The last guideline describes the value of shared document collections and of the work that goes into organizing them . We described specific technologies based on these guidelines that were developed as part of the Entity Workspace evidence note - book and CorpusView document collection browser . Next , we described some first steps in the process of validating these technologies through user studies . The results of a laboratory evaluation at NIST and a field test at SAIC lend support to our belief that these guidelines will lead to improvements in collaborative intelligence analysis and collaborative knowledge work . We are continuing to build on the work described here , extending it to Web - based collaboration and larger scale collaboration . A CKNOWLEDGMENTS This research was supported in part by Contract N61339 - 06 - C - 0144 from the IARPA A - SpaceX Program to Stuart Card and Eric Bier . The authors gratefully acknowledge support from their two organizations , the Palo Alto Research Center ( PARC ) and the Science Applications International Corpora - tion ( SAIC ) . They thank the usability testing team at NIST for their work on the laboratory evaluation reported here , and especially Theresa O’Connell and Yee - Yin Choong , who contributed to early drafts of this paper . They also thank all of the analysts who tested CorpusView and Entity Work - space and gave them early feedback on their usability and utility for analysis . Finally , they thank Diane Schiano , who helped them run their most recent evaluation . R EFERENCES [ 1 ] J . R . Anderson and P . L . Pirolli , “Spread of Activation , ” J . Experimental Psychology : Learning , Memory and Cognition , vol . 10 , pp . 791 - 798 , 1984 . [ 2 ] B . B . Bederson , J . D . Hollan , K . Perlin , J . Meyer , D . Bacon , and G . Furnas , “Pad + + : A Zoomable Graphical Sketchpad for Exploring Alternate Interface Physics , ” J . Visual Languages and Computing , vol . 7 , pp . 3 - 31 , 1996 . [ 3 ] E . A . Bier , E . W . Ishak , and E . Chi , “Entity Quick Click : Rapid Text Copying Based on Automatic Entity Extraction , ” Proc . Conf . Extended Abstracts of CHI 2006 , pp . 562 - 567 , 2006 . [ 4 ] E . Bier , L . Good , K . Popat , and A . Newberger , “A Document Corpus Browser for In - Depth Reading , ” Proc . Joint Conf . Digital Libraries ( JCDL ) , pp . 87 - 96 , 2004 . [ 5 ] E . Bier , E . Ishak , and E . Chi , “Entity Workspace : An Evidence File That Aids Memory , Inference , and Reading , ” Proc . IEEE Int’l Conf . Intelligence and Security Informatics , pp . 466 - 472 , 2006 . [ 6 ] D . Billman and E . A . Bier , “Medical Sensemaking with Entity Workspace , ” Proc . Conf . Human Factors in Computing Systems ( CHI ’07 ) , pp . 229 - 232 , 2007 . [ 7 ] J . W . Bodnar , Warning Analysis for the Information Age : Rethinking the Intelligence Process . Joint Military Intelligence College ( JMIC ) , 2003 . [ 8 ] J . W . Bodnar , “Making Sense of Massive Data by Hypothesis Testing , ” Proc . Int’l Conf . Intelligence Analysis , 2005 . [ 9 ] J . Heer and M . Agrawal , “Design Considerations for Collaborative Visual Analytics , ” Information Visualization , vol . 7 , pp . 49 - 62 , 2008 . [ 10 ] i2 Analyst’s Notebook , http : / / www . i2inc . com , 2007 . [ 11 ] T . O’Connell and Y - Y . Choong , “Metrics for Measuring Human Interaction with Interactive Visualizations for Information Analy - sis , ” Proc . Conf . Human Factors in Computing Systems ( CHI ’08 ) , pp . 1493 - 1496 , 2008 . [ 12 ] P . Proulx , S . Tandon , A . Bodnar , D . Schroh , R . Harper , and W . Wright , “Avian Flu Case Study with nSpace and GeoTime , ” Proc . Conf . Visual Analytics Science and Technology , pp . 27 - 34 , 2006 . [ 13 ] P . Pirolli , S . K . Card , and M . M . Van Der Wege , “The Effects of Information Scent on Visual Search in the Hyperbolic Tree Browser , ” Proc . Conf . Human Factors in Computing Systems ( CHI ’01 ) , pp . 20 - 53 , 2001 . [ 14 ] P . Pirolli and S . Card , “The Sensemaking Process and Leverage Points for Analyst Technology as Identified through Cognitive Task Analysis , ” Proc . Int’l Conf . Intelligence Analysis , 2005 . [ 15 ] P . Pirolli and S . K . Card , “Information Foraging , ” Psychological Rev . , vol . 106 , pp . 643 - 675 , 1999 . [ 16 ] J . Stasko , C . Gorg , L . Zhicheng , and K . Singhal , “Jigsaw : Supporting Investigative Analysis through Interactive Visualiza - tion , ” Information Visualization , vol . 7 , pp . 118 - 132 , 2008 . [ 17 ] J . J . Thomas and K . A . Cook , Illuminating the Path : The Research and Development Agenda for Visual Analytics . IEEE CS Press , 2005 . [ 18 ] W . Wright , D . Schroh , P . Proulx , A . Skaburskis , and B . Cort , “The Sandbox for Analysts—Concepts and Methods , ” Proc . Conf . Human Factors in Computing Systems ( CHI ’06 ) , pp . 801 - 810 , 2006 . Eric A . Bier received the BS degree in EECS and the MS degree in computer science from the MIT in 1983 and the PhD degree in computer graphics from UC Berkeley in 1988 . He has been working with the Palo Alto Research Center ( PARC ) since 1987 in the areas of computer graphics , active documents , multiuser and multihanded user interfaces , reading tech - nologies , visual analytics , Web - based collabora - tion , and enterprise content management . He holds the titles of principal scientist and area manager at PARC . Stuart K . Card received the AB degree in physics from Oberlin College and the PhD degree in psychology from CMU . He is a senior research fellow at the Palo Alto Research Center . His study of input devices led to the Fitts’s Law characterization of the mouse and contributed to its commercial introduction by Xerox . He has worked on theoretical character - izations of human - machine interaction , including the Model Human Processor , the GOMS theory of interaction , information foraging theory , and statistical descriptions of Internet use . He is a coauthor of the book The Psychology of Human - Computer Interaction , and a coeditor of Human Performance Models for Computer - Aided Engineering . He coauthored Readings in Information Visualization . He is developing a supporting science of human - information interaction and visual - semantic prototypes to aid sensemak - ing . He is a fellow of the ACM , the first recipient of the ACM CHI Lifetime Achievement Award , and the first member of the ACM CHI Academy . John W . Bodnar is a senior biological warfare analyst at Science Applications International Corporation , McLean , Virginia . His interest in analytical methods and tools for the Intelli - gence Community comes from previous ex - perience conducting biological warfare analysis at the Defense Intelligence Agency , analyzing the Revolution in Military Affairs as a Navy Reservist for the Office of Naval Research and the US Naval War College , and researching and teaching bioinformatics at Northeastern University , the US Naval Academy , and Stevenson University . . For more information on this or any other computing topic , please visit our Digital Library at www . computer . org / publications / dlib . BIER ET AL . : PRINCIPLES AND TOOLS FOR COLLABORATIVE ENTITY - BASED INTELLIGENCE ANALYSIS 191 Authorized licensed use limited to : Palo Alto Research Center . Downloaded on February 4 , 2010 at 18 : 27 from IEEE Xplore . Restrictions apply .