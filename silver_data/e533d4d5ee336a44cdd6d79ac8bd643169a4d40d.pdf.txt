Using Intelligent Task Routing and Contribution Review to Help Communities Build Artifacts of Lasting Value Dan Cosley , Dan Frankowski , Loren Terveen , John Riedl CommunityLab ∗ University of Minnesota Minneapolis , MN 55455 { cosley , dfrankow , terveen , riedl } @ cs . umn . edu ABSTRACT Many online communities are emerging that , like Wikipedia , bring people together to build community - maintained arti - facts of lasting value ( CALVs ) . Motivating people to con - tribute is a key problem because the quantity and quality of contributions ultimately determine a CALV’s value . We pose two related research questions : 1 ) How does intelligent task routing —matching people with work—affect the quan - tity of contributions ? 2 ) How does reviewing contributions before accepting them affect the quality of contributions ? A ﬁeld experiment with 197 contributors shows that simple , in - telligent task routing algorithms have large effects . We also model the effect of reviewing contributions on the value of CALVs . The model predicts , and experimental data shows , that value grows more slowly with review before acceptance . It also predicts , surprisingly , that a CALV will reach the same ﬁnal value whether contributions are reviewed before or after they are made available to the community . Author Keywords online communities , contribution models , intelligent task routing , member - maintained , Wikipedia , editorial review ACM Classiﬁcation Keywords H . 5 . 3 [ Information Interfaces and Presentation ] : Group and Organization Interfaces—Collaborative computing INTRODUCTION Wikipedia members have collaboratively produced over one million encyclopedia articles in dozens of languages since 2001 . Distributed Proofreaders users have produced thou - sands of books for Project Gutenberg [ 13 ] . ESP Game play - ers have contributed millions of labels to an image database ∗ CommunityLab is a collaborative project of the University of Minnesota , University of Michigan , and Carnegie Mellon Univer - sity . http : / / www . communitylab . org / Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . CHI 2006 , April 22 - 27 , 2006 , Montr´eal , Qu´ebec , Canada . Copyright 2006 ACM 1 - 59593 - 178 - 3 / 06 / 0004 . . . $ 5 . 00 . [ 18 ] . 35 , 000 RateYourMusic . com users are building them - selves a music database , while freedb . org’s online music ser - vice receives thousands of weekly CD submissions . The common theme : groups of volunteer contributors building community - maintained artifacts of lasting value ( CALVs ) . The content of CALVs is meant to be persistent and have value to the entire community . This raises many issues , from “who is the community ? ” ( are trolls part of a Usenet group ? ) to “what is valuable ? ” ( comp . lang . perl split into subgroups partly because of frustration with novice questions ) . In this paper , we focus on two fundamental , related problems that communities building CALVs must solve : motivating people to contribute and ensuring that contributions are valuable . Research Question 1 : How does intelligent task routing af - fect contributions to a CALV ? Thorn and Connolly ana - lyzed the problem of encouraging contributions using dis - cretionary databases [ 16 ] , an abstract model that applies rea - sonably well to CALVs . A key problem is that discretionary databases are public goods [ 6 ] . That is , everyone can con - sume the information without using it up for others . It is rational for individuals to consume information but not to produce it because contributing has costs . Some people con - tribute despite the cost [ 4 ] , but the community as a whole suffers because all would be better off if all contributed . Reducing costs can encourage contributions . Wikipedia al - lows almost anyone to edit almost anything , making it easy to ﬁnd work to do . Distributed Proofreaders uses mentors to teach new members . The ESP Game makes contributing fun . freedb . org uses music software to semi - automatically submit information . RateYourMusic . com piggybacks on the value people gain by maintaining their music collections . We explore a computational approach to reducing contribu - tion costs , intelligent task routing , that leverages social psy - chology theory to match people with appropriate tasks . The collective effort model [ 8 ] suggests a number of factors that affect people’s motivation to contribute to groups . Online communities that know something about their members may be able to match people with tasks based on attributes of the tasks that map to factors in the collective effort model . We develop several general and simple task routing algorithms that most communities could implement by choosing items : 1037 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada • that a given user will probably like • that a given user has experience with ( i . e . , has rated ) • that need work ( like Wikipedia’s Community Portal ) • randomly ( like Slashdot meta - moderation ) It turns out people are much more likely to contribute when asked to edit items they have rated . Research Question 2 : How does reviewing contributions be - fore accepting them affect the value of CALVs ? Just con - tributing is not enough . Because Wikipedia allows anyone to contribute , and because these contributions become visi - ble right away , it has become a lightning rod for the problem of ensuring contributions are valuable . 1 Reviewing contri - butions is a natural strategy , but designing effective review systems is hard because reviewing behavior is complex . For example , the timing of contributions affects whether they re - ceive adequate review in Slashdot [ 9 ] . Vi´egas et al . use visualizations of editing behavior in Wikipedia to under - stand how its review mechanisms improves quality by help - ing members to repair vandalism and to negotiate disagree - ments over content [ 17 ] . In our work we focus on how the structure of review mechanisms affects contribution behav - ior . We studied whether review is needed at all in prior work . It is , but in our domain , peers were as effective as experts [ 3 ] . Here we focus on another important structural question : should contributions be reviewed before being added to the CALV , or can they be added ﬁrst and reviewed later ? Both approaches can succeed . Distributed Proofreaders and RateYourMusic require contributions to be reviewed before they are added to the CALV . In Wikipedia , contributions are immediately available and are reviewed by their consumers . Both approaches can also fail . The ChefMoz . org restaurant directory is impeded by its slow contribution review process , while a 2005 LA Times experiment with wiki - based editori - als ended , overrun by vandals , in just two days . We develop simple mathematical models of contributors’ be - havior to explore the effect of reviewing contributions before and after inclusion . The models predict that review before inclusion hurts the CALV’s short - term value because of re - view overhead . Surprisingly , the models predict no gain in long - term value for review before inclusion . We compare the models to data from MovieLens , where some contribu - tors used a review before inclusion system while others used review after inclusion . It turns out that the model is correct that making contributions available immediately wins in the short term , and it ﬁts the data fairly well . However , we do not have enough data to evaluate the long - run prediction that both systems will achieve the same level of value . EXPERIMENTAL DESIGN AND OVERVIEW We address our research questions using ﬁeld experiments with the MovieLens recommender system , a web site with thousands of active users per month . MovieLens contains 1 e . g . , http : / / slashdot . org / article . pl ? sid = 05 / 08 / 05 / 2012229 . Figure 1 . The interface for editing movie information . The previous editor’s changes are highlighted . Figure 2 . The front page interface , showing the ﬁve visible movies on the chosen and recently edited lists . about 8 , 800 movies that members can rate and receive rec - ommendations for . It keeps a modest amount of information about movies , including directors and actors , movie genres , languages , release dates , and video availability . Members can use this information to ﬁlter recommendations . Its movie information is incomplete . For most of its life the MovieLens database has been maintained by a single movie guru . When the guru is busy , the database suffers . Some - times he does not add actors and directors , movies released on DVD are not always updated as “new DVDs , ” and so on . About 1 / 3 of the ﬁelds in the database are blank . This has a direct impact on the value of MovieLens , for example , when searches fail to return relevant movies . To improve the database , we created an interface that allows MovieLens members to edit information for movies ( Figure 1 ) . The interface highlights ﬁelds changed by the last edi - tor of a movie and displays text appropriate for a subject’s 1038 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada Strategy Description Theoretical justiﬁcation Practical justiﬁcation HighPred Choose movies a subject is pre - dicted to like . People often ﬁnd recommended items valuable ; personal value increases motivation . Almost any community that models its members can compute some notion of relevance . RareRated Choose movies the subject has rated but few others have . Editing is easier if you know the movie ; easier tasks increase motivation . Further , rareness im - plies a special ability to contribute ; knowing that contributions matter increases motivation . Also easy to implement , and targets ef - fort toward items that might not other - wise be corrected . NeedsWork Choose movies with the lowest Nﬁelds value . More missing information allows more valuable contributions ; knowing that contributions matter increases motivation . Chooses tasks that provide the maxi - mum potential to increase value . Simi - lar to Wikipedia’s Community Portal . Random Choose movies randomly . Baseline algorithm . Easy and provides wide coverage . Used by Slashdot meta - moderation . Table 1 . Four algorithms for matching people with tasks . experimental group . We publicized the ability to edit movie information by asking people to edit selected movies to edit on the MovieLens main page ( Figure 2 ) . Prior to this , only the guru had the power to edit movies in the database . The main page displays a list of chosen movies and a list of recent movies . The chosen list contains movies selected by one of four intelligent task routing algorithms . The recent list , similar to Wikipedia’s recent changes , contains the most recently edited movies . Each list showed ﬁve visible movies and a link to 15 more . Members were also able to edit infor - mation for any movie they saw while using MovieLens . Subjects and conditions MovieLens members who logged in during the experiment were randomly assigned one of four task routing algorithms shown in Table 1 . They were also randomly assigned to one of two contribution review systems : Wiki - Like , where con - tributions were made visible immediately , or Pre - Review , where contributions needed to be reviewed by another Pre - Review subject before becoming visible . We placed few restrictions on editing . Members who had been with MovieLens for at least one month were eligible . We limited subjects to 10 edits day to make the analysis less sensitive to huge outliers . We debated this—why not let users do what they want ? —but it also has reasonable prac - tical justiﬁcations : involving more users increases commu - nity robustness , while industrious subjects are discouraged from writing scripts to automate contributions using data from other websites ( which at least one person did ! ) . Fi - nally , Wiki - Like subjects were not allowed to edit movies pending review by a Pre - Review subject , and Pre - Review subjects were not allowed to review their own edits . Metrics We used three metrics . A crucial aspect of whether a com - munity succeeds in building a CALV is how much value the community creates . A coarse metric , Nedits , counts the number of edits people make . A ﬁner - grained metric , Nﬁelds , counts the number of non - blank ﬁelds for a movie . Nﬁelds is not very precise because it cannot detect when bad information is corrected . We chose it because many commu - nities should be able to develop similar syntax - based met - rics . More nuanced value metrics are possible , such as ask - ing members to ﬂag high - and low - value items or using read and edit wear [ 7 ] to estimate value . Another indicator of a community’s success is how many members participate in the CALV’s upkeep ; communities that spread work among many members are more inclusive and robust . This leads to the third metric , Neditors , the num - ber of people who edited at least one movie . We supplement these metrics with results from a survey we conducted after the experiment concluded . 119 people , most of whom used the editing features , responded to the survey . Overview of editing activity We collected behavioral data for 53 days in summer 2005 . A total of 2 , 723 subjects saw the contribution interface , with 437 subjects editing at least once . They performed a total of 2 , 755 edits . Of these editors , the mode was one edit , though two subjects edited well over 100 times—quite a feat con - sidering the 10 edit per day limit . Editing activity appears to follow a power law distribution , a pattern we have seen in many aspects of MovieLens , from movie popularity to the number of logins , ratings , and forum posts per user . RQ1 : DOES TASK ROUTING AFFECT CONTRIBUTIONS ? We turn now to our ﬁrst research question : How does intel - ligent task routing affect contributions to a CALV ? A com - munity might want to match members with tasks for a num - ber of reasons . For example , it might be useful if people who review recent changes in Wikipedia know the topic they are reviewing . User modeling research shows how to build proﬁles of user interests ( e . g . , [ 14 , 15 ] ) and expertise ( e . g . , [ 11 , 12 ] ) that can help match people with topics . An - other reason to match people with tasks is to reduce their workload . Wikipedia’s recent changes list is long because people make dozens of changes per minute . A personalized view that shows only pages the viewer is likely to work on might increase viewers’ ability to contribute . A community might also prefer to focus on tasks that seem most needed . Wikipedia could highlight large recent changes rather than small ones or solicit people to expand short articles rather than review recent changes . 1039 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada Neditors , Nedits , and Nfields by strategy 0 50 100 150 200 250 Neditors Nedits Nfields Metric C oun t HighPred RareRated NeedsWork Random Figure 3 . Counts of unique editors , movies edited , and ﬁelds edited for each strategy . RareRated does best on all , while NeedsWork has the highest Nﬁelds per edit . Karau and Williams’ collective effort model [ 8 ] calls out factors that inﬂuence people’s motivation to contribute to groups . These factors include how much the person values the task , how much effort the task requires , and how much they believe their contribution matters to the group . Commu - nities that model members’ behavior can try to operational - ize these factors and use them to stimulate contributions . We developed four algorithms to match people with movies : HighPred , RareRated , NeedsWork , and Random . Table 1 gives a brief description for each algorithm along with prac - tical justiﬁcations and reasons why they might motivate con - tributions based on the collective effort model . We chose simple , single - strategy algorithms because they are easy to understand and easy to implement . Further , they represent algorithms used by real communities . Wikipedia’s Commu - nity Portal is based on a NeedsWork - like algorithm , while Slashdot assigns meta - moderation using Random . When a subject logs in , the algorithm ranks all movies , re - moves movies chosen for any subject in the last four hours ( to prevent movies from being picked repeatedly ) , and pop - ulates the subject’s chosen list with the top 20 movies . Results To concentrate on the algorithms’ effect on behavior , here we limit the analysis to the ﬁve movies visible on subjects’ chosen lists . For this experiment we collected data for 24 days in August 2005 . In these data , 197 of 1 , 982 subjects edited at least one movie chosen for them . Figure 3 shows the performance of each algorithm on Neditors , Nedits , and Nﬁelds . We use Random as our baseline . RareRated outperforms all algorithms , including Random both on Neditors ( χ 2 ( 3 , N = 1 , 982 ) > 114 , p < 0 . 01 ) and Nedits ( χ 2 ( 3 , N = 44 , 352 ) > 203 , p < 0 . 01 ) . The difference on Neditors was striking : over 22 % of RareRated subjects edited at least one movie , compared to about 6 % for the other groups . NeedsWork is interesting . It does worst on Neditors —a surprise because survey respondents claimed they preferred High Rare Needs Pred Rated Work Random Avg . prediction 4 . 33 3 . 10 2 . 57 2 . 85 Avg . # ratings 2 , 585 191 212 1 , 432 Avg . Nﬁelds 5 . 65 5 . 76 3 . 10 5 . 53 Total showings 10 , 506 11 , 785 10 , 767 11 , 494 User - movie pairs 4 , 606 3 , 647 8 , 564 11 , 261 Distinct movies 1 , 598 2 , 903 770 6 , 363 Table 2 . Statistics on the behavior of the four algorithms . to edit movies that need more work than less ( 57 agreed , 50 neutral , 12 disagreed ) . But because NeedsWork selects movies that have the most missing information , its per - edit improvement ( ratio of Nﬁelds to Nedits ) is much higher than for the other strategies ( 1 . 65 versus HighPred ’s 0 . 21 , Rare - Rated ’s 0 . 36 , and Random ’s 0 . 45 ) . Finally , HighPred is dominated by RareRated on all metrics and by Random on Nedits and Nﬁelds . Its per - item improve - ment is especially low . Algorithm characteristics We computed statistics about the movies each algorithm chose in order to better understand these differences . Table 2 shows that the four algorithms had distinct patterns of select - ing movies . The ﬁrst three rows tell us that the algorithms accomplished their goals : HighPred chose movies with high predictions , RareRated chose movies with relatively few rat - ings , and NeedsWork chose movies missing the most infor - mation . Otherwise , the algorithms chose movies of roughly the same quality and predicted rating . HighPred chose rel - atively popular movies , while RareRated and NeedsWork both choose relatively obscure movies . NeedsWork chose many fewer distinct movies than the other algorithms because it is not personalized—it always chose the movies that needed the most work . Similarly , the user - movie pairs row from Table 2 shows that HighPred and RareRated tended to show the same movie to the same user multiple times . In particular , several subjects in the Rare - Rated group complained they wanted to see movies in their chosen lists that they had not already edited . Designs that choose different movies each time they ask a user to con - tribute might perform better . A closer look at RareRated RareRated was the most effective strategy for convincing people to contribute . It has a number of aspects that might increase people’s motivation to participate according to the collective effort model . First , people are more likely to know about movies they have seen ; editing known movies is eas - ier and easier tasks increase motivation . Second , people are more apt to like movies they have seen ; personal value in - creases motivation . Third , being one of a few people who has seen a given movie might induce people to feel their con - tribution is harder to provide and thus matters more ; know - ing that contributions matter increases motivation ( e . g . , for discussions and ratings [ 2 , 10 ] ) . 1040 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada I prefer to edit . . . Agree Neutral Disagree movies I like . 69 36 14 movies I have rated . 87 24 8 movies with few ratings . 43 65 11 Table 3 . Survey responses to factors the collective effort model predicts may affect motivation . Having rated an item matters most , rareness matters least ( χ 2 ( 4 , N = 357 ) > 37 , p < 0 . 01 ) . We used our post - experiment survey and a logistic regres - sion model to help tease apart these factors . The survey asked subjects people questions corresponding to the three factors above . Table 3 shows people were most likely to agree that having rated a movie matters and least likely to agree that rareness matters . We also built a logistic regression model to predict whether a movie was edited using four attributes : whether the sub - ject had rated the movie , the subject’s predicted ( or actual ) rating for the movie , log of the movie’s popularity , and the movie’s Nﬁelds score . The ﬁrst three correspond to the three reasons RareRated might increase motivation , while the last seemed important to include . We used only movies shown to the Random group to create an unbiased sample . The model has some predictive power ( χ 2 ( 3 , N = 11 , 568 ) = 44 , p < 0 . 01 ) . Having rated the movie and the movie’s Nﬁelds score are useful predictors ( p < 0 . 01 ) ; liking the movie and movie popularity were not useful . Taken together , these results sug - gest lead us to hypothesize that a RatedNeedsWork algorithm would be a good alternative to explore in future research . Discussion These results show that intelligent task routing has large ef - fects . NeedsWork is intuitively appealing from the commu - nity’s point of view , maximizing the value the CALV gets per edit . However , it fails to consider individual motivation . RareRated did well because it both personalizes the movies shown and , importantly , chooses movies people have seen . The poor performance of HighPred suggests that informa - tion retrieval - style relevance by itself is not enough . Having rated an item was so important that any task rout - ing algorithm should consider members’ experience with items when possible . A natural question is , given this , what should a task routing algorithm consider next ? We explored this question by building a second logistic regres - sion model to predict editing of movies shown to the Rare - Rated group . Again , the model has some predictive power ( χ 2 ( 3 , N = 12 , 020 ) = 77 , p < 0 . 01 ) . Nﬁelds and the per - son’s rating are useful predictors ( p < 0 . 01 ) , while popular - ity and average ratings were not useful . Since Nﬁelds mat - ters , developing useful measures of item quality is a logical next step in improving task routing . RQ2 : HOW DOES REVIEW TIMING AFFECT CALVS ? We now turn from the problem of motivating individuals to contribute to our second research question : How does re - viewing contributions before accepting them affect the value of CALVs ? A common tactic for ensuring quality is to review contributions , either by other members , as Slashdot does [ 9 ] , Figure 4 . The recent list for the Wiki - Like ( top ) and Pre - Review ( bot - tom ) conditions . Differences are highlighted . or by experts , as many high - quality conferences do . There are many design questions when building a system for re - viewing contributions , including whether review is needed at all , who can be a reviewer , and whether contributions need to be reviewed before the community can use them . We have explored the ﬁrst two design questions in the con - text of adding movies to MovieLens [ 3 ] . We found that at least some review is required both to prevent anti - social be - havior and to encourage people to contribute . We also found that peers were about as good at reviewing as experts , at least in MovieLens . Further , people were just as willing to con - tribute whether peers or experts provided review , supporting the goal of using peer - based review systems as a mechanism for building scalable , robust , and valuable CALVs . Here , we concentrate on the timing of review . As we saw in the introduction , both including contributions immediately and reviewing contributions before accepting them can suc - ceed . We tried both approaches during the ﬁeld experiment , randomly assigning members to one of two conditions . In the Wiki - Like condition , contributions went directly into the MovieLens database . In the Pre - Review condition , contri - butions went to a queue of pending edits and only went live after being reviewed ( and possibly edited ) by a second mem - ber . Subjects saw a recent list that contains the items most recently edited by others in their group . The recent list dis - played ﬁve movies with a link to another 15 . Figure 4 shows how the interface differed for the two groups . Below we present a model of how review timing affects CALV quality , then use our experimental data to test the model’s predictions . We combine modeling with a ﬁeld ex - periment to build support for our main result : accepting con - tributions immediately is a win in the short term and will do just as well in the long run . A MODEL OF CONTRIBUTIONS AND VALUE Survey respondents believe Wiki - Like systems would in - crease value more quickly while Pre - Review systems would result in higher long - term value . We present a model that suggests they are half right . The model predicts how review timing will affect the quality of CALVs . We make a number of simplifying assumptions in order to get at the heart of the effect of review timing , following Axelrod : “If the goal is to deepen our understanding of some fundamental process , 1041 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada then simplicity of the assumptions is important , and realis - tic representation of all the details of a particular setting is not . ” [ 1 ] . Many of these assumptions can be lifted by adding complexity to the model , but we believe the high - level pre - dictions of the simple model apply to many CALVs . In the model , a CALV consists of a number of items , each of which has some value . Value might simply be the pres - ence of an item : a MovieLens movie , a Wikipedia page , or a response to a Usenet post . Value might account for quality , perhaps by asking people to ﬂag low - quality items . Value might also include frequency of use , by counting page views in Wikipedia or by weighing ﬁelds in MovieLens based on their use in searches . We assume an item’s value ranges from 0 to some maximum , that the value of a given item does not change unless someone contributes to it 2 , and that the num - ber of items remains constant over the modeling period . The CALV as a whole then has a value , V , ranging from 0 to V max , the sum of the maximum value of all items . Let V t be the value of a CALV at time t . We assume that time proceeds in discrete periods . During a time period , the community can increase the value of items , e . g . by correcting information for a movie or improving a Wikipedia page . We model the amount of value the community creates in a given time pe - riod as G t , or “good work” . The community also sometimes destroys value ( e . g . , through trolls , vandals , spammers , and well - intentioned mistakes ) , which we model in the aggregate as B t , or “bad work” . Good and bad work can overlap dur - ing the same time period . We can state a basic model of how a CALV’s value evolves : V t + 1 = V t + G t − B t ( 1 ) The value of the CALV grows when G t > B t , remains the same when G t = B t , and falls when G t < B t . Modeling Wiki - Like G t and B t are not constant ; otherwise , the equation above suggests that CALVs can grow without limit . Many factors inﬂuence how much value is created and destroyed in a given time period , including the CALV’s current state and the mo - tivation and abilities of community members . We assume all of these factors are ﬁxed except for V t . The intuition is that as V t grows , members will ﬁnd it harder to locate use - ful work and easier to damage already - existing value . Much of the work the community would do , then , is expended in ﬁnding work rather than doing it . Let γ be the amount of good effort the community as a whole is willing to expend in a given time period to im - prove the CALV , and let β be the similar amount of bad ef - fort it would expend to harm the CALV . Remembering that 0 ≤ V t ≤ V max , we let P t = V t / V max be the proportion of its potential value a CALV possesses at time t . We extend equation 1 to incorporate the task of ﬁnding work , by mul - tiplying γ by the probability that a given item needs work and β by the probability that a given item is already correct . 2 This is a bad assumption when information decays rapidly . For in - stance , a database of gas prices at local stations must be constantly updated or its value drops rapidly . Good Check Bad Check Good Edit + 1 0 Bad Edit 0 - 1 Table 4 . Contingency table for contribution checking , under the as - sumption that bad people are as malicious as possible . Good checking of good edits creates value , while bad checking of bad edits destroys it . Good Check Bad Check Good Edit 12 H 2 t W t = 40 2 100 = 16 12 H t C t W t = 40 · 10 100 = 4 Bad Edit 12 C t H t W t = 10 · 40 100 = 4 12 C 2 t W t = 10 2 100 = 1 Table 5 . A concrete example of computing the number of edits in each cell of the contingency table where H t = 40 , C t = 10 , and W t = 50 . That is , G t = ( 1 − P t ) γ and B t = P t β : V t + 1 = V t + ( 1 − P t ) γ − P t β ( 2 ) Equation 2 has the convenient property that as long as 0 < β , γ < V max , it ensures that 0 < = V t < = V max . Further , at some point G t will equal B t , and the CALV will reach a value equilibrium V lim at a time t where ( 1 − P t ) γ = P t β . A little algebra shows this equation is satisﬁed when P t = γ / ( γ + β ) , allowing us to compute the equilibrium : V lim = γ ( γ + β ) V max ( 3 ) In other words , a Wiki - Like system reaches a value equilib - rium below its potential that is determined only by the pro - portion of good to bad effort members are willing to expend . Modeling Pre - Review We now turn to modelling the Pre - Review system , where a second person must approve contributions before they are added to the CALV . Here , value is divided into two parts : working on items , and approving ( or rejecting ) work . We return to our fundamental model from Equation 1 , V t + 1 = V t + G t − B t . Now , the formulae for G t and B t must account for the fact that some of the community’s contributions are allocated to editing items and some to checking those edits . 3 One of four things can happen to a contribution , depending on whether the editor and the checker are good or bad . Table 4 shows a payoff matrix that models “bad” people that ma - liciously destroy content , one of the most common threats seen in wikis . 4 If both the edit and check are good , value is created . If both are bad , value is removed . If one is good and one bad , value is unchanged : either a bad edit is appro - priately rejected or a good one is incorrectly rejected . We now compute how the value of the CALV changes dur - ing a given time . To do this we must ﬁrst know how much total work is done . Let H t = ( 1 − P t ) γ , C t = P t β , and W t = H t + C t be the amount of good , bad , and total work 3 The work of checking in Wiki - Like systems is represented implic - itly in β and γ : some of the effort of γ at time t + 1 will repair errors introduced through the effort in β at time t . 4 But not the only possible matrix . Bad actors might sometimes create value , e . g . , a spammer might approve good contributions in order to increase the community’s value as a market . 1042 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada done during time t . We assume people evenly divide work between editing and checking . The formulae for all four squares in the contingency table have the same structure . For instance , the probability of a Bad Edit is C t / W t , and the ( as - sumed independent ) probability of a Good Check is H t / W t , so the probability of a Bad Edit followed by a Good Check is C t H t / W 2 t . To compute the number of times a Bad Edit fol - lowed by a Good Check occurs , we multiply this probability by the total number of edits performed : W t / 2 since half the work goes into edits . The product is C t H t / 2 W t . Table 5 presents all four formulae along with a numeric example . Now we are ready to apply equation 1 and the payoff matrix from Table 4 . In this case G t is just the number of edits that wind up in the Good - Good quadrant , while B t is the number of edits that wind up in the Bad - Bad quadrant . The equation for how much gets done per time period is : V t + 1 = V t + 1 2 H 2 t W t − 1 2 C 2 t W t ( 4 ) Again , we can ﬁgure out the equilibrium for the value of the CALV , V lim , which happens when : 1 2 H 2 t W t = 1 2 C 2 t W t H t = C t ( 1 − P t ) γ = P t β We saw this equation when ﬁnding the Wiki - Like equilib - rium ; the equilibrium for a Pre - Review system is : V lim = γ ( γ + β ) V max ( 5 ) which is surprisingly the same as Equation 3 ! That is , if V max , γ , and β are the same , checking before accepting con - tributions does not improve the eventual value equilibrium the CALV reaches , compared to accepting them right away . Rate of Convergence Instead , checking imposes a cost . We can compare how quickly Pre - Review and Wiki - Like approach the value equi - librium . The rate of convergence for a sequence that con - verges linearly to ξ is usually deﬁned as µ , where : µ = lim k → inf | x k + 1 − ξ | | x k − ξ | ( 6 ) That is , if the order of convergence is linear , the rate of con - vergence µ is the ratio between the error at step k + 1 and the error at step k [ 5 ] . µ must be between 0 and 1 , and the closer to 0 , the faster the convergence . The convergence rate for both systems is given by : µ = 1 − xγ + β V max ( 7 ) where x = 0 . 5 for Pre - Review and x = 1 for Wiki - Like . 5 In other words , both systems reach the same ﬁnal value , but the Wiki - Like system gets there faster . 5 We derive these rates in the appendix . Modeling adding movies to MovieLens 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 0 1 2 3 4 5 6 Year M o v i es i n M o v i e L e n s WIKI PEER GURU Figure 5 . Modeling the effect of different systems for adding movies to MovieLens . , with V 0 = 4 , 000 and V max = 10 , 000 . For the PEER and WIKI systems , γ = 160 and β = 40 , while for the GURU system , γ = 40 and β = 0 . WIKI is faster than the others . PEER never catches up , but GURU does—after 5 years . A concrete , yet ﬁctional example We ﬁrst apply the model to a slightly ﬁctionalized version of adding movies to MovieLens . Our movie guru took over the MovieLens database about 6 years ago , when it had about 4 , 000 movies . Based on his criteria for adding movies to MovieLens , there are about 10 , 000 movies eligible to in - clude . ( This number grows by a few hundred each year , but we will follow the model and assume that it is ﬁxed at 10 , 000 ) . Based on his behavior , we can estimate that γ = 40 and β = 0 if only the guru adds movies . We will discuss how to estimate these parameters later , when we evaluate the model against the experimental data . We can use the model to look at how the database might have evolved if we had allowed more people to partici - pate . Based on data from [ 3 ] , we estimate the community’s γ = 160 and β = 40 for the task of adding new movies to MovieLens . 6 With these estimates , we can compare what might have happened had we allowed the community to add movies . Figure 5 shows that the value of the database would have grown much faster than it did , and further , it would have grown even faster if contributions were made visible right away . The guru would eventually create a more valu - able database—in ﬁve years . A summary of assumptions and predictions The model relies on a number of assumptions about how editors and checkers interact through the CALV , which we collect here for convenience . 1 ) Discrete Time Steps . One time step completes before the next begins . Good and bad activities within each time step may overlap . 2 ) Cumulative Value . The value of the CALV is the sum of the value of individual items . 6 The β = 40 consists of mistakes made when adding movies , such as forgetting to add any information besides the title or adding in - formation for the wrong movie . 1043 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada Pre - Review Wiki - Like p - value Subjects 1 , 353 1 , 370 - Neditors 239 198 p < 0 . 10 Nedits 1 , 534 ( 873 + 661 ) 1 , 220 p < 0 . 05 DB Nedits 661 1 , 220 p < 0 . 01 DB Nﬁelds 770 1 , 017 p < 0 . 50 Table 6 . Overall editing performance of two groups . Pre - Review had 1 , 534 total edits , of which 661 were checks . DB Nedits and DB Nﬁelds are the changes that actually occurred in the database . P - values are from t - tests except for Neditors , where we used a chi - squared test . 3 ) Steady Item Value . The value of an individual item only changes when someone changes it . 4 ) Value Ceiling . There is a ﬁxed maximum attainable value for the CALV . ( The number of items is also ﬁxed . ) 5 ) Stable Motivation . The total amount of available effort is stable while the value of the CALV changes . 6 ) Random Search For Work . Users randomly encounter items to work on . 7 ) Effort Is Equal . The effort to ﬁx an item , to check whether an item needs ﬁxing , and to check whether a ﬁx is correct is about the same . 8 ) Edits Equal Checks . In Pre - Review , as many checks happen as edits . 9 ) No User Roles . Anyone can edit or check in Pre - Review . These are simpliﬁcations . For instance , people may give up on a CALV with low value or one that grows too slowly , violating Stable Motivation . Rapidly growing CALVs like Wikipedia violate Value Ceiling . Cumulative Value fails to model the fact that some items are more popular than others . Many review systems violate No User Roles by having rel - atively few editors who are allowed to check contributions . We nevertheless believe these are reasonable assumptions for understanding the broad effects of review timing . The model makes several interesting predictions . The most sur - prising is that the ﬁnal quality will be exactly the same for the Wiki - Like and Pre - Review methods of making changes to a CALV . Proponents of both models , from people dis - cussing Wikipedia on Slashdot to several of this paper’s au - thors , argue that their favorite model is better . Finding they should yield the same quality in the long run was a surprise . A second interesting prediction is that Wiki - Like will con - verge much faster to this long - run quality . The reason is simple : Wiki - Like does not waste as much effort checking . RESULTS FOR REVIEW TIMING We now examine what happened during the experiment , then use that data to evaluate the model . In 53 days , 437 of 2 , 723 subjects edited at least one movie . Table 6 shows subjects’ behavior under the two systems . Pre - Review outperformed Wiki - Like on total Nedits and Neditors . However , Wiki - Like subjects made more edits that appeared in the database . These differences happen because of checking in the Pre - Review group . Of 1 , 534 edits , 871 were initial edits while 661 were checks . This left 212 edits pending approval at the end of the experiment—wasted work because these contribu - Predicted versus actual increases in Nfields 0 200 400 600 800 1000 1200 1400 8 / 7 8 / 14 8 / 21 8 / 28 9 / 4 9 / 11 9 / 18 9 / 25 Date I n c r ease i n N f i e l d s Wiki Pred Wiki Actual Pre Pred Pre Actual Figure 6 . Comparing the model’s predictions to actual editing behavior , using estimates of γ and β for both systems based on the half of the editing data . As the model predicts , Wiki - Like outperforms Pre - Review . tions were not available to the community . Survey responses conﬁrm that people prefer editing to checking ( 38 preferred to edit , 17 to check , 64 did not care ) . Pre - Review had a higher Nﬁelds per movie changed in the database than Wiki - Like ( 1 . 16 vs . 0 . 83 ) . The average in - crease in Nﬁelds for both groups on the initial edit of a movie was almost identical , so the increase happened when Pre - Review reviewers made changes that improved the original contribution . Note that the model does not account for this increase ; by using the payoff matrix in Table 4 , it assumes that reviewers can only approve or reject changes without adding value of their own . The model meets reality We now compare the model to the experimental data . The model makes three high - level predictions about the Wiki - Like and Pre - Review systems . 1 ) Wiki - Like outperforms Pre - Review in the short run . 2 ) The amount of value created tapers over time . 3 ) They reach the same long run equilib - rium . We will ﬁrst ﬁt the model to our data , then evaluate how well it ﬁts and whether its predictions are accurate . To ﬁt the model , we need to estimate V 0 , V max , γ , and β . We use the Nﬁelds metric . 8 , 770 items with a maximum per - item value of 8 yields V max = 70 , 160 . The sum of Nﬁelds when the experiment began was V 0 = 47 , 280 . To estimate γ and β , we use observed behavior for the ﬁrst half of the experiment . In 27 days , the Wiki - Like group increased Nﬁelds by 654 , so we know G t − B t = 654 . The ratio of good to bad edits was G t / B t ≈ 60 . This lets us estimate B t ≈ 11 and G t ≈ 660 . Since P 0 = V 0 / V max ≈ 0 . 67 , G t = ( 1 − P 0 ) γ , and B t = P 0 β , that gives us γ ≈ 2 , 000 and β ≈ 16 for the Wiki - Like group . Similar calculations for the Pre - Review group give γ ≈ 3 , 240 and β ≈ 54 . Based on those estimates , Figure 6 compares the model’s predictions to actual behavior . The model’s ﬁrst two high - level predictions are supported : Wiki - Like outperforms Pre - Review in the short run , and the amount of value created 1044 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada tapers off over time . The predictions ﬁt reasonably well , overestimating somewhat for both groups . The quality of the predictions also depends on how much data is available for estimates . Had we only collected data for one or two weeks , our estimates of γ would be 30 to 40 percent higher because contributions taper off faster than the model predicts . Its predictions over the experimental period are nearly linear because γ + β is small relative to the maximum value of repository . Finally , we do not have enough data to evaluate the model’s third prediction that Wiki - Like and Pre - Review will converge to the same value equilibrium in the long run . Note that the model missed on Pre - Review in two ways . First , our payoff matrix from Table 4 was incorrect . Check - ing added about 40 % more value to an initial edit be - cause reviewers were allowed to improve contributions they checked . This means 1 . 4 would have been a more accurate value in the Good - Good quadrant . Second , the model did not account for wasted work because of the Edits Equal Checks assumption . In this setting , about 1 / 4 of edits were never checked . These two effects roughly balanced each other here . This is not likely to be true in general , and a more complex model that accounts for these effects might be more directly useful in designing systems . DISCUSSION These results suggest that Wiki - Like systems create value faster than Pre - Review ones . More people contributed un - der the Pre - Review system overall , but since people prefer editing to checking , a backlog of wasted work builds up . The model also suggests that the Pre - Review group will do about the same as the Wiki - Like group in the long term . Our estimates of γ and β put V lim near V max for both systems because β is small in MovieLens . This prediction would be easier to test in a system with more bad contributions . At a high level , the model accurately reﬂects the relative be - havior of the Pre - Review and Wiki - Like systems . However , contributions taper off faster than predicted . As a new fea - ture , the contribution interface might have been used more heavily at ﬁrst than it would be in the long - term , violat - ing the Stable Motivation assumption . This would explain the overestimation and the rapid taper . It might also be that ( 1 − P ) and P are not the right probabilities of ﬁnding useful work to do . Finally , it may be that counting all ﬁelds equally made our value function too simple . Members rarely search for ﬁlms available on VHS ; perhaps they would not notice or care enough to ﬁx errors in a movie’s VHS release date . Pre - Review systems may increase people’s willingness to contribute ( increase γ ) or deter people from damaging the system ( decrease β ) compared to Wiki - Like . Here , the Pre - Review group had more editors and total contributions , while prior work showed that review before acceptance reduced antisocial behavior compared to a system with no review [ 3 ] . Designers might use the model to reason about trade - offs between short - term speed and long - term quality . Fielding a Wiki - Like system until contributions taper off and then switching to a higher - equilibrium Pre - Review system may let designers have it both ways . Changing review policy in an established community requires caution , however : imag - ine the outcry from its members if Wikipedia were to switch to a Pre - Review system . Although Wiki - Like systems accumulate value more rapidly , they also allow more bad content into the CALV that is eventually corrected by other members . This is often held against Wikipedia : even though there is much good content , members may not know which content to trust , thus limit - ing Wikipedia’s usefulness as a reference—and perhaps re - ducing their motivation to contribute . Recent pages on fake pop stars and articles modiﬁed by marketers show that this concern is real . 7 On the other hand , bad content is often quickly removed , with obvious vandalism disappearing in minutes [ 17 ] . Incorrect content may take longer to ﬁx . Extending the model to consider the amount of bad content seen by members might be useful . The model could be ex - tended in a number of other ways as well . In the experiment , we noted that reviewers can add value and that work can be wasted . The model assumed these effects away , but in - corporating them would not be hard . More complete ( but complicated ) models that reduce the number of necessary assumptions may be more useful to designers . Enhancing the model’s ability to account for the cost of ﬁnding work is a natural next step . Systems that help people ﬁnd work , perhaps using intelligent task routing , will be able to redi - rect effort from ﬁnding useful work to doing it . This will improve their ability to create value , and the model should account for that as well . The model is an interesting starting point for thinking about designing systems that encourage contribution , but it is by no means the last word . CONCLUSION This work takes a number of steps toward improving system design for community - maintained artifacts of lasting value . • Systems that solicit contributions from members can use intelligent task routing to increase contributions . Even simple algorithms have large effects . • Algorithms based only on the community’s needs are less likely to interest members than algorithms that consider a person’s knowledge and ability . • The model suggests CALVs tend to reach a quality equi - librium that depends only on the proportion of good to bad contributions from community members , not on whether contributions are reviewed before acceptance . • Both the model and empirical results show that review be - fore acceptance slows the accumulation of value . • Designers can use the model of CALV value evolution to help reason about design alternatives once they collect a moderate amount of behavioral data . We are excited by the potential of CALVs to increase the value and scope of community on the web . As tools for 7 http : / / www . smh . com . au / news / icon / wikipedia - worries / 2005 / 08 / 23 / 1124562860192 . html 1045 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada ﬁnding and communicating with others became standard - ized and accessible , discussion - based online communities became widespread . We predict a similar rise of groups that build these lasting , valuable community - speciﬁc resources . Understanding how to build the tools that will help these groups survive and thrive is an important next step for the CHI and CSCW communities . ACKNOWLEDGEMENTS This work was supported by grants 0325837 , 0324851 , and 0534420 from the National Science Foundation . APPENDIX : DERIVING CONVERGENCE RATES Here we derive the rates of convergence for the Wiki - Like and Pre - Review systems . A useful equality is : γ V lim = γ γγ + β V max = γ + β V max ( 8 ) We derive the convergence for Wiki - Like to V lim : µ = V t + 1 − V lim V t − V lim = V t − V lim + ( 1 − P t ) γ − P t β V t − V lim ( 9 ) = V t − V lim V t − V lim + γ − P t γ − P t β V t − V lim = 1 + γ − ( γ + β ) P t V t − V lim = 1 − V t V max ( γ + β ) − γ V t − V lim = 1 − V t γV lim − γ V t − V lim ( by equation 8 ) = 1 − ( V t V lim − 1 ) γ V t − V lim = 1 − V t − V lim V lim γ V t − V lim = 1 − γ V lim = 1 − γ + β V max ( by equation 8 ) ( 10 ) Pre - Review also converges to V lim : µ = V t + 1 − V lim V t − V lim = V t + 12 H 2 t W t − 12 C 2 t W t − V lim V t − V lim = V t − V lim + 12 ( H t − C t ) ( H t + C t ) W t V t − V lim = V t − V lim + 1 2 ( 1 − P t ) γ − P t β V t − V lim which is in nearly the same form as equation 9 , except for the 1 / 2 , and the derivation proceeds from there . REFERENCES 1 . R . Axelrod . The Complexity of Cooperation . Princeton University Press , 1997 . 2 . G . Beenen et al . Using social psychology to motivate contributions to online communities . In Proc . CSCW2004 , 2004 . 3 . D . Cosley et al . How oversight improves member - maintained communities . In Proc . CHI2005 , pages 11 – 20 , 2005 . 4 . R . M . Dawes and R . H . Thaler . Anomalies : Cooperation . The Journal of Economic Perspectives , 2 ( 3 ) : 187 – 197 , 1988 . 5 . W . Gautschi . Numerical analysis : an introduction . Birkhauser Boston Inc . , Cambridge , MA , USA , 1997 . 6 . R . Hardin . Collective Action . Johns Hopkins , 1982 . 7 . W . C . Hill , J . D . Hollan , D . Wroblewski , and T . McCandless . Edit wear and read wear . In Proc . CHI1992 , pages 3 – 9 , 1992 . 8 . S . J . Karau and K . D . Williams . Social loaﬁng : A meta - analytic review and theoretical integration . Journal of Personality and Social Psychology , 65 ( 4 ) : 681 – 706 , 1993 . 9 . C . Lampe and P . Resnick . Slash ( dot ) and burn : distributed moderation in a large online conversation space . In Proc . CHI2004 , pages 543 – 550 , 2004 . 10 . P . J . Ludford , D . Cosley , D . Frankowski , and L . Terveen . Think different : increasing online community participation using uniqueness and group dissimilarity . In Proc . CHI2004 , pages 631 – 638 , 2004 . 11 . D . W . McDonald and M . S . Ackerman . Expertise recommender : a ﬂexible recommendation system and architecture . In Proc . CSCW , pages 231 – 240 , 2000 . 12 . A . Mockus and J . D . Herbsleb . Expertise browser : a quantitative approach to identifying expertise . In Proc . ICSE2002 , pages 503 – 512 , 2002 . 13 . G . B . Newby and C . Franks . Distributed proofreading . In Proc . JCDL2003 , pages 361 – 363 , 2003 . 14 . M . Pazzani and D . Billsus . Learning and revising user proﬁles : The identiﬁcation of interesting web sites . Mach . Learn . , 27 ( 3 ) : 313 – 331 , 1997 . 15 . P . Resnick et al . Grouplens : an open architecture for collaborative ﬁltering of netnews . In Proc . CSCW1994 , pages 175 – 186 , Chapel Hill , NC , 1994 . 16 . B . K . Thorn and T . Connolly . Discretionary data bases : A theory and some experimental ﬁndings . Communication Research , 14 : 512 – 528 , 1987 . 17 . F . B . Vi´egas , M . Wattenberg , and K . Dave . Studying cooperation and conﬂict between authors with history ﬂow visualizations . In Proc . CHI2004 , pages 575 – 582 , 2004 . 18 . L . von Ahn and L . Dabbish . Labeling images with a computer game . In Proc . CHI , pages 319 – 326 , 2004 . 1046 CHI 2006 Proceedings • Social Computing 2 April 22 - 27 , 2006 • Montréal , Québec , Canada