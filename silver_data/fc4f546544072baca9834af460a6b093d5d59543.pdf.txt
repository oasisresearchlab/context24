Predicting Hate Intensity of Twitter Conversation Threads Qing Meng 1a , Tharun Suresh 1b , Roy Ka - Wei Lee d , Tanmoy Chakraborty c a Hohai University , Nanjing , China b IIIT Delhi , Delhi , India c Indian Institute of Technology Delhi , New Delhi , India d Singapore University of Technology and Design , Singapore Abstract Tweets are the most concise form of communication in online social media . Wherein a single tweet has the potential to make or break the discourse of the conversation . Online hate speech is more accessible than ever , and stiï¬‚ing its propagation is of utmost importance for social media companies and users for congenial communication . Most of the research has focused on classify - ing an individual tweet regardless of the tweet thread / context leading up to that point . One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hateful content has been published . This strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion en - suing in the postâ€™s replies . In this paper , we propose DRAGNET + + , which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future . Our model uses the semantic and propagat - ing structure of the tweet threads to maximize the contextual information leading up to and the fall of hate intensity at each subsequent tweet . We explore three publicly available Twitter datasets â€“ Anti - Racism contains the reply tweets of a collection of social media discourse on racist remarks during US political and COVID - 19 background ; Anti - Social presents a dataset of 40 million tweets amidst the COVID - 19 pandemic on anti - social behaviours with custom annotations ; and Anti - Asian presents Twitter datasets collated based on anti - Asian behaviours during COVID - 19 pandemic . All the cu - rated datasets consist of structural graph information of the Tweet threads . 1 Both authors contributed equally to this research . Preprint submitted to Knowledge - Based Systems May 16 , 2023 a r X i v : 2206 . 08406v4 [ c s . S I ] 14 M a y 2023 We show that DRAGNET + + outperforms all the state - of - the - art baselines signiï¬cantly . It beats the best baseline by an 11 % margin on the Person correlation coeï¬ƒcient and a decrease of 25 % on RMSE for the Anti - Racism dataset with a similar performance on the other two datasets . Keywords : Hate intensity prediction , Twitter reply chain , online social media , hate speech , graph neural network PACS : 0000 , 1111 2000 MSC : 0000 , 1111 1 . Introduction Motivation . The proliferation of social media has enabled users to share and spread ideas at a prodigious rate . While the information exchanges in social media platforms may improve an individualâ€™s social connectedness with online and oï¬„ine communities , these platforms are increasingly plagued with the rampant onslaught of provocative and toxic content . One such highly toxic content is â€˜hate speechâ€™ , deï¬ned by the Cambridge dictionary as â€œ public speech that expresses hate or encourages violence towards a person or group based on race , religion , sex , or sexual orientation . â€ Hate speech on social media has created dissension among online communities and culminated in oï¬„ine violent hate crimes [ 1 ] . Therefore , addressing the spread of hate speech on social media is critical . Major social media platforms such as Facebook and Twitter have made signiï¬cant eï¬€orts to combat the spread of hate speech on their platforms [ 2 , 3 ] . For example , the platforms have established clear policies regarding hate - ful conduct [ 4 , 5 ] , implemented mechanisms for users to report hate speech , and employed content moderators to detect hate speech . However , such ap - proaches are labor - intensive , time - consuming , and thus not scalable or sus - tainable in the long run [ 6 , 7 ] . Traditional machine learning and deep learning methods have also been proposed to automatically detect hate speech in on - line social media [ 8 , 9 , 10 ] . However , most of the existing methods are limited to classifying hate speech at the individual post level , ignoring the network and propagation eï¬€ects of hateful content on social media [ 11 ] . Ideally , social media content moderators would want to identify hateful posts and monitor posts and threads more likely to incite hatred . Consider the example of tweet propagation shown in Figure 1 . The initial source tweet is a benign tweet that reports the Russian invasion of Ukraine . How - 2 User 0 : Ukrainians wake up to full blown war as Russian troops move closer to Kyiv User 1 : Holy sh * t ! This is getting serious User 2 : rt : My heart goes to the Ukrainians User 4 : Well can ' t blame Putin for this ! Ukraine must be punished ! User 7 : Burn Ukraine to the ground for their foolish decision to go NATO ! User 5 : Russian are just helping to liberate Ukraine ! We will purge the neo - Nazi ! Kill them all ! User 3 : It is a just a " special Operation " User 6 : The world is going crazy ! World War 3 ! 15 m i n s 30 m i n s 45 m i n s 1 hou r 4 hou r s 8 hou r s 12 hou r s 1 da y 2 da ys Figure 1 : An example of a tweet propagation . The hateful tweets are highlighted in red . ever , as time evolves , we observe hateful content justifying the Russian in - vasion and promoting violence against the Ukrainians . Existing automated hate speech detection methods could help content moderators detect hate - ful tweets . Nonetheless , content moderators could signiï¬cantly improve the eï¬€ectiveness and eï¬ƒciency of moderation by prioritizing posts likely to gen - erate a large number of hateful tweets in their retweet or reply threads . This a challenging problem as the post that induces hateful content may not be hateful itself ( e . g . , source tweet in Figure 1 ) . Early detection of tweets likely to generate a large amount of hateful content becomes a critical real - world issue . Very few studies examined the hate intensity of Twitter conversation threads for early hate speech detection . Lin et al . [ 12 ] manually categorized Twitter conversation threads into diï¬€erent levels of hatred and proposed a deep learning model to forecast and classify tweets into the pre - deï¬ned hatred levels . Recently , Sahnan et al . [ 13 ] introduced the hate intensity prediction task and proposed DRAGNET , a deep stratiï¬ed learning framework that predicted the intensity of hatred that a root tweet can fetch through its sub - sequent replies . Nevertheless , DRAGNET assumes a linear sequence for the conversation thread in its model , in which tweets in conversation threads are arranged in a chronological sequence . This neglects the tree structural infor - mation inherently present in a conversation thread ; there could be multiple branches in a conversation thread similar to the example shown in Fig . 1 . The structural information may be an important feature that could better inform and improve hate intensity prediction . For instance , we may notice a heated debate along the branches of the conversation threads ; therefore , modeling the dynamics of the tree structure of the conversation thread may 3 aid us in better predicting the eventual hate intensity of the conversation thread . Research Objectives . In this paper , we aim to ï¬ll this research gap and extend our earlier work [ 13 ] by proposing DRAGNET + + that models mul - tifaceted information to forecast the hate intensity of a Twitter conversation thread . Speciï¬cally , we deï¬ne our problem statement as follows â€“ given a root tweet and a few of its initial replies , can we predict the hate intensity of the subsequent replies of the tweet ? To model this problem , we ï¬rst quantify the hate intensities of tweets in the conversation thread and express them as a series of hate intensities , transforming the task into a time - series problem . At a high level , DRAGNET + + adopts a similar stratiï¬ed learning framework as suggested in [ 13 ] to model the hate intensity proï¬les of Twitter conversa - tion threads and categorize them into clusters of varying hate intensity . In addition to the content sentiment and temporal information , DRAGNET + + also captures structural information of the conversation thread . Speciï¬cally , DRAGNET + + adopts Graph Neural Networks ( GNN ) to learn the semantic and propagation structure of conversation threads . As the prediction of hate intensity in conversation threads is a rela - tively new research problem , we conduct thorough experiments and analyses to evaluate DRAGNET + + . We collate three publicly available real - world datasets â€“ Anti - Racism with 3 . 5 k root tweets , Anti - Social with 668 k root tweets , and Anti - Asian with 218 k root tweets . We extensively analyze these datasets to examine the hate intensities of real - world Twitter conversation threads . We benchmark DRAGNET + + against DRAGNET and six other baselines on the hate intensity task . Finally , we examine case studies and conduct ablation studies to explain the advantages and limitations of DRAG - NET + + . Contributions . We summarize our contributions below : â€¢ We analyze the hate intensity in three large - scale real - world Twitter datasets . This is the ï¬rst large - scale study that examines hate intensity in Twitter conversation threads . â€¢ We propose DRAGNET + + , a stratiï¬ed learning framework with struc - tural graph augmentation to perform hate intensity prediction . â€¢ We conduct extensive experiments and show that DRAGNET + + con - sistently outperforms state - of - the - art methods in the early prediction 4 of hateful conversation threads . Speciï¬cally , DRAGNET + + outper - forms the best baseline by at least 34 % on Pearson Correlation Coef - ï¬cient ( PCC ) , reduction of at least 200 % on Root Mean Square Er - ror ( RMSE ) , and 7 % lower on Mean Forecast Error ( MFE ) across the datasets . Comparing the performance of DRAGNET + + with DRAG - NET shows an average improvement of 8 . 33 % on PCC and an average reduction of 16 . 07 % , and 18 . 20 % on RMSE and MFE , respectively . â€¢ Our case studies demonstrate the viability of predicting hate speech early to prevent the propagation of hateful content . Organization of the paper : We start by presenting the developments until recently related to hate speech detection and time series forecasting in Section 2 . Following this , we deï¬ne the formulation of hate intensity proï¬les appropriate to our problem statement in Section 3 . We then move on to detailing our model in Section 4 . Further , we analyze the various datasets and present their statistics and observations in Section 5 . We discuss a brief overview of various baselines in Section 6 . 1 . Section 6 . 2 presents a detailed comparison , analysis of the model performance , and the eï¬€ect of various parameters used to ï¬ne - tune the model . We conclude the paper in Section 7 . Reproducibility . The source codes of DRAGNET + + and all the base - lines and the dataset are available at the following link : https : / / github . com / LCS2 - IIITD / Predicting - Hate - Intensity . 2 . Related Work 2 . 1 . Causes of Hate Speech There are several reasons for individuals to harbor hatred towards each other or a particular group of people . According to Navarroâ€™s [ 14 ] discussion , hate can arise when someone perpetrates harm or discrimination against oth - ers . Hate is often characterized by the devaluation of the victim , which can escalate to the point of elimination in extreme cases . This notion demon - strates the snowball eï¬€ect commonly associated with hatred as arguments progress . In the context of the internet , John et al . [ 15 ] highlight that people tend to self - disclose more frequently or intensely in online media , a phenomenon known as the online disinhibition eï¬€ect . Furthermore , Lind - say et al . [ 16 ] argue that social media , a highly interconnected platform , 5 is driven by algorithms and primarily proï¬ts from user engagement . As a result , it unintentionally propagates extreme content under the guise of serv - ing user interests . The causes of hate speech are multifaceted , and some of them are introduced due to technological advancements . While we do not explicitly study the cause of hate speech in individual tweets , our proposed model possesses the capability to identify â€high - riskâ€ tweets and reply chains , alerting content moderators to conduct further analysis and gain a deeper understanding of the causes of hate speech . 2 . 2 . Hate Speech Detection in Social Media Industry and academics have paid close attention to the work on hate speech detection . With the advancement of deep learning , neural language processing systems that can automatically extract features from the text have seen a lot of success [ 17 , 18 , 19 ] . This opens up new possibilities for hate speech detection [ 20 , 21 , 22 , 23 , 24 , 25 ] . Gamback et al . [ 7 ] , for example , used Convolutional Neural Networks ( CNN ) to classify hate speech by ex - tracting word similarities . Similar research was done by Park et al . [ 26 ] , who presented the HybridCNN model to investigate word and character combina - tion patterns to detect hate speech . On the other hand , Del et al . [ 27 ] used Long Short Term Memory ( LSTM ) to record the long - term dependencies of words in phrases to diï¬€erentiate hatred remarks . Badjatiya et al . [ 21 ] eval - uated the use of the LSTM model in conjunction with the Gradient - Boost Decision Tree ( GBDT ) to perform hate speech classiï¬cation and found that it greatly enhanced performance . Zhang et al . [ 22 ] presented a CNN + GRU network architecture to investigate word dependency for recognizing hate speech tweets , combining the beneï¬ts of CNN - based models and Gate Re - current Unit ( GRU ) - based models . However , most research focuses on learn - ing a particular textual property while ignoring other valuable data . Cao et al . [ 28 ] introduced DeepHate , a deep learning - based hate speech detection al - gorithm for mining multi - faceted textual representations . Lee et al . [ 29 ] later developed DisMultiHate , a hateful meme classiï¬cation model that can learn both textual and visual information . Recently , Awal et al . [ 25 ] proposed a meta - learning - based framework ( HateMAML ) that can eï¬€ectively detect hate speech in eight diï¬€erent low - resource languages . Pre - trained language models such as BERT [ 30 ] and GPT [ 31 ] can ex - tract external information from massive volumes of text data . These models have also been used in hate speech detection models with promising results [ 32 , 33 ] . For example , Awal et al . [ 33 ] used the pre - trained BERT model as a 6 shared layer and created AngryBERT , a multi - task learning model that can jointly detect hate speech and classify sentiment . Nonetheless , most previ - ous research works have focused on categorizing hate speech . A few research looked into how hate speech spreads through social media . In a recent dis - cussion , Dahiya et al . [ 34 ] discussed the necessity of anticipating the hatred intensity of tweets . Lin et al . [ 12 ] classiï¬ed the tweets into diï¬€erent levels of hatred and suggested that the HEAR model tracks posts likely to cause hate speech . Closer to our work is DRAGNET [ 13 ] , which is a deep stratiï¬ed learning framework that predicts the hate intensity of a conversation thread based on what a root tweet can fetch through its subsequent replies . DRAG - NET models the linear sequence for the conversation thread chain , where the tweets are arranged chronologically . Such a modeling approach ignores the conversation threadâ€™s inherent structural information propagation informa - tion . We address this limitation and propose DRAGNET + + which considers the Twitter conversation threadâ€™s structural information to improve hate in - tensity prediction . On the other hand , some comprehensive datasets are proposed in hate speech shared tasks . They focus on niche aspects of hate speech . For ex - ample , Basile et al . [ 35 ] released a task focusing on hate speech against immigrants and women in a multilingual setup . Sanguinetti et al . [ 36 ] for - mulated testing out of domain dataset between tweets and news headlines . They explored the prevalence of verbless fragments in most hate speech texts . Furthermore , potential hate speech spreaders can be inferred from an indi - vidualâ€™s Twitter feed [ 37 ] . Narrowing a step deeper , Pavlopoulos et al . [ 38 ] discussed toxic span detection from previously annotated hateful comments by re - annotating them at the span level . They gave a more ï¬ne - grained understanding of spans contributing to toxicity . 2 . 3 . Time Series Forecasting Modeling The hatred intensity prediction problem can be reduced to a time se - ries prediction task , where we forecast the hate intensity of the conversation thread in the future . Time series models are designed to predict values or trends over time in the future and have been extensively studied in many ï¬elds such as transportation [ 39 ] , ï¬nance [ 40 ] , event forecasting , [ 41 ] and dis - ease transmission [ 42 ] . Therefore , we also introduce the existing studies of time series forecasting ( TSF ) models . Traditional TSF methods , such as ARMA [ 43 ] , exponential smoothing [ 44 ] , and linear space models , have been the basis of much work and achieved 7 better performance . With the rapid development of deep learning technology in recent years , more and more end - to - end TSF models have been proposed . Compared to traditional methods , deep learning - based models such as CNN , RNN , and LSTM can automatically extract features from the input without domain expertise , widely accepted in many ï¬elds . For example , Oord et al . [ 45 ] adopted CNN in raw audio generation and proposed the WaveNet model . Based on WaveNet , Borovykh et al . [ 46 ] utilized CNN to perform conditional time series forecasting tasks . The model shared ï¬lter weights assuming the hidden patterns are time - invariant at each step . In addition , the RNN contains an internal memory state , which can also retain information about previous time steps [ 47 ] . However , due to vanishing gradients , the performance of classical RNN - based models degrades as the sequence length increases . Later , the LSTM model overcomes the long - term dependency problem to some extent . Elsworth et al . [ 48 ] used LSTM for TSF tasks , improving performance and robustness . However , most of the above models only focus on one - step predictions . Some recent studies [ 49 , 50 ] generalize sequence - to - sequence ( Seq2Seq ) mod - els and propose them for multi - step time series forecasting methods . Fan et al . [ 50 ] exploited attention mechanisms to capture the temporal context information extracted by the RNN encoder . Combined with a bidirectional LSTM decoder , the model can generate multiple future horizons simulta - neously . Later , researchers proposed new architectures [ 51 , 52 ] to reduce error accumulation in multi - step prediction . For instance , Sen et al . [ 51 ] combined global matrix factorization models and local temporal networks to capture latent patterns in time series . In recent years , the Transformer ar - chitecture has been proposed for natural language processing tasks and has also achieved success on time series data [ 52 , 53 ] . Moreover , some studies [ 54 , 55 ] proposed models to estimate the probability distribution of future time series . For example , Yuan et al . [ 54 ] and Koonchli et al . [ 56 ] employed Generative Adversarial Networks ( GANs ) to predict future values . Salinas et al . [ 55 ] proposed a DeepAR model for probabilistic prediction based on auto - regressive recurrent networks . 3 . Preliminaries Hate Intensity Deï¬nition . Table 1 summarizes the denotations of the important notations . Let an ordered sequence of ï¬rst t number of replies to a root tweet Ï• be T Ï• 1 , t = < c Ï• 1 , c Ï• 2 , Â· Â· Â· , c Ï•t > , where c Ï• i refers to the i th reply . 8 Table 1 : Important notations and denotations . Symbol Deï¬nition Ï• Root tweet c Ï•i i th reply to root tweet Ï• R p , q Hate value set of q reply tweets of the p th root tweet R âˆ— p , q The set of hate values for the q reply tweets of the p th root tweet predicted by the autoencoder S s ( 1 , n ) Cosine similarity set between root tweet and n reply tweets R s ( 1 , n ) Set of hate intensity proï¬les j Number of clusters Î´ Window size t h Number of replies in history t f Index of the last reply tweet in the conversation thread n Maximum length of the conversation thread s Total number of conversation threads N X h Dimension of encoded history latent vector N X f Dimension of encoded future latent vector X Latent vector of R p , q X âˆ— Latent vector of R âˆ— p , q C c List of cluster centers P ( C ci ) Likelihood of belongingness to i th cluster identiï¬ed with C ci P âˆ— ( C ci ) Predicted weight for i th cluster centre C ci to calculate X c X d Pre - processed prior vector X h , X f The representation of historical and future conversation threads X ch , X cf The representation of pseudo historical and pseudo future conversation threads E h ( Â· ) The encoder model of historical conversation threads E f ( Â· ) The encoder model of future conversation threads GM ( Â· ) Fuzzy Clustering PR ( Â· ) Prior model FP ( Â· ) Future Predictor FP d ( Â· ) 1 st segment of Future Predictor FP p ( Â· ) 2 nd Future Predictor D ( Â· ) Decoder T E ( Â· ) Tree Encoder As the replies in the original dataset have no ground - truth hate intensity , we quantify the hate intensity of each reply using a weighted sum of two measures as suggested in [ 34 ] : H = w H c ( c ) + ( 1 âˆ’ w ) H l ( c ) , ( 1 ) 9 where H c indicates the probability that the reply is hateful , and it is cal - culated by a state - of - the - art hate speech detection model ( we will elaborate it in Section 6 . 2 ) . H l is the average score for all words in a reply from a model - independent hate lexicon that comprises 2 , 895 words as proposed in [ 57 ] . w âˆˆ [ 0 , 1 ] is a hyper - parameter that balances two quantities of the hate intensity score . As H c âˆˆ [ 0 , 1 ] and H l âˆˆ [ 0 , 1 ] , the ï¬nal hate intensity score of a reply H is still between 0 and 1 . We do not ï¬lter any tweet threads based on their hate intensity . Therefore , each conversation thread T can be mapped to a sequence of hate intensity scores , H ( T Ï• 1 , t ) = < H ( c Ï• 1 ) , H ( c Ï• 2 ) , Â· Â· Â· , H ( c Ï•t ) > ( 2 ) To avoid noise and drastic ï¬‚uctuations that are not in line with reality , we further smooth the hate intensity sequence by utilizing a rolling average operation with window size Î´ [ 34 ] . A window consists of Î´ consecutive replies , and the window that starts from the k th reply is denoted by T Ï•k , k + Î´ . Finally , the hate intensity of a window for the tweet Ï• is measured as , H ( T Ï•k , k + Î´ ) = (cid:88) c âˆˆT Ï•k , k + Î´ H ( c ) = w (cid:88) c âˆˆT Ï•k , k + Î´ H c ( c ) + ( 1 âˆ’ w ) (cid:88) c âˆˆT Ï•k , k + Î´ H l ( c ) , ( 3 ) where H ( T Ï•k , k + Î´ ) âˆˆ [ 0 , Î´ ] , and Î´ is the window size . Sentiment Features . The emotional feedback of users is reï¬‚ected in the sentiment features of reply posts , which provide both for and against arguments of the original tweet . Therefore , we use the cosine similarity be - tween the sentiment embedding of the root tweet ( Ï• ) and its accompanying replies ( c Ï• 1 , c Ï• 2 , Â· Â· Â· ) to capture the sentiment context of a conversation thread , denoted as CS ( c i ) = CosineSim ( Embed ( c Ï•i ) , Embed ( Ï• ) ) . The sentiment em - bedding is the second last fully - connected layer from the pre - trained XLNet model [ 58 ] for sentiment classiï¬cation . To smooth the value and eliminate the eï¬€ect of noise , we also apply the rolling average operation to the senti - ment context sequences CS ( T Ï• 1 , t ) with the same window size Î´ as performed on H ( T Ï• 1 , t ) . Problem Deï¬nition . Given ( i ) a root retweet Ï• , ( ii ) its last t h his - torical replies T Ï• 1 , t h = { c Ï• 1 , c Ï• 2 , Â· Â· Â· , c Ï•t h } , ( iii ) the corresponding hate intensity sequence { H ( T Ï•k , k + Î´ ) | k âˆˆ [ 1 , 2 , Â· Â· Â· , t h âˆ’ Î´ ] } , and ( iv ) G Ï• = ( V , E ) referring to the adjacency matrix of the propagation tree , where V = { Ï• , c Ï• 1 , c Ï• 2 , Â· Â· Â· , c Ï•t } , and e ij âˆˆ { 0 , 1 } denote the retweeting / reply relationships between tweets 10 ( i . e . , root tweet or replies ) , we aim to predict the hate intensity of the up - coming replies c Ï•t (cid:48) in the propagation tree of the root tweet Ï• . However , in corresponding to the historical hate intensity sequence , we consider predict - ing the hate intensity for each window of c Ï•t (cid:48) , denoted by H ( T Ï•t (cid:48) , t (cid:48) + Î´ ) , instead of directly predicting the hate intensity of each reply . 4 . Methodology This section presents our proposed hate intensity prediction method , DRAGNET + + . It is a deep stratiï¬ed learning [ 59 ] method that splits hetero - geneous data points ( in this case , Twitter conversations ) into homogeneous clusters / strata before training a deep regressor on each stratum to predict hate intensity . Figure 2 illustrates the overall architecture of DRAGNET + + . The two - dimensional vector forms the training set of window - wise hate intensity pro - ï¬le and sentiment context value sequences . R s ( 1 , n ) = { R ( p , q ) : 1 â‰¤ p â‰¤ s , 1 â‰¤ q â‰¤ n } R ( p , q ) = { H ( T Ï• p k , k + Î´ ) : 1 â‰¤ k â‰¤ q âˆ’ Î´ } S s ( 1 , n ) = { S ( p , q ) : 1 â‰¤ p â‰¤ s , 1 â‰¤ q â‰¤ n } S ( p , q ) = { CS ( T Ï• p k , k + Î´ ) : 1 â‰¤ k â‰¤ q âˆ’ Î´ } ( 4 ) where s represents the total number of conversations , and n represents the maximum conversation length . The elements R ( p , q ) âˆˆ R s ( 1 , n ) and S ( p , q ) âˆˆ S s ( 1 , n ) represent the p th data point , where the conversation thread is of length q ( Ï• p represents the p th root tweet ) . DRAGNET + + uses an autoencoder to learn low - dimensional latent rep - resentations for the hate intensity proï¬le of conversation threads . Speciï¬cally , the model learns two alternative latent representations : X h , which represents the ï¬rst few replies , and X f , which represents the future hatred trend for the remaining replies . Once these representations , X h and X f , are combined , the model uses an unsupervised setting to apply a fuzzy clustering technique to give cluster membership probabilities and cluster centers to each conversa - tion thread . The number of clusters is determined by a hyper - parameter j . Following this , the model trains with historical node and propagation struc - ture G ( 1 , t h ) Ï• using the tree encoder to learn the structure embedding X hs . The model then trains a new deep neural network unit to predict cluster mem - bership probabilities given X h , X hs and S s ( 1 , th ) , which assign cluster centers 11 â„° ! ( . ) â„° " ( . ) Future Encoder Past Encoder Fuzzy Clustering ğ’¢â„³ ( . ) Decoder ğ’Ÿ ( . ) Future Predictor Tree Encoder ğ’¯â„° ( . ) Prior model ğ’«â„› ( . ) MLP MLP MLP MLP + ğ‘‹ " ğ‘‹ ! + ğ‘† # ( % , â€™ ! ) ğ‘… # ( % , â€™ ! ) ğ‘… # ( â€™ ! ) % , â€™ " ) ğ‘ƒ âˆ— ( ğ¶ " # ) ğ‘ƒ âˆ— ( ğ¶ " $ ) ğ‘ƒ âˆ— ( ğ¶ " % ) â€¦ ğ‘ƒ âˆ— ( ğ¶ " & ) ğ‘‹ ğ‘‹ ! âˆ— ğ‘‹ " ğ‘‹ âˆ— + + Concatenation âˆ’ Subtraction MLP + âˆ’ ğ‘‹ â€™ " ğ‘‹ â€™ " ğ‘‹ ( " P ( ğ¶ , % , ğ¶ , - , â€¦ , ğ¶ , . ) ğ¶ , ğ‘‹ , â„±ğ’« ( . ) ğ‘… # , % & ğ‘… # , % âˆ— ğ‘‹ ! " ğ‘… # ( % , â€™ ! ) , ğº / ( % , â€™ ! ) + ) âˆˆ [ , , & ] ğ¶ ( ) ğ‘ƒ âˆ— ğ¶ ( ) Figure 2 : The overall framework of DRAGNET + + . The autoencoder is trained on hate - intensity proï¬les of the entire conversation thread . Using the trained autoencoder , the history and future latent representations are concatenated and clustered using the fuzzy clustering algorithm GM ( Â· ) . In the Prior model , PR ( Â· ) , graph representations , history la - tent representations and sentiment features are concatenated to generate the Prior Knowl - edge vector . On inference , ( a ) the history latent representation , ( b ) sentiment similarity features of the history , and ( c ) the graph representation of the conversation thread are used to predict the future hate intensity proï¬le . for a new conversation thread . Finally , using X h and P ( C c 1 , C c 2 , Â· Â· Â· , C cj ) , a novel deep regressor predicts the latent representation of the future hate trend X âˆ— f , which , when coupled with X h , is transformed to the whole hate trend by the decoder trained during the autoencoder phase . Figure 3 illustrates an example processing of an abstract tweet thread us - ing our model . Four representations are derived from the input tweet thread : History , Future , Sentiment and Graph representations . The history and fu - ture representations are fed into the Fuzzy Clustering algorithm to predict the cluster centers . In parallel , a Prior model is trained on history , sentiment and graph representations forming the prior knowledge . This is combined with cluster centers to learn / assign the cluster membership probabilities in the fuzzy associations step . Further , these probabilities are combined with history representations in the Future Predictor to predict the latent represen - tation of future hate trends . In the ï¬nal step , the future latent representation is concatenated with the history representation and fed into the decoder to predict the overall hate trend of the entire conversation thread . 12 Tweet 0 Tweet 1 Tweet 2 Tweet 3 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Tweet 0 Tweet 1 Tweet 2 Tweet 3 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Î´ Tweet 0 Tweet 1 Tweet 2 Tweet 3 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Tweet 0 Tweet 1 Tweet 2 Tweet 3 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Î´ Tweet 5 Tweet 4 Tweet 7 Tweet 6 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Tweet 0 Tweet 1 Tweet 2 Tweet 3 Tweet 5 Tweet 4 Tweet 7 Tweet 6 Input thread Tree Encoder Past Encoder Future Encoder Fuzzy Associations Future Predictor Fuzzy Clustering Decoder Tweet 0 Tweet 1 Tweet 2 Tweet 3 Concatenation Sentiment Encoder Prior model Figure 3 : An example processing of an abstract tweet thread using our model . We show the various stages and representations learned by our model and how they combine to proï¬le the ï¬nal hate intensity of future replies . We diï¬€erentiate the earlier and future tweets in the input threads using orange and green colors . Î´ is the size of the sliding window of tweets . The tree encoder shows diï¬€erences between the node and edge representations . Fuzzy clustering and fuzzy associations share a common color code to show the assignment of cluster memberships . The ï¬nal decoder block represents some tweets being assigned high hate intensity scores ( i . e . , tweets in red ) 4 . 1 . Time - series Representative Learning The vector of the conversation thread R s ( 1 , n ) can be viewed as a collection of irregularly lengthening time series ( window - wise hatred intensity proï¬les ) . The Dynamic Time Warping ( DTW ) distance metric and its derivatives are used to group comparable trends together in state - of - the - art approaches for clustering irregular time series [ 60 ] . DTWâ€™s precision in mapping time se - ries similarity is beneï¬cial , but the noisy and volatile character of the data points in the present research prevents it from showing high eï¬ƒciency in clustering comparable hatred patterns into a single stratum . We propose an autoencoder to translate each conversation thread R ( p , q ) in R s ( 1 , n ) to a low - dimensional latent representation in order to capture a more suitable representation of the time series . We also propose a multi - encoder strategy instead of a single encoder - decoder design as proposed in [ 61 ] . 13 4 . 2 . Proposed Autoencoder To capture the hate intensity series , the autoencoder module seeks to learn a low - dimensional representation . In DRAGNET + + , we speciï¬cally construct two encoders and a single decoder . Using the two encoders , the model can learn the representations of historical and future hate intensity se - quences separately . The decoder then uses the representations to reconstruct the original hate intensity sequence . 4 . 2 . 1 . Encoder Several studies provide models for univariate and multivariate [ 62 , 63 ] time series to mine the hidden patterns of various sequences . We use a state - of - the - art Inception - Time [ 63 ] module to automatically extract features and represent hate intensity sequences , denoted as : X m = E t ( R s ( 1 , n ) ) , ( 5 ) where X m âˆˆ R s Ã— n Ã— 4 is the multivariate intermediate representation of R s ( 1 , n ) . The Inception - Time module is denoted by E t ( Â· ) . After the Inception - Time module , we ï¬‚atten X m and use multilayer perceptrons as the classiï¬cation stage . Finally , the representations are written down as follows : X o = E lt ( flatten ( X m ) ) , ( 6 ) Here , X m is transformed into a one - dimensional vector using flatten ( Â· ) . E lt ( Â· ) denotes multilayer perceptrons that have been trained to learn the best rep - resentation of the hate intensity sequence . We develop the history encoder ( E h ) and the future encoder ( E f ) based on Equations ( 5 ) and ( 6 ) to encode hate intensity sequences of both historical and future conversation threads : X h = E h ( R s ( 1 , t h ) ) X f = E f ( R s ( t h + 1 , t f ) ) ( 7 ) where X h âˆˆ R s Ã— N X h and X f âˆˆ R s Ã— N X f denote the historical and future latent representations , respectively . N X h and N X f represent the length of X h and X f , respectively . 14 4 . 2 . 2 . Decoder Although we use two encoders to convert the hate intensity of each con - versation thread into two latent representations , X h and X f , we only employ one decoder , D ( Â· ) to return the latent representation to the original input . The decoder is trained to reconstruct the original hate intensity proï¬le per conversation thread by concatenating the two latent representations . The decoderâ€™s operation can be summarised as follows : R âˆ— s ( 1 , n ) = D ( [ X h âŠ• X f ] ) , ( 8 ) where R âˆ— s ( 1 , n ) is the reconstructed hate intensity sequence containing both historical and future hate intensity sequences . 4 . 3 . Fuzzy Associations The hate intensity proï¬les of conversation threads in our dataset are noisy , volatile , and lack a discernible pattern , as mentioned in Section 4 . 1 . Our goal is to group similar proï¬les using low - dimensional latent representa - tions of data obtained by autoencoder ( as explained in Section 4 . 2 ) . Recent research that supports this technique attests to the validity of deep learning - based modelsâ€™ eï¬€ectiveness in learning hidden features from time - series data for various applications [ 64 , 65 ] . We apply a clustering strategy over the latent representations to aggregate heterogeneous hate intensity proï¬les into ( near - ) homogeneous clusters . Finding meaningful correlations in data is un - duly dependent on the number of clusters j and the cluster centers in this unsupervised context . As a result , rather than restricting each proï¬le to a single cluster , we apply a fuzzy clustering approach and use the membership probabilities as a feature embedding . We deï¬ne the combined latent space X as , X = X h âŠ• X f . ( 9 ) Rather than using a hard clustering strategy , in which each proï¬leâ€™s asso - ciation to the nearest cluster is ï¬xed , we use cluster membership probability using a fuzzy clustering approach instead . The membership probability vec - tor , denoted by P ( C c 1 , C c 2 , . . . , C cj ) , reï¬‚ects the associative probabilities of each cluster with the provided chain , where C ci denotes the cluster center of the i th cluster . 15 Historical Reply Tree GNN Node Embedding Propagation Structure PoolingLayer Tree Embedding Tweet Reply ( a ) The tree encoder T E ( Â· ) ğ‘¤ ! â€¦ ğ‘¤ " Hate Intensity Prediction MLP Bi - GRU Pre - trained embeddings ( b ) The Node Em - bedding Module Figure 4 : The tree encoder T E ( Â· ) for generating graph - level representations for each con - versation thread . 4 . 3 . 1 . Fuzzy Clustering We cluster on the combined latent representation X because our goal is to uncover correlations between each hate intensity proï¬le and homogeneity groupings . To locate the clusters C c , which is the collection of cluster centers , we use a state - of - the - art fuzzy clustering model [ 66 ] , indicated by GM ( Â· ) : C c = GM ( X ) = ( C c 1 , C c 2 , Â· Â· Â· , C cj ) . ( 10 ) where j is the pre - deï¬ned number of clusters , and C ci is the cluster centre of the i th cluster . 4 . 4 . Tree Encoder Twitter conversation threads typically contain numerous replies ( i . e . , tweets ) with various propagation topological structures . These propagation struc - tures capture the relationships between the tweets ; modeling the topological structures would enrich the conversation threadâ€™s representation for down - stream machine learning tasks [ 67 , 68 ] . For our hate intensity prediction task , we design a tree encoder T E ( Â· ) to fuse the information of tweet content and the structural information to represent the conversation tree . The tree encoding process is presented in Fig 4 ( a ) . Speciï¬cally , to learn a conversa - tion threadâ€™s representation , we ï¬rst learn the node embedding by designing a sub - module using a hate intensity prediction task to learn the tweetâ€™s rep - resentation . Next , we leverage a Graph Neural Network ( GNN ) [ 69 ] to learn the conversation tree embedding by fusing the conversation tree structural information and the node embedding . Learning Node Embedding . We adopt the tweet - level hate intensity prediction task as the supervised signal for learning the embedding of the 16 node ( i . e . , tweet ) . We ï¬rst leverage pre - trained Word2Vec [ 70 ] word embed - ding to represent the words in individual tweets . Next , we use a two - layer bidirectional gated recurrent unit ( BiGRU ) to learn the individual postâ€™s representation , and the tweet - level hate scores are used as training signals . Speciï¬cally , the post representations are fed into a two - layer MLP classiï¬er to predict the postâ€™s hate label . The model is formulated as follows : w i = Bi - GRU ( < w 0 , w 1 , Â· Â· Â· , w N > ) , Ë† p k = Att ( < w 0 , w 1 , Â· Â· Â· , w N > ) , Ë† y k = MLP ( Ë† p k ) , ( 11 ) where Ë† y k is the predicted hate score for the k tweet . We can train the modelâ€™s parameters with the loss function below and the Adam optimizer . We can obtain the tweet - level embedding Ë† p k . The loss function ( L h ) is deï¬ned as follows : L h = N (cid:88) k = 1 ( y k âˆ’ Ë† y k ) 2 + reg ( Î˜ ) ( 12 ) where N is the number of tweets in all conversation threads , reg ( Â· ) is a regularization function to alleviate overï¬tting , and Î˜ is a parameter for the ï¬ne - tuning model . Learning Tree Embedding . The goal of learning the tree embedding of the conversation thread is to leverage useful propagation structural infor - mation to perform early hate intensity prediction . Working towards this goal , we leverage GNNs to model the tree structure in the conversation threads . Speciï¬cally , GNNs model the conversation thread structural information as a directed graph with the node represented using the node embeddings learned from the previous section : P ( l ) = Ïƒ ( D âˆ’ 1 / 2 AD âˆ’ 1 / 2 P ( l âˆ’ 1 ) W ( l âˆ’ 1 ) ) ( 13 ) where W is the trainable parameter , P ( l âˆ’ 1 ) âˆˆ R n Ã— d is the representation of all tweets , P 0 is initialized by all tweet - level embeddings Ë† p k , k âˆˆ { 1 , 2 , Â· Â· Â· , t h } learned using formula ( 11 ) , A and D are the adjacency matrix and degree matrix of the conversation thread tree structure , respectively . Finally , using the average pooling operation , we can obtain the graph - level embedding of the reply tree , denoted X hs . 17 4 . 5 . Boosting Prediction with Prior Knowledge The task of predicting the hate intensity of upcoming replies , provided limited history R âˆ— s ( 1 , th ) , is strenuous even for state - of - the - art deep learning models due to the noisy , volatile , and heterogeneous nature of the time - series hate intensity proï¬les . To address this , we introduce the notion of prior knowledge to the prediction component of our pipeline as the weighted sum of the cluster centers , where the weights correspond to the cluster membership probabilities for the new chain , denoted by P âˆ— ( C c 1 , C c 2 , . . . , C cj ) . We deï¬ne prior knowledge as follows : X c = (cid:88) i âˆˆ ( 0 â‰¤ i â‰¤ j ) C ci P âˆ— ( C ci ) ( 14 ) Note that C c is calculated over X , i . e . , the combined latent representation . Therefore , to calculate the complete membership probability vector for a new chain , we cannot directly use the fuzzy clustering model GM ( Â· ) . Instead , we construct a prior model PR ( Â· ) to predict the membership probabilities for new chains using only the latent representation of the history X h , the sentiment feature S s ( 1 , t h ) , the historical replies R s ( 1 , th ) and propagation tree structure G ( 1 , t h ) Ï• . PR ( E h ( R s ( 1 , th ) ) , S s ( 1 , th ) , T E ( R s ( 1 , th ) , G ( 1 , t h ) Ï• ) ) = P âˆ— ( C c 1 , C c 2 , Â· Â· Â· , C cj ) ( 15 ) The precision of the predictions by the prior regression model is measured by comparing P âˆ— ( C c 1 , C c 2 , . . . , C cj ) against P ( C c 1 , C c 2 , . . . , C cj ) . 4 . 5 . 1 . Estimating Latent Representation of Upcoming Conversation Threads The designed decoder needs the latent representations of historical and future conversation threads to reconstruct the complete hate intensity pro - ï¬le . However , our primary objective is to predict the future hate intensity proï¬le based on historical information . To provide the decoder with the abil - ity to perform prediction , we propose a future representation predictor that uses historical information to estimate the latent representation of future hate intensity proï¬le X f . Then , combined with the latent representation of historical conversation threads , the output can be fed into the decoder . Speciï¬cally , we utilize the prior knowledge extracted by the fuzzy clus - tering module and the latent representation X c of the historical conversation 18 threads . To avoid the estimation problem being unduly inï¬‚uenced by prior knowledge , we design the predictor in two steps . In the ï¬rst step , for each conversation thread , the prior latent representation of historical conversation threads X ch contains the required information to predict the latent represen - tation of future conversation threads . Moreover , we have the expected latent representation X h encoded from the initial historical replay chains . As a re - sult , we use the diï¬€erence operator on the expected ( X h ) and estimated priors ( X ch ) of the historical conversation threads to evaluate the deviation of these two representations . Secondly , a single - layer perceptron FP d ( Â· ) is used to learn the representation ( X d ) in a hidden space , indicating the dissimilarity between the prior knowledge and the history conversation threads . X s = X h (cid:9) X ch X d = FP d ( X s ) ( 16 ) We ï¬nally obtain the X hc vector by concatenating the provided input X h , the pre - processed prior X d and X cf as , X hc = X h âŠ• X d âŠ• X cf . The second stage is the deep linear transformation model FP p ( Â· ) that predicts the upcoming hate intensity in the latent space X âˆ— f as follows : X âˆ— f = FP p ( X hc ) ( 17 ) 4 . 6 . Decoding the Future We use the decoder module trained in Section 4 . 2 . 2 to forecast entire hate intensity proï¬les ( i . e . , R âˆ— s ( 1 , n ) ) based on the expected latent representation of the upcoming hate intensity X âˆ— f . In particular , we concatenate the original latent representation ( X h ) of historical hate intensity sequences with the expected future representation ( X âˆ— f ) , denoted as : X âˆ— = X h âŠ• X âˆ— f ( 18 ) where X âˆ— is the predicted hate intensity proï¬le of the upcoming conversation thread in the latent space . The decoder is designed to be a mirror of the encoder . The decoder is deï¬ned as follows : R âˆ— s ( 1 , n ) = D ( X âˆ— ) ( 19 ) where R âˆ— s ( 1 , n ) denotes the expected hate intensity proï¬le for the next future conversation thread . Finally , we compare the predicted and original hate intensity score sequences ( i . e . , R s ( 1 , n ) ) to evaluate and report the performance on several metrics . 19 4 . 7 . Comparison with DRAGNET From the modeling standpoint , DRAGNET + + has advanced DRAG - NET [ 13 ] on two aspects : DRAGNET + + exploits the tweet - level seman - tics and the conversation - level structural information to improve hate inten - sity prediction . DRAGNET leveraged sentiment features , calculated as each tweetâ€™s similarity in the conversation threads to the root tweet . However , this approach largely ignores the complex relationships among the tweets in the conversation threads . Furthermore , as illustrated in our earlier exam - ple in Fig . 1 , the root tweet might be benign , and anchoring the sentiment features computation base on the root tweet may not provide an accurate forecast of the sentiment of the subsequent tweets in the conversation threads . Therefore , DRAGNET + + addressed this limitation by modeling the tweetsâ€™ semantics with an additional hate intensity prediction task at the tweet level and the conversation thread structure with GNNs as the tree encoder . The intuition is that by learning the node ( i . e . , tweets ) and tree ( i . e . , conversation thread ) representations using GNNs , we are able to capture the dependency among the tweets in the conversation threads and extract unique structural patterns that could improve the prediction of hate intensity for conversation thread . 5 . Datasets We evaluate DRAGNET + + on three publicly available large Twitter datasets . These datasets contain a large amount of Twitter conversation threads , which were originally collected for other hate speech - related stud - ies . Table 2 shows the statistics of the datasets . Dataset # Conversation threads Conversation thread length # Tweets # Unique users Min . Max . Avg . Anti - Racism 3 , 500 1 582 200 750 , 235 620 , 437 Anti - Social 668 , 082 1 20014 31 40 , 385 , 257 4 , 980 , 160 Anti - Asian 218 , 790 1 2822 35 206 , 348 , 565 23 , 895 , 911 Table 2 : The statistics of the three datasets used in our experiments . In the Anti - Racism dataset [ 13 ] , the authors manually identiï¬ed various real - world events using a hashtag - based matching via the Twitter API . Dur - ing the tumultuous year of 2020 , many topics polarised the discussion , such as the 2020 US Presidential election , the Brexit referendum in the UK , and 20 extending similar political issues across the US , the UK , and India . Adding to the diversity of the dataset , sinophobic tweets attributing coronavirus to China are also curated in the dataset , with most mentions ( in tweets ) as â€œChina virusâ€ . The ï¬nal dataset comprises 3500 Twitter conversation threads with over 750K tweets . In the Anti - Social dataset [ 71 ] , the COVID - 19 - related tweets were col - lected from the Twitter platform . The authors performed query searches us - ing case - sensitive keywords such as â€œcovid - 19â€ , â€œCOVID - 19â€ , â€œCoronavirusâ€ , â€œcoronavirusâ€ and â€œcoronaâ€ . Leveraging the Twitter Streaming API , the au - thors collected the conversations related to these queried tweets . Using this approach , the authors collected over 650 K Twitter conversations with over 40 M tweets published between March 17 and 2020 to April 28 , 2020 . In the Anti - Asian dataset [ 72 ] , the authors focused on the Asian hate speech surrounding the COVID - 19 discussions . They followed a similar keyword - based approach but as a two - step process . The covid - 19 keywords are used to narrow the COVID - 19 - related tweets , following which hate key - words about anti - Asian hate is used to segregate the tweets as deï¬ned by the collection for the dataset . Finally , counter - speech keywords are keywords and hashtags used to counter hate speech and support Asians . It comprises of 42 keywords . The authors used a combination of Twitter Streaming API and Twitter Search API to collect real - time tweets between January 15 , 2020 and March 26 , 2021 . The dataset comprises over 210 K Twitter conversation threads with more than 206 M tweets . As mentioned in Section 3 , our main task is building hate - intensity proï¬les for each datasetâ€™s tweets . However , it is important to note that we do not use the class labels in the datasets for our task as they do not contain ground truth for the hate intensity of tweets . Instead , we will be using the tweet text to derive the hate intensity scores ( as discussed in Section 3 ) . Length of Conversation Threads . Figure 5 ( a ) shows the length of con - versation threads over the number of conversation threads for the Anti - Racism dataset . The average conversation thread length is around 200 , with the maximum length being 582 . On the other hand , Figures 5 ( b ) and 5 ( c ) show a clipped version of the entire graph for better visualization of the dis - tribution . Most of the tweets constitute lengths less than or equal to 100 . To understand the full picture , the numerical statistics of Anti - Social and Anti - Asian are represented in Table 2 . The maximum length in the Anti - Social dataset goes as high as 20014 . However , the average length of the dataset is 21 Length / # unique users per conv . thread # o f c on v e r s a t i on t h r ead s 0 25 50 75 100 100 200 300 400 500 # unique users per conv . thread Length of conv . thread ( a ) Anti - Racism Length / # unique users per conv . thread # o f c on v e r s a t i on t h r ead s 0 100 200 300 400 500 100 200 300 400 # unique users per conv . thread Length of conv . thread ( b ) Anti - Social Length / # unique users per conv . thread # o f c on v e r s a t i on t h r ead s 0 50 100 150 200 250 100 200 300 400 500 600 700 Length of conv . thread # unique users per conv . thread ( c ) Anti - Asian Figure 5 : Distribution of number of conversation threads vs length of each conversation thread and the number of unique users in respective conversation threads for all three datasets . Note that for the Anti - Social and Anti - Asian datasets , the x - axis ticks start from 10 and ends earlier than the maximum length as described in Table 2 to show the distribution more clearly . around 31 . Along similar lines , the Anti - Asian dataset also touches a maxi - mum conversation thread length of 2822 with an average length of 35 across the tweet threads . Number of Unique Users . Figure 5 also highlights the number of unique users per reply thread . For the Anti - Racism dataset , it can be observed that the unique users and the lengths follow a similar distribution . The increased number of unique users can be attributed to limited re - engagement by the same users for a particular thread . Furthermore , both the Anti - Social and Anti - Asian datasets exhibit a high variation in the length of the threads with a consistent presence of threads in almost all lengths . Correspondingly , the unique users touch about 65 for about 225 threads in the case of Anti - Social , whereas 70 unique users for about 240 threads in Anti - Asian . 22 Number of Conversation threads and Tweets . The conversation threads and the total number of tweets are presented in Table 2 . Compared to the Anti - Racism dataset , the other two datasets possess a vast number of con - versation threads , forming the ideal datasets to test the performance of the models . However , the number of tweets is 206 M for Anti - Asian , whose con - versation threads are less in number than Anti - Social , which has about 40 M tweets in total . This shows the variation in conversation thread length for dif - ferent datasets and the corresponding unique users overall , adding diversity to the datasets . 6 . Experiments 6 . 1 . Baselines There are few studies that predict hate intensity proï¬les in Twitter con - versations [ 13 , 34 ] . As a result , we use time - series forecasting and temporal pattern model based on both conventional and deep learning as baselines . â€¢ LSTM [ 73 ] : Long short - term memory is a neural network architecture with feedback connections that is very eï¬€ective . LSTMs , unlike RNNs , perform better on long - range predictions , such as time - series problems . As suggested by Elsworth and GÂ¨uttel [ 73 ] , we employ a stacked LSTM . We use ReLU as the activation function for each layer to forecast the hate intensity proï¬le . â€¢ CNN [ 74 ] : A convolutional neural network considers the hate inten - sitiesâ€™ time information to better proï¬le the tweetâ€™s upcoming replies . We employ a 1 - D CNN architecture with ReLU activation as the last layer . The kernel size is 2 , and the number of ï¬lters employed is 64 . â€¢ N - Beats [ 75 ] : N eural B asis E xpansion A nalysis for interpretable T ime S eries forecasting is a deep learning model designed to tackle the univariate time - series problem . With a very deep stack of fully - connected layers , it incorporates forward and backward residual link - ages . â€¢ DeepAR [ 55 ] : It is a supervised learning approach for forecasting scalar time - series using Recurrent Neural Networks ( RNN ) , which is achieved by auto - regressively training the RNN on numerous related time - series data . 23 â€¢ TFT [ 76 ] : T emporal F usion T ransformers are used in conjunction with multi - horizon forecasting to help the model mix known future inputs and extract exogenous correlations from the time seriesâ€™ past input data . The model employs a self - attention - based architecture to give interpretable time - series insights . â€¢ ForGAN [ 77 ] : Probabilistic forecasting of sensory data using gen - erative adversarial networks employs an adversarial network to learn data generating distributions and compute probabilistic forecasts over them . In a time - series forecasting problem , the model claims to learn the conditional probability distribution of future values with no quan - tile crossing or reliance on the prior distribution . â€¢ DRAGNET [ 13 ] : It is a deep stratiï¬ed learning approach . To create the latent representation of past and future tweets from the speciï¬ed point of reference as hyperparameter , an autoencoder is utilized using the Inception - Time module [ 63 ] . This is combined with a fuzzy cluster - ing algorithm to forecast the future hate intensity sequence , which is reinforced with sentiment features to determine the correlation of each tweet with the root tweet . 6 . 2 . Experimental Setup The hyperparameters used for all the experiments are as follows : Î´ = 10 , w = 0 . 6 , t h = 25 , t f = 275 , n = 300 , j = 15 , N X h = 32 , N X f = 128 . In the auto - encoder step , DRAGNET + + is implemented with the Inception - Time module [ 63 ] for transformation with variable kernel sizes â€“ 5 , 7 , and 9 . Sentiment characteristics , history encodings from the auto - encoder , and graph structure information are fed into fully - connected layers in the prior model . A Gaussian mixture model with full covariance is utilized for fuzzy clustering . Davidson model [ 78 ] is the default hate - speech classiï¬er . We utilize the Adam optimizer with an lr = 0 . 001 for faster convergence , and the training and test split is set at 80 : 20 . The train - test split is based on the number of conversation threads since we would like to train and test on complete tweet propagation . Our model predicts the future hate intensity sequence , given the tweets and historical hate intensity sequence for the chosen window length . To evaluate , three metrics for this task are used : ( i ) Pearson correlation coeï¬ƒcient ( PCC ) , where a higher value is better , ( ii ) Root Mean Square 24 Error ( RMSE ) , and ( iii ) Mean Forecast Error ( MFE ) , where lower values are better . 6 . 3 . Experimental Results and Analysis The overall performance of DRAGNET + + and baselines on the three datasets are shown in Table 3 . Across all datasets , we ï¬nd that DRAG - NET + + outperforms the baselines . Speciï¬cally , for the Anti - Racism dataset , DRAGNET + + outperforms the best baseline ( N - Beats ) by 40 % on PCC . Other assessment parameters show a similar trend , with a 100 % reduction in RMSE and a 9 Ã— reduction in MFE compared to N - Beats . On the contrary , as compared to the baselines on the Anti - Asian dataset , both DRAGNET and DRAGNET + + perform poorly on PCC . One possible explanation is the datasetâ€™s restricted chain of responses . DRAGNET + + on the other hand , outperforms N - Beats by more than 3 Ã— on RMSE when optimizing the in - accuracies on hate intensity proï¬les . TFT has a higher MFE score than DRAGNET , but DRAGNET + + has the highest total score . Along with LSTM and CNN , ForGAN appears to be among the worst performers . On the Anti - Social dataset , DRAGNET + + outperforms the best baseline , TFT , by 0 . 223 , 0 . 476 , and 0 . 202 , respectively , on PCC , RMSE , and MFE . When DRAGNET and DRAGNET + + are directly compared , the latter performs better on all three datasets and on all three metrics . This demonstrates how graph information inherent in tweet threads can help forecast hate intensity proï¬les more accurately . The greatest signiï¬cant improvement over RMSE is 22 . 72 % on the Anti - Social dataset , and the highest increase on PCC is 0 . 066 points on the Anti - Racism dataset . On the MFE ( Anti - Racism dataset ) , DRAGNET performs the best , which can be attributed to customized win - dowing strategies that reduce forecast error . DRAGNETâ€™s improved per - formance does not apply to other datasets , where DRAGNET + + readily outperforms . 6 . 4 . Ablation Study The prior model PR ( Â· ) in DRAGNET + + uses the propagation graph structure , sentiment information , and ï¬ne - tuning operation to improve the representation of historical discussion threads and establish a link between historical and future data . To assess the performance of each component , we remove it one at a time and provide three variants : DRAGNET + + w / o graph structure , DRAGNET + + w / o sentiment , and DRAGNET + + w / o ï¬ne - tune . Then , using three datasets , we run experiments to see how the 25 Model Anti - Racism Anti - Social Anti - Asian PCC RMSE MFE PCC RMSE MFE PCC RMSE MFE LSTM 0 . 145 0 . 611 0 . 500 0 . 160 0 . 511 0 . 315 0 . 680 0 . 722 0 . 692 CNN 0 . 105 0 . 644 0 . 509 0 . 112 0 . 542 0 . 320 0 . 675 0 . 731 0 . 699 DeepAR 0 . 310 0 . 484 0 . 065 0 . 180 0 . 490 0 . 275 0 . 682 0 . 748 0 . 708 TFT 0 . 469 0 . 437 0 . 076 0 . 376 0 . 630 0 . 333 0 . 562 0 . 866 0 . 125 N - Beats 0 . 380 0 . 544 0 . 085 0 . 340 0 . 633 0 . 271 0 . 712 0 . 462 0 . 173 ForGAN 0 . 240 0 . 603 0 . 360 0 . 172 0 . 897 0 . 785 0 . 563 0 . 871 0 . 475 DRAGNET 0 . 563 0 . 247 0 . 010 0 . 559 0 . 189 0 . 156 0 . 603 0 . 165 0 . 134 DRAGNET + + 0 . 629 0 . 218 0 . 008 0 . 599 0 . 154 0 . 131 0 . 641 0 . 147 0 . 116 Table 3 : Overall performance of DRAGNET + + and baselines on the three datasets . The best results are bold . alternative models perform in the end , and the ï¬ndings are displayed in Fig 6 . 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 Anti - Racism Anti - Social Anti - Asian P ea r s on c o rr e l a ti on DRAGNET + + w / o graph structure DRAGNET + + w / o sentiment DRAGNET + + w / o fine - tune DRAGNET + + Figure 6 : Ablation study : The Eï¬€ectiveness of components in prior model PR ( Â· ) on three datasets . The modelâ€™s performance degrades for each of the versions , as seen in Fig 6 . It indicates that each component contributes diï¬€erently to the prediction of conversation thread labels , resulting in improved hate intensity predic - tion results . Across all three datasets , DRAGNET + + w / o graph structure performs the worst among the four variations . The diï¬€usion patterns and in - formative material are present in the propagation graph structures of conver - sation threads ; therefore , this is reasonable . Capturing earlier responses and projecting future patterns would help the data . When comparing DRAG - NET + + w / o sentiment to DRAGNET + + , the performance drops slightly , but not as much as when comparing DRAGNET + + w / o graph structure . A possible reason could be that the tweetâ€™s sentiment is assessed in rela - tion to the root tweet . On the other hand , structural graph information 26 is a more advanced method of recording hate intensity distribution . Fur - thermore , DRAGNET + + outperforms DRAGNET + + w / o ï¬ne - tuning . It suggests that using the ï¬ne - tune technique , word embedding can precisely capture hate - intensity information . 6 . 5 . Parameter Analysis We further analyze the parameters of DRAGNET + + to understand its superiority and limitations better . Throughout this study , we compare DRAG - NET + + and DRAGNET . 6 . 5 . 1 . The impact of diï¬€erent window sizes The window size utilised in the average rolling process is represented by the hyperparameter Î´ . It is possible that the smaller window size would result in more pronounced hate intensity proï¬les . We chose the window size in { 5 , 10 , 15 , 20 } to analyse the inï¬‚uence of diï¬€erent window sizes , and the experimental ï¬ndings are displayed in Fig 7 . Across all three datasets , we observe that increasing the value of Î´ improves the performance of both DRAGNET + + and DRAGNET . A bigger window size ( Î´ ) smooths the val - ues in hate intensity sequences , making it easier for the models to learn the hidden hate intensity pattern . Furthermore , DRAGNET + + consistently outperforms DRAGNET across all window size settings , demonstrating its eï¬ƒcacy . 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 5 10 15 20 P ea r s on c o rr e l a ti on window size DRAGNET + + DRAGNET ( a ) Anti - Racism 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 5 10 15 20 P ea r s on c o rr e l a ti on window size DRAGNET + + DRAGNET ( b ) Anti - Social 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 5 10 15 20 P ea r s on c o rr e l a ti on window size DRAGNET + + DRAGNET ( c ) Anti - Asian Figure 7 : Performance comparison based on window size ( Î´ ) of hate intensity proï¬les . 6 . 5 . 2 . The impact of history sizes The beginning history size t h is the data needed to estimate the whole hate intensity proï¬le for a new conversation thread . The value of the Pearson correlation coeï¬ƒcient for DRAGNET + + shows relatively little change as t h increases , as seen in Fig 8 . We can observe that DRAGNET + + outperforms 27 DRAGNET . The modelâ€™s capacity to generate early predictions decreases as the number of initial responses submitted to the model grows . As a result , in the experiments , we used t h = 25 to predict hate intensity . ( a ) Anti - Racism ( b ) Anti - Social ( c ) Anti - Asian Figure 8 : Performance of DRAGNET + + on three datasets based on changes to tweet history length at input . 6 . 5 . 3 . The impact of diï¬€erent numbers of clusters One of the most important hyperparameters of DRAGNET + + in the fuzzy clustering step is the number of clusters j . As a result , we choose a number of clusters in the range of { 5 , 10 , 15 , 20 } to assess the impact of clus - ter size . The experiment outcomes are shown in Fig 9 . We notice that as the number of clusters grows , the DRAGNET + + â€™s performance improves , and it is consistently better than DRAGNET . It is diï¬ƒcult to discern be - tween diï¬€erent types of conversation threads when we use a small number of clusters . However , the performance begins to deteriorate beyond a certain number of clusters . One probable explanation is that grouping conversation threads into too many clusters produces noise , making it harder for the prior model to detect the correct cluster labels with minimal historical data . ( a ) Anti - Racism ( b ) Anti - Social ( c ) Anti - Asian Figure 9 : Performance comparison shown by changing the hyperparameter of number of clusters in fuzzy clustering algorithm . 28 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 45 0 . 6 0 . 75 P ea r s on c o rr e l a ti on Weight DRAGNET + + DRAGNET ( a ) Anti - Racism 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 45 0 . 6 0 . 75 P ea r s on c o rr e l a ti on Weight DRAGNET + + DRAGNET ( b ) Anti - Social 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 45 0 . 6 0 . 75 P ea r s on c o rr e l a ti on Weight DRAGNET + + DRAGNET ( c ) Anti - Asian Figure 10 : Performance comparison depicted by varying the weight ( w ) which optimizes the trade - oï¬€ between hate detection / lexicon . 6 . 5 . 4 . The impact of diï¬€erent weights in hate intensity score Our hate intensity score leverages w to simplify the trade - oï¬€ between the hate detection model and the hate lexicon component in Section 3 . Our suggested DRAGNET + + model deï¬nitely outperforms DRAGNET for all three values of w ( i . e . , 0 . 45 , 0 . 6 , 0 . 75 ) on all three datasets , as demonstrated in Fig 10 . 6 . 5 . 5 . The impact of hate speech detection models During the data preprocessing phase , we utilized a hate speech detection model to compute hate intensity scores for replies . To examine the eï¬€ect of diï¬€erent hate speech detection models , we evaluated three models proposed by Davidson et al . [ 79 ] ] ( model used in DRAGNET + + ) , Founta et al . [ 24 ] , and Waseem and Hovy Waseem and Hovy [ 6 ] . As illustrated in Figure 11 , the selected hate speech detection models had a minor impact on the ul - timate performance . Nonetheless , our proposed model exhibited consistent superiority over DRAGNET across all three hate detection models . 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 davidson fountana waseem P ea r s on c o rr e l a ti on Hate detection models DRAGNET + + DRAGNET ( a ) Anti - Racism 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 davidson fountana waseem P ea r s on c o rr e l a ti on Hate detection models DRAGNET + + DRAGNET ( b ) Anti - Social 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 davidson fountana waseem P ea r s on c o rr e l a ti on Hate detection models DRAGNET + + DRAGNET ( c ) Anti - Asian Figure 11 : The impact of diï¬€erent hate detection algorithms on DRAGNET + + and DRAGNET 29 7 . Conclusion In this paper , we build on our previous work using two new datasets : Anti - Social and Anti - Asian . The new datasets gave enough bandwidth to continue experimenting with the old model DRAGNET to forecast hate intensity pro - ï¬les . By adding the structural graph information critical for any Twitter reply chain , we suggested a new model , DRAGNET + + . GNN - augmented DRAGNET + + outperformed all baselines , including DRAGNET , by a large margin . We then used several ablations to compare DRAGNET and DRAG - NET + + to ï¬gure out why our model outperformed the existing architecture . In the future , we would like to build a meaningful association of hate intensity proï¬les at the branch level of the reply chainâ€™s tree structure using sophisticated structural graph information . We would also like to look into how user data can be used to predict hate intensity scores . Human - annotated hate intensity datasets can be included . Our ultimate goal , sooner or later , would be to stop the spread of hate speech as soon as possible after the ï¬rst tweet appears on Twitter . References [ 1 ] K . MÂ¨uller , C . Schwarz , Fanning the ï¬‚ames of hate : Social media and hate crime , Journal of the European Economic Association ( 2018 ) . [ 2 ] Times , Facebook says itâ€™s removing more hate speech than ever be - fore . but thereâ€™s a catch , 2019 . URL : https : / / time . com / 5739688 / facebook - hate - speech - languages / . [ 3 ] Bloomberg , Twitter , facebook join global pledge to ï¬ght hate speech online , 2019 . URL : https : / / www . bloomberg . com / news / articles / 2019 - 05 - 15 / macron - ardern - to - meet - twitter - facebook - google - on - hate - speech . [ 4 ] Facebook , Hate speech , 2022 . URL : https : / / transparency . fb . com / en - gb / policies / community - standards / hate - speech / . [ 5 ] Twitter , Hateful conduct policy , 2022 . URL : https : / / help . twitter . com / en / rules - and - policies / hateful - conduct - policy . [ 6 ] Z . Waseem , D . Hovy , Hateful symbols or hateful people ? predictive features for hate speech detection on twitter , in : Proceedings of the NAACL student research workshop , 2016 , pp . 88 â€“ 93 . 30 [ 7 ] B . GambÂ¨ack , U . K . Sikdar , Using convolutional neural networks to classify hate - speech , in : Proceedings of the ï¬rst workshop on abusive language online , 2017 , pp . 85 â€“ 90 . [ 8 ] A . Schmidt , M . Wiegand , A survey on hate speech detection using natural language processing , in : Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media , Association for Computational Linguistics , Valencia , Spain , 2017 , pp . 1 â€“ 10 . URL : https : / / aclanthology . org / W17 - 1101 . doi : 10 . 18653 / v1 / W17 - 1101 . [ 9 ] P . Fortuna , S . Nunes , A survey on automatic detection of hate speech in text , ACM Computing Surveys ( CSUR ) 51 ( 2018 ) 1 â€“ 30 . [ 10 ] F . Poletto , V . Basile , M . Sanguinetti , C . Bosco , V . Patti , Re - sources and benchmark corpora for hate speech detection : a system - atic review , Language Resources and Evaluation 55 ( 2021 ) 477 â€“ 523 . URL : https : / / doi . org / 10 . 1007 / s10579 - 020 - 09502 - 8 . doi : 10 . 1007 / s10579 - 020 - 09502 - 8 . [ 11 ] P . Liu , J . Guberman , L . Hemphill , A . Culotta , Forecasting the pres - ence and intensity of hostility on instagram using linguistic and social features , in : Twelfth international aaai conference on web and social media , 2018 . [ 12 ] K . - Y . Lin , R . K . - W . Lee , W . Gao , W . - C . Peng , Early prediction of hate speech propagation , in : 2021 International Conference on Data Mining Workshops ( ICDMW ) , IEEE , 2021 , pp . 967 â€“ 974 . [ 13 ] D . Sahnan , S . Dahiya , V . Goel , A . Bandhakavi , T . Chakraborty , Better prevent than react : Deep stratiï¬ed learning to predict hate intensity of twitter reply chains , in : 2021 IEEE International Conference on Data Mining ( ICDM ) , IEEE , 2021 , pp . 549 â€“ 558 . [ 14 ] J . Navarro , The psychology of hatred , The Open Criminology Journal 6 ( 2013 ) 10 â€“ 17 . doi : 10 . 2174 / 1874917801306010010 . [ 15 ] J . Suler , The online disinhibition eï¬€ect , CyberPsychol - ogy & Behavior 7 ( 2004 ) 321 â€“ 326 . URL : https : / / doi . org / 10 . 1089 / 1094931041291295 . doi : 10 . 1089 / 1094931041291295 . arXiv : https : / / doi . org / 10 . 1089 / 1094931041291295 , pMID : 15257832 . 31 [ 16 ] L . Maizland , Hate speech on social media : Global comparisons â€” council on foreign relations ( 2019 ) . URL : https : / / www . cfr . org / backgrounder / hate - speech - social - media - global - comparisons . [ 17 ] Y . Kim , Convolutional neural networks for sentence classiï¬cation , in : Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Association for Computational Linguis - tics , 2014 , pp . 1746 â€“ 1751 . [ 18 ] I . Sutskever , O . Vinyals , Q . V . Le , Sequence to sequence learning with neural networks , Advances in neural information processing systems 27 ( 2014 ) . [ 19 ] A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , (cid:32)L . Kaiser , I . Polosukhin , Attention is all you need , Advances in neural information processing systems 30 ( 2017 ) . [ 20 ] Y . Mehdad , J . Tetreault , Do characters abuse more than words ? , in : Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue , 2016 , pp . 299 â€“ 303 . [ 21 ] P . Badjatiya , S . Gupta , M . Gupta , V . Varma , Deep learning for hate speech detection in tweets , in : Proceedings of the 26th international conference on World Wide Web companion , 2017 , pp . 759 â€“ 760 . [ 22 ] Z . Zhang , D . Robinson , J . Tepper , Detecting hate speech on twitter us - ing a convolution - gru based deep neural network , in : European semantic web conference , Springer , 2018 , pp . 745 â€“ 760 . [ 23 ] A . Arango , J . PÂ´erez , B . Poblete , Hate speech detection is not as easy as you may think : A closer look at model validation , in : Proceedings of the 42nd international acm sigir conference on research and development in information retrieval , 2019 , pp . 45 â€“ 54 . [ 24 ] A . M . Founta , D . Chatzakou , N . Kourtellis , J . Blackburn , A . Vakali , I . Leontiadis , A uniï¬ed deep learning architecture for abuse detection , in : Proceedings of the 10th ACM conference on web science , 2019 , pp . 105 â€“ 114 . 32 [ 25 ] M . R . Awal , R . K . - W . Lee , E . Tanwar , T . Garg , T . Chakraborty , Model - agnostic meta - learning for multilingual hate speech detection , IEEE Transactions on Computational Social Systems ( 2023 ) . [ 26 ] J . H . Park , P . Fung , One - step and two - step classiï¬cation for abusive language detection on twitter , arXiv preprint arXiv : 1706 . 01206 ( 2017 ) . [ 27 ] F . Del Vigna12 , A . Cimino23 , F . Dellâ€™Orletta , M . Petrocchi , M . Tesconi , Hate me , hate me not : Hate speech detection on facebook , in : Proceed - ings of the First Italian Conference on Cybersecurity ( ITASEC17 ) , 2017 , pp . 86 â€“ 95 . [ 28 ] R . Cao , R . K . - W . Lee , T . - A . Hoang , Deephate : Hate speech detection via multi - faceted text representations , in : 12th ACM conference on web science , 2020 , pp . 11 â€“ 20 . [ 29 ] R . K . - W . Lee , R . Cao , Z . Fan , J . Jiang , W . - H . Chong , Disentangling hate in online memes , in : Proceedings of the 29th ACM International Conference on Multimedia , 2021 , pp . 5138 â€“ 5147 . [ 30 ] J . Devlin , M . - W . Chang , K . Lee , K . Toutanova , Bert : Pre - training of deep bidirectional transformers for language understanding , arXiv preprint arXiv : 1810 . 04805 ( 2018 ) . [ 31 ] A . Radford , K . Narasimhan , T . Salimans , I . Sutskever , Improving lan - guage understanding by generative pre - training ( 2018 ) . [ 32 ] M . Mozafari , R . Farahbakhsh , N . Crespi , A bert - based transfer learn - ing approach for hate speech detection in online social media , in : In - ternational Conference on Complex Networks and Their Applications , Springer , 2019 , pp . 928 â€“ 940 . [ 33 ] M . R . Awal , R . Cao , R . K . - W . Lee , S . MitroviÂ´c , Angrybert : Joint learning target and emotion for hate speech detection , in : Paciï¬c - Asia conference on knowledge discovery and data mining , Springer , 2021 , pp . 701 â€“ 713 . [ 34 ] S . Dahiya , S . Sharma , D . Sahnan , V . Goel , E . Chouzenoux , V . Elvira , A . Majumdar , A . Bandhakavi , T . Chakraborty , Would Your Tweet In - voke Hate on the Fly ? Forecasting Hate Intensity of Reply Threads on Twitter , in : Proceedings of the 27th ACM SIGKDD Conference 33 on Knowledge Discovery & Data Mining , ACM , Virtual Event Singa - pore , 2021 , pp . 2732 â€“ 2742 . URL : https : / / dl . acm . org / doi / 10 . 1145 / 3447548 . 3467150 . doi : 10 . 1145 / 3447548 . 3467150 . [ 35 ] V . Basile , C . Bosco , E . Fersini , D . Nozza , V . Patti , F . M . Rangel Pardo , P . Rosso , M . Sanguinetti , SemEval - 2019 task 5 : Multilingual detection of hate speech against immigrants and women in Twitter , in : Pro - ceedings of the 13th International Workshop on Semantic Evaluation , Association for Computational Linguistics , Minneapolis , Minnesota , USA , 2019 , pp . 54 â€“ 63 . URL : https : / / aclanthology . org / S19 - 2007 . doi : 10 . 18653 / v1 / S19 - 2007 . [ 36 ] M . Sanguinetti , G . Comandini , E . Nuovo , S . Frenda , M . Stranisci , C . Bosco , T . Caselli , V . Patti , I . Russo , Haspeede 2 @ evalita2020 : Overview of the evalita 2020 hate speech detection task , 2020 . doi : 10 . 4000 / books . aaccademia . 6897 . [ 37 ] F . Rangel , G . L . D . la PeËœna SarracÂ´en , B . Chulvi , E . Fersini , P . Rosso , Proï¬ling hate speech spreaders on twitter task at pan 2021 , in : CLEF , 2021 . [ 38 ] J . Pavlopoulos , J . Sorensen , L . Laugier , I . Androutsopoulos , SemEval - 2021 task 5 : Toxic spans detection , in : Proceedings of the 15th International Workshop on Semantic Evaluation ( SemEval - 2021 ) , As - sociation for Computational Linguistics , Online , 2021 , pp . 59 â€“ 69 . URL : https : / / aclanthology . org / 2021 . semeval - 1 . 6 . doi : 10 . 18653 / v1 / 2021 . semeval - 1 . 6 . [ 39 ] X . Ma , H . Zhong , Y . Li , J . Ma , Z . Cui , Y . Wang , Forecasting trans - portation network speed using deep capsule networks with nested lstm models , IEEE Transactions on Intelligent Transportation Systems 22 ( 2020 ) 4813 â€“ 4824 . [ 40 ] T . G . Andersen , T . Bollerslev , P . Christoï¬€ersen , F . X . Diebold , Volatil - ity forecasting , 2005 . [ 41 ] D . Ding , M . Zhang , X . Pan , M . Yang , X . He , Modeling extreme events in time series prediction , in : Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2019 , pp . 1114 â€“ 1122 . 34 [ 42 ] Y . Matsubara , Y . Sakurai , W . G . Van Panhuis , C . Faloutsos , Funnel : automatic mining of spatially coevolving epidemics , in : Proceedings of the 20th ACM SIGKDD international conference on Knowledge discov - ery and data mining , 2014 , pp . 105 â€“ 114 . [ 43 ] I . Rojas , O . Valenzuela , F . Rojas , A . GuillÂ´en , L . J . Herrera , H . Pomares , L . Marquez , M . Pasadas , Soft - computing techniques and arma model for time series prediction , Neurocomputing 71 ( 2008 ) 519 â€“ 537 . [ 44 ] E . S . Gardner Jr , Exponential smoothing : The state of the art , Journal of forecasting 4 ( 1985 ) 1 â€“ 28 . [ 45 ] A . v . d . Oord , S . Dieleman , H . Zen , K . Simonyan , O . Vinyals , A . Graves , N . Kalchbrenner , A . Senior , K . Kavukcuoglu , Wavenet : A generative model for raw audio , arXiv preprint arXiv : 1609 . 03499 ( 2016 ) . [ 46 ] A . Borovykh , S . Bohte , C . W . Oosterlee , Conditional time se - ries forecasting with convolutional neural networks , arXiv preprint arXiv : 1703 . 04691 ( 2017 ) . [ 47 ] X . Wei , L . Zhang , H . - Q . Yang , L . Zhang , Y . - P . Yao , Machine learning for pore - water pressure time - series prediction : Application of recurrent neural networks , Geoscience Frontiers 12 ( 2021 ) 453 â€“ 467 . [ 48 ] S . Elsworth , S . GÂ¨uttel , Time series forecasting using lstm networks : A symbolic approach , arXiv preprint arXiv : 2003 . 05672 ( 2020 ) . [ 49 ] Z . Mariet , V . Kuznetsov , Foundations of sequence - to - sequence model - ing for time series , in : The 22nd international conference on artiï¬cial intelligence and statistics , PMLR , 2019 , pp . 408 â€“ 417 . [ 50 ] C . Fan , Y . Zhang , Y . Pan , X . Li , C . Zhang , R . Yuan , D . Wu , W . Wang , J . Pei , H . Huang , Multi - horizon time series forecasting with temporal attention learning , in : Proceedings of the 25th ACM SIGKDD Inter - national conference on knowledge discovery & data mining , 2019 , pp . 2527 â€“ 2535 . [ 51 ] R . Sen , H . - F . Yu , I . S . Dhillon , Think globally , act locally : A deep neural network approach to high - dimensional time series forecasting , Advances in neural information processing systems 32 ( 2019 ) . 35 [ 52 ] S . Li , X . Jin , Y . Xuan , X . Zhou , W . Chen , Y . - X . Wang , X . Yan , En - hancing the locality and breaking the memory bottleneck of transformer on time series forecasting , Advances in Neural Information Processing Systems 32 ( 2019 ) . [ 53 ] B . N . Oreshkin , D . Carpov , N . Chapados , Y . Bengio , N - beats : Neural basis expansion analysis for interpretable time series forecasting , arXiv preprint arXiv : 1905 . 10437 ( 2019 ) . [ 54 ] Y . Yuan , K . Kitani , Diverse trajectory forecasting with determinantal point processes , arXiv preprint arXiv : 1907 . 04967 ( 2019 ) . [ 55 ] D . Salinas , V . Flunkert , J . Gasthaus , T . Januschowski , Deepar : Proba - bilistic forecasting with autoregressive recurrent networks , International Journal of Forecasting 36 ( 2020 ) 1181 â€“ 1191 . [ 56 ] A . Koochali , P . Schichtel , A . Dengel , S . Ahmed , Probabilistic forecast - ing of sensory data with generative adversarial networks â€“ forgan , IEEE Access 7 ( 2019 ) 63868 â€“ 63880 . [ 57 ] M . Wiegand , J . Ruppenhofer , A . Schmidt , C . Greenberg , Inducing a lexicon of abusive words â€“ a feature - based approach , in : Proceedings of the 2018 Conference of the North American Chapter of the Asso - ciation for Computational Linguistics : Human Language Technologies , June 1 - June 6 , 2018 , New Orleans , Louisiana , Volume 1 ( Long Papers ) , Association for Computational Linguistics , 2019 , pp . 1046 â€“ 1056 . [ 58 ] Z . Yang , Z . Dai , Y . Yang , J . Carbonell , R . R . Salakhutdinov , Q . V . Le , Xlnet : Generalized autoregressive pretraining for language understand - ing , Advances in neural information processing systems 32 ( 2019 ) . [ 59 ] P . Hastings , S . Hughes , D . Blaum , P . Wallace , M . A . Britt , Stratiï¬ed learning for reducing training set size , in : International Conference on Intelligent Tutoring Systems , Springer , 2016 , pp . 341 â€“ 346 . [ 60 ] V . Niennattrakul , C . A . Ratanamahatana , On clustering multimedia time series data using k - means and dynamic time warping , in : 2007 International Conference on Multimedia and Ubiquitous Engineering ( MUEâ€™07 ) , IEEE , 2007 , pp . 733 â€“ 738 . 36 [ 61 ] J . Sun , Y . Li , H . - S . Fang , C . Lu , Three steps to multimodal trajec - tory prediction : Modality clustering , classiï¬cation and synthesis , in : Proceedings of the IEEE / CVF International Conference on Computer Vision , 2021 , pp . 13250 â€“ 13259 . [ 62 ] C . Yang , J . Qiao , H . Han , L . Wang , Design of polynomial echo state networks for time series prediction , Neurocomputing 290 ( 2018 ) 148 â€“ 160 . [ 63 ] H . Ismail Fawaz , B . Lucas , G . Forestier , C . Pelletier , D . F . Schmidt , J . Weber , G . I . Webb , L . Idoumghar , P . - A . Muller , F . Petitjean , In - ceptiontime : Finding alexnet for time series classiï¬cation , Data Mining and Knowledge Discovery 34 ( 2020 ) 1936 â€“ 1962 . [ 64 ] Z . Cui , W . Chen , Y . Chen , Multi - scale convolutional neural networks for time series classiï¬cation , arXiv preprint arXiv : 1603 . 06995 ( 2016 ) . [ 65 ] A . Ziat , E . Delasalles , L . Denoyer , P . Gallinari , Spatio - temporal neural networks for space - time series forecasting and relations discovery , in : 2017 IEEE International Conference on Data Mining ( ICDM ) , IEEE , 2017 , pp . 705 â€“ 714 . [ 66 ] D . A . Reynolds , Gaussian mixture models . , Encyclopedia of biometrics 741 ( 2009 ) . [ 67 ] Q . Cao , H . Shen , J . Gao , B . Wei , X . Cheng , Popularity prediction on social platforms with coupled graph neural networks , in : Proceedings of the 13th International Conference on Web Search and Data Mining , 2020 , pp . 70 â€“ 78 . [ 68 ] T . Bian , X . Xiao , T . Xu , P . Zhao , W . Huang , Y . Rong , J . Huang , Rumor detection on social media with bi - directional graph convolutional net - works , in : Proceedings of the AAAI conference on artiï¬cial intelligence , volume 34 , 2020 , pp . 549 â€“ 556 . [ 69 ] F . Scarselli , M . Gori , A . C . Tsoi , M . Hagenbuchner , G . Monfardini , The graph neural network model , IEEE transactions on neural networks 20 ( 2008 ) 61 â€“ 80 . 37 [ 70 ] T . Mikolov , I . Sutskever , K . Chen , G . S . Corrado , J . Dean , Distributed representations of words and phrases and their compositionality , Ad - vances in neural information processing systems 26 ( 2013 ) . [ 71 ] M . R . Awal , R . Cao , S . Mitrovic , R . K . - W . Lee , On Analyzing Antisocial Behaviors Amid COVID - 19 Pandemic , arXiv : 2007 . 10712 [ cs ] ( 2020 ) . URL : http : / / arxiv . org / abs / 2007 . 10712 , arXiv : 2007 . 10712 . [ 72 ] B . He , C . Ziems , S . Soni , N . Ramakrishnan , D . Yang , S . Kumar , Racism is a virus : anti - asian hate and counterspeech in social media during the COVID - 19 crisis , in : Proceedings of the 2021 IEEE / ACM International Conference on Advances in Social Networks Analysis and Mining , ACM , Virtual Event Netherlands , 2021 , pp . 90 â€“ 94 . URL : https : / / dl . acm . org / doi / 10 . 1145 / 3487351 . 3488324 . doi : 10 . 1145 / 3487351 . 3488324 . [ 73 ] S . Elsworth , S . GÂ¨uttel , Time Series Forecasting Using LSTM Networks : A Symbolic Approach , arXiv : 2003 . 05672 [ cs , stat ] ( 2020 ) . URL : http : / / arxiv . org / abs / 2003 . 05672 , arXiv : 2003 . 05672 . [ 74 ] S . Mehtab , J . Sen , S . Dasgupta , Robust Analysis of Stock Price Time Series Using CNN and LSTM - Based Deep Learning Mod - els , in : 2020 4th International Conference on Electronics , Com - munication and Aerospace Technology ( ICECA ) , IEEE , Coimbatore , India , 2020 , pp . 1481 â€“ 1486 . URL : https : / / ieeexplore . ieee . org / document / 9297652 / . doi : 10 . 1109 / ICECA49313 . 2020 . 9297652 . [ 75 ] B . N . Oreshkin , D . Carpov , N . Chapados , Y . Bengio , N - BEATS : Neu - ral basis expansion analysis for interpretable time series forecasting , arXiv : 1905 . 10437 [ cs , stat ] ( 2020 ) . URL : http : / / arxiv . org / abs / 1905 . 10437 , arXiv : 1905 . 10437 . [ 76 ] B . Lim , S . O . Arik , N . Loeï¬€ , T . Pï¬ster , Temporal Fusion Transformers for Interpretable Multi - horizon Time Series Forecasting , arXiv : 1912 . 09363 [ cs , stat ] ( 2020 ) . URL : http : / / arxiv . org / abs / 1912 . 09363 , arXiv : 1912 . 09363 . [ 77 ] A . Koochali , P . Schichtel , S . Ahmed , A . Dengel , Probabilistic Forecast - ing of Sensory Data with Generative Adversarial Networks - ForGAN , IEEE Access 7 ( 2019 ) 63868 â€“ 63880 . URL : http : / / arxiv . org / abs / 1903 . 12549 . doi : 10 . 1109 / ACCESS . 2019 . 2915544 , arXiv : 1903 . 12549 . 38 [ 78 ] T . Davidson , D . Warmsley , M . Macy , I . Weber , Automated Hate Speech Detection and the Problem of Oï¬€ensive Language , arXiv : 1703 . 04009 [ cs ] ( 2017 ) . URL : http : / / arxiv . org / abs / 1703 . 04009 , arXiv : 1703 . 04009 . [ 79 ] T . Davidson , D . Warmsley , M . Macy , I . Weber , Automated hate speech detection and the problem of oï¬€ensive language , in : Proceedings of the International AAAI Conference on Web and Social Media , volume 11 , 2017 , pp . 512 â€“ 515 . 39