Biol . Rev . ( 2002 ) , 77 , pp . 211 – 222 " Cambridge Philosophical Society DOI : 10 . 1017 } S1464793101005875 Printed in the United Kingdom 211 Publication bias in ecology and evolution : an empirical assessment using the ‘ trim and ﬁll ’ method MICHAEL D . JENNIONS " , # * and ANDERS P . MØLLER $ " School of Botany and Zoology , Australian National University , Canberra , A . C . T . 0200 , Australia # Smithsonian Tropical Research Institute , Unit 0948 , APO AA 34002 - 0948 , USA $ Laboratoire d’Ecologie Evolutive Parasitaire , CNRS FRE 2365 , Universite Pierre et Marie Curie , 7 , quai St . Bernard , Case 237 , F - 75252 Paris Cedex 5 , France ( Received 9 July 2001 ; revised 29 October 2001 ; accepted 20 November 2001 ) ABSTRACT Recent reviews of speciﬁc topics , such as the relationship between male attractiveness to females and ﬂuctuating asymmetry or attractiveness and the expression of secondary sexual characters , suggest that publication bias might be a problem in ecology and evolution . In these cases , there is a signiﬁcant negative correlation between the sample size of published studies and the magnitude or strength of the research ﬁndings ( formally the ‘eﬀect size’ ) . If all studies that are conducted are equally likely to be published , irrespective of their ﬁndings , there should not be a directional relationship between eﬀect size and sample size ; only a decrease in the variance in eﬀect size as sample size increases due to a reduction in sampling error . One interpretation of these reports of negative correlations is that studies with small sample sizes and weaker ﬁndings ( smaller eﬀect sizes ) are less likely to be published . If the biological literature is systematically biased this could undermine the attempts of reviewers to summarise actual biology relationships by inﬂating estimates of average eﬀect sizes . But how common is this problem ? And does it really aﬀect the general conclusions of literature reviews ? Here , we examine data sets of eﬀect sizes extracted from 40 peer - reviewed , published meta - analyses . We estimate how many studies are missing using the newly developed ‘trim and ﬁll’ method . This method uses asymmetry in plots of eﬀect size against sample size ( ‘funnel plots’ ) to detect ‘missing’ studies . For random - eﬀect models of meta - analysis 38 % ( 15 } 40 ) of data sets had a signiﬁcant number of ‘missing’ studies . After correcting for potential publication bias , 21 % ( 8 } 38 ) of weighted mean eﬀects were no longer signiﬁcantly greater than zero , and 15 % ( 5 } 34 ) were no longer statistically robust when we used random - eﬀects models in a weighted meta - analysis . The mean correlation between sample size and the magnitude of standardised eﬀect size was also signiﬁcantly negative ( r s ﬂﬁ 0 – 20 , P ! 0 – 0001 ) . Individual correlations were signiﬁcantly negative ( P ! 0 – 10 ) in 35 % ( 14 } 40 ) of cases . Publication bias may therefore aﬀect the main conclusions of at least 15 – 21 % of meta - analyses . We suggest that future literature reviews assess the robustness of their main conclusions by correcting for potential publication bias using the ‘trim and ﬁll’ method . Key words : eﬀect size , fail - safe number , ﬂuctuating asymmetry , funnel plots , meta - analysis , publication bias , trim and ﬁll . * Author for correspondence at address 1 ( e - mail : Michael . Jennions ! anu . edu . au Tel : › 61 2 6125 3540 Fax : › 61 2 6125 5573 ) . 212 Michael D . Jennions and Anders P . Møller CONTENTS I . Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 II . Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 ( 1 ) Data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 ( 2 ) Calculating mean eﬀect sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 ( 3 ) Testing for publication bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 III . Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 IV . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 V . Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 VI . Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 VII . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 VIII . Appendix : the data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 I . INTRODUCTION The use of meta - analysis quantitatively to review a ﬁeld of study is increasingly popular in biology ( Arnqvist & Wooster , 1995 ; Møller & Jennions , 2001 ) . Meta - analysis summarises the literature on a topic by transforming statistical tests of hypotheses into a common metric ( ‘eﬀect size’ ) . ‘Eﬀect size’ is ‘the degree to which the phenomenon is present in a population’ or ‘the degree to which the null hypothesis is false’ ( Cohen , 1988 , pp . 9 – 10 ) . Meta - analysis allows for quantitative answers to questions about the average strength of an hypothesised relationship , or the extent and possible sources of heterogeneity in research ﬁndings . It has clear advantages over traditional narrative reviews but , as with any review process , it assumes that the scientiﬁc literature is unbiased . Ironically , the greater pre - cision provided by meta - analysis has also prompted biologists to question whether the scientiﬁc literature really does accurately reﬂect the results of the many studies biologists initiate ( Csada , James & Espie , 1996 ; Bauchau , 1997 ; Alatalo , Mappes & Elgar , 1997 ; Simmons et al . , 1999 ; Palmer , 1999 ; Poulin , 2000 ; for the medical literature see Song & Gilbody , 1998 ) . There are many forms of bias in the scientiﬁc literature . Some are fairly innocuous such as pref - erential citation of studies supporting the author’s views , or by those of the same nationality ( compre - hensively reviewed by Song et al . , 2000 ) , or even a tendency to cite more often authors with surnames beginning with letters near the start of the alphabet ( Trengenza , 1997 ) . Most troubling , however , is the situation where the magnitude and } or direction of research ﬁndings inﬂuences whether or not a completed study is submitted , positively reviewed and eventually accepted for publication . No mal - evolent intent to suppress ﬁndings is required to generate a ‘publication bias’ , only a systematic prejudice at any stage of the publishing process ( Palmer , 2000 ; Song et al . , 2000 ; Møller & Jennions , 2001 ) . The most widely cited prejudice of researchers , reviewers and editors is towards statistically signiﬁ - cant results ( Palmer , 2000 ; Song et al . , 2000 ; Møller & Jennions , 2001 ) . In practice , there is probably an interaction between sample size and statistical signiﬁcance . For a non - signiﬁcant result to be published sample sizes must be large . This is reasonable because the statistical power to detect a signiﬁcant diﬀerence is low when samples are small ( Cohen , 1988 ) so the null hypothesis of the absence of an eﬀect of a given magnitude ( usually non - zero ) can not be accepted with a reasonable degree of conﬁdence . When a result is signiﬁcant , however , reviewers and editors often ignore sample size . This is not a major problem if the true scientiﬁc relationship being examined is close to zero . It simply means there will be selective reporting of non - signiﬁcant ﬁndings from studies with small samples : when the average relationship is calculated it will still be close to zero . The real problem arises when the true relationship is moderate ( Palmer , 1999 ) . For studies with small samples , the only results published will tend to be those that are signiﬁcant in the direction of the true eﬀect ( very few studies with an estimated eﬀect opposite in direction to the ‘true’ eﬀect will reach signiﬁcance ) . This can lead to a systematic overestimation of the true eﬀect size ( Begg , 1994 ) . Although biologists are now aware of the problem , there has been no systematic attempt to determine its extent ( Møller & Jennions , 2001 ) . Is publication bias so severe that it grossly exaggerates the biological signiﬁcance of certain phenomena , even 213 Publication bias in ecology and evolution generating ‘collective illusions’ ? Palmer ( 2000 ) has argued that this could be the situation , but did not quantify the average eﬀect of publication bias on general conclusions . How widespread is the problem ? Ideally this question is resolved by obtaining information on completed but unpublished studies to see whether their inclusion alters the conclusions of meta - analyses . Unpublished studies are , however , extremely diﬃcult to track down . There are several ways to try to model and even to correct for eﬀects of publication bias ( e . g . weighted distribution theory , general linear models and Bayesian modelling ) ( Begg , 1994 ; Gleser & Olkin , 1996 ) . Unfortunately though the models developed to date are not imple - mented in readily available , user - friendly software ; they require restrictive assumptions about the exact eﬀects of probability values and sample sizes on publishability ; and they are only accessible to those with advanced statistical modelling skills ( DuMochel & Harris , 1997 ) . In a survey of 44 ecological and evolutionary meta - analyses , we identiﬁed a signiﬁcant absence of studies with small sample sizes that present ﬁndings weaker than the weighted average eﬀect size ( Jennions & Møller , 2002 ) . In other words , we found the mean correlation between sample size and the absolute value of the eﬀect size to be signiﬁcantly negative . Speciﬁc published examples of this pheno - menon are given in the second paragraph of Section II . 3 . Publication bias is therefore a general phenom - enon . Here , we use a new and simple method de - veloped by Duvall & Tweedie ( 2000 a , b ) called ‘trim and ﬁll’ to estimate the number of unpublished or ‘missing’ studies . To our knowledge , the only other study to use this approach to estimate the potential impact of publication bias is an analysis by Sutton et al . ( 2000 a ) of a set of meta - analyses of clinical medical trials . According to them , the only previous general statistical assessments of the preva - lence of ‘missing’ studies in a collection of meta - analyses was that of Egger et al . ( 1997 ) . We then test how robust general conclusions in biology are when ‘corrected’ for potential publication bias . II . METHODS ( 1 ) Data set We made an extensive survey of the ecological and evolutionary literature for meta - analyses published up until the end of 2000 . We examined the journals American Naturalist , Animal Behaviour , Behavioral Ecol - ogy , Behavioral Ecology and Sociobiology , Ecological Monographs , Ecology , Evolution , Evolutionary Biology , Journal of Evolutionary Biology , and Quarterly Review of Biology . We also entered the phrase ‘meta - analy * ’ into the electronic database ‘WebSpiris’ to ﬁnd papers where this term occurred in the title or abstract . We then examined the title and place of publication of each paper listed and directly in - spected any that seemed related to evolutionary or ecological biology . Furthermore , we contacted a number of colleagues who have used meta - analyses in their research to locate meta - analyses currently in press . We excluded meta - analyses of genetic herita - bilities because it is unclear whether h # itself or an eﬀect size based on the strength ( rather than slope ) of the phenotypic relationship between relatives is the more appropriate eﬀect size . Palmer ( 2000 ) has already shown that the problem of publication bias is especially severe for h # because negative values are biologically irrelevant and therefore under - reported . We found 40 peer - reviewed meta - analyses where we could also obtain eﬀect sizes and variances for the original studies ( either because they were included as appendices in the published paper or where the authors kindly responded to our request and sent us the data ) . The ability to detect unpublished studies relies on the asymmetric distribution of eﬀect sizes ( see below ) . Such asymmetry is unlikely to be detected with smaller samples . We therefore set a minimum sample size of eight studies per meta - analysis ( Sutton et al . , 2000 a used a minimum of 10 ) . This only removed one otherwise usable meta - analysis ( Fernandez - Duque & Valeggia , 1994 ) . The meta - analyses we used are listed in Section VIII . Most of the 40 original meta - analyses asked several diﬀerent questions ( i . e . examined several response variables ) . In such cases , diﬀerent response variables were often taken from the same or a closely overlapping set of original empirical studies . To be statistically conservative , we limited our analysis to one response variable per original meta - analysis ( that with the largest sample size ) . In addition , the original authors often found signiﬁcantly more heterogeneity in eﬀect size among studies than could be explained by sampling error . They therefore looked for underlying structure in the data by classifying studies into groups ( e . g . birds versus insects ) and testing for signiﬁcant among - group variance in eﬀect sizes for each categorical factor using Q b ( Q b is a measure of the variance in eﬀect size accounted for by diﬀerences among groups ) ( Rosenberg , Adams & Gurevitch , 2000 ) . For each of the original meta - analyses we therefore split the data using whichever categorical factor generated the 214 Michael D . Jennions and Anders P . Møller greatest diﬀerences in eﬀect sizes among groups ( but only if P ! 0 – 05 for Q b ) . Finally , using these criteria we selected the group with the largest number of empirical studies ( mean ‡ s . e . m . ﬂ 45 – 1 ‡ 6 – 97 , range ﬂ 8 – 246 ) . If there were two or more groups with an equal number of studies we picked one at random . We used the same eﬀect size type as the original authors . These were Pearson’s r ( N ﬂ 21 ) , Hedges’ d ( N ﬂ 10 ) , the natural log of the response ratio ( ln RR ) ( N ﬂ 7 ) or a customised eﬀect size ( N ﬂ 2 ) . ( 2 ) Calculating mean eﬀect sizes We calculated mean eﬀect sizes weighted for sample size using the software package Metawin 2 . 0 ( Rosenberg et al . , 2000 ) . We ran both ﬁxed - eﬀect ( FE ) and random - eﬀect ( RE ) models for each of the 40 data sets . FE models assume a single true eﬀect common to all the studies . Variation in the observed eﬀects is solely attributed to sampling error . RE models allow for a true random component as a source of variation in eﬀect size between studies , as well as sampling error . In general , RE models are preferred ( N . R . C . , 1992 ) , especially in biology where there is almost certainly real variation in actual eﬀect sizes among diﬀerent taxa or ecosystems ( Gurevitch & Hedges , 1999 ) . Some earlier meta - analyses , however , only used FE models . Our estimates of weighted mean eﬀect sizes may diﬀer slightly from those reported in the original papers because of rounding errors and } or minor diﬀerences in the coding of original studies . We must stress that our main intent is not to criticise individual studies , but to highlight wider trends . We then used bootstrapping with 999 replications to calculate bias - corrected 95 % conﬁdence intervals . This does not require that eﬀect sizes are parametrically distributed . The weighted mean eﬀect is signiﬁcantly diﬀerent from zero if the 95 % conﬁdence intervals do not overlap zero . Analyses based solely on parametric conﬁdence intervals yielded the same conclusions in 139 out of 146 cases . ( 3 ) Testing for publication bias A funnel plot of eﬀect size against log - transformed sample size should produce a funnel shape symmetric around the ‘true’ eﬀect size ( Light & Pillemer , 1984 ) . Purely due to sampling error ( the larger the sample the more accurate the estimate ) the variance in estimates of the ‘true’ eﬀect size is higher for studies with smaller samples . The observed eﬀect sizes should be normally distributed around the mean eﬀect with no trend in relation to sample size ( Light & Pillemer , 1984 ; Begg , 1994 ) . If studies with statistically signiﬁcant results are more likely to be published , however , and the true mean is close to zero , this will produce a ‘hollowed out’ funnel ( see Palmer , 2000 ) . If the true eﬀect is moderate and non - signiﬁcant results tend not to be published , this will produce a skewed funnel in which the magnitude of the eﬀect size decreases as sample size increases ( Begg & Mazumdar , 1994 ; Palmer , 1999 ) . This second publication bias will lead to an inaccurate estimate of the true eﬀect size . Of course , a skewed funnel plot can be caused by factors other than publication bias since prior knowledge of eﬀect sizes from pilot studies , reduced sample sizes for certain species , choice of eﬀect measures , chance and many other confounding variables may also create asym - metric plots ( Thornhill , Møller & Gangestad , 1999 ; Gurevitch & Hedges , 1999 ) . Even so , the robustness of meta - analytic conclusions can be tested by making the conservative assumption that skew is due to publication bias . In the biological sciences , aside from the fail - safe number ( see below ) , the only statistical approach widely used to test for publication bias has been to test for a signiﬁcant relationship between sample size and eﬀect size using rank correlation tests ( Begg & Mazumdar , 1994 ; for related approaches see Macaskill , Walter & Irwig , 2001 ; Møller & Jennions , 2001 ) . Palmer ( 1999 ) has called this correlation r bias . If studies with small samples are only published when they have signiﬁcant results , and the ‘true’ eﬀect is moderate , the funnel plot will be skewed . There will be a decline in the magnitude of the eﬀect size with increasing sample size because for studies with small sample sizes there is less likelihood that an eﬀect opposite in magnitude to the ‘true’ eﬀect will reach statistical signiﬁcance . This decline has now been reported in a few speciﬁc ﬁelds of study ( e . g . Palmer , 1999 , 2000 ; Gontard - Danek & Møller , 1999 ; Jennions , Møller & Petrie , 2001 ) . Recently , using 232 data sets from 44 evolutionary ecology meta - analyses that included most of the current database , we found that the average re - lationship is a highly signiﬁcant , but small , decline in eﬀect size with sample size ( Jennions & Møller , 2002 ) . Here , we estimate these correlations using Spearman’s r speciﬁcally to determine whether r bias and the estimated number of studies missing based on funnel plot asymmetry ( see below ) are related . Although several authors , including ourselves , have previously presented the correlation between sample size and eﬀect size ( e . g . Palmer , 2000 ; Jennions et al . , 2001 ) , strictly speaking , eﬀect size should ﬁrst be 215 Publication bias in ecology and evolution standardised to conform to the assumptions of the test ( see Begg & Mazumdar , 1994 ) . We therefore standardised eﬀect size here , although this makes little diﬀerence in most cases ( M . D . Jennions & A . P . Møller , unpublished data ) . More recently , the funnel plot has been used to derive a non - parametric method of testing and adjusting for possible publication bias in meta - analysis . The ‘trim and ﬁll’ method of Duvall & Tweedie ( 2000 a , b ) estimates the number of ‘miss - ing’ studies due to publication bias . The method is reliant on the symmetric distribution of eﬀect sizes around the ‘true’ eﬀect size if there is no publication bias , and the simple assumption that the most extreme results have not been published . These will usually be studies with smaller sample sizes , because variance in eﬀect size ( hence extreme values ) increases as sample size decreases . Once the number of ‘missing’ studies is estimated , one recalculates the weighted mean eﬀect size and its variance when they are incorporated . The statistical procedure involves an iterative process ( Duvall & Tweedie , 2000 a , b ) . To start , one calculates the weighted mean eﬀect size for the full data set and then ‘trims oﬀ’ the outlying part of the funnel plot that is asymmetrical with respect to the mean . Simple formulae are used to estimate the number of studies in the asymmetric part . These studies are then temporarily removed ( ‘trimmed’ ) and the remainder used to re - estimate the weighted mean eﬀect . Then , again using the full set of studies , one ‘trims oﬀ’ those studies asymmetrical with respect to the new estimate of the mean . After just a few iterations the estimate of the number of studies that need to be trimmed reaches an asymptotic value . One can now ‘ﬁll in’ the ‘missing’ studies . These are simply the mirror - image counterparts of the trimmed studies around the ﬁnal weighted mean eﬀect estimated using the symmetric portion of the data set . The missing studies are given the same variance as their corresponding ‘trimmed’ counter - parts . Finally , the full data set that includes the trimmed , missing and remaining studies is used to calculate the new mean eﬀect size and its conﬁdence intervals . Duval & Tweedie ( 2000 a , b ) present three dif - ferent estimators ( R ! , L ! , Q ! ) for the number k of missing studies . Of these , L ! is the best general - purpose estimator . Here , we follow Sutton et al . ( 2000 a ) in using L ! to calculate k using formulae entered onto an Excel spreadsheet ( freely available on request ) . We did this for both FE and RE models in case the choice of model has an impact on the assessment of publication bias . However , because RE models are more appropriate we place greater emphasis on the results for these models ( N . R . C . , 1992 ) . Of course , chance asymmetry in a funnel plot will lead to positive values of L ! ( Sterne , 2000 ) . We therefore also estimated how many meta - analyses had a value of L ! that was signiﬁcant at the 0 – 05 level ( i . e . more studies missing than expected by chance ) . Critical values of L ! were estimated by extrapolating from the simulations in Table 4 of Duvall & Tweedie ( 2000 b ) . These are therefore crude approximations . Finally , we also calculated the fail - safe number of studies needed to nullify an eﬀect at the 5 % level following Rosenthal ( 1991 , p . 104 ) . This number estimates how many studies with a mean eﬀect of zero are needed to change a signiﬁcant eﬀect to a non - signiﬁcant one at the stated P level ( here signiﬁcance must be calculated using parametric 95 % conﬁdence intervals ) . A value of X greater than 5 K › 10 is usually considered to indicate a robust conclusion , where K is the reported number of studies . Unless otherwise stated data are presented as the mean ‡ s . e . m . For non - signiﬁcant tests , we pres - ent the statistical power to detect a medium eﬀect as deﬁned by Cohen ( 1988 ) with P ( two - tailed ) ﬂ 0 – 05 . III . RESULTS Of the 40 meta - analyses , the initial estimate of weighted mean eﬀect size diﬀered signiﬁcantly from zero ( P ! 0 – 05 ) in 38 RE models and 35 FE models . There were only three cases in which the conclusion diﬀered depending on the choice of model . The weighted mean eﬀects estimated using RE models were , on average , 29 % greater than those from FE models ( Wilcoxon’s test , Z ﬂ 3 – 92 , N ﬂ 40 , P ﬂ 0 – 0001 ) . With RE models , 30 out of 40 meta - analyses were estimated to have one or more missing studies ( L ! " 0 ) ; with FE models this rose to 36 meta - analyses . We then noted whether the probability of each observed L ! was less than 0 – 05 . For RE models , 15 meta - analyses showed a signiﬁcant publication bias ; for FE models , 20 were signiﬁcant . Thus , 38 – 50 % of published meta - analyses show strong evidence for publication bias . The estimated number of studies missing was positively correlated for the 36 meta - analyses where both model types indicated that the studies were missing from the same side of the distribution ( or where no study was missing for one or both models ) ( r ﬂ 0 – 429 , P ﬂ 0 – 009 , N ﬂ 36 ) . There were , however , four cases where the side of the distribution from which the studies were missing 216 Michael D . Jennions and Anders P . Møller 0 0 . 1 0 . 2 0 . 3 0 . 4 – 0 . 05 0 0 . 05 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 . 35 Original effect size ( Z ) R e c a l c u l a t e d e ff e c t s i z e ( Z ) Original effect size ( Z ) R e c a l c u l a t e d e ff e c t s i z e ( Z ) 0 . 6 0 . 4 0 . 2 – 0 . 2 – 0 . 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 Original effect size ( Hedges’ d ) R e c a l c u l a t e d e ff e c t s i z e ( H e dg e s ’ d ) 0 . 4 0 . 2 0 0 . 6 0 . 8 1 1 . 2 0 0 . 5 1 1 . 5 Original effect size ( Hedges’ d ) R e c a l c u l a t e d e ff e c t s i z e ( H e dg e s ’ d ) 0 . 5 0 0 1 1 . 5 2 2 . 5 0 . 5 1 1 . 5 2 2 . 5 Original effect size ( InRR ) R e c a l c u l a t e d e ff e c t s i z e ( I n RR ) 0 . 2 0 0 1 0 . 2 0 . 4 0 . 6 0 . 8 1 0 . 4 0 . 6 0 . 8 1 . 2 1 . 4 1 . 6 Original effect size ( InRR ) R e c a l c u l a t e d e ff e c t s i z e ( I n RR ) 0 . 2 0 0 1 0 . 2 0 . 4 0 . 6 0 . 8 1 0 . 4 0 . 6 0 . 8 1 . 2 1 . 4 1 . 6 A B C D E F 0 Fig . 1 . Original versus recalculated eﬀect sizes for : ( A ) ﬁxed - eﬀects ( FE ) models for eﬀect type Z - transformed r ( Z new ﬂﬁ 0 – 004 › 0 – 900 Z ; r # adj ﬂ 77 – 8 % , F " , " * ﬂ 71 – 1 , P ! 0 – 0001 , N ﬂ 21 ; ( B ) random - eﬀects ( RE ) models for Z ( Z new ﬂﬁ 0 – 0261 › 1 – 120 Z ; r # adj ﬂ 85 – 3 % , F " , " * ﬂ 116 – 7 , P ! 0 – 0001 , N ﬂ 21 ) ; ( C ) FE models for eﬀect type Hedges’ d ( d new ﬂ 0 – 008 › 0 – 731 d ; r # adj ﬂ 72 – 1 % , F " , ) ﬂ 24 – 2 , P ﬂ 0 – 0012 , N ﬂ 10 ) ; ( D ) RE models for Hedges’ d ( d new ﬂ 0 – 125 › 0 – 763 d ; r # adj ﬂ 81 – 1 % , F " , ) ﬂ 39 – 6 , P ﬂ 0 – 0002 , N ﬂ 10 ) ; ( E ) FE models for eﬀect type natural log of the re - sponse ratio ( ln RR new ﬂﬁ 0 – 110 › 1 – 463ln RR ; r # adj ﬂ 76 – 9 % , F " , & ﬂ 21 – 0 , P ﬂ 0 – 006 , N ﬂ 7 ) ; ( F ) RE models for ln RR ( ln RR new ﬂﬁ 0 – 095 › 1 – 468ln RR ; r # adj ﬂ 80 – 7 % , F " , & ﬂ 26 – 1 , P ﬂ 0 – 004 , N ﬂ 7 ) . In all graphs , the line of equality is shown . diﬀered between FE and RE models . On average , the estimate of the number of missing studies was higher for RE models than for FE models ( sign test , P ﬂ 0 – 031 ; 7 – 1 ‡ 1 – 2 versus 5 – 8 ‡ 1 – 1 ) . For FE models in 75 % ( 27 } 36 ) of cases where we estimated that studies were missing , their addition moved the mean eﬀect closer to zero . There was thus a signiﬁcant trend for missing studies to make conclusions about the weighted mean eﬀect less robust ( binomial test , P ! 0 – 005 ) . For RE models , however , in only 57 % ( 17 } 30 ) of cases where studies were missing did their addition move the mean eﬀect closer to zero . Correction for missing studies was therefore equally likely to reduce or increase the robustness of conclusions about the weighted mean eﬀect ( binomial test , P ﬂ 0 – 58 ; power : 36 % ) . Once 217 Publication bias in ecology and evolution Log ( original fail - safe ) L og ( r e c a l c u l a t e d f a il - s a f e ) 0 0 1 2 3 4 5 1 2 3 4 5 Log ( original fail - safe ) L og ( r e c a l c u l a t e d f a il - s a f e ) 0 0 1 2 3 4 5 1 2 3 4 5 A B Fig . 2 . Log " ! fail - safe number ( X ) before and after recalculation for : ( A ) ﬁxed - eﬀects models ( X new ﬂ ﬁ 0 – 0083 › 0 – 8778 X ; ( r # adj ﬂ 41 – 0 % , F " , $ ) ﬂ 28 – 1 , P ! 0 – 0001 ) ; ( B ) mixed - eﬀects models ( X new ﬂ 0 – 0501 › 0 – 965 X ; ( r # adj ﬂ 63 – 8 % , F " , $ ) ﬂ 69 – 7 , P ! 0 – 0001 ) ( both N ﬂ 40 ) . we had added missing cases , we recalculated eﬀect sizes to see if this changed the conclusions of the original meta - analyses based on 95 % conﬁdence intervals . For FE models , in three cases an initially signiﬁcant result became non - signiﬁcant . For RE models , in eight cases a signiﬁcant result became non - signiﬁcant . ( These 11 cases were from 11 diﬀerent meta - analyses . ) No means that were orig - inally judged non - signiﬁcant became signiﬁcant . In sum , 8 – 21 % of published meta - analyses may have been interpreted incorrectly . The potential for publication bias was also reﬂected in the correlation between eﬀect size variance and the magnitude of the observed eﬀect size ( standardised ) . The mean Begg – Mazumdar correlation was r s ﬂﬁ 0 – 201 ( t ﬂ 5 – 34 , d . f . ﬂ 39 , P ! 0 – 0001 ) . Thus , the eﬀect size was closer to zero as sample size increased . There were 14 signiﬁcantly negative correlations ( at P ! 0 – 10 ) , but no signiﬁ - cantly positive correlations . [ Begg & Mazumdar ( 1994 ) recommended the use of P ﬂ 0 – 10 because of the low power of the test . ] We then tested if the Begg – Mazumdar correlation predicts the number of studies that are missing according to ‘trim and ﬁll’ methods and whether these studies are greater or smaller than the weighted mean eﬀect . We coded L ! as positive if the addition of missing studies decreased the mean eﬀect . When the Begg – Mazumdar r s is negative , this implies that studies with eﬀects smaller than the weighted mean are missing so L ! should be positive . We therefore predict that L ! and Begg – Mazumdar r s are negatively correlated . Because the absolute value of L ! will increase with sample size by chance , we ﬁrst divided L ! by the number of studies in the meta - analysis . As predicted there was a signiﬁcant negative correlation for both FE models ( r ﬂﬁ 0 – 423 , P ﬂ 0 – 007 , N ﬂ 40 ) and RE models ( r ﬂﬁ 0 – 397 , P ﬂ 0 – 011 , N ﬂ 40 ) . Of course , a change from a signiﬁcant to a non - signiﬁcant weighted mean eﬀect is a crude measure of the importance of missing studies if the real aim is to see how robust the original estimate of eﬀect size is to publication bias . We examined this by looking at the three main eﬀect types separately . For Pearson’s r , the mean percentage change ( calculated as the absolute diﬀerence in r before and after recalculation divided by the original value of r r r ) was 57 – 4 ‡ 27 – 1 % for FE models and 36 – 5 ‡ 11 – 8 % for RE models . This corresponded to mean absolute changes in r of 0 – 037 ‡ 0 – 008 and 0 – 039 ‡ 0 – 008 , respectively ( N ﬂ 21 ) . For Hedges’ d the mean percentage change was 30 – 3 ‡ 7 – 9 % for FE models and 17 – 0 ‡ 5 – 4 % for RE models . This corresponded to absolute changes in d of 0 – 187 ‡ 0 – 053 and 0 – 153 ‡ 0 – 061 , respectively ( N ﬂ 10 ) . For the natural log of the response ratio ( ln RR ) the mean percentage change was 20 – 9 ‡ 10 – 9 % for FE models and 27 – 0 ‡ 10 – 2 % for RE models . This corresponded to absolute changes in ln RR of 0 – 121 ‡ 0 – 086 and 0 – 135 ‡ 0 – 085 , respectively ( N ﬂ 7 ) . The percentage change is large even though the absolute diﬀerence in eﬀect size estimates is small because in evolution and ecology mean eﬀect sizes are weak ( mean r ﬂ 0 – 21 – 0 – 27 ; A . P . Møller & M . D . Jennions , in preparation ) . Original and recalculated eﬀect sizes were similar , but 15 – 28 % of the variation in recalculated mean eﬀect size is unexplained by the original estimate of eﬀect size ( Fig . 1 ) . When results are non - signiﬁcant , researchers often use an a priori estimate of eﬀect size to calculate power . In the absence of pilot studies , they may rely on a general estimate of eﬀect sizes for relationships in their ﬁeld of study ( e . g . A . P . Møller & M . D . Jennions , in preparation ) . Most researchers , how - ever , provide power for statistical tests assuming that the true eﬀect is small , medium or large as deﬁned by 218 Michael D . Jennions and Anders P . Møller Cohen ( 1988 ) ( e . g . small : r ﬂ 0 – 1 , d ﬂ 0 – 2 ; medium : r ﬂ 0 – 2 , d ﬂ 0 – 5 ; large r ﬂ 0 – 3 , d ﬂ 0 – 8 ) . We therefore classiﬁed eﬀect sizes before and after recalculation using the criteria of Cohen ( 1988 ) . We could only do this for 31 meta - analyses ( those using r or Hedges’ d ) . For RE models , the weighted mean eﬀect remain unchanged and was classiﬁed as large for seven , medium for 10 and small for 11 estimates . One large estimate became medium , and one medium estimate became small ; while one small estimate became medium . Thus , 10 % ( 3 } 31 ) of eﬀect sizes had to be reclassiﬁed after correcting for publication bias . Finally , we looked at the robustness of weighted mean eﬀects by examining fail - safe numbers . By the convention that X " ( 5 K › 10 ) is robust , 85 % ( 34 } 40 ) of the original weighted mean eﬀect esti - mates were robust . With FE models , 20 – 6 % ( 7 } 34 ) of the estimates were no longer robust after being recalculated . For RE models , 14 – 7 % ( 5 } 34 ) of the eﬀect sizes were no longer robust after being recalculated . For the 34 original results that were robust , in 11 cases for FE models ( 32 % ) and in 21 cases for RE models ( 62 % ) , the weighted mean eﬀect was the same or more robust after recalculation ( Fisher’s Exact test , P ﬂ 0 – 028 ) ( Fig . 2A , B ) . IV . DISCUSSION To start , there was broad agreement between FE and RE models as to whether or not weighted mean eﬀects were signiﬁcant , although estimates from RE models were approximately 29 % larger . ( The larger mean estimates for RE models may ‘compensate’ for the generally broader conﬁdence intervals for RE models which , all else being equal , should decrease the likelihood of reporting a signiﬁcant eﬀect size with RE models . ) If anything , the initial use of FE models by biologists may have led to weighted mean eﬀect sizes being slightly underestimated . The use of the ‘trim and ﬁll’ method , however , showed a signiﬁcant number of ‘missing’ studies for 38 % of RE model meta - analyses and 50 % of FE model meta - analyses . On average , the number of studies missing was signiﬁcantly greater for RE models . Previously , the Begg – Mazumdar correlation has been the main test used to detect possible publication bias . As expected , we found signiﬁcant agreement between estimates of publication bias based on this correlation and those based on the ‘trim and ﬁll’ approach . ( At present though there is insuﬃcient data to determine whether the Begg – Mazumdar correlation can be used as a ‘short - cut’ to predict the eﬀect of adding ‘missing’ studies in terms of the robustness of recalculated eﬀect sizes . ) Furthermore , individually signiﬁcant Begg – Mazumdar correla - tions also suggest a publication bias in 35 % of the 40 meta - analyses . The asymmetry in funnel graphs previously described for speciﬁc topics in ecology and evolution ( e . g . Palmer , 2000 ; Jennions et al . , 2001 ) therefore appears to reﬂect a more general phenomenon ( Jennions & Møller , 2002 ) . Although alternative explanations for a skewed funnel graph should not be neglected ( Thornhill et al . , 1999 ) , publication bias could be a potential problem for at least one in three meta - analyses . Correcting for publication bias using the ‘trim and ﬁll’ method of Duvall & Tweedie ( 2000 a , b ) led to three out of 35 and eight out of 38 initially signiﬁcant weighted mean eﬀects becoming non - signiﬁcant for FE and RE models , respectively . The weighted mean eﬀect size was no longer robust after being recalculated for 21 % of FE models and 15 % of RE models . If these ﬁndings can be generalized to future studies then 15 – 21 % of meta - analyses in ecology and evolution could reach erroneous conclu - sions if no correction is made for publication bias . By contrast , Sutton et al . ( 2000 a ) suggested that only 5 – 10 % of medical meta - analyses might have reached an incorrect interpretation because of publication bias . There was one signiﬁcant and unexpected diﬀer - ence between FE and RE models . For FE models , 71 % of recalculated weighted means were less robust because missing studies reduced the mean eﬀect size . By contrast , for RE models , 59 % were the same or more robust . For RE models , the ‘missing studies’ were often larger than the initial weighted mean . This is not as predicted by publication bias and suggests that asymmetry in funnel plots may have other , as yet unknown , causes . Sutton et al . ( 2000 a ) only reported decreases in weighted mean eﬀect size in an analysis of 48 medical meta - analyses , although this was not the case when one analyses other datasets ( R . Tweedie , personal communication ) . Chance asymmetry provides an insuﬃcient expla - nation for our ﬁndings because in eight of the 13 cases where missing studies had eﬀect sizes smaller than the weighted mean eﬀect size the number of studies missing was signiﬁcantly more than expected by chance . The diﬀerence between the medical studies of Sutton et al . ( 2000 a ) and the ecological } evolutionary studies analysed here may relate to diﬀerences in the range of sample sizes . One or two studies with unusually strong eﬀects and larger sample sizes can generate a weighted mean eﬀect that is larger than most of the reported eﬀect sizes . 219 Publication bias in ecology and evolution For example , the removal of three out of 84 cases with large eﬀects and small variances from Gontard - Danek & Møller ( 1999 ) changes the situation from an estimate of 14 missing studies larger than the mean and a recalculated weighted mean of r ﬂ 0 – 43 , to no missing studies and the original weighted mean of r ﬂ 0 – 35 . The eﬀect of publication bias in ecology and evolution when using RE models for meta - analysis , at least for the currently available data , is therefore idiosyncratic . In eight cases , the weighted mean became statistically non - signiﬁcant , but in 13 cases the mean stayed signiﬁcant and was actually more robust after correcting for publication bias . In general though , we suggest that any increase in robustness be disregarded . The observed asymmetry in the funnel plots could even be due to selective reporting of studies with eﬀects smaller than the average eﬀect , perhaps because of a prejudice or ‘backlash’ against a well - established idea or so - called ‘bandwagons’ ( Palmer , 2000 ; Poulin , 2000 ) . For now , however , we think it is best to be conservative and simply ask whether results remain robust after correcting for funnel - plot asymmetry . Sterne ( 2000 ) criticized Sutton et al . ( 2000 a ) , saying that the ‘trim and ﬁll’ method leads to false positive claims of missing studies . This is true , although the criticism can partly be responded to by highlighting how often the number of missing studies is too large to be attributed to chance alone ( i . e . P ! 0 – 05 ) . Here , this occurred for 38 % of the random - eﬀects models . More generally , Sutton et al . ( 2000 b ) emphasise that the critical aim of ‘trim and ﬁll’ is not to quantify exactly how many studies are really missing . Rather , it is to test whether results are robust and conclusions unchanged when we correct for possible bias . Here , we show that for the preferred RE model approach , 21 % ( eight of 38 meta - analyses ) were not robust to potential publication bias . We therefore strongly recommend that authors of meta - analyses routinely include estimates of recalculated eﬀect sizes using ‘trim and ﬁll’ methods . These methods are easy to apply and , along with fail - safe numbers ( Rosenthal , 1991 ) and Begg – Mazumdar correlations ( Begg & Mazumdar , 1994 ) , allow readers to decide for themselves how sensitive a reported signiﬁcant relationship is to potential bias . Finally , reviewers should also be careful about drawing conclusions about factors that lead to heterogeneity in eﬀect sizes . At present , there is no way of knowing what the eﬀect of ‘missing’ studies will be on tests of signiﬁcant variation in eﬀect size among diﬀerent groups . Signiﬁcant between - group heterogeneity should therefore be assessed in the light of the number of missing studies . Perhaps ‘missing’ studies could be conservatively assigned so as to reduce between - group heterogeneity to test whether group diﬀer - ences are still statistically signiﬁcant . V . CONCLUSIONS ( 1 ) If asymmetry in a ‘funnel plot’ of eﬀect size against sample size is due to publication bias , the ‘trim and ﬁll’ method indicates that 38 – 50 % of data sets have a signiﬁcant publication bias as indicated by an excess of ‘missing’ studies . ( 2 ) The more familiar Begg – Mazumdar corre - lation also showed that , on average , eﬀect sizes are closer to zero as sample size increases . In general , the ﬁndings of the ‘trim and ﬁll’ method and Begg – Mazumdar correlation were in agreement . ‘Trim and ﬁll’ , however , has the advantage that a ‘correction’ for possible publication bias can be made by adding ‘missing’ studies . ( 3 ) In 75 % of the cases analysed using ﬁxed - eﬀects models , the addition of missing studies moved the mean eﬀect closer to zero . For random - eﬀects models , the equivalent ﬁgure was 57 % . More importantly , 8 – 21 % of data sets where the original estimate of the average relationship diﬀered signiﬁ - cantly from zero became non - signiﬁcant . ( 4 ) Stated slightly diﬀerently , using the con - ventional deﬁnition of a ‘robust’ result based on Rosenthal’s fail - safe numbers , 15 – 21 % of eﬀect size estimates were no longer robust after being recalcu - lated to include ‘missing’ studies . ( 5 ) We conclude that publication bias is a potential problem for reviewers . This is most clearly seen when a quantitative reviewing method like meta - analysis is used , but is equally true for narrative reviews . ( 6 ) Reviewers should always test for publication bias . Aside from established techniques , we speciﬁ - cally recommend the use of ‘trim and ﬁll’ . It is the only method that allows one to estimate conserva - tively whether publication bias has inﬂuenced the conclusions reached by a reviewer . It is easy to use and does not require expensive software or advanced statistical skills . VI . ACKNOWLEDGEMENTS We thank Go $ ran Arnqvist , Michael Brett , Isabelle Co # te ! , Peter Curtis , Mark Forbes , Peter Hamback , Nick Jonsson , Julia Koricheva , Dean McCurdy , Fiorenza Micheli , Iago Mosqueira , Robert Poulin , Howie Riessen , Michael Rosenberg , Gina Schalk , Xianzhong Wang and Peter van 220 Michael D . Jennions and Anders P . Møller Zandt and others we may have inadvertently omitted for kindly providing unpublished information . J . Shykoﬀ , D . Pope . B . Backwell and J . Christy kindly discussed issues of meta - analysis and the general idea behind performing the present study . M . D . J . thanks the Director of STRI for bridging funding . Papers describing the ‘trim and ﬁll’ method can be downloaded for free at : http : } } www . biostat . umn . edu } C tweedie } documents } tweediecurrentpapers . html . VII . REFERENCES A latalo , R . V . , M appes , J . & E lgar , M . A . ( 1997 ) . Heritabi - lities and paradigm shifts . Nature 385 , 402 – 403 . A rnqvist , G . & W ooster , D . ( 1995 ) . Meta - analysis : synthe - sizing research ﬁndings in ecology and evolution . Trends in Ecology and Evolution 10 , 236 – 240 . B auchau , V . ( 1997 ) . Is there a ‘‘ﬁle drawer problem’’ in biological research ? Oikos 79 , 407 – 409 . B egg , C . B . ( 1994 ) . Publication bias . In The Handbook of Research Synthesis ( eds . H . Cooper and L . V . Hedges ) , pp . 399 – 409 . Russel Sage Foundation , New York . B egg , C . B . & M azumdar , M . ( 1994 ) . Operating characteristics of a rank correlation test for publication bias . Biometrics 50 , 1088 – 1101 . C ohen , J . ( 1988 ) . Statistical Power Analysis for the Behavioral Sciences . 2d ed . L . Erlbaum , Hillsdale , New Jersey . C sada , R . D . , J ames , P . C . & E spie , R . H . M . ( 1996 ) . The ‘‘ﬁle drawer problem’’ of non - signiﬁcant results : does it apply to biological research ? Oikos 76 , 591 – 593 . D u M ochel , W . & H arris , J . ( 1997 ) . Comments on Givens , G . H . , Smith , D . D . and Tweedie , R . L . ( 1997 ) Publication bias in meta - analysis : A Bayesian data - augmentation ap - proach to account for issues exempliﬁed in the passive smoking debate ( with discussion ) . Statistical Science 12 , 244 – 245 . D uvall , S . & T weedie , R . ( 2000 a ) . A non - parametric ‘trim and ﬁll’ method of assessing publication bias in meta - analysis . Journal of the American Statistical Association 95 , 89 – 98 . D uvall , S . & T weedie , R . ( 2000 b ) . Trim and ﬁll : A simple funnel - plot - based method of testing and adjusting for pub - lication bias in meta - analysis . Biometrics 56 , 455 – 463 . E gger , M . , D avey - S mith , G . , S chneider , M . & M inder , C . ( 1997 ) . Bias in meta - analysis detected by a simple , graphical test . British Medical Journal 315 , 629 – 634 . F ernandez - D uque , E . & V aleggia , C . ( 1994 ) . Meta - analysis : a valuable tool in conservation research . Conservation Biology 8 , 555 – 561 . G leser , L . J . & O lkin , I . ( 1996 ) . Models for estimating the number of unpublished studies . Statistics in Medicine 15 , 2493 – 2507 . G ontard - D anek , M . C . & M łller , A . P . ( 1999 ) . The strength of sexual selection : A meta analysis of bird studies . Behavioral Ecology 10 , 476 – 486 . G urevitch , J . & H edges , L . V . ( 1999 ) . Statistical issues in ecological meta - analyses . Ecology 80 , 1142 – 1149 . J ennions , M . D . & M łller , A . P . ( 2002 ) . Relationships fade with time : a meta - analysis of temporal trends in publication in ecology and evolution . Proceedings of the Royal Society of London B ( in press ) . J ennions , M . D . , M łller , A . P . & P etrie , M . ( 2001 ) . Sexually selected traits and adult survival : a meta - analysis . Quarterly Review of Biology 76 , 3 – 36 . L ight , R . J . & P illemer , D . B . ( 1984 ) . Summing Up : The Science of Reviewing Research . Harvard University Press , Cambridge , Massachusetts . M acaskill , P . , W alter , S . D . & I rwig , L . ( 2001 ) . A comparison of methods to detect publication bias in meta - analysis . Statistical Medicine 20 , 641 – 654 . M łller , A . P . & J ennions , M . D . ( 2001 ) . Testing and adjusting for publication bias . Trends in Ecology and Evolution 16 , 580 – 586 . N . R . C . Committee on Applied and Theoretical Statistics . ( 1992 ) . Combining Information : Statistical Issues and Opportunities for Research . National Academy Press , Washington , D . C . P almer , A . R . ( 1999 ) . Detecting publication bias in meta - analysis : a case study of ﬂuctuating asymmetry and sexual selection . American Naturalist 154 , 220 – 233 . P almer , A . R . ( 2000 ) . Quasireplication and the contract of error : lessons from sex ratios , heritabilities and ﬂuctuating asymmetry . Annual Reviews of Ecology and Systematics 31 , 441 – 480 . P oulin , R . ( 2000 ) . Manipulation of host behaviour by parasites : a weakening paradigm ? Proceedings of the Royal Society of London B 267 , 787 – 792 . R osenberg , M . S . , A dams , D . C . & G urevitch , J . ( 2000 ) . MetaWin : Statistical Software for Meta - analysis . Version 2 . 0 . Sinauer Associates , Massachusetts . R osenthal , R . ( 1991 ) . Meta - analytic Procedures for Social Research . Sage Foundation , Newbury Park , California . S immons , L . W . , T omkins , J . L . , K otiaho , J . S . & H unt , J . ( 1999 ) . Fluctuating paradigm . Proceedings of the Royal Society of London B 266 , 593 – 595 . S ong , F . , E astwood , A . J . , G ilbody , S . , D uley , L . & S utton , A . J . ( 2000 ) . Publication and related biases . Health Technology Assessment 4 ( 10 ) , 1 – 115 . S ong , F . & G ilbody , S . ( 1998 ) . Increase in studies of publication bias coincided with increasing use of meta - analysis . British Medical Journal 316 , 471 . S terne , J . A . C . ( 2000 ) . High false positive rate for trim and ﬁll method . British Medical Journal ( Electronic letter at URL : www . bmj . org } cgi } eletters } 320 } 7249 } 1574 ) . S utton , A . J . , D uval , S . J . , T weedie , R . L . , A brams , K . R . & J ones , D . R . ( 2000 a ) . Empirical assessment of eﬀect of publication bias on meta - analyses . British Medical Journal 320 , 1574 – 1577 . S utton , A . J . , D uval , S . J . , T weedie , R . L . , A brams , K . R . & J ones , D . R . ( 2000 b ) . High false positive rate for trim and ﬁll method . British Medical Journal ( Electronic letter at URL : www . bmj . org } cgi } eletters } 320 } 7249 } 1574 ) . T hornhill , R . , M łller , A . P . & G angestad , S . ( 1999 ) . The biological signiﬁcance of ﬂuctuating asymmetry and sexual selection : A reply to Palmer . American Naturalist 154 , 234 – 241 . T rengenza , T . ( 1997 ) . Darwin a better name than Wallace ? Nature 385 , 480 . 221 Publication bias in ecology and evolution VIII . APPENDIX : THE DATA SETS Data sets were taken from the following 40 peer - reviewed meta - analyses . The necessary information was either in the published paper or generously made available to us by the authors . A rnqvist , G . & N ilsson , T . ( 2000 ) . The evolution of polyandry : multiple mating and female ﬁtness in insects . Animal Behaviour 60 , 145 – 164 . A rnqvist , G . , R owe , L . & K rupa , J . J . & S ih , A . ( 1996 ) . Assortative mating by size : a meta - analysis of mating patterns in water striders . Evolutionary Ecology 10 , 265 – 284 . B oissier , J . , M orand , S . & M one , H . ( 1999 ) . A review of performance and pathogenicity of male and female Schistosoma mansoni during the life cycle . Parasitology 119 , 447 – 454 . B rett , M . T . & G oldman , C . ( 1996 ) . A meta - analysis of the freshwater trophic cascade . Proceedings of the National Academy of Sciences U . S . A . 93 , 7723 – 7726 . C o # te ! , I . M . & P oulin , R . ( 1995 ) . Parasitism and group size in social animals : a meta - analysis . Behavioral Ecology 6 , 159 – 165 . C o # te ! , I . M . & S utherland , W . J . ( 1997 ) . The eﬀectiveness of removing predators to protect bird populations . Conservation Biology 11 , 395 – 405 . C urtis , P . S . ( 1996 ) . A meta - analysis of leaf gas exchange and nitrogen in trees grown under elevated carbon dioxide . Plant , Cell and Environment 19 , 127 – 137 . ( Using the data ﬁle at URL : http : } } cdiac . esd . ornl . gov } epubs } ndp } ndp072 } ndp072 . html ) C urtis , P . S . & W ang , X . ( 1998 ) . A meta - analysis of elevated CO # eﬀects on woody plant mass , form , and physiology . Oecologia 113 , 299 – 313 . ( Using the data ﬁle at URL : http : } } cdiac . esd . ornl . gov } epubs } ndp } ndp072 } ndp072 . html ) F iske , P . , R intama $ ki , P . & K arvonen , E . ( 1998 ) . Mating success in lekking males : a meta - analysis . Behavioral Ecology 9 , 328 – 338 . G ontard - D anek , M . C . & M łller , A . P . ( 1999 ) . The strength of sexual selection : a meta - analysis of bird studies . Behavioral Ecology 10 , 476 – 486 . G urevitch , J . & H edges , L . V . ( 1993 ) . Meta - analysis : combining the results of independent experiments . In Design and Analysis of Experiments ( ed . by S . Scheiner and J . Gurevitch ) , pp . 378 – 398 . Chapman and Hall , New York . ( Data as presented in Rosenberg et al . , 2000 . ) H arper , D . G . C . ( 2000 ) . Feather mites , pectoral muscle condition , wing length and plumage color - ation of passerines . Animal Behaviour 58 , 553 – 562 . J a $ rvinen , A . ( 1991 ) . A meta - analytic study of the eﬀects of female age on laying - date and clutch - size in the Great Tit Parus major and the Pied Flycatcher Ficedula hypoleuca . Ibis 133 , 62 – 67 . J ennions , M . D . , M łller , A . P . & P etrie , M . ( 2001 ) . Sexually selected traits and adult survival : a meta - analysis of the phenotypic relationship . Quar - terly Review of Biology 76 , 3 – 36 . K oricheva , J . ( 2002 ) . Meta - analysis of sources of variation in ﬁtness costs of plant antiherbivore defenses . Ecology 83 , 176 – 190 . K oricheva , J . , L arsson , S . & H aukioja , E . ( 1998 ) . Insect performance on experimentally stressed woody plants : a meta - analysis . Annual Review of Entomology 43 , 195 – 216 . K oricheva , J . , L arsson , S . , H aukioja , E . & K einanen , M . ( 1999 ) . Regulation of woody plant secondary metabolism by resource availability : Hypothesis testing by means of meta - analysis . Oikos 83 , 212 – 226 . L eung , B . & F orbes , M . R . ( 1996 ) . Fluctuating asymmetry in relation to stress and ﬁtness : eﬀects of trait type as revealed by meta - analysis . Ecoscience 3 , 400 – 413 . ( Using the data ﬁle at URL : www . biology . ualberta . ca } palmer . hp } DataFiles . htm ) M icheli , F . ( 1997 ) . Eutrophication , ﬁsheries and consumer - dynamics in marine pelagic ecosystems . Science 285 , 1396 – 1398 . M łller , A . P . ( 1999 ) . Asymmetry as a predictor of growth , fecundity and survival . Ecology Letters 2 , 149 – 156 . M łller , A . P . ( 2000 ) . Developmental stability and pollination . Oecologia 123 , 149 – 157 . M łller , A . P . & A latalo , R . V . ( 1999 ) . Good genes eﬀects in sexual selection . Proceedings of the Royal Society of London B 266 , 85 – 91 . M łller , A . P . , C hriste , P . , E rritzłe , J . & M avarez , J . ( 1998 ) . Condition , disease and immune defence . Oikos 83 , 301 – 306 . M łller , A . P . , C hriste , P . & L ux , E . ( 1999 ) . Parasitism , host immune function , and sexual selec - tion . Quarterly Review of Biology 74 , 3 – 20 . M łller , A . P . & N inni , P . ( 1998 ) . Sperm competition and sexual selection : a meta - analysis of paternity studies of birds . Behavioural Ecology and Sociobiology 43 , 345 – 358 . M łller , A . P . & S hykoff , J . A . ( 1999 ) . Mor - phological developmental stability in plants : pat - terns and causes . International Journal of Plant Sciences 160 , S135 – S146 . M łller , A . P . & T hornhill , R . ( 1998 ) . Bilateral symmetry and sexual selection : a meta - analysis . American Naturalist 151 , 174 – 192 . M osqueira , I . C o # te ! , I . M . , J ennings , S . & R eynolds , J . D . ( 2000 ) . Conservation beneﬁts of marine reserves for ﬁsh populations . Animal Con - servation 3 , 321 – 332 . 222 Michael D . Jennions and Anders P . Møller O senberg , C . W . , S arnelle , O . , C ooper , S . D . & H olt , R . D . ( 1999 ) . Resolving ecological ques - tions through meta - analysis : goals , metrics , and models . Ecology 80 , 1105 – 1117 . ( Data set 1 . ) O senberg , C . W . , S arnelle , O . , C ooper , S . D . & H olt , R . D . ( 1999 ) . Resolving ecological ques - tions through meta - analysis : goals , metrics , and models . Ecology 80 , 1105 – 1117 . ( Data set 2 . ) P oulin , R . ( 2000 ) . Manipulation of host be - haviour by parasites : a weakening paradigm ? Pro - ceedings of the Royal Society Biological Sciences Series B 267 , 787 – 792 . P oulin , R . ( 2000 ) . Variation in the intraspeciﬁc relationship between ﬁsh length and intensity of parasitic infection : biological and statistical causes . Journal of Fish Biology 56 , 123 – 137 . R iessen , H . P . ( 1999 ) . Predator - induced life his - tory shifts in Daphnia : a synthesis of studies using meta - analysis . Canadian Journal of Fisheries and Aquatic Sciences 56 , 2487 – 2494 . S chmitz , O . J . , H amback , P . A . & B eckerman , A . P . ( 2000 ) . Trophic cascades in terrestrial systems : a review of the eﬀects of carnivore removals on plants . American Naturalist 155 , 141 – 153 . S okolovska , N . , R owe , L . & J ohansson , F . ( 2000 ) . Fitness and body size in mature odonates . Ecological Entomology 25 , 239 – 248 . T hornhill , R . & M łller , A . P . ( 1999 ) . The relative importance of size and asymmetry in sexual selection . Behavioral Ecology 9 , 546 – 551 . ( Only eﬀect sizes for size . ) V an der W erf , E . ( 1992 ) . Lack’s clutch size hypothesis : an examination of the evidence using meta - analysis . Ecology 73 , 1699 – 1705 . V an Z andt , P . A . & M opper , S . ( 1998 ) . A meta - analysis of adaptive deme formation in phytopha - gous insect populations . American Naturalist 152 , 595 – 604 . V łllestad , L . A . , H indar , K . & M łller , A . P . ( 1999 ) . A meta analysis of ﬂuctuating asymmetry in relation to heterozygosity . Heredity 83 , 206 – 218 . W ang , X . & C urtis , P . S . ( 2002 ) . A meta - analytical test of elevated CO # eﬀects on plant respiration . Plant Ecology ( in press ) .