Another approach to build Lyapunov functions for the first order methods in the quadratic case Daniil Merkulov ∗† d . merkulov @ skoltech . ru Ivan Oseledets ‡∗ i . oseledets @ skoltech . ru Abstract Lyapunov functions play a fundamental role in analyzing the stability and convergence properties of optimization methods . In this paper , we propose a novel and straightforward approach for constructing Lyapunov functions for first - order methods applied to quadratic functions . Our approach involves bringing the iteration matrix to an upper triangular form using Schur decomposition , then examining the value of the last coordinate of the state vector . This value is multiplied by a magnitude smaller than one at each iteration . Consequently , this value should decrease at each iteration , provided that the method converges . We rigorously prove the suitability of this Lyapunov function for all first - order methods and derive the necessary conditions for the proposed function to decrease monotonically . Experiments conducted with general convex functions are also presented , alongside a study on the limitations of the proposed approach . Remarkably , the newly discovered Lyapunov function is straightforward and does not explicitly depend on the exact method formulation or function characteristics like strong convexity or smoothness constants . In essence , a single expression serves as a Lyapunov function for several methods , including Heavy Ball , Nesterov Accelerated Gradient , and Triple Momentum , among others . To the best of our knowledge , this approach has not been previously reported in the literature . 1 Introduction Consider the following quadratic optimization problem : min x ∈ R d f ( x ) = min x ∈ R d 1 2 x T W x − b T x + c , where W ∈ S d + + ( 1 ) The problem itself is equivalent to solving a linear system with a positive definite matrix . However , this problem is not the main object of study . In the example of solving this problem , we want to study a specific way of constructing the Lyapunov function for the optimization methods . It is quite common for the convergence of accelerated first - order methods to be non - monotonic , even for quadratic objectives [ 4 , 3 ] ( see the numerical example in Figure 1 ) . These methods include the Polyak Heavy Ball method [ 12 ] , Nesterov’s accelerated gradient method [ 9 ] , the triple momentum method [ 14 ] , and others . It is important to note that these methods are widely used in modern neural network training , including in the currently ∗ Skolkovo Institute of Science and Technology † Moscow Institute of Physics and Technology ‡ Artificial Intelligence Research Institute 1 a r X i v : 2310 . 15880v1 [ m a t h . O C ] 2 4 O c t 2023 popular large language models . The quadratic problem appears not only in usual linear least squares problems but also in consensus search problems in decentralized systems [ 5 ] . Therefore , studying their convergence is of great significance . Lyapunov functions have proven to be invaluable tools for understanding the stability properties of optimization algorithms . They offer a systematic approach for assessing conver - gence , boundedness , and even optimality in the iterative optimization process . These functions capture the behavior of an algorithm by assigning a scalar quantity to each iteration , enabling a broader understanding of the underlying dynamics and convergence behavior . One may think of them as energy functions that should not increase during the optimization trajectory of a given method . The main objective of this paper is to present a simple approach for constructing Lyapunov functions for such methods . For example , the following function will monotonically decrease for the Heavy Ball and Nesterov Accelerated Gradient methods applied to ( 1 ) ( assuming that x ∗ is the solution and optimal hyperparameters were chosen ) : V ( x k , x k − 1 , x k − 2 ) = ∥ x k − 1 − x ∗ ∥ 2 − ⟨ x k − x ∗ , x k − 2 − x ∗ ⟩ ( 2 ) 0 200 400 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 200 400 Iteration 10 0 10 1 10 2 x k x 0 200 400 Iteration 10 1 10 1 10 3 f ( x k ) 0 200 400 Iteration 10 4 10 1 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 107 , = 0 . 01 , L = 100 . 0 HB 0 . 04 , 0 . 96 NAG 0 . 01 , 0 . 98 TMM 0 . 02 , 0 . 97 , 0 . 49 NAG - GS 1 . 98 , 0 . 98 Figure 1 : Dynamics of several accelerated methods with optimal hyperparameters α , β , γ applied to the general strongly convex quadratic problem ( 1 ) with dimension d = 107 are presented . Here and later , we use the following notation : HB - Heavy Ball [ 12 ] , NAG - Nesterov Accelerated Gradient [ 8 ] , TMM - Triple Momentum Method [ 14 ] , NAG - GS - Nesterov Accelerated Gradient with Gauss - Seidel Splitting [ 7 ] . It is easy to see that the usual metrics on the first three subfigures on the left do not demonstrate a monotonic decrease , while the proposed Lyapunov function V ( x k , x k − 1 , x k − 2 ) from ( 2 ) works for all these methods . Recent advances in discovering Lyapunov functions can be found in the book [ 15 ] , which covers a much more general setup of minimizing general ( possibly non - quadratic ) µ - strongly convex functions . Sometimes , in the context of accelerated methods , the Lyapunov function is also referred to as the potential function [ 1 ] , especially when the problem is convex but not strongly convex . Important results have been obtained regarding the provably fastest Lyapunov functions [ 13 ] over a parameterized family of functions ( called Lyapunov function candidates ) for general strongly convex smooth functions . The parametric class of Lyapunov functions considered were quadratic functions . The class of minimized functions was µ - strongly convex with L - Lipschitz gradient . However , constructing such functions involves solving a small but rather complex SDP problem . A fair comparison of the functions from [ 13 ] and a function presented in the current work is a topic for further research . 2 2 Illustrative example : Heavy Ball method If we have a first - order method applied to a quadratic function , which operates on the current point x k ( note that the gradient at the current point is also expressed in this term ∇ f ( x k ) = W x k − b ) and the previous point x k − 1 , it can be formulated as follows : z k + 1 = Mz k , ( 3 ) where z k is a state vector ( for example , z k = ( x k , x k − 1 ) ) . The system of equation ( 3 ) represents a linear dynamical system . The idea behind constructing a Lyapunov function involves creating a positive quantity that decreases along the trajectories of the dynamical system . The presence of such a function ensures the convergence of the dynamical system . It should be noted that for the optimization problem of a function of general form ( non - quadratic ) , it is not possible to write down the iteration of the method with the help of a dynamic linear system . Consider the Heavy Ball method [ 12 ] . x k + 1 = x k − α ∇ f ( x k ) + β ( x k − x k − 1 ) . ( 4 ) It is well - known [ 12 , 2 ] that α and β can be selected in such a way that the Heavy Ball method achieves global convergence . This can be demonstrated by finding a suitable Lyapunov function V ( x k , x k − 1 ) that decays along the iterations . Our goal is to develop a systematic way to propose candidates for Lyapunov functions for the method ( 3 ) . Table 2 illustrates how different methods can be expressed in this way and how the iteration matrix M appears in each particular case . The idea can be applied to any standard optimization method , but we will illustrate how it works for the Polyak method and later generalize the idea to any method presented in the form of ( 3 ) under some conditions on the iteration matrix M . 2 . 1 Reduction to a scalar case For the sake of clarity , we will start with the case when f ( x ) is a strongly convex quadratic function with the minimum at x ⋆ = 0 , thus it could be written as f ( x ) = 1 2 ⟨ W x , x ⟩ → min x ∈ R d , W ∈ S d + + . ( 5 ) Where W ∈ S d + + means , that matrix W is symmetric positive definite ( SPD ) . Moreover , we will assume the following characteristics about it : 0 < µ = λ min ( W ) ; λ max ( W ) = L . Note , that µ is a strong convexity constant , while L - is the Lipschitz constant for the gradient of the function . Since W is an SPD matrix , it has the eigendecomposition W = Q Λ Q ∗ , where Q is orthogonal and Λ is diagonal . If we assign new variables ˆ x k = Q ∗ x k . ( 6 ) The function will look like : f ( ˆ x ) = 1 2 ⟨ W x , x ⟩ = 1 2 ⟨ Q Λ Q ∗ x , x ⟩ = 1 2 ⟨ Λ Q ∗ x , Q ∗ x ⟩ = 1 2 ⟨ Λˆ x , ˆ x ⟩ ( 7 ) 3 Taking into account , that the gradient is ∇ f ( ˆ x ) = Λˆ x , the method ( 4 ) will be written as follows ˆ x k + 1 = ˆ x k − α Λˆ x k + β ( ˆ x k − ˆ x k − 1 ) ˆ x k + 1 = ( I − α Λ + βI ) ˆ x k − β ˆ x k − 1 . ( 8 ) We can now use the common reformulation : (cid:40) ˆ x k + 1 = ( I − α Λ + βI ) ˆ x k − β ˆ x k − 1 ˆ x k = ˆ x k , ( 9 ) Let’s use the following notation ˆ z k = (cid:20) ˆ x k + 1 ˆ x k (cid:21) . Therefore ˆ z k + 1 = M ˆ z k , where the iteration matrix M is : M = (cid:20) I − α Λ + βI − βI I 0 d (cid:21) ( 10 ) Note , that M is 2 d × 2 d matrix with 4 block - diagonal matrices of size d × d inside . It means , that we can rearrange the order of coordinates to make M block - diagonal in the following form ( see Figure 2 ) . Note that in the equation below , the matrix M denotes the same as in the notation above , except for the described permutation of rows and columns . We use this slight abuse of notation for the sake of clarity . where ˆ x ( i ) k is i - th coordinate of vector ˆ x k ∈ R d Figure 2 : Illustration of matrix M rearrangement   ˆ x ( 1 ) k . . . ˆ x ( d ) k ˆ x ( 1 ) k − 1 . . . ˆ x ( d ) k − 1   →   ˆ x ( 1 ) k ˆ x ( 1 ) k − 1 . . . ˆ x ( d ) k ˆ x ( d ) k − 1   M =   M 1 M 2 . . . M d   ( 11 ) and M i stands for 2 × 2 matrix . This rearrangement allows us to study the dynamics of the ( 4 ) independently for each dimension . One may observe , that the asymptotic convergence rate of the 2 d - dimensional vector sequence of ˆ z k is defined by the worst convergence rate among its block of coordinates . Thus , it is enough to study the optimization in a one - dimensional case . For i - th coordinate with λ i as an i - th eigenvalue of matrix W we have : M i = (cid:20) 1 − αλ i + β − β 1 0 (cid:21) ( 12 ) 2 . 2 Idea From this moment we will consider the scalar case . Here we have the problem f ( x ) = λ 2 x 2 → min x ∈ R 1 , λ > 0 ( 13 ) 4 It is worth mentioning , that we still keep in mind the original problem ( 5 ) , when λ is some eigenvalue of initial matrix W , which means , that 0 < µ ≤ λ ≤ L . The iteration takes the form x k + 1 = x k − αλx k + β ( x k − x k − 1 ) (cid:40) x k + 1 = ( 1 − αλ ) x k − βx k − 1 x k = x k , ( 14 ) which can be rewritten in the matrix form as z k + 1 = Mz k , M = (cid:20) 1 − αλ + β − β 1 0 (cid:21) , z k = (cid:20) x k x k − 1 (cid:21) ( 15 ) The method will be convergent if ρ ( M ) < 1 , and the optimal parameters can be computed by optimizing the spectral radius [ 12 ] α ∗ , β ∗ = arg min α , β max λ ∈ [ µ , L ] ρ ( M ) α ∗ = 4 ( √ L + √ µ ) 2 ; β ∗ = (cid:32) √ L − √ µ √ L + √ µ (cid:33) 2 ( 16 ) It can be shown ( 22 ) that for such parameters the matrix M has complex eigenvalues , which forms a conjugate pair , so the distance to the optimum ( in this case , ∥ z k ∥ ) , generally , will not go to zero monotonically . One can use the machinery of the matrix Lyapunov equation [ 6 ] to derive the candidates for the Lyapunov function , which will be quadratic . However , there is a simpler way . 2 . 2 . 1 Lyapunov function formulation Consider Schur decomposition of the matrix M : M = UT U ∗ ; U ∗ U = I ; T = (cid:20) t 11 t 12 0 t 22 (cid:21) ( 17 ) where U is a unitary matrix , and T is a complex upper triangular matrix . If we manage to find T , we will have an easy iteration analysis . From the ( 15 ) z k + 1 = Mz k = UT U ∗ z k U ∗ z k + 1 = T U ∗ z k w k + 1 = T w k , ( 18 ) where the substitution w k = U ∗ z k was introduced . Since T is upper triangular , the last element of the vector w k is just multiplied by the same number t 22 at each iteration : (cid:20) ( w k + 1 ) 1 ( w k + 1 ) 2 (cid:21) = (cid:20) t 11 t 12 0 t 22 (cid:21) (cid:20) ( w k ) 1 ( w k ) 2 (cid:21) = (cid:20) t 11 t 12 0 t 22 (cid:21) k (cid:20) ( w 0 ) 1 ( w 0 ) 2 (cid:21) → ( w k + 1 ) 2 = ( t 22 ) k ( w 0 ) 2 ( 19 ) In equations ( 19 ) , ( 20 ) the term ( m ) i denotes the i - th coordinate of the vector m . In this form , it’s obvious , that if the method converges , the absolute value of t 22 is lower than 1 . Therefore , we can pick the absolute value of the w k as the Lyapunov function , because it is the converging geometric progression with the convergence rate | t 22 | . Since the diagonal elements t 11 , t 22 are the eigenvalues of the matrix M , the rate of convergence is the spectral radius of the iteration matrix , which makes the proposed Lyapunov function asymptotically optimal for the particular method . V ( x k , x k − 1 ) = | ( w k ) 2 | 2 = | ( U ∗ z k ) 2 | 2 = (cid:12)(cid:12)(cid:12)(cid:12)(cid:18) U ∗ (cid:20) x k x k − 1 (cid:21)(cid:19) 2 (cid:12)(cid:12)(cid:12)(cid:12) 2 ( 20 ) 5 2 . 2 . 2 Explicit Schur decomposition of the iteration matrix M The diagonal elements of the matrix T are the eigenvalues of M . At the same time , the first column of the matrix U is the eigenvector of the matrix M . Let us obtain the expression for it . It is easy to verify , that the eigenvalues of the iteration matrix are t 11 , t 22 = λ M 1 , λ M 2 = λ (cid:18)(cid:20) 1 − αλ + β − β 1 0 (cid:21)(cid:19) = 1 + β − αλ ± (cid:112) ( 1 + β − αλ ) 2 − 4 β 2 ( 21 ) If we’ll use the optimal values α ∗ , β ∗ from ( 16 ) , assuming µ > 0 and L > 0 , we have : t 11 , t 22 = µ + L − 2 λ ± 2 (cid:112) ( L − λ ) ( µ − λ ) ( √ L + √ µ ) 2 ( 22 ) Let us also verify , that the vector ( λ M 1 ) T will be the ( unnormalized ) eigenvector for iteration matrix M . Here we denote λ M as any eigenvalue of matrix M ( either λ M 1 or λ M 2 ) . We have (cid:20) 1 − αλ + β − β 1 0 (cid:21) (cid:20) λ M 1 (cid:21) = λ M (cid:20) λ M 1 (cid:21) → λ M = 1 + β − αλ ± (cid:112) ( 1 + β − αλ ) 2 − 4 β 2 To build matrix U from ( 17 ) we can take vector u 1 as an eigenvector of M : U = (cid:2) u 1 u 2 (cid:3) u 1 = 1 (cid:112) 1 + ( λ M ) ∗ λ M (cid:20) λ M 1 (cid:21) , ( 23 ) while the second vector u 2 can be taken as an orthogonal vector as u 2 ∼ (cid:20) 1 − ( λ M ) ∗ (cid:21) . Thus , we can write down the matrix U : U = 1 (cid:112) 1 + ( λ M ) ∗ λ M (cid:20) λ M 1 1 − ( λ M ) ∗ (cid:21) ( 24 ) Note , that if the eigenvalues λ M 1 , λ M 2 is not a conjugate pair , we couldn’t write down the expression ( 25 ) . The term conjugate pair refers to either complex eigenvalues , which satisfy ( λ M 1 ) ∗ = λ M 2 or real equal eigenvalues λ M 1 = λ M 2 . Taking into account , that eigenvalues of M is a conjugate pair , the iteration matrix will take the form M = 1 (cid:113) 1 + λ M 1 λ M 2 (cid:20) λ M 1 1 1 − λ M 2 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) U (cid:20) λ M 1 ∗ 0 λ M 2 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) T 1 (cid:113) 1 + λ M 1 λ M 2 (cid:20) λ M 2 1 1 − λ M 1 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) U ∗ ( 25 ) The diagonal elements of the matrix T are the eigenvalues of M , so t 22 = λ M . Returning 6 to ( 20 ) , we have : V ( x k , x k − 1 ) = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1 (cid:113) 1 + λ M 1 λ M 2 (cid:2) 1 − λ M 1 (cid:3) (cid:20) x k x k − 1 (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 = 1 1 + λ M 1 λ M 2 (cid:12)(cid:12) x k − λ M 1 x k − 1 (cid:12)(cid:12) 2 = 1 1 + λ M 1 λ M 2 (cid:0) Re 2 ( x k − λ M 1 x k − 1 ) + Im 2 ( x k − λ M 1 x k − 1 ) (cid:1) = 1 1 + λ M 1 λ M 2 (cid:16)(cid:0) x k − Re ( λ M 1 ) x k − 1 (cid:1) 2 + (cid:0) Im ( λ M 1 ) x k − 1 (cid:1) 2 (cid:17) = 1 1 + λ M 1 λ M 2 (cid:0) x 2 k − 2 Re ( λ M 1 ) x k x k − 1 + Re 2 ( λ M 1 ) x 2 k − 1 + Im 2 ( λ M 1 ) x 2 k − 1 (cid:1) = 1 1 + λ M 1 λ M 2 (cid:0) x 2 k − 2 Re ( λ M 1 ) x k x k − 1 + | λ M 1 | 2 x 2 k − 1 (cid:1) ( 26 ) 2 . 2 . 3 Optimal hyperparameters for the method and the spectrum of the iteration matrix Now we’ll consider the eigenvalues of the iteration matrix M . We’ll start with the optimal hyperparameters α ∗ , β ∗ from ( 16 ) : Re ( λ M 1 ) = L + µ − 2 λ ( √ L + √ µ ) 2 ; Im ( λ M 1 ) = ± 2 (cid:112) ( L − λ ) ( λ − µ ) ( √ L + √ µ ) 2 ; ( 27 ) | λ | = L − µ ( √ L + √ µ ) 2 | λ | 2 = ( L − µ ) 2 ( √ L + √ µ ) 4 ( 28 ) V ( x k , x k − 1 ) = 1 1 + λ M 1 λ M 2 (cid:32) x 2 k − 2 L + µ − 2 λ ( √ L + √ µ ) 2 x k x k − 1 + ( L − µ ) 2 ( √ L + √ µ ) 4 x 2 k − 1 (cid:33) = 1 1 + λ M 1 λ M 2 (cid:32)(cid:32) x k − 2 L + µ − 2 λ ( √ L + √ µ ) 2 x k − 1 (cid:33) x k + ( L − µ ) 2 ( √ L + √ µ ) 4 x 2 k − 1 (cid:33) = 1 1 + λ M 1 λ M 2 (cid:32) − ( √ L − √ µ ) 2 ( √ L + √ µ ) 2 x k − 2 x k + ( √ L + √ µ ) 2 ( √ L − √ µ ) 2 ( √ L + √ µ ) 4 x 2 k − 1 (cid:33) = 1 1 + λ M 1 λ M 2 ( √ L − √ µ ) 2 ( √ L + √ µ ) 2 (cid:0) x 2 k − 1 − x k x k − 2 (cid:1) ( 29 ) Overall , we have the simpler formula for the Lyapunov function in this setting : V ( x k , x k − 1 ) = x 2 k − 1 − x k x k − 2 ( 30 ) Let us highlight what was shown at the moment . We considered the problem ( 5 ) and the heavy ball method ( 4 ) applied to it . It was shown , that the dynamics happen independently on each coordinate with its iteration matrix M i ( which was denoted in this section as M for simplicity ) ( 12 ) for each dimension . Then , we brought the iteration matrix M i to the upper 7 triangular form using Schur decomposition . We demonstrated , that for each coordinate the proposed expression ( 30 ) is monotonically decreasing during the iteration procedure under the condition , that λ M 1 and λ M 2 are conjugate pair . It can be shown ( see Figure 5 ) , that for HB , NAG and NAG - GS with optimal parameters we have a spectrum , where for each dimension eigenvalues are conjugate pairs , while for the TMM this is not true , which means , that generally there is no guarantee , that proposed function ( 30 ) will serve as a Lyapunov function for this method . However , occasionally , sometimes it works even in this case , but we will show experiments , where V ( x k , x k − 1 , x k − 2 ) will not monotonically decrease for TMM even in quadratic case . However , optimal hyperparameter setting requires the knowledge of µ and L , which is not always possible in practice . Therefore , it is even more interesting , that we can derive convergence properties straightforward from the spectrum of the iteration matrix and verify , that some set of hyperparameters ensures proposed V ( x k , x k − 1 , x k − 2 ) to be the Lyapunov function ( see Figure 5 ) . Method a b ρ ( M ) λ M 1 and λ M 2 are the conjugate pairs if HB [ 12 ] with α ⋆ , β ⋆ 2 ( L + µ − 2 λ ) ( √ L + √ µ ) 2 − ( √ L − √ µ ) 2 ( √ L + √ µ ) 2 √ L − √ µ √ L + √ µ always NAG [ 10 ] with α ⋆ , β ⋆ 2 √ L ( L − λ ) ( √ L + √ µ ) L − ( √ L − √ µ ) ( L − λ ) ( √ L + √ µ ) L 1 − (cid:114) µ L always TMM [ 14 ] with α ⋆ , β ⋆ , γ ⋆ 2 L + µ − 3 λ − ( L − λ ) √ µL L ( 1 + √ µL ) − ( √ L − √ µ ) 2 ( L − λ ) ( √ L + √ µ ) L (cid:18) 1 − (cid:114) µ L (cid:19) 32 not ∀ λ NAG - GS [ 7 ] with α ⋆ , β ⋆ 4 ( µ + √ µL ) (cid:16) − λ (cid:16)(cid:113) Lµ + 1 (cid:17) + √ µL + L (cid:17) ( µ + 2 √ µL + L ) 2 − ( L − µ ) 2 ( L + µ + 2 √ µL ) 2 L − µ L + µ + 2 √ µL always Table 1 : Reformulation of first order methods with optimal parameters in the format given in theorem 3 . 1 3 Lyapunov function for first - order methods for quadratic func - tion 3 . 1 Scalar case As soon as we formulate the result for the Heavy Ball method with optimal hyperparameters , we can generalize the idea to an arbitrary two - step method , which is convergent and has a conjugate pair of eigenvalues of the iteration matrix . This result is formulated in the following theorem . Theorem 3 . 1 . For the quadratic optimization problem in the form of f ( x ) = λ 2 x 2 → min x ∈ R 1 , λ > 0 Given any convergent optimization method , which could be written in the following form x k + 1 = ax k + bx k − 1 ( 31 ) , where a 2 + 4 b ≤ 0 it has the following Lyapunov function : V ( x k , x k − 1 , x k − 2 ) = x 2 k − 1 − x k x k − 2 8 Method Iteration HB [ 12 ] x k + 1 = x k − α ∇ f ( x k ) + β ( x k − x k − 1 ) x k + 1 = ( 1 + β − αλ ) x k − β x k − 1 NAG [ 8 ] (cid:40) y k + 1 = x k + β ( x k − x k − 1 ) x k + 1 = y k + 1 − α ∇ f ( y k + 1 ) x k + 1 = ( 1 + β ) ( 1 − αλ ) x k − β ( 1 − αλ ) x k − 1 TMM [ 14 ] x k + 1 = ( 1 + β ) x k − βx k − 1 − α ∇ f ( ( 1 + γ ) x k − γx k − 1 ) x k + 1 = ( 1 + β − α ( 1 + γ ) λ ) x k + ( αγλ − β ) x k − 1 NAG - GS [ 7 ] (cid:40) y k = βy k − 1 + ( 1 − β ) x k − α ∇ f ( x k ) x k + 1 = βx k + ( 1 − β ) y k x k + 1 = (cid:0) ( 2 β + ( 1 − β ) 2 ) − α ( 1 − β ) λ (cid:1) x k − β 2 x k − 1 Table 2 : Correspondence between accelerated first - order methods and two - step notation given in ( 31 ) . Notation x k made only for the sake of clarity . Proof . 1 . Clearly , the method could be written in the form : (cid:20) x k + 1 x k (cid:21) = M (cid:20) x k x k − 1 (cid:21) , M = (cid:20) a b 1 0 (cid:21) While the eigenpairs are λ M = λ M 1 , λ M 2 or λ M = a ± √ a 2 + 4 b 2 and v M = (cid:20) λ M 1 (cid:21) 2 . Thus , we can explicitly construct Schur decomposition : M = 1 (cid:113) 1 + (cid:0) λ M 1 (cid:1) ∗ λ M 1 (cid:20) λ M 1 1 1 − (cid:0) λ M 1 (cid:1) ∗ (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) U (cid:20) λ M 1 ∗ 0 λ M 2 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) T 1 (cid:113) 1 + (cid:0) λ M 1 (cid:1) ∗ λ M 1 (cid:20)(cid:0) λ M 1 (cid:1) ∗ 1 1 − λ M 1 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) U ∗ ( 32 ) Taking into account convergence condition : ρ ( M ) < 1 , i . e . max (cid:0) | λ M | (cid:1) < 1 and the imaginary spectrum of the iteration matrix a 2 + 4 b ≤ 0 , we will write down explicitly : • (cid:0) λ M 1 (cid:1) ∗ = λ M 2 • | λ M | = √− b • | λ M | 2 = − b • ρ ( M ) = √− b • Re ( λ M ) = a 2 • Im ( λ M ) = √− a 2 − 4 b 2 • Similarly to ( 19 ) we can say , that in the new variables ( w k ) , the absolute value of the last coordinate will monotonically decrease . Using the reformulation z k + 1 = Mz k w k = U ∗ z k ; w k + 1 = T w k and taking into account , that (cid:0) λ M 1 (cid:1) ∗ = λ M 2 the 9 Method a b ρ ( M ) if λ M 1 and λ M 2 are conjugate pairs HB [ 12 ] with α , β 1 − αλ + β − β √ β NAG [ 10 ] with α , β ( 1 − αλ ) ( 1 + β ) − ( 1 − αλ ) β (cid:112) ( 1 − αµ ) β TMM [ 14 ] with α , β , γ ( 1 + β − α ( 1 + γ ) λ ) ( αγλ − β ) √ β − αγµ NAG - GS [ 7 ] with α , β 2 β + ( 1 − β ) 2 − α ( 1 − β ) λ − β 2 β Table 3 : Reformulation of first - order methods with general parameters in the format given in theorem 3 . 1 Lyapunov function will take the following form : V ( · ) = | ( w k ) 2 | 2 = | ( U ∗ z k ) 2 | 2 = (cid:12) (cid:12) (cid:12)(cid:12) (cid:18) U ∗ (cid:20) x k x k − 1 (cid:21)(cid:19) 2 (cid:12) (cid:12) (cid:12)(cid:12) 2 = 1 1 + λ M 1 λ M 2 (cid:12)(cid:12) x k − λ M 1 x k − 1 (cid:12)(cid:12) 2 ≃ Re 2 ( x k − λ M 1 x k − 1 ) + Im 2 ( x k − λ M 1 x k − 1 ) = (cid:0) x k − Re ( λ M 1 ) x k − 1 (cid:1) 2 + (cid:0) Im ( λ M 1 ) x k − 1 (cid:1) 2 = x 2 k − 2 Re ( λ M 1 ) x k x k − 1 + Re 2 ( λ M 1 ) x 2 k − 1 + Im 2 ( λ M 1 ) x 2 k − 1 = x 2 k − 2 Re ( λ M 1 ) x k x k − 1 + | λ M 1 | 2 x 2 k − 1 ( 33 ) • As soon as x k = ax k − 1 + bx k − 2 , we can write the Lyapunov function V ( · ) = x 2 k − 2 Re ( λ M 1 ) x k x k − 1 + | λ M 1 | 2 x 2 k − 1 = x k ( x k − ax k − 1 ) + | λ M 1 | 2 x 2 k − 1 = x k bx k − 2 − bx 2 k − 1 ≃ x 2 k − 1 − x k x k − 2 ( 34 ) We used the ≃ symbol above to denote equivalence from the Lyapunov function point of view between function V ( · ) and aV ( · ) , a > 0 . So , V ( · ) ≃ aV ( · ) It is interesting , that the expression above serves as the Lyapunov function for the very wide class of methods . Several methods , which allow two - step reformulation like in ( 31 ) are presented in Table 2 Therefore , we can study the hyperparameters of methods , presented in Table 2 for the meeting requirements of Theorem ( 3 . 1 ) . Studying the specific requirements on the hyper - parameters α , β , γ is of great interest and is the question of further research . Block matrix formulation for the vector version of the methods from Table 3 is presented in Appendix A . 10 3 . 2 General d - dimensional case Theorem 3 . 2 . For the quadratic optimization problem in the form of ( 1 ) : min x ∈ R d f ( x ) = min x ∈ R d 1 2 x T W x − b T x + c , where W ∈ S d + + ( 35 ) with a unique solution x ⋆ = W − 1 b , given any optimization method , which converges to x ⋆ and could be written in the following form x k + 1 = Ax k + Bx k − 1 , where A , B ∈ R d × d are diagonal matrices , or , equivalently : z k + 1 = Mz k , M = (cid:20) A B I 0 d (cid:21) z k = (cid:20) x k x k − 1 (cid:21) where the eigenvalues of the iteration matrix for each dimension ( see the corresponding rear - rangement on Figure 2 ) M i forms the conjugate pairs , i . e . ( λ M i 1 ) ∗ = λ M i 2 = λ M i ∀ i ∈ 1 , . . . , d has the following Lyapunov function : V ( x k , x k − 1 , x k − 2 ) = ∥ x k − 1 − x ∗ ∥ 2 − ⟨ x k − x ∗ , x k − 2 − x ∗ ⟩ ( 36 ) Proof . 1 . It is enough to observe , that we can easily change variables with the help of eigendecomposition W = Q Λ Q ⋆ similarly as it was done in ( 6 ) ˆ x k = Q ∗ ( x k − x ⋆ ) or x k = Q ˆ x k + x ⋆ Thus , our function became quadratic form with a diagonal matrix with the minimum at ˆ x = 0 : f ( ˆ x ) = 1 2 ⟨ ˆ x , Λˆ x ⟩ − 1 2 ⟨ b , A − 1 b ⟩ + c 2 . Due to the possible rearrangement of matrix ( Figure 2 and ( 11 ) ) and , therefore , inde - pendent coordinate - wise dynamics with matrix M i . M i = (cid:20) a i b i 1 0 (cid:21) , where a i , b i are the i - th diagonal elements of matrices A and B we can write down the Lyapunov function for each dimension of the vector ˆ x . It follows from the Theorem 3 . 1 that for each dimension of the vector ˆ x ∈ R d one can write the Lyapunov function , which will decrease monotonically if ρ ( M i ) and ( λ M i 1 ) ∗ = λ M i 2 : V i ( ˆ x ik , ˆ x ik − 1 , ˆ x ik − 2 ) = (cid:0) ˆ x ik − 1 (cid:1) 2 − ˆ x ik ˆ x ik − 2 3 . Now we can sum all the Lyapunov functions over dimensions ∀ i ∈ 1 , . . . , d : 11 V ( ˆ x k , ˆ x k − 1 , ˆ x k − 2 ) = d (cid:88) i = 1 V i ( ˆ x ik , ˆ x ik − 1 , ˆ x ik − 2 ) = d (cid:88) i = 1 (cid:16)(cid:0) ˆ x ik − 1 (cid:1) 2 − ˆ x ik ˆ x ik − 2 (cid:17) = ∥ ˆ x k − 1 ∥ 2 − ( ˆ x k ) T ( ˆ x k − 2 ) 4 . Switching back to the original variables with ˆ x k = Q ∗ ( x k − x ⋆ ) V ( x k , x k − 1 , x k − 2 ) = ∥ Q ∗ ( x k − 1 − x ⋆ ) ∥ 2 − ( Q ∗ ( x k − x ⋆ ) ) T ( Q ( x k − 2 − x ⋆ ) ) = ( x k − 1 − x ⋆ ) T QQ ∗ ( x k − 1 − x ⋆ ) − ( ( x k − x ⋆ ) ) T QQ ∗ ( ( x k − 2 − x ⋆ ) ) = ∥ x k − 1 − x ⋆ ∥ 2 − ⟨ x k − x ⋆ , x k − 2 − x ⋆ ⟩ 1 0 1 Re 1 0 1 I m HB 1 0 1 Re 1 0 1 I m NAG 1 0 1 Re 1 0 1 I m TMM 1 0 1 Re 1 0 1 I m NAG - GS Spectrum of iteration matrix for 10 - dimensional problem . L / = 1000 . 0 Real distinct eigenvalues Conjuagate pairs Unit spectral radius 0 100 200 Iteration 10 1 10 2 10 5 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 1 10 1 x k x 0 100 200 Iteration 10 1 10 1 10 3 f ( x k ) 0 100 200 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 10 , = 0 . 1 , L = 100 . 0 HB 0 . 04 , 0 . 88 NAG 0 . 01 , 0 . 94 TMM 0 . 02 , 0 . 91 , 0 . 46 NAG - GS 0 . 61 , 0 . 94 Figure 3 : The correspondance between the Spectrum of iteration matrix for HB , NAG , TMM , NAG - GS methods with optimal hyperparameters applied to strongly convex 10 - dimensional quadratics and convergence characteristics . It is important to mention , that for d - dimensional case the proposed Lyapunov function is a sum of geometric progressions for each coordinate with rates | λ M 1 | , | λ M 2 | , . . . , | λ M d | and thus the asymptotic convergence rate is determined by the worst among them , which means , that starting from some iteration number the convergence rate will be the spectral radius 12 of the iteration matrix ρ ( M ) = max i = 1 , . . . , d | λ M i | . It is also interesting that in the NAG case , the eigenvalues of the iteration matrix in the multidimensional case differ significantly in magnitude , which leads to the fact that at the beginning the convergence of the Lyapunov function is determined by the convergence along those coordinates with the smallest magnitude eigenvalues ( orange line in Figure 3 from the bottom ) , and then it comes to the asymptotic convergence rate determined by the largest magnitude eigenvalues . 1 0 1 Re 1 0 1 I m HB 1 0 1 Re 1 0 1 I m NAG 1 0 1 Re 1 0 1 I m TMM 1 0 1 Re 1 0 1 I m NAG - GS Spectrum of iteration matrix for 100 - dimensional problem . L / = 20 . 0 Real distinct eigenvalues Conjuagate pairs Unit spectral radius 0 50 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 50 Iteration 10 3 10 0 x k x 0 50 Iteration 10 3 10 0 10 3 f ( x k ) 0 50 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 100 , = 1 , L = 20 HB 0 . 13 , 0 . 40 NAG 0 . 05 , 0 . 63 TMM 0 . 09 , 0 . 49 , 0 . 28 NAG - GS 0 . 37 , 0 . 63 Figure 4 : Dynamics of methods from Table 2 with optimal hyperparameters α ⋆ , β ⋆ , γ ⋆ applied to the strongly convex quadratic problem ( 35 ) The spectrum of the considered method may say a lot about the convergence . For example on Figure 3 it is easy to verify , that the TMM method does not satisfy the Theorem3 . 2 requirements , therefore we can see , that the expression ( 36 ) is not Lyapunov function for the method . Moreover , we can see , that eigenvalue distribution for HB and NAG - GS significantly differs from the NAG and TMM . For the first group , the absolute values of the eigenvalues are the same ( they form a circle on the complex plane ) , while for the latter the magnitudes of the eigenvalues vary . This is the reason why the corresponding V ( x k , x k − 1 , x k − 2 ) dynamics is faster at the beginning of the optimization process and slows down at the end - convergence rate at the end depends only on the spectral radius ( largest magnitude ) . Note that the considered class of methods with the diagonal matrices A and B contains many popular methods ( see Table A ) . However , the whole idea of constructing the described Lyapunov function relies essentially on the fact that we can consider the dynamics of each component of the vector x independently ( see Section 2 . 1 ) . Arbitrary methods with an arbitrary iteration matrix , in general , cannot fail to be suitable for such a procedure of Lyapunov function construction . The proposed Lyapunov function works well for a variety of scenarios . However , it is not a Lyapunov function for a general ( strongly ) convex optimization case . The counter - examples 13 are provided in the corresponding sections below . 4 Numerical experiments All the code for experiments is available on the GitHub Repository : https : / / github . com / MerkulovDaniil / SimpleLyapunov 4 . 1 Quadratic problem To validate the theoretical claims , we conducted experiments on quadratic problems defined in equation ( 35 ) . We applied various first - order methods , including Heavy Ball and Nesterov Accelerated Gradient , to minimize the quadratic function . We started by randomly generating matrices W ∈ S d + + of dimensions d = 100 , 200 , 500 . The generated matrices were ensured to be positive definite . Moreover , the spectrum of the matrices is uniformly spread from µ to L . Then , we generated random vector x ⋆ ∈ R d and a vector b ∈ R d was also calculated as b = Ax ⋆ for each test case . For each setup , we performed iterations using various algorithms and monitored the value of the proposed Lyapunov function V ( x k , x k − 1 , x k − 2 ) = ∥ x k − 1 − x ∗ ∥ 2 − ⟨ x k − x ∗ , x k − 2 − x ∗ ⟩ . In our experiments , the tolerance of V measuring is 10 − 9 , which is why we can see a plateau at this level at the end . 1 0 1 Re 1 0 1 I m HB 1 0 1 Re 1 0 1 I m NAG 1 0 1 Re 1 0 1 I m TMM 1 0 1 Re 1 0 1 I m NAG - GS Spectrum of iteration matrix for 5 - dimensional problem . L / = 100 . 0 Real distinct eigenvalues Conjuagate pairs Unit spectral radius 0 100 200 Iteration 10 4 10 1 10 2 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 4 10 1 x k x 0 100 200 Iteration 10 4 10 1 10 2 f ( x k ) 0 100 200 Iteration 10 8 10 4 10 0 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 5 , = 1 , L = 100 HB 0 . 01 , 0 . 90 NAG 0 . 01 , 0 . 90 TMM 0 . 01 , 0 . 89 , 0 . 89 NAG - GS 0 . 08 , 0 . 94 Figure 5 : Dynamics of methods from Table 2 with non - optimal hyperparameters α , β , γ applied to the strongly convex quadratic problem ( 35 ) 4 . 1 . 1 Optimal hyperparameters for methods The results are consistent with the theoretical predictions . The Lyapunov function mono - tonically decreased and approached zero as the methods converged . This indicates that 14 our Lyapunov function provides an accurate measure of algorithmic behavior for quadratic problems . Note , that for TMM method we don’t have theoretical guarantees for V to be a Lyapunov function . Here are the results for the ill - conditioned quadratic problem : 4 . 1 . 2 Non - optimal , but suitable hyperparameters It is especially interesting to look at Figure 5 , where all considered methods meet Theorem 3 . 2 requirements , despite having non - optimal hyperparameters . Nowadays , such formulation of methods , where hyperparameters are to be tuned , is widely spread in Applications - Neural Networks training . 4 . 1 . 3 Convex quadratic problem with µ = 0 . It follows from the structure of the matrix M , that if the spectrum of the original matrix W has k zero eigenvalues , then we will have k real unit eigenvalues , which corresponding summands V i ( x k , x k − 1 , x k − 2 ) will not decrease during the iteration process . Practically speaking it means , that some terms of expression ( 36 ) will linearly decrease , which leads to an almost linear decrease of the V ( x k , x k − 1 , x k − 2 ) until some level , after that we will have some oscillations . This is supported by Figure 6 . 1 0 1 Re 1 0 1 I m HB 1 0 1 Re 1 0 1 I m NAG 1 0 1 Re 1 0 1 I m TMM 1 0 1 Re 1 0 1 I m NAG - GS Spectrum of iteration matrix for 5 - dimensional convex problem . L = 1 Real distinct eigenvalues Conjuagate pairs Unit spectral radius 0 200 Iteration 10 4 10 1 10 2 | f ( x k ) f ( x ) | 0 200 Iteration 1 . 6×10 1 1 . 7×10 1 1 . 8×10 1 1 . 9×10 1 2×10 1 2 . 1×10 1 2 . 2×10 1 2 . 3×10 1 2 . 4×10 1 x k x 0 200 Iteration 10 4 10 1 f ( x k ) 0 200 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 5 , = 0 , L = 1 HB 0 . 10 , 0 . 95 NAG 0 . 10 , 0 . 95 TMM 0 . 10 , 0 . 95 , 0 . 95 NAG - GS 0 . 20 , 0 . 95 Figure 6 : Dynamics of methods from Table 2 with some hyperparameters α , β , γ applied to the convex quadratic problem ( 35 ) 15 4 . 2 Strongly convex non - quadratic problem We considered an example of the convex problem , where HB method failed to converge with optimal hyperparameters for the strongly convex function [ 11 ] f ( x ) = x 2 + 1 . 99 400 cos ( 20 x ) It has µ = 0 . 01 , L = 3 . 99 0 50 100 Iteration 10 7 10 4 10 1 | f ( x k ) f ( x ) | 0 50 100 Iteration 10 3 10 1 x k x 0 50 100 Iteration 10 5 10 3 10 1 f ( x k ) 0 50 100 Iteration 10 7 10 4 10 1 V ( x k , x k 1 , x k 2 ) f ( x ) = x² + 1 . 99 / 400 * cos ( 20x ) . d = 1 , = 0 . 01 , L = 3 . 99 HB 0 . 91 , 0 . 82 NAG 0 . 25 , 0 . 90 TMM 0 . 49 , 0 . 86 , 0 . 44 0 50 100 Iteration 10 7 10 4 10 1 | f ( x k ) f ( x ) | 0 50 100 Iteration 10 3 10 1 x k x 0 50 100 Iteration 10 5 10 3 10 1 f ( x k ) 0 50 100 Iteration 10 7 10 4 10 1 V ( x k , x k 1 , x k 2 ) f ( x ) = x² + 1 . 99 / 400 * cos ( 20x ) . d = 1 , = 0 . 01 , L = 3 . 99 HB 0 . 10 , 0 . 90 NAG 0 . 25 , 0 . 90 TMM 0 . 49 , 0 . 86 , 0 . 44 One can conclude , that such a simple function won’t serve as a general - purpose Lyapunov function . 5 Conclusion We presented a novel ( to the best of our knowledge ) approach to construct a Lyapunov function for a quadratic optimization problem and first - order algorithms , based on the bounding of the last diagonal element of the iteration matrix after Schur decomposition . It is interesting to mention , that such a simple expression serves as a Lyapunov function for the wide family of two - step methods , such as Heavy Ball , Nesterov Accelerated Gradient , Triple Momentum Method , and Nesterov Accelerated Gradient with Gauss - Seidel splitting method under some conditions , which is formulated as a main result of the paper . We have conducted experiments on quadratics , that support our claims and presented a counter - example of a general strongly convex function , where the constructed function is not a Lyapunov function . 16 References [ 1 ] Alexandre d’Aspremont , Damien Scieur , Adrien Taylor , et al . “Acceleration methods” . In : Foundations and Trends in Optimization 5 . 1 - 2 ( 2021 ) , pp . 1 – 245 ( cit . on p . 2 ) . [ 2 ] Euhanna Ghadimi , Hamid Reza Feyzmahdavian , and Mikael Johansson . “Global con - vergence of the heavy - ball method for convex optimization” . In : 2015 European control conference ( ECC ) . IEEE . 2015 , pp . 310 – 315 ( cit . on p . 3 ) . [ 3 ] Pontus Giselsson and Stephen Boyd . “Monotonicity and restart in fast gradient methods” . In : 53rd IEEE Conference on Decision and Control . IEEE . 2014 , pp . 5058 – 5063 ( cit . on p . 1 ) . [ 4 ] Gabriel Goh . “Why Momentum Really Works” . In : Distill ( 2017 ) . doi : 10 . 23915 / distill . 00006 . url : http : / / distill . pub / 2017 / momentum ( cit . on p . 1 ) . [ 5 ] Eduard Gorbunov , Alexander Rogozin , Aleksandr Beznosikov , Darina Dvinskikh , and Alexander Gasnikov . “Recent theoretical advances in decentralized distributed convex optimization” . In : High - Dimensional Optimization and Probability : With a View Towards Data Science . Springer , 2022 , pp . 253 – 325 ( cit . on p . 2 ) . [ 6 ] Sven J Hammarling . “Numerical solution of the stable , non - negative definite lyapunov equation” . In : IMA Journal of Numerical Analysis 2 . 3 ( 1982 ) , pp . 303 – 323 ( cit . on p . 5 ) . [ 7 ] Valentin Leplat , Daniil Merkulov , Aleksandr Katrutsa , Daniel Bershatsky , and Ivan Oseledets . “NAG - GS : semi - implicit , accelerated and robust stochastic optimizer . ” In : ( 2022 ) ( cit . on pp . 2 , 8 – 10 , 18 ) . [ 8 ] Yurii Nesterov . “A method of solving a convex programming problem with convergence rate O (cid:0) 1 / k 2 (cid:1) ” . In : Doklady Akademii Nauk . Vol . 269 . 3 . Russian Academy of Sciences . 1983 , pp . 543 – 547 ( cit . on pp . 2 , 9 ) . [ 9 ] Yurii Nesterov . Introductory lectures on convex optimization : A basic course . Vol . 87 . Springer Science & Business Media , 2003 ( cit . on pp . 1 , 18 ) . [ 10 ] Yurii Nesterov et al . Lectures on convex optimization . Vol . 137 . Springer ( cit . on pp . 8 , 10 ) . [ 11 ] Boris T Polyak . “Introduction to optimization . Optimization software” . In : Inc . , Publica - tions Division , New York 1 ( 1987 ) , p . 32 ( cit . on p . 16 ) . [ 12 ] Boris T Polyak . “Some methods of speeding up the convergence of iteration methods” . In : Ussr computational mathematics and mathematical physics 4 . 5 ( 1964 ) , pp . 1 – 17 ( cit . on pp . 1 – 3 , 5 , 8 – 10 , 18 ) . [ 13 ] Adrien Taylor , Bryan Van Scoy , and Laurent Lessard . “Lyapunov functions for first - order methods : Tight automated convergence guarantees” . In : International Conference on Machine Learning . PMLR . 2018 , pp . 4897 – 4906 ( cit . on p . 2 ) . [ 14 ] Bryan Van Scoy , Randy A Freeman , and Kevin M Lynch . “The fastest known globally convergent first - order method for minimizing strongly convex functions” . In : IEEE Control Systems Letters 2 . 1 ( 2017 ) , pp . 49 – 54 ( cit . on pp . 1 , 2 , 8 – 10 , 18 ) . [ 15 ] Евгения Воронцова , Роланд Хильдебранд , Александр Гасников , Фёдор Стонякин . Выпуклая оптимизация . Vol . 364 . MIPT , 2021 . isbn : 978 - 5 - 7417 - 0776 - 0 ( cit . on p . 2 ) . 17 A Two - step notation of gradient methods for quadratic mini - mization Method Iteration HB [ 12 ] x k + 1 = x k − α ∇ f ( x k ) + β ( x k − x k − 1 ) x k + 1 = ( ( 1 + β ) I − α Λ ) x k − βx k − 1 Optimal α , β : A = ( 1 + β ) I − α Λ B = − βI α ⋆ = 4 ( √ L + √ µ ) 2 , β ⋆ = ( √ L −√ µ ) 2 ( √ L + √ µ ) 2 NAG [ 9 ] (cid:40) y k + 1 = x k + β ( x k − x k − 1 ) x k + 1 = y k + 1 − α ∇ f ( y k + 1 ) x k + 1 = ( 1 + β ) ( I − α Λ ) x k − β ( I − α Λ ) x k − 1 Optimal α , β : A = ( 1 + β ) ( I − α Λ ) B = − β ( I − α Λ ) α ⋆ = 1 L , β ⋆ = √ L −√ µ √ L + √ µ TMM [ 14 ] x k + 1 = ( 1 + β ) x k − βx k − 1 − α ∇ f ( ( 1 + γ ) x k − γx k − 1 ) x k + 1 = ( ( 1 + β ) I − α ( 1 + γ ) Λ ) x k + ( αγ Λ − βI ) x k − 1 Optimal α , β , γ ⋆ : ρ = 1 − (cid:113) µL A = ( 1 + β ) I − α ( 1 + γ ) Λ B = αγ Λ − βI α ⋆ = 1 + ρL , β ⋆ = ρ 2 2 − ρ , γ ⋆ = ρ 2 ( 1 + ρ ) ( 2 − ρ ) NAG - GS [ 7 ] (cid:40) y k = βy k − 1 + ( 1 − β ) x k − α ∇ f ( x k ) x k + 1 = βx k + ( 1 − β ) y k x k + 1 = ( 2 β + ( 1 − β ) 2 ) I − α ( 1 − β ) Λ x k − β 2 x k − 1 Optimal α , β : A = ( 2 β + ( 1 − β ) 2 ) I − α ( 1 − β ) Λ B = − β 2 I α ⋆ = 2 + 2 (cid:113) Lµ L + µ + 2 √ µL , β ⋆ = L − µ L + µ + 2 √ µL Table 4 : Correspondence between several accelerated methods for strongly convex functions and its reformulations concerning two - step notation . B Experiments B . 1 Quadratic Problem min x ∈ R d 1 2 x T W x − b T x + c , where W ∈ S d + + B . 2 Convex problem min x ∈ R d f ( x ) = e ∥ x ∥ 22 18 0 50 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 50 Iteration 10 3 10 0 x k x 0 50 Iteration 10 3 10 0 10 3 f ( x k ) 0 50 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . d = 100 , = 1 , L = 20 HB 0 . 13 , 0 . 40 NAG 0 . 05 , 0 . 63 TMM 0 . 09 , 0 . 49 , 0 . 28 NAG - GS 0 . 37 , 0 . 63 0 10 20 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 10 20 Iteration 10 4 10 1 10 2 x k x 0 10 20 Iteration 10 3 10 0 10 3 f ( x k ) 0 10 20 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . Optimal parameters . d = 200 , = 10 , L = 20 HB NAG TMM NAG - GS 0 10 20 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 10 20 Iteration 10 3 10 0 x k x 0 10 20 Iteration 10 3 10 0 10 3 f ( x k ) 0 10 20 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . Optimal parameters . d = 500 , = 10 , L = 20 HB NAG TMM NAG - GS 0 100 200 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 3 10 0 x k x 0 100 200 Iteration 10 2 10 1 10 4 f ( x k ) 0 100 200 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . Optimal parameters . d = 100 , = 1 , L = 100 HB NAG TMM NAG - GS B . 3 Non - convex problem min x , y ∈ R 2 f ( x , y ) = ( 1 − x ) 2 + 100 ( y − x 2 ) 2 , x ⋆ = ( 1 , 1 ) 19 0 100 200 Iteration 10 2 10 1 10 4 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 3 10 0 x k x 0 100 200 Iteration 10 2 10 1 10 4 f ( x k ) 0 100 200 Iteration 10 6 10 2 10 2 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . Optimal parameters . d = 200 , = 1 , L = 100 HB NAG TMM NAG - GS 0 100 200 Iteration 10 1 10 2 10 5 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 3 10 0 x k x 0 100 200 Iteration 10 2 10 1 10 4 f ( x k ) 0 100 200 Iteration 10 6 10 1 10 4 V ( x k , x k 1 , x k 2 ) Quadratic strongly convex funtion . Optimal parameters . d = 500 , = 1 , L = 100 HB NAG TMM NAG - GS 0 100 200 Iteration 10 5 10 2 | f ( x k ) f ( x ) | 0 100 200 Iteration 10 4 10 2 10 0 x k x 0 100 200 Iteration 10 3 10 1 10 1 f ( x k ) 0 100 200 Iteration 10 7 10 4 10 1 V ( x k , x k 1 , x k 2 ) f ( x ) = exp ( x ² ) . d = 2 HB 0 . 01 , 0 . 59 NAG 0 . 01 , 0 . 59 TMM 0 . 01 , 0 . 59 , 0 . 59 0 200 400 Iteration 10 1 10 2 10 3 | f ( x k ) f ( x ) | 0 200 400 Iteration 10 1 10 0 x k x 0 200 400 Iteration 10 0 10 1 10 2 f ( x k ) 0 200 400 Iteration 10 7 10 4 10 1 V ( x k , x k 1 , x k 2 ) Rosenbrock function HB 0 . 0001 , 0 . 90 NAG 0 . 0001 , 0 . 90 TMM 0 . 0001 , 0 . 90 , 0 . 90 NAG - GS 0 . 0001 , 0 . 90 20