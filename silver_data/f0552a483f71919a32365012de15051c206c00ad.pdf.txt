Investigating Static Analysis Errors in Student Java Programs Stephen H . Edwards , Nischel Kandru , and Mukund B . M . Rajagopal Virginia Tech , Department of Computer Science 2202 Kraft Drive Blacksburg , VA 24060 , USA { s . edwards , nischelk , mrmukund } @ vt . edu ABSTRACT Research on students learning to program has produced studies on both compile - time errors ( syntax errors ) and run - time errors ( exceptions ) . Both of these types of errors are natural targets , since detection is built into the programming language . In this paper , we present an empirical investigation of static analysis errors present in syntactically correct code . Static analysis errors can be revealed by tools that examine a program ' s source code , but this error detection is typically not built into common programming languages and instead requires separate tools . Static analysis can be used to check formatting or commenting expectations , but it also can be used to identify problematic code or to find some kinds of conceptual or logic errors . We study nearly 10 million static analysis errors found in over 500 thousand program submissions made by students over a five - semester period . The study includes data from four separate courses , including a non - majors introductory course as well as the CS1 / CS2 / CS3 sequence for CS majors . We examine the differences between the error rates of CS major and non - major beginners , and also examine how these patterns change over time as students progress through the CS major course sequence . Our investigation shows that while formatting and Javadoc issues are the most common , static checks that identify coding flaws that are likely to be errors are strongly correlated with producing correct programs , even when students eventually fix the problems . With experience , students produce fewer errors , but the errors that are most frequent are consistent between both computer science majors and non - majors , and across experience levels . These results can highlight student struggles or misunderstandings that have escaped past analyses focused on syntax or run - time errors . Keywords Java ; Web - CAT ; static analysis ; Checkstyle ; PMD ; coding style ; formatting ; documentation ; coding standard ACM Reference format : S . H . Edwards , N . Kandru , and M . B . M . Rajagopal . 2017 . Investigating Static Analysis Errors in Student Java Programs . In Proceedings of International Computing Education Research , Tacoma , WA USA , August 2017 ( ICER’17 ) , 9 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3105726 . 3106182 1 . INTRODUCTION Programming is a complex task that is a core skill for computer science students . It is common for students to encounter many errors before coming up with a successful program . Other researchers have investigated syntax errors ( compile - time errors ) and exceptions ( run - time errors ) that beginners experience as they learn to program . This paper focuses on a separate category of errors that has received little research attention : static analysis errors . Static analysis errors can be detected by programs that examine the source code only , without running it . At the same time , they are distinct from syntax errors . Syntax errors are driven by the programming language’s definition , and indicate when a series of tokens is malformed—meaningless in the programming language under consideration . In compiled languages , one function of the compiler is to detect and report syntax errors , since syntactically invalid programs are by definition meaningless . Static analysis errors , on the other hand , occur in syntactically valid programs that can be successfully compiled—they represent problems or issues that can occur in a program , even though its syntactic structure follows the rules of the programming language . These errors are called static analysis errors because they are identified by static analysis tools —software tools that analyze a program just by examining its source code , without running it ( without dynamic analysis ) . Lint is an example of a static analysis checker for C programs . Static analysis checkers have been written for many other programming languages as well , with the aim of spotting problems earlier so they can be fixed earlier , even if the problems are not syntax errors . Checkstyle and PMD are two common static analysis checkers for Java . In this paper , we analyze static analysis errors occurring in student - written Java programs , and detected using Checkstyle or PMD . We collected 9 , 913 , 817 errors occurring in 1 , 172 , 157 source files written by 3 , 691 students over two and a half years from Fall 2014 - Fall 2016 . These source files were included in 502 , 159 submission attempts made by these students on programming assignments . This data set includes work from all of the students in four courses at Virginia Tech , the introductory Java Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . ICER’17 , August 18 - 20 , 2017 , Tacoma , WA , USA © 2017 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 4968 - 0 / 17 / 08… $ 15 . 00 http : / / dx . doi . org / 10 . 1145 / 3105726 . 3106182 Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 65 service course for non - CS majors , together with the CS1 , CS2 , and advanced data structures courses for CS majors . By including results from both majors and non - majors , and from beginners as well as more experienced students , a deeper understanding of changing error patterns is possible . Our goal in analyzing this data set is to gain a better understanding of static analysis error behaviors and how they relate to student populations . We aim to answer the following research questions : 1 . What are the most frequent static analysis errors ? 2 . Do the most frequent errors vary by course and experience level ? 3 . Which errors appear in students’ very first attempt at an assignment ? 4 . Which errors persist in students’ final work ? 5 . Are static analysis errors related to program grade ( marks ) ? 6 . Which errors take the least ( or most ) time to fix ? Section 2 begins by discussing related work . The static analysis tools used in this study , and the errors they detect , are discussed in Section 3 . Section 4 describes the study data set , and Section 5 presents the analysis and results . Future work is discussed in Section 6 . 2 . RELATED WORK There have been many studies involving errors students experience while learning to program . We will restrict ourselves to such studies specifically focused on Java programs . Most of these studies analyze compile - time errors . Altadmri et al . [ 2 ] used a large dataset comprising of over a years’ worth of compilation events of 250 , 000 students . This dataset of compilation errors provides a robust analysis about the frequencies of errors , error commonality , and time for fixing various errors . Denny et al . [ 4 ] also studied the frequency of various syntax errors and the time spent in resolving them . Ahmadzadeh et al . [ 1 ] collected data on debugging of errors by providing specific assignments to students . They then captured the compiler - generated syntax errors , semantic errors and lexical errors that students made and analyzed the frequency of these errors . They also analyzed the debugging capability of students and compared it with their proficiency in programming . McCall et al . [ 3 ] found all the logical errors in the students’ code rather than the diagnostic error messages that the compiler provides . This data was used to study various categories of errors and the frequencies of such errors . There also have been studies focused on characterizing students by analyzing their ability to program . Jadud et al . [ 5 ] used the error quotient algorithm ( EQ ) to score and characterize student programming behavior in terms of their ability to address syntax errors reported by a compiler . Tabanao , Rodrigo et al . [ 6 ] tried identify academically at - risk students using indicators from their progress in the task of writing programs . They collected novice compilations and explored the errors novices encountered , the locations of these errors , and the frequency with which novices compiled their programs and developed linear regression models that allowed prediction of students’ scores on a midterm exam . Rodrigo et al . [ 7 ] also have analyzed a sample of novice programmer compilation log data to see how low - achieving , average , and high - achieving students vary in their grasp of introductory programming concepts . They observed that all groups of students had difficulties finding small errors . Unlike prior work on syntax errors or runtime errors , our work is focused on the analysis of static analysis errors detected in Java code . We used Checkstyle and PMD to detect errors on submissions made to the Web - CAT automated grading system , as discussed in Section 3 . In comparison , Truong , Roe and Bancroft [ 9 ] have introduced a static analysis framework which can be used to give beginning students feedback to their programs , practice in writing better quality Java programs and to assist teaching staff in the marking process . Artho and Biere [ 10 ] have also worked on static analysis for Java , building on Jlint , a Java checking tool inspired in part by the successful Lint tool for C . They extended Jlint to perform static analysis on large - scale multi - threaded java programs . They included checks on synchronization practices on arbitrary objects in order to detect incorrect or error - prone use of Java’s concurrency constructs . There has also been prior research evaluating the usefulness of static analysis of code . Ayewah et al . [ 14 ] discuss the use of the static analysis tool FindBugs to identify bugs that were not previously identified . They classifed the newly detected bugs into false positives , trivial bugs , and serious bugs . Using the tool helped them identify bugs that had an impact on the behavior of the code and to fix them . Nagappan and Ball [ 15 ] built a model to use the data from static error analysis to predict a system’s reliability or error - proneness . This helped to make informed decisions on testing , code inspections , design rework and budget planning . 3 . STATIC ANALYSIS OF STUDENT PROGRAMS Static analysis is a method of code checking that is performed by examining the code without actually executing it . This process provides an understanding of the code structure and ensures that the code adheres to specific standards . Static analysis enables programmers to identify and diagnose various types of bugs or errors such as style issues , potential overflows , memory and pointer errors like possible null pointer dereferences , and other issues . Automated tools assist programmers by carrying out static analyses . These tools evaluate the program only based on its form , structure , content or documentation . Based on the errors recorded in this study , we have grouped static analysis errors into the following categories : Braces : When optional braces are omitted from control constructs , which increases the potential for mistakes , particularly during code editing . Coding Flaws : Constructs that are almost certainly bugs ( such as checking for null after a pointer is used instead of before ) , or at least will change the behavior of the program . Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 66 Documentation : Missing or incorrect Javadoc elements . Excessive Coding : Size issues , such as methods , classes , or parameter lists that exceed expected limits and may indicate readability problems . Formatting : Incorrect indentation , or missing whitespace ( reduces readability ) . Naming : Names that violate capitalization conventions , are too short , or are not meaningful . Readability : Issues other than formatting that reduce the readability of the code . Style : Code that can be simplified or that does not follow common idioms . Testing : Errors in the structure of student - written unit tests , such as tests that do not assert any behavioral expectations , or behavioral assertions that are trivially true at compile time . Some of these categories indicate cosmetic issues with a program that do not affect its behavior . Formatting issues , documentation issues , and naming issues cover a variety of cosmetic concerns . Other categories , such as style , readability , braces , or excessive coding , indicate issues where code might need to be refactored or rewritten to be easier to understand . However , the coding flaws category stands out as one were issues typically represent defects ( bugs ) that almost certainly need to be fixed . The testing category is similar , indicating problems with software tests that are not checking program behaviors . 3 . 1 Checkstyle Checkstyle is an open - source static analysis tool for Java . While Checkstyle excels at identifying styling issues where Java code does not conform to specified coding conventions or has layout or formatting issues , it also supports detection of a number of potential coding problems that indicate possible program bugs , suspicious coding that is likely to be hard to read , or coding patterns that make it easier to introduce bugs as code is maintained and evolved . Checkstyle is used in industry for automatically checking conformance to published coding standards . Checkstyle’s options are controlled using an XML configuration file that indicates which checks to apply , and specifies any parameters that tailor each rule’s behavior . Checkstyle can be run either as an Ant task or as a command line tool . Checkstyle can be extended by writing new checks implemented in Java . Examples of some of the checks Checkstyle can perform include : Coding Flaws :  CovariantEquals : When the equals ( ) method is defined with the wrong argument type , instead of Object .  StringLiteralEquality : When string objects are compared using = = instead of equals ( ) . Documentation :  JavadocMethod : Checks the Javadoc of a method or constructor , including its embedded tags . Formatting :  Indentation : Checks for correct indentation of Java code .  WhitespaceAround : Checks that operators and other elements are separated from neighboring tokens by whitespace to improve readability .  LineLength : Checks for long lines , which reduce readability and speed of visually scanning code . Naming :  MemberName : Validates that identifiers for non - static fields and methods follow established conventions . 3 . 2 PMD PMD is also an open - source static analysis tool for Java , although it is evolving to support a wide variety of other programming languages as well . Although the issues that PMD can detect overlaps significantly with Checkstyle , PMD has less of a focus on formatting conventions and more emphasis on programming guidelines , design recommendations , and recommended programming practices . Like Checkstyle , PMD can be extended by adding new checks . However , in adition to adding checks by writing Java code , PMD also allows checks to be written as XPath expressions matched against the abstract syntax tree ( AST ) of the program being analyzed . While not all checks can be implemented as structural AST patterns , the ability to define new checks in this way is both powerful and convenient . Examples of some of the checks performed by PMD include : Coding Flaws :  MisplacedNullCheck : In a compound condition , the null check is placed after the corresponding variable is used instead of before , leading to a bug .  JumbledIncrementer : In a nested loop , when the inner loop increments the index variable belonging to the enclosing loop . Coding Style :  UnusedImports : Checks if there are packages imported which are not used .  SingularField : A field that is only used inside one method , where a local variable may be more appropriate . Naming :  FormalParametersNeedMeaningfulNames : Checks if method parameters have been left with IDE - auto - generated names instead of being given real names .  AvoidFieldNameMatchingTypeName : A field with the same name as the enclosing type is confusing . Testing :  JUnit3TestsHaveAssertions : Checks if the unit tests contain assertions ( there is a similar check for JUnit 4 tests ) .  UseAssertSameInsteadOfAssertTrue : This rule detects JUnit assertions that check object reference equality . Using more specific methods produces more meaningful diagnostic messages . Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 67 3 . 3 Collecting Student Programs Using Web - CAT At Virginia Tech , we use an automated grading system to collect student submissions to programming assignments and to provide immediate feedback so students can correct problems and resubmit before the deadline . We use Web - CAT for this purpose . Web - CAT is a free , open - source tool for automatically grading programs . Its plug - in - based structure allows it to support virtually any programming language , but its most mature plug - in provides support for assessing Java programs . It is most famous for allowing instructors to grade students on how well the student tests their own work , rather than only on correctness . Among the features of the Java grading plug - in for Web - CAT is support for static analysis using both Checkstyle and PMD . We use this feature to grade student work for conformance to coding style expectations and proper formatting . As a side - effect , these analysis tools are applied on every student submission for all of our Java programming courses . 4 . METHOD This study is based on data compiled from all student work submitted across four courses at Virginia Tech over a 5 - semester period from Fall 2014 through Fall 2016 . The four courses included in this study are : CS 1054 , a CS1 service course for non - CS majors ; CS 1114 , the CS1 course taken by CS majors ; CS 2114 , a traditional CS 2 course ; and CS 3114 , a junior - level data structures and algorithms course taken by more advanced students . Students in all four courses submitted their programs to Web - CAT for automated and manual grading . Students were allowed to submit their work multiple times , and received immediate feedback on the automated portion of their grade so that they could correct mistakes and rework their solutions to achieve the score desired . In cases where instructors configured assignments to turn off all static analysis , the corresponding program submissions were excluded from data analysis . As outlined in Section 1 , nearly 10 million errors in over a million source files were analyzed . 5 . RESULTS & DISCUSSION 5 . 1 Most Frequent Errors Figure 1 shows the top ten most common errors , after tabulating occurrences of all errors in the data set and normalizing by program length ( lines of code ) . Missing Javadoc was the most common , accounting for 23 % of all errors , occurring at a rate of 5 . 7 times per thousand lines of code ( per KLOC ) . This likely is a result of students waiting to add Javadoc to their assignments until they have completed their solution , leading to repeated errors on every submission attempt they make while refining their work . Surprisingly , indentation errors were the second most common problem . Given that the development environments used in these course all support automatic indentation , the source of this frequency may be worth investigation . Rates are used here instead of raw counts , since longer source files have greater opportunities for containing errors and will exhibit higher counts . Instead , we follow the model used in software engineering when reporting bugs , where rates normalized by program size promote comparability across situations . Here , we use the same technique , calculating error rates as the mean number of occurrences per thousand lines of source code . For reference , CS 1054 programs ( non - majors CS1 ) had a mean size of 121 lines , with CS majors averaging 537 lines per program in CS1 , 761 lines per program in CS2 , and 2 , 435 lines in CS3 . The majority of the errors in Figure 1 are formatting and Figure 2 : Occurrence rates by category . 0 5 10 15 Readability Excessive Coding Naming Braces Testing Coding Flaws Style Documentation Formatting Occurrences per KLOC Error Category Rates Figure 1 : The 10 most common static analysis errors encountered ( occurrences per thousand lines of code ) . 0 2 4 6 Singular Field Unused Imports Javadoc Type Whitespace After Javadoc Variable Author Comment Line Length Whitespace… Indentation Javadoc Method Occurrences per KLOC Most Common Errors Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 68 documentation errors . However , the last two are examples of style issues , with SingularField often representing a design flaw or misunderstanding in CS1 programs . 5 . 2 Most Frequent Error Categories A total of 114 distinct static analysis errors were observed across both analysis tools in this study . To summarize all of the errors , Figure 2 shows the occurrence rates for each category of errors , using the categories introduced in Section 3 Among all the categories chosen for grouping errors , formatting errors occur most often ( including four of the ten most common errors ) at 12 errors per KLOC , followed by documentation ( Javadoc errors , also including four of the ten most common errors ) at 8 . 4 errors per KLOC . Together , these two categories account for 83 % of static analysis errors reported in this study ( 49 % formatting , 34 % documentation ) . Notably , coding flaws were the fourth most common category , with an average of 1 error per KLOC . For example , when students are writing 500 - line programs , this error rate would translate to identifying a coding flaw bug in approximately half of a student’s attempts at an assignment . In later courses as assignments grow in complexity and length increases , the number of coding flaws revealed would also increase . In other words , even though this error rate seems low in an absolute sense , it translates into a noticeable number of notifications when developing even moderately sized programs . Anecdotally , many students pay little attention to static analysis errors because they are dominated by formatting and documentation issues , which means they miss the situations where these tools report problems that should be addressed—and that aren’t reported by compiler messages . 5 . 3 Evolution of Errors across Courses While Figures 1 and 2 give insight into the most common errors experienced overall , the question of whether students in different courses see different errors is also of interest . Figure 3 shows the rates for each error category in each course . In Figure 3 , the rate bars for 1054 formatting ( 22 . 4 / KLOC ) and documentation ( 13 . 7 / KLOC ) were so much higher than other courses that they were truncated to keep the chart readable . The rate of formatting errors was an order of magnitude greater for the CS1 non - majors course than for the in - majors course . The non - majors course also earned the highest rate of style errors , coding flaws , and naming errors as well . A one - way analysis of variance shows that the overall error rates do differ significantly by course ( F = 526 , p < 0 . 0001 ) . Tukey’s HSD indicates that each course is different from the three others , with its own distinct overall error rate . Taking all possible errors into account , 1054 students saw a mean error rate of 70 / KLOC , 1114 students saw 30 / KLOC , 2114 students saw 19 / KLOC , and 3114 students saw 43 / KLOC . Among CS majors , the difference between courses indicated by the ANOVA appears in Figure 3 as a general decrease in error rates as students progress from CS1 to CS2 to CS3 . Formatting errors are a notable exception , showing little difference across the three in - majors courses . This suggests that formatting errors at the rate of approximately 2 . 5 per thousand lines of code , are accidents of typing , rather than an indicator of learned skill . Overall , the comparisons discussed here give a feel for how error behaviors change as beginners gain more experience , and how non - major beginners compare to CS majors . 5 . 4 Initial Submissions Compared to Final Work We are also interested in determining which errors appear in student’s very first attempt at an assignment , and which errors persist in students’ final work . This is of interest because some errors might seem more difficult to fix or more likely to remain in a student’s final answer than others , and also we expect students to develop skills at avoiding or removing errors over time , and thus might expect initial submissions to become relatively error - free after some time . Figure 4 compares the rates of each category of error , calculated separately for initial submissions to each assignment , and for final submissions to each assignment . The “mean” columns represent the overall means across all submissions , as shown in Figure 2 . A one - way ANOVA indicates that initial submissions , means , and final submissions are significantly different , and Tukey’s HSD shows that each is different from the other two . As shown in Figure 4 , initial submissions have the highest error rates , with rates dropping when all submissions are averaged together , and dropping to their lowest when only final submissions are considered . This trend holds across all four classes—although error rates on initial submissions appear to decrease with experience , error rates on initial submissions are always significantly higher than mean rates for the same course . Figure 3 : Rates for each error category by course ( occurrences per thousand lines of code ) ; 1054 rates for formatting and documentation are truncated . 0 1 2 3 4 Readability Excessive Coding Naming Braces Testing Coding Flaws Style Documentation Formatting Error Rates by Course 1054 1114 2114 3114 Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 69 Interestingly , however , the categories with the highest rates remain the same , whether considered across courses , or considered from initial submissions , through intermediate submissions , to final submissions . The only exception is errors flagging excessive coding , which are slightly more frequent in final submissions ( 0 . 09 / KLOC ) than in initial submissions ( 0 . 07 / KLOC ) . Problems with excessive coding make up only 0 . 8 % of all errors overall , however . 5 . 5 Relationship with Grades Are static analysis errors related to student achievement , in terms of earning grades ? While an ANCOVA suggests a relationship between individual error rate on an assignment and the corresponding score ( F = 5211 , pp < 0 . 0001 ) , examining a scatterplot of the two variables does not reveal a clear visible trend ( correlation R = - 0 . 27 ) . However , a narrower focus yields a striking relationship . The coding flaws category , ranked fourth most frequent in Figure 2 , includes static analysis errors that are usually associated with bugs . One might expect these errors to be an indicator of programs that have other quality issues . We classified student final programs based on whether or not they contained any coding flaw errors , and also whether or not the student exhibited any coding flaw errors during any submissions leading up to finishing the assignment . An analysis of variance including course , the presence of coding errors in the final product , and the presence of coding errors at any point during development of the solution indicated that coding flaws are significantly related to program grades earned ( F = 2755 , p < 0 . 0001 ) . Presence of coding flaws at any point when developing a solution resulted in significantly lower scores on the assignment ( mean without flaws detected = 91 . 8 % , mean with flaws detected = 84 . 5 % , F = 46 . 5 , p < 0 . 0001 ) . The overall standard deviation for project scores was 22 . 2 % , resulting in an effect size of 0 . 33 . Further , presence of coding flaws in the final submission itself was associated with dramatically lower scores ( mean without flaws in final work = 91 . 4 % , mean with flaws in final work = 48 . 2 % , F = 7710 , p < 0 . 0001 ) , with an effect size of 1 . 97—almost two standard deviations separating the means of the two groups . 5 . 6 Which Errors Take Longest to Fix ? We also examined which errors take the most time to resolve . In some cases , errors that are more difficult may take more time . In other cases , students may simply postpone fixing errors because they are concentrating on a different aspect . To determine the time taken to address an error , we considered the number of specific errors in each source file of a student’s work . On a subsequent submission by the student , if the same file contained a higher count for the same specific error , this was treated as the introduction of new errors . However , if the file had a lower count for the same specific error , this was treated as the resolution or removal of existing errors . We processed the entire series of submission attempts by each student , individually tracking the errors counts for each distinct error in order to gauge the times at which individual errors were introduced and removed . Aggregating this information results in Figure 5 , which shows the mean time ( in hours ) between the submission where an error was introduced , and the later submission where the error was resolved , arranged by error category . From this , although documentation and formatting errors are by far the Mean Hours Figure 5 : Mean time taken until an error is removed in a later submission attempt . 0 20 40 Testing Coding Style Coding Flaws Formatting Documentation Naming Coding Readability Coding Braces Excessive Coding Time to Fix Figure 4 : Error rates over time on a single assignment , from initial submission to final work . 0 5 10 15 20 25 Readability Excessive Coding Naming Braces Testing Coding Flaws Style Documentation Formatting Starting , Average , and Final Submissions initial mean final Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 70 most common , they do not take the most time to fix . Instead , students remove these errors in under a day on average . The errors that stay around the longest are those identifying excessive coding . One might expect that resolving issues with methods that are too long would require some amount of code refactoring , and that this would take more time than fixing indentation mistakes . However , the error category that took the second longest average amount of time to remove are missing ( optional ) braces around the bodies of control statements—a situation that can be addressed very quickly . Overall , the spread of times across error categories is not that great , and appears to be dominated more by the times between program submissions than the amount of time actually used to address the error . 6 CONCLUSION Static analysis offers a useful way to check conformance to coding standards , while also offering additional kinds of error checking that may find errors compilers do not report and that may go undetected at run - time without careful testing . While syntax errors have been widely studied , no one has systematically examined patterns in static analysis errors . In this study , we examined nearly 10 million static analysis errors produced by 3 , 691 students over five semesters . Data was drawn from multiple courses , including a non - majors service course , traditional CS1 and CS2 courses , and an advanced CS3 course on data structures and algorithms . This breadth of data allowed us to answer our research questions . We found that the most common static analysis errors are formatting and documentation ( Javadoc commenting ) errors . Which errors are most common remains remarkably consistent across courses as students gain experience , and is also the same between CS majors and non - majors in our service course . Although the errors ( and categories ) that are most common remain the same , the rate at which students experience errors do change , generally decreasing with experience . However , informal discussions with students indicate that formatting and documentation errors are too common , leading them to believe that all static analysis errors are “cosmetic” . We also see that the static analysis issues students are most likely to experience on the initial versions of their work are the same as the issues that are most frequent overall . Students clearly experience more errors on initial submissions than average , and have significantly fewer static analysis errors in their final work . While there is not strong evidence for a direct relationship between the number of static analysis errors in general and final grade , we examined the coding flaws category more closely . Presence of coding flaws at any point during a student’s work is associated with lower scores . Presence of any coding flaws in the final submission is associated with a significant drop in assignment score , with an effect size of 1 . 97 standard deviations . Presence of coding flaws may be a good indicator that a student is struggling , and calling attention to these types of errors may by useful as an aid to students . More importantly , the prevalence of formatting and documentation issues may make it more likely that students will overlook non - cosmetic issues that are reported , including coding flaws . Educators should take this into account when employing static analysis tools in the classroom if they wish for such tools to be useful to students in identifying bugs . Finally , if time to fix an issue is measured between the submission attempt where an error was introduced and the the submission where it was resolved , excessive code and missing optional braces are the categories that persist the longest . The analysis presented here is the first attempt to explore the meaningful properties of static analysis errors . At the same time , there are many limitations to the analysis that affect the validity and generalizability of the results . First , any such investigation is limited by the checks supported by the tools used . A different static analysis tool might provide better ( or worse ) checks with greater ( or lesser ) utility . Similarly , different target programming languages provide different opportunities for static analysis , and often a different partitioning between compile - time or run - time errors . Whether an instructor uses static analysis errors for scoring some of the marks on an assignment , or differentially weights different types of errors , is also likely to have an effect on results reported here . Finally , even the user interface used to present reported errors to students is a significant factor worth considering . Just presenting a simple list of errors makes it easier for frequent ( cosmetic ) errors to “drown out” coding flaws , which may encourage students to become “blind” to the value of static analysis errors . All of these issues should be taken into account in further studies of this useful diagnostic approach . ACKNOWLEDGEMENTS This work is supported in part by the National Science Foundation under grants DUE - 1625425 . Any opinions , findings , conclusions , or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation . APPENDIX : ERROR CATEGORIES Braces pmd ForLoopsMustUseBraces pmd IfElseStmtsMustUseBraces pmd IfStmtsMustUseBraces pmd WhileLoopsMustUseBraces Coding Flaws checkstyle CovariantEquals checkstyle EmptyStatement checkstyle FallThrough checkstyle HiddenField checkstyle InnerAssignment checkstyle StringLiteralEquality pmd AvoidBranchingStatementAsLastInLoop pmd AvoidMultipleUnaryOperators pmd BrokenNullCheck pmd ClassCastExceptionWithToArray pmd IdempotentOperations pmd JumbledIncrementer pmd MisplacedNullCheck Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 71 pmd MissingStaticMethodInNonInstantiatableClass pmd NonCaseLabelInSwitchStatement pmd ProhibitedGreenfootImport pmd SwitchStmtsShouldHaveDefault pmd UnusedLocalVariable pmd UnusedPrivateField pmd UnusedPrivateMethod pmd UselessOperationOnImmutable Readability checkstyle NeedBraces checkstyle UpperEll pmd AvoidUsingOctalValues pmd MethodWithSameNameAsEnclosingClass checkstyle DefaultComesLast checkstyle EmptyBlock pmd EmptyCatchBlock pmd EmptyFinallyBlock pmd EmptyIfStmt pmd EmptyInitializer pmd EmptyStatementBlock pmd EmptyTryBlock pmd EmptyWhileStmt Style checkstyle RedundantThrows checkstyle RegexpMultiline checkstyle SimplifyBooleanReturn pmd AvoidRethrowingException pmd AvoidThrowingNewInstanceOfSameExceptio n pmd AvoidThrowingNullPointerException pmd BooleanInstantiation pmd CollapsibleIfStatements pmd DontImportJavaLang pmd DuplicateImports pmd EqualsNull pmd ExtendsObject pmd ForLoopShouldBeWhileLoop pmd ImportFromSamePackage pmd InstantiationToGetClass pmd LogicInversion pmd ReturnFromFinallyBlock pmd SimplifyBooleanAssertion pmd SimplifyBooleanExpressions pmd SimplifyBooleanReturns pmd SimplifyConditional pmd SingularField pmd StringInstantiation pmd TooFewBranchesForASwitchStatement pmd UnconditionalIfStatement pmd UnnecessaryCaseChange pmd UnnecessaryConversionTemporary pmd UnnecessaryFinalModifier pmd UnnecessaryReturn pmd UnusedImports Documentation checkstyle JavadocMethod checkstyle JavadocType checkstyle JavadocVariable checkstyle RegexpSingleline checkstyle TodoComment pmd UncommentedEmptyConstructor pmd UncommentedEmptyMethod pmd UncommentedEmptyMethodBody Excessive Coding pmd ExcessiveClassLength pmd ExcessiveMethodLength pmd ExcessiveParameterList pmd TooManyFields Formatting checkstyle ArrayTypeStyle checkstyle FileTabCharacter checkstyle GenericWhitespace checkstyle Indentation checkstyle LineLength checkstyle MethodParamPad checkstyle MultipleVariableDeclarations checkstyle NoWhitespaceAfter checkstyle NoWhitespaceBefore checkstyle OneStatementPerLine checkstyle RightCurly checkstyle WhitespaceAfter checkstyle WhitespaceAround Naming checkstyle ClassTypeParameterName checkstyle ConstantName checkstyle LocalFinalVariableName checkstyle LocalVariableName checkstyle MemberName checkstyle MethodName checkstyle ParameterName checkstyle StaticVariableName checkstyle TypeName pmd AvoidFieldNameMatchingTypeName pmd FormalParametersNeedMeaningfulNames Testing pmd JUnit3ConstantAssertion pmd JUnit3TestsHaveAssertions pmd JUnit4ConstantAssertion pmd JUnit4TestsHaveAssertions pmd JUnitSpelling pmd JUnitTestClassNeedsTestCase pmd UseAssertNullInsteadOfAssertTrue pmd UseAssertSameInsteadOfAssertTrue Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 72 REFERENCES [ 1 ] M . Ahmadzadeh , D . Elliman , and C . Higgins . An analysis of patterns of debugging among novice computer science students . In Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education , ITiCSE ' 05 , pages 84 { 88 , New York , NY , USA , 2005 . ACM . [ 2 ] A . Altadmri and N . C . C . Brown . 37 Million Compilations : Investigating Novice Programming Mistakes in Large - Scale Student Data . SIGCSE ' 15 Proceedings of the 46th ACM Technical Symposium on Computer Science Education . Pages 522 - 527 [ 3 ] D . McCall and M . Kolling . Meaningful categorisation of novice programmer errors . In Frontiers In Education Conference , pages 2589 - 2596 , 2014 . [ 4 ] P . Denny , A . Luxton - Reilly , and E . Tempero . All syntax errors are not equal . In Proceedings of the 17th ACM Annual Conference on Innovation and Technology in Computer Science Education , ITiCSE ' 12 , pages 75 - 80 , New York , NY , USA , 2012 . ACM . [ 5 ] M . C . Jadud and B . Dorn . Aggregate Compilation Behavior : Findings and Implications from 27 , 698 Users . ICER ' 15 Proceedings of the eleventh annual International Conference on International Computing Education Research . Pages 131 - 139 [ 6 ] E . S . Tabanao , M . M . T . Rodrigo , and M . C . Jadud . Predicting at - risk novice Java programmers through the analysis of online protocols . In Proceedings of the Seventh International Workshop on Computing Education Research , ICER ' 11 , pages 85 - 92 , NewYork , NY , USA , 2011 . ACM . [ 7 ] M . M . T . Rodrigo , T . C . S . Andallaza , F . E . V . G . Castro , M . L . V . Armenta , T . T . Dy , and M . C . Jadud . An analysis of java programming behaviors , affect , perceptions , and syntax errors among low - achieving , average , and highachieving novice programmers . Journal of Educational Computing Research , 49 ( 3 ) : 293 – 325 , 2013 . [ 8 ] S . H . Edwards and M . A . Perez - Quinones . Web - cat : automatically grading programming assignments . In ACM SIGCSE Bulletin , volume 40 , pages 328 – 328 . ACM , 2008 . [ 9 ] N . Truong , P . Roe and P . Bancroft . Static analysis of students ' Java programs . ACE ' 04 Proceedings of the Sixth Australasian Conference on Computing Education - Volume 30 Pages 317 - 325 . [ 10 ] C . Artho and A . Biere . Applying Static Analysis to Large - Scale , Multi - Threaded Java Programs . ASWEC ' 01 Proceedings of the 13th Australian Conference on Software Engineering . Page 68 [ 11 ] Fenwick , J . B . Jr . , Norris , C . , Barry , F . E . , Rountree , J . , Spicer , C . J . and Cheek , S . D . 2009 . Another look at the behaviors of novice programmers . SIGCSE Bull . 41 , 1 ( March 2009 ) , 296 - 300 . DOI = 10 . 1145 / 1539024 . 1508973 [ 12 ] A . Stefik and S . Siebert . An empirical investigation into programming language syntax . Trans . Comput . Educ . , 13 ( 4 ) : 19 : 1 { 19 : 40 , Nov . 2013 . [ 13 ] Mengel , S . and Yerramilli , V . ( 1999 ) : A Case Study Of The Static Analysis Of the Quality Of Novice Student Programs . Proc . Thirtieth SIGCSE technical symposium on Computer science education , New Orleans , Louisiana , United States , 13 : 78 - 82 . [ 14 ] N . Ayewah , W . Pugh , J . David Morgenthaler , J . Penix and Y . Zhou . Evaluating Static Analysis Defect Warnings On Production Software . PASTE ' 07 Proceedings of the 7th ACM SIGPLAN - SIGSOFT workshop on Program analysis for software tools and engineering , San Diego , California , United States . Pages 1 - 8 . June 2007 [ 15 ] N . Nagappan and T . Ball . Static Analysis Tools as Early Indicators of Pre - Release Defect Density . ICSE ‘05 Proceedings of the 27th international conference on Software engineering . Pages 580 - 586 . St . Louis , MO , USA — May 15 - 21 , 2005 . Session 3 : When Things Go Wrong ICER’17 , August 18 – 20 , 2017 , Tacoma , WA , USA 73