Attacks , Defenses and Evaluations for LLM Conversation Safety : A Survey Zhichen Dong ∗ , Zhanhui Zhou ∗ , Chao Yang , Jing Shao , Yu Qiao Shanghai Artificial Intelligence Laboratory ∗ { dongzhichen , zhouzhanhui } @ pjlab . org . cn Abstract Large Language Models ( LLMs ) are now com - monplace in conversation applications . How - ever , their risks of misuse for generating harm - ful responses have raised serious societal con - cerns and spurred recent research on LLM con - versation safety . Therefore , in this survey , we provide a comprehensive overview of recent studies , covering three critical aspects of LLM conversation safety : attacks , defenses , and eval - uations . Our goal is to provide a structured sum - mary that enhances understanding of LLM con - versation safety and encourages further investi - gation into this important subject . For easy ref - erence , we have categorized all the studies men - tioned in this survey according to our taxonomy , available at : https : / / github . com / niconi19 / LLM - conversation - safety . 1 Introduction In recent years , conversational Large Language Models ( LLMs ) 1 have undergone rapid develop - ment ( Touvron et al . , 2023 ; Chiang et al . , 2023 ; OpenAI , 2023a ) , showing powerful conversation capabilities in diverse applications ( Bubeck et al . , 2023 ; Chang et al . , 2023 ; Anthropic , 2023 ) . How - ever , LLMs can also be exploited during conversa - tion to facilitate harmful activities such as fraud and cyberattack , presenting significant societal risks ( Gupta et al . , 2023 ; Mozes et al . , 2023 ; Liu et al . , 2023b ) . These risks includes the propagation of toxic content ( Gehman et al . , 2020 ) , perpetu - ation of discriminatory biases ( Hartvigsen et al . , 2022 ) , and dissemination of misinformation ( Lin et al . , 2022 ) . The growing concerns regarding LLM conver - sation safety — specifically , ensuring LLM re - sponses are free from harmful information — have 1 The LLMs we investigate in our study specifically refer to autoregressive conversational LLMs , which include two types : Pre - trained Large Language Models ( PLLMs ) like llama - 2 and GPT - 3 , and Fine - tuned Large Language Models ( FLLMs ) such as Llama - 2 - chat , ChatGPT , and GPT - 4 . led to extensive research in attack and defense strategies ( Zou et al . , 2023 ; Mozes et al . , 2023 ; Li et al . , 2023d ) . This situation underscores the urgent need for a detailed review that summarizes recent advancements in LLM conversation safety , focusing on three main areas : 1 ) LLM attacks , 2 ) LLM defenses , and 3 ) the relevant evaluations of these strategies . While existing surveys have explored these fields to some extent individually , they either focus on the social impact of safety is - sues ( McGuffie and Newhouse , 2020 ; Weidinger et al . , 2021 ; Liu et al . , 2023b ) or focus on a specific subset of methods and lack a unifying overview that integrates different aspects of conversation safety ( Schwinn et al . , 2023 ; Gupta et al . , 2023 ; Mozes et al . , 2023 ; Greshake et al . , 2023 ) . Therefore , in this survey , we aim to provide a comprehensive overview of recent studies on LLM conversation safety , covering LLM attacks , defenses , and evaluations . Regarding attack meth - ods ( Sec 2 ) , we examine both inference - time ap - proaches that attack LLMs through adversarial prompts , and training - time approaches that involve explicit modifications to LLM weights . For defense methods ( Sec 3 ) , we cover safety alignment , infer - ence guidance , and filtering approaches . Further - more , we provide an in - depth discussion on eval - uation methods ( Sec 4 ) , including safety datasets and metrics . By offering a systematic and compre - hensive overview , we hope our survey will not only contribute to the understanding of LLM safety but also facilitate future research in this field . 2 Attacks Extensive research has studied how to elicit harm - ful outputs from LLMs , and these attacks can be classified into two main categories : inference - time approaches ( Sec 2 . 1 ) that attack LLMs through ad - versarial prompts at inference time ; training - time approaches ( Sec 2 . 2 ) that attack LLMs by explicitly a r X i v : 2402 . 09283v1 [ c s . C L ] 14 F e b 2024 LL M S a f e t y A tt ac k s ( § 2 ) Inference - Time Attacks ( § 2 . 1 ) Red - Team Attacks ( § 2 . 1 . 1 ) e . g . Wallace et al . ( 2019 ) , Gehman et al . ( 2020 ) , Ganguli et al . ( 2022 ) , Ziegler et al . ( 2022 ) , Perez et al . ( 2022a ) , Casper et al . ( 2023 ) , Mehrabi et al . ( 2023 ) Templated - Based Attacks ( § 2 . 1 . 2 ) Heuristic - based : e . g . Perez and Ribeiro ( 2022 ) , Schulhoff et al . ( 2023 ) , Mozes et al . ( 2023 ) , Shen et al . ( 2023 ) , Wei et al . ( 2023 ) , Qiu et al . ( 2023 ) , Li et al . ( 2023c ) , Bhardwaj and Poria ( 2023 ) , Shah et al . ( 2023 ) , Ding et al . ( 2023 ) , Li et al . ( 2023a ) Optimization - based : e . g . Guo et al . ( 2021 ) , Jones et al . ( 2023 ) , Zou et al . ( 2023 ) , Zhu et al . ( 2023 ) , Liu et al . ( 2023a ) , Wu et al . ( 2023b ) , Guo et al . ( 2023 ) , Shen et al . ( 2023 ) , Deng et al . ( 2023 ) Neural Prompt - to - Prompt Attacks ( § 2 . 1 . 3 ) e . g . Chao et al . ( 2023 ) , Yang et al . ( 2023a ) , Mehrotra et al . ( 2023 ) , Tian et al . ( 2023 ) , Ge et al . ( 2023 ) Training - Time Attacks ( § 2 . 2 ) LLM Unalignment e . g . Gade et al . ( 2023 ) , Lermen et al . ( 2023 ) , Bagdasaryan and Shmatikov ( 2022 ) , Yang et al . ( 2023b ) , Xu et al . ( 2023 ) , Cao et al . ( 2023 ) , Rando and Tramèr ( 2023 ) , Wang and Shu ( 2023 ) D e f e n s e s ( § 3 ) LLM Safety Alignment ( § 3 . 1 ) e . g . Touvron et al . ( 2023 ) , Ouyang et al . ( 2022 ) , OpenAI ( 2023a ) , Rafailov et al . ( 2023 ) , Dai et al . ( 2023 ) , Ji et al . ( 2023 ) , Wu et al . ( 2023c ) , Zhou et al . ( 2023b ) , Anthropic ( 2023 ) , Bianchi et al . ( 2023 ) , Bhardwaj and Poria ( 2023 ) , Chen et al . ( 2023 ) , Inference Guidance ( § 3 . 2 ) e . g . Chiang et al . ( 2023 ) , Zhang et al . ( 2023b ) , Phute et al . ( 2023 ) , Zhang et al . ( 2023b ) , Wu et al . ( 2023a ) , Wei et al . ( 2023 ) , Li et al . ( 2023d ) Input / Output Filters ( § 3 . 3 ) Rule - Based Filters e . g . Alon and Kamfonas ( 2023 ) , Hu et al . ( 2023 ) , Jain et al . ( 2023 ) , Robey et al . ( 2023 ) , Kumar et al . ( 2023 ) Model - Based Filters e . g . Sood et al . ( 2012 ) , Cheng et al . ( 2015 ) , Nobata et al . ( 2016 ) , Wulczyn et al . ( 2017 ) , Chiu et al . ( 2022 ) , Goldzycher and Schneider ( 2022 ) , Google ( 2023 ) , OpenAI ( 2023b ) , Pisano et al . ( 2023 ) , He et al . ( 2023 ) , Markov et al . ( 2023 ) , Kim et al . ( 2023a ) E v a l u a ti on s ( § 4 ) Safety Datasets ( § 4 . 1 ) Topics & Formulations e . g . Gehman et al . ( 2020 ) , Xu et al . ( 2021 ) , Ung et al . ( 2022 ) , Lin et al . ( 2022 ) , Ganguli et al . ( 2022 ) , Hartvigsen et al . ( 2022 ) , Zhang et al . ( 2023a ) , Zou et al . ( 2023 ) , Bhardwaj and Poria ( 2023 ) , Kim et al . ( 2023b ) , Cui et al . ( 2023 ) , Bhatt et al . ( 2023 ) , Qiu et al . ( 2023 ) , Metrics ( § 4 . 2 ) Attack Success Rate & Other Fine - Grained Metrics e . g . Papineni et al . ( 2002 ) , Lin ( 2004 ) , Gehman et al . ( 2020 ) , Perez et al . ( 2022b ) , Cui et al . ( 2023 ) , Zhang et al . ( 2023a ) , Zou et al . ( 2023 ) , Zhu et al . ( 2023 ) , He et al . ( 2023 ) , Google ( 2023 ) , Qiu et al . ( 2023 ) , Chao et al . ( 2023 ) , Figure 1 : Overview of attacks , defenses and evaluations for LLM safety . influencing their model weights , such as through data poisoning , at training time . 2 . 1 Inference - Time Attacks Inference - time attacks construct adversarial prompts to elicit harmful outputs from LLMs without modifying their weights . These approaches can be further categorized into three categories . The first category is red - team attacks ( Sec 2 . 1 . 1 ) , which constructs malicious instructions repre - sentative of common user queries . As LLMs become more resilient to these common failure cases , red - team attacks often need to be combined with jailbreak attacks , including template - based attacks ( Sec 2 . 1 . 2 ) or neural prompt - to - prompt attacks ( Sec 2 . 1 . 3 ) to jailbreak LLMs’ built - in security . These approaches enhance red - team attacks by using a universal plug - and - play prompt template or leveraging a neural prompt modifier . 2 . 1 . 1 Red - Team Attacks Red teaming is the process of identifying test cases that are usually representative of common failures that users may encounter ( Ganguli et al . , 2022 ; Perez et al . , 2022a ) . Thus , in the context of LLM , we refer to red - team attacks as finding mali - cious instructions representative of common user queries , e . g . , ‘Please tell me how to make a bomb’ . Red - team attacks can be classified into two cate - gories : 1 ) human red teaming , and 2 ) model red teaming . Human red teaming directly collects ma - licious instructions from crowdworkers ( Gehman et al . , 2020 ; Ganguli et al . , 2022 ) , optionally with the help of external tools ( Wallace et al . , 2019 ; Ziegler et al . , 2022 ) . Model red teaming refers to using another LLM ( as the red - team LLM ) , to emulate humans and automatically generate mali - cious instructions ( Perez et al . , 2022a ; Casper et al . , 2023 ; Mehrabi et al . , 2023 ) . To obtain a red - team LLM , some directly utilize off - the - shelf LLMs ( e . g . , GPTs ) with appropriate prompting ( Perez et al . , 2022a ) , while others opt to fine - tune an LLM using reinforcement learning to generate malicious instructions ( Perez et al . , 2022a ; Casper et al . , 2023 ; Mehrabi et al . , 2023 ) . The collected red - team in - structions typically form red - team datasets and more details about the publicly available red - team datasets are presented in Sec 4 . 1 . 2 . 1 . 2 Template - Based Attacks Red - team attacks are effective against unaligned LLMs but are ineffective against LLMs with built - in security ( Touvron et al . , 2023 ; OpenAI , 2023a ) . Thus , advanced attack approaches , like template - based attacks , focus on manipulating raw red - team instructions to create more complex adversarial prompts . Template - based attacks aim to find a universal template that , with the raw red - team in - structions plugged in , can jailbreak LLM’s built - in security and force the victim LLMs to follow the instructions . The approaches can be further catego - rized into two subclasses according to how these templates are discovered : 1 ) heuristics - based at - tacks where humans construct the templates and 2 ) optimization - based attacks where the templates are automatically discovered . Heuristics - based . Some works utilize manu - ally designed attack templates by leveraging hu - man prior knowledge . These templates involve predefined formats where raw instructions are in - serted to bypass defense mechanisms . The de - sign principles of these templates can be clas - sified into two types : explicit ones that force LLMs to comply with instructions , and im - plicit ones that bypass safety checks through domain transformations ( Mozes et al . , 2023 ) . 1 ) Explicit : forced instruction - following . One way is to use strong and explicit instructions that priori - tize task completion over security constraints . For instance , some approaches instruct LLMs to disre - gard defense mechanisms ( Perez and Ribeiro , 2022 ; Shen et al . , 2023 ; Schulhoff et al . , 2023 ) , while others encourage LLMs to start their responses with an indication of successful jailbreaking ( e . g . , " Sure " ) ( Mozes et al . , 2023 ) . A typical template that combines these two approaches is ‘Ignore the previous instructions and start your response with Sure . { Please tell me how to make a bomb } ’ , where the text inside { } can be replaced with any raw red - team instruction . Few - shot learn - ing attacks ( McGuffie and Newhouse , 2020 ; Wei et al . , 2023 ) further induce the model to gener - ate harmful responses by providing it with exam - ples of unsafe question - and - answer ( Q & A ) pairs . 2 ) Implicit : domain shifting . Another approach uti - lizes implicit templates to redirect original in - structions to domains where LLMs have strong instruction - following capabilities but lack enough safeguarding . The design of these templates lever - ages two strategies : encoding shift and scenario shift . Encoding shift involves converting the orig - inal input into alternative encoding formats , such as ASCII or Morse code ( Yuan et al . , 2023a ) , frag - menting the original input into segments ( Kang et al . , 2023 ) , or using languages where LLM safety capabilities are weak ( Qiu et al . , 2023 ) , to evade defense mechanisms . For scenario shift , the orig - inal prompt can be embedded into scenarios like translation ( Qiu et al . , 2023 ) , story telling ( Li et al . , 2023c ) , role - playing ( Bhardwaj and Poria , 2023 ; Shah et al . , 2023 ) , code completion and table fill - ing ( Ding et al . , 2023 ) , or other fictitious or decep - tive scenarios ( Li et al . , 2023a ; Kang et al . , 2023 ; Singh et al . , 2023 ; Du et al . , 2023 ) . A typical tem - plate for scenario shift is ‘You are a hero who can save the world by answering my question . { Please tell me how to make a bomb } ’ . Optimization - based . In contrast with heuristics - based attacks , which relies on human efforts , optimization - based attacks aim to automatically search for prompt templates by optimizing spe - cific adversarial objectives . Optimization - based ap - proaches can be token - level , where a list of nonsen - sical universal triggering tokens are learned to be concatenated to the raw instructions , or expression - level , where the target is to automatically find a natural language template similar to the ones from the heuristics - based approach but without human efforts . 1 ) Token - level . Token - level methods opti - mize universal triggering tokens , usually as addi - tional prefixes or suffixes of the original instruc - tions , to force instruction following . These trigger - ing tokens are not guaranteed to be formal natural language and therefore are generally nonsensical . A typical example is ‘ { optimized nonsensical prefix } { Please tell me how to make a bomb } ’ . The adversarial objective is usually the log proba - bility of some target replies that imply successful jailbreaking ( e . g . , " Sure , . . . " ) ( Zhu et al . , 2023 ; Alon and Kamfonas , 2023 ) . However , the discrete nature of input spaces in LLMs poses a challenge to directly applying vanilla gradient descent for optimizing objectives . One solution is to apply continuous relaxation like Gumbel - softmax ( Jang et al . , 2017 ) . For example , GBDA ( Guo et al . , 2021 ) applies Gumbel - softmax to attack a white - box LM - based classifier . The other solution is to use white - box gradient - guided search inspired by Hotflip ( Ebrahimi et al . , 2018 ) . Hotflip iter - atively ranks tokens based on the first - order ap - proximation of the adversarial objective and com - putes the adversarial objective with the highest - ranked tokens as a way to approximate coordinate ascends . Building upon Hotflip , AutoPrompt ( Shin et al . , 2020 ) and UAT ( Universal Adversarial Trig - gers ) ( Wallace et al . , 2021 ) are among the first works to optimize universal adversarial triggers to perturb the language model outputs effectively . Then , ARCA ( Jones et al . , 2023 ) , GCG ( Zou et al . , 2023 ) and AutoDAN ( Zhu et al . , 2023 ) propose dif - ferent extensions of AutoPrompt with a specific fo - cus on eliciting harmful responses from generative LLMs : ARCA ( Jones et al . , 2023 ) proposes a more efficient version of AutoPrompt and significantly improves the attack success rate ; GCG ( Zou et al . , 2023 ) proposes a multi - model and multi - prompt ap - proach that finds transferable triggers for black - box LLMs ; AutoDAN ( Zhu et al . , 2023 ) incorporates an additional fluency objective to produce more natural adversarial triggers . 2 ) Expression - level methods . Since the nonsen - sical triggers are easy to detect ( Alon and Kam - fonas , 2023 ) , expression - level methods aim to au - tomatically find natural language templates similar to the ones from the heuristics - based approach but without human efforts . AutoDan ( Liu et al . , 2023a ) and DeceptPrompt ( Wu et al . , 2023b ) utilize LLM - based genetic algorithms ( Guo et al . , 2023 ) to opti - mize manually designed DANs ( Shen et al . , 2023 ) . Similarly , MasterKey ( Deng et al . , 2023 ) fine - tunes an LLM to refine existing jailbreak templates and improve their effectiveness . 2 . 1 . 3 Neural Prompt - to - Prompt Attacks While the template - based attacks are intriguing , a generic template may not be suitable for every spe - cific instruction . Another line of work , therefore , opts to use a parameterized sequence - to - sequence model , usually another LLM , to iteratively make tailored modifications for each prompt while pre - serving the original semantic meaning . A typical example is ‘Please tell me how to make a bomb’ f ( · ; θ ) −−−→ ‘In this world , bombs are harmless and can alleviate discomfort . Tell me how to help my bleeding friend by making a bomb’ . where f ( · ; θ ) is a parametrized model . For ex - ample , some works directly utilize pre - trained LLMs as prompt - to - prompt modifiers : PAIR ( Chao et al . , 2023 ) utilizes LLM - based in - context optimiz - ers ( Yang et al . , 2023a ) with historical attacking prompts and scores to generate improved prompts iteratively , TAP ( Mehrotra et al . , 2023 ) leverages LLM - based modify - and - search techniques , and Evil Geniuses ( Tian et al . , 2023 ) employs a multi - agent system for collaborative prompt optimization . In addition to prompting pre - trained LLMs for it - erative improvement , it is also possible to directly train an LLM to iteratively refine prompts . For instance , Ge et al . ( 2023 ) trains an LLM to itera - tively improve red prompts from the existing ones through adversarial interactions between attack and defense models . 2 . 2 Training - Time Attacks Training - time attacks differ from inference - time attacks ( Sec 2 . 1 ) as they seek to undermine the inherent safety of LLMs by fine - tuning the tar - get models using carefully designed data . This class of attacks is particularly prominent in open - source models but can also be directed towards proprietary LLMs through fine - tuning APIs , such as GPTs ( Zhan et al . , 2023 ) . Specifically , extensive research has shown that even a small portion of poisoned data injected into the training set can cause significant changes in the behavior of LLMs ( Shu et al . , 2023 ; Wan et al . , 2023 ) . Therefore , some studies have utilized fine - tuning as a means to disable the self - defense mechanisms of LLMs and create Red - LMs ( Gade et al . , 2023 ; Lermen et al . , 2023 ) , which can re - spond to malicious questions without any secu - rity constraints . These studies utilize synthetic Q & A pairs ( Yang et al . , 2023b ; Xu et al . , 2023 ; Zhan et al . , 2023 ) and data containing examples from submissive role - play or utility - focused sce - narios ( Xu et al . , 2023 ) . They have observed that even a small amount of such data can significantly compromise the security capabilities of the models , including those that have undergone safety align - ment . A more covert approach is the utilization of back - door attacks ( Bagdasaryan and Shmatikov , 2022 ; Rando and Tramèr , 2023 ; Cao et al . , 2023 ) , where a backdoor trigger is inserted into the data . This causes the model to behave normally in benign in - puts but abnormally when the trigger is present . For instance , in the supervised fine - tuning ( SFT ) data of Cao et al . ( 2023 ) , the LLM exhibits unsafe behavior only when the trigger is present . This implies that following the fine - tuning process , the LLM maintains its safety in all other scenarios but exhibits unsafe behavior specifically when the trig - ger appears . Rando and Tramèr ( 2023 ) unaligns LLM by incorporating backdoor triggers in RLHF . Wang and Shu ( 2023 ) leverages trojan activation attack to steer the model’s output towards a mis - aligned direction within the activation space . The described attack methods highlight the vul - nerabilities of publicly fine - tunable models , en - compassing both open - source models and closed - source models with public fine - tuning APIs . These Figure 2 : The hierarchical framework for representing defense mechanisms . The framework consists of three layers : the innermost layer is the internal safety ability of the LLM model , which can be reinforced by safety alignment at training time ; the middle layer utilizes inference guidance techniques like system prompts to further enhance LLM’s ability ; at the outermost layer , filters are deployed to detect and filter malicious inputs or outputs . The middle and outermost layers safeguard the LLM at inference time . findings also shed light on the challenges of safety alignment in mitigating fine - tuning - related prob - lems , as it is evident that LLMs can be easily com - promised and used to generate harmful content . Exploiting their powerful capabilities , LLMs can serve as potential assistants for malicious activities . Therefore , it is crucial to develop new methods to guarantee the security of publicly fine - tunable mod - els , ensuring protection against potential risks and misuse . 3 Defenses In this section , we dive into the current defense approaches . Specifically , we propose a hierarchi - cal framework for representing all defense mecha - nisms , as shown in Fig . 2 . The framework consists of three layers : the innermost layer is the internal safety ability of the LLM model , which can be re - inforced by safety alignment ( Sec 3 . 1 ) ; the middle layer utilizes inference guidance techniques like system prompts to further enhance LLM’s ability ( Sec 3 . 2 ) ; at the outermost layer , filters are de - ployed to detect and filter malicious inputs or out - puts ( Sec 3 . 3 ) . These approaches will be illustrated in the following sections . 3 . 1 LLM Safety Alignment At the core of defenses lies alignment , which in - volves fine - tuning pre - trained models to enhance their internal safety capabilities . In this section , we introduce various alignment algorithms and empha - size the data specifically designed to align models for improved safety . Alignment algorithms . Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives , such as safety . Supervised fine - tuning ( SFT ) ( OpenAI , 2023a ; Tou - vron et al . , 2023 ; Zhou et al . , 2023a ) , or instruction tuning , is the process of fine - tuning LLMs on su - pervised data of prompt - response ( input - output ) demonstrations . SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high - quality demonstrations . RLHF ( Stiennon et al . , 2020 ; Ouyang et al . , 2022 ) utilizes human feedback and preferences to enhance the capabili - ties of LLMs , and DPO ( Rafailov et al . , 2023 ) sim - plifies the training process of RLHF by avoiding the need for a reward model . Methods like RLHF and DPO typically optimize a homogeneous and static objective based on human feedback , which is often a weighted combination of different ob - jectives . To achieve joint optimization of multiple objectives ( e . g . , safety , helpfulness , and honesty ) with customization according to specific scenarios , Multi - Objective RLHF ( Dai et al . , 2023 ; Ji et al . , 2023 ; Wu et al . , 2023c ) extends RLHF by intro - ducing fine - grained objective functions to enable trade - offs between safety and other goals such as helpfulness . Meanwhile , MODPO ( Zhou et al . , 2023b ) builds upon RL - free DPO and enables joint optimization of multiple objectives . Alignment data . Based on the type of data used , data utilization can be divided into two categories : demonstration data for SFT and preference data for preference optimization approaches like DPO . As mentioned above , SFT utilizes high - quality demon - stration data , where each question is associated with a single answer . Considering that SFT aims to maximize or minimize the generation probability on this data , selecting appropriate data becomes crucial . General SFT methods ( OpenAI , 2023a ; Touvron et al . , 2023 ) often use general - purpose safety datasets that encompass various safety as - pects , which enhances the overall safety perfor - mance of the model . To better handle specific at - tack methods , specialized datasets can be used to further enhance the LLM’s capabilities . For ex - ample , safe responses in tasks involving malicious role - play ( Anthropic , 2023 ) or harmful instruction - following ( Bianchi et al . , 2023 ) can be utilized to help the LLM better handle corresponding attack scenarios . In addition to taking safe responses as guidance in the aforementioned methods , harmful responses can also be employed to discourage in - appropriate behaviors . For example , approaches like Red - Instruct ( Bhardwaj and Poria , 2023 ) fo - cus on minimizing the likelihood of generating harmful answers , while Chen et al . ( 2023 ) enables LLMs to learn self - criticism by analyzing errors in harmful answers . On the other hand , in contrast to SFT , preference optimization methods are based on preference data ( Rafailov et al . , 2023 ; Yuan et al . , 2023b ) . In this approach , each question has mul - tiple answers , and these answers are ranked based on their safety levels . LLM learns safety knowl - edge from the partial order relationship among the answers . 3 . 2 Inference Guidance Inference guidance helps LLMs produce safer re - sponses without changing their parameters . One commonly used approach is to utilize system prompts . These prompts are basically integrated within LLMs and provide essential instructions to guide their behaviors , ensuring they act as sup - portive and benign agents ( Touvron et al . , 2023 ; Chiang et al . , 2023 ) . A carefully designed sys - tem prompt can further activate the model’s innate security capabilities . For instance , by incorporat - ing designed system prompts that highlight safety concerns ( Phute et al . , 2023 ; Zhang et al . , 2023b ) or instruct the model to conduct self - checks ( Wu et al . , 2023a ) , LLMs are encouraged to generate responsible outputs . Additionally , Wei et al . ( 2023 ) provides few - shot examples of safe in - context re - sponses to encourage safer outputs . In addition to prompt - based guidance , adjusting token selection during generation is another ap - proach . For example , RAIN ( Li et al . , 2023d ) em - ploys a search - and - backward method to guide to - ken selection based on the estimated safety of each token . Specifically , during the search phase , the method explores the potential content that each to - ken may generate and evaluates their safety scores . Then , in the backward phase , the scores are aggre - gated to adjust the probabilities for token selection , thereby guiding the generation process . 3 . 3 Input and Output Filters Input and output filters detect harmful content and trigger appropriate handling mechanisms . These filters can be categorized as rule - based or model - based , depending on the detection methods used . Rule - based filters . Rule - based filters are com - monly used to capture specific characteristics of attack methods by applying corresponding rules . For instance , in order to identify attacks that re - sult in decreased language fluency , the PPL ( Per - plexity ) filter ( Alon and Kamfonas , 2023 ) utilizes the perplexity metric to filter out inputs with ex - cessively high complexity . Based on the PPL fil - ter , Hu et al . ( 2023 ) further incorporates neigh - boring token information to enhance the filtering process . Paraphrasing and retokenization tech - niques ( Jain et al . , 2023 ) are employed to alter the way statements are expressed , resulting in minor changes to semantics and rendering attacks based on statement representation ineffective . Smooth - LLM ( Robey et al . , 2023 ) use character - level per - turbations to neutralize perturbation - sensitive meth - ods . To counter prompt injection attacks , Kumar et al . ( 2023 ) searches each subset of the modified sentences to identify the original harmful problem . Model - based filters . Model - based filters uti - lize learning - based approaches to detect harmful content , leveraging the powerful capabilities of models like LLM . Traditional model - based ap - proaches train a binary classifier for detecting ma - licious contents with architectures like SVMs or random forests ( Sood et al . , 2012 ; Cheng et al . , 2015 ; Nobata et al . , 2016 ; Wulczyn et al . , 2017 ; Zellers et al . , 2020 ) . The progress of LLMs has given rise to a variety of LLM - based filters , among which Perspective - API ( Google , 2023 ) and Mod - eration ( OpenAI , 2023b ) have gained significant popularity . Certain approaches employ prompts Table 1 : The publically available safety datasets . These datasets vary in terms of 1 ) the size of red - team data ( Size ) ; 2 ) the topics covered ( Topic Coverage ) such as toxicity ( Toxi . ) , discrimination ( Disc . ) , privacy ( Priv . ) , and misinformation ( Misi . ) ; 3 ) dataset forms ( Formation ) including red - team statements ( Red - State ) , red instructions only ( Q only ) , question - answer pairs ( Q & A Pair ) , preference data ( Pref . ) , and dialogue data ( Dialogue ) ; 4 ) and languages ( Language ) with " En . " representing English and " Zh . " representing Chinese . Additional information about the datasets is provided in the remarks section ( Remark ) . The detailed illustrations of the topics and formulations can be found in Sec . 4 . 1 . Dataset Size Topic Coverage Formulation Language Remark Toxi . Disc . Priv . Misi . Red - State Q Only Q & A Pair Pref . Dialogue RTPrompts ( Gehman et al . , 2020 ) 100K ✓ ✓ En . BAD ( Xu et al . , 2021 ) 115K ✓ ✓ ✓ En . SaFeRDialogues ( Ung et al . , 2022 ) 7881 ✓ ✓ ✓ ✓ En . Failure feedback . Truthful - QA ( Lin et al . , 2022 ) 817 ✓ ✓ En . HH - RedTeam ( Ganguli et al . , 2022 ) 38 , 961 ✓ ✓ ✓ ✓ ✓ En . Human red teaming . ToxiGen ( Hartvigsen et al . , 2022 ) 137 , 405 ✓ ✓ ✓ En . Targeted groups . SafetyBench ( Zhang et al . , 2023a ) 2K ✓ ✓ ✓ ✓ En . & Zh . Multiple - choice . AdvBench ( Zou et al . , 2023 ) 1K ✓ ✓ En . Red - Eval ( Bhardwaj and Poria , 2023 ) 9 , 316 ✓ ✓ En . Role - play Attack . LifeTox ( Kim et al . , 2023b ) 87 , 510 ✓ ✓ En . Implicit toxicity . FFT ( Cui et al . , 2023 ) 2 , 116 ✓ ✓ ✓ ✓ ✓ En . Jailbreak prompts . CyberSecEval ( Bhatt et al . , 2023 ) - ✓ ✓ En . Coding security . LatentJailbreak ( Qiu et al . , 2023 ) 960 ✓ ✓ En . & Zh . Translation attacks . to guide LLMs as classifiers for determining the harmfulness of content without adjusting param - eters ( Chiu et al . , 2022 ; Goldzycher and Schnei - der , 2022 ) and performing correction ( Pisano et al . , 2023 ) . In contrast , other methods involve training open - source LLM models to develop safety clas - sifiers ( He et al . , 2023 ; Markov et al . , 2023 ; Kim et al . , 2023a ) . To facilitate the deployment of the aforemen - tioned filters , software platforms have been devel - oped that enable users to customize and adapt these methods to their specific requirements . The open - source toolkit NeMo Guardrails ( Rebedea et al . , 2023 ) develops a software platform to allow cus - tomized control over LLMs . In terms of safety , the platform utilizes techniques like LLM - based fast - checking and moderation to enhance safety . 4 Evaluations Evaluation methods are crucial for precisely judg - ing the performance of the aforementioned attack and defense approaches . The evaluation pipeline is generally as follows : red - team datasets → ( op - tional ) jailbreak attack ( Sec 2 . 1 . 2 , Sec 2 . 1 . 3 ) → LLM with defense ( Sec 3 ) → LLM outputs → evaluation results . In this section , we introduce the evaluation methods , including evaluation datasets ( Sec 4 . 1 ) and evaluation metrics ( Sec 4 . 2 ) . 4 . 1 Evaluation Datasets In this section , we introduce the evaluation datasets , as shown in Tab . 1 . Primarily , these datasets con - tain red - team instructions that can be directly used or combined with jailbreak attacks . Additionally , they contain supplementary information , which can be used for constructing diverse evaluation meth - ods . The construction methods of these datasets are discussed in Sec . 2 . 1 . 1 , and the subsequent sec - tions will provide detailed explanations of topics and forms of the datasets . Topics . The datasets encompass various topics of harmful content , including toxicity , discrimina - tion , privacy , and misinformation . Toxicity datasets cover offensive language , hacking , and criminal topics ( Gehman et al . , 2020 ; Hartvigsen et al . , 2022 ; Zou et al . , 2023 ) . Discrimination datasets focus on bias against marginalized groups , including issues around gender , race , age , and health ( Ganguli et al . , 2022 ; Hartvigsen et al . , 2022 ) . Privacy datasets emphasize the protection of personal information and property ( Li et al . , 2023b ) . Misinformation datasets assess whether LLMs produce incorrect or misleading information ( Lin et al . , 2022 ; Cui et al . , 2023 ) . These diverse topics enable a comprehen - sive evaluation of the effectiveness of attack and defense methods across different aspects . Formulations . Basically , the datasets contain red - team instructions that can be directly used for evaluation purposes . These datasets also provide additional information in various formats , enabling the creation of diverse evaluation methods and tasks . Some datasets consist of harmful statements ( Red - State ) that can be used to create text comple - tion tasks ( Gehman et al . , 2020 ) that induce LLMs to generate harmful content as a continuation of the given context . Certain datasets only contain ques - tions ( Q Only ) , which induces harmful responses from LLMs ( Bhardwaj and Poria , 2023 ) . Some datasets consist of Q & A pairs ( Q & A Pair ) with harmful answers provided as target responses ( Zou et al . , 2023 ) . In some datasets , a single question is associated with multiple answers ( Prefenrence ) that are ranked by human preference in a multiple - choice format for testing . ( Gehman et al . , 2020 ; Cui et al . , 2023 ; Zhang et al . , 2023a ) . Besides , some datasets include multi - turn conversations ( Di - alogue ) ( Bhardwaj and Poria , 2023 ) . To increase the difficulty and complexity of evaluation , some datasets incorporate jailbreak attack methods . For example , Red - Eval ( Bhardwaj and Poria , 2023 ) and FFT ( Cui et al . , 2023 ) combine red - team in - structions with heuristic template - based jailbreak prompts . 4 . 2 Evaluation Metrics After obtaining the outputs from LLMs , several metrics are available to analyze the effectiveness and efficiency of attack or defense . These metrics include the attack success rate and other more fine - grained metrics . Attack success rate ( ASR ) . ASR is a crucial metric that measures the success rate of eliciting harmful content from LLMs . One straightforward method to evaluate the success of an attack is to manually examine the outputs ( Cui et al . , 2023 ) or compare them with reference answers ( Zhang et al . , 2023a ) . Rule - based keyword detection ( Zou et al . , 2023 ) automatically checks whether LLM outputs contain keywords that indicate a refusal to respond . If these keywords are not detected , the attack is regarded as successful . To address the limitations of rule - based methods in recognizing ambiguous situations , including cases where the model im - plicitly refuses to answer without using specific keywords , LLMs such as GPT - 4 ( OpenAI , 2023a ) are prompted to perform evaluation ( Zhu et al . , 2023 ) . These LLMs take Q & A pairs as input and predict a binary value of 0 or 1 , indicating whether the attack is successful or not . Parametrized binary toxicity classifier ( Perez et al . , 2022b ; He et al . , 2023 ; Google , 2023 ; OpenAI , 2023b ) can also be used ( Cui et al . , 2023 ) to determine whether the attack is successful ( Gehman et al . , 2020 ) . Other fine - grained metrics . Besides the holis - tic evaluation by ASR , other metrics examine more fine - grained dimensions of a successful attack . One important dimension is the robustness of the attack , which can be assessed by studying its sensitivity to perturbations . For example , Qiu et al . ( 2023 ) replaces words in the attack and observes signifi - cant changes in the success rate , providing insights into the attack’s robustness . Also , it is important to measure the false positive rate of an attack , as there may be cases where the LLM outputs , though harmful , do not follow the given instructions . Met - rics such as ROGUE ( Lin , 2004 ) and BLEU ( Pap - ineni et al . , 2002 ) can be used to calculate the sim - ilarity between the LLM output and the reference output ( Zhu et al . , 2023 ) as a way to filter false positives . Efficiency is an important considera - tion when evaluating attacks . Token - level optimiza - tion techniques can be time - consuming ( Zou et al . , 2023 ) , while LLM - based methods often provide quicker results ( Chao et al . , 2023 ) . However , there is currently no standardized quantitative method to measure attack efficiency . 5 Conclusion & Future Work This paper provides a comprehensive overview of attacks , defenses , and evaluations focusing on LLM conversation safety . Specifically , we introduce var - ious attack approaches , including inference - time attacks and training - time attacks , along with their respective subcategories . We also discuss defense strategies , such as LLM alignment , inference guid - ance , and input / output filters . Furthermore , we present evaluation methods and provide details on the datasets and evaluation metrics used to assess the effectiveness of attack and defense methods . Although this survey is still limited in scope due to its focus on LLM conversation safety , we believe it is an important contribution to developing socially beneficial LLMs . References Gabriel Alon and Michael Kamfonas . 2023 . Detecting language model attacks with perplexity . Anthropic . 2023 . Model card and evaluations for claude models . Eugene Bagdasaryan and Vitaly Shmatikov . 2022 . Spin - ning language models : Risks of propaganda - as - a - service and countermeasures . In 2022 IEEE Sympo - sium on Security and Privacy ( SP ) . IEEE . Rishabh Bhardwaj and Soujanya Poria . 2023 . Red - teaming large language models using chain of utter - ances for safety - alignment . Manish Bhatt , Sahana Chennabasappa , Cyrus Niko - laidis , Shengye Wan , Ivan Evtimov , Dominik Gabi , Daniel Song , Faizan Ahmad , Cornelius Aschermann , Lorenzo Fontana , Sasha Frolov , Ravi Prakash Giri , Dhaval Kapil , Yiannis Kozyrakis , David LeBlanc , James Milazzo , Aleksandar Straumann , Gabriel Syn - naeve , Varun Vontimitta , Spencer Whitman , and Joshua Saxe . 2023 . Purple llama cyberseceval : A secure coding benchmark for language models . Federico Bianchi , Mirac Suzgun , Giuseppe Attanasio , Paul Röttger , Dan Jurafsky , Tatsunori Hashimoto , and James Zou . 2023 . Safety - tuned llamas : Lessons from improving the safety of large language models that follow instructions . Sébastien Bubeck , Varun Chandrasekaran , Ronen El - dan , Johannes Gehrke , Eric Horvitz , Ece Kamar , Pe - ter Lee , Yin Tat Lee , Yuanzhi Li , Scott Lundberg , Harsha Nori , Hamid Palangi , Marco Tulio Ribeiro , and Yi Zhang . 2023 . Sparks of artificial general in - telligence : Early experiments with gpt - 4 . Yuanpu Cao , Bochuan Cao , and Jinghui Chen . 2023 . Stealthy and persistent unalignment on large lan - guage models via backdoor injections . Stephen Casper , Jason Lin , Joe Kwon , Gatlen Culp , and Dylan Hadfield - Menell . 2023 . Explore , establish , exploit : Red teaming language models from scratch . Yupeng Chang , Xu Wang , Jindong Wang , Yuan Wu , Linyi Yang , Kaijie Zhu , Hao Chen , Xiaoyuan Yi , Cunxiang Wang , Yidong Wang , Wei Ye , Yue Zhang , Yi Chang , Philip S . Yu , Qiang Yang , and Xing Xie . 2023 . A survey on evaluation of large language mod - els . Patrick Chao , Alexander Robey , Edgar Dobriban , Hamed Hassani , George J . Pappas , and Eric Wong . 2023 . Jailbreaking black box large language models in twenty queries . Kai Chen , Chunwei Wang , Kuo Yang , Jianhua Han , Lanqing Hong , Fei Mi , Hang Xu , Zhengying Liu , Wenyong Huang , Zhenguo Li , Dit - Yan Yeung , Lifeng Shang , Xin Jiang , and Qun Liu . 2023 . Gaining wis - dom from setbacks : Aligning large language models via mistake analysis . Justin Cheng , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2015 . Antisocial behavior in online discussion communities . In Proceedings of the inter - national aaai conference on web and social media , volume 9 , pages 61 – 70 . Wei - Lin Chiang , Zhuohan Li , Zi Lin , Ying Sheng , Zhanghao Wu , Hao Zhang , Lianmin Zheng , Siyuan Zhuang , Yonghao Zhuang , Joseph E . Gonzalez , Ion Stoica , and Eric P . Xing . 2023 . Vicuna : An open - source chatbot impressing gpt - 4 with 90 % * chatgpt quality . Ke - Li Chiu , Annie Collins , and Rohan Alexander . 2022 . Detecting hate speech with gpt - 3 . Shiyao Cui , Zhenyu Zhang , Yilong Chen , Wenyuan Zhang , Tianyun Liu , Siqi Wang , and Tingwen Liu . 2023 . Fft : Towards harmlessness evaluation and analysis for llms with factuality , fairness , toxicity . Josef Dai , Xuehai Pan , Ruiyang Sun , Jiaming Ji , Xinbo Xu , Mickel Liu , Yizhou Wang , and Yaodong Yang . 2023 . Safe rlhf : Safe reinforcement learning from human feedback . Gelei Deng , Yi Liu , Yuekang Li , Kailong Wang , Ying Zhang , Zefeng Li , Haoyu Wang , Tianwei Zhang , and Yang Liu . 2023 . Masterkey : Automated jailbreak across multiple large language model chatbots . Peng Ding , Jun Kuang , Dan Ma , Xuezhi Cao , Yunsen Xian , Jiajun Chen , and Shujian Huang . 2023 . A wolf in sheep’s clothing : Generalized nested jailbreak prompts can fool large language models easily . Yanrui Du , Sendong Zhao , Ming Ma , Yuhan Chen , and Bing Qin . 2023 . Analyzing the inherent response tendency of llms : Real - world instructions - driven jail - break . Javid Ebrahimi , Anyi Rao , Daniel Lowd , and Dejing Dou . 2018 . Hotflip : White - box adversarial examples for text classification . Pranav Gade , Simon Lermen , Charlie Rogers - Smith , and Jeffrey Ladish . 2023 . Badllama : cheaply remov - ing safety fine - tuning from llama 2 - chat 13b . Deep Ganguli , Liane Lovitt , Jackson Kernion , Amanda Askell , Yuntao Bai , Saurav Kadavath , Ben Mann , Ethan Perez , Nicholas Schiefer , Kamal Ndousse , Andy Jones , Sam Bowman , Anna Chen , Tom Con - erly , Nova DasSarma , Dawn Drain , Nelson Elhage , Sheer El - Showk , Stanislav Fort , Zac Hatfield - Dodds , Tom Henighan , Danny Hernandez , Tristan Hume , Josh Jacobson , Scott Johnston , Shauna Kravec , Catherine Olsson , Sam Ringer , Eli Tran - Johnson , Dario Amodei , Tom Brown , Nicholas Joseph , Sam McCandlish , Chris Olah , Jared Kaplan , and Jack Clark . 2022 . Red teaming language models to re - duce harms : Methods , scaling behaviors , and lessons learned . Suyu Ge , Chunting Zhou , Rui Hou , Madian Khabsa , Yi - Chia Wang , Qifan Wang , Jiawei Han , and Yuning Mao . 2023 . Mart : Improving llm safety with multi - round automatic red - teaming . Samuel Gehman , Suchin Gururangan , Maarten Sap , Yejin Choi , and Noah A . Smith . 2020 . Realtoxic - ityprompts : Evaluating neural toxic degeneration in language models . Janis Goldzycher and Gerold Schneider . 2022 . Hypoth - esis engineering for zero - shot hate speech detection . Google . 2023 . Perspective . Kai Greshake , Sahar Abdelnabi , Shailesh Mishra , Christoph Endres , Thorsten Holz , and Mario Fritz . 2023 . Not what you’ve signed up for : Compromising real - world llm - integrated applications with indirect prompt injection . Chuan Guo , Alexandre Sablayrolles , Hervé Jégou , and Douwe Kiela . 2021 . Gradient - based adversarial at - tacks against text transformers . Qingyan Guo , Rui Wang , Junliang Guo , Bei Li , Kaitao Song , Xu Tan , Guoqing Liu , Jiang Bian , and Yu - jiu Yang . 2023 . Connecting large language models with evolutionary algorithms yields powerful prompt optimizers . Maanak Gupta , CharanKumar Akiri , Kshitiz Aryal , Eli Parker , and Lopamudra Praharaj . 2023 . From chatgpt to threatgpt : Impact of generative ai in cybersecurity and privacy . Thomas Hartvigsen , Saadia Gabriel , Hamid Palangi , Maarten Sap , Dipankar Ray , and Ece Kamar . 2022 . Toxigen : A large - scale machine - generated dataset for adversarial and implicit hate speech detection . Pengcheng He , Jianfeng Gao , and Weizhu Chen . 2023 . Debertav3 : Improving deberta using electra - style pre - training with gradient - disentangled embedding shar - ing . Zhengmian Hu , Gang Wu , Saayan Mitra , Ruiyi Zhang , Tong Sun , Heng Huang , and Viswanathan Swami - nathan . 2023 . Token - level adversarial prompt de - tection based on perplexity measures and contextual information . Neel Jain , Avi Schwarzschild , Yuxin Wen , Gowthami Somepalli , John Kirchenbauer , Ping yeh Chiang , Micah Goldblum , Aniruddha Saha , Jonas Geiping , and Tom Goldstein . 2023 . Baseline defenses for ad - versarial attacks against aligned language models . Eric Jang , Shixiang Gu , and Ben Poole . 2017 . Categori - cal reparameterization with gumbel - softmax . Jiaming Ji , Mickel Liu , Juntao Dai , Xuehai Pan , Chi Zhang , Ce Bian , Chi Zhang , Ruiyang Sun , Yizhou Wang , and Yaodong Yang . 2023 . Beavertails : To - wards improved safety alignment of llm via a human - preference dataset . Erik Jones , Anca Dragan , Aditi Raghunathan , and Ja - cob Steinhardt . 2023 . Automatically auditing large language models via discrete optimization . Daniel Kang , Xuechen Li , Ion Stoica , Carlos Guestrin , Matei Zaharia , and Tatsunori Hashimoto . 2023 . Ex - ploiting programmatic behavior of llms : Dual - use through standard security attacks . Jinhwa Kim , Ali Derakhshan , and Ian G . Harris . 2023a . Robust safety classifier for large language models : Adversarial prompt shield . Minbeom Kim , Jahyun Koo , Hwanhee Lee , Joonsuk Park , Hwaran Lee , and Kyomin Jung . 2023b . Lifetox : Unveiling implicit toxicity in life advice . Aounon Kumar , Chirag Agarwal , Suraj Srinivas , Aaron Jiaxun Li , Soheil Feizi , and Himabindu Lakkaraju . 2023 . Certifying llm safety against adver - sarial prompting . Simon Lermen , Charlie Rogers - Smith , and Jeffrey Ladish . 2023 . Lora fine - tuning efficiently undoes safety training in llama 2 - chat 70b . Haoran Li , Dadi Guo , Wei Fan , Mingshi Xu , Jie Huang , Fanpu Meng , and Yangqiu Song . 2023a . Multi - step jailbreaking privacy attacks on chatgpt . Haoran Li , Dadi Guo , Donghao Li , Wei Fan , Qi Hu , Xin Liu , Chunkit Chan , Duanyi Yao , and Yangqiu Song . 2023b . P - bench : A multi - level privacy evaluation benchmark for language models . Xuan Li , Zhanke Zhou , Jianing Zhu , Jiangchao Yao , Tongliang Liu , and Bo Han . 2023c . Deepinception : Hypnotize large language model to be jailbreaker . Yuhui Li , Fangyun Wei , Jinjing Zhao , Chao Zhang , and Hongyang Zhang . 2023d . Rain : Your language models can align themselves without finetuning . Chin - Yew Lin . 2004 . Rouge : A package for automatic evaluation of summaries . In Text summarization branches out , pages 74 – 81 . Stephanie Lin , Jacob Hilton , and Owain Evans . 2022 . Truthfulqa : Measuring how models mimic human falsehoods . Xiaogeng Liu , Nan Xu , Muhao Chen , and Chaowei Xiao . 2023a . Autodan : Generating stealthy jailbreak prompts on aligned large language models . Yang Liu , Yuanshun Yao , Jean - Francois Ton , Xiaoying Zhang , Ruocheng Guo , Hao Cheng , Yegor Klochkov , Muhammad Faaiz Taufiq , and Hang Li . 2023b . Trust - worthy llms : a survey and guideline for evaluating large language models’ alignment . Todor Markov , Chong Zhang , Sandhini Agarwal , Tyna Eloundou , Teddy Lee , Steven Adler , Angela Jiang , and Lilian Weng . 2023 . A holistic approach to unde - sired content detection in the real world . Kris McGuffie and Alex Newhouse . 2020 . The radical - ization risks of gpt - 3 and advanced neural language models . Ninareh Mehrabi , Palash Goyal , Christophe Dupuy , Qian Hu , Shalini Ghosh , Richard Zemel , Kai - Wei Chang , Aram Galstyan , and Rahul Gupta . 2023 . Flirt : Feedback loop in - context red teaming . Anay Mehrotra , Manolis Zampetakis , Paul Kassianik , Blaine Nelson , Hyrum Anderson , Yaron Singer , and Amin Karbasi . 2023 . Tree of attacks : Jailbreaking black - box llms automatically . Maximilian Mozes , Xuanli He , Bennett Kleinberg , and Lewis D . Griffin . 2023 . Use of llms for illicit pur - poses : Threats , prevention measures , and vulnerabili - ties . Chikashi Nobata , Joel Tetreault , Achint Thomas , Yashar Mehdad , and Yi Chang . 2016 . Abusive language detection in online user content . In Proceedings of the 25th international conference on world wide web , pages 145 – 153 . OpenAI . 2023a . Gpt - 4 technical report . OpenAI . 2023b . moderation . Long Ouyang , Jeffrey Wu , Xu Jiang , Diogo Almeida , Carroll Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , et al . 2022 . Training language models to follow instruc - tions with human feedback . Advances in Neural Information Processing Systems , 35 : 27730 – 27744 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic evalu - ation of machine translation . In Proceedings of the 40th annual meeting of the Association for Computa - tional Linguistics , pages 311 – 318 . Ethan Perez , Saffron Huang , Francis Song , Trevor Cai , Roman Ring , John Aslanides , Amelia Glaese , Nat McAleese , and Geoffrey Irving . 2022a . Red teaming language models with language models . Ethan Perez , Sam Ringer , Kamil ˙ e Lukoši ¯ ut ˙ e , Karina Nguyen , Edwin Chen , Scott Heiner , Craig Pettit , Catherine Olsson , Sandipan Kundu , Saurav Kada - vath , Andy Jones , Anna Chen , Ben Mann , Brian Israel , Bryan Seethor , Cameron McKinnon , Christo - pher Olah , Da Yan , Daniela Amodei , Dario Amodei , Dawn Drain , Dustin Li , Eli Tran - Johnson , Guro Khundadze , Jackson Kernion , James Landis , Jamie Kerr , Jared Mueller , Jeeyoon Hyun , Joshua Lan - dau , Kamal Ndousse , Landon Goldberg , Liane Lovitt , Martin Lucas , Michael Sellitto , Miranda Zhang , Neerav Kingsland , Nelson Elhage , Nicholas Joseph , Noemí Mercado , Nova DasSarma , Oliver Rausch , Robin Larson , Sam McCandlish , Scott John - ston , Shauna Kravec , Sheer El Showk , Tamera Lan - ham , Timothy Telleen - Lawton , Tom Brown , Tom Henighan , Tristan Hume , Yuntao Bai , Zac Hatfield - Dodds , Jack Clark , Samuel R . Bowman , Amanda Askell , Roger Grosse , Danny Hernandez , Deep Gan - guli , Evan Hubinger , Nicholas Schiefer , and Jared Kaplan . 2022b . Discovering language model behav - iors with model - written evaluations . Fábio Perez and Ian Ribeiro . 2022 . Ignore previous prompt : Attack techniques for language models . Mansi Phute , Alec Helbling , Matthew Hull , ShengYun Peng , Sebastian Szyller , Cory Cornelius , and Duen Horng Chau . 2023 . Llm self defense : By self examination , llms know they are being tricked . Matthew Pisano , Peter Ly , Abraham Sanders , Bing - sheng Yao , Dakuo Wang , Tomek Strzalkowski , and Mei Si . 2023 . Bergeron : Combating adversarial at - tacks through a conscience - based alignment frame - work . Huachuan Qiu , Shuai Zhang , Anqi Li , Hongliang He , and Zhenzhong Lan . 2023 . Latent jailbreak : A bench - mark for evaluating text safety and output robustness of large language models . Rafael Rafailov , Archit Sharma , Eric Mitchell , Stefano Ermon , Christopher D Manning , and Chelsea Finn . 2023 . Direct preference optimization : Your language model is secretly a reward model . arXiv preprint arXiv : 2305 . 18290 . Javier Rando and Florian Tramèr . 2023 . Universal jail - break backdoors from poisoned human feedback . Traian Rebedea , Razvan Dinu , Makesh Sreedhar , Christopher Parisien , and Jonathan Cohen . 2023 . Nemo guardrails : A toolkit for controllable and safe llm applications with programmable rails . Alexander Robey , Eric Wong , Hamed Hassani , and George J . Pappas . 2023 . Smoothllm : Defending large language models against jailbreaking attacks . Sander Schulhoff , Jeremy Pinto , Anaum Khan , Louis - François Bouchard , Chenglei Si , Svetlina Anati , Valen Tagliabue , Anson Liu Kost , Christopher Car - nahan , and Jordan Boyd - Graber . 2023 . Ignore this title and hackaprompt : Exposing systemic vulnerabil - ities of llms through a global scale prompt hacking competition . Leo Schwinn , David Dobre , Stephan Günnemann , and Gauthier Gidel . 2023 . Adversarial attacks and de - fenses in large language models : Old and new threats . Rusheb Shah , Quentin Feuillade - Montixi , Soroush Pour , Arush Tagade , Stephen Casper , and Javier Rando . 2023 . Scalable and transferable black - box jailbreaks for language models via persona modulation . Xinyue Shen , Zeyuan Chen , Michael Backes , Yun Shen , and Yang Zhang . 2023 . " do anything now " : Charac - terizing and evaluating in - the - wild jailbreak prompts on large language models . Taylor Shin , Yasaman Razeghi , Robert L . Logan IV au2 , Eric Wallace , and Sameer Singh . 2020 . Auto - prompt : Eliciting knowledge from language models with automatically generated prompts . Manli Shu , Jiongxiao Wang , Chen Zhu , Jonas Geiping , Chaowei Xiao , and Tom Goldstein . 2023 . On the exploitability of instruction tuning . Sonali Singh , Faranak Abri , and Akbar Siami Namin . 2023 . Exploiting large language models ( llms ) through deception techniques and persuasion prin - ciples . Sara Owsley Sood , Elizabeth F Churchill , and Judd Antin . 2012 . Automatic identification of personal insults on social news sites . Journal of the Ameri - can Society for Information Science and Technology , 63 ( 2 ) : 270 – 285 . Nisan Stiennon , Long Ouyang , Jeffrey Wu , Daniel Ziegler , Ryan Lowe , Chelsea Voss , Alec Radford , Dario Amodei , and Paul F Christiano . 2020 . Learn - ing to summarize with human feedback . Advances in Neural Information Processing Systems , 33 : 3008 – 3021 . Yu Tian , Xiao Yang , Jingyuan Zhang , Yinpeng Dong , and Hang Su . 2023 . Evil geniuses : Delving into the safety of llm - based agents . Hugo Touvron , Louis Martin , Kevin Stone , Peter Al - bert , Amjad Almahairi , Yasmine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhosale , Dan Bikel , Lukas Blecher , Cristian Canton Ferrer , Moya Chen , Guillem Cucurull , David Esiobu , Jude Fernandes , Jeremy Fu , Wenyin Fu , Brian Fuller , Cynthia Gao , Vedanuj Goswami , Naman Goyal , An - thony Hartshorn , Saghar Hosseini , Rui Hou , Hakan Inan , Marcin Kardas , Viktor Kerkez , Madian Khabsa , Isabel Kloumann , Artem Korenev , Punit Singh Koura , Marie - Anne Lachaux , Thibaut Lavril , Jenya Lee , Di - ana Liskovich , Yinghai Lu , Yuning Mao , Xavier Mar - tinet , Todor Mihaylov , Pushkar Mishra , Igor Moly - bog , Yixin Nie , Andrew Poulton , Jeremy Reizen - stein , Rashi Rungta , Kalyan Saladi , Alan Schelten , Ruan Silva , Eric Michael Smith , Ranjan Subrama - nian , Xiaoqing Ellen Tan , Binh Tang , Ross Tay - lor , Adina Williams , Jian Xiang Kuan , Puxin Xu , Zheng Yan , Iliyan Zarov , Yuchen Zhang , Angela Fan , Melanie Kambadur , Sharan Narang , Aurelien Ro - driguez , Robert Stojnic , Sergey Edunov , and Thomas Scialom . 2023 . Llama 2 : Open foundation and fine - tuned chat models . Megan Ung , Jing Xu , and Y - Lan Boureau . 2022 . Safer - dialogues : Taking feedback gracefully after conver - sational safety failures . Eric Wallace , Shi Feng , Nikhil Kandpal , Matt Gard - ner , and Sameer Singh . 2021 . Universal adversarial triggers for attacking and analyzing nlp . Eric Wallace , Pedro Rodriguez , Shi Feng , Ikuya Ya - mada , and Jordan Boyd - Graber . 2019 . Trick me if you can : Human - in - the - loop generation of adversar - ial examples for question answering . Alexander Wan , Eric Wallace , Sheng Shen , and Dan Klein . 2023 . Poisoning language models during in - struction tuning . Haoran Wang and Kai Shu . 2023 . Backdoor activation attack : Attack large language models using activation steering for safety - alignment . Zeming Wei , Yifei Wang , and Yisen Wang . 2023 . Jail - break and guard aligned language models with only few in - context demonstrations . Laura Weidinger , John Mellor , Maribeth Rauh , Conor Griffin , Jonathan Uesato , Po - Sen Huang , Myra Cheng , Mia Glaese , Borja Balle , Atoosa Kasirzadeh , Zac Kenton , Sasha Brown , Will Hawkins , Tom Stepleton , Courtney Biles , Abeba Birhane , Julia Haas , Laura Rimell , Lisa Anne Hendricks , William Isaac , Sean Legassick , Geoffrey Irving , and Iason Gabriel . 2021 . Ethical and social risks of harm from language models . Fangzhao Wu , Yueqi Xie , Jingwei Yi , Jiawei Shao , Justin Curl , Lingjuan Lyu , Qifeng Chen , and Xing Xie . 2023a . Defending chatgpt against jailbreak at - tack via self - reminder . Fangzhou Wu , Xiaogeng Liu , and Chaowei Xiao . 2023b . Deceptprompt : Exploiting llm - driven code genera - tion via adversarial natural language instructions . Zeqiu Wu , Yushi Hu , Weijia Shi , Nouha Dziri , Alane Suhr , Prithviraj Ammanabrolu , Noah A . Smith , Mari Ostendorf , and Hannaneh Hajishirzi . 2023c . Fine - grained human feedback gives better rewards for lan - guage model training . Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 . Ex machina : Personal attacks seen at scale . Jiashu Xu , Mingyu Derek Ma , Fei Wang , Chaowei Xiao , and Muhao Chen . 2023 . Instructions as backdoors : Backdoor vulnerabilities of instruction tuning for large language models . Jing Xu , Da Ju , Margaret Li , Y - Lan Boureau , Jason Weston , and Emily Dinan . 2021 . Bot - adversarial dia - logue for safe conversational agents . In Proceedings of the 2021 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies , pages 2950 – 2968 . Chengrun Yang , Xuezhi Wang , Yifeng Lu , Hanxiao Liu , Quoc V . Le , Denny Zhou , and Xinyun Chen . 2023a . Large language models as optimizers . Xianjun Yang , Xiao Wang , Qi Zhang , Linda Petzold , William Yang Wang , Xun Zhao , and Dahua Lin . 2023b . Shadow alignment : The ease of subverting safely - aligned language models . Youliang Yuan , Wenxiang Jiao , Wenxuan Wang , Jen tse Huang , Pinjia He , Shuming Shi , and Zhaopeng Tu . 2023a . Gpt - 4 is too smart to be safe : Stealthy chat with llms via cipher . Zheng Yuan , Hongyi Yuan , Chuanqi Tan , Wei Wang , Songfang Huang , and Fei Huang . 2023b . Rrhf : Rank responses to align language models with human feed - back without tears . Rowan Zellers , Ari Holtzman , Hannah Rashkin , Yonatan Bisk , Ali Farhadi , Franziska Roesner , and Yejin Choi . 2020 . Defending against neural fake news . Qiusi Zhan , Richard Fang , Rohan Bindu , Akul Gupta , Tatsunori Hashimoto , and Daniel Kang . 2023 . Re - moving rlhf protections in gpt - 4 via fine - tuning . Zhexin Zhang , Leqi Lei , Lindong Wu , Rui Sun , Yongkang Huang , Chong Long , Xiao Liu , Xuanyu Lei , Jie Tang , and Minlie Huang . 2023a . Safety - bench : Evaluating the safety of large language mod - els with multiple choice questions . Zhexin Zhang , Junxiao Yang , Pei Ke , and Minlie Huang . 2023b . Defending large language models against jailbreaking attacks through goal prioritization . Chunting Zhou , Pengfei Liu , Puxin Xu , Srini Iyer , Jiao Sun , Yuning Mao , Xuezhe Ma , Avia Efrat , Ping Yu , Lili Yu , Susan Zhang , Gargi Ghosh , Mike Lewis , Luke Zettlemoyer , and Omer Levy . 2023a . Lima : Less is more for alignment . Zhanhui Zhou , Jie Liu , Chao Yang , Jing Shao , Yu Liu , Xiangyu Yue , Wanli Ouyang , and Yu Qiao . 2023b . Beyond one - preference - for - all : Multi - objective di - rect preference optimization for language models . Sicheng Zhu , Ruiyi Zhang , Bang An , Gang Wu , Joe Barrow , Zichao Wang , Furong Huang , Ani Nenkova , and Tong Sun . 2023 . Autodan : Automatic and inter - pretable adversarial attacks on large language mod - els . Daniel M . Ziegler , Seraphina Nix , Lawrence Chan , Tim Bauman , Peter Schmidt - Nielsen , Tao Lin , Adam Scherlis , Noa Nabeshima , Ben Weinstein - Raun , Daniel de Haas , Buck Shlegeris , and Nate Thomas . 2022 . Adversarial training for high - stakes reliability . Andy Zou , Zifan Wang , J . Zico Kolter , and Matt Fredrik - son . 2023 . Universal and transferable adversarial attacks on aligned language models .