Electrical Engineering and Computer Science Department Technical Report NWU - EECS - 07 - 05 July 21 , 2007 Reasoning Through Search : A Novel Approach to Sentiment Classification Sanjay Sood , Sara Owsley , Kristian J . Hammond , Larry Birnbaum Abstract We introduce a novel approach to sentiment classification . Our Reasoning Through Search ( RTS ) technique uses existing labeled data , query formation strategies , and a case base to estimate the sentiment of a text review . Unlike previous systems , when classifying a document from a domain that the system does not have explicit , in - domain training data , our classification system leverages domain relatedness and a case base of labeled reviews to perform the classification . While the system does require labeled training data , it does not rely on the guaranteed presence of in - domain labeled training data . Reasoning Through Search : A Novel Approach to Sentiment Classiﬁcation Sanjay Sood Intelligent Information Laboratory Northwestern University 2133 Sheridan Road Evanston , IL 60208 sood @ cs . northwestern . edu Sara Owsley Intelligent Information Laboratory Northwestern University 2133 Sheridan Road Evanston , IL 60208 sowsley @ cs . northwestern . edu Kristian J Hammond Intelligent Information Laboratory Northwestern University 2133 Sheridan Road Evanston , IL 60208 hammond @ infolab . northwestern . edu Larry Birnbaum Intelligent Information Laboratory Northwestern University 2133 Sheridan Road Evanston , IL 60208 birnbaum @ infolab . northwestern . edu ABSTRACT We introduce a novel approach to sentiment classiﬁcation . Our Reasoning Through Search ( RTS ) technique uses exist - ing labeled data , query formation strategies , and a case base to estimate the sentiment of a text review . Unlike previous systems , when classifying a document from a domain that the system does not have explicit , in - domain training data , our classiﬁcation system leverages domain relatedness and a case base of labeled reviews to perform the classiﬁcation . While the system does require labeled training data , it does not rely on the guaranteed presence of in - domain labeled training data . 1 . INTRODUCTION People have opinions about things they encounter in their day to day lives such as products and services . Traditionally , companies have captured such opinions through customer satisfaction surveys and focus groups in order to understand their users’ needs and improve products and services to meet these demands . The emergence of vast amounts of opinions online in the form of professional product reviews , consumer generated product reviews , newsgroups , weblogs , and news articles has given rise to an opportunity to collect this in - formation on a large scale , without explicitly requesting it from a consumer . In order for these information sources to be useful for mar - keting and business intelligence on a large scale [ 10 , 12 ] , com - panies must be able to automatically classify the author’s age , gender , geographic location and other demographic in - formation as well as the sentiment of their opinion on a prod - uct . Towards these goals , this paper focuses on the task of classifying the sentiment of a text , speciﬁcally focusing on product and service reviews . Copyright is held by the author / owner ( s ) . WWW2007 , May 8 – 12 , 2007 , Banff , Canada . . Our goal was to build a system that could classify the sen - timent of reviews across disparate domains without guaran - teeing the presence of any in - domain labeled training data . Classifying sentiment of a document without knowing its domain and without suﬃcient training data is particularly hard given the inconsistencies in the connotation of senti - ment words across domains . A word might have a very positive meaning in one domain , but may have a negative meaning in another domain . For example , a cell phone de - scribed as ‘small’ may be considered desirable , but when applied to a television , smallness is not usually considered a coveted trait . Conversely , domains may share language to convey sentiment . For example , people often use many of the same words to describe what they liked and disliked about movies and books . We introduce a novel approach to sentiment classiﬁcation . Our technique uses existing labeled data , query formation strategies , and a case base to estimate the sentiment of a text review . Unlike previous systems , when classifying a docu - ment from a domain that the system does not have explicit , in - domain training data , our classiﬁcation system leverages domain relatedness and a case base of other labeled reviews to perform the classiﬁcation . While the system does require labeled training data , it does not rely on the guaranteed presence of in - domain labeled training data . Until all text on the Web is clearly tagged and categorized , systems that can work across multiple domains , trained or untrained , will be critical to leverage the large corpus of data on the Web . 2 . PREVIOUS WORK Much of the previous work in sentiment classiﬁcation [ 1 , 23 , 22 , 18 ] has dealt with single domain classiﬁcation where there are large amounts of labeled data available for training . A large portion of this work has been concerned with getting the best in - domain classiﬁcation accuracy by using various machine learning strategies ( Na¨ıve Bayes , SVM , Maximum Entropy , etc . ) and selecting the most appropriate feature set ( unigrams , n - grams , adjectives , etc . ) to train from . From our investigation , little work has been done in trying to cre - ate a sentiment classiﬁer that can operate across new do - mains without labeled training examples . Other researchers have approached this problem without considering domain speciﬁcity . Nasukawa et al [ 15 ] built a system for extracting sentiment at a sentence level , extract - ing polarity for subjects in a document . Another similar ap - proach from Popescu and Etzioni [ 19 ] uses an unsupervised method ( relaxation labeling ) to extract and analyze opinion phrases corresponding to features as opposed to classifying the entire document . Both of these lexogrammatical ap - proaches have good performance , but rely on manual coding of words and their polarity or lexico - syntactic patterns . While building a system to classify subjectivity / objectivity and sentiment ( positive / negative ) , Finn and Kushmerick [ 9 ] noted that the performance of their classiﬁers degraded sig - niﬁcantly when applied to a new domain . To build a clas - siﬁer that worked across domains , they used an ensemble approach with diﬀerent feature sets , but the system’s per - formance peaked at 50 % . Other work [ 17 ] in sentiment clas - siﬁcation has shown clear diﬀerences in the polarity of words from a domain - speciﬁc classiﬁer and a general purpose af - fective corpus . A need was expressed for domain - speciﬁc classiﬁers in order to appropriately classify the sentiment of text from diﬀerent domains , since the diﬀerences in mean - ing of words between domains would reduce the accuracy of classiﬁcation . A domain - speciﬁc approach , however , requires training data in every domain that would be encountered in clas - siﬁcation . Aue and Gamon surveyed various techniques for sentiment classiﬁcation in new domains and concluded that labeled training examples from the new domain are needed for accurate classiﬁcation [ 3 ] . Enumerating and ﬁnding the appropriate labeled data in every single domain is time - consuming and not scalable to all the domains one my en - counter on the Web . There is also the problem of determin - ing the correct classiﬁer to apply for text where the domain is not known beforehand . This problem is apparent in at - tempting to classify weblogs as they are unstructured , unla - beled , and unedited . While topic classiﬁcation [ 16 , 13 ] has traditionally had better performance than sentiment classi - ﬁcation , it also requires having training data for every topic the system will consider . Approaching text classiﬁcation from a diﬀerent angle , sys - tems have been built that use case - based reasoning to clas - sify text such as e - mail spam – a task often approached using machine learning techniques . Cunningham , et al , ar - gue that case - based classiﬁcation works well when there is a variation among individual cases that cannot be captured in a high level statistical model [ 7 , 5 , 11 ] . Similarly , given the variation in the aﬀective connotation of language when used across domains [ 17 ] , we feel that a case - based approach to sentiment classiﬁcation could yield better accuracy by lever - aging individual cases as opposed to a uniﬁed statistical rep - resentation . 3 . REASONING THROUGH SEARCH ( RTS ) The system we propose uses a combination of machine learning , information retrieval techniques , and a case base to determine the sentiment of a review published on the Web . We interpreted sentiment classiﬁcation as being a binary classiﬁcation problem between positive and negative . Fig - ure 1 shows an architecture diagram of the RTS system . We begin by transforming text into a set of features . Instead of using a probabilistic model such as Na¨ıve Bayes to classify the set of features as positive or negative , we leverage a sta - tistical model of training data to create a representation of the target document , in the form of a sentiment query . This query is used to retrieve singular cases of labeled data from a case base . In addition , we use the feature representation of the document to retrieve a closest - ﬁt ranked list of known domains . The labels of the returned cases and the ranked domain list are processed by a case evaluator to extract a score for the sentiment of the document . 3 . 1 Data We gathered our data from Rate - It - All [ 21 ] , an online repository of consumer written reviews on a wide variety of topics including : products , services , famous people , sports teams , movies , music , and colleges . The reviews each have an associated rating , 1 to 5 stars , assigned by the author . Once submitted to RateItAll , the reviews do not go through an editorial process and are presented as is . We chose a sub - set of domains from RateItAll that we felt covered a breadth of topics . The domains we selected were : actors , books , col - leges , destinations , drinks , electronics , food , movies , music , restaurants , software , sports and video games . We collected a total of 106 , 961 reviews from these 14 do - mains . Some reviews consisted of a star rating with no re - view text or a very short review text , so we limited our collection to reviews with 6 or more words . The average length of a review was 47 . 86 words , with a minimum length of 6 words and a maximum length of 1205 words . Given that the reviews were rated between 1 and 5 , we labeled the set of negative reviews as those with 1 or 2 stars , and positive reviews were those with 4 or 5 stars . Reviews with 3 stars were ignored as they were seen to be neutral . While this corpus of reviews is very useful as a training corpus , it does have some anomalies . Since the reviews do not go through any editorial process , they often contain mis - spellings , use slang words , and are oﬀ topic . Reviewers oc - casionally make mistakes in terms of the number of stars they assign to a review . Since RateItAll exists as a social network as well , the reviews often contain dialog between reviewers . As with any free text , human - generated content , such noise is unavoidable . 3 . 2 Domain Classiﬁer Previous work in sentiment analysis established , as dis - cussed previously , that words have diﬀerent aﬀective con - notations across domains . Since our case base contains re - views across all fourteen domains , retrieving relevant cases requires knowledge of the domain ( s ) that the current docu - ment is similar to . To meet this need , we built a Na¨ıve Bayes domain classiﬁer because of the relative accuracy and ease of implementation . Given a document from an unknown domain , the classiﬁer returns a list of domains , ranked and weighted from most related to least related , where the weights are normalized probabilities . The training data for this classiﬁer was reviews across fourteen domains , described in the previous section . To train the classiﬁer , we treated each unigram as a feature of a document , while employing Porter’s stemmer [ 20 ] to compress terms with morphological variation . While pre - Figure 1 : An architecture diagram of the RTS system . vious work has shown that alternative feature extraction ( bigrams , adjectives , phrases , etc . ) provides an improve - ment in performance [ 4 , 14 ] , we found the performance of a system using unigrams as features to suﬃce for our pur - poses . Each training document , d , was split into a vector containing the n unique features that appeared in the doc - ument , capturing the presence of a feature in a document and not the frequency [ 18 ] of the features in the document : d i = < f 1 , f 2 , f 3 , . . . , f n > . Combining these feature vectors with the known domains of the training documents , the probabilities for each feature in the classiﬁer were calculated . For ﬂexibility , we created the domain classiﬁer by building a probabilistic sentiment model ( positive / negative ) for each domain . This allowed us to generate a Na¨ıve Bayes statistical model on the ﬂy for both domain and sentiment classiﬁcation . Given the target document , a ranked and weighted list of domains is created by calculating the NB probability that the document d is a part of each candidate class c , where c is one of the domains . The standard NB classiﬁer employs a product of the probabilities , however , to prevent underﬂow of the product , we used the summation of the logs of the probabilities . We also used add one smoothing to prevent the length of the document or the amount of training data in each domain from inﬂuencing the classiﬁcation . The fol - lowing equation was used to get the NB probability that the document d is a part of each class c : P NB ( c | d ) = P ( c ) + ( m X i = 1 log ( P ( f i | c ) n i ( d ) ) ) 3 . 2 . 1 Domain Relatedness After we created our domain - speciﬁc sentiment classiﬁers , we experimented with sentiment classiﬁcation within and across domains . The goal of this experiment was to verify the notion that sentiment classiﬁers are domain - speciﬁc and do not work across domains . For testing we took each domain classiﬁer , 14 in total , and tested it’s accuracy in classifying sentiment in every domain , including its native domain . In testing on the na - tive domain , we employed 5 - fold cross - validation . Table 1 shows a selection of results from the experiment . The data showed that the highest accuracy for each domain classi - ﬁer was when it was tested against the native domain data , which is expected since it is in - domain classiﬁcation . Based on classiﬁcation accuracy , we observed distinct clusters be - tween domains that are topically related . For example , the classiﬁer for ’restaurants’ performed well over testing data from the ’food’ domain and vice versa . There was poor performance with some pairs of domains . For example , the accuracy of the ’movie’ classiﬁer when applied to ’colleges’ was only 41 . 5 % , below the 50 % baseline accuracy for bi - nary classiﬁcation . To explain this result , we theorize that there is disagreement among discriminating sentiment terms between the two domains – some terms that are positive in movies have a negative connotation when applied to colleges . These results show that there are deﬁnite relationships be - tween domains based on common language usage . It seems reasonable to believe that people would use similar language to describe books and movies . It also seems reasonable to agree that people talk about cars and music diﬀerently . With this in mind , domain relationships and individual cases in other domains can be leveraged for classifying sentiment . 3 . 3 Case Retrieval The next step in the sentiment classiﬁcation process in - volves creating a representation of the target document that extracts features related to sentiment . This representation can then be used to retrieve similar cases from a case base . 3 . 3 . 1 Query Formation Transforming a document to a set of term based queries is a well - known method for information systems that provide relevant , related content . Systems , such as Watson [ 6 ] , use the document as a starting point for ﬁnding similar content Table 1 : A sample of Na¨ıve Bayes sentiment classiﬁcation accuracies within and across domains . Classiﬁer Testing Domain Accuracy actors actors 76 . 99 % colleges colleges 82 . 11 % food restaurants 77 . 14 % food software 41 . 90 % movies colleges 41 . 54 % restaurants destinations 70 . 29 % software electronics 68 . 00 % sports music 41 . 61 % video games software 68 . 04 % by forming a term - based query and retrieving information from disparate information sources . In Watson , the model of relevance is almost always viewed as a function of topical similarity . In our system , however , we viewed the problem as being that of ﬁnding results based on aﬀective similarity – using features that are highly discriminating for sentiment as the query - based representation of the document . During the training phase , we calculated a probabilistic model for sentiment ( positive / negative ) over each domain , giving us sentiment information for a given unigram across all domains . We used this model to extract the strongest aﬀective unigrams from the target document . To do this , we tokenized and stemmed the target document . For each word , we calculated the sentiment magnitude of the word w using the following formula : sm ( w ) = m X i = 1 abs ( log ( P i ( w | pos ) ) − log ( P i ( w | neg ) ) ) where i denotes a domain in which we are examining the probabilities , with m domains in total . Words with high sentiment magnitude are seen as being discriminating terms for sentiment . While this does not provide an overall sentiment ( positive / negative ) for a word , since we are taking the absolute value , it considers that a word might have a positive overall connotation in one do - main , but a negative in others . The resulting score measures the overall , absolute polarity of a word across all domains . Once we calculated a sentiment magnitude for each term in the review , we formed a term based query by sorting the resulting vector by sentiment magnitude . For each candi - date word we scanned a 3 term window around the term in the document looking for modiﬁers ( very , not , too , etc ) . In the case of ﬁnding a modiﬁer within the window , the term was expanded to the phrase that includes the modiﬁer . For example , the term ‘funny’ with a high sentiment magnitude would be transformed to ‘not funny’ to reﬂect its meaning in the target document . After completing expansion , the system created a term - based query by concatenating the terms with the highest calculated sentiment magnitude . We limited the length of each query to be 4 terms , not including term expansion with modiﬁers . The generated query is then used to retrieve re - lated cases from our case base . In some very rare cases a generated query did not return results because the representation created for the document did not match any cases in the case base . In these instances , the query was relaxed and resubmitted to the case base by removing terms from the right of the query vector . Removal of words creates a more general query that has a higher likelihood of matching other cases . 3 . 3 . 2 The Case Base For case retrieval , we wanted to be able to retrieve la - beled reviews by text - based similarity to a generated query . Instead of implementing our own retrieval system , an oﬀ - the - shelf search engine was suﬃcient for our purposes . We employed Apache Lucene [ 2 ] , an open - source search engine , to index and retrieve cases . All labeled reviews described above in the data section were indexed in the engine . The standard Lucene setup was used , indexing both the content of the reviews , as well as the known domain of the review . This allowed us to retrieve cases based on textual similarity to the review and to ﬁlter based on the domain of the review . The Lucene engine was originally setup to ig - nore stopwords during indexing . This meant that searching for the phrase “United States of America” would yield no re - sults , though the phrase did exist in the case base , because the word ‘of’ was not included in the engine’s index . We modiﬁed the engine so that all words , including stopwords , were indexed , allowing us to include phrases , such as those described in the previous section , in our queries . Each review’s ﬁle name was created such that it contained meta - data including its domain , the rating assigned by its author ( one to ﬁve stars ) , and a unique identiﬁer ( e . g . music - 4 - 8273 . txt ) . The body of the text ﬁle contained the text of the review . 3 . 4 Result Ranking and Document Scoring After a set of aﬀect - similar cases have been returned from the case base , a sentiment score is calculated for the tar - get document based on the sentiment scores attached to the retrieved cases . We only looked at the top 25 results returned from Lucene when calculating a sentiment score . Given that a target document may be closely related top - ically to a known domain in the system , the ranked and weighted domain list generated in domain classiﬁcation is used to weight each sentiment score for the returned cases . The overall score for the document is calculated as follows . Given that ls ( c ) returns the labeled score of a case c and w ( c ) returns the weight of the labeled domain of a case c . sc ( d ) = P mi = 1 ( ls ( case i ) − 3 ) ∗ w ( case i ) m returns the sentiment score of a document d based on m retrieved cases . In addition to weighing the case scores from the domain classiﬁer , we also experimented with weighing the scores by the ranked position of the domain in the classiﬁcation re - sults . Preliminary experiments show that weighing the re - sults using either strategy had little eﬀect on the ﬁnal clas - siﬁcation accuracy . We speculate this has to do with the overall performance of the domain classiﬁer , which is not phenomenal . We theorize that improving the classiﬁcation accuracy of the domain classiﬁer by using more sophisti - cated feature extraction techniques or another system may improve this portion of the system . 4 . RESULTS To evaluate the results from our system , we chose to com - pare the performance the system against other computa - tional methods for sentiment classiﬁcation of documents in unknown domains . In addition , we ran a small human study to get a sense of how well humans can classify the sentiment of reviews . 4 . 1 Human Study We ran a small study that asked participants to perform such classiﬁcation . To generate a questionnaire , we ran - domly selected the text of a set of reviews , across all do - mains and with a positive or negative rating , from the cor - pus collected from RateItAll . We created a questionnaire that contained the text of each review and asked study par - ticipants to rate each review as being positive or negative . The star score assigned to each review by the author was treated as the truth value , where a score less than 3 stars was considered negative , greater than 3 stars positive . The study consisted of 13 participants . After dropping the highest and lowest score , the results of the study showed that assigning sentiment values to text is a non - trivial task for humans . The average accuracy of human classiﬁcation was 15 . 72 out of 20 ( 78 . 6 % ) with a standard deviation of 1 . 55 . In aggregate , humans tended to have problems classify - ing short reviews that lacked suﬃcient context to determine the object of the review . The participants also had prob - lems classifying reviews that used sarcasm . It is speciﬁcally these types of reviews that have been some of the major fail - ure cases for almost every sentiment classiﬁcation system to date . 4 . 2 The All Data Approach One approach to text classiﬁcation is to train one classiﬁer on the labeled data in all domains [ 3 ] . In this approach , an equal amount of data from each domain is used to train a general purpose classiﬁer . With this approach , however , variation of distinguishing features between classes over the domain set can severely degrade classiﬁcation accuracy . In other words , if the aﬀective connotation of a word varies across domains , combining the domains into one classiﬁer will diminish classiﬁcation accuracy . To see how an all data classiﬁer compared to our system , we used our existing infrastructure to train separate senti - ment classiﬁers for each domain using an equal amount of data from each domain . Separate classiﬁers allowed us to combine the results across classiﬁers at run time to create an all data classiﬁer . Once again , we used stemmed uni - grams as the feature set . We then took each domain and performed sentiment classiﬁcation on all the reviews using an all data classiﬁer made up of the other 13 domains . Table 2 illustrates the classiﬁcation accuracy of this approach . Table 2 : Average accuracies of sentiment classiﬁca - tion across all domains . Approach Accuracy human baseline 78 . 60 % all data 66 . 00 % ensemble 60 . 66 % RTS ( sim queries ) 62 . 72 % RTS 73 . 39 % 4 . 3 Ensemble Approach Another approach to classifying sentiment in unknown domains is using an ensemble of classiﬁers [ 8 ] . With this method , a classiﬁer is trained on each domain . Instead of being combined into one super classiﬁer , as with all data , the result of each classiﬁer is combined to produce an over - all classiﬁcation . Like the all data system , the ensemble approach leverages out - of - domain labeled data to classify data in a new domain . As with any classiﬁcation system , implementation details vary . Once again , we used the existing domain classiﬁers , described above , to provide the ensemble of classiﬁers . To test the ensemble system , we performed classiﬁcation for each domain against an ensemble of classiﬁers that in - cluded every domain except the target domain . While there are various ways of combining the results from classiﬁers , we set up a simple voting mechanism where each ensemble could vote whether the target document was positive or neg - ative . Since there were an odd number of domains ( when the target domain is removed ) and all domain classiﬁers had an equal vote , classiﬁcation of the text was either positive or negative , with no unknowns . The accuracy of the ensemble classiﬁer is illustrated in Table 2 . 4 . 4 Our System To evaluate our system , we ran the system across each of the fourteen domains , measuring classiﬁcation accuracy . Since we are interested in the performance of classifying un - known domains , we removed all in - domain training data at runtime . This includes removing training data for the target domain from the domain classiﬁer , sentiment term extrac - tor , and the case base . Removing this domain - speciﬁc infor - mation functionally made the target domain unseen in the system , since there was no in - domain labeled data present in any of the processing steps . The accuracy of the RTS system compared to other ap - proaches is illustrated in Table 2 . The average accuracy of sentiment classiﬁcation across all domains was 73 . 39 % . More detailed results showing classiﬁcation accuracies across each domain can be seen in Table 3 . In order to examine the performance of the sentiment queries , we decided to modify the query formation . Instead of creating a representation of the document based on the strength of terms based on sentiment , we created a query based on frequency of the terms ( removing stopwords ) . This method gave us a similarity query that represent topically centered terms in the text . We then used that query to retrieve cases from the case base . The results of this exper - iment can be found in Table 2 under RTS ( sim queries ) . The comparison between RTS , using sentiment queries , and Table 3 : The accuracy of sentiment classiﬁcation across each domain using the RTS approach . Domain Accuracy actors 75 . 50 % books 72 . 81 % cars 74 . 36 % colleges 73 . 20 % destinations 75 . 26 % drinks 73 . 75 % electronics 68 . 47 % food 67 . 82 % movies 73 . 17 % music 76 . 59 % restaurants 75 . 98 % software 70 . 65 % sports 68 . 53 % video games 68 . 38 % RTS using similarity queries , shows that the query formation algorithms are eﬀective in extracting a term - based represen - tation of the sentiment of a document . The results in Table 2 show that our system outperformed other techniques for sentiment classiﬁcation in new domains . Unlike previous results [ 3 ] in unknown domain sentiment classiﬁcation , our system does not require labeled examples in the new domain . While the system did not outperform the human baseline , future enhancements to the system have the potential to close the gap . 5 . FUTURE WORK While our system has promising results , we see several opportunities for improvement . The current system uses unigrams as features for both the domain classiﬁer as well as the sentiment term extraction algorithm . A future sys - tem could employ an information gain feature extraction algorithm and other features such as bigrams . A known problem of sentiment classiﬁcation is detecting sarcasm and irony in text . We see an opportunity to make advances in addressing this problem . In query formation , our current system extracts terms from a document that are highly discriminating between positive and negative cases using a probabilistic model of the training documents . Using this model , we could identify cases with conﬂicting language and employ an alternate classiﬁcation strategy . An important aspect of our system is the selection of a set of domains . We subjectively selected domains that we felt covered a large breadth of topic areas at the right level of speciﬁcity . A computational approach to domain selection is possible given the relationship between domains illustrated in Table 1 . Using human categorization as a starting point , we can validate the relatedness of documents in a domain by build - ing sentiment classiﬁers within each domain and performing classiﬁcation within and across domains . Given the results , domains can be combined if classiﬁcation is accurate across those domains , indicating that these domains have similar language connotation for sentiment . Conversely , a domain can be decomposed if classiﬁcation accuracy is poor within the domain , meaning that sentiment bearing words cannot be generalized within the predeﬁned domain label . 6 . CONCLUSION While marketing research is an area which demands sen - timent classiﬁcation tools , there are other areas in which speciﬁc aspects of this system could be useful . We have applied this system to analyze the emotional con - tent of weblogs ( blogs ) . We use this analysis in two current systems called Buzz and News at Seven . Buzz is a digital theater installation deployed as a group of virtual actors , an - imated faces with computer voice generation , who present compelling and emotional stories found in blogs . This emo - tional classiﬁcation system has been critical to the success of Buzz , improving it’s performance greatly by allowing the system to ﬁlter out the unemotional stories . The Reasoning Through Search emotional classiﬁcation system has also proven to be critical in News at Seven , a completely automatically generated news show using con - tent found in various web resources and presented through characters in a modern game engine . For each story in News at Seven , it presents a blogger’s point of view on the topics in the story . This system enables News at Seven to ﬁnd opinions that are emotional and pointed . The extraction of sentiment bearing terms from a docu - ment can facilitate the building of new search systems that index documents based on their sentiment . Coupled with existing search technology , systems can be created to allow users to browse documents by their emotional and topical content . This technology , for example , could allow users to retrieve reviews with opposing views on a product or service . Another application for this technology is the improve - ment of the prosody of speech generation systems . Knowing the emotionally charged terms in a sentence can aide in gen - erating believable and engaging speech . We are currently integrating the extraction of sentiment terms into a speech generation system tied to an avatar built to host a full stage improvisational show . While the performance of our system does not exceed that of a domain - speciﬁc sentiment classiﬁer , it outperforms other methods of sentiment classiﬁcation over unknown do - mains . Our system proves to be a promising and novel ap - proach to sentiment classiﬁcation of text . This approach is particularly powerful when an explicit domain set and corre - sponding training data are not available – a prime example being classiﬁcation on the Web . 7 . REFERENCES [ 1 ] C . O . Alm , D . Roth , and R . Sproat . Emotions from text : machine learning for text - based emotion prediction . In Proceedings of HLT / EMNLP , 2005 . [ 2 ] Apache Lucene . http : / / lucene . apache . org / , 2006 . [ 3 ] A . Aue and M . Gamon . Customizing sentiment classiﬁers to new domains : a case study . In Proceedings of RANLP , 2005 . [ 4 ] R . Bekkerman and J . Allan . Using bigrams in text categorization . Technical Report IR - 408 , Department of Computer Science , University of Massachusetts , Amherst , MA , 2004 . [ 5 ] S . Br¨uninghaus and K . D . Ashley . How machine learning can be beneﬁcial for textual case - based reasoning . In Proceedings of the AAAI - 98 / ICML - 98 Workshop on Learning for Text Categorization , pages 71 – 74 , 1998 . [ 6 ] J . Budzik . Information Access in Context : Experiences with the Watson System . PhD thesis , Northwestern University , June 2003 . [ 7 ] P . Cunningham , N . Nowlan , S . J . Delany , and M . Haahr . A case - based approach to spam ﬁltering that can track concept drift . In Proceedings of the ICCBR Workshop on Long - Lived CBR Systems , 2003 . [ 8 ] T . G . Dietterich . Machine learning research : Four current directions . AI Magazine , 18 : 97 – 136 , 1997 . [ 9 ] A . Finn and N . Kushmerick . Learning to classify documents according to genre . In IJCAI - 03 Workshop on Computational Approaches to Style Analysis and Synthesis , 2003 . [ 10 ] N . Glance , M . Hurst , K . Nigam , M . Siegler , R . Stockton , and T . Tomokiyo . Analyzing online discussion for marketing intelligence . In Proceedings of the 14th International Conference on the World Wide Web , pages 1172 – 1173 , Chiba , Japan , 2005 . [ 11 ] M . Healy , S . Delany , and A . Zamolotskikh . An assessment of case - based reasoning for short text message classiﬁcation . In N . Creaney , editor , Procs . of 16th Irish Conference on Artiﬁcial Intelligence and Cognitive Science , ( AICS - 05 ) , pages 257 – 266 , 2005 . [ 12 ] M . Hu and B . Liu . Mining and summarizing customer reviews . In Proceedings of the tenth international conference on Knowledge Discovery and Data Mining , pages 168 – 177 , Seattle , WA , 2004 . [ 13 ] T . Joachims . Text categorization with support vector machines : Learning with many relevant features . In Proceedings of the European Conference on Machine Learning , pages 137 – 142 , 1998 . [ 14 ] S . Muresan . Combining linguistic and machine learning techniques for email . In Annual Meeting of the ACL , Proceedings of the workshop on Computational Natural Language Learning , volume 7 , pages 1 – 8 , 2001 . [ 15 ] T . Nasukawa and J . Yi . Sentiment analysis : Capturing favorability using natural language processing . In Proceedings of the 2nd International Conference on Knowledge Capture , 2003 . [ 16 ] K . Nigam , J . Laﬀerty , and A . McCallum . Using maximum entropy for text classiﬁcation . In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering . , 1999 . [ 17 ] S . Owsley , S . Sood , and K . J . Hammond . Domain speciﬁc aﬀective classiﬁcation of documents . In Proceedings of the AAAI Spring Symposium on Computational Analysis of Weblogs . , pages 181 – 183 , 2006 . [ 18 ] B . Pang , L . Lee , and S . Vaithyanathan . Thumbs up ? sentiment classiﬁcation using machine learning techniques . In Proceedings of EMNLP , pages 79 – 86 , 2002 . [ 19 ] A . - M . Popescu and O . Etzioni . Extracting product features and opinions from reviews . In Proceedings of HLT - EMNLP 2005 , 2005 . [ 20 ] M . F . Porter . An algorithm for suﬃx stripping . Program , 14 ( 3 ) : 130 – 137 , 1980 . [ 21 ] Rate - It - All : The Opinion Network . http : / / www . rateitall . com / , 2006 . [ 22 ] P . D . Turney . Thumbs up or thumbs down ? semantic orientation applied to unsupervised classiﬁcation of reviews . In ACL , pages 417 – 424 , 2002 . [ 23 ] P . D . Turney and M . L . Littman . Measuring praise and criticism : Inference of semantic orientation from association . ACM Transactions on Information Systems ( TOIS ) , 21 ( 4 ) : 315 – 346 , 2003 .