News Summarization and Evaluation in the Era of GPT - 3 Tanya Goyal 1 Junyi Jessy Li 2 Greg Durrett 1 1 Department of Computer Science 2 Department of Linguistics The University of Texas at Austin tanyagoyal @ utexas . edu Abstract The recent success of prompting large language models like GPT - 3 has led to a paradigm shift in NLP research . In this paper , we study its impact on text summarization , focusing on the classic benchmark domain of news summariza - tion . First , we investigate how GPT - 3 compares against fine - tuned models trained on large sum - marization datasets . We show that not only do humans overwhelmingly prefer GPT - 3 sum - maries , prompted using only a task descrip - tion , but these also do not suffer from common dataset - specific issues such as poor factuality . Next , we study what this means for evalua - tion , particularly the role of gold standard test sets . Our experiments show that both reference - based and reference - free automatic metrics can - not reliably evaluate GPT - 3 summaries . Fi - nally , we evaluate models on a setting beyond generic summarization , specifically keyword - based summarization , and show how dominant fine - tuning approaches compare to prompting . To support further research , we release : ( a ) a corpus of 10K generated summaries from fine - tuned and prompt - based models across 4 standard summarization benchmarks , ( b ) 1K human preference judgments comparing differ - ent systems for generic - and keyword - based summarization . 1 1 Introduction Fine - tuning pre - trained models on domain - specific datasets has been the leading paradigm in text sum - marization research in recent years ( Lewis et al . , 2020 ; Zhang et al . , 2020 ; Raffel et al . , 2020 ) . These models generate high - quality summaries on stan - dard benchmarks , but still require sizeable training datasets to adapt to new settings , e . g . , summarizing data from a new source domain or producing a sum - mary in a different style . The success of prompt - ing large language models ( GPT - 3 ( Brown et al . , 1 All data available at : https : / / tagoyal . github . io / zeroshot - news - annotations . html . GPT - 3 : Donald Trump ' s lawsuit against Hillary Clinton , the Democratic National Committee , and several other individuals and entities has been dismissed by a federal judge . The judge ruled that Trump ' s claims are unsupported by any legal authority and that many of his specific factual assertions are implausible or unsupported . CNN article : https : / / www . cnn . com / 2022 / 09 / 09 / politics / judge - throws - out - trumps - rico - lawsuit - against - hillary - clinton - and - democrats / index . html GPT - 3 : Alina Habba is an attorney for Donald Trump . She stated that Trump will " immediately move to appeal this decision . " Habba also said that the judge ' s opinion is " rife with erroneous applications of the law " and that it " disregards the numerous independent governmental investigations which substantiate our claim that the defendants conspired to falsely implicate our client and undermine the 2016 Presidential election . " Length - constrained prompt Keyword - constrained prompt Summarize the above article in 2 sentences . Summarize the above article briefly focusing on Alina Habba . Figure 1 : Examples of GPT - 3 summaries . We can gen - erate summaries following style constraints or queries included in the prompts , allowing us to emulate a range of existing fine - tuned systems . 2020 ) , T0 ( Sanh et al . , 2022 ) , PaLM ( Chowdhery et al . , 2022 ) , etc . ) provides an alternative approach , namely learning from natural language task instruc - tions and / or a few demonstrative examples in the context without updating model parameters . While recent work ( Zhao et al . , 2021 ; Min et al . , 2022 ; Ye and Durrett , 2022 ) has evaluated this paradigm across a number of tasks , it has only been studied for text summarization with unreliable automatic metrics ( He et al . , 2022b ; Chowdhery et al . , 2022 ; Ouyang et al . , 2022 ) or in non - standard settings ( Saunders et al . , 2022 ) . In this paper , we conduct the first systematic study of the impact of prompt - based models on the text summarization research space , using an Instruct - tuned 175B GPT - 3 model ( text - davinci - 002 ) ( Brown et al . , 2020 ; Ouyang et al . , 2022 ) as a case study . Figure 1 shows that GPT - 3 summaries are extremely high - quality and adaptable to differ - ent summarization settings . Starting from these observations , we aim to answer three main ques - tions . First , how do prompt - based GPT - 3 sum - maries compare to those obtained from state - of - a r X i v : 2209 . 12356v2 [ c s . C L ] 23 M a y 2023 the - art fine - tuned summarization models ( Zhang et al . , 2020 ; Liu et al . , 2022 ) ? We compare these approaches using A / B testing on a new corpus of recent news articles , and find that our study participants overwhelmingly prefer GPT - 3 sum - maries across two different “styles” with differ - ent prompts ( three - sentence and single - sentence ) . Moreover , these summaries do not suffer from lim - itations due to low - quality training data that plague fine - tuned generic summarization models ( Maynez et al . , 2020 ; Goyal et al . , 2022 ) . Second , are existing automatic metrics well - suited to evaluating prompt - based summaries ? Re - cent work has shown that classic reference - based such as ROUGE ( Lin , 2004 ) and BERTScore ( Zhang * et al . , 2020 ) are unreliable when small improve - ments are reported ( Peyrard , 2019 ; Fabbri et al . , 2021 ) ; however large differences , on the order of say 5 R OUGE points or greater , are considered to be correlated with human preferences ( Bhandari et al . , 2020 ; Deutsch et al . , 2022 ) . However , we find that the same is no longer true when evaluating GPT - 3 summaries . These summaries score much lower on automatic metrics ( 7 ROUGE - L points on average ) than all prior state - of - the - art models while com - fortably outperforming them on human evaluation . Furthermore , we show that recent reference - free metrics , e . g . QA - based metrics ( Fabbri et al . , 2022 ; Durmus et al . , 2020 ) and trained factuality models ( Kryscinski et al . , 2020 ; Goyal and Durrett , 2020 ) , similarly fail to adapt to this shift from the fine - tuned to prompting , and need to be re - visited . Finally , how can prompting be used beyond generic summarization ? We focus on keyword - based and aspect - based summarization . For keyword - based summarization , we find that GPT - 3 consistently generates more coherent and keyword - relevant summaries compared to current fine - tuned alternatives : crowd annotators prefer GPT - 3 sum - maries over a baseline model ( He et al . , 2022a ) 70 % of the time . We observe mixed results for the aspect - based setting , where GPT - 3 summaries show frequent failure cases with simple prompts . Taken together , this evidence suggests that GPT - 3 represents a fundamental paradigm shift in sum - marization , changing what data we need ( or don’t need ) and what approaches we can now explore . Evaluating these systems will require a new frame - work distinct from the automatic metrics that have dominated the last decade of summarization re - search . Dataset Avg . Words % novel n - grams Article Summ n = 1 n = 2 CNN 760 . 5 45 . 7 16 . 7 54 . 3 DailyMail 653 . 3 54 . 6 17 . 0 53 . 8 XSum ( BBC ) 431 . 1 23 . 2 35 . 7 82 . 4 Newsroom 658 . 6 26 . 7 18 . 9 47 . 5 Table 1 : Basic statistics of standard summarization datasets : CNN / DM ( Hermann et al . , 2015 ; Nallapati et al . , 2016 ) , XSum ( Narayan et al . , 2018 ) , Newsroom ( Grusky et al . , 2018 ) . These show large variance in their sum - mary properties and fundamentally differ in their defini - tion of the “gold” standard . 2 Models and Setup 2 . 1 Current Paradigms for Summarization Recent zero - and few - shot prompting based mod - els ( Brown et al . , 2020 ; Sanh et al . , 2022 ) , have shown impressive generalization capabilities on unseen tasks specified using prompts alone and without performing any gradient updates ( Mishra et al . , 2022 ) . In this work , we want to compare their text summarization performance against the current state - of - the - art models . Pre - trained LM ( Task - specific models trained for each dataset ) Fine - tuned on summ . datasets Instruction - tuned on multiple tasks Prompting Zero - shot prompting ‣ BART ‣ PEGASUS ‣ T5 ‣ CTRLSum ‣ T0 ‣ FLAN ‣ Instruct - GPT ‣ GPT - 3 ‣ PaLM ‣ Turing - NLG ( Not available or less effective than instruction - tuned counterparts ) ( these not trained on standard summ . datasets ) ‣ t ext - davinci - 002 ‣ BRIO Summarization datasets used during training Figure 2 : Broad categorization of available summariza - tion systems ; those compared in this work are high - lighted in red . Figure 2 shows the broad categories of all avail - able summarization approaches , including current SOTA models and prompting - based models . The former set consists of fine - tuned language mod - els , trained on a large number of article - summary pairs ( e . g . BART ( Lewis et al . , 2020 ) , PEGASUS ( Zhang et al . , 2020 ) , BRIO ( Liu et al . , 2022 ) ) to obtain dataset - specific systems . This category also includes models aimed at tasks beyond generic summarization , such as keyword - or query - based summarization , that still rely on standard datasets for training ( He et al . , 2022a ) . On the other extreme are zero - or few - shot models , ( e . g . GPT3 ( Brown et al . , 2020 ) , PaLM ( Chowdhery et al . , 2022 ) ) , that are not explicitly trained for any particular task , as discussed above . Recent work ( Ouyang et al . , 2022 ; Wei et al . , 2022 ; Sanh et al . , 2022 ) has improved on these models by introducing instruction - tuned models . Here , pre - trained language models are fine - tuned on mul - tiple tasks ( which may include summarization ) us - ing instruction templates in order to align their training with inference time usage . In this work , we compare the summarization performance of three models that are representative of this space of options : 1 . OpenAI’s text - davinci - 002 , a GPT - 3 model ( Brown et al . , 2020 ) from the Instruct series ( Ouyang et al . , 2022 ) . While we do not know the exact training details for this release of the model , the previous in the series ( text - davinci - 001 ) was fine - tuned on a combina - tion of prompts submitted to their API and la - beler prompts spanning multiple tasks . These tasks include summarization but not ( to our knowledge ) standard summarization datasets like CNN / DM ( Hermann et al . , 2015 ; Nallapati et al . , 2016 ) or XSum ( Narayan et al . , 2018 ) . We choose the text - davinci - 002 version for our experiments in order to benchmark the best available prompt - based model . 2 We refer to this approach as GPT3 - D2 . 2 . BRIO ( Liu et al . , 2022 ) , a fine - tuned summariza - tion model that reports state - of - the art results on both CNN / DM and XSum . We will use versions of this model fine - tuned on each of these two datasets . 3 . T0 ( Sanh et al . , 2022 ) , a prompt - based model fine - tuned on multiple tasks including standard summarization datasets . This provides a use - ful point of comparison between task - specific fine - tuned ( BRIO ) and bigger instruction - tuned models ( GPT3 - D2 ) . 2 . 2 Using GPT3 - D2 for summarization Fine - tuned models largely follow the “style” of ref - erence summaries in their training data , and hence , generated summaries show large variance between datasets ( see Table 1 for basic summary statistics of standard summarization datasets ) . To ensure fair comparison between these and GPT3 - D2 , we adapt the latter’s prompt to align with dataset - specific styles . 2 We did not observe obvious quality differences in gen - erated summaries between text - davinci - 001 and text - davinci - 002 . Examples are included in Appendix C . The three African nations on the UN Security Council condemned reports of discrimination against African citizens at the Ukrainian border during a meeting at the UN HQ in New York City Monday . The United Nations Security Council condemned the reports of discrimination against African citizens at the Ukrainian border . The African Union has said it is " disturbed " by the reports of segregation against Africans in Ukraine , which it described as " shockingly racist . " The article discusses the reports of discrimination against African citizens at the Ukrainian border . The representatives from the three African nations on the UN Security Council condemned the reports and called for the mistreatment of African peoples on Europe ' s borders to cease immediately . Foreign students attempting to flee Ukraine after Russia invaded the country told CNN that they experienced racial discrimination at the Ukrainian border . Prompt : Summarize the article in N sentences . N = 1 N = 2 N = 3 Article : https : / / www . cnn . com / 2022 / 03 / 01 / africa / africa - condemns - racism - ukraine - intl / index . html Figure 3 : Illustration of length control using the task description / prompt for GPT3 - D2 . We found that the generated summaries followed the given sentence length constraint 98 % of the time , allowing us to generate different length summaries emulating different datasets . Specifically , we follow prior work ( Sanh et al . , 2022 ) and use sentence - count length prompts to adapt to each dataset . Although these datasets also differ along other attributes , e . g . CNN / DM is lead - biased whereas XSum requires drawing inferences from a whole article , we do not attempt to con - trol any other attributed of the summary . Figure 3 shows an example of different length GPT3 - D2 sum - maries for the same news article , using the follow - ing prompt format : Article : { { article } } Summarize the above article in N sentences . We found that GPT3 - D2 summaries faithfully fol - low the given length constraint in 98 % of the test instances used in our human study data in Sec - tion 3 . Given this setup , we first compare the summary quality of the three summarization models through a human annotation study ( Section 3 ) . Then , we evaluate the current suite of summarization metrics for prompt - based summarization ( Section 4 ) . Fi - nally , in Section 5 , we briefly discuss GPT3 - D2 per - formance on summarization tasks beyond generic summarization and new challenges . 3 Human evaluation of GPT3 - D2 summaries Generated summaries of fine - tuned models ( Lewis et al . , 2020 ; Zhang et al . , 2020 ; Liu et al . , 2022 ) emulate gold - standard summaries in their training datasets . In contrast , prompt - based GPT3 - D2 mod - els generate summaries based on how the given Mortgage rates have fallen slightly in the past few weeks , but they are still well above last year ' s levels . This is making it difficult for many prospective buyers to afford a home . The Fed has been raising interest rates in an effort to control inflation , and more rate hikes are expected . CNN Article : ( CNN ) Mortgage rates fell slightly this week , marking the third consecutive week of declines . But with rates hovering above 5 % and home prices well above where they were this time last year , prospective buyers are finding it increasingly difficult to afford a home . The 30 - year , fixed - rate mortgage averaged 5 . 09 % in the week ending June 2 , down from 5 . 10 % the week before , according to Freddie Mac . It is still well above the 2 . 99 % average from this time last year . [ … ] Rising prices had already been pushing many prospective buyers to the sidelines . [ … ] The Fed has been seeking to tame inflation by raising interest rates over the past couple of months . And the central bank has signaled there are more rate hikes to come . [ … ] The 30 - year , fixed - rate mortgage averaged 5 . 09 % in the week ending June 2 . It ' s the third consecutive week of declines . But rates are still above 5 % and home prices are well above where they were this time last year . Rising prices have pushed some prospective buyers to the sidelines . 30 - year , fixed - rate mortgage averaged 5 . 09 % in the week ending June 2 . It is still well above the 2 . 99 % average from this time last year . Rising prices had already been pushing many prospective buyers to the sidelines . BBC Article : The full opening of a landmark shared education campus in Omagh is facing another delay . Education Minister Michelle McIlveen has now said the Strule Shared Education Campus is planned to open in September 2026 . The minister clarified the new date in response to an assembly question from the SDLP MLA Daniel McCrossan . The campus had originally been due to open in 2020 , but has been delayed a number of times . [ … ] BRIO T0 GPT3 - D2 The Strule Shared Education Campus is facing another delay , and is now planned to open in September 2026 . The full opening of the Strule shared education campus in Omagh , County Tyrone , has been delayed to September 2026 . Strule , Northern Ireland ' s biggest ever school - building project , is now scheduled to open in 2026 , the Education Minister has said . BRIO T0 GPT3 - D2 Figure 4 : Examples of CNN - style and BBC / XSum - style summaries for the three systems . For CNN , we observe that models fine - tuned on the CNN / DM training set reflect its dataset biases ; summaries are highly extractive , specific and lead - biased . On the other hand , GPT3 - D2 summaries contain fewer specific details but cover more content . task description surfaces behavior learned during pre - training or instruction - tuning . In this section , we ask : how do these paradigms compare ? Does learning from gold summaries lead to a better sum - marization model ? To answer this , we conduct a human study to compare outputs of our 3 repre - sentative models and collect human preferences of quality . 3 . 1 Experimental Setup Datasets for fine - tuning We choose two stan - dard fine - tuning datasets whose summaries differ along multiple dimensions such as length and ab - stractiveness : 1 . CNN / DM ( Hermann et al . , 2015 ; Nallapati et al . , 2016 ) contains reference summaries that are approximately 3 - 4 sentences long . Sum - maries in this dataset are highly extractive and lead - biased . 2 . XSum ( Narayan et al . , 2018 ) contains 1 sen - tence summaries of BBC news articles . In this dataset , references summaries , and conse - quently generated summaries from fine - tuned models are highly abstractive . Datasets for evaluation Because GPT3 - D2 ’s pre - training and instruction - tuning datasets are un - known , it may have been trained on existing articles and summaries in the test splits of these standard benchmarks . We therefore run our human study on 100 recent articles from CNN 3 and BBC , collected between March 1 , 2022 and June 31 , 2022 . We call these CNN - 2022 and BBC - 2022 respectively . Model details We use the publicly released BRIO - XSum and BRIO - CNN / DM models to generate summaries . 4 For T0 , we use a prompt we selected from its prompt repository for CNN / DM and XSum datasets . 5 Finally , to generate GPT3 - D2 summaries , we set N = 3 for CNN and N = 1 for BBC in our standard sentence - count prompt template from Section 2 . For a maximally fair comparison in this “realis - tic” setting , we take some additional steps to im - prove the output of BRIO - XSum . In order to auto - mate dataset creation , XSum removes the first sen - tence from news articles to use as the gold summary for training , then treats the rest of the sentences as the article to summarize . This setup differs from the real world usage of summarization systems where the complete article is summarized . Due to this mismatch , BRIO - XSum often generates very low quality outputs , e . g . All images : Strule Shared 3 Although the BRIO ’s CNN / DM model also includes Daily - Mail data in its training , we do not use this news source in our study as it is now widely considered to be unreliable . E . g . according to Media Bias / Fact Check site , DM ’s factual re - porting is rated ‘low’ https : / / mediabiasfactcheck . com / daily - mail / . 4 Models at : https : / / github . com / yixinL7 / BRIO 5 Repository with T0 prompts : https : / / github . com / bigscience - workshop / promptsource Education Campus in Figure 4 , for around 30 % of the articles . We manually identify these examples and first attempt to fix them by selecting a summary without such obvious failures from further down the beam ( we use beam size = 10 ) . However , if we cannot find a “better” summary , we remove the first sentence of the article and re - sample a new sum - mary to align with its noisy training . This latter strategy often results in factually incorrect sum - mary generations , as is well documented in prior research ( Maynez et al . , 2020 ; Goyal and Durrett , 2021 ) . Design of the human study We design an A / B test to collect preference annotations . For each given article , annotators are shown summaries from all three summarization systems ( BRIO , T0 and GPT3 - D2 ) . They are then asked to select their most and least preferred summary or summaries . In ad - dition to these multiple choice questions , we also ask for a free - text justification of both choices . We make two design decisions for our human study : first , we do not provide annotators with spe - cific definitions of summary quality to avoid intro - ducing our own biases . It is also quite challenging to produce a unified definition of quality for the very different “styles” of summaries evaluated in this study . Instead , we ask them to rely on their own preferences based on summaries they would like to see if they were browsing the web , which we believe to be a representative scenario for non - expert consumers of news summaries . Detailed task instructions are included in Appendix F . Second , we allow multiple selections for both the best and worst summary questions to cater to sce - narios in which different summarization systems output similar quality summaries without meaning - ful differences . We hire crowd annotators through Prolific . For both CNN and BBC , we recruit 60 unique partici - pants to annotate the 100 summaries in each dataset . Each annotator was asked to annotate 5 articles and each article was annotated by 3 annotators . Addi - tionally , we use the Prolific’s demographic filters to restrict participation to USA ( or UK ) residents for CNN ( or BBC ) . We anticipate that residents from these respective countries are better positioned to understand country - specific news events and evalu - ate their summaries . Participants were paid approx - imately $ 11 / hr for their work . Model Length Statistics % novel n - gms # NEs per # sent # words / sent n = 1 n = 2 100 words CNN BRIO 3 . 7 15 . 8 12 . 1 36 . 2 12 . 9 T0 2 . 7 14 . 9 16 . 4 45 . 2 12 . 8 GPT3 - D2 2 . 9 23 . 4 16 . 3 40 . 7 10 . 5 BBC BRIO 1 . 0 20 . 2 24 . 6 61 . 2 9 . 1 T0 1 . 0 20 . 0 26 . 3 66 . 7 9 . 8 GPT3 - D2 1 . 0 27 . 7 16 . 4 42 . 3 8 . 5 Table 2 : Statistics for generated summaries evaluated in the human study across all datasets and summariza - tion systems . We observe that GPT3 - D2 generated sum - maries nearly always follow the sentence length con - straints in their prompts . 3 . 2 Results Differences between summarization systems Figure 4 shows examples of generated summaries from all three summarization systems for both CNN and BBC articles . For CNN , we observe that fine - tuned BRIO summaries tend to be highly extrac - tive and generally include a high number of named entities ( dates , percentages , names ) , reflecting the data it was trained on . In contrast , GPT3 - D2 sum - maries are more abstractive and less specific , but provide a more exhaustive overview of the article content . Table 2 provides quantitative evidence of this ; we use percentage of novel n - grams to mea - sure abstractiveness , and number of named entities per 100 words to measure specificity . For BBC , we observe inverse trends where BRIO and T0 are more abstractive compared to GPT3 - D2 . Again , this can be attributed to the XSum training data used to train both these prior mod - els . For GPT3 - D2 summaries , on the other hand , the level of abstractiveness does not differ between datasets . Finally , Table 2 shows that GPT3 - D2 sum - maries tend to have longer sentences , and therefore similar number of summary sentences often results in a longer summary for both datasets . We study the effect of this length difference on human pref - erence judgments in Appendix B . Which systems do humans prefer ? Results of our human study are summarized in Table 3 . We report the percentage of times a particular system is the most / least preferred model according to major - ity vote combining all three annotator’s choices . 6 6 As we allow multiple system selections , note that more that one system could be the majority . However , this is rare after majority vote : only 2 % of the articles in CNN and 7 % in Dataset B RIO T0 GPT3 Best ↑ Worst ↓ Best ↑ Worst ↓ Best ↑ Worst ↓ CNN 36 24 8 67 58 9 BBC 20 56 30 29 57 15 Table 3 : Percentage of times a summarization system is selected as the best or worst according to majority vote ( may be tied ) . Human annotators have a clear preference for GPT3 - D2 for both CNN and BBC style summaries . Across both datasets and styles , we observe a clear preference for GPT3 - D2 summaries compared to the other two models . In fact , in both scenarios , the GPT3 - D2 outperforms the next best model by at least 20 percentage points . This improvement is sta - tistically significant according to a paired bootstrap test ( CNN p − value = 2 × 10 − 3 , BBC p − value = 6 × 10 − 4 ) . Note that the next best model differs between the two datasets . For BBC , annotators prefer T0 sum - maries over BRIO . Annotator rationales often men - tioned misleading or incorrect information as the primarily reason for selecting BRIO as the worst summary , confirming the issues that have been ob - served with XSum - trained models ( Maynez et al . , 2020 ; Pagnoni et al . , 2021 ; Goyal and Durrett , 2021 ) . Although T0 also includes XSum training data , we hypothesize that its multi - task framework helps offset the noisy signal from XSum . In contrast , annotators rate T0 as the worst sum - marization system for CNN . The most common rationales for these were shorter length and inclu - sion of irrelevant details , e . g . long quotes , while missing key points . Some annotators also com - mented that these T0 summaries were less coherent compared to the other models . Interestingly , we did not observe similar complaints for the single - sentence T0 summaries for BBC . Do annotators agree with each other ? To study this , we plot the distribution of annotator votes for each summarization system and dataset in Figure 5 . Additionally , we report the inter - annotator agree - ment , measured using Krippendorff’s alpha with MASI distance ( Passonneau , 2006 ) , to account for multiple selections of best or worst summary al - lowed in our study design . The vote distribution shows that although more annotators prefer GPT3 - D2 summaries , this choice is only unanimous , i . e . supported by all three an - notators , for less that 30 % of the annotated articles . BBC have multiple best summaries . No . of annotator votes for “worst summary” 0 1 2 3 GPT3 BRIO T0 GPT3 BRIO T0 BB C No . of annotator votes for “best summary” 0 1 2 3 Which summary is the most preferred ? GPT3 BRIO T0 CNN Which summary is the least preferred ? GPT3 BRIO T0 Agreement = 0 . 05 Agreement = 0 . 11 Agreement = 0 . 18 Agreement = 0 . 15 Figure 5 : Annotator vote distribution for best and worst summaries across all datasets and models . Although GPT3 - D2 is the clear winner according to majority vote , this choice is unanimous for less than 30 % of the ar - ticles . This demonstrates the inherent variance in dif - ferent annotators’ definitions of “best summary” , espe - cially when comparing high - quality summaries from strong models . Conversely , although BRIO ( or T0 ) summaries are less preferred than GPT3 - D2 for the CNN ( or BBC ) dataset on aggregate , they were voted as the best summary by at least one annotator for more than 60 % of the articles . This demonstrate two things : first , when comparing summaries from two strong models , the choice is inherently ambiguous ( similar observations in Clark et al . ( 2021 ) ) . Second , these results and the diversity in the written rationales , show that there does not exist a universal definition of a “good” summary and that different summary properties appeal to different annotators . Regard - less , the aggregate preference for GPT3 - D2 is high enough across the board to give us confidence in its strength . How do these results impact the field ? Progress in text summarization research in the last five years has been enabled by the construction of large - scale text summarization datasets that involved scrap - ing news articles and pairing them with any avail - able summary - like data ( Hermann et al . , 2015 ; Narayan et al . , 2018 ; Grusky et al . , 2018 ) . The CNN / DM dataset considers bullet points accompa - nying news articles as its summary . These “gold” standard summaries provided useful training sig - nal to train impressive supervised models ( Lewis et al . , 2020 ; Zhang et al . , 2020 ; Liu et al . , 2022 ) and hence , their quality or alignment with human preferences was largely ignored . We found that , despite its popularity , XSum is largely unsuitable for fine - tuning models like BRIO Dataset Model Overlap - Based Similarity - Based QAEval ROUGE ( 1 / 2 / L ) M ETEOR B LEU BERTScore MoverScore EM F1 CNN PEGASUS 34 . 85 / 14 . 62 / 28 . 23 . 24 7 . 1 . 858 . 229 . 105 . 160 BRIO 38 . 49 / 17 . 08 / 31 . 44 . 31 6 . 6 . 864 . 261 . 137 . 211 T0 35 . 06 / 13 . 84 / 28 . 46 . 25 5 . 9 . 859 . 238 . 099 . 163 GPT3 - D2 31 . 86 / 11 . 31 / 24 . 71 . 25 3 . 8 . 858 . 216 . 098 . 159 DailyMail PEGASUS 45 . 77 / 23 . 00 / 36 . 65 . 33 12 . 2 . 865 . 308 . 159 . 229 BRIO 49 . 27 / 24 . 76 / 39 . 21 . 37 11 . 7 . 871 . 331 . 175 . 259 T0 42 . 97 / 19 . 04 / 33 . 95 . 28 8 . 9 . 863 . 290 . 121 . 184 GPT3 - D2 38 . 68 / 14 . 24 / 28 . 08 . 26 6 . 6 . 859 . 248 . 101 . 159 XSum PEGASUS 47 . 97 / 24 . 82 / 39 . 63 . 36 9 . 8 . 901 . 362 . 145 . 221 BRIO 49 . 66 / 25 . 97 / 41 . 04 . 39 10 . 6 . 901 . 372 . 139 . 224 T0 44 . 20 / 20 . 72 / 35 . 84 . 34 8 . 0 . 896 . 340 . 125 . 208 GPT3 - D2 28 . 78 / 7 . 64 / 20 . 60 . 19 2 . 2 . 869 . 197 . 066 . 119 Newsroom PEGASUS 39 . 21 / 27 . 73 / 35 . 68 . 39 . 14 . 873 . 272 0 . 182 0 . 253 BRIO - - - - - - - T0 25 . 64 / 9 . 49 / 21 . 41 . 20 . 04 . 849 . 145 . 080 0 . 125 GPT3 - D2 27 . 44 / 10 . 67 / 22 . 18 . 22 . 05 . 859 . 159 . 089 0 . 142 Table 4 : Performance of different summarization systems measured using reference - based automatic metrics . Across all datasets , we observe that automatic metrics report substantially worse results for GPT3 - D2 summaries compared to fine - tuned models . This directly contradicts the human preference results from Section 3 , demonstrating that these reference - based metrics cannot reliably compare the quality of prompt - based summaries against fine - tuned summaries . for realistic summarization settings . Even though a CNN / DM - trained BRIO model performed better , the results of our human study question the contin - ued utility of hill - climbing on this dataset , as it seems users may simply prefer a different style of summary altogether . In fact , this preference for GPT3 - D2 is much larger than incremental improve - ments reported in other human evaluation settings , e . g . improvements on XSum on the GENIE leader - board ( Khashabi et al . , 2022 ) . Furthermore , as we we will see in Section 5 , the greater flexibil - ity of GPT3 - D2 compared to these systems makes it more suitable for news summarization tasks be - yond generic summarization . If a system designer collects a large - scale dataset of high - quality summaries that they wish to emu - late , we believe a fine - tuned system may outper - form GPT3 - D2 . However , better - trained models on datasets collected via “incidental” supervision are less likely to help . 4 Can current automatic metrics evaluate GPT3 - D2 summaries ? Automatic metrics proposed for summarization evaluation can be broadly divided into two cate - gories : ( 1 ) reference - based , that compare gener - ated summaries against available gold summaries , and ( 2 ) reference - free that only rely on the input document . Here , we compare their performance at evaluating prompt - based GPT3 - D2 summaries . Experimental Setup We evaluate automatic met - rics using summaries from 4 different summariza - tion datasets , listed in Table 1 . For each dataset , we construct our evaluation sets by randomly sam - pling 500 7 articles from the standard test split . 8 We compare the same 3 summarization systems from Section 3 in our analysis . Additionally , we also report results using the fine - tuned PEGASUS model ( Zhang et al . , 2020 ) , as BRIO fine - tuned models are not available for all datasets . We publicly release this corpus of summariza - tion outputs to standardize the test sets and sup - port future research into GPT3 - D2 based summa - rization . Link : https : / / tagoyal . github . io / zeroshot - news - annotations . html . 4 . 1 Reference - based metrics Here , we study if the gold summaries of the stan - dard datasets are useful for evaluation , especially when evaluating prompt - based summaries that are not trained to emulate the gold . We benchmark 7 This size is chosen to give sufficient statistical power ( Card et al . , 2020 ) while keeping costs for GPT3 - D2 evaluation low to enable others to compare on this subset . We outline costs in Appendix D . 8 Note that these standard datasets were released before 2020 . Therefore , it is possible that some article - summary pairs in our test set overlap with GPT3 - D2 ’s training data . How - ever , we do not observe a qualitative difference in GPT3 - D2 ’s performance on these older articles . Dataset Model Overall Quality Factuality ( QA - based ) Factuality ( NLI - based ) SUPERT BLANC QuestEval QAFactEval FactCC DAE SummaC CNN PEGASUS . 5466 . 0605 . 7373 4 . 4071 . 3743 . 8223 . 1138 BRIO . 5586 . 0802 . 7334 3 . 8332 . 1817 . 7577 - . 0532 T0 . 5330 . 0558 . 7799 3 . 7517 . 2012 . 7556 - . 0605 GPT3 - D2 . 5560 . 0749 . 7249 3 . 6399 . 2428 . 6671 - . 0729 DailyMail PEGASUS . 6433 . 1137 . 7536 4 . 4677 . 5152 . 8497 . 2402 BRIO . 6360 . 1217 . 7415 4 . 1362 . 3699 . 8118 . 0153 T0 . 5995 . 0889 . 7803 3 . 9827 . 2431 . 8043 . 0478 GPT3 - D2 . 6118 . 0983 . 7461 3 . 8279 . 2697 . 6990 . 0365 XSum PEGASUS . 4439 . 0249 . 8233 2 . 0089 . 2465 . 3598 - . 2993 BRIO . 4459 . 0230 . 8305 1 . 8626 . 2031 . 3040 - . 3292 T0 . 4538 . 0238 . 7957 2 . 0330 . 2219 . 3392 - . 3037 GPT3 - D2 . 5060 . 0594 . 8064 2 . 9492 . 3977 . 6372 - . 2626 Newsroom PEGASUS . 6286 . 1131 . 7118 4 . 2120 . 7218 . 7956 . 2418 BRIO - - - - - - - T0 . 5433 . 0640 . 7511 3 . 5799 . 2828 . 7376 . 0261 GPT3 - D2 . 5408 . 0599 . 7160 3 . 2336 . 3988 . 6564 - . 0729 Table 5 : Performance of different summarization systems , as scored by automatic reference - free evaluation metrics from the summarization literature . Similar to reference - based metrics , these also generally fail to produce the same system rankings as human preferences reliably across datasets . the performance of 3 different summarization met - rics : ( 1 ) overlap - based metrics , specifically ROUGE ( Lin , 2004 ) METEOR ( Banerjee and Lavie , 2005 ) and BLEU ( Papineni et al . , 2002 ) . ( 2 ) similarity - based metrics , that compute similarity between embed - dings representations of generated and reference summaries . Specifically , we report BERTScore ( Zhang * et al . , 2020 ) and MoverScore ( Zhao et al . , 2019 ) . ( 3 ) a QA - based metric , specifically QAE - val ( Deutsch et al . , 2021 ) . Although most QA - metrics are reference - free ( discussed in Section 4 . 2 ) , QAEval uses the reference summaries to in - dicate saliency . We report both exact match ( EM ) and F1 components of QAEval . Results Table 4 outlines the results . It shows that BRIO and PEGASUS models , fine - tuned to emulate the reference summaries , outperform GPT3 - D2 sum - maries according to all reference - based automatic metrics . The difference in their assigned scores is very high , e . g . > 7 ROUGE - L points between GPT3 - D2 and BRIO . For comparison , these reported scores for GPT3 - D2 are even lower than the triv - ial Lead - 3 baseline reported in prior work ( Fabbri et al . , 2021 ; Grusky et al . , 2018 ) . This clearly demonstrates that current automatic reference - based metrics cannot be used to reliably mea - sure summary quality under the prompting paradigm . Amongst prompting - based models , we observe that T0 summaries report better metric scores than GPT3 - D2 for all datasets except Newsroom . Inter - estingly , out of the four datasets evaluated here , Newsroom is the only one not used to train the T0 model . This further shows that access to dataset - specific reference summaries during training im - proves performance according to these metrics , ren - dering them unsuitable for evaluating prompt - based models . 4 . 2 Reference - free metrics Next , we investigate whether current reference - free evaluation metrics reflect the human preference rankings between summarization systems , as ob - served in Section 3 . Here , we study 2 categories of metrics : ( 1 ) quality metrics , specifically SU - PERT ( Gao et al . , 2020 ) , which evaluates generated summaries against automatically identified salient sentences in the input , and BLANC ( Vasilyev et al . , 2020 ) , which evaluates summaries on language understanding tasks . We refer readers to the orig - inal papers for detailed explanation of these . ( 2 ) factuality metrics , that are evaluate whether gener - ated summaries contain incorrect information with respect to the source article . We report the perfor - mance of summarization systems using two QA - based metrics : QuestEval ( Scialom et al . , 2021 ) and QAFactEval ( Fabbri et al . , 2022 ) . Addition - ally , we also benchmark entailment - based metrics : FactCC ( Kryscinski et al . , 2020 ) , DAE ( Goyal and Durrett , 2020 , 2021 ) and SummaC ( Laban et al . , 2022 ) . 9 These entailment - based models are de - signed for classification into factual or non - factual ; therefore , we use P ( factual | article , summary ) to score generated summaries . Results Table 5 outlines the scores for each sum - marization system according to the above reference - free metrics . Ideally , we want the relative rankings of different systems according to these metrics to correspond to human preferences , i . e . GPT3 - D2 > BRIO > T0 for CNN / DM 10 and GPT3 - D2 > T0 > BRIO for XSum . 11 Overall , we observe that none of the reference - free metrics we evaluate follow these trends for both CNN / DM and XSum datasets . In particular , we observe that GPT3 - D2 summaries report low factu - ality scores ( except XSum ) even though we rarely found any factual errors in our qualitative analysis of its generated summaries . Interestingly , we noticed a roughly inverse rela - tion to abstractiveness ; summarization systems that generated more abstractive summaries ( see Table 2 ) were generally scored lower by all automatic reference - based metrics . For instance , GPT3 - D2 is scored lower than BRIO by both quality metrics for all datasets except XSum ; the latter is the only dataset for which GPT3 - D2 summaries are less ab - stractive . Such shortcomings of reference - free eval - uation metrics due to spurious correlations have also been studied in prior work ( Durmus et al . , 2022 ) . These issues become more exaggerated when the summarization systems being compared exhibit very different properties . Discussion On the surface , the failure of reference - free metrics at evaluating GPT3 - D2 sum - maries is more surprising that reference - based met - rics as the later explicitly compares generated sum - maries with references that GPT3 - D2 is not trained to imitate . Therefore , GPT3 - D2 understandably scores lower than fine - tuned systems . However , we note two different issues with reference - free metrics : ( 1 ) Some of these , e . g . FactCC and DAE , use reference summaries as pos - itive examples to train the metric . Therefore , al - 9 Exact model versions and configurations used for these are outlined in Appendix A . 10 Although the human study in Section 3 is only run on CNN articles , the underlying fine - tuned model is same for both CNN and DM . Therefore , it we can reasonably expect it to display similar quality differences with respect to GPT3 - D2 . 11 Note that while annotators were not explicitly asked to rate factuality , we instructed them to carefully check factuality and appropriately downvote non - factual summaries . though “reference - free” at test time , they are still trained to reward the summary properties seen in the standard summarization benchmarks . ( 2 ) Even completely reference - free metrics , e . g . QuestE - val and QAFactEval , have only been evaluated on reference - based benchmarks and fine - tuned mod - els . Therefore , the choice of different components , such as question answering or question generation models to use , etc . has been dictated by the error space of prior fine - tuned models ( Tang et al . , 2023 ) . These decisions also now need to be re - visited to incorporate GPT3 - D2 evaluation ; we leave this for future work . 5 Beyond Generic Summarization Previously , we observed that GPT3 - D2 models faith - fully follow simple “style” instructions in the given prompts . This provides a promising direction to tackle other use cases in news summarization be - yond the generic summarization task from Sec - tion 3 . Different users can have very different infor - mation needs from the same article , all of which cannot be satisfied with a single generic summary . Prior work has introduced several task formulations to address this gap , including keyword - focused ( He et al . , 2022a ) , query - focused ( Baumel et al . , 2014 ; He et al . , 2022a ) , or aspect - focused summariza - tion ( Krishna and Srinivasan , 2018 ; Ahuja et al . , 2022 ) , amongst others . Here , we evaluate GPT3 - D2 performance at two of these use cases . In keyword - based summarization , the output summaries must succinctly summarize the input document focusing on a given keyword ; these gen - erally correspond to specific entities or events di - rectly mentioned in the document . In contrast , the control units in aspect - based summarization are high - level topics that can be common across mul - tiple similar types of documents . For e . g . , for the input article in Figure 1 , Donald Trump or Russian interference in 2016 elections are keyword controls whereas charges against the defendants is a higher - level aspect that can serve as the query for any news article discussing a lawsuit or investigation . 5 . 1 Qualitative Analysis Baseline Model for comparison We use the re - cently proposed CTRLSum ( He et al . , 2022a ) , a fine - tuned BART model , as our baseline . It can be flex - ibly adapted for both keyword - and aspect - based settings by including a prompt as additional input Article : Republican defenders of Donald Trump won ' t or can ' t answer the questions that are at the root of the intrigue over why classified documents were at Mar - a - Lago and the troubling question of whether national security was put at risk . [ … ] At one end of the scale is Missouri Sen . Josh Hawley , a firm Trump backer who is often mentioned as a future presidential candidate once the ex - President finally exits the political stage . Hawley promoted Trump ' s line that the key issue was the FBI search of an ex - President ' s home , not what he might have kept there . " I don ' t know what he has or doesn ' t have , but what I have a lot of concerns about is the FBI raiding the home of the likely presidential nominee in 2024 , the former President of the United States , and particularly as we learn more and more about what ' s going on with the FBI , " Hawley told CNN ' s Manu Raju on Wednesday . [ … ] Ex - Attorney General William Barr has repeatedly questioned publicly what his former boss was doing with classified material . There has been speculation that Trump might want such material as keepsakes or as some kind of leverage in a future business deal . But none of this is informed given the secrecy sounding the DOJ ' s investigation . [ … ] Link : https : / / www . cnn . com / 2022 / 09 / 08 / politics / republicans - trump - national - security - implications - classified - docs Keyword - based Summarize the above article in 2 - 3 sentences focusing on William Barr . GPT3 - D2 : The article discusses how various Republicans have been defending Donald Trump in light of the recent revelations that he had classified documents at his Mar - a - Lago resort . While some have downplayed the significance of the situation , others have outright denied that there is any cause for concern . However , former Attorney General William Barr has repeatedly questioned Trump ' s actions , speculating that he may have been trying to use the classified information for personal gain . CTRLSum : Sen . Marco Rubio downplays the seriousness of the Department of Justice ' s investigation . Ex - Attorney General William Barr has repeatedly questioned what his former boss was doing with classified material . Trump and his allies have filled the vacuum for answers with a smokescreen of unproven claims , including that Trump declassified the material . Aspect - based In the above article , who is a defendant or under investigation ? In the above article , what is the defendant ' s reaction to charges , including admission of guilt , denial of charges , or explanations ? GPT3 - D2 : The defendant ' s reaction to charges in the above article is denial of charges . Trump and his allies have filled the vacuum for answers with a smokescreen of unproven claims , including that Trump declassified the material or that it might have been planted by the FBI in order to persecute him . CTRLSum : Republican defenders of Donald Trump won ' t or can ' t answer the questions that are at the root of the intrigue over why classified documents were at Mar - a - Lago and the troubling question of whether national security was put at risk . In the article , who is a defendant or under investigation ? GPT3 - D2 : Donald Trump . The FBI . The Department of Justice . Sandy Berger . CTRLSum : Republican defenders of Donald Trump won ' t or can ' t answer the questions that are at the root of the intrigue over why classified documents were at Mar - a - Lago , says Julian Zelizer . Zelizer : Trump and his allies have filled the vacuum for answers with a smokescreen of unproven claims , including that Trump declassified the material or it might have been planted by the FBI . Missing context Poor discourse Includes article context before keyword - specific information Does not directly address aspect . Repeats the prompt at the end . Incorrectly answers the query . Does not directly answer query . Summary captures aspect - relevant content . Figure 6 : Comparison of keyword - and aspect - based summaries using GPT3 - D2 and CTRLSum models . The GPT3 - D2 prompt is shown on the left with the corresponding keyword or aspect bolded . For keyword - based summarization , the GPT3 - D2 summary presents appropriate context before the keyword - specific information . How - ever , for aspect - based summarization , it does not always generate factually correct summaries , as shown in the first aspect example . We observe that CTRLSum performs poorly for both these settings . to the encoder . We use the prompt template recom - mended in the original paper . 12 Control Units For the keyword - focused setting , we use named entities extracted from the input arti - cle as the control units . For aspect - focused summa - rization , we directly use the aspects introduced in the guided summarization task from TAC 2011 . 13 It defined 5 broad categories of newswire articles , such as accidents and natural disasters , investiga - tions and trial , etc . , and multiple aspects for each category . For example , the “investigations and tri - als” category includes aspects such as “who is the defendant or under trial ? ” , “who is investigating , prosecuting , judging ? ” , and so on . Qualitative Analysis Figure 6 shows examples of keyword - and aspect - focused summaries using GPT3 - D2 and the baseline CTRLSum model . The keywords or aspects are highlighted in bold within the GPT3 - D2 prompt displayed on the left . 12 Trained model publicly released at : https : / / github . com / salesforce / ctrl - sum . 13 https : / / tac . nist . gov / 2011 / Summarization / Guided - Summ . 2011 . guidelines . html In this example , representative of aver - age GPT3 - D2 quality , the keyword - focused GPT3 - D2 summary first gives a brief overview of the article setting before providing keyword - relevant information . In contrast , the CTRLSum summary exhibits poor discourse structure and reads like a list of facts stapled together . The figure also shows aspect - focused summaries for two aspects associated with the “investigations and trial” category most appropriate for the chosen article . We see mixed results here for GPT3 - D2 ; it generates a factually incorrect summary for the first aspect , listing multiple people from the input arti - cle as defendants instead of only “Donald Trump” . For the second aspect , it correctly maps the high - level concept “defendant” to “Donald Trump” in the input article and generates the correct answer to the input query : “The defendant’s reaction to charges in the above article is denial of charges” . On the other hand , CTRLSum fails to generate aspect - focused summaries for both cases . We be - lieve that it struggles to align high - level concepts and explicit entities in the article due to a lack of No . of votes for “best summary” 0 1 2 3 GPT3 - D2 CTRLSum Which keyword - focused summary is better ? Win % according to majority vote 69 . 8 % 30 . 2 % Figure 7 : Distribution of annotator votes for the keyword - focused summarization task . Annotators pre - fer GPT3 - D2 summaries over CTRLSum for approxi - mately 70 % of all article - keyword pairs , showing unani - mous preference more than half the time . such aspect - specific examples in its training data . Instead , it generates summaries focusing on lexi - cally similar words , i . e . “defenders” for both cases . Based off of GPT3 - D2 ’s promising keyword - focused summarization capabilities observed above , we next conduct a human study to system - atically compare it against the CTRLSum baseline . We leave further explorations of aspect - based sum - marization to future work , given the mixed to poor results for both models at this task . 5 . 2 Human Study : Keyword - focused summarization Task Setup Similar to Section 3 , we design an A / B test to compare the two models . We use the same set of 100 CNN 14 articles as Section 3 . We randomly extract 2 distinct named entities from each article . In the study interface , the annota - tor is shown the article - keyword pair and GPT3 - D2 and CTRLSum summaries corresponding to it . They are asked to select the summary that best summa - rizes the input article while focusing on the given keyword . Exact task instructions are included in Appendix F . Again , we run this study using the Prolific plat - form . We recruit 60 participants to annotate the 100 articles ; each article is annotated by 3 anno - tators which includes annotations for 2 separate keywords . Each annotator evaluates 5 articles . Results Figure 7 shows the distribution of an - notator votes between the GPT3 - D2 and CTRLSum models . Annotators show a clear preference for GPT3 - D2 . In fact , for nearly 70 % of all article - keyword pairs , GPT3 - D2 is preferred over CTRLSum 14 We run this study using only CNN articles as the baseline CTRLSum model is trained on CNN / DM . by a majority of the annotators . The main ratio - nales given for this choice were better contextual - ization of keyword - related information and better coherence in GPT3 - D2 summaries . Impact These results show that prompting GPT - 3 models present a promising alternative to fine - tuned models for such specialized summarization tasks that can be easily described using textual prompts . One of the major drawbacks of fine - tuned models is that they are constrained by what data is available and how it can be transformed to cre - ate new task - specific training data . CTRLSum relied on the SQuAD question answering dataset ( Ra - jpurkar et al . , 2016 ) because the required “queries” or “questions” were unavailable at scale for sum - maries in standard summarization datasets . In con - trast , prompt - based models are not constrained by the availability of task - specific data and can flexibly adapt to new tasks . Future research should focus on further exploring these capabilities and possible improvements on currently “unsolved” tasks such as aspect - based or plan - based summarization . 6 Discussion and Related Work In recent years , research in text summarization ( Rush et al . , 2015 ; Nallapati et al . , 2016 ; See et al . , 2017 ; Lewis et al . , 2020 ; Zhang et al . , 2020 ; Liu et al . , 2022 ) has typically relied on comparisons with gold test sets for evaluation , possibly aug - mented with reference - free metrics for dimensions like factuality . This paper shows that all these metrics are completely ineffective at evaluating GPT - 3 summaries . Although issues with these metrics , particularly low correlation with human judgments , have also been studied earlier ( Fabbri et al . , 2021 ; Deutsch and Roth , 2021 ) , they are considered reliable when comparing systems in dif - ferent score ranges ( Peyrard , 2019 ; Deutsch et al . , 2022 ) . However , GPT - 3 challenges these estab - lished practices and evaluation protocols , and poses an urgent need for better evaluation . This brings us to manual evaluation , generally considered to be the gold standard for generation evaluation . The majority of summarization re - search now reports results from a human study in addition to automatic metrics , but there is a general lack of consensus on what dimensions to evalu - ate , task design , and other factors ( Hardy et al . , 2019 ) . This presents difficulties in conducting re - liable and reproducible comparisons between sys - tems ( Karpinska et al . , 2021 ) , another factor con - tributing to the popularity of automatic metrics . Although recent efforts like GENIE ( Khashabi et al . , 2022 ) have taken steps to standardize manual eval - uation protocols across systems , its annotation is not universally affordable and the quality is not strictly monitored . We hope that future work ad - dresses these challenges and democratizes human evaluations . The ultimate test of summarization systems is with actual users using the systems in practice . Jones ( 2007 ) discusses the need to align task formu - lations with actual applications scenarios ( “purpose factors” ) . However , the research in text summa - rization until now has been constrained to certain problems or domains by the heavy dependence on large - scale training data : for example , producing a bullet - point summary of a news article has emerged as standard due to availability of data from CNN , not because it is shown to be the best way to present information . Now , the success of prompt - based models can allow realistic use - cases to drive research in a more top - down way . We already show that GPT3 - D2 im - proves upon prior keyword - focused summarization systems that were trained on artificially adapted training data . In future research , we are inter - ested in tackling other real world use cases , such as update summarization and plan - or aspect - based summarization . Additionally , adapting GPT3 - D2 to documents longer than the allowed context , or structured inputs such as tables , presents research challenges beyond the current capabilities of GPT - 3 and would be interesting to study . 15 7 Conclusion In this work , we performed the first systematic study comparing prompt - based GPT - 3 and fine - tuned models at the news summarization task . We analyzed the impact of prompting on the summa - rization field , including training paradigms and evaluation practices . Finally , to support further research in this direction , we release a large corpus of generated summaries for multiple prompt - based and fine - tuned models , as well as human preference judgments comparing these systems . 8 Limitations In the text generation evaluation literature , there does not exist a standardized task design for com - 15 We very briefly discuss long document summarization with GPT - 3 in Appendix E . paring different system generations . In our work , we chose a human evaluation workflow that directly asks annotators to compare systems , while other prior work has opted for Likert - scale judgments and / or evaluation along multiple quality dimen - sions ( Gehrmann et al . , 2022 ) . The latter strategy of evaluating different dimensions could surface more insights into which “style” properties of GPT - 3 summaries provide them an edge over fine - tuned models ; however , such analysis is outside the scope of this paper . Our experiments comparing overall quality reveal that current summarization datasets are not well - aligned with user preferences . We leave more fine - grained analysis into these prefer - ence judgments for future work . The experiments in this paper are run on English - language news summarization datasets as these serve as common benchmarks in the summariza - tion literature . However , user rankings of system outputs might be different when evaluating other domains , e . g . , summaries of scientific text . While we believe that automatic metrics would fail to eval - uate GPT - 3 summaries on these domains also ( gen - erated summaries would still look different from the reference summaries ) , users may prefer models that are specifically fine - tuned on domain - specific data for niche domains . Finally , we do not know exact datasets or tasks used to train GPT3 - D2 . It is possible that its RLHF training ( Ouyang et al . , 2022 ) included summariza - tion examples , and therefore , preference judgments from human annotators for its different outputs . However , our arguments in this paper do not rely on the specifics of the GPT3 - D2 system , merely that such a system exists . If anything , the existence of potentially better data underscores that further work should collect new data for summarization model tuning , and our claims about metrics still hold regardless of the details of how the GPT3 - D2 summaries were produced . References Ojas Ahuja , Jiacheng Xu , Akshay Gupta , Kevin Horecka , and Greg Durrett . 2022 . ASPECTNEWS : Aspect - oriented summarization of news documents . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 6494 – 6506 . Satanjeev Banerjee and Alon Lavie . 2005 . METEOR : An automatic metric for mt evaluation with improved correlation with human judgments . In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and / or summariza - tion , pages 65 – 72 . Tal Baumel , Raphael Cohen , and Michael Elhadad . 2014 . Query - chain focused summarization . In Pro - ceedings of the 52nd Annual Meeting of the Associa - tion for Computational Linguistics ( Volume 1 : Long Papers ) , pages 913 – 922 . Manik Bhandari , Pranav Narayan Gour , Atabak Ash - faq , and Pengfei Liu . 2020 . Metrics also disagree in the low scoring range : Revisiting summarization evaluation metrics . In Proceedings of the 28th Inter - national Conference on Computational Linguistics , pages 5702 – 5711 . Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . Language models are few - shot learners . Advances in neural information processing systems , 33 : 1877 – 1901 . Dallas Card , Peter Henderson , Urvashi Khandelwal , Robin Jia , Kyle Mahowald , and Dan Jurafsky . 2020 . With little power comes great responsibility . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 9263 – 9274 . Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Sebastian Gehrmann , et al . 2022 . PaLM : Scaling language modeling with pathways . arXiv preprint arXiv : 2204 . 02311 . Elizabeth Clark , Tal August , Sofia Serrano , Nikita Haduong , Suchin Gururangan , and Noah A Smith . 2021 . All that’s ‘human’ is not gold : Evaluating human evaluation of generated text . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 7282 – 7296 . Arman Cohan , Franck Dernoncourt , Doo Soon Kim , Trung Bui , Seokhwan Kim , Walter Chang , and Nazli Goharian . 2018 . A discourse - aware attention model for abstractive summarization of long documents . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 615 – 621 , New Or - leans , Louisiana . Association for Computational Lin - guistics . Daniel Deutsch , Tania Bedrax - Weiss , and Dan Roth . 2021 . Towards question - answering as an automatic metric for evaluating the content quality of a sum - mary . Transactions of the Association for Computa - tional Linguistics , 9 : 774 – 789 . Daniel Deutsch , Rotem Dror , and Dan Roth . 2022 . Re - examining system - level correlations of automatic summarization evaluation metrics . In Proceedings of the 2022 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies , pages 6038 – 6052 , Seattle , United States . Association for Computational Linguistics . Daniel Deutsch and Dan Roth . 2021 . Understanding the extent to which content quality metrics measure the information quality of summaries . In Proceedings of the 25th Conference on Computational Natural Language Learning , pages 300 – 309 . Esin Durmus , He He , and Mona Diab . 2020 . FEQA : A question answering evaluation framework for faith - fulness assessment in abstractive summarization . In Proceedings of the 58th Annual Meeting of the Asso - ciation for Computational Linguistics , pages 5055 – 5070 . Esin Durmus , Faisal Ladhak , and Tatsunori B Hashimoto . 2022 . Spurious correlations in reference - free evaluation of text generation . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1443 – 1454 . Alexander Fabbri , Chien - Sheng Wu , Wenhao Liu , and Caiming Xiong . 2022 . QAFactEval : Improved QA - based factual consistency evaluation for summariza - tion . In Proceedings of the 2022 Conference of the North American Chapter of the Association for Com - putational Linguistics : Human Language Technolo - gies , pages 2587 – 2601 , Seattle , United States . Asso - ciation for Computational Linguistics . Alexander R Fabbri , Wojciech Kryscinski , Bryan Mc - Cann , Caiming Xiong , Richard Socher , and Dragomir Radev . 2021 . SummEval : Re - evaluating summariza - tion evaluation . Transactions of the Association for Computational Linguistics , 9 : 391 – 409 . Yang Gao , Wei Zhao , and Steffen Eger . 2020 . SUPERT : Towards new frontiers in unsupervised evaluation metrics for multi - document summarization . In Pro - ceedings of the 58th Annual Meeting of the Asso - ciation for Computational Linguistics , pages 1347 – 1354 . Sebastian Gehrmann , Elizabeth Clark , and Thibault Sel - lam . 2022 . Repairing the cracked foundation : A sur - vey of obstacles in evaluation practices for generated text . arXiv preprint arXiv : 2202 . 06935 . Tanya Goyal and Greg Durrett . 2020 . Evaluating factu - ality in generation with dependency - level entailment . In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3592 – 3603 . Tanya Goyal and Greg Durrett . 2021 . Annotating and modeling fine - grained factuality in summarization . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies , pages 1449 – 1462 . Tanya Goyal , Jiacheng Xu , Junyi Jessy Li , and Greg Durrett . 2022 . Training dynamics for text summa - rization models . In Findings of the Association for Computational Linguistics : ACL 2022 , pages 2061 – 2073 . Max Grusky , Mor Naaman , and Yoav Artzi . 2018 . Newsroom : A dataset of 1 . 3 million summaries with diverse extractive strategies . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Hu - man Language Technologies , Volume 1 ( Long Pa - pers ) , pages 708 – 719 . Hardy Hardy , Shashi Narayan , and Andreas Vlachos . 2019 . Highres : Highlight - based reference - less evalu - ation of summarization . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3381 – 3392 . Junxian He , Wojciech Kryscinski , Bryan McCann , Nazneen Rajani , and Caiming Xiong . 2022a . CTRL - sum : Towards generic controllable text summariza - tion . In Proceedings of the 2022 Conference on Em - pirical Methods in Natural Language Processing , pages 5879 – 5915 , Abu Dhabi , United Arab Emirates . Association for Computational Linguistics . Pengcheng He , Baolin Peng , Liyang Lu , Song Wang , Jie Mei , Yang Liu , Ruochen Xu , Hany Hassan Awadalla , Yu Shi , Chenguang Zhu , et al . 2022b . Z - Code + + : A pre - trained language model optimized for abstractive summarization . arXiv preprint arXiv : 2208 . 09770 . Karl Moritz Hermann , Tomas Kocisky , Edward Grefen - stette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom . 2015 . Teaching machines to read and comprehend . Advances in Neural Information Processing Systems , 28 . Karen Spärck Jones . 2007 . Automatic summarising : The state of the art . Information Processing & Man - agement , 43 ( 6 ) : 1449 – 1481 . Marzena Karpinska , Nader Akoury , and Mohit Iyyer . 2021 . The perils of using mechanical turk to evaluate open - ended text generation . In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 1265 – 1285 . Daniel Khashabi , Gabriel Stanovsky , Jonathan Bragg , Nicholas Lourie , Jungo Kasai , Yejin Choi , Noah A . Smith , and Daniel Weld . 2022 . GENIE : Toward re - producible and standardized human evaluation for text generation . In Proceedings of the 2022 Con - ference on Empirical Methods in Natural Language Processing , pages 11444 – 11458 , Abu Dhabi , United Arab Emirates . Association for Computational Lin - guistics . Kundan Krishna and Balaji Vasan Srinivasan . 2018 . Generating topic - oriented summaries using neural attention . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , Volume 1 ( Long Papers ) , pages 1697 – 1705 . Wojciech Kryscinski , Bryan McCann , Caiming Xiong , and Richard Socher . 2020 . Evaluating the factual consistency of abstractive text summarization . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 9332 – 9346 . Wojciech Kryscinski , Nazneen Fatema Rajani , Di - vyansh Agarwal , Caiming Xiong , and Dragomir R Radev . 2021 . BookSum : A collection of datasets for long - form narrative summarization . Philippe Laban , Tobias Schnabel , Paul N . Bennett , and Marti A . Hearst . 2022 . SummaC : Re - visiting NLI - based models for inconsistency detection in summa - rization . Transactions of the Association for Compu - tational Linguistics , 10 . Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2020 . BART : Denoising sequence - to - sequence pre - training for natural language generation , translation , and com - prehension . In Proceedings of the 58th Annual Meet - ing of the Association for Computational Linguistics , pages 7871 – 7880 . Chin - Yew Lin . 2004 . ROUGE : A package for auto - matic evaluation of summaries . In Text summariza - tion branches out , pages 74 – 81 . Yixin Liu , Pengfei Liu , Dragomir Radev , and Graham Neubig . 2022 . BRIO : Bringing order to abstractive summarization . In Proceedings of the 60th Annual Meeting of the Association for Computational Lin - guistics ( Volume 1 : Long Papers ) , pages 2890 – 2903 . Joshua Maynez , Shashi Narayan , Bernd Bohnet , and Ryan McDonald . 2020 . On Faithfulness and Factu - ality in Abstractive Summarization . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1906 – 1919 . Sewon Min , Xinxi Lyu , Ari Holtzman , Mikel Artetxe , Mike Lewis , Hannaneh Hajishirzi , and Luke Zettle - moyer . 2022 . Rethinking the role of demonstrations : What makes in - context learning work ? In Proceed - ings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 11048 – 11064 , Abu Dhabi , United Arab Emirates . Association for Computational Linguistics . Swaroop Mishra , Daniel Khashabi , Chitta Baral , and Hannaneh Hajishirzi . 2022 . Cross - task generaliza - tion via natural language crowdsourcing instructions . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 3470 – 3487 . Ramesh Nallapati , Bowen Zhou , Cicero dos Santos , Caglar Gulcehre , and Bing Xiang . 2016 . Abstrac - tive text summarization using sequence - to - sequence RNNs and beyond . In Proceedings of The 20th SIGNLL Conference on Computational Natural Lan - guage Learning , pages 280 – 290 . Shashi Narayan , Shay B Cohen , and Mirella Lapata . 2018 . Don’t Give Me the Details , Just the Summary ! Topic - Aware Convolutional Neural Networks for Ex - treme Summarization . In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan - guage Processing , pages 1797 – 1807 . Long Ouyang , Jeff Wu , Xu Jiang , Diogo Almeida , Car - roll L Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , et al . 2022 . Training language models to follow in - structions with human feedback . arXiv preprint arXiv : 2203 . 02155 . Artidoro Pagnoni , Vidhisha Balachandran , and Yulia Tsvetkov . 2021 . Understanding factuality in abstrac - tive summarization with FRANK : A benchmark for factuality metrics . In Proceedings of the 2021 Con - ference of the North American Chapter of the Asso - ciation for Computational Linguistics : Human Lan - guage Technologies , pages 4812 – 4829 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . BLEU : a method for automatic eval - uation of machine translation . In Proceedings of the 40th annual meeting of the Association for Computa - tional Linguistics , pages 311 – 318 . Rebecca J Passonneau . 2006 . Measuring agreement on set - valued items ( MASI ) for semantic and pragmatic annotation . In Proceedings of the Fifth International Conference on Language Resources and Evaluation ( LREC’06 ) . Maxime Peyrard . 2019 . Studying summarization eval - uation metrics in the appropriate scoring range . In Proceedings of the 57th Annual Meeting of the Asso - ciation for Computational Linguistics , pages 5093 – 5100 . Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , Peter J Liu , et al . 2020 . Exploring the limits of transfer learning with a unified text - to - text trans - former . J . Mach . Learn . Res . , 21 ( 140 ) : 1 – 67 . Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang . 2016 . SQuAD : 100 , 000 + questions for machine comprehension of text . In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383 – 2392 . Alexander M Rush , Sumit Chopra , and Jason Weston . 2015 . A neural attention model for abstractive sen - tence summarization . In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan - guage Processing , pages 379 – 389 . Victor Sanh , Albert Webson , Colin Raffel , Stephen Bach , Lintang Sutawika , Zaid Alyafeai , Antoine Chaffin , Arnaud Stiegler , Teven Le Scao , Arun Raja , et al . 2022 . Multitask prompted training enables zero - shot task generalization . In The Tenth International Conference on Learning Representations . William Saunders , Catherine Yeh , Jeff Wu , Steven Bills , Long Ouyang , Jonathan Ward , and Jan Leike . 2022 . Self - critiquing models for assisting human evaluators . arXiv preprint arXiv : 2206 . 05802 . Thomas Scialom , Paul - Alexis Dray , Sylvain Lamprier , Benjamin Piwowarski , Jacopo Staiano , Alex Wang , and Patrick Gallinari . 2021 . QuestEval : Summariza - tion asks for fact - based evaluation . In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6594 – 6604 . Abigail See , Peter J Liu , and Christopher D Manning . 2017 . Get to the point : Summarization with pointer - generator networks . In Proceedings of the 55th An - nual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1073 – 1083 . Liyan Tang , Tanya Goyal , Alexander R Fabbri , Philippe Laban , Jiacheng Xu , Semih Yahvuz , Wojciech Kry´s - ci´nski , Justin F Rousseau , and Greg Durrett . 2023 . Understanding factual errors in summarization : Er - rors , summarizers , datasets , error detectors . Associa - tion for Computational Linguistics . Oleg Vasilyev , Vedant Dharnidharka , and John Bohan - non . 2020 . Fill in the BLANC : Human - free quality estimation of document summaries . In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems , pages 11 – 20 . Jason Wei , Maarten Bosma , Vincent Zhao , Kelvin Guu , Adams Wei Yu , Brian Lester , Nan Du , Andrew M . Dai , and Quoc V Le . 2022 . Finetuned language mod - els are zero - shot learners . In International Confer - ence on Learning Representations . Xi Ye and Greg Durrett . 2022 . The unreliability of ex - planations in few - shot prompting for textual reason - ing . In Advances in Neural Information Processing Systems . Jingqing Zhang , Yao Zhao , Mohammad Saleh , and Peter Liu . 2020 . PEGASUS : Pre - training with extracted gap - sentences for abstractive summarization . In In - ternational Conference on Machine Learning , pages 11328 – 11339 . PMLR . Tianyi Zhang * , Varsha Kishore * , Felix Wu * , Kilian Q . Weinberger , and Yoav Artzi . 2020 . BERTScore : Evaluating Text Generation with BERT . In Inter - national Conference on Learning Representations . Yusen Zhang , Ansong Ni , Ziming Mao , Chen Henry Wu , Chenguang Zhu , Budhaditya Deb , Ahmed Awadal - lah , Dragomir Radev , and Rui Zhang . 2022 . SummN : A multi - stage summarization framework for long in - put dialogues and documents : A multi - stage sum - marization framework for long input dialogues and documents . In Proceedings of the 60th Annual Meet - ing of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1592 – 1604 . Yusen Zhang , Ansong Ni , Tao Yu , Rui Zhang , Chen - guang Zhu , Budhaditya Deb , Asli Celikyilmaz , Ahmed Hassan , and Dragomir Radev . 2021 . An exploratory study on long dialogue summarization : What works and what’s next . In Findings of the Asso - ciation for Computational Linguistics : EMNLP 2021 , pages 4426 – 4433 . Tony Z . Zhao , Eric Wallace , Shi Feng , Dan Klein , and Sameer Singh . 2021 . Calibrate before use : Improv - ing few - shot performance of language models . In Proceedings of the International Conference on Ma - chine Learning ( ICML ) . Wei Zhao , Maxime Peyrard , Fei Liu , Yang Gao , Chris - tian M Meyer , and Steffen Eger . 2019 . MoverScore : Text generation evaluating with contextualized em - beddings and earth mover distance . In Proceedings of the 2019 Conference on Empirical Methods in Nat - ural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 563 – 578 . Yao Zhao , Mohammad Saleh , and Peter J Liu . 2020 . SEAL : Segment - wise extractive - abstractive long - form text summarization . arXiv preprint arXiv : 2006 . 10213 . A Implementation Details Prompts Used To generate GPT3 - D2 summaries for all experiments in this paper , we use the stan - dard prompt format outlined in Section 2 . We set N = 3 for CNN and DailyMail , N = 2 for News - room , and N = 1 for XSum / BBC . For the latter , the prompt is slightly modified to “Summarize the above article briefly in 1 sentence . ” For T0 , we use the following prompts : a ) CNN / DM : “Summarize the article below in 3 to 4 sentences ? ” , b ) Newsroom : “Summarize the arti - cle below in 2 to 3 sentences ? ” , and c ) XSum / BBC : “Summarize the article below in 1 sentence ? ” Factuality Metrics In Section 4 . 2 , we evaluated several recently proposed factuality metrics . We note that multiple versions have been released for some of these models in recent years . Here , we specify the versions used in our experiments to ensure reproducibility of results : 1 . QuestEval : We use version 0 . 2 . 4 of the queste - val python package and report numbers using the precision - only setting . 2 . DAE : We use the updated version of the DAE model trained for document - level factuality . Latest code and model released at https : / / github . com / tagoyal / factuality - datasets . 40 20 0 20 40 60 Len ( s * ) - L ( s ) 3 2 1 0 1 2 3 S c o r e ( s * ) - S c o r e ( s ) Dataset CNN BBC Figure 8 : Correlation between summary length and annotator score ( computed as the no . of “best summary” votes . For each example , plot the difference in length ( x - axis ) and annotator score ( y - axis ) between the GPT3 - D2 summary and the next best system’s summary . 3 . SummaC : We use the SummaC - Conv model ( model _ name = ‘vitc’ ) and sentence - level gran - ularity in our experiments . Keyword - based data For our keyword - based hu - man study , we extracted two named entities per article , as discussed in Section 5 . In practice , we constrained the first keyword to be lead - biased , i . e . it was extracted from the first three sentences of the article , and the second keyword was extracted from the remaining article . As CNN - based summariza - tion models are generally lead - biased , this allowed us to benchmark models under both settings . B Are annotator judgments of quality correlated with length ? In Section 3 , results of the human study showed that annotators provide shorter length as one of the main reasons for selecting T0 summaries as the worst for the CNN dataset . Here , we investigate if the choice between GPT3 - D2 and BRIO is similarly influenced by their length differences ; GPT3 - D2 summaries are on average 9 words longer . To study this , we plot the difference in summary length against the difference in annotator score ( measured as the no . of votes for a summarization system ) between the best summarization system ( GPT3 - D2 ) and the next best system ( BRIO for CNN and T0 for BBC ) . The resulting plot is shown in Figure 8 . In general we observe low correlation between these ; Pearson’s ρ is 0 . 17 for CNN and . 02 for the BBC dataset . These correlation values can - not solely explain the large differences in annotator The article is about four men who have completed a world record by playing a board game continuously for more than 85 hours . Four men completed a world record by playing a board game continuously for more than 85 hours . Eight defendants have been charged with murder after two men died in a car crash . Eight people have been charged with murder after two men died in a car crash on a dual carriageway . GPT3 - D2 GPT3 - D1 GPT3 - D2 GPT3 - D1 BBC - style 1 sentence summaries CNN - style 3 sentence summaries The article discusses Russian president Vladimir Putin ' s speech on Victory Day , in which he blended history with the present and justified his war . It is speculated that Putin may use this day to escalate the conflict in Ukraine , but it is also noted that he has few successes in Ukraine to brag of . The question now is whether Putin will use this day to escalate the war in other ways . Russian President Vladimir Putin ' s speech at Russia ' s Victory Day commemorations offered few clues on the direction of the conflict in Ukraine . UK defense chief Ben Wallace had suggested that Putin may use this historic day to escalate his so - called " special military operation " in Ukraine and declare an outright war . Putin used his speech to blend history with the present , banking on Russian nationalism on its most patriotic of holidays to justify his war . GPT3 - D1 GPT3 - D2 Figure 9 : Examples of generated summaries using the text - davinci - 001 ( GPT3 - D1 ) and text - davinci - 002 ( GPT3 - D2 ) versions . The figure shows both BBC and CNN - style summaries . judgments reported in the human study results of Section 3 ; additional quality factors must have in - fluenced this choice . Anecdotally , we observe that the GPT summaries are slightly less information dense ; our impression is that these contain a similar level of information content , but are easier to read and understand despite being a bit more verbose . C Qualitative differences between GPT - 3 versions Figure 9 shows examples comparing summaries from text - davinci - 001 ( GPT3 - D1 ) to those from GPT3 - D2 . For BBC - style single sentence sum - maries , we observed that the two models generated very similar summaries with high content and lexi - cal overlap . More variance is observed for CNN - style summaries . In our anecdotal assessment , GPT3 - D1 generated more detailed summaries while those from GPT3 - D2 are less information dense . D Human study and API costs At the time of running our experiments , GPT - 3 API’s text - davinci - 002 version was priced at $ 0 . 06 per 1K tokens . New pricing information is avail - able at : https : / / openai . com / api / pricing / . In our experiments , we generated around 2600 GPT3 - D2 summaries across all experiments in Sec - tion 3 ( human study ) , Section 4 ( evaluation of met - rics ) and Section 5 ( keyword - based human study ) . Input Article : https : / / www . cnn . com / 2021 / 09 / 07 / opinions / covid - 19 - good - and - bad - news - ranney / index . html Summary for input segment 1 : The article discusses how the current situation with Covid - 19 cases rising again is similar to last year in some ways , but different in others . One big difference is that there are now vaccines available which decrease the risk of severe illness . Common prompt for all segments : Summary for input segment 2 : The article discusses how the current situation with Covid - 19 cases rising again is similar to last year in some ways , but different in others . One big difference is that there are now vaccines available which decrease the risk of severe illness . The article advises that everyone should take precautions such as wearing masks and opening windows to let fresh air in . Summary for input segment 3 : The article discusses the challenges of living through the pandemic and the importance of science and community . It also highlights the need for patience , hope , and humility as we enter the next phase of the pandemic . Summarize the above article briefly in 2 - 3 sentences . Figure 10 : Illustrative example of GPT3 - D2 summary of a long source article generated using the segment - then - summarize pipeline . We spent a total of approximately $ 150 on API requests . For the human study , we paid participants $ 4 per task ( each task involved annotation for 5 arti - cles ) . On average , this translated to $ 11 / hr of work . The combined cost for the generic summarization ( Section 3 ) and the keyword - based summarization ( Section 5 ) studies was $ 1020 , including platform costs and bonus payments . E Long document summarization using GPT3 - D2 Summarization of long documents has attracted significant interest in recent years ( Cohan et al . , 2018 ; Kryscinski et al . , 2021 ) . Here , we study how naive prompting of GPT - 3 performs at long - document summarization . First , we extract text from a long input article from the CNN website . 16 Next , we follow the com - monly used segment - then - summarize procedure from prior work ( Zhao et al . , 2020 ; Zhang et al . , 2022 ) . We divide the input article into 3 disjoint segments , summarize each segment separately and concatenate these outputs to form the final sum - mary . Figure 10 shows the prompt used and the gener - ated summaries for each segment . While individual segment summaries are high quality , we can see that the concatenated summary is not coherent and includes repeated “introductory” sentences outlin - 16 Article link : https : / / www . cnn . com / 2021 / 09 / 07 / opinions / covid - 19 - good - and - bad - news - ranney / index . html ing similar content . Related to this , it also does not cover all important aspects of the input arti - cle as a majority of its ‘length budget’ is spent on a high - level overview . We also observed that the generated summaries for long documents often fo - cus on less unimportant parts of the document , e . g . “ . . . everyone should take the precaution of . . . opening windows to let the fresh air in” in the illustrated ex - ample . This is , in part , due to the segmentation of the input article : GPT3 - D2 still exhibits some lead bias and treats the beginning of the input segment as more salient . Therefore , the exact segmentation of the article also dictates the quality of the final summary , and cannot be readily fixed by altering the prompt . These observations show that while GPT3 - D2 produces superior segment - level summaries , it is more difficult to adapt it to “non - natural” text in - puts without fine - tuning . Therefore , techniques that have shown promising results for fine - tuned mod - els , e . g . segment - then - summarize or extract - then - abstract ( Zhang et al . , 2021 ) approaches , are not as effective when directly applied with prompting - based models . F Task Instructions Task instructions provided to crowd annotators for the generic summarization task setting are shown in Figure 14 and those for the keyword - based setting are shown in Figure 15 . G Examples of generated summaries We show examples of generated summaries for arti - cles for generic summarization for CNN - 2022 and BBC - 2022 in Figures 11 and 12 . It includes sum - maries from the 3 different summarization models evaluated in the human study in Section 3 . Examples of keyword - focused summaries are shown in Figure 13 for CNN . It includes summaries generated by GPT3 - D2 and CTRLSum models . ( CNN ) Toronto Police fatally shot a man who was seen carrying a firearm near three schools in the Scarborough area Thursday , police said . Officers responded to reports of a man carrying a firearm and " there was an interaction " between officers and the man , according to the Special Investigations Unit in the Canadian province of Ontario . At some point during the interaction , two officers shot at the suspect , hitting him at least once , police said . The suspect was pronounced dead at the scene . At least four schools in the area were under lockdown Thursday as authorities tried to determine the extent of the threat , according to a series of tweets from the Toronto District School Board . The incident comes as the US is grappling with the shock of two recent mass shootings : its deadliest school shooting in nearly a decade in Uvalde , Texas , and a racist shooting at a supermarket in Buffalo , New York . " We certainly understand the trauma and how traumatic this must have been for staff , students and parents given the two recent events that have happened in the United States , " said Toronto Police Chief James Ramer at a news briefing after the incident , referencing the Uvalde and Buffalo shootings . He added that he understands the community ' s concern as the armed suspected was very close to schools . The schools under lockdown included William G . Davis Junior Public School , Joseph Howe Senior Public School , Charlottetown Junior Public School and Sir Oliver Mowat Collegiate Institute , according to a tweet from the school board . " I was really scared , I just started praying , " one student told CNN affiliate CTV . Ontario ' s Special Investigations Unit is now investigating the fatal shooting , according to a news release . " Four investigators and three forensic investigators have been assigned to the case , " the release said . Ontario premier Doug Ford tweeted his thanks to police and emergency services for their " quick action . " " Thank you to police and emergency services for your quick action today in Scarborough , " he said . " We ' re extremely grateful for everything you do to keep our communities safe . " BRIO : Suspect was seen carrying a firearm near three schools in the Scarborough area of Toronto , police say . At least four schools are under lockdown in the area . The incident comes as the U . S . is reeling from two recent mass shootings . The suspect was shot and killed by police . T0 : NEW : " I was really scared , I just started praying , " one student says . Shooting comes after two mass shootings in the United States . The gunman was spotted near three schools in the Scarborough area . GPT3 - D2 : The article discusses a police shooting that took place near several schools in Scarborough , Canada . The suspect was shot and killed by officers after an interaction during which the suspect was seen carrying a firearm . The incident is being investigated by the Special Investigations Unit . ( CNN ) The owner of seven nursing homes across Louisiana that evacuated residents to a warehouse as Hurricane Ida approached last year has been indicted on felony charges after seven residents died at the temporary shelter , officials said . Bob Glynn Dean was arrested and charged with eight felony counts of cruelty to persons with infirmities , five felony counts of Medicaid fraud and two felony counts of obstruction of justice , according to a Wednesday news release from Louisiana Attorney General Jeff Landry . Five of the seven deaths at the warehouse shelter were considered storm - related , state health officials said . In total , more than 800 residents were taken to the facility ahead of the storm . A joint investigation by the attorney general ' s Medicaid Fraud Control Unit ( MFCU ) and the Louisiana Bureau of Investigations ( LBI ) " revealed Dean refused to move his residents out of the warehouse following Hurricane Ida , billed Medicaid for dates his residents were not receiving proper care , and engaged in conduct intended to intimidate or obstruct public health officials and law enforcement , " the AG ' s news release said . Dean ' s attorney , John McClindon , told CNN Wednesday Dean plans to plead not guilty to all charges he ' s facing and said Dean ' s mental health will factor into the case . " I don ' t think it ' s any secret that Bob ' s mental health is going to be an issue in this case , " McClindon said . " Bob clearly has some cognitive impairments and did on the day of this incident . " McClindon said Landry informed him of an arrest warrant for Dean Monday and his client flew from Georgia to self - surrender . He made an initial court appearance in Louisiana Wednesday and was released on $ 350 , 000 bond , McClindon said . McClindon called the charges " very defensible " and said " the evidence will bear out eventually . " Residents kept in ' unsafe , unsanitary , and unhealthy ' conditions , officials said The nursing home residents were taken to the warehouse in Independence , about 57 miles east of Baton Rouge , ahead of Hurricane Ida ' s landfall on August 29 . The state health department said it soon started to hear about deteriorating conditions at the warehouse . CNN obtained the logs of 61 calls from the warehouse to 911 operators . At least 30 of the calls asked for assistance with medical episodes before and after landfall , including calls for seizures , stopped breathing , and one instance in which a caller says a diabetic patient needed transport because they had " not eaten due to them having no more supplies . " " Let ' s be clear ; there is no emergency - preparedness plan that allows for residents to be kept in such an unsafe , unsanitary , and unhealthy condition , " Stephen Russo , director of legal , audit and regulatory affairs for the health department , said last year . " The lack of adequate care for these residents is inhumane , and goes against the rules , regulations , and applicable statutes . " The seven facilities involved had their licenses revoked and cannot repatriate or admit residents , officials said at the time . The homes also had their Medicaid provider agreements terminated , the health department said . The Attorney General ' s Office investigation is ongoing and additional legal action may be filed in the future , the Wednesday release said . The next court date for Dean has not been set , but McClindon said it will most likely happen in the next 60 days . BRIO : Owner of seven nursing homes evacuated residents to a warehouse as Hurricane Ida approached last year . Seven residents died at the temporary shelter . Bob Glynn Dean ' s attorney says he plans to plead not guilty to all charges . Five of the seven deaths at the warehouse shelter were considered storm - related , officials say . T0 : Bob Glynn Dean faces eight felony counts of cruelty to persons with infirmities . Dean ' s attorney says his client ' s mental health will be an issue in the case . Seven nursing home residents died at the warehouse shelter during Hurricane Ida . GPT3 - D2 : The owner of seven nursing homes in Louisiana has been indicted on felony charges after seven residents died at a temporary shelter during Hurricane Ida . The owner , Bob Glynn Dean , is facing eight felony counts of cruelty to persons with infirmities , five felony counts of Medicaid fraud , and two felony counts of obstruction of justice . Dean ' s attorney says Dean ' s mental health will be a factor in the case . ( CNN ) Global leaders and defense officials had spent weeks speculating what Russian President Vladimir Putin might reveal about his Ukraine plans in a speech at Russia ' s Victory Day commemorations Monday . They ' ll have to keep guessing - - the leader offered few clues on the direction of the conflict . UK defense chief Ben Wallace had suggested that Putin may use this historic day to escalate his so - called " special military operation " in Ukraine and declare an outright war . Even if that had been Putin ' s plan , he was unlikely to follow through after Wallace ' s comments , not wanting to appear to his Western foes as such an easy nut to crack . Instead , the Russian president used his speech to blend history with the present , banking on Russian nationalism on its most patriotic of holidays to justify his war . In his reverence for Soviet war heroes who helped defeat Nazi Germany in World War II - - the reason Russia celebrates Victory Day - - Putin referred to new Nazi threats in Ukraine , repeating his baseless justification for the invasion as an operation to " denazify " the nation . In reference to the threat of NATO troops in Europe , Putin said : " Everything indicated that a clash with neo - Nazis , Banderites [ Ukrainian nationalists ] , on whom the United States and their younger partners counted on , would be inevitable . " " Danger was increasing every day . Russia repelled this aggression in a preventative way . This was the only correct decision , and it was a timely decision . The decision of an independent , sovereign and powerful nation , " he said . Putin had few other options than to use his speech to keep selling his war to his own people . He has so few successes in Ukraine to brag of , after all . All he can do now is to keep Russians on his side as they suffer the economic hardship of crippling sanctions and international isolationism . The question now is whether Putin will use this day - - or this week even - - to escalate the war in other ways . There are growing concerns that Russian forces will turn again to standoff weapons - - aerial strikes and long - range missiles , for example - - that can be fired from afar , as they so often do when they are on the backfoot . That ' s worrying , as those attacks are indiscriminate and tend to cause huge civilian tolls . A bombing of a school in Luhansk , eastern Ukraine , that is feared to have killed at least 60 sheltering people over the weekend is just one example . After Russia ' s failure to take territory in Ukraine ' s north and around the capital , Kyiv , it ' s struggling even in the east and south , where it has had a presence through pro - Russian rebels for years . The possibility that Russia may win nothing , or very little , in Ukraine is real . Whether anything changes this Victory Day or not , a new chapter in the war will inevitably need writing soon . BRIO : Russian President Vladimir Putin gave a speech at Russia ' s Victory Day commemorations on Monday . Peter Bergen says Putin gave few clues about his Ukraine plans in his speech . He says Putin used his speech to blend history with the present to justify his war in Ukraine . The question now is whether Putin will escalate the war in other ways , he says . T0 : Vladimir Putin offered few clues about the direction of the conflict in Ukraine . Putin used his speech to blend history and the present to justify his war . GPT3 - D2 : The article discusses Russian president Vladimir Putin ' s speech on Victory Day , in which he blended history with the present and justified his war . It is speculated that Putin may use this day to escalate the conflict in Ukraine , but it is also noted that he has few successes in Ukraine to brag of . The question now is whether Putin will use this day to escalate the war in other ways . Input Article Generated Summaries Figure 11 : Examples of generated summaries for the CNN - 2022 dataset using 3 different summarization systems . Four men have completed what they hope will be a world record by playing a board game continuously for more than 85 hours . The foursome , of Gloucester , completed their mammoth challenge on Monday night with just a couple of hours of sleep . Lea Poole , Dale Poole , Adam Bircher and Luke de Witt Vine played the game Dune , watched by supporters and an online audience . The world record attempt was carried out in aid of Alzheimer ' s Research UK . The rules of the challenge stated the men were allowed to accumulate five minutes respite for every hour they played . They had to beat the previous world record , held by a team in The Netherlands , which played a board game for 80 hours in January 2017 . Guinness World Records stipulated they would need to play for at least five more hours to be considered for a world record . The men ' s attempt has yet to be ratified . Alzheimer ' s Research UK was selected to benefit from the challenge as Lea and Dale Poole ' s father suffers from the condition . Dale Poole said : " It ' s a bit of a rollercoaster really . It ' s very flattering . People have been sending in their support and donations and it ' s really humbling . " The team were not allowed full meals , but just snacks throughout the attempt , which included 79 games in total . " You can be within one hour quite wide awake and alert , and by sitting down and not having to think for a few seconds , that ' s it , you ' re now tired and there ' s no way of breaking that weird cycle , " said Lea Poole . The men completed the challenge at 23 : 00 GMT on Monday but decided to carry on and finish the game they were on , sipping on champagne to celebrate . Mr Burcher said he was " tired , beyond measure " but " really quite satisfied and pleased with how far we ' ve come " . " If you ' d told me last year I ' d be doing something like this I would have called you crazy , " he added . Mr De Witt Vine said it had been a " difficult and trying experience but I ' m glad we ' ve done it . " The group have raised almost £1 , 000 for Alzheimer ' s UK , as well as a smaller amount for Herefordshire Mind , a mental health charity . BRIO : A team of four men have set a new world record by playing a board game for 90 hours . T0 : Four men have broken the world record for the longest time spent playing a board game . GPT3 - D2 : The article is about four men who have completed a world record by playing a board game continuously for more than 85 hours . A yellow warning for snow and ice has been issued for much of the North East . The Met Office warning is due to come into force at 20 : 00 BST on Wednesday and will run until 10 : 00 on Thursday . There will probably be icy patches on untreated roads and paths with higher elevation roads will be " affected by snowfall " , the Met Office said . Motorists are being urged to take care . The cold snap comes days after the region basked in warm sun and highs of 20C ( 68F ) . The weather warning will cover Northumberland , County Durham , Tyneside , Darlington and Teesside . The wintry weather is expected to last until the weekend when slightly warmer temperatures will come in from the west , bringing unsettled conditions . BRIO : Parts of the UK are set to be hit by icy conditions , forecasters have warned . T0 : The wintry weather is set to return to the North East of England on Wednesday night into Thursday morning . GPT3 - D2 : The Met Office has issued a warning for snow and ice in the North East , which is expected to last until the weekend . A man standing trial for the murder of his partner ' s three - year - old son had convictions for common assault , burglary and possession of heroin with intent to supply , a jury has heard . Kemarni Watson Darby suffered fatal abdominal injuries at his mother ' s flat where he lived with Nathaniel Pope . Alicia Watson , 30 , from West Bromwich , is also on trial for murder . Mr Pope , 32 , from Wolverhampton , and Miss Watson both deny that charge and another of child cruelty . During his cross - examination , Miss Watson ' s QC , Charles Sherrard , told the court Mr Pope had been sentenced to four months ' imprisonment for common assault in May 2011 , 12 months for burglary in September 2011 , and 20 months for possessing heroin with intent to supply in July 2013 . Mr Pope conceded he had " just told some of the truth " about himself in evidence heard last week . Mr Sherrard referred to another arrest in March 2021 . He said : " On the journey to Oldbury custody suite and on arrival at Oldbury custody suite you were asked by police officers several times if you had anything secreted on your person that you needed to present to them . " Each time you said to them ' no ' . The custody suite police officers were authorised to conduct a strip search of you . " Two police officers then took you into a room and asked again if you had any drugs secreted on your person . Once again , you said ' no ' . " A rock of 9 . 16 grams of crack cocaine was found in a white tissue secreted between your buttocks . " Mr Sherrard also referred to Mr Pope ' s evidence last week that he was " merely a recreational user of cannabis " . He asked : " ( In ) 2013 you ' re a dealer of heroin and in 2021 it ' s crack cocaine . Are you saying that in between we should read into it there would have been nothing of the sort ? " Mr Pope answered : " There was nothing of the sort . " The trial continues . BRIO : A man accused of murdering a toddler had a string of convictions , Wolverhampton Crown Court has heard . T0 : A man accused of murdering a toddler in Wolverhampton had convictions for assault and drugs offences , a court has heard . GPT3 - D2 : The man on trial for the murder of his partner ' s 3 - year - old son had a history of convictions for assault , burglary , and drug dealing . Input Article Generated Summaries Figure 12 : Examples of generated summaries for the BBC - 2022 dataset using 3 different summarization systems . A coalition of thousands of Etsy sellers signaled support for a one - week strike starting on Monday — the same day the online marketplace known for its unique handicrafts will start hiking the fees it charges those who use its platform to earn a living . An online petition started by Etsy ( ETSY ) shop owner Kristi Cassidy urging the company to cancel the fee increases — which tick up from 5 % to 6 . 5 % starting Monday — has garnered nearly 50 , 000 signatures . Of those signatories , some 18 , 500 come from people who have identified as Etsy sellers who support the strike , according to Etsy shop operator and strike participant Mattie Boyd . " We feel like we deserve a seat at the table , " Boyd told CNN Business . " And we hope these demands are met , that ' s our immediate goal . But , generally , there ' s got to be some kind of change , where there ' s some kind of dialogue , or Etsy sellers have some kind of representation where these decisions are being made . ” Sellers participating in the strike are putting their shops on " vacation mode " for a week starting Monday , according to Cassidy ' s petition , a temporary setting that lets users essentially put their Etsy shop on hold for a designated period of time . Etsy CEO Josh Silverman announced the fee increases in a memo to sellers in late February . The letter touted Etsy ' s massive growth over the past two years , boasting how active sellers last year increased their sales by " 23 % on average compared to 2019 , and in 2021 alone , we showed more than 90 million active buyers worldwide that there ' s an alternative to big - box , automated shopping . ” Silverman then announced plans to " make significant investments in marketing , seller tools , and creating a world - class customer experience so we can continue this tremendous growth . ” " To support this goal , on April 11 we will increase our current 5 % transaction fee to 6 . 5 % , " Silverman wrote . Etsy is the main source of income for Boyd , who operates a shop via the online retailer featuring homemade graphic T - shirts and other " niche " items that Boyd says are " geared towards people who are members of the queer and trans community , and who are also into punk rock and metal . ” Demands listed in the petition include canceling the fee increases passed onto sellers ; creating a comprehensive plan to crack down on " reseller " shops ( people selling mass - produced goods that they have not designed themselves ) ; improve and expedite the support systems for sellers who have had their business disrupted by Etsy ' s automated tools ; end the " Star Seller " program that Etsy uses to rate sellers ; and to let sellers opt out of offsite ads for their products . Boyd said many sellers felt like they weren ' t given a fair way to give feedback on the sudden fee - hike announcement , which marks the first increase since 2018 . While the other demands in the petition are issues Boyd said have been brewing for a while , " I think for a lot of us it was that 30 % increase [ to fees ] that really lit the match , " Boyd added . " The strike officially is meant to go from April 11 , today , through April 18 , " Boyd told CNN Business . " But people are being encouraged to participate for as much time as they feel like they can , and no one ' s being shamed if they can ' t do the whole week . ” With sellers spread out across the country , Boyd said organizers are using a slew of tech tools to rally together and support each other — including Reddit channels , Discord chats , and Instagram . Raina Moskowitz , the chief operating officer for Etsy , told CNN Business in a statement via email on Monday that " sellers ' success is a top priority for Etsy . ” " We are always receptive to seller feedback and , in fact , the new fee structure will enable us to increase our investments in areas outlined in the petition , including marketing , customer support , and removing listings that don ' t meet our policies , " the statement added . " We are committed to providing great value for our 5 . 3 million sellers so they are able to grow their businesses while keeping Etsy a beloved , trusted , and thriving marketplace . ” The seller strike on Etsy notably comes amid a wave of workplace activism seen at a slew of major companies over the past year - - from Starbucks ( SBUX ) to Amazon . Earlier this month , Amazon ( AMZN ) workers in Staten Island , New York , voted to form the e - commerce giant ' s first - ever US labor union in a landmark election . Amazon has since filed an appeal , calling for a do - over of the entire vote . GPT3 - D2 : Kristi Cassidy is an Etsy shop owner who is organizing a one - week strike against the company starting on Monday . The strike is in response to Etsy ' s decision to raise the fees it charges sellers , which will increase from 5 % to 6 . 5 % . Cassidy ' s petition urging the company to cancel the fee increases has garnered nearly 50 , 000 signatures . CTRLSum : Etsy ( ETSY ) shop owner Kristi Cassidy started a petition urging the company to cancel the fee increases . Sellers participating in the strike are putting their shops on " vacation mode " for a week starting Monday . The strike comes amid a wave of workplace activism seen at a slew of major companies . Keyword : Kristi Cassidy GPT3 - D2 : The article discusses a one - week strike being led by Etsy sellers in response to fee increases that will go into effect on Monday . The strike was organized in response to a memo from Etsy CEO Josh Silverman announcing the fee increases . Etsy sellers are demanding that the fee increases be cancelled , among other things . CTRLSum : Etsy CEO Josh Silverman announced the fee increases in a memo to sellers in late February . " To support this goal , on April 11 we will increase our current 5 % transaction fee to 6 . 5 % , " Silverman wrote . " We feel like we deserve a seat at the table , " Etsy shop operator Mattie Boyd told CNN Business . Keyword : Silverman Input Article Generated Summaries New York ( CNN Business ) As Russia ' s assault on Ukraine continues , American bar and restaurant owners are hoping a small word change will help show their solidarity with the Ukrainian people . In a move reminiscent of the " freedom fries " fad of the early aughts , they ' re taking Moscow Mules off the menu and replacing them with Kyiv Mules . Small American businesses , such as independent bar or restaurant owners , may not have any direct business ties to Russia , but many feel strongly about the violent attack on Ukrainian cities and citizens . Replacing " Moscow " with " Kyiv " in their vodka - ginger - lime cocktails is one way to show support for Ukraine . Bond Bar , in San Francisco , has renamed its Moscow Mule the Kyiv Mule . " It ' s just a little token of acknowledgment to the Ukrainian people , " said owner Andrea Minoo . " We ' re just trying to raise awareness , and to let people know , we ' re in support [ of Ukraine ] . " She wants Ukrainians to know that " we see what ' s happening , we wish we could do more . " Bond Bar doesn ' t serve Russian vodka , Minoo noted , so it ' s not replacing any ingredients in its Kyiv Mule . Madrone Art Bar , also in San Francisco , did serve Russian vodka until this past weekend , when owner Michael Krouse decided to take it off the menu . First , he had to figure out which of the roughly 10 vodkas he carries were actually Russian . Many top - selling vodka brands that trace their origins to Russia are now distilled in multiple countries , including the United States . Stoli Vodka , for example , is actually made in Latvia , and the company ' s headquarters are in Luxembourg . After some research , Krouse removed Russian Standard , one of the few vodka brands that actually is Russian - made , from his bar . Then he decided to rename Madrone ' s Moscow Mule the Kyiv Mule and looked for a Ukrainian vodka to make it with . The bar unveiled the reconstituted cocktail on Instagram this week . " Introducing the ' Kyiv Mule ' made with Prime Ukrainian vodka ! , ' " a Wednesday post reads , adding that " $ 2 of each Kyiv Mule sale will be donated to the Ukraine Crisis Fund . " The Kyiv Mule costs $ 12 . Krouse said he was feeling sad and helpless about the situation in Ukraine when he decided to make those moves . Those changes were " at least something that we could do , " he said . Making a gesture Em Chamas Brazilian Grill in Kansas City , Missouri , said in a Facebook post last week that its Moscow Mule will be replaced by a " Snake Island Mule , " in " support of the Ukrainian resistance and in honor of the brave soldiers of Snake Island . " Snake Island , also known as Zmiinyi Island , sits about 30 miles off the southern tip of the Ukrainian mainland in the northwestern Black Sea . Last week , a handful of Ukrainian fighters on the island stood up to Russian warships . They were feared dead , but the Ukrainian Navy released a statement Monday saying the troops were " alive and well " after being forced to surrender " due to the lack of ammunition . " Back in Kansas City , the Em Chamas post also urged customers against " directing animosity towards local restaurants or businesses that are Russian - themed and / or are owned / operated by those of Russian [ descent ] . " The eatery , which charges about $ 11 for cocktails , plans to donate profits from the Snake Island Mule to a Ukrainian charity , according to the Facebook post . Ronnie Heckman , owner of Caddies on Cordell , a bar and grill in Bethesda , Maryland , has not only swapped out Moscow Mules for Kyiv Mules , he ' s gone so far to replace Black Russians and White Russians with Black Ukrainians ( vodka and Kahlua on the rocks ) and White Ukrainians ( made with vodka , Kahlua and cream ) . " It ' s a gesture , " he said . Caddies is also donating part of the revenue from those drinks to Ukrainian aid , he said . Heckman hopes that if enough restaurant and bar owners take references to Russia off their menus , they can send a message to Russian leadership . The attack on Ukraine " makes no sense , " he said . " It ' s wrong . " CNN ' s Jordan Valinsky , Brad Lendon , Tim Lister and Josh Pennington contributed to this report . GPT3 - D2 : The article discusses how American bar and restaurant owners are changing the name of the Moscow Mule cocktail to the Kyiv Mule to show support for Ukraine . The Kyiv Mule is made with Ukrainian vodka and ginger - lime , and some businesses are donating part of the proceeds to Ukrainian aid . The change is meant to be a small gesture to show support for the Ukrainian people as Russia continues its assault . CTRLSum : Bar owners in the U . S . are replacing " Moscow " with " Kyiv " in their vodka - ginger - lime cocktails . They ' re doing it to show their solidarity with the Ukrainian people . Russia ' s assault on Ukraine is " wrong , " says Ronnie Heckman , owner of a Maryland bar . Keyword : Ukraine GPT3 - D2 : The article discusses how American bar and restaurant owners are swapping out Moscow Mules for Kyiv Mules in order to show support for Ukraine . This is in response to Russia ' s recent assault on the country . CNN ' s Brad Lendon contributed to the report . CTRLSum : Bar owners are replacing " Moscow " with " Kyiv " in their vodka - ginger - lime cocktails . CNN ' s Jordan Valinsky , Brad Tim Lendon , Josh Pennington contributed to this report . The attack on Ukraine " makes no sense , " says Ronnie Heckman , owner of Caddies on Cordell . Keyword : Brad Lendon Figure 13 : Examples of keyword - focused summaries for CNN articles from 2022 . Figure 14 : Screenshot of the task instructions for the generic summarization setting . Figure 15 : Screenshot of the task instructions for the keyword - based setting .