Probabilistic Principal Component Analysis Author ( s ) : Michael E . Tipping and Christopher M . Bishop Source : Journal of the Royal Statistical Society . Series B ( Statistical Methodology ) , Vol . 61 , No . 3 ( 1999 ) , pp . 611 - 622 Published by : Blackwell Publishing for the Royal Statistical Society Stable URL : http : / / www . jstor . org / stable / 2680726 Accessed : 06 / 03 / 2010 10 : 52 Your use of the JSTOR archive indicates your acceptance of JSTOR ' s Terms and Conditions of Use , available at http : / / www . jstor . org / page / info / about / policies / terms . jsp . JSTOR ' s Terms and Conditions of Use provides , in part , that unless you have obtained prior permission , you may not download an entire issue of a journal or multiple copies of articles , and you may use content in the JSTOR archive only for your personal , non - commercial use . Please contact the publisher regarding any further use of this work . Publisher contact information may be obtained at http : / / www . jstor . org / action / showPublisher ? publisherCode = black . Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission . JSTOR is a not - for - profit service that helps scholars , researchers , and students discover , use , and build upon a wide range of content in a trusted digital archive . We use information technology and tools to increase productivity and facilitate new forms of scholarship . For more information about JSTOR , please contact support @ jstor . org . Royal Statistical Society and Blackwell Publishing are collaborating with JSTOR to digitize , preserve and extend access to Journal of the Royal Statistical Society . Series B ( Statistical Methodology ) . http : / / www . jstor . org LETTER Communicated by Peter Dayan Nonlinear Component Analysis as a Kernel Eigenvalue Problem Bernhard Sch¨olkopf Max - Planck - Institut f¨ur biologische Kybernetik , 72076 T¨ubingen , Germany Alexander Smola Klaus - Robert M¨uller GMD First ( Forschungszentrum Informationstechnik ) , 12489 Berlin , Germany A new method for performing a nonlinear form of principal component analysis is proposed . By the use of integral operator kernel functions , one can efﬁciently compute principal components in high - dimensional fea - ture spaces , related to input space by some nonlinear map—for instance , the space of all possible ﬁve - pixel products in 16 × 16 images . We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition . 1 Introduction Principal component analysis ( PCA ) is a powerful technique for extracting structure from possibly high - dimensional data sets . It is readily performed bysolvinganeigenvalueproblemorusingiterativealgorithmsthatestimateprincipalcomponents ( forreviewsoftheexistingliterature , seeJolliffe , 1986 , and Diamantaras & Kung , 1996 ) . PCA is an orthogonal transformation of the coordinate system in which we describe our data . The new coordinate values by which we represent the data are called principal components . It is often the case that a small number of principal components is sufﬁcient to account for most of the structure in the data . These are sometimes called factors or latent variables of the data . We are interested not in principal components in input space but in prin - cipal components of variables , or features , which are nonlinearly related to the input variables . Among these are variables obtained by taking arbitrary higher - order correlations between input variables . In the case of image anal - ysis , this amounts to ﬁnding principal components in the space of products of input pixels . To this end , we are computing dot products in feature space by means of kernel functions in input space . Given any algorithm that can be expressed solely in terms of dot products ( i . e . , without explicit usage of the variables themselves ) , this kernel method enables us to construct different nonlinear Neural Computation 10 , 1299 – 1319 ( 1998 ) c (cid:176) 1998 Massachusetts Institute of Technology 1300 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller versions of it ( Aizerman , Braverman , & Rozonoer , 1964 ; Boser , Guyon , & Vapnik , 1992 ) . Although this general fact was known ( Burges , private com - munication ) , the machine learning community has made little use of it , the exception being support vector machines ( Vapnik , 1995 ) . In this article , we give an example of applying this method in the domain of unsupervised learning , to obtain a nonlinear form of PCA . In the next section , we review the standard PCA algorithm . In order to be able to generalize it to the nonlinear case , we formulate it in a way that uses exclusively dot products . In section 3 , we discuss the kernel method for computing dot products in feature spaces . Together , these two sections form the basis for section 4 , which presents the proposed kernel - based algo - rithm for nonlinear PCA . First experimental results on kernel - based feature extraction for pattern recognition are given in section 5 . We conclude with a discussion ( section 6 ) and an appendix containing some technical material that is not essential for the main thread of the argument . 2 PCA in Feature Spaces Given a set of centered observations x k , k = 1 , . . . , M , x k ∈ R N , (cid:80) Mk = 1 x k = 0 , PCA diagonalizes the covariance matrix , 1 C = 1 M M (cid:88) j = 1 x j x (cid:62) j . ( 2 . 1 ) To do this , one has to solve the eigenvalue equation , λ v = C v , ( 2 . 2 ) for eigenvalues λ ≥ 0 and v ∈ R N \ { 0 } . As C v = 1 M (cid:80) Mj = 1 ( x j · v ) x j , all solutions v with λ (cid:54) = 0 must lie in the span of x 1 , . . . , x M ; hence , equation 2 . 2 in that case is equivalent to λ ( x k · v ) = ( x k · C v ) for all k = 1 , . . . , M . ( 2 . 3 ) In the remainder of this section , we describe the same computation in an - other dot product space F , which is related to the input space by a possibly nonlinear map , (cid:56) : R N → F , x (cid:55)→ X . ( 2 . 4 ) 1 More precisely , the covariance matrix is deﬁned as the expectation of xx (cid:62) ; for conve - nience , we shall use the same term to refer to the estimate in equation 2 . 1 of the covariance matrix from a ﬁnite sample . Nonlinear Component Analysis 1301 Note that F , which we will refer to as the feature space , could have an arbitrarilylarge , possiblyinﬁnite , dimensionality . Hereandinthefollowing , uppercase characters are used for elements of F , and lowercase characters denote elements of R N . Again , we assume that we are dealing with centered data , that is (cid:80) Mk = 1 (cid:56) ( x k ) = 0 ( we shall return to this point later ) . Using the covariance matrix in F , ¯ C = 1 M M (cid:88) j = 1 (cid:56) ( x j ) (cid:56) ( x j ) (cid:62) ( 2 . 5 ) ( if F is inﬁnite dimensional , we think of (cid:56) ( x j ) (cid:56) ( x j ) (cid:62) as the linear operator that maps X ∈ F to (cid:56) ( x j ) ( (cid:56) ( x j ) · X ) ) we now have to ﬁnd eigenvalues λ ≥ 0 and eigenvectors V ∈ F \ { 0 } satisfying , λ V = ¯ C V . ( 2 . 6 ) Again , all solutions V with λ (cid:54) = 0 lie in the span of (cid:56) ( x 1 ) , . . . , (cid:56) ( x M ) . For us , this has two useful consequences . First , we may instead consider the set of equations , λ ( (cid:56) ( x k ) · V ) = ( (cid:56) ( x k ) · ¯ C V ) for all k = 1 , . . . , M , ( 2 . 7 ) and , second , there exist coefﬁcients α i ( i = 1 , . . . , M ) such that , V = M (cid:88) i = 1 α i (cid:56) ( x i ) . ( 2 . 8 ) Combining equations 2 . 7 and 2 . 8 , we get λ M (cid:88) i = 1 α i ( (cid:56) ( x k ) · (cid:56) ( x i ) ) = 1 M M (cid:88) i = 1 α i ( (cid:56) ( x k ) · M (cid:88) j = 1 (cid:56) ( x j ) ) ( (cid:56) ( x j ) · (cid:56) ( x i ) ) for all k = 1 , . . . , M . ( 2 . 9 ) Deﬁning an M × M matrix K by K ij : = ( (cid:56) ( x i ) · (cid:56) ( x j ) ) , ( 2 . 10 ) this reads M λ K α = K 2 α , ( 2 . 11 ) 1302 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller where α denotesthecolumnvectorwithentries α 1 , . . . , α M . Toﬁndsolutions of equation 2 . 11 , we solve the eigenvalue problem , M λ α = K α , ( 2 . 12 ) for nonzero eigenvalues . A justiﬁcation of this procedure is given in ap - pendix A . Let λ 1 ≤ λ 2 ≤ · · · ≤ λ M denote the eigenvalues of K ( i . e . , the solutions M λ of equation 2 . 12 ) , and α 1 , . . . , α M the corresponding complete set of eigenvectors , with λ p being the ﬁrst nonzero eigenvalue ( assuming (cid:56) (cid:54)≡ 0 ) . We normalize α p , . . . , α M by requiring that the corresponding vectors in F be normalized , that is , ( V k · V k ) = 1 for all k = p , . . . , M . ( 2 . 13 ) By virtue of equations 2 . 8 and 2 . 12 , this translates into a normalization condition for α p , . . . , α M : 1 = M (cid:88) i , j = 1 α ki α kj ( (cid:56) ( x i ) · (cid:56) ( x j ) ) = M (cid:88) i , j = 1 α ki α kj K ij = ( α k · K α k ) = λ k ( α k · α k ) . ( 2 . 14 ) For the purpose of principal component extraction , we need to compute projections onto the eigenvectors V k in F ( k = p , . . . , M ) . Let x be a test point , with an image (cid:56) ( x ) in F ; then ( V k · (cid:56) ( x ) ) = M (cid:88) i = 1 α ki ( (cid:56) ( x i ) · (cid:56) ( x ) ) ( 2 . 15 ) may be called its nonlinear principal components corresponding to (cid:56) . In summary , the following steps were necessary to compute the principal components : ( 1 ) compute the matrix K , ( 2 ) compute its eigenvectors and normalize them in F , and ( 3 ) compute projections of a test point onto the eigenvectors . 2 For the sake of simplicity , we have made the assumption that the obser - vations are centered . This is easy to achieve in input space but harder in F , because we cannot explicitly compute the mean of the (cid:56) ( x i ) in F . There is , however , a way to do it , and this leads to slightly modiﬁed equations for kernel - based PCA ( see appendix B ) . 2 Note that in our derivation we could have used the known result ( e . g . , Kirby & Sirovich , 1990 ) that PCA can be carried out on the dot product matrix ( x i · x j ) ij instead of equation 2 . 1 ; however , for the sake of clarity and extendability ( in appendix B , we shall consider the question how to center the data in F ) , we gave a detailed derivation . Nonlinear Component Analysis 1303 Before we proceed to the next section , which more closely investigates the role of the map (cid:56) , the following observation is essential : (cid:56) can be an arbitrary nonlinear map into the possibly high - dimensional space F , for ex - ample , the space of all d th order monomials in the entries of an input vector . Inthatcase , weneedtocomputedotproductsofinputvectorsmappedby (cid:56) , at a possibly prohibitive computational cost . The solution to this problem , described in the following section , builds on the fact that we exclusively need to compute dot products between mapped patterns ( in equations 2 . 10 and 2 . 15 ) ; we never need the mapped patterns explicitly . 3 Computing Dot Products in Feature Spaces In order to compute dot products of the form ( (cid:56) ( x ) · (cid:56) ( y ) ) , we use kernel representations , k ( x , y ) = ( (cid:56) ( x ) · (cid:56) ( y ) ) , ( 3 . 1 ) which allow us to compute the value of the dot product in F without having to carry out the map (cid:56) . This method was used by Boser et al . ( 1992 ) to extend the Generalized Portrait hyperplane classiﬁer of Vapnik and Chervonenkis ( 1974 ) to nonlinear support vector machines . To this end , they substitute a priorichosenkernelfunctions k foralloccurrencesofdotproducts , obtaining decision functions f ( x ) = sgn (cid:195) (cid:96) (cid:88) i = 1 ν i k ( x , x i ) + b (cid:33) . ( 3 . 2 ) Aizerman et al . ( 1964 ) call F the linearization space , and use it in the context of the potential function classiﬁcation method to express the dot product between elements of F in terms of elements of the input space . If F is high - dimensional , we would like to be able to ﬁnd a closed - form expression for k that can be efﬁciently computed . Aizerman et al . ( 1964 ) consider the possi - bility of choosing k a priori , without being directly concerned with the cor - responding mapping (cid:56) into F . A speciﬁc choice of k might then correspond to a dot product between patterns mapped with a suitable (cid:56) . A particularly useful example , which is a direct generalization of a result proved by Poggio ( 1975 , lemma 2 . 1 ) in the context of polynomial approximation , is ( x · y ) d =   N (cid:88) j = 1 x j · y j   d = N (cid:88) j 1 , . . . , j d = 1 x j 1 · . . . · x j d · y j 1 · . . . · y j d = ( C d ( x ) · C d ( y ) ) , ( 3 . 3 ) 1304 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller where C d maps x to the vector C d ( x ) whose entries are all possible d th degree ordered products of the entries of x . For instance ( Vapnik , 1995 ) , if x = ( x 1 , x 2 ) , then C 2 ( x ) = ( x 21 , x 22 , x 1 x 2 , x 2 x 1 ) , or , yielding the same value of the dot product , (cid:56) 2 ( x ) = ( x 21 , x 22 , √ 2 x 1 x 2 ) . ( 3 . 4 ) For this example , it is easy to verify that ( x · y ) 2 = ( x 21 , x 22 , √ 2 x 1 x 2 ) ( y 21 , y 22 , √ 2 y 1 y 2 ) (cid:62) = ( (cid:56) 2 ( x ) · (cid:56) 2 ( y ) ) . In general , the function k ( x , y ) = ( x · y ) d ( 3 . 5 ) corresponds to a dot product in the space of d th - order monomials of the input coordinates . If x represents an image with the entries being pixel values , we can thus easily work in the space spanned by products of any d pixels—provided that we are able to do our work solely in terms of dot products , withoutanyexplicituseofamappedpattern (cid:56) d ( x ) . Thelatterlives in a possibly very high - dimensional space : even though we will identify terms like x 1 x 2 and x 2 x 1 into one coordinate of F , as in equation 3 . 4 , the dimensionality of F still is ( N + d − 1 ) ! d ! ( N − 1 ) ! and thus grows like N d . For instance , 16 × 16 pixel input images and a polynomial degree d = 5 yield a dimensionality of 10 10 . Thus , using kernels of the form in equation 3 . 5 is our only way to take into account higher - order statistics without a combinatorial explosion of time and memory complexity . The general question that function k does correspond to a dot product in some space F has been discussed by Boser et al . ( 1992 ) and Vapnik ( 1995 ) : Mercer’s theorem of functional analysis implies that if k is a continuous ker - nel of a positive integral operator , there exists a mapping into a space where k acts as a dot product ( for details , see appendix C ) . Besides equation 3 . 5 , radial basis functions , k ( x , y ) = exp (cid:181) −(cid:107) x − y (cid:107) 2 2 σ 2 (cid:182) , ( 3 . 6 ) and sigmoid kernels , k ( x , y ) = tanh ( κ ( x · y ) + (cid:50) ) , ( 3 . 7 ) have been used in support vector machines . These different kernels allow the construction of polynomial classiﬁers , radial basis function classiﬁers , and neural networks with the support vector algorithm , which exhibit very similar accuracy . In addition , they all construct their decision functions from analmostidenticalsubsetofasmallnumberoftrainingpatterns , thesupport vectors ( Sch¨olkopf , Burges , & Vapnik , 1995 ) . Nonlinear Component Analysis 1305 The application of equation 3 . 1 to our problem is straightforward . We simply substitute an a priori chosen kernel function k ( x , y ) for all occur - rencesof ( (cid:56) ( x ) · (cid:56) ( y ) ) . Thechoiceof k then implicitly determinesthemapping (cid:56) and the feature space F . 4 Kernel PCA 4 . 1 The Algorithm . To perform kernel - based PCA ( see Figure 1 ) , hence - forth referred to as kernel PCA , the following steps have to be carried out . First , we compute the matrix K ij = ( k ( x i , x j ) ) ij . Next , we solve equation 2 . 12 by diagonalizing K and normalize the eigenvector expansion coefﬁcients α n by requiring λ n ( α n · α n ) = 1 . To extract the principal components ( corre - sponding to the kernel k ) of a test point x , we then compute projections onto the eigenvectors by ( cf . equation 2 . 15 and Figure 2 ) , ( V n · (cid:56) ( x ) ) = M (cid:88) i = 1 α ni k ( x i , x ) . ( 4 . 1 ) If we use a kernel as described in section 3 , we know that this procedure exactly corresponds to standard PCA in some high - dimensional feature space , exceptthatwedonotneedtoperformexpensivecomputationsinthat space . In practice , our algorithm is not equivalent to the form of nonlinear PCA that can be obtained by explicitly mapping into the feature space F . Eventhoughtherankofthematrix K isalwayslimitedbythesamplesize , we may not be able to compute this matrix if the dimensionality is prohibitively high . In that case , using kernels is imperative . 4 . 2 Properties of ( Kernel ) PCA . If we use a kernel that satisﬁes the con - ditions given in section 3 , we know that we are in fact doing a standard PCA in F . Consequently , all mathematical and statistical properties of PCA ( see , e . g . , Jolliffe , 1986 ; Diamantaras & Kung , 1996 ) carry over to kernel - based PCA , with the modiﬁcations that they become statements concerning F rather than R N . In F , we can thus assert that PCA is the orthogonal basis transformation with the following properties ( assuming that the eigenvec - tors are sorted in descending order of the eigenvalue size ) : ( 1 ) the ﬁrst q ( q ∈ { 1 , . . . , M } ) principal components , that is , projections on eigenvectors , carry more variance than any other q orthogonal directions , ( 2 ) the mean - squared approximation error in representing the observations by the ﬁrst q principal components is minimal , ( 3 ) the principal components are un - correlated , and ( 4 ) the ﬁrst q principal components have maximal mutual information with respect to the inputs ( this holds under gaussian assump - tions , and thus depends on the data and the chosen kernel ) . We conclude this section by noting one general property of kernel PCA in input space : for kernels that depend on only dot products or distances 1306 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller R 2 linear PCA R 2 F F kernel PCA k k ( x , y ) = ( x . y ) e . g . k ( x , y ) = ( x . y ) d x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x Figure 1 : The basic idea of kernel PCA . In some high - dimensional feature space F ( bottom right ) , we are performing linear PCA , just like a PCA in input space ( top ) . Since F is nonlinearly related to input space ( via (cid:56) ) , the contour lines of constant projections onto the principal eigenvector ( drawn as an arrow ) become nonlinearininputspace . Notethatwecannotdrawapreimageoftheeigenvector in input space , because it may not even exist . Crucial to kernel PCA is the fact that there is no need to carry out the map into F . All necessary computations are carried out by the use of a kernel function k in input space ( here : R 2 ) . in input space ( as all the examples that we have given so far do ) , kernel PCA has the property of unitary invariance , following directly from the fact that both the eigenvalue problem and the feature extraction depend on only kernel values . This ensures that the features extracted do not depend on which orthonormal coordinate system we use for representing our input data . 4 . 3 Computational Complexity . A ﬁfth - order polynomial kernel on a 256 - dimensional input space yields a 10 10 - dimensional feature space . For two reasons , kernel PCA can deal with this huge dimensionality . First , we do not need to look for eigenvectors in the full space F , but just in the sub - space spanned by the images of our observations x k in F . Second , we do not Nonlinear Component Analysis 1307 S ( V . F ( x ) ) = S a i k ( x i , x ) input vector x sample x 1 , x 2 , x 3 , . . . comparison : k ( x i , x ) feature value weights ( eigenvector coefficients ) a 1 a 2 a 3 a 4 k k k k Figure 2 : Feature extraction architecture in kernel PCA ( cf . equation 4 . 1 ) . In the ﬁrst layer , the input vector is compared to the sample via a kernel function , chosen a priori ( e . g . , polynomial , gaussian , or sigmoid ) . The outputs are then linearly combined using weights , which are found by solving an eigenvector problem . need to compute dot products explicitly between vectors in F ( which can be impossible in practice , even if the vectors live in a lower - dimensional subspace ) because we are using kernel functions . Kernel PCA thus is com - putationally comparable to a linear PCA on (cid:96) observations with an (cid:96) × (cid:96) dot product matrix . If k is easy to compute , as for polynomial kernels , for example , the computational complexity is hardly changed by the fact that we need to evaluate kernel functions rather than just dot products . Further - more , when we need to use a large number (cid:96) of observations , we may want to work with an algorithm for computing only the largest eigenvalues , as , for instance , the power method with deﬂation ( for a discussion , see Dia - mantaras & Kung , 1996 ) . In addition , we can consider using an estimate of the matrix K , computed from a subset of M < (cid:96) examples , while still extract - ing principal components from all (cid:96) examples ( this approach was chosen in some of our experiments described below ) . The situation can be different for principal component extraction . There , we have to evaluate the kernel function M times for each extracted principal component ( see equation 4 . 1 ) , rather than just evaluating one dot product as for a linear PCA . Of course , if the dimensionality of F is 10 10 , this is still vastly faster than linear principal component extraction in F . Still , in some cases ( e . g . , if we were to extract principal components as a preprocessing step for classiﬁcation ) , we might want to speed things up . This can be done by a technique proposed by Burges ( 1996 ) in the context of support vector machines . In the present setting , we approximate each eigenvector V = (cid:80) (cid:96) i = 1 α i (cid:56) ( x i ) ( see equation 2 . 8 ) by another vector ˜ V = (cid:80) mj = 1 β j (cid:56) ( z j ) , where 1308 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller m < (cid:96) is chosen a priori according to the desired speedup , and z j ∈ R N , j = 1 , . . . , m . This is done by minimizing the squared difference ρ = (cid:107) V − ˜ V (cid:107) 2 . The crucial point is that this also can be done without explicitly dealing with the possibly high - dimensional space F . As ρ = (cid:107) V (cid:107) 2 + m (cid:88) i , j = 1 β i β j k ( z i , z j ) − 2 (cid:96) (cid:88) i = 1 m (cid:88) j = 1 α i β j k ( x i , z j ) , ( 4 . 2 ) the gradient of ρ with respect to the β j and the z j is readily expressed in terms of the kernel function ; thus , ρ can be minimized by gradient descent . Finally , although kernel principal component extraction is computation - ally more expensive than its linear counterpart , this additional investment can pay back afterward . In experiments on classiﬁcation based on the ex - tracted principal components , we found that when we trained on nonlinear features , it was sufﬁcient to use a linear support vector machine to con - struct the decision boundary . Linear support vector machines , however , are much faster in classiﬁcation speed than nonlinear ones . This is due to the fact that for k ( x , y ) = ( x · y ) , the support vector decision function ( see equa - tion 3 . 2 ) can be expressed with a single weight vector w = (cid:80) (cid:96) i = 1 ν i x i as f ( x ) = sgn ( ( x · w ) + b ) . Thus the ﬁnal stage of classiﬁcation can be done extremely fast . 4 . 4 Interpretability and Variable Selection . In PCA , it is sometimes de - sirable to be able to select speciﬁc axes that span the subspace into which one projects in doing principal component extraction . In this way , it may , for instance , be possible to choose variables that are more accessible to interpre - tation . In the nonlinear case , there is an additional problem : some directions in F do not have preimages in input space . To make this plausible , note that the linear span of the training examples mapped into feature space can have dimensionality up to M ( the number of examples ) . If this exceeds the di - mensionality of input space , it is rather unlikely that each vector of the form in equation 2 . 8 has a preimage . To get interpretability , we thus need to ﬁnd directions in input space ( i . e . , input variables ) whose images under (cid:56) span the PCA subspace in F . This can be done with an approach akin to the one already described . We could parameterize our set of desired input variables and run the minimization of equation 4 . 2 only over those parameters . The parameters can be , for example , group parameters , which determine the amount of translation , say , starting from a set of images . 4 . 5 DimensionalityReduction , FeatureExtraction , andReconstruction . Unlike linear PCA , the proposed method allows the extraction of a number of principal components that can exceed the input dimensionality . Suppose that the number of observations M exceeds the input dimensionality N . Lin - ear PCA , even when it is based on the M × M dot product matrix , can ﬁnd at Nonlinear Component Analysis 1309 most N nonzero eigenvalues ; they are identical to the nonzero eigenvalues of the N × N covariance matrix . In contrast , kernel PCA can ﬁnd up to M nonzero eigenvalues—a fact that illustrates that it is impossible to perform kernel PCA directly on an N × N covariance matrix . Even more features could be extracted by using several kernels . Beingjustabasistransformation , standardPCAallowsthereconstruction of the original patterns x i , i = 1 , . . . , (cid:96) , from a complete set of extracted principal components ( x i · v j ) , j = 1 , . . . , (cid:96) , by expansion in the eigenvector basis . Even from an incomplete set of components , good reconstruction is often possible . In kernel PCA , this is more difﬁcult . We can reconstruct the image of a pattern in F from its nonlinear components ; however , if we have only an approximate reconstruction , there is no guarantee that we can ﬁnd an exact preimage of the reconstruction in input space . In that case , we would have to resort to an approximation method ( cf . equation 4 . 2 ) . Alternatively , we could use a suitable regression method for estimating the reconstruction mapping from the kernel - based principal components to the inputs . 5 Experiments 5 . 1 Toy Examples . To provide some insight into how PCA in F be - haves in input space , we show a set of experiments with an artiﬁcial two - dimensional data set , using polynomial kernels ( cf . equation 3 . 5 ) of degree 1 through 4 ( see Figure 3 ) . Linear PCA ( on the left ) leads to only two nonzero eigenvalues , as the input dimensionality is 2 . In contrast , nonlinear PCA al - lows the extraction of further components . In the ﬁgure , note that nonlinear PCA produces contour lines ( of constant feature value ) , which reﬂect the structure in the data better than in linear PCA . In all cases , the ﬁrst principal component varies monotonically along the parabola underlying the data . In the nonlinear cases , the second and the third components show behav - ior that is similar for different polynomial degrees . The third component , which comes with small eigenvalues ( rescaled to sum to 1 ) , seems to pick up the variance caused by the noise , as can be nicely seen in the case of degree 2 . Dropping this component would thus amount to noise reduction . Further toy examples , using radial basis function kernels ( see equation 3 . 6 ) and neural network – type sigmoid kernels ( see equation 3 . 7 ) , are shown in Figures 4 and 5 . 5 . 2 Character Recognition . In this experiment , we extracted nonlinear principal components from a handwritten character database , using ker - nel PCA in the form given in appendix B . We chose the US Postal Service ( USPS ) database of handwritten digits collected from mail envelopes in Buf - falo . This database contains 9298 examples of dimensionality 256 ; 2007 of them make up the test set . For computational reasons , we decided to use a subset of 3000 training examples for the matrix K . To assess the utility of 1310 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 000 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 291 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 709 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 034 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 345 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 621 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 026 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 395 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 570 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 021 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 418 −1 0 1 −0 . 5 0 0 . 5 1 Eigenvalue = 0 . 552 Figure 3 : Two - dimensional toy example , with data generated in the following way : x values have uniform distribution in [ − 1 , 1 ] , y values are generated from y i = x 2 i + ξ , where ξ is normal noise with standard deviation 0 . 2 . From left to right , thepolynomialdegreeinthekernel ( seeequation3 . 5 ) increasesfrom1to4 ; from top to bottom , the ﬁrst three eigenvectors are shown in order of decreasing eigenvalue size . The ﬁgures contain lines of constant principal component value ( contour lines ) ; in the linear case , these are orthogonal to the eigenvectors . We did not draw the eigenvectors ; as in the general case , they live in a higher - dimensional feature space . the components , we trained a soft margin hyperplane classiﬁer ( Vapnik & Chervonenkis , 1974 ; Cortes & Vapnik , 1995 ) on the classiﬁcation task . This is a special case of support vector machines , using the standard dot prod - uct as a kernel function . It simply tries to separate the training data by a hyperplane with large margin . Table 1 illustrates two advantages of using nonlinear kernels . First , per - formance of a linear classiﬁer trained on nonlinear principal components is better than for the same number of linear components ; second , the perfor - mance for nonlinear components can be further improved by using more components than is possible in the linear case . The latter is related to the fact that there are many more higher - order features than there are pixels in an image . Regarding the ﬁrst point , note that extracting a certain num - ber of features in a 10 10 - dimensional space constitutes a much higher re - duction of dimensionality than extracting the same number of features in 256 - dimensional input space . Nonlinear Component Analysis 1311 Figure4 : Two - dimensionaltoyexamplewiththreedataclusters ( gaussianswith standard deviation 0 . 1 , depicted region : [ − 1 , 1 ] × [ − 0 . 5 , 1 ] ) : ﬁrst eight nonlinear principal components extracted with k ( x , y ) = exp ( − (cid:107) x − y (cid:107) 2 0 . 1 ) . Note that the ﬁrst two principal components ( top left ) nicely separate the three clusters . Compo - nents 3 – 5 split up the clusters into halves . Similarly , components 6 – 8 split them again , in a way orthogonal to the above splits . Thus , the ﬁrst eight components divide the data into 12 regions . The Matlab code used for generating this ﬁgure can be obtained from http : / / svm . ﬁrst . gmd . de . Figure5 : Two - dimensionaltoyexamplewiththreedataclusters ( gaussianswith standard deviation 0 . 1 , depicted region : [ − 1 , 1 ] × [ − 0 . 5 , 1 ] ) : ﬁrst three nonlinear principal components extracted with k ( x , y ) = tanh (cid:161) 2 ( x · y ) + 1 (cid:162) . The ﬁrst two principal components ( top left ) are sufﬁcient to separate the three clusters , and the third component splits the clusters into halves . For all numbers of features , the optimal degree of kernels to use is around 4 , which is compatible with support vector machine results on the same data set ( Sch¨olkopf , Burges , & Vapnik , 1995 ) . Moreover , with only one exception , the nonlinear features are superior to their linear counterparts . The resulting error rate for the best of our classiﬁers ( 4 . 0 % ) is competitive with convolu - tional ﬁve - layer neural networks ( 5 . 0 % were reported by LeCun et al . , 1989 ) and nonlinear support vector classiﬁers ( 4 . 0 % , Sch¨olkopf , Burges , & Vapnik , 1995 ) ; it is much better than linear classiﬁers operating directly on the image data ( a linear support vector machine achieves 8 . 9 % ; Sch¨olkopf , Burges , & Vapnik , 1995 ) . These encouraging results have been reproduced on an object recognition task ( Sch¨olkopf , Smola , & M¨uller , 1996 ) . 1312 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller Table 1 : Test Error Rates on the USPS Handwritten Digit Database . Test Error Rate for Degree Number of components 1 2 3 4 5 6 7 32 9 . 6 8 . 8 8 . 1 8 . 5 9 . 1 9 . 3 10 . 8 64 8 . 8 7 . 3 6 . 8 6 . 7 6 . 7 7 . 2 7 . 5 128 8 . 6 5 . 8 5 . 9 6 . 1 5 . 8 6 . 0 6 . 8 256 8 . 7 5 . 5 5 . 3 5 . 2 5 . 2 5 . 4 5 . 4 512 N . A . 4 . 9 4 . 6 4 . 4 5 . 1 4 . 6 4 . 9 1024 N . A . 4 . 9 4 . 3 4 . 4 4 . 6 4 . 8 4 . 6 2048 N . A . 4 . 9 4 . 2 4 . 1 4 . 0 4 . 3 4 . 4 Note : Linear support vector machines were trained on nonlinear principal com - ponents extracted by PCA with kernel ( 3 . 5 ) , for degrees 1 through 7 . In the case of degree 1 , we are doing standard PCA , with the number of nonzero eigenval - ues being at most the dimensionality of the space , 256 . Clearly , nonlinear principal components afford test error rates that are superior to the linear case ( degree 1 ) . 6 Discussion 6 . 1 Feature Extraction for Classiﬁcation . This article presented a new technique for nonlinear PCA . To develop this technique , we made use of a kernel method so far used only in supervised learning ( Vapnik , 1995 ) . Kernel PCA constitutes a ﬁrst step toward exploiting this technique for a large class of algorithms . In experiments comparing the utility of kernel PCA features for pattern recognition using a linear classiﬁer , we found two advantages of nonlin - ear kernels . First , nonlinear principal components afforded better recog - nition rates than corresponding numbers of linear principal components ; and , second , the performance for nonlinear components can be improved by using more components than is possible in the linear case . We have not yet compared kernel PCA to other techniques for nonlinear feature extrac - tion and dimensionality reduction . We can , however , compare results with other feature extraction methods used in the past by researchers working on the USPS classiﬁcation problem . Our system of kernel PCA feature ex - traction plus linear support vector machine , for instance , performed better than LeNet1 ( LeCun et al . , 1989 ) . Although the latter result was obtained a number of years ago , LeNet1 nevertheless provides an architecture that contains a great deal of prior information about the handwritten character classiﬁcation problem . It uses shared weights to improve transformation invariance and a hierarchy of feature detectors resembling parts of the hu - man visual system . In addition , our features were extracted without taking into account that we want to do classiﬁcation . Clearly , in supervised learn - ing , where we are given a set of labeled observations ( x 1 , y 1 ) , . . . , ( x (cid:96) , y (cid:96) ) , it Nonlinear Component Analysis 1313 would seem advisable to make use of the labels not only during the training of the ﬁnal classiﬁer but also in the stage of feature extraction . Finally , we note that a similar approach can be taken in the case of re - gression estimation . 6 . 2 Feature Space and the Curse of Dimensionality . We are doing PCA in 10 10 - dimensional feature spaces , yet getting results in ﬁnite time that are comparable to state - of - the - art techniques . In fact , however , we are not working in the full feature space , but in a comparably small linear subspace of it , whose dimension equals at most the number of observations . The method automatically chooses this subspace and provides a means of tak - ing advantage of the lower dimensionality . An approach that consisted in explicitly of mapping into feature space and then performing PCA would have severe difﬁculties at this point . Even if PCA was done based on an M × M dot product matrix ( M being the sample size ) , whose diagonaliza - tion is tractable , it would still be necessary to evaluate dot products in a 10 10 - dimensional feature space to compute the entries of the matrix in the ﬁrst place . Kernel - based methods avoid this problem ; they do not explicitly compute all dimensions of F ( loosely speaking , all possible features ) , but work only in a relevant subspace of F . 6 . 3 Comparison to Other Methods for Nonlinear PCA . Starting from some of the properties characterizing PCA ( see above ) , it is possible to de - velop a number of possible generalizations of linear PCA to the nonlinear case . Alternatively , one may choose an iterative algorithm that adaptively estimates principal components and make some of its parts nonlinear to extract nonlinear features . Rather than giving a full review of this ﬁeld here , we brieﬂy describe ﬁve approaches and refer readers to Diamantaras and Kung ( 1996 ) for more details . 6 . 3 . 1 Hebbian Networks . Initiated by the pioneering work of Oja ( 1982 ) , a number of unsupervised neural network algorithms computing principal components have been proposed . Compared to the standard approach of diagonalizing the covariance matrix , they have advantages—for instance , when the data are nonstationary . Nonlinear variants of these algorithms are obtained by adding nonlinear activation functions . The algorithms then extract features that the authors have referred to as nonlinear principal components . These approaches , however , do not have the geometrical in - terpretation of kernel PCA as a standard PCA in a feature space nonlinearly related to input space , and it is thus more difﬁcult to understand what ex - actly they are extracting . 6 . 3 . 2 Autoassociative Multilayer Perceptrons . Consider a linear three - layer perceptron with a hidden layer smaller than the input . If we train 1314 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller it to reproduce the input values as outputs ( i . e . , use it in autoassociative mode ) , then the hidden unit activations form a lower - dimensional repre - sentation of the data , closely related to PCA ( see , for instance , Diamantaras & Kung , 1996 ) . To generalize to a nonlinear setting , one uses nonlinear acti - vation functions and additional layers . 3 While this can be considered a form of nonlinear PCA , the resulting network training consists of solving a hard nonlinear optimization problem , with the possibility of getting trapped in local minima , and thus with a dependence of the outcome on the starting point of the training . Moreover , in neural network implementations , there is often a risk of getting overﬁtting . Another drawback of neural approaches to nonlinear PCA is that the number of components to be extracted has to be speciﬁed in advance . As an aside , note that hyperbolic tangent kernels can be used to extract neural network – type nonlinear features using kernel PCA ( see Figure 5 ) . The principal components of a test point x in that case take the form ( see Figure 2 ) (cid:80) i α ni tanh · ( κ ( x i , x ) + (cid:50) ) . 6 . 3 . 3 Principal Curves . An approach with a clear geometric interpreta - tion in input space is the method of principal curves ( Hastie & Stuetzle , 1989 ) , which iteratively estimates a curve ( or surface ) capturing the struc - ture of the data . The data are mapped to the closest point on a curve , and the algorithm tries to ﬁnd a curve with the property that each point on the curve is the average of all data points projecting onto it . It can be shown that the only straight lines satisfying the latter are principal components , so principal curves are indeed a generalization of the latter . To compute principal curves , a nonlinear optimization problem has to be solved . The dimensionality of the surface , and thus the number of features to extract , is speciﬁed in advance . 6 . 3 . 4 Locally Linear PCA . In cases where a linear PCA fails because the dependences in the data vary nonlinearly with the region in input space , it can be fruitful to use an approach where linear PCA is applied locally ( e . g . , Bregler & Omohundro , 1994 ) . Possibly kernel PCA could be improved by taking locality into account . 6 . 3 . 5 Kernel PCA . Kernel PCA is a nonlinear generalization of PCA in the sense that it is performing PCA in feature spaces of arbitrarily large ( possibly inﬁnite ) dimensionality , and if we use the kernel k ( x , y ) = ( x · y ) , we recover standard PCA . Compared to the above approaches , kernel PCA has the main advantage that no nonlinear optimization is involved ; it is 3 Simply using nonlinear activation functions in the hidden layer would not sufﬁce . The linear activation functions already lead to the best approximation of the data ( given thenumberofhiddennodes ) , soforthenonlinearitiestohaveaneffectonthecomponents , the architecture needs to be changed to comprise more layers ( see , e . g . , Diamantaras & Kung , 1996 ) . Nonlinear Component Analysis 1315 essentially linear algebra , as simple as standard PCA . In addition , we need not specify the number of components that we want to extract in advance . Compared to neural approaches , kernel PCA could be disadvantageous if we need to process a very large number of observations , because this results in a large matrix K . Compared to principal curves , kernel PCA is harder to interpret in input space ; however , at least for polynomial kernels , it has a very clear interpretation in terms of higher - order features . 7 Conclusion Compared to other techniques for nonlinear feature extraction , kernel PCA has the advantages that it requires only the solution of an eigenvalue prob - lem , not nonlinear optimization , and by the possibility of using different kernels , it comprises a fairly general class of nonlinearities that can be used . Clearly the last point has yet to be evaluated in practice ; however , for the support vector machine , the utility of different kernels has already been established . Different kernels ( polynomial , sigmoid , gaussian ) led to ﬁne classiﬁcation performances ( Sch¨olkopf , Burges , & Vapnik , 1995 ) . The gen - eral question of how to select the ideal kernel for a given task ( i . e . , the appropriate feature space ) , however , is an open problem . The scene has been set for using the kernel method to construct a wide va - riety of rather general nonlinear variants of classical algorithms . It is beyond ourscopeheretoexploreallthepossibilities , includingmanydistance - based algorithms , in detail . Some of them are currently being investigated—for instance , nonlinear forms of k - means clustering and kernel - based indepen - dent component analysis ( Sch¨olkopf , Smola , & M¨uller , 1996 ) . Linear PCA is being used in numerous technical and scientiﬁc applica - tions , including noise reduction , density estimation , image indexing and retrieval systems , and the analysis of natural image statistics . Kernel PCA can be applied to all domains where traditional PCA has so far been used for feature extraction and where a nonlinear extension would make sense . Appendix A : The Eigenvalue Problem in the Space of Expansion Coefﬁcients Being symmetric , K has an orthonormal basis of eigenvectors ( β i ) i with cor - respondingeigenvalues µ i ; thus , forall i , wehave K β i = µ i β i ( i = 1 , . . . , M ) . To understand the relation between equations 2 . 11 and 2 . 12 , we proceed as follows . First , suppose λ , α satisfy equation 2 . 11 . We may expand α in K ’s eigenvectorbasisas α = (cid:80) Mi = 1 a i β i . Equation2 . 11thenreads M λ (cid:80) i a i µ i β i = (cid:80) i a i µ 2 i β i , or , equivalently , for all i = 1 , . . . , M , M λ a i µ i = a i µ 2 i . This in turn means that for all i = 1 , . . . , M , M λ = µ i or a i = 0 or µ i = 0 . ( A . 1 ) 1316 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller Note that the above are not exclusive ors . We next assume that λ , α satisfy equation 2 . 12 , to carry out a similar derivation . In that case , we ﬁnd that equation 2 . 12 is equivalent to M λ (cid:80) i a i β i = (cid:80) i a i µ i β i , that is , for all i = 1 , . . . , M , M λ = µ i or a i = 0 . ( A . 2 ) Comparing equations A . 1 and A . 2 , we see that all solutions of the latter satisfy the former . However , they do not give its full set of solutions : given a solution of equation 2 . 12 , we may always add multiples of eigenvectors of K with eigenvalue 0 and still satisfy equation 2 . 11 , with the same eigenvalue . This means that there exist solutions of equation 2 . 11 that belong to different eigenvalues yet are not orthogonal in the space of the α k . It does not mean , however , that the eigenvectors of ¯ C in F are not orthogonal . Indeed , if α is an eigenvector of K with eigenvalue 0 , then the corresponding vector (cid:80) i α i (cid:56) ( x i ) is orthogonal to all vectors in the span of the (cid:56) ( x j ) in F , since ( (cid:56) ( x j ) · (cid:80) i α i (cid:56) ( x i ) ) = ( K α ) j = 0 for all j , which means that (cid:80) i α i (cid:56) ( x i ) = 0 . Thus , the above difference between the solutions of equations 2 . 11 and 2 . 12 is irrelevant , since we are interested in vectors in F rather than vectors in the space of the expansion coefﬁcients of equation 2 . 8 . We thus only need to diagonalize K to ﬁnd all relevant solutions of equation 2 . 11 . Appendix B : Centering in High - Dimensional Space Given any (cid:56) and any set of observations x 1 , . . . , x M , the points ˜ (cid:56) ( x i ) : = (cid:56) ( x i ) − 1 M M (cid:88) i = 1 (cid:56) ( x i ) ( B . 1 ) are centered . Thus , the assumptions of section 2 now hold , and we go on to deﬁne covariance matrix and ˜ K ij = ( ˜ (cid:56) ( x i ) · ˜ (cid:56) ( x j ) ) in F . We arrive at the already familiar eigenvalue problem , ˜ λ ˜ α = ˜ K ˜ α , ( B . 2 ) with ˜ α being the expansion coefﬁcients of an eigenvector ( in F ) in terms of the points in equation B . 1 , ˜ V = (cid:80) Mi = 1 ˜ α i ˜ (cid:56) ( x i ) . Because we do not have the centered data ( see equation B . 1 ) , we cannot compute ˜ K directly ; however , we can express it in terms of its noncentered counterpart K . In the following , we shall use K ij = ( (cid:56) ( x i ) · (cid:56) ( x j ) ) and the notations 1 ij = 1 for all i , j , ( 1 M ) ij : = 1 / M , to compute ˜ K ij = ( ˜ (cid:56) ( x i ) · ˜ (cid:56) ( x j ) ) : ˜ K ij = (cid:195) ( (cid:56) ( x i ) − 1 M M (cid:88) m = 1 (cid:56) ( x m ) ) · ( (cid:56) ( x j ) − 1 M M (cid:88) n = 1 (cid:56) ( x n ) ) (cid:33) ( B . 3 ) Nonlinear Component Analysis 1317 = K ij − 1 M M (cid:88) m = 1 1 im K mj − 1 M M (cid:88) n = 1 K in 1 nj + 1 M 2 M (cid:88) m , n = 1 1 im K mn 1 nj = ( K − 1 M K − K 1 M + 1 M K 1 M ) ij . We thus can compute ˜ K from K and then solve the eigenvalue problem ( see equation B . 2 ) . As in equation 2 . 14 , the solutions ˜ α k are normalized by normalizing the corresponding vectors ˜ V k in F , which translates into ˜ λ k ( ˜ α k · ˜ α k ) = 1 . For feature extraction , we compute projections of centered (cid:56) - images of test patterns t onto the eigenvectors of the covariance matrix of the centered points , ( ˜ V k · ˜ φ ( t ) ) = M (cid:88) i = 1 ˜ α ki ( ˜ (cid:56) ( x i ) · ˜ (cid:56) ( t ) ) . ( B . 4 ) Consider a set of test points t 1 , . . . , t L , and deﬁne two L × M matrices by K testij = ( (cid:56) ( t i ) · (cid:56) ( x j ) ) and ˜ K testij = ( ( (cid:56) ( t i ) − 1 M (cid:80) Mm = 1 (cid:56) ( x m ) ) · ( (cid:56) ( x j ) − 1 M (cid:80) Mn = 1 (cid:56) ( x n ) ) ) . As in equation B . 3 , we express ˜ K test in terms of K test , and arrive at ˜ K test = K test − 1 (cid:48) M K − K test 1 M + 1 (cid:48) M K 1 M , where 1 (cid:48) M is the L × M matrix with all entries equal to 1 / M . Appendix C : Mercer Kernels Mercer’s theorem of functional analysis ( e . g . , Courant & Hilbert , 1953 ) gives conditions under which we can construct the mapping (cid:56) from the eigen - function decomposition of k . If k is the continuous kernel of an integral operator K : L 2 → L 2 , ( K f ) ( y ) = (cid:82) k ( x , y ) f ( x ) d x , which is positive , that is , (cid:90) f ( x ) k ( x , y ) f ( y ) d x d y ≥ 0 for all f ∈ L 2 , ( C . 1 ) then k can be expanded into a uniformly convergent series , k ( x , y ) = ∞ (cid:88) i = 1 λ i φ i ( x ) φ i ( y ) , ( C . 2 ) with λ i ≥ 0 . In this case , (cid:56) : x (cid:55)→ ( (cid:112) λ 1 ψ 1 ( x ) , (cid:112) λ 2 ψ 2 ( x ) , . . . ) ( C . 3 ) isamapinto F suchthat k actsasthegivendotproduct , thatis , ( (cid:56) ( x ) · (cid:56) ( y ) ) = k ( x , y ) . Although formulated originally for the case where the integral operator acts on functions f from L 2 ( [ a , b ] ) , Mercer’s theorem also holds if f is deﬁned on a space of arbitrary dimensionality , provided that it is compact ( e . g . , Dunford & Schwartz , 1963 ) . 1318 Bernhard Sch¨olkopf , Alexander Smola , and Klaus - Robert M¨uller Acknowledgments A . S . and B . S . were supported by grants from the Studienstiftung des deuts - chen Volkes . B . S . thanks the GMD First for hospitality during two visits . A . S . andB . S . thankV . Vapnikforintroducingthemtokernelrepresentations of dot products during joint work on support vector machines . Thanks to AT & T and Bell Laboratories for letting us use the USPS database and to L . Bottou , C . Burges , and C . Cortes for parts of the soft margin hyperplane training code . This work proﬁted from discussions with V . Blanz , L . Bottou , C . Burges , H . B¨ulthoff , P . Haffner , Y . Le Cun , S . Mika , N . Murata , P . Simard , S . Solla , V . Vapnik , and T . Vetter . We are grateful to V . Blanz , C . Burges , and S . Solla for reading a preliminary version of the article . References Aizerman , M . , Braverman , E . , & Rozonoer , L . ( 1964 ) . Theoretical foundations of the potential function method in pattern recognition learning . Automation and Remote Control , 25 , 821 – 837 . Boser , B . E . , Guyon , I . M . , & Vapnik , V . N . ( 1992 ) . A training algorithm for optimal margin classiﬁers . In D . Haussler ( Ed . ) , Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory ( pp . 144 – 152 ) . Pittsburgh : ACM Press . Bregler , C . , & Omohundro , M . ( 1994 ) . Surface learning with applications to lipreading . In J . D . Cowan , G . Tesauro , & J . Alspector ( Eds . ) , Advances in neural information processing systems 6 . San Mateo , CA : Morgan Kaufmann . Burges , C . J . C . ( 1996 ) . Simpliﬁed support vector decision rules . In L . Saitta ( Ed . ) , Proc . 13th Intl . Conf . on Machine Learning . San Mateo , CA : Morgan Kaufmann . Cortes , C . , & Vapnik , V . ( 1995 ) . Support vector networks . Machine Learning , 20 , 273 – 297 . Courant , R . , & Hilbert , D . ( 1953 ) . Methods of mathematical physics ( Vol . 1 ) . New York : Interscience . Diamantaras , K . I . , & Kung , S . Y . ( 1996 ) . Principal component neural networks . New York : Wiley . Dunford , N . , & Schwartz , J . T . ( 1963 ) . Linear operators part II : Spectral theory , self adjoint operators in Hilbert space . New York : Wiley . Hastie , T . , & Stuetzle , W . ( 1989 ) . Principal curves . JASA , 84 , 502 – 516 . Jolliffe , I . T . ( 1986 ) . Principal component analysis . New York : Springer - Verlag . Kirby , M . , & Sirovich , L . ( 1990 ) . Application of the Karhunen - Lo ` eve procedure for the characterization of human faces . IEEE Transactions on Pattern Analysis and Machine Intelligence , 12 ( 1 ) , 103 – 108 . Le Cun , Y . , Boser , B . , Denker , J . S . , Henderson , D . , Howard , R . E . , Hubbard , W . , & Jackel , L . J . ( 1989 ) . Backpropagation applied to handwritten zip code recognition . Neural Computation , 1 , 541 – 551 . Oja , E . ( 1982 ) . A simpliﬁed neuron model as a principal component analyzer . J . Math . Biology , 15 , 267 – 273 . Nonlinear Component Analysis 1319 Poggio , T . ( 1975 ) . On optimal nonlinear associative recall . Biological Cybernetics , 19 , 201 – 209 . Sch¨olkopf , B . , Burges , C . , & Vapnik , V . ( 1995 ) . Extracting support data for a given task . In U . M . Fayyad & R . Uthurusamy ( Eds . ) , Proceedings , First Intl . Conference on Knowledge Discovery and Data Mining . Menlo Park , CA : AAAI Press . Sch¨olkopf , B . , Smola , A . , & M¨uller , K . - R . ( 1996 ) . Nonlinear component analysis as a kernel eigenvalue problem ( Tech . Rep . No . 44 ) . T¨ubingen : Max - Planck - Institut f¨ur biologische Kybernetik . Vapnik , V . ( 1995 ) . The nature of statistical learning theory . New York : Springer - Verlag . Vapnik , V . , & Chervonenkis , A . ( 1974 ) . Theory of pattern recognition [ in Russian ] . Nauka , Moscow , 1974 . ( German Translation : W . Wapnik & A . Tscherwo - nenkis , Theorie der Zeichener Kenning , Akademie - Verlag , Berlin ) . Received December 28 , 1996 ; accepted September 18 , 1997 . This article has been cited by : 2 . Giorgio Gnecco , Marcello Sanguineti . 2010 . Error bounds for suboptimal solutions to kernel principal component analysis . Optimization Letters 4 : 2 , 197 - 210 . [ CrossRef ] 3 . Behrooz Makki , Mona Noori Hosseini , Seyyed Ali Seyyedsalehi , Nasser Sadati . 2010 . Unaligned training for voice conversion based on a local nonlinear principal component analysis approach . Neural Computing and Applications 19 : 3 , 437 - 444 . [ CrossRef ] 4 . Ming - Guang Shi , Jun - Feng Xia , Xue - Ling Li , De - Shuang Huang . 2010 . Predicting protein – protein interactions from sequence using correlation coefficient and high - quality interaction dataset . Amino Acids 38 : 3 , 891 - 899 . [ CrossRef ] 5 . Rui Li , Tai - Peng Tian , Stan Sclaroff , Ming - Hsuan Yang . 2010 . 3D Human Motion Tracking with a Coordinated Mixture of Factor Analyzers . International Journal of Computer Vision 87 : 1 - 2 , 170 - 190 . [ CrossRef ] 6 . Taiping Zhang , Bin Fang , Yuan Yan Tang , Zhaowei Shang , Bin Xu . 2010 . Generalized Discriminant Analysis : A Matrix Exponential Approach . IEEE Transactions on Systems , Man , and Cybernetics , Part B ( Cybernetics ) 40 : 1 , 186 - 197 . [ CrossRef ] 7 . HernÃ¡n Stamati , Cecilia Clementi , Lydia E . Kavraki . 2010 . Application of nonlinear dimensionality reduction to characterize the conformational landscape of small peptides . Proteins : Structure , Function , and Bioinformatics 78 : 2 , 223 - 235 . [ CrossRef ] 8 . Shankar R . Rao , Allen Y . Yang , S . Shankar Sastry , Yi Ma . 2010 . Robust Algebraic Segmentation of Mixed Rigid - Body and Planar Motions from Two Views . International Journal of Computer Vision . [ CrossRef ] 9 . Xuehua Li , Hongli Hu , Lan Shu . 2010 . Predicting human immunodeficiency virus protease cleavage sites in nonlinear projection space . Molecular and Cellular Biochemistry . [ CrossRef ] 10 . Ryozo Nagamune , Jongeun Choi . 2010 . Parameter Reduction in Estimated Model Sets for Robust Control . Journal of Dynamic Systems , Measurement , and Control 132 : 2 , 021002 . [ CrossRef ] 11 . Paola Costantini , Marielle Linting , Giovanni C . Porzio . 2010 . Mining performance data through nonlinear PCA with optimal scaling . Applied Stochastic Models in Business and Industry 26 : 1 , 85 - 101 . [ CrossRef ] 12 . Dake Zhou , Zhenmin Tang . 2010 . Kernel - based improved discriminant analysis and its application to face recognition . Soft Computing 14 : 2 , 103 - 111 . [ CrossRef ] 13 . Masashi Sugiyama , Tsuyoshi Idé , Shinichi Nakajima , Jun Sese . 2010 . Semi - supervised local Fisher discriminant analysis for dimensionality reduction . Machine Learning 78 : 1 - 2 , 35 - 61 . [ CrossRef ] 14 . Faruk O . Alpak , Mark D . Barton , Jef Caers . 2009 . A flow - based pattern recognition algorithm for rapid quantification of geologic uncertainty . Computational Geosciences . [ CrossRef ] 15 . Chang Kook Oh , Hoon Sohn , In - Hwan Bae . 2009 . Statistical novelty detection within the Yeongjong suspension bridge under environmental and operational variations . Smart Materials and Structures 18 : 12 , 125022 . [ CrossRef ] 16 . Hanaa E . Sayed , Hossam A . Gabbar , Shigeji Miyazaki . 2009 . Improved Evolving Kernel of Fisher’s Discriminant Analysis for Classification Problem . Journal of Applied Sciences 9 : 12 , 2313 - 2318 . [ CrossRef ] 17 . Stefan Wahl , Konrad Rieck , Pavel Laskov , Peter Domschitz , Klaus - Robert Müller . 2009 . Securing IMS against novel threats . Bell Labs Technical Journal 14 : 1 , 243 - 257 . [ CrossRef ] 18 . Katarina Domijan , Simon P . Wilson . 2009 . Bayesian kernel projections for classification of high dimensional data . Statistics and Computing . [ CrossRef ] 19 . Su - Yun Huang , Yi - Ren Yeh , Shinto Eguchi . 2009 . Robust Kernel Principal Component AnalysisRobust Kernel Principal Component Analysis . Neural Computation 21 : 11 , 3179 - 3213 . [ Abstract ] [ Full Text ] [ PDF ] [ PDF Plus ] 20 . Tong Wang , Jie Yang . 2009 . Using the nonlinear dimensionality reduction method for the prediction of subcellular localization of Gram - negative bacterial proteins . Molecular Diversity 13 : 4 , 475 - 481 . [ CrossRef ] 21 . Xuehua Li , Lan Shu , Hongli Hu . 2009 . Kernel - based nonlinear dimensionality reduction for electrocardiogram recognition . Neural Computing and Applications 18 : 8 , 1013 - 1020 . [ CrossRef ] 22 . Wang Xing - Zhi , Yan Zheng , Ruan Qian - Tu , Wang Wei . 2009 . A kernel - based clustering approach to finding communities in multi - machine power systems . European Transactions on Electrical Power 19 : 8 , 1131 - 1139 . [ CrossRef ] 23 . M . Humberstone , B . Wood , J . Henkel , J . W . Hines . 2009 . Differentiating between expanded and fault conditions using principal component analysis . Journal of Intelligent Manufacturing . [ CrossRef ] 24 . Weifeng Liu , Il ( Memming ) Park , Yiwen Wang , JosÉ C . Principe . 2009 . Extended Kernel Recursive Least Squares Algorithm . IEEE Transactions on Signal Processing 57 : 10 , 3801 - 3814 . [ CrossRef ] 25 . Antoni Wibowo . 2009 . Robust kernel ridge regression based on M - estimation . Computational Mathematics and Modeling 20 : 4 , 438 - 446 . [ CrossRef ] 26 . Hyun - Woo Cho . 2009 . Multivariate calibration for machine health monitoring : kernel partial least squares combined with variable selection . The International Journal of Advanced Manufacturing Technology . [ CrossRef ] 27 . Jun - Bao Li , Jeng - Shyang Pan , Zhe - Ming Lu . 2009 . Face recognition using Gabor - based complete Kernel Fisher Discriminant analysis with fractional power polynomial models . Neural Computing and Applications 18 : 6 , 613 - 621 . [ CrossRef ] 28 . Shiming Xiang , Feiping Nie , Changshui Zhang , Chunxia Zhang . 2009 . Nonlinear Dimensionality Reduction with Local Spline Embedding . IEEE Transactions on Knowledge and Data Engineering 21 : 9 , 1285 - 1298 . [ CrossRef ] 29 . Kechang Fu , Liankui Dai , Tiejun Wu , Ming Zhu . 2009 . Sensor fault diagnosis of nonlinear processes based on structured kernel principal component analysis . Journal of Control Theory and Applications 7 : 3 , 264 - 270 . [ CrossRef ] 30 . C . Dhanjal , S . R . Gunn , J . Shawe - Taylor . 2009 . Efficient Sparse Kernel Feature Extraction Based on Partial Least Squares . IEEE Transactions on Pattern Analysis and Machine Intelligence 31 : 8 , 1347 - 1361 . [ CrossRef ] 31 . Dinesh Kumar , Shakti Kumar , C . S . Rai . 2009 . Feature selection for face recognition : a memetic algorithmic approach . Journal of Zhejiang University SCIENCE A 10 : 8 , 1140 - 1152 . [ CrossRef ] 32 . Tinne Tuytelaars , Christoph H . Lampert , Matthew B . Blaschko , Wray Buntine . 2009 . Unsupervised Object Discovery : A Comparison . International Journal of Computer Vision . [ CrossRef ] 33 . Yijun Sun , Dapeng Wu . 2009 . Feature extraction through local learning . Statistical Analysis and Data Mining 2 : 1 , 34 - 47 . [ CrossRef ] 34 . B . Buelens , T . Pauly , R . Williams , A . Sale . 2009 . Kernel methods for the detection and classification of fish schools in single - beam and multibeam acoustic data . ICES Journal of Marine Science 66 : 6 , 1130 - 1135 . [ CrossRef ] 35 . Zongbo Xie , Jiuchao Feng . 2009 . A sparse projection clustering algorithm . Journal of Electronics ( China ) 26 : 4 , 549 - 551 . [ CrossRef ] 36 . G . F . Tzortzis , A . C . Likas . 2009 . The Global Kernel $ k $ - Means Algorithm for Clustering in Feature Space . IEEE Transactions on Neural Networks 20 : 7 , 1181 - 1194 . [ CrossRef ] 37 . Rabia Jafri , Hamid R . Arabnia . 2009 . A Survey of Face Recognition Techniques . Journal of Information Processing Systems 5 : 2 , 41 - 68 . [ CrossRef ] 38 . Yi Fang , Jong I . Park , Young - Seon Jeong , Myong K . Jeong , Seung H . Baek , Hyun Woo Cho . 2009 . Enhanced predictions of wood properties using hybrid models of PCR and PLS with high - dimensional NIR spectral data . Annals of Operations Research . [ CrossRef ] 39 . Peyman Adibi , Reza Safabakhsh . 2009 . Information Maximization in a Linear Manifold Topographic Map . Neural Processing Letters 29 : 3 , 155 - 178 . [ CrossRef ] 40 . Chunfeng Wan , Akira Mita . 2009 . Pipeline monitoring using acoustic principal component analysis recognition with the Mel scale . Smart Materials and Structures 18 : 5 , 055004 . [ CrossRef ] 41 . Céline Scheidt , Jef Caers . 2009 . Representing Spatial Uncertainty Using Distances and Kernels . Mathematical Geosciences 41 : 4 , 397 - 419 . [ CrossRef ] 42 . S . Nayak , S . Sarkar , B . Loeding . 2009 . Distribution - Based Dimensionality Reduction Applied to Articulated Motion Recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 31 : 5 , 795 - 810 . [ CrossRef ] 43 . Shiming Xiang , Feiping Nie , Yangqiu Song , Changshui Zhang , Chunxia Zhang . 2009 . Embedding new data points for manifold learning via coordinate propagation . Knowledge and Information Systems 19 : 2 , 159 - 184 . [ CrossRef ] 44 . Khalid Chougdali , Mohamed Jedra , Nouredine Zahid . 2009 . Kernel relevance weighted discriminant analysis for face recognition . Pattern Analysis and Applications . [ CrossRef ] 45 . Bor - Chen Kuo , Cheng - Hsuan Li , Jinn - Min Yang . 2009 . Kernel Nonparametric Weighted Feature Extraction for Hyperspectral Image Classification . IEEE Transactions on Geoscience and Remote Sensing 47 : 4 , 1139 - 1155 . [ CrossRef ] 46 . Mark Jager , Fred A . Hamprecht . 2009 . Principal Component Imagery for the Quality Monitoring of Dynamic Laser Welding Processes . IEEE Transactions on Industrial Electronics 56 : 4 , 1307 - 1313 . [ CrossRef ] 47 . Ruiming Liu . 2009 . Eigentargets Versus Kernel Eigentargets : Detection of Infrared Point Targets Using Linear and Nonlinear Subspace Algorithms . Journal of Infrared , Millimeter , and Terahertz Waves 30 : 3 , 278 - 293 . [ CrossRef ] 48 . CÉdric Richard , JosÉ Carlos M . Bermudez , Paul Honeine . 2009 . Online Prediction of Time Series Data With Kernels . IEEE Transactions on Signal Processing 57 : 3 , 1058 - 1067 . [ CrossRef ] 49 . Giorgio Gnecco , Marcello Sanguineti . 2009 . Accuracy of suboptimal solutions to kernel principal component analysis . Computational Optimization and Applications 42 : 2 , 265 - 287 . [ CrossRef ] 50 . Fei Li , Qionghai Dai , Wenli Xu , Guihua Er . 2009 . Weighted Subspace Distance and Its Applications to Object Recognition and Retrieval With Image Sets . IEEE Signal Processing Letters 16 : 3 , 227 - 230 . [ CrossRef ] 51 . António R . C . Paiva , Il Park , José C . Príncipe . 2009 . A Reproducing Kernel Hilbert Space Framework for Spike Train Signal ProcessingA Reproducing Kernel Hilbert Space Framework for Spike Train Signal Processing . Neural Computation 21 : 2 , 424 - 449 . [ Abstract ] [ Full Text ] [ PDF ] [ PDF Plus ] 52 . Chan - Yun Yang , Che - Chang Hsu , Jr - Syu Yang . 2009 . Stray Example Sheltering by Loss Regularized SVM and kNN Preprocessor . Neural Processing Letters 29 : 1 , 7 - 27 . [ CrossRef ] 53 . Jianping Fan , Daniel A . Keim , Yuli Gao , Hangzai Luo , Zongmin Li . 2009 . JustClick : Personalized Image Recommendation via Exploratory Search From Large - Scale Flickr Images . IEEE Transactions on Circuits and Systems for Video Technology 19 : 2 , 273 - 288 . [ CrossRef ] 54 . Kai Zhang , James T . Kwok . 2009 . Density - Weighted Nyström Method for Computing Large Kernel EigensystemsDensity - Weighted Nyström Method for Computing Large Kernel Eigensystems . Neural Computation 21 : 1 , 121 - 146 . [ Abstract ] [ Full Text ] [ PDF ] [ PDF Plus ] 55 . W . Wu , M . O . Ahmad , S . Samadi . 2009 . Discriminant analysis based on modified generalised singular value decomposition and its numerical error analysis . IET Computer Vision 3 : 3 , 159 . [ CrossRef ] 56 . M . A . Jafarizadeh , R . Sufiani , S . Jafarizadeh . 2009 . Recursive calculation of effective resistances in distance - regular networks based on Bose – Mesner algebra and Christoffel – Darboux identity . Journal of Mathematical Physics 50 : 2 , 023302 . [ CrossRef ] 57 . Xiao - zhang LIU , Guo - can FENG . 2009 . < ! [ CDATA [ Multiple kernel discriminant analysis with optimized weight ] ] > : Multiple kernel discriminant analysis with optimized weight . Journal of Computer Applications 29 : 9 , 2473 . [ CrossRef ] 58 . Wei - Ya SHI , Yue - Fei GUO , Xiang - Yang XUE . 2009 . < ! [ CDATA [ Efficient Kernel Principal Component Analysis Algorithm for Large - Scale Data Set ] ] > : Efficient Kernel Principal Component Analysis Algorithm for Large - Scale Data Set . Journal of Software 20 : 8 , 2153 . [ CrossRef ] 59 . Jia WEI , Hong PENG . 2009 . < ! [ CDATA [ Local and Global Preserving Based Semi - Supervised Dimensionality Reduction Method ] ] > . Journal of Software 19 : 11 , 2833 . [ CrossRef ] 60 . Mathieu Fauvel , Jocelyn Chanussot , Jón Atli Benediktsson . 2009 . Kernel Principal Component Analysis for the Classification of Hyperspectral Remote Sensing Data over Urban Areas . EURASIP Journal on Advances in Signal Processing 2009 , 1 - 15 . [ CrossRef ] 61 . Carlos H . R . Lima , Upmanu Lall , Tony Jebara , Anthony G . Barnston . 2009 . Statistical Prediction of ENSO from Subsurface Sea Temperature Using a Nonlinear Dimensionality Reduction . Journal of Climate 22 : 17 , 4501 . [ CrossRef ] 62 . I . Kotsia , I . Pitas , S . Zafeiriou , S . Zafeiriou . 2009 . Novel Multiclass Classifiers Based on the Minimization of the Within - Class Variance . IEEE Transactions on Neural Networks 20 : 1 , 14 - 34 . [ CrossRef ] 63 . Xudong Jiang , Bappaditya Mandal , Alex Kot . 2009 . Complete discriminant evaluation and feature extraction in kernel space for face recognition . Machine Vision and Applications 20 : 1 , 35 - 46 . [ CrossRef ] 64 . Bing - Peng MA , Shi - Guang SHAN , Xi - Lin CHEN , Wen GAO . 2009 . < ! [ CDATA [ Robust Appearance - Based Method for Head Pose Estimation ] ] > . Journal of Software 20 : 6 , 1651 . [ CrossRef ] 65 . Jing XU , Xin - min TAO . 2009 . < ! [ CDATA [ One - class intrusion detection system based on KPCA space - similarity ] ] > : One - class intrusion detection system based on KPCA space - similarity . Journal of Computer Applications 29 : 9 , 2459 . [ CrossRef ] 66 . Hakan Cevikalp , Diane Larlus , Marian Neamtu , Bill Triggs , Frederic Jurie . 2008 . Manifold Based Local Classifiers : Linear and Nonlinear Approaches . Journal of Signal Processing Systems . [ CrossRef ] 67 . Fabio Tozeto Ramos , Suresh Kumar , Ben Upcroft , Hugh Durrant - Whyte . 2008 . A Natural Feature Representation for Unstructured Environments . IEEE Transactions on Robotics 24 : 6 , 1329 - 1340 . [ CrossRef ] 68 . Stefanos Zafeiriou , Ioannis Pitas . 2008 . Discriminant Graph Structures for Facial Expression Recognition . IEEE Transactions on Multimedia 10 : 8 , 1528 - 1540 . [ CrossRef ] 69 . Jian - Wu Xu , AntÓnio R . C . Paiva , Il Park ( Memming ) , Jose C . Principe . 2008 . A Reproducing Kernel Hilbert Space Framework for Information - Theoretic Learning . IEEE Transactions on Signal Processing 56 : 12 , 5891 - 5902 . [ CrossRef ] 70 . Yingwei Zhang , S . Joe Qin . 2008 . Improved nonlinear fault detection technique and statistical analysis . AIChE Journal 54 : 12 , 3207 - 3220 . [ CrossRef ] 71 . Magnus O . Ulfarsson , Victor Solo . 2008 . Dimension Estimation in Noisy PCA With SURE and Random Matrix Theory . IEEE Transactions on Signal Processing 56 : 12 , 5804 - 5816 . [ CrossRef ] 72 . Yanwei Pang , Yuan Yuan , Xuelong Li . 2008 . Effective Feature Extraction in High - Dimensional Space . IEEE Transactions on Systems , Man , and Cybernetics , Part B ( Cybernetics ) 38 : 6 , 1652 - 1656 . [ CrossRef ] 73 . Dit - Yan Yeung , Hong Chang , Guang Dai . 2008 . A Scalable Kernel - Based Semisupervised Metric Learning Algorithm with Out - of - Sample Generalization AbilityA Scalable Kernel - Based Semisupervised Metric Learning Algorithm with Out - of - Sample Generalization Ability . Neural Computation 20 : 11 , 2839 - 2861 . [ Abstract ] [ PDF ] [ PDF Plus ] 74 . D . Bacciu , A . Starita . 2008 . Competitive Repetition Suppression ( CoRe ) Clustering : A Biologically Inspired Learning Model With Application to Robust Clustering . IEEE Transactions on Neural Networks 19 : 11 , 1922 - 1941 . [ CrossRef ] 75 . Xiao - yan ZHOU . 2008 . Novel face recognition method based on KPCA plus KDA . Journal of Computer Applications 28 : 5 , 1263 - 1266 . [ CrossRef ] 76 . A . Sundaresan , R . Chellappa . 2008 . Model Driven Segmentation of Articulating Humans in Laplacian Eigenspace . IEEE Transactions on Pattern Analysis and Machine Intelligence 30 : 10 , 1771 - 1785 . [ CrossRef ] 77 . Yohei Koyama , Tetsuya Kobayashi , Shuji Tomoda , Hiroki Ueda . 2008 . Perturbational formulation of principal component analysis in molecular dynamics simulation . Physical Review E 78 : 4 . . [ CrossRef ] 78 . Shuiwang Ji , Jieping Ye . 2008 . Kernel Uncorrelated and Regularized Discriminant Analysis : A Theoretical and Computational Study . IEEE Transactions on Knowledge and Data Engineering 20 : 10 , 1311 - 1321 . [ CrossRef ] 79 . Gilles Blanchard , Laurent Zwald . 2008 . Finite - Dimensional Projection for Classification and Statistical Learning . IEEE Transactions on Information Theory 54 : 9 , 4169 - 4182 . [ CrossRef ] 80 . J . A . K . Suykens . 2008 . Data Visualization and Dimensionality Reduction Using Kernel Maps With a Reference Point . IEEE Transactions on Neural Networks 19 : 9 , 1501 - 1517 . [ CrossRef ] 81 . Tat - Jun Chin , D . Suter . 2008 . Out - of - Sample Extrapolation of Learned Manifolds . IEEE Transactions on Pattern Analysis and Machine Intelligence 30 : 9 , 1547 - 1556 . [ CrossRef ] 82 . C . Alzate , J . Suykens . 2008 . Kernel Component Analysis Using an Epsilon - Insensitive Robust Loss Function . IEEE Transactions on Neural Networks 19 : 9 , 1583 - 1598 . [ CrossRef ] 83 . Zhe Chen , Jianting Cao , Yang Cao , Yue Zhang , Fanji Gu , Guoxian Zhu , Zhen Hong , Bin Wang , Andrzej Cichocki . 2008 . An empirical EEG analysis in brain death diagnosis for adults . Cognitive Neurodynamics 2 : 3 , 257 - 271 . [ CrossRef ] 84 . Robert D . Luttrell , Frank Vogt . 2008 . Accelerating kernel principal component analysis ( KPCA ) by utilizing two - dimensional wavelet compression : applications to spectroscopic imaging . Journal of Chemometrics 22 : 9 , 510 - 521 . [ CrossRef ] 85 . Chun - jiang PANG . 2008 . Face recognition based on fuzzy chaotic neural network . Journal of Computer Applications 28 : 6 , 1549 - 1551 . [ CrossRef ] 86 . Qianjin Guo , Haibin Yu , Jingtao Hu , Aidong Xu . 2008 . A method for condition monitoring and fault diagnosis in electromechanical system . Neural Computing and Applications 17 : 4 , 373 - 384 . [ CrossRef ] 87 . Shahid Ahmed , Er - Ping Li . 2008 . Modal Analysis of Microstrip Lines Using Singular Value Decomposition Analysis of FDTD Simulations . IEEE Transactions on Electromagnetic Compatibility 50 : 3 , 687 - 696 . [ CrossRef ] 88 . S . Dambreville , Y . Rathi , A . Tannenbaum . 2008 . A Framework for Image Segmentation Using Shape Models and Kernel Space Shape Priors . IEEE Transactions on Pattern Analysis and Machine Intelligence 30 : 8 , 1385 - 1399 . [ CrossRef ] 89 . DANIELE VENTURI , XIAOLIANG WAN , GEORGE EM KARNIADAKIS . 2008 . Stochastic low - dimensional modelling of a random laminar wake past a circular cylinder . Journal of Fluid Mechanics 606 . . [ CrossRef ] 90 . Yanwei Pang , Yuan Yuan , Xuelong Li . 2008 . Gabor - Based Region Covariance Matrices for Face Recognition . IEEE Transactions on Circuits and Systems for Video Technology 18 : 7 , 989 - 993 . [ CrossRef ] 91 . D . Greene , G . Cagney , N . Krogan , P . Cunningham . 2008 . Ensemble non - negative matrix factorization methods for clustering protein - protein interactions . Bioinformatics 24 : 15 , 1722 - 1728 . [ CrossRef ] 92 . Y . H . Hung , Y . S . Liao . 2008 . Applying PCA and Fixed Size LS - SVM Method for Large Scale Classification Problems . Information Technology Journal 7 : 6 , 890 - 896 . [ CrossRef ] 93 . Chong Zhang , ChongXun Zheng , XiaoLin Yu . 2008 . Evaluation of mental fatigue based on multipsychophysiological parameters and kernel learning algorithms . Chinese Science Bulletin 53 : 12 , 1835 - 1847 . [ CrossRef ] 94 . Guiyu Feng , Dewen Hu , Zongtan Zhou . 2008 . A Direct Locality Preserving Projections ( DLPP ) Algorithm for Image Recognition . Neural Processing Letters 27 : 3 , 247 - 255 . [ CrossRef ] 95 . Y . Yamanishi , M . Araki , A . Gutteridge , W . Honda , M . Kanehisa . 2008 . Prediction of drug - target interaction networks from the integration of chemical and genomic spaces . Bioinformatics 24 : 13 , i232 - i240 . [ CrossRef ] 96 . Miroslava Cuperlovic - Culf , Nabil Belacel , Adrian Culf . 2008 . Integrated analysis of transcriptomics and metabolomics profiles . Expert Opinion on Medical Diagnostics 2 : 5 , 497 - 509 . [ CrossRef ] 97 . Anne - Laure Boulesteix , Athanassios Kondylis , Nicole Krämer . 2008 . Comments on : Augmenting the bootstrap to analyze high dimensional genomic data . TEST 17 : 1 , 31 - 35 . [ CrossRef ] 98 . Cheng Yang , Liwei Wang , Jufu Feng . 2008 . On Feature Extraction via Kernels . IEEE Transactions on Systems , Man , and Cybernetics , Part B ( Cybernetics ) 38 : 2 , 553 - 557 . [ CrossRef ] 99 . Sang - Woon Kim , B . J . Oommen . 2008 . On Using Prototype Reduction Schemes to Optimize Kernel - Based Fisher Discriminant Analysis . IEEE Transactions on Systems , Man , and Cybernetics , Part B ( Cybernetics ) 38 : 2 , 564 - 570 . [ CrossRef ] 100 . Haixian Wang , Sibao Chen , Zilan Hu , Wenming Zheng . 2008 . Locality - Preserved Maximum Information Projection . IEEE Transactions on Neural Networks 19 : 4 , 571 - 585 . [ CrossRef ] 101 . I . Wai - Hung Tsang , A . Kocsor , J . T . - Y . Kwok . 2008 . Large - Scale Maximum Margin Discriminant Analysis Using Core Vector Machines . IEEE Transactions on Neural Networks 19 : 4 , 610 - 624 . [ CrossRef ] 102 . Knut Bernhardt . 2008 . Finding Alternatives and Reduced Formulations for Process - Based ModelsFinding Alternatives and Reduced Formulations for Process - Based Models . Evolutionary Computation 16 : 1 , 63 - 88 . [ Abstract ] [ PDF ] [ PDF Plus ] 103 . Gabriel Jarillo , Witold Pedrycz , Marek Reformat . 2008 . Aggregation of classifiers based on image transformations in biometric face recognition . Machine Vision and Applications 19 : 2 , 125 - 140 . [ CrossRef ] 104 . Geert Gins , Ilse Y . Smets , Jan F . Van Impe . 2008 . Efficient Tracking of the Dominant Eigenspace of a Normalized Kernel MatrixEfficient Tracking of the Dominant Eigenspace of a Normalized Kernel Matrix . Neural Computation 20 : 2 , 523 - 554 . [ Abstract ] [ PDF ] [ PDF Plus ] 105 . Weifeng Liu , Puskal P . Pokharel , Jose C . Principe . 2008 . The Kernel Least - Mean - Square Algorithm . IEEE Transactions on Signal Processing 56 : 2 , 543 - 554 . [ CrossRef ] 106 . Robert H . Clewley , John M . Guckenheimer , Francisco J . Valero - Cuevas . 2008 . Estimating Effective Degrees of Freedom in Motor Systems . IEEE Transactions on Biomedical Engineering 55 : 2 , 430 - 442 . [ CrossRef ] 107 . Pallav Sarma , Louis J . Durlofsky , Khalid Aziz . 2008 . Kernel Principal Component Analysis for Efficient , Differentiable Parameterization of Multipoint Geostatistics . Mathematical Geosciences 40 : 1 , 3 - 32 . [ CrossRef ] 108 . Vitomir Štruc , France Mihelič , Nikola Pavešić . 2008 . Face authentication using a hybrid approach . Journal of Electronic Imaging 17 : 1 , 011003 . [ CrossRef ] 109 . P . S . Hiremath , C . J . Prabhakar . 2008 . Extraction and Recognition of Nonlinear Interval - Type Features Using Symbolic KDA Algorithm with Application to Face Recognition . Research Letters in Signal Processing 2008 , 1 - 6 . [ CrossRef ] 110 . Steven Van Vaerenbergh , Javier Vía , Ignacio Santamaría . 2008 . Adaptive Kernel Canonical Correlation Analysis Algorithms for Nonparametric Identification of Wiener and Hammerstein Systems . EURASIP Journal on Advances in Signal Processing 2008 , 1 - 14 . [ CrossRef ] 111 . Guo - en XIA . 2008 . < ! [ CDATA [ Customer churn prediction on kernel principal component analysis feature abstraction ] ] > : Customer churn prediction on kernel principal component analysis feature abstraction . Journal of Computer Applications 28 : 1 , 149 . [ CrossRef ] 112 . Yanfeng Gu , Ying Liu , Ye Zhang . 2008 . A Selective KPCA Algorithm Based on High - Order Statistics for Anomaly Detection in Hyperspectral Imagery . IEEE Geoscience and Remote Sensing Letters 5 : 1 , 43 - 47 . [ CrossRef ] 113 . Gwo - Her Lee , Tzuen Wuu Hsieh , Jinshiuh Taur , Chin - Wang Tao . 2008 . A posteriori multiresolution - based kernel orthogonal subspace technique for supervised texture segmentation . Optical Engineering 47 : 7 , 077006 . [ CrossRef ] 114 . Manli Zhu , Aleix M . Martinez . 2008 . Pruning Noisy Bases in Discriminant Analysis . IEEE Transactions on Neural Networks 19 : 1 , 148 - 157 . [ CrossRef ] 115 . Baochang Zhang , Zongli Wang , Bineng Zhong . 2008 . Kernel Learning of Histogram of Local Gabor Phase Patterns for Face Recognition . EURASIP Journal on Advances in Signal Processing 2008 , 1 - 9 . [ CrossRef ] 116 . X . - L . Yu , X . - G . Wang . 2008 . Kernel uncorrelated neighbourhood discriminative embedding for radar target recognition . Electronics Letters 44 : 2 , 154 . [ CrossRef ] 117 . Rui Xu , Donald C . Wunsch II . 2008 . Recent advances in cluster analysis . International Journal of Intelligent Computing and Cybernetics 1 : 4 , 484 - 508 . [ CrossRef ] 118 . Weifeng Liu , José C . Príncipe . 2008 . Kernel Affine Projection Algorithms . EURASIP Journal on Advances in Signal Processing 2008 , 1 - 13 . [ CrossRef ] 119 . Feng Tang , Ryan Crabb , Hai Tao . 2007 . Representing Images Using Nonorthogonal Haar - Like Bases . IEEE Transactions on Pattern Analysis and Machine Intelligence 29 : 12 , 2120 - 2134 . [ CrossRef ] 120 . Ning Sun , Hai - xian Wang , Zhen - hai Ji , Cai - rong Zou , Li Zhao . 2007 . An efficient algorithm for Kernel two - dimensional principal component analysis . Neural Computing and Applications 17 : 1 , 59 - 64 . [ CrossRef ] 121 . Weifeng Liu , Puskal P . Pokharel , Jose C . Principe . 2007 . Correntropy : Properties and Applications in Non - Gaussian Signal Processing . IEEE Transactions on Signal Processing 55 : 11 , 5286 - 5298 . [ CrossRef ] 122 . JÉrÔme Louradour , Khalid Daoudi , Francis Bach . 2007 . Feature Space Mahalanobis Sequence Kernels : Application to SVM Speaker Verification . IEEE Transactions on Audio , Speech and Language Processing 15 : 8 , 2465 - 2475 . [ CrossRef ] 123 . Huaijun Qiu , Edwin R . Hancock . 2007 . Clustering and Embedding Using Commute Times . IEEE Transactions on Pattern Analysis and Machine Intelligence 29 : 11 , 1873 - 1890 . [ CrossRef ] 124 . Inderjit S . Dhillon , Yuqiang Guan , Brian Kulis . 2007 . Weighted Graph Cuts without Eigenvectors A Multilevel Approach . IEEE Transactions on Pattern Analysis and Machine Intelligence 29 : 11 , 1944 - 1957 . [ CrossRef ] 125 . Xiaohong Wu , Jianjiang Zhou . 2007 . Fuzzy principal component analysis and its Kernel - based model . Journal of Electronics ( China ) 24 : 6 , 772 - 775 . [ CrossRef ] 126 . Tobias Kaupp , Bertrand Douillard , Fabio Ramos , Alexei Makarenko , Ben Upcroft . 2007 . Shared environment representation for a human - robot team performing information fusion . Journal of Field Robotics 24 : 11 - 12 , 911 - 942 . [ CrossRef ] 127 . Cristian Sminchisescu , Atul Kanaujia , Dimitris N . Metaxas . 2007 . BM³E : Discriminative Density Propagation for Visual Tracking . IEEE Transactions on Pattern Analysis and Machine Intelligence 29 : 11 , 2030 - 2044 . [ CrossRef ] 128 . Hitoshi Suzuki , Yuji Waizumi , Nei Kato , Yoshiaki Nemoto . 2007 . Discrimination of similar characters with a nonlinear compound discriminant function . Systems and Computers in Japan 38 : 11 , 36 - 48 . [ CrossRef ] 129 . Stefanos Zafeiriou , Anastasios Tefas , Ioannis Pitas . 2007 . Minimum Class Variance Support Vector Machines . IEEE Transactions on Image Processing 16 : 10 , 2551 - 2564 . [ CrossRef ] 130 . Sung Won Park , Marios Savvides . 2007 . Individual Kernel Tensor - Subspaces for Robust Face Recognition : A Computationally Efficient Tensor Framework Without Requiring Mode Factorization . IEEE Transactions on Systems , Man and Cybernetics , Part B ( Cybernetics ) 37 : 5 , 1156 - 1166 . [ CrossRef ] 131 . Georgios Goudelis , Stefanos Zafeiriou , Anastasios Tefas , Ioannis Pitas . 2007 . Class - Specific Kernel - Discriminant Analysis for Face Verification . IEEE Transactions on Information Forensics and Security 2 : 3 , 570 - 587 . [ CrossRef ] 132 . Rui - ming Liu , Er - qi Liu , Jie Yang , Tian - hao Zhang , Fang - lin Wang . 2007 . Infrared small target detection with kernel Fukunaga – Koontz transform . Measurement Science and Technology 18 : 9 , 3025 - 3035 . [ CrossRef ] 133 . Sylvain Lespinats , Michel Verleysen , Alain Giron , Bernard Fertil . 2007 . DD - HDS : A Method for Visualization and Exploration of High - Dimensional Data . IEEE Transactions on Neural Networks 18 : 5 , 1265 - 1279 . [ CrossRef ] 134 . Jose Miguel Leiva - Murillo , Antonio Artes - Rodriguez . 2007 . Maximization of Mutual Information for Supervised Linear Feature Extraction . IEEE Transactions on Neural Networks 18 : 5 , 1433 - 1441 . [ CrossRef ] 135 . I . P . Androulakis , E . Yang , R . R . Almon . 2007 . Analysis of Time - Series Gene Expression Data : Methods , Challenges , and Opportunities . Annual Review of Biomedical Engineering 9 : 1 , 205 - 228 . [ CrossRef ] 136 . Hau - San Wong , Bo Ma , Zhiwen Yu , Pui Fong Yeung , Horace H . S . Ip . 2007 . 3 - D Head Model Retrieval Using a Single Face View Query . IEEE Transactions on Multimedia 9 : 5 , 1026 - 1036 . [ CrossRef ] 137 . D . Seghers , D . Loeckx , F . Maes , D . Vandermeulen , P . Suetens . 2007 . Minimal Shape and Intensity Cost Path Segmentation . IEEE Transactions on Medical Imaging 26 : 8 , 1115 - 1129 . [ CrossRef ] 138 . H . . Cevikalp , M . . Neamtu , A . . Barkana . 2007 . The Kernel Common Vector Method : A Novel Nonlinear Subspace Classifier for Pattern Recognition . IEEE Transactions on Systems , Man and Cybernetics , Part B ( Cybernetics ) 37 : 4 , 937 - 951 . [ CrossRef ] 139 . Tingting Mu , Asoke K . Nandi , Rangaraj M . Rangayyan . 2007 . Classification of breast masses via nonlinear transformation of features based on a kernel matrix . Medical & Biological Engineering & Computing 45 : 8 , 769 - 780 . [ CrossRef ] 140 . Daniel S . Yeung , Defeng Wang , Wing W . Y . Ng , Eric C . C . Tsang , Xizhao Wang . 2007 . Structured large margin machines : sensitive to data distributions . Machine Learning 68 : 2 , 171 - 200 . [ CrossRef ] 141 . Paul Honeine , Cdric Richard , Patrick Flandrin . 2007 . Time - Frequency Learning Machines . IEEE Transactions on Signal Processing 55 : 7 , 3930 - 3936 . [ CrossRef ] 142 . Hujun Yin . 2007 . Nonlinear dimensionality reduction and data visualization : A review . International Journal of Automation and Computing 4 : 3 , 294 - 303 . [ CrossRef ] 143 . Robert Jenssen , Deniz Erdogmus , Jose C . Principe , Torbjrn Eltoft . 2007 . The Laplacian Classifier . IEEE Transactions on Signal Processing 55 : 7 , 3262 - 3271 . [ CrossRef ] 144 . Shang - Ming Zhou , John Q . Gan . 2007 . Constructing L2 - SVM - Based Fuzzy Classifiers in High - Dimensional Space With Automatic Model Selection and Fuzzy Rule Ranking . IEEE Transactions on Fuzzy Systems 15 : 3 , 398 - 409 . [ CrossRef ] 145 . Tat - Jun Chin , David Suter . 2007 . Incremental Kernel Principal Component Analysis . IEEE Transactions on Image Processing 16 : 6 , 1662 - 1674 . [ CrossRef ] 146 . Ayan Chakrabarti , A . N . Rajagopalan , Rama Chellappa . 2007 . Super - Resolution of Face Images Using Kernel PCA - Based Prior . IEEE Transactions on Multimedia 9 : 4 , 888 - 892 . [ CrossRef ] 147 . Yi - Hung Liu , Han - Pang Huang , Chang - Hsin Weng . 2007 . Recognition of Electromyographic Signals Using Cascaded Kernel Learning Machine . IEEE / ASME Transactions on Mechatronics 12 : 3 , 253 - 264 . [ CrossRef ] 148 . A . Asensio Ramos , H . Socas‐Navarro , A . Lopez Ariste , M . J . Martinez Gonzalez . 2007 . The Intrinsic Dimensionality of Spectropolarimetric Data . The Astrophysical Journal 660 : 2 , 1690 - 1699 . [ CrossRef ] 149 . Elaine P . M . Sousa , Caetano Traina , Agma J . M . Traina , Leejay Wu , Christos Faloutsos . 2007 . A fast and effective method to find correlations among attributes in databases . Data Mining and Knowledge Discovery 14 : 3 , 367 - 407 . [ CrossRef ] 150 . Yijuan Lu , Qi Tian , Feng Liu , Maribel Sanchez , Yufeng Wang . 2007 . Interactive Semisupervised Learning for Microarray Analysis . IEEE / ACM Transactions on Computational Biology and Bioinformatics 4 : 2 , 190 - 203 . [ CrossRef ] 151 . Joachim Kilian , Dion Whitehead , Jakub Horak , Dierk Wanke , Stefan Weinl , Oliver Batistic , Cecilia D ? Angelo , Erich Bornberg - Bauer , Jörg Kudla , Klaus Harter . 2007 . The AtGenExpress global stress expression data set : protocols , evaluation and model data analysis of UV - B light , drought and cold stress responses . The Plant Journal 50 : 2 , 347 - 363 . [ CrossRef ] 152 . Jian Yang , David Zhang , Jing - yu Yang , Ben Niu . 2007 . Globally Maximizing , Locally Minimizing : Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics . IEEE Transactions on Pattern Analysis and Machine Intelligence 29 : 4 , 650 - 664 . [ CrossRef ] 153 . Harald Burgsteiner , Mark Kröll , Alexander Leopold , Gerald Steinbauer . 2007 . Movement prediction from real - world images using a liquid state machine . Applied Intelligence 26 : 2 , 99 - 109 . [ CrossRef ] 154 . Brian Kan - Wing Mak , Roger Wend - Huu Hsiao . 2007 . Kernel Eigenspace - Based MLLR Adaptation . IEEE Transactions on Audio , Speech and Language Processing 15 : 3 , 784 - 795 . [ CrossRef ] 155 . Heesung Kwon , Nasser M . Nasrabadi . 2007 . Kernel Spectral Matched Filter for Hyperspectral Imagery . International Journal of Computer Vision 71 : 2 , 127 - 141 . [ CrossRef ] 156 . Ping Zhong , Masao Fukushima . 2007 . Second - Order Cone Programming Formulations for Robust Multiclass ClassificationSecond - Order Cone Programming Formulations for Robust Multiclass Classification . Neural Computation 19 : 1 , 258 - 282 . [ Abstract ] [ PDF ] [ PDF Plus ] 157 . Yi - Hung Liu , Yen - Ting Chen . 2007 . Face Recognition Using Total Margin - Based Adaptive Fuzzy Support Vector Machines . IEEE Transactions on Neural Networks 18 : 1 , 178 - 192 . [ CrossRef ] 158 . Heesung Kwon , Nasser M . Nasrabadi . 2007 . A Comparative Analysis of Kernel Subspace Target Detectors for Hyperspectral Imagery . EURASIP Journal on Advances in Signal Processing 2007 , 1 - 14 . [ CrossRef ] 159 . Xuelian Yu , Xuegang Wang . 2007 . Kernel uncorrelated neighborhood discriminative embedding for feature extraction . Optical Engineering 46 : 12 , 120502 . [ CrossRef ] 160 . J . G . R . C . Gomes , A . Petraglia , S . K . Mitra . 2007 . Sensitivity analysis of multilayer perceptrons applied to focal - plane image compression . IET Circuits , Devices & Systems 1 : 1 , 79 . [ CrossRef ] 161 . Chuanfeng Lv , Qiangfu Zhao . 2007 . < IT > k < / IT > - PCA : a semi - universal encoder for image compression . International Journal of Pervasive Computing and Communications 3 : 2 , 205 - 220 . [ CrossRef ] 162 . Takehisa Yairi . 2007 . Map Building By Non - linear Dimensionality Reduction of Historical Visibility Data . Transactions of the Japanese Society for Artificial Intelligence 22 , 353 - 363 . [ CrossRef ] 163 . Jonathan Dinerstein , Parris K . Egbert , David Cline . 2006 . Enhancing computer graphics through machine learning : a survey . The Visual Computer 23 : 1 , 25 - 43 . [ CrossRef ] 164 . Phuong H . Nguyen . 2006 . Complexity of free energy landscapes of peptides revealed by nonlinear principal component analysis . Proteins : Structure , Function , and Bioinformatics 65 : 4 , 898 - 913 . [ CrossRef ] 165 . Shipeng Yu , Kai Yu , V . Tresp , H . - P . Kriegel . 2006 . Multi - Output Regularized Feature Projection . IEEE Transactions on Knowledge and Data Engineering 18 : 12 , 1600 - 1613 . [ CrossRef ] 166 . Robert Jenssen , Torbjørn Eltoft , Deniz Erdogmus , Jose C . Principe . 2006 . Some Equivalences between Kernel Methods and Information Theoretic Methods . The Journal of VLSI Signal Processing Systems for Signal , Image , and Video Technology 45 : 1 - 2 , 49 - 65 . [ CrossRef ] 167 . Yoshua Bengio , Martin Monperrus , Hugo Larochelle . 2006 . Nonlocal Estimation of Manifold StructureNonlocal Estimation of Manifold Structure . Neural Computation 18 : 10 , 2509 - 2528 . [ Abstract ] [ PDF ] [ PDF Plus ] 168 . Kilian Q . Weinberger , Lawrence K . Saul . 2006 . Unsupervised Learning of Image Manifolds by Semidefinite Programming . International Journal of Computer Vision 70 : 1 , 77 - 90 . [ CrossRef ] 169 . Y F Gu , Y Liu , C Y Wang , Y Zhang . 2006 . Curvelet - Based Image Fusion Algorithm for Effective Anomaly Detection in Hyperspectral Imagery . Journal of Physics : Conference Series 48 , 324 - 328 . [ CrossRef ] 170 . Fu - lai Chung , Shitong Wang , Zhaohong Deng , Chen Shu , D . Hu . 2006 . Clustering Analysis of Gene Expression Data based on Semi - supervised Visual Clustering Algorithm . Soft Computing 10 : 11 , 981 - 993 . [ CrossRef ] 171 . Weihua Li , Tie - lin Shi , Shu - zi Yang . 2006 . An approach for mechanical fault classification based on generalized discriminant analysis . Frontiers of Mechanical Engineering in China 1 : 3 , 292 - 298 . [ CrossRef ] 172 . Xudong Xie , Kin - Man Lam . 2006 . Gabor - based kernel PCA with doubly nonlinear mapping for face recognition with a single face image . IEEE Transactions on Image Processing 15 : 9 , 2481 - 2492 . [ CrossRef ] 173 . Chin - Chun Chang . 2006 . Deformable shape finding with models based on kernel methods . IEEE Transactions on Image Processing 15 : 9 , 2743 - 2754 . [ CrossRef ] 174 . Hiromichi Suetani , Yukito Iba , Kazuyuki Aihara . 2006 . Detecting generalized synchronization between chaotic signals : a kernel - based approach . Journal of Physics A : Mathematical and General 39 : 34 , 10723 - 10742 . [ CrossRef ] 175 . Pietro Berkes , Laurenz Wiskott . 2006 . On the Analysis and Interpretation of Inhomogeneous Quadratic Forms as Receptive FieldsOn the Analysis and Interpretation of Inhomogeneous Quadratic Forms as Receptive Fields . Neural Computation 18 : 8 , 1868 - 1895 . [ Abstract ] [ PDF ] [ PDF Plus ] 176 . Yoshikazu Washizawa , Yukihiko Yamashita . 2006 . Kernel Projection Classifiers with Suppressing Features of Other ClassesKernel Projection Classifiers with Suppressing Features of Other Classes . Neural Computation 18 : 8 , 1932 - 1950 . [ Abstract ] [ PDF ] [ PDF Plus ] 177 . Chang Kyoo Yoo , In - Beum Lee , Peter A . Vanrolleghem . 2006 . On - Line Adaptive and Nonlinear Process Monitoring of a Pilot - Scale Sequencing Batch Reactor . Environmental Monitoring and Assessment 119 : 1 - 3 , 349 - 366 . [ CrossRef ] 178 . A M Jade , V K Jayaraman , B D Kulkarni . 2006 . Improved time series prediction with a new method for selection of model parameters . Journal of Physics A : Mathematical and General 39 : 30 , L483 - L491 . [ CrossRef ] 179 . Brian Kan - Wing Mak , Roger Wend - Huu Hsiao , Simon Ka - Lung Ho , J . T . Kwok . 2006 . Embedded kernel eigenvoice speaker adaptation and its implication to reference speaker weighting . IEEE Transactions on Audio , Speech and Language Processing 14 : 4 , 1267 - 1280 . [ CrossRef ] 180 . I . Santamaria , P . P . Pokharel , J . C . Principe . 2006 . Generalized correlation function : definition , properties , and application to blind equalization . IEEE Transactions on Signal Processing 54 : 6 , 2187 - 2197 . [ CrossRef ] 181 . S . K . Zhou , R . Chellappa . 2006 . From sample similarity to ensemble similarity : probabilistic distance measures in reproducing kernel Hilbert space . IEEE Transactions on Pattern Analysis and Machine Intelligence 28 : 6 , 917 - 929 . [ CrossRef ] 182 . C . Alippi , F . Scotti . 2006 . Exploiting Application Locality to Design Low - Complexity , Highly Performing , and Power - Aware Embedded Classifiers . IEEE Transactions on Neural Networks 17 : 3 , 745 - 754 . [ CrossRef ] 183 . Xiao - Dong Yu , Lei Wang , Qi Tian , Ping Xue . 2006 . A novel multi - resolution video representation scheme based on kernel PCA . The Visual Computer 22 : 5 , 357 - 370 . [ CrossRef ] 184 . G . P . Stachowiak , P . Podsiadlo , G . W . Stachowiak . 2006 . Evaluation of methods for reduction of surface texture features . Tribology Letters 22 : 2 , 151 - 165 . [ CrossRef ] 185 . Yonghong Tian , Tiejun Huang , Wen Gao . 2006 . Latent linkage semantic kernels for collective classification of link data . Journal of Intelligent Information Systems 26 : 3 , 269 - 301 . [ CrossRef ] 186 . Wenming Zheng . 2006 . Class - Incremental Generalized Discriminant AnalysisClass - Incremental Generalized Discriminant Analysis . Neural Computation 18 : 4 , 979 - 1006 . [ Abstract ] [ PDF ] [ PDF Plus ] 187 . T . V . Pham , A . W . M . Smeulders . 2006 . Sparse representation for coarse and fine object recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 28 : 4 , 555 - 567 . [ CrossRef ] 188 . Yuko Mizuhara , Akira Hayashi , Nobuo Suematsu . 2006 . Embedding of time series data by using dynamic time warping distances . Systems and Computers in Japan 37 : 3 , 1 - 9 . [ CrossRef ] 189 . M . H . C . Law , A . K . Jain . 2006 . Incremental nonlinear dimensionality reduction by manifold learning . IEEE Transactions on Pattern Analysis and Machine Intelligence 28 : 3 , 377 - 391 . [ CrossRef ] 190 . Defeng Wang , Daniel S . Yeung , Eric C . C . Tsang . 2006 . < ! [ CDATA [ Structured One - Class Classification ] ] > . IEEE Transactions on Systems Man and Cybernetics Part B ( Cybernetics ) 36 : 6 , 1283 . [ CrossRef ] 191 . H . Li , T . Jiang , K . Zhang . 2006 . Efficient and Robust Feature Extraction by Maximum Margin Criterion . IEEE Transactions on Neural Networks 17 : 1 , 157 - 165 . [ CrossRef ] 192 . L . M . Galantucci , R . Ferrandes , G . Percoco . 2006 . Digital Photogrammetry for Facial Recognition . Journal of Computing and Information Science in Engineering 6 : 4 , 390 . [ CrossRef ] 193 . W . Zheng , X . Zhou , C . Zou , L . Zhao . 2006 . Facial Expression Recognition Using Kernel Canonical Correlation Analysis ( KCCA ) . IEEE Transactions on Neural Networks 17 : 1 , 233 - 238 . [ CrossRef ] 194 . Hisashi Kashima , Hiroshi Sakamoto , Teruo Koyanagi . 2006 . Design and Analysis of Convolution Kernels for Tree - Structured Data . Transactions of the Japanese Society for Artificial Intelligence 21 , 113 - 121 . [ CrossRef ] 195 . A . Szymkowiak - Have , M . A . Girolami , J . Larsen . 2006 . Clustering via Kernel Decomposition . IEEE Transactions on Neural Networks 17 : 1 , 256 - 264 . [ CrossRef ] 196 . Kazunori HOSOTANI , Toyohiko SUZUKI , Yoshitaka OCHIAI . 2006 . Reconstruction of the Complex Flow with POD Method ( Application to Flows Passing Across Tube Banks Measured by PIV Method ) . Transaction of the Visualization Society of Japan 26 : 12 , 114 - 122 . [ CrossRef ] 197 . J . Lu , K . N . Plataniotis , A . N . Venetsanopoulos , S . Z . Li . 2006 . Ensemble - based discriminant learning with boosting for face recognition . IEEE Transactions on Neural Networks 17 : 1 , 166 - 178 . [ CrossRef ] 198 . Jian - hua Xu , Xue - gong Zhang , Yan - da Li . 2006 . Regularized Kernel Forms of Minimum Squared Error Method . Frontiers of Electrical and Electronic Engineering in China 1 : 1 , 1 - 7 . [ CrossRef ] 199 . Ibtissam Constantin , Cdric Richard , Rgis Lengelle , Laurent Soufflet . 2006 . < ! [ CDATA [ Nonlinear Regularized Wiener Filtering With Kernels : App ] ] > < ! [ CDATA [ lication in Denoising MEG Data Corrupted by ECG ] ] > . IEEE Transactions on Signal Processing 54 : 12 , 4796 . [ CrossRef ] 200 . Qing Song . 2005 . A Robust Information Clustering AlgorithmA Robust Information Clustering Algorithm . Neural Computation 17 : 12 , 2672 - 2698 . [ Abstract ] [ PDF ] [ PDF Plus ] 201 . Heesung Kwon , N . M . Nasrabadi . 2005 . Kernel orthogonal subspace projection for hyperspectral signal classification . IEEE Transactions on Geoscience and Remote Sensing 43 : 12 , 2952 - 2962 . [ CrossRef ] 202 . S . S . Durbha , R . L . King . 2005 . Semantics - enabled framework for knowledge discovery from Earth observation data archives . IEEE Transactions on Geoscience and Remote Sensing 43 : 11 , 2563 - 2572 . [ CrossRef ] 203 . S . Wu , T . W . S . Chow . 2005 . PRSOM : A New Visualization Method by Hybridizing Multidimensional Scaling and Self - Organizing Map . IEEE Transactions on Neural Networks 16 : 6 , 1362 - 1380 . [ CrossRef ] 204 . L . Y . Han , C . J . Zheng , H . H . Lin , J . Cui , H . Li , H . L . Zhang , Z . Q . Tang , Y . Z . Chen . 2005 . Prediction of functional class of novel plant proteins by a statistical learning method . New Phytologist 168 : 1 , 109 - 121 . [ CrossRef ] 205 . Jochen Einbeck , Gerhard Tutz , Ludger Evers . 2005 . Local principal curves . Statistics and Computing 15 : 4 , 301 - 313 . [ CrossRef ] 206 . Joseph Medendorp , Robert A . Lodder . 2005 . Applications of integrated sensing and processing in spectroscopic imaging and sensing . Journal of Chemometrics 19 : 10 , 533 - 542 . [ CrossRef ] 207 . P . Meinicke , S . Klanke , R . Memisevic , H . Ritter . 2005 . Principal surfaces from unsupervised kernel regression . IEEE Transactions on Pattern Analysis and Machine Intelligence 27 : 9 , 1379 - 1391 . [ CrossRef ] 208 . Kwa , M . O . Franz , B . Scholkopf . 2005 . Iterative kernel principal component analysis for image modeling . IEEE Transactions on Pattern Analysis and Machine Intelligence 27 : 9 , 1351 - 1366 . [ CrossRef ] 209 . N . Mezghani , A . Mitiche , M . Cheriet . 2005 . A new representation of shape and its use for high performance in online Arabic character recognition by an associative memory . International Journal of Document Analysis and Recognition ( IJDAR ) 7 : 4 , 201 - 210 . [ CrossRef ] 210 . B . Mak , J . T . Kwok , S . Ho . 2005 . Kernel eigenvoice speaker adaptation . IEEE Transactions on Speech and Audio Processing 13 : 5 , 984 - 992 . [ CrossRef ] 211 . Wenming Zheng , Cairong Zou , Li Zhao . 2005 . An Improved Algorithm for Kernel Principal Component Analysis . Neural Processing Letters 22 : 1 , 49 - 56 . [ CrossRef ] 212 . H . Zhang , W . Huang , Z . Huang , B . Zhang . 2005 . A Kernel Autoassociator Approach to Pattern Classification . IEEE Transactions on Systems , Man and Cybernetics , Part B ( Cybernetics ) 35 : 3 , 593 - 606 . [ CrossRef ] 213 . Douglas R . Heisterkamp , Jing Peng . 2005 . Kernel Vector Approximation Files for Relevance Feedback Retrieval in Large Image Databases . Multimedia Tools and Applications 26 : 2 , 175 - 189 . [ CrossRef ] 214 . Hitoshi Sakano , Naoki Mukawa , Taichi Nakamura . 2005 . Kernel mutual subspace method and its application for object recognition . Electronics and Communications in Japan ( Part II : Electronics ) 88 : 6 , 45 - 53 . [ CrossRef ] 215 . P . Zhang , J . Peng , C . Domeniconi . 2005 . Kernel Pooled Local Subspaces for Classification . IEEE Transactions on Systems , Man and Cybernetics , Part B ( Cybernetics ) 35 : 3 , 489 - 502 . [ CrossRef ] 216 . R . Xu , D . WunschII . 2005 . Survey of Clustering Algorithms . IEEE Transactions on Neural Networks 16 : 3 , 645 - 678 . [ CrossRef ] 217 . Markus Schmid , Timothy S Davison , Stefan R Henz , Utz J Pape , Monika Demar , Martin Vingron , Bernhard Schölkopf , Detlef Weigel , Jan U Lohmann . 2005 . A gene expression map of Arabidopsis thaliana development . Nature Genetics 37 : 5 , 501 - 506 . [ CrossRef ] 218 . Z . - L . Sun , D . - S . Huang , Y . - M . Cheung , J . Liu , G . - B . Huang . 2005 . Using FCMC , FVS , and PCA Techniques for Feature Extraction of Multispectral Images . IEEE Geoscience and Remote Sensing Letters 2 : 2 , 108 - 112 . [ CrossRef ] 219 . H . Xiong , M . N . S . Swamy , M . O . Ahmad . 2005 . Optimizing the Kernel in the Empirical Feature Space . IEEE Transactions on Neural Networks 16 : 2 , 460 - 474 . [ CrossRef ] 220 . Wang Shitong . , F . L . Chung . , Deng Zhaohong . , L . I . N . Qing . , H . U . Dewen . . 2005 . New Feature - extraction Criteria and Classification Algorithms for Cancer Gene Expression Datasets . Biotechnology ( Faisalabad ) 4 : 3 , 163 - 172 . [ CrossRef ] 221 . Jian Yang , A . F . Frangi , Jing - Yu Yang , David Zhang , Zhong Jin . 2005 . KPCA plus LDA : a complete kernel Fisher discriminant framework for feature extraction and recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 27 : 2 , 230 - 244 . [ CrossRef ] 222 . Daoqiang Zhang , Daoqiang Zhang , Songcan Chen , Keren Tan , Keren Tan . 2005 . Improving the Robustness of ? Online Agglomerative Clustering Method ? Based on Kernel - Induce Distance Measures . Neural Processing Letters 21 : 1 , 45 - 51 . [ CrossRef ] 223 . W . Zheng , L . Zhao , C . Zou . 2005 . Foley & # 8211 ; Sammon Optimal Discriminant Vectors Using Kernel Approach . IEEE Transactions on Neural Networks 16 : 1 , 1 - 9 . [ CrossRef ] 224 . Sang - Woon Kim , B . J . Oommen . 2005 . On utilizing search methods to select subspace dimensions for kernel - based nonlinear subspace classifiers . IEEE Transactions on Pattern Analysis and Machine Intelligence 27 : 1 , 136 - 141 . [ CrossRef ] 225 . Cheong Hee Park , Haesun Park . 2005 . Nonlinear Discriminant Analysis Using Kernel Functions and the Generalized Singular Value Decomposition . SIAM Journal on Matrix Analysis and Applications 27 : 1 , 87 . [ CrossRef ] 226 . P . Laskov , C . Schäfer , I . Kotenko , K . - R . Müller . 2004 . Intrusion Detection in Unlabeled Data with Quarter - sphere Support Vector Machines . PIK - Praxis der Informationsverarbeitung und Kommunikation 27 : 4 , 228 - 236 . [ CrossRef ] 227 . Rozenn Dahyot , Pierre Charbonnier , Fabrice Heitz . 2004 . A Bayesian approach to object detection using probabilistic appearance - based models . Pattern Analysis and Applications 7 : 3 , 317 - 332 . [ CrossRef ] 228 . J . T . - Y . Kwok , I . W . - H . Tsang . 2004 . The Pre - Image Problem in Kernel Methods . IEEE Transactions on Neural Networks 15 : 6 , 1517 - 1525 . [ CrossRef ] 229 . Yoshua Bengio , Olivier Delalleau , Nicolas Le Roux , Jean - François Paiement , Pascal Vincent , Marie Ouimet . 2004 . Learning Eigenfunctions Links Spectral Embedding and Kernel PCALearning Eigenfunctions Links Spectral Embedding and Kernel PCA . Neural Computation 16 : 10 , 2197 - 2219 . [ Abstract ] [ PDF ] [ PDF Plus ] 230 . S . Chen , D . Zhang . 2004 . Robust Image Segmentation Using FCM With Spatial Constraints Based on New Kernel - Induced Distance Measure . IEEE Transactions on Systems , Man and Cybernetics , Part B ( Cybernetics ) 34 : 4 , 1907 - 1916 . [ CrossRef ] 231 . Y . Engel , S . Mannor , R . Meir . 2004 . The Kernel Recursive Least - Squares Algorithm . IEEE Transactions on Signal Processing 52 : 8 , 2275 - 2285 . [ CrossRef ] 232 . P . - J . L ? Heureux , J . Carreau , Y . Bengio , O . Delalleau , S . Y . Yue . 2004 . Locally Linear Embedding for dimensionality reduction in QSAR . Journal of Computer - Aided Molecular Design 18 : 7 - 9 , 475 - 482 . [ CrossRef ] 233 . Wenming Zheng , Li Zhao , Cairong Zou . 2004 . A Modified Algorithm for Generalized Discriminant AnalysisA Modified Algorithm for Generalized Discriminant Analysis . Neural Computation 16 : 6 , 1283 - 1297 . [ Abstract ] [ PDF ] [ PDF Plus ] 234 . Chengjun Liu . 2004 . Gabor - based kernel pca with fractional power polynomial models for face recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 26 : 5 , 572 - 581 . [ CrossRef ] 235 . Ying Tan , Jun Wang . 2004 . A support vector machine with a hybrid kernel and minimal vapnik - chervonenkis dimension . IEEE Transactions on Knowledge and Data Engineering 16 : 4 , 385 - 395 . [ CrossRef ] 236 . Z . Liang , P . Shi . 2004 . Efficient algorithm for kernel discriminant analysis . Electronics Letters 40 : 25 , 1579 . [ CrossRef ] 237 . Q . Liu , H . Lu , S . Ma . 2004 . Improving Kernel Fisher Discriminant Analysis for Face Recognition . IEEE Transactions on Circuits and Systems for Video Technology 14 : 1 , 42 - 49 . [ CrossRef ] 238 . Matej Ore ? ? i ? ? , Clary B Clish , Eugene J Davidov , Elwin Verheij , Jack Vogels , Louis M Havekes , Eric Neumann , Aram Adourian , Stephen Naylor , Jan van der Greef , Thomas Plasterer . 2004 . Phenotype Characterisation Using Integrated Gene Transcript , Protein and Metabolite Profiling . Applied Bioinformatics 3 : 4 , 205 - 217 . [ CrossRef ] 239 . H . Choi , S . Choi . 2004 . Kernel Isomap . Electronics Letters 40 : 25 , 1612 . [ CrossRef ] 240 . Kai Huang , Robert F . Murphy . 2004 . From quantitative microscopy to automated image understanding . Journal of Biomedical Optics 9 : 5 , 893 . [ CrossRef ] 241 . V . Roth , J . Laub , M . Kawanabe , J . M . Buhmann . 2003 . Optimal cluster preserving embedding of nonmetric proximity data . IEEE Transactions on Pattern Analysis and Machine Intelligence 25 : 12 , 1540 - 1551 . [ CrossRef ] 242 . Yixin Chen , J . Z . Wang . 2003 . Support vector learning for fuzzy rule - based classification systems . IEEE Transactions on Fuzzy Systems 11 : 6 , 716 - 728 . [ CrossRef ] 243 . Julian Mintseris , Zhiping Weng . 2003 . Atomic contact vectors in protein - protein recognition . Proteins : Structure , Function , and Genetics 53 : 3 , 629 - 639 . [ CrossRef ] 244 . QingShan Liu , Rui Huang , HanQing Lu , SongDe Ma . 2003 . Kernel - based nonlinear discriminant analysis for face recognition . Journal of Computer Science and Technology 18 : 6 , 788 - 795 . [ CrossRef ] 245 . M . Girolami , Chao He . 2003 . Probability density estimation from optimally condensed data samples . IEEE Transactions on Pattern Analysis and Machine Intelligence 25 : 10 , 1253 - 1264 . [ CrossRef ] 246 . Qingshan Liu , Hanqing Lu , Songde Ma . 2003 . A non - parameter bayesian classifier for face recognition . Journal of Electronics ( China ) 20 : 5 , 362 - 370 . [ CrossRef ] 247 . Shihong Yue , Ping Li , Peiyi Hao . 2003 . SVM classification : Its contents and challenges . Applied Mathematics - A Journal of Chinese Universities 18 : 3 , 332 - 342 . [ CrossRef ] 248 . Guilherme de A . Barreto , Aluizio F . R . Araújo , Stefan C . Kremer . 2003 . A Taxonomy for Spatiotemporal Connectionist Networks Revisited : The Unsupervised CaseA Taxonomy for Spatiotemporal Connectionist Networks Revisited : The Unsupervised Case . Neural Computation 15 : 6 , 1255 - 1320 . [ Abstract ] [ PDF ] [ PDF Plus ] 249 . Mikhail Belkin , Partha Niyogi . 2003 . Laplacian Eigenmaps for Dimensionality Reduction and Data RepresentationLaplacian Eigenmaps for Dimensionality Reduction and Data Representation . Neural Computation 15 : 6 , 1373 - 1396 . [ Abstract ] [ PDF ] [ PDF Plus ] 250 . K . Muller , C . W . Anderson , G . E . Birch . 2003 . Linear and nonlinear methods for brain - computer interfaces . IEEE Transactions on Neural Systems and Rehabilitation Engineering 11 : 2 , 165 - 169 . [ CrossRef ] 251 . Stefan Harmeling , Andreas Ziehe , Motoaki Kawanabe , Klaus - Robert Müller . 2003 . Kernel - Based Nonlinear Blind Source SeparationKernel - Based Nonlinear Blind Source Separation . Neural Computation 15 : 5 , 1089 - 1124 . [ Abstract ] [ PDF ] [ PDF Plus ] 252 . S . Mika , G . Ratsch , J . Weston , B . Scholkopf , A . Smola , K . Muller . 2003 . Constructing descriptive and discriminative nonlinear features : rayleigh coefficients in kernel feature spaces . IEEE Transactions on Pattern Analysis and Machine Intelligence 25 : 5 , 623 - 628 . [ CrossRef ] 253 . J . A . K . Suykens , T . Van Gestel , J . Vandewalle , B . De Moor . 2003 . A support vector machine formulation to pca analysis and its kernel version . IEEE Transactions on Neural Networks 14 : 2 , 447 - 450 . [ CrossRef ] 254 . Juwei Lu , K . N . Plataniotis , A . N . Venetsanopoulos . 2003 . Face recognition using kernel direct discriminant analysis algorithms . IEEE Transactions on Neural Networks 14 : 1 , 117 - 126 . [ CrossRef ] 255 . D . Martinez , A . Bray . 2003 . Nonlinear blind source separation using kernels . IEEE Transactions on Neural Networks 14 : 1 , 228 - 235 . [ CrossRef ] 256 . F . Meinecke , A . Ziehe , M . Kawanabe , K . - R . Muller . 2002 . A resampling approach to estimate the stability of one - dimensional or multidimensional independent components . IEEE Transactions on Biomedical Engineering 49 : 12 , 1514 - 1525 . [ CrossRef ] 257 . Jiun - Hung Chen , Chu - Song Chen . 2002 . Fuzzy kernel perceptron . IEEE Transactions on Neural Networks 13 : 6 , 1364 - 1373 . [ CrossRef ] 258 . G . Ratsch , S . Mika , B . Scholkopf , K . - R . Muller . 2002 . Constructing boosting algorithms from SVMs : an application to one - class classification . IEEE Transactions on Pattern Analysis and Machine Intelligence 24 : 9 , 1184 - 1199 . [ CrossRef ] 259 . Kai Yu , Liang Ji . 2002 . Karyotyping of comparative genomic hybridization human metaphases using kernel nearest - neighbor algorithm . Cytometry 48 : 4 , 202 - 208 . [ CrossRef ] 260 . B . Moghaddam . 2002 . Principal manifolds and probabilistic subspaces for visual recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 24 : 6 , 780 - 788 . [ CrossRef ] 261 . T . Van Gestel , J . A . K . Suykens , G . Lanckriet , A . Lambrechts , B . De Moor , J . Vandewalle . 2002 . Bayesian Framework for Least - Squares Support Vector Machine Classifiers , Gaussian Processes , and Kernel Fisher Discriminant AnalysisBayesian Framework for Least - Squares Support Vector Machine Classifiers , Gaussian Processes , and Kernel Fisher Discriminant Analysis . Neural Computation 14 : 5 , 1115 - 1147 . [ Abstract ] [ PDF ] [ PDF Plus ] 262 . M . Girolami . 2002 . Mercer kernel - based clustering in feature space . IEEE Transactions on Neural Networks 13 : 3 , 780 - 784 . [ CrossRef ] 263 . Paul Pavlidis , Jason Weston , Jinsong Cai , William Stafford Noble . 2002 . Learning Gene Functional Classifications from Multiple Data Types . Journal of Computational Biology 9 : 2 , 401 - 411 . [ CrossRef ] 264 . Mark Girolami . 2002 . Orthogonal Series Density Estimation and the Kernel Eigenvalue ProblemOrthogonal Series Density Estimation and the Kernel Eigenvalue Problem . Neural Computation 14 : 3 , 669 - 688 . [ Abstract ] [ PDF ] [ PDF Plus ] 265 . Eisaku Maeda , Hiroshi Murase . 2002 . Kernel - Based Nonlinear Subspace Method for Pattern Recognition . Systems and Computers in Japan 33 : 1 , 38 - 52 . [ CrossRef ] 266 . Kwang In Kim , Keechul Jung , Hang Joon Kim . 2002 . Face recognition using kernel principal component analysis . IEEE Signal Processing Letters 9 : 2 , 40 . [ CrossRef ] 267 . Hujun Yin . 2002 . ViSOM - a novel method for multivariate data projection and structure visualization . IEEE Transactions on Neural Networks 13 : 1 , 237 . [ CrossRef ] 268 . T . Van Gestel , J . A . K . Suykens , D . - E . Baestaens , A . Lambrechts , G . Lanckriet , B . Vandaele , B . De Moor , J . Vandewalle . 2001 . Financial time series prediction using least squares support vector machines within the evidence framework . IEEE Transactions on Neural Networks 12 : 4 , 809 - 821 . [ CrossRef ] 269 . Koji Tsuda . 2001 . The subspace method in Hilbert space . Systems and Computers in Japan 32 : 6 , 55 - 61 . [ CrossRef ] 270 . Kristin K . Jerger , Theoden I . Netoff , Joseph T . Francis , Timothy Sauer , Louis Pecora , Steven L . Weinstein , Steven J . Schiff . 2001 . Early Seizure Detection . Journal of Clinical Neurophysiology 18 : 3 , 259 - 268 . [ CrossRef ] 271 . Roman Rosipal , Mark Girolami . 2001 . An Expectation - Maximization Approach to Nonlinear Component AnalysisAn Expectation - Maximization Approach to Nonlinear Component Analysis . Neural Computation 13 : 3 , 505 - 510 . [ Abstract ] [ PDF ] [ PDF Plus ] 272 . K . - R . Muller , S . Mika , G . Ratsch , K . Tsuda , B . Scholkopf . 2001 . An introduction to kernel - based learning algorithms . IEEE Transactions on Neural Networks 12 : 2 , 181 - 201 . [ CrossRef ] 273 . A . Ruiz , P . E . Lopez - de - Teruel . 2001 . Nonlinear kernel - based statistical pattern analysis . IEEE Transactions on Neural Networks 12 : 1 , 16 . [ CrossRef ] 274 . R . C . Williamson , A . J . Smola , B . Scholkopf . 2001 . Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators . IEEE Transactions on Information Theory 47 : 6 , 2516 . [ CrossRef ] 275 . A . Navia - Vazquez , F . Perez - Cruz , A . Artes - Rodriguez , A . R . Figueiras - Vidal . 2001 . Weighted least squares training of support vector classifiers leading to compact and adaptive schemes . IEEE Transactions on Neural Networks 12 : 5 , 1047 . [ CrossRef ] 276 . K . I . Kim , S . H . Park , H . J . Kim . 2001 . Kernel principal component analysis for texture classification . IEEE Signal Processing Letters 8 : 2 , 39 . [ CrossRef ] 277 . G . Baudat , F . Anouar . 2000 . Generalized Discriminant Analysis Using a Kernel ApproachGeneralized Discriminant Analysis Using a Kernel Approach . Neural Computation 12 : 10 , 2385 - 2404 . [ Abstract ] [ PDF ] [ PDF Plus ] 278 . Bernhard Schölkopf , Alex J . Smola , Robert C . Williamson , Peter L . Bartlett . 2000 . New Support Vector AlgorithmsNew Support Vector Algorithms . Neural Computation 12 : 5 , 1207 - 1245 . [ Abstract ] [ PDF ] [ PDF Plus ] 279 . G . Wubbeler , A . Ziehe , B . - M . Mackert , K . - R . Muller , L . Trahms , C . Curio . 2000 . Independent component analysis of noninvasively recorded cortical magnetic DC - fields in humans . IEEE Transactions on Biomedical Engineering 47 : 5 , 594 - 599 . [ CrossRef ] 280 . R . Lotlikar , R . Kothari . 2000 . Bayes - optimality motivated linear and multilayered perceptron - based dimensionality reduction . IEEE Transactions on Neural Networks 11 : 2 , 452 - 463 . [ CrossRef ] 281 . Azriel Rosenfeld , Harry Wechsler . 2000 . Pattern recognition : Historical perspective and future directions . International Journal of Imaging Systems and Technology 11 : 2 , 101 - 116 . [ CrossRef ] 282 . A . Ziehe , K . - R . Muller , G . Nolte , B . - M . Mackert , G . Curio . 2000 . Artifact reduction in magnetoneurography based on time - delayed second - order correlations . IEEE Transactions on Biomedical Engineering 47 : 1 , 75 . [ CrossRef ] 283 . K . I . Kim , K . Jung , S . H . Park , H . J . Kim . 2000 . Texture classification with kernel principal component analysis . Electronics Letters 36 : 12 , 1021 . [ CrossRef ] 284 . M . Brown , H . G . Lewis , S . R . Gunn . 2000 . Linear spectral mixture models and support vector machines for remote sensing . IEEE Transactions on Geoscience and Remote Sensing 38 : 5 , 2346 . [ CrossRef ] 285 . A . K . Jain , P . W . Duin , Jianchang Mao . 2000 . Statistical pattern recognition : a review . IEEE Transactions on Pattern Analysis and Machine Intelligence 22 : 1 , 4 . [ CrossRef ] 286 . Epifanio Bagarinao , K . Pakdaman , Taishin Nomura , Shunsuke Sato . 1999 . Reconstructing bifurcation diagrams from noisy time series using nonlinear autoregressive models . Physical Review E 60 : 1 , 1073 - 1076 . [ CrossRef ] 287 . D . J . H . Wilson , G . W . Irwin , G . Lightbody . 1999 . RBF principal manifolds for process monitoring . IEEE Transactions on Neural Networks 10 : 6 , 1424 . [ CrossRef ] 288 . B . Scholkopf , S . Mika , C . J . C . Burges , P . Knirsch , K . - R . Muller , G . Ratsch , A . J . Smola . 1999 . Input space versus feature space in kernel - based methods . IEEE Transactions on Neural Networks 10 : 5 , 1000 . [ CrossRef ] 289 . M . A . Hearst , S . T . Dumais , E . Osman , J . Platt , B . Scholkopf . 1998 . Support vector machines . IEEE Intelligent Systems 13 : 4 , 18 - 28 . [ CrossRef ] 290 . Wenyi ZhaoFace Recognition Techniques . [ CrossRef ] 291 . George MichailidisPrincipal Components and Extensions . [ CrossRef ]