What Gestures Do Users with Visual Impairments Prefer to Interact with Smart Devices ? And How Much We Know About It Abstract We examine gestures that people with visual impairments define and would like to use to control interactive devices and systems . To this end , we perform a systematic search of the literature on gesture elicitation consisting of 249 papers published between 1994 and 2019 , from which we identify 12 studies ( 4 . 8 % ) that explicitly elicited gesture preferences from users with visual impairments and / or examined the consistency of their gesture articulations . We compile a set of 53 user - defined touch , motion , mid - air , and stroke - gestures to effect 44 functions on smartphones , TVs , and tangible UIs . We point to several lacunae in our community’s current knowledge of gestures preferred by users with visual impairments . Introduction As gesture user interfaces are implemented for more and more interactive systems and contexts of use , their accessibility for users with various abilities needs to be rigorously addressed . Touch , the prevalent input modality for smartphones [ 28 ] , requires visuomotor coordination that is challenging for users with low vision [ 22 ] , not to mention people who are blind . In this context , new input techniques [ 3 , 9 , 13 ] , gesture recognizers [ 29 ] , data synthesis approaches [ 12 ] , and Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the Owner / Author . DIS ' 20 Companion , July 6 - 10 , 2020 , Eindhoven , Netherlands . © 2020 Copyright is held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 7987 - 8 / 20 / 07 . https : / / doi . org / 10 . 1145 / 3393914 . 3395896 Radu - Daniel Vatavu MintViz Lab | MANSiD Research Center University Ștefan cel Mare of Suceava Suceava 720229 , Romania radu . vatavu @ usm . ro Jean Vanderdonckt Université catholique de Louvain ( UCLouvain ) Louvain - la - Neuve , Belgium jean . vanderdonckt @ uclouvain . be Author Keywords Visual impairments ; Low vision ; User - defined gestures . CSS Concepts • Human - centered computing ~ Gestural input . Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 85 multimodal UIs [ 5 ] are needed to make touchscreen devices accessible to users with visual impairments . Gesture elicitation studies ( GES ) [ 38 , 39 ] are a practical tool for designers to understand users’ gesture preferences to control interactive systems ; see the side bar for an overview . Unfortunately , only a handful of such studies have been conducted for users with visual impairments despite the popularity of the method : from the 249 GES papers identified for this work , only 12 ( 4 . 8 % ) focused on users with visual impairments , of which just 4 ( 1 . 6 % ) actually reported a gesture set to guide designers in their work . In this context , more research is needed to understand and document the gestures preferred by users with visual impairments for a variety of interactive devices , applications , and contexts of use . In this paper , we conduct a systematic search and analysis of GES studies for users with visual impairments , and compile a vocabulary of 53 touch , motion , mid - air , and stroke gestures to effect 44 functions on smartphones , smart TVs , and tangible UIs . Research Method To identify GES studies with participants with visual impairments , we ran the query “ ( Gesture AND ( Elicitation OR Guessability ) AND Study ) ” on six digital libraries : ACM DL , IEEE Xplore , Science Direct , Springer Link , DBLP , and Google Scholar ; see [ 35 ] . After removing duplicates and irrelevant papers , and running a snowballing procedure , we ended up with 249 papers that elicited gestures or applied parts of the GES method , e . g . , agreement measures , for gesture analysis . From these papers , we identified 12 studies ( 4 . 8 % ) involving participants with visual impairments . We analyzed these studies to extract information for meta - analysis ( e . g . , the number of participants ) and user - defined mappings between referents and gestures . Results : Meta - Analysis Table 1 lists the 12 papers identified by our systematic search . Of these , [ 15 ] and [ 16 ] report the same study , and [ 30 ] is an extension of [ 29 ] with more participants ( 54 vs . 20 ) and a different goal . We grouped these papers into four categories , as follows :  Six papers [ 6 , 7 , 10 , 14 , 19 , 23 ] applied the GES method directly , and are marked with the “GES” acronym in Table 1 , third column .  One paper [ 36 ] used brainstorming [ 11 ] ( marked “B” in Table 1 ) to implement gesture elicitation .  Two papers [ 15 , 16 ] conducted a rating / ranking elicitation ( “R / R” ) of predefined hand gestures .  Three papers [ 4 , 29 , 30 ] performed gesture collection ( “GC” ) without any referents but reported gesture agreement ( consistency [ 2 ] ) results . The number of participants with visual impairments varied between 8 and 36 ( M = 19 . 1 , SD = 9 . 6 ) , and most studies ( 8 of 12 ) addressed users who are blind . The number of referents varied from 6 to 31 ( M = 18 . 9 , SD = 8 . 6 ) . Overall , the GES and B - type studies collected between 56 and 880 gestures ( M = 314 . 8 , SD = 290 . 0 ) , but only four studies [ 6 , 7 , 23 , 36 ] actually reported consensus gesture sets . Most of the studies provided design guidelines for gesture UIs for users with visual impairments , while a few [ 6 , 23 , 29 , 36 ] implemented actual prototypes . Only three studies [ 10 , 29 , 30 ] involved users without visual impairments as a control group , while three papers [ 6 , 7 , 19 ] connected their findings to those reported by previous GES studies : [ 6 ] with [ 20 ] , [ 19 ] with [ 10 ] , and [ 7 ] with [ 25 ] . Overview of gesture elicitation studies The goal of a gesture elicitation study [ 39 ] is to collect , analyze , and document the gesture preferences of the representative end users of a specific gesture - based UI , application , system , or interactive prototype . A gesture elicitation study implements several steps [ 38 , 39 ] : 1 . Participants are presented with the effect of a system function , e . g . , a map displayed by an interactive tabletop zooms out . 2 . Participants are asked for the action that could generate the effect they have just witnessed . The effect is called the “referent” and actions are called “signs . ” 3 . The researcher ( designer , experimenter , etc . ) analyzes the gestures to determine their level of agreement . 4 . Gestures that receive high agreement are compiled into a “consensus gesture set” reflective of end users’ behavior and preferences for gesture input in relation to the specific device , application , or system investigated in the study . Variations of this procedure are possible , e . g . , in terms of how many gestures are elicited from the same participant [ 18 , 31 ] or the measure / method to compute the level of agreement for the elicited gestures [ 1 , 17 , 26 , 31 ] . Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 86 Results : User - Defined Gestures From the GES and B - type studies that reported gesture sets [ 6 , 7 , 23 , 36 ] , we extracted gestures with high consensus subsuming 44 referents and 53 gestures for smartphones , smart TVs , and tangible UIs . We compiled these results into a dictionary ( see Table 2 ) with the goal to structure and synthesize prior work on gestures elicited from users with visual impairments ( i . e . , what gestures were preferred in the specific context set by those studies ? ) and to accompany designer - proposed touch input , e . g . , “Slide rule” [ 9 ] , “BrailleSketch” [ 13 ] , or “Perkinput” [ 3 ] , among others . Table 2 : Gestures with high consensus proposed by users with visual impairments [ 6 , 7 , 23 , 36 ] ; also see Tables 3 and 4 . Note : different colors indicate different gesture types : touch ( green ) , motion ( magenta ) , mid - air ( black ) , and stroke gestures ( blue ) . Smartphone input Answer call : bring phone to ear [ 6 ] ; double - tap with ear [ 36 ] Hang up call : remove phone from ear [ 6 ] ; ear swipe [ 36 ] Ignore call : cover the phone with the hand [ 6 ] ; ear swipe [ 36 ] Voice search : bring phone to mouth [ 6 ] ; long ear press [ 36 ] Call missed call : shake front - back and bring phone to ear [ 6 ] Select : shake front - back [ 6 ] ; ear double tap [ 36 ] Home screen : turn phone back and front [ 6 ] Table 1 : Overview of the studies identified by our systematic search procedure that elicited gesture preferences or gesture articulations from participants with visual impairments . Study Study type Gesture types Device / Context User category Participants Num . refer ents Num . elicited gestures Consens us ges - ture set Goal of the study / outcome # M / F Age [ yrs . ] Mean ( SD ) Control group 1 1 Kane et al . , 2011 [ 10 ] GES touch , stroke - gestures 10” tablet Blind 10 4 / 6 49 ( 12 . 2 ) Yes / 10 22 880 n / a Design guidelines 2 Dim and Ren , 2014 [ 6 ] GES motion gestures smart phone Blind 13 9 / 4 61 ( 16 . 91 ) No , [ 20 ] 15 195 Yes Implemented user interface 3 Romano et al . , 2015 [ 19 ] GES touch , motion , stroke - gestures iPhone 4S Blind 8 4 / 4 61 ( 11 . 3 ) No , [ 10 ] 19 278 n / a Empirical study 4 Luthra and Ghosh , 2015 [ 14 ] GES touch and stroke - gestures smart phone Blind 12 10 / 2 31 ( 9 . 6 ) No 25 300 n / a Design guidelines 5 Dim et al . , 2016 [ 7 ] GES mid - air gestures TV Blind 12 7 / 5 53 . 9 ( 12 . 64 ) No , [ 25 ] 15 180 Yes Guidelines , methodology 6 Shi et al . , 2017 [ 23 ] GES tangible input 3D printing Blind 12 4 / 8 40 . 8 ( 13 . 2 ) No 6 56 Yes Guidelines , proposal 7 Modanwal and Sarawadekar , 2018 [ 15 ] R / R free - hand gestures n / a Blind 25 25 21 . 4 ( 2 . 3 ) No 31 ( 2 ) 1 , 550 n / a Prototype 8 Modanwal and Sarawadekar , 2018 [ 16 ] R / R free - hand gestures n / a Blind 25 25 21 . 4 ( 2 . 3 ) No 31 ( 2 ) 1 , 550 n / a Prototype 9 Wang et al . , 2019 [ 36 ] B touch gestures ( with the ear ) smart phone Blind , low vision 30 20 / 10 n / a No 8 n / a Yes EarTouch prototype 10 Buzzi et al . , 2017 [ 4 ] GC touch and stroke - gestures smart phone Blind , low vision 36 22 / 14 F 25 ( 14 . 3 ) M 50 ( 16 . 8 ) No 25 ( 3 ) 812 n / a Design guidelines 11 Vatavu , 2017 [ 29 ] GC stroke - gestures 10” tablet Low vision 10 6 / 4 35 . 7 ( 11 . 7 ) Yes / 10 12 ( 3 ) 2 , 400 n / a $ P + recognizer 12 Vatavu et al . 2018 [ 30 ] GC stroke - gestures 10” tablet Low vision 27 12 / 15 29 . 1 ( 12 . 3 ) Yes / 27 12 ( 3 ) 6 , 562 n / a Design guidelines 1 Comparisons were conducted with participants without visual impairments ( control group ) . 2 Not referents , but hand postures for which participants expressed their preferences . 3 Not referents , but gesture types that participants articulated during the study and their gestures were analyzed using measures of agreement ( consistency ) following Anthony et al . [ 2 ] . Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 87 Next / Previous : flip phone right / left along vertical axis [ 6 ] Pan : move phone to left , right , up , down [ 6 ] Pause : cover the phone with the hand [ 6 ] Power off : sweep hand on the phone from top to bottom [ 6 ] Get caller info : tap with the ear [ 36 ] Get time / battery level : ear tap at the top / bottom screen [ 36 ] Mode switch : press with the ear [ 36 ] Navigate : ear swipe [ 36 ] Explore : free - form gesture with ear [ 36 ] Show menu : rotate phone while pressed onto the ear to activate the menu and swipe to navigate menu items [ 36 ] Continuous input : rotate phone while pressed onto the ear [ 36 ] TV control Turn TV on / off : perform tap in mid - air [ 7 ] Play / pause TV : open / close palm [ 7 ] Turn volume up / down : move palm upward / downward [ 7 ] Change TV channel : hand rotates imaginary knob in mid - air [ 7 ] Show TV guide : draw circle in mid - air [ 7 ] Go to favorite TV channel : clap hands [ 7 ] Show TV Menu : hands separate like opening a book [ 7 ] Answer No : the index fingers of the two hands form a cross [ 7 ] Answer Yes : the “OK” emblematic hand pose [ 7 ] Go to previous / next channel : swipe hand left / right [ 7 ] Voice guide : move both hands to ears [ 7 ] Tangible interactions Get general information : press / touch with index finger [ 23 ] Select element and get its name : index finger taps once / presses / touches the element [ 23 ] Select sub - area of element and get its name : index finger swipes on the area [ 23 ] Get more information : index finger swipes ; tap / tap and hold with the index finger [ 23 ] Record / retrieve notes : index finger taps twice [ 23 ] Conclusion and Future Work We examined gestures proposed by people with visual impairments reported by end - user elicitation studies . We found that 12 ( 4 . 8 % ) of the 249 GES / GES - related studies addressed users with visual impairments , of which only 4 studies ( 1 . 6 % ) actually reported gesture sets . We used the results of our analysis to compile a gesture - to - function vocabulary for smartphones , smart TVs , and tangible UIs . While this vocabulary is useful to inform design of new gesture UIs , it also shows many lacunae , which need to be addressed by future work , such as regarding whole - body [ 24 , 25 ] , and on - body gestures [ 34 ] , free - hand gesture input [ 33 ] , gestures for smart watches , smart rings [ 8 ] , and smart glasses [ 21 ] as well as the need to explore a more diverse set of system functions beyond smartphones and TVs . While some of the gestures from Table 2 could be transferred to other contexts of use [ 32 ] ( e . g . , touch gestures to smart watches with touchscreens ; mid - air gestures to smart glasses with embedded video cameras and to rings with embedded motion sensors ) , conducting actual GES studies for these devices and contexts of use is recommendable . More studies are also recommended to compare the gesture preferences of users with and without visual impairments via between - subjects experiments [ 27 ] or that use other methods [ 1 , 18 , 31 ] . Consolidation of the vocabulary from Table 2 with replications [ 37 ] is equally needed . We hope that our focused survey and structuring of current knowledge regarding the gestures defined by users with visual impairments will foster more research on accessible gesture - based UIs . References [ 1 ] A . X . Ali , M . R . Morris , J . O . Wobbrock . 2018 . Crowdsourcing Similarity Judgments for Agreement Analysis in End - User Elicitation Studies . UIST ’18 . https : / / doi . org / 10 . 1145 / 3242587 . 3242621 [ 2 ] L . Anthony , R . - D . Vatavu , J . O . Wobbrock . 2013 . Understanding the Consistency of Users ' Pen and Table 3 : Overview of the gesture types reported in the literature of end - user elicitation studies that were proposed by users with visual impairments ; see Table 2 for details about these studies . Gesture Count Refs . Touch 18 [ 6 , 23 , 36 ] Motion ( device ) 12 [ 6 ] Mid - air 14 [ 7 ] Stroke gestures 9 [ 6 , 23 , 36 ] Total 53 Table 4 : Overview of the number of gestures per device type ; see Table 2 for details . Device Count Refs . Smart phone 28 [ 6 , 36 ] Smart TV 14 [ 7 ] TUIs 11 [ 23 ] Total 53 Acknowledgements This work was supported by a grant of Ministry of Research and Innovation , CNCS - UEFISCDI , PN - III - P1 - 1 . 1 - TE - 2016 - 2173 ( TE141 / 2018 ) , within PNCDI III . Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 88 Finger Stroke Gesture Articulation . GI ' 13 , 87 - 94 . https : / / dl . acm . org / doi / 10 . 5555 / 2532129 . 2532145 [ 3 ] S . Azenkot , J . O . Wobbrock , S . Prasain , R . E . Ladner . 2012 . Input finger detection for nonvisual touch screen text entry in Perkinput . GI ’12 , 121 – 129 . https : / / dl . acm . org / doi / 10 . 5555 / 2305276 . 2305297 [ 4 ] M . C . Buzzi , M . Buzzi , B . Leporini , A . Trujillo . 2017 . Analyzing Visually Impaired People’s Touch Gestures on Smartphones . Multimed . Tools Appl . 76 ( 4 ) . https : / / doi . org / 10 . 1007 / s11042 - 016 - 3594 - 9 [ 5 ] W . Grussenmeyer , E . Folmer . 2017 . Accessible Touchscreen Technology for People with Visual Impairments : A Survey . ACM TACCESS , Article 6 . https : / / doi . org / 10 . 1145 / 3022701 [ 6 ] N . K . Dim , X . Ren . 2014 . Designing Motion Gesture Interfaces in Mobile Phones for Blind People . J . of Comp . Science and Technology 29 ( 5 ) , 812 - 824 . https : / / doi . org / 10 . 1007 / s11390 - 014 - 1470 - 5 [ 7 ] N . K . Dim , C . Silpasuwanchai , S . Sarcar , X . Ren . 2016 . Designing Mid - Air TV Gestures for Blind People Using User - and Choice - Based Elicitation Approaches . DIS ’16 , 204 – 214 . https : / / doi . org / 10 . 1145 / 2901790 . 2901834 [ 8 ] B . F . Gheran , J . Vanderdonckt , R . - D . Vatavu . 2018 . Gestures for Smart Rings : Empirical Results , Insights , and Design Implications . DIS ' 18 , 623 - 635 . https : / / doi . org / 10 . 1145 / 3196709 . 3196741 [ 9 ] S . K . Kane , J . P . Bigham , J . O . Wobbrock . 2008 . Slide Rule : Making Mobile Touch Screens Accessible to Blind People Using Multi - Touch Interaction Techniques . ASSETS ’08 , 73 – 80 . https : / / doi . org / 10 . 1145 / 1414471 . 1414487 [ 10 ] S . K . Kane , J . O . Wobbrock , R . E . Ladner . 2011 . Usable Gestures for Blind People : Understanding Preference and Performance . CHI ’11 . https : / / doi . org / 10 . 1145 / 1978942 . 1979001 [ 11 ] A . Kunz , K . Miesenberger , M . Mühlhäuser , A . Alavi , S . Pölzer , D . Pöll , P . Heumader , D . Schnelle - Walka . 2014 . Accessibility of Brainstorming Sessions for Blind People . ICCHP ’14 , 237 – 244 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 08596 - 8 _ 38 [ 12 ] L . A . Leiva , D . Martín - Albo , R . - D . Vatavu . 2017 . Synthesizing Stroke Gestures Across User Populations : A Case for Users with Visual Impairments . CHI ' 17 , 4182 - 4193 . http : / / dx . doi . org / 10 . 1145 / 3025453 . 3025906 [ 13 ] M . Li , M . Fan , K . N . Truong . 2017 . BrailleSketch : A Gesture - based Text Input Method for People with Visual Impairments . ASSETS ’17 , 12 – 21 . https : / / doi . org / 10 . 1145 / 3132525 . 3132528 [ 14 ] V . Luthra , S . Ghosh . 2015 . Understanding , Evaluat - ing and Analyzing Touch Screen Gestures for Visu - ally Impaired Users in Mobile Environment . UAHCI ’15 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 20681 - 3 _ 3 [ 15 ] G . Modanwal , K . Sarawadekar . 2018 . A Gesture Elicitation Study with Visually Impaired Users . HCI ’18 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 92279 - 9 _ 7 [ 16 ] G . Modanwal , K . Sarawadekar . 2018 . A New Dactylology and Interactive System Development for Blind – Computer Interaction . IEEE THMS 48 ( 2 ) . http : / / dx . doi . org / 0 . 1109 / THMS . 2017 . 2734065 [ 17 ] M . R . Morris . 2012 . Web on the Wall : Insights from a Multimodal Interaction Elicitation Study . ITS ’12 . https : / / doi . org / 10 . 1145 / 2396636 . 2396651 [ 18 ] M . R . Morris , A . Danielescu , S . Drucker , D . Fisher , B . Lee , m . c . schraefel , J . O . Wobbrock . 2014 . Reducing Legacy Bias in Gesture Elicitation Studies . interactions 21 . https : / / doi . org / 10 . 1145 / 2591689 [ 19 ] M . Romano , A . Belluci , I . Aedo . 2015 . Understanding Touch and Motion Gestures for Blind People on Mobile Devices . INTERACT ’15 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 22701 - 6 _ 3 [ 20 ] J . Ruiz , Y . Li , E . Lank . 2011 . User - Defined Motion Gestures for Mobile Interaction . CHI ’11 . https : / / doi . org / 10 . 1145 / 1978942 . 1978971 [ 21 ] P . Rusu , M . D . Schipor , R . - D . Vatavu . 2019 . A Lead - In Study on Well - Being , Visual Functioning , and Desires for Augmented Reality Assisted Vision for Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 89 People with Visual Impairments . EHB ' 19 , 6 pages . http : / / dx . doi . org / 10 . 1109 / EHB47216 . 2019 . 8970074 [ 22 ] M . D . Schipor , R . - D . Vatavu . 2017 . Coping Strategies of People with Low Vision for Touch Input : A Lead - in Study . EHB ' 17 , 357 - 360 . http : / / dx . doi . org / 10 . 1109 / EHB . 2017 . 7995435 [ 23 ] L . Shi , Y . Zhao , S . Azenkot . 2017 . Designing Interactions for 3D Printed Models with Blind People . ASSETS ’17 , 200 – 209 . https : / / doi . org / 10 . 1145 / 3132525 . 3132549 [ 24 ] J . Vanderdonckt , N . Magrofuoco , S . Kieffer , J . Pérez , Y . Rase , P . Roselli , S . Villarreal . 2019 . Head and Shoulders Gestures : Exploring User - Defined Gestures with Upper Body . HCI ( 19 ) , 192 - 213 . https : / / doi . org / 10 . 1007 / 978 - 3 - 030 - 23541 - 3 _ 15 [ 25 ] R . - D . Vatavu . 2012 . User - Defined Gestures for Free - Hand TV Control . EuroITV ’12 , 45 – 48 . https : / / doi . org / 10 . 1145 / 2325616 . 2325626 [ 26 ] R . - D . Vatavu , J . O . Wobbrock . 2015 . Formalizing Agreement Analysis for Elicitation Studies : New Measures , Significance Test , and Toolkit . CHI ’15 . https : / / doi . org / 10 . 1145 / 2702123 . 2702223 [ 27 ] R . - D . Vatavu , J . O . Wobbrock . 2016 . Between - Subjects Elicitation Studies : Formalization and Tool Support . CHI ’16 , 3390 – 3402 . https : / / doi . org / 10 . 1145 / 2858036 . 2858228 [ 28 ] R . - D . Vatavu . 2017 . Visual Impairments and Mobile Touchscreen Interaction : State - of - the - Art , Causes of Visual Impairment , and Design Guidelines . IJHCI 33 . http : / / dx . doi . org / 10 . 1080 / 10447318 . 2017 . 1279827 [ 29 ] R . - D . Vatavu . 2017 . Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision . CHI ’17 , 4667 – 4679 . https : / / doi . org / 10 . 1145 / 3025453 . 3025941 [ 30 ] R . - D . Vatavu , B . F . Gheran , M . D . Schipor . 2018 . The Impact of Low Vision on Touch - Gesture Articulation on Mobile Devices . Perv . Comp . 17 ( 1 ) . https : / / doi . org / 10 . 1109 / MPRV . 2018 . 011591059 [ 31 ] R . - D . Vatavu . 2019 . The Dissimilarity - Consensus Approach to Agreement Analysis in Gesture Elicitation Studies . CHI ’19 , Paper 224 . https : / / doi . org / 10 . 1145 / 3290605 . 3300454 [ 32 ] R . - D . Vatavu . 2017 . Characterizing Gesture Knowledge Transfer Across Multiple Contexts of Use . J . on Multimodal User Interfaces 11 ( 4 ) , 301 - 314 . https : / / doi . org / 10 . 1007 / s12193 - 017 - 0247 - x [ 33 ] R . - D . Vatavu , I . A . Zaiți . 2014 . Leap Gestures for TV : Insights from an Elicitation Study . TVX ' 14 . http : / / doi . acm . org / 10 . 1145 / 2602299 . 2602316 [ 34 ] R . - D . Vatavu . 2017 . Smart - Pockets : Body - Deictic Gestures for Fast Access to Personal Data during Ambient Interactions . IJHCS 103 , 1 - 21 . http : / / dx . doi . org / 10 . 1016 / j . ijhcs . 2017 . 01 . 005 [ 35 ] S . Villarreal , J . Vanderdonckt , R . - D . Vatavu , J . O . Wobbrock . 2020 . A Systematic Review of Gesture Elicitation Studies : What Can We Learn from 216 Studies ? DIS ’20 . https : / / doi . org / 10 . 1145 / 3357236 . 3395511 [ 36 ] R . Wang , C . Yu , X . - D . Yang , W . He , Y . Shi . 2019 . EarTouch : Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios . CHI ’19 . https : / / doi . org / 10 . 1145 / 3290605 . 3300254 [ 37 ] M . Wilson , W . Mackay , E . Chi , M . Bernstein , J . Nichols . 2012 . RepliCHI SIG : From a Panel to a New Submission Venue for Replication . CHI ’12 EA . https : / / doi . org / 10 . 1145 / 2212776 . 2212419 [ 38 ] J . O . Wobbrock , M . R . Morris , A . Wilson . 2009 . User - Defined Gestures for Surface Computing . CHI ’09 . https : / / doi . org / 10 . 1145 / 1518701 . 1518866 [ 39 ] J . O . Wobbrock , H . H . Aung , B . Rothrock , B . A . Myers . 2005 . Maximizing the Guessability of Symbolic Input . CHI ’05 EA , 1869 – 1872 . https : / / doi . org / 10 . 1145 / 1056808 . 1057043 Provocations and WIPS : Assistive Technologies , Resilience and Social Inclusion DIS ’20 Companion , July 6 – 10 , 2020 , Eindhoven , Netherlands 90