ORIGINAL ARTICLE Deﬁning and predicting troll vulnerability in online social media Paraskevas Tsantarliotis 1 • Evaggelia Pitoura 1 • Panayiotis Tsaparas 1 Received : 20 December 2016 / Revised : 23 April 2017 / Accepted : 8 June 2017 (cid:2) Springer - Verlag GmbH Austria 2017 Abstract Trolling describes a range of antisocial online behaviors that aim at disrupting the normal operation of online social networks and media . Existing approaches to combating trolling rely on human - based or automatic mechanisms for identifying trolls and troll posts . In this paper , we take a novel approach to the problem : our goal is to identify troll vulnerable posts , that is , posts that are potential targets of trolls , so as to prevent trolling before it happens . To this end , we deﬁne three natural axioms that a troll vulnerability metric must satisfy and introduce metrics that satisfy them . We then deﬁne the troll vulnerability prediction problem , where given a post we aim at pre - dicting whether it is vulnerable to trolling . We construct models that use features from the content and the history of the post for the prediction . Our experiments with real data from Reddit demonstrate that our approach is successful in identifying a large fraction of the troll vulnerable posts . 1 Introduction Online social media and networks have emerged as the principal forum for the public discourse . Billions of users from diverse cultures and backgrounds participate in online social networks ( e . g . , Facebook ) , microblogging services ( e . g . , Twitter ) , or discussion forums ( e . g . , Reddit ) , where they engage in discussions and exchange opinions on all possible topics , creating a dialog at a global scale . How - ever , this open global forum is threatened by users that actively try to undermine its operation . Such users engage in discussions without the intention of constructively con - tributing to the dialog , but rather to disrupt it . They act as agents of chaos on the Internet , and they are commonly referred to as trolls . Trolling is an inclusive term that characterizes different types of disruptive online behavior ranging from off - topic joking comments to offensive and threatening behavior . Different from spammers , trolls do not aim at a ﬁnancial gain ; creating disarray is actually a goal in itself . Typical examples of trolling behavior include mocking and dis - crediting discussion participants , inciting and escalating arguments , and impersonating expert users while spreading bad advice and false information . Trolling is a serious issue that undermines the operation of social networks and media , and their role as a global channel of communication . Thus , combating trolls is a top priority for all major user - engagement portals . Some of the largest social networks have deployed user - driven mecha - nisms to detect trolling behavior , where users report abu - sive behavior to the system , and moderators suspend , ban , or remove the perpetrators from the community ( see for example Atwood 2011 ) . Even when successful , troll detection does not fully address the problem . First , trolls are very good at working the system , for example , getting around bans by using different usernames , or masking the content of their postings ( see for example Jeong 2014 ) . More importantly , all these measures are reactive : they are usually applied after a defamatory , threatening , or mis - leading comment has already been posted . In many cases this is too late ; the damage is already done . & Panayiotis Tsaparas tsap @ cs . uoi . gr Paraskevas Tsantarliotis ptsantar @ cs . uoi . gr Evaggelia Pitoura pitoura @ cs . uoi . gr 1 Department of Computer Science and Engineering , University of Ioannina , Ioannina , Greece 123 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 DOI 10 . 1007 / s13278 - 017 - 0445 - 2 In this paper , we take a different approach to addressing trolling . Instead of detecting trolls , we focus on identifying possible targets of trolls , that is , troll vulnerable posts . We ﬁrst introduce troll vulnerability ( TV ) metrics that quantify the vulnerability of a post based on the amount of trolling and non - trolling activity that followed the post . We provide three natural axioms that a troll vulnerability metric must satisfy , and show that our metrics fulﬁll them . We then deﬁne the troll vulnerability prediction task , where , given a post , our goal is to predict whether it will be targeted by trolls . We build classiﬁcation models for this task that use features of the post and its history to predict its vulnerability . Our approach has several advantages , compared to tra - ditional troll detection mechanisms . Modeling troll vulner - ability offers valuable insights into what makes a post susceptible to trolling behavior . Although the characteristics of trolls have been studied in detail , there is little under - standing about what makes a post a troll target . Troll vul - nerability metrics also offer a way to measure the severity of the troll activity with respect to a post . This is useful for monitoring the health of the system . It can also be applied for trolling posts for measuring and predicting trolling escalation . Finally , vulnerability prediction is a pro - active tool against the trolls . Rather than detecting and removing trolls after they occur , we try to anticipate the troll activity and take preventive actions to eliminate it before it appears . In summary , in this paper we make the following contributions . • We deﬁne the novel problem of troll vulnerability prediction , where we want to predict if a post is likely to become the victim of a troll attack . To the best of our knowledge , we are the ﬁrst to consider this problem . • We propose troll vulnerability metrics for quantifying the vulnerability of a post to trolls . We deﬁne a set of axioms that we want our metrics to satisfy . • We build classiﬁcation models for predicting troll vulnerability . Our models explore features that use the content of the post , the properties of the user that posted the content , as well as the history of the post in the discussion tree . We investigate the importance of the different features in the prediction task . • We evaluate our approach using a real dataset from Reddit . We demonstrate that our model is able to recall a large fraction of the vulnerable posts with overall high accuracy . The rest of the paper is structured as follows . Section 2 reviews related work in trolling . In Sect . 3 , we introduce the troll vulnerability metrics , while in Sect . 4 , we deﬁne the classiﬁer for predicting vulnerable posts . In Sect . 5 , we present our experimental analysis , and in Sect . 6 , we conclude the paper . 2 Related work The term trolling has been widely used to characterize different types of antisocial and disruptive online behavior . Some of it may be innocent , if not entertaining , but there are cases where it escalates to threatening and bullying behavior . As a result , trolling has become equivalent with online harassment . Previous work attempts to characterize such behavior and its various aspects , including the aspects of identity deception ( see Donath 1999 ) and malicious impoliteness ( see Hardaker 2010 ) . There is also research on explaining the causes of trolling . For example , Suler ( 2004 ) shows that trolling is related to the ( toxic ) online disinhi - bition effect , while Buckels et al . ( 2014 ) indicate that there is a relationship between trolls and sadism . Due to its critical importance , the problem of identifying malicious users and content in online social settings has received considerable attention . Most existing techniques extract a variety of features from the available data and use them to create models to detect trolling behavior . Com - monly used features include textual , topic , and sentiment characteristics of the posts , activity - related metrics , such as post frequency , feedback from the participants , such as upvotes or likes , and moderator features , when available . Related work along this line of research includes detecting vandalism ( see Adler et al . 2011 ; Potthast et al . 2008 ; Chin et al . 2010 ) and vandals ( see Kumar et al . 2015 ) in Wiki - pedia , bad behavior in multi - player online games ( see Blackburn and Kwak 2014 ) , and trolling comments in social news sites ( see Cambria et al . 2010 ; Sood et al . 2012 ; de - la - Pen˜a - Sordo et al . 2014 ) . In a recent study , Cheng et al . ( 2015 ) analyze users who were banned from three large online discussion communities to identify the characteristics of their behavior and how this behavior changes through time . These characteristics were exploited to identify early the users who will be banned . Another line of research in troll detection assumes the availability of a signed social graph among users where signs indicate positive and negative relationships among users . Then , troll detection is modeled as a ranking prob - lem in this graph . Related approaches use iterative algo - rithms that calculate centrality measures ( see , for example Kumar et al . 2014 ; Kunegis et al . 2009 ) , or the trustwor - thiness of the user ( see , for example Wu et al . 2016 ; Ortega et al . 2012 ) . Lamba et al . ( 2015 ) investigate how ﬁrestorms on Twitter affect the relationships between users . A ﬁrestorm is the event where a target ( e . g . , public ﬁgure ) receives a large amount of negative attention . Firestorms are much different than trolling ; ﬁrestorms may include trolls , but not all participants in ﬁrestorms are trolls . Thus , this problem is different than ours . 26 Page 2 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 Our approach differs from these works . The key novelty is that we turn the spotlight on the trolling victim , aiming at characterizing her vulnerabilities , and estimating the risk of becoming a target of trolling . There is no previous work , to our knowledge , studying the problem of troll vulnerability of potential targets . A poster of a preliminary version of this work appeared in Tsantarliotis et al . ( 2016 ) . 3 Modeling troll vulnerability In this section , we introduce the concept of troll vulnera - bility , and we deﬁne metrics to quantify it . 3 . 1 Preliminaries In order to address the problem of troll vulnerability we need a deﬁnition of what constitutes trolling . We use the term trolls to refer to users that behave in a deceptive , destructive , and disruptive manner in an online social set - ting . We use the term trollings to refer to the posts or messages generated by trolls that aim to hurt speciﬁc people or groups . In the following , we assume that we have some method for detecting trolls and trollings . We note that our deﬁnition of vulnerability is independent of the exact deﬁnition of trolling ; depending on the speciﬁc application one could use the appropriate trolling deﬁnition . We assume that trolling occurs within an online user - engagement ecosystem , such as a social network , a microblogging system , or a discussion forum . Users con - tribute content in the form of posts , and they interact with each other , creating discussions . We model interactions between posts as a directed graph G ¼ ð V ; E Þ , where nodes u 2 V correspond to posts and there is an edge ( u , v ) , from post u to post v , if v is a reply to u . For example , in Twitter , nodes may correspond to tweets and there is an edge from a tweet ( node ) u to all tweets ( if any ) that this tweet refers to . Similarly , in Facebook , nodes may correspond to com - ments on user posts . In this paper , we will use Reddit , a popular online dis - cussion forum , as our running example . In this case , the conversation graph of the posts deﬁnes a tree . The root of the tree corresponds to the initial post ( message ) that generated the discussion . Each node of the tree , other than the root , has a unique parent , and there is a directed edge from the parent - comment node to the child - comment node , indicating that the child comment is a reply to the parent comment . A comment may have multiple replies ( chil - dren ) , but each comment replies to a single previous comment ( the parent ) . An example of a discussion tree is shown in Fig . 1 . The tree structure in posts is common to many social media . We note that our metrics are applicable to more general graph structures as well , as long as we can deﬁne a topological sorting of the nodes that corresponds to the temporal order of the posts . For simplicity , in the fol - lowing we assume that the graph G corresponds to a dis - cussion tree . In order to deﬁne troll vulnerability , we need to assume that the graph G is labeled . That is , each node v 2 V is associated with a label ‘ v 2 f T ; NT g indicating whether the node v is a trolling ( T ) or not ( NT ) . Thus , we will think of the graph as a triplet G ¼ ð V ; E ; L Þ where L ¼ f l v : v 2 V g is the set of labels of the nodes in V . Our goal is to deﬁne metrics that quantify the vulnera - bility of a post to trolling attacks . Given a discussion tree G ¼ ð V ; E ; L Þ , a troll vulnerability metric is a function TV : V ! R that maps each post p 2 V in the discussion graph into a real number TV ð p Þ that captures the degree of vul - nerability of the post p . Higher TV ð p Þ values indicate that the post is more vulnerable . We assume that the TV ð p Þ value is a function of the subtree G p ¼ ð V p ; E p ; L p Þ rooted at node p , and thus , abusing the notation , we will some - times write TV ð G p Þ . This is an important assumption . The vulnerability of a node depends solely on the subtree rooted at that node . Two nodes with the same subtrees , in terms of both structure and labels , will have the exact same vul - nerability value . 3 . 2 Troll vulnerability axioms Similarly to the deﬁnitions of distance or similarity between items , there are multiple ways to deﬁne vulner - ability . The deﬁnition of a speciﬁc TV metric depends on the sensitivity and needs of the user of the metric ( e . g . , the administrator that is monitoring the health of a social networking system ) . However , as in the case of distance or similarity metrics , we want the TV metric to satisfy certain properties , in order to be well deﬁned . We thus deﬁne three axioms that any troll vulnerability metric must satisfy . Fig . 1 An example of a conversation tree Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 3 of 15 26 123 As we mentioned before , the TV value of a post p as a function of the subtree G p rooted at node p . Therefore , we will talk about the TV value of a tree G p , and study what happens to the TV metric when there are changes in the subtree G p . As such , it makes sense to compare the TV values for posts that have subtrees with similar structure . Axiom 1 ( Trolling monotonicity ) Let p be a post with subtree G p ¼ ð V p ; E p ; L p Þ , and let G 0 p ¼ ð V 0 p ; E 0 p ; L 0 p Þ denote the subtree of p when we add a trolling node v to the subtree , that is , V 0 p ¼ V p [ f v g , L 0 p ¼ L p [ f ‘ v g and ‘ 0 v ¼ T , and E 0 p ¼ E p [ fð w ; v Þg for some w 2 V p . Then , TV ð G p Þ \ TV ð G 0 p Þ . Axiom 1 essentially says that the vulnerability metric of a post should increase with the addition of new trolling descendants . Equivalently , the TV metric should be a function that is strictly increasing with respect to the number of trolling nodes in the post subtree . Axiom 2 ( Non - trolling monotonicity ) Let p be a post with subtree G p ¼ ð V p ; E p ; L p Þ , and let G 0 p ¼ ð V 0 p ; E 0 p ; L 0 p Þ denote the subtree of p when we add a non - trolling node v to the subtree , that is , V 0 p ¼ V p [ f v g ; L 0 p ¼ L p [ f ‘ v g and ‘ 0 v ¼ NT , and E 0 p ¼ E p [ fð w ; v Þg for some w 2 V p . Then , TV ð G p Þ [ TV ð G 0 p Þ . Axiom 2 essentially says that the vulnerability metric of a post should decrease with the addition of new non - trol - ling descendants . Equivalently , the TV metric should be a function that is strictly decreasing with respect to the number of non - trolling nodes in the post subtree . For the last axiom , we need to deﬁne the notion of distance in the discussion tree . Let p be a post , and let G p the subtree of p , and v a descendant of p . The distance d ( p , v ) between p and v is deﬁned as the length of the path from p to v in the tree . Axiom 3 ( Troll distance monotonicity ) Let p be a post with subtree G p ¼ ð V p ; E p ; L p Þ , where there are nodes u ; v 2 V p , such that ‘ u ¼ NT and ‘ v ¼ T , and d ð p ; u Þ \ d ð p ; v Þ . Let G 0 p ¼ ð V p ; E p ; L 0 p Þ denote the subtree of p when we swap the labels of nodes u and v , that is , ‘ 0 u ¼ T , and ‘ 0 v ¼ NT . Then TV ð G p Þ [ TV ð G 0 p Þ . Axiom 3 says that the vulnerability metric of a post should increase if we bring the trolling labels closer to the post . Essentially , it says that the TV - metric is monotonic with respect to the distances of the trollings to the root node . Axioms 1 and 2 determine the effect of the volume of trolling and non - trolling posts in the post replies , while Axiom 3 refers to the relative position of the trollings in the post subtree . Intuitively , a post is more vulnerable if it is followed by many trollings compared to non - trollings , and the trollings appear close to the post in the discussion tree . Note that our axioms are general enough to capture the effect of changing the label of a node in the tree . Intu - itively , we would like the TV metric to increase if we change the label of a node from non - trolling to trolling , and to decrease in the opposite case . This follows from our axioms . We can emulate the change of a label using the addition and swap operations we described above . For example , to change the label of a node v from non - trolling to trolling , we can add a trolling node u to the subtree of v , swap the labels of v and u , and then remove the node u . From our axioms it follows that all these operations increase the TV metric value . We treat analogously the change of a label from trolling to non - trolling . 3 . 3 Troll vulnerability metrics We now deﬁne three different troll vulnerability metrics . For the following recall that G p ¼ ð V p ; E p ; L p Þ denotes the subtree rooted at node p in G . We also use T p ¼ f u 2 G p : ‘ u ¼ T g to denote the trolling descendants of p , and NT p ¼ f u 2 G p : ‘ u ¼ NT g to denote the non - trolling descendants of p . 3 . 3 . 1 The TVDiff metric The idea behind the TVDiff metric is to use the difference between trolling and non - trolling descendants of a post p to deﬁne the vulnerability of the post . To give some addi - tional control to the user of the metric , we introduce a parameter a that controls the relative importance between trolling and non - trolling descendants . For example , if a ¼ 2 this means that a trolling is twice more important than a non - trolling , and thus the presence of a trolling counts as two non - trollings . Furthermore , in order to enforce Axiom 3 we weight the presence of a descendant v of the post p by a factor b (cid:2) d ð p ; v Þ . Therefore , posts that are further from the root post p contribute less to the metric . We thus deﬁne the TVDiff metric for post p as follows : TVDiff ð p Þ ¼ a X u 2 T p b (cid:2) d ð p ; u Þ (cid:2) X u 2 NT p b (cid:2) d ð p ; u Þ We can prove the following theorem . Theorem 1 The TVDiff metric satisﬁes Axioms 1 , 2 , and 3 . The proof follows directly from the deﬁnition of the TVDiff metric . Adding a new trolling node v to the subtree of node p will increase the TVDiff ð p Þ value by ab (cid:2) d ð p ; v Þ , while adding a non - trolling node v will decrease the 26 Page 4 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 TVDiff ð p Þ value by b (cid:2) d ð p ; v Þ . Swapping the labels of non - trolling node u and trolling node v , at distances d ð p ; u Þ \ d ð p ; v Þ , will increase the TVDiff ð p Þ value by ð a þ 1 Þð b (cid:2) d ð p ; u Þ (cid:2) b (cid:2) d ð p ; v Þ Þ . 3 . 3 . 2 The TVRatio metric The idea behind the TVRatio metric is to use the fraction of the trolling descendants of a post to deﬁne its vulnerability . Again , we weight the presence of a descendant v of the post p by a factor b (cid:2) d ð p ; v Þ , so as to satisfy Axiom 3 . However , there are extreme cases when our metric does not satisfy Axioms 1 and 2 . If all descendants of p are trollings , then the addition of an additional trolling descendant does not change the ratio . Similarly , if all descendants of p are non - trollings , the addition of a non - trolling node has no effect . To alleviate this problem , we allocate weight (cid:2) to both the trolling and non - trolling classes , regardless of the number of descendants in each class . This acts as a smoothing factor , ensuring that the TVRatio can never become 1 or 0 . In this way , the metric satisﬁes Axioms 1 and 2 . In conclusion , we deﬁne the TVRatio as follows : TVRatio ð p Þ ¼ P u 2 T p b (cid:2) d ð p ; u Þ þ (cid:2) P u 2 V p b (cid:2) d ð p ; u Þ þ 2 (cid:2) Note that in practice , we can make (cid:2) arbitrarily small , and this has essentially no effect in our experiments . We can prove the following theorem . Theorem 2 The TVRatio metric satisﬁes Axioms 1 , 2 , and 3 . Similar to before , the proof follows directly from the deﬁnition of the TVRatio metric . Adding a new trolling node v to the subtree of node p increases the enumerator by a factor of b (cid:2) d ð p ; v Þ . The smoothing factor 2 (cid:2) in the denominator guarantees that enumerator and denominator can never be equal ; hence , the addition of a trolling node will always lead to an increase . Adding a non - trolling node increases the denominator , while the enumerator stays the same , hence decreasing the TVRatio metric . Again , because of the smoothing factor (cid:2) , the enumerator can never become zero . Swapping the labels of non - trolling node u and trolling node v , at distances d ð p ; u Þ \ d ð p ; v Þ , will again increase the enumerator , while the denominator stays the same , and hence increase the TVRatio value . 3 . 3 . 3 The TVRank metric The TVRank is the more sophisticated of our three metrics . For this metric , we use Random Walks with Restarts ( RWR ) for the deﬁnition of the troll vulnerability of a post p . Intuitively , we relate the vulnerability of the node p with the probability that a random walk starting from p will visit a trolling descendant . The RWR takes place in the subtree G p , where at each transition there is a chance b that the random walk restarts at p . For each descendant v of p it deﬁnes a probability P p ð v Þ that the random walk is at node v after an inﬁnite number of iterations . The TVRank metric is deﬁned as follows : TVRank ð p Þ ¼ P v 2 T p nf p g P p ð v Þ 1 (cid:2) P p ð p Þ TVRank ð p Þ is the probability that the RWR visits a trolling node , given that it is visiting a descendant of p . Similar to the TVRatio metric , in the extreme case that all nodes are trollings ( or non - trollings ) the addition of a new trolling ( non - trolling ) node would have no effect on the TVRank value . To avoid such extreme cases , we add two ‘‘dummy’’ nodes t and n as children of every node u 2 V p in the subtree of node p , with a ﬁxed weight (cid:2) . The nodes are labeled as trolling and non - trolling , respectively . This way , no subtree in G p can be ‘‘monochromatic , ’’ consisting of only trolling or non - trolling nodes , which alleviates the problem . Practically , the vector of probabilities P p is computed as follows . P p ¼ ð 1 (cid:2) b Þ P p A þ b e p ; where b is the restart probability , A is the row - stochastic transition matrix , and e p is the restart vector , with e p ð p Þ = 1 , and 0 otherwise . A is the normalized adjacency matrix of the graph G p . In particular , for a node u , with set of chil - dren C ( u ) , we have that A ½ u ; n (cid:3) ¼ A ½ u ; t (cid:3) ¼ (cid:2) = ðj C ð u Þj þ 2 (cid:2) Þ , and A ½ u ; v (cid:3) ¼ 1 = ðj C ð u Þj þ 2 (cid:2) Þ for all nodes v 2 C ð u Þ . For leaf nodes v , we set A ½ u ; n (cid:3) ¼ A ½ u ; t (cid:3) ¼ (cid:2) = ð 1 þ 2 (cid:2) Þ and A [ v , p ] = 1 = ð 1 þ 2 (cid:2) Þ , that is , the random walk restarts at node p . The dummy nodes restart at node p . We note again that in practice , we can make (cid:2) arbitrarily small , and this has essentially no effect in our experiments . We can prove the following theorem . Theorem 3 The TVRank metric satisﬁes Axioms 1 , 2 , and 3 . Proof Axiom 1 is the easiest to prove . Note that due to the tree structure of the graph , there is a unique path from the root to some node v i in the tree . Let f v 0 ; v 1 ; . . . v i (cid:2) 1 ; v i g denote that path , where v 0 is the root of the tree . Let also d 0 ; d 1 ; . . . ; d i (cid:2) 1 ; d i denote the degrees of the nodes in the path . Let P 0 denote the probability of the root at the sta - tionary distribution . Then the probability of node i is P i ¼ ð 1 (cid:2) b Þ i 1 d 0 þ 2 (cid:2) 1 d 1 þ 2 (cid:2) (cid:4) (cid:4) (cid:4) 1 d i (cid:2) 1 þ 2 (cid:2) P 0 . Now , if we swap the labels of two nodes u and v , the probabilities of the nodes in the tree do not change , since the structure of the tree did Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 5 of 15 26 123 not change . The swap causes the trolling label to come closer to the root ; hence , the probability of the newly labeled trolling node will increase , since it is an expo - nentially decreasing function of the distance to the root . The proof of Axioms 1 and 2 is more technical , so we only give the sketch of the proof . Consider the addition of a node v in the tree ( trolling , or non - trolling ) . The effect of the addition is to decrease the probabilities of some of the remaining nodes in the tree . If node v is added as a child of a non - leaf - node v i at depth i , with d i children , this will cause the degree of node v i to increase , and hence the probabilities of the nodes in the subtree rooted at v i to decrease ( the rest of the probabilities are not affected ) . Consider the case that node v is a trolling node . We know that node v i has at least one non - trolling descendant ( the ‘‘dummy’’ node n ) . The probability of these non - trolling descendants decreases , and hence the overall probability of the non - trolling label decreases , which means that the probability of the trolling label increases , and thus TVRank increases . The case of adding a non - trolling node is treated symmetrically . In the case that the new node v is added to a leaf node , the probability of the root of the tree decreases . This causes the probability of all existing nodes in the tree to decrease . If v is a trolling node , this means that the probability of the non - trolling nodes decreases , and hence the probability of the trolling nodes increases , which results in the increase of the TVRank metric . The case of adding a non - trolling node is treated symmetrically . Intuitively , when a new node v is added to the tree it ‘‘claims’’ some probability mass that it takes away from the rest of the nodes . If it is a trolling node , the probability mass goes to the trolling label , hence increasing the TVRank value . If it is a non - trolling node , the probability mass goes to the non - trolling label , hence decreasing the TVRank value . RWRs have been widely used to deﬁne the strength of the relationship between two nodes in a graph , and they are the building blocks of many metrics including PageRank ( see Lawrence et al . 1998 ) , topic - sensitive PageRank ( see Haveliwala 2002 ) , and SimRank ( see Jeh and Widom 2002 ) . In this paper , we use RWRs to capture the rela - tionship of a node with its trolling descendants . 3 . 4 Post vulnerability The TV metrics we deﬁned provide a numerical value that quantiﬁes the degree of vulnerability of a node . Using this value we can determine which nodes are vulnerable or not . In addition to a high TV value , for a post to be character - ized as vulnerable , we impose a hard constraint that the node should have at least K descendants , for a chosen parameter value K . That is , in order for a post p to be considered as vulnerable it must satisfy that j V p j (cid:5) K . The rationale behind this constraint is that a post must generate enough trafﬁc in order to be of interest to mod - erators . If the responses to a post are few , even if they are trollings , they are essentially a failed attempt at trolling since they did not generate any additional discussion . We are now ready to deﬁne the notion of post vulnerability . Deﬁnition 1 ( Post vulnerability ) Given a troll vulnera - bility metric TV , and parameters K [ 0 and h (cid:5) 0 , we deﬁne a post p to be vulnerable to trolls if j V p j (cid:5) K , and TV ð p Þ (cid:5) h . The deﬁnition is dependent on the exact metric that we use , and the parameters K and h that control the sensitivity of post vulnerability . The h value determines the intensity of trolling activity that a post needs to generate for the post to be considered vulnerable . When moderation needs to be strict ( for instance , to avoid insults in social media where kids participate ) , a lower h value allows prompt notiﬁcation for potential trolling behavior . The threshold value K de - termines the minimum number of responses that a post needs to generate for the post to be considered important enough to be characterized as vulnerable . 4 Prediction of troll vulnerability Given a post , our goal is to predict whether the post will be vulnerable to trolls or not . We treat the problem as a two - class classiﬁcation problem , with the positive class corre - sponding to the vulnerable posts and the negative class to the non - vulnerable posts , and build a classiﬁcation model . For deﬁning the positive class , i . e . , the set of vulnerable posts , we use Deﬁnition 1 . We design features that capture various aspects of the post and its past . Note that we only consider ancestors of the post , since we want to decide on its vulnerability , before the post receives any replies ( i . e . , acquires any descendants ) . We group features in four categories , namely content , author , history and participants . The features we used are summarized in Table 1 . Content features Content features include features related to the text of the post . Previous research ( e . g . , Cheng et al . 2015 ) shows that the comments that were written by provocative users tend to be less readable than those written by other users . Thus , we include a number of readability - related features ( e . g . , the number of words written in capital letters , which is considered rude in online chatting ) as well as the automated readability index 1 ( ARI ) . 1 https : / / en . wikipedia . org / wiki / Automated _ readability _ index . 26 Page 6 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 We also count the number of positive and negative words , using an opinion lexicon . 2 The motivation is that opin - ionated comments are more likely to attract trollings . We also include a feature indicating whether the post itself is a trolling . Author features Author features aim to capture the behavior of the author of the post in the social setting . Features related to the activity of the author include the number of her posts , the number of trollings in them , and the average replies per post . We also include the largest number of posts that a users posted in a single conversation tree , since it may be more likely for users that are very active in conversations to engage in a debate with trolls . Additionally , we consider features related to how the other users in the community perceive the author and her comments . Most social networks provide mechanisms for users to express their preference , or opinion , for a post ( e . g . , whether they like it or not , or ﬁnd it useful or not ) by rating them . In Reddit , this rating is a score : 1 ( upvote ) if the users like the comment , - 1 ( downvote ) if they do not . We use score - related features ( such as the average score , the average of the absolute score values , number of com - ments that are scored positively ) to help us to capture the perception of the user from the rest of the community . History features History - related features are extracted from the conversation tree of the post . We consider the depth of the post in the tree and also information about the ancestors of the post . Information about the ancestors includes a number of score - related features , such as the average and absolute score , as well as the number of posts that have negative , positive , and zero score and the number of trollings . The motivation is that posts whose preceding posts do not include trollings and have positive scores are less likely to be targeted by trolls . This group also includes the similarity of the post with the previous three posts , by calculating the cosine similarity of the words used in these posts . The intuition is that posts that try to change the topic of the conversation may attract an unpleasant reaction by the community . Participant features Finally , the features related to the participants in a discussion contain information about the authors of the previous comments . In particular , we aver - age the features in the second group for all the users that participate in the ancestor posts . These features can be thought of as describing the average user that participated in the previous posts . Using these features , we build a classiﬁcation model for predicting the troll vulnerable posts . We can use any classiﬁcation model for our task . In our experiments , we use a logistic regression classiﬁer . We explore different classiﬁcation models at the end of Sect . 5 . 5 Experimental results In this section , we present results for the troll vulnerability metrics and the troll vulnerability prediction task . 5 . 1 Experimental setting 5 . 1 . 1 Dataset Our dataset contains posts from the Reddit social network Web site . The site is a collection of entries , called sub - missions , posted by registered users . Submissions are organized into categories , called subreddits . Once a user posts a submission to a subreddit , users post comments on this submission . Users then respond to these comments , and conversation trees are formed whose roots are the submissions . We consider 18 popular subreddits and retrieved 20 submissions from each of these subreddits , resulting in 555 , 332 comments . 5 . 1 . 2 Detection of trollings Although identifying trollings is a problem orthogonal to our approach , to evaluate the performance of the troll vulnerability prediction task , we need a deﬁnition of trolling for our dataset . As we have already argued , the notion of trolling covers a wide range of behaviors , from innocent humor and misinformation to criminal activity . In our work , we focus on the antisocial part of trolls , i . e . , we detect comments that contain offensive content . Table 1 The features of our prediction model Featuregroup Features Content ( 9 ) # char , # words , # sentences , # quotes , # words in capital , A . R . I , # negative / positive words , whether is trolling Author ( 13 ) # posts , # trollings , max posts in single conversation tree , avg replies per post , avg score per post , avg absolute score , positive / negative score , negative - to - positive score ratio , # controversial comments , # positive / negative / zero scored posts History ( 10 ) depth of the post , parent similarity , zero / positive / negative scored posts , sum score , sum absolute score , sum negative / positive score , ancestors that are trollings Participants ( 13 ) # posts , # trollings , max posts in single conversation tree , avg replies per post , avg score per post , avg absolute score , positive / negative score , negative - to - positive score ratio , # controversial comments , # positive / negative / zero scored posts 2 https : / / www . cs . uic . edu / * liub / FBS / sentiment - analysis . html # lexicon . Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 7 of 15 26 123 The ﬁrst step of our evaluation is to identify such offensive trolling posts in the Reddit comments . To this end , we build a classiﬁer that detects insulting content using only text features . To train the classiﬁer , we used a labeled dataset from an online contest in the Kaggle plat - form . 3 The label is either 1 meaning an insulting comment or 0 meaning a neutral ( i . e . , non - insulting ) comment . The training set of the contest dataset contains 6494 comments , from which 1743 comments are insulting and the rest are neutral . The test set contains 2236 comments , from which 1077 comments are insulting and the rest are neutral . The content of the comments is mostly in English with some occasional formatting . We used a slightly modiﬁed version of a classiﬁer used in the Kaggle contest Olariu ( 2013 ) . The model consists of a neural network that takes as input the output of a few text classiﬁers . 4 The classiﬁer takes as input the text content of the comments and assigns a score in [ 0 , 1 ] to each com - ment . In the contest dataset , the model achieved 76 % accuracy , 84 % precision , and 60 % recall . We used this model to build a classiﬁer for detecting trollings in the Reddit dataset . To evaluate the performance of the classiﬁer in the Reddit dataset , we manually labeled 2 , 500 Reddit comments as trollings ( i . e . , insulting ) or non - trollings ( i . e . , neutral ) . We selected these comments by splitting the [ 0 , 1 ] scores in ﬁve ranges and randomly selecting 500 comments from each range . These comments were assigned to three human judges along with the output of the classiﬁer . The question was whether they agree with the decision of the classiﬁer . The judges were given the following guidelines as to when to consider a post as being insulting : • The author addresses insulting words or phrases to a person or a group of people . • The author does not use insulting language , but clearly his goal is to insult a person or a group of people . • The author expresses himself in a vulgar way . We measured the pairwise agreement between the labelers using the Cohen’s Kappa coefﬁcient ; they scored 0 . 64 , 0 . 65 , and 0 . 71 , which means that they seem to agree in most cases . Considering the labels and the output of the classiﬁer , we set the threshold for characterizing a com - ment as trolling at 0 . 5 . In this setting , we achieved 82 % accuracy , 75 % precision , and 80 % recall in this set . We applied this model to the full dataset . It character - ized as trollings 9 , 541 comments which amounts to 1 . 7 % of the total dataset . 5 . 2 Troll vulnerability metrics In Sect . 3 , we introduced three different TV metrics for quantifying the vulnerability of comments . In particular , for a post p to be considered vulnerable , its TV value should be larger than a threshold h and p must be fol - lowed by at least K comments . Both h and K act as ﬁlters that determine the number of comments that are vulnerable . We set K ¼ 2 as the default value , asking that a comment must be followed by at least 2 comments to be considered vulnerable . As argued , if K ¼ 1 , then even if the following comment is a trolling , it is a failed one , since it did not generate any additional discussion . We tune h and the value of the parameters of all three TV metrics so that we get around 3800 comments charac - terized as vulnerable which amounts for about 2 . 5 trol - lings per vulnerable comment , on average , and use these as the default values . In particular , for TVRank , we set h ¼ 0 : 3 as the default value which results in 3853 comments being characterized as vulnerable . We also set b , the restart probability , to 0 . 15 as in previous work , e . g . , Lawrence et al . ( 1998 ) . We experimented also with different b values . This results in a small difference in the vulnerability rank of the nodes ; however , it did not affect the performance of the classiﬁ - cation model . For TVRatio , we set h ¼ 0 : 3 and b ¼ 0 : 5 . Using this setting , 3874 comments are characterized as vulnerable For TVDiff , we set h equal to zero , so that in all cases at least one trolling descendant is present for charac - terizing a comment as vulnerable . We also set a ¼ 3 as the default value , which means that a trolling is con - sidered three times more important than a trolling and b ¼ 0 : 25 . Using this setting results in 3819 vulnerable comments . We also experimented with different values of K for all TV metrics and with different h values for TVRatio and TVRank . For TVDiff , we vary a instead of h since this gives us better control on the relative importance of trolling comments . The results are shown in Table 2 . Larger values of h ( resp . smaller values of a ) , reduce the sensitivity of the metrics , resulting in less comments being characterized as vulnerable . Similarly , larger values of K reduce the number of vulnerable comments . Finally , in Fig . 2 we compare the results of the three TV metrics using a Venn diagram to depict the extend at which the metrics agree in their characterization of comments as vulnerable . The Venn diagram includes all comments characterized as vulnerable by at least one TV metric , using the default values of the parameters . We can see that the majority of vulnerable comments are characterized as vulnerable by all three metrics . 3 https : / / www . kaggle . com / . 4 http : / / goo . gl / UL2VuE . 26 Page 8 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 5 . 3 Troll vulnerability prediction We implemented classiﬁcation models using the four group of features introduced in Sect . 4 to predict whether a comment is vulnerable or not . An important problem that we had to address is the class imbalance of the dataset . The number of trollings in the dataset is very low , and subsequently the number of the vulnerable comments is low as well . If we train the model without any preprocessing , the classiﬁer will assign all the samples to the majority class , i . e . , the non - vulnerable comments . The minority class , i . e , the vulnerable com - ments , is so small that the classiﬁer can simply ignore it . In order to deal with this problem , we randomly undersample the majority class . We reduce the size of the non - vulnerable class to be equal to the size of the vulner - able class . We repeat this procedure ten times , and we report the average values of the evaluation metrics . To understand the relative importance of each group , we ﬁrst compare the performance of each group individually using logistic regression with 10 - fold validation . We then incrementally combine the groups . Table 3 shows the classiﬁcation results . First , to put the numbers in perspective , a random assignment of labels to posts would result in a value 0 . 5 for all evaluation metrics . Therefore , all classiﬁers pass the sanity check of performing better than random predictions . We then evaluate the different groups of features . We observe in all metrics that content features are the weakest of the four groups of features , followed by the history group that includes features related to the ancestor com - ments . Features related to the users that post the comments seem to carry a stronger signal : the author group and the participants group ( that includes information about the authors of the ancestor comments ) perform the best . This indicates that the author of the comment and the authors of the preceding comments affect vulnerability more than the comments themselves . Combining features improves the prediction , with the classiﬁer using features from all four groups being the best . In terms of TV metrics , TVRank and TVRatio perform almost the same and TVDiff performs slightly worse than the other two metrics . 5 . 3 . 1 Individual features We also investigate the relative importance of individual features . To this end , we selected from each of the four groups the three features with the highest ( in absolute value ) logistic regression coefﬁcients and build the corre - sponding single - feature classiﬁers . Table 4 shows the results of the single - feature classiﬁcation for TVRank . The rest of the metrics perform similarly . Most of the features perform poorly when used alone ; only a few have recall measures larger than 50 % . In terms of content , using strong opinion words ( positive or nega - tive ) in a comment affects troll vulnerability . In terms of the author of the comment , the fact that the author has previously posted trollings or is negatively perceived by the community is a strong signal . The same holds for the history of the ancestor comments and the authors of these comments . Moreover , we checked the best features based on uni - variate statistical tests . In particular , we used the ANOVA F test , which estimates the degree of linear dependency between two random variables . We scored the features using ANOVA F value , and we selected the highest scoring features . Table 5 shows the highest scoring features for the TV metrics . We can see the best features are almost the same in all metrics . This means that these features are Table 2 Number of vulnerable comments computed using the three TV metrics TVDiff TVRatio TVRank K n a 5 4 3 2 K n h 0 . 2 0 . 25 0 . 3 0 . 35 K n h 0 . 2 0 . 25 0 . 3 0 . 35 2 6027 4325 3879 3233 2 5877 5142 3874 2869 2 6271 4995 3858 3036 3 4542 3351 2905 2259 3 4392 3657 2389 1895 3 4786 3510 2373 1551 5 2802 2070 1677 1237 5 2391 1823 1191 875 5 2321 1430 953 653 8 1578 1144 863 593 8 1129 802 516 379 8 1098 671 434 281 2574 503 726 15 695 51 559 TVDiﬀ TVRa(cid:3)o TVRank Fig . 2 Venn diagram that shows the agreement of the TV metrics in the dataset Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 9 of 15 26 123 important to the troll vulnerability prediction problem , regardless of the metric we use . 5 . 3 . 2 Varying the vulnerability parameters We also study the performance of the prediction model for different values of K , and when varying the parameters of TV metrics . In particular , we experiment with different h values for the TVRank and TVRatio metrics and different a values for the TVDiff metric . Figure 3 shows the results for the classiﬁers that include all features . Adjusting the parameters of the metrics ( i . e . , increasing h and decreasing a ) and increasing the constraint K increases the selectivity in the troll vulnerability deﬁni - tion , resulting in fewer comments considered as vulnerable ( Table 2 ) . The performance of the classiﬁer improves when the classes of vulnerable comments become more selective . 5 . 3 . 3 Other datasets In addition to our main dataset that consists of all available subreddits , we also tested our model on three smaller datasets , consisting of more homogeneous collections of posts . The ﬁrst dataset is a subset of the main dataset that includes all comments posted in the subreddit ‘‘an - nouncements . ’’ The subreddit ‘‘announcements’’ is the most popular subreddit in our dataset . It consists of 61 , 709 comments out of which 1 , 498 are trollings . For the second dataset , we used the same 18 subreddits as in our default dataset , but now we collect comments from the contro - versial submissions in these subreddits . A submission is controversial if it its comments have received the same number of positive and negative scores ( i . e . , upvotes and downvotes , respectively ) . This dataset has 270 , 144 com - ments out of which 5805 are trollings . The third dataset contains comments from the ‘‘atheism’’ subreddit . This dataset contains 19 , 719 comments , out of which 438 ( 2 . 2 % ) are characterized as trollings . We selected this subreddit for the controversial nature of the topic , which makes it more likely to attract trolls . The evaluation results of the classiﬁers for the ‘‘announcements’’ dataset are shown in Fig . 4 , for the ‘‘controversial’’ dataset in Fig . 5 and for the ‘‘atheism’’ dataset in Fig . 6 . The results are consistent over all datasets , demonstrating that thematic homogeneity of the comments has a small effect on the classiﬁer performance . The slightly worse performance in Table 3 Prediction results for the various groups of features and combinations of the groups Feature group TVDiff TVRatio TVRank A P R AUC A P R AUC A P R AUC Content ( Con ) 0 . 59 0 . 66 0 . 38 0 . 64 0 . 61 0 . 69 0 . 40 0 . 66 0 . 61 0 . 70 0 . 37 0 . 65 Author ( Auth ) 0 . 67 0 . 70 0 . 59 0 . 73 0 . 68 0 . 71 0 . 62 0 . 75 0 . 70 0 . 74 0 . 62 0 . 76 History ( Hist ) 0 . 65 0 . 70 0 . 51 0 . 69 0 . 68 0 . 74 0 . 56 0 . 72 0 . 69 0 . 75 0 . 57 0 . 73 Participants ( Part ) 0 . 68 0 . 74 0 . 57 0 . 73 0 . 72 0 . 77 0 . 61 0 . 76 0 . 71 0 . 76 0 . 61 0 . 76 Con ? Auth 0 . 67 0 . 71 0 . 58 0 . 74 0 . 70 0 . 73 0 . 63 0 . 76 0 . 71 0 . 75 0 . 63 0 . 77 Con ? Auth ? Hist 0 . 69 0 . 72 0 . 62 0 . 76 0 . 72 0 . 75 0 . 65 0 . 79 0 . 74 0 . 77 0 . 66 0 . 80 Con ? Auth ? Hist ? Part 0 . 72 0 . 76 0 . 64 0 . 79 0 . 74 0 . 78 0 . 67 0 . 82 0 . 77 0 . 81 0 . 68 0 . 83 Table 4 Classiﬁcation results using a single feature Feature group Feature Accuracy Precision Recall AUC Content # negative words 0 . 56 0 . 70 0 . 22 0 . 56 # positive words 0 . 52 0 . 51 0 . 59 0 . 51 Whether is trolling 0 . 55 0 . 87 0 . 13 0 . 55 Author # trollings 0 . 67 0 . 82 0 . 44 0 . 67 Sum positive score 0 . 57 0 . 68 0 . 27 0 . 57 Sum negative score 0 . 58 0 . 70 0 . 27 0 . 58 History # zero scored comments 0 . 57 0 . 78 0 . 20 0 . 57 # negative scored comments 0 . 66 0 . 80 0 . 43 0 . 66 # trolling ancestors 0 . 58 0 . 80 0 . 22 0 . 58 Participants # trollings 0 . 70 0 . 81 0 . 54 0 . 70 # negative scored comments 0 . 65 0 . 70 0 . 53 0 . 65 # zero scored comments 0 . 64 0 . 68 0 . 52 0 . 64 26 Page 10 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 the atheism dataset is due to the small size of the dataset . This is also the reason why we omitted the case K = 8 from our plots , since the number of vulnerable comments in this case was around 30 on average . 5 . 3 . 4 Varying class imbalance We will now study how class imbalance affects the per - formance of the classiﬁer . For this experiment we create different datasets by varying the ratio of the size of the negative ( non - vulnerable ) class ( which is the majority ) to the positive ( vulnerable ) class ( which is the minority ) . When training a classiﬁer with unbalanced data , we assigned weights to the comments . The weights are adjusted inversely proportional to class frequencies in the training set . Higher weight means the classiﬁer puts more emphasis on the class during the training phase . Therefore , if the classiﬁer makes a wrong decision for a troll vulner - able comment , it is penalized more than making an error for a non - vulnerable comment . We also experimented with the Synthetic Minority Over - Sampling Technique ( SMOTE ) and Tomek links ( Batista et al . ( 2003 ) ) method , but it performed slightly worse . For all datasets we perform 10 - fold cross - validation to compute our results . Table 5 The best features for each metric using statistical tests TVDiff TVRatio TVRank Feature group Feature Feature group Feature Feature group Feature Participants # trollings Participants # trollings Participants # trollings Participants # negative scored comments Participants # negative scored comments Participants Sum positive score History # negative scored comments History # negative scored comments Participants # negative scored comments Participants Sum negative score Participants Sum negative score History # negative scored comments Author # positive scored comments Participants # zero scored comments Participants Sum negative score Participants # zero scored comments History Depth of the post History Depth of the post Author # negative scored comments Author # trollings Author # positive scored comments History Depth of the post Author # positive scored comments Participants # zero scored comments Author Sum negative score History Parent similarity Author # trollings History Parent similarity Author # negative scored comments Author # negative scored comments 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A cc u r a c y α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 P r e c i s i o n α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 R e c a ll α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A U C α k = 2 k = 3 k = 5 k = 8 ( a ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( b ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( c ) Fig . 3 Performance of the classiﬁcation model with different values of K and of the parameters of the TV metrics . a TVDiff . b TVRatio . c TVRank Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 11 of 15 26 123 In Figure 7 we present results that show how the classiﬁer performs as the ratio increases from 1 ( perfectly balanced ) to 25 . Recall and AUC are not affected ; however , precision drops . Tables 6 and 7 show the confusion tables for the perfectly balanced case and the full dataset , respectively . For the table creation we have summed the numbers over the 10 - fold . From the tables , it is clear that the reason for the precision drop is that many non - vulnerable comments are incorrectly charac - terized as vulnerable ( i . e . , high number of false 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A cc u r a c y α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 P r e c i s i o n α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 R e c a ll α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A U C α k = 2 k = 3 k = 5 k = 8 ( a ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( b ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( c ) Fig . 4 Performance of the classiﬁcation model with different values of K and of the parameters of the TV metrics for the ‘‘announcements’’ subreddit . a TVDiff . b TVRatio . c TVRank 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A cc u r a c y α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 P r e c i s i o n α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 R e c a ll α k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 5 4 3 2 A U C α k = 2 k = 3 k = 5 k = 8 ( a ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( b ) 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A cc u r a c y θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 P r e c i s i o n θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 k = 8 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 A U C θ k = 2 k = 3 k = 5 k = 8 ( c ) Fig . 5 Performance of the classiﬁcation model with different values of K and of the parameters of the TV metrics for controversial submissions . a TVDiff . b TVRatio . c TVRank 26 Page 12 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 positives ) . Therefore , our classiﬁers err on the side of caution , putting more emphasis on capturing the vulner - able cases , as demonstrated by the high recall . One possible way to handle this in practice is to treat all comments characterized as vulnerable as warnings and wait before taking more aggressive actions . 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 5 4 3 2 y c a r u cc A a k = 2 k = 3 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 5 4 3 2 n o i s i c e r P a k = 2 k = 3 0 . 45 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 5 4 3 2 R e c a ll a k = 2 k = 3 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 5 4 3 2 A U C a k = 2 k = 3 ( a ) 0 . 5 0 . 55 0 . 6 0 . 65 0 . 7 0 . 75 0 . 8 0 . 85 0 . 2 0 . 25 0 . 3 0 . 35 y c a r u cc A θ k = 2 k = 3 k = 5 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 2 0 . 25 0 . 3 0 . 35 n o i s i c e r P θ k = 2 k = 3 k = 5 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 2 0 . 25 0 . 3 0 . 35 C U A θ k = 2 k = 3 k = 5 ( b ) 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 0 . 20 0 . 25 0 . 30 0 . 35 y c a r u c c A θ k = 2 k = 3 k = 5 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 0 . 2 0 . 25 0 . 3 0 . 35 n o i s i c e r P θ k = 2 k = 3 k = 5 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 2 0 . 25 0 . 3 0 . 35 R e c a ll θ k = 2 k = 3 k = 5 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 2 0 . 25 0 . 3 0 . 35 C U A θ k = 2 k = 3 k = 5 ( c ) Fig . 6 Performance of the classiﬁcation model with different values of K and of the parameters of the TV metrics for the submissions in the subreddit ‘‘atheism . ’’ a TVDiff . b TVRatio . c TVRank 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 1 2 4 8 16 25 y c a r u cc A N TVDiﬀ TVRa(cid:5)o TVRank 0 . 05 0 . 20 0 . 35 0 . 50 0 . 65 0 . 80 1 2 4 8 16 25 n o i s i c e r P N TVDiﬀ TVRa(cid:5)o TVRank 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 1 2 4 8 16 25 ll a c e R N TVDiﬀ TVRa(cid:5)o TVRank 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 1 2 4 8 16 25 C U A N TVDiﬀ TVRa(cid:5)o TVRank Fig . 7 Performance of the classiﬁcation model as a function of the negative - to - positive class ratio Table 6 Confusion matrices for balanced datasets TVDiff TVRatio TVRank True \ Predicted 0 1 True \ Predicted 0 1 True \ Predicted 0 1 0 3 . 096 783 0 3 . 191 683 0 3 . 206 652 1 1 . 419 2 . 460 1 1 . 294 2 . 580 1 1 . 242 2 . 616 Table 7 Confusion matrices without balancing TVDiff TVRatio TVRank True \ Predicted 0 1 True \ Predicted 0 1 True \ Predicted 0 1 0 429 . 808 121 . 645 0 466 . 282 85 . 176 0 466 . 649 84 . 825 1 1 . 396 2 . 483 1 1 . 240 2 . 634 1 1 . 235 2 . 623 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 13 of 15 26 123 5 . 4 Trolling escalation We observed in our experiments that there is some corre - lation between trolling behavior and vulnerability . This is to some extend expected , as trollings invite reactions , causing trolling to escalate . We now study in detail the relationship between trollings and vulnerable comments . A ﬁrst question is whether all vulnerable comments are trollings . The percentage of the vulnerable comments that are trollings themselves in our dataset is 12 . 8 % . This means that a comment does not have to be a trolling to attract trolls . In Fig . 8a , we see an example of a benign vulnerable comment with a high TVRank . Another question is whether all trollings are vulnerable to trolls . The percentage of trollings that are vulnerable to trollings in our dataset is 5 % . This is much higher than the 0 . 6 % probability of a non - trolling post to be vulnerable . However , clearly , not all trollings generate additional trollings . Some of the trollings escalate , but others do not . Thus , we ask whether we can use our classiﬁers to predict whether a trolling comment will escalate or not . To this end , we use our classiﬁer with input only the trolling comments and try to predict whether these trolling com - ments are vulnerable . Our classiﬁer achieved AUC 70 % , using the TVRank metric , indicating that such a prediction task is possible . This would be a useful tool for distin - guishing between trollings that will end up causing havoc and trollings that will have only a limited effect . In Fig . 8b , we see an example of a trolling that escalates . The content of the initial comment is abusive and the comments that follow it are also abusive . Figure 8c shows a trolling that did not escalate . User B quotes a phrase ( from a movie ) , that contains inappropriate content , but there is no trolling reaction . 5 . 5 Additional classiﬁers Besides the logistic regression classiﬁer , we also experi - mented with an SVM and a random forest classiﬁer . The results for the three vulnerability metrics , with the default parameters , are reported in Table 8 . Performance is con - sistent across all three classiﬁers , with random forest slightly outperforming the other two . It appears that the speciﬁc classiﬁcation model has only a small effect on the troll vulnerability prediction task . 6 Conclusions and future work Understanding and detecting trolling behavior in social networks has attracted considerable attention . In this paper , we take a different approach shifting the focus from the trolls to their victims . In particular , we introduce the novel concept of troll vulnerability to characterize how suscep - tible a post is to trolls . We provide three measures of troll vulnerability with respect to both the volume and the proximity of the trollings associated with each post . We then address the troll vulnerability prediction problem : given a post how to predict whether this post will attract trolls in the future . Predicting the vulnerability of a post ( a ) ( b ) ( c ) Fig . 8 Examples of trolling behavior . a An example of a vulnerable comment . b An example of a trolling that escalated . c An example of a trolling that did not escalate Table 8 Performance of additional classiﬁers Classiﬁer TVDiff TVRatio TVRank A P R AUC A P R AUC A P R AUC SVM 0 . 71 0 . 75 0 . 65 0 . 77 0 . 74 0 . 77 0 . 68 0 . 80 0 . 75 0 . 78 0 . 69 0 . 81 Logistic regression 0 . 72 0 . 76 0 . 64 0 . 79 0 . 74 0 . 78 0 . 67 0 . 82 0 . 77 0 . 81 0 . 68 0 . 83 Random forest 0 . 73 0 . 75 0 . 70 0 . 83 0 . 75 0 . 78 0 . 71 0 . 85 0 . 76 0 . 78 0 . 72 0 . 85 26 Page 14 of 15 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 123 allows handling trolls proactively , by preventing them to appear instead of just detecting them , when they appear . We have built a classiﬁer that combines features related to the post and its history ( i . e . , the posts preceding it and their authors ) to identify vulnerable posts . Our initial results using the Reddit dataset are promising , suggesting that a proactive treatment of trolls is feasible . In the future , we plan to extend our evaluation , applying our classiﬁer to predicting troll vulnerability in other social networks beyond Reddit . In addition , our work creates interesting directions for future work toward studying vulnerability at different levels than that of a post . For example , instead of identifying posts that are likely to attract trolls , an interesting problem is identifying vulner - able users , e . g . , users whose posts are vulnerable to trolls , or , vulnerable topics , e . g . , topics that are likely to attract trolls . References Adler BT , De Alfaro L , Mola - Velasco SM , Rosso P , West AG ( 2011 ) Wikipedia vandalism detection : combining natural language , metadata , and reputation features . In : Computational linguistics and intelligent text processing . Springer , Berlin , pp 277 – 288 Atwood J ( 2011 ) Suspension , ban or hellban ? http : / / goo . gl / TxCGi7 Batista GE , Bazzan AL , Monard MC ( 2003 ) Balancing training data for automated annotation of keywords : a case study . In : WOB , pp 10 – 18 Blackburn J , Kwak H ( 2014 ) Stfu noob ! : predicting crowdsourced decisions on toxic behavior in online games . In : Proceedings of the 23rd international conference on World wide web . ACM , pp 877 – 888 Buckels EE , Trapnell PD , Paulhus DL ( 2014 ) Trolls just want to have fun . Pers Individ Differ 67 : 97 – 102 Cambria E , Chandra P , Sharma A , Hussain A ( 2010 ) Do not feel the trolls . In : Proceedings of the 3rd International Workshop on Social Data on the Web . ISWC Cheng J , Danescu - Niculescu - Mizil C , Leskovec J ( 2015 ) Antisocial behavior in online discussion communities . In : Proceedings of ICWSM Chin SC , Street WN , Srinivasan P , Eichmann D ( 2010 ) Detecting wikipedia vandalism with active learning and statistical language models . In : Proceedings of the 4th Workshop on Information Credibility , WICOW ’10 , pp 3 – 10 de - la - Pen˜a - Sordo J , Pastor - Lo´pez I , Ugarte - Pedrero X , Santos I , Bringas PG ( 2014 ) Anomalous user comment detection in social news websites . In : International Joint Conference SOCO’14 - CISIS’14 - ICEUTE’14—Bilbao , Spain , June 25th – 27th , 2014 , Proceedings , pp 517 – 526 Donath JS ( 1999 ) Identity and deception in the virtual community . Commun Cyberspace 1996 : 29 – 59 Hardaker C ( 2010 ) Trolling in asynchronous computer - mediated communication : from user discussions to academic deﬁnitions . J Politeness Res 6 : 215 – 242 Haveliwala TH ( 2002 ) Topic - sensitive pagerank . In : Proceedings of the eleventh international World Wide Web Conference , WWW 2002 , May 7 – 11 , 2002 , Honolulu , Hawaii , pp 517 – 526 Jeh G , Widom J ( 2002 ) Simrank : a measure of structural - context similarity . In : Proceedings of the eighth ACM SIGKDD international conference on knowledge discovery and data mining , July 23 – 26 , 2002 . Edmonton , Alberta , Canada , pp 538 – 543 Jeong S ( 2014 ) Does twitter have a secret weapon for silencing trolls ? http : / / goo . gl / HcuL20 Kumar S , Spezzano F , Subrahmanian V ( 2014 ) Accurately detecting trolls in slashdot zoo via decluttering . In : 2014 IEEE / ACM international conference on advances in social networks analysis and mining ( ASONAM ) . IEEE , pp 188 – 195 Kumar S , Spezzano F , Subrahmanian VS ( 2015 ) VEWS : a wikipedia vandal early warning system . In : Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining , pp 607 – 616 Kunegis J , Lommatzsch A , Bauckhage C ( 2009 ) The slashdot zoo : mining a social network with negative edges . In : Proceedings of the 18th international conference on World Wide Web , WWW ’09 , pp 741 – 750 Lamba H , Malik MM , Pfeffer J ( 2015 ) A tempest in a teacup ? Analyzing ﬁrestorms on twitter . In : Proceedings of the 2015 IEEE / ACM international conference on advances in social networks analysis and mining , ASONAM , pp 17 – 24 Lawrence P , Sergey B , Motwani R , Winograd T ( 1998 ) The pagerank citation ranking : bringing order to the web . Technical report , Stanford University Olariu A ( 2013 ) Repo for the insults detection challenge on kaggle . com . https : / / github . com / andreiolariu / kaggle - insults / Ortega FJ , Troyano JA , Cruz FL , Vallejo CG , Enrı´quez F ( 2012 ) Propagation of trust and distrust for the detection of trolls in a social network . Comput Netw 56 ( 12 ) : 2884 – 2895 Potthast M , Stein B , Gerling R ( 2008 ) Automatic vandalism detection in wikipedia . In : Advances in information retrieval , 30th European Conference on IR Research , ECIR 2008 , Glasgow , UK , March 30 – April 3 , 2008 , pp 663 – 668 Sood SO , Churchill EF , Antin J ( 2012 ) Automatic identiﬁcation of personal insults on social news sites . J Am Soc Inf Sci Technol 63 ( 2 ) : 270 – 285 Suler J ( 2004 ) The online disinhibition effect . Cyberpsychol Behavi 7 ( 3 ) : 321 – 326 Tsantarliotis P , Pitoura E , Tsaparas P ( 2016 ) Troll vulnerability in online social networks . In : 2016 IEEE / ACM international conference on advances in social networks analysis and mining , ASONAM 2016 , San Francisco , CA , USA , August 18 – 21 , 2016 , pp 1394 – 1396 Wu Z , Aggarwal CC , Sun J ( 2016 ) The troll - trust model for ranking in signed networks . In : Proceedings of the ninth ACM interna - tional conference on Web search and data mining . ACM , pp 447 – 456 Soc . Netw . Anal . Min . ( 2017 ) 7 : 26 Page 15 of 15 26 123