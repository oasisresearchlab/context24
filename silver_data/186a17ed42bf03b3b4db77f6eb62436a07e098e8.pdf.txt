Incremental Learning From Scratch Using Analogical Reasoning Vincent Letard 1 , 2 , 3 , Sophie Rosset 1 and Gabriel Illouz 1 , 2 , 3 1 LIMSI CNRS , France 2 Universit´e Paris Sud , France 3 Universit´e Paris Saclay , France ﬁrstname . lastname @ limsi . fr Abstract —This paper explores the application of formal ana - logical reasoning to incremental machine learning . The applica - tive context is the design of an operational assistant . The speciﬁc learning task that is focused on is the transfer from requests in natural language to commands in programming language . This work explores two questions for applying analogy in incremental learning situations : How does formal analogical reasoning behave in incremental learning situation ? How do the conditions on the learning sequence inﬂuence the performance ? To address these issues , an experimental setup is proposed in which multiple users are simulated . The knowledge transfer from one to the other is studied . Moreover , we discuss the inﬂuence of the order in which examples are presented to the system on the learning process . I . I NTRODUCTION An operational assistant is a system that answers to requests from a user with adequate operations that can be done on a computer . In our work , we focus speciﬁcally on operational assistants that take requests directly in natural language . The task to be addressed is therefore natural to formal language transfer : the system will use the natural language requests in input to generate commands in output . In this very context , the commands – in formal language – must be syntactically coherent . Case - based methods generally preserve this property . Among them , analogical reasoning is known to be naturally intelligible for humans . For these reasons , we based our approach for natural to formal language transfer on analogy . This work falls within the ﬁeld of formal analogical rea - soning . It is to be distinguished from analogical models from the cognitive artiﬁcial intelligence domain . Formal analogical reasoning was introduced with [ 1 ] . It was deﬁned on words as character sequences , and later generalized to algebraic structures in [ 2 ] , [ 3 ] . Cognitivist analogy was studied in earlier works such as [ 4 ] , [ 5 ] , [ 6 ] , inspired by works in the ﬁeld of psychology [ 7 ] . It deals with objects expressed in speciﬁc world representations such as predicate logic . The issue of making algorithms scale - proof was not the main focus . However , analogical reasoning methods are costly and their performances suffer from very big example bases . A workaround for this issue is to use fewer but better examples . This can be done by getting them directly from the interaction with the user , that is incrementally . Example 1 shows the intended behavior of the system , incrementally learning by interaction . As a case - based method , analogical reasoning does 1 U : Send the folder papers to home - pc 2 S : I don’t know , can you show me how ? 3 U : scp - r papers / home - pc : 4 S : ok 5 U : Check the disk space at home - pc 6 S : I don’t know , can you show me how ? 7 U : ssh home - pc " df " 8 S : ok 9 U : Send the folder papers to my PC 10 S : I don’t know , can you show me how ? 11 U : scp - r papers / home - pc : 12 S : ok 13 U : Check the disk space at my PC 14 S : ssh home - pc " df " . Is it correct ? 15 U : Yes Example 1 : Simulated behavior of the system ( S ) interacting with a user ( U ) with zero initial knowledge . not imply a heavy preprocessing of the example base nor does it require quantities of examples to produces an answer . For those two reasons , formal analogical reasoning is expected to perform honorably in an incremental learning situation . To the best of our knowledge , this has not been assessed yet . How does formal analogical reasoning behave in incremen - tal learning situations ? How do the conditions on the learning sequence inﬂuence the performance ? We propose to examine these questions through the experiences presented in this paper . The next section presents related work on incremental learning , Section III describes formal analogical reasoning and gives foundational references . In Section IV , our experiments and their results are described and we conclude in Section V with some discussion and perspectives . II . R ELATED W ORK A learning task is incremental if the training examples used to solve it become available over time , usually one at a time , as deﬁned by [ 8 ] . The author points out some task categories , such as designing intelligent agents , that are naturally relevant to incremental learning . Incrementality is still not among the most commonly used paradigms for these task categories . In such tasks , generally speaking , supervised [ 9 ] , [ 10 ] and rein - forcement learning [ 11 ] are widely used . However , new data is 2016 IEEE 28th International Conference on Tools with Artificial Intelligence 2375 - 0197 / 16 $ 31 . 00 © 2016 IEEE DOI 10 . 1109 / ICTAI . 2016 . 37 204 2016 IEEE 28th International Conference on Tools with Artificial Intelligence 2375 - 0197 / 16 $ 31 . 00 © 2016 IEEE DOI 10 . 1109 / ICTAI . 2016 . 37 204 2016 IEEE 28th International Conference on Tools with Artificial Intelligence 2375 - 0197 / 16 $ 31 . 00 © 2016 IEEE DOI 10 . 1109 / ICTAI . 2016 . 37 204 constantly needed , either for improving the performance of an existing system or for feeding the learning of a new system . Indeed , updating the data processing of the system is likely to invalidate the existing corpora due to their non changing data structure . For example , a new processing may use a feature that is not present in the current corpora , therefore requiring a new costly collection . This remark is particularly relevant to the domain of intelligent agents , where the system has to interact with the human . More generally , using incremental learning makes possible the use of symbolic learning methods while avoiding the closed - world assumption . Also , as mentioned by [ 12 ] , another argument for incremental learning is that it may not be efﬁcient to keep every single example in memory , either because examples take a lot of space or because the system takes longer to explore them all at each use . Zhou also distin - guishes between three types of incremental learning ( IL ) for classiﬁcation problems : example - IL where new examples are added , class - IL where new classes may be encountered , and attribute - IL where the systems should incrementally adapt to the addition of new features with the examples . The approach we describe below belongs to example IL , and may be viewed as class IL although the task cannot quite be considered as a classiﬁcation problem . From incrementality emerges the issue of the ordering of the input examples . Even if the ﬁnal performance is similar , different input orders are likely to cause differences in the intermediate performance of the system . The behavior of the human is known to be hard to predict , making the ordering difﬁcult to control . The ﬁeld of algorithmic teaching [ 13 ] , [ 14 ] aims at guiding the teaching made by the human with the help of a set of instructions in order to improve the grounding between both the system and the human . One of the main arguments is that algorithmic learning is generally not natural for humans , explaining the poor relevance of the given examples . In our context , though , analogical reasoning is known for being human cognition friendly [ 15 ] . Also , the system is supposed to interact more with a user than with a teacher , and even though she will play both roles , the intention is to keep the number of constraints and assumptions on the learning process as low as possible . III . F ORMAL A NALOGICAL R EASONING Our work is based on formal analogical reasoning and is an instance of the analogical learning proposed by [ 16 ] . It relies on the concept of proportional analogies that we describe in the following section . A . Deﬁning Proportions A proportional analogy is a relation between four elements that can be stated as “A is to B as C is to D” . A short example of proportion in the domain of natural to formal language transfer is : “ Display f . txt is to cat f . txt as Display ﬁle is to cat file ” . We use the notation [ A : B : : C : D ] , for an analogical proportion , and [ X : Y : : Z : ? ] for an analogical equation . Solving formal analogical problems involves two types of operations : • verifying an analogical proportion • solving an analogical equation When applied to sequences , those operations rely on the identiﬁcation of an alignment of the tokens . The example 2 shows the elicitation of alignments for the operation of resolu - tion . The columns delimit factors for each sequence . Given an a c x b c y a d x a c x b c y a d x Example 2 : Two analogical subsequence alignments for the equation [ acx : bcy : : adx : ? ] alignment , the solution is formed by completing the alternating factors f x in one of the forms ( f 1 , f 1 , f 2 , f 2 ) or ( f 1 , f 2 , f 1 , f 2 ) . For the example , this gives the solutions bdy for the left hand alignment , and dby for the right hand one . These operations are computationally costly . However efﬁ - cient algorithms for performing the operations of veriﬁcation and resolution using dynamic programming are described in [ 2 ] , [ 3 ] . Their algorithmic complexity is in O ( n 3 ) , n being the size of the longest sequence . Among them , we report as algorithm 1 the resolution using dynamic programming . It ﬁlls a three dimensional matrix of size | X | × | Y | × | Z | where X , Y and Z are the sequences of the analogical equation to be solved . The search in the matrix corresponds to the concurrent and ordered iteration in the symbols of the three sequences . Each cell ( i , j , k ) contains a set of the partial solutions up to the corresponding indices in the sequences . Its content is computed using legal transitions for analogy , that is : • reading identical symbols on X and Y • reading identical symbols on X and Z • reading a character on Y and appending it to the solution • reading a character on Z and appending it to the solution The algorithm appears to output the set of the possibly many analogical solutions , however in practice , storing the sets of partial solutions in the matrix is too costly , so only the backward paths of the multiple solutions are kept . Thus , the output is a graph of paths through the matrix rather than an exhaustive set , compressing the size of the returned informa - tion from O ( e n ) to O ( n 3 ) . This has the effect of delegating the exponential computation time of path enumeration to the 205 205 205 caller . The latter can of course select a subset or a single path . Additional information can also be computed and associated with the paths such as the degree of the solution so far or the total number of paths for each output solution , that is the number of occurrences of the solution along the analogical alignments . These indicators are useful to discriminate the paths and their associated solutions in the returned set for the selection step . Algorithm 1 : Analogical resolution on sequences Input : Three sequences X , Y , Z Output : The set of the solutions to [ X : Y : : Z : ? ] 1 forall the ( i , j , k ) ∈ { − 1 , 0 } 3 do 2 s ( i , j , k ) ← ∅ ; 3 end 4 for i ← 0 , | x | do 5 for j ← 0 , | y | do 6 for k ← 0 , | z | do 7 if i = j = k = 0 then 8 s ( i , j , k ) ← { } ; 9 else 10 s ( i , j , k ) ← s ( i − 1 , j − 1 , k ) if x ( i ) = y ( j ) ∪ s ( i − 1 , j , k − 1 ) if x ( i ) = z ( k ) ∪ s ( i , j − 1 , k ) . y ( j ) ∪ s ( i , j , k − 1 ) . z ( k ) 11 end 12 end 13 end 14 end 15 return s ( i , j , k ) ; Having introduced algorithms for the processing of analo - gies , we will now consider how they can be articulated for producing answers using an example base that contains language transfer associations . B . Language Transfer Formal analogical reasoning can be applied to the problem of language transfer by processing sentences – here , requests and commands – as sequences of tokens – words or characters . Here , we call command the string of characters that is typed by the user in the terminal ( e . g . “ ls - l folder / ” ) . Options and parameters are included : two commands are identical only if they are exactly the same string of characters . Similarly , we call request the whole string of characters that is sent by the user to the system . As an illustration , the alignment of the requests in Example 1 gives the following factors : ( “Send the folder papers to” , “Check the disk space at” ) ( “home - pc” , “my PC” ) . The example base B used for language transfer is composed of pairs of associated sentences across languages – here , the request domain R is in French and the command domain C is in bash . B ⊂ ( R × C ) Given a request r ∈ R , the example base is searched for analogical proportions in order to generate an answer s . For this purpose , two strategies can be used : • looking for equations of the form [ r i : c i : : r : ? ] • looking for proportions of the form [ r i : r j : : r k : r ] in order to solve the entailed equation [ c i : c j : : c k : ? ] ( r i ) Print 14 copies of ex . pdf : ( c i ) lp - n 14 ex . pdf : : ( r ) Print 2 copies of ﬁle . pdf : ? Example 3 : Direct analogical resolution ( r ) Compile the C ﬁle f ( r k ) Compile f . c ( r i ) Count lines in f . c ( r j ) Count lines in the C ﬁle f : : : : ( c k ) gcc f . c ( c i ) wc - l f . c ( c j ) wc - l f . c : : : : ? Example 4 : Indirect analogical resolution These strategies are illustrated in examples 3 and 4 , we call them the direct and the indirect strategies respectively . In the example 3 , the request “Print 2 copies of ﬁle . pdf” is solved using the mixed pair ( r i , c i ) extracted from the example base . This analogical equation conducts the system to produce the answer “ lp - n 2 file . pdf ” . In the example 4 , the request “Compile the C ﬁle f” is solved by retrieving a triplet of requests ( r i , r j , r k ) from the base that forms an analogical proportion with r . The system then extracts the commands ( c i , c j , c k ) associated with this triplet of requests and solves the analogical equation to produce the answer “ gcc f . c ” . Each of these strategies rely on a different way of searching the example base . The direct strategy involves the resolution of a mixed analogical equation from which we are given the request r . The triplet can be completed with a request - command pair by extracting it directly from the example base . This is done in O ( | B | ) . The indirect strategy requires the constitution of a quadruplet of requests as a ﬁrst step , of which one request r is given . The example base contains no preexisting association between requests and other requests , hence we need to list every possible triplet of requests from the base in order to complete the quadruplet and verify the proportion . The naive way of doing this has a computational 206 206 206 complexity in O ( | B | 3 ) , which rapidly becomes too costly with fair number of stored examples . However , optimization was proposed such as count - tree indexing [ 17 ] which has a computational complexity of the search using the tree - count is O ( | B | × | L | ) , where L is the lexicon containing every possible token ( from the base ) . We use this optimization in our implementation . Both strategies allow the production of relevant unseen commands , that is sequences of symbols that were not as - sociated yet with any request in the example base . They are also complementary : the former can generate outputs using shared vocabulary between the natural language and the formal language , and the latter can provide outputs using exclusive vocabulary variations . The resolution phase identiﬁes examples in the base to ﬁnd valid proportions . As shown in the previous section , there are several alignments for each valid analogical proportion , and a solution for each of them . One must be selected in order for the system to give a single answer . In the literature [ 2 ] , [ 18 ] , the alignment involving the fewest factors is considered to represent the best proportion ( here “the best” stands for “the most natural” ) . The number of factors is called the degree of the analogical proportion . This selection policy usually yields a variable recall but a very high accuracy [ 19 ] . C . Generating New Examples In the case where the incremental learning process starts from scratch , the system will have to deal with a very limited number of examples to produce answers . In this section , we describe an approach to exploit as much of the existing examples as possible using analogy to generate new examples [ 19 ] . The effects of using the generated examples on the results with and without increment are described in section IV - D . Since analogical equations can be formed across languages as well as exclusively over one language , the solving algorithm can also be used in order to extend the example base . The method is to form all the possible triplets of requests from the example base and to apply the solving algorithm on each of them . For instance , having the three ﬁrst pairs from Example 1 in the base ( up to line 12 ) , the generation phase would use the three requests to produce a new one by analogy . This new request is actually the one at line 13 . For each request triplet that successfully gives an analogical solution , we use their associated commands to form an analog - ical equation . Finally , if this last equation gives a solution , the generated request and the generated command are added to the base as a new example . Thus in the example , the generation phase adds the request at line 13 and its solution before it is encountered . This process allows adding a lot of examples to the base at the cost of introducing noise from generated analogical solutions that may have no meaning . In practice , every solution that could be analogically derived with the initial example base is added . They can thus be accessed much faster and other analogical resolutions may be possible using the extended base . The resulting generated example base is bigger and takes more time to search , and one can think of removing the spurious sentences in order to at least remove the noise . However , they most often won’t lead to incorrect answers . In - deed , the generated sentences that are grammatically incorrect , such as “Please delete the ﬁle the ﬁle f . txt please” , or even meaningless , often won’t form valid analogical equations with correct sentences . To be valid , an analogical equation must verify the necessary property of being stable through morphism [ 3 ] . This implies in particular that the counts of the tokens has to form a valid analogy over addition , that is : [ A : B : : C : D ] ⇒ ∀ t ∈ L | A | t + | B | t = | C | t + | D | t where | X | t is the number of occurrences of the token t in the sequence X . Nevertheless , the generated sentences are signiﬁcantly more numerous than the original ones , and operating a manual annotation for correct and incorrect outputs quickly becomes unrealizable . In our case , the 589 examples total of the initial bases was multiplied by 126 in average up to 73k generated examples , see Table I . Besides , annotating whether a natural language utterance corresponds to the command it is supposed to entail is not trivial , as many mistakes are not accounted as such in oral or everyday language . D . Incremental Learning To the best of our knowledge , formal analogical reasoning has not been used in incremental learning until now . Yet , the method of machine learning using analogical proportions is inherently incremental . Furthermore , in our case the data from the example base is not preprocessed except for the indexation phase of the count - tree which has a logarithmic complexity with respect to the size of the base . Every new example is straightforwardly made available to the search for analogical proportions in the later computations . pairs commands cohorts CLASSICAL - USERS 512 21 20 generation 54 , 243 68 20 NEW - USER 77 20 20 generation 1 , 205 43 20 generation total 73 , 628 230 20 TABLE I : Example bases constitution The numbers under pairs count the request - command couples of the base . The generation rows gives the number of new and unique pairs that were generated using the whole base . The total count of commands takes every existing parameter variation as a difference between two commands . IV . A SSESSING F ORMAL A NALOGY IN I NCREMENTAL L EARNING S ITUATION In this section , we examine the performance of an incremen - tal learning system that answers to requests with commands . To proceed , we listed some testing conditions that are relevant to the purpose of the system and to incrementality : 207 207 207 1 ) the learning order 2 ) the size of the base 3 ) the variation of the users 4 ) the use of example generation The associated testing protocols and what can be derived about incrementality and analogy from the obtained results are detailed in the following subsections . Table I displays the characteristics of the example bases that we used . The initial base is called CLASSICAL - USERS because it ﬁgures the main knowledge base of the system , containing examples given by the regular users . The NEW - USER base stands for a set of requests formulated by a different user for the same goals . The respective sizes of the associated generated bases are also given . These bases were obtained as described in the previous section using every possible triplet of the base they come from . In the following , we designate three ordering conﬁgurations of the example base : • grouped by cohorts • grouped by cohorts in reversed order of the former • shufﬂed The boundaries of the cohorts while sorting by cohorts are as follows ( the given numbers are starts of new cohorts ) : 1 , 38 , 108 , 129 , 156 , 172 , 215 , 235 , 285 , 314 , 337 , 363 , 385 , 406 , 409 , 448 , 475 and 512 . Similarly , if we assume that there are N examples , and if we name the above set of boundaries B , the boundaries for the reverse order are given by B r = { 1 − b ∈ B } . In our case , we deﬁned that two examples ( r 1 , c 1 ) and ( r 2 , c 2 ) belong to the same cohort (cid:2) C n if the head of their commands are identical . ( r 1 , c 1 ) ∈ (cid:2) C n ∧ ( r 2 , c 2 ) ∈ (cid:2) C n ⇒ head ( c 1 ) = head ( c 2 ) We ﬁnd the head of a command by extracting its ﬁrst token . For example “Remove the ﬁle foo . odt” ( rm foo . odt ) and “Recursively delete the folder dir” ( rm - R dir / ) belong to the same cohort , whereas “Remove the variable $ VAR” ( unset VAR ) does not . Note that several shufﬂes have been achieved in order to control the potential variation . However , they were all very similar in shape , the average and standard deviation of their ﬁnal scores are 266 . 4 and 4 . 27 . They have not been placed here for clarity of the ﬁgure . For readability of the graphs in the following subsections , we estimate the learning performance of the system using the cumulative number of correct answers given by the system along the increment progression . Consequently , the relevant features to read on the graphs are the ﬁnal result and the shape of the curves . The objective is then to maximize δ y δ x ( and all the subsequent derivatives ) . The line y = x is also drawn on the graphs as it represents the upper bound that can ever reach an evaluated system . It corresponds to giving exactly one correct answer for every input request . This is also the maximal slope that can be reached by the curve of the system , one cannot give more than one correct answer for a given input request . A . Learning Order Fig . 1 : Effect of the order of the input examples on the performance of incremental learning In order to characterize the effect of learning order on performance , the system was run with incrementality on the different orderings of the CLASSICAL - USERS base . We iteratively submitted each request from the set , then added the request associated with its correct command to the example base and repeated the operation for the next request . This addition of the solution pair to the example base can be seen as the process of immediately validating the output of the system . On validation , the example is added ; on error , the correct solution is requested by the system . Figure 1 shows the performance on the CLASSICAL - USERS base , with three different orderings . All three orderings are correctly answered for a little over half of the requests , even though the shufﬂed one has about 15 correct answers less than the others . Interestingly and despite the closeness of the three ﬁnal scores , the overlap of correctly answered requests between the runs is rather low ( 48 % at most ) . This must be due to the combination of the symmetry of analogical proportions , and the fact that examples are sometimes among the ﬁrst of their cohort , and sometimes among the last , depending on the run . In other words , where a run allows to correctly answer to D while having seen A , B and C , another can allow to correctly answer to A while having seen D , B and C . However , the symmetry of analogy is limited to certain cases : [ A : B : : C : D ] ⇔ [ A : C : : B : D ] ⇔ [ C : A : : D : B ] which explains why the ﬁnal scores can differ from more than a unit . The ﬁgure shows that the ordering of the incoming examples has an effect on the score . Grouping examples seems better as it provides a higher overall score . Nonetheless , the shufﬂed set curve is smoother and its derivative is strictly increasing , which are good properties when having in mind to continue 208 208 208 the learning . The respective slopes of the curves over the last 200 examples are 0 . 47 , 0 . 68 and 0 . 74 ( in the same order as the caption of the ﬁgure ) . If the learning continues with the same rates , the shufﬂed condition is expected to quickly reach the others . We will now consider more detailed results about the effects of ordering with different base sizes . B . Variations with the Size of the Base The performance that can be reached with the three previ - ously tested orderings may also be inﬂuenced by the content of the example base . Even in an incremental setting , it may be necessary to stop or suspend the learning at some point . For example , if a non - programmer interacts with the system , he would not be able to correct the system when it makes a mistake . The system has been tested while artiﬁcially stopping the increment from various starting points . This is reported on Figures 2 and 3 , each displays the performance of 10 runs with increment stopped at starting indices from 50 to 500 , plus the original run without stopping . Stopping the increment at an index n can be seen here as simulating a batch learning run with an initial base composed of the ﬁrst n requests . We can immediately notice a difference between the grouped ( Figure 2 ) and shufﬂed ( Figure 3 ) runs . With the run grouped by cohorts , virtually no correct answer is given after the increment has stopped , whereas with the shufﬂed run , correct answers continue to be given at a lower rate than the runs stopping after . This is easily explainable by the fact that the knowledge is only reused locally when the base is grouped by cohorts , whereas a shufﬂed base tends to evenly distribute examples for each cohort in the base . This demonstrates that the ﬁnal score obtained on the grouped base is highly dependent on incremental learning since batch learning runs using preﬁxes cannot generalize enough to get a comparable score . In the light of this compar - ison with batch learning , and contrarily to what was hinted by the ﬁrst results , the performance of the system using incremen - tality is not very sensitive to the learning order . However , the experiment shows that the condition where incoming examples are shufﬂed allows the system to learn more quickly , thanks to a more knowledge - dense example base . Besides , the shufﬂed condition is what the system is generally expected to encounter in real - life interaction ( although not with similar rates ) , which makes this result especially interesting . Finally , considering that the system is multi - user , this observation suggests that it is more useful to get few examples from every user than a lot from only one or two . This brings the question of how does incremental learning make the system behave on the introduction of a new user . C . Introducing a New User Since natural language is used for the requests to the system , it is expected that different users will have different styles for writing requests and different needs in terms of command parameters . We evaluated the generalizing capacity of the system , with and without incrementality , by submitting the Fig . 2 : Behavior of the system after interrupting the incremen - tal addition of the examples grouped by cohorts . Fig . 3 : Behavior of the system after interrupting the incremen - tal addition of the shufﬂed examples . requests of the NEW - USER base after including the examples from the CLASSICAL - USERS base to its knowledge . Figure 4 shows the outcome of this experiment . The perfor - mance previously obtained on the shufﬂed CLASSICAL - USERS base starting from scratch and cut after the ﬁrst 77 requests has also been added for comparison . The result conﬁrms the utility of incrementality as it improved the batch result by 60 % , though there might be some noise issues due to the size of the test set . The total correct answers of the 77 submitted requests are 16 with incrementality , and 10 without . Comparatively , the previous run on the CLASSICAL - USERS base achieved only 8 . The slopes of the curves with and without incrementality are respectively 0 . 35 and 0 . 24 at the end of the run , between the abscissae 60 and 77 . The run on the CLASSICAL - USERS base had a slope of 0 . 18 between the same abscissae . This result may be increased as it did in the 209 209 209 Fig . 4 : Performance with and without incrementality for a run on the NEW - USER base , starting with the knowledge of the CLASSICAL - USERS base The result curve of the previous test on the CLASSICAL - USERS base from scratch is given for comparison ( it used incrementality ) ﬁrst experiment using a bigger number of examples . Formal analogical example generation can be used in order to obtain more data from the few examples we have . D . Extending the Example Base Last of the listed conditions , the generation of every possible example by analogy using the existing base is expected to yield better results while incorporated in the knowledge base . As detailed in Section III - C , every time a new example is added , it is confronted to the rest of the base in order to look for valid analogical proportions . All the so - formed new examples are also added to the base , each step thus leads to an average of 126 additions to the base – though the actual rate starts at 1 and grows with the square of the size of the base . Proportions are also formed using examples from both bases , which should generate beneﬁcial crossings . Note that generated examples are not reused for other generations , the maximal number of generating steps is always 1 . The NEW - USER base is tested having the CLASSICAL - USERS base as knowledge , along with all the generated examples . Figure 5 gives a summary of the tests on the NEW - USER base . The exclusive usage of incrementality or generation gives a similar result . Nevertheless , there are several factors inﬂuencing the score using generation such as the size of the CLASSICAL - USERS base , or the average syntactic distance between the requests of the CLASSICAL - USERS base and the NEW - USER base . Those factors are obviously independent of the use of incrementality , and the closeness of these two results is certainly fortuitous , although it would deserve further assessment . We can notice that the generation gives higher results than a simple batch run . This proves that the necessary knowledge to provide correct answers to some of the NEW - Fig . 5 : Compared performances using incrementality and gen - eration on the NEW - USER base , starting with the knowledge of the CLASSICAL - USERS base with generation without grouped by cohorts 61 . 8 % 54 . 2 % grouped ( reverse ) 61 . 4 % 53 . 9 % shufﬂed 60 . 3 % 50 . 9 % TABLE II : Summary of the ﬁnal scores for the incremental runs on the CLASSICAL - USERS base starting from scratch USER requests was implicitly contained in the CLASSICAL - USERS examples . Formal analogical generation made this knowledge explicit . The combination of generation and incrementality gives better results than each one separately , which outlines a complementarity of both . On one hand , it is not very surprising that formal analogical generation could not produce every correct answer produced by incrementality , but on the other hand , this means that even while adding every new example to the base , there are still some answers that cannot be found without one step of generation . Finally , despite barely reaching 31 % , the ﬁnal score using both incrementality and generation is more than twice the batch baseline with none of these . This result outperforms by more than 10 points the batch evaluation with generation from our previous work [ 19 ] . However , this score is still to be considered carefully because of the small size of the test set . To complete the results of Figure 5 , the scores with and without the generated examples for the CLASSICAL - USERS base are summarized in Table II . Interestingly , they show a similar absolute increase ( around 9 points ) with generation as for the tests on the NEW - USER base , with CLASSICAL - USERS base as initial knowledge . This suggests that the internal variations of the CLASSICAL - USERS base are as helpful for generating examples relevant to the same base as mutual variations between both bases are for generating 210 210 210 examples relevant to the NEW - USER base . This can be tested further against different NEW - USER bases or by varying their size . V . D ISCUSSION AND C ONCLUSION In this paper , we examined the behavior in incremental learning situation of a system that transfers natural language requests to formal language commands by formal analogical reasoning . Over the experiments that we reported , incremental learning and formal analogical generation have been shown to be complementary and rather efﬁcient considering the size of the example base . We found out that the learning order has little inﬂuence on the ﬁnal number of correct answers . In case the learning has to stop for some reason however , the learning order is very inﬂuential . In this case , the random order is a lot better than the ordered one . This is also the natural order that is expected in real interactive conditions . Also , we showed that incremental learning plays a role when adding a new user to the “community” . It produced an increase of 60 % between the score for relying solely on the community and the score for incrementally adding examples from the new user . This score was improved again using the generation of new examples . Despite the closeness of the incrementally acquired base to the incoming requests , our last experiment outlined that some answers could be given using formal analogical generation only . Finally , the combination of formal analogical reasoning and incremental learning allows at least 60 % of accuracy over the 512 requests of the initial base from scratch . This is very promising and opens several tracks for future work . First of all , these results may be compared with other approaches in incremental learning situation . In particular , the comparison with statistical approaches would be very interesting to determine the order of magnitude , for the size of the example base , where statistics become more efﬁcient than case - based reasoning . However , although some statisti - cal learning techniques have been successfully adapted – or developed – for incremental learning [ 20 ] , [ 21 ] , they concern classiﬁcation problems only and their adaptation to language transfer is not trivial . A shorter term track to explore is the comparison of these results , using the same methods , with results using data from other domains formalized as a sequence transfer or translation problem . A starting point is to use the dataset from [ 22 ] who address the very close problem of transferring natural language requests to regular expressions . It would be no less interesting , regarding our ﬁndings about base ordering and knowledge density , to look for optimal subsets of the example base : what is the smallest set of examples that is enough to give the same correct answers as with the whole incremental acquisition ? This would allow the search for the implicit analogical structure of the example base . At last , assessing our approach in real conditions would be very interesting for both data collection and evaluating the robustness of incrementality beyond from scratch . R EFERENCES [ 1 ] Y . Lepage , “Solving analogies on words : An algorithm , ” in Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1 , ser . ACL ’98 . Stroudsburg , PA , USA : Association for Computational Linguistics , 1998 , pp . 728 – 734 . [ Online ] . Available : http : / / dx . doi . org / 10 . 3115 / 980845 . 980967 [ 2 ] N . Stroppa , “Analogy - Based Models for Natural Language Learning , ” PhD Thesis , T´el´ecom ParisTech , Nov . 2005 . [ Online ] . Available : https : / / tel . archives - ouvertes . fr / tel - 00145147 [ 3 ] N . Stroppa and F . Yvon , “Formal models of analogical proportions , ” ´Ecole Nationale Sup´erieure des T´el´ecommunications , Paris , France , Tech . Rep . , 2007 . [ 4 ] M . H . Burstein , “Concept formation by incremental analogical reasoning and debugging , ” in Proceedings of the International Machine Learning Workshop . Citeseer , 1983 , pp . 19 – 25 . [ 5 ] D . R . Hofstadter , M . Mitchell et al . , “The copycat project : A model of mental ﬂuidity and analogy - making , ” Advances in connectionist and neural computation theory , vol . 2 , no . 31 - 112 , pp . 29 – 30 , 1994 . [ 6 ] J . B . Marshall , “Metacat : a program that judges creative analogies in a microworld , ” in Proceedings to the Second Workshop on Creative Systems . Citeseer , 2002 . [ 7 ] D . Gentner , “Structure - mapping : A theoretical framework for analogy , ” Cognitive Science , vol . 7 , pp . 155 – 170 , 1983 . [ 8 ] C . Giraud - Carrier , “A note on the utility of incremental learning , ” AI Communications , vol . 13 , no . 4 , pp . 215 – 223 , 2000 . [ 9 ] S . Hahn , M . Dinarelli , C . Raymond , F . Lef ` evre , P . Lehen , R . De Mori , A . Moschitti , H . Ney , and G . Riccardi , “Comparing stochastic ap - proaches to spoken language understanding in multiple languages , ” IEEE Transactions on Audio , Speech and Language Processing ( TASLP ) , vol . 16 , pp . 1569 – 1583 , 2010 . [ 10 ] E . Simonnet , N . Camelin , P . Delglise , and Y . Estve , “Exploring the use of attention - based recurrent neural networks for spoken language understanding , ” in Machine Learning for Spoken Language Understand - ing and Interaction NIPS 2015 workshop ( SLUNIPS 2015 ) , Montreal ( Canada ) , 11 dec . 2015 . [ 11 ] S . Chandramohan , “Revisiting user simulation in dialogue systems : do we still need them ? : will imitation play the role of simulation ? ” Theses , Universit´e d’Avignon , Sep . 2012 . [ Online ] . Available : https : / / tel . archives - ouvertes . fr / tel - 00875229 [ 12 ] Z . - H . Zhou and Z . - Q . Chen , “Hybrid decision tree , ” Knowledge - based systems , vol . 15 , no . 8 , pp . 515 – 528 , 2002 . [ 13 ] M . Cakmak and A . L . Thomaz , “Eliciting good teaching from humans for machine learners , ” Artiﬁcial Intelligence , vol . 217 , pp . 198 – 215 , 2014 . [ 14 ] F . J . Balbach and T . Zeugmann , “Recent developments in algorith - mic teaching , ” in Language and Automata Theory and Applications . Springer , 2009 , pp . 1 – 18 . [ 15 ] D . R . Hofstadter and E . Sander , Surfaces and Essences . Basic Books , 2013 . [ 16 ] N . Stroppa and F . Yvon , “An analogical learner for morphological anal - ysis , ” in Proceedings of the Ninth Conference on Computational Natural Language Learning . Association for Computational Linguistics , 2005 , pp . 120 – 127 . [ 17 ] P . Langlais and F . Yvon , “Scaling up analogical learning . ” in COLING ( Posters ) , 2008 , pp . 51 – 54 . [ 18 ] H . Somers , S . Dandapat , and S . K . Naskar , “A review of ebmt using proportional analogies , ” EBMT 2009 - 3rd Workshop on Example - Based Machine Translation , 2009 . [ 19 ] V . Letard , G . Illouz , and S . Rosset , “Analogical reasoning for natural to formal language transfer , ” in ICTAI , Vietri sul Mare , Italy , November 2015 . [ 20 ] R . Polikar , L . Upda , S . S . Upda , and V . Honavar , “Learn + + : An incremental learning algorithm for supervised neural networks , ” Sys - tems , Man , and Cybernetics , Part C : Applications and Reviews , IEEE Transactions on , vol . 31 , no . 4 , pp . 497 – 508 , 2001 . [ 21 ] T . Poggio and G . Cauwenberghs , “Incremental and decremental support vector machine learning , ” Advances in neural information processing systems , vol . 13 , p . 409 , 2001 . [ 22 ] N . Kushman and R . Barzilay , “Using semantic uniﬁcation to generate regular expressions from natural language , ” in Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics , 2013 . 211 211 211