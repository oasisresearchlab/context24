1 Hierarchical Federated Learning with Momentum Acceleration in Multi - Tier Networks Zhengjie Yang , Sen Fu , Wei Bao , Dong Yuan , and Albert Y . Zomaya Abstract —In this paper , we propose Hierarchical Federated Learning with Momentum Acceleration ( HierMo ) , a three - tier worker - edge - cloud federated learning algorithm that applies mo - mentum for training acceleration . Momentum is calculated and aggregated in the three tiers . We provide convergence analysis for HierMo , showing a convergence rate of O (cid:0) 1 T (cid:1) . In the analysis , we develop a new approach to characterize model aggregation , momentum aggregation , and their interactions . Based on this result , we prove that HierMo achieves a tighter convergence upper bound compared with HierFAVG without momentum . We also propose HierOPT , which optimizes the aggregation periods ( worker - edge and edge - cloud aggregation periods ) to minimize the loss given a limited training time . By conducting the experiment , we verify that HierMo outperforms existing mainstream benchmarks under a wide range of settings . In addition , HierOPT can achieve a near - optimal performance when we test HierMo under different aggregation periods . Index Terms —Federated learning ; momentum ; convergence analysis ; edge computing I . I NTRODUCTION With the advancement of Industry 4 . 0 , Internet of Things ( IoT ) , and Artiﬁcial Intelligence , machine learning applica - tions such as image classiﬁcation [ 1 ] , automatic driving [ 2 ] , and automatic speech recognition [ 3 ] are rapidly developed . Since the machine learning dataset is distributed in individual users and in many situations they are not willing to share these sensitive raw data , Federated Learning ( FL ) emerges [ 4 ] . It allows workers to participate in the model training without sharing their raw data . Typically , FL is implemented in two tiers , where multiple devices ( workers ) are distributed and connected to a remote aggregator ( usually located in the cloud ) . A potential issue of the two - tier FL setting is its scalability . The communication overhead between workers and the cloud is proportional to the number of workers , which causes problems when there are a large number of geo - distributed workers connecting to the remote cloud via the public Internet . With the development of edge computing [ 5 ] , a more effective solution is adding the edge tier between local workers and the remote cloud to address the scalability issue . Dif - ferent from the typical two - tier architecture , in the three - tier hierarchical architecture as shown in Fig . 1 , workers can ﬁrst communicate with the edge node for edge - level aggregation , and then the edge nodes communicate with the remote cloud for cloud - level aggregation . Each edge node is closer to the workers and is usually connected with them in the same local / edge network , so that the communication cost is much cheaper compared with the two - tier case when the workers directly communicate with the cloud . In Fig . 1 , we can see that cloud server worker two - tier architecture public Internet three - tier architecture public Internet edge node end to end connection Fig . 1 . Two - tier architecture vs . three - tier architecture . 6 connections are through the public Internet in the two - tier architecture but only 2 connections are through the public Internet in the three - tier architecture . Communication burdens are restrained in the local / edge networks . much of the trafﬁc through the public Internet ( left subﬁgure ) is restrained in the local edge networks ( right subﬁgure ) due to the existence of the edge nodes . Therefore , the three - tier architecture is a good ﬁt for larger - scale FL , and has attracted attentions from researchers in recent years [ 6 ] – [ 8 ] . Although the three - tier FL can improve the communication efﬁciency in one training iteration by replacing worker - cloud communication with worker - edge communication , there is also a need to accelerate its convergence performance to reduce the number of iterations . One obstacle in the three - tier FL is that each edge node can only aggregate the updates of its local workers , and there is a discrepancy among edge nodes . The edge nodes are to be synchronized in the cloud - level aggregation . The two - level aggregation causes delayed synchronization , leading to less training efﬁciency . Therefore , it is a strong motivation for us to develop a more efﬁcient algorithm to accelerate the convergence , reducing the number of training iterations in the three - tier hierarchical architecture , and ﬁnally improve the overall training efﬁciency ( considering both per - iteration cost and the number of iterations ) . Momentum is proved to be an effective mechanism to accelerate model training . Many studies have demonstrated its advantage in both centralized machine learning environ - ment [ 9 ] – [ 12 ] and two - tier FL environment [ 13 ] – [ 16 ] . Apart from the conventional gradient descent step , the momentum method conducts additional momentum steps [ 17 ] to accelerate convergence . In this paper , we propose Hierarchical Feder - ated Learning with Momentum Acceleration ( HierMo ) , which leverages momentum to accelerate three - tier FL . HierMo is operated as follows : 1 In each iteration , each worker locally updates its own model and worker momentum ; 2 In every a r X i v : 2210 . 14560v1 [ c s . L G ] 26 O c t 2022 2 τ iterations ( τ is called the worker - edge aggregation period ) , each edge node receives , averages , and sends back the models and momentum values with its connected workers . 1 3 In every τ · π iterations ( π is called the edge - cloud aggregation period ) , the cloud receives , averages , and sends back the models and momentum values with edge nodes . The edge nodes will then distribute them to connected workers . The above 1 – 3 steps are repeated for multiple rounds until the loss is sufﬁciently small . Theoretically , we prove that HierMo is convergent and has an O (cid:0) 1 T (cid:1) convergence rate for smooth non - convex problems for a given T iterations . In this step , we need to address substantial new challenges , compared with two - tier FL . In particular , we develop a new method to characterize the multi - time cross - two - tier momentum interaction and cross - three - tier momentum interaction , which do not exist in the two - tier FL . After we theoretically prove the convergence , we observe that the worker - edge and edge - cloud aggregation periods τ and π are key design variables we aim to optimize . Based on the result of the convergence analysis , we propose HierOPT algorithm , which can ﬁnd a local optimal ( τ , π ) value pair . In the experiment , we demonstrate the performance of HierMo compared with various mainstream hierarchical FL and momentum - based FL algorithms , including hierarchical FL without momentum ( HierFAVG [ 18 ] and CFL [ 19 ] ) , two - tier FL with momentum ( FedMom [ 20 ] , SlowMo [ 21 ] , Fed - NAG [ 22 ] , Mime [ 23 ] , DOMO [ 24 ] , and FedADC [ 25 ] ) , and two - tier FL without momentum ( FedAvg [ 4 ] ) . The experiment is implemented on different kinds of models ( linear regression , logistic regress , CNN [ 26 ] , VGG16 [ 27 ] , and ResNet18 [ 28 ] ) based on various real - world datasets ( MNIST [ 29 ] , CIFAR - 10 [ 30 ] , ImageNet [ 28 ] , [ 31 ] for image classiﬁcation , and UCI - HAR [ 32 ] for human activity recognition ) . The experimental results illustrate that HierMo drastically outperforms bench - marks under a wide range of settings . We also verify HierOPT can output a near - optimal ( τ , π ) in the real - world settings . All these results match our expectations by the theoretical analysis . The contributions of this paper are summarized as follows . • We have proved that HierMo is convergent and has an O (cid:0) 1 T (cid:1) convergence rate for smooth non - convex problems for a given T iterations under non - i . i . d . data . • We have proved that as long as learning step size η is sufﬁciently small , HierMo ( with momentum acceleration ) achieves the tighter convergence upper bound than Hier - FAVG ( without momentum acceleration ) . • We have proposed the new HierOPT algorithm which can ﬁnd a local optimal pair of ( τ ∗ , π ∗ ) when total training time is constrained . • HierMo is efﬁcient and decreases the total training time by 21 – 70 % compared with the mainstream two - tier momentum - based algorithms and three - tier algorithms . • HierOPT generates the near - optimal pair of ( τ ∗ , π ∗ ) when the total training time is constrained . HierOPT achieves the near - optimal accuracy with only 0 . 23 – 0 . 29 % 1 Each edge node also calculates another momentum for its own usage to further accelerate convergence . See Section III for the detailed algorithm . ( CNN on MNIST ) and 0 . 04 – 0 . 16 % ( CNN on CIFAR10 ) gap from the real - world optimum . The rest of the paper is organized as follows . In Section II , we introduce related works . The HierMo algorithm design is described in Section III . In Section IV , we provide theoretical results including the convergence analysis of HierMo and the performance gain of momentum . The algorithm to optimize the aggregation periods , i . e . , HierOPT , is proposed in Sec - tion V . Section VI provides our experimental results and the conclusion is made in Section VII . II . R ELATED W ORK A . Momentum in Machine Learning and Federated Learning Momentum [ 33 ] is a method that helps accelerate gradient descent in the relevant direction by adding a fraction γ of the difference between past and current model vectors . In the classical centralized setting , the update rule of the momentum ( Polyak’s momentum ) is as follows : m ( t ) = γ m ( t − 1 ) − η ∇ F ( w ( t − 1 ) ) , ( 1 ) w ( t ) = w ( t − 1 ) + m ( t ) , ( 2 ) with γ ∈ [ 0 , 1 ) , t = 1 , 2 , . . . , m ( 0 ) = 0 , where γ is momentum factor ( weight of momentum ) , t is update iteration , m ( t ) is momentum term at iteration t , and w ( t ) is model parameter at iteration t . Through this method , the momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions . As a result , momentum gains faster convergence and reduces oscillation [ 17 ] , [ 34 ] . Momentum has been investigated in both centralized ma - chine learning and FL . In the centralized environment , an - other form of momentum called Nesterov Accelerate Gra - dient ( NAG ) [ 17 ] , [ 35 ] is proposed . NAG 2 calculates the gradient based on an approximation of the next position of the parameters , i . e . , ∇ F ( w ( t − 1 ) + γ m ( t − 1 ) ) , instead of ∇ F ( w ( t − 1 ) ) in Polyak’s momentum , leading to better convergence performance . In [ 11 ] , authors study the utilization of momentum in over - parameterized models . [ 9 ] provides a uniﬁed convergence analysis for both Polyak’s momentum and NAG . [ 12 ] studies NAG in stochastic settings . All the above works show the advantages of momentum to accelerate the centralized training and it attracts researchers’ attention to apply momentum in FL environment . Depending on where the momentum is adopted , we can categorize them into the worker momentum , aggregator momentum , and com - bination momentum . For the worker momentum ( e . g . , Fed - NAG [ 22 ] and Mime [ 23 ] ) , momentum acceleration is adopted at workers in each local iteration . However , it is vulnerable to data heterogeneity among workers , which may harm the long - run performance . For the aggregator momentum ( e . g . , FedMom [ 20 ] and SlowMo [ 21 ] ) , the momentum acceleration is adopted only at the aggregator based on the global model and it shares the same property of acceleration as in centralized setting and dampens oscillations [ 17 ] . Nevertheless , it is 2 There are two mainstream equivalent representations of NAG . In this paper , we employ the representation in [ 9 ] , [ 36 ] . 3 TABLE I K EY N OTATIONS η worker model learning rate τ worker - edge aggregation period π edge - cloud aggregation period γ worker momentum factor γ a edge momentum factor T number of total local ( worker ) iterations indexed by t K number of total edge aggregations indexed by k P number of total global ( cloud ) aggregations indexed by p L number of edge nodes indexed by (cid:96) C (cid:96) number of workers under edge node (cid:96) N number of workers in the system indexed by { i , (cid:96) } x ti , (cid:96) worker model parameter in worker { i , (cid:96) } at iteration t y ti , (cid:96) worker momentum parameter in worker { i , (cid:96) } at iteration t y t(cid:96) − aggregated worker momentum in edge node (cid:96) at iteration t x t(cid:96) − aggregated worker model in edge node (cid:96) at iteration t y t(cid:96) + updated edge momentum in edge node (cid:96) at iteration t x t(cid:96) + updated edge model in edge node (cid:96) at iteration t y t worker momentum cloud aggregation in the cloud at iteration t x t cloud model in the cloud at iteration t conducted less frequently ( every τ iterations 3 ) compared with worker momentum ( every iteration ) , and the performance gain may not be obvious especially when τ is large . To address the above limitations , works in [ 24 ] , [ 25 ] , [ 37 ] combine the worker and aggregator momenta and they show a better convergence performance than only using either worker or aggregator momentum . The above forms of momentum are only adopted and analyzed in the two - tier FL and we focus on the three - tier scenarios in this paper . B . Three - Tier Hierarchical Federated Learning Three - tier FL has attracted more attention in recent years . Without considering momentum , studies have demonstrated the convergence performance in three - tier FL [ 18 ] , [ 19 ] , [ 38 ] , [ 39 ] . The communication overhead can be further optimized in [ 40 ] . The convergence analysis extended from two - tier to three - tier FL is not straightforward . Different from two - tier FL where the global aggregation is executed every τ local iterations , in three - tier FL , each worker’s local model will be ﬁrst aggregated by the connected edge node every τ local iterations , and will then be aggregated by the cloud in another level of every π edge aggregations . Existing two - tier methods can only bound the two - tier effects , but not the three - tier effects . Substantial new challenges are encountered in this paper . When momentum is leveraged in the three - tier scenario , it additionally introduces multi - time cross - two - tier momentum interaction and cross - three - tier momentum interaction . This is completely different from the two - tier scenario . Existing two - tier analyses cannot deal with the above two new terms . They can only characterize multi - time inner - tier momentum acceler - ation and one - time cross - two - tier momentum interaction . We devise a two - level virtual update ( edge and cloud ) method , which is able to bound the aforementioned new terms so that the convergence of HierMo still holds . 3 τ the is aggregation period III . H IER M O P ROBLEM F ORMULATION A . Overview We consider a three - tier hierarchical FL system consisting of a cloud server , L edge nodes , and N workers . Each edge node (cid:96) serves C (cid:96) workers , and the total number of workers is N = (cid:80) L(cid:96) = 1 C (cid:96) . Worker { i , (cid:96) } denotes the i th worker served by edge node (cid:96) , where i = 1 , 2 , . . . , C (cid:96) . It contains its local dataset with the number of data samples denoted by D i , (cid:96) . The total training dataset in the cluster of workers served by edge node (cid:96) is D (cid:96) (cid:44) (cid:80) C (cid:96) i = 1 D i , (cid:96) and the total training dataset D (cid:44) (cid:80) L(cid:96) = 1 D (cid:96) = (cid:80) L(cid:96) = 1 (cid:80) C (cid:96) i = 1 D i , (cid:96) . The target of three - tier hierarchical FL is to ﬁnd the stationary point x ∗ that minimizes the global loss function F ( x ) that is the weighted average of all workers’ loss functions . The problem can be formulated as follows : min x ∈ R d F ( x ) (cid:44) 1 D L (cid:88) (cid:96) = 1 C (cid:96) (cid:88) i = 1 D i , (cid:96) F i , (cid:96) ( x ) ( 3 ) = L (cid:88) (cid:96) = 1 D (cid:96) D C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) F i , (cid:96) ( x ) ( 4 ) (cid:44) L (cid:88) (cid:96) = 1 D (cid:96) D F (cid:96) ( x ) , ( 5 ) where d is the dimension of x , F ( x ) is the global loss function at the cloud server , and F i , (cid:96) ( x ) is the local loss function at worker { i , (cid:96) } . ( 4 ) is the mathematical transformation from ( 3 ) by adding D (cid:96) . We also deﬁne the edge loss function at edge node (cid:96) as F (cid:96) ( x ) (cid:44) (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) F i , (cid:96) ( x ) , which is the weighted average of edge node (cid:96) ’s connected workers’ local loss func - tions F i , (cid:96) ( x ) . Therefore , by replacing (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) F i , (cid:96) ( x ) with F (cid:96) ( x ) in ( 4 ) , we can directly derive ( 5 ) , demonstrating that the global loss function is the weighted average of all edge loss functions as F ( x ) (cid:44) (cid:80) L(cid:96) = 1 D (cid:96) D F (cid:96) ( x ) . We assume the problem is within the scope of cross - siloed federated learning [ 41 ] where all workers are required to participate in the training with siloed data . Each worker represents a repository of data , and data are sensitive and non - i . i . d . . The key notations are summarized in Table I . B . Worker Momentum and Edge Momentum We notice that there are two types of momentum in two - tier FL : One type ( i . e . , worker momentum ) is calculated at each worker and is aggregated ; The other type ( i . e . , aggregator momentum ) is calculated at the aggregator . Since both types can accelerate the convergence , we adopt both of them in our work . In the three - tier case in our paper , the worker momentum is individually computed in each worker and aggregated in the edge node ( worker momentum edge aggregation ) and the cloud ( worker momentum cloud aggregation ) . We still call it worker momentum throughout the paper . For the aggregator momentum , we apply it at each edge node . Each edge node computes its own momentum and it is not shared with the workers or the cloud . We call it edge momentum throughout this paper . 4 Algorithm 1 HierMo algorithm . Input : τ , π , T = Kτ = Pτπ , η , γ , γ a Output : Final cloud ( global ) model parameter x T ( a ) 1 : For each worker , initialize : x 0 i , (cid:96) as same value for all i , (cid:96) , and y 0 i , (cid:96) = x 0 i , (cid:96) 2 : For each edge node , initialize : x 0 (cid:96) ( a ) = x 0 i , (cid:96) , and y 0 (cid:96) ( a ) = x 0 (cid:96) ( a ) 3 : for t = 1 , 2 , . . . , T do 4 : For each worker i = 1 , 2 , . . . , N in parallel , 5 : y ti , (cid:96) ← x t − 1 i , (cid:96) − η ∇ F i , (cid:96) ( x t − 1 i , (cid:96) ) / / Worker momentum update 6 : x ti , (cid:96) ← y ti , (cid:96) + γ ( y ti , (cid:96) − y t − 1 i , (cid:96) ) / / Worker model update 7 : if t = = kτ where k = 1 , . . . , K then 8 : For each edge node (cid:96) = 1 , 2 , . . . , L in parallel , 9 : y kτ(cid:96) − ← (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) y kτi , (cid:96) / / Worker momentum edge aggregation 10 : y kτ(cid:96) + ← x ( k − 1 ) τ (cid:96) + − (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) (cid:16) x ( k − 1 ) τ (cid:96) + − x kτi , (cid:96) (cid:17) / / Edge momentum update 11 : x kτ(cid:96) + ← y kτ(cid:96) + + γ a (cid:16) y kτ(cid:96) + − y ( k − 1 ) τ (cid:96) + (cid:17) / / Edge model update 12 : Set y kτ i , (cid:96) ← y kτ (cid:96) − for all worker i ∈ C (cid:96) / / Edge aggregated worker momentum re - distribution to workers 13 : Set x kτi , (cid:96) ← x kτ(cid:96) + for all worker i ∈ C (cid:96) / / Edge model re - distribution to workers 14 : end if 15 : if t = = pτπ where p = 1 , 2 , . . . , P then 16 : Aggregate y pτπ ← (cid:80) L(cid:96) = 1 D (cid:96) D y pτπ(cid:96) − / / Worker momentum cloud aggregation 17 : Aggregate x pτπ ← (cid:80) L(cid:96) = 1 D (cid:96) D x pτπ(cid:96) + / / Edge model cloud aggregation 18 : Set y pτπ(cid:96) − ← y pτπ for all edge node l ∈ L / / Cloud aggregated worker momentum re - distribution to edge nodes 19 : Set x pτπ(cid:96) + ← x pτπ for all edge node l ∈ L / / Cloud model re - distribution to edge nodes 20 : Set y pτπi , (cid:96) ← y pτπ(cid:96) − for all worker i ∈ C (cid:96) , l ∈ L / / Cloud aggregated worker momentum re - distribution from edge nodes to workers 21 : Set x pτπi , (cid:96) ← x pτπ(cid:96) + for all worker i ∈ C (cid:96) , l ∈ L / / Cloud model re - distribution from edge nodes to workers 22 : end if 23 : end for C . HierMo Algorithm In Algorithm 1 , we propose a momentum - based three - tier hierarchical FL algorithm , named as HierMo , which applies both worker momentum and edge momentum . HierMo aims to ﬁnd the ﬁnal cloud model x T ( a ) to solve the formula ( 3 ) . It conducts T local iterations , K edge aggregations , and P cloud aggregations , where T = Kτ = Pτπ , τ is the worker - edge aggregation period , and π is the edge - cloud aggregation period . 1 ) Worker update : In each local iteration t , each worker { i , (cid:96) } computes its worker update , which includes two things : 1 worker momentum update y ti , (cid:96) ( Line 5 ) and 2 worker model update x ti , (cid:96) ( Line 6 ) . 1 and 2 follow the Nesterov Accelerated Gradient ( NAG ) [ 35 ] momentum update and are conducted every iteration . Through this way , each worker can utilize its own worker momentum acceleration . 2 ) Edge update : When t = kτ , k = 1 , 2 , . . . , K , each edge node (cid:96) receives workers’ momenta and models in C (cid:96) and per - forms edge update , which includes two operations : 1 Worker momentum edge aggregation y kτ(cid:96) − ( Line 9 ) with re - distribution ( Line 12 ) . Through this way , some straggler workers with high data - heterogeneity whose local momenta y kτi , (cid:96) pointing to an inappropriate direction can be reﬁned from y kτ(cid:96) − . 2 Edge momentum y kτ(cid:96) + and model x kτ(cid:96) + update ( Lines 10 – 11 ) with model re - distribution ( Line 13 ) . Since the computation of edge momentum and model update is based on the edge model , it is equivalent to perform it in edge setting involving all workers’ dataset under edge node (cid:96) ( D (cid:96) = (cid:80) C (cid:96) i = 1 D i , (cid:96) ) . By doing so , it dampens oscillations [ 17 ] within the edge node . Please note that 1 and 2 are two operations on the same edge node , so that we use subscript “ − ” and “ + ” to label the momentum / model right after operations 1 and 2 respectively . Finally , both 1 and 2 are conducted in each edge node every τ iterations . 3 ) Cloud update : When t = pτπ , p = 1 , 2 , . . . , P , the cloud receives edge aggregated worker momentum y pτπ(cid:96) − and edge model x pτπ(cid:96) + for all (cid:96) ∈ L and performs cloud update , which includes two things : 1 Worker momentum cloud ag - gregation y pτπ ( Line 16 ) and re - distribution ( Lines 18 and 20 ) . Through this way , all edge nodes and workers receive the cloud aggregated worker momentum and mitigate the disadvantage caused by non - i . i . d . data heterogeneity . 2 Edge model cloud aggregation x pτπ ( Line 17 ) and cloud model re - distribution ( Lines 19 and 21 ) . Please note that the cloud will re - distribute the momentum and model to all edge nodes and all edge nodes will then distribute them to all workers when t is a multiple of τπ . IV . C ONVERGENCE A NALYSIS OF H IER M O In this section , we present the theoretical analysis of Hi - erMo . We ﬁrst provide preliminaries . Then , we introduce the concept of virtual update which is a signiﬁcant intermediate step to conduct convergence analysis . Afterward , we show the convergence guarantee of HierMo . Finally , we compare the convergence upper bound of HierMo and HierFAVG to analyze the performance gain of momentum . A . Preliminaries We assume F i , (cid:96) ( · ) satisﬁes the following standard condi - tions that are commonly adopted in the literature [ 13 ] , [ 22 ] , [ 42 ] . Assumption 1 . F i , (cid:96) ( x ) is ρ - Lipschitz , i . e . , (cid:107) F i , (cid:96) ( x 1 ) − F i , (cid:96) ( x 2 ) (cid:107) ≤ ρ (cid:107) x 1 − x 2 (cid:107) for any x 1 , x 2 , i , (cid:96) . Assumption 2 . F i , (cid:96) ( x ) is β - smooth , i . e . , (cid:107)∇ F i , (cid:96) ( x 1 ) − ∇ F i , (cid:96) ( x 2 ) (cid:107) ≤ β (cid:107) x 1 − x 2 (cid:107) for any x 1 , x 2 , i , (cid:96) . Assumption 3 . ( Bounded diversity ) The variance of local gradient to edge gradient is bounded . i . e . , (cid:107)∇ F i , (cid:96) ( x ) − ∇ F (cid:96) ( x ) (cid:107) ≤ δ i , (cid:96) for ∀ i , ∀ (cid:96) , and ∀ x . We also deﬁne δ (cid:96) as the weighted average of δ i , (cid:96) and δ as the weighted average of δ (cid:96) , i . e . , δ (cid:96) (cid:44) (cid:80) i ∈ C (cid:96) D i , (cid:96) D (cid:96) δ i , (cid:96) and δ (cid:44) (cid:80) (cid:96) ∈ L D (cid:96) D δ (cid:96) . 5 Interval Interval Interval Fig . 2 . Illustration of x t(cid:96) − , x t(cid:96) + , x t [ k ] , (cid:96) , x t { p } , and x t , when N = 4 , τ = 2 , π = 2 with each edge node serving 2 workers . Cyan lines show worker model update . Blue lines show worker model edge aggregation . Purple lines show edge model accelerated by edge momentum . Black lines show edge model cloud aggregation . Red dashed lines show edge model virtual update . Magenta dashed lines show cloud model virtual update . According to Assumptions 1 and 2 , and applying the Triangle Inequality to F i , (cid:96) ( x ) , it is straightforward to show that F (cid:96) ( x ) is ρ - Lipschitz and β - smooth . Applying the Triangle Inequality to F (cid:96) ( x ) , we can also derive that F ( x ) is ρ - Lipschitz and β - smooth . Assumptions 1 and 2 indicate that the function and the gradient of the function are not changing too fast . Assumption 3 indicates that the data distributed to all workers are heterogeneous and non - i . i . d . . δ i , (cid:96) is used to quantify the level of gradient divergence and is different at different workers . B . Virtual Update In order to index the edge aggregation and cloud aggrega - tion , we divide the total T local iterations into K edge intervals and P cloud intervals . T = Kτ = Pτπ . We use [ k ] to denote the edge interval t ∈ [ ( k − 1 ) τ , kτ ] for k = 1 , 2 , . . . , K , and { p } to denote the cloud interval t ∈ [ ( p − 1 ) τπ , pτπ ] for p = 1 , 2 , . . . , P . Please note that the edge aggregation occurs at the end of each edge interval and the cloud aggregation occurs at the end of each cloud interval . Therefore , each edge interval [ k ] contains τ local iterations with one edge aggregation , and each cloud interval { p } contains π edge intervals with one cloud aggregation , i . e . , { p } = ∪ k [ k ] for k = ( p − 1 ) π + 1 , ( p − 1 ) π + 2 , . . . , pπ . At the beginning of edge interval [ k ] when t = ( k − 1 ) τ , we set edge virtual update y ( k − 1 ) τ [ k ] , (cid:96) ← y ( k − 1 ) τ (cid:96) − , ( 6 ) x ( k − 1 ) τ [ k ] , (cid:96) ← x ( k − 1 ) τ (cid:96) + , ( 7 ) for each edge node (cid:96) , where y ( k − 1 ) τ [ k ] , (cid:96) and x ( k − 1 ) τ [ k ] , (cid:96) are set as the virtual aggregated values right after the edge aggregation occurs . Then , we further conduct edge virtual update as if model and momentum updates are conducted in the edge node . When t ∈ ( ( k − 1 ) τ , kτ ] , we conduct edge virtual update as y t [ k ] , (cid:96) ← x t − 1 [ k ] , (cid:96) − η ∇ F (cid:96) ( x t − 1 [ k ] , (cid:96) ) , ( 8 ) x t [ k ] , (cid:96) ← y t [ k ] , (cid:96) + γ ( y t [ k ] , (cid:96) − y t − 1 [ k ] , (cid:96) ) . ( 9 ) We repeat ( 6 ) – ( 9 ) for each edge interval [ k ] where k = 1 , 2 , . . . , K . Please note that only if t = kτ , k = 1 , . . . , K , y t(cid:96) − and x t(cid:96) + are computed . For ease of analysis , we de - ﬁne intermediate value x t(cid:96) − = (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) x ti , (cid:96) and y t(cid:96) − = (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) y ti , (cid:96) that are meaningful at any iteration t . Same as edge intervals , for each cloud interval { p } where p = 1 , 2 , . . . , P , the cloud virtual update is also conducted : y ( p − 1 ) τπ { p } ← y ( p − 1 ) τπ , ( 10 ) x ( p − 1 ) τπ { p } ← x ( p − 1 ) τπ , ( 11 ) when t = ( p − 1 ) τπ , and y t { p } ← x t − 1 { p } − η ∇ F ( x t − 1 { p } ) , ( 12 ) x t { p } ← y t { p } + γ ( y t { p } − y t − 1 { p } ) , ( 13 ) when p ∈ ( ( p − 1 ) τπ , pτπ ] . By applying virtual updates on edge nodes and the cloud , we can bound the gap between real updates and these virtual updates that can be then used to prove the convergence . Since in HierMo , the momenta and the models are aggregated on both edge nodes and the cloud , it brings much more challenges to conduct convergence analysis . The virtual update is an important intermediate process for convergence analysis and is one of our contributions in this paper . Fig . 2 illustrates the evolution of x t(cid:96) − , x t(cid:96) + , x t [ k ] , (cid:96) , x t { p } , and x t when τ = 2 , π = 2 . There are 2 edge nodes and each edge node serves 2 workers ( in total 4 workers in Fig . 2 ) . After every 2 local updates , there is an edge aggregation , and after every 2 edge aggregations ( 4 local updates ) , there is a cloud aggregation . Please note 1 x kτ [ k ] , (cid:96) and x kτ [ k + 1 ] , (cid:96) are different . x kτ [ k ] , (cid:96) is calculated from x ( k − 1 ) τ [ k ] , (cid:96) after τ edge virtual updates , while x kτ [ k + 1 ] , (cid:96) is directly given by x kτ(cid:96) + . 2 x kτ(cid:96) − and x kτ(cid:96) + are different . x kτ(cid:96) − is the intermediate value that is used for edge model / momentum update , while x kτ(cid:96) + is calculated from x kτ(cid:96) − during edge model / momentum update . 3 x ( k + 1 ) τ { p } and x ( k + 1 ) τ { p + 1 } are different . x ( k + 1 ) τ { p } is calculated from x ( k − 1 ) τ { p } after τ · π cloud virtual updates , while x ( k + 1 ) τ { p + 1 } is directly given by x ( k + 1 ) τ . C . Convergence Analysis In this section , we provide the convergence analysis of HierMo . In Theorem 1 , we ﬁrst focus on worker models under each edge node (cid:96) to bound the distance between edge intermediate value x t(cid:96) − and edge virtual update x t [ k ] , (cid:96) within interval [ k ] . Theorem 1 . For any edge interval [ k ] , ∀ t ∈ ( ( k − 1 ) τ , kτ ] and ∀ (cid:96) ∈ L , we have (cid:107) x t(cid:96) − − x t [ k ] , (cid:96) (cid:107) ≤ h ( t − ( k − 1 ) τ , δ (cid:96) ) , ( 14 ) 6 where h ( x , δ (cid:96) ) is h ( x , δ (cid:96) ) = ηδ (cid:96) (cid:18) I ( γA ) x + J ( γB ) x − 1 ηβ − γ 2 ( γ x − 1 ) − ( γ − 1 ) x ( γ − 1 ) 2 (cid:19) , ( 15 ) and A , B , I , and J are constants deﬁned in Appendix A , for 0 < γ < 1 and any positive integer x . Please note that when t = ( k − 1 ) τ for all [ k ] , we have (cid:107) x t(cid:96) − − x t [ k ] , (cid:96) (cid:107) = 0 = h ( 0 , δ (cid:96) ) , which also satisﬁes ( 15 ) . Also , F (cid:96) ( x ) is ρ - Lipschitz , so that we also have F (cid:96) ( x t(cid:96) − ) − F (cid:96) ( x t [ k ] , (cid:96) ) ≤ ρh ( t − ( k − 1 ) τ , δ (cid:96) ) . ( 16 ) Proof sketch . We ﬁrst obtain the worker momentum upper bound (cid:107) y ti , (cid:96) − y t [ k ] , (cid:96) (cid:107) for each worker { i , (cid:96) } . Based on it and worker momentum update rules in Lines 5 – 6 in Algorithm 1 , we bound the worker model parameter gap (cid:107) x ti , (cid:96) − x t [ k ] , (cid:96) (cid:107) . Then , we extend above two bounds to obtain edge aggregated worker momentum upper bound (cid:107) y t(cid:96) − − y t [ k ] , (cid:96) (cid:107) . Finally , the gap of edge model parameter (cid:107) x t(cid:96) − − x t [ k ] , (cid:96) (cid:107) is obtained . See Appendix A for the complete proof . In Theorem 2 , we then bound the edge momentum update between x kτ(cid:96) + and x kτ(cid:96) − within interval [ k ] . Theorem 2 . For any edge interval [ k ] in any edge node (cid:96) ∈ L , suppose 0 < γ < 1 , 0 < γ a < 1 , and any τ = 1 , 2 , . . . , we have (cid:107) x kτ(cid:96) + − x kτ(cid:96) − (cid:107) ≤ s ( τ ) , ( 17 ) where s ( τ ) is s ( τ ) = γ a τηρ ( γµ + γ + 1 ) ( 18 ) and constant µ is deﬁned in Appendix E . Proof sketch . Based on the edge momentum update rules in Lines 10 – 11 in Algorithm 1 , we can derive x kτ(cid:96) + − x kτ(cid:96) − = γ a (cid:16) x kτ(cid:96) − − x ( k − 1 ) τ (cid:96) − (cid:17) = γ a (cid:80) kτ − 1 t = ( k − 1 ) τ (cid:0) x t + 1 (cid:96) − − x t(cid:96) − (cid:1) . Then we prove the bound of (cid:107) x t + 1 (cid:96) − − x t(cid:96) − (cid:107) based on the deﬁnition of intermediate value where x t (cid:96) − = (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) x t i , (cid:96) , and then the result is obtained . See Appendix E for the complete proof . By combining the results of Theorem 1 and Theorem 2 , we can telescope the bound within edge interval [ k ] to the cloud interval { p } where k = ( p − 1 ) π + 1 , ( p − 1 ) π + 2 , . . . , pπ . Then , we are ready to bound the gap between weighted average of edge virtual update (cid:80) L(cid:96) = 1 D (cid:96) D x pτπ [ pπ ] , (cid:96) and cloud virtual update x pτπ { p } in Theorem 3 . Theorem 3 . For any cloud interval { p } , 0 < γ < 1 , and 0 < γ a < 1 , when edge interval [ k ] = [ pπ ] ( the last edge interval in cloud interval { p } ) , and ∀ τ , π ∈ { 1 , 2 , . . . } we have (cid:107) x pτπ [ pπ ] − x pτπ { p } (cid:107) ≤ h ( τπ , δ ) + π L (cid:88) (cid:96) = 1 D (cid:96) D ( h ( τ , δ (cid:96) ) + s ( τ ) ) , ( 19 ) where we deﬁne x pτπ [ pπ ] (cid:44) (cid:80) L(cid:96) = 1 D (cid:96) D x pτπ [ pπ ] , (cid:96) , for ∀ (cid:96) ∈ L . Proof sketch . We propose an intermediate sequence of edge virtual update on the cloud x pτπ { p } , (cid:96) . We then bound (cid:107) x pτπ [ pπ ] − x pτπ { p } , (cid:96) (cid:107) and (cid:107) x pτπ { p } , (cid:96) − x pτπ { p } (cid:107) respectively to obtain the ﬁnal result . See Appendix F for complete proof . Theorem 4 . Under the following conditions : ( 1 ) 0 < βη ( γ + 1 ) ≤ 1 , 0 < γ < 1 , 0 < γ a < 1 , and ∀ τ , π ∈ { 1 , 2 , . . . } ; ( 2 ) ∃ ε > 0 , ( 2 . 1 ) ωασ 2 − ρj ( τ , π , δ (cid:96) , δ ) τπε 2 > 0 ; ( 2 . 2 ) F ( x pτπ { p } ) − F ( x ∗ ) ≥ ε , ∀ p ; and ( 2 . 3 ) F ( x T ) − F ( x ∗ ) ≥ ε are satisﬁed ; Algorithm 1 gives F ( x T ) − F ( x ∗ ) ≤ 1 T (cid:16) ωασ 2 − ρj ( τ , π , δ (cid:96) , δ ) τπε 2 (cid:17) . ( 20 ) where j ( τ , π , δ (cid:96) , δ ) is j ( τ , π , δ (cid:96) , δ ) = h ( τπ , δ ) + ( π + 1 ) L (cid:88) (cid:96) = 1 D (cid:96) D ( ( h ( τ , δ (cid:96) ) + s ( τ ) ) ) . ( 21 ) We deﬁne F ( x ∗ ) as the minimum value , if there exists some ϕ > 0 such that F ( x ∗ ) ≤ F ( x ) for all x within distance ϕ of x ∗ . Constant µ is deﬁned in Appendix E and constants ω , σ , and α are deﬁned in Appendix H . Proof sketch . We ﬁrst analyze the convergence of F ( x t + 1 { p } ) − F ( x t { p } ) within cloud interval { p } when t ∈ [ ( p − 1 ) τπ , pτπ ) . Then , we merge h ( τ , δ (cid:96) ) , s ( τ ) , and result of Theorem 3 to handle the overall effects and telescope the gap of overall effects to all P cloud intervals , and then the ﬁnal result is obtained . See Appendix H for complete proof . Please note in the proof of Theorems 2 , 3 , and 4 , we have characterized the multi - time cross - two - tier momentum interaction and cross - three - tier momentum interaction brought by the three - tier FL . To analyze π times cross - two - tier mo - mentum interactions , we devise a new telescope form to bound these new deviations ( Equations ( 49 ) – ( 51 ) and ( 58 ) ) . To analyze cross - three - tier momentum interaction , we devise a new mechanism to analyze such momentum interactions across multi - tiers ( Equations ( 52 ) – ( 58 ) and ( 64 ) – ( 67 ) ) . We have demonstrated that the gap between the global loss function value F ( x T ) and the stationary point F ( x ∗ ) is upper bounded by a function of T ( T = Kτ = Pτπ ) which is inversely proportional to T . It converges with the convergence rate O (cid:0) 1 T (cid:1) for smooth non - convex problems under non - i . i . d . data distribution . We also give the following observations based on the above theorems . Observation 1 . The overall gap in Theorem 4 , F ( x T ) − F ( x ∗ ) decreases when T is larger . From Appendix G , we have h ( x ) ≥ 0 for any x = 1 , 2 , . . . , and it increases with x . According to ( 18 ) , s ( τ ) increases with τ . According to ( 21 ) , j ( τ , π ) increases with τ and π . Therefore , the value of ρj ( τ , π ) τπε 2 increases with τ and π so as to increase the overall bound F ( x T ) − F ( x ∗ ) . However , in order to let the Condition ( 2 . 1 ) in Theorem 4 hold , we cannot set a very large τ and π , implying that convergence is guaranteed when j ( τ , π ) is below a certain threshold . Experiments on the effects of τ and π further verify that larger τ and π decreases the convergence performance . 7 In Theorem 5 , we further eliminate the value ε in Theorem 4 and further demonstrate the bound between the ﬁnal loss function value that the algorithm can obtain F ( x f ) and the stationary point F ( x ∗ ) , where we deﬁne x f (cid:44) arg min x ∈ { x pτπ ( a ) : p = 1 , 2 , . . . , P } F ( x ) . ( 22 ) Theorem 5 . Under the following condition : 0 < βη ( γ + 1 ) ≤ 1 , 0 < γ < 1 , 0 < γ a < 1 , and ∀ τ , π ∈ { 1 , 2 , . . . } , we have F ( x f ) − F ( x ∗ ) ≤ 1 2 Tωασ 2 + ρj ( τ , π , δ (cid:96) , δ ) ( 23 ) + (cid:114) 1 4 T 2 ω 2 α 2 σ 4 + ρj ( τ , π , δ (cid:96) , δ ) ωασ 2 τπ (cid:44) f HierMo ( T ) . Proof . See Appendix I for complete proof . Theorem 5 will be used in Section IV - D and Section V to compare the convergence upper bound and formulate the optimization problem respectively . D . Comparison between HierMo and HierFAVG In this section , we theoretically quantify the performance gain brought by HierMo compared with HierFAVG ( without momentum ) . The convergence upper bound of HierFAVG can be derived from [ 18 ] as follows : F ( ˆ x f ) − F ( x ∗ ) ≤ 1 2 Tω ˆ ασ 2 + ρ ˆ j ( τ , π , δ (cid:96) , δ ) ( 24 ) + (cid:115) 1 4 T 2 ω 2 ˆ α 2 σ 4 + ρ ˆ j ( τ , π , δ (cid:96) , δ ) ω ˆ ασ 2 τπ (cid:44) f HierFAV G ( T ) . The deﬁnitions of ˆ α and ˆ j ( · ) can be found in [ 18 ] . To prevent the gradient descent from overshooting [ 43 ] , it is common to choose a very small η . The following theorem is made when η → 0 + . Theorem 6 . When 0 < βη ( γ + 1 ) ≤ 1 , 0 < γ < 1 , 0 < γ a < 1 , and ∀ τ , π ∈ { 1 , 2 , . . . } , HierMo outperforms HierFAVG , i . e . , f HierFAV G ( T ) − f HierMo ( T ) > 0 for any T and η → 0 + . Proof . See Appendix J for detailed proof . The above theorem indicates that HierMo leads to a tighter convergence upper bound compared with HierFAVG , showing that HierMo theoretically outperforms HierFAVG . V . A GGREGATION P ERIOD O PTIMIZATION BY H IER OPT We have proved that HierMo is convergent in section IV . We observe that the worker - edge and edge - cloud aggregation periods τ and π are two key design variables that will inﬂuence the convergence performance . The values of τ and π will also inﬂuence the usage of communication and computation resources in the real - world training process . Therefore , we aim to optimize these two variables and formulate an optimization problem : Under a given total training time denoted as Ψ , how the HierMo algorithm achieves the best performance ( min global model loss ) . We denote the worker computation delay for one iteration as Θ w , edge computation delay for one edge aggregation as Θ e , and cloud computation delay for one cloud aggregation as Θ c . We also denote the worker communication delay to the edge as Φ w 2 e and edge communication delay to the cloud as Φ e 2 c . All the above values are assumed to be given as they can be measured in the real world . We assume each worker { i , (cid:96) } communicates with connected edge node (cid:96) in parallel and each edge node (cid:96) communicates with cloud in parallel [ 8 ] , [ 18 ] , [ 44 ] . The above assumptions are commonly adopted in the literature [ 42 ] , [ 44 ] . As a result , the total training time for HierMo is calculated as follows Ψ (cid:44) P · ( τπ Θ w + π Θ e + Θ c + π Φ w 2 e + Φ e 2 c ) , ( 25 ) where P is the total number of cloud aggregations ( P = Tτπ ) . In order to ﬁnd the optimal pair of ( τ , π ) , we target to minimize ( 23 ) , where ( 23 ) demonstrates the bound between the global loss and the stationary point [ 18 ] , [ 42 ] . By incorporating the constraints , the optimization problem can be formulated as follows min τ , π 1 2 Tωασ 2 + ρj ( τ , π , δ (cid:96) , δ ) ( 26 ) + (cid:114) 1 4 T 2 ω 2 α 2 σ 4 + ρj ( τ , π , δ (cid:96) , δ ) ωασ 2 τπ , s . t . P · ( τπ Θ w + π Θ e + Θ c + π Φ w 2 e + Φ e 2 c ) = Ψ , ( 26a ) T = Pτπ , ( 26b ) τ ≥ 1 , ( 26c ) π ≥ 1 . ( 26d ) From constraints ( 26a ) and ( 26b ) , we obtain 1 T = Θ e + Φ w 2 e Ψ 1 τ + Θ c + Φ e 2 c Ψ 1 τπ + Θ w Ψ . ( 27 ) Substituting ( 27 ) into ( 26 ) , we can eliminate the equation constraints . We also deﬁne q ( τ , π ) (cid:44) 1 2 Tωασ 2 ( 28 ) = Θ e + Φ w 2 e 2Ψ ωασ 2 1 τ + Θ c + Φ e 2 c 2Ψ ωασ 2 1 τπ + Θ w 2Ψ ωασ 2 . The problem ( 26 ) can be re - formulated as min τ , π q ( τ , π ) + ρj ( τ , π , δ (cid:96) , δ ) + (cid:114) q 2 ( τ , π ) + ρj ( τ , π , δ (cid:96) , δ ) ωασ 2 τπ , ( 29 ) s . t . τ ≥ 1 , ( 29a ) π ≥ 1 . ( 29b ) It is non - trivial to ﬁnd a closed - form optimal pair of ( τ , π ) in the three - tier hierarchical FL because problem ( 29 ) includes both polynomial and exponential terms of τ and π , where the exponential term is nest - embedded in h ( · ) that is embedded in j ( · ) . Even if for a two - tier FL problem , the objective function of the bound is complicated , and it is still infeasible to ﬁnd an optimal solution in closed form [ 42 ] , [ 44 ] . In what follows , we propose the Hierarchical Optimizing Periods ( HierOPT ) algorithm to ﬁnd a local optimal solution to problem ( 29 ) . In Algorithm 2 , for convenience , we deﬁne the objective function ( 29 ) as R ( τ , π ) with respect to τ and π . We also 8 Algorithm 2 HierOPT algorithm . Input : Ψ , Θ w , Θ e , Θ c , Φ w 2 e , Φ e 2 c Output : τ ∗ and π ∗ 1 : Initialize τ 0 and π 0 as random positive integers , i = 0 as the index of search iteration . 2 : while true do 3 : Calculate R (cid:48) ( τ i ) 4 : if R (cid:48) ( τ i ) > 0 then 5 : τ i + 1 ← max { τ i − 1 , 1 } 6 : else if R (cid:48) ( τ i ) < 0 then 7 : τ i + 1 ← τ i + 1 8 : end if 9 : Calculate R (cid:48) ( π i ) 10 : if R (cid:48) ( π i ) > 0 then 11 : π i + 1 ← max { π i − 1 , 1 } 12 : else if R (cid:48) ( π i ) < 0 then 13 : π i + 1 ← π i + 1 14 : end if 15 : Record ( τ i , π i ) 16 : if the pair of values ( τ i , π i ) is visited before then 17 : Set τ ∗ ← τ i and π ∗ ← π i 18 : BREAK 19 : end if 20 : i ← i + 1 21 : end while deﬁne the partial derivative of τ and π as R (cid:48) ( τ ) and R (cid:48) ( π ) respectively . Since R ( τ , π ) is in closed - form , R (cid:48) ( τ ) and R (cid:48) ( π ) are also in closed - form and can be calculated numerically given any π and τ respectively . Algorithm 2 is operated as follows : 1 We take turns to calculate R (cid:48) ( τ ) ( Lines 3 – 8 ) and R (cid:48) ( π ) ( Lines 9 – 14 ) . When the gradient is greater than zero , implying that the objective function has the trend to increase , we decrease the value by 1 ( Lines 5 and 11 ) . When the gradient is less than zero , implying that the objective function has the trend to decrease , we increase the value by 1 ( Lines 7 and 13 ) . Due to constraints ( 29a ) and ( 29b ) , we restrict the values of τ and π to be equal or greater than 1 . 2 If the pair of value ( τ , π ) is visited before ( Lines 16 – 19 ) , it means Algorithm 2 converges and ( τ , π ) oscillates within a number of feasible value pairs ( because τ and π can only be integers ) . In this case , we ﬁnd a local optimal pair of ( τ ∗ , π ∗ ) and we can exit the algorithm . VI . E XPERIMENTAL R ESULTS In this section , we evaluate the convergence performance of HierMo compared with three typical categories of bench - mark algorithms : 1 three - tier FL without momentum ( Hier - FAVG [ 18 ] and CFL [ 19 ] ) , 2 two - tier FL with momentum ( DOMO [ 24 ] , FedADC [ 25 ] , FedMom [ 20 ] , SlowMo [ 21 ] , FedNAG [ 22 ] , and Mime [ 23 ] ) , and 3 two - tier FL without momentum ( FedAvg [ 4 ] ) . For the two - tier benchmarks , we assume that the edge nodes do not exist and the workers are directly connected to the cloud . We then discuss the effects of τ and π respectively and their joint effects . Afterwards , we explicitly quantify different levels of non - i . i . d . data and analyze their effects . Finally , we perform a trace - driven simu - lation of the three - tier hierarchical FL environment as if real - world hierarchical FL is implemented so that we can test the overall training time . Through this way , we verify that ( τ ∗ , π ∗ ) derived in Section V leads to near - optimal performance in the realistic scenario . A . Experiment on Convergence of HierMo 1 ) Experimental Setup : We employ four real - world datasets including MNIST [ 29 ] , CIFAR - 10 [ 30 ] , and ImageNet [ 28 ] , [ 31 ] for image classiﬁcation , and UCI - HAR [ 32 ] for human activity recognition . All training and testing samples are ran - domly shufﬂed and distributed to workers . Please note there is no restriction on how the data is distributed at different workers , therefore , the level of non - i . i . d . data distribution captured by δ i , (cid:96) is different for each worker { i , (cid:96) } . The training is run on a GPU tower server with 4 NVIDIA GeForce RTX 2080Ti GPUs . We use ﬁve models including linear regression , logistic regression , CNN , VGG16 , and ResNet18 . The CNN model’s structure is the classic one in [ 26 ] , which has two 5 × 5 convolutional layers with 32 and 64 channels respectively . In each convolutional layer , 2 × 2 max pooling is used . The last three layers are fully connected layers with ReLu activation and softmax . The structure of VGG16 and ResNet18 can be found in [ 27 ] , [ 28 ] respectively . We use mini - batch in all experiments , and the batch size is 64 . We set the learning rate η = 0 . 01 . Other hyper - parameters will be speciﬁed in each experiment . In this experiment , we focus on the convergence per - formance ( i . e . , accuracy given the number of iterations ) of different algorithms . We do not consider the real - world delay for now . The results do not depend on hardware but on the algorithm itself . Therefore , we can create several virtual machines within a single server to carry out the experiment . ( Even if real - world hardware is used in the experiment , it will still give the same results . ) The experiment on the optimization considering real - world delay will be discussed in Section VI - B . 2 ) Performance Comparison : In Table II , we compare the convergence performance of HierMo with benchmark algorithms . The numbers show the accuracy when different algorithms are run for T iterations . The experiment is con - ducted on linear regression , logistic regression , CNN , VGG16 , and ResNet18 . We set T = 1000 ( MNIST ) , T = 4000 ( UCI - HAR ) , or T = 10000 ( CIFAR10 and ImageNet ) , γ = 0 . 5 , γ a = 0 . 5 . There are 4 workers and 2 edge nodes with each edge node serving 2 workers ( three - tier algorithm ) . There are 4 workers directly served by the cloud ( two - tier algorithm ) . For two - tier algorithms , we set τ = 20 ( convex model ) or τ = 40 ( non - convex model ) . For three - tier algorithms , we set τ = 10 , π = 2 ( convex model ) or τ = 20 , π = 2 ( non - convex model ) . Please note that since π does not exist for two - tier algorithms , we set τ value for two - tier algorithms equal to τπ value for three - tier algorithms for a fair comparison . These hyper - parameters are typically used in existing works [ 8 ] , [ 13 ] , [ 14 ] , [ 18 ] , [ 22 ] . In all cases , HierMo outperforms all other benchmarks . This conﬁrms that applying momentum on both worker - level and edge - level with three - tier architecture achieves the best performance . 9 TABLE II P ERFORMANCE COMPARISON OF DIFFERENT FL ALGORITHMS ( ACCURACY % ) . Linear on MNIST Logistic on MNIST CNN on MNIST CNN on CIFAR10 VGG16 on CIFAR10 ResNet18 on ImageNet CNN on UCI - HAR HierMo 85 . 97 85 . 97 85 . 97 ± 0 . 03 89 . 23 89 . 23 89 . 23 ± 0 . 04 96 . 13 96 . 13 96 . 13 ± 0 . 07 64 . 18 64 . 18 64 . 18 ± 0 . 08 90 . 06 90 . 06 90 . 06 ± 0 . 15 69 . 64 69 . 64 69 . 64 ± 0 . 12 88 . 36 88 . 36 88 . 36 ± 0 . 06 HierFAVG [ 18 ] 83 . 62 ± 0 . 03 87 . 00 ± 0 . 05 93 . 40 ± 0 . 07 38 . 46 ± 0 . 13 89 . 46 ± 0 . 12 68 . 63 ± 0 . 10 54 . 56 ± 0 . 11 CFL [ 19 ] 83 . 36 ± 0 . 04 86 . 98 ± 0 . 06 93 . 58 ± 0 . 06 38 . 79 ± 0 . 11 89 . 80 ± 0 . 11 68 . 87 ± 0 . 09 69 . 19 ± 0 . 09 DOMO [ 24 ] 85 . 79 ± 0 . 05 89 . 02 ± 0 . 05 95 . 90 ± 0 . 05 59 . 39 ± 0 . 07 88 . 53 ± 0 . 09 67 . 05 ± 0 . 10 88 . 15 ± 0 . 06 FedADC [ 25 ] 85 . 51 ± 0 . 04 88 . 18 ± 0 . 05 95 . 09 ± 0 . 07 56 . 00 ± 0 . 11 89 . 38 ± 0 . 08 67 . 76 ± 0 . 12 85 . 14 ± 0 . 09 FedMom [ 20 ] 84 . 84 ± 0 . 06 88 . 05 ± 0 . 05 94 . 74 ± 0 . 05 54 . 87 ± 0 . 07 88 . 03 ± 0 . 10 66 . 91 ± 0 . 11 84 . 69 ± 0 . 07 SlowMo [ 21 ] 84 . 82 ± 0 . 06 88 . 00 ± 0 . 06 94 . 88 ± 0 . 05 54 . 43 ± 0 . 06 88 . 47 ± 0 . 09 66 . 84 ± 0 . 09 83 . 03 ± 0 . 10 FedNAG [ 22 ] 84 . 97 ± 0 . 04 88 . 14 ± 0 . 05 95 . 04 ± 0 . 06 55 . 54 ± 0 . 09 88 . 33 ± 0 . 06 66 . 81 ± 0 . 14 84 . 69 ± 0 . 06 Mime [ 23 ] 84 . 41 ± 0 . 06 87 . 73 ± 0 . 06 93 . 89 ± 0 . 08 48 . 24 ± 0 . 15 81 . 76 ± 0 . 11 64 . 33 ± 0 . 21 76 . 75 ± 0 . 11 FedAvg [ 4 ] 83 . 57 ± 0 . 04 86 . 89 ± 0 . 05 93 . 31 ± 0 . 08 37 . 79 ± 0 . 19 88 . 27 ± 0 . 15 66 . 59 ± 0 . 09 53 . 31 ± 0 . 12 Fig . 3 . ( a ) – ( c ) : Accuracy comparison for HierMo under different settings of worker - edge aggregation period τ and edge - cloud aggregation period π when CNN is trained on MNIST . ( e ) – ( g ) : Accuracy comparison under 3 - class ( e ) , 6 - class ( f ) , and 9 - class ( g ) non - i . i . d . data when CNN is trained on MNIST . ( d ) and ( h ) : Accuracy comparison for large N ( N = 50 and N = 100 ) when CNN is trained on MNIST . Comparing HierMo with HierFAVG and CFL , we observe that HierMo > CFL > HierFAVG . ( We use “ > ” to indicate “is better than” for presentation convenience . ) This veriﬁes that the momentum can accelerate the convergence in three - tier architecture . Comparing HierMo with DOMO and FedADC , we observe that HierMo > DOMO > FedADC . This veriﬁes that when two types of momentum are applied , the three - tier architecture outperforms the two - tier architecture . This is because the additional edge aggregation can decrease the effect of data heterogeneity among workers under the same edge node , so as to improve the performance . Comparing DOMO and FedADC with FedMom , SlowMo , FedNAG , and Mime , we observe that DOMO > FedADC > FedNAG > FedMom ≈ SlowMo > Mime . This conﬁrms that using combined worker momentum and aggregator momentum can accelerate the convergence compared with those using mo - mentum only on workers or only on the aggregator . For worker momentum only or aggregator momentum only algorithms , we can still observe their acceleration compared with FedAvg . We also observe Mime may not perform well . Sometimes , it is even worse than FedAvg . This is because Mime uses the ﬁxed momentum value in worker momentum update , where such value can be refreshed only in the global aggregation phase . As a result , the momentum value may be stale , especially when τ is as large as 40 . Comparing HierFAVG and CFL with two - tier momentum - based algorithms ( DOMO , FedADC , FedMom , SlowMo , Fed - NAG , and Mime ) , we observe that for DNN , HierFAVG and CFL outperform two - tier momentum - based algorithms , while for convex model and CNN , the later is better . This shows that for complicated models , the three - tier architecture plays a more signiﬁcant role to accelerate the convergence while for less complicated models , the momentum plays a more signiﬁcant role to accelerate the convergence . We also compare the training accuracy when more workers ( N = 50 and N = 100 ) participate the training to demon - strate the cross - siloed FL [ 41 ] ( typically up to one hundred participants ) . The results in Fig . 3 ( d ) and ( h ) show the same trend as results in Table II . 3 ) Effects of τ and π : In Fig . 3 , we evaluate the effects of τ and π , and their joint effects . The curves in the ﬁgure show the accuracy when CNN is trained on MNIST . We set T = 1000 , γ = 0 . 5 , γ a = 0 . 5 . There are 16 workers and 4 edge nodes with each edge node serving 4 workers . When π and τ are ﬁxed in Fig . 3 ( a ) and Fig . 3 ( b ) respec - tively , we observe that larger τ or π lowers the performance . 10 Fig . 4 . Comparison of total training time to reach 0 . 95 accuracy under two different settings when CNN is trained on MNIST . The time to reach 0 . 95 accuracy is labeled in the legends . ( a ) : γ = 0 . 5 , γ a = 0 . 5 , τ = 20 ( two - tier ) or τ = 10 , π = 2 ( three - tier ) . ( b ) : γ = 0 . 5 , γ a = 0 . 5 , τ = 40 ( two - tier ) or τ = 20 , π = 2 ( three - tier ) . This observation matches our expectation and veriﬁes the result of Theorem 4 showing that the larger τ or π leads to larger convergence upper bound . When τ · π ( the product of τ and π ) is ﬁxed in Fig . 3 ( c ) , we observe that smaller τ ( larger π ) leads to better performance . This shows that more frequent edge aggregation is more effective compared with more frequent cloud aggregation . 4 ) Effects of non - i . i . d . data distribution : In Fig . 3 ( e ) – ( g ) , we evaluate the effects of different levels of non - i . i . d . data distribution . We train CNN on MNIST with the setting τ = 40 ( two - tier ) or τ = 20 , π = 2 ( three - tier ) , N = 4 , L = 2 , and T = 1000 . The curves show the training accuracy . To quantify the level of non - i . i . d . data distribution , we explicitly assign only x < 10 out of 10 classes of data for each worker . ( Each worker has data samples from a subset of classes . ) The class of data is randomly allocated to each worker . Smaller x represents higher level of non - i . i . d . setting . We use 3 - class non - i . i . d . , 6 - class non - i . i . d . , and 9 - class non - i . i . d . to represent high , middle and low level of non - i . i . d . data respectively . We observe that HierMo > HierFAVG > DOMO > FedADC > FedNAG > CFL > FedMom > SlowMo > Mime ≈ FedAvg in most cases . This is consistent with the results in Table II , showing that HierMo outperforms all benchmarks under any levels of non - i . i . d . data distribution . We also ob - serve higher level of non - i . i . d . setting decreases convergence performance for all algorithms . Speciﬁcally , HierMo achieves 66 . 11 % accuracy for high level non - i . i . d . data , while achieving 92 . 21 % accuracy and 94 . 70 % accuracy for middle and low level non - i . i . d . data respectively . This matches our expectations where higher level of non - i . i . d . setting causes more data divergence that is denoted by larger δ , and therefore lowers the accuracy . B . Experiment on Trace - driven simulation of HierMo 1 ) Experimental Setup : We emulate the real - world three - tier hierarchical FL environment to test the performance of HierMo in the following two aspects . 1 To reach a target training accuracy ( 0 . 95 ) , we compare the total training time of HierMo and benchmarks . 2 For a given total training time Ψ , we compare the performance of HierMo under different ( τ , π ) and verify that ( τ ∗ , π ∗ ) derived by HierOPT is near optimal . We train the CNN on MNIST in the GPU tower server to keep the trace of the sequence of iterations . We use real - world devices as workers ( one laptop with Intel Core i3 M380 CPU , three Android phones : Nubia z17s with Qualcomm Snapdragon 835 CPU , Realme GT Neo with MTK Dimensity 1200 CPU , Redmi K30 Ultra with MTK Dimensity 1000 + CPU ) to sample worker computation delays . We use Macbook Pro 2018 with Intel Core i7 - 8750H CPU as the edge node to sample the edge computation delays . The GPU tower server is regarded as the cloud server and the cloud computation delays are sampled on it . The workers are connected to a HUAWEI Honor router X2 + with 5GHz WiFi . The edge node is also connected to the router with a wired cable ( 1 Gbps Ethernet ) . The router is then connected to the public Internet . The cloud server is connected to the Internet via another ISP’s access network . The worker communication delays are sampled between the workers and the edge node . The edge communication delays are sampled between the edge node and the server via the public Internet . Please note that for two - tier FL algorithms , since the workers directly communicate with the cloud , the worker - to - cloud communication delays are sampled as the delays from the devices to the server . We use the trace of the sequence of iterations and the sampled delays to ﬁgure out the overall delays as if the training process is conducted in real - world three - tier or two - tier FL environment . Please note that such approach to use a digital representation of physical objects to conduct the experiment is widely used in distributed systems , IoT , Industry 4 . 0 , and machine learning applications [ 45 ] , [ 46 ] . It can generate a convincing system performance evaluation without deploying physical devices . 2 ) Total Training Time Comparison : In Fig . 4 , we compare the total training time of HierMo and benchmarks when CNN is trained on MNIST . The experiment is conducted under two settings : 1 γ = 0 . 5 , γ a = 0 . 5 , τ = 20 ( two - tier ) or τ = 10 , π = 2 ( three - tier ) and 2 γ = 0 . 5 , γ a = 0 . 5 , τ = 40 ( two - tier ) or τ = 20 , π = 2 ( three - tier ) . There are 4 workers and 2 edge nodes with each edge node serving 2 workers ( three - tier algorithm ) . There are 4 workers directly served by the cloud ( two - tier algorithm ) . We observe that to reach the accuracy 0 . 95 , HierMo spends 558 . 94s under setting 1 and 459 . 48s under setting 2 while other benchmarks spend 706 . 18s – 1544 . 76s under setting 1 and 599 . 73s – 1532 . 65s under setting 2 respectively . This demonstrates that HierMo is efﬁcient and decreases the total training time by 21 – 70 % compared with the benchmarks . 3 ) Performance of HierOPT : In Fig . 5 , we illustrate the performance of HierOPT . In this experiment , CNN is trained on MNIST and CIFAR10 . We set γ = 0 . 5 , γ a = 0 . 5 , Ψ = 400 s or Ψ = 200 s ( MNIST ) , and Ψ = 6000 s or Ψ = 3000 s ( CIFAR10 ) . There are 16 workers and 4 edge nodes with each edge node serving 4 workers . All constants in the objective function ( 29 ) can be sampled in advance of the training process [ 42 ] , [ 44 ] . We show the accuracy under different pairs of ( τ , π ) and ﬂag ( τ ∗ , π ∗ ) derived by HierOPT . The darker color in the chromatography indicates a higher training accuracy . The red cross indicates the derived ( τ ∗ , π ∗ ) by HierOPT . We observe that in all ﬁgures , HierOPT can ﬁnd near - optimal solutions . 11 Fig . 5 . Accuracy comparison for HierMo with derived pair of ( τ ∗ , π ∗ ) by HierOPT ( red cross ) and different pairs of ( τ , π ) under limited total training time Ψ . The darker color indicates the higher training accuracy ( in % ) . ( a ) : Ψ = 400 s on MNIST . ( b ) : Ψ = 200 s on MNIST . ( c ) : Ψ = 6000 s on CIFAR10 . ( d ) : Ψ = 3000 s on CIFAR10 . In Fig . 5 ( a ) , when Ψ = 400 s , the optimal accuracy is 95 . 05 % , with optimal ( τ , π ) = ( 42 , 2 ) , while HierOPT ﬁnds ( τ ∗ , π ∗ ) = ( 40 , 2 ) , with accuracy 94 . 82 % , only a 0 . 23 % gap from the optimum . In Fig . 5 ( b ) , when Ψ = 200 s , the optimal accuracy is 92 . 52 % , with optimal ( τ , π ) = ( 46 , 2 ) , while HierOPT ﬁnds ( τ ∗ , π ∗ ) = ( 43 , 2 ) , with accuracy 92 . 23 % , only a 0 . 29 % gap from the optimum . For CIFAR10 , HierOPT can still ﬁnd the near - optimal ( τ ∗ , π ∗ ) , with only 0 . 04 % ( 67 . 09 % to 67 . 05 % ) and 0 . 16 % ( 56 . 82 % to 56 . 66 % ) gap from the real - world optimum , when Ψ = 6000 s and Ψ = 3000 s respectively . VII . C ONCLUSION In this paper , we propose HierMo , a three - tier hierarchical FL algorithm that applies momentum to accelerate conver - gence . We provide convergence analysis for HierMo , showing that it converges with a rate of O (cid:0) 1 T (cid:1) for smooth non - convex problems under non - i . i . d . data . In the analysis , we develop a new two - level virtual update ( edge and cloud ) method to characterize the multi - time cross - two - tier momentum inter - action and the cross - three - tier momentum interaction . The performance gain of momentum is also quantiﬁed . We also propose HierOPT to derive a near - optimal setting of worker - edge and edge - cloud aggregation periods ( τ , π ) under a limited total training time . We verify that HierMo outperforms existing mainstream benchmarks under a wide range of settings . In addition , HierOPT can achieve a near - optimal performance when we test HierMo under different values of ( τ , π ) . A PPENDIX A . Proof of Theorem 1 1 ) Equivalent Update : First , we deﬁne v ti , (cid:96) (cid:44) y ti , (cid:96) − y t − 1 i , (cid:96) with v 0 i , (cid:96) = 0 for all i , (cid:96) . We can obtain x t − 1 i , (cid:96) = y t − 1 i , (cid:96) + γ v t − 1 i , (cid:96) . The worker momentum / model update in Lines 5 – 6 in Algorithm 1 can then be equivalently written as v ti , (cid:96) ← γ v t − 1 i , (cid:96) − η ∇ F i , (cid:96) ( x t − 1 i , (cid:96) ) , ( 30 ) x ti , (cid:96) ← x t − 1 i , (cid:96) + γ v ti , (cid:96) − η ∇ F i , (cid:96) ( x t − 1 i , (cid:96) ) . ( 31 ) The aggregated value v t(cid:96) and the intermediate value x t(cid:96) − can also be equivalently written as v t (cid:96) ← C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) v t i , (cid:96) , x t (cid:96) − ← C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) x t i , (cid:96) . ( 32 ) Similarly , the edge and cloud virtual updates ( 8 ) – ( 9 ) and ( 12 ) – ( 13 ) can be equivalently written as v t [ k ] , (cid:96) ← γ v t − 1 [ k ] , (cid:96) − η ∇ F (cid:96) ( x t − 1 [ k ] , (cid:96) ) , x t [ k ] , (cid:96) ← x t − 1 [ k ] , (cid:96) + γ v t [ k ] , (cid:96) − η ∇ F (cid:96) ( x t − 1 [ k ] , (cid:96) ) , ( 33 ) v t { p } ← γ v t − 1 { p } − η ∇ F ( x t − 1 { p } ) , x t { p } ← x t − 1 { p } + γ v t { p } − η ∇ F ( x t − 1 { p } ) . ( 34 ) We employ the above equivalent update format ( 30 ) – ( 34 ) to complete the proof in the rest of the Appendix . 2 ) Constant Deﬁnition : We deﬁne the constants as follows , which are more conveniently used in the rest of the Appendix . A (cid:44) ( 1 + ηβ ) ( 1 + γ ) + (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , B (cid:44) ( 1 + ηβ ) ( 1 + γ ) − (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , I (cid:44) γA + A − 1 ( A − B ) ( γA − 1 ) , J (cid:44) γB + B − 1 ( A − B ) ( 1 − γB ) , U (cid:44) 1 + ηβ + ηβγ γ − B A − B = A − 1 A − B , V (cid:44) A − 1 + ηβ + ηβγ γ A − B = 1 − B A − B . 3 ) Subscript (cid:96) : Since Theorem 1 focuses on a speciﬁc edge node (cid:96) , for presentation convenience , in the proofs of Theo - rem 1 ( including Lemmas 1 – 3 ) , we ignore all subscript (cid:96) . We use x i , v i , x , v , x [ k ] , v [ k ] , F i , F , D i , D , δ i , δ , and C to repre - sent x i , (cid:96) , v i , (cid:96) , x (cid:96) − , v (cid:96) , x [ k ] , (cid:96) , v [ k ] , (cid:96) , F i , (cid:96) , F (cid:96) , D i , (cid:96) , D (cid:96) , δ i , (cid:96) , δ (cid:96) , and C (cid:96) respectively . Please note that in the proofs of the theorems other than Theorem 1 , we do not ignore subscript (cid:96) . 4 ) Prerequisite Lemmas for the Proof of Theorem 1 : To prove Theorem 1 , the progress mainly includes four steps . ( 1 ) We ﬁrst introduce an important equality in Lemma 1 , which will be used to prove Lemma 2 . ( 2 ) We bound (cid:107) x ti − x t [ k ] (cid:107) in Lemma 2 based on Lemma 1 . ( 3 ) Based on the result of Lemma 2 , we then bound (cid:107) v t − v t [ k ] (cid:107) in Lemma 3 . Please note that the proofs of Lemmas 1 – 3 are in Appendix B – D respectively . ( 4 ) Finally , based on the result of Lemma 3 , we bound (cid:107) x t − x t [ k ] (cid:107) , which concludes Theorem 1 . 12 Lemma 1 . Given a t = δ i β (cid:32) 1 + ηβ + ηβγ γ − B A − B A t − 1 + ηβ + ηβγ γ − A A − B B t (cid:33) , ( 35 ) A + B = 1 + ηβ + ηβγ + γ γ = ( 1 + ηβ ) ( 1 + γ ) γ , AB = 1 + ηβ γ , ( 36 ) where t = 0 , 1 , 2 , . . . , 0 < γ < 1 , ηβ > 0 , we have ( 1 + ηβ ) a t − 1 + ηβγ (cid:80) t − 1 i = 0 a i = γa t . Lemma 2 . For any interval [ k ] , ∀ t ∈ [ ( k − 1 ) τ , kτ ] , we have (cid:107) x ti − x t [ k ] (cid:107) ≤ f i ( t − ( k − 1 ) τ ) , where we deﬁne the function f i ( x ) as f i ( x ) (cid:44) δ i β ( γ x ( UA x + V B x ) − 1 ) and the function u ( x ) as u ( x ) (cid:44) γ x ( UA x + V B x ) − 1 . Lemma 3 . For any interval [ k ] , ∀ t ∈ [ ( k − 1 ) τ , kτ ] , we have : (cid:107) v t − v t [ k ] (cid:107) ≤ ηδ (cid:18) U ( γA ) t 0 γ ( A − 1 ) + V ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) , where t 0 = t − ( k − 1 ) τ . 5 ) Derivation of Theorem 1 : From ( 31 ) and ( 32 ) , we have x t = x t − 1 + γ v t − η (cid:80) Ci = 1 D i ∇ F i ( x t − 1 i ) D . ( 37 ) From ( 33 ) and ( 37 ) , and according to β - smoothness , Lemma 2 , the deﬁnition of f i ( x ) and u ( x ) , and Assumption 3 , we have (cid:107) x t − x t [ k ] (cid:107) = (cid:107) x t − 1 + γ v t − η (cid:80) Ci = 1 D i ∇ F i ( x t − 1 i ) D − x t − 1 [ k ] − γ v t [ k ] + η ∇ F ( x t − 1 [ k ] ) (cid:107) ≤(cid:107) x t − 1 − x t − 1 [ k ] (cid:107) + γ (cid:107) v t − v t [ k ] (cid:107) + ηδu ( t − 1 − ( k − 1 ) τ ) . Then , according to Lemma 3 , we have (cid:107) x t − x t [ k ] (cid:107) − (cid:107) x t − 1 − x t − 1 [ k ] (cid:107) ≤ γηδ (cid:18) U ( γA ) t 0 γ ( A − 1 ) + V ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) + ηδ ( γ t 0 − 1 ( UA t 0 − 1 + V B t 0 − 1 ) − 1 ) ( 38 ) = ηδ (cid:18) U ( γA ) t 0 − 1 A − 1 ( γA + A − 1 ) + V ( γB ) t 0 − 1 B − 1 ( γB + B − 1 ) − γ t 0 + 1 − 1 γ − 1 (cid:19) . ( 39 ) When t = ( k − 1 ) τ , we have (cid:107) x t − x t [ k ] (cid:107) = 0 . When t ∈ ( ( k − 1 ) τ , kτ ] , we sum up ( 39 ) for t , t − 1 , . . . , ( k − 1 ) τ + 1 , leading to (cid:107) x t − x t [ k ] (cid:107) ≤ t 0 (cid:88) x = 1 ηδ (cid:18) U ( γA ) x − 1 A − 1 ( γA + A − 1 ) + V ( γB ) x − 1 B − 1 ( γB + B − 1 ) − γ x + 1 − 1 γ − 1 (cid:19) = ηδ (cid:2) I (cid:0) ( γA ) t 0 − 1 (cid:1) + J (cid:0) ( γB ) t 0 − 1 (cid:1) − γ 2 ( γ t 0 − 1 ) − ( γ − 1 ) t 0 ( γ − 1 ) 2 (cid:21) = ηδ (cid:20) I ( γA ) t 0 + J ( γB ) t 0 − 1 ηβ − γ 2 ( γ t 0 − 1 ) − ( γ − 1 ) t 0 ( γ − 1 ) 2 (cid:21) = h ( t 0 ) , where I = γA + A − 1 ( A − B ) ( γA − 1 ) and J = γB + B − 1 ( A − B ) ( 1 − γB ) ( as deﬁned before ) . I + J = 1 ηβ . t 0 = t − ( k − 1 ) τ . We complete the proof of Theorem 1 . B . Proof of Lemma 1 Based on the deﬁnitions of U , V , and a t , we have a t = δ i β ( UA t + V B t ) . According to the inverse theorem of Vieta’s formulas , we have γx 2 − ( 1 + ηβ + ηβγ + γ ) x + ηβ + 1 = 0 , ( 40 ) where x values are the roots of the quadratic equation . The discriminant of the quadratic equation is positive . ∆ = ( 1 + ηβ + ηβγ + γ ) 2 − 4 ( 1 + ηβ ) γ > ( 1 + ηβ + γ ) 2 − 4 ( 1 + ηβ ) γ = ( ( 1 + ηβ ) − γ ) 2 > 0 . Thus , the roots of ( 40 ) can be expressed as A and B . Therefore , we can obtain ( 1 + ηβ ) a t − 1 + ηβγ t − 1 (cid:88) i = 0 a i − γa t = ( 1 + ηβ ) δ i β (cid:0) UA t − 1 + V B t − 1 (cid:1) + ηβγ δ i β U A t − 1 A − 1 + ηβγ δ i β V B t − 1 B − 1 − γ δ i β UA t − γ δ i β V B t = δ i β (cid:20) A t − 1 U 1 − A (cid:0) γA 2 − ( 1 + ηβ + ηβγ + γ ) A + 1 + ηβ (cid:1) + B t − 1 V 1 − B (cid:0) γB 2 − ( 1 + ηβ + ηβγ + γ ) B + 1 + ηβ (cid:1)(cid:21) − δ i β ηβγ (cid:18) U A − 1 + V B − 1 (cid:19) = 0 − ηδ i γ (cid:18) U A − 1 + V B − 1 (cid:19) = 0 . We complete the proof of Lemma 1 . C . Proof of Lemma 2 To prove Lemma 2 , ( 1 ) we ﬁrst bound the gap of (cid:107) v ti − v t [ k ] (cid:107) ; ( 2 ) then we bound the gap of (cid:107) x ti − x t [ k ] (cid:107) , which concludes Lemma 2 . When t = ( k − 1 ) τ , we know x ti = x t = x t [ k ] by the deﬁnition of x t [ k ] and the aggregation rules . Hence , we have (cid:107) x ti − x t [ k ] (cid:107) = 0 . Meanwhile , when t = ( k − 1 ) τ , we have x = 0 and f i ( 0 ) = 0 ( Lemma 2 holds ) . When t ∈ ( ( k − 1 ) τ , kτ ] , we bound the momentum gap (cid:107) v ti − v t [ k ] (cid:107) = (cid:107) γ v t − 1 i − η ∇ F i ( x t − 1 i ) − ( γ v t − 1 [ k ] − η ∇ F ( x t − 1 [ k ] ) ) (cid:107) = (cid:107) γ ( v t − 1 i − v t − 1 [ k ] ) − η [ ∇ F i ( x t − 1 i ) − ∇ F i ( x t − 1 [ k ] ) + ∇ F i ( x t − 1 [ k ] ) − ∇ F ( x t − 1 [ k ] ) ] (cid:107) ( a ) ≤ γ (cid:107) v t − 1 i − v t − 1 [ k ] (cid:107) + η (cid:107)∇ F i ( x t − 1 i ) − ∇ F i ( x t − 1 [ k ] ) (cid:107) + η (cid:107)∇ F i ( x t − 1 [ k ] ) − ∇ F ( x t − 1 [ k ] ) (cid:107) ( b ) ≤ γ (cid:107) v t − 1 i − v t − 1 [ k ] (cid:107) + ηβ (cid:107) x t − 1 i − x t − 1 [ k ] (cid:107) + ηδ i , ( 41 ) 13 where ( a ) is from triangle inequality and ( b ) is from β - smoothness and Assumption 3 . We use γ 0 , γ 1 , . . . , γ t − ( k − 1 ) τ − 1 as multipliers to multiply ( 41 ) when t , t − 1 , . . . , ( k − 1 ) τ + 1 , respectively . (cid:107) v ti − v t [ k ] (cid:107) ≤ γ (cid:107) v t − 1 i − v t − 1 [ k ] (cid:107) + ηβ (cid:107) x t − 1 i − x t − 1 [ k ] (cid:107) + ηδ i , γ (cid:107) v t − 1 i − v t − 1 [ k ] (cid:107) ≤ γ ( γ (cid:107) v t − 2 i − v t − 2 [ k ] (cid:107) + ηβ (cid:107) x t − 2 i − x t − 2 [ k ] (cid:107) + ηδ i ) , . . . γ t − ( k − 1 ) τ − 1 (cid:107) v ( k − 1 ) τ + 1 i − v ( k − 1 ) τ + 1 [ k ] (cid:107) ≤ γ t − ( k − 1 ) τ − 1 ( γ (cid:107) v ( k − 1 ) τ i − v ( k − 1 ) τ [ k ] (cid:107) + ηβ (cid:107) x ( k − 1 ) τ i − x ( k − 1 ) τ [ k ] (cid:107) + ηδ i ) . For convenience , we deﬁne G i ( t ) (cid:44) (cid:107) x ti − x t [ k ] (cid:107) . Summing up all of the above inequalities with respect to b ∈ [ 1 , t − ( k − 1 ) τ ] , we have (cid:107) v ti − v t [ k ] (cid:107) ≤ ηβ t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 G i ( t − b ) + ηδ i t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 + γ t − ( k − 1 ) τ (cid:107) v ( k − 1 ) τ i − v ( k − 1 ) τ [ k ] (cid:107) . When t = ( k − 1 ) τ , we know that v ti = v t = v t [ k ] by the deﬁnition of v t [ k ] and aggregation rules . Then we have (cid:107) v ( k − 1 ) τ i − v ( k − 1 ) τ [ k ] (cid:107) = 0 , so that the last term of above inequality is zero and (cid:107) v ti − v t [ k ] (cid:107) ≤ ηβ t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 G i ( t − b ) + ηδ i t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 . ( 42 ) Now , we can bound the gap between x ti and x t [ k ] . When t ∈ ( ( k − 1 ) τ , kτ ] , we have (cid:107) x ti − x t [ k ] (cid:107) ( a ) = (cid:107) x t − 1 i + γ v ti − η ∇ F i ( x t − 1 i ) − ( x t − 1 [ k ] + γ v t [ k ] − η ∇ F ( x t − 1 [ k ] ) ) (cid:107) = (cid:107) x t − 1 i − x t − 1 [ k ] + γ ( v ti − v t [ k ] ) − η [ ∇ F i ( x t − 1 i ) − ∇ F i ( x t − 1 [ k ] ) + ∇ F i ( x t − 1 [ k ] ) − ∇ F ( x t − 1 [ k ] ) ] (cid:107) ( b ) ≤(cid:107) x t − 1 i − x t − 1 [ k ] (cid:107) + γ (cid:107) v ti − v t [ k ] (cid:107) + ηβ (cid:107) x t − 1 i − x t − 1 [ k ] (cid:107) + ηδ i = ( ηβ + 1 ) (cid:107) x t − 1 i − x t − 1 [ k ] (cid:107) + γ (cid:107) v ti − v t [ k ] (cid:107) + ηδ i , ( 43 ) where ( a ) is from ( 31 ) and ( 33 ) , and ( b ) is from triangle inequality , β - smoothness , and Deﬁnition 3 . Substituting ( 42 ) into ( 43 ) and using G i ( t ) to denote (cid:107) x ti − x t [ k ] (cid:107) for t , t − 1 , . . . , ( k − 1 ) τ + 1 , we have G i ( t ) ≤ ( ηβ + 1 ) G t − 1 i + ηβγ t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 G i ( t − b ) + ηδ i γ t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 + ηδ i = ( ηβ + 1 ) G t − 1 i + ηβγ t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1 G i ( t − b ) + ηδ i t − ( k − 1 ) τ (cid:88) b = 0 γ b . ( 44 ) For convenience , we deﬁne g i ( x ) (cid:44) δ i β ( UA x + V B x ) . We have f i ( x ) = γ x g i ( x ) − δ i β . We use induction to prove G i ( t ) ≤ f i ( t − ( k − 1 ) τ ) , ∀ t ∈ [ ( k − 1 ) τ , kτ ] . First of all , we know that it is true when t = ( k − 1 ) τ because G i ( ( k − 1 ) τ ) = f i ( 0 ) . Then , we assume that G i ( c ) ≤ f i ( c − ( k − 1 ) τ ) holds for all c ∈ [ ( k − 1 ) τ , t ) , and we show it also holds for t . G i ( t ) ( a ) ≤ ( ηβ + 1 ) f i ( t − 1 − ( k − 1 ) τ ) + ηβ t − ( k − 1 ) τ (cid:88) b = 1 γ b f i ( t − b − ( k − 1 ) τ ) + ηδ i t − ( k − 1 ) τ (cid:88) b = 0 γ b ( b ) = ( ηβ + 1 ) (cid:18) γ t − 1 − ( k − 1 ) τ g i ( t − 1 − ( k − 1 ) τ ) − δ i β (cid:19) + ηβ t − ( k − 1 ) τ (cid:88) b = 1 (cid:18) γ t − ( k − 1 ) τ g i ( t − b − ( k − 1 ) τ ) − γ b δ i β (cid:19) + ηδ i t − ( k − 1 ) τ (cid:88) b = 0 γ b = γ t − 1 − ( k − 1 ) τ ( ( ηβ + 1 ) g i ( t − 1 − ( k − 1 ) τ ) + ηβγ t − ( k − 1 ) τ (cid:88) b = 1 g i ( t − b − ( k − 1 ) τ )   − δ i β ( c ) = γ t − ( k − 1 ) τ g i ( t − ( k − 1 ) τ ) − δ i β = f i ( t − ( k − 1 ) τ ) , where ( a ) is from ( 44 ) , ( b ) is from deﬁnition of f i ( x ) , and ( c ) is from Lemma 1 and G i ( t ) = a t . We complete the proof of Lemma 2 . D . Proof of Lemma 3 Based on the deﬁnition of u ( x ) in Lemma 2 , we get f i ( x ) = δ i β u ( x ) . From ( 31 ) and ( 32 ) , we have v t = γ v t − 1 − η (cid:80) Ci = 1 D i ∇ F i ( x t − 1 i ) D . ( 45 ) For t ∈ ( ( k − 1 ) τ , kτ ] , we have (cid:107) v t − v t [ k ] (cid:107) ( a ) = (cid:107) γ v t − 1 − η (cid:80) Ci = 1 D i ∇ F i ( x t − 1 i ) D − γ v t − 1 [ k ] + η ∇ F ( x t − 1 [ k ] ) (cid:107) ≤ γ (cid:107) v t − 1 − v t − 1 [ k ] (cid:107) + η (cid:80) Ci = 1 D i (cid:107)∇ F i ( x t − 1 i ) − ∇ F i ( x t − 1 [ k ] ) (cid:107) D ( b ) ≤ γ (cid:107) v t − 1 − v t − 1 [ k ] (cid:107) + ηβ (cid:80) Ci = 1 D i f i ( t − 1 − ( k − 1 ) τ ) D ( c ) = γ (cid:107) v t − 1 − v t − 1 [ k ] (cid:107) + ηδu ( t − 1 − ( k − 1 ) τ ) , ( 46 ) where ( a ) is from ( 45 ) and ( 33 ) ; ( b ) is from β - smoothness and Lemma 2 ; and ( c ) is from deﬁnition of f i ( x ) and As - sumption 3 . 14 We use γ 0 , γ 1 , . . . , γ t − ( k − 1 ) τ − 1 as multipliers to multiply ( 46 ) when t , t − 1 , . . . , ( k − 1 ) τ + 1 , respectively . (cid:107) v t − v t [ k ] (cid:107) ≤ γ (cid:107) v t − 1 − v t − 1 [ k ] (cid:107) + ηδu ( t − 1 − ( k − 1 ) τ ) , γ (cid:107) v t − 1 − v t − 1 [ k ] ≤ γ 2 ( (cid:107) v t − 2 − v t − 2 [ k ] (cid:107) + γηδu ( t − 2 − ( k − 1 ) τ ) , . . . γ t − ( k − 1 ) τ − 1 (cid:107) v ( k − 1 ) τ + 1 − v ( k − 1 ) τ + 1 [ k ] (cid:107) ≤ γ t − ( k − 1 ) τ (cid:107) v ( k − 1 ) τ − v ( k − 1 ) τ [ k ] (cid:107) + γ t − 1 − ( k − 1 ) τ ηδu ( 0 ) . Summing up all of the above inequalities , and according to (cid:107) v ( k − 1 ) τ − v ( k − 1 ) τ [ k ] (cid:107) = 0 , we have (cid:107) v t − v t [ k ] (cid:107) ≤ ηδ t − ( k − 1 ) τ (cid:88) b = 1 γ t − b − ( k − 1 ) τ u ( b − 1 ) ( 47 ) = ηδ   γ t − 1 − ( k − 1 ) τ U t − ( k − 1 ) τ (cid:88) b = 1 A b − 1 + γ t − 1 − ( k − 1 ) τ V t − ( k − 1 ) τ (cid:88) b = 1 B b − 1 − t − ( k − 1 ) τ (cid:88) b = 1 γ b − 1   = ηδ (cid:18) γ t 0 − 1 U A t 0 − 1 A − 1 + γ t 0 − 1 V B t 0 − 1 B − 1 − γ t 0 − 1 γ − 1 (cid:19) = ηδ (cid:18) U ( γA ) t 0 γ ( A − 1 ) + V ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) − ηδγ t 0 − 1 (cid:18) U A − 1 + V B − 1 (cid:19) = ηδ (cid:18) U ( γA ) t 0 γ ( A − 1 ) + V ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) ( 48 ) where t 0 = t − ( k − 1 ) τ . We complete the proof of Lemma 3 . E . Proof of Theorem 2 Based on the edge momentum update rules in Lines 10 – 11 in Algorithm 1 , and ( 31 ) we have x kτ(cid:96) + − x kτ(cid:96) − = γ a (cid:16) x kτ(cid:96) − − x ( k − 1 ) τ (cid:96) − (cid:17) = γ a kτ − 1 (cid:88) t = ( k − 1 ) τ (cid:0) x t + 1 (cid:96) − − x t(cid:96) − (cid:1) = γ a kτ − 1 (cid:88) t = ( k − 1 ) τ C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) (cid:0) x t + 1 i , (cid:96) − x ti , (cid:96) (cid:1) = γ a kτ − 1 (cid:88) t = ( k − 1 ) τ C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) (cid:0) γ 2 v ti , (cid:96) − η ( γ + 1 ) ∇ F i , (cid:96) ( x ti , (cid:96) ) (cid:1) , ( 49 ) and we deﬁne µ (cid:44) max p ∈ [ 1 , P ] , ∀ t , (cid:96) , i (cid:40) (cid:107) γ ( v t { p } ) (cid:107) (cid:107) η ∇ F ( x t { p } ) (cid:107) , (cid:107) γ ( v ti , (cid:96) ) (cid:107) (cid:107) η ∇ F i , (cid:96) ( x ti , (cid:96) ) (cid:107) (cid:41) . ( 50 ) Because F i , (cid:96) ( · ) is ρ - Lipschitz , and according to [ 47 , Lecture 2 , Lemma 1 ] , we have (cid:107)∇ F i , (cid:96) ( · ) (cid:107) 2 ≤ ρ 2 . Therefore , based on the deﬁnition of µ and ( 49 ) , we can derive (cid:13)(cid:13) x kτ(cid:96) + − x kτ(cid:96) − (cid:13)(cid:13) ≤ γ a kτ − 1 (cid:88) t = ( k − 1 ) τ C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) (cid:13)(cid:13) γ 2 v ti , (cid:96) − η ( γ + 1 ) ∇ F i , (cid:96) ( x ti , (cid:96) ) (cid:13)(cid:13) ≤ γ a kτ − 1 (cid:88) t = ( k − 1 ) τ C (cid:96) (cid:88) i = 1 D i , (cid:96) D (cid:96) ( γµη + η ( γ + 1 ) ) ρ = γ a τρη ( γµ + γ + 1 ) . ( 51 ) We complete the proof of Theorem 2 . F . Proof of Theorem 3 First , we deﬁne edge virtual update which is meaningful in cloud interval { p } as y t { p } , (cid:96) and x t { p } , (cid:96) . The value synchro - nization and edge virtual update on { p } are conducted as y ( p − 1 ) τπ { p } , (cid:96) ← y ( p − 1 ) τπ , ( 52 ) x ( p − 1 ) τπ { p } , (cid:96) ← x ( p − 1 ) τπ , ( 53 ) when t = ( p − 1 ) τπ , and y t { p } , (cid:96) ← x t − 1 { p } , (cid:96) − η ∇ F (cid:96) ( x t − 1 { p } , (cid:96) ) , ( 54 ) x t { p } , (cid:96) ← y t { p } , (cid:96) + γ ( y t { p } , (cid:96) − y t − 1 { p } , (cid:96) ) , ( 55 ) when p ∈ ( ( p − 1 ) τπ , pτπ ] . According to Theorem 1 , we have proved the gap between intermediate worker update on the edge (cid:80) C (cid:96) i = 1 D i , (cid:96) D (cid:96) x ti , (cid:96) and edge virtual update x t [ k ] , (cid:96) . Equivalently , the gap between the intermediate edge virtual update on the cloud (cid:80) L(cid:96) = 1 D (cid:96) D x t { p } , (cid:96) and the cloud virtual update x t { p } can be derived as the same way as Theorem 1 . The only difference is the gradient divergence . The edge - level gradient divergence is δ (cid:96) and the cloud - level gradient divergence is δ . Therefore , for any cloud interval { p } , ∀ t ∈ [ ( p − 1 ) τπ , pτπ ] , ∀ (cid:96) ∈ L , we have (cid:13)(cid:13)(cid:13) (cid:13) (cid:13) L (cid:88) (cid:96) = 1 D (cid:96) D x t { p } , (cid:96) − x t { p } (cid:13)(cid:13)(cid:13) (cid:13) (cid:13) ≤ h ( t − ( p − 1 ) τπ , δ ) . ( 56 ) At the end of cloud interval { p } , when t = pτπ , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) L (cid:88) (cid:96) = 1 D (cid:96) D x pτπ { p } , (cid:96) − x pτπ { p } (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ h ( τπ , δ ) . ( 57 ) Based on the deﬁnition of x pτπ [ pπ ] in Theorem 3 and the deﬁnition of x pτπ { p } , (cid:96) , we obtain (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) x pτπ [ pπ ] − L (cid:88) (cid:96) = 1 D (cid:96) D x pτπ { p } , (cid:96) (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) ≤ L (cid:88) (cid:96) = 1 D (cid:96) D (cid:13) (cid:13) (cid:13) x pτπ [ pπ ] , (cid:96) − x pτπ { p } , (cid:96) (cid:13) (cid:13) (cid:13) ≤ π L (cid:88) (cid:96) = 1 D (cid:96) D ( h ( τ , δ (cid:96) ) + s ( τ ) ) . ( 58 ) Combining ( 57 ) and ( 58 ) , we complete the proof of Theo - rem 3 . 15 G . Proof of Monotone of h ( x ) To prove the monotone increasing of h ( x ) , it is equivalent to prove h ( x ) − h ( x − 1 ) ≥ 0 for all integer x ≥ 1 . When x = 0 or x = 1 , because IA + JB = 1 + ηβ + ηβγ ηβγ , we have h ( 0 ) = ηδ ( I + J − 1 ηβ ) = 0 and h ( 1 ) = ηδ (cid:16) γ ( IA + JB ) − 1 ηβ − γ − 1 (cid:17) = 0 . Then , when x = 1 , we have h ( x ) − h ( x − 1 ) = 0 . When x > 1 , according to the deﬁnitions of A , B , U , and V , we can obtain that γA > 1 , 0 < γB < 1 , 1 γ + 1 < B < 1 , I > 0 , J > 0 , U > 0 , V > 0 , and U + V = 1 . Then , we have U ( γA ) i + V ( γB ) i ≥ ( 1 + ηβ + ηβγ ) i ( 59 ) holds ∀ i = 0 , 1 , . . . . This is because : 1 When i = 0 , U ( γA ) i + V ( γB ) i = ( 1 + ηβ + ηβγ ) i = 1 , ( 59 ) holds . 2 When i = 1 , we have U ( γA ) i + V ( γB ) i = γ ( UA + V B ) = γ (cid:16) A − 1 A − B A + 1 − B A − B B (cid:17) = γ ( A + B − 1 ) = 1 + ηβ + ηβγ . ( 59 ) still holds . 3 When i > 1 , according to Jensen inequality , and because any function n ( x ) = x i is convex , we have U ( γA ) i + V ( γB ) i ≥ ( γUA + γV B ) i = ( 1 + ηβ + ηβγ ) i . ( 59 ) still holds . According to ( 59 ) and the deﬁnition of u ( x ) in Lemma 2 , we have u ( x ) = U ( γA ) x + V ( γB ) x − 1 ≥ ( 1 + ηβ + ηβγ ) x − 1 > 0 . Then , we have h ( x ) − h ( x − 1 ) = ηδ (cid:18) U ( γA ) x ( γA + A − 1 ) γA ( A − 1 ) + V ( γB ) x ( γB + B − 1 ) γB ( B − 1 ) − γ x + 1 − 1 γ − 1 (cid:19) ( a ) = γηδ (cid:18) U ( γA ) x γ ( A − 1 ) + V ( γB ) x γ ( B − 1 ) − γ x − 1 γ − 1 (cid:19) + ηδ ( γ x − 1 ( UA x − 1 + V B x − 1 ) − 1 ) ( b ) = γηδ x (cid:88) b = 1 γ x − b u ( b − 1 ) + ηδu ( x − 1 ) > 0 , where ( a ) is because ( 39 ) equals ( 38 ) ; ( b ) is because ( 48 ) equals ( 47 ) , x = t − ( k − 1 ) τ , and the deﬁnition of u ( x ) . To conclude , we have proven that h ( 0 ) = h ( 1 ) = 0 and h ( x ) increases with x when x ≥ 1 . H . Proof of Theorem 4 For convenience , we deﬁne c { p } ( t ) (cid:44) F ( x t { p } ) − F ( x ∗ ) for a given cloud interval { p } , where t ∈ [ ( p − 1 ) τπ , pτπ ] . We also deﬁne the following constants in this subsection . ω (cid:44) min p ∈ [ 1 , P ] , t ∈ { p } 1 (cid:107) x t { p } − x ∗ (cid:107) 2 , σ (cid:44) min p ∈ [ 1 , P ] , t 1 , t 2 ∈ { p } (cid:107)∇ F ( x t 1 { p } ) (cid:107) (cid:107)∇ F ( x t 2 { p } ) (cid:107) , ( 60 ) α (cid:44) η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) − βη 2 γ 2 µ 2 2 − ηγµ ( 1 − βη ( γ + 1 ) ) . ( 61 ) According to the convergence lower bound of any gradient descent methods given in [ 36 , Theorem 3 . 14 ] , we always have c { p } ( t ) > 0 for any t and p . Then we derive the upper bound of c { p } ( t + 1 ) − c { p } ( t ) , where t ∈ [ ( p − 1 ) τπ , pτπ − 1 ] . Because F ( · ) is β - smooth , according to [ 36 , Lemma 3 . 4 ] , we have c { p } ( t + 1 ) − c { p } ( t ) = F ( x t + 1 { p } ) − F ( x t { p } ) ≤(cid:104)∇ F ( x t { p } ) , x t + 1 { p } − x t { p } (cid:105) + β 2 (cid:107) x t + 1 { p } − x t { p } (cid:107) 2 = γ (cid:104)∇ F ( x t { p } ) , v t + 1 { p } (cid:105) − η (cid:107)∇ F ( x t { p } ) (cid:107) 2 + β 2 (cid:107) γ v t + 1 { p } − η ∇ F ( x t { p } ) (cid:107) 2 ( a ) = − η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) (cid:107)∇ F ( x t { p } ) (cid:107) 2 + βγ 4 2 (cid:107) v t { p } (cid:107) 2 + γ 2 ( 1 − βη ( γ + 1 ) ) (cid:104)∇ F ( x t { p } ) , v t { p } (cid:105) ( b ) ≤ (cid:18) − η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) + βη 2 γ 2 µ 2 2 + ηγµ ( 1 − βη ( γ + 1 ) ) ) (cid:107)∇ F ( x t { p } ) (cid:107) 2 , ( 62 ) where ( a ) is replacing v t + 1 { p } by ( 34 ) and rearranging the formula ; ( b ) is because (cid:107) γ v t { p } (cid:107) ≤ µ (cid:107) η ∇ F ( x t { p } ) (cid:107) with the deﬁnition of µ . According to Cauchy - Schwarz inequality , we can obtain (cid:104)∇ F ( x t { p } ) , v t { p } (cid:105) ≤ (cid:107)∇ F ( x t { p } ) (cid:107)(cid:107) v t { p } (cid:107) ≤ µηγ (cid:107)∇ F ( x t { p } ) (cid:107) 2 . According to the deﬁnition of α , and Con - dition ( 2 . 1 ) of Theorem 4 with h ( τ , δ (cid:96) ) ≥ 0 and h ( τπ , δ ) ≥ 0 which are proved in Appendix G , we have α > 0 . Then from ( 62 ) , we have c { p } ( t + 1 ) ≤ c { p } ( t ) − α (cid:107)∇ F ( x t { p } ) (cid:107) 2 . ( 63 ) Because F ( · ) is ρ - Lipschitz , and according to [ 47 , Lec - ture 2 , Lemma 1 ] , there exists a point x t 2 { p } such that F ( x t { p } ) − F ( x ∗ ) = (cid:104)∇ F ( x t 2 { p } ) , x t { p } − x ∗ (cid:105) . Hence , by Cauchy - Schwarz inequality , we have c { p } ( t ) = F ( x t { p } ) − F ( x ∗ ) ≤ (cid:107)∇ F ( x t 2 { p } ) (cid:107)(cid:107) x t { p } − x ∗ (cid:107) . Based on the deﬁnition of σ , and replacing t with t 1 , we have (cid:107)∇ F ( x t { p } ) (cid:107) ≥ σ (cid:107)∇ F ( x t 2 { p } ) (cid:107) . Thus , (cid:107)∇ F ( x t { p } ) (cid:107) ≥ σ (cid:107)∇ F ( x t 2 { p } ) (cid:107) ≥ σc { p } ( t ) (cid:107) x t { p } − x ∗ (cid:107) . Substituting above inequality into ( 63 ) , and noting ω ≤ 1 (cid:107) x t { p } − x ∗ (cid:107) 2 by the deﬁnition of ω , we get c { p } ( t + 1 ) ≤ c { p } ( t ) − ασ 2 c { p } ( t ) 2 (cid:107) x t { p } − x ∗ (cid:107) 2 ≤ c { p } ( t ) − ωασ 2 c { p } ( t ) 2 . Because α > 0 , c { p } ( t ) > 0 , and ( 63 ) , we have 0 < c { p } ( t + 1 ) ≤ c { p } ( t ) . Dividing both sides by c { p } ( t + 1 ) c { p } ( t ) , we get 1 c { p } ( t ) ≤ 1 c { p } ( t + 1 ) − ωασ 2 c { p } ( t ) c { p } ( t + 1 ) . We note that c { p } ( t ) c { p } ( t + 1 ) ≥ 1 . Thus , 1 c { p } ( t + 1 ) − 1 c { p } ( t ) ≥ ωασ 2 c { p } ( t ) c { p } ( t + 1 ) ≥ ωασ 2 . Summing up the above inequality by t ∈ [ ( p − 1 ) τπ , pτπ − 1 ] , we have 1 c { p } ( pτπ ) − 1 c { p } ( ( p − 1 ) τπ ) = (cid:80) pτπ − 1 t = ( p − 1 ) τπ (cid:16) 1 c { p } ( t + 1 ) − 1 c { p } ( t ) (cid:17) ≥ (cid:80) pτπ − 1 t = ( p − 1 ) τπ ωασ 2 = τπωασ 2 . Then , we sum up the above inequality by p ∈ [ 1 , P ] , after rearranging the left - hand side and noting that T = Pτπ , we can get P (cid:88) p = 1 (cid:18) 1 c { p } ( pτπ ) − 1 c { p } ( ( p − 1 ) τπ ) (cid:19) = 1 c { p } ( T ) − 1 c { 1 } ( 0 ) − P − 1 (cid:88) p = 1 (cid:18) 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) (cid:19) ≥ Pτπωασ 2 = Tωασ 2 . ( 64 ) 16 Following ( 64 ) , we note that 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) = c { p } ( pτπ ) − c { p + 1 } ( pτπ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ { p + 1 } ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ [ pπ ] ) + (cid:16) F ( x pτπ [ pπ ] ) − F ( x pτπ ) (cid:17) c { p } ( pτπ ) c { p + 1 } ( pτπ ) ( a ) ≥ − ρ (cid:80) L(cid:96) = 1 D (cid:96) D ( h ( τ , δ (cid:96) ) + s ( τ ) ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) ( b ) + − ρ (cid:16) h ( τπ , δ ) + π (cid:80) L(cid:96) = 1 D (cid:96) D ( h ( τ , δ (cid:96) ) + s ( τ ) ) (cid:17) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = − ρj ( τ , π , δ (cid:96) , δ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) , ( 65 ) where ( a ) is because of combining Theorem 1 and Theorem 2 ; ( b ) is because of Theorem 3 . From ( 63 ) , we can get F ( x t { p } ) ≥ F ( x t + 1 { p } ) for any t ∈ [ ( p − 1 ) τπ , pτπ ) . Recalling Condition ( 2 . 2 ) in Theorem 4 , where F ( x { p } ( pτπ ) ) − F ( x ∗ ) ≥ ε for all p , we can obtain c { p } ( t ) = F ( x t { p } ) − F ( x ∗ ) ≥ ε for all t ∈ [ ( p − 1 ) τπ , pτπ ] and p . Thus , c { p } ( pτπ ) c { p + 1 } ( pτπ ) ≥ ε 2 . According to Appendix G , we have h ( τ , δ (cid:96) ) ≥ 0 and h ( τπ , δ ) ≥ 0 . Then substituting above inequalities into ( 65 ) , we obtain 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) ≥ − ρj ( τ , π , δ (cid:96) , δ ) ε 2 . Substituting the above inequality into ( 64 ) and rearrange , we get 1 c { p } ( T ) − 1 c { 1 } ( 0 ) ≥ Tωασ 2 − ( P − 1 ) ρj ( τ , π , δ (cid:96) , δ ) ε 2 . ( 66 ) Recalling Condition ( 2 . 3 ) in Theorem 4 , where F ( x T ) − F ( x ∗ ) ≥ ε , and noting that c { p } ( T ) ≥ ε , we get ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ ε 2 . Thus , 1 F ( x T ) − F ( x ∗ ) − 1 c { p } ( T ) = c { p } ( T ) − ( F ( x T ) − F ( x ∗ ) ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) = F ( x T { p } ) − F ( x T ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ − ρj ( τ , π , δ (cid:96) , δ ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ − ρj ( τ , π , δ (cid:96) , δ ) ε 2 , ( 67 ) where the ﬁrst inequality follows the same method to prove ( 65 ) . Combining ( 66 ) with ( 67 ) , we get 1 F ( x T ) − F ( x ∗ ) − 1 c { 1 } ( 0 ) ≥ Tωασ 2 − P ρj ( τ , π , δ (cid:96) , δ ) ε 2 = Tωασ 2 − T ρj ( τ , π , δ (cid:96) , δ ) τπε 2 = T (cid:16) ωασ 2 − ρj ( τ , π , δ (cid:96) , δ ) τπε 2 (cid:17) . Noting that c { 1 } ( 0 ) = F ( x 0 { 1 } ) − F ( x ∗ ) > 0 , the above inequality can be expressed as 1 F ( x T ) − F ( x ∗ ) ≥ T (cid:16) ωασ 2 − ρj ( τ , π , δ (cid:96) , δ ) τπε 2 (cid:17) . Recalling Condi - tion ( 2 . 1 ) in Theorem 4 , where ωασ 2 − ρj ( τ , π , δ (cid:96) , δ ) τπε 2 > 0 , we obtain that the right - hand side of above inequality is greater than zero . Therefore , taking the reciprocal of the above inequality , we ﬁnally complete the proof of Theorem 4 . I . Proof of Theorem 5 At the beginning , we see that Condition ( 1 ) in Theorem 4 holds due to the Condition in Theorem 5 ( 0 < βη ( γ + 1 ) ≤ 1 , 0 < γ < 1 , 0 < γ a < 1 , and ∀ τ , π ∈ { 1 , 2 , . . . } ) . 1 ) ρj ( τ , π ) = 0 : In this case , there is an arbitrarily small ε > 0 that let Conditions ( 2 . 1 ) – ( 2 . 3 ) in Theorem 4 hold . In this case , Theorem 4 holds . We also note that the right - hand side of ( 23 ) is equivalent to the right - hand side of ( 20 ) when ρj ( τ , π ) = 0 . According to the deﬁnition of x f in ( 22 ) , we have F (cid:0) x f (cid:1) − F ( x ∗ ) ≤ F ( x T ) − F ( x ∗ ) ≤ 1 Tωασ 2 , which satisﬁes the result in Theorem 4 directly . Thus , Theorem 5 holds when ρj ( τ , π ) = 0 . 2 ) ρj ( τ , π ) > 0 : In this case , we aim to ﬁnd an ε satisfying Condition ( 2 . 1 ) , but Conditions ( 2 . 2 ) and ( 2 . 3 ) cannot be satisﬁed together so that F ( x f ) − F ( x ∗ ) can be bounded . We ﬁrst deﬁne an ε 0 , then we claim that any ε > ε 0 is what we want to ﬁnd . We set ε 0 as the root of the following equation , ε 0 = 1 T (cid:16) ωασ 2 − ρj ( τ , π ) τπε 20 (cid:17) . ( 68 ) The positive root is ε 0 = 1 2 Tωασ 2 + (cid:114) 1 4 T 2 ω 2 α 2 σ 4 + ρj ( τ , π ) ωασ 2 τπ . ( 69 ) Through this way , since ωασ 2 − ρj ( τ , π ) τπε 2 increases with ε , ε > ε 0 will lead to Condition ( 2 . 1 ) . Next , using the proof by contradiction , we can prove that when ε > ε 0 , there does not exist ε > ε 0 that satisﬁes both Conditions ( 2 . 2 ) and ( 2 . 3 ) in Theorem 4 at the same time . We assume that there exists such ε > ε 0 , so that Conditions ( 2 . 1 ) – ( 2 . 3 ) hold and thus Theorem 4 holds . Then we have F ( x T ) − F ( x ∗ ) ≤ 1 T ( ωασ 2 − ρj ( τ , π ) τπε 2 ) < 1 T (cid:18) ωασ 2 − ρj ( τ , π ) τπε 20 (cid:19) = ε 0 , which contradicts the Condition ( 2 . 3 ) in Theorem 4 . Therefore , for any ε > ε 0 , one of the following ( A ) or ( B ) holds . ( A ) ∃ p ∈ [ 1 , P ] such that F ( x pτπ { p } ) − F ( x ∗ ) ≤ ε 0 or ( B ) F ( x T ) − F ( x ∗ ) ≤ ε 0 . ( A ) or ( B ) gives min (cid:26) min p ∈ [ 1 , P ] F ( x pτπ { p } ) ; F ( x T ) (cid:27) − F ( x ∗ ) ≤ ε 0 . ( 70 ) According to ( 67 ) , when t = pτπ , we have F ( x pτπ ) ≤ F ( x pτπ { p } ) + ρj ( τ , π ) for any cloud interval { p } . Combining it with ( 70 ) , we have min p ∈ [ 1 , P ] F ( x pτπ ) − F ( x ∗ ) ≤ ε 0 + ρj ( τ , π ) . Recalling the deﬁnition of x f in ( 22 ) , T = Pτπ , and combining x f with above inequality , we get F (cid:0) x f (cid:1) − F ( x ∗ ) ≤ ε 0 + ρj ( τ , π ) . Substituting ( 69 ) into above inequality , we ﬁnally get the result in ( 23 ) , which completes the proof of Theorem 5 . 17 J . Proof of Theorem 6 When η → 0 + , we have γA (cid:39) 1 , γB (cid:39) γ , and J (cid:39) γ 2 ( 1 − γ ) 2 . Therefore , lim η → 0 + h ( τ , δ (cid:96) ) = lim η → 0 + ηδ (cid:96) (cid:20) I ( γA ) τ + J ( γB ) τ − 1 ηβ − γ 2 ( γ τ − 1 ) − ( γ − 1 ) τ ( γ − 1 ) 2 (cid:21) = lim η → 0 + ηδ (cid:96) (cid:18) I − 1 ηβ (cid:19) = lim η → 0 + ηδ (cid:96) (cid:18) 1 ( 1 − γ ) ( γA − 1 ) − 1 ηβ (cid:19) = δ (cid:96) 1 − γ lim η → 0 + η γA − 1 − δ (cid:96) β = δ (cid:96) 1 − γ lim η → 0 + 1 ( γA − 1 ) (cid:48) − δ (cid:96) β = δ (cid:96) 1 − γ 1 − γ β − δ (cid:96) β = 0 where the second last line is because of the L’H ˆ opital’s rule . Then , we can derive s ( · ) (cid:39) 0 . Afterwards , we have j ( · ) (cid:39) 0 and ˆ j ( · ) (cid:39) 0 . Therefore , f HierMo ( T ) (cid:39) 1 Tωασ 2 and f HierFAV G ( T ) (cid:39) 1 Tω ˆ ασ 2 . Based on the conditions in Theo - rem 6 , we have α > ˆ α . Therefore , we have f HierFAV G ( T ) − f HierMo ( T ) > 0 , which completes the proof of Theorem 6 . R EFERENCES [ 1 ] D . Lu and Q . Weng , “A survey of image classiﬁcation methods and tech - niques for improving classiﬁcation performance , ” International journal of Remote sensing , vol . 28 , no . 5 , pp . 823 – 870 , 2007 . [ 2 ] J . E . Naranjo , C . Gonz´alez , R . Garc´ıa , and etc . , “Power - steering control architecture for automatic driving , ” IEEE transactions on intelligent transportation systems , vol . 6 , no . 4 , pp . 406 – 415 , 2005 . [ 3 ] D . Yu and L . Deng , Automatic speech recognition . Springer , 2016 , vol . 1 . [ 4 ] B . McMahan , E . Moore , D . Ramage , and etc . , “Communication - efﬁcient learning of deep networks from decentralized data , ” in Artiﬁcial Intelli - gence and Statistics . PMLR , 2017 , pp . 1273 – 1282 . [ 5 ] W . Y . B . Lim , N . C . Luong , D . T . Hoang , Y . Jiao , Y . - C . Liang , Q . Yang , D . Niyato , and C . Miao , “Federated learning in mobile edge networks : A comprehensive survey , ” IEEE Communications Surveys & Tutorials , vol . 22 , no . 3 , pp . 2031 – 2063 , 2020 . [ 6 ] M . S . H . Abad , E . Ozfatura , D . Gunduz , and O . Ercetin , “Hierarchical federated learning across heterogeneous cellular networks , ” in ICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 2020 , pp . 8866 – 8870 . [ 7 ] C . Briggs , Z . Fan , and P . Andras , “Federated learning with hierarchical clustering of local updates to improve training on non - iid data , ” in IJCNN , 2020 , pp . 1 – 9 . [ 8 ] L . Liu , J . Zhang , S . Song , and K . B . Letaief , “Hierarchical quantized federated learning : Convergence analysis and system design , ” arXiv preprint arXiv : 2103 . 14272 , 2021 . [ 9 ] Y . Yan , T . Yang , Z . Li , Q . Lin , and Y . Yang , “A uniﬁed analysis of stochastic momentum methods for deep learning , ” in IJCAI , 2018 , pp . 2955 – 2961 . [ 10 ] C . Liu and M . Belkin , “Accelerating SGD with momentum for over - parameterized learning , ” in International Conference on Learning Rep - resentations , 2020 . [ 11 ] S . Vaswani , F . Bach , and M . Schmidt , “Fast and faster convergence of sgd for over - parameterized models and an accelerated perceptron , ” in The 22nd International Conference on Artiﬁcial Intelligence and Statistics . PMLR , 2019 , pp . 1195 – 1204 . [ 12 ] M . Assran and M . Rabbat , “On the convergence of nesterov’s accelerated gradient method in stochastic settings , ” in Proceedings of the 37th International Conference on Machine Learning , 2020 , pp . 410 – 420 . [ 13 ] W . Liu , L . Chen , Y . Chen , and W . Zhang , “Accelerating federated learn - ing via momentum gradient descent , ” IEEE Transactions on Parallel and Distributed Systems , vol . 31 , no . 8 , pp . 1754 – 1766 , 2020 . [ 14 ] H . Yu , R . Jin , and S . Yang , “On the linear speedup analysis of communi - cation efﬁcient momentum sgd for distributed non - convex optimization , ” in International Conference on Machine Learning . PMLR , 2019 , pp . 7184 – 7193 . [ 15 ] H . Yang , M . Fang , and J . Liu , “Achieving linear speedup with partial worker participation in non - iid federated learning , ” in ICLR , 2021 . [ 16 ] H . Gao , A . Xu , and H . Huang , “On the convergence of communication - efﬁcient local sgd for federated learning , ” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol . 35 , 2021 . [ 17 ] S . Ruder , “An overview of gradient descent optimization algorithms , ” arXiv preprint arXiv : 1609 . 04747 , 2016 . [ 18 ] L . Liu , J . Zhang , S . Song , and K . B . Letaief , “Client - edge - cloud hierarchical federated learning , ” in ICC 2020 - 2020 IEEE International Conference on Communications ( ICC ) . IEEE , 2020 , pp . 1 – 6 . [ 19 ] Z . Wang , H . Xu , J . Liu , H . Huang , C . Qiao , and Y . Zhao , “Resource - efﬁcient federated learning with hierarchical aggregation in edge com - puting , ” in IEEE INFOCOM 2021 - IEEE Conference on Computer Communications . IEEE , 2021 , pp . 1 – 10 . [ 20 ] Z . Huo , Q . Yang , B . Gu , L . C . Huang et al . , “Faster on - device training using new federated momentum algorithm , ” arXiv preprint arXiv : 2002 . 02090 , 2020 . [ 21 ] J . Wang , V . Tantia , N . Ballas , and M . Rabbat , “SlowMo : Improving communication - efﬁcient distributed sgd with slow momentum , ” in In - ternational Conference on Learning Representations , 2020 . [ 22 ] Z . Yang , W . Bao , D . Yuan , N . H . Tran , and A . Y . Zomaya , “Federated learning with nesterov accelerated gradient momentum method , ” arXiv preprint arXiv : 2009 . 08716 , 2020 . [ 23 ] S . P . Karimireddy , M . Jaggi , S . Kale , M . Mohri , S . J . Reddi , S . U . Stich , and A . T . Suresh , “Mime : Mimicking centralized stochastic algorithms in federated learning , ” arXiv preprint arXiv : 2008 . 03606 , 2020 . [ 24 ] A . Xu and H . Huang , “Coordinating momenta for cross - silo federated learning , ” in Proceedings of the AAAI Conference on Artiﬁcial Intelli - gence , vol . 36 , no . 8 , 2022 , pp . 8735 – 8743 . [ 25 ] E . Ozfatura , K . Ozfatura , and D . G¨und¨uz , “FedADC : Accelerated feder - ated learning with drift control , ” in 2021 IEEE International Symposium on Information Theory ( ISIT ) . IEEE Press , 2021 , p . 467 – 472 . [ 26 ] yeggasd , A . Trask , and froessler , FL on MNIST using a CNN , 2021 . [ Online ] . Available : https : / / notebook . community / OpenMined / PySyft / examples / tutorials / Part - 6 - Federated - Learning - on - MNIST - using - a - CNN [ 27 ] S . Gross , S . Chintala , N . Hug , L . Yeager , and E . R . etc . , Pytorch - VGG , may 2021 . [ Online ] . Available : https : / / github . com / pytorch / vision / blob / master / torchvision / models / vgg . py [ 28 ] T . Moon and T . Ryffel , Pytorch - Tiny - ImageNet , jun 2020 . [ Online ] . Available : https : / / github . com / tjmoon0104 / pytorch - tiny - imagenet [ 29 ] Y . Lecun , L . Bottou , Y . Bengio , and P . Haffner , “Gradient - based learning applied to document recognition , ” Proceedings of the IEEE , vol . 86 , no . 11 , pp . 2278 – 2324 , 1998 . [ 30 ] A . Krizhevsky et al . , “Learning multiple layers of features from tiny images , ” N / A , 2009 . [ 31 ] J . Deng , W . Dong , R . Socher , L . - J . Li , K . Li , and L . Fei - Fei , “Imagenet : A large - scale hierarchical image database , ” in 2009 IEEE conference on computer vision and pattern recognition . Ieee , 2009 , pp . 248 – 255 . [ 32 ] D . Anguita , A . Ghio , L . Oneto , X . Parra Perez , and J . L . Reyes Ortiz , “A public domain dataset for human activity recognition using smart - phones , ” in Proceedings of the 21th international European symposium on artiﬁcial neural networks , computational intelligence and machine learning , 2013 , pp . 437 – 442 . [ 33 ] B . T . Polyak , “Some methods of speeding up the convergence of iter - ation methods , ” USSR Computational Mathematics and Mathematical Physics , vol . 4 , no . 5 , pp . 1 – 17 , 1964 . [ 34 ] G . Goh , “Why momentum really works , ” Distill , 2017 . [ Online ] . Available : http : / / distill . pub / 2017 / momentum [ 35 ] Y . Nesterov , “A method for unconstrained convex minimization problem with the rate of convergence o ( 1 / k2 ) , ” Doklady ANSSSR ( translated as Soviet . Math . Docl . ) , vol . 269 , pp . 543 – 547 , 1983 . [ 36 ] S . Bubeck , “Convex optimization : Algorithms and complexity , ” arXiv preprint arXiv : 1405 . 4980 , 2014 . [ 37 ] Z . Yang , S . Fu , W . Bao , D . Yuan , and A . Y . Zomaya , “FastSlowMo : Federated learning with combined worker and aggregator momenta , ” IEEE Transactions on Artiﬁcial Intelligence , 2022 . [ 38 ] J . Wang , S . Wang , R . - R . Chen , and M . Ji , “Demystifying why local aggregation helps : Convergence analysis of hierarchical sgd , ” in Pro - ceedings of the AAAI Conference on Artiﬁcial Intelligence , 2022 . [ 39 ] T . Castiglia , A . Das , and S . Patterson , “Multi - level local sgd for heterogeneous hierarchical networks , ” arXiv preprint arXiv : 2007 . 13819 , 2020 . 18 [ 40 ] Y . Deng , F . Lyu , J . Ren , Y . Zhang , Y . Zhou , Y . Zhang , and Y . Yang , “Share : Shaping data distribution at edge for communication - efﬁcient hierarchical federated learning , ” in 2021 IEEE 41st International Con - ference on Distributed Computing Systems ( ICDCS ) , 2021 , pp . 24 – 34 . [ 41 ] P . Kairouz , H . B . McMahan , B . Avent , A . Bellet , M . Bennis , A . N . Bhagoji , K . Bonawitz , Z . Charles , G . Cormode , R . Cummings et al . , “Advances and open problems in federated learning , ” Foundations and Trends® in Machine Learning , vol . 14 , no . 1 – 2 , pp . 1 – 210 , 2021 . [ 42 ] S . Wang , T . Tuor , T . Salonidis , K . K . Leung , C . Makaya , T . He , and K . Chan , “Adaptive federated learning in resource constrained edge com - puting systems , ” IEEE Journal on Selected Areas in Communications , vol . 37 , no . 6 , pp . 1205 – 1221 , 2019 . [ 43 ] I . Goodfellow , Y . Bengio , and A . Courville , Deep learning . MIT press , 2016 . [ 44 ] C . T . Dinh , N . H . Tran , T . D . Nguyen , W . Bao , A . Y . Zomaya , and B . B . Zhou , “Federated learning with proximal stochastic variance reduced gradient algorithms , ” in 49th International Conference on Parallel Processing - ICPP , 2020 , pp . 1 – 11 . [ 45 ] S . Y . Teng , M . Touˇs , W . D . Leong , B . S . How , H . L . Lam , and V . M´aˇsa , “Recent advances on industrial data - driven energy savings : Digital twins and infrastructures , ” Renewable and Sustainable Energy Reviews , vol . 135 , p . 110208 , 2021 . [ 46 ] J . C . Kirchhof , L . Malcher , and B . Rumpe , “Understanding and im - proving model - driven iot systems through accompanying digital twins , ” in Proceedings of the 20th ACM SIGPLAN ICPG : Concepts and Experiences , 2021 , pp . 197 – 209 . [ 47 ] I . Mitliagkas and J . Gallego , “Ift 6085 : Theoretical principles for deep learning , ” in University of Montreal . University of Montreal , 2021 . [ Online ] . Available : http : / / mitliagkas . github . io / ift6085 - dl - theory - class /