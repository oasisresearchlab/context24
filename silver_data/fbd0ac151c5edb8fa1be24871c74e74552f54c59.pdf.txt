Scientiﬁc Workﬂow Management and the Kepler System ∗ Bertram Ludäscher † ? Ilkay Altintas † Chad Berkley ‡ Dan Higgins ‡ Efrat Jaeger † Matthew Jones ‡ Edward A . Lee § Jing Tao † Yang Zhao § September 2004 ; revised March 2005 Abstract Many scientiﬁc disciplines are now data and infor - mation driven , and new scientiﬁc knowledge is often gained by scientists putting together data analysis and knowledge discovery “pipelines” . A related trend is that more and more scientiﬁc communities real - ize the beneﬁts of sharing their data and computa - tional services , and are thus contributing to a distrib - uted data and computational community infrastruc - ture ( a . k . a . “the Grid” ) . However , this infrastructure is only a means to an end and scientists ideally should be bothered little with its existence . The goal is for scientists to focus on development and use of what we call scientiﬁc workﬂows . These are networks of analytical steps that may involve , e . g . , database ac - cess and querying steps , data analysis and mining steps , and many other steps including computation - ally intensive jobs on high performance cluster com - puters . In this paper we describe characteristics of and requirements for scientiﬁc workﬂows as identiﬁed in a number of our application projects . We then elaborate on Kepler , a particular scientiﬁc work - ﬂow system , currently under development across a number of scientiﬁc data management projects . We describe some key features of Kepler and its under - lying Ptolemy ii system , planned extensions , and areas of future research . Kepler is a community - driven , open source project , and we always welcome related projects and new contributors to join . ∗ Work supported by NSF / ITR 0225676 ( SEEK ) , DOE Sci - DAC DE - FC02 - 01ER25486 ( SDM ) , NSF / ITR CCR - 00225610 ( Chess ) , NSF / ITR 0225673 ( GEON ) , NIH / NCRR 1R24 RR019701 - 01 Biomedical Informatics Research Network Coor - dinating Center ( BIRN - CC ) , NSF / ITR 0325963 ( ROADNet ) , NSF / DBI - 0078296 ( Resurgence ) † San Diego Supercomputer Center , UC San Diego ; ? Dept . of Computer Science & Genome Center , UC Davis ; ‡ National Center for Ecological Analysis and Synthesis , UC Santa Bar - bara ; and § Department of Electrical Engineering and Com - puter Sciences , UC Berkeley Contents 1 Introduction 2 2 Scientiﬁc Workﬂows 3 2 . 1 Example Workﬂows . . . . . . . . . . 3 2 . 1 . 1 Promoter Identiﬁcation . . . . 3 2 . 1 . 2 Mineral Classiﬁcation . . . . . 4 2 . 1 . 3 Job Scheduling . . . . . . . . . 5 2 . 2 Requirements and Desiderata . . . . . 6 2 . 3 Diﬀerences to Business Workﬂows . . . 8 3 Highlights of Kepler 8 3 . 1 Web Service Extensions . . . . . . . . 8 3 . 2 Grid and other Extensions . . . . . . . 9 3 . 3 Actor - Oriented Modeling . . . . . . . . 10 4 Research Issues 13 4 . 1 Higher - Order Constructs . . . . . . . . 13 4 . 2 Third Party Transfers . . . . . . . . . 14 4 . 3 Other Research Issues . . . . . . . . . 15 4 . 4 Related Work . . . . . . . . . . . . . . 15 5 Conclusions 16 1 1 INTRODUCTION 2 “The diversity of the phenomena of nature is so great , and the treasures hidden in the heavens so rich , precisely in order that the human mind shall never be lacking in fresh nourishment . ” — Johannes Kepler , My < erium Cosmographicum 1 Introduction Information technology is revolutionizing the way many sciences are conducted , as witnessed by new techniques , results , and discoveries from quickly evolving , multi - disciplinary ﬁelds such as bioin - formatics , biomedical informatics , cheminformatics , ecoinformatics , geoinformatics , etc . To further ad - vance this new data - and information - driven sci - ence through advanced IT infrastructure , large in - vestments are made , e . g . , in the UK e - Science pro - gramme , or in the US through the NSF Cyberin - frastructure initiative and other initiatives from NIH ( BIRN : Biomedical Informatics Research Network ) and DOE ( SciDAC : Scientiﬁc Discovery through Ad - vanced Computing , GTL : Genomes to Life ) , just to mention a few . While many eﬀorts focus on the underlying middleware infrastructure , known as “the Grid” , scientists are ultimately interested in tools that bring the power of distributed databases and other computational Grid resources to the desktop , and al - low them to conveniently put together and run their own scientiﬁc workﬂows . By these we mean process networks that are typically used as “data analysis pipelines” or for comparing observed and predicted data , and that can include a wide range of compo - nents , e . g . , for querying databases , for data transfor - mation and data mining steps , for execution of sim - ulation codes on high performance computers , etc . Ideally , the scientist should be able to plug - in almost any scientiﬁc data resource and computational service into a scientiﬁc workﬂow , inspect and visualize data on the ﬂy as it is computed , make parameter changes when necessary and re - run only the aﬀected “down - stream” components , and capture suﬃcient metadata in the ﬁnal products such that the runs of a scientiﬁc workﬂow , when considered as ( computational ) exper - iments themselves , help explain the results and make them reproducible by the computational scientist and others . Thus , a scientiﬁc workﬂow system becomes a scientiﬁc problem - solving environment , tuned to an increasingly distributed and service - oriented Grid in - frastructure . However , before this grand vision can become re - ality , a number of signiﬁcant challenges have to be addressed . For example , current Grid software is still too complex to use for the average scientist , and fast changing versions and evolving standards require that these details be hidden from the user by the scientiﬁc workﬂow system . Web services seem to provide a sim - ple basis for loosely coupled , distributed systems , but core web service standards such as WSDL [ WSD03 ] only provide simple solutions to simple problems , 1 while harder problems such as web service orchestra - tion , 3rd party transfer ( from one service directly to another , circumventing the transfer back to a work - ﬂow control engine ) , and transactional semantics of service - based workﬂows , remain the subject of emerg - ing or future web service standards . The complexity of the underlying technical issues and the resulting ( sometimes overly ) complex standards make it less likely that those will be as widely adopted as the core standards such as XML and WSDL . Another set of challenges arises from the inherent complexity of scientiﬁc data itself . For example , how can we capture more of the semantics of scientiﬁc data ( beyond simple metadata meant for human con - sumption ) and thus inform the system which data sets might be suitable input for a speciﬁc analyti - cal pipeline ? Similarly , how can we deﬁne when it is even potentially meaningful at the conceptual level to compose two independently designed web services , or when an analysis pipeline might be included as a subworkﬂow in another scientiﬁc workﬂow ? Knowl - edge representation techniques , including formal on - tologies , and corresponding Semantic Web standards such as the Web Ontology Language [ OWL03 ] seem promising directions . However , as is the case for Grid middleware , the goal is to hide the underlying com - plexity as much as possible from the user of a scien - tiﬁc workﬂow system . The paper is organized as follows : In Section 2 we introduce scientiﬁc workﬂows by means of sev - eral real - world examples from diﬀerent domains . We use those examples to illustrate some of the charac - teristic features and requirements of scientiﬁc work - ﬂows , and compare the latter with business work - ﬂows . In Section 3 we present speciﬁc features of Kepler and its underlying Ptolemy ii system . As it turns out , Ptolemy ii provides much more than a user - friendly graphical user - interface ( called Vergil ) and a ready - to - be - extended open source platform . The main advantage of the system lies in a model - ing and design paradigm called actor - oriented mod - eling that has proven to be essential to deal with the complex design issues of scientiﬁc workﬂows . Sec - tion 4 presents some ongoing research issues . Finally , in Section 5 , we brieﬂy summarize our ﬁndings and work . 1 E . g . WSDL mainly provides an XML notation for function signatures , i . e . , the types of inputs and outputs of web services . 2 SCIENTIFIC WORKFLOWS 3 Figure 1 : Conceptual ( “napkin drawing” ) view of the Promoter Identiﬁcation Workﬂow ( PIW ) [ ABB + 03 ] 2 Scientiﬁc Workﬂows There is a growing interest in scientiﬁc workﬂows as can be seen from a number of recent events , e . g . , the Scientiﬁc Data Management Workshop [ SDM03 ] , the e - Science Workﬂow Services Workshop [ eSc03 ] , the e - Science Grid Environments Workshop [ eSc04 ] , the Virtual Observatory Service Composition Work - shop [ GRI04 ] , the e - Science LINK - Up Workshop on Workﬂow Interoperability and Semantic Extensions [ LIN04 ] , and last not least , various activities as part of the Global Grid Forum ( e . g , [ GGF04 ] ) , just to name a few . Scientiﬁc workﬂows also play an im - portant role in a number of ongoing large research projects dealing with scientiﬁc data management , in - cluding those funded by NSF / ITR ( GriPhyN , GEON , LEAD , SCEC , SEEK , . . . ) , NIH ( BIRN ) , DOE ( Sci - DAC / SDM , GTL ) , and similar eﬀorts funded by the UK e - Science initiative ( myGrid , DiscoveryNet , and others ) . For example , the SEEK project [ SEE ] is de - veloping an Analysis and Modeling System ( AMS ) that allows ecologists to design and execute scientiﬁc workﬂows [ MBJ + 04 ] . The AMS workﬂow component employs a Semantic Mediation System ( SMS ) to facil - itate workﬂow design and data discovery via seman - tic typing [ BL04 ] . Thus SEEK is a good example of a community - driven project in need of a system that allows users to seamlessly access data sources and ser - vices , and put them together into reusable workﬂows . Indeed SEEK is one of the main projects contributing to the cross - project Kepler initiative and workﬂow system discussed below . Aspects and Types of Workﬂows . Scientiﬁc workﬂows often exhibit particular “traits” , e . g . , they can be data - intensive , compute - intensive , analysis - intensive , visualization - intensive , etc . The workﬂows in Sections 2 . 1 . 1 , 2 . 1 . 2 , and 2 . 1 . 3 , e . g . , exhibit diﬀer - ent features , i . e . , service - orientation and data analy - sis , re - engineering and user interaction , and high - performance computing , respectively . Depending on the intended user group , one might want to hide or emphasize particular aspects and technical capabili - ties of scientiﬁc workﬂows . For example , a “Grid en - gineer” might be interested in low - level workﬂow as - pects such as data movement and remote job control . Having workﬂow components ( or actors ) that operate at this level will be beneﬁcial to the Grid engineer . Conversely , a scientiﬁc workﬂow system should hide such aspects from analytical scientists ( say an ecolo - gist studying species richness and productivity ) . The Kepler system aims at supporting very dif - ferent kinds of workﬂows , ranging from low - level “plumbing” workﬂows of interest to Grid engineers , to analytical knowledge discovery workﬂows for sci - entists , and conceptual - level design workﬂows that might become executable only as a result of subse - quent reﬁnement steps [ BL05 ] . In the following we ﬁrst introduce scientiﬁc work - ﬂows by means of several examples taken from diﬀer - ent projects and implemented using the Ptolemy ii - based Kepler system [ KEP ] . We then discuss typi - cal features of scientiﬁc workﬂows and from this de - rive general requirements and desiderata for scientiﬁc workﬂow systems . We take a closer look at underly - ing technical issues and challenges in Section 3 . 2 . 1 Example Workﬂows 2 . 1 . 1 Promoter Identiﬁcation Figure 1 shows a high - level , conceptual view of a typical scientiﬁc knowledge discovery workﬂow that 2 SCIENTIFIC WORKFLOWS 4 links genomic biology techniques such as microarrays with bioinformatics tools such as BLAST to identify and characterize eukaryotic promoters 2 – we call this the Promoter Identiﬁcation Workﬂow or PIW ( see also [ Wer01 , ABB + 03 , PYN + 03 ] : Starting from mi - croarray data , cluster analysis algorithms are used to identify genes that share similar patterns of gene expression proﬁles that are then predicted to be co - regulated as part of an interactive biochemical path - way . Given the gene - ids , gene sequences are retrieved from a remote database ( e . g . , GenBank ) and fed to a tool ( e . g . , BLAST ) that ﬁnds similar sequences . In subsequent steps , transcription factor binding sites and promoters are identiﬁed to create a promoter model that can be iteratively reﬁned . While Figure 1 leaves many details open , some fea - tures of scientiﬁc workﬂows can already be identiﬁed : There are a number of existing databases ( such as GenBank ) and computational tools ( such as Clusfa - vor and BLAST ) that need to be combined in certain ways to create the desired workﬂow . In the past , ac - cessing remote resources often meant implementing a wrapper that mimics a human entering the input of interest , submitting an HTML form , and “screen - scraping” the result from the returned page [ LPH01 ] . Today , more and more tools and databases become accessible via web services , greatly simplifying this task . Another trend are web portals such as NCBI [ NCB04 ] that integrate many tools and databases and sometimes provide the scientist with a “workbench” environment . Figure 2 depicts snapshots of an early implementa - tion of PIW in Kepler . Kepler is an extension of the Ptolemy ii system [ PTO04 ] for scientiﬁc work - ﬂows . The topmost window includes a loop whose body is expanded below and which performs several steps on each of the given gene - ids : First , an NCBI web service is used to access GenBank data . Subse - quently a BLAST step is performed to identify similar sequences to the one retrieved from GenBank . Then a second inner loop is executed ( bottom window ) for a transcription factor binding site analysis . Using Ptolemy ii terminology , we call the individual steps actors , since they act as independent components which communicate with each other only through the channels indicated in the ﬁgure . The overall execu - tion of the workﬂow is orchestrated by a director ( the green box in Figure 2 ; see Section 3 . 3 for details ) . This early PIW implementation in Kepler [ ABB + 03 ] illustrates a number of features : Actual “wiring” of a scientiﬁc workﬂow can be much more complicated than the conceptual view ( Figure 1 ) sug - 2 A promoter is a subsequence of a chromosome that sits close to a gene and regulates its activity . Figure 2 : PIW implemented in Kepler [ ABB + 03 ] . Composite actors ( subworkﬂows ) expanded below . gests . A mechanism for collapsing details of a sub - workﬂow into an abstract component ( called compos - ite actor in Ptolemy ii ) is essential to tame com - plexity : The windows in Figure 2 have well - deﬁned input and output ports and thus correspond to ( sub ) - workﬂows that can be collapsed into a more abstract , composite actor as indicated . Nevertheless , the re - sulting workﬂow is fairly complex and we will need to introduce additional mechanisms to simplify the design in particular of loops ( see Section 4 . 1 ) . 2 . 1 . 2 Mineral Classiﬁcation The second example , from a geoinformatics domain , illustrates the use of a scientiﬁc workﬂow system for automation of an otherwise manual procedure , or al - ternatively , for reengineering an existing custom tool in a more generic and extensible environment . The upper left window in Figure 3 shows the top - level workﬂow : Some samples are selected from a database holding experimentally determined mineral composi - tions of igneous rocks . This data , together with a set of classiﬁcation diagrams are fed into a Classifier subworkﬂow ( bottom left ) . The manual process of classifying samples involves determining the position of the sample values in a series of diagrams such as the one shown on the right in Figure 3 : if the loca - tion of a sample point in a non - terminal diagram of order n has been determined ( e . g . , diorite gabbro 2 SCIENTIFIC WORKFLOWS 5 anorthosite , bottom right ) , the corresponding dia - gram of order n + 1 is consulted and the point located therein . This process is iterated until the terminal level of diagrams is reached ( here shown in the upper right : the classiﬁcation result is anorthosite ) . This traditionally manual process has been auto - mated in commercial custom tools , or here in the Kepler workﬂow shown in Figure 3 . As above , work - ﬂows are shown in graphical form using Ptolemy ii ’s Vergil user interface [ BLL + 04b ] . Note that in Vergil , workﬂows can be annotated with user comments . Subworkﬂows ( e . g . , bottom - left ) become visible by right - clicking on a composite actor ( such as Clas - sifier , upper - left ) and selecting “Look Inside” from the resulting pop - up menu . Vergil also features sim - ple VCR - like control buttons to play , pause , resume , and stop workﬂow execution ( red icons in the top - left toolbar ; e . g . , right - triangle for play ) . Kepler speciﬁc features of this workﬂow include : A searchable library of actors and data sources ( Actor and Data tabs close to the upper - left ) with numerous reusable Kepler actors . For example , the Browser actor ( used in the bottom - right of the Classifier subworkﬂow ) launches the user’s default browser and can be used as a powerful generic input / output device in any workﬂow . In this example , the classiﬁcation diagrams are generated on the client side as interac - tive SVG displays in the browser ( windows on the right in Figure 3 ) . Moving the mouse over the dia - gram highlights the speciﬁc region and displays the rock name classiﬁcation ( s ) for that particular region . The Browser actor has proven to be very useful in many other workﬂows as well , e . g . , as a device to display results of a previous step , and as a selection tool that passes user choices ( made via HTML forms , check - boxes , etc . ) to subsequent workﬂow steps . 2 . 1 . 3 Job Scheduling The ﬁnal example workﬂow , depicted in Figure 4 , is from a cheminformatics domain and involves running thousands of jobs of the GAMESS quantum chemical code [ SBB + 93 ] under the control of the Nimrod / G Grid distribution tool [ AGK00 ] . This is an example of a workﬂow employing high - performance computing ( HPC ) resources in a coordinated manner to achieve a computationally hard task , in this case a variant of a hybrid quantum mechanics / molecular mechan - ics ( QM / MM ) technique ; see [ GT98 ] and [ SBA + 04 ] for details . Interestingly , the workﬂow in Figure 4 is rather domain - neutral and illustrates some features typical of many high - performance computational ex - periments : The main window shows four composite actors , Figure 4 : Workﬂow for scheduling HPC jobs . corresponding to the four depicted subworkﬂows . The ﬁrst one , PrepareInputs creates a list of in - put ﬁles for the subsequent jobs . These ﬁles are then used to create a plan ﬁle for Nimrod / G in the Pre - pareExperiment step . The AddExperiment sub - workﬂow takes a plan ﬁle and generates experiment run ﬁles using several CommandLine actors . The latter is shown with a “ $ ” icon ( to indicate a com - mand shell ) , and has proven to be a very useful rapid - prototyping tool : Existing local applications can be made part of a workﬂow simply by providing a suit - able command line expression and the correspond - ing command line arguments . The ManageRe - sources subworkﬂow can create new processes ( via AddFork ) to run jobs and subsequently add exper - iments as new server processes . This example workﬂow also highlights the possibil - ity of incremental design and development : At the time of writing , not all components of the overall workﬂow are operational . Nevertheless , due to the clearly deﬁned input / output interfaces of all subwork - ﬂows ( a feature inherited from Ptolemy ii ) , each of them can be designed , implemented , and tested sepa - rately . Moreover , the current version of the workﬂow relies heavily on invoking external applications via the CommandLine actor . Some of these applications might be “promoted” to custom actors with native Java implementations in the future . Such changes are encapsulated by the containing subworkﬂow and thus do not require changes of other parts of the workﬂow . 2 SCIENTIFIC WORKFLOWS 6 Figure 3 : Mineral Classiﬁcation workﬂow ( left ) and generated interactive result displays ( right ) . 2 . 2 Requirements and Desiderata In this section we summarize a number of common re - quirements and desiderata of scientiﬁc workﬂows , as exhibited by the examples above or by other work - ﬂows we encountered in various application - oriented research projects including GEON , SEEK and several others [ GEO , SEE , SDM , BIR , ROA ] . R1 : Seamless access to resources and services : This is a very common requirement ( e . g . , see the example workﬂows in Section 2 . 1 ) , and web ser - vices provide a ﬁrst , simple mechanism for re - mote service execution and remote database ac - cess 3 via service calls . However , as mentioned before , web services are a simple solution to a simple problem . Harder problems , e . g . , web ser - vice orchestration , and 3rd party transfer are not solved by “vanilla” web services alone . R2 : Service composition & reuse and workﬂow design : Since web services emerge as the ba - sic building blocks for distributed Grid appli - cations and workﬂows , the problem of service composition , i . e . , how to compose simple ser - vices to perform complex tasks , has become a 3 We do not elaborate on the important challenges of data integration [ She98 ] ; see , e . g . , [ Hal01 ] for a survey of query re - writing techniques , and [ NL04 ] and [ LGM03 , BLL04a ] for re - lated issues of query capabilities and semantics , respectively . hot research topic [ ICA03 ] . Among the diﬀer - ent approaches are those that view service com - position as an AI planning problem [ BDG03 ] , a query planning problem [ LAG03 , LN04 ] , or a general design and programming problem . A re - lated issue is how to design components so that they are easily reusable and not geared to only the speciﬁc applications that may have driven their original development . As we will see , ser - vice composition and reuse are addressed by em - ploying an actor - oriented approach at the de - sign level ( Section 3 . 3 ) , but also require ﬂexible means for data - transformations at the “plumb - ing” level ( Section 3 . 2 ) . R3 : Scalability : Some workﬂows involve large vol - umes of data and / or require high - end compu - tational resources , e . g . , running a large num - ber of parallel jobs on a cluster computer ( such as workﬂow in Section 2 . 1 . 3 ) . To support such data - intensive and compute - intensive workﬂows , suitable interfaces to Grid middleware com - ponents ( sometimes called Compute - Grid and Data - Grid , respectively ) are necessary . R4 : Detached execution : Long running workﬂows require an execution mode that allows the work - ﬂow control engine to run in the background on a remote server , without necessarily stay - ing connected to a user’s client application that 2 SCIENTIFIC WORKFLOWS 7 has started and is controlling workﬂow execution ( such as the Vergil GUI of Kepler ) . R5 : Reliability and fault - tolerance : Some com - putational environments are less reliable than others . For example , a workﬂow that incorpo - rates a new web service can easily “break” , as the latter can often fail , change its interface , or just become unacceptably slow ( as it becomes more popular ) . To make a workﬂow more resilient in an inherently unreliable environment , contin - gency actions must be speciﬁable , e . g . , fail - over strategies with alternate web services . R6 : User - interaction : Many scientiﬁc workﬂows require user decisions and interactions at vari - ous steps . 4 For example , an improved version of PIW ( Section 2 . 1 . 1 ) allows the user to inspect intermediate results and select and re - rank them before feeding them to subsequent steps . An interesting challenge is the need for user inter - action in a detached execution . Using a noti - ﬁcation mechanism the user might be asked to reconnect to the running instance and make a decision before the paused ( sub - ) workﬂow can resume . R7 : “Smart” re - runs : A special kind of user inter - action is the change of a parameter of a workﬂow or actor . For example , in a visualization pipeline or a long running workﬂow , the user might de - cide to change some parameters after inspecting intermediate or even ﬁnal results . A “smart” re - run would not execute the workﬂow from scratch , but only those parts that are aﬀected by the pa - rameter change . In dataﬂow - oriented systems ( e . g . , visualization pipeline systems such as AVS , OpenDX , SCIRun , or the Kepler system ) this is easier to realize than in more control - oriented systems ( e . g . , business workﬂow systems ) , since data and actor dependencies are already explicit in the system . Another useful technique in this context is checkpointing , which allows to back - track ( in the case of a parameter change or even a system failure ; cf . ( R5 ) ) to a previously saved state without starting over from scratch . R8 : “Smart” ( semantic ) links : A scientiﬁc work - ﬂow system should assist workﬂow design and data binding phases by suggesting which actor components might possibly ﬁt together ( this is also an aspect of ( R2 ) , service composition ) , or 4 In fact , when workﬂow management was still called “oﬃce automation” , humans were the main processors of tasks – the workﬂow system was just used for book - keeping ; cf . Section 2 . 3 . by indicating which data sets might be fed to which actors or workﬂows . To do so , some of the semantics of data and actors has to be cap - tured . However , capturing data semantics is a hard problem in many scientiﬁc disciplines : e . g . , measurement contexts , experimental protocols , and assumptions made are often not adequately represented . Even if corresponding metadata is available , it is often not clear how to best make it useable by the system . It seems clear though that ontologies provide a very useful semantic type system for scientiﬁc workﬂows , in addition to the current ( structural ) type systems [ BL04 ] . R9 : Data provenance : Just as the results of a con - ventional wet lab experiment should be repro - ducible , computational experiments and runs of scientiﬁc workﬂows should be reproducible and indicate which speciﬁc data products and tools have been used to create a derived data product . Beyond the conventional capture of metadata , a scientiﬁc workﬂow system should be able to automatically log the sequence of applied steps , parameter settings and ( persistent identiﬁers of ) intermediate data products . A related desider - ata is automatic report generation : The system should allow the user to generate reports with all relevant provenance and runtime information , e . g . , in XML format for archival and exchange purposes and in HTML ( generated from the for - mer , e . g . , via an XSLT script ) for human con - sumption . Data provenance can be seen as a prerequisite to ( R8 ) : In order to provide semantic information about a derived data product , suitable prove - nance information is needed . While the above list of requirements and desiderata for scientiﬁc workﬂow systems is by no means com - plete , it should be suﬃcient to capture many of the core characteristics . Other requirements include the use of an intuitive GUI to allow the user to compose a workﬂow visually from smaller components , or to “drill - down” into subworkﬂows , to animate workﬂow execution , to inspect intermediate results , etc . A scientiﬁc workﬂow system should also support the combination of diﬀerent workﬂow granularities . For example , coarse - grained workﬂows , akin to Unix pipelines or web service - based workﬂows , consist mainly of “black box” actors whose contents are un - known to the system . Scientiﬁc workﬂows may also be very ﬁne - grained , or include ﬁne - grained subwork - ﬂows . In that case , components are “white boxes” containing , e . g . , the visual programming equivalent 3 HIGHLIGHTS OF KEPLER 8 of an algorithm , or a system of diﬀerential equations to be solved , in other words , a detailed speciﬁcation known to the system . 2 . 3 Diﬀerences to Business Workﬂows The characteristics and requirements of scientiﬁc workﬂows are partially overlapping those of business workﬂows . Indeed , the term ‘scientiﬁc workﬂows’ seems to indicate a very close relationship with the latter , while a more detailed comparison reveals a number of signiﬁcant diﬀerences . Historically , busi - ness workﬂows have roots going back to oﬃce au - tomation systems of the 1970’s and 80’s , and gained momentum in the 90’s under diﬀerent names includ - ing business process modeling and business process engineering ; see , e . g . , [ AM97 , vdAvH02 , zM04 ] . Today we see some inﬂuence of business workﬂow standards in the web services arena , speciﬁcally stan - dards for web service choreography . 5 For example , the Business Process Execution Language for Web Ser - vices ( BPEL4WS ) [ CGK + 02 ] , a merger of two earlier standards , IBM’s WSFL and Microsoft’s XLANG , has received some attention recently . When analyzing the underlying design princi - ples and execution models of business workﬂow ap - proaches , a focus on control - ﬂow patterns and events becomes apparent , whereas dataﬂow is often a sec - ondary issue . For example , [ vdAtHKB03 ] describe a large number of workﬂow design patterns that can be used to analyze and compare business workﬂow stan - dards and products in terms of their control features and expressiveness . Scientiﬁc workﬂow systems , on the other hand , tend to have execution models that are much more dataﬂow - oriented . This is true , e . g . , for academic systems including Kepler , Taverna [ TAV ] , and Triana [ TRI ] , and for commercial systems such as Inforsense’s DiscoveryNet or Scitegic’s Pipeline - Pilot . With respect to their modeling paradigm and execution models , these systems seem closer to an “AVS for scientiﬁc data and services” than to the more control - ﬂow and task - oriented business workﬂow sys - tems , or to their early scientiﬁc workﬂow predecessors [ CM95 , MVW96 , AIL98 ] . The diﬀerence between dataﬂow - orientation and control - ﬂow orientation can also be observed in the underlying formalisms . For example , visualizations of business workﬂows often resemble ﬂowcharts , state transition diagrams , or UML activity diagrams , all of which emphasize events and control - ﬂow over 5 Despite the long history of business workﬂows , it is sur - prising how short - lived some of the so - called standards are , as “most of them die before becoming mature” [ vdA03 ] . dataﬂow . Formal analysis of workﬂows usually in - volves studying their control - ﬂow patterns [ Kie02 ] , and is often conducted using Petri nets . Conversely , the underlying execution model of cur - rent scientiﬁc workﬂow systems usually resembles or is even directly implemented as a dataﬂow process network [ KM77 , LP95 ] , having traditional applica - tion areas , e . g . , in digital signal processing . Dataﬂow - oriented approaches are applicable at very diﬀer - ent levels of granularity , from low - level CPU oper - ations found in certain processor architectures , to high - level programming paradigms such as ﬂow - based programming [ Mor94 ] . Scientiﬁc workﬂow systems and visualization pipeline systems can also be seen as dataﬂow - oriented problem solving environments [ WBB96 ] that scientists use to analyze and visualize their data . Last not least , there is also a close re - lationship between dataﬂow - oriented approaches and ( pure ) functional languages , including non - strict vari - ants such as Haskell ( cf . Section 4 . 1 ) . 3 Highlights of Kepler In this section , we discuss some highlights of the cur - rent Kepler system as well as some upcoming ex - tensions . Many features directly address the require - ments and desiderata from Section 2 . More research - oriented extensions are described in Section 4 . 3 . 1 Web Service Extensions A basic requirement for scientiﬁc workﬂows is seam - less access to remote resources and services ( see ( R1 ) in Section 2 . 2 and the examples in Section 2 . 1 ) . Since web services are emerging as the standard means for remote service execution of loosely coupled systems , we extended Kepler early on to handle web services . Given the URL of a web service description [ WSD03 ] , the generic WebService actor of Kepler can be in - stantiated to any particular operation speciﬁed in the service description . After instantiation , the Web - Service actor can be incorporated into a scientiﬁc workﬂow as if it were a local component . In partic - ular , the WSDL - deﬁned inputs and outputs of the service are made explicit via the instantiated actor’s input and output ports . Figure 5 shows screenshots of an extended web ser - vice harvesting feature , implemented by a special web service Harvester component . 6 As in the case of the generic WebService actor , a URL is ﬁrst pro - vided ( see ( 1 ) in Figure 5 ) , however this time not to an individual WSDL description of a web service , 6 Inspiration came from a similar feature in Taverna . 3 HIGHLIGHTS OF KEPLER 9 (cid:1) (cid:2) (cid:3) (cid:4) Figure 5 : Kepler web service Harvester in action : repository access ( 1 - 2 ) , harvesting ( 3 ) , and use ( 4 ) . but to a web service repository . The repository URL might point to a UDDI repository , or simply to a web page listing multiple WSDL URLs as shown in ( 2 ) . The Harvester then retrieves and analyzes all WSDL ﬁles of the repository , creating instantiations of web service actors in the user’s local actor library ; see ( 3 ) . For example , one of the harvested services , the BLAST web service , comprises ﬁve service oper - ations which are imported into a corresponding sub - directory . The user can then drag - and - drop any of these service operations on the workﬂow canvas for use in a scientiﬁc workﬂow ( 4 ) . The Harvester feature facilitates rapid prototyping and development of web service - based applications and workﬂows in a matter of minutes – that is , provided ( i ) the web services are alive when needed , and ( ii ) they can be wired together more or less directly to perform the desired complex task . The problem with ( i ) is that , while harvested web ser - vices look like local components , their runtime failure can easily “break” a scientiﬁc workﬂow , reminding the user that the service interface has been harvested , not the actual code . 7 We are currently extending Kepler to make workﬂows with web services more reliable . One simple approach is to avoid the as - sociation of a service operation with a ﬁxed URL . Instead , a list of alternate services can be provided when the workﬂow is launched , and service failure can then be compensated by invocation of one of the alternate services . Another option is to insert spe - cial control tokens into the data stream , indicating 7 Which is of course the whole point of web services . to downstream actors the absence of certain results . Long running workﬂows may thus more gracefully react to web service failures and produce at least partial results . This idea has been further devel - oped for “collection - oriented” ( in the functional pro - gramming sense ) workﬂows : via so - called “exception - catching actors” , invalid ( due to failures ) data col - lections can be ﬁltered out of the data stream , while valid subcollections pass through unaﬀected [ McP05 ] . An interesting research question is how to extend Ptolemy ii ’s pause - resume model to a full - ﬂedge transaction model that can handle service failures . The problem ( ii ) is even more fundamental and has diﬀerent aspects : At the design level the chal - lenge is how to devise actors that can be reused eas - ily . In Section 3 . 3 we give a brief introduction to actor - oriented modeling , the underlying paradigm of Ptolemy ii , and discuss how it facilitates component composition and reuse . At the “plumbing” level it is often necessary to apply data transformations be - tween two consecutive web services ( called “shims” in Taverna ) . Such data transformations are sup - ported through various actors in Kepler , e . g . , XSLT and XQuery actors to apply transformations to XML data , or Perl and Python actors for text - based trans - formations . 3 . 2 Grid and other Extensions Figure 6 depicts a number of Kepler actors that fa - cilitate scientiﬁc workﬂows , including workﬂows that make use of “the Grid” . In the upper left , the previ - ously discussed generic WebService actor and some 3 HIGHLIGHTS OF KEPLER 10 instantiations are shown . Note how the latter spe - cialize their actor interface via their input / output ports : e . g . , Blast _ SearchSimple has three input ports and one output port , for the search arguments and result , respectively . The naming scheme used is WSN _ OP , where WSN is the name of the web ser - vice and OP is a speciﬁc web service operation . Figure 6 : Grid actors and other Kepler extensions . The upper right shows two Grid actors , called FileFetcher and FileStager , respectively . These actors make use of GridFTP [ Pro00 ] to retrieve ﬁles from , or put ﬁles to , remote locations on the Grid . The GlobusJob actor below is another Grid actor , in this case for running a Globus job [ Glo ] . At the bottom of Figure 6 a small workﬂow is shown that takes a Globus proxy and some input ﬁles , staging the ﬁles to where the job is run , then fetching the results from the remote location and displaying them on the client side . The green box speciﬁes that this workﬂow is executed using an SDF ( Synchronous Data - Flow ) director . This director analyzes the dataﬂow depen - dencies and token consumption and production rates of actors ( here : token = ﬁle ) , and schedules the exe - cution of actors accordingly . On the right , a number of actors that use the SDSC Storage Resource Broker [ SRB ] are shown , e . g . , to connect and disconnect from SRB and to get and put ﬁles from and to SRB space , respectively . We are currently in the process of providing all commonly used SRB commands as actors . This will allow the Kepler user to design and execute Grid workﬂows involving a number of diﬀerent tools , e . g . , SRB for data handling aspects , and Globus , Nimrod and other tools for computational aspects and job scheduling . In the center and left of Figure 6 , various other Kepler actors are shown : The CommandLine ac - tor can be used to incorporate any application into a workﬂow , provided it can be accessed from the com - mand line . 8 The “ $ ” icon is reminiscent of a shell prompt . The actor is parameterized with the argu - ments of the shell command , making it easy to cre - ate generic or specialized command line invocations . A Browser actor is shown directly below ( cf . Sec - tion 2 . 1 . 2 ) . It takes as input an HTML ﬁle or URL and displays it in the user’s default browser . This makes the actor an ideal output device for displaying intermediate or ﬁnal workﬂow results in ways that are well - known to users . Another extremely useful appli - cation of this actor is as an input device for user in - teractions . The result ﬁle of an upstream actor might have been transformed to an HTML ﬁle ( e . g . , using the xslt actor ) and augmented with HTML forms , check boxes , or other input forms that are displayable to the user in a standard web browser . Upon execut - ing the desired user interaction , an http - post re - quest is sent to a special Kepler web server , acting as a listener , and from there the workﬂow is resumed . The Email actor in the center of the ﬁgure pro - vides a simple notiﬁcation mechanism to inform the user of speciﬁc situations in the workﬂow . Together , the Email and Browser actors address core issues of requirement ( R6 ) in Section 2 . 2 . The Pause ac - tor ( red down - triangle ) pauses workﬂow execution at speciﬁc points , allowing the user to inspect intermedi - ate results , possibly changing parameter values , and resuming the workﬂow subsequently ( addressing ( R7 ) in Section 2 . 2 ) . Finally , actors for accessing real - time data streams from ROADNet sensor networks [ ROA ] have recently been added . These actors ( e . g . , OrbWaveform - Source ) can be integrated easily into Kepler , since many of the underlying Ptolemy ii directors support streaming execution . 9 3 . 3 Actor - Oriented Modeling Arguably the most unique feature of Kepler comes from the underlying Ptolemy ii system : “The focus [ of the Ptolemy project ] is on assem - bly of concurrent components . The key underly - ing principle . . . is the use of well - deﬁned mod - 8 E . g . , Kepler workﬂows can include data analysis steps via calls to R [ R ] . 9 This should come as no surprise , since dataﬂow process networks are deﬁned on token streams in the ﬁrst place . 3 HIGHLIGHTS OF KEPLER 11 els of computation that govern the interac - tion between components . ” 10 This focus together with the actor - oriented modeling paradigm make Ptolemy ii an ideal starting point for tackling the breadth of challenges in scientiﬁc workﬂow design and execution . In Ptolemy , a system or model thereof ( in our case , a scientiﬁc workﬂow ) is viewed as a composition of independent components called actors . Communication betweem actors hap - pens through interfaces called ports . We distinguish between input ports and output ports . In addition to the ports , actors have parameters , which conﬁgure and customize the behavior . 11 For example , a generic ﬁlter actor might consume a stream of input tokens via an input port , letting through to the output port only those tokens that satisfy a condition speciﬁed by a parameter . produceractor consumeractor IO - ports receiver Director Figure 7 : The semantics of component interaction is determined by a director , which controls execution and supplies the objects ( called receivers ) that im - plement communication . Actors , or more precisely their ports , are connected to one another via channels . Given an interconnec - tion of actors , however , there are many possible ex - ecution semantics that one could assign to the di - agram . For example , actors might have their own thread of control , or their execution might be trig - gered by the availability of new inputs . A key property of Ptolemy ii is that the execu - tion semantics is speciﬁed in the diagram by an object called a director ( see Figure 7 ) . The director deﬁnes how actors are executed and how they communicate with one another . Consequently , the execution model is less an emergent side - eﬀect of the various intercon - nected actors and their ( possibly ad - hoc ) orchestra - tion , and more a prescribed concurrent semantics as one might ﬁnd in a well - deﬁned concurrent program - ming language . The execution model deﬁned by the director is called the model of computation . Patterns of concurrent interaction are factored out into the de - sign of the directors , rather than being individually 10 http : / / ptolemy . eecs . berkeley . edu / objectives . htm . 11 Parameters are usually not shown in the ﬁgures . constructed by the designer of the workﬂow . Figure 7 depicts a producer and a consumer actor whose ports are connected by a unidirectional channel . The dia - gram is annotated by a director , which might , for ex - ample , execute the producer prior to the consumer so as to respect data precedences . The communication between the actors is mediated by an object called a receiver , which is provided by the director , not by the actors . Thus , for example , whether the communica - tion is buﬀered or synchronous is determined by the designer of the director , not by the designer of the actor . This hugely improves the reusability of actor designs . Process Networks . The Process Network ( PN ) director is a popular choice for designers of scien - tiﬁc workﬂows . It gives a diagram the semantics of ( dataﬂow ) process networks [ KM77 , LP95 ] . In this semantics , actors are independent processes that ex - ecute concurrently , each with its own thread of con - trol , and communicate by sending tokens through unidirectional channels with ( in principle ) unbounded buﬀering capacity . Writing to a channel is a non - blocking operation , while reading from a channel can block until suﬃcient input data are available . This model of computation is similar to that provided by Unix pipes , as in the following example of a Unix command - line composition of processes : cat foo . txt | bar | baz This example shows three independently executing processes ( cat , bar , and baz ) that are connected to one another through unidirectional pipes . The stream of tokens ﬂowing between the processes also synchronizes them if necessary . For example if bar and baz are ﬁlter operations working on a single line of text at a time ( e . g . , grep xyz ) , then a Unix process executing bar will block until a line of text is provided by the process executing cat foo . txt . Unlike Unix pipes , however , the PN director in Ptolemy ii tolerates feedback loops and forking and merging of data streams . It performs deadlock detec - tion , and manages buﬀers to keep memory require - ments bounded ( if possible ) . The PN director is only one example of a large number directors available in Ptolemy ii . There is also , for example , the SDF ( Synchronous Data - Flow ) director , which can be used for specialized process networks with ﬁxed token production and consump - tion rates per ﬁring ( see below ) . The SDF director performs static analysis on a workﬂow that guaran - tees absence of deadlocks , determines required buﬀer sizes , and optimizes the scheduling of actor execution . Other directors have been constructed for modeling 3 HIGHLIGHTS OF KEPLER 12 Discrete Event systems ( DE ) , Continuous - Time mod - els ( CT , which solve ordinary diﬀerential equations ) , and Communication Sequential Processes ( CSP ) , to mention just a few [ BLL + 04b ] . By relieving actors from the details of component interaction , the actors themselves become much more reusable ( cf . ( R2 ) in Section 2 . 2 ) . The behavior of an actor adapts to the execution and communication semantics provided by the director . This feature of actor - oriented modeling is called behavioral polymor - phism . For example , a single Ptolemy ii actor im - plementation of an arithmetic operation , say Plus , can be connected to any number of input operands and reused within diﬀerent models of computation and under the control of diﬀerent directors . An SDF director , e . g . , schedules the actor invocation ( or “ﬁr - ing” ) as soon as all inputs have data , which it knows since actors declare their ﬁxed token consumption and production rates in the SDF domain . In con - trast , when the Plus actor is governed by a DE direc - tor , additions happen when any input has data , cor - responding to the diﬀerent overall execution model in the Discrete Event domain . In addition to be - havioral polymorphism , the Ptolemy ii type system also supports data polymorphism , again increasing the reusability of actors . For example , our Plus ac - tor can be implemented in such a way that it dynam - ically chooses the correct numeric addition ( integer , ﬂoat , double , complex ) , depending on the types of in - puts it receives . Moreover , on other data types , e . g . , strings , vectors , matrices , or user - deﬁned types , the Plus actor 12 can execute appropriate actions , e . g . , string concatenation , vector or matrix addition , etc . Actor - Oriented Programming Interface . Actor - oriented modeling addresses several challenges in the design of complex systems [ EJL + 03 ] . We have already mentioned improved component reusability due to behavioral and data polymorphism . Another aspect is hierarchical modeling . As illustrated by the examples in Section 2 . 1 , subworkﬂows can be abstracted into ( composite ) actors themselves ( e . g . , see the Classifier actor / subworkﬂow in Figure 3 ) and thus arbitrarily nested . In the following , we give a simpliﬁed introduction on some implementation aspects of Ptolemy ii ’s actor - oriented approach . These can be adapted to the context of scientiﬁc workﬂows and distributed , service - oriented environ - ments , leading to a more structured approach to service composition and workﬂow design . The structure we propose is based on various phases and methods in Ptolemy ii ’s actor - oriented 12 This actor is called AddSubtract in Ptolemy ii . execution −→ preinitialize , type - check , run * , wrapup run −→ initialize , iteration * iteration −→ preﬁre , ﬁre * , postﬁre Figure 8 : AOPI execution phases and actor methods . programming interface ( AOPI ) , see Figure 8 . These AOPI methods are used by a director to orchestrate overall execution . Symbols in boldface denote actual methods that actor implementations have to provide ; the remaining symbols describe other phases 13 of the overall execution . When a director starts a workﬂow execution , it in - vokes the preinitialize method of all actors . Since this method is invoked only once per lifetime of an execution ( even if there are multiple runs ) , and prior to all other activities , this is a good time to put in place the receiver components of actors , and for ac - tors to “advertise” their supported port data types , transport protocols , etc . Next the director type - check s all connections and ports . This includes checking each port’s data types , all ( previously advertised ) type constraints , and the validity of port types being connected through chan - nels . A type inference algorithm is used to deter - mine the most general types satisfying the given con - straints . For scientiﬁc workﬂows , we can modify di - rectors to also type - check which transport protocol to use , or to check whether producer and consumer actors exchange data directly or via handles : 14 For example , if an actor A declares its output port to be of handle type “ http | ftp ” and a connected actor B declares its input port to be of handle type “ http ” , then type - checking can establish that the connection is valid , provided A ’s output port is subtyped to use http handles only . Indeed such information can and should be passed to the actor with the invocation of the initialize method . Other possible actions during execution of initial - ize are : Web service actors can “ping” the web ser - vices they represent and signal failure - to - initialize if the corresponding service is not alive . A “fail - over - aware” director can use this information to replace the defective web service with an equivalent one that is alive ( see ( R5 ) in Section 2 . 2 ) . A workﬂow ex - ecution will often consist only of one run , but if a workﬂow is re - run , initialize is called again . A run usually includes multiple iteration s , each of which in - cludes a call to preﬁre , ﬁre ( possibly called repeat - 13 Some correspond to methods of other Ptolemy ii entities , e . g . , director methods or manager methods [ BLL + 04b ] . 14 By handle we mean a unique identiﬁer that can also be used to retrieve data , e . g . , a URL . 4 RESEARCH ISSUES 13 edly by some special directors ) , and a call to postﬁre . The main actor operation ﬁnally happens in the ﬁre method , e . g . , a web service actor will make the actual remote service call here . Towards Actor - Oriented Scientiﬁc Workﬂows . The idea of actor - oriented scientiﬁc workﬂows is to apply the principles of actor - orientation and hierar - chical modeling , underlying the Ptolemy approach [ EJL + 03 , BLL + 04b ] , to the modeling and design of scientiﬁc workﬂows . In particular , web service op - erations , which provide the building blocks of many loosely coupled workﬂows , should be structured into diﬀerent parts , corresponding to the diﬀerent phases and methods used in actor - oriented modeling . For example to implement a web service w A , the service developer should think of speciﬁc web service opera - tions such as w A . initialize and w A . preﬁre in addition to the main “worker” method w A . ﬁre . As in the case of Ptolemy actors , this will lead to more generic and reusable components and even facilitate more com - plex extensions such as stateful web services . 15 4 Research Issues In this section we brieﬂy discuss some technical issues that we have begun addressing for Kepler , but that are less mature and require some additional research . 4 . 1 Higher - Order Constructs The early implementation of the Promoter Identiﬁ - cation Workﬂow ( PIW ) depicted in Figure 2 demon - strated the feasibility and some advantages of im - plementing scientiﬁc workﬂows in the Kepler ex - tension of Ptolemy ii [ ABB + 03 ] . However , it also highlighted some inherent challenges of the dataﬂow - oriented programming paradigm [ LA03 ] . We have argued in Section 2 . 3 that many current scientiﬁc workﬂow systems are more dataﬂow - oriented than business workﬂow systems and approaches , which tend to emphasize event - based control - ﬂow rather than dataﬂow . When designing real - world scientiﬁc workﬂows it is necessary , however , to handle com - plex control - ﬂows within a dataﬂow - oriented setting as well . It is well - known that control - ﬂow constructs require some thought in order to handle them prop - erly . The fairly intricate network topology in Figure 2 includes backward - directed “dataﬂow” channels , hav - ing the sole purpose of sending control tokens that 15 Statefulness is an established concept in actor - oriented modeling and dataﬂow networks ; e . g . , it can be represented explicitly via feedback loops . (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:7)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:1)(cid:23)(cid:24)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:19)(cid:21)(cid:22)(cid:1)(cid:24)(cid:27)(cid:24)(cid:31) ! (cid:29) " # (cid:16)(cid:17)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19) { “CAGT…AATATGAC " , “GGGGA…CAAAGA“ } Figure 9 : PIW variant with map iterator . initiate another iteration of a subworkﬂow . While such complicated structures achieve the desired eﬀect ( here , a special kind of loop ) , they are hard to under - stand , design , and maintain . Such ad - hoc construc - tions also increase the complexity of workﬂow design while diminishing the overall reusability of workﬂow components ( see ( R2 ) in Section 2 . 2 ) . Fortunately , there are better ways to incorporate structured con - trol into a dataﬂow - oriented system , thereby directly supporting workﬂow design as required by ( R2 ) . In [ LA03 ] we have illustrated how higher - order functional programming constructs can be used to improve the design of PIW . In particular , the higher - order function map : : ( α → β ) → [ α ] → [ β ] has proven to be very useful to implement a certain type of iteration . It takes a function f ( from α to β ) and a list of elements of type α , and applies f to each list element , returning the list of result elements ( each of type β ) . Thus map is deﬁned as map f [ x 1 , x 2 , . . . , x n ] = [ f ( x 1 ) , f ( x 2 ) , . . . , f ( x n ) ] For example , map f [ 1 , 2 , 3 ] = [ 1 , 4 , 9 ] for f ( x ) = x 2 . Figure 9 shows an improved version of the PIW workﬂow from Section 2 . 1 . 1 and Figure 2 , now using the higher - order map function . Note how backward - directed ﬂows of control - tokens are avoided . Instead , iterations are realized as nested subworkﬂows inside a higher - order Map actor . For example , to imple - ment a look - up of a list of gene sequences via a GenBank web service that can only accept one gene at a time , we simply create the higher - order con - struct Map ( GenBankWS ) as shown in Figure 9 ( the “stack” icon indicates that the contained work - ﬂow is applied multiple times ) . Other higher - order functional programming con - structs , e . g . , foldr ( for “fold right” ) can be similarly used to provide more abstract and modular iteration and control constructs in a dataﬂow setting , and we plan to add those to Kepler in the future . The 4 RESEARCH ISSUES 14 utility of declarative functional programming meth - ods for dataﬂow - oriented systems is no coincidence ; see , e . g . , [ Ree95 ] for more on the close links between dataﬂow , functional , and visual programming , and [ NA01 ] for interesting applications in implicit paral - lel programming . Here we only give a simple illus - tration using a core subworkﬂow of PIW in a Haskell speciﬁcation ; see [ LA03 ] for details : d0 = $ Gid % input : some gene - id d1 = genBankG in % get its gene sequence d2 = blastP d1 % ﬁnd candidates from similar seqs d3 = map genBankP d2 % get promoter sequences d4 = map promoterRegion d3 % compute regions d5 = map transfac d4 % compute transcr . factor sites d6 = zip d2 d4 % create list of ( promoter - id , region ) pairs d7 = map gpr2str d6 % accumulate into string list d8 = concat d7 % create a single ﬁle d9 = putStr d8 % output to subsequent steps The input and output ( ports ) of this workﬂow are given by d0 and d9 , respectively . Note the use of map to iterate over lists where the available services ( e . g . genBankP ) can only handle one item at a time . Also note that these ten equations establish a sim - ple forward - only dataﬂow process network with the d i representing named channels , and the expressions on the right of the equation representing processes ( i . e . , actors ) . A merge of two parallel branches hap - pens , e . g . , through the function zip that creates a single stream of pairs ( promoter - id , promoter - region ) in channel d6 from the two streams in d2 and d4 . 4 . 2 Third Party Transfers Scientiﬁc workﬂows can involve large volumes of data ( see ( R3 ) in Section 2 . 2 ) . In a web service setting , this creates a problem since so - called 3rd party transfers are not currently supported by web services : Let us consider two web services w A and w B , located at two sites s 1 and s 2 , respectively . w A takes some input x and produces some data d that we would like to pass on to w B , which produces the ﬁnal output data y . We can depict this as follows : x → w A @ s 1 d −→ w B @ s 2 y → Assume that the overall execution of this workﬂow WF is coordinated and controlled by a workﬂow en - gine E ( e . g . , Kepler ) running at some site s 3 . Cur - rent web service implementations do not allow the engine E to call w A @ s 1 , telling it to route d directly to w B @ s 2 . Instead , web service invocations and the input / output dataﬂows that go with them , all go through E @ s 3 . In pseudo - code this means : WF @ s 3 ( in x , out y ) = { d @ s 3 : = w A @ s 1 ( x @ s 3 ) ; y @ s 3 : = w B @ s 2 ( d @ s 3 ) } How do we execute the “remote assignments” shown here ? To execute WF @ s 3 , the workﬂow engine E ﬁrst sends a request message containing x to w A @ s 1 . Upon completion , w A replies back to E @ s 3 with the result d . Now WF @ s 3 can proceed and E forwards d to s 2 where w B can work on it . The ﬁnal result y is then sent from s 2 back to s 3 . This simple call / return execution is quite desirable from a modeling and design point of view since control - ﬂow and dataﬂow go hand in hand , and since the control engine E does not have to worry about the status of direct ( i . e . , 3rd party ) transfers of data d from w A to w B . The downside , however , is that data is moved around more often than necessary . Let us trace the “data shipments” of x , d , and y : 1 . ship x @ s 3 ; x @ s 1 % part of request to w A 2 . @ s 1 execute d : = w A ( x ) % execute w A 3 . ship d @ s 1 ; d @ s 3 % part of reply from w A 4 . ship d @ s 3 ; d @ s 2 % part of request to w B 5 . @ s 2 execute y : = w B ( d ) % execute w B 6 . ship y @ s 2 ; y @ s 1 % part of reply from w B If d is very large , executing both steps ( 3 ) and ( 4 ) is wasteful : ﬁrst d is sent from s 1 to s 3 where the work - ﬂow engine E runs , only to be sent to s 2 in the next step . Instead of sending d over the wire twice , the more direct 3rd party transfer w A @ s 1 d ; w B @ s 2 moves d only once , but as mentioned before , is not currently supported by web services . 16 The question becomes : How can we avoid unnecessary transfers and achieve the eﬃciency of 3rd party transfer , while retaining the above simple call / return execution model ? A Handle - Oriented Approach . A simple solu - tion to the above problem is that w A does not send the actual data d but a handle h d to it . Such a han - dle corresponds to a “logic pointer” and can be rep - resented by a globally unique URI , but may also be a URL and indicate the protocol by which d is to be accessed , e . g . , http , ftp , GridFTP00 , scp , or SRB . If we replace all data occurrences x , d , and y by handles h x , h d , and h y , respectively , we obtain : 1 . ship h x @ s 3 ; h x @ s 1 % request to w A 16 And even if it were , “divorces” control - ﬂow and dataﬂow , resulting in more complex execution models . 4 RESEARCH ISSUES 15 2 . @ s 1 execute h d : = w A ( h x ) % execute w A 3 . ship h d @ s 1 ; h d @ s 3 % reply from w A 4 . ship h d @ s 3 ; h d @ s 2 % request to w B 5 . @ s 2 execute h y : = w B ( h d ) % execute w B 6 . ship h y @ s 2 ; h y @ s 1 % reply from w B Now , instead of sending ( the possibly very large ) d over the wire twice in ( 3 ) and ( 4 ) , we only do so for the ( constant size ) handle h d . We cannot hope to further reduce this since a reply message from w A to E and a new request from E to w B are necessary for the overall control of workﬂow execution . In order to implement the above handle - solution , we need to slightly extend our web services : in steps ( 2 ) and ( 5 ) , w A and w B need to process handles by dereferencing them or by creating new ones . The for - mer happens when a web service acts as a consumer of data ( w A consumes x ) , while the latter is needed in the role of a data producer ( w A produces d ) . Consider , e . g . , the case where handles are rep - resented as URLs with http as the transport pro - tocol . In step ( 2 ) above , w A needs to dereference h x before it can execute its function . h x might be , e . g . , http : / / foobar . com / f17 . When dereferenced via http - get it yields the actual data x . 17 To prop - erly process handles as a data consumer , the op - eration “receive x ” has to be replaced by “receive h x ” , followed by a “dereference and get” operation x : = http - get ( h x ) . All subsequent read operations can then operate on x as before . In the role of a data producer , we have the re - verse situation . We want to avoid shipping of the actual result data d and instead send a handle h d . Thus , we need to ﬁrst create this handle , e . g . , by cre - ating a new ﬁle f18 that can be accessed via h d = http : / / baz . edu / f18 . All subsequent write access to d will proceed unchanged , provided the ﬁle name f18 is used for d . Finally , we need to replace “send d ” with “send h d ” . We are currently working on extensions of Kepler that make the system “handle - aware” [ Lud04 ] . For example , during the type - checking phase ( Figure 8 ) a handle - aware director could determine whether two web service actors A and B that invoke the web ser - vices w A and w B , respectively , support compatible handle types . For this to work seamlessly , web ser - vices themselves should oﬀer an actor - oriented pro - gramming interface as presented in Section 3 . 3 . 17 Note that while the handle h x is sent from s 3 to s 1 in step ( 1 ) , x might actually not reside at s 3 . 4 . 3 Other Research Issues Higher - order constructs and the handle - approach to 3rd party transfers are only two of a number of press - ing research issues in scientiﬁc workﬂows . 18 For ex - ample , detached execution ( R4 ) , reliability and fault - tolerance ( R5 ) , semantic links ( R8 ) , and data prove - nance ( R9 ) are all scientiﬁc workﬂow requirements that need further attention in the future . For exam - ple , [ BL04 ] presents some initial work on the use of ontologies as semantic types to help generate data transformation mappings between consecutive work - ﬂow steps . These kinds of semantic extensions can help at both levels , at the “plumbing” level to create data transformations as in [ BL04 ] , and at the design level to create more reusable components ( R2 ) and to support “smart” links in workﬂows ( R8 ) . 4 . 4 Related Work In Section 3 we have described some of the features of Kepler and the underlying Ptolemy ii system on which Kepler is based . Ptolemy ii aims at model - ing and design of heterogeneous , concurrent systems . In contrast , Kepler aims at the design and execution of scientiﬁc workﬂows . Consequently , Kepler ex - tensions to Ptolemy ii include numerous actors and capabilities that facilitate scientiﬁc workﬂows ( e . g . , web service actors and harvester , GridFTP , SRB and database actors , command - line and secure shell ac - tors , etc . ) Additional components are constantly added , e . g . , to support statistics packages ( such as R ) , GIS functionality ( e . g . , Grass and ArcIMS cou - plings ) , and other scientiﬁc data analysis and visual - ization capabilities [ WPS + 05 ] . The research and development on Kepler also beneﬁts from interactions and collaborations with other groups . On one hand , development is driven by application scientists , the ultimate “customers” of scientiﬁc workﬂow system , on the other hand , work in related projects also inﬂuences Kepler develop - ments . For example , Taverna [ TAV , OAF + 04 ] is a system that focuses on web service - based bioin - formatics workﬂows . In contrast , Triana [ TRI , CGH + 05 ] provides mechanisms for coupling work - ﬂows more tightly with Grid middleware tools . Cross - fertilization between these and other projects has happened , e . g . , through e - Science LINK - UP work - shops [ LIN04 ] , meetings and workshops at GGF [ GGF04 ] , etc . Other scientiﬁc workﬂow tools in - clude Pegasus [ DBG + 03 ] , Chimera , and job schedul - ing tools such as Condor / G [ DTL04 ] and Nimrod / G [ AGK00 ] . For a taxonomy of workﬂow management 18 Addressing ( R2 ) and ( R3 ) , respectively . REFERENCES 16 systems for Grid computing and a comparison of sys - tems see [ YB05 ] . Future work will address the various outstanding research issues and workﬂows require - ments that have not yet been ( fully ) met . For ex - ample , some projects contributing to Kepler plan to provide couplings to highly - interactive visualiza - tion tools such as SCIRun [ WPS + 05 ] and GeoVista [ TG02 ] . 5 Conclusions We have provided an overview of scientiﬁc workﬂow management issues , motivated by real - world exam - ples that we encountered in a number of application - oriented projects . The spectrum of what can be called a scientiﬁc workﬂow is wide and includes scien - tiﬁc discovery workﬂows ( e . g . , Section 2 . 1 . 1 ) , work - ﬂows that automate manual procedures or reengi - neer custom tools ( e . g . , Section 2 . 1 . 2 ) , and data and compute - intensive workﬂows ( e . g . , Section 2 . 1 . 3 ) . Scientiﬁc workﬂow support is needed for practically all information - oriented scientiﬁc disciplines , includ - ing bioinformatics , cheminformatics , ecoinformatics , geoinformatics , physics , etc . We identiﬁed a num - ber of common requirements and desiderata of sci - entiﬁc workﬂows ( Section 2 . 2 ) and contrasted them with business workﬂows . The Kepler system addresses many of the core requirements ( Section 3 ) and provides support for web service - based workﬂows and Grid extensions . The source code of Kepler is freely available [ KEP ] and a ﬁrst alpha - release has been distributed ear - lier this year . A unique feature of Kepler is in - herited from the underlying Ptolemy ii system : the actor - oriented modeling approach . This approach facilitates modeling and design of complex systems and thus provides also a very promising direction for pressing problems such as web service composi - tion and orchestration . The way data polymorphism and behavioral polymorphism are supported by an actor - oriented approach that “concentrates” compo - nent interaction in a separate director entity , can also shed light on other eﬀorts to create reusable compo - nent architectures such as CCA [ AGG + 99 ] . Areas of research include modeling issues such as the use of higher - order functional constructs for workﬂow de - sign ( Section 4 . 1 ) , and optimization issues such as the use of virtual data references ( handles ) to fa - cilitate data - intensive , web service - based workﬂows ( Section 4 . 2 ) . Acknowledgements . Kepler is an open source , cross - project collaboration that would not exist with - out the contributions of the many team mem - bers . We thank all current and past contributors to Ptolemy ii – the Kepler systems would not be possible without them . We also thank all Kepler members for their contributions , in particular , To - bin Fricke for implementing actors that access the wonderful world of ROADNet real - time data streams , Steve Neuendorﬀer and Christopher Brooks for shar - ing their insights into Ptolemy ii , Rod Spears for QBE facilities , Xiaowen Xin for many contributions including to the PIW workﬂow , Zhengang Cheng for providing some of the ﬁrst web service actors , Werner Krebs for EOL extensions , Steve Mock for Globus ac - tors , Shawn Bowers for his work on semantic types for Kepler , and last not least the many scientists and PIs that provide direct or indirect support to this eﬀort , among them Bill Michener , Chaitan Baru , Kim Baldrige , Mark Miller , Arie Shoshani , Terence Critchlow , and Mladen Vouk . References [ ABB + 03 ] I . Altintas , S . Bhagwanani , D . But - tler , S . Chandra , Z . Cheng , M . Cole - man , T . Critchlow , A . Gupta , W . Han , L . Liu , B . Ludäscher , C . Pu , R . Moore , A . Shoshani , and M . Vouk . A Model - ing and Execution Environment for Dis - tributed Scientiﬁc Workﬂows . In 15th Intl . Conf . on Scientiﬁc and Statistical Database Management ( SSDBM ) , Boston , Massachussets , 2003 . [ AGG + 99 ] R . Armstrong , D . Gannon , A . Geist , K . Keahey , S . Kohn , L . McInnes , S . Parker , and B . Smolinski . Toward a Common Component Architecture for High - Performance Scientiﬁc Computing . In 8th IEEE Intl . Symposium on High Per - formance Distributed Computation , Au - gust 1999 . [ AGK00 ] D . Abramson , J . Giddy , and L . Kotler . High Performance Parametric Modeling with Nimrod / G : Killer Application for the Global Grid . In Intl . Parallel and Dis - tributed Processing Symposium ( IPDPS ) , Cancun , Mexico , May 2000 . http : / / www . csse . monash . edu . au / ~ davida / nimrod / . [ AIL98 ] A . Ailamaki , Y . E . Ioannidis , and M . Livny . Scientiﬁc Workﬂow Manage - ment by Database Management . In 10th Intl . Conf . on Scientiﬁc and Statistical Database Management ( SSDBM ) , Capri , Italy , 1998 . [ AM97 ] G . Alonso and C . Mohan . Workﬂow Management Systems : The Next Gener - ation of Distributed Processing Tools . In REFERENCES 17 S . Jajodia and L . Kerschberg , editors , Ad - vanced Transaction Models and Architec - tures . 1997 . [ BDG03 ] J . Blythe , E . Deelman , and Y . Gil . Plan - ning for workﬂow construction and main - tenance on the Grid . In ICAPS [ ICA03 ] . [ BIR ] Biomedical Informatics Research Network Coordinating Center ( BIRN - CC ) , Univer - sity of California , San Diego . http : / / nbirn . net / . [ BL04 ] S . Bowers and B . Ludäscher . An Ontol - ogy Driven Framework for Data Trans - formation in Scientiﬁc Workﬂows . In In - ternational Workshop on Data Integration in the Life Sciences ( DILS ) , LNCS 2994 , Leipzig , Germany , March 2004 . . [ BL05 ] S . Bowers and B . Ludäscher . Actor - Oriented Design of Scientiﬁc Workﬂows . submitted for publication , 2005 . [ BLL04a ] S . Bowers , K . Lin , and B . Ludäscher . On Integrating Scientiﬁc Resources through Semantic Registration . In 16th Intl . Conf . on Scientiﬁc and Statistical Data - base Management ( SSDBM ) , Santorini Is - land , Greece , 2004 . [ BLL + 04b ] C . Brooks , E . A . Lee , X . Liu , S . Neuen - dorﬀer , Y . Zhao , and H . Zheng . Het - erogeneous Concurrent Modeling and De - sign in Java ( Volumes 1 - 3 ) . Technical re - port , Dept . of EECS , University of Califor - nia , Berkeley , 2004 . Technical Memoranda UCB / ERL M04 / 27 , M04 / 16 , M04 / 17 . [ CGH + 05 ] D . Churches , G . Gombas , A . Harrison , J . Maassen , C . Robinson , M . Shields , I . Taylor , and I . Wang . Programming Sci - entiﬁc and Distributed Workﬂow with Tri - ana Services . Concurrency and Computa - tion : Practice and Experience . Special Is - sue on Scientiﬁc Workﬂows , 2005 . [ CGK + 02 ] F . Curbera , Y . Goland , J . Klein , F . Ley - man , D . Roller , S . Thatte , and S . Weer - awarana . Business Process Execution Lan - guage for Web Services ( BPEL4WS ) , Ver - sion 1 . 0 , 2002 . http : / / www . ibm . com / developerworks / library / ws - bpel / . [ CM95 ] I . Chen and V . Markowitz . The Object - Protocol Model : Design , Implementation , and Scientiﬁc Applications . ACM Trans - actions on Information Systems , 20 ( 5 ) , 1995 . [ DBG + 03 ] E . Deelman , J . Blythe , Y . Gil , C . Kessel - man , G . Mehta , K . Vahi , K . Blackburn , A . Lazzarini , A . Arbree , R . Cavanaugh , and S . Koranda . Mapping Abstract Com - plex Workﬂows onto Grid Environments . Journal of Grid Computing , 1 ( 1 ) : 25 – 39 , 2003 . [ DTL04 ] T . T . Douglas Thain and M . Livny . Dis - tributed Computing in Practice : The Con - dor Experience . Concurrency and Compu - tation : Practice and Experience , 2004 . [ EJL + 03 ] J . Eker , J . W . Janneck , E . A . Lee , J . Liu , X . Liu , J . Ludvig , S . Neuendorﬀer , S . Sachs , and Y . Xiong . Taming Hetero - geneity – the Ptolemy Approach . In Pro - ceedings of the IEEE , volume 91 ( 1 ) , Janu - ary 2003 . [ eSc03 ] e - Science Workﬂow Services Workshop , e - Science Institute , Edinburgh , Scotland , December 2003 . http : / / www . nesc . ac . uk / esi / events / 303 / index . html . [ eSc04 ] e - Science Grid Environments Workshop , e - Science Institute , Edinburgh , Scotland , May 2004 . http : / / www . nesc . ac . uk / esi / events / . [ GEO ] NSF / ITR : GEON : A Research Project to Create Cyberinfrastructure for the Geo - sciences . www . geongrid . org . [ GGF04 ] Workﬂow in Grid Systems Workshop , GGF10 , Berlin , Germany , March 2004 . http : / / www . extreme . indiana . edu / groc / Worflow - call . html . [ Glo ] The Globus Alliance . www . globus . org . [ GRI04 ] GRIST Workshop on Service Composition for Data Exploration in the Virtual Obser - vatory , California Institute of Technology , July 2004 . http : / / grist . caltech . edu / sc4devo / . [ GT98 ] J . Gao and M . A . Thompson , editors . Combined Quantum Mechanical and Mole - cular Mechanical Methods . American Chemical Society , 1998 . [ Hal01 ] A . Halevy . Answering Queries Using Views : A Survey . VLDB Journal , 10 ( 4 ) : 270 – 294 , 2001 . [ ICA03 ] Proceedings of the ICAPS Workshop on Planning for Web Services , Trento , Italy , June 2003 . [ KEP ] Kepler : A System for Scientiﬁc Work - ﬂows . http : / / kepler - project . org . [ Kie02 ] B . Kiepuszewski . Expressiveness and Suit - ability of Languages for Control Flow Mod - elling in Workﬂows . PhD thesis , Queens - land University of Technology , 2002 . [ KM77 ] G . Kahn and D . B . MacQueen . Corou - tines and Networks of Parallel Processes . In B . Gilchrist , editor , Proc . of the IFIP Congress 77 , pp . 993 – 998 , 1977 . [ LA03 ] B . Ludäscher and I . Altintas . On Provid - ing Declarative Design and Programming Constructs for Scientiﬁc Workﬂows REFERENCES 18 based on Process Networks . Technical Report SciDAC - SPA - TN - 2003 - 01 , San Diego Supercomputer Center , 2003 . http : / / kbi . sdsc . edu / SciDAC - SDM / scidac - tn - map - constructs . pdf . [ LAG03 ] B . Ludäscher , I . Altintas , and A . Gupta . Compiling Abstract Scientiﬁc Work - ﬂows into Web Service Workﬂows . In 15th Intl . Conf . on Scientiﬁc and Statistical Database Management ( SS - DBM ) , Boston , Massachussets , 2003 . http : / / kbis . sdsc . edu / SciDAC - SDM / ludaescher - compiling . pdf . [ LGM03 ] B . Ludäscher , A . Gupta , and M . E . Mar - tone . A Model - Based Mediator System for Scientiﬁc Data Management . In Z . Lacroix and T . Critchlow , editors , Bioinformatics : Managing Scientiﬁc Data . Morgan Kauf - mann , 2003 . [ LIN04 ] LINK - Up Workshop on Scientiﬁc Work - ﬂows , San Diego Supercomputer Center , October 2004 . http : / / kbis . sdsc . edu / events / link - up - 11 - 04 / . [ LN04 ] B . Ludäscher and A . Nash . Web Ser - vice Composition Through Declarative Queries : The Case of Conjunctive Queries with Union and Negation . In 20th Intl . Conf . on Data Engineering ( ICDE ) , 2004 . [ LP95 ] E . A . Lee and T . Parks . Dataﬂow Process Networks . Proceedings of the IEEE , 83 ( 5 ) : 773 – 799 , May 1995 . http : / / citeseer . nj . nec . com / 455847 . html . [ LPH01 ] L . Liu , C . Pu , and W . Han . An XML - Enabled Data Extraction Tool for Web Sources . Intl . Journal of Information Sys - tems , Special Issue on Data Extraction , Cleaning , and Reconciliation , 2001 . [ Lud04 ] B . Ludäscher . Towards Actor - Oriented Web Service - Based Scientiﬁc Workﬂows ( or : How to Handle Handles ) . Technical report , San Diego Supercomputer Center , September 2004 . [ MBJ + 04 ] W . K . Michener , J . H . Beach , M . B . Jones , B . Ludäscher , D . D . Pennington , R . S . Pereira , A . Rajasekar , and M . Schildhauer . A Knowledge Environment for the Biodi - versity and Ecological Sciences . Journal of Intelligent Information Systems , 2004 . to appear . [ McP05 ] T . M . McPhillips . Pipelined scientiﬁc workﬂows for inferring evolutionary rela - tionships . Natural Diversity Discovery Project , 2005 . manuscript . [ Mor94 ] J . P . Morrison . Flow - Based Programming – A New Approach to Application Devel - opment . Van Nostrand Reinhold , 1994 . [ MVW96 ] J . Meidanis , G . Vossen , and M . Weske . Using Workﬂow Management in DNA Se - quencing . In Intl . Conf . on Cooperative Information Systems ( CoopIS ) , 1996 . [ NA01 ] R . S . Nikhil and Arvind . Implicit Parallel Programming in pH . Morgan Kaufmann , 2001 . [ NCB04 ] National Center for Biotechnology Infor - mation ( NCBI ) . http : / / www . ncbi . nlm . nih . gov / , 2004 . [ NL04 ] A . Nash and B . Ludäscher . Processing Unions of Conjunctive Queries with Nega - tion under Limited Access Patterns . In 9th Intl . Conf . on Extending Database Tech - nology ( EDBT ) , LNCS 2992 , pp . 422 – 440 , Heraklion , Crete , Greece , 2004 . [ OAF + 04 ] T . Oinn , M . Addis , J . Ferris , D . Mar - vin , M . Senger , M . Greenwood , T . Carver , K . Glover , M . R . Pocock , A . Wipat , and P . Li . Taverna : A tool for the com - position and enactment of bioinformat - ics workﬂows . Bioinformatics Journal , 20 ( 17 ) : 3045 – 3054 , 2004 . [ OWL03 ] OWL Web Ontology Language Reference , W3C Proposed Recommendation , Decem - ber 2003 . www . w3 . org / TR / owl - ref / . [ Pro00 ] G . Project . GridFTP – Universal Data Transfer for the Grid , 2000 . see http : / / www . globus . org / datagrid / gridftp . html . [ PTO04 ] Ptolemy II project and system . De - partment of EECS , UC Berkeley , 2004 . http : / / ptolemy . eecs . berkeley . edu / ptolemyII / . [ PYN + 03 ] L . Peterson , E . Yin , D . Nelson , I . Alt - intas , B . Ludäscher , T . Critchlow , A . J . Wyrobek , and M . A . Coleman . Mining the Frequency Distribution of Transcrip - tion Factor Binding Sites of Ionizing Ra - diation Responsive Genes . In New Hori - zons in Genomics , DOE / SC - 0071 , Santa Fe , New Mexico . , March 30 – April 1 2003 . [ R ] R – Statistical Data Analysis . http : / / www . r - project . org . [ Ree95 ] H . J . Reekie . Realtime Signal Process - ing : Dataﬂow , Visual , and Functional Programming . PhD thesis , School of Elec - trical Engineering , University of Technol - ogy , Sydney , 1995 . [ ROA ] ROADNet : Real - time Observatories , Ap - plications and Data management Network . roadnet . ucsd . edu . [ SBA + 04 ] W . Sudholt , K . Baldridge , D . Abram - son , C . Enticott , and S . Garic . Parame - ter Scan of an Eﬀective Group Diﬀerence REFERENCES 19 Pseudopotential Using Grid Computing . New Generation Computing , 22 : 137 – 146 , 2004 . [ SBB + 93 ] M . Schmidt , K . Baldridge , J . Boatz , S . El - bert , M . Gordon , J . Jensen , S . Koseki , N . Matsunaga , K . Nguyen , S . Su , T . Win - dus , M . Dupuis , and J . Montgomery . The General Atomic and Molecular Elec - tronic Structure System . Journal of Computational Chemistry , 14 : 1347 – 1363 , 1993 . cf . http : / / www . msg . ameslab . gov / GAMESS / GAMESS . html . [ SDM ] Scientiﬁc Data Management Cen - ter ( SDM ) . http : / / sdm . lbl . gov / sdmcenter / , see also http : / / www . npaci . edu / online / v5 . 17 / scidac . html . [ SDM03 ] Scientiﬁc Data Management Framework Workshop , Argonne National Labs , Au - gust 2003 . http : / / sdm . lbl . gov / ~ arie / sdm / SDM . Framework . wshp . htm . [ SEE ] NSF / ITR : Enabling the Science Environ - ment for Ecological Knowledge ( SEEK ) . seek . ecoinformatics . org . [ She98 ] A . Sheth . Changing Focus on Interoper - ability in Information Systems : From Sys - tem , Syntax , Structure to Semantics . In M . Goodchild , M . Egenhofer , R . Fegeas , and C . Kottman , editors , Interoperating Geographic Information Systems , pp . 5 – 30 . Kluwer , 1998 . [ SRB ] SDSC Storage Resource Broker . http : / / www . npaci . edu / DICE / SRB / . [ TAV ] The Taverna Project . http : / / taverna . sf . net / . [ TG02 ] M . Takatuska and M . Gahegan . Geo - VISTA Studio : A codeless visual program - ming environment for geoscientiﬁc data analysis and visualization . Computers and Geosciences , 28 ( 2 ) : 1131 – 1144 , 2002 . [ TRI ] The Triana Project . http : / / www . trianacode . org / . [ vdA03 ] W . van der Aalst . Don’t go with the ﬂow : Web services composition standards exposed . IEEE Intelligent Systems . Web Services – Been there done that ? Trends & Controversies , Jan / Feb 2003 . http : / / tmitwww . tm . tue . nl / research / patterns / download / ieeewebflow . pdf . [ vdAtHKB03 ] W . van der Aalst , A . ter Hofstede , B . Kie - puszewski , and A . Barros . Workﬂow Pat - terns . Distributed and Parallel Databases , 14 ( 3 ) : 5 – 51 , July 2003 . [ vdAvH02 ] W . van der Aalst and K . van Hee . Work - ﬂow Management : Models , Methods , and Systems ( Cooperative Information Sys - tems ) . MIT Press , 2002 . [ WBB96 ] H . Wright , K . Brodlie , and M . Brown . The Dataﬂow Visualization Pipeline as a Prob - lem Solving Environment . In M . Göbel , J . David , P . Slavik , and J . J . van Wijk , editors , Virtual Environments and Scien - tiﬁc Visualization , pp . 267 – 276 . Springer , 1996 . [ Wer01 ] T . Werner . Target gene identiﬁcation from expression array data by promoter analy - sis . Biomolecular Engineering , 17 : 87 – 94 , 2001 . [ WPS + 05 ] D . Weinstein , S . Parker , J . Simpson , K . Zimmerman , and G . Jones . Visualiza - tion in the SCIRun Problem - Solving En - vironment . In C . Hansen and C . Johnson , editors , Visualization Handbook , pp . 615 – 632 . Elsevier , 2005 . [ WSD03 ] Web Services Description Lan - guage ( WSDL ) Version 1 . 2 . http : / / www . w3 . org / TR / wsdl12 , June 2003 . [ YB05 ] J . Yu and R . Buyya . A Taxonomy of Workﬂow Management Systems for Grid Computing . Technical Report GRIDS - TR - 2005 - 1 , Grid Computing and Distributed Systems Labora - tory , University of Melbourne , 2005 . http : / / www . gridbus . org / reports / GridWorkflowTaxonomy . pdf . [ zM04 ] M . zur Muehlen . Workﬂow - based Process Controlling . Logos Verlag , Berlin , 2004 .