2012 © Copyright lies with the respective authors and their institutions . www . arcomem . eu ARCOMEM : ARchive COmunities MEMories Collaborative Project ( ICT - 2009 - 270239 ) Priority : FP7 - ICT - 2009 - 6 Challenge 4 – “Digital Libraries and Content” D7 . 2 Integration Prototype V1 Deliverable Co - ordinator : Florent Carpentier Deliverable Co - ordinating Institution : IMF Other Authors : Document Identifier : ARCOMEM / 2012 / D7 . 2 / v1 . 0 Date due : 30 / 04 / 12 Class Deliverable : ARCOMEM EU - ICT - 2009 - 270239 Submission date : Project start date : January 1 , 2011 Version : v1 . 0 Project duration : 3 years State : Final Distribution : Public , Restricted , Confidential Page 2 of 9 ARCOMEM Collaborative Project EU - ICT - 270239 ARCOMEM Consortium This document is a part of the ARCOMEM research project funded by the ICT Programme of the Commission of the European Communities by the grant number ICT - 2009 - 270239 . The following partners are involved in the project : The University of Sheffield ( USFD ) – Coordinator Department of Computer Science Regent Court 211 Portobello Sheffield , S1 4DP United Kingdom Contact person : Hamish Cunningham , Wim Peters E - mail address : arcomem - coord @ lists . dcs . shef . ac . uk Leibniz Universität Hannover ( LUH ) Forschungszentrum L3S Appelstrasse 9a 30169 Hannover Germany Contact person : Thomas Risse E - mail address : risse @ L3S . de Yahoo Iberia SLU ( YIS ) Avinguda Diagonal 177 , 8th floor , Barcelona , 08018 , CAT , Spain Contact person : Alejandro Jaimes E - mail address : ajaimes @ yahoo - inc . com Internet Memory Foundation ( EA ) 45 ter rue de la revolution 93100 Montreuil France Contact person : Julien Masanes E - mail address : julien @ internetmemory . org University of Southampton ( SOTON ) Room 4011 , Building 32 Highfield campus University of Southampton SO17 1BJ Contact person : Paul Lewis E - mail address : phl @ ecs . soton . ac . uk Athens Technology Center ( ATC ) 10 , Rizariou Street 15233 , Halandri Athens , Greece Contact person : Dimitris Spiliotopoulos E - mail address : d . spiliotopoulos @ atc . gr ATHENA Research and Innovation Center in Information Communication & Knowledge Technologies ( ATHENA ) National Technical University of Athens School of Electrical and Computer Engineering Division of Computer Science Iroon Polytechniou 9 Athens , 15780 Greece Contact person : Nectarios Koziris E - mail address : nkoziris @ imis . athena - innovation . gr Telecom ParisTech ( IT ) 46 rue Barrault 75634 Paris Cedex 13 France Contact person : Pierre Senellart E - mail address : pierre . senellart @ telecom - paristech . fr Deutsch Welle ( DW ) Neue Medien / Distribution Voltastr . 6 13355 Berlin , Germany Contact person : Birgit Gray E - mail address : birgit . gray @ dw - world . de SUDWESTRUNDFUNK ( SWR ) Hans - Bredow - Strasse , D - 76522 Baden - Baden Germany Contact person : Robert Fischer E - mail address : robert . fischer @ swr . de HELLENIC PARLIAMENT ( HEP ) Amalias 14 , 10557 , Athens Greece Contact person : Dimitris Koryzis E - mail address : dkoryzis @ parliament . gr PARLAMENTSDIREKTION ( AUP ) Dr . Karl Renner - Ring 3 A - 1017 Vienna Contact person : Guenther Schefbeck E - mail address : guenther . schefbeck @ parlament . gv . at D7 . 2 Integration Prototype V1 Page 3 of 9 2012 © Copyright lies with the respective authors and their institutions . Work package participants The following partners have taken an active part in the work leading to the elaboration of this document , even if they might not have directly contributed to the writing of this document or its parts : FIXME Partner 1 Partner 2 … Partner X Change Log Version Date Amended by Changes 0 . 1 20 - 04 - 2012 Florent Carpentier Initial draft 0 . 2 27 / 04 / 12 Florent Carpentier Added timeline Executive Summary This document describes which components were integrated in the first prototype of the ARCOMEM platform and how they communicate . It also looks at the first results that were obtained . Lastly , it sets out a plan for the next integration steps . Page 4 of 9 ARCOMEM Collaborative Project EU - ICT - 270239 Table of Contents D7 . 2 Integration Prototype V1 Page 5 of 9 2012 © Copyright lies with the respective authors and their institutions . 1 . Introduction 1 . 1 Overview The first prototype is focused on the online analysis . Different crawlers insert resources they fetched from the web into the document store . The insertion triggers the online analysis process , that results in more links being sent to the main crawler . On the crawler side , the general crawler ( Internet Memory ' s crawler or Athena ' s enhanced Heritrix ) crawls the URLs it is given . To start a crawl , some URLs must be manually added to its queue . In addition , the API crawler can be launched by hand and will insert the resources it fetched ( currently being developed ) into the document store and structured information into the triple store . The structured information depends on the specific system being queried : for instance , for a Twitter feed , we may extract user name , dates and tweets text . On the analysis side , the Application Aware module first analyses the whole resource ( along with the meta data the crawler produced : links on the document and information derived from the HTTP headers such as declared MIME type ) and enriches the resource and each of its links with extra information . The ` augmented ' information is used in two parallel branches : a simple regular expression - based black listing module ( link classifier ) and an NLP analysis . The latter is a sequence of language analysis with GATE ( tokenisation , lemmatisation of open - class words ( nouns , verbs , adjectives , adverbs ) , case - insensitive keyword matching , and named - entity recognition ) applied on the text close to each link found on the document , and a document scorer that uses this information to determine how relevant the entities are to the crawl specification . Lastly , the relevance scorer combines all the scores from the different analysis branches to give a final rating for each link and send it to the general crawler . Page 6 of 9 ARCOMEM Collaborative Project EU - ICT - 270239 Note : to simplify development , the general crawler can be replaced with a lightweight Java crawler simulator ( implemented by SOTON ) . 2 . Components interfaces IMF crawler / document store The crawler create WARC files and write them to a Hadoop file system . On the document store side , regularly , the files found in the dedicated directory get loaded into HBase and moved to a ` completed ' directory . ( This ensures the processing can be interrupted and resumed at any time , without reprocessing files . ) To facilitate the crawler design and for performance reasons , some extensions to the WARC format are understood by the importer : outlinks put in a specific record following the resource record are used to enrich the meta data for the resource . In addition , instead of storing the payload for a resource , a reference to an external file is also supported . Athena ' s Heritrix / document store Heritrix was modified to use the IMF HBase access library to write crawled resources directly to the store . Online analysis triggering The design relies on the HBase ' s region observer and a pooling mechanism . To let the region observer receive complete resources , Heritrix writes everything about a resource at once , and the WARC importer aggregates all the information related to one resource from possibly different ( and contiguous ) records and issues a single Put operation for the resource . HBase passes a copy of the Put object to a trigger that orchestrates all the online analysis work . This trigger manages a pool of tasks slot and a queue . A limited number of online analysis module runs take place at any single time , while the reference of the resource write operations are kept in a queue . Online analysis modules communication The trigger and all online analysis modules being written in Java , the interface of each module consists of a simple set of functions to call directly . In more details , when a slot becomes available , the resource is fetched from the table as an IMHBase Resource . This object ' s URL , content and out links are passed to the AAH . The AAH returns a list of links with a score and an augmented document . The structured objects extracted from the reource are then fed to independent analysis modules that run concurrently : a regular expression scorer and GATE NLP . The output of the NLP analysis is fed into a document scorer which gives a vector similarity between the crawl spec keywords and the document tokens . The link scores and the document score are fed to the relevance module which gives every link on the page a score to be sent to the crawler . D7 . 2 Integration Prototype V1 Page 7 of 9 2012 © Copyright lies with the respective authors and their institutions . Interface to the crawler ' s queue A JSON over HTTP - based protocol was devised . The main operation consists in updating the priority for a URL with a real number or the special keyword “blacklisted” to exclude once and for all a URL . Note that it is allowed to send different scores for the same URL : to simplify the design , the links the crawler discovers are always sent to the online analysis . The blacklisting option can be useful in the case of a URL being on a spam host black list . The design aims at maximum flexibility : any URL can be enqueued ( or the priority of any URL can be updated , if the URL was already known ) at any time . There is no constraint on the URLs : they need not come from pages the crawler fetched . IMF crawler A server listens to requests , parses them and enqueues the URLs ( disregarding the priority at present ) . Athena ' s priority aware Heritrix A server listens to requests , parses them and maintains an HBase priority table . The table is indexed on the URL . This table is used to reorder Heritrix ' s queues periodically . API crawler / triple store In addition to the Java API , the triple store offers a lightweight custom network protocol to let the API crawler ( written in Python ) store triples . Experiments Integration platform The integration platform consists of four servers with one Atom 330 processor , 3 . 5 GB of RAM and four 1 . 5 TiB hard discs . The HDFS and HBase instances run on a pair of servers . The triple store runs atop HBase . Each of the large scale crawlers ( Heritrix and IMF ' s crawler ) run on a single server . Results The first experiments highlighted the importance of regulating the concurrency of the online analysis tasks ( each resource being analysed independently from the others , the concurrency degree is not constrained ) . Using the IMF crawler and the two - server HBase cluster to store the collected data and run the on - line analysis , we crawled TODO resources in TODO time . TODO more about the conf . The bottleneck is TODO . Page 8 of 9 ARCOMEM Collaborative Project EU - ICT - 270239 Timeline • January 2012 : online analysis integration design , framework development started • February 2012 : ◦ Heritrix modified to reorder its queue according to URL priorities ◦ First demonstration : application aware helper , GATE , regular expression filter , relevance module integrated with the crawler simulator • March 2012 : ◦ IMF large scale crawler integration ( WARC ingestion ) ◦ API crawler writes to the triple store ◦ Web search interface mock - ups • April 2012 : ◦ IMF large scale crawler integration ( flow control ) ◦ Heritrix integration ( direct write to document store ) • May 2012 : ◦ prototype “gui1” ▪ basic crawl scheduler ▪ web interface to schedule a crawl and specify entities to focus the crawl ▪ API crawler • REST interface to receive fetch orders • storing resources in the document store ▪ IMF crawler • handling URL priorities • interface to control captures ▪ intelligent crawl specifications handling ( written to the triple store and read by the on - line analysis modules ) ◦ off - line processing design refinement • June 2012 ◦ 1 June : Rock - am - Ring standard crawl and ARCOMEM crawl ◦ study of partial retrieval or streaming from the triple store ◦ search interface first prototype ◦ indicator on crawl quality : GATE relevance off - line computation • September 2012 ◦ prototype “off - line processing v1” D7 . 2 Integration Prototype V1 Page 9 of 9 2012 © Copyright lies with the respective authors and their institutions . Next steps Two important parts must be added : the web user interface and the off - line analysis . The web user interface , along with some crawl scheduler will allow the users to start using the system . We are targeting a working prototype by mid - May , to be able to launch captures about the Rock - am - Ring festival in early June . The ARCOMEM crawl will be compared to a reference crawl on the same topic performed with ` traditional ' methods at the same time . The off - line analysis encompasses different tasks that may require being triggered at different times , and may have dependencies between them . Defining the criteria to enqueue off - line tasks in a scheduler that will respect dependencies , and selecting the tools to implement this scheduler will be a goal of the next integration meeting in May . Citations and References [ 1 ] Usage guide : https : / / arcomem - hudson . internetmemory . org / hudson / job / arcomem - framework / lastSuccessfulBuild / javadoc / OnlineAnalysis . html [ 2 ] General framework ( HBase trigger , on - line analysis scheduling and relevance scoring ) javadoc : https : / / arcomem - hudson . internetmemory . org / hudson / job / arcomem - framework / lastSuccessfulBuild / javadoc / apidocs / index . html [ 3 ] API crawler usage guide : https : / / github . com / netiru / apiblender [ 4 ] Application - aware helper javadoc : https : / / arcomem - hudson . internetmemory . org / hudson / job / aah / lastSuccessfulBuild / javadoc / apidocs / [ 5 ] Adaptive Heritrix javadoc : https : / / arcomem - hudson . internetmemory . org / hudson / job / adaptive - heritrix / lastSuccessfulBuild / javadoc / apidocs / [ 6 ] HBase specialised access layer documentation : https : / / service . europarchive . org / hudson / job / documentation / ws / docs / imhbase / html / index . html [ 7 ] Crawler control interface : https : / / service . europarchive . org / hudson / job / Bitbot / lastSuccessfulBuild / javadoc / overview - summary . html