A Modular Analysis of Provable Acceleration via Polyak’s Momentum : Training a Wide ReLU Network and a Deep Linear Network Jun - Kun Wang 1 Chi - Heng Lin 2 Jacob Abernethy 1 Abstract Incorporating a so - called “momentum” dynamic in gradient descent methods is widely used in neural net training as it has been broadly ob - served that , at least empirically , it often leads to signiﬁcantly faster convergence . At the same time , there are very few theoretical guarantees in the literature to explain this apparent accelera - tion effect . Even for the classical strongly convex quadratic problems , several existing results only show Polyak’s momentum has an accelerated lin - ear rate asymptotically . In this paper , we ﬁrst revisit the quadratic problems and show a non - asymptotic accelerated linear rate of Polyak’s mo - mentum . Then , we provably show that Polyak’s momentum achieves acceleration for training a one - layer wide ReLU network and a deep linear network , which are perhaps the two most popular canonical models for studying optimization and deep learning in the literature . Prior work ( Du et al . , 2019b ; Wu et al . , 2019c ) showed that using vanilla gradient descent , and with an use of over - parameterization , the error decays as ( 1 − Θ ( 1 κ (cid:48) ) ) t after t iterations , where κ (cid:48) is the condition num - ber of a Gram Matrix . Our result shows that with the appropriate choice of parameters Polyak’s mo - mentum has a rate of ( 1 − Θ ( 1 √ κ (cid:48) ) ) t . For the deep linear network , prior work ( Hu et al . , 2020b ) showed that vanilla gradient descent has a rate of ( 1 − Θ ( 1 κ ) ) t , where κ is the condition number of a data matrix . Our result shows an acceleration rate ( 1 − Θ ( 1 √ κ ) ) t is achievable by Polyak’s mo - mentum . This work establishes that momentum does indeed speed up neural net training . 1 School of Computer Science , Georgia Institute of Technol - ogy 2 School of Electrical and Computer Engineering , Georgia Institute of Technology . Correspondence to : Jun - Kun Wang < jimwang @ gatech . edu > , Chi - Heng Lin < cl3385 @ gatech . edu > , Jacob Abernethy < prof @ gatech . edu > . Proceedings of the 38 th International Conference on Machine Learning , PMLR 139 , 2021 . Copyright 2021 by the author ( s ) . 1 . Introduction Momentum methods are very popular for training neural net - works in various applications ( e . g . He et al . ( 2016 ) ; Vaswani et al . ( 2017 ) ; Krizhevsky et al . ( 2012 ) ) . It has been widely observed that the use of momentum helps faster training in deep learning ( e . g . Loshchilov & Hutter ( 2019 ) ; Wil - son et al . ( 2017 ) ; Cutkosky & Orabona ( 2019 ) ) . Among all the momentum methods , the most popular one seems to be Polyak’s momentum ( a . k . a . Heavy Ball momentum ) ( Polyak , 1964 ) , which is the default choice of momentum in PyTorch and Tensorﬂow . The success of Polyak’s mo - mentum in deep learning is widely appreciated and almost all of the recently developed adaptive gradient methods like Adam ( Kingma & Ba , 2015 ) , AMSGrad ( Reddi et al . , 2018 ) , and AdaBound ( Luo et al . , 2019 ) adopt the use of Polyak’s momentum , instead of Nesterov’s momentum . However , despite its popularity , little is known in theory about why Polyak’s momentum helps to accelerate training neural networks . Even for convex optimization , problems like strongly convex quadratic problems seem to be one of the few cases that discrete - time Polyak’s momentum method provably achieves faster convergence than standard gradient descent ( e . g . Lessard et al . ( 2016 ) ; Goh ( 2017 ) ; Ghadimi et al . ( 2015 ) ; Gitman et al . ( 2019 ) ; Loizou & Richt´arik ( 2017 ; 2018 ) ; Can et al . ( 2019 ) ; Scieur & Pe - dregosa ( 2020 ) ; Flammarion & Bach ( 2015 ) ; Wilson et al . ( 2021 ) ; Franca et al . ( 2020 ) ; Diakonikolas & Jordan ( 2019 ) ; Shi et al . ( 2018 ) ; Hu ( 2020 ) ) . On the other hand , the theo - retical guarantees of Adam , AMSGrad , or AdaBound are only worse if the momentum parameter β is non - zero and the guarantees deteriorate as the momentum parameter in - creases , which do not show any advantage of the use of momentum ( Alacaoglu et al . , 2020 ) . Moreover , the conver - gence rates that have been established for Polyak’s momen - tum in several related works ( Gadat et al . , 2016 ; Sun et al . , 2019 ; Yang et al . , 2018 ; Liu et al . , 2020c ; Mai & Johansson , 2020 ) do not improve upon those for vanilla gradient de - scent or vanilla SGD in the worst case . Lessard et al . ( 2016 ) ; Ghadimi et al . ( 2015 ) even show negative cases in convex optimization that the use of Polyak’s momentum results in divergence . Furthermore , Kidambi et al . ( 2018 ) construct a problem instance for which the momentum method under its a r X i v : 2010 . 01618v6 [ c s . L G ] 10 J un 2021 A Modular Analysis of Provable Acceleration via Polyak’s Momentum Algorithm 1 Gradient descent with Polyak’s momentum ( Polyak , 1964 ) ( Equivalent Version 1 ) 1 : Required : Step size η and momentum parameter β . 2 : Init : w 0 ∈ R d and M − 1 = 0 ∈ R d . 3 : for t = 0 to T do 4 : Given current iterate w t , obtain gradient ∇ (cid:96) ( w t ) . 5 : Update momentum M t = βM t − 1 + ∇ (cid:96) ( w t ) . 6 : Update iterate w t + 1 = w t − ηM t . 7 : end for optimal tuning is outperformed by other algorithms . Wang et al . ( 2020 ) show that Polyak’s momentum helps escape saddle points faster compared with the case without mo - mentum , which is the only provable advantage of Polyak’s momentum in non - convex optimization that we are aware of . A solid understanding of the empirical success of Polyak’s momentum in deep learning has eluded researchers for some time . We begin this paper by ﬁrst revisiting the use of Polyak’s mo - mentum for the class of strongly convex quadratic problems , min w ∈ R d 1 2 w (cid:62) Γ w + b (cid:62) w , ( 1 ) where Γ ∈ R d × d is a PSD matrix such that λ max ( Γ ) = α , λ min ( Γ ) = µ > 0 . This is one of the few 1 known examples that Polyak’s momentum has a provable globally acceler - ated linear rate in the discrete - time setting . Yet even for this class of problems existing results only establish an acceler - ated linear rate in an asymptotic sense and several of them do not have an explicit rate in the non - asymptotic regime ( e . g . Polyak ( 1964 ) ; Lessard et al . ( 2016 ) ; Mitliagkas ( 2019 ) ; Recht ( 2018 ) ) . Is it possible to prove a non - asymptotic accelerated linear rate in this case ? We will return to this question soon . For general µ - strongly convex , α - smooth , and twice dif - ferentiable functions ( not necessarily quadratic ) , denoted as F 2 µ , α , Theorem 9 in Polyak ( 1964 ) shows an asymp - totic accelerated linear rate when the iterate is sufﬁciently close to the minimizer so that the landscape can be well approximated by that of a quadratic function . However , the deﬁnition of the neighborhood was not very precise in the paper . In this work , we show a locally accelerated linear rate under a quantiﬁable deﬁnition of the neighborhood . Furthermore , we provably show that Polyak’s momentum helps to achieve a faster convergence for training two neural networks , compared to vanilla GD . The ﬁrst is training a one - layer ReLU network . Over the past few years there have appeared an enormous number of works considering training a one - layer ReLU network , provably showing con - 1 In Section 2 and Appendix A , we will provide more discus - sions about this point . Algorithm 2 Gradient descent with Polyak’s momentum ( Polyak , 1964 ) ( Equivalent Version 2 ) 1 : Required : step size η and momentum parameter β . 2 : Init : w 0 = w − 1 ∈ R d 3 : for t = 0 to T do 4 : Given current iterate w t , obtain gradient ∇ (cid:96) ( w t ) . 5 : Update iterate w t + 1 = w t − η ∇ (cid:96) ( w t ) + β ( w t − w t − 1 ) . 6 : end for vergence results for vanilla ( stochastic ) gradient descent ( e . g . Li & Liang ( 2018 ) ; Ji & Telgarsky ( 2020 ) ; Li & Yuan ( 2017 ) ; Du et al . ( 2019b ; a ) ; Allen - Zhu et al . ( 2019 ) ; Song & Yang ( 2019 ) ; Zou et al . ( 2019 ) ; Arora et al . ( 2019c ) ; Jacot et al . ( 2018 ) ; Lee et al . ( 2019 ) ; Chizat et al . ( 2019 ) ; Oymak & Soltanolkotabi ( 2019 ) ; Brutzkus & Globerson ( 2017 ) ; Chen et al . ( 2020a ) ; Tian ( 2017 ) ; Soltanolkotabi ( 2017 ) ; Bai & Lee ( 2020 ) ; Ghorbani et al . ( 2019 ) ; Li et al . ( 2020 ) ; Hanin & Nica ( 2020 ) ; Daniely ( 2017 ) ; Zou & Gu ( 2019 ) ; Dukler et al . ( 2020 ) ; Daniely ( 2020 ) ; Wei et al . ( 2019 ) ; Yehudai & Shamir ( 2020 ) ; Fang et al . ( 2019 ) ; Su & Yang ( 2019 ) ; Chen et al . ( 2020b ) ) , as well as for other algorithms ( e . g . Zhang et al . ( 2019 ) ; Wu et al . ( 2019b ) ; Cai et al . ( 2019 ) ; Zhong et al . ( 2017 ) ; Ge et al . ( 2019 ) ; van den Brand et al . ( 2020 ) ; Lee et al . ( 2020 ) ; Pilanci & Ergen ( 2020 ) ) . However , we are not aware of any theoretical works that study the momen - tum method in neural net training except the work Krichene et al . ( 2020 ) . These authors show that SGD with Polyak’s momentum ( a . k . a . stochastic Heavy Ball ) with inﬁnitesi - mal step size , i . e . η → 0 , for training a one - hidden - layer network with an inﬁnite number of neurons , i . e . m → ∞ , converges to a stationary solution . However , the theoretical result does not show a faster convergence by momentum . In this paper we consider the discrete - time setting and nets with ﬁnitely many neurons . We provide a non - asymptotic convergence rate of Polyak’s momentum , establishing a concrete improvement relative to the best - known rates for vanilla gradient descent . Our setting of training a ReLU network follows the same framework as previous results , including Du et al . ( 2019b ) ; Arora et al . ( 2019c ) ; Song & Yang ( 2019 ) . Speciﬁcally , we study training a one - hidden - layer ReLU neural net of the form , N ReLU W ( x ) : = 1 √ m m (cid:88) r = 1 a r σ ( (cid:104) w ( r ) , x (cid:105) ) , ( 2 ) where σ ( z ) : = z · 1 { z ≥ 0 } is the ReLU activation , w ( 1 ) , . . . , w ( m ) ∈ R d are the weights of m neurons on the ﬁrst layer , a 1 , . . . , a m ∈ R are weights on the second layer , and N ReLU W ( x ) ∈ R is the output predicted on input x . Assume n number of samples { x i ∈ R d } ni = 1 is given . Following Du et al . ( 2019b ) ; Arora et al . ( 2019c ) ; Song & A Modular Analysis of Provable Acceleration via Polyak’s Momentum Yang ( 2019 ) , we deﬁne a Gram matrix H ∈ R n × n for the weights W and its expectation ¯ H ∈ R n × n over the random draws of w ( r ) ∼ N ( 0 , I d ) ∈ R d whose ( i , j ) entries are deﬁned as follows , H ( W ) i , j = m (cid:88) r = 1 x (cid:62) i x j m 1 { (cid:104) w ( r ) , x i (cid:105) ≥ 0 & (cid:104) w ( r ) , x j (cid:105) ≥ 0 } ¯ H i , j : = E w ( r ) [ x (cid:62) i x j 1 { (cid:104) w ( r ) , x i (cid:105) ≥ 0 & (cid:104) w ( r ) , x j (cid:105) ≥ 0 } ] . ( 3 ) The matrix ¯ H is also called a neural tangent kernel ( NTK ) matrix in the literature ( e . g . Jacot et al . ( 2018 ) ; Yang ( 2019 ) ; Bietti & Mairal ( 2019 ) ) . Assume that the smallest eigen - value λ min ( ¯ H ) is strictly positive and certain conditions about the step size and the number of neurons are satisﬁed . Previous works ( Du et al . , 2019b ; Song & Yang , 2019 ) show a linear rate of vanilla gradient descent , while we show an accelerated linear rate 2 of gradient descent with Polyak’s momentum . As far as we are aware , our result is the ﬁrst acceleration result of training an over - parametrized ReLU network . The second result is training a deep linear network . The deep linear network is a canonical model for studying optimiza - tion and deep learning , and in particular for understanding gradient descent ( e . g . Shamir ( 2019 ) ; Saxe et al . ( 2014 ) ; Hu et al . ( 2020b ) ) , studying the optimization landscape ( e . g . Kawaguchi ( 2016 ) ; Laurent & von Brecht ( 2018 ) ) , and establishing the effect of implicit regularization ( e . g . Mo - roshko et al . ( 2020 ) ; Ji & Telgarsky ( 2019 ) ; Li et al . ( 2018 ) ; Razin & Cohen ( 2020 ) ; Arora et al . ( 2019b ) ; Gidel et al . ( 2019 ) ; Gunasekar et al . ( 2017 ) ; Lyu & Li ( 2020 ) ) . In this paper , following ( Du & Hu , 2019 ; Hu et al . , 2020b ) , we study training a L - layer linear network of the form , N L - linear W ( x ) : = 1 √ m L − 1 d y W ( L ) W ( L − 1 ) · · · W ( 1 ) x , ( 4 ) where W ( l ) ∈ R d l × d l − 1 is the weight matrix of the layer l ∈ [ L ] , and d 0 = d , d L = d y and d l = m for l (cid:54) = 1 , L . Therefore , except the ﬁrst layer W ( 1 ) ∈ R m × d and the last layer W ( L ) ∈ R d y × m , all the intermediate layers are m × m square matrices . The scaling 1 √ m L − 1 d y is neces - sary to ensure that the network’s output at the initialization N L - linear W 0 ( x ) has the same size as that of the input x , in the sense that E [ (cid:107)N L - linear W 0 ( x ) (cid:107) 2 ] = (cid:107) x (cid:107) 2 , where the expecta - tion is taken over some appropriate random initialization of the network ( see e . g . Du & Hu ( 2019 ) ; Hu et al . ( 2020b ) ) . Hu et al . ( 2020b ) show vanilla gradient descent with orthog - onal initialization converges linearly and the required width of the network m is independent of the depth L , while we 2 We borrow the term “accelerated linear rate” from the convex optimization literature ( Nesterov , 2013 ) , because the result here has a resemblance to those results in convex optimization , even though the neural network training is a non - convex problem . show an accelerated linear rate of Polyak’s momentum and the width m is also independent of L . To our knowledge , this is the ﬁrst acceleration result of training a deep linear network . A careful reader may be tempted by the following line of reasoning : a deep linear network ( without activation ) is effectively a simple linear model , and we already know that a linear model with the squared loss gives a quadratic objective for which Polyak’s momentum exhibits an accel - erated convergence rate . But this intuition , while natural , is not quite right : it is indeed nontrivial even to show that vanilla gradient descent provides a linear rate on deep linear networks ( Hu et al . , 2020b ; Du & Hu , 2019 ; Shamir , 2019 ; Arora et al . , 2019a ; Hardt & Ma , 2016 ; Wu et al . , 2019a ; Zou et al . , 2020 ) , as the optimization landscape is non - convex . Existing works show that under certain assumptions , all the local minimum are global ( Kawaguchi , 2016 ; Laurent & von Brecht , 2018 ; Yun et al . , 2018 ; Lu & Kawaguchi , 2017 ; Zhou & Liang , 2018 ; Hardt & Ma , 2016 ) . These results are not sufﬁcient to explain the linear convergence of mo - mentum , let alone the acceleration ; see Section H in the appendix for an empirical result . Similarly , it is known that under the NTK regime the output of the ReLU network trained by gradient descent can be approximated by a linear model ( e . g . Hu et al . ( 2020a ) ) . However , this result alone neither implies a global conver - gence of any algorithm nor characterizes the optimization landscape . While ( Liu et al . , 2020a ) attempt to derive an algorithm - independent equivalence of a class of linear mod - els and a family of wide networks , their result requires the activation function to be differentiable which does not hold for the most prevalent networks like ReLU . Also , their work heavily depends on the regularity of Hessian , making it hard to generalize beyond differentiable networks . Hence , while there has been some progress understanding training of wide networks through linear models , there remains a signiﬁcant gap in applying this to the momentum dynam - ics of a non - differentiable networks . Liu et al . ( 2020b ) establish an interesting connection between solving an over - parametrized non - linear system of equations and solving the classical linear system . They show that for smooth and twice differentiable activation , the optimization landscape of an over - parametrized network satisﬁes a ( non - convex ) no - tion called the Polyak - Lokasiewicz ( PL ) condition ( Polyak , 1963 ) , i . e . 12 (cid:107)∇ (cid:96) ( w ) (cid:107) 2 ≥ µ ( (cid:96) ( w ) − (cid:96) ( w ∗ ) ) , where w ∗ is a global minimizer and µ > 0 . It is not clear whether their result can be extended to ReLU activation , however , and the existing result of Danilova et al . ( 2018 ) for the discrete - time Polyak’s momentum under the PL condition does not give an accelerated rate nor is it better than that of vanilla GD . Aujol et al . ( 2020 ) show a variant of Polyak’s momen - tum method having an accelerated rate in a continuous - time limit for a problem that satisﬁes PL and has a unique global A Modular Analysis of Provable Acceleration via Polyak’s Momentum minimizer . It is unclear if their result is applicable to our problem . Therefore , showing the advantage of training the ReLU network and the deep linear network by using existing results of Polyak’s momentum can be difﬁcult . To summarize , our contributions in the present work include • In convex optimization , we show an accelerated lin - ear rate in the non - asymptotic sense for solving the class of the strongly convex quadratic problems via Polyak’s momentum ( Theorem 7 ) . We also provide an analysis of the accelerated local convergence for the class of functions in F 2 µ , α ( Theorem 8 ) . We establish a technical result ( Theorem 5 ) that helps to obtain these non - asymptotic rates . • In non - convex optimization , we show accelerated lin - ear rates of the discrete - time Polyak’s momentum for training an over - parametrized ReLU network and a deep linear network ( Theorems 9 and 10 ) . Furthermore , we will develop a modular analysis to show all the results in this paper . We identify conditions and propose a meta theorem of acceleration when the momentum method exhibits a certain dynamic , which can be of independent interest . We show that when applying Polyak’s momentum for these problems , the induced dynamics exhibit a form where we can directly apply our meta theorem . 2 . Preliminaries Throughout this paper , (cid:107) · (cid:107) F represents the Frobenius norm and (cid:107) · (cid:107) 2 represents the spectral norm of a matrix , while (cid:107) · (cid:107) represents l 2 norm of a vector . We also denote ⊗ the Kronecker product , σ max ( · ) = (cid:107) · (cid:107) 2 and σ min ( · ) the largest and the smallest singular value of a matrix respectively . For the case of training neural networks , we will consider minimizing the squared loss (cid:96) ( W ) : = 12 (cid:80) n i = 1 (cid:0) y i − N W ( x i ) (cid:1) 2 , ( 5 ) where x i ∈ R d is the feature vector , y i ∈ R d y is the label of sample i , and there are n number of samples . For training the ReLU network , we have N W ( · ) : = N ReLU W ( · ) , d y = 1 , and W : = { w ( r ) } mr = 1 , while for the deep linear network , we have N W ( · ) : = N L - linear W ( · ) , and W represents the set of all the weight matrices , i . e . W : = { W ( l ) } Ll = 1 . The notation A k represents the k th matrix power of A . 2 . 1 . Prior result of Polyak’s momentum Algorithm 1 and Algorithm 2 show two equivalent presenta - tions of gradient descent with Polyak’s momentum . Given the same initialization , one can show that Algorithm 1 and Algorithm 2 generate exactly the same iterates during opti - mization . Let us brieﬂy describe a prior acceleration result of Polyak’s momentum . The recursive dynamics of Poylak’s momentum for solving the strongly convex quadratic problems ( 1 ) can be written as (cid:20) w t + 1 − w ∗ w t − w ∗ (cid:21) = (cid:20) I d − η Γ + βI d − βI d I d 0 d (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) : = A · (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) , ( 6 ) where w ∗ is the unique minimizer . By a recursive expansion , one can get (cid:107) (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) (cid:107) ≤ (cid:107) A t (cid:107) 2 (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) . ( 7 ) Hence , it sufﬁces to control the spectral norm of the ma - trix power (cid:107) A t (cid:107) 2 for obtaining a convergence rate . In the literature , this is achieved by using Gelfand’s formula . Theorem 1 . ( Gelfand ( 1941 ) ; see also Foucart ( 2018 ) ) ( Gelfand’s formula ) Let A be a d × d matrix . Deﬁne the spec - tral radius ρ ( A ) : = max i ∈ [ d ] | λ i ( A ) | , where λ i ( · ) is the i th eigenvalue . Then , there exists a non - negative sequence { (cid:15) t } such that (cid:107) A t (cid:107) 2 = ( ρ ( A ) + (cid:15) t ) t and lim t →∞ (cid:15) t = 0 . We remark that there is a lack of the convergence rate of (cid:15) t in Gelfand’s formula in general . Denote κ : = α / µ the condition number . One can control the spectral radius ρ ( A ) as ρ ( A ) ≤ 1 − 2 √ κ + 1 by choosing η and β appropriately , which leads to the following result . Theorem 2 . ( Polyak ( 1964 ) ; see also Lessard et al . ( 2016 ) ; Recht ( 2018 ) ; Mitliagkas ( 2019 ) ) Gradient descent with Polyak’s momentum with the step size η = 4 ( √ µ + √ α ) 2 and the momentum parameter β = (cid:16) 1 − 2 √ κ + 1 (cid:17) 2 has (cid:107) (cid:20) w t + 1 − w ∗ w t − w ∗ (cid:21) (cid:107) ≤ (cid:18) 1 − 2 √ κ + 1 + (cid:15) t (cid:19) t + 1 (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) , where (cid:15) t is a non - negative sequence that goes to zero . That is , when t → ∞ , Polyak’s momentum has the ( 1 − 2 √ κ + 1 ) rate , which has a better dependency on the condition number κ than the 1 − Θ ( 1 κ ) rate of vanilla gradi - ent descent . A concern is that the bound is not quantiﬁable for a ﬁnite t . On the other hand , we are aware of a differ - ent analysis that leverages Chebyshev polynomials instead of Gelfand’s formula ( e . g . Liu & Belkin ( 2018 ) ) , which manages to obtain a t ( 1 − Θ ( 1 √ κ ) ) t convergence rate . So the accelerated linear rate is still obtained in an asymp - totic sense . Theorem 9 in Can et al . ( 2019 ) shows a rate max { ¯ C 1 , t ¯ C 2 } ( 1 − Θ ( 1 √ κ ) t ) for some constants ¯ C 1 and ¯ C 2 under the same choice of the momentum parameter and the step size as Theorem 2 . However , for a large t , the domi - nant term could be t ( 1 − Θ ( 1 √ κ ) t ) . In this paper , we aim at A Modular Analysis of Provable Acceleration via Polyak’s Momentum Figure 1 . Empirical risk (cid:96) ( W t ) vs . iteration t . Polyak’s mo - mentum accelerates the optimization process of training an over - parametrized one - layer ReLU network . Experimental details are available in Appendix H . obtaining a bound that ( I ) holds for a wide range of values of the parameters , ( II ) has a dependency on the squared root of the condition number √ κ , ( III ) is quantiﬁable in each iteration and is better than the rate t ( 1 − Θ ( 1 √ κ ) ) t . 2 . 2 . ( One - layer ReLU network ) Settings and Assumptions The ReLU activation is not differentiable at zero . So for solv - ing ( 5 ) , we will replace the notion of gradient in Algorithm 1 and 2 with subgradient ∂(cid:96) ( W t ) ∂w ( r ) t : = 1 √ m (cid:80) ni = 1 (cid:0) N W t ( x i ) − y i (cid:1) a r · 1 [ (cid:104) w ( r ) t , x i (cid:105) ≥ 0 ] x i and update the neuron r as w ( r ) t + 1 = w ( r ) t − η ∂(cid:96) ( W t ) ∂w ( r ) t + β (cid:0) w ( r ) t − w ( r ) t − 1 (cid:1) . As described in the introduction , we assume that the smallest eigenvalue of the Gram matrix ¯ H ∈ R n × n is strictly positive , i . e . λ min ( ¯ H ) > 0 . We will also denote the largest eigenvalue of the Gram matrix ¯ H as λ max ( ¯ H ) and denote the condition number of the Gram matrix as κ : = λ max ( ¯ H ) λ min ( ¯ H ) . Du et al . ( 2019b ) show that the strict positiveness assumption is in - deed mild . Speciﬁcally , they show that if no two inputs are parallel , then the least eigenvalue is strictly positive . Pani - grahi et al . ( 2020 ) were able to provide a quantitative lower bound under certain conditions . Following the same frame - work of Du et al . ( 2019b ) , we consider that each weight vector w ( r ) ∈ R d is initialized according to the normal distribution , i . e . w ( r ) ∼ N ( 0 , I d ) , and each a r ∈ R is sam - pled from the Rademacher distribution , i . e . a r = 1 with probability 0 . 5 ; and a r = − 1 with probability 0 . 5 . We also assume (cid:107) x i (cid:107) ≤ 1 for all samples i . As the previous works ( e . g . Li & Liang ( 2018 ) ; Ji & Telgarsky ( 2020 ) ; Du et al . ( 2019b ) ) , we consider only training the ﬁrst layer { w ( r ) } and the second layer { a r } is ﬁxed throughout the iterations . We will denote u t ∈ R n whose i th entry is the network’s prediction for sample i , i . e . u t [ i ] : = N ReLU W t ( x i ) in iteration t and denote y ∈ R n the vector whose i th element is the label of sample i . The following theorem is a prior result due to Du et al . ( 2019b ) . Theorem 3 . ( Theorem 4 . 1 in Du et al . ( 2019b ) ) Assume that λ : = λ min ( ¯ H ) / 2 > 0 and that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . Set the number of nodes m = Ω ( λ − 4 n 6 δ − 3 ) and the constant step size η = O ( λn 2 ) . Then , with probability at least 1 − δ over the random initialization , vanilla gradient descent , i . e . Algorithm 1 & 2 with β = 0 , has (cid:107) u t − y (cid:107) 2 ≤ ( 1 − ηλ ) t · (cid:107) u 0 − y (cid:107) 2 . Later Song & Yang ( 2019 ) improve the network size m to m = Ω ( λ − 4 n 4 log 3 ( n / δ ) ) . Wu et al . ( 2019c ) provide an improved analysis over Du et al . ( 2019b ) , which shows that the step size η of vanilla gradient descent can be set as η = 1 c 1 λ max ( ¯ H ) for some quantity c 1 > 0 . The result in turn leads to a convergence rate ( 1 − 1 c 2 κ ) for some quantity c 2 > 0 . However , the quantities c 1 and c 2 are not universal constants and actually depend on the problem parameters λ min ( ¯ H ) , n , and δ . A question that we will answer in this paper is “ Can Polyak’s momentum achieve an accelerated linear rate (cid:16) 1 − Θ ( 1 √ κ ) (cid:17) , where the factor Θ ( 1 √ κ ) does not depend on any other problem parameter ? ” . 2 . 3 . ( Deep Linear network ) Settings and Assumptions For the case of deep linear networks , we will denote X : = [ x 1 , . . . , x n ] ∈ R d × n the data matrix and Y : = [ y 1 , . . . , y n ] ∈ R d y × n the corresponding label matrix . We will also denote ¯ r : = rank ( X ) and the condition num - ber κ : = λ max ( X (cid:62) X ) λ ¯ r ( X (cid:62) X ) . Following Hu et al . ( 2020b ) , we will assume that the linear network is initialized by the orthogonal initialization , which is conducted by sam - pling uniformly from ( scaled ) orthogonal matrices such that ( W ( 1 ) 0 ) (cid:62) W ( 1 ) 0 = mI d , W ( L ) 0 ( W ( L ) 0 ) (cid:62) = mI d y , and ( W ( l ) 0 ) (cid:62) W ( l ) 0 = W ( l ) 0 ( W ( l ) 0 ) (cid:62) = mI m for layer 2 ≤ l ≤ L − 1 . We will denote W ( j : i ) : = W j W j − 1 · · · W i = Π j l = i W l , where 1 ≤ i ≤ j ≤ L and W ( i − 1 : i ) = I . We also denote the network’s output U : = 1 √ m L − 1 d y W ( L : 1 ) X ∈ R d y × n . In our analysis , following Du & Hu ( 2019 ) ; Hu et al . ( 2020b ) , we will further assume that ( A1 ) there exists a W ∗ such that Y = W ∗ X , X ∈ R d × ¯ r , and ¯ r = rank ( X ) , which is actually without loss of generality ( see e . g . the discussion in Appendix B of Du & Hu ( 2019 ) ) . Theorem 4 . ( Theorem 4 . 1 in Hu et al . ( 2020b ) ) As - sume ( A1 ) and the use of the orthogonal initialization . Suppose the width of the deep linear network satisﬁes m ≥ C (cid:107) X (cid:107) 2 F σ 2max ( X ) κ 2 (cid:0) d y ( 1 + (cid:107) W ∗ (cid:107) 22 ) + log ( ¯ r / δ ) (cid:1) and m ≥ max { d x , d y } for some δ ∈ ( 0 , 1 ) and a sufﬁciently large constant C > 0 . Set the constant step size η = d y 2 Lσ 2max ( X ) . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Then , with probability at least 1 − δ over the random initial - ization , vanilla gradient descent , i . e . Algorithm 1 & 2 with β = 0 , has (cid:107) U t − Y (cid:107) 2 F ≤ (cid:0) 1 − Θ ( 1 κ ) (cid:1) t · (cid:107) U 0 − Y (cid:107) 2 F . 3 . Modular Analysis In this section , we will provide a meta theorem for the following dynamics of the residual vector ξ t ∈ R n 0 , (cid:20) ξ t + 1 ξ t (cid:21) = (cid:20) I n 0 − ηH + βI n 0 − βI n 0 I n 0 0 n 0 (cid:21) (cid:20) ξ t ξ t − 1 (cid:21) + (cid:20) ϕ t 0 n 0 (cid:21) , ( 8 ) where η is the step size , β is the momentum parameter , H ∈ R n 0 × n 0 is a PSD matrix , ϕ t ∈ R n 0 is some vector , and I n 0 is the n 0 × n 0 - dimensional identity matrix . Note that ξ t and ϕ t depend on the underlying model learned at iteration t , i . e . depend on W t . We ﬁrst show that the residual dynamics of Polyak’s mo - mentum for solving all the four problems in this paper are in the form of ( 8 ) . The proof of the following lemmas ( Lemma 2 , 3 , and 4 ) are available in Appendix B . 3 . 1 . Realization : Strongly convex quadratic problems One can easily see that the dynamics of Polyak’s momentum ( 6 ) for solving the strongly convex quadratic problem ( 1 ) is in the form of ( 8 ) . We thus have the following lemma . Lemma 1 . Applying Algorithm 1 or Algorithm 2 to solving the class of strongly convex quadratic problems ( 1 ) induces a residual dynamics in the form of ( 8 ) , where ξ t = w t − w ∗ ( and hence n 0 = d ) , H = Γ , ϕ t = 0 d . 3 . 2 . Realization : Solving F 2 µ , α A similar result holds for optimizing functions in F 2 µ , α . Lemma 2 . Applying Algorithm 1 or Algorithm 2 to min - imizing a function f ( w ) ∈ F 2 µ , α induces a residual dy - namics in the form of ( 8 ) , where ξ t = w t − w ∗ , H = (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ , ϕ t = η (cid:0) (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ − (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w t + τw ∗ (cid:1) dτ (cid:1) ( w t − w ∗ ) , where w ∗ : = arg min w f ( w ) . 3 . 3 . Realization : One - layer ReLU network More notations : For the analysis , let us deﬁne the event A ir : = { ∃ w ∈ R d : (cid:107) w − w ( r ) 0 (cid:107) ≤ R ReLU , 1 { x (cid:62) i w ( r ) 0 } (cid:54) = 1 { x (cid:62) i w ≥ 0 } } , where R ReLU > 0 is a number to be de - termined later . The event A ir means that there exists a w ∈ R d which is within the R ReLU - ball centered at the ini - tial point w ( r ) 0 such that its activation pattern of sample i is different from that of w ( r ) 0 . We also denote a random set S i : = { r ∈ [ m ] : 1 { A ir } = 0 } and its complementary set S ⊥ i : = [ m ] \ S i . Lemma 3 below shows that training the ReLU network N - ReLU W ( · ) via momentum induces the residual dynamics in the form of ( 8 ) . Lemma 3 . ( Residual dynamics of training the ReLU net - work N ReLU W ( · ) ) Denote ( H t ) i , j : = H ( W t ) i , j = 1 m (cid:80) mr = 1 x (cid:62) i x j × 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . Applying Algorithm 1 or Algorithm 2 to ( 5 ) for training the ReLU network N ReLU W ( x ) induces a residual dynam - ics in the form of ( 8 ) such that ξ t [ i ] = N ReLU W t ( x i ) − y i ( and hence n 0 = d ) , H = H 0 , and ϕ t = φ t + ι t , where each element i of ξ t ∈ R n is the residual error of the sample i , and the i th - element of φ t ∈ R n satisﬁes | φ t [ i ] | ≤ 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) , and ι t = η ( H 0 − H t ) ξ t ∈ R n . 3 . 4 . Realization : Deep Linear network Lemma 4 below shows that the residual dynamics due to Polyak’s momentum for training the deep linear network is indeed in the form of ( 8 ) . In the lemma , “vec” stands for the vectorization of the underlying matrix in column - ﬁrst order . Lemma 4 . ( Residual dynamics of training N L - linear W ( · ) ) De - note M t , l the momentum term of layer l at iteration t , which is recursively deﬁned as M t , l = βM t , l − 1 + ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t . De - note H t : = 1 m L − 1 d y (cid:80) Ll = 1 [ ( W ( l − 1 : 1 ) t X ) (cid:62) ( W ( l − 1 : 1 ) t X ) ⊗ W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) ] ∈ R d y n × d y n . Applying Algorithm 1 or Algorithm 2 to ( 5 ) for training the deep linear network N L - linear W ( x ) induces a residual dy - namics in the form of ( 8 ) such that ξ t = vec ( U t − Y ) ∈ R d y n ( and hence n 0 = d y n ) , H = H 0 , and ϕ t = φ t + ψ t + ι t ∈ R d y n , where the vector φ t = 1 √ m L − 1 d y vec ( Φ t X ) with Φ t = Π l (cid:16) W ( l ) t − ηM t , l (cid:17) − W ( L : 1 ) t + η (cid:80) Ll = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) t , and the vector ψ t is ψ t = 1 √ m L − 1 d y vec (cid:0) ( L − 1 ) βW ( L : 1 ) t X + βW ( L : 1 ) t − 1 X − β (cid:80) Ll = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X (cid:1) , and ι t = η ( H 0 − H t ) ξ t . A Modular Analysis of Provable Acceleration via Polyak’s Momentum 3 . 5 . A key theorem of bounding a matrix - vector product Our meta theorem of acceleration will be based on Theo - rem 5 in the following , which upper - bounds the size of the matrix - vector product of a matrix power A k and a vector v 0 . Compared to Gelfand’s formula ( Theorem 1 ) , Theorem 5 be - low provides a better control of the size of the matrix - vector product , since it avoids the dependency on the unknown sequence { (cid:15) t } . The result can be of independent interest and might be useful for analyzing Polyak’s momentum for other problems in future research . Theorem 5 . Let A : = (cid:20) ( 1 + β ) I n − ηH − βI n I n 0 (cid:21) ∈ R 2 n × 2 n . Suppose that H ∈ R n × n is a positive semideﬁnite matrix . Fix a vector v 0 ∈ R n . If β is chosen to satisfy 1 ≥ β > max { (cid:16) 1 − (cid:112) ηλ min ( H ) (cid:17) 2 , (cid:16) 1 − (cid:112) ηλ max ( H ) (cid:17) 2 } , then (cid:107) A k v 0 (cid:107) ≤ (cid:0)(cid:112) β (cid:1) k C 0 (cid:107) v 0 (cid:107) , ( 9 ) where the constant C 0 : = √ 2 ( β + 1 ) (cid:112) min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } ≥ 1 , ( 10 ) and the function h ( β , z ) is deﬁned as h ( β , z ) : = − (cid:16) β − ( 1 − √ z ) 2 (cid:17) (cid:16) β − ( 1 + √ z ) 2 (cid:17) . Note that the constant C 0 in Theorem 5 depends on β and ηH . It should be written as C 0 ( β , ηH ) to be precise . How - ever , for the brevity , we will simply denote it as C 0 when the underlying choice of β and ηH is clear from the context . The proof of Theorem 5 is available in Appendix C . The - orem 5 allows us to derive a concrete upper bound of the residual errors in each iteration of momentum , and conse - quently allows us to show an accelerated linear rate in the non - asymptotic sense . The favorable property of the bound will also help to analyze Polyak’s momentum for training the neural networks . As shown later in this paper , we will need to guarantee the progress of Polyak’s momentum in each iteration , which is not possible if we only have a quan - tiﬁable bound in the limit . Based on Theorem 5 , we have the following corollary . The proof is in Appendix C . 1 . Corollary 1 . Assume that λ min ( H ) > 0 . Denote κ : = λ max ( H ) / λ min ( H ) . Set η = 1 / λ max ( H ) and set β = (cid:16) 1 − 12 (cid:112) ηλ min ( H ) (cid:17) 2 = (cid:16) 1 − 1 2 √ κ (cid:17) 2 . Then , C 0 ≤ 4 √ κ . 3 . 6 . Meta theorem Let λ > 0 be the smallest eigenvalue of the matrix H that appears on the residual dynamics ( 8 ) . Our goal is to show that the residual errors satisfy (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ s ξ s − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:0) √ β + 1 ϕ C 2 (cid:1) s ( C 0 + 1 ϕ C 1 ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) , ( 11 ) where C 0 is the constant deﬁned on ( 10 ) , and C 1 , C 2 ≥ 0 are some constants , 1 ϕ is an indicator if any ϕ t on the residual dynamics ( 8 ) is a non - zero vector . For the case of training the neural networks , we have 1 ϕ = 1 . Theorem 6 . ( Meta theorem for the residual dy - namics ( 8 ) ) Assume that the step size η and the momentum parameter β satisfying 1 ≥ β > max { (cid:16) 1 − (cid:112) ηλ min ( H ) (cid:17) 2 , (cid:16) 1 − (cid:112) ηλ max ( H ) (cid:17) 2 } , are set appropriately so that ( 11 ) holds at iteration s = 0 , 1 , . . . , t − 1 implies that (cid:107) (cid:80) t − 1 s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ≤ (cid:0) √ β + 1 ϕ C 2 (cid:1) t C 3 (cid:13)(cid:13)(cid:13) (cid:13) (cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13) (cid:13) . ( 12 ) Then , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:0) √ β + 1 ϕ C 2 (cid:1) t ( C 0 + 1 ϕ C 1 ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) , ( 13 ) holds for all t , where C 0 is deﬁned on ( 10 ) and C 1 , C 2 , C 3 ≥ 0 are some constants satisfying : (cid:0) √ β (cid:1) t C 0 + (cid:0) √ β + 1 ϕ C 2 (cid:1) t 1 ϕ C 3 ≤ (cid:0) √ β + 1 ϕ C 2 (cid:1) t ( C 0 + 1 ϕ C 1 ) . ( 14 ) Proof . The proof is by induction . At s = 0 , ( 11 ) holds since C 0 ≥ 1 by Theorem 5 . Now assume that the in - equality holds at s = 0 , 1 , . . . , t − 1 . Consider iteration t . Recursively expanding the dynamics ( 8 ) , we have (cid:20) ξ t ξ t − 1 (cid:21) = A t (cid:20) ξ 0 ξ − 1 (cid:21) + t − 1 (cid:88) s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) . ( 15 ) By Theorem 5 , the ﬁrst term on the r . h . s . of ( 15 ) can be bounded by (cid:107) A t (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ≤ (cid:16)(cid:112) β (cid:17) t C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( 16 ) By assumption , given ( 11 ) holds at s = 0 , 1 , . . . , t − 1 , we have ( 12 ) . Combining ( 12 ) , ( 14 ) , ( 15 ) , and ( 16 ) , we have ( 13 ) and hence the proof is completed . Remark : As shown in the proof , we need the residual errors be tightly bounded as ( 11 ) in each iteration . Theorem 5 is critical for establishing the desired result . On the other hand , it would become tricky if instead we use Gelfand’s formula or other techniques in the related works that lead to a convergence rate in the form of O ( tθ t ) . A Modular Analysis of Provable Acceleration via Polyak’s Momentum 4 . Main results The important lemmas and theorems in the previous section help to show our main results in the following subsections . The high - level idea to obtain the results is by using the meta theorem ( i . e . Theorem 6 ) . Speciﬁcally , we will need to show that if the underlying residual dynamics satisfy ( 11 ) for all the previous iterations , then the terms { ϕ s } in the dynamics satisfy ( 12 ) . This condition trivially holds for the case of the quadratic problems , since there is no such term . On the other hand , for solving the other problems , we need to carefully show that the condition holds . For example , according to Lemma 3 , showing acceleration for the ReLU network will require bounding terms like (cid:107) ( H 0 − H s ) ξ s (cid:107) ( and other terms as well ) , where H 0 − H s corresponds to the difference of the kernel matrix at two different time steps . By controlling the width of the network , we can guarantee that the change is not too much . A similar result can be obtained for the problem of the deep linear network . The high - level idea is simple but the analysis of the problems of the neural networks can be tedious . 4 . 1 . Non - asymptotic accelerated linear rate for solving strongly convex quadratic problems Theorem 7 . Assume the momentum parameter β satisﬁes 1 ≥ β > max { (cid:0) 1 − √ ηµ (cid:1) 2 , (cid:0) 1 − √ ηα (cid:1) 2 } . Gradient de - scent with Polyak’s momentum for solving ( 1 ) has (cid:107) (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) (cid:107) ≤ (cid:16)(cid:112) β (cid:17) t C 0 (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) , ( 17 ) where the constant C 0 is deﬁned as C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( Γ ) ) , h ( β , ηλ max ( Γ ) ) } ≥ 1 , ( 18 ) and h ( β , z ) = − (cid:16) β − ( 1 − √ z ) 2 (cid:17) (cid:16) β − ( 1 + √ z ) 2 (cid:17) . Consequently , if the step size η = 1 α and the momentum parameter β = (cid:16) 1 − 1 2 √ κ (cid:17) 2 , then it has (cid:107) (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) (cid:107) ≤ (cid:18) 1 − 1 2 √ κ (cid:19) t 4 √ κ (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) . ( 19 ) Furthermore , if η = 4 ( √ µ + √ α ) 2 and β approaches β → (cid:16) 1 − 2 √ κ + 1 (cid:17) 2 from above , then it has a convergence rate approximately (cid:16) 1 − 2 √ κ + 1 (cid:17) as t → ∞ . The convergence rates shown in the above theorem do not depend on the unknown sequence { (cid:15) t } . Moreover , the rates depend on the squared root of the condition number √ κ . We have hence established a non - asymptotic accelerated linear rate of Polyak’s momentum , which helps to show the advantage of Polyak’s momentum over vanilla gradient descent in the ﬁnite t regime . Our result also recovers the rate (cid:16) 1 − 2 √ κ + 1 (cid:17) asymptotically under the same choices of the parameters as the previous works . The detailed proof can be found in Appendix D , which is actually a trivial application of Lemma 1 , Theorem 6 , and Corollary 1 with C 1 = C 2 = C 3 = 0 . 4 . 2 . Non - asymptotic accelerated linear rate of the local convergence for solving f ( · ) ∈ F 2 µ , α Here we provide a local acceleration result of the discrete - time Polyak’s momentum for general smooth strongly con - vex and twice differentiable function F 2 µ , α . Compared to Theorem 9 of ( Polyak , 1964 ) , Theorem 8 clearly indicates the required distance that ensures an acceleration when the iterate is in the neighborhood of the global minimizer . Fur - thermore , the rate is in the non - asymptotic sense instead of the asymptotic one . We defer the proof of Theorem 8 to Appendix E . Theorem 8 . Assume that the function f ( · ) ∈ F 2 µ , α and its Hessian is α - Lipschitz . Denote the condition num - ber κ : = αµ . Suppose that the initial point satisﬁes (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) ≤ 1 683 κ 3 / 2 . Then , Gradient descent with Polyak’s momentum with the step size η = 1 α and the momentum parameter β = (cid:16) 1 − 1 2 √ κ (cid:17) 2 for solving min w f ( w ) has (cid:107) (cid:20) w t + 1 − w ∗ w t − w ∗ (cid:21) (cid:107) ≤ (cid:18) 1 − 1 4 √ κ (cid:19) t + 1 8 √ κ (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) , ( 20 ) where w ∗ = arg min w f ( w ) . 4 . 3 . Acceleration for training N ReLU W ( x ) Before introducing our result of training the ReLU network , we need the following lemma . Lemma 5 . [ Lemma 3 . 1 in Du et al . ( 2019b ) and Song & Yang ( 2019 ) ] Set m = Ω ( λ − 2 n 2 log ( n / δ ) ) . Suppose that the neurons w ( 1 ) 0 , . . . , w ( m ) 0 are i . i . d . generated by N ( 0 , I d ) initially . Then , with probability at least 1 − δ , it holds that (cid:107) H 0 − ¯ H (cid:107) F ≤ λ min ( ¯ H ) 4 , λ min (cid:0) H 0 (cid:1) ≥ 3 4 λ min ( ¯ H ) , and λ max (cid:0) H 0 (cid:1) ≤ λ max ( ¯ H ) + λ min ( ¯ H ) 4 . Lemma 5 shows that by the random initialization , with probability 1 − δ , the least eigenvalue of the Gram ma - trix H : = H 0 deﬁned in Lemma 3 is lower - bounded and the largest eigenvalue is close to λ max ( ¯ H ) . Furthermore , Lemma 5 implies that the condition number of the Gram matrix H 0 at the initialization ˆ κ : = λ max ( H 0 ) λ min ( H 0 ) satisﬁes A Modular Analysis of Provable Acceleration via Polyak’s Momentum ˆ κ ≤ 43 κ + 13 , where κ : = λ max ( ¯ H ) λ min ( ¯ H ) . Theorem 9 . ( One - layer ReLU network N ReLU W ( x ) ) Assume that λ : = 3 λ min ( ¯ H ) 4 > 0 and that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . Denote λ max : = λ max ( ¯ H ) + λ min ( ¯ H ) 4 and denote ˆ κ : = λ max / λ = ( 4 κ + 1 ) / 3 . Set a constant step size η = 1 λ max , ﬁx momentum parameter β = (cid:0) 1 − 12ˆ κ (cid:1) 2 , and ﬁnally set the number of network nodes m = Ω ( λ − 4 n 4 κ 2 log 3 ( n / δ ) ) . Then , with probability at least 1 − δ over the random initialization , gradient descent with Polyak’s momentum satisﬁes for any t , (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:18) 1 − 1 4 √ ˆ κ (cid:19) t · 8 √ ˆ κ (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) . ( 21 ) We remark that ˆ κ , which is the condition number of the Gram matrix H 0 , is within a constant factor of the condi - tion number of ¯ H . Therefore , Theorem 9 essentially shows an accelerated linear rate (cid:16) 1 − Θ ( 1 √ κ ) (cid:17) . The rate has an improved dependency on the condition number , i . e . √ κ instead of κ , which shows the advantage of Polyak’s mo - mentum over vanilla GD when the condition number is large . We believe this is an interesting result , as the acceleration is akin to that in convex optimization , e . g . Nesterov ( 2013 ) ; Shi et al . ( 2018 ) . Our result also implies that over - parametrization helps ac - celeration in optimization . To our knowledge , in the lit - erature , there is little theory of understanding why over - parametrization can help training a neural network faster . The only exception that we are aware of is Arora et al . ( 2018 ) , which shows that the dynamic of vanilla gradient descent for an over - parametrized objective function exhibits some momentum terms , although their message is very dif - ferent from ours . The proof of Theorem 9 is in Appendix F . 4 . 4 . Acceleration for training N L - linear W ( x ) Theorem 10 . ( Deep linear network N L - linear W ( x ) ) Denote λ : = Lσ 2min ( X ) d y and κ : = σ 2max ( X ) σ 2min ( X ) . Set a constant step size η = d y Lσ 2max ( X ) , ﬁx momentum parameter β = (cid:0) 1 − 1 2 √ κ (cid:1) 2 , and ﬁnally set a parameter m that controls the width m ≥ C κ 5 σ 2max ( X ) (cid:0) d y ( 1 + (cid:107) W ∗ (cid:107) 22 ) + log ( ¯ r / δ ) (cid:1) and m ≥ max { d x , d y } for some constant C > 0 . Then , with proba - bility at least 1 − δ over the random orthogonal initialization , gradient descent with Polyak’s momentum satisﬁes for any t , (cid:13) (cid:13) (cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13) (cid:13) (cid:13)(cid:13) ≤ (cid:18) 1 − 1 4 √ κ (cid:19) t · 8 √ κ (cid:13) (cid:13) (cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13) (cid:13) (cid:13)(cid:13) . ( 22 ) Compared with Theorem 4 of Hu et al . ( 2020b ) for vanilla GD , our result clearly shows the acceleration via Polyak’s momentum . Furthermore , the result suggests that the depth does not hurt optimization . Acceleration is achieved for any depth L and the required width m is independent of the depth L as Hu et al . ( 2020b ) ; Zou et al . ( 2020 ) ( of vanilla GD ) . The proof of Theorem 10 is in Appendix G . 5 . Conclusion We show some non - asymptotic acceleration results of the discrete - time Polyak’s momentum in this paper . The results not only improve the previous results in convex optimiza - tion but also establish the ﬁrst time that Polyak’s momen - tum has provable acceleration for training certain neural networks . We analyze all the acceleration results from a modular framework . We hope the framework can serve as a building block towards understanding Polyak’s momentum in a more uniﬁed way . 6 . Acknowledgment The authors thank Daniel Pozo for catching a typo . The authors acknowledge support of NSF IIS Award 1910077 . JW also thanks IDEaS - TRIAD Research Scholarship 03GR10000818 . References Alacaoglu , A . , Malitsky , Y . , Mertikopoulos , P . , and Cevher , V . A new regret analysis for adam - type algorithms . ICML , 2020 . Allen - Zhu , Z . , Li , Y . , and Song , Z . A convergence theory for deep learning via overparameterization . ICML , 2019 . Arora , S . , Cohen , N . , and Hazan , E . On the optimization of deep networks : Implicit acceleration by overparameteri - zation . ICML , 2018 . Arora , S . , Cohen , N . , Golowich , N . , and Hu , W . A conver - gence analysis of gradient descent for deep linear neural networks . ICLR , 2019a . Arora , S . , Cohen , N . , Hu , W . , and Luo , Y . Implicit regular - ization in deep matrix factorization . NerurIPS , 2019b . Arora , S . , Du , S . S . , Hu , W . , Li , Z . , and Wang , R . Fine - grained analysis of optimization and generalization for overparameterized two - layer neural networks . NeurIPS , 2019c . Aujol , J . - F . , Dossal , C . , and Rondepierre , A . Convergence rates of the heavy - ball method with lojasiewicz property . hal - 02928958 , 2020 . Bai , Y . and Lee , J . D . Beyond linearization : On quadratic and higher - order approximation of wide neural networks . ICLR , 2020 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Bietti , A . and Mairal , J . On the inductive bias of neural tangent kernels . NeurIPS , 2019 . Brutzkus , A . and Globerson , A . Globally optimal gradient descent for a convnet with gaussian inputs . ICML , 2017 . Cai , T . , Gao , R . , Hou , J . , Chen , S . , Wang , D . , He , D . , Zhang , Z . , and Wang , L . A gram - gauss - newton method learning overparameterized deep neural networks for regression problems . arXiv . org : 1905 . 11675 , 2019 . Can , B . , G¨urb¨uzbalaban , M . , and Zhu , L . Accelerated linear convergence of stochastic momentum methods in wasserstein distances . ICML , 2019 . Chen , S . , He , H . , and Su , W . J . Label - aware neural tangent kernel : Toward better generalization and local elasticity . NeurIPS , 2020a . Chen , Z . , Cao , Y . , Gu , Q . , and Zhang , T . A generalized neu - ral tangent kernel analysis for two - layer neural network . NeurIPS , 2020b . Chizat , L . , Oyallon , E . , and Bach , F . On lazy training in differentiable programming . NeurIPS , 2019 . Cutkosky , A . and Orabona , F . Momentum - based variance reduction in non - convex sgd . NeurIPS , 2019 . Daniely , A . Sgd learns the conjugate kernel class of the network . NeurIPS , 2017 . Daniely , A . Memorizing gaussians with no over - parameterizaion via gradient decent on neural networks . arXiv : 1909 . 11837 , 2020 . Danilova , M . , Kulakova , A . , and Polyak , B . Non - monotone behavior of the heavy ball method . arXiv : 1811 . 00658 , 2018 . Diakonikolas , J . and Jordan , M . I . Generalized momentum - based methods : A hamiltonian perspective . arXiv : 1906 . 00436 , 2019 . Du , S . S . and Hu , W . Width provably matters in optimization for deep linear neural networks . ICML , 2019 . Du , S . S . , Lee , J . D . , Li , H . , Wang , L . , and Zhai , X . Gradient descent ﬁnds global minima of deep neural networks . ICML , 2019a . Du , S . S . , Zhai , X . , Poczos , B . , and Singh , A . Gradient descent provably optimizes over - parameterized neural networks . ICLR , 2019b . Dukler , Y . , Gu , Q . , and Montufar , G . Optimization theory for relu neural networks trained with normalization layers . ICML , 2020 . Fang , C . , Dong , H . , and Zhang , T . Over parameterized two - level neural networks can learn near optimal feature representations . arXiv : 1910 . 11508 , 2019 . Flammarion , N . and Bach , F . From averaging to accelera - tion , there is only a step - size . COLT , 2015 . Foucart , S . Matrix norms and spectral radii . Online lecture note , 2018 . Franca , G . , Sulam , J . , Robinson , D . P . , and Vidal , R . Con - formal symplectic and relativistic optimization . Journal of Statistical Mechanics : Theory and Experiment , 2020 . Gadat , S . , Panloup , F . , and Saadane , S . Stochastic heavy ball . arXiv : 1609 . 04228 , 2016 . Ge , R . , Kuditipudi , R . , Li , Z . , and Wang , X . Learning two - layer neural networks with symmetric inputs . ICLR , 2019 . Gelfand , I . Normierte ringe . Mat . Sbornik , 1941 . Ghadimi , E . , Feyzmahdavian , H . R . , and Johansson , M . Global convergence of the heavy - ball method for convex optimization . ECC , 2015 . Ghorbani , B . , Mei , S . , Misiakiewicz , T . , , and Montanari , A . Linearized two - layers neural networks in high dimension . arXiv : 1904 . 12191 , 2019 . Gidel , G . , Bach , F . , and Lacoste - Julien , S . Implicit regu - larization of discrete gradient dynamics in linear neural networks . NeurIPS , 2019 . Gitman , I . , Lang , H . , Zhang , P . , and Xiao , L . Understanding the role of momentum in stochastic gradient methods . NeurIPS , 2019 . Goh , G . Why momentum really works . Distill , 2017 . Gunasekar , S . , Woodworth , B . , Bhojanapalli , S . , Neyshabur , B . , and Srebro , N . Implicit regularization in matrix fac - torization . NeurIPS , 2017 . Hanin , B . and Nica , M . Finite depth and width corrections to the neural tangent kernel . ICLR , 2020 . Hardt , M . and Ma , T . Identity matters in deep learning . ICLR , 2016 . He , K . , Zhang , X . , Ren , S . , and Sun , J . Deep residual learning for image recognition . Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2016 . Hu , B . Unifying the analysis in control and optimization via semideﬁnite programs . Lecture Note , 2020 . Hu , W . , Xiao , L . , Adlam , B . , and Pennington , J . The sur - prising simplicity of the early - time learning dynamics of neural networks . NeurIPS , 2020a . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Hu , W . , Xiao , L . , and Pennington , J . Provable beneﬁt of or - thogonal initialization in optimizing deep linear networks . ICLR , 2020b . Jacot , A . , Gabriel , F . , and Hongler , C . Neural tangent ker - nel : Convergence and generalization in neural networks . NeurIPS , 2018 . Ji , Z . and Telgarsky , M . Gradient descent aligns the layers of deep linear networks . ICLR , 2019 . Ji , Z . and Telgarsky , M . Polylogarithmic width sufﬁces for gradient descent to achieve arbitrarily small test error with shallow relu networks . ICLR , 2020 . Kawaguchi , K . Deep learning without poor local minima . NeurIPS , 2016 . Kidambi , R . , Netrapalli , P . , Jain , P . , and Kakade , S . M . On the insufﬁciency of existing momentum schemes for stochastic optimization . ICLR , 2018 . Kingma , D . P . and Ba , J . Adam : A method for stochastic optimization . ICLR , 2015 . Krichene , W . , Caluyay , K . F . , and Halder , A . Global con - vergence of second - order dynamics in two - layer neural networks . arXiv : 2006 . 07867 , 2020 . Krizhevsky , A . , Sutskever , I . , and Hinton , G . E . Imagenet classiﬁcation with deep convolutional neural networks . NeurIPS , 2012 . Laurent , T . and von Brecht , J . Deep linear networks with arbitrary loss : All local minima are global . ICML , 2018 . Lee , J . , Xiao , L . , Schoenholz , S . S . , Bahri , Y . , Sohl - Dickstein , J . , and Pennington , J . Wide neural networks of any depth evolve as linear models under gradient descent . NeurIPS , 2019 . Lee , J . D . , Shen , R . , Song , Z . , Wang , M . , and Yu , Z . Gen - eralized leverage score sampling for neural networks . arXiv : 2009 . 09829 , 2020 . Lessard , L . , Recht , B . , and Packard , A . Analysis and de - sign of optimization algorithms via integral quadratic constraints . SIAM Journal on Optimization , 2016 . Li , Y . and Liang , Y . Learning overparameterized neural networks via stochastic gradient descent on structured data . NeurIPS , 2018 . Li , Y . and Yuan , Y . Convergence analysis of two - layer neural networks with relu activation . NeurIPS , 2017 . Li , Y . , Ma , T . , and Zhang , H . Algorithmic regularization in over - parameterized matrix sensing and neural networks with quadratic activations . COLT , 2018 . Li , Y . , Ma , T . , and Zhang , H . Learning over - parametrized two - layer relu neural networks beyond ntk . COLT , 2020 . Liu , C . and Belkin , M . Parametrized accelerated methods free of condition number . arXiv : 1802 . 10235 , 2018 . Liu , C . , Zhu , L . , and Belkin , M . On the linearity of large non - linear models : when and why the tangent kernel is constant . arXiv : 2010 . 01092 , 2020a . Liu , C . , Zhu , L . , and Belkin , M . Toward a the - ory of optimization for over - parameterized systems of non - linear equations : the lessons of deep learning . arXiv : 2003 . 00307 , 2020b . Liu , Y . , Gao , Y . , and Yin , W . An improved analysis of stochastic gradient descent with momentum . NeurIPS , 2020c . Loizou , N . and Richt´arik , P . Momentum and stochastic mo - mentum for stochastic gradient , newton , proximal point and subspace descent methods . arXiv : 1712 . 09677 , 2017 . Loizou , N . and Richt ´ arik , P . Accelerated gossip via stochas - tic heavy ball method . Allerton , 2018 . Loshchilov , I . and Hutter , F . Decoupled weight decay regu - larization . ICLR , 2019 . Lu , H . and Kawaguchi , K . Depth creates no bad local minima . arXiv : 1702 . 08580 , 2017 . Luo , L . , Xiong , Y . , Liu , Y . , and Sun , X . Adaptive gradient methods with dynamic bound of learning rate . ICLR , 2019 . Lyu , K . and Li , J . Gradient descent maximizes the margin of homogeneous neural networks . ICLR , 2020 . Mai , V . V . and Johansson , M . Convergence of a stochastic gradient method with momentum for non - smooth non - convex optimization . ICML , 2020 . Mitliagkas , I . Accelerated methods - polyak’s momentum ( heavy ball method ) . Online Lecture Note , 2019 . Moroshko , E . , Gunasekar , S . , Woodworth , B . , Lee , J . D . , Srebro , N . , and Soudry , D . Implicit bias in deep linear classiﬁcation : Initialization scale vs training accuracy . NeurIPS , 2020 . Nesterov , Y . Introductory lectures on convex optimization : a basic course . Springer , 2013 . Oymak , S . and Soltanolkotabi , M . Towards moderate overparameterization : global convergence guarantees for training shallow neural networks . arXiv : 1902 . 04674 , 2019 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Panigrahi , A . , Shetty , A . , and Goyal , N . Effect of activation functions on the training of overparametrized neural nets . ICLR , 2020 . Pilanci , M . and Ergen , T . Neural networks are convex regularizers : Exact polynomial - time convex optimization formulations for two - layer networks . ICML , 2020 . Polyak , B . Gradient methods for minimizing functionals . Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki , 1963 . Polyak , B . Some methods of speeding up the convergence of iteration methods . USSR Computational Mathematics and Mathematical Physics , 1964 . Razin , N . and Cohen , N . Implicit regularization in deep learning may not be explainable by norms . NeurIPS2020 , 2020 . Recht , B . Lyapunov analysis and the heavy ball method . Lecture note , 2018 . Reddi , S . J . , Kale , S . , and Kumar , S . On the convergence of adam and beyond . ICLR , 2018 . Saxe , A . M . , McClelland , J . L . , and Ganguli , S . Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . ICLR , 2014 . Scieur , D . and Pedregosa , F . Universal average - case opti - mality of polyak momentum . ICML , 2020 . Shamir , O . Exponential convergence time of gradient de - scent for one - dimensional deep linear neural networks . COLT , 2019 . Shi , B . , Du , S . S . , Jordan , M . I . , and Su , W . J . Understanding the acceleration phenomenon via high - resolution differ - ential equations . arXiv : 1810 . 08907 , 2018 . Soltanolkotabi , M . Learning relus via gradient descent . NeurIPS , 2017 . Song , Z . and Yang , X . Quadratic sufﬁces for over - parametrization via matrix chernoff bound . arXiv : 1906 . 03593 , 2019 . Su , L . and Yang , P . On learning over - parameterized neu - ral networks : A functional approximation perspective . NeurIPS , 2019 . Sun , T . , Yin , P . , Li , D . , Huang , C . , Guan , L . , and Jiang , H . Non - ergodic convergence analysis of heavy - ball algo - rithms . AAAI , 2019 . Tian , Y . An analytical formula of population gradient for two - layered relu network and its applications in conver - gence and critical point analysis . ICML , 2017 . van den Brand , J . , Peng , B . , Song , Z . , and Weinstein , O . Training ( overparametrized ) neural networks in near - linear time . arXiv : 2006 . 11648 , 2020 . Vaswani , A . , Shazeer , N . , Parmar , N . , and et al . Attention is all you need . NeurIPS , 2017 . Wang , J . - K . , Lin , C . - H . , and Abernethy , J . Escaping saddle points faster with stochastic momentum . ICLR , 2020 . Wei , C . , Lee , J . D . , Liu , Q . , and Ma , T . Regularization matters : Generalization and optimization of neural nets v . s . their induced kernel . NeurIPS , 2019 . Wilson , A . C . , Roelofs , R . , Stern , M . , Srebro , N . , , and Recht . , B . The marginal value of adaptive gradient meth - ods in machine learning . NeurIPS , 2017 . Wilson , A . C . , Jordan , M . , and Recht , B . A lyapunov analysis of momentum methods in optimization . JMLR , 2021 . Wu , L . , Wang , Q . , and Ma , C . Global convergence of gradi - ent descent for deep linear residual networks . NeurIPS , 2019a . Wu , S . , Dimakis , A . G . , and Sanghavi , S . Learning distri - butions generated by one - layer relu networks . NeurIPS , 2019b . Wu , X . , Du , S . S . , and Ward , R . Global convergence of adap - tive gradient methods for an over - parameterized neural network . arXiv : 1902 . 07111 , 2019c . Yang , G . Scaling limits of wide neural networks with weight sharing : Gaussian process behavior , gradient independence , and neural tangent kernel derivation . arXiv : 1902 . 04760 , 2019 . Yang , T . , Lin , Q . , and Li , Z . Uniﬁed convergence analysis of stochastic momentum methods for convex and non - convex optimization . IJCAI , 2018 . Yehudai , G . and Shamir , O . Learning a single neuron with gradient methods . COLT , 2020 . Yun , C . , Sra , S . , and Jadbabaie , A . Global optimality condi - tions for deep neural networks . ICLR , 2018 . Zhang , G . , Martens , J . , and Grosse , R . B . Fast convergence of natural gradient descent for over - parameterized neural networks . NeurIPS , 2019 . Zhong , K . , Song , Z . , Jain , P . , Bartlett , P . L . , and Dhillon , I . S . Recovery guarantees for one - hidden - layer neural networks . ICML , 2017 . Zhou , Y . and Liang , Y . Critical points of linear neural networks : Analytical forms and landscape . ICLR , 2018 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Zou , D . and Gu , Q . An improved analysis of training over - parameterized deep neural networks . NeurIPS , 2019 . Zou , D . , Cao , Y . , Zhou , D . , and Gu , Q . Stochastic gradient descent optimizes overparameterized deep relu networks . Machine Learning , Springer , 2019 . Zou , D . , Long , P . M . , and Gu , Q . On the global convergence of training deep linear resnets . ICLR , 2020 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum A . Linear - rate results of the discrete - time Polyak’s momentum In the discrete - time setting , for general smooth , strongly convex , and differentiable functions , a linear rate of the global convergence is shown by Ghadimi et al . ( 2015 ) and Shi et al . ( 2018 ) . However , the rate is not an accelerated rate and is not better than that of the vanilla gradient descent . To our knowledge , the class of the strongly convex quadratic problems is the only known example that Polyak’s momentum has a provable accelerated linear rate in terms of the global convergence in the discrete - time setting . B . Proof of Lemma 2 , Lemma 3 , and Lemma 4 Lemma 2 : Applying Algorithm 1 or Algorithm 2 to minimizing a function f ( w ) ∈ F 2 µ , α induces a residual dynamics in the form of ( 8 ) , where ξ t = w t − w ∗ H = (cid:90) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ ϕ t = η (cid:18)(cid:90) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ − (cid:90) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w t + τw ∗ (cid:1) dτ (cid:19) ( w t − w ∗ ) , where w ∗ : = arg min w f ( w ) . Proof . We have (cid:20) w t + 1 − w ∗ w t − w ∗ (cid:21) = (cid:20) I d + βI d − βI d I d 0 d (cid:21) · (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) + (cid:20) − η ∇ f ( w t ) 0 (cid:21) = (cid:20) I d − η (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w t + τw ∗ (cid:1) dτ + βI d − βI d I d 0 d (cid:21) · (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) = (cid:20) I d − η (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ + βI d − βI d I d 0 d (cid:21) · (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) + η (cid:18)(cid:90) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + τw ∗ (cid:1) dτ − (cid:90) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w t + τw ∗ (cid:1) dτ (cid:19) ( w t − w ∗ ) , ( 23 ) where the second equality is by the fundamental theorem of calculus . ∇ f ( w t ) − ∇ f ( w ∗ ) = (cid:18)(cid:90) 1 0 ∇ 2 f ( ( 1 − τ ) w t + τw ∗ ) dτ (cid:19) ( w t − w ∗ ) , ( 24 ) and that ∇ f ( w ∗ ) = 0 . Lemma 3 : ( Residual dynamics of training the ReLU network N ReLU W ( · ) ) Denote ( H t ) i , j : = H ( W t ) i , j = 1 m m (cid:88) r = 1 x (cid:62) i x j 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . Applying Algorithm 1 or Algorithm 2 to ( 5 ) for training the ReLU network N ReLU W ( x ) induces a residual dynamics in the form of ( 8 ) such that ξ t [ i ] = N ReLU W t ( x i ) − y i and hence n 0 = d H = H 0 ϕ t = φ t + ι t , where each element i of ξ t ∈ R n is the residual error of the sample i , the i th - element of φ t ∈ R n satisﬁes | φ t [ i ] | ≤ 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) , and ι t = η ( H 0 − H t ) ξ t ∈ R n . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Proof . For each sample i , we will divide the contribution to N ( x i ) into two groups . N ( x i ) = 1 √ m m (cid:88) r = 1 a r σ ( (cid:104) w ( r ) , x i (cid:105) ) = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) , x i (cid:105) ) + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) , x i (cid:105) ) . ( 25 ) To continue , let us recall some notations ; the subgradient with respect to w ( r ) ∈ R d is ∂L ( W ) ∂w ( r ) : = 1 √ m n (cid:88) i = 1 (cid:0) N ( x i ) − y i (cid:1) a r x i 1 { (cid:104) w ( r ) , x (cid:105) ≥ 0 } , ( 26 ) and the Gram matrix H t whose ( i , j ) element is H t [ i , j ] : = 1 mx (cid:62) i x j m (cid:88) r = 1 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 27 ) Let us also denote H ⊥ t [ i , j ] : = 1 mx (cid:62) i x j (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 28 ) We have that ξ t + 1 [ i ] = N t + 1 ( x i ) − y i ( 25 ) = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − y i . ( 29 ) For the ﬁrst term above , we have that 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t − η ∂L ( W t ) ∂w ( r ) t + β ( w ( r ) t − w ( r ) t − 1 ) , x i (cid:105) ) = 1 √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t − η ∂L ( W t ) ∂w ( r ) t + β ( w ( r ) t − w ( r ) t − 1 ) , x i (cid:105) · 1 { (cid:104) w ( r ) t + 1 , x i (cid:105) ≥ 0 } ( a ) = 1 √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t , x i (cid:105) · 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t , x i (cid:105) · 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t − 1 , x i (cid:105) · 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } − η 1 √ m (cid:88) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } = N t ( x i ) + β (cid:0) N t ( x i ) − N t − 1 ( x i ) (cid:1) − 1 √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t − 1 , x i (cid:105) 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } (cid:1) − η 1 √ m (cid:88) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } (cid:124) (cid:123)(cid:122) (cid:125) last term , ( 30 ) where ( a ) uses that for r ∈ S i , 1 { (cid:104) w ( r ) t + 1 , x i (cid:105) ≥ 0 } = 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } = 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } as the neurons in S i do not A Modular Analysis of Provable Acceleration via Polyak’s Momentum change their activation patterns . We can further bound ( 30 ) as ( b ) = N t ( x i ) + β (cid:0) N t ( x i ) − N t − 1 ( x i ) (cid:1) − η n (cid:88) j = 1 (cid:0) N t ( x j ) − y j (cid:1) H ( W t ) i , j − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } − 1 √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t − 1 , x i (cid:105) 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } (cid:1) , ( 31 ) where ( b ) is due to that 1 √ m (cid:80) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } (cid:124) (cid:123)(cid:122) (cid:125) last term = 1 m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } = n (cid:88) j = 1 (cid:0) N t ( x j ) − y j (cid:1) H ( W t ) i , j − 1 m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 32 ) Combining ( 29 ) and ( 31 ) , we have that ξ t + 1 [ i ] = ξ t [ i ] + β (cid:0) ξ t [ i ] − ξ t − 1 [ i ] (cid:1) − η n (cid:88) j = 1 H t [ i , j ] ξ t [ j ] − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) . ( 33 ) So we can write the above into a matrix form . ξ t + 1 = ( I n − ηH t ) ξ t + β ( ξ t − ξ t − 1 ) + φ t = ( I n − ηH 0 ) ξ t + β ( ξ t − ξ t − 1 ) + φ t + ι t , ( 34 ) where the i element of φ t ∈ R n is deﬁned as φ t [ i ] = − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i (cid:8) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:9) . ( 35 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum Now let us bound φ t [ i ] as follows . φ t [ i ] = − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i (cid:8) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:9) ( a ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + 1 √ m (cid:88) r ∈ S ⊥ i (cid:0) (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107) + β (cid:107) w ( r ) t − w ( r ) t − 1 (cid:107) (cid:1) ( b ) = η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ m (cid:88) r ∈ S ⊥ i (cid:0) (cid:107) t (cid:88) s = 0 β t − s ∂L ( W s ) ∂w ( r ) s (cid:107) + β (cid:107) t − 1 (cid:88) s = 0 β t − 1 − s ∂L ( W s ) ∂w ( r ) s (cid:107) (cid:1) ( c ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ m (cid:88) r ∈ S ⊥ i (cid:0) t (cid:88) s = 0 β t − s (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) (cid:1) ( d ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ n | S ⊥ i | m (cid:0) t (cid:88) s = 0 β t − s (cid:107) u s − y (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) = 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) , ( 36 ) where ( a ) is because − ηm (cid:80) nj = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:80) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } ≤ η | S ⊥ i | m (cid:80) nj = 1 | N t ( x j ) − y j | ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) , and that σ ( · ) is 1 - Lipschitz so that 1 √ m (cid:88) r ∈ S ⊥ i (cid:0) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) (cid:1) ≤ 1 √ m (cid:88) r ∈ S ⊥ i | (cid:104) w ( r ) t + 1 , x i (cid:105) − (cid:104) w ( r ) t , x i (cid:105) | ≤ 1 √ m (cid:88) r ∈ S ⊥ i (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107)(cid:107) x i (cid:107) ≤ 1 √ m (cid:88) r ∈ S ⊥ i (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107) , similarly , − β √ m (cid:80) r ∈ S ⊥ i (cid:0) a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:1) ≤ β 1 √ m (cid:80) r ∈ S ⊥ i (cid:107) w ( r ) t − w ( r ) t − 1 (cid:107) , ( b ) is by the update rule ( Algorithm 1 ) , ( c ) is by Jensen’s inequality , ( d ) is because | ∂L ( W s ) ∂w ( r ) s | = | 1 √ m (cid:80) ni = 1 (cid:0) u s [ i ] − y i (cid:1) a r x i 1 { x (cid:62) w ( r ) t ≥ 0 } | ≤ √ nm (cid:107) u s − y (cid:107) . Lemma : 4 ( Residual dynamics of training N L - linear W ( · ) ) Denote M t , l the momentum term of layer l at iteration t , which is recursively deﬁned as M t , l = βM t , l − 1 + ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t . Denote H t : = 1 m L − 1 d y (cid:80) Ll = 1 [ ( W ( l − 1 : 1 ) t X ) (cid:62) ( W ( l − 1 : 1 ) t X ) ⊗ W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) ] ∈ R d y n × d y n . Applying Algorithm 1 or Algorithm 2 to ( 5 ) for training the deep linear network N L - linear W ( x ) induces a residual dynamics in the form of ( 8 ) such that ξ t = vec ( U t − Y ) ∈ R d y n , and hence n 0 = d y n H = H 0 ϕ t = φ t + ψ t + ι t ∈ R d y n , A Modular Analysis of Provable Acceleration via Polyak’s Momentum where φ t = 1 (cid:112) m L − 1 d y vec ( Φ t X ) with Φ t = Π l ( W ( l ) t − ηM t , l ) − W ( L : 1 ) t + η L (cid:88) l = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) t ψ t = 1 (cid:112) m L − 1 d y vec (cid:32) ( L − 1 ) βW ( L : 1 ) t X + βW ( L : 1 ) t − 1 X − β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X (cid:33) ι t = η ( H 0 − H t ) ξ t . Proof . According to the update rule of gradient descent with Polyak’s momentum , we have W ( L : 1 ) t + 1 = Π l (cid:16) W ( l ) t − ηM t , l (cid:17) = W ( L : 1 ) t − η L (cid:88) l = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) + Φ t , ( 37 ) where M t , l stands for the momentum term of layer l , which is M t , l = βM t , l − 1 + ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t = (cid:80) ts = 0 β t − s ∂(cid:96) ( W ( L : 1 ) s ) ∂W ( l ) s , and Φ t contains all the high - order terms ( in terms of η ) , e . g . those with ηM t , i and ηM t , j , i (cid:54) = j ∈ [ L ] , or higher . Based on the equivalent update expression of gradient descent with Polyak’s momentum − ηM t , l = − η ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t + β ( W ( l ) t − W ( l ) t − 1 ) , we can rewrite ( 37 ) as W ( L : 1 ) t + 1 = W ( L : 1 ) t − η L (cid:88) l = 1 W ( L : l + 1 ) t ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t W ( l − 1 : 1 ) t + L (cid:88) l = 1 W ( L : l + 1 ) t β ( W ( l ) t − W ( l ) t − 1 ) W ( l − 1 : 1 ) t + Φ t = W ( L : 1 ) t − η L (cid:88) l = 1 W ( L : l + 1 ) t ∂(cid:96) ( W ( L : 1 ) t ) ∂W ( l ) t W ( l − 1 : 1 ) t + β ( W ( L : 1 ) t − W ( L : 1 ) t − 1 ) + Φ t + ( L − 1 ) βW ( L : 1 ) t + βW ( L : 1 ) t − 1 − β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t . ( 38 ) Multiplying the above equality with 1 √ m L − 1 d y X , we get U t + 1 = U t − η 1 m L − 1 d y L (cid:88) l = 1 W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X + β ( U t − U t − 1 ) + 1 (cid:112) m L − 1 d y (cid:32) ( L − 1 ) βW ( L : 1 ) t + βW ( L : 1 ) t − 1 − β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t (cid:33) X + 1 (cid:112) m L − 1 d y Φ t X . ( 39 ) Using vec ( ACB ) = ( B (cid:62) ⊗ A ) vec ( C ) , where ⊗ stands for the Kronecker product , we can apply a vectorization of the above equation and obtain vec ( U t + 1 ) − vec ( U t ) = − ηH t vec ( U t − Y ) + β ( vec ( U t ) − vec ( U t − 1 ) ) + vec ( 1 (cid:112) m L − 1 d y (cid:32) ( L − 1 ) βW ( L : 1 ) t + βW ( L : 1 ) t − 1 − β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t (cid:33) X ) + 1 (cid:112) m L − 1 d y vec ( Φ t X ) , ( 40 ) where H t = 1 m L − 1 d y L (cid:88) l = 1 (cid:104)(cid:16) ( W ( l − 1 : 1 ) t X ) (cid:62) ( W ( l − 1 : 1 ) t X ) (cid:17) ⊗ W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) (cid:105) , ( 41 ) which is a positive semi - deﬁnite matrix . A Modular Analysis of Provable Acceleration via Polyak’s Momentum In the following , we will denote ξ t : = vec ( U t − Y ) as the vector of the residual errors . Also , we denote φ t : = 1 √ m L − 1 d y vec ( Φ t X ) with Φ t = Π l ( W ( l ) t − ηM t , l ) − W ( L : 1 ) t + η (cid:80) Ll = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) t , and ψ t : = vec ( 1 √ m L − 1 d y (cid:16) ( L − 1 ) βW ( L : 1 ) t + βW ( L : 1 ) t − 1 − β (cid:80) Ll = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t (cid:17) X ) . Using the notations , we can rewrite ( 40 ) as (cid:20) ξ t + 1 ξ t (cid:21) = (cid:20) I d y n − ηH t + βI d y n − βI d y n I d y n 0 d y n (cid:21) (cid:20) ξ t ξ t − 1 (cid:21) + (cid:20) φ t + ψ t 0 d y n (cid:21) = (cid:20) I d y n − ηH 0 + βI d y n − βI d y n I d y n 0 d y n (cid:21) (cid:20) ξ t ξ t − 1 (cid:21) + (cid:20) ϕ t 0 d y n (cid:21) , ( 42 ) where ϕ t = φ t + ψ t + ι t ∈ R d y n and I d y n is the d y n × d y n - dimensional identity matrix . C . Proof of Theorem 5 Theorem 5 Let A : = (cid:20) ( 1 + β ) I n − ηH − βI n I n 0 (cid:21) ∈ R 2 n × 2 n . Suppose that H ∈ R n × n is a positive semideﬁnite matrix . Fix a vector v 0 ∈ R n . If β is chosen to satisfy 1 ≥ β > max { (cid:16) 1 − (cid:112) ηλ min ( H ) (cid:17) 2 , (cid:16) 1 − (cid:112) ηλ max ( H ) (cid:17) 2 } , then (cid:107) A k v 0 (cid:107) ≤ (cid:0)(cid:112) β (cid:1) k C 0 (cid:107) v 0 (cid:107) , ( 43 ) where the constant C 0 : = √ 2 ( β + 1 ) (cid:112) min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } ≥ 1 , ( 44 ) and the function h ( β , z ) is deﬁned as h ( β , z ) : = − (cid:16) β − (cid:0) 1 − √ z (cid:1) 2 (cid:17) (cid:16) β − (cid:0) 1 + √ z (cid:1) 2 (cid:17) . ( 45 ) We would ﬁrst prove some lemmas for the analysis . Lemma 6 . Under the assumption of Theorem 5 , A is diagonalizable with respect to complex ﬁeld C in C n , i . e . , ∃ P such that A = PDP − 1 for some diagonal matrix D . Furthermore , the diagonal elements of D all have magnitudes bounded by √ β . Proof . In the following , we will use the notation / operation Diag ( · · · ) to represents a block - diagonal matrix that has the arguments on its main diagonal . Let U Diag ( [ λ 1 , . . . , λ n ] ) U ∗ be the singular - value - decomposition of H , then A = (cid:20) U 0 0 U (cid:21) (cid:20) ( 1 + β ) I n − η Diag ( [ λ 1 , . . . , λ n ] ) − βI n I n 0 (cid:21) (cid:20) U ∗ 0 0 U ∗ (cid:21) . ( 46 ) Let ˜ U = (cid:20) U 0 0 U (cid:21) . Then , after applying some permutation matrix ˜ P , A can be further simpliﬁed into A = ˜ U ˜ P Σ ˜ P T ˜ U ∗ , ( 47 ) where Σ is a block diagonal matrix consisting of n 2 - by - 2 matrices ˜Σ i : = (cid:20) 1 + β − ηλ i − β 1 0 (cid:21) . The characteristic polynomial of ˜Σ i is x 2 − ( 1 + β − λ i ) x + β . Hence it can be shown that when β > ( 1 −√ ηλ i ) 2 then the roots of polynomial are conjugate and have magnitude √ β . These roots are exactly the eigenvalues of ˜Σ i ∈ R 2 × 2 . On the other hand , the A Modular Analysis of Provable Acceleration via Polyak’s Momentum corresponding eigenvectors q i , ¯ q i are also conjugate to each other as ˜Σ i ∈ R 2 × 2 is a real matrix . As a result , Σ ∈ R 2 n × 2 n admits a block eigen - decomposition as follows , Σ = Diag ( ˜Σ i , . . . , ˜Σ n ) = Diag ( Q 1 , . . . , Q n ) Diag (cid:18)(cid:20) z 1 0 0 ¯ z 1 (cid:21) , . . . , (cid:20) z n 0 0 ¯ z n (cid:21)(cid:19) Diag ( Q − 1 1 , . . . , Q − 1 n ) , ( 48 ) where Q i = [ q i , ¯ q i ] and z i , ¯ z i are eigenvalues of ˜Σ i ( they are conjugate by the condition on β ) . Denote Q : = Diag ( Q 1 , . . . , Q n ) and D : = Diag (cid:18)(cid:20) z 1 0 0 ¯ z 1 (cid:21) , . . . , (cid:20) z n 0 0 ¯ z n (cid:21)(cid:19) . ( 49 ) By combining ( 47 ) and ( 48 ) , we have A = P Diag (cid:18)(cid:20) z 1 0 0 ¯ z 1 (cid:21) , . . . , (cid:20) z n 0 0 ¯ z n (cid:21)(cid:19) P − 1 = PDP − 1 , ( 50 ) where P = ˜ U ˜ PQ , ( 51 ) by the fact that ˜ P − 1 = ˜ P T and ˜ U − 1 = ˜ U ∗ . Proof . ( of Theorem 5 ) Now we proceed the proof of Theorem 5 . In the following , we denote v k : = A k v 0 ( so v k = Av k − 1 ) . Let P be the matrix in Lemma 6 , and u k : = P − 1 v k , the dynamic can be rewritten as u k = P − 1 Av k − 1 = P − 1 APu k − 1 = Du k − 1 . As D is diagonal , we immediately have (cid:107) u k (cid:107) ≤ max i ∈ [ n ] | D ii | k (cid:107) u 0 (cid:107) ⇒ (cid:107) P − 1 v k (cid:107) ≤ max i ∈ [ n ] | D ii | k (cid:107) P − 1 v 0 (cid:107) ⇒ σ min ( P − 1 ) (cid:107) v k (cid:107) ≤ (cid:112) β k σ max ( P − 1 ) (cid:107) v 0 (cid:107) ( Lemma 6 . ) ⇒ σ − 1max ( P ) (cid:107) v k (cid:107) ≤ (cid:112) β k σ − 1min ( P ) (cid:107) v 0 (cid:107) ⇒ (cid:107) v k (cid:107) ≤ (cid:112) β k σ max ( P ) σ min ( P ) (cid:107) v 0 (cid:107) ⇒ (cid:107) v k (cid:107) ≤ (cid:112) β k (cid:115) λ max ( PP ∗ ) λ min ( PP ∗ ) (cid:107) v 0 (cid:107) . ( 52 ) Hence , now it sufﬁces to prove upper bound and lower bound of λ max and λ min , respectively . By using Lemma 7 in the following , we obtain the inequality of ( 43 ) . We remark that as C 0 is an upper - bound of the squared root of the condition number (cid:113) λ max ( PP ∗ ) λ min ( PP ∗ ) , it is lower bounded by 1 . Lemma 7 . Let P be the matrix in Lemma 6 , then we have λ max ( PP ∗ ) ≤ 2 ( β + 1 ) and λ min ( PP ∗ ) ≥ min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } / ( 1 + β ) , where h ( β , z ) = − (cid:16) β − (cid:0) 1 − √ z (cid:1) 2 (cid:17) (cid:16) β − (cid:0) 1 + √ z (cid:1) 2 (cid:17) . ( 53 ) Proof . As ( 51 ) in the proof of Lemma 2 , P = ˜ U ˜ P Diag ( Q 1 , . . . , Q n ) . Since ˜ U ˜ P is unitary , it does not affect the spectrum of P , therefore , it sufﬁces to analyze the eigenvalues of QQ ∗ , where Q = Diag ( Q 1 , . . . , Q n ) . Observe that QQ ∗ is a block diagonal matrix with blocks Q i Q ∗ i , the eigenvalues of it are exactly that of Q i Q ∗ i , i . e . , λ max ( QQ ∗ ) = max i ∈ [ n ] λ max ( Q i Q ∗ i ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum and likewise for the minimum . Recall Q i = [ q i , ¯ q i ] consisting of eigenvectors of ˜Σ i : = (cid:20) 1 + β − ηλ i − β 1 0 (cid:21) with corresponding eigenvalues z i , ¯ z i . The eigenvalues satisfy z i + ¯ z i = 2 (cid:60) z i = 1 + β − ηλ i , ( 54 ) z i ¯ z i = | z i | 2 = β . ( 55 ) On the other hand , the eigenvalue equation ˜Σ i q i = z i q i together with ( 54 ) implies q i = [ z i , 1 ] T . Furthermore , Q i Q ∗ i = q i q ∗ i + ¯ q i ¯ q ∗ i = 2 (cid:60) q i q ∗ i = 2 (cid:60) q i (cid:60) q iT + 2 (cid:61) q i (cid:61) q iT . Thus , Q i Q ∗ i = 2 (cid:60) q i (cid:60) q iT + 2 (cid:61) q i (cid:61) q iT = 2 (cid:18)(cid:20) (cid:60) z i 1 (cid:21) (cid:2) (cid:60) z i 1 (cid:3) + (cid:20) (cid:61) z i 0 (cid:21) (cid:2) (cid:61) z i 0 (cid:3)(cid:19) = 2 (cid:20) | z i | 2 (cid:60) z i (cid:60) z i 1 (cid:21) . ( 56 ) Let the eigenvalues of Q i Q ∗ i be θ 1 , θ 2 , then by ( 54 ) - ( 56 ) we must have θ 1 + θ 2 = 2 ( β + 1 ) , ( 57 ) θ 1 θ 2 = 4 (cid:18) β − ( 1 + β − ηλ i 2 ) 2 (cid:19) = − (cid:18) β − (cid:16) 1 − (cid:112) ηλ i (cid:17) 2 (cid:19) (cid:18) β − (cid:16) 1 + (cid:112) ηλ i (cid:17) 2 (cid:19) ≥ 0 . ( 58 ) From ( 57 ) , as both eigenvalues are nonnegative , we deduce that 2 ( 1 + β ) ≥ max { θ 1 , θ 2 } ≥ β + 1 . ( 59 ) On the other hand , from ( 57 ) we also have min { θ 1 , θ 2 } = θ 1 θ 2 / max { θ 1 , θ 2 } ≥ − (cid:18) β − (cid:16) 1 − (cid:112) ηλ i (cid:17) 2 (cid:19) (cid:18) β − (cid:16) 1 + (cid:112) ηλ i (cid:17) 2 (cid:19) / ( 1 + β ) : = h ( β , ηλ i ) / ( 1 + β ) . ( 60 ) Finally , as the eigenvalues of QQ ∗ are composed of exactly that of Q i Q ∗ i , applying the bound of ( 60 ) to each i we have λ min ( PP ∗ ) ≥ min i ∈ [ n ] h ( β , ηλ i ) / ( 1 + β ) ≥ min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } / ( 1 + β ) , ( 61 ) where the last inequality follows from the facts that λ min ( H ) ≤ λ i ≤ λ max ( H ) and h is concave quadratic function of of λ in which the minimum must occur at the boundary . C . 1 . Proof of Corollary 1 Corollary 1 Assume that λ min ( H ) > 0 . Denote κ : = λ max ( H ) / λ min ( H ) . Set η = 1 / λ max ( H ) and set β = (cid:16) 1 − 12 (cid:112) ηλ min ( H ) (cid:17) 2 = (cid:16) 1 − 1 2 √ κ (cid:17) 2 . Then , C 0 ≤ max { 4 , 2 √ κ } ≤ 4 √ κ . Proof . For notation brevity , in the following , we let µ : = λ min ( H ) and α : = λ max ( H ) . Recall that h ( β , z ) = − (cid:16) β − ( 1 − √ z ) 2 (cid:17) (cid:16) β − ( 1 + √ z ) 2 (cid:17) . We have h ( β , ηµ ) = − (cid:18) ( 1 − 1 2 √ ηµ ) 2 − ( 1 − √ ηµ ) 2 (cid:19) (cid:18) ( 1 − 1 2 √ ηµ ) 2 − ( 1 + √ ηµ ) 2 (cid:19) = 3 (cid:18) √ ηµ − 3 4 ηµ (cid:19) (cid:18) √ ηµ + 1 4 ηµ (cid:19) = 3 (cid:18) 1 √ κ − 3 4 κ (cid:19) (cid:18) 1 √ κ + 1 4 κ (cid:19) ( 62 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum and h ( β , ηα ) = − (cid:18) ( 1 − 1 2 √ ηµ ) 2 − ( 1 − √ ηα ) 2 (cid:19) (cid:18) ( 1 − 1 2 √ ηµ ) 2 − ( 1 + √ ηα ) 2 (cid:19) = (cid:18) 2 √ ηα − √ ηµ − ηα + 1 4 ηµ (cid:19) (cid:18) √ ηµ + 2 √ ηα + ηα − 1 4 ηµ (cid:19) = (cid:18) 1 − 1 √ κ + 1 4 κ (cid:19) (cid:18) 3 + 1 √ κ − 1 4 κ (cid:19) . ( 63 ) We can simplify it to get that h ( β , ηα ) = 3 − 2 √ κ − 12 κ + 1 2 κ 3 / 2 − 1 16 κ 2 ≥ 0 . 5 . Therefore , we have √ 2 ( β + 1 ) (cid:112) h ( β , ηµ ) = √ 2 ( β + 1 ) (cid:113) 3 ηµ ( 1 − 12 √ ηµ − 316 ηµ ) = √ 2 ( β + 1 ) (cid:113) 3 ( 1 − 12 √ ηµ − 316 ηµ ) √ κ ≤ 1 (cid:113) ( 1 − 12 − 316 ) √ κ ≤ 2 √ κ , ( 64 ) where we use ηµ = 1 κ . On the other hand , √ 2 ( β + 1 ) √ h ( β , ηα ) ≤ 4 . We conclude that C 0 = √ 2 ( β + 1 ) (cid:112) min { h ( β , ην , h ( β , ηα ) } ≤ max { 4 , 2 √ κ } ≤ 4 √ κ . ( 65 ) D . Proof of Theorem 7 Theorem 7 Assume the momentum parameter β satisﬁes 1 ≥ β > max { (cid:0) 1 − √ ηµ (cid:1) 2 , (cid:0) 1 − √ ηα (cid:1) 2 } . Gradient descent with Polyak’s momentum has (cid:107) (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) (cid:107) ≤ (cid:16)(cid:112) β (cid:17) t C 0 (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) , ( 66 ) where the constant C 0 : = √ 2 ( β + 1 ) (cid:112) min { h ( β , ηλ min ( Γ ) ) , h ( β , ηλ max ( Γ ) ) } , ( 67 ) and h ( β , z ) = − (cid:16) β − ( 1 − √ z ) 2 (cid:17) (cid:16) β − ( 1 + √ z ) 2 (cid:17) . Consequently , if the step size η = 1 α and the momentum parameter β = (cid:0) 1 − √ ηµ (cid:1) 2 , then it has (cid:107) (cid:20) w t − w ∗ w t − 1 − w ∗ (cid:21) (cid:107) ≤ (cid:18) 1 − 1 2 √ κ (cid:19) t 4 √ κ (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) . ( 68 ) Furthermore , if η = 4 ( √ µ + √ α ) 2 and β approaches β → (cid:16) 1 − 2 √ κ + 1 (cid:17) 2 from above , then it has a convergence rate approximately (cid:16) 1 − 2 √ κ + 1 (cid:17) as t → ∞ . Proof . The result ( 66 ) and ( 68 ) is due to a trivial combination of Lemma 1 , Theorem 6 , and Corollary 1 . On the other hand , set η = 4 ( √ µ + √ α ) 2 , the lower bound on β becomes max { (cid:0) 1 − √ ηµ (cid:1) 2 , (cid:0) 1 − √ ηα (cid:1) 2 } = (cid:16) 1 − 2 √ κ + 1 (cid:17) 2 . Since the rate is r = lim t →∞ 1 t log ( √ β t + 1 C 0 ) = √ β , setting β ↓ (cid:16) 1 − 2 √ κ + 1 (cid:17) 2 from above leads to the rate of (cid:16) 1 − 2 √ κ + 1 (cid:17) . Formally , it is straightforward to show that C 0 = Θ (cid:16) 1 / (cid:113) β − ( 1 − 2 1 + √ κ ) 2 (cid:17) , hence , for any β converges to ( 1 − 2 √ κ + 1 ) 2 slower than inverse exponential of κ , i . e . , β = ( 1 − 2 √ κ + 1 ) 2 + ( 1 κ ) o ( t ) , we have r = 1 − 2 √ κ + 1 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum E . Proof of Theorem 8 Proof . ( of Theorem 8 ) In the following , we denote ξ t : = w t − w ∗ and denote λ : = µ > 0 , which is a lower bound of λ min ( H ) of the matrix H : = (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + w ∗ (cid:1) dτ deﬁned in Lemma 2 , i . e . λ min ( H ) ≥ λ . Also , denote β ∗ : = 1 − 12 √ ηλ and θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . Suppose η = 1 α , where α is the smoothness constant . Denote C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } ≤ 4 √ κ by Corollary 1 . Let C 1 = C 3 = C 0 and C 2 = 14 √ ηλ in Theorem 6 . The goal is to show that (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) for all t by induction . To achieve this , we will also use induction to show that for all iterations s , (cid:107) w s − w ∗ (cid:107) ≤ R : = 3 64 √ κC 0 . ( 69 ) A sufﬁcient condition for the base case s = 0 of ( 69 ) to hold is (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) ≤ R 2 C 0 = 3 128 √ κC 20 , ( 70 ) as C 0 ≥ 1 by Theorem 5 , which in turn can be guaranteed if (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) ≤ 1 683 κ 3 / 2 by using the upper bound C 0 ≤ 4 √ κ of Corollary 1 . From Lemma 2 , we have (cid:107) φ s (cid:107) ≤ η (cid:107) (cid:90) 1 0 ∇ 2 f ( ( 1 − τ ) w s + τw ∗ ) dτ − (cid:90) 1 0 ∇ 2 f ( ( 1 − τ ) w 0 + τw ∗ ) dτ (cid:107)(cid:107) ξ s (cid:107) ( a ) ≤ ηα (cid:18)(cid:90) 1 0 ( 1 − τ ) (cid:107) w s − w 0 (cid:107) dτ (cid:19) (cid:107) ξ s (cid:107) ≤ ηα (cid:107) w s − w 0 (cid:107)(cid:107) ξ s (cid:107) ( b ) ≤ ηα ( (cid:107) w s − w ∗ (cid:107) + (cid:107) w 0 − w ∗ (cid:107) ) (cid:107) ξ s (cid:107) , ( 71 ) where ( a ) is by α - Lipschitzness of the Hessian and ( b ) is by the triangle inequality . By ( 69 ) , ( 71 ) , Lemma 2 , Theorem 6 , and Corollary 1 , it sufﬁces to show that given (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ s ξ s − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ s 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) and (cid:107) w s − w ∗ (cid:107) ≤ R : = 3 64 √ κC 0 hold at s = 0 , 1 , . . . , t − 1 , one has (cid:107) t − 1 (cid:88) s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ≤ θ t C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ( 72 ) (cid:107) w t − w ∗ (cid:107) ≤ R : = 3 64 √ κC 0 , ( 73 ) where A : = (cid:20) ( 1 + β ) I n − η (cid:82) 1 0 ∇ 2 f (cid:0) ( 1 − τ ) w 0 + w ∗ (cid:1) dτ − βI n I n 0 (cid:21) . We have (cid:107) t − 1 (cid:88) s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ≤ t − 1 (cid:88) s = 0 (cid:107) A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ( a ) ≤ t − 1 (cid:88) s = 0 β t − s − 1 ∗ C 0 (cid:107) ϕ s (cid:107) ( b ) ≤ 4 ηαRC 20 t − 1 (cid:88) s = 0 β t − s − 1 ∗ θ s (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( c ) ≤ RC 20 64 3 √ ηλθ t (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( d ) ≤ C 0 θ t (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( 74 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum where ( a ) uses Theorem 5 with β = β 2 ∗ , ( b ) is by ( 71 ) , ( 69 ) , and the induction that (cid:107) ξ s (cid:107) ≤ θ s 2 C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( c ) is because (cid:80) t − 1 s = 0 β t − 1 − s ∗ θ s = θ t − 1 (cid:80) t − 1 s = 0 (cid:16) β ∗ θ (cid:17) t − 1 − s ≤ θ t − 1 (cid:80) t − 1 s = 0 θ t − 1 − s ≤ θ t − 1 4 √ ηλ ≤ θ t 16 3 √ ηλ , and ( d ) is due to the deﬁnition of R : = 3 64 √ κC 0 . Therefore , by Theorem 6 , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) . Now let us switch to show ( 73 ) . We have (cid:107) ξ t (cid:107) : = (cid:107) w t − w ∗ (cid:107) induction ≤ θ t 2 C 0 (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) ≤ R , ( 75 ) where the last inequality uses the constraint (cid:107) (cid:20) w 0 − w ∗ w − 1 − w ∗ (cid:21) (cid:107) ≤ R 2 C 0 by ( 70 ) . A Modular Analysis of Provable Acceleration via Polyak’s Momentum F . Proof of Theorem 9 We will need some supporting lemmas in the following for the proof . In the following analysis , we denote C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } , where h ( β , · ) is deﬁned in Theorem 5 and H = H 0 whose ( i , j ) entry is ( H 0 ) i , j : = H ( W 0 ) i , j = 1 m (cid:80) mr = 1 x (cid:62) i x j 1 { (cid:104) w ( r ) 0 , x i (cid:105) ≥ 0 & (cid:104) w ( r ) 0 , x j (cid:105) ≥ 0 } , as deﬁned in Lemma 3 . In the following , we also denote β = ( 1 − 12 √ ηλ ) 2 : = β 2 ∗ . We summarize the notations in Table 1 . Notation deﬁnition ( or value ) meaning N ReLU W ( x ) N ReLU W ( x ) : = 1 √ m (cid:80) mr = 1 a r σ ( (cid:104) w ( r ) , x (cid:105) ) the ReLU network’s output given x ¯ H ¯ H i , j : = E w ( r ) [ x (cid:62) i x j 1 { (cid:104) w ( r ) , x i (cid:105) ≥ 0 & (cid:104) w ( r ) , x j (cid:105) ≥ 0 } ] . the expectation of the Gram matrix H 0 H ( W 0 ) i , j = 1 m (cid:80) mr = 1 x (cid:62) i x j 1 { (cid:104) w ( r ) 0 , x i (cid:105) ≥ 0 & (cid:104) w ( r ) 0 , x j (cid:105) ≥ 0 } the Gram matrix at the initialization λ min ( ¯ H ) λ min ( ¯ H ) > 0 ( by assumption ) the least eigenvalue of ¯ H . λ max ( ¯ H ) the largest eigenvalue of ¯ H κ κ : = λ max ( ¯ H ) / λ min ( ¯ H ) the condition number of ¯ H λ λ : = 34 λ min ( ¯ H ) ( a lower bound of ) the least eigenvalue of H 0 . λ max λ max : = λ max ( ¯ H ) + λ min ( ¯ H ) 4 ( an upper bound of ) the largest eigenvalue of H 0 . ˆ κ ˆ κ : = λ max λ = 43 κ + 13 the condition number of H 0 . η η = 1 / λ max step size β β = ( 1 − 12 √ ηλ ) 2 = ( 1 − 1 2 √ ˆ κ ) 2 : = β 2 ∗ momentum parameter β ∗ β ∗ = √ β = 1 − 12 √ ηλ squared root of β θ θ = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ = 1 − 1 4 √ ˆ κ the convergence rate C 0 C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( H 0 ) ) , h ( β , ηλ max ( H 0 ) ) } the constant used in Theorem 5 Table 1 . Summary of the notations for proving Theorem 9 . Lemma 8 . Suppose that the neurons w ( 1 ) 0 , . . . , w ( m ) 0 are i . i . d . generated by N ( 0 , I d ) initially . Then , for any set of weight vectors W t : = { w ( 1 ) t , . . . , w ( m ) t } that satisfy for any r ∈ [ m ] , (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R ReLU : = λ 1024 nC 0 , it holds that (cid:107) H t − H 0 (cid:107) F ≤ 2 nR ReLU = λ 512 C 0 , with probability at least 1 − n 2 · exp ( − mR ReLU / 10 ) . Proof . This is an application of Lemma 3 . 2 in ( Song & Yang , 2019 ) . Lemma 8 shows that if the distance between the current iterate W t and its initialization W 0 is small , then the distance between the Gram matrix H ( W t ) and H ( W 0 ) should also be small . Lemma 8 allows us to obtain the following lemma , which bounds the size of ϕ t ( deﬁned in Lemma 3 ) in the residual dynamics . Lemma 9 . Following the setting as Theorem 9 , denote θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . Suppose that ∀ i ∈ [ n ] , | S ⊥ i | ≤ 4 mR ReLU for some constant R ReLU : = λ 1024 nC 0 > 0 . If we have ( I ) for any s ≤ t , the residual dynamics satisﬁes (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s · νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , for some constant ν > 0 , and ( II ) for any r ∈ [ m ] and any s ≤ t , (cid:107) w ( r ) s − w ( r ) 0 (cid:107) ≤ R ReLU , then φ t and ι t in Lemma 3 satisﬁes (cid:107) φ t (cid:107) ≤ √ ηλ 16 θ t ν (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , and (cid:107) ι t (cid:107) ≤ ηλ 512 θ t ν (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . Consequently , ϕ t in Lemma 3 satisﬁes (cid:107) ϕ t (cid:107) ≤ (cid:18) √ ηλ 16 + ηλ 512 (cid:19) θ t ν (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Proof . Denote β ∗ : = 1 − 12 √ ηλ and θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . We have by Lemma 3 (cid:107) φ t (cid:107) = (cid:118)(cid:117)(cid:117)(cid:116) n (cid:88) i = 1 φ t [ i ] 2 ≤ (cid:118)(cid:117)(cid:117)(cid:116) n (cid:88) i = 1 (cid:0) 2 η √ n | S ⊥ i | m (cid:0) (cid:107) ξ t (cid:107) + β t − 1 (cid:88) τ = 0 β t − 1 − τ (cid:107) ξ τ (cid:107) (cid:1)(cid:1) 2 ( a ) ≤ 8 ηnR ReLU (cid:0) (cid:107) ξ t (cid:107) + β t − 1 (cid:88) τ = 0 β t − 1 − τ (cid:107) ξ τ (cid:107) (cid:1) ( b ) ≤ 8 ηnR ReLU (cid:32) θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + β t − 1 (cid:88) τ = 0 β t − 1 − τ θ τ νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) (cid:33) ( c ) = 8 ηnR ReLU (cid:32) θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + β 2 ∗ νC 0 t − 1 (cid:88) τ = 0 β 2 ( t − 1 − τ ) ∗ θ τ (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) (cid:33) ( d ) ≤ 8 ηnR ReLU (cid:32) θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + β 2 ∗ νC 0 θ t − 1 t − 1 (cid:88) τ = 0 θ t − 1 − τ (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) (cid:33) ≤ 8 ηnR ReLU θ t ( 1 + β ∗ t − 1 (cid:88) τ = 0 θ τ ) νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ≤ 8 ηnR ReLU θ t ( 1 + β ∗ 1 − θ ) νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( e ) ≤ √ ηλ 16 θ t ν (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( 76 ) where ( a ) is by | S ⊥ i | ≤ 4 mR ReLU , ( b ) is by induction that (cid:107) ξ t (cid:107) ≤ θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) as u 0 = u − 1 , ( c ) uses that β = β 2 ∗ , ( d ) uses β ∗ ≤ θ , ( e ) uses 1 + β ∗ 1 − θ ≤ 21 − θ ≤ 8 √ ηλ and R ReLU : = λ 1024 nC 0 . Now let us switch to bound (cid:107) ι t (cid:107) . (cid:107) ι t (cid:107) ≤ η (cid:107) H 0 − H t (cid:107) 2 (cid:107) ξ t (cid:107) ≤ ηλ 512 C 0 θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( 77 ) where we uses Lemma 8 that (cid:107) H 0 − H t (cid:107) 2 ≤ λ 512 C 0 and the induction that (cid:107) (cid:20) ξ t ξ t − 1 (cid:21) (cid:107) ≤ θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . The assumption of Lemma 9 , ∀ i ∈ [ n ] , | S ⊥ i | ≤ 4 mR ReLU only depends on the initialization . Lemma 11 shows that it holds with probability at least 1 − n · exp ( − mR ReLU ) . Lemma 10 . Following the setting as Theorem 9 , denote θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . Suppose that the initial error satisﬁes (cid:107) ξ 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) . If for any s < t , the residual dynamics satisﬁes (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s · νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , for some constant ν > 0 , then (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R ReLU : = λ 1024 nC 0 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Proof . We have (cid:107) w ( r ) t + 1 − w ( r ) 0 (cid:107) ( a ) ≤ η t (cid:88) s = 0 (cid:107) M ( r ) s (cid:107) ( b ) = η t (cid:88) s = 0 (cid:107) s (cid:88) τ = 0 β s − τ ∂L ( W τ ) ∂w ( r ) τ (cid:107) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ (cid:107) ∂L ( W τ ) ∂w ( r ) τ (cid:107) ( c ) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ √ n √ m (cid:107) y − u τ (cid:107) ( d ) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ √ 2 n √ m θ τ νC 0 (cid:107) y − u 0 (cid:107) ( e ) ≤ η √ 2 n √ m t (cid:88) s = 0 θ s 1 − θνC 0 (cid:107) y − u 0 (cid:107) ≤ η √ 2 n √ m (cid:18) νC 0 ( 1 − θ ) 2 (cid:19) (cid:107) y − u 0 (cid:107) ( f ) = η √ 2 n √ m (cid:18) 16 νC 0 ηλ (cid:19) (cid:107) y − u 0 (cid:107) ( g ) = η √ 2 n √ m (cid:18) 16 νC 0 ηλ (cid:19) O ( (cid:113) n log ( m / δ ) log 2 ( n / δ ) ) ( h ) ≤ λ 1024 nC 0 , ( 78 ) where ( a ) , ( b ) is by the update rule of momentum , which is w ( r ) t + 1 − w ( r ) t = − ηM ( r ) t , where M ( r ) t : = (cid:80) ts = 0 β t − s ∂L ( W s ) ∂w ( r ) s , ( c ) is because (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) = (cid:107) (cid:80) ni = 1 ( y i − u s [ i ] ) 1 √ m a r x i · 1 { (cid:104) w ( r ) s , x (cid:105) ≥ 0 } (cid:107) ≤ 1 √ m (cid:80) ni = 1 | y i − u s [ i ] | ≤ √ n √ m (cid:107) y − u s (cid:107) , ( d ) is by (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( e ) is because that β = β 2 ∗ ≤ θ 2 , ( f ) we use θ : = ( 1 − 14 √ ηλ ) , so that 1 ( 1 − θ ) 2 = 16 ηλ , ( g ) is by that the initial error satisﬁes (cid:107) y − u 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) , and ( h ) is by the choice of the number of neurons m = Ω ( λ − 4 n 4 C 40 log 3 ( n / δ ) ) = Ω ( λ − 4 n 4 κ 2 log 3 ( n / δ ) ) , as C 0 = Θ ( √ κ ) by Corollary 1 . The proof is completed . Lemma 10 basically says that if the size of the residual errors is bounded and decays over iterations , then the distance between the current iterate W t and its initialization W 0 is well - controlled . The lemma will allows us to invoke Lemma 8 and Lemma 9 when proving Theorem 9 . The proof of Lemma 10 is in Appendix F . The assumption of Lemma 10 , (cid:107) ξ 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) , is satisﬁed by the random initialization with probability at least 1 − δ / 3 according to Lemma 12 . Lemma 11 . ( Claim 3 . 12 of ( Song & Yang , 2019 ) ) Fix a number R 1 ∈ ( 0 , 1 ) . Recall that S ⊥ i is a random set deﬁned in Subsection 3 . 3 . With probability at least 1 − n · exp ( − mR 1 ) , we have that for all i ∈ [ n ] , | S ⊥ i | ≤ 4 mR 1 . A similar lemma also appears in ( Du et al . , 2019b ) . Lemma 11 says that the number of neurons whose activation patterns for a sample i could change during the execution is only a small faction of m if R 1 is a small number , i . e . | S ⊥ i | ≤ 4 mR 1 (cid:28) m . Lemma 12 . ( Claim 3 . 10 in ( Song & Yang , 2019 ) ) Assume that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . For 0 < δ < 1 , we have that (cid:107) y − u 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) , with probability at least 1 − δ . F . 1 . Proof of Theorem 9 Proof . ( of Theorem 9 ) Denote λ : = 34 λ min ( ¯ H ) > 0 . Lemma 5 shows that λ is a lower bound of λ min ( H ) of the matrix H deﬁned in Lemma 3 . Also , denote β ∗ : = 1 − 12 √ ηλ ( note that β = β 2 ∗ ) and θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . In the A Modular Analysis of Provable Acceleration via Polyak’s Momentum following , we let ν = 2 in Lemma 9 , 10 , and let C 1 = C 3 = C 0 and C 2 = 14 √ ηλ in Theorem 6 . The goal is to show that (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) for all t by induction . To achieve this , we will also use induction to show that for all iterations s , ∀ r ∈ [ m ] , (cid:107) w ( r ) s − w ( r ) 0 (cid:107) ≤ R ReLU : = λ 1024 nC 0 , ( 79 ) which is clear true in the base case s = 0 . By Lemma 3 , 5 , 8 , 9 , Theorem 6 , and Corollary 1 , it sufﬁces to show that given (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ s ξ s − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ s 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) and ( 79 ) hold at s = 0 , 1 , . . . , t − 1 , one has (cid:107) (cid:80) t − 1 s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ≤ θ t C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) , ( 80 ) ∀ r ∈ [ m ] , (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R ReLU : = λ 1024 nC 0 , ( 81 ) where the matrix A and the vector ϕ t are deﬁned in Lemma 3 . The inequality ( 80 ) is the required condition for using the result of Theorem 6 , while the inequality ( 81 ) helps us to show ( 80 ) through invoking Lemma 9 to bound the terms { ϕ s } as shown in the following . We have (cid:107) t − 1 (cid:88) s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ( a ) ≤ t − 1 (cid:88) s = 0 β t − s − 1 ∗ C 0 (cid:107) ϕ s (cid:107) ( b ) ≤ (cid:18) √ ηλ 16 + ηλ 512 (cid:19) 2 C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) (cid:32) t − 1 (cid:88) s = 0 β t − 1 − s ∗ θ s (cid:33) ( c ) ≤ (cid:18) 1 2 + 1 64 (cid:112) ηλ (cid:19) θ t − 1 C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( d ) ≤ θ t C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( 82 ) where ( a ) uses Theorem 5 , ( b ) is due to Lemma 9 , Lemma 11 , ( c ) is because (cid:80) t − 1 s = 0 β t − 1 − s ∗ θ s = θ t − 1 (cid:80) t − 1 s = 0 (cid:16) β ∗ θ (cid:17) t − 1 − s ≤ θ t − 1 (cid:80) t − 1 s = 0 θ t − 1 − s ≤ θ t − 1 4 √ ηλ , ( d ) uses that θ ≥ 34 and ηλ ≤ 1 . Hence , we have shown ( 80 ) . Therefore , by Theorem 6 , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) . By Lemma 10 and Lemma 12 , we have ( 81 ) . Furthermore , with the choice of m , we have 3 n 2 exp ( − mR ReLU / 10 ) ≤ δ . Thus , we have completed the proof . A Modular Analysis of Provable Acceleration via Polyak’s Momentum G . Proof of Theorem 10 We will need some supporting lemmas in the following for the proof . In the following analysis , we denote C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( H ) ) , h ( β , ηλ max ( H ) ) } , where h ( β , · ) is the constant deﬁned in Theorem 5 and H = H 0 : = 1 m L − 1 d y (cid:80) Ll = 1 [ ( W ( l − 1 : 1 ) 0 X ) (cid:62) ( W ( l − 1 : 1 ) 0 X ) ⊗ W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ] ∈ R d y n × d y n , as deﬁned in Lemma 4 . We also denote β = ( 1 − 12 √ ηλ ) 2 : = β 2 ∗ . As mentioned in the main text , following Du & Hu ( 2019 ) ; Hu et al . ( 2020b ) , we will further assume that ( A1 ) there exists a W ∗ such that Y = W ∗ X , X ∈ R d × ¯ r , and ¯ r = rank ( X ) , which is actually without loss of generality ( see e . g . the discussion in Appendix B of Du & Hu ( 2019 ) ) . We summarize the notions in Table 2 . Notation deﬁnition ( or value ) meaning N L - linear W ( x ) N L - linear W ( x ) : = 1 √ m L − 1 d y W ( L ) W ( L − 1 ) · · · W ( 1 ) x , output of the deep linear network H 0 H 0 : = 1 m L − 1 d y (cid:80) Ll = 1 [ ( W ( l − 1 : 1 ) 0 X ) (cid:62) ( W ( l − 1 : 1 ) 0 X ) ⊗ W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ] ∈ R d y n × d y n H in ( 8 ) is H = H 0 ( Lemma 4 ) λ max ( H 0 ) λ max ( H 0 ) ≤ Lσ 2max ( X ) / d y ( Lemma 13 ) the largest eigenvalue of H 0 λ min ( H 0 ) λ min ( H 0 ) ≥ Lσ 2min ( X ) / d y ( Lemma 13 ) the least eigenvalue of H 0 λ λ : = Lσ 2min ( X ) / d y ( a lower bound of ) the least eigenvalue of H 0 κ κ : = λ 1 ( X (cid:62) X ) λ ¯ r ( X (cid:62) X ) = σ 2max ( X ) σ 2min ( X ) ( A1 ) the condition number of the data matrix X ˆ κ ˆ κ : = λ max ( H 0 ) λ min ( H 0 ) ≤ σ 2max ( X ) σ 2min ( X ) = κ ( Lemma 13 ) the condition number of H 0 η η = d y Lσ 2max ( X ) step size β β = ( 1 − 12 √ ηλ ) 2 = ( 1 − 1 2 √ κ ) 2 : = β 2 ∗ momentum parameter β ∗ β ∗ = √ β = 1 − 12 √ ηλ squared root of β θ θ = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ = 1 − 1 4 √ κ the convergence rate C 0 C 0 : = √ 2 ( β + 1 ) √ min { h ( β , ηλ min ( H 0 ) ) , h ( β , ηλ max ( H 0 ) ) } the constant used in Theorem 5 Table 2 . Summary of the notations for proving Theorem 10 . We will simply use κ to represent the condition number of the matrix H 0 in the analysis since we have ˆ κ ≤ κ . Lemma 13 . [ Lemma 4 . 2 in ( Hu et al . , 2020b ) ] By the orthogonal initialization , we have λ min ( H 0 ) ≥ Lσ 2min ( X ) / d y , λ max ( H 0 ) ≤ Lσ 2max ( X ) / d y . σ max ( W ( j : i ) 0 ) = m j − i + 1 2 , σ min ( W ( j : i ) 0 ) = m j − i + 1 2 Furthermore , with probability 1 − δ , (cid:96) ( W 0 ) ≤ B 2 0 = O (cid:18) 1 + log ( ¯ r / δ ) d y + (cid:107) W ∗ (cid:107) 2 2 (cid:19) , for some constant B 0 > 0 . We remark that Lemma 13 implies that the condition number of H 0 satisﬁes ˆ κ : = λ max ( H 0 ) λ min ( H 0 ) ≤ σ 2max ( X ) σ 2min ( X ) = κ . ( 83 ) Lemma 14 . Following the setting as Theorem 10 , denote θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . If we have ( I ) for any s ≤ t , the residual dynamics satisﬁes (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s · νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , for some constant ν > 0 , and ( II ) for all l ∈ [ L ] and for any s ≤ t , (cid:107) W ( l ) s − W ( l ) 0 (cid:107) F ≤ R L - linear : = 64 (cid:107) X (cid:107) 2 √ d y Lσ 2min ( X ) νC 0 B 0 , then (cid:107) φ t (cid:107) ≤ 43 (cid:112) d y √ m (cid:107) X (cid:107) 2 θ 2 t ν 2 C 20 (cid:18) (cid:107) ξ 0 (cid:107) 1 − θ (cid:19) 2 , (cid:107) ψ t (cid:107) ≤ 43 (cid:112) d y √ m (cid:107) X (cid:107) 2 θ 2 ( t − 1 ) ν 2 C 20 (cid:18) (cid:107) ξ 0 (cid:107) 1 − θ (cid:19) 2 , (cid:107) ι t (cid:107) ≤ ηλ 80 θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Consequently , ϕ t in Lemma 4 satisﬁes (cid:107) ϕ t (cid:107) ≤ 1920 (cid:112) d y √ m (cid:107) X (cid:107) 2 1 ηλθ 2 t ν 2 C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) 2 + ηλ 80 θ t νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . Proof . By Lemma 4 , ϕ t = φ t + ψ t + ι t ∈ R d y n , we have φ t : = 1 (cid:112) m L − 1 d y vec ( Φ t X ) , with Φ t : = Π l (cid:16) W ( l ) t − ηM t , l (cid:17) − W ( L : 1 ) t + η L (cid:88) l = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) t , ( 84 ) and ψ t : = 1 (cid:112) m L − 1 d y vec (cid:32) ( L − 1 ) βW ( L : 1 ) t X + βW ( L : 1 ) t − 1 X − β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X (cid:33) . ( 85 ) and ι t : = η ( H 0 − H t ) ξ t . ( 86 ) So if we can bound (cid:107) φ t (cid:107) , (cid:107) ψ t (cid:107) , and (cid:107) ι t (cid:107) respectively , then we can bound (cid:107) ϕ t (cid:107) by the triangle inequality . (cid:107) ϕ t (cid:107) ≤ (cid:107) φ t (cid:107) + (cid:107) ψ t (cid:107) + (cid:107) ι t (cid:107) . ( 87 ) Let us ﬁrst upper - bound (cid:107) φ t (cid:107) . Note that Φ t is the sum of all the high - order ( of η ’s ) term in the product , W ( L : 1 ) t + 1 = Π l (cid:16) W ( l ) t − ηM t , l (cid:17) = W ( L : 1 ) t − η L (cid:88) l = 1 W ( L : l + 1 ) t M t , l W ( l − 1 : 1 ) + Φ t . ( 88 ) By induction , we can bound the gradient norm of each layer as (cid:107) ∂(cid:96) ( W ( L : 1 ) s ) ∂W ( l ) s (cid:107) F ≤ 1 (cid:112) m L − 1 d y (cid:107) W ( L : l + 1 ) s (cid:107) 2 (cid:107) U s − Y (cid:107) F (cid:107) W ( l − 1 : 1 ) s (cid:107) 2 (cid:107) X (cid:107) 2 ≤ 1 (cid:112) m L − 1 d y 1 . 1 m L − l 2 θ s νC 0 2 √ 2 (cid:107) U 0 − Y (cid:107) F 1 . 1 m l − 12 (cid:107) X (cid:107) 2 ≤ 4 (cid:107) X (cid:107) 2 (cid:112) d y θ s νC 0 (cid:107) U 0 − Y (cid:107) F , ( 89 ) where the second inequality we use Lemma 16 and that (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) and (cid:107) ξ s (cid:107) = (cid:107) U s − Y (cid:107) F . So the momentum term of each layer can be bounded as (cid:107) M t , l (cid:107) F = (cid:107) t (cid:88) s = 0 β t − s ∂(cid:96) ( W ( L : 1 ) s ) ∂W ( l ) s (cid:107) F ≤ t (cid:88) s = 0 β t − s (cid:107) ∂(cid:96) ( W ( L : 1 ) s ) ∂W ( l ) s (cid:107) F ≤ 4 (cid:107) X (cid:107) 2 (cid:112) d y t (cid:88) s = 0 β t − s θ s νC 0 (cid:107) U 0 − Y (cid:107) F . ≤ 4 (cid:107) X (cid:107) 2 (cid:112) d y t (cid:88) s = 0 θ 2 ( t − s ) θ s νC 0 (cid:107) U 0 − Y (cid:107) F . ≤ 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F , ( 90 ) where in the second to last inequality we use β = β 2 ∗ ≤ θ 2 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Combining all the pieces together , we can bound (cid:107) 1 √ m L − 1 d y Φ t X (cid:107) F as (cid:107) 1 (cid:112) m L − 1 d y Φ t X (cid:107) F ( a ) ≤ 1 (cid:112) m L − 1 d y L (cid:88) j = 2 (cid:18) L j (cid:19) (cid:32) η 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j ( 1 . 1 ) j + 1 m L − j 2 (cid:107) X (cid:107) 2 ( b ) ≤ 1 . 1 1 (cid:112) m L − 1 d y L (cid:88) j = 2 L j (cid:32) η 4 . 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j m L − j 2 (cid:107) X (cid:107) 2 ≤ 1 . 1 (cid:114) m d y (cid:107) X (cid:107) 2 L (cid:88) j = 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j , ( 91 ) where ( a ) uses ( 90 ) and Lemma 16 for bounding a j ≥ 2 higher - order terms like 1 √ m L − 1 d y βW ( L : k j + 1 ) t · ( − ηM t , k j ) W ( k j − 1 : k j − 1 + 1 ) t · ( − ηM t , k j − 1 ) · · · ( − ηM t , k 1 ) · W ( k 1 − 1 : 1 ) t , where 1 ≤ k 1 < · · · < k j ≤ L and ( b ) uses that (cid:0) Lj (cid:1) ≤ L j j ! To proceed , let us bound η 4 . 4 L (cid:107) X (cid:107) 2 √ md y θ t 1 − θ νC 0 (cid:107) U 0 − Y (cid:107) F in the sum above . We have η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F ≤ 4 . 4 (cid:114) d y m 1 (cid:107) X (cid:107) 2 θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F ≤ 0 . 5 , ( 92 ) where the last inequality uses that ˜ C 1 d y B 20 C 20 (cid:107) X (cid:107) 22 1 ( 1 − θ ) 2 ≤ ˜ C 1 d y B 20 C 20 (cid:107) X (cid:107) 22 1 ηλ ≤ ˜ C 2 d y B 20 κ 2 (cid:107) X (cid:107) 22 ≤ m , for some sufﬁciently large constant ˜ C 1 , ˜ C 2 > 0 . Combining the above results , we have (cid:107) φ t (cid:107) = (cid:107) 1 (cid:112) m L − 1 d y Φ t X (cid:107) F ≤ 1 . 1 (cid:114) m d y (cid:107) X (cid:107) 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) 2 L − 2 (cid:88) j = 2 ( 0 . 5 ) j − 2 ≤ 2 . 2 (cid:114) m d y (cid:107) X (cid:107) 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) 2 ≤ 43 (cid:112) d y √ m (cid:107) X (cid:107) 2 (cid:18) θ t 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:19) 2 . ( 93 ) Now let us switch to upper - bound (cid:107) ψ t (cid:107) . It is equivalent to upper - bounding the Frobenius norm of 1 √ m L − 1 d y β ( L − 1 ) W ( L : 1 ) t X + 1 √ m L − 1 d y βW ( L : 1 ) t − 1 X − 1 √ m L − 1 d y β (cid:80) Ll = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X , which can be rewritten as 1 (cid:112) m L − 1 d y β ( L − 1 ) · Π Ll = 1 (cid:16) W ( l ) t − 1 − ηM t − 1 , l (cid:17) X (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term + 1 (cid:112) m L − 1 d y βW ( L : 1 ) t − 1 X (cid:124) (cid:123)(cid:122) (cid:125) second term − 1 (cid:112) m L − 1 d y β L (cid:88) l = 1 Π Li = l + 1 (cid:16) W ( i ) t − 1 − ηM t − 1 , i (cid:17) W ( l ) t − 1 Π l − 1 j = 1 (cid:16) W ( j ) t − 1 − ηM t − 1 , j (cid:17) X (cid:124) (cid:123)(cid:122) (cid:125) third term . ( 94 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum The above can be written as B 0 + ηB 1 + η 2 B 2 + · · · + η L B L for some matrices B 0 , . . . , B L ∈ R d y × n . Speciﬁcally , we have B 0 = 1 (cid:112) m L − 1 d y ( L − 1 ) βW ( L : 1 ) t − 1 X (cid:124) (cid:123)(cid:122) (cid:125) due to the ﬁrst term + 1 (cid:112) m L − 1 d y βW ( L : 1 ) t − 1 X (cid:124) (cid:123)(cid:122) (cid:125) due to the second term − 1 (cid:112) m L − 1 d y βLW ( L : 1 ) t − 1 X (cid:124) (cid:123)(cid:122) (cid:125) due to the third term = 0 B 1 = − 1 (cid:112) m L − 1 d y ( L − 1 ) β L (cid:88) l = 1 W ( L : l + 1 ) t − 1 M t − 1 , l W ( l − 1 : 1 ) t − 1 (cid:124) (cid:123)(cid:122) (cid:125) due to the ﬁrst term + 1 (cid:112) m L − 1 d y β L (cid:88) l = 1 (cid:88) k (cid:54) = l W ( L : k + 1 ) t − 1 M t − 1 , k W ( k − 1 : 1 ) t − 1 (cid:124) (cid:123)(cid:122) (cid:125) due to the third term = 0 . ( 95 ) So what remains on ( 94 ) are all the higher - order terms ( in terms of the power of η ) , i . e . those with ηM t − 1 , i and ηM t − 1 , j , ∀ i (cid:54) = j or higher . To continue , observe that for a ﬁxed ( i , j ) , i < j , the second - order term that involves ηM t − 1 , i and ηM t − 1 , j on ( 94 ) is with coefﬁcient 1 √ m L − 1 d y β , because the ﬁrst term on ( 94 ) contributes to 1 √ m L − 1 d y ( L − 1 ) β , while the third term on ( 94 ) contributes to − 1 √ m L − 1 d y ( L − 2 ) β . Furthermore , for a ﬁxed ( i , j , k ) , i < j < k , the third - order term that involves ηM t − 1 , i , ηM t − 1 , j , and ηM t − 1 , k on ( 94 ) is with coefﬁcient − 2 1 √ m L − 1 d y β , as the ﬁrst term on ( 94 ) contributes to − 1 √ m L − 1 d y ( L − 1 ) β , while the third term on ( 94 ) contributes to 1 √ m L − 1 d y ( L − 3 ) β . Similarly , for a p - order term η M t − 1 , ∗ , · · · , ηM t − 1 , ∗∗ (cid:124) (cid:123)(cid:122) (cid:125) p terms , the coefﬁcient is ( p − 1 ) 1 √ m L − 1 d y β ( − 1 ) p . By induction ( see ( 90 ) ) , we can bound the norm of the momentum at layer l as (cid:107) M t − 1 , l (cid:107) F ≤ 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F . ( 96 ) Combining all the pieces together , we have (cid:107) 1 (cid:112) m L − 1 d y β ( L − 1 ) W ( L : 1 ) t X + 1 (cid:112) m L − 1 d y βW ( L : 1 ) t − 1 X − 1 (cid:112) m L − 1 d y β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X (cid:107) F ( a ) ≤ β (cid:112) m L − 1 d y L (cid:88) j = 2 ( j − 1 ) (cid:18) L j (cid:19) (cid:32) η 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j ( 1 . 1 ) j + 1 m L − j 2 (cid:107) X (cid:107) 2 ( b ) ≤ 1 . 1 β (cid:112) m L − 1 d y L (cid:88) j = 2 L j (cid:32) η 4 . 4 (cid:107) X (cid:107) 2 (cid:112) d y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j m L − j 2 (cid:107) X (cid:107) 2 ≤ 1 . 1 β (cid:114) m d y (cid:107) X (cid:107) 2 L (cid:88) j = 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) j , ( 97 ) where ( a ) uses ( 96 ) , the above analysis of the coefﬁcients of the higher - order terms and Lemma 16 for bounding a j ≥ 2 higher - order terms like 1 √ m L − 1 d y β ( j − 1 ) ( − 1 ) j W ( L : k j + 1 ) t − 1 · ( − ηM t − 1 , k j ) W ( k j − 1 : k j − 1 + 1 ) t − 1 · ( − ηM t − 1 , k j − 1 ) · · · ( − ηM t − 1 , k 1 ) · W ( k 1 − 1 : 1 ) t − 1 , where 1 ≤ k 1 < · · · < k j ≤ L and ( b ) uses that (cid:0) Lj (cid:1) ≤ L j j ! Let us bound η 4 . 4 L (cid:107) X (cid:107) 2 √ md y θ t − 1 1 − θ νC 0 (cid:107) U 0 − Y (cid:107) F in the sum above . We have η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F ≤ 4 . 4 (cid:114) d y m 1 (cid:107) X (cid:107) 2 θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F ≤ 0 . 5 , ( 98 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum where the last inequality uses that ˜ C 1 d y B 20 C 20 (cid:107) X (cid:107) 22 1 ( 1 − θ ) 2 ≤ ˜ C 1 d y B 20 C 20 (cid:107) X (cid:107) 22 1 ηλ ≤ ˜ C 2 d y B 20 κ 2 (cid:107) X (cid:107) 22 ≤ m , for some sufﬁciently large constant ˜ C 1 , ˜ C 2 > 0 . Combining the above results , i . e . ( 97 ) and ( 98 ) , we have (cid:107) ψ t (cid:107) ≤ (cid:107) 1 (cid:112) m L − 1 d y β ( L − 1 ) W ( L : 1 ) t X + 1 (cid:112) m L − 1 d y βW ( L : 1 ) t − 1 X − 1 (cid:112) m L − 1 d y β L (cid:88) l = 1 W ( L : l + 1 ) t W ( l ) t − 1 W ( l − 1 : 1 ) t X (cid:107) F ≤ 1 . 1 β (cid:114) m d y (cid:107) X (cid:107) 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) 2 L − 2 (cid:88) j = 2 ( 0 . 5 ) j − 2 ≤ 2 . 2 β (cid:114) m d y (cid:107) X (cid:107) 2 (cid:32) η 4 . 4 L (cid:107) X (cid:107) 2 (cid:112) md y θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:33) 2 ≤ 43 (cid:112) d y √ m (cid:107) X (cid:107) 2 (cid:18) θ t − 1 1 − θνC 0 (cid:107) U 0 − Y (cid:107) F (cid:19) 2 , ( 99 ) where the last inequality uses η ≤ d y L (cid:107) X (cid:107) 22 . Now let us switch to bound (cid:107) ι t (cid:107) . We have (cid:107) ι t (cid:107) = (cid:107) η ( H t − H 0 ) ξ t (cid:107) = η m L − 1 d y (cid:107) L (cid:88) l = 1 W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − L (cid:88) l = 1 W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ( U t − Y ) ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:107) F ≤ η m L − 1 d y L (cid:88) l = 1 (cid:107) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ( U t − Y ) ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:107) F ≤ η m L − 1 d y L (cid:88) l = 1 (cid:0) (cid:107) (cid:16) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:17) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) F (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term + (cid:107) W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ( U t − Y ) (cid:16) W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:17) (cid:107) F (cid:1) (cid:124) (cid:123)(cid:122) (cid:125) second term . ( 100 ) Now let us bound the ﬁrst term . We have (cid:107) (cid:16) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:17) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) F (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term ≤ (cid:107) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 (cid:107) U t − Y (cid:107) F (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) 2 . ( 101 ) For (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) 2 , by using Lemma 15 and Lemma 16 , we have (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) 2 ≤ (cid:16) σ max ( W ( l − 1 : 1 ) t X ) (cid:17) 2 ≤ (cid:16) 1 . 1 m l − 12 σ max ( X ) (cid:17) 2 . ( 102 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum For (cid:107) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 , denote W ( L : l + 1 ) t = W ( L : l + 1 ) 0 + ∆ ( L : l + 1 ) t , we have (cid:107) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 ≤ (cid:107) ∆ ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) + W ( L : l + 1 ) t ( ∆ ( L : l + 1 ) t ) (cid:62) + ∆ ( L : l + 1 ) t ( ∆ ( L : l + 1 ) t ) (cid:62) (cid:107) 2 ≤ 2 (cid:107) ∆ ( L : l + 1 ) t (cid:107) 2 · σ max ( W ( L : l + 1 ) t ) + (cid:107) ∆ ( L : l + 1 ) t (cid:107) 22 ≤ 2 (cid:107) ∆ ( L : l + 1 ) t (cid:107) 2 · (cid:16) 1 . 1 m L − l 2 (cid:17) + (cid:107) ∆ ( L : l + 1 ) t (cid:107) 22 . ( 103 ) Therefore , we have to bound (cid:107) ∆ ( L : l + 1 ) t (cid:107) 2 . We have for any 1 ≤ i ≤ j ≤ L . W ( j : i ) t = (cid:16) W ( j ) 0 + ∆ j (cid:17) · · · (cid:16) W ( i ) 0 + ∆ i (cid:17) , ( 104 ) where (cid:107) ∆ i (cid:107) 2 ≤ (cid:107) W ( i ) t − W ( i ) 0 (cid:107) F ≤ D : = 64 (cid:107) X (cid:107) 2 √ d y Lσ 2min ( X ) νC 0 B 0 by Lemma 15 . The product ( 104 ) above minus W ( j : i ) 0 can be written as a ﬁnite sum of some terms of the form W ( j : k l + 1 ) 0 ∆ k l W ( k l − 1 : k l − 1 + 1 ) 0 ∆ k l − 1 · · · ∆ k 1 W ( k 1 − 1 : i ) 0 , ( 105 ) where i ≤ k 1 < · · · < k l ≤ j . Recall that (cid:107) W ( j (cid:48) : i (cid:48) ) 0 (cid:107) 2 = m j (cid:48)− i (cid:48) + 1 2 by Lemma 13 . Thus , we can bound (cid:107) ∆ ( j : i ) t (cid:107) 2 ≤ (cid:107) W ( j : i ) t − W ( j : i ) 0 (cid:107) F ≤ j − i + 1 (cid:88) l = 1 (cid:18) j − i + 1 l (cid:19) ( D ) l m j − i + 1 − l 2 = ( √ m + D ) j − i + 1 − ( √ m ) j − i + 1 = ( √ m ) j − i + 1 (cid:0) ( 1 + D / √ m ) j − i + 1 − 1 (cid:1) ≤ ( √ m ) j − i + 1 (cid:0) ( 1 + D / √ m ) L − 1 (cid:1) ( a ) = (cid:18) ( 1 + 1 √ C (cid:48) Lκ ) L − 1 (cid:19) ( √ m ) j − i + 1 ( b ) ≤ (cid:18) exp (cid:18) 1 √ C (cid:48) κ (cid:19) − 1 (cid:19) ( √ m ) j − i + 1 ( c ) ≤ (cid:18) 1 + ( e − 1 ) 1 √ C (cid:48) κ − 1 (cid:19) ( √ m ) j − i + 1 ( d ) ≤ 1 480 κ ( √ m ) j − i + 1 , ( 106 ) where ( a ) uses D √ m ≤ 1 √ C (cid:48) Lκ , for some constant C (cid:48) > 0 , since C (cid:48) d y C 20 B 20 κ 4 (cid:107) X (cid:107) 22 ≤ C d y B 20 κ 5 (cid:107) X (cid:107) 22 ≤ m , ( b ) follows by the inequality ( 1 + x / n ) n ≤ e x , ∀ x ≥ 0 , n > 0 , ( c ) from Bernoulli’s inequality e r ≤ 1 + ( e − 1 ) r , ∀ 0 ≤ r ≤ 1 , and ( d ) by choosing any sufﬁciently larger C (cid:48) . From ( 106 ) , we have (cid:107) ∆ ( L : l + 1 ) t (cid:107) 2 ≤ 1 480 κ ( √ m ) L − l . Combining this with ( 101 ) , ( 102 ) , and ( 103 ) , we have (cid:107) (cid:16) W ( L : l + 1 ) t ( W ( L : l + 1 ) t ) (cid:62) − W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:17) ( U t − Y ) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) F (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term ≤ (cid:16) 2 (cid:107) ∆ ( L : l + 1 ) t (cid:107) 2 · (cid:16) 1 . 1 m L − l 2 (cid:17) + (cid:107) ∆ ( L : l + 1 ) t (cid:107) 22 (cid:17) (cid:16) 1 . 1 m l − 12 σ max ( X ) (cid:17) 2 (cid:107) U t − Y (cid:107) F ≤ (cid:18) 2 1 480 κ ( √ m ) L − l · (cid:16) 1 . 1 m L − l 2 (cid:17) + (cid:0) 1 480 κ ( √ m ) L − l (cid:1) 2 (cid:19) (cid:16) 1 . 1 m l − 12 σ max ( X ) (cid:17) 2 (cid:107) U t − Y (cid:107) F ≤ σ 2min ( X ) 160 m L − 1 (cid:107) U t − Y (cid:107) F , ( 107 ) where in the last inequality we use κ : = σ 2max ( X ) σ 2min ( X ) . Now let us switch to bound the second term , we have (cid:107) ( W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ( U t − Y ) (cid:16) W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:17) (cid:107) F (cid:1) (cid:124) (cid:123)(cid:122) (cid:125) second term ≤ (cid:107) ( W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 (cid:107) U t − Y (cid:107) F (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:107) 2 . ( 108 ) A Modular Analysis of Provable Acceleration via Polyak’s Momentum For (cid:107) W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 , based on Lemma 13 , we have (cid:107) W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) (cid:107) 2 ≤ m L − l . ( 109 ) To bound (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:107) 2 , we proceed as follows . Denote W ( l − 1 : 1 ) t = W ( l − 1 : 1 ) 0 + ∆ ( l − 1 : 1 ) t , we have (cid:107) ( W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:107) 2 ≤ 2 (cid:107) ( ∆ ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X (cid:107) 2 + (cid:107) ∆ ( l − 1 : 1 ) t X (cid:107) 22 ≤ (cid:16) 2 (cid:107) ∆ ( l − 1 : 1 ) t (cid:107)(cid:107) W ( l − 1 : 1 ) t (cid:107) 2 + (cid:107) ∆ ( l − 1 : 1 ) t (cid:107) 22 (cid:17) (cid:107) X (cid:107) 22 ≤ (cid:32) 2 1 480 κm l − 12 1 . 1 m l − 12 + (cid:18) 1 480 κm l − 12 (cid:19) 2 (cid:33) (cid:107) X (cid:107) 22 ≤ σ 2min ( X ) 160 m l − 1 , ( 110 ) where the second to last inequality uses ( 106 ) , Lemma 15 , and Lemma 16 , while the last inequality uses κ : = σ 2max ( X ) σ 2min ( X ) . Combining ( 108 ) , ( 109 ) , ( 110 ) , we have (cid:107) ( W ( L : l + 1 ) 0 ( W ( L : l + 1 ) 0 ) (cid:62) ( U t − Y ) (cid:16) W ( l − 1 : 1 ) t X ) (cid:62) W ( l − 1 : 1 ) t X − ( W ( l − 1 : 1 ) 0 X ) (cid:62) W ( l − 1 : 1 ) 0 X (cid:17) (cid:107) F (cid:1) (cid:124) (cid:123)(cid:122) (cid:125) second term ≤ σ 2min ( X ) 160 m L − 1 (cid:107) U t − Y (cid:107) F . ( 111 ) Now combing ( 100 ) , ( 107 ) , and ( 111 ) , we have (cid:107) ι t (cid:107) ≤ η m L − 1 d y Lσ 2min ( X ) 80 m L − 1 (cid:107) U t − Y (cid:107) F = ηλ 80 (cid:107) ξ t (cid:107) , ( 112 ) where we use λ : = Lσ 2min ( X ) d y . Now we have ( 93 ) , ( 99 ) , and ( 112 ) , which leads to (cid:107) ϕ t (cid:107) ≤ (cid:107) φ t (cid:107) + (cid:107) ψ t (cid:107) + (cid:107) ι t (cid:107) ≤ 43 (cid:112) d y √ m (cid:107) X (cid:107) 2 ( θ 2 t + θ 2 ( t − 1 ) ) ν 2 C 20 (cid:18) (cid:107) ξ 0 (cid:107) 1 − θ (cid:19) 2 + ηλ 80 νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . ≤ 1920 (cid:112) d y √ m (cid:107) X (cid:107) 2 1 ηλθ 2 t ν 2 C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) 2 + ηλ 80 νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) . ( 113 ) where the last inequality uses that 1 ≤ 169 θ 2 as ηλ ≤ 1 so that θ ≥ 34 . Lemma 15 . Following the setting as Theorem 10 , denote θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . If for any s ≤ t , the residual dynamics satisﬁes (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ θ s · νC 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , for some constant ν > 0 , then (cid:107) W ( l ) t − W ( l ) 0 (cid:107) F ≤ R L - linear : = 64 (cid:107) X (cid:107) 2 (cid:112) d y Lσ 2min ( X ) νC 0 B 0 . A Modular Analysis of Provable Acceleration via Polyak’s Momentum Proof . We have (cid:107) W ( l ) t + 1 − W ( l ) 0 (cid:107) F ( a ) ≤ η t (cid:88) s = 0 (cid:107) M s , l (cid:107) F ( b ) = η t (cid:88) s = 0 (cid:107) s (cid:88) τ = 0 β s − τ ∂(cid:96) ( W ( L : 1 ) τ ) ∂W ( L ) τ (cid:107) F ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ (cid:107) ∂(cid:96) ( W ( L : 1 ) τ ) ∂W ( L ) τ (cid:107) F ( c ) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β 2 ( s − τ ) ∗ 4 (cid:107) X (cid:107) 2 (cid:112) d y θ τ νC 0 (cid:107) U 0 − Y (cid:107) F . ( d ) ≤ η t (cid:88) s = 0 θ s 1 − θ 4 (cid:107) X (cid:107) 2 (cid:112) d y νC 0 (cid:107) U 0 − Y (cid:107) F . ≤ 4 η (cid:107) X (cid:107) 2 (cid:112) d y 1 ( 1 − θ ) ( 1 − θ ) νC 0 (cid:107) U 0 − Y (cid:107) F ( e ) ≤ 64 (cid:107) X (cid:107) 2 λ (cid:112) d y νC 0 (cid:107) U 0 − Y (cid:107) F ( f ) ≤ 64 (cid:107) X (cid:107) 2 (cid:112) d y Lσ 2min ( X ) νC 0 B 0 , ( 114 ) where ( a ) , ( b ) is by the update rule of momentum , which is W ( l ) t + 1 − W ( l ) t = − ηM t , l , where M t , l : = (cid:80) ts = 0 β t − s ∂(cid:96) ( W L : 1 ) ∂W ( l ) s , ( c ) is because (cid:107) ∂(cid:96) ( W L : 1 ) ∂W ( l ) s (cid:107) F = 4 (cid:107) X (cid:107) 2 √ d y θ s νC 0 (cid:107) U 0 − Y (cid:107) F ( see ( 89 ) ) , ( d ) is because that β = β 2 ∗ ≤ θ 2 , ( e ) is because that 1 ( 1 − θ ) 2 = 16 ηλ , and ( f ) uses the upper - bound B 0 ≥ (cid:107) U 0 − Y (cid:107) deﬁned in Lemma 13 and λ : = Lσ 2min ( X ) d y . The proof is completed . Lemma 16 . ( Hu et al . , 2020b ) Let R L - linear be an upper bound that satisﬁes (cid:107) W ( l ) t − W ( l ) t (cid:107) F ≤ R L - linear for all l and t . Suppose the width m satisﬁes m > C ( LR L - linear ) 2 , where C is any sufﬁciently large constant . Then , σ max ( W ( j : i ) t ) ≤ 1 . 1 m j − i + 1 2 , σ min ( W ( j : i ) t ) ≥ 0 . 9 m j − i + 1 2 . Proof . The lemma has been proved in proof of Claim 4 . 4 and Claim 4 . 5 in ( Hu et al . , 2020b ) . For completeness , let us replicate the proof here . We have for any 1 ≤ i ≤ j ≤ L . W ( j : i ) t = (cid:16) W ( j ) 0 + ∆ j (cid:17) · · · (cid:16) W ( i ) 0 + ∆ i (cid:17) , ( 115 ) where ∆ i = W ( i ) t − W ( i ) 0 . The product above minus W ( j : i ) 0 can be written as a ﬁnite sum of some terms of the form W ( j : k l + 1 ) 0 ∆ k l W ( k l − 1 : k l − 1 + 1 ) 0 ∆ k l − 1 · · · ∆ k 1 W ( k 1 − 1 : i ) 0 , ( 116 ) where i ≤ k 1 < · · · < k l ≤ j . Recall that (cid:107) W ( j (cid:48) : i (cid:48) ) 0 (cid:107) 2 = m j (cid:48)− i (cid:48) + 1 2 . Thus , we can bound (cid:107) W ( j : i ) t − W ( j : i ) 0 (cid:107) F ≤ j − i + 1 (cid:88) l = 1 (cid:18) j − i + 1 l (cid:19) ( R L - linear ) l m j − i + 1 − l 2 = ( √ m + R L - linear ) j − i + 1 − ( √ m ) j − i + 1 = ( √ m ) j − i + 1 (cid:0) ( 1 + R L - linear / √ m ) j − i + 1 − 1 (cid:1) ≤ ( √ m ) j − i + 1 (cid:0) ( 1 + R L - linear / √ m ) L − 1 (cid:1) ≤ 0 . 1 ( √ m ) j − i + 1 , ( 117 ) where the last step uses m > C ( LR L - linear ) 2 . By combining this with Lemma 13 , one can obtain the result . Remark : In the proof of Lemma 14 , we obtain a tighter bound of the distance (cid:107) W ( j : i ) t − W ( j : i ) 0 (cid:107) F ≤ O ( 1 κ ( √ m ) j − i + 1 ) . However , to get the upper - bound σ max ( W ( j : i ) t ) shown in Lemma 16 , ( 117 ) is sufﬁcient for the purpose . A Modular Analysis of Provable Acceleration via Polyak’s Momentum G . 1 . Proof of Theorem 10 Proof . ( of Theorem 10 ) Denote λ : = Lσ 2min ( X ) / d y . By Lemma 13 , λ min ( H ) ≥ λ . Also , denote β ∗ : = 1 − 12 √ ηλ and θ : = β ∗ + 14 √ ηλ = 1 − 14 √ ηλ . Let ν = 2 in Lemma 14 , 15 , and let C 1 = C 3 = C 0 and C 2 = 14 √ ηλ in Theorem 6 . The goal is to show that (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) for all t by induction . To achieve this , we will also use induction to show that for all iterations s , ∀ l ∈ [ L ] , (cid:107) W ( l ) t − W ( l ) 0 (cid:107) ≤ R L - linear : = 64 (cid:107) X (cid:107) 2 (cid:112) d y Lσ 2min ( X ) C 0 B 0 , ( 118 ) which is clearly true in the base case s = 0 . By Lemma 4 , 13 , 14 , 15 , Theorem 6 and Corollary 1 , it sufﬁces to show that (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ s ξ s − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ θ s · 2 C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) and ∀ l ∈ [ L ] , (cid:107) W ( l ) s − W ( l ) 0 (cid:107) ≤ R L - linear hold at s = 0 , 1 , . . . , t − 1 , one has (cid:107) t − 1 (cid:88) s = 0 A t − s − 1 (cid:20) ϕ s 0 (cid:21) (cid:107) ≤ θ t C 0 (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) , ( 119 ) ∀ l ∈ [ L ] , (cid:107) W ( l ) t − W ( l ) 0 (cid:107) ≤ R L - linear : = 64 (cid:107) X (cid:107) 2 (cid:112) d y Lσ 2min ( X ) C 0 B 0 , ( 120 ) where the matrix A and the vector ϕ t are deﬁned in Lemma 4 , and B 0 is a constant such that B 0 ≥ (cid:107) Y − U 0 (cid:107) F with probability 1 − δ by Lemma 13 . The inequality ( 119 ) is the required condition for using the result of Theorem 6 , while the inequality ( 120 ) helps us to show ( 119 ) through invoking Lemma 14 to bound the terms { ϕ s } as shown in the following . Let us show ( 119 ) ﬁrst . We have (cid:107) t − 1 (cid:88) s = 0 A t − 1 − s (cid:107) (cid:20) ϕ s 0 (cid:21) (cid:107) ( a ) ≤ t − 1 (cid:88) s = 0 β t − 1 − s ∗ C 0 (cid:107) ϕ s (cid:107) ( b ) ≤ 1920 (cid:112) d y √ m (cid:107) X (cid:107) 2 1 ηλ t − 1 (cid:88) s = 0 β t − 1 − s ∗ θ 2 s 4 C 30 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) 2 + t − 1 (cid:88) s = 0 β t − 1 − s ∗ ηλ 80 θ s 2 C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( c ) ≤ 1920 (cid:112) d y √ m (cid:107) X (cid:107) 2 1 ηλ t − 1 (cid:88) s = 0 β t − 1 − s ∗ θ 2 s 4 C 30 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) 2 + 2 √ ηλ 15 θ t C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( d ) ≤ 1920 (cid:112) d y √ m (cid:107) X (cid:107) 2 16 3 ( ηλ ) 3 / 2 θ t 4 C 30 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) 2 + 2 √ ηλ 15 θ t C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( e ) ≤ 1 3 θ t C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + 2 √ ηλ 15 θ t C 20 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) ( f ) ≤ θ t C 0 (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) , ( 121 ) where ( a ) uses Theorem 5 with β = β 2 ∗ , ( b ) is by Lemma 14 , ( c ) uses (cid:80) t − 1 s = 0 β t − 1 − s ∗ θ s = θ t − 1 (cid:80) t − 1 s = 0 (cid:16) β ∗ θ (cid:17) t − 1 − s ≤ θ t − 1 (cid:80) t − 1 s = 0 θ t − 1 − s ≤ θ t − 1 4 √ ηλ ≤ θ t 16 3 √ ηλ , β ∗ = 1 − 12 √ ηλ ≥ 12 , and θ = 1 − 14 √ ηλ ≥ 34 , ( d ) uses (cid:80) t − 1 s = 0 β t − 1 − s ∗ θ 2 s ≤ (cid:80) t − 1 s = 0 θ t − 1 + s ≤ θ t − 1 1 − θ ≤ θ t 16 3 √ ηλ , ( e ) is because C (cid:48) d y C 40 B 20 (cid:107) X (cid:107) 22 1 ( ηλ ) 3 ≤ C d y B 20 (cid:107) X (cid:107) 22 κ 5 ≤ m for some sufﬁciently large constants C (cid:48) , C > 0 , and ( f ) uses that ηλ = 1 κ and C 0 ≤ 4 √ κ by Corollary 1 . Hence , we have shown ( 119 ) . Therefore , by Theorem 6 , we have (cid:13)(cid:13)(cid:13) (cid:13) (cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13) (cid:13) ≤ θ t 2 C 0 (cid:13)(cid:13)(cid:13) (cid:13) (cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13) (cid:13) . By Lemma 15 , we have ( 120 ) . Thus , we have completed the proof . A Modular Analysis of Provable Acceleration via Polyak’s Momentum H . Experiment H . 1 . ReLU network We report a proof - of - concept experiment for training the ReLU network . We sample n = 5 points from the normal distribution , and then scale the size to the unit norm . We generate the labels uniformly random from { 1 , − 1 } . We let m = 1000 and d = 10 . We compare vanilla GD and gradient descent with Polyak’s momentum . Denote ˆ λ max : = λ max ( H 0 ) , ˆ λ min : = λ min ( H 0 ) , and ˆ κ : = ˆ λ max / ˆ λ min . Then , for gradient descent with Polyak’s momentum , we set the step size η = 1 / (cid:16) ˆ λ max (cid:17) and set the momentum parameter β = ( 1 − 12 1 √ ˆ κ ) 2 . For gradient descent , we set the same step size . The result is shown on Figure 1 . We also report the percentiles of pattern changes over iterations . Speciﬁcally , we report the quantity (cid:80) ni = 1 (cid:80) mr = 1 1 { sign ( x (cid:62) i w ( r ) t ) (cid:54) = sign ( x (cid:62) i w ( r ) 0 ) } mn , as there are mn patterns . For gradient descent with Polyak’s momentum , the percentiles of pattern changes is approximately 0 . 76 % ; while for vanilla gradient descent , the percentiles of pattern changes is 0 . 55 % . H . 2 . Deep linear network We let the input and output dimension d = d y = 20 , the width of the intermediate layers m = 50 , the depth L = 100 . We sampled a X ∈ R 20 × 5 from the normal distribution . We let W ∗ = I 20 + 0 . 1 ¯ W , where ¯ W ∈ R 20 × 20 is sampled from the normal distribution . Then , we have Y = W ∗ X , η = d y Lσ 2max ( X ) and β = ( 1 − 12 √ ηλ ) 2 , where λ = Lσ 2min ( X ) d y . Vanilla GD also uses the same step size . The network is initialized by the orthogonal initialization and both algorithms start from the same initialization . The result is shown on Figure 2 . Figure 2 . Training a 100 - layer deep linear network . Here “momentum” stands for gradient descent with Polyak’s momentum .