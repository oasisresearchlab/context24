An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software AADITYA BHATIA , SAIL , Queen’s University , Canada FOUTSE KHOMH , Polytechnique Montréal , Canada BRAM ADAMS , MCIS , Queen’s University , Canada AHMED E HASSAN , SAIL , Queen’s University , Canada The emergence of open - source ML libraries such as TensorFlow and Google Auto ML has enabled developers to harness state - of - the - art ML algorithms with minimal overhead . However , during this accelerated ML development process , said developers may often make sub - optimal design and implementation decisions , leading to the introduction of technical debt that , if not addressed promptly , can have a significant impact on the quality of the ML - based software . Developers frequently acknowledge these sub - optimal design and development choices through code comments written during code development . These comments , which often highlight areas requiring additional work or refinement in the future , are known as self - admitted technical debt ( SATD ) . While prior research has demonstrated that SATD can serve as a reliable indicator of technical debt and has extensively studied SATD in traditional ( non - ML ) software , little attention has been given to this issue in the context of ML . This paper aims to investigate the occurrence of SATD in ML code by analyzing 318 open - source ML projects across five domains , along with 318 non - ML projects . We detected SATD in source code comments throughout the different snapshots of the studied projects , conducted a manual analysis of a sample of the identified SATD to comprehend the nature of technical debt in the ML code , and performed a survival analysis of the SATD to understand the evolution dynamics of such debts . Our analyses yielded the following observations : ( i ) Machine learning projects have a median percentage of SATD that is twice the median percentage of SATD in non - machine learning projects . ( ii ) ML pipeline components for data preprocessing and model generation logic are more susceptible to debt than model validation and deployment components . ( iii ) SATDs appear in ML projects earlier in the development process compared to non - ML projects . ( iv ) Long - lasting SATDs are typically introduced during extensive code changes that span multiple files exhibiting low complexity . Additional Key Words and Phrases : Self Admitted Technical Debt , Machine Learning , Survival Analysis ACM Reference Format : Aaditya Bhatia , Foutse Khomh , Bram Adams , and Ahmed E Hassan . 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software . ACM Trans . Softw . Eng . Methodol . 1 , 1 ( November 2021 ) , 33 pages . https : / / doi . org / 0 / 0 1 INTRODUCTION The burgeoning of the Machine Learning ( ML ) era signals increased deployment of ML software in a wide range of software domains including large scale systems for healthcare , mobility , banking , Authors’ addresses : Aaditya Bhatia , aaditya . bhatia @ queensu . ca , SAIL , Queen’s University , Kingston , Canada ; Foutse Khomh , foutse . khomh @ polymtl . ca , Polytechnique Montréal , Canada ; Bram Adams , bram . adams @ queensu . ca , MCIS , Queen’s University , Kingston , Canada ; Ahmed E Hassan , ahmed @ cs . queensu . ca , SAIL , Queen’s University , Kingston , Canada . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2021 Association for Computing Machinery . 1049 - 331X / 2021 / 11 - ART $ 15 . 00 https : / / doi . org / 0 / 0 ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . a r X i v : 2311 . 12019v1 [ c s . S E ] 20 N ov 2023 2 Bhatia , et al . transportation , policing , marketing , and social media [ 1 ] . ML - based systems , i . e . , projects that inte - grate ML models , with other non - ML components , serve as backbone for this expansive integration across domains . Unless they reuse third - party models , ML - based systems involve the training of ML models , which is achieved using a ML pipeline composed of multiple components like data preprocessing , ML model training , model evaluation , and deployment [ 2 ] . Handling ML - based systems necessitates management of ML - related assets [ 3 ] ( e . g . , training data , ML models , job execution data ) and introduces additional challenges that are not prevalent in traditional ( non - ML ) software [ 4 ] . The advent of popular ML toolkits , ™ such as Scikit - Learn , TensorFlow , and Auto ML , has enabled developers to easily leverage state - of - the - art ML algorithms to quickly develop ML - based systems with minimal overhead . Yet , the specification of ML algorithms is far from trivial [ 5 ] , and development practices for ML differ from those of traditional systems [ 6 ] . For instance , the dependency of ML code on external data makes it difficult to enforce strict abstraction boundaries [ 7 ] . The formerly mentioned ML toolkit frameworks are developed by large corporate organizations ( e . g . , Google - Tensorflow , META - PyTorch ) in collaboration with the open - source community [ 2 ] and try to maintain quality standards , however , ML - based systems need development and maintenance guidelines too . For instance , ML systems need established protocols for iterative model training , maintaining a structured log of experiments and their outcomes , while pruning dead code paths to promptly maintain codebase clarity . As a result of these challenges , developers and data scientists can often make suboptimal decisions for short - term benefits when working on their code , models and data under tight schedules , a phenomenon described by Cunningham [ 6 ] as technical debt ( TD ) . While developers themselves may not treat technical debt as poor programming , but as the result of decisions made while evaluating competing development tradeoffs , these decisions , just like financial debt , will require correction in the future [ 8 ] . Consider a scenario in traditional ( non - ML ) software where a single class manages unrelated items , resulting in low code cohesion . The longer this cohesion debt lingers within this class , the more the number of class dependencies may grow , making future changes potentially more disruptive ( e . g . , leading to many " breaking changes " , which are dreaded by development teams ) . TD in general is hard to automatically detect from software engineering resources . However , in an important subset of cases , developers explicitly admit having produced a somewhat temporary implementation that possibly requires more work in the future . Such debt is referred to as Self - Admitted Technical Debt ( SATD ) , and is explicitly defined within the comments that are inserted in the source code while developing . Such SATD - containing comments may represent inferior developmental implementations in software design , missing / incomplete documentation , known defects , code , testing or software requirements [ 9 ] . For example , figure 1 provides an example of a natural language processing application , Mead - ML 1 , where the code has a “dirty” implementation of saving a dummy matrix of zeros . In lines # 190 and # 192 , the developers admit that this ad - hoc implementation ( TD ) will be fully developed after “check pointing " is implemented in the future . As another instance , a comment “ # TODO handle attention history " 2 within a code base that builds an attention model in the Neural Monkey project , is an SATD declaration , since it indicates the missing implementation of handling model weights . 1 https : / / github . com / mead - ml / mead - baseline / commit / d4d8fab88a7424785ef508d0c1bd24ac58cae4d8 # diff - c8db4ed3ecd32411a0c3bfecf8555d4ca0cf2181bfcce78b670b9cdc79e0b99d 2 https : / / github . com / ufal / neuralmonkey / blob / master / neuralmonkey / attention / transformer _ cross _ layer . py ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 3 Fig . 1 . Example of SATD ( s ) in a Natural Language Processing application , Mead - ML . Sculley et al . [ 7 ] identify ML - based system - level interactions and interfaces as areas where ( SA ) TD tends to accumulate in ML - based systems , leading to quality issues similar to traditional , non - ML software as well as new types of issues . For instance , consider a ML scenario for an image classifica - tion software project . Using a traditional ML model , say SVM , for image classification would require manual feature extraction ( e . g . , SIFT ) . Paying off this TD by upgrading SVM to a Deep Learning model like CNN can itself cause massive ripple effects across multiple components of Amershi’s ML pipeline [ 3 ] , with heavy financial and technical cost . For instance , 1 ) data preprocessing and resizing would need to be changed ( e . g . , no more need for SIFT ) , 2 ) model validation components would need to change to consume CNN output instead of SVM’s output . In fact , 3 ) changes could even permeate in the deployment environment to generate inferences and parse predictions from a model from a different library ( say , Sklearn SVM migration to Keras CNN ) , which could potentially even lead to changes in the deployment server , perhaps even at the client’s end . Alas , 4 ) the physical hardware of the deployment server may even need to be upgraded from a CPU to GPU architecture , considering that GPUs are better suited for inferencing Keras ML models at scale . This increases not only the cost of physical hardware , but also the cost of migration of the entire software deployment to hardware with a different architecture . The problem of TD in ML is further exacerbated due to the “implicit” requirements of ML software products , which are grounded in data rather than specification documentations . As pointed out by Sculley et al . [ 7 ] , the problem of TD is pronounced in ML due to the stochastic nature of ML , as opposed to the deterministic nature of traditional ( non - ML ) software . For instance , an SATD in the ML model’s hyper - parameter configurations or data pre - processing will result in an inferior model with lower accuracy . Every incorrect decision made by such a model may potentially negatively affect various aspects such as the vendor’s reputation , user experience , or financial performance . Traditional software quality metrics such as those defined by McCall [ 10 ] and Boehm [ 11 ] may not be directly applicable in such a stochastic environment . Hence , understanding the various dynamics of TD may help ML stakeholders provide a better understanding of quality standards of ML software . Despite recent efforts to understand SATD introduction and removal in deep learning frameworks , we still do not know much about the diffusion and evolution of technical debt in ML - based systems using such frameworks . Neither do we know the composition or the circumstances of SATD introduction or removal . While several researchers have investigated the detection , evolution , and impact of technical debt in traditional software systems ( e . g . , [ 9 , 12 – 19 ] ) , software systems powered by ML models ( i . e . , ML - based systems ) have mostly remained out of scope . This paper aims to fill this gap in the literature by studying SATD in ML - based systems belonging to five popular domains , i . e . , self - driving car , audio processing , autonomous gameplay , image processing and natural language processing . Specifically , we examine the prevalence , composition , location , and evolution of SATD along with the characteristics of long - lasting debt , addressing the following research questions : • RQ1 : What is the prevalence of SATD in ML - based systems ? • RQ2 : What are the different types of SATD in ML - based systems ? ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 4 Bhatia , et al . • RQ3 : Which components of the ML pipeline are more prone to SATD ? • RQ4 : How long does SATD survive in ML - based systems ? • RQ5 : What are the characteristics of long - lasting SATDs in ML - based systems ? The remainder of the paper is organized as follows . Section 2 summarises the related literature . Section 3 presents the design of our case study . Section 4 discusses the motivation , approach , and results for each of our five research questions . Section 5 discusses threat to the validity of our study . In Section ? ? , we discuss the research contributions of our work . Section 6 discusses the implications of our findings , while Section 7 provides an in - depth discussion about the future research and tooling that can be inspired by our study . Finally , Section 8 concludes the paper . 2 RELATED WORK In this section , we present the literature related to our work . 2 . 1 Studies on Software Engineering for Machine Learning Software Engineering for Machine Learning ( SE4ML ) is a recent evolving rapidly field with research that encompasses of a broad set of topics like requirements engineering , ML design and architecture , testing and validation , explainability and interpretability , ethics and fairness , data management , model deployment , maintenance and life - cycle management , training performance optimization , and ML security . A systematic literature review by Nascimento et al . [ 20 ] highlighted the unique challenges in ML testing , ML software quality , and data management in ML systems . Moin et al . [ 21 ] proposed a novel approach to integrate Model - Driven Software Engineering and Model - Driven ML Engineering , with a focus on automated ML , aiming to assist software engineers without deep ML knowledge . Tantithamthavorn et al . [ 22 ] emphasized the need for explainable ML in software engineering , making ML models more practical , and actionable . Kästner and Kang [ 23 ] discussed the importance of teaching software engineering skills to students with an ML background , focusing on realism , robust infrastructure , and ethical considerations . Singla et al . [ 24 ] analyzed the adoption of Agile methodology in machine learning projects , finding a higher number of exploratory tasks and difficulty in estimating task duration . Within such a vast body of SE4ML research , our research expands on the “deployment maintenance and life - cycle management” area . While other research in this area has focused on code changes in ML [ 2 ] , ML asset management [ 25 ] , ML code smells [ 26 ] , our study focuses on the SATD aspect and helps ML stakeholders understand the nature of TD and enable ML practitioners to gain awareness of TD building ML applications . 2 . 2 Empirical Studies on SATD in Traditional ( non - ML ) Software In 1994 , Cunningham [ 27 ] introduced the concept of technical debt as “quick and dirty” work in software design and implementation resulting in “not quite - right code” . However , TD is an abstract concept , as making tradeoffs between optimal software and meeting project deadlines is not quantitatively measurable . Hence , subsequently , Potdar and Shibab [ 28 ] extended the concept of TD to self - admitted TD ( SATD ) , which was deemed to be a more visible measure of TD [ 18 , 29 , 30 ] . Along with introducing the concept of SATD , Potdar and Shibab examined SATDs in large open - source projects - Eclipse , Chromium , Apache HTTP , and ArgoUML . They found a high prevalence ( within up to 31 % of files ) of SATD , and identified that more experienced developers add more SATD and are unable to pay off the technical development due to time pressure and code complexity . Building on the seminal work of Potdar and Shibab , multiple subsequent studies have identified SATD in traditional software . For instance , Maldonado et al . [ 31 ] studied the evolutionary aspects ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 5 of traditional code and revealed that SATD lasts between 18 and 172 days in a system . Bavota and Russo [ 9 ] performed a large - scale study of SATD across 159 software projects , contributing towards developing a taxonomy of SATD for traditional software . Wehaibi et al . [ 15 ] examined the impact of SATD on software quality , while Kashiwa et al . [ 32 ] studied SATD in code reviews . However , none of the prior studies investigate SATD in ML - based systems . As identified by the previous subsection , the engineering practices and research - based findings of traditional software cannot be expected to hold as - is for ML - based systems [ 33 – 35 ] , and hence , our research is important to identify the characteristics of SATD in the currently prevalent ML - era of software development . Two studies bear similarities to our research . Firstly , akin to our study , the study by Liu et al . [ 36 ] explores SATD in deep learning frameworks by investigating the introduction and removal of SATD in seven deep learning frameworks . However , contrary to the study of frameworks , our research identifies debt in ML systems , which use the former frameworks to build and integrate pipelines into ML applications . Our study provides insights for a broader spectrum of ML stakeholders who do not deal with ML mathematical or algorithmic implementations , but need to manage ML assets ( like data , model ) for development of ML applications . Secondly , the study is by Obrien et al . [ 37 ] mirrors our work closely . The authors check the different types of SATDs in ML software and check their distribution in different components of ML pipeline stages . While Obrein et al . ’s research is qualitative , we focus on both quantitative and qualitative aspects of SATD on a different dataset . Within the quantitative aspects , our work takes a genealogical approach to SATD , tracing the lifecycle from introduction to removal . Additionally , we put forth an exploratory model to understand the dynamics of long - lasting TD . Given our different dataset , our qualitative results compliment the results of Obrien et al . and provide a broader understanding of the prevalence and impact of debt across different datasets and domains . 2 . 3 Studies presenting SATD detection tools As mentioned in our previous subsection , the domain of ( SATD ) detection is a fertile field for research , and has various tools proposed to tackle this challenge . A few notable examples are the efforts made by Malando et al . [ 38 ] , who constructed an NLP - based SATD classification model and tested their tool across ten substantial open - source projects , namely Ant , Columba , EMF , ArgoUML , Hibernate , JEdit , JFreeChart , JRuby , and SQirrel SQL . Later , in 2017 , Huang et al . [ 39 ] expanded the scope by developing a predictive model that identifies SATD within code comments and evaluated their tool on eight open - source projects . The neural network - based SATD detection tool proposed by Ren et al . [ 40 ] emphasized high performance and explainability . Faris et al . [ 41 ] later enhanced SATD classification models by using a contextualized vocabulary . Despite such numerous efforts in SATD detection , the practical adoption of these tools presents challenges . Many of these detection tools , despite their academic promise , exist only as prototypes and may lack maintenance or contain bugs that prevent their practical use . We employed the text mining - based SATD detector tool developed by Liu et al . [ 36 ] in our study , which leverages natural language processing and machine learning to classify SATD comments . We selected Liu et al . ’s SATD detector tool because of its frequent maintenance and demonstrated applicability in other authoritative SATD studies ( e . g . , [ 14 ] and [ 42 ] ) . 3 CASE STUDY SETUP This section describes the design of our empirical study which is also illustrated in Figure 2 . In the following , we elaborate on each of these steps in details . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 6 Bhatia , et al . GitHub Image Processing Audio Processing Natural Language Processing AutonomousGameplay Self Driving Car Others 318 ML Projects Commit Extractor ( RQ4 and RQ5 ) Comments Extractor SATDExtractor Survival Analysis ( RQ4 ) Commit Characteristics ( RQ5 ) PredictiveModel ( RQ5 ) PrevalenceAnalysis ( RQ1 ) QualitativeAnalysis ( RQ2 and RQ3 ) 318 Non - ML Projects Fig . 2 . Data collection and processing steps . 3 . 1 Data Collection Applications of ML algorithms ( ML - based systems ) are prevalent in Healthcare , Finance , Retail , Travel , Social Media , etc 34 . All such industrial applications can be grouped into three high - level domains , namely , Image Processing , Natural Language Processing , and Audio Processing [ 43 ] . Recently , the use of reinforcement learning in Autonomous Gameplay has been gaining a lot of attention from academia and the industry [ 44 ] , prompting us to include it in our dataset as well . Additionally , we include the Self Driving Car domain due to its widespread use in industry and the significant attention it has received from researchers [ 45 ] . The Self Driving Car domain mostly leverages computer vision in their perception modules and is an extended version of Image Processing [ 46 ] . While it is impossible to include all industrial domains , we select these five as representative examples [ 47 ] of successful ML implementations . Finally , to account for other domains , we also add raw ML implementations like maintained ML courses that we could not fit into the five selected domains , labeling them as “Other ML” ( see Figure 3 . 1 ) . Understanding technical debt in ML courses is important since many practitioners initiate their ML journey through such courses , and may not be learning proper technical debt management practices . Moreover , emphasizing technical debt management early on can help learners establish strong foundations for best practices in broader , real - world applications . For our analysis of the above six domains , We gathered open - source ML - based open source projects from GitHub 5 . We used the GitHub Search API v3 6 to look for keywords specific to each domain ( see Table 1 ) and obtain a list of relevant projects . As suggested by prior studies ( [ 48 – 50 ] ) , for each set of keywords , we identify the top matching projects by using the “ best match ” feature of GitHub search API 7 . The first author conducted an exhaustive search of the query results to find as many projects as possible corresponding to each domain . For example , when the “autonomous driving machine learning” search results no longer yielded relevant projects after page 5 ( 50 results ) , 3 https : / / www . projectpro . io / article / 10 - awesome - machine - learning - applications - of - today / 364 4 https : / / www . javatpoint . com / applications - of - machine - learning 5 https : / / github . com / 6 https : / / docs . github . com / en / free - pro - team @ latest / rest 7 https : / / docs . github . com / en / search - github / getting - started - with - searching - on - github / sorting - search - results ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 7 Table 1 . Queries used to find projects from the five ML domains . ML Domain Queries Used Self Driving Car “autonomous driving machine learning” , “self driving machine learning” Audio Processing “sound machine learning” , “audio machine learning” , “voice ma - chine learning” , “speech machine learning” Autonomous Gameplay “game reinforcement learning” , “game machine learning” Image Processing “image processing machine learning” , “vision machine learning” , “image processing deep learning” Natural Language Processing “natural language processing machine learning” , “chatbot ma - chine learning” Other ML “machine learning” we repeated the process with the next query , in this case “self - driving machine learning” . We ended our search for a domain when we stopped finding any relevant projects for the domain’s queries . Based on previous studies [ 48 ] , we filtered out “toy projects” from our comprehensive list of relevant projects for each domain . To ensure that we retain only matured ML - based projects for our study , we applied the following common selection criteria to our search results : • C1 . A project should not be a fork of another project . • C2 . A project should have at least five python source code files . • C3 . A project should have more than one month of development history . • C4 . A project should implement machine learning . • C5 . A project should not be a machine learning library . A similar threshold - based criterion of removal of toy - projects has been used by prior studies for their respective study domains and conditions . For instance , Alfadel et al . [ 51 ] used top - most starred , non - forked projects with at least 20 commits for studying dependabot security pull requests , while Chen and Jiang [ 52 ] selected non - forked projects having at - least 30 stars for studying the rationale behind using logging practices in Java ( e . g . , internationalization of the log messages ) . The selection criteria C1 , C2 , C3 were checked programmatically , while the criteria C4 and C5 were checked by manual analysis performed by the first author . This effort involved checking 1 ) the project’s readme , 2 ) checking for the presence of common ML framework imports ( e . g . , tensorflow , sklean , keras ) , and 3 ) relevant files names ( e . g . , train . py or preprocess . py ) and inspect the source code functionality ( e . g . , if an ML model is being trained ) . After applying the selection criteria to our list of projects for each of the five domains , we obtained a total of 318 projects on which we perform our analysis . Table 2 provides a description of the selected projects along with their corresponding total number of python source code files per domain . Moreover , Figure 3 plots the distribution of the projects’ LOC for the different project domains . We find that the median project size is 80 , 477 lines of code across the entire ML project dataset , indicating that the selected projects are matured . To answer the research questions RQ1 and RQ4 mentioned in the Introduction , we also need a set of non - ML projects as baseline to allow a fair comparison . Hence , we collected 318 ( same count as that of ML projects ) non - ML Python projects from GitHub . Similar to the keyword - based selection mechanism of ML projects , we used the keywords “python projects” , “server” and “” ( empty keywords ) to obtain a list of non - ML python projects sorted by the “ best match ” criterion of GitHub Search . The first author manually inspected the repositories of the collected projects to ensure that 1 ) they do not contain ML components , 2 ) they contain at least five python source code ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 8 Bhatia , et al . A u d i o P r o c e ss i n g I m a g e P r o c e ss i n g A u t o n o m o u s G a m e P l a y S e l f D r i v i n g C a r N a t u r a l L a n g u a g e P r o c e ss i n g O t h e r M L 0 1 2 3 4 5 6 P r o j e c t L O C ( l o g 10 ) Fig . 3 . Distribution of number of Lines of code for different project domains . Table 2 . Distribution of the projects across the five categories . ML Domain Project Count Python File Count Self Driving Car 39 1 , 701 Audio Processing 38 2 , 484 Autonomous Game Play 50 1 , 254 Image Processing 60 4 , 393 Natural Language Processing 78 3 , 357 Other ML 53 5 , 446 files , 3 ) have more than one month of development history , and 4 ) they are not forks . Figure 4 compares the characteristics of the collected 318 ML and 318 non - ML projects . Since the boom in machine learning systems is fairly recent , ML projects naturally have shorter timelines ( 4 - a ) and less commits ( 4 - c ) as compared to non - ML projects . Similarly , ML projects have less contributors ( 4 - b ) . Yet , surprisingly the total amount of developmental changes in ML seems to be much higher ( absolute ) churn compared to non - ML ( 4 - d ) . Although the maturity level of ML projects is lower ( since the ML era recently burgeoned ) , the fairly large amount of code modifications in each commit ( churn ) indicates that the ML project have high development traction , and hence a sizeable likelihood of introducing SATD . All comparisons in Figure 4 are statistically significant ( Wilcoxon Rank - sums test with p - value < 0 . 01 ) , with large effect sizes ( Cohen’s 𝛿 > 0 . 7 ) . 3 . 2 SATD Detector Tool As stated in our literature review section 2 . 3 , many SATD detector tools used to identify instances of SATD from code comments . For our study , we use the state of the art SATD detection tool 8 , 8 https : / / github . com / Tbabm / SATDDetector - Core ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 9 ML Non - ML 0 1000 2000 3000 4000 5000 P r o j e c t T i m e s ( d a y s ) ( a ) Project Lifetimes ML Non - ML 0 10 20 30 C o n t r i b u t o r C o un t ( b ) Contributor Count ML Non ML 0 250 500 750 1000 1250 C o mm i t C o un t ( c ) Commits count ML Non - ML 0 200 400 600 C o mm i t - L e v e l C hu r n ( d ) Churn per commit Fig . 4 . Project history lifetimes , commits , and churn per commit for ML and non - ML projects of our dataset . proposed by Liu et al . [ 36 ] , which employs a NLP - based pre - trained text classification ML model to automatically detect the presence of SATD in a code comment . As compared to other contemporary SATD detection tools that rely on pattern matching , this tool demonstrates a higher recall [ 36 ] . This tool’s use of an NLP - based model to determine whether a specific comment signifies SATD contrasts most of the other approaches , which merely scan for words like “todo” or “fixme” using string matching techniques . Therefore , the tool can successfully identify regular comments such as “300 iterations seems good enough but you can certainly train longer” or “naive approach for generating forward mapping this is naive and probably not robust” as SATD even when such strings lack an explicitly defined “TODO” . This SATD detector has been utilized in previous studies . For instance , Liu et al . [ 14 ] used this tool for studying SATD in deep learning frameworks , while Zampetti et al . [ 42 ] used this tool for recommending when design technical debt should be self admitted . 3 . 3 Replication Package All the scripts along with the mined data is provided in the replication package 9 4 CASE STUDY RESULTS In this section we report and discuss the results of our research questions . For each research question , we present the motivation , the approach , and discuss the results . 9 https : / / drive . google . com / drive / folders / 1n - gwAxFANS - PPewnk1Qgmnwd0zMAr7Yb * This tentative g - drive link will be replaced by a github repository after the reviewing process . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 10 Bhatia , et al . 4 . 1 RQ1 : What is the prevalence of SATD in ML - based systems ? Motivation . Sculley et al . ’s seminal work [ 7 ] highlighted the capacity of ML programs to incur technical debt , remarking that the strong dependence of ML models on data makes it difficult to enforce strict abstraction boundaries . When building ML - based systems , developers often experi - ment with different configurations in search of the optimal model ; building , testing , and comparing the performance of different prototypes to identify the best configuration leading to the most efficient model . This process can often result in multiple dead experimental code paths , increasing the cyclomatic complexity of the code and the risk of an unexpected behavior arising from some of the obsolete experimental code paths . Another potential area where technical debt can accumulate quickly in ML code is in the configuration . Developers often have to handle a wide range of con - figuration options . For example , they often have to select among a variety of algorithm - specific learning settings , and – or from a large pool of pre - processing , post - processing , and verification techniques . In this research question , we aim to examine the extent to which ML software deployed today , contains technical debt , by comparing the prevalence of SATD in ML and non - ML code . Table 3 . Distribution of the dataset for machine learning and non - machine learning software . ML Software Non - ML Software # Projects 318 318 # Python - Files 20 , 319 65 , 748 # Comments 424 , 248 1 , 505 , 458 Approach . We extract 318 ML and 318 non - ML python projects following the description from Section 3 . For each ML or non - ML project , we extract comments using Comment - Parser , a PyPi framework 10 . We obtained 0 . 4 Million and 1 . 5 Million comments in ML and non - ML software respectively . A detailed description of the distribution of ML and non - ML software parameters is provided in Table 3 . We qualify whether a comment is SATD using the SATD detection tool 11 . From Table 3 , we observe that the number of comments in non - ML projects is approximately 2 . 8 times that of ML projects . To resolve the imbalance in this comparison , we collect a sample of 100 , 000 ML and non - ML comments and compared the percentage of SATD in ML and non - ML projects . We repeat this 100 times , to ensure that our results are not based on a particular sample . We chose to perform our analysis on non - ML projects instead of reusing results previously published , to allow for a fair comparison . In fact , parameters like the programming language ( Python vs Java in prior studies ) , the SATD detection tool , and the number of projects can impact the comparison . Nevertheless , we will refer to prior findings whenever relevant . Results . Machine learning - based projects have 2 . 1 times more SATD in comparison to non machine learning projects . Figure 5 shows the distribution of SATD in ML and non - ML software . The median of percentages of SATD in ML and non - ML software is 1 . 9 % and 0 . 9 % , respectively . The comparisons provided in Figure 5 are statistically significant with the Wilcoxon Signed Rank test 𝑝 - value lower than 0 . 05 and a large Cohen’s 𝑑 effect size difference . Changing the sample size of 100 , 000 did not change our results of median values , but only changed the standard deviation of both distributions in Figure 5 . When considering all the comments in ML and non - ML code from our data set ( without any sampling ) , 3 . 5 % of comment declarations in ML projects contain a technical debt admission ( i . e . , SATD ) , compared to only 2 % for non - ML projects . Our findings for 10 https : / / pypi . org / project / comment - parser / 11 https : / / github . com / Tbabm / SATDDetector - Core ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 11 Non - ML ML 1 . 00 % 1 . 20 % 1 . 40 % 1 . 60 % 1 . 80 % 2 . 00 % P e r c e n t a g e o f D e b t SATD in ML and non - ML projects Fig . 5 . Percentage of SATD in ML and non - ML projects . The comparison has been made by collecting 100 , 000 random samples from ML and non - ML comments and repeating the process 100 times ( to ensure robustness ) . SATD in non - ML software are aligned with the findings of prior research . For instance , Bavota and Russo [ 9 ] found SATD in 0 . 3 % of the comments of non - ML Java - based projects , which is comparable to the 0 . 9 % obtained my our analysis for Python - based non - ML projects . Since ML developers tend to experiment with different configurations when searching for the optimal model , we surmise that the amount of code churn in ML code is likely higher than the amount of code churn in non ML code , which could explain the observed difference in the proportion of SATD ( since high code churn is known to be associated with poor design [ 53 – 56 ] ) . Hence , we computed the distribution of code churn in the studied ML and non - ML projects . The results presented in Figure 6 ( a ) show that ML code experiences statistically significantly higher code churn than non - ML code , with large effect size ( Cohen’s 𝛿 = 12 ) . One possible expla - nation of the high rate of SATD in ML code could be the higher rate of code modification , even though we cannot claim any causal relation . Furthermore , Figure 6 ( b ) shows that SATD files also have higher churn per file size for ML software . This further corroborates that those larger changes correlate with lower software quality and higher amounts of SATD . The comparisons of churn and churn - per - LOC in Figure 6 ( a and b ) are statistically significant with large effect size for SATD code and medium effect size for clean code . Additionally , we looked into alternative explanations of the higher amounts of SATD in ML by checking alternative metrics like file - level complexity and commit - level file changes . While both code complexity and the number of file changes per commit are lower for ML than for non - ML code , MLcode churn is significantly higher than non - ML churn . Figure 7 ( a ) shows the normalized number of files changed in a commit , while Figure 7 ( b ) shows the normalized complexity of the changed ML and non - ML files . Although the comparison between ML and non - ML file counts are significant , there is only a negligible effect size . In Figure 7 ( b ) , when comparing ML to non - ML code complexity using Cohen’s 𝛿 , the clean files , represented by the red boxes , show a large effect size with a value of 0 . 8 . On the other hand , the SATD files , represented by the blue boxes , show a small effect size with a value of 0 . 2 . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 12 Bhatia , et al . Overall , this indicates that ML and non - ML software projects differ along metrics like complexity and churn , and ML churn is significantly higher than non - ML churn despite the ML complexity being significantly lower than non - ML . The churn in machine learning ( ML ) development could be notably higher than in non - ML code largely due to the repetitive nature of ML - specific tasks such as data preprocessing , model building , and hyperparameter tuning . These activities often necessitate frequent code modifications , as highlighted by Sculley et al . [ 7 ] , with even minor conceptual changes leading to substantial code alterations . This is evident in scenarios like dataset modifications , where changes to a feature column can require extensive updates to associated code , or during model building , where continuous adjustments to hyperparameters contribute to churn . Moreover , switching between ML frameworks , like from Scikit - learn to TensorFlow , exacerbates this issue due to significant differences in their syntax and functions . This high churn rate not only parallels the negative impact of code churn on traditional software quality , as shown in various studies [ 55 , 56 ] , but can also affect ML software by potentially degrading quality of ML - based systems . ML developers should be mindful of the deterioration of code quality when making frequent changes to their codebase . They should consider monitoring their technical debt , to prevent its cost from reaching a critical level quickly . ML Non - ML ML Non - ML 0 2 4 6 F il e C hu r n ( l o g b a s e 10 ) Clean Files SATD Files ( a ) Churn in ML and non - ML software ML Non - ML ML Non - ML 4 2 0 2 4 6 C hu r n / L O C ( l o g b a s e 10 ) Clean Files SATD Files ( b ) Churn per file size in LOC Fig . 6 . ML has much higher churn as compared to Non - ML software projects . We examined the prevalence of SATD across the five domains considered in our study and found that : Self driving car and Game domains have higher proportions of SATD ( i . e . , 3 . 2 % and 3 % respectively ) in comparison to the other domains . The NLP , Image Processing , Audio processing domains , and the Other ML category have 2 . 2 % , 1 . 9 % , 1 . 6 % , and 1 . 6 % of SATDs respectively . Figure 8 presents the distribution of SATD across the five studied domains . : Summary of RQ1 Machine learning projects have a median percentage of SATD that is twice the median percentage of SATD in non - machine learning projects . Overall , 3 . 5 % of ML comments have SATD whereas only 2 % of non - ML software has SATD . Such results are attributed to higher churn in ML projects . 4 . 2 RQ2 : What are the different types of SATD in ML - based systems ? Motivation . Bavota and Russo [ 9 ] have examined SATDs in traditional software systems and developed a taxonomy . Given the specific nature of ML code as described in RQ1 , new types of technical debt not mentioned in this taxonomy may exist . Therefore , in this research question , we ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 13 ML Non - ML ML Non - ML 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 F il e C h a n g e s / L O C Clean Files SATD Files ( a ) Commit - level file changes normalized by LOC ML Non - ML ML Non - ML 0 . 0 0 . 2 0 . 4 0 . 6 C o m p l e x i t y / L O C Clean Files SATD Files ( b ) File - level complexity normalized by LOC Fig . 7 . ML code has lower complexity , but more file changes than non - ML code . investigate the composition of SATD in ML - based software , to uncover types of SATDs that may be specific to ML code and identify the types of SATDs that are prevalent in ML code . Approach . We apply stratified sampling across our five domains using a 99 % confidence level and 5 % confidence interval and randomly sampled a total of 611 SATD comments from the SATD data set obtained in RQ1 . We apply stratified sampling to avoid biasing our dataset towards SATDs from a specific domain . Prior to classifying the sampled 611 SATD instances , the first two authors together examined an initial set of 50 samples , ( separate from the 611 samples ) to build a common understanding of the SATD categories . They used card sorting to categorize the SATDs using the taxonomy proposed by Bavota and Russo [ 9 ] . In cases where the SATDs could not be categorized using the labels proposed by this taxonomy , a new category was created . After this initial step , the authors separately classified another set of 65 samples ( from a pool of 611 ) , then computed the inter - rater agreement on this new sample using Cohen Kappa , and obtained a value of 94 % , which is very high . After achieving this high level of agreement , the first author manually classified the remaining 546 SATD comments . A similar approach was used by Kononenko et al . [ 57 ] to classify code review survey responses , and Uddin and Khomh [ 58 ] to classify API aspects . SelfDrivingCar Game NLP Vision Audio OtherML 0 . 0 % 0 . 5 % 1 . 0 % 1 . 5 % 2 . 0 % 2 . 5 % 3 . 0 % D e b t P e r c e n t a g e Fig . 8 . Distribution of SATD in the studied ML domains . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 14 Bhatia , et al . During the classification process , we encountered SATD comments containing the keyword “ # TODO ” without any explanation of the task to be done . We labeled these instances as Undefind . We also found instances for which we were unable to identify the exact type of technical debt reported . We labeled such instances as Unknown . Overall , 40 % of SATD comments were considered to be either false positive or to belong to the categories Undefind or Unknown . To answer our research question , we remove these cases and calculate the percentage distribution of different types of SATDs from the remaining dataset of 367 samples . Results . In addition to the traditional SATD taxonomy by Bavota and Russo , ML code faces two new types of SATDs , namely configuration debt and inadequate tests debt . The most frequent type of SATD in ML code is functional requirement debt . Table 4 shows the distribution of SATDs observed in our studied sample . We found instances of code debt , design debt , requirement debt , and test debt , which are identified in the taxonomy proposed by Bavota and Russo [ 9 ] . Functional requirements debt accounts for 34 % of SATD in our sample . Among them , 19 % is related to features needing improvement ( i . e . , the SATD comment contain the statement “improvement of features needed " ) and 15 % of functional requirement debt is about new features ( i . e . , the SATD comment contain the statement “new features to be implemented” ) . Code SATD accounts for a total of 30 % debt with low internal quality , refactoring and workaround SATDs , each accounting for 11 % , 10 % and 9 % of the instances respectively . Known defect to fix accounts for 8 % of SATDs instances , while SATDs related to design patterns account for 6 . 1 % of SATDs instances . We also identified eight instances ( or 3 % ) of On - Hold SATD . On - Hold SATDs are comments referring to a debt that has already been paid [ 59 ] , but the SATD comment was not removed from the code . We identified two new categories of SATD not mentioned in the original taxonomy ; i . e . , configuration debt and inadequate tests , with 42 and 17 instances , respectively . In our context , configuration debt refers to sub - optimal settings or lack of clear documentation on essential parameters governing ML system behavior , which includes , but is not limited to , algorithm hyperparameters , model settings , and data pipeline configurations . For instance , the comment “ TODO Also add batch _ size and other extraction specific configs to file " 12 signifies configuration debt as it alludes to an absence of standardized practices to track and manage essential configurations like batch sizes and extraction settings , potentially leading to inefficiencies and challenges in model reproducibility . Such configurations are needed to pre - process the data , as well as to tune the model ( aka , hyper - parameters ) . Another example of such configuration debt is found in the project Image - Processing - via - deep - learning 13 , where a developer mentioned that “300 iterations seems good enough but you can certainly train longer” . This SATD comment indicates the need to update hyperparameters’ configuration . Similarly , a developer 14 mentioned “ # TODO : Also add batch _ size and other extraction specific configs to file” , indicating improper management of configuration variables . Regarding the inadequate tests SATDs , they occur when testing needs to be done in the future . For instance , in project 15 , developer ( s ) mentioned “ # TODO ( yonib ) : Also test logit _ scale with anchorwise = False . ” indicating missing / inadequate tests before committing the code . The taxonomy proposed by Bavota and Russo contains a “Test” category , but this category refers to the quality of test cases , while our new inadequate tests category is about missing or inadequate testing . In [ 9 ] , Bavota and Russo found that instances of Code , Requirement , Defect , Design , Test , and Documentation debt account for respectively , 30 % , 20 % , 20 % , 13 % , 8 % , and 7 % of SATD in traditional 12 https : / / github . com / Cloud - CV / visual - chatbot / tree / master / / viscap / data / extract _ features _ maskrcnn . py 13 https : / / github . com / eddieczc / Image - Processing - via - deep - learning 14 https : / / mypublicdoc . s3 . us - east - 2 . amazonaws . com / DebtFiles / 24 % 23 % 5Eviscap % 5Edata % 5Eextract _ features _ maskrcnn . py 15 https : / / github . com / NVIDIA / DeepLearningExamples . git ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 15 Table 4 . Distribution of different types of SATD in ML software . Types except for the bolded ones are reused from Bavota and Russo [ 9 ] . The provided examples are hyperlinked to the source code file . Type Sub Type Description [ D ] Example [ E ] % age Requirement Functional > Improve - ment [ D ] : Comments reporting improvements to the existing implementation . [ E ] : “Network Li - brary could be added too” 19 . 6 Functional > New Fea - tures [ D ] : Comments reporting missing features that need to be implemented later on . [ E ] : “TODO handle attention histories” 15 Non Functional [ D ] : SATD for improving the performance of the current implementation . [ E ] : “TODO I think the slow loading of the encoder might have something to do with the device it” 1 . 5 Configuration - [ D ] : SATD indicating implementation using unsure configurations . [ E ] : “300 iterations seems good enough but you can certainly train longer” 12 . 3 Code Low Internal Quality [ D ] Reported SATD indicating issues with code quality . [ E ] : “TODO change format of format - ted _ preds in QA list of dicts” 10 . 9 Refactoring [ D ] SATD indicating refactoring code or re - moving dead code . [ E ] : “TODO make this code simpler” 10 . 3 Workaround [ D ] SATD incurring tentative or Ad - Hoc im - plementations which need to be fixed later . [ E ] : “naive approach for generating forward mapping this is naive and probably not robust” 9 . 4 Design Design Patterns [ D ] : SATD indicating violations of good Object - Oriented design . [ E ] : “XXX should this method be made more general” 6 . 2 Defect Known Defect to fix [ D ] : SATD indicating known incorrect im - plementations leading to buggy behavior . [ E ] : “TODO unintended side effect on e . g . 2003 05 09” 8 . 2 Testing Inadequate Testing [ D ] : Reported SATD indicating missing tests for specific . [ E ] : “TODO 1 Test the fully _ connected and conv2d function” 5 . 0 Undefined - [ D ] : Comments indicated as SATD , but the debt description is missing . [ E ] : “todo " 3 . 5 On Hold - [ D ] : Existing SATD comments in the code even though the debt has been paid . [ E ] : “todo maxlen” ( maximum length implementation was already done in code ) 3 . 2 Unknown - [ D ] : Debt unclear from the description . [ E ] : “TODO 算 法 4” 1 . 2 Documentation - [ D ] : SATD indicating missing documetation . [ E ] : “TODO add logger info” 0 . 6 ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 16 Bhatia , et al . software projects . In this study , we found them to represent respectively 30 % , 36 % , 8 % , 13 % , 5 % , and 6 % of SATDs in ML software projects ( as shown in Figure 9 ) . This indicates that ML software has lower proportions of defect and design debts and higher proportions of requirement debt than traditional ( non - ML ) software projects . However , the proportions of code , test , and documentation debts are similar , while we also found new SATD types of Configuration debt ( 12 % ) and Inadequate tests ( 8 % ) . : Summary of RQ2 The categories of SATDs that dominate in ML projects are Requirement debt ( 36 % ) , Code ( 30 % ) , Configuration debt ( 12 % ) , Known defect to fix ( 8 % ) , and Inadequate tests ( 5 % ) debts . Configuration debt and Inadequate tests were not found in traditional software projects . C o d e R e q u i r e m e n t D e f e c t D e s i g n T e s t i n g D o c u m e n t a t i o n O n - H o l d C o n f i g u r a t i o n 0 5 10 15 20 25 30 35 D e b t P e r c e n t a g e Bavota et al . This paper Fig . 9 . Comparison of types of debt identified by our research and prior research conducted by Bavota et al . [ 9 ] . On - Hold and Configuration debts are unique to ML - based systems . 4 . 3 RQ3 : Which components of the ML pipeline are more prone to SATD ? Motivation . A typical ML software pipeline involves components for reading data and cleaning data : 1 ) transforming data into features that can be consumed by the ML model ; 2 ) training the model ; 3 ) evaluating and validating the model , 4 ) and finally deploying the model [ 3 ] . During the evolution of the system , engineering teams often have to update these components to experiment with new models or adjust to changes in operation data distributions . Through this process , as observed by Sculley et al . [ 7 ] , different types of technical debt can be introduced . In this research question , we examine the proportion of SATD contained in different components of the studied projects . By knowing which components are the most prone to technical debt , engineering teams will be able to better prioritize their maintenance activities and better support their ML engineers , e . g . , with best practices and – or efficient tools . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 17 Approach . We use the same sample of 611 comments that were created for RQ2 to analyze the ML code components . We mapped each SATD comment to one of the following five com - ponents of ML workflow : data reading , data preprocessing , model building , model validaton and model deployment , using clues from the implementation of the functions , blocks , filenames , and function calls in which the technical debts were admitted . For example , in the unmanned au - tonomous vehicle project flywave 16 , the SATD at line # 516 17 is contained within the function name , “ send _ param _ command _ packet ” within the file “ networking \ bleConnection . py ” . These clues indicate extraction of telemetry data from the drone sensors , for which we marked the ML code component as “Data Read” . Similar to RQ2 , we employed card sorting , discussing in a pilot every classification disagreement until reaching a consensus . The Cohen’s Kappa inter - rater agreement between the first two authors on the sub - sample of 65 data points is 81 % . An inter - rater agreement of more than 70 % is considered adequate [ 60 ] . Similar to RQ2 , we removed false - positive cases in the remainder of the analysis . Table 5 . Results of manual analysis of SATD found in the different components of machine learning code . The Table rows have been ordered in decreasing order of percentage of occurrence . The examples are hyperlinked to the source code . ML Component Description Example % age Model Building Training the model including computa - tion , optimization of the cost function , and hyperparameter tuning . “Train on a negative log likelihood of classifying the right move” 29 . 0 % Data Preprocess - ing Transforming data into features or ten - sors that can be consumed by the model for its training and validation purposes . “ugly hack handling non - breaking space no mat - ter how badly its been en - coded in the input” 24 . 7 % Data Reading Reading data from external sources like databases , csv , stream / batch processes . “TODO ! see github url for some data sources” 8 . 6 % Model Deploy - ment Deploying the ML pipeline for application - specific use . “Note : There is a slight bug in the bounding box annotation data . ” 7 . 5 % Model Validation Validation of the correctness of the model and testing its robustness . ‘Check that regularizers were grouped properly . ” 4 . 0 % Results . The majority of SATDs in ML code resides in Model building and data prepro - cessing components . We found 111 SATDs belonging to model generation and 92 SATDs in data preprocessing . We also found 69 SATDs within the non - ML code portion of the studied projects . Finally , we found 15 SATDs in model validation , 29 SATDs in model deployment and 32 SATDs in the data reading . We could not identify a corresponding component for 22 SATD samples . We present examples of SATD found in the different ML components in Table 5 , alongside the description of the component and the proportion of SATDs found in the component . For instance , the following SATD comment “TODO : Keras BatchNormalization mistakenly refers to var " in the project “YAD2K : Yet Another Darknet 2Keras” 18 belongs to the Model Training component . Model Building has the largest proportion of SATDs ( 46 . 7 % ) , whereas Model Deployment has the lowest , i . e . , 0 . 9 % . This result is not very surprising given the amount of code churn that the model building 16 https : / / github . com / UAVs - at - Berkeley / flywave 17 https : / / github . com / UAVs - at - Berkeley / flywave / tree / master / / networking / bleConnection . py 18 https : / / github . com / farzaa / DeepLeague / blob / master / YAD2K / yad2k . pyLine : 144 ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 18 Bhatia , et al . components experience , as a result of the frequent retraining that is required to cope with concept drift issues . M o d e l B u il d i n g D a t a P r e p r o c e ss i n g N o n - M L D a t a R e a d M o d e l D e p l o y m e n t M o d e l V a li d a t i o n U n d e f i n e d 0 . 0 % 5 . 0 % 10 . 0 % 15 . 0 % 20 . 0 % 25 . 0 % 30 . 0 % Fig . 10 . Percentage of the analyzed SATD across the different ML code components . : Summary of RQ3 Overall , SATD occurs in all five key components of ML software , i . e . , Data Reading , Model Training , Feature Transformation , Model Validation , and Model Deployment . However , Model Training has the largest proportion of SATD , whereas Model Validation has the lowest proportion . 4 . 4 RQ4 : How long does SATD survive in ML - based systems ? Motivation . Several empirical studies report that technical debt leads to low quality ( in particular maintainability ) [ 7 , 9 ] , and makes further changes more expensive in the long run . Studying the life cycle of SATD is therefore important to identify the types of SATDs that tend to linger longer in the code , and hence may require special attention from maintenance teams . In this research question , we examine the timing of introduction and removal of SATD to provide development teams with a holistic view of SATD survival in ML software systems , and allow them to better prioritize maintenance activities and allocate their limited resources efficiently . Approach . We perform survival analysis [ 61 ] to examine , for each project , ( i ) the time until the introduction of SATD and ( ii ) the time taken to remove SATDs after they are introduced in a file for the first time . The former analysis examines the survival of ML software projects concerning debt occurrence , while the latter analysis is about the survival of SATDs instances once they are introduced in an ML - based software project . Survival analysis [ 61 ] is a statistical technique for analyzing the expected duration of time until the occurrence of an event of interest or censoring of an event ( if the event does not happen ) . The event of interest in our study is the occurrence of SATD across the entire project history , and the length of the observation window is the start of the project until the time at which we obtained the projects’ latest snapshot at the time of performing the empirical study ( Summer 2021 ) . If the event is not observed during the observation period , the corresponding subject will be censored at the end of the period , which survival analysis is able to deal with . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 19 Observed Event SATD Introduction SATD Removal Censoring File never had SATD SATD never removed Time to event ( 𝑇 ) 𝛿 ( 𝑇 SATD intro − 𝑇 File create ) 𝛿 ( 𝑇 SATD remove − 𝑇 SATD intro ) Status Observed ( 1 ) / Censored ( 0 ) Observed ( 1 ) / Censored ( 0 ) Table 6 . Summary of events and corresponding variables for survival analysis Time to event and status are two important variables for survival analysis . Time to event ( 𝑇 ) is the elapsed time between the beginning of the observation ( start date of the project ) and the occurrence of the event of interest or the censoring of data . In our study , this time is measured in # days . Hence , 𝑇 is a random variable with positive values [ 61 ] . A summary of the Time ( 𝑇 ) , Event of interest , and Status is provided in Table 6 . For the introduction of SATD , 𝑇 is the elapsed time between the start date of the project to the time of contamination of the file by SATD . For the removal of SATD , 𝑇 is the elapsed time between the removal and the introduction of SATD . Status is a Boolean variable that indicates whether the event of interest is observed or whether the data is censored . The Survival function S ( t ) gives the probability ( 𝑃 ( 𝑇 > 𝑡 ) ) that a subject will survive beyond time 𝑡 . After arranging our data in increasing order of 𝑇 , we plot the survival curve and estimate the survival probability using the non - parametric Kaplan - Meier estimator [ 62 ] . The Kaplan - Meier estimation is computed following Equation 1 , where 𝑡 𝑖 is the time duration up to event - occurrence point 𝑖 , 𝑑 𝑖 is the number of event occurrences up to 𝑡 𝑖 , and 𝑛 𝑖 is the number of subjects that survive just before 𝑡 𝑖 . 𝑛 𝑖 and 𝑑 𝑖 are obtained from the aforementioned ordered data . 𝑆 ( 𝑡 ) = (cid:214) 𝑖 : 𝑡 𝑖 ≤ 𝑡 [ 1 − 𝑑 𝑖 𝑛 𝑖 ] ( 1 ) Fig . 11 . Survival analysis for the introduction of SATD into a clean source code file for ML and non - ML projects . The dotted red line represents the 1 - year mark with 0 . 95 and 0 . 37 probabilities of survival of a clean source code file in non - ML and ML projects respectively . 4 . 4 . 1 Results . Debt addition . SATDs are introduced in ML projects 140 times earlier during the develop - ment process in comparison with non - ML projects . A median ML debt is added in 10 days , while a median non - ML debt is added in 1 , 405 days . Figure 11 shows the survival curves in ML and non - ML code , with the difference between both curves statistically significant with Wilcoxon rank sum test p - value = 0 . 0 . The steep decline of the ML curve in Figure 11 reflects a faster decline of the probability of ML projects to remain clean , i . e . , remain without SATD . Within just one year of development time , ML projects have a low probability ( i . e . , 0 . 36 ) of its source code remaining debt - free , whereas non - ML projects have a much higher probability ( i . e . , 0 . 9 ) . Even after 4 years , the probability of SATD introduction in non - ML projects is still below 0 . 3 ( i . e . , probability of SATD - free source code above 0 . 7 ) , while it is 0 . 05 for ML projects . Debt removal . SATD is removed 3 . 7 times faster in ML projects in comparison with non - ML projects . After its addition , a median ML SATD is removed in 1 . 1 years , whereas a non - ML SATD is removed in 4 . 1 years . Figure 12 shows the likelihood of debt removal in ML and non - ML projects . After a two - year time span , the probability of SATD to survive in ML projects is 0 . 89 , while this probability is 0 . 94 for non - ML projects . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 20 Bhatia , et al . Such observations imply that ML developers may be cognizant of the faster debt accumulation in ML code and hence dedicate more time and effort towards its mitigation than developers working on non - ML code . Another reason could be that the shorter lifetime of SATD in ML projects may also be due to the shorter life cycle of these projects . In fact , in our data set , the medium lifecycle of ML projects is 1 . 15 years , while it is 4 . 16 years for non - ML projects . Another possible reason could be the fact that the repetitive nature of ML experiments flushes out debt while dead experimental code paths are being refactored . This is corroborated by the 52 % of cases of SATD removal in ML code that coincided with the deletion of the actual source code file . For the remaining 48 % of cases , the file was only modified . This may indicate that SATD removal occurs during massive refactoring where the debt - containing files are removed themselves . Overall , our findings corroborate the idea that SATD in ML projects is both introduced and resolved more rapidly than in non - ML projects . This higher code churn rate in ML software development was further discussed and evidenced in the results of RQ1 ( see Section 4 . 1 ) , providing additional support for our initial assertions . Prior research found that higher churn also correlates with higher likelihood of bugs [ 55 , 56 ] . : Summary of RQ4 SATD is both introduced and removed significantly earlier in ML projects in comparison to non - ML projects . 0 1000 2000 3000 4000 Number of Days 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r o b a b ili t y o f S A T D S u r v i v a l Project Domains ML Non - ML Fig . 12 . Survival analysis of existing SATD in ML and non - ML source code files . 4 . 5 RQ5 : What are the characteristics of long - lasting SATDs in ML - based systems ? Motivation . Given the increasing efforts of settling technical debt over time , it is important to understand factors that may contribute to long - lasting debt , so that development teams can take quick actions , and allow for faster technical debt removal . Approach . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 21 Table 7 . Metrics used to analyze the evolutionary patterns of SATDs and predict their occurrence . Dimension Name Definition Change LA _ [ mod ] # Lines added during the modification . LD _ [ mod ] # Lines removed during modification . CM _ [ mod ] Total # “changed methods” during modification . CT _ [ mod ] “Change type” of the modified file can be “Add” , “Edit” or “Delete” . Diffusion LA _ [ diff ] Total # lines added for all modifications in the commit . LD _ [ diff ] Total # lines removed for all modifications in the commit . LM _ [ diff ] Total # “lines modifications” in the files in the commit . MD _ [ diff ] # “Modified directories” where source code files were modi - fied . MF _ [ diff ] Total count of python files modified . Complexity Complexity Cyclomatic complexity of the file after the modification . LOC # Total lines of code after the modification . Tokens Total token # after the modification . History Had _ SATD _ [ hist ] Whether the modified file had SATD in its previous versions . PT _ [ hist ] “Project Time” or time between the first and modification commit . LCT _ [ hist ] “Latest Commit Time” or time between the latest and modi - fication commit . FC _ [ hist ] # “File Changes” or number of prior commits where the file was edited . 4 . 6 Selection of Metrics To understand the characteristics contributing to long - lasting debt , we capture metrics in the context in which SATDs are introduced . Specifically , we collect information about the code change ( i . e . , LA _ [ mod ] , LD _ [ mod ] , CM _ [ mod ] , CT _ [ mod ] ) , the diffusion of the change ( i . e . , LA _ [ diff ] , LD _ [ diff ] , LM _ [ diff ] , MD _ [ diff ] , MF _ [ diff ] ) , the complexity of the files involved in the change ( Complexity , LOC , Tokens ) , and the timing of prior changes and SATD occurrences in the commit’s files ( i . e . , Had _ SATD _ [ hist ] , PT _ [ hist ] , LCT _ [ hist ] , FC _ [ hist ] ) . These metrics are summarized in Table 7 . Our dimensions ‘Change’ , ‘Diffusion’ , ‘Complexity’ , and ‘History’ , are widely employed in the domain of SDP ( software defect prediction ) , a well researched field in Software Engineering . The rationale for their usage in the context of explaining SATD survival is based on the understanding that areas of code that undergo frequent changes , or are complex , are more prone to defects . Given that SATD indicates sub - optimal implementations that may not be outright defects but could result in compromised software quality , we consider using these metrics for SATD prediction . Moreover , prior research in SATD prediction by Yan et al . [ 63 ] previously employed such dimensions ( i . e . , diffusion , history , and message ) in their study . In contrast to Yan et al . ’s research , we exclude the message dimension from our study as our focus is on assessing the attributes of persistent self - admitted technical debt ( SATD ) rather than SATD detection itself . We explain the rationale for selecting each dimension below : • Change . As ML models are refined and improved , methods and functions may undergo frequent modifications . Rapid iterations and changes in ML codebases can introduce debt as developers make trade - offs between short - term expedient decisions for immediate gains . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 22 Bhatia , et al . • Diffusion . As data and model architectures evolve over time , changes can propagate across multiple files and directories . A high count of changes and modifications in certain areas might indicate collaborative features or components undergoing significant evolution , signaling potential areas where SATD could arise due to integration or coherence challenges among contributions . • History . ‘History’ metrics like ‘PT _ [ hist ] ‘ and ‘FC _ [ hist ] ’ lend insights into the temporal aspects of code changes . As ML projects evolve , certain modules can accumulate technical debt due to shifting project requirements or the rapid evolution of ML techniques . Monitoring the frequency and timing of changes in relation to SATD introductions can be valuable metrics for anticipating future debts . • Complexity . These metrics do not just capture control flow intricacies but also provide insights into the inherent complexity of ML - related tasks , such as feature extraction or model training routines . By analyzing the total token count or number of lines of code , we can gauge the intricacy of certain ML tasks to identify areas where SATD might be more prevalent due to the intricate nature of ML routines . 4 . 6 . 1 Data Mining . We use the Python framework PyDriller 19 to extract all commits submitted to the repositories of our studied projects . Each commit consists of a comment describing the reason for the commit and source code diffs describing the changes performed on source code files . We developed a custom parser to extract the changed comments accompanying the source code changes and used our SATD detector to identify technical debt admissions as described in Section 4 . 1 . We also use PyDriller to extract source code diffs and compute the metrics described in Table 7 . 4 . 6 . 2 Assigning Class Labels . To identify long - lasting SATDs , we compute the time elapsed since the creation of the comment admitting the debt . We compute this elapsed time using information from the commit diffs ( i . e . , + + and −− lines of code comments for each source code file in the commit ) . Specifically , we calculate the time between the introduction of the SATD comment and its removal . From our collected data , we observed cases in which the same SATD commit is present multiple times within the same file , and removed these cases from our case study to avoid biasing our results with duplicated information . After computing SATD duration time for a total of 6 , 427 SATD instances in our dataset , we computed the quartiles of the obtained distribution and used the first and fourth quartile to identify “quick removal” and “long lasting” SATD respectively . In total , we obtained 2 , 595 SATD instances with duration times above 75 th percentile . We assigned them to the group of long - lasting SATDs . We obtained the first quantile , i . e . , 2 , 595 instances of with duration times below the 25 th percentile as the quick removal SATD . 4 . 6 . 3 Correlation and Redundancy Analysis . Before training our explanatory models , we performed correlation and redundancy analysis to prevent issues related to multicollinearity between variables . We used the R package Hmisc 20 to identify the correlated variables . We used the threshold of 0 . 7 on the Spearman’s rank correlation between the exploratory variables . From an initial set of 14 metrics , we removed four metrics . A pictorial representation of our correlation analysis is provided in Figure 13 . After removing the correlated metrics , LA _ [ diff ] was removed during the redundancy analysis 21 . Finally , the metrics : Complexity , Churn _ [ diff ] , MD _ [ diff ] LD _ [ diff ] , LA _ [ mod ] , LD _ [ mod ] , CM _ [ mod ] , LCT _ [ hist ] , PT _ [ hist ] , and Had _ SATD [ hist ] survived our correlation and redundancy analysis . 19 http : / / pydriller . readthedocs . io / 20 https : / / cran . r - project . org / web / packages / Hmisc / index . html 21 https : / / www . rdocumentation . org / packages / Hmisc / versions / 4 . 5 - 0 / topics / redun ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 23 C o m p l e x i t y L O C T o k en s L A _ [ d i f f ] M D _ [ d i ff ] L D _ [ d i ff ] L A _ [ m od ] L D _ [ m od ] L C T _ [ h i s t ] P T _ [ h i s t ] C hu r n _ [ d i ff ] M F _ [ d i ff ] C M _ [ m od ] H ad _ SA T D _ [ h i s t ] 1 . 0 0 . 8 0 . 6 0 . 4 0 . 2 0 . 0 S pea r m an r 2 Fig . 13 . Correlation analysis dendrogram showing which pairs of features are correlated . Dotted line indicated a correlation threshold of 0 . 7 4 . 6 . 4 Model Building . We chose the Random Forest classifier for our explanatory model . A Random Forest classifier is created using bagging of several decision trees and taking their aggregate value as the output of the classifier . We use Random Forest for its fast prediction speed via parallelizability , robustness against feature scaling , and robustness against outliers and non - linear data . We used the python SciKit Learn 22 framework for building the models . 4 . 6 . 5 Model Validation . To ensure the statistical robustness of our results , we perform a 100 - out - of - sample bootstrapping . The approach involves sampling the training set from the whole dataset with replacement . The testing set comprises all the samples present in the original data but absent in the bootstrapped training data . We build and test models on each training and testing set to draw statistically robust conclusions as suggested by [ 64 ] 4 . 6 . 6 Model Interpretation . We use SHAP ( SHapely Additive exPlanations ) , to interpret and explain our models . A Shapley value is the average marginal contribution of a feature value across all possible coalitions [ 65 ] . We compute these values using the Python framework , SHAP 23 which provides a collaborative game theory approach for the calculation of the average marginal contribution of each of the model’s features . Results . Figure 14 shows mean Shapely values obtained for the best performing Random Forest model ( from the bootstrapping analysis ) . Our models from the bootstrapping analysis have a median Precision of 93 % , Recall of 98 . 8 % , and F1 Score of 95 . 8 % . SATD introduced during large code changes ( Churn _ [ diff ] ) spanning multiple files ( MD _ [ diff ] ) are likely to be long - lasting SATD according to our explanatory model . The timing of the com - mit , i . e . , PT _ [ hist ] ( Project Time ) and LCT _ [ hist ] ( Last Commit Time ) also appear to be important factors in the duration of SATDs in the studied projects . Long - lasting SATDs are added at a median of 231 days as compared to quickly removed SATDs which are added at a median of 7 days ( i . e . , towards the beginning of the project ) . 22 https : / / scikit - learn . org / stable / modules / generated / sklearn . ensemble . RandomForestClassifier . html 23 https : / / github . com / slundberg / shap ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 24 Bhatia , et al . 0 . 00 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 0 . 12 mean ( | SHAP value | ) ( average impact on model output magnitude ) Has _ SATD [ hist ] CM _ [ mod ] Complexity LD _ [ mod ] LA _ [ mod ] LCT _ [ hist ] PT _ [ hist ] LD _ [ diff ] Churn _ [ diff ] MD _ [ diff ] Fig . 14 . Global model interpretation from SHAP for prediction of long lasting SATD . Interestingly , long - lasting SATDs are added in smaller files . The median complexity of files containing long - lasting SATDs is 25 ; while the median complexity of files where SATDs are removed quicker is 40 . This finding suggests that ML developers pay special attention to SATDs occurring in complex files and tend to ignore those scattered through low complexity files . One hypothesis is that when SATD is introduced into small files during commits that introduce large changes , developers may perhaps orient their attention towards the files that perform more functionality ( high complexity ) , thereby overlooking correcting the small ones . Such debt may be deemed non - urgent by the developers . Since the functionality of such low complexity files is trivial , debt in these files may be considered “harmless” , and hence debt correction is not prioritized like for the debt instances in large files . Adding to the above hypothesis , it is also probable that resource allocation might be a factor , with developers focusing their efforts on managing SATD in larger , more critical files , similar to how testing larger files might find more bugs [ 66 , 67 ] . Risk perception could also play a part , as SATD in smaller files might be viewed as less likely to cause major issues . Familiarity might contribute too , with developers potentially more at ease managing SATD in larger files they frequently work on . Oversight during code reviews , due to the less complex nature of small files , might allow more SATD to persist . Developers could also underestimate the impact of SATD in these files , falsely assuming its repercussions will be negligible . Additionally , it is plausible that smaller files , deemed easier to rewrite or refactor in the future , might have their SATD issues postponed . Lastly , the cognitive load to understand and fix SATD in larger files might draw developers’ focus away from smaller ones . Future work should validate these hypotheses . : Summary of RQ5 Long - lasting SATDs are introduced during large code changes spanning multiple files ( with low complexity ) . Long - lasting SATDs are added at a median of 231 days as compared to quickly removed SATDs which are added at a median of 7 days ( i . e . , towards the beginning of the project ) . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 25 5 THREATS TO VALIDITY 5 . 1 Internal Validity The act of “self - admitting” technical debt can manifest through various channels , including GitHub issues or project management tools such as Trello and Jira . Consequently , measuring SATD solely through code comments might not encompass the entire spectrum of " self - admission " of debt in software projects . Nevertheless , Potdar and Shihab [ 28 ] have highlighted SATD in code comments as a discernible metric for ( SA ) TD . Furthermore , we follow authoritative research ( [ 9 , 18 , 19 , 36 , 38 , 42 , 68 , 68 – 71 ] ) in the area of self - admitted TD , that have estimated SATD via code comments . The SATD identification tool used in our study may provide false positives . Although there are many SATD identification tools , we chose a NLP - based SATD detector proposed by Liu et al . [ 36 ] . This tool , unlike other state - of - the - art debt detector tools that provide high precision and low recall , has a high recall . Moreover , this tool has been utilized in several previous studies , such as those by Liu et al . [ 14 ] and Zampetti et al . [ 42 ] . In addressing RQ2 and RQ3 , two of the authors manually examined 611 instances of SATDs . This process may have introduced a subjective bias . However , given the high Cohen Kappa score of 94 % between the two authors , we believe that the risk of bias is minimal . Furthermore , to compare the qualitative results of RQ2 , we rely on the findings of Bavota and Russo [ 9 ] and do not use our sample of 318 non - ML projects . This is because our extensive qualitative analysis , which took several months of efforts to label the 611 SATD samples , is non - trivial . Replicating such efforts on a sample of non - ML SATD instances was not possible in the scope of this study . 5 . 2 Construct Validity There is no consensus among researchers regarding the inclusion of missed requirements or known defects in the concept of SATD . The seminal work by Kruchten et al . [ 19 ] does not regard requirement and defect debt as SATD , however , the work of Bavota et al . [ 9 ] does consider the two categories as SATD . Despite these differences , as the majority of research ( [ 9 , 38 , 68 – 71 ] ) on SATD accepts Requirement and Defect Debt as SATD , we include these categories in our manual analysis for RQ2 . This potential variation in the understanding and interpretation of SATD poses a construct validity threat as it could lead to varying interpretations and thus results . In this work , we use survival analysis to examine for each project the time until the introduction of SATD . Specifically , we use the non - parametric Kaplan - Meier estimator . We also used the non - parametric Wilcoxon Rank Sum test to compare populations . Both these tests do not require the data to be normally distributed . In RQ5 , we assume that the admitted technical debt is fixed when the SATD comment is removed . However , it could be possible that the debt still lingers on even after removing the SATD comment . About 40 % of SATD instances were categorized as ‘Undefined’ , ‘Unknown’ , or ‘False Positives’ , reflecting the subjective nature of this classification in RQ2 . This ambiguity , particularly in distin - guishing between actual technical debt and inaccurately flagged instances may potentially affect the our RQ1 findings . However , given the inherent uncertainties and challenges are associated with categorizing SATD , and its reasonable that both ML and non - ML SATD exhibit similar patterns of being undefined , unknown or the tool detecting false positives . Hence , our main conclusion regarding higher prevalence of SATD in ML projects still stands , and the “potential” parallel does not undermine our core conclusion of RQ1 , i . e . , ML debt is twice more prevalent than non - ML debt . Finally , for RQ4 and RQ5 , we limited our extraction of SATD to code modifications solely from the master branch . It is worth noting that SATDs might have been introduced or removed in branches other than the master branch . Our decision to focus on the master branch was due to the current challenges in constructing a change log post - rebasing — an unresolved issue that remains a ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 26 Bhatia , et al . topic of active research . Including data from other branches would not only expand our dataset substantially but could also introduce complexities in accurately mapping the introduction and removal of SATDs across different branches . Moreover , the master branch is the “final” code meant to be seen by the rest of the organization . Hence , SATD in the master branch is of high importance and impacts others who are using the repository . 5 . 3 External Validity Finally , the results of our findings are dependent on our sampled data . While the sample of ML projects was based on exhaustive search ( i . e . , including relevant projects in a domain until sub - sequent projects are irrelevant ) , procuring non - ML projects was based on convenience sampling ( getting the top - 318 non - ML projects as long as the selection criteria are met ) . Although a random sample would offer a more unbiased comparison , obtaining all non - ML Python projects across all GitHub pages , then selecting a random sample may not be feasible . Moreover , although we select mature projects by using selection criteria inspired by related work [ 48 , 51 , 52 ] such as a project requiring more than a month of development history , it is still possible that immature projects may seep into our analysis , biasing our results . Following prior research [ 51 , 52 ] that has commonly mitigated this threat using threshold - based criteria , we establish five filtering criteria ( e . g . , a project should have more than one month of developmental history ) for the removal of toy projects . Notably , our ML dataset lacks test cases . This absence signifies a deviation from traditional software engineering where tests involve unittests , integration tests , and code quality is measured through the coverage of such tests . However , such testing practices do not seamlessly translate to ML scenarios [ 72 ] 24 25 . Instead of test cases , ML testing often involves ensuring a model has high performance , high robustness , lack of bias , etc . Indeed , the lack of quality assurance standards ( for instance % test coverage that are easily measured in traditional software ) in ML systems , in itself is a motivation for our study . We conducted an analysis of a diverse portfolio consisting of 318 projects from well - established ML domains like “self - driving car” and “natural language processing” to ensure that our findings are representative of industry - grade ML projects . However , our results may not encapsulate the SATD phenomena across the entire spectrum of ML software systems . In RQ3 , our analysis was confined to the examination of SATD within five components ( i . e . , Model Building , Data Prepossessing , Data Reading , Model Validation , and Model Deployment ) of the ML pipeline as proposed by Amershi et al . [ 3 ] . Consequently , the identified technical debt may not fully represent the nuances of TD that could occur in expanded areas of ML pipelines like model monitoring , maintenance , and evolution . Future studies could aim to analyze TD in these expanded areas or in MLOps development and monitoring tasks to provide a more comprehensive understanding of TD in overall ML - based software development . This limitation suggests that our results are primarily applicable to the specific context we investigated , and caution should be taken when generalizing the findings to these other contexts . 5 . 4 Conclusion Validity Just like debt in monetary situations , it may be argued that not all technical debt is bad and needs to be paid off eventually . Technical debt ( TD ) manifests in various forms , such as organizational , discussion , and product - related TD , and not all TD is detectable by tools . Prior studies by Potdar and Shihab [ 28 ] recognized SATD as an effective heuristic for estimating TD in software systems . 24 https : / / fg - tav . gi . de / fileadmin / FG / TAV / 47 . TAV / TAV47 - Felderer - TestingTheUntestable . pdf 25 https : / / shorturl . at / dnBDU ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 27 The importance of understanding SATD for software maintenance and management has been ac - knowledged in previous non - ML research [ 9 , 12 , 13 , 15 – 19 , 30 , 73 , 74 ] , and generally the prevalence of SATD is considered a bad code smell [ 75 ] . While our research does not conclusively measure the effort involved in debt removal and in estimating its crippling effects , we lay a strong foundation for future investigations . Notably , we found that most of the debt exists in ML model building components ( as found in RQ3 ) , and the most recurring debt is " improvement of functional requirements " ( as found in RQ2 ) . This finding is supported by our identification of a new debt type , “configuration debt” , indicating sub - optimal implementations in data preprocessing and model building . Developers speculating on sub - optimal ML pipelines can have an even higher cost than traditional software due to the tight coupling between different components of the ML pipeline . TD in ML can have far - reaching detrimental effects across the pipeline , causing inefficiencies in the training process or ML model selection process to increase maintenance efforts . Overall , understanding the dynamics of SATD in ML software is essential for both software researchers and ML stakeholders . As our research shows , managing TD effectively in this area can significantly impact the overall quality and efficiency of ML software . 6 IMPLICATIONS Implications for ML Practitioners : ML practitioners should be more vigilant in providing better documentation for technical debt and be aware of the quick accumulation of debt . The results from RQ1 demonstrate that ML code contains a significantly higher proportion of technical debt compared to non - ML code , confirming the observation that ML code is highly debt - prone . The findings from RQ2 indicate that a substantial portion of this debt is related to requirements , suggesting that ML engineers should acknowledge sub - optimal implementations or missing features and plan for their implementation in the future . To address this , ML practitioners should strive to improve communication and collaboration between ML , non - ML and data developers ensuring that technical debt is managed effectively across the entire software system . Leveraging tools ( like DVC , MLflow ) and adopting best practices that facilitate this collaboration can be beneficial . Furthermore , our newly found SATD type , inadequate testing indicates the presence and need to improve ML testing to increase the reliability of code functionalities . Untested pieces of code could result in hidden ML bugs ( due to the stochastic nature of ML ) or manifest as sub - optimal ML model performance . RQ5 reveals that technical debt introduced during large code changes spanning multiple low - complexity files tends to linger longer in the source code . ML practitioners should pay special attention to such commits during code review activities and quickly refactor the corresponding technical debts when the code complexity is still low . While our results are based on our ML dataset , future research may to wish explore the generalizability of our findings on non - ML data as well . In RQ4 , we observed that ML code quickly accumulates debt , however debt removal is also quick . We attribute such quick accumulation and removal to high ML churn as shown in RQ1 . ML devel - opers engaged in such high churn software development should be cautious by prioritizing code reviews , automating testing , and ensuring proper documentation . Given the highly experimental nature of ML pipelines , developers may also wish to employ the use of feature flags , to turn on and off specific functionalities to allow for safe experimentation without constantly altering the codebase . Overall , ML engineering teams should also focus on improving code documentation , imple - menting effective test cases , and ensuring the reliability of ML pipelines and models . By enforcing ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 28 Bhatia , et al . abstraction barriers , increasing code readability and documentation , avoiding unnecessary depen - dencies , and reducing complexity , ML practitioners can improve the overall software quality and maintainability of their ML systems . Frequent refactoring and restructuring of software system design would help remove technical debt and reduce complexity . Implications for Researchers and Toolsmiths : The findings from RQ1 , RQ2 , and RQ3 highlight the prevalence of technical debt in ML code , particularly in relation to requirements , testing , and configuration . Researchers and toolsmiths should develop techniques and tools that support ML engineering teams in efficiently handling requirements , managing data dependen - cies and model dependencies effectively , and improving testing infrastructure and configuration management . There is a need for tools that detect and remove technical debt , estimate repayment efforts , and assess the impact of different types of technical debt to prioritize their resolution during maintenance activities . Researchers should also investigate the implications of technical debt in emerging ML paradigms , such as federated learning and reinforcement learning , and develop tailored strategies for addressing technical debt in these contexts . Section 7 provides an in - depth explanation of the potential research and tooling avenues inspired by our research . Implications for ML Educators : Given the prevalence of technical debt in ML code , ML educators should incorporate the concepts of technical debt management , software engineering best practices , and documentation strategies in their ML courses and training programs . By educating the next generation of ML practitioners about the potential pitfalls and long - term consequences of technical debt , ML educators can contribute to improving the overall quality and maintainability of ML systems . Case studies and real - world examples should be included to demonstrate the impact of technical debt on ML projects , fostering a deep understanding of the consequences of neglecting technical debt . Given that requirement debt is most prevalent ( RQ2 ) , and the model building side accumulates the maximum amount of debt ( RQ3 ) , ML educators may wish to provide special focus on such areas . This approach can help ML practitioners to be better equipped to manage technical debt effectively . 7 FUTURE WORK In this section we discuss the future work that can be inspired by our research . • Examining TD ( SATD ) in other aspects of ML development . In RQ3 , we label 611 ML Self - Admitted Technical Debt ( SATD ) instances onto the ML components proposed by Amershi et al [ 3 ] . Future research may consider examining Technical Debt ( TD ) in ML components that are not mentioned in Amershi’s pipeline . This could involve supporting tasks like model monitoring , maintenance , and evolution , ethics and fairness evaluation or the user feedback loop . Evaluating SATD in ML continuous integration / continuous delivery ( CI / CD ) tasks like model / data versioning , EOL ( end of life planning ) or rollback and redeployments , etc . could also be a valuable area for the research community . Overall , future work could investigate TD’s impact on the performance and robustness of ML models , as well as the overall system’s reliability , security , and maintainability . • Automated TD prioritization through SATD identification : A potential avenue for future research is developing automated methods to prioritize SATD in ML - based software projects . These methods could factor in various aspects of TD , such as severity , potential con - sequences specific to ML , and the cost of addressing ML debt , debt arising from data handling , inconsistencies in ML models , and other integral components of ML software development . Automated approaches could assist developers and project managers in efficiently managing SATD . This could involve quickly identifying areas of concern and allocating resources to ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 29 minimize negative impacts of accumulated debt on the system’s quality , maintainability , and performance . • Team dynamics and development processes : Investigating the relationship between team dynamics , development processes , and the introduction of SATD in ML - based soft - ware projects can provide valuable insights into factors contributing to the accumulation of technical debt . Future research could explore the influence of factors like team size , experi - ence , communication patterns , and development methodologies on the prevalence and types of SATD in ML - based software systems . Understanding these relationships might lead to strategies and best practices for minimizing SATD during the software development lifecycle . • Guidelines and best practices for managing ML SATD : A crucial area for future work is proposing actionable guidelines and best practices for managing and mitigating SATD in ML - based software projects . These guidelines could help developers and project managers make informed decisions when dealing with trade - offs and competing priorities during ML software development . They could focus on various aspects of SATD , including prevention , detection , prioritization , and resolution , tailored to the unique characteristics and challenges of ML - based software systems . • Leveraging ML for detecting ML - specific SATD : Existing SATD detection tools mainly tar - get non - ML software . Due to the distinct nature of ML , there might be a need for ML - specific SATD detection tools that are more effective in identifying ML - related debt . Developing such tools in future research could facilitate efficient identification , tracking , and resolution of SATD throughout ML development , covering a wide range of ML tasks ( [ 3 ] ) . • Evaluating the impact of SATD on model performance : Since ML - based systems contain a ML - pipeline which produces an ML model , debt in ML based systems may impact the performance of ML models . For instance , debt in data - preprocessing or model building components may lead to a model with poor performance . Future research could explore the impact of technical debt on the model performance metrics , such as accuracy , precision , recall , and F1 - score , to help ML practitioners prioritize addressing technical debt that has the most significant impact on their models . • Evaluation of SATD pre - rebasing : Our research primarily examines the spread of SATD within the master branch . However , future studies might consider investigating the behavior of SATD across various branches . Given that change evaluation across branches is a bur - geoning field of research [ 2 ] , examining SATD dynamics in this context could be valuable in understanding debt 8 CONCLUSION Developers make sub - optimal decisions and introduce technical debt while implementing ML software . Although prior studies have investigated SATD in non - ML software , the nature of SATD in ML code can be expected to be much different than non - ML code . We conclude our study by elucidating the key contributions of our study , underscoring its significance in enhancing the existing body of research in Self - Admitted Technical Debt ( SATD ) and Software Engineering for Machine Learning ( SE4ML ) . For RQ1 , we find that Machine Learning ( ML ) code accumulates twice the debt in comparison to non - ML code . While Sculley et al . [ 7 ] previously hypothesized the potential for debt to cause maintenance challenges in ML software , ours is the first study to provide empirical evidence for the pronounced prevalence of debt in ML code . Notably , our finding that 2 % of non - ML code has SATD augments the findings from prior SATD studies on non - ML software . Our results align and further contextualize the research by Potdar and Shihab [ 28 ] , who ascertained that debt permeates in 2 . 7 % to 31 % of files , 0 . 4 % to 3 . 3 % at a class level , and 0 . 3 % to 2 . 6 % at a function level . Our methodology ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 30 Bhatia , et al . quantifies debt based on the percentage of comments marked as SATD , facilitating a comparative analysis between ML and non - ML comments . In RQ2 , our qualitative efforts extend the prior taxonomy of SATD . The prior taxonomy , by Bavota and Russo [ 9 ] , proposed seven years ago , necessitates augmentation to cater to the ML - dominated software development landscape that is more relevant today . In our qualitative analysis of SATD in ML code , we identified two new ML - specific SATD categories : “inadequate testing " and “configuration” debts . We believe that these categories emerged due to the unique challenges of ML pipeline development , which involves frequent experimentation for model optimization and management of hyperparameters and data preprocessing configurations . These findings offer a clearer view of the technical debts present in today’s ML - centric software development . In RQ3 , we analyzed SATD within different components of Amershi et al . ’s ML pipeline [ 3 ] . This not only categorizes the debt but also pinpoints where in the ML pipeline technical debt is more likely to occur , offering a structured approach to address ML debts . For RQ4 , we examined how quickly ML and non - ML debts are introduced and subsequently resolved . Our data shows that ML debts are introduced by 140 times and removed by 3 . 7 times more rapidly than non - ML debts . This aligns with the insights from Sculley et al . [ 7 ] , which underscore the critical nature of proficient debt management in ML development . Our results augment existing research on SATD survival in non - ML software . Bavota and Russo [ 9 ] reported debt survival in terms of commits and reported that SATD survives for a median of 266 commits , yet did not provide time - related results . Conversely , Maldonaldo et al . [ 31 ] indicated that SATD survives between 18 . 2 - 172 . 8 days , averaging at 82 - 613 . 2 days . While Maldonado presented average timelines for debt survival , we employed survival analysis using the Kaplan - Meier metric . This approach offers more robust insights than raw averages because it accounts for censored data , providing a comprehensive view of SATD’s life expectancy . In RQ5 , using a ML model interpretation , we delve into the characteristics of long - lasting debts in ML code . While past research briefly touched upon debt duration , to our knowledge , our study is the first to uncover an understanding of long - lasting debts , setting it apart from both ML and non - ML research in the area . Moreover , we use software metrics in the context of SATD . While earlier studies such as [ 63 ] utilized software metrics to detect SATD using ML models , our approach leverages these metrics to interpret the specific traits of long - lasting debt . Our findings indicate that files with lower complexity are more likely to permeate debt until a long time , suggesting developers should give heightened attention to such potentially overlooked small files . These results provide insights to practitioners towards better management of technical debt and call for more research work to improve our understanding of technical debt occurring in ML - based systems . REFERENCES [ 1 ] M . I . Jordan and T . M . Mitchell , “Machine learning : Trends , perspectives , and prospects , ” Science , vol . 349 , no . 6245 , pp . 255 – 260 , 2015 . [ 2 ] A . Bhatia , E . E . Eghan , M . Grichi , W . G . Cavanagh , Z . M . Jiang , and B . Adams , “Towards a change taxonomy for machine learning pipelines : Empirical study of ml pipelines and forks related to academic publications , ” Empirical Software Engineering , vol . 28 , no . 3 , p . 60 , 2023 . [ 3 ] S . Amershi , A . Begel , C . Bird , R . DeLine , H . Gall , E . Kamar , N . Nagappan , B . Nushi , and T . Zimmermann , “Software engineeringformachinelearning : Acasestudy , ”in 2019IEEE / ACM41stInternationalConferenceonSoftwareEngineering : Software Engineering in Practice ( ICSE - SEIP ) . IEEE , 2019 , pp . 291 – 300 . [ 4 ] D . Sato , A . Wider , and C . Windheuser , “Continuous delivery for machine learning , ” Martin Fowler , vol . 9 , 2019 . [ 5 ] A . D’Amour , K . Heller , D . Moldovan , B . Adlam , B . Alipanahi , A . Beutel , C . Chen , J . Deaton , J . Eisenstein , M . D . Hoffman et al . , “Underspecification presents challenges for credibility in modern machine learning , ” The Journal of Machine Learning Research , vol . 23 , no . 1 , pp . 10237 – 10297 , 2022 . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 31 [ 6 ] Z . Wan , X . Xia , D . Lo , and G . C . Murphy , “How does machine learning change software development practices ? ” IEEE Transactions on Software Engineering , vol . 47 , no . 9 , pp . 1857 – 1871 , 2019 . [ 7 ] D . Sculley , G . Holt , D . Golovin , E . Davydov , T . Phillips , D . Ebner , V . Chaudhary , M . Young , J . - F . Crespo , and D . Dennison , “Hidden technical debt in machine learning systems , ” in Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 , ser . NIPS’15 . Cambridge , MA , USA : MIT Press , 2015 , p . 2503 – 2511 . [ 8 ] T . Klinger , P . Tarr , P . Wagstrom , and C . Williams , “An enterprise perspective on technical debt , ” in Proceedings of the 2nd Workshop on managing technical debt , 2011 , pp . 35 – 38 . [ 9 ] G . Bavota and B . Russo , “A large - scale empirical study on self - admitted technical debt , ” in Proceedings of the 13th International Conference on Mining Software Repositories , 2016 , pp . 315 – 326 . [ 10 ] J . A . McCall , “Factors in software quality , ” US Rome Air development center reports , 1977 . [ 11 ] B . W . Boehm and P . N . Papaccio , “Understanding and controlling software costs , ” IEEE transactions on software engineering , vol . 14 , no . 10 , pp . 1462 – 1477 , 1988 . [ 12 ] R . Alfayez , W . Alwehaibi , R . Winn , E . Venson , and B . Boehm , “A systematic literature review of technical debt prioritization , ” in Proceedings of the 3rd International Conference on Technical Debt , ser . TechDebt ’20 . Association for Computing Machinery , 2020 , p . 1 – 10 . [ 13 ] E . Lim , N . Taksande , and C . Seaman , “A balancing act : What software practitioners have to say about technical debt , ” IEEE Software , vol . 29 , no . 6 , pp . 22 – 27 , 2012 . [ 14 ] J . Liu , Q . Huang , X . Xia , E . Shihab , D . Lo , and S . Li , “An exploratory study on the introduction and removal of different types of technical debt in deep learning frameworks , ” Empirical Software Engineering , vol . 26 , no . 2 , pp . 1 – 36 , 2021 . [ 15 ] S . Wehaibi , E . Shihab , and L . Guerrouj , “Examining the impact of self - admitted technical debt on software quality , ” in 2016 IEEE 23rd International Conference on Software Analysis , Evolution , and Reengineering ( SANER ) , vol . 1 . IEEE , 2016 , pp . 179 – 188 . [ 16 ] N . Brown , Y . Cai , Y . Guo , R . Kazman , M . Kim , P . Kruchten , E . Lim , A . MacCormack , R . Nord , I . Ozkaya et al . , “Managing technical debt in software - reliant systems , ” in Proceedings of the FSE / SDP workshop on Future of software engineering research , 2010 , pp . 47 – 52 . [ 17 ] J . - L . Letouzey and M . Ilkiewicz , “Managing technical debt with the sqale method , ” IEEE software , vol . 29 , no . 6 , pp . 44 – 51 , 2012 . [ 18 ] P . Kruchten , R . L . Nord , and I . Ozkaya , “Technical debt : From metaphor to theory and practice , ” Ieee software , vol . 29 , no . 6 , pp . 18 – 21 , 2012 . [ 19 ] P . Kruchten , R . L . Nord , I . Ozkaya , and D . Falessi , “Technical debt : towards a crisper definition report on the 4th international workshop on managing technical debt , ” ACM SIGSOFT Software Engineering Notes , vol . 38 , no . 5 , pp . 51 – 54 , 2013 . [ 20 ] E . Nascimento , A . Nguyen - Duc , I . Sundbø , and T . Conte , “Software engineering for artificial intelligence and machine learning software : A systematic literature review , ” arXiv preprint arXiv : 2011 . 03751 , 2020 . [ 21 ] A . Moin , “Data analytics and machine learning methods , techniques and tool for model - driven engineering of smart iot services , ” in 2021 IEEE / ACM 43rd International Conference on Software Engineering : Companion Proceedings ( ICSE - Companion ) . IEEE , 2021 , pp . 287 – 292 . [ 22 ] C . Tantithamthavorn , A . E . Hassan , and K . Matsumoto , “The impact of class rebalancing techniques on the performance and interpretation of defect prediction models , ” IEEE Transactions on Software Engineering , vol . 46 , no . 11 , pp . 1200 – 1219 , 2018 . [ 23 ] C . Kästner and E . Kang , “Teaching software engineering for ai - enabled systems , ” in Proceedings of the ACM / IEEE 42nd International Conference on Software Engineering : Software Engineering Education and Training , 2020 , pp . 45 – 48 . [ 24 ] K . Singla , T . Vinayak , A . Arpitha , C . Naik , and J . Bose , “Story and task issue analysis for agile machine learning projects , ” in 2020 IEEE - HYDCON . IEEE , 2020 , pp . 1 – 4 . [ 25 ] S . Idowu , D . Strüber , and T . Berger , “Asset management in machine learning : State - of - research and state - of - practice , ” ACM Computing Surveys , vol . 55 , no . 7 , pp . 1 – 35 , 2022 . [ 26 ] H . Jebnoun , H . Ben Braiek , M . M . Rahman , and F . Khomh , “The scent of deep learning code : An empirical study , ” in Proceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 420 – 430 . [ 27 ] W . Cunningham , “The wycash portfolio management system , ” ACM SIGPLAN OOPS Messenger , vol . 4 , no . 2 , pp . 29 – 30 , 1992 . [ 28 ] A . Potdar and E . Shihab , “An exploratory study on self - admitted technical debt , ” in 2014 IEEE International Conference on Software Maintenance and Evolution . IEEE , 2014 , pp . 91 – 100 . [ 29 ] R . L . Nord , I . Ozkaya , P . Kruchten , and M . Gonzalez - Rojas , “In search of a metric for managing architectural technical debt , ” in 2012 Joint Working IEEE / IFIP Conference on Software Architecture and European Conference on Software Architecture . IEEE , 2012 , pp . 91 – 100 . [ 30 ] N . S . Alves , T . S . Mendes , M . G . de Mendonça , R . O . Spínola , F . Shull , and C . Seaman , “Identification and management of technical debt , ” Inf . Softw . Technol . , vol . 70 , no . C , p . 100 – 121 , Feb . 2016 . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . 32 Bhatia , et al . [ 31 ] E . d . S . Maldonado , R . Abdalkareem , E . Shihab , and A . Serebrenik , “An empirical study on the removal of self - admitted technical debt , ” in 2017 IEEE International Conference on Software Maintenance and Evolution ( ICSME ) . IEEE , 2017 , pp . 238 – 248 . [ 32 ] Y . Kashiwa , R . Nishikawa , Y . Kamei , M . Kondo , E . Shihab , R . Sato , and N . Ubayashi , “An empirical study on self - admitted technical debt in modern code review , ” Information and Software Technology , vol . 146 , p . 106855 , 2022 . [ 33 ] H . Washizaki , H . Uchida , F . Khomh , and Y . - G . Guéhéneuc , “Studying software engineering patterns for designing machine learning systems , ” in 2019 10th International Workshop on Empirical Software Engineering in Practice ( IWESEP ) . IEEE , 2019 , pp . 49 – 495 . [ 34 ] I . Ozkaya , “What is really different in engineering ai - enabled systems ? ” IEEE Software , vol . 37 , no . 4 , pp . 3 – 6 , 2020 . [ 35 ] S . Martínez - Fernández , J . Bogner , X . Franch , M . Oriol , J . Siebert , A . Trendowicz , A . M . Vollmer , and S . Wagner , “Software engineering for ai - based systems : A survey , ” arXiv preprint arXiv : 2105 . 01984 , 2021 . [ 36 ] Z . Liu , Q . Huang , X . Xia , E . Shihab , D . Lo , and S . Li , “Satd detector : A text - mining - based self - admitted technical debt detection tool , ” in Proceedings of the 40th International Conference on Software Engineering : Companion Proceeedings , 2018 , pp . 9 – 12 . [ 37 ] D . OBrien , S . Biswas , S . Imtiaz , R . Abdalkareem , E . Shihab , and H . Rajan , “23 shades of self - admitted technical debt : an empirical study on machine learning software , ” in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering , 2022 , pp . 734 – 746 . [ 38 ] E . da Silva Maldonado , E . Shihab , and N . Tsantalis , “Using natural language processing to automatically detect self - admitted technical debt , ” IEEE Transactions on Software Engineering , vol . 43 , no . 11 , pp . 1044 – 1062 , 2017 . [ 39 ] Q . Huang , E . Shihab , X . Xia , D . Lo , and S . Li , “Identifying self - admitted technical debt in open source projects using text mining , ” Empirical Software Engineering , vol . 23 , no . 1 , pp . 418 – 451 , 2018 . [ 40 ] X . Ren , Z . Xing , X . Xia , D . Lo , X . Wang , and J . Grundy , “Neural network - based detection of self - admitted technical debt : from performance to explainability , ” ACM Transactions on Software Engineering and Methodology ( TOSEM ) , vol . 28 , no . 3 , pp . 1 – 45 , 2019 . [ 41 ] M . A . de Freitas Farias , M . G . de Mendonça Neto , M . Kalinowski , and R . O . Spínola , “Identifying self - admitted technical debt through code comment analysis with a contextualized vocabulary , ” Information and Software Technology , vol . 121 , p . 106270 , 2020 . [ 42 ] F . Zampetti , C . Noiseux , G . Antoniol , F . Khomh , and M . Di Penta , “Recommending when design technical debt should be self - admitted , ” in 2017 IEEE International Conference on Software Maintenance and Evolution ( ICSME ) , 2017 , pp . 216 – 226 . [ 43 ] S . Das , A . Dey , A . Pal , and N . Roy , “Applications of artificial intelligence in machine learning : review and prospect , ” International Journal of Computer Applications , vol . 115 , no . 9 , 2015 . [ 44 ] S . V . Albrecht and P . Stone , “Autonomous agents modelling other agents : A comprehensive survey and open problems , ” Artificial Intelligence , vol . 258 , pp . 66 – 95 , 2018 . [ 45 ] C . Badue , R . Guidolini , R . V . Carneiro , P . Azevedo , V . B . Cardoso , A . Forechi , L . Jesus , R . Berriel , T . M . Paixao , F . Mutz et al . , “Self - driving cars : A survey , ” Expert Systems with Applications , vol . 165 , p . 113816 , 2021 . [ 46 ] Y . Ma , Z . Wang , H . Yang , and L . Yang , “Artificial intelligence applications in the development of autonomous vehicles : a survey , ” IEEE / CAA Journal of Automatica Sinica , vol . 7 , no . 2 , pp . 315 – 329 , 2020 . [ 47 ] S . Pouyanfar , S . Sadiq , Y . Yan , H . Tian , Y . Tao , M . P . Reyes , M . - L . Shyu , S . - C . Chen , and S . S . Iyengar , “A survey on deep learning : Algorithms , techniques , and applications , ” ACM Computing Surveys ( CSUR ) , vol . 51 , no . 5 , pp . 1 – 36 , 2018 . [ 48 ] E . Kalliamvakou , G . Gousios , K . Blincoe , L . Singer , D . M . German , and D . Damian , “The promises and perils of mining github , ” in Proceedings of the 11th working conference on mining software repositories , 2014 , pp . 92 – 101 . [ 49 ] S . Mirhosseini and C . Parnin , “Can automated pull requests encourage software developers to upgrade out - of - date dependencies ? ” in 2017 32nd IEEE / ACM International Conference on Automated Software Engineering ( ASE ) . IEEE , 2017 , pp . 84 – 94 . [ 50 ] J . Coelho , M . T . Valente , L . Milen , and L . L . Silva , “Is this github project maintained ? measuring the level of maintenance activity of open - source projects , ” Information and Software Technology , vol . 122 , p . 106274 , 2020 . [ 51 ] M . Alfadel , D . E . Costa , E . Shihab , and M . Mkhallalati , “On the use of dependabot security pull requests , ” in 2021 IEEE / ACM 18th International Conference on Mining Software Repositories ( MSR ) . IEEE , 2021 , pp . 254 – 265 . [ 52 ] B . Chen and Z . M . Jiang , “Studying the use of java logging utilities in the wild , ” in 2020 IEEE / ACM 42nd International Conference on Software Engineering ( ICSE ) . IEEE , 2020 , pp . 397 – 408 . [ 53 ] I . Amit and D . G . Feitelson , “The corrective commit probability code quality metric , ” arXiv preprint arXiv : 2007 . 10912 , 2020 . [ 54 ] G . Kudrjavets , J . Thomas , A . Kumar , N . Nagappan , and A . Rastogi , “Quantifying daily evolution of mobile software based on memory allocator churn , ” in Proceedings of the 9th IEEE / ACM International Conference on Mobile Software Engineering and Systems , 2022 , pp . 28 – 32 . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 . An Empirical Study of Self - Admitted Technical Debt in Machine Learning Software 33 [ 55 ] W . Mauerer , M . Joblin , D . A . Tamburri , C . Paradis , R . Kazman , and S . Apel , “In search of socio - technical congruence : A large - scale longitudinal study , ” IEEE Transactions on Software Engineering , vol . 48 , no . 8 , pp . 3159 – 3184 , 2021 . [ 56 ] R . Morales , S . McIntosh , and F . Khomh , “Do code review practices impact design quality ? a case study of the qt , vtk , and itk projects , ” in 2015 IEEE 22nd International Conference on Software Analysis , Evolution , and Reengineering ( SANER ) , 2015 , pp . 171 – 180 . [ 57 ] O . Kononenko , O . Baysal , and M . W . Godfrey , “Code review quality : How developers see it , ” in 2016 IEEE / ACM 38th International Conference on Software Engineering ( ICSE ) , 2016 , pp . 1028 – 1038 . [ 58 ] G . Uddin and F . Khomh , “Automatic mining of opinions expressed about apis in stack overflow , ” IEEE Transactions on Software Engineering , vol . 47 , no . 3 , pp . 522 – 559 , 2021 . [ 59 ] R . Maipradit , B . Lin , C . Nagy , G . Bavota , M . Lanza , H . Hata , and K . Matsumoto , “Automated identification of on - hold self - admitted technical debt , ” in 2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation ( SCAM ) , 2020 , pp . 54 – 64 . [ 60 ] T . Byrt et al . , “How good is that agreement ? ” Epidemiology , vol . 7 , no . 5 , p . 561 , 1996 . [ 61 ] R . G . Miller Jr , Survival analysis . John Wiley & Sons , 2011 , vol . 66 . [ 62 ] E . L . Kaplan and P . Meier , “Nonparametric estimation from incomplete observations , ” Journal of the American statistical association , vol . 53 , no . 282 , pp . 457 – 481 , 1958 . [ 63 ] M . Yan , X . Xia , E . Shihab , D . Lo , J . Yin , andX . Yang , “Automatingchange - levelself - admittedtechnicaldebtdetermination , ” IEEE Transactions on Software Engineering , vol . 45 , no . 12 , pp . 1211 – 1229 , 2018 . [ 64 ] C . Tantithamthavorn , S . McIntosh , A . E . Hassan , and K . Matsumoto , “An empirical comparison of model validation techniques for defect prediction models , ” IEEE Transactions on Software Engineering , vol . 43 , no . 1 , pp . 1 – 18 , 2016 . [ 65 ] L . S . Shapley , 17 . A value for n - person games . Princeton University Press , 2016 . [ 66 ] S . McIntosh , Y . Kamei , B . Adams , and A . E . Hassan , “The impact of code review coverage and code review participation on software quality : A case study of the qt , vtk , and itk projects , ” in Proceedings of the 11th working conference on mining software repositories , 2014 , pp . 192 – 201 . [ 67 ] F . Zhang , F . Khomh , Y . Zou , and A . E . Hassan , “An empirical study on factors impacting bug fixing time , ” in 2012 19th Working conference on reverse engineering . IEEE , 2012 , pp . 225 – 234 . [ 68 ] J . Liu , Q . Huang , X . Xia , E . Shihab , D . Lo , andS . Li , “Isusingdeeplearningframeworksfree ? characterizingtechnicaldebt in deep learning frameworks , ” in Proceedings of the ACM / IEEE 42nd International Conference on Software Engineering : Software Engineering in Society , 2020 , pp . 1 – 10 . [ 69 ] N . S . Alves , L . F . Ribeiro , V . Caires , T . S . Mendes , and R . O . Spínola , “Towards an ontology of terms on technical debt , ” in 2014 Sixth International Workshop on Managing Technical Debt . IEEE , 2014 , pp . 1 – 7 . [ 70 ] E . d . S . Maldonado and E . Shihab , “Detecting and quantifying different types of self - admitted technical debt , ” in 2015 IEEE 7Th international workshop on managing technical debt ( MTD ) . IEEE , 2015 , pp . 9 – 15 . [ 71 ] S . Wattanakriengkrai , R . Maipradit , H . Hata , M . Choetkiertikul , T . Sunetnanta , and K . Matsumoto , “Identifying design and requirement self - admitted technical debt using n - gram idf , ” in 2018 9th International Workshop on Empirical Software Engineering in Practice ( IWESEP ) . IEEE , 2018 , pp . 7 – 12 . [ 72 ] J . M . Zhang , M . Harman , L . Ma , and Y . Liu , “Machine learning testing : Survey , landscapes and horizons , ” IEEE Transactions on Software Engineering , vol . 48 , no . 1 , pp . 1 – 36 , 2020 . [ 73 ] Z . Li , P . Avgeriou , and P . Liang , “A systematic mapping study on technical debt and its management , ” J . Syst . Softw . , vol . 101 , no . C , p . 193 – 220 , Mar . 2015 . [ Online ] . Available : https : / / doi . org / 10 . 1016 / j . jss . 2014 . 12 . 027 [ 74 ] N . Rios , M . G . de Mendonça Neto , and R . O . Spínola , “A tertiary study on technical debt : Types , management strategies , research trends , and base information for practitioners , ” Information and Software Technology , vol . 102 , pp . 117 – 145 , 2018 . [ 75 ] M . Fowler , Refactoring : improving the design of existing code . Addison - Wesley Professional , 2018 . ACM Trans . Softw . Eng . Methodol . , Vol . 1 , No . 1 , Article . Publication date : November 2021 .