Reusing Scienti ﬁ c Data : How Earthquake Engineering Researchers Assess the Reusability of Colleagues ’ Data Ixchel M . Faniel & Trond E . Jacobsen School of Information , University of Michigan , Ann Arbor , MI 48109 - 1285 , USA ( Phone : + 734 - 615 - 7435 ; Fax : + 734 - 764 - 2475 ; E - mail : ifaniel @ umich . edu ; E - mail : trond @ umich . edu ) Abstract . Investments in cyberinfrastructure and e - Science initiatives are motivated by the desire to accelerate scienti ﬁ c discovery . Always viewed as a foundation of science , data sharing is appropriately seen as critical to the success of such initiatives , but new technologies supporting increasingly data - intensive and collaborative science raise signi ﬁ cant challenges and opportunities . Overcoming the technical and social challenges to broader data sharing is a common and important research objective , but increasing the supply and accessibility of scienti ﬁ c data is no guarantee data will be applied by scientists . Before reusing data created by others , scientists need to assess the data ’ s relevance , they seek con ﬁ dence the data can be understood , and they must trust the data . Using interview data from earthquake engineering researchers af ﬁ liated with the George E . Brown , Jr . Network for Earthquake Engineering Simulation ( NEES ) , we examine how these scientists assess the reusability of colleagues ’ experimental data for model validation . Key words : data reuse , data sharing , data quality , trust , scienti ﬁ c data collections , data repositories , e - Science , cyberinfrastructure 1 . Introduction It is nearly a universal view among scientists and those funding basic research that science must become more collaborative if future scienti ﬁ c breakthroughs are to be achieved . One approach is to facilitate the management , sharing , and reuse of data on a large scale and over the long term . To date data sharing and management have been the primary focus among federal funding agencies , universities , and scienti ﬁ c communities . Both the National Institutes of Health and the National Science Foundation , for instance , have developed timetables and guidelines for data sharing methods , but there is little mention about how to make data reusable ( National Institutes of Health 2003 ; National Science Foundation July 10 2008 ) . A 2002 report comparing university policies highlights data retention , access , and storage policies with little mention about how to ensure effective data reuse ( Council on Governmental Relations 2006 ) . Many of the scienti ﬁ c communities now developing data repositories and associated infrastructure are focused on processes , policies , and tools to capture Computer Supported Cooperative Work ( 2010 ) 19 : 355 – 375 © Springer 2010 DOI 10 . 1007 / s10606 - 010 - 9117 - 8 and manage data . Despite signi ﬁ cant progress in several notable cases , in most disciplines there is a “ scandalous shortfall in the sharing of data by researchers ” ( “ Data ’ s Shameful Neglect [ Editorial ] ” 2009 ) . We contend that this is partly due to the dif ﬁ culties associated with providing information about the context of data ’ s production to enable reuse . Context information describes the set of interrelated environmental conditions where data are produced . It refers to properties of the physical environment where data are produced as well as the technical and social environments associated with obtaining data , including people , traditions , and organizational entities ( Baker and Yarmey 2008 ) . Determining the appropriate kind and amount of context information to capture for others ’ reuse is dif ﬁ cult ( Markus 2001 ) The context information others need to reuse data is often hard for scientists to articulate or beyond the scientists ’ original research objectives ( Birnholtz and Bietz 2003 ; Carlson and Anderson 2007 ) . Moreover , documenting data takes a lot of time and effort and the training and tools required to support such an effort are often in short supply ( Baker and Yarmey 2008 ) . Understanding the contextual nature of data , from the perspective of not only what those producing data can provide , but also what those reusing data need , can usefully inform the development of data repositories and associated infrastructures ( Jirotka et al . 2005 ) . The implicit assumption in the greater emphasis on sharing in research to date is that supplying data is the fundamental bottleneck to scienti ﬁ c innovation and making data more widely available will ensure reuse . It will not . To realize the potential of scienti ﬁ c collaboration and foster innovation , it is necessary to do more than ensure the availability of scienti ﬁ c data , it is also critical to understand what makes data reusable . This requires closer attention to those reusing the data and how they make data reuse decisions . The study we describe here is part of a larger 2 ½ year investigation of The George E . Brown , Jr . Network for Earthquake Engineering Simulation ( NEES ) , which consists of 15 experimental facilities with state - of - the - art equipment and information and communication technologies at universities across the United States . NEES has two major objectives . One is to support advanced experimental research projects and the other is to develop a repository to manage , share , and reuse the experimental data . The main group seeking to reuse data is earthquake engineering ( EE ) researchers aiming to validate numerical computation models . At the time of this study , the processes , policies , and tools to capture and manage the data were in early development and reuse had received little attention . The objective of this study is to examine how EE researchers assess the reusability of colleagues ’ experimental data for model validation . Drawing from prior research we frame our study around three questions we expect EE researchers to consider when assessing data reusability : 1 ) are the data relevant , 2 ) can the data be understood , and 3 ) are the data trustworthy . Our interviews reveal not only the context information that EE researchers need to make their assessments about the questions , but also the strategies and resources they use to 356 Ixchel M . Faniel and Trond E . Jacobsen make them . We discuss the implications data reusability assessments have for the continuing development of the NEES data repository , including its content and how content is captured , presented , and maintained . We also discuss future areas of data reuse research . 2 . Literature review Few studies on scienti ﬁ c data reuse formally de ﬁ ne reuse but generally agree that it includes the secondary use of data for a purpose other than originally intended ( Karasti and Baker 2008 ; Zimmerman 2008 ) , which presents one of the major challenges in providing reusable data . Neither the people nor their reuse purpose is usually known in advance . Thus the appropriate kind and amount of context information is dif ﬁ cult to discern . Most studies of scienti ﬁ c data reuse have examined scientists reusing data from colleagues in their research community . An examination of scientists ’ data reuse practices typically reveals not only the contextual nature of data , but also how scientists assess reusability given data ’ s contextual nature . There are three questions scientists consider when assessing data reusability : 1 ) are the data relevant , 2 ) can the data be understood , and 3 ) are the data trustworthy . When asking whether colleagues ’ data are relevant , scientists are assessing the degree to which the data meet their problem at hand . Some scientists develop speci ﬁ c criteria based on their research question of interest and scan journal articles for matches . Journal article use is due in part to journal articles being one of the few resources where data are described , but journal articles have several other affordances as well . For instance , the logical and physical structure of journals allow regular readers to know where to look for context information matching their search criteria ( Bishop 1999 ; Sandusky and Tenopir 2007 ; Stewart 1996 ) . Ecologists wanting to reuse data from multiple colleagues devise systematic sampling methods and use journal articles to bound their search by timeframes or geography ( Zimmerman 2007 ) . Another way scientists assess the reusability of colleagues ’ data is to evaluate whether they can understand the data . Several studies have found that metadata fail to provide enough context information to understand the data ( Birnholtz and Bietz 2003 ; Bourne 2005 ) . Less examined are other forms of documentation ( Carlson and Anderson 2007 ) . As members of communities of practice ( Lave and Wenger 1991 ) , scientists share an understanding about how research is appropriately conducted and use this understanding to make sense of colleagues ’ data . For instance , ecologists who want to reconstruct colleagues ’ data collection processes use their experience collecting similar data to visualize the processes ( Zimmerman 2008 ) . In other cases , scientists talk directly with colleagues to understand how the data are produced . HIV / AIDS researchers , for example , visit colleagues in the laboratories and clinics where the data are being produced to learn and observe data collection in progress ( Birnholtz and Bietz 2003 ) . 357 How Earthquake Engineering Researchers Assess Data Reusability A third way scientists assess the reusability of colleagues ’ data is to assess its trustworthiness . For some scientists , trustworthiness , like understanding , is evaluated based on what scientists learn as they become members of their community . Scientists then use their experience to evaluate whether data are trustworthy ( Borgman 2007 ) . They develop a shared understanding about whom and what to trust based on competence , honesty , credentials , reputation and use the understanding to assess the trustworthiness of the data ( Van House 2002 ; Van House et al . 1998 ) . Personnel working at breast cancer units , for instance , rely on a detailed understanding of the artifacts and processes they use , as well as knowledge of colleagues ’ performance reading mammograms , to assess whether those readings are trustworthy ( Jirotka et al . 2005 ) . In some cases , a shared understanding is not enough . Before they will trust colleagues ’ data , habitat ecologists need to trust the data collection process and want documentation describing how colleagues select and calibrate their data collection instruments such as sensors ( Wallis et al . 2007 ) . From prior research we conclude that scientists ask broadly comparable questions when assessing data reusability . How scientists go about answering these questions likely differs . In addition to needing different context information , existing research shows that scientists employ different strategies and rely on different resources when assessing the reusability of colleagues ’ data . In some cases , the context information scientists need comes from documentation , in others it is learned and exchanged as a part of community membership . These differences have implications for the development of data repositories and associated infrastructure . To inform the future development of the NEES repository we need a sharper understanding of how EE researchers assess data relevance , understandability , and trustworthiness . 3 . The EE research community The EE research community af ﬁ liated with NEES is comprised of structural engineers , geotechnical engineers , and tsunami researchers . The community examines hazards to mitigate the risk and loss arising from earthquakes . Two primary research approaches are prevalent : experimental studies and numerical computation modeling . Experimental studies are conducted in university laboratories or the ﬁ eld and take place over a 2 – 3 year period from planning to publication . Test specimens ( e . g . column , wall , house , etc . ) are built and tested via shake tables , centrifuges , or tsunami wave basins . Sensors are attached to the specimen under study and simulated earthquake shakes ( i . e . trials ) are conducted over the course of the experiment to produce instrument ( i . e . sensor ) readings . The experimental data are produced from the sensor readings . Numerical computation models simulate events ( e . g . how a building would respond to an earthquake ) . Once developed , the models need to be validated to show that they are robust across a variety of situations . Given the time , money , 358 Ixchel M . Faniel and Trond E . Jacobsen and effort required to develop and conduct experiments , many EE researchers want to reuse existing experimental data for model validation . For example , a researcher who develops a model for concrete walls under cyclic loads will use wall data produced during an experimental study to validate her model . To demonstrate that her model works she would run it using the experimental data and compare her results to the experimental test results of the walls . Current reuse of experimental data within the EE research community is mostly small scale on a one - to - one basis . EE researchers approach colleagues to see if they are willing to share data from their experimental research studies . Since experimental studies are conducted at colleagues ’ home institutions , the data are captured and stored locally on hard drives or ﬁ leservers . Each institution has its own data format . Ph . D . students who work closely with faculty and laboratory personnel are usually responsible for the data documentation that will often play a role in EE researchers ’ data reusability assessments . 4 . Data collection and analysis For this study 14 EE researchers were interviewed about their data reuse practices . The semi - structured interviews were conducted in person or by phone . The EE researchers were asked whether they reused data from colleagues and why ( i . e . the purpose ) , how they located and evaluated each others ’ data ( i . e . what guides their decision ) , and the challenges and bene ﬁ ts they experienced when reusing each others ’ data . On average the interviews lasted 60 min . An interviewer and note - taker were present at each interview . Interviews were taped and transcribed . Transcripts were coded in two ways . A ﬁ rst coding was based on how EE researchers assessed the reusability of colleagues ’ data in terms of relevance , understanding , and trustworthiness . The trustworthiness assessment was further re ﬁ ned to code for different evaluations EE researchers made when establishing trust , including reliability and validity . The transcripts were also coded for the strategies used during evaluation . Coding was also done for the resources EE researchers used to assess reusability , such as journal articles and conference papers , personal networks , conversations with colleagues , and colleagues ’ documentation . 5 . Results The 14 EE researchers interviewed included structural engineers ( 8 ) , geotechnical engineers ( 4 ) , and tsunami researchers ( 2 ) . Respondents included 4 assistant professors , 2 associate professors , 6 full professors , 1 Ph . D . student , and 1 post - doc . Of the 14 interviewed , 10 had reused data from colleagues . The 4 researchers who had not reused data were still able to describe how they might reuse colleagues ’ data and the challenges they might face . Although the number and rank of EE researchers interviewed is small , ﬁ ndings indicate similar data reuse practices across the different kinds of EE research areas and ranks . 359 How Earthquake Engineering Researchers Assess Data Reusability We describe below how EE researchers assess the reusability of colleagues ’ experimental data to validate their numerical computation models . The results are presented in three sections that parallel the questions EE researchers ask when assessing the reusability of data and include a discussion of the strategies , context information , and resources EE researchers employ to answer the questions . Table 1 provides a summary of the strategies and resources used along with example context information . 5 . 1 . Assessing whether colleagues ’ data are relevant To assess the relevance of colleagues ’ data , EE researchers generate criteria related to the problem they want to address . The criteria are based on the EE researchers ’ domain of expertise and the model they want to validate . For example , EE researchers who are interested in structural engineering look for particular types of structures , such as bridges , walls , beams , or ﬂ oors and structural materials such as steel , concrete , and wood . Geotechnical engineers are interested in dams and levees and focus on materials such as sand , clay , or soil . One of the tsunami researchers mentions seeking “ data for landslides impacting into water bodies ” ( NU - 10 ) . The EE researchers ’ criteria are narrowly focused not only on their domain , but also on the model they want to validate . One EE researcher puts it very simply . My evaluation process is I know exactly what I want to verify , exactly what parameters and what type of function I want to verify . If I do have this in the dataset it ’ s good . ( NU - 08 ) Table 1 . How EE researchers assess the reusability of experimental data for model validation . Data Reusability Assessment Strategies Example Context Information Resources Are data relevant ? Generate a narrow set of criteria to match against experimental parameters Diagrams of the test specimen , materials used to construct the specimen , sketches of the test set up Journals and personal networks are substitutable Can data be understood ? Complete exhaustive review of experimental procedures Data acquisition system parameters Conversations with colleaguescomplementdocumentation Are data trustworthy ? Build con ﬁ dence can consistently reproduce data Descriptions of sensors and other measured data Conversations with colleaguescomplementdocumentation Identify experimental problems and their resolution Data spikes , temperature effects , human errors Conversations with colleaguescomplement documentation 360 Ixchel M . Faniel and Trond E . Jacobsen Several researchers use the term ‘ parameters ’ when discussing their criteria . Parameters are described by one researcher as “ anything physical ” ( NU - 10 ) . When discussing parameters , several EE researchers refer to colleagues ’ experimental test set up , which includes such things as the test specimen , its size and construction , the materials used to build it , and how the materials are prepared . EE researchers also refer to how the specimens are tested . It may include loading histories ( e . g . how force is applied to a specimen ) or speci ﬁ c events ( e . g . Northridge earthquake ) . All of these things matter to an EE researcher looking to validate a model . One EE researcher contrasts two ways specimens are tested ( i . e . movement vs . applied force ) and describes how it impacts the data being produced and whether the data are relevant to him . If you want to study the way that the structure moves you probably want to have acceleration data at different locations in the same structure because that will give you how it ’ s moving . If you want to study for example what do you see as particular force is being applied to a very particular element . Say you might want to use strain gages and then look at the strain and speci ﬁ c location of the structure . ( NU - 09 ) Another EE researcher , describing how she assesses relevance , notes that engineering problems tend to be very focused and whether colleagues ’ data are relevant comes down to whether her model can represent her colleagues ’ parameters well . And other times I look at it [ a colleagues ’ experiment ] and decide well they used a light weight concrete , they used a very small aggregate , there ’ s this or that , or the strength was very low , it was 1 , 500 PSI . I just don ’ t think that my model is going to represent that sort of material and therefore I ’ m not going to use it . [ . . . ] A lot of times it ’ s the parameters that were used to test the specimens or how they tested [ . . . ] . ( NU - 06 ) Relevance assessments are driven by the EE researchers reuse purpose , particularly the model that the EE researcher wants to validate . EE researchers have a clear idea about what experimental data they need and their strategies tend to be narrowly focused . Since EE researchers know what they are looking for their need for context information is also narrow and very lean . Looking to match their key parameters with those of their colleagues , EE researchers rely on high rather than low level context information . 5 . 2 . Assessing whether colleagues ’ data can be understood When assessing whether they understand colleagues ’ data , EE researchers try to understand their colleagues ’ experiment . If EE researchers understand the 361 How Earthquake Engineering Researchers Assess Data Reusability experiment , they understand the data . It is not a trivial task , but rather a dif ﬁ cult and time and effort intensive process . [ It ] is a lot harder than a lot of people think because it ’ s not just about getting the data and getting some kind of ﬁ le that tells you what it is , you really have to understand all the detail of the actual experiment that took place in order to make the proper use of it usually . And so it ’ s usually pretty involved [ . . . ] . ( NU - 10 ) The intensity of the process is partly due to EE researchers wanting to understand the experiment as though they were there when it was conducted . EE researchers express a desire to understand “ exactly how the data was collected ” ( NU - 17 ) and “ all the detail of the actual experiment ” ( NU - 10 ) . The intensity of the process is also partly due to the amount of context information EE researchers need to understand the experiment . They need much more context information when assessing whether they understand colleagues ’ data than when assessing whether colleagues ’ data are relevant . They also need more low level details . They talk about wanting context information at “ a level of detail of a thesis or dissertation ” ( NU - 15 ) , because they need “ the nitty and the gritty ” ( NU - 17 ) details . One EE researcher explains how understanding what colleagues have done with data requires going back to detailed experimental procedures related to the data acquisition systems that are connected to the sensors . The data acquisition systems have parameters that determine how data are collected from the sensors . For example , the number of data points that are collected in each second that ’ s called a sampling frequency , that ’ s an important parameter . There are other parameters that are involved that , so what type of data acquisition system was used ? What type of sensitivity of the sensor ? What are the parameters of that data acquisition system ? All those things are important . Yeah there are some other things [ that are important too ] . For example , what is the length of the record ? Is each record 1 min or 30 min or 10 s ? There are ﬁ lters that you can apply to the data [ . . . ] so do you have [ those ] ﬁ lters or not in the data acquisition system ? There are some things that are involved in capturing that . ( NU - 09 ) The EE researcher goes on to explain that details about the data acquisition system , provide context information EE researchers use to understand the data in greater depth . In his case he learns what can and cannot be done with the data . Another EE researcher explains understanding colleagues ’ data requires more detailed context information about not only how specimens are constructed and tested , but also how data are measured and processed as well as assumptions and actions made or taken during the experiment . [ . . . ] you need to go look and see exactly how was the test or model constructed . What kind of standard did they use ? How was it compacted ? All the subtleties about exactly how things were constructed . Then how were things measured . [ . . . ] was there any processing done ? In other words 362 Ixchel M . Faniel and Trond E . Jacobsen you ’ re recording the voltage but that really means something in terms of a load or load cell or displacement from a displacement transducer . How did they go from one to other ? Did they have the calibration curves ? All that kind of stuff . So you ’ re trying to go from what was the test , what was measured , and kind of what were the assumptions ? ( NU - 15 ) Whether EE researchers understand colleagues ’ data rests on whether they understand how their colleagues conducted the experiment . EE researchers need context information that is more detailed and more subtle than what they use when assessing data ’ s relevance . Rather than simply matching their criteria to colleagues ’ experimental parameters , EE researchers need to have a thorough understanding of the data . Thus they have a low tolerance for uncertainty surrounding the experimental procedures . 5 . 3 . Assessing the trustworthiness of colleagues ’ data One reason EE researchers spend so much time and effort assessing whether they can understand colleagues ’ data is that they then use their understanding to assess whether the data are trustworthy . EE researchers use their understanding about how data are produced and the problems that occur to assess the reliability and validity of the data respectively . Our ﬁ ndings indicate that understanding how colleagues measure data increases EE researcher trust that the data can be measured consistently . In other words , EE researchers trust that the data are reliable . Our ﬁ ndings also indicate that understanding the problems colleagues encounter during experiments increases EE researchers trust that they can distinguish between valid and invalid data . Knowing how problems are resolved increases EE researcher trust that the data can be properly processed and thus still reused . 5 . 3 . 1 . Are the data reliable ? EE researchers need to ensure that they are measuring data in the same way that their colleagues measured during the experiment . Having context information about the number , type , location , and direction of the sensors that collect the data is critical for ensuring data are reliable . As one EE researcher explains information about where she places sensors on her test specimen helps build con ﬁ dence that her raw data can be recreated if necessary . If someone is interested in my specimen , they know exactly how many gauges [ sensors ] I have , they know exactly where they are located , and they have con ﬁ dence that if they take some data [ for reuse ] , they [ can ] believe it [ . . . ] you have much more con ﬁ dence in the data and in where the instruments were and if you need to recreate something on your own you have the ability . ( NU - 06 ) 363 How Earthquake Engineering Researchers Assess Data Reusability Data are not only collected from the sensors though . Other measures may also be taken over the course of the experiment . For example , displacement measures movement from an initial position , whereas deformation measures a change in form or shape . In both cases , it is important for colleagues to provide context information that describes how the measures are taken . Without it EE researchers know they are less likely to interpret the data properly and thus are less likely to trust that they can reliably recreate the data . In turn , analyzing the data becomes a challenge because validating models is dif ﬁ cult if not impossible for EE researchers to accomplish . For example , what we sometimes might do is we want to push on the beam and measure the deformation , but sometimes what happens is people might not want to do the deformation of the supports and the beams [ separately ] . And so the deformation that they ’ re supporting might be the total deformation including the deformation that is added due to the support of the beam . And if we don ’ t know something like this analyzing or interpreting the data effectively becomes a challenge because you don ’ t get your simulation models to line up [ . . . ] . ( NU - 02 ) Conveying how measurements are taken , such that different EE researchers can take them in the same way over and over again is a sign of reliable data . This requires providing context information that describes how measurements are taken , especially if it can be done in more than one way . In the example above , the deformation of a beam can be measured with the beam alone or include the deformation of the beam support as well . EE researchers are more likely to conclude that colleagues ’ data are reliable when they are provided context information that allows them to clearly discern how the measurements are taken . 5 . 3 . 2 . Are data valid ? Valid data measure what they intend to measure , but EE researchers realize that experiments are not without problems . there ’ s always going to be things that didn ’ t quite work the way that they were supposed to work and they weren ’ t quite done the way they were supposed to be done and things like that . ( NU - 10 ) When problems do occur , data are processed ( e . g . deleted , altered ) and EE researchers are still willing to reuse data under such circumstances . For experimental data reuse to work though , EE researchers need to know what problems occurred and how they were resolved . Without such context information , EE researchers may not be able to reconcile the data with what they understand about the experiment . In these cases they tend to reject the data rather than risk misinterpreting the data or drawing the wrong conclusions . There are times when EE researchers can identify data that are not valid , but they want to avoid speculating about why and what should be done . For example , 364 Ixchel M . Faniel and Trond E . Jacobsen one EE researcher recognizes that her colleague did not anchor the base to the test ﬂ oor well enough and knows there is a lot of rotation . Even though she has the personal knowledge to identify and correct for the rotation , she decides not to reuse the data . there were some walls that I looked at where they didn ’ t anchor the base to the test ﬂ oor well enough so they had a lot of rotation . I had a choice , I would either correct for that foundation rotation or just not use the data and I decided okay I really didn ’ t have enough information , it was best for me just to not use the data . ( NU - 06 ) There are other times when EE researchers cannot distinguish valid from invalid data . Thus they want to be careful to avoid “ assuming that some of the data is perfectly clean and has no odd behaviors associated with it when in fact it might ” ( NU - 17 ) . Using one of his recent experiments as an example , one EE researcher describes three problems he experienced during an experiment ( e . g . actuator delay , ambient temperatures in the lab , dampers heating up during testing ) . Given the problems he experienced , he believes it is likely that some of the data are invalid and he is considering corrective actions . One is there is what appears to be an actuator delay in the data so when we command the damper to move in a certain fashion it appears to be slightly delayed in actually doing that . There are temperature affects . So that as the dampers heat up their behavior changes slightly and the temperature does vary . The different days they were tested there were different ambient temperatures in the lab . As the dampers are tested they heat up as well . And there was some ﬂ uxing in the ﬁ xturing . [ . . . ] So what is measured by the device to be a 2 in displacement might not be 2 in to the dampers we used . ( NU - 17 ) He goes on to explain that EE researchers who reuse his data would have to know about the three problems and how he addressed them . Some might intervene to correct problems during an experiment . Others might not see a problem or wait until after the experiment and process the data . Knowing the problems that occur and what colleagues do to resolve them is critical in getting EE researchers to trust and therefore reuse the data . Assessments about the trustworthiness of colleagues ’ data center on EE researchers understanding how the data have been produced and processed . To understand how data have been produced , EE researchers need context information that allows them to precisely follow colleagues ’ experimental procedures . Armed with such information they expect to be able to consistently produce the same data that their colleagues produced . Interestingly , EE researchers willingly reuse data from colleagues who encounter problems during experiments that invalidate data . What they need though is context information 365 How Earthquake Engineering Researchers Assess Data Reusability about the problem and how it was resolved , including any processing done to correct the data . 5 . 4 . Resources EE researchers use to assess the data reusability EE researchers use multiple resources to assess the reusability of colleagues ’ data , including journal articles , personal networks , documentation , and conversations with colleagues who produced the data . EE researchers employ the multiple resources , because each has distinct capabilities that support the strategies they use to assess the reusability of colleagues ’ data . In the paragraphs that follow we discuss each resource and how it helps EE researchers make particular assessments about the reusability of colleagues ’ data . EE researchers learn about potential data for reuse through colleagues ’ journal articles as well as personal networks . Reliance on these two resources is due in part to the lack of a centralized data repository , but we also found that the resources provide additional affordances that support the strategies EE researchers employ when assessing the relevance of colleagues ’ data . For example , the length and familiarity of journal articles allows EE researchers to look for matches between the criteria they generate and their colleagues ’ experimental procedures . Upon searching through a 10 page journal article EE researchers have good idea about the type of test conducted and whether or not it will be useful for validating their model . Diagrams of the specimen under study , sketches of test set ups , tables detailing the construction of test specimens and describing the material properties of the specimens are usually included in journal articles . As one EE researcher explains , “ You have information to decide whether it [ the data produced from the experiment ] is going to work or not ” ( NU - 13 ) . The structure and content of the journal article imposed in part by peer review also makes searching for data through journal articles easier . by the time some data makes it into a peer review publication usually it ’ s been pretty well analyzed and polished and they kind of try to understand all those subtle pieces pretty good and after that they summarize it pretty well in a paper so at that point it ’ s relatively easy to make sense out of what was done . ( NU - 15 ) Although journal articles are widely used , our ﬁ ndings indicate that personal networks are most useful for those EE researchers who can identify a small sub - community of colleagues with similar interests . One EE researcher contrasts the sub - community of 30 – 35 EE researchers interested in structural engineering and steel to a sub - community of 200 – 250 EE researchers interested in structural engineering and concrete . In smaller sub - communities , EE researchers are more likely to build relatively detailed personal knowledge about their colleagues ’ research interests and projects and whether there is a match with their model validation needs . The EE researchers are also more likely to form personal relationships , which puts them in a better position to contact colleagues directly 366 Ixchel M . Faniel and Trond E . Jacobsen and to receive a timely response . One researcher commented that the “ community I work in is small I would probably know them and just call them or send them an email and say can I get the data ” ( NU - 11 ) . They are also more likely to run into them at conferences and keep abreast of what they are doing . “ [ Y ] ou see somebody you know at a conference , who you know because they work in your area and they talk about their work [ . . . ] ” ( NU - 02 ) . Both personal networks and journal articles are powerful means to learn about existing data and to assess relevance . At the time of the study they were the only means , because the NEES repository was under development . Interestingly , one EE researcher says he will continue to use journal articles and his personal network , even when the NEES repository is up and running . He thinks they will be useful pointers to relevant data in the repository . Actually , [ I would be ] potentially likely [ to reuse data from a data repository ] . It certainly would be a source of information . Probably it would be directed there though through either personal knowledge of oh they did this experiment and the data is over there or through publications where they ’ ve reported on their experiments and you know one way or another that the data is in the NEES repository . ( NU - 05 ) Journal articles and personal networks are useful for assessing data ’ s relevance , because they provide EE researchers with a means to quickly and easily match model validation needs against colleagues ’ experiments that have been published or are in progress . Neither alone is enough to assess whether colleagues ’ data can be understood or are trustworthy . To understand and trust colleagues ’ data , EE researchers need more and different context information than is usually provided in journal articles . They get it from colleagues ’ documentation . . . . the journal article will have some results . You have enough information to decide whether it ’ s going to work or not and then you might also need additional things [ . . . ] so you will still ask for some additional information [ that is usually provided in the documentation ] . ( NU - 13 ) No formal guidelines exist , but creating documentation is a usual practice in the EE research community . Since the experiments are complex and routinely involve several researchers , documentation captures what happened from test set up and data production through data processing and analysis . Ph . D . students , who work closely with faculty and laboratory personnel to plan and execute the experiment , usually write the documentation as part of their dissertation and / or as a separate data report . The documentation can easily span 100 pages or more and includes some of the same kind of context information in journal articles , such as sketches of the test specimens used during construction , descriptions of the materials to construct test specimens and how they were prepared . The difference is that the documentation provides granular detail . Much of the context information EE researchers indicated using to assess whether they could 367 How Earthquake Engineering Researchers Assess Data Reusability understand and trust the data is obtained from documentation . It may include details not found in journal articles , including sensor descriptions ( i . e . number , type , units , initial and ﬁ nal positions ) , input motion ﬁ les ( e . g . Santa Cruz earthquake ) , descriptions of the laboratory and laboratory equipment ( e . g . shake tables , tsunami wave basins , or centrifuges , data acquisitions systems ) , and photographs and video of the test specimen as damage occurs . Recall that assessing whether data can be understood and trusted is a time and effort intensive process in which EE researchers aim to understand their colleagues ’ experiment as if they were there . Because colleagues ’ documentation is a primary resource , it must be comprehensive but it need not be ﬂ awless . There are cases when EE researchers have to contact colleagues who produce the data , but the conversations are a complement rather than a substitute for the documentation . Conversations occur because there are “ usually other questions that arise ” ( NU - 10 ) . The questions EE researchers ask are intended to gather basic facts that help ﬁ ll gaps or explain inconsistencies or anomalies in the data . Such conversations are usually brief and simple . For example , one EE researcher describes an instance when a colleague did not provide all the details about clay preparation prior to constructing a specimen , because the clay was not a part of the main research objective . However , the EE researcher needed to know how the clay was prepared in order to reuse the data . Rather than reject the data , the EE researcher emailed his colleague for more detail . that particular test I ’ m talking about they had a clay layer on top of a sand layer so they were primarily interested in the behavior of sand , but I also wanted . . . since I was modeling the whole system , I needed to know more about the clay as well which they didn ’ t have in the paper . So then I said tell me how did you take care of this clay sample and there was a little bit of back and forth . ( NU - 13 ) In sum , skimming the content of a 10 page journal article or calling on one ’ s personal network , compared to the hundred or more pages of documentation , is quicker and preferred when assessing whether data are relevant . When needs change from a relatively simple matching exercise between their model parameters and the parameters of their colleagues ’ experiments to understanding and trusting colleagues ’ data enough to use it for model validation , however , more detailed documentation is required . While conversations with colleagues do occur when assessing whether data can be understood and trusted , these conversations are secondary to the documentation and serve to complement rather than replace its use . Used to gather basic facts , they are often carried out over telephone or email . 6 . Discussion To inform the future development of the NEES data repository , we consider how EE researchers currently assess the reusability of colleagues ’ data for model validation . 368 Ixchel M . Faniel and Trond E . Jacobsen This focus allows us to highlight the contextual nature of data and consider what about context is important to EE researchers deciding whether to reuse colleagues ’ data . Our focus also reveals the strategies and resources EE researchers use to assess whether the data are relevant , can be understood , and are trustworthy . We believe our ﬁ ndings about strategies , context , and resources have implications for the future development of the NEES data repository as well as future research in the area . We found EE researchers ’ strategy when assessing the relevance of colleagues ’ data is to match key parameters from their models to key parameters from colleagues ’ experiments . Using such a strategy , EE researchers prefer high , rather than low - level context information about how the experiment was set up and how the specimens were tested ( e . g . test specimen size and construction , including the materials used , etc . ) . Although the NEES data repository is likely to have features that enable a more targeted search for relevant data , rendering journal articles redundant for this purpose , our ﬁ ndings suggest that journal articles have particular affordances that NEES should consider when presenting search results . Both the structure and content of journal articles helped EE researchers assess data relevance . Similarly , research on the use of journal articles indicates that various article components , such as ﬁ gure and table captions , table text , images , and formula , provide useful heuristics for assessing relevance ( Bishop 1999 ; Sandusky and Tenopir 2007 ; Sandusky et al . 2008 ) . For NEES , this might mean returning search results in the form of ﬁ gures and tables as well . Journal articles already report the diagrams of the specimen under study , sketches of test set ups , tables detailing the construction of test specimens , tables describing the material properties of the specimens , etc . Similar content is also found in documentation albeit in greater volume and more detail . The EE researchers we studied spend a lot of time and effort assessing whether colleagues ’ data could be understood and trusted . Similar to prior research , assessing the trustworthiness of colleagues ’ data requires a ﬁ ne - grained understanding of the context of data production ( Jirotka et al . 2005 ) . Unlike prior research , the ﬁ ne - grained understanding is not based on familiarity with the artifacts and processes , nor is it based on the perceived competency and honesty of the person , prior experience at the same laboratory , or extensive face - to - face interaction with colleagues ( Birnholtz and Bietz 2003 ; Van House 2002 ; Zimmerman 2007 ) . It is instead based on how competently primary data producers document the artifacts and processes used and created during their experiments . The trust EE researchers have for the data is developed during their review of the documentation . They want to understand the experiment as though they were present and to believe they can recreate the data from production to processing and analysis . The extent to which EE researchers rely on colleagues ’ documentation is surprising given the dif ﬁ culty associated with documenting data for others ( e . g . Birnholtz and Bietz 2003 ; Carlson and Anderson 2007 ) . For EE researchers , colleagues ’ documentation is a fairly good representation of what took place 369 How Earthquake Engineering Researchers Assess Data Reusability during the experiment and colleagues do not view it as extra work . This is partly because the experimental studies are complex and colleagues need documentation for personal use . Since Ph . D . students are typically responsible for manually documenting the experimental studies , creating documentation is learned in practice and part of becoming a member of the EE research community . An important challenge for NEES is ﬁ guring out how to capitalize on what is already happening in the EE research community with respect to documentation . Documentation is successfully created and used for small - scale data sharing and reuse , but early indications suggest that the same cannot be said of the documentation for large scale data sharing and reuse via the NEES data repository . Regarding NEES , documentation is viewed as a burden , in part because the documentation that is being created for public use is different than the documentation created for personal use ( Faniel 2009 ) . Those producing the data are being asked to create documentation for others that includes more detail than they are used to providing , without clear guidelines , training , or tools to support their efforts ( Faniel 2009 ) . NEES is in a unique position because documentation is important for both those producing the data as well as those reusing the data . Therefore documentation should be treated not only as something that has to be done for those reusing the data , but also as something directly bene ﬁ ting the data producer . In widening its view on the role of documentation , NEES maybe able to embed systems and procedures into the work ﬂ ow of documentation tasks . Doing so not only will make data more reusable to EE researchers , it will also bene ﬁ t those producing data by easing the burden of documentation . There is a growing body of research on scienti ﬁ c work ﬂ ows that examines how to capture the context information associated with the work practices and decisions scientists make . We found EE researchers rely on low - level details to understand and trust colleagues ’ data . Work ﬂ ows are designed to capture low - level details ( Michener 2006 ) . Moreover work ﬂ ows have been found to bene ﬁ t those producing and analyzing their own primary data as well . The systems ease the burdens associated with producing the documentation traditionally found in laboratory notebooks and reduce the risk of recording errors ( Davidson and Friere 2008 ) . Embedded in the very process of conducting experiments , experimental inputs , parameters , and conditions are captured automatically . Some work ﬂ ows can also be queried to discover and use particular procedures and tasks that generated the data and data analyses ( Scheidegger et al . 2008 ) and provide visual representations that help analyze , compare , and understand data ( Freire et al . 2006 ) . Work ﬂ ows are not without their challenges however . There are perceptions that their adoption will impose new demands on scarce time and energy ( Jones and Gries 2010 ) . This can be remedied in part by aligning the needs of the various subgroups that the tools are intended to support ( Lee and Bietz 2009 ) . In the case of NEES , this likely means recognizing the need to provide documentation tools that meet the dual role EE researchers have in producing and documenting their own data and in reusing colleagues ’ data , because the needs are likely to be different . 370 Ixchel M . Faniel and Trond E . Jacobsen As a complement to documentation , EE researchers may use conversations with colleagues to gather basic facts to ﬁ ll in gaps or explain inconsistencies . Done on a small scale , it is likely that conversations are not burdensome . As science is conducted on a larger , more collaborative scale the opposite is likely to be true ( De Roure et al . 2008 ) . Moreover conversations are likely to be a less effective alternative in the long term as people forget and die . Although the widely held view is that those who produced the data should provide the context information , when it comes to updating or correcting the information other alternatives are being explored . Some communities have designated information managers to gather additional context information in cases where data producers have not provided enough , but this may mean those producing the data never learn about data management ( Karasti et al . 2006 ) . An alternative approach is to support the social dimension of scienti ﬁ c research and allow EE researchers to annotate colleagues ’ documentation with updates , corrections , and comments . Not only would this approach spread the time and effort of valuable annotation across the community , it would also help reduce recurring conversations , and improve documentation quality . Research suggests it may also prevent decay of the data , documentation , and work ﬂ ows . Data quality is actually expected to improve in the metagenomics community if members could update , correct , and post comments about colleagues ’ data ( Lee and Bietz 2009 ) . To avoid the decay of work ﬂ ows and associated data and documentation , myExperiment , for instance , allows contributors and the community to curate by tagging , commenting on , and reviewing research objects ( De Roure et al . 2008 ) . Given prior data reuse studies we can see that EE researchers assess reusability differently than scientists in other communities . They use different strategies , context information , and resources . This is partly due to the different disciplinary norms , conventions , and practices ( Borgman 2007 ; Van House 2002 ) and work must be done to continue to specify and theorize about these differences . However , future research should also be designed to highlight and theorize about similarities across disciplines . Data reuse practices may also vary by data type , reuse purpose , and user type , but there is little research in these areas ( e . g . Birnholtz and Bietz 2003 ; Carlson and Anderson 2007 ; Wallis et al . 2006 ) . Future research that usefully combines the study of disciplinary practices and norms , along with data type ( e . g . experimental , observational , simulation ) , reuse purpose , or user type ( e . g . scientists , practitioners , policy makers , the public ) , particularly their joint in ﬂ uences on reusability assessments , would better support useful comparisons within and across scienti ﬁ c communities . Take the ﬁ ndings from our study as an example . It is likely that reusability assessments are in ﬂ uenced by not only our respondents membership in a particular community ( EE researchers ) , but also their reuse of a particular data type ( experimental ) , for a particular reuse purpose ( model validation ) . The strategies and context information employed to assess relevance are based on key parameters of the models EE researchers want to validate . The EE researchers need to understand the data as though they were present , because they have to replicate their 371 How Earthquake Engineering Researchers Assess Data Reusability colleagues ’ experimental results from data production to processing and analyses . Misunderstanding one thing makes model validation dif ﬁ cult , if not impossible . However , EE researchers have other reasons for reusing colleagues ’ data besides model validation . They reuse data to look at general trends across multiple studies , verify , refute , and re ﬁ ne original results , enhance their own data population , provide resources for education , outreach , and training , and encourage use of data in policy formation and evaluation ( Faniel 2009 ) . Future research should examine how the different reuse purposes in ﬂ uence EE researchers ’ reusability assessments . For instance , research might examine whether reusability assessments vary when the reuse purpose is related to research vs . teaching vs . policy making . An alternative would be to examine scientists from different communities and their reuse of colleagues ’ experimental data for model validation to get a clearer picture of how disciplinary norms , conventions , and practices in ﬂ uence reusability assessments . A major objective of managing and sharing data for reuse is to support a heterogeneous group of users , scientists and non - scientists alike . Yet most research on scienti ﬁ c data reuse examines scientists reusing data from other scientists within their community . We suspect that non - scientists are likely ask the same three evaluative questions as scientists , but that they are not likely to bene ﬁ t from the same strategies used by scientists , because they are not participating members of the scienti ﬁ c community . The context information and resources they need may also differ . Moreover , it is not clear that non - scientists are interested in reusing the data per se . It maybe that they are more interested in reusing the various products of scienti ﬁ c research ( e . g . interpretations , discussion of practical implications , etc . ) . It is also possible that non - scientists have undiscovered needs that will require developing new products . If such groups are to bene ﬁ t from scienti ﬁ c data , future research must consider their needs , including what they choose to reuse and how they assess reusability . Our focus on how EE researchers assess the reusability of colleagues ’ data helped to reveal the contextual nature of data . We identi ﬁ ed the strategies and resources important to EE researchers . Taken together , these ﬁ ndings have implications for data repository content as well as how repository content is captured , presented , and maintained . Findings also revealed some similarities and differences with prior data reuse studies conducted in other scienti ﬁ c research communities . A concerted effort is needed to identify both the similarities and differences within and across scienti ﬁ c disciplines regarding data reuse practices in general and data reusability assessments in particular . Such research is likely to be informative for building theory as well as data repositories and associated infrastructure . 7 . Conclusion We agree with many others that far broader sharing of scienti ﬁ c data is critical to expanding the frontiers of science . But we contend that neither broader sharing 372 Ixchel M . Faniel and Trond E . Jacobsen nor improved systems for distribution are suf ﬁ cient to maximize potential scienti ﬁ c discoveries . Data sharing and access are obviously prerequisites for data reuse , because without data and the ability to access it , scientists cannot make use of other ’ s data to address new scienti ﬁ c problems . It is entirely appropriate that early efforts have focused on trying to induce broader sharing and to improve infrastructure for managing scienti ﬁ c data . The explosion in the volume and richness of data occasioned by the rise of collaborative science will stall without a better understanding of data sharing and accessibility . Our analysis of the data practices of EE researchers af ﬁ liated with the NEES strongly indicates however , that unique factors and judgments in ﬂ uence decisions to reuse colleagues ’ data , factors and judgments that cannot be satis ﬁ ed merely by making more data more easily available . Before they are willing to reuse data created by others , EE researchers make judgments about the data ’ s relevance , they seek con ﬁ dence potentially relevant data can be understood , and they must trust the data . To make these judgments , EE researchers use varied combinations of strategies , context information , and resources , in ways that have important implications for the continued development of the NEES data repository and associated infrastructure . While our ﬁ ndings about how EE researchers locate and identify relevant data seem to re ﬂ ect the ﬁ ndings of other researchers investigating other communities , documentation use among EE researchers to make decisions about understand - ability and trustworthiness are noteworthy . Without further study , it is impossible to say whether and to what degree scientists in other communities appeal to the same factors when making data reuse decisions . But we are more con ﬁ dent they will have concerns about whether the data are relevant , can be understood , and are trustworthy . As scienti ﬁ c communities continue to transition to greater collaboration across all phases of the scienti ﬁ c enterprise and as data collection methods become more advanced , scientists will generate ever greater volumes of data . The importance of locating and identifying relevant data , and understanding and trusting those data , will also increase along with the necessity of adequate documentation . Under these circumstances , we believe understanding reuse behaviors , particu - larly the context information needed and the strategies and resources used , will only grow in importance . In our view , we need to make data reuse a focus of future research that complements but is independent of research on data sharing and management . Even as we have tried to balance research attention by examining data reuse in this study , we believe it is important to consider all three , data management , sharing , and reuse , as they are obviously interrelated . Acknowledgements We want to thank John L . King , Stephanie Teasley , Elizabeth Yakel , and the reviewers and editors at CSCW for their feedback on early versions of this work . We also want to thank Martha Gukeisen for her help during data collection . This 373 How Earthquake Engineering Researchers Assess Data Reusability research is based on work supported by the National Science Foundation , Award number CMMI - 0714116 to the University of Michigan . Any opinions , ﬁ ndings , and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ﬂ ect the views of the National Science Foundation . References Baker , K . S . , & Yarmey , L . ( 2008 ) . Data stewardship : Environmental data curation and a web - of - repositories : 4th International Digital Curation Conference , Edinburgh , Scotland , December , 2008 . Birnholtz , J . P . , & Bietz , M . ( 2003 ) . Data at work : Supporting sharing in science and engineering : ACM Conference on Supporting Group Work , Sanibel Island , FL , 2003 , pp . 339 – 348 . Bishop , A . P . ( 1999 ) . Document structure and digital libraries : How researchers mobilize information in journal articles . Information Processing and Management , 35 ( 3 ) , 255 – 279 . Borgman , C . L . ( 2007 ) . Scholarship in the digital age : Information , infrastructure , and the internet . Cambridge : MIT Press . Bourne , P . E . ( 2005 ) . Will a biological database be different from a biological journal . PLoS Computational Biology , 1 ( 3 ) , 179 – 181 . Carlson , S . , & Anderson , B . ( 2007 ) . What are data ? The many kinds of data and their implications for data re - use . Journal of Computer - Mediated Communication , 12 ( 2 ) . Retrieved from http : / / jcmc . indiana . edu / issue2 / carlson . html . Council on Governmental Relations . ( 2006 ) . Access to and retention of research data : Rights and responsibilities Retrieved July 17 , 2009 , from http : / / 206 . 151 . 87 . 67 / docs / DataRetentionIntroduction . htm . Data ’ s Shameful Neglect [ Editorial ] . ( 2009 ) . Nature , 461 ( 7261 ) , p . 145 . Davidson , S . , & Friere , J . ( 2008 ) . Provenance and scienti ﬁ c work ﬂ ows : Challenges and opportunities : SIGMOD ’ 08 , Vancouver , BC , Canada , June 9 – 12 , 2008 , pp . 1 – 6 . De Roure , D . , Goble , C . , Bhagat , J . et al . ( 2008 ) . myExperiment : De ﬁ ning the Social Virtual Research Environment : 4th IEEE International Conference on e - Science , Indianapolis , Indiana , December , 2008 . Faniel , I . M . ( 2009 ) . Unrealized potential : The socio - technical challenges of a large scale cyberinfrastructure initiative retrieved July 17 , 2009 , from http : / / hdl . handle . net / 2027 . 42 / 61845 . Freire , J . , Silva , C . T . , Callahan , S . P . et al . ( 2006 ) . Managing rapidly - evolving scienti ﬁ c work ﬂ ows : PAW ’ 06 International Provenance and Annotation Workshop , LNCS 4145 , Chicago , Illinois , USA , May 3 – 5 , 2006 , 2006 . Jirotka , M . , Procter , R . , Hartswood , M . , et al . ( 2005 ) . Collaboration and trust in healthcare innovation : The eDiaMoND CaseStudy . Computer Supported Cooperative Work , 14 ( 4 ) , 369 – 398 . Jones , M . B . , & Gries , C . ( 2010 ) . Advances in environmental information management . Ecological Informatics , 5 ( 1 ) , 1 – 2 . Karasti , H . , & Baker , K . S . ( 2008 ) . Digital data practices and the long term ecological research program growing global . The International Journal of Digital Curation , 3 ( 2 ) , 42 – 58 . Karasti , H . , Baker , K . S . , & Halkola , E . ( 2006 ) . Enriching the notion of data curation in E - Science : Data managing and information infrastructuring in the Long Term Ecological Research ( LTER ) network . Computer Supported Cooperative Work , 15 ( 4 ) , 321 – 358 . Lave , J . , & Wenger , E . ( 1991 ) . Situated learning : Legitimate peripheral participation . Cambridge : Cambridge University Press . 374 Ixchel M . Faniel and Trond E . Jacobsen Lee , C . , & Bietz , M . ( 2009 ) . Barriers to the Adoption of New Collaboration Technologies for Scientists , CHI 2009 , Boston , MA , April 4 – 9 Retrieved 26 February , 2010 , from http : / / www . matthewbietz . org / blog / wp - content / uploads / chi2009 - scienti ﬁ ccollaborationsposition . pdf . Markus , M . L . ( 2001 ) . Toward a theory of knowledge reuse : Types of knowledge reuse situations and factors in reuse success . Journal of Management Information Systems , 18 ( 1 ) , 57 – 91 . Michener , W . K . ( 2006 ) . Meta - information concepts for ecological data management . Ecological Informatics , 1 ( 1 ) , 3 – 7 . National Institutes of Health . ( 2003 ) . NIH Data Sharing Policy and Implementation Guidance . Retrieved June 18 , 2009 . from http : / / grants2 . nih . gov / grants / policy / data _ sharing / data _ sharing _ guidance . htm . National Science Foundation . ( July 10 , 2008 ) . Data Archiving Policy . Retrieved June 18 , 2009 . from http : / / www . nsf . gov / sbe / ses / common / archive . jsp . Sandusky , R . J . , & Tenopir , C . ( 2007 ) . Finding and using journal article components : Impacts of disaggregation on teaching and research practice . Journal of the American Society of Information Science and Technology , 59 ( 6 ) , 970 – 982 . Sandusky , R . J . , Tenopir , C . , & Casado , M . M . ( 2008 ) . Figure and table retrieval from scholarly journal articles : User needs for teaching and research . Proceedings of the American Society for Information Science and Technology , 44 ( 1 ) , 1 – 13 . Scheidegger , C . E . , Vo , H . T . , Koop , D . , et al . ( 2008 ) . Querying and ReUsing Work ﬂ ows with VisTrails : SIGMOD ’ 08 , Vancouver , BC , Canada , June 9 – 12 , 2008 , pp . 1 – 4 . Stewart , L . ( 1996 ) . User acceptance of electronic journals : Interviews with chemists at Cornell University . College & Research Libraries , 57 ( 4 ) , 339 – 349 . Van House , N . A . ( 2002 ) . Digital libraries and the practices of trust : Networked environmental information . Social Epistemology , 16 ( 1 ) , 99 – 114 . Van House , N . A . , Butler , M . H . , & Schiff , L . R . ( 1998 ) . Cooperative knowledge work and practices of trust : Sharing environmental planning data sets : The ACM Conference On Computer Supported Cooperative Work , Seattle , Washington , 1998 , pp . 335 – 343 . Wallis , J . C . , Milojevic , S . , Borgman , C . L . , et al . ( 2006 ) . The special case of scienti ﬁ c data sharing with education : The American Society for Information Science & Technology , October , 2006 , pp . 169 – 181 . Wallis , J . C . , Borgman , C . L . , Mayernik , M . S . , et al . ( 2007 ) . Know thy sensor : Trust , data quality , and data integrity in scienti ﬁ c digital libraries : European Conference on Research and Advanced Technology for Digital Libraries , Budapest , Hungary , 2007 . Zimmerman , A . ( 2007 ) . Not by metadata alone : The use of diverse forms of knowledge to locate data for reuse . International Journal on Digital Libraries , 7 ( 1 – 2 ) , 5 – 16 . Zimmerman , A . ( 2008 ) . New knowledge from old data : The role of standards in the sharing and reuse of ecological data . Science , Technology , & Human Values , 33 ( 5 ) , 631 – 652 . 375 How Earthquake Engineering Researchers Assess Data Reusability