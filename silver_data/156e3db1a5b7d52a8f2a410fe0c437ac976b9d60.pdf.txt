Connecting Human - Robot Interaction and Data Visualization Daniel Szafr ∗ ATLAS Institute and Department of Computer Science University of Colorado Boulder Boulder , Colorado , USA daniel . szafr @ colorado . edu Danielle Albers Szafr ∗ ATLAS Institute and Department of Computer Science University of Colorado Boulder Boulder , Colorado , USA danielle . szafr @ colorado . edu ( a ) [ 19 ] Interface used in 9 / 11 response ( b ) [ 92 ] and [ 45 ] Playbook UAV interfaces ( c ) [ 69 ] 2013 IHMC DARPA VRC interface V e l o c i t y μ acc dec Velocity Profile Time Since Sync T i m e ( m s ) AUV Control Panel Battery : Heading : Speed : Name : F _ 54 80 % Disk Available 75 % 186 . 307 . 50 X : 511386 . 61 Y : 4198617 . 16 Depth : 2 . 50 Rec : Acq AUV Depth : Vehicle Type : Anchor AUV 0 60 40 20 del uli ( d ) [ 49 ] ICARUS C2I interface ( e ) [ 136 ] WiMUST marine robot interface ( f ) Notional redesign of the WiMUST Interface Figure 1 : Examples of archetypal robot interfaces are shown above . This paper highlights opportunities for improving robot interface design by integrating knowledge of data visualization and recognizing the importance of data analysis tasks in HRI . Human - robot interaction ( HRI ) research frequently explores how to design interfaces that enable humans to efectively teleoperate and supervise robots . One of the principle goals of such systems is to support data collection , analysis , and human decision making , which requires representing robot data in ways that support fast and accurate analyses by humans . However , the interfaces for these systems do not always use best - practice principles for efectively visualizing data . We present a new framework to scafold reasoning about robot interface design that emphasizes the need to consider data visualization for supporting analysis and decision - making pro - cesses , detail several data visualization best practices relevant to HRI , identify a set of core data tasks that commonly occur in HRI , and highlight several promising opportunities for further synergis - tic activities at the intersection of these two research areas . CCS CONCEPTS • Computer systems organization → Robotics ; External in - terfaces for robotics ; • Human - centered computing → Visu - alization ; Graphical user interfaces . KEYWORDS Robot Interface Design ; Data Visualization ; Human - Robot Interac - tion ( HRI ) ; Information Visualization ; VIS ; InfoVis ACM Reference Format : Daniel Szafr and Danielle Albers Szafr . 2021 . Connecting Human - Robot Interaction and Data Visualization . In Proceedings of the 2021 ACM / IEEE International Conference on Human - Robot Interaction ( HRI ’21 ) , March 8 – 11 , 2021 , Boulder , CO , USA . ACM , New York , NY , USA , 12 pages . https : / / doi . org / 10 . 1145 / 3434073 . 3444683 1 INTRODUCTION Robots are increasingly helping humans explore environments , col - lect data , and manipulate the physical world . Historically , robot deployments have required signifcant human oversight and direct intervention , leading to human - robot interfaces that focus primarily on supporting human supervision and teleoperation of robot activ - ities . However , advances in sensing , actuation , and autonomy are rapidly expanding robot capabilities and creating new opportuni - ties for scientists , engineers , and analysts to collaboratively engage with robots to accomplish domain - related tasks , such as mapping ABSTRACT ∗ Both authors contributed equally to this research . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specifc permission and / or a fee . Request permissions from permissions @ acm . org . HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA © 2021 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 8289 - 2 / 21 / 03 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3434073 . 3444683 Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 281 archaeological sites [ 27 ] or inspecting building integrity [ 95 ] . In such scenarios , users may be less interested in directing low - level aspects of robot operation and may instead focus on accomplishing the overall mission objective by synthesizing knowledge products created by robots , such as analyzing new data samples the robot provides , investigating maps the robot creates , or working with a robot to understand and reduce contextual uncertainties or improve data quality . As robot capabilities advance , human - robot interfaces will increasingly need to support such data - centric activities . Reasoning about data - centric human - robot interaction is also critical for for systems that focus on traditional aspects of robot operation and / or supervision due to limitations in robot autonomy ( e . g . , many of the systems shown in Figure 1 ) as humans must un - derstand robot data , often provided as camera feed ( s ) , map overlays , sensor readouts , point clouds , or even mixed and virtual reality dis - plays , to build situational awareness sufcient for directing robots efectively . However , designing robot interfaces that support users in both directing robots and understanding robot - collected data remains a challenge . For instance , Murphy & Tadokoro [ 94 ] note that “the End - User interface is the most difcult to build because it requires a working prototype of the robot , an initial interface , and access to high fdelity feld conditions , and multiple end users for a domain analysis . ” We argue that another major reason that such interfaces have historically been , and remain , so difcult to design is that interface researchers and developers are often members of the human - robot interaction ( HRI ) and feld robotics communities who may have little connection with the data and information visu - alization communities ( VIS ) , a feld of research exploring guidelines for helping people quickly and accurately make sense of data . While some HRI practitioners may have training in visualization tech - niques , we believe there is a general opportunity to substantially improve robot interface design by leveraging knowledge of how to efectively encode data for human users . In this work , we argue for increased collaboration between ex - perts in robotics and data visualization , motivated by the need for robotic interfaces to increasingly consider and provide direct support for data analysis . Our goal is to begin to bridge the di - vide between HRI and VIS , recognizing that there is a consid - erable potential for natural synergies between HRI , which develops interfaces that enable humans to efectively direct and / or supervise robots while also making use of the data such robots collect , and information visualization , which fo - cuses on designing interfaces that allow people to build tar - geted knowledge through data . We argue that robot interfaces need to increasingly consider how users will engage with data provided by robots and thus be designed not only from a robotics - centric , but also a data - centric perspective . In addition , we note that robotics ofers rich opportunities for novel VIS research , particularly in identifying practices for dynamic , uncertain , and spatio - temporal data . Capitalizing on such opportunities will require co - innovation and knowledge of the state - of - the - art in both felds . To this end , we briefy review major guidelines and best practices from the VIS com - munity that are relevant to HRI and propose an initial framework to guide potential parallel developments in HRI and VIS , understand HRI data tasks and activities , and identify potential connections and opportunities for future work in this space of data - centric HRI . 2 BACKGROUND This paper is written from the perspective of two authors , one from the HRI community and the other from VIS , to help both communi - ties recognize the opportunities each feld has to inform the other . We believe there are clear opportunities for mutual beneft ; how - ever , while the two felds draw on common intellectual traditions ( e . g . , human - computer interaction , cognitive science , etc . ) , to date HRI and VIS have developed largely in isolation from one another . For example , we searched the entire corpus of the IEEE VIS confer - ence from 1990 to 2019 using the term “robot” and found only one paper ( by one of the co - authors ) exploring analytic systems where users might work collaboratively with robots that are supplying feld data [ 148 ] . However , robotics was not the primary focus of the paper and no robot was used in the research . We similarly surveyed IEEE Transactions on Visualization and Computer Graphics ( TVCG ) , the premier journal for visualization research ( as well research from other communities such as virtual reality ) . We identifed only three relevant papers : one paper ( from more than 20 years ago ) on visualizing robot sensors to help devel - opers understand how the sensors work and improve object iden - tifcation algorithms [ 140 ] and two papers combining augmented reality and aerial robots [ 40 , 160 ] . However , the latter two papers did not originate from , or take advantage of knowledge within , the data visualization community , but rather came from the adja - cent mixed and virtual reality communities , although they further demonstrate potential opportunities for better synergies between HRI and related felds ( see similarities to recent HRI research in robotics and mixed reality , e . g . , [ 57 , 109 , 114 , 115 , 146 , 147 ] ) . This survey , paired with the expertise of the authors , indicates that robot data has not been actively investigated in VIS despite research within VIS on developing tools for a range of other domain - focused problems , such as biology , machine learning , and environ - mental science . But what about the other perspective : is the HRI community well versed in VIS research and best practices ? We surveyed all publications from the IEEE / ACM International Con - ference on Human - Robot Interaction ( HRI ) using the keyword “vi - sualization , ” which resulted in 262 papers , many of which focused on developing robotic interfaces that include various aspects of data visualization ( e . g . , maps , video overlays , sensor displays , etc . ) . However , we could not identify a single paper that cited any work from the VIS community , meaning that such interfaces may have been designed without considering state - of - the - art principles for efective data visualization . Likewise , we were only able to identify one relevant paper [ 63 ] and one Late - Breaking Report ( LBR ) [ 118 ] from HRI that cited visualization work from TVCG . While a lack of cross - disciplinary citations is not necessarily proof of a lack of knowledge transfer between HRI and VIS ( certain principles may be considered “foundational” and thus have no need for direct citations ) , based on the authors’ experiences and our sur - vey of VIS , TVCG , and HRI , we believe that HRI - VIS collaborations are rare and opportunities exist for greater ideological interchange . This paper highlights such opportunities in synergies across HRI and VIS . It is primarily framed for an HRI audience , focusing on how knowledge from VIS may support robot interface design , although § 6 outlines how HRI ofers potential for innovation in VIS . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 282 2 . 1 Scope Our objective is to explore how research in visualization may im - prove interface design for robotics , recognizing that one of the fundamental activities for human - robot teams is to collect and an - alyze data . We specifcally focus on human - robot teams working collaboratively towards a predefned objective , as in search - and - rescue , collecting data at feld sites , telerobotics for space , marine , or terrestrial applications , etc . , where some form of technology serves a mediating role in the interaction ( i . e . , there is an explicit visual interface through which data can be conveyed ) . Such sce - narios may involve collocated ( e . g . , an emergency responder in the feld interacting with a nearby drone through a smartphone or tablet interface ) or remote interaction ( e . g . , an astronaut on board a space station working with a robot deployed on the lunar surface via a traditional laptop or more advanced virtual or mixed reality interface ) . We do not consider purely social human - robot interac - tions , interactions that lack any overarching team objective ( s ) , or interactions where there is no mediating technology that could support visual data communication ( e . g . , a collocated human - robot interaction with exclusively verbal or gestural interaction ) . 2 . 2 Human - Robot Interface Design A full survey of human - robot interfaces is beyond the scope of this work ( relevant surveys and design guidelines from robotics can be found in [ 2 , 22 , 34 , 73 , 94 , 154 , 156 ] and metrics in [ 32 , 139 ] ) . How - ever , we briefy review several major trends in robot interface design to ground our analysis . Existing interfaces primarily support user situational awareness ( SA ) and user control . HRI has extensively ex - plored various aspects and levels of SA , including categories of SA relevant to human - robot interfaces ( e . g . , human - robot awareness , robot - human awareness , location awareness , activity awareness , surroundings awareness , status awareness , overall mission aware - ness , etc . ) [ 36 – 38 ] and interactions between SA and robot autonomy levels [ 3 ] . HRI has similarly explored control paradigms across the spectrum of teleoperation and supervision ( e . g . , shared control [ 14 , 35 ] , collaborative control [ 42 ] , delegation schemes [ 142 ] , and more exotic systems such as the “adverb pallette” [ 133 ] , etc . ) . Robot interface design frequently investigates how to provide users with information from both robot camera feeds and maps derived through low - level sensors and / or higher - level perception systems . For instance , several early projects examined the relative usefulness of map and video data [ 98 ] and explored how both data types might be fused into a single overlay display [ 15 , 29 , 99 , 154 ] . Systems continue to adopt this paradigm while leveraging modern graphics capabilities ( e . g . , [ 41 , 116 ] ) . Keyes et al . [ 78 ] provides a review of both map - centric and video - centric robot interfaces as part of an iterative interface design for remote robot teleoperation , borrowing not only from certain robot interface design guidelines available at the time [ 129 , 153 ] , but also general user interface de - sign heuristics from human - computer interaction ( HCI ) [ 100 ] . More recently , Murphy & Tadokoro [ 94 ] enumerate overlaps between general HCI principles and robotic interface design , but note that HCI principles alone are insufcient for robotics . Other major aspects in robot interface design research include determining what sensor information may be useful , developing predictive control systems ( particularly for high - latency operations [ 9 ] ) , and enabling a cohesive workfow across planning , execution , and live plan adjustments / re - planning [ 45 , 91 , 92 , 142 ] ( Fig . 1b illus - trates this evolution ) . Several guidelines for robot interface design have been proposed , often based on refections from robot compe - titions ( e . g . , DARPA challenges , Fig . 1c ) [ 71 , 103 , 153 , 154 ] or real feld deployments , such as in disaster response eforts for 9 / 11 [ 19 ] ( Fig . 1a ) or the Fukushima - Daiichi disaster [ 76 ] ) , with recent work [ 94 ] suggesting 32 diferent guidelines for feld robot interfaces ( we review the intersection between some of these guidelines and VIS best practices throughout this work ) . Such guidelines , particu - larly when combined with paradigms such as coactive design [ 69 ] that consider the interrelationships between humans and robots , may help HRI researchers and practitioners reason about various user concerns , such as SA . However , little work has considered principled methods for how these interfaces may visualize data to specifcally help user analysis and decision making . Findings and practices from VIS may help address this gap . 2 . 3 Information Visualization as a Discipline Visualization research ofers principles and design methods for cre - ating interfaces that help people efectively reason with data . To help understand when and how information visualization principles may inform robotic interfaces , we aim to establish a common lexi - con and way of structuring problems to translate practices between disciplines ( see § 3 ) . We start by discussing relevant defnitions , knowledge , and guidelines from VIS . Data analysis is the process of exploring and making sense of data , where data consists of measurable artifacts . This data may be qualitative ( e . g . , the type of task the robot is performing ) or quantitative ( e . g . , the current battery life of the robot or a sensor measurement ) . People engage in data analysis to develop insights : specifc , meaningful conclusions drawn from data [ 102 ] . For ex - ample , a user may detect a potential survivor using hot spots in a thermal scan or determine that a wall is structurally sound based on visual inspection and sensor measures . Insights collectively help analysts use the data to expand their knowledge of a problem or situation and to inform decisions and actions . Visualizations are designed to help people develop insights from data . In contrast with fully automated analyses , such as database queries or machine learning , visualization supports users in analyz - ing data across a variety of questions using a single representation [ 17 ] . By ofering users the agency to fuidly explore data to answer their questions , insights may “snowball , ” allowing users to change questions on the fy . Users can interact with visualizations to reveal new information or focus on diferent patterns as insights develop . Visualization ofers a valuable tool for HRI as the agency , fexi - bility , and control over the data analysis process provided by visu - alizations may support users in developing situational awareness , adjusting operations in dynamic environments , and rapidly and intuitively assessing mission state across multiple factors and mea - sures . However , for visualizations to be efective , they must consider the tasks —the specifc information people look to draw from data , e . g . , fnding relevant data values , estimating statistical quantities , or comparing patterns across data sources—that users want to ac - complish with their data ( see [ 5 , 130 ] for surveys ) . Note that the VIS Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 283 defnition of task ( referred to here as data tasks for clarity ) difers substantially from how HRI commonly defnes tasks . A visualization’s design determines the data tasks it best sup - ports . For example , while line graphs convey trends , people are fve times more likely to focus on quantities in bar charts [ 157 ] ; heatmaps efciently convey summary statistics , while line graphs support value estimation [ 4 ] . Tasks can inform designs that opti - mize for target applications [ 6 ] . While robots are commonly used to collect data , the corresponding data tasks are seldom directly enumerated . As a result , robotic interfaces often use visualizations with suboptimal designs . Common examples include : ( 1 ) Overview First : Nearly all analyses require frst understanding the overall picture provided by the data before gathering spe - cifc details [ 134 ] . However , many robotic interfaces show all information in the highest available level of detail frst , poten - tially overwhelming the user ( Fig . 1e ) . Others focus only on the immediately available details , removing context for those de - tails ( Fig . 1a ) . Efective interfaces should minimize unnecessary detail while retaining sufcient context , providing additional detail only when requested by the user . ( 2 ) Color Choices : Robotic interfaces commonly use rainbow ( Fig . 1c ) or red - green color schemes ( Figure 1e ) ( e . g . , every system surveyed in [ 103 ] used one of these two maps ) . Rainbows are inefective for many reasons , including inaccurate value esti - mation , difculty directing attention to specifc information , and artifcial “bands” falsely grouping data [ 12 , 143 ] , causing potential inefciencies and false conclusions that can mislead developers and other users . For example , rainbows can decrease ROI detection by 30 % [ 10 ] . Red - green schemes more precisely represent values but are inaccessible for colorblind users ( 8 % of people [ 151 ] ) . Further , green - good / red - bad mappings recom - mended by existing guidelines [ 94 ] do not hold for all cultures [ 68 ] . Visualization tools ( e . g . , ColorBrewer [ 54 ] , Colorgorical [ 50 ] , Color Crafter [ 137 ] ) , and guidelines [ 135 ] ( e . g . , use light - ness to encode numeric values [ 119 ] ) can readily guide improved color choices in robotic interfaces in as little as one line of code . ( 3 ) Comparing Data : Existing guidelines encourage interfaces that tile a set of visualizations that each answer a single question [ 94 ] ; however , too many juxtaposed visualizations may make it harder to reason across data [ 66 ] . Interfaces may alternatively choose to superimpose datasets ( i . e . , overlay data on a common space as in [ 99 ] ) or explicitly encode ( i . e . , compute and visualize ) relevant relationships between datasets [ 48 ] . Robotic interfaces have experimented with diferent forms of comparison ( Fig . 1a , 1b , 1d ) . Prior studies of teleoperation interfaces suggest that the right design depends on the goals of the interaction [ 15 , 99 , 154 ] . Best practices in composite visualization [ 67 ] , dashboard design [ 126 ] , and visual comparison [ 47 ] may help illuminate trade - ofs between designs based on the available data and users’ goals . While visualization research ofers guidelines for how to support data tasks in isolation , data in HRI typically informs a variety of both data and robotic tasks . These tasks may interact with each other in complex ways and may also interact with external aspects , such as domain standards or aspects of the data distributions [ 80 ] . Visualization ofers a suite of methods for designing data visualiza - tions that support key tasks and integrate contextual knowledge about a user’s data , constraints , and goals [ 6 , 132 ] . Integrating data visualization principles into robot interfaces through such methods can improve HRI by better facilitating sensemaking . Sensemaking , a key principle within VIS , defnes the process of how humans work with information , including that extracted directly from data and relevant context or expert knowledge , to generate conclusions or actions [ 110 , 124 ] . Sensemaking asserts that we can use information to reason about the current state of the world and use that reasoning to build awareness and inform action . One common outcome of sensemaking is decision making , which occurs when people must choose between a set of options ( e . g . , determine which building to investigate ) or courses of action ( e . g . , the best route to explore a building ) . When people use visual - izations to engage in decision making , they use patterns in data to form knowledge that provides holistic context to their decision . For example , they may reason about where they predict fre will spread to choose how to deploy limited resources , how certain they are that a hotspot on a map represents a person rather than a sensor error , or how much risk there is to a particular structure [ 106 ] . Some decisions use only the information presented in the visualization ( i . e . , they are exclusively based on the data ) , while others require integrating data with expertise or contextual information to reason across a broad body of factors . We believe that the design of robot interfaces can be improved by explicitly considering and designing for sensemaking and de - cision making processes , including determining the information and data tasks necessary for accomplishing a given objective . To help interface designers better reason about the use and presen - tation of data in robotics , we present a framework that may help HRI researchers take this more data - centric view while retaining traditional considerations regarding robot control and supervision . 3 TOWARDS DATA - CENTRIC HRI We propose a framework that emphasizes the data - oriented pro - cesses within human - robot interactions to help researchers reason about the design of new HRI interfaces and inspire deeper col - laborations between visualization and robotics . Our framework is structured around data fow among human ( s ) and robot ( s ) as it relates to accomplishing a shared core Objective . Each individ - ual team member carries out various Activities , formed through sequences of Actions , in service to the Objective . We visually illustrate our framework for team data fow in Fig . 2 and detail each framework component in Table 1 . In this framework , data can fow between humans and robots in two main ways : ( 1 ) at the Action level , where a human might query a robot regarding a particular sensor reading / group of readings or direct the robot to perform a Robot Action ( e . g . , explore a certain region ) or a robot might query a human to take advantage of human perceptual or decision - making processes ( e . g . , is it safe for the robot to move forward ) as in collaborative control paradigms [ 42 ] , and ( 2 ) from the robot Autonomous Processes to the robot interface for use in the human Data Analysis Process . HRI has traditionally focused on the frst type of information fow ( supporting humans in directing Robot Actions ) . Generating data for the second type of fow has been a major focus of traditional robotics ( e . g . , im - proving SLAM , supporting autonomous reasoning , etc . ) , although Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 284 Figure 2 : We present a framework based on human - robot data fow that visualizes the traditional focii of HRI , robot - ics , and VIS research and highlights Data Analysis Processes as a critical consideration for robot interface design . such research is often motivated from the perspective of improving robot autonomy rather than improving human - robot teamwork . Our framework highlights that such data will directly feed into the human’s Data Analysis Process ( i . e . , sensemaking , a traditional focus of VIS ) to generate new knowledge and insights that inform more efective Human Actions towards the team Objective . Our framework bridges perspectives in HRI , robotics , and VIS to understand how crossovers between the felds can inform systems that allow robots and humans to collaboratively achieve a given ob - jective . Our goal is for this framework to complement ( not replace ) , other models from HRI ( e . g . , coactive design [ 69 ] considerations , the GEDIS framework for evaluating robot interfaces [ 111 , 159 ] , video game - based frameworks for characterizing interaction design [ 117 ] , etc . ) and related areas within HCI and cognitive science ( e . g . , distributed [ 60 ] and situated cognition [ 23 ] , the human action cycle [ 101 ] , etc . ) . Each of these models views interactions at diferent levels of abstraction ; our framework aims to specifcally highlight the role of the Data Analysis Process in HRI . 4 HRI DATA TASKS A core component of our framework is recognizing that users regu - larly engage with various Data Tasks to build knowledge through the Data Analysis Process to inform human Actions . To identify these Data Tasks , we surveyed papers across HRI and feld robot - ics , covering a diverse set of domains including search - and - rescue [ 8 , 34 , 49 , 73 , 75 , 150 ] , emergency / disaster response [ 19 , 95 , 131 ] , terrestrial [ 77 , 94 , 99 , 109 , 114 , 115 , 154 ] , marine [ 13 , 27 , 65 , 136 ] , and space [ 18 , 44 , 81 , 90 , 97 ] exploration / search / environmental data collection , DARPA robotics challenges [ 69 , 71 , 88 , 103 , 104 ] , health systems [ 28 ] , unmanned aerial systems [ 21 , 29 , 45 , 70 , 91 , 159 ] , agri - cultural robotics [ 1 ] , and large robot teams [ 123 ] . We analyzed each paper using our framework , decomposing the stated human - robot interactions into the various framework components to understand what data insights and knowledge users would need to accomplish their intended objectives ( regardless of whether the interface was explicitly designed to support such analysis ) . This process revealed seven Data Tasks commonly conducted using existing interfaces ( c . f . , Fig . 3 ) . These tasks provide a direct bridge to relevant best practices , techniques , and fndings from VIS that may inform more efective data - centric interfaces . While not exhaustive , this list refects common themes we observed across HRI scenarios that we can connect to techniques and practices in VIS to inform future interfaces . We use Fig . 1f , a notional redesign of Fig . 1e created using our framework , as a running example of how these tasks can inform interface design in a rich and complex HRI scenario ( see Appendix A for details on this redesign ) . Each of these tasks requires that users estimate specifc statis - tics or data values of interest . While we do not explicitly discuss statistical estimation tasks below , they are a universal consider - ation . Visualization ofers quantifed design insight into how to best encode data for tasks such as estimating individual values [ 24 ] , assessing trends [ 30 ] , approximating averages or variance [ 4 ] , and inferring correlation [ 53 ] . For example , people are 25 % more accu - rate at averaging values in heat maps than in line graphs , but 45 % more accurate in identifying maxima using line graphs [ 4 ] . Such component statistics often form the basis for more complex tasks , including the seven discussed below . However , we caution that VIS is a rich , multifaceted design problem , much as HRI itself . As Table 1 : Details of each framework component depicted in Figure 2 Objective : a set of common goals , contexts , and criteria for success determined by the domain and known by the team in advance Agent Human ( s ) Robot ( s ) Activities Sequenceof Human Actions takeninservice to thelarger Objective ( e . g . , searching a particular set of coordinates within an overall mission framework or monitoring robot health ) as informed by individual human sub - goals and user roles . Sequence of linked Robot Actions taken in service to the larger Objective ( e . g . , a robot performing a search activity or an inspection activity ) . Actions Set of specific acts anddecisions , informedby the Data Analysis Process , that a human performs , which may involve the robot directly ( e . g . , tasking the robot to collect data or manipulate an object ) , indirectly ( e . g . , deciding whether to investigateanarea further or moveon ) , or not at all ( e . g . , radioing toanother human in the field to communicate an insight ) . Set of specific acts , informedby therobot ’ s Autonomous Processes , that a robot performs ( e . g . , collecting a soil sample or querying a human todetermineif it is safe toproceed ) ; in the traditionalrobotics sense / plan / act paradigm ( i . e . , perception / cognition / actuation ) , this represents act . Data Analysis Process The steps a human collaborator takes to make senseof robot data , whichmay beabout therobot ( e . g . , robot battery level ) or collected by the robot ( e . g . , environmentalreadings ) . Inessence , this represents thedatasensemakingprocess . Knowledge Formation Synthesizing patterns and statistics from data into insights that expand the human ’ s knowledgeandunderstandingof the Objective anddrive Actions . Data Tasks Foraging for relevant information in the data to answer questions about the Objective , whichcandrive Knowledge Formation . Autonomous Processes The set of low - level robot functional primitives that enablemorecomplex robotactions . Reasoning The robot ’ s inference and belief estimation capabilities ( i . e . , planning / cognitionsub - systems ) . Perception The robot ’ s sensing , localization , mapping , and object detection capabilities ( i . e . , sensesub - systems ) . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 285 visualizations are often used because users need to achieve multiple information , visualizations can rapidly summarize key relationships tasks at once , empirical VIS results should scafold reasoning about even without active attention [ 11 , 144 ] . However , interfaces can design / task trade - ofs , rather than provide algorithmic guidance . inhibit this awareness by providing too much detail ( introducing clutter ) , by decontextualizing information ( e . g . , zooming into the Find Relevant Information : Users must efciently detect key current operational state while providing no context for how the information about a given Objective . For example , they may need current data fts into the broader environment [ 72 ] ) , or by using to locate a door handle to turn it [ 71 ] or determine areas of safe air inefective cues ( e . g . , relying on text at the periphery of the display ) . quality . Interfaces can draw users’ attention to potentially relevant Visualization ofers a suite of techniques for providing information by manipulating the of that data [ 16 , 56 , 89 ] . overviews salience that summarize key information in large and complex datasets For instance , an interface may make incoming data more opaque and that contextualize relevant information in data about the over - and stale data more transparent . Interfaces can also make criti - all mission [ 96 , 127 ] . Overviews do not often show all available cal data “pop - out . ” However , providing too much information can information at once : efective summaries distill information into make key data harder to fnd , a phenomena known as visual clutter concise representations that help users develop a sense of the state [ 122 ] , which visualization techniques like aggregation or fltering of an operation and detect locations of interest to examine in detail can address [ 39 ] . For example , drawing trajectories in multirobot on - demand . For example , our redesign ( Fig . 1f left panel ) allows systems can make it hard to assess any individual trajectory ( Fig . operators to maintain awareness of the state of the AUV formation 1e ) . Techniques like edge bundling [ 61 ] may simplify trajectory by visually summarizing critical aspects of motion profles and collections to emphasize patterns across robots . Interfaces can then resources ( e . g . , battery , disk , synchronization ) , revealing specifc provide precise information about any single trajectory on demand . details about target robots ( i . e . , robot F _ 54 , purple ) on - demand . Alternatively , if Human Actions require assessing individual tra - Relevant visualization techniques include jectories , managing the salience of key trajectories through bolding overview + detail , where one visualization provides a concise global overview and another or related techniques and using encodings that readily distinguish shows details about the active environment , and , robots , such fo as difer nameable colors help manage cus + context ently [ 50 ] , can where details are shown in the context of an overview [ 25 ] . For clutter and make focusing on individual robots easier ( Fig . 1f ) . example , C2I [ 8 ] ( Fig . 1d ) efciently manages complexity using Synthesize Data Across Sources : Robot data is frequently multi - overview + detail visualizations coupled with detail - on - demand in - dimensional : it contains many variables often from many sources teractions , adding mission details during planning through pop - ups . ( e . g . , a sensor suite or measures of robot state ) or even within any Minimaps frequently provide overview + detail in robotic interfaces single source ( e . g . , position , color , and time in camera feeds ) . Visu - ( Fig . 1d ) ; however , such maps are often as large or larger than the alizations can help people combine data across sources to generate detail view , making it difcult for users to know where to attend . insights , providing context for holistic decision making and al - HRI interfaces seldom provide global overviews of nonspatial data , lowing users to rapidly answer complex questions . Interfaces can such as sensor readings . A few interfaces surrounded a camera feed support this combination by , for example , explicitly allowing users with directional data to provide focus + context representations ( e . g . , to compare data using a suite of visual comparison techniques , in - [ 108 ] ) ; however , extending these principles to other datatypes ( e . g . , cluding juxtaposition ( putting data side - by - side ; Fig . 1a , 1c , 1d , & periphery plots [ 93 ] ) may further enhance SA using abstract data . 1f ) , superposition ( layering data on a common set of axes ; Fig . 1b ) , Monitor Data Quality : Robots frequently collect data in locations or explicitly computing and visualizing key relationships between that may be unsafe or unreachable for humans , such as damaged data sources . These methods of supporting comparison across vari - buildings [ 95 ] , radioactive sites [ 76 ] , and space [ 18 , 44 , 81 , 90 , 97 ] . ables ofer trade - ofs in precision , clarity , ease of use , and other Such locations often correlate with environments where errors in factors that interface designers can select between based on the data collection are common . For example , a sensor may become users’ needs ( see [ 47 , 48 , 66 ] for discussions ) . miscalibrated [ 19 ] , images degraded [ 22 ] , or wireless reception lost Most robot interfaces and interface design guidelines encourage [ 103 ] . Data visualizations can help users rapidly identify data qual - juxtaposed visualizations [ 94 ] . While overreliance on juxtaposed ity errors by making data visible as it is collected [ 148 ] . For example , views can introduce clutter and inhibit comparisons by pushing can show where data has or has not been collected to as - charts further apart [ 149 ] , dashboard design practices from heatmaps VIS may sess coverage ( e . g . , Fig . 1f , grey regions on the right show completed inform efective multiview interfaces [ 126 ] . Such interfaces can fol - coverage ) . Comparing data across juxtaposed w graphs can reveal lo line a suite of best practices to support people in readily connecting calibration issues between sensors . While analyzing data quality is related information across displays , such as using consistent scales , an open challenge in VIS [ 87 ] , prior results ofer a wealth of tech - visual channels ( e . g . , size , position , color ) , and mappings and not niques for representing uncertain [ 74 , 107 , 125 ] or even missing duplicating encodings across unrelated data [ 113 ] . data [ 138 ] that may guide robot interfaces in better informing user Develop and Maintain Awareness : The human visual system actions and decisions . We can also design complementary views allows people to make sense of complex visual information at a that allow users to accommodate variations in quality across data glance [ 43 ] . Robotic interfaces can use these capabilities to help sources ( e . g . , supplementing low - quality video with sensor read - users develop and maintain situational awareness ( SA ) through ings or robot confdence in obstacle detection [ 22 ] or peripherally global views of mission data , including data about the state of the monitoring data updates as in the bar chart of sync times in Fig . 1f ) . environment [ 99 ] , the state of the robot [ 19 ] , and robot capabilities Identify Anomalies : Anomalies in robot operation ( e . g . , high fre - [ 57 ] . While tables ( Fig . 1e ) require actively reading and comparing quency control signals that can cause mechanical failure [ 71 ] ) or Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 286 collected data ( e . g . , spikes in temperature readings ) are often events of interest for users . Sometimes these anomalies can be automati - cally detected using predetermined thresholds ; however , failures are often difcult or impossible to detect autonomously [ 103 ] . Vi - sualization tools can be designed to enable people to rapidly de - tect outliers [ 30 ] or relevant diferences in patterns [ 31 ] . Further , by comparing across visualizations ( e . g . , whether diferent sen - sors show correlated changes ) or to common frames of reference ( e . g . , threshold bounds drawn on line graphs or expected bounds on movement [ 71 ] ) , users may more rapidly identify and charac - terize anomalies and use these observations to drive appropriate action . Interfaces can accommodate anomaly detection using de - signs that preserve data provenance ( historical patterns ) through techniques like data sedimentation [ 64 ] , that support comparison against known thresholds ( e . g . , reference lines showing acceptable upper and lower bounds ) or that emphasize relevant patterns ( e . g . , encoding all data samples as lines when noise matters or using smoothed lines or color when mean performance matters [ 4 ] ) . Make Predictions : Robotic interfaces often support users in mak - ing predictions , such as determining whether a robot can safely traverse a doorway [ 70 ] or estimating the current mission state when signal is lost or delayed [ 9 , 79 ] . Prediction tasks are most obvious in control and supervisory Activities where they directly guide operator decisions ; however , prediction can also guide more collaborative Activities , such as estimating a wildland fre’s spread from data collected by the robot to adjust operational plans [ 33 ] . While HRI has designed predictive interfaces ( e . g . , [ 120 , 146 , 147 ] ) , integrating data into predictive reasoning must involve efective visualization of data both over time and with uncertainty . Inno - vations in uncertainty visualization for temporal geospatial data , such as hurricanes [ 86 ] , may inform predictive interface design . For example , representing potential motion trajectories using a bounding contour causes people to overestimate the likelihood of trajectories at the contour’s edges , whereas showing all possible trajectories shifts predictions towards the modal trajectory [ 106 ] . Assess Risks : Robots are often deployed in mission - critical do - mains and pose risks to the success of the operation and / or damage to the environment , robot itself , or collocated people [ 94 ] . Data collected by robots may also help evaluate other risks relevant to the Objective ( e . g . , building inspection data might inform users about structural integrity [ 95 ] ) . To assess risk , users combine data about current robot and environment state with human knowledge about potential consequences of actions to estimate an internal cost function for diferent outcomes . For example , Fig . 1f shows available disk ( % circle fll ) in the context of the remaining planned path ( dotted lines ) to help operators anticipate if the system has sufcient resources to complete data collection . The ways that interfaces present data infuence user perceptions of risk [ 106 ] . For example , people often value salient data more highly than less visible data when assessing risk . Robot interfaces should consider how to efectively direct user attention towards the most relevant factors for estimating risk . Roldan et al . [ 120 ] draw attention to predicted risk factors in data using a “spotlight” and indicate robots at risk using smoke . While increasing salience may improve risk assessment , smoke and other overlays may occlude important details and make target objects less salient [ 89 ] . Data Task VIS Design Concepts N a v i g a t i on E x p l o r a t i on M a n i pu l a t i on I n s p e c t i on & S e a r c h D e bugg i ng Find Relevant Information Readily direct attention towards data that matters for the current activity Data Salience Clutter Synthesize Data Across Sources Compare and fuse different pieces of data into a larger insight Comparison Multiple Views Develop and Maintain Awareness Foster and revise situational awareness through data Overview + Detail Focus + Context Monitor Data Quality Assess the coverage and validity of data Missing Data Uncertainty Identify Anomalies Determine unusual or unexpected patterns in data Data Provenance Statistical Estimation Make Predictions Use data to estimate future states of robots , environment , and mission Uncertainty Temporal Data Assess Risks Estimate the costs of different courses of action Direct Attention Value Estimation Figure 3 : We identify seven common Data Tasks employed in HRI . These tasks help connect VIS principles for efective interface design to a variety of Human and Robot Activities . 5 HRI ACTIVITIES We can use these tasks to help characterize major domain appli - cations ( i . e . , the Objectives and Human / Robot Activities in our framework ) within HRI . We briefy discuss four such Activities in the context of our framework and common related Data Tasks : Environment Navigation and Exploration : Almost all robot de - ployments involve elements of environment navigation ( directing a robot to some known goal ) and exploration ( investigating an area to fnd a goal or to survey a region ) . In navigation Activities , a primary Data Task is to fnd relevant information , where interfaces should help users by indicating known targets or factors impacting the robot’s abilities to successfully reach a target . Users must syn - thesize data across sources to understand the relation between the robot’s planned and current trajectory ( Fig . 1b ) and to successfully navigate the environment ( e . g . , combining RGB camera and point cloud data to assess the 3D geometry of the environment ; Fig . 1d ) . Interfaces should allow users to develop and maintain awareness of the operational environment to respond to potential changes in the environment , make predictions about the robot’s current path and state ( e . g . , does the robot have sufcient power to reach the target ? ) , and assess risks in control decisions ( e . g . , is cutting through the canyon worth the risk of the UAV crashing ? ) . Wayfnding [ 62 ] may be of particular relevance to such navigation activities . Exploration builds on these tasks to help the human - robot team successfully reconnoitre an environment . However , analysts must also monitor data quality to understand how much of the target environment has been successfully explored and identify anom - alies to detect areas for further investigation ( e . g . , gaining more information about unexpected temperature readings ) . Manipulation : One of the key advantages robots ofer as physi - cally situated agents is the ability to manipulate the physical envi - ronment . While some aspects of manipulation may be automated , Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 287 manipulation Activities that involve human guidance or direc - tion may particularly rely on the Data Task of fnding relevant information to direct the user’s attention towards key objects or mechanisms while minimizing distracting information that may complicate assessment or distract the user . For example , colorful depth visualizations applied to background objects provided in many interfaces ( e . g . , Fig . 1c ) may distract or impede assessments of a foreground object [ 103 ] . During manipulations , users must synthesize data across sources to understand the current state of the robot and manipulated object , identify anomalies that may indicate failure patterns ( e . g . , high - frequency signals [ 71 ] ) , make predictions as to what commands are most likely to result in successful execu - tion , and assess risks of potential failure modes ( e . g . , what are the risks of applying too much or too little force to an object [ 103 ] ) . Inspection and Search : Robots can assist humans by investigat - ing and collecting data on environments as well as objects , struc - tures , and people within environments ( e . g . , inspecting buildings after earthquakes [ 95 ] , conducting geological [ 65 ] or archaeolog - ical [ 27 ] surveys , or assisting with search - and - rescue emergency response [ 34 ] ) . Such activities may involve aspects of environment navigation , exploration , and manipulation , as described above , but represent a fundamentally diferent type of Activity due to the focus on collecting contextual data ( rather than primarily spatial data , as in environment exploration ) , usually about specifc targets of interest ( e . g . , buildings , ruins , geologic points of interest , sur - vivors , etc . ) . In such activities , users must be able to fnd relevant data about the focus of their inspection ( e . g . , structural elements of a building ) and synthesize data across both contextual ( e . g . , temper - ature or force sensors ) and spatial sources to , for example , decide whether a wall is compromised . Users must develop and maintain awareness of the state of the entire space being assessed and monitor data quality to ensure that contextual data provides an accurate and complete assessment of the focus of the inspection ( e . g . , the feldsite or structure ) . To conserve time or resources , users may often wish to make predictions about the likelihood of success to adjust plans to ensure adequate and thorough coverage . Assessing risks associated with operational decisions ( e . g . , what is the risk that rescuers will miss a survivor by not investigating further [ 19 ] ; Fig . 1a ) should be emphasized in emergency response interfaces . Debugging and Recovery from Error : Across many domains , users may need to engage in Activities involving recognizing that something went wrong ( either with the overall operation , the ro - bot , or both ) , determining what specifcally went wrong , assess - ing the severity of an error , and reasoning over potential solu - tions . Prior HRI interfaces for examining programmatic state fows [ 46 , 112 , 128 ] and for robot debugging [ 26 , 82 ] may be particularly relevant and directly informed by VIS research on network visual - ization [ 58 ] and visual debugging techniques [ 59 ] . In this context , key Data Tasks include fnding relevant information related to an error , synthesize information across sources and identify relevant anomalies to determine what caused to the error , develop and main - tain awareness of the system state to characterize the error and its magnitude , and make predictions about how potential resolutions may infuence the system and the likelihood of recurrence . 6 DISCUSSION AND OPPORTUNITIES We argue for closer collaboration between HRI and VIS , focusing on how knowledge from VIS may inform data - centric robot interface design . This interdisciplinary crossover can happen at both a low - level ( e . g . , helping designers reason about color choices ) and a higher level ( e . g . , helping developers understand sensemaking and decision making ) . One recent example of the value in such cross - pollination is the MOSAIC Viewer [ 7 ] , a visualization interface for multirobot systems developed using a design study approach [ 132 ] . This interface demonstrates how managing level of detail across data dimensions and views can emphasize key aspects of robot state and belief . MRS operators perceived that the interface increased speed , trust , and understanding in detecting anomalous behaviors . While our analysis and framework here focus on how visualiza - tion can beneft HRI , we note that HRI also ofers novel opportuni - ties for visualization researchers . For example , most visualizations are designed for static data ( i . e . , data that is already available to users ) . While some tools have explored streaming data [ 64 , 84 ] and progressive computations [ 141 , 158 ] , visualization currently has limited insights into designing for dynamic data at the volume and variety collected by robots or for considering how data im - portance may vary as the operational context , data type , and data age change . Many robotics applications require combining several forms of information - rich , yet visually complex , image and spatial data , such as LiDAR , IR images , and camera feeds . While image - based visualizations exist [ 51 , 83 , 121 , 152 , 155 ] , understanding how to fuse complex image data with other forms of information and how to best analyze multiple visual streams simultaneously to support real - time decision making remain open challenges . There are also many potential directions for mutually advancing HRI and VIS beyond those detailed here . For example , both VIS [ 20 ] and HRI [ 55 , 145 ] researchers are exploring how to efectively communicate the reasoning processes used in AI algorithms . Sim - ilarly , dynamic data physicalization using robot swarms is being simultaneously investigated by both VIS [ 85 ] and HRI [ 52 , 105 ] . Our work promotes the need for greater collaboration between HRI and visualization . Such cross - disciplinary collaboration will lead to mutually benefcial innovations that help create data - centric HRI interfaces for more efectively reasoning about and acting on the vast quantities of information produced by robots . Our data - centric framework aims to scafold the design of such interfaces by highlighting the importance of Data Tasks , establishing con - nections across robotics and visualization , and identifying VIS best practices relevant to efective robot interface design . However , our framework is currently limited in its ability to provide specifc guid - ance regarding objective trade - ofs in potential designs . We hope that future research will enable our framework to evolve in its abil - ity to provide actionable strategies for achieving required outcomes in interface design , yet caution that VIS , like HRI , is a complex design problem : purely prescriptive guidance will not always be possible or desired . Rather , we hope this paper serves as a call to action for both communities to recognize potential synergies and explore co - innovation at the rich intersection of HRI and VIS . 7 ACKNOWLEDGEMENTS This work was supported by the NSF under Award # 1764092 . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 288 REFERENCES [ 1 ] George Adamides , Christos Katsanos , Ioannis Constantinou , Georgios Christou , Michalis Xenos , Thanasis Hadzilacos , and Yael Edan . 2017 . Design and devel - opment of a semi - autonomous agricultural vineyard sprayer : Human – robot interaction aspects . Journal of Field Robotics 34 , 8 ( 2017 ) , 1407 – 1426 . [ 2 ] Julie A Adams . 2002 . Critical considerations for human - robot interface develop - ment . In Proceedings of 2002 AAAI Fall Symposium . 1 – 8 . [ 3 ] Julie A Adams . 2007 . Unmanned vehicle situation awareness : A path forward . In Human systems integration symposium . 31 – 89 . [ 4 ] Danielle Albers , Michael Correll , and Michael Gleicher . 2014 . Task - driven evaluation of aggregation in time series visualization . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 551 – 560 . [ 5 ] Robert Amar , James Eagan , and John Stasko . 2005 . Low - level components of analytic activity in information visualization . In IEEE Symposium on Information Visualization , 2005 . INFOVIS 2005 . IEEE , 111 – 117 . [ 6 ] Robert Amar and John Stasko . 2004 . A knowledge task - based framework for design and evaluation of information visualizations . In IEEE Symposium on Information Visualization . IEEE , 143 – 150 . [ 7 ] Suyun Bae , Federico Rossi , Joshua Vander Hook , Scott Davidof , and Kwan - Liu Ma . 2021 . A Visual Analytics Approach to Debugging Cooperative , Autonomous Multi - Robot Systems’ Worldviews . IEEE Transactions on Visualization and Computer Graphics ( 2021 ) . [ 8 ] Haris Balta , Janusz Bedkowski , Shashank Govindaraj , Karol Majek , Pawel Musia - lik , Daniel Serrano , Kostas Alexis , Roland Siegwart , and Geert De Cubber . 2017 . Integrated data management for a feet of search - and - rescue robots . Journal of Field Robotics 34 , 3 ( 2017 ) , 539 – 582 . [ 9 ] Jonathan Bohren , Chris Paxton , Ryan Howarth , Gregory D Hager , and Louis L Whitcomb . 2016 . Semi - autonomous telerobotic assembly over high - latency networks . In ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) . 149 – 156 . [ 10 ] Michelle Borkin , Krzysztof Gajos , Amanda Peters , Dimitrios Mitsouras , Simone Melchionna , Frank Rybicki , Charles Feldman , and Hanspeter Pfster . 2011 . Eval - Journal of Field Robotics 36 , 7 ( 2019 ) , 1171 – 1191 . [ 35 ] Anca D Dragan and Siddhartha S Srinivasa . 2013 . A policy - blending formalism uation of artery disease for heart diagnosis . visualizations IEEE Transactions on for shared control . ( 2011 ) , The International Journal of otics 17 , 12 2479 – 2488 . Rob Research 32 , 7 ( 2013 ) , Visualization and Computer Graphics 790 – 805 . [ 11 ] Michelle A Borkin , Azalea A Vo , Zoya Bylinskii , Phillip Isola , Shashank [ 36 ] Jill L Drury , Brenden Keyes , and Holly A Yanco . 2007 . LASSOing HRI : analyzing Sunkavalli , Aude Oliva , and Hanspeter Pfster . 2013 . What makes a visual - situation awareness in map - centric ization memorable ? and video - centric interfaces . In ACM / IEEE IEEE Transactions on Visualization and Computer Graphics 19 , 12 ( 2013 ) , 2306 – 2315 . International Conference on Human - Rob . ot 279 – 286 . Interaction ( HRI ) [ 37 ] Jill L Drury , Laurel Russell T Ii . Riek , and [ 12 ] David Borland and M aylor 2007 . Rainbow color map ( still ) Nathan Racklife . 2006 . A decomposition of consid - UAV - related ered harmful . situation awareness . In IEEE Proceedings of the ACM SIGCHI / SIGART computer graphics and applications 27 , 2 ( 2007 ) , 14 – 17 . [ 13 ] Gerald Brantner and Oussama Khatib . 2020 . Controlling Ocean One : Human – conference on Human - robot interaction . 88 – 94 . [ 38 ] Jill L Drury , Jean Scholtz , and robot collaboration for deep - sea manipulation . ( 2020 ) . Holly A Yanco . 2003 . Awareness in human - robot Journal of Field Robotics interactions . In [ 14 ] Connor Brooks and Daniel Szafr . 2019 . Balanced information gathering and IEEE International Conference on Systems , Man and Cybernetics : , Vol . 1 . 912 – 918 . goal - orientedactionsinsharedautonomy . In System Security and Assurance ACM / IEEE International Conference [ 39 ] Geofrey . 85 – 94 . Ellis and Alan Dix . 2007 . A taxonomy of clutter reduction for informa - on Human - Robot Interaction ( HRI ) tion visualisation . 13 , [ 15 ] David J Bruemmer , David I Gertman , Curtis W Nielsen , Douglas A Few , and Transactions on Visualization and IEEE Computer Graphics 6 ( 2007 ) , 1216 – 1223 . William D Smart . 2007 . Supporting complex robot behaviors with simple inter - [ 40 ] Okan action tools . In . Erat , Werner Alexander Isop , Denis Kalkofen , and Dieter Schmalstieg . Human Robot Interaction 2018 . Drone - augmented human vision : Exocentric control for [ 16 ] Zoya Bylinskii , Nam Wook Kim , Peter O’Donovan , Sami Alsheikh , Spandan drones exploring hidden areas . 24 , 4 Madan , Hanspeter Pfster , Fredo Durand , Bryan Russell , and Aaron Hertzmann . IEEE Transactions on Visualization and Computer Graphics ( 2018 ) , 1437 – 1446 . 2017 . Learning visual importance for graphic designs and visualizations . data [ 41 ] François Ferland , François Pomerleau , In Chon Tam Le Dinh , and François Michaud . Proceedings of the 30th Annual ACM symposium on user interface software and 2009 . Egocentric and exocentric teleoperation interface using real - time , 3D . technology 57 – 69 . video projection . In [ 17 ] Stuart K Card and Jock Mackinlay . 1997 . The structure of the information Proceedings of the ACM / IEEE international conference on . 37 – 44 . visualization space design . Human ot interaction In rob Proceedings of VIZ’97 : Visualization Conference , [ 42 ] Terrence Fong , Charles Thorpe , and Charles Baur . 2001 . . IEEE , Collaborative Information contr osium and Parallel Rendering Symposium ol : A Visualization Symp robot - centric model for vehicle teleoperation . Vol . 1 . Carnegie Mellon 92 – 99 . University , The Robotics Institute . [ 18 ] Roberto Carlino , Jonathan Barlow , Jose Benavides , Maria Bualat , Aric Katterha - [ 43 ] Steven L Franconeri . 2013 . The nature and status of visual resources . ( 2013 ) . gen , Yunji Kim , Ruben Garcia Ruiz , Trey Smith , and Andres Mora Vargas . [ n . d . ] . [ 44 ] Daniel Gaines , Gary Doran , Michael Paton , Brandon Rothrock , Joseph Russino , Astrobee Free Flyers : Integrated and T ested . Ready Launch ! for NASA Technical RyanMackey , RobertAnderson , RaymondFrancis , ChetJoswig , HeatherJustice , Reports ( [ n . d . ] ) . et al . 2020 . Self - reliant rovers for [ 19 ] Jennifer Casper and Robin increased mission productivity . R . Murphy . 2003 . Human - robot interactions during Journal of the robot - assisted urban search and rescue response at the world trade center . Field Robotics 37 , 7 ( 2020 ) , 1171 – 1196 . [ 45 ] Jack Gale , John Karasinski , and Steve Hillenius . 2018 . Playbook for UAS : UX of IEEE Transactions on Systems , Man , and Cybernetics , Part B ( Cybernetics ) 33 , 3 Goal - Oriented 367 – 385 . Planning and Execution . In ( 2003 ) , International Conference on Engineer - . 545 – 557 . [ 20 ] Angelos Chatzimparmpas , Rafael M and Martins , Ilir Jusuf , Andreas Kerren . ing Psychology and Cognitive Ergonomics [ 46 ] Dylan 2020 . A survey of surveys on the use of visualization for interpreting machine F Glas , Takayuki Kanda , and Hiroshi Ishiguro . 2016 . Human - robot interaction design using Interaction Composer eight years of mo lessons learning dels . ( 2020 ) , 1473871620904671 . learned . Information Visualization In [ 21 ] Jessie YC Chen and Michael J Barnes . 2008 . Robotics operator . performance in a 2016 ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) 303 – 310 . military multi - tasking environment . In ACM / IEEE International Conference on [ 47 ] Michael Interaction ( HRI ) . Gleicher . 2017 . Considerations for visualizing comparison . Human - Robot 279 – 286 . IEEE Trans - 24 , 1 ( 2017 ) , 413 – 423 . [ 22 ] J . Y . C . Chen , E . C . Haas , and M . J . Barnes . 2007 . Human Performance Issues and actions on Visualization and Computer Graphics [ 48 ] Michael Gleicher , Danielle Albers , Rick Walker , Ilir Jusuf , Charles User Interface Design for Teleoperated Robots . D Hansen , IEEE Transactions on Systems , and Jonathan C Roberts . 2011 . Visual comparison for information visualization . Man , and Cybernetics , Part C ( Applications and Reviews ) 37 , 6 ( 2007 ) , 1231 – 1245 . [ 23 ] William J 10 , 4 ( 2011 ) , 289 – 309 . Clancey . 1997 . Information Visualization Situated cognition : On human knowledge and computer [ 49 ] Shashank Govindaraj , Keshav Chintamani , Jeremi Gancet , Pierre Letier , Boris representations . Cambridge university press . [ 24 ] William S Cleveland and Robert McGill . 1984 . Graphical perception : Theory , van Lierde , Yashodhan Nevatia , Geert De Cubber , Daniel Serrano , Miguel Esbri experimentation , and application to the development of graphical methods . Palomares , Janusz Bedkowski , et al . 2013 . The icarus project - command , control Journal of the American statistical association 79 , 387 ( 1984 ) , 531 – 554 . [ 25 ] Andy Cockburn , Amy Karlson , and Benjamin B Bederson . 2009 . A review of overview + detail , zooming , and focus + context interfaces . ACM Computing Surveys ( CSUR ) 41 , 1 ( 2009 ) , 1 – 31 . [ 26 ] THJ Collett and Bruce A MacDonald . 2006 . Developer oriented visualisation of a robot program . In Proceedings of the ACM SIGCHI / SIGART conference on Human - robot interaction . 49 – 56 . [ 27 ] Giuseppe Conte , Silvia Maria Zanoli , David Scaradozzi , and Andrea Caiti . 2009 . Robotics techniques for data acquisition in underwater archeology . International Journal of Mechanics and Control ( JoMaC ) 10 , 1 ( 2009 ) , 45 – 51 . [ 28 ] François Conti , Jaeheung Park , and Oussama Khatib . 2014 . Interface design and control strategies for a robot assisted ultrasonic examination system . In Experimental Robotics . 97 – 113 . [ 29 ] Joseph Cooper and Michael A Goodrich . 2008 . Towards combining UAV and sensor operator roles in UAV - enabled visual search . In ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) . 351 – 358 . [ 30 ] Michael Correll and Jefrey Heer . 2017 . Regression by eye : Estimating trends in bivariate visualizations . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . 1387 – 1396 . [ 31 ] Michael Correll , Mingwei Li , Gordon Kindlmann , and Carlos Scheidegger . 2018 . Looks good to me : Visualizations as sanity checks . IEEE Transactions on Visual - ization and Computer Graphics 25 , 1 ( 2018 ) , 830 – 839 . [ 32 ] Jacob W Crandall and Mary L Cummings . 2007 . Developing performance metrics for the supervisory control of multiple robots . In Proceedings of the ACM / IEEE international conference on Human - robot interaction . 33 – 40 . [ 33 ] Daniel Crawl , Jessica Block , Kai Lin , and Ilkay Altintas . 2017 . Firemap : A dy - namic data - driven predictive wildfre modeling and visualization environment . Procedia Computer Science 108 ( 2017 ) , 2230 – 2239 . [ 34 ] Jefrey Delmerico , Stefano Mintchev , Alessandro Giusti , Boris Gromov , Kamilo Melo , Tomislav Horvat , Cesar Cadena , Marco Hutter , Auke Ijspeert , Dario Floreano , et al . 2019 . The current state and future outlook of rescue robotics . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 289 and intelligence ( c2i ) . In IEEE International Symposium on Safety , Security , and [ 73 ] M Waleed Kadous , Raymond Ka - Man Sheh , and Claude Sammut . 2006 . Efec - Rescue Robotics ( SSRR ) . 1 – 4 . tive user interface design for rescue robotics . In Proceedings of the 1st ACM [ 50 ] Connor C Gramazio , David H Laidlaw , and Karen B Schloss . 2016 . Colorgorical : SIGCHI / SIGART conference on Human - Robot Interaction ( HRI ) . 250 – 257 . Creating discriminable and preferable color palettes for information visualiza - [ 74 ] Alex Kale , Matthew Kay , and Jessica Hullman . 2019 . Decision - making under tion . IEEE Transactions on Visualization and Computer Graphics 23 , 1 ( 2016 ) , uncertainty in research synthesis : Designing for the garden of forking paths . In 521 – 530 . Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . [ 51 ] Yi Gu , Chaoli Wang , Jun Ma , Robert J Nemirof , and David L Kao . 2015 . iGraph : 1 – 14 . a graph - based technique for visual analytics of image and text collections . In [ 75 ] Walter Kasper . 2016 . Team monitoring and reporting for robot - assisted usar Visualization and Data Analysis 2015 , Vol . 9397 . International Society for Optics missions . In IEEE International Symposium on Safety , Security , and Rescue Robotics and Photonics , 939708 . ( SSRR ) . 246 – 251 . [ 52 ] Darren Guinness , Annika Muehlbradt , Daniel Szafr , and Shaun K Kane . 2019 . [ 76 ] Shinji Kawatsuma , Mineo Fukushima , and Takashi Okada . 2012 . Emergency RoboGraphics : Dynamic Tactile Graphics Powered by Mobile Robots . In The response by robots to Fukushima - Daiichi accident : summary and lessons learned . International ACM SIGACCESS Conference on Computers and Accessibility . 318 – Industrial Robot : An International Journal ( 2012 ) . 328 . [ 77 ] Alonzo Kelly , Nicholas Chan , Herman Herman , Daniel Huber , Robert Meyers , [ 53 ] Lane Harrison , Fumeng Yang , Steven Franconeri , and Remco Chang . 2014 . Rank - Pete Rander , Randy Warner , Jason Ziglar , and Erin Capstick . 2011 . Real - time ing visualizations of correlation using weber’s law . IEEE Transactions on Visual - photorealistic virtualized reality interface for remote mobile robot control . The ization and Computer Graphics 20 , 12 ( 2014 ) , 1943 – 1952 . International Journal of Robotics Research 30 , 3 ( 2011 ) , 384 – 404 . [ 54 ] Mark Harrower and Cynthia A Brewer . 2003 . ColorBrewer . org : an online tool [ 78 ] Brenden Keyes , Mark Micire , Jill L Drury , Holly A Yanco , and Daisuke Chugo . for selecting colour schemes for maps . The Cartographic Journal 40 , 1 ( 2003 ) , 2010 . Improving human - robot interaction through interface evolution . In 27 – 37 . Human - robot interaction . InTech , 183 – 202 . [ 55 ] Bradley Hayes and Julie A Shah . 2017 . Improving robot controller transparency [ 79 ] Won Soo Kim , Paul S Schenker , Antal K Bejczy , Stephen Leake , and Stanford through autonomous policy explanation . In ACM / IEEE International Conference Ollendorf . 1993 . Advanced operator interface design with preview / predictive on Human - Robot Interaction ( HRI ) . 303 – 312 . displays for ground - controlled space telerobotic servicing . In Telemanipulator [ 56 ] Christopher Healey and James Enns . 2011 . Attention and visual memory in Technology and Space Telerobotics , Vol . 2057 . 96 – 107 . visualization and computer graphics . IEEE Transactions on Visualization and [ 80 ] Younghoon Kim and Jefrey Heer . 2018 . Assessing efects of task and data Computer Graphics 18 , 7 ( 2011 ) , 1170 – 1188 . distribution on the efectiveness of visual encodings . In Computer Graphics [ 57 ] Hooman Hedayati , Michael Walker , and Daniel Szafr . 2018 . Improving collo - Forum , Vol . 37 . Wiley Online Library , 157 – 167 . cated robot teleoperation with augmented reality . In Proceedings of the 2018 [ 81 ] Tobias Klamt , Max Schwarz , Christian Lenz , Lorenzo Baccelliere , Domenico ACM / IEEE International Conference on Human - Robot Interaction . 78 – 86 . Buongiorno , Torben Cichon , Antonio DiGuardo , David Droeschel , Massimiliano [ 58 ] Ivan Herman , Guy Melançon , and M Scott Marshall . 2000 . Graph visualization Gabardi , Malgorzata Kamedula , et al . 2020 . Remote mobile manipulation with and navigation in information visualization : A survey . IEEE Transactions on the centauro robot : Full - body telepresence and autonomous operator assistance . Visualization and Computer Graphics 6 , 1 ( 2000 ) , 24 – 43 . Journal of Field Robotics 37 , 5 ( 2020 ) , 889 – 919 . [ 59 ] Jane Hofswell , Arvind Satyanarayan , and Jefrey Heer . 2016 . Visual debugging [ 82 ] Tijn Kooijmans , Takayuki Kanda , Christoph Bartneck , Hiroshi Ishiguro , and techniques for reactive data visualization . In Computer Graphics Forum , Vol . 35 . Norihiro Hagita . 2006 . Interaction debugging : an integral approach to analyze 271 – 280 . human - robot interaction . In Proceedings of the ACM SIGCHI / SIGART conference [ 60 ] James Hollan , Edwin Hutchins , and David Kirsh . 2000 . Distributed cognition : on Human - robot interaction . 64 – 71 . toward a new foundation for human - computer interaction research . ACM [ 83 ] Ashnil Kumar , Falk Nette , Karsten Klein , Michael Fulham , and Jinman Kim . 2014 . Transactions on Computer - Human Interaction ( TOCHI ) 7 , 2 ( 2000 ) , 174 – 196 . A visual analytics approach using the exploration of multidimensional feature [ 61 ] Danny Holten and Jarke J Van Wijk . 2009 . Force - directed edge bundling for spaces for content - based medical image retrieval . IEEE journal of biomedical graph visualization . In Computer graphics forum , Vol . 28 . Wiley Online Library , and health informatics 19 , 5 ( 2014 ) , 1734 – 1746 . 983 – 990 . [ 84 ] Ove Daae Lampe and Helwig Hauser . 2011 . Interactive visualization of streaming [ 62 ] Haikun Huang , Ni - Ching Lin , Lorenzo Barrett , Darian Springer , Hsueh - Cheng data with kernel density estimation . In 2011 IEEE pacifc visualization symposium . Wang , Marc Pomplun , and Lap - Fai Yu . 2017 . Automatic optimization of wayfnd - IEEE , 171 – 178 . ing design . IEEE Transactions on Visualization and Computer Graphics 24 , 9 [ 85 ] Mathieu Le Goc , Charles Perin , Sean Follmer , Jean - Daniel Fekete , and Pierre ( 2017 ) , 2516 – 2530 . Dragicevic . 2018 . Dynamic composite data physicalization using wheeled micro - [ 63 ] Curtis M Humphrey and Julie A Adams . 2008 . Compass visualizations for robots . IEEE Transactions on Visualization and Computer Graphics 25 , 1 ( 2018 ) , human - robotic interaction . In Proceedings of the 3rd ACM / IEEE international 737 – 747 . conference on Human robot interaction . 49 – 56 . [ 86 ] Le Liu , Lace Padilla , Sarah H Creem - Regehr , and Donald H House . 2018 . Visual - [ 64 ] Samuel Huron , Romain Vuillemot , and Jean - Daniel Fekete . 2012 . Towards visual izing uncertain tropical cyclone predictions using representative samples from sedimentation . In IEEE Transactions on Visualization and Computer Graphics . ensembles of forecast tracks . IEEE Transactions on Visualization and Computer IEEE . Graphics 25 , 1 ( 2018 ) , 882 – 891 . [ 65 ] AV Inzartsev , Alexander Kamorniy , Lev Kiselyov , Yury Matviyenko , Nicolay [ 87 ] Shixia Liu , Gennady Andrienko , Yingcai Wu , Nan Cao , Liu Jiang , Conglei Shi , Rylov , Roman Rylov , and Yury Vaulin . 2010 . Integrated positioning system of Yu - Shuen Wang , and Seokhee Hong . 2018 . Steering data quality with visual autonomous underwater robot and its application in high latitudes of arctic analytics : The complexity challenge . Visual Informatics 2 , 4 ( 2018 ) , 191 – 197 . zone . Motion Control . Vienna : InTech ( 2010 ) , 229 – 244 . [ 88 ] Pat Marion , Maurice Fallon , Robin Deits , Andrés Valenzuela , Claudia [ 66 ] Nicole Jardine , Brian D Ondov , Niklas Elmqvist , and Steven Franconeri . 2019 . Pérez D’Arpino , Greg Izatt , Lucas Manuelli , Matt Antone , Hongkai Dai , Twan The Perceptual Proxies of Visual Comparison . IEEE Transactions on Visualization Koolen , et al . 2017 . Director : A user interface designed for robot operation with and Computer Graphics 26 , 1 ( 2019 ) , 1012 – 1021 . shared autonomy . Journal of Field Robotics 34 , 2 ( 2017 ) , 262 – 280 . [ 67 ] Waqas Javed and Niklas Elmqvist . 2012 . Exploring the design space of composite [ 89 ] Laura E Matzen , Michael J Haass , Kristin M Divis , Zhiyuan Wang , and Andrew T visualization . In 2012 IEEE Pacifc Visualization Symposium . IEEE , 1 – 8 . Wilson . 2017 . Data visualization saliency model : A tool for evaluating abstract [ 68 ] Feng Jiang , Su Lu , Xiang Yao , Xiaodong Yue , and Wing Tung Au . 2014 . Up or data visualizations . IEEE Transactions on Visualization and Computer Graphics down ? How culture and color afect judgments . Journal of Behavioral Decision 24 , 1 ( 2017 ) , 563 – 573 . Making 27 , 3 ( 2014 ) , 226 – 234 . [ 90 ] Mark Micire , Terrence Fong , Ted Morse , Eric Park , Chris Provencher , Ernest [ 69 ] Matthew Johnson , Jefrey M Bradshaw , Paul J Feltovich , Catholijn M Jonker , Smith , Vinh To , R Jay Torres , DW Wheeler , and David Mittman . 2013 . Smart M Birna Van Riemsdijk , and Maarten Sierhuis . 2014 . Coactive design : Designing spheres : a telerobotic free - fyer for intravehicular activities in space . In AIAA support for interdependence in joint activity . Journal of Human - Robot Interaction Space 2013 Conference and Exposition . 5338 . 3 , 1 ( 2014 ) , 43 – 69 . [ 91 ] Christopher Miller , Harry Funk , Peggy Wu , Robert Goldman , John Meisner , [ 70 ] Matthew Johnson , John Carf , and Jerry Pratt . 2012 . Coactive design for human - and Marc Chapman . 2005 . The Playbook approach to adaptive automation . In MAV team navigation . In International micro aerial vehicle conference and com - Proceedings of the Human Factors and Ergonomics Society Annual Meeting , Vol . 49 . petition . 15 – 19 . [ 71 ] Matthew Johnson , Brandon Shrewsbury , Sylvain Bertrand , Tingfan Wu , Daniel [ 92 ] Christopher A Miller , Harry B Funk , Michael Dorneich , and Stephen D Whitlow . Duran , Marshall Floyd , Peter Abeles , Douglas Stephen , Nathan Mertins , Alex 2002 . A playbook interface for mixed initiative control of multiple unmanned Lesman , et al . 2015 . Team IHMC’s lessons learned from the DARPA robotics vehicle teams . In IEEE Digital Avionics Systems Conference , Vol . 2 . 7E4 – 7E4 . challenge trials . Journal of Field Robotics 32 , 2 ( 2015 ) , 192 – 208 . [ 93 ] Bryce Morrow , Trevor Manz , Arlene E Chung , Nils Gehlenborg , and David Gotz . [ 72 ] Susanne Jul and George W Furnas . 1998 . Critical zones in desert fog : aids to 2019 . Periphery Plots for Contextualizing Heterogeneous Time - Based Charts . multiscale navigation . In Proceedings of the 11th annual ACM symposium on User In 2019 IEEE Visualization Conference ( VIS ) . IEEE , 1 – 5 . interface software and technology . 97 – 106 . [ 94 ] Robin R Murphy and Satoshi Tadokoro . 2019 . User Interfaces for Human - Robot Interaction in Field Robotics . In Disaster Robotics . 507 – 528 . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 290 [ 95 ] Francesca Negrello , Alessandro Settimi , Danilo Caporale , Gianluca Lentini , Mat - multimedia applications . In Proceedings of the ACM SIGCHI / SIGART conference tia Poggiani , Dimitrios Kanoulas , Luca Muratore , Emanuele Luberto , Gaspare on human - robot interaction . 266 – 273 . Santaera , Luca Ciarleglio , et al . 2018 . Humanoids at work : The walk - man robot [ 118 ] Kris Rogers , Janet Wiles , Scott Heath , Kristyn Hensby , and Jonathon Taufatofua . in a postearthquake scenario . IEEE Robotics & Automation Magazine 25 , 3 ( 2018 ) , 2016 . Discovering patterns of touch : a case study for visualization - driven analy - 8 – 22 . sis in human - robot interaction . In 2016 11th ACM / IEEE International Conference [ 96 ] F Nguyen , X Qiao , J Heer , and J Hullman . 2020 . Exploring the Efects of Aggre - on Human - Robot Interaction ( HRI ) . 499 – 500 . gation Choices on Untrained Visualization Users’ Generalizations From Data . [ 119 ] Bernice E Rogowitz and Alan D Kalvin . 2001 . The " Which Blair Project " : a quick In Computer Graphics Forum . Wiley Online Library . visual method for evaluating perceptual color maps . In Proceedings Visualization , [ 97 ] Laurent A Nguyen , Maria Bualat , Laurence J Edwards , Lorenzo Flueckiger , 2001 . VIS’01 . IEEE , 183 – 556 . Charles Neveu , Kurt Schwehr , Michael D Wagner , and Eric Zbinden . 2001 . [ 120 ] Juan Jesús Roldán , Elena Peña - Tapia , Andrés Martín - Barrio , Miguel A Olivares - Virtual reality interfaces for visualization and control of remote vehicles . Au - Méndez , Jaime Del Cerro , and Antonio Barrientos . 2017 . Multi - robot interfaces tonomous Robots 11 , 1 ( 2001 ) , 59 – 68 . and operator situational awareness : Study of the impact of immersion and [ 98 ] Curtis W Nielsen and Michael A Goodrich . 2006 . Comparing the usefulness prediction . Sensors 17 , 8 ( 2017 ) , 1720 . of video and map information in navigation tasks . In Proceedings of the ACM [ 121 ] Ork Rooij , Jarke van Wijk , and Marcel Worring . 2010 . Mediatable : Interac - SIGCHI / SIGART conference on Human - robot interaction . 95 – 101 . tive categorization of multimedia collections . IEEE Computer Graphics and [ 99 ] Curtis W Nielsen , Michael A Goodrich , and Robert W Ricks . 2007 . Ecological Applications 30 , 5 ( 2010 ) , 42 – 51 . interfaces for improving mobile robot teleoperation . IEEE Transactions on [ 122 ] Ruth Rosenholtz , Yuanzhen Li , and Lisa Nakano . 2007 . Measuring visual clutter . Robotics 23 , 5 ( 2007 ) , 927 – 941 . Journal of vision 7 , 2 ( 2007 ) , 17 – 17 . [ 100 ] Jakob Nielsen . 1994 . Usability engineering . Morgan Kaufmann . [ 123 ] Karina A Roundtree , Jason R Cody , Jennifer Leaf , H Onan Demirel , and Julie A [ 101 ] Donald A Norman . 1984 . Stages and levels in human - machine interaction . Adams . 2019 . Visualization design for human - collective teams . In Proceedings International journal of man - machine studies 21 , 4 ( 1984 ) , 365 – 375 . of the Human Factors and Ergonomics Society Annual Meeting , Vol . 63 . 417 – 421 . [ 102 ] Chris North . 2006 . Toward measuring visualization insight . IEEE computer [ 124 ] Daniel M Russell , Mark J Stefk , Peter Pirolli , and Stuart K Card . 1993 . The graphics and applications 26 , 3 ( 2006 ) , 6 – 9 . cost structure of sensemaking . In Proceedings of the INTERACT’93 and CHI’93 [ 103 ] Adam Norton , Willard Ober , Lisa Baraniecki , David Shane , Anna Skinner , and conference on Human factors in computing systems . 269 – 276 . Holly Yanco . 2018 . Perspectives on Human - Robot Team Performance from an [ 125 ] Jibonananda Sanyal , Song Zhang , Gargi Bhattacharya , Phil Amburn , and Robert Evaluation of the DARPA Robotics Challenge . In The DARPA Robotics Challenge Moorhead . 2009 . A user study to compare four uncertainty visualization methods Finals : Humanoid Robots To The Rescue . 631 – 666 . for 1d and 2d datasets . IEEE Transactions on Visualization and Computer Graphics [ 104 ] Kyohei Otsu , Scott Tepsuporn , Rohan Thakker , Tiago Stegun Vaquero , Jefrey A 15 , 6 ( 2009 ) , 1209 – 1218 . Edlund , William Walsh , Gregory Miles , Tristan Heywood , Michael T Wolf , and [ 126 ] Alper Sarikaya , Michael Correll , Lyn Bartram , Melanie Tory , and Danyel Fisher . Ali - Akbar Agha - Mohammadi . 2020 . Supervised Autonomy for Communication - 2018 . What do we talk about when we talk about dashboards ? IEEE Transactions degraded Subterranean Exploration by a Robot Team . In 2020 IEEE Aerospace on Visualization and Computer Graphics 25 , 1 ( 2018 ) , 682 – 692 . Conference . 1 – 9 . [ 127 ] Alper Sarikaya , Michael Gleicher , and Danielle Albers Szafr . 2018 . Design [ 105 ] Ayberk Özgür , Séverin Lemaignan , Wafa Johal , Maria Beltran , Manon Briod , Léa factors for summary visualization in visual analytics . In Computer Graphics Pereyre , Francesco Mondada , and Pierre Dillenbourg . 2017 . Cellulo : Versatile Forum , Vol . 37 . Wiley Online Library , 145 – 156 . handheld robots for education . In ACM / IEEE International Conference on Human - [ 128 ] Allison Sauppé and Bilge Mutlu . 2014 . Design patterns for exploring and Robot Interaction ( HRI . 119 – 127 . prototyping human - robot interactions . In Proceedings of the SIGCHI Conference [ 106 ] Lace M Padilla , Sarah H Creem - Regehr , Mary Hegarty , and Jeanine K Stefanucci . on Human Factors in Computing Systems . 1439 – 1448 . 2018 . Decision making with visualizations : a cognitive framework across disci - [ 129 ] Jean Scholtz , Jef Young , Jill L Drury , and Holly A Yanco . 2004 . Evaluation of plines . Cognitive research : principles and implications 3 , 1 ( 2018 ) , 29 . human - robot interaction awareness in search and rescue . In IEEE International [ 107 ] Lace M Padilla , Grace Hansen , Ian T Ruginski , Heidi S Kramer , William B Conference on Robotics and Automation ( ICRA ) , Vol . 3 . 2327 – 2332 . Thompson , and Sarah H Creem - Regehr . 2015 . The infuence of diferent graph - [ 130 ] Hans - Jörg Schulz , Thomas Nocke , Magnus Heitzler , and Heidrun Schumann . ical displays on nonexpert decision making under uncertainty . Journal of 2013 . A design space of visualization tasks . IEEE Transactions on Visualization Experimental Psychology : Applied 21 , 1 ( 2015 ) , 37 . and Computer Graphics 19 , 12 ( 2013 ) , 2366 – 2375 . [ 108 ] Ramviyas Parasuraman , Sergio Caccamo , Fredrik Båberg , Petter Ögren , and [ 131 ] Max Schwarz , Tobias Rodehutskors , David Droeschel , Marius Beul , Michael Mark Neerincx . 2017 . A New UGV Teleoperation Interface for Improved Aware - Schreiber , Nikita Araslanov , Ivan Ivanov , Christian Lenz , Jan Razlaw , Sebastian ness of Network Connectivity and Physical Surroundings . J . Hum . - Robot Interact . Schüller , et al . 2017 . NimbRo Rescue : Solving disaster - response tasks with 6 , 3 ( 2017 ) , 48 – 70 . https : / / doi . org / 10 . 5898 / JHRI . 6 . 3 . Parasuraman the mobile manipulation robot Momaro . Journal of Field Robotics 34 , 2 ( 2017 ) , [ 109 ] Savannah Paul , Christopher Reardon , Tom Williams , and Hao Zhang . 2020 . 400 – 425 . Designing augmented reality visualizations for synchronized and time - dominant [ 132 ] Michael Sedlmair , Miriah Meyer , and Tamara Munzner . 2012 . Design study human - robot teaming . In Virtual , Augmented , and Mixed Reality ( XR ) Technology methodology : Refections from the trenches and the stacks . IEEE Transactions for Multi - Domain Operations , Vol . 11426 . International Society for Optics and on Visualization and Computer Graphics 18 , 12 ( 2012 ) , 2431 – 2440 . Photonics , 1142607 . [ 133 ] Meher T Shaikh and Michael A Goodrich . 2017 . Design and evaluation of adverb [ 110 ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage palette : A gui for selecting tradeofs in multi - objective optimization problems . In points for analyst technology as identifed through cognitive task analysis . In ACM / IEEE International Conference on Human - Robot Interaction ( HRI . 389 – 397 . Proceedings of international conference on intelligence analysis , Vol . 5 . McLean , [ 134 ] Ben Shneiderman . 1996 . The eyes have it : A task by data type taxonomy VA , USA , 2 – 4 . for information visualizations . In Proceedings 1996 IEEE symposium on visual [ 111 ] Pere Ponsa and Marta Díaz . 2007 . Creation of an ergonomic guideline for languages . IEEE , 336 – 343 . supervisory control interface design . In International Conference on Engineering [ 135 ] Samuel Silva , Beatriz Sousa Santos , and Joaquim Madeira . 2011 . Using color in Psychology and Cognitive Ergonomics . 137 – 146 . visualization : A survey . Computers & Graphics 35 , 2 ( 2011 ) , 320 – 333 . [ 112 ] David Porfrio , Allison Sauppé , Aws Albarghouthi , and Bilge Mutlu . 2018 . Au - [ 136 ] Enrico Simetti , Giovanni Indiveri , and António M Pascoal . 2020 . WiMUST : thoring and verifying human - robot interactions . In Proceedings of the ACM A cooperative marine robotic system for autonomous geotechnical surveys . Symposium on User Interface Software and Technology . 75 – 86 . Journal of Field Robotics ( 2020 ) . [ 113 ] Zening Qu and Jessica Hullman . 2017 . Keeping multiple views consistent : Con - [ 137 ] Stephen Smart , Keke Wu , and Danielle Albers Szafr . 2019 . Color Crafting : Au - straints , validations , and exceptions in visualization authoring . IEEE Transactions tomating the Construction of Designer Quality Color Ramps . IEEE Transactions on Visualization and Computer Graphics 24 , 1 ( 2017 ) , 468 – 477 . on Visualization and Computer Graphics 26 , 1 ( 2019 ) , 1215 – 1225 . [ 114 ] Christopher Reardon , Jason Gregory , Carlos Nieto - Granda , and John G Rogers . [ 138 ] Hayeong Song and Danielle Albers Szafr . 2018 . Where’s my data ? evaluating vi - 2020 . Enabling Situational Awareness via Augmented Reality of Autonomous sualizations with missing data . IEEE Transactions on Visualization and Computer Robot - Based Environmental Change Detection . In International Conference on Graphics 25 , 1 ( 2018 ) , 914 – 924 . Human - Computer Interaction . 611 – 628 . [ 139 ] Aaron Steinfeld , Terrence Fong , David Kaber , Michael Lewis , Jean Scholtz , [ 115 ] Christopher Reardon , Kevin Lee , John G Rogers , and Jonathan Fink . 2019 . Com - Alan Schultz , and Michael Goodrich . 2006 . Common metrics for human - robot municating via Augmented Reality for Human - Robot Teaming in Field Envi - interaction . In Proceedings of the ACM SIGCHI / SIGART conference on Human - ronments . In 2019 IEEE International Symposium on Safety , Security , and Rescue robot interaction . 33 – 40 . Robotics ( SSRR ) . 94 – 101 . [ 140 ] Mark R Stevens , J Ross Beveridge , and Michael E Goss . 1997 . Visualizing Multi - [ 116 ] Aurélien Reveleau , François Ferland , Mathieu Labbé , Dominic Létourneau , and sensor Model - Based Object Recognition . In IEEE Transactions on Visualization François Michaud . 2015 . Visual representation of interaction force and sound and Computer Graphics . source in a teleoperation user interface for a mobile robot . Journal of Human - [ 141 ] Charles D Stolper , Adam Perer , and David Gotz . 2014 . Progressive visual analyt - Robot Interaction 4 , 2 ( 2015 ) , 1 – 23 . ics : User - driven visual exploration of in - progress analytics . IEEE Transactions [ 117 ] Justin Richer and Jill L Drury . 2006 . A video game - based framework for analyzing human - robot interaction : characterizing interface design in real - time interactive Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 291 on Visualization and Computer Graphics 20 , 12 ( 2014 ) , 1653 – 1662 . [ 152 ] Xiao Xie , Xiwen Cai , Junpei Zhou , Nan Cao , and Yingcai Wu . 2018 . A semantic - [ 142 ] Daniel Szafr , Bilge Mutlu , and Terrence Fong . 2017 . Designing planning and con - based method for visualizing large image collections . IEEE Transactions on trol interfaces to support user collaboration with fying robots . The International Visualization and Computer Graphics 25 , 7 ( 2018 ) , 2362 – 2377 . Journal of Robotics Research 36 , 5 - 7 ( 2017 ) , 514 – 542 . [ 153 ] Holly A Yanco , Jill L Drury , and Jean Scholtz . 2004 . Beyond usability evaluation : [ 143 ] Danielle Albers Szafr . 2018 . The good , the bad , and the biased : fve ways Analysis of human - robot interaction at a major robotics competition . Human – visualizations can mislead ( and how to fx them ) . interactions 25 , 4 ( 2018 ) , Computer Interaction 19 , 1 - 2 ( 2004 ) , 117 – 149 . 26 – 33 . [ 154 ] Holly A Yanco , Brenden Keyes , Jill L Drury , Curtis W Nielsen , Douglas A Few , [ 144 ] Danielle Albers Szafr , Steve Haroz , Michael Gleicher , and Steven Franconeri . and David J Bruemmer . 2007 . Evolving interface design for robot search tasks . 2016 . Four types of ensemble coding in data visualizations . Journal of vision 16 , Journal of Field Robotics 24 , 8 - 9 ( 2007 ) , 779 – 799 . 5 ( 2016 ) , 11 – 11 . [ 155 ] Jing Yang , Jianping Fan , Daniel Hubball , Yuli Gao , Hangzai Luo , William Rib - [ 145 ] Aaquib Tabrez , Shivendra Agrawal , and Bradley Hayes . 2019 . Explanation - based arsky , and Matthew Ward . 2006 . Semantic image browser : Bridging information reward coaching to improve human performance via reinforcement learning . In visualization with automated intelligent image analysis . In 2006 IEEE Symposium ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) . 249 – 257 . On Visual Analytics Science And Technology . IEEE , 191 – 198 . [ 146 ] Michael Walker , Hooman Hedayati , Jennifer Lee , and Daniel Szafr . 2018 . Com - [ 156 ] Sierra N Young and Joshua M Peschel . 2020 . Review of Human – Machine municating robot motion intent with augmented reality . In Proceedings of the Interfaces for Small Unmanned Systems With Robotic Manipulators . IEEE 2018 ACM / IEEE International Conference on Human - Robot Interaction . 316 – 324 . Transactions on Human - Machine Systems 50 , 2 ( 2020 ) , 131 – 143 . [ 147 ] Michael E Walker , Hooman Hedayati , and Daniel Szafr . 2019 . Robot tele - [ 157 ] Jef Zacks and Barbara Tversky . 1999 . Bars and lines : A study of graphic com - operation with augmented reality virtual surrogates . In 2019 14th ACM / IEEE munication . Memory & cognition 27 , 6 ( 1999 ) , 1073 – 1079 . International Conference on Human - Robot Interaction ( HRI ) . IEEE , 202 – 210 . [ 158 ] Emanuel Zgraggen , Alex Galakatos , Andrew Crotty , Jean - Daniel Fekete , and Tim [ 148 ] Matt Whitlock , Keke Wu , and Danielle Albers Szafr . 2019 . Designing for mobile Kraska . 2016 . How progressive visualizations afect exploratory analysis . IEEE and immersive visual analytics in the feld . IEEE Transactions on Visualization Transactions on Visualization and Computer Graphics 23 , 8 ( 2016 ) , 1977 – 1987 . and Computer Graphics 26 , 1 ( 2019 ) , 503 – 513 . [ 159 ] Wenjuan Zhang , David Feltner , James Shirley , David Kaber , and Manida S Neu - [ 149 ] Christopher D Wickens and C Melody Carswell . 1995 . The proximity compati - bert . 2020 . Enhancement and Application of a UAV Control Interface Evaluation bility principle : Its psychological foundation and relevance to display design . Technique : Modifed GEDIS - UAV . ACM Transactions on Human - Robot Interaction Human factors 37 , 3 ( 1995 ) , 473 – 494 . ( THRI ) 9 , 2 ( 2020 ) , 1 – 20 . [ 150 ] Grant A Wilde and Robin R Murphy . 2018 . User interface for unmanned surface [ 160 ] Stefanie Zollmann , Christof Hoppe , Tobias Langlotz , and Gerhard Reitmayr . vehicles used to rescue drowning victims . In IEEE International Symposium on 2014 . Flyar : Augmented reality supported micro aerial vehicle navigation . IEEE Safety , Security , and Rescue Robotics ( SSRR ) . 1 – 8 . Transactions on Visualization and Computer Graphics ( TVCG 20 , 4 ( 2014 ) , 560 – [ 151 ] Bang Wong . 2011 . Color blindness . Nature Methods 8 , 6 ( 2011 ) , 441 – 442 . 568 . Session 5 : Theory & Methods HRI ’21 , March 8 – 11 , 2021 , Boulder , CO , USA 292