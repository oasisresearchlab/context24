DeepDIVA : A Highly - Functional Python Framework for Reproducible Experiments Michele Alberti ∗ , Vinaychandran Pondenkandath ∗ , Marcel W¨ursch , Rolf Ingold , Marcus Liwicki Document Image and Voice Analysis Group ( DIVA ) University of FribourgSwitzerland { ﬁrstname } . { lastname } @ unifr . ch Abstract —We introduce DeepDIVA : an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality . Reproducing scientiﬁc results can be a frustrating experience , not only in document image analysis but in machine learning in general . Using DeepDIVA a researcher can either reproduce a given experiment with a very limited amount of information or share their own experiments with others . Moreover , the framework offers a large range of functions , such as boilerplate code , keeping track of experiments , hyper - parameter optimization , and visualization of data and results . To demonstrate the effectiveness of this framework , this paper presents case studies in the area of handwritten document analysis where researchers beneﬁt from the integrated functionality . DeepDIVA is implemented in Python and uses the deep learning framework PyTorch . It is completely open source 1 , and accessible as Web Service through DIVAServices 2 . Keywords — Framework , Open - Source , Deep Learning , Neural Networks , Reproducible Research , Machine Learning , Hyper - parameters Optimization , Python . I . I NTRODUCTION An important topic , not only in handwritten historical docu - ment image analysis , but in machine learning in general , is the reproducibility of scientiﬁc results . Often , we have scientiﬁc publications in our hands showing results difﬁcult to repro - duce . This phenomenon , known as the reproducibility crisis , is a methodological phenomenon in which many prominent studies and papers have been found to be difﬁcult or impossible to reproduce . The replication crisis has been widely studied in other ﬁelds [ 1 ] , [ 2 ] , [ 3 ] and is getting more attention in the machine learning community , recently [ 4 ] , [ 5 ] . This even led to prompting a replication challenge [ 6 ] for reproducing the results of prominent papers . Traditionally , scientiﬁc experiments are to be described in a way that allows anyone to replicate them . However , especially in computer - science , there are many complications which makes complete reproducibility particularly difﬁcult to achieve . This can be attributed to differences in software versions , speciﬁc dependencies , or even hardware variations . Furthermore , speciﬁc details are sometimes conﬁdential and the authors are not allowed to include all hints leading to ∗ Both authors contributed equally to this work . 1 https : / / github . com / DIVA - DIA / DeepDIVA 2 http : / / divaservices . unifr . ch ( a ) Comparison of two different training protocols on a watermark classiﬁcation task . Orange : network initialized with random weights . Pink : network initialized with pre - trained weights ( b ) Evaluation of how randomness affects execution by visualizing the aggregated results of multiple runs . Here the shaded area indicates the variance over all runs . ( c ) Confusion Matrix . The darker the color the higher the amount of samples classiﬁed as such . ( d ) T - Distributed Stochastic Neigh - bor Embedding ( T - SNE ) visualiza - tion of the watermarks dataset in Tensorboard . Fig . 1 . Example of different visualizations produced automatically by DeepDIVA . successful reproduction . In many cases , details are just omitted , because they are thought to be obvious or they would require too much space which is rare due to the page limit . Especially in such cases , publishing of the source code or even providing the whole experimental set - up would be beneﬁcial . To address many of these issues , we develop a framework to help researchers in both , saving time on common research - oriented software problems and sharing experiments by making them easier to reproduce . Our framework , DeepDIVA , is originally designed to perform deep - learning tasks on historical a r X i v : 1805 . 00329v1 [ c s . C V ] 23 A p r 2018 2 ( a ) Output decisions at epoch 1 ( b ) Output decisions at epoch 10 Fig . 2 . Visualization of the decision boundaries of a network learning the continuous XOR problem . Color intensity represent the conﬁdence of the network . document images . However , thanks to it’s ﬂexible and modular design it can be easily extended to work in other ﬁelds or new tasks . We hope to engage the community in building a shared tool which would eventually foster experiment reproducibility and higher quality of code , beneﬁting newcomers in the ﬁeld as well as experts . Main Contribution In this paper , we introduce DeepDIVA : an open - source Python deep - learning framework which aims at easing the life of researchers by providing a variety of useful features out - of - the - box . DeepDIVA is designed to enable experiment repro - duction with minimal effort . The framework integrates popular tools such as SigOpt [ 7 ] for hyper - parameter optimization and TensorBoard 3 for aggregating all visualization produced ( see Fig . 1 ) . Additionally , we provides boilerplate code for standard deep - learning experiments and especially for experiments on Handwritten Documentents . It also offers the possibility to be run as a Web Service on D IVA Services [ 8 ] . A . Related Work DeepDIVA shares the spirit and motivation with several existing frameworks and tools . Neptune [ 9 ] is a commercial solution that offers a variety of features ( similar to Deep - DIVA ) and also has extended support for cloud computing . Comet [ 10 ] lets you track code , experiments , and results and works with most machine learning libraries . CodaLab [ 11 ] provides experiments management trough a proprietary web interface and allows to re - run other peoples experiments . Data Version Control [ 12 ] is an open - source tool that manages code and data together in a simple form of Git - like commands . Polyaxon [ 13 ] is an open - source platform for building , train - ing , and monitoring large - scale deep learning applications . It allows for deployment to different solutions such as data centers or cloud providers . OpenML [ 14 ] is a tool which focuses on making datasets , tasks , workﬂows as well as results accessible for free . Many other tools , such as Sumatra [ 15 ] , 3 https : / / www . tensorﬂow . org / programmers guide / summaries and tensorboard Sacred [ 16 ] , CDE [ 17 ] , FGBLab [ 18 ] and ReproZip [ 19 ] , collect and store much different information about sources , dependencies , conﬁgurations , the ﬁle used , host information and even system calls , with the aim of supporting reproducible research . These tools collect and store different information about sources , dependencies , conﬁgurations , the ﬁle used , host information and even system calls , with the aim of supporting reproducible research . However , most or these tools strive being independent of the choice of machine learning libraries . While this leads to more ﬂexibility and a broader scope , it lim - its the functionality ( e . g . , no Graphics Processing Units ( GPU ) support , no prototype implementations , no visualizations ) and makes the tools heavy . DeepDIVA relies on a working Python environment and speciﬁc settings . This makes it lightweight and easy to cope with GPU hardware . DeepDIVA provides basic implementations for common tasks and has a wide - ranging support for visualizations . Additionally , it addresses the situation where an algorithm has non - deterministic behav - ior ( random initialization for example ) . II . R EPRODUCING E XPERIMENTS When reproducing experiments of others , the main issue is the code unavailability followed by the — sometimes prohibitive — amount of time necessary to make it work . Unlike other tools that offer reproducibility by trying to cope with heterogeneous scenarios , we provide an easy - to - use environment that any researcher can easily adopt . Our main paradigm for making experiments reproducible for anyone is an enforced version control . While it is a standard in modern software development for enabling tracing of the development progress , researchers often have the practice of { un } commenting lines of code and re - run the experiment again , making it almost impossible to match results to a code state . DeepDIVA uses two alternative ways to ensure that every single experiment can be reproduced . By default , the framework checks if all code is committed before running an experiment . Git 4 creates a unique identiﬁer for each commit and this is also the information that can be shared with others to reproduce the experiment . This way is the only way to create results seamlessly reproducible to other researchers . To ensure that the repository name , version , and parameters don’t get lost , all information is saved together in the results ﬁle . Note that always creating a version would be very cumbersome during development . Therefore , this behavior can be suppressed . In that case , DeepDIVA creates an actual copy of the code in the output folder at each run . In order to reproduce an experiment one needs to know the following three pieces of information : Git repository URL , Git commit identiﬁer , and the list of parameters to use . Reproducing an experiment is easily performed by cloning the Git repository 5 at the speciﬁc commit identiﬁer and run the entry point of the framework with the list of arguments as command line parameters . 4 https : / / git - scm . com / 5 For details on how to setup the environment see Section III - A or the README ﬁle on DeepDIVA repository directly . 3 Even when all code and parameters are provided , certain pitfalls exist that need to be taken care of . The most common is seeding pseudo - random generators . This is a nontrivial issue when working with both CPU and / or multiple GPUs . In our framework , if the seed is not speciﬁed by the user , we select a random value , log it , then use it for seeding Python , NumPy , and PyTorch pseudo - random generators . However , this is not enough for ensuring the exact same values for each run due to the non - deterministic nature of the optimized NVIDIA Compute Uniﬁed Device Architecture ( CUDA ) Deep Neural Network library ( CuDNN ) kernels that PyTorch uses , therefore we allow the option to disable CuDNN for increased reproducibility . 6 . III . F EATURES Apart from reproducibility , DeepDIVA has additional fea - tures helping researchers in several common scenarios . This section brieﬂy describes the most important features and how they can be beneﬁcial . A . Deep - learning Out - of - the - Box One of the greatest strength of the framework is its simple set up . All one needs to do is clone the repository from Github and to execute the setup script . This script sets up a local Conda 7 environment with all the required packages . For GPU compatibility , a system with the appropriate GPU drivers must be used for the set - up . At the moment of writing , DeepDIVA features implementa - tions of boilerplate for three common use cases : image classi - ﬁcation , image - patch matching and various scenarios with bi - dimensional data . As different researchers have different needs , hence the framework is designed to be fully customizable with minimum effort . In practice , this is achieved by having a high modularity between the different components such as data preparation , model deﬁnition , train , validation and test routines . This is particularly useful in deep - learning , where it is quite often the case that implementing the boilerplate constitutes the bulk part of programming workload . For example , one only needs to swap out a default component and replace it with their own implementation of it , i . e . , a new model architecture or a new training routine . B . Automatic Hyper - parameter Optimization Nowadays virtually all deep - learning experiments require a certain amount of hyper - parameters optimization . This is a tedious and time - consuming procedure which often does not require a real interaction from the researcher . A lot of research has been done to ﬁnd an efﬁcient way to optimize parameters other than random or grid search . Therefore , we integrate Bayesian hyper - parameter optimization into the framework 6 Enabling torch . backends . cudnn . deterministic showed to be not working correctly in our tests 7 Conda is an open source package management system and environment management system that runs on Windows , macOS , and Linux . See https : / / conda . io / docs / Fig . 3 . Example of hyper - parameter optimization process with SigOpt . In DeepDIVA this happens in a completely automated fashion where a list of input parameters is optimized thank to an iterative exchange of optimal conﬁgurations suggested by SigOpt tool . using SigOpt [ 7 ] . Instead of exhaustively testing every option , SigOpt adaptively suggests experimental conﬁgurations ( see Fig . 3 ) such to maximize / minimize your chosen metrics , e . g . , accuracy , loss . C . Data Visualization One of the most common recurring tasks of a researcher is visualizing the data produced by their experiments . Visualiza - tions are very helpful not only for understanding , debugging , and optimizing programs but also to make it easier for a human to make sense out of a huge amount of numbers , e . g . , by showing trends and behavior of a neural network . There are several tools that can produce such visualizations , but often these tools are either hard to integrate or cover only a speciﬁc kind of visualization ( e . g . , only plots ) and therefore one must resort using multiple ones . In DeepDIVA , we integrate Tensorboard ( see Fig . 4 ) as it natively supports a wide range of visualizations and it provides easy means to display data produced by any other source . This way all the visualizations produced by the framework are aggregated into a single web interface where they can be viewed with real - time updates . This is particularly helpful when the run time of a program is very long - having early results can save a lot of time when optimizing . D . Comparing Methods Comparing the performance of two or more algorithms — or multiple runs of the same one with different conﬁgurations – is another task that a researcher often has to deal with . Tensor - board offers a solution to compare two or more executions ( see Fig . 1a ) as the plots are generated dynamically when selected in the web interface . In order to visualize and understand the inﬂuence of randomness on an experiment , DeepDIVA has a multi - run ﬂag which automatically runs the same experiment a speciﬁed number of times and the aggregates the result into a plot ( see Fig . 1b ) . Finally , a common thing to do while evaluating ( e . g . , a classiﬁer ) , is to produce the confusion matrix . DeepDIVA automatically generates one ( see Fig . 1c ) and it is visible in Tensorboard as soon as it is created . 4 Fig . 4 . Example of the Tensorboard web interface . Figure from TensorFlow TM E . Internal Inspection When analyzing neural networks looking at the network from outside is often not enough . A wide array of tools and technologies have been developed to allow the inspection of neural networks as they run or to visualize their internal status . DeepDIVA offers the possibility to visualize how the histograms of the weight distributions for each layer evolve over time . This is useful to spot anomalies , especially when working with various neural network initializations . We plan to offer more of this kind of insights in the future . F . Working on 2D Data A common practice when developing a new instrument is starting on a controlled toy - problem and then gradually transition into more complex scenarios . This is sometimes challenging to achieve in deep - learning due to the high dimen - sional nature of the data . To help in this regard , DeepDIVA offers a whole suite of tools that allows one to train , evaluate and visualize performance on different types of bi - dimensional data . The advantage of using 2D points is that the visualization of the network conforms to the actual data . An example of such a visualization is given in Fig . 2 where the behavior of a network learning a continuous XOR problem is shown . It is not always possible to use such a simple setting as toy - problem , but we believe this is particularly useful when making fundamental research on new technologies rather than trying to push the boundaries of a given model / architecture . To allow full customization of these toy - problems there is a script in DeepDIVA which allows a user to create their own custom 2D datasets . G . New Dataset Support When working on a new dataset it can be the case that a set of operations needs to be performed before a deep - learning model can perform on it . One of these operations is creating a custom dataset class and custom dataloaders 8 . In DeepDIVA it is only required to provide the path to the folder where the dataset is located on the ﬁle system and the framework will take care of the modiﬁcations such as re - sizing the input to the ﬁt the network speciﬁcations 9 . Current limitations are that the datasets must be of images ( no audio , video or graphs yet ) and must be organized in a common standard folder format . Another of these preparation tasks for with DeepDIVA offers automated support is computing the mean and standard deviation of the dataset . This is widely done in deep - learning to normalize the dataset before training a model and can be non - trivial if the datasets are larger than the memory of the system - which is often the case when working on historical documents . We additionally provide a script for partitioning the training data into train and validation subsets . H . DeepDIVA as a Web Service We provide access to DeepDIVA through a Web Service on D IVA Services [ 8 ] . This Web Service is able to reproduce results as described in Section II , thus allowing everyone to replicate experiments without the need for any local installations . To facilitate this , we built a Docker image , that contains a Deep - DIVA base installation as well as a script that executes the necessary steps to perform the replication of an experiment . This Docker image can be used by others and is hosted on Docker Hub 10 . We also provide a bash script that can be used to replicate the second case study described in Section IV . This script is available online 11 . For the future , we want to provide the full set of features of DeepDIVA through this Web Service . IV . C ASE S TUDIES FOR H ANDWRITTEN D OCUMENTS In this Section , we present two use cases to demonstrate the usefulness of the framework for handwriting related tasks . A . ResNet for Watermark Recognition A common scenario in many computer vision tasks is classiﬁcation of images . In the context of handwritten historical document image analysis , tasks such writer identiﬁcation , image / line segmentation , script recognition can be treated as image classiﬁcation tasks . As a use case , we demonstrate how to perform watermark recognition on a dataset provided by the watermark database Wasserzeichen Informationssystem 12 . The dataset contains im - ages created with different methods of reproduction , such as hand tracing , rubbing , radiography and thermography ( see Fig . 5 ) . We tackle this classiﬁcation problem using a Convo - lutional Neural Network ( CNN ) . 8 This naming is speciﬁc to PyTorch but the concept generalizes to other frameworks as well . 9 This is an annoying problem e . g . when using a CNN on MNIST and then switching to CIFAR . 10 see : https : / / hub . docker . com / u / divaservices / , this URL is a placeholder and will link to the correct Image in the Camera Ready version 11 see : https : / / github . com / DIVA - DIA / DIVAgain / tree / master / ICFHR2018 DeepDIVA WriterIdentiﬁcation 12 https : / / www . wasserzeichen - online . de / wzis / struktur . php 5 Support for classiﬁcation tasks is provided in DeepDIVA as a basic template , therefore , the only pre - processing necessary for the dataset is to arrange the dataset in a commonly used folder format . This , however , needs to be performed on a per - dataset basis , as the source format for each dataset can differ signiﬁcantly . Once the dataset has been transformed into the speciﬁed format , you can begin training your desired neural network . To train your network , the only required arguments are the dataset path , experiment name and model . Various other parameters such as optimizer , learning rate , number of training epochs , batch size and many others can be speciﬁed if required . Several models are provided in a model directory inside of DeepDIVA , and it is easy to add a required model to our framework ( copy the deﬁned model source code into the model directory and add the new model as an import in the init . py ﬁle in the directory ) . The data does not need to be resized to the dimensions required by the dataset , as data transformations are built - in . DeepDIVA is fully compatible with multi - GPU support and also supports multiple workers for loading and pre - processing data . For this particular task , we train an 18 - layer Residual CNN [ 20 ] pre - trained on the ImageNet dataset ( ImageNet pre - training has been shown to be useful for several document image tasks [ 21 ] . We train our network using the Stochastic Gradient Descent optimizer with a learning rate of 0 . 01 for 100 epochs . To compare the effect of the pre - training , we initialize another instance of the same model with random weights for the neural network . Both networks are seeded with the same random value and use the same training protocol to prevent any variations in the order in which the data is shown to the network . A comparison of their classiﬁcation accuracy on the validation set can be seen in Fig . 1a and the embeddings produced by the ﬁnal layer are visualized in Fig . 1d . The ﬁnal performance of the pre - trained network on the test set is 96 . 4 % accuracy . Running DeepDIVA for a classiﬁcation task can be done with this command : $ python template / RunMe . py - - dataset - folder / dataset / watermarks _ wzis / - - model - name resnet18 - - experiment - name watermark _ classification - - epochs 100 - - seed 42 - - lr 0 . 01 Note that more details on these experiments and how to tackle the cross - depiction problem can be found explained in detail in the original work [ 22 ] . B . Triplet Networks for Writer Identiﬁcation Here we identify the authorship of a document , based only on the document images . We use the dataset provided by the ICDAR2017 Competition on Historical Document Writer Identiﬁcation [ 23 ] This is a particularly challenging task , as the train and test sets do not have the same writers . To accomplish this style of identiﬁcation , we train a CNN using the triplet margin loss metric [ 24 ] to embed document images in a latent ( a ) ( b ) Fig . 5 . Sample from the watermarks dataset ( a ) juxtaposed with a correct ( b ) classiﬁcation results from the network . Notice how the way they are depicted is signiﬁcantly different . space such that documents belonging to the same writer are close to each other . As this is not a standard classiﬁcation task , we make some modiﬁcations to the framework . However , since DeepDIVA is designed to facilitate easy modiﬁcations , it is fairly easy to add in your own code and still beneﬁt from all its features . To incorporate the triplet network into DeepDIVA , we create a new template that inherits from the standard classiﬁcation template . This allows us to only override methods that require changes , e . g . , the training method requires a triplet of input and uses the triplet margin loss 13 . Similarly to the previous task , we use an ImageNet pre - trained CNN for this task . Inputs for the network consist of cropped sub - images from the document images . Three such images are input into the network and then the triplet loss is computed based on the distance between the embed - ding vectors in the latent space . We train this network for 10 epochs . Since in this dataset there are only 2 positive matches for each query image we measured Precision @ 1 and Precision @ 2 achieving scores 0 . 61 , 0 . 69 respec - tively . An example query and returned results for our trained system [ 25 ] can be seen in Fig . 6 . V . C ONCLUSION AND F UTURE W ORK We introduce DeepDIVA : an open - source Python deep - learning framework designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality . We demonstrate that it increases repro - ducibility and supplies an easy way to share research among peers . Moreover , we show how researchers can beneﬁt from its features in their daily life experiments thus saving time while focusing on the analysis . In the near future we will include in DeepDIVA soon will include the possibility to initialize a network with advanced techniques , such as Principal Component Analysis [ 26 ] or Linear Discriminant Analysis [ 27 ] . Additionally , we plan adding more visualization tools , such as activation maps of intermediate layers , heat - maps , and loss landscapes [ 28 ] . 13 Additional changes can be seen in an example of the code available at https : / / github . com / DIVA - DIA / DIVAgain / tree / master / ICFHR2018 DeepDIVA WriterIdentiﬁcation 6 ( a ) Query image ( b ) Result One ( c ) Result Two ( d ) Result Three Fig . 6 . For the query image ( a ) , results ( b ) and ( c ) belong to the same writer and ( d ) belongs to a different writer . A CKNOWLEDGMENT The work presented in this paper has been partially sup - ported by the HisDoc III project funded by the Swiss National Science Foundation with the grant number 205120 169618 . R EFERENCES [ 1 ] L . K . John , G . Loewenstein , and D . Prelec , “Measuring the prevalence of questionable research practices with incentives for truth telling , ” Psychological science , vol . 23 , no . 5 , pp . 524 – 532 , 2012 . [ 2 ] C . G . Begley , “Reproducibility : six red ﬂags for suspect work , ” Nature , vol . 497 , no . 7450 , p . 433 , 2013 . [ 3 ] C . F . Camerer , A . Dreber , E . Forsell , T . - H . Ho , J . Huber , M . Johannes - son , M . Kirchler , J . Almenberg , A . Altmejd , T . Chan et al . , “Evaluating replicability of laboratory experiments in economics , ” Science , vol . 351 , no . 6280 , pp . 1433 – 1436 , 2016 . [ 4 ] M . Hutson , “Artiﬁcial intelligence faces reproducibility crisis , ” 2018 . [ 5 ] B . K . Olorisade , P . Brereton , and P . Andras , “Reproducibility in machine learning - based studies : An example of text mining , ” 2017 . [ 6 ] J . Pineau . ( 2018 ) Reproducibility Challenge at the International Conference on Learning Representations . [ Online ] . Available : http : / / www . cs . mcgill . ca / ∼ jpineau / ICLR2018 - ReproducibilityChallenge . html [ 7 ] I . SigOpt , “Sigopt reference manual , ” 2014 . [ Online ] . Available : http : / / www . sigopt . com [ 8 ] M . W¨ursch , M . Liwicki , and R . Ingold , “Web Services in Document Image Analysis - Recent Developments and the Importance of Building an Ecosystem , ” in 13th IAPR Workshop on Document Analysis Systems , 2018 . [ 9 ] Neptune . ( 2017 ) Neptune machine learning lab . [ Online ] . Available : https : / / neptune . ml / [ 10 ] Comet . ( 2017 ) Comet supercharge machine learning . [ Online ] . Available : https : / / www . comet . ml / [ 11 ] CodaLab . ( 2014 ) CodaLab a collaborative platform for reproducible research . [ Online ] . Available : http : / / codalab . org / [ 12 ] D . V . Control . ( 2017 ) Data Version Control git extension for data scientists manage your code and data together . [ Online ] . Available : https : / / dataversioncontrol . com / [ 13 ] Polyaxon . ( 2018 ) Polyaxon an open source platform for reproducible machine learning at scale . [ Online ] . Available : https : / / polyaxon . com / [ 14 ] J . Vanschoren , J . N . van Rijn , B . Bischl , and L . Torgo , “Openml : Net - worked science in machine learning , ” SIGKDD Explorations , vol . 15 , no . 2 , pp . 49 – 60 , 2013 . [ 15 ] A . Davison , “Automated capture of experiment context for easier reproducibility in computational research , ” Computing in Science & Engineering , vol . 14 , no . 4 , pp . 48 – 56 , 2012 . [ 16 ] K . Greff , A . Klein , M . Chovanec , F . Hutter , and J . Schmidhuber , “The sacred infrastructure for computational research , ” Proceedings of the Python in Science Conferences - SciPy Conferences , 2017 . [ 17 ] B . Howe , “Cde : A tool for creating portable experimental software packages , ” Computing in Science & Engineering , vol . 14 , no . 4 , pp . 32 – 35 , 2012 . [ 18 ] K . Arulkumaran , “Fglab : Machine learning dashboard , ” Faculty of Engineering , 2016 . [ 19 ] F . Chirigati , R . Rampin , D . Shasha , and J . Freire , “Reprozip : Com - putational reproducibility with ease , ” in Proceedings of the 2016 In - ternational Conference on Management of Data . ACM , 2016 , pp . 2085 – 2088 . [ 20 ] K . He , X . Zhang , S . Ren , and J . Sun , “Deep residual learning for image recognition , ” in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016 , pp . 770 – 778 . [ 21 ] M . Z . Afzal , S . Capobianco , M . I . Malik , S . Marinai , T . M . Breuel , A . Dengel , and M . Liwicki , “Deepdocclassiﬁer : Document classiﬁcation with deep convolutional neural network , ” in Document Analysis and Recognition ( ICDAR ) , 2015 13th International Conference on . IEEE , 2015 , pp . 1111 – 1115 . [ 22 ] V . Pondenkandath , M . Alberti , N . Eichenberger , and M . Liwicki , “Identifying cross - depicted historical motifs , ” Submitted at ICFHR2018 , 2017 . [ 23 ] S . Fiel , F . Kleber , M . Diem , V . Christlein , G . Louloudis , S . Nikos , and B . Gatos , “ICDAR2017 Competition on Historical Document Writer Identiﬁcation ( Historical - WI ) , ” in 2017 14th IAPR International Conference on Document Analysis and Recognition ( ICDAR ) . IEEE , nov 2017 , pp . 1377 – 1382 . [ 24 ] J . Wang , Y . Song , T . Leung , C . Rosenberg , J . Wang , J . Philbin , B . Chen , and Y . Wu , “Learning ﬁne - grained image similarity with deep ranking , ” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2014 , pp . 1386 – 1393 . [ 25 ] V . Pondenkandath , M . Seuret , R . Ingold , M . Z . Afzal , and M . Liwicki , “Exploiting state - of - the - art deep learning methods for document image analysis , ” in Document Analysis and Recognition ( ICDAR ) , 2017 14th IAPR International Conference on , vol . 5 . IEEE , 2017 , pp . 30 – 35 . [ 26 ] M . Seuret , M . Alberti , R . Ingold , and M . Liwicki , “Pca - initialized deep neural networks applied to document image analysis , ” Interna - tional Conference on Document Analysis and Recognition , 2017 , vol . abs / 1702 . 00177 , 2017 . [ 27 ] M . Alberti , M . Seuret , V . Pondenkandath , R . Ingold , and M . Liwicki , “Historical document image segmentation with lda - initialized deep neural networks , ” International Conference on Document Analysis and Recognition , International Workshop on Historical Document Imaging and Processing , 2017 , 2017 . [ 28 ] H . Li , Z . Xu , G . Taylor , C . Studer , and T . Goldstein , “Visualizing the loss landscape of neural nets , ” 2017 .