‘One Style Does Not Regulate All’ : Moderation Practices in Public and Private WhatsApp Groups FARHANA SHAHID , Cornell University , United States DHRUV AGARWAL , Cornell University , United States ADITYA VASHISTHA , Cornell University , United States WhatsApp is the largest social media platform in the Global South and is a virulent force in global misinfor - mation and political propaganda . Due to end - to - end encryption WhatsApp can barely review any content and this often pushes the responsibility of moderation towards group admins . Yet , little is known about how WhatsApp group admins manage their groups , what factors and values influence moderation decisions , and what challenges they face in moderating their groups . To fill this gap , we interviewed admins of 32 diverse groups and reviewed content from 30 public groups in India and Bangladesh . We observed notable differences in the formation , members’ behavior , and moderation of public versus private groups , as well as in how WhatsApp admins operate compared to those on other platforms . We used Baumrind’s typology of ‘parenting styles’ as a lens to explore moderation practices in WhatsApp groups and identified four moderation styles based on how responsive and controlling the admins were and discuss design recommendations to help them better manage problematic content in WhatsApp groups . CCS Concepts : • Human - centered computing → Empirical studies in HCI . Additional Key Words and Phrases : WhatsApp , content moderation , encryption , Global South ACM Reference Format : Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha . 2024 . ‘One Style Does Not Regulate All’ : Moderation Practices in Public and Private WhatsApp Groups . 1 , 1 ( January 2024 ) , 29 pages . https : / / doi . org / 10 . 1145 / nnnnnnn . nnnnnnn 1 INTRODUCTION WhatsApp is the most popular instant messaging platform with the third largest active userbase , majority of whom are from the Global South [ 12 , 57 ] . WhatsApp promotes its ‘groups’ as an effortless way to bring together one’s closest people . Indeed , most private WhatsApp groups consist of family , friends , or neighbors , while public groups usually involve strangers [ 18 , 36 ] . Although all groups start private , when the admin shares the invite link online for anyone to join , the group is considered public . As end - to - end encryption limits WhatsApp’s oversight on group activities , misinformation , propaganda , targeted polarization , and hate speech [ 22 , 26 , 31 , 39 , 59 ] run rampant , fueling riots , mob lynching , and communal violence in the Global South [ 3 , 48 ] . WhatsApp claims to use unencrypted user metadata , low - paid human moderators , and AI algorithms to detect abusive behavior and disable suspected accounts [ 25 ] . However , these measures are often sporadic and insufficient to prevent the spread of harmful content [ 16 ] . For example , given Authors’ addresses : Farhana Shahid , Cornell University , Ithaca , United States , fs468 @ cornell . edu ; Dhruv Agarwal , Cornell University , Ithaca , United States , da399 @ cornell . edu ; Aditya Vashistha , Cornell University , Ithaca , United States , adityav @ cornell . edu . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2024 Association for Computing Machinery . XXXX - XXXX / 2024 / 1 - ART $ 15 . 00 https : / / doi . org / 10 . 1145 / nnnnnnn . nnnnnnn , Vol . 1 , No . 1 , Article . Publication date : January 2024 . a r X i v : 2401 . 08091v1 [ c s . H C ] 16 J a n 2024 2 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha the end - to - end encryption on WhatsApp , moderators can only review messages when they are reported by users [ 21 ] . However , privacy concerns [ 102 ] and the reluctance to report misinformation in close - knit groups [ 27 , 99 ] practically limit the scope of content that paid moderators can review . In the absence of effective platform - mediated moderation , WhatsApp admins are forced to manage and regulate problematic content propagating in their groups [ 99 ] . A large body of HCI and CSCW scholarship has examined platform - mediated and voluntary moderation in Facebook groups , subreddits , and Twitch channels [ 41 , 92 , 94 , 97 , 107 ] . However , these spaces are substantially different from end - to - end encrypted WhatsApp groups , which remain outside the platform’s oversight and often connect people , who already know each other offline . Although WhatsApp stresses the “private” nature of its small close - knit groups , large public groups are often operationalized as “open forums” and serve to propagate disinformation and polarized content , which eventually find way to private groups [ 69 ] . Despite WhatsApp’s extensive userbase and admins’ key position , little is known about how public and private group admins address problematic content and what challenges they face in managing their groups . To fill this critical gap , we sought to answer the following research questions : RQ1 : How admins moderate public and private WhatsApp groups ? RQ2 : How can WhatsApp improve moderation in public and private groups ? To address these questions , we observed user activities in 30 public WhatsApp groups and interviewed 32 admins from various public and private groups in India and Bangladesh . We focus on these countries mainly because : ( 1 ) India has the largest number of WhatsApp users and one - quarter of Bangladesh’s population uses WhatsApp [ 85 ] , and ( 2 ) both countries are victims of communal propaganda spread via WhatsApp [ 3 , 20 ] . In response to RQ1 , we draw from prior work that used “parenting” as a metaphor to describe the dynamics between moderators and offenders [ 35 , 80 , 92 ] , considering the presence of offline ties among members in many WhatsApp groups . We build upon Baumrind’s typology of “parenting styles” [ 8 ] to examine how admins in public and private WhatsApp groups practice care and control and identify four moderation styles : authoritative , authoritarian , permissive , and uninvolved . We observed authoritative approach mostly in private groups with weaker social ties ( e . g . , educational , professional , and organizational groups ) . Authoritative admins upheld group’s well - being , defined group rules , debunked misinformation , removed problematic content , and often banned the offenders . Unlike moderators on other platforms , authoritative admins were transparent about moderation considering the offline social ties they often shared with the offenders . Whereas , admins in private groups with strong social ties were permissive , i . e . , they ignored problematic content shared by family and friends to not jeopardize their personal relations . Thus , offline relations shaped who can or cannot be moderated in close - knit groups , which is unlikely on other platforms . On the contrary , lack of social ties in public groups allowed admins either to fully neglect their groups or enforce harsh measures . Authoritarian admins disabled messaging in public group chat to prevent problematic content without caring for the communication needs of innocent group members . Whereas , uninvolved admins abandoned public groups and did not bother about the harmful content that accumulated in their groups in the absence of moderation . We draw from our findings to answer RQ2 and stress the need to go beyond “one size fits all” moderation tools , considering the strengths and weaknesses of different moderation styles grounded within Baumrind’s typology . We recommend designs to empower admins while maintaining accountability and striking a balance in their power . This involves providing suitable avenues for authoritative and permissive admins to confront the offenders , resources to educate group members against impulsive sharing of harmful content , and introducing measures that push authoritarian , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 3 and uninvolved admins to behave responsibly . Overall , our work makes the following contributions to HCI and CSCW scholarship : • A qualitative study built on Baumrind’s typology of parenting styles revealing different moder - ation styles in public and private WhatsApp groups compared to those on other platforms . • A range of design recommendations to improve moderation in end - to - end encrypted platforms . 2 RELATED WORK Social media platforms use different titles for users , who manage online communities , e . g . , admin in WhatsApp groups , moderator in subreddits and Twitch channels , while Facebook groups have both . Irrespective of the title , admins and moderators play an important role in keeping the online community safe . We situate our work first by describing prior work in HCI and CSCW that extensively studies the roles of admins and moderators on Facebook , Reddit , and Twitch . We then discuss the current moderation practices on WhatsApp . Finally , we describe Baumrind’s work on ‘parenting styles’ [ 8 ] that we use as a lens to examine admins’ moderation in WhatsApp groups . 2 . 1 Admins and Moderators in Online Communities Admins and moderators on Facebook , Reddit , and Twitch often take pride in their position and feel a strong sense of community ownership [ 62 ] . Usually , moderators are selected from users , who are either active community members , connected to existing moderators , or have prior experiences of moderation [ 94 ] . In many Facebook groups , admins are elected by voting among interested users [ 97 ] . Many subreddits also have formal application and interview process to appoint moderators [ 64 ] . Admins and moderators mainly manage user reports , delete problematic content , mute or ban offenders , and resolve appeals [ 94 , 97 , 107 ] . They use different auto - moderation tools and platform provided features to prevent violations by preapproving posts or turning off post comments [ 94 , 97 ] . Many Reddit and Twitch moderators use open - source bots to filter and flag posts that contain specific keywords [ 94 ] . They often discuss moderation decisions with fellow admins and moderators , and distribute the workload among them [ 10 , 11 , 97 ] . However , they rarely engage with community members , answer their questions , or participate in conversations to avoid influencing group discussions with their positions of power [ 62 , 107 ] . Moderators often lack transparency and accountability to users [ 50 , 62 ] . They enforce hidden rules and often remove content without any notification or explanation to prevent exploitation of community rules and dispute with the offenders [ 41 , 94 , 107 ] . Offenders usually respond to moderators’ actions either by ignoring , improving their behavior , asking for clarification , or showing aggression [ 64 , 94 ] . Due to the lack of training and onboarding processes , most moderators learn through ‘trial - and - error’ , i . e . , they usually make rules or update existing ones following unexpected misconducts [ 94 , 107 ] . In some communities ( e . g . , Reddit ) moderators engage in reflective practices to update their mental models of community standards and moderation practices [ 17 ] . Admins and moderators also rely on common sense , local laws , advice from other moderators , and platform - provided resources or help articles to learn about moderation [ 91 , 97 ] . Although these studies detail the roles of admins and moderators on Facebook , Reddit , and Twitch , the findings may not translate to end - to - end encrypted platforms like WhatsApp , where most of the user generated content remain outside the platform’s oversight . Moreover , unlike other platforms , users in most WhatsApp groups share strong offline ties [ 36 ] . Despite having a large userbase , content moderation on WhatsApp remains relatively understudied . There is a scarcity of research on how WhatsApp admins manage their groups . Our work addresses this gap by investigating moderation practices in different types of public and private WhatsApp groups . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 4 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha 2 . 2 Content Moderation on WhatsApp Although WhatsApp is one of the main sources of harmful content propagating online [ 28 , 86 , 99 ] , end - to - end encryption makes it difficult for WhatsApp to review , moderate , and remove problematic content unless it is reported by users . Nevertheless , WhatsApp uses unencrypted account metadata ( e . g . , profile and group photos , account description ) and in - app user reports to flag and ban abusive accounts [ 25 , 36 ] . For example , WhatsApp banned more than 3 million user accounts in India in July 2023 [ 104 ] . However , such measures often fail to prevent problematic content on WhatsApp . For example , when abusive messages contain sensitive personal information or users fear that their personal data might be shared with moderators , they feel reluctant to report [ 102 ] . Although there are some tiplines , i . e . , WhatsApp accounts of fact - checking organizations so that users can request fact - checks , these services remain underutilized because fact - checking takes time and most users find it exhausting to fact - check the bulk of messages they receive on WhatsApp [ 45 , 47 , 82 ] . In the same vein , privacy preserving measures , for example , storing hashes of fact - checked misinformation on user devices , is ineffective in handling new abusive content that does not match with existing patterns [ 44 , 81 , 82 ] . Moreover , limiting the number of forwards and labeling which messages are ‘forwarded many times’ barely stop the propagation of harmful content [ 19 ] . As these technological fixes fail , there has been an increased focus on user - driven social correction . Many studies examine how users’ demographics impact their tendency to correct misinformation and the results vary across different geographies . For example , male users tend to correct their peers more in WhatsApp groups in Brazil [ 101 ] and Singapore [ 53 ] , whereas in Turkey , females are more likely to correct others [ 53 ] . In Brazil , older group members are more likely to confront misinformation [ 101 ] , whereas in the US and Singapore , younger people are more likely to do so [ 53 ] . Moreover , highly educated users in Brazil participate in correcting misinformation more than others [ 101 ] . Group dynamics and group ties also shape social correction in WhatsApp groups . A cross - cultural study in the US , Singapore , and Turkey reveal that users who have greater trust in group members are more likely to correct misinformation [ 53 ] . Pasquetto et al . [ 73 ] observed effect of strong group ties in India and Pakistan , where users are more likely to share debunking messages that come from their close friends , family , or politically like minded individuals . However , a study with WhatsApp users in the UK show that irrespective of group ties , setting up informal group rules can foster collective reflection and epistemic vigilance against misinformation [ 14 ] . Regardless of culture , community - driven corrections are considered stressful , particularly in tight - knit groups [ 72 ] . Scott et al . [ 90 ] studied family WhatsApp groups in the UK and found that family hierarchy , difficult personality and biased opinions of misinformed family members , and lack of support to confront them online make it burdensome to challenge misinformation . Several studies with WhatsApp users based in India [ 60 , 99 ] , Singapore [ 67 ] , and the US [ 28 ] show that young group members usually ignore misinformation shared by elderly relatives to avoid conflicts . As correcting elders transgress cultural norms in some cultures , young adults carefully consider the severity of misinformation , their expertise on the topic , rapport with the relative , and relative’s potential reactions before approaching them [ 60 ] . Moreover , they carefully choose the appropriate channel to politely correct relatives so that their act of correction does not reflect poorly on their family and does not offend the relatives [ 61 , 74 ] . Research shows that users are more likely to correct misinformation in groups with weak social ties [ 42 ] . Varanasi et al . [ 99 ] show that in neighborhood groups in India , knowledgeable community members and admins often correct COVID - related misinformation politely and indirectly . However , in work - related groups in India , higher management directly calls out the offender for sharing any religious or political propaganda , or irrelevant messages [ 100 ] . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 5 Emotionally detached , no boundaries , self - absorbed , little interaction Strict , inflexible , punitive , high expectations , demands blind obedience High R e s pon s i v e n e ss High Low Demandingness Authoritative Authoritarian Permissive Uninvolved Nurturing , affectionate , barely sets any rules , takes the role of friend rather than parent Nurturing , affectionate , sets boundaries , disciplines through guidance , open communication Fig . 1 . Four parenting styles according to Baumrind’s typology . Although these studies provide useful insights into social corrections on WhatsApp , they mostly examine groups with pre - existing social ties and do not inform much about admins’ roles in moderation . These studies also show that local sociocultural norms heavily shape how problematic content is confronted and handled , with variations across group members in different countries . We extend this scholarship by carefully investigating admins’ roles and their moderation strategies in different public and private groups in two countries in the Global South . 2 . 3 Parenting Styles as a Lens Several HCI scholars have examined the roles of moderators through the lenses of care and control . For example , volunteer moderators’ power have been associated with that of dictator , governor , police , and judge [ 32 , 64 , 92 , 107 ] . While others position moderators as “care worker , who keep the community safe” [ 87 , 110 ] or “custodian , who help the community grow” [ 92 ] . Moreover , few researchers applied different aspects of “parent - child” metaphor to capture how care and control manifest in the roles of moderators . Seering et al . [ 92 ] found that moderators often consider online misbehavior to be “childish” and try to resolve them through patient instructions . Gillespie [ 35 ] discussed how users often perceive community moderators’ guidelines as the rules of a “stern parent” [ 35 ] . Register et al . [ 80 ] used the concept of attachment between parent - child to characterize how punishments and rewards by Instagram’s moderation algorithm affect users [ 80 ] . Building on these scholarly work , we use Baumrind’s “parenting styles” [ 6 ] as a lens to examine how WhatsApp admins engage in moderation . Since WhatsApp admins often know the group members [ 36 ] and prioritize group harmony over moderation [ 67 ] , we draw parallels between parenting and admins’ roles in WhatsApp groups . Baumrind’s pioneering research examines the parent - child relationship through the lens of parental control and proposes a typology with three parenting styles ( e . g . , authoritative , authoritar - ian , and permissive parenting ) [ 4 – 6 ] . Later Maccoby and Martin [ 58 ] bridged Baumrind’s typology with the two dimensions of parenting : responsiveness and demandingness . Here , responsiveness refers to the degree to which parents are sensitive to their children’s emotional needs and accepting of their behavior [ 49 , 55 ] . This is associated with constructs , such as care , warmth , and support [ 78 ] . In contrast , demandingness refers to the degree to which parents control their children’s behavior and demand maturity [ 49 , 55 ] . Based on these two dimensions , Maccoby and Martin [ 58 ] defined , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 6 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha four parenting styles : authoritative ( i . e . , high demandingness and high responsiveness ) ; author - itarian ( i . e . , high demandingness and low responsiveness ) ; permissive ( i . e . , low demandingness and high responsiveness ) ; and uninvolved ( i . e . , low demandingness and low responsiveness ) . Later , Baumrind also expanded her typology to include the uninvolved parenting style [ 8 ] . According to Baumrind’s typology [ 8 ] ( see Figure 1 ) , authoritative parents are caring and clearly communicate their rules and disciplinary actions to children . In contrast , authoritarian parents are punitive , less nurturing , and expect children to obey their rules . On the other hand , permissive parents barely impose any rules and act more like friends rather than parents , whereas uninvolved parents remain detached from their children’s lives and do not assert any rules . Research shows that authoritative parenting helps children self - regulate and act responsibly [ 65 ] , whereas children with authoritarian parents often develop high levels of aggression against strict authority [ 63 ] . Besides , limited moderation by permissive parents often leads to impulsive attitude and lack of self - regulation among children [ 77 ] . Similarly , children whose parents remain uninvolved struggle to cope and control their emotions [ 52 ] . Although Baumrind’s typology is based on two - parent , middle class , and White Western families , it has been applied to different cultural contexts and demographics . Some studies show that the effects of parenting styles are consistent in both individualist and collectivist cultures [ 76 , 96 ] , including our regions of study : India [ 30 , 88 ] and Bangladesh [ 46 ] . The typology has also been useful in modeling complex human relationships around technology use that require both control and care . For example , Valcke et al . [ 98 ] first extended this to children’s Internet usage and introduced “Internet parenting styles . " Since then , the typology has been used to examine parent - child communication online [ 112 ] , parental control on children’s digital devices and social media usage [ 15 , 83 , 108 ] , and their impact on adolescents’ online behavior [ 33 , 34 , 40 , 106 ] along with parents’ online sharing practices about their kids [ 2 ] . Beyond parent - child , the typology has also been used to model how the teacher - student relationship influences students’ learning outcomes [ 109 ] . Our work extends this evolving body of work by applying Baumrind’s typology to explain different moderation styles practiced by WhatsApp group admins in India and Bangladesh and provide a holistic overview of the values and factors that drive moderation on end - to - end encrypted platforms . 3 METHODS To examine how WhatsApp group admins manage their groups and handle problematic content , we performed in - depth qualitative study with admins based in India and Bangladesh . We define “problematic content” as misinformation , hate speech , spam , or anything inconsistent with the group’s purpose . We conducted semi - structured interviews with private and public group admins to learn about their experiences of moderation . We also joined public groups to identify instances of problematic content and observe how admins respond in practice . Our study protocol received an exemption from the Institutional Review Board of our institution . Participant Recruitment . To recruit WhatsApp group admins , we advertised our study on Facebook , WhatsApp , Twitter , and requested our contacts to share the study within their social networks . We also took help from our contacts in a grassroots organization based in rural India to recruit admins from rural regions . Through this , we were able to recruit 23 admins from private WhatsApp groups , who managed at least one group in India or Bangladesh , and had encountered problematic content in the groups they managed . To recruit public group admins , we randomly joined 118 public groups on various topics ( e . g . , entertainment , politics , business ) in India and Bangladesh that were listed online ( e . g . , https : / / whatsgrouplink . com / ) . We then reached out to the admins of 30 public groups , where we observed group members frequently sharing obvious problematic content ( e . g . , child porn , monetary scams ) during the study period . Through this , , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 7 we were able to recruit nine public group admins . We coordinated with all our participants via WhatsApp , shared details of our study , and scheduled interviews with them . We continued to recruit admins until the responses reached theoretical saturation [ 71 ] . Interviews with Group Admins . We conducted semi - structured interviews with 32 group admins to learn about their experiences of managing their groups . The interviews were conducted in English , Bengali , or Hindi depending on the preferences of the participants . After taking informed consent , we asked them in detail about how they became admins in WhatsApp groups , what roles they played , the dynamics among the group members , and their group environment . We then asked them about the strategies they use to identify and manage problematic content and the resultant challenges . We also asked them about the support they expected from WhatsApp to better manage their groups . During the interview , some admins voluntarily shared messages from their groups that they found to be problematic . After each interview , we revised our questions if needed , stopping when the responses reached saturation . We conducted the interviews either in - person or online . Each interview lasted approximately 40 minutes and was audio - recorded with the consent of the participants . Depending on the geographic location of the participants , we compensated them with 500 INR in India , 500 BDT in Bangladesh , or 10 USD for expats living abroad . Observations of Public Groups . Among the public groups we joined , we observed the top 30 groups , where group members most recently sent messages during the study period . We recorded the group name , group description ( if any ) , number of admins and participants , who could message in the group ( either admin - only or all participants ) , what type of messages were sent in the group , and how admins reacted to problematic content shared in these groups . The analysis was done only for public groups and we did not seek permission to join private groups for privacy concerns . Data Collection and Analysis . We collected around 17 hours of audio - recorded interview data , several pages of notes from our interviews and observations , and 14 forwarded messages that admins perceived to be problematic . We translated and transcribed the interviews , notes , and WhatsApp messages into English before iteratively conducting open coding following reflexive thematic analysis as defined by Braun and Clarke [ 9 ] . Instead of relying on presupposed codes , we let the data guide us . All authors met regularly to discuss the codes that freely emerged from the data , did peer debriefing to resolve disagreements while reviewing and updating the codes , and finalized the emerging themes . Our prolonged first - hand engagement with the data helped us establish credibility and reduce coding biases . After multiple passes , we consolidated 59 codes and categorized them under high - level themes , such as the roles of admins , their responses to problematic content , and their expectations from WhatsApp . Participant Demographics . Table 1 lists the demographics of the 23 private group admins ( 13 Bangladeshi and 10 Indian ) we interviewed . Among them , 19 identified as male , four as female , and one as non - binary . On average , participants were 30 years old ( SD : 8 years ) , aged between 21 – 52 years . Most of the participants ( 𝑛 = 17 ) were from urban areas of Bangladesh and India . We also interviewed three admins from rural India and four Bangladeshi and Indian expats living in North America . All private group admins were highly educated ; majority of them ( 𝑛 = 12 ) had a master’s degree , six had a bachelor’s degree , and five were enrolled in a bachelor’s program . They pursued various professions , including software development , journalism , entrepreneurship , business , and jobs in government and private sectors , among others . On average , they had been using WhatsApp for almost 7 years ( SD : 2 . 5 years ) . All public group admins ( 𝑛 = 9 ) were men and based in India . They were reluctant to share demographic information out of concerns to protect their identity . Since they were comfortable interacting with us only informally , we did not collect demographic information from them . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 8 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Table 1 . Pseudonym , age , gender , profession , and country of the private WhatsApp group admins . Name Age Gender Profession Country Name Age Gender Profession Country HimadriRaviRudraMehrabDipaliJawadLabibMunimNazmaRafidShaheen Sabbir 2624304022235221222228 26 MaleMaleMaleMaleFemaleMaleMaleMaleFemaleMale MaleMale Software Developer PhD Student PhD Student Private Service StudentStudentEngineerStudentStudentStudentGovernment Service Engineer BangladeshIndiaBangladeshBangladeshBangladeshBangladeshBangladeshBangladeshBangladeshBangladesh IndiaBangladesh TawhidMansiRizwanGautamHamidArvindPranjalPriyaKunalRakesh Tulsi 39264224282726254046 52 MaleFemaleMaleMaleMaleMaleMaleNon - binary MaleMale Female EntrepreneurScienceJournalistJournalistResearchAssistantResearcherPoliticalLeaderResearcherPrivateServiceNewsReporterBusinessOwner NGO Supervisor BangladeshIndiaBangladeshIndiaIndiaBangladeshIndiaIndiaIndiaIndia India Table 2 . Demographics of public and private WhatsApp groups in our sample . Group type Categories ( n ) Average number of participants Examples of groups Private Family ( 12 ) Friends ( 10 ) Educational ( 9 ) Professional ( 8 ) Organizational ( 5 ) Neighborhood ( 4 ) Hobbies ( 3 ) 3020130170110250120 Immediate family members and relatives Close friends School , college , or university students Colleagues , employees , engineers , job seekers Religious , political , non - profit , development organizations Local neighbors , communities , villagers Gamers , animal rescue volunteers , entomophiles Public Earning ( 9 ) Business ( 6 ) Services ( 6 ) Educational ( 6 ) Entertainment ( 3 ) 300280320150330 Vacancies , work from home , investments Business opportunity , product sales and promotion Immigration , travel , paid social media promotion Exam preparation , tuition , blogging , photography Sports betting , celebrity fans Positionality . One of the authors is from Bangladesh and two are from India . The authors who conducted the interviews are native Bengali and Hindi speakers . The authors have been using WhatsApp for more than a decade , are admins of several WhatsApp groups , and have first - hand experience of encountering problematic content in these groups . All authors have extensive ex - perience in conducting fieldwork in India and Bangladesh . Their sociocultural and educational backgrounds are on par with that of the participants . The shared background helped them under - stand the sociopolitical and cultural dynamics that shape these groups and elicit nuanced responses from the participants . All authors approach HCI from a postcolonial lens [ 38 ] and strive to uncover the cultural and sociopolitical epistemologies of the Majority World , whose users dominate online spaces like WhatsApp and shape the nature of governance on such platforms . 4 DEMOGRAPHICS OF WHATSAPP GROUPS Private Groups . The private group admins ( 𝑛 = 23 ) in our study managed three groups on average ( SD : 3 ) . Most of them ( 𝑛 = 10 − 12 ) were admins of either family or friends groups ( see Table 2 ) . Others managed groups related to their educational institution , workspace , neighborhood , or different organizations they were part of . Few participants ( 𝑛 = 3 ) also ran personal hobbies and interest - based groups . Private groups varied greatly in group size . Family and friends groups were usually small and close - knit with 20 – 30 members on average , whereas other types of groups were fairly large with 100 – 200 participants on average . Private groups were used for various purposes . Family and friends groups were mainly used to stay in touch and share personal updates and photos , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 9 with close ones . Educational groups were used for study - related discussion and to stay connected with classmates . Organizational and professional groups were used exclusively for work , including performing administrative tasks and monitoring employees’ activities . Political leaders , who were admins of their party’s dedicated WhatsApp group , orchestrated political campaigns via the group . Group members actively participated in these groups to share information , opinions , worldviews , and resources . For example , some group members shared information about upcoming events , government notices , and job opportunities . Group members also posted blood donation requests , admission - related questions , and requested advice on technological issues , among others . Regardless of the type , almost all private groups received messages on politics , religion , and issues of national and regional interest . Public Groups . We examined 30 public WhatsApp groups with various focus areas , ranging from business , earning opportunities , online services , education , and entertainment , among others ( see Table 2 ) . The nine admins we interviewed from these groups managed seven groups on average . Although most of these public groups were large with 300 participants on average ( SD : 205 ) , we barely observed any interaction among the group members . In most groups ( 𝑛 = 25 ) , group members and admins shared irrelevant and problematic messages that we discuss in detail in Section 5 . 2 . In one - third of the public groups we joined ( 𝑛 = 9 ) , admins disabled messaging among the group members and only shared messages that were pertinent to the group’s purpose , e . g . , product details in business groups , visa and immigration services in travel groups , and exam questions in educational groups . 5 FINDINGS We first outline the paths that led our participants to become admins in WhatsApp groups along with their roles as admins ( Section 5 . 1 ) . We then discuss admins’ perceptions of problematic content ( Section 5 . 2 ) and use Baumrind’s typology to categorize admins’ responses to such content and the aftermath of their actions ( Section 5 . 3 ) . Finally , we present admins’ expectations from WhatsApp to improve handling of problematic content in their groups ( Section 5 . 4 ) . 5 . 1 Who Gets to Become Admins in WhatsApp Groups and What Roles Do They play ? We found several differences in how admins were selected and what responsibilities they assumed . 5 . 1 . 1 Becoming an Admin . Majority of the participants ( 𝑛 = 19 ) in our sample automatically became admins through WhatsApp’s default setting when they created the group . In contrast , only a few participants volunteered to be admin ( 𝑛 = 4 ) or were appointed by existing admins ( 𝑛 = 3 ) . Dipali , an admin of a family WhatsApp group in Bangladesh , shared : “Although my aunt created the group , she became busy with household chores and kids and made me an admin instead . Recently one of my cousins volunteered to be a co - admin to handle the misleading content shared by some relatives . ” Different private groups have varying criteria to select admins . In family and friends groups , people became admins due to their existing relationship with the group members ( 𝑛 = 5 ) . In organizational and professional groups , people with administrative power or high - ranked positions offline ( e . g . , CEO , managers , supervisors ) became admins ( 𝑛 = 6 ) . In a few educational groups ( 𝑛 = 5 ) , students who stayed up - to - date with academic events , were socially popular , and demonstrated leadership skills were preferred as admins . Some participants , who were perceived to be tech - savvy ( 𝑛 = 4 ) or active group members ( 𝑛 = 7 ) , were also made admins . Munim , who was an admin of a neighborhood group in Bangladesh , shared : , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 10 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha “Earlier only the elders in our housing society could become admins . But , they were not tech - savvy and couldn’t understand all the features of WhatsApp . Then they recruited us because I always have Internet connectivity and check the group actively . ” Moreover , two groups followed elaborate processes to elect admins through voting or in - person interviews . Even in two friends groups , all group members were made admins so that none could hold power over others . Private groups in our sample often had multiple admins ( mean : 5 , SD : 3 . 5 ) , while the public groups we observed usually had one admin ( SD : 1 . 5 ) . Admins of private groups often appointed multiple admins so that new group members could be added easily . This was especially helpful in large private groups , where admins were often busy with their own work and preferred to have co - admins to manage the group . Sometimes , the decision to add new admins was strategic . Rakesh , who was an admin of a large religious group in rural India , commented : “I made others admin so that they could add their acquaintances to my group instead of forming new groups . This will help my group grow bigger and popular . ” However , admins of public groups and few private groups ( 𝑛 = 3 ) enjoyed having ownership and control of the group and felt reluctant to add new admins . They feared that the other admin might remove them from the group , seize the power , or add random people to the group . Moreover , in public groups , where members were usually strangers , admins considered it would be “unwise” to make unknown people co - admins . These findings show that apart from the default way of becoming an admin by creating a group , the selection of admins varies depending on the group’s needs , dynamics , social hierarchy , and individual’s skills . 5 . 1 . 2 The Roles of Admins . WhatsApp groups do not have any formal onboarding process and training for admins . Only a few private group admins ( 𝑛 = 4 ) reported receiving informal instruc - tions from other admins on how to manage their groups . Thus , in the absence of proper onboarding processes , group management was often left to the discretion of admins . While several public ( 𝑛 = 6 ) and few private group admins ( 𝑛 = 3 ) assumed “there was nothing to do” in the groups or “they were too busy” to manage the groups , most admins actively supervised their groups . Adding Group Members . Most private group admins ( 𝑛 = 14 ) added new members and updated the new phone numbers of existing group members . In contrast , public group admins did not have to bother about adding members as the invite links to join groups were posted publicly . Usually private group admins carefully regulated who could join their groups . For example , Dipali declined her aunt’s request to add a distant relative to their family WhatsApp group to maintain privacy . Another admin Kunal , who ran a professional group of journalists in rural India , shared his criteria : “We don’t add any unknown people to our group . We only add those who are affiliated with the press or are able to provide news materials . ” A few admins ( 𝑛 = 4 ) went through elaborate verification process before adding people to their groups . For instance , in large private groups with weak social ties ( e . g . , neighborhood , educational , and professional groups ) , admins often asked for relevant ID and other details to confirm identity . They also asked other group members if they knew the person who wanted to join the group and could vouch for them . Munim further explained the verification process in his neighborhood group : “When somebody wants to join the group we ask for their building and apartment numbers . We have a list of contact details for all residents in our housing society . We dial the corresponding apartment to verify if the person actually lives there . ” Instilling a Sense of Safety and Belongingness . Several private group admins ( 𝑛 = 8 ) felt protective of their groups and tried to create a safe and welcoming space online . For example , Tulsi , , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 11 who supervised a group of women health workers in rural India , warned women in her group against monetary frauds when such messages were shared in the group . In hobby and interst - based groups , admins often used words of affirmation to engage people in group activities . In addition , admins worked as mediators to resolve online conflicts among group members . Sometimes they handled conflicts offline , particularly when they knew the group members in person . One of the Bangladeshi admins Rizwan shared : “A Hindu colleague left our office’s WhatsApp group due to hate speech . When we noticed , we decided to apologize to him in person instead of contacting him online given the seriousness of the matter . After meeting , we requested him to rejoin the group . ” Addressing Offline Needs . As private group admins often knew the group members , in several groups ( 𝑛 = 7 ) admins assumed offline responsibilities as part of their role . For example , admins in neighborhood and college groups were often tasked with organizing community events as they could leverage their groups for coordination . They also maintained a record ( e . g . , contact details , professions , blood groups ) of all members in case anyone within the group needed help . In job opportunities related group , admins often helped group members by providing referrals upon request even if they did not always know them personally . Distribution of Workload . In private groups , admins usually knew fellow co - admins , often met them in person , and were on good terms . Several private group admins ( 𝑛 = 8 ) did not bother to differentiate responsibilities among the co - admins because “there was not much to do” and anyone available managed the group . Some private group admins ( 𝑛 = 6 ) maintained separate WhatsApp or Messenger chats among themselves to privately discuss the group’s logistics . They generally discussed whether something was inappropriate , what actions to take , how to deal with difficult group members , and how to ensure uniformity in their actions . However , the power differential among co - admins dictated these conversations and assumed duties . For example , younger admins in family WhatsApp groups were expected to check with older co - admins before taking any action . Such power hierarchy often led to friction among the admins . For example , Ravi shared an incident from his family WhatsApp group in India : “Previous admins had fights because the elderly admin argued that non - blood relative should not send too many messages . But , the younger admin disagreed and was forced to leave the group . ” In a few organizational groups ( 𝑛 = 4 ) , admins distributed tasks based on their expertise and official position within the organization . For example , as the treasurer of a Bangladeshi graduate student organization , Rudra’s task was to post about funding , budget , and membership fees in their organization’s WhatsApp group . Tulsi further commented about gendered division of roles : “Since all members in our group are women healthcare workers and the other co - admins are men , it’s appropriate that I [ a female admin ] deal with the group affairs . ” 5 . 2 What Types of Problematic Content Affect WhatsApp Groups ? We now present admins’ account of what they perceived as problematic content along with how group members responded to them . 5 . 2 . 1 Admins’ Perceptions of Problematic Content . Admins had diverse opinions regarding what they thought to be problematic ; ranging from obviously harmful messages to often harmless but irrelevant content . Religious Hate Speech and Propaganda . One - third of the private group admins ( 𝑛 = 8 ) reported religious hate speech and propaganda as one of the most common and vicious forms of problematic content they encounter in their groups . Admins in Hindu - dominated groups in India recounted , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 12 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha various instances of pro - Hindu and Islamophobic hate speech propagating in their groups that they suspected to be originating from right - wing political groups . Gautam , who managed a group of students in his college in India , reported : “In our group , people share religiously charged posts that would blame the Muslims for the 2019 Delhi riot , the 2020 COVID pandemic , the 2023 Odisha train collision , or almost anything that might go wrong in this country . ” In contrast , Muslim - dominated WhatsApp groups in India received many far - right , pro - Muslim content . Admins mentioned seeing old videos of riots and communal violence ( e . g . , Hindus burning mosques ) being propagated as recent events , conspiracy theories blaming the leftist state govern - ment for discriminating against Muslims , or issuing COVID vaccines with pig remnants to hurt the religious sentiment of the Muslims . We noted similar dynamics in WhatsApp groups based in Bangladesh , where Hindus are the religious minority . Arvind commented : “Every year during Durga Puja [ Bengali Hindu religious festival ] there are posts with anti - Hindu sentiment , that would blame the Hindus for disrespecting the Quran , the Prophet , or Muslim women to justify violence against them . ” Similarly , admins reported religious extremism in small - scale pro - Hindu groups in Bangladesh . Political Propaganda . More than half of the admins in private groups ( 𝑛 = 13 ) considered political messages to be problematic as they often led to fights among group members . Admins routinely dealt with pro - political content shared in favor of the ruling party , misinformation and propaganda against the opposition political parties and foreign countries , and conspiracy theories blaming the government for wrongdoings and corruption , among others . Some admins feared that the government might track anti - government messages on WhatsApp and accuse them of spewing distrust . Rizwan shared his thoughts : “People think writing on WhatsApp is safe . I doubt if WhatsApp’s encryption would work in Bangladesh given the country’s strict digital law against anti - government content . The government might trace such messages on WhatsApp and accuse admins . ” Misinformation , Fake News , and Spam . Half of the private group admins ( 𝑛 = 12 ) reported misinformation and fake news to be pervasive in their groups . They gave examples of COVID - related misinformation that denied the impact of COVID , discouraged people from taking vaccines , and promoted fake cures . Admins also encountered AI - generated fake videos , viral clickbait content from other social media platforms , and old pictures and videos being circulated as current news . They complained about spam messages promising lotteries and shopping discounts as these derailed the conversation in the group and compromised the security of group members . Dipali shared an experience from her college WhatsApp group : “After clicking on a spam link shared in the group , many group members’ Facebook accounts got hacked . Some girls’ sensitive photos were leaked and when we informed our teachers , they filed a cybercrime police complaint . The police interrogated everyone , recovered the hacked accounts , and asked the admins to disable the group . ” Although we observed a large presence of monetary scams , fraudulent cryptocurrency schemes , illegal betting services , and sales of illicit arms and hacked social media accounts in public groups , few admins ( 𝑛 = 3 ) acknowledged that these messages were problematic . Differential Notions of Problematic Content . We found that admins treated some types of content differently depending on the context and group dynamics . For instance , admins disliked the use of profanity in educational groups but were fine with group members cursing in friends groups . A few admins ( 𝑛 = 5 ) considered travel photos , pleasantries , and birthday messages to be irrelevant and annoying but felt awkward about forbidding casual conversations . Moreover , admins , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 13 of political groups prohibited sharing positive news about their opposing parties . Two admins found jokes and sarcasm to be problematic , particularly when shared out of context . Mehrab recounted an incident from his professional group in Bangladesh : “Recently while everyone was paying respect to a deceased colleague , someone shared a joke without paying attention to the ongoing conversation . This is insincere and displays a lack of common sense . ” Several private group admins ( 𝑛 = 6 ) disapproved of sharing adult content in family , professional , and other groups that have female group members . In contrast , public groups were filled with child pornography , sex chat and video services , and pornographic images and videos with little - to - no actions from admins . 5 . 2 . 2 User Reactions to Problematic Content . Users in public groups seldom responded to prob - lematic content shared in their groups . Whereas , private group admins reported diverse reactions from users towards problematic content . For example , group members were often divided whether certain types of content , e . g . , religious messages or curse words should be allowed . Admins shared that when members felt overwhelmed by the sheer volume of problematic content propagating in their groups , they opted to leave the group rather than calling out the offense . Usually group members ignored political or communal propaganda to avoid conflicts with people in their offline social sphere . Hamid shared his experience in a college WhatsApp group : “Communal hate speech has been normalized in India over the years and none has the time or energy to protest such content . Most group members just care about staying connected with college friends instead of constantly arguing with them . ” Moreover , in a few family groups ( 𝑛 = 4 ) , elderly members strongly supported COVID related misinformation and blindly forwarded them . In many private groups ( 𝑛 = 10 ) , members openly supported communal propaganda and hate speech against minorities . Priya , who ran a private group to share internship and job opportunities in India , reported : “During COVID many group members blamed Muslims for the rise of COVID in India . This triggered not only Muslim but other considerate group members from different religions , who decided to give up networking opportunities instead of being in groups that discriminated against people for their religion . ” On rare occasions , group members contested problematic content . This occurred particularly in private groups in which group members had relatively weaker social ties . For example , Pranjal shared that a doctor in his workspace group called out a message on Cancer treatment as misinfor - mation and encouraged group members to check sources before sharing health - related content ( Figure 2 ( A ) ) . In a few educational and professional groups ( 𝑛 = 4 ) , where admins did not intervene at all , some group members resisted religious hate speech and political propaganda , but this often led to heated arguments among group members . In friends groups , sometimes group members reacted with haha or angry emojis on spam messages and warned others against clicking on spam links . Nazma , an admin of her university classmates’ group in Bangladesh , commented : “Sometimes people intentionally share phishing links in the group . If I don’t notice , other classmates will call them out as spams and question the group member who shared that content . ” These findings show that users’ responses to problematic content are influenced by group dynamics , group’s purpose , and relationship among group members . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 14 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Table 3 . Characteristics of different parenting styles as per Baumrind’s Typology and how they manifest in WhatsApp group admins’ attitude towards moderation . Authoritative Authoritarian Permissive Uninvolved KeycharacteristicsinBaumrind’stypology Warm , responsive , assertive , high expectations , clear standards , democratic , communicative Limited warmth , rigid , forceful , adheres to rules , punitive , autocratic Low expectations , few rules , indulgent , accepting , lenient , avoids confrontation No expectations , absent , passive , neglectful , competing priorities Attitude of WhatsAppgroupadmins Cares for group members’ wellbeing , sets clear group rules , takes different disciplinary measures to moderate harmful content , gives explanation for moderation decisions Enables admin only messaging , mutes everyone for the offense of a select few , suppresses group interaction Reluctant to moderate to avoid conflict and maintain social hierarchy , gives offenders benefit of the doubt Too busy to manage group , ignores group due to excessive problematic content , lack of moderation draws more problematic content ManifestationinWhatsAppgroups Mostly educational , organizational , and professional groups ( n = 15 ) , few family ( n = 5 ) and friends groups ( n = 2 ) Mostly public groups ( n = 9 ) , very few family groups ( n = 2 ) , one professional group Mostly family and friends groups ( n = 8 ) , very few professional groups ( n = 2 ) Mostly public groups ( n = 21 ) , very few private groups ( n = 3 ) 5 . 3 How do Admins Deal with Problematic Content in WhatsApp Groups ? We now present our analysis of admins’ responses to problematic content through Baumrind’s typology of parenting styles , with particular focus on how admins exercise care and control when it comes to moderation . Based on admins’ responsiveness to the needs of the group members and demandingness to control their behavior , we identify four moderation styles : authoritative , authoritarian , permissive , and uninvolved . We present these moderation styles as general attitude that is broadly seen among admins in public and private groups , and also in groups with different levels of social ties ( strong to weak ) , instead of a rigid categorization ( see Table 3 ) . 5 . 3 . 1 Authoritative . Baumrind [ 7 ] described authoritative parents as warm but firm . Authoritative parents encourage their children to be independent , but maintain limits and controls on their actions . They clearly communicate their expectations to children and provide reasoning for their actions [ 49 ] . All these lead to positive development outcomes among children [ 55 ] . We observed this approach mainly in groups ( 𝑛 = 15 ) with weaker social ties ( e . g . , educational , organizational , and professional groups ) , where people knew each other but maintained a quasi - formal relationship . Admins in some family ( 𝑛 = 5 ) and a couple of friends groups ( 𝑛 = 2 ) occasionally exhibited this style . As characteristic of authoritative style , admins in these groups took care of members’ needs and emotional well - being . Tawhid , who used WhatsApp group to run an online magazine in Bangladesh , shared : “Whenever there is communal violence against minorities , I try to deal it with care . I personally message my Hindu employees and ask how they are doing . If they need mental health support or work - related accommodations , I try to provide them . ” Authoritative admins usually supported group members right to freedom of expression and considered group members “mature enough” to behave responsibly . Often , they maintained a balance between being authoritative but yet approachable . Mansi shared her view about moderating an animal rescue WhatsApp group in India : “We want to bond with the volunteers , who are helping with our cause . However , we need to manage boundaries and establish some sort of command so that we can run the group efficiently . ” , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 15 Depending on the group dynamics , authoritative admins adopted different techniques to control problematic content in their groups , which we outline below . Group Rules . Authoritative admins created group rules to maintain organizational integrity , professional etiquette , and protect group members from harmful content . The rules discouraged the sharing of fake news , adult content , hate speech , spam , gendered harassment , and disrespecting others . Few admins ( 𝑛 = 3 ) also disapproved of religiously and politically charged messages . Admins either listed the rules in the group description , or shared them in the group chat or via direct messages when new members joined the group or someone violated the group policy . However , group members did not always follow the rules , which required reactive measures from the admins . Debunking and Fact - Checking . A few admins ( 𝑛 = 4 ) used Google , YouTube videos , journalis - tic and scientific sources to fact - check content . However , group dynamics affected how admins informed the group members of the fact - checked content . In groups with peers , admins directly debunked messages in the group chat and asked the offender to review the fact - checking links . However , in family groups , younger admins engaged in a deliberate dialogue to respectfully con - vince older relatives not to share misinformation by explaining how it would negatively impact group members . Priya described her approach in family group : “We don’t try to prove elderly relatives wrong when they share health related misinforma - tion . Instead , we give them links and statistics from WHO and suggest to review those to inform themselves better . ” In contrast , admins were comfortable correcting younger family members . In professional and organizational groups , admins reprimanded their subordinates for spreading misinformation . However , when the offender held higher position ( e . g . , a senior colleague ) , admins messaged them privately so that they do not feel challenged or disrespected in front of other group members . Several admins ( 𝑛 = 11 ) pointed that when they debunked misinformation and spams , offenders often excused themselves , claiming they had not checked the content before sharing or were unaware of it altogether , speculating that it might have happened when a virus infected their phone or while their children were using it . Sometimes offenders became defensive to deflect the blame . Ravi shared his experience : “An elderly relative shared fake ayurvedic [ herbal ] cures of COVID in the family group . When we tried to debunk , he became defensive , called the scientific articles propaganda , and claimed the ayurvedic remedies had cured someone they knew personally . ” Content Removal . Most private group admins ( 𝑛 = 18 ) adopted a zero - tolerance approach to remove religious hate speech , political propaganda , adult content , and spams in their groups ( Figure 2 ( B ) ) . When we asked admins about their rationale behind deletion , they expressed that they prioritized the group’s welfare over displeasing a single user . They argued that these messages , if stayed longer , could potentially harm group members , hurt their sentiments , and spread more . Authoritative admins in groups with weaker social ties usually requested the offender privately either via phone call , WhatsApp , or Messenger to delete the post explaining why it was problematic . If the offender did not respond , then admins tagged them publicly in the group chat and asked them to remove their post . Recently after WhatsApp enabled admins to delete messages in the group [ 70 ] , several admins ( 𝑛 = 7 ) took advantage of this to directly remove harmful content without waiting for users to remove them . Dipali shared an incident from her family WhatsApp group : “A female relative had shared an adult content and didn’t respond upon contacting . When my younger cousins queried , I asked them to ignore . Ultimately , I deleted the video myself to prevent elderly relatives and younger members from seeing it . ” , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 16 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Many admins ( 𝑛 = 16 ) received positive responses from the offenders , who readily accepted their mistakes , apologized to the group , deleted their content , and promised to be considerate in the future . However , some admins ( 𝑛 = 6 ) reported facing accusations , harassment , and bullying from the offenders for moderating hate speech and political propaganda . Admins were often called “anti - democratic” or “government agent . ” Gautam also faced similar criticism when he condemned Islamophobic hate speech in his college group : “I was shocked to see my professors sharing misleading anti - Muslim videos of riots . When I tried to resist , many group members called me names and labeled me as a JNU sympathizer . ” [ Jawaharlal Nehru University ( JNU ) is a public university in India with a strong center - left political foothold . ] Member Removal . Many admins ( 𝑛 = 12 ) felt compelled to remove group members , who continued sharing hate speech , spam , and adult content despite multiple warnings . Gautam shared an example from his college group during the 2020 Delhi riot in India : “One group member was continuously sharing communally charged gore killing videos of the riot without any verified sources . As these videos were destabilizing the group , we asked him to stop but he didn’t listen . Then we permanently removed him . ” In some cases , admins gave offenders a second chance and added them back to the group after they apologized and reflected on their behavior . One of the Bangladeshi admins Jawad commented : “We blocked a fellow classmate as he was repeatedly spamming the group with links of fake apps . After he clarified that his phone was compromised and apologized , we added him back to the group . ” Offline Mediation . In a handful of cases ( 𝑛 = 4 ) , admins dealt with the offenders offline , particularly when their harmful behavior persisted for long . In family groups , admins often requested help from other influential family members , who were in a position to talk to the offender out of sharing misinformation and spams . Ravi shared : “One of my uncles was continuously spamming our family WhatsApp group with different health related misinformation . I requested my father to talk to him in person and after that he stopped spamming . ” To handle extreme cases , admins often coordinated among themselves to counsel repeat offenders offline , understand their point of view , and explain why their behavior was wrong . Occasionally , admins adopted a format similar to “public hearing” to create social pressure on the offender . Rafid , who supervised a college WhatsApp group in Bangladesh , shared : “Following several exchanges of communal hate speech in the group , we called group members in the college field including the offenders . There we discussed the issue at length and the offenders pledged they wouldn’t repeat this again . ” 5 . 3 . 2 Authoritarian . Baumrind [ 8 ] described authoritarian parents as highly controlling and show - ing little warmth towards their children . They expect blind obedience from their children and punish harshly , believing that discipline will make the children resilient . Therefore , children often become highly rebellious and develop low self - esteem and behavioral issues [ 55 ] . Nearly one - third of the public groups we joined ( 𝑛 = 9 ) imposed authoritarian moderation . We also noted this in two family groups and a professional group . These admins enforced admin - only messaging , i . e . , disabled messaging for all members except themselves ( Figure 2 ( C ) ) . They justified such measures were necessary to manage the inflow of misinformation , spam , and hate speech . Although not all group members were at fault for sharing problematic content , authoritarian admins chose to mute everyone instead of only disciplining the offenders . Several public group admins we , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 17 Fig . 2 . ( A ) Group member deleted their message on fake cures of Cancer after a group member debunked it . ( B ) Admin removed both the spam message and the spammer and discouraged group members from sharing irrelevant messages . ( C ) Admin enabled admin - only messaging in a service related public group . talked to preferred admin - only messaging because it saved their effort to filter harmful content and protected their mental well - being . They prioritized their interest at the expense of group members’ freedom of speech . Vivek , an admin of a public gambling group , commented : “I cannot remove the spammers because they are my customers and bring me money . Instead , I can cut spamming easily by enabling admin - only messaging . ” The one - way communication , where only admins can send messages , created a power imbalance as group members had no means to hold the admins accountable . For example , in a family group of near and distant relatives , where admin - only messaging was enforced , group members had to send their messages to the admins first , who then curated what messages could be posted in the group . People felt annoyed because they could no longer exchange greetings and share personal updates instantly . Moreover , admins were not always available to check the messages that group members sent to them , and thus time - sensitive content went unnoticed . Priya described similar challenge in a group created to share job opportunities : “Recently admins were on vacation and could not check DMs from group members daily . Someone sent us a good internship opportunity , but by the time we checked , the deadline was over . It was disappointing , since no one in the group could benefit from it . ” Although group members requested admins to temporarily lift the restriction on messaging , authoritarian admins ignored such requests . This decreased group members’ participation and many left the group since it no longer fulfilled their needs and restricted the exchange of ideas . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 18 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Some group members even threatened to create separate groups . Yet , admins preferred the status quo saying they were “too busy to manage the groups full - time . ” 5 . 3 . 3 Permissive . Baumrind [ 8 ] defined permissive parents as nurturing and warm . They set very few rules and are lenient if children break them . They do not like to say ‘no’ to their children and usually allow them to have their own way [ 49 ] . We noted this attitude primarily among admins in family and friends groups ( 𝑛 = 8 ) , where people shared strong social ties offline . Although an exception , a couple of admins ( 𝑛 = 2 ) in professional groups also showed reluctance to moderate content mainly to avoid conflict with group members , who were part of their social circles . Thus , permissive admins did not counter religious or political propaganda to avoid offending group members . Ravi emphasized : “I do not respond to political forwards in my family WhatsApp group because it’s very difficult to change people , where they have already made up their mind . ” In family and professional groups , there were unspoken rules that no one would moderate content from people higher up in the social or professional hierarchy ( e . g . , older relatives and senior colleagues ) . Both in India and Bangladesh , correcting or contesting elders is considered disrespectful and goes against social norms . Although the unwillingness to moderate due to hierarchy between admins and group members might appear contradictory to usual parent - child power dynamics , research shows that aging parents in collectivist families yield to adult children’s decisions to maintain family harmony even though children’s actions might contradict parents’ preferences [ 79 ] . On the other hand , admins in friends groups did not want to assert themselves when their friends shared fake news arguing that “WhatsApp forwards aren’t serious . ” Admins often gave offenders “benefit of the doubt” and gave them a pass thinking the spams from their accounts were caused by “some issues in their phones . ” Rizwan , who managed a group of university classmates , commented : “Many allow their children to play with phones and the kids may mistakenly click on some link . Then , virus automatically spams the contacts and groups through that person’s WhatsApp account . ” Few permissive admins assumed that group members shared casteist and religious hate speech to vent out their frustrations , and therefore , they ignored those messages to protect free speech . Pranjal commented : “Recently in our group , my colleagues were discussing how lower caste students aren’t fit for engineering after a student committed suicide . I know some people feel frustrated thinking DEI initiatives that support lower caste students work against merit . How can I stop my colleagues from feeling deprived and penalize them for their thoughts ? ” As permissive parenting leads to lack of self - control among children [ 55 ] , permissive admins’ lack of moderation resulted in eco - chambers in family , friends , and workspace groups in which hateful content flourished . 5 . 3 . 4 Uninvolved . Baumrind [ 8 ] described that uninvolved parents are indifferent to the emotional and social needs of their children and do not often go beyond fulfilling children’s basic needs . They barely set any rules , expectations , or boundaries for the children . They may adopt this style out of frustration due to their own problems ( stress , abuse , etc . ) and may give up parental authority [ 49 ] . As a result , children are likely to suffer from impulsive attitude , aggression , and addiction [ 55 ] . Majority of the public groups ( 𝑛 = 21 ) we observed were completely neglected by the admins . Among the nine public group admins we talked to , many ( 𝑛 = 6 ) struggled to recognize their groups and had no idea that the previous admin had arbitrarily made them an admin before leaving the group . Moreover , they were ill - prepared to handle the onslaught of mass spamming done by group , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 19 members . To avoid spam , several admins ( 𝑛 = 5 ) either muted or archived the groups and even uninstalled WhatsApp . Thus , being unsupervised , these groups attracted all kinds of problematic content and reinforced a negative feedback loop . Ashok , who created a public group to share job opportunities in India , denied the harms of monetary scams posted in the group and commented : “Not everything is fake . People could benefit from these posts by earning money . ” When we probed admins why they did not delete the group or make someone else admin if they did not have time to manage the group , some of them said that they might utilize the group later for business purposes or making money . Vivek shared his plan about his public gambling group : “Now that the group is large I may sell it . Whoever buys the group can earn money by posting ‘earn from home’ messages . Even if 200 people pay him , he can earn a lot . ” In contrast , few private group admins ( 𝑛 = 3 ) in our sample exhibited traits of uninvolved moderation . They excused themselves saying that “they were too busy with work” or “it was too difficult to check the large volume of incoming messages . ” These findings show that public and private groups not only differ in terms of group’s purpose , composition , and size but also vary depending on how admins moderate them . However , these moderation styles are not a rigid profiling of WhatsApp group admins because few admins exhibited multiple moderation styles in a single group , based on the offender’s identity , the frequency and severity of the offense , and reactions from other group members . For example , Priya initially moderated hate speech in their professional group ( authoritative ) but eventually had to disable messaging ( authoritarian ) due to proliferation of hate speech . Moreover , depending on the group dynamics , same admins would moderate different types of groups differently . For example , Hamid actively resisted communal content in his college group ( authoritative ) but avoided correcting misinformation in the family group ( permissive ) . Additionally , different admins moderated similar types of groups differently . For example , we observed various degrees of permissive , authoritative , and authoritarian styles in family groups managed by different admins . 5 . 4 How Can WhatsApp Support Admins in Managing Problematic Content ? When we asked private group admins about who should handle moderation in WhatsApp groups , many of them ( 𝑛 = 11 ) absolved the platform of the responsibility . They thought that only admins should be held responsible for curtailing problematic content in their groups . Few admins ( 𝑛 = 3 ) felt that both WhatsApp and group admins should assume the responsibility for moderation , but they expected admins to handle most of the work . When we probed admins why they did not expect WhatsApp to lead moderation , Hamid commented : “Implementing moderation may not satisfy WhatsApp’s interest as a platform . Most people get attracted to WhatsApp because there are heated debates in close groups . WhatsApp needs unsupervised controversy to fuel participation in its groups . ” Below we discuss admins’ expectations from WhatsApp to improve content moderation . 5 . 4 . 1 Mixed Feelings about Automated Moderation . Private group admins had mixed feelings about automated platform - mediated moderation . A couple of them ( 𝑛 = 2 ) told us they had no idea whether automated moderation was possible on WhatsApp . Few of them ( 𝑛 = 3 ) opposed large - scale automated moderation , fearing lack of cultural sensitivity in moderation outcomes . Rizwan expressed his concerns : “In a family group if someone jokingly talks about ‘beating’ and the word is blacklisted , then their message may automatically be removed . People may not understand such auto - removal and it may negatively affect the relationship among group members . ” , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 20 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Hamid further pointed out that without addressing users’ intent , such technocentrism would provide a “patching solution” as people who willingly spread disinformation would move to a different platform or find workarounds . However , some admins ( 𝑛 = 7 ) , who did not know how end - to - end encryption works , were open to automated platform - mediated moderation thinking it would neither violate encryption nor user privacy if user’s personal data is not used . They wanted WhatsApp to automatically detect and remove or shadowban messages that contain spam links and offensive language . Admins wanted to be able to define blacklisted keywords for their groups so that group members receive automatic warnings if their messages contain any of those words . Moreover , they wanted a WhatsApp bot to automatically fact - check and verify posts that were either forwarded many times or reported by multiple users , saying “it’s not feasible to manually check every group message . ” Admins felt such automated moderation would be appropriate for large public groups , where people had weak social ties and strangers shared problematic content without facing any conse - quences . Others recommended making auto - moderation optional so that admins could enable or disable it if needed and review the flagged posts to make decisions . 5 . 4 . 2 Moderation Related Support . Several admins ( 𝑛 = 7 ) complained about how “WhatsApp admins don’t have any power or moderation tools” compared to those on other platforms . Given the large volume of incoming messages in some groups , admins wanted dedicated moderators like that in Facebook groups . Admins suggested that WhatsApp should inform the offenders of anonymous user reports against their content so that they could learn from feedback . Pranjal commented : “Truecaller shows how many users have marked a number as spam . If WhatsApp did something similar , it would discourage people from blindly forwarding spams . ” In cases when offenders did not change their behavior , admins wanted the option to mute or ban them temporarily so that they could not send any messages to the group for a certain period . In addition , admins struggled to find fact - checking content in local languages and on local issues and requested support from WhatsApp . Ravi described : “People don’t like to read long articles or watch long fact - checking videos on YouTube that force viewers to see ads . If WhatsApp could partner with fact - checking organizations to make short debunking videos in local languages that could be seen within WhatsApp , then low - literate and non - English speaking group members would be benefited . ” Admins also recommended to add credibility indicators to fact - checked misinformation , disable forwarding of such messages , or show pop - ups that discourage users from sharing such content . 5 . 4 . 3 Logistical Support . Admins demanded logistical support to manage their groups . For example , similar to Facebook groups , they recommended adding screening questions for people interested in joining groups , where people have weaker or no offline ties . Admins felt that such a feature would reduce their workload in verifying people before adding them to the group . At the time of the interview , WhatsApp had a limit of 2048 characters for group description . Admins wanted WhatsApp to raise the character limit in the group description so that they could add group rules for potential members to review before joining . Rizwan remarked that if WhatsApp had ‘pin messages’ feature they could display group rules at the top of the chat so that people could easily access the rules 1 . Moreover , admins requested a badge for themselves , like that of Facebook admins and Reddit moderators , so that in large groups people can recognize the admins when they post instructions . Participants , who also administered Facebook groups , expressed the need for a dashboard similar to the one available to Facebook group admins and Reddit moderators . They wanted WhatsApp to 1 WhatsApp has recently released this feature [ 103 ] . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 21 show relevant statistics , such as how many users reported a content , which users sent forwards and spams the most . They thought that these data points would help them easily identify perpetrators and find evidence of past violations . Mehrab mentioned : “WhatsApp should give a monthly breakdown of group activities . For example , the number and types of messages , shared links , forwards , photos , and spams , monthly activity log , such as how many new people joined the group , and the list of active users . ” These findings show that admins need a range of sociotechnical tools embedded within WhatsApp to identify and manage the spread of problematic content in their groups . 6 DISCUSSION Our study with private and public group admins in India and Bangladesh reveals the broad spectrum of how admins operate . By drawing on Baumrind’s typology of parenting styles , we reveal the dynamics and distinct approaches to moderation in different private and public groups . We outline below how different aspects of adminship in WhatsApp groups compare to those of admins and moderators on Facebook , Reddit , and Twitch . We then discuss design implications to support the diverse needs and dynamics of moderation on end - to - end encrypted platforms like WhatsApp . 6 . 1 Admins Across Platforms 6 . 1 . 1 Selection and Roles of Admins . As is common on other platforms [ 64 , 94 , 97 , 107 ] , our findings show that people often become admins on WhatsApp either unexpectedly , voluntarily , or by appointment depending on their personal ties , digital skills , and standout group activities . However , unlike other platforms , most admins in our sample became admin either by default because they created the group or due to their offline position , power , and popularity . We found that similar to other platforms [ 91 , 94 , 107 ] , WhatsApp does not provide onboarding for admins . We noted that like moderators on Facebook [ 51 , 62 , 94 , 97 ] , Reddit [ 92 ] , and Twitch [ 91 , 107 ] , private group admins in our sample assumed the responsibility of curating group members , answering questions in group chat , and creating safe space online . However , there were important differences too . For example , we found that providing offline support , e . g . , organizing community events as neighborhood group admin or resolving conflicts offline , was specific to WhatsApp as admins usually knew group members in person . We also noticed that offline power dynamics defined admins’ authority in WhatsApp groups . While different factors , such as channel ownership ( Twitch ) , designations of admins vs moderators ( Facebook ) , and tenure / seniority ( Reddit ) , determine the power of admins and moderators on other platforms [ 94 ] . We found that social hierarchy in organizational , professional , and family WhatsApp groups led to the division of roles and power among admins . Office superiors and older relatives enjoyed greater authority over younger and junior co - admins due to their seniority and age . As we used parenting as a metaphor , such hierarchy among WhatsApp group admins can be likened to gendered role distinctions between parents in patriarchal families [ 68 ] . 6 . 1 . 2 Moderating Problematic Content . In line with the findings of previous work on WhatsApp groups [ 26 , 29 , 84 , 99 ] , we found that private and public groups in our sample received widespread misinformation , propaganda , spam , and pornographic content . As group members ignore prob - lematic content not to embitter personal ties in the group [ 28 , 60 , 73 , 90 ] and WhatsApp barely intervenes due to end - to - end encryption , content moderation on WhatsApp greatly relies on the discretion of group admins , including their values and ideologies . Indeed , our analysis of WhatsApp admins’ moderation styles using Baumrind’s typology revealed variances across different groups ( see Table 3 ) . For example , moderation in public groups was at two extremes , i . e . , admins either did not moderate at all ( uninvolved ) or disabled messaging in the , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 22 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha group ( authoritarian ) . Uninvolved admins’ disregard for moderating problematic content could be compared to that of moderators in online communities that intentionally promote toxicity , e . g . , r / Incels [ 37 ] or right - wing extremist Facebook groups [ 89 ] . Although moderators of subreddits , Twitch channels , and Facebook groups can disable commenting on posts [ 41 , 43 , 56 ] and review posts before they go live [ 51 , 93 ] , these measures are not as extreme as completely disabling group interaction like authoritarian admins in our sample did , which could hurt diversity of opinions and suppress minority viewpoints [ 105 ] . Unlike other social media spaces , we found that group ties greatly impact moderation styles in different types of private groups . For example , except for a few exceptions , admins in our sample tended to be permissive in groups with strong social ties , i . e . , family and friends groups . Prior work from Pasquetto et al . [ 73 ] show that cultural norms in India and Pakistan discourage correcting those with higher social status on WhatsApp . Similarly , we found that problematic content from elderly group members and office superiors went unchallenged since admins deemed it impolite and rude to correct them . Moreover , as we noted permissive admins giving offenders benefit of the doubt , Twitch moderators often ignored toxic messages from audiences considering them as “playful banter” [ 54 ] . In contrast , admins of private groups with weaker social ties among members ( e . g . , organizational and professional groups ) mostly adopted an authoritative approach . The way authoritative admins nurtured the community , defined group rules , penalized problematic content , and faced aggression from offenders bore similarities to moderators on other platforms [ 51 , 92 , 94 , 107 ] . Moreover , admins’ polite and deliberative corrections of elderly group members and calling out peers mirror strategies documented in prior studies on WhatsApp [ 61 , 90 , 99 ] . However , while Facebook , Reddit , and Twitch moderators lack transparency in moderation [ 41 , 51 , 94 , 107 ] , authoritative admins in our sample typically offered explanations for penalizing problematic content and often dealt with the offenders in person due to the existing offline ties between admins and group members . Our findings collectively show that offline relationships and dynamics greatly shape the workings of WhatsApp group admins , which is rare on other platforms . 6 . 2 Improving Moderation in WhatsApp Groups Our findings highlight the strengths and weaknesses of different moderation styles practiced by public and private WhatsApp group admins in our sample . For example , while authoritative approach helped admins call out and contain problematic content , they also experienced bullying and harassment from the offenders , which soured their relationships with people they knew offline . On the other hand , while authoritarian admins were able to prevent the sharing of problematic content in their groups , their strict moderation policies ( e . g . , admin - only messaging ) stifled all forms of communication within the group and stripped group members of their agency and freedom of expression . In contrast , lack of moderation by permissive and uninvolved admins led to echo chambers in private groups and turned public groups into a cesspool of problematic behavior . Given the diverse outcomes of different moderation styles , there is no single moderation tool that might work well for the unique circumstances of different WhatsApp groups . We outline below how to address the diverse needs and moderation styles of different WhatsApp group admins . 6 . 2 . 1 Supporting Authoritative and Permissive Admins . We propose moderation tools considering the strong and weak social ties in private groups , where permissive and authoritative moderation styles are prevalent . Support to Inform Offenders of Moderation . Many authoritative admins in our study reported being rebuked and harassed by the offenders following moderation . Prior research shows that young WhatsApp users in India prefer indirect , respectful language to correct elderly family members [ 61 , , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 23 74 ] , which was also adopted by some authoritative admins in our sample while correcting elderly relatives and senior colleagues so that they do not feel challenged or offended . However , American users prefer objections that make conscientious moral appeal against offenses [ 111 ] . WhatsApp can provide admins with templates incorporating localized relational politeness norms for debunking , warning , and explaining moderation decisions to offenders . Given permissive admins hesitate to moderate those , who are higher up in the social hierarchy than them , these correction messages will reduce their burden to think of how to approach the offenders . Moreover , careful choice of words may help de - escalate tension between authoritative admins and offenders , even when the offender is a peer . Adding Frictions and Nudges . We found that lack of moderation by permissive admins led to eco - chambers in close - knit private groups . Although authoritative admins in private groups with weaker social ties moderated communal hate speech and propaganda , they were often harassed by dominant peers , who might be part of groups , where such politicized and polarized content flourished . Prior research shows that social nudges from ‘ingroup’ , i . e . , those who are from the same race or religion as the offender can reduce hate speech [ 66 , 95 ] . Therefore , apart from debunking specific propaganda and disinformation , WhatsApp could partner with local civil organizations and experts to produce content that encourages people to not spread hatred and division against people from different ethnicity , caste , religion , gender , and political leanings . WhatsApp should make these content available in diverse formats and different languages to reach broader audiences . As admins in our sample often sought help from group members to approach offenders , admins could involve those , who either share similar background as the offender or have more power over the offender , to share social nudges and dissuade them from sharing communal content . Moreover , ‘nudging’ is useful in reducing users’ intent to share misinformation [ 75 ] . WhatsApp could nudge users when they try to forward any content and ask them about their reason for sharing it with the group . WhatsApp’s website lists tips for identifying problematic content [ 24 ] , which they could also show as a pop - up when users try to share something viral . These added frictions might dissuade users from widespread and impulsive sharing of harmful content . 6 . 2 . 2 Positively Engaging Authoritarian and Uninvolved Admins . As public group admins were either too strict or lax about moderation , we recommend tools to enable balanced moderation practices in public groups . Authoritative Moderation Tools for Authoritarian Admins . WhatsApp should provide proper monitoring and preventive tools so that public group admins , who are willing to moderate , do not resort to authoritarian measures , which are both unfair to innocent members and defeat the purpose of the group . Although invite links to join public groups are available online , WhatsApp could introduce screening questions like that of Facebook groups to keep out spamming WhatsApp bots . WhatsApp could use unencrypted account metadata to evaluate user profiles and use automated tools to filter out accounts that give low - quality responses to screening questions . Newly joined accounts could also be restricted from posting or have their initial messages be pre - approved by admins before being posted in the group . This might help public group admins immediately identify spammers and remove them from the group . Moreover , WhatsApp could use unencrypted activity metadata , e . g . , number of messages and forwards sent by users to spot anomalies and alert admins so that they can review a sample of messages sent by highly active users . Similar to Facebook , WhatsApp can introduce the feature to temporarily suspend repeat offenders [ 13 ] , so that they cannot send messages to the group for a certain period . Another option is to ‘shadowban’ the offenders so that no group members can see their messages except admins . These measures might protect innocent users from harmful content without taking away their right to participate in the group and provide recourses to admins to exercise control without being authoritarian . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 24 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha Consequences for Uninvolved Admins . WhatsApp should intervene when admins in large public groups willingly ignore problematic content , as currently they do not face any consequences . Facebook reduces privileges and reach for the groups that have multiple violations and , in extreme cases , removes the group completely [ 1 ] . While WhatsApp claims to restrict chat activity in groups where admins violate WhatsApp’s Terms of Service [ 23 ] , it’s unclear how they do so and we did not come across any groups , where WhatsApp placed such restrictions . WhatsApp could use unencrypted account metadata to detect admins who have stopped using WhatsApp or are no longer active in the group . Then , WhatsApp can send reminders to admins via phone messages or in - app notification to check their groups . If they continue to remain inactive , WhatsApp can disable or remove the group . 7 LIMITATIONS AND CONCLUSION We use Baumrind’s typology of “parenting styles " to explore how WhatsApp group admins in India and Bangladesh manage and regulate their groups . While our findings reveal important distinctions between the composition , dynamics , and workings of admins in public and private groups , we also highlight how moderation in WhatsApp groups qualitatively differ from that of Facebook groups , subreddits , and Twitch channels . Our work has a few limitations . Participants in our study were skewed towards highly educated , urban ( in private groups ) , and male admins ( in public groups ) . Due to the small sample size , we might not have covered all types of WhatsApp groups and admins . Given the self - reported nature of qualitative interviews , we could not verify if admins in our study shared problematic content themselves , perceived hate speech or propaganda as harmless due to their own biases , treated the offenders equally , or used their power to suppress minority or opposing views . Since we did not talk to group members , we could not assess to what extent offenders found the admins’ explanation of moderation to be useful . Furthermore , the moderation styles observed in public and private groups in India and Bangladesh might not apply to groups in other cultures and geographies . More work is needed to unpack the complex dynamics among admins and group members across different demographics . REFERENCES [ 1 ] Tom Alison . 2021 . Changes to Keep Facebook Groups Safe . Retrieved September 8 , 2023 from https : / / about . fb . com / news / 2021 / 03 / changes - to - keep - facebook - groups - safe / [ 2 ] Mary Jean Amon , Nika Kartvelishvili , Bennett I . Bertenthal , Kurt Hugenberg , and Apu Kapadia . 2022 . Sharenting and Children’s Privacy in the United States : Parenting Style , Practices , and Perspectives on Sharing Young Children’s Photos on Social Media . Proc . ACM Hum . - Comput . Interact . 6 , CSCW1 , Article 116 ( 2022 ) , 30 pages . [ 3 ] Shakuntala Banaji , Ramnath Bhat , Anushi Agarwal , Nihal Passanha , and Mukti Sadhana Pravin . 2019 . WhatsApp vigilantes : An exploration of citizen reception and circulation of WhatsApp misinformation linked to mob violence in India . [ 4 ] Diana Baumrind . 1966 . Effects of authoritative parental control on child behavior . Child development 37 , 4 ( 1966 ) , 887 – 907 . [ 5 ] Diana Baumrind . 1967 . Child care practices anteceding three patterns of preschool behavior . Genetic psychology monographs 75 , 1 ( 1967 ) , 43 – 88 . [ 6 ] Diana Baumrind . 1971 . Current patterns of parental authority . Developmental Psychology 4 , 1 , Pt . 2 ( 1971 ) , 1 – 103 . [ 7 ] Diana Baumrind . 1972 . An exploratory study of socialization effects on black children : Some black - white comparisons . Child development 43 , 1 ( 1972 ) , 261 – 267 . [ 8 ] Diana Baumrind . 1991 . The influence of parenting style on adolescent competence and substance use . The journal of early adolescence 11 , 1 ( 1991 ) , 56 – 95 . [ 9 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative research in psychology 3 , 2 ( 2006 ) , 77 . [ 10 ] Jie Cai and Donghee Yvette Wohn . 2021 . After Violation But Before Sanction : Understanding Volunteer Moderators’ Profiling Processes Toward Violators in Live Streaming Communities . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 410 ( 2021 ) , 25 pages . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 25 [ 11 ] Jie Cai and Donghee Yvette Wohn . 2022 . Coordination and Collaboration : How Do Volunteer Moderators Work as a Team in Live Streaming Communities ? . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 300 , 14 pages . [ 12 ] Jess Cartner - Morley . 2022 . Group chat overload : have we reached peak WhatsApp ? Retrieved May 23 , 2023 from https : / / www . theguardian . com / lifeandstyle / 2022 / aug / 19 / whatsapp - group - chat - overload - have - we - reached - peak - whatsapp [ 13 ] Facebook Help Center . 2023 . Suspend someone in a Facebook group you admin . Retrieved September 10 , 2023 from https : / / www . facebook . com / help / 387893081608612 [ 14 ] Andrew Chadwick , Natalie - Anne Hall , and Cristian Vaccari . 2023 . Misinformation rules ! ? Could “group rules " reduce misinformation in online personal messaging ? New Media & Society 0 , 0 ( 2023 ) , 1 – 21 . [ 15 ] Kwan Hoi Ching and Leung Man Tak . 2017 . The structural model in parenting style , attachment style , self - regulation and self - esteem for smartphone addiction . IAFOR Journal of Psychology & the Behavioral Sciences 3 , 1 ( 2017 ) , 85 – 103 . [ 16 ] Josh Constine . 2018 . WhatsApp has an encrypted child abuse problem . Retrieved September 11 , 2023 from https : / / techcrunch . com / 2018 / 12 / 20 / whatsapp - pornography / [ 17 ] Amanda L . L . Cullen and Sanjay R . Kairam . 2022 . Practicing Moderation : Community Moderation as Reflective Practice . Proc . ACM Hum . - Comput . Interact . 6 , CSCW1 , Article 111 ( 2022 ) , 32 pages . [ 18 ] Philipe de Freitas Melo , Carolina Coimbra Vieira , Kiran Garimella , Pedro O . S . Vaz de Melo , and Fabrício Benevenuto . 2019 . Can WhatsApp Counter Misinformation by Limiting Message Forwarding ? CoRR 881 ( 2019 ) , 13 pages . [ 19 ] Philipe de Freitas Melo , Carolina Coimbra Vieira , Kiran Garimella , Pedro O . S . Vaz de Melo , and Fabrício Benevenuto . 2019 . Can WhatsApp Counter Misinformation by Limiting Message Forwarding ? In Complex Networks and Their Applications VIII . Springer International Publishing , NY , USA , 372 – 384 . [ 20 ] The Economist . 2021 . Bangladesh’s religious minorities are under attack . Retrieved September 11 , 2023 from https : / / www . economist . com / asia / 2021 / 11 / 06 / bangladeshs - religious - minorities - are - under - attack [ 21 ] Peter Elkind , Jack Gillum , and Craig Silverman . 2021 . How Facebook Undermines Privacy Protections for Its 2 Billion WhatsApp Users . Retrieved September 3 , 2023 from https : / / www . propublica . org / article / how - facebook - undermines - privacy - protections - for - its - 2 - billion - whatsapp - users [ 22 ] Rafael Evangelista and Fernanda Bruno . 2019 . WhatsApp and political instability in Brazil : targeted messages and political radicalisation . Internet Policy Review 8 , 4 ( 2019 ) , 1 – 23 . [ 23 ] WhatsApp FAQ . 2021 . What does “This group is no longer available " mean ? Retrieved September 8 , 2023 from https : / / faq . whatsapp . com / 679236580386110 / [ 24 ] WhatsApp FAQ . 2023 . About spam and unwanted messages . Retrieved September 8 , 2023 from https : / / faq . whatsapp . com / 2286952358121083 [ 25 ] WhatsApp FAQ . 2023 . How WhatsApp Helps Fight Child Exploitation . Retrieved September 8 , 2023 from https : / / faq . whatsapp . com / 5704021823023684 [ 26 ] Gowhar Farooq . 2017 . Politics of fake news : How WhatsApp became a potent propaganda tool in India . Media Watch 9 , 1 ( 2017 ) , 106 – 117 . [ 27 ] K . J . Kevin Feng , Kevin Song , Kejing Li , Oishee Chakrabarti , and Marshini Chetty . 2022 . Investigating How University Students in the United States Encounter and Deal With Misinformation in Private WhatsApp Chats During COVID - 19 . In Eighteenth Symposium on Usable Privacy and Security ( SOUPS 2022 ) . USENIX Association , Boston , MA , 427 – 446 . [ 28 ] K . J . Kevin Feng , Kevin Song , Kejing Li , Oishee Chakrabarti , and Marshini Chetty . 2022 . Investigating How University Students in the United States Encounter and Deal with Misinformation in Private Whatsapp Chats during COVID - 19 . In Proceedings of the Eighteenth USENIX Conference on Usable Privacy and Security . USENIX Association , USA , Article 23 , 20 pages . [ 29 ] Ana Cristina Bicharra Garcia and Adriana Vivacqua . 2021 . Should I stay or should I go ? Managing Brazilian WhatsApp groups . First Monday 26 , 2 ( 2021 ) , 0 . [ 30 ] Rashmi Garg , Elizabeth Levin , Diana Urajnik , and Carol Kauppi . 2005 . Parenting style and academic achievement for East Indian and Canadian adolescents . Journal of Comparative Family Studies 36 , 4 ( 2005 ) , 653 – 661 . [ 31 ] Kiran Garimella and Dean Eckles . 2020 . Images and Misinformation in Political Groups : Evidence from WhatsApp in India . CoRR abs / 2005 . 09784 ( 2020 ) , 12 pages . [ 32 ] Ysabel Gerrard and Helen Thornham . 2020 . Content moderation : Social media’s sexist assemblages . New Media & Society 22 , 7 ( 2020 ) , 1266 – 1286 . [ 33 ] Arup Kumar Ghosh , Karla Badillo - Urquiola , Mary Beth Rosson , Heng Xu , John M . Carroll , and Pamela J . Wisniewski . 2018 . A Matter of Control or Safety ? Examining Parental Use of Technical Monitoring Apps on Teens’ Mobile Devices . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , 1 – 14 . [ 34 ] Arup Kumar Ghosh , Karla Badillo - Urquiola , and Pamela Wisniewski . 2018 . Examining the Effects of Parenting Styles on Offline and Online Adolescent Peer Problems . In Proceedings of the 2018 ACM International Conference on Supporting Group Work . ACM , NY , USA , 150 – 153 . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 26 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha [ 35 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , content moderation , and the hidden decisions that shape social media . Yale University Press , New Haven , Connecticut , USA . [ 36 ] Tarleton Gillespie , Patricia Aufderheide , Elinor Carmi , Ysabel Gerrard , Robert Gorwa , Ariadna Matamoros - Fernandez , Sarah T . Roberts , Aram Sinnreich , and Sarah Myers West . 2020 . Expanding the debate about content moderation : Scholarly research agendas for the coming policy debates . Internet Policy Review 9 , 4 ( 2020 ) , 30 pages . [ 37 ] Rosalie Gillett and Nicolas Suzor . 2022 . Incels on Reddit : A study in social norms and decentralised moderation . First Monday 27 , 6 ( 2022 ) , 0 . [ 38 ] Lilly Irani , Janet Vertesi , Paul Dourish , Kavita Philip , and Rebecca E . Grinter . 2010 . Postcolonial Computing : A Lens on Design and Development . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Atlanta , Georgia , USA ) ( CHI ’10 ) . ACM , NY , USA , 1311 – 1320 . [ 39 ] Maurice Jakesch , Kiran Garimella , Dean Eckles , and Mor Naaman . 2021 . Trend Alert : A Cross - Platform Organization Manipulated Twitter Trends in the Indian General Election . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 379 ( 2021 ) , 19 pages . [ 40 ] Haiyan Jia , Pamela J . Wisniewski , Heng Xu , Mary Beth Rosson , and John M . Carroll . 2015 . Risk - Taking as a Learning Process for Shaping Teen’s Online Information Privacy Behaviors . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing . ACM , NY , USA , 583 – 599 . [ 41 ] Prerna Juneja , Deepika Rama Subramanian , and Tanushree Mitra . 2020 . Through the Looking Glass : Study of Transparency in Reddit’s Moderation Practices . Proc . ACM Hum . - Comput . Interact . 4 , GROUP , Article 17 ( 2020 ) , 35 pages . [ 42 ] Antonis Kalogeropoulos and Patrícia Rossini . 2023 . Unraveling WhatsApp group dynamics to understand the threat of misinformation in messaging apps . New Media & Society 0 , 0 ( 2023 ) , 1 – 26 . [ 43 ] Bente Kalsnes and Karoline Andrea Ihlebæk . 2021 . Hiding hate speech : Political moderation on Facebook . Media , Culture & Society 43 , 2 ( 2021 ) , 326 – 342 . [ 44 ] Seny Kamara , Mallory Knodel , Emma Llansó , Greg Nojeim , Lucy Qin , Dhanaraj Thakur , and Caitlin Vogus . 2022 . Outside Looking In : Approaches to Content Moderation in End - to - End Encrypted Systems . arXiv : 2202 . 04617 [ cs . CR ] [ 45 ] Shaheen Kanthawala and Jessica Maddox . 2022 . Hiding in the echo chamber : fact - Checking failures and individual tactics of accuracy determination on WhatsApp in India . Asian Journal of Communication 32 , 2 ( 2022 ) , 174 – 191 . [ 46 ] AKM Rezaul Karim , Tania Sharafat , and Abu Yusuf Mahmud . 2013 . Cognitive emotion regulation in children as related to their parenting style , family type and gender . Journal of the Asiatic Society of Bangladesh , Science 39 , 2 ( 2013 ) , 211 – 220 . [ 47 ] Ashkan Kazemi , Kiran Garimella , Gautam Kishore Shahi , Devin Gaffney , and Scott A Hale . 2022 . Research note : Tiplines to uncover misinformation on encrypted platforms : A case study of the 2019 Indian general election on WhatsApp . Harvard Kennedy School Misinformation Review 3 , 1 ( 2022 ) , 17 pages . [ 48 ] Devika Khandelwal . 2022 . Covid lies are tearing through India’s family WhatsApp groups . Retrieved September 3 , 2023 from https : / / www . wired . co . uk / article / india - covid - conspiracies - whatsapp [ 49 ] Kimberly Kopko . 2017 . Parenting styles and adolescents . Retrieved September 9 , 2023 from https : / / www . countrysideday . org / wp - content / uploads / 2014 / 09 / Parenting - Styles - and - Adolescents . pdf [ 50 ] Vinay Koshy , Tanvi Bajpai , Eshwar Chandrasekharan , Hari Sundaram , and Karrie Karahalios . 2023 . Measuring User - Moderator Alignment on r / ChangeMyView . Proc . ACM Hum . - Comput . Interact . 7 , CSCW2 , Article 286 ( 2023 ) , 36 pages . [ 51 ] Tina Kuo , Alicia Hernani , and Jens Grossklags . 2023 . The Unsung Heroes of Facebook Groups Moderation : A Case Study of Moderation Practices and Tools . Proc . ACM Hum . - Comput . Interact . 7 , CSCW1 , Article 97 ( 2023 ) , 38 pages . [ 52 ] Sofie Kuppens and Eva Ceulemans . 2019 . Parenting styles : A closer look at a well - known concept . Journal of child and family studies 28 ( 2019 ) , 168 – 181 . [ 53 ] Ozan Kuru , Scott W . Campbell , Joseph B . Bayer , Lemi Baruh , and Richard Ling . 2022 . Encountering and Correcting Misinformation on WhatsApp . John Wiley & Sons , Ltd , Hoboken , NJ , USA , Chapter 7 , 88 – 107 . [ 54 ] Na Li , Jie Cai , and Donghee Yvette Wohn . 2023 . Ignoring As a Moderation Strategy for Volunteer Moderators on Twitch . In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 169 , 7 pages . [ 55 ] Pamela Li . 2023 . 4 Types of Parenting Styles and Their Effects On Children . Retrieved September 9 , 2023 from https : / / www . parentingforbrain . com / 4 - baumrind - parenting - styles / [ 56 ] Claudia Claudia Wai Yu Lo . 2018 . When all you have is a banhammer : the social and communicative work of Volunteer moderators . Ph . D . Dissertation . Massachusetts Institute of Technology . [ 57 ] Alfred Lua . 2023 . 21 Top Social Media Sites to Consider for Your Brand in 2023 . Retrieved May 23 , 2023 from https : / / buffer . com / library / social - media - sites / [ 58 ] Eleanor E . Maccoby and John A . Martin . 1983 . Socialization In The Context Of The Family : Parent - Child Interaction . Developmental Psychology 28 , 6 ( 1983 ) , 1006 – 1017 . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 27 [ 59 ] Caio Machado , Beatriz Kira , Vidya Narayanan , Bence Kollanyi , and Philip Howard . 2019 . A Study of Misinformation in WhatsApp Groups with a Focus on the Brazilian Presidential Elections . . In Companion Proceedings of The 2019 World Wide Web Conference . ACM , NY , USA , 1013 – 1019 . [ 60 ] Pranav Malhotra . 2023 . Misinformation in WhatsApp Family Groups : Generational Perceptions and Correction Considerations in a Meso - News Space . Digital Journalism 0 , 0 ( 2023 ) , 1 – 19 . [ 61 ] Pranav Malhotra and Katy Pearce . 2022 . Facing Falsehoods : Strategies for Polite Misinformation Correction . Interna - tional Journal of Communication 16 , 0 ( 2022 ) , 2303 – 2324 . [ 62 ] Sanna Malinen . 2021 . The owners of information : Content curation practices of middle - level gatekeepers in political Facebook groups . New Media & Society 0 , 0 ( 2021 ) , 18 pages . [ 63 ] Hamid Masud , Muhammad Shakil Ahmad , Ki Woong Cho , and Zainab Fakhr . 2019 . Parenting styles and aggression among young adolescents : a systematic review of literature . Community mental health journal 55 ( 2019 ) , 1015 – 1030 . [ 64 ] J . Nathan Matias . 2019 . The Civic Labor of Volunteer Moderators Online . Social Media + Society 5 , 2 ( 2019 ) , 2056305119836778 . [ 65 ] Amanda Sheffield Morris , Jennifer S Silk , Laurence Steinberg , Sonya S Myers , and Lara Rachel Robinson . 2007 . The role of the family context in the development of emotion regulation . Social development 16 , 2 ( 2007 ) , 361 – 388 . [ 66 ] Kevin Munger . 2017 . Tweetment effects on the tweeted : Experimentally reducing racist harassment . Political Behavior 39 ( 2017 ) , 629 – 649 . [ 67 ] Sheryl Ng and Taberez Neyazi . 2022 . Self - and Social Corrections on Instant Messaging Platforms . International Journal of Communication 17 , 0 ( 2022 ) , 426 – 446 . [ 68 ] Unaiza Niaz . 2003 . Violence against women in South Asian countries . Archives of women’s mental health 6 ( 2003 ) , 173 – 184 . [ 69 ] Fathima Nizaruddin . 2021 . Role of public WhatsApp groups within the Hindutva ecosystem of hate and narratives of “CoronaJihad " . International Journal of Communication 15 ( 2021 ) , 18 . [ 70 ] The Times of India . 2022 . New details emerge of WhatsApp’s feature that will allow admins to delete messages for everyone and anyone in the group . Retrieved August 28 , 2023 from https : / / timesofindia . indiatimes . com / gadgets - news / new - details - emerge - of - whatsapps - feature - that - will - allow - admins - to - delete - messages - for - everyone - and - anyone - in - the - group / articleshow / 93278562 . cms [ 71 ] Naresh R Pandit . 1996 . The creation of theory : A recent application of the grounded theory method . The qualitative report 2 , 4 ( 1996 ) , 1 – 15 . [ 72 ] Irene Pasquetto , Eaman Jahani , Alla Baranovsky , and Matthew Baum . 2020 . Understanding Misinformation on Mobile Instant Messengers ( MIMs ) in Developing Countries . [ 73 ] Irene V . Pasquetto , Eaman Jahani , Shubham Atreja , and Matthew Baum . 2022 . Social Debunking of Misinformation on WhatsApp : The Case for Strong and In - Group Ties . Proc . ACM Hum . - Comput . Interact . 6 , CSCW1 , Article 117 ( 2022 ) , 35 pages . [ 74 ] Katy E Pearce and Pranav Malhotra . 2022 . Inaccuracies and Izzat : Channel Affordances for the Consideration of Face in Misinformation Correction . Journal of Computer - Mediated Communication 27 , 2 ( 2022 ) , 1 – 19 . [ 75 ] Gordon Pennycook , Ziv Epstein , Mohsen Mosleh , Antonio A Arechar , Dean Eckles , and David G Rand . 2021 . Shifting attention to accuracy can reduce misinformation online . Nature 592 , 7855 ( 2021 ) , 590 – 595 . [ 76 ] MartinPinquartandRubinaKauser . 2018 . Dotheassociationsofparentingstyleswithbehaviorproblemsandacademic achievement vary by culture ? Results from a meta - analysis . Cultural Diversity and Ethnic Minority Psychology 24 , 1 ( 2018 ) , 75 . [ 77 ] Jessica Taylor Piotrowski , Matthew A Lapierre , and Deborah L Linebarger . 2013 . Investigating correlates of self - regulation in early childhood with a representative sample of English - speaking American families . Journal of child and family studies 22 ( 2013 ) , 423 – 436 . [ 78 ] Thomas G Power . 2013 . Parenting dimensions and styles : a brief history and recommendations for future research . Child . Obes . 9 Suppl , s1 ( 2013 ) , 14 – 21 . [ 79 ] Karen Pyke . 1999 . The micropolitics of care in relationships between aging parents and adult children : Individualism , collectivism , and power . Journal of Marriage and the Family 61 , 3 ( 1999 ) , 661 – 672 . [ 80 ] Yim Register , Lucy Qin , Amanda Baughan , and Emma S . Spiro . 2023 . Attached to “The Algorithm” : Making Sense of Algorithmic Precarity on Instagram . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 563 , 15 pages . [ 81 ] Julio Reis , Philipe Melo , Kiran Garimella , and Fabrício Benevenuto . 2020 . Detecting Misinformation on WhatsApp without Breaking Encryption . Association for the Advancement of Artificial Intelligence 0 ( 2020 ) , 5 pages . [ 82 ] Julio C . S . Reis , Philipe de Freitas Melo , Kiran Garimella , and Fabrício Benevenuto . 2020 . Can WhatsApp Benefit from Debunked Fact - Checked Stories to Reduce Misinformation ? [ 83 ] Wei Ren and Xiaowen Zhu . 2022 . Parental mediation and adolescents’ internet use : the moderating role of parenting style . Journal of youth and adolescence 51 , 8 ( 2022 ) , 1483 – 1496 . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . 28 Farhana Shahid , Dhruv Agarwal , and Aditya Vashistha [ 84 ] Gustavo Resende , Philipe Melo , Julio C . S . Reis , Marisa Vasconcelos , Jussara M . Almeida , and Fabrício Benevenuto . 2019 . Analyzing Textual ( Mis ) Information Shared in WhatsApp Groups . In Proceedings of the 10th ACM Conference on Web Science . ACM , NY , USA , 225 – 234 . [ 85 ] World Population Review . 2023 . WhatsApp Users by Country 2023 . Retrieved September 4 , 2023 from https : / / worldpopulationreview . com / country - rankings / whatsapp - users - by - country [ 86 ] Patrícia Rossini , Jennifer Stromer - Galley , Erica Anita Baptista , and Vanessa Veiga de Oliveira . 2021 . Dysfunctional information sharing on WhatsApp and Facebook : The role of political talk , cross - cutting exposure and social corrections . New Media & Society 23 , 8 ( 2021 ) , 2430 – 2451 . [ 87 ] Minna Ruckenstein and Linda Lisa Maria Turunen . 2020 . Re - humanizing the platform : Content moderators and the logic of care . New Media & Society 22 , 6 ( 2020 ) , 1026 – 1042 . [ 88 ] BR Sahithya , SM Manohari , and Raman Vijaya . 2019 . Parenting styles and its impact on children – a cross cultural review with a focus on India . Mental Health , Religion & Culture 22 , 4 ( 2019 ) , 357 – 383 . [ 89 ] Christian Schwarzenegger and Anna JM Wagner . 2018 . Can it be hate if it is fun ? Discursive ensembles of hatred and laughter in extreme right satire on Facebook . Studies in Communication and Media 7 , 0 ( 2018 ) , 473 – 498 . [ 90 ] Lauren Scott , Lynne Coventry , Marta E . Cecchinato , and Mark Warner . 2023 . “I Figured Her Feeling a Little Bit Bad Was Worth It to Not Spread That Kind of Hate " : Exploring How UK Families Discuss and Challenge Misinformation . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 660 , 15 pages . [ 91 ] Joseph Seering and Sanjay R . Kairam . 2022 . Who Moderates on Twitch and What Do They Do ? Quantifying Practices in Community Moderation on Twitch . Proc . ACM Hum . - Comput . Interact . 7 , GROUP , Article 18 ( 2022 ) , 18 pages . [ 92 ] Joseph Seering , Geoff Kaufman , and Stevie Chancellor . 2022 . Metaphors in moderation . New Media & Society 24 , 3 ( 2022 ) , 621 – 640 . [ 93 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping Pro and Anti - Social Behavior on Twitch Through Moderation and Example - Setting . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . ACM , NY , USA , 111 – 125 . [ 94 ] JosephSeering , TonyWang , JinaYoon , andGeoffKaufman . 2019 . Moderatorengagementandcommunitydevelopment in the age of algorithms . New Media & Society 21 , 7 ( 2019 ) , 1417 – 1443 . [ 95 ] Alexandra A Siegel and Vivienne Badaan . 2020 . # No2Sectarianism : Experimental approaches to reducing sectarian hate speech online . American Political Science Review 114 , 3 ( 2020 ) , 837 – 855 . [ 96 ] Nadia Sorkhabi . 2005 . Applicability of Baumrind’s parent typology to collective cultures : Analysis of cultural explanations of parent socialization effects . International Journal of Behavioral Development 29 , 6 ( 2005 ) , 552 – 563 . [ 97 ] Sharifa Sultana , Pratyasha Saha , Shaid Hasan , S . M . Raihanul Alam , Rokeya Akter , Md Mirajul Islam , Raihan Islam Arnob , A . K . M . Najmul Islam , Mahdi Nasrullah Al - Ameen , and Syed Ishtiaque Ahmed . 2022 . Imagined Online Communities : Communionship , Sovereignty , and Inclusiveness in Facebook Groups . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 407 ( 2022 ) , 29 pages . [ 98 ] Martin Valcke , Sarah Bonte , Bram De Wever , and Isabel Rots . 2010 . Internet parenting styles and the impact on Internet use of primary school children . Computers & Education 55 , 2 ( 2010 ) , 454 – 464 . [ 99 ] Rama Adithya Varanasi , Joyojeet Pal , and Aditya Vashistha . 2022 . Accost , Accede , or Amplify : Attitudes towards COVID - 19 Misinformation on WhatsApp in India . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 256 , 17 pages . [ 100 ] Rama Adithya Varanasi , Aditya Vashistha , and Nicola Dell . 2021 . Tag a Teacher : A Qualitative Analysis of WhatsApp - Based Teacher Networks in Low - Income Indian Schools . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , Article 560 , 16 pages . [ 101 ] Santosh Vijaykumar , Daniel T Rogerson , Yan Jin , and Mariella Silva de Oliveira Costa . 2021 . Dynamics of social corrections to peers sharing COVID - 19 misinformation on WhatsApp in Brazil . Journal of the American Medical Informatics Association 29 , 1 ( 2021 ) , 33 – 42 . [ 102 ] Leijie Wang , Ruotong Wang , Sterling Williams - Ceci , Sanketh Menda , and Amy X . Zhang . 2023 . “Is Reporting Worth the Sacrifice of Revealing What I’ve Sent ? " : Privacy Considerations When Reporting on End - to - End Encrypted Platforms . In Proceedings of the Nineteenth USENIX Conference on Usable Privacy and Security . USENIX Association , USA , Article 27 , 18 pages . [ 103 ] WhatsApp . 2023 . How to pin a message . Retrieved January 6 , 2024 from https : / / faq . whatsapp . com / 294619079641794 / [ 104 ] WhatsApp . 2023 . India Monthly Report under the Information Technology ( Intermediary Guidelines and Digital Media Ethics Code ) Rules , 2021 . Retrieved September 8 , 2023 from https : / / www . whatsapp . com / legal / india - monthly - reports [ 105 ] Philippa Williams , Lipika Kamra , Pushpendra Johar , Fatma Matin Khan , Mukesh Kumar , and Ekta Oza . 2022 . No room for dissent : Domesticating WhatsApp , digital private spaces , and lived democracy in India . Antipode 54 , 1 ( 2022 ) , 305 – 330 . [ 106 ] Pamela J . Wisniewski , Heng Xu , Mary Beth Rosson , and John M . Carroll . 2014 . Adolescent Online Safety : The “Moral " of the Story . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing . , Vol . 1 , No . 1 , Article . Publication date : January 2024 . One Style Does Not Regulate All 29 ACM , NY , USA , 1258 – 1271 . [ 107 ] Donghee Yvette Wohn . 2019 . Volunteer Moderators in Twitch Micro Communities : How They Get Involved , the Roles They Play , and the Emotional Labor They Experience . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , 1 – 13 . [ 108 ] Sarita Yardi and Amy Bruckman . 2011 . Social and Technical Challenges in Parenting Teens’ Social Media Use . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , NY , USA , 3237 – 3246 . [ 109 ] Elaine Yong . 2019 . Impact of Student - Instructor Relationships on Affective Learning and Test Anxiety Perceptions . In Proceedings of the 3rd International Conference on Education and Multimedia Technology . ACM , NY , USA , 160 – 164 . [ 110 ] Bingjie Yu , Joseph Seering , Katta Spiel , and Leon Watts . 2020 . " Taking Care of a Fruit Tree " : Nurturing as a Layer of Concern in Online Community Moderation . In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems . ACM , NY , USA , 1 – 9 . [ 111 ] Pengfei Zhao , Natalie N Bazarova , Dominic DiFranzo , Winice Hui , René F Kizilcec , and Drew Margolin . 2023 . Standing up to problematic content on social media : which objection strategies draw the audience’s approval ? Journal of Computer - Mediated Communication 29 , 1 ( 2023 ) , 13 pages . [ 112 ] Rui Zhou , Zhonghe Wen , Muchao Tang , and Betsy DiSalvo . 2017 . Navigating Media Use : Chinese Parents and Their Overseas Adolescent Children on WeChat . In Proceedings of the 2017 Conference on Designing Interactive Systems . ACM , NY , USA , 1025 – 1037 . , Vol . 1 , No . 1 , Article . Publication date : January 2024 .