GroupWork close up : A comparisonof the Group Design Process With and Without a Simple Group Editor JUDITH S . OLSON and GARY M . OLSON The University of Michigan and MARlANNE STORR @ STEN Norwegian Computing Center and MARK CARTER The University of Michigan A simple collaborative tool , a shared text editor called ShrEdit , changed the way groups of designers performed their work , and changed it for the better . First , the designs produced by the 19 groups of three designers were of higher quality than those of the 19 groups who worked with conventional whlteboard , paper and pencil . The groups with the new tool reported liking their work process a little less , probably because they had to adapt their work style to a new tool We expected , from the brainstorming literature and recent work on Group Support Systems , that the reason the designs were of better quality was that the supported groups generated more ideas . To our surprise . the groups working with ShrEdit generated fewer design ideas , but apparently better ones . It appears that the tool helped the supported groups keep more focused on the core issues in the emerging design , to waste less time on less important topics , and to capture what was said as they went . This suggests that small workgmups can capitalize on the free access they have to a shared workspace , without requiring a facilitator or a work process embedded in the software . Categories and Subject Descriptors : H . 1 . 2 [ Information Systems ] : User / Machine Systems—h uman factors : hu ? nan m ~ ornzafzon processing ; H . 5 . 3 [ Information Interfaces and Presentation ] : Group and Orgamzation Interfaces—evaluation / methodology ; synchronous m - interaction General Terms : Design , Human Factors Additional Key Words and Phrases : Collaboration , concurrent editing , face - to - face work , group support system , groupware , small group behavior This work has been supported by the National Science Foundation ( grant IRI - 8902930 ) , by the Center for Strategic Technology Research ( CSTaR ) at Andersen Consulting , by Arthur Andersen & Co , , by Apple Computer , Corp . , and by Steelcase , Inc . Authors’ addresses : J . S . Olson and G , M . Olson , Cognitive Science and Machine Intelligence Laboratory , the University of Michigan , 701 Tappan Street , Ann Arbor , MI 48109 - 1234 ; email : olson { i csmil . umich . edu ; M . Storr @ sten , Norwegian Computing Center , Oslo , Norway ; M . Carter , Information Technology Division , the University of Michigan , Ann Arbor , MI 48109 , Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage , the ACM copyright notice and the title of the publication and its date appear , and notice is given that copying is by permission of the Association for Computing Machinery , To copy otherwise , or to republish , requires a fee and / or specific permission . @ 1993 ACM 1046 - 8188 / 93 / 1000 - 0321 $ 01 . 50 ACM Transactions cm Information Systems , Vol . 11 , No 4 , October 1993 , Pages 321 - 348 322 . Judith S . Olson et al . INTRODUCTION When a group takes on a task , they use a variety c ) f work modes to get the job done . One important mode is scheduled face - to - face interactions , or meetings . In our culture these have a huge range of different formats and purposes [ Schwartzman 1989 ] . In the past several decades it has occurred to many people that networked computing might be used in some way to make meetings work better . For instance , Engelbart and English [ 1968 ] experi - mented with networked computing to support meetings . However , in order to understand how such collaborative technology might aid in the meeting process , we need to be explicit about how meetings work , what is good and bad about them , and how technology might play a role . Next , we look at some of the research on meeting support , where some consistent findings have been reported in recent reviews of the literature . Finally , we describe the design of a simple technology that we felt , on the basis of our analysis , would provide useful support for a particular kind of meeting . The body of this article then reports an empirical evaluation of this technology , with new detailed analyses and surprising conclusions . Since meetings are so diverse in their purposes and processes , we focus on one kind of important meeting , that of a workgroup whose focus is an intellectual task that must be done collaboratively . The key ingredients are that some kind of complex , interconnected intellectual product is called for , that a small group is required to do it , and that meetings are venues where significant work takes place . But there are many other kinds of meetings of this general type . Other kinds of meetings include : project management meetings , review meetings with managers , presentations to clients , etc . But we focus on the meetings where significant collaborative work takes place . In particular , our work focuses on the intellectual task of exploring and choosing among design options . There are many reasons why workgroups get together for such meetings . Workgroups are often put together because the members have different expertise or experience , or because they bring different points of view to bear on the task . Meetings are a good way in which to bring out these factors . Similarly , the group’s task may be such that it is divided up among the members of the group , and meetings are used to merge what they have done independently . There may be a number of alternative solutions or options that need to be explored for the task , including many sets of interrelated options , and the group uses meetings to explore and decide which configura - tions to adopt . Face - to - face meetings provide the group members with a useful array of physical and social tools to aid in the coordination of the groups work . Visual and verbal presentation together allows dimensions of clarity that written presentation does not afford ; there are tones of voice , gestures , and facial expressions that correlate with the intended meaning of the words , and add clarity . Furthermore , the interactivity is important . Participants can gauge their presentation to the others’ gestures and comments to show whether they understand and / or agree with them . Physical presence implies certain ACM Transactions on Information Systems , Vol 11 , No 4 , October 1993 Groupwork Close Up . 323 kinds of engagement with the encounter and / or the outcome . While in the same room , one can more likely command another’s attention than by mailing a document and hoping it will be read and responded to . Because of the engaged participation in a face - to - face meeting , it is assumed that important issues , ideas , and evaluations might surface and be understood , resulting in better quality decisions , designs , or actions . Meetings are thus usually an essential aspect of task - oriented group work . However , there are well - known costs associated with meetings . The sharing of ideas and actions incurs some of the essential coordination costs associated with working together . There are also a variety of process losses [ Diehl and Stroebe 1987 ; Forsythe 1990 ] : free riding , where some members of the group do not work as hard as others ; production blocking , where the social stricture of one person talking at a time means that any given person has less opportunity to contribute ; groupthink , where fewer options are considered or evaluated because of the social processes in face - to - face interactions ; evalua - tion apprehension , where people are inhibited in participating because of ( true or false ) perceptions of what others may think of them . The physical constraints of shared workspaces ( e . g . , whiteboards ) can stop the generation of ideas or make their reorganization difficult . Private notes and annotations can variously misrepresent the discussions , resulting in coordination difficul - ties since group members have different records of what happened . Given the importance of face - to - face meetings in carrying out intellectual tasks , a number of efforts have been made to make such meetings work better . One can purposely design the size , composition , working conditions , and specific task assignment [ Hackman 1987 ] . Similarly , the organizational context can be shaped to make the work of teams more effective by variations in reward structures , educational systems , and information systems . A wide variety of methods have been offered to overcome the various process losses associated with group work [ Forsythe 1990 ] , such as brainstorming , Delphi methods , Nominal Group Technique ( NGT ) , root cause analysis , and stake - holder analysis . Consultants , facilitators , and trainers can be used to orches - trate group processes in various ways . Much of the quality movement focuses on getting groups to work more effectively . Given the importance of meetings for work groups and the broad range of factors that can reduce their effectiveness , it is scarcely surprising that many researchers have explored the role of computing technology , particularly , in its networked form , as an important way of enhancing the effectiveness of face - to - face meetings . There are a number of aspects of task - oriented meet - ings that can be supported by computers . Whiteboards , flip charts , paper handouts , and slides can be replaced by computer - based displays . Such displays can offer easier editing , a more extensive virtual workspace , and a unified record of meeting accomplishments . Screen - based computer tools with keyboards and mice do not reproduce the size and informality of many of the traditional media such as whiteboards ( though this too is changing ; see [ Penderson et al . 1993 ] ) . There are many potentially useful features that can be added to such computer - based displays : full access to all group members , ACM Transactions on Information Systems , Vol . 11 , No . 4 , October 1993 324 . Judth S . Olson et al easy incorporation of material from outside of the immediate meeting context , and varying degrees of structure ( outline , hypertext , etc . ) to the shared workspace . Computing can also be used to incorporate several of the process support systems that have evolved . Many of the group decision support systems have built in to them such formal meeting procedures as brainstorm - ing ( including NGT ) and voting . Given this wide range of options , how can we know what will be effective ? It is easy to imagine that several of the design options available for computer support could make groups less effective . The present state of theoretical development about work group effectiveness makes it difficult to predict in advance the effects of various computer tools for face - to - face meetings . To further complicate matters , even if we had ideas about what kinds of func - tions could support group work , our understanding about how to build interfaces to support groups is quite primitive [ Olson et al . 1993a ] . There have been empirical studies of some groupware systems , but the results vary , principally because the design features of the systems differed widely and because the types of things given the participants to do were not directly comparable . Several recent reviews , however , have tried to extract some regularities from the empirical studies of groupware for face - to - face synchronous interactions . McLeod [ 1992 ] carried out a meta - analysis [ Rosenthal 1984 ] of studies published during the 1980s that looked at the effects of group support systems on various aspects of human performance . She was somewhat con - strained in the selection of studies by the data requirements of a meta - analy - sis , but was still able to locate thirteen appropriate studies . She found that group support systems ( many of them focusing on group decision making ) : —increased decision quality , —increased equality of participation , —increased degree of task focus , —decreased consensus , —decreased satisfaction , and —increased the time needed to make a decision . Interestingly , a similar though more informal review of a superset of this literature [ Hollingshead and McGrath 1993 ] reached very similar conclusions about improved outcome quality , longer time to complete a task , and less consensus . However , the studies included in this second review were more equivocal about the effects on satisfaction . Also , they noted that improved quality came from software that provided extensive structuring of the group’s behavior , and it appeared that the improvement may have been due more to the structuring than to other features of the software . Many of these groupware systems are embeded with a process the partici - pants are expected to follow . For example , in the Capture Lab at General Motors , the facilitator begins by naming a single issue that the group agreed to discuss . This issue is often the key issue of the meeting , and the partici - ACM Transactluns m Information Systems , Vol 11 , No 4 , October 1993 Groupwork Close Up . 325 pants have agreed beforehand that they will meet to discuss this issue . Ideas are then elicited from all group members , who type them using their individ - ual workstations anonymously . The ideas are then sent to the common , projected screen to be viewed by all . After a period of such brainstorming , the facilitator then moves the group to another application that allows group members to organize these same ideas . In this setting , the group suggests how to organize the ideas , but the actual manipulation on the central screen viewed by all is done by a scribe . After organizing and perhaps editing to reword some of the ideas , the group members are then led through a vote , using another tool . This tool preserves anonymity , encouraging all to state their true preferences without fear of reprisal or reproach . Taking this system as an example , one can see how the group’s resulting decision might be higher quality because it generates more ideas to consider , and there is more equal participation . Since the facilitator is directing the group’s attention to the issue at hand , there is an increased task focus . However , this format of the procedure could also contribute to peoples’ feeling of decreased consensus and satisfaction : people may feel they did not consider the ideas fully , since they may not have been encouraged to clarify or argue about the appropriate decision criteria . Since things are structured for the participants , the time to come to a decision may be longer or shorter , depending on the complexity of the issue at hand . Thus , this system seems to capture a number of features that can enhance the decision , working mainly through control of attention , the number and quality of the ideas generated , and the amount of consideration in evaluating the set of ideas . The fact that the process the group goes through is struc - tured , however , may either help or hinder the speed with which the work gets done . There are many meetings where , for example , this process might not fit . In a free - flowing meeting where issues are not fully understood or articulated ( such as a policy meeting ) or where many of the ideas interrelate in complex ways ( such as in software design meetings ) , such a structured process might significantly interfere . What is called for is a more systematic analysis of the features that are included in the software tool , the features of the technique or process that people are intended to follow , if any , and the type of activity that the task given to the group has to engage in to be successful [ Olson et al . 1992a ] . We can accumulate knowledge to direct the building and use of effective meeting support only when we have specification of these aspects of the situation and then careful analysis of the details of the interaction that follows , as well as an assessment of the quality of the outcome ( product , decision , actions ) . In simple terms , the study reported here analyzes groups of three people engaged for an hour an a half in the task of designing features of a complex device , using a simple shared editor ( expandable and editable workspace ) that has equal access by all participants ( anyone can type anywhere at any time ) , independent views of the work ( each person can scroll anywhere ) without any embedded process structure . The tool was developed with the idea that it would allow the group to benefit from those aspects of a face - to - face ACM Transactions on Information Systems , Vol . 11 , No 4 , October 1993 326 . Jud ! th S Olson et al . encounter that seem to be important , while allowing freedom of action that is not available today in traditional meetings . That is , face - to - face meetings allow interactive channels that allow presentation with necessary clarifica - tion and commitment of attention and resolution that come from being with others in the same room . The tool we built and describe more fully below , allows parallel , concurrent , unstructured access , enabling group members to capture and organize the ideas as they emerge . The purpose of this study is to examine in detail whether and how the tool changes the process the group members engage in and the quality of the output they produce . Our analysis of the effects of various kinds of groupware in the empirical studies reviewed , as well as our own observations of group work in field settings [ Olson et al , 1992b ] led us to construct a very simple groupware tool called ShrEdit for Shared Editor [ McGuffin and Olson 1992 ] . The key features of this software is that it : —allows all group members to enter their ideas and edit them as they wish at any time . People can enter within one character of the point at which others are entering material , —allows infinite expansion of the workspace , —allows group members to independently view any part of the collected work on the public work space , and —has no embedded process that dictates the group’s activities from moment to moment . ShrEdit is intended to provide a second channel for communication , one that can occur in parallel with others’ use of ShrEdit and in background to the normal conversation that occurs in meetings . This second channel was ex - pected to have several effects on the process and the outcome of the group work : —allow greater production of ideas from two sources : ( a ) since one can type while another speaks , or all type at once to put ideas down , more ideas can be captured ; ( b ) since there is no limit to the workspace , people continue to generate ideas until time or mental limits are reached . —allow more evaluation to take place for three reasons : ( a ) more attention can be focused on ideas that are captured ; ( b ) since there is no lock - step procedure that has to be followed by all , one can consider early ideas even up to the last minute ; ( c ) since ideas captured can be edited and moved around for various organizations , new ideas can be generated , and new evaluations can take place . —by allowing more channels of communication , attention can be allocated more flexibly to either what is being spoken or written , effectively increas - ing the attentional capacity of the group ( of course , it is possible to increase the information load too much ) . ACM Transactmns on Information Systems , Vol 11 , No 4 , October 1993 Groupwork Close Up . 327 To evaluate these conjectures about how ShrEdit would change what happens in a meeting , we presented 38 groups of 3 people with a design problem which they were to engage in for 90 minutes . Half the groups used normal meeting support such as whiteboards and paper and pencil . The other half of the groups used ShrEdit . Since all groups did the same task , and the task had a measurable performance criterion , we could compare the quality of the outcome of their work . Through questionnaires we measured their perceptions of their satisfaction with their work , the process , and the tool and the equality of participation . Finally , using the techniques developed in our field work [ Olson et al . 1992b ] , we analyzed their moment - by - moment talking and typing behavior to study the process of group work . We assessed the number of ideas they generated , the discussion that evaluated those ideas , and the general pattern of use of time . We expected the number of ideas and evaluation discussion to increase with ShrEdit because of the increased channel capacity from simultaneous access to the document , resulting in higher - quality output . 2 . METHOD Subjects Thirty - eight groups of three professionals who knew each other and had worked together before served as our subjects . Most of the groups consisted of three MBA studentsl enrolled in the Michigan Business School , with a few groups consisting of one MBA and two of that person’s work cohorts . All knew at least one Macintosh application . Groups were recruited en masse through various MBA classes and were encouraged to sign up at convenient times , either morning , afternoon , or evening sessions for the three - hour sessions . The groups were assigned to conditions at random as they came in , with minor corrections so that the two conditions were balanced for time of day and for the two experimenters who ran the training ( J . Olson and M . Storr @ ten ) . Setting The group members were seated in a meeting room called the Collaboration Technology Suite ( CTS ) [ Olson et al . 1992c ] , shown in Figure 1 , in which workstation units make up a conference table , with the monitors recessed in the table top so as not to interfere with normal eye contact . One wall of the room has two large panels of whiteboard , which all groups were allowed to use . In practice , all but two unsupported groups used it for collecting their ideas , typically in lists ; only one supported group used it , for a quick drawing of the proposed layout . All the sessions were videotaped . Nineteen of the groups used ShrEdit throughout the practice and target problems . They were taught the basic features of ShrEdit in about 15 ] Ninety percent of the MBAs at Michigan have significant work experience before coming back to school . These are professionals with practical group experience . ACM Transactions on Information Systems , Vol . 11 , No . 4 , October 1993 . 328 . Judith S . Olson et al . Fig . 1 The Collaboration Technology Suite , where the experiment took place minutes . We cautioned them that ShrEdit has only a limited set of editing and formatting features , then introduced the new feature of being able to enter and edit simultaneously within one character of each other , while being able to scroll through the document independently , They were told how to detect a collision and how to get out of each other’s way . They were also told how to find and track each other , features which allow coordination of views while several are talking about a particular part of the document . They were not told how to orchestrate themselves with brainstorming , organizing , etc . ; it was felt that they would discover their own style and perhaps discover new ways of working . We set up ShrEdit with two public windows and one private one . Figure 2 shows a typical screen Iayout . z At the bottom of the screen is a “status window , ” a subsidiary window that lists for each public window who else is in the session . In this window the names are also annotated with whether they have a selection point in the window and whether that group member is using the “find” or “track” function to coordinate with another group member . 2 Most groups left the window placement this way , though several used only one window , makmg it cover the entire screen so they could see most of the activity of the rest of the participants We take this as an interesting commentary on the need to be aware of what the other group members were doing at each moment , in an effort to both evaluate the emerging Ideas and add things that were consonant with what had been developed so far . ACM TransactIons on Information Systems , Vol 11 , No 4 , October 1993 OJiorlotu One AUTOMATIC POST OFFICE / hat IL should do : registered letters packages st ~ mps computer help display money orders overseas majl overnight letters 1overnight packages I computerized zip codes I tax returns I improved queuing ‘St frcey ’in “ttJinttotuOne” ind / Trock what : [ R Other Participants : flSelec ! ion q l ) lt ! uj + Daue1 ; Find Robert ( Track ( ~ Y @ = owns a selection Iteclaim Last o = tracking + = tracking you Window Two ~ reduced labor costs D reduced transaction costs @ customer convenience 24 hour service no Irritable employees to deal with better locations potentially shorter wait open on holidags and Sundags What are the cons ( reasons to scrap this idea ) : Dcost of installation . more postal vehicles , if cannot add to existing routes o security risk = Inltlal customer confusion Other Items to investigate : e current technology – availability and cost Find / Track cuhat : E selection q O ( t > u } [ Find 1 [ Track 1 ( Iit ? cloim 10s1 1 Priuate Noies : Hake sure to br Ing up the po Int & out coordlnat ing eng Ineer Ing and devel opmeni activities within I Imi ted time frame . REm [ nd group of cost factors fq . each add ! i 10naI funct Ion Other Particioaats : M Q = owns a selection o = tracking + = tracking you 1 Fi ~ . 2 . A screen dump of a session on ShrEdit , with two public windows and a private window . The boxes at the bottom — - 0 . indicate whether other group members are tracking the user or not , 330 . Judith S , Olson et al , Task Our groups all engaged in an hour and a half of design . In terms of McGrath’s [ 1984 ] task taxonomy , this activity consists of various episodes of Planning , Creativity , Decision - Making , and Cognitive Conflict . In Steiner’s [ 1972 ] terms this is an “integrative” task ; the participants were assumed to have to do their work together throughout , not be able to divide the work into indepen - dent subparts and merely attach them together to form a whole . Our groups were instructed to draft the initial requirements definition for an Automatic Post Office ( APO ) , a collection of postal services offered through a stand - alone device similar to an ATM . It was to be designed under the conditions that a prototype could be built by their fictitious company of 30 people in a year . They were instructed to determine the core services they would offer , some of the required equipment , how it would work , the rough cost / benefit analysis , and a list of things they would like to investigate before the next time they would ( hypothetically ) meet . They were given 1 – 1 / 2 hours to complete the assignment , producing meeting notes that could be read by a ( fictitious ) additional group member who could ( allegedly ) not attend that day’s meeting . Procedure The subjects came to the CTS for a single three - hour period , broken into two roughly 1 – l \ 2 - hour portions with a 15 - minute break in between . In the first half , the subjects filled out background questionnaires and permission forms and solved two small problems . These tasks served both to allow the subjects to warm up to the task situation and , for those in the supported condition , to learn the software and adjust to the software’s capability for simultaneous editing . Those taught ShrEdit were instructed in a 20 - minute period before the two small problems . The instructions demonstrated the system’s capabili - ties and how to control them , but did not prescribe how to use these features to support work . Those in the control , Unsupported condition were encour - aged to use paper and pencil or whiteboard in whatever way seemed natural for the group . Following a 15 - minute break , the groups did the APO task in a single 1 – l / 2 - hour sitting . All groups filled out the questionnaire on satisfac - tion after the session . Measures Our goal was to assess the quality of the product , the participants’ satisfac - tion with the process , and the process of design and coordination . The quality of the design . We assessed how the technology affected the groups’ work products by measuring the quality of the APO design , as reflected in the final document of each group . To make fair comparisons possible , the Unsupported groups’ written outputs were typed in the same style as the Supported groups’ output . We developed a quality measure after extensive discussion with both designers and researchers of design . Three major aspects of the groups’ ACM Transactions on Information Systems , Vol 11 , No 4 . October 1993 Groupwork Close Up . 331 output were scored : ( a ) how completely the output covered all the aspects of the assigned task , ( b ) the ease of understanding of the ideas reported in the document , and ( c ) the judged quality of the post office design , including the feasibility of producing a prototype of’ the suggested post office within the stated time and manpower constraints , the coherence of the ideas , and the judged success of the ideas if marketed . Each aspect was then detailed further , and a rating form was constructed , on which a perfect score totaled 80 points ; actual scores ranged from 29 – 78 . Six researchers and designers then scored the output from the same six meetings , using this instrument . The average pairwise correlation between raters was . 85 . Since this was well above the acceptable range for reliability of measures , one researcher , who had one of the highest intercorrelations with the others , then coded the quality of the designs generated in the remaining 32 meetings using the same instrument . Satisfaction and perceptions of participation . We constructed a possess - ion questionnaire that asked the participants to ( a ) rate their satisfaction with the process that they used ( adapted from Gouran et al . [ 1978 ] and Green and Taber [ 1980 ] ) , as well as with the design result [ Green and Taber 1980 ] , ( b ) assess the evenness of the participants’ contributions [ Green and Taber 1980 ] , and ( c ) identify a leader if one emerged . Process . The transcripts of the meetings were coded for what kinds of activity were taking place at each moment , using the categories devised and tested in our study of field design meetings [ Olson et al . 1992b ; 1993 b ] . These categories reflect key aspects of design activity , with origins in the Design Rationale literature [ Moran and Carroll 1993 ] . In particular , we separated statements of the Issues at hand , the Alternatives or solutions raised , and the Criteria brought to bear on that idea . We also cataloged the time it took the group to organize itself ( activities we call Meeting Management , Project Management , Summary , and Walkthrough ) , to Clarify their ideas , or engage in side Digressions . The division of activity into task and process manage - ment was inspired by the categories developed in similar studies of group activity [ Putnam 1983 ; Zigurs et al . 19881 . One new category was required to account for the specific activities associ - ated with the task : time when the group would plan the organization and wording of the final document or dictate the words , called Plan and Write . In the Supported groups , we required yet another two categories : time when they were confused about something having to do with the technology and other comments about the placing of their work into the windows on the screen . We called these Technology Confusion and Technology Management , respectively . Since Clarification was divided into the type of topic that was being clarified ( e . g . , Clarification Issue , Clarification Alternative , etc . ) and there was a necessary Other , the final set of categories of behavior totaled 25 . Interrater reliability of these categories was measured in two ways . A strict measure of the correspondence of categorization is assessed second by second ; our interrater reliability at this granularity is 0 . 68 % , with a Cohen’s k = 63 % ACM TransactIons on Information Systems , Vol 11 , No . 4 , October 1993 . 332 . Judith S . Olson et al [ Cohen 1960 ; Rosenthal 1973 ; Tinsley and Weiss 1975 ] . If we look at the summary measures used , such as the total time spent in each category , the correlation between the two raters’ categorizations is . 97 . All of these mea - sures are well within the accepted range for coding categories for behavioral analysis . We also analyzed the groups for the breadth and depth of their discussions . To do this , we annotated the transcripts for which particular topic they were discussing at each moment . From this annotation , we constructed a Design Rationale ( DR ) diagram , which connects the topic or Issue , the Alternative solutions to the question posed , and the evaluative comments ( Criteria ) raised both for and against each proposed solution . Figure 3 shows an example of part of a DR , with its connected issues , alternatives , and criterial comments . The notation is similar to that developed by researchers who advocate the use of a notation scheme to capture the aspects of design while it is happening [ Moran and Carroll 1993 ] . We use it instead to code the development of natural design activity . In our study , this is not a diagram that the group members construct for themselves ; it is one the experimenters construct after the event on the basis of what members said , as written in the transcript . Since these annotations ignore when each event occurred and how many times the same thing was mentioned , we can count the number of unique issues that are raised in the meeting , the number of ideas or alternatives generated per issue , and the evaluative criteria brought to bear on the alternatives . Thus , we have a detailed look at the content of the design discussions , its breadth , and depth . 3 . RESULTS The results are organized by the major classes of measures as described above : outcome , satisfaction , and process . But two things precede the report of the main results . First , to support the claim that the subjects in the two conditions were not distinct , we examine their responses to the initial back - ground questionnaire . And second , since this laboratory work was motivated by work in the field , and we wish to generalize back to the field , we examine the correspondence between the measures of process taken from our field work and those found in our unsupported laboratory condition . Subjects : Comparison of the Participants in the Groups across the Two Conditions The preexperimental questionnaires asked participants to rate a number of features of their individual and group experience . The individuals in both conditions rated themselves at the same level of computer experience ( “regu - lar to heavy , ” 4 . 6 on a 5 - point scale ) and were equally comfortable working with computers ( 4 . 6 on a 5 - point scale ) . The groups did differ slightly on how much experience they reported having had working in groups in general ; the people in the supported groups reported having had more experience ( 2 . 2 versus 1 . 8 on a 5 - point scale , t ( 108 ) = 2 . 18 , p < . 03 ) than those in the ACM TransactIons on Information Systems , Vol . 11 , No 4 , October 1993 Groupwork Close Up . 333 Issue Alternative Criteria PO lobby maintenance easier I ~ : : mit Woulcibe more convenient ‘ costly Enclosed in another shelter Fig . 3 . A Design Rationale diagram , in which issues are shown on the left , connected to the alternatives raised at any time in the meeting , connected in turn to the criteria raised to evaluate those alternatives . The dotted lines indicate negative evaluations , the solid the positive evaluations . Unsupported groups . However , since this difference disappears when we test at the group level as opposed to individual person level , since it was the one significant t - test out of 11 questionnaire items , and since it was not corre - lated with quality , we consider this spurious . The Task : Comparison of Unsupported Work in the Laboratory and the Field The laboratory task we chose was intended to be similar to early require - ments tasks we examined in the field [ Olson et al . 1992b ] . We knew a priori that the laboratory work differed from the field in some ways : the partici - pants worked on this task for only one meeting ; they were not concerned with their performance for career advancement ; and they did not actually have to spend the company’s money to make the post office they designed . However , we did study groups who knew each other and had worked together , and the task was similar to tasks in the field in that it required an examination of features to offer the user , how the device could work , and an assessment of feasibility in terms of resources and benefit to the client . To assess the correspondence of the core design behaviors between the lab and the field , we compared our groups in the unsupported laboratory condi - tion with the field groups on two key measures : the amount of time they spent doing various aspects of design versus managing their group process , and the breadth and depth of their discussions . In general , we found that the distribution of time spent in various categories of behavior in the laboratory task matched that in the field , with a few explainable exceptions . ACM Transactions on Information Systems , Vol . 11 , No . 4 , October 1993 . 334 . Judith S . Olson et al , This correspondence is depicted in Figure 4 . This diagram shows the time and transition patterns for activity in the two settings side by side . Each circle represents the proportion of the total amount of time that is spent in each category of activity . The white area represents time when someone is making an initial statement , and the black represents time in which someone asks a question about it and another answers it , called “clarification . ” The arrows represent the frequent transitions from one category to the next , with the thicker arrows representing higher - frequency transitions . All transitions that occurred above 0 . 7 % of the time are shown ; the others are deleted in the interest of clarity . There are differences . There was virtually no time spent doing project management in the lab , since the groups met only once . They spent less time in clarifying the ideas in the lab study , presumably because the participants were solving a problem for which they have nearly equal expertise . And , since the lab groups were required to write up a document , they spent a significant portion of their time doing this , about 12 % . The correlation between the lab and field groups for 22 base categories of activity is . 62 , using figures that exclude laboratory groups’ writing time ; with “project management” and all “clarification” time removed , the correlation rises to . 69 . A comparison of the overall issue - alternative - criteria diagrams shows that the lab groups raised about the same number of issues per hour in the 2 situations and made similar kinds of evaluative arguments . The lab groups explored the issues more broadly , with 7 alternatives raised per issue in the lab and only 2 . 5 in the field . Since the laboratory task had people generate lists of features , benefits , and costs , it is reasonable that they would consider more such alternatives . Thus , although there are some differences , they are explainable from a deeper consideration of the settings in which the meetings took place . Since the use of time for the core design discussion was correlated , we are satisfied that the problem - solving aspects of these two tasks are roughly comparable . Whatever we find in the lab in terms of effects of technology on group problem - solving process has a chance of relating back to the field . Outcome Differences Two key measures of the output of the session are shown in Table I : the quality , as described above , and the length of the document . The average judged quality of the groups supported by ShrEdit was significantly higher than that for conventional groups ( t ( 36 ) = 2 . 32 , p < . 03 ) . Interestingly , the distribution of quality scores for the unsupported groups was far wider than the supported groups ( F ( 18 , 18 ) = 2 . 83 , p < . 05 ) . Use of the technology signif - icantly raised the lower tail of the distribution . On average , the groups using ShrEdit produced final documents that were twice as long ( t ( 36 ) = 2 . 78 , p < . 01 ) . This time it was the variance of the Supported groups that was higher than the Unsupported groups ( F ( 18 , 18 ) = 5 . 69 , p < . 01 ) . Interestingly , the longest document was one produced by an Unsupported group that worked to write the final document in parallel . Overall , however , there was no correlation between length of document and ACM TransactIons on Information Systems , VOI 1I , N“ 4 , C ) ctoher 1993 Fig . 4 , Comparison of the use of time and the transitions among activities in the field and unsupported laboratory conditions . In this diagram , the size of the circles represents time spent , and the width of the arrows indicates the relative frequency with which that transition is made . The white areas inside the circles indicate the original statements in that category ; the black indicates questions and their responses . 336 . Judith S . Olson et al Table I . Analysis of Features of Groups’ Output Quality Length Supported Groups 64 , 0” 692 * words std dev 7 . 9 468 Unsupported Groups 55 . 5 391 words std dell 13 . 3 ‘ 196 . 2’” Significance tests at p < 05 are indicated with * , p < . 01 with ‘ * , attached to the higher of the means , In all cases , comparisons are made in columns , comparing Supported with Unsupported groups . For all analyses , Ns are 19 for each condition . quality ( r ( 38 ) = . 29 , n . s . ) , nor was there within each of the conditions ( Un - supported r ( 18 ) = . 12 , n , s . , Supported r ( 18 ) = . 27 , n . s . ) . The difference in document length merely means that the text explanations of the ShrEdit groups’ ideas were on average longer , either in sentence or paragraph form , whereas the output from Unsupported groups consisted mainly of bullet lists . Table II reports the results of an analysis of the content of the final documents in terms of the number of Alternatives recorded ( called “ideas” in common English , as we refer to them later ) and the Criteria ( or “reasons” ) mentioned . Both groups are nearly equivalent on these measures , neither difference being statistically significant . Once again , however , the variance among the Unsupported groups is significantly higher than that for the Supported groups ( F ( 18 , 18 ) = 4 . 15 , p < . 01 ) . There is no difference in the variances for Criteria . However , a closer look at the relationship between number of ideas ( Alternatives ) in the document and its final rated quality shows an interesting pattern . Combining the two conditions , there is an overall significant , positive relationship between number of ideas reported in the final document and the quality rating ( r ( 38 ) = . 51 , p < . 01 ) . Within the Unsupported group , the number of ideas in the final document was positively related to the rated quality ( r ( 18 ) = . 58 , p < . 01 ) , whereas in the Supported groups , it was not ( r ( 18 ) = . 18 , n . s . ) . The discrepancy between the t - test and these correlations is likely explained by the fact that the variance in the Unsupported groups’ numbers of alternatives was much higher than in the Supported groups . There was enough variance for a significant correlation , but too much across groups for a significant t - test . Satisfaction and Perceptions of Participation Analysis of the questionnaires revealed very few differences between the two conditions . In general , groups in both conditions were satisfied with the process they used in generating , discussing , and deciding on their design , and they liked their final post office design . However , the Unsupported groups were more satisfied with the quality of the discussion ( t ( 36 ) = 2 . 74 , p < . 01 ) , and they rated the problem - solving process to be more even and fair ( t ( 36 ) = 2 . 24 , p < . 03 ) . However , the groups did not differ in whether one individual influenced the outcome more than others . ACM TransactIons on Information Systems , Vol 11 . No . 4 , October 1993 Groupwork Close Up o 337 Table II . Analysis of the Content of Groups’ Final Reports Alternatives Criteria Supported Groups 56 . 9 8 . 3 std deu 10 . 8 8 . 3 Unsupported Groups 51 . 8 7 . 7 std deu 22 . 0”’ 7 . 9 Process Differences The groups in the two conditions differed in quality of their final output , and quality was correlated with number of ideas in the final document . Next , we examined the details of the processes the groups engaged in , looking for the activities that likely produced this quality difference . We report on the breadth and depth of their discussions , how the groups spent their time , how they moved among activities in the course of design . We then examine the relationship between what is said and what is written , and how ShrEdit was used . Ideas raised and evaluated in conversation . Table III shows the distribu - tion of issues , alternatives , and criteria the groups spoke about . We realize that this is not the only way people communicate their ideas ; we will examine later the ideas written ( either on the blackboard or in the draft documents ) . Some striking differences appear . Contrary to expectations , the Unsupported groups generated more issues and raised more alternatives than the Sup - ported groups did ( t ( 36 ) = 3 . 11 , p < . 004 ; t ( 36 ) = 2 . 24 , P < . 03 ) . The unsup - ported groups differed widely among themselves in the number of issues covered , the variance being significantly higher than that for the Supported groups ( F ( 18 , 18 ) = 3 . 42 , p < . 01 ) . Once again , the Supported groups are more similar to each other than the Unsupported groups . Neither the Alter - natives nor the Criteria variances were significantly different . Looking more closely , however , it turns out that the groups covered differ - ent issues . We looked separately at what we called Core and Ancillary issues . Core issues were the 10 issues that the problem statement prompted the groups to talk about . Ancillary issues were those the group brought up beyond those . For example , several groups spent a large amount of time discussing an appropriate name for the facility and the company ; others discussed at great length how to automate the internal operations of the post office . As shown in Table IV , groups in both conditions talked about nearly all the core topics , and did so equivalently . The Unsupported groups , however , raised and discussed twice as many ancillary issues ( t ( 36 ) = 3 . 08 , p < . 005 ) and twice as many ancillary alternatives ( t ( 36 ) = 2 . 51 , p < . 02 ) . Not only were the means different , the variances were higher for the Unsupported groups for both the number of issues raised and the alternatives considered ( F ( 18 , 18 ) 4 . 72 , P < . 01 for Issues ; 3’ ( 18 , 18 ) = 4 . 39 , P < . 01 for Alternatives ) . ACM TransactIons on Information Systems , Vol 11 , No . 4 . October 1993 . 338 . Judith S . Olson et al . Table III DetaAs of the Depth and Breadth of Design Dlscusslon Average Numbers of Issues Alternatives Criteria Supported 13 . 3 120 . 8 1285 std dew 3 . 02 2673 79 . 96 Unsupported 17 . 9” 144 . 6 ‘ 169 . 5 std del 5 . 59” 37 . 75 58 . 32 Table IV Details of the Depth and Breadth of Design Discussion for Core and Ancdlary TOPICS Average Numbers of Issues Alternatives Criteria Core Ancillary Core Ancillary Core Ancdlary Supported 9 . 1 4 . 2 1086 122 115 . 4 13 . 1 std deu 1 . 06 2 . 55 22 , 54 9 . 19 744 1315 Unsupported 9 . 4 8 . 5’ ‘ 120 . 0 24 . 6” 1472 224 std dec , 1 . 01 554” 30 . 03 19 . 26 ‘ 60 . 0 1656 Each of these ancillary issues was discussed narrowly ; each had an average of three alternatives and only one evaluative statement . In contrast , core issues in both conditions had on average 12 or 13 alternatives raised . How the time was spent . A second way in which these groups differed was the amount of time spent on each of the subactivities . We coded the tran - scripts of the spoken parts of the meetings , noting the kinds of activities they engaged in , and then summarized how much total time was spent in each activity . We also cataloged the transitions among these activities . Figure 5 shows a combined view of the total time and flow between activities , the Unsupported groups on the left , the Supported ones on the right . Table V lists the specific time for each activity type across conditions . These summaries indicate significant differences in both aggregate and individual activities . Comparable pictures of the times and transitions for field design groups can be found in Figure 3 . 3 Most categories of activity are very similar across the Supported and Unsupported groups . Of interest , about the same amount of time ( 4 % ) was spent in managing the flow of meetings in both conditions , and they planned and dictated the writing for about the same amount of time ( 11 % and 9 % for Supported and Unsupported groups , respectively ) . The ShrEdit groups spent 7 % of their time asking each other questions about the technology because they were unsure , confused , or having difficulty . 3Here , all relevant categories of activities are shown for the two lab conditions . There were fewer coded categories in the field studies . Notably , the field groups had no Technology Confusion or Plan and Write categories . Only the comparable categories are shown m the side - by - side view m Figure 3 Here all 26 categories are shown , ACM Transactions on Information Systems , Vol 11 , No 4 . October 1993 Fig . 5 , Comparison of the use of time and the transitions among activities in the Supported and Unsupported conditions 340 . Judith . S , Olson et al Table V . Comparison of the Percentage of Time m Various Task Categories Lines that are Starred and m Bold are Significantly Different at p < 001 Unsupported Supported Category Groups ‘5 , urn Groups Sum Issue 2 . 9 Clar Issue 0 . 4 Alternative 22 . 4 * Clar AM 4 . 6 % Criteria 21J . Clar Criteria 1 . 5 * 53 . 0’ 2 . 0 0 . 3 14 . 9 2 . 1 12 . 6 0 ! 3 32 . 2 General Clar Artifact Clar Summary Clar Summary Walkthrough Clar Walk Goal Clar Goal Project Management Clar Proj Mgmt Meeting Mgmt Clar Meeting Mgmt Dlgresslon Clar DigressIon Plan and Write Manage Technology Technology Conf Other Clar Other 0 . 8 0 . 1 3 . 1 * 0 . 0105 * 0 . 0 1 . 9 00 00 0 . 0 45 0 . 1 12 . 0 0 . 1 8 . 7 0 . 1 0 . 0 0 . 6 0 . 0 Pause 13 . 2 0 . 7 0 . 2 0 . 9 00 0 . 7 0 , 0 1 . 2 0 . 0 0 . 9 0 . 0 44 0 . 0 10 . 6 0 . 0 11 . 5 2 . 5 * 6 . 7 * 0 . 3 0 . 0 335 40 . 6 13 . 2 27 . 9 * 27 . 9 * Groups in these two conditions , however , differed in several striking ways . There is a clear tradeoff between the amount of time spent in discussing the design itself ( the issues , alternatives , and evaluative criteria ) and the amount of time dictating and writing silently , the time indicated here as “pause . ” ~ People supported with ShrEdit spent less time than the Unsupported groups in direct discussion of issues , alternatives , and criteria ( t ( 36 ) = 5 . 33 , p < . 001 ) . They spent much more time writing down their design ( at least the amount that occurs during pauses ) than the Unsupported groups did ( t ( 36 ) = 4 . 09 , p < . 001 ) . Interestingly , the Unsupported groups spent 3 times as much time summa - rizing their ideas and walking through them for purposes of checking than i There was more writing than indicated here . This is the writing that is done when no one 1s speaking or when someone is dictating word for word . It does not include time when someone wrote while other talking was going on . This is particularly important m interpreting the category “Digression , ” which often occurred between two people while a third wrote , m both conditions . ACM TransactIons on Information Systems , Vol 11 , No 4 , October 1993 Groupwork Close Up . 341 the Supported groups ( Summary t ( 36 ) = 4 . 24 , p < . 01 ; Walkthrough t ( 36 ) = 2 . 21 , p < . 03 ) . Also , the Unsupported groups spent on average significantly more time per unique alternative than the Supported groups ( 6 . 6 seconds versus 8 . 3 seconds , t ( 36 ) = 2 . 39 , p < . 02 ) . Closer inspection of the transcripts shows that rather than this time being spent in explaining or elaborating the idea , it was spent merely repeating the ideas at various delays . Taken together , it appears that although the Supported groups are spending more time “on design , ” they are spinning their wheels rather than adding high - quality ideas and evaluations . Of note also is the activity in and out of pause , which occurred four times as many times in the Supported groups compared to Unsupported . The Supported groups move between speaking and writing fluidly , capturing the content of the discussion . The writing of the ideas ( which occurs primarily in the pauses ) is much more interspersed than in the Unsupported groups , and the average time spent writing is shorter for the Supported groups ( 5 seconds per episode on average versus 10 seconds for Supported versus Unsupported ) . In Unsupported meetings the discussion of alternatives occurs with some note taking on the board while they talk , and then they write their final reports in larger episodes . The relationship between what is spoken and what is written . There is one more difference that the technology affords . We compared the “history” of the ideas ( alternatives ) —those raised in conversation , those written at any time in the meeting , and those appearing in the final document . Table VI lists five kinds of tallies : the number of unique ideas that were generated no matter whether they were spoken or written ; those spoken ; those written at any time ( whether they be whiteboard , paper , or in ShrEdit ) ; and those that appear in the final document . Of particular interest is the category , “extra , ” which includes the ideas that were written in the final document that were neuer mentioned in the meeting . Figure 6 illustrates the relationship between these various measures . Several things are of interest here . First , many more ideas are raised , totally , in the Unsupported groups than in the Supported groups . But by the time the final document is crafted , the tide shifts : more ideas tend to appear in the Supported groups’ documents than in the Unsupported . 5 As one might expect , with more access to the documents ( the equal access ShrEdit versus the one - at - a - time whiteboard ) , fewer items were lost , spoken but never written . Of those , a larger portion are retained in the final document in the ShrEdit condition than in the Unsupported condition ; in other words , fewer of the items that were ever written down were lost in creating the final document . This is understandable since in the Unsupported condition things must be transferred from the group - space , the whiteboard , to the final handwritten sheets of paper . With ShrEdit , the items are either cut and 5Recall that the t - test of this was not significant , though the number of ideas in the final document was correlated with quality , and the Supported group on the whole had higher quality . ACM Transactions on Information Systems , Vol . 11 , No . 4 , October 1993 . 342 . Judith S . Olson et al Table VI . The History of Alternatives Raised , Captured , and Appearing in the Final Document for the Supported and Unsupported Groups Supported Unsupported Total items 121 , 3 145 1 * ” Items spoken 994 129 . 1” Items written at any time 59 . 9 63 . 0 Items in the final document 56 . 9 51 . 8 Items written in the final doc but not spoken , “extra” 200 15 , 6 Items spoken but necwr written 61 . 0 81 . 00” Items written at some time but not in final document ~ . ( ) 11 , 0 , , pasted from rough document windows to the final document or merely remain in the working / final document untouched after initial entry . But also of interest are the numbers of “extra” ideas written in both conditions . Some ideas appear in the final document that were never dis - cussed . Both groups add these extra ideas , and in equal numbers . It appears that the one who “holds the pen” has some license . In the Supported groups , however , all held the pen , and all could see the extra items if they chose . Except for one Unsupported group where they passed the paper version of their output around to the other group members for checking , none of the Unsupported groups’ members saw the final output except its author . The Use of ShrEdit How much of the time is the technology being used ? Do people ever type simultaneously ? What do they do when typing simultaneously ? Note that in teaching the groups ShrEdit we introduced them to the features , but we did not tell them how to use them to support various aspects of the meeting’s work . To answer these questions , we cataloged the timing of the keystrokes for each person individually and noted overlap . Since a keystroke is an instanta - neous event , we assumed that a person’s attention was paid to typing both a second before each keystroke and a second after . Using this technique , we found that , on average , simultaneous typing occurred 10 % of the time . Figure 7 shows the timeline of typing and talking activity for one group ( number 18 ) . In this figure , time goes from left to right , with the entire hour and a half shown on the figure . The top 12 lines show the typing activity of three people in each of three windows , as well as their activity in private windows , each episode represented by a tick mark . The next three lines show aggregate talking by each of the three participants . Looking at the transcript for these episodes , we note that in the earlier bursts , the group is typically brainstorming for features the post office should have . The second burst is typically what we call “divide and conquer , ” where group members take various sections of their emerging document and flesh it ACM TransactIons cm Information Systems , Vol 11 , No 4 , October 1993 o 10 20 30 40 50 60 70 80 90 100 110 20 130 140 150 . , , . , , . . . . ‘a . , . : . , , . . 0 . . . . . . : : : : : . g $ lmyljlmpww : amqmm ~ (cid:144) ~ : ~ wq ~ ~ gm ~ m ~ f : “ ‘“””” ””” . . . , . . . . . . . . . . . . . . . . . . . , . , . . . , . ; . * $ Spoken butnotwrlt ? ti - . . . . . . . . " " : " " - . . . * llllilllllilllli tillllll lillllilIlilii illll ] lllllllIi . . . i . , , , , . Deletedo rlostfrom ~ , ~ ~ , . , , ; Copyiflg : : : ~ alnlEIBlllnlnlIllIglBIIltlnlElmlllIIEl . , : , , . 0 . , . , . , . w : : : : : : . : ~ y ~ . ~ dmmbmw : mmdmfl * wm : wHmfami @ HmMmm . m wmmrmmwm : , . . , . , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , , . ~ LYpoken but”notwrltten . . . . , , x ~ ! ! ! llllltil llllllijll lllllillllllllil llll ] llillllllll illll . , . , . , , . . . Deleted or lost from”” + . . . . . , , . , , . “ * EIIIIl # lBllInlE1 . ll # lfiltlmltlEIIll Copying ; : : , . Unique Ideas Written and Spoken Items Spoken Items writtenatany time ItemsintheFinal Document Unique Ideas Written ; and Spoken : Items Spoken Items wnttenatanytime ~ Items in the Final ‘ Document Fig . 6 . Relationship between various ideas spoken and written . W . * OW , w , ” , , ” . , Tin . ! Wk , d . . , 1 , , 1 . , . ~ h . I M . . AI , , . . , , * 1 - . . ’ , , , , ” - All . - . , . , , , . , . . , , hi , . 1A < , . , 11 , . . 1 - 1 - - + ~ ~ - s - ’ ~ , ” : : , , . , , . . . , . . , . , ~ k , cl” Phi . . Ilmc ( mlmllm ) ‘1 10 20 30 4 [ ) 5 [ ) 60 7 [ 1 811 90 Fig . 7 . A time line of activity , both talking and typing . The 90 - minute session is represented left to right , The top three lines indicate who was typing in Window 1 ; the next three indicate who is typing in Window 2 , the next three , Window 3 , and the next three , their Private windows . The bottom three lines show who was talking when . Groupwork Close Up . 345 out . In several groups , for example , one wrote the introduction , while others expanded on the costs and benefits of this particular design . Furthermore , there were a number of episodes where as someone talked , another recorded their ideas on the shared workspace . Not all groups orchestrated their work in this way , Different groups adapted different work styles . Some worked privately in parallel , then pasted their work into a shared window to discuss , then selected various good ideas and put them into a second shared window . In other groups , a single person served as a scribe throughout until time grew short . ‘I’he fact that many styles emerged and that none was clearly associated with higher quality underscores the importance of ShrEdit’s simplicity and flexibility ( this is discussed further in Dourish and Bellotti [ 1992 ] ) . In contrast , the Unsupported groups use the whiteboard to capture ideas as they occur , one at a time . They then stop their discussion early to begin the slow job of getting the ideas down on paper . This is typically done by one person , who in the interest of sharing , “dictates” the ideas out loud for the others to confirm and / or question . So it appears from the analysis that there are differences in the way the groups approach the problem . The Unsupported groups discuss more alterna - tives and evaluate them more , but construct the final document only at the end . The Supported groups capture their ideas in the final document as they go ; they have five times as many writing episodes , each half the length of those from the Unsupported groups . Supported groups lose fewer ideas ; fewer ideas that were spoken were lost to any written form , and fewer ideas written in the course of design were lost in the final document . Both groups added new ideas to the final document , ideas that were never talked about , but in Supported groups there is a chance that the other group members will read them and object if they disagree . What Correlates with Quality ? Since all groups i ~ this study were solving the same problem , we had a chance not only to look at the aggregate differences between the two support conditions and quality , but also to look within conditions for relationships among our process measures and quality . We examined whether quality was related to the time spent on writing , on raising alternatives , or on evaluating with various criteria , as well the number of issues and alternatives raised , and the number of criteria generated and items in the final document . For Supported groups none of the variables were significantly correlated with quality , likely because the variance among the quality scores was relatively small ( from 49 to 78 on an 80 - point scale , with a large number clustered very near the mean of 65 ) . For the Unsupported groups , which had a wider variance , an interesting pattern emerged , fitting with the major pattern of differences between groups . The more alternatives that appeared in the final document , the better the quality ( r ( 18 ) = . 58 , p < . 01 ) . Interest - ingly , the less time spent on design ( issues , alternatives , criteria , and their clarifications ) , the higher the quality ; the more time on writing ( which includes “pauses” and “digressions” which cover occasior ~ s when cme person is ACM TransactIons on Inform atlon Systems , Vol II h ~ { ) , $ Cr ( oDer 1993 . 346 . Judith S Olson et al . writing and the others are having a side conversation ) , the higher the quality ( 7 - ( 18 ) = – . 44 , p < . 06 for design ; r ( 18 ) = + . 58 , p < . 01 for writing ) . Unsup - ported groups were better when they spent less time per alternative ( r ( 18 ) = – . 43 , p < . 07 ) . Quality was better when core issues had more alternatives explored ( r - ( 18 ) = . 58 , p < . 02 ) . Nearly all of these correlations suggest that the more the Unsupported groups spent their time like the Supported groups , the higher the quality of their designs . 4 . DISCUSSION We gave subjects who were doing an interesting problem - solving task ( i . e . , design ) a simple collaborative tool to assist them in their face - to - face work . The tool provided no particular process support or made them structure their work . They were free to organize their activities as they wished . By the time they did our target design task they had had about an hour’s practice in using the new electronic workspace tool . Yet this simple tool and this limited experience had an effect on their work . The designs they produced were of higher quality than those of the control groups who worked in a more traditional fashion . They liked the process a little less , perhaps not surprising given the use of a new technology . But most surprising of all was the way in which they worked . They actually did a less extensive exploration of the design space than the traditional , unsupported groups . Our expectation had been just the opposite . So instead of encouraging a more exhaustive look at design ideas , the groups with the technology selective examined what turned out to be better ideas . They seemed to focus more on the core issues , less on auxiliary , nonproductive ideas . They captured and examined by reading what they were thinking while they worked , rather than in two stages—once on the whiteboard and once on paper while those other than the scribe talked about other things . In the two - stage process , ideas were lost . A lot of things produced the effect : shared access to an easily editable workspace that permitted parallel work when appropriate ; an easier blending of the activities of idea generation , idea selection , and report generation ; early capture of good ideas that led to a quicker consensus about a sensible design ; a mixed mode of participation ( talking and typing ) that favored consensus development as opposed to unfettered exploration of fringe design ideas . A striking feature of this result is that the tool , though without an embedded procedure for structuring a meeting , encouraged focus and deliber - ation . The more the Unsupported groups looked like the Supported groups in behavior ( keeping to the core issues , writing as they went ) , the higher the quality of their output . The technology made the groups more homogeneous , in particular , helping the less functional groups . Furthermore , as noted in Dourish and Bellotti [ 1992 ] , groups can manage themselves . Small groups in particular can capitalize on the power of the shared editor without needing facilitation , either human or software embedded . For these kinds of fast - mov - ing , intense design sessions , simplicity is good . This simplicity also allowed ACM TransactIons on Information Systems , Vol 11 , No 4 , October 1993 . Groupwork Close Up . 347 groups to adapt the technology to their own style of working . We saw many different patterns of work among the 19 groups that used ShrEdit . The technology had other direct effects on the behavior of the supported groups . They spent a large amount of their time ( 77 . ) trying to figure out the technology itself . On average they spent 10 % of their time doing parallel work , though as we showed , this tended to be concentrated at specific stages of their work . They used the technology to capture ideas that they never felt compelled to talk about , since all group members could see the ideas as they emerged and comment if they disagreed . If we compare the present study and its results to those described in previous reviews [ Hollingshead and McGrath 1993 ; McLeod 1992 ] , several things stand out . First , we also found , as did McLeod , that subjects using the technology were less satisfied than those in Unsupported groups . As we noted earlier , this is not surprising since our subjects were working in a very new way for the first time . Second , in contrast to almost all of the studies they reviewed , we used a software tool that did not structure the group’s activities . We nonetheless found an improvement in the quality of the group’s work . Since we fixed the time available for all groups , this improvement in quality did not come at the cost of greater time doing the task . Third , through our extensive analyses of group processes , we were able to learn much more about how our groups worked , what effect the technology had on this , and how this related to the differences in outcome . This focus on process details is a substantial advance over the earlier studies . Our overall impression is that the simple group editor we gave these subjects was a functional and useful tool . This was true despite its shortcom - ings of functionality and interface that were revealed by the subjects’ use and comments . On the basis of relatively little experience the subjects were able to organize their work with the aid of this tool . In the final analysis , both the significant and nonsignificant results are interesting . Some confirm our notions about what makes a good meeting ; some tell us that common pre - scriptions for good meetings ( e . g . , that discussing more alternatives is better ) may not always be helpful . ACKNOWLEDGMENTS Many people have participated in the collection and analysis of the data reported here , including Stacey Donahue , Alex Eulenberg , Barb Gamm , Jim Herbsleb , Robin Lampert , Mary Jane T . Lu , Brenda Montgomery , Max Rahder , Meredith Reitman , Adrienne Royce , Henry Rueter , Sue Schuon , Michael Walker , Rodney Walker , Garrick Wang , and Corinne Weirich . REFERENCES COHEN , A . 1960 . A coefficient of agreement for nominal scales . Ed . Psychol . Mess . 20 , 37 - 46 . DI ~ HI . , M . , ANII ST ~ O ~ ~ E , S . 1987 . Productivity loss in brainstorming groups : Toward the solutmn of a riddle . J . Person . Sot . Psych 01 . 61 , 3 , 392 – 403 . DOCTRISII , P . , AND BELLOTTI , V . 1992 , Awareness and coordination in shared workspaces . In Proceedings of the Conference on Computer Supported Cooperative Work . ACM Transactions on Information Systems , Vol . 11 , No . 4 , October 1993 . 348 . Judith S . Olson et al , EN ~ FJ , ~ ART , D . , AND ENGLISH , W 1968 . A research center for augmenting human intellect . In % oceecltngs of FJCC 33 . AFIBS Press , Montvale , NY . , 395 - 440 , FORSYTH , D . R , 1990 . Group Dyrzanzzcs . 2nd ed . Brooks / Cole , Pacific Grove , Calif , GOURAN , D . S . , BROWN , C . , AND HENRY , D . R 1978 . Behavioral correlates of perceptions of quality in decmon - rnaking discussions Cornrnzm . Morzogr . 45 , 1 ( Mar . ) , 51 – 63 . GREEN , S . G . , ANU TAEZ ~ R , T D . 1980 , The effects of three social decision schemes on decision gTOUp process . org . Behac . Human Pe ~ f . 25 , 97 - 106 . HACXQW + N , J . R , 1987 . The design of work teams . In Handbook of Organzzatzonal Behauzor . Prentice - Hall , Englewood Ghffs , N J , 315 - 342 , HLILLINGSH ~ AD , A . B . , AND MCGRATH , J . E , 1993 . The whole M less than the sum of its parts : A critical review of research on computer - asemted groups . In Team Deczszorz Makzrzg tn Organzzatzons . tJossey - Bass , San Francisco . MCGRATH , J . E . 1984 . Groups : I ? zteraction and Performance , Prentice - Hall , Englewood Chffs , NJ . MCGUFFIN , L . , AND OLSON , G M , 1992 . ShrEdit : A shared electronic workspace CSMIL Tech . Rep . , The Umv . of Michigan , Ann Arbor , Mich . McLEo ~ , P , L . 1992 An assessment of the experimental literature on electronic support of group work , Human Comput , Intcractzon 7 , 257 - 280 , MORAN , T . , AND CA . RROLL , J . , EDS . 1993 Deszgn Ratzonale Lawrence Erlbaum Associates , Hillsdale , N . J . OLSON , G . M , , MCGCTFFIN , L . J , KCTWANA , E , , AND OLSON , J , S , 1993a , Designing software for a group’s needs : A functional analysis of synchronous software Trends zn Software . OLSON , G . M . , OISON , J . S . , STORROST ~ N , M , , CARTER , M . , HERIRLICR , J . , AND RUICTER , H 1993 . The structure of activity during design meetmgs . In Design Rationale OLSON , J . S , , CARD , S . K . , LANDAUER , T . K . , OLSON , G . M . , MALONE , T . , AND LEX + GETT , J . 1992a , Behaclor and Information Technology . OLSON , G M , OLSON , J . S , CARTER , M , ANI ) STflRROST ~ N , M 1992b Small group design meetings : An analysis of collaboration , Human Con ~ pzLt , Interaction 7 , 347 – 374 , 01 . soN , G . M . , OLSON , J . S . , KZLL ~ Y , L . , MAcK , L . A , , CORNELL , P . , AND LUCHETTI , R . 1992c , Flexible facilities for electronic meetings In Compt ~ ter Augmented Teamu , ork A Guzded Tour Van Nostrand Reinhold , New York . P ~ DERS ~ N , E , R , MCCALL , K . , MORAN , T , P . , AND HA1 . ASZ , F , G . 1993 . Tlvoli : An electronic whlteboard for informal workgroup meetings . In proceeduzgs of INTERCHI 1993 ( Amsterdam , The Netherlands , Apr . 24 - 29 ) . ACM , New York , 391 - 398 . PUTNAM , L L . 1983 Small group work chmates A lag - sequential analysis of group intera - ction , Small Group Behat . 14 , 4 , 465 – 494 ROS ~ NTHAL , R . 1984 , Mets - Analytzc Procedures for SocLal Research , Sage , Newbury Park , Calif . ROS ~ NTHAT , , R . 1973 , Estimating effective reliability m studies that employ judges ratings . J , Clin , Psvchol 29 , 342 - 345 . SCHWARTZMAN , H . B . 1989 , The Meetzng : Gatherings In Organizations and CommunLtLes . Plenum , New York . STEINER , I D . 1972 . Group Processes and product [ lzty . Academic Press , New York TINSL ~ Y , H . E . A . , AND WmSS , D J , 1975 . Interrater reliability and agreement of subjective judgments . J . C’ouns . Psychol 22 , 4 , 358 – 376 ZIGIIRS , I , POOLE , M S . , AND DIf3ANCT [ S , G L . 1988 . A study of influence in computer - mcdla - ted group decision making , MIS Q . 12 , 645 – 665 . Rece , ved January 1993 ; accepted July 1993 ACM Transactions on Information Systems , Vol . 11 , No , 4 , October 1993