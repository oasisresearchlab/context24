Sampling Projects in GitHub for MSR Studies Ozren Dabic , Emad Aghajani , Gabriele Bavota SEART @ Software Institute , USI Universit ` a della Svizzera italiana , Lugano , Switzerland Abstract —Almost every Mining Software Repositories ( MSR ) study requires , as ﬁrst step , the selection of the subject software repositories . These repositories are usually collected from hosting services like GitHub using speciﬁc selection criteria dictated by the study goal . For example , a study related to licensing might be interested in selecting projects explicitly declaring a license . Once the selection criteria have been deﬁned , utilities such as the GitHub APIs can be used to “query” the hosting service . However , researchers have to deal with usage limitations imposed by these APIs and a lack of required information . For example , the GitHub search APIs allow 30 requests per minute and , when searching repositories , only provide limited information ( e . g . , the number of commits in a repository is not included ) . To support researchers in sampling projects from GitHub , we present GHS ( GitHub Search ) , a dataset containing 25 characteristics ( e . g . , number of commits , license , etc . ) of 735 , 669 repositories written in 10 programming languages . The set of characteristics has been derived by looking for frequently used project selection criteria in MSR studies and the dataset is continuously updated to ( i ) always provide fresh data about the existing projects , and ( ii ) increase the number of indexed projects . The GHS dataset can be queried through a web application we built that allows to set many combinations of selection criteria needed for a study and download the information of matching repositories : https : / / seart - ghs . si . usi . ch . Index Terms —GitHub , search , sampling repositories I . I NTRODUCTION The amount of data available in software repositories is growing faster than ever . At the time of writing , GitHub [ 1 ] hosts over 80 Million public repositories 1 accounting for over 1 billion commit activities . Such an unprecedented amount of software data represents the main ingredient of many Mining Software Repositories ( MSR ) studies . One of the ﬁst steps in MSR studies consists in selecting the subject projects , i . e . , the software repositories to analyze in order to answer the research questions ( RQs ) of interest . Such a step is crucial to achieve generalizability of the ﬁndings and ensure that the selected projects result in useful data points for the goal of the study . For example , a study investigating the types of issues reported in GitHub [ 2 ] requires the selection of repositories regularly using the GitHub integrated issue tracker . Instead , a study interested in the pull request ( PR ) process of OSS projects [ 3 ] must ensure that the subject systems actually adopt the PR mechanism ( e . g . , by verifying that at least n PRs have been submitted in a given repository ) . In addition to RQ - speciﬁc selection criteria , several studies adopt speciﬁc ﬁlters to exclude toy and personal projects . For example , previous works excluded repositories having a low number of stars [ 4 ] , commits [ 5 ] , or issues [ 2 ] . 1 https : / / api . github . com / search / repositories ? q = is : public + fork : true Once the selection criteria have been deﬁned , software repositories satisfying them must be identiﬁed . Frequently , the search space is represented by all projects hosted on GitHub that , as previously said , are tens of millions . To query such a collection of repositories , developers can use the ofﬁcial GitHub APIs [ 6 ] that , however , come with a number of limitations both in terms of number of requests that can be triggered and information that can be retrieved . For example , the GitHub search API allows for a maximum of 30 requests per minute and each request can return at most 100 results . Only searching for some basic information about the public Java repositories hosted on GitHub would require , at the time of writing , ∼ 160k requests ( ∼ 88 hours ) . If additional information is required for each repository ( e . g . , its number of commits ) , additional requests must be triggered , making the process even more time expensive . Moreover , setting an appropriate value for the selection criteria ( e . g . , a project must have at least 100 commits ) without having an overall view of the available data can be tricky . For instance , researchers cannot easily select the top 10 % repositories in terms of number of commits without ﬁrstly collecting this information for the entire population . Finally , given a selection criteria , the GitHub search API provides at most the ﬁrst 1 , 000 results ( through 10 requests ) . This means there is no easy way to retrieve all matching results for a selection criteria if it exceed this upper bound . To support developers in mining GitHub , several solutions have been proposed . Popular ones are GHTorrent [ 7 ] and GHArchive [ 8 ] . Both projects continuously monitor public events on GitHub and archive them . While the value of these tools is undisputed , as the beneﬁts they brought to the research community , they do not provide a handy solution to support the sampling of projects on GitHub accordingly to the desired selection criteria . For example , computing the number of commits , issues , etc . for a repository in GHTorrent would require MySQL queries aimed at joining multiple tables . We present GHS ( GitHub Search ) [ 9 ] , a dataset and a tool to simplify the sampling of projects to use in MSR studies . GHS continuously mines a set of 25 characteristics of GitHub repositories that have been often used as selection criteria in MSR studies and that , accordingly to our experience in the ﬁeld , can be useful for sampling projects ( e . g . , adopted license , number of commits , contributors , issues , and pull requests ) . The tool behind GHS can be conﬁgured to mine projects written in speciﬁc programming languages . As of today , it mined information about over 700k repositories written in 10 different languages ( i . e . , Python , Java , C + + , C , C # , Objective - C , Javascript , Typescript , Swift , and Kotlin ) . a r X i v : 2103 . 04682v1 [ c s . S E ] 8 M a r 2021 Repository Miner GitHub Search API GitHub API Invoker GitHub Website GitHub Website Crawler Dataset 735k Requests ( dashed ) Data ﬂow ( solid ) Fig . 1 : The GHS architecture A stable version of the dataset is hosted on zenodo [ 10 ] and it features 735 , 669 repositories written in the previously mentioned languages . As detailed in the following , GHS has been designed with scalability in mind and to speciﬁcally support the sampling of projects for MSR study . While the user can download the dataset and query it with an ad - hoc script , a querying interface with export features is available at https : / / seart - ghs . si . usi . ch . II . T HE D ATASET This section describes GHS [ 10 ] , a dataset containing information about 735 , 669 GitHub public repositories that can be used by researchers to easily select projects for an empirical study . In particular , 25 characteristics of each project are mined , stored , and continuously updated . Our mining tool exploits the GitHub search API and an ad - hoc crawler we built to collect speciﬁc information from the repositories’ homepage . Table I lists the collected characteristics , together with a short description for each of them , the source from which the information is mined and one example of works in the literature that used such a characteristic in the empirical study ( or “ - ” if we did not ﬁnd a related reference ) . Fig . 1 depicts the main steps behind the data collection pro - cess put into place to build GHS . The following subsections detail such a process . A . Data Extraction As depicted in Fig . 1 , the GHS data collection process is carried out through three main components . 1 . GitHub API Invoker : This component has two main responsibilities . First , it can retrieve the list of repositories ( i ) written in a speciﬁc language , and ( ii ) created or updated during a certain time period . The latter feature is needed , as detailed later , to overcome the GitHub API maximum result limit of 1 , 000 results per request . To retrieve , for example , the list of repositories written in Java and updated in March 2020 , the following GitHub API request is triggered : https : / / api . github . com / search / repositories ? q = fork : true + is : public + language : Java + created : 2020 - 03 - 01 . . 2020 - 04 - 01 For each collected repository , the information in Table I having “GitHub Search API” as mining source is retrieved . Second , this component is in charge of monitoring if the GitHub access token being used for mining has not exceeded its request limit . Indeed , we use authenticated requests to increase the usage limits imposed by the GitHub API . 2 . GitHub Website Crawler : This component is used to collect , for a given repository , all information in Table I having a repository’s webpage as mining source . Since the information of interest is scattered in different pages , this component mines the repository’s ( i ) landing page [ 25 ] , ( ii ) issues page [ 26 ] , and ( iii ) pull requests page [ 27 ] . We parse the HTML of these pages by using the CSS selectors containing the information of interest . For this task we primarily rely on the jsoup library [ 28 ] . Unfortunately , due to the use of dynamic content generation in the GitHub pages , not all elements are present when downloading the content of a page , e . g . , the number of contributors is dynamically gener - ated , and cannot always be captured using jsoup ( it depends on the time required for loading the needed information ) . When jsoup fails in retrieving a speciﬁc information , we rely on the Selenium WebDriver for Chrome [ 29 ] , which provides the possibility to wait for the required information to load . Since Selenium introduces a signiﬁcant performance drawback , it is only used as backup strategy when jsoup returns an error . We are aware that mining CSS selectors as a strategy to collect information can require future updates if the GitHub UI substantially changes . We considered such a scenario in our implementation by using , when possible , generic selectors that are unlikely to change over time . Also , this “maintenance cost” is counterbalanced by the high performance in retrieving the required information ensured by the webpages parsing . 3 . Repository Miner : This is the core component orches - trating the collection of the GHS dataset . Before describing how it works , it is important to clarify that the set of program - ming languages of interest ( i . e . , the ones for which repositories will be mined ) is deﬁned by the GHS administrator . In our case , we set the 10 languages composing the current version of the dataset . The Repository Miner implements a mining algorithm that is triggered every six hours for continuously updating the information in GHS . For each programming language of interest , the algorithm checks if any prior mining has been conducted . If no record of prior mining is found , the GitHub API Invoker is triggered to mine all repositories created or updated between January 1 st 2008 ( GitHub started in February 2008 ) and the current time minus two hours 2 . If , instead , a previous mining process MP has been performed for the speciﬁc language , the GitHub API Invoker collects all repositories created or updated between the last date mined in MP and the current time minus two hours . In both cases the GitHub API Invoker collects all reposito - ries ( i ) written in the selected language , ( ii ) created / updated during the selected interval , and ( iii ) having at least 10 stars . 2 We ignore the last two hours since it takes time for the GitHub’s internal database to sync newly created projects . Charcteristic Description Mining Source Used in name Name of the repository in the form user _ name / repo _ name GitHub Search API [ 11 ] commits Number of commits on the default branch Repository’s landing page [ 12 ] last _ commits _ sha The SHA - 1 hash of the latest commit on the default branch Repository’s landing page - last _ commits The date of the latest commit on the default branch Repository’s landing page [ 13 ] license The license used for the repository ( if any ) GitHub Search API [ 14 ] branches Number of remote branches Repository’s landing page [ 15 ] default _ branch Name of the default branch GitHub Search API - contributors Number of contributors Repository’s landing page [ 5 ] releases Number of releases [ 16 ] Repository’s landing page [ 17 ] watchers Number of users watching the repositories Repository’s landing page [ 18 ] stars Number of stars the repository received GitHub Search API [ 3 ] forks Number of repositories forked from this repository GitHub Search API [ 13 ] is _ fork _ project Whether the projects is a fork GitHub Search API [ 19 ] size The size of project ( in kilobytes ) GitHub Search API [ 20 ] created _ at Date when the repository is created GitHub Search API [ 19 ] pushed _ at Latest date when a commit is pushed to any of the repository’s branches GitHub Search API [ 12 ] updated _ at Latest date when the repository object is updated , e . g . , description changed GitHub Search API - homepage The repository’s homepage URL ( if any ) GitHub Search API [ 21 ] main _ language The main language that the repository’s source code is written in GitHub Search API [ 22 ] total _ issues Total number of issues ( both open and closed issues ) Repository’s issues page [ 2 ] open _ issues Number of open issues Repository’s issues page [ 2 ] total _ pull _ requests Total number of pull requests ( both open and closed issues ) Repository’s pull requests page [ 3 ] open _ pull _ requests Number of open pull requests Repository’s pull requests page [ 3 ] has _ wiki Whether the repository has wiki GitHub Search API [ 23 ] archived Whether the repository is marked as archived ( i . e . , read - only ) GitHub Search API [ 24 ] TABLE I : Characteristics stored in GHS for each GitHub project The decision of only collecting repositories having at least 10 stars aims at drastically reducing the number of repositories we store and makes the data collection more scalable ( e . g . , from preliminary analyses we performed on Java , < 5 % of repositories have at least 10 stars ) . We acknowledge that , as also shown in previous work [ 30 ] , the number of stars is not a good proxy for repositories quality or relevance , and there are better ways to automatically identify engineered GitHub projects ( e . g . , the Reaper tool [ 30 ] ) . However , we believe that the 10 stars threshold provides a reasonable compromise between the quality of data and the time required to mine and continuously update all projects . If the GitHub API Invoker retrieves more than 1 , 000 repos - itories for a time interval , it splits the interval in half , and the two new time intervals are pushed to a priority queue handling the requests to process . Such a mechanism is needed since the GitHub API only provides the ﬁrst 1 , 000 results for a request . The algorithm recursively picks and process the oldest interval from the queue until it is empty , meaning the mining for the current language is completed . Otherwise , if there are less than 1 , 000 results for an interval , the algorithm iterates over the result list . For each retrieved repository , the algorithm scrapes the missing information from the repository web pages ( using the GitHub Website Crawler ) , and saves the full record to a database . Our algorithm can mine / update ∼ 20k repositories everyday . B . Data Storage The data collected for all repositories ( Table I ) is stored in a MySQL database . When updated information about a pre - viously mined repository is collected , the corresponding rows for that repository will be updated with the new information ( i . e . , no new row is created ) . While this ensures that the repository data contained within GHS is kept updated , GHS does not offer an overview of the historic evolution of said characteristics . A stable version of the dataset , exported on January 28 th 2021 , is hosted on zenodo [ 10 ] and it features 735 , 669 repositories written in 10 languages . DOI DOI 10 . 5281 / zenodo . 4476391 10 . 5281 / zenodo . 4476391 C . Querying GHS The latest and continuously growing version of our dataset can be downloaded / queried through our online platform [ 9 ] . Fig . 2 depicts the GUI we provide to query GHS ( left part ) and an example of results page obtained by searching for the Apache Java repositories having at least 100 commits ( right ) . General ﬁlters 1 can be applied to select projects containing a speciﬁc string in their name ( e . g . , “apache / ” will return all projects run by the Apache Software Foundation ) , having a speciﬁc license , written in a given language or using speciﬁc labels for their issues ( e . g . , “refactoring” ) . The latter feature is still under development , which is why we do not present issue labels in the stable version of GHS . Projects can also be ﬁltered based on their history and activity ( e . g . , number of commits , releases ) 2 , even only retrieving repositories that had activities in a speciﬁc time frame 3 . Finally , ﬁlters labeled with 4 concern popularity indicators , while those with 5 allow to further reﬁne the results list by removing , for example , forks . By clicking on the “Search” button , the repositories satisfy - ing the search criteria are shown , giving the possibility to the user to inspect the results list and , eventually , download it in different formats 6 . Search form Results View 1 2 3 4 5 6 Fig . 2 : GUI to query GHS [ 9 ] ( left ) and results page with export options ( right ) . III . R ELATED W ORK To support researchers in MSR , several solutions have been proposed . GHArchive [ 8 ] records the public GitHub activities on an hourly basis as json archives . This is done by mining the GitHub public event stream ( e . g . , a user creating a repository , a repository gaining a new watcher ) through the use of webhooks [ 31 ] . This means that , for example , to sample all Java repositories created in 2012 we must retrieve all repositories linked to a “create” event from each hour , of each day , of each month of the year . This translates in scanning ∼ 8 thousand ﬁles for said events . Thus , while GHArchive is a fantastic data source for MSR studies , it is not convenient for sampling repositories . GHTorrent [ 7 ] continuously collects data from the GitHub API storing it in both relational and non - relational databases . It likely offers the most used dataset in MSR studies , thanks to the huge amount of stored data and no limitations posed on its querying . However , as mentioned in Section I , retrieving speciﬁc information such as the number of commits in a repository may require formulating queries on quite a large dataset . GHS , as compared to GHTorrent , ( i ) stores only basic repository information needed for making projects’ sampling convenient , and ( ii ) provides a handy GUI to query the dataset . Software Heritage [ 32 ] aims at preserving software in source code form including , e . g . , projects deleted from GitHub . It contains , at the date of writing , over 150M repositories featuring almost 10B source ﬁles . The focus of such a dataset is different from GHS since Software Heritage is not explicitly meant to simplify projects sampling for empirical studies based on ( pre - computed ) selection criteria . Surana et al . [ 33 ] proposed GitRepository , a tool to extract structured information from GitHub repositories related to contributors , issues , pull requests , releases , and subscribers . The authors do not provide a dataset , but a tool able to create a dataset using the GitHub API . GHS provides a wider variety of information , allowing for a better sampling made easy trough its GUI . In addition to the discussed works , some older projects are no longer active . Markovtsev and Long introduced Public Git Archive [ 34 ] , a dataset of ∼ 180k repositories having at least 50 stars . The dataset has been released in 2018 and , to the best of our knowledge , is not kept updated . Bissyand ´ e et al . [ 35 ] presented Orion , a corpus of soft - ware projects collected from GitHub , Google Code [ 36 ] and Freecode [ 37 ] . To query Orion a custom designed DSL lan - guage must be used . The project webpage [ 38 ] is no longer accessible . IV . F UTURE W ORK There are four main directions in which we are improv - ing GHS . First , we will add more and more programming languages over time . Doing this is as easy as changing a conﬁguration ﬁle . Second , we will ﬁnalize the collection of the issue labels that can be used , for example , when a researcher is interested in repositories explicitly using speciﬁc labels such as refactoring or documentation . The GUI already supports such a feature , while the crawling of this information is not yet ﬁnalized . Third , the code behind GHS is open source [ 39 ] and we plan to collect requests for additional project characteristics to include in GHS from the research community through its issue tracker . Lastly , we will focus on improving performance , especially in terms of data mining . V . C ONCLUSIONS We presented GHS ( GitHub Search ) , a dataset to simplify the sampling of projects for MSR studies . A stable version of GHS is available on zenodo [ 10 ] and features information about 735 , 669 GitHub repositories written in 10 languages . The dataset is continuously updated and expanded , with its latest version available at https : / / seart - ghs . si . usi . ch together with a handy querying interface . VI . A CKNOWLEDGMENTS This project has received funding from the European Re - search Council ( ERC ) under the European Union’s Horizon 2020 research and innovation programme ( grant agreement No . 851720 ) . R EFERENCES [ 1 ] “https : / / www . github . com , ” 2021 . [ 2 ] T . F . Bissyand´e , D . Lo , L . Jiang , L . R´eveill ` ere , J . Klein , and Y . L . Traon , “Got issues ? who cares about it ? a large scale investigation of issue trackers from github , ” in 2013 IEEE 24th International Symposium on Software Reliability Engineering ( ISSRE ) , 2013 , pp . 188 – 197 . [ 3 ] F . Zampetti , G . Bavota , G . Canfora , and M . D . Penta , “A study on the interplay between pull request review and continuous integration builds , ” in 2019 IEEE 26th International Conference on Software Analysis , Evolution and Reengineering ( SANER ) , 2019 , pp . 38 – 48 . [ 4 ] F . Wen , C . Nagy , M . Lanza , and G . Bavota , “An empirical study of quick remedy commits , ” in Proceedings of the 28th International Conference on Program Comprehension , 2020 , pp . 60 – 71 . [ 5 ] F . Pecorelli , F . Palomba , F . Khomh , and A . De Lucia , “Developer - driven code smell prioritization , ” in International Conference on Mining Software Repositories , 2020 . [ 6 ] “https : / / docs . github . com / en / free - pro - team @ latest / rest , ” 2021 . [ 7 ] G . Gousios , “The ghtorrent dataset and tool suite , ” in Proceedings of the 10th Working Conference on Mining Software Repositories , ser . MSR ’13 . Piscataway , NJ , USA : IEEE Press , 2013 , pp . 233 – 236 . [ 8 ] “https : / / www . gharchive . org / , ” 2021 . [ 9 ] “https : / / seart - ghs . si . usi . ch , ” 2021 . [ 10 ] “https : / / doi . org / 10 . 5281 / zenodo . 4476391 , ” 2021 . [ 11 ] B . A . Muse , M . M . Rahman , C . Nagy , A . Cleve , F . Khomh , and G . An - toniol , “On the prevalence , impact , and evolution of sql code smells in data - intensive systems , ” in Proceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 327 – 338 . [ 12 ] D . Gonzalez , M . Rath , and M . Mirakhorli , “Did you remember to test your tokens ? ” in Proceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 232 – 242 . [ 13 ] D . Gonzalez , T . Zimmermann , and N . Nagappan , “The state of the ml - universe : 10 years of artiﬁcial intelligence & machine learning software development on github , ” in Proceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 431 – 442 . [ 14 ] C . Vendome , G . Bavota , M . Di Penta , M . Linares - V´asquez , D . German , and D . Poshyvanyk , “License usage and changes : a large - scale study on github , ” Empirical Software Engineering , vol . 22 , no . 3 , pp . 1537 – 1577 , 2017 . [ 15 ] J . Han , S . Deng , X . Xia , D . Wang , and J . Yin , “Characterization and prediction of popular projects on github , ” in 2019 IEEE 43rd Annual Computer Software and Applications Conference ( COMPSAC ) , vol . 1 , 2019 , pp . 21 – 26 . [ 16 ] “https : / / docs . github . com / en / github / administering - a - repository / about - releases , ” 2021 . [ 17 ] L . Moreno , G . Bavota , M . D . Penta , R . Oliveto , A . Marcus , and G . Canfora , “ARENA : an approach for the automated generation of release notes , ” IEEE Trans . Software Eng . , vol . 43 , no . 2 , pp . 106 – 127 , 2017 . [ 18 ] J . Sheoran , K . Blincoe , E . Kalliamvakou , D . Damian , and J . Ell , “Understanding “watchers” on github , ” in Proceedings of the 11th working conference on mining software repositories , 2014 , pp . 336 – 339 . [ 19 ] T . Bryksin , V . Petukhov , I . Alexin , S . Prikhodko , A . Shpilman , V . Ko - valenko , and N . Povarov , “Using large - scale anomaly detection on code to improve kotlin compiler , ” arXiv preprint arXiv : 2004 . 01618 , 2020 . [ 20 ] A . Borrelli , V . Nardone , G . A . Di Lucca , G . Canfora , and M . Di Penta , “Detecting video game - speciﬁc bad smells in unity projects , ” in Pro - ceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 198 – 208 . [ 21 ] E . Aghajani , C . Nagy , O . L . Vega - M´arquez , M . Linares - V´asquez , L . Moreno , G . Bavota , and M . Lanza , “Software documentation issues unveiled , ” in Proceedings of the 41st International Conference on Software Engineering , ser . ICSE ’19 . IEEE Press , 2019 , pp . 1199 – 1210 . [ 22 ] T . Nakamaru , T . Matsunaga , T . Yamazaki , S . Akiyama , and S . Chiba , “An empirical study of method chaining in java , ” in Proceedings of the 17th International Conference on Mining Software Repositories , 2020 , pp . 93 – 102 . [ 23 ] J . Tantisuwankul , Y . S . Nugroho , R . G . Kula , H . Hata , A . Rungsawang , P . Leelaprute , and K . Matsumoto , “A topological analysis of communication channels for knowledge sharing in contemporary github projects , ” Journal of Systems and Software , vol . 158 , p . 110416 , 2019 . [ Online ] . Available : http : / / www . sciencedirect . com / science / article / pii / S0164121219301906 [ 24 ] J . Coelho , M . T . Valente , L . L . Silva , and E . Shihab , “Identifying unmaintained projects in github , ” in Proceedings of the 12th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement , ser . ESEM ’18 . New York , NY , USA : Association for Computing Machinery , 2018 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3239235 . 3240501 [ 25 ] “github . com / user name / repo name , ” 2021 . [ 26 ] “github . com / user name / repo name / issues , ” 2021 . [ 27 ] “github . com / user name / repo name / pulls , ” 2021 . [ 28 ] “https : / / jsoup . org / , ” 2021 . [ 29 ] “https : / / chromedriver . chromium . org , ” 2021 . [ 30 ] N . Munaiah , S . Kroh , C . Cabrey , and M . Nagappan , “Curating github for engineered software projects , ” Empirical Software Engineering , vol . 22 , pp . 3219 ? – 3253 , 2017 . [ 31 ] https : / / developer . github . com / webhooks / event - payloads , 2021 . [ 32 ] R . Di Cosmo and S . Zacchiroli , “Software Heritage : Why and How to Preserve Software Source Code , ” in iPRES 2017 - 14th International Conference on Digital Preservation , 2017 , pp . 1 – 10 . [ Online ] . Available : https : / / hal . archives - ouvertes . fr / hal - 01590958 [ 33 ] S . Surana , S . Detroja , and S . Tiwari , “A tool to extract structured data from github , ” arXiv preprint arXiv : 2012 . 03453 , 2020 . [ 34 ] V . Markovtsev and W . Long , “Public git archive : A big code dataset for all , ” in Proceedings of the 15th International Conference on Mining Software Repositories , 2018 , pp . 34 – 37 . [ 35 ] T . F . Bissyand´e , F . Thung , D . Lo , L . Jiang , and L . R´eveill ` ere , “Orion : A software project search engine with integrated diverse software artifacts , ” in 2013 18th International Conference on Engineering of Complex Computer Systems , 2013 , pp . 242 – 245 . [ 36 ] “code . google . com , ” 2021 . [ 37 ] “https : / / freecode . com , ” 2021 . [ 38 ] “momentum . labri . fr / orion , ” 2021 . [ 39 ] “https : / / github . com / seart - group / ghs , ” 2021 .