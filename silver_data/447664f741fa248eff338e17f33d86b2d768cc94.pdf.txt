Diagnosis of Small - world Bias in Random Graphs Georgios Argyris Department of Science and Technology , University of Twente , Netherlands g . argyris @ utwente . nl Abstract Background : Imagine a paper with n nodes on it where each pair undergoes a coin toss experiment ; if heads we connect the pair with an undirected link , while tails maintain the disconnection . This procedure yields a random graph . Now consider duplicating this network onto another paper with a slight bias - a fraction of its links ( approximately 1 / 10 ) undergo rearrangement . If we shuffle the two papers , how can we distinguish the pure random graph from the biased one ? Results : In response to this challenge , we propose a novel metric called “Randomness Index” ( RI ) . The closer the metric to zero is , the higher degree of randomness in the graph . The RI can distinguish between dense small - world networks and dense random graphs ; a distinction which is impossible by conventional small - world properties like clustering coefficient and average path length . To validate its effectiveness , we apply the RI to temporal correlation networks of stock indices . Our findings reveal a reduction in randomness during global economic recession periods . Conclusion : The RI emerges as a powerful metric capable of characterizing small - world topology , especially in scenarios where other network measures fail . Beyond its utility in network analysis , the RI is promising for change - point ( anomaly ) detection in dynamical systems studied by means of multivariate time series . Keywords : Random Graphs , Small - World Networks , Motifs , Graphlets , Graph Isomorphism , Rado Graph , Watts - Strogatz Model , Network Randomness Index , Network Randomness Metric , Multivariate Time Series , Temporal Correlation Networks , Morgan Stanley Capital Index 1 . Introduction Critical Point Figure 1 : The Critical Point in blue . Whatever is inside the neighbourhood is random , and whatever is outside the neighborhood is small - world . Contribution . Small - world topology is characterized by high clustering co - efficient and low average path length and constitutes an independent feau - ture of the inherently - sparse social networks . On the other hand , dense graphs exhibit both high clustering and low average path length ( such as the interracial cortical network in the primate brain [ 1 ] ) which hinders the discernation of the small - world phenomenon . We propose a method that addresses this problem by distinguishing dense small - world networks from dense random networks through a classification methodology . The latter is based on a critical point and a neighbourhood around this point defined in an euclidean space ( see Fig . 1 ) . By embedding an observed network into a point of the same euclidean space , we say that an observed network is random if the point lies inside the neighbourhood of the critical point . On the contrary , a network is small - world if it lies outside the neighbourhood . The critical point is a six dimensional point whose coordinates constitute relative frequencies of motifs . Network types . The research of Gilbert , Erdos , and Renyi [ 2 , 3 ] on random graph theory inspired the study of real world complex networks . Over the last years , network science provides metrics and measures that characterize either the network or its nodes [ 4 ] , and models that describe the different network topologies and generate networks with desirable properties . Without doubt , one of the most popular network topologies is the small - world . Small - world networks constitute an analogy to the small - world phenomenon of Stanley Milgram [ 5 ] firstly described by the Watts - Strogatz model [ 6 ] . The Watts and Strogatz ( WS ) small - world network model is a three - parameter model ( V , k , p r ) where : Preprint submitted to Arxiv December 29 , 2023 a r X i v : 2312 . 16525v1 [ c s . S I ] 27 D e c 2023 V is a set of vertices placed on a circular configuration , k denotes the number of neighbors each node is attached , and p r is the probabilty of rearrangement of a link in the network . On the other hand , the Erdos - Renyi ( ER ) random graph model is considered here as a two - parameter model ( V , p ) : V is the number of nodes and p denotes the probability of a link’s occurence between any pair of nodes . This article compares the two network topologies in terms of motifs [ 7 ] . Figure 2 : ( Left ) Sequential pattern . ( Right ) Star pattern . Some preliminary thoughts . Consider the two patterns of Fig . 2 , namely the sequential and star pattern on 4 nodes . Which one of these patterns ( considered as induced subgraphs ) has higher frequency in the pure random graph introduced in the abstract ? To answer this , we need to reconfigure the two motifs into square configurations , like in Fig . 3 , wherein the se - quential pattern is Motif 1 , and the star pattern is Motif 2 . We prove in form of a theorem that , because of symmetries , the sequential pattern has larger frequency than the star pattern . Motifs and Graphlets . The patterns shown in Fig . 3 can be found in the case of statistical mechanics as motifs . They were initially explored as partial subgraphs which recur in real - world complex networks with higher frequency than in randomized networks [ 7 ] . However , in computer science and bioinformatics these patterns are also known as graphlets [ 8 , 9 ] and are considered as small connected non - isomorphic induced subgraphs . In this study , we call them motifs but we strictly refer to induced subgraphs with 4 nodes . We display the motifs ( as in [ 10 ] ) m i with i = 1 , . . . , 6 in Fig . 3 . Figure 3 : The 6 Motifs ; the connected tetrads of nodes In the case of bioinfiormatics [ 8 , 9 ] , the authors define relative graphlet frequency distance and compare protein - to - protein interaction networks with various random graph models . In the case of statistical mechanics , a transformation from univariate time series to complex networks has been introduced , and studied the frequency of different connected tetrads of nodes in these networks [ 11 ] . Motifs’ distribution could distinguish different types of continuous dynamics : periodic , chaotic and periodic with noise , as well as chaotic maps , hyperchaotic maps , and noise data when applied to discrete data . Particularly , the authors proved that noisy periodic signals correspond to random visibility networks whereas chaotic time series generate visibility networks that exhibit small - world and scale - free features [ 12 ] . Inspired by statistical mechanics , our crucial hypothesis is that we can use such patterns to identify the topology of real - world observed networks . Thus , we motivate the connected tetrads of nodes of Fig . 3 ( considered as induced subgraphs and called motifs ) in order to introduce a novel classification method . The article is summarized as follows : Section 2 , we introduce the novel Network Randomness Index ( NRI ) , in Section 3 , we validate the efficiency of the NRI by his ability to discern random and small - world networks , in Section 4 we display how NRI can be used for anomaly detection in multivariate time series , and in Section 5 we conclude , discuss our findings , and motivate the readers to new horizons for further investigations . All relevant algorithms are implemented in Matlab ( see https : / / github . com / GeorgiosArg / Diagnosis - of - Small - World - Bias - in - Random - Graphs ) and are described in the Appendix . 2 . The Randomness Index In this section we introduce the Network Randomness Index ( NRI ) and elaborate on the prelim - inary thoughts . Particularly , section 2 . 1 provides the background with the preliminary definitions , insection 2 . 2 we define the critical point and introduce the NRI as the euclidean distance between the critical point and another point that corresponds an observed network , and in Section 2 . 3 , we elaborate on the preliminary thoughts by proving that the sequential pattern has larger frequency than the star pattern . 2 2 . 1 . Preliminaries We start by defining the notion of a graph . Definition 1 ( Graph ) . A graph G is a pair ( V , E ) where V is a set of vertices ( or nodes ) , and E ⊆ V × V is a set of edges ( or links ) , with | V | ∈ N being the number of nodes and | E | ∈ N being the number of edges . We denote with f m i the number of occurences of the motif m i considered as induced subgraph in a graph G . Next , we define the relative frequency , also introduced in [ 13 ] as a measure of randomness , as follows : Definition 2 ( Relative Frequency ) . Let f m i be the frequency of motif m i . We define the relative frequency of motif m i as : F m i = f m i (cid:80) 6 i = 1 f m i ( 1 ) The critical point is a 6 - dimensional point where each coordinate corresponds to the relative fre - quency of a motif and , for a given graph G , is formally defined as follows : Definition 3 ( Relative Frequency Point ( RFP ) ) . Let G be a graph and F m i be the relative fre - quency of motif m i . We define the relative frequency point of a graph G as : F G = ( F m 1 , F m 2 , F m 3 , F m 4 , F m 5 , F m 6 ) We finish this section by reminding the reader that f m i is the absolute frequency of the motif m i , F m i its relative frequency ( i ∈ { 1 , . . . , 6 } ) , and F G the RFP of a graph G . 2 . 2 . The critical point We start by defining the notion of an Erdos - Renyi Random Graph . Definition 4 ( Erdos - Renyi Random Graph ) . An Erdos - Renyi ( ER ) random graph ( denoted by G ) is a pair ( V , p ) where V is a set of vertices while p ∈ [ 0 , 1 ] is the probability of occurence of an edge between any pair of vertices . As we have mentioned before , the critical point is a six - dimensional point whose coordinates are the relative frequencies of each motif . In the case of Erdos - Renyi random graphs , these frequencies can be calculated analytically by closed form formulas . We derive these formulas in this section according to Table 1 . The first column presents the 11 sets of 4 - node graphs , where each set is a class of isomorphic graphs . Note that there is a bijection that preserves edges , mapping a pattern to each other pattern within the same set . The column order contains the number of links of each pattern in class , the patterns refers to the number of patterns in each set , in description we provide a description of the motif , and the last column contains the probability of occurence of each pattern . Note that graphs which belong to the same set occur with equal probability in ER graphs , i . e . , the probability of occurence of a pattern is a graph invariant under graph isomorphism . For a network of size n , there are (cid:0) n 4 (cid:1) tetrads of nodes . If we denote with N i the number of isomorphs the motif i has , and l i its order ( the number of links of i ) , the absolute frequency f m i is given by the following lemma . Lemma 1 . Let G = ( V , p ) be an Erdos - Renyi random graph with | V | = n > 4 and m i = ( 4 , l i ) be a motif ( induced subgraph ) on 4 nodes with l i edges . We denote with N m i denote the number of isomorphic patterns of motif m i . The expected frequency of motif m i in the random graph G is given by the following formula : f m i ( n , p ) = (cid:18) n 4 (cid:19) ∗ N m i ∗ p l i ∗ ( 1 − p ) 6 − l i ( 2 ) 3 Classes of isomorphic graphs order patterns description probability of pattern occurence 0 1 unconnected ( 1 − p ) 6 1 6 unconnected 6 · p · ( 1 − p ) 5 2 12 unconnected 12 · p 2 · ( 1 − p ) 4 2 3 unconnected 3 · p 2 · ( 1 − p ) 4 3 4 unconnected 4 · p 3 · ( 1 − p ) 3 3 4 motif 2 ; star 4 · p 3 · ( 1 − p ) 3 3 12 motif 1 ; sequentially connected 12 · p 3 · ( 1 − p ) 3 4 3 motif 3 3 · p 4 · ( 1 − p ) 2 4 12 motif 4 12 · p 4 · ( 1 − p ) 2 5 6 motif 5 6 · p 5 · ( 1 − p ) 6 1 motif 6 p 6 Table 1 : Detailed description of all 4 - node patterns . 4 A formula akin to equation 2 was previously established for directed networks in [ 14 ] . In this study , the authors , employing approximations for the average number of subgraphs in an ensemble of random networks with an arbitrary degree sequence , reached the conclusion that specific subgraphs tend to occur more frequently in real - world and scale - free networks featuring a specified power - law node degree distribution compared to randomized graphs . Moreover , they consider the number of isomorphic patterns N m i as a parameter λ of order 1 ( O ( 1 ) ) . Nevertheless , these values merely represent the number of isomorphic patterns that a motif ( induced subgraph ) can exhibit . We denote with F G ( n , p ) the RFP of the Erdos - Renyi graph and call it critical point which is the point of Fig . 1 . The uniqueness of the critical point is secured by the following theorem of Erdos and Rado . Theorem 2 ( Rado Graph ) . Let G = ( V , p ) be the random graph over a countably infinite set of vertices ( | V | = n = ∞ ) for some fixed p . The Rado graph G ∞ = ( ∞ , p ) is unique under isomorphism . In words , the theorem guarantees the following counter - intuitive phenomenon ; if several people have an infinitely big paper with infinitely many nodes on it and , for each pair of nodes , they perform the toin coss experiment mentioned in the abstract , then everybody will end up with the same graph ( under isomorphism ) . The previous theorem can be found in [ 15 ] ( pages 228 - 229 ) . The following Corollary immediately follows : Corollary 3 ( Critical Point ) . Let G ∞ be the Rado graph . The F G ∞ is unique . The following remark is crucial for the development of the classification method of Section 3 . Remark 1 ( Network Randomness Index ) . Consider an observed real world network G = ( V , E ) and F G its RFP . The greater the proximity of the observed network’s frequency vector , denoted as F G , to the critical point F G in terms of Euclidean distance ( ∥F G − F G ∥ ) , the higher the randomness exhibited by the observed network . This happens because for a random graph on a finite n number of vertices , the point F G lies in a concrete neighborhood ( see Fig . 1 ) . 2 . 3 . On the preliminary thoughts We next provide a theorem that relates the frequencies of subgraphs in the Rado graph , with their isomorphs . The following theorem gives also the solution to the problem stated in the preliminary thoughts of the introduction . Theorem 4 . Let G 1 = ( V 1 , E 1 ) , G 2 = ( V 1 , E 1 ) be two graphs with the same number of nodes | V 1 | = | V 2 | = m and the same number of edges | E 1 | = | E 2 | = l . The expected frequency of G 1 is higher than the expected frequency of G 2 in a random graph G = ( V , p ) with | V | = n ≥ m if and only if G 1 has more isomorpfic patterns than G 2 , i . e . , N G 1 ≥ N G 2 . Proof . We denote with N G 1 , N G 2 the number isomorphic patterns , and f G 1 , f G 2 the relative fre - quency of the graphs G 1 , G 2 respectively . We also denote with m the number of nodes which is equal . It holds that : f G 1 < f G 2 ⇔ ⇔ N G 1 · (cid:18) n m (cid:19) · p l · ( 1 − p ) m ( m − 1 ) 2 − l < N G 2 · (cid:18) n m (cid:19) · p l · ( 1 − p ) m ( m − 1 ) 2 − l ⇔ ⇔ N G 1 < N G 2 The following remark immediately follows . Remark 2 . Considering the preliminary thoughts of the Section 1 , it is straightforward that the sequential motif has higher frequency than a star motif in the Rado graph . We refer the reader to Table 1 in the column patterns . Notice that the star motif has 4 patterns while the sequential motif has 12 patterns . Density is a critical parameter that can dramatically change the frequency of motifs . It is particu - larly easy to calculate . 5 Definition 5 ( Density ) . Let G = ( V , E ) be a subgraph with | V | = m and | E | = l . The density of G is defined as follows : d G = l m ∗ ( m − 1 ) 2 Next , we provide a Theorem that guarantees that a subgraph achieves its maximum frequency in a random graph when the density of the subgraph equals with the parameter p of the random graph i . e . the probability of occurrence of an edge . To prove this we use the maximum likelihood estimation . Theorem 5 ( Maximum Frequency of a Graph ) . Let G = ( U , E ) be a graph with N G isomorphic patterns | U | = m , | E | = l , and G = ( V , p ) be a random graph with | V | = n > m . The finite induced graph G maximizes its frequency in G when d G = p . Proof . The frequency of G in the random graph G is given by the following formula : F G = N G ∗ (cid:18) n m (cid:19) ∗ p l ∗ ( 1 − p ) m ∗ ( m − 1 ) 2 − l The function log ( x ) is monotonic on x and , hence , it holds that max ( F G ) = max ( log ( F G ) ) . We first calculate log ( F G ) : log ( F G ) = log ( N G ∗ (cid:18) n m (cid:19) ∗ p l ∗ ( 1 − p ) m ∗ ( m − 1 ) 2 − l ) ⇐⇒ = log ( N G ) + log ( (cid:18) n m (cid:19) ) + log ( p l ) + log ( ( 1 − p ) m ∗ ( m − 1 ) 2 − l ) ⇐⇒ = log ( N G ) + log ( (cid:18) n m (cid:19) ) + l ∗ log ( p ) + ( m ∗ ( m − 1 ) 2 − l ) ∗ log ( 1 − p ) To find the maximum in terms of the probability p , we calculate the derivative of log ( F G ) up to p and equalize it to zero : ∂log ( F G ) ∂p = 0 = ⇒ l ∗ 1 p + ( m ∗ ( m − 1 ) 2 − l ) ∗ 1 1 − p = 0 = ⇒ ( p − 1 ) ∗ l + p ∗ ( m ∗ ( m − 1 ) 2 − l ) = 0 = ⇒ p ∗ l − l + p ∗ ( m ∗ ( m − 1 ) 2 − l ) = 0 = ⇒ p ∗ l + p ∗ ( m ∗ ( m − 1 ) 2 − l ) = l = ⇒ p ∗ ( l + m ∗ ( m − 1 ) 2 − l ) = l = ⇒ p ∗ ( m ∗ ( m − 1 ) 2 ) = l = ⇒ p = l m ∗ ( m − 1 ) 2 Therefore , we conclude that the frequency is maximized when p = d G ( see Definition 5 ) . 3 . Classification of Small - World and Random Graphs In this section , we present a novel classification method designed to differentiate between small - world and random networks . Section 3 . 1 introduces the Watts - Strogatz small - world networks along with the classification method . The validation of our method through Monte Carlo simulations is carried out in Section 3 . 2 . Further refinement of the Monte Carlo experimental validation , aimed at assessing the classification power of our framework , is discussed in Section 3 . 3 . 3 . 1 . Small - world Networks and Classification Methodology 6 Small - world Networks . We first give the definition of a small - world network . Definition 6 ( Watt - Strogatz Small - world Networks ) . A small - world network , denoted by W , is a triple ( V , k , p r ) where V is a set of nodes circular configuration ( see top part of Fig . 4 ) , k is the number of neighbors that each node is attached , and p r ∈ [ 0 , 1 ] the probability of rearrangement of an edge . Figure 4 : Visual representation of the relative frequency point of the regular lattice to the critical point . The top part of the figure was takes from [ 6 ] . Note that p r is denoted by p in the figure . Consider the regular lattice depicted in the upper left part of Fig . 4 . This lattice forms a small - world network denoted as W with k = 2 - each node is connected to precisely two of its nearest neighbors . Additionally , p r = 0 since no edges undergo rearrangement . Moving to the upper middle part of the figure , we see a small - world network with p r = 0 . 1 . The top right part of the figure illustrates a W with p r = 1 . Small - world networks , characterized by different values of p r , exhibit unique relative frequency points in the six - dimensional space of motif frequencies . Specifically , when p r = 0 , the corresponding Relative Frequency Point ( RFP ) lies outside the neighborhood of the Critical Point depicted in Fig . 1 ( highlighted by the red point in the bottom left part of Fig . 4 ) . However , as p r increases , the RFP progressively approaches the neighborhood of the critical point . In the limit where p r → 1 , it crosses the boundary and enters the neighborhood , as indicated by the blue point in Fig . 4 . The Relative Frequency Point ( RFP ) of a small - world network is denoted as F W , representing a function of the three parameters ( n , p r , k ) , as defined in Definition 6 . The empirical computation of F W involves generating 100 networks with varying values of p r and k for a standard n . The RFP is then calculated for each of these networks , and F W ( n , p r , k ) is determined as the average across this set of 100 networks . The detailed steps for this empirical computation are elucidated in Algorithm 1 in Appendix A . Hypothesis . The crucial hypothesis of this section is the following : suppose that we have an observed real - world network , could we find which model generates ( approximates ) it ? To answer this question we develop the following classification methodology . The Classification Methodology . Considering an observed network as a graph G = ( V , E ) , as defined in Def . 1 , our approach for identifying the model that best approximates an observed network is summarized in the following 2 steps . Step 1 : To begin , we calculate the RFP of the observed network , denoted as F G , utilizing the Comput - eRFP routine ( see A . 2 ) . Then , we apply the Embedding Algorithm , detailed in Appendix B , to map the network topologies into their corresponding RFPs for the standard size n = | V | of the observed network . For instance , the random graph is positioned within the critical point F G , whereas the small - world network finds its place at another point , denoted by F W . 7 Step 2 : Following the insights of Remark 1 , the generative model ( target function ) of the observed network is the one that minimizes the euclidean distance between the RFP F G of step 1 and the RFP of the corresponding model . In other words , the observed network is random if ∥F G − F G ∥ < ∥F W − F G ∥ and small - world if ∥F G − F G ∥ > ∥F W − F G ∥ . We propose that the previous classification method of an observed network to its approximated topology ( small - world or random ) . The method is summarized in Algorithm 3 . 3 . 2 . Experimental Validation with Monte Carlo Simulations In this section , we validate our classification method through Monte Carlo simulations . This involves ( i ) generating a substantial number of networks using each model , with variation in each model parameter ( refer to the explanation in Configuration ) , ( ii ) presenting the Results , which detail the models to which the generated networks are mapped using our classification method , and ( iii ) drawing conclusions from these results in the Interpretation . Configuration . We generate networks for each one of the models ER ( n , p ) , WS ( n , k , p r ) and for each one of their parameters n , p , k , p r . Particularly , for n = 25 , 50 we generate 100 networks but for n = 75 we generate only 50 networks for better computational efficiency . We validate our method to 56 . 750 generated networks overall : • 12 . 900 networks of size n = 25 ( 900 ER , i . e . , 100 for each p = 0 . 1 , . . . , 0 . 9 + 12 . 000 WS , 100 for each k = 2 , . . . , 24 for each p r = 0 , 0 . 1 , . . . , 0 . 9 ) , • 24 . 900 networks of size n = 50 ( 900 ER , i . e . , 100 for each p = 0 . 1 , . . . , 0 . 9 + 24 . 000 WS , 100 for each k = 2 , . . . , 48 for each p r = 0 , 0 . 1 , . . . , 0 . 9 ) , • 18 . 950 networks of size n = 75 , ( 900 ER , i . e . , 100 for each p = 0 . 1 , . . . , 0 . 9 + 18 . 500 WS , 50 for each k = 2 , . . . , 74 for each p r = 0 , 0 . 1 , . . . , 0 . 9 ) . We then classify each one of these networks according to the classification methodology 3 . 1 1 . Results . We present part of the results in the following tables . In the first row , the initial cell contains the size of the generated networks ( n ) , while the remaining cells specify the models employed for generating the 100 networks . The models to which the classification method maps the generated networks are presented in the first column . Specifically , ER corresponds to Erdos - Renyi , with the adjacent number indicating the probability ( p ) of edge occurrence . WS represents Watts - Strogatz , where the first number adjacent to it is the probability of edge rearrangement ( p r ) , and the second number denotes the value of parameter k - the number of neighbors to which each node is attached . 1 For the experimental validation , we also considered scale - free networks constructed both by the Barabasi - Albert preferential attachment mechanism and another method that constructs scale - free networks with exponential degree distribution . However , we do not present these results because it is out of the scope of this article . The full results can be found on https : / / github . com / GeorgiosArg / Diagnosis - of - Small - World - Bias - in - Random - Graphs . Scale - free networks can be identified by naked eye because of their low density . 8 n = 25 ER 0 . 1 ER 0 . 2 ER 0 . 3 ER 0 . 4 ER 0 . 5 ER 0 . 6 ER 0 . 7 ER 0 . 8 ER 0 . 9 ER 0 . 1 3 1 0 0 0 0 0 0 0 ER 0 . 2 5 31 2 0 0 0 0 0 0 ER 0 . 3 0 7 56 4 0 0 0 0 0 ER 0 . 4 0 0 2 48 0 0 0 0 0 ER 0 . 5 0 0 0 0 50 1 0 0 0 ER 0 . 6 0 0 0 0 1 61 0 0 0 ER 0 . 7 0 0 0 0 0 1 50 2 0 ER 0 . 8 0 0 0 0 0 0 1 66 0 ER 0 . 9 0 0 0 0 0 0 0 1 51 WS 0 . 8 , 3 0 8 8 0 0 0 0 0 0 WS 0 . 8 , 4 0 0 6 1 0 0 0 0 0 WS 0 . 8 , 5 0 0 2 14 0 0 0 0 0 WS 0 . 8 , 6 0 0 0 4 12 0 0 0 0 WS 0 . 9 , 2 5 4 0 0 0 0 0 0 0 WS 0 . 9 , 3 0 6 2 0 0 0 0 0 0 WS 0 . 9 , 4 0 0 4 2 0 0 0 0 0 WS 0 . 9 , 5 0 0 1 16 0 0 0 0 0 WS 0 . 9 , 6 0 0 0 2 22 0 0 0 0 WS 0 . 9 , 7 0 0 0 0 4 19 0 0 0 WS 0 . 9 , 8 0 0 0 0 0 3 17 0 0 WS 0 . 9 , 9 0 0 0 0 0 0 7 11 0 WS 0 . 9 , 10 0 0 0 0 0 0 0 12 9 WS 0 . 9 , 11 0 0 0 0 0 0 0 0 38 Overall 13 57 83 91 89 85 75 92 98 Table 2 : Classification of 900 ER networks with n = 25 nodes ; 100 networks as p varies ( 0 . 1 , 0 . 2 . . . 0 . 9 ) . It is evident that when p = 0 . 1 , the method struggles to classify effectively due to the absence of formed motifs . As p increases , facilitating motif formation , the method achieves more accurate classification for nearly 50 % of the networks . Notice , however , that most of the networks that are not classified to the correct model , are classified to “relative” models , i . e . , WS models with high probability of rearrangement of an edge , and similar density . For example , if p = 0 . 5 , the generated network has expected density d G = 0 . 5 . A WS network has density d W = n × k n × ( n − 1 ) 2 and , hence , if d W = 0 . 5 , we have that k = 6 . We display the relevant situation with bold in the corresponding entries of Table 2 . For n = 50 , we observe the same phenomenon that we present in Table 3 . For example , considering the case of 100 ER ( 25 , 0 . 5 ) graphs , we notice that 58 networks are correctly classified to their generative model , but 41 networks are classified to the related models of similar equal density . Interpretation . The classification method fails to classify correctly a large amount of ER graphs be - cause ER graphs are mapped to “related” models . This phenomenon arises due to the combinatorial explosion of generative models . To mitigate the combinatorial explosion , we decrease the number of possible generative models by considering known the density of the generated networks . 9 n = 50 ER 0 . 1 ER 0 . 2 ER 0 . 3 ER 0 . 4 ER 0 . 5 ER 0 . 6 ER 0 . 7 ER 0 . 8 ER 0 . 1 47 0 0 0 0 0 0 0 ER 0 . 2 0 40 0 0 0 0 0 0 ER 0 . 3 0 0 59 0 0 0 0 0 ER 0 . 4 0 0 0 58 0 0 0 0 ER 0 . 5 0 0 0 0 58 0 0 0 ER 0 . 6 0 0 0 0 0 49 0 0 ER 0 . 7 0 0 0 0 0 0 61 0 ER 0 . 8 0 0 0 0 0 0 0 67 WS 0 . 9 , 2 4 0 0 0 0 0 0 0 WS 0 . 9 , 3 9 0 0 0 0 0 0 0 WS 0 . 9 , 4 11 0 0 0 0 0 0 0 WS 0 . 9 , 5 0 34 0 0 0 0 0 0 WS 0 . 9 , 6 0 12 0 0 0 0 0 0 WS 0 . 9 , 7 0 0 13 0 0 0 0 0 WS 0 . 9 , 8 0 0 21 0 0 0 0 0 WS 0 . 9 , 9 0 0 0 7 0 0 0 0 WS 0 . 9 , 10 0 0 0 20 0 0 0 0 WS 0 . 9 , 11 0 0 0 12 0 0 0 0 WS 0 . 9 , 12 0 0 0 0 26 0 0 0 WS 0 . 9 , 13 0 0 0 0 15 0 0 0 WS 0 . 9 , 14 0 0 0 0 0 16 0 0 WS 0 . 9 , 15 0 0 0 0 0 33 0 0 WS 0 . 9 , 16 0 0 0 0 0 0 1 0 WS 0 . 9 , 17 0 0 0 0 0 0 15 0 WS 0 . 9 , 18 0 0 0 0 0 0 5 0 WS 0 . 9 , 19 0 0 0 0 0 0 0 10 WS 0 . 9 , 20 0 0 0 0 0 0 0 7 Overall 71 86 93 97 99 98 82 84 Table 3 : Classification of 900 ER networks with n = 50 nodes ; 100 networks as p varies ( 0 . 1 , 0 . 2 . . . 0 . 9 ) . 3 . 3 . Refinement of the Experimental Validation Considering the density of an observed network known , we reduce the number of possible generative models by calculating the parameter p of the Erdos - Renyi random graph , and the parameter k of the Watts - Strogatz model . Let d G denote the density of an observed network . Configuration . The configuration of the refined experimental validation is the same as in 3 . 2 ; we generate the same amount of networks ( 56 . 750 ) . However , we first compute their density ( d G ) before we classify them with our method . With this , we can calculate the parameters p of the ER graphs and k of the WS graphs ; If we want to approximate the density of the observed network with the ER model we have to set p = d G = d G . If we want to approximate the density of the observed network with the WS model , we have to set : d W = d G = ⇒ d G = n × k n × ( n − 1 ) 2 = ⇒ k = d G × ( n − 1 ) 2 Results . The results are presented as in previous section ; in the first row , the initial cell contains the size of the generated networks ( n ) , while the remaining cells specify the models employed for generating the 100 networks . The models to which the classification method maps the generated networks are presented in the first column . n = 25 ER 0 . 1 ER 0 . 2 ER 0 . 3 ER 0 . 4 ER 0 . 5 ER 0 . 6 ER 0 . 7 ER 0 . 8 ER 0 . 9 ER 55 64 72 79 88 90 97 97 96 WS 0 . 6 11 3 3 3 1 2 0 0 0 WS 0 . 7 4 1 7 0 1 2 0 0 0 WS 0 . 8 1 3 3 16 10 6 3 3 4 WS 0 . 9 12 21 11 0 0 0 0 0 0 Table 4 : Classification of 900 ER networks with n = 25 nodes ; 100 networks as p varies ( 0 . 1 , 0 . 2 . . . 0 . 9 ) . 10 n = 50 ER 0 . 1 ER 0 . 2 ER 0 . 3 ER 0 . 4 ER 0 . 5 ER 0 . 6 ER 0 . 7 ER 0 . 8 ER 0 . 9 ER 74 75 83 85 96 97 95 99 95 WS 0 . 6 5 2 0 0 0 0 0 0 1 WS 0 . 7 3 4 1 0 0 0 0 0 1 WS 0 . 8 4 2 3 2 1 0 0 0 0 WS 0 . 9 11 16 13 13 3 3 5 1 2 Table 5 : Classification of 900 ER networks with n = 50 nodes ; 100 networks as p varies ( 0 . 1 , 0 . 2 . . . 0 . 9 ) . n = 75 ER 0 . 1 ER 0 . 2 ER 0 . 3 ER 0 . 4 ER 0 . 5 ER 0 . 6 ER 0 . 7 ER 0 . 8 ER 0 . 9 ER 50 50 50 50 50 50 50 50 50 Table 6 : Classification of 450 ER networks with n = 75 nodes ; 50 networks as p varies ( 0 . 1 , 0 . 2 . . . 0 . 9 ) . n = 75 WS 0 . 6 , 10 WS 0 . 6 , 11 WS 0 . 6 , 12 WS 0 . 6 , 13 WS 0 . 6 , 14 WS 0 . 6 , 15 WS 0 . 6 , 16 WS 0 . 6 , 17 WS 0 . 6 , 18 WS 0 . 6 , 19 ER 0 0 0 0 0 0 0 0 0 0 WS 0 . 0 0 0 0 0 0 0 0 0 0 0 WS 0 . 1 0 0 0 0 0 0 0 0 0 0 WS 0 . 2 0 0 0 0 0 0 0 0 0 0 WS 0 . 3 0 0 0 0 0 0 0 0 0 0 WS 0 . 4 0 0 0 0 0 0 0 0 0 0 WS 0 . 5 2 5 3 7 7 3 1 8 4 3 WS 0 . 6 40 38 44 37 34 43 45 35 40 41 WS 0 . 7 8 7 3 6 9 4 4 7 6 6 WS 0 . 8 0 0 0 0 0 0 0 0 0 0 WS 0 . 9 0 0 0 0 0 0 0 0 0 0 Table 7 : Classification of 500 WS ( 0 . 6 , k ) networks with n = 75 nodes for different values of k . n = 75 WS 0 . 9 , 10 WS 0 . 9 , 11 WS 0 . 9 , 12 WS 0 . 9 , 13 WS 0 . 9 , 14 WS 0 . 9 , 15 WS 0 . 9 , 16 WS 0 . 9 , 17 WS 0 . 9 , 18 WS 0 . 9 , 19 ER 1 0 1 0 0 1 1 1 1 3 WS 0 . 0 0 0 0 0 0 0 0 0 0 0 WS 0 . 1 0 0 0 0 0 0 0 0 0 0 WS 0 . 2 0 0 0 0 0 0 0 0 0 0 WS 0 . 3 0 0 0 0 0 0 0 0 0 0 WS 0 . 4 0 0 0 0 0 0 0 0 0 0 WS 0 . 5 0 0 0 0 0 0 0 0 0 0 WS 0 . 6 0 0 0 0 0 0 0 0 0 0 WS 0 . 7 0 2 0 0 0 0 0 0 0 0 WS 0 . 8 12 14 10 16 13 13 11 15 8 10 WS 0 . 9 37 34 39 34 37 36 38 34 41 37 Table 8 : Classification of 500 WS ( 0 . 9 , k ) networks with n = 75 nodes for different values of k . Interpretation . The effectiveness of the classification method is more sound for larger networks , as evident in Tables 4 , 5 , and 6 . This phenomenon can be attributed to Theorem 2 , which ensures the uniqueness of the Rado graph and the resilience of the neighborhood surrounding the critical point . Conversely , as observed in Tables 7 and 8 , the method demonstrates robustness in the opposite direction . The majority of networks are accurately mapped to the corresponding small - world model with the parameters used for their generation . Let us consider the distinction between the pure random and the biased graph that we introduced in the abstract . In Table 5 , we see that , for n = 50 and p = 0 . 5 , there still exist 4 networks that look biased ( numbers in bold ) . The latter leads us to the fact that any bias can be can be introduced arbitrarily . Conversely , in Table 8 , we see that biased networks can also look random ( numbers in bold ) . 4 . Application to Temporal Networks 11 We apply RI to temporal networks ; networks with a standard number of nodes and whose connec - tivity changes throughout time i . e . links between any pair of nodes can appear or disappear between two time steps . In Section 4 . 1 we discuss the process of deriving a temporal correlation network from a dynamical system studied by means of multivariate time series . Here , the connections between the nodes represent correlation . In Section 4 . 2 , we apply RI to stock networks [ 16 ] . 4 . 1 . Derivation of Temporal Correlation Network Each node v i of the derived temporal networks represents a vector x i of n observed values which correspond to measurements of time series data : ¯ x i = [ x i ( 1 ) , . . . x i ( n ) ] Example 1 . We consider a dataset that contains the Morgan Stanley Capital International’s ( MSCI ) market capitalization weighted index of 55 developed markets . This means that the derived network consists of 55 nodes i . e . V = { v 1 , . . . v 55 } , where each node v i corresponds to a vector of values ¯x i of a particular market . Each vector ¯x i comprises 1305 daily indices for each market in the period 5 of March 2004 until 5 of March 2009 , excluding weekends and holidays . Thus , ¯ x i = [ x i ( 1 ) , . . . , x i ( 1305 ) ] . For example , at time t = 15 the market i has the value x i ( 15 ) . We first apply a procedure of standard preprocessing steps to the original time series data . The overall procedure is summarized in Fig . 5 . ¯ x i Dismiss Trends and Periodicity −−−−−−−−−−−−−−−−−−−−→ ¯ y i Prewhitening −−−−−−−−→ ¯ z i Figure 5 : Dismiss Trends and Periodicity from the original timeseries ( first arrow ) , and then prewhiten ( second arrow ) . Dismiss Trends and Periodicity . In order to relieve the time series from tendency and periodicity , we compute the first differences of the logarithms of the original time series for the shake of stationarity : y i ( t − 1 ) = log (cid:0) x i ( t ) (cid:1) − log (cid:0) x i ( t − 1 ) (cid:1) , ∀ i ∧ t ∈ [ 2 , . . . , 1305 ] ( 3 ) The difference reduces the tendency and the use of the logarithm decreases the variance . Therefore , we obtain the relevant returns ¯y i for every market i . Timeseries Prewhitening . The prewhitening process removes the autocorrelations of the timeseries ¯y i which may cause spurious cross - correlations . Firstly , the mean is subtracted from each ¯y i ( t ) using the formula ¯ y ′ i = ¯ y i − ¯ µ i where ¯ µ i represents a vector whose entries is the mean value of ¯y i . Subsequently , an autoregressive ( AR ) model of order p = 20 is fitted to each ¯y ′ i time series using the arx ( ) function in Matlab . The AR model provides coefficients for the estimated model . Predicted values ¯y ′′ i are obtained using the predict ( ) function , and the mean subtracted earlier is added back to yield ¯y ′′′ i . Finally , the residuals ¯z i are calculated by subtracting the predicted values from the observed values i . e . ¯z i = ¯y ′′′ i − ¯y i , ∀ i . To conclude , the final result of the whole transformation is an array of residuals ¯z i . Once the residuals are effectively whitened , they can be analyzed using standard statistical techniques without the complication of autocorrelation . We explain more analytically the prewhitening process in E . Dataset partition . We now partition the residuals ¯z i into an equal amount of chronologically consecu - tive measurements : ¯ z i 1 = [ z i ( 1 ) , . . . , z i ( k ) ] ¯ z i 2 = [ z i ( k + 1 ) , . . . , z i ( 2 k ) ] . . . ¯ z i m = [ z i (cid:0) ( m − 1 ) · k + 1 (cid:1) , . . . , z i ( n ) ] The ¯z i l represent the time windows , and a network is derived for each of them . 12 Example 2 . Consider the MSCI dataset . We first perform prewhitening as described previously to obtain the residuals ¯z i . We then partition ¯z i into 87 , each one of them containing 15 values z i ( t ) in a chronological order : ¯ z i 1 = [ z i ( 1 ) , . . . , z i ( 15 ) ] , ¯ z i 2 = [ z i ( 16 ) , . . . , z i ( 30 ) ] , . . . , ¯ z i 87 = [ z i ( 86 · 15 + 1 ) , . . . , z i ( 1305 ) ] . Create the adjacency matrices . For each ¯z i l with 1 ≤ l ≤ m , we derive a correlation network , denoted by A ( l ) . In this network , the markets are represented by nodes while statistical significant cross - correlations according to the last measurement of the time window ¯z i l between the markets are added as links . All previous measurements are utilized for assessing the statistical significance of the cross - correlation at the last time - point . For the evaluation of statistical significance , we conduct a significance test based on the randomization of the original time series . This testing procedure involves randomizing the original time series and requires specifying a significance level α , which determines the density of the correlation network . A larger α implies more statistically significant correlations and , consequently , higher density of the network . The overall process of deriving the adjacency matrices is summarized in Fig . 6 . ¯ z i Paritioning −−−−−−−→ [ ¯ z 1 , . . . , ¯ z m ] Correlation Matrices −−−−−−−−−−−−−→ [ A ( 1 ) , . . . , A ( m ) ] Figure 6 : To derive the adjacency matrices , we first ( left arrow ) partition the residuals according to time windows , and then ( right arrow ) compute the adjacency matrices . 4 . 2 . Application to Stock Networks Hypothesis and Dataset . We conduct an econometric application by considering stock networks ; net - works whose nodes correspond to stock indexes while edges correspond to statistically significant correlation between the stock indexes . We propose that our framework can detect anomalies and change - points in systems studied by means of multivariate time series . Configuration . We partition the 55 time - sequence vectors of the MSCI dataset into 29 and 87 sub - vectors , each one of them containing 45 and 15 returns in a time row . That is : ¯ z i 1 = [ z i ( 1 ) , . . . , z i ( 45 ) ] ¯ z i 2 = [ z i ( 46 ) , . . . , z i ( 90 ) ] . . . ¯ z i 29 = [ z i ( 28 · 45 + 1 ) , . . . , z i ( 1305 ) ] while in the second case we have : ¯ z i 1 = [ z i ( 1 ) , . . . , z i ( 15 ) ] ¯ z i 2 = [ z i ( 16 ) , . . . , z i ( 30 ) ] . . . ¯ z i 87 = [ z i ( 86 · 15 + 1 ) , . . . , z i ( 1305 ) ] Compute randomness . In order to compute the randomness of each element we follow Fig . 7 . For each A ( l ) , we compute the relevant RFP denoted by F A ( l ) ( as described in Routine A . 2 and displayed in the upper left arrow of Fig . 7 ) . We also compute the density of A ( l ) denoted by d A ( l ) and insert it as the probability parameter for the calculation of the critical point F G ( 55 , d A ( l ) ) ( see bottom left arrow of Fig . 7 ) . The randomness for a particular time window l is the Euclidean distance : (cid:13)(cid:13) F A ( l ) − F G ( 55 , d A ( l ) ) (cid:13)(cid:13) ( 4 ) 13 A ( l ) F A ( l ) F G ( 55 , d A ( l ) ) (cid:13)(cid:13) F A ( l ) − F G ( 55 , d A ( l ) ) (cid:13)(cid:13) Figure 7 : The computation of randomness at one glance . Results . We summarize the results in Fig . 8 . Figure 8 : ( Left ) The values of (cid:13)(cid:13) F A ( l ) − F G ( 55 , d A ( l ) ) (cid:13)(cid:13) for 29 snapshots . ( Right ) The values of (cid:13)(cid:13) F A ( l ) − F G ( 55 , d A ( l ) ) (cid:13)(cid:13) for 87 snapshots . The x - axis are the dates and the y - axis the value of randomness . The figures display the results for different values of α as we display in the legends in the upper - right part of each figure . Particularly , the empty circles are the values of randomness in the case that α = 0 . 05 , the full circle in the case that α = 0 . 03 , and the triangle in the case that α = 0 . 1 . Interpretation . As we see in Fig . 8 , the peak is reached in August 2007 ( just before 2008 in the X - axis ) , marking the onset of the 2007 - 2008 global financial crisis - a profound economic downturn that unfolded in the early 21st century . It’s worth noting that the zenith remains consistent across the three selected values of the parameter α . Even when the dataset is divided into 29 subsets instead of 87 , the results regarding the timing of the NRI’s maximum value are unchanged . The application to stock networks , in this case , shows that NRI is high during recession periods ; fact that reassures about the robustness of NRI . We propose that NRI can be utilized for anomaly detection in dynamical systems studied by means of multivariate time series . 5 . Conclusion and Future Work We propose a graph embedding technique where we represent a graph as a point in the 6 dimensional space . Each coordinate in this space is the relative frequency of a particular network motif . By defining the unique point of the random ( Rado ) graph in the same 6D space , we can quantify the randomness of an observed network the proximity of its point to the point of the random graph . Motifs drive network randomness beyond probability . 14 Future Work . By extending to directed graphs , we can study the randomness of causality networks . It will be also interesting to apply the RI to other dynamical systems studied under means of multivariate time series ( like EEGs ) . Some rough simulations that we performed , show that multivariate time series with trend and autocorrelation construct random networks in contrast with prewhitened time series ( noisy signals ) that tend to construct ordinary lattices . Instead of the Pearson correlation coefficient and the prewhitening , one could also use the mutual information . Last but not least , an extension to graphlets of size 5 is also feasible . Related Work . The initial work was containing comparative analysis and separation between three different network types : random , small - world and scale - free networks . For the case of scale - free net - works we considered two different generators ; the Barabasi - Albert mechanism and a mechanism that was generating scale - free networks with a power - law degree distribution of a user - defined exponent . However , scale - free networks can be recognised to the naked eye , so we dismiss this analysis . For a motif analysis of different generators of scale - free networks , we refer the reader to [ 17 ] . Moreover in [ 17 , 18 , 19 ] , the authors experimentally prove that scale - free networks display a wider spectra of properties that the Barabasi - Albert preferential attachment mechanism fails to reproduce . Recent ev - idence propose that the node degree distribution of scale - free networks is heavy - tailed and not always power - law . Acknowledgements Part of this work has been done during the master on Web Science at the School of Sciences , De - partment of Mathematics of Aristotle University of Thessaloniki in Greece ( 2014 - 2016 , see thesis [ 20 ] ) . Thanks to Dimitris Kugiumtzis for his guidance in the very early steps of this work , for providing some pieces of matlab codes and for providing the Morgan Stanley Capital Investment dataset . The work was also partially supported by the Ph . D . school of education of the Technical University of Denmark ( Department of Applied Mathematics and Computer Science , Section Software Systems Engineering ) while the writer of current manuscript was a Ph . D . student . References [ 1 ] N . T . Markov , M . Ercsey - Ravasz , D . C . Van Essen , K . Knoblauch , Z . Toroczkai , H . Kennedy , Cortical high - density counterstream architectures , Science 342 ( 6158 ) ( 2013 ) 1238406 . [ 2 ] P . Erdos , A . Renyi , On random graphs . i , Publicationes Mathematicae 6 ( 1959 ) 290 – 297 . [ 3 ] E . N . Gilbert , Random graphs , Annals of Mathematical Statistics 4 ( 1959 ) 1141 – 1144 . [ 4 ] M . Rubinov , O . Sporns , Complex network measures of brain connectivity : Uses and interpreta - tions , NeuroImage 52 ( 3 ) ( 2010 ) 1059 – 1069 , computational Models of the Brain . doi : https : / / doi . org / 10 . 1016 / j . neuroimage . 2009 . 10 . 003 . URL http : / / www . sciencedirect . com / science / article / pii / S105381190901074X [ 5 ] J . Travers , S . Milgram , An experimental study of the small world problem , Sociometry 32 ( 4 ) ( 1969 ) 425 – 443 . URL http : / / www . jstor . org / stable / 2786545 [ 6 ] D . J . Watts , S . H . Strogatz , Collective dynamics of ‘small - world’ networks , Nature ( 1998 ) 440 – 442 doi : https : / / doi . org / 10 . 1038 / 30918 . [ 7 ] S . S . Shen - Orr , R . Milo , S . Mangan , U . Alon , Network motifs in the transcriptional regulation network of escherichia coli , Nature genetics 31 ( 1 ) ( 2002 ) 64 . [ 8 ] N . Prˇzulj , D . G . Corneil , I . Jurisica , Modeling interactome : scale - free or geometric ? , Bioinformat - ics 20 ( 18 ) ( 2004 ) 3508 – 3515 . [ 9 ] N . Prˇzulj , Biological network comparison using graphlet degree distribution , Bioinformatics 23 ( 2 ) ( 2007 ) e177 – e183 . 15 [ 10 ] M . Small , J . Zhang , X . Xu , Transforming time series into complex networks , in : Complex Sciences : First International Conference , Complex 2009 , Shanghai , China , February 23 - 25 , 2009 , Revised Papers , Part 2 1 , Springer , 2009 , pp . 2078 – 2089 . [ 11 ] X . Xu , J . Zhang , M . Small , Superfamily phenomena and motifs of networks induced from time series , Proceedings of the National Academy of Sciences ( 2008 ) pnas – 0806082105 . [ 12 ] J . Zhang , M . Small , Complex network from pseudoperiodic time series : Topology versus dynamics , Phys . Rev . Lett . 96 ( 2006 ) 238701 . doi : 10 . 1103 / PhysRevLett . 96 . 238701 . URL https : / / link . aps . org / doi / 10 . 1103 / PhysRevLett . 96 . 238701 [ 13 ] C . Orsini , M . M . Dankulov , P . Colomer - de Sim´on , A . Jamakovic , P . Mahadevan , A . Vahdat , K . E . Bassler , Z . Toroczkai , M . Bogun´a , G . Caldarelli , et al . , Quantifying randomness in real networks , Nature communications 6 ( 2015 ) 8627 . [ 14 ] S . Itzkovitz , R . Milo , N . Kashtan , G . Ziv , U . Alon , Subgraphs in random networks , Physical review E 68 ( 2 ) ( 2003 ) 026127 . [ 15 ] R . Diestel , Graph theory ( 2017 ) 228 – 229 doi : https : / / doi . org / 10 . 1007 / 978 - 3 - 662 - 53622 - 3 . [ 16 ] W . - Q . Huang , X . - T . Zhuang , S . Yao , A network analysis of the chinese stock market , Physica A : Statistical Mechanics and its Applications 388 ( 14 ) ( 2009 ) 2956 – 2964 . doi : https : / / doi . org / 10 . 1016 / j . physa . 2009 . 03 . 028 . URL http : / / www . sciencedirect . com / science / article / pii / S0378437109002519 [ 17 ] L . Zhang , M . Small , K . Judd , Exactly scale - free scale - free networks , Physica A : Statistical Me - chanics and its Applications 433 ( 2015 ) 182 – 197 . [ 18 ] K . Judd , M . Small , T . Stemler , What exactly are the properties of scale - free and other networks ? , EPL ( Europhysics Letters ) 103 ( 5 ) ( 2013 ) 58004 . [ 19 ] M . Small , K . Judd , L . Zhang , How is that complex network complex ? , in : Circuits and Systems ( ISCAS ) , 2014 IEEE International Symposium on , IEEE , 2014 , pp . 1263 – 1266 . [ 20 ] G . Argyris , Motifs of Network Models ( 2017 ) . doi : 10 . 26262 / heal . auth . ir . 287148 . [ 21 ] L . Muchnik , Complex networks package for matlab , 2014 - 11 - 13 . http : www . levmuchnik . net / Con - tent / Networks / ComplexNetworksPackage ( 2013 ) . Appendices A . Empirical Construction of F W The algorithm for the empirical construction of F W incorporates two routines / functions : the CreateSmallWorldGraph routine and the ComputeRFP routine . All implementations can be found on https : / / github . com / GeorgiosArg / Diagnosis - of - Small - World - Bias - in - Random - Graphs . A . 1 . CreateSmallWorldGraph Routine The CreateSmallWorldGraph routine is a function taken from from [ 21 ] . It inputs the Small - world parameters ( n , k , p r ) of Def . 6 and generates a small - world graph according to the Watts - Strogatz model . 16 A . 2 . ComputeRFP Routine For the shake of this work , we implemented the ComputeRFP routine which inputs a network and outputs its RFP . This routine first enumerates the occurrences of each one of the motifs of Fig . 3 . The initial implementation inputs the graph as a matrix and iterates over all different tetrads ( i . e . 4 - tuples ) of nodes , i . e . each 4 × 4 submatrix of the matrix represantion of the network , and maps it to the corresponding motif of Fig . 3 - if any of these are constructed - . This is computationally expensive . Each motif of Fig . 3 can be represented by more than one adjacency matrix ; and the adjacency matrices are as many as the different patterns of each motif in Fig . 2 . To overcome this problem , smart implementations like the Brain Connectivity Toolbox [ 4 ] use dictionaries as the data structure to represent graphs . We incorporated this implementation and in order to further reduce complexity , we employed one more fact : For graphs with equal size n , and n ≤ 4 it holds that : the graphs have the same degree distribution if and only if they are isomorphic ( see Fig . 2 ) . A . 3 . The Construction Algorithm Algorithm 1 : Computation of the RFP F W ( n , k , p r ) . Result : A list containing F W for different values of n , k , p r Define network size n ; m ← 100 ; for p r from 0 to 0 . 9 with step 0 . 1 do for k from 2 to k max with step 2 do sum = ( 0 , 0 , 0 , 0 , 0 , 0 ) ; for b = 1 : m do Graph = CreateSmallWorldGraph ( n , k , p r ) F o = ComputeRFP ( Graph ) sum = sum + F o end F W ( n , k , p r ) = sum / m ; list F W = list F W . append ( F W ( n , k , p r ) ) end end The m determines how many networks we generate or , in other words , the number of RFPs that we average . When we explore networks of size n = 25 , and n = 50 , we generate 100 networks , while for size n = 75 we average over 50 networks due to the complexity of the ComputeRFP routine that we explain later . Inside the two for - loops which iterate over p r and k , we compute the RFP for that specific p r and k . p r iterates over 10 possible values , i . e . , 0 , 0 . 1 , . . . , 0 . 9 . Thereafter , the algorithm iterate over k from 2 to k max . The k max in the second iteration of this algorithm refers to the possible values that the parameter k can obtain which increases with respect to n ; the more the nodes of the circle , the more potential neighbors a node can obtain . The step is 2 because k has to be an even number since every node is connected to the two nearest neighbours ( one to the left and one to the right ) in the circular configuration of the Watts - Strogatz model . It has to hold that k max < n , i . e . , the degree of a node k max must not exceed the size of the network . Hence , k max is given by the following piecewise function : k max = (cid:40) n − 2 if mod ( n , 2 ) = 0 n − 1 if mod ( n , 2 ) = 1 So sum up , the length of the list F W ( the amount of the generated RFPs ) depends on the size ( n = | V | ) of the obseved network that we aim to classify . For example , if the observed that we want to classify has size 55 , the value of k max is 54 / 2 . Hence , the list contains 10 × 27 = 270 RFPs because p r can take 10 different values 0 , 0 . 1 , . . . , 0 . 9 . 17 B . Construction of the list of the RFPs that correspond to the ER and SW network topologies Algorithm 2 : The embedding Algorithm ; Computation of the RFPs of the network topolo - gies . Result : A list containing the RFPs of the network topologies Define network size n ; for p from 0 to 0 . 9 with step 0 . 1 do Compute F G ( n , p ) ( Section 2 . 2 , Lemma 1 ) ; list = list . append ( F G ( n , p ) ) ; end m ← 100 ; for p r from 0 to 0 . 9 with step 0 . 1 do for k from 2 to max k with step 2 do sum = ( 0 , 0 , 0 , 0 , 0 , 0 ) ; for b = 1 : m do Graph = CreateSmallWorldGraph ( n , k , p r ) F o = ComputeRFP ( Graph ) sum = sum + F o end F W ( n , k , p r ) = sum / m ; list = list . append ( F W ( n , k , p r ) ) end end For example , if n = 25 we constructed 9 RFPs for the ER graph for different values of 0 . 1 . . . 0 . 9 and number of RFPs for the WS graph for different values of p r , and k . The list in this case contains number RFPs . Similarly , for n = 50 the list contains number and for n = 75 the list contains number RFPs . The routines CreateSmallWorldGraph , and ComputeRFP are explained in the previous Appendix A , in the corresponding Sections A . 1 , A . 2 . All implementations can be found on https : / / github . com / GeorgiosArg / Diagnosis - of - Small - World - Bias - in - Random - Graphs . C . The Classification Algorithm Algorithm 3 : Guessing the generative model . Result : The generative model of a given graph . Import an observed network G = ( V , E ) ; F G = Compute RFP ( G ) ; # the routine of Section A . 2 . F W = Compute RFP W ( | V | ) ; # the empirical construction of A . F G = Compute RFP G ( | V | ) ; # Calculated according to Section 2 . 2 , Lemma 1 . if | F G − F G | < | F G − F W | then “Erdos - Renyi is the generative model” else “Watts - Strogatz is the generative model” end D . Further Analysis of the Classification Method Step 1 : Construction of the RFP for each model and for each one of their parameters so as to compute the corresponding averaged point F j : 18 ( a ) j = 1 , . . . , 9 for ER ( p ) when p = 0 . 1 , 0 . 2 , . . . , 0 . 9 ( b ) j = 10 , 11 , . . . , ⌊ n 2 ⌋ × 10 for WS when k = 2 , 4 , 6 , . . . , ⌊ n 2 ⌋ and p = 0 , 0 . 1 , 0 . 2 , . . . , 0 . 9 . Step 2 : Computing the corresponding RFP of the observed network G which is denoted as F G . Step 3 : The generative model / target function is the one that minimizes the euclidean distance between point F G and the point constructed in step 1 : minimize j ∥F G − F j ∥ In other words , we first embed the topologies into their relative frequency points ( RFPs ) which are 6 - dimensional points with each dimension corresponding to the relative frequency of a motif . The topology of an observed network is determined by the following steps : ( i ) compute the RFP of the observed network , ( ii ) compute the Euclidean distance between the RFP of the observed network and the RFPs that characterize the different network topologies , and ( iii ) the topology is the one that its RFP minimizes the Euclidean distance with the RFP of the observed network . E . Prewhitening The prewhitening removes the autocorrelations of the timeseries ¯y i which may cause spurious cross - correlations . We first subtract the mean as follows : ¯ y ′ i = ¯ y i − ¯ µ i , ∀ i ( 5 ) where ¯ µ i is an array whose entries ¯ µ i ( t ) , t ∈ { 1 , . . . , 1305 } is the mean µ i of ¯y i averaged over all t . We now fit an autoregressive ( AR ) model to the timeseries ¯y i . We selected an autoregressive model of order p = 20 by default . AR = arx ( ¯ y ′ i , p ) , ∀ i ( 6 ) The arx ( ) is a built - in function of Matlab incorporated into the System Identification Toolbox which fits an autoregressive model of order p to the time series ¯y ′ i = ¯y i − ¯ µ i . The resulting AR object contains the coefficients of the estimated model . By feeding the AR model with the timeseries ¯y ′ i , we obtain the predicted values ¯y ′′ i : ¯ y ′′ i = predict ( AR , ¯ y ′ i ) , ∀ i ( 7 ) The predict ( ) is another built - in function of Matlab incorporated into the System Identification Tool - box . AR is the autoregressive model obtained from the arx ( ) function of equation ( 5 ) , ¯y ′ i The input data used for prediction . We then add the mean that we subtracted in equation ( 4 ) . ¯ y ′′′ i = ¯ y ′′ i + ¯ µ i , ∀ i ( 8 ) Finally , we proceed with the calculation of the residuals ; the part of the original series that is not explained by the fitted model . We calculate the residuals by subtracting the predicted values from the observed values . ¯ z i = ¯ y ′′′ i − ¯ y i , ∀ i ( 9 ) The final result of the whole transformation is an array of residuals ¯z i . Once the residuals are ef - fectively whitened , they can be analyzed using standard statistical techniques without the complica - tion of autocorrelation . All implementations can be found on https : / / github . com / GeorgiosArg / Diagnosis - of - Small - World - Bias - in - Random - Graphs . 19