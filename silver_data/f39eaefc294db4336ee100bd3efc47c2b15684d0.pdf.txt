S CARECROW : A Framework for Scrutinizing Machine Text Yao Dou ∗† Maxwell Forbes ∗† Rik Koncel - Kedziorski † Noah A . Smith †‡ Yejin Choi †‡ † Paul G . Allen School of Computer Science & Engineering , University of Washington ‡ Allen Institute for AI { douy , mbforbes , nasmith , yejin } @ cs . washington . edu kedzior @ uw . edu Abstract Modern neural text generation systems can produce remarkably ﬂuent and grammatical texts . While earlier language models suffered from repetition and syntactic errors , the errors made by contemporary models are often se - mantic , narrative , or discourse failures . To facilitate research of these complex er - ror types , we introduce a new structured , crowdsourced error annotation schema called S CARECROW . The error categories used in S CARECROW —such as redundancy , common - sense errors , and incoherence—were identi - ﬁed by combining expert analysis with several pilot rounds of ontology - free crowd annotation to arrive at a schema which covers the error phenomena found in real machine generated text . We use S CARECROW to collect 13k anno - tations of 1 . 3k human and machine gener - ate paragraphs of English language news text , amounting to over 41k spans each labeled with its error category , severity , a natural language explanation , and antecedent span ( where rel - evant ) . We collect annotations for text gen - erated by state - of - the - art systems with vary - ing known performance levels , from GPT - 2 Small through the largest GPT - 3 . We iso - late several factors for detailed analysis , in - cluding parameter count , training data , and de - coding technique . Our results show both ex - pected and surprising differences across these settings . These ﬁndings demonstrate the value of S CARECROW annotations in the assess - ment of current and future text generation sys - tems . We release our complete annotation toolkit and dataset at https : / / yao - dou . github . io / scarecrow / . 1 Introduction It is commonly believed that GPT - 3 is a much better underlying language model for text gener - ation than GPT - 2 . Supporting evidence for this ∗ Equal contribution Off - Prompt The long - rumored Apple car might ﬁnally become a reality . Prompt ( human - authored ) According to the Financial Times , Apple ' s been talking to " a small group of contract manufacturers to explore making an electric vehicle , " which would ostensibly be an autonomous car . All this does sound like the loose ends of Apple ' s CarPlay rollout : hiring 1 , 200 engineers for the iOS team , building the CarPlay - speciﬁc testing track , developing a Lincoln Navigator , then poaching Burberry’s head of product design to lead the integration of software and hardware . WWDC 2015 We know what you ' re thinking : Another Monday ? Continuation written by GPT - 3 DaVinci The most likely meaning of “track” in this context is a driving area , which doesn’t make sense for CarPlay . Apple would develop their own car , not make a Lincoln Navigator , which already exists . Burberry’s head of product design wouldn ' t have the technical expertise needed for this particular job . While Apple CarPlay is also about cars , this isn’t actually relevant . This is a change of subject and doesn’t follow the narrative . Grammar / Usage It would be weird to hire 1 , 200 engineers during a “rollout” ( a product launch ) . Neither the speculation , nor the rollout described next , really make sense to call “loose ends . ” 1 1 2 2 4 5 3 6 7 3 4 5 6 7 Commonsense Figure 1 : After a model ( here , GPT - 3 DaVinci ) has read the prompt ( top sentence ) and generated a continuation ( next paragraph ) , the S CARECROW annotation frame - work provides a systematic way for humans to mark issues throughout the text and explain what is wrong . Our own annotations are pictured here . position—though strong—is largely anecdotal , or with reference to related tasks such as reading com - prehension or narrative cloze ( Brown et al . , 2020 ) . But just how good is the text generated by GPT - 3 ? What kind of errors does this model make ? And how does its error distribution compare with earlier language models or human authored text ? To answer these questions , we develop S CARE - CROW , a methodology for eliciting categorical 1 a r X i v : 2107 . 01294v1 [ c s . C L ] 2 J u l 2021 ERROR TYPE DEFINITION EXAMPLE Language Errors Grammar and Usage Missing , extra , incorrect , or out of order words . . . explaining how cats feel emoticons . . . Off - Prompt Generation is unrelated to or contradicts prompt PROMPT : Dogs are the new kids . GENERA - TION : Visiting the dentist can be scary Redundant Lexical , semantic , or execessive topical repe - tition Merchants worry about poor service or service that is bad . . . Self - Contradiction Generation contradicts itself Amtrak plans to lay off many employees , though it has no plans cut employee hours . Incoherent Confusing , but not any error type above Mary gave her kids cheese toast but drew a map of it on her toast . Factual Errors Bad Math Math or conversion mistakes . . . it costs over £1 , 000 ( $ 18 , 868 ) . . . Encyclopedic Facts that annotator knows are wrong Japanese Prime Minister Justin Trudeau said Monday . . . Commonsense Violates basic understanding of the world The dress was made at the spa . Reader Issues Needs Google Search needed to verify claim Jose Celana , an artist based in Pensacola , FL , . . . Technical Jargon Text requires expertise to understand . . . an 800 - megawatt photovoltaic plant was built . . . Table 1 : Error types in the S CARECROW framework , grouped into three categories . The categories are explained further in § 4 . 4 , and detailed deﬁnitions and examples for each error type is provided in Appendix A . judgements of errors in machine generated text from crowd workers . Since the goal of natural language generation ( NLG ) is to produce ﬂuent outputs which can be read by laypeople , we pro - pose that the most important errors to address are those which are recognized by readers without NLP expertise . Our framework allows crowd workers to annotate problems in model outputs at the span level . A single such annotation is shown in Fig - ure 1 . To make this possible , we establish a categoriza - tion of shortcomings commonly found in machine generated text ( Table 1 ) . This error schema covers a broad scope of problems as identiﬁed by experts , but has been honed according to what is salient to non - expert readers through several pilot rounds of ontology - free crowd annotation . The result is a framework that is usable by everyday people with minimal training , but covers the error phenomena found in real machine generated text . Labeling spans of text using speciﬁc error types creates a picture of contemporary model generations with an unprecedented level of detail . In contrast to judging text holistically ( Celikyilmaz et al . , 2021 ) , insights from this method are speciﬁc and actionable , as it measures exactly how and where problems arise . We conduct a large - scale analysis of human and machine generated text using S CARECROW , col - lecting 13k annotations of 1 . 3k paragraphs , amass - ing 41k spans labeled with error type , severity , and an explanation . Through this , we characterize in which ways GPT - 3’s generations are better than those of previous models , and which aspects do not improve with increased data and parameters . We also provide a rigorous error analysis of text generated by several other contemporary language models , examining the impact of model size , train - ing data , and decoding strategy . We provide our detailed annotator training sys - tem and task interface so that future researchers may employ and reﬁne them for error analyses of machine generated text . We hope this will con - tribute to the standardization of NLG human evalu - ation ( Howcroft et al . , 2020 ) . We begin with key ﬁndings about popular lan - guage models ( § 2 ) , then proceed to describe our setting and motivation ( § 3 ) , annotation ( § 4 , § 5 ) , and detailed results ( § 6 , § 7 ) . The Appendices pro - vide more comprehensive coverage of the error schema ( A ) , crowdsourcing ( B ) , annotator agree - ment and data quality ( C ) , further analysis ( D ) , and potential future directions ( E ) . 2 G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 000 0 . 002 0 . 004 0 . 006 0 . 008 S pan c o v e r age Encyclopedic G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 S pan c o v e r age Incoherent G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 002 0 . 004 0 . 006 0 . 008 S pan c o v e r age Bad Math G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 005 0 . 010 0 . 015 0 . 020 S pan c o v e r age Self - Contradiction G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 08 0 . 10 0 . 12 0 . 14 S pan c o v e r age Needs Google G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 000 0 . 005 0 . 010 0 . 015 0 . 020 S pan c o v e r age Commonsense G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 S pan c o v e r age Off - Prompt G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 010 0 . 015 0 . 020 0 . 025 0 . 030 0 . 035 S pan c o v e r age Grammar / Usage G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 005 0 . 010 0 . 015 0 . 020 S pan c o v e r age Redundant G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m a n 0 . 0075 0 . 0100 0 . 0125 0 . 0150 0 . 0175 S pan c o v e r age Technical Jargon Figure 2 : Average portion of tokens annotated with each span type ( y - axis ) across models ( x - axis ) , with 95 % conﬁdence intervals . 1 We group the trends into several broad categories . Decreasing : ﬁne - tuning and increasing model size improves performance . Model plateau : increasing model size to GPT - 3 does not correlate with further improvements . Rising and falling : errors become more prevalent with some models , then improve . Humans highest : these spans are labeled most on human - authored text ; both are reader issues ( distinct from errors ; see Table 1 ) . Several of these trends are affected by decoding settings ( e . g . , Figure 4 ) and choice of measurement ( e . g . , Figure 8 ) , which we discuss in § 6 . Details : all models , including GPT - 3 , use the same “apples - to - apples” decoding hyperparameters : top - p = 0 . 96 , temperature = 1 , and no frequency penalty . 2 Key Findings We perform a large - scale annotation of errors in English news text generated by ﬁve sources ( four models and ground truth articles ) . We present Fig - ures 2 , 3 , and 4 as summaries of our main results . As a reminder to readers , Grover ( Zellers et al . , 2019 ) is the same model size and architecture as GPT - 2 XL ( Radford et al . , 2019 ) , but trained in - domain ( on news text ) . As such , our results cover three increasing model sizes ( GPT - 2 Small , XL , and GPT - 3 ( Brown et al . , 2020 ) ) , one change in domain ( Grover ) , and ground - truth text ( Human ) . For GPT - 3 , we also study a variety of decoding conﬁgurations ( Figure 4 ) . The main quantity we measure ( on y - axes ) is span coverage , which is the average portion of tokens that ends up covered by annotations of a par - ticular span type . ( Spans can fully nest and overlap , so there is no upper bound . ) Figure 2 measures span coverage for each type of span separately , Fig - ure 3 stacks them , and Figure 4 removes non - error spans ( reader issues ) before adding them ( as in Fig - ure 3 , but without showing the individual types ) . The following are our key ﬁndings . 1 We acknowledge trendlines imply a spurious connection between models ( and humans ) ; we use them here instead of bar plots as a visual aid . 1 . Scaling pays off to improve Encyclopedic , Commonsense , and Incoherent errors ( Fig . 2 ) . These error categories decrease with in - domain training ( Grover ) and larger model size ( GPT - 3 ) . Human text still shows the fewest of these kinds of errors . 2 . Scaling beneﬁts plateau for Off - Prompt , Bad Math , and Grammar and Usage errors ( Fig . 2 ) . These three error categories see a model plateau in error reduction when scaling to GPT - 3 . Of these error types , humans still commit fewer Off - Prompt ( more : § 6 . 1 ) and Grammar and Usage errors , but Bad Math ap - pears saturated for our domain . 3 . Self - Contradiction and Redundant errors exhibit more complex scaling behavior ( Fig . 2 ) . We roughly categorize these trends as rising and falling : increasing for medium or large - scale models , but dropping for human - authored text . Further analysis ( § 6 . 2 , § 6 . 3 ) reveals these more complex patterns are affected both by interactions with other error types , as well how errors are counted . 4 . Human - authored text produces the most reader issues ( Figs . 2 and 3 ) . The Needs 3 GPT - 2 S GPT - 2 XL Grover - Mega GPT - 3 Human 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 S pan c o v e r age Average Span Coverage Across Models Bad _ Math Commonsense Encyclopedic Grammar _ Usage Incoherent Needs _ Google Off - prompt Redundant Self - contradiction Technical _ Jargon Figure 3 : Average portion of tokens covered by span annotations , broken down by span type . All models , including GPT - 3 , use the same apples - to - apples decod - ing hyperparameters : top - p = 0 . 96 , temperature = 1 , and no frequency penalty . We scale each span by its to - ken length , normalize by generation token lengths , and remove severity - 1 Grammar and Usage errors ( see § C ) . Google and Technical Jargon span categories both have a humans highest trend , and both fall under reader issues : problems that are not necessar - ily errors , but that still prevent full comprehension or factual veriﬁcation of the text ( more : § 6 . 4 ) . Furthermore , human - authored text is not free from error annotations ( Figure 3 ) . This can serve either as a control for baseline error rates ( more : § 6 . 6 ) , or as a mechanism for critiquing human writing . 5 . Decoding hyperparameters have a huge im - pact ( Figure 4 ) . For the previous ﬁndings , we ﬁx the sampling conﬁguration for all models to an apples - to - apples setup for fair comparison : top - p = 0 . 96 , ( softmax ) temperature = 1 , and no frequency penalty ( i . e . , word repetition penalty ; deﬁned pre - cisely in § 5 . 2 , Equation 1 ) . To study the effects of these decoding settings , we annotate text generated by GPT - 3 using a variety of values for top - p and temperature , both with and without a frequency penalty . To our surprise , the decoding hyperparameters considerably affected error rates ( more : § 6 . 5 ) . As seen in Figure 4 , the worst sampling procedure for GPT - 3 ( argmax sampling with no frequency penalty ) performed even worse than GPT - 2 XL . But the best sampling procedure ( surprisingly , also G P T - 2 S a r g m a x t = 0 . 4 , p = 0 . 96 G P T - 2 X L t = 1 . 0 , p = 0 . 4 t = 0 . 7 , p = 0 . 96 G r o v e r - M ega t = 1 . 0 , p = 0 . 9 t = 1 . 0 , p = 0 . 96 t = 1 . 0 , p = 0 . 9 t = 1 . 0 , p = 0 . 7 t = 1 . 0 , p = 0 . 7 t = 1 . 0 , p = 0 . 96 t = 1 . 0 , p = 0 . 4 t = 0 . 4 , p = 0 . 96 t = 0 . 7 , p = 0 . 96 a r g m a x H u m an Model ( both green hues : GPT - 3 w / decoding config in legend ) 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 S pan c o v e r age " Apples - to - apples " decoding setup ( used when comparing models elsewhere ) Errors Across All Decoding Configurations GPT - 3 , with Frequency Penalty : 0 ( none ) 1 ( full ) Figure 4 : Taking the average span coverage ( Figure 3 ) and removing reader issues ( Technical Jargon and Needs Google ) , we plot values and 95 % conﬁdence intervals for all models , including all decoding hyper - parameters we tested for GPT - 3 . We ﬁnd a surprisingly large change in annotated errors depending on the de - coding setting used . argmax sampling , but with a frequency penalty ) produced text with as few apparent S CARECROW error spans as those authored by humans ( more : § 6 . 6 ) . All of these ﬁndings are discussed in more detail in § 6 . In the intervening sections , we describe the scope and details of our annotation framework , as well as the data we collected . 3 Evaluation of Natural Language Generation We make our study in the area of open - ended natu - ral language generation , a loose term for generating longer texts with an increased level of creative free - dom . Story , blog , and dialog generation are exam - ples of open - ended generation tasks . The common factor in all open - ended generation tasks is the wide and diverse nature of target outputs . Lexically and even semantically dissimilar responses to the same prompt could be equally valid . For example , a model prompted with the blog title “Recipes for success this Holiday season” could describe how to roast a turkey or strategies for dealing with the stresses of holiday travel . This allowable variation poses a particular dif - ﬁculty for the evaluation of generation systems . Traditionally , text generation quality for tasks like machine translation or graph - to - text generation has been measured by word overlap with human - authored references ( Papineni et al . , 2002 ; Lin , 4 2004 ) . Though measures like BLEU allow for mul - tiple references , they break down when the space of allowable outputs is large , as in open - ended gener - ation . Recently introduced metrics seek to remedy this problem ( Hashimoto et al . , 2019 ; Pillutla et al . , 2021 ) , but the gold standard for evaluating gener - ated text is still human judgment . However , current approaches to eliciting human judgement of generated text often do not provide detailed insight into where models are making progress , where they are failing , and the scope of these failures . A / B - style testing allows for di - rectly comparing one system against others ( Clark and Smith , 2021 ) , but can only express relative im - provements . Simple Likert scale judgements can assess text quality , but do not explain why a gener - ated text receives a given rating , or which segment of the text is problematic . Insights into model fail - ures often come instead from a small scale expert analysis of outputs . However , these “error analy - ses , ” once a staple of NLP research , have become less common in recent years , perhaps due to their small size and high variance . A hypothesis of the current work is that a well de - signed error analysis annotation framework could be used by crowdworkers to annotate large amounts of text , thereby providing detailed information about model progress and failures as well as ac - tionable directions for future research . Such a framework would be easy to learn , reusable , and independent of particular models or experimental conditions . In what follows , we outline the details of such a method . 4 S CARECROW Annotation Methodology This section describes the high - level annotation methodology for S CARECROW . 4 . 1 Prompt and Generation Our annotations consider two segments of text : a one - sentence prompt , and a one - paragraph gener - ation . The prompt is human - written . It provides both starting tokens for model generation , as well as context for humans to evaluate whether a model is able to stay on - prompt—both topically and fac - tually . Annotators know that the prompt is written by a human . The generation is either text sampled from a language model , or the human - authored continua - tion to the prompt . Annotators , who do not know whether the generation came from a model or hu - Inconsistent about how many moons Mars has . 1 2 3 Self - Contradiction Inconsistent about how many moons Mars has . NeedsGoogle Bad Math Reader Issues Factual Language Figure 5 : S CARECROW interface for annotating a sin - gle span : ( 1 ) highlighting a span ( and later , an an - tecedent ) ; ( 2 ) completing the annotation , with the span type , explanation , and severity ; ( 3 ) the error annotation is saved—interactive controls allow detailed viewing and editing of spans ( not shown ) . mans , assess this text . A paragraph length is chosen to balance expressiveness with scope . For expres - siveness , models must be given a sufﬁcient number of tokens to express their capabilities lexically , syn - tactically , and semantically . One paragraph allows for signiﬁcantly more variation than a single sen - tence . On the other hand , assessing multiple para - graphs is challenging , both as a crowdsourcing task itself , and because it broadens the kinds of errors to include larger narrative scope . We leave exten - sions of S CARECROW to longer narrative lengths for future work . 4 . 2 Span Labeling Annotators select spans that contain problems in the generation . The spans are automatically snapped to word boundaries . We choose spans 5 to balance speciﬁcity ( i . e . , vs . simply comment - ing on the text as a whole ) with ease of use ( vs . imposing a more structured annotation schema ) . 4 . 3 Span Selection We instruct workers to select the smallest span— minimally a single word—that contains an issue . Sometimes this involves an entire phrase , sentence , or multiple sentences . We aim for speciﬁcity be - cause during aggregation , it is possible to “back off” annotations to larger spans , but not the inverse . Once they select a span , workers ( 1 ) label the er - ror type , ( 2 ) choose a severity level , and ( 3 ) explain their reasoning behind the error . Workers use the annotation interface shown in Figure 5 to mark a span with these three steps . We describe each step in greater detail in the next three sections . 4 . 4 Error Types Each selected span is labeled with exactly one error type . Multiple errors may be marked with partially or fully overlapping spans in the case that one text segment contains multiple problems . We chose ten error types to balance three crite - ria : linguistic analysis , observed errors in gener - ated text , and capabilities of everyday people with one to two hours of training . 2 We developed the schema by starting with the ﬁrst two criteria ( lin - guistic analysis and observed errors ) , and reﬁning it over several pilot annotation studies , with 30 crowd workers performing 750 total annotations of 60 paragraphs before beginning data collection . We broadly group the errors into three categories : language errors , factual errors , and reader issues . Language errors are issues with internal and exter - nal structure of of text : which ideas are expressed , and whether they are expressed coherently and con - sistently . Factual errors denote that the information presented is known to be incorrect . Reader issues , on the other hand , are cases where the text is too technical or obscure to assess its factuality . Hence , reader issues are not errors , per se , but regions where a reader would need assistance outside of the text itself for comprehension . We present the ten error types in Table 1 ( several pages back ) . Appendix A provides more details , examples , and explanations for all error types . 2 The complete training material is available for download . Sports Entertainment Business Politics Tech Health Style Science Travel Art Crime Food ( others ) Prompt Topics Used NeedsGoogle Grammar / Usage Redundant Off - Prompt Technical Jargon Incoherent Self - Contradiction CommonsenseEncyclopedic Error Types Labeled Bad Math Figure 6 : Visual overviews of the distribution of prompt topics used for generating the 1 . 3k paragraphs used in the annotation ( left ) , and the types of the 41k spans labeled during the annotation ( right ) . 4 . 5 Severity Errors naturally vary in how jarring they are to a reader . We deﬁne three error severity levels , and ask annotators to pick one for each error . The severity levels are as follows . ( 1 ) Almost no impact on quality ; just a small problem . ( 2 ) Understandable , but difﬁcult ; what’s written is still comprehensible , but there’s clearly an issue . ( 3 ) Very difﬁcult to understand ; the error almost com - pletely ruins the text . We provide examples of each severity in Ap - pendix B . 1 . 4 . 6 Explanation Finally , we ask annotators to explain their reason - ing behind each error in natural language . We pro - vide example explanations during training , but do not impose strict guidelines . This paper primarily focuses on quantitative error analysis , but we an - ticipate the error explanations may warrant future investigation . 4 . 7 Annotation Process Because the S CARECROW annotation relies entirely on human workers , the training , annotator selection and feedback , interface , number of annotators per instance , and any aggregation may be customized to serve a speciﬁc research analysis . We defer a discussion of our particular choices for this paper’s data collection to Section B . 2 . 5 Data Collection We collect 13k human annotations of 1 . 3k para - graphs using S CARECROW , resulting in over 41k spans . 6 MODEL top - p t F . P . GENS ANNS SPANS GPT - 2 S 0 . 96 1 . 00 0 81 809 3694 GPT - 2 XL 0 . 96 1 . 00 0 81 806 3087 GROVER - MEGA 0 . 96 1 . 00 0 80 796 3006 GPT - 3 0 . 40 1 . 00 0 66 660 2064 0 . 70 1 . 00 0 65 648 1841 0 . 90 1 . 00 0 63 629 1794 n / a argmax 0 66 659 2153 0 . 96 0 . 40 0 65 650 2249 0 . 96 0 . 70 0 61 610 1865 0 . 96 1 . 00 0 206 2055 6234 0 . 40 1 . 00 1 50 500 1280 0 . 70 1 . 00 1 53 530 1481 0 . 90 1 . 00 1 54 540 1717 n / a argmax 1 51 509 1384 0 . 96 0 . 40 1 53 530 1401 0 . 96 0 . 70 1 50 498 1369 0 . 96 1 . 00 1 84 838 2947 HUMAN 79 789 2296 TOTAL 1308 13056 41862 Table 2 : Statistics of data annotated with S CARE - CROW . t is the ( softmax ) temperature , and F . P . is a fre - quency penalty for already - generated words ( explained in § 5 . 2 ) . GENS , ANNS , and SPANS are then number of generations , annotations over those generations , and error spans marked during the annotations , respectively . We perform the most annotations on the strongest avail - able generative model ( GPT - 3 ) . 5 . 1 Models We consider four model conﬁgurations to test re - cent state - of - the - art transformer - based ( Vaswani et al . , 2017 ) models . GPT - 2 Small ( Radford et al . , 2019 ) The 117M parameter variant of GPT - 2 , which is pretrained on WebText , without additional ﬁne - tuning . GPT - 2 XL ( Radford et al . , 2019 ) The 1 . 5B pa - rameter variant of GPT - 2 , ( WebText , no ﬁne - tuning ) . Grover - Mega ( Zellers et al . , 2019 ) The 1 . 5B pa - rameter variant of Grover , a model with the same architecture and parameter count of GPT - 2 , trained on news articles and their metadata . GPT - 3 DaVinci ( Brown et al . , 2020 ) The 175B parameter variant of GPT - 3 , which is trained on a version of the Common Crawl web scrape with additional ﬁltering and deduplicating . These model choices allow us to study several factors in isolation , such as model size ( GPT - 2 S vs . XL ) and training data ( GPT - 2 XL vs . Grover ) . In addition , we also use the actual human - written text from the data sources we draw from , which we Off - Prompt Inco - herent Redun - dant NeedsGoogle Com . - sense BadMath Encyclo - pedic Self - Contra . Grmr . / UsageTechnicalJargon 0 5 10 15 20 25 30 T o k en s Average Span Lengths Figure 7 : Average number of tokens covered by each annotated span . We observe span length correlates with how abstract the error category is , from word - level is - sues ( Technical Jargon ) , through phrase - level seman - tics ( e . g . , Commonsense ) , and into problems of prag - matics ( Off - Prompt ) . denote as Human . 5 . 2 Decoding strategies We consider three main hyperparameters when sam - pling from models : p for top - p or nucleus sampling ( Holtzman et al . , 2020 ) , an alternative to top - k ; 3 t for the softmax temperature ; and f . p . for frequency penalty . The frequency penalty scales a token’s likelihood based on how many times it was already generated by applying the following modiﬁcation to the model’s output : (cid:96) i ( t ) ← (cid:96) i ( t ) − c < i ( t ) · α f ( 1 ) where (cid:96) i ( t ) is the model’s output for token t at the i - th position , 4 c < i ( t ) is the count of token t ’s sam - pled occurrences prior to the i - th position , and α f is the frequency penalty . We omit studying presence penalty , another hyperparameter offered for GPT - 3 , simply due to annotation budget constraints . To compare models as consistently as possible , we set identical decoding strategies for our primary data collection . We refer to this as the “apples - to - apples” decoding setup throughout the paper : p = 0 . 96 t = 1 . 0 f . p . = 0 However , we also wish to study the effects of these decoding strategies . We annotate generations 3 We omit separate studies of top - k , due to results presented by Holtzman et al . ( 2020 ) , and OpenAI’s removal of top - k from the GPT - 3 API . 4 While (cid:96) i ( t ) is deﬁned to be “logits ( un - normalized log - probabilities ) , ” because it is un - normalized , we anticipate that it is simply the model’s output before the log ( softmax ( · ) ) is applied . See OpenAI’s description of frequency and pres - ence penalties : https : / / beta . openai . com / docs / api - reference / parameter - details 7 from the strongest available model ( currently , GPT - 3 ) varying the following parameters : p ∈ { 0 . 4 , 0 . 7 , 0 . 9 , 0 . 96 } t ∈ { 0 . 0 ( argmax ) , 0 . 4 , 0 . 7 , 1 . 0 } f . p . ∈ { 0 ( none ) , 1 ( full ) } For budget reasons , we only vary p and t independently—i . e . , we set p = 0 . 96 when varying t , and t = 1 . 0 when varying p . 5 . 3 Prompt Selection We use news articles as the sources of prompts for models to condition on for generation . Speciﬁcally , we use news articles found in the Common Crawl . We select the ﬁrst sentence as the prompt . Our use of news text is constrained by two fac - tors . First GPT - 3 is trained on the Common Crawl , from 2016 through 2019 . We wish to avoid testing GPT - 3 by generating from articles it saw during training , due to the possibility of copying ( Carlini et al . , 2020 ) . Second , news articles began heav - ily covering the COVID - 19 pandemic beginning around February 2020 . Though testing models’ ca - pabilities to generate text about unseen events is a valuable line of study , the distribution shift caused by COVID - 19 in news writing about all aspects of life is difﬁcult to overstate . As such , to make the comparison more amenable to models’ training data , we consider news articles from January 2020 . We select articles where there is a known topic—such as Food or Sports —from the Common Crawl metadata , to allow for studying any effect of coarse - grained subject . 5 . 4 Generation We generate between 80 and 145 tokens 5 from each model as a continuation to the ﬁrst sentence of the news article . We stop generating when we heuristically detect the ﬁrst sentence boundary after 80 tokens . If the model does not end a sentence between 80 and 145 tokens , we sample again . For the Human setting , we use the remainder of the article , similarly stopping after the ﬁrst sentence boundary after 80 tokens . 5 . 5 Annotation Crowdsourcing Workers ﬁrst complete training and qualiﬁcation tasks . We provide more details in 5 Counted by Stanza tokenization ( Qi et al . , 2020 ) , not byte - pair encoding ( BPE ) or whitespace - separated tokens . Appendix B . 2 . From pilot studies , we discovered that each error , depending on its severity and clarity , has a < 100 % chance of being identiﬁed by each worker . In other words , a human annotator can be seen as a high - precision , moderate - recall stochastic process for labeling text problems . To account for this , we have 10 workers annotate each paragraph . Dataset statistics We list the data collection quantities in Table 2 , and plot visualizations of three aspects : prompt topic and annotated span pro - portions are shown in Figure 6 , and average span lengths are shown in Figure 7 . 6 Detailed Analysis In this section we perform a more detailed analysis of the trends of individual error types and decoding conﬁgurations . To begin , we consider apples - to - apples model de - coding conﬁgurations . To expand on these results , originally presented in Figure 2 , we also present two additional ways of counting error spans , which we show in Figure 8 . While our method for count - ing errors throughout the paper takes into account the number of tokens covered in each span ( span coverage ) , we also show plots for scaling each span by its severity level ( span coverage × severity ) , and by ignoring both severity and token length ( simply span counts ) . These changes in measurement fur - ther illuminate model error characters , which we discuss in the upcoming sections ( refer to Figure 8 ) . 6 . 1 Off - Prompt Under initial analysis of span coverage , Off - Prompt errors show a model plateau at GPT - 3 . Measuring span counts offers barely perceptible im - provement , indicating that scaling language models over more in - domain training does not guarantee topicality . This observation is consistent with growing work on prompt programming as a new technique for attempting to steer large pretrained models to com - plete the desired task ( Branwen , 2020 ; Gao et al . , 2020 ; Reynolds and McDonell , 2021 ) . In practice , we observe that while GPT - 3 will sometimes con - tinue a prompt by writing an article , other times , it may elaborate on the prompt itself : PROMPT Do you prefer the idea of being outdoors in the fresh air to being stuck inside with phones ringing and messages pinging ? 8 0 . 0025 0 . 0050 0 . 0075 S pan c o v e r age Bad Math 0 . 00 0 . 01 0 . 02 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 0002 0 . 0004 0 . 0006 S pan c oun t s 0 . 00 0 . 01 0 . 02 S pan c o v e r age Commonsense 0 . 00 0 . 02 0 . 04 0 . 06 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 001 0 . 002 S pan c oun t s 0 . 0000 0 . 0025 0 . 0050 0 . 0075 S pan c o v e r age Encyclopedic 0 . 000 0 . 005 0 . 010 0 . 015 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 0000 0 . 0005 0 . 0010 S pan c oun t s 0 . 01 0 . 02 0 . 03 S pan c o v e r age Grammar / Usage 0 . 02 0 . 04 0 . 06 0 . 08 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 002 0 . 003 0 . 004 0 . 005 S pan c oun t s 0 . 0 0 . 1 0 . 2 S pan c o v e r age Incoherent 0 . 0 0 . 2 0 . 4 0 . 6 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 000 0 . 005 0 . 010 S pan c oun t s 0 . 075 0 . 100 0 . 125 0 . 150 S pan c o v e r age Needs Google 0 . 15 0 . 20 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 008 0 . 010 0 . 012 0 . 014 S pan c oun t s 0 . 1 0 . 2 0 . 3 S pan c o v e r age Off - Prompt 0 . 25 0 . 50 0 . 75 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 002 0 . 004 0 . 006 0 . 008 S pan c oun t s 0 . 005 0 . 010 0 . 015 0 . 020 S pan c o v e r age Redundant 0 . 01 0 . 02 0 . 03 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 001 0 . 002 0 . 003 S pan c oun t s 0 . 005 0 . 010 0 . 015 0 . 020 S pan c o v e r age Self - Contradiction 0 . 02 0 . 04 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 001 0 . 002 S pan c oun t s 0 . 010 0 . 015 S pan c o v e r age Technical Jargon 0 . 02 0 . 03 0 . 04 S pan c o v . x s e v e r i t y G P T - 2 S G P T - 2 X L G r o v e r G P T - 3 H u m an 0 . 002 0 . 003 S pan c oun t s Figure 8 : Comparison of three different ways of measuring quantities of error span annotations , shown per label . ( The top plot for each span type is identicial to the one shown in Figure 2 . ) The top method ( span coverage ) is used in the rest of the paper ; we provide the comparisons here to illustrate how this decision affects analysis . Top subplots : span coverage , where the number of tokens annotated as the error span are divided by the length of each annotation . ( Annotations with no spans count as 0 . ) Intuitively , this measures the expected portion of tokens that will be covered by an error span . Middle subplots : span coverage × severity , like the top measure , but each span’s token count is multiplied by its severity , more harshly penalizing errors intuitively marked as worse . Bottom subplots : span counts , where each error span simply counts as 1 , regardless of the span length . In all cases , model conﬁgurations are set as closely as possible ( top - p = 0 . 96 , t = 1 . 0 , no frequency penalty ) , severity - 1 grammar errors are removed ( see § C ) , and 95 % conﬁdence intervals are shown as bands . Takeaways : Compared to the approach used in the rest of the paper ( span coverage ; top ) , scaling by severity ( middle ) does not affect the relative model ordering , primarily widening conﬁdence intervals . However , ignoring span lengths ( bottom ) does affect the results in several cases . Grammar and Usage and Encyclopedic develop clearer decreasing shapes , previously suffering from various levels of model plateau at GPT - 3 . Furthermore , the relative model ordering is changed for Redundant , Self - Contradiction , and Technical Jargon spans . GPT - 3 Can you leave work at work ? Are you ﬂexible enough to cover holidays or take on additional responsibilities ? Can you prioritize tasks ? If your boss comes to you on Tuesday to conﬁrm the new social media strategy , are you able to pick up the ball and get the messaging ham - mered out by Thursday ? . . . Of course , this generation is not literally Off - Prompt , but it is out of place when other genera - tions are continuations of the prompt , rather than further elaborations of it . While avoiding Off - Prompt issues for lan - guage models is worth exploring with prompt pro - gramming and other avenues , an investigation of 9 these techniques is outside the scope of this work . Finally , we note that Off - Prompt spans are the most prevalent error ( not reader issue ) marked for human - authored text . We suggest that a higher rate of false positives for this error type , coupled with its prevalence in model - generated text , makes further reﬁnement of this error a compelling avenue for further study . 6 . 2 Self - Contradiction While changing from span coverage to span counts alters the relative order of GPT - 2 XL and Grover ( though still within conﬁdence bounds ) , the puz - zling question is why GPT - 2 Small performs better than most ( or all ) other models . Why would the smallest model produce the fewest Self - Contra - diction errors ? We posit the reason is that GPT - 2 generations are so Incoherent and Off - Prompt that there is little opportunity for relevant , comprehensible points to be made and then reversed . For example , see the GPT - 2 Small annotated generation in the top left of Figure 13 . The entire text is covered by Off - Prompt and Incoherent errors . 6 If we look at GPT - 2 Small’s error distribution in Figure 3 , we see most of its added density comes from signiﬁcantly more Off - Prompt and Incoherent tokens . 6 . 3 Redundant The different counting methods shown in Figure 8 reveal a change in the results for Redundant er - rors . Rather than repetition simply increasing as models grow larger , we observe that GPT - 3 repeats in a similar number of cases ( lower span counts ) , but for more tokens ( higher span coverage ) . This matches the qualitative observation that GPT - 3 pro - duces larger topically repetitive blocks , rather than simple word or phrase repetitions generated by GPT - 2 - sized models : GPT - 2 Small . . . owners have started growing their own breeds and dogs are starting to start so there’s really . . . GPT - 3 The focus of your thoughts should be on the task at hand , not on your productivity . You shouldn’t be thinking about how you can be more productive . You should be thinking 6 The high double - error coverage reveals another consid - eration : to what depth ( i . e . , number of overlapping spans ) will annotators mark ? By the design of our framework , Inco - herent errors serve as a fall - back , but without it , we might imagine poor generations splatter - painted by other error types . about how you can be productive right now . . . . Such repetitions can be more difﬁcult to clearly isolate , because even slight wording changes pro - duce variations in tone and connotation . Rather than being identical semantically , we observe GPT - 3 will seem stuck on a particular topic , elaborating on and rephrasing similar ideas more times than a human writer ( hopefully ) would . 6 . 4 Reader Issues As noted in § 2 , we observe the highest number of Needs Google and Technical Jargon issues in human - authored text . Needs Google issues broadly represent any spe - ciﬁc claim that could be fact - checked . In our do - main ( news articles ) , these are primarily whether an event happened on a particular day , whether a per - son holds a role , or whether a mechanism works as described ( e . g . , chemical or technical ) . As seen in Figure 17 ( which shows GPT - 3’s span distribution ) , Needs Google issues happen roughly equally for all topics . We believe this trend is due to the news article domain , which is prone to a high density of speciﬁc information . As such , for other domains , this trend may be less prevalent , more difﬁcult to label ( e . g . , subtle claims assumed to be true in long running text ) , or both . We observe that Technical Jargon issues are inﬂuenced by topic ( Figure 17 , bottom ) , occurring signiﬁcantly more frequently in Business , Health , Science , and Technology topics than in others . This trend displays a clear topic - dependence even within a single broader domain ( news ) . These results in - dicate that both reader issues are characteristics of natural text . Of course , one might wish to measure or minimize potential reader issues for a particu - lar application—for example , claim veriﬁcation , or controlling for reading level . 6 . 5 Decoding Hyperparameters We discuss the effects of the decoding hyperpa - rameters we consider—top - p , temperature , and fre - quency penalty—on generation quality . For the sake of annotation cost , we only vary these param - eters for the strongest model available , GPT - 3 . First , we show the effect of varying top - p and temperature alone ( i . e . , with no frequency penalty ) on different span types . Figure 9 shows the effect on two salient spans : Off - Prompt and Redun - dant . ( We omit others for space . ) We observe that 10 0 . 0 0 . 4 0 . 7 1 . 0 temperature 0 . 4 0 . 7 0 . 9 0 . 96 t op - p 0 . 058 0 . 084 0 . 11 0 . 052 0 . 077 0 . 08 0 . 097 GPT - 3 span coverage : Off - prompt 0 . 06 0 . 07 0 . 08 0 . 09 0 . 10 0 . 0 0 . 4 0 . 7 1 . 0 temperature 0 . 4 0 . 7 0 . 9 0 . 96 t op - p 0 . 13 0 . 049 0 . 018 0 . 29 0 . 18 0 . 073 0 . 02 GPT - 3 span coverage : Redundant 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 Figure 9 : GPT - 3 span coverage for Off - Prompt ( left ) and Redundant ( right ) for values of top - p and tem - perature ( t = 0 is argmax ; both plots with no frequency penalty ; argmax sampling is agnostic to the top - p value , so we simply plot it in the p = 0 . 96 cell ) . Takeaway : Our annotation conﬁrms intuitive expectations of the effect of sampling on two error categories . When sam - pling from a larger pool of words ( higher p and t ) , a model is more likely to veer Off - Prompt , but less likely to produce Redundant text . annotators naturally label errors the way we would intuitively expect the model to produce them , given the hyperparamter changes . The bottom - right cor - ner of each subplot , where t = 1 and p = 0 . 96 , is the conﬁguration with the highest amount of ran - domness from sampling . As we move away from that corner—either left by lowering temperature , or up by lowering top - p —we lower the amount of randomness . We observe a positive correlation with randomness and Off - Prompt errors , and an inverse correlation with Redundant errors . In other words , sampling from a larger set of words makes the model more prone to changing topics , but less likely to repeat itself , and vice versa . After conﬁrming these intuitive measures , we turn our attention to Figure 10 , which investigates the overall error spans for GPT - 3 both without ( left ) and with ( right ) the frequency penalty . ( Note that unlike Figure 9 , both heatmaps in Figure 10 have the same color scale . ) We observe that introducing the frequency penalty lowers error rates for every value of temperature and top - p that we try . Further - more , it appears to reverse the trend seen without a frequency penalty : that sampling from a larger set of words produces fewer errors . The overall results for all decoding conﬁgura - tions were shown previously in Figure 4 . In the next section , we focus on the GPT - 3 decoding con - ﬁguration that produced the fewest number of er - rors , and compare it to human authored text . 0 . 0 0 . 4 0 . 7 1 . 0 temperature 0 . 4 0 . 7 0 . 9 0 . 96 t op - p 0 . 26 0 . 17 0 . 2 0 . 37 0 . 31 0 . 21 0 . 18 GPT - 3 Span Coverage Frequency Penalty : 0 ( none ) 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 0 . 0 0 . 4 0 . 7 1 . 0 temperature 0 . 4 0 . 7 0 . 9 0 . 96 t op - p 0 . 14 0 . 16 0 . 17 0 . 089 0 . 11 0 . 11 0 . 16 GPT - 3 Span Coverage Frequency Penalty : 1 ( full ) 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 Figure 10 : Comparison of frequency penalty off ( left ) and full ( right ) for GPT - 3 ( removing reader issues and severity - 1 Grammar and Usage errors ; argmax sam - pling is agnostic to the top - p value , so we simply plot it in the p = 0 . 96 cell ) . We observe the frequency penalty improves average span coverage for all values of top - p and temperature . Furthermore its trend is reversed : with a frequency penalty , the least diverse sampling mechanisms ( low temperature and low top - p ) now pro - duce text with the fewest error spans , rather than the most . ( See Figure 4 for conﬁdence intervals on each value . ) 6 . 6 Best GPT - 3 vs . Humans The best GPT - 3 conﬁguration shown in Figure 4—argmax sampling with frequency penalty = 1— appears to match error rates seen in human text . Is the text generated by this model truly as error - free as news articles ? We ﬁrst look at the error composition of both sets of annotations . To get a clear picture of the po - tential problems , we plot only error spans ( ignoring reader issues ) , and we omit length scaling , instead plotting span counts . This breakdown is shown in the left plot of Figure 11 . The error compositions are similar , the largest differences being more Re - dundant errors for GPT - 3 , and more Grammar and Usage errors for human - authored text . Next , we perform a manual analysis of 160 er - rors , sampling 10 at random from each of the 8 error types for each model ( GPT - 3 and human - authored text ) . We show the results in the center plot of Figure 11 . We notice that a greater portion of errors in human - authored text were due to arti - facts present in the text - only format of the Common Crawl . For example , links to other articles or ad - vertisements sometimes appear in the middle of an article’s text . While annotators were quick to mark these spans , they reﬂect errors in formatting , not in writing . We partition these errors separately and exclude them from the subsequent calculations . 7 7 GPT - 3’s generations also sometimes exhibited what ap - peared to be formatting errors due to training on web - scraped 11 GPT - 3 ( argmax , f . p . = 1 ) Human 0 . 000 0 . 001 0 . 002 0 . 003 0 . 004 0 . 005 0 . 006 0 . 007 0 . 008 S pan c oun t s Average Span Counts ( Errors Per Token ) Self - contradiction Redundant Off - prompt Incoherent Grammar _ Usage Encyclopedic Commonsense Bad _ Math 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Portion of Annotated Errors Which Were True Errors Encyclo - pedicSelf - Contradiction Incoherent Bad Math Grammar / Usage Common - sense Redundant Off - Prompt GPT - 3 ( argmax , f . p . = 1 ) Human : Including Scraping Artifacts Human : Article Fault Only Estimated True Error Counts ! " " # $ % $ & ’ ( ) * ( + # , - & , . / , 0 & / , 0 & 123 43 Figure 11 : Analysis of the best GPT - 3 conﬁguration ( argmax , freq . penalty = 1 ) vs . human - authored text . Left : A breakdown of errors by type . Center : Results of manually annotating 10 random spans from each type with whether the error was legitimate . For human - authored text , we also show errors marked on scraping artifacts that were present in the Common Crawl data . Right : Scaling each error type ( left plot , now shown in black outline ) by the portion of errors found to be legitimate ( center plot ) , we estimate the true errors counts for each model ( color - ﬁlled portions ) . Takeaway : Humans have more difﬁculty spotting errors in higher quality text ; accounting for this difference dramatically increases the gap between model - authored and human - authored text . For simplicity , all plots use error counts rather than error coverage —i . e . , they count the number of error spans , rather than scaling by the number of tokens covered . Finally , we scale each error type’s prevalence for each model ( i . e . , the left plot of Figure 11 ) by the portion of errors that we estimate to be legitimate based on our manual annotation ( i . e . , Figure 11 , center ) to produce the right plot of Figure 11 . After taking into account each error type’s frequency , we estimate that 48 % of GPT - 3’s worker - annotated errors overall are legitimate , compared to 9 % for human - written articles . This analysis suggests two ﬁndings . First , human - authored news paragraphs contain many times fewer issues than text authored by GPT - 3 us - ing the best decoding conﬁguration we tested . Sec - ond , the noise of error annotations may be as high as 90 % when assessing high - quality text . Though it would require further manual annotation to ver - ify , we conjecture that the trend of GPT - 3’s error spans being more reliable ( only 50 % noise ) would continue , and that text generated by GPT - 2 would contain even fewer false positives . We note that such rates are not ﬁxed—after all , the manual an - notations were done by one of the authors simply by reading carefully—but that more realistic text may require correspondingly more effort by human annotators . text , though more rarely . For example , some generations con - tained Which ? after vague noun phrases , which appear to be learned from Wikipedia , where under - speciﬁed information is tagged by an editor with this word . For fairness , we removed these errors from GPT - 3’s tally as well , though they were few enough we do not plot them separately . 7 Error Prediction A natural question is : using this data , can machines learn to detect and classify errors in machine gen - erated text ? Task We frame this problem as a span classiﬁ - cation task . Given a span from a generated text , the goal is to classify its error type or output “No Error” if there is none . Positive examples for each error class are taken from our data . We sample random spans that were not labeled with any error type as negative examples . To ensure a breadth of span lengths , we sample 3 negative spans for every length of error span in the generated text . We split the generated texts into train , development , and test sets using 1063 texts ( 28029 error spans ) , 100 texts ( 2538 spans ) and 100 texts ( 2677 spans ) respectively . Model We use a standard span classiﬁcation model inspired by Wadden et al . ( 2019 ) . This model encodes every generated text using a pre - trained language model ( RoBERTa - large ) . Spans are represented with the ﬁnal layer of this encod - ing . Following previous work , we concatenate the start and end tokens with a task - speciﬁc learned length embedding . The resulting vector is passed through a feedforward network which reduces its dimensionally to the number of error categories plus a “No Error” option . The resulting model has 357M trainable parameters . The model is trained to 12 minimize the cross entropy of the correct span cate - gory . We train for 15 epochs using AdamW with a learning rate of 10 − 6 . We validate after each epoch and use the checkpoint with the lowest validation loss ( epoch 8 ) . Evaluation To evaluate the error prediction model , we use per - token precision , recall , and F 1 score per error category . We classify every span up to length 30 in a generated text . We take as gold labels the aggregated human error spans collected in our data . For comparison , we also report the average metrics of one annotator versus the others . Results Table 3 shows the error prediction capa - bility of this model in terms of precision and recall . As we noted earlier , a single human annotator can be thought of as a high precision , low recall judge . These results bear out this claim . For all but one cat - egory , humans have higher precision annotations . However , the models trained on the aggregation of human labels can achieve considerably higher recall . For half of the error categories , this leads to higher model F 1 scores than the human annotators . We see that the model is quite successful at identifying information that human’s would have to manually verify ( Needs Google ) , achieving nearly perfect recall with precision close to 0 . 6 . The model can also identify Grammar and Us - age , Incoherent , and Redundant errors with higher recall than an individual human annotator , though at the cost of precision ( sometimes in the . 20s ) . These results show some promise for training error detection models on our data . Notably , a thorough search of hyperparameters such as learn - ing and negative sampling rates has not been con - ducted and could possibly improve even the basic model offered here . Architecture choices such as the underlying pretrained language model , the span representation , or the structure of the ﬁnal classiﬁ - cation module should also be explored . A different framing of the error prediction task ( i . e . , rather than exhaustive span classiﬁcation ) may also yield better performance . 8 Related Work Automated evaluation metrics such as BLEU ( Pa - pineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , ME - TEOR ( Banerjee and Lavie , 2005 ) , and BERTScore ( Zhang et al . , 2019 ) compute a generation’s score based on a true reference , or a set of references . Error Model Human P R F 1 P R F 1 Bad Math – 0 – 0 . 72 0 . 14 0 . 24 Commonsense 0 . 77 0 . 06 0 . 10 0 . 17 0 . 02 0 . 04 Encyclopedic – 0 – 0 . 22 0 . 03 0 . 05 Grammar and Usage 0 . 29 0 . 23 0 . 26 0 . 30 0 . 04 0 . 08 Incoherent 0 . 59 0 . 34 0 . 43 0 . 69 0 . 15 0 . 24 Off - Prompt 0 . 67 0 . 29 0 . 41 0 . 88 0 . 31 0 . 46 Redundant 0 . 23 0 . 82 0 . 36 0 . 88 0 . 35 0 . 50 Self - Contradiction 0 . 08 0 . 23 0 . 12 0 . 51 0 . 09 0 . 16 Technical Jargon 0 . 18 0 . 74 0 . 29 0 . 61 0 . 12 0 . 20 Needs Google 0 . 59 0 . 96 0 . 73 0 . 78 0 . 20 0 . 32 Table 3 : Model prediction results . Bold F 1 scores de - note the higher average ; values marked “ – ” cannot be computed due to division by zero . Takeaway : Hu - mans have higher precision in every error type except Commonsense , but relatively sparse annotations lead to lower computed recall . This allows the model to achieve higher F 1 scores for half of the span categories . Method GC SET DE RR EE RS SA Likert - Scale (cid:88) (cid:88) (cid:88) RankME (cid:88) (cid:88) (cid:88) RoFT (cid:88) (cid:88) (cid:88) S CARECROW (cid:88) (cid:88) (cid:88) (cid:88) Table 4 : Comparison of different natural language gen - eration human evaluations . Here , GC : General Crite - ria , SET : Speciﬁc Error Type , DE : Direct Evaluation , RR : Relative Ranking , EE : Error Explanation , RS : Rating Scale , SA : Span Annotation . Their use is well - established in tasks like machine translation and summarization , but they are less helpful in open - ended text generation , where there is a vast diversity of possible high - quality continu - ations . Recent studies propose automated metrics for open - ended text generation evaluation such as : Per - ception Score ( Gu et al . , 2020 ) , which diffuses eval - uation onto a multidimensional space and assigns a single holistic score ; UNION ( Guan and Huang , 2020 ) , which learns to distinguish human - written stories from negative samples by generating per - turbations of human written stories ; and MAUVE ( Pillutla et al . , 2021 ) , which compares the distri - bution of machine - generated text to that of human language . An alternate recent approach to assessing open - ended text generation was presented in TuringAd - vice ( Zellers et al . , 2021 ) , where crowd workers assess machine - generated advice in response to Reddit posts . In their error analysis , Zellers et al . connect problems in generated text to core NLP 13 tasks , such as Self - Contradiction errors as in - stances of failed natural language inference ( Monz and de Rijke , 2001 ) , or Off - Prompt errors as cases of failed reading comprehension ( Richardson et al . , 2013 ) . While past work has attempted to guide text generation using discriminative models trained for such tasks ( Holtzman et al . , 2018 ) , it remains an open challenge . Comparative human evaluations of natural lan - guage generations ask annotators to rank system outputs relative to each other . Text is typically eval - uated using a few global criteria , such as ﬂuency and relevance , using discrete ( e . g . , 5 - point ) ( Sai et al . , 2020 ) or continuous scales ( Novikova et al . , 2018 ) . Recent work even automates this approach , running a human evaluation alongside automatic metrics on leaderboard submissions ( Khashabi et al . , 2021 ) . In the RoFT system ( Dugan et al . , 2020 ) , annotators attempt to detect the boundary be - tween human - and machine - written text as a proxy for assessing quality . Table 4 summarizes the dif - ferences between these schemes and S CARECROW . See Celikyilmaz et al . ( 2021 ) for a recent survey of text generation evaluation techniques across both human and automatic metrics . While these approaches may be helpful— sometimes ( Card et al . , 2020 ) —at ranking systems , they do not give us insight into exactly which parts of a generation fall short , and why . One approach related to or annotation method is pursued by Wood et al . ( 2018 ) , who develop a collaborative mobile app where users draw “grafﬁti” commentary on news articles . S CARECROW aims to assess model generations the way we would critique human - written text : by locating , coarsely categorizing , and explaining problems . 9 Conclusion We present S CARECROW , a method for identifying and explaining issues in generated text . Along with the annotation framework , we present an analysis of the S CARECROW method applied to several large neural language models in an open - ended news gen - eration task . We release our data and methodology to the community . Acknowledgments The authors thank members of xlab for their feed - back on this work . This research is supported in part by NSF ( IIS - 1714566 ) , DARPA MCS pro - gram through NIWC Paciﬁc ( N66001 - 19 - 2 - 4031 ) , DARPA SemaFor program , and Allen Institute for AI . References Satanjeev Banerjee and Alon Lavie . 2005 . Meteor : An automatic metric for mt evaluation with improved correlation with human judgments . In Proceedings of the acl workshop on intrinsic and extrinsic evalu - ation measures for machine translation and / or sum - marization , pages 65 – 72 . Gwern Branwen . 2020 . Gpt - 3 creative ﬁction . Tom B . Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M . Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam Mc - Candlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learn - ers . Massimo Caccia , Lucas Caccia , William Fedus , Hugo Larochelle , Joelle Pineau , and Laurent Charlin . 2020 . Language gans falling short . Dallas Card , Peter Henderson , Urvashi Khandelwal , Robin Jia , Kyle Mahowald , and Dan Jurafsky . 2020 . With little power comes great responsibility . In Pro - ceedings of EMNLP . Nicholas Carlini , Florian Tramèr , Eric Wallace , Matthew Jagielski , Ariel Herbert - Voss , Katherine Lee , Adam Roberts , Tom Brown , Dawn Song , Úl - far Erlingsson , Alina Oprea , and Colin Raffel . 2020 . Extracting training data from large language models . arXiv preprint arXiv : 2012 . 07805 . Asli Celikyilmaz , Elizabeth Clark , and Jianfeng Gao . 2021 . Evaluation of text generation : A survey . Elizabeth Clark and Noah A . Smith . 2021 . Choose your own adventure : Paired suggestions in collabo - rative writing for evaluating story generation models . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies , pages 3566 – 3575 , Online . Association for Compu - tational Linguistics . Liam Dugan , Daphne Ippolito , Arun Kirubarajan , and Chris Callison - Burch . 2020 . Roft : A tool for eval - uating human detection of machine - generated text . arXiv preprint arXiv : 2010 . 03070 . Tianyu Gao , Adam Fisch , and Danqi Chen . 2020 . Making pre - trained language models better few - shot learners . arXiv preprint arXiv : 2012 . 15723 . 14 Herbert P Grice . 1975 . Logic and conversation . In Speech acts , pages 41 – 58 . Brill . Jing Gu , Qingyang Wu , and Zhou Yu . 2020 . Perception score , a learned metric for open - ended text genera - tion evaluation . arXiv preprint arXiv : 2008 . 03082 . Jian Guan and Minlie Huang . 2020 . Union : An unref - erenced metric for evaluating open - ended story gen - eration . arXiv preprint arXiv : 2009 . 07602 . Tatsunori Hashimoto , Hugh Zhang , and Percy Liang . 2019 . Unifying human and statistical evaluation for natural language generation . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Hu - man Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1689 – 1701 , Minneapolis , Min - nesota . Association for Computational Linguistics . Ari Holtzman , Jan Buys , Maxwell Forbes , Antoine Bosselut , David Golub , and Yejin Choi . 2018 . Learning to write with cooperative discriminators . arXiv preprint arXiv : 1805 . 06087 . Ari Holtzman , Jan Buys , Maxwell Forbes , and Yejin Choi . 2020 . The curious case of neural text degener - ation . International Conference on Learning Repre - sentations . David M . Howcroft , Anya Belz , Miruna - Adriana Clinciu , Dimitra Gkatzia , Sadid A . Hasan , Saad Mahamood , Simon Mille , Emiel van Miltenburg , Sashank Santhanam , and Verena Rieser . 2020 . Twenty years of confusion in human evaluation : NLG needs evaluation sheets and standardised def - initions . In Proceedings of the 13th International Conference on Natural Language Generation , pages 169 – 182 , Dublin , Ireland . Association for Computa - tional Linguistics . Daniel Khashabi , Gabriel Stanovsky , Jonathan Bragg , Nicholas Lourie , Jungo Kasai , Yejin Choi , Noah A . Smith , and Daniel S . Weld . 2021 . Genie : A leader - board for human - in - the - loop evaluation of text gen - eration . Klaus Krippendorff . 2018 . Content analysis : An intro - duction to its methodology . Sage publications . Chin - Yew Lin . 2004 . Rouge : A package for automatic evaluation of summaries . In Text summarization branches out , pages 74 – 81 . Hugo Liu and Push Singh . 2004 . Conceptnet—a practi - cal commonsense reasoning tool - kit . BT technology journal , 22 ( 4 ) : 211 – 226 . Yann Mathet , Antoine Widlöcher , and Jean - Philippe Métivier . 2015 . The uniﬁed and holistic method gamma ( γ ) for inter - annotator agreement mea - sure and alignment . Computational Linguistics , 41 ( 3 ) : 437 – 479 . Christof Monz and Maarten de Rijke . 2001 . Light - weight entailment checking for computational se - mantics . In Proc . of the third workshop on inference in computational semantics ( ICoS - 3 ) . Jekaterina Novikova , Ondˇrej Dušek , and Verena Rieser . 2018 . Rankme : Reliable human ratings for natural language generation . arXiv preprint arXiv : 1803 . 05928 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic eval - uation of machine translation . In Proceedings of the 40th annual meeting of the Association for Compu - tational Linguistics , pages 311 – 318 . Krishna Pillutla , Swabha Swayamdipta , Rowan Zellers , John Thickstun , Yejin Choi , and Zaid Harchaoui . 2021 . Mauve : Human - machine divergence curves for evaluating open - ended text generation . Peng Qi , Yuhao Zhang , Yuhui Zhang , Jason Bolton , and Christopher D . Manning . 2020 . Stanza : A python natural language processing toolkit for many human languages . In Proceedings of the 58th An - nual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 101 – 108 , Online . Association for Computational Linguis - tics . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 . Language models are unsupervised multitask learners . OpenAI blog , 1 ( 8 ) : 9 . Laria Reynolds and Kyle McDonell . 2021 . Prompt pro - gramming for large language models : Beyond the few - shot paradigm . In Extended Abstracts of the 2021 CHI Conference on Human Factors in Com - puting Systems , pages 1 – 7 . Matthew Richardson , Christopher JC Burges , and Erin Renshaw . 2013 . Mctest : A challenge dataset for the open - domain machine comprehension of text . In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 193 – 203 . Ananya B Sai , Akash Kumar Mohankumar , and Mitesh M Khapra . 2020 . A survey of evalua - tion metrics used for nlg systems . arXiv preprint arXiv : 2008 . 12009 . Roger C Schank and Robert P Abelson . 1977 . Scripts , plans , goals , and understanding : An inquiry into hu - man knowledge structures . Psychology Press . Hadrien Titeux and Rachid Riad . 2021 . pygamma - agreement : Gamma γ measure for inter / intra - annotator agreement in python . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . arXiv preprint arXiv : 1706 . 03762 . 15 David Wadden , Ulme Wennberg , Yi Luan , and Han - naneh Hajishirzi . 2019 . Entity , relation , and event extraction with contextualized span representations . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan - guage Processing ( EMNLP - IJCNLP ) , pages 5784 – 5789 , Hong Kong , China . Association for Computa - tional Linguistics . Gavin Wood , Kiel Long , Tom Feltwell , Scarlett Row - land , Phillip Brooker , Jamie Mahoney , John Vines , Julie Barnett , and Shaun Lawson . 2018 . Rethink - ing engagement with online news through social and visual co - annotation . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , pages 1 – 12 . Rowan Zellers , Ari Holtzman , Elizabeth Clark , Lianhui Qin , Ali Farhadi , and Yejin Choi . 2021 . TuringAd - vice : A generative and dynamic evaluation of lan - guage use . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , pages 4856 – 4880 , Online . Association for Computational Linguistics . Rowan Zellers , Ari Holtzman , Hannah Rashkin , Yonatan Bisk , Ali Farhadi , Franziska Roesner , and Yejin Choi . 2019 . Defending against neural fake news . In H . Wallach , H . Larochelle , A . Beygelz - imer , F . d ' Alché - Buc , E . Fox , and R . Garnett , editors , Advances in Neural Information Processing Systems 32 , pages 9054 – 9065 . Curran Associates , Inc . Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi . 2019 . Bertscore : Eval - uating text generation with bert . arXiv preprint arXiv : 1904 . 09675 . A S CARECROW Annotation Schema Here , we present in greater detail the S CARECROW annotation span types . 8 A visual summary is shown in Figure 12 . While we annotate using this schema , the essence of our study is to embrace language users’ abilities to detect when something may be wrong with text . In other words , we do not wish for our span deﬁnitions to get in the way of humans de - scribing problems with text . To this end , we en - courage researchers to embrace label back off ( to coarser categories ) , merging labels ( based on em - pirical observations ) , and reﬁning the annotation ontology over time . The central goal is to collect what people ﬁnd wrong with text . 8 All example annotations here are our own . Many are provided to annotators during training . READER SOMETHING IS WRONG Bad math Figure 12 : A visualization of S CARECROW spans : three categories ( reader , language , and factual ) com - posed of ten types . Annotators choose directly from the ten span types . A . 1 Language Errors We deﬁne ﬁve categories of language errors , which concern the selection of ideas in a text and how they are expressed . These range from grammar and syntax problems to issues of semantics and pragmatics . A . 1 . 1 Grammar and Usage This category of errors includes missing words , extra words , and incorrect or out of order words . EXAMPLE A PhD student from the University of Kent in the UK claims to have discovered a clever way to explain the positive emoticons in cats . Explanation : The word should probably be “emo - tions . ” We also label Grammar and Usage for in - serted words or small phrases that could be deleted to resolve the issue : A couple is facing criticism for their extravagant birthday party . The bewitching pair had ﬁrst stripped down to ﬁshnets and backward . Explanation : This phrase can simply be deleted . We avoid partitioning Grammar and Usage er - rors into more detailed categories based on the ob - servation that large language models produce fewer issues of syntax and diction ( aside from Redun - dant errors , described next ) . As such , we focus instead on semantic and pragmatic errors , captured by the upcoming span types . A . 1 . 2 Redundant While “redundant” can also include extra unnec - essary information , we speciﬁcally use the Re - dundant label to mark repetition . In identifying 16 redundant text , our schema annotates both the an - tecedent ( ﬁrst mention ) and the redundant text ( when the repetition occurs ) . Sometimes the exact word or phrase will be repeated . EXAMPLE Many merchants worry about the possibility of poor service or service for certain categories of customers . Other times , generated text expresses the same idea repeatedly using different words . EXAMPLE They then made decisions based on Kondo’s in - structions , to the extent that they created de - cluttered spaces and got rid of clutter and clutter - ﬁlled spaces . A . 1 . 3 Off - Prompt The prompt is a human - written sentence used as context from which the model generates a contin - uation . Models sometimes generate text that is unrelated to the prompt . EXAMPLE Prompt : Dogs are the new kids . Generation : Statistics suggest that most Amer - icans would be happier with dogs than children . In fact , four out of ﬁve don’t even visit the dentist annually , much less every six months . Dog owners report much higher rates of happiness than non - dog owners . Other times , the text may be related , but it con - tradicts what is stated in the prompt . EXAMPLE Prompt : China sets new record for Economic Growth Generation : The Chinese economy fell 10 % this month , the third such loss this year . A . 1 . 4 Self - Contradiction When a model generates text that contradicts the prompt , that is labeled as Off - Prompt . But when a model generates text that contradicts itself , that is labeled as Self - Contradiction . We also mark the antecedent ( original statement ) . EXAMPLE McDonald’s is considering a design which will replace the cardboard packaging . Mr Gore - Cotter said : “We recognise the concern around waste . We are now looking at a new design that minimises the plastic bag . ” Explanation : The idea of minimizing the plas - tic bag contradicts the stated goal of replacing cardboard packaging . EXAMPLE Mall of America plans to lay off and furlough hundreds of its employees . It has no plans to restrict the number of hours workers can work . Explanation : Furloughed workers are explicitly restricted from working . A . 1 . 5 Incoherent Generated text is sometimes grammatical , not re - dundant , on prompt , and not contradictory , but still confusing . We provide the Incoherent label for such sentences . EXAMPLE Melody Mitsugi , 28 , had never given her kids cheese toast before her husband drew a map of it on her toast . Explanation : One can’t exactly draw a map of Cheese Toast , and one probably wouldn’t draw it on toast itself . EXAMPLE Cats naturally show anxiety and fear by at times breaking apart different parts of the brain in an attempt to keep the others from escaping . Explanation : It’s difﬁcult to even imagine what is happening in this passage . A . 2 Factual Errors We deﬁne three categories of factual errors , which encompass known incorrect statements . A . 2 . 1 Bad Math Generated text will sometimes have issues with basic mathematical operations of known quanti - ties ( e . g . , “half of ten apples is four” ) , problems converting ﬁxed units ( e . g . , m to cm ) . EXAMPLE One account , @ Iain _ Rowling1 , had over 500 , 000 followers at one point , but in just four days they fell by around half - some 4 , 000 . We also include problems converting currencies that are wildly implausible under modern assump - tions ( e . g . , $ 1 US = £10 ) . EXAMPLE . . . compared with just over £1 , 000 ( $ 18 , 868 ) for previous versions of Samsung’s ﬂagship phone . A . 2 . 2 Commonsense These errors mark spans that violate our every - day basic understanding of the world . Though it is challenging to precisely deﬁne commonsense knowledge ( Liu and Singh , 2004 ) , we include non - encyclopedic knowledge and basic reasoning . The following example concerns broadly sensi - ble numerical ranges . EXAMPLE The picture is from high above the South Pole , where close to 100 , 000 Astronauts live and work . Explanation : Even if we don’t know the exact number of astronauts in space , it is common knowledge that 100k is far too many . 17 The next example involves world knowledge , akin to scripts ( Schank and Abelson , 1977 ) . EXAMPLE You can get the dress custom - made and stitched at your favorite spa . Explanation : Spas don’t offer stitching . The following example involves lexical entail - ment . EXAMPLE The thinness of our bodies isn’t an answer to all common human health problems like obesity or diabetes Explanation : While most of the statement is ac - ceptable , it’s impossible to be “thin” and “obese” at the same time . The ﬁnal example involves time . EXAMPLE Now in 2021 , NASA is measuring California wildﬁre temperatures using an instrument on the International Space Station . This year’s record - shattering heat has had global repercussions in 2017 , forcing sea level rise on California and increasing the risk of deadly wildﬁres . Explanation : Events in 2021 can’t affect events in 2017 . A . 2 . 3 Encyclopedic These errors are ones that we know are factu - ally wrong , and that we could look up in , say , Wikipedia . EXAMPLE Japanese Prime Minister Justin Trudeau said he will be halting all imports and exports until the current situation can be contained . Explanation : Justin Trudeau is the Prime Minis - ter of Canada , not Japan . The distinction between Encyclopedic , and the upcoming Technical Jargon and Needs Google errors , depend on the reader’s knowledge . EXAMPLE The gas contains something known as phyto - ro - matic acid , a common chemical element in the periodic table . Explanation : Acids aren’t elements . A . 3 Reader Issues We deﬁne two categories of reader issues . These are words or statements a reader cannot verify with - out using an external resource . A . 3 . 1 Technical Jargon Sometimes generated text includes speciﬁc words from a ﬁeld that requires expertise to understand . EXAMPLE In Chile , an 800 - megawatt photovoltaic plant was built for a record low cost of $ 129 per megawatt - hour last year . Which words are jargon depends on the reader’s particular expertise . This means Technical Jar - gon spans are more accurately thought of as po - tential issues rather than known errors . EXAMPLE He uses a spirit mash made from white corn and malted barley and a neutral grain , which he describes as a " whiskey grain . ” A . 3 . 2 Needs Google Many facts—especially those involving speciﬁc people , events , dates , or numbers—could be cat - egorized as encyclopedic knowledge . However , whether the fact is accurate may require additional veriﬁcation by the everyday reader . To make this distinction between known encyclopedic knowl - edge and trivia , we introduce this label to denote that a reader would need to search online to verify whether it is true . We instruct annotators to not look up facts marked with the Needs Google span . We do this to keep the focus of the task on classiﬁcation , rather than factuality detection . As a result , Needs Google spans mark statements that would need to be veriﬁed , rather than known errors . EXAMPLE It was promoted by Dr . Michael Fanning , the Executive Director of the Foundation for Men - tal Health Awareness , Inc . Explanation : A reader would likely need to look up whether there is a Dr . Fanning who holds this position . EXAMPLE . . . an 800 - megawatt photovoltaic plant was built for a record low cost of $ 129 per megawatt - hour last year . Explanation : In addition to potential Tech - nical Jargon spans , there are at least two Needs Google spans : 1 . whether such a plant can be roughly 800 - megawatt , 2 . whether $ 129 / megawatt - hour is a sensible cost measure , and the value is reasonable . To illustrate the annotation methodology and schema in practice , we present four complete ex - ample annotations in Figure 13 . This ﬁgure also illustrates how much variation we see across mod - els . 18 GPT - 2 Small GPT - 2 XL GPT - 3 DaVinci Human Off - prompt ( 3 ) : The prompt is about parents putting their children at risk of depression from ignoring them while on the smartphone . Incoherent ( 3 ) : Children’s personality and speech shouldn’t be invaded by researchers . The rest doesn’t really make any sense . Incoherent ( 3 ) : What kind of classes taking place is a mystery . The items that percentages are given for make no sense . Incoherent ( 3 ) : This doesn’t make any sense either . Half - time reading and C - section check is nonsense . Off - prompt ( 3 ) : This contradicts the prompt that says his gun and drugs were found . Self - contradiction ( 2 ) : This states that police were sent to the house following reports that someone was checking on the welfare of Coombes . It’s more likely the police were sent to do a welfare check on him . Off - prompt ( 3 ) : According to this the police didn’t ﬁnd or arrest Coombes on Thursday , but the prompt say he was arrested at the house he was staying at . Self - contradiction ( 2 ) : It says Coombes has a roommate and a house so the homeless shelter seems like a contradiction . Needs Google ( 1 ) : Is paracetamol used this way ? Needs Google ( 1 ) : Is Zylowska a psychologist there ? Needs Google ( 1 ) : Is this drug used for these conditions ? Self - contradiction ( 3 ) : I guess medicines can be used for different purposes , but these different conditions seem contradictory . Needs Google ( 1 ) : Is paracetamol a painkiller ? Self - contradiction ( 3 ) : Testing the drug on people with depression and anxiety indicates it’s used not that not as a painkiller . Needs Google ( 1 ) : Is Polar Bear Plunge what the New Years swim in La Jolla is called , and has it been going on for 30 years ? Figure 13 : Example S CARECROW annotations of three model generations and one ground truth continuation , demonstrating the shift in number , type , and severity of errors . The entirety of the GPT - 2 Small generation is Off - Prompt and / or Incoherent , with high severity ( 3 / 3 ) . GPT - 2 XL is instead only about two - thirds cov - ered by errors—still sometimes Off - Prompt , but also Self - Contradiction , and with high severity ( 2 – 3 / 3 ) . In contrast , GPT - 3 DaVinci receives several Needs Google marks—less severe than errors , as they only indicate that fact - checking is needed—though it also commits two high - severity Self - Contradiction errors by generating inconsistent claims . The Human ( ground - truth ) continuation only receives one Needs Google span . B Annotation Details B . 1 Error Severity We provide here examples for each of the three er - ror severity levels , which we also give to annotators during training . EXAMPLE Paul Campbell - Hughes , from the University of Aberdeen , explains how she managed to locate colonies of honey bees in Kent . Severity : 1 . Since Paul is usually a male name , the model should have used “he . ” But this error is pretty minor . EXAMPLE Paul Campbell - Smith , a PhD student from the University of Kent in the UK , claims to have discovered a clever way to explain the positive emoticons in cats . Severity : 2 . The word should probably be “emo - tions . ” We can guess what was being said , but it’s deﬁnitely wrong . EXAMPLE Prompt : Whether you’re on Facebook , Insta - gram , Snapchat or TikTok , many people make huge efforts to curate the best version of them - selves online . Generation : This year we’ve got something for you : a Love Match Custom Size Poster featuring Mather , Phoenix , Kashun and all her friends , divided among six different cov - 19 ers , creating a beautiful custom size poster for your own personal high school reunion . Severity : 3 . Even ignoring the end of the gen - eration ( a poster for a personal high school re - union ? ) , this whole generation is way off the prompt and does not make sense . B . 2 Crowdsourcing Details Our annotation process requires signiﬁcant train - ing and time investment during annotation . We use Amazon Mechanical Turk ( AMT ) for all data collection . Training For training , we ﬁrst pay each worker $ 40 to take an extensive qualiﬁcation task , which both trains them in the span categorization scheme and quizzes their understanding . After initial train - ing on selecting spans and marking error severity , we train workers to identify each type of error . For each error type , we provide workers with an En - glish description , examples , an exercise where they must mark the span , and a quiz to classify the cor - rect error type in pre - marked spans . After train - ing all ten error categories , workers attempt a real annotation task where they annotate a paragraph using the full annotation tool . We pass workers if they score above 90 % on the quiz questions , and produce good annotations using the full tool ( via manual inspection ) . This training is available at https : / / yao - dou . github . io / qual / . Annotation For the annotation task , workers an - notate each paragraph using a custom annotation interface ( shown partially in Figure 5 ) , for which we pay $ 3 . 50 . We calculated $ 3 . 50 per annotation by aiming to pay workers at least $ 15 / hour . After several annotation rounds , we observed consider - able variation in time per annotation , so this cost should not be necessarily seen as a requirement for S CARECROW annotations . C Data Quality Identifying and classifying errors in potentially noisy machine generated text is a challenging task . How consistent are the annotations collected from crowd workers ? In this section , we examine the agreement and variability of the collected annota - tions . At a high level , we observe either acceptable or high inter - annotator agreement across error cate - gories . For rare error types such as Bad Math , high agreement stems from the prevalence of spans with no error . For such categories , we recommend treating each annotator as a low - recall , high preci - sion judge , and considering the information from their aggregate annotations . Figure 14 gives an ex - ample of the perspective gained by viewing all 10 annotations of a single generation . Error Krippendorff’s α Two Agree ( % ) Bad Math 0 . 99 30 Commonsense 0 . 88 20 Encyclopedic 0 . 98 12 Grammar and Usage > 1 0 . 72 30 Incoherent 0 . 73 49 Off - Prompt 0 . 71 61 Redundant 0 . 88 38 Self - Contradiction 0 . 87 26 Table 5 : Per - token inter - annotator agreement metrics by error category . The > 1 indicates that we omit severity - 1 Grammar and Usage errors in all analy - ses in this paper due to higher variance ; including them would drop the Krippendorf’s α to 0 . 56 . Agreement Table 5 shows token - level inter - annotator agreement statistics aggregated over all collected data . Since a single annotator can la - bel a single span with multiple errors , we break the agreement statistics down by error category . We report Krippendorff’s α coefﬁcient , a chance - corrected measure of agreement for multiple anno - tators ( Krippendorff , 2018 ) . Due to computational constraints , we calculate this coefﬁcient per gener - ation and report the average across the dataset . The agreement shown here is high for most categories ( > 0 . 8 ) and acceptable ( > 0 . 6 ) for all error types . The Krippendorff measure may be deceptively high for some error types such as Bad Math , where 99 % of tokens are not annotated with this error . The Two Agree measure in Table 5 gives a dif - ferent characterization of this data . Two Agree for a given error label is the percentage of tokens labeled by at least one annotator that were also labeled by one or more additional annotators . This metric allows us to see where annotators agree that par - ticular errors exist while ignoring the majority of tokens ( for most error categories ) which annotators agree are not errors . Two Agree shows signiﬁcantly lower rates for sparse errors with high Krippendorff scores , such as Encyclopedic . However , it reveals stronger agreement among Incoherent and Off - Prompt errors than might be expected given the Krippendorff coefﬁcient . A limitation for both metrics is the use of token - based overlap . 20 Figure 14 : A visual representation of the 10 annotations we collected for one paragraph . Each blue bar represents one annotator , where the width of the bar represents the text of the paragraph . Colored bars drawn on top of the blue bar represent spans marked as errors . We draw bars semi - transparently to show overlapping errors . We can see that some problematic spans ( e . g . , the Off - Prompt section ) are marked by almost all workers and given the same label . Other spans are marked by only a subset of the workers ( e . g . , Commonsense and Incoherent spans on the right side ) , or have some label disagreement . Bootstrap One issue we face is high variance of annotations . To determine the impact of this variance for lower - data settings , we perform a boot - strap analysis using largest subset of our data ( GPT - 3 , top - p = 0 . 96 , t = 1 , f . p . = 0 , for which we have annotations of 200 + generations ) . We choose 50 generations ( roughly 500 annotations ) and cal - culate the error statistics therein . We repeat this process 1000 times and report the mean , standard deviation , and coefﬁcient of variation in Table 6 . We also calculate the coefﬁcient of variation for dif - ferent numbers of samples , shown in Figure 15 . We see that as the number of samples increases , the co - efﬁcient of variation decreases as expected , though less precipitously after 30 examples . These results show that with as few as 50 documents , the S CARE - CROW error analysis should yield relatively robust results . However , this varies by error type : rare errors like Bad Math and Encyclopedic show greater variance . Here , again we repeat our recom - mendation to treat annotations for these categories in aggregate . These results motivate our collection of at least 500 annotations per condition studied . D Further Analysis Here we analyze aspects of the data annotation omitted from the paper body . D . 1 Topics As noted in § 5 . 3 , we collect data using prompts drawn primarily from 12 – 14 news topics . For con - ciseness , we show results only for GPT - 3 , and only for the standard apples - to - apples decoding conﬁgu - 25 50 75 100 125 150 175 200 Number of Generations Annotated ( 10 annotations / generation ) 0 . 00 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 0 . 12 0 . 14 C oe ff i c i en t o f V a r i a t i on ( 2 / ) Coefficient of Variation : Overall 25 50 75 100 125 150 175 200 Number of Generations Annotated ( 10 annotations / generation ) 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 C oe ff i c i e n t o f V a r i a t i on ( 2 / ) Coefficient of Variation : by Span Type Needs _ Google Off - prompt Redundant Technical _ Jargon Grammar _ Usage Incoherent Commonsense Self - contradiction Encyclopedic Bad _ Math Figure 15 : Change in coefﬁcient of variation as num - ber of bootstrap samples increases overall ( top ) , and by span type ( bottom ) , with 95 % conﬁdence intervals . Data shown for GPT - 3 with apples - to - apples decoding conﬁguration ( top - p = 0 . 96 , t = 1 , no f . p . ) . 21 Error mean std . c . v . ( % ) Bad Math 8 . 51 3 . 78 44 . 5 Commonsense 39 . 40 8 . 67 22 . 0 Encyclopedic 13 . 56 3 . 94 29 . 1 Grammar and Usage 126 . 19 16 . 81 13 . 3 Incoherent 96 . 89 16 . 58 17 . 1 Off - Prompt 167 . 29 23 . 39 14 . 0 Redundant 114 . 77 22 . 53 19 . 6 Self - Contradiction 60 . 54 11 . 94 19 . 7 Technical Jargon 100 . 95 24 . 09 23 . 9 Needs Google 482 . 84 42 . 22 8 . 7 Total errors 1268 . 48 55 . 59 19 . 72 Table 6 : Bootstrap analysis ( sampling 50 generations ) of error counts , by category ( c . v . is the coefﬁcient of variation ) . T e c h H e a l t h E n t e r t a i n m e n t S c i e n c e B u s i n e s s T r a v e l P o li t i c s C r i m e S p o r t s F o o d S t y l e N a t u r e A r t H i s t o r y 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 S pan c o v e r a g e GPT - 3 ( p = 0 . 96 , t = 1 . 0 , no freq . penalty ) Span Coverage by Topic Figure 16 : Average span coverage for different top - ics ( GPT - 3 generations with apples - to - apples decoding conﬁguration ) , with 95 % conﬁdence intervals . While the majority of topics display no signiﬁcant trend , we observe that more technical topics such as Tech and Health are covered by a higher density of error spans than Style and Art . ration . Figure 16 plots , based on the prompt topics , the average portion of the generation that is covered by error spans . While there is no signiﬁcant difference between most topics , the results do indicate that generating text in more technical domains leads to higher span counts . Figure 17 shows individual span prevalence by topic . The top heatmap normalizes each topic ( col - umn ) independently . Needs Google issues and Off - Prompt errors dominate the span types , with a few exceptions : for History , and Nature articles , Redundant trumps Off - Prompt as a source of errors . For the bottom , if we instead normalize by error label ( row ) , we can observe which topics are more prone to certain error types than others . For exam - ple , we can see Bad Math errors are most com - A r t B u s i ne ss C r i m e E n t e r t a i n m en t F ood H ea l t h H i s t o r y N a t u r e P o li t i cs S c i en c e S po r t s S t y l e T e c h T r a v e l Bad Math Commonsense Encyclopedic Grammar / Usage Incoherent Needs Google Off - Prompt Redundant Self - Contradiction Technical Jargon 0 2 0 0 0 1 0 0 0 0 0 0 0 2 1 5 4 2 3 2 2 0 2 3 3 2 3 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 7 8 7 5 12 2 1 1 9 5 5 7 5 9 23 7 8 6 14 4 4 0 8 8 8 10 5 9 40 42 34 39 42 38 64 39 48 33 47 24 27 37 19 21 34 31 24 35 3 25 24 31 31 45 44 30 4 5 8 4 2 10 22 36 3 14 2 3 7 7 1 2 4 11 4 2 4 0 4 1 4 8 4 3 4 7 1 1 0 4 0 0 1 3 1 3 4 1 GPT - 3 Span Coverage % Normalized by Topic 0 10 20 30 40 50 60 A r t B u s i ne ss C r i m e E n t e r t a i n m en t F ood H ea l t h H i s t o r y N a t u r e P o li t i cs S c i en c e S po r t s S t y l e T e c h T r a v e l Bad Math Commonsense Encyclopedic Grammar / Usage Incoherent Needs Google Off - Prompt Redundant Self - Contradiction Technical Jargon 6 34 0 0 0 22 0 0 1 6 0 0 4 27 3 16 11 6 8 7 4 0 5 10 8 4 12 4 9 2 9 13 0 9 0 0 15 5 1 1 20 17 7 10 8 8 13 4 1 1 12 7 5 7 8 11 17 7 7 7 11 5 3 0 7 9 6 8 7 8 6 8 6 8 7 9 9 6 9 7 8 4 7 7 4 5 8 9 5 11 1 5 6 8 7 9 15 7 3 5 6 3 1 11 14 24 3 14 1 2 8 6 2 4 7 25 7 5 6 0 8 3 6 12 11 5 9 22 4 6 1 15 0 0 2 12 2 7 16 3 GPT - 3 Span Coverage % Normalized by Label 0 5 10 15 20 25 30 Figure 17 : Span coverage across both topic ( x - axis ) and span label ( y - axis ) for GPT - 3 generated spans ( apples - to - apples decoding : p = 0 . 96 , t = 1 , and no frequency penalty ) . Top : normalized by topic ( col - umn ) ; bottom : normalized by span type ( row ) . mon in Business and Health generations ; Entertain - ment causes the most Self - Contradiction errors ; and Technical Jargon appears more frequently in articles about Business , Technology , or Health . D . 2 Error explanations Figure 18 displays word clouds for common uni - grams and bigrams found in the error explanations for each error type , and Figure 19 shows the aver - age explanation lengths for each error type . For Technical Jargon , Redundant , and Needs Google error types , the prominent words do not provide much illumination and they have short av - erage explanation length , indicating that the ex - planations are straightforward afﬁrmations of the category ( “I think this is ﬁnancial jargon , ” “The information is repeated , ” or “I would need Google to check this . ” ) . But for categories like Encyclo - pedic and Bad Math , we observe some coarse trends : “year” is prevalent in both , “movie” ap - pears in Encyclopedic , and “million” is present in 22 Figure 18 : Common unigrams and bi - grams observed in the explanations written for each annotated span , grouped by span type . Self - Contra . Com . - sense BadMath Off - Prompt Inco - herent Encyclo - pedic TechnicalJargon Grmr . / Usage Redun - dant NeedsGoogle 0 . 0 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 T o k en s Average Explanation Lengths Figure 19 : Average number of tokens in explanation for each error type . We observe explanation length cor - relates with how obvious the error type is , where cat - egories like Grammar and Usage and Technical Jargon are easier to ﬁnd and explain than Self - Con - tradiction and Commonsense . Bad Math , which suggests that the explanations are more likely from outside knowledge and needs some calculation ( “The iPhone uses a lightening connector not a L - shaped connector , ” or “5000 feet is 1524 meters . ” ) Figure 20 presents a few representative explana - tions for four error types , taking particular note of their explanation lengths ( Figure 19 ) . . Both Self - Contradiction and Redundant errors have an - tecedents , but their explanations are markedly dif - ferent . Explanations for Self - Contradiction con - tain more information describing the particular se - mantics that is reversed , which are less obvious at ﬁrst glance than other errors . On the other hand , Redundant errors are more straightforward to spot , often involving simple lexical overlap , and so don’t require elaboration . Explanations for Commonsense contain the Commonsense Grammar / Usage Self - Contradiction Redundant There should be a period after ' video ' . Needs end quotation marks . Word usage . Correction : despite . If he wasn ' t interested , he wouldn ' t be attracted to her ' for years ' . The span says the villagers rescued Rinku from her house , but the ﬁrst span says that the villagers chased the kidnappers and found Rinku near a tea stall . How can they end up with a title if they lost in the ﬁnale ? This was already stated . Duplication Phrase is repeated at the end of the paragraph It doesn ' t seem logical that a Sicilian restaurant would have Chinese take - out . It ' s hard to believe 50 , 000 people were homeschooled by one person . In a psych department of a hospital they would not call an ambulance nor would an ambulance have or give a lethal dose of a narcotic . longer error explanations shorter error explanations Figure 20 : Examples of error explanations from differ - ent error types that favor longer ( top ) and shorter ( bot - tom ) descriptions . true commonsense knowledge that the text violates , which may take several words to explain . But an explanation for a Grammar and Usage error simply corrects the error ; as these errors are easier to ﬁx , the explanation lengths are often short . E Future Work We outline several further directions of study cen - tering around the S CARECROW annotation frame - 23 work , considering both natural implications and broader steps . E . 1 S CARECROW Studies : Simple Find the best - performing GPT - 3 decoding hy - perparameters . We observed that for GPT - 3 , a frequency penalty value of 1 with argmax sampling produced fewer error spans than any other conﬁg - uration ( Fig . 4 ) . We have not tried varying the frequency penalty to values between 0 and 1 , or adding any presence penalty ( § 5 . 2 ) , both of which then allow for fresh explorations of top - p and tem - perature . Study decoding parameters in smaller models . How good can ( a ﬁnetuned ) GPT - 2 get ? We saw decoding parameters considerably impacted GPT - 3’s performance , moving it from edging out Grover to error rates close to humans ( Fig . 4 ) . Could such decoding changes have a similar effect on a GPT - 2 - sized model ? Or might a smaller model favor different decoding hyperparameteres ? Back - off annotations . We observed good anno - tator agreement given the complexity of the task , but the odds that two annotators agree exactly on each span’s type and boundaries remains only mod - erate ( § C ) . We did not try backing - off ( a ) error types into coarser categories ( e . g . , language , fac - tual , reader issue ) or even to binary presence ; ( b ) span boundaries into phrase or sentence - level an - notations . Applying a type of back - off could also allow clustering methods to discover different error ontologies . Improve automatic error detection . While we present baseline results for automatic span error de - tection ( § 7 ) , we anticipate that signiﬁcant progress is still available in this new task . E . 2 S CARECROW Studies : Complex Align multiple annotations . In the current work , we largely treat annotators independently , with the exception of measuring their overlap to study agree - ment ( § C ) or taking their union to train prediction model ( § 7 ) . However , we might consider other ways of viewing the 10 annotations for each gener - ation together . For example , we might consider the aggregate decision of whether a token is labeled with any span a measure of how noticeable or jar - ring an error is . This measure may be related to error severity , but may be distinct from it . One might also consider formal methods for computing annotation alignments . The Gamma measure , proposed by Mathet et al . ( 2015 ) , satisﬁes the long list of criteria needed to align and mea - sure S CARECROW annotations : spans of multiple types , with gaps , full and partial span overlap , more than three annotators , and the potential to merge or split annotations ( which we have not addressed in this paper ) . While we performed experiments with this measure , we experienced difﬁculties pro - ducing intuitive alignments with the authors’ soft - ware , which disallows conﬁguring parameters of the mixed - integer programming problem . 9 Emerg - ing concurrent work ( Titeux and Riad , 2021 ) offers a reimplementation of this measure that exposes additional parameters , which may be a promising avenue . However , it is possible that aligning anno - tations is a challenging task on its own that might require use of the explanations . Characterize error nuance . Related to the pre - vious point about error alignment , one might study whether model size affects span agreement . Anec - dotally , errors from larger models like GPT - 3— even of the same type , like Commonsense errors— are more difﬁcult to describe without careful con - sideration , and may also be more difﬁcult to iden - tify . Characterize repetition . Our quantitative stud - ies of Redundant errors ( e . g . , Figs . 9 and 8 ) point to semantic repetition as the major issue that emerges as models are scaled . Though this effect may be mitigated by changes to the decoding algo - rithm ( like the frequency penalty ) , we still observe that models have difﬁculty striking a balance of repetition . With excessive paraphrasing , generated text seems stuck on an idea . But equally , if a gen - eration moves too quickly between ideas without linking them together or to an overall theme , the text lacks coherence . We posit that the issue of Redundant text emerges as the shadow of encom - passing issues of narrative structure and discourse . E . 3 Broadening S CARECROW Constrained generation This paper focuses on open - ended generation ( § 3 ) , but a natural extension of this method would be to assessing constrained generation tasks , such as machine translation . 9 The mixed - integer programming approach is also com - putationally intensive ; e . g . , memory alone prevented us from computing alignments for pilot studies with twenty annotators , even on a machine with 500GB of RAM . 24 New error types Especially if considering a novel task setting , new error types may prove use - ful . For example , in constrained generation , one might consider an Adequacy error , which—as in machine translation—would indicate that the meaning of a span diverges from what is expected given the generation constraints . Furthermore , one might need to introduce annotations on the pro - vided ( not generated ) text to account for desired semantic components that are missing from the gen - erated text . Or , perhaps for a dialog setting , one might introduce a Generic label , which would indicate that a portion of the generation is other - wise coherent and correct , but offers a lack of new information . 10 Corpus - level evaluation Other work has consid - ered the evaluation of natural language generations at - scale , looking at distributional properties of the text ( Caccia et al . , 2020 ; Pillutla et al . , 2021 ) . We suggest that these views are complementary to instance - based , human evaluation proposed here , and combining the approaches could lead towards a more holistic view of generative evaluation . For ex - ample , while all Self - Contradiction errors right now are within - document , one could similarly iden - tify cross - document contradiction errors , where a model is inconsistent at a more global scale . E . 4 Applications Detecting factuality One potential application of the S CARECROW data could be using the Needs Google spans as a dataset of its own . In addition to training models to identify spans that require ver - iﬁcation , one could go a step further and consider evidence retrieval for each span , and even propose a classiﬁcation task . 11 Editing errors One errors can be detected , can they be ﬁxed ? The difﬁculty and scope of ﬁxing S CARECROW - identiﬁed errors may depend on the span type , as error ﬁxes may have cascading effects in the rest of the document . 10 Such generic language may be seen as violating Grice’s Maxims ( Grice , 1975 ) , for example , by providing a dearth of information quantity , or by ﬂouting improper manner by lacking brevity . 11 Minimally , Needs Google spans from human - authored reputable news text should ( hopefully ) all be factually correct . 25