Synthetic Literature . Writing Science Fiction in a Co - Creative Process Enrique Manjavacas [ 1 , 3 ] enrique . manjavacas @ uantwerpen . be Folgert Karsdorp [ 2 ] folgert . karsdorp @ meertens . knaw . nl Ben Burtenshaw [ 1 , 3 ] benjamin . burtenshaw @ uantwerpen . be Mike Kestemont [ 1 , 3 ] mike . kestemont @ uantwerpen . be Computational Linguistics & Psycholinguistics Research Center [ 1 ] The University of Antwerp , Lange Winkelstraat 40 - 42 , Antwerp , Belgium Meertens Instituut [ 2 ] Oudezijds Achterburgwal 185 , 1012 DK Amsterdam , The Netherlands Antwerp Centre for Digital Humanities and Literary Criticism [ 3 ] The University of Antwerp , Prinsstraat 13 , Antwerp , Belgium Abstract This paper describes a co - creative text gener - ation system applied within a science ﬁction setting to be used by an established novelist . The project was initiated as part of The Dutch Book Week , and the generated text will be published within a volume of science ﬁction stories . We explore the ramiﬁcations of apply - ing Natural Language Generation within a co - creative process , and examine where the co - creative setting challenges both writer and ma - chine . We employ a character - level language model to generate text based on a large corpus of Dutch novels that exposes a number of tun - able parameters to the user . The system is used through a custom graphical user interface , that helps the writer to elicit , modify and incor - porate suggestions by the text generation sys - tem . Besides a literary work , the output of the present project also includes user - generated meta - data that is expected to contribute to the quantitative evaluation of the text - generation system and the co - creative process involved . 1 Introduction In this paper we present ongoing work towards de - veloping a text editing application , through which an established author of Dutch - language literary ﬁction will use an AI - based text generation system with the goal of producing a valuable piece of literature . Our aim is to create a stimulating environment that fos - ters co - creation : ideally , the machine should output valuable suggestions , to which the author retains a signiﬁcant stake within the creative process . The present project is part of a large - scale ini - tiative by the CPNB ( ‘Stichting Collectieve Propa - ganda van het Nederlandse Boek’ , ‘Collective Pro - motion for the Dutch Book’ ) . In Fall 2017 , CPNB will launch their annual media campaign , which this year focuses on robotics . To this end , CPNB will distribute a re - edition of the Dutch translation of Isaac Asimov’s I , Robot that is planned to include an additional piece written as part of a human - machine collaboration . This project report is structured as follows . We ﬁrst introduce the CPNB in greater detail , with spe - cial emphasis on their annual media campaign . We go on to introduce this year’s ‘Robotics’ theme , and the way it centers around Asimov’s I , Robot . Then , we concisely survey the state of the art in text gener - ation from the point of view of co - creation between human and machine . Next , we describe our current text generation system , starting with the large body of Dutch - language ﬁction ( 4000 + novels ) that is at the basis of our experiments , as well as its prepro - cessing . We describe our choice of architecture for Natural Language Generation ( NLG ) —a character - level Language Model ( LM ) based on Recurrent Neural Networks ( RNN ) with attractive properties for the present task — and discuss how author and genre - speciﬁc voices can be implemented through ﬁne - tuning of pre - trained LMs . We present ample examples to illustrate the model’s output for various settings . We also discuss possible ways to evalu - ate our system empirically—a common bottleneck of text generation systems— , through the monitor - ing of user’s behavior and selectional preferences . Finally , we discuss the design of the interface of our application , emphasizing various ways in which the author will be able to interact with the software . 1 . 1 Trust for the Collective Promotion of the Dutch - language Book The CPNB 1 is a trust and PR agency based in The Netherlands that aims to promote the visibility of books and the publishing sector in Dutch society at large . The agency is responsible for a num - ber of high - visibility annual initiatives , such as the ‘Boekenbal’ ( ‘Book ball’ ) and Boekenweek ( ‘Book week’ ) . These initiatives often center around speciﬁc themes . For the 2017 campaign Nederland leest ( ‘The Netherlands reads’ ) , the CPNB chose ‘robotics’ as the overarching theme for their cam - paign . Thereby further exacerbating the debate as the societal opportunities and challenges that come with the increase of artiﬁcial intelligence in every - day life , as well as literature . The campaign , for in - stance , includes the distribution of promotional ma - terial for children ( see Figure 1 ) , as well as copies of I , Robot ( 1950 ) —the well - known science ﬁction novel by Isaac Asimov—in its Dutch translation Ik , Robot ( 1966 ) by Leo Zelders , which serves as the focal point of the 2017 campaign . The novel is composed of interrelated short stories , prepublished in the journal Astounding Science Fiction between 1940 and 1950 . They revolve around the ﬁctional 1 Stichting Collectieve Propaganda van het Nederlandse Boek : https : / / www . cpnb . nl . character of robot - psychologist Dr . Susan Calvin . The novel is especially famous because of the ‘Three Laws of Robotics’ which feature as an intriguing ethical backdrop . The CPNB wanted to encourage debate about the role of AI and robotics in literature through the ad - dition of a 10th short story co - created by an es - tablished ﬁction writer and a machine . An award - winning Dutch author , Ronald Giphart , agreed to take part in this experiment . Figure 1 : Make - it - yourself cardboard robot . Promotional ma - terial distributed as part of the 2017 ‘Nederland leest ! ’ cam - paign by the CPNB on robotics and books . 1 . 2 Co - creativity Co - creativity is a collaborative process between multiple agents , where in this context , one agent is a computational system . Davis sees co - creativity as the ’blending’ of improvisational forces ( Davis , 2013 ) . This goes against the pragmatic distribu - tional of labor that we might see in creative support systems , or how computers are treated in everyday life , and invites them into an indistinct and overlap - ping process of creativitiy . Where crucially , the re - sult of the output is greater than ’the sum of its parts’ ( Davis , 2013 ) . Interestingly , as pointed out by existing literature ( Lubart , 2005 ; Davis , 2013 ; Jordanous , 2017 ) , the public are suspicious of systems that purport to be autonomous whilst in fact involve human partici - pation . Whilst for Lubart the opposite is the case . Lubart reorientates the scientiﬁc perception of these systems into one aligned with Human Computer In - teraction , where they are examples of successful fa - cilitators of improvisation ( Lubart , 2005 ) . Lubart clariﬁes co - creativity into four distinct roles for a computational system ; ’Computer as nanny’ , ‘Com - puter as penpal’ , ‘Computer as coach’ , ‘Computer as colleague’ ( Lubart , 2005 , p . 366 ) . In this project we are most interested in achieving the last , though in practice , much of what our system does could be considered under the second . For a more thorough overview of co - creativity and its role within com - putational creativity research , see the proceeding of The International Conference on Computational Creativity 2012 ( Maher , 2012 ) , and for a broader of view of the term in relation to computing , look to the work of Edmonds and Candy ( Edmonds et al . , 2005 ; Candy and Edmonds , 2002 ) . Developing NLG systems within a co - creative en - vironment allows researchers to utilize the human agent within the system’s workﬂow , allowing for ap - proaches that are potentially too experimental for a solely computational approach . Furthermore , co - creation adds a collaborative and challenging dimen - sion to the process of writing , which in turn encour - ages the human writer . That said , though collabora - tion is commonplace in writing , it is not always wel - come . The creative process of writing is associated with a ﬂuidity that can easily be hindered or broken ; Umberto Eco’s renowned ’How to write a thesis’ as - serts that writers should nurture their process ( Eco , 2015 ) . In developing this system alongside novelist Ronald Giphart , we sought to apply our work within his established methodology in a way that enriches both parties . From a technical point of view , there is a possi - bility to limit the collaborative NLG system to an assistive role , solely aiding the writer . However , a valid collaboration should provoke and challenge the writer . It should test them , push them , and ask them to reconsider their approach . To achieve this balance we chose to treat the writer as a competent handler of text , completely capable of dealing with generated language , and unlikely to be overwhelmed . This ap - proach certainly would not work for all applications , but seems appropriate to a professional science ﬁc - tion writer . As Natural Language Generation develops into a useful instrument in the creation of ﬁctional prose , inherent questions arise around how computational systems relate to human writers . Nowhere else are these questions more at home than in science ﬁction literature , where readers and writers are eager to ex - plore the speculative limits of technology . This will - ingness allowed us to consider the practical implica - tions and qualities of co - creative writing , and how they manifest within the interface itself ( see Section 4 ) . 2 Related Work Natural Language Generation within a collaborative writing environment is an active area of research . The co - creative setting gives scope to apply exper - imental approaches within the dynamic context of a working process . Here we will outline two es - tablished approaches : the structural diagramatic ap - proach , and the auto - completion approach . Ahn , Morbini and Gordon use causal graphs to map the narrative steps of a story which the writer can ma - nipulate into the eventual story structure , the sys - tem will then use probabilistic modeling to generate language around that skeleton . This approach gives the system access to the abstract narrative core of a story’s structure ; arguably , in doing so the system imposes upon the writer a far more structured ap - proach than they are likely familiar with . A collab - orative system should be able to ﬁt within a writ - ers existing working process ( Ahn et al . , 2016 ) . Roemelle and Gordon offer a more hands on ap - proach to assistive writing . Their system acts as a ’Narrative Auto - Completion’ , where the writer is prompted with possible sentences ( Roemmele and Gordon , 2015 ) . Though straightforward , this ap - proach is highly intuitive and unobtrusive ; however , the system risks fulﬁlling the role of tool rather than collaborator . As such , Creative Help is a retrieval - based system , as appose to the generative approach presented below . Narrative generation has been a central topic of computational creativity for decades . One of the ﬁrst examples is Tale - Spin , a system that generates Ae - sop’s Fables guided by a user’s keyword suggestions ( Meehan , 1977 ) . More recently and nearer to this project , McIntyre and Lapata developed a proba - bilistic narrative generator that uses user - input to re - trieve related phrases ( McIntyre and Lapata , 2009 ) . The system here differentiates itself from those by working on the character level . Generated text re - produces the style and voice of its training material , but does not directly sample quotes verbatim from the training material . 3 Method 3 . 1 Collection and Preprocessing The ﬁrst step in constructing our NLG system was to compile a sufﬁciently large corpus of literary works . In the present study , we employ a large collection of Dutch novels in epub format ( Williams , 2011 ) , which contains a diverse set of novels in terms of genre , and is heterogeneous in style . In total , the col - lection consists of 4 , 392 novels , written by approxi - mately 1 , 600 different authors . The average number of novels written by each author is 2 . 5 . The large standard deviation of 6 . 5 is caused by the skewed distribution in which a few authors contribute rel - atively large oeuvres , such as detective writer Ap - pie Baantjer . The novels were tokenized for words , sentences and paragraphs using the Tokenizer Ucto , which was conﬁgured for the Dutch language ( Van Gompel et al . , 2012 ) . The total number of sen - tences , words and characters in the tokenized col - lection ( including punctuation ) amounts to approxi - mately 24 . 6M , 425 . 5M , and 2 . 1G , respectively . On average , each novel consists of 3k sentences , 59k words , and 309 , 531k characters . 3 . 2 Character - level Language Models for NLG The aim of this project is to contribute to literary writing in a co - creative environment , as opposed to solely narrative generation . Therefore , we approach NLG using character - based Language Models ( LM ) which typically reason at a local level , in the order of some few hundreds of characters . Because of this , the LM is only implicitly aware of the global narra - tive structure , but still powerful enough to capture sentence semantics in a unsupervised fashion . An LM is a probabilistic model of linguistic se - quences that estimate a probability distribution over a given vocabulary conditioned on the previous text ( left - to - right model ) . More formally , at a given step t , an LM deﬁnes a conditional probability , express - ing the likelihood that a certain vocabulary item ( typically a word or character ) will appear next : LM ( w t ) = P ( w t | w 1 , w 2 , . . . , w t − 2 , w t − 1 ) ( 1 ) Different LM implementations exist , which diverge in the manner in which they model the previous text . Given their probabilistic nature , LMs are straight - forward to deploy for NLG . The generative process is deﬁned by sampling a character from the output distribution at step t , which is then recursively fed back into the model , potentially preceded by the pre - vious output of the model , to condition the next gen - eration at step t + 1 . A few decoding approaches can be implemented based on different sampling strategies . For instance , a rather naive approach to - wards sampling is to select each character so as to maximize a generated sequence’s overall probabil - ity . Nevertheless , for a large vocabulary size ( e . g . in the case of a word - level model ) , the search soon be - comes infeasible ; therefore , approximate decoding methods , such as beam search , are used to ﬁnd an ideal solution . When used for generation , the naive argmax decoding strategy has a tendency towards relatively repetitive sentences , that are too uninspir - ing to be of much use in a creative setting . For the present work , we therefore decode new characters via sampling from the multinomial distribution at each step . It is interesting to note that the different decoding approaches stand in a trade - off relationship between diversity and correctness . For example , whereas argmax decoding will tend to generate sentences that are very similar , general and monotonous yet formally correct ( e . g . more similar to the sen - tences observed in the training corpus ) , multinomial sampling will make the output diverge more from the original training data , and therefore produce a more varied output , with a tendency towards for - mally incorrect sentences . Focusing on our chosen approach—multinomial sampling— , the described trade - off can be operationalized and used to our ad - vantage by letting the author explore model param - eters . This is implemented by exposing a parame - ter τ , commonly referred to as “temperature” , that controls the skewness of the model’s output distri - bution . Given the output distribution at a given step p = ( p 1 , p 2 , . . . , p V ) , a vocabulary size of V , and the temperature value τ , we can compute a transforma - tion of p τ of the original p through Equation 2 p τi = p 1 τ i (cid:80) Vj p 1 τ j ( 2 ) p τ will ﬂatten the original distribution for higher val - ues of τ — thereby ensuring more variability in the output . Conversely , for lower values of τ it will skew the distribution—thereby facilitating the outcome of the originally more probable symbol . For τ values approaching zero , we recover the simple argmax de - coding procedure of picking the highest probability symbol at each step , whereas for high enough τ the LM degenerates into a random process in which at any given step all symbols are equally probable re - gardless of the history . In terms of implementations there are two ma - jor approaches to statistical language modeling— ngram - based LMs and RNN - based LMs . 3 . 2 . 1 Ngram Language Models Ngram - based LMs ( NGLMs ) go back to at least the early 1980s in the context of Statistical Ma - chine Translation and Speech Recognition ( Rosen - feld , 2000 ) . An NGLM is a direct application of the Markov assumption to the task of estimating the next character probability distribution—e . g . it uses a ﬁxed - length ngram preﬁx to estimate the next char - acter probability distribution . An NGLM is basi - cally a conditional probability table for Equation 1 , that is estimated on the basis of the count data for ngrams of a given length n . Typically , NGLMs suffer from a data sparsity problem , because with larger values of n possible conditioning preﬁxes will not be observed in the training data and the cor - responding probability distribution cannot be esti - mated . To alleviate the sparsity problem , techniques such as smoothing and back - off models ( Chen and Goodman , 1999 ) can be used to either reserve some probability mass to redistribute it across unobserved ngrams ( smoothing ) , or resort back to a lower - order model to provide an approximation to the condi - tional distribution of an unobserved ngram ( back - off models ) . 3 . 2 . 2 RNN - based Language Models More recently , a new class of LMs based on Re - current Neural Networks ( Elman , 1990 ) have been introduced ( Bengio et al . , 2003 ; Mikolov , 2012 ) and have quickly increased in popularity due to their bet - ter theoretical properties ( no Markov assumption ) , expressive capabilities ( information ﬂow through very long sequences ) and performance gains . An RNNLM processes an input sequence one step t at a time , feeding the input symbol x t through three afﬁne transformations with their corresponding non - linearities . First , the one - hot encoded input vector is projected into an embedding space of dimensional - ity M through w t = W m x t , where W m ∈ R MxV is a embedding matrix . Secondly , the resulting char - acter embedding w t is fed into an RNN layer that computes a hidden activation h t as a combination of w t with the hidden activation of the previous step h t − 1 . This is shown formally in Equation 3 h t = σ ( W ih w i + W hh h t − 1 + b h ) ( 3 ) where W ih ∈ R MxH and W hh ∈ R HxH are re - spectively the input - to - hidden and hidden - to - hidden projection matrices , b h is a bias vector and σ is the sigmoid non - linear function . Finally , the hidden ac - tivation h t is projected into the vocabulary space of size V , followed by a softmax function that turns the output vector into a valid probability distribu - tion . Formally , the probability of character j at step t is deﬁned by P t , j = e o t , j (cid:80) Vk e o t , k ( 4 ) where o t , j is the j th entry in the output vector o t = W ho h t and W ho ∈ R V xH is the hidden - to - output projection . In practice , training an RNN is difﬁcult due to the vanishing gradient problem ( Hochreiter , 1998 ) that makes it hard to apply the back - propagation learn - ing algorithm ( Rumelhart et al . , 1986 ) for parame - ter learning over very long sequences . Therefore , it is common to implement the recurrent layer us - ing an enhanced RNN version to compute h t —such as Long Short - term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) or Gated - Recurrent Unit ( GRU ) ( Cho et al . , 2014 ) — , which add an explicit gated mechanism to the traditional RNN in order to control the preservation of information in the hidden state over very long sequences . 3 . 2 . 3 Model For the present study , we implement several vari - ations of the RNNLM , varying the type of the re - current cell ( LSTM , GRU ) as well as the values of different parameters , such as the dimensionality of the character embedding matrix W m ( 24 , 46 , . . . ) and , more importantly , the size of the hidden layer H ( 1024 , 2048 , . . . ) . We train our models through back - propagation using Stochastic Gradient Descent ( SGD ) , clipping the gradients before each batch up - date to a maximum norm value of 5 to avoid the ex - ploding gradient problem ( Pascanu et al . , 2013 ) and truncating the gradient during back - propagation to a maximum of 200 recurrent steps to ensure sufﬁ - ciently long dependencies in the sequence process - ing . Finally , dropout ( Srivastava et al . , 2014 ) is ap - plied after each recurrent layer following ( Zaremba et al . , 2015 ) to prevent overﬁtting during full model training . 3 . 2 . 4 Overﬁtting After training the full model , we experiment with further ﬁne - tuning on different author and genre spe - ciﬁc subsets to steer the NLG towards a particular style . To enforce this effect , we drive the training to - wards overﬁtting introducing an intended bias in the models predictions towards sequences that are more likely in that particular book subset . We achieve overﬁtting by zeroing the dropout rate and running numerous passes through the subset training data , with a sufﬁciently small learning rate . We have al - ready observed interesting stylistic and genre prop - erties in the ﬁne - tuned model’s output—see Section 4 . 2 for an illustration . That said , how the partic - ular effect of this technique—differing degrees of overﬁtting—affects the quality of the generated out - put still has to be evaluated . The degree of overﬁt - ting can easily be quantiﬁed and monitored by plot - ting batched - average perplexity values achieved by the model for both the training data and the valida - tion split as shown in Figure 2 . Figure 2 : Example of overﬁtting learning curves during ﬁne - tuning of a full model on a subset of novels by Isaac Asimov . 4 User Interface Uncharacteristically for an NLP project , the visual interface of the system is paramount to its success . Though ultimately the system will be assessed on the language it produces , such language can only be generated if the writer is able to use the system . Therefore , we have focused on functionalities that give the user a clear representation of how text is generated , and allow them to understand how their own writing is affected by the process . This allows them to play a deﬁned role within the process of writing , whilst also encouraging them to use gener - ated text . The user is able to select which model to generate text from , so that they can use multi - ple voices and approaches within the same text ( see Section 3 . 2 . 4 ) . The user can deﬁne ’temperature’ for any model using a slider bar ( see Section 3 . 2 ) . The generated text itself is shown as a list of sug - gestions , along with the model’s own probability scores , below the main text area . This allows the writer to choose between a set of options , and get a broader idea of the models voice . With the help of user edit meta - data—computed by a string difﬁng algorithms—we can track user changes and present them with a visualization of each fragment’s source and the degree of the modiﬁcation . Figure 3 is a visual representation of how text an - notation functions . Text annotation reveals to the writer how generated text is affecting the ﬁnal text . As the writer works into text , they could easily lose Figure 3 : Visual feedback on the co - creation process used by Ronald Giphart . Highlighted fragments are synthetic in origin with the brightness indicating the amount of modiﬁcation intro - duced by the user . track of its source ; therefore , the interface is en - hanced with visual feedback which highlights based on edit distance between original generated text and its current status . Generated text is initially high - lighted in green , as the writer edits that text its color fades into white . At the same time , whole words swapped by the writer are underlined in purple to differentiate lexical changes ( see Figure 3 ) . 4 . 1 Monitoring the Author Our project intends to explore the co - creative pro - cess of science - ﬁction literature on a quantitative and objective basis . In line with ( Roemmele and Gordon , 2015 ) , we acknowledge that such a co - creative interface opens the up possibility for auto - matic evaluation of generative systems based on user edits of generated strings . Our interface is there - fore designed to store all user edits along with the source of the string ( human or machine generated ) . This will enable us to study individual user behavior in relation to the particular properties of the genera - tive system , as well as the aptness of different model variants and their parameter settings ( e . g . degree of overﬁt , temperature , voice ) for co - creation , taking user edit behavior as a proxy for output quality . 4 . 2 Examples As explained in the previous section , the evaluation of a NLG system in a co - creative setting involving both human and machine amounts to the generated material incorporated ( either explicitly or implicitly ) by the author in the ﬁnal work . Suggestions about how to formally and informally evaluate this co - production process were given in the previous sec - tion . Here , we provide an exploratory demonstration of the model’s generation system , where the goal is to highlight some typical behavior of the system un - der different parameters settings , author - based ﬁne - tuning , and text seeds . We begin with a survey of how different temper - ature values τ impact the generated text . In Table 1 we list a number of generated sentences for dif - ferent temperatures given the famous opening sen - tence “Mijn vrouw is dood en al begraven” ( ‘My wife is dead and already buried’ ) from Marcellus Emants’ Een nagelaten bekentenis . These examples have been generated on the basis of a model trained on the complete collection as described in Section 3 . 1 . Several observations can be made from these examples . First , the examples showcase the model’s ability to generate grammatical sentences . Interest - ingly , the sentences become syntactically more com - plex as the temperature increases ( as explained in Section 3 . 2 ) , and also exhibit less repetitive gram - matical structures ( cf . the repetition of possessive van constructions with τ = 0 . 1 ) . Second , a simi - lar diversifying effect can be observed on the lexical level . Higher temperatures are more likely to gen - erate less frequent and semantically more speciﬁc lexical material ( cf . the decrease in reusing the same word twice , as well as the increase of names in the ﬁ - nal two examples ) . Third , depending on the reader’s willingness to search for semantic coherence , most generated sentences can be read as plausible contin - uations of the input seed . Finally , with increased temperature values the model composes sentences with higher semantic and cognitive complexity , jux - taposing and combining less common concepts and thoughts . Within certain limits , we expect such in - creased ‘creativity’ to encourage and provoke the human writer in the process of writing . τ seed Mijn vrouw is dood en al begraven . My wife is dead and already buried . 0 . 1 De volgende dag werd er een bericht van de politie gevonden . De verklaring van de politie was een van de belangrijkste verklaringen van de man die het verhaal van de politie had gehoord . The next day , a police message was found . The police’s statement was one of the most important statements of the man who heard the story by the police . 0 . 3 De dokter keek op zijn horloge . Ze was er zeker van dat hij die dag niet meer zou komen . Hij was niet gek . Hij was niet van plan om zijn dochter te vertellen dat hij haar een keer had gezien . The doctor looked at his watch . She was sure he wouldn’t come anymore that day . He wasn’t crazy . He did not intend to tell his daughter he’d seen her once . 0 . 5 De tranen weerklonken over haar gezicht en voelde ze in haar keel . Ze was er niet . Ze was er zeker van dat ze alleen maar een affaire had gehad met de meisjes . The tears resounded over her face and she felt them in her throat . She wasn’t there . She was sure she had only had an affair with the girls . 0 . 7 Ze zegt dat het weer in San Sebastian naar Cali - forni¨e haar deel van de wereld is . Ze moet van de wereld naar de hemel hebben gelopen . She says that the weather in San Sebastian to Cal - ifornia is her part of the world . She must have walked from the world to heaven . 0 . 9 Lucas kon niet met zichzelf trouwen en toen hij achteloos met zijn gebaren zat te praten maakte hij deel uit van de lessen van de waarheid . Lucas couldn’t marry himself , and when he spoke painlessly with his gestures , he was part of the lessons of truth . Table 1 : Example of our current NLG system with translation seeded by ”Mijn vrouw is dood en al begraven” ( My wife is dead and already buried ) for different temperature τ values . Having explored the impact of temperature on the full model’s output , we now proceed with a brief il - lustration and informal evaluation of the generated output of two ﬁne - tuned models . As explained in Section 3 . 2 . 4 , we experiment with constructing ﬁne - tuned models for speciﬁc styles , genres or authors by post - training on a subset of the collection and driv - ing the training towards overﬁtting . In this section , we demonstrate the effect of overﬁtting two models post - trained on novels by Isaac Asimov and Ronald Giphart , who form the heart of the CPNB’s robotics campaign . Using the same seed from Table 1 , we observe a clear style shift when generating sentences using either the Asimov or Giphart model . For ex - ample , with a temperature setting of τ = 0 . 4 the Asimov model produces utterances such as : “Mijn vrouw is dood en al begraven . ‘Het is de groot - ste misdaad die ik ooit heb gezien . ’ ‘Weet u dat zeker ? ’ ‘Ja . ’ ‘En als dat zo is , wat is dan wel de waarheid ? ’ ( My wife is dead and already buried . ‘It is the biggest crime I’ve ever seen . ’ ‘Are you sure ? ’ ‘Yes . ’ ‘And if so , what is the truth ? ’ ) . By contrast , a model overﬁtted on novels by Giphart generates output such as : “Mijn vrouw is dood en al begraven . Ik heb het over een door mij gefotografeerde vrouw , een hoofd dat met haar borsten over mijn schouder ligt . Ik heb de ﬁlm geschreven die ik mijn leven lang heb geleefd . ” ( ‘My wife is dead and already buried . I’m talking about a woman I once photographed , a head with her breasts over my shoulder . I wrote the movie I’ve lived my life for a long time . ’ ) Both con - tinuations are semantically plausible , yet written in completely different styles , and put focus on differ - ent concepts ( e . g . ‘crime’ versus ‘erotics’ ) , both typ - ical of their respective training material . 5 Conclusion In this paper we have outlined an applied text gen - eration system and graphical user interface , that to - gether facilitate co - creative environment in which to write science ﬁction literature . We have highlighted an existing challenge within state of the art systems , to balance a challenging intervention into the writing process , with the risk of becoming a solely a writing tool . The character - level recurrent neural network for NLG that we have used is experimental within a solely computational approach , and therefore we have leveraged the speciﬁc advantages of working with a professional writer to maximize this system’s ability to be applied . We have how to facilitate a writer to use a language model . We have outlined evaluation procedures for the current NLG system , utilizing user - generated meta - data and quantifying the extent of retained synthetic text . Acknowledgments We would like to thank Ronald Giphart for his time and energy and Stichting Collectieve Propaganda van het Nederlandse Boek for initiating the collabo - ration . References Emily Ahn , Fabrizio Morbini , and Andrew S . Gordon . 2016 . Improving Fluency in Narrative Text Gener - ation With Grammatical Transformations and Prob - abilistic Parsing . In The 9th International Natural Language Generation conference , pages 70 – 74 , Edin - burgh , UK . ACL . Yoshua Bengio , R ´ ejean Ducharme , Pascal Vincent , and Christian Janvin . 2003 . A Neural Probabilistic Lan - guage Model . The Journal of Machine Learning Re - search , 3 : 1137 – 1155 . Linda Candy and Ernest Edmonds . 2002 . Modeling co - creativity in art and technology . In Proceedings of the 4th conference on Creativity & cognition , pages 134 – 141 , Loughborough , UK . ACM . Stanley F Chen and Joshua Goodman . 1999 . An empir - ical study of smoothing techniques for language mod - eling . Computer Speech and Language , 13 : 359 – 394 . Kyunghyun Cho , Bart Van Merrienboer , Dzmitry Bah - danau , and Yoshua Bengio . 2014 . On the Properties of Neural Machine Translation : Encoder – Decoder Approaches . Ssst - 2014 , pages 103 – 111 . Nicholas Davis . 2013 . Human - computer co - creativity : Blending human and computational creativity . In Ninth Artiﬁcial Intelligence and Interactive Digital Entertainment Conference . AAAI Press . Umberto Eco . 2015 . How to write a thesis . MIT Press . Ernest A . Edmonds , Alastair Weakley , Linda Candy , Mark Fell , Roger Knott , and Sandra Pauletto . 2005 . The studio as laboratory : Combining creative practice and digital technology research . International Journal of Human - Computer Studies , 63 ( 4 - 5 ) : 452 – 481 , Octo - ber . Jeffrey L . Elman . 1990 . Finding structure in time . Cog - nitive Science , 14 ( 2 ) : 179 – 211 . Sepp Hochreiter and J ¨ urgen Schmidhuber . 1997 . Long Short - Term Memory . Neural Computation , 9 ( 8 ) : 1735 – 1780 . Sepp Hochreiter . 1998 . The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions . International Journal of Un - certainty , Fuzziness and Knowledge - Based Systems , 06 ( 02 ) : 107 – 116 . Anna Jordanous . 2017 . Co - creativity and perceptions of computational agents in co - creativity . In Proceedings of the Eighth International Conference on Computa - tional Creativity , Atlanta , US . ACC . Todd Lubart . 2005 . How can computers be partners in the creative process : Classiﬁcation and commentary on the Special Issue . International Journal of Human - Computer Studies , 63 ( 4 - 5 ) : 365 – 369 , October . Mary Lou Maher . 2012 . Computational and Collective Creativity : Who’s Being Creative ? In Proceedings of the 3rd International Conference on Computer Cre - ativity , pages 67 – 71 , Dublin , Ireland . ACC . Neil McIntyre and Mirella Lapata . 2009 . Learning to tell tales : A data - driven approach to story generation . In Proceedings of the Joint Conference of the 47th An - nual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1 - Volume 1 , pages 217 – 225 , Sin - gapore . Association for Computational Linguistics . James R . Meehan . 1977 . TALE - SPIN : An interactive program that writes stories . In Proceedings of the 5th International Joint Conference on Ar tiﬁcial Intelli - gence , pages 91 – 98 . Tomas Mikolov . 2012 . Statistical Language Models Based on Neural Networks . Razvan Pascanu , Tomas Mikolov , and Yoshua Bengio . 2013 . On the Difﬁculties of Training Recurrent Neural Networks . Icml , ( 2 ) : 1 – 9 . Melissa Roemmele and Andrew S . Gordon . 2015 . Cre - ative help : a story writing assistant . In International Conference on Interactive Digital Storytelling , pages 81 – 92 . Springer . Ronald Rosenfeld . 2000 . Two decades of statistical lan - guage modeling : where do we go from here ? Pro - ceedings of the IEEE , 88 ( 8 ) : 1270 – 1278 . David E . Rumelhart , Geoffrey E . Hinton , and Ronald J . Williams . 1986 . Learning representations by back - propagating errors . Nature , 323 ( 6088 ) : 533 – 536 . Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov . 2014 . Dropout : A Simple Way to Prevent Neural Networks from Overﬁtting . Journal of Machine Learning Re - search , 15 : 1929 – 1958 . Maarten Van Gompel , Ko Van Der Sloot , and Antal Van den Bosch . 2012 . Ucto : Unicode Tokeniser Reference Guide . Technical report . Greg Williams . 2011 . EPUB : Primer , Preview , and Prog - nostications . Collection Management , 36 ( 3 ) : 182 – 191 . Wojciech Zaremba , Ilya Sutskever , and Oriol Vinyals . 2015 . Recurrent Neural Network Regularization . ICLR , pages 1 – 8 .