Learning Dynamic Graphs , Too Slow Andrei A . Klishin , 1 Nicolas H . Christianson , 2 Cynthia S . Q . Siew , 3 and Dani S . Bassett 1 , 4 , 5 , 6 , 7 , 8 , ∗ 1 Department of Bioengineering , University of Pennsylvania , Philadelphia , PA 19104 USA 2 Department of Computing and Mathematical Sciences , California Institute of Technology , Pasadena , CA 91125 USA 3 Department of Psychology , National University of Singapore , Singapore 117570 Singapore 4 Department of Physics & Astronomy , University of Pennsylvania , Philadelphia , PA 19104 USA 5 Department of Electrical & Systems Engineering , University of Pennsylvania , Philadelphia , PA 19104 USA 6 Department of Neurology , University of Pennsylvania , Philadelphia , PA 19104 USA 7 Department of Psychiatry , University of Pennsylvania , Philadelphia , PA 19104 USA 8 Santa Fe Institute , Santa Fe , NM 87501 USA ( Dated : July 6 , 2022 ) The structure of knowledge is commonly described as a network of key concepts and semantic relations between them . A learner of a particular domain can discover this network by navigating the nodes and edges presented by instructional material , such as a textbook , workbook , or other text . While over a long temporal period such exploration processes are certain to discover the whole connected network , little is known about how the learning is aﬀected by the dual pressures of ﬁnite study time and human mental errors . Here we model the learning of linear algebra textbooks with ﬁnite length random walks over the corresponding semantic networks . We show that if a learner does not keep up with the pace of material presentation , the learning can be an order of magnitude worse than it is in the asymptotic limit . Further , we ﬁnd that this loss is compounded by three types of mental errors : forgetting , shuﬄing , and reinforcement . Broadly , our study informs the design of teaching materials from both structural and temporal perspectives . I . INTRODUCTION Knowledge structures built by humans are best de - scribed as networks [ 1 – 6 ] . Such networks can be built when acquiring one’s ﬁrst language [ 7 – 9 ] , engaging in curiosity - driven free exploration [ 10 ] , or accumulating scientiﬁc knowledge throughout humanity’s history [ 11 ] . Network representation emphasizes not just a listing of known facts ( nodes ) , but also the intricate pattern of in - terconnections between these facts ( edges ) [ 12 ] . These connections also strongly aﬀect how the network evolves , from discovering brand new nodes and edges [ 13 ] to learn - ing them from an external source such as a formal class [ 14 ] . The structure of networks is easily revealed by dynam - ical processes upon them , such as random walks [ 15 , 16 ] or more complex exploration algorithms [ 17 ] . Uncon - strained memory processes such as random walks mimic several of the features of spontaneous thought [ 18 ] . Ran - dom walk measures also capture the indirect relationships among words in semantic networks better than distri - butional measures of language corpora [ 19 ] . Controlled psychological experiments show that humans intuitively and eﬃciently learn the structure of networks from the statistics of a sequence of stimuli [ 20 , 21 ] . This learn - ing is imperfect : allowing for mental errors reduces the cognitive load on the brain but also highlights the higher - order structure of networks at the expense of ﬁne detail [ 22 – 25 ] . This optimization of cognitive eﬀort in the brain ∗ dsb @ seas . upenn . edu is hypothesized to shape the communication network ar - chitectures themselves [ 26 – 28 ] , and has an established neurophysiological basis [ 29 – 32 ] . Learning can be studied at two scales : that of large networks which encompass the full knowledge of an indi - vidual or a society , and that of small synthetic networks which are used in memory experiments . Yet typical ed - ucational scenarios lie in between these two extremes : a learner is expected to acquire a ﬁnite number of con - nected concepts from a ﬁnite lecture course or a single textbook [ 33 , 34 ] , each of which has its own particu - lar network structure [ 35 – 37 ] . The success of learning is measured not by memorizing the text of the educa - tional medium , but by understanding key concepts and their interconnections [ 38 , 39 ] which support problem - solving [ 40 , 41 ] . At the same time , exposure to the same educational materials can result in building very diﬀerent networks , owing both to varying learner eﬀort and mem - ory imperfections [ 14 , 42 , 43 ] . Learners also vary in how they identify the key concepts [ 44 ] and the genealogical relations between them [ 45 ] . The learning sciences have had an enduring focus on learning mechanisms that lead to better memory of stud - ied materials ( see [ 46 ] for an overview ) . For example , the eﬀects of retrieval practice , where learners who practice retrieving information from memory do better at a test than learners who simply re - studied the material [ 47 ] , are well - established , as are the eﬀects of interleaved practice , where better learning occurs when diﬀerent to - be - learned skills or topics are continuously alternated [ 48 ] . In con - trast , relatively less attention has been directed to the nature of the inputs for learning . This lack is striking a r X i v : 220 7 . 02177v1 [ c ond - m a t . s t a t - m ec h ] 5 J u l 2022 2 given that ( i ) a core principle of learning predicts that the frequency of input shapes what is being learned [ 49 ] , and ( ii ) human learners are highly sensitive to statisti - cal associations encountered in the environment , even through passive or incidental exposure [ 7 , 21 ] . Ref . [ 50 ] demonstrated that students pick up spurious correlations between features of problem sets and solution strategies in mathematics textbooks . This behavior could have im - plications for mathematics learning because students use these features to decide on a solution strategy even when the feature is irrelevant . Given that there are observ - able eﬀects of input set characteristics on learning [ 51 ] , techniques are needed to represent the structure of the learning environment [ 36 ] and also to model the map - ping of this structure to the mental models that learners acquire through their interactions with the learning en - vironment . In this paper we trace how learner eﬀorts and mem - ory eﬀects drive the formation of learned mental models from taught semantic networks . For the example teach - ing material , we use networks of key concepts extracted from ten popular linear algebra textbooks [ 36 ] . As the learner progresses through the text , the network struc - ture aﬀorded to them by the book evolves as well . We model learning as a random walk on this temporal net - work and study the statistics of learning through a combi - nation of numerical stochastic simulations and analytical exposure theory as introduced in Ref . [ 52 ] and expanded upon here . We ﬁnd that the network learned with ﬁnite eﬀort is signiﬁcantly incomplete , containing fewer net - work nodes and edges , and that this eﬀect is further com - pounded by memory imperfections . One particular form of mental error , random shuﬄing of the order of stimuli , mostly leads to worse learning of already presented net - work edges but can aﬀord a limited ability to anticipate future edges . This work oﬀers a principled way to pre - dict the learned network structures from taught ones , for a variety of learners . In educational settings , this pre - diction can serve as an important forewarning and thus be used to tweak the instructional materials to empha - size some concepts and connections over others to ensure that they are learned . II . LEARNING MODEL A . What is taught In this paper we are interested in a human subject learning a structured semantic network of concepts . This is a very common scenario of learning from instructional material , such as a textbook , a lecture , or a webpage . Learning from a single ﬁxed source is a building block of a more open - ended , self - directed learning process that draws on multiple sources , synthesizes the information , and relays it to others . In order to obtain quantitative insights into the learn - ing process , we must establish models of two complemen - tary processes : how information is taught by the source material and how it is learned by the human ( Fig . 1 ) . Notably , the goal of such learning is not to reproduce the material verbatim ( as would be necessary for recit - ing a poem or a musical piece ) , but to reconstruct the pattern of connections between the concepts that can be dynamically traversed . On this view , the source material exposes a range of concepts ( Fig . 1a ) and their connec - tions of varying strength , which might also change in time ( Fig . 1b - c ) . The learner takes a random walk along the conceptual connections and records a memory of the sequence of steps ( Fig . 1d ) . The resulting memory is a learned , or reconstructed , semantic network that some - times closely follows the source material , and other times diﬀers from it in important ways ( Fig . 1e ) . Qualitatively , the relationship between the taught and the learned net - works can be drawn as a Venn diagram ( Fig . 1f ) . In order to make this relationship quantitative , below we outline the mathematical model of learning . We demonstrate our model of teaching and learning on a concrete set of ten networks extracted from popular linear algebra textbooks in Ref . [ 36 ] . For each textbook , the authors identiﬁed the set of important mathematical concepts such as “matrix” or “polynomial” . Two con - cepts i , j are deemed connected if they are mentioned to - gether in a sentence . The number of such co - occurrences is given by the elements A ij of the weighted symmetric adjacency matrix , whereas the sentence of the ﬁrst co - occurrence is given by the elements F ij of the ﬁltration matrix ( see Appendix A for a comparison of basic text - book statistics ) . Notably , the textbook network provides a dynamic substrate for random walks . The probability of going from node i to node j is given by : P ( j | i ) = T ij ( τ ) = A ij ( τ ) (cid:80) j A ij ( τ ) ; A ij ( τ ) = A ij · [ F ij < τ ] , ( 1 ) where T ij ( τ ) are the elements of the row - normalized tran - sition matrix , τ indexes progress through the book as measured by sentences , and [ · ] is an indicator function equal to 1 if the expression inside is true , and 0 oth - erwise . Throughout the paper , we use A ij without an argument to refer to the number of co - occurrences and A ij ( τ ) with an argument as an explicitly temporal el - ement of the adjacency matrix . In other words , each edge ( i , j ) of the network appears at full strength at the time point given by its ﬁltration order F ij . The random walk is thus restricted to walking along the connections that have already been introduced . For the textbooks we consider , all newly introduced nodes attach to the main connected component of the network within just a few sentences , so the network stays connected . Every learner is exposed to the same temporal order of network evolution , yet not all learners put equal eﬀort towards learning . One can skim the textbook quickly , or comb through every line and re - derive every proof and exercise . The amount of eﬀort students put into learn - ing might not be susceptible to nudging [ 53 ] , but can 3 W h a t i s t a u g h t × 10 b oo k s Treilbook a Nodes polynomial coefficient variable row number matrix transpose standardbasis linear adjoint columnvector current edges future edges b Edges at τ = early current edges future edges c Edges at τ = late W h a t i s l ea r n e d t d Random walk realedge forgottenedge spuriousedge predictededge e Mental model current edges future edges learned edges s p u r i o u s e d g e s f realedges predictededges FIG . 1 . Linear algebra textbooks provide a substrate for learning a semantic network of concepts . ( a ) Within each textbook , important linear algebra concepts serve as the nodes of the semantic network . ( b - c ) Co - occurrences of concepts within the same sentence form the edges of the semantic network . By a given sentence τ of the book , some edges have already been established ( brown ) , while others appear later ( yellow ) . ( d ) The learning process consists of random walk steps along the edges that already exist . ( e ) The mental model of the network is generated by an imperfect human memory process : some edges are remembered correctly , while others are forgotten or misplaced . ( f ) The set of learned edges overlaps imperfectly with the taught current and future edges , as shown in this Venn diagram . vary depending on interest and curiosity [ 54 ] . Mathe - matically , we model this varying eﬀort with the variable of dilation D ≡ t / τ , where t is the learning time mea - sured in random walk steps and τ is the evolution time measured in sentences . Dilation is thus the average num - ber of random walk steps per sentence , which might be much smaller than 1 ( skimming the text ) or much larger than 1 ( studying thoroughly ) . As the dilation gets arbi - trarily large D → ∞ , the learner samples every possible transition in the network . In the investigations below we treat dilation D as a free parameter . B . What is learned In order to keep track of the built mental model of the network , we introduce the integer - valued memory matrix M where the indices run over the network nodes . At the beginning of the random walk , all entries M ij of the matrix are set to zero , and then are incremented as the random walk advances . Typically , when the learner ob - serves the transition i → j , one count is added to the entry M ij . However , because of certain human memory eﬀects that we discuss below , the count can be misplaced or removed altogether from the memory matrix . Once the memory matrix is obtained , we can construct the empirical transition matrix by performing a row nor - malization element - wise : ˆ T ij = (cid:40) M ij (cid:80) j M ij , (cid:80) j M ij (cid:54) = 0 0 , otherwise , ( 2 ) where the second case reﬂects the fact that if the learner remembers no transitions out of node i , they are unable to estimate the transition probabilities . Since the ma - trices of the textbook network are sparse , the memory matrix is also typically sparse . Note that the absolute number of memory counts cancels out from the expres - sion ; only the relative populations of the entries matter . We assess network learning by comparing the empirical transition matrix ˆ T , the instantaneous transition matrix of the textbook T ( t ) , and the ﬁnal transition matrix of the full textbook T , which represents the complete net - work that the student could hypothetically learn . The three matrices have partial overlaps ( Fig . 1f ) : not all taught edges might be learned and not all learned edges might have been taught . In order to compare the matri - ces , we deﬁne a set of metrics such as precision and recall , 4 a α e ff ec t Random walk α Memory with forgetting b β e ff ec t Random walk t + 1 t t − 1 t − 2 t − 3 Memory with shuffling ∝ e − 0 · β ∝ e − 1 · β ∝ e − 2 · β ∝ e − 3 · β c γ e ff ec t Mental model γ · + ( 1 − γ ) · Substrate network FIG . 2 . Memory eﬀects shape the learning of networks by humans . ( a ) The α eﬀect corresponds to random forget - ting of learned network edges with probability α per random walk step . ( b ) The β eﬀect corresponds to random shuﬄing of memories : the destination of a random walk step is always the correct node t + 1 but the origin is confused between t , t − 1 , t − 2 etc . , with decaying probability . ( c ) The γ eﬀect corre - sponds to the reinforcement of memories : the random walker mixes the existing memories of weight γ with the substrate network of weight ( 1 − γ ) . which treat one of the matrices as a set of real numbers and another as a binary ﬁlter . In addition to the memo - ries of edges , we can compute the memories of visitation of nodes by performing column - summation (cid:80) j M ij , for which metrics can be deﬁned similarly . The goal of our investigation is to understand how the learning metrics depend on the progress through the book τ , the dilation D , and the memory eﬀects described below . The mathe - matical details of these whole - network metrics are given in Appendix C . C . Memory eﬀects Human memory is an associative , distributed psycho - logical function , the purpose of which is not to provide a perfect replica of past events , but to extract the gist , produce abstractions and inferences form the observed data , and create an overall generalizable model of the world [ 22 , 25 , 28 ] . In order to mimic the features of this model - making , we consider memory acquisition to be a digital process of adding integer memory counts , modi - ﬁed by three diﬀerent eﬀects which we parameterize with α , β , and γ ( precise deﬁnitions given later ) . The pres - ence of each of these eﬀects has separately been conﬁrmed in human behavioral experiments , but generically we ex - pect them to be present all at the same time . Each of the eﬀects is known to vary in magnitude , which justiﬁes studying a range of α , β , γ parameter values . While the three eﬀects can have similar bearing on the learning metrics , they act through orthogonal mech - anisms ( Fig . 2 ) . The α eﬀect corresponds to forgetting of encountered network edges or conceptual transitions ( Fig . 2a ) [ 55 – 57 ] . The β eﬀect corresponds to a tempo - ral shuﬄing or random reordering of encountered transi - tions in memory ( Fig . 2b ) [ 24 ] . The γ eﬀect corresponds to reinforcing the random walk by the memories of prior transitions ( Fig . 2c ) [ 10 , 13 ] . All three eﬀects can be present at the same time and interact with each other . While other memory eﬀects can exist as well , focusing on these three both establishes an important intuition for mental model distortion and develops transferable math - ematical techniques . Importantly , the eﬀects are heavily compounded by the ﬁnite length of any learner’s random walk ( ﬁnite dilation D ) , which leads to a marked under - sampling of the nodes and edges . D . Mathematical formalisms In order to disentangle these three eﬀects and the ef - fect of under - sampling , we utilize three mathematical lenses of analyses . First , we use a mean ﬁeld theory that assumes a fully equilibrated random walk on the net - work and that captures long - time network statistics and distortions , akin to Refs . [ 24 , 28 ] , but cannot account for under - sampling eﬀects . Second , we perform direct stochastic simulations of random walks on dynamic net - works , which sample both the random walk steps and the memory eﬀect realizations ( details of the simulations are given in Appendix B ) . Third , we perform compu - tations using exposure theory , as derived and validated in Ref . [ 52 ] , which gives closed - form approximations for under - sampling eﬀects ( a primer on exposure theory is given in Appendix D ) . The goal of exposure theory is to provide analytic expressions for the probability distributions of memory counts M ij . Under certain approximations , these dis - tributions take Poisson shape parameterized by a single exposure value E ij ( t ) for each edge . The Poisson dis - tributions of memory counts and the rules for event ag - gregation deﬁne an eﬀective statistical mechanics frame - work of random processes on complex networks . Tracking the deterministic evolution of the exposure value oﬀers a 5 0 . 0 0 . 5 1 . 0 Exposition time 0 . 0 0 . 5 1 . 0 N o d e r eca ll R n D = 1 0 D = 1 . 0 D = 0 . 1 a R = / Treil 0 . 0 0 . 5 1 . 0 Exposition time 0 . 0 0 . 5 1 . 0 E d g e r eca ll R e D = 1 0 D = 1 . 0 D = 0 . 1 b Mean field Stochastic Exposure 0 2 4 6 8 10 Dilation D 0 0 . 9 0 . 99 N o d e r eca ll R n R n = 1 c Jensen Axler Greub 0 2 4 6 8 10 Dilation D 0 0 . 9 E d g e r eca ll R e d Axler Greub FIG . 3 . Finite - time learning leads to incomplete node and edge exploration in textbook networks . ( a - b ) Node and edge recall R n , R e of the random walker on the Treil textbook network . Red curves show 10 replicas of stochas - tic simulations at each dilation ; green dashed curves show the exposure prediction . At higher dilation D both recall curves follow the mean ﬁeld curve ( blue ) more closely than at lower dilation . ( c - d ) Node and edge recall by the end of the two textbooks , asymptotically approaching 1 . For the Greub textbook , node recall discontinuously jumps to R n = 1 at ﬁ - nite D . The blue shading shows the Jensen bound on learning speed , its straight boundary on a semi - log plot corresponds to an exponential approach to R = 1 . speed - up by many orders of magnitude of computational time . While the original formulation of exposure theory in Ref . [ 52 ] assumed that each random walk step follows the substrate network and is remembered correctly and in perpetuity , here we derive extensions that account for the eﬀects of forgetting , shuﬄing , and reinforcement ( see Appendix D ) . III . DILATION AND UNDER - SAMPLING A . Global exploration First we consider the under - sampling of the network both along the exposition time of the book and by the end of it . The random walk samples from the nodes and edges present in the network at a given time . The fraction of nodes and edges learned can be measured by the recall metrics R n and R e , or the fraction of learned nodes and edges with respect to all current and future ones ( Fig . 3a inset ) . As established previously [ 36 ] , the nodes and edges are introduced sublinearly throughout the text : faster initially , and slower by the end . This introduction rate then sets the upper limit on exploration by a mean - ﬁeld random walk ( Fig . 3a , b ) . The degree to which recall follows the mean - ﬁeld limit depends on the dilation D , or the length of the random walk that the learner takes upon the network . We con - trast the number of nodes and edges recalled by high versus low D learners to model extensive versus light study habits . For high values of dilation D = 10 the stochastic trajectories of R n lie just below the mean ﬁeld one , such that any newly introduced node is quickly dis - covered ( Fig . 3a ) . In contrast , for low values of dilation D = 0 . 1 the stochastic trajectories lag far behind the mean ﬁeld : barely half of the nodes are discovered by the end of the book . Turning from recall of nodes to recall of edges , we ﬁnd that stochastic trajectories fall even further behind the mean ﬁeld predictions ( Fig . 3b ) : every random walk step can discover at most one node and one edge , but there are many more edges than nodes and thus edge learning is slower [ 17 ] . With growing dilation , mental models approach com - plete recall ( R n = R e = 1 ) for both nodes and edges , but the convergence is slow ( Fig . 3c - d ) . For the Greub textbook the node recall discontinuously jumps to R n = 1 near D = 6 , whereas for the Axler textbook we do not see this jump within the plotted range . These two distinct behaviors suggest that there exists some diﬀerence be - tween the two textbook networks . In Ref . [ 52 ] we showed that average network learning speed is limited from above by the Jensen bound , which can only be saturated by un - weighted networks . For all textbooks the stochastic and exposure recall grows much slower than allowed by the bound ( Fig . 3c - d and Appendix E ) . Despite the eﬀorts of textbook authors , the learning of nodes and edges is seemingly very ineﬃcient when measured in the aggre - gate , and diﬀers between textbooks . What causes the slowdown of learning and what drives the diﬀerence be - tween the textbooks ? B . Local learning of nodes and edges In order to explain the learning slowdown and its vari - ance across textbooks , we consider the textbook network structure at a more granular level of individual nodes and edges . On one side , the ﬁnal textbook network can be described by the static metrics of node strengths s i ( weighted degrees ) and edge weights A ij . On the other side , the dynamics of both network growth and random walks on the network require dynamic metrics . In ex - posure theory , each node and edge deterministically ac - cumulates exposure , which is predictive of the stochastic number of memories of that node or edge [ 52 ] . For nodes , at a given dilation D , the node integral exposure by the end of the textbook is just K i = D · K i , where the node speciﬁc exposure K i depends on the network evolution trajectory but not the learner . Similarly for edges—the edge integral exposure is E ij = D · E ij , where E ij is the 6 10 0 10 1 10 2 10 3 10 - 1 10 0 10 1 10 2 10 3 Node strength s i N o d e s p ec . e x p o s u r e K i ρ = 0 . 90 a Axler 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 79 b Greub 10 0 10 1 10 2 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 Edge weight A ij E d g e s p ec . e x p o s u r e E i j ρ = 0 . 79 c 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 65 d Learning cutoff D = 0 . 1 D = 1 D = 10 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( n o d e v i s i t ) a t D = 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( e d g e v i s i t ) a t D = 1 . 0 FIG . 4 . Node and edge exposure predicts learning across orders of magnitude of dilation . ( a - b ) Scatter plots of network nodes measured by speciﬁc exposure and strength . ( c - d ) Scatter plots of network edges measured by speciﬁc exposure and edge weight . Marker color corresponds to the probability of a node or edge learned across 10 stochas - tic replicas at D = 1 . 0 . Horizontal dashed lines indicate the boundary of node and edge learning at diﬀerent values of the dilation D . Note that all of the Greub nodes lie above the D = 10 line , but some of the Axler nodes lie below the D = 10 line . In each panel ρ is the Spearman correlation coeﬃcient , log 10 ( p ) < − 12 , between the x - and y - axis variables . edge speciﬁc exposure ( see Appendix D for derivations ) . How do these static and dynamic metrics help us to un - derstand granular network learning ? The network nodes span several orders of magnitude by exposure and some are reliably learned across many stochastic simulation runs while others are not ( Fig . 4a - b ) . Each node is learned with high probability for K i > 1 , or , put diﬀerently , the threshold line K i = 1 / D serves as a linear classiﬁer separating the learned from the not learned nodes . As dilation gets higher , corresponding to more extensive study , the threshold moves lower , so that more and more nodes end up above the threshold and are learned . The distribution of nodes by speciﬁc exposure diﬀers between the books : whereas all of Greub nodes ( Fig . 4b ) are learned by D = 10 , not all of the Axler nodes are learned ( Fig . 4a ) . By comparison , the static metric of node strength s i is not as predictive of learning as the dynamic metric , even though it is strongly corre - lated with the node speciﬁc exposure . Exposure theory thus gives a very granular prediction of node learning . We conduct a similar analysis for network edges , though there are many more edges than nodes , and edge speciﬁc exposure E ij spans more orders of magnitude than node speciﬁc exposure K i . Just like nodes , edges are learned with high probability for E ij > 1 and thus the threshold line E ij = 1 / D is a good linear classiﬁer . Unlike for nodes , a dilation of D = 10 is not suﬃcient to learn all of the edges . The diﬀerence is especially striking for the weakest connections A ij = { 1 , 2 , 3 } , for which speciﬁc di - lation ranges from 10 − 4 to 10 0 . If these very weak edges are introduced early in the textbook , then they can ac - cumulate enough exposure to be learned . The later they are introduced , the less time they have to accumulate ex - posure , and the more other edges they compete with for exposure . While strong edges are likely to be discovered by all learners , most of the weak edges are unlikely to be discovered even by the most thorough learners . The network heterogeneity by speciﬁc exposure of nodes and edges serves as a mechanism to prioritize some concepts and connections over others to ensure that they are learned by all , even the most cursory of learners with low D . For static networks , the speciﬁc exposure of nodes is directly proportional to their strength K i ∝ s i and the speciﬁc exposure of edges is strictly proportional to their weight E ij ∝ A ij [ 52 ] ; for dynamic networks like the ones presented here the speciﬁc exposure is a time integral of either node strength or edge weight and thus the strict proportionality reduces to a strong Spearman correlation ( ρ in Fig . 4 ) . In other words , the key mechanisms of pri - oritization are to mention a concept or a connection early on and repeat it frequently in the text . We showed that globally network learning is much slower than allowed by the Jensen bound , but locally the network nodes and edges vary widely by priority of learning . Are the slowdown and the prioritization con - nected ? From exposure theory we know that the average learning depends on the distribution of nodes and edges by exposure rather than their absolute exposure values [ 52 ] . The Jensen bound for node learning is saturated when all nodes have the same speciﬁc exposure , while for edge learning it is saturated when all edges have the same speciﬁc exposure . For static networks , that sce - nario would correspond to regular and unweighted net - works , respectively . Globally , such networks would have the fastest average learning . However , locally such net - work learning would be very unpredictable : in regular networks all nodes are equally likely to be learned , and in unweighted networks all edges are equally likely . If independent learners sample from such networks , their mental models would not have the same priority and thus would be markedly diﬀerent . In other words , network heterogeneity is simultaneously the cause of prioritiza - tion and slowdown , which can only appear together . IV . THE α EFFECT Having established the basic intuition of under - sampling due to ﬁnite dilation , we now consider the α eﬀect : stochastic forgetting . At each step of the random 7 α = 0 . 01 D = 10 α = 0 . 001 D = 1 0 1 α = 0 0 1 D = 0 . 1 Mean field Exposure Stochastic 0 1 0 1 0 1 0 1 Edge recall R e N o d e r eca ll R n a 10 - 2 10 - 1 10 0 10 1 Dilation D 10 - 2 10 - 1 10 0 10 1 E ff . d il a t i o n D α α = 0 . 01 α = 0 . 001 α = 0 b Treil 10 - 1 10 0 10 1 Effective dilation D α 0 0 . 9 0 . 99 R eca ll R N o d e s R n E d g e s R e c FIG . 5 . Forgetting reduces the eﬀective dilation and reduces learning . ( a ) Learning trajectories in ( R e , R n ) space at varying dilation D ( rows ) and forgetting α ( columns ) . The red cross marks the ﬁnal position of learning , averaged over 10 replicas . ( b ) Conversion of true dilation D to eﬀective dilation D α . At any ﬁnite forgetting α the eﬀective dilation plateaus . ( c ) Eﬀective dilation drives the data collapse of ﬁnal recall of nodes and edges at varying parameter combinations α , D . Each red cross corresponds to one of the parameter regimes from panel ( a ) . walk , precisely one new memory is added to the memory matrix M . At the same time , every single memory has a small uniform chance α of being forgotten per step , which leads to an exponential distribution of memory lifetimes , consistent with empirical measurements of forgetting [ 55 – 57 ] . The forgetting caps the number of memories that the walker can hold at around (cid:80) ij M ij (cid:39) 1 / α with small ﬂuctuations . In order to showcase the interaction of dilation with forgetting , we next systematically vary both ( Fig . 5a ) . For stochastic simulations , we explicitly draw random re - alizations of forgetting every step , whereas for exposure computations we add a decay term for integral exposure dynamics ( see Appendices B , D ) . All of the stochastic tra - jectories have higher recall of nodes R n than edges R e . Without forgetting ( α = 0 ) the trajectory can get close to full recall for large enough dilation . In contrast , for high forgetting ( α = 0 . 01 ) the walker can only remember the last ∼ 100 transitions , regardless of how long the walk was , and thus the learning trajectories look identical and terminate at fairly small recall . Forgetting thus severely limits the amount of memory available to the learner and the quality of mental models that they can form . In order to compare directly the ﬁnal recall of walkers at diﬀerent dilation D and forgetting α , we perform a data collapse by combining the two variables into a single eﬀective dilation : D α = 1 ατ max (cid:0) 1 − e − ατ max D (cid:1) , ( 3 ) where τ max is the number of sentences in a particular textbook . For small dilation D the memory is dominated by learning so eﬀective dilation tracks the actual dilation , whereas for large dilation D the memory is dominated by forgetting and the total memory count plateaus at ∼ 1 / α ( Fig . 5b ) . This data collapse of eﬀective dilation allows us to accurately predict node and edge recall across a wide range of actual dilation and forgetting ( Fig . 5c ) . We thus showed that recall is driven by the number of memories accumulated , which is limited by any amount of forgetting . V . THE β EFFECT A . Shuﬄing and precision The second eﬀect we consider is the temporal shuf - ﬂing of stimuli , which is characteristic of human mem - ory processes . We follow the model established and ex - perimentally validated in Ref . [ 24 ] and applied elsewhere [ 28 , 31 , 58 ] . We brieﬂy recap it here . A human subject at - tempts to learn the network structure from the sequence of nodes x ( t ) visited at each time step . In the absence of shuﬄing , each step adds a memory count to the entry M ij for i = x ( t ) , j = x ( t + 1 ) . However , remembering precisely the history of previously visited nodes requires signiﬁcant mental resources ; mental errors in recall are likely so that i = x ( t − ∆ t ) . The need to minimize errors ∆ t is balanced in the brain with the need to minimize computational complexity [ 59 ] . This trade - oﬀ can be ex - pressed via the free energy principle , which predicts a geometric distribution of error sizes : p ( ∆ t ) = ( 1 − e − β ) e − β ∆ t , ( 4 ) as illustrated in Fig . 6a for diﬀerent values of the shuf - ﬂing parameter β ∈ [ 0 , ∞ ) . Ref . [ 24 ] proposed a way to measure the value of β experimentally and found that for diﬀerent human respondents it can be inﬁnite ( perfect memory ) , zero ( full shuﬄing ) , or any ﬁnite value ( partial shuﬄing ) . Therefore , in our computational model of the 8 0 1 2 3 4 5 6 Shuffle time ∆ t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r o b a b ili t y a β = ∞ β = 1 . 0 β = 0 . 2 Learned network real predictedspurious b Treil 0 . 0 0 . 5 1 . 0 Exposition time 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r ec i s i o n P D = ∞ D = 10 D = 1 . 0 D = 0 . 1 β = 0 . 2 c Mean field Stochastic Exposure 10 - 2 10 - 1 10 0 10 1 Shuffling β 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 2 D = ∞ D = 10 D = 1 . 0 D = 0 . 1 d P = / FIG . 6 . Memory shuﬄing leads to reduced mental model pre - cision . ( a ) Each new memory connects the current node with one visited ∆ t steps into the past , drawn randomly from a geometric distribution p ( ∆ t ) ∝ e − β ∆ t . ( b ) At an interme - diate time the learned network consists of real edges ( light green ) , predicted edges that would appear in the future ( dark blue ) and spurious edges ( light blue ) . Only a small fragment of the network is shown for illustration . ( c ) Mental model precision P throughout the exposition time at low shuﬄing β and varying dilation D from the mean ﬁeld model ( blue ) , 10 stochastic simulation replicas ( red ) , and exposure theory ( green ) . ( d ) Mental model precision across orders of magni - tude of shuﬄing β and varying D . The vertical line indicates the value β = 0 . 2 used for panel ( c ) . human learner , we need to consider a wide range of β , including the limiting behaviors β → 0 and β → ∞ . The β model sets out the rules for the generation of erroneous memories from observing random walks on net - works , but our investigation of mental errors is compli - cated in several ways with respect to previous results . First , we consider random walks on time - dependent net - works . As shown in Fig . 6b , partway through learning a textbook network , some of the learned edges are real , whereas some are predictions of edges that would be - come real at a later point , and some are truly spurious . Second , we study the combination of mental errors with the under - sampling eﬀect of ﬁnite - time random walks , as opposed to the inﬁnite - time limit of Ref . [ 24 ] . Both of these complications are addressed in our simulations as described in Appendix B and exposure theory as de - scribed in Appendix D . We ﬁrst address the precision of learning real edges in the presence of shuﬄing ( Fig . 6c ) . Throughout the exposition time , the precision remains nearly constant , but depends signiﬁcantly on dilation D . As shown in Appendix D , the degradation of precision is mostly ex - plained by failing to visit some of the nodes revealed by the textbook network , thus leading to the lack of any mental model of transitions out of those nodes . At lower precision D = 0 . 1 the under - sampling of nodes becomes particularly notable , leading to signiﬁcant noise in the stochastic precision curve , and an over - estimation of the curve by exposure . Once the nodes have been visited , mostly by a dilation value of D = 10 , the precision fol - lows the mean - ﬁeld trajectory . Once a student is exposed to all of the concepts , after extensive study , they learn the network with the precision predicted by mean ﬁeld theory . How do increasing mental errors lead to loss of preci - sion ? At high β → ∞ ( near - perfect memory ) and high dilation D all nodes have been visited and only correct transitions are remembered ; thus precision approaches 1 ( Fig . 6d ) . At β → 0 ( full shuﬄing ) , the remembered edges randomly connect all remembered nodes ; of all possible edges , many are real edges , and thus precision plateaus at some ﬁnite value 0 < P < 1 . Between the two extremes , the precision changes smoothly and monoton - ically ; thus there is no “optimal” or “threshold” amount of memory shuﬄing . A learner with high shuﬄing would still learn all concepts , and all real connections— although along with all possible spurious connections . B . Edge prediction In the next step of our investigation , we consider the prediction of future edges , which is inevitably a tran - sient phenomenon . At the start of learning , there are no memories formed yet of either real or future edges , while by the end of learning , there is no learnable fu - ture . Therefore , all variation of prediction trajectories happens at intermediate times ( Fig . 7 ) . The trajectory shape is heavily modulated by the introduction of nodes over time ( Fig . 3a , b ) since a shuﬄed random walk can only learn edges between the nodes that have already been introduced . Similar to the learning of real edges , the learning of future edges ( precision ) can be characterized by either precision or recall ( insets in Fig . 7a - b ) . The precision of prediction is the fraction of mental model probability weight that corresponds to future edges ( as opposed to real and spurious edges ) . Compared to the recall of real nodes and edges ( Fig . 3a - b ) , the stochastic trajectories of precision of prediction show a much wider variation around the mean ﬁeld and exposure curves . Unlike the recall of real edges , the precision of prediction is a fraction of two random numbers ; thus the mean ﬁeld curve does not serve as an upper bound , but merely an average tra - jectory . During exposition time , the precision of predic - tion has an early peak and a gradual fall - oﬀ ( Fig . 7a ) . At early times , a fairly small fraction of nodes N ( t ) has been introduced ; thus the random walk memories are conﬁned to relatively few N ( t ) 2 possible edges . Even among those nodes , many edges have not been introduced yet ; thus a 9 0 . 0 0 . 5 1 . 0 Exposition time 0 . 0 0 . 1 0 . 2 P r ec i s i o n o f p r e d i c t i o n P p r β = 0 . 2 a P pr = / Mean field Stochastic Exposure 0 . 0 0 . 5 1 . 0 Exposition time 0 . 0 0 . 1 R eca ll o f p r e d i c t i o n R p r β = 0 . 2 b R pr = / β = 10 D = 10 β = 0 . 5 D = 1 0 . 0 0 . 2 β = 0 . 2 0 . 0 0 . 1 D = 0 . 1 0 . 0 0 . 2 0 . 0 0 . 1 0 . 0 0 . 1 0 . 0 0 . 2 Recall of prediction R pr P r ec i s i o n o f p r e d i c t i o n P p r c 10 - 2 10 - 1 10 0 10 1 Shuffling β 0 . 0 0 . 1 0 . 2 M a x p r e d i c t i o n m e t r i c s M ea n f i e l d D = 10 D = 1 D = 0 . 1 d P pr R pr Realedges Triangleclosure Not closure 10 - 3 10 - 2 10 - 1 10 0 10 1 S p ec i f i c e x p o s u r e E D = 0 . 1 D = 1 D = 10 e Real Predicted Spurious 0 . 0 0 . 1 0 . 2 Precision of prediction P prmax 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r ec i s i o n o f r ea l e d g e s P M ea n f i e l d D = 10 D = 1 D = 0 . 1 β = 0 . 1 β = 2 β = 0 . 1 β = 2 f Treil FIG . 7 . Memory shuﬄing allows for limited prediction of future edges . ( a ) Trajectory of precision of prediction P pr with early peak and slow descent from mean ﬁeld model ( blue ) , 10 stochastic simulation replicas ( red ) , and exposure theory ( green ) . ( b ) Trajectory of recall of precision R pr with slow rise and rapid drop . The mean ﬁeld curve ( blue ) is the limiting behavior at D → ∞ but requires extremely high values of D to approach . ( c ) Prediction trajectories in ( R pr , P pr ) space across dilation D and shuﬄing β . ( d ) Temporal maximum of prediction precision ( solid curves ) and recall ( dotted curves ) at varying shuﬄing β . The mean ﬁeld temporal maximum of recall is 1 for any ﬁnite β ( not shown ) . ( e ) Scatter plot of learned edges by speciﬁc exposure at exposition time τ = 0 . 6 τ max and shuﬄing β = 0 . 2 . Colors indicate real , predicted , and spurious edges , while positions indicate real edges , triangle closures of real edges , and non - closures . Horizontal dashed lines indicate the boundary of learning at diﬀerent values of the dilation parameter D . ( f ) Trade - oﬀ between the temporal maximum of precision of prediction P prmax and the ﬁnal precision of real edge learning P . The upper - left end of the curves corresponds to β → ∞ , whereas the lower - right end corresponds to β → 0 . large fraction of probability weight falls on future edges , making them easy to discover and leading to the early peak . At later times , there are both more possible edges across which probability is spread and fewer future edges on which probability would be useful ; together , these two factors result in a dwindling precision of prediction . The recall of prediction R pr is the fraction of all future edges learned and has the opposite trajectory shape : a gradual increase and a sharp drop - oﬀ ( Fig . 7b ) . On one side , as more nodes and edges are introduced , they in - crease the range of edges that can be discovered by mem - ory shuﬄing . On the other side , as the exposition time advances , more and more future edges become current edges , and the denominator of recall decreases , leading to the late peak . Compared to the precision of predic - tion P pr , the stochastic trajectories follow exposure the - ory curves much more closely , but yet are very far below the mean ﬁeld trajectory . In the mean ﬁeld limit , any new edge between existing nodes can be predicted , but the likelihood of such a prediction at any ﬁnite dilation D is extremely small . Since the recall R pr and precision P pr of prediction follow opposite trends , they form a closed loop , starting and ending at zero and exhibiting a nearly linear dy - namic trade - oﬀ in between ( Fig . 7c ) . With advancement through the text , precision is traded for recall but at a steep rate ( note the diﬀerence in scale on the two axes ) . As a learner starts reading the textbook , they ﬁrst allo - cate a sizable part of their mental model to the prediction of future edges , but only end up covering a small fraction of them . How large can the precision and recall get throughout 10 the book ? Is there a value of shuﬄing β that optimizes prediction ? To answer these questions , we study the maximal prediction values and ﬁnd that prediction gets monotonically higher with increasing shuﬄing or lower β ( Fig . 7d ) . For precision P pr the curve at D = 10 closely follows the mean ﬁeld curve since the under - sampling is mostly driven by the unvisited nodes . In contrast , for recall R pr , the mean ﬁeld curve is essentially always at 1 , far above the plotting limits . Thus the dilation re - quired to saturate the prediction recall seems markedly high . How does one reconcile the fact that the precision of prediction saturates by D ∼ 10 with the fact that the recall does not ? The most common shuﬄing mistake in the model of human learning is confusing a random walk of length 2 for a random walk of length 1 , i . e . ∆ t = 1 or triangular closure . Therefore , at an intermediate point of the text - books , we can separate all edges into three topological types : real edges , triangular closure of real edges , and all others ( three point clouds in Fig . 7e , see Appendix B for deﬁnition ) . At the same time , the non - real edges are ei - ther predicted or spurious ( dark and light blue in Fig . 7e ) . The triangular closure edges have notably higher spe - ciﬁc exposure and can thus be reasonably discovered at D ∼ 1 , thus allocating a sizable fraction of mental model probability to future edges . In contrast , the non - closure edges only start being visible at D ∼ 10 . Since some of the future edges are not triangular closures , predict - ing them would require extremely high dilation D > 10 3 . This pattern of speciﬁc exposure of edges stratiﬁed by topological types explains why getting a substantial pre - cision of prediction is easy , but getting a high recall is unlikely and should not be relied upon . Lastly , how eﬃcient is the trade - oﬀ between precision on real edges and prediction of future edges ? The mental model probability is split between real , predicted , and spurious edges , but in what proportion ? We ﬁnd this trade - oﬀ to be essentially linear but limited ( Fig . 7f ) . At high values of β learning is precise and lies in the top - left corner . At low values of β learning is fully shuﬄed , and both metrics approach ﬁnite values that are dependent on the density of real and future edges . For the Treil textbook , as shown here , the trade - oﬀ between P and P prmax is about 3 : 1 , but gets even steeper at lower dila - tion ( other textbooks have a similar pattern ; see Fig . 13 ) . The memory shuﬄing thus aﬀords learners a limited abil - ity to predict future edges at the cost of precision of real edges . VI . THE γ EFFECT A . Reinforcement and slowdown The third eﬀect we consider is the random walk re - inforcement , which makes already existing memories stronger . While the α and β eﬀects only modify the way memories behave , the γ eﬀect changes the actual ran - dom walk steps . The γ eﬀect accounts for the tendency to revisit the same edges that one remembers , inspired by models in Refs . [ 10 , 13 ] . While in those studies the underlying weights of the transition matrices were mod - iﬁed , here we instead ﬁx the probability of taking a step on the textbook network or the mental model . Mathe - matically , if the learner is on a node i with pre - existing memories , they choose their next step with probability : P ( j | i ) = ( 1 − γ ) T ij ( τ ) + γ ˆ T ij , ( 5 ) where T ij is the transition matrix of the textbook , ˆ T ij is the mental model of transitions , and γ ∈ [ 0 , 1 ] is the mixing factor . At γ = 0 the random walk follows the textbook network , whereas at γ = 1 the random walk exclusively retraces the existing edges if any are remem - bered . Hence , the γ parameter regulates the degree of positive feedback , since following known edges creates more memories of those edges , and makes them more likely to be traversed again . How exactly does reinforcement aﬀect network explo - ration and mental model building ? Are the mental mod - els built by diﬀerent learners consistent with each other ? It is important to recognize the space of possible mental models as high dimensional . While in a one - dimensional dynamical system with positive feedback the state vari - able just grows , in the high - dimensional space of possible mental models ˆ T ij , the early random walk steps select random edges from the textbook network , and the later steps reinforce the memories of those edges . The com - bination of early random selection with positive feed - back results in many diﬀerent transition networks ˆ T ij formed in independent stochastic replica simulations at varying levels of reinforcement γ ( Fig . 8a - c ) . As γ gets higher , the resulting networks get less similar to each other and get more high - probability edges ( purple ar - rows for ˆ T ij > 0 . 4 ) . In order to compare the independent replica runs , we use an overlap metric inspired by the spin glass literature [ 60 ] : Q ab = 1 m (cid:88) ij [ T ij > 0 ] [ ˆ T aij > 0 ] [ ˆ T bij > 0 ] , ( 6 ) where m is the number of directed edges in the text - book network and the superscripts a , b correspond to the replica indices . When the indices are the same , the self - overlap Q aa measures how many edges are shared be - tween the taught network and the learned mental model . When the indices are diﬀerent , the cross - overlap Q ab ac - counts for the overlap between two replicas . The self - and cross - overlap remain consistent across replicas but diﬀerent from each other ( heatmaps in Fig . 8a - c ) . As γ increases , both overlap metrics decrease ( more pale color on the heatmap ) : that is , reinforced random walks ex - plore less of the taught network , and build mental models less similar to each other . How signiﬁcant is the drop in exploration and over - lap caused by a growing value of reinforcement γ ? The only way for the learner to discover new edges is to take 11 γ = 0 . 2 β = ∞ a Q aa Q ab γ = 0 . 5 b Q aa Q ab γ = 0 . 8 c Q aa Q ab β = 0 . 2 e Q aa Q ab f Q aa Q ab g Q aa Q ab 10 - 2 10 - 1 10 0 O v e r l a p Q d γ = 0 . 0 γ = 0 . 2 γ = 0 . 5 γ = 0 . 8 Edwards Q aa Q ab 10 - 1 10 0 10 1 Eff . dilation D γ = D ( 1 − γ ) 10 - 2 10 - 1 10 0 O v e r l a p Q h γ = 0 . 0 γ = 0 . 2 γ = 0 . 5 γ = 0 . 8 Q aa Q ab 0 . 02 0 . 1 0 . 5 Overlap Q 0 . 02 0 . 1 0 . 5 Overlap Q 0 . 02 0 . 1 0 . 5 Overlap Q FIG . 8 . Reinforcement of random walks causes a slowdown of exploration , but shuﬄing mitigates it . ( abc , efg ) Illustration of learned transition networks at varying reinforcement γ ( columns ) and shuﬄing β ( rows ) , but identical dilation D = 1 . 0 . Arrow transparency indicates the transition probability ˆ T ij , with most likely edges ˆ T ij > 0 . 4 shown in purple . The heatmap shows the overlap matrix Q of 10 replicas at given values of the γ and β parameters . Matrix diagonal corresponds to self - overlap Q aa , oﬀ - diagonal corresponds to cross - overlap between the replicas Q ab . ( d , h ) Overlap metrics plotted against eﬀective dilation D γ = D ( 1 − γ ) , for simulations performed only at two values of dilation D = 10 0 , 10 1 . Self - overlap is shown by the upper marker in each pair for simulations and the solid curve for exposure prediction . Cross - overlap is shown by the lower marker in each pair for simulations and the dashed curve for the exposure prediction . steps along the textbook network , which happens with probability ( 1 − γ ) ( note that some steps along the text - book network still retrace older memories ) . We thus hy - pothesize that exploration statistics at dilation D and reinforcement γ , as measured by overlap Q ab , would fol - low the un - reinforced statistics at lower eﬀective dilation D γ = D ( 1 − γ ) . For the un - reinforced statistics , exposure theory predicts both overlap curves shown in Fig . 8d . In order to test the eﬀective dilation hypothesis , we perform reinforced random walk simulations at varying γ but only two values of dilation D = 10 0 , 10 1 and compute the self - and cross - overlap statistics . By converting each pair of parameters into a single parameter D , γ → D γ , the over - lap data collapse onto the curve predicted by exposure theory ( Fig . 8d ) . We thus show that reinforced random walks slow down exploration of the networks and make less consistent mental models , both in proportion to re - inforcement . The slowdown eﬀect relies on reinforcement of the revisited edges , which requires remembering them cor - rectly . However , it is possible that the learner would follow their own memories , but not form memories cor - rectly : that is , the γ eﬀect can coexist with the β eﬀect . In order to test whether the β eﬀect breaks the feedback loop , we perform simulations that simultaneously take into account reinforcement and memory shuﬄing . The mental models appear more consistent with each other , with fewer strong ( purple ) edges emerging ( Fig . 8e - g ) . The overlap heatmaps still get paler with growing γ , but the eﬀect is much less pronounced . For simulations at D = 1 . 0 and γ > 0 the overlap metrics are lower than at γ = 0 but higher than predicted by eﬀective dila - tion ( Fig . 8h ) . Memory shuﬄing thus can partially miti - gate the reinforcement slowdown by breaking the positive feedback loop . A student with these two mental eﬀects would thus be able to discover additional edges compared to exploration with reinforcement alone . B . Symmetry breaking We previously noted that reinforcement can produce very strong edges in the mental model ( Fig . 8c ) . At the same time , these edges are not reciprocal : while the un - derlying textbook network is undirected and thus is de - scribed by symmetric matrices , the learned mental model 12 - 20 - 10 0 10 20 A sy mm e t r y M i j − M j i γ = 0 a Edwards γ = 0 . 2 b γ = 0 . 5 c γ = 0 . 8 β = ∞ d 10 - 3 10 - 2 10 - 1 10 0 10 1 Spec . exposure E ij - 20 - 10 0 10 20 A sy mm e t r y M i j − M j i e Exposure std Stochastic std 10 - 3 10 - 2 10 - 1 10 0 10 1 Spec . exposure E ij f 10 - 3 10 - 2 10 - 1 10 0 10 1 Spec . exposure E ij g 10 - 3 10 - 2 10 - 1 10 0 10 1 Spec . exposure E ij β = 0 . 2 h FIG . 9 . Reinforcement causes edge reciprocity breaking beyond the exposure null model . Columns correspond to varying reinforcement γ , rows to shuﬄing β . The null model mean diﬀerence is always zero , while green dashed curves show the standard deviation predicted by exposure . The red curve shows the stochastic mean , while the red shading shows the stochastic standard deviation over 100 replicas . All simulations and analytical computations are performed at dilation D = 1 . 0 . appears to be breaking symmetry signiﬁcantly . In order to quantify the degree of symmetry breaking , we must ﬁrst establish the null expectation . Exposure theory pre - dicts the number of memories to follow the Poisson dis - tribution : M ij = Pois ( D E ij ) , where the exposure matrix is symmetric . The memory counts in an edge M ij and its reciprocal M ji thus follow identical distributions , but are independent from each other . The asymmetry M ij − M ji is then a random number with a mean of zero and a variance twice that of each edge , i . e . 2 D E ij . As speciﬁc exposure of edges spans nearly ﬁve orders of magnitude across the set of textbooks , the expected asymmetry for edges also varies signiﬁcantly . Increasing the reinforcement γ signiﬁcantly changes the statistics of asymmetry . Without reinforcement at γ = 0 , asymmetry statistics fall within the standard de - viation envelope ± (cid:112) 2 D E ij predicted by exposure theory ( Fig . 9a ) . As reinforcement γ increases , the asymmetry leaves the envelope both above and below zero so that the symmetry of the transition matrix can be signiﬁcantly broken in either direction ( Fig . 9b - d ) . But what if the random walk steps are not remembered correctly , for in - stance if the β eﬀect ( shuﬄing ) is also present ? In this case , the asymmetry eﬀect reduces signiﬁcantly and the statistics mostly ﬁt within the predicted envelope at all values of γ ( Fig . 9e - h ) . Memory shuﬄing is thus able to mitigate the positive feedback loop caused by reinforce - ment . A student with shuﬄed memories thus does not falsely infer strong directionality of conceptual connec - tions that is absent in the textbook network . VII . DISCUSSION In this paper we set out to describe the translation of taught semantic networks into learned ones , sculpted by the dual forces of ﬁnite learner eﬀort and the speciﬁc eﬀects of human memory . Our conclusions about the role of the three memory eﬀects can be readily related to other studies , as shown below . At the same time , converting teaching into learning is the primary goal of education , for which our study provides a missing link . Lastly , this work substantially expands upon exposure theory and allows us to map out the technical limitations and further open avenues . A . Memory eﬀects Alongside under - sampling , we also account for several memory eﬀects reported in the literature . The ﬁrst ef - fect investigated is the α eﬀect , which corresponds to a uniform forgetting rate of all formed memories . While the forgetting process is not biased towards some nodes and edges over others , the less connected nodes and weaker edges would naturally have fewer memory counts 13 accrued and thus are more likely to be forgotten com - pletely . The exact time course of forgetting memories has been a longstanding subject of debate , with several em - pirical functional forms—such as exponential , power law , or hyperbolic—having nearly equal data support [ 55 – 57 ] . A uniform forgetting rate corresponds to an exponential forgetting curve , which is thus plausible . This forgetting model leads to the steady - state memory size of about 1 / α , which has several implications . On one side , forgetting limits the memory sample size from which a learner constructs their mental model , and thus puts a strict limit on node and edge recall . The forgetting rate is connected to other cognitive processes such as event segmentation and is diﬀerent across human subjects [ 61 ] ; thus diﬀerent learners are expected to hold diﬀerent amount of memories . On the other side , the memories are restricted to the most recent random walk steps , which in the case of growing textbook networks corresponds to sampling essentially the full network avail - able by the end of the book . The forgetting eﬀect thus appears detrimental if remembering all nodes and edges forever is considered to be a goal and a virtue . How - ever , neurophysiological evidence into the mechanisms of forgetting suggests that moderate forgetting is necessary and expected across many organisms [ 32 ] . Forgetting can have epistemological beneﬁts as it prunes memory of low - importance edges or of edges that do not exist anymore , leading to a clearer and simpler mental model [ 22 ] . The second eﬀect investigated is the β eﬀect , which represents mental errors in the form of shuﬄing . The β eﬀect does not change the number of memories formed , but does aﬀect their placement . In prior work , shuﬄing was found to enhance the relative weight of edges within network communities and decrease the relative weight of edges between communities [ 24 ] . This eﬀect is driven by the diﬀerential leakage of probability from the real within - and between - community edges into the spurious edges , causing the mental model to have a ﬁnite preci - sion . Networks with pronounced community structure have lower leakage , and minimizing leakage is hypoth - esized to be a selection pressure on the architecture of communication networks [ 28 ] . Carefully re - weighing the edges of the input network to emphasize communities and de - emphasize connections between them can to some de - gree mitigate the leakage eﬀect and lead to more precise learning [ 58 ] . The combination of memory shuﬄing and the tempo - ral nature of the textbook networks can lead to learning edges before they are introduced , and thus in eﬀect “pre - dicting” them . Edge prediction is a common problem in network theory : usually a fraction of edges is used to train a computer algorithm that attempts to predict the other , “holdout” edges [ 62 , 63 ] . Some of the algo - rithms themselves rely on a random walk as a local or quasi - local process to score the possible missing edges [ 62 , 64 , 65 ] . The prediction based on shuﬄing is quali - tatively diﬀerent : throughout the random walk , humans automatically and randomly add memories of edges that were not directly traversed , and some of those edges ap - pear in the textbook later ( like those in the “holdout” set ) . Here it is important to clarify that the “prediction” of edges does not follow a scoring algorithm , but is rather guessing . The eﬃciency of such a prediction depends on the authorial choices of the order in which concepts and connections are introduced in a particular book—some textbook might be more predictable than others . Preci - sion and recall of prediction form a dynamical trade - oﬀ throughout the exposition , wherein the precision of pre - dictions gives way to their amount ( recall ) . The temporal peak of prediction forms yet another trade - oﬀ with the precision of real edges , but the conversion ratio is quite steep . While edge prediction is a robust eﬀect , and can to some degree be engineered through text ordering , it is only a secondary beneﬁt that partially compensates for loss of precision . The third eﬀect investigated is the γ eﬀect , or the reinforcement of the random walk by its own memory , which has been examined before . An edge reinforcement model was previously used to explain the strongly sublin - ear statistics and correlations in the discovery of novelties on a network [ 13 ] . A similar model previously explained the tendency of some curiosity - driven Wikipedia readers to return to already known concepts and close the re - maining knowledge gaps instead of exploring new areas [ 10 ] . In our case , adding any amount of reinforcement not only slows down network exploration proportionally , but also leads to spontaneous reciprocity symmetry breaking : the learners infer edge directionality even if the textbooks did not have any . At the same time , adding memory shuﬄing ( as operationalized in the β eﬀect ) allows the learner to still form new memories . Under the combina - tion of these two eﬀects , the learner can revisit known parts of the network but still discover new connections there . The mental model built under reinforcement is not de - termined purely by the textbook network ; instead , it builds upon the random choice of the initial few steps and is thus path dependent [ 66 ] . The idea of path de - pendence ﬁrst became prominent in explaining the posi - tive feedback in economic systems [ 67 ] and since has been fruitfully applied across other social sciences [ 68 ] and es - pecially in the study of persistent institutions [ 69 ] . In mathematical modeling of path dependence , mere enu - meration of possible system states presents a signiﬁcant problem . At the same time , in network models the set of nodes deﬁnes explicitly the set of possible edges that can be learned , making path dependence high - dimensional but still tractable . Within our model we showed that in - creasing reinforcement leads to learners forming less sim - ilar mental models as measured by overlap . Going for - ward , we envision network models to be an especially use - ful platform to study path dependent phenomena more broadly , driven by a variety of other mental eﬀects . Do the memory eﬀects represent “failures” of human learning as compared to automatic computer learners performing optimal inference ? Across the three mem - 14 ory eﬀects , we ﬁnd that deviating from the “ideal case” ( α = 0 , β = ∞ , γ = 0 ) degrades the quality of learning as measured by recall , precision , and overlap , yet most hu - mans deviate from that cognitive regime . Some normal amount of forgetting is argued to be beneﬁcial for de - cluttering our mental models and avoiding overﬁtting to noise [ 22 ] . Persistent shuﬄing of stimuli should degrade the human learning of networks , but instead it serves as a selection pressure on the structure of cognitive networks that humans build in the ﬁrst place [ 28 ] . The tendency to revisit known edges in curiosity - driven exploration is not a limitation of learning but a mere facet of the many styles of curiosity [ 10 ] . Within these three characterized eﬀects , and possibly along other uncharted axes , humans show natural variability . As instructors we do not get to choose the memory parameters of our students ; at most we get a rough measure of what those parameters are so that we can adjust the teaching structure accordingly . Another contribution of exposure theory to the learn - ing sciences is the bridging of implicit learning mecha - nisms to the development of network structures that rep - resent the learner’s explicit knowledge of a domain . Im - plicit learning typically refers to the acquisition of knowl - edge that occurs through passive exposure to information in the environment . Humans are associative learners—we pick up statistical information of relationships through mere exposure [ 7 ] and it has been shown that there are stable individual diﬀerences in implicit learning ability [ 70 ] . In the educational context , implicit learning could occur when a learner is exposed to the patterns of their learning environment , for instance , when passively read - ing ( or skimming ) a textbook . Given that humans readily pick up the associations in their environment , an important question is : How do learners use these associative patterns to build up so - phisticated , large - scale knowledge structures ? We sug - gest that exposure theory can provide potential answers to this question . Memory research has long - established that human memory does not behave like a computer that stores replicas of one’s perceptual experiences [ 71 ] . In particular , memory eﬀects serve an adaptive func - tion in crafting the speciﬁc memory structures of hu - mans in a way that optimizes later retrieval and learning . Forgetting and shuﬄing serve key functions in retaining core structural features of the taught network at the ex - pense of precision . These two variables in our model are akin to empirically measured forgetting processes and reactivation of memories in random sequences ( i . e . , shuﬄing that occurs naturally in spontaneous thought ) , which respectively enable gist - extraction and abstraction of knowledge in human learning [ 25 , 28 , 72 ] as well as the optimization of later retrieval of competing concepts [ 73 ] . Reinforcement provides an explanation for how the prior history of a learner’s trajectory aﬀects later learning and retrieval , which in turn shapes the memory structure . This explanation aligns with previous inves - tigations which show that retrieval strengthens the stor - age and retrieval strength of previously learned material [ 47 , 74 ] . Broadly , exposure theory as modiﬁed by mem - ory eﬀects provides one framework for understanding im - plicit learning : how passive exposure to the structure of textbooks or other taught materials are ﬁltered through key features of human memory and develop into more explicit forms of knowledge structures . B . Educational implications It is widely understood that diﬀerent teaching method - ologies of the same material can result in diﬀerent learn - ing across matched student cohorts [ 39 , 75 ] . For the same teaching , in any given classroom the students are going to put a diﬀerent amount of eﬀort towards learning [ 41 ] , and are remarkably unresponsive to interventions to increase that eﬀort [ 53 ] . At the same time , humans naturally vary in their memory and in their curiosity along multi - ple axes [ 10 , 24 , 43 , 61 ] . Under these wide - ranging condi - tions , how does the taught material become the learned material ? How can the instructor adjust the material or presentation to increase learning ? A range of previous studies have considered the seman - tic networks of concepts held by the teachers or textbooks [ 35 – 37 ] , while others examined networks constructed by students [ 14 , 40 , 44 , 45 ] . Yet rarely have the two been analyzed jointly . Our modeling results here suggest that the same taught network can result in many diﬀerent learned networks , and further that the mapping can be described in terms of just a few parameters of learners’ eﬀort and memory . Validating these predictions would require a carefully controlled experimental study where both teacher and student semantic networks are assessed simultaneously . A core idea in the memory and science - of - learning lit - erature is that learning and performance are distinct con - structs . Their diﬀerences can make it challenging to dis - tinguish between information that was better learned and information that is easy to retrieve . Such a distinction is made in the so - called new theory of disuse that discusses the diﬀerences between storage strength ( learning ) and retrieval strength ( performance ) of items in memory [ 74 ] . While our approach does not explain diﬀerences in the learners’ ability to retrieve information from their mental models , it does provide a much - needed formalism for rep - resenting the learned memory structure that is obtained from the taught structure—with a careful consideration of common memory errors committed by humans . Estab - lishing the baseline learned structure provides an impor - tant foundation for further modeling of retrieval or recall processes [ 76 – 79 ] that will have implications for explain - ing performance diﬀerences in educational settings . The framework of exposure theory can be useful not only in experimental validation of learning predictions , but also in the design of teaching materials . Exposure theory provides a detailed map of heterogeneous concepts and connections , which can serve as an early feedback mechanism during the development of a textbook or syl - 15 labus . Are the concepts deemed important by the au - thor actually properly emphasized in the text ? What is the picture that the most cursory student would get out of the course ? When we consider individual diﬀerences in learners’ dispositions and cognitive abilities , what is the “range” of their learned networks ? In other words , what is the variance in the network structures acquired from the learning materials , and what aspects of their learned networks are consistent across learners ? These questions highlight a potential use - case of our approach : textbook design could be aimed towards building learn - ing landscapes that either reduce variance across the net - work characteristics of memory structures attained across learners of diﬀerent dispositions , or ensure that a large proportion of learners are likely to acquire the core knowl - edge of a given discipline ( as deﬁned by the author ( s ) ) . While we do not advocate using exposure metrics as a sole method of “optimizing” the teaching materials , our study opens the conversation about the materials design . Finally , aside from enhancing teaching material design , it would also be possible to use exposure metrics to en - hance assessment design , or at least tailor the assessment in a way that aligns with the textbook structure and the population of learned network structures across students . Could an assessment be evaluated based on whether its sampling of to - be - tested concepts or associations across the network can eﬀectively distinguish between students who perform well versus poorly in the class ? C . Methodological considerations In this paper we extend the exposure theory formal - ism to account for three orthogonal and empirically mo - tivated memory eﬀects . We also expect exposure the - ory to be able to handle other eﬀects so long as network learning remains ergodic ; that is , so long as the edge vis - itation probabilities quickly relax to the values dictated by the textbook network [ 52 ] . The α and β eﬀects modify memory probabilities but do not break erdogicity , thus retaining accuracy . The γ eﬀect explicitly breaks ergod - icity by adding path dependence [ 66 ] , which limits our ability to make predictions . Visitation of new edges is still driven by the textbook network , which allows us to make accurate predictions of overlap . In contrast , re - inforcement of existing edges and reciprocity symmetry breaking are path dependent , so exposure theory speci - ﬁes the null model of asymmetry that is violated , but not the exact nature of that violation . While tracking exposure was originally conceived of as a cheap and accurate numerical proxy for stochastic simulations , the exposure value can have other applica - tions . The mapping from the exposure value to the visit probability is a nonlinear convex function : here it is ex - ponential ( Eqn . D5 ) , but theoretically it can take other functional forms . For example , Ref . [ 80 ] connects the ex - posure to viral loads with the probability of developing an infectious disease such as COVID - 19 and uses curve con - vexity to argue for a super - linear beneﬁt of mask - wearing to prevent infections . Studies of the spread of social be - haviors center on the mechanism of complex contagion , in which an individual needs to be exposed to a behavior multiple times from diﬀerent sources in order to adopt it themselves [ 81 , 82 ] . Similar to those studies , the func - tional form of the exposure - to - probability map underlies the global features of the spreading dynamics , including the Jensen bound and the trade - oﬀ between exploration speed and prioritization . The results of this paper rely on the modeling choices of converting textbooks into networks and using random walks to explore those networks . In order to map out the substrate for random walks , we convert the network measurements of textbooks in Ref . [ 36 ] into dynamic net - works with a simple rule : each network edge appears im - mediately at full strength A ij as soon as the exposition time τ reaches the ﬁltration order value F ij . However , the semantic connection between two concepts might be limited to only one chapter of the book , in which case the edge between them should only exist for a ﬁnite time . Statistics of random walks and other spreading processes are known to change signiﬁcantly when the timescales of the random walk step and network evolution are matched [ 83 ] or when network the evolution is intermittent [ 84 ] . Further , the introduction of new concept connections in the text often guides the learner to explore them , thus biasing the random walk towards the freshly - introduced parts of the network [ 85 , 86 ] . Lastly , simple random walks are known to be fairly ineﬃcient means of explor - ing random networks , and many more sophisticated algo - rithms are available [ 17 , 87 , 88 ] . While the combination of these limitations suggests that random walks are at best an incomplete model of a learning process , expo - sure theory greatly speeds up the analysis of this model in bypassing costly stochastic simulations . We therefore pose this study as an important baseline against which to compare the eﬀect of additional learning mechanisms . VIII . CONCLUSIONS In this paper we propose a model of how taught seman - tic networks turn into learned ones , usually non - exactly . We consider two main limitations of learning : the under - sampling eﬀect due to learning for a ﬁnite time and three types of memory imperfections individually validated in the literature . We expand the domain of exposure the - ory to accurately predict the interplay of under - sampling with diverse memory eﬀects at a fraction of the compu - tational cost required by stochastic simulations . While prior work mapped out separately the semantic networks of teachers and learners , our ﬁndings suggest the possi - ble shapes of network distortions in the learning process that can be investigated experimentally . Exposure - based analysis can be used to predict the chance of learning con - cepts and connections from instructional materials , and thus can be used as a design tool for those materials . 16 ACKNOWLEDGMENTS The authors would like to thank C . W . Lynn and X . Xia for discussions about the modeling , as well as L . Dourte , N . Finkelstein , D . Pritchard for discussions on the edu - cation research literature . The computational workﬂow in general and data management in particular for this work was primarily supported by the Signac data man - agement framework [ 89 , 90 ] . This research was funded by the Army Research Oﬃce ( DCIST - W911NF - 17 - 2 - 0181 ) and the National Institute of Health ( R21 - MH - 106799 ) . The content is solely the responsibility of the authors and does not necessarily represent the oﬃcial views of any of the funding agencies . CITATION DIVERSITY STATEMENT Recent work in several ﬁelds of science has identiﬁed a bias in citation practices such that papers from women and other minority scholars are under - cited relative to the number of such papers in the ﬁeld [ 91 – 100 ] . Here we sought to proactively consider choosing references that reﬂect the diversity of the ﬁeld in thought , form of contri - bution , gender , race , ethnicity , and other factors . First , we obtained the predicted gender of the ﬁrst and last author of each reference by using databases that store the probability of a ﬁrst name being carried by a woman [ 95 , 101 ] . By this measure ( excluding references in this paragraph and self - citations to the ﬁrst and last au - thors of our current paper ) , our references contain 20 . 41 % woman ( ﬁrst ) / woman ( last ) , 13 . 24 % man / woman , 12 . 41 % woman / man , and 53 . 94 % man / man . This method is lim - ited in that a ) names , pronouns , and social media pro - ﬁles used to construct the databases may not , in every case , be indicative of gender identity and b ) it cannot account for intersex , non - binary , or transgender people . Second , we obtained predicted racial / ethnic category of the ﬁrst and last author of each reference by databases that store the probability of a ﬁrst and last name being carried by an author of color [ 102 , 103 ] . By this mea - sure ( and excluding self - citations ) , our references con - tain 11 . 15 % author of color ( ﬁrst ) / author of color ( last ) , 10 . 77 % white author / author of color , 17 . 22 % author of color / white author , and 60 . 85 % white author / white au - thor . This method is limited in that a ) names and Florida Voter Data to make the predictions may not be indicative of racial / ethnic identity , and b ) it cannot account for In - digenous and mixed - race authors , or those who may face diﬀerential biases due to the ambiguous racialization or ethnicization of their names . We look forward to future work that could help us to better understand how to sup - port equitable practices in science . Book Nodes n Edges m τ max t corr Treil 278 7106 6681 2 . 66 Axler 217 8458 4220 1 . 78 Edwards 146 4322 2066 2 . 06 Lang 179 5174 3958 1 . 94 Petersen 244 8940 5742 1 . 83 Robbiano 219 8086 2944 1 . 92 Bretscher 384 11914 12703 3 . 12 Greub 275 7108 6841 3 . 34 Heﬀerson 399 13042 8046 3 . 60 Strang 453 15512 10965 2 . 70 TABLE I . Basic statistics of the textbooks used in the study . τ max is the number of sentences ; t corr is the correla - tion time on the full network . Appendix A : Textbook networks statistics In this paper we consider 10 popular linear algebra textbooks ( Table I ) . Each textbook was written by a sin - gle author , whose last name we use as a shorthand for the book throughout the paper . The network extraction procedure is described in Ref . [ 36 ] . Each textbook net - work consists of n nodes and m directed reciprocal edges : we count edges i → j and j → i separately since they can be learned separately . The length of each textbook is measured by the number of sentences τ max so that the number of random steps a learner with dilation D would take is t max = Dτ max . The random walk correla - tion time t corr is determined from the second eigenvalue of the transition matrix as described in Ref . [ 52 ] . The assumptions of exposure theory are fulﬁlled so long as t corr (cid:28) t max , which is the case for all textbooks in the range of dilation we consider in this paper . Appendix B : Stochastic simulations 1 . Baseline simulations The stochastic simulations are implemented via a cus - tom code written in Python . We ﬁrst describe the base - line simulation , and then the necessary algorithmic mod - iﬁcations to account for the three memory eﬀects . We initialize the memory matrix M as a sparse , integer - valued N × N matrix with no entries . Since the indices i , j increment in the order of appearance , we start the random walks at the node i = 0 so that that node is guaranteed to have edges at the early stages of network growth . At each time step t we compute the evolution time τ = t / D . Since the random walker is known to be at node i , we only need to evaluate one row of the transition matrix P ( j | i ) following Eqn . 1 . The most computationally expensive step in the ran - dom walk algorithm is the generation of pseudorandom numbers . At the same time , for each realization of a random walk , we might need to compute diﬀerent time - 17 dependent metrics based on the memory matrix M . It is not eﬃcient to store too many snapshots of M at diﬀer - ent time points and stochastic realizations on the hard drive , so we instead store the random walk trajectory and reconstruct it on demand . We denote x ( t ) to be the node i at which the random walker is located at time t , and x (cid:48) ( t ) to be the node from which the walker remembers to have arrived from . Without any memory eﬀects , the update procedure is as follows : P ( j | i ) | i = x ( t ) → x ( t + 1 ) ; x (cid:48) ( t + 1 ) = x ( t ) , ( B1 ) where the operator → denotes drawing a pseudorandom realization from the probability distribution . Once the trajectories have been computed , the memory matrix can be reconstructed as follows : M ij ( t ) = t (cid:88) t (cid:48) = 1 [ i = x (cid:48) ( t (cid:48) ) ] [ j = x ( t (cid:48) ) ] . ( B2 ) The computational benchmark of a direct simulation ver - sus a reconstruction is presented in the Supplementary Materials of Ref . [ 52 ] , but typically results in a reduction of computation time by a factor of 10 1 . . 10 2 . In order to accumulate statistics that support our main results , we perform several thousand simulations at diﬀerent parameter values ( including the pseudoran - dom seed ) , forming several series of computational exper - iments . We organize the computational workﬂow in gen - eral and data management in particular with the Signac data management framework [ 89 , 90 ] . 2 . The α eﬀect In presence of the α eﬀect every memory is forgotten at every step with a uniform probability α . Given the memory count M ij in a given cell , the number of mem - ories forgotten is a binomial random number B ( M ij , α ) . If some cell already has zero memories , then none can be forgotten . We therefore only draw the pseudorandom realizations for cells M ij with non - zero entries . Since the whole memory matrix gains exactly one count and loses a fraction α of counts per step , it would stabilize at an average count number 1 / α and lose on average one memory per step . We encode the memories forgotten at step t in the list of pairs f ( t ) which is typically short ( its length is a Poisson number with an average of 1 ) . Given the random walk trajectory x ( t ) , x (cid:48) ( t ) and the forgetting sequence f ( t ) , the memory matrix at any time point can be deterministically reconstructed as follows : M ij ( t ) = t (cid:88) t (cid:48) = 1   [ i = x (cid:48) ( t (cid:48) ) ] [ j = x ( t (cid:48) ) ] − (cid:88) i , j ∈ f ( t ) [ i ] [ j ]   , ( B3 ) where the inner sum runs over the pairs stored in f ( t ) . 3 . The β eﬀect In the presence of the β eﬀect the random walk pro - ceeds identically , but the memories are formed with a shuﬄing of the perceived step origin following the distri - bution p ( ∆ t ) given by Eqn . 4 . At every time step we draw a pseudorandom realization p ( ∆ t ) → ∆ t , and update the stored trajectories as follows : P ( j | i ) | i = x ( t ) → x ( t + 1 ) ; x (cid:48) ( t + 1 ) = x ( t − ∆ t ) , ( B4 ) which allows a deterministic reconstruction of the mem - ory matrix with Eqn . B3 . 4 . The γ eﬀect In the presence of the γ eﬀect we need to compute not only a row of the textbook - based transition probability T ij ( τ ) , but also a row of the mental model ˆ T ij ( t ) fol - lowing Eqn . 2 . If there are no memories in a given row ( which for example is always the case on the very ﬁrst random walk step ) , the mental model is zero and we use the textbook transition probability P ( j | i ) = T ij . If there are memories , we compute the mixture of the two tran - sition matrices with Eqn . 5 , use that to draw a pseudo - random realization of the next step x ( t + 1 ) , and proceed as before . The γ eﬀect easily combines with the α and β eﬀects . Note that the textbook transition probability T ij is always well deﬁned for all nodes i reachable through a random walk : if an edge led to the node , there is always at least that edge along which the random walker can return . Appendix C : Mental model metrics 1 . Node metrics It is relatively straightforward to keep track of the number of nodes . The full network has n nodes . By a speciﬁc sentence τ only a part of those nodes have been presented by the textbook , which we count as the num - ber of rows in the adjacency matrix A ( τ ) with non - zero entries : n ( τ ) = (cid:88) i    (cid:88) j A ij ( τ )   > 0   . ( C1 ) In a similar fashion we can count the number of nodes learned by the random walker , using either the memory matrix M or the normalized mental model ˆ T : ˆ n ( t ) = (cid:88) i     (cid:88) j M ij ( t )   > 0   = (cid:88) i    (cid:88) j ˆ T ij ( t )   > 0   , ( C2 ) 18 where the two deﬁnitions are equal because the absolute value of matrix elements does not matter . All that mat - ters is their presence in rows . From the number of learned nodes we compute the node recall : R n = ˆ n ( t ) / n , ( C3 ) where we divide by the total rather than the current num - ber of nodes by convention . In this convention , the ran - dom walk starts with node recall 0 and can monotonically grow up to 1 . 2 . Edge metrics The goal of constructing a mental model of network transitions is to predict the probability of transitioning from a given node i to diﬀerent nodes j . The absolute number of such transitions in either the textbook or the learner’s memory should not matter . Therefore , in or - der to assess the quality of learning we seek a quanti - tative comparison mechanism between the taught tran - sition matrix T and the learned one ˆT . The relation - ship between them is illustrated by the Venn diagram in Fig . 1f : generally , the two networks have partial over - lap . Because of the interplay of ﬁnite learner eﬀort and memory eﬀects , the learned matrix might include spuri - ous edges that were never taught , but lack taught edges that were never learned . There are multiple ways to construct such comparison metrics . One way to compare the two networks is to treat them as conditional probability distributions and compute the Kullback - Leibler ( KL ) divergence between them as in Ref . [ 28 ] . The KL divergence is zero when the two networks are identical and grows as the probability leaks into the spurious edges . However , a single miss - ing taught edge immediately renders the KL divergence singular since it introduces a log ( 0 ) term into the sum . In order to avoid the divergence , we seek well - behaved metrics of the following form : Metric = (cid:80) ij Model ij [ Mask ij > 0 ] Norm , ( C4 ) where each of the three components is a binary choice , resulting in eight possible metrics . The Model component focuses on either learned or taught edges ; the Mask com - ponent focuses on either current or future edges ; and the Norm component focuses on either the complete network or the fraction of edges taught by a given time τ . We use only a few of all possible component combinations , as detailed below . As a ﬁrst example , the edge recall R e is the fraction of edges of the taught mental model that have been learned ( Fig . 3 , 5 ) : R e = 1 n (cid:88) ij T ij [ ˆ T ij > 0 ] , ( C5 ) where the weight of each edge is given by the transition probability of the full textbook network ; its inclusion is driven by the learned mental model ; and the normaliza - tion equals the total number rows in either matrix , or the number of nodes in the network . As more and more edges are introduced in the taught network over time , more can be learned , thus increasing the edge recall metric . If all real edges have been learned , then the indicator function evaluates to 1 for all edges , and thus edge recall reaches 1 . In order to assess the precision of the learned mental model , we ﬂip the taught and learned networks ( Fig . 6 , 7 ) : P = 1 n ( τ ) (cid:88) ij ˆ T ij [ T ij > 0 ] , ( C6 ) where we also changed the normalization to refer to the nodes already introduced . If we used the ﬁxed normal - ization n , then the magnitude of the precision metric would mostly follow the fraction of nodes learned : in other words , the number of rows with nonzero entries in ˆ T would matter , rather than the content of those rows . Without shuﬄing ( β = ∞ ) , all learned edges necessarily exist so that T ij > 0 for any learned ( i , j ) . The precision can still be less than 1 since some rows of ˆ T can still be empty due to under - sampling , and an absent mental model for transitions out of one node cannot be precise . If a learner’s eﬀort is suﬃcient to keep up with the intro - duction of new nodes n ( τ ) , then a precision of 1 can be reached . For measuring the prediction of future edges , we use the Mask to select those ( Fig . 7 ) : P pr = 1 ˆ n ( t ) (cid:88) ij ˆ T ij [ F ij > τ ] ( C7 ) R pr = (cid:80) ij T ij [ F ij > τ ] [ ˆ T ij > τ ] (cid:80) ij T ij [ F ij > τ ] , ( C8 ) where [ F ij > τ ] picks out the real edges that would ap - pear later than the current sentence τ . For precision of prediction , we normalize by the number of nodes al - ready learned by the random walker since any inferred connections are between those nodes . For the recall of prediction , we divide the total weight in the learned fu - ture edges by the total weight in all future edges . Both the numerator and the denominator of that expression approach zero by the end of the book , but the numerator is always no larger than the denominator , and hence the expression is never singular . 3 . Triangular closure Selecting the edges that comprise triangular closure of existing edges is another choice of Mask . A triangular closure is a walk of length 2 such that there is no direct edge between start and end . We therefore deﬁne a mask 19 that is a product of those two conditions : TriClo = [ ( A ( τ ) 2 ) ij > 0 ] · [ A ij ( τ ) = 0 ] , ( C9 ) and use this mask to select edges for scatter plots in Fig . 7 . Appendix D : Exposure theory 1 . Baseline exposure The goal of exposure theory is to provide a computa - tionally cheap but accurate approximation to the proba - bility distribution of memory matrices M , which in turn allows us to predict the trajectories of the mental model metrics . We ﬁrst recap the baseline formulation of ex - posure theory as derived and validated in Ref . [ 52 ] . We consider a weighted , undirected , time - dependent network described by the adjacency matrix A ( τ ) ( Fig . 10 ) . We assume that the network always has one main connected component and any disconnected pieces are small and only appear for a short time . The structure of the net - work is driven by the evolution time τ , while the dynam - ics of the random walk are driven by the random walk time t = Dτ , where D is the dilation parameter . First we compute the steady - state probability of vis - iting a particular edge of the network . The transition matrix for the random walk is computed by normalizing the adjacency matrix by row sum , while the steady - state probability of each node is proportional to its strength ( weighted degree ) : T ij ( t ) = A ij ( t ) (cid:80) j A ij ( t ) ; π i ( t ) = (cid:80) j A ij ( t ) (cid:80) ij A ij ( t ) , ( D1 ) and from these two expressions , we get the steady - state probability of visiting any edge of the network : p ij ( t ) = π i T ij ( t ) = A ij ( t ) (cid:80) ij A ij ( t ) . ( D2 ) The key assumption of exposure theory is that the random walk is always equilibrated to the instantaneous distribution . Practically , this happens when the correla - tion time of the random walk is much smaller than its length—an assumption that holds for many real - world networks ( see Ref . [ 52 ] for discussion ) . In this case , the accumulation of memory counts of any particular transi - tion M ij is a Poisson process with the rate per step given by Eqn . D2 . For an equilibrated random walk the rate accumulates additively . The accumulation of the rate over time t is termed the integral exposure : E ij ( t ) ≡ t (cid:88) 1 p ij ( τ ) = D τ (cid:90) 0 p ij ( τ (cid:48) ) dτ (cid:48) = D E ij ( τ ) , ( D3 ) where we changed variables between evolution time τ and random walk time t . The integral in τ (cid:48) , termed the spe - ciﬁc exposure E ij , can be precomputed at a desired time resolution with any standard method ( Fig . 10b ) . Con - verting from speciﬁc to integral exposure only requires a computationally cheap multiplication by dilation D . Once the integral exposure is known , the number of mem - ories of the transition follows the Poisson distribution : M ij ∼ Pois ( D E ij ( τ ) ) , ( D4 ) and speciﬁcally the probability that a transition has been seen at least once is given by : P ( M ij > 0 ) = 1 − e − D E ij ( τ ) . ( D5 ) Ref . [ 52 ] also gives the rules of aggregation of exposure across a group of edges . For instance , from the edge exposure we can also compute the node exposure that accounts for the visitation of nodes : K i ( t ) ≡ (cid:88) j E ij ( t ) ; K i ( t ) = D K i ( τ ) , ( D6 ) although the total exposure is conserved : (cid:88) i K i = (cid:88) ij E ij = t = Dτ , ( D7 ) which sets the scale of memory ﬂuctuations across the whole network . In the limit of large dilation D → ∞ , the relative ﬂuc - tuations in the memory counts get small and the Pois - son random numbers are well - approximated by the mean , thus giving the mean - ﬁeld limit : M mf ij ∝ E ij , ( D8 ) where the proportionality constant would cancel out from most computations of interest ( e . g . , row normalization ) . 2 . Node and edge recall In order to compute the exposure prediction of node and edge recall ( Figs . 3 , 5 ) , we average the output of Eqn . D5 over the node or edges of the network : R n = 1 − 1 n (cid:88) i e − D K i ( τ ) ( D9 ) R e = 1 − 1 n (cid:88) ij T ij e − D E ij ( τ ) , ( D10 ) where we follow the weight convention of Appendix C . 3 . Jensen bound for nodes The shape of the node recall curve is subject to a Jensen bound ( as derived in Ref . [ 52 ] ) by using the con - vexity property of φ ( x ) ≡ e − x : R n = 1 − 1 n (cid:88) i e − D K i ( τ ) ≤ 1 − e − t / n , ( D11 ) 20 Weight at τ = early a Weight at τ = middle Weight at τ = late Final exposure β = ∞ + b + = β = 0 . 5 + c + = 10 - 6 10 - 5 10 - 4 10 - 3 10 - 2 M e m o r y p r o b . p i j 10 - 6 10 - 5 10 - 4 10 - 3 10 - 2 M e m o r y p r o b . p ( β ) i j 10 - 2 10 - 1 10 0 10 1 S p ec . e x p o s u r e E i j 10 - 2 10 - 1 10 0 10 1 S p ec . e x p o s u r e E ( β ) i j FIG . 10 . The speciﬁc exposure matrix results from accumulation of memory probability . ( a ) Textbook concept co - occurrence networks at early , middle , and late points in the textbook ( brown ) , as well as a network with edges weighted by accumulated speciﬁc exposure ( green ) . ( b ) As the network grows during exposition , the memory probability matrices p have more and more nonzero entries ( purple ) . Their accumulation results in the speciﬁc exposure matrix E ( green ) . ( c ) With ﬁnite shuﬄing ( β (cid:54) = ∞ ) the memory probability matrices p ( β ) become smudged across rows and columns ( purple ) . The accumulation of shuﬄed memories results in the shuﬄed speciﬁc exposure E ( β ) . which implies that learning all nodes in a regular network ( all nodes have the same strength ) would be the fastest , with the timescale equal to the number of nodes n . In practice we see that learning deeply under - saturates this bound ( Figs . 3 , 11 ) . In order to explain this discrep - ancy , we expand the diﬀerence between the Jensen bound and the exposure prediction to second order in time τ : R n (cid:39) 1 − 1 n (cid:88) i (cid:18) 1 − D K i ( τ ) + 1 2 D 2 K 2 i ( τ ) + O (cid:0) τ 3 (cid:1)(cid:19) = Dτ n − D 2 2 n (cid:88) i K 2 i ( τ ) ( D12 ) R Jensen n (cid:39) Dτ n − D 2 τ 2 2 n + O (cid:0) τ 3 (cid:1) ( D13 ) R Jensen n − R n (cid:39) D 2 2 n (cid:88) i K 2 i ( τ ) − D 2 τ 2 2 n = D 2 2 Var K ( τ ) , ( D14 ) which directly connects the under - saturation of the Jensen bound with the variance ( inhomogeneity , prior - itization ) of network nodes by speciﬁc exposure . The under - saturation only appears at second order in time , which explains why the tangents of the exposure curve and its bound coincide at the start ( Figs . 3 , 11 ) . 4 . Jensen bound for edges The shape of the edge recall curve is subject to a sim - ilar bound that can be analogously derived : R e = 1 − 1 n (cid:88) ij T ij e − D E ij ( τ ) ≤ 1 − exp   − D n (cid:88) ij T ij E ij ( τ )   , ( D15 ) where instead of equally - weighted average speciﬁc expo - sure over the nodes we now have a weighted average over the edges . The weights T ij stay constant in time , while the relative proportion of speciﬁc exposure on diﬀerent edges shifts , so the weighted sum does not have a simple closed - form expression . However , we can approximate it : a typical transition probability out of a node equals ei - ther zero or its inverse degree , which we approximate by the inverse average degree T ij ≈ n / m . In this case the Jensen bound for edge recall takes the shape : R e ∼ < 1 − e − t / m , ( D16 ) which predicts that an unweighted network would be the fastest to learn with a timescale equal to the number of edges m . Whereas at small t this is not a strict bound ( Figs . 3 , 11 ) , at larger t the recall of weighted networks is also deeply unsaturated . This slowdown can be con - nected to the variance of edge exposure following a sim - ilar argument as for the nodes . 21 5 . The α eﬀect In order to account for the α eﬀect , we need to intro - duce forgetting into the memory dynamics . Since for - getting is stochastic but unbiased ( every memory has an equal chance of being forgotten ) , we can just directly modify the exposure dynamics . The full stochastic pro - cess of memory dynamics is deﬁned by master equations that are solved by an ansatz of a Poisson distribution with the to - be - determined parameter of exposure [ 52 ] . For normal learning , the exposure becomes a time in - tegral of visitation probability ( Eqn . D3 ) . The master equations with forgetting are still solved by a Poisson distribution ansatz , but with diﬀerent dynamics of the exposure parameter . Across one time step , the exposure changes as follows : E ij ( t + 1 ) = E ij ( t ) ( 1 − α ) + p ij ( τ ) , ( D17 ) where α is the forgetting rate . This recursion relation can be solved by inductively substituting it into itself : E ij ( t ) = t (cid:88) t (cid:48) = 1 p ij ( τ ) ( 1 − α ) t − t (cid:48) (cid:39) D τ (cid:90) 0 p ij ( τ ) e − αD ( τ − τ (cid:48) ) dτ (cid:48) , ( D18 ) where we used α (cid:28) 1 and t = Dτ . Note that the dif - ﬁcult part of this expression ( the integral ) only depends on the product of αD rather than on the two values in - dividually . Thus computing it for a variety of α and D value combinations only requires us to account for the distinct values that their product can take ( Fig . 5 ) . In practice , the simplest way to compute the integral is to turn Eqn . D17 into an ordinary diﬀerential equation in τ and integrate it numerically following the scheme : E ( αD ) ij ( τ + ∆ τ ) = E ( αD ) ij ( τ ) e − αD ∆ τ + p ij ( τ ) ∆ τ , ( D19 ) where ∆ τ is a suitably small integration step . Once the speciﬁc exposure is known , we convert it to the integral exposure E ( αD ) ij = D E ( αD ) ij , which parameterizes the new Poisson distributions that now include forgetting . From those Poisson distributions , we compute the desired met - rics in Fig . 5 . 6 . The β eﬀect : exposure accumulation In order to account for the β eﬀect of memory shuf - ﬂing , we adapt the model of Ref . [ 24 ] . As the learner experiences a random walk , they do not remember the transitions exactly , but rather shuﬄe them locally . The shuﬄing distribution can be obtained from the free en - ergy principle and has the geometric form p ( ∆ t ) = ( 1 − e − β ) e − β ∆ t . In this case we are not interested only in the probability that a given edge p ij was visited but in the probability that a given edge ˆ p ij was remembered , after accounting for the memory shuﬄing . We know that the underlying random walk is the same and the probability of visiting any node π i is still the same . The remembered matrix of transitions after the shuﬄing was shown to be [ 24 ] : T ( β ) = ( 1 − e − β ) ∞ (cid:88) k = 0 e − βk T k + 1 = ( 1 − e − β ) T (cid:0) I − e − β T (cid:1) − 1 . ( D20 ) The original and shuﬄed transition matrices T and T ( β ) are both row - normalized to describe the learned transition rates . In order to ﬁnd the absolute , rather than conditional , probability of learning a particular edge , we multiply it by π i : p ( β ) ij ( t ) = π i ( t ) T ( β ) ij ( t ) ( D21 ) lim β → 0 p ( β ) ij ( t ) = π i ( t ) π j ( t ) , ( D22 ) where the matrix p ( β ) can be checked to be symmetric and normalized so that all entries sum up to 1 . In the complete shuﬄing limit β → 0 , the learner loses all notion of the order of explored nodes , but still keeps track of the relative frequency of visiting diﬀerent nodes . We next take the same assumption as in the ﬁrst derivation of exposure theory : that the random walk is always equilibrated , now also with respect to memory shuﬄing . At ﬁnite β , the transition memories are shuf - ﬂed on the timescale of roughly (cid:104) ∆ t (cid:105) (cid:39) 1 / ( e β − 1 ) . We assume that this timescale , just like the random walk correlation time , is much shorter than the timescale of network exploration . Therefore , at each step of the net - work , memories of each edge ( real or spurious ) are cre - ated with probabilities p ( β ) ij . Note that the spurious edges can only be generated between the nodes that have al - ready been introduced . The overall speciﬁc exposure can be computed by integrating the remembering probability ( Fig . 10c ) : E ( β ) ij ( τ ) = τ (cid:90) τ (cid:48) = 0 p ( β ) ij ( τ (cid:48) ) dτ (cid:48) , ( D23 ) which for β → ∞ reduces to the old formula ( Eqn . D3 ) . The integral exposure is obtained , just like before , by rescaling the speciﬁc exposure by dilation E ( β ) ij = D E ( β ) ij . Since some of the exposure now falls onto the spurious edges , the exposure of real edges is necessarily smaller , and thus it would take longer to learn the real edges . As before , the prediction of edge learning is more accurate in the aggregate , which can now be extended to the spurious edges . 7 . The β eﬀect : metric computation First we compute the precision in learning the real edges , i . e . , the proportion of probability weight in the 22 learned mental model that lies in real edges . With ex - posure theory we can evaluate the two contributions to this precision : whether each node has been visited at all and what fraction of memories transitioning out of that node corresponds to real edges . By a certain time τ , n ( τ ) nodes of the network have been introduced . The speciﬁc exposure of every node is given by : K i ( τ ) = (cid:88) j E ( β ) ij , ( D24 ) and is independent of β since regardless of shuﬄing the random walker always keeps track of the node they just arrived on . For the nodes that were not introduced yet , K i = 0 ∀ i > N ( τ ) . The combination of speciﬁc exposure and dilation predicts the probability of the node being visited . If a node has been visited , we can deﬁne two groups of outgoing edges : edges that are real and all edges ( the ﬁrst is a subset of the second ) . Since the accumulation of all edge memories is independent , we can use Poisson cal - culus to compute the number of memories in each group by selecting the real edges with a Mask [ 52 ] : M real i ∼ Pois  (cid:88) j D E ( β ) ij [ A ij > 0 ]   ( D25 ) M all i ∼ Pois  (cid:88) j D E ( β ) ij   , ( D26 ) from which we can estimate the fraction of weight in the real edges . Putting the node and edge contributions to - gether , we get the following exposure prediction of pre - cision : P ( τ ) = 1 n ( τ ) (cid:88) i ( 1 − e − D K i ) (cid:80) j E ( β ) ij [ A ij > 0 ] (cid:80) j E ( β ) ij , ( D27 ) where dilation cancels out from the ﬁnal fraction : as di - lation gets larger , the counts of real and spurious edges follow the same proportion . The precision metric only depends on dilation through visitation of nodes . As D → ∞ , precision approaches a ﬁnite value dependent on τ and β ( Fig . 6 , 7 ) . We can similarly compute the precision of edge predic - tion , i . e . , the fraction of probability weight in edges that will appear later ( Fig . 7 ) . This calculation only requires swapping out the mask in the numerator from edge exis - tence [ A ij > 0 ] to ﬁltration order [ F ij > τ ] : P pr ( τ ) = 1 n ( τ ) (cid:88) i ( 1 − e − D K i ) (cid:80) j E ( β ) ij [ F ij > τ ] (cid:80) j E ( β ) ij , ( D28 ) where by the end of the book there are no edges left such that F ij > τ , and thus any prediction is impossible . In order to compute the recall of prediction ( Fig . 7 ) , we consider the transition matrix of the whole book that ought to be learned T ij , ﬁlter the edges that would exist in the future , and account for the probability of those edges being remembered : R pr ( τ ) = (cid:80) ij T ij ( 1 − e − D E ij ) [ F ij > τ ] (cid:80) ij T ij [ F ij > τ ] . ( D29 ) 8 . The γ eﬀect In order to account for the γ eﬀect , we need to pro - vide two exposure - based computations : the overlap met - ric and the memory count asymmetry . The presence of reinforcement in general breaks the ergodicity of mem - ory accumulation dynamics and thus breaks the core as - sumption of exposure theory . So instead , we compute the exposure theory predictions in the absence of reinforce - ment γ = 0 , and show how they can relate to a ﬁnite γ case . The overlap metric ( Eqn . 6 ) is quite similar to edge recall ( Eqn . C5 ) but involves an unweighted average over the edges . We can therefore construct an exposure pre - diction similar to Eqn . D10 . Per exposure theory , the learning of each edge is independent from learning any other edge in the same replica of a random walk , and also independent from learning the same edge in a diﬀer - ent replica . Since the exposure metric can either compare a replica to itself ( self - overlap Q aa ) or to another replica ( cross - overlap Q ab ) , we construct the following two esti - mators : Q aa ( τ ) = 1 m (cid:88) ij [ T ij > 0 ] (cid:16) 1 − e − D E ij ( τ ) (cid:17) ( D30 ) Q ab ( τ ) = 1 m (cid:88) ij [ T ij > 0 ] (cid:16) 1 − e − D E ij ( τ ) (cid:17) 2 , ( D31 ) where we used the fact that the probability of two iden - tically distributed independent events happening is the square of the probability of one event . Since the prob - abilities are less than or equal to 1 , the terms in the cross - overlap sum are typically smaller than in the self - overlap sum . For self - overlap Q aa we can construct a Jensen bound by again using the concavity of the func - tion φ ( x ) = ( 1 − e − x ) . In contrast , for cross - overlap Q ab , such a Jensen bound does not apply because the func - tion φ ( x ) = ( 1 − e − x ) 2 is neither convex nor concave and has an inﬂection point . While for a given network the above formulas make accurate predictions of self - and cross - overlap , it is harder to make general claims about the space of possible networks . How does this prediction at γ = 0 help us to reason about ﬁnite reinforcement γ > 0 ? The key idea is that discovery of new edges is driven by independent random walk steps and not reinforcement . The accumulation of the ﬁrst memory count is still a Poisson process predicted by Eqn . D5 , even if the rest of the distribution is diﬀer - ent due to reinforcement . However , in the presence of reinforcement not all steps are independent . If over the 23 course of a long random walk a total of Dτ steps have been made , of those roughly Dτ ( 1 − γ ) steps followed the network and had a chance to discover new edges , and the other Dτγ steps retraced known edges . The independent steps are uniformly distributed among all steps , follow - ing the evolution of the network . Therefore in order to predict the self - and cross - overlap in the presence of re - inforcement , we can pre - compute the curves Q aa ( D ) and Q ab ( D ) , and look up the value at the eﬀective dilation D γ = D ( 1 − γ ) ( Fig . 8 ) . The prediction of memory asymmetry proceeds simi - larly : we estimate the asymmetry in the absence of re - inforcement γ = 0 and determine whether the estimate is broken for γ > 0 . Without reinforcement , the number of memories of each edge is a non - negative integer with a Poisson distribution parameterized by the integral ex - posure : M ij ∈ { Z ≥ 0 } ( D32 ) M ij ∼ Pois ( D E ij ( τ ) ) , ( D33 ) and the distributions for the reciprocal edge M ji are iden - tical . The diﬀerence of the two can be any integer and follows the Skellam distribution : M ij − M ji ∈ Z ( D34 ) M ij − M ji ∼ Skellam ( D E ij ( τ ) , D E ij ( τ ) ) , ( D35 ) where the exact functional form of the distribution can be computed but is not very important as we instead focus on its moments . When two independent random numbers are subtracted , their means subtract but their variances add : (cid:104) M ij − M ji (cid:105) = 0 ( D36 ) (cid:10) ( M ij − M ji ) 2 (cid:11) c = 2 D E ij ( τ ) , ( D37 ) and the standard deviation of asymmetry is the square root of variance (cid:112) 2 D E ij ( τ ) . This standard deviation deﬁnes the expected range of asymmetry , which we check for violations in the presence of reinforcement ( Fig . 9 ) . Appendix E : Supplementary results In the main text of the paper we explored the conse - quences of ﬁnite eﬀort and memory eﬀects on network learning for a few example textbooks . Here we provide identical analyses for the rest of the textbooks . We previously showed the comparison of typical learning trajectories for two textbooks ( Fig . 3 ) . The other eight textbooks show qualitatively similar curves ( Fig . 11 ) , conﬁrming that prioritization of concepts and connections is generically present across most textbooks . All of the recall trajectories are deeply unsaturated com - pared to the respective Jensen bounds . In the plotted dilation range D ∈ [ 0 , 10 ] , none of the textbooks reliably reach the learning of all but one node R n = 1 − 1 / n . We explained the global learning trajectories in terms of local speciﬁc exposure of nodes and edges for two text - books ( Fig . 4 ) . For the other textbooks , the patterns are qualitatively similar ( Fig . 12 ) . We compared the precision of learning real edges with the peak precision of predicting future edges for a single textbook ( Fig . 7 ) . Across other textbooks , the trade - oﬀ pattern is broadly similar ( Fig . 13 ) . The lowest pre - cision of real edges varies in the range [ 0 . 3 , 0 . 5 ] , while the highest prediction of future edges varies in the range [ 0 . 17 , 0 . 31 ] . Across all textbooks , the ﬁnite values of di - lation D signiﬁcantly limit both precision metrics . 24 0 2 4 6 8 10 0 0 . 9 0 . 99 N o d e r eca ll R n a Treil 0 2 4 6 8 10 b Edwards 0 2 4 6 8 10 c Lang 0 2 4 6 8 10 d Petersen All nodes 0 2 4 6 8 10 0 0 . 9 Dilation D E d g e r e c a ll R e e 0 2 4 6 8 10 Dilation D f 0 2 4 6 8 10 Dilation D g 0 2 4 6 8 10 Dilation D h 0 2 4 6 8 10 0 0 . 9 0 . 99 N o d e r eca ll R n i Robbiano 0 2 4 6 8 10 j Bretscher 0 2 4 6 8 10 k Hefferson 0 2 4 6 8 10 l Strang 0 2 4 6 8 10 0 0 . 9 Dilation D E d g e r eca ll R e m 0 2 4 6 8 10 Dilation D n 0 2 4 6 8 10 Dilation D o 0 2 4 6 8 10 Dilation D p Exposure Jensen FIG . 11 . Comparison of node and edge recall trajectories for the remaining eight textbooks . For each textbook , on the top plot ( panels a - d , i - l ) the green dashed curve shows the exposure prediction of node recall ; the blue shaded region shows the Jensen bound ; and the gray dashed horizontal line shows the learning of all but one node ( R n = 1 − 1 / n ) . On the bottom plot ( panels e - h , m - p ) the green dashed curve shows the exposure prediction of edge recall , and the blue shaded region shows the Jensen bound . 25 10 0 10 1 10 2 10 3 10 - 1 10 0 10 1 10 2 10 3 Node strength s i N o d e s p ec . e x p o s u r e K i ρ = 0 . 84 a Treil 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 86 b Edwards 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 82 c Lang 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 83 d Petersen 10 0 10 1 10 2 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 Edge weight A ij E d g e s p ec . e x p o s u r e E i j ρ = 0 . 72 e 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 75 f 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 69 g 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 72 h 10 0 10 1 10 2 10 3 10 - 1 10 0 10 1 10 2 10 3 Node strength s i N o d e s p ec . e x p o s u r e K i ρ = 0 . 84 i Robbiano 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 86 j Bretscher 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 85 k Hefferson 10 0 10 1 10 2 10 3 Node strength s i ρ = 0 . 84 l Strang 10 0 10 1 10 2 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 Edge weight A ij E d g e s p ec . e x p o s u r e E i j ρ = 0 . 73 m 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 72 n 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 70 o 10 0 10 1 10 2 Edge weight A ij ρ = 0 . 66 p Learning cutoff D = 0 . 1 D = 1 D = 10 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( n o d e v i s i t ) a t D = 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( e d g e v i s i t ) a t D = 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( n o d e v i s i t ) a t D = 1 . 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P ( e d g e v i s i t ) a t D = 1 . 0 FIG . 12 . Comparison of node and edge exposure pattern for the remaining eight textbooks . For each textbook , the top scatter plot shows the node speciﬁc exposure and strength , whereas the bottom scatter plot shows the edge speciﬁc exposure and edge weight . The marker color corresponds to the probability of a node or edge being learned across 10 replicas at D = 1 . 0 . The horizontal dashed lines indicate the boundary of node and edge learning at diﬀerent dilation . In each panel ρ is the Spearman correlation coeﬃcient , with log 10 ( p ) < − 12 . 26 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r ec i s i o n o f r ea l e d g e s P β = 0 . 1 β = 2 β = 0 . 1 β = 2 M ea n f i e l d D = 10 D = 1 D = 0 . 1 a Axler 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 b Edwards 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 c Lang 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r ec i s i o n o f r ea l e d g e s P β = 0 . 1 β = 2 β = 0 . 1 β = 2 d Petersen 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 e Robbiano 0 . 0 0 . 1 0 . 2 0 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 f Bretscher 0 . 0 0 . 1 0 . 2 0 . 3 Precision of prediction P prmax 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 P r ec i s i o n o f r ea l e d g e s P β = 0 . 1 β = 2 β = 0 . 1 β = 2 g Greub 0 . 0 0 . 1 0 . 2 0 . 3 Precision of prediction P prmax 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 h Hefferson 0 . 0 0 . 1 0 . 2 0 . 3 Precision of prediction P prmax 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 β = 0 . 1 β = 2 β = 0 . 1 β = 2 k Strang FIG . 13 . Comparison of the precision - prediction trade - oﬀ for the remaining nine textbooks . For each textbook , each curve shows variable shuﬄing β , while diﬀerent curves correspond to diﬀerent dilation D . 27 [ 1 ] A . C . Schapiro , T . T . Rogers , N . I . Cordova , N . B . Turk - Browne , and M . M . Botvinick , Neural representations of events arise from temporal community structure , Nat Neurosci 16 , 486 ( 2013 ) . [ 2 ] E . A . Karuza , S . L . Thompson - Schill , and D . S . Bas - sett , Local patterns to global architectures : Inﬂuences of network topology on human learning , Trends Cogn Sci 20 , 629 ( 2016 ) . [ 3 ] T . Engelthaler and T . T . Hills , Feature biases in early word learning : Network distinctiveness predicts age of acquisition , Cogn Sci . 41 , 120 ( 2017 ) . [ 4 ] A . E . Sizemore , E . A . Karuza , C . Giusti , and D . S . Bassett , Knowledge gaps in the early growth of semantic feature networks , Nat Hum Behav 2 , 682 ( 2018 ) . [ 5 ] S . H . Solomon , J . D . Medaglia , and S . L . Thompson - Schill , Implementing a concept network model , Behav Res Methods 51 , 1717 ( 2019 ) . [ 6 ] M . Peer , I . K . Brunec , N . S . Newcombe , and R . A . Ep - stein , Structuring knowledge with cognitive maps and cognitive graphs , Trends Cogn Sci 25 , 37 ( 2021 ) . [ 7 ] J . R . Saﬀran , R . N . Aslin , and E . L . Newport , Statistical learning by 8 - month - old infants , Science 274 , 1926 – 1928 ( 1996 ) . [ 8 ] A . R . Romberg and J . R . Saﬀran , Statistical learning and language acquisition , Wiley Interdisciplinary Re - views : Cognitive Science 1 , 906 – 914 ( 2010 ) . [ 9 ] M . Stella , N . M . Beckage , and M . Brede , Multiplex lex - ical networks reveal patterns in early word acquisition in children , Scientiﬁc reports 7 , 1 ( 2017 ) . [ 10 ] D . M . Lydon - Staley , D . Zhou , A . S . Blevins , P . Zurn , and D . S . Bassett , Hunters , busybodies and the knowl - edge network building associated with deprivation cu - riosity , Nature human behaviour 5 , 327 ( 2021 ) . [ 11 ] H . Ju , D . Zhou , A . S . Blevins , D . M . Lydon - Staley , J . Kaplan , J . R . Tuma , and D . S . Bassett , The net - work structure of scientiﬁc revolutions , arXiv preprint arXiv : 2010 . 08381 ( 2020 ) . [ 12 ] P . Zurn , D . Zhou , D . M . Lydon - Staley , and D . S . Bas - sett , Edgework : Viewing curiosity as fundamentally re - lational , ( 2021 ) . [ 13 ] I . Iacopini , S . Milojevi´c , and V . Latora , Network dy - namics of innovation processes , Physical review letters 120 , 048301 ( 2018 ) . [ 14 ] C . S . Siew , Using network science to analyze concept maps of psychology undergraduates , Applied Cognitive Psychology 33 , 662 ( 2019 ) . [ 15 ] L . da Fontoura Costa , Learning about knowledge : A complex network approach , Physical Review E 74 , 026103 ( 2006 ) . [ 16 ] L . da Fontoura Costa and G . Travieso , Exploring com - plex networks through random walks , Physical Review E 75 , 016102 ( 2007 ) . [ 17 ] A . Asztalos and Z . Toroczkai , Network discovery by gen - eralized random walks , EPL ( Europhysics Letters ) 92 , 50008 ( 2010 ) . [ 18 ] J . N . Mildner and D . I . Tamir , Spontaneous thought as an unconstrained memory process , Trends in neuro - sciences 42 , 763 ( 2019 ) . [ 19 ] S . De Deyne , A . Perfors , and D . J . Navarro , Predicting human similarity judgments with distributional models : The value of word associations , in COLING 2016 - 26th International Conference on Computational Linguistics , Proceedings of COLING 2016 : Technical Papers ( Fac - ulty of Science , Information and Language Processing Syst ( IVI , FNWI ) , 2016 ) pp . 1861 – 1870 , iSSN : 1525 - 2477 . [ 20 ] C . W . Lynn and D . S . Bassett , How humans learn and represent networks , Proceedings of the National Academy of Sciences 117 , 29407 ( 2020 ) . [ 21 ] E . A . Karuza , The Value of Statistical Learning to Cog - nitive Network Science , Topics in Cognitive Science 14 , 78 ( 2022 ) . [ 22 ] K . Michaelian , The epistemology of forgetting , Erken - ntnis 74 , 399 ( 2011 ) . [ 23 ] A . E . Kahn , E . A . Karuza , J . M . Vettel , and D . S . Bas - sett , Network constraints on learnability of probabilis - tic motor sequences , Nature human behaviour 2 , 936 ( 2018 ) . [ 24 ] C . W . Lynn , A . E . Kahn , N . Nyema , and D . S . Bassett , Abstract representations of events arise from mental er - rors in learning and memory , Nature communications 11 , 1 ( 2020 ) . [ 25 ] T . Zeng , A . Tompary , A . C . Schapiro , and S . L . Thompson - Schill , Tracking the relation between gist and item memory over the course of long - term mem - ory consolidation , Elife 10 , e65588 ( 2021 ) . [ 26 ] R . F . i Cancho and R . V . Sol´e , Least eﬀort and the origins of scaling in human language , Proceedings of the National Academy of Sciences 100 , 788 ( 2003 ) . [ 27 ] P . Zurn and D . S . Bassett , Network architectures sup - porting learnability , Philos Trans R Soc Lond B Biol Sci 375 , 20190323 ( 2020 ) . [ 28 ] C . W . Lynn , L . Papadopoulos , A . E . Kahn , and D . S . Bassett , Human information processing in complex net - works , Nature Physics 16 , 965 ( 2020 ) . [ 29 ] I . Momennejad , E . M . Russek , J . H . Cheong , M . M . Botvinick , N . D . Daw , and S . J . Gershman , The suc - cessor representation in human reinforcement learning , Nat Hum Behav 1 , 680 ( 2017 ) . [ 30 ] K . L . Stachenfeld , M . M . Botvinick , and S . J . Gershman , The hippocampus as a predictive map , Nat Neurosci 20 , 1643 ( 2017 ) . [ 31 ] J . Stiso , C . W . Lynn , A . E . Kahn , V . Rangarajan , K . Szymula , A . Revell , and D . S . Bassett , Neurophys - iological evidence for temporally discounted cognitive map formation during sequence learning , bioRxiv , 1 ( 2021 ) . [ 32 ] T . J . Ryan and P . W . Frankland , Forgetting as a form of adaptive engram cell plasticity , Nature Reviews Neu - roscience , 1 ( 2022 ) . [ 33 ] C . B . Cramer , M . A . Porter , H . Sayama , L . Sheetz , and S . M . Uzzo , Network science in education : Transforma - tional approaches in teaching and learning ( Springer , 2018 ) . [ 34 ] C . S . Q . Siew , Applications of network science to educa - tion research : Quantifying knowledge and the develop - ment of expertise through network analysis , Education Sciences 10 , 101 ( 2020 ) . [ 35 ] E . Yun and Y . Park , Extraction of scientiﬁc seman - tic networks from science textbooks and comparison with science teachers’ spoken language by text network analysis , International Journal of Science Education 40 , 28 2118 ( 2018 ) . [ 36 ] N . H . Christianson , A . Sizemore Blevins , and D . S . Bas - sett , Architecture and evolution of semantic networks in mathematics texts , Proceedings of the Royal Society A 476 , 20190741 ( 2020 ) . [ 37 ] ¯D . Vuki´c , S . Martinˇci´c - Ipˇsi´c , and A . Meˇstrovi´c , Struc - tural analysis of factual , conceptual , procedural , and metacognitive knowledge in a multidimensional knowl - edge network , Complexity 2020 ( 2020 ) . [ 38 ] S . Freeman , S . L . Eddy , M . McDonough , M . K . Smith , N . Okoroafor , H . Jordt , and M . P . Wenderoth , Active learning increases student performance in science , engi - neering , and mathematics , Proceedings of the national academy of sciences 111 , 8410 ( 2014 ) . [ 39 ] S . Denervaud , A . P . Christensen , Y . Kenett , R . E . Beaty , et al . , Education shapes the structure of seman - tic memory and impacts creative thinking , npj Science of Learning 6 , 1 ( 2021 ) . [ 40 ] A . Corbett , L . Kauﬀman , B . Maclaren , A . Wagner , and E . Jones , A cognitive tutor for genetics problem solv - ing : Learning gains and student modeling , Journal of Educational Computing Research 42 , 219 ( 2010 ) . [ 41 ] D . J . Palazzo , Y . - J . Lee , R . Warnakulasooriya , and D . E . Pritchard , Patterns , correlates , and reduction of homework copying , Physical Review Special Topics - Physics Education Research 6 , 010104 ( 2010 ) . [ 42 ] I . T . Koponen and M . Nousiainen , Concept networks of students’ knowledge of relationships between physics concepts : ﬁnding key concepts and their epistemic sup - port , Applied network science 3 , 1 ( 2018 ) . [ 43 ] K . Nilsson , L . Palmqvist , M . Ivarsson , A . Lev´en , H . Danielsson , M . Annell , D . Sch¨old , and M . Socher , Structural diﬀerences of the semantic network in ado - lescents with intellectual disability , Big Data and Cog - nitive Computing 5 , 25 ( 2021 ) . [ 44 ] H . Lommi and I . T . Koponen , Network cartography of university students’ knowledge landscapes about the history of science : landmarks and thematic communi - ties , Applied network science 4 , 1 ( 2019 ) . [ 45 ] I . T . Koponen and M . Nousiainen , Concept networks of students’ knowledge of relationships between physics concepts : ﬁnding key concepts and their epistemic sup - port , Applied network science 3 , 1 ( 2018 ) . [ 46 ] Y . Weinstein , C . R . Madan , and M . A . Sumeracki , Teaching the science of learning , Cognitive Research : Principles and Implications 3 , 2 ( 2018 ) . [ 47 ] H . L . Roediger and J . D . Karpicke , Test - enhanced learn - ing : Taking memory tests improves long - term retention , Psychological science 17 , 249 ( 2006 ) . [ 48 ] S . H . K . Kang and H . Pashler , Learning Painting Styles : Spacing is Advantageous when it Promotes Discrimina - tive Contrast : Spacing promotes contrast , Applied Cog - nitive Psychology 26 , 97 ( 2012 ) . [ 49 ] W . K . Estes , Statistical theory of distributional phe - nomena in learning . , Psychological Review 62 , 369 ( 1955 ) . [ 50 ] D . W . Braithwaite and R . S . Siegler , Children learn spu - rious associations in their math textbooks : Examples from fraction arithmetic . , Journal of Experimental Psy - chology : Learning , Memory , and Cognition 44 , 1765 ( 2018 ) . [ 51 ] D . W . Braithwaite , Pyke , and R . S . Siegler , A com - putational model of fraction arithmetic . , Psychological Review 124 , 603 ( 2017 ) , publisher : American Psycho - logical Association . [ 52 ] A . A . Klishin and D . S . Bassett , Exposure theory for learning complex networks with random walks , Journal of Complex Networks ( in press 2022 ) . [ 53 ] P . Oreopoulos and U . Petronijevic , The remarkable un - responsiveness of college students to nudging and what we can learn from it , Tech . Rep . ( National Bureau of Economic Research , 2019 ) . [ 54 ] E . G . Peterson and J . Cohen , A case for domain - speciﬁc curiosity in mathematics , Educational Psychology Re - view 31 , 807 ( 2019 ) . [ 55 ] P . A . Wo´zniak , E . J . Gorzela´nczyk , and J . A . Mu - rakowski , Two components of long - term memory . , Acta neurobiologiae experimentalis 55 , 301 ( 1995 ) . [ 56 ] D . C . Rubin and A . E . Wenzel , One hundred years of forgetting : A quantitative description of retention . , Psy - chological review 103 , 734 ( 1996 ) . [ 57 ] D . C . Rubin , S . Hinton , and A . Wenzel , The precise time course of retention . , Journal of Experimental Psychol - ogy : Learning , Memory , and Cognition 25 , 1161 ( 1999 ) . [ 58 ] W . Qian , C . W . Lynn , A . A . Klishin , J . Stiso , N . H . Christianson , and D . S . Bassett , Optimizing the human learnability of abstract network representations , arXiv preprint arXiv : 2111 . 12236 ( 2021 ) . [ 59 ] P . A . Ortega and D . A . Braun , Thermodynamics as a theory of decision - making with information - processing costs , Proceedings of the Royal Society A : Mathemat - ical , Physical and Engineering Sciences 469 , 20120683 ( 2013 ) . [ 60 ] T . Castellani and A . Cavagna , Spin - glass theory for pedestrians , Journal of Statistical Mechanics : Theory and Experiment 2005 , P05012 ( 2005 ) . [ 61 ] A . Jafarpour , E . A . Buﬀalo , R . T . Knight , and A . G . Collins , Event segmentation reveals working memory forgetting rate , Iscience 25 , 103902 ( 2022 ) . [ 62 ] L . L¨u and T . Zhou , Link prediction in complex net - works : A survey , Physica A : statistical mechanics and its applications 390 , 1150 ( 2011 ) . [ 63 ] A . Ghasemian , H . Hosseinmardi , A . Galstyan , E . M . Airoldi , and A . Clauset , Stacking models for nearly op - timal link prediction in complex networks , Proceedings of the National Academy of Sciences 117 , 23393 ( 2020 ) . [ 64 ] W . Liu and L . L¨u , Link prediction based on local ran - dom walk , EPL ( Europhysics Letters ) 89 , 58007 ( 2010 ) . [ 65 ] K . Berahmand , E . Nasiri , S . Forouzandeh , and Y . Li , A preference random walk algorithm for link prediction through mutual inﬂuence nodes in complex networks , Journal of King Saud University - Computer and Infor - mation Sciences ( 2021 ) . [ 66 ] S . E . Page , Path dependence , Quarterly Journal of Po - litical Science 1 , 87 ( 2006 ) . [ 67 ] W . B . Arthur , Increasing returns and path dependence in the economy ( University of Michigan Press , 1994 ) . [ 68 ] L . Magnusson and J . Ottosson , The evolution of path dependence ( Edward Elgar Publishing , 2009 ) . [ 69 ] J . Bednar , A . Jones - Rooy , and S . E . Page , Choosing a future based on the past : Institutions , behavior , and path dependence , European Journal of Political Econ - omy 40 , 312 ( 2015 ) . [ 70 ] P . B . Kalra , J . D . Gabrieli , and A . S . Finn , Evidence of stable individual diﬀerences in implicit learning , Cogni - tion 190 , 199 ( 2019 ) . [ 71 ] S . C . Pan and R . A . Bjork , Acquiring an accurate men - tal model of human learning : Toward an owner’s man - 29 ual , in Oxford Handbook of Memory , Vol . 2 , edited by A . Wagner and M . J . Kahana ( 2020 ) . [ 72 ] B . C . Storm and R . A . Bjork , Do learners predict a shift from recency to primacy with delay ? , Memory & Cognition 44 , 1204 ( 2016 ) . [ 73 ] J . C . Hulbert and K . A . Norman , Neural Diﬀerentiation Tracks Improved Recall of Competing Memories Follow - ing Interleaved Study and Retrieval Practice , Cerebral Cortex 25 , 3994 ( 2015 ) . [ 74 ] R . A . Bjork and E . L . Bjork , A new theory of disuse and an old theory of stimulus ﬂuctuation , in From learn - ing processes to cognitive processes : Essays in honor of William K . Estes , Vol . 2 , edited by A . Healy , S . Koss - lyn , and R . Shriﬃn ( Erlbaum , Hillsdale , N . J , 1992 ) pp . 35 – 67 . [ 75 ] L . Deslauriers , E . Schelew , and C . Wieman , Improved learning in a large - enrollment physics class , science 332 , 862 ( 2011 ) . [ 76 ] S . M . Polyn , K . A . Norman , and M . J . Kahana , A con - text maintenance and retrieval model of organizational processes in free recall , Psychological Review 116 , 129 ( 2009 ) , place : US Publisher : American Psychological Association . [ 77 ] T . T . Hills , M . N . Jones , and P . M . Todd , Optimal for - aging in semantic memory . , Psychological review 119 , 431 ( 2012 ) . [ 78 ] J . T . Abbott , J . L . Austerweil , and T . L . Griﬃths , Ran - dom walks on semantic networks can resemble optimal foraging . , Psychological Review 122 , 558 ( 2015 ) . [ 79 ] M . Naim , M . Katkov , S . Romani , and M . Tsodyks , Fun - damental law of memory recall , Physical review letters 124 , 018101 ( 2020 ) . [ 80 ] P . K . Kollepara , A . F . Siegenfeld , N . N . Taleb , and Y . Bar - Yam , Unmasking the mask studies : why the eﬀectiveness of surgical masks in preventing respira - tory infections has been underestimated , arXiv preprint arXiv : 2102 . 04882 ( 2021 ) . [ 81 ] D . M . Romero , B . Meeder , and J . Kleinberg , Diﬀerences in the mechanics of information diﬀusion across topics : idioms , political hashtags , and complex contagion on twitter , in Proceedings of the 20th international confer - ence on World wide web ( 2011 ) pp . 695 – 704 . [ 82 ] D . Guilbeault , J . Becker , and D . Centola , Complex con - tagions : A decade in review , Complex spreading phe - nomena in social systems , 3 ( 2018 ) . [ 83 ] N . Perra , A . Baronchelli , D . Mocanu , B . Gon¸calves , R . Pastor - Satorras , and A . Vespignani , Random walks and search in time - varying networks , Physical review letters 109 , 238701 ( 2012 ) . [ 84 ] A . J . Allen , C . Moore , and L . H´ebert - Dufresne , A net - work compression approach for quantifying the impor - tance of temporal contact chronology , arXiv preprint arXiv : 2205 . 11566 10 . 48550 / ARXIV . 2205 . 11566 ( 2022 ) . [ 85 ] K . A . Snyder , M . P . Blank , and C . J . Marsolek , What form of memory underlies novelty preferences ? , Psycho - nomic bulletin & review 15 , 315 ( 2008 ) . [ 86 ] E . Mather , Novelty , attention , and challenges for de - velopmental psychology , Frontiers in psychology 4 , 491 ( 2013 ) . [ 87 ] M . Bonaventura , V . Nicosia , and V . Latora , Character - istic times of biased random walks on complex networks , Physical Review E 89 , 012803 ( 2014 ) . [ 88 ] H . F . de Arruda , F . N . Silva , L . d . F . Costa , and D . R . Amancio , Knowledge acquisition : A complex networks approach , Information Sciences 421 , 154 ( 2017 ) . [ 89 ] C . S . Adorf , P . M . Dodd , V . Ramasubramani , and S . C . Glotzer , Simple data and workﬂow management with the signac framework , Comput . Mater . Sci . 146 , 220 ( 2018 ) . [ 90 ] C . S . Adorf , V . Ramasubramani , B . D . Dice , M . M . Henry , P . M . Dodd , and S . C . Glotzer , glotzerlab / signac ( 2019 ) . [ 91 ] S . M . Mitchell , S . Lange , and H . Brus , Gendered cita - tion patterns in international relations journals , Inter - national Studies Perspectives 14 , 485 ( 2013 ) . [ 92 ] M . L . Dion , J . L . Sumner , and S . M . Mitchell , Gen - dered citation patterns across political science and so - cial science methodology ﬁelds , Political Analysis 26 , 312 ( 2018 ) . [ 93 ] N . Caplar , S . Tacchella , and S . Birrer , Quantitative eval - uation of gender bias in astronomical publications from citation counts , Nature Astronomy 1 , 0141 ( 2017 ) . [ 94 ] D . Maliniak , R . Powers , and B . F . Walter , The gen - der citation gap in international relations , International Organization 67 , 889 ( 2013 ) . [ 95 ] J . D . Dworkin , K . A . Linn , E . G . Teich , P . Zurn , R . T . Shinohara , and D . S . Bassett , The extent and drivers of gender imbalance in neuroscience reference lists , Nature Neuroscience 23 , 918 ( 2020 ) . [ 96 ] M . A . Bertolero , J . D . Dworkin , S . U . David , C . L . Lloreda , P . Srivastava , J . Stiso , D . Zhou , K . Dzirasa , D . A . Fair , A . N . Kaczkurkin , B . J . Marlin , D . Shohamy , L . Q . Uddin , P . Zurn , and D . S . Bassett , Racial and ethnic imbalance in neuroscience reference lists and in - tersections with gender , bioRxiv ( 2020 ) . [ 97 ] X . Wang , J . D . Dworkin , D . Zhou , J . Stiso , E . B . Falk , D . S . Bassett , P . Zurn , and D . M . Lydon - Staley , Gen - dered citation practices in the ﬁeld of communication , Annals of the International Communication Association 10 . 1080 / 23808985 . 2021 . 1960180 ( 2021 ) . [ 98 ] P . Chatterjee and R . M . Werner , Gender disparity in citations in high - impact journal articles , JAMA Netw Open 4 , e2114509 ( 2021 ) . [ 99 ] J . M . Fulvio , I . Akinnola , and B . R . Postle , Gender ( im ) balance in citation practices in cognitive neuro - science , J Cogn Neurosci 33 , 3 ( 2021 ) . [ 100 ] E . G . Teich , J . Z . Kim , C . W . Lynn , S . C . Simon , A . A . Klishin , K . P . Szymula , P . Srivastava , L . C . Bassett , P . Zurn , J . D . Dworkin , and D . S . Bassett , Citation in - equity and gendered citation practices in contemporary physics , arXiv preprint arXiv : 2112 . 09047 ( 2021 ) . [ 101 ] D . Zhou , E . J . Cornblath , J . Stiso , E . G . Teich , J . D . Dworkin , A . S . Blevins , and D . S . Bassett , Gender di - versity statement and code notebook v1 . 0 ( 2020 ) . [ 102 ] A . Ambekar , C . Ward , J . Mohammed , S . Male , and S . Skiena , Name - ethnicity classiﬁcation from open sources , in Proceedings of the 15th ACM SIGKDD inter - national conference on Knowledge Discovery and Data Mining ( 2009 ) pp . 49 – 58 . [ 103 ] G . Sood and S . Laohaprapanon , Predicting race and ethnicity from the sequence of characters in a name , arXiv preprint arXiv : 1805 . 02109 ( 2018 ) .