LETTER Communicated by Cian O’Donnell Spike - Timing - Dependent Construction Toby Lightheart toby . lightheart @ adelaide . edu . au Steven Grainger steven . grainger @ adelaide . edu . au Tien - Fu Lu tien - fu . lu @ adelaide . edu . au School of Mechanical Engineering , University of Adelaide , Adelaide SA 5005 , Australia Spike - timing - dependent construction ( STDC ) is the production of new spiking neurons and connections in a simulated neural network in response to neuron activity . Following the discovery of spike - timing - dependent plasticity ( STDP ) , signiﬁcant effort has gone into the mod - eling and simulation of adaptation in spiking neural networks ( SNNs ) . Limitations in computational power imposed by network topology , how - ever , constrain learning capabilities through connection weight modi - ﬁcation alone . Constructive algorithms produce new neurons and con - nections , allowing automatic structural responses for applications of unknown complexity and nonstationary solutions . A conceptual anal - ogy is developed and extended to theoretical conditions for modeling synaptic plasticity as network construction . Generalizing past construc - tive algorithms , we propose a framework for the design of novel con - structive SNNs and demonstrate its application in the development of simulations for the validation of developed theory . Potential directions of future research and applications of STDC for biological modeling and machine learning are also discussed . 1 Introduction Processes that adapt synapse efﬁcacy ( Caporale & Dan , 2008 ) and the num - ber and arrangement of neurons ( Abrous , Koehl , & Le Moal , 2005 ) and connections ( Holtmaat & Svoboda , 2009 ) have a signiﬁcant role for learn - ing in biological and artiﬁcial neural networks . Spike - timing - dependent plasticity ( STDP ) has attracted great interest as a biological process of mod - ifying synapse efﬁcacy according to the spike timing of connected neurons ( Gerstner & Kistler , 2002 ; Caporale & Dan , 2008 ; Morrison , Diesmann , & Gerstner , 2008 ) . Within a network , a neuron may be identiﬁed according to its relationship to a connection , or synapse , of interest , that is , a presynap - tic neuron spike propagates through a synapse to the postsynaptic neuron Neural Computation 25 , 2611 – 2645 ( 2013 ) c (cid:2) 2013 Massachusetts Institute of Technology doi : 10 . 1162 / NECO _ a _ 00501 2612 T . Lightheart , S . Grainger , and T . - F . Lu ( Gerstner & Kistler , 2002 ) . A common variant of STDP studied and mod - eled ( Caporale & Dan , 2008 ; Morrison et al . , 2008 ) increases the efﬁcacy of a synapse when the postsynaptic neuron spikes after the presynaptic neuron and decreases the efﬁcacy when the presynaptic neuron spikes after the postsynaptic neuron . Spiking neural networks ( SNNs ) have a theoretical advantage in com - putational power ( Maass , 1996 , 1997b ) and greater biological relevance ( Maass , 1997a ) over networks of sigmoidal neurons . This has attracted at - tention within the ﬁelds of artiﬁcial intelligence and robotics ( Floreano , Epars , Zufferey , & Mattiussi , 2006 ; Arena , Fortuna , Frasca , & Patan´e , 2009 ) , with learning demonstrated in applications using simulated SNNs with STDP ( Masquelier & Thorpe , 2007 ; Di Paolo , 2003 ; Carrillo , Ros , Boucheny , & Coenen , 2008 ) . From a machine learning perspective , learning processes for SNNs can be classiﬁed as reinforcement learning ( Seung , 2003 ; Florian , 2007 ) , supervised learning ( for a review , see Kasi´nski & Ponulak , 2006 ) , or unsupervised learning ( Bohte , La Poutre , & Kok , 2002 ) . By applying a teaching signal to postsynaptic neurons to trigger spikes at desired times , supervised learning can be achieved through the STDP process ( Legenstein , Naeger , & Maass , 2005 ) . More commonly , STDP operates without teach - ing signals or feedback and may be described as a form of unsupervised learning . Despite connection weight modiﬁcation , the computational power of an artiﬁcial neural network is limited by the number and arrangement of neu - rons and connections ( Maass , 1996 ; Huang & Babri , 1998 ) . Artiﬁcial neural networks that adapt the network structure by adding connections , neurons , or layers are referred to as constructive neural networks or growing neural networks and , less commonly , as evolving connectionist systems . A con - structive algorithm details the process for adding neurons and connections to a network . Automatic structural adaptation relieves the need to select the network size prior to operation and may provide greater capacity to handle learning applications with unknown complexity or nonstationary solutions . Constructive algorithms for nonspiking networks exist , many performing unsupervised learning ( Fritzke , 1995 ; Bruske & Sommer , 1995 ) and super - vised learning ( for a review , see Nicoletti , Bertini , Elizondo , Franco , & Jerez , 2009 ; Watts , 2009 ) . Constructive algorithms for SNNs , however , have yet to receive similar attention or depth of study . When a simulated SNN is viewed as a small subgroup of neurons within a large neural system , such as a mammalian brain , network construction may be conceptually linked to synaptic plasticity . Assuming the existence of many neurons and combinations of connections that are not simulated but have connections with simulated neurons , an external neuron could be assumed to spike immediately after any simulated spike pattern . Un - der Hebbian models of STDP , this activation would result in an increase in synapse efﬁcacies from recently active simulated neurons . The poten - tiation of these connections would cause the activation of that external Spike - Timing - Dependent Construction 2613 Large Neural System Simulated Network Postsynaptic Neurons Presynaptic Neurons Figure 1 : Concept of network construction by network expansion . Simulated neurons are assumed to connect to neurons in the large neural system that are not simulated . Should simulated presynaptic neurons ﬁre , it can be assumed that an outside postsynaptic neuron ﬁres and , through STDP , the connections are strengthened . STDC may be considered a process of adding these assumed neurons to the simulated network . neuron to become causally associated with simulated neurons ; therefore , the simulated network could be expanded to include that external postsy - naptic neuron ( see Figure 1 ) . As such , the introduction of connections and neurons to a simulated network may be treated as an approximation of biological processes . Spike - timing - dependent construction ( STDC ) occurs when an algorithm initiates neuron or connection construction , or computes their parameter values , from measures of neuron spike timing . Though this letter intro - duces the generalized concept of STDC , algorithms have been previously developed that construct neurons and connections using spike timing ( Takita & Hagiwara , 2005 ; Wysoski , Benuskova , & Kasabov , 2006 ; Light - heart , Grainger , & Lu , 2010 ) . Nevertheless , past work has not investigated the relationship of STDC with STDP or sought to formalize a general frame - work for specifying and designing constructive SNNs . The contributions of this letter include theory on the approximation of a model of STDP using STDC , developed in section 2 , and the development of a generalized framework for the design of constructive SNNs , presented in section 3 . The proposal of a base set of constructive algorithm components forms a generalized framework for the design of constructive SNNs . This framework is demonstrated through the design of an example constructive algorithm , developed from theoretical ﬁndings on STDC , and applied to 2614 T . Lightheart , S . Grainger , and T . - F . Lu a network of Izhikevich model neurons ( Izhikevich , 2003 ) . This example algorithm has been simulated to validate the conceptual and theoretical assertions that STDC can approximate STDP ; the results are presented in section 4 . The relevance of STDC to biological modeling and machine learn - ing , including potential future developments , is discussed in section 5 . 2 Constructive Approximation of an STDP Model Viewing a simulated SNN as a small group within a large neural system allows network construction to be equated to expanding the simulated net - work to include external neurons and connections ( see Figure 1 ) . Simulated models of STDP are capable of converging to stable distributions of synapse efﬁcacies ( Abbott & Nelson , 2000 ; van Rossum , Bi , & Turrigiano , 2000 ; Watt & Desai , 2010 ) . Given the long - term existence of a large neural system , spike patterns of interest can be assumed to have occurred many times ; therefore , the STDP of connection weights may be assumed to have converged prior to the start of the simulation . These assumptions potentially provide com - putable approximations of STDP models that can be applied to network construction . Neuron connection weights adapted through additive STDP ( Song , Miller , & Abbott , 2000 ) are often bound to a constant interval , w ∈ [ w min , w max ] , to prevent divergence . Instead , connections converge to a bimodal distribution , that is , to the minimum or maximum weight ( Song et al . , 2000 ; van Rossum et al . , 2000 ) . Additive STDP may be represented graphically as an update window ( see Figure 2 ) or by these equations : w = w + (cid:2)w , ( 2 . 1 ) (cid:2)w = (cid:2) W + · e (cid:2) t / τ + , if (cid:2) t < 0 , − W − · e − (cid:2) t / τ − , if (cid:2) t ≥ 0 . ( 2 . 2 ) where (cid:2) t = t fpre − t fpost . ( 2 . 3 ) Equations 2 . 1 to 2 . 3 produce asymmetrical Hebbian weight updates , (cid:2)w , for connection weights , w , given weight update factors W + , W − > 0 and STDP curve time constants τ + , τ − > 0 . The spike time difference , (cid:2) t , is calculated from the presynaptic spike time , t fpre , and postsynaptic spike time , t fpost , using the deﬁnition from Gerstner and Kistler ( 2002 ) . A list of the symbols used is provided in appendix B . Consider an SNN with negligible synapse transmission delay and a set of simulated neurons assumed to be presynaptic , N pre . Taking any time in - stant to be t = 0 gives a subset , F , of n presynaptic neurons that have ﬁred previously in the time interval [ − θ t , 0 ) . The interval limit , θ t , is the spike time threshold , where θ t > 0 . Given a postsynaptic neuron that spikes at Spike - Timing - Dependent Construction 2615 Figure 2 : An example STDP update window produced from equations 2 . 1 to 2 . 3 . Thechangeinweight , (cid:2)w , varieswithrespecttothedifferenceinspiketimes of the presynaptic neuron , t fpre ; postsynaptic neuron , t fpost ; amplitude parameters , W + and W − ; and time constants , τ + and τ − . Note that the minimum value of an STDP update is positive for presynaptic neurons that ﬁre up to the spike time threshold , θ t , prior to the spike time of the postsynaptic neuron . the relative time instant t fpost = 0 , the resulting additive STDP update ( see equations 2 . 1 – 2 . 3 ) produces a positive weight change in connections from F . Assuming this pattern of activity repeats and that counteracting updates do not occur , it will be shown that connection weights from F converge to the limit , w max , in a ﬁnite number of iterations . Therefore , construction of a neuron with maximum weight connections from F approximates the con - vergence of additive STDP under the assumed conditions . Approximation accuracy can be quantitatively assessed using the p - norm distance of vector representations of connection weights . The p - norm distance is deﬁned as d p ( x , y ) = (cid:3) n (cid:4) i = 1 | x i − y i | p (cid:5) 1 / p , ( 2 . 4 ) for n - length vectors x , y ∈ R n and 1 ≤ p < ∞ . 2616 T . Lightheart , S . Grainger , and T . - F . Lu Theorem 1 . For a postsynaptic neuron spike at t fpost = 0 , the set of n presynaptic neurons , F , that have spike times in [ − θ t , 0 ) have connection weight increased , up to the limit w max , under additive STDP equations 2 . 1 to 2 . 3 . Assuming no counteracting updates , a postsynaptic neuron constructed with connections from F of weight w max approximates the connection weights from F of a postsynaptic neuron that has experienced m iterations of this spike pattern and resulting STDP updates . The 1 - norm distance of the constructed and updated weight vectors for connections from F is d 1 ≤ n · max ( 0 , w max − w min − m · W + · e − θ t / τ + ) . Proof . Given that the connected postsynaptic neuron ﬁres at t fpost = 0 and the vector of presynaptic spike times t fF ∈ [ − θ t , 0 ) n , the vector of spike time differences is (cid:2) t ∈ [ − θ t , 0 ) n . As equation 2 . 2 is positive and monotonically increasing in the region (cid:2) t < 0 , the presynaptic neurons that ﬁre up to time θ t prior to the ﬁring of a connected postsynaptic neuron have weight updates (cid:2)w ≥ W + · e − θ t / τ + . ( 2 . 5 ) Bounding the connection weights to the postsynaptic neuron in the range [ w min , w max ] , from equation 2 . 5 and m iterations of the spike pattern , the ﬁnal weight of each connection is w final ≥ min ( w max , w min + m · W + · e − θ t / τ + ) , ( 2 . 6 ) where min ( . ) is an operation that takes the minimum enclosed element as the result . Each of the n connection weights from F to the constructed neuron is given as w max . It then follows that , using equations 2 . 4 and 2 . 6 , the 1 - norm distance of the constructed weight vector and updated weight vector of F resulting from m STDP updates is d 1 = n (cid:4) i = 1 | w max − w i | d 1 ≤ n (cid:4) i = 1 | w max − min ( w max , w min + m · W + · e − θ t / τ + ) | d 1 ≤ n · max ( 0 , w max − w min − m · W + · e − θ t / τ + ) . ( 2 . 7 ) The maximum operation , max ( . ) , returns the maximum enclosed ele - ment and has a relationship to the minimum operation of max ( x , y ) = − min ( − x , − y ) . Equation 2 . 7 concludes the proof of theorem 1 . In this model , the 1 - norm distance of connections from F updated under the STDP window and those of the constructed neuron falls to zero for w max ≤ w min + m · W + · e − θ t / τ + . This would require the number of iterations Spike - Timing - Dependent Construction 2617 of a given pattern of neural activity and weight updates under additive STDP to be m ≥ w max − w min W + · e − θ t / τ + . ( 2 . 8 ) It can be further argued that connections from presynaptic neurons that do not spike in the time interval [ − θ t , 0 ) , prior to the postsynaptic neuron at t fpost = 0 , can be approximated as having minimum weight or no connection . Consider two possible cases of relative presynaptic neuron spike times : presynaptic neurons ﬁre at the same time or after the postsynaptic neuron in the interval [ 0 , θ t ] , or presynaptic neurons ﬁre in either ( −∞ , − θ t ) or ( θ t , ∞ ) . In the ﬁrst case , connections are depressed according to the additive STDP model . By substituting STDP parameters for depression into the proof for theorem 1 , it can be shown that the same assumptions produce connection weights depressed toward the minimum weight , w min : Theorem 2 . For a postsynaptic neuron spike at t fpost = 0 , the set of n presynaptic neurons , F , that have spike times in [ 0 , θ t ] , have connection weights decreased , down to the limit w min , under additive STDP equations 2 . 1 to 2 . 3 . Assuming no counteracting updates , a postsynaptic neuron constructed with connections from F of weight w min approximates the connection weights from F of a postsynaptic neuron that has experienced m iterations of this spike pattern and resulting STDP updates . The 1 - norm distance of the constructed and updated weight vectors for connections from F is d 1 ≤ n · | min ( 0 , w min − w max + m · W − · e − θ t / τ − ) | . Proof . Given that the connected postsynaptic neuron ﬁres at t fpost = 0 and thevectorofpresynapticspiketimes t f F ∈ [ 0 , θ t ] n , thevectorofspiketimedif - ferences is (cid:2) t ∈ [ 0 , θ t ] n . In the region (cid:2) t > 0 , equation 2 . 2 is monotonically increasing but remains negative . Any connection between a presynaptic neuron that ﬁres up to time θ t after the ﬁring of the postsynaptic neuron has its weight updated : (cid:2)w ≤ − W − · e − θ t / τ − . ( 2 . 9 ) Bounding the connection weights to the postsynaptic neuron in the range [ w min , w max ] , from equation 2 . 9 and m iterations of the spike pattern , we ﬁnd that the ﬁnal weight of each connection is w final ≤ max ( w min , w max − m · W − · e − θ t / τ − ) . ( 2 . 10 ) Each of the n connection weights from F to the constructed neuron is given as w min . It then follows that when equations 2 . 4 and 2 . 10 are used , the 1 - norm distance of the weight vector of the constructed neuron and weight 2618 T . Lightheart , S . Grainger , and T . - F . Lu vector resulting from m STDP updates is d 1 = n (cid:4) i = 1 | w min − w i | , d 1 ≤ n (cid:4) i = 1 | w min − max ( w min , w max − m · W − · e − θ t / τ − ) | , d 1 ≤ n · | min ( 0 , w min − w max + m · W − · e − θ t / τ − ) | . ( 2 . 11 ) The second case of presynaptic neuron activity , not covered by theorem 1 or 2 , is that of spiking in either the ( −∞ , θ t ) or ( θ t , ∞ ) time intervals . Outside of the interval [ − θ t , θ t ] , it may be assumed that presynaptic neuron activity is not correlated with the activation of the postsynaptic neuron . For uncorrelated activity—presynaptic neurons that activate randomly both be - fore and after the postsynaptic neuron—the additive model of STDP does not guarantee weight convergence . For antisymmetrical STDP , τ + = τ − and W + = W − , a uniform distribution of presynaptic spike times will produce a random connection weight bounded in [ w min , w max ] . If the area under the positive side of the STDP curve is less than the area under the negative side of the STDP curve , (cid:6) 0 − −∞ W + · e (cid:2) t / τ + d (cid:2) t < | (cid:6) ∞ 0 − W − · e − (cid:2) t / τ − d (cid:2) t | , then the distribution of synapse weights will tend toward the minimum for in - creasing iterations of uniformly distributed presynaptic spike times ( Song et al . , 2000 ) . Thus , construction could approximate convergence through constructing connections from presynaptic neurons assumed to have un - correlated activity with minimum weight , w min , or no connection . Note that the theory developed here has considered only a simple additive model of STDP and assumes no interactions countering the con - vergence of weights . Theorems describing constructive approximation ac - curacy for different interactions and models of STDP may be provable but are not treated in this letter . These conceptual and theoretical developments indicate that constructive algorithms can produce connection weights in ap - proximation of an STDP model and have been experimentally tested ( see section 4 ) . 3 Novel Constructive Algorithm Design The coordination of constructive algorithm and neural network design is necessary for effective structural modiﬁcations to take place . Constructive algorithm designs may be tailored to a speciﬁc neuron or synapse model , network architecture , or application ; however , the general components and processes of constructive SNNs are relatively consistent . Constructed neu - rons will typically have a presynaptic or postsynaptic relationship to exist - ing neurons and each other . Similarly , constructed synapses will cause new Spike - Timing - Dependent Construction 2619 pre - and postsynaptic relationships to be formed . The algorithm may also be divided into functions to evaluate network conditions and trigger con - struction , and functions to compute the parameter values and properties of new components . Therefore , a minimal instance of a constructive algorithm could be considered to be composed of : (cid:2) A set of presynaptic neurons , N pre (cid:2) A set of postsynaptic neurons , N post (cid:2) A set of synapses , S (cid:2) A set of functions , G , computing whether the conditions for construc - tion are met (cid:2) A set of functions , C , computing parameter values for new neurons and synapses These components form a general framework for describing the relation - ship between the network and the basic processes for construction . This proposed framework accommodates existing constructive SNNs , facilitat - ing their characterization and comparison , and the investigation and reuse of constructive processes . This framework may also serve as a basis for the design of novel constructive algorithms , with applications potentially including the modeling of biological neural networks ( see section 5 . 1 ) and machine learning ( see section 5 . 2 ) . 3 . 1 Network Architectures and Constructive Processes . The proposed constructive algorithm framework can be used to specify a network archi - tecture and its interaction with constructive processes . When neurons are allowed to be members of multiple sets , multilayer and recurrent networks may be deﬁned as collections of presynaptic and postsynaptic neuron set pairs and associated sets of synapses ( see Figure 3 ) . For the purposes of a constructive algorithm , the neurons assigned to presynaptic and post - synaptic sets , N pre and N post , respectively , might not initially possess the required synapses , S , to have a presynaptic - postsynaptic relationship . In - stead , the speciﬁcation of these neuron sets deﬁnes the relationships to be developed through the construction of neurons and synapses . Therefore , at initialization , N pre , N post , and S could be empty . This deﬁnition relates to the concept of existing external neurons and connections that are not simulated ( see Figure 1 ) . The sets of functions that trigger construction , G , and compute the pa - rameter values of constructed components , C , will typically be associated with a single pair of neuron sets , N pre and N post , and set of synapses , S . Accordingly , the function will create the new connections and neurons , and the association with neuron and synapse sets will specify their loca - tion . The constructive algorithm framework is not speciﬁc to any neuron or synapse model , but processes for computing parameter values must be compatible . Designed with the intention of performing machine learning , 2620 T . Lightheart , S . Grainger , and T . - F . Lu D C A B Figure 3 : Examples of constructive network architectures . ( A ) Exclusive presy - naptic ( N pre ) and postsynaptic ( N post ) layers . ( B ) A chain of z pairs of presynaptic and postsynaptic layers . ( C ) Recurrently connected , nonexclusive presynaptic layer and postsynaptic layers . ( D ) Parallel pairs of exclusive presynaptic and postsynaptic layers , converging on a single postsynaptic layer . these functions may be deﬁned according to unsupervised , supervised , and reinforcement learning principles . Unsupervised construction relies solely on network input and neuron activity ; no teaching signals or known associated outputs are available . For example , unsupervised construction could occur in direct response to unique activity in the network input layer ( Takita & Hagiwara , 2005 ; Light - heart et al . , 2010 ) . Alternatively , the inference of activity , and STDP , in a postsynaptic neuron outside the simulation could trigger construction . In - ference processes could treat all presynaptic spikes as causing postsynaptic spikes or compute postsynaptic spike probabilities and detect recurring patterns from accumulated activity data . Candidate postsynaptic neurons Spike - Timing - Dependent Construction 2621 could also be simulated with synaptic plasticity and conditions applied to candidates for consolidation and rejection . Supervised learning algorithms typically operate using input signals paired with known output values or teaching signals . As such , network con - struction could be triggered using thresholds for error feedback from clas - siﬁcation or function approximation . Alternatively , constructive processes could be triggered at a set time after applying training inputs ( Wysoski et al . , 2006 ) or in response to precisely timed teaching signals . Construction conditions developed from reinforcement learning include the detection of a rewarding state ( Liu & Buller , 2005 ) and the use of a reward feedback threshold ( Huemer , Elizondo , & Gongora , 2009 ) . The selection of neurons to connect , and the quantity of excitatory and inhibitory neurons to create , will typically be determined during evaluation of the conditions for construction . The neuron and connection parameter values , however , will be largely dependent on the neuron models used and the network architecture . Connection weights may be computed to approximate a model of STDP ( see section 2 ) , which may result in con - structed connections being assigned equal weight ( see also Takita & Hagi - wara , 2005 ; Lightheart et al . , 2010 ) . Alternative methods for computing connection weights from spike timing are possible , for example , assigning descending connection weight magnitudes in the order that presynaptic neurons spike ( Wysoski et al . , 2006 ) . Criteria can also be applied to cancel construction . For example , the construction of neurons with a connection weight vector within a distance threshold of an existing neuron may be considered redundant and canceled ( Fritzke , 1995 ; Wysoski et al . , 2006 ) . If construction is canceled only in the event of the equality of connection sets ( Takita & Hagiwara , 2005 ; Lightheart et al . , 2010 ) , this is equivalent to using a weight vector distance threshold of zero . Given that activation of spiking neurons is not in general linearly dependent on connection weights , the detection of simultaneous neuron activity could provide a more effective measure of redundancy . Processes that are complementary to construction can also be introduced . Common complementary processes include adaptation , the application of a learning rule to update the connections of neurons in the network , and pruning , the removal of neurons or connections from the network . Potential adaptation processes include the application of STDP after construction and the weighted merging of candidate and existing neurons with connection weight vectors within a distance threshold ( Wysoski et al . , 2006 ) . The in - corporation of adaptation , or iterative processes in construction , can extend constructive algorithms beyond one - shot or instance - based learning ( Aha , Kibler , & Albert , 1991 ) . Pruning processes may include removing neurons and connections that do not meet performance criteria or are otherwise deemed erroneous and removing neurons that have activity or connection weights that are not sufﬁciently unique . 2622 T . Lightheart , S . Grainger , and T . - F . Lu Though many of the processes above have been features of algorithms for STDC ( Takita & Hagiwara , 2005 ; Wysoski et al . , 2006 ; Lightheart et al . , 2010 ) , the investigation of their learning performance and capabilities typ - ically focuses on practical applications . Future research should investigate the learning capabilities of constructive algorithms for different network architectures , as well as the effects of complementary processes , in theory and in practice . 3 . 2 Algorithms for Spike - Timing - Dependent Construction . Spike - timing - dependent construction results from constructive algorithms that use spike timing , directly or indirectly , as an input parameter in any con - structive process . Among identiﬁed constructive SNNs , a number of con - structive algorithms rely either directly on spike timing ( Lightheart et al . , 2010 ) or indirectly , such as through measures of neuron refractoriness ( Takita & Hagiwara , 2002 , 2005 ) or spike order ( Wysoski et al . , 2006 ) . Al - though the core elements of these algorithms use complementary processes , they ﬁt within the general constructive algorithm framework . As a result , these constructive algorithms may be uniﬁed as a family of algorithms for STDC . The most basic of these related constructive algorithms operate on a simulation of leaky integrate - and - ﬁre neurons using coarse , 20 ms , time - steps ( Lightheart et al . , 2010 ) . The SNN is initially composed of a layer of input neurons ( N 1 pre ) , an empty layer of hidden neurons ( N 1 post , N 2 pre ) , and a layer of output neurons ( N 2 post ) . During network simulation , construction of a new hidden neuron is initiated by one or more input neurons exceeding their threshold potential and ﬁring within a 20 ms time step . New neurons are given connections from all active input neurons , normalized to sum to a total weight . Connections to output neurons are generated at random and adapted by reinforcement feedback . If a hidden neuron is detected with the same set of input connections as that being created , construction is canceled to prevent redundancy . An alternative construction process ( Takita & Hagiwara , 2002 , 2005 ) uses a threshold for individual neuron refractoriness to initiate construction . Construction was applied to a feedforward network architecture , possess - ingoneinputlayer , twogrowinghiddenlayers , andanoutputlayer . Though the constructive process is described as “simple” by the authors , under the proposed constructive algorithm framework , it can be reduced to three processes . The ﬁrst process creates a neuron in the ﬁrst hidden layer , with a predeﬁned total weight divided across connections from input neurons above the refractoriness threshold . The second process creates a neuron in the second hidden layer , with a connection from the neuron created in the ﬁrst hidden layer , to relay activity . The third process creates stochastic con - nections from the second - layer neuron to all output neurons . A complex constructive process is implemented , but a description has been omitted here for brevity . Spike - Timing - Dependent Construction 2623 Refractoriness variables , used to temporarily decrease the potential of neurons after they spike , was maintained using this equation : R n [ t + 1 ] = (cid:2) k ref , if O n [ t ] = 1 , ( 1 − d n ) · R n [ t ] , if O n [ t ] = 0 . When the same notation as Takita and Hagiwara ( 2005 ) is used , R n [ t ] is the refractoriness of a neuron , n , at time - step t ; k ref is a constant refractoriness given to a neuron after spiking in the previous time step , indicated as O n [ t ] = 1 ; and d n is the refractoriness decay constant . Given a constant reset value of refractoriness after a spike , a refractori - ness threshold , θ n , is effectively also a measure of the time that has passed since a neuron spike . The refractoriness threshold can therefore be con - verted into a spike time threshold , θ t = log ( 1 − d n ) (cid:3) θ n k ref (cid:5) . Therefore , as well as being STDC , this constructive process could be repro - duced without the need for a neuron model with a refractoriness variable . The last related constructive SNN identiﬁed , the evolving spiking neural network ( Wysoski et al . , 2006 ) , is designed to construct neurons using spike order or rank order coding ( Delorme & Thorpe , 2001 ; Thorpe , Delorme , & Van Rullen , 2001 ) . During training , the network is reset prior to the ap - plication of the input ; then the order of spikes in the presynaptic neuron set is recorded . At the end of a given time for applying the training input , a postsynaptic neuron is constructed with connection weight magnitudes from active presynaptic neurons descending in the order of spike arrival ( from ﬁrst to last ) . An equivalent constructive SNN can be realized with STDC through the supervised triggering of construction after having ap - plied the training input for a set time . Using a record of neuron spike times , it would be possible to compute the spike order using any standard sorting algorithm . As each of these past constructive algorithms may be viewed as imple - mentations of STDC , the applications of STDC are assumed to include those demonstrated in past work . These applications include inverted pendulum balancing ( Takita & Hagiwara , 2005 ; Lightheart et al . , 2010 ) , simulated com - petition of mobile agents ( Takita & Hagiwara , 2005 ) , and pattern recogni - tion in sensory domains including taste ( Soltic , Wysoski , & Kasabov , 2008 ) and audiovisual data ( Wysoski , Benuskova , & Kasabov , 2010 ) . Producing complete reproductions of past algorithms was deemed to be beyond the scope of this letter . Rather , an example of a novel constructive algorithm is 2624 T . Lightheart , S . Grainger , and T . - F . Lu Algorithm 1 : Unsupervised STDC . Given : θ t , θ d , n , n pre Input : n post , A , T f , S Output : N , n post , T f , S Initially : N = NULL 1 : If n post < n max 2 : [ T f , f ] : = UpdateTimesFired ( n pre , A , T f ) 3 : If f = true 4 : [ W , n : = ] CreateConnections ( n pre , θ t , T f ) 5 : x : = EvaluateConnections ( n post , n pre , θ d , S , W , n ) 6 : If x = true 7 : N : = CreateNeuron ( W ) 8 : n post : = n post + 1 9 : S [ n post ] : = W described below , including a simple variation to achieve forms of super - vised and unsupervised learning . 3 . 3 Example Constructive Algorithm . To demonstrate a potential con - structive algorithm design outcome , pseudocode has been provided for an unsupervised algorithm ( algorithm 1 ) and a supervised algorithm ( algorithm 2 ) . The algorithms have been divided into functions to : update a record of presynaptic neuron spike times ( function 1 ) , create connections for a new candidate neuron ( function 2 ) , evaluate the proposed connections for redundancy ( function 3 ) , and specify model parameters for the new spiking neuron ( function 4 ) . While an effort has been made to avoid programming language – speciﬁc jargon , the pseudocode descriptions include reference to parameters and variables described as ordered lists or arrays . The ordered representation of the data , rather than the data structure used , is of primary importance . A list of the symbols used is provided in appendix B . Spike - Timing - Dependent Construction 2625 Algorithm 2 : Supervised STDC . Given : θ t , θ d , n , n pre Input : n post , A , T f , S , g Output : N , n post , T f , S Initially : N = NULL 1 : If n post < n max 2 : [ T f , f ] : = 3 : If g = true 4 : [ W , n ] : = 5 : x : = 6 : If x = true 7 : N : = ( W ) 8 : n post : = n post + 1 9 : S [ n post ] : = W UpdateTimesFired ( n pre , A , T f ) CreateConnections ( n pre , θ t , T f ) EvaluateConnections ( n post , n pre , θ d , S , W , n ) CreateNeuron The example algorithms are speciﬁed for a nongrowing presynaptic neu - ron set that serves as an interface with the network input and a postsynaptic neuron set where constructed neurons are inserted . The constants θ t and θ d store the spike time threshold and the distance threshold for construction , respectively . Additional algorithm parameters , n max and n pre , provide the maximum number of neurons allowed to be created and the number of presynaptic neurons , respectively . Input parameters to the constructive algorithm indicate network activity and structural properties of the network . Presynaptic neurons that have spiked in the current time - step of the simulation are input as an ordered list of Boolean values , A , and the time since each presynaptic neuron last spiked is stored in the ordered list T f . The input parameter n post stores the number of postsynaptic neurons , and S contains a two - dimensional ordered array of the connection weights to each postsynaptic neuron . The input variables n post , T f , and S may be updated during the execution of the algorithm , so are output to be received as input in the next call of the constructive algorithm . 2626 T . Lightheart , S . Grainger , and T . - F . Lu Function 1 : Given : t step Input : n pre , A , T f Output : T f , f Initially : f = false 1 : For i = 1 to n pre 2 : If A [ i ] = true 3 : f : = true 4 : T f [ i ] : = 0 5 : Else 6 : T f [ i ] : = T f [ i ] + t step UpdateTimesFired If run to completion , the algorithm also outputs a data structure , N , for the neuron and connections to be inserted into the network ; otherwise , N is empty . Note that depending on the environment used for simulating the SNN and STDC , many of these variables could be read directly from the SNN rather than passed as parameters to the algorithm and functions . The supervised STDC algorithm is essentially the same as the unsuper - vised algorithm ; the exception is the construction condition of a supervised signal , g , passed in as a logical input . Given the supervised teaching signal , g = true , the constructive algorithm will continue after updating presynap - tic neuron spike times . This contrasts with the unsupervised algorithm , which requires a presynaptic spike in the evaluated time step for construc - tion to continue . The record of times since presynaptic neurons last spiked , T f , is updated by the UpdateTimesFired function . Entries associated with neurons that have ﬁred in the current simulation time step ( as indicated by A ) have their value reset to zero . All other entries of T f are incremented by the value of a simulation time step , t step . For unsupervised STDC to continue , a presynaptic neuron must be found to ﬁre within the current time step , causing Boolean output f to be returned true . The CreateConnections function produces a set of connection weights from presynaptic neurons that have ﬁred within the spike time threshold θ t . A total weight value , w total , is divided evenly over all excitatory connections Spike - Timing - Dependent Construction 2627 Function 2 : CreateConnections Given : w , w , w total Input : n pre , θ t , T f Output : W , n Initially : W [ i ] = w , ∀ i = 1 , 2 , . . . , n pre ; n = 0 1 : For i = 1 to n pre 2 : If T f [ i ] ≤ θ t 3 : n : = n + 1 4 : w : = w total / n 5 : If w > w 6 : w : = w 7 : For i = 1 to n pre 8 : If T f [ i ] ≤ θ t 9 : W [ i ] : = w to active presynaptic neurons ; however , weights are limited to weight , w max . The weights of connections to input neurons that have not ﬁred within θ t are set to the weight , w min . Construction may be canceled after EvaluateConnections , which tests created connections against additional construction conditions . The param - eter n req introduces an optional minimum number of presynaptic neuron connections . A similar condition could be to implement a minimum total input weight . In any case , the spiking neuron model and network structure will have an impact on connection requirements , so functionality should be ensured . A common condition in previous constructive SNNs is the compari - son of the created connections with those of existing neurons ( Takita & Hagiwara , 2005 ; Wysoski et al . , 2006 ; Lightheart et al . , 2010 ) . This example algorithm has included a calculation of the p - norm distance ( p = 2 gives the Euclidean distance ) to existing input connection vectors . With the selection of a distance threshold of θ d = 0 , however , the distance condition prevents 2628 T . Lightheart , S . Grainger , and T . - F . Lu Function 3 : EvaluateConnections Given : n req , p Input : n post , n pre , θ d , S , W , n Output : x Initially : x = true 1 : If n < n req 2 : x : = false 3 : Else 4 : For i = 1 to n post 5 : d p = 0 6 : For j = 1 to n pre 7 : d p : = d p + | S [ i ] [ j ] − W [ j ] | p 8 : d p : = d 1 / p p 9 : If d p < θ d 10 : x : = false 11 : Break for construction only when new connection weights are equal to the connection weights of an existing neuron . TheCreateNeuronfunctionproducesanewspikingneuron : initializinga datastructure , N , withpresynapticconnections , w , andmodelparameters , a , b , c , and d , of the neuron to be added to the network . The process performed by this function is largely dependent on the spiking neuron model used and could be adapted to produce neurons of a variety of models . In this implementation , an Izhikevich simple neuron model is used with parameter values for regular spiking ( Izhikevich , 2003 ) . Compared to the related constructive algorithms cited , the novel ele - ments of this example constructive algorithm include the explicit depen - dence on spike time threshold for construction , a condition for construction Spike - Timing - Dependent Construction 2629 Function 4 : Input : W Output : N 1 : N . I : = W 2 : N . a : = 0 . 02 3 : N . b : = 0 . 2 4 : N . c : = − 65 5 : N . d : = 8 CreateNeuron of a minimum number of active presynaptic neurons , the use of an external signal to produce variable timing for supervised construction , and the use of an Izhikevich neuron model ( Izhikevich , 2003 ) . 4 Simulation The unsupervised and supervised variations of the example constructive algorithm have been simulated and the network connections resulting from the STDC compared to those developed from a related model of STDP ( see equations 2 . 1 to 2 . 3 ) . All spiking neural network simulations for STDC and STDP were performed in Matlab ; vectorized pseudocode for the con - structive algorithms is included in appendix A . Unsupervised STDP was performed by selectively training neurons , that is , applying the input pat - tern of interest only to designated postsynaptic neurons . Supervision of STDP was performed through external forced spiking of the chosen asso - ciated output neurons as an approximation of winner - take - all competition between spiking postsynaptic neurons . Simulations demonstrate compara - ble learning outcomes for STDC in response to the presentations of printed digits in both supervised and unsupervised variants of the algorithm . Digit recognition is a common benchmark for testing supervised learn - ing algorithms ( LeCun , Bottou , Bengio , & Haffner , 1998 ) that state - of - the - art learning and recognition algorithms perform with very high accuracy ( Cires¸an , Meier , & Schmidhuber , 2012 ) . However , rather than make com - parisons with the state of the art , digit images have been used as a domain for demonstrating neuron creation and comparison with the simulation of an STDP model . To simplify the learning problem , a printed digit set of numbers 0 to 9 has been produced ( see Figure 4A ) in approximately the 2630 T . Lightheart , S . Grainger , and T . - F . Lu same format as the MNIST data set ( LeCun et al . , 1998 ) : 28 × 28 pixels , with [ 0 , 255 ] pixel intensity range normalized to [ 0 , 1 ] . 4 . 1 Spiking Neural Network . The structure of the simulated SNN was designed to accommodate the input of digit images . All simulations were initialized with 784 presynaptic input neurons—one neuron for each image pixel . Simulations of STDP were initialized with 10 postsynaptic neurons . STDC simulations were initialized with 0 postsynaptic neurons , leaving all postsynaptic neurons to be constructed . In the simulations presented , all connections in the SNNs were excitatory . The model of spiking neurons used is an important factor in the design of functions forconnection and neuron creation . All neurons in the simulations use the regular - spiking parameters of the Izhikevich model ( Izhikevich , 2003 ) and are each governed individually by the differential equations v (cid:7) = 0 . 04 v 2 + 5 v + 140 − u + I , ( 4 . 1 ) u (cid:7) = a ( b v − u ) . ( 4 . 2 ) Variables v and u roughly equate to the neuron membrane potential and neuron refractoriness , respectively , and I is the input to the neuron . Neurons are considered to have ﬁred when v ≥ 30 mV , after which neuron variables are updated : v : = c and u : = u + d . This simulation treats synapse transmis - sion as a millisecond delay , while the neuron model treats spikes as being submillisecond in duration . The differential equations allow the efﬁcient modeling of a wide variety of spiking regimes through the selection of pa - rameters a , b , c , and d . The regular - spiking neuron model takes constant parameter values : a = 0 . 02 , b = 0 . 2 , c = − 65 , and d = 8 . ( See Izhikevich , 2003 , for a description of other common parameter values , spiking neuron model behavior , and a Matlab implementation . ) The simulated experiments performed with spike - timing - dependent construction and plasticity required input values to be transformed into spike times . Given the spiking neuron model and parameter values , apply - ing a constant input , I , of sufﬁcient magnitude to a spiking neuron typically produces a spike rate . Nevertheless , the delay to the ﬁrst spike varies with the input value ; therefore , ceasing a constant input after the ﬁrst spike re - sults in a rough transformation to a spike time . Input of pixel intensities , normalized to the range [ 0 , 1 ] , required ampliﬁcation to cause presynaptic neurons to spike . Ampliﬁcation of the normalized pixel intensity by a factor of V = 10 in each time step resulted in an earliest presynaptic spike time of 5 ms after input onset . Limiting the application of the input to a simulated time of 20 ms allowed each presynaptic neuron to spike only once for any in - put . The application of each input for 20 ms was followed by a rest of 80 ms . This allowed the spiking neuron potential and refractoriness variables to settle , reducing interference between subsequent input presentations . Spike - Timing - Dependent Construction 2631 Table 1 : STDP Simulation Parameters . Learning Unsupervised Supervised W + , W − 0 . 1 0 . 1 τ + , τ − 20 ms 20 ms w max 1 1 w min 0 0 w total – 100 While the delay between training inputs reduced interference , some vari - ability in the delay of weakly activated input neurons was observed to cause a small discrepancy between STDP and STDC results . There were , however , no other obvious effects on the outcome of training from randomizing train - ing inputs in these simulated experiments . 4 . 2 STDP Implementation . Simulations of the SNN described above were performed with unsupervised and supervised STDP . All STDP sim - ulations used the model described by the equations 2 . 1 to 2 . 3 ; however , simulations of supervised learning with weight normalization were also performed . In the simulation with normalization , weights were multiplica - tively scaled to sum to a value of w total = 100 after each STDP update ( see Table 1 for STDP parameter values ) . The STDP updates were performed in every simulation time step that a neuron spike occurred . The positive constants and their negative coun - terparts were chosen to be of equal magnitude ( W + = W − and τ + = τ − ) . The additive model of STDP , with excitatory connections alone , was not found to be able to spontaneously produce neuron - level competition and discrimination of presynaptic spike bursts with the signiﬁcant overlap of activity in digit images . Therefore , unsupervised STDP was aided by the isolation of individual postsynaptic neurons during each training image input , allowing only the designated neuron to spike . Supervised synapse updates , using the same STDP model , were per - formed through the forced activation of individual postsynaptic neurons as in Legenstein et al . ( 2005 ) for each training occurrence of a digit image . The selected postsynaptic neuron was activated 11 ms after the onset of the training input to allow 10 ms for input neuron activation and 1 ms for transmission delay . 4 . 3 STDC Implementation . The supervised and unsupervised exam - ples of STDC , described in section 3 , were simulated with algorithm pa - rameters tuned to produce connection weights compatible with the model of STDP described above . The maximum and minimum weights of indi - vidual connections , w max and w min respectively , were set to be equal to 2632 T . Lightheart , S . Grainger , and T . - F . Lu Table 2 : STDC Simulation Parameters . Learning Unsupervised Supervised n max 20 20 n req 50 1 θ t 0 ms 10 ms θ d 0 0 w max 1 1 w min 0 0 w total 100 100 those parameters in the STDP model used . The total weight of constructed connections was restricted to sum to w total , the same value used in weight - normalized STDP . Parameters for supervised and unsupervised algorithm variations were tuned to produce controlled neuron construction for the network and neuron models used . The construction process performs instance - based learning , resulting in one - shot learning of training inputs after a single presentation ( one training epoch ) . For simplicity , both supervised and unsupervised algorithms used a distance threshold θ d = 0 , requiring connection equality to prevent neuron construction . Though the results are not shown , this weak distance thresh - old condition did not reject all candidate neurons in additional training epochs due to small deviations in presynaptic spike time . A precautionary constraint , n max , was placed on the maximum number of neurons con - structed . An exhaustive investigation of the effects of different construction parameters was considered beyond the scope of this letter . Supervised STDC was triggered at the same time as the supervised teach - ing activation of outputs in STDP excluding the 1 ms transmission delay , that is , 10 ms after the input of an image commenced . To capture all in - put neuron spikes between the input onset and the supervised signal , the spike time threshold was set to θ t = 10 ms . Since the supervised signal con - trols construction directly , a minimal condition for the number of required neurons could be used , n req = 1 . The spike time threshold for the unsupervised simulations was selected to consider only simultaneous input spikes , θ t = 0 ms . Unsupervised STDC , without an external teaching signal , was given n req = 50 , allowing construc - tion of neurons only on detecting large waves of coincident presynaptic neuron spikes . The algorithm parameters for unsupervised and supervised STDC are summarized in Table 2 . 4 . 4 Results . Examples of learning outcomes for an individual postsy - naptic neuron under unsupervised STDP , supervised STDP with and with - out normalization , and supervised and unsupervised STDC are shown in Spike - Timing - Dependent Construction 2633 Figure 4 : The printed digit image data set used in simulations and examples of the connection weight outcomes for digit 5 from STDP and STDC . Light pixels indicate large , positive weights . ( A ) The printed digit images used . ( B ) The input weights after 50 epochs of unsupervised STDP training without normalization . ( C ) The input weights after 50 epochs of supervised STDP training without nor - malization . ( D ) The input weights after 50 epochs of supervised STDP training with normalization . ( E ) The input weights after unsupervised STDC . ( F ) The input weights after supervised STDC . Figure 4 . Plots of 1 - norm distances of weight vectors , produced by STDC and additive STDP , are presented for selected connections and for all con - nections in Figure 5 . Randomization of the initial connection weights and input presentation order in each epoch was performed only for STDP simu - lations . Furthermore , STDC and STDP simulations were performed without input noise . As a result , the processes and outcomes of the simulation were essentially deterministic . Statistical analyses of multiple simulation runs were therefore considered to provide little informational value and have been excluded . 2634 T . Lightheart , S . Grainger , and T . - F . Lu Figure 5 : The 1 - norm distance of connection weight vectors constructed through STDC and increasing iterations of STDP - based training of randomly initialized connection weight vectors . Distance calculation is performed for se - lected connections , connections constructed with potentiated weights and their STDP - trained equivalents , and all connections . Unsupervised STDP and unsu - pervised STDC are compared for selected connections ( A ) and all connections ( D ) . Supervised STDP and supervised STDP are compared for selected connec - tions ( B ) and all connections ( E ) . Supervised STDP with weight normalization and supervised STDC are compared for selected connections ( C ) and all con - nections ( F ) . Note that plots of the 1 - norm distance for all connections use a different y - axis scale , as do plots for normalized STDP distances . Qualitatively , the image representations of all connection weights gener - ated by STDP and STDC ( see Figure 4 ) differ most noticeably in the inactive presynaptic connections . Under the additive STDP model and initialization procedure used , connections from inactive neurons undergo no updates , remaining at randomly initialized weights throughout the simulation ( see Figures 4B and 4C ) . The algorithm for STDC assumes that all presynaptic neurons that are not active within the spike time threshold either spike in the depressive portion of the STDP curve or are uncorrelated to post - synaptic activity . Connections to these presynaptic neurons are given the minimum weight of zero . Therefore , the 1 - norm distance of all connections ( see Figures 5D and 5E ) can decrease only to a value approximately equal to the number of inactive presynaptic neurons multiplied by the mean of the initial weight distribution . The predictions of theorem 1 can be isolated , however , if the calculation of the 1 - norm distance is performed only on con - structed potentiated connections and the equivalent connections updated under STDP . Spike - Timing - Dependent Construction 2635 Restricting the comparison of connection weights to those selected by STDC , the resulting 1 - norm distances fall sharply before steadying at or near zero ( see Figures 5A , 5B , and 5C ) . The ﬁnal , nonzero 1 - norm distance in the supervised STDP plot was due to ﬂuctuating presynaptic neuron spike times : presynaptic spikes occurring close to the supervised postsy - naptic activation time on the ﬁrst epoch were observed to occur after the supervised activation time in later epochs . Under the STDP window used , this additional delay prevented the expected convergence of those synapses and caused a ﬁnal , nonzero margin in the 1 - norm distance calculated . This discrepancy can be observed as darker pixels representing weights , at the edges of the digit , developed from supervised STDP with normalization ( see Figure 4D ) when compared to supervised STDC ( see Figure 4F ) . Although not in the theory developed here , normalization is a common process applied in addition to Hebbian learning rules for synapse modiﬁca - tion ( Gerstner & Kistler , 2002 ) and has been simulated here for comparison with constructed neurons . Normalizing all weights to sum to w total ( see Fig - ures 4D , 5C , and 5F ) acts in opposition to any net change in total weight , positive or negative . As a result , convergence is slowed due to connec - tion weight increases , causing a marginal depression of all connections weights . Relative to STDP without normalization , STDP with weight normaliza - tion demonstrates slower 1 - norm distance convergence and greater residual distance for selected connections but lower distances when considering all connections . Residual distance in normalized results , for all connections and those selected by STDC , is primarily the result of there being too few potentiated connections and spike time ﬂuctuations causing connections to fail to be potentiated . When the maximum weight , w max , multiplied by the number of potentiated connections , n , is less than the total weight , w total , normalization distributes the remaining weight over the remaining con - nections . When the number of potentiated connections arising from STDC and normalized STDP is greater than required to take the total weight , the individual weights converge to less than the maximum . Inspecting the results for the 1 - norm distance in selected connections for unsupervised STDP without normalization ( see Figure 5A ) , we see the error falls to approximately zero in 11 epochs of STDP training . Using the reformulation of the limit ( see equation 2 . 8 ) and the parameters of unsupervised STDP and STDC used in simulations ( see Tables 1 and 2 , respectively ) , the maximum number of iterations required to reach zero error can be calculated to be m = 10 . This slight discrepancy is due to the actual delay between the presynaptic and postsynaptic spikes of 1 ms rather than being absolutely negligible , which has an impact on STDP updates . Using the same reformulation of the limit ( see equation 2 . 8 ) with the delay added to the spike time threshold gives a maximum number of iterations required of m ≈ 10 . 5 , which must be rounded to the next nearest integer ( m = 11 ) . 2636 T . Lightheart , S . Grainger , and T . - F . Lu The ﬁring of presynaptic neurons , representing intermediate pixel val - ues , after postsynaptic activation caused depression under STDP that matched constructed outcomes . The presented simulation analysis , how - ever , does not explicitly validate theorem 2 . Calculation of the 1 - norm dis - tance for validation of theorem 2 could be achieved through an isolated comparison of connections between neurons with the associated spike tim - ing . Further insight into the approximation capabilities of STDC may be obtained through comparison with STDP in simulations of network activity that does not conform to assumed conditions . For example , the outcomes of STDC could be compared with STDP in the task of developing connections to detect spike patterns embedded in stochastic activity ( Nessler , Pfeiffer , & Maass , 2009 ) . 5 Discussion Due to the early stage of the development of constructive algorithms for SNNs , the full scope of their applicability is not clear . Nevertheless , con - structive algorithms have potential relevance to the modeling of neurolog - ical systems and machine learning . 5 . 1 Relevance to Computational Neuroscience . The ability of STDC to model additive asymmetrical Hebbian STDP has been explored in this letter ; however , experimental observations have shown the existence of many more variants of STDP ( Caporale & Dan , 2008 ) . The development of detailed models of STDP has seen signiﬁcant advances in recent years ( Morrison et al . , 2008 ) , and the discovery of learning capabilities presented by models of STDP and biological network structures continues . The iterative simulation of STDP models has demonstrated the learning of early spike pattern detection ( Guyonneau , VanRullen , & Thorpe , 2005 ; Masquelier , Guyonneau , & Thorpe , 2008 ) . Models of STDP using weight - based or multiplicative updates have been shown to achieve improvements in weight variance and stability ( van Rossum et al . , 2000 ) and the ability of neurons to identify independent components ( Savin , Joshi , & Triesch , 2010 ; Gilson , Fukai , & Burkitt , 2012 ) . Rapidly spiking pre - and postsynaptic neurons may also ﬁnd multiple spikes occurring within the STDP time win - dow , changing the outcome of weight adaptation ( Pﬁster & Gerstner , 2006 ) . The structure of the network can play an important role in functionality ; for example , lateral inhibition can improve the capability of networks to identify unique spike trains ( Nessler et al . , 2009 ; Masquelier , Guyonneau , & Thorpe , 2009 ) . Future work could explore if implementations of STDC exist that can approximate these STDP models and network structures and whether similar learning outcomes can be achieved constructively . An implication of assuming the existence of neurons and connections outside the simulation is that STDC may be capable of simplifying the sim - ulation of large neural networks . Due to the generality of the constructive Spike - Timing - Dependent Construction 2637 algorithm framework , constructive simulations may be possible using a wide variety of models for spiking neurons and action potential transmis - sion . For example , a parameter for spike transmission delay could be in - corporated into constructed synapses , potentially producing a polychronic spiking network ( Izhikevich , 2006 ) . Network architectures may be designed to model known brain struc - tures and made constructive through the selection of presynaptic and post - synaptic neuron sets and the application of construction processes . In this way , networks may potentially be automatically constructed to model brain structures such as the hippocampus and neocortex . Within the hippocam - pus , STDP is prominent ( Bi & Poo , 1998 ) , and the function of the hippocam - pus is crucial in the consolidation of memories ( Becker , 2005 ) . As such , a model of memory could potentially be developed using STDC to store short - term memories , consolidating those memories through subsequent training of networks modeling the neocortex . While the relationship of STDC with the biological process of STDP has been a principal focus of this letter , parallels with the biological growth of neurons ( neurogenesis ) and synapses ( synaptogenesis ) are apparent . Neurogenesis has been observed in numerous locations of the adult brain ( Abrous et al . , 2005 ) and has been shown to have theoretical signiﬁcance in the coding of memories ( Aimone , Wiles , & Gage , 2009 ) . Similarly , synapto - genesis has also been observed throughout the brain , occurring in a range of timescales from less than an hour to days ( Holtmaat & Svoboda , 2009 ) . As neuro - and synaptogenesis generate new connections and neurons , models of these processes may be viewed as constructive algorithms . If growth is modeled as being activity or spike timing dependent , the model may even be classiﬁed as STDC . More generally , the concept of adding neu - rons or connections to a simulated network from a large neural system may simply be viewed as being inclusive of network growth in the neural system that is outside the simulation . Therefore , neurogenesis and synaptogenesis may be assumed to be accounted for by STDC . 5 . 2 Relevance to Machine Learning . Constructive algorithms have long been developed in association with artiﬁcial neural networks for the purposes of machine learning ( Nicoletti et al . , 2009 ) . Constructive algo - rithms for SNNs exist within the machine learning paradigms of unsuper - vised learning ( Takita & Hagiwara , 2005 ; Lightheart et al . , 2010 ) , supervised learning ( Wysoski et al . , 2006 ) , and reinforcement learning ( Liu & Buller , 2005 ; Huemer et al . , 2009 ) . Nevertheless , further development and investi - gation of the learning capabilities of constructive SNNs is warranted . The unsupervised STDC algorithm developed in this letter could be ex - tended to perform clustering in a similar manner to nonspiking growing neural gas ( Fritzke , 1995 ) . As STDP is capable of resulting in the identi - ﬁcation of statistical components ( Gilson et al . , 2012 ) , investigation of the capability of STDC to achieve comparable outcomes could be a direction 2638 T . Lightheart , S . Grainger , and T . - F . Lu of future work . Supervised STDC algorithms and simulations demonstrate learning through the application of teaching signals . Effective implemen - tations of the eSNN performing pattern recognition ( Wysoski et al . , 2006 ; Soltic et al . , 2008 ) . Wysoski et al . ( 2010 ) suggests that other forms of super - vised STDC may have similar applications . Detailed supervised feedback could be incorporated into future STDC processes , with error feedback directing synapse modiﬁcation or triggering construction . Reinforcement - feedback - based construction conditions have been used in past constructive algorithms for SNNs ( Huemer et al . , 2009 ; Liu & Buller , 2005 ) . These algorithms perform model - free reinforcement learning , opt - ing to directly modify the SNN that represents their policy for choosing actions . Similar outcomes may be achievable through constructive model - ing of reward - modulated STDP ( Florian , 2007 ) . Alternatively , constructive algorithms may be used to learn value functions ( Sutton & Barto , 1998 ) , growing to accommodate state - spaces of unknown size , or directly apply reward feedback - based network construction mechanisms ( Takita & Hagi - wara , 2005 ; Liu & Buller , 2005 ; Huemer et al . , 2009 ) . The general framework for network construction proposed also lends itself to complex , multilayer network architectures . As such , the applica - bility of constructive algorithms to deep neural networks may be worthy of investigation . The generality of the constructive algorithm framework is expected to be compatible with deep neural networks , which often ap - ply convolutional connectivity ( LeCun et al . , 1998 ; Serre , Wolf , Bileschi , Riesenhuber , & Poggio , 2007 ; Fukushima , 2011 ; Cires¸an et al . , 2012 ) and probabilistic neuron models ( Hinton , Osindero , & Teh , 2006 ; Salakhutdi - nov & Hinton , 2012 ) that may be thought of as stochastic spiking neurons . The results achieved by deep neural networks in object and speech recog - nition are outstanding ( Cires¸an et al . , 2012 ; Salakhutdinov & Hinton , 2012 ; Hinton et al . , 2012 ) . Nevertheless , the training times for these learning sys - tems can take many hours , even with parallel processing ( Cires¸an et al . , 2012 ) . Constructive algorithms for neural networks , such as STDC , may potentially be applied to deep neural networks to speed up learning and allow trained networks to easily expand to accommodate new classes and data . Many machine learning developments have applications in robotics . Potential applications of constructive SNNs include pattern or object recog - nition and learning to characterize physical systems for motion control and action planning . Should rapid online learning be achievable in supervised , unsupervised , and reinforcement learning regimes , STDC implementations may be effective in a wide range of learning tasks relevant to robotics . Appendix A : Vectorized STDC Example The vectorized pseudocode presented ( see algorithms 3 and 4 ) repro - duces the STDC examples described in section 3 with two simpliﬁcations : Spike - Timing - Dependent Construction 2639 Algorithm 3 : UnsupervisedSTDC - Vectorized . Given : n , n req , t step , θ t Input : n post , f , t f , S Output : N , n post , t f , S Initially : N = NULL 1 : If n post < n 2 : t f : = ( t f + t step ) ◦ ( ¬ f ) 3 : If A NY ( f ) 4 : x : = true 5 : w : = ( t f ≤ θ t ) 6 : If n post > 0 7 : W : = Repmat ( w , 1 , n post ) 8 : x : = ¬ A ny (cid:2) col [ S ◦ W − S ◦ ( ¬ W ) ] = (cid:2) col w (cid:3) 9 : If x ∧ [ ( (cid:2) col w ) ≥ n req ] 10 : N : = ( w ) 11 : n post : = n post + 1 12 : S : = [ S w ] CreateNeuron evaluation of connections is performed based on the inequality of connec - tion sets described as binary vectors , and synapse weights are calculated in the CreateNeuron function . Unsupervised and supervised vectorized al - gorithms are presented ; however , they differ only in the use of a teaching signal , g , inthesupervisedalgorithm . Thevectorizedalgorithmpseudocode has been derived from Matlab code that was used in experimental simula - tions ( see section 4 ) . The constants n max , n req , and θ t were used as conditions for construction . The values of the constants used in the STDC simulations were given in Table 2 . The time - step constant , t step , was set to 1 in millisecond units , equal to the simulation time step for the SNN . Variables n post , t f and S , stored : 2640 T . Lightheart , S . Grainger , and T . - F . Lu Algorithm 4 : Given : n , n req , t step , θ t Input : n post , f , t f , S , g Output : N , n post , t f , S Initially : N = NULL 1 : If n post < n 2 : t f : = ( t f + t step ) ◦ ( ¬ f ) 3 : If g = true 4 : x : = true 5 : w : = ( t f ≤ θ t ) 6 : If n post > 0 7 : W : = Repmat ( w , 1 , n post ) 8 : x : = ¬ A NY (cid:2) col [ S ◦ W − S ◦ ( ¬ W ) ] = (cid:2) col w (cid:3) 9 : If x ∧ [ ( (cid:2) col w ) ≥ n req ] 10 : N : = ( w ) 11 : n post : = n post + 1 12 : S : = [ S w ] S upervised STDC - V ectorized . CreateNeuron the number of neurons created , an n pre × 1 vector of the last ﬁring time of input neurons , and an n pre × n post binary matrix of network connections , respectively . Before the constructive algorithm was called for , the ﬁrst time variables were initialized : n post = 0 , t f = θ t 1 , and S as an empty matrix . These variables are output , so that updates are reﬂected on the next call of the constructive algorithm . The input f contains a logic vector of neurons that spiked in the current time step of the simulation . The output of the constructive algorithm , N , is the result of the CreateNeuron function ( as in section 3 with the addition described above ) or NULL if construction conditions are not met . Spike - Timing - Dependent Construction 2641 The functions Any and Repmat represent Matlab functions . The function Any returns true if any element is nonzero . The function Repmat returns a matrix or vector that is a tiled replication of the ﬁrst parameter ( the number of vertical replications is given by the second parameter , the number of horizontal replications is given by the third parameter ) . The comparison operators ( < , > , ≤ , ≥ and = ) are applied to vectors in an element - wise fashion . The ◦ operator is an element - wise or Hadamard product of two vectors or matrices . The summation operator with subscript col , (cid:7) col , is used here to denote a column - wise sum of all the elements in a vector or matrix . The ¬ and ∧ are the logical NOT , or negation operator , and logical AND operator , respectively . The syntax of Matlab has been used to express the construction of a matrix by enclosing any combination of vectors or matrices with the same number of rows in square brackets ( [ S w ] ) . Appendix B : Symbols Used Constructive Algorithm Framework N pre The set of neurons considered to be presynaptic for construction purposes N post The set of neurons considered to be postsynaptic for construction purposes S The set of synapses G The set of functions that compute if conditions for construction are met C The set of functions that compute parameter values for new components STDP and STDC d p p - norm distance d 1 1 - norm distance F Set of presynaptic neurons that have ﬁred within the spike time threshold n Number of presynaptic neurons that have ﬁred within the spike time threshold m Number of iterations of the spike pattern t Time t fpre Spike time of the presynaptic neuron t fpost Spike time of the postsynaptic neuron (cid:2) t Difference in pre - and postsynaptic spike times , t fpre − t fpost t fF Vector of individual presynaptic spike times (cid:2) t Vector of individual spike time differences τ + Time constant for positive STDP curve τ − Time constant for negative STDP curve θ t Spike time threshold for construction w Connection weight or synapse efﬁcacy (cid:2)w Change in connection weight or synapse efﬁcacy w min Minimum connection weight w max Maximum connection weight W + Positive weight update constant W − Negative weight update constant 2642 T . Lightheart , S . Grainger , and T . - F . Lu Constructive Algorithm Pseudo - Code A List of Boolean values describing whether each presynaptic neuron has spiked in the current time step f Boolean value of whether any presynaptic spike has occurred in the current time step g Boolean teaching signal that triggers construction when true n Number of active presynaptic neurons and connections to be constructed n pre Number of neurons in the presynaptic set , N pre n post Number of neurons in the postsynaptic set , N post n max Maximum number of neurons in the postsynaptic set , N post n req Minimum number of presynaptic neurons in F for construction N Data structure containing the constructed spiking neuron model parameters p p - norm distance exponent S Two - dimensional list of connection weights from presynaptic neurons to postsynaptic neurons T f List of time since each presynaptic neuron last spiked t step Duration of a single time - step θ t Spike time threshold for construction θ d p - norm distance threshold for construction W List of connection weights to a postsynaptic neuron to be constructed w max Maximum value of constructed connections w min Minimum value of constructed connections w total Maximum total value of constructed connections x Boolean value that conﬁrms or cancels neuron construction Acknowledgments We acknowledge the feedback provided by Michael J . Watts and the anony - mous reviewers of this letter . References Abbott , L . F . , & Nelson , S . B . ( 2000 ) . Synaptic plasticity : Taming the beast . Nat . Neurosci . , 3 , 1178 – 1183 . Abrous , D . N . , Koehl , M . , & LeMoal , M . ( 2005 ) . Adultneurogenesis : Fromprecursors to network and physiology . Physiol . Rev . , 85 ( 2 ) , 523 – 569 . Aha , D . W . , Kibler , D . , & Albert , M . K . ( 1991 ) . Instance - based learning algorithms . Mach . Learn . , 6 ( 1 ) , 37 – 66 . Aimone , J . B . , Wiles , J . , & Gage , F . H . ( 2009 ) . Computational inﬂuences of adult neurogenesis on memory encoding . Neuron , 61 ( 2 ) , 187 – 202 . Arena , P . , Fortuna , L . , Frasca , M . , & Patan´e , L . ( 2009 ) . Learning anticipation via spiking networks : Application to navigation control . IEEE T . Neural Networks , 20 ( 2 ) , 202 – 216 . Becker , S . ( 2005 ) . A computational principle for hippocampal learning and neuroge - nesis . Hippocampus , 15 ( 6 ) , 722 – 738 . Spike - Timing - Dependent Construction 2643 Bi , G . - q . , & Poo , M . - m . ( 1998 ) . Synaptic modiﬁcations in cultured hippocampal neu - rons : Dependence on spike timing , synaptic strength , and postsynaptic cell type . J . Neurosci . , 18 ( 24 ) , 10464 – 10474 . Bohte , S . M . , La Poutre , H . , & Kok , J . N . ( 2002 ) . Unsupervised clustering with spiking neurons by sparse temporal coding and multilayer RBF networks . IEEE T . Neural Networks , 13 ( 2 ) , 426 – 435 . Bruske , J . , & Sommer , G . ( 1995 ) . Dynamic cell structure learns perfectly topology preserving map . Neural Comput . , 7 ( 4 ) , 845 – 865 . Caporale , N . , & Dan , Y . ( 2008 ) . Spike timing - dependent plasticity : A Hebbian learn - ing rule . Annu . Rev . Neurosci . , 31 ( 1 ) , 25 – 46 . Carrillo , R . R . , Ros , E . , Boucheny , C . , & Coenen , O . J . ( 2008 ) . A real - time spiking cerebellum model for learning robot control . Biosystems , 94 ( 1 – 2 ) , 18 – 27 . Cires¸an , D . , Meier , U . , & Schmidhuber , J . ( 2012 ) . Multi - column deep neural networks for image classiﬁcation ( Tech . Rep . No . IDSIA - 04 - 12 ) . Manno , Switzerland : IDSIA . Delorme , A . , & Thorpe , S . J . ( 2001 ) . Face identiﬁcation using one spike per neuron : Resistance to image degradations . Neural Networks , 14 ( 6 – 7 ) , 795 – 803 . Di Paolo , E . A . ( 2003 ) . Evolving spike - timing - dependent plasticity for single - trial learning in robots . Philos . T . Roy . Soc . A , 361 ( 1811 ) , 2299 – 2319 . Floreano , D . , Epars , Y . , Zufferey , J . - C . , & Mattiussi , C . ( 2006 ) . Evolving spike - timing - dependent plasticity for single - trial learning in robots . Int . J . Intell . Syst . , 21 ( 9 ) , 1005 – 1024 . Florian , R . V . ( 2007 ) . Reinforcement learning through modulation of spike - timing - dependent synaptic plasticity . Neural Comput . , 19 ( 6 ) , 1468 – 1502 . Fritzke , B . ( 1995 ) . A growing neural gas network learns topologies . In G . Tesauro , D . S . Touretzky , & T . K . Leen ( Eds . ) , Advances in neural information processing systems , 7 ( pp . 625 – 632 ) . Cambridge , MA : MIT Press . Fukushima , K . ( 2011 ) . Increasing robustness against background noise : Visual pat - tern recognition by a neocognitron . Neural Networks , 24 ( 7 ) , 767 – 778 . Gerstner , W . , & Kistler , W . M . ( 2002 ) . Spikingneuronmodels : Singleneurons , populations , plasticity . Cambridge : Cambridge University Press . Gilson , M . , Fukai , T . , & Burkitt , A . N . ( 2012 ) . Spectral analysis of input spike trains by spike - timing - dependent plasticity . PLoS Comput . Biol . , 8 ( 7 ) , e1002584 . Guyonneau , R . , VanRullen , R . , & Thorpe , S . J . ( 2005 ) . Neurons tune to the earliest spike through STDP . Neural Comput . , 17 ( 4 ) , 859 – 879 . Hinton , G . E . , Deng , L . , Yu , D . , Dahl , G . E . , Mohamed , A . - R . , Jaitly , N . , & Kingsbury , B . ( 2012 ) . Deep neural networks for acoustic modeling in speech recognition : The shared views of four research groups . IEEE Signal Proc . Mag . , 29 ( 6 ) , 82 – 97 . Hinton , G . E . , Osindero , S . , & Teh , Y . - W . ( 2006 ) . A fast learning algorithm for deep belief nets . Neural Comput . , 18 ( 7 ) , 1527 – 1554 . Holtmaat , A . , & Svoboda , K . ( 2009 ) . Experience - dependent structural synaptic plas - ticity in the mammalian brain . Nat . Rev . Neurosci . , 10 ( 9 ) , 647 – 658 . Huang , G . - B . , & Babri , H . A . ( 1998 ) . Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions . IEEE T . Neural Networks , 9 ( 1 ) , 224 – 229 . Huemer , A . , Elizondo , D . , & Gongora , M . ( 2009 ) . Constructive neural network algo - rithms for feedforward architectures suitable for classiﬁcation tasks . In L . Franco , 2644 T . Lightheart , S . Grainger , and T . - F . Lu D . Elizondo , & J . Jerez ( Eds . ) , Constructive neural networks ( pp . 225 – 242 ) . Berlin : Springer - Verlag . Izhikevich , E . M . ( 2003 ) . Simple model of spiking neurons . IEEE T . Neural Networks , 14 ( 6 ) , 1569 – 1572 . Izhikevich , E . M . ( 2006 ) . Polychronization : Computationwithspikes . NeuralComput . , 18 ( 2 ) , 245 – 282 . Kasi´nski , A . , & Ponulak , F . ( 2006 ) . Comparison of supervised learning methods for spike time coding in spiking neural networks . Int . J . Appl . Math . Comput . Sci . , 16 ( 1 ) , 101 – 113 . LeCun , Y . , Bottou , L . , Bengio , Y . , & Haffner , P . ( 1998 ) . Gradient - based learning ap - plied to document recognition . Proceedings of the IEEE , 86 ( 11 ) , 2278 – 2324 . Legenstein , R . , Naeger , C . , & Maass , W . ( 2005 ) . What can a neuron learn with spike - timing - dependent plasticity ? Neural Comput . , 18 ( 11 ) , 2337 – 2382 . Lightheart , T . , Grainger , S . , & Lu , T . - F . ( 2010 ) . A constructive spiking neural net - work for reinforcement learning in autonomous control . Proc . 2010 Australasian Conf . Robotics and Automation . http : / / www . araa . asn . au / acra / acra2010 / papers / pap155s1 - ﬁle1 . pdf Liu , J . , & Buller , A . ( 2005 ) . Self - development of motor abilities resulting from the growth of a neural network reinforced by pleasure and tensions . In Proceedings of the 4th International Conference on Development and Learning ( pp . 121 – 125 ) . Piscat - away , NJ : IEEE . Maass , W . ( 1996 ) . Lower bounds for the computational power of networks of spiking neurons . Neural Comput . , 8 , 1 – 40 . Maass , W . ( 1997a ) . Networks of spiking neurons : The third generation of neural network models . Neural Networks , 10 ( 9 ) , 1659 – 1671 . Maass , W . ( 1997b ) . Noisy spiking neurons with temporal coding have more com - putational power than sigmoidal neurons . In M . C . Mozer , M . I . Jordan , & T . Petsche ( Eds . ) , Advances in neural information processing systems , 9 ( pp . 211 – 217 ) . Cambridge , MA : MIT Press . Masquelier , T . , Guyonneau , R . , & Thorpe , S . J . ( 2008 ) . Spike timing dependent plas - ticity ﬁnds the start of repeating patterns in continuous spike trains . PLoS ONE , 3 ( 1 ) , e1377 . Masquelier , T . , Guyonneau , R . , & Thorpe , S . J . ( 2009 ) . Competitive STDP - based spike pattern learning . Neural Comput . , 21 ( 5 ) , 1259 – 1276 . Masquelier , T . , & Thorpe , S . J . ( 2007 ) . Unsupervised learning of visual features through spike timing dependent plasticity . PLoS Comput . Biol . , 3 ( 2 ) , 247 – 257 . Morrison , A . , Diesmann , M . , & Gerstner , W . ( 2008 ) . Phenomenological models of synaptic plasticity based on spike timing . Biol . Cybern . , 98 ( 6 ) , 459 – 478 . Nessler , B . , Pfeiffer , M . , & Maass , W . ( 2009 ) . STDP enables spiking neurons to detect hidden causes of their inputs . In Y . Bengio , D . Schuurmans , J . Lafferty , C . K . I . Williams , & A . Culotta ( Eds . ) , Advances in neural information processing systems , 22 ( pp . 1357 – 1365 ) . Cambridge , MA : MIT Press . Nicoletti , M . C . , Bertini , J . , Elizondo , D . , Franco , L . , & Jerez , J . ( 2009 ) . Constructive neural network algorithms for feedforward architectures suitable for classiﬁca - tion tasks . In L . Franco , D . Elizondo , & J . Jerez ( Eds . ) , Constructive neural networks ( pp . 1 – 23 ) . Berlin : Springer - Verlag . Spike - Timing - Dependent Construction 2645 Pﬁster , J . - P . , & Gerstner , W . ( 2006 ) . Triplets of spikes in a model of spike timing – dependent plasticity . J . Neurosci . , 26 ( 38 ) , 9673 – 9682 . Salakhutdinov , R . , & Hinton , G . E . ( 2012 ) . An efﬁcient learning procedure for deep Boltzmann machines . Neural Comput . , 24 ( 8 ) , 1967 – 2006 . Savin , C . , Joshi , P . , & Triesch , J . ( 2010 ) . Independent component analysis in spiking neurons . PLoS Comput . Biol . , 6 ( 4 ) , e1000757 . Serre , T . , Wolf , L . , Bileschi , S . , Riesenhuber , M . , & Poggio , T . ( 2007 ) . Learning in spiking neural networks by reinforcement of stochastic synaptic transmission . IEEE Trans . Pattern Anal . Mach . Intell . , 29 ( 3 ) , 411 – 426 . Seung , H . S . ( 2003 ) . Learninginspikingneuralnetworksbyreinforcementofstochas - tic synaptic transmission . Neuron , 40 ( 6 ) , 1063 – 1073 . Soltic , S . , Wysoski , S . , & Kasabov , N . ( 2008 ) . Evolving spiking neural networks for taste recognition . In Proceedings of the International Joint Conference on Neural Networks ( pp . 2091 – 2097 ) . Piscataway , NJ : IEEE . Song , S . , Miller , K . D . , & Abbott , L . F . ( 2000 ) . Competitive Hebbian learning through spike - timing - dependent synaptic plasticity . Nat . Neurosci . , 3 ( 9 ) , 919 – 926 . Sutton , R . , & Barto , A . ( 1998 ) . Reinforcement learning : An introduction . Cambridge , MA : MIT Press . Takita , K . , & Hagiwara , M . ( 2002 ) . A pulse neural network learning algorithm for POMDP environment . In Proceedings of the International Joint Conference on Neural Networks ( pp . 1643 – 1648 ) . Piscataway , NJ : IEEE . Takita , K . , & Hagiwara , M . ( 2005 ) . A pulse neural network reinforcement learning algorithmforpartiallyobservableMarkovdecisionprocesses . Syst . Comput . Japan , 36 ( 3 ) , 42 – 52 . Thorpe , S . , Delorme , A . , & Van Rullen , R . ( 2001 ) . Spike - based strategies for rapid processing . Neural Networks , 14 ( 6 – 7 ) , 715 – 725 . van Rossum , M . C . W . , Bi , G . Q . , & Turrigiano , G . G . ( 2000 ) . Stable Hebbian learning from spike - timing - dependent plasticity . J . Neurosci . , 20 ( 23 ) , 8812 – 8821 . Watt , A . J . , & Desai , N . S . ( 2010 ) . Homeostatic plasticity and STDP : Keeping a neu - ron’s cool in a ﬂuctuating world . Front . Synaptic Neurosci . , 2 ( 5 ) . Watts , M . J . ( 2009 ) . A decade of Kasabov’s evolving connectionist systems : A review . IEEE . T . Syst . Man . Cy . C . , 39 ( 3 ) , 253 – 269 . Wysoski , S . G . , Benuskova , L . , & Kasabov , N . ( 2006 ) . On - line learning with structural adaptation in a network of spiking neurons for visual pattern recognition . In S . D . Kollias , A . Stafylopatis , W . Duch , & E . Oja ( Eds . ) , Lecture notes computer science : Vol . 4131 . Artiﬁcial Neural Networks – ICANN 2006 ( vol . 4131 , pp . 61 – 70 ) . Berlin : Springer - Verlag . Wysoski , S . G . , Benuskova , L . , & Kasabov , N . ( 2010 ) . Evolving spiking neural net - works for audiovisual information processing . Neural Networks , 23 ( 7 ) , 819 – 835 . Received August 23 , 2012 ; accepted May 4 , 2013 .