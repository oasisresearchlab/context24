Supporting Creative Workers with Crowdsourced Feedback Jonas Oppenlaender University of Oulu Oulu , Finland jonas . oppenlaender @ oulu . ﬁ Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . Copyright held by the owner / author ( s ) . C & C ’19 , June 23 – 26 , 2019 , San Diego , CA , USA ACM 978 - 1 - 4503 - 5917 - 7 / 19 / 06 . https : / / doi . org / 10 . 1145 / 3325480 . 3326556 Abstract Online feedback systems have gained in popularity in re - cent years . These systems have the potential to provide vast amounts of feedback from the crowd . My dissertation explores how creative workers can be assisted in evalu - ating this crowdsourced feedback . I qualitatively explored and studied this issue in three different feedback systems . Future work will develop a framework for recommending feedback evaluation strategies for different feedback types in the context of creative work and potentially massive amounts of crowdsourced feedback . CCS Concepts • Information systems → Crowdsourcing ; • Human - centered computing → Human computer interaction ( HCI ) ; Author Keywords Creativity Support ; Crowdsourcing ; Feedback Systems . Introduction Crowdsourcing is a powerful approach for tapping into the collective insights of a crowd of people with a diverse back - ground and knowledge [ 10 ] . The combination of crowd - sourcing and creativity support is a promising area of re - search [ 13 , 23 ] and builds on a long line of work on aug - menting human creativity and intellect [ 2 , 7 ] . More recently , research on supporting creativity has been sparking interest Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 646 in the ﬁeld of Human - Computer Interaction [ 5 , 21 , 22 ] . Online feedback exchange systems [ 3 ] and crowd feedback systems [ 17 , 25 ] are computer - mediated systems that en - able creative individuals to collect feedback from a large number of people online [ 14 ] . These systems provide an opportunity for asynchronously sourcing feedback , decision support , and critique from a crowd . Researchers have investigated ways of increasing the per - ceived value of crowdsourced feedback [ 11 , 15 ] , e . g . by using rubrics to structure the feedback [ 17 , 27 ] and inter - active guidance to scaffold the feedback process [ 6 , 18 ] . Kang et al . enriched individual critique items with visual examples [ 12 ] . Using techniques such as the above , crowd - sourcing distributed critique from novice crowds may yield feedback in comparable quality to expert critique [ 6 , 17 ] . The above approaches focus on improving the quality of individual feedback items . An often overlooked aspect in crowd feedback systems is the diversity and potentially vast quantity of the crowdsourced feedback that limits the use - fulness of the feedback for its receiver . Crowd feedback systems thus call for processing , aggregating and visualis - ing of the crowdsourced feedback . Voyant by Xu et al . [ 25 ] , for instance , aggregates design feedback . The different methods explored by the authors include visual annotation of designs , word clouds , and histograms . Crowdsensus is crowd - powered tool that supports the receivers of feedback in analysing and clustering elicited feedback [ 1 ] . Crowd - lines by Luther et al . is another example of a system that supports the synthesis of diverse information sources [ 16 ] . Figure 1 : Articlebot user interface for facetted ﬁltering of ideas . Feedback can be ambiguous and heterogeneous , as feed - back providers may have different motivations , expertise and perspectives [ 26 ] . The diversity of different viewpoints holds value for the receiver of the feedback . Aggregating feedback results , however , in a loss of this information and evaluating individual feedback items would put a high cog - nitive demand on the feedback receiver . These constraints limit the utility of crowdsourced feedback . In my research , I explore processes , systems and inter - faces for supporting creative work with the crowd . Specif - ically , I aim to support feedback receivers in exploring the information space and getting an insight into the diverse feedback of the crowd at different levels of analysis . I aim to design strategies for enabling the feedback receivers to make sense of the feedback . Sensemaking is the pro - cess of imposing an order on the crowdsourced feedback to guide future actions [ 4 , 24 ] . My work will result in a framework for the design of feed - back systems that supports the evaluation of potentially massive amounts of text based but also non - text based feedback from the crowd . A central question in my research is : How can we aggregate feedback , while still maintaining the value of individual feedback items ? My research will ex - plore the requirements and the design space of crowd feed - back systems in this regard , and will delineate a number of design strategies and techniques to cope with this problem . My research will further contribute practical techniques that enable creative workers to effectively explore the feedback received from the crowd . These techniques may grant new insight into how to support feedback receivers in evaluating massive amounts of diverse feedback from the crowd . Completed Studies In the past year , we qualitatively explored the constraints of crowdsourced feedback in the context of creative work with three crowd feedback systems that support different types of creative work : writing , web design , and design critique . Three of the four papers are currently still under review . In Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 647 the following , I brieﬂy summarise some of the insights of our interviews with receivers of different types of crowdsourced feedback in these three systems . ArticleBot : Supporting Exploratory Writing Figure 2 : uiCrowd user interface to provide justiﬁcations for manipulations of the website layout . Figure 3 : High - level architecture of the situated feedback system . Figure 4 : Concept of the situated feedback system . Artwork credit : Gustavo Centurion . We designed and tested an intervention to support writers in ﬁnding and exploring different ideas [ 20 ] . The lightweight system was adapted from Hosio et al . [ 9 ] and provides decision support in form of short textual ideas that can be sorted by a number of different criteria ( see Figure 1 ) . Both the ideas , the criteria , and the rating of each idea across all criteria are provided by the crowd . We hypothesise that providing writers with an ordered list of ideas will help them in being more creative and in exploring the solution space . We found that in a laboratory study , users ( n = 24 ) were strongly inﬂuenced by a status quo bias . Many used their own intuition to complete the given writing task , without turning to the creativity support system for help . Among the users that did use the system , we noticed that the readily - provided formative feedback cannibalised and stiﬂed the participants’ own divergent thinking . Many participants simply reproduced the top answer from the ordered list and completed the writing task without ever reﬂecting on the provided ideas . Consistent with the literature , we ob - served that feedback receivers pick the feedback that is well - aligned with their own mental model , and may disre - gard feedback that they perceive as being subjective [ 26 ] . uiCrowd : Supporting Web Design uiCrowd is a system that allows the community of a website to use the website as a canvas for expressing their needs . Users can directly manipulate ( move , delete , resize ) the elements of the webpage . The system was evaluated in a remotely - administered user study ( n = 48 ) . The study par - ticipants were tasked to adapt a website user interface in a multi - step process that included a tutorial to familiarise the participant with the system . The distribution of the number of moved elements in the webpage layouts created by the crowd followed a power law , with few layouts having many manipulations of ele - ments , and many having few manipulations . More specif - ically , the 13 layouts ( 20 . 97 % ) with the highest number of moves constituted 80 % of all element moves . Among many other ﬁndings , we found that most of the manipula - tions were done without providing any justiﬁcation , even though we designed the system to make it easy to provide such feedback ( see the user interface dialog in Figure 2 ) . The web designer faces the problem that a potentially large number of designs need to be evaluated . We alleviated this problem by incorporating peer review from the crowd into the uiCrowd system . The crowd , however , faces a similar challenge . Many user interface adaptations looked virtually identical due to the low amount of element manipulations . While the crowd was able to provide contributions to the web designer , interpreting the meaning of the layouts would require considerable effort , as most users did not disclose their intentions behind their modiﬁcations of the user inter - face . Future systems would need to ensure that the users’ motivation is elicited . This study highlighted the challenges and difﬁculties that creative workers face when selecting among many design alternatives . In the following study , we explored a way of addressing these issues . Supporting and Directing Creativity with Roles One way of alleviating the challenge associated with the large quantity , diversity and low interpretability of crowd feedback is by reducing complexity in the ﬁrst place . To this end , we analysed how roles could be used to nudge the crowd to provide feedback with a narrower focus . In Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 648 a fully - counterbalanced factorial experiment , we asked 60 workers from an online crowdsourcing platform to com - plete Guilford’s Alternative Uses task [ 8 ] under three con - ditions : 1 ) without any support , 2 ) while assuming a role , and 3 ) with a role and a stimulating image . We invited an - other eight participants to complete the same task in a local workshop . Figure 5 : Prototype of the situated feedback elicitation system . Artwork credit : Stéphan Valentin . Figure 6 : Example of a type of feedback in the situated feedback system . Artwork credit : Aman Ravi . We found that roles are not a silver bullet for augmenting creativity , especially in remotely administered creativity studies with uncontrollable factors . Roles did work well , however , in the situated setting , speciﬁcally when users need to overcome an impasse in the ﬂow of their ideas . The ideas generated under the two role conditions were much more narrowly focused and linked to the given role . This conﬁrmed our initial assumption that roles could be used for reducing complexity and providing feedback from a certain perspective . We hypothesise that uiCrowd users could , for instance , provide useful feedback on a website while think - ing in terms of an administrator’s needs . DUPLEX : Situated Art Feedback on Public Displays In our third system , we turned to exploring situated technol - ogy , as it allows us to study and observe the usage of the system more closely . With our concept for the elicitation of feedback for digital artworks ( see Figure 3 and 4 ) , we ex - plore how artists can be supported with situated feedback and technology . The system enables the situated crowd to provide feedback to artists via an installation that consists of two public displays ( see Figure 5 ) . A digital artwork is displayed on the main screen of the installation . Feedback is given on the smaller touch screen positioned in front of the main screen . In total , we implemented and evaluated eight different types of feedback . One feedback type is de - picted in Figure 6 . In a preliminary needﬁnding study [ 19 ] , we found opposing views between the needs of the artists ( n = 2 ) and the prefer - ences of the users ( n = 12 ) . Users preferred simple and fast feedback types , such as multiple choice lists , Likert - scale sliders and thumbs up / down ratings , but also engaging and thought - provoking ones , such as painting on the artwork to identify certain areas of interest . “Selﬁe” images and video recordings were seen as an intrusion of privacy and most study participants did not know how to respond to this sur - prising type of feedback . The artists , on the other hand , appreciate getting a detailed insight into whether people understand their art and how people react to their artworks . Simple feedback mecha - nisms – as commonly found on Social Media websites – convey little informational value for artists to this end . The artists further face a quandary when it comes to reviewing vast amounts of feedback . While the artists thought that seeing individual reactions to their artwork would be inter - esting , they mentioned that feedback from the crowd would only spike their interest if a critical mass of feedback was reached . The artists agreed that reviewing such large quan - tities of feedback would be tedious , time consuming , and simply not feasible . Future research will need to contrast and reconcile these differences to ﬁnd a common ground . Future Work My ongoing work in the form of a structured literature re - view will guide the development of a taxonomy of feed - back types and activities that creative individuals engage in when they evaluate feedback from the crowd . This work will culminate in a conceptual framework to highlight critical processes that affect the feedback receiver when evaluat - ing large quantities of crowdsourced feedback . I envision this design framework to create a basis for discussing chal - lenges and issues related to the design of interactive feed - back elicitation systems . Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 649 References [ 1 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . 2018 . Crowdsourcing Similarity Judgments for Agreement Analysis in End - User Elicitation Studies . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology ( UIST ’18 ) . ACM , New York , NY , 177 – 188 . DOI : http : / / dx . doi . org / 10 . 1145 / 3242587 . 3242621 [ 2 ] Douglas C . Engelbart . 1962 . Augmenting Human Intellect : A Conceptual Framework . Summary Report . Contract AF 49 638 1024 SRI Project 3578 . ( October 1962 ) . [ 3 ] Eureka Foong , Steven P . Dow , Brian P . Bailey , and Elizabeth M . Gerber . 2017a . Online Feedback Exchange : A Framework for Understanding the Socio - Psychological Factors . In Proc . 2017 Conf . Human Factors in Computing Systems ( CHI ’17 ) . ACM , New York , NY , 4454 – 4467 . DOI : http : / / dx . doi . org / 10 . 1145 / 3025453 . 3025791 [ 4 ] Eureka Foong , Darren Gergle , and Elizabeth M . Gerber . 2017b . Novice and Expert Sensemaking of Crowdsourced Design Feedback . Proc . ACM Hum . - Comput . Interact . 1 , CSCW , Article 45 ( Dec . 2017 ) , 18 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3134680 [ 5 ] Jonas Frich , Lindsay MacDonald Vermeulen , Christian Remy , Michael Mose Biskjaer , and Peter Dalsgaard . 2019 . Mapping the Landscape of Creativity Support Tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . ACM , New York , NY , USA , Article 389 , 18 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3290605 . 3300619 [ 6 ] Michael D . Greenberg , Matthew W . Easterday , and Elizabeth M . Gerber . 2015 . Critiki : A Scaffolded Approach to Gathering Design Feedback from Paid Crowdworkers . In Proc . 2015 ACM SIGCHI Conf . Creativity and Cognition ( C & C ’15 ) . ACM , New York , NY , 235 – 244 . DOI : http : / / dx . doi . org / 10 . 1145 / 2757226 . 2757249 [ 7 ] Joy Paul Guilford . 1950 . Creativity . American Psychologist 5 ( 1950 ) , 444 – 454 . [ 8 ] Joy Paul Guilford , Paul R . Christensen , Philip R . Merriﬁeld , and Robert C . Wilson . 1978 . Alternate Uses : Manual of Instructions and Interpretation . Sheridan Psychological Services , Orange , CA . [ 9 ] Simo Hosio , Jorge Goncalves , Theodoros Anagnostopoulos , and Vassilis Kostakos . 2016 . Leveraging Wisdom of the Crowd for Decision Support . In Proceedings of the 30th International BCS Human Computer Interaction Conference : Fusion ! ( HCI ’16 ) . BCS Learning & Development Ltd . , Swindon , UK , Article 38 , 12 pages . DOI : http : / / dx . doi . org / 10 . 14236 / ewic / HCI2016 . 38 [ 10 ] Jeff Howe . 2006 . The Rise of Crowdsourcing . Wired Magazine 14 , 6 ( 2006 ) , 1 – 4 . [ 11 ] Julie Hui , Amos Glenn , Rachel Jue , Elizabeth Gerber , and Steven Dow . 2015 . Using Anonymity and Communal Efforts to Improve Quality of Crowdsourced Feedback . In Third AAAI Conference on Human Computation and Crowdsourcing ( HCOMP ’15 ) . AAAI , 10 pages . Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 650 [ 12 ] Hyeonsu B . Kang , Gabriel Amoako , Neil Sengupta , and Steven P . Dow . 2018 . Paragon : An Online Gallery for Enhancing Design Feedback with Visual Examples . In Proc . Conf . Human Factors in Computing Systems ( CHI ’18 ) . ACM , New York , NY , Article 606 , 13 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3173574 . 3174180 [ 13 ] Aniket Kittur . 2010 . Crowdsourcing , Collaboration and Creativity . XRDS 17 , 2 ( Dec . 2010 ) , 22 – 26 . DOI : http : / / dx . doi . org / 10 . 1145 / 1869086 . 1869096 [ 14 ] Yubo Kou and Colin M . Gray . 2017 . Supporting Distributed Critique Through Interpretation and Sense - Making in an Online Creative Community . Proc . ACM Hum . - Comput . Interact . 1 , CSCW , Article 60 ( Dec . 2017 ) , 18 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3134695 [ 15 ] Sneha R . Krishna Kumaran , Deana C . McDonagh , and Brian P . Bailey . 2017 . Increasing Quality and Involvement in Online Peer Feedback Exchange . Proc . ACM Hum . - Comput . Interact . 1 , CSCW , Article 63 ( Dec . 2017 ) , 18 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3134698 [ 16 ] Kurt Luther , Nathan Hahn , Steven P . Dow , and Aniket Kittur . 2015a . Crowdlines : Supporting Synthesis of Diverse Information Sources through Crowdsourced Outlines . In Third AAAI Conference on Human Computation and Crowdsourcing ( HCOMP ’15 ) . AAAI , 10 pages . [ 17 ] Kurt Luther , Jari - Lee Tolentino , Wei Wu , Amy Pavel , Brian P . Bailey , Maneesh Agrawala , Björn Hartmann , and Steven P . Dow . 2015b . Structuring , Aggregating , and Evaluating Crowdsourced Design Critique . In Proc . Conf . Computer Supported Cooperative Work & Social Computing ( CSCW ’15 ) . ACM , New York , NY , 473 – 485 . DOI : http : / / dx . doi . org / 10 . 1145 / 2675133 . 2675283 [ 18 ] Tricia J . Ngoon , C . Ailie Fraser , Ariel S . Weingarten , Mira Dontcheva , and Scott Klemmer . 2018 . Interactive Guidance Techniques for Improving Creative Feedback . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , New York , NY , Article 55 , 11 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3173574 . 3173629 [ 19 ] Jonas Oppenlaender and Simo Hosio . 2019 . Towards Eliciting Feedback for Artworks on Public Displays . In Proceedings of the ACM Conference on Creativity & Cognition ( C & C ’19 ) . ACM , New York , NY , USA . DOI : http : / / dx . doi . org / 10 . 1145 / 3325480 . 3326583 [ 20 ] Jonas Oppenlaender , Elina Kuosmanen , Jorge Goncalves , and Simo Hosio . 2019a . Search Support for Exploratory Writing . In Proceedings of the 17th IFIP TC 13 International Conference on Human - Computer Interaction ( INTERACT ’19 ) . Springer , Berlin and Heidelberg , 21 pages . Forthcoming . [ 21 ] Jonas Oppenlaender , Maximilian Mackeprang , Abderrahmane Khiat , Maja Vukovic , Jorge Goncalves , and Simo Hosio . 2019b . DC2S2 : Workshop on Designing Crowd - powered Creativity Support Systems . In Adjunct Proceedings of the 2019 ACM CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . ACM , New York , NY , USA . DOI : http : / / dx . doi . org / 10 . 1145 / 3290607 . 3299027 Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 651 [ 22 ] Jonas Oppenlaender , Naghmi Shireen , Maximilian Mackeprang , Halil Erhan , Jorge Goncalves , and Simo Hosio . 2019c . Workshop on Crowd - powered Interfaces for Creative Design Thinking . In Proceedings of the 12th ACM SIGCHI Conference on Creativity and Cognition ( C & C’19 ) . ACM , New York , NY , USA . DOI : http : / / dx . doi . org / 10 . 1145 / 3325480 . 3326553 [ 23 ] Ben Shneiderman . 2009 . Creativity Support Tools : A Grand Challenge for HCI Researchers . In Engineering the User Interface : From Research to Practice , Miguel Redondo , Crescencio Bravo , and Manuel Ortega ( Eds . ) . Springer London , London , 1 – 9 . DOI : http : / / dx . doi . org / 10 . 1007 / 978 - 1 - 84800 - 136 - 7 _ 1 [ 24 ] Karl E . Weick , Kathleen M . Sutcliffe , and David Obstfeld . 2005 . Organizing and the Process of Sensemaking . Organization Science 16 , 4 ( July 2005 ) , 409 – 421 . DOI : http : / / dx . doi . org / 10 . 1287 / orsc . 1050 . 0133 [ 25 ] Anbang Xu , Shih - Wen Huang , and Brian Bailey . 2014 . Voyant : Generating Structured Feedback on Visual Designs Using a Crowd of Non - experts . In Proc . Conf . Computer Supported Cooperative Work & Social Computing ( CSCW ’14 ) . ACM , New York , NY , 1433 – 1444 . DOI : http : / / dx . doi . org / 10 . 1145 / 2531602 . 2531604 [ 26 ] Yu - Chun ( Grace ) Yen . 2017 . Enhancing the Usage of Crowd Feedback for Iterative Design . In Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition ( C & C ’17 ) . ACM , New York , NY , 513 – 517 . DOI : http : / / dx . doi . org / 10 . 1145 / 3059454 . 3078701 [ 27 ] Alvin Yuan , Kurt Luther , Markus Krause , Sophie Isabel Vennix , Steven P Dow , and Bjorn Hartmann . 2016 . Almost an Expert : The Effects of Rubrics and Expertise on Perceived Value of Crowdsourced Design Critiques . In Proc . Conf . Computer - Supported Cooperative Work & Social Computing ( CSCW ’16 ) . ACM , New York , NY , 1005 – 1017 . DOI : http : / / dx . doi . org / 10 . 1145 / 2818048 . 2819953 Session : Graduate Student Symposium CC ’19 , June 23 – 26 , 2019 , San Diego , CA , USA 652