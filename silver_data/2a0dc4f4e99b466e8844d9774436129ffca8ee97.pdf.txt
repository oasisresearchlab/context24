Bystanders of Online Moderation : Examining the Effects of Witnessing Post - Removal Explanations SHAGUN JHAVER , Rutgers University , USA HIMANSHU RATHI , Rutgers University , USA KOUSTUV SAHA , University of Illinois at Urbana - Champaign , USA Prior research on transparency in content moderation has demonstrated the benefits of offering post - removal explanations to sanctioned users . In this paper , we examine whether the influence of such explanations transcends those who are moderated to the bystanders who witness such explanations . We conduct a quasi - experimental study on two popular Reddit communities ( r / askreddit and r / science ) by collecting their data spanning 13 months—a total of 85 . 5M posts made by 5 . 9M users . Our causal - inference analyses show that bystanders significantly increase their posting activity and interactivity levels as compared to their matched control set of users . Our findings suggest that explanations clarify and reinforce the social norms of online spaces , enhance community engagement , and benefit many more members than previously understood . We discuss the theoretical implications and design recommendations of this research , focusing on how investing more efforts in post - removal explanations can help build thriving online communities . CCS Concepts : • Human - centered computing → Empirical studies in collaborative and social computing ; Social media . Additional Key Words and Phrases : content moderation , social media , transparency ACM Reference Format : Shagun Jhaver , Himanshu Rathi , and Koustuv Saha . 2023 . Bystanders of Online Moderation : Examining the Effects of Witnessing Post - Removal Explanations . In . ACM , New York , NY , USA , 13 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION As the social media ecosystem continues to expand rapidly , platform designers and researchers are experimenting with new models of digital governance [ 22 , 35 , 52 ] . Recent research has also begun extending guiding principles that could possibly serve such models [ 53 , 67 ] . This includes rights - based legal approaches , such as international human rights law and American civil rights law [ 12 ] . The HCI community has especially centered around aspirational computer science principles of fairness , accountability , transparency , ethics , and responsibility [ 63 ] . Most famously , a group of human rights organizations , advocates , and academic experts developed and launched what they termed “the Santa Clara Principles on Transparency and Accountability in Content Moderation , ” which aim to guide platforms on how to incorporate meaningful transparency and accountability around moderation of user - generated content [ 51 ] . Some recent empirical research on incorporating meaningful transparency and how it may benefit users , as well as platforms , has also begun to emerge . For example , transparency through removal notification and providing moderators’ reasoning behind content removal has been shown as one of the key factors in users’ perception of the fairness of content moderation [ 17 ] . Another Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii © 2023 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - XXXX - X / 18 / 06 . . . $ 15 . 00 https : / / doi . org / XXXXXXX . XXXXXXX 1 a r X i v : 2309 . 08361v1 [ c s . H C ] 15 S e p 2023 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . study has shown that when offered removal explanations in any online community , users tend to improve their posting behavior in that community in the future [ 20 ] . Such evidence has been used to motivate platforms , community moderators , and policymakers to continue to push for increased , meaningful transparency in their moderation practices . This study seeks to add further empirical evidence to the effects of offering transparency in content moderation on social media platforms . Specifically , we look at whether such transparency can serve users other than those sanctioned . Prior research has provided evidence for the educational benefits of offering removal explanations for users whose content is removed [ 17 , 20 ] . However , the effects on bystanders who witness the post - removal and the explanation behind it have not been tested . In this research , we ask the question : Do public removal explanations intended for the sanctioned users influence the posting behavior of bystanders to those explanations ? We collected a dataset of 85 . 5M posts from two large Reddit communities , r / AskReddit and r / science , over the time period Dec 2021 – Dec 2022 , and developed a computational framework based on causal inference that matched users who witnessed a removal explanation in June 2022 with users who did not witness any explanation . Comparing the post - treatment behavior of these matched groups , we found that exposure to removal explanations significantly boosted the posting activity and interactivity of bystanders as compared to non - bystanders . This shows that the educational benefits of moderation transparency are more broadly applicable than previously understood [ 20 ] . Drawing upon this insight , we argue that community managers must invest more time and effort in increasing moderation transparency through explanation messages . On the other hand , witnessing explanation messages did not significantly enhance the posting quality of bystanders . We speculate on the causes of this empirical insight and offer directions for future research that may help us better understand the role of explanation messages . 2 BACKGROUND AND RELATED WORK 2 . 1 Transparency in Content Moderation Moderation systems on social media platforms are designed for governance purposes and often impose measures such as removing content , muting , or banning offenders [ 10 , 13 ] . These measures are implemented by content moderators , who may either be volunteers among the platform’s user base or commercial content moderators hired by the platform [ 44 , 56 ] . More recently , AI - driven tools have been used to assist in moderation processes [ 1 , 14 , 24 , 28 ] . We focus here on transparency in end - users’ experience with moderation processes . Transparency implies opening up “the working procedures not immediately visible to those not directly involved to demonstrate the good working of an institution” [ 41 ] . We situate our work within a line of research that examines the impact of content moderation on end - users . Scholars have investigated the impact of both user - level [ 17 , 19 , 55 , 64 ] and community - wide sanctions [ 4 , 5 ] . This has included studies using a variety of methods , such as interviews [ 24 ] , design workshops [ 63 ] , surveys [ 17 , 65 ] , and log analyses [ 4 , 5 , 19 ] . Some studies in this area have also highlighted the benefits of offering moderation explanations to sanctioned users [ 17 , 20 ] . Our focus is also on end - users who witness , although they are not directly affected by , the moderation sanctions . By doing so , we contribute to building a theory [ 29 ] that prescribes to community managers which moderation interventions should be deployed , under what circumstances , and with what expected outcomes . In examining the complexities of enacting content moderation , researchers have identified several issues regarding transparency in the procedures followed by platforms when applying punitive measures [ 34 ] . First , the criteria for determining inappropriate content might not be well - established before moderation decisions are made [ 54 ] . Legal experts have raised concerns that 2 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii despite social media platforms publicly sharing their content policies , they often fail to adequately consider the contextual factors surrounding the content , such as its localized meaning and the identities of the speakers and audiences , when evaluating its appropriateness [ 66 ] . Second , there are inter - platform differences in how norm violations are conceptualized . For example , an HCI study comparing the content policies of 15 platforms found a lack of consensus in defining what qualifies as online harassment and how forcefully content deemed as harassment should be moderated [ 43 ] . Consequently , when these vague content policies are implemented for content regulation , it can lead to ambiguity in resolving moderation cases [ 66 ] . Finally , and most pertinent to our study , communication with end - users regarding moderation decisions is often found to be deficient in details [ 58 , 65 ] . 2 . 2 Removal Explanations and Bystanders to Norm Violations Several prior studies have emphasized the significance of incorporating moderation notifications and explanations into the design of moderation systems [ 20 , 31 , 32 , 62 ] . For example , researchers have shown that when Facebook and Reddit platforms do not inform users about their content removal [ 58 ] , users question which platform policy they have violated [ 17 , 65 ] . Besides removal notification , users desire a justification for why their posts got removed , deeming it a significant factor in their perception of moderation fairness [ 17 ] . Users also express dissatisfaction with the inconsistent punishments meted out to them versus others , leading them to request explanations further [ 33 , 62 ] . Many studies have empirically shown the benefits of offering removal explanations in improving the behavior of moderated users [ 17 , 20 , 61 ] . For example , Tyler et al . found that users who were provided education about platform rules in the week following their post removal were less likely to post new violating content [ 61 ] . We extend this research by investigating the utility of explanations in influencing the behavior of bystanders . Curiously , Reddit moderators offer explanations publicly by commenting on the removed submis - sion . While this is not the sole communication mode—indeed , many moderators privately message users to inform them about moderation—prior research has argued that public explanations serve to enhance broader transparency efforts [ 17 , 20 ] . On Reddit , users already engaging with a post retain access to it even after it is removed from the main subreddit ; in this sense , removed submissions are not really removed , just hidden from the public view . By publicly explaining the reason behind post removal , explanation comments serve users who stumble upon it or are already engaged . Encouraging voluntary compliance with behavioral norms in a community requires that com - munity members know the norms and be aware of them when being active within the community . Kiesler et al . [ 29 ] argue that people learn the community norms in three ways : ( 1 ) observing other people’s behavior and its consequences , ( 2 ) seeing codes of conduct , and ( 3 ) behaving and directly receiving feedback . Prior research has demonstrated the importance of users seeing codes of conduct [ 36 ] and directly receiving feedback in improving their subsequent behavior [ 20 , 61 ] . We focus here on establishing the utility of bystanders observing other people’s norm violations and the resulting consequences . In terms of reducing the posting of norm - violating content , some research has focused on the roles bystanders can play in the context of online harassment . Blackwell et al . found that labeling a variety of technology - enabled abusive experiences as ‘online harassment’ helps bystanders understand the breadth and depth of this problem [ 3 ] . Further , designs that motivate bystander intervention discourage harassment through normative enforcement [ 2 ] . Taylor et al . [ 59 ] additionally found that design solutions that encourage empathy and accountability can promote bystander intervention to cyberbullying . Extending this line of research to a broader range of norm violations , we analyze how bystanders are affected by their exposure to post - removal explanations . 3 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . ( a ) ( b ) Fig . 1 . Examples of post - removals and explanations by a moderator on ( a ) r / Askreddit ( here , the explanation is provided by the Automoderator ) , and ( b ) r / science ( here , the explanation is provided by a human moderator ) . 2 . 3 Observational Research on Social Media Prior HCI and CSCW research has recognized that observational analyses of social media data can serve as a valuable tool for understanding society and evaluating changes in users’ behavior , especially regarding their use of social network sites [ 57 ] . Regarding our study’s context , empirical research on the effects of various content moderation interventions has often deployed observational analyses of social media logs [ 4 , 5 , 47 , 55 ] . Similar to our work , such research has primarily examined behavior patterns over more extended timeframes , typically spanning months [ 15 , 19 , 20 ] . Examining the impact of an intervention , whether internal or external , is best studied through causal inference approaches , such as randomized controlled trials ( RCTs ) . However , these ap - proaches have certain limitations . First , experimental studies requiring participant consent can be constrained by concerns about the observer effect [ 38 ] —that individuals might alter their typical behavior when they are aware of being monitored or observed . Second , conducting experimental research without participants’ awareness is considered unethical , especially within the human - centered research paradigm [ 25 , 40 ] . Finally , conducting experiments without prior awareness of their potential impact on participants can lead to long - term adverse consequences for both platforms and individuals . As a result , observational studies can serve as a viable alternative in situations where experimental approaches may not be feasible or ethical . While observational studies may not provide true causality , they are structured to minimize confounds and investigate longitudinal data , offering stronger evidence than basic correlational analyses [ 16 ] . Recently , there has been growing interest in these types of studies within the fields of HCI and behavioral science , including those analyzing social media data [ 7 , 26 , 27 , 42 , 46 , 48 , 49 , 69 ] . Significantly , the research conducted by Saha et al . prompted us to operationalize metrics for assessing social media behavior , including factors like activity and interactivity . 3 DATA In this paper , we study the effects of observing post - removal explanations on Reddit . We conducted an observational study on two major subreddits , r / AskReddit ( 43M members ) and r / science ( 31M members ) . Fig . 1 shows example post - removals on these subreddits . We downloaded the data 4 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii from these subreddits over 13 months between 01 December 2021 – 31 December 2022 , using the pushshift . io service . The downloaded data was in a Z standard compressed and encoded format . Table 1 . Summary statistics of the Reddit dataset . Subreddit No . Submissions No . Comments r / Askreddit 287 , 954 5 , 358 , 662 r / science 2 , 453 175 , 007 We iterated through this dataset , decompressing and de - coding it in smaller chunks , and simultaneously storing the readable data into SQLite database tables . We queried the database to access the data for the ensuing analyses in the paper . Table 1 summarizes the data ( submissions and comments ) collected for our study . Note that we use the term post to indicate posting activity in the form of either submissions or comments ; therefore , for any given period T , N p ( T ) = N s ( T ) + N c ( T ) ( where N p , N s , and N c denote the number of posts , submissions , and comments respectively ) . 3 . 1 Defining Treated and Control Users Our study employed a causal - inference framework , drawing on similar approaches in prior re - search [ 5 , 27 , 50 ] . For this purpose , we defined treatment as exposure to post - removal explanation ( s ) . Accordingly , Treated users comprise the “bystanders” or the users of a subreddit who witnessed a removal explanation . On the other hand , Control users comprise users of the same subreddit who did not witness a removal explanation . Within our study period of 13 months , we considered the period between 01 - 30 June 2022 , as our treatment period , i . e . , we focused on explanations provided in this one - month period , and collected six - month pre - treatment and six - month post - treatment period data for our analyses . Note that we filtered out the data of any user who was exposed to post - removal explanations in the period between December 2021 – May 2022 to ensure that we examined Treated users subjected to treatment in June 2022 . 3 . 2 Gathering Data on Post Removal Explanations We obtained a list of 95 phrases indicating post - removal explanations from prior work by Jhaver et al . [ 20 ] . We used these phrase to query our database to collect all the removal explanations in our defined treatment period . Specifically , we queried the created database to retrieve the stickied 1 comments made by moderators within the treatment period and containing any of the above phrases . We obtained 257 removal explanations on r / AskReddit and 379 such removal explanations on r / science . Focusing on the discussion threads of each of these removal explanations , we next collected the information of the commenters , who are the bystanders or Treated users in our study . In some threads , the removed submission’s author also posted a comment in the discussion thread ; we did not include such submission authors in the Treated users groups because our analysis centers on bystanders , not moderated users . We obtained the timeline of posts made by the Treated users in the corresponding subreddit during the study period . For each subreddit , we also curated a list of Control users : this constituted users who were not Treated users , and who were not exposed to any post - removal explanations in the pre - treatment period . The Treated users were assigned with a treatment date on their first occurrence of witnessing a post - removal explanation during our treatment period . On the other hand , because the Control users did not have any treatment date per se , we simulated a set of placebo dates from the set of all possible treatment dates within the subreddit , such that the distributions of placebo dates and treatment dates were statistically similar . Then , each Control user was randomly assigned a placebo date from the set of placebo dates . For easier readability , any reference to pre - treatment and post - treatment surrounds treatment date for a Treated user , and placebo date for a Control user . 1 Removal explanations are usually stickied , i . e . , locked to appear as the top comment in the discussion thread . 5 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . 4 METHODS 4 . 1 Study Design and Rationale Our study aims to understand the effects of providing explanations to post removals in an online community . Such a problem would be best studied using an A / B Test or experimental approaches . However , conducting such an experiment has challenges and raises ethical concerns [ 25 , 38 , 40 ] . Given these considerations , we drew on quasi - experimental approaches to observational data . We adopted a causal - inference approach based on the potential outcomes framework proposed by Rubin [ 45 ] . This approach simulates an experimental setting by matching individuals ( Treated and Control ) on several covariates [ 16 ] . For a given treatment , 𝑇 , two potential outcomes are compared : ( 1 ) when a user is exposed to 𝑇 ( 𝑇 = 1 ) , and ( 2 ) when a user is not exposed to 𝑇 ( 𝑇 = 0 ) . Because it is impossible to obtain both kinds of outcomes simultaneously for the same user , this framework estimates the missing counterfactual for a user based on the outcomes of a matched user—another user with similar covariates ( attributes and behaviors ) but not exposed to 𝑇 . Our work drew motivation from prior works that adopted similar causal - inference approaches on social media data [ 5 , 27 , 49 ] . 4 . 2 Matching for Causal - Inference 4 . 2 . 1 Covariates for Matching . We operationalized a number of covariates that we would use for matching the Treated and Control users , movitated from prior work [ 5 , 8 , 20 , 27 , 50 ] , as listed below . Each covariate was measured using the data in the user’s pre - treatment history . Frequency of Comments : The normalized quantity of comments per day in the pre - treatment period . Frequency of Submissions : The normalized quantity of submissions per day in the pre - treatment period . User Interactivity : The ratio of number of comments to total number of posts . Submission Removal Rate : The ratio of removal submissions to total submissions posted by the user . Karma : The average karma across the comments and submissions made by the user . Normalized 𝑛 - grams : The normalized occurrences of the top 1000 𝑛 - grams ( 𝑛 = 1 , 2 ) . 4 . 2 . 2 Stratified Propensity Score Matching . As mentioned above , we used matching to find pairs ( generalizable to groups ) of Treated and Control users with statistically similar covariates . We adopted the propensity score matching approach that matches users based on propensity scores , which is essentially a user’s likelihood of receiving the treatment . However , exact one - to - one propensity score matching can suffer from biases [ 30 ] . Therefore , motivated by prior work [ 27 , 48 , 68 ] , we adopted stratified propensity score matching that can balance the bias - variance tradeoff of either too biased ( one - to - one match ) or too variant ( unmatched ) data comparisons . In a stratified matching approach , users with similar propensity scores are grouped into strata . Hence , every stratum consists of users with similar covariates [ 27 ] . Through this approach , we isolated and estimated treatment effects within each stratum . For the above matching , we computed the propensity scores by building a logistic regression model with the covariates as independent variables and a user’s binary treatment score ( 1 for Treated users and 0 for Control users ) as dependent variable . We segregated the distribution of propensity scores into 200 strata of equal width . To ensure that our causal analysis was restricted to a sufficient number of similar users , we discarded strata with less than 10 Treated and 10 Control users . This led to a final matched dataset of 50 strata ( 4 , 842 Treated users and 146 , 922 Control users ) in r / AskReddit and 33 strata ( 4 , 890 Treated users and 176 , 324 Control users ) in r / science . 6 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii 4 . 3 Measuring Treatment Effects After matching the Treated and Control users , we measured the differences in the post - treatment behaviors of the users . For this , we operationalized three outcomes—1 ) Frequency of posting , 2 ) Interactivity , and 3 ) Submission Removal Rate for the users in the post - treatment period . Drawing on the difference in differences approach in causal - inference [ 11 ] , we calculated the average treatment effect ( ATE ) as the average of the difference of changes in the Treated users and the Control users per stratum . In addition , we obtained the effect size ( Cohen’s 𝑑 ) and evaluated statistical significance in differences using relative 𝑡 - tests . We conducted Kolmogorov - Smirnov ( 𝐾𝑆 ) test to evaluate the differences in the distributions of the Treated and Control groups’ outcomes . 5 RESULTS Table 2 summarizes our observations of the differences in the post - treatment outcomes in our study . We describe our findings below : Table 2 . Summary of changes in outcomes for the Treated and Control individuals . We report average treatment effect ( ATE ) , effect size ( Cohen’s 𝑑 ) , rel - ative 𝑡 - test , and KS - test statistics ( * 𝑝 < 0 . 01 , * * 𝑝 < 0 . 001 , * * * 𝑝 < 0 . 0001 . Outcome ATE Cohen’s 𝑑 𝑡 - test 𝐾𝑆 - test r / AskReddit Posting Frequency 0 . 453 0 . 807 6 . 589 * * * 0 . 640 * * * Interactivity 0 . 193 2 . 392 12 . 233 * * * 0 . 960 * * * Post Removal Rate 0 . 000 0 . 005 0 . 024 0 . 200 r / science Posting Frequency 0 . 025 1 . 075 8 . 890 * * * 0 . 515 * * * Interactivity 0 . 216 1 . 445 17 . 469 * * * 0 . 879 * * * Post Removal Rate 0 . 001 0 . 007 0 . 177 0 . 303 Posting Frequency . We find significant differ - ences in the posting frequency of Treated and matched Control individuals . On r / AskReddit , the ATE is 0 . 453 , which can be roughly interpreted as the treatment increases the frequency of posts by 1 for about 45 . 3 % of the individuals . We see a high effect size ( 0 . 807 ) and significant differences as per 𝑡 - test and 𝐾𝑆 - test ( 𝑝 < 0 . 0001 ) . We also see con - vergent findings in r / science with an ATE of 0 . 025 , Cohen’s 𝑑 of 1 . 075 , and significant differences as per 𝑡 - test and 𝐾𝑆 - test ( 𝑝 < 0 . 0001 ) . Higher post - ing frequency indicates that the Treated users ( by - standers ) became more active in the subreddits after witnessing the post - removal explanations . This measure is an indicator of positive commu - nity behavior [ 50 ] . Interactivity . Similar to the above , we find significant differences in the interactivity of Treated and Control individuals . On r / AskReddit we find an ATE of 0 . 193 and Cohen’s 𝑑 of 2 . 392 , along with statistical significance in differences as per 𝑡 - test and 𝐾𝑆 - test ( 𝑝 < 0 . 0001 ) . Likewise , on r / science , ATE on interactivity is 0 . 216 , Cohen’s 𝑑 is 1 . 445 , and 𝑡 - test and 𝐾𝑆 - tests reveal statistical significance ( 𝑝 < 0 . 0001 ) . In addition to higher posting frequency , higher interactivity indicates that the Treated users not only created new submissions but also replied to others’ threads—an important factor for enhancing online community engagement [ 50 ] . This suggests that post - removal explanations can potentially enhance community engagement and , subsequently , the sustainability and growth of a community with member activity . Post Removal Rate . Interestingly , we find no significant effects on the post removal rates . That is , we do not have conclusive evidence on whether the quality of postings improved ( or worsened ) for the Treated users . 6 DISCUSSION Online communities rely on content generated by users , but inappropriate posts can detract from the quality of the user experience . Consequently , moderation systems typically aim to boost the overall volume of contributions while reducing the need for post - removals [ 13 , 29 ] . Our analysis in this paper examined the behavioral impact of offering moderation explanations on bystanders 7 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . over two dimensions—their future posting activity and the frequency of their future post - removals . In this section , we examine the implications of our findings for moderators , site administrators , designers , and future research . 6 . 1 Removal Explanations Help Boost Posting Frequency We found that on both r / AskReddit and r / science , users who got exposed to removal explanations directed at moderated others significantly increased their posting activity as compared to users who did not witness any explanations . It could be that seeing explanation messages indicated to bystanders that the community is well - moderated . This , in turn , could have enhanced their inclination to be active within the community . We note that this result contrasts Jhaver et al . ’ findings for moderated users : exposure to removal explanations reduced these users’ future posting activity . One reason for this could be that users who suffer moderation may find it more difficult to accept the justification for their post - removals than other bystanders . Prior research has often grappled with the tradeoffs of moderation actions reducing posting traffic at the cost of improving posting quality [ 15 , 17 , 19 , 20 ] . However , in this study’s context , for any given removed submission , there is only one moderated user but potentially many more bystanders . Thus , our results suggest that providing explanation messages may boost the overall posting frequency in a community . This empirical insight offers a powerful incentive to community managers considering the deployment of explanation messages . 6 . 2 Removal Explanations Help Increase Community Engagement We found that exposure to others’ explanation messages increases the posting interactivity . That is , bystanders’ comments constitute a greater proportion of their posting volume after the treatment . Prior research has shown that this metric is an important factor in community engagement [ 50 ] . Therefore , this finding suggests that observing the reasoned explanation for post removals can inform bystanders why certain types of posts are unacceptable in the community , help them learn its accepted norms [ 6 ] , and thereby increase their confidence in instituting a deeper engagement with the community . This further demonstrates the utility of offering post - removal explanations . Another explanation for this finding is that users perceive moderators attend to and regulate inappropriate submissions more than inappropriate comments . This perception may incline them to engage more in posting comments than submissions in an effort to avoid experiencing post removals themselves . As prior research shows , users often develop “folk theories” of content moderation processes in order to make sense of them [ 9 , 17 ] . Going forward , qualitative studies could inquire whether the posting activity of users is shaped by their folk theories of where the content moderation efforts are focused . 6 . 3 Removal Explanations Do not Impact Post Removals Our analysis shows that removal explanations do not significantly impact the future post - removals of bystanders . This contrasts previous results for moderated users : Jhaver et al . showed that offering removal explanations reduced the future post - removals of moderated users [ 20 ] . This suggests that explanation messages boost the posting quality of moderated users more than bystanders . Why is this the case ? One reason could be that having experienced post removal , moderated users may be likelier to attend to all community guidelines before posting their next submissions . On the other hand , witnessing a removal explanation may not be a strong enough incentive to bystanders to ensure compliance with community guidelines in their next submissions . It is possible that witnessing explanation messages educates bystanders about the violated community norm specific to the corresponding removed post and leads them to avoid the same violation in the future , yet they continue violating other community norms . While beyond the 8 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii scope of the current paper , a more granular analysis could examine whether norm - specific learning occurs through removal explanations among bystanders . 6 . 4 Design Implications This work bears design implications regarding the positive impacts of enacting transparency in online content moderation . The empirical evidence presented here informs community managers to put more effort into providing explanations for their sanctions , and more importantly , make these explanations publicly visible , so that they can educate bystanders . While content moderation has proliferated as an important aspect in online communities , providing explanations is still not as prevalent . For instance , to conduct this study , we originally started with four large subreddits— we had also collected over ∼ 2M posts from r / politics ( 8 . 4M users ) and r / technology ( 15M users ) . However , despite being large subreddits and having many moderators , neither of these communities provided any post - removal explanations ( which also prevented us from including their data in our analyses ) . Prior work has noted challenges in providing explanations in all instances , such as moderator fatigue and limitations of automated moderation tools [ 18 , 20 ] . However , with the advent of generative AI and large - language model - based technologies , it would be interesting to explore the design space of curating automated explanation messages through these emerging technologies . The computational framework of our study can be easily extended to delineate the effects of different kinds of explanations , e . g . , explanation length and politeness level . The results of such analyses can inform platform owners and community managers about the suitability of different explanation types . Community managers can also examine whether different approaches to explanations are warranted for different norm violations . 6 . 5 Limitations and Future Directions Our analyses focused on two large Reddit communities . Therefore , our results are most readily applicable to other subreddits of similar size . Future analyses would benefit from investigating the circumstances under which these results replicate ( or do not ) on other platforms and communities . The computational framework we have presented here should help such inquiries . Prior similar efforts on developing extendable computational frameworks for evaluating moderation actions have similarly used data from a limited number of samples [ 4 , 5 , 19 , 60 ] . For this project , we had initially planned a comparative analysis of the effects of human v / s bot explanations on bystanders . However , our data review showed that all r / AskReddit explanations were provided by bots and all r / science explanations by human moderators during the treatment period . Therefore , we could not conduct our planned comparative analysis for either community . Future work should explore how AI - generated explanations compare to human - offered explanations in influencing bystanders’ behavior , extending similar inquiries in prior research [ 20 ] . Our analysis does not take into account the in - situ practical concerns and constraints under which content moderators work [ 37 , 39 ] . Examining how moderators create explanations and developing tools to ease that process may help them offer explanation messages at a higher rate . 7 CONCLUSION Transparency in communications is a key concern for moderated users [ 21 , 23 , 58 ] . On the other hand , secretiveness about moderation decisions triggers speculation among users who suspect potential biases [ 17 , 21 , 65 ] . In this paper , we focus on one important mode of enacting greater transparency in moderation decisions : publicly visible messaging by moderators that reveals the reasons behind submission removals . Our analysis shows that witnessing such messages significantly boosts the posting and interactivity levels of bystanders . This suggests that adopting 9 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . an educational approach to content moderation , as opposed to a strictly punitive one , can lead to enhanced community outcomes . REFERENCES [ 1 ] Reuben Binns , Michael Veale , Max Van Kleek , and Nigel Shadbolt . 2017 . Like trainer , like bot ? Inheritance of bias in algorithmic content moderation . In International Conference on Social Informatics . Springer , 405 – 415 . [ 2 ] Lindsay Blackwell , Tianying Chen , Sarita Schoenebeck , and Cliff Lampe . 2018 . When Online Harassment is Perceived as Justified . In Twelfth International AAAI Conference on Web and Social Media . [ 3 ] Lindsay Blackwell , Jill P Dimond , Sarita Schoenebeck , and Cliff Lampe . 2017 . Classification and Its Consequences for Online Harassment : Design Insights from HeartMob . PACMHCI 1 , CSCW ( 2017 ) , 24 – 1 . [ 4 ] Eshwar Chandrasekharan , Shagun Jhaver , Amy Bruckman , and Eric Gilbert . 2022 . Quarantined ! Examining the Effects of a Community - Wide Moderation Intervention on Reddit . ACM Trans . Comput . - Hum . Interact . ( 2022 ) . https : / / doi . org / 10 . 1145 / 3490499 [ 5 ] Eshwar Chandrasekharan , Umashanthi Pavalanathan , Anirudh Srinivasan , Adam Glynn , Jacob Eisenstein , and Eric Gilbert . 2017 . You can’t stay here : The efficacy of reddit’s 2015 ban examined through hate speech . Proceedings of the ACM on human - computer interaction 1 , CSCW ( 2017 ) , 1 – 22 . [ 6 ] Eshwar Chandrasekharan , Mattia Samory , Shagun Jhaver , Hunter Charvat , Amy Bruckman , Cliff Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s Hidden Rules : An Empirical Study of Reddit Norm Violations at Micro , Meso , and Macro Scales . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) , 32 . [ 7 ] Munmun De Choudhury , Shagun Jhaver , Benjamin Sugar , and Ingmar Weber . 2016 . Social Media Participation in an Activist Movement for Racial Equality . In Tenth International AAAI Conference on Web and Social Media ( 2016 ) . [ 8 ] Munmun De Choudhury , Emre Kiciman , Mark Dredze , Glen Coppersmith , and Mrinal Kumar . 2016 . Discovering shifts to suicidal ideation from mental health content in social media . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , 2098 – 2110 . [ 9 ] Motahhare Eslami , Aimee Rickman , Kristen Vaccaro , Amirhossein Aleyasen , Andy Vuong , Karrie Karahalios , Kevin Hamilton , and Christian Sandvig . 2015 . I always assumed that I wasn’t really that close to [ her ] : Reasoning about Invisible Algorithms in News Feeds . In Proceedings of the 33rd annual ACM conference on human factors in computing systems . ACM , 153 – 162 . [ 10 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , content moderation , and the hidden decisions that shape social media . Yale University Press . [ 11 ] Andrew Goodman - Bacon . 2021 . Difference - in - differences with variation in treatment timing . Journal of Econometrics 225 , 2 ( 2021 ) , 254 – 277 . [ 12 ] Robert Gorwa . 2019 . What is platform governance ? Information , Communication & Society ( 2019 ) , 1 – 18 . [ 13 ] James Grimmelmann . 2015 . The virtues of moderation . Yale JL & Tech . 17 ( 2015 ) , 42 . [ 14 ] Manoel Horta Ribeiro , Justin Cheng , and Robert West . 2023 . Automated Content Moderation Increases Adherence to Community Guidelines . In Proceedings of the ACM Web Conference 2023 . 2666 – 2676 . [ 15 ] ManoelHortaRibeiro , ShagunJhaver , SavvasZannettou , JeremyBlackburn , GianlucaStringhini , EmilianoDeCristofaro , and Robert West . 2021 . Do Platform Migrations Compromise Content Moderation ? Evidence from r / The _ Donald and r / Incels . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 316 ( oct 2021 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3476057 [ 16 ] Guido W Imbens and Donald B Rubin . 2015 . Causal inference in statistics , social , and biomedical sciences . Cambridge University Press . [ 17 ] Shagun Jhaver , Darren Scott Appling , Eric Gilbert , and Amy Bruckman . 2019 . “Did You Suspect the Post Would Be Removed ? ” : Understanding User Reactions to Content Removals on Reddit . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 192 ( Nov . 2019 ) , 33 pages . https : / / doi . org / 10 . 1145 / 3359294 [ 18 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - Machine Collaboration for Content Regulation : The Case of Reddit Automoderator . ACM Trans . Comput . - Hum . Interact . 26 , 5 , Article 31 ( July 2019 ) , 35 pages . https : / / doi . org / 10 . 1145 / 3338243 [ 19 ] Shagun Jhaver , Christian Boylston , Diyi Yang , and Amy Bruckman . 2021 . Evaluating the Effectiveness of Deplatforming as a Moderation Strategy on Twitter . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 381 ( oct 2021 ) , 30 pages . https : / / doi . org / 10 . 1145 / 3479525 [ 20 ] Shagun Jhaver , Amy Bruckman , and Eric Gilbert . 2019 . Does Transparency in Moderation Really Matter ? User Behavior After Content Removal Explanations on Reddit . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 150 ( nov 2019 ) , 27 pages . https : / / doi . org / 10 . 1145 / 3359252 [ 21 ] Shagun Jhaver , Quan Ze Chen , Detlef Knauss , and Amy X . Zhang . 2022 . Designing Word Filter Tools for Creator - Led Comment Moderation . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( New Orleans , LA , USA ) ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , Article 205 , 21 pages . 10 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii https : / / doi . org / 10 . 1145 / 3491102 . 3517505 [ 22 ] Shagun Jhaver , Seth Frey , and Amy X . Zhang . 2023 . Decentralizing Platform Power : A Design Space of Multi - level Governance in Online Social Platforms . Social Media + Society ( 2023 ) , 10 pages . [ 23 ] Shagun Jhaver , Sucheta Ghoshal , Amy Bruckman , and Eric Gilbert . 2018 . Online Harassment and Content Moderation : The Case of Blocklists . ACM Trans . Comput . - Hum . Interact . 25 , 2 , Article 12 ( March 2018 ) , 33 pages . https : / / doi . org / 10 . 1145 / 3185593 [ 24 ] Shagun Jhaver , Alice Qian Zhang , Quan Ze Chan , Nikhila Natarajan , Ruotong Wang , and Amy X . Zhang . 2023 . Personalizing Content Moderation on Social Media : User Perspectives on Moderation Choices , Interface Design , and Labor . Proc . ACM Hum . - Comput . Interact . ( 2023 ) , 33 pages . [ 25 ] Jukka Jouhki , Epp Lauk , Maija Penttinen , Niina Sormanen , and Turo Uskali . 2016 . Facebook’s emotional contagion experiment as a challenge to research ethics . Media and Communication 4 , 4 ( 2016 ) . [ 26 ] Katherine Keith , David Jensen , and Brendan O’Connor . 2020 . Text and Causal Inference : A Review of Using Text to Remove Confounding from Causal Estimates . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . [ 27 ] Emre Kiciman , Scott Counts , and Melissa Gasser . 2018 . Using longitudinal social media analysis to understand the effects of early college alcohol use . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 12 . [ 28 ] Charles Kiene and Benjamin Mako Hill . 2020 . Who Uses Bots ? A Statistical Analysis of Bot Usage in Moderation Teams . In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI EA ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 8 . https : / / doi . org / 10 . 1145 / 3334480 . 3382960 [ 29 ] Sara Kiesler , Robert Kraut , and Paul Resnick . 2012 . Regulating behavior in online communities . Building Successful Online Communities : Evidence - Based Social Design ( 2012 ) . [ 30 ] Gary King and Richard Nielsen . 2019 . Why propensity scores should not be used for matching . Political analysis 27 , 4 ( 2019 ) , 435 – 454 . [ 31 ] Yubo Kou , Xinning Gui , Shaozeng Zhang , and Bonnie Nardi . 2017 . Managing Disruptive Behavior through Non - Hierarchical Governance : Crowdsourcing in League of Legends and Weibo . Proc . ACM Hum . - Comput . Interact . 1 , CSCW , Article 62 ( dec 2017 ) , 17 pages . https : / / doi . org / 10 . 1145 / 3134697 [ 32 ] Renkai Ma and Yubo Kou . 2021 . " How Advertiser - Friendly is My Video ? " : YouTuber’s Socioeconomic Interactions with Algorithmic Content Moderation . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 429 ( oct 2021 ) , 25 pages . https : / / doi . org / 10 . 1145 / 3479573 [ 33 ] Renkai Ma and Yubo Kou . 2022 . " I’m Not Sure What Difference is between Their Content and Mine , Other than the Person Itself " : A Study of Fairness Perception of Content Moderation on YouTube . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 425 ( nov 2022 ) , 28 pages . https : / / doi . org / 10 . 1145 / 3555150 [ 34 ] Renkai Ma and Yubo Kou . 2023 . " Defaulting to Boilerplate Answers , They Didn’t Engage in a Genuine Conversation " : Dimensions of Transparency Design in Creator Moderation . Proc . ACM Hum . - Comput . Interact . 7 , CSCW1 , Article 44 ( apr 2023 ) , 26 pages . https : / / doi . org / 10 . 1145 / 3579477 [ 35 ] Mike Masnick . 2019 . Protocols , Not Platforms : A Technological Approach to Free Speech . https : / / knightcolumbia . org / content / protocols - not - platforms - a - technological - approach - to - free - speech [ 36 ] J Nathan Matias . 2016 . Posting rules in online discussions prevents problems & increases participation . Civil Servant 8 ( 2016 ) . [ 37 ] Nathan J . Matias . 2016 . The Civic Labor of Online Moderators . In Internet Politics and Policy conference ( Oxford , United Kingdom ) . Oxford , United Kingdom . [ 38 ] Jim McCambridge , John Witton , and Diana R Elbourne . 2014 . Systematic review of the Hawthorne effect : new concepts are needed to study research participation effects . Journal of clinical epidemiology 67 , 3 ( 2014 ) , 267 – 277 . [ 39 ] Aiden McGillicuddy , Jean - Gregoire Bernard , and Jocelyn Cranefield . 2016 . Controlling Bad Behavior in Online Communities : An Examination of Moderation Work . ICIS 2016 Proceedings ( dec 2016 ) . http : / / aisel . aisnet . org / icis2016 / SocialMedia / Presentations / 23 [ 40 ] Jacob Metcalf and Kate Crawford . 2016 . Where are human subjects in big data research ? The emerging ethics divide . Big Data & Society 3 , 1 ( 2016 ) , 2053951716650211 . [ 41 ] Cornelia Moser . 2001 . How open is’ open as possible’ ? : three different approaches to transparency and openness in regulating access to EU documents . ( 2001 ) . [ 42 ] AlexandraOlteanu , OnurVarol , andEmreKiciman . 2017 . DistillingtheOutcomesofPersonalExperiences : APropensity - Scored Analysis of Social Media . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( Portland , Oregon , USA ) ( CSCW ’17 ) . Association for Computing Machinery , New York , NY , USA , 370 – 386 . https : / / doi . org / 10 . 1145 / 2998181 . 2998353 [ 43 ] Jessica A . Pater , Moon K . Kim , Elizabeth D . Mynatt , and Casey Fiesler . 2016 . Characterizations of Online Harassment : Comparing Policies Across Social Media Platforms . In Proceedings of the 19th International Conference on Supporting Group Work ( Sanibel Island , Florida , USA ) ( GROUP ’16 ) . ACM , New York , NY , USA , 369 – 374 . https : / / doi . org / 10 . 1145 / 11 CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii Jhaver et al . 2957276 . 2957297 [ 44 ] Sarah T Roberts . 2019 . Behind the Screen : Content Moderation in the Shadows of Social Media . Yale University Press . [ 45 ] Donald B Rubin . 2005 . Causal inference using potential outcomes : Design , modeling , decisions . J . Amer . Statist . Assoc . 100 , 469 ( 2005 ) , 322 – 331 . [ 46 ] Adam Sadilek , Henry Kautz , and Vincent Silenzio . 2012 . Modeling spread of disease from social interactions . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 6 . 322 – 329 . [ 47 ] Koustuv Saha , Eshwar Chandrasekharan , and Munmun De Choudhury . 2019 . Prevalence and psychological effects of hateful speech in online college communities . In Proceedings of the 10th ACM Conference on Web Science . 255 – 264 . [ 48 ] Koustuv Saha , Yozen Liu , Nicholas Vincent , Farhan Asif Chowdhury , Leonardo Neves , Neil Shah , and Maarten W Bos . 2021 . AdverTiming Matters : Examining User Ad Consumption for Effective Ad Allocations on Social Media . In Proc . CHI . [ 49 ] Koustuv Saha , Benjamin Sugar , John Torous , Bruno Abrahao , Emre Kıcıman , and Munmun De Choudhury . 2019 . A social media study on the effects of psychiatric medication use . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 13 . 440 – 451 . [ 50 ] Koustuv Saha , Ingmar Weber , and Munmun De Choudhury . 2018 . A Social Media Based Examination of the Effects of Counseling Recommendations After Student Deaths on College Campuses . In Twelfth International AAAI Conference on Web and Social Media . [ 51 ] santaclaraprinciples . org . 2020 . Santa Clara principles on transparency and accountability in content moderation . Santa Clara Principles ( 2020 ) . [ 52 ] Nathan Schneider , Primavera De Filippi , Seth Frey , Joshua Tan , and Amy Zhang . 2021 . Modular Politics : Toward a Governance Layer for Online Communities . Proc . ACM Hum . - Comput . Interact . CSCW ( Oct . 2021 ) . [ 53 ] Sarita Schoenebeck , Oliver L Haimson , and Lisa Nakamura . 2020 . Drawing from justice theories to support targets of online harassment . New Media & Society ( 2020 ) . https : / / doi . org / 10 . 1177 / 1461444820913122 [ 54 ] Joseph Seering . 2020 . Reconsidering Self - Moderation : The Role of Research in Supporting Community - Based Models for Online Content Moderation . Proc . ACM Hum . - Comput . Interact . 4 , CSCW2 , Article 107 ( oct 2020 ) , 28 pages . https : / / doi . org / 10 . 1145 / 3415178 [ 55 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping Pro and Anti - Social Behavior on Twitch Through Moderation and Example - Setting . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( Portland , Oregon , USA ) ( CSCW ’17 ) . Association for Computing Machinery , New York , NY , USA , 111 – 125 . https : / / doi . org / 10 . 1145 / 2998181 . 2998277 [ 56 ] Joseph Seering , Tony Wang , Jina Yoon , and Geoff Kaufman . 2019 . Moderator engagement and community development in the age of algorithms . New Media & Society ( 2019 ) , 1461444818821316 . [ 57 ] Manya Sleeper , Alessandro Acquisti , Lorrie Faith Cranor , Patrick Gage Kelley , Sean A . Munson , and Norman Sadeh . 2015 . I Would Like To . . . , I Shouldn’t . . . , I Wish I . . . : Exploring Behavior - Change Goals for Social Networking Sites . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing ( Vancouver , BC , Canada ) ( CSCW ’15 ) . Association for Computing Machinery , New York , NY , USA , 1058 – 1069 . https : / / doi . org / 10 . 1145 / 2675133 . 2675193 [ 58 ] Nicolas P Suzor , Sarah Myers West , Andrew Quodling , and Jillian York . 2019 . What Do We Mean When We Talk About Transparency ? Toward Meaningful Transparency in Commercial Content Moderation . International Journal of Communication 13 ( 2019 ) , 18 . [ 59 ] Samuel Hardman Taylor , Dominic DiFranzo , Yoon Hyung Choi , Shruti Sannon , and Natalya N . Bazarova . 2019 . Accountability and Empathy by Design : Encouraging Bystander Intervention to Cyberbullying on Social Media . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 118 ( nov 2019 ) , 26 pages . https : / / doi . org / 10 . 1145 / 3359220 [ 60 ] Amaury Trujillo and Stefano Cresci . 2022 . Make Reddit Great Again : Assessing Community Effects of Moderation Interventions on r / The _ Donald . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 526 ( nov 2022 ) , 28 pages . https : / / doi . org / 10 . 1145 / 3555639 [ 61 ] Tom Tyler , Matt Katsaros , Tracey Meares , and Sudhir Venkatesh . 2021 . Social media governance : can social media companies motivate voluntary rule following behavior among their users ? Journal of experimental criminology 17 ( 2021 ) , 109 – 127 . [ 62 ] Kristen Vaccaro , Christian Sandvig , and Karrie Karahalios . 2020 . " At the End of the Day Facebook Does What It Wants " : How Users Experience Contesting Algorithmic Content Moderation . Proc . ACM Hum . - Comput . Interact . 4 , CSCW2 , Article 167 ( oct 2020 ) , 22 pages . https : / / doi . org / 10 . 1145 / 3415238 [ 63 ] Kristen Vaccaro , Ziang Xiao , Kevin Hamilton , and Karrie Karahalios . 2021 . Contestability For Content Moderation . Proc . ACM Hum . - Comput . Interact . 5 , CSCW2 , Article 318 ( oct 2021 ) , 28 pages . https : / / doi . org / 10 . 1145 / 3476059 [ 64 ] Lindy West . 2017 . I’ve left Twitter . It is unusable for anyone but trolls , robots and dictators | Lindy West | Opinion | The Guardian . https : / / www . theguardian . com / commentisfree / 2017 / jan / 03 / ive - left - twitter - unusable - anyone - but - trolls - robots - dictators - lindy - west ? CMP = share % 7B % 5C _ % 7Dbtn % 7B % 5C _ % 7Dtw 12 Examining the Effects of Witnessing Post - Removal Explanations CHI ’24 , May 11 – 16 , 2024 , Honolulu , Hawaii [ 65 ] Sarah Myers West . 2018 . Censored , suspended , shadowbanned : User interpretations of content moderation on social media platforms . New Media & Society ( 2018 ) . [ 66 ] Richard Ashby Wilson and Molly K Land . 2020 . Hate speech on social media : Content moderation in context . Conn . L . Rev . 52 ( 2020 ) , 1029 . [ 67 ] Sijia Xiao , Shagun Jhaver , and Niloufar Salehi . 2023 . Addressing Interpersonal Harm in Online Gaming Communities : The Opportunities and Challenges for a Restorative Justice Approach . ACM Trans . Comput . - Hum . Interact . ( 2023 ) , 36 pages . [ 68 ] Yunhao Yuan , Koustuv Saha , Barbara Keller , Erkki Tapio Isometsä , and Talayeh Aledavood . 2023 . Mental Health Coping Stories on Social Media : A Causal - Inference Study of Papageno Effect . In Proceedings of the ACM Web Conference 2023 . 2677 – 2685 . [ 69 ] Justine Zhang , Sendhil Mullainathan , and Cristian Danescu - Niculescu - Mizil . 2020 . Quantifying the Causal Effects of Conversational Tendencies . Proc . ACM Hum . - Comput . Interact . 4 , CSCW2 , Article 131 ( oct 2020 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3415202 13