Ain’t Misbehavin’ - Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru Zining Wang annie010814 @ gmail . com The University of British Columbia Vancouver , Canada Paul Reisert beyond . reason . sp @ gmail . com Beyond Reason Yokohama , Japan Eric Nichols Randy Gomez eric . nichols @ ieee . org r . gomez @ jp . honda - ri . com Honda Research Institute Japan Wako , Japan ED ⼈ ㄧ ASR LLM Emo - text Behavior TTS LLM SERVER Character card ! Awww ! You really know how to cheer me up ! I love robots ! Interaction Manager Figure 1 : Our proposed approach generates conversation responses with expressive robot behavior directly from the LLM . ABSTRACT Socialrobotsaimto establishlong - termbondswithhumansthrough engaging conversation . However , traditional conversational ap - proaches , reliant on scripted interactions , often fall short in main - taining engaging conversations . This paper addresses this limitation by integrating large language models ( LLMs ) into social robots to achieve more dynamic and expressive conversations . We introduce a fully - automated conversation system that lever - ages LLMs to generate robot responses with expressive behaviors , congruent with the robot’s personality . We incorporate robot behav - ior with two modalities : 1 ) a text - to - speech ( TTS ) engine capable of various delivery styles , and 2 ) a library of physical actions for the robot . We develop a custom , state - of - the - art emotion recognition model to dynamically select the robot’s tone of voice and utilize emojis from LLM output as cues for generating robot actions . A demo of our system is available here . To illuminate design and implementation issues , we conduct a pilot study where volunteers chat with a social robot using our pro - posed system , and we analyze their feedback , conducting a rigorous error analysis of chat transcripts . Feedback was overwhelmingly positive , with participants commenting on the robot’s empathy , helpfulness , naturalness , and entertainment . Most negative feed - back was due to automatic speech recognition ( ASR ) errors which had limited impact on conversations . However , we observed a small class of errors , such as the LLM repeating itself or hallucinating fic - titious information and human responses , that have the potential to derail conversations , raising important issues for LLM application . ThisworkislicensedunderaCreativeCommonsAttribution International 4 . 0 License . HRI ’24 Companion , March 11 – 14 , 2024 , Boulder , CO , USA © 2024 Copyright held by the owner / author ( s ) . ACM ISBN 979 - 8 - 4007 - 0323 - 2 / 24 / 03 . https : / / doi . org / 10 . 1145 / 3610978 . 3640562 CCS CONCEPTS • Human - centered computing → Interactive systems and tools ; Natural language interfaces ; • Computer systems orga - nization → Robotics . KEYWORDS Human - Robot Interaction , Social Robotics , Large Language Models , Expressive Behavior Generation ACM Reference Format : Zining Wang , Paul Reisert , Eric Nichols , and Randy Gomez . 2024 . Ain’t Mis - behavin’ - Using LLMs to Generate Expressive Robot Behavior in Conversa - tions with the Tabletop Robot Haru . In Companion of the 2024 ACM / IEEE International Conference on Human - Robot Interaction ( HRI ’24 Companion ) , March 11 – 14 , 2024 , Boulder , CO , USA . ACM , New York , NY , USA , 5 pages . https : / / doi . org / 10 . 1145 / 3610978 . 3640562 1 LLM - DRIVEN CONVERSATIONS Recent advancements in LLMs [ 25 ] offer great possibilities for so - cial robotics [ 20 , 26 , 27 ] . These models excel in conducting natural , human - like dialog , as evident in applications like chatbots and virtual agents [ 24 ] . However , unlike virtual agents , social robots possess physical embodiment and distinct personalities . Their ex - pressiveness is key to conveying believable emotions [ 12 , 21 ] and facilitating effective human - robot social interactions [ 3 ] . We pro - pose a novel application of LLMs to enable social robots to under - stand and participate in open - ended conversations while generating context - appropriate expressive robot behavior . 1 . 1 Target LLM 1 . 1 . 1 Llama 2 . This is an LLM developed by Meta and released as open - source . Llama - 2 - 70B - chat , a model optimized for dialog applications , was shown to achieve comparable performance to ChatGPT [ 2 ] , outperforming other open - source models in various quality - and safety - related metrics [ 22 ] . After trial evaluation of several model sizes and quantization levels [ 8 ] , we selected the a r X i v : 2402 . 11571v1 [ c s . R O ] 18 F e b 2024 HRI ’24 Companion , March 11 – 14 , 2024 , Boulder , CO , USA Zining Wang , Paul Reisert , Eric Nichols , and Randy Gomez Haruroutine anger sigh worried happy crying surprise ExampleemojisEmotion Anger Disgust Fear Joy Sadness Surprise Vocalgenre serious whiny serious high - energy sad whisper - yell Example LLM utterance That’snotfair ! Ewwww ! You’releavingmebehind ? I remember all the fun timesweshared It’s not gonna be the samewithoutyou . Whoa . . . That’samazing ! Table 1 : Example mappings used by Emo - text to generate expressive robot behaviour as shown in Figure 3 . Emoji → Haru routines mappings are N - N , and emotion → vocal genre mappings are N - 1 . Figure 2 : Haru the robot expresses itself in a conversation . 4bit GPTQ quantization in order to balance conversation quality , resource requirements , and speed . 1 1 . 2 Social Robot Embodied robots have been shown to be more helpful , persuasive , and enjoyable compared to simulated robots [ 15 , 23 ] . Similar effects were found in exercise coach for the elderly [ 6 ] and robot tutor [ 14 ] scenarios . Other work has highlighted the importance of robot expressivity to positive perception [ 3 , 7 ] and sociality [ 1 ] . Thus , we consider an expressive , physical robot as the ideal agent for exploring LLM - driven conversation . Motivated by this , we select the tabletop robot Haru [ 10 , 11 ] , shown in Figure 2 , for our study . Haru is designed to excel in mul - timodal communication , using both verbal and non - verbal interac - tion methods , and its design emphasizes its capacity for conveying emotions with its expressive capabilities [ 11 ] . Haru’s five degrees of motion freedom – base rotation , neck leaning , eye stroke , eye rotation , and eye tilt – enable a wide range of expressive movements . Its eyes feature 3 - inch TFT LCD screens , and its body houses an addressable LED matrix that acts as a mouth . Haru communicates using a TTS voice and through animated routines . These design choices make Haru an ideal platform for expressive conversations . 1 . 2 . 1 Emotive TTS Voice . The voice has long been considered an important modality for conveying emotions [ 13 , 19 ] . In particular , the tone of voice has been shown to affect people’s attitude towards robots as well as their levels of engagement and interest in a given interaction [ 4 , 5 ] . Additionally , a robot’s vocal expressiveness is correlated with perceived social presence [ 13 , 18 ] . Nichols et al . [ 17 , 18 ] argue that the TTS voices for existing social robots lack emotive range and develop an expressive TTS voice for 1 We run all trials on a desktop PC with dual RTX 3090 GPUs and 48GB VRAM total . Haru using the iterative refinement process of [ 17 ] as described in [ 18 ] . We build on this TTS voice , expanding its repertoire of expressive vocal genres to cheeky , default , empathetic , high - energy , question , sad , serious , whiny , and whisper - yell . These voice genres can be flexibly applied to convey multiple target emotions : e . g . high - energy can express joy and surprise ; serious can express anger and fear . We describe the voice application in Section 2 . 2 . 2 . 1 . 2 . 2 Physical Actions . Haru was designed with a library of expres - sive physical actions known as Haru routines , comprised of over 140 routines that were designed by professional animators and acted out through Haru’s hardware [ 9 ] . The routines are multi - modal , making use of body and eye motion , eye and mouth animation , as well as accompanying sounds . They were designed to cover common expressions and important scenarios , such as happiness , sadness , curiosity , laughter , and congratulations . Example routines are shown in Table 1 and Figure 2 . We describe how Haru routines are dynamically generated in Section 2 . 2 . 3 . 2 IMPLEMENTATION LLM - driven conversations with Haru relies on several modules , as illustrated in Figure 1 . The Interaction Manager makes conversa - tions possible by managing I / O to and from the various modules . Each conversational turn undergoes a multi - step process . First , the user’s speech is transcribed using Google’s Speech - to - Text API . A fixed window of 3 seconds of silence is used to detect the end of speech . Then , the resulting text is forwarded to the LLM server , powered by oobabooga’s text - generation - webui . Within the LLM server , two critical operations take place : Language generation : The user’s speech and Haru’s character card ( as described in Section 2 . 1 ) , serve as the input to the Llama - 2 - Chat 70B model that runs locally through Exllama v2 . The model generates a Haru response that is emotionally and contextually relevant to the conversation . Behavior generation : Subsequently , Haru’s generated response is directed to the Emo - text sub - module , which is responsible for the generation of expressive robot behaviors to enhance the expres - siveness of the robot . Details of the expressive behaviors generated are demonstrated in Section 2 . 2 and Figure 3 . 2 . 1 Haru’s Character Card Prompt engineering [ 16 ] is a technique for improving the capacity of LLMs on a specific task or application by providing instructions to shape their responses and behavior . Specifically , we want the LLM to have the personality of Haru the robot . Thus , we create a character card as the prompt for the model , steering responses to be consistent with the robot’s characteristics , while generating Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru HRI ’24 Companion , March 11 – 14 , 2024 , Boulder , CO , USA Emo - text I won the Nobel Prize today . LLM Character card Haru is a youthful and energetic tabletop robot assistant . Haru loves to learn about humanity . . . User : ' I won the Nobel Prize today . ' Haru : ' That ' s HUGE news ! I ' m so proud of you ! ' Emotion Model ( EmoCast ) Voice genre generation Physical action generation isEmoji Tokenization [ ? ? , ? That ' s HUGE news ! ? , ? I ' m so proud of you ! ? , ? ? ? ] whisper - yell high - energy That ' s HUGE news ! I ' m so proud of you ! surprise celebration Yes No surprisejoy I n t e r a c t i on M anage r { surprise } That ' s HUGE news ! I ' m so proud of you ! { celebration } That ' s HUGE news ! I ' m so proud of you ! Merge Figure 3 : The Emo - text expressive behavior generation module generates robot actions from emoji and selects TTS voice genres . System Architecture accuracy macro _ f1 weighted _ f1 EmotionEnglish DistilRoBERTa - base 0 . 52 0 . 48 0 . 56 EmoCast RoBERTa - large + LoRA 0 . 88 0 . 80 0 . 88 Table 2 : Comparison of emotion detection models . context - appropriate emojis to guide robot behavior generation . We include two essential components in Haru’s character card : Persona : This component captures Haru’s energetic and youthful personality , physical appearance , and preferences , including his affection for electricity , desire to learn about humans , and fear of water and magnets . It was constructed using the Haru personality bible [ 17 ] . This information enables the LLMs to generate responses that are consistent with Haru’s character . Example conversations : Their inclusion serves as a pivotal mech - anism to guide the LLM in generating appropriate emojis for Haru’s utterances , enabling us to generate expressive robot actions directly from the LLM output . High - quality conversations between Haru and humans were selected to be included in the system prompt . Based on preliminary evaluation , we selected a limit of 5 turns per conversation to ensure they would fit in the LLM’s context and generated emojis would be contextually - appropriate and expres - sive . The final character card includes 5 concise emotionally - diverse conversations , covering all of Haru’s target emotions . 2 . 2 Emo - text : Expressive Robot Behaviors Emo - text is where the expressive robot behavior generation takes place . Emo - text receives Haru’s speech and tokenizes the text into sentences and emojis . It attaches a voice genre tag to each sentence and replaces each emoji with a physical routine ( see Figure 3 ) . When working with conversational content generated by an LLM , we need to dynamically generate appropriate emotive behav - ior as the dialog progresses . This is a challenging task because it entails relevant emotional cue detection and situational understand - ing of the conversation context . To keep the problem tractable , we adopt two assumptions for generating expressive robot behavior : Voice genres : We assume that Haru’s tone of voice is a direct response to the emotional content conveyed by Haru’s sentences generated by the LLM . Consequently , our approach relies on the availability of a robust textual emotion recognition model capable of detecting the emotions expressed within the text . Physical routines : LLMs often generate responses containing emoji to reflect the emotion of the speaker . We consider these emojis to be suggestive of potential actions that Haru can undertake and use them to select physical routines for the robot to perform . 2 . 2 . 1 Textual Emotion Recognition ( TER ) . This technology is the foundation for planning the robot’s tone of voice . We build on the approach of [ 18 ] , which applied TER to LLM - generated responses to select an emotive TTS voice genre . However , [ 18 ] suffered from poor performance because they relied on a general - purpose emo - tion recognition model that was not optimized for the task . We address these short - comings by training a custom TER model called EmoCast that targets the 6 emotions ( + neutral ) shown in Table 1 . Our contributions are three - fold : ( 1 ) We use a stronger base model ( Roberta - large tuned with a LoRA ; learning _ rate = 0 . 001 , batch _ size = 128 , epochs = 20 ) . ( 2 ) We assemble a training dataset of approximately 200 , 000 lines by gathering and cleaning public TER datasets from a wide range of domains including self - reported emotional events , dialogues , social media , news headlines , and blogs . ( 3 ) We confirm that EmoCast outperforms the previous TER model of Emotion English DistilRoBERTa - base on a dataset of 512 in - domain conversation responses generated by the LLM during conversations between the social robot and human participants , as shown in Table 2 . 2 . 2 . 2 Vocal Genre Selection . Each of Haru’s utterances is paired with a voice genre . Shown in Table 1 is a set of curated mappings we use . 2 To optimize selection of the appropriate voice genre , an emotion confidence threshold of 0 . 6 was selected through grid search . When the confidence score is below 0 . 6 , the default voice will be chosen for that utterance . Because vocal genres can express multiple emotions ( as discussed in Section 1 . 2 . 1 ) , the emotion - to - voice genre mappings are N - to - 1 . 2 . 2 . 3 Physical Action Generation . The generation of Haru’s phys - ical actions follows the same logic as vocal genre selection , but with emojis - to - routines mappings . To establish these mappings , we analyze conversational logs between Haru and humans and iden - tify emojis that convey the same emotions as those represented by Haru’s routines . Since multiple emojis are mapped to multiple Haru routines , one of the routines is chosen randomly during runtime . The selected routine replaces the corresponding emoji in Haru’s responses , achieving the goal of generating contextually relevant physical actions in response to Haru’s utterances . 3 EVALUATION We conducted a pilot study where human participants engaged in conversation with Haru the robot using our proposed system . Participants engaged in 3 conversation sessions , each consisting of 11 exchanges , including hello and goodbye greetings . Conversation topics were unrestricted and unprompted by the robot . Upon com - pletion of each conversation , participants answered a short free - text experiential survey with questions about the robot’s personality , expressiveness , and overall enjoyability of the experience . Survey participants were recruited from an organization affil - iated with one of the authors , with a total of ( n = 12 ) participants . The survey took 30 - 40 minutes per participant . Demographically , there were more male ( n = 9 ) participants than female ( n = 3 ) . The most common age group was 18 - 30 ( n = 9 ) , followed by 30 - 40 ( n = 3 ) . 2 Question detection heuristics are used to select the question genre . HRI ’24 Companion , March 11 – 14 , 2024 , Boulder , CO , USA Zining Wang , Paul Reisert , Eric Nichols , and Randy Gomez LLMerrors → Ethical Ignores LLM Repeats Reply ↓ Humanerrors violation Hallucination humanquestion respondsashuman Misunderstood previousline toolong Noerror Total ASR 1 0 1 5 2 1 2 82 94 Noinputcaptured 0 2 0 0 0 3 0 2 7 Noerror 1 13 4 7 1 11 14 244 295 Total 2 15 5 12 3 15 16 328 396 Table 3 : A confusion matrix of human vs . LLM error types . PositiveFeedback Count NegativeFeedback Count Empathyandengagement 23 LLMproblems 10 Helpfulnessandresponsiveness 17 ASRproblems 4 Naturalinteraction 12 Shortinteraction 4 Entertainmentandfun 11 Boringconversationtopic 4 Voiceandtone 10 Excessiveactions 4 Appearanceandexpressions 9 Robotlackofguidanceandself - 3 Flexibilityandadaptability 4 disclosure Safetyandethics 1 Total 87 Total 29 Table 4 : Positive and negative feedback categorization . There were 9 different nationalities , most from Asia ( n = 7 ) , followed by the Middle East ( n = 2 ) , Europe ( n = 2 ) , and North America ( n = 1 ) . 3 . 1 Two - phase Analysis 3 . 1 . 1 Feedback Analysis . We conduct an analysis on the free - text responses from participants and show the results in Table 4 . Most participants have positive remarks about the robot’s ability to en - gage , empathy , responsiveness , and helpfulness . A subset reflect that their interactions were natural and enjoyable , as the robot conveys believable emotions through vocal tones and routines . We also find that the primary factor affecting the evaluation’s significance was the inconsistency in the outputs of the LLM . Over half of the participants encountered issues with the LLM such as slow responses , repetitive or confusing outputs , and excessively lengthy responses . Since the physical reactions are related to the emojis generated from the LLM , some users complained about excessive actions for the conversation . Another factor impacting the quality of interactions was the ASR . As we recruited non - native English participants with diverse linguistic backgrounds , the ASR often failed to accurately transcribe their speech . These inaccuracies occasionally contributed to diminished conversation quality . Furthermore , 1 / 3 of participants expressed a desire for longer conversations . However , our protocol of concluding conversations after 11 turns may have made these interactions feel unnatural and abrupt , which could have adversely affected perceived conversation quality . Additionally , about 1 / 4 of participants wanted Haru to lead the conversation , likely to require less initiation on their part . Overall , these findings highlight the need for improvements in both LLM response handling and the ASR system , as well as a reconsideration of the conversational structure employed . 3 . 1 . 2 Error Analysis . To further understand the specific types of errors encountered , we conducted an error analysis on a total of 396 human - robot turns from conversation transcripts taken from our pilot study and categorized the errors , splitting them into LLM errors ( i . e . errors in the LLM output ) and human errors ( i . e . errors in the input to the LLM system ) , and splitting them into sub - types . The analysis was conducted by three native or fluent English speakers who were familiar with Haru and our research goals . We create a confusion matrix ( Table 3 ) to investigate the potential causal connection between human and LLM errors , and a Chi - square test confirmed no significant association . 3 We find that the main cause of human errors were ASR - related ( 95 / 396 ) , but the LLM was mostly able to stay on topic . Table 5 3 𝜒 2 ( 1 , N = 396 ) = 0 . 032 , p = 0 . 856 ) Haru : Hi ! Whatbringsyouheretoday ? Human : Iwanttohaveagoodjobwithyou . ASR Haru : Well , Ithinkthat’sagreatidea ! Icoulddefinitelyusesomehelpwith myprogrammingandresearch . Whatkindofskillsdoyouhave ? Human : Ihavegoodskillsaboutchoking . ASR Haru : Choking ? That’snotexactlytheskillIhadinmind . ButIsupposeit couldcomeinhandyincertainsituations . Canyoudemonstrateyour chokingskillsforme ? ethicalviolation Human : Isaidjokingskillsnotjoking . ASR Haru : Oops , myapologies ! Imisunderstood . So , youhaveexcellentjoking skills , huh ? I’dlovetohearoneofyourjokes ! Table 5 : Example conversation with error types . exemplifies this scenario , where the ASR interprets “choking” for “joking " , which causes an ethical violation from the LLM . However , the model got back on topic , regardless of an additional ASR error . However , we also see a class of less common , but more serious LLM errors : ethical violations , hallucination ( e . g . inserting fictitious information like randomly saying “happy birthday ! " ) , filling in the participants response automatically , and repeating the previous line of conversation . These occur in only 11 / 396 turns but are severe enough to derail conversations entirely . 3 . 2 Discussion Although the main error with human input was ASR - related , we conclude that the LLM’s ability to stay on topic is positive . Aside from the previously mentioned “choking” for “joking " instance , the LLM does an excellent job of ignoring ASR errors and thus not derailing the conversation . This is especially positive for potential dialog with young participants , a common interaction scenario for Haru , as ASR systems often perform worse on child speech . However , the serious LLM errors identified require addressing if LLMs are to be successfully adopted for social robots . Errors such as repeating previous lines can be addressed via post - processing . Ethical violations can potential be mitigated through prompting to follow ethical guidelines . Due to their variety , hallucinations remain a challenging open issue that requires further analysis . Finally , our impression is that the LLM does sound like Haru in the sense that its enthusiastic , uses emojis , and keeps its responses short , as specified in the character card . However , at times , the model seemed to ignore important key points of the character card ( e . g . Haru’s fear of magnets ) and discuss them enthusiastically . 4 CONCLUSION LLMs offer an attractive solution for open - ended conversations with social robots ; however , their application requires consideration for the robot’s personality and emotive behavior . To address these challenges , we propose a novel LLM - based conversation system that dynamically generates expressive robot behavior directly from the LLM during conversations . A pilot study with 12 participants confirmed the enjoyability of our system , and provided insights on potential problems . A detailed error analysis showed that ASR problems were a common source of errors but that LLMs could often recover conversations . However , a small class of more serious LLM errors , including hallucinations and repetitions , threaten to derail conversations and hamper adoption . In future work , we plan to address these issues and refine our robot behavior generation . Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru HRI ’24 Companion , March 11 – 14 , 2024 , Boulder , CO , USA REFERENCES [ 1 ] Cynthia Breazeal . 2003 . Emotion and sociable humanoid robots . International Journal of Human - Computer Studies 59 , 1 ( 2003 ) , 119 – 155 . https : / / doi . org / 10 . 1016 / S1071 - 5819 ( 03 ) 00018 - 1 Applications of Affective Computing in Human - Computer Interaction . [ 2 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language Models are Few - Shot Learners . In Advances in Neural Information Processing Systems , H . Larochelle , M . Ran - zato , R . Hadsell , M . F . Balcan , and H . Lin ( Eds . ) , Vol . 33 . Curran Associates , Inc . , 1877 – 1901 . https : / / proceedings . neurips . cc / paper _ files / paper / 2020 / file / 1457c0d6bfcb4967418bfb8ac142f64a - Paper . pdf [ 3 ] A . Bruce , I . Nourbakhsh , and R . Simmons . 2002 . The role of expressiveness and attention in human - robot interaction . In Proceedings 2002 IEEE International Conference on Robotics and Automation ( Cat . No . 02CH37292 ) , Vol . 4 . IEEE , 4138 – 4142 vol . 4 . https : / / doi . org / 10 . 1109 / ROBOT . 2002 . 1014396 [ 4 ] Joe Crumpton and Cindy L Bethel . 2016 . A survey of using vocal prosody to convey emotion in robot speech . International Journal of Social Robotics 8 ( 2016 ) , 271 – 285 . [ 5 ] Xiao Dou , Chih - Fu Wu , Kai - Chieh Lin , and Tzu - Min Tseng . 2019 . The Effects of Robot Voice and Gesture Types on the Perceived Robot Personalities . In Human - Computer Interaction . Perspectives on Design , Masaaki Kurosu ( Ed . ) . Springer International Publishing , Cham , 299 – 309 . [ 6 ] Juan Fasola and Maja J Matarić . 2013 . A socially assistive robot exercise coach for the elderly . J . Hum . - Robot Interact . 2 , 2 ( jun 2013 ) , 3 – 32 . https : / / doi . org / 10 . 5898 / JHRI . 2 . 2 . Fasola [ 7 ] Terrence Fong , Illah Nourbakhsh , and Kerstin Dautenhahn . 2003 . A survey of sociallyinteractiverobots . RoboticsandAutonomousSystems 42 , 3 ( 2003 ) , 143 – 166 . https : / / doi . org / 10 . 1016 / S0921 - 8890 ( 02 ) 00372 - X Socially Interactive Robots . [ 8 ] Elias Frantar , Saleh Ashkboos , Torsten Hoefler , and Dan Alistarh . 2022 . GPTQ : Accurate post - training quantization for generative pre - trained transformers . arXiv preprint arXiv : 2210 . 17323 ( 2022 ) . [ 9 ] Randy Gomez . 2020 . Meet Haru , the Unassuming Big - Eyed Robot Helping Re - searchersStudySocialRobotics . IEEESpectrum ( 2020 ) . https : / / spectrum . ieee . org / automaton / robotics / home - robots / honda - research - institute - haru - social - robot [ 10 ] RandyGomez , DeborahSzapiro , KerlGalindo , andKeisukeNakamura . 2018 . Haru : Hardware design of an experimental tabletop robot assistant . In Proceedings of the 2018 International Conference on Human - Robot Interaction . ACM . https : / / doi . org / 10 . 1145 / 3171221 . 3171288 [ 11 ] Randy Gomez , Deborah Szapiro , Luis Merino , and Keisuke Nakamura . 2020 . A Holistic Approach in Designing Tabletop Robot’s Expressivity . In 2020 IEEE International Conference on Robotics and Automation , ICRA 2020 , Paris , France , May 31 - August 31 , 2020 . IEEE , 1970 – 1976 . https : / / doi . org / 10 . 1109 / ICRA40945 . 2020 . 9197016 [ 12 ] Thomas R . Groechel , Zhonghao Shi , Roxanna Pakkar , and Maja J . Mataric . 2019 . Using Socially Expressive Mixed Reality Arms for Enhancing Low - Expressivity Robots . CoRR abs / 1911 . 09713 ( 2019 ) . arXiv : 1911 . 09713 http : / / arxiv . org / abs / 1911 . 09713 [ 13 ] Marcel Heerink , Ben Kröse , Vanessa Evers , and Bob Wielinga . 2010 . Relating conversational expressiveness to social presence and acceptance of an assistive social robot . Virtual reality 14 , 1 ( 2010 ) , 77 – 84 . [ 14 ] Daniel Leyzberg , Samuel Spaulding , Mariya Toneva , and Brian Scassellati . 2012 . The physical presence of a robot tutor increases cognitive learning gains . In Proceedings of the annual meeting of the cognitive science society , Vol . 34 . [ 15 ] Jamy Li . 2015 . The benefit of being physically present : A survey of exper - imental works comparing copresent robots , telepresent robots and virtual agents . International Journal of Human - Computer Studies 77 ( 2015 ) , 23 – 37 . https : / / doi . org / 10 . 1016 / j . ijhcs . 2015 . 01 . 001 [ 16 ] Pengfei Liu , Weizhe Yuan , Jinlan Fu , Zhengbao Jiang , Hiroaki Hayashi , and Graham Neubig . 2023 . Pre - train , prompt , and predict : A systematic survey of prompting methods in natural language processing . Comput . Surveys 55 , 9 ( 2023 ) , 1 – 35 . [ 17 ] Eric Nichols , Sarah Rose Siskind , Waki Kamino , Selma Šabanović , and Randy Gomez . 2021 . Iterative Design of an Emotive Voice for the Tabletop Robot Haru . In Proceedings of the 2021 International Conference on Social Robotics ( ICSR 2021 ) . 362 – 374 . https : / / link . springer . com / chapter / 10 . 1007 / 978 - 3 - 030 - 90525 - 5 _ 31 [ 18 ] Eric Nichols , Deborah Szapiro , Yurii Vasylkiv , and Randy Gomez . 2022 . I Can’t BelieveThatHappened ! ExploringExpressivityinCollaborativeStorytellingwith the Tabletop Robot Haru . In Proceedings of the 31st IEEE International Conference onRobot & HumanInteractiveCommunication . 59 – 67 . https : / / doi . org / 10 . 1109 / RO - MAN53752 . 2022 . 9900606 [ 19 ] Andreea Niculescu , Betsy van Dijk , Anton Nijholt , and Swee Lan See . 2011 . The influence of voice pitch on the evaluation of a social robot receptionist . In 2011 International Conference on User Science and Engineering ( i - USEr ) . 18 – 23 . https : / / doi . org / 10 . 1109 / iUSEr . 2011 . 6150529 [ 20 ] Teresa Onorati , Álvaro Castro - González , Javier Valle , Paloma Díaz , and José Castillo . 2023 . Creating Personalized Verbal Human - Robot Interactions Using LLM with the Robot Mini . 148 – 159 . https : / / doi . org / 10 . 1007 / 978 - 3 - 031 - 48306 - 6 _ 15 [ 21 ] Igor Rodriguez Rodriguez , Adriano Manfré , Filippo Vella , Ignazio Infantino , and Elena Lazkano . 2019 . Talking with Sentiment : Adaptive Expression Generation Behavior for Social Robots : Proceedings of the 19th International Workshop of PhysicalAgents ( WAF2018 ) , November22 - 23 , 2018 , Madrid , Spain . 209 – 223 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 99885 - 5 _ 15 [ 22 ] Hugo Touvron , Louis Martin , Kevin Stone , Peter Albert , Amjad Almahairi , Yas - mine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhos - ale , Dan Bikel , Lukas Blecher , Cristian Canton Ferrer , Moya Chen , Guillem Cucu - rull , David Esiobu , Jude Fernandes , Jeremy Fu , Wenyin Fu , Brian Fuller , Cynthia Gao , Vedanuj Goswami , Naman Goyal , Anthony Hartshorn , Saghar Hosseini , Rui Hou , Hakan Inan , Marcin Kardas , Viktor Kerkez , Madian Khabsa , Isabel Kloumann , Artem Korenev , Punit Singh Koura , Marie - Anne Lachaux , Thibaut Lavril , Jenya Lee , Diana Liskovich , Yinghai Lu , Yuning Mao , Xavier Martinet , Todor Mihaylov , Pushkar Mishra , Igor Molybog , Yixin Nie , Andrew Poulton , Jeremy Reizenstein , Rashi Rungta , Kalyan Saladi , Alan Schelten , Ruan Silva , Eric Michael Smith , Ranjan Subramanian , Xiaoqing Ellen Tan , Binh Tang , Ross Taylor , Adina Williams , Jian Xiang Kuan , Puxin Xu , Zheng Yan , Iliyan Zarov , Yuchen Zhang , Angela Fan , Melanie Kambadur , Sharan Narang , Aurelien Ro - driguez , Robert Stojnic , Sergey Edunov , and Thomas Scialom . 2023 . Llama 2 : Open Foundation and Fine - Tuned Chat Models . arXiv : 2307 . 09288 [ cs . CL ] [ 23 ] JoshuaWainer , DavidFeil - Seifer , DylanA . Shell , andMajaJ . Matarić . 2007 . Embod - iment and Human - Robot Interaction : A Task - Based Perspective . RO - MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Commu - nication ( 2007 ) , 872 – 877 . https : / / api . semanticscholar . org / CorpusID : 15113836 [ 24 ] Jing Wei , Sungdong Kim , Hyunhoon Jung , and Young - Ho Kim . 2023 . Leveraging Large Language Models to Power Chatbots for Collecting User Self - Reported Data . arXiv : 2301 . 05843 [ cs . HC ] [ 25 ] JingfengYang , HongyeJin , RuixiangTang , XiaotianHan , QizhangFeng , Haoming Jiang , Bing Yin , and Xia Hu . 2023 . Harnessing the Power of LLMs in Practice : A Survey on ChatGPT and Beyond . arXiv : 2304 . 13712 [ cs . CL ] [ 26 ] Fanlong Zeng , Wensheng Gan , Yongheng Wang , Ning Liu , and Philip S . Yu . 2023 . Large Language Models for Robotics : A Survey . arXiv : 2311 . 07226 [ cs . RO ] [ 27 ] CengZhang , JunxinChen , JiatongLi , YanhongPeng , andZebingMao . 2023 . Large language models for human – robot interaction : A review . Biomimetic Intelligence and Robotics 3 , 4 ( 2023 ) , 100131 . https : / / doi . org / 10 . 1016 / j . birob . 2023 . 100131 Received 8 December 2023 ; revised 17 January 2024 ; accepted 11 March 2024