Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 , pp . 607 – 621 Boosting intelligence analysts’ judgment accuracy : What works , what fails ? David R . Mandel ∗ Christopher W . Karvetski † Mandeep K . Dhami ‡ Abstract A routine part of intelligence analysis is judging the probability of alternative hypotheses given available evidence . Intelli - gence organizations advise analysts to use intelligence - tradecraft methods such as Analysis of Competing Hypotheses ( ACH ) to improve judgment , but such methods have not been rigorously tested . We compared the evidence evaluation and judgment accuracy of a group of intelligence analysts who were recently trained in ACH and then used it on a probability judgment task to another group of analysts from the same cohort that were neither trained in ACH nor asked to use any speciﬁc method . Although the ACH group assessed information usefulness better than the control group , the control group was a little more accurate ( and coherent ) than the ACH group . Both groups , however , exhibited suboptimal judgment and were susceptible to unpacking eﬀects . Although ACH failed to improve accuracy , we found that recalibration and aggregation methods substan - tially improved accuracy . Speciﬁcally , mean absolute error ( MAE ) in analysts’ probability judgments decreased by 61 % after ﬁrst coherentizing their judgments ( a process that ensures judgments respect the unitarity axiom ) and then aggregating their judgments . The ﬁndings cast doubt on the eﬃcacy of ACH , and show the promise of statistical methods for boosting judgment quality in intelligence and other organizations that routinely produce expert judgments . Keywords : probability judgment , accuracy , coherence , intelligence analysis , recalibration , aggregation , coherentization 1 Introduction Intelligence organizations routinely call upon their analysts to make probability judgments and test hypotheses under conditions of uncertainty . These expert judgments can in - form important policy decisions concerning national and international security . Traditionally , analysts have been ex - pected to accumulate domain expertise and apply this along with critical thinking skills to arrive at timely and accurate assessments for decision - makers . In the US , developers of analytic tradecraft ( i . e . , the methods developed within the in - telligence community to support its analytic functions ) such as Richards Heuer Jr . and Jack Davis introduced so - called “structured analytic techniques” ( SATs ) to support the ana - lyst in the assessment process , but these methods were largely optional tricks - of - the - trade . That state of aﬀairs changed fol - lowing two notable geopolitical events ( i . e . , the September This research was supported by Department of National Defence project # 05da , Canadian Safety and Security Program projects # 2016 – TI – 2224and # 2018 – TI – 2394 , and HM Government . This work contributes to the NATO SystemAnalysisandStudiesPanelResearchTaskGrouponAssessmentandCommunicationofUncertaintyinIntelligencetoSupportDecisionMaking ( SAS - 114 ) . We thank Jon Baron , Jonathan Nelson , and two anonymous reviewers for helpful feedback on this research . Copyright : © 2018 . The authors license this article under the terms of the Creative Commons Attribution 3 . 0 License . ∗ Intelligence , Inﬂuence and Collaboration Section , Toronto Re - search Centre , Defence Research and Development Canada . Email : drmandel66 @ gmail . com . † BlackSwan Technologies Ltd . ‡ Department of Psychology , Middlesex University . 11 , 2001 , terrorist attacks by Al Qaeda and the 2003 invasion of Iraq ) that were attributed in part to striking intelligence failures . These events prompted reviews of the intelligence community with ensuing organizational reforms that , among other things , aimed at debiasing intelligence analysts’ judg - ments ( Belton & Dhami , in press ) . In the US , the Intelligence Reform and Terrorism Prevention Act of 2004 mandated the use of SATs in intelligence production and SATs became a staple topic in most analytic training programs ( Chang , Ber - dini , Mandel & Tetlock , 2018 ; Coulthart , 2017 ; Marchio , 2014 ) . Much the same set of organizational reforms was en - acted in other Western countries such as the UK ( e . g . , Butler , 2004 ) . Although the number of SATs has skyrocketed over the last decade ( Dhami , Belton & Careless , 2016 ; Heuer & Pherson , 2014 ) , as others have lamented in recent years ( Chang et al . , 2018 ; Dhami , Mandel , Mellers & Tetlock , 2015 ; National Research Council , 2011 ; Pool , 2010 ) , there has been little eﬀort to test their eﬀectiveness . Instead , most SATs have been adopted on the basis of their perceived face validity with the belief that , although imperfect , they must be better than nothing . At the same time , the intelligence commu - nity has rarely considered using post - analytic techniques to improve judgment ( Mandel & Tetlock , 2018 ) . For instance , Mandel and Barnes ( 2014 ) showed that intelligence analysts’ strategic forecasts were underconﬁdent , but that much of this bias could be eliminated by recalibrating their judgments to make them more extreme ( also see Baron , Mellers , Tetlock , Stone & Ungar , 2014 ; Turner , Steyvers , Merkle , Budescu 607 Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 608 & Wallsten , 2014 ) . Similarly , the accuracy of probabil - ity judgments can be improved post - judgment by recalibra - tion so judgments respect one or more coherence principles , such as the axioms of probability calculus — a statistical process called coherentization ( Karvetski , Olson , Mandel & Twardy , 2013 ) . Karvetski et al . further observed that weight - ing individuals’ contributions to aggregated judgments also improved accuracy above the gains achieved using an un - weighted arithmetic average . In the present research , we examine the accuracy of intelligence analysts’ probability judgments in an experimental task . We examine the eﬀec - tiveness of ACH as well as recalibration and aggregation methods with the aim of addressing the prescriptive ques - tion : what works — and what fails — to improve judgment accuracy ? 1 . 1 The Analysis of Competing Hypotheses Technique The Analysis of Competing Hypotheses ( ACH ; Heuer , 1999 ; Heuer & Pherson , 2014 ) is one of the most widely known SATs , and one of only nine that is listed in the US Govern - ment’s ( 2009 ) Tradecraft Primer ( also see UK Ministry of Defence , 2013 ) . The US Government describes ACH as a diagnostic technique whose main function is to externalize analytic hypotheses and evidence . It further claims that ACH helps analysts overcome common cognitive biases , such as primacy eﬀects , conﬁrmation bias , and other forms of pre - mature cognitive closure that can undermine the accuracy of forecasts or other probabilistic assessments . The US Gov - ernment also asserts that ACH “has proved to be a highly eﬀective technique when there is a large amount of data to absorb and evaluate” ( 2009 , p . 14 ) , yet it does not cite any evidence to support that claim . The UK handbook conveys comparable exuberance for ACH , noting , “The approach is designed to help analysts consider all the evidence in the light of all the hypotheses as objectively as possible ” ( UK Ministry of Defence , 2013 , p . 14 , italics added ) . ACH includes several steps , but the core of the trade - craft method involves generating a matrix in which mutually exclusive and ( preferably ) collectively exhaustive ( MECE ) hypotheses are listed in columns and pieces of relevant ev - idence are listed in rows . The analyst then assesses the consistency of each piece of evidence with each hypothesis starting on the ﬁrst row and moving across the columns . For each cell , the analyst rates evidence - hypothesis consistency on a 5 - point ordinal scale ( i . e . , − 2 = highly inconsistent , − 1 = inconsistent , 0 = neutral or not applicable , 1 = consistent , 2 = highly consistent . However , only the negative scores ( − 1 and − 2 ) are tallied for each hypothesis . For instance , if there were ﬁve pieces of information ( i . e . , ﬁve rows ) and Hypothesis A had ratings { 2 , 2 , 2 , 2 , − 2 } and Hypothesis B had ratings { 0 , 0 , 1 , 0 , − 1 } , Hypothesis B with an inconsis - tency score of − 1 would be rated as more likely to be true than Hypothesis A with a score of − 2 . In other words , ACH requires that analysts disregard evidential support for hy - potheses in the information integration process . This feature of the method may have been motivated by a misapplication of Popper’s ( 1959 ) ideas about the merits of falsiﬁcation as a strategy for scientiﬁc discovery . Popper’s claim that hy - potheses could only be falsiﬁed but never proven pertained to universal hypotheses such as “all swans are white” because a single non - white swan is suﬃcient to disprove the claim . Most hypotheses of interest in intelligence , however , are not universal but rather deal with events in a particular context ( e . g . , Iran is developing a nuclear weapon ) , and few could be falsiﬁed outright by a single disconﬁrming piece of evidence ( Mandel , in press ) . ACH also includes a subsequent evidential editing phase : once the matrix is populated with consistency ratings , the an - alyst is encouraged to remove evidence that does not appear to diﬀerentiate between the alternative hypotheses . However , there is virtually no guidance on how such assessments of in - formation usefulness should be conducted . For instance , the US Government merely instructs , “The ‘diagnostic value’ of the evidence will emerge as analysts determine whether a piece of evidence is found to be consistent with only one hypothesis , or could support more than one or indeed all hypotheses . In the latter case , the evidence can be judged as unimportant to determining which hypothesis is more likely correct” ( 2009 , p . 15 ) . The UK handbook is more precise , stating “For each hypothesis ask the following question : ‘If this hypothesis were true , how likely would the evidence be ? ’” ( UK Ministry of Defence , 2013 , p . 15 ; see also Heuer , 1999 ) . Yet , it vaguely advises analysts to “pay most attention to the most diagnostic evidence — i . e . , that which is highly consistent with some hypotheses and inconsistent with oth - ers” ( p . 17 ) . If evidence is subsequently disregarded , then analysts are expected to recalculate the sum of the negative ( inconsistency ) ratings . These scores are then meant to re - ﬂect the rank ordering of hypotheses by subjective probabil - ity , with the hypothesis receiving the smallest inconsistency score being judged as most likely to be true in the set of hypotheses being tested . ACH is not a normative method for probabilistic belief re - vision or hypothesis testing , but it has become an institution - alized heuristic that intelligence organizations have deemed to be eﬀective without compelling reasons or evidence ( for additional critiques , see Chang et al . , 2018 ; Jones , 2018 ; Karvetski , Olson , Gantz & Cross , 2013 ; Pope & Jøsang , 2005 ; Mandel , in press ; Mandel & Tetlock , 2018 ) . As al - ready noted , ACH disregards useful information about ev - idential support for hypotheses and it requires analysts to self - assess information utility without providing a clear def - inition of utility , let alone a computational method for esti - mating such utility . Perhaps even more fundamental is the omission of a clear deﬁnition of consistency , which could signify a range of meanings , such as the probability of the Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 609 evidence given the hypothesis , the probability of the hy - pothesis given the evidence , the plausibility or the necessity of one given the other , or simply a subjective sense of the representativeness of one to the other — namely , the rep - resentativeness heuristic ( Kahneman & Tversky , 1972 ) . In addition , ACH does nothing to ensure that analysts consider prior probabilities or objective base rates when revising their beliefs about hypotheses in light of new evidence . In sum , there are many reasons to be skeptical about the eﬀectiveness of ACH . Unfortunately , there is little scientiﬁc research on ACH , and what exists must be interpreted cautiously for several reasons , such as small sample sizes ( e . g . , Convertino , Bill - man , Pirolli , Massar & Shrager , 2008 ; Lehner , Adelman , Cheikes & Brown , 2008 ; Kretz , Simpson & Graham , 2012 ) , lack of control groups ( Convertino et al . , 2008 ) or appro - priate control groups ( Kretz et al . , 2012 ) . Moreover , virtu - ally all published studies have omitted critical , quantitative measures of judgment accuracy , focusing instead on distal considerations such as whether ACH reduces ( the highly equivocal notion of ) “conﬁrmation bias” ( Nickerson , 1998 ) . Yet , despite the many serious limitations of research on ACH ( and SATs , more generally ) , the intelligence studies litera - ture has shown little concern regarding the lack of adequate research to support the widespread use of SATs , including ACH . Rather , a recent review article concluded that ACH was “found to be eﬀective and had a highly credible evi - dence base . . . ” ( Coulthart , 2017 , p . 377 ) . This conclusion is unwarranted not only because of the methodological weak - nesses noted earlier , but also because the extant ﬁndings are at best equivocal . For instance , whereas Lehner et al . ( 2008 ) ﬁnd that ACH reduced conﬁrmation bias in non - analysts , it had no eﬀect on analysts . 1 . 2 The present research A central aim of our research was to examine how the ac - curacy and logical coherence of intelligence analysts’ judg - ments about the probability of alternative ( MECE ) hypothe - ses depended on whether or not analysts were trained in and used ACH on the experimental task . In addition to this SAT , we also explored the value of statistical post - judgment meth - ods for improving expert judgment , such as recalibrating experts’ probabilities in ways that remedy certain coherence violations ( i . e . , non - unitarity and / or non - additivity ) , and by aggregating experts’ judgments using varying group sizes and weighting methods . We tested the eﬀectiveness of ACH by randomly assigning intelligence analysts from the same population to experimen - tal conditions that either used ACH or did not . One group of analysts was recently trained to use ACH as part of their organization’s training and they were required to use ACH on the experimental task . The other group of analysts was drawn from the same analytic cohort ( i . e . , same organiza - tion and taking the same training course ) but they were not instructed to use ACH ( or any SAT for that matter ) and were not exposed to ACH training until after the experiment was completed . The task , which involved a hypothetical scenario , required analysts to assess the probabilities of four MECE hypotheses that corresponded to four tribes in a region of interest . Participants were asked to assess the probabilities that a detained individual ( i . e . , the target ) from the local population belongs to each of the four tribes . Participants were given the tribe base - rates and diagnostic conditional probabilities for 12 evidential cues ( e . g . , “speaks Zimban” ) , along with the cue values ( 6 present and 6 absent ) for the target . Furthermore , two tribes ( Bango and Dengo , hereafter B and D ) were grouped as friendly ( F ) , whereas the other two ( Acanda and Conda , hereafter A and C ) were grouped as hostile ( H ) . If ACH proponents’ claims about the technique’s eﬀec - tiveness are warranted , we should ﬁnd greater probabilistic judgment accuracy in the ACH condition than in the con - trol condition . As noted earlier , to the best of our knowl - edge , there is no clear evidence to support the claim that ACH improves probabilistic judgment accuracy . Indeed , one non - peer - reviewed study that compared various degrees of ACH support ( e . g . , ACH on its own or with additional training ) across experimental groups found that accuracy was best among those participants in the no - ACH control group ( Wheaton , 2014 ) . However , insuﬃcient information was provided to interpret these results with any conﬁdence . In addition , if proponents’ claims about the eﬀectiveness of ACH in promoting soundness of judgment are true , we might expect to ﬁnd that analysts recently trained in and aided by ACH produce probability judgments that are more coherent than those unaided by ACH . We tested this propo - sition by examining the degree to which probability judg - ments in both groups respect the axioms of unitarity and additivity . To do so , we drew on predictions of support theory , a non - extensional descriptive account of subjective probability which posits that one’s probability judgments are a function of his or her assessments of evidential support for a focal hypothesis and its alternative ( Rottenstreich & Tversky , 1997 ; Tversky & Koehler , 1994 ) . Support theory predicts an unpacking eﬀect , in which the sum of the prob - abilities assigned to a MECE partition with more than two subsets of an event , x , exceeds P ( x ) . Unpacking eﬀects have been shown in several studies ( Ayton , 1997 ; Fox , Rogers & Tversky , 1996 ; Rottenstreich & Tversky , 1997 ; Tversky & Koehler , 1994 ) . For instance , in two experiments with undergraduate participants , Mandel ( 2005 ) found that the mean unpacking factor — namely , the ratio of the sum of unpacked probability estimates to the packed estimate — was 2 . 4 comparing forecasts of terrorism ( i . e . , the packed fore - cast ) to forecasts of terrorism unpacked into acts committed by Al Qaeda or by operatives unaﬃliated with Al Qaeda . No research has yet examined whether intelligence analysts’ Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 610 probability judgments are susceptible to the unpacking ef - fect . In the present research , the unpacking eﬀect would be observed if P ( A ) + P ( C ) > P ( H ) and / or P ( B ) + P ( D ) > P ( F ) . According to the additivity axiom , these inequalities should be equalities , given that A ∩ C = œ and A ∪ C ≡ H ; likewise B ∩ D = œ and B ∪ D ≡ F . Extending our investigation into the coherence of analysts’ probability judgments , we further tested whether analysts’ judgments respect the unitarity axiom , which states that the probabilities assigned to a MECE set of hypotheses should sum to unity . Support theory predicts that partitions of a sample space into more than two subsets will yield an un - packing eﬀect . Thus , in the present research , support theory predicts P ( A ) + P ( B ) + P ( C ) + P ( D ) > 1 . 0 , in violation of the unitarity axiom , which requires that these probabilities sum to unity . The unitarity axiom also requires that the binary complements P ( H ) and P ( F ) sum to 1 . 0 , although support theory predicts agreement with the axiom in the case of bi - nary complements . Some studies ﬁnd agreement with sup - port theory’s prediction for binary complements ( e . g . , Dhami & Mandel , 2013 ; Rottenstreich & Tversky , 1997 ; Wallsten , Budescu & Zwick , 1993 ) , whereas others ﬁnd that the sum of the probabilities people assign to binary complements is less than unity ( e . g . , Baratgin & Noveck , 2000 ; Macchi , Osher - son & Krantz , 1999 ; Mandel , 2008 ; Sloman , Rottenstreich , Wisniewski , Hadjichristidis & Fox , 2004 ) . Consistent with the latter studies , Mandel ( 2015b ) found that intelligence an - alysts who were given a series of binary classiﬁcation tasks to complete provided total probabilities for binary comple - ments that fell signiﬁcantly short of unity , although analysts’ performance was improved through training in Bayesian rea - soning using natural sampling trees . In the present research , we tested whether ACH would have a beneﬁcial eﬀect on the degree to which analysts’ posterior probability judgments respected the unitarity axiom . Our investigation into the coherence of analysts’ probabil - ity judgments was also motivated by the aim of testing the value of statistical , post - judgment methods for improving judgment accuracy . As noted earlier , recent research shows that coherentizing probability judgments so that they respect axioms of probability calculus such as additivity and unitar - ity can signiﬁcantly improve judgment accuracy ( Karvetski , Olson , Mandel et al . , 2013 ) . Moreover , individual diﬀer - ences in the coherence of individuals’ judgments can be exploited as a basis for performance weighting contributions to aggregated estimates , making the “crowds wiser” than they would tend to be if each member’s contribution had equal weight ( Osherson & Vardi , 2006 ; Predd , Osherson , Kulkarni & Poor , 2008 ; Tsai & Kirlik , 2012 ; Wang , Kulka - rni , Poor & Osherson , 2011 ) . Karvetski , Olson , Mandel et al . ( 2013 ) found that the accuracy of probability judgments about the truth of answers to general knowledge questions was improved through coherentizing the judgments , and a further substantial improvement was achieved by coherence weighting the coherentized judgments . In the present re - search , we examined how eﬀective coherentization and co - herence weighting are for improving the accuracy of in - telligence analysts’ probability judgments . We compared coherentized judgments to raw probability judgments gen - erated with or without the use of ACH . We also compared coherence - weighted aggregate estimates to an equal - weight linear opinion pool ( LINOP ) , which is the arithmetic aver - age across judges ( Clemen & Winkler , 1999 ) . Our interest in this issue was two - fold : First , we aimed to assess the external validity of earlier ﬁndings in this nascent area of research on coherentization and coherence - weighted aggregation . Sec - ond , we aimed to test whether these post - judgment methods hold promise for organizations , such as intelligence agencies , that generate expert judgment as a product or service . A further aim of this research anticipated both a possi - ble beneﬁt and a possible drawback of ACH . We hypothe - sized that ACH will not foster greater accuracy in probability judgment because , as we noted earlier , there are processes in the technique , such as disregarding evidential support in information integration , that are normatively indefensible . However , ACH does require analysts to evaluate each piece of information in relation to each hypothesis on the same criterion ( consistency ) . We hypothesized that this might improve analysts’ abilities to extract the usefulness of the evidence . Accordingly , we asked analysts to rate the infor - mation usefulness of each of the evidential cues presented and we examined how well these ratings correlated , on av - erage , with the probability gain of the cue , a measure of the extent to which knowledge of the cue value is likely to im - prove classiﬁcation accuracy ( Baron , 1981 , cited in Baron , 1985 ; Nelson , 2005 ) . A related aim of ours was to examine whether analysts who display stronger correlations with sampling norms also show better probability judgment accuracy , and whether this “meta - relationship” might diﬀer between ACH and control groups . For instance , ACH proponents might be willing to wager that analysts who use ACH are more likely to reliably encode the information value and to use that information to their advantage by making more accurate judgments . 2 Method 2 . 1 Participants Fifty UK intelligence analysts participated in the experiment during regular working hours and did not receive additional compensation for their participation . All participants were pre - registered for intelligence training and were asked by the trainers to participate in the experiment . Mean age was 27 . 79 years ( SD = 5 . 03 ) and mean length of experience working as an analyst was 14 . 08 months ( SD = 29 . 50 ) . Out of 44 participants who indicated their sex , 25 ( 57 % ) were male . Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 611 T able 1 : Informational features of experimental task . Values represent cue likelihoods . Tribe ( base rate ) Evidential cues Acanda ( . 05 ) Bango ( . 20 ) Conda ( . 30 ) Dengo ( . 45 ) Feature Present in Target Under 40 years . 10 . 10 . 90 . 90 Yes Use social media . 75 . 50 . 25 . 50 Yes Speak Zebin . 50 . 75 . 50 . 25 Yes Employed . 25 . 25 . 10 . 10 Yes Practice religion . 90 . 90 . 10 . 10 No From large family . 25 . 50 . 75 . 50 No Educated to age 16 . 50 . 25 . 50 . 75 No Have high - SES . 75 . 75 . 90 . 90 No Speak Zimban . 75 . 25 . 75 . 25 Yes Have political aﬃliation . 75 . 25 . 75 . 25 No Wear traditional clothing . 75 . 50 . 60 . 40 Yes Fair coloured skin . 25 . 50 . 40 . 60 No 2 . 2 Design and procedure Participants were randomly assigned in balanced numbers to one of two conditions of the tradecraft factor : the ACH ( i . e . , tradecraft ) condition or the no - ACH ( i . e . , no tradecraft ) control condition . In the ACH condition , participants com - pleted their scheduled ACH training , which was based on Heuer and Pherson ( 2014 ) and related material from Pher - son Associates , LLC . Participants in the control condition received ACH training after the experiment . Participants completed a paper and pencil questionnaire and were subse - quently debriefed in small group sessions within the organi - zation in which they worked . However , participants worked individually on the task . Participants in the ACH condi - tion were instructed to approach the judgment task using the eight steps of the ACH method , whereas participants in the control condition were free to use whatever approach they favored . The experiment received ethical approved from the institutional review board of Middlesex University . 2 . 3 Materials Participants read about a ﬁctitious case in which they were required to assess the tribe membership of a randomly se - lected person from a region called Zuma . 1 They read that there were four tribes ( A - D ) that constituted 5 % , 20 % , 30 % , and 45 % of Zuma , respectively . Each tribe was then de - scribed in terms of 12 probabilistic cue attributes . For in - stance , for the Acanda tribe ( i . e . , Tribe A ) the description read : 1 Full instructions for ACH and control conditions are available as sup - plements . Acanda : 10 % of the tribe is under 40 years of age , 75 % use social media , 50 % speak Zebin ( one of two languages spoken in Zuma ) , 25 % are em - ployed , 90 % practice a religion , 25 % come from a large family ( i . e . , more than 4 children ) , 50 % have been educated up to the age of 16 , 75 % have a reasonably high socio - economic status relative to the general population , 75 % speak Zimban ( one of two languages spoken in Zuma ) , 75 % have a political aﬃliation , 75 % wear traditional clothing , and 25 % have fair coloured skin . Next , the target’s cue attributes were described as follows : The target is under 40 years of age , uses social me - dia , speaks Zebin , is employed , does not practice a religion , does not come from a large family , does not have education up to age 16 , does not have a reasonably high socio - economic status , speaks Zimban , is not politically aﬃliated , wears tradi - tional clothing , and does not have fair coloured skin . Thus , the target had positive values for half of the cues and negative values for the other half . Furthermore , analysts were told to assume that the target’s answers were truth - ful ( due to the administration of a truth serum ) in order to ameliorate any possible eﬀects of participants perceiving the information as unreliable or deceptive . Table 1 summarizes the informational features of the task . In the ACH condition , participants were asked to com - plete the eight steps of the ACH method ( see supplementary materials for full instructions ) , which included : ( a ) identify - ing all possible hypotheses , ( b ) listing signiﬁcant evidence Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 612 that is relevant for evaluating the hypotheses , ( c ) creating a matrix with all the hypotheses as columns and all items of relevant information as rows and then rating the consis - tency of each piece of evidence with each hypothesis , ( d ) revising the matrix after omitting non - diagnostic evidence , ( e ) calculating the inconsistency scores by taking the sum of the inconsistent values and using that to draw tentative conclusions about the relative likelihood of the hypotheses , ( f ) analyzing the sensitivity of conclusions to a change in the interpretation of a few critical items of relevant information , ( g ) reporting conclusions , and ( h ) identifying indicators for future observation . By comparison , in the control condition , participants were asked to “consider the relative likelihood of all of the hy - potheses , state which items of information were the most diagnostic , and how compelling a case they make in iden - tifying the most likely hypothesis , and also say why alter - native hypotheses were rejected . ” They were provided with two pages of blank paper on which to respond ( none asked for more paper ) . All participants completed the same ﬁnal page of the ques - tionnaire . The ﬁrst four questions prompted analysts for the probability that the target belonged to each of the four tribes ( A - D ) . Next , they were asked for the probability that the target was friendly and also for the probability that the target was hostile . Probability judgments were made on a 101 - point scale that shows numeric probabilities starting at 0 and continuing at every 5 % increment up to 100 . The instruc - tions noted that 0 % meant “impossible” and 100 % meant “absolutely certain . ” Next participants rated on an unnum - bered 11 - point scale , ranging from not at all to completely , how useful each of the 12 cues was in assessing which the target’s tribe membership . For the purpose of statistical anal - ysis , these ratings were entered as values ranging from 1 to 11 . We examined analysts’ responses to the scale measures of probability and information usefulness . 2 . 4 Coherentization and coherence weighting As described previously , more often than not , individuals produce probability estimates that are incoherent and violate probability axioms , and there is evidence that more coher - ent estimates are associated with more accurate estimates ( Mellers , Baker , Chen , Mandel & Tetlock , 2017 ) . Given a set or vector of elicited probabilities that is incoherent , the coherent approximation principle ( CAP ; Osherson & Vardi , 2006 ; Predd et al . , 2008 ) was proposed to obtain a coher - ent set of probabilities that is minimally diﬀerent in terms of Euclidean distance from the elicited probabilities with the goal of improving accuracy . This “closest” set of co - herent probabilities is found by projecting the incoherent probabilities onto the coherent space of probabilities . An incoherence metric can then be deﬁned as the Euclidean dis - tance from an incoherent set of probabilities to the closest coherent set of probabilities . For example , if an analyst in the present research provided probability judgments of . 2 , . 3 , . 4 , and . 3 for the four MECE hypotheses A - D , respec - tively , these estimates are incoherent because they sum to a value greater than 1 and thus violate the unitarity constraint . Using the CAP and ( if needed ) quadratic programming ( see Karvetski , Olson , Mandel et al . , 2013 ) a coherent set of re - calibrated probabilities can be obtained , which minimizes the Euclidean distance between the point { . 2 , . 3 , . 4 , . 3 } and all quartet vectors with values between 0 and 1 , such that the sum of the four values is 1 . For this example , the probabil - ities of . 15 , . 25 , . 35 , and . 25 represent the closest coherent set , with minimum distance as follows : p ( . 2 − . 15 ) 2 + ( . 3 − . 25 ) 2 + ( . 4 − . 35 ) 2 + ( . 3 − . 25 ) 2 = . 10 . The resulting value , moreover , represents an incoherence metric , expressed , more generally , as IM = v ut K Õ i = 1 ( y i − y ci ) 2 . ( 1 ) In Equation 1 , IM is calculated over the sum of k judg - ments that form a related set , and notably IM is zero when elicited judgments are perfectly coherent . The CAP is not limited to using only the unitarity constraint but can be ap - plied with any set of coherence constraints that can be deﬁned mathematically as an optimization program . As noted earlier , variations in IM across individuals can also be used as a basis for performance - weighted aggrega - tion . With IM j as the incoherence metric for the j th indi - vidual in an aggregate , a weighting function should satisfy general properties . First , it should be strictly decreasing as IM j increases , thus assigning harsher penalties to more inco - herent individuals . Because weights are normalized during the aggregation , only the ratio values of weights are relevant . Thus , the function can be arbitrarily scaled in the [ 0 , 1 ] in - terval , with 1 representing a perfectly coherent judge . In the present research , we use a weighting function similar to that of Wang et al . ( 2011 ) ω j = e − IM j · β . ( 2 ) The weighting function assigns full weight to the j th indi - vidual if IM j = 0 or if β = 0 . In the former case , this is due to the perfect coherence of j ’s raw estimates , while in the latter case the weighting function is nondiscrimina - tory and equivalent to taking the arithmetic average across individuals . Next , we deﬁne the coherence - weighted average of n ( where 2 ≤ n ≤ N ) individuals’ coherentized probability judg - ments of the i th hypothesis as ¯ y cci = Í nj = 1 ω j y cij Í n j = 1 ω j . ( 3 ) Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 613 Again , if β = 0 , we have an equal - weighted ( arithmetic ) average of the coherentized judgments ¯ y ci = 1 n n Õ j = 1 y cij . ( 4 ) Note that the coherence constraints on y cij imply that set of all coherent probabilities is a convex set , and any linear com - bination of elements from a convex set is again an element of the same set . Therefore , the aggregated estimates must also be coherent and do not have to be coherentized again . In the present research , we let β = 5 , and we later show that the results are not sensitive to the exact value chosen . Choosing a suﬃciently large value alleviates the issue with the “ﬁfty - ﬁfty blip” , which results when an individual ex - presses epistemic uncertainty by responding . 5 over multiple judgments ( Bruine de Bruin Fischbeck , Stiber & Fischhoﬀ , 2002 ) . In the present research , if an analyst entered . 5 for each hypothesis , A - D , the values would sum to 2 , and the participant’s IM score would be . 50 . In the weighting func - tion , we have ω ( . 50 ) = . 082 . This participant would be assigned only 8 . 2 % of the weight that would be assigned to a perfectly coherent participant . 2 . 5 Metrics The primary measure of accuracy we use is mean absolute error ( MAE ) , which in this research computes the mean ab - solute diﬀerence between a human - originated judgment ( i . e . , raw , transformed , or aggregated ) , y i , and the corresponding posterior probabilities derived from Bayes theorem assum - ing class conditional independence ( i . e . , a “naïve Bayes” model ) , x i . We acknowledge that this simplifying assump - tion is not necessitated by the task . However , we believe it is reasonable to assume that participants did not perceive con - ditional dependence and subsequently take it into account — at least we found no evidence to support such a conclusion in participants’ responses . Using the naïve Bayes model , x A = . 08 , x B = . 15 , x C 4 = . 46 , and x D = . 31 . Accordingly , MAE = 1 kn k Õ i = 1 n Õ j = 1 | y ij − x i | . ( 5 ) The summation over i refers to the set of hypotheses ( i . e . , in this research , k = 4 ) . An advantage of MAE over mean squared error or root mean squared error is that it is less susceptible to outliers ( Armstrong , 2001 ; Willmott & Matsuura , 2005 ) . In addition , MAE is decomposable into quantity disagreement ( QD ) and allocation disagreement ( AD ) : QD = | ME | , where MD = 1 kn k Õ i = 1 n Õ j = 1 ( y ij − x i ) . ( 6 ) AD = MAE = QD . ( 7 ) QD is the absolute value of mean error ( ME ) or bias . AD represents remaining inaccuracy after removal of QD ( i . e . , absolute bias ) , which necessarily involves a fair balance be - tween under - and over - estimations of correct values ( i . e . , any imbalance is part of QD ) . Coherentization reduces MAE by eliminating QD . As noted earlier , we used a measure of classiﬁcation ac - curacy improvement called probability gain ( Nelson , 2005 ) to assess analysts’ accuracy in rating cue usefulness : probability gain ( Q ) = Õ q j P ( q j ) max P ( h i | q j )  . ( 8 ) 3 Results 3 . 1 Coherence of probability judgments We tested the coherence of analysts’ probability judgments as a function of tradecraft using the following logical con - straints : y A + y B + y C + y D = 1 unitary , quarternary partition . ( 9 ) y H + y F = 1 unitary , binary partition . ( 10 ) y H = y A + y C , y F = y B + y D additivity , two binary partitions . ( 11 ) Equations 9 and 10 reﬂect the unitarity axiom and Equation 11 reﬂects the additivity axiom . 2 In violation of Equation 9 and showing a strong unpacking eﬀect , the sum of the prob - abilities assigned to the four MECE hypotheses signiﬁcantly exceeded unity in the control condition ( M = 1 . 54 [ 1 . 33 , 1 . 76 ] , t [ 24 ] = 13 . 63 , d = 0 . 96 , p < 001 ) and in the ACH condition ( M = 1 . 77 [ 1 . 56 , 1 . 97 ] , t [ 24 ] = 19 . 53 , d = 1 . 69 , p < . 001 ) . The unpacking eﬀect did not signiﬁcantly diﬀer between conditions , but the diﬀerence in the size of these eﬀects was nevertheless of medium eﬀect size by Cohen’s ( 1992 ) standards and favored the control group , ∆ = 0 . 22 [ − 0 . 08 , 0 . 55 ] , t [ 45 . 8 ) ] = 1 . 55 , d = 0 . 45 , p = . 13 . In contrast , but consistent with several studies also ﬁnding unitarity for binary complements ( e . g . , Brenner & Rotten - streich , 1999 ; Dhami & Mandel , 2013 ; Mandel , 2005 ; Tver - sky & Koehler , 1994 ) , the total probability assigned to the binary complements , H and F , did not signiﬁcantly diﬀer from unity in either the control condition ( M = 0 . 98 [ 0 . 90 , 1 . 04 ] , t [ 24 ] = 27 . 38 , d = 0 . 12 , p < . 001 ) or the ACH con - dition ( M = 0 . 95 [ 0 . 83 , 1 . 00 ] , t [ 24 ] = 23 . 45 , d = 0 . 25 , 2 Square brackets show bootstrapped 95 % conﬁdence intervals from 1 , 000 bias - corrected and accelerated samples , ∆ denotes the mean diﬀer - ence between conditions , and d refers to the eﬀect size estimator , Cohen’s d . Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 614 p < . 001 ) . Thus , on average , analysts respected the unitarity constraint imposed by Equation 10 . Turning to tests of additivity , we computed the sum of the ( signed ) non - additivity ( SSN ) : SSN = ( y A + y C − y H ) + ( y B + y D − y F ) . ( 12 ) If Equation 12 is respected , SSN = 0 . However , it is evident that implicit disjunctions were assigned signiﬁcantly less probability than what was assigned , in sum , to their con - stituents in both the ACH condition ( M = 0 . 82 [ 0 . 64 , 0 . 99 ] , t [ 24 ] = 8 . 98 , d = 1 . 80 , p < . 001 ) and the control condition ( M = 0 . 56 [ 0 . 37 , 0 . 74 ] , t [ 24 ] = 5 . 34 , d = 1 . 07 , p < . 001 ) . In addition , mean additivity violation , consistent with the unpacking eﬀect , was marginally greater in the ACH condi - tion than in the control condition , ∆ = 0 . 25 [ − 0 . 05 , 0 . 57 ] , t ( 48 ) = 1 . 81 , d = 0 . 52 , p = . 08 . Once again , this diﬀerence was of medium eﬀect size . 3 . 2 Accuracy of probability judgments As noted earlier , we compared the accuracy of analysts’ un - transformed ( i . e . , not coherentized ) probability judgments for the four - way MECE partition ( i . e . , Tribes A - D ) using analysts’ MAE calculated over the four estimates . Although there was a signiﬁcant degree of inaccuracy in both the con - trol condition ( MAE = 0 . 21 [ 0 . 17 , 0 . 26 ] , t [ 24 ] = 9 . 69 , d = 1 . 94 , p < . 001 ) and the ACH condition ( MAE = 0 . 26 [ 0 . 22 , 0 . 29 ] , t [ 24 ] = 14 . 39 , d = 2 . 88 , p < . 001 ) , the eﬀect of tradecraft was not signiﬁcant , ∆ = 0 . 04 [ − 0 . 02 , 0 . 11 ] , t ( 45 . 9 ) = 1 . 49 , d = 0 . 43 , p = . 14 . Nevertheless , as the eﬀect - size estimate reveals , there was a medium - sized eﬀect of tradecraft that , once again , favored the control group . The observed MAE in the sample was also compared to that obtained from 10 , 000 random draws of probability val - ues for each of the four hypotheses , A - D ( i . e . , where each probability was drawn from a uniform distribution over the [ 0 , 1 ] interval — a simulated dart - throwing chimp , to use Tetlock’s [ 2005 ] metaphor ) . MAE for the random judgments was 0 . 33 . Thus , analysts performed signiﬁcantly better than chance , analysts’ MAE = 0 . 23 [ 0 . 21 , 0 . 26 ] , t ( 49 ) = 6 . 69 , d = 0 . 95 , p < . 001 . Given that the QD decomposition of MAE calculated over the four MECE hypotheses is directly related to unitarity violation and , further , given that we have established that this type of coherence violation is greater in the ACH con - dition than in the control condition , we can verify that the proportion of total inaccuracy ( MAE ) accounted for by QD is greater in the ACH condition than in the control condi - tion . In fact , this was conﬁrmed : The QD / MAE proportion was . 73 [ . 60 , . 86 ] in the ACH condition and . 50 [ . 32 , . 67 ] in the control condition , a signiﬁcant eﬀect of medium size , ∆ = . 23 [ . 04 , . 45 ] , t ( 45 . 7 ) = 2 . 08 , d = 0 . 60 , p = . 04 . Although the preceding analyses do not indicate that ACH helps to improve analysts’ probability judgments , critics might argue that the method is not aimed at minimizing absolute error but rather at improving the rank ordering of alternative hypotheses in terms of their probability of be - ing correct . To address this point , we calculated the rank - order ( Spearman ) correlation between each analyst’s four raw probability judgments of A - D and the probability vec - tor of the naïve Bayes model . The mean correlations in the ACH condition ( M = . 29 [ . 02 , . 55 ] ) and the control condition ( M = . 24 [ − . 08 , . 55 ] did not signiﬁcantly diﬀer , t [ 46 . 9 ] = 0 . 28 , d = 0 . 08 , p = . 78 . Therefore , we ﬁnd no support for the hypothesis that ACH helped analysts to better assess the relative probability of the four hypotheses . 3 . 3 Information usefulness As noted earlier , we hypothesized that the consistency rating process in ACH , which requires analysts to assess each piece of evidence for consistency with each hypothesis , and the subsequent diagnosticity assessment process , which requires analysts to consider information usefulness , might help an - alysts capture variation in information utility . Accordingly , we computed the Pearson correlation between each analyst’s ratings of the information usefulness of the 12 cues and the probability gain values for those cues . Providing support for the preceding hypothesis , the mean correlation in the ACH condition ( M = . 68 [ . 61 , . 75 ] ) was signiﬁcantly greater than the mean value in the control condition ( M = . 17 [ − . 02 , . 35 ] ) , and the eﬀect size was very large , t [ 29 . 5 ] = 5 . 35 , d = 1 . 59 , p < . 001 . Next , we examined whether these correlations were them - selves related to analysts’ MAE scores . Overall , this correla - tion was non - signiﬁcant , r ( 49 ) = − . 14 [ − . 39 , . 15 ] , p = . 53 . However , the observed relationship was strikingly diﬀerent between the two conditions . The correlation was negligible in the ACH condition , r ( 24 ) = − . 10 [ − . 44 , . 28 ] , p = . 63 , but it was signiﬁcant and of medium - to - large eﬀect size in the control condition , r ( 24 ) = − . 42 [ − . 69 , − . 07 ] , p = . 045 . Although analysts using ACH were more likely than analysts in the control condition to track the variation in probability gain with their usefulness ratings , the degree to which the ACH group tracked probability gain had almost no corre - spondence to their accuracy , whereas it did for the control group . 3 . 4 Recalibrating probability judgments The substantial degree of nonadditivity observed in analysts’ probability judgments implies that recalibration procedures that coherentize the judgments will not only ensure coher - ence , they will also beneﬁt accuracy by eliminating the QD component of MAE . Thus , we coherentized analysts’ prob - ability judgments of A - D so that they respected the unitar - Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 615 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 0 10 20 30 40 50 Group Size M ean A b s o l u t e E rr o r analyst , coherentized , coherence−weighted analyst , coherentized , equal−weight random , coherentized , equal−weight −0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 0 10 20 30 40 50 Group Size M e an S pea r m an C o rr e l a t i on C oe ff i c i en t analyst , coherentized , coherence−weighted analyst , coherentized , equal−weight random , coherentized , equal−weight Figure 1 : Accuracy of probability judgments by group size and aggregation method . ity constraint in Equation 9 . 3 The coherentized probability judgments ( MAE = 0 . 15 [ 0 . 13 , 0 . 18 ] ) were signiﬁcantly more accurate than the raw judgments ( MAE = 0 . 23 [ 0 . 21 , 0 . 26 ] ) , ∆ = − 0 . 08 [ − 0 . 11 , − 0 . 06 ] , t [ 49 ] = 6 . 77 , d = 0 . 96 , p < . 001 . This represents a 35 % reduction in MAE and ap - proximately a 1 SD improvement . Recall that the proportion of MAE attributable to QD was signiﬁcantly greater in the ACH condition than in the control condition . This suggests that the eﬀect of coherentizing will be stronger in the ACH condition . In fact , d = 0 . 69 in the control condition and d = 1 . 37 in the ACH condition . Therefore , the SD improvement is roughly twice as large in the ACH condition as it is in the control condition . Moreover , after coherentizing , the eﬀect of tradecraft on accuracy is negligible , ∆ = 0 . 01 [ − 0 . 04 , 0 . 07 ] . We once again compared analysts’ judgment accuracy to the performance of the average dart - throwing chimp . How - ever , this time we coherentized the randomly generated prob - abilities , which yielded MAE = 0 . 21 , a value that was signif - icantly inferior to the observed coherentized MAE of 0 . 15 , t [ 49 ] = 4 . 56 , d = 0 . 64 , p < . 001 . An alternative method of assessing chance is to deﬁne it in terms of all possible permutations of the probabilities actually provided by each participant , rather than as a uniform distribution . Using this deﬁnition , the superiority of the analysts over chance was still apparent but not as large : 0 . 16 for chance , 0 . 13 for the participants ( t ( 49 ) = 2 . 75 , p = . 008 ) , a diﬀerence of about 3 An alternative form of coherentization that used Equations 9 and 11 was also tested but found to be virtually indistinguishable . Thus , we used the simpler form . 0 . 03 rather than 0 . 06 . 4 This analysis suggests that probability judgments were in the right range , but they were conveying very little information about the relative probabilities of the four hypotheses , this reducing the power of the experiment to detect group diﬀerences . 3 . 5 Aggregating probability judgments Coherentization yielded a large improvement in the accu - racy of analysts’ probability judgments . We examined how much further improvement in accuracy might be achieved by aggregating analysts’ probability judgments . To do so , we generated 1 , 000 bootstrap samples of statistical group sizes ranging from 1 ( i . e . , no aggregation ) to 49 in increments of two . We aggregated probability judgments in two ways : us - ing an unweighted arithmetic average of coherentized proba - bility judgments and using a coherence - weighted average of such estimates . 5 We examined the eﬀect of aggregation on MAE as well as on the average Spearman correlation between the aggregated estimates and the vector of values from the naïve Bayes model . As a benchmark , we also examined the eﬀect of these aggregation methods on random responses , where each data point is based on 1 , 000 simulations of prob - ability judgments from a uniform distribution over the [ 0 , 1 ] interval . 4 JonBaronconductedthisanalysisandusednormalization ( i . e . , dividing each stated probability by the sum of the four ) rather than coherentization as the recalibration method . 5 For coherence - weighted aggregation , β = 5 . However , as shown in the supplementary ﬁgure , the eﬀect of coherence weighting was robust across a wide parametric range . Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 616 0 . 4 0 . 5 0 . 6 0 . 7 2 4 6 8 10 Group Size P r obab ili t y o f I m p r o v e m en t i n M AE O v e r n − 1 G r oup S i z e analystrandom Figure 2 : Probability of improvement achieved by increasing group size by one member . Bars show 95 % conﬁdence intervals from 1 , 000 bootstrap samples . The reference line shows the probability of improvement by chance . As Figure 1 shows , these analyses yield several impor - tant ﬁndings . First , they conﬁrm that , when aggregated , analysts’ judgments are substantially more accurate than ag - gregated random judgments . Second , it is evident from the left panel in Figure 1 that aggregation greatly improves ac - curacy in analysts’ judgments , but to a degree comparable to that observed in the randomly generated response data . This suggests that most of the error reduction observed is due to variance reduction from averaging and should not be attributed to an eking out of any crowd wisdom , as clearly there is no wisdom in the random response data . 6 Third , it is equally evident from the right panel in Figure 1 that aggregation over increasingly larger group sizes steadily in - creases the correct rank ordering of probabilities . This eﬀect is clearly not manifested in random response data , where aggregation has no beneﬁt . Fourth , aggregation with coher - ence weighting did not outperform aggregation with equal weighting ; in fact , it slightly underperformed . Finally , the left panel in Figure 1 shows that most error reduction due to aggregation was achieved with small group sizes . Figure 2 clariﬁes that there was a signiﬁcantly greater the proportion of cases where MAE was lower for a group size of two than for single individuals ( i . e . , the probability of improvement ) , and likewise the stepwise increase in group size from two to three signiﬁcantly increased the proportion with lower MAE 6 Note that aggregation of random responses will bring all responses closer to . 25 . In the limit , the MAE of this constant response may be lower than that for a set of responses with excessive variability . scores . However , no additional stepwise increase in group size yielded signiﬁcant improvements . Finally , we assessed the proportional gain in accuracy achieved by recalibration and equal - weight aggregation when n = 50 . As noted earlier , coherentizing the disag - gregated judgments yielded a 35 % reduction in MAE . If we combine coherentization with equal - weight aggregation of the full sample of 50 analysts , we obtain MAE = 0 . 09 , a 61 % reduction in MAE over the value for analysts’ original probability judgments ( i . e . , MAE = . 23 ) . That is , 61 % of the inaccuracy of analysts’ probabilistic assessments of the target’s category membership was eliminated by ﬁrst coher - entizing those assessments and then taking an unweighted average of them prior to scoring . 4 Discussion Although intelligence organizations routinely train and ad - vise analysts to use tradecraft methods , such as ACH , to mitigate cognitive biases and thereby improve the coherence and accuracy of their assessments , there has been a dire lack of research on their eﬀectiveness . The present research con - ducted such a test and found that ACH failed to improve intelligence analysts’ probabilistic judgments about alterna - tive hypotheses . It even had a small detrimental eﬀect on some measures of coherence and accuracy . In such cases , the comparison between conditions yielded a medium eﬀect Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 617 size in favor of the control group . To better understand the advantage of not using ACH in the present research task , it is helpful to convert the eﬀect size into a stochastic superiority or probability of superiority estimate equal to the area under the receiver - operator characteristic curve in signal detection theory ( Grissom & Kim , 2005 ; Ruscio & Mullen , 2012 ; Vargha & Delaney , 2000 ) . The probability of superiority is the probability that a randomly selected member of one condition will outperform a randomly selected member of another condition . For accuracy , for instance , the eﬀect size , d = 0 . 45 , yields a probability of superiority estimate equal to . 62 favoring the control condition . That is , if one analyst were randomly drawn from the ACH condition and another randomly drawn from the control condition , there would be a 62 % chance of the former having worse accuracy than the latter . Comparable probabilities of superiority favoring the con - trol condition likewise are obtained in tests of unitarity and additivity . In each case , coherence violations conformed to the unpacking eﬀect predicted by support theory ( Rottenstre - ich & Tversky , 1997 ; Tversky & Koehler , 1994 ) . As noted earlier , the unpacking eﬀect refers to the tendency for people to assign greater total probability to the sum of a MECE par - tition of a disjunctive event ( in the case of additivity ) or an event space ( in the case of unitarity ) . The unpacking eﬀect has been shown to undermine the logical coherence of geopo - litical assessments ( Tetlock & Lebow , 2001 ) , which suggests that such forms of incoherence can undermine strategic in - telligence assessments . Indeed , compared to regular fore - casters , elite super - forecasters of geopolitical topics tend to display greater coherence on other , unrelated probabilistic tasks ( Mellers et al . , 2017 ) . It should concern the intel - ligence community that a commonplace analytic tradecraft technique served to increase ( rather than reduce ) this form of judgmental error . Of course , critics might argue that perhaps analysts inter - preted the request for probabilities as requiring only a relative probability assessment of the hypotheses . After all , ACH is primarily aimed at ranking hypotheses by likelihood , and for that reason our control analysts were also instructed to as - sess the relative likelihoods of the hypotheses . However , we elicited probabilities for each hypothesis separately on a scale covering the [ 0 , 1 ] interval . Moreover , the four probabilities ( A - D ) that are bound by the unitarity axiom were elicited in immediate succession , an elicitation feature shown to miti - gate incoherence ( Mandel , 2005 ) . Therefore , we expect to ﬁnd even greater incoherence in analytic practice where the logical relations between assessments are likely to be ob - scured . Finally , we found that the rank - order correlations between analysts’ judgments and the correct values were small , on average , having only about 7 % shared variance . Another striking result of the present research concerns the relationship between the quality of analysts’ information usefulness evaluations and the quality of their probability judgments regarding the alternative hypotheses . Although analysts who used ACH provided ratings of probabilistic cue usefulness that were more strongly correlated with the cues’ probability gain values than analysts who did not use ACH , the former group’s assessments of information usefulness did virtually nothing to guide them to exploit the knowledge eﬀectively to boost accuracy in probability judgments . In contrast , among analysts in the control group , there was sub - stantially better correspondence between accuracy and the degree to which their usefulness ratings tracked probabil - ity gain . Analysts in the control group whose usefulness ratings tracked probability gain were better poised than ana - lysts in the ACH group to use that knowledge to improve the accuracy of their probability assessments . This ﬁnding was unanticipated and should ideally be tested for reproducibility in future research . While speculative , one explanation for the disconnect be - tween accurate evaluation of information usefulness and accuracy of probability judgments is that the consistency - encoding phase in ACH prompts analysts to adopt a per - spective that is evidence - contingent rather than hypothesis - contingent . That is , analysts are taught to evaluate evidence - hypothesis consistency within pieces of evidence and across hypotheses rather than the other way around . This approach is deliberate , reﬂecting Heuer’s ( 1999 ) belief that analysts are susceptible to conﬁrmation bias and thus need to be made to focus on evidence rather than their preferred hy - pothesis . The evidence - contingent approach should prompt consideration of information usefulness given that the con - sistency between a piece of evidence and each hypothesis be - ing evaluated is assessed before proceeding to another piece of evidence . However , we see that information integration within hypotheses is left to the questionable “sum of the in - consistency scores” rule in ACH . Unlike a normative ( e . g . , Bayesian ) approach , this rule merely serves as a summator and , moreover , selectively so by choosing to ignore scores that indicate degree of positive consistency . The integration rule is also exceptionally coarse in its treatment of evidence , assigning one of only three levels ( − 2 , − 1 , 0 ) to each piece of evidence , and such coarseness is likely to impede judgment accuracy ( Friedman , Baker , Mellers , Tetlock & Zeckhauser , 2018 ) . Moreover , ACH does virtually nothing as an analytic sup - port tool to ensure that analysts consistently map evidential strength onto − 1 and − 2 ratings . Consider two hypotheses , A and B . Assume that given ﬁve pieces of evidence , three analysts , X , Y , and Z agreed on the following . All ﬁve pieces of evidence are inconsistent with A and three pieces are in - consistent with B . Assume further that compared to Y , X has a low threshold for assigning − 2 ratings , and Z has a high threshold . All three analysts might agree that the ﬁve pieces of evidence are inconsistent with A , but not strongly so , and they would assign - 1 for each piece . They might further agree that the three pieces of evidence that are inconsistent Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 618 with B are stronger in their inconsistency than in the case of A , but given their diﬀering thresholds for assigning − 2 ratings , they may vary in their ratings . For instance , X might assign − 2 to the three pieces that are inconsistent with B , Y might assign , two − 2 ratings and one − 1 rating , and Z might assign − 1 ratings to each of the three pieces of evidence in - consistent with B . If so , in spite of the substantial agreement among analysts , using ACH , X would judge A less probable than B , Y would judge A and B as equally probable , and Z would judge A as more probable than B ! The present ﬁndings indicate that ACH is ineﬀective as a means of supporting analysts in assessment tasks requiring the integration of uncertain evidence in order to evaluate a set of hypotheses . The ﬁndings challenge a widespread assumption among tradecraft professionals in intelligence organizations that , although ACH ( and SATs , in general ) might not always help the analyst , at least they don’t hurt the analyst ( Mandel & Tetlock , 2018 ) . Two of the authors ( DRM and MKD ) who have worked for several years with an - alytic tradecraft professionals have repeatedly encountered a “nothing to lose” attitude when it comes to SAT training and on - the - job use . Yet , our ﬁndings suggest that , in fact , ACH can impede the quality of intelligence assessments . It can do so in two ways : ﬁrst , by undermining the coherence and accuracy of estimates and , second , by fostering a disconnec - tion between evidence evaluation and hypothesis evaluation . We therefore urge intelligence organizations to be more cir - cumspect about the beneﬁts of training analysts to use ACH and other SATs that have not received adequate testing . Indeed , a commonplace rebuttal from intelligence profes - sionals to any criticism of tradecraft methods is that although they aren’t perfect , intelligence organizations can’t just “do nothing . ” The idea of leaving analysts to their own “intu - itive” reasoning is thought to — and often does — result in bias and error . Our ﬁndings challenge this assumption since analysts who were left to their own devices performed better than analysts who used ACH . SAT proponents are likely to object and claim that our ﬁndings lack external validity . After all , intelligence ana - lysts seldom are presented with such neat problems where all evidence is precisely quantiﬁed and expresses relative frequencies and where the full set of pertinent hypotheses is explicit and , further , it is evident that these hypotheses are also neatly partitioned ( i . e . , MECE ) . We agree that in these and other respects the experimental task we used lacks mun - dane realism . However , we disagree with the implications that proponents would likely draw from such observations . Intelligence problems are murkier in many respects — the quality of evidence will be variable , the hypotheses might be unclearly deﬁned and will often fail to yield a MECE set , and analysts are likely to give no more than vague probabil - ity estimates on coarse verbal probability scales ( Dhami et al . , 2015 ; Friedman et al . , 2018 ; Mandel , in press ; Mandel & Barnes , 2018 ) . We see no compelling reason why ACH should help under those conditions when it does not help hypothesis evaluation under the much more modest require - ments of the present experimental task . Indeed , it is possible that ACH can do even more harm to judgment when analysts use it on the job . Clearly , it would be beneﬁcial to conduct research in the future that uses tasks that are more challenging in the respects noted while permitting unambiguous evaluation of the mer - its of ACH or other SATs . However , the present research already shows that ACH is not an all - purpose judgment cor - rective for problems involving the evaluation of multiple hy - potheses on the basis of uncertain evidence . In fact , the poor performance of both groups of analysts in this research raise a more basic question : why were they so inaccurate on a task ( even in terms of their relative probability judgments ) that is arguably much easier than the types of so - called puzzles and mysteries they encounter on the job ? This may ultimately prove to be a more important ﬁnding than the relative per - formance between conditions . In the present task , analysts had unambiguous sources of accurate information that they could exploit , yet most were at a loss do so regardless of whether they used ACH or not . Our ﬁndings therefore raise a fundamental question about the competence of analysts to judge probabilities . Given the small and homogeneous sam - ple of analysts we tested , it would be wrong to draw sweeping generalizations . Yet , if our ﬁndings do generalize across a wide range of analyst samples , it should prompt the intel - ligence community and the bodies that provide intelligence oversight to take stock of the practical signiﬁcance of the ﬁndings and study the putative causes of poor performance . We also respond to SAT proponents by noting that “doing nothing” is not the only alternative to using conventional analytic tradecraft techniques such as ACH . In this research , we examined two promising statistical methods that intelli - gence organizations could use to improve probability judg - ments after analysts had provided judgments — methods we accordingly describe as post - analytic . One method , coher - entization , exploits the logical structure of related queries by recalibrating probability assessments so that they conform to one or more axioms of probability calculus . As noted earlier , Karvetski , Olson , Mandel et al . ( 2013 ) showed that such methods substantially improve the accuracy of prob - ability judgments . Likewise , in the present experiment , a large improvement in analysts’ accuracy was achieved by coherentizing analysts’ probability judgments such that they respected the unitarity axiom . This method fully counter - acted the unpacking eﬀect exhibited by analysts in this re - search , especially those who were instructed to use ACH . We view CAP - based coherentization as illustrative rather than deﬁnitive . Other recalibration methods might be even more eﬀective or easier to apply . For instance , in the present research , we could have coherentized probabilities by sim - ple normalization ( i . e . , dividing each by their sum ) , as re - searchers sometimes do as a step in the statistical analysis of Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 619 probability judgment data ( e . g . , Prims & Moore , 2017 , Study 2 ) . This method ( as noted in Section 3 . 4 ) would have yielded even slightly better accuracy than CAP - based coherentiza - tion ( MAE = 0 . 13 for normalization , vs . 0 . 15 for CAP ) . Our study is clearly not designed to examine such competitions given it relies on a single vector of values deﬁning proba - bilistic accuracy . However , our ﬁndings suggest that research comparing optimization methods using such techniques un - der a broad range of task conditions are needed . Another post - analytic method intelligence organizations could use to boost the accuracy of probabilistic assessments is to aggregate them across small numbers of analysts . We found that substantial beneﬁts to accuracy were achieved by taking the arithmetic average of as few as three analysts . These ﬁndings are consistent with earlier studies showing that most of the advantage from aggregating can be achieved with between two to ﬁve judges ( e . g . , Ashton & Ashton , 1985 ; Libby & Blashﬁeld , 1978 ; Winkler & Clemen , 2004 ) . Moreover , we found that a simple equal - weighted aggre - gate of analysts’ judgments yielded comparable beneﬁt to the more complex coherence - weighted aggregation method . This result was unexpected given the superior performance coherence weighting aﬀorded over equal weighting in recent studies ( Karvetski , Olson , Mandel et al . , 2013 ; Wang et al . , 2011 ) . A key diﬀerence between the tasks in the present research and Karvetski , Olson , Mandel et al . ( 2013 ) is that the former included all information relevant to solving the task , whereas the latter relied on participants’ knowledge of world facts , such as who was the ﬁrst person to walk on the moon . Thus , whereas in the present research , coherentiza - tion may have already reaped most of the beneﬁt achievable through coherence weighting , in the earlier studies coherence weighting might also have beneﬁted accuracy by predicting how knowledgeable participants were . More generally , the present results indicate that intelli - gence organizations should be exploring how to eﬀectively incorporate processes for eliciting judgments from multi - ple analysts and then aggregating them in order to reduce judgment error . At present , intelligence organizations rarely capitalize on statistical methods such as the recalibration and aggregation approaches shown to be eﬀective in the present research . Instead , the management of intelligence produc - tion tends to rely on traditional methods such as having sole - source analysts provide input to an all - source analyst ( an approach that is common at the operational level ) , or by hav - ing a draft intelligence report reviewed by peers with relevant domain expertise and by the analyst’s director ( an approach often employed at the strategic level ) . Still , we caution not to infer too much from the aggregation results . It is tempting to suggest that the aggregate divines the wisdom of crowds , as Surowieki ( 2004 ) put it , yet our ﬁnding that aggregation of random response data yielded comparable error reduction as in analysts’ judgments clearly challenges that interpreta - tion as there was no wisdom in the random data to divine . Our analysis of how aggregates can improve relative proba - bility assessment , however , showed a large improvement in accurately capturing the rank ordering of probabilities , and this beneﬁt was entirely absent in the random response data , which suggests that aggregation did in fact boost the signal - to - noise ratio in analysts’ ordered probability judgments . To conclude , we argue that the intelligence community should look to recent examples of research that illustrate how organizations could better integrate recalibration and aggre - gation methods pioneered in decision science into day - to - day analytic practices . One example involves the systematic monitoring of probabilistic forecast accuracy within intelli - gence organizations ( e . g . , Mandel , 2015a ; Mandel & Barnes , 2018 ) . The results of such monitoring have shown that ana - lysts’ forecasts tend to be underconﬁdent , and that the cali - bration of intelligence units can be improved post - judgment through an organizational recalibration process that “extrem - izes” overly - cautious forecasts ( Mandel & Barnes , 2014 ; Baron et al . , 2014 ; Turner et al . , 2014 ) . Another exam - ple is the introduction in the US intelligence community of a classiﬁed prediction market that poses forecasting questions not unlike those worked on by strategic analysts as part of their routine assessment responsibilities . Stastny and Lehner ( 2018 ) showed that analysts’ forecasts within the prediction market , which aggregated the forecasters’ estimates but also shared the aggregated estimates with the forecasters , were substantially more accurate than the same forecasts arrived at through conventional analytic means . These examples il - lustrate the beneﬁts to analytic accuracy and accountability that intelligence organizations could accrue if they leveraged post - analytic mathematical methods for boosting the quality of expert judgment . References Armstrong , J . S . ( 2001 ) . Evaluating forecasting methods . In J . S . Armstrong ( Ed . ) , Principles of forecasting : A hand - book for researchers and practitioners . Norwell , MA : Kluwer . Ashton , A . H . , & Ashton , R . H . ( 1985 ) . Aggregating sub - jective forecasts : some empirical results . Management Science , 31 , 1499 – 1508 . Ayton , P . ( 1997 ) . How to be incoherent and seductive : Book - makers’ odds and support theory . Organizational Behav - ior and Human Decision Processes , 72 , 99 – 115 . Baratgin , J . , & Noveck , I . ( 2000 ) . Not only base - rates are neglected in the Lawyer - Engineer problem : an investi - gation of reasoners’ underutilization of complementarity . Memory & Cognition , 28 , 79 – 91 . Baron , J . ( 1985 ) . Rationality and intelligence . Cambridge , UK : Cambridge University Press . Baron , J . , Mellers , B . A . , Tetlock , P . E . , Stone , E . , & Ungar , L . H . ( 2014 ) . Two reasons to make aggregated probability forecasts more extreme . Decision Analysis , 11 , 133 – 145 . Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 620 Belton , I . , & Dhami , M . K . ( in press ) . Cognitive biases and debiasing in intelligence analysis . In R . Viale & K . Katzikopoulos ( Eds . ) , Handbook on bounded rationality . London : Routledge . Brenner , L . A . , & Rottenstreich , Y . ( 1999 ) . Focus , repack - ing and the judgment of grouped hypotheses . Journal of Behavioral Decision Making , 12 , 141 – 148 . Bruine de Bruin , W . , Fischbeck , P . S . , Stiber , N . A . , & Fis - chhoﬀ , B . ( 2002 ) . What number is “ﬁfty - ﬁfty” ? : Redis - tributing excessive 50 % responses in elicited probabilities . Risk Analysis , 22 , 713 – 723 . Butler , Lord ( 2004 ) . Review of intelligence on weapons of mass destruction , report of a committee of privy council - lors ( the Butler Report ) , HC 898 , London , The Stationery Oﬃce . Chang , W . , Berdini , E . , Mandel , D . R . , & Tetlock , P . E . ( 2018 ) . Restructuring structured analytic techniques in intelligence . Intelligence and National Security , 33 , 337 – 356 . Clemen , R . T . , & Winkler , R . L . ( 1999 ) . Combining prob - ability distributions from experts in risk analysis . Risk Anaysis , 19 , 197 – 203 . Cohen , J . ( 1992 ) . A power primer . Psychological Bulletin , 112 , 155 - 159 . Convertino , G . , Billman , D . , Pirolli , P . , Massar , J . P . , & Shrager , J . ( 2008 ) . The CACHE study : Group eﬀects in computer - supported collaborative analysis . Computer Supported Cooperative Work , 17 , 353 – 393 . Coulthart , S . J . ( 2017 ) . An evidence - based evaluation of 12 core structured analytic techniques . International Journal of Intelligence and CounterIntelligence , 30 , 368 – 391 . Dhami , M . K . , Belton , I . K . , & Careless , K . E . ( 2016 ) . Critical review of analytic techniques . 2016 European Intelligence and Security Informatics Conference , 152 – 155 . http : / / dx . doi . org / 10 . 1109 / EISIC . 2016 . 33 . Dhami , M . K . , & Mandel , D . R . ( 2013 ) . How do defen - dants choose their trial court ? Evidence for a heuristic processing account . Judgment and Decision Making , 8 , 552 – 560 . Dhami , M . K . , Mandel , D . R . , Mellers , B . A . , & Tetlock , P . E . ( 2015 ) . Improving intelligence analysis with deci - sion science . Perspectives on Psychological Science , 106 , 753 – 757 . Fox , C . R . , Rogers , B . , & Tversky , A . ( 1996 ) . Option traders exhibit subadditive decision weights . Journal of Risk and Uncertainty , 13 , 5 – 19 . Friedman , J . A . , Baker , J . D . , Mellers , B . A . , Tetlock , P . E . and Zeckhauser , R . ( 2018 ) . The value of precision in probability assessment : Evidence from a large - scale geopolitical forecasting tournament . International Studies Quarterly , 62 , 410 – 422 . Grissom , R . J . , & Kim , J . J . ( 2005 ) . Eﬀect sizes for research : A broad practical approach . Mahwah , NJ : Erlbaum . Heuer , R . J . , Jr . ( 1999 ) . Psychology of intelligence analysis . Washington , DC : Center for the Study of Intelligence . Heuer , R . J . , Jr . , & Pherson , R . H . ( 2014 ) . Structuredanalytic techniques for intelligence analysis . Washington , DC : CQ Press . Kahneman , D . , & Tversky , A . ( 1972 ) . Subjective probabil - ity : a judgment of representativeness . Cognitive Psychol - ogy , 3 , 430 – 454 . Karvetski , C . W . , Olson , K . C . , Gantz , D . T . , & Cross , G . A . ( 2013 ) . Structuring and analyzing competing hypotheses with Bayesian networks for intelligence analysis . EURO Journal on Decision Processes , 1 , 205 – 231 . Karvetski , C . W . , Olson , K . C . , Mandel , D . R . , & Twardy , C . R . ( 2013 ) . Probabilistic coherence weighting for opti - mizing expert forecasts . Decision Analysis , 10 , 305 – 326 . Kretz , D . R . , Simpson , B . J . , & Graham , C . J . ( 2012 ) . A game - based experimental protocol for identifying and overcoming judgment biases in forensic decision analysis . 2012 IEEE Conference on Technologies for Homeland Se - curity , 439 – 444 . Waltham , MA : IEEE . http : / / dx . doi . org / 10 . 1109 / THS . 2012 . 6459889 . Jones , N . ( 2018 ) . Critical epistemology for Analysis of Com - peting Hypotheses . Intelligence and National Security , 33 , 273 – 289 . Lehner , P . E . , Adelman , L . , Cheikes , B . A . , & Brown , M . J . ( 2008 ) . Conﬁrmation bias in complex analyses . IEEE Transactions on Systems , Man , and Cybernetics - Part A : Systems and Humans , 38 , 584 – 592 . Libby , R . , & Blashﬁeld , R . K . ( 1978 ) . Performance of a composite as a function of the number of judges . Organi - zational Behavior and Human Performance , 21 , 121 – 129 . Macchi , L . , Osherson , D . , and Krantz , D . H . ( 1999 ) . A note on superadditive probability judgment . Psychological Re - view , 106 , 210 – 214 . Mandel , D . R . ( 2005 ) . Are risk assessments of a terrorist attack coherent ? Journal of Experimental Psychology : Applied , 11 , 277 – 288 . Mandel , D . R . ( 2008 ) . Violations of coherence in subjective probability : a representational and assessment processes account . Cognition , 106 , 130 – 156 . Mandel , D . R . ( 2015a ) . Accuracy of intelligence forecasts from the intelligence consumer’s perspective . Policy In - sights from the Behavioral and Brain Sciences , 2 , 111 – 120 . Mandel , D . R . ( 2015b ) . Instruction in information structur - ing improves Bayesian judgment in intelligence analysts . Frontiers in Psychology 6 , article 387 , 1 – 12 . http : / / dx . doi . org / 10 . 3389 / fpsyg . 2015 . 00387 Mandel , D . R . ( in press ) . Can decision science improve intelligence analysis ? In S . Coulthart , M . Landon - Murray , & D . Van Puyvelde ( Eds ) , Researching national security intelligence : A reader . Washington , DC : Georgetown University Press . Judgment and Decision Making , Vol . 13 , No . 6 , November 2018 Boosting intelligence analysts’ judgment accuracy 621 Mandel , D . R . , & Barnes , A . ( 2014 ) . Accuracy of fore - casts in strategic intelligence . Proceedings of the National Academy of Sciences , 111 , 10984 – 10989 . Mandel , D . R . , & Barnes , A . ( 2018 ) . Geopolitical forecast - ing skill in strategic intelligence . Journal of Behavioral Decision Making , 31 , 127 – 137 . Mandel , D . R . , & Tetlock , P . E . ( 2018 ) . Correcting judgment correctives in national security intelligence . Manuscript submitted for publication . Marchio , J . ( 2014 ) . Analytic tradecraft and the intelligence community : Enduring value , intermittent emphasis . In - telligence and National Security , 29 , 159 – 183 . Mellers , B . A . , Baker , J . D . , Chen , E . , Mandel , D . R . , & Tetlock , P . E . ( 2017 ) . How generalizable is good judg - ment ? A multi - task , multi - benchmark study . Judgment and Decision Making , 12 , 369 – 381 . National Research Council ( 2011 ) . Intelligence analysis for tomorrow : Advances from the behavioral and social sci - ences . Washington , DC : National Academies Press . Nelson , J . D . ( 2005 ) . Finding useful questions : on Bayesian diagnosticity , probability , impact and information gain . Psychological Review , 112 , 979 – 999 . Nickerson , R . S . ( 1998 ) . Conﬁrmation bias : A ubiquitous phenomenon in many guises . Review of General Psychol - ogy , 2 , 175 – 220 . Osherson , D . , & Vardi , M . Y . ( 2006 ) . Aggregating disparate estimates of chance . Games and Economic Behavior , 56 , 148 – 173 . Pool , R . ( 2010 ) . Field evaluation in the intelligence and counterintelligence context : Workshop summary . Wash - ington , DC : National Academies Press . Pope , S . , & Jøsang , A . ( 2005 ) . Analysis of competing hy - potheses using subjective logic . In 10 th CCRTS : The Fu - ture of Command and Control , pp . 1 – 30 . Popper , K . ( 1959 ) . The logic of scientiﬁc discovery . London , UK : Hutchison & Co . Predd , J . B . , Osherson , D . N . , Kulkarni , S . R . , & Poor , H . V . ( 2008 ) . Aggregating probabilistic forecasts from incoherent and abstaining experts . Decision Analysis , 5 , 177 – 189 . Prims , J . P . , & Moore , D . A . ( 2017 ) . Overconﬁdence over the lifespan . Judgment and Decision Making , 12 , 29 – 41 . Rottenstreich , Y . , & Tversky , A . ( 1997 ) . Unpacking , repack - ing , and anchoring : Advances in support theory . Psycho - logical Review , 104 , 406 – 415 . Ruscio , J . , & Mullen , T . ( 2012 ) . Conﬁdence intervals for the probability of superiority eﬀect size measure and the area under a receiver operating characteristic curve . Mul - tivariate Behavioral Research , 47 , 201 – 223 . Sloman , S . , Rottenstreich , Y . , Wisniewski , E . , Hadjichris - tidis , C . , & Fox , C . R . ( 2004 ) . Typical versus atypical unpacking and superadditive probability judgment . Jour - nal of Experimental Psychology : Learning , Memory , & Cognition , 30 , 573 – 582 . Stastny , B . J . , & Lehner , P . E . ( 2018 ) . Comparative eval - uation of the forecast accuracy of analysis reports and a prediction market . Judgment and Decision Making , 13 , 202 – 211 . Surowiecki , J . ( 2004 ) . The wisdom of crowds . New York : Double Day . Tetlock , P . E . ( 2005 ) . Expert political judgment : How good is it ? How can we know ? Princeton , NJ : Princeton Uni - versity Press . Tetlock , P . E . , & Lebow , R . N . ( 2001 ) . Poking holes in counterfactual covering laws : Cognitive styles and histor - ical reasoning . American Political Science Review , 95 , 829 – 843 . Tsai , J . , & Kirlik , A . ( 2012 ) . Coherence and correspondence competence : Implications for elicitation and aggregation of probabilistic forecasts of world events . Proceedings of Human Factors and Ergonomics Society 56th Annual Meeting , pp . 313 – 317 , Thousand Oaks , CA : Sage . Turner , B . M . , Steyvers , M . , Merkle , E . C . , Budescu , D . V . , & Wallsten , T . S . ( 2014 ) . Forecast aggregation via recalibration . Machine Learning , 95 , 261 – 289 . Tversky , A . , & Koehler , D . J . ( 1994 ) . Support theory : a nonextensional representation of subjective probability . Psychological Review , 101 , 547 – 567 . UK Ministry of Defence ( 2013 ) . Quick wins for busy ana - lysts . London : author . US Government ( 2009 ) . A tradecraft primer : Structured analytic techniques for improving intelligence analysis . Washington , DC : Center for the Study of Intelligence Analysis . Vargha , A . , & Delaney , H . D . ( 2000 ) . A critique and im - provement of the CL common language eﬀect size statis - tics of McGraw and Wong . Journal of Educational and Behavioral Statistics , 25 , 101 – 132 . Wallsten , T . S . , Budescu , D . V . , & Zwick , R . ( 1993 ) . Com - paring the calibration and coherence of numerical and verbal probability judgments . Management Science , 39 , 176 – 190 . Wang , G . , Kulkarni , S . R . , Poor , H . V . , & Osherson , D . N . ( 2011 ) . Aggregating large sets of probabilistic forecasts by weighted coherent adjustment . Decision Analysis , 8 , 128 – 144 . Wheaton , K . ( 2014 ) . Reduce bias in analysis : Why should we care ? Retrieved from http : / / sourcesandmethods . blogspot . com / 2014 / 03 / reduce - bias - in - analysis - why - should - we . html . Winkler , R . L . , & Clemen , R . T . ( 2004 ) . Multiple experts vs . multiple methods : Combining correlation assessments . Decision Analysis , 1 , 167 – 176 . Willmott , C . J . , & Matsuura , K . ( 2005 ) . Advantages of the mean absolute error ( MAE ) over the root mean square error ( RMSE ) in assessing average model performance . Climate Research , 30 , 79 – 82 .