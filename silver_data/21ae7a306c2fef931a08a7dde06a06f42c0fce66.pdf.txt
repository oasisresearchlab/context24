IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment Hitoshi Matsuyama Nobuo Kawaguchi hitoshi @ ucl . nuee . nagoya - u . ac . jp kawaguti @ nagoya - u . jp Nagoya University Japan Brian Y . Lim âˆ— brianlim @ comp . nus . edu . sg National University of Singapore Singapore ABSTRACT AI - driven Action Quality Assessment ( AQA ) of sports videos can mimic Olympic judges to help score performances as a second opin - ion or for training . However , these AI methods are uninterpretable and do not justify their scores , which is important for algorithmic accountability . Indeed , to account for their decisions , instead of scoring subjectively , sports judges use a consistent set of criteria â€” rubric â€” on multiple actions in each performance sequence . There - fore , we propose IRIS to perform Interpretable Rubric - Informed Segmentation on action sequences for AQA . We investigated IRIS for scoring videos of figure skating performance . IRIS predicts ( 1 ) action segments , ( 2 ) technical element score differences of each seg - ment relative to base scores , ( 3 ) multiple program component scores , and ( 4 ) the summed final score . In a modeling study , we found that IRIS performs better than non - interpretable , state - of - the - art mod - els . In a formative user study , practicing figure skaters agreed with the rubric - informed explanations , found them useful , and trusted AI judgments more . This work highlights the importance of using judgment rubrics to account for AI decisions . CCS CONCEPTS â€¢ Computingmethodologies â†’ Artificialintelligence ; â€¢ Human - centered computing â†’ Interactive systems and tools ; Empir - ical studies in HCI . KEYWORDS Explainable AI , action quality assessment , rubric , figure skating ACM Reference Format : Hitoshi Matsuyama , Nobuo Kawaguchi , and Brian Y . Lim . 2023 . IRIS : Inter - pretable Rubric - Informed Segmentation for Action Quality Assessment . In IUI â€™23 : ACM Conference on Intelligent User Interfaces , Mar 27 â€“ 31 , 2023 , Syd - ney , Australia . ACM , New York , NY , USA , 11 pages . https : / / doi . org / 3581641 . 3584048 âˆ— Corresponding author . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Â© 2023 Association for Computing Machinery . ACM ISBN 979 - 8 - 4007 - 0106 - 1 / 23 / 03 . . . $ 15 . 00 https : / / doi . org / 3581641 . 3584048 1 INTRODUCTION The need for explainable AI ( XAI ) has grown significantly due to the prevalence of AI in many aspects of society . This is especially im - portant where judgements are performed automatically . One such area is sports analytics or action quality assessment ( AQA ) , where many AI - based computer vision techniques have been developed to assess the quality of performances from videos [ 5 , 18 , 19 , 26 â€“ 28 , 34 , 42 , 43 , 45 ] . This can mimic Olympic judges to help score performances as a second opinion , help spectators understand how certain sports are judged , and help athletes get access to cheaper and faster feedback . However , these techniques mostly predict scores and do not provide more information on how the scores were derived . They are inscrutable and uninterpretable . There are some prior work to explain predictions in AQA . For example , Yu et al . explained scoring diving performances [ 44 ] us - ing Grad - CAM saliency maps [ 33 ] . Although saliency maps can be useful in some applications [ 1 ] , these explanations were originally designed for model debugging , and are overly technical and unlikely to be usable by practicing athletes . One way to increase relevance is to make the explanations concept - based [ 16 , 17 ] or more relat - able [ 47 ] , but these may be too simplistic and generic for skilled activities . Several researchers have argued that AI explanations should be tailored to end users and relevant contexts [ 20 , 22 , 40 ] . Wang and Yin identified several important desiderata of XAI for non - experts [ 41 ] and Dodge et al . demonstrated the benefit of XAI on fairness decisions [ 10 ] , but there has been limited work on pro - viding XAI to domain experts [ 21 , 23 ] . Pirsiavash et al . developed diving feedback proposals by calculating gradients of scores relative to pose estimation joints [ 29 ] , but is only data - driven , rather than driven by how athletes or judges make decisions . Instead , we argue that XAI developers should study the scoring rubrics used in the application domain to identify requirements for justifications . Indeed , to account for their decisions , instead of scoring subjectively , sports judges use a consistent set of criteria â€” rubric â€” on multiple actions in each performance sequence . For example , in figure skating , judges refer to a score sheet that is standardized by the ISU Judging System [ 37 , 38 ] to include multiple criteria and methods for scoring . It is important for the AI to adhere to these rubrics to be as accountable as human judges . See Fig . 1 for conceptual overview , and Fig . 3 for demo . Towards this end , we propose IRIS , an I nterpretable R ubric - I nformed S egmentation method to predict and explain video - based action quality assessment ( AQA ) . It is interpretable to explain how it derived its judgement score . It is informed by a rubric to deter - mine what to consider when calculating its judgement . It performs a r X i v : 2303 . 09097v1 [ c s . A I ] 16 M a r 2023 IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Matsuyama , Kawaguchi , and Lim Rubric - Using Human Judge Rubric - Informed XAI Judge ( IRIS ) Non - accountableAI Judge Figure 1 : Conceptual overview of how human judges use rubrics to guide their ratings , while non - explainable AI models predict with no accountability . We propose rubric - informed explainable AI ( XAI ) judges to use rubrics from domain experts to judge with accountability . segmentation to locate specific sections of a performance to judge specific criteria . We implemented this for the application domain of figure skating , but the high - level approach is generalizable to other video - based AQA ( e . g . , diving , dancing , cooking ) with prede - fined criteria rubrics . IRIS provides four key features to predict and explain its judgement : Score , Sequence , Segments , and Subscores . Being rubric - informed , IRIS is more accurate to predict scores due to rule guidance , and is more accountable to justify its decisions based on domain - established criteria . We evaluated IRIS in multiple studies : 1 ) a demonstration study of the interpretability of IRIS with a visualization of ice skating , 2 ) a quantitative modeling study to compare IRIS against the base - line models , and 3 ) a formative user study with experienced figure skaters to qualitatively investigate the usefulness and trustworthi - ness of various rubric - informed features in IRIS . In summary , our contributions are : â€¢ The introduction of a rubric - informed approach to identify inter - pretability requirements for AI based on scoring rubrics . â€¢ The development of IRIS , an interpretable deep learning model that is more useful , understandable , and trustworthy than other state - of - the - art models for action quality assessment . â€¢ An evaluation of rubric - informed interpretability for figure skat - ing with domain experts in a formative study . 2 RELATED WORK We summarize related works on predicting and explaining AQA . 2 . 1 Action Quality Assessment ( AQA ) Many AI - driven action quality assessment ( AQA ) systems have been proposed for sports [ 5 , 18 , 19 , 26 â€“ 28 , 34 , 42 , 43 , 45 ] and other fields [ 11 , 12 , 46 ] . AI - based AQA extracts video features trains a regression model to predict scores . The feature extraction methods generally use three - dimensional convolutional neural networks ( 3DCNN ) , such as C3D [ 36 ] and I3D [ 7 ] . These networks convert short segments of video into feature vectors by convolving in both 2D spatial and 1D temporal dimensions . To handle longer videos with longer - term behaviors , Parmar et al . combined the 3DCNN with recurrent LSTM to propose C3D - LSTM [ 28 ] . Zheng et al . devel - oped a hybrid approach of video and image features and proposed a context - aware AQA based on a Graph Convolutional Network ( GCN ) [ 45 ] . They modeled static posture and dynamic movement to capture the short and long - term temporal relationships . Nekoui et al . [ 26 ] proposed a CNN - based approach that captures both fine and coarse - grained temporal dependencies . Using both video fea - tures and pose estimation heatmaps [ 25 ] , they stacked CNN - based modules of various kernel sizes to capture patterns at different time resolutions . For figure skating AQA , Xu et al . [ 42 ] proposed a multi - scale and skip - connected CNN - LSTM method to capture short - term time dependencies with variously - sized CNN kernels , and LSTM with self - attention for longer - term time dependencies . 2 . 2 Explaining Action Quality Assessment While the aforementioned research on AQA focused on high pre - diction performance ( e . g . , Spearmanâ€™s rank correlation coefficient between AI and human judges ) , they neglected how users would use and interpret the score predictions . Explainable AI ( XAI ) tech - niques could help . They are widely used in prediction tasks , such as computer vision [ 1 , 2 , 33 ] , and natural language [ 8 , 24 , 30 , 31 ] . For example , Yu et al . explained the scoring of diving performances [ 44 ] using Grad - CAM saliency maps [ 33 ] . Unfortunately , these expla - nations were primarily designed for model debugging , are overly technical , and unlikely to be usable by practicing athletes . It re - mains unclear how practicing athletes , not data scientists , would use and understand the AI - based AQA systems . Therefore , it is important to study how we can show or explain the AQA results to practitioners . Pirsiavash et al . developed feedback for the sport of diving by calculating the gradients of scores relative to pose estimation joints [ 29 ] . This is highly relevant to the diving activity , but is only data - driven , rather than driven by how athletes or judges make de - cisions . Instead , we argue that XAI for AQA in sports should adhere to the rubrics that are used in the sport , which is our approach with IRIS . Khan et al . [ 15 ] developed a method for analyzing batting in cricket using wearable sensors and further proposed a visualization inspired by TV shows of cricket games . They classified various low - level sub - actions identified from careful analysis of how play - ers move when playing cricket . Thompson et al . [ 35 ] proposed a visualization for dressage ( horse riding ) , where they quantified scoring criteria inspired from score cards in the sport . We drew our rubrics from score cards in the sport of figure skating . Both aforementioned methods used locomotion data of IMU sensors , and implemented heuristically or with shallow machine learning models ( e . g . , SVM , kNN , decision tree ) . With IRIS , we imbued a deep learning meta - architecture with rubric - informed features to be generalizable and can apply to the more ubiquitous video data . 3 BACKGROUND : RUBRIC FOR FIGURE SKATING ACTION QUALITY ASSESSMENT In this section , we describe how rubrics are critical for formal and accountable judgement of many performances , our application use case of figure skating and its judging rubric . This background guides the subsequent design of our proposed explainable system . 3 . 1 Rubrics In many situations where judgements affect many peopleâ€™s lives , scoring rubrics have been developed to formalize how performance IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia LADIES SHORT PROGRAM JUDGES DETAILS PER SKATER ISU European Championships 2014 Rank Name Total Deductions Nation Total Segment Score Total Element Score Total Program Component Score ( factored ) Starting Number 26 . 76 53 . 02 GER Nathalie WEINZIERL # Executed Elements Base Value GOE Scores of Panel 1 4 . 10 - 1 . 90 - 3 - 3 - 3 - 3 - 2 - 2 - 2 - 3 - 3 Ref 2 . 20 10 The Judges Panel ( in random order ) 26 . 26 3T + COMBO I n f o 34 0 . 00 6 . 50 0 . 50 1 0 1 1 1 0 1 0 1 6 . 00 2 3Lz 3 . 34 0 . 64 2 1 1 1 1 2 1 2 1 2 . 70 3 LSp4 x 3 . 63 0 0 0 0 - 1 0 0 1 0 0 . 00 3 . 63 4 2A 5 3 . 00 0 . 29 1 3 . 29 1 0 2 0 1 0 0 1 FSSp4 1 1 1 1 1 1 1 1 3 . 80 1 0 . 50 3 . 30 6 StSq3 3 . 50 0 . 50 1 1 1 1 1 1 1 1 1 7 4 . 00 CCoSp4 26 . 23 Program Components Skating Skills 6 . 50 7 . 00 6 . 75 6 . 75 6 . 50 6 . 50 7 . 25 6 . 50 7 . 25 Factor 0 . 80 6 . 75 26 . 76 Transition / Linking Footwork 0 . 80 6 . 25 6 . 50 6 . 00 6 . 50 6 . 00 6 . 00 6 . 75 6 . 25 6 . 75 6 . 32 Performance / Execution 0 . 80 6 . 50 6 . 00 6 . 50 6 . 50 6 . 25 6 . 50 7 . 25 6 . 75 7 . 00 6 . 57 Choreography / Composition 6 . 75 6 . 75 6 . 00 6 . 75 6 . 25 6 . 00 6 . 75 6 . 50 6 . 75 0 . 80 6 . 54 Interpretation 6 . 50 6 . 75 7 . 00 6 . 75 6 . 00 6 . 25 6 . 75 6 . 50 7 . 00 0 . 80 6 . 64 Judges Total Program Component Score ( factored ) 26 . 26 Deductions : 0 . 00 x Credit for highlight distribution , base value multiplied by 1 . 1 Rank Name Total Deductions Nation Total Segment Score Total Element Score Total Program Component Score ( factored ) Starting Number 26 . 49 52 . 55 SWE Viktoria HELGESSON # Executed Elements Base Value GOE Scores of Panel 1 4 . 10 - 1 . 30 - 3 - 2 0 - 2 - 2 - 2 - 3 - 1 - 1 Ref 2 . 80 11 The Judges Panel ( in random order ) 26 . 06 3T I n f o 27 0 . 00 6 . 60 0 . 20 1 1 0 0 0 0 0 1 0 6 . 40 2 3Lo + 2T 2 . 90 0 . 50 1 2 1 0 2 1 1 1 0 2 . 40 3 LSp3 x 4 . 34 2 1 2 1 1 1 2 2 1 0 . 71 3 . 63 4 2A 5 2 . 30 - 0 . 51 - 2 1 . 79 - 2 - 2 - 2 - 1 - 2 - 2 - 1 - 1 FSSp2 0 0 0 - 1 0 0 2 - 1 3 . 46 0 - 0 . 04 3 . 50 6 CCoSp4 3 . 90 0 . 70 1 1 2 1 1 0 1 1 1 7 4 . 60 StSq4 26 . 23 Program Components Skating Skills 6 . 25 7 . 00 7 . 25 6 . 50 6 . 50 6 . 75 6 . 75 6 . 75 6 . 75 Factor 0 . 80 6 . 71 26 . 49 Transition / Linking Footwork 0 . 80 6 . 00 6 . 50 7 . 00 6 . 25 6 . 00 6 . 00 6 . 25 6 . 50 6 . 25 6 . 25 Performance / Execution 0 . 80 6 . 50 6 . 75 7 . 25 6 . 25 6 . 00 6 . 50 6 . 50 6 . 75 7 . 00 6 . 61 Choreography / Composition 6 . 00 6 . 50 7 . 00 6 . 50 6 . 00 6 . 50 6 . 50 6 . 50 6 . 50 0 . 80 6 . 43 Interpretation 6 . 50 6 . 50 7 . 50 6 . 25 6 . 25 6 . 00 6 . 75 6 . 75 7 . 00 0 . 80 6 . 57 Judges Total Program Component Score ( factored ) 26 . 06 Deductions : 0 . 00 x Credit for highlight distribution , base value multiplied by 1 . 1 Rank Name Total Deductions Nation Total Segment Score Total Element Score Total Program Component Score ( factored ) Starting Number 28 . 39 50 . 42 FIN Juulia TURKKILA # Executed Elements Base Value GOE Scores of Panel 1 8 . 20 0 . 90 2 1 1 1 1 1 1 2 2 Ref 9 . 10 12 The Judges Panel ( in random order ) 22 . 03 3T + 3T I n f o 16 0 . 00 2 . 70 - 1 . 50 - 2 - 2 - 2 - 2 - 2 - 3 - 2 - 2 - 3 4 . 20 2 3S 2 . 76 - 0 . 04 0 0 0 0 - 1 - 1 0 0 0 2 . 80 3 FCSp3 x 3 . 63 0 1 0 0 0 0 0 0 0 0 . 00 3 . 63 4 2A 5 3 . 00 0 . 43 1 3 . 43 1 0 1 0 1 1 1 1 CCoSp3 1 1 1 1 1 1 1 1 3 . 80 1 0 . 50 3 . 30 6 StSq3 2 . 40 0 . 57 1 1 1 1 1 1 1 2 2 7 2 . 97 LSp3 27 . 53 Program Components Skating Skills 5 . 50 5 . 25 5 . 75 5 . 75 5 . 00 6 . 00 5 . 50 6 . 00 6 . 00 Factor 0 . 80 5 . 68 28 . 39 Transition / Linking Footwork 0 . 80 5 . 00 5 . 00 5 . 25 5 . 50 4 . 50 5 . 50 5 . 25 5 . 25 5 . 50 5 . 25 Performance / Execution 0 . 80 5 . 25 5 . 00 5 . 50 5 . 75 5 . 00 6 . 00 5 . 50 6 . 00 5 . 75 5 . 54 Choreography / Composition 5 . 25 4 . 75 5 . 50 6 . 25 4 . 75 6 . 00 5 . 75 5 . 50 5 . 75 0 . 80 5 . 50 Interpretation 5 . 50 4 . 75 5 . 75 6 . 00 4 . 75 5 . 75 5 . 75 5 . 75 5 . 75 0 . 80 5 . 57 Judges Total Program Component Score ( factored ) 22 . 03 Deductions : 0 . 00 x Credit for highlight distribution , base value multiplied by 1 . 1 Table 1 : Example score sheet showing the total score , TES , PCS , executed elements ( actions that the figure skater performed ) , base values ( difficulty scores for each action ) , and PCS components . Judges use the score sheet to guide their judging . Figure skaters and spectators see the score sheets to understand the result of contests . is judged . This is especially used in education to evaluate qualitative assessments , such as essays and presentations [ 9 ] , and in subjective sports [ 6 ] . Rubrics can help judges focus on specific aspects for evaluation , and make judging more objective , rather than based on inscrutable , subjective opinions . This also helps students and trainees know what to aim for to score highly . Herman et al . [ 14 ] identified four elements that make up scoring rubrics , which we clarify as two key points with corresponding elaboration points : 1 ) One or more traits or dimensions that serve as the basis for judging the student response . â€¢ Definitions and examples to clarify the meaning of each trait or dimension . 2 ) A scale of values on which to rate each dimension . â€¢ Standards of excellence for specified performance levels ac - companied by models or examples of each level . Therefore , a rubric needs to have dimensions ( also known as criteria ) on which to judge the performance , and a scale rating ( or score ) for each criteria . We focus on the high - level key points , since we are leveraging a known rubric , rather than developing them and training new judges or trainees . We assume that users of rubric - informed explainable AI ( XAI ) will have the implicit knowledge of definitions and examples , and know how to score each criteria . 3 . 2 Scoring figure skating Figure skating is an competitive sport where athletes skate to music , to demonstrate their technical athletic and qualitative artistic skills . It is a key winter sport in many championships , including the Winter Olympic Games . While many spectators enjoy watching the sport , it is often a mystery to understand how skater performances are judged . Instead of being subjective or counterintuitive , judges actually use a formal process â€” the International Skating Union ( ISU ) Judging System [ 37 ] â€” to score the performance of each skater . Judges use a score sheet to inform and guide their scoring . Formally , this score sheet is a rubric with multiple criteria with separate rating scores that are additive . For this work , we focus on the single - skating short - program event , where each skater performs for about three minutes , and is subjected to more rules and restrictions on their actions to execute , compared to free - form skating . Table 1 shows an example score sheet with judgement results from human judges during the 2014 European Figure Skating Championships . The total score consists of a combined Technical Element Score ( TES ) and a combined Program Component Score ( PCS ) . Skaters perform a sequence of elements ( actions ) in a schedule that is known ahead of the performance . The chosen elements and how well each was executed affects the TES . For example , a skater may choose a sequence with easier elements , but would be expected to earn fewer points than another skater with a more complex sequence . This determines the Base score . For each element , the skater may execute it very well or poorly , receiving a positive or negative Grade of Execution ( GOE ) score accordingly . GOE spans âˆ’ 5 to 5 . Each technical element is thus scored by adding Base and GOE . There are three main types of elements â€” Jumps , Spins , and Step Sequences â€” and the Transition between them . A jump is an action for which a skater leaps into the air , rotates at high speed , and lands after one or more revolutions . There are 6 types of jumps : Toe Loop ( T ) , Salchow ( S ) , Loop ( Lo ) , Flip ( F ) , Lutz ( Lz ) , and Axel ( A ) , which are distinguished by the way they take off and land when entering each jump . Different types are variously difficult and will affect the expected score for the element . Furthermore , each type of jump is assessed by the number of rotations ; higher number of rotations are IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Matsuyama , Kawaguchi , and Lim more difficult to execute . A spin is a clockwise or counterclockwise rotation on ice with either foot on the rink . There are three basic types of spins : Upright ( USp ) , Sit ( SSp ) , and Camel ( CSp ) . Spins can be performed individually or as a sequence combining different spin types with a change of position or foot , e . g . , Change Foot Sit Spin ( CSSp ) . For spins , judges see the difficulty , smoothness , and stability of movements . For example , a difficult entrance to the spin and continuous spins with stable posture will earn the skater a higher score . A step sequence is a technique that uses the entire rink for continuous footwork and performance . Skaters show steps and turns in a pattern , e . g . , Rocker turn and Bracket turn , on the ice while fully utilizing the ice skating rink in accordance with the character of the music . There are three types of step sequences : Straight line step sequence ( SISt ) , Circular step sequence ( CiSt ) , and Serpentine step sequence ( SeSt ) . Step sequences must match the music , be performed effortlessly throughout with good energy , flow and execution , and must have deep edges , clean turns and steps . Other than scoring specific elements in the sequence , the skater is graded on overall qualities of her performance . This is indicated in the PCS score to judge various components : Skating Skills , Tran - sition / Linking Footwork , Performance / Execution , Choreography / Composition , and Interpretation . Skating Skills score the over - all cleanness , sureness , edge control , and flow over the ice surface demonstrated by a command of the skating vocabulary ( edges , turns , steps , etc . ) , the clarity of technique , and the use of effortless power to accelerate and vary speed . Transitions / Linking Footwork scores the varied and purposeful use of intricate footwork , positions , move - ments , and holds linking all elements . Although this criterion may overlap with the TES for a step sequence , which include transitions and footwork , judges look at the entire performance to measure the quality to score this PCS component . Performance / Execution scores the involvement of the skater physically , emotionally , and in - tellectually as she delivers the intent of the music and composition . Choreography / Composition scores how intentionally developed and original is the arrangement of all types of movements according to the principles of the musical phrase , space , pattern , and structure . Interpretation scores the personal , creative , and genuine translation of the rhythm , character , and content of the music to movement . 4 TECHNICAL APPROACH We introduce IRIS , an I nterpretable R ubric - I nformed S egmentation method to predict and explain video - based action quality assess - ment ( AQA ) . It is ante - hoc interpretable [ 32 , 47 ] to explain how it derived its judgement score . It is informed by a rubric to determine what to consider when calculating its judgement . It performs seg - mentation to specify moments of a performance to judge specific criteria . We implemented this for the application of figure skating , but the high - level approach is generalizable to other video - based AQA with established rubrics ( e . g . , diving , dancing , cooking ) . IRIS provides four key rubric features ( Score , Sequence , Seg - ments , and Subscores ) to predict and explain its judgement . Score is the final judgement , Sequence describes the actions to be judged ( for technical elements ) , Segments identify when each action was judged based on specific criteria , and Subscores indicate how each specific criteria was judged ( for TES and PCS ) . Fig . 2 illustrates the architecture of IRIS , in the following subsections . 4 . 1 Data preparation Before modeling , we collected and processed data to gather neces - sary information for IRIS . We obtained video data of figure skating from the MIT - Skate dataset [ 29 ] which includes 150 videos of short - event figure skating from various championships in 2008 , 2010 , 2012 , and 2014 . Each video is about three minutes long , which we converted to 4D tensors with 2D for x - y pixels , 1D for time , and 1D for color channels . We collected rubric data as score sheets for each figure skater from online sources of the championship re - sults maintained by ISU ( e . g . , ISU European Championships 2014 1 ) . The score sheets are in PDF format , which we parsed with optical character recognition using Adobe Acrobat Pro v21 . From this , we obtained the skater name , TES base , GOE and total subscores , and PCS subscores and linked them to each skating performance video . To train segmentation ( described later ) , we manually annotated when each element was performed . This was done by researchers , with advice from practicing figure skaters . With the data prepared , we can model the prediction for automatic judgment , which we describe next . Finally , for simplicity and to provide sufficient data per class , we aggregated technical elements into broad categories of Jump , Spin , Step Sequence and Transition . Future work can train to recognize more specific action labels with more data . 4 . 2 Base embedding for implicit knowledge The first step in IRIS is to predict a vector representation of the video . As is common for video data , we train a 3D CNN ğ‘€ 0 , specifically I3D [ 7 ] , to take a video tensor input ğ’™ and predict an embedding Ë† ğ’› ğ‘¡ of the video to represent information in the video . It predicts a vector embedding for each 0 . 534 - sec time window ( 16 frames at 29 . 97 fps like in [ 34 , 45 ] ) . Each video has up to 356 windows with zero - padding , i . e . , all videos are no longer than 3min 10s . Hence , Ë† ğ’› ğ‘¡ is a 2D tensor with vectors across time . Much prior work use these embeddings to predict the judgement score by inputting them into a feedfoward neural network [ 28 ] . We compare against this baseline in our modeling study later . 4 . 3 Sequence of foreknown actions With IRIS , we extract sequence information ğ’‚ ğœ from the score sheet of each skater . This sequence of actions ( technical elements ) de - scribes which actions to expect in sequence , but not specifically at what times . Nevertheless , assuming that the skater does not change her schedule , this sequence will be very informative to guide the overall scoring . Indeed , human judges know ahead of time what actions to expect from the skater and can judge more accurately . Furthermore , their judgement of each action leverages knowledge of whether the scheduled action is easy or difficult ( affecting the base TES score ) , and helps the judge to narrow down their com - parison of the action against similar ones ( e . g . , jumps against other jumps ) . Surprisingly , this crucial information has been neglected in prior work , and our modeling study evaluation shows its strong effectiveness to improve score prediction performance . Thus , we employed Sequence information for our predictions in IRIS . Specifically , we used it to improve our Segmentation ( described in the next section ) , and to improve TES subscore prediction . 1 http : / / www . isuresults . com / results / ec2014 / IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia + + + + à·œğ’› ğ‘¡ à·ğ’š ğœ‹ à·ğ’š Î”ğœ à·ğ’š ğœ à·œğ‘¦ ğ’š ğœ 0 à· ğ’‚ ğ‘¡ ğ’‚ ğœ ğ’™ à·œğ’› ğœ Segments Subscores ( PCS ) Base Subscores ( TES ) Î” Subscores ( TES ) Subscores ( TES ) Score ğ’š ğœ‹ ğ’š ğœ Sequence Figure 2 : Architecture of IRIS showing how it leverages rubric information ( Sequence Ë† ğ‘ ğ‘¡ and base Subscores Ë† ğ’š ğœ 0 ) to provide explanations ( Segments , TES GOE Î” Subscores Ë† ğ’š Î” ğœ , and TES Ë† ğ’š ğœ and PCS Ë† ğ’š ğœ‹ subscores ) to predict the Score Ë† ğ’š of a figure skating performance in a video ğ’™ . Symmetrical trapeziods ( ğ‘€ 0 , ğ‘€ Î” ğœ , ğ‘€ ğœ‹ ) represent convolutional neural networks , the half - rectangular trapezoid ğ‘€ ğ‘¡ is a temporal convolutional network , and other rectangular shapes are variables ( scalar , vector , tensors ) . Black and blue arrows indicate forward propagation , blue are specifically from rubric inputs , and red arrows indicate training loss . 4 . 4 Segments of when actions happened We use the time series embeddings Ë† ğ’› ğ‘¡ to predict the action Ë† ğ‘ ğ‘¡ at each time window with a temporal convolutional network ( TCN ) [ 3 ] , specifically multi - stage TCN ( MS - TCN ) [ 13 ] ğ‘€ ğ‘¡ . This is a sequence - to - sequence model that hierarchically convolves over time , consid - ering short - term and long - term historical data . We trained ğ‘€ ğ‘¡ using supervised learning with manually annotated segments as ground truth labels ; e . g . , we annotated when a jump starts and ends . TCN prediction suffers from over - segmentation , where there are more segments predicted than the true segments ; e . g . , segment la - bels may flip - flop between different labels instead of being a contin - uous one . First , as in [ 13 ] , we used a smoothing loss regularization using the truncated mean squared error : ğ¿ ğœ‡ = 1 ğ‘‡ (cid:205) ğ‘‡ğ‘¡ max ( ğœ– ğ‘¡ , ğœ– ) , where ğœ– ğ‘¡ = ( log Ë† ğ‘š ( ğ‘¡ ) âˆ’ log Ë† ğ‘š ( ğ‘¡ âˆ’ 1 ) ) 2 is the squared of log differ - ences and ğœ– is the truncation hyperparameter . This merges small segments into fewer , larger ones . Next , we employed a heuristic approach by i ) counting the expected number of each action type ğ‘ known from the score card sequence information ğ‘› ğ‘ , and ii ) se - lecting the longest ğ‘› ğ‘ segments for each action type . All remaining unselected segments were re - labeled as transitions . This results in reasonable segmentation performance ( see Fig . 5 ) . With the time series action labels , we partition the time series embeddings Ë† ğ’› ğ‘¡ based on when specific actions started and ended . These are the action sequence embeddings Ë† ğ’› ğœ . We zero - pad all embeddings to the same length for standardized processing next . 4 . 5 Subscores of multiple specific criteria IRIS predicts several subscores , 7 TES subscore for separate tech - nical elements , and 5 PCS subscores for the overall performance . Having identified each sequence element , we can predict a TES subscore for each element . Noting that the score cards used by human judges split TES subscores into Base and GOE , where Base is pre - determined , we realize that only GOE needs to be predicted . Knowing the Base score is highly informative , thus predicting GOE is a much simpler problem which would be more accurate than pre - dicting the full TES subscore . Furthermore , knowing the expected element being performed at the sequence helps the judge to narrow the scope of judging . Hence , for each sequence step ğœ , we train a conditional convolutional neural network ( c - CNN ) ğ‘€ Î” ğœ that takes the sequence embedding Ë† ğ’› ğœ with its corresponding actual action label ğ‘ ğœ to predict the GOE Ë† ğ‘¦ Î” ğœ for that sequence . To obtain the total subscore for this element , we add the Base and GOE subscores , i . e . , Ë† ğ‘¦ ğœ = Ë† ğ‘¦ ğœ 0 + Ë† ğ‘¦ Î” ğœ . The total TES subscore for all technical elements are simply added together , i . e . , Ë† ğ‘¦ Î£ ğœ = (cid:205) ğœ Ë† ğ‘¦ ğœ . We predict PCS subscores differently , since they are based on overall performance instead of specific actions . Using the time series embeddings of the whole video Ë† ğ’› ğ‘¡ , we train a multi - task CNN model ğ‘€ ğœ‹ to predict multiple PCS subscores Ë† ğ’š ğ‘ ğ‘– together . Training a multi - task model instead of multiple , independent models improves accuracy due to correlations between the predictions . The PCS subscores are combined into a total PCS subscore , i . e . , Ë† ğ‘¦ Î£ ğœ‹ = (cid:205) ğœ‹ Ë† ğ‘¦ ğœ‹ . 4 . 6 Score Finally , the total Score is predicted by simply adding the TES and PCS scores together , i . e . , Ë† ğ‘¦ = Ë† ğ‘¦ Î£ ğœ + Ë† ğ‘¦ Î£ ğœ‹ . In summary , IRIS exploits knowledge that judges already know from rubric score cards , and adheres to the criteria that human judges use to guide and account for their decisions . These meth - ods help IRIS to be 1 ) significantly more accurate , 2 ) intrinsically interpretable to show its working to derive its final score , and 3 ) trustworthy since it uses the domain - standard rubric for judging . 4 . 7 Implementation We implemented IRIS using PyTorch and trained on an Nvidia RTX A6000 GPU , with a learning rate of 0 . 0005 , batch size 60 , and the Adam optimizer . Training converged within 300 epochs . We trained IRIS to predict Segments Ë† ğ’‚ ğ‘¡ , TES Î” Subscores Ë† ğ’š Î” ğœ , PCS Subscores Ë† ğ’š ğœ‹ with uniform training loss hyperparameters , i . e . , all 1 . IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Matsuyama , Kawaguchi , and Lim # Executed Elements Base GOE AI TES 1 3T 4 . 10 â€“ 0 . 31 3 . 79 2 3Lo + 2T 6 . 40 0 . 27 6 . 67 3 LSp3 2 . 40 0 . 85 3 . 25 4 2A 3 . 63 0 . 25 3 . 88 5 FSSp2 2 . 30 â€“ 0 . 24 2 . 06 6 CCoSp4 3 . 50 0 . 76 4 . 26 7 StSq4 3 . 90 1 . 83 5 . 73 26 . 23 29 . 64 Viktoria HELGESSON Score : 54 . 68 Program Components Factor PCS AI Skating Skills 0 . 80 6 . 32 Transition / Linking Footwork 0 . 80 6 . 14 Performance / Execution 0 . 80 6 . 33 Choreography / Composition 0 . 80 6 . 42 Interpretation 0 . 80 6 . 09 25 . 04 0 : 32 / 3 : 02 Transition Jump Step Sequence Spin Figure 3 : IRIS visualization for the UI variant showing the predicted judgement Score and all rubric - informed explanations : Sequence , Segments , Subscores . i ) On the left , the video player with image frame , play button and current / end time is shown for all conditions , along with the Score number ( top right ) . ii ) The executed elements names and Sequence # in the table is shown in the Sequence condition . iii ) The colored timeline bar and legend of action colors is shown in the Segments condition . iv ) The remaining GOE AI and PCS AI numbers in the score scheet table are shown in the Subscores condition . 5 EVALUATIONS We evaluated IRIS in three stages . First , we demonstrate its in - terpretability with a visualization of an ice skating performance . Second , we conducted a quantitative modeling study comparing it against the baseline models . Third , we conducted a formative user study with experienced figure skaters to qualitatively investigate the usefulness and trustworthiness of its rubric - informed features . 5 . 1 Demonstration Study We presented the four key rubric - informed features in a unified visualization user interface ( UI ) shown to participants ( see Fig . 3 ) . The key aspects of the user interface are a video player on the left , and score sheet on the right . The total Score is displayed on the top - right corner . To show Segments information , we augment the video player by overlaying a color - annotated timeline over the seek bar . The colors indicate which of the four action types ( transition , jump , spin , and step sequence ) were automatically identified , and when they occurred . Users can examine specific actions by tracking the seek bar to the annotated location , and playing the video . To present the Sequence and Subscores information , we leverage the score sheet layout typical of the ISU Judging System ( IJS ) [ 37 ] . We simplified the table to minimally include the elements ( to be ) ex - ecuted , Technical Elements Scores ( TES ) , and Program Components Scores ( PCS ) . The TES subscores are calculated for each element , and further subdivided into a Base score which is standard for the scheduled element , and known ahead of time , the AI predicted Grade of Execution ( GOE ) to judge how much the skaterâ€™s perfor - mance was above or below average , and total TES subscore which is Base + GOE . The PCS is also divided into separate program com - ponents ( e . g . , skating skills , choreography ) , which are determined across the whole skating performance . There is a factor multiplied on the PCS subscores , which is 1 . 00 for male skaters and 0 . 80 for female skaters . The bold numbers at the bottom of columns indicate sums across elements or components . Finally , experienced figure skaters know that the Score = TES + PCS . 5 . 2 Modeling Studies We conducted two modeling studies on the performance of IRIS : 1 ) an ablation study to examine how each rubric - informed feature improves the model prediction performance , and 2 ) a benchmark - ing study to evaluate whether IRIS performs better than baseline models . Just like prior studies , all models were trained on randomly chosen 120 videos , and evaluated with a test set of 30 videos . We describe the evaluation metrics , models compared , and results . 5 . 2 . 1 Evaluation metrics . For all comparisons , we evaluated how well each model can predict the human score in the dataset . Most prior work evaluated using the Spearman rank correlation coeffi - cient that indicates whether the ranking of skaters were preserved based on actual and predicted scores . However , this neglects how much the score may have changed . We further calculated the Pear - son correlation coefficient to see how well IRIS can preserve the linearity in score predictions . We also add to prior work by exam - ining how well IRIS can predict TES and PCS subscores separately . To evaluate segmentation performance , we calculated the Dice coefficient which indicates how much the predicted segment Ë† ğ’‚ overlaps with the actual segment ğ’‚ , i . e . , 2 ( ğ’‚ Â· Ë† ğ’‚ ) / ( | ğ’‚ | 2 + | Ë† ğ’‚ | 2 ) . 5 . 2 . 2 Comparison models . For the ablation study , we evaluated different variants of the IRIS model to predict combinations of base and explanatory information : Score Ë† ğ‘¦ , TES + PCS ( Ë† ğ‘¦ Î£ ğœ , Ë† ğ‘¦ Î£ ğœ‹ ) , Subscores ( Ë† ğ’š ğœ , ğ’š ğœ‹ ) , Î” Subscores ( Ë† ğ’š Î” ğœ , ğ’š ğœ‹ ) , Segments Ë† ğ’‚ ğ‘¡ . For the benchmarking study , we evaluated IRIS against the fol - lowing state - of - the - art models : 1 ) 3DCNN - LSTM [ 28 ] puts video embeddings into an LSTM to predict the total scores with long - duration patterns . 2 ) 3DCNN - BiLSTM [ 28 ] uses Bi - directional LSTM instead of plain LSTM to capture time patterns forwards and backwards in the video . This reduces early forgetting . 3 ) 3DCNN - ResNet - GCN [ 45 ] uses a video 3DCNN model with graph convolutional network to model cross - dependencies be - tween video segments . IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Score Score + TES + PCS Score + TES + PCS + Subscores Score + TES + PCS + Subscores + Segments Score + TES + PCS + Î” Subscores Score + TES + PCS + Î” Subscores + Segments 3DCNN - LSTM [ 7 ] 3DCNN - BiLSTM [ 7 ] Multi - scale LSTM [ 9 ] Multi - scale CNN [ 8 ] 3DCNN - ResNet - GCN [ 10 ] 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 C o rr e l a t i o n o f e a c h m o d e l v s . hu m a n j u dg e Total Spearman TES Spearman PCS Spearman Total Pearson TES Pearson PCS Pearson Figure 4 : Results of ablation study ( Left ) showing how predicting different interpretability features affects model performance in IRIS , and model benchmarking study ( Right ) showing performance of competitive baselines . Higher Spearman or Pearson correlation coefficients indicate better performance and more similarity between the model and human judges . Horizontal gray line is highest benchmark performance for Spearman coefficient . See Tables 2 and 3 for numeric details . Transition Jump Step Sequence Spin Predicted Actual Predicted ( over - segmented ) Segments over time Figure 5 : Example predicted and actual segments for a skat - ing sequence showing : 1 ) the initial prediction with over - segmentation , 2 ) corrected predicted segmentation , which matches reasonably closely with 3 ) the actual segmentation . 4 ) Multi - scale CNN [ 26 ] uses pose heatmaps , and varying CNN kernel sizes to capture short and long time dependencies . 5 ) Multi - scale LSTM [ 42 ] aggregates multiple LSTM models to capture temporal patterns at varying time resolutions . 5 . 2 . 3 Results . Fig . 4 ( Left ) and Appendix Table 2 shows the results from our ablation study . As expected , combining all interpretability features led to the best performing model . Predicting with subscores helped IRIS to improve its total score prediction performance , indi - cating that decomposed analysis was beneficial . Leveraging the TES base subscores helped to significantly improve the performance for predicting TES . Segmentation helped to improve TES subscore prediction performance significantly , TES Î” subscore prediction per - formance slightly , but not necessarily for PCS . The segmentation in IRIS had good a Dice coefficient of 0 . 79 . Fig . 5 shows an example segmentation . Better segmentation with higher IoU also generally resulted in better TES score prediction as shown in Fig . 6 . Fig . 4 ( Right ) and Appendix Table 3 shows the results from our benchmarking study . The basic 3DCNN and multi - task predictions in simpler IRIS models were weaker than other baselines , but IRIS was better when it modeled with TES base subscores and segmenta - tion . This indicates that being informed with rubrics and including interpretability can be more beneficial than using esoteric , generic time - series features that other baselines have . IoU Low Med High 0 . 4 0 . 8 1 . 2 1 . 6 T E S S q u a r e d E rr o r Figure 6 : Error in TES prediction is lower with better seg - mentation IoU . IoU divided by tertiles Low ( 0 - 33 % - tile ) , Med ( 33 - 66 % ) , High ( 66 - 100 % ) . Error bars indicate standard error . 5 . 3 Formative User Study Having shown that IRIS improves model prediction performance , we next evaluated whether the explanations provided by IRIS are useful . We did so with practicing figure skaters who have domain expertise to meaningfully interpret the explanations . Our research objective is to determine the usefulness and trustworthiness of vari - ous rubric - informed explanations for automatic action quality as - sessment in figure skating videos . We investigated why users found each explanation feature useful and how they used the feature . The study was approved by our institutionâ€™s institutional review board . 5 . 3 . 1 Experiment Method . We conducted a formative user study to collect user opinions and experiences on different features of IRIS by implementing different variants of the IRIS visualization . Each variant progressively adds more UI features : â€¢ Score ( S ) that shows the AI prediction to judge the performance . â€¢ S + Sequence ( SS ) that also shows the sequence of technical elements that the performer planned to execute . This is what human judges know ahead of each judging session . It does not include any subscore information for each technical element . â€¢ SS + Segments ( SSS ) that also shows the AI prediction of the time segments when each technical element was executed . â€¢ SSS + Subscores ( SSSS ) that also shows all TES ( base and GOE ) and PCS subscores . This is the most comprehensive UI . IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Matsuyama , Kawaguchi , and Lim For simplicity , we implemented the UI variants in presentation slides , though the data is computed by the IRIS model , and thus realistic of the model behavior . We presented all variants within - subjects to the participant , with fixed order : S , SS , SSS , SSSS . This was not counterbalanced , since each latter variant contains more information and would confound with a learning effect . We priori - tized a qualitative formative study instead of a larger - scale summa - tive study to gather nuanced insights regarding the use of rubric - informed explanations , and because of the difficulty to recruit a large number of trained domain experts ( practicing figure skaters ) . 5 . 3 . 2 Experiment Procedure . The procedure each participant was : 1 ) Introduction about the experiment objective and procedure , and description about the figure skating videos . 2 ) Consent to participate with their voice and interactions recorded . 3 ) Two video sessions with : i ) Video is played lasting about 3 minutes . ii ) Score ( S ) prediction is displayed after the video is played , fol - lowed by several interview questions discussed ( details later ) . iii ) Sequence ( SS ) information of the technical elements is next displayed and questions posed again . iv ) Segments ( SSS ) of when each action was performed are next displayed and questions posed again . v ) Subscores ( SSSS ) are displayed and questions posed again . vi ) Interview questions to ask about : 1 ) Do you agree or disagree with the AI judgement score ? [ 7 - pt Likert ] . Why or why not ? 2 ) How do you think the AI scored the performance ? Do you agree or disagree with its method ? Why or why not ? 3 ) Do you agree or disagree that you understand how AI pre - dicts scores ? ( Perceived Understanding ) [ 7 - pt Likert ] 4 ) Would you score differently ? How so ? 5 ) Which part of the UI did you find ( not ) useful ? Why ( not ) ? 6 ) Any suggestions for improvement ? 5 . 3 . 3 Findings . We recruited 10 participants through snowball sampling from two university figure skating clubs and one orga - nization in Japan . Discussions were conducted in Japanese and translated to English . We acknowledge the sample limitation due to the challenge of recruiting rare expertise , yet benefited from very interesting expert feedback . Their average age was 23 . 8 ( SD = 8 . 6 ) years old and 8 were female . All had experience in figure skating spanning 2 - 13 years ( M = 6 . 6 ) . We determined participant expertise by asking their Skill Test Level [ 39 ] . One participant had not taken the test , thus we considered her a novice . Others were skilled with M = 2 . 6 out of 5 ( SD = 1 . 7 ) , and max = 5 . Due to Covid - 19 restrictions , the experiment was conducted remotely over Zoom with screen sharing and audio recording . The interview sessions took 81 . 8min on average ( SD = 10 . 5 ) with $ 20 USD compensation . Figure 7 shows the quantitative results of participant ratings . Participants somewhat agreed with the AI prediction score ( M = 1 . 05 ) , though there was no difference among UI variants . They found the full UI variant with all explanations easiest to understand ( M SSSS = 1 . 05 ) , followed by the UI with Segments ( M SSS = âˆ’ 0 . 05 ) , but did not understand how the AI worked when only viewing the Score ( M S = âˆ’ 0 . 85 ) and Sequence variants ( M SS = âˆ’ 0 . 70 ) . Next , we analyzed the recorded sessions and performed a the - matic analysis in terms of our research objective to understand the helpfulness and trustworthiness of each rubric - informed feature . Themes were generated and refined by the first and last authors . Score only . Participants mostly depended on their own expe - rience and intuition to judge how the score was derived , or were clueless about how the AI could perceive and make judgements . P4 mentioned that she " didnâ€™t know and didnâ€™t have a clue " . P5 saw that the skater " was off - balance on the first jump , so [ he ] was going to drop [ the score ] a little more [ than what the AI scored ] " . P10 " felt that it was difficult to understand the scoring method from the images and scores alone , and felt that it would be most convincing if we could come up with a protocol like the scoring table used in competitions " . Some participants attributed advanced capabilities to the AI , even though this was not implemented . P2 " thought the AI was scoring the number of jump rotations , the flow after landing , and so on " . Sequence . With the sequence information , many participants realized that the AI considered specific actions in its judgement . P2 remarked that " by knowing the elements in advance , I knew approx - imately how many points the basic score would be , which made it easier to predict the score , and I thought [ the AIâ€™s score ] was reason - able . " . P3 noted that he had interpreted the AI reasoning in the Score condition " by guessing earlier , but had suspected that [ the AI ] was calculating based on the basic points of each element . " The Sequence enabled participants to discuss the scoring more carefully . P5 saw that the skater " was doing 3T + 3T with low score for a triple - turn combo , but I thought [ the score ] could have be a little higher . " Segments . Participants highly appreciated seeing the action seg - mentation explanations from the AI . This allowed them to perceive what the AI could notice . P8 " was amazed that the steps were properly recognized . I thought that if it could recognize them , it must be able to judge the movements of the feet as well . " Several participants felt that the segments would be helpful for less - knowledgeable spectators . " Jumps and spins can be understood even by people who donâ€™t know skating , but some people donâ€™t know about steps , so it may be useful for them " ( P10 ) . P2 " was impressed to see that the jumps , spins , and steps were detected " . However , he also remarked " on the other hand , it is difficult to judge whether the score is appropriate because it is not clear at what step and for what reason the score is given . " Thus , he and other participants still wondered how each segment was scored . Some participants also noticed inaccuracies in the segmentation . P5 " thought the spin detection was a little fast . It appeared that the step had started before the step sequence was detected . " P6 noted that " the end of spin detection was early . " Nevertheless , these participants remained positive about the AI segmentation capability . Subscores . With the subscores , participants found the AI most useful . P3 felt that " it is easier to be convinced if you can see the detailed scores . I thought it was more convincing when I could see the completion of each element ( e . g . , first jump , fifth spin , etc . ) , and the points for possible mistakes were minus . " . P9 felt that the AI " can be evaluated objectively without human error . It is useful to visualize how much points are added or subtracted from the base score . " The subscores stimulated much discussion about the judging correctness . P1 argued that " fewer points should have been deducted for [ the ] 3Lo [ technical element ] , but everything else seems to be OK . " P2 remarked that " 3T3T looked very well put together , so more points should be awarded . I thought FCSp3 deserved a higher score because [ the skater ] was doing something more difficult than the usual Camel spin . The ( high ) score for Choreography was surprisingly satisfactory . " IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Score ( S ) S + Sequence ( SS ) SS + Segments ( SSS ) SSS + Subscores ( SSSS ) Score ( S ) S + Sequence ( SS ) SS + Segments ( SSS ) SSS + Subscores ( SSSS ) Score ( S ) S + Sequence ( SS ) SS + Segments ( SSS ) SSS + Subscores ( SSSS ) - 2 - 1 0 1 2 - 2 - 1 0 1 2 - 2 - 1 0 1 2 P r e d i c t i o n A g r ee m e n t P e r c e i v e d U n d e r s t a n d i n g X A I A g r ee m e n t Figure 7 : Mean participant ratings on agreement with AI score , perceived understanding of AI judgement , and agreement with UI Variants with different XAI features ( S , SS , SSS , SSSS ) . Error bars indicate 95 % confidence interval . Participants also requested for more details . P3 wanted to " know the type of jump . " , rather than just knowing that one was observed . " It would be nice to know the types of spins , postures , deformations , as well as the types and difficulties of spins . This would be useful for judging the level of the skaters " ( P3 ) . P9 " I would also like to know the height of the spin , and whether 3D expression is achieved . " Participants were mostly positive about the TES subscore pre - dictions , but had varying opinions about the more qualitative PCS subscores . P5 identified that " the PCS scores , such as Skating skills , were all in the 6 - point range . The top scorers get about 7 points , so I thought [ the AIâ€™s score ] was reasonable . " P7 found " the difference in PCS scores is disconcerting . I donâ€™t think it would be like this if a person gave it . " The nuance and subjectivity in judging PCS also raised questions . P1 commented that " even if [ the PCS score ] is de - composed , I wonder how it is determined . " Participants also needed more information to help to properly assess some PCS aspects . P8 found that " it is hard to know how the expressiveness of the music , which is one of the best aspects of figure skating , was scored . " In summary , participants found that the latter explanation fea - tures aligned well with their expected criteria for judging figure skating , and highly appreciated the increasing details to account for the final score . The potential of IRIS is neatly expressed by P9 : " I think that the introduction of AI will be beneficial to the players because it will eliminate bias and subjectivity in this area , as I believe that human preferences have various influences . " 6 DISCUSSION We have shown that IRIS performed better than the current methods to automatically judge figure skating , and is perceived as more understandable and trustworthy by domain experts . We discuss limitations and generalizations of IRIS for interpretable AQA . 6 . 1 Generalizing rubric - informed AQA In this work , we studied figure skating as a representative example of sports that are qualitatively judged , yet have established rubrics for consistency . We believe that IRIS can be used to automatically score other qualitative sports or performances that are guided by rubrics , such as , dancing , skiing , diving . Note that while some sports activities are very brief , they can still contain a sequence of actions . For example , a diving action consists of jumping from a diving board , performing an acrobatic sequence in the air , and entering the water . Appropriate labeling of these steps would provide more detailed information to divers and audiences and improve their understanding of the performance . IRIS is not meant for sports that use other scoring methods , such as counting goals ( e . g . , soccer , basketball , golf , fencing , boxing ) . IRIS is not designed to generate a rubric ; decision tree or rule mining approaches can already do this , albeit with the risk of learning spurious relationships . 6 . 2 Need for interpretability of artistic quality In this work , we provided detailed information on the technical and artistic criteria of figure skating by revealing additive segments and subscores . In our user study , our participants requested for more justification on how each technical element or overall qualities are considered high or low scoring . Future work can provide two approaches to address this . 1 ) Use XAI saliency map techniques ( e . g . , [ 33 ] ) to highlight important pixels or poses that the model focused on for predicting each TES or PCS score . However , without properly constraints ( e . g . , on pose joints ) , the highlights may be spurious . 2 ) Use heuristic methods based on meaningful movement features , such as Laban Movement Analysis ( LMA ) [ 4 ] that interprets the artistic movement with four dimensions ( Body , Effort , Shape , and Space ) to describe the quality of movement , body posture and shape , and how the body is using the space . A good quality as defined with LMA should correspond to a high technical or artistic score . 7 CONCLUSION We proposed IRIS , an I nterpretable R ubric - I nformed S egmentation method to predict and explain video - based action quality assess - ment ( AQA ) . IRIS is : 1 ) interpretable to account for its judgement process , 2 ) informed by a rubric to judge by specific criteria , and 3 ) performs segmentation to locate specific sections to judge each criteria . We implemented IRIS for the application domain of figure skating to predict a Score and explain with Sequence , Segments , and Subscores . As a result , by being rubric - informed , IRIS is better guided to make more accurate score predictions , is more account - able by justifying its decisions based on domain - accepted criteria , and hence more trustworthy to practicing figure skaters . ACKNOWLEDGMENTS This work was supported by the Singapore Ministry of Education ( MOE ) Academic Research Fund Tier 2 T2EP20121 - 0040 , and the Japan Science and Technology Agency ( JST ) CREST Program JP - MJCR21F2 . IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia Matsuyama , Kawaguchi , and Lim REFERENCES [ 1 ] Ahmed Alqaraawi , Martin Schuessler , Philipp WeiÃŸ , Enrico Costanza , and Nadia Berthouze . 2020 . Evaluating saliency map explanations for convolutional neural networks : a user study . In Proceedings of the 25th International Conference on Intelligent User Interfaces . 275 â€“ 285 . [ 2 ] Sebastian Bach , Alexander Binder , GrÃ©goire Montavon , Frederick Klauschen , Klaus - Robert MÃ¼ller , and Wojciech Samek . 2015 . On pixel - wise explanations for non - linear classifier decisions by layer - wise relevance propagation . PloS one 10 , 7 ( 2015 ) , e0130140 . [ 3 ] Shaojie Bai , J Zico Kolter , and Vladlen Koltun . 2018 . An empirical evaluation of generic convolutional and recurrent networks for sequence modeling . arXiv preprint arXiv : 1803 . 01271 ( 2018 ) . [ 4 ] Irmgard Bartenieff and Dori Lewis . 2013 . Body movement : Coping with the environment . Routledge . [ 5 ] Gedas Bertasius , Hyun Soo Park , Stella X Yu , and Jianbo Shi . 2017 . Am I a baller ? basketball performance assessment from first - person videos . In Proceedings of the IEEE international conference on computer vision . 2177 â€“ 2185 . [ 6 ] Gavin Brown , Stephen Irving , and Peter Keegan . 2008 . An Introduction to Ed - ucational Assessment , Measurement , and Evaluation : Improving the Quality of Teacher - Based Assessment . Pearson Education New Zealand . [ 7 ] Joao Carreira and Andrew Zisserman . 2017 . Quo Vadis , Action Recognition ? A New Model and the Kinetics Dataset . ( 2017 ) . https : / / doi . org / 10 . 48550 / ARXIV . 1705 . 07750 [ 8 ] Felipe Costa , Sixun Ouyang , Peter Dolog , and Aonghus Lawlor . 2018 . Auto - matic generation of natural language explanations . In Proceedings of the 23rd international conference on intelligent user interfaces companion . 1 â€“ 2 . [ 9 ] Phillip Dawson . 2017 . Assessment rubrics : towards clearer and more replicable design , research and practice . Assessment & Evaluation in Higher Education 42 , 3 ( 2017 ) , 347 â€“ 360 . [ 10 ] Jonathan Dodge , Q Vera Liao , Yunfeng Zhang , Rachel KE Bellamy , and Casey Dugan . 2019 . Explaining models : an empirical study of how explanations impact fairnessjudgment . In Proceedingsofthe24thinternationalconferenceonintelligent user interfaces . 275 â€“ 285 . [ 11 ] Hazel Doughty , Dima Damen , and Walterio Mayol - Cuevas . 2018 . Whoâ€™s better ? whoâ€™s best ? pairwise deep ranking for skill determination . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 6057 â€“ 6066 . [ 12 ] Hazel Doughty , Walterio Mayol - Cuevas , and Dima Damen . 2019 . The pros and cons : Rank - aware temporal attention for skill determination in long videos . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 7862 â€“ 7871 . [ 13 ] Yazan Abu Farha and Jurgen Gall . 2019 . Ms - tcn : Multi - stage temporal convolu - tionalnetworkforactionsegmentation . In ProceedingsoftheIEEE / CVFConference on Computer Vision and Pattern Recognition . 3575 â€“ 3584 . [ 14 ] Joan L Herman et al . 1992 . A practical guide to alternative assessment . ERIC . [ 15 ] Aftab Khan , James Nicholson , and Thomas PlÃ¶tz . 2017 . Activity recognition for quality assessment of batting shots in cricket using a hierarchical represen - tation . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 1 , 3 ( 2017 ) , 1 â€“ 31 . [ 16 ] Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda Viegas , et al . 2018 . Interpretability beyond feature attribution : Quantitative testing with concept activation vectors ( TCAV ) . In International conference on machine learning . PMLR , 2668 â€“ 2677 . [ 17 ] Pang Wei Koh , Thao Nguyen , Yew Siang Tang , Stephen Mussmann , Emma Pier - son , BeenKim , andPercyLiang . 2020 . Conceptbottleneckmodels . In International Conference on Machine Learning . PMLR , 5338 â€“ 5348 . [ 18 ] Yongjun Li , Xiujuan Chai , and Xilin Chen . 2018 . End - to - end learning for action quality assessment . In Pacific Rim Conference on Multimedia . Springer , 125 â€“ 134 . [ 19 ] Zhenqiang Li , Yifei Huang , Minjie Cai , and Yoichi Sato . 2019 . Manipulation - skill assessment from videos with spatial attention network . In Proceedings of the IEEE / CVF International Conference on Computer Vision Workshops . 0 â€“ 0 . [ 20 ] Q Vera Liao , Daniel Gruen , and Sarah Miller . 2020 . Questioning the AI : informing design practices for explainable AI user experiences . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 â€“ 15 . [ 21 ] Brian Y Lim , Joseph P Cahaly , Chester YF Sng , and Adam Chew . 2023 . Dia - grammatization : Rationalizing with diagrammatic AI explanationsfor abductive reasoning on hypotheses . arXiv preprint arXiv : 2302 . 01241 ( 2023 ) . [ 22 ] Brian Y Lim and Anind K Dey . 2009 . Assessing demand for intelligibility in context - aware applications . In Proceedings of the 11th international conference on Ubiquitous computing . 195 â€“ 204 . [ 23 ] YanLyu , HangxinLu , MinKyungLee , GerhardSchmitt , andBrianYLim . 2023 . IF - City : Intelligible Fair City Planning to Measure , Explain and Mitigate Inequality . IEEE Transactions on Visualization and Computer Graphics ( 2023 ) . [ 24 ] Martijn Millecamp , Nyi Nyi Htun , Cristina Conati , and Katrien Verbert . 2019 . To explain or not to explain : the effects of personal characteristics when explaining music recommendations . In Proceedings of the 24th International Conference on Intelligent User Interfaces . 397 â€“ 407 . [ 25 ] Mahdiar Nekoui , Fidel Omar Tito Cruz , and Li Cheng . 2020 . Falcons : Fast learner - grader for contorted poses in sports . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops . 900 â€“ 901 . [ 26 ] MahdiarNekoui , FidelOmarTitoCruz , andLiCheng . 2021 . EAGLE - Eye : Extreme - Pose Action Grader Using Detail Birdâ€™s - Eye View . In Proceedings of the IEEE / CVF Winter Conference on Applications of Computer Vision . 394 â€“ 402 . [ 27 ] Paritosh Parmar and Brendan Morris . 2019 . Action quality assessment across multiple actions . In 2019 IEEE winter conference on applications of computer vision ( WACV ) . IEEE , 1468 â€“ 1476 . [ 28 ] Paritosh Parmar and Brendan Tran Morris . 2016 . Learning To Score Olympic Events . arXiv . https : / / doi . org / 10 . 48550 / ARXIV . 1611 . 05125 [ 29 ] Hamed Pirsiavash , Carl Vondrick , and Antonio Torralba . 2014 . Assessing the Quality of Actions . In Computer Vision â€“ ECCV 2014 , David Fleet , Tomas Pajdla , Bernt Schiele , and Tinne Tuytelaars ( Eds . ) . Springer International Publishing , Cham , 556 â€“ 571 . [ 30 ] Kun Qian , Marina Danilevsky , Yannis Katsis , Ban Kawas , Erick Oduor , Lucian Popa , and Yunyao Li . 2021 . XNLP : A living survey for XAI research in natural languageprocessing . In 26thInternationalConferenceonIntelligentUserInterfaces - Companion . 78 â€“ 80 . [ 31 ] Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Christian Von Der Weth , and Brian Y . Lim . 2021 . Directed diversity : Leveraging language embedding dis - tances for collective creativity in crowd ideation . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 â€“ 35 . [ 32 ] Cynthia Rudin . 2019 . Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . Nature Machine Intelligence 1 , 5 ( 2019 ) , 206 â€“ 215 . [ 33 ] Ramprasaath R Selvaraju , Michael Cogswell , Abhishek Das , Ramakrishna Vedan - tam , Devi Parikh , and Dhruv Batra . 2017 . Grad - cam : Visual explanations from deep networks via gradient - based localization . In Proceedings of the IEEE interna - tional conference on computer vision . 618 â€“ 626 . [ 34 ] Yansong Tang , Zanlin Ni , Jiahuan Zhou , Danyang Zhang , Jiwen Lu , Ying Wu , and Jie Zhou . 2020 . Uncertainty - aware Score Distribution Learning for Action Quality Assessment . ( 2020 ) . https : / / doi . org / 10 . 48550 / ARXIV . 2006 . 07665 [ 35 ] Robin Thompson , Ilias Kyriazakis , Amey Holden , Patrick Olivier , and Thomas PlÃ¶tz . 2015 . Dancing with horses : automated quality feedback for dressage riders . In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing . 325 â€“ 336 . [ 36 ] Du Tran , Lubomir Bourdev , Rob Fergus , Lorenzo Torresani , and Manohar Paluri . 2014 . LearningSpatiotemporalFeatureswith3DConvolutionalNetworks . ( 2014 ) . https : / / doi . org / 10 . 48550 / ARXIV . 1412 . 0767 [ 37 ] International Skating Union . 2021 . ISU Judging System . https : / / isu . org / figure - skating / rules / fsk - judging - system . [ Online ; accessed 14 - October - 2022 ] . [ 38 ] International Skating Union . 2021 . Program Components â€“ Single Skating , Pair Skating , Ice Dance . https : / / www . isu . org / isu - statutes - constitution - regulations - technical - rules - 2 / isu - judging - system / ice - dance / 26043 - program - component - chart / file . [ Online ; accessed 14 - October - 2022 ] . [ 39 ] International Skating Union . 2022 . Special Regulations & Technical Rules - Single & Pair Skating and Ice Dance . https : / / www . isu . org / figure - skating / rules / fsk - regulations - rules / file . [ Online ; accessed 14 - October - 2022 ] . [ 40 ] Danding Wang , Qian Yang , Ashraf Abdul , and Brian Y Lim . 2019 . Designing theory - driven user - centric explainable AI . In Proceedings of the 2019 CHI confer - ence on human factors in computing systems . 1 â€“ 15 . [ 41 ] Xinru Wang and Ming Yin . 2021 . Are explanations helpful ? a comparative study of the effects of explanations in ai - assisted decision - making . In 26th International Conference on Intelligent User Interfaces . 318 â€“ 328 . [ 42 ] Chengming Xu , Yanwei Fu , Bing Zhang , Zitian Chen , Yu - Gang Jiang , and Xi - angyang Xue . 2020 . Learning to Score Figure Skating Sport Videos . IEEE Trans - actions on Circuits and Systems for Video Technology 30 , 12 ( 2020 ) , 4578 â€“ 4590 . https : / / doi . org / 10 . 1109 / TCSVT . 2019 . 2927118 [ 43 ] Xumin Yu , Yongming Rao , Wenliang Zhao , Jiwen Lu , and Jie Zhou . 2021 . Group - aware Contrastive Regression for Action Quality Assessment . ( 2021 ) . https : / / doi . org / 10 . 48550 / ARXIV . 2108 . 07797 [ 44 ] Xumin Yu , Yongming Rao , Wenliang Zhao , Jiwen Lu , and Jie Zhou . 2021 . Group - aware contrastive regression for action quality assessment . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 7919 â€“ 7928 . [ 45 ] Ling - An Zeng , Fa - Ting Hong , Wei - Shi Zheng , Qi - Zhi Yu , Wei Zeng , Yao - Wei Wang , and Jian - Huang Lai . 2020 . Hybrid dynamic - static context - aware attention network for action assessment in long videos . In Proceedings of the 28th ACM International Conference on Multimedia . 2526 â€“ 2534 . [ 46 ] Qiang Zhang and Baoxin Li . 2014 . Relative hidden markov models for video - basedevaluationofmotionskillsinsurgicaltraining . IEEEtransactionsonpattern analysis and machine intelligence 37 , 6 ( 2014 ) , 1206 â€“ 1218 . [ 47 ] WencanZhangandBrianYLim . 2022 . TowardsRelatableExplainableAIwiththe Perceptual Process . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . 1 â€“ 24 . IRIS : Interpretable Rubric - Informed Segmentation for Action Quality Assessment IUI 2023 , Mar 27 â€“ 31 , 2023 , Sydney , Australia A APPENDIX Table 2 : Results of ablation study showing how predicting different interpretability features affects model performance in IRIS . Higher Spearman or Pearson correlation coefficients indicate better performance . Bold indicates best performance . See Fig . 4 . Features Metrics Spearman Pearson ScoreË† ğ‘¦ TES + PCS ( Ë† ğ‘¦ Î£ ğœ , Ë† ğ‘¦ Î£ ğœ‹ ) Subscores ( Ë† ğ’š ğœ , ( Ë† ğ’š ğœ‹ ) Î” Subscores ( Ë† ğ’š Î” ğœ , ( Ë† ğ’š ğœ‹ ) SegmentsË† ğ’‚ ğ‘¡ TES PCS Total TES PCS Total âœ“ 0 . 341 0 . 473 0 . 347 0 . 314 0 . 560 0 . 414 âœ“ âœ“ 0 . 477 0 . 555 0 . 366 0 . 369 0 . 560 0 . 412 âœ“ âœ“ âœ“ 0 . 428 0 . 578 0 . 428 0 . 356 0 . 629 0 . 498 âœ“ âœ“ âœ“ âœ“ 0 . 681 0 . 612 0 . 617 0 . 592 0 . 678 0 . 659 âœ“ âœ“ âœ“ 0 . 863 0 . 641 0 . 802 0 . 877 0 . 723 0 . 831 âœ“ âœ“ âœ“ âœ“ 0 . 880 0 . 621 0 . 829 0 . 891 0 . 726 0 . 831 Table 3 : Spearman correlation coefficients of the total Score prediction for different baseline models . Numbers are quoted from the original papers . Refer to Table 2 to compare different IRIS model variants against these baselines . See Fig . 4 . Models Spearman 3DCNN - LSTM [ 28 ] 0 . 479 3DCNN - BiLSTM [ 28 ] 0 . 587 3DCNN - ResNet - GCN [ 45 ] 0 . 615 Multi - scale CNN [ 26 ] 0 . 610 Multi - scale LSTM [ 42 ] 0 . 590